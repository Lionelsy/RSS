<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 07 Aug 2025 14:44:01 +0800</lastBuildDate>
    <item>
      <title>Occupancy Learning with Spatiotemporal Memory</title>
      <link>http://arxiv.org/abs/2508.04705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV2025. Project website:  https://matthew-leng.github.io/stocc&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ST-Occ是一种场景级的占用表示学习框架，通过时空记忆和记忆注意力两个核心设计，有效学习具有时间一致性的时空特征，解决了跨帧聚合3D占用信息的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D occupancy成为自动驾驶中很有前景的环境感知表示方法，可以精细建模周围环境，但存在高处理成本和体素不确定性、动态性等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决高效跨多个输入帧聚合3D占用信息的挑战，提高3D占用预测任务中的时空表示质量。&lt;h4&gt;方法&lt;/h4&gt;提出ST-Occ框架，包含时空记忆（捕捉全面历史信息并高效存储）和记忆注意力（将当前占用表示条件化到时空记忆上，具有不确定性和动态感知模型）两个核心设计。&lt;h4&gt;主要发现&lt;/h4&gt;通过利用多帧输入之间的时间依赖性，显著提高了3D占用预测任务中学习的时空表示。&lt;h4&gt;结论&lt;/h4&gt;ST-Occ比最先进的方法高出3 mIoU，并将时间不一致性降低了29%，表明该方法在3D占用预测任务中具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;三维占用成为自动驾驶中很有前景的感知表示方法，可以精细建模周围环境。然而，由于高处理成本和体素的不确定性和动态性，很难高效地跨多个输入帧聚合三维占用信息。为解决这一问题，我们提出了ST-Occ，一种场景级的占用表示学习框架，能够有效学习具有时间一致性的时空特征。ST-Occ包含两个核心设计：时空记忆，捕捉全面的历史信息并通过场景级表示高效存储；以及记忆注意力，将当前占用表示条件化到时空记忆上，具有不确定性和动态感知模型。我们的方法通过利用多帧输入之间的时间依赖性，显著提高了为3D占用预测任务学习的时空表示。实验表明，我们的方法比最先进的方法高出3 mIoU，并将时间不一致性降低了29%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效地聚合3D占用表示在多个输入帧上的时间信息，以改善自动驾驶中的环境感知。这个问题在现实中很重要，因为自动驾驶需要准确理解周围环境，而3D占用表示能提供比传统BEV表示更详细的几何和语义信息；时间信息的整合可以提高感知系统的鲁棒性，特别是在遮挡、光照变化等挑战性场景中；现有的时间融合方法在处理3D占用表示时面临效率和有效性挑战，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有时间融合方法在3D占用表示上的局限性，包括效率、不确定性和动态性问题。他们借鉴了BEVFormer（时间自注意力）和BEVDet（历史特征融合）等基于BEV的时间建模方法，以及PasCo的不确定性感知思路。在此基础上，创新性地提出了'统一时间建模'范式，设计了场景中心坐标下的时空记忆模块来高效存储历史信息，并通过记忆注意力模块实现不确定性和动态感知的融合，解决了现有方法的三个主要挑战：效率、不确定性和动态性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用场景中心坐标下的时空记忆高效存储和利用历史信息，并通过记忆注意力模块将当前帧占用表示与历史信息融合，同时处理不确定性和动态性问题。整体流程：1)输入多视图图像，通过占用编码器提取自车中心的占用表示；2)从时空记忆中提取相关历史信息；3)使用记忆注意力模块融合当前表示与历史信息，生成融合表示；4)更新时空记忆，存储融合表示和时间属性；5)使用融合表示进行占用预测。时空记忆存储历史类别激活、平均对数方差和占用流等属性，记忆注意力通过MLP预测不确定性并使用占用流补偿动态实例运动。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一时间建模范式，在场景中心坐标下构建统一记忆，大幅减少内存计算需求；2)时空记忆模块，存储全面历史信息；3)记忆注意力模块，实现不确定性和动态感知的融合；4)时间一致性评估指标mSTCV。相比之前工作不同：1)效率更高，训练内存比递归方法少10%，比堆叠方法少40%，推理FPS达8.65；2)性能更好，Occ3D基准测试上高出最先进方法3 mIoU；3)时间一致性降低29%；4)能更有效地利用长期时间依赖(2.8倍于FB-OCC)；5)明确处理不确定性和动态性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ST-Occ提出了一种基于时空记忆的场景级别3D占用表示学习方法，通过统一的时间建模范式，实现了高效、鲁棒且具有不确定性和动态感知能力的占用预测，显著提升了自动驾驶环境感知的性能和时间一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D occupancy becomes a promising perception representation for autonomousdriving to model the surrounding environment at a fine-grained scale. However,it remains challenging to efficiently aggregate 3D occupancy over time acrossmultiple input frames due to the high processing cost and the uncertainty anddynamics of voxels. To address this issue, we propose ST-Occ, a scene-leveloccupancy representation learning framework that effectively learns thespatiotemporal feature with temporal consistency. ST-Occ consists of two coredesigns: a spatiotemporal memory that captures comprehensive historicalinformation and stores it efficiently through a scene-level representation anda memory attention that conditions the current occupancy representation on thespatiotemporal memory with a model of uncertainty and dynamic awareness. Ourmethod significantly enhances the spatiotemporal representation learned for 3Doccupancy prediction tasks by exploiting the temporal dependency betweenmulti-frame inputs. Experiments show that our approach outperforms thestate-of-the-art methods by a margin of 3 mIoU and reduces the temporalinconsistency by 29%.</description>
      <author>example@mail.com (Ziyang Leng, Jiawei Yang, Wenlong Yi, Bolei Zhou)</author>
      <guid isPermaLink="false">2508.04705v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
  <item>
      <title>BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.04702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出BEVCon，一个简单而有效的对比学习框架，用于提高自动驾驶中的鸟瞰图（BEV）感知能力。通过引入两个对比学习模块，该方法改进了BEV特征和图像骨干网络，在nuScenes数据集上实现了最先进基线最高2.4%的mAP提升。&lt;h4&gt;背景&lt;/h4&gt;鸟瞰图（BEV）感知提供了周围环境自上而下的视图表示，对3D目标检测、分割和轨迹预测等任务至关重要。以往的研究主要集中在增强BEV编码器和任务特定的头部，而忽略了BEV模型中表征学习的潜力。&lt;h4&gt;目的&lt;/h4&gt;探索表征学习在BEV感知模型中的潜力，通过对比学习框架来提高BEV感知性能。&lt;h4&gt;方法&lt;/h4&gt;BEVCon引入了两个对比学习模块：1）实例特征对比模块，用于优化BEV特征；2）视角对比模块，增强图像骨干网络。基于检测损失的密集对比学习改进了BEV编码器和骨干网络的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的大量实验表明，BEVCons实现了持续的性能提升，比最先进的基线最高提升2.4%的mAP。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了表征学习在BEV感知中的关键作用，并为传统的任务特定优化提供了互补的途径。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了BEVCon，一个简单而有效的对比学习框架，旨在提高自动驾驶中的鸟瞰图（BEV）感知。BEV感知提供了周围环境自上而下的视图表示，使其对3D目标检测、分割和轨迹预测任务至关重要。虽然先前的工作主要集中在增强BEV编码器和任务特定的头部，但我们解决了BEV模型中表征学习的未被充分探索的潜力。BEVCon引入了两个对比学习模块：一个用于优化BEV特征的实例特征对比模块和一个增强图像骨干网络的视角对比模块。在检测损失基础上设计的密集对比学习导致BEV编码器和骨干网络的改进特征表示。在nuScenes数据集上的大量实验证明，BEVCons实现了持续的性能提升，比最先进的基线最高提升2.4%的mAP。我们的结果强调了表征学习在BEV感知中的关键作用，并为传统的任务特定优化提供了互补的途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决BEV（鸟瞰图）感知模型中的表示学习不足问题。当前研究主要集中在优化BEV编码器和任务特定头，而对表示学习的潜力探索较少。这个问题在现实中非常重要，因为BEV感知是自动驾驶和机器人系统的关键组件，提供统一的环境表示用于3D目标检测、分割和轨迹预测等任务。改进表示学习可以在不增加额外数据或标注的情况下，充分利用现有训练数据中的信息，提升模型性能，这对实际应用中数据有限和硬件受限的场景尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先进行了初步实验，发现传统的对比学习方法（如MoCo v2）在BEV感知任务中效果不佳，原因包括驾驶数据集样本多样性不足，以及图像级对比方法不适合关注特定对象的需求。基于这些发现，作者设计了专门的BEVCon框架，包含两个对比学习模块：实例特征对比模块和透视视图对比模块。作者借鉴了SimCLR等对比学习的基本思想，但针对BEV感知的特殊挑战进行了定制化设计，如使用区域感知池化解决标注框重叠问题，以及多层级特征对比增强监督信号。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入两个专门的对比学习模块，增强BEV感知模型中的表示学习，同时与检测任务结合实现联合训练。整体流程：1) 输入多摄像头图像并生成对比样本对；2) 使用EMA更新的图像主干网络提取特征；3) 通过视图变换模块将图像特征转换为BEV表示；4) 实例特征对比模块对同一实例在不同增强视图中的BEV特征进行对比；5) 透视视图对比模块对图像中与标注相关的区域特征进行对比；6) 结合对比学习损失和检测损失进行联合优化训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 首次将对比学习引入BEV检测模型增强表示学习；2) 设计两个专门的对比学习模块；3) 创建统一框架实现对比学习与检测任务的联合优化；4) 证明方法在各种BEV架构上的通用性。相比之前工作，不同之处在于：专注于表示学习而非架构优化；解决传统对比学习方法在BEV中的样本多样性和对比层次问题；不需要额外数据或标注；通过实例级和区域级对比提供更精细的监督信号。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BEVCon通过两个专门的对比学习模块，在不增加额外数据或标注的情况下，显著提升了BEV感知模型的表示学习能力和检测性能，且可广泛应用于各种BEV感知架构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present BEVCon, a simple yet effective contrastive learning frameworkdesigned to improve Bird's Eye View (BEV) perception in autonomous driving. BEVperception offers a top-down-view representation of the surroundingenvironment, making it crucial for 3D object detection, segmentation, andtrajectory prediction tasks. While prior work has primarily focused onenhancing BEV encoders and task-specific heads, we address the underexploredpotential of representation learning in BEV models. BEVCon introduces twocontrastive learning modules: an instance feature contrast module for refiningBEV features and a perspective view contrast module that enhances the imagebackbone. The dense contrastive learning designed on top of detection lossesleads to improved feature representations across both the BEV encoder and thebackbone. Extensive experiments on the nuScenes dataset demonstrate that BEVConachieves consistent performance gains, achieving up to +2.4% mAP improvementover state-of-the-art baselines. Our results highlight the critical role ofrepresentation learning in BEV perception and offer a complementary avenue toconventional task-specific optimizations.</description>
      <author>example@mail.com (Ziyang Leng, Jiawei Yang, Zhicheng Ren, Bolei Zhou)</author>
      <guid isPermaLink="false">2508.04702v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Robust Intervention Representations with Delta Embeddings</title>
      <link>http://arxiv.org/abs/2508.04492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;因果表征学习是提高模型泛化能力和鲁棒性的重要方法。本文提出关注干预表征而非仅关注场景变量表征，能有效提高分布外鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习在过去几年引起了显著研究兴趣，因为它可以改善模型的泛化能力和鲁棒性。干预图像对的因果表征具有这样的特性：只有与受干预/动作影响的场景元素相对应的变量才会从开始状态到结束状态发生变化。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索干预表征在提高模型分布外(OOD)鲁棒性方面的作用，提出一种不需要额外监督的因果表征学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出干预可以用一个对视觉场景不变且在影响的因果变量方面稀疏的'因果Delta嵌入'(Causal Delta Embedding)来表示。基于此，提出一个能够从图像对中学习因果表征的框架，不需要额外的监督。&lt;h4&gt;主要发现&lt;/h4&gt;在Causal Triplet挑战中的实验表明，因果Delta嵌入在OOD设置中非常有效，在合成和真实世界基准中都显著超过了基线性能。&lt;h4&gt;结论&lt;/h4&gt;关注干预表征是提高模型分布外鲁棒性的有效策略，为因果表征学习提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;因果表征学习在过去几年引起了显著的研究兴趣，作为提高模型泛化能力和鲁棒性的一种手段。干预图像对的因果表征具有这样的特性：只有与受干预/动作影响的场景元素相对应的变量才会从开始状态到结束状态发生变化。尽管该领域的大多数工作都集中在识别和表示因果模型下的场景变量，而较少关注干预本身的表征。在这项工作中，我们表明，提高分布外(OOD)鲁棒性的有效策略是关注潜在空间中的干预表征。具体来说，我们提出干预可以用一个对视觉场景不变且在影响的因果变量方面稀疏的'因果Delta嵌入'来表示。利用这一见解，我们提出了一个能够从图像对中学习因果表征的框架，不需要任何额外的监督。在Causal Triplet挑战中的实验表明，因果Delta嵌入在OOD设置中非常有效，在合成和真实世界基准中都显著超过了基线性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning has attracted significant research interestduring the past few years, as a means for improving model generalization androbustness. Causal representations of interventional image pairs, have theproperty that only variables corresponding to scene elements affected by theintervention / action are changed between the start state and the end state.While most work in this area has focused on identifying and representing thevariables of the scene under a causal model, fewer efforts have focused onrepresentations of the interventions themselves. In this work, we show that aneffective strategy for improving out of distribution (OOD) robustness is tofocus on the representation of interventions in the latent space. Specifically,we propose that an intervention can be represented by a Causal Delta Embeddingthat is invariant to the visual scene and sparse in terms of the causalvariables it affects. Leveraging this insight, we propose a framework that iscapable of learning causal representations from image pairs, without anyadditional supervision. Experiments in the Causal Triplet challenge demonstratethat Causal Delta Embeddings are highly effective in OOD settings,significantly exceeding baseline performance in both synthetic and real-worldbenchmarks.</description>
      <author>example@mail.com (Panagiotis Alimisis, Christos Diou)</author>
      <guid isPermaLink="false">2508.04492v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Representation Learning with Massive Unlabeled Data for Rumor Detection</title>
      <link>http://arxiv.org/abs/2508.04252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用大规模未标记主题数据和通用图自监督学习方法提高谣言检测的泛化能力，解决了现有方法难以获得大规模标记数据集和面对新事件性能下降的问题。&lt;h4&gt;背景&lt;/h4&gt;社交媒体发展导致谣言迅速传播，对社会和经济造成很大危害。基于谣言传播结构学习的方法相比其他方法特别有效。&lt;h4&gt;目的&lt;/h4&gt;解决现有谣言检测方法中难以获得大规模标记数据集导致的泛化能力低和性能下降问题，提高模型在各种主题上的语义学习能力。&lt;h4&gt;方法&lt;/h4&gt;使用三种典型图自监督方法（InfoGraph、JOAO和GraphMAE）在两种训练策略下进行验证；收集覆盖十年多种主题的谣言数据集缓解未标记主题数据与谣言数据之间的时间和主题差异；利用社交媒体平台抓取的大规模未标记主题数据结合声明传播结构。&lt;h4&gt;主要发现&lt;/h4&gt;通用图自监督学习方法优于专门为谣言检测任务设计的方法，在少样本条件下表现良好，展示了借助大量未标记主题数据集的更好泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过大规模未标记主题数据和通用图自监督学习方法可以有效提高谣言检测的泛化能力，特别是在面对新事件时。&lt;h4&gt;翻译&lt;/h4&gt;随着社交媒体的发展，谣言迅速传播，对社会和经济造成极大危害。因此，开发了许多有效的谣言检测方法，其中基于谣言传播结构学习的方法相比其他方法尤为有效。然而，现有方法仍存在许多问题，包括难以获得大规模标记的谣言数据集，这导致泛化能力低，并且在新的热点事件或新兴事件上性能下降，因为谣言具有时效性。为了解决上述问题，本研究利用从微博和Twitter等社交媒体平台抓取的大规模未标记主题数据，结合声明传播结构，提高图表示学习模型在各种主题上的语义学习能力。我们使用三种典型的图自监督方法（InfoGraph、JOAO和GraphMAE）在两种常用的训练策略下，验证通用图半监督方法在谣言检测任务中的性能。此外，为缓解未标记主题数据与谣言数据之间的时间和主题差异，我们还收集了来自微博辟谣平台的覆盖十年（从2022年前十年）多种主题的谣言数据集。实验表明，这些通用图自监督学习方法优于之前专门为谣言检测任务设计的方法，并且在少样本条件下表现良好，展示了借助我们大量未标记主题数据集的更好泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the development of social media, rumors spread quickly, cause great harmto society and economy. Thereby, many effective rumor detection methods havebeen developed, among which the rumor propagation structure learning basedmethods are particularly effective compared to other methods. However, theexisting methods still suffer from many issues including the difficulty toobtain large-scale labeled rumor datasets, which leads to the lowgeneralization ability and the performance degeneration on new events sincerumors are time-critical and usually appear with hot topics or newly emergentevents. In order to solve the above problems, in this study, we usedlarge-scale unlabeled topic datasets crawled from the social media platformWeibo and Twitter with claim propagation structure to improve the semanticlearning ability of a graph reprentation learing model on various topics. Weuse three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAEin two commonly used training strategies, to verify the performance of generalgraph semi-supervised methods in rumor detection tasks. In addition, foralleviating the time and topic difference between unlabeled topic data andrumor data, we also collected a rumor dataset covering a variety of topics overa decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Ourexperiments show that these general graph self-supervised learning methodsoutperform previous methods specifically designed for rumor detection tasks andachieve good performance under few-shot conditions, demonstrating the bettergeneralization ability with the help of our massive unlabeled topic dataset.</description>
      <author>example@mail.com (Chaoqun Cui, Caiyan Jia)</author>
      <guid isPermaLink="false">2508.04252v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Fair Multi-View Clustering</title>
      <link>http://arxiv.org/abs/2508.04071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对抗性公平多视图聚类（AFMVC）框架，将公平性学习集成到表示学习过程中，通过对抗性训练去除敏感属性信息，确保聚类分配不受敏感属性影响。&lt;h4&gt;背景&lt;/h4&gt;聚类分析是数据挖掘和机器学习中的基本问题，多视图聚类因其能够整合多个视图的互补信息而受到关注。然而，现有方法主要关注聚类性能，而忽略了公平性这一以人为中心应用中的关键问题。&lt;h4&gt;目的&lt;/h4&gt;解决多视图聚类中的公平性问题，提出一种不依赖敏感属性与底层聚类结构对齐假设的方法，实现公平且有效的聚类。&lt;h4&gt;方法&lt;/h4&gt;提出对抗性公平多视图聚类（AFMVC）框架，采用对抗性训练从学习特征中去除敏感属性信息，并通过KL散度将视图特定聚类分配与公平不变共识分布对齐，以保持聚类一致性。&lt;h4&gt;主要发现&lt;/h4&gt;通过KL散度对齐视图特定聚类分配与公平不变共识分布可以在不显著损害公平性的情况下保持聚类一致性，为框架提供了理论保证。&lt;h4&gt;结论&lt;/h4&gt;AFMVC在具有公平性约束的数据集上实现了卓越的公平性和具有竞争力的聚类性能，优于现有的多视图聚类和公平感知聚类方法。&lt;h4&gt;翻译&lt;/h4&gt;聚类分析是数据挖掘和机器学习中的一个基本问题。近年来，多视图聚类由于其能够整合来自多个视图的互补信息而受到越来越多的关注。然而，现有方法主要关注聚类性能，而公平性-以人为中心应用中的关键关注点-在很大程度上被忽视了。尽管最近的研究探索了多视图聚类中的群体公平性，但大多数方法对聚类分配施加显式正则化，依赖于敏感属性和底层聚类结构之间的对齐。然而，这种假设在实践中往往不成立，并且可能降低聚类性能。在本文中，我们提出了一种对抗性公平多视图聚类（AFMVC）框架，将公平性学习集成到表示学习过程中。具体而言，我们的方法采用对抗性训练从根本上从学习到的特征中去除敏感属性信息，确保结果聚类分配不受其影响。此外，我们通过KL散度将视图特定的聚类分配与公平不变的共识分布对齐，理论上证明了这保持了聚类一致性而不会显著损害公平性，从而为我们的框架提供了额外的理论保证。在具有公平性约束的数据集上进行的大量实验表明，与现有的多视图聚类和公平感知聚类方法相比，AFMVC实现了卓越的公平性和具有竞争力的聚类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cluster analysis is a fundamental problem in data mining and machinelearning. In recent years, multi-view clustering has attracted increasingattention due to its ability to integrate complementary information frommultiple views. However, existing methods primarily focus on clusteringperformance, while fairness-a critical concern in human-centeredapplications-has been largely overlooked. Although recent studies have exploredgroup fairness in multi-view clustering, most methods impose explicitregularization on cluster assignments, relying on the alignment betweensensitive attributes and the underlying cluster structure. However, thisassumption often fails in practice and can degrade clustering performance. Inthis paper, we propose an adversarial fair multi-view clustering (AFMVC)framework that integrates fairness learning into the representation learningprocess. Specifically, our method employs adversarial training to fundamentallyremove sensitive attribute information from learned features, ensuring that theresulting cluster assignments are unaffected by it. Furthermore, wetheoretically prove that aligning view-specific clustering assignments with afairness-invariant consensus distribution via KL divergence preservesclustering consistency without significantly compromising fairness, therebyproviding additional theoretical guarantees for our framework. Extensiveexperiments on data sets with fairness constraints demonstrate that AFMVCachieves superior fairness and competitive clustering performance compared toexisting multi-view clustering and fairness-aware clustering methods.</description>
      <author>example@mail.com (Mudi Jiang, Jiahui Zhou, Lianyu Hu, Xinying Liu, Zengyou He, Zhikui Chen)</author>
      <guid isPermaLink="false">2508.04071v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>FeDaL: Federated Dataset Learning for Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2508.04045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, scaling FL to time series foundation models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为联邦数据集学习(FeDaL)的新方法，用于解决时间序列基础模型(TSFMs)中的数据集异质性问题，通过联邦学习范式学习数据集无关的时间表示，并添加领域偏差消除和全局偏差消除两种机制来提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;数据集异质性引入了显著的领域偏差，从根本上损害了时间序列基础模型(TSFMs)的泛化能力，而这一挑战尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;重新思考使用联邦学习范式发展TSFMs，解决异构时间序列数据的问题，提高模型的跨数据集泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出联邦数据集学习(FeDaL)方法，利用联邦学习的分布式架构将异构时间序列数据集分解为共享的通用知识和保留的个性化知识，并添加领域偏差消除(DBE)和全局偏差消除(GBE)两种互补机制来减轻局部和全局偏差。&lt;h4&gt;主要发现&lt;/h4&gt;在跨越8个任务的8个真实世界数据集上评估了FeDaL的跨数据集泛化能力，包括表示学习和下游时间序列分析，并与54个基线方法进行比较；分析了联邦扩展行为，显示了数据量、客户端数量和加入率在去中心化情况下如何影响模型性能。&lt;h4&gt;结论&lt;/h4&gt;FeDaL方法能有效处理时间序列数据中的异构性问题，通过联邦学习架构和偏差消除机制显著提高了模型泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;数据集异质性引入了显著的领域偏差，从根本上损害了时间序列基础模型(TSFMs)的泛化能力，然而这一挑战仍然探索不足。本文使用联邦学习范式重新思考了TSFMs的发展。我们提出了一种新型的联邦数据集学习(FeDaL)方法，通过学习数据集无关的时间表示来处理异构时间序列。具体而言，联邦学习的分布式架构自然地将异构TS数据集分解为共享的通用知识和保留的个性化知识。此外，基于TSFM架构，FeDaL通过添加两种互补机制明确减轻了局部和全局偏差：领域偏差消除(DBE)和全局偏差消除(GBE)。FeDaL的跨数据集泛化能力已在跨越8个任务的8个真实世界数据集上进行了广泛评估，包括表示学习和下游时间序列分析，并与54个基线方法进行了比较。我们进一步分析了联邦扩展行为，展示了在去中心化情况下，数据量、客户端数量和加入率如何影响模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset-wise heterogeneity introduces significant domain biases thatfundamentally degrade generalization on Time Series Foundation Models (TSFMs),yet this challenge remains underexplored. This paper rethink the development ofTSFMs using the paradigm of federated learning. We propose a novel FederatedDataset Learning (FeDaL) approach to tackle heterogeneous time series bylearning dataset-agnostic temporal representations. Specifically, thedistributed architecture of federated learning is a nature solution todecompose heterogeneous TS datasets into shared generalized knowledge andpreserved personalized knowledge. Moreover, based on the TSFM architecture,FeDaL explicitly mitigates both local and global biases by adding twocomplementary mechanisms: Domain Bias Elimination (DBE) and Global BiasElimination (GBE). FeDaL`s cross-dataset generalization has been extensivelyevaluated in real-world datasets spanning eight tasks, including bothrepresentation learning and downstream time series analysis, against 54baselines. We further analyze federated scaling behavior, showing how datavolume, client count, and join rate affect model performance underdecentralization.</description>
      <author>example@mail.com (Shengchao Chen, Guodong Long, Jing Jiang)</author>
      <guid isPermaLink="false">2508.04045v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval</title>
      <link>http://arxiv.org/abs/2508.04028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双提示学习框架DCAR，用于解决图像文本检索任务中区分细粒度属性和相似子类别的挑战，通过联合优化属性和类别特征实现了精确的图像文本匹配。&lt;h4&gt;背景&lt;/h4&gt;提示学习在适应预训练视觉语言模型到各种下游任务(如图像分类)方面表现出色，但在图像文本检索任务中的应用更具挑战性。&lt;h4&gt;目的&lt;/h4&gt;实现精确的图像文本匹配，提高CLIP模型在下游ITR任务上的性能，特别是解决区分细粒度属性和相似子类别的问题。&lt;h4&gt;方法&lt;/h4&gt;提出双提示学习与联合类别-属性重加权(DCAR)框架，从语义和视觉维度动态调整提示向量：(1)在属性层面，基于文本-图像互信息相关性动态更新属性描述权重；(2)在类别层面，引入多角度负样本并采用类别匹配加权来学习子类别区分。&lt;h4&gt;主要发现&lt;/h4&gt;挑战在于下游数据中细粒度属性和相似子类别的区分；通过构建FDRD数据集(包含1,500+细粒度类别和230,000+图像-标题对)验证了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;DCAR在FDRD数据集上的实验表明，该方法在现有基线上达到了最先进的性能，能有效解决下游ITR任务中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;最近，提示学习在将预训练的视觉语言模型(VLMs)适应到各种下游任务(如图像分类)方面表现出显著成功。然而，将其应用于下游的图像文本检索(ITR)任务更具挑战性。我们发现挑战在于区分下游数据的细粒度属性和相似子类别。为了应对这一挑战，我们提出了双提示学习与联合类别-属性重加权(DCAR)，这是一个新颖的双提示学习框架，用于实现精确的图像文本匹配。该框架从语义和视觉维度动态调整提示向量，以提高CLIP在下游ITR任务上的性能。基于提示范式，DCAR联合优化属性和类别特征以增强细粒度表示学习。具体而言，(1)在属性层面，它基于文本-图像互信息相关性动态更新属性描述的权重；(2)在类别层面，它引入多角度的负样本并采用类别匹配加权来学习子类别区分。为了验证我们的方法，我们构建了细粒度类别描述检索数据集(FDRD)，它作为下游数据领域中ITR的有挑战性的基准。它覆盖了超过1,500个下游细粒度类别和230,000个图像-标题对，并具有详细的属性注释。在FDRD上的大量实验表明，DCAR在现有基线上达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, prompt learning has demonstrated remarkable success in adaptingpre-trained Vision-Language Models (VLMs) to various downstream tasks such asimage classification. However, its application to the downstream Image-TextRetrieval (ITR) task is more challenging. We find that the challenge lies indiscriminating both fine-grained attributes and similar subcategories of thedownstream data. To address this challenge, we propose Dual prompt Learningwith Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learningframework to achieve precise image-text matching. The framework dynamicallyadjusts prompt vectors from both semantic and visual dimensions to improve theperformance of CLIP on the downstream ITR task. Based on the prompt paradigm,DCAR jointly optimizes attribute and class features to enhance fine-grainedrepresentation learning. Specifically, (1) at the attribute level, itdynamically updates the weights of attribute descriptions based on text-imagemutual information correlation; (2) at the category level, it introducesnegative samples from multiple perspectives with category-matching weighting tolearn subcategory distinctions. To validate our method, we construct theFine-class Described Retrieval Dataset (FDRD), which serves as a challengingbenchmark for ITR in downstream data domains. It covers over 1,500 downstreamfine categories and 230,000 image-caption pairs with detailed attributeannotations. Extensive experiments on FDRD demonstrate that DCAR achievesstate-of-the-art performance over existing baselines.</description>
      <author>example@mail.com (Yifan Wang, Tao Wang, Chenwei Tang, Caiyang Yu, Zhengqing Zang, Mengmi Zhang, Shudong Huang, Jiancheng Lv)</author>
      <guid isPermaLink="false">2508.04028v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification</title>
      <link>http://arxiv.org/abs/2508.03967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RAVID，首个利用视觉检索增强生成(RAG)的AI生成图像检测框架。通过动态检索相关图像并融合视觉语言模型，RAVID显著提高了检测的准确性和鲁棒性，在UniversalFakeDetect基准测试上达到93.85%的平均准确率。&lt;h4&gt;背景&lt;/h4&gt;现有RAG方法主要集中在文本领域，对视觉知识探索不足；同时，现有检测方法在泛化能力和鲁棒性方面存在困难，通常依赖低级伪影和模型特定特征，限制了适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效检测AI生成图像的框架，解决现有方法在泛化能力和鲁棒性方面的局限性，探索视觉检索增强生成在图像检测领域的应用。&lt;h4&gt;方法&lt;/h4&gt;RAVID使用微调的CLIP图像编码器(RAVID CLIP)并通过添加类别相关提示改进表示学习；集成视觉语言模型(VLM)来融合检索到的图像与查询；给定查询图像时，RAVID生成嵌入并检索相关图像，将这些图像与查询结合形成增强输入供VLM处理。&lt;h4&gt;主要发现&lt;/h4&gt;在UniversalFakeDetect基准测试上(涵盖19个生成模型)，RAVID达到93.85%的平均准确率；在退化条件下，RAVID的平均准确率为80.27%，优于最先进的模型C2P-CLIP(63.44%)；在高斯模糊和JPEG压缩场景中均有显著改进。&lt;h4&gt;结论&lt;/h4&gt;RAVID成功将视觉检索增强生成技术应用于AI生成图像检测，通过动态检索相关图像并利用视觉语言模型融合，有效解决了现有方法在泛化能力和鲁棒性方面的局限。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们介绍了RAVID，这是第一个利用视觉检索增强生成(RAG)的AI生成图像检测框架。虽然RAG方法在减轻基础模型的事实性错误方面显示出潜力，但它们主要集中在文本领域，导致视觉知识探索不足。与此同时，现有的检测方法在泛化能力和鲁棒性方面存在困难，通常依赖于低级伪影和模型特定特征，限制了它们的适应性。为了解决这个问题，RAVID动态检索相关图像以增强检测。我们的方法使用微调的CLIP图像编码器RAVID CLIP，并通过添加类别相关提示来改进表示学习。我们进一步集成了视觉语言模型(VLM)来融合检索到的图像与查询，丰富输入并提高准确性。给定查询图像，RAVID使用RAVID CLIP生成嵌入，从数据库中检索最相关的图像，并将这些图像与查询图像结合，形成VLM的增强输入。在涵盖19个生成模型的UniversalFakeDetect基准测试上，RAVID以93.85%的平均准确率实现了最先进的性能。在鲁棒性方面，RAVID也优于传统方法，即使在图像退化(如高斯模糊和JPEG压缩)的情况下也能保持高准确率。具体而言，在退化条件下，RAVID的平均准确率为80.27%，而最先进的模型C2P-CLIP为63.44%，这表明在高斯模糊和JPEG压缩场景中都有一致的改进。代码将在接受后公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce RAVID, the first framework for AI-generated imagedetection that leverages visual retrieval-augmented generation (RAG). While RAGmethods have shown promise in mitigating factual inaccuracies in foundationmodels, they have primarily focused on text, leaving visual knowledgeunderexplored. Meanwhile, existing detection methods, which struggle withgeneralization and robustness, often rely on low-level artifacts andmodel-specific features, limiting their adaptability. To address this, RAVIDdynamically retrieves relevant images to enhance detection. Our approachutilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced withcategory-related prompts to improve representation learning. We furtherintegrate a vision-language model (VLM) to fuse retrieved images with thequery, enriching the input and improving accuracy. Given a query image, RAVIDgenerates an embedding using RAVID CLIP, retrieves the most relevant imagesfrom a database, and combines these with the query image to form an enrichedinput for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on theUniversalFakeDetect benchmark, which covers 19 generative models, show thatRAVID achieves state-of-the-art performance with an average accuracy of 93.85%.RAVID also outperforms traditional methods in terms of robustness, maintaininghigh accuracy even under image degradations such as Gaussian blur and JPEGcompression. Specifically, RAVID achieves an average accuracy of 80.27% underdegradation conditions, compared to 63.44% for the state-of-the-art modelC2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEGcompression scenarios. The code will be publicly available upon acceptance.</description>
      <author>example@mail.com (Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid)</author>
      <guid isPermaLink="false">2508.03967v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>CoughViT: A Self-Supervised Vision Transformer for Cough Audio Representation Learning</title>
      <link>http://arxiv.org/abs/2508.03764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ISWC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoughViT的新型预训练框架，用于学习通用咳嗽声音表示，以解决呼吸声音诊断中的标签和数据稀缺问题，提高诊断性能。&lt;h4&gt;背景&lt;/h4&gt;医生通常在诊断过程中评估呼吸声音以了解患者气道状况。近年来，基于AI的呼吸声音诊断系统在呼吸疾病检测中显示出成功，代表了早期和可及诊断的重要进展。&lt;h4&gt;目的&lt;/h4&gt;提出CoughViT框架，用于学习通用咳嗽声音表示，以增强有限数据任务中的诊断性能。&lt;h4&gt;方法&lt;/h4&gt;采用掩码数据建模以自监督学习方式训练特征编码器，以解决标签稀缺问题。在三个诊断重要的咳嗽分类任务上评估与其他预训练策略的比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，CoughViT的表示在增强下游任务性能方面匹配或超过了当前最先进的监督音频表示。&lt;h4&gt;结论&lt;/h4&gt;CoughViT框架能够有效解决呼吸声音诊断中的数据稀缺问题，提高诊断性能。&lt;h4&gt;翻译&lt;/h4&gt;医生通常在诊断过程中评估呼吸声音，为了解患者气道状况提供见解。近年来，基于AI的呼吸声音诊断系统在呼吸疾病检测中显示出成功。这些系统代表了早期和可及诊断的重要进展，对于及时治疗至关重要。然而，标签和数据稀缺仍然是关键挑战，特别是对于COVID-19以外的疾病，限制了诊断性能和可靠评估。在本文中，我们提出了CoughViT，一种新颖的预训练框架，用于学习通用咳嗽声音表示，以增强有限数据任务中的诊断性能。为了解决标签稀缺问题，我们采用掩码数据建模以自监督学习方式训练特征编码器。我们在三个诊断重要的咳嗽分类任务上评估了我们的方法与其他预训练策略的比较。实验结果表明，我们的表示在增强下游任务性能方面匹配或超过了当前最先进的监督音频表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physicians routinely assess respiratory sounds during the diagnostic process,providing insight into the condition of a patient's airways. In recent years,AI-based diagnostic systems operating on respiratory sounds, have demonstratedsuccess in respiratory disease detection. These systems represent a crucialadvancement in early and accessible diagnosis which is essential for timelytreatment. However, label and data scarcity remain key challenges, especiallyfor conditions beyond COVID-19, limiting diagnostic performance and reliableevaluation. In this paper, we propose CoughViT, a novel pre-training frameworkfor learning general-purpose cough sound representations, to enhance diagnosticperformance in tasks with limited data. To address label scarcity, we employmasked data modelling to train a feature encoder in a self-supervised learningmanner. We evaluate our approach against other pre-training strategies on threediagnostically important cough classification tasks. Experimental results showthat our representations match or exceed current state-of-the-art supervisedaudio representations in enhancing performance on downstream tasks.</description>
      <author>example@mail.com (Justin Luong, Hao Xue, Flora D. Salim)</author>
      <guid isPermaLink="false">2508.03764v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Entity Representation Learning Through Onsite-Offsite Graph for Pinterest Ads</title>
      <link>http://arxiv.org/abs/2508.02609v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络和知识图谱嵌入的广告排名模型改进方法，通过构建大规模异构图和创新的TransRA模型，结合大型ID嵌入表技术和基于注意力机制的KGE微调方法，显著提升了广告系统的点击率和转化率预测性能，并在Pinterest的实际应用中取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已广泛应用于工业推荐系统，如GraphSage、TwHIM和LiGNN等模型。这些模型基于用户在平台上的活动构建图，以有效学习节点嵌入。然而，除了用户的线上活动外，他们的线下转化数据对广告模型捕捉购物兴趣同样重要。&lt;h4&gt;目的&lt;/h4&gt;为了更好地利用线下转化数据并探索线上与线下活动之间的联系，本研究旨在构建一个结合用户线上广告互动和线下转化活动的大规模异构图，并开发一种有效的方法将图嵌入整合到广告排名模型中。&lt;h4&gt;方法&lt;/h4&gt;研究构建了一个基于用户线上广告互动和线下转化活动的大规模异构图，并提出了TransRA（带锚点的TransR）这一新型知识图谱嵌入模型。为了解决广告排名模型难以直接整合知识图谱嵌入的问题，采用了大型ID嵌入表技术，并在广告排名模型中创新了一种基于注意力机制的KGE微调方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过所提出的方法，研究在点击率(CTR)和转化率(CVR)预测模型中观察到了显著的AUC提升。该框架已在Pinterest的广告参与模型中部署，带来了2.69%的CTR提升和1.34%的CPC降低。&lt;h4&gt;结论&lt;/h4&gt;研究提出的技术可以被其他大规模工业模型采用，有效提升广告系统的性能表现。通过结合线上和线下用户活动数据，以及创新的图嵌入和微调方法，能够显著改善广告排名模型的预测能力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNN)已广泛应用于工业推荐系统，如GraphSage、TwHIM和LiGNN等模型。在这些工作中，图是基于用户在平台上的活动构建的，并开发了各种图模型来有效学习节点嵌入。除了用户的线上活动外，他们的线下转化数据对广告模型捕捉购物兴趣至关重要。为了更好地利用线下转化数据并探索线上与线下活动之间的联系，我们基于用户的线上广告互动和选择性的线下转化活动构建了一个大规模异构图。此外，我们引入了TransRA（带锚点的TransR），这是一种新型的知识图谱嵌入(KGE)模型，能够更有效地将图嵌入整合到广告排名模型中。然而，我们的广告排名模型最初难以直接整合知识图谱嵌入，并且在离线实验中只观察到适度的提升。为了应对这一挑战，我们采用了大型ID嵌入表技术，并在广告排名模型中创新了一种基于注意力机制的KGE微调方法。结果，我们在点击率(CTR)和转化率(CVR)预测模型中观察到了显著的AUC提升。此外，该框架已在Pinterest的广告参与模型中部署，并带来了2.69%的CTR提升和1.34%的CPC降低。我们相信本文提出的技术可以被其他大规模工业模型所利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNN) have been extensively applied to industryrecommendation systems, as seen in models like GraphSage\cite{GraphSage},TwHIM\cite{TwHIM}, LiGNN\cite{LiGNN} etc. In these works, graphs wereconstructed based on users' activities on the platforms, and various graphmodels were developed to effectively learn node embeddings. In addition tousers' onsite activities, their offsite conversions are crucial for Ads modelsto capture their shopping interest. To better leverage offsite conversion dataand explore the connection between onsite and offsite activities, weconstructed a large-scale heterogeneous graph based on users' onsite adinteractions and opt-in offsite conversion activities. Furthermore, weintroduced TransRA (TransR\cite{TransR} with Anchors), a novel Knowledge GraphEmbedding (KGE) model, to more efficiently integrate graph embeddings into Adsranking models. However, our Ads ranking models initially struggled to directlyincorporate Knowledge Graph Embeddings (KGE), and only modest gains wereobserved during offline experiments. To address this challenge, we employed theLarge ID Embedding Table technique and innovated an attention based KGEfinetuning approach within the Ads ranking models. As a result, we observed asignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)prediction models. Moreover, this framework has been deployed in Pinterest'sAds Engagement Model and contributed to $2.69\%$ CTR lift and $1.34\%$ CPCreduction. We believe the techniques presented in this paper can be leveragedby other large-scale industrial models.</description>
      <author>example@mail.com (Jiayin Jin, Zhimeng Pan, Yang Tang, Jiarui Feng, Kungang Li, Chongyuan Xiang, Jiacheng Li, Runze Su, Siping Ji, Han Sun, Ling Leng, Prathibha Deshikachar)</author>
      <guid isPermaLink="false">2508.02609v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Composed Object Retrieval: Object-level Retrieval via Composed Expressions</title>
      <link>http://arxiv.org/abs/2508.04424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了组合对象检索(COR)这一新任务，超越了传统图像级别检索，实现了对象级别的精确性，并构建了大规模数据集COR127K和统一模型CORE，显著提升了多模态系统中基于用户意图的细粒度视觉内容检索能力。&lt;h4&gt;背景&lt;/h4&gt;基于用户意图检索细粒度视觉内容在多模态系统中仍然是一个挑战。现有的组合图像检索(CIR)方法虽然结合了参考图像与检索文本，但仅限于图像级别的匹配，无法定位特定对象。&lt;h4&gt;目的&lt;/h4&gt;提出组合对象检索(COR)这一全新任务，实现对象级别的精确检索，能够基于组合表达式（结合参考对象和检索文本）检索和分割目标对象，并构建相应的大规模数据集和有效模型。&lt;h4&gt;方法&lt;/h4&gt;构建了COR127K大规模基准数据集，包含408个类别中的127,166个检索三元组；提出了CORE统一端到端模型，集成了参考区域编码、自适应视觉-文本交互和区域级对比学习技术。&lt;h4&gt;主要发现&lt;/h4&gt;CORE模型在基础类别和新颖类别上都显著优于现有模型，为这一具有挑战性的任务建立了简单有效的基线，同时为细粒度多模态检索研究开辟了新方向。&lt;h4&gt;结论&lt;/h4&gt;组合对象检索(COR)任务通过实现对象级别的精确检索，解决了传统方法无法定位特定对象的局限性，为多模态系统中的细粒度视觉内容检索提供了新思路和有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于用户意图检索细粒度视觉内容在多模态系统中仍然是一个挑战。虽然当前组合图像检索(CIR)方法结合了参考图像与检索文本，但它们仅限于图像级别匹配，无法定位特定对象。为此，我们提出了组合对象检索(COR)，这是一个全新的任务，超越了图像级别检索，实现了对象级别的精确性，能够基于组合表达式（结合参考对象和检索文本）检索和分割目标对象。COR在检索灵活性方面提出了重大挑战，要求系统能够识别满足组合表达式的任意对象，同时避免在同一场景中语义相似但不相关的负面对象。我们构建了COR127K，这是第一个大规模COR基准数据集，包含408个类别中的127,166个检索三元组，具有各种语义变换。我们还提出了CORE，一个统一的端到端模型，集成了参考区域编码、自适应视觉-文本交互和区域级对比学习。大量实验表明，CORE在基础类别和新颖类别上都显著优于现有模型，为这一具有挑战性的任务建立了简单有效的基线，同时为细粒度多模态检索研究开辟了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving fine-grained visual content based on user intent remains achallenge in multi-modal systems. Although current Composed Image Retrieval(CIR) methods combine reference images with retrieval texts, they areconstrained to image-level matching and cannot localize specific objects. Tothis end, we propose Composed Object Retrieval (COR), a brand-new task thatgoes beyond image-level retrieval to achieve object-level precision, allowingthe retrieval and segmentation of target objects based on composed expressionscombining reference objects and retrieval texts. COR presents significantchallenges in retrieval flexibility, which requires systems to identifyarbitrary objects satisfying composed expressions while avoiding semanticallysimilar but irrelevant negative objects within the same scene. We constructCOR127K, the first large-scale COR benchmark that contains 127,166 retrievaltriplets with various semantic transformations in 408 categories. We alsopresent CORE, a unified end-to-end model that integrates reference regionencoding, adaptive visual-textual interaction, and region-level contrastivelearning. Extensive experiments demonstrate that CORE significantly outperformsexisting models in both base and novel categories, establishing a simple andeffective baseline for this challenging task while opening new directions forfine-grained multi-modal retrieval research.</description>
      <author>example@mail.com (Tong Wang, Guanyu Yang, Nian Liu, Zongyan Han, Jinxing Zhou, Salman Khan, Fahad Shahbaz Khan)</author>
      <guid isPermaLink="false">2508.04424v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification</title>
      <link>http://arxiv.org/abs/2508.04308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17th International Conference on Computational Collective  Intelligence 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WSS-CL的两阶段高效机器遗忘方法用于图像分类，通过权重显著性关注关键模型参数，在logit空间进行高效遗忘，并在特征空间中通过对比学习最大化遗忘与保留数据的距离，实验证明该方法显著提高了遗忘效果且性能损失可忽略。&lt;h4&gt;背景&lt;/h4&gt;机器遗忘（从已训练模型中高效删除特定数据的影响）仍是一个具有挑战性的问题。当前主要关注数据中心或权重策略的机器学习方法在实现精确遗忘、保持稳定性和确保跨领域适用性方面经常遇到挑战。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的两阶段高效机器遗忘方法用于图像分类，利用权重显著性将遗忘过程集中在关键模型参数上，缩小与'精确'遗忘之间的性能差距。&lt;h4&gt;方法&lt;/h4&gt;提出WSS-CL方法，包含两个阶段：1）遗忘阶段：最大化输出logits与聚合伪标签之间的KL散度，在logit空间实现高效遗忘；2）对抗微调阶段：以自监督方式引入对比学习，通过缩放特征表示最大化遗忘和保留数据样本在特征空间中的距离，其中遗忘样本和配对的增强样本作为正对，保留样本作为负对。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，与最先进的方法相比，所提出的方法在遗忘效果上有显著提高，且性能损失可忽略不计，表明该方法在监督和自监督设置中具有可用性。&lt;h4&gt;结论&lt;/h4&gt;WSS-CL方法通过权重显著性关注关键参数，结合两阶段处理策略，显著提高了机器遗忘的效率，同时保持了模型性能，适用于多种学习场景。&lt;h4&gt;翻译&lt;/h4&gt;机器遗忘，即从已训练模型中高效删除特定数据的影响，仍然是一个具有挑战性的问题。当前主要关注数据中心或权重策略的机器学习方法在实现精确遗忘、保持稳定性和确保跨领域适用性方面经常遇到挑战。在这项工作中，我们引入了一种新的用于图像分类的两阶段高效机器遗忘方法，基于权重显著性，利用权重显著性将遗忘过程集中在关键模型参数上。我们的方法被称为权重显著性软引导对比学习用于高效机器遗忘图像分类（WSS-CL），显著缩小了与'精确'遗忘之间的性能差距。首先，遗忘阶段最大化输出logits与聚合伪标签之间的KL散度，在logit空间中实现高效遗忘。接下来，对抗微调阶段以自监督方式引入对比学习。通过使用缩放的特征表示，它最大化了遗忘和保留数据样本在特征空间中的距离，其中遗忘样本和配对的增强样本作为正对，而保留样本在对比损失计算中作为负对。实验评估表明，与最先进的方法相比，我们提出的方法在遗忘效果上有显著提高，且性能损失可忽略不计，表明其在监督和自监督设置中的可用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine unlearning, the efficient deletion of the impact of specific data ina trained model, remains a challenging problem. Current machine unlearningapproaches that focus primarily on data-centric or weight-based strategiesfrequently encounter challenges in achieving precise unlearning, maintainingstability, and ensuring applicability across diverse domains. In this work, weintroduce a new two-phase efficient machine unlearning method for imageclassification, in terms of weight saliency, leveraging weight saliency tofocus the unlearning process on critical model parameters. Our method is calledweight saliency soft-guided contrastive learning for efficient machineunlearning image classification (WSS-CL), which significantly narrows theperformance gap with "exact" unlearning. First, the forgetting stage maximizeskullback-leibler divergence between output logits and aggregated pseudo-labelsfor efficient forgetting in logit space. Next, the adversarial fine-tuningstage introduces contrastive learning in a self-supervised manner. By usingscaled feature representations, it maximizes the distance between the forgottenand retained data samples in the feature space, with the forgotten and thepaired augmented samples acting as positive pairs, while the retained samplesact as negative pairs in the contrastive loss computation. Experimentalevaluations reveal that our proposed method yields much-improved unlearningefficacy with negligible performance loss compared to state-of-the-artapproaches, indicative of its usability in supervised and self-supervisedsettings.</description>
      <author>example@mail.com (Thang Duc Tran, Thai Hoang Le)</author>
      <guid isPermaLink="false">2508.04308v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval</title>
      <link>http://arxiv.org/abs/2508.04162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SSEmb，一种能够捕捉数学公式结构和语义特征的新型嵌入框架，在ARQMath-3公式检索任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;公式检索是数学信息检索领域的一个重要话题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉数学公式结构和语义特征的新型嵌入框架，以提高公式检索性能。&lt;h4&gt;方法&lt;/h4&gt;1) 结构上：使用图对比学习编码表示为操作符图的公式，并通过替换策略引入图数据增强方法；2) 语义上：使用Sentence-BERT编码公式周围的文本；3) 对每个查询和候选公式，分别计算结构相似性和语义相似性，并通过加权方案进行融合。&lt;h4&gt;主要发现&lt;/h4&gt;在ARQMath-3公式检索任务中，SSEmb在P'@10和nDCG'@10指标上比现有的基于嵌入的方法高出5个百分点以上；SSEmb提高了其他方法所有运行的性能，并与Approach0结合时达到了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;SSEmb是一种有效的数学公式嵌入框架，能够同时捕捉公式结构和语义特征，显著提高了公式检索性能。&lt;h4&gt;翻译&lt;/h4&gt;公式检索是数学信息检索领域的一个重要话题。我们提出了SSEmb，一种能够捕捉数学公式结构和语义特征的新型嵌入框架。结构上，我们使用图对比学习来编码表示为操作符图的公式。为了增强结构多样性同时保持这些公式图的有效性，我们引入了一种通过替换策略实现的新的图数据增强方法。语义上，我们使用Sentence-BERT来编码公式周围的文本。最后，对于每个查询及其候选公式，分别计算结构相似性和语义相似性，然后通过加权方案进行融合。在ARQMath-3公式检索任务中，SSEmb在P'@10和nDCG'@10指标上比现有的基于嵌入的方法高出5个百分点以上。此外，SSEmb提高了其他方法所有运行的性能，并与Approach0结合时达到了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Formula retrieval is an important topic in Mathematical InformationRetrieval. We propose SSEmb, a novel embedding framework capable of capturingboth structural and semantic features of mathematical formulas. Structurally,we employ Graph Contrastive Learning to encode formulas represented as OperatorGraphs. To enhance structural diversity while preserving mathematical validityof these formula graphs, we introduce a novel graph data augmentation approachthrough a substitution strategy. Semantically, we utilize Sentence-BERT toencode the surrounding text of formulas. Finally, for each query and itscandidates, structural and semantic similarities are calculated separately andthen fused through a weighted scheme. In the ARQMath-3 formula retrieval task,SSEmb outperforms existing embedding-based methods by over 5 percentage pointson P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runsof other methods and achieves state-of-the-art results when combined withApproach0.</description>
      <author>example@mail.com (Ruyin Li, Xiaoyu Chen)</author>
      <guid isPermaLink="false">2508.04162v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Search and Recommendation through Latent Cross Reasoning</title>
      <link>http://arxiv.org/abs/2508.04152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种潜在的交叉推理框架，通过识别和利用有用的搜索行为信号来改进推荐系统性能。&lt;h4&gt;背景&lt;/h4&gt;搜索和推荐是现代在线平台的基础组成部分，但有效利用搜索行为改进推荐具有挑战性。用户搜索历史常包含嘈杂或不相关信号，现有方法通常联合或单独编码搜索和推荐历史，未明确识别真正有用的搜索行为。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架来识别真正有用的搜索行为信号，利用这些信号改善推荐性能。&lt;h4&gt;方法&lt;/h4&gt;设计了一个潜在的交叉推理框架，首先编码用户的搜索和推荐历史以捕获全局兴趣，然后迭代推理搜索行为提取有益信号。使用对比学习使潜在推理状态与目标项保持一致，并引入强化学习直接优化排序性能。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试上，该方法与强基线相比显示出一致的改进，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;推理在增强感知推荐的搜索中起着重要作用，所提出的框架能有效识别和利用有用的搜索信号。&lt;h4&gt;翻译&lt;/h4&gt;搜索和推荐(S&amp;R)是现代在线平台的基本组成部分，然而有效利用搜索行为来改进推荐仍然是一个具有挑战性的问题。用户搜索历史通常包含嘈杂或不相关的信号，这些信号甚至可能降低推荐性能，而现有方法通常联合或单独编码S&amp;R历史，而没有明确识别哪些搜索行为真正有用。受人类决策过程的启发，人们首先识别推荐意图，然后推理相关证据，我们设计了一个潜在的交叉推理框架，首先编码用户的S&amp;R历史以捕获全局兴趣，然后迭代地推理搜索行为以提取对推荐有益的信号。采用对比学习使潜在推理状态与目标项保持一致，并进一步引入强化学习直接优化排序性能。在公共基准上的大量实验证明了与强基线相比的一致性改进，验证了推理在增强感知推荐的搜索中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Search and recommendation (S&amp;R) are fundamental components of modern onlineplatforms, yet effectively leveraging search behaviors to improverecommendation remains a challenging problem. User search histories oftencontain noisy or irrelevant signals that can even degrade recommendationperformance, while existing approaches typically encode S&amp;R histories eitherjointly or separately without explicitly identifying which search behaviors aretruly useful. Inspired by the human decision-making process, where one firstidentifies recommendation intent and then reasons about relevant evidence, wedesign a latent cross reasoning framework that first encodes user S&amp;R historiesto capture global interests and then iteratively reasons over search behaviorsto extract signals beneficial for recommendation. Contrastive learning isemployed to align latent reasoning states with target items, and reinforcementlearning is further introduced to directly optimize ranking performance.Extensive experiments on public benchmarks demonstrate consistent improvementsover strong baselines, validating the importance of reasoning in enhancingsearch-aware recommendation.</description>
      <author>example@mail.com (Teng Shi, Weicong Qin, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu)</author>
      <guid isPermaLink="false">2508.04152v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Contrastive Learning for Federated Learning</title>
      <link>http://arxiv.org/abs/2508.04005v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DCFL的新框架，解决了联邦学习中对比学习因有限样本违反渐近假设的问题，通过解耦对比损失实现了在数据量有限情况下的有效学习。&lt;h4&gt;背景&lt;/h4&gt;联邦学习是一种分布式机器学习范式，允许多个参与者通过交换模型更新而非原始数据来训练共享模型。然而，由于客户端间的数据异构性，其性能相比集中式方法会降低。&lt;h4&gt;目的&lt;/h4&gt;解决对比学习在联邦学习环境中的适用性问题，特别是在每个客户端只有少量数据的情况下。&lt;h4&gt;方法&lt;/h4&gt;引入联邦学习解耦对比学习(DCFL)框架，将现有对比损失解耦为对齐和均匀性两个目标，实现独立校准吸引力和排斥力，无需依赖渐近假设。&lt;h4&gt;主要发现&lt;/h4&gt;DCFL实现了正样本间更强的对齐和负样本间更好的均匀性；在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准基准测试上，持续优于现有最先进的联邦学习方法。&lt;h4&gt;结论&lt;/h4&gt;DCFL提供了一种适合联邦学习环境的对比学习方法，特别适用于客户端数据量有限的情况。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习是一种分布式机器学习范式，允许多个参与者通过交换模型更新而非原始数据来训练共享模型。然而，由于客户端间的数据异构性，其性能相比集中式方法会降低。虽然对比学习已成为缓解这一问题的有前景方法，但我们的理论分析揭示了一个根本性冲突：其在联邦学习有限样本情况下的渐近假设（无限数量的负样本）被违反。为解决这个问题，我们引入了联邦学习解耦对比学习(DCFL)，一种将现有对比损失解耦为两个目标的新框架。将损失解耦为对齐和均匀性组件，使得能够独立校准吸引力和排斥力，而不依赖于渐近假设。这种策略提供了一种适合联邦学习环境的对比学习方法，特别是在每个客户端只有少量数据的情况下。我们的实验结果表明，与现有对比学习方法相比，DCFL实现了正样本之间更强的对齐和负样本之间更好的均匀性。此外，在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准基准上的实验结果表明，DCFL持续优于最先进的联邦学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning is a distributed machine learning paradigm that allowsmultiple participants to train a shared model by exchanging model updatesinstead of their raw data. However, its performance is degraded compared tocentralized approaches due to data heterogeneity across clients. Whilecontrastive learning has emerged as a promising approach to mitigate this, ourtheoretical analysis reveals a fundamental conflict: its asymptotic assumptionsof an infinite number of negative samples are violated in finite-sample regimeof federated learning. To address this issue, we introduce DecoupledContrastive Learning for Federated Learning (DCFL), a novel framework thatdecouples the existing contrastive loss into two objectives. Decoupling theloss into its alignment and uniformity components enables the independentcalibration of the attraction and repulsion forces without relying on theasymptotic assumptions. This strategy provides a contrastive learning methodsuitable for federated learning environments where each client has a smallamount of data. Our experimental results show that DCFL achieves strongeralignment between positive samples and greater uniformity between negativesamples compared to existing contrastive learning methods. Furthermore,experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, andTiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-artfederated learning methods.</description>
      <author>example@mail.com (Hyungbin Kim, Incheol Baek, Yon Dohn Chung)</author>
      <guid isPermaLink="false">2508.04005v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework</title>
      <link>http://arxiv.org/abs/2508.03989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了PrivCLIP，一种动态、用户可控的少样本隐私保护感知框架，允许用户通过分类活动为敏感、非敏感或中性来指定隐私偏好，使用多模态对比学习对齐IMU传感器数据与自然语言描述，并通过数据转换实现隐私保护。&lt;h4&gt;背景&lt;/h4&gt;在配备IMU传感器的现代设备中，用户可控隐私很重要，因为这些设备持续收集可能暴露敏感行为的时间序列数据。现有隐私保护方法多依赖静态标签或大量私有训练数据，缺乏适应性和用户自主性。&lt;h4&gt;目的&lt;/h4&gt;开发一种动态、用户可控、少样本的隐私保护感知框架，使用户能够灵活指定和修改隐私偏好，同时保持数据效用。&lt;h4&gt;方法&lt;/h4&gt;PrivCLIP采用多模态对比学习将IMU传感器数据与自然语言活动描述对齐在共享嵌入空间，实现敏感活动少样本检测。检测到敏感活动时，使用语言引导的活动清理器和IMU-GPT运动生成模块将数据转换为符合隐私要求的版本。&lt;h4&gt;主要发现&lt;/h4&gt;在多个人体活动识别数据集上评估表明，PrivCLIP在隐私保护和数据效用方面显著优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;PrivCLIP提供了一种有效的隐私保护解决方案，既尊重用户隐私偏好变化，又保持数据实用性，解决了现有方法在适应性和用户自主性方面的局限。&lt;h4&gt;翻译&lt;/h4&gt;用户可控的隐私在现代感知系统中很重要，因为隐私偏好可能因人而异，并可能随时间演变。这在配备惯性测量单元传感器的设备中尤其相关，这些设备持续收集丰富的时间序列数据，可能会无意中暴露敏感的用户行为。虽然先前的工作已经提出了传感器数据的隐私保护方法，但大多数依赖于静态、预定义的隐私标签或需要大量私有训练数据，限制了它们的适应性和用户自主性。在这项工作中，我们介绍了PrivCLIP，一种动态、用户可控、少样本的隐私保护感知框架。PrivCLIP允许用户通过将活动分类为敏感、非敏感或中性来指定和修改其隐私偏好。利用多模态对比学习方法，PrivCLIP将IMU传感器数据与自然语言活动描述在共享嵌入空间中对齐，实现敏感活动的少样本检测。当检测到隐私敏感活动时，系统使用语言引导的活动清理器和运动生成模块将原始数据转换为语义上类似于非敏感活动的符合隐私要求的版本。我们在多个人体活动识别数据集上评估了PrivCLIP，并证明它在隐私保护和数据效用方面都显著优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User-controllable privacy is important in modern sensing systems, as privacypreferences can vary significantly from person to person and may evolve overtime. This is especially relevant in devices equipped with Inertial MeasurementUnit (IMU) sensors, such as smartphones and wearables, which continuouslycollect rich time-series data that can inadvertently expose sensitive userbehaviors. While prior work has proposed privacy-preserving methods for sensordata, most rely on static, predefined privacy labels or require largequantities of private training data, limiting their adaptability and useragency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,few-shot privacy-preserving sensing framework. PrivCLIP allows users to specifyand modify their privacy preferences by categorizing activities as sensitive(black-listed), non-sensitive (white-listed), or neutral (gray-listed).Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMUsensor data with natural language activity descriptions in a shared embeddingspace, enabling few-shot detection of sensitive activities. When aprivacy-sensitive activity is identified, the system uses a language-guidedactivity sanitizer and a motion generation module (IMU-GPT) to transform theoriginal data into a privacy-compliant version that semantically resembles anon-sensitive activity. We evaluate PrivCLIP on multiple human activityrecognition datasets and demonstrate that it significantly outperforms baselinemethods in terms of both privacy protection and data utility.</description>
      <author>example@mail.com (Ajesh Koyatan Chathoth, Shuhao Yu, Stephen Lee)</author>
      <guid isPermaLink="false">2508.03989v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Marco-Voice Technical Report</title>
      <link>http://arxiv.org/abs/2508.02038v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report. Our code and dataset are publicly available at  https://github.com/AIDC-AI/Marco-Voice and  https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个名为Marco-Voice的多功能语音合成系统，集成了声音克隆和情感控制功能，通过创新的解耦机制和情感嵌入方法实现了高质量、自然的语音生成。&lt;h4&gt;背景&lt;/h4&gt;在语音合成领域，长期以来存在如何生成高度表达性、可控且自然语音的挑战，同时需要在不同语言和情感背景下保持说话人身份的真实性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解决语音合成中长期挑战的系统，实现高度表达性、可控性和自然语音生成，同时保持说话人身份的真实性。&lt;h4&gt;方法&lt;/h4&gt;研究引入了有效的说话人-情感解耦机制，采用批内对比学习实现说话人身份和情感风格的独立操作，并开发了旋转情感嵌入集成方法以实现平滑的情感控制。同时构建了CSEMOTIONS数据集，包含6位专业说话人的10小时普通话语音，涵盖7种情感类别。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Marco-Voice系统在客观和主观指标上都有显著改进，在语音清晰度和情感丰富度方面表现具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;Marco-Voice代表了表达性神经语音合成领域的重大进展，代码和数据集已公开，可供研究人员使用。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了一个多功能语音合成系统，将声音克隆和情感控制语音合成集成在一个统一框架内。这项工作的目标是解决在实现高度表达性、可控性和自然语音生成方面的长期挑战，同时在不同语言和情感背景下真实保留说话人身份。我们的方法引入了有效的说话人-情感解耦机制，采用批内对比学习，能够独立操作说话人身份和情感风格，以及旋转情感嵌入集成方法用于平滑的情感控制。为了支持全面的训练和评估，我们构建了CSEMOTIONS，这是一个高质量的情感语音数据集，包含来自六位专业说话人的10小时普通话语音，涵盖七种情感类别。大量实验表明，我们的系统Marco-Voice在客观和主观指标上都取得了显著改进。进行了全面评估和分析，结果显示Marco-Voice在语音清晰度和情感丰富度方面具有竞争力的性能，代表了表达性神经语音合成领域的重大进展。我们的代码和数据集分别在GitHub和Hugging Face上公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a multifunctional speech synthesis system that integratesvoice cloning and emotion control speech synthesis within a unified framework.The goal of this work is to address longstanding challenges in achieving highlyexpressive, controllable, and natural speech generation that faithfullypreserves speaker identity across diverse linguistic and emotional contexts.Our approach introduces an effective speaker-emotion disentanglement mechanismwith in-batch contrastive learning, enabling independent manipulation ofspeaker identity and eemotional style, as well as rotational emotionalembedding integration method for smooth emotion control. To supportcomprehensive training and evaluation, we construct CSEMOTIONS, a high-qualityemotional speech dataset containing 10 hours of Mandarin speech from sixprofessional speakers across seven emotional categories. Extensive experimentsdemonstrate that our system, Marco-Voice, achieves substantial improvements inboth objective and subjective metrics. Comprehensive evaluations and analysiswere conducted, results show that MarcoVoice delivers competitive performancein terms of speech clarity and emotional richness, representing a substantialadvance in the field of expressive neural speech synthesis. Our code anddataset are publicly available at https://github.com/AIDC-AI/Marco-Voice andhttps://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively.</description>
      <author>example@mail.com (Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang)</author>
      <guid isPermaLink="false">2508.02038v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2508.02762v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  COLM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Context-Adaptive Multi-Prompt Embedding的新方法，用于增强视觉语言对比学习中的语义表示。通过引入多个结构化提示和自适应令牌，结合预训练大语言模型和特殊的损失函数，该方法在图像-文本和视频-文本检索任务中取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;标准CLIP风格模型依赖于单一的文本嵌入，这在捕捉输入文本的多样语义方面存在局限性，需要一种能够丰富语义表示的新方法。&lt;h4&gt;目的&lt;/h4&gt;增强视觉语言对比学习中的语义表示，使文本表示与视觉特征实现更丰富的语义对齐，提高检索任务的性能。&lt;h4&gt;方法&lt;/h4&gt;引入多个结构化提示，每个提示包含不同的自适应令牌以捕捉文本的不同语义方面；使用预训练大语言模型作为文本编码器，在单个前向传播中联合处理所有提示；将提示嵌入组合成统一文本表示；引入多样性正则化损失和否定感知损失以促进语义多样性和表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;Context-Adaptive Multi-Prompt Embedding方法在图像-文本和视频-文本检索基准测试中取得了一致的性能改进。&lt;h4&gt;结论&lt;/h4&gt;通过多提示结构和特殊的损失函数设计，Context-Adaptive Multi-Prompt Embedding能够有效增强视觉语言对比学习中的语义表示能力，提升检索性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了上下文自适应多提示嵌入，一种新颖的用于增强视觉语言对比学习中语义表示的方法。与依赖单一文本嵌入的标准CLIP风格模型不同，我们的方法引入了多个结构化提示，每个提示包含一个不同的自适应令牌，能够捕捉输入文本的多样语义方面。我们在CLIP框架内使用预训练大语言模型作为文本编码器，在单个前向传播中联合处理所有提示。将得到的提示嵌入组合成统一的文本表示，使文本表示与视觉特征实现更丰富的语义对齐。为进一步促进语义多样性和表示质量，我们引入了多样性正则化损失和否定感知损失，鼓励提示之间的专业化并提高对比判别能力。我们的方法在图像-文本和视频-文本检索基准测试中取得了持续的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Context-Adaptive Multi-Prompt Embedding, a novel approach toenrich semantic representations in vision-language contrastive learning. Unlikestandard CLIP-style models that rely on a single text embedding, our methodintroduces multiple structured prompts, each containing a distinct adaptivetoken that captures diverse semantic aspects of the input text. We leverage apretrained LLM as the text encoder within the CLIP framework, processing allprompts jointly in a single forward pass. The resulting prompt embeddings arecombined into a unified text representation, enabling semantically richeralignment with visual features. To further promote semantic diversity andrepresentation quality, we incorporate a diversity regularization loss and anegation-aware loss, encouraging specialization across prompts and improvingcontrastive discrimination. Our method achieves consistent improvements on bothimage-text and video-text retrieval benchmarks.</description>
      <author>example@mail.com (Dahun Kim, Anelia Angelova)</author>
      <guid isPermaLink="false">2508.02762v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Pivoting Manipulation with Force and Vision Feedback Using Optimization-based Demonstrations</title>
      <link>http://arxiv.org/abs/2508.01082v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合基于模型和基于学习方法的框架，用于学习闭环枢轴操作。该方法利用接触隐式轨迹优化和演示引导的深度强化学习，实现样本高效学习，并通过特权训练策略实现仿真到现实迁移，使机器人仅依靠本体感受、视觉和力传感就能执行枢轴操作。&lt;h4&gt;背景&lt;/h4&gt;非抓取操作由于物体、环境和机器人之间复杂的接触交互而具有挑战性。基于模型的方法能在接触约束下高效生成轨迹，但对模型不准确敏感且需要特权信息；而基于学习的方法对建模误差更具鲁棒性，但需要大量数据。&lt;h4&gt;目的&lt;/h4&gt;结合基于模型和基于学习的方法的优势，提出一个用于学习闭环枢轴操作的框架，实现样本高效学习，并开发一种使机器人仅依靠本体感受、视觉和力传感就能执行枢轴操作的仿真到现实迁移方法。&lt;h4&gt;方法&lt;/h4&gt;利用计算高效的接触隐式轨迹优化(CITO)设计演示引导的深度强化学习(RL)，实现样本高效学习；提出使用特权训练策略的仿真到现实迁移方法，使机器人能够仅使用本体感受、视觉和力传感执行枢轴操作。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个枢轴任务上进行了评估，证明能够成功实现仿真到现实的迁移，机器人无需访问特权信息即可执行枢轴操作。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功结合了基于模型和基于学习的方法的优势，解决了非抓取操作的挑战，实现了高效的样本学习和仿真到现实迁移。&lt;h4&gt;翻译&lt;/h4&gt;非抓取操作由于物体、环境和机器人之间复杂的接触交互而具有挑战性。基于模型的方法可以在接触约束下高效生成机器人和物体的复杂轨迹。然而，它们往往对模型不准确敏感，且需要访问特权信息(如物体质量、大小、姿态)，使其不太适合处理新物体。相比之下，基于学习的方法通常对建模误差更具鲁棒性，但需要大量数据。在本文中，我们结合这两种方法，提出了一个用于学习闭环枢轴操作的框架。通过利用计算高效的接触隐式轨迹优化(CITO)，我们设计了演示引导的深度强化学习(RL)，实现了样本高效学习。我们还提出了使用特权训练策略的仿真到现实迁移方法，使机器人能够仅依靠本体感受、视觉和力传感执行枢轴操作，无需访问特权信息。我们的方法在多个枢轴任务上进行了评估，证明能够成功实现仿真到现实迁移。我们方法的概述和硬件实验可在 https://youtu.be/akjGDgfwLbM?si=QVw6ExoPy2VsU2g6 查看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-prehensile manipulation is challenging due to complex contactinteractions between objects, the environment, and robots. Model-basedapproaches can efficiently generate complex trajectories of robots and objectsunder contact constraints. However, they tend to be sensitive to modelinaccuracies and require access to privileged information (e.g., object mass,size, pose), making them less suitable for novel objects. In contrast,learning-based approaches are typically more robust to modeling errors butrequire large amounts of data. In this paper, we bridge these two approaches topropose a framework for learning closed-loop pivoting manipulation. Byleveraging computationally efficient Contact-Implicit Trajectory Optimization(CITO), we design demonstration-guided deep Reinforcement Learning (RL),leading to sample-efficient learning. We also present a sim-to-real transferapproach using a privileged training strategy, enabling the robot to performpivoting manipulation using only proprioception, vision, and force sensingwithout access to privileged information. Our method is evaluated on severalpivoting tasks, demonstrating that it can successfully perform sim-to-realtransfer. The overview of our method and the hardware experiments are shown athttps://youtu.be/akjGDgfwLbM?si=QVw6ExoPy2VsU2g6</description>
      <author>example@mail.com (Yuki Shirai, Kei Ota, Devesh K. Jha, Diego Romeres)</author>
      <guid isPermaLink="false">2508.01082v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning</title>
      <link>http://arxiv.org/abs/2508.04565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为TAlignDiff的自动牙齿对齐新方法，基于扩散变换学习，结合点云回归网络和扩散变换矩阵去噪模块，解决了传统方法无法捕捉变换矩阵分布特征的问题。&lt;h4&gt;背景&lt;/h4&gt;正畸治疗依赖于牙齿对齐，这会影响咬合功能、面部美学和患者的生活质量。当前深度学习方法主要通过点对点几何约束预测变换矩阵，但这些矩阵与人类口腔解剖结构相关的分布特征难以被传统方法捕捉。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的自动牙齿对齐方法，能够更好地捕捉变换矩阵的分布特征，提高正畸治疗的效果。&lt;h4&gt;方法&lt;/h4&gt;TAlignDiff方法包含两个主要组件：基于点云的回归网络(PRN)和基于扩散的变换矩阵去噪模块(DTMD)。几何约束损失监督PRN学习点云级别的对齐，DTMD作为辅助模块从临床数据中学习变换矩阵的潜在分布。两者集成在一个统一框架中，实现几何约束和扩散细化之间的双向反馈。&lt;h4&gt;主要发现&lt;/h4&gt;大量的消融和比较实验证明了TAlignDiff方法的有效性和优越性，该方法在正畸治疗中具有应用潜力。&lt;h4&gt;结论&lt;/h4&gt;基于扩散变换学习的TAlignDiff方法能够更好地捕捉变换矩阵的分布特征，为正畸治疗提供更有效的自动牙齿对齐解决方案。&lt;h4&gt;翻译&lt;/h4&gt;正畸治疗依赖于牙齿对齐，这显著影响咬合功能、面部美学和患者的生活质量。当前的深度学习方法主要通过对施加点对点几何约束来预测变换矩阵进行牙齿对齐。然而，这些矩阵可能与人类口腔的解剖结构有关，并具有特定的分布特征，而先前工作中的确定性点对点几何约束无法捕捉这些特征。为此，我们引入了一种名为TAlignDiff的新自动牙齿对齐方法，该方法基于扩散变换学习支持。TAlignDiff包含两个主要组件：一个主要的基于点云的回归网络(PRN)和一个基于扩散的变换矩阵去噪模块(DTMD)。几何约束损失监督PRN学习点云级别的对齐。DTMD作为辅助模块，从临床数据中学习变换矩阵的潜在分布。我们将基于点云的变换回归和基于扩散的变换建模集成到一个统一框架中，允许几何约束和扩散细化之间的双向反馈。大量的消融和比较实验证明了我们方法的有效性和优越性，突显了其在正畸治疗中的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动牙齿排列问题，即如何通过深度学习预测牙齿的变换矩阵实现牙齿自动对齐。这个问题在现实中非常重要，因为牙齿排列是正畸治疗的核心，直接影响咬合功能、面部美观和患者生活质量；传统正畸规划依赖医生经验，耗时且主观；自动排列可提供清晰的治疗结果可视化，帮助医生制定准确计划，提高正畸护理效率和精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析当前深度学习方法通过点对点几何约束预测变换矩阵的局限性，指出这些方法无法捕捉变换矩阵与口腔解剖结构相关的特定分布特征。作者借鉴了Lei等人提出的TADPM扩散模型框架，但发现其直接从高维几何特征回归变换矩阵导致计算复杂且需要大数据集。因此，作者设计了TAlignDiff，先用点云回归网络预测初始变换矩阵，再用轻量级扩散模型建模变换矩阵的潜在分布，通过比较预测和真实矩阵的噪声估计来优化结果，降低了输入维度并提高对小数据集的适应性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合显式几何约束和隐式分布建模，通过点云回归网络(PRN)和基于扩散的变换矩阵去噪模块(DTMD)双向协同工作。整体流程为：1)输入未对齐的3D牙齿点云；2)PRN使用PointNet编码器提取全局和局部特征，回归预测变换矩阵；3)应用几何约束损失(点重建损失和质心偏移损失)确保几何一致性；4)DTMD通过扩散过程学习变换矩阵分布，对比去噪损失优化预测；5)联合优化所有损失函数，分阶段训练模型；6)输出对齐后的牙齿点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的框架设计，整合几何约束变换回归与扩散辅助分布学习；2)双向反馈机制，通过比较预测和真实矩阵的噪声估计迭代优化；3)轻量级扩散模型，专注于变换矩阵而非原始几何数据。相比之前工作，TAlignDiff不同于TADPM直接从高维特征回归变换矩阵，而是先预测后细化，降低计算复杂度；区别于传统几何约束方法仅关注点云重建，忽略了变换矩阵的分布特征；优于其他深度学习方法仅关注几何特征提取，同时考虑几何一致性和分布建模。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TAlignDiff通过结合几何约束的变换回归与扩散辅助的分布学习，提出了一种更准确、更可靠的自动牙齿排列方法，显著提高了正畸治疗规划的效率和精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Orthodontic treatment hinges on tooth alignment, which significantly affectsocclusal function, facial aesthetics, and patients' quality of life. Currentdeep learning approaches predominantly concentrate on predicting transformationmatrices through imposing point-to-point geometric constraints for toothalignment. Nevertheless, these matrices are likely associated with theanatomical structure of the human oral cavity and possess particulardistribution characteristics that the deterministic point-to-point geometricconstraints in prior work fail to capture. To address this, we introduce a newautomatic tooth alignment method named TAlignDiff, which is supported bydiffusion-based transformation learning. TAlignDiff comprises two maincomponents: a primary point cloud-based regression network (PRN) and adiffusion-based transformation matrix denoising module (DTMD).Geometry-constrained losses supervise PRN learning for point cloud-levelalignment. DTMD, as an auxiliary module, learns the latent distribution oftransformation matrices from clinical data. We integrate point cloud-basedtransformation regression and diffusion-based transformation modeling into aunified framework, allowing bidirectional feedback between geometricconstraints and diffusion refinement. Extensive ablation and comparativeexperiments demonstrate the effectiveness and superiority of our method,highlighting its potential in orthodontic treatment.</description>
      <author>example@mail.com (Yunbi Liu, Enqi Tang, Shiyu Li, Lei Ma, Juncheng Li, Shu Lou, Yongchu Pan, Qingshan Liu)</author>
      <guid isPermaLink="false">2508.04565v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space</title>
      <link>http://arxiv.org/abs/2508.04286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 15 figures, and will be published in IEEE TVCG&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PKSS-Align的鲁棒点云配准方法，能够在Pre-Kendall形状空间上测量点云间的相似性，无需点到点或点到平面的度量。该方法能够处理相似变换、非均匀密度、随机噪声点和缺陷部分等多种影响因素，通过简单的并行加速实现高效配准，实验证明其性能优于现有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;点云配准是3D视觉和计算机图形学领域的一个经典话题。现有的点云配准方法通常对相似变换（平移、缩放和旋转）、噪声点和不完整的几何结构敏感。特别是，点云的非均匀尺度和缺陷部分增加了配准任务陷入局部最优的概率。&lt;h4&gt;目的&lt;/h4&gt;提出一种鲁棒点云配准方法PKSS-Align，能够处理各种影响因素，包括相似变换、非均匀密度、随机噪声点和缺陷部分。&lt;h4&gt;方法&lt;/h4&gt;在Pre-Kendall形状空间（PKSS）上测量基于形状特征的点云相似性，这是一种基于形状测量的方案，不需要点到点或点到平面的度量。所采用的测量可以被视为流形度量，对欧几里得坐标系中的各种表示具有鲁棒性。得益于这种测量，可以同时为具有上述影响的点云直接生成变换矩阵。该方法不需要数据训练和复杂的特征编码。&lt;h4&gt;主要发现&lt;/h4&gt;通过简单的并行加速，该方法在实际应用中实现了效率和可行性的显著提高。实验证明，该方法优于相关的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;PKSS-Align是一种鲁棒、高效且实用的点云配准方法，能够处理各种复杂情况下的点云配准问题。&lt;h4&gt;翻译&lt;/h4&gt;点云配准是3D视觉和计算机图形学领域的一个经典话题。通常，配准的实现通常对相似变换（平移、缩放和旋转）、噪声点和不完整的几何结构敏感。特别是，点云的非均匀尺度和缺陷部分增加了配准任务陷入局部最优的概率。在本文中，我们提出了一种鲁棒点云配准方法PKSS-Align，可以处理各种影响，包括相似变换、非均匀密度、随机噪声点和缺陷部分。提出的方法在Pre-Kendall形状空间（PKSS）上测量基于形状特征的点云相似性，这是一种基于形状测量的方案，不需要点到点或点到平面的度量。所采用的测量可以被视为流形度量，对欧几里得坐标系中的各种表示具有鲁棒性。得益于这种测量，可以同时为具有上述影响的点云直接生成变换矩阵。提出的方法不需要数据训练和复杂的特征编码。基于简单的并行加速，它可以在实践中实现效率和可行性的显著提高。实验表明，我们的方法优于相关的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云配准中的鲁棒性问题，包括相似变换（平移、缩放、旋转）、非均匀密度、随机噪声点和缺陷部分的影响。这个问题在现实中非常重要，因为点云配准是3D视觉和计算机图形学的核心课题，广泛应用于同步定位与地图构建(SLAM)、自动驾驶系统、3D重建等领域。在实际扫描过程中，这些因素会导致传统配准方法容易陷入局部最优解，特别是在处理具有非均匀尺度和缺陷部分的点云时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云配准技术的局限性：基于距离的方法（如ICP）对初始姿态敏感；基于几何特征的方法（如NDT）性能受特征质量影响且效率低；基于深度编码的方法（如PointNetLK）受限于训练数据。作者借鉴了之前工作KSS-ICP中的Kendall形状空间理论，但指出了其两个局限性：计算效率低和对不同尺度、缺陷部分的点云配准受限。为解决这些问题，作者设计了PKSS-Align方法，在Pre-Kendall形状空间上实现，包括PKSS映射、形状测量和全局搜索三个核心组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在Pre-Kendall形状空间上测量点云间的形状相似性，不依赖点对点或点对平面度量，而是使用流形度量来处理各种变换和噪声影响。整体流程分为三步：首先通过PKSS映射减少平移和尺度影响，包括重采样和异常值剔除；然后使用PKSS形状测量描述点云对齐状态，基于分区结构和代表样本考虑外部轮廓和内部几何特征；最后在SO(3)空间中进行并行加速的全局搜索，找到最优变换矩阵，同时处理不同尺度和缺陷部分。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：PKSS-based映射减少平移和尺度影响；PKSS-based形状测量提供鲁棒流形度量，克服了Hausdorff度量的局限；全局搜索方案在SO(3)中并行加速，避免局部最优。相比之前工作（尤其是KSS-ICP），PKSS-Align计算效率更高，能同时处理不同尺度、非均匀密度、噪声点和缺陷部分，不需要复杂特征训练，且形状测量更准确鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PKSS-Align通过在Pre-Kendall形状空间上进行鲁棒形状测量和全局搜索，实现了对受相似变换、非均匀密度、噪声点和缺陷影响的点云的高效精确配准，无需复杂特征训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a classical topic in the field of 3D Vision andComputer Graphics. Generally, the implementation of registration is typicallysensitive to similarity transformations (translation, scaling, and rotation),noisy points, and incomplete geometric structures. Especially, the non-uniformscales and defective parts of point clouds increase probability of struck localoptima in registration task. In this paper, we propose a robust point cloudregistration PKSS-Align that can handle various influences, includingsimilarity transformations, non-uniform densities, random noisy points, anddefective parts. The proposed method measures shape feature-based similaritybetween point clouds on the Pre-Kendall shape space (PKSS),\textcolor{black}{which is a shape measurement-based scheme and doesn't requirepoint-to-point or point-to-plane metric.} The employed measurement can beregarded as the manifold metric that is robust to various representations inthe Euclidean coordinate system. Benefited from the measurement, thetransformation matrix can be directly generated for point clouds with mentionedinfluences at the same time. The proposed method does not require data trainingand complex feature encoding. Based on a simple parallel acceleration, it canachieve significant improvement for efficiency and feasibility in practice.Experiments demonstrate that our method outperforms the relevantstate-of-the-art methods.</description>
      <author>example@mail.com (Chenlei Lv, Hui Huang)</author>
      <guid isPermaLink="false">2508.04286v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2508.04236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PIS3R的图像拼接解决方案，基于深度3D重建概念，能够有效处理具有大视差的图像。该方法通过视觉几何基础Transformer获取相机参数和3D场景重建，然后将点云投影到参考视图实现像素对齐，最后使用点条件图像扩散模块处理伪影，生成高质量的拼接结果。&lt;h4&gt;背景&lt;/h4&gt;图像拼接旨在将两个从不同视点拍摄的图像对齐为一个无缝的、更宽的图像。然而，当3D场景包含深度变化且相机基线较大时，会出现明显的视差现象，即场景元素在不同视图中的相对位置有很大差异。大多数现有的拼接方法难以有效处理具有大视差的图像。&lt;h4&gt;目的&lt;/h4&gt;解决具有大视差的图像拼接问题，提出一种对大视差鲁棒的图像拼接解决方案，保留3D摄影测量背景下所有像素的几何完整性，使其可直接应用于下游3D视觉任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为PIS3R的图像拼接解决方案，基于新颖的深度3D重建概念。首先，使用视觉几何基础Transformer处理两个具有大视差的输入图像，获取内参、外参以及密集的3D场景重建。然后，使用恢复的相机参数将重建的密集点云重新投影到指定的参考视图上，实现逐像素对齐并生成初始拼接图像。最后，提出一个点条件图像扩散模块来处理初始拼接中可能出现的孔洞或噪声等伪影，以获得精细化的结果。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，该解决方案对大视差具有很好的容忍度，能够保留3D摄影测量背景下所有像素的几何完整性，可直接应用于下游3D视觉任务（如SfM）。实验结果表明，该算法对具有大视差的图像提供了准确的拼接结果，并且在定性和定量上都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;PIS3R是一种有效的图像拼接方法，能够处理具有大视差的图像。该方法通过深度3D重建实现了精确的像素对齐，并能够处理拼接过程中的伪影问题。实验验证了该方法的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;图像拼接旨在将两个从不同视点拍摄的图像对齐为一个无缝的、更宽的图像。然而，当3D场景包含深度变化且相机基线显著时，会出现明显的视差现象，即场景元素在不同视图中的相对位置有很大差异。大多数现有的拼接方法难以有效处理具有大视差的图像。为了应对这一挑战，在本文中，我们提出了一种名为PIS3R的图像拼接解决方案，基于新颖的深度3D重建概念，对大视差具有鲁棒性。首先，我们将视觉几何基础Transformer应用于两个具有大视差的输入图像，以获取内参和外参参数，以及密集的3D场景重建。随后，我们使用恢复的相机参数将重建的密集点云重新投影到指定的参考视图上，实现逐像素对齐并生成初始拼接图像。最后，为了进一步处理初始拼接中可能出现的孔洞或噪声等伪影，我们提出一个点条件图像扩散模块以获得精细化的结果。与现有方法相比，我们的解决方案对大视差具有很好的容忍度，并提供的结果完全保留了3D摄影测量背景下所有像素的几何完整性，使其可直接应用于下游3D视觉任务，如SfM。实验结果表明，所提出的算法对具有大视差的图像提供了准确的拼接结果，并且在定性和定量上都优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是图像拼接中处理极大视差（very large parallax）的问题。当图像场景有深度变化且相机基线较大时，传统拼接方法难以有效处理，导致场景元素在不同视图中的相对位置差异很大。这个问题在现实中非常重要，因为图像拼接技术在自动驾驶、医学成像、监控和虚拟现实等多个领域都有广泛应用，而且现有方法大多只关注视觉上的无缝融合，忽略了拼接结果是否保持了底层3D投影几何，这对于支持下游3D视觉任务（如SfM、SLAM）至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有图像拼接方法的局限性来设计新方法。他们发现传统自适应变形方法在极大视差条件下表现不佳，视频拼接方法不适用于静态图像，现有方法忽略了3D几何一致性。作者借鉴了现有的深度3D重建技术，特别是VGGT（Visual Geometry Grounded Transformer）模型，该模型能同时估计相机参数和密集3D场景重建。基于此，他们设计了一个三阶段流程：深度3D重建、点云重投影和基于扩散的拼接优化，从而解决了极大视差图像拼接问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; PIS3R方法的核心思想是基于深度3D重建而非传统特征匹配来实现图像拼接。通过深度3D重建获取场景精确几何表示，然后通过重投影将不同视角图像对齐到同一视图中。整体流程包括三步：1)使用VGGT模型进行深度3D重建，获取点云和相机参数；2)将重建点云投影到参考视图，实现像素级对齐生成初始拼接图像；3)使用点条件图像扩散模块优化结果，处理空洞和噪声等伪影，生成最终精细拼接图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于深度3D重建的新图像拼接框架；2)明确保留3D几何一致性，支持下游3D视觉任务；3)首次将深度3D重建技术应用于图像拼接；4)开发点条件图像扩散模块优化重投影结果。相比之前工作，PIS3R不依赖传统特征匹配和单应性变换，不使用自适应变形或接缝切割，不仅关注视觉质量，还强调保持3D几何一致性，能有效处理极大视差条件下的图像拼接，而现有方法在此条件下表现不佳。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PIS3R通过基于深度3D重建的创新方法，解决了图像拼接中处理极大视差的挑战，同时保持了3D几何一致性，使拼接结果不仅视觉质量高，还能直接支持下游3D视觉任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image stitching aim to align two images taken from different viewpoints intoone seamless, wider image. However, when the 3D scene contains depth variationsand the camera baseline is significant, noticeable parallax occurs-meaning therelative positions of scene elements differ substantially between views. Mostexisting stitching methods struggle to handle such images with large parallaxeffectively. To address this challenge, in this paper, we propose an imagestitching solution called PIS3R that is robust to very large parallax based onthe novel concept of deep 3D reconstruction. First, we apply visual geometrygrounded transformer to two input images with very large parallax to obtainboth intrinsic and extrinsic parameters, as well as the dense 3D scenereconstruction. Subsequently, we reproject reconstructed dense point cloud ontoa designated reference view using the recovered camera parameters, achievingpixel-wise alignment and generating an initial stitched image. Finally, tofurther address potential artifacts such as holes or noise in the initialstitching, we propose a point-conditioned image diffusion module to obtain therefined result.Compared with existing methods, our solution is very largeparallax tolerant and also provides results that fully preserve the geometricintegrity of all pixels in the 3D photogrammetric context, enabling directapplicability to downstream 3D vision tasks such as SfM. Experimental resultsdemonstrate that the proposed algorithm provides accurate stitching results forimages with very large parallax, and outperforms the existing methodsqualitatively and quantitatively.</description>
      <author>example@mail.com (Muhua Zhu, Xinhao Jin, Chengbo Wang, Yongcong Zhang, Yifei Xue, Tie Ji, Yizhen Lao)</author>
      <guid isPermaLink="false">2508.04236v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation</title>
      <link>http://arxiv.org/abs/2508.04033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE/RSJ International Conference on Intelligent Robots  and Systems (IROS), 2025. 8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合单目摄像头图像和2D雷达点云数据的NLoS行人定位框架，用于解决城市环境中路边停车造成的盲点问题，提高对突然出现行人的检测能力。&lt;h4&gt;背景&lt;/h4&gt;城市环境中路边停车造成的NLoS盲点对道路安全构成重大挑战；现有方法主要依赖预定义空间信息或简单假设，限制了泛化能力；停放的车辆是动态的，卫星地图等预定义信息无法准确反映实时路况。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖预定义空间信息的NLoS行人定位框架，提高在动态城市环境中对突然出现行人的检测能力。&lt;h4&gt;方法&lt;/h4&gt;通过图像分割检测停放车辆，估计深度推断空间特征，并利用2D雷达点云数据精炼信息实现精确空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;在真实城市道路环境中的实验评估表明，该方法增强了早期行人检测，有助于提高道路安全性。&lt;h4&gt;结论&lt;/h4&gt;结合视觉和雷达数据的方法能够更准确地处理由路边停车造成的NLoS盲点问题，特别是在行人突然出现的场景中。&lt;h4&gt;翻译&lt;/h4&gt;由于路边停车导致的城市环境中存在非视距盲点，特别是行人突然出现的情况，对道路安全构成了重大挑战。毫米波技术利用衍射和反射来观察NLoS区域，最近的研究已证明其检测被遮挡物体的潜力。然而，现有方法主要依赖预定义的空间信息或假设简单的墙壁反射，从而限制了它们的泛化能力和实际适用性。当行人在停放的车辆之间突然出现时，这些停放的车辆充当临时空间障碍物，形成特殊挑战。此外，由于停放的车辆是动态的，可能会随时间重新定位，从卫星地图或其他预定义来源获得的空间信息可能无法准确反映实时道路状况，导致传感器解释错误。为解决这一限制，我们提出了一种结合单目摄像头图像和2D雷达点云数据的NLoS行人定位框架。所提出的方法首先通过图像分割检测停放的车辆，估计深度以推断近似空间特征，然后使用2D雷达点云数据进一步精炼信息，以实现精确的空间推理。在真实城市道路环境中进行的实验评估表明，所提出的方法增强了早期行人检测，有助于提高道路安全。补充材料可在https://hiyeun.github.io/NLoS/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决城市环境中路边停放车辆造成的非视线(NLoS)盲区问题，特别是行人突然从这些停放的车辆之间出现时的检测困难。这个问题在现实中非常重要，因为这些盲区可能导致驾驶员无法及时发现行人，尤其是儿童突然冲出马路的情况，反应时间不足可能导致严重事故。现有方法主要依赖预定义的空间信息或假设简单的墙壁反射，限制了它们在真实世界场景中的适用性，且停放车辆是动态的，预定义的空间信息可能无法准确反映实时路况。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性，包括依赖预定义空间信息和假设简单反射表面。然后寻找互补传感器：毫米波雷达可以观察NLoS区域但数据稀疏，相机图像包含丰富环境特征但距离估计受光照影响。作者设计融合方法，利用单目相机进行车辆分割和深度估计，再使用雷达点云精炼位置信息。该方法借鉴了现有工作，如使用Depth Anything V2进行深度估计，YOLOv8进行图像分割，DBSCAN进行点云聚类，以及射线追踪技术分析反射路径。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是融合相机图像和2D雷达点云数据来推断空间信息，并利用这些信息分析雷达反射路径，实现NLoS行人的早期检测。整体流程分为四步：1)车辆推断：从相机图像检测车辆并估计深度，再用雷达数据精炼位置；2)空间信息推断：结合两种数据推断车辆空间状态，使用帧平均稳定信息；3)目标定位：通过射线追踪估计反射路径定位NLoS目标；4)最终位置确定：应用DBSCAN去除噪声，过滤幽灵反射，计算聚类质心作为预测位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的基于图像的雷达点云解释管道，用于定位停车辆间突然出现的NLoS行人；2)使用从距离测量不精确的图像中提取深度信息来解释稀疏雷达点云的空间推断方法；3)在真实世界户外道路环境中进行实际验证。相比之前工作，本文不依赖预定义空间信息而是实时推断，处理更复杂的场景(多车辆之间的区域)，结合相机和雷达的多模态优势，并在实际道路环境中而非受控环境中验证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种融合单目相机和2D雷达点云数据的新方法，能够在真实道路环境中实时推断停车辆之间的空间信息，并通过分析雷达反射路径实现非视线行人的早期检测，从而提高道路安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadsideparking in urban environments poses a significant challenge to road safety,particularly due to the sudden emergence of pedestrians. mmWave technologyleverages diffraction and reflection to observe NLoS regions, and recentstudies have demonstrated its potential for detecting obscured objects.However, existing approaches predominantly rely on predefined spatialinformation or assume simple wall reflections, thereby limiting theirgeneralizability and practical applicability. A particular challenge arises inscenarios where pedestrians suddenly appear from between parked vehicles, asthese parked vehicles act as temporary spatial obstructions. Furthermore, sinceparked vehicles are dynamic and may relocate over time, spatial informationobtained from satellite maps or other predefined sources may not accuratelyreflect real-time road conditions, leading to erroneous sensor interpretations.To address this limitation, we propose an NLoS pedestrian localizationframework that integrates monocular camera image with 2D radar point cloud(PCD) data. The proposed method initially detects parked vehicles through imagesegmentation, estimates depth to infer approximate spatial characteristics, andsubsequently refines this information using 2D radar PCD to achieve precisespatial inference. Experimental evaluations conducted in real-world urban roadenvironments demonstrate that the proposed approach enhances early pedestriandetection and contributes to improved road safety. Supplementary materials areavailable at https://hiyeun.github.io/NLoS/.</description>
      <author>example@mail.com (Hee-Yeun Kim, Byeonggyu Park, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seung-Woo Seo, Seong-Woo Kim)</author>
      <guid isPermaLink="false">2508.04033v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.03925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新型的扩散模型，用于生成具有点对应关系的基于点的形状表示，特别是在医学影像中海马体形状的生成方面取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;传统统计形状模型广泛考虑了点对应关系，但当前深度学习方法忽略了这一点，专注于无序点云。现有深度生成模型无法生成具有点对应关系的形状。&lt;h4&gt;目的&lt;/h4&gt;制定一种能够生成真实基于点形状表示的扩散模型，同时保留训练数据中存在的点对应关系。&lt;h4&gt;方法&lt;/h4&gt;使用基于点的形状表示数据，这些数据具有从开放获取影像研究系列3(OASIS-3)导出的对应关系，开发了一种对应关系保留的扩散模型。&lt;h4&gt;主要发现&lt;/h4&gt;该模型能够生成高度真实的基于点的海马体形状表示，优于现有方法。此外，该模型可用于条件生成健康和阿尔茨海默病受试者的形状，以及通过反事实生成预测疾病进展的形态变化。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的对应关系保留扩散模型在医学形状生成领域具有应用潜力，特别是在神经退行性疾病研究中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种扩散模型，旨在生成具有对应关系的基于点的形状表示。传统统计形状模型已经广泛考虑了点对应关系，但当前的深度学习方法没有考虑这一点，而是专注于无序点云。当前用于点云的深度生成模型并不能解决生成具有点对应关系的形状的问题。这项工作旨在制定一个扩散模型，能够生成真实的基于点的形状表示，并保留训练数据中存在的点对应关系。使用从开放获取影像研究系列3(OASIS-3)导出的具有对应关系的形状表示数据，我们证明我们的对应关系保留模型相比现有方法能够生成高度真实的基于点的海马体形状表示。我们进一步展示了我们的生成模型在下游任务中的应用，如健康和AD受试者的条件生成，以及通过反事实生成预测疾病进展的形态变化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何生成具有点对应关系的基于点的形状表示问题。在生物医学研究中，特别是神经科学领域，解剖形状分析对于理解发育、衰老和疾病进展非常重要。例如，阿尔茨海默病会导致海马体萎缩，而基于点的形状表示能精确量化健康人与患者间的局部形态变化，这对疾病诊断和研究至关重要。点对应关系能确保不同形状间的相同解剖位置可比，这是当前深度生成方法无法做到的。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从传统统计形状模型获得灵感，这些模型已考虑点对应关系，但被当前深度学习方法忽略。他们借鉴了PointNet架构，使用共享线性权重而非卷积层，但保留了点的顺序以维持对应关系。还参考了transformer中的位置嵌入概念，设计了对应关系嵌入来编码点的解剖位置信息。此外，他们采用了扩散模型这一强大的生成框架，并引入掩码自注意力机制来建模点间的空间关系，综合这些创新设计出了新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个能保持点对应关系的扩散模型，生成具有解剖意义的形状表示。整体流程分两部分：前向过程逐渐向原始点云添加噪声，使其逐渐变模糊；生成过程则反向操作，学习从噪声中恢复出有意义的形状。具体实现上，使用类似U-Net的架构，通过共享线性权重的RFT块处理点数据，在中间层注入对应关系嵌入确保点的解剖位置一致，并在瓶颈处用掩码自注意力建模点间空间关系。训练时，网络学习预测添加的噪声，通过最小化预测噪声与实际噪声的差异来优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将点对应关系引入深度生成模型，专门针对生物医学形状设计；2)提出对应关系嵌入机制，确保生成形状保持解剖对应关系；3)设计特殊网络架构，使用共享线性权重和掩码自注意力；4)在生物医学形态分析中成功应用扩散模型。相比之前工作，不同在于：传统点云生成方法(Luo等人,2021; Zeng等人,2022)将点视为独立采样，忽略了对应关系；PCA方法虽能生成形状但缺乏多样性和细节；本文是首个专门设计用于保持点对应关系的生成模型，对生物医学形态分析尤为重要。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的对应关系保留扩散模型，首次实现了深度学习生成具有解剖对应关系的基于点的形状表示，为生物医学形态分析提供了强大的新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a diffusion model designed to generate point-based shaperepresentations with correspondences. Traditional statistical shape models haveconsidered point correspondences extensively, but current deep learning methodsdo not take them into account, focusing on unordered point clouds instead.Current deep generative models for point clouds do not address generatingshapes with point correspondences between generated shapes. This work aims toformulate a diffusion model that is capable of generating realistic point-basedshape representations, which preserve point correspondences that are present inthe training data. Using shape representation data with correspondences derivedfrom Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that ourcorrespondence-preserving model effectively generates point-based hippocampalshape representations that are highly realistic compared to existing methods.We further demonstrate the applications of our generative model by downstreamtasks, such as conditional generation of healthy and AD subjects and predictingmorphological changes of disease progression by counterfactual generation.</description>
      <author>example@mail.com (Shen Zhu, Yinzhu Jin, Ifrah Zawar, P. Thomas Fletcher)</author>
      <guid isPermaLink="false">2508.03925v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction</title>
      <link>http://arxiv.org/abs/2508.02408v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GR-Gaussian的基于图的3D高斯飞溅框架，用于解决CT重建中的针状伪影问题，提高稀疏视图条件下的重建准确性。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅(3DGS)已成为CT重建的一种有前景的方法，但现有方法依赖于视图内点的平均梯度幅度，这通常在稀疏视图条件下导致严重的针状伪影。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在稀疏视图条件下产生针状伪影的问题，提高稀疏视图条件下的重建准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了GR-Gaussian框架，包含两个关键创新：1)去噪点云初始化策略，减少初始化误差并加速收敛；2)像素图感知梯度策略，使用基于图的密度差异改进梯度计算，提高分割准确性和密度表示。&lt;h4&gt;主要发现&lt;/h4&gt;在X-3D和真实数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提升0.67 dB和0.92 dB，SSIM增益为0.011和0.021。&lt;h4&gt;结论&lt;/h4&gt;GR-Gaussian在具有挑战性的稀疏视图条件下适用于准确的CT重建。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅(3DGS)已成为CT重建的一种有前景的方法。然而，现有方法依赖于视图内点的平均梯度幅度，这通常在稀疏视图条件下导致严重的针状伪影。为应对这一挑战，我们提出了GR-Gaussian，一种基于图的3D高斯飞溅框架，可抑制针状伪影并提高稀疏视图条件下的重建准确性。我们的框架引入了两个关键创新：1)一种去噪点云初始化策略，可减少初始化误差并加速收敛；2)一种像素图感知梯度策略，使用基于图的密度差异改进梯度计算，提高分割准确性和密度表示。在X-3D和真实数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提升0.67 dB和0.92 dB，SSIM增益为0.011和0.021。这些结果突显了GR-Gaussian在具有挑战性的稀疏视图条件下进行准确CT重建的适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏视角CT重建中的针状伪影问题。这个问题在现实中非常重要，因为在医学成像和工业检测等领域，减少X射线曝光剂量对患者或检测对象至关重要，这自然会导致采集的投影数据稀疏。稀疏视角条件下的CT重建是一个高度不适定的问题，传统方法难以获得高质量重建结果，而现有方法要么需要大量标注数据，要么计算成本极高，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D高斯飞溅方法在稀疏视角CT重建中产生针状伪影的原因，发现这些伪影源于保留了具有小梯度的大高斯核，且缺乏对点间关系的考虑。基于这一分析，作者引入图结构来建模点之间的关系，设计了两个关键创新：去噪点云初始化策略和像素图感知梯度策略。该方法借鉴了3D高斯飞溅技术、基于NeRF的方法、传统CT重建方法以及图神经网络的思想，特别是利用KNN方法构建图结构，但针对CT重建的特殊需求进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入图结构来建模点之间的关系，解决稀疏视角CT重建中的针状伪影问题。整体实现流程包括：1) 使用增强的FDK方法生成初始体积；2) 应用去噪点云初始化策略对点云进行高斯滤波，减少噪声和伪影；3) 使用KNN算法构建图结构，顶点表示高斯核位置，边连接邻近高斯核；4) 训练优化阶段包括投影光栅化计算光度损失、体素化进行3D正则化、应用像素图感知梯度策略增强梯度计算；5) 根据增强后的梯度决定是否分裂或克隆高斯核；6) 使用多种损失函数进行优化；7) 通过体素化生成最终的密度体积。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于图的辐射高斯表示，考虑点与点之间的空间关系；2) 去噪点云初始化策略，使用高斯滤波减少初始化误差；3) 像素图感知梯度策略，利用密度差异增强梯度计算；4) 图拉普拉斯正则化，鼓励相邻高斯核之间的密度平滑。相比之前的工作，不同之处在于：与传统3D高斯飞溅方法相比，引入了图结构和密度差异来细化梯度计算；与基于NeRF的方法相比，显著提高了渲染效率；与传统CT重建方法相比，在稀疏视角条件下能产生更高质量的重建结果且计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GR-Gaussian通过引入图结构建模点间关系，结合去噪初始化和像素图感知梯度策略，有效解决了稀疏视角CT重建中的针状伪影问题，显著提高了重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CTreconstruction. However, existing methods rely on the average gradientmagnitude of points within the view, often leading to severe needle-likeartifacts under sparse-view conditions. To address this challenge, we proposeGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppressesneedle-like artifacts and improves reconstruction accuracy under sparse-viewconditions. Our framework introduces two key innovations: (1) a Denoised PointCloud Initialization Strategy that reduces initialization errors andaccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy thatrefines gradient computation using graph-based density differences, improvingsplitting accuracy and density representation. Experiments on X-3D andreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNRimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. Theseresults highlight the applicability of GR-Gaussian for accurate CTreconstruction under challenging sparse-view conditions.</description>
      <author>example@mail.com (Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang)</author>
      <guid isPermaLink="false">2508.02408v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>GraphProp: Training the Graph Foundation Models using Graph Properties</title>
      <link>http://arxiv.org/abs/2508.04594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了GraphProp，一种强调结构泛化的图基础模型训练方法，通过预测图不变量和利用结构GFM的表示作为位置编码来提高图模型在跨域任务中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;图基础模型需要在图分类等图级任务中具有强大的泛化能力，有效的GFM训练需要捕获跨不同域的一致信息。&lt;h4&gt;目的&lt;/h4&gt;解决传统GFMs缺乏跨域结构泛化能力的问题，提高图模型在跨域任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;GraphProp包含两个训练阶段：首先通过预测图不变量训练结构GFM，捕获抽象结构信息；然后使用结构GFM的表示作为位置编码训练全面GFM，利用特定域的节点属性和图标签改善跨域节点特征泛化。&lt;h4&gt;主要发现&lt;/h4&gt;图结构比节点特征和图标签提供更多跨域一致的信息；传统GFMs主要关注节点特征转移而缺乏结构泛化能力；GraphProp在监督学习和少样本学习中表现优异，尤其在处理无节点属性的图时。&lt;h4&gt;结论&lt;/h4&gt;GraphProp通过强调结构泛化，显著提高了图基础模型在跨域任务中的泛化能力，特别是在处理没有节点属性的图时。&lt;h4&gt;翻译&lt;/h4&gt;这项工作专注于训练图基础模型，使其在图分类等图级任务中具有强大的泛化能力。有效的GFM训练需要捕获跨不同域的一致信息。我们发现与节点特征和图标签相比，图结构提供了更多跨域一致的信息。然而，传统的GFMs主要关注将不同域的节点特征转移到统一的表示空间，但往往缺乏跨域的结构泛化能力。为此，我们引入了GraphProp，它强调结构泛化。GraphProp的训练过程包含两个主要阶段。首先，我们通过预测图不变量来训练结构GFM。由于图不变量仅依赖于抽象结构，不依赖于特定的标记或绘制，这种结构GFM能够捕获抽象结构信息，并提供可在不同域之间比较的判别性图表示。在第二阶段，我们使用结构GFM提供的表示作为位置编码来训练全面的GFM。这一阶段利用特定域的节点属性和图标签来进一步改善跨域节点特征泛化。我们的实验证明，GraphProp在监督学习和少样本学习中显著优于竞争对手，特别是在处理没有节点属性的图时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work focuses on training graph foundation models (GFMs) that have stronggeneralization ability in graph-level tasks such as graph classification.Effective GFM training requires capturing information consistent acrossdifferent domains. We discover that graph structures provide more consistentcross-domain information compared to node features and graph labels. However,traditional GFMs primarily focus on transferring node features from variousdomains into a unified representation space but often lack structuralcross-domain generalization. To address this, we introduce GraphProp, whichemphasizes structural generalization. The training process of GraphPropconsists of two main phases. First, we train a structural GFM by predictinggraph invariants. Since graph invariants are properties of graphs that dependonly on the abstract structure, not on particular labellings or drawings of thegraph, this structural GFM has a strong ability to capture the abstractstructural information and provide discriminative graph representationscomparable across diverse domains. In the second phase, we use therepresentations given by the structural GFM as positional encodings to train acomprehensive GFM. This phase utilizes domain-specific node attributes andgraph labels to further improve cross-domain node feature generalization. Ourexperiments demonstrate that GraphProp significantly outperforms thecompetitors in supervised learning and few-shot learning, especially inhandling graphs without node attributes.</description>
      <author>example@mail.com (Ziheng Sun, Qi Feng, Lehao Lin, Chris Ding, Jicong Fan)</author>
      <guid isPermaLink="false">2508.04594v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use</title>
      <link>http://arxiv.org/abs/2508.04482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACL 2025 (Oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文对操作系统代理（OS Agents）进行了全面综述，OS Agents是基于多模态大语言模型的AI助手，能在操作系统环境中通过图形用户界面等接口自动化完成任务。论文介绍了其基础组成部分、构建方法、评估标准，并讨论了当前挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;创造像钢铁侠中的J.A.R.V.I.S一样全能的AI助手一直是人们的梦想。随着多模态大语言模型的发展，基于(M)LLMs的OS Agents通过在操作系统环境中操作计算设备来自动化任务，已取得显著进展。&lt;h4&gt;目的&lt;/h4&gt;整合OS Agent领域的研究现状，为学术研究和工业发展提供指导，通过系统性地介绍基础知识、构建方法、评估标准和未来方向，帮助研究人员和开发者更好地理解和应用这一技术。&lt;h4&gt;方法&lt;/h4&gt;采用系统综述方法，首先阐明OS Agent的基础知识，包括环境、观察空间和行动空间等关键组成部分，以及理解、规划和基础能力等核心功能；然后探讨构建OS Agent的方法论，专注于领域特定的基础模型和代理框架；最后评估OS Agent在各种任务中的评估协议和基准。&lt;h4&gt;主要发现&lt;/h4&gt;OS Agent领域已取得显著进展，但仍面临安全和隐私、个性化和自我进化等挑战。开源GitHub资源为这一领域的进一步创新提供了支持。&lt;h4&gt;结论&lt;/h4&gt;OS Agent代表了AI助手发展的重要方向，将多模态大语言模型与操作系统环境相结合，能实现更复杂的任务自动化。未来研究应关注安全性、隐私保护、个性化和自我进化等方面。&lt;h4&gt;翻译&lt;/h4&gt;创造像钢铁侠中的J.A.R.V.I.S一样全能的人工智能助手的梦想一直吸引着人们的想象力。随着多模态大语言模型的发展，这一梦想正逐渐成为现实，因为基于(M)LLMs的代理使用计算设备在操作系统提供的环境和界面中操作来自动化任务，已经取得了显著进展。本文对这些先进的代理进行了全面综述，并将其命名为操作系统代理（OS Agents）。我们首先阐明OS Agent的基础知识，探讨其关键组成部分，包括环境、观察空间和行动空间，并概述了理解、规划和基础等基本能力。然后我们研究了构建OS Agent的方法论，专注于领域特定的基础模型和代理框架。对评估协议和基准的详细回顾强调了OS Agent如何在各种任务中被评估。最后，我们讨论了当前挑战并确定了未来研究的有前途方向，包括安全和隐私、个性化和自我进化。本综述旨在整合OS Agent领域的研究现状，为学术研究和工业发展提供见解。维护了一个开源GitHub仓库作为动态资源，以促进该领域的进一步创新。我们提供了论文的9页版本，已被ACL 2025接受，为该领域提供了简洁的概述。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The dream to create AI assistants as capable and versatile as the fictionalJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolutionof (multi-modal) large language models ((M)LLMs), this dream is closer toreality, as (M)LLM-based Agents using computing devices (e.g., computers andmobile phones) by operating within the environments and interfaces (e.g.,Graphical User Interface (GUI)) provided by operating systems (OS) to automatetasks have significantly advanced. This paper presents a comprehensive surveyof these advanced agents, designated as OS Agents. We begin by elucidating thefundamentals of OS Agents, exploring their key components including theenvironment, observation space, and action space, and outlining essentialcapabilities such as understanding, planning, and grounding. We then examinemethodologies for constructing OS Agents, focusing on domain-specificfoundation models and agent frameworks. A detailed review of evaluationprotocols and benchmarks highlights how OS Agents are assessed across diversetasks. Finally, we discuss current challenges and identify promising directionsfor future research, including safety and privacy, personalization andself-evolution. This survey aims to consolidate the state of OS Agentsresearch, providing insights to guide both academic inquiry and industrialdevelopment. An open-source GitHub repository is maintained as a dynamicresource to foster further innovation in this field. We present a 9-pageversion of our work, accepted by ACL 2025, to provide a concise overview to thedomain.</description>
      <author>example@mail.com (Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shenzhi Wang, Xinchen Xu, Shuofei Qiao, Zhaokai Wang, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, Fei Wu)</author>
      <guid isPermaLink="false">2508.04482v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions</title>
      <link>http://arxiv.org/abs/2508.04470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了名为FedHiP的异构不变性个性化联邦学习方案，通过分析性解决方案避免基于梯度的更新，解决了联邦学习中数据异构性问题，实现了理想的异构不变性特性。&lt;h4&gt;背景&lt;/h4&gt;个性化联邦学习(PFL)已成为一种普遍范式，通过协作训练同时适应每个客户端的本地应用来提供个性化模型。然而，现有PFL方法通常面临客户端间普遍存在的数据异构性(非IID数据)这一重大挑战，这严重阻碍了收敛并降低了性能。&lt;h4&gt;目的&lt;/h4&gt;从根本上解决基于梯度的更新对非IID数据的固有敏感性问题，弥补研究空白，提出一个能够处理数据异构性的个性化联邦学习方案。&lt;h4&gt;方法&lt;/h4&gt;研究提出了FedHiP方案，包含三个阶段：分析性本地训练、分析性全局聚合和分析性本地个性化。该方法利用自监督预训练的趋势，将基础模型作为冻结主干进行无梯度特征提取，并进一步开发了分析分类器进行无梯度训练。&lt;h4&gt;主要发现&lt;/h4&gt;FedHiP方案的理想特性是异构不变性，意味着无论其他客户端的数据分布如何非IID，每个个性化模型保持相同。在基准数据集上的广泛实验验证了FedHiP方案的优越性，在准确性上比最先进的基线方法高出至少5.79%-20.97%。&lt;h4&gt;结论&lt;/h4&gt;通过避免基于梯度的更新，FedHiP方案从根本上解决了联邦学习中数据异构性问题，实现了理想的异构不变性，显著提高了个性化模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，个性化联邦学习(PFL)已成为一种普遍范式，通过协作训练同时适应每个客户端的本地应用来提供个性化模型。现有PFL方法通常面临一个重大挑战，即客户端间普遍存在的数据异构性(非IID数据)，这严重阻碍了收敛并降低了性能。我们确定根本问题在于长期以来对基于梯度的更新的依赖，这些更新本质上对非IID数据敏感。为了从根本上解决这个问题并弥补研究空白，在本文中，我们提出了一个异构不变性个性化联邦学习方案，名为FedHiP，通过分析性(即闭式)解决方案避免基于梯度的更新。具体来说，我们利用自监督预训练的趋势，利用基础模型作为冻结主干进行无梯度特征提取。在特征提取器之后，我们进一步开发了分析分类器进行无梯度训练。为了支持集体泛化和个体个性化，我们的FedHiP方案包含三个阶段：分析性本地训练、分析性全局聚合和分析性本地个性化。FedHiP方案的分析性解决方案使其具有异构不变性的理想特性，这意味着无论其他客户端的数据分布如何非IID，每个个性化模型保持相同。在基准数据集上的广泛实验验证了我们的FedHiP方案的优越性，在准确性上比最先进的基线方法高出至少5.79%-20.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lately, Personalized Federated Learning (PFL) has emerged as a prevalentparadigm to deliver personalized models by collaboratively training whilesimultaneously adapting to each client's local applications. Existing PFLmethods typically face a significant challenge due to the ubiquitous dataheterogeneity (i.e., non-IID data) across clients, which severely hindersconvergence and degrades performance. We identify that the root issue lies inthe long-standing reliance on gradient-based updates, which are inherentlysensitive to non-IID data. To fundamentally address this issue and bridge theresearch gap, in this paper, we propose a Heterogeneity-invariant PersonalizedFederated learning scheme, named FedHiP, through analytical (i.e., closed-form)solutions to avoid gradient-based updates. Specifically, we exploit the trendof self-supervised pre-training, leveraging a foundation model as a frozenbackbone for gradient-free feature extraction. Following the feature extractor,we further develop an analytic classifier for gradient-free training. Tosupport both collective generalization and individual personalization, ourFedHiP scheme incorporates three phases: analytic local training, analyticglobal aggregation, and analytic local personalization. The closed-formsolutions of our FedHiP scheme enable its ideal property of heterogeneityinvariance, meaning that each personalized model remains identical regardlessof how non-IID the data are distributed across all other clients. Extensiveexperiments on benchmark datasets validate the superiority of our FedHiPscheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%in accuracy.</description>
      <author>example@mail.com (Jianheng Tang, Zhirui Yang, Jingchao Wang, Kejia Fan, Jinfeng Xu, Huiping Zhuang, Anfeng Liu, Houbing Herbert Song, Leye Wang, Yunhuai Liu)</author>
      <guid isPermaLink="false">2508.04470v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration</title>
      <link>http://arxiv.org/abs/2508.04450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TotalRegistrator是一个基于UNet架构和场分解策略的多器官图像配准框架，能够在不同解剖区域间实现高效配准，通过大规模数据集训练和多种外部数据集验证，证明了该方法在多器官配准任务中的优越性和强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;图像配准是临床实践中分析纵向和多阶段CT图像的基本技术，但现有方法大多针对单器官应用，限制了它们在其他解剖区域的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时对多个解剖区域进行配准的图像配准框架，提高方法的泛化能力，使其能够应用于不同的解剖区域。&lt;h4&gt;方法&lt;/h4&gt;提出了TotalRegistrator，使用标准UNet架构和新颖场分解策略的图像配准框架；构建了一个包含695个全身配对CT扫描的大规模纵向数据集；将TotalRegistrator与经典迭代算法和基础模型进行基准测试；在三个外部数据集上评估模型的鲁棒性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在内部数据集上，所提出的方法在多器官腹部配准中通常优于基线方法，但在肺部配准性能上略有下降；在分布外数据集上，尽管没有针对这些任务进行微调，但与领先的单一器官模型相比取得了具有竞争力的结果，证明了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TotalRegistrator是一个有效的多器官图像配准框架，具有强大的泛化能力，源代码将在GitHub上公开提供。&lt;h4&gt;翻译&lt;/h4&gt;图像配准是临床实践中分析纵向和多阶段CT图像的基本技术。然而，大多数现有方法针对单器官应用定制，限制了它们在其他解剖区域的泛化能力。这项工作提出了TotalRegistrator，一个图像配准框架，能够使用标准UNet架构和新颖的场分解策略同时对多个解剖区域进行配准。该模型轻量级，训练仅需11GB GPU内存。为了训练和评估我们的方法，我们构建了一个包含695个全身配对CT扫描的大规模纵向数据集，这些扫描来自在不同时间点获取的个体患者。我们将TotalRegistrator与通用的经典迭代算法和最近的图像配准基础模型进行了基准测试。为了进一步评估鲁棒性和泛化能力，我们在三个外部数据集上评估了我们的模型：来自Learn2Reg挑战赛的公开胸部和腹部数据集，以及合作医院的私人多阶段腹部数据集。内部数据集上的实验结果表明，所提出的方法在多器官腹部配准中通常优于基线方法，但在肺部配准性能上略有下降。在分布外数据集上，尽管没有针对这些任务进行微调，但与领先的单一器官模型相比取得了具有竞争力的结果，证明了其强大的泛化能力。源代码将在以下公开提供：https://github.com/DIAGNijmegen/oncology_image_registration.git。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有CT图像配准方法大多针对单一器官设计，难以泛化到其他解剖区域的问题。这个问题在临床实践中很重要，因为癌症治疗中常见全身成像，需要一个通用配准模型来简化临床流程，减少对特定任务模型的需求，这些特定模型通常需要大量资源训练维护，且难以处理跨越多个解剖区域的工作流程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统配准方法计算密集且需要参数调整，难以应用于临床环境；然后指出现有深度学习方法虽然直接预测变形场，但大多局限于单一器官。作者借鉴了uniGradICON等基础模型的思想，但发现其依赖异构数据集且计算资源需求大。因此，作者提出使用单一精心策划的纵向数据集从头训练，采用区域特定的场分解策略，借鉴了UNet架构、多级级联架构和无监督学习框架（如VoxelMorph）的思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是场分解策略：将全身视图分为胸部、腹部和骨骼区域，每个区域分配独立注册块，最后由整体块融合产生连贯变形场。实现流程包括：1)收集695对全身CT扫描数据；2)预处理图像并重采样；3)构建基于UNet的注册块架构；4)按顺序训练各块(仿射→骨骼→胸部→腹部→全身)；5)结合互信息损失、Dice损失和正则化训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)区域特定的场分解策略；2)轻量级可在11GB GPU上运行的模型；3)包含695对扫描的大规模纵向数据集，其中104例由专家标注。相比之前工作，该方法使用单一数据集而非多样化公共数据集，采用场分解而非单一变形场，训练时独立训练各块减少内存需求，并增加了骨骼注册块扩展解剖覆盖范围。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TotalRegistrator提出了一种基于场分解策略的轻量级通用CT图像配准模型，能在标准GPU上高效训练，实现对多个解剖区域的准确配准，同时保持与最先进方法相当的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image registration is a fundamental technique in the analysis of longitudinaland multi-phase CT images within clinical practice. However, most existingmethods are tailored for single-organ applications, limiting theirgeneralizability to other anatomical regions. This work presentsTotalRegistrator, an image registration framework capable of aligning multipleanatomical regions simultaneously using a standard UNet architecture and anovel field decomposition strategy. The model is lightweight, requiring only11GB of GPU memory for training. To train and evaluate our method, weconstructed a large-scale longitudinal dataset comprising 695 whole-body(thorax-abdomen-pelvic) paired CT scans from individual patients acquired atdifferent time points. We benchmarked TotalRegistrator against a genericclassical iterative algorithm and a recent foundation model for imageregistration. To further assess robustness and generalizability, we evaluatedour model on three external datasets: the public thoracic and abdominaldatasets from the Learn2Reg challenge, and a private multiphase abdominaldataset from a collaborating hospital. Experimental results on the in-housedataset show that the proposed approach generally surpasses baseline methods inmulti-organ abdominal registration, with a slight drop in lung alignmentperformance. On out-of-distribution datasets, it achieved competitive resultscompared to leading single-organ models, despite not being fine-tuned for thosetasks, demonstrating strong generalizability. The source code will be publiclyavailable at: https://github.com/DIAGNijmegen/oncology_image_registration.git.</description>
      <author>example@mail.com (Xuan Loc Pham, Gwendolyn Vuurberg, Marjan Doppen, Joey Roosen, Tip Stille, Thi Quynh Ha, Thuy Duong Quach, Quoc Vu Dang, Manh Ha Luu, Ewoud J. Smit, Hong Son Mai, Mattias Heinrich, Bram van Ginneken, Mathias Prokop, Alessa Hering)</author>
      <guid isPermaLink="false">2508.04450v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Foundation Models for Mitotic Figure Classification</title>
      <link>http://arxiv.org/abs/2508.04441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基础模型在有丝分裂图像分类任务中的应用，通过低秩适应(LoRA)技术调整模型，发现仅使用10%的训练数据就能接近100%数据可用性时的性能水平，并在跨域泛化方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型性能随数据数量和多样性增加而提升，但在病理学等医学影像领域，特定任务的标记图像数据通常有限。自监督学习技术可以利用大量未标记数据训练基础模型，提供语义丰富的特征向量，解决数据有限问题。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型在有丝分裂图像分类任务中的应用，有丝分裂计数是某些肿瘤的独立预后标志物，也是肿瘤分级系统的一部分。&lt;h4&gt;方法&lt;/h4&gt;研究多个当前基础模型的数据扩展规律，评估模型对未见肿瘤域的鲁棒性，使用注意力机制的LoRA调整模型，并与端到端训练的基线模型（CNN和Vision Transformers）进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA调整的基础模型性能优于标准线性调整的模型；仅使用10%的训练数据，性能接近100%数据可用性时的水平；最新的基础模型使用LoRA调整几乎消除了在未见肿瘤域上的性能差距；传统架构的完整微调仍具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;基础模型，特别是使用LoRA调整的模型，在有限数据条件下表现优异，且在跨域泛化方面也有良好表现。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型的性能已知随数据数量和多样性的增加而提升。在病理学，如同许多其他医学影像领域，特定任务的标记图像通常可用性有限。自监督学习技术使得能够利用大量未标记数据训练大规模神经网络，即基础模型，这些模型可以通过提供语义丰富的特征向量来解决有限数据问题，这些特征向量可以很好地泛化到新任务，只需最少的训练努力，从而提高模型性能和鲁棒性。在这项工作中，我们研究了基础模型在有丝分裂图像分类任务中的应用。有丝分裂计数可以从这一分类任务中推导出来，它是某些肿瘤的独立预后标志物，也是某些肿瘤分级系统的一部分。特别是，我们研究了多个当前基础模型的数据扩展规律，并评估它们对未见肿瘤域的鲁棒性。除了常用的线性探测范式外，我们还使用注意力机制的低秩适应(LoRA)来调整模型。我们将所有模型与端到端训练的基线模型（CNN和Vision Transformers）进行比较。我们的结果表明，LoRA调整的基础模型比使用标准线性调整的模型提供更优越的性能，仅使用10%的训练数据就能达到接近100%数据可用性时的性能水平。此外，最新基础模型的LoRA调整在评估未见肿瘤域时几乎消除了域外性能差距。然而，传统架构的完整微调仍然具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of deep learning models is known to scale with data quantityand diversity. In pathology, as in many other medical imaging domains, theavailability of labeled images for a specific task is often limited.Self-supervised learning techniques have enabled the use of vast amounts ofunlabeled data to train large-scale neural networks, i.e., foundation models,that can address the limited data problem by providing semantically richfeature vectors that can generalize well to new tasks with minimal trainingeffort increasing model performance and robustness. In this work, weinvestigate the use of foundation models for mitotic figure classification. Themitotic count, which can be derived from this classification task, is anindependent prognostic marker for specific tumors and part of certain tumorgrading systems. In particular, we investigate the data scaling laws onmultiple current foundation models and evaluate their robustness to unseentumor domains. Next to the commonly used linear probing paradigm, we also adaptthe models using low-rank adaptation (LoRA) of their attention mechanisms. Wecompare all models against end-to-end-trained baselines, both CNNs and VisionTransformers. Our results demonstrate that LoRA-adapted foundation modelsprovide superior performance to those adapted with standard linear probing,reaching performance levels close to 100% data availability with only 10% oftraining data. Furthermore, LoRA-adaptation of the most recent foundationmodels almost closes the out-of-domain performance gap when evaluated on unseentumor domains. However, full fine-tuning of traditional architectures stillyields competitive performance.</description>
      <author>example@mail.com (Jonas Ammeling, Jonathan Ganz, Emely Rosbach, Ludwig Lausser, Christof A. Bertram, Katharina Breininger, Marc Aubreville)</author>
      <guid isPermaLink="false">2508.04441v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones</title>
      <link>http://arxiv.org/abs/2508.04379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近期研究表明，在图像上预训练的视觉模型可通过将时间序列预测重新表述为图像重建任务，在时间序列预测中表现良好，但面临三个关键挑战：数据模态差异、多元预测差异和概率预测差异。为解决这些问题，研究者提出VisionTS++，一种基于视觉模型的时间序列基础模型，包含三项创新：基于视觉模型的过滤机制、彩色多元转换方法和多分位数预测方法。实验证明，该方法在多个基准测试中取得了最先进的结果，MSE降低比专用模型高6%-44%，在12个概率预测设置中的9个中排名第一。&lt;h4&gt;背景&lt;/h4&gt;近期研究表明，在图像上预训练的视觉模型可以通过将时间序列预测重新表述为图像重建任务，在时间序列预测中表现良好，表明它们可能成为通用时间序列基础模型的潜力。&lt;h4&gt;目的&lt;/h4&gt;弥合视觉模型到时间序列跨模态迁移的三个关键差距：数据模态差异、多元预测差异和概率预测差异，并提出一种基于视觉模型的时间序列基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出VisionTS++，一种基于视觉模型的时间序列基础模型，包含三项创新：(1)基于视觉模型的过滤机制识别高质量时间序列数据；(2)彩色多元转换方法将多元时间序列转换为多子图RGB图像；(3)多分位数预测方法使用并行重建头生成不同分位数水平的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在分布内和分布外时间序列预测基准上取得了最先进(SOTA)的结果，在MSE降低方面比专用时间序列基础模型高出6%-44%，在12个概率预测设置中的9个中排名第一。&lt;h4&gt;结论&lt;/h4&gt;该工作为跨模态知识转移建立了新范式，推动了通用时间序列基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，在图像上预训练的视觉模型可以通过将预测重新表述为图像重建任务，在时间序列预测中表现良好，表明它们作为通用时间序列基础模型的潜力。然而，由于三个关键差异，从视觉到时间序列的有效跨模态转移仍然具有挑战性：(1)结构化、有界的图像数据与无界、异构的时间序列之间的数据模态差距；(2)标准RGB三通道视觉模型与需要建模具有任意数量变量的时间序列之间的多元预测差距；(3)大多数视觉模型的确定性输出格式与需要不确定性感知的概率预测要求之间的概率预测差距。为了弥合这些差距，我们提出了VisionTS++，一种基于视觉模型的时间序列基础模型，在大规模时间序列数据集上进行持续预训练，包括三项创新：(1)基于视觉模型的过滤机制，用于识别高质量时间序列数据，从而减轻模态差距并提高预训练稳定性；(2)彩色多元转换方法，将多元时间序列转换为多子图RGB图像，捕捉复杂的变量间依赖关系；(3)多分位数预测方法，使用并行重建头生成不同分位数水平的预测，从而更灵活地近似任意输出分布，无需限制性的先验分布假设。在分布内和分布外时间序列预测基准上评估，该模型取得了最先进的结果，在MSE降低方面比专用时间序列基础模型高出6%-44%，在12个概率预测设置中的9个中排名第一。我们的工作为跨模态知识转移建立了新范式，推动了通用时间序列基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have revealed that vision models pre-trained on images canperform well in time series forecasting by reformulating forecasting as animage reconstruction task, suggesting their potential as universal time seriesfoundation models. However, effective cross-modal transfer from vision to timeseries remains challenging due to three key discrepancies: (1) data-modalitygap between structured, bounded image data and unbounded, heterogeneous timeseries; (2) multivariate-forecasting gap between standard RGBthree-channel-based vision models and the need to model time series witharbitrary numbers of variates; and (3) probabilistic-forecasting gap betweenthe deterministic output formats of most vision models and the requirement foruncertainty-aware probabilistic predictions. To bridge these gaps, we proposeVisionTS++, a vision-model-based TSFM that performs continual pre-training onlarge-scale time series datasets, including 3 innovations: (1) avision-model-based filtering mechanism to identify high-quality time seriesdata, thereby mitigating modality gap and improving pre-training stability, (2)a colorized multivariate conversion method that transforms multivariate timeseries into multi-subfigure RGB images, capturing complex inter-variatedependencies; and (3) a multi-quantile forecasting approach using parallelreconstruction heads to generate forecasts of different quantile levels, thusmore flexibly approximating arbitrary output distributions without restrictiveprior distributional assumptions. Evaluated on both in-distribution andout-of-distribution TSF benchmarks, \model achieves SOTA results, outperformingspecialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12probabilistic forecasting settings. Our work establishes a new paradigm forcross-modal knowledge transfer, advancing the development of universal TSFMs.</description>
      <author>example@mail.com (Lefei Shen, Mouxiang Chen, Xu Liu, Han Fu, Xiaoxue Ren, Jianling Sun, Zhuo Li, Chenghao Liu)</author>
      <guid isPermaLink="false">2508.04379v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</title>
      <link>http://arxiv.org/abs/2508.04361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了OmniPlay评估基准，用于测试多模态模型在动态交互环境中的智能表现。通过五个游戏环境评估六种领先模型，发现它们在记忆任务上表现出色但在推理和规划方面存在系统性失败，揭示了模型融合机制的脆弱性和'少即是多'的悖论现象。&lt;h4&gt;背景&lt;/h4&gt;现有通用基础模型(如Gemini和GPT-4o)展示了多模态能力，但现有评估无法测试它们在动态、交互式世界中的智能。静态基准缺乏自主性，交互式基准存在模态瓶颈，通常忽略关键听觉和时间线索。&lt;h4&gt;目的&lt;/h4&gt;为了弥合评估差距，引入OmniPlay这一诊断基准，旨在评估和探索代理模型在完整感官谱系中的融合和推理能力。&lt;h4&gt;方法&lt;/h4&gt;OmniPlay建立在模态相互依赖的核心理念上，包含五个游戏环境，系统性地创造协同和冲突场景，迫使代理执行真正的跨模态推理。对六种领先全模态模型进行了全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现模型在高保真记忆任务上表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。这种脆弱性源于脆弱的融合机制，导致在模态冲突下性能灾难性下降，并发现'少即是多'的悖论现象。&lt;h4&gt;结论&lt;/h4&gt;实现稳健AGI的路径需要超越规模扩展的研究重点，明确解决协同融合问题。&lt;h4&gt;翻译&lt;/h4&gt;虽然像Gemini和GPT-4o这样的通用基础模型展示了令人印象深刻的多模态能力，但现有的评估无法测试它们在动态、交互式世界中的智能。静态基准缺乏自主性，而交互式基准则存在严重的模态瓶颈，通常忽略关键的听觉和时间线索。为了弥合这一评估差距，我们引入了OmniPlay，这是一个诊断基准，不仅用于评估，还旨在探索代理模型在完整感官谱系中的融合和推理能力。OmniPlay建立在模态相互依赖的核心理念上，包含一套五个游戏环境，这些环境系统性地创造协同和冲突场景，迫使代理执行真正的跨模态推理。我们对六种领先的全模态模型的全面评估揭示了一个关键的两极分化：它们在高保真记忆任务上表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。我们证明这种脆弱性源于脆弱的融合机制，这导致在模态冲突下性能灾难性下降，并发现了一个反直觉的'少即是多'悖论，即移除感官信息反而可能提高性能。我们的研究结果表明，实现稳健AGI的路径需要超越规模扩展的研究重点，明确解决协同融合问题。我们的平台可在https://github.com/fuqingbie/omni-game-benchmark上匿名查看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While generalist foundation models like Gemini and GPT-4o demonstrateimpressive multi-modal competence, existing evaluations fail to test theirintelligence in dynamic, interactive worlds. Static benchmarks lack agency,while interactive benchmarks suffer from a severe modal bottleneck, typicallyignoring crucial auditory and temporal cues. To bridge this evaluation chasm,we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,but to probe the fusion and reasoning capabilities of agentic models across thefull sensory spectrum. Built on a core philosophy of modality interdependence,OmniPlay comprises a suite of five game environments that systematically createscenarios of both synergy and conflict, forcing agents to perform genuinecross-modal reasoning. Our comprehensive evaluation of six leading omni-modalmodels reveals a critical dichotomy: they exhibit superhuman performance onhigh-fidelity memory tasks but suffer from systemic failures in challengesrequiring robust reasoning and strategic planning. We demonstrate that thisfragility stems from brittle fusion mechanisms, which lead to catastrophicperformance degradation under modality conflict and uncover a counter-intuitive"less is more" paradox, where removing sensory information can paradoxicallyimprove performance. Our findings suggest that the path toward robust AGIrequires a research focus beyond scaling to explicitly address synergisticfusion. Our platform is available for anonymous review athttps://github.com/fuqingbie/omni-game-benchmark.</description>
      <author>example@mail.com (Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He)</author>
      <guid isPermaLink="false">2508.04361v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Chain of Questions: Guiding Multimodal Curiosity in Language Models</title>
      <link>http://arxiv.org/abs/2508.04350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了问题链(CoQ)框架，一种好奇心驱动的推理方法，使多模态语言模型能够动态生成针对性问题并选择性地激活相关感官模态，从而提高推理能力和准确性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的推理能力通过思维链和逐步解释方法得到显著提升，但这些改进尚未完全过渡到多模态环境中，模型需要主动决定在复杂现实环境中使用哪些感官模态。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法使多模态语言模型能够动态生成关于周围环境的针对性问题，引导模型选择性地激活相关模态，收集准确推理和响应生成所需的关键信息。&lt;h4&gt;方法&lt;/h4&gt;提出问题链(CoQ)框架，并通过整合WebGPT、ScienceQA、AVSD和ScanQA数据集构建新的多模态基准数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;CoQ方法提高了基础模型有效识别和整合相关感官信息的能力，导致准确性提高、推理过程可解释性增强以及与多样化多模态任务的对齐。&lt;h4&gt;结论&lt;/h4&gt;CoQ框架能够改善多模态语言模型的推理能力，使其能够更好地处理复杂的现实世界环境。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型中的推理能力已通过思维链和明确的逐步解释方法得到显著提升。然而，这些改进尚未完全过渡到多模态环境中，在多模态环境中，模型必须主动决定在与复杂的现实世界环境互动时使用哪些感官模态，如视觉、音频或空间感知。在本文中，我们引入了问题链(CoQ)框架，这是一种好奇心驱动的推理方法，它鼓励多模态语言模型动态生成关于其周围环境的针对性问题。这些生成的问题引导模型选择性地激活相关模态，从而收集准确推理和响应生成所需的关键信息。我们在一个新颖的多模态基准数据集上评估了我们的框架，该数据集是通过整合WebGPT、ScienceQA、AVSD和ScanQA数据集构建的。实验结果表明，我们的CoQ方法提高了基础模型有效识别和整合相关感官信息的能力，从而提高了准确性、推理过程的可解释性以及与多样化多模态任务的对齐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning capabilities in large language models (LLMs) have substantiallyadvanced through methods such as chain-of-thought and explicit step-by-stepexplanations. However, these improvements have not yet fully transitioned tomultimodal contexts, where models must proactively decide which sensorymodalities such as vision, audio, or spatial perception to engage wheninteracting with complex real-world environments. In this paper, we introducethe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approachthat encourages multimodal language models to dynamically generate targetedquestions regarding their surroundings. These generated questions guide themodel to selectively activate relevant modalities, thereby gathering criticalinformation necessary for accurate reasoning and response generation. Weevaluate our framework on a novel multimodal benchmark dataset, assembled byintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental resultsdemonstrate that our CoQ method improves a foundation model's ability toeffectively identify and integrate pertinent sensory information. This leads toimproved accuracy, interpretability, and alignment of the reasoning processwith diverse multimodal tasks.</description>
      <author>example@mail.com (Nima Iji, Kia Dashtipour)</author>
      <guid isPermaLink="false">2508.04350v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks</title>
      <link>http://arxiv.org/abs/2508.04316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于掩码自编码器的DAS信号识别基础模型MAEPD，通过视觉提示调优(VPT)方法，在仅微调0.322%参数的情况下实现了96.94%的分类准确率，超越了传统方法并减少了45%的训练时间。&lt;h4&gt;背景&lt;/h4&gt;分布式声学传感(DAS)技术在多个领域有广泛应用，但由于异构传感环境导致的数据分布差异给数据驱动的人工智能模型带来挑战，限制了跨域泛化能力，并面临标记训练数据短缺的问题。&lt;h4&gt;目的&lt;/h4&gt;解决DAS数据分布差异导致的模型泛化能力有限和标记数据短缺问题，开发一个通用的DAS信号识别基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出MAEPD模型，在包含635,860个样本的多类型DAS数据集上进行自监督掩码重建预训练，捕获DAS信号的深层语义特征；采用视觉提示调优(VPT)进行下游任务，冻结预训练骨干参数，只微调插入到Transformer编码器层中的少量可学习视觉提示向量。&lt;h4&gt;主要发现&lt;/h4&gt;在室内步态识别任务中，VPT-Deep方法仅微调0.322%参数就达到96.94%分类准确率，比传统完全微调方法高0.61%，同时训练时间减少45%；模型在管道泄漏检测中也表现出强大性能。&lt;h4&gt;结论&lt;/h4&gt;MAEPD作为基础模型具有通用性、高效性和可扩展性，为解决DAS领域信号识别模型泛化能力有限的问题提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;分布式声学传感(DAS)技术在各个领域发现越来越多的应用。然而，由于异构传感环境导致的数据分布差异给数据驱动的人工智能(AI)模型带来挑战，限制了跨域泛化能力并面临标记训练数据短缺的问题。为解决这些问题，本研究提出了一种基于掩码自编码器的DAS信号识别基础模型，名为MAEPD。MAEPD模型在包含635,860个样本的数据集上进行预训练，涵盖DAS步态时空信号、用于周界安全的2D GASF图像、用于管道泄漏的2D时频图像，以及包括鲸鱼发声和地震活动的开放数据集信号，使用自监督掩码重建任务来捕获DAS信号的深层语义特征。采用视觉提示调优(VPT)进行下游识别任务。该方法冻结预训练骨干参数，只微调插入到Transformer编码器层中的一小组可学习视觉提示向量。在NVIDIA GeForce RTX 4080 Super平台上使用室内步态识别作为下游任务验证MAEPD。VPT-Deep方法仅微调0.322%的参数就实现了96.94%的分类准确率，超越了传统的完全微调(FFT)方法0.61%，并将训练时间减少了45%。该模型在管道泄漏检测中也表现出强大的性能，证实了MAEPD作为基础模型的通用性、高效性和可扩展性。这种方法为解决DAS领域信号识别模型泛化能力有限的问题提供了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributed Acoustic Sensing (DAS) technology finds growing applicationsacross various domains. However, data distribution disparities due toheterogeneous sensing environments pose challenges for data-driven artificialintelligence (AI) models, limiting cross-domain generalization and facing ashortage of labeled training data. To address these issues, this study proposesa foundational model for DAS signal recognition based on a Masked Autoencoder,named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples,encompassing DAS gait spatiotemporal signals, 2D GASF images for perimetersecurity, 2D time-frequency images for pipeline leakage, and open-datasetsignals including whale vocalizations and seismic activities, using aself-supervised mask reconstruction task to capture deep semantic features ofDAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognitiontasks. This method freezes the pretrained backbone parameters and fine-tunesonly a small set of learnable visual prompt vectors inserted into theTransformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Superplatform validate MAEPD using indoor gait recognition as a downstream task. TheVPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322%of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT)method by 0.61% and reducing training time by 45%. The model also exhibitsrobust performance in pipeline leakage detection, confirming the generality,efficiency, and scalability of MAEPD as a foundational model. This approachoffers a novel paradigm for addressing the limited generalization of signalrecognition models in the DAS domain.</description>
      <author>example@mail.com (Kun Gui, Hongliang Ren, Shang Shi, Jin Lu, Changqiu Yu, Quanjun Cao, Guomin Gu, Qi Xuan)</author>
      <guid isPermaLink="false">2508.04316v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Edge2Prompt: Modality-Agnostic Model for Out-of-Distribution Liver Segmentation</title>
      <link>http://arxiv.org/abs/2508.04305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Edge2Prompt是一种新颖的模态无关肝脏分割管道，结合经典边缘检测与基础模型，能有效处理分布内和分布外数据，在数据稀缺场景下表现优异。&lt;h4&gt;背景&lt;/h4&gt;肝脏分割对于肿瘤切除或移植等手术的术前规划至关重要，但由于模态特定工具和数据稀缺，在临床工作流程中实施面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种模态无关的肝脏分割方法，能够推广到分布外(OOD)数据，解决临床应用中的数据稀缺问题。&lt;h4&gt;方法&lt;/h4&gt;Edge2Prompt管道首先从输入图像中提取模态无关的边缘图，然后通过U-Net处理这些边缘图生成基于logit的提示，这些提示引导Segment Anything Model 2(SAM-2)生成2D肝脏分割，最后将2D分割重建为3D体积。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态CHAOS数据集上评估，Edge2Prompt在分布内训练和测试时与经典分割方法具有竞争力；在数据稀缺场景下表现更优；在OOD任务上达到86.4%的平均Dice分数，比U-Net基线高27.4%，比其他自提示方法高9.1%。&lt;h4&gt;结论&lt;/h4&gt;Edge2Prompt成功桥接了经典模型和基础模型，实现了临床适应性高、数据效率高的肝脏分割方法。&lt;h4&gt;翻译&lt;/h4&gt;肝脏分割对于肿瘤切除或移植等手术的术前规划至关重要，但由于模态特定工具和数据稀缺，在临床工作流程中实施面临挑战。我们提出了Edge2Prompt，一种新颖的模态无关肝脏分割管道，能够推广到分布外(OOD)数据。我们的方法结合了经典边缘检测和基础模型。首先从输入图像中提取模态无关的边缘图，然后通过U-Net处理生成基于logit的提示。这些提示引导Segment Anything Model 2(SAM-2)生成2D肝脏分割，随后可重建为3D体积。在多模态CHAOS数据集上的评估显示，Edge2Prompt在分布内训练和测试时与经典分割方法具有竞争力，并且在数据稀缺场景下由于SAM-2模块而优于其他方法。此外，它在OOD任务上达到86.4%的平均Dice分数，比U-Net基线高27.4%，比其他自提示方法高9.1%，证明了其有效性。这项工作将经典模型和基础模型桥接起来，实现了临床适应性高、数据效率高的分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Liver segmentation is essential for preoperative planning in interventionslike tumor resection or transplantation, but implementation in clinicalworkflows faces challenges due to modality-specific tools and data scarcity. Wepropose Edge2Prompt, a novel pipeline for modality-agnostic liver segmentationthat generalizes to out-of-distribution (OOD) data. Our method integratesclassical edge detection with foundation models. Modality-agnostic edge mapsare first extracted from input images, then processed by a U-Net to generatelogit-based prompts. These prompts condition the Segment Anything Model 2(SAM-2) to generate 2D liver segmentations, which can then be reconstructedinto 3D volumes. Evaluated on the multi-modal CHAOS dataset, Edge2Promptachieves competitive results compared to classical segmentation methods whentrained and tested in-distribution (ID), and outperforms them in data-scarcescenarios due to the SAM-2 module. Furthermore, it achieves a mean Dice Scoreof 86.4% on OOD tasks, outperforming U-Net baselines by 27.4% and otherself-prompting methods by 9.1%, demonstrating its effectiveness. This workbridges classical and foundation models for clinically adaptable,data-efficient segmentation.</description>
      <author>example@mail.com (Nathan Hollet, Oumeymah Cherkaoui, Philippe C. Cattin, Sidaty El hadramy)</author>
      <guid isPermaLink="false">2508.04305v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>What Holds Back Open-Vocabulary Segmentation?</title>
      <link>http://arxiv.org/abs/2508.04211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at ICCV 25 Workshop: What is Next in  Multimodal Foundation Models?&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对开放词汇分割模型的性能瓶颈问题，提出了新的oracle组件来识别和解耦这些瓶颈，并通过实验提供了重要发现，为未来研究指明了方向。&lt;h4&gt;背景&lt;/h4&gt;标准分割设置无法训练出识别训练分类之外概念的模型。开放词汇方法通过语言-图像预训练有望解决这一问题，但性能已停滞近两年。&lt;h4&gt;目的&lt;/h4&gt;识别并解决开放词汇分割模型中的性能瓶颈，提升模型对训练分类外概念的识别能力。&lt;h4&gt;方法&lt;/h4&gt;提出新的oracle组件，利用真实信息来识别和解耦导致开放词汇模型性能停滞的瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;验证实验提供了关于开放词汇模型失败原因的重要经验发现，并提出了有前途的未来研究方向。&lt;h4&gt;结论&lt;/h4&gt;通过识别和解耦瓶颈，该研究为开放词汇分割模型的未来发展提供了新思路和方向。&lt;h4&gt;翻译&lt;/h4&gt;标准的分割设置无法提供能够识别训练分类之外概念的模型。开放词汇方法承诺通过使用数十亿图像-标题对进行语言-图像预训练来弥补这一差距。不幸的是，我们观察到由于几个瓶颈，这一承诺未能实现，导致性能停滞了近两年。本文提出了新的oracle组件，利用真实信息来识别和解耦这些瓶颈。所呈现的验证实验提供了重要的经验发现，这些发现对开放词汇模型的失败提供了更深入的见解，并提出了突出的方法来解锁未来的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Standard segmentation setups are unable to deliver models that can recognizeconcepts outside the training taxonomy. Open-vocabulary approaches promise toclose this gap through language-image pretraining on billions of image-captionpairs. Unfortunately, we observe that the promise is not delivered due toseveral bottlenecks that have caused the performance to plateau for almost twoyears. This paper proposes novel oracle components that identify and decouplethese bottlenecks by taking advantage of the groundtruth information. Thepresented validation experiments deliver important empirical findings thatprovide a deeper insight into the failures of open-vocabulary models andsuggest prominent approaches to unlock the future research.</description>
      <author>example@mail.com (Josip Šarić, Ivan Martinović, Matej Kristan, Siniša Šegvić)</author>
      <guid isPermaLink="false">2508.04211v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork</title>
      <link>http://arxiv.org/abs/2508.04163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合基于知识和数据驱动方法的架构，用于解决AI代理在辅助角色中临时团队协作的问题。该架构使每个代理能够通过非单调逻辑推理，利用先验常识知识、快速学习和修订的模型来预测其他代理行为，以及基于基础模型中类似情况的通用知识来预测未来目标。&lt;h4&gt;背景&lt;/h4&gt;AI代理在辅助角色中经常需要与其他代理（人类、AI系统）协作，而无需事先协调。目前最先进的方法通常采用数据驱动的方法，需要大量标记的先前观测数据集，缺乏透明度，并且难以快速修改现有知识以应对变化。随着代理数量的增加，决策的复杂性使得有效协作变得困难。&lt;h4&gt;目的&lt;/h4&gt;提倡利用基于知识和数据驱动方法的互补优势来进行临时团队推理和学习。&lt;h4&gt;方法&lt;/h4&gt;提出一种架构，使每个临时代理能够通过非单调逻辑推理确定其行动：(a)使用先验常识领域特定知识；(b)使用快速学习和修订的模型来预测其他代理的行为；(c)基于现有基础模型中类似情况的通用知识来预测抽象的未来目标。在VirtualHome（真实的基于物理的3D模拟环境）中实验评估该架构的能力。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中没有明确提及具体发现，但暗示了所提出架构在解决临时团队协作问题上的有效性。&lt;h4&gt;结论&lt;/h4&gt;摘要中没有明确结论，但暗示了所提出的基于知识和数据驱动相结合的方法对于解决临时团队协作问题有潜力。&lt;h4&gt;翻译&lt;/h4&gt;部署在辅助角色中的AI代理通常必须与其他代理（人类、AI系统）协作，而无需事先协调。此类临时团队中被认为最先进的方法通常采用数据驱动的方法，需要大量标记的先前观测数据集，缺乏透明度，并且难以快速修改现有知识以应对变化。随着代理数量的增加，决策的复杂性使得有效协作变得困难。本文提倡利用基于知识和数据驱动方法的互补优势来进行临时团队推理和学习。对于任何给定目标，我们的架构使每个临时代理能够通过非单调逻辑推理确定其行动：(a)先验常识领域特定知识；(b)快速学习和修订以预测其他代理行为的模型；(c)基于现有基础模型中类似情况的通用知识预测抽象未来目标。我们在VirtualHome（一个真实的基于物理的3D模拟环境）中实验评估了我们架构的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI agents deployed in assistive roles often have to collaborate with otheragents (humans, AI systems) without prior coordination. Methods consideredstate of the art for such ad hoc teamwork often pursue a data-driven approachthat needs a large labeled dataset of prior observations, lacks transparency,and makes it difficult to rapidly revise existing knowledge in response tochanges. As the number of agents increases, the complexity of decision-makingmakes it difficult to collaborate effectively. This paper advocates leveragingthe complementary strengths of knowledge-based and data-driven methods forreasoning and learning for ad hoc teamwork. For any given goal, ourarchitecture enables each ad hoc agent to determine its actions throughnon-monotonic logical reasoning with: (a) prior commonsense domain-specificknowledge; (b) models learned and revised rapidly to predict the behavior ofother agents; and (c) anticipated abstract future goals based on genericknowledge of similar situations in an existing foundation model. Weexperimentally evaluate our architecture's capabilities in VirtualHome, arealistic physics-based 3D simulation environment.</description>
      <author>example@mail.com (Hasra Dodampegama, Mohan Sridharan)</author>
      <guid isPermaLink="false">2508.04163v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>STARE: Predicting Decision Making Based on Spatio-Temporal Eye Movements</title>
      <link>http://arxiv.org/abs/2508.04148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为STARE的深度学习架构，用于从原始注视点或眼睛在决策环境图像上的时间序列预测各种消费者选择行为。&lt;h4&gt;背景&lt;/h4&gt;目前还没有可用于从原始注视点或眼睛在决策环境图像上的时间序列预测各种消费者选择行为的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度学习架构，能够从眼睛运动的时间序列数据有效预测消费者选择行为。&lt;h4&gt;方法&lt;/h4&gt;提出名为STARE的架构，使用新的标记化策略将眼动坐标映射到预定义的兴趣区域，使数据可用于基于T5架构的Chronos时间序列基础模型，并添加共同注意力和/或交叉注意力机制捕捉眼动特征。与多种最先进方法在多个数据集上进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体研究结果，只提到了与现有方法的比较研究。&lt;h4&gt;结论&lt;/h4&gt;这是开发和测试基于眼运动神经生理学的视觉注意力动力学的深度学习架构的第一步。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种深度学习架构，用于从决策环境图像的原始注视点或眼睛注视的时间序列预测各种消费者选择行为，目前此类基础模型尚不存在。该架构名为STARE（眼动时空注意力表示），采用新的标记化策略，将眼动时间序列的x和y像素坐标映射到预定义的连续兴趣区域。这种标记化使时空眼动数据可用于Chronos（一种基于T5架构的时间序列基础模型），并添加了共同注意力和/或交叉注意力来捕捉眼动的方向性和/或双眼间的影响。我们在多个数据集上将STARE与几种最先进的方法进行比较，目的是从眼动预测消费者选择行为。因此，我们迈出了开发和测试基于眼动神经生理学的视觉注意力动力学的深度学习架构的第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The present work proposes a Deep Learning architecture for the prediction ofvarious consumer choice behaviors from time series of raw gaze or eye fixationson images of the decision environment, for which currently no foundationalmodels are available. The architecture, called STARE (Spatio-Temporal AttentionRepresentation for Eye Tracking), uses a new tokenization strategy, whichinvolves mapping the x- and y- pixel coordinates of eye-movement time series onpredefined, contiguous Regions of Interest. That tokenization makes thespatio-temporal eye-movement data available to the Chronos, a time-seriesfoundation model based on the T5 architecture, to which co-attention and/orcross-attention is added to capture directional and/or interocular influencesof eye movements. We compare STARE with several state-of-the art alternativeson multiple datasets with the purpose of predicting consumer choice behaviorsfrom eye movements. We thus make a first step towards developing and testing DLarchitectures that represent visual attention dynamics rooted in theneurophysiology of eye movements.</description>
      <author>example@mail.com (Moshe Unger, Alexander Tuzhilin, Michel Wedel)</author>
      <guid isPermaLink="false">2508.04148v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving</title>
      <link>http://arxiv.org/abs/2508.04128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Neural Mixture of Brain Regional Experts (Neuro-MoBRE)的通用神经生理解码框架，旨在解决神经生理建模中普遍存在的数据异质性挑战。该框架结合脑区-时间嵌入机制和专家混合方法，有效处理来自不同脑区和任务的神经信号，并在多种任务上展现出优越的性能和强大的零样本解码泛化能力。&lt;h4&gt;背景&lt;/h4&gt;神经生理学解码对推进脑机接口(BCI)技术发展至关重要，已从深度学习进步中显著受益。然而，现有解码方法主要局限于单任务场景和单个受试者，限制了其应用性和泛化性。创建大规模神经生理基础模型的努力虽显示出前景，但由于受试者和任务间普遍存在的数据异质性，仍面临重大挑战。简单地增加模型参数和数据集大小而不明确解决异质性，无法复制自然语言处理中的扩展成功。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用解码框架，明确设计用于处理神经生理建模中普遍存在的数据异质性问题，提高解码方法的泛化能力和适用范围。&lt;h4&gt;方法&lt;/h4&gt;引入Neuro-MoBRE框架，包含：1)脑区-时间嵌入机制结合专家混合方法，将不同脑区的神经信号分配给专门区域专家；2)区域掩码自编码预训练策略，增强受试者间表征一致性；3)任务解缠结的信息聚合方法，有效处理特定任务的神经常态变化。&lt;h4&gt;主要发现&lt;/h4&gt;在来自11个受试者跨越五种不同任务（包括复杂语言解码和癫痫发作诊断）的颅内记录评估中，Neuro-MoBRE超越了先前方法，并在未见过的受试者上表现出强大的零样本解码泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Neuro-MoBRE框架通过明确处理神经生理数据中的异质性，显著提高了神经生理解码的性能和泛化能力，为脑机接口技术的发展提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;神经生理学解码对推进脑机接口(BCI)技术发展至关重要，并已从最近的深度学习进步中显著受益。然而，现有的解码方法主要仍局限于单任务场景和单个受试者，限制了它们更广泛的应用性和泛化性。创建大规模神经生理基础模型的努力显示出前景，但由于受试者和解码任务之间普遍存在的数据异质性，仍面临重大挑战。简单地增加模型参数和数据集大小而不明确解决这种异质性，无法复制在自然语言处理中看到的扩展成功。在此，我们引入了神经脑区专家混合模型(Neural Mixture of Brain Regional Experts, Neuro-MoBRE)，这是一种通用解码框架，明确设计用于处理神经生理建模中普遍存在的数据异质性。Neuro-MoBRE结合了脑区-时间嵌入机制和专家混合方法，将来自不同脑区的神经信号分配给统一的嵌入基础上的专门区域专家，从而明确解决结构和功能异质性。此外，我们的区域掩码自编码预训练策略进一步增强了受试者之间的表征一致性，辅以针对有效处理特定任务神经变异而设计的任务解缠结信息聚合方法。在来自11个受试者跨越五种不同任务的颅内记录评估中，包括复杂语言解码和癫痫发作诊断，结果表明Neuro-MoBRE超越了先前的方法，并在未见过的受试者上表现出强大的零样本解码泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neurophysiological decoding, fundamental to advancing brain-computerinterface (BCI) technologies, has significantly benefited from recent advancesin deep learning. However, existing decoding approaches largely remainconstrained to single-task scenarios and individual subjects, limiting theirbroader applicability and generalizability. Efforts towards creatinglarge-scale neurophysiological foundation models have shown promise, butcontinue to struggle with significant challenges due to pervasive dataheterogeneity across subjects and decoding tasks. Simply increasing modelparameters and dataset size without explicitly addressing this heterogeneityfails to replicate the scaling successes seen in natural language processing.Here, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),a general-purpose decoding framework explicitly designed to manage theubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBREincorporates a brain-regional-temporal embedding mechanism combined with amixture-of-experts approach, assigning neural signals from distinct brainregions to specialized regional experts on a unified embedding basis, thusexplicitly resolving both structural and functional heterogeneity.Additionally, our region-masked autoencoding pre-training strategy furtherenhances representational consistency among subjects, complemented by atask-disentangled information aggregation method tailored to effectively handletask-specific neural variations. Evaluations conducted on intracranialrecordings from 11 subjects across five diverse tasks, including complexlanguage decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBREsurpasses prior art and exhibits robust generalization for zero-shot decodingon unseen subjects.</description>
      <author>example@mail.com (Di Wu, Yifei Jia, Siyuan Li, Shiqi Zhao, Jie Yang, Mohamad Sawan)</author>
      <guid isPermaLink="false">2508.04128v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>WiFo-CF: Wireless Foundation Model for CSI Feedback</title>
      <link>http://arxiv.org/abs/2508.04068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为WiFo-CF的新型无线基础模型，专为CSI反馈设计，能够适应异构配置，通过多用户多速率自监督预训练策略和S-R MoE架构实现，在模拟和真实场景中表现优异，并能有效促进下游任务适应。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的CSI反馈方案具有强大压缩能力，但通常受限于固定系统配置，限制了泛化能力和灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决传统CSI反馈方案的局限性，提出一种能够适应异构配置(如变化的信道维度、反馈速率和数据分布)的统一框架。&lt;h4&gt;方法&lt;/h4&gt;WiFo-CF采用两大关键创新：(1)多用户、多速率自监督预训练策略；(2)共享和路由专家混合(S-R MoE)架构。同时利用首个异构信道反馈数据集支持大规模预训练。&lt;h4&gt;主要发现&lt;/h4&gt;WiFo-CF在模拟和真实场景中的分布内和分布外数据上实现了卓越性能，学习到的表示有效促进了基于CSI的室内定位等下游任务的适应。&lt;h4&gt;结论&lt;/h4&gt;WiFo-CF通过创新架构和预训练策略解决了传统CSI反馈方案的局限性，提供了更好的泛化能力和灵活性，验证了其可扩展性和部署潜力。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的信道状态信息(CSI)反馈方案展示了强大的压缩能力，但通常受限于固定系统配置，限制了它们的泛化和灵活性。为应对这一挑战，我们提出了WiFo-CF，一种专为CSI反馈设计的新型无线基础模型，通过其关键创新独特地适应异构配置，如变化的信道维度、反馈速率和数据分布，在一个统一框架内：(1)多用户、多速率自监督预训练策略；(2)共享和路由专家混合(S-R MoE)架构。支持WiFo-CF大规模预训练的是首个异构信道反馈数据集，其多样化的模式使模型能够在模拟和真实场景中的分布内和分布外数据上实现卓越性能。此外，学习到的表示有效促进了基于CSI的室内定位等下游任务的适应，验证了WiFo-CF的可扩展性和部署潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based channel state information (CSI) feedback schemesdemonstrate strong compression capabilities but are typically constrained tofixed system configurations, limiting their generalization and flexibility. Toaddress this challenge, WiFo-CF, a novel wireless foundation model tailored forCSI feedback, is proposed, uniquely accommodating heterogeneous configurationssuch as varying channel dimensions, feedback rates, and data distributionswithin a unified framework through its key innovations: (1) a multi-user,multi-rate self-supervised pre-training strategy; and (2) a Mixture of Sharedand Routed Expert (S-R MoE) architecture. Supporting the large-scalepre-training of WiFo-CF is the first heterogeneous channel feedback dataset,whose diverse patterns enable the model to achieve superior performance on bothin-distribution and out-of-distribution data across simulated and real-worldscenarios. Furthermore, the learned representations effectively facilitateadaptation to downstream tasks such as CSI-based indoor localization,validating WiFo-CF's scalability and deployment potential.</description>
      <author>example@mail.com (Liu Xuanyu, Gao Shijian, Liu Boxun, Cheng Xiang, Yang Liuqing)</author>
      <guid isPermaLink="false">2508.04068v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>VeriGUI: Verifiable Long-Chain GUI Dataset</title>
      <link>http://arxiv.org/abs/2508.04026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了VeriGUI，一个新型的可验证长链GUI数据集，用于促进在真实计算机环境中运行的通用GUI智能体的开发和评估。&lt;h4&gt;背景&lt;/h4&gt;最近的研究致力于构建能够执行复杂图形用户界面（GUI）任务的自主智能体，这些研究有潜力彻底改变人机交互。然而，现有工作主要集中在短期交互，并且仅依赖结果验证，限制了它们在真实世界GUI应用中的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;创建一个专门设计用于开发和评估能够处理长期任务分解和执行的通用GUI智能体的数据集。&lt;h4&gt;方法&lt;/h4&gt;VeriGUI数据集强调两个关键维度：1）长链复杂性，任务被分解为跨越数百步的相互依赖的子任务序列，设计允许任何子任务作为有效起点；2）子任务级别可验证性，支持在每个子任务内的多样化探索策略，同时确保每个子任务级别的目标保持可验证和一致。数据集包含桌面和Web上的GUI任务轨迹，由人类专家注释。&lt;h4&gt;主要发现&lt;/h4&gt;使用不同基础模型的多种智能体在VeriGUI上的广泛实验显示处理长期任务时存在显著性能差距，突显了GUI智能体需要更强大的规划和决策能力。&lt;h4&gt;结论&lt;/h4&gt;GUI智能体需要发展更强大的规划和决策能力来处理长期任务，而VeriGUI数据集为评估和开发这些智能体提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究致力于构建能够执行复杂图形用户界面（GUI）任务的自主智能体，有潜力彻底改变人机交互。尽管结果令人鼓舞，但现有工作主要集中在短期交互，并仅依赖结果验证，从而限制了它们在需要长期任务分解和执行的真实世界GUI应用中的可扩展性。在这项工作中，我们介绍了VeriGUI，一个新型的可验证长链GUI数据集，旨在促进在真实计算机环境中运行的通用GUI智能体的开发和评估。我们的数据集强调两个关键维度：（1）长链复杂性，任务被分解为跨越数百步的相互依赖的子任务序列，明确设计允许任何子任务作为有效起点；（2）子任务级别可验证性，使每个子任务内能够进行多样化的探索策略，同时确保每个子任务级别的目标保持可验证和一致。该数据集包含桌面和Web上的GUI任务轨迹，由人类专家注释。在VeriGUI上使用不同基础模型的多种智能体的广泛实验揭示了处理长期任务时的显著性能差距，突显了GUI智能体需要更强大的规划和决策能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have delved into constructing autonomous agents capable ofperforming complex Graphical User Interface (GUI)-based computer tasks, withthe potential to revolutionize human-computer interaction. Despite encouragingresults, existing efforts mainly focus on short-term interactions and rely onoutcome-only verification, thereby limiting their scalability in real-world GUIapplications that demand long-horizon task decomposition and execution. In thiswork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designedto facilitate the development and evaluation of generalist GUI agents operatingin realistic computer environments. Our dataset emphasizes two criticaldimensions: (1) long-chain complexity, with tasks decomposed into a sequence ofinterdependent subtasks spanning hundreds of steps, explicitly designed toallow any subtask to serve as a valid starting point; and (2) subtask-levelverifiability, which enables diverse exploration strategies within eachsubtask, while ensuring that each subtask-level goal remains verifiable andconsistent. The dataset consists of GUI task trajectories across both desktopand web, annotated by human experts. Extensive experiments on VeriGUI usingvarious agents with different foundation models reveal significant performancegaps in handling long-horizon tasks, highlighting the need for more robustplanning and decision-making capabilities in GUI agents.</description>
      <author>example@mail.com (Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Wendong Fan, Ge Zhang, Jiajun Shi, Weihao Xuan, Jiaxing Huang, Shuang Luo, Fang Wu, Heli Qi, Qingcheng Zeng, Ziqi Ren, Jialiang Gao, Jindi Lv, Junjie Wang, Aosong Feng, Heng Zhou, Wangchunshu Zhou, Zhenfei Yin, Wenlong Zhang, Guohao Li, Wenhao Yu, Irene Li, Lei Ma, Lei Bai, Qunshu Lin, Mingli Song, Dacheng Tao)</author>
      <guid isPermaLink="false">2508.04026v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models</title>
      <link>http://arxiv.org/abs/2508.03998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种社交机器人共同主持人系统，通过分析多模态会议数据为人类主持人提供微妙提示，帮助解决团体会议中的挑战。该系统使用代理概念瓶颈模型(CBM)进行推理，确保决策过程的透明度和可信度。&lt;h4&gt;背景&lt;/h4&gt;成功的团体会议需要促进个人目标设定和执行，同时加强团体内的社会关系。主持人需要能够察觉参与者的脱离、个人目标设定困难以及人际交往问题等需要干预的微妙动态。然而，主持人面临的挑战和认知负荷创造了对具身技术的需求，这种技术应能解读社会交流并意识到个体需求，而非仅依赖'黑盒'模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种社交机器人共同主持人，能够分析多模态会议数据，并向人类主持人提供微妙提示，帮助主持人在团体会议中更好地识别需要干预的情况。&lt;h4&gt;方法&lt;/h4&gt;机器人使用代理概念瓶颈模型(CBM)进行推理，该模型基于人类可解释的概念（如参与者的参与度和情感）做出决策。核心贡献是一种迁移学习框架，将基础模型(FM)的广泛社会理解提炼到专门的、透明的CBM中。&lt;h4&gt;主要发现&lt;/h4&gt;概念驱动的系统在预测干预需求方面明显优于直接零样本FMs，并支持实时推理修正。该模型能够泛化到不同团体，并成功将资深主持人的专业知识迁移以提高新手的性能。&lt;h4&gt;结论&lt;/h4&gt;通过将专家的认知模型转移到可解释的机器人伙伴中，这项工作为增强复杂社会领域中人类能力提供了强大的蓝图。&lt;h4&gt;翻译&lt;/h4&gt;成功的团体会议，如团体行为改变项目、工作会议和其他社会背景中实施的会议，必须促进个人目标设定和执行，同时加强团体内的社会关系。因此，理想的主持人必须对脱离参与、个人目标设定和执行困难以及人际交往问题等需要干预的微妙动态保持敏感。主持人面临的挑战和认知负荷创造了对具身技术的关键需求，这种技术能够解读社会交流，同时意识到团体中个体的需求，并提供超越仅识别社会线索的强大但'黑盒'基础模型(FMs)的透明建议。我们通过一个社交机器人共同主持人来解决这一重要需求，该机器人分析多模态会议数据并向主持人提供微妙提示。机器人的推理由代理概念瓶颈模型(CBM)提供动力，该模型基于人类可解释的概念（如参与者的参与度和情感）做出决策，确保透明度和可信度。我们的核心贡献是一种迁移学习框架，将FM的广泛社会理解提炼到我们专门的、透明的CBM中。这种概念驱动的系统在预测干预需求方面明显优于直接零样本FMs，并支持对其推理的实时人工修正。关键性地，我们展示了稳健的知识迁移：该模型能够泛化到不同团体，并成功将资深人类主持人的专业知识迁移以提高新手的性能。通过将专家的认知模型转移到可解释的机器人伙伴中，我们的工作为增强复杂社会领域中人类能力提供了强大的蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Successful group meetings, such as those implemented in groupbehavioral-change programs, work meetings, and other social contexts, mustpromote individual goal setting and execution while strengthening the socialrelationships within the group. Consequently, an ideal facilitator must besensitive to the subtle dynamics of disengagement, difficulties with individualgoal setting and execution, and interpersonal difficulties that signal a needfor intervention. The challenges and cognitive load experienced by facilitatorscreate a critical gap for an embodied technology that can interpret socialexchanges while remaining aware of the needs of the individuals in the groupand providing transparent recommendations that go beyond powerful but "blackbox" foundation models (FMs) that identify social cues. We address thisimportant demand with a social robot co-facilitator that analyzes multimodalmeeting data and provides discreet cues to the facilitator. The robot'sreasoning is powered by an agentic concept bottleneck model (CBM), which makesdecisions based on human-interpretable concepts like participant engagement andsentiments, ensuring transparency and trustworthiness. Our core contribution isa transfer learning framework that distills the broad social understanding ofan FM into our specialized and transparent CBM. This concept-driven systemsignificantly outperforms direct zero-shot FMs in predicting the need forintervention and enables real-time human correction of its reasoning.Critically, we demonstrate robust knowledge transfer: the model generalizesacross different groups and successfully transfers the expertise of seniorhuman facilitators to improve the performance of novices. By transferring anexpert's cognitive model into an interpretable robotic partner, our workprovides a powerful blueprint for augmenting human capabilities in complexsocial domains.</description>
      <author>example@mail.com (Xinyu Zhao, Zhen Tan, Maya Enisman, Minjae Seo, Marta R. Durantini, Dolores Albarracin, Tianlong Chen)</author>
      <guid isPermaLink="false">2508.03998v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation</title>
      <link>http://arxiv.org/abs/2508.03820v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  64 Pages, 9 Algorithms, 22 Theorems, 10 Lemmas, 2 Figures, 3 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Bernoulli-LoRA，一个统一并扩展现有LoRA方法的新理论框架，引入概率伯努利机制选择更新矩阵，分析了多种变体并证明其收敛性，通过实验验证了理论发现和方法有效性。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)是适应大型基础模型到特定任务的关键方法，随着模型规模指数级增长而变得尤为重要。在PEFT方法中，LoRA因其有效性和简单性而突出，但现有方法缺乏充分的理论理解。&lt;h4&gt;目的&lt;/h4&gt;引入Bernoulli-LoRA，一个新的理论框架，统一并扩展现有的LoRA方法，同时保持理论上的可处理性。&lt;h4&gt;方法&lt;/h4&gt;引入一个概率性的伯努利机制来选择要更新的矩阵，这种方法包含了并推广了各种现有的更新策略，同时保持了理论上的可处理性。&lt;h4&gt;主要发现&lt;/h4&gt;在非凸优化文献的标准假设下，分析了框架的多个变体，为每个变体建立了收敛保证。此外，还将分析扩展到凸非光滑函数，为常数步长和自适应步长提供了收敛率。&lt;h4&gt;结论&lt;/h4&gt;通过各种任务的广泛实验验证了理论发现，并证明了该方法的有效性。这项工作是开发有理论基础且在实践中有效的PEFT方法的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)已成为将大型基础模型适应到特定任务的关键方法，尤其随着模型规模继续呈指数级增长时。在PEFT方法中，低秩适应(LoRA)因其有效性和简单性而脱颖而出，将适应表示为两个低秩矩阵的乘积。尽管大量实证研究证明了LoRA的实际效用，但对这类方法的理论理解仍然有限。最近关于RAC-LoRA的研究(arXiv:2410.08305)为严格分析迈出了初步步伐。在这项工作中，我们介绍了Bernoulli-LoRA，这是一个统一并扩展现有LoRA方法的新理论框架。我们的方法引入了一个概率性的伯努利机制来选择要更新的矩阵。这种方法包含了并推广了各种现有的更新策略，同时保持了理论上的可处理性。在非凸优化文献的标准假设下，我们分析了框架的几种变体：Bernoulli-LoRA-GD、Bernoulli-LoRA-SGD、Bernoulli-LoRA-PAGE、Bernoulli-LoRA-MVR、Bernoulli-LoRA-QGD、Bernoulli-LoRA-MARINA和Bernoulli-LoRA-EF21，为每个变体建立了收敛保证。此外，我们将分析扩展到凸非光滑函数，为常数步长和自适应(Polyak-type)步长提供了收敛率。通过各种任务的广泛实验，我们验证了理论发现并证明了我们方法的有效性。这项工作是开发有理论基础且在实践中有效的PEFT方法的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach foradapting large foundational models to specific tasks, particularly as modelsizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,expressing adaptations as a product of two low-rank matrices. While extensiveempirical studies demonstrate LoRA's practical utility, theoreticalunderstanding of such methods remains limited. Recent work on RAC-LoRA(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,we introduce Bernoulli-LoRA, a novel theoretical framework that unifies andextends existing LoRA approaches. Our method introduces a probabilisticBernoulli mechanism for selecting which matrix to update. This approachencompasses and generalizes various existing update strategies whilemaintaining theoretical tractability. Under standard assumptions fromnon-convex optimization literature, we analyze several variants of ourframework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, andBernoulli-LoRA-EF21, establishing convergence guarantees for each variant.Additionally, we extend our analysis to convex non-smooth functions, providingconvergence rates for both constant and adaptive (Polyak-type) stepsizes.Through extensive experiments on various tasks, we validate our theoreticalfindings and demonstrate the practical efficacy of our approach. This work is astep toward developing theoretically grounded yet practically effective PEFTmethods.</description>
      <author>example@mail.com (Igor Sokolov, Abdurakhmon Sadiev, Yury Demidovich, Fawaz S Al-Qahtani, Peter Richtárik)</author>
      <guid isPermaLink="false">2508.03820v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons</title>
      <link>http://arxiv.org/abs/2508.03785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 7 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SoilNet，一种多模态多任务模型，用于解决土壤层分类这一具有挑战性的问题。该方法通过整合图像数据和地理时空元数据，采用结构化模块化管道，能够处理复杂的层次标签结构，对土壤健康监测具有重要意义。&lt;h4&gt;背景&lt;/h4&gt;基础模型虽在多领域取得进展，但经验科学中的某些问题如土壤层分类仍难以受益。土壤层分类因其多模态、多任务特性和复杂的层次结构标签分类法而具有挑战性。准确的土壤层分类对监测土壤健康至关重要，直接影响农业生产力、粮食安全、生态系统稳定性和气候韧性。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态多任务模型来解决土壤层分类问题，特别是处理其复杂的层次结构标签特性。&lt;h4&gt;方法&lt;/h4&gt;提出了SoilNet模型，通过结构化模块化管道解决问题：1)整合图像数据和地理时空元数据；2)预测深度标记将土壤剖面分割成层候选；3)为每个segment提取层特定形态特征；4)基于多模态连接特征向量预测层标签；5)利用基于图的标签表示处理土壤层间的复杂层次关系。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界土壤剖面数据集上验证了所提方法的有效性，能够处理标签数量大、不平衡且结构复杂的层次分类问题。&lt;h4&gt;结论&lt;/h4&gt;SoilNet为土壤层分类提供了有效解决方案，所有代码和实验已开源在GitHub仓库中。&lt;h4&gt;翻译&lt;/h4&gt;尽管基础模型最近的进展已在许多领域提高了最先进水平，但经验科学中的一些问题尚未能从这一进展中受益。例如，土壤层分类仍然具有挑战性，因为它具有多模态、多任务特性以及复杂的层次结构标签分类法。准确的土壤层分类对监测土壤健康至关重要，直接影响农业生产力、粮食安全、生态系统稳定性和气候韧性。在这项工作中，我们提出了SoilNet——一种多模态多任务模型，通过结构化模块化管道解决这个问题。我们的方法整合图像数据和地理时空元数据，首先预测深度标记，将土壤剖面分割成层候选。每个segment由一组特定于层的形态特征表征。最后，基于多模态连接的特征向量预测层标签，利用基于图的标签表示来考虑土壤层之间的复杂层次关系。我们的方法旨在解决复杂的层次分类问题，其中可能的标签数量非常大、不平衡且结构复杂。我们在真实世界土壤剖面数据集上证明了我们方法的有效性。所有代码和实验都可以在我们的GitHub仓库中找到：https://github.com/calgo-lab/BGR/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advances in foundation models have improved the state of the artin many domains, some problems in empirical sciences could not benefit fromthis progress yet. Soil horizon classification, for instance, remainschallenging because of its multimodal and multitask characteristics and acomplex hierarchically structured label taxonomy. Accurate classification ofsoil horizons is crucial for monitoring soil health, which directly impactsagricultural productivity, food security, ecosystem stability and climateresilience. In this work, we propose $\textit{SoilNet}$ - a multimodalmultitask model to tackle this problem through a structured modularizedpipeline. Our approach integrates image data and geotemporal metadata to firstpredict depth markers, segmenting the soil profile into horizon candidates.Each segment is characterized by a set of horizon-specific morphologicalfeatures. Finally, horizon labels are predicted based on the multimodalconcatenated feature vector, leveraging a graph-based label representation toaccount for the complex hierarchical relationships among soil horizons. Ourmethod is designed to address complex hierarchical classification, where thenumber of possible labels is very large, imbalanced and non-triviallystructured. We demonstrate the effectiveness of our approach on a real-worldsoil profile dataset. All code and experiments can be found in our repository:https://github.com/calgo-lab/BGR/</description>
      <author>example@mail.com (Teodor Chiaburu, Vipin Singh, Frank Haußer, Felix Bießmann)</author>
      <guid isPermaLink="false">2508.03785v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2508.04059v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了O-Bench，首个专门针对遮挡感知的视觉问答基准测试，通过分层合成方法创建了1,365张图像和4,588个问答对，评估显示当前多模态大语言模型在遮挡感知方面与人类表现存在显著差距，并识别出三种典型失败模式。&lt;h4&gt;背景&lt;/h4&gt;遮挡感知是人类水平空间理解的关键基础，它整合了视觉识别和推理的挑战。尽管多模态大语言模型已显示出显著能力，但它们在遮挡感知方面的表现尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入O-Bench，第一个专门为遮挡感知设计的视觉问答基准测试，以解决当前多模态大语言模型在遮挡感知方面表现不足的问题。&lt;h4&gt;方法&lt;/h4&gt;基于SA-1B数据集，通过新颖的分层合成方法构建1,365张具有语义连贯遮挡场景的图像，并采用可靠、半自动的工作流程标注了涵盖五个定制任务的4,588个问答对，随后对22个代表性多模态大语言模型进行了广泛评估并与人类基线进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;当前多模态大语言模型与人类之间存在显著性能差距，这种差距不能通过模型扩展或思考过程充分弥补。研究识别出三种典型失败模式：过于保守的偏见、脆弱的格式塔预测以及在定量任务上的挣扎。&lt;h4&gt;结论&lt;/h4&gt;O-Bench不仅可以为遮挡感知提供重要的评估工具，还能启发多模态大语言模型的发展以实现更好的视觉智能，该基准测试将在论文发表后公开提供。&lt;h4&gt;翻译&lt;/h4&gt;遮挡感知，作为人类水平空间理解的关键基础，体现了整合视觉识别和推理的挑战。尽管多模态大语言模型已显示出显著能力，但它们在遮挡感知方面的表现尚未被充分探索。为解决这一差距，我们引入了O-Bench，这是第一个专门为遮挡感知设计的视觉问答基准测试。基于SA-1B，我们通过一种新颖的分层合成方法构建了1,365张具有语义连贯遮挡场景的图像。在此基础上，我们采用可靠、半自动的工作流程，总共标注了涵盖五个定制任务的4,588个问答对。我们对22个代表性多模态大语言模型进行的广泛评估，与人类基线相比，揭示了当前多模态大语言模型与人类之间的显著性能差距，我们发现这种差距不能通过模型扩展或思考过程充分弥补。我们进一步识别出三种典型的失败模式，包括过于保守的偏见、脆弱的格式塔预测以及在定量任务上的挣扎。我们相信O-Bench不仅可以为遮挡感知提供重要的评估工具，还能启发多模态大语言模型的发展以实现更好的视觉智能。我们的基准测试将在论文发表后公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occlusion perception, a critical foundation for human-level spatialunderstanding, embodies the challenge of integrating visual recognition andreasoning. Though multimodal large language models (MLLMs) have demonstratedremarkable capabilities, their performance on occlusion perception remainsunder-explored. To address this gap, we introduce O-Bench, the first visualquestion answering (VQA) benchmark specifically designed for occlusionperception. Based on SA-1B, we construct 1,365 images featuring semanticallycoherent occlusion scenarios through a novel layered synthesis approach. Uponthis foundation, we annotate 4,588 question-answer pairs in total across fivetailored tasks, employing a reliable, semi-automatic workflow. Our extensiveevaluation of 22 representative MLLMs against the human baseline reveals asignificant performance gap between current MLLMs and humans, which, we find,cannot be sufficiently bridged by model scaling or thinking process. We furtheridentify three typical failure patterns, including an overly conservative bias,a fragile gestalt prediction, and a struggle with quantitative tasks. Webelieve O-Bench can not only provide a vital evaluation tool for occlusionperception, but also inspire the development of MLLMs for better visualintelligence. Our benchmark will be made publicly available upon paperpublication.</description>
      <author>example@mail.com (Zhaochen Liu, Kaiwen Gao, Shuyi Liang, Bin Xiao, Limeng Qiao, Lin Ma, Tingting Jiang)</author>
      <guid isPermaLink="false">2508.04059v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.03060v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CHARM的新型互补学习框架，用于模态不可知语义分割，旨在实现跨模态内容的协同调和而非同质化，从而保留各模态的独特优势并实现互补融合。&lt;h4&gt;背景&lt;/h4&gt;现有的模态不可知语义分割方法通常依赖于显式特征对齐来实现模态同质化，这会稀释各模态的独特优势并破坏它们固有的互补性。&lt;h4&gt;目的&lt;/h4&gt;实现协同调和而非同质化，通过隐式对齐内容同时保留模态特定优势，从而实现跨模态互补，达到真正的多样性和谐。&lt;h4&gt;方法&lt;/h4&gt;CHARM框架包含两个主要组件：1）互感单元（MPU），通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文；2）双路径优化策略，将训练解耦为互补融合学习的协作学习策略（CoL）和保护模态特定优化的个体增强策略（InE）。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，并且在脆弱模态上有显著提升。&lt;h4&gt;结论&lt;/h4&gt;这项工作将重点从模型同质化转向调和，实现了跨模态互补，达到真正的多样性和谐。&lt;h4&gt;翻译&lt;/h4&gt;模态不可知语义分割（MaSS）旨在实现跨任意输入模态组合的鲁棒场景理解。现有方法通常依赖于显式特征对齐来实现模态同质化，这稀释了各模态的独特优势并破坏了它们固有的互补性。为了实现协同调和而非同质化，我们提出了CHARM，一种新型互补学习框架，通过两个组件隐式对齐内容同时保留模态特定优势：（1）互感单元（MPU），通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文，发现模态交互对应关系；（2）双路径优化策略，将训练解耦为互补融合学习的协作学习策略（CoL）和保护模态特定优化的个体增强策略（InE）。在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，并且在脆弱模态上有显著提升。这项工作将重点从模型同质化转向调和，实现了跨模态互补，达到真正的多样性和谐。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决模态无关语义分割（MaSS）中的模态同质化问题。现有方法通过显式特征对齐实现模态一致性，但这会稀释各模态的独特优势，破坏它们固有的互补性。这个问题在现实中很重要，因为不同传感器（如RGB、LiDAR、深度相机等）在不同环境条件下各有优势（如雨天RGB可能退化而LiDAR仍有效），当某些传感器不可用或降级时，系统需要利用可用模态组合保持鲁棒性能，而现有方法无法充分利用模态间的互补信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有MaSS方法的局限性：显式特征对齐虽然能处理稳健模态组合，但会抑制脆弱模态特征并减弱模态间互补性。他们提出从'同质化'转向'和谐化'的范式，强调模态间的互补合作而非一致性。设计上借鉴了多模态语义分割领域的特征融合和注意力机制，特别是改进了Transformer中的自注意力机制以支持跨模态交互（MPU模块），并受特征对齐思想启发设计了双路径优化策略（CoL和InE），但将它们解耦并协同工作，实现模态互补与个体增强的平衡。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是实现'协作和谐化'而非'同质化'，通过互感单元（MPU）实现模态间隐式对齐，同时采用双路径优化策略平衡协作学习和个体增强。整体流程为：1)共享权重编码器独立提取各模态特征；2)双路径处理：协作学习路径（CoL）通过鲁棒性评估加权融合并经MPU跨模态交互，个体增强路径（InE）为脆弱模态提供保护性学习空间；3)两条路径特征输入分割头并行预测；4)使用交叉熵损失进行联合优化；5)MPU通过窗口式跨模态注意力使各模态同时作为查询和上下文，实现隐式对齐。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有：1)从同质化到和谐化的范式转变，保留各模态独特优势同时实现互补；2)互感单元（MPU）实现隐式模态对齐，无需显式约束；3)双路径优化策略（CoL促进模态互补，InE保护脆弱模态）。相比之前工作：区别于显式对齐方法（如MAGIC、Any2Seg）不使用KL散度等约束强制特征一致性；区别于传统多模态分割方法不依赖预设主-从模态关系；区别于模态丢弃策略不仅模拟缺失场景还主动增强脆弱模态表现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CHARM通过引入互感单元和双路径优化策略，实现了跨模态的协作和谐化而非同质化，在保留各模态独特优势的同时显著提升了模态无关语义分割的性能，特别是在脆弱模态组合上的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust sceneunderstanding across arbitrary combinations of input modality. Existing methodstypically rely on explicit feature alignment to achieve modal homogenization,which dilutes the distinctive strengths of each modality and destroys theirinherent complementarity. To achieve cooperative harmonization rather thanhomogenization, we propose CHARM, a novel complementary learning frameworkdesigned to implicitly align content while preserving modality-specificadvantages through two components: (1) Mutual Perception Unit (MPU), enablingimplicit alignment through window-based cross-modal interaction, wheremodalities serve as both queries and contexts for each other to discovermodality-interactive correspondences; (2) A dual-path optimization strategythat decouples training into Collaborative Learning Strategy (CoL) forcomplementary fusion learning and Individual Enhancement Strategy (InE) forprotected modality-specific optimization. Experiments across multiple datasetsand backbones indicate that CHARM consistently outperform the baselines, withsignificant increment on the fragile modalities. This work shifts the focusfrom model homogenization to harmonization, enabling cross-modalcomplementarity for true harmony in diversity.</description>
      <author>example@mail.com (Lekang Wen, Jing Xiao, Liang Liao, Jiajun Chen, Mi Wang)</author>
      <guid isPermaLink="false">2508.03060v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding</title>
      <link>http://arxiv.org/abs/2508.03725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LLM4-IC8K的新型框架，用于从IC机械图纸自动进行封装几何标记。该方法将IC机械图纸视为图像，利用大型语言模型进行结构化几何解释，通过两阶段训练和专门的数据集支持，解决了当前大型多模态模型在几何感知方面的不准确问题。&lt;h4&gt;背景&lt;/h4&gt;集成电路的印刷电路板引脚几何形状标记对于定义组件与PCB布局之间的物理接口至关重要。然而，由于引脚绘制结构化和抽象图表标注的存在，自动解析和精确的几何建模仍然极具挑战性。目前尚无直接从IC机械图纸自动进行封装几何标记的方法。&lt;h4&gt;目的&lt;/h4&gt;研究大型多模态模型在解决IC引脚几何理解方面的视觉感知性能，解决当前LMMs在几何感知不准确的问题，并提出一个有效的框架来自动处理IC封装几何标记。&lt;h4&gt;方法&lt;/h4&gt;提出LLM4-IC8K框架，将IC机械图纸视为图像，利用LLMs进行结构化几何解释。模仿工程师的逐步推理方法，解决三个子任务：感知引脚数量、计算每个引脚的中心坐标、估计单个引脚的尺寸。采用两阶段框架：首先在合成的IC引脚图上训练LMMs学习基本几何推理，然后在真实数据图纸上进行微调。引入了ICGeo8K数据集，包含8,608个标记样本。&lt;h4&gt;主要发现&lt;/h4&gt;当前LMMs在几何感知方面存在严重的不准确性，这阻碍了它们在解决引脚几何标记问题上的性能。通过实验证明，提出的模型在提出的基准测试中优于最先进的LMMs。&lt;h4&gt;结论&lt;/h4&gt;LLM4-IC8K框架能够有效解决IC封装几何标记问题。通过两阶段训练和专门的数据集支持，该方法提高了模型在实际场景中的鲁棒性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;印刷电路板集成电路的引脚几何形状标记对于定义组件与PCB布局之间的物理接口至关重要，需要卓越的视觉感知能力。然而，由于引脚绘制结构化和抽象图表标注的存在，自动解析和精确的引脚几何建模仍然极具挑战性。尽管其重要性，但目前尚无直接从IC机械图纸自动进行封装几何标记的方法。在本文中，我们首先研究了大型多模态模型在解决IC引脚几何理解时的视觉感知性能。我们的发现表明，当前的LMMs在几何感知方面存在严重的不准确性，这阻碍了它们在解决引脚几何标记问题上的性能。为解决这些局限性，我们提出了LLM4-IC8K，一种将IC机械图纸视为图像并利用LLMs进行结构化几何解释的新型框架。为模仿工程师使用的逐步推理方法，LLM4-IC8K解决了三个子任务：感知引脚数量、计算每个引脚的中心坐标和估计单个引脚的尺寸。我们提出了一个两阶段框架，首先在合成的IC引脚图上训练LMMs学习基本几何推理，然后在真实数据图纸上进行微调，以提高实际场景中的鲁棒性和准确性。为此，我们引入了ICGeo8K，一个包含8,608个标记样本的多模态数据集，包括4,138个手工制作的IC引脚样本和4,470个合成生成的样本。大量实验证明，我们的模型在提出的基准测试中优于最先进的LMMs。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Printed-Circuit-board (PCB) footprint geometry labeling of integratedcircuits (IC) is essential in defining the physical interface betweencomponents and the PCB layout, requiring exceptional visual perceptionproficiency. However, due to the unstructured footprint drawing and abstractdiagram annotations, automated parsing and accurate footprint geometry modelingremain highly challenging. Despite its importance, no methods currently existfor automated package geometry labeling directly from IC mechanical drawings.In this paper, we first investigate the visual perception performance of LargeMultimodal Models (LMMs) when solving IC footprint geometry understanding. Ourfindings reveal that current LMMs severely suffer from inaccurate geometricperception, which hinders their performance in solving the footprint geometrylabeling problem. To address these limitations, we propose LLM4-IC8K, a novelframework that treats IC mechanical drawings as images and leverages LLMs forstructured geometric interpretation. To mimic the step-by-step reasoningapproach used by human engineers, LLM4-IC8K addresses three sub-tasks:perceiving the number of pins, computing the center coordinates of each pin,and estimating the dimensions of individual pins. We present a two-stageframework that first trains LMMs on synthetically generated IC footprintdiagrams to learn fundamental geometric reasoning and then fine-tunes them onreal-world datasheet drawings to enhance robustness and accuracy in practicalscenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with8,608 labeled samples, including 4138 hand-crafted IC footprint samples and4470 synthetically generated samples. Extensive experiments demonstrate thatour model outperforms state-of-the-art LMMs on the proposed benchmark.</description>
      <author>example@mail.com (Yida Wang, Taiting Lu, Runze Liu, Lanqing Yang, Yifan Yang, Zhe Chen, Yuehai Wang, Yixin Liu, Kaiyuan Lin, Xiaomeng Chen, Dian Ding, Yijie Li, Yi-Chao Chen, Yincheng Jin, Mahanth Gowda)</author>
      <guid isPermaLink="false">2508.03725v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning</title>
      <link>http://arxiv.org/abs/2508.04549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ACMMM2025 (Dataset track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种面向海洋对象的两阶段视频字幕生成流程，解决了海洋视频理解中的挑战，并发布了相关数据集和代码。&lt;h4&gt;背景&lt;/h4&gt;海洋视频因海洋物体动态变化、相机运动和水下场景复杂性而给视频理解带来显著挑战。现有视频字幕数据集通常专注于通用或以人为中心的领域，难以推广到海洋环境的复杂性中，也无法有效分析海洋生物。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频字幕方法在海洋环境中的局限性，提出专门针对海洋视频理解的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种全面的视频理解基准，利用视频、文本和分割掩码的三元组来促进视觉定位和字幕生成。同时采用视频分割技术检测场景变化中的显著物体转换，以丰富字幕内容的语义。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法改善了海洋视频理解和分析能力，促进了海洋视频生成的发展，视频分割技术对检测场景变化中的显著物体转换非常有效。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的基准和方法论，有效提升了海洋视频理解的质量，相关数据集和代码已在https://msc.hkustvgd.com上发布。&lt;h4&gt;翻译&lt;/h4&gt;海洋视频由于海洋物体和周围环境的动态变化、相机运动以及水下场景的复杂性，给视频理解带来了重大挑战。现有的视频字幕数据集通常专注于通用或以人为中心的领域，往往无法推广到海洋环境的复杂性中，也无法获得关于海洋生活的洞察。为解决这些局限性，我们提出了一种两阶段的面向海洋对象的视频字幕生成流程。我们引入了一个全面的视频理解基准，利用视频、文本和分割掩码的三元组来促进视觉定位和字幕生成，从而改善海洋视频理解和分析，以及海洋视频生成。此外，我们强调了视频分割在检测场景变化中显著物体转换方面的有效性，这显著丰富了字幕内容的语义。我们的数据集和代码已在https://msc.hkustvgd.com上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758198&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine videos present significant challenges for video understanding due tothe dynamics of marine objects and the surrounding environment, camera motion,and the complexity of underwater scenes. Existing video captioning datasets,typically focused on generic or human-centric domains, often fail to generalizeto the complexities of the marine environment and gain insights about marinelife. To address these limitations, we propose a two-stage marineobject-oriented video captioning pipeline. We introduce a comprehensive videounderstanding benchmark that leverages the triplets of video, text, andsegmentation masks to facilitate visual grounding and captioning, leading toimproved marine video understanding and analysis, and marine video generation.Additionally, we highlight the effectiveness of video splitting in order todetect salient object transitions in scene changes, which significantly enrichthe semantics of captioning content. Our dataset and code have been released athttps://msc.hkustvgd.com.</description>
      <author>example@mail.com (Quang-Trung Truong, Yuk-Kwan Wong, Vo Hoang Kim Tuyen Dang, Rinaldi Gotama, Duc Thanh Nguyen, Sai-Kit Yeung)</author>
      <guid isPermaLink="false">2508.04549v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Quantum and Classical Sequential Models for Urban Telecommunication Forecasting</title>
      <link>http://arxiv.org/abs/2508.04488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了经典和量子启发的序列模型在预测米兰电信数据集中的单变量SMS活动时间序列的性能，比较了LSTM和四种量子模型在不同序列长度下的表现，发现量子增强并非普遍有利，其效果依赖于具体任务和架构设计。&lt;h4&gt;背景&lt;/h4&gt;研究使用米兰电信活动数据集，但由于数据完整性限制，仅关注每个空间网格单元的SMS-in信号。时间序列预测是电信网络管理和资源分配的重要任务。&lt;h4&gt;目的&lt;/h4&gt;评估不同序列模型（包括经典LSTM和量子启发模型）在预测SMS活动方面的性能，并研究序列长度对模型表现的影响，以及量子增强的有效性条件。&lt;h4&gt;方法&lt;/h4&gt;比较了五种模型：LSTM（基线）、Quantum LSTM (QLSTM)、Quantum Adaptive Self-Attention (QASA)、Quantum Receptance Weighted Key-Value (QRWKV)和Quantum Fast Weight Programmers (QFWP)。实验在不同输入序列长度（4, 8, 12, 16, 32和64）下进行，所有模型都基于历史值预测下一个10分钟的SMS-in值。&lt;h4&gt;主要发现&lt;/h4&gt;不同模型对序列长度表现出不同的敏感性，量子增强并非在所有情况下都优于经典模型，量子模块的有效性高度依赖于特定任务和架构设计，反映了模型大小、参数化策略和时间建模能力之间的权衡关系。&lt;h4&gt;结论&lt;/h4&gt;量子增强序列模型的效果不是普遍的，而是与具体应用场景和模型架构密切相关。在选择量子增强模型时，需要考虑任务特性、序列长度需求和模型复杂度之间的平衡。&lt;h4&gt;翻译&lt;/h4&gt;在本研究中，我们评估了经典和量子启发的序列模型在预测米兰电信活动数据集中的单变量传入短信活动（SMS-in）时间序列的性能。由于数据完整性限制，我们仅专注于每个空间网格单元的SMS-in信号。我们在不同的输入序列长度（4、8、12、16、32和64）下比较了五种模型：LSTM（基线）、量子LSTM（QLSTM）、量子自适应自注意力（QASA）、量子感受加权键值（QRWKV）和量子快速权重编程器（QFWP）。所有模型都经过训练，仅基于给定序列窗口内的历史值来预测下一个10分钟的SMS-in值。我们的研究结果表明，不同模型对序列长度表现出不同的敏感性，这表明量子增强并非普遍有利。相反，量子模块的有效性高度依赖于特定任务和架构设计，反映了模型大小、参数化策略和时间建模能力之间的固有权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we evaluate the performance of classical and quantum-inspiredsequential models in forecasting univariate time series of incoming SMSactivity (SMS-in) using the Milan Telecommunication Activity Dataset. Due todata completeness limitations, we focus exclusively on the SMS-in signal foreach spatial grid cell. We compare five models, LSTM (baseline), Quantum LSTM(QLSTM), Quantum Adaptive Self-Attention (QASA), Quantum Receptance WeightedKey-Value (QRWKV), and Quantum Fast Weight Programmers (QFWP), under varyinginput sequence lengths (4, 8, 12, 16, 32 and 64). All models are trained topredict the next 10-minute SMS-in value based solely on historical valueswithin a given sequence window. Our findings indicate that different modelsexhibit varying sensitivities to sequence length, suggesting that quantumenhancements are not universally advantageous. Rather, the effectiveness ofquantum modules is highly dependent on the specific task and architecturaldesign, reflecting inherent trade-offs among model size, parameterizationstrategies, and temporal modeling capabilities.</description>
      <author>example@mail.com (Chi-Sheng Chen, Samuel Yen-Chi Chen, Yun-Cheng Tsai)</author>
      <guid isPermaLink="false">2508.04488v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning</title>
      <link>http://arxiv.org/abs/2508.04416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VITAL框架，通过工具增强学习提升多模态大语言模型的长视频推理能力，解决了现有方法中跨模态交互有限和幻觉增加的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)的视频推理能力对视频问答和时间定位等下游任务至关重要，但现有基于文本的思维链推理方法存在跨模态交互有限和幻觉增加的问题，特别是在处理较长视频时。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频推理方法中的跨模态交互有限和幻觉增加问题，提升模型在长视频场景下的推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出Video Intelligence via Tool-Augmented Learning (VITAL)框架，使用视觉工具箱按需密集采样视频帧并生成多模态思维链；构建MTVR-CoT-72k和MTVR-RL-110k两个多任务数据集；提出Difficulty-aware Group Relative Policy Optimization算法(DGRPO)缓解多任务强化学习中的难度不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;时间定位和问答任务对视频理解具有相互促进作用。&lt;h4&gt;结论&lt;/h4&gt;在11个视频理解基准测试上，VITAL展现出先进的推理能力，在视频问答和时间定位任务上优于现有方法，特别是在长视频场景中。所有代码、数据和模型权重将公开提供。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的视频推理能力对于视频问答和时间定位等下游任务至关重要。虽然最近的方法已经探索了基于文本的思维链(CoT)推理用于MLLMs，但这些方法通常受到跨模态交互有限和幻觉增加的限制，特别是在处理较长视频或推理链时。为了解决这些挑战，我们提出了通过工具增强学习的视频智能(VITAL)，这是一种新颖的端到端智能体视频推理框架。通过视觉工具箱，模型可以按需密集采样新的视频帧，并生成多模态思维链以进行精确的长视频推理。我们观察到时间定位和问答对视频理解任务有相互促进作用。因此，我们构建了两个高质量的多任务视频推理数据集MTVR-CoT-72k用于监督微调，MTVR-RL-110k用于强化学习。此外，我们提出了一个难度感知的组相对策略优化算法(DGRPO)来缓解多任务强化学习中的难度不平衡问题。在11个具有挑战性的视频理解基准上的广泛实验证明了VITAL的先进推理能力，在视频问答和时间定位任务上优于现有方法，特别是在长视频场景中。所有代码、数据和模型权重都将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The video reasoning ability of multimodal large language models (MLLMs) iscrucial for downstream tasks like video question answering and temporalgrounding. While recent approaches have explored text-based chain-of-thought(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modalinteraction and increased hallucination, especially with longer videos orreasoning chains. To address these challenges, we propose Video Intelligencevia Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoningframework. With a visual toolbox, the model can densely sample new video frameson demand and generate multimodal CoT for precise long video reasoning. Weobserve that temporal grounding and question answering are mutually beneficialfor video understanding tasks. Therefore, we construct two high-qualitymulti-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning andMTVR-RL-110k for reinforcement learning. Moreover, we propose aDifficulty-aware Group Relative Policy Optimization algorithm (DGRPO) tomitigate difficulty imbalance in multi-task reinforcement learning. Extensiveexperiments on 11 challenging video understanding benchmarks demonstrate theadvanced reasoning ability of VITAL, outperforming existing methods in videoquestion answering and temporal grounding tasks, especially in long videoscenarios. All code, data and model weight will be made publicly available.</description>
      <author>example@mail.com (Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, Yansong Tang)</author>
      <guid isPermaLink="false">2508.04416v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding</title>
      <link>http://arxiv.org/abs/2508.04369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为时间采样策略优化(TSPO)的新方法，通过强化学习提升多模态大语言模型(MLLMs)对长时间视频的理解能力。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉语言任务中已显示出显著进展，但在处理长时间视频输入时仍面临挑战。这种限制源于MLLMs的上下文限制和训练成本，需要在将视频输入MLLMs之前进行稀疏帧采样。现有的视频MLLMs采用无训练的均匀采样或关键帧搜索，可能会错过关键事件或受预训练模型事件理解能力的限制。同时，由于稀疏帧采样的无监督和非可微分性质，构建基于训练的方法仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频MLLMs在处理长时间视频时的局限性，提出一种通过强化学习提升MLLMs长时间视频语言理解能力的方法。&lt;h4&gt;方法&lt;/h4&gt;提出时间采样策略优化(TSPO)方法：1. 提出可训练的事件感知时间智能体，捕捉事件-查询相关性以执行概率性关键帧选择；2. 提出TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现基于规则奖励的高效端到端组相对优化；3. 为TSPO训练提出包含全面时间数据和视频Needle-in-a-Haystack数据的长时间视频训练数据构建管道；4. 整合基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。&lt;h4&gt;主要发现&lt;/h4&gt;全面的实验表明，TSPO在多个长时间视频理解基准测试中取得了最先进的性能，并且在不同的前沿视频-MLLMs中显示出可转移的能力。&lt;h4&gt;结论&lt;/h4&gt;TSPO方法有效解决了MLLMs处理长时间视频输入的挑战，通过强化学习和事件感知的时间采样策略，显著提升了模型对长时间视频的理解能力，并且该方法具有很好的通用性和可转移性。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉语言任务中已显示出显著进展，但在处理长时间视频输入时仍面临挑战。这种限制源于MLLMs的上下文限制和训练成本，需要在将视频输入MLLMs之前进行稀疏帧采样。现有的视频MLLMs采用无训练的均匀采样或关键帧搜索，可能会错过关键事件或受预训练模型事件理解能力的限制。同时，由于稀疏帧采样的无监督和非可微分性质，构建基于训练的方法仍然具有挑战性。为解决这些问题，我们提出时间采样策略优化(TSPO)，通过强化学习提升MLLMs的长时间视频语言理解能力。具体而言，我们首先提出一种可训练的事件感知时间智能体，它捕捉事件-查询相关性以执行概率性关键帧选择。然后，我们提出TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现基于规则奖励的高效端到端组相对优化。此外，为TSPO的训练，我们提出了一种包含全面时间数据和视频Needle-in-a-Haystack数据的长时间视频训练数据构建管道。最后，我们整合基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。全面的实验表明，我们的TSPO在多个长时间视频理解基准测试中取得了最先进的性能，并且在不同的前沿视频-MLLMs中显示出可转移的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in vision-language tasks, yet they still face challenges whenprocessing long-duration video inputs. The limitation arises from MLLMs'context limit and training costs, necessitating sparse frame sampling beforefeeding videos into MLLMs. Existing video MLLMs adopt training-free uniformsampling or keyframe search, which may miss critical events or be constrainedby the pre-trained models' event understanding capabilities. Meanwhile,building a training-based method remains challenging due to the unsupervisedand non-differentiable nature of sparse frame sampling. To address theseproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancingMLLMs' long-form video-language understanding via reinforcement learning.Specifically, we first propose a trainable event-aware temporal agent, whichcaptures event-query correlation for performing probabilistic keyframeselection. Then, we propose the TSPO reinforcement learning paradigm, whichmodels keyframe selection and language generation as a joint decision-makingprocess, enabling end-to-end group relative optimization with efficientrule-based rewards. Furthermore, for the TSPO's training, we propose a longvideo training data construction pipeline with comprehensive temporal data andvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answeringaccuracy and temporal locating reward mechanisms to optimize the temporalsampling policy. Comprehensive experiments show that our TSPO achievesstate-of-the-art performance across multiple long video understandingbenchmarks, and shows transferable ability across different cutting-edgeVideo-MLLMs.</description>
      <author>example@mail.com (Canhui Tang, Zifan Han, Hongbo Sun, Sanping Zhou, Xuchong Zhang, Xin Wei, Ye Yuan, Jinglin Xu, Hao Sun)</author>
      <guid isPermaLink="false">2508.04369v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</title>
      <link>http://arxiv.org/abs/2508.04043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了VisualTrans，这是第一个专门为视觉转换推理(VTR)在真实人类-物体交互场景设计的全面基准测试，包含12种操作任务和472个高质量问答对，通过6种子任务类型评估三个推理维度。&lt;h4&gt;背景&lt;/h4&gt;视觉转换推理是智能体理解动态场景、建模因果关系和预测未来状态的重要认知能力，但现有基准测试存在模拟到现实的差距、任务复杂度有限和推理覆盖不完整等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准测试的局限性，为真实世界场景中的视觉转换推理提供一个全面、高质量的评估工具。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于第一人称操作视频的可扩展数据流程，整合任务选择、图像对提取、自动化元数据注释和结构化问题生成，并通过人工验证确保质量和可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;最先进的视觉语言模型在静态空间任务中表现良好，但在动态、多步骤推理场景（如中间状态识别和转换序列规划）中存在明显不足，揭示了时间建模和因果推理的基本弱点。&lt;h4&gt;结论&lt;/h4&gt;研究结果为开发更强大和可推广的视觉转换推理系统提供了明确方向，相关数据集和代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;视觉转换推理是一种重要的认知能力，它使智能体能够理解动态场景、建模因果关系并预测未来状态，从而指导行动并为高级智能系统奠定基础。然而，现有的基准测试存在模拟到现实的差距、任务复杂度有限和推理覆盖不完整等问题，限制了它们在真实世界场景中的实际应用。为了解决这些局限性，我们引入了VisualTrans，这是第一个专门为真实人类-物体交互场景中的视觉转换推理设计的全面基准测试。VisualTrans包含12种语义多样的操作任务，并通过6种明确定义的子任务类型系统评估三个基本推理维度：空间、程序和定量。该基准测试包含472个高质量问答对，格式包括多项选择、开放式计数和目标列举。我们引入了一个可扩展的数据构建流程，基于第一人称操作视频，整合了任务选择、图像对提取、使用大型多模态模型的自动化元数据注释和结构化问题生成。人工验证确保了最终基准测试的高质量和可解释性。对各种最先进的视觉语言模型的评估显示，它们在静态空间任务中表现出色。然而，在动态、多步骤推理场景中，特别是在中间状态识别和转换序列规划方面，它们显示出明显的不足。这些发现揭示了时间建模和因果推理的基本弱点，为未来研究指明了明确方向，旨在开发更强大和可推广的视觉转换推理系统。数据集和代码可在https://github.com/WangYipu2002/VisualTrans获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual transformation reasoning (VTR) is a vital cognitive capability thatempowers intelligent agents to understand dynamic scenes, model causalrelationships, and predict future states, and thereby guiding actions andlaying the foundation for advanced intelligent systems. However, existingbenchmarks suffer from a sim-to-real gap, limited task complexity, andincomplete reasoning coverage, limiting their practical use in real-worldscenarios. To address these limitations, we introduce VisualTrans, the firstcomprehensive benchmark specifically designed for VTR in real-worldhuman-object interaction scenarios. VisualTrans encompasses 12 semanticallydiverse manipulation tasks and systematically evaluates three essentialreasoning dimensions - spatial, procedural, and quantitative - through 6well-defined subtask types. The benchmark features 472 high-qualityquestion-answer pairs in various formats, including multiple-choice, open-endedcounting, and target enumeration. We introduce a scalable data constructionpipeline built upon first-person manipulation videos, which integrates taskselection, image pair extraction, automated metadata annotation with largemultimodal models, and structured question generation. Human verificationensures the final benchmark is both high-quality and interpretable. Evaluationsof various state-of-the-art vision-language models show strong performance instatic spatial tasks. However, they reveal notable shortcomings in dynamic,multi-step reasoning scenarios, particularly in areas like intermediate staterecognition and transformation sequence planning. These findings highlightfundamental weaknesses in temporal modeling and causal reasoning, providingclear directions for future research aimed at developing more capable andgeneralizable VTR systems. The dataset and code are available athttps://github.com/WangYipu2002/VisualTrans.</description>
      <author>example@mail.com (Yuheng Ji, Yipu Wang, Yuyang Liu, Xiaoshuai Hao, Yue Liu, Yuting Zhao, Huaihai Lyu, Xiaolong Zheng)</author>
      <guid isPermaLink="false">2508.04043v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>From Waveforms to Pixels: A Survey on Audio-Visual Segmentation</title>
      <link>http://arxiv.org/abs/2508.03724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对视听分割领域的全面综述，涵盖了问题定义、数据集、评估指标和方法进展，分析了各种架构和训练范式，并指出了当前挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;视听分割是多模态感知领域的重要研究方向，旨在通过结合视觉和音频信息来识别和分割视频中产生声音的物体，实现细粒度的物体级理解。&lt;h4&gt;目的&lt;/h4&gt;提供视听分割领域的全面概述，分析现有方法和训练范式，比较不同方法的性能，并指出当前挑战和未来发展方向。&lt;h4&gt;方法&lt;/h4&gt;综述分析了多种方法，包括单模态和多模态编码架构、视听融合策略、解码器设计，以及从完全监督到弱监督和无监督的训练范式。通过标准基准对各种方法进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;不同架构选择、融合策略和训练范式对AVS性能有显著影响。当前方法面临时间建模有限、视觉模态偏向、复杂环境鲁棒性不足和高计算需求等挑战。&lt;h4&gt;结论&lt;/h4&gt;未来研究方向包括改进时间推理和多模态融合、利用基础模型提升泛化和少样本学习能力、减少对标记数据的依赖，以及融入更高级别的推理以实现更智能的AVS系统。&lt;h4&gt;翻译&lt;/h4&gt;视听分割旨在通过利用视觉和音频两种模态来识别和分割视频中产生声音的物体。它已成为多模态感知领域的重要研究方向，能够实现细粒度的物体级理解。在本综述中，我们提供了AVS领域的全面概述，涵盖其问题定义、基准数据集、评估指标和方法进展。我们分析了多种方法，包括单模态和多模态编码架构、视听融合的关键策略以及各种解码器设计。此外，我们探讨了主要的训练范式，从完全监督学习到弱监督和无监督方法。值得注意的是，我们在标准基准上对AVS方法进行了广泛比较，突出了不同架构选择、融合策略和训练范式对性能的影响。最后，我们概述了当前面临的挑战，如时间建模有限、对视觉模态的偏向、复杂环境中缺乏鲁棒性以及高计算需求，并提出了有前途的未来方向，包括改进时间推理和多模态融合、利用基础模型实现更好的泛化和少样本学习、通过自监督和弱监督学习减少对标记数据的依赖，以及融入更高级别的推理以实现更智能的AVS系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Segmentation (AVS) aims to identify and segment sound-producingobjects in videos by leveraging both visual and audio modalities. It hasemerged as a significant research area in multimodal perception, enablingfine-grained object-level understanding. In this survey, we present acomprehensive overview of the AVS field, covering its problem formulation,benchmark datasets, evaluation metrics, and the progression of methodologies.We analyze a wide range of approaches, including architectures for unimodal andmultimodal encoding, key strategies for audio-visual fusion, and variousdecoder designs. Furthermore, we examine major training paradigms, from fullysupervised learning to weakly supervised and training-free methods. Notably, weprovide an extensive comparison of AVS methods across standard benchmarks,highlighting the impact of different architectural choices, fusion strategies,and training paradigms on performance. Finally, we outline the currentchallenges, such as limited temporal modeling, modality bias toward vision,lack of robustness in complex environments, and high computational demands, andpropose promising future directions, including improving temporal reasoning andmultimodal fusion, leveraging foundation models for better generalization andfew-shot learning, reducing reliance on labeled data through selfand weaklysupervised learning, and incorporating higher-level reasoning for moreintelligent AVS systems.</description>
      <author>example@mail.com (Jia Li, Yapeng Tian)</author>
      <guid isPermaLink="false">2508.03724v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Perch 2.0: The Bittern Lesson for Bioacoustics</title>
      <link>http://arxiv.org/abs/2508.04665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Perch 2.0是一个扩展到多类群的生物声学预训练模型，通过自蒸馏和新的训练标准实现了最先进的性能，并在海洋迁移学习任务上表现出色，尽管几乎没有海洋训练数据。&lt;h4&gt;背景&lt;/h4&gt;生物声学领域需要高性能的预训练模型来处理多种物种的声音数据，而现有的模型大多专注于特定类群（如鸟类）。&lt;h4&gt;目的&lt;/h4&gt;扩展Perch模型的训练范围，从单一类群（鸟类）扩展到多类群数据集，提高模型在生物声学任务中的通用性和性能。&lt;h4&gt;方法&lt;/h4&gt;使用自蒸馏训练方法，结合原型学习分类器和新的源预测训练标准，对大型多类群数据集进行监督训练。&lt;h4&gt;主要发现&lt;/h4&gt;Perch 2.0在BirdSet和BEANS基准测试中取得了最先进的性能；尽管几乎没有海洋训练数据，它在海洋迁移学习任务上仍优于专业海洋模型；细粒度物种分类可能是生物声学中特别稳健的预训练任务。&lt;h4&gt;结论&lt;/h4&gt;Perch 2.0通过扩展训练数据和改进训练方法，成功提升了模型在多种生物声学任务中的性能，展示了多类群预训练在生物声学领域的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Perch是一个用于生物声学的高性能预训练模型。它以监督方式训练，可以为数千种发声物种提供即用型分类分数，同时也为迁移学习提供了强大的嵌入表示。在这个新版本Perch 2.0中，我们将训练范围从 exclusively 鸟类扩展到一个大型多类群数据集。该模型使用自蒸馏训练，结合了原型学习分类器和一种新的源预测训练标准。Perch 2.0在BirdSet和BEANS基准测试中获得了最先进的性能。尽管几乎没有海洋训练数据，它在海洋迁移学习任务上仍优于专业海洋模型。我们提出了假设，解释为什么细粒度物种分类是生物声学中特别稳健的预训练任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Perch is a performant pre-trained model for bioacoustics. It was trained insupervised fashion, providing both off-the-shelf classification scores forthousands of vocalizing species as well as strong embeddings for transferlearning. In this new release, Perch 2.0, we expand from training exclusivelyon avian species to a large multi-taxa dataset. The model is trained withself-distillation using a prototype-learning classifier as well as a newsource-prediction training criterion. Perch 2.0 obtains state-of-the-artperformance on the BirdSet and BEANS benchmarks. It also outperformsspecialized marine models on marine transfer learning tasks, despite havingalmost no marine training data. We present hypotheses as to why fine-grainedspecies classification is a particularly robust pre-training task forbioacoustics.</description>
      <author>example@mail.com (Bart van Merriënboer, Vincent Dumoulin, Jenny Hamer, Lauren Harrell, Andrea Burns, Tom Denton)</author>
      <guid isPermaLink="false">2508.04665v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Cluster-specific ranking and variable importance for Scottish regional deprivation via vine mixtures</title>
      <link>http://arxiv.org/abs/2508.04533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种使用藤混合模型对苏格兰区域进行社会经济剥夺聚类的方法，通过分析21个连续指标发现社会经济因素特别是收入和就业率是剥夺的主要驱动力。&lt;h4&gt;背景&lt;/h4&gt;社会经济剥夺是公共健康的关键决定因素，苏格兰政府通过苏格兰多重剥夺指数(SIMD)强调了这一点。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于多重剥夺指标对苏格兰区域进行聚类的方法，使用藤混合模型来捕捉指标间的复杂关系。&lt;h4&gt;方法&lt;/h4&gt;使用藤 copulas 捕捉指标间的尾部依赖和非对称关系；从拟合的藤混合模型中获得每个区域属于各聚类的后验概率；通过排序区域构建聚类驱动的剥夺排名；采用留一变量法评估变量重要性；分析苏格兰格拉斯哥及其周边1964个区域的21个连续指标。&lt;h4&gt;主要发现&lt;/h4&gt;社会经济措施，特别是收入和就业率，是剥夺的主要驱动力；某些健康和犯罪相关指标影响较小；变量重要性的方法和识别聚类的拟合藤结构分析结果一致。&lt;h4&gt;结论&lt;/h4&gt;藤混合模型能有效捕捉指标间的复杂关系；社会经济因素在剥夺评估中比健康和犯罪因素更为重要。&lt;h4&gt;翻译&lt;/h4&gt;社会经济剥夺是公共健康的关键决定因素，正如苏格兰政府的苏格兰多重剥夺指数(SIMD)所强调的那样。我们提出了一种使用藤混合模型基于多重剥夺指标对苏格兰区域进行聚类的方法。该框架利用藤 copulas 的灵活性来捕捉指标间的尾部依赖和非对称关系。从拟合的藤混合模型中，我们获得每个区域属于各聚类的后验概率。这允许通过根据区域属于最剥夺聚类的概率进行排序来构建由聚类驱动的剥夺排名。为了评估这种无监督学习设置中的变量重要性，我们采用留一变量法，通过重新拟合每个变量缺失的模型并计算贝叶斯信息准则的变化。我们对苏格兰格拉斯哥及其周边1964个区域的21个连续指标的分析表明，社会经济措施，特别是收入和就业率，是剥夺的主要驱动力，而某些健康和犯罪相关指标影响较小。这些发现在变量重要性的方法和识别聚类的拟合藤结构分析中是一致的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Socioeconomic deprivation is a key determinant of public health, ashighlighted by the Scottish Government's Scottish Index of Multiple Deprivation(SIMD). We propose an approach for clustering Scottish zones based on multipledeprivation indicators using vine mixture models. This framework uses theflexibility of vine copulas to capture tail dependent and asymmetricrelationships among the indicators. From the fitted vine mixture model, weobtain posterior probabilities for each zone's membership in clusters. Thisallows the construction of a cluster-driven deprivation ranking by sortingzones according to their probability of belonging to the most deprived cluster.To assess variable importance in this unsupervised learning setting, we adopt aleave-one-variable-out procedure by refitting the model without each variableand calculating the resulting change in the Bayesian information criterion. Ouranalysis of 21 continuous indicators across 1964 zones in Glasgow and thesurrounding areas in Scotland shows that socioeconomic measures, particularlyincome and employment rates, are major drivers of deprivation, while certainhealth- and crime-related indicators appear less influential. These findingsare consistent across the approach of variable importance and the analysis ofthe fitted vine structures of the identified clusters.</description>
      <author>example@mail.com (Özge Şahin, Ozan Evkaya, Ariane Hanebeck)</author>
      <guid isPermaLink="false">2508.04533v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images</title>
      <link>http://arxiv.org/abs/2508.03996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大规模预训练数据集对仅使用2D图像的深度学习模型在食物营养成分估计任务上的性能影响，发现专有数据集预训练的模型表现优于公共数据集预训练的模型，而数据集规模并非唯一决定因素。&lt;h4&gt;背景&lt;/h4&gt;从图像估计食物营养成分对健康和饮食监测至关重要，但仅使用2D图像面临食物呈现方式、光照变化以及缺乏深度信息推断体积质量的挑战。此外，该领域的可重复性受到最先进方法依赖专有数据集进行大规模预训练的限制。&lt;h4&gt;目的&lt;/h4&gt;研究大规模预训练数据集特征如何影响仅使用2D图像的深度学习模型在营养估计任务上的性能表现。&lt;h4&gt;方法&lt;/h4&gt;微调和评估在ImageNet和COYO两个大型公共数据集上预训练的Vision Transformer模型，并与在JFT-300M专有数据集上预训练的最先进方法及基线CNN模型（InceptionV2和ResNet-50）进行比较。在具有高精度营养注释的Nutrition5k数据集上进行实验，使用平均绝对误差和平均绝对百分比误差进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型。出乎意料的是，在大型COYO数据集上预训练的模型表现不如在ImageNet上预训练的模型，这与初始假设相矛盾。&lt;h4&gt;结论&lt;/h4&gt;分析提供了定量证据，强调了预训练数据集特征（包括规模、领域相关性和策划质量）在2D营养估计有效迁移学习中的关键作用，表明数据集的质量和领域相关性比单纯的规模更为重要。&lt;h4&gt;翻译&lt;/h4&gt;从图像估计食物的营养成分是一项具有重大健康和饮食监测意义的关键任务。这具有挑战性，特别是仅依靠2D图像时，由于食物呈现方式的多样性、光照的变化，以及在没有深度信息的情况下推断体积和质量的固有困难。此外，该领域的可重复性受到最先进方法依赖专有数据集进行大规模预训练的阻碍。在本文中，我们研究了大规模预训练数据集对仅使用2D图像的深度学习模型在营养估计任务上的性能影响。我们微调和评估了在两个大型公共数据集ImageNet和COYO上预训练的Vision Transformer模型，将其性能与基线CNN模型（InceptionV2和ResNet-50）以及在专有JFT-300M数据集上预训练的最先进方法进行比较。我们在Nutrition5k数据集上进行了大量实验，这是一个具有高精度营养注释的真实世界餐盘的大规模收集。我们使用平均绝对误差和平均绝对百分比误差进行的评估显示，在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型。出乎意料的是，在此特定回归任务中，在大型COYO数据集上预训练的模型表现不如在ImageNet上预训练的模型，这与我们的初始假设相矛盾。我们的分析提供了定量证据，强调了预训练数据集特征（包括规模、领域相关性和策划质量）在2D营养估计有效迁移学习中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the nutritional content of food from images is a critical taskwith significant implications for health and dietary monitoring. This ischallenging, especially when relying solely on 2D images, due to thevariability in food presentation, lighting, and the inherent difficulty ininferring volume and mass without depth information. Furthermore,reproducibility in this domain is hampered by the reliance of state-of-the-artmethods on proprietary datasets for large-scale pre-training. In this paper, weinvestigate the impact of large-scale pre-training datasets on the performanceof deep learning models for nutritional estimation using only 2D images. Wefine-tune and evaluate Vision Transformer (ViT) models pre-trained on two largepublic datasets, ImageNet and COYO, comparing their performance againstbaseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art methodpre-trained on the proprietary JFT-300M dataset. We conduct extensiveexperiments on the Nutrition5k dataset, a large-scale collection of real-worldfood plates with high-precision nutritional annotations. Our evaluation usingMean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) revealsthat models pre-trained on JFT-300M significantly outperform those pre-trainedon public datasets. Unexpectedly, the model pre-trained on the massive COYOdataset performs worse than the model pre-trained on ImageNet for this specificregression task, refuting our initial hypothesis. Our analysis providesquantitative evidence highlighting the critical role of pre-training datasetcharacteristics, including scale, domain relevance, and curation quality, foreffective transfer learning in 2D nutritional estimation.</description>
      <author>example@mail.com (Michele Andrade, Guilherme A. L. Silva, Valéria Santos, Gladston Moreira, Eduardo Luz)</author>
      <guid isPermaLink="false">2508.03996v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions</title>
      <link>http://arxiv.org/abs/2508.03980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种实时手表系统，能够检测面对面和虚拟社交互动，在真实世界环境中实现了73.18%的检测准确率，为理解日常社交体验提供了新工具。&lt;h4&gt;背景&lt;/h4&gt;社交互动是日常生活的基础，对幸福感至关重要。新兴技术为不显眼监测行为提供了机会，但自动检测社交互动（特别是通过可穿戴设备）仍研究不足。现有系统通常局限于受控环境，仅限面对面互动，且依赖严格假设，降低了捕捉多样化现实世界互动的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够检测面对面和虚拟互动的实时、手表系统，解决现有系统的局限性，提高在真实世界环境中捕捉社交互动的能力。&lt;h4&gt;方法&lt;/h4&gt;利用迁移学习检测前景语音(FS)，并基于FS和耳语等对话线索推断互动边界。在真实世界环境中评估系统，涉及11名参与者，总共38天（平均=3.45天，标准差=2.73）。&lt;h4&gt;主要发现&lt;/h4&gt;系统实现了73.18%的互动检测准确率。对6名参与者的后续调查显示，检测互动的召回率达到完美水平（100%）。&lt;h4&gt;结论&lt;/h4&gt;初步研究结果展示了该系统捕捉日常生活中互动的潜力，为针对社交焦虑的个性化干预等应用提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;社交互动是日常生活的基本组成部分，对幸福感起着关键作用。随着新兴技术提供了不显眼监测行为的机会，人们越来越有兴趣利用这些技术更好地理解社交体验。然而，自动检测互动（特别是通过可穿戴设备）仍然研究不足。现有系统通常局限于受控环境，仅限于面对面互动，并依赖严格的假设，如固定时间窗口内存在两个说话者。这些限制降低了它们捕捉多样化现实世界互动的泛化能力。为了解决这些挑战，我们开发了一个实时、手表系统，能够检测面对面和虚拟互动。该系统利用迁移学习检测前景语音（FS），并根据FS和耳语等对话线索推断互动边界。在涉及11名参与者总共38天（平均=3.45天，标准差=2.73）的真实世界评估中，系统实现了73.18%的互动检测准确率。对6名参与者的后续调查显示，检测互动的召回率达到完美。这些初步研究结果证明了我们的系统捕捉日常生活中互动的潜力，为针对社交焦虑的个性化干预等应用提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3714394.3754435&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social interactions are a fundamental part of daily life and play a criticalrole in well-being. As emerging technologies offer opportunities tounobtrusively monitor behavior, there is growing interest in using them tobetter understand social experiences. However, automatically detectinginteractions, particularly via wearable devices, remains underexplored.Existing systems are often limited to controlled environments, constrained toin-person interactions, and rely on rigid assumptions such as the presence oftwo speakers within a fixed time window. These limitations reduce theirgeneralizability to capture diverse real-world interactions. To address thesechallenges, we developed a real-time, on-watch system capable of detecting bothin-person and virtual interactions. The system leverages transfer learning todetect foreground speech (FS) and infers interaction boundaries based upon FSand conversational cues like whispering. In a real-world evaluation involving11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), thesystem achieved an interaction detection accuracy of 73.18%. Follow-up with sixparticipants indicated perfect recall for detecting interactions. Thesepreliminary findings demonstrate the potential of our system to captureinteractions in daily life, providing a foundation for applications such aspersonalized interventions targeting social anxiety.</description>
      <author>example@mail.com (Md Sabbir Ahmed, Arafat Rahman, Mark Rucker, Laura E. Barnes)</author>
      <guid isPermaLink="false">2508.03980v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data</title>
      <link>http://arxiv.org/abs/2508.03921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了主动学习和迁移学习结合在跨领域时间序列异常检测中的效果，发现不应用聚类时效果最佳，主动学习确实能提高性能但改进速度较慢，且性能提升与所选点数量呈线性关系。&lt;h4&gt;背景&lt;/h4&gt;研究关注跨领域时间序列数据中的异常检测问题，探索主动学习与迁移学习结合的有效性。&lt;h4&gt;目的&lt;/h4&gt;评估主动学习和迁移学习结合在跨领域时间序列数据异常检测中的有效性，并分析聚类与主动学习之间的交互作用。&lt;h4&gt;方法&lt;/h4&gt;研究使用了聚类和主动学习的结合方法，并在多个数据集上评估了迁移学习与主动学习结合的性能上限，使用了改进的实验设计，确保采样和测试池使用不同的数据样本。&lt;h4&gt;主要发现&lt;/h4&gt;1) 聚类和主动学习之间存在交互作用，通常不应用聚类时能达到最佳性能；2) 使用主动学习添加新样本能提高模型性能，但改进速度比文献报道的慢；3) 迁移学习与主动学习结合的性能最初会提高，但随着更多目标点被选入训练，性能最终开始下降；4) 性能下降表明主动学习在排序数据点方面表现良好，将不太有用的点推向选择过程末尾。&lt;h4&gt;结论&lt;/h4&gt;综合结果表明，主动学习是有效的，但模型性能的提升与所选点的数量呈线性平坦函数关系，而非文献中可能暗示的更快速改进。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了结合主动学习和迁移学习在跨领域时间序列数据异常检测中的有效性。我们的结果表明聚类和主动学习之间存在交互作用，通常情况下，不应用聚类（即使用单个聚类）时能达到最佳性能。此外，我们发现使用主动学习向训练集添加新样本确实能提高模型性能，但改进速度一般比文献报道的要慢。我们将这种差异归因于改进的实验设计，其中使用了不同的数据样本进行采样和测试。最后，我们在多个数据集上评估了迁移学习与主动学习结合的性能上限，发现性能最初会提高，但随着更多目标点被选入训练，性能最终开始下降。这种性能下降可能表明主动学习过程在排序数据点方面做得很好，将不太有用的点推向选择过程的末尾，而这种下降发生在这些不太有用的点最终被添加时。综合来看，我们的结果表明主动学习是有效的，但模型性能的提升与所选点的数量呈线性平坦函数关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper examines the effectiveness of combining active learning andtransfer learning for anomaly detection in cross-domain time-series data. Ourresults indicate that there is an interaction between clustering and activelearning and in general the best performance is achieved using a single cluster(in other words when clustering is not applied). Also, we find that adding newsamples to the training set using active learning does improve modelperformance but that in general, the rate of improvement is slower than theresults reported in the literature suggest. We attribute this difference to animproved experimental design where distinct data samples are used for thesampling and testing pools. Finally, we assess the ceiling performance oftransfer learning in combination with active learning across several datasetsand find that performance does initially improve but eventually begins to tailoff as more target points are selected for inclusion in training. This tail-offin performance may indicate that the active learning process is doing a goodjob of sequencing data points for selection, pushing the less useful pointstowards the end of the selection process and that this tail-off occurs whenthese less useful points are eventually added. Taken together our resultsindicate that active learning is effective but that the improvement in modelperformance follows a linear flat function concerning the number of pointsselected and labelled.</description>
      <author>example@mail.com (John D. Kelleher, Matthew Nicholson, Rahul Agrahari, Clare Conran)</author>
      <guid isPermaLink="false">2508.03921v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.03863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to be presented at IEEE PIMRC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于众包用户关键性能指标和监管数据的时空频谱需求预测框架，实现了比传统ITU模型更准确的预测结果和更好的跨区域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;准确的频谱需求预测对现代无线通信网络的频谱分配、监管规划和可持续发展至关重要，支持国际电信联盟建立公平的频谱分配政策和改进拍卖机制，以满足5G、6G和物联网等新兴技术的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效的时空预测框架，利用众包用户关键性能指标和监管数据来建模和预测频谱需求，为政策制定者和监管机构提供更可靠的频谱管理工具。&lt;h4&gt;方法&lt;/h4&gt;结合高级特征工程、全面相关性分析和迁移学习技术，利用细粒度、数据驱动的洞察来考虑频谱利用的空间和时间变化，避免传统ITU模型中任意输入和不切实际假设的限制。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法实现了卓越的预测精度和跨区域泛化能力，与ITU估计作为基准的比较评估证明了该框架能够提供更现实和可操作的预测结果。&lt;h4&gt;结论&lt;/h4&gt;该研究验证了所提出方法的有效性，其有望成为监管机构和政策制定者加强频谱管理和规划的稳健方法。&lt;h4&gt;翻译&lt;/h4&gt;准确的频谱需求预测对于现代无线通信网络中的明智频谱分配、有效的监管规划和促进可持续发展至关重要。它支持国际电信联盟领导的政府工作，建立公平的频谱分配政策，改进拍卖机制，并满足先进5G、未来6G和物联网等新兴技术的需求。本文提出了一种有效的时空预测框架，利用众包用户关键性能指标和监管数据来建模和预测频谱需求。所提出的方法通过结合高级特征工程、全面相关性分析和迁移学习技术，实现了卓越的预测精度和跨区域泛化能力。与经常受限于任意输入和不切实际假设的传统ITU模型不同，这种方法利用细粒度、数据驱动的洞察来考虑频谱利用的空间和时间变化。与作为基准的ITU估计进行的比较评估强调了我们的框架提供更现实和可操作预测的能力。实验结果验证了我们的方法的有效性，突显其作为政策制定者和监管机构加强频谱管理和规划的稳健方法的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate spectrum demand prediction is crucial for informed spectrumallocation, effective regulatory planning, and fostering sustainable growth inmodern wireless communication networks. It supports governmental efforts,particularly those led by the international telecommunication union (ITU), toestablish fair spectrum allocation policies, improve auction mechanisms, andmeet the requirements of emerging technologies such as advanced 5G, forthcoming6G, and the internet of things (IoT). This paper presents an effectivespatio-temporal prediction framework that leverages crowdsourced user-side keyperformance indicators (KPIs) and regulatory datasets to model and forecastspectrum demand. The proposed methodology achieves superior prediction accuracyand cross-regional generalizability by incorporating advanced featureengineering, comprehensive correlation analysis, and transfer learningtechniques. Unlike traditional ITU models, which are often constrained byarbitrary inputs and unrealistic assumptions, this approach exploits granular,data-driven insights to account for spatial and temporal variations in spectrumutilization. Comparative evaluations against ITU estimates, as the benchmark,underscore our framework's capability to deliver more realistic and actionablepredictions. Experimental results validate the efficacy of our methodology,highlighting its potential as a robust approach for policymakers and regulatorybodies to enhance spectrum management and planning.</description>
      <author>example@mail.com (Amin Farajzadeh, Hongzhao Zheng, Sarah Dumoulin, Trevor Ha, Halim Yanikomeroglu, Amir Ghasemi)</author>
      <guid isPermaLink="false">2508.03863v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training</title>
      <link>http://arxiv.org/abs/2508.03742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过提升视觉语义密度来改善视觉-语言预训练在医疗诊断中性能的方法，有效解决了低信噪比医学图像与高信噪比报告之间的语义密度差距问题。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言预训练在发展多功能和通用医疗诊断能力方面有很大潜力，但将低信噪比医学图像与高信噪比报告对齐时存在语义密度差距，导致视觉对齐偏差。&lt;h4&gt;目的&lt;/h4&gt;提升视觉语义密度以改善医学图像与报告之间的对齐效果，增强医疗诊断能力。&lt;h4&gt;方法&lt;/h4&gt;一方面，通过疾病级别的视觉对比学习增强视觉语义，加强模型区分正常和异常样本的能力；另一方面，引入解剖正常性建模方法，利用VQ-VAE在潜在空间重建正常视觉嵌入，通过分布偏移放大异常信号。&lt;h4&gt;主要发现&lt;/h4&gt;增强的视觉表示有效捕捉诊断相关语义，促进与诊断报告的高效准确对齐；在多个CT数据集上实验，达到最先进的零样本性能；在15个器官54种疾病中平均AUC达84.9%，显著超越现有方法；展示了优秀的迁移学习能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过提升视觉语义密度，有效解决了医学图像与报告之间的语义密度差距问题，显著提高了医疗诊断的性能。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言预训练在发展多功能和通用医疗诊断能力方面有很大潜力。然而，将低信噪比的医学图像与高信噪比的报告进行对齐时存在语义密度差距，导致视觉对齐偏差。在本文中，我们提出通过提升视觉语义密度来改善对齐效果。一方面，我们通过疾病级别的视觉对比学习来增强视觉语义，加强模型区分每个解剖结构正常和异常样本的能力。另一方面，我们引入解剖正常性建模方法为每个解剖结构建模正常样本的分布，利用VQ-VAE在潜在空间中重建正常的视觉嵌入。这个过程通过利用异常样本中的分布偏移来放大异常信号，增强模型对异常属性的感知和辨别能力。增强的视觉表示有效地捕捉了诊断相关的语义，促进了与诊断报告更高效和准确的对齐。我们在两个胸部CT数据集CT-RATE和Rad-ChestCT以及一个腹部CT数据集MedVL-CT69K上进行了大量实验，全面评估了在胸部和腹部CT场景多个任务中的诊断性能，达到了最先进的零样本性能。值得注意的是，我们的方法在15个器官的54种疾病中平均AUC达到84.9%，显著超越现有方法。此外，我们展示了预训练模型的优秀迁移学习能力。代码可在https://github.com/alibaba-damo-academy/ViSD-Boost获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language pre-training (VLP) has great potential for developingmultifunctional and general medical diagnostic capabilities. However, aligningmedical images with a low signal-to-noise ratio (SNR) to reports with a highSNR presents a semantic density gap, leading to visual alignment bias. In thispaper, we propose boosting vision semantic density to improve alignmenteffectiveness. On one hand, we enhance visual semantics through disease-levelvision contrastive learning, which strengthens the model's ability todifferentiate between normal and abnormal samples for each anatomicalstructure. On the other hand, we introduce an anatomical normality modelingmethod to model the distribution of normal samples for each anatomy, leveragingVQ-VAE for reconstructing normal vision embeddings in the latent space. Thisprocess amplifies abnormal signals by leveraging distribution shifts inabnormal samples, enhancing the model's perception and discrimination ofabnormal attributes. The enhanced visual representation effectively capturesthe diagnostic-relevant semantics, facilitating more efficient and accuratealignment with the diagnostic report. We conduct extensive experiments on twochest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset,MedVL-CT69K, and comprehensively evaluate the diagnosis performance acrossmultiple tasks in the chest and abdominal CT scenarios, achievingstate-of-the-art zero-shot performance. Notably, our method achieved an averageAUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existingmethods. Additionally, we demonstrate the superior transfer learningcapabilities of our pre-trained model. Code is available athttps://github.com/alibaba-damo-academy/ViSD-Boost.</description>
      <author>example@mail.com (Weiwei Cao, Jianpeng Zhang, Zhongyi Shui, Sinuo Wang, Zeli Chen, Xi Li, Le Lu, Xianghua Ye, Tingbo Liang, Qi Zhang, Ling Zhang)</author>
      <guid isPermaLink="false">2508.03742v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation</title>
      <link>http://arxiv.org/abs/2508.04645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by KDD 2025 Research Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络在链接预测任务中的应用挑战，并提出了一种基于预训练的解决方案。通过研究节点级和边级信息的可迁移性，采用后期融合策略和专家混合框架，有效解决了LP任务中的监督有限、初始化敏感和泛化能力差等问题，在保持高性能的同时大幅降低了计算开销。&lt;h4&gt;背景&lt;/h4&gt;链接预测(LP)是图机器学习中的关键任务。尽管图神经网络(GNNs)近年来显著提升了LP性能，但现有方法面临三大挑战：稀疏连接提供的监督有限、对初始化敏感以及在分布偏移下泛化能力差。LP与节点分类不同，它本质上是一个需要集成节点级和边级信息的成对任务。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决LP任务中的关键挑战，探索预训练作为解决方案，研究节点级和边级模块的可迁移性，并提出有效的方法来组合这些模块，同时处理预训练数据的多样性，避免负迁移，并实现快速适应。&lt;h4&gt;方法&lt;/h4&gt;作者提出了多种创新方法：1)首次系统性研究不同模块的可迁移性；2)提出后期融合策略有效组合节点级和边级信息的输出；3)引入专家混合(MoE)框架处理预训练数据多样性，避免负迁移；4)开发参数高效微调策略实现快速适应。这些方法共同构成了一个完整的预训练-微调框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在两个领域的16个数据集上均有效，在低资源链接预测任务上取得了最先进的性能，与端到端训练方法相比获得了具有竞争力的结果，同时计算开销降低了10,000倍以上。&lt;h4&gt;结论&lt;/h4&gt;预训练方法可以有效解决链接预测任务中的关键挑战，通过后期融合策略和专家混合框架，在保持高性能的同时大幅降低计算开销，为链接预测任务提供了一种高效可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;链接预测(LP)是图机器学习中的一个关键任务。虽然图神经网络(GNNs)最近显著提升了LP性能，但现有方法面临关键挑战，包括来自稀疏连接的有限监督、对初始化的敏感性以及在分布偏移下的泛化能力差。我们探索预训练作为解决这些挑战的方案。与节点分类不同，LP本质上是一个成对任务，需要集成节点级和边级信息。在这项工作中，我们进行了首次关于这些不同模块可迁移性的系统性研究，并提出了一种后期融合策略，以有效组合它们的输出以改善性能。为了处理预训练数据的多样性和避免负迁移，我们引入了一个专家混合(MoE)框架，该框架在不同的专家中捕获不同的模式，促进预训练模型在多样化下游数据集上的无缝应用。为了快速适应，我们开发了一种参数高效的微调策略，使预训练模型能够以最小的计算开销适应未见过的数据集。在两个领域的16个数据集上的实验证明了我们方法的有效性，在低资源链接预测上取得了最先进的性能，与端到端训练的方法相比获得了具有竞争力的结果，计算开销降低了10,000倍以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link Prediction (LP) is a critical task in graph machine learning. WhileGraph Neural Networks (GNNs) have significantly advanced LP performancerecently, existing methods face key challenges including limited supervisionfrom sparse connectivity, sensitivity to initialization, and poorgeneralization under distribution shifts. We explore pretraining as a solutionto address these challenges. Unlike node classification, LP is inherently apairwise task, which requires the integration of both node- and edge-levelinformation. In this work, we present the first systematic study on thetransferability of these distinct modules and propose a late fusion strategy toeffectively combine their outputs for improved performance. To handle thediversity of pretraining data and avoid negative transfer, we introduce aMixture-of-Experts (MoE) framework that captures distinct patterns in separateexperts, facilitating seamless application of the pretrained model on diversedownstream datasets. For fast adaptation, we develop a parameter-efficienttuning strategy that allows the pretrained model to adapt to unseen datasetswith minimal computational overhead. Experiments on 16 datasets across twodomains demonstrate the effectiveness of our approach, achievingstate-of-the-art performance on low-resource link prediction while obtainingcompetitive results compared to end-to-end trained methods, with over 10,000xlower computational overhead.</description>
      <author>example@mail.com (Yu Song, Zhigang Hua, Harry Shomer, Yan Xie, Jingzhe Liu, Bo Long, Hui Liu)</author>
      <guid isPermaLink="false">2508.04645v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape</title>
      <link>http://arxiv.org/abs/2508.04542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过分析超过5000个身份盗窃和欺诈案例，构建了身份生态系统图模型，开发了隐私风险预测框架，用于评估个人数据泄露风险及连锁反应。&lt;h4&gt;背景&lt;/h4&gt;个人和组织难以在没有基本了解相关隐私风险的情况下保护个人信息。身份盗窃和欺诈事件频发，但缺乏对哪些个人数据易受攻击、暴露频率及后果的系统理解。&lt;h4&gt;目的&lt;/h4&gt;识别哪些类型的个人数据被暴露、暴露频率如何、暴露后果是什么，并开发一种方法来预测当某些个人信息被泄露时其他信息可能被泄露的概率。&lt;h4&gt;方法&lt;/h4&gt;分析超过5000个身份盗窃和欺诈的实证案例；构建身份生态系统图，其中节点代表可识别个人信息的属性，边代表它们之间的实证披露关系；利用图结构和图论、图神经网络开发隐私风险预测框架。&lt;h4&gt;主要发现&lt;/h4&gt;研究确定了哪些类型的个人数据最容易暴露、暴露的频率以及暴露的后果；开发的隐私风险预测框架能够有效预测当某些个人信息被泄露时其他信息可能被泄露的概率。&lt;h4&gt;结论&lt;/h4&gt;身份生态系统图和隐私风险预测框架为理解和预测个人信息泄露风险提供了有效工具，能够回答给定身份属性的披露是否可能导致另一个属性披露的核心问题。&lt;h4&gt;翻译&lt;/h4&gt;个人和组织在没有基本了解相关隐私风险的情况下难以保护个人信息。通过分析超过5000个身份盗窃和欺诈的实证案例，本研究确定了哪些类型的个人数据被暴露、暴露频率如何以及暴露的后果是什么。我们构建了一个身份生态系统图——一个基于图的基础模型，其中节点代表可识别个人信息的属性，边代表它们之间的实证披露关系（例如，一个PII属性由于另一个属性的暴露而被暴露的概率）。利用这种图结构，我们开发了一个隐私风险预测框架，使用图论和图神经网络来估计当某些PII属性被泄露时进一步泄露的可能性。结果表明，我们的方法有效地回答了核心问题：给定身份属性的披露是否可能导致另一个属性的披露？&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; It is difficult for individuals and organizations to protect personalinformation without a fundamental understanding of relative privacy risks. Byanalyzing over 5,000 empirical identity theft and fraud cases, this researchidentifies which types of personal data are exposed, how frequently exposuresoccur, and what the consequences of those exposures are. We construct anIdentity Ecosystem graph--a foundational, graph-based model in which nodesrepresent personally identifiable information (PII) attributes and edgesrepresent empirical disclosure relationships between them (e.g., theprobability that one PII attribute is exposed due to the exposure of another).Leveraging this graph structure, we develop a privacy risk prediction frameworkthat uses graph theory and graph neural networks to estimate the likelihood offurther disclosures when certain PII attributes are compromised. The resultsshow that our approach effectively answers the core question: Can thedisclosure of a given identity attribute possibly lead to the disclosure ofanother attribute?</description>
      <author>example@mail.com (Haoran Niu, K. Suzanne Barber)</author>
      <guid isPermaLink="false">2508.04542v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks</title>
      <link>http://arxiv.org/abs/2508.04436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合轨迹规划框架，结合基于学习的方法适应性和基于优化方法的正式安全保证，用于解决自动驾驶高速公路驾驶中的碰撞风险问题。该框架采用两层架构，实现了高成功率和实时性能。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶高速公路驾驶面临环境快速变化和反应时间有限的挑战，导致较高的碰撞风险，需要可靠高效的轨迹规划方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂现实世界紧急场景中生成平滑、无碰撞轨迹的轨迹规划方法，同时保证实时性能。&lt;h4&gt;方法&lt;/h4&gt;提出混合轨迹规划框架，采用两层架构：上层使用图神经网络(GNN)预测纵向速度曲线，下层将路径优化表述为混合整数二次规划(MIQP)问题，引入离散化车辆几何的线性近似降低计算复杂度，并执行严格时空不重叠约束确保碰撞避免。&lt;h4&gt;主要发现&lt;/h4&gt;该规划器在复杂紧急场景中生成高度平滑、无碰撞轨迹，成功率超过97%，平均规划时间为54毫秒，确认了其实时能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的混合轨迹规划框架成功结合了学习方法的适应性和优化方法的安全保证，能够在复杂紧急场景中高效生成安全轨迹，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶高速公路驾驶由于环境快速变化和反应时间有限而呈现高碰撞风险，需要可靠高效的轨迹规划。本文提出了一种混合轨迹规划框架，整合了基于学习方法的适应性和基于优化方法的正式安全保证。该框架采用两层架构：上层使用在真实高速公路数据上训练的图神经网络(GNN)预测类人的纵向速度曲线，下层利用表述为混合整数二次规划(MIQP)问题的路径优化。主要贡献是下层的路径优化模型，该模型引入了离散化车辆几何的线性近似，显著降低了计算复杂度，同时强制执行严格的时空不重叠约束，在整个规划范围内正式保证碰撞避免。实验结果表明，该规划器在复杂的现实世界紧急场景中生成高度平滑、无碰撞的轨迹，成功率超过97%，平均规划时间为54毫秒，从而确认了其实时能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous highway driving presents a high collision risk due tofast-changing environments and limited reaction time, necessitating reliableand efficient trajectory planning. This paper proposes a hybrid trajectoryplanning framework that integrates the adaptability of learning-based methodswith the formal safety guarantees of optimization-based approaches. Theframework features a two-layer architecture: an upper layer employing a graphneural network (GNN) trained on real-world highway data to predict human-likelongitudinal velocity profiles, and a lower layer utilizing path optimizationformulated as a mixed-integer quadratic programming (MIQP) problem. The primarycontribution is the lower-layer path optimization model, which introduces alinear approximation of discretized vehicle geometry to substantially reducecomputational complexity, while enforcing strict spatiotemporal non-overlappingconstraints to formally guarantee collision avoidance throughout the planninghorizon. Experimental results demonstrate that the planner generates highlysmooth, collision-free trajectories in complex real-world emergency scenarios,achieving success rates exceeding 97% with average planning times of 54 ms,thereby confirming real-time capability.</description>
      <author>example@mail.com (Yujia Lu, Chong Wei, Lu Ma)</author>
      <guid isPermaLink="false">2508.04436v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition</title>
      <link>http://arxiv.org/abs/2508.04381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ProtoN少样本学习框架，通过基于图的方法联合处理同一身份的多个耳部印象，显著提高了耳部生物识别的准确率，在有限数据条件下达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;耳部生物识别是一种稳定且非接触式的身份识别方式，但其有效性受到标注数据稀缺和类内变化大的限制。现有方法通常单独处理单个印象，限制了捕捉一致性和区分性表示的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够联合处理多个印象、捕捉一致性和区分性表示的少样本学习框架，以提高耳部生物识别的准确性，克服数据稀缺和类内变化的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出ProtoN框架，使用基于图的方法将每个印象表示为类特定图中的一个节点，同时包含一个可学习的原型节点。通过原型图神经网络层处理该图，采用双路径消息传递机制优化表示，并使用跨图原型对齐策略和混合损失函数增强区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准耳部数据集上的实验表明，ProtoN实现了最先进的性能，Rank-1识别准确率高达99.60%，等错误率低至0.025，证明了在有限数据条件下少样本耳部识别的有效性。&lt;h4&gt;结论&lt;/h4&gt;ProtoN框架通过联合处理多个印象和基于图的方法，有效解决了耳部生物识别中的数据稀缺和类内变化问题，为少样本耳部识别提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;耳部生物识别为身份识别提供了一种稳定且非接触式的模态，但其有效性仍受限于标注数据的稀缺和显著的类内变化。现有方法通常单独从单个印象中提取身份特征，限制了它们捕捉一致性和区分性表示的能力。为了克服这些局限性，提出了一种名为ProtoN的少样本学习框架，使用基于图的方法联合处理同一身份的多个印象。每个印象在类特定图中表示为一个节点，同时包含一个可学习的原型节点，该节点编码身份级信息。该图由原型图神经网络层处理，该层专门设计通过双路径消息传递机制来优化印象和原型表示。为了进一步增强区分能力，PGNN包含跨图原型对齐策略，通过强制类内紧凑性同时保持类间区分性来提高类可分性。此外，采用混合损失函数来平衡周期性和全局分类目标，从而改善嵌入空间的总体结构。在五个基准耳部数据集上的大量实验表明，ProtoN实现了最先进的性能，Rank-1识别准确率高达99.60%，等错误率低至0.025，显示了在有限数据条件下少样本耳部识别的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ear biometrics offer a stable and contactless modality for identityrecognition, yet their effectiveness remains limited by the scarcity ofannotated data and significant intra-class variability. Existing methodstypically extract identity features from individual impressions in isolation,restricting their ability to capture consistent and discriminativerepresentations. To overcome these limitations, a few-shot learning framework,ProtoN, is proposed to jointly process multiple impressions of an identityusing a graph-based approach. Each impression is represented as a node in aclass-specific graph, alongside a learnable prototype node that encodesidentity-level information. This graph is processed by a Prototype Graph NeuralNetwork (PGNN) layer, specifically designed to refine both impression andprototype representations through a dual-path message-passing mechanism. Tofurther enhance discriminative power, the PGNN incorporates a cross-graphprototype alignment strategy that improves class separability by enforcingintra-class compactness while maintaining inter-class distinction.Additionally, a hybrid loss function is employed to balance episodic and globalclassification objectives, thereby improving the overall structure of theembedding space. Extensive experiments on five benchmark ear datasetsdemonstrate that ProtoN achieves state-of-the-art performance, with Rank-1identification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as0.025, showing the effectiveness for few-shot ear recognition under limiteddata conditions.</description>
      <author>example@mail.com (Santhoshkumar Peddi, Sadhvik Bathini, Arun Balasubramanian, Monalisa Sarma, Debasis Samanta)</author>
      <guid isPermaLink="false">2508.04381v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities</title>
      <link>http://arxiv.org/abs/2508.04235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为CASCAD的新型电路感知SAT求解框架，通过利用图神经网络计算的门级条件概率直接利用电路级信息，显著提高了SAT求解器效率。在逻辑等价性检查基准测试中，CASCAD相比最先进的基于CNF的方法将求解时间减少了最多10倍。&lt;h4&gt;背景&lt;/h4&gt;电路可满足性(CSAT)在电子设计自动化中起着关键作用。解决CSAT问题的标准工作流程是将电路转换为合取范式(CNF)，并使用基于冲突驱动子句学习(CDCL)的通用SAT求解器。然而，这个过程固有地丢弃了丰富的结构和功能信息，导致求解器性能次优。&lt;h4&gt;目的&lt;/h4&gt;解决传统CSAT求解方法中丢弃电路结构和功能信息的问题，提高SAT求解器的效率。&lt;h4&gt;方法&lt;/h4&gt;作者提出了CASCAD，一种新颖的电路感知SAT求解框架，直接利用通过图神经网络(GNN)计算的电路级条件概率。通过显式建模门级条件概率，CASCAD动态引导两个关键的CDCL启发式方法——变量相位选择和子句管理，从而提高求解器效率，并采用概率引导的子句过滤策略。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的实际逻辑等价性检查(LEC)基准测试中，CASCAD相比最先进的基于CNF的方法将求解时间减少了最多10倍，并通过概率引导的子句过滤策略额外实现了23.5%的运行时间减少。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了在SAT求解器中保留电路级结构洞察力的重要性，为未来提高SAT求解效率和EDA工具设计提供了坚实的基础。&lt;h4&gt;翻译&lt;/h4&gt;电路可满足性(CSAT)在电子设计自动化中起着关键作用。解决CSAT问题的标准工作流程将电路转换为合取范式(CNF)，并采用由冲突驱动子句学习(CDCL)提供支持的通用SAT求解器。然而，这个过程固有地丢弃了丰富的结构和功能信息，导致求解器性能次优。为了解决这一限制，我们引入了CASCAD，一种新颖的电路感知SAT求解框架，直接利用通过图神经网络(GNN)计算的电路级条件概率。通过显式建模门级条件概率，CASCAD动态引导两个关键的CDCL启发式方法——变量相位选择和子句管理，显著提高了解决器效率。在具有挑战性的实际逻辑等价性检查(LEC)基准测试上的广泛评估表明，CASCAD相比最先进的基于CNF的方法将求解时间减少了最多10倍，并通过我们的概率引导子句过滤策略额外实现了23.5%的运行时间减少。我们的结果强调了在SAT求解器中保留电路级结构洞察力的重要性，为未来提高SAT求解效率和EDA工具设计提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Circuit Satisfiability (CSAT) plays a pivotal role in Electronic DesignAutomation. The standard workflow for solving CSAT problems converts circuitsinto Conjunctive Normal Form (CNF) and employs generic SAT solvers powered byConflict-Driven Clause Learning (CDCL). However, this process inherentlydiscards rich structural and functional information, leading to suboptimalsolver performance. To address this limitation, we introduce CASCAD, a novelcircuit-aware SAT solving framework that directly leverages circuit-levelconditional probabilities computed via Graph Neural Networks (GNNs). Byexplicitly modeling gate-level conditional probabilities, CASCAD dynamicallyguides two critical CDCL heuristics -- variable phase selection and clausemanagementto significantly enhance solver efficiency. Extensive evaluations onchallenging real-world Logical Equivalence Checking (LEC) benchmarksdemonstrate that CASCAD reduces solving times by up to 10x compared tostate-of-the-art CNF-based approaches, achieving an additional 23.5% runtimereduction via our probability-guided clause filtering strategy. Our resultsunderscore the importance of preserving circuit-level structural insightswithin SAT solvers, providing a robust foundation for future improvements inSAT-solving efficiency and EDA tool design.</description>
      <author>example@mail.com (Jiaying Zhu, Ziyang Zheng, Zhengyuan Shi, Yalun Cai, Qiang Xu)</author>
      <guid isPermaLink="false">2508.04235v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>The Ubiquitous Sparse Matrix-Matrix Products</title>
      <link>http://arxiv.org/abs/2508.04077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;关于稀疏矩阵运算及其在多个领域应用的统一处理研究&lt;h4&gt;背景&lt;/h4&gt;稀疏矩阵与另一个矩阵（密集或稀疏）的乘法是数据科学应用中的基础运算，广泛应用于图算法、稀疏连接神经网络、图神经网络、聚类以及生物测序数据的许多对多比较等领域&lt;h4&gt;目的&lt;/h4&gt;提供稀疏矩阵运算及其丰富应用空间的统一处理方法&lt;h4&gt;方法&lt;/h4&gt;研究在任意代数半环（其中标量运算被用户定义函数重载）和更一般的异构代数（其中输入矩阵的定义域可以不同）上的稀疏矩阵乘法运算&lt;h4&gt;主要发现&lt;/h4&gt;稀疏矩阵乘法可以在不同代数结构上进行，包括具有特定属性的用户定义函数重载的代数半环和具有不同定义域的异构代数&lt;h4&gt;结论&lt;/h4&gt;稀疏矩阵运算及其统一处理方法适用于机器学习、计算生物学和化学、图算法和科学计算等多个领域&lt;h4&gt;翻译&lt;/h4&gt;稀疏矩阵与另一个（密集或稀疏）矩阵的乘法是一种基本运算，它捕捉了许多数据科学应用的计算模式，包括但不限于图算法、稀疏连接神经网络、图神经网络、聚类以及生物测序数据的许多对多比较。在许多应用场景中，矩阵乘法发生在任意代数半环上，其中标量运算被具有特定属性的用户定义函数重载，或者在更一般的异构代数上，甚至输入矩阵的定义域也可以不同。在这里，我们提供了稀疏矩阵运算及其丰富应用空间的统一处理方法，包括机器学习、计算生物学和化学、图算法和科学计算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiplication of a sparse matrix with another (dense or sparse) matrix is afundamental operation that captures the computational patterns of many datascience applications, including but not limited to graph algorithms, sparselyconnected neural networks, graph neural networks, clustering, and many-to-manycomparisons of biological sequencing data.  In many application scenarios, the matrix multiplication takes places on anarbitrary algebraic semiring where the scalar operations are overloaded withuser-defined functions with certain properties or a more general heterogenousalgebra where even the domains of the input matrices can be different. Here, weprovide a unifying treatment of the sparse matrix-matrix operation and its richapplication space including machine learning, computational biology andchemistry, graph algorithms, and scientific computing.</description>
      <author>example@mail.com (Aydın Buluç)</author>
      <guid isPermaLink="false">2508.04077v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.03783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 3 figures, Affiliation updated to match user registration&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种使用强化学习代理系统探测图神经网络解码器脆弱性的新框架，并通过对抗训练显著提高了解码器的鲁棒性，为开发更可靠的量子计算解码器提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为量子错误解码的一种强大数据驱动方法，能直接从综合症数据中学习复杂噪声特征，但这些解码器对细微对抗性扰动的鲁棒性仍是关键问题。&lt;h4&gt;目的&lt;/h4&gt;引入新框架，使用强化学习代理作为对手，系统探测GNN解码器的脆弱性，寻找能导致解码器错误分类的最小综合症修改。&lt;h4&gt;方法&lt;/h4&gt;将框架应用于在谷歌量子AI实验表面码数据上训练的图注意力网络解码器，训练强化学习代理寻找最小综合症修改以造成解码错误。&lt;h4&gt;主要发现&lt;/h4&gt;强化学习代理能成功识别特定关键脆弱性，以最少比特翻转实现高攻击成功率；通过对抗训练（用生成的对抗性例子重新训练模型）可显著提高解码器鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;自动化的脆弱性发现和有针对性重新训练的迭代过程，为开发容错量子计算中的更可靠鲁棒神经网络解码器提供了有前途的方法。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为量子错误校正(QEC)解码的一种强大数据驱动方法，能够直接从综合症数据中学习复杂的噪声特征。然而，这些解码器对细微的、对抗性扰动的鲁棒性仍然是一个关键开放问题。这项工作引入了一种新框架，使用强化学习(RL)代理系统性地探测GNN解码器的脆弱性。RL代理被训练为对手，目标是寻找导致解码器错误分类的最小综合症修改。我们将该框架应用于在谷歌量子AI的实验表面码数据上训练的图注意力网络(GAT)解码器。结果表明，RL代理能够成功识别特定的、关键的脆弱性，以最少的比特翻转次数实现高攻击成功率。此外，我们证明通过对抗训练（使用RL生成的对抗性例子重新训练模型）可以显著提高解码器的鲁棒性。这种自动化的脆弱性发现和有针对性的重新训练的迭代过程，为开发用于容错量子计算的更可靠和更鲁棒的神经网络解码器提供了有前途的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approachfor Quantum Error Correction (QEC) decoding, capable of learning complex noisecharacteristics directly from syndrome data. However, the robustness of thesedecoders against subtle, adversarial perturbations remains a critical openquestion. This work introduces a novel framework to systematically probe thevulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. TheRL agent is trained as an adversary with the goal of finding minimal syndromemodifications that cause the decoder to misclassify. We apply this framework toa Graph Attention Network (GAT) decoder trained on experimental surface codedata from Google Quantum AI. Our results show that the RL agent cansuccessfully identify specific, critical vulnerabilities, achieving a highattack success rate with a minimal number of bit flips. Furthermore, wedemonstrate that the decoder's robustness can be significantly enhanced throughadversarial training, where the model is retrained on the adversarial examplesgenerated by the RL agent. This iterative process of automated vulnerabilitydiscovery and targeted retraining presents a promising methodology fordeveloping more reliable and robust neural network decoders for fault-tolerantquantum computing.</description>
      <author>example@mail.com (Ryota Ikeda)</author>
      <guid isPermaLink="false">2508.03783v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM</title>
      <link>http://arxiv.org/abs/2508.03782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, 1 table. Affiliation updated to match user  registration&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究测试了知识蒸馏是否能提高量子错误校正解码器中图神经网络(GNN)的性能。研究发现，尽管知识蒸馏模型的测试准确率与基线模型相当，但其训练时间显著增加，表明现代GNN架构能够直接从硬件数据学习错误相关性，无需理论模型指导。&lt;h4&gt;背景&lt;/h4&gt;量子错误校正(QEC)解码器的性能对于实现实用量子计算机至关重要。近年来，图神经网络(GNNs)已成为一种有前景的方法，但其训练方法尚未成熟。&lt;h4&gt;目的&lt;/h4&gt;测试知识蒸馏(将经典算法如最小权完美匹配(MWPM)的理论知识转移到GNNs)是否能有效提高GNN解码器性能的假设。&lt;h4&gt;方法&lt;/h4&gt;比较两个基于图注意力网络(GAT)架构的模型，该架构将时间信息作为节点特征。第一个是纯数据驱动模型(基线)，仅使用真实标签训练；第二个则基于MWPM的理论错误概率融入了知识蒸馏损失。使用Google的公开实验数据进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;知识蒸馏模型的最终测试准确率与基线几乎相同，但其训练损失收敛更慢，训练时间增加了约5倍。&lt;h4&gt;结论&lt;/h4&gt;现代GNN架构具有很高的能力，能够直接从真实硬件数据中高效学习复杂的错误相关性，而无需近似理论模型的指导。&lt;h4&gt;翻译&lt;/h4&gt;量子错误校正(QEC)解码器的性能对于实现实用量子计算机至关重要。近年来，图神经网络(GNNs)已成为一种有前景的方法，但其训练方法尚未成熟。通常预期，将经典算法(如最小权完美匹配MWPM)的理论知识转移到GNNs(一种称为知识蒸馏的技术)可以有效地提高性能。在这项工作中，我们通过严格比较两个基于图注意力网络(GAT)架构的模型来测试这一假设，该架构将时间信息作为节点特征。第一个是纯数据驱动的模型(基线)，仅使用真实标签进行训练，而第二个则基于MWPM的理论错误概率融入了知识蒸馏损失。使用Google的公开实验数据，我们的评估显示，尽管知识蒸馏模型的最终测试准确率与基线几乎相同，但其训练损失收敛更慢，训练时间增加了约5倍。这一结果表明，现代GNN架构具有很高的能力，能够直接从真实硬件数据中高效学习复杂的错误相关性，而无需近似理论模型的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of decoders in Quantum Error Correction (QEC) is key torealizing practical quantum computers. In recent years, Graph Neural Networks(GNNs) have emerged as a promising approach, but their training methodologiesare not yet well-established. It is generally expected that transferringtheoretical knowledge from classical algorithms like Minimum Weight PerfectMatching (MWPM) to GNNs, a technique known as knowledge distillation, caneffectively improve performance. In this work, we test this hypothesis byrigorously comparing two models based on a Graph Attention Network (GAT)architecture that incorporates temporal information as node features. The firstis a purely data-driven model (baseline) trained only on ground-truth labels,while the second incorporates a knowledge distillation loss based on thetheoretical error probabilities from MWPM. Using public experimental data fromGoogle, our evaluation reveals that while the final test accuracy of theknowledge distillation model was nearly identical to the baseline, its trainingloss converged more slowly, and the training time increased by a factor ofapproximately five. This result suggests that modern GNN architectures possessa high capacity to efficiently learn complex error correlations directly fromreal hardware data, without guidance from approximate theoretical models.</description>
      <author>example@mail.com (Ryota Ikeda)</author>
      <guid isPermaLink="false">2508.03782v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Drone Detection with Event Cameras</title>
      <link>http://arxiv.org/abs/2508.04564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文调查了事件视觉作为解决无人机检测挑战的解决方案。事件相机通过消除运动模糊和在极端光照条件下保持一致检测来克服传统相机的限制。论文涵盖了从数据表示到使用尖峰神经网络的先进处理流程的最先进技术，并讨论了实时跟踪、轨迹预测和通过螺旋桨签名分析进行唯一识别等更复杂的任务，表明事件视觉为下一代反无人机系统提供了强大基础。&lt;h4&gt;背景&lt;/h4&gt;无人机扩散带来了显著的安全挑战。传统监控系统，特别是基于帧的相机，难以可靠检测这些目标，原因包括无人机体积小、高度灵活、运动模糊以及在挑战性光照条件下性能差。&lt;h4&gt;目的&lt;/h4&gt;调查事件视觉这一新兴领域作为无人机检测问题的稳健解决方案，展示该技术如何为下一代可靠、低延迟和高效的反无人机系统提供基础。&lt;h4&gt;方法&lt;/h4&gt;回顾基于事件的无人机检测的最先进技术，从数据表示方法到使用尖峰神经网络的高级处理流程。讨论扩展到实时跟踪、轨迹预测和通过螺旋桨签名分析进行唯一识别等更复杂的任务。&lt;h4&gt;主要发现&lt;/h4&gt;事件相机几乎消除了运动模糊，使在极端光照条件下保持一致的检测成为可能。它们的稀疏、异步输出抑制静态背景，实现对运动线索的低延迟关注。&lt;h4&gt;结论&lt;/h4&gt;通过检查当前方法论、可用数据集和技术的独特优势，论文证明了事件视觉为可靠、低延迟和高效的反无人机系统提供了强大基础。&lt;h4&gt;翻译&lt;/h4&gt;无人机的扩散带来了显著的安全和挑战。传统的监控系统，特别是传统的基于帧的相机，由于无人机体积小、高度灵活以及由此产生的运动模糊和在挑战性光照条件下性能差，难以可靠地检测这些目标。本文调查了事件视觉这一新兴领域作为这些问题的稳健解决方案。事件相机几乎消除了运动模糊，使在极端光照条件下保持一致的检测成为可能。它们稀疏、异步的输出抑制静态背景，实现对运动线索的低延迟关注。我们回顾了基于事件的无人机检测的最先进技术，从数据表示方法到使用尖峰神经网络的先进处理流程。讨论超越了简单的检测，涵盖了实时跟踪、轨迹预测和通过螺旋桨签名分析进行唯一识别等更复杂的任务。通过检查当前的方法论、可用的数据集和技术的独特优势，这项工作证明了事件视觉为下一代可靠、低延迟和高效的反无人机系统提供了强大的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The diffusion of drones presents significant security and safety challenges.Traditional surveillance systems, particularly conventional frame-basedcameras, struggle to reliably detect these targets due to their small size,high agility, and the resulting motion blur and poor performance in challenginglighting conditions. This paper surveys the emerging field of event-basedvision as a robust solution to these problems. Event cameras virtuallyeliminate motion blur and enable consistent detection in extreme lighting.Their sparse, asynchronous output suppresses static backgrounds, enablinglow-latency focus on motion cues. We review the state-of-the-art in event-baseddrone detection, from data representation methods to advanced processingpipelines using spiking neural networks. The discussion extends beyond simpledetection to cover more sophisticated tasks such as real-time tracking,trajectory forecasting, and unique identification through propeller signatureanalysis. By examining current methodologies, available datasets, and thedistinct advantages of the technology, this work demonstrates that event-basedvision provides a powerful foundation for the next generation of reliable,low-latency, and efficient counter-UAV systems.</description>
      <author>example@mail.com (Gabriele Magrini, Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Pietro Pala)</author>
      <guid isPermaLink="false">2508.04564v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis</title>
      <link>http://arxiv.org/abs/2508.00381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的焊接缺陷检测方法，结合了自适应神经网络架构和可解释AI技术，通过专家验证和可信AI原则，确保了检测系统的可靠性和透明度，解决了传统检测方法的局限性，提高了在关键环境中的安全性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;焊接缺陷检测对油气行业管道系统的安全性和可靠性至关重要，特别是在具有挑战性的海洋和近海环境中。传统无损检测方法常无法检测到细微或内部缺陷，导致潜在故障和昂贵停机时间。现有基于神经网络的缺陷分类方法依赖任意选择的预训练架构，缺乏可解释性，引发部署安全担忧。&lt;h4&gt;目的&lt;/h4&gt;解决传统检测方法的局限性，提高焊接缺陷检测的准确性和可靠性，增强系统的可解释性和透明度，建立可信的自动化决策系统。&lt;h4&gt;方法&lt;/h4&gt;提出'Adapt-WeldNet'自适应框架，系统评估各种预训练架构、迁移学习策略和自适应优化器；开发缺陷检测可解释性分析(DDIA)框架，使用Grad-CAM和LIME等可解释AI技术，结合ASNT NDE II级专业人士的领域评估，采用人机循环方法并遵循可信AI原则。&lt;h4&gt;主要发现&lt;/h4&gt;Adapt-WeldNet能优化缺陷检测并提供可操作的见解；DDIA框架增强系统透明度；结合专家验证确保检测系统的可靠性、公平性和问责制；提高性能和可解释性，增强信任度。&lt;h4&gt;结论&lt;/h4&gt;通过提高性能和可解释性，这项工作增强了焊接缺陷检测系统的信任度、安全性和可靠性，支持海洋和近海环境中的关键操作。&lt;h4&gt;翻译&lt;/h4&gt;焊接缺陷检测对确保油气行业管道系统的安全性和可靠性至关重要，特别是在具有挑战性的海洋和近海环境中。传统无损检测方法常常无法检测到细微或内部缺陷，导致潜在故障和昂贵的停机时间。此外，现有的基于神经网络的缺陷分类方法通常依赖于任意选择的预训练架构，缺乏可解释性，引发了部署安全担忧。为应对这些挑战，本文介绍了'Adapt-WeldNet'，这是一种用于焊接缺陷检测的自适应框架，系统评估各种预训练架构、迁移学习策略和自适应优化器，以确定性能最佳模型和超参数，优化缺陷检测并提供可操作的见解。此外，还提出了新颖的缺陷检测可解释性分析(DDIA)框架，以增强系统透明度。DDIA采用可解释AI技术，如Grad-CAM和LIME，并结合经过认证的ASNT NDE II级专业人士验证的领域特定评估。采用人机循环方法并遵循可信AI原则，DDIA确保了缺陷检测系统的可靠性、公平性和问责制，通过专家验证培养对自动化决策的信心。通过提高性能和可解释性，这项工作增强了焊接缺陷检测系统的信任度、安全性和可靠性，支持海洋和近海环境中的关键操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weld defect detection is crucial for ensuring the safety and reliability ofpiping systems in the oil and gas industry, especially in challenging marineand offshore environments. Traditional non-destructive testing (NDT) methodsoften fail to detect subtle or internal defects, leading to potential failuresand costly downtime. Furthermore, existing neural network-based approaches fordefect classification frequently rely on arbitrarily selected pretrainedarchitectures and lack interpretability, raising safety concerns fordeployment. To address these challenges, this paper introduces``Adapt-WeldNet", an adaptive framework for welding defect detection thatsystematically evaluates various pre-trained architectures, transfer learningstrategies, and adaptive optimizers to identify the best-performing model andhyperparameters, optimizing defect detection and providing actionable insights.Additionally, a novel Defect Detection Interpretability Analysis (DDIA)framework is proposed to enhance system transparency. DDIA employs ExplainableAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specificevaluations validated by certified ASNT NDE Level II professionals.Incorporating a Human-in-the-Loop (HITL) approach and aligning with theprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, andaccountability of the defect detection system, fostering confidence inautomated decisions through expert validation. By improving both performanceand interpretability, this work enhances trust, safety, and reliability inwelding defect detection systems, supporting critical operations in offshoreand marine environments.</description>
      <author>example@mail.com (Kamal Basha S, Athira Nambiar)</author>
      <guid isPermaLink="false">2508.00381v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
  <item>
      <title>Correspondence-Free Fast and Robust Spherical Point Pattern Registration</title>
      <link>http://arxiv.org/abs/2508.02339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种具有线性时间复杂度的球面模式旋转估计算法，通过将球面模式表示为离散3D点集，重新表述为球面点集对齐问题，并引入了三种新算法。&lt;h4&gt;背景&lt;/h4&gt;现有球面旋转估计方法基于球面函数交叉相关最大化，计算复杂度大于O(n³)，且在异常值污染下缺乏充分评估。&lt;h4&gt;目的&lt;/h4&gt;开发一种具有线性时间复杂度O(n)的球面模式间旋转估计算法，提高计算效率和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将球面模式显式表示为单位球面上的离散3D点集，将旋转估计重新表述为球面点集对齐问题（3D单位向量的Wahba问题），并提出了三种新算法：SPMC、FRS和SPMC+FRS混合方法。&lt;h4&gt;主要发现&lt;/h4&gt;在S²域和无对应关系设置下，所提算法比当前最先进方法快10倍以上，在存在异常值的情况下准确度高10倍以上，通过'鲁棒向量对齐数据集'上的广泛模拟得到验证。&lt;h4&gt;结论&lt;/h4&gt;该方法已成功应用于点云配准和球面图像旋转估计两个实际任务中。&lt;h4&gt;翻译&lt;/h4&gt;现有球面模式间的旋转估计方法通常依赖于两个球面函数之间的球面交叉相关最大化。然而，这些方法在旋转空间离散化上的计算复杂度大于立方级，并且在大量异常值污染下缺乏广泛评估。为此，我们提出了一种具有线性时间复杂度的球面模式间旋转估计算法。与现有的基于球面函数的方法不同，我们将球面模式显式表示为单位球面上的离散3D点集，将旋转估计重新表述为球面点集对齐问题（即3D单位向量的Wahba问题）。基于我们的几何表述，球面模式对齐算法自然地与3D单位向量的Wahba问题框架相契合。具体而言，我们引入了三种新算法：(1) 基于相关的球面模式匹配，(2) 快速旋转搜索，以及(3) 结合前两种方法优势的混合方法。我们的实验表明，在球面域和无对应关系设置下，我们的算法比当前解决Wahba问题的最先进方法快10倍以上，且在存在异常值的情况下准确度高10倍以上。我们通过在新的球面模式数据集'鲁棒向量对齐数据集'上的广泛模拟验证了我们的方法。此外，我们将我们的方法适应到两个实际任务：(i) 点云配准和(ii) 球面图像的旋转估计。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决球面点模式之间的旋转估计问题，即在单位球面上找到两个点集之间的最优旋转矩阵，使一个点集旋转后能与另一个点集对齐。这个问题在现实中非常重要，因为它与Wahba问题和正交Procrustes问题密切相关，广泛应用于姿态估计、点云配准、图像拼接、3D重建和机器人等领域。现有方法计算复杂度高（大于O(n^3)），难以处理实时应用，且在高旋转误差和异常值情况下表现不佳，限制了这些技术的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于球面互相关方法的局限性，特别是高计算复杂度和对大旋转误差的不适应性。然后，他们将球面模式重新表示为单位球面上的离散3D点集，将问题转化为球面点集对齐问题（即3D单位向量的Wahba问题）。作者借鉴了现有工作中的Wahba问题理论基础、球面坐标系统、直方图表示和交叉相关等概念，但创新性地提出了三种新算法（SPMC、FRS和SPMC+FRS）和2D直方图表示方法，利用均值方向和北方向来对齐点集，并通过1D圆形交叉相关来估计旋转。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将球面上的点集转换为2D直方图表示，利用均值方向和北方向对齐点集，使用1D圆形交叉相关来估计旋转，并通过迭代优化提高准确性。整体实现流程（以SPMC算法为例）：1) 计算每个点集的旋转矩阵，使它们的均值方向与北极方向对齐；2) 使用旋转矩阵将点集旋转到与北极对齐的位置；3) 从对齐的点集计算二元直方图；4) 在固定直方图和移动直方图之间执行1D圆形交叉相关，找到最优偏移量；5) 计算所需的旋转矩阵；6) 计算最优旋转矩阵。FRS算法则是迭代优化方法，每一步都计算中间旋转矩阵并更新源点集，直到达到收敛。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出三种新算法（SPMC、FRS和SPMC+FRS）；2) 实现线性时间复杂度O(n)，而现有方法大于O(n^3)；3) 创建'Robust Vector Alignment Dataset'数据集；4) 提出'Centroid Aware Spherical Embedding'方法；5) 提出球面图像转换为球面点云的新方法。相比之前的工作，本文方法在计算效率上显著提升，能够处理高达90%的异常值情况，适用于整个SO(3)旋转空间，并成功扩展到点云配准和球面图像旋转估计等实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种计算复杂度为O(n)的球面点模式旋转估计方法，相比现有方法在速度和准确性上都有显著提升，并能有效处理噪声和异常值情况，同时扩展到了点云配准和球面图像旋转估计等实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing methods for rotation estimation between two spherical($\mathbb{S}^2$) patterns typically rely on spherical cross-correlationmaximization between two spherical function. However, these approaches exhibitcomputational complexities greater than cubic $O(n^3)$ with respect to rotationspace discretization and lack extensive evaluation under significant outliercontamination. To this end, we propose a rotation estimation algorithm betweentwo spherical patterns with linear time complexity $O(n)$. Unlike existingspherical-function-based methods, we explicitly represent spherical patterns asdiscrete 3D point sets on the unit sphere, reformulating rotation estimation asa spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).Given the geometric nature of our formulation, our spherical pattern alignmentalgorithm naturally aligns with the Wahba problem framework for 3D unitvectors. Specifically, we introduce three novel algorithms: (1) SPMC (SphericalPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) ahybrid approach (SPMC+FRS) that combines the advantages of the previous twomethods. Our experiments demonstrate that in the $\mathbb{S}^2$ domain and incorrespondence-free settings, our algorithms are over 10x faster and over 10xmore accurate than current state-of-the-art methods for the Wahba problem withoutliers. We validate our approach through extensive simulations on a newdataset of spherical patterns, the ``Robust Vector Alignment Dataset."Furthermore, we adapt our methods to two real-world tasks: (i) Point CloudRegistration (PCR) and (ii) rotation estimation for spherical images.</description>
      <author>example@mail.com (Anik Sarker, Alan T. Asbeck)</author>
      <guid isPermaLink="false">2508.02339v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2508.03609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为evTransFER的基于迁移学习的框架和架构，用于基于事件相机的面部表情识别，实现了93.6%的识别率，比现有方法提高了25.9个百分点以上。&lt;h4&gt;背景&lt;/h4&gt;事件相机是一种受生物启发的视觉传感器，能够异步捕捉像素强度变化，具有微秒级延迟、高时间分辨率和高动态范围，为场景的时空动态信息提供了有价值的输入。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于迁移学习的框架和架构，用于基于事件相机的面部表情识别。&lt;h4&gt;方法&lt;/h4&gt;设计了一个特征提取器编码面部时空动态，通过在面部重建问题上训练对抗生成方法并将编码器权重迁移到表情识别系统；提出结合LSTM的架构捕获长期表情动态；引入新的基于事件的TIE表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习方法显著提高了面部表情识别能力；在e-CK+数据库上达到93.6%的识别率；与最先进方法相比准确率提高25.9个百分点以上。&lt;h4&gt;结论&lt;/h4&gt;提出的evTransFER框架在基于事件的面部表情识别任务中取得了显著成果，性能明显优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;事件相机是受生物启发的视觉传感器，可异步捕捉像素强度变化，具有微秒级延迟、高时间分辨率和高动态范围，为场景的时空动态信息提供了有价值的输入。在本文工作中，我们提出了evTransFER，一个基于迁移学习的框架和架构，用于基于事件相机的面部表情识别。主要贡献是一个特征提取器，用于编码面部的时空动态，通过在一个不同问题（面部重建）上训练对抗生成方法，然后将训练好的编码器权重迁移到面部表情识别系统。我们表明，与从头开始训练网络相比，这种提出的迁移学习方法大大提高了识别面部表情的能力。此外，我们提出了一种结合LSTM的架构，用于捕获长期的面部表情动态，并引入了一种新的基于事件的表示方法，称为TIE，这些都进一步提高了结果。我们在基于事件的面部表情数据库e-CK+上评估了提出的框架，并将其与最先进的方法进行比较。结果表明，提出的evTransFER框架在e-CK+数据库上实现了93.6%的识别率，与类似问题中最先进的性能相比，显著提高了准确性（25.9个百分点或更多）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event-based cameras are bio-inspired vision sensors that asynchronouslycapture per-pixel intensity changes with microsecond latency, high temporalresolution, and high dynamic range, providing valuable information about thespatio-temporal dynamics of the scene. In the present work, we proposeevTransFER, a transfer learning-based framework and architecture for faceexpression recognition using event-based cameras. The main contribution is afeature extractor designed to encode the spatio-temporal dynamics of faces,built by training an adversarial generative method on a different problem(facial reconstruction) and then transferring the trained encoder weights tothe face expression recognition system. We show that this proposed transferlearning method greatly improves the ability to recognize facial expressionscompared to training a network from scratch. In addition, we propose anarchitecture that incorporates an LSTM to capture longer-term facial expressiondynamics, and we introduce a new event-based representation, referred to asTIE, both of which further improve the results. We evaluate the proposedframework on the event-based facial expression database e-CK+ and compare it tostate-of-the-art methods. The results show that the proposed frameworkevTransFER achieves a 93.6\% recognition rate on the e-CK+ database,significantly improving the accuracy (25.9\% points or more) when compared tostate-of-the-art performance for similar problems.</description>
      <author>example@mail.com (Rodrigo Verschae, Ignacio Bugueno-Cordova)</author>
      <guid isPermaLink="false">2508.03609v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study</title>
      <link>http://arxiv.org/abs/2508.01352v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 4 figures and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度学习的框架，用于从H&amp;E染色全幻灯图像预测EGFR突变状态，结合视觉变换器病理基础模型和基于注意力的多实例学习架构，在印度队列和TCGA数据集上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;肺腺癌是非小细胞肺癌的一种亚型，约46%的LUAD患者存在EGFR基因突变，这类患者可使用特定酪氨酸激酶抑制剂治疗。东南亚人群EGFR突变发生率显著高于高加索人(39-64% vs 7-22%)。H&amp;E染色全幻灯成像常规用于癌症分期和分型筛查，AI模型在癌症检测和分类方面显示出良好前景。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习框架来预测EGFR突变状态，以辅助临床决策。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于视觉变换器病理基础模型和基于注意力的多实例学习架构的深度学习框架。使用印度队列(170个WSI)的数据进行训练，并在两个独立数据集上评估：内部测试集(印度队列中的30个WSI)和外部测试集(TCGA的86个WSI)。&lt;h4&gt;主要发现&lt;/h4&gt;模型在两个数据集上均表现出一致的性能，内部测试集AUC为0.933(±0.010)，外部测试集AUC为0.965(±0.015)。该框架可在小数据集上高效训练，与先前研究相比实现了优越性能，不受训练领域限制。&lt;h4&gt;结论&lt;/h4&gt;研究证明了使用常规病理幻灯片准确预测EGFR突变状态的可行性，特别是在资源有限条件下，使用基础模型和基于注意力的多实例学习方法。&lt;h4&gt;翻译&lt;/h4&gt;肺腺癌(LUAD)是非小细胞肺癌(NSCLC)的一种亚型。EGFR基因突变的LUAD约占LUAD病例的46%。携带EGFR突变的患者可以用特异性酪氨酸激酶抑制剂(TKIs)治疗。因此，预测EGFR突变状态有助于临床决策。H&amp;E染色全幻灯成像(WSI)是常规进行的癌症分期和分型筛查程序，特别是对东南亚人群，其突变发生率明显高于高加索人(39-64%比7-22%)。最近AI模型的进展在癌症检测和分类方面显示出有希望的结果。在本研究中，我们提出了一种基于视觉变换器(ViT)病理基础模型和基于注意力的多实例学习(ABMIL)架构的深度学习(DL)框架，用于从H&amp;E WSI预测EGFR突变状态。开发的管道使用印度队列(170个WSI)的数据进行训练，并在两个独立数据集上进行评估：内部测试集(印度队列中的30个WSI)和来自TCGA的外部测试集(86个WSI)。模型在两个数据集上均表现出一致的性能，内部和外部测试集的AUC分别为0.933(±0.010)和0.965(±0.015)。该框架可以在小数据集上高效训练，与先前研究相比实现了优越的性能，无论训练领域如何。当前研究证明了使用常规病理幻灯片准确预测EGFR突变状态的可行性，特别是在资源有限的条件下，使用基础模型和基于注意力的多实例学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung adenocarcinoma (LUAD) is a subtype of non-small cell lung cancer(NSCLC). LUAD with mutation in the EGFR gene accounts for approximately 46% ofLUAD cases. Patients carrying EGFR mutations can be treated with specifictyrosine kinase inhibitors (TKIs). Hence, predicting EGFR mutation status canhelp in clinical decision making. H&amp;E-stained whole slide imaging (WSI) is aroutinely performed screening procedure for cancer staging and subtyping,especially affecting the Southeast Asian populations with significantly higherincidence of the mutation when compared to Caucasians (39-64% vs 7-22%). Recentprogress in AI models has shown promising results in cancer detection andclassification. In this study, we propose a deep learning (DL) framework builton vision transformers (ViT) based pathology foundation model andattention-based multiple instance learning (ABMIL) architecture to predict EGFRmutation status from H&amp;E WSI. The developed pipeline was trained using datafrom an Indian cohort (170 WSI) and evaluated across two independent datasets:Internal test (30 WSI from Indian cohort) set, and an external test set fromTCGA (86 WSI). The model shows consistent performance across both datasets,with AUCs of 0.933 (+/-0.010), and 0.965 (+/-0.015) for the internal andexternal test sets respectively. This proposed framework can be efficientlytrained on small datasets, achieving superior performance as compared toseveral prior studies irrespective of training domain. The current studydemonstrates the feasibility of accurately predicting EGFR mutation statususing routine pathology slides, particularly in resource-limited settings usingfoundation models and attention-based multiple instance learning.</description>
      <author>example@mail.com (Sagar Singh Gwal, Rajan, Suyash Devgan, Shraddhanjali Satapathy, Abhishek Goyal, Nuruddin Mohammad Iqbal, Vivaan Jain, Prabhat Singh Mallik, Deepali Jain, Ishaan Gupta)</author>
      <guid isPermaLink="false">2508.01352v2</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MPCA-based Domain Adaptation for Transfer Learning in Ultrasonic Guided Waves</title>
      <link>http://arxiv.org/abs/2508.02726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于多线性主成分分析(MPCA)的新型迁移学习框架，用于解决超声导波结合机器学习在结构健康监测中面临的数据稀缺和泛化能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;超声导波是薄壁结构健康监测中有前景的诊断工具，与机器学习结合可实现实时监测。然而，基于超声导波的机器学习方法大规模部署受数据稀缺限制，且在不同材料和传感器配置间泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;解决基于超声导波的机器学习方法在数据稀缺和跨材料/传感器配置泛化能力有限的问题，开发有效的域适应迁移学习框架。&lt;h4&gt;方法&lt;/h4&gt;首先训练用于回归的卷积神经网络实现平板结构损伤定位，然后结合多线性主成分分析和微调技术使CNN适应不同平板。通过对源域和目标域联合应用MPCA提取共享特征，实现无需预先假设维度的有效域适应。最后在12个涉及不同复合材料和传感器阵列的案例中测试该方法。&lt;h4&gt;主要发现&lt;/h4&gt;统计指标显示MPCA应用前后域对齐情况显著改善，与标准迁移学习技术相比，所提方法大幅降低了定位误差，在多种复合材料和传感器配置的案例中表现出色。&lt;h4&gt;结论&lt;/h4&gt;提出的方法是一种鲁棒、数据高效且基于统计的迁移学习框架，适用于超声导波为基础的结构健康监测应用。&lt;h4&gt;翻译&lt;/h4&gt;超声导波代表了一种用于薄壁结构健康监测的有前景的诊断工具，它们与机器学习算法的结合日益增多，以实现实时监测能力。然而，基于超声导波的机器学习方法的大规模部署受到数据稀缺和跨不同材料及传感器配置泛化能力有限的约束。为解决这些局限性，这项工作提出了一种基于多线性主成分分析的新型迁移学习框架。首先，训练一个用于回归的卷积神经网络来执行平板结构的损伤定位。然后，结合多线性和微调使CNN能够应用于不同的平板。通过对源域和目标域联合应用多线性，该方法提取共享的潜在特征，实现了无需预先假设维度的有效域适应。在多线性之后，微调使预训练的CNN能够适应新域，而无需大量训练数据。所提出的多线性基础迁移学习方法在12个涉及不同复合材料和传感器阵列的案例研究中进行了测试。使用统计指标评估了多线性前后的域对齐情况，结果表明与标准迁移学习技术相比，定位误差显著降低。因此，所提出的方法成为一种适用于超声导波为基础的健康监测的鲁棒、数据高效和基于统计的迁移学习框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultrasonic Guided Waves (UGWs) represent a promising diagnostic tool forStructural Health Monitoring (SHM) in thin-walled structures, and theirintegration with machine learning (ML) algorithms is increasingly being adoptedto enable real-time monitoring capabilities. However, the large-scaledeployment of UGW-based ML methods is constrained by data scarcity and limitedgeneralisation across different materials and sensor configurations. To addressthese limitations, this work proposes a novel transfer learning (TL) frameworkbased on Multilinear Principal Component Analysis (MPCA). First, aConvolutional Neural Network (CNN) for regression is trained to perform damagelocalisation for a plated structure. Then, MPCA and fine-tuning are combined tohave the CNN work for a different plate. By jointly applying MPCA to the sourceand target domains, the method extracts shared latent features, enablingeffective domain adaptation without requiring prior assumptions aboutdimensionality. Following MPCA, fine-tuning enables adapting the pre-trainedCNN to a new domain without the need for a large training dataset. The proposedMPCA-based TL method was tested against 12 case studies involving differentcomposite materials and sensor arrays. Statistical metrics were used to assessdomains alignment both before and after MPCA, and the results demonstrate asubstantial reduction in localisation error compared to standard TL techniques.Hence, the proposed approach emerges as a robust, data-efficient, andstatistically based TL framework for UGW-based SHM.</description>
      <author>example@mail.com (Lucio Pinello, Francesco Cadini, Luca Lomazzi)</author>
      <guid isPermaLink="false">2508.02726v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Transfer Learning Methods on Real-World Data Streams: A Case Study in Financial Fraud Detection</title>
      <link>http://arxiv.org/abs/2508.02702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures, submitted to ECML PKDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种数据操作框架，用于模拟随时间变化的数据可用性场景，支持迁移学习算法在动态环境下的更真实评估和比较。该框架通过重新采样数据集创建多个领域，并应用真实的领域转换引入领域间变异性，从而模拟大量现实的实验变体。&lt;h4&gt;背景&lt;/h4&gt;当目标领域的数据有限时，迁移学习方法可用于在相关数据丰富的领域开发模型后部署到目标领域。然而，这些方法通常基于特定的静态假设设计，与许多实际应用中数据和标签可用性随时间变化的情况不符。现有评估方法在静态假设下进行，导致对模型实际性能的不切实际期望。&lt;h4&gt;目的&lt;/h4&gt;为了支持迁移学习算法和模型更真实、实际的评估和比较，提出一个数据操作框架，该框架能够模拟随时间变化的数据可用性场景，创建多个领域，并通过应用真实的领域转换引入领域间变异性。&lt;h4&gt;方法&lt;/h4&gt;提出一个数据操作框架，具有三种能力：(1)模拟随时间变化的数据可用性场景，(2)通过重新采样给定数据集创建多个领域，(3)通过应用真实的领域转换（如创建时间协变量和概念偏移）引入领域间变异性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在专有卡支付数据集和公开的银行账户欺诈(BAF)数据集上进行案例研究，证明了框架的有用性。该框架通过提供随时间和现实数据可用性场景下评估迁移学习方法的能力，促进了对模型和算法行为的理解。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过模拟大量现实的实验变体，提供了关于算法在动态环境中潜在行为的更多信息，支持对迁移学习算法更真实、实际的评估和比较，有助于在实际环境中为新领域部署模型时做出更好的决策。&lt;h4&gt;翻译&lt;/h4&gt;当目标领域的数据有限时，可以使用迁移学习(TL)方法在相关数据丰富的领域开发模型，然后在目标领域部署。然而，这些迁移学习方法通常基于对可用标记和未标记目标数据数量的特定、静态假设设计。这与许多实际应用形成对比，因为在实际应用中，数据和相应标签的可用性会随时间变化。由于迁移学习方法的评估通常也在相同静态数据可用性假设下进行，这会导致对其在实际环境中性能的不切实际的期望。为了支持迁移学习算法和模型更真实、实际的评估和比较，我们提出了一种数据操作框架，该框架(1)模拟随时间变化的数据可用性场景，(2)通过重新采样给定数据集创建多个领域，(3)通过应用真实的领域转换（例如创建各种可能的时间协变量和概念偏移）引入领域间变异性。这些能力使能够模拟大量现实的实验变体，进而提供有关算法在动态环境中部署时潜在行为的更多信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When the available data for a target domain is limited, transfer learning(TL) methods can be used to develop models on related data-rich domains, beforedeploying them on the target domain. However, these TL methods are typicallydesigned with specific, static assumptions on the amount of available labeledand unlabeled target data. This is in contrast with many real worldapplications, where the availability of data and corresponding labels variesover time. Since the evaluation of the TL methods is typically also performedunder the same static data availability assumptions, this would lead tounrealistic expectations concerning their performance in real world settings.To support a more realistic evaluation and comparison of TL algorithms andmodels, we propose a data manipulation framework that (1) simulates varyingdata availability scenarios over time, (2) creates multiple domains throughresampling of a given dataset and (3) introduces inter-domain variability byapplying realistic domain transformations, e.g., creating a variety ofpotentially time-dependent covariate and concept shifts. These capabilitiesenable simulation of a large number of realistic variants of the experiments,in turn providing more information about the potential behavior of algorithmswhen deployed in dynamic settings. We demonstrate the usefulness of theproposed framework by performing a case study on a proprietary real-world suiteof card payment datasets. Given the confidential nature of the case study, wealso illustrate the use of the framework on the publicly available Bank AccountFraud (BAF) dataset. By providing a methodology for evaluating TL methods overtime and in realistic data availability scenarios, our framework facilitatesunderstanding of the behavior of models and algorithms. This leads to betterdecision making when deploying models for new domains in real-worldenvironments.</description>
      <author>example@mail.com (Ricardo Ribeiro Pereira, Jacopo Bono, Hugo Ferreira, Pedro Ribeiro, Carlos Soares, Pedro Bizarro)</author>
      <guid isPermaLink="false">2508.02702v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation</title>
      <link>http://arxiv.org/abs/2507.21455v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in the Proceedings of the International Conference on  Learning Representations (ICLR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'自监督数据集蒸馏'的方法，通过创新参数化、预增强处理和轻量级网络建模技术，有效提取真实数据集信息，产生具有更好跨架构泛化能力的压缩数据集，在多个实验中表现出优越性。&lt;h4&gt;背景&lt;/h4&gt;大型深度模型的训练需要更大的数据集，但数据集规模的快速增长带来了显著的训练成本挑战，甚至导致计算费用过高。&lt;h4&gt;目的&lt;/h4&gt;旨在将图像及其自监督训练的表示蒸馏成一个压缩的数据集，有效提取真实数据集的丰富信息，产生具有更好跨架构泛化能力的蒸馏数据集。&lt;h4&gt;方法&lt;/h4&gt;提出了三种新技术：1) 通过不同的低维基对图像和表示进行创新参数化；2) 通过预增强处理解决数据增强随机性引起的不稳定性问题；3) 利用轻量级网络建模同一图像增强视图表示之间的连接，产生更紧凑的蒸馏对。&lt;h4&gt;主要发现&lt;/h4&gt;在各种数据集上进行的广泛实验验证了该方法在蒸馏效率、跨架构泛化和迁移学习性能方面的优越性。&lt;h4&gt;结论&lt;/h4&gt;自监督数据集蒸馏是一种有效的方法，可以提取真实数据集的丰富信息，所提出的技术能够更忠实和紧凑地保留原始数据集的关键特征，在多个方面表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型数据集对于训练大型深度模型至关重要，但数据集规模的快速增长带来了显著的训练成本挑战，甚至导致计算费用过高。数据集蒸馏最近成为一种流行技术，通过学习高度紧凑的代表样本集来减少数据集大小，其中用这些样本训练的理想模型应与使用完整数据集训练的模型具有相当的性能。然而，大多数现有的数据集蒸馏工作都集中在监督数据集上，而我们则旨在将图像及其自监督训练的表示蒸馏成一个压缩集。这一过程被称为自监督数据集蒸馏，有效提取了真实数据集的丰富信息，产生了具有增强跨架构泛化能力的压缩集。特别是，为了更忠实和紧凑地保留原始数据集的关键特征，我们提出了几种新技术：1) 我们通过不同的低维基对图像和表示进行创新参数化，实验表明基的选择起着关键作用；2) 我们通过预增强处理解决了自监督学习中关键组件但被先前自监督数据集蒸馏工作低估的数据增强随机性引起的不稳定性问题；3) 我们进一步利用轻量级网络建模同一图像增强视图表示之间的连接，产生更紧凑的蒸馏对。在各种数据集上进行的广泛实验验证了我们的方法在蒸馏效率、跨架构泛化和迁移学习性能方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although larger datasets are crucial for training large deep models, therapid growth of dataset size has brought a significant challenge in terms ofconsiderable training costs, which even results in prohibitive computationalexpenses. Dataset Distillation becomes a popular technique recently to reducethe dataset size via learning a highly compact set of representative exemplars,where the model trained with these exemplars ideally should have comparableperformance with respect to the one trained with the full dataset. While mostof existing works upon dataset distillation focus on supervised datasets, weinstead aim to distill images and their self-supervisedly trainedrepresentations into a distilled set. This procedure, named as Self-SupervisedDataset Distillation, effectively extracts rich information from real datasets,yielding the distilled sets with enhanced cross-architecture generalizability.Particularly, in order to preserve the key characteristics of original datasetmore faithfully and compactly, several novel techniques are proposed: 1) weintroduce an innovative parameterization upon images and representations viadistinct low-dimensional bases, where the base selection for parameterizationis experimentally shown to play a crucial role; 2) we tackle the instabilityinduced by the randomness of data augmentation -- a key component inself-supervised learning but being underestimated in the prior work ofself-supervised dataset distillation -- by utilizing predeterminedaugmentations; 3) we further leverage a lightweight network to model theconnections among the representations of augmented views from the same image,leading to more compact pairs of distillation. Extensive experiments conductedon various datasets validate the superiority of our approach in terms ofdistillation efficiency, cross-architecture generalization, and transferlearning performance.</description>
      <author>example@mail.com (Sheng-Feng Yu, Jia-Jiun Yao, Wei-Chen Chiu)</author>
      <guid isPermaLink="false">2507.21455v2</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Minimal Convolutional RNNs Accelerate Spatiotemporal Learning</title>
      <link>http://arxiv.org/abs/2508.03614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICANN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MinConvLSTM和MinConvGRU两种新型时空模型，结合了卷积网络的空间特征和最小可并行RNN的训练效率，实现了完全并行训练并保留了局部空间建模能力，在训练速度和预测精度上都优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;传统ConvRNN模型在教师强制训练过程中需要顺序隐藏状态更新，这是一个主要瓶颈，限制了训练效率。&lt;h4&gt;目的&lt;/h4&gt;开发结合卷积网络空间归纳偏置和最小可并行RNN训练效率的时空模型，消除顺序更新的需求，提高训练效率。&lt;h4&gt;方法&lt;/h4&gt;将MinLSTM和MinGRU的对数域前缀和公式扩展到卷积架构，实现完全并行训练；在MinConvLSTM中融入受xLSTM启发的指数门控机制，简化对数域计算。&lt;h4&gt;主要发现&lt;/h4&gt;在Navier-Stokes动力学和实际地势数据两个时空预测任务上，新模型在训练速度上显著优于标准ConvLSTMs和ConvGRUs，同时实现了更低的预测误差，即使在闭环自回归模式下也是如此。&lt;h4&gt;结论&lt;/h4&gt;最小化的循环结构与卷积输入相结合为时空序列建模提供了高效且有吸引力的选择，弥合了循环简单性和空间复杂性之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了MinConvLSTM和MinConvGRU，两种结合了卷积循环网络空间归纳偏置与最小可并行化RNN训练效率的新型时空模型。我们的方法将MinLSTM和MinGRU的对数域前缀和公式扩展到卷积架构，实现了完全并行训练，同时保留了局部空间建模能力。这消除了教师强制训练过程中顺序隐藏状态更新的需求——这是传统ConvRNN模型的主要瓶颈。此外，我们在MinConvLSTM中融入了受xLSTM架构启发的指数门控机制，进一步简化了对数域计算。我们的模型结构最小化且计算效率高，参数数量减少，可扩展性提高。我们在两个时空预测任务上评估了我们的模型：Navier-Stokes动力学和实际地势数据。在训练速度方面，我们的架构显著优于标准的ConvLSTMs和ConvGRUs。此外，我们的模型在两个领域中都实现了更低的预测误差，即使在闭环自回归模式下也是如此。这些发现表明，当与卷积输入聚合相结合时，最小化的循环结构为时空序列建模提供了一个高效且有吸引力的选择，弥合了循环简单性和空间复杂性之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MinConvLSTM and MinConvGRU, two novel spatiotemporal models thatcombine the spatial inductive biases of convolutional recurrent networks withthe training efficiency of minimal, parallelizable RNNs. Our approach extendsthe log-domain prefix-sum formulation of MinLSTM and MinGRU to convolutionalarchitectures, enabling fully parallel training while retaining localizedspatial modeling. This eliminates the need for sequential hidden state updatesduring teacher forcing - a major bottleneck in conventional ConvRNN models. Inaddition, we incorporate an exponential gating mechanism inspired by the xLSTMarchitecture into the MinConvLSTM, which further simplifies the log-domaincomputation. Our models are structurally minimal and computationally efficient,with reduced parameter count and improved scalability. We evaluate our modelson two spatiotemporal forecasting tasks: Navier-Stokes dynamics and real-worldgeopotential data. In terms of training speed, our architectures significantlyoutperform standard ConvLSTMs and ConvGRUs. Moreover, our models also achievelower prediction errors in both domains, even in closed-loop autoregressivemode. These findings demonstrate that minimal recurrent structures, whencombined with convolutional input aggregation, offer a compelling and efficientalternative for spatiotemporal sequence modeling, bridging the gap betweenrecurrent simplicity and spatial complexity.</description>
      <author>example@mail.com (Coşku Can Horuz, Sebastian Otte, Martin V. Butz, Matthias Karlbauer)</author>
      <guid isPermaLink="false">2508.03614v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways</title>
      <link>http://arxiv.org/abs/2508.03672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Inland-LOAM，一种专门用于内河水道环境的激光雷达SLAM框架，通过改进特征提取和水表面平面约束解决垂直漂移问题，并实现从3D点云到2D语义地图的转换，提供实时导航参数计算和海岸线提取功能。&lt;h4&gt;背景&lt;/h4&gt;准确的地理空间信息对安全、自主的内河运输至关重要，但现有航道图缺乏实时细节，传统激光雷达SLAM在水道环境中表现不佳，导致垂直漂移和非语义地图问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对内河水道环境的激光雷达SLAM框架，提高定位精度，生成语义地图，并提取海岸线，为自主内河运输提供可靠的地理空间信息。&lt;h4&gt;方法&lt;/h4&gt;提出Inland-LOAM框架，采用改进的特征提取技术，应用水表面平面约束减轻垂直漂移；使用基于体素的几何分析将3D点云转换为结构化2D语义地图；开发自动模块提取海岸线并导出为IENC兼容的轻量级格式。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上评估显示，Inland-LOAM比现有最先进方法实现更高定位精度；生成的语义地图和海岸线与真实世界条件一致，为增强态势感知提供可靠数据。&lt;h4&gt;结论&lt;/h4&gt;Inland-LOAM有效解决了内河水道环境中的SLAM挑战，为自主内河运输提供了可靠的地理空间信息支持，代码和数据集将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;准确的地理空间信息对安全、自主的内河运输至关重要，因为现有航道图(IENC)缺乏实时细节，而传统激光雷达SLAM在水道环境中表现不佳。这些挑战导致垂直漂移和非语义地图，阻碍了自主导航。本文介绍了Inland-LOAM，一种用于水道的激光雷达SLAM框架。它使用改进的特征提取和水表面平面约束来减轻垂直漂移。新的管道使用基于体素的几何分析将3D点云转换为结构化的2D语义地图，能够实时计算桥梁净空高度等导航参数。自动模块提取海岸线并将其导出为轻量级、IENC兼容的格式。在真实世界数据集上的评估显示，Inland-LOAM比最先进方法实现了更高的定位精度。生成的语义地图和海岸线与真实世界条件一致，为增强态势感知提供了可靠数据。代码和数据集将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决内陆水道环境中的高精度定位和语义地图构建问题。传统电子导航图缺乏实时细节，而现有LiDAR SLAM方法难以处理内陆水道的动态岸线和反光水面等特殊环境，导致定位漂移和地图缺乏语义信息。这个问题很重要，因为内陆水道运输是欧洲关键货运网络，但导航存在安全风险，自动化船只需要准确的环境信息来确保安全和提高效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了内陆水道环境的特殊挑战，包括动态岸线和缺乏结构化几何特征等。然后评估了现有LiDAR SLAM方法如LOAM和LeGO-LOAM的局限性，针对这些不足设计了专门改进。方法确实借鉴了现有工作，如基于LOAM的里程计框架、概率占据格子概念、RANSAC平面拟合和图优化技术，但针对水道环境进行了特别优化，如使用水面作为全局约束和改进的特征提取方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用水面作为全局平面约束来减少垂直漂移，并通过几何分析将3D点云转换为结构化的语义地图。整体流程包括：1)预处理和平面拟合，过滤船只自身点并拟合水面平面；2)改进的特征提取，使用方差方法区分平滑和纹理表面；3)两阶段LiDAR里程计和映射，结合水面约束进行联合优化；4)将点云转换为体素网格并构建2D语义地图，分类为水平平面、垂直平面等结构类别；5)提取岸线和桥梁特征，生成IENC兼容格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)改进的特征提取方法，使用方差量化局部变化，更有效区分表面特征；2)将水面作为全局平面约束的联合优化，显著减少垂直漂移；3)基于体素的结构语义映射，通过几何分析生成包含导航意义的分类地图；4)自动地图转换模块，提取岸线并导出为IENC兼容格式。相比之前工作，不同之处在于专门针对内陆水道环境设计，使用水面而非地面作为约束，生成结构语义地图而非简单点云，并直接与导航标准兼容。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Inland-LOAM通过专门为内陆水道环境设计的改进特征提取、水面约束优化和基于体素的语义映射方法，实现了高精度定位和结构化语义地图生成，为自主水面航行器提供了可靠且最新的地理空间信息。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate geospatial information is crucial for safe, autonomous InlandWaterway Transport (IWT), as existing charts (IENC) lack real-time detail andconventional LiDAR SLAM fails in waterway environments. These challenges leadto vertical drift and non-semantic maps, hindering autonomous navigation.  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. Ituses an improved feature extraction and a water surface planar constraint tomitigate vertical drift. A novel pipeline transforms 3D point clouds intostructured 2D semantic maps using voxel-based geometric analysis, enablingreal-time computation of navigational parameters like bridge clearances. Anautomated module extracts shorelines and exports them into a lightweight,IENC-compatible format.  Evaluations on a real-world dataset show Inland-LOAM achieves superiorlocalization accuracy over state-of-the-art methods. The generated semanticmaps and shorelines align with real-world conditions, providing reliable datafor enhanced situational awareness. The code and dataset will be publiclyavailable</description>
      <author>example@mail.com (Zhongbi Luo, Yunjia Wang, Jan Swevers, Peter Slaets, Herman Bruyninckx)</author>
      <guid isPermaLink="false">2508.03672v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation</title>
      <link>http://arxiv.org/abs/2508.03526v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了CollaBot框架，用于机器人协作操作大型物体。该框架包括场景分割、协作抓取规划和无碰撞轨迹生成，能够在不同数量机器人和各种任务中实现协作操作。&lt;h4&gt;背景&lt;/h4&gt;机器人学研究的一个重要课题是机器人系统如何与物理世界交互。传统操作任务主要针对小型物体，但在工厂或家庭环境中，经常需要移动大型物体（如桌子），这些任务通常需要多机器人系统协作完成。先前的研究缺乏能够扩展到任意大小机器人并推广到各种任务的框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的协作操作框架，能够处理不同数量机器人和各种任务的大型物体操作。&lt;h4&gt;方法&lt;/h4&gt;1. 使用SEEM进行场景分割和目标物体的点云提取；2. 提出协作抓取框架，将任务分解为局部抓取姿态生成和全局协作；3. 设计两阶段规划模块，生成无碰撞轨迹以完成任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在不同数量机器人和各种物体、任务条件下，该框架的成功率达到52%，证明了所提出框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;CollaBot框架是一个通用的同时协作操作框架，能够有效处理多机器人协作操作大型物体的任务。&lt;h4&gt;翻译&lt;/h4&gt;机器人学中的一个中心研究课题是如何利用该系统与物理世界交互。传统的操作任务主要关注小型物体。然而，在工厂或家庭环境中，经常需要移动大型物体，例如移动桌子。这些任务通常需要多机器人系统协同工作。先前的研究缺乏能够扩展到任意大小机器人并推广到各种任务的框架。在这项工作中，我们提出了CollaBot，一个用于同时协作操作的通用框架。首先，我们使用SEEM进行场景分割和目标物体的点云提取。然后，我们提出了一个协作抓取框架，将任务分解为局部抓取姿态生成和全局协作。最后，我们设计了一个两阶段规划模块，可以生成无碰撞轨迹来完成任务。实验表明，在不同数量机器人和各种物体、任务条件下，成功率达到52%，表明了所提出框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central research topic in robotics is how to use this system to interactwith the physical world. Traditional manipulation tasks primarily focus onsmall objects. However, in factory or home environments, there is often a needfor the movement of large objects, such as moving tables. These tasks typicallyrequire multi-robot systems to work collaboratively. Previous research lacks aframework that can scale to arbitrary sizes of robots and generalize to variouskinds of tasks. In this work, we propose CollaBot, a generalist framework forsimultaneous collaborative manipulation. First, we use SEEM for scenesegmentation and point cloud extraction of the target object. Then, we proposea collaborative grasping framework, which decomposes the task into local grasppose generation and global collaboration. Finally, we design a 2-stage planningmodule that can generate collision-free trajectories to achieve this task.Experiments show a success rate of 52% across different numbers of robots,objects, and tasks, indicating the effectiveness of the proposed framework.</description>
      <author>example@mail.com (Kun Song, Shentao Ma, Gaoming Chen, Ninglong Jin, Guangbao Zhao, Mingyu Ding, Zhenhua Xiong, Jia Pan)</author>
      <guid isPermaLink="false">2508.03526v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>A Closed-Loop Multi-Agent Framework for Aerodynamics-Aware Automotive Styling Design</title>
      <link>http://arxiv.org/abs/2508.03370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型的多智能体框架，用于自动化汽车外部设计工作流程，平衡美学与空气动力学性能，加速开发周期。&lt;h4&gt;背景&lt;/h4&gt;汽车外部设计的核心挑战是在主观美学和客观空气动力学性能之间取得平衡，同时显著加速开发周期。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的、由大型语言模型驱动的多智能体框架，自动化从模糊需求到3D概念模型性能验证的端到端工作流程。&lt;h4&gt;方法&lt;/h4&gt;工作流程分为两个阶段：概念生成阶段（智能体协作解释模糊设计需求，生成概念草图，使用扩散模型生成逼真渲染图）和性能验证阶段（将渲染图转换为3D点云，基于轻量级代理模型的阻力预测智能体提供阻力系数和压力场的即时预测，替代耗时的CFD模拟）。&lt;h4&gt;主要发现&lt;/h4&gt;将创意生成与快速工程验证循环无缝集成到一个统一的自动化系统中。&lt;h4&gt;结论&lt;/h4&gt;为在设计的最早阶段高效平衡创意探索与工程约束提供了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;汽车外部设计的核心挑战在于平衡主观美学与客观空气动力学性能，同时显著加速开发周期。为此，我们提出了一种新颖的、由大型语言模型驱动的多智能体框架，自动化了从模糊需求到3D概念模型性能验证的端到端工作流程。工作流程分为两个阶段：概念生成和性能验证。在第一阶段，智能体协作解释模糊设计需求，生成概念草图，并使用扩散模型生成逼真渲染图。在第二阶段，将渲染图转换为3D点云，基于轻量级代理模型的阻力预测智能体提供阻力系数和压力场的即时预测，替代耗时的CFD模拟。这项工作的主要贡献是将创意生成与快速工程验证循环无缝集成到一个统一的自动化系统中，为在设计的最早阶段高效平衡创意探索与工程约束提供了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The core challenge in automotive exterior design is balancing subjectiveaesthetics with objective aerodynamic performance while dramaticallyaccelerating the development cycle. To address this, we propose a novel,LLM-driven multi-agent framework that automates the end-to-end workflow fromambiguous requirements to 3D concept model performance validation. The workflowis structured in two stages: conceptual generation and performance validation.In the first stage, agents collaborate to interpret fuzzy design requirements,generate concept sketches, and produce photorealistic renderings usingdiffusion models. In the second stage, the renderings are converted to 3D pointclouds, where a Drag Prediction Agent, built upon a lightweight surrogatemodel, provides near-instantaneous predictions of the drag coefficient andpressure fields, replacing time-consuming CFD simulations. The primarycontribution of this work is the seamless integration of creative generationwith a rapid engineering validation loop within a unified, automated system,which provides a new paradigm for efficiently balancing creative explorationwith engineering constraints in the earliest stages of design.</description>
      <author>example@mail.com (Xinyu Jin, Shengmao Yan, Qingtao Wang, Shisong Deng, Yanzhen Jiang, Shuangyao Zhao)</author>
      <guid isPermaLink="false">2508.03370v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Advancing Precision in Multi-Point Cloud Fusion Environments</title>
      <link>http://arxiv.org/abs/2508.03179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted for publication in Communications in Computer and  Information Science, Springer&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究专注于视觉工业检测领域，通过评估点云技术和多点云匹配方法，提出了一种新的合成数据集和CloudCompare插件，以提高自动化检测系统的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;视觉工业检测是现代制造业中的重要环节，而点云技术作为一种三维数据处理方法，在工业检测中具有广泛应用。然而，点云数据的处理和比较仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过改进点云处理方法，提高工业视觉检测的准确性和效率，同时提供标准化的评估工具。&lt;h4&gt;方法&lt;/h4&gt;研究采用了点云评估和多点云匹配方法，引入了合成数据集用于定量评估，并开发了CloudCompare插件用于点云合并和表面缺陷可视化。&lt;h4&gt;主要发现&lt;/h4&gt;研究提出了一个合成数据集用于评估配准方法和距离度量，以及一个新颖的CloudCompare插件，能够有效合并多个点云并可视化表面缺陷。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的数据集和工具，研究显著提高了自动化工业检测系统的准确性和效率，为视觉工业检测领域提供了有价值的贡献。&lt;h4&gt;翻译&lt;/h4&gt;这项研究专注于通过评估点云和多点云匹配方法进行视觉工业检测。我们还引入了一个合成数据集，用于定量评估配准方法和各种点云比较的距离度量。此外，我们提出了一种新颖的CloudCompare插件，用于合并多个点云和可视化表面缺陷，从而提高自动化检测系统的准确性和效率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多点云融合环境中的精度问题，特别是改进点云配准方法和开发相关工具。这个问题在现实中非常重要，因为制造业快速发展需要更高效的质量控制，而目前人工检测过程缓慢、不可靠且成本高。自动化检测系统可以更快、更一致地工作，从长远来看需要更少投资，帮助企业提高制造质量标准。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云获取和配准方法，包括激光和相机-based方法，特别关注了ICP算法及其变体Global ICP和Pose Graph，以及深度学习方法。他们借鉴了现有工作，但发现传统Pose Graph方法使用点对平面ICP不估计法向量，可能导致错误对齐。因此，他们设计了一种改进方法，使用Generalized ICP考虑表面法线信息，确定平面间距离而非点对平面距离，从而提高配准精度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是改进Pose Graph配准方法，通过考虑点云表面法线信息来提高配准精度，确保只有具有相似法线方向的点被对齐。整体流程包括：1)使用结构光传感器获取物体部分点云；2)预处理去除背景和噪声；3)利用运动学信息进行初始对齐；4)使用改进的Pose Graph方法合并多个部分点云；5)将合并点云与地面真相网格比较进行距离估计；6)可视化距离图以识别缺陷。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)改进的Pose Graph方法(Refined Pose Graph)，考虑表面法线信息提高配准精度；2)参数调查，研究配准参数的选择和相互关系；3)开发专门的CloudCompare插件用于多视图点云配准。相比之前工作，不同之处在于传统方法使用点对平面ICP可能错误对齐，而改进方法使用Generalized ICP考虑法线信息；开发了专门的插件提供用户友好界面；并确定了算法参数间的关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过改进Pose Graph配准方法并开发专门的CloudCompare插件，显著提高了多点云融合环境的精度，为工业检测提供了一种更准确、更高效的自动化解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research focuses on visual industrial inspection by evaluating pointclouds and multi-point cloud matching methods. We also introduce a syntheticdataset for quantitative evaluation of registration method and various distancemetrics for point cloud comparison. Additionally, we present a novelCloudCompare plugin for merging multiple point clouds and visualizing surfacedefects, enhancing the accuracy and efficiency of automated inspection systems.</description>
      <author>example@mail.com (Ulugbek Alibekov, Vanessa Staderini, Philipp Schneider, Doris Antensteiner)</author>
      <guid isPermaLink="false">2508.03179v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Medical Point Cloud Shape Learning: Registration, Reconstruction and Variation</title>
      <link>http://arxiv.org/abs/2508.03057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提供了基于学习的医学点云形状分析的全面系统综述，重点关注配准、重建和变异建模三个基本任务，回顾了2021-2025年的文献，总结了方法、数据集、评估指标，并讨论了临床应用、挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;点云已成为3D医学成像的重要表示方式，是传统体素或网格方法的紧凑且保留表面的替代方案。深度学习的进步使得直接从点云数据中提取、建模和分析解剖形状成为可能。&lt;h4&gt;目的&lt;/h4&gt;提供基于学习的医学点云形状分析的全面和系统综述，总结代表性方法、数据集和评估指标，强调临床应用和医学领域的独特挑战，并指出未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;回顾2021年至2025年的最新文献，总结代表性方法、数据集和评估指标，分析临床应用和医学领域的独特挑战，并探讨当前局限性和未来方向。&lt;h4&gt;主要发现&lt;/h4&gt;主要趋势包括混合表示的集成、大规模自监督模型和生成技术的应用；当前局限性包括数据稀缺、患者间变异性以及临床部署对可解释性和稳健解决方案的需求。&lt;h4&gt;结论&lt;/h4&gt;点云在医学成像中具有广阔应用前景，但需要解决数据稀缺、变异性大等问题，并开发可解释、稳健的解决方案以满足临床需求，未来应继续推进基于点云的形状学习在医学成像中的应用。&lt;h4&gt;翻译&lt;/h4&gt;点云已成为3D医学成像中日益重要的表示方式，提供了传统基于体素或网格方法的紧凑且保留表面的替代方案。深度学习的最新进展使得直接从点云数据中提取、建模和分析解剖形状成为可能。本文提供了基于学习的医学点云形状分析的全面和系统综述，重点关注三个基本任务：配准、重建和变异建模。我们回顾了2021年至2025年的最新文献，总结了代表性方法、数据集和评估指标，并强调了临床应用和医学领域的独特挑战。主要趋势包括混合表示的集成、大规模自监督模型和生成技术。我们还讨论了当前局限性，如数据稀缺、患者间变异性以及临床部署对可解释性和稳健解决方案的需求。最后，概述了推进基于点云的形状学习在医学成像中的未来方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学点云形状学习的系统性综述需求，特别关注配准、重建和变化建模三个核心任务。这个问题很重要，因为点云已成为3D医学成像的重要表示形式，深度学习的进步使得直接从点云数据中提取和分析解剖形状成为可能，但医学领域面临数据稀缺、解剖变异性、跨模态不一致性和临床需求等独特挑战，且缺乏对医学点云形状学习的统一比较概述。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为综述论文，作者遵循PRISMA指南，系统检索了2021-2025年Web of Science和Scopus数据库中的相关文献，使用关键词筛选后纳入35篇研究。作者借鉴了现有文献收集方法，并参考了之前的综述工作，指出之前的综述要么集中在通用点云处理，要么强调基于图像的学习流程，而对医学领域的解剖先验和临床约束关注有限，因此需要专门针对医学点云形状学习的系统性综述。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供对医学点云形状学习的全面和系统概述，特别关注配准、重建和变化建模三个基本任务。整体流程包括：1) 文献收集：遵循PRISMA指南系统检索筛选；2) 内容组织：将方法分为三大类及其子任务；3) 分析比较：分析比较各种任务的最先进方法；4) 讨论挑战：讨论相关挑战和未解决问题；5) 展望未来：确定趋势并提出未来方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次全面覆盖医学成像中的形状学习应用；系统回顾过去五年的医学点云研究；总结最先进方法及其模型、数据集和损失函数；讨论挑战并建议未来方向。相比之前工作，这篇综述专注于医学点云而非通用点云处理；关注三个特定任务而非泛泛点云处理；覆盖最新研究(2021-2025)；强调解剖先验和临床约束；提供专门的医学数据集和评估指标概述。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提供了对医学点云形状学习的全面系统性综述，重点关注配准、重建和变化三个核心任务，为研究人员提供了宝贵的参考框架和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds have become an increasingly important representation for 3Dmedical imaging, offering a compact, surface-preserving alternative totraditional voxel or mesh-based approaches. Recent advances in deep learninghave enabled rapid progress in extracting, modeling, and analyzing anatomicalshapes directly from point cloud data. This paper provides a comprehensive andsystematic survey of learning-based shape analysis for medical point clouds,focusing on three fundamental tasks: registration, reconstruction, andvariation modeling. We review recent literature from 2021 to 2025, summarizerepresentative methods, datasets, and evaluation metrics, and highlightclinical applications and unique challenges in the medical domain. Key trendsinclude the integration of hybrid representations, large-scale self-supervisedmodels, and generative techniques. We also discuss current limitations, such asdata scarcity, inter-patient variability, and the need for interpretable androbust solutions for clinical deployment. Finally, future directions areoutlined for advancing point cloud-based shape learning in medical imaging.</description>
      <author>example@mail.com (Tongxu Zhang, Zhiming Liang, Bei Wang)</author>
      <guid isPermaLink="false">2508.03057v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model</title>
      <link>http://arxiv.org/abs/2508.02858v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MIDAR的LiDAR检测模仿模型，解决了现有自动驾驶仿真工具在感知建模和可扩展性之间的矛盾，能够从微观交通模拟器中生成接近真实的LiDAR检测数据，为协同感知研究提供了有效解决方案。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶技术发展，利用多车协同感知数据增强交通应用的研究日益增多。由于大规模真实AV部署不切实际，仿真成为主要研究方法。基于游戏引擎的模拟器(如CARLA)能生成高保真传感器数据但在多AV场景下可扩展性差，而微观交通模拟器(如SUMO)虽高效扩展却缺乏感知建模能力。&lt;h4&gt;目的&lt;/h4&gt;弥合高保真模拟器与高效可扩展模拟器之间的差距，开发一种能够利用微观交通模拟器已有车辆级特征来近似真实LiDAR检测的模型。&lt;h4&gt;方法&lt;/h4&gt;提出MIDAR模型，基于周围车辆的空间布局和尺寸从理想LiDAR检测结果预测真正例和假负例；构建改进的多视线路径图编码车辆间遮挡关系；采用GRU增强的APPNP架构将特征从自车和遮挡车辆传播到预测目标。&lt;h4&gt;主要发现&lt;/h4&gt;MIDAR在nuScenes AD数据集上达到0.909的AUC，成功近似主流3D LiDAR检测模型CenterPoint的检测结果；两种基于协同感知的交通应用验证了此类真实检测建模的必要性，尤其对需要准确个体车辆观测的任务。&lt;h4&gt;结论&lt;/h4&gt;MIDAR可无缝集成到交通模拟器和轨迹数据集中，为自动驾驶协同感知研究提供了有效工具，将在发表后开源。&lt;h4&gt;翻译&lt;/h4&gt;随着自动驾驶(AD)技术的进步，越来越多的研究开始关注利用多辆自动驾驶汽车(AV)收集的协同感知(CP)数据来增强交通应用。由于大规模真实世界AV部署的不切实际性，仿真已成为大多数研究的主要方法。虽然基于游戏引擎的模拟器(如CARLA)能生成高保真的原始传感器数据(如LiDAR点云)并可用于产生真实的检测结果，但在多AV场景下面临可扩展性挑战。相比之下，微观交通模拟器(如SUMO)能高效扩展但缺乏感知建模能力。为了弥合这一差距，我们提出了MIDAR，一种模仿LiDAR检测的模型，利用微观交通模拟器中现成的车辆级特征来近似真实的LiDAR检测。具体而言，MIDAR基于周围车辆的空间布局和尺寸，从理想的LiDAR检测结果预测真正例(TPs)和假负例(FNs)。构建了一个改进的多视线路径(RM-LoS)图来编码车辆之间的遮挡关系，在此基础上，MIDAR采用GRU增强的APPNP架构，将特征从自车和遮挡车辆传播到预测目标。在nuScenes AD数据集上，MIDAR在近似CenterPoint(主流3D LiDAR检测模型)生成的检测结果方面达到了0.909的AUC。两种基于CP的交通应用进一步验证了此类真实检测建模的必要性，特别是对需要准确个体车辆观测的任务(如位置、速度、车道索引)。如应用所示，MIDAR可以无缝集成到交通模拟器和轨迹数据集中，将在发表后开源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶和交通应用研究中高保真传感器数据生成与大规模仿真效率之间的平衡问题。高保真模拟器(如CARLA)能生成真实传感器数据但难以扩展到多车场景，而微观交通模拟器(如SUMO)虽高效却缺乏真实感知建模能力。这个问题很重要，因为随着自动驾驶技术发展，合作感知(CP)研究需要既高效又真实的仿真环境来评估交通应用，但现有方法在仿真精度和效率之间难以取得平衡。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有模拟器的优缺点，发现高保真模拟器缺乏可扩展性而微观模拟器缺乏感知建模能力。他们借鉴了LoS图概念用于表示车辆间的遮挡关系，以及APPNP架构用于图节点分类。在此基础上，作者设计了改进的多跳LoS图(RM-LoS)来更好地捕捉部分遮挡影响，并引入GRU增强APPNP架构实现更灵活的信息混合。这种方法基于车辆级特征预测检测结果，而非生成原始点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建改进的多跳视线图(RM-LoS)编码车辆遮挡关系，使用图神经网络预测LiDAR检测结果。流程包括：1)构建RM-LoS图，节点表示车辆，边表示视线关系；2)用MLP将节点特征转换为嵌入；3)通过APPNP算法传播特征；4)集成GRU实现灵活信息混合；5)用线性解码器预测车辆是否被检测到；6)使用nuScenes数据集和CenterPoint检测结果训练模型，输出TPs和FNs预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首创的LiDAR检测模拟模型，平衡仿真效率和检测真实性；2)改进的多跳视线图(RM-LoS)，更好捕捉部分遮挡；3)GRU增强的APPNP架构，实现灵活信息混合；4)轻量级即插即用模型，可集成到多种平台。相比之前工作，MIDAR不同于高保真模拟器的高计算需求，也不同于简化模型的随机假设，而是基于实际车辆布局和尺寸特征预测检测结果，在保持高效的同时实现了高精度(AUC=0.909)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MIDAR提出了一种轻量级即插即用的LiDAR检测模拟模型，通过改进的多跳视线图和GRU增强的图神经网络架构，在微观交通模拟器中高效生成接近真实的LiDAR检测结果，解决了高保真仿真与大规模应用之间的效率与真实性权衡问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As autonomous driving (AD) technology advances, increasing research hasfocused on leveraging cooperative perception (CP) data collected from multipleAVs to enhance traffic applications. Due to the impracticality of large-scalereal-world AV deployments, simulation has become the primary approach in moststudies. While game-engine-based simulators like CARLA generate high-fidelityraw sensor data (e.g., LiDAR point clouds) which can be used to producerealistic detection outputs, they face scalability challenges in multi-AVscenarios. In contrast, microscopic traffic simulators such as SUMO scaleefficiently but lack perception modeling capabilities. To bridge this gap, wepropose MIDAR, a LiDAR detection mimicking model that approximates realisticLiDAR detections using vehicle-level features readily available frommicroscopic traffic simulators. Specifically, MIDAR predicts true positives(TPs) and false negatives (FNs) from ideal LiDAR detection results based on thespatial layouts and dimensions of surrounding vehicles. A Refined Multi-hopLine-of-Sight (RM-LoS) graph is constructed to encode the occlusionrelationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNParchitecture to propagate features from the ego AV and occluding vehicles tothe prediction target. MIDAR achieves an AUC of 0.909 in approximating thedetection results generated by CenterPoint, a mainstream 3D LiDAR detectionmodel, on the nuScenes AD dataset. Two CP-based traffic applications furthervalidate the necessity of such realistic detection modeling, particularly fortasks requiring accurate individual vehicle observations (e.g., position,speed, lane index). As demonstrated in the applications, MIDAR can beseamlessly integrated into traffic simulators and trajectory datasets and willbe open-sourced upon publication.</description>
      <author>example@mail.com (Tianheng Zhu, Yiheng Feng)</author>
      <guid isPermaLink="false">2508.02858v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings</title>
      <link>http://arxiv.org/abs/2508.03453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了两种数据增强策略在文本嵌入对比学习中的效果，发现裁剪增强方法优于dropout方法，并表明自监督微调在特定领域内可快速生成高质量文本嵌入。&lt;h4&gt;背景&lt;/h4&gt;文本嵌入在多种NLP应用中扮演重要角色，当前最佳嵌入模型通过监督微调预训练语言模型获得，而计算机视觉领域已成功应用自监督训练方法。&lt;h4&gt;目的&lt;/h4&gt;系统比较文本嵌入对比学习中两种数据增强策略的效果，评估嵌入质量，并探索自监督微调在文本嵌入生成中的应用价值。&lt;h4&gt;方法&lt;/h4&gt;在MTEB基准和领域内评估中比较裁剪和dropout两种增强方法，研究表示质量与transformer层的关系，以及仅微调最后几层的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;裁剪增强明显优于dropout方法；领域外数据上自监督方法质量低于监督SOTA；领域内数据上短暂自监督微调可产生高质量嵌入；表示质量随transformer层深入而提高；仅微调最后几层即可达到相似质量。&lt;h4&gt;结论&lt;/h4&gt;自监督微调，特别是使用裁剪增强，可在特定领域快速生成高质量文本嵌入，微调transformer最后几层就足以获得高质量表示。&lt;h4&gt;翻译&lt;/h4&gt;文本嵌入，即整个文本的向量表示，在许多NLP应用中扮演着重要角色，例如检索增强生成、情感分析、聚类或可视化文本集合以进行数据探索。目前，表现最佳的嵌入模型是通过精心筛选的文本对进行大规模监督微调，从预训练语言模型中衍生出来的。在这里，我们系统地比较了在文本嵌入对比学习中生成正样本对的两种最著名的数据增强策略。我们在MTEB和额外的领域内评估中评估了嵌入质量，并表明裁剪增强方法明显优于基于dropout的方法。我们发现，在领域外数据上，产生的嵌入质量低于监督SOTA模型，但对于领域内数据，自监督微调在非常短暂的微调后就能产生高质量的文本嵌入，有时仅略低于监督SOTA。最后，我们表明表示质量随着最后transformer层的提高而增加，并且仅微调那些最后几层就足以达到相似的嵌入质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text embeddings, i.e. vector representations of entire texts, play animportant role in many NLP applications, such as retrieval-augmentedgeneration, sentiment analysis, clustering, or visualizing collections of textsfor data exploration. Currently, top-performing embedding models are derivedfrom pre-trained language models via extensive supervised fine-tuning usingcurated text pairs. This contrasts with computer vision, where self-supervisedtraining based on data augmentations has demonstrated remarkable success. Herewe systematically compare the two most well-known augmentation strategies forpositive pair generation in contrastive learning of text embeddings. We assessembedding quality on MTEB and additional in-domain evaluations and show thatcropping augmentation strongly outperforms the dropout-based approach. We findthat on out-of-domain data, the quality of resulting embeddings is below thesupervised SOTA models, but for in-domain data, self-supervised fine-tuningproduces high-quality text embeddings after very short fine-tuning, sometimesonly marginally below the supervised SOTA. Finally, we show that representationquality increases towards the last transformer layers, which undergo thelargest change during fine-tuning; and that fine-tuning only those last layersis sufficient to reach similar embedding quality.</description>
      <author>example@mail.com (Rita González-Márquez, Philipp Berens, Dmitry Kobak)</author>
      <guid isPermaLink="false">2508.03453v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding</title>
      <link>http://arxiv.org/abs/2508.03201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AlignCAT的新型弱监督视觉定位框架，通过粗粒度和细粒度对齐模块解决现有方法在跨模态推理方面的不足，有效区分文本表达中的细微语义差异。&lt;h4&gt;背景&lt;/h4&gt;弱监督视觉定位旨在基于文本描述定位图像中的对象，但现有方法缺乏强大的跨模态推理能力，难以区分文本表达中的细微语义差异，这种局限性源于基于类别和基于属性的歧义性。&lt;h4&gt;目的&lt;/h4&gt;解决现有弱监督视觉定位方法中跨模态推理不足的问题，提高区分文本表达中细微语义差异的能力。&lt;h4&gt;方法&lt;/h4&gt;引入AlignCAT，一种基于查询的语义匹配框架，包含粗粒度对齐模块（利用类别信息和全局上下文减轻类别不一致对象的干扰）和细粒度对齐模块（利用描述性信息和词级文本特征实现属性一致性），通过充分利用语言线索过滤未对齐的视觉查询，提高对比学习效率。&lt;h4&gt;主要发现&lt;/h4&gt;在RefCOCO、RefCOCO+和RefCOCOg三个视觉定位基准测试上进行了大量实验，验证了AlignCAT在两种视觉定位任务上优于现有弱监督方法。&lt;h4&gt;结论&lt;/h4&gt;AlignCAT通过有效的跨模态对齐方法解决了现有方法的局限性，提出的粗粒度和细粒度对齐模块共同提高了视觉-语言对齐效果，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;弱监督视觉定位旨在基于文本描述定位图像中的对象。尽管已有显著进展，但现有方法缺乏强大的跨模态推理能力，难以区分文本表达中的细微语义差异，这种局限性源于基于类别和基于属性的歧义性。为解决这些挑战，我们引入AlignCAT，一种新颖的基于查询的语义匹配框架，用于弱监督视觉定位。为增强视觉-语言对齐，我们提出一个粗粒度对齐模块，利用类别信息和全局上下文，有效减轻来自类别不一致对象的干扰。随后，一个细粒度对齐模块利用描述性信息并捕获词级文本特征，以实现属性一致性。通过充分利用语言线索，我们提出的AlignCAT逐步过滤掉未对齐的视觉查询，提高对比学习效率。在RefCOCO、RefCOCO+和RefCOCOg三个视觉定位基准测试上的大量实验验证了AlignCAT在两种视觉定位任务上优于现有弱监督方法。我们的代码可在以下网址获取：https://github.com/I2-Multimedia-Lab/AlignCAT。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised visual grounding (VG) aims to locate objects in imagesbased on text descriptions. Despite significant progress, existing methods lackstrong cross-modal reasoning to distinguish subtle semantic differences in textexpressions due to category-based and attribute-based ambiguity. To addressthese challenges, we introduce AlignCAT, a novel query-based semantic matchingframework for weakly supervised VG. To enhance visual-linguistic alignment, wepropose a coarse-grained alignment module that utilizes category informationand global context, effectively mitigating interference fromcategory-inconsistent objects. Subsequently, a fine-grained alignment moduleleverages descriptive information and captures word-level text features toachieve attribute consistency. By exploiting linguistic cues to their fullestextent, our proposed AlignCAT progressively filters out misaligned visualqueries and enhances contrastive learning efficiency. Extensive experiments onthree VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify thesuperiority of AlignCAT against existing weakly supervised methods on two VGtasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.</description>
      <author>example@mail.com (Yidan Wang, Chenyi Zhuang, Wutao Liu, Pan Gao, Nicu Sebe)</author>
      <guid isPermaLink="false">2508.03201v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation</title>
      <link>http://arxiv.org/abs/2508.03104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 18 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出HiTeC，一个针对文本属性超图的两阶段分层对比学习框架，解决了现有方法在处理文本信息超图时的三个主要限制，并实现了可扩展且有效的自监督学习。&lt;h4&gt;背景&lt;/h4&gt;对比学习已成为无监督超图学习的主导范式，但现实世界中的超图节点通常与丰富的文本信息相关联，这在先前的工作中被忽视了。&lt;h4&gt;目的&lt;/h4&gt;填补现有对比学习方法在文本属性超图上应用的研究空白，提出一个可扩展且有效的自监督学习框架。&lt;h4&gt;方法&lt;/h4&gt;HiTeC采用两阶段设计：第一阶段使用结构感知的对比目标预训练文本编码器；第二阶段引入提示增强的文本增强和语义感知的超边丢弃两种增强策略；此外还提出了多尺度对比损失，通过子图级别对比更好地捕获长程依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过将文本编码器预训练与超图对比学习解耦，两阶段设计在不损害表示质量的情况下提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;大量实验证实了HiTeC在文本属性超图上的有效性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;对比学习已成为无监督超图学习的主导范式，使模型能够在没有昂贵标签的情况下进行有效训练。然而，现实世界中的超图节点通常与丰富的文本信息相关联，这在先前的工作中被忽视了。将现有的基于对比学习的方法直接应用于文本属性超图会导致三个关键限制：(1)常用的与图无关的文本编码器忽视了文本内容与超图拓扑之间的相关性，导致次优的表示。(2)它们依赖于随机数据增强，引入噪声并削弱了对比目标。(3)主要关注节点和超边级别的对比信号，限制了捕获长程依赖关系的能力。虽然HyperBERT开创了TAHGs上的对比学习，但其协同训练范式可扩展性差。为了填补这一研究空白，我们引入了HiTeC，一个具有语义感知增强的两阶段分层对比学习框架，用于在TAHGs上进行可扩展且有效的自监督学习。在第一阶段，我们使用结构感知的对比目标预训练文本编码器，以克服传统方法的与图无关的性质。在第二阶段，我们引入两种语义感知增强策略，包括提示增强的文本增强和语义感知的超边丢弃，以促进信息视图的生成。此外，我们还提出了一个多尺度对比损失，通过基于s-walk的子图级别对比扩展了现有目标，以更好地捕获长程依赖关系。通过将文本编码器预训练与超图对比学习解耦，这种两阶段设计在不损害表示质量的情况下提高了可扩展性。大量实验证实了HiTeC的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning (CL) has become a dominant paradigm for self-supervisedhypergraph learning, enabling effective training without costly labels.However, node entities in real-world hypergraphs are often associated with richtextual information, which is overlooked in prior works. Directly applyingexisting CL-based methods to such text-attributed hypergraphs (TAHGs) leads tothree key limitations: (1) The common use of graph-agnostic text encodersoverlooks the correlations between textual content and hypergraph topology,resulting in suboptimal representations. (2) Their reliance on random dataaugmentations introduces noise and weakens the contrastive objective. (3) Theprimary focus on node- and hyperedge-level contrastive signals limits theability to capture long-range dependencies, which is essential for expressiverepresentation learning. Although HyperBERT pioneers CL on TAHGs, itsco-training paradigm suffers from poor scalability. To fill the research gap,we introduce HiTeC, a two-stage hierarchical contrastive learning frameworkwith semantic-aware augmentation for scalable and effective self-supervisedlearning on TAHGs. In the first stage, we pre-train the text encoder with astructure-aware contrastive objective to overcome the graph-agnostic nature ofconventional methods. In the second stage, we introduce two semantic-awareaugmentation strategies, including prompt-enhanced text augmentation andsemantic-aware hyperedge drop, to facilitate informative view generation.Furthermore, we propose a multi-scale contrastive loss that extends existingobjectives with an $s$-walk-based subgraph-level contrast to better capturelong-range dependencies. By decoupling text encoder pretraining from hypergraphcontrastive learning, this two-stage design enhances scalability withoutcompromising representation quality. Extensive experiments confirm theeffectiveness of HiTeC.</description>
      <author>example@mail.com (Mengting Pan, Fan Li, Xiaoyang Wang, Wenjie Zhang, Xuemin Lin)</author>
      <guid isPermaLink="false">2508.03104v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning</title>
      <link>http://arxiv.org/abs/2508.03102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了Causal CLIP Adapter (CCA)框架，通过独立成分分析(ICA)解耦CLIP的视觉特征，并利用CLIP的跨模态对齐能力，在少样本学习中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;Few-shot learning通常需要使用有限的标记数据有效适应模型。然而，大多数现有的少样本学习方法依赖于纠缠表示，要求模型仅使用有限的监督隐式地恢复解耦过程以获得解耦表示，这阻碍了有效的适应。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，明确解耦从CLIP提取的视觉特征，减少可训练参数数量，减轻过拟合，并在少样本学习中提高分类准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 使用无监督独立成分分析(ICA)明确解耦从CLIP提取的视觉特征；2. 通过微调基于CLIP的文本分类器单向增强CLIP的跨模态对齐；3. 通过交叉注意力机制双向增强CLIP的跨模态对齐；4. 将单模态和跨模态分类输出线性组合以提高分类准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在11个基准数据集上的广泛实验表明，该方法在少样本性能和对分布偏移的鲁棒性方面始终优于最先进的方法，同时保持计算效率。&lt;h4&gt;结论&lt;/h4&gt;CCA框架通过显式解耦视觉特征和增强CLIP的跨模态对齐能力，有效解决了少样本学习中纠缠表示的问题，减少了可训练参数，减轻了过拟合，并在多个基准测试中取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;小样本学习通常需要使用有限的标记数据有效适应模型。然而，大多数现有的小样本学习方法依赖于纠缠表示，要求模型仅使用有限的监督隐式地恢复解耦过程以获得解耦表示，这阻碍了有效的适应。最近的理论研究表明，多模态对比学习方法如CLIP可以线性变换地解耦潜在表示。鉴于此，我们提出了因果CLIP适配器(CCA)，这是一个新颖的框架，使用无监督独立成分分析(ICA)明确解耦从CLIP提取的视觉特征。这消除了从标记数据学习解耦过程的需要，从而减少了可训练参数的数量并减轻了过拟合。更进一步，虽然ICA可以获得视觉解耦表示，但它也可能破坏CLIP的内部和跨模态对齐。为了对抗这一点，CCA进一步利用CLIP固有的跨模态对齐，通过两种方式增强它：单向地，通过微调基于CLIP的文本分类器；双向地，通过交叉注意力机制，通过相互丰富视觉和文本表示。单模态和跨模态分类输出可以有效地线性组合以提高分类准确性。在11个基准数据集上的广泛实验表明，我们的方法在少样本性能和对分布偏移的鲁棒性方面始终优于最先进的方法，同时保持计算效率。代码将在https://github.com/tianjiao-j/CCA上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot learning (FSL) often requires effective adaptation of models usinglimited labeled data. However, most existing FSL methods rely on entangledrepresentations, requiring the model to implicitly recover the unmixing processto obtain disentangled representations using only limited supervision, whichhinders effective adaptation. Recent theoretical studies show that multimodalcontrastive learning methods, such as CLIP, can disentangle latentrepresentations up to linear transformations. In light of this, we propose theCausal CLIP Adapter (CCA), a novel framework that explicitly disentanglesvisual features extracted from CLIP using unsupervised Independent ComponentAnalysis (ICA). This removes the need to learn the unmixing process from thelabeled data, thereby reducing the number of trainable parameters andmitigating overfitting. Taking a step further, while ICA can obtain visualdisentangled representations, it may also disrupt CLIP's intra- and inter-modalalignment. To counteract this, CCA further leverages CLIP's inherentcross-modal alignment by enhancing it in two ways: unidirectionally, throughfine-tuning a CLIP-based text classifier, and bidirectionally, via across-attention mechanism that enriches visual and textual representationsthrough mutual interaction. Both unimodal and cross-modal classificationoutputs can be effectively combined linearly to improve classificationaccuracy. Extensive experiments on 11 benchmark datasets demonstrate that ourmethod consistently outperforms state-of-the-art approaches in terms offew-shot performance and robustness to distributional shifts, while maintainingcomputational efficiency. Code will be available athttps://github.com/tianjiao-j/CCA.</description>
      <author>example@mail.com (Tianjiao Jiang, Zhen Zhang, Yuhang Liu, Javen Qinfeng Shi)</author>
      <guid isPermaLink="false">2508.03102v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification</title>
      <link>http://arxiv.org/abs/2508.03081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对比跨袋增强($C^2Aug$)方法，用于解决基于多实例学习的全切片图像分类中伪袋增强方法多样性受限的问题，并通过引入袋级和组级对比学习框架提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;当前基于多实例学习(MIL)的全切片图像(WSI)分类中的伪袋增强方法从有限数量的袋中采样实例，导致多样性受限。&lt;h4&gt;目的&lt;/h4&gt;增加伪袋的多样性，解决引入新实例后关键实例数量增加导致的问题，并提高模型在肿瘤面积小的测试切片上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出对比跨袋增强($C^2Aug$)方法从同一类的所有袋中采样实例增加伪袋多样性，并引入袋级和组级对比学习框架增强具有不同语义含义的特征的区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;引入新实例到伪袋中会增加关键实例的数量，导致包含少量关键实例的伪袋出现次数减少，这限制了模型在肿瘤面积小的测试切片上的性能。&lt;h4&gt;结论&lt;/h4&gt;$C^2Aug$在多个评估指标上一致优于现有的最先进方法。&lt;h4&gt;翻译&lt;/h4&gt;最近的基于多实例学习(MIL)的全切片图像(WSI)分类中的伪袋增强方法从有限数量的袋中采样实例，导致多样性受限。为解决这一问题，我们提出了对比跨袋增强($C^2Aug$)方法，从同一类的所有袋中采样实例以增加伪袋的多样性。然而，将新实例引入伪袋会增加关键实例(如肿瘤实例)的数量。这种增加导致包含少量关键实例的伪袋出现次数减少，从而限制了模型性能，特别是在肿瘤面积小的测试切片上。为解决这一问题，我们引入了袋级和组级对比学习框架，以增强具有不同语义含义的特征的区分能力，从而提高模型性能。实验结果表明，$C^2Aug$在多个评估指标上一致优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent pseudo-bag augmentation methods for Multiple Instance Learning(MIL)-based Whole Slide Image (WSI) classification sample instances from alimited number of bags, resulting in constrained diversity. To address thisissue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sampleinstances from all bags with the same class to increase the diversity ofpseudo-bags. However, introducing new instances into the pseudo-bag increasesthe number of critical instances (e.g., tumor instances). This increase resultsin a reduced occurrence of pseudo-bags containing few critical instances,thereby limiting model performance, particularly on test slides with smalltumor areas. To address this, we introduce a bag-level and group-levelcontrastive learning framework to enhance the discrimination of features withdistinct semantic meanings, thereby improving model performance. Experimentalresults demonstrate that $C^2Aug$ consistently outperforms state-of-the-artapproaches across multiple evaluation metrics.</description>
      <author>example@mail.com (Bo Zhang, Xu Xinan, Shuo Yan, Yu Bai, Zheng Zhang, Wufan Wang, Wendong Wang)</author>
      <guid isPermaLink="false">2508.03081v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec</title>
      <link>http://arxiv.org/abs/2508.02849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SecoustiCodec，一种跨模态对齐的低比特率流式语音编解码器，解决了现有方法在语义编码中的多个挑战，包括残余语言外信息、语义完整性不足、重构能力有限和缺乏流式支持。&lt;h4&gt;背景&lt;/h4&gt;语音编解码器在统一语音和文本语言模型中扮演着重要角色，但现有方法在语义编码方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有语音编解码方法在语义编码方面的挑战，包括残余的语言外信息（如音色、情感）、语义完整性不足、重构能力有限和缺乏流式支持。&lt;h4&gt;方法&lt;/h4&gt;提出SecoustiCodec，在单一码本空间中解耦语义和语言外信息；引入语言外编码确保语义完整性和重构保真度；提出基于VAE和FSQ的语义专用高效量化方法；提出基于对比学习的语义解耦方法，对齐文本和语音在联合多模态帧级空间；提出声学约束的多阶段优化策略确保稳健稳定的收敛。&lt;h4&gt;主要发现&lt;/h4&gt;SecoustiCodec在0.27/1 kbps比特率下分别实现了1.77/2.58的SOTA重构质量（PESQ）。&lt;h4&gt;结论&lt;/h4&gt;SecoustiCodec成功解决了现有语音编解码方法的多个挑战，实现了高质量重构能力并支持流式处理，已开源演示、代码和模型权重。&lt;h4&gt;翻译&lt;/h4&gt;语音编解码器在统一语音和文本语言模型中扮演着关键桥梁的角色。现有的编解码方法在语义编码方面面临几个挑战，例如残余的语言外信息（如音色、情感）、语义完整性不足、重构能力有限以及缺乏流式支持。为了解决这些挑战，我们提出了SecoustiCodec，一种跨模态对齐的低比特率流式语音编解码器，在单一码本空间中解耦语义和语言外信息。为确保语义完整性和重构保真度，引入了语言外编码来桥接语义和声学编码之间的信息鸿沟。提出了基于VAE（变分自编码器）和FSQ（有限标量量化）的语义专用高效量化方法。这种方法缓解了令牌的长尾分布问题，同时保持高码本利用率。提出了基于对比学习的语义解耦方法，在联合多模态帧级空间中对齐文本和语音，有效从语义编码中移除语言外信息。提出了声学约束的多阶段优化策略，确保稳健和稳定的收敛。图~\ref{fig:pesq_kbps_below_2kbps}显示SecoustiCodec在0.27/1 kbps下分别实现了1.77/2.58的SOTA（最先进）重构质量（PESQ）。SecoustiCodec的代码和模型权重将在同行评审完成后开源。我们已经开源了SecoustiCodec的演示、代码和模型权重。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech codecs serve as a crucial bridge in unifying speech and text languagemodels. Existing codec methods face several challenges in semantic encoding,such as residual paralinguistic information (e.g., timbre, emotion),insufficient semantic completeness, limited reconstruction capability, and lackof support for streaming. To address these challenges, we proposeSecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec thatdisentangles semantic and paralinguistic information in a single-codebookspace. To ensure semantic completeness and reconstruction fidelity,paralinguistic encoding is introduced to bridge the information gap betweensemantic and acoustic encoding. A semantic-only efficient quantization methodbased on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) isproposed. This approach alleviates the long-tail distribution problem of tokenswhile maintaining high codebook utilization. A semantic disentanglement methodbased on contrastive learning is proposed, which aligns text and speech in ajoint multimodal frame-level space, effectively removing paralinguisticinformation from semantic encoding. An acoustic-constrained multi-stageoptimization strategy is proposed to ensure robust and stable convergence.Figure~\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.The code and model weights for SecoustiCodec will be open-sourced upon thecompletion of the peer-review process. We've open-sourced SecoustiCodec's demo,code, and model weights.</description>
      <author>example@mail.com (Chunyu Qiang, Haoyu Wang, Cheng Gong, Tianrui Wang, Ruibo Fu, Tao Wang, Ruilong Chen, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Longbiao Wang, Jianwu Dang, Jianhua Tao)</author>
      <guid isPermaLink="false">2508.02849v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Context-Adaptive Multi-Prompt LLM Embedding for Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2508.02762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出Context-Adaptive Multi-Prompt Embedding方法，通过多个结构化提示增强视觉语言对比学习中的语义表示，相比标准CLIP模型的单一文本嵌入，能捕捉输入文本的多样语义方面。&lt;h4&gt;背景&lt;/h4&gt;标准CLIP风格模型依赖单一文本嵌入来表示视觉语言数据，可能无法充分捕捉文本的丰富语义信息。&lt;h4&gt;目的&lt;/h4&gt;增强视觉语言对比学习中的语义表示，实现文本与视觉特征之间更丰富的语义对齐，提高检索性能。&lt;h4&gt;方法&lt;/h4&gt;引入多个结构化提示，每个提示包含不同的自适应标记来捕捉输入文本的不同语义方面；所有提示在一个前向传播中联合处理；提示嵌入被组合成统一的文本表示；加入多样性正则化损失和否定感知损失来促进语义多样性和表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过多个提示的组合和专门的损失函数，能够实现语义更丰富的表示，提高对比判别能力，在图像-文本和视频-文本检索任务中取得一致改进。&lt;h4&gt;结论&lt;/h4&gt;Context-Adaptive Multi-Prompt Embedding方法能有效增强视觉语言对比学习中的语义表示，在多种检索任务中展现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出上下文自适应多提示嵌入，这是一种在视觉语言对比学习中丰富语义表示的新方法。与依赖单一文本嵌入的标准CLIP风格模型不同，我们的方法引入了多个结构化提示，每个提示包含不同的自适应标记，捕捉输入文本的多样语义方面。我们在单个前向传播中联合处理所有提示。将提示组合成统一的文本表示，实现与视觉特征的更丰富语义对齐。为进一步促进语义多样性和表示质量，我们纳入多样性正则化损失和否定感知损失，鼓励提示专业化并提高对比判别能力。我们的方法在图像-文本和视频-文本检索基准测试中取得了一致的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Context-Adaptive Multi-Prompt Embedding, a novel approach toenrich semantic representations in vision-language contrastive learning. Unlikestandard CLIP-style models that rely on a single text embedding, our methodintroduces multiple structured prompts, each containing a distinct adaptivetoken that captures diverse semantic aspects of the input text. We process allprompts jointly in a single forward pass. The resulting prompt embeddings arecombined into a unified text representation, enabling semantically richeralignment with visual features. To further promote semantic diversity andrepresentation quality, we incorporate a diversity regularization loss and anegation-aware loss, encouraging specialization across prompts and improvingcontrastive discrimination. Our method achieves consistent improvements on bothimage-text and video-text retrieval benchmarks.</description>
      <author>example@mail.com (Dahun Kim, Anelia Angelova)</author>
      <guid isPermaLink="false">2508.02762v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>ECGTwin: Personalized ECG Generation Using Controllable Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.02720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ECGTwin，一个两阶段框架用于个性化心电图生成，解决了在没有真实标签情况下提取个体特征和注入各种心脏条件两个基本挑战，能够生成高质量、多样化的ECG信号并保留个体特定特征。&lt;h4&gt;背景&lt;/h4&gt;个性化心电图生成可将传统医疗转变为更准确的个体化范式，同时保留群体水平ECG合成的优势。但面临两个基本挑战：无真实标签时提取个体特征，以及注入多种心脏条件时不混淆生成模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架解决个性化ECG生成的两个基本挑战，生成高质量、多样化的ECG信号并保留个体特定特征，增强下游应用中的ECG自动诊断能力。&lt;h4&gt;方法&lt;/h4&gt;ECGTwin采用两阶段框架：第一阶段是通过对比学习训练的个体基础提取器，从参考ECG中捕获个人特征；第二阶段是将个体特征与目标心脏条件通过新颖的AdaX条件注入器整合到基于扩散的生成过程中，通过两个专用途径注入信号。&lt;h4&gt;主要发现&lt;/h4&gt;定性和定量实验表明，ECGTwin能通过细粒度生成可控性生成高质量、多样化的ECG信号，同时保留个体特定特征。此外，该模型显示出增强下游ECG自动诊断的潜力。&lt;h4&gt;结论&lt;/h4&gt;ECGTwin成功解决了个性化ECG生成中的基本挑战，为精确个性化医疗解决方案提供了可能性，能够生成高质量的个性化ECG信号并应用于实际医疗诊断。&lt;h4&gt;翻译&lt;/h4&gt;个性化心电图(ECG)生成是模拟特定条件下患者心电图数字孪生的过程。它有可能将传统医疗保健转变为更准确的个体化范式，同时保留传统群体水平ECG合成的主要优势。然而，这一有前景的任务提出了两个基本挑战：在没有真实标签的情况下提取个体特征，以及在混淆生成模型的情况下注入各种类型的心脏条件。在本文中，我们提出了ECGTwin，一个两阶段框架，旨在解决这些挑战。在第一阶段，通过对比学习训练的个体基础提取器稳健地捕获来自参考ECG的个人特征。在第二阶段，提取的个体特征与目标心脏条件通过我们新颖的AdaX条件注入器整合到基于扩散的生成过程中，该注入器通过两个专用且专门的途径注入这些信号。定性和定量实验都证明，我们的模型不仅能够通过提供细粒度的生成可控性来生成高质量、多样化的ECG信号，还能保留个体特定特征。此外，ECGTwin显示出增强下游应用中ECG自动诊断的潜力，确认了精确个性化医疗解决方案的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized electrocardiogram (ECG) generation is to simulate a patient'sECG digital twins tailored to specific conditions. It has the potential totransform traditional healthcare into a more accurate individualized paradigm,while preserving the key benefits of conventional population-level ECGsynthesis. However, this promising task presents two fundamental challenges:extracting individual features without ground truth and injecting various typesof conditions without confusing generative model. In this paper, we presentECGTwin, a two-stage framework designed to address these challenges. In thefirst stage, an Individual Base Extractor trained via contrastive learningrobustly captures personal features from a reference ECG. In the second stage,the extracted individual features, along with a target cardiac condition, areintegrated into the diffusion-based generation process through our novel AdaXCondition Injector, which injects these signals via two dedicated andspecialized pathways. Both qualitative and quantitative experiments havedemonstrated that our model can not only generate ECG signals of high fidelityand diversity by offering a fine-grained generation controllability, but alsopreserving individual-specific features. Furthermore, ECGTwin shows thepotential to enhance ECG auto-diagnosis in downstream application, confirmingthe possibility of precise personalized healthcare solutions.</description>
      <author>example@mail.com (Yongfan Lai, Bo Liu, Xinyan Guan, Qinghao Zhao, Hongyan Li, Shenda Hong)</author>
      <guid isPermaLink="false">2508.02720v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting</title>
      <link>http://arxiv.org/abs/2508.03589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种名为VITA的变分预训练框架，解决了农业产量预测中AI模型在偏离历史趋势时表现不佳的问题。该模型通过利用丰富的天气数据作为代理目标进行预训练，可以在仅使用基本天气统计数据的情况下进行微调，并在美国玉米带的763个县中实现了最先进的玉米和大豆产量预测性能，特别是在极端天气年份表现突出。&lt;h4&gt;背景&lt;/h4&gt;准确的作物产量预测对全球粮食安全至关重要。然而，当前的AI模型在产量偏离历史趋势时表现不佳。这一问题源于关键的数据挑战，包括丰富的预训练天气数据集与可用于微调的有限数据之间的主要不对称性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服数据限制、在各种条件下（特别是极端天气条件下）准确预测作物产量的AI模型，使其在数据稀缺地区更加实用。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了VITA（用于非对称数据的变分推理Transformer）变分预训练框架。该框架不依赖输入重建，而是在预训练期间使用详细的天气变量作为代理目标，并通过自监督特征掩码学习预测丰富的大气状态。这使得模型在部署时只需使用基本天气统计数据即可进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在美国玉米带的763个县中，VITA在预测玉米和大豆产量方面实现了最先进的性能。2. 在正常条件下，VITA持续提供优越的性能。3. 在极端天气年份，VITA的优势尤为明显，具有统计学上的显著改善。4. VITA使用更少的数据就能优于先前的框架（如GNN-RNN），使其在现实世界应用中更加实用，特别是在数据稀缺地区。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了领域感知的AI设计如何能够克服数据限制，并在不断变化的气候中支持有弹性的农业预测。VITA模型通过有效处理天气数据的不对称性，为作物产量预测提供了更准确、更可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的作物产量预测对全球粮食安全至关重要。然而，当产量偏离历史趋势时，当前的AI模型普遍表现不佳。这一问题源于关键的数据挑战，包括丰富的预训练天气数据集与可用于微调的有限数据之间的主要不对称性。我们引入了VITA（用于非对称数据的变分推理Transformer），这是一个解决这种不对称性的变分预训练框架。VITA不依赖输入重建，而是在预训练期间使用详细的天气变量作为代理目标，并通过自监督特征掩码学习预测丰富的大气状态。这使得模型在部署时只需使用基本天气统计数据即可进行微调。应用于美国玉米带的763个县，VITA在所有评估场景中实现了预测玉米和大豆产量的最先进性能。虽然在正常条件下它持续提供优越的性能，但其优势在极端天气年份尤为明显，具有统计学上的显著改善。重要的是，VITA使用更少的数据就能优于先前的框架（如GNN-RNN），使其在现实世界应用中更加实用——特别是在数据稀缺地区。这项工作展示了领域感知的AI设计如何能够克服数据限制，并在不断变化的气候中支持有弹性的农业预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate crop yield forecasting is essential for global food security.However, current AI models systematically underperform when yields deviate fromhistorical trends. This issue arises from key data challenges, including amajor asymmetry between rich pretraining weather datasets and the limited dataavailable for fine-tuning. We introduce VITA (Variational Inference Transformerfor Asymmetric data), a variational pretraining framework that addresses thisasymmetry. Instead of relying on input reconstruction, VITA uses detailedweather variables as proxy targets during pretraining and learns to predictrich atmospheric states through self-supervised feature masking. This allowsthe model to be fine-tuned using only basic weather statistics duringdeployment. Applied to 763 counties in the U.S. Corn Belt, VITA achievesstate-of-the-art performance in predicting corn and soybean yields across allevaluation scenarios. While it consistently delivers superior performance undernormal conditions, its advantages are particularly pronounced during extremeweather years, with statistically significant improvements (paired t-test, $p\approx 0.01$). Importantly, VITA outperforms prior frameworks like GNN-RNNusing less data, making it more practical for real-world use--particularly indata-scarce regions. This work highlights how domain-aware AI design canovercome data limitations and support resilient agricultural forecasting in achanging climate.</description>
      <author>example@mail.com (Adib Hasan, Mardavij Roozbehani, Munther Dahleh)</author>
      <guid isPermaLink="false">2508.03589v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>GaitAdapt: Continual Learning for Evolving Gait Recognition</title>
      <link>http://arxiv.org/abs/2508.03375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GaitAdapter的无需回放的步态识别持续学习方法，通过GPAK模块和EDSN方法有效解决了模型在新数据集上重新训练时难以保留先前知识的问题，显著提高了步态识别的判别能力。&lt;h4&gt;背景&lt;/h4&gt;当前步态识别方法通常需要在遇到新数据集时重新训练，但重新训练的模型难以保留之前数据集的知识，导致在早期测试集上性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一个持续的步态识别任务GaitAdapt，支持步态识别能力的逐步增强，并根据各种评估场景进行系统分类。同时提出GaitAdapter，一种无需回放的步态识别持续学习方法。&lt;h4&gt;方法&lt;/h4&gt;GaitAdapter集成了GaitPartition Adaptive Knowledge (GPAK)模块，使用图神经网络从当前数据中聚合常见的步态模式到一个由图向量构建的存储库中，用于提高新任务中步态特征的判别能力。还引入了一种基于负样本的欧几里得距离稳定性方法(EDSN)，确保来自不同类别的新增步态样本在之前的和当前的步态任务中保持相似的相对空间分布。&lt;h4&gt;主要发现&lt;/h4&gt;广泛评估表明，GaitAdapter有效保留了从不同任务中获取的步态知识，与替代方法相比表现出明显优越的判别能力。&lt;h4&gt;结论&lt;/h4&gt;GaitAdapter方法解决了步态识别中持续学习的知识保留问题，提高了模型在多任务环境下的性能。&lt;h4&gt;翻译&lt;/h4&gt;当前步态识别方法通常需要在遇到新数据集时重新训练。然而，重新训练的模型经常难以保留来自之前数据集的知识，导致在早期测试集上性能显著下降。为了解决这些挑战，我们提出了一个持续的步态识别任务，称为GaitAdapt，它支持随着时间的推移逐步增强步态识别能力，并根据各种评估场景进行系统分类。此外，我们提出了GaitAdapter，一种用于步态识别的无回放持续学习方法。这种方法集成了GaitPartition Adaptive Knowledge (GPAK)模块，采用图神经网络从当前数据中聚合常见的步态模式到一个由图向量构建的存储库中。随后，使用该存储库来提高新任务中步态特征的判别能力，从而增强模型有效识别步态模式的能力。我们还引入了一种基于负样本的欧几里得距离稳定性方法(EDSN)，确保来自不同类别的新增步态样本在之前的和当前的步态任务中保持相似的相对空间分布，从而减轻任务变化对原始领域特征判别性的影响。广泛的评估表明，GaitAdapter有效保留了从不同任务中获取的步态知识，与替代方法相比表现出明显优越的判别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current gait recognition methodologies generally necessitate retraining whenencountering new datasets. Nevertheless, retrained models frequently encounterdifficulties in preserving knowledge from previous datasets, leading to asignificant decline in performance on earlier test sets. To tackle thesechallenges, we present a continual gait recognition task, termed GaitAdapt,which supports the progressive enhancement of gait recognition capabilitiesover time and is systematically categorized according to various evaluationscenarios. Additionally, we propose GaitAdapter, a non-replay continuallearning approach for gait recognition. This approach integrates theGaitPartition Adaptive Knowledge (GPAK) module, employing graph neural networksto aggregate common gait patterns from current data into a repositoryconstructed from graph vectors. Subsequently, this repository is used toimprove the discriminability of gait features in new tasks, thereby enhancingthe model's ability to effectively recognize gait patterns. We also introduce aEuclidean Distance Stability Method (EDSN) based on negative pairs, whichensures that newly added gait samples from different classes maintain similarrelative spatial distributions across both previous and current gait tasks,thereby alleviating the impact of task changes on the distinguishability oforiginal domain features. Extensive evaluations demonstrate that GaitAdaptereffectively retains gait knowledge acquired from diverse tasks, exhibitingmarkedly superior discriminative capability compared to alternative methods.</description>
      <author>example@mail.com (Jingjie Wang, Shunli Zhang, Xiang Wei, Senmao Tian)</author>
      <guid isPermaLink="false">2508.03375v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Online Continual Graph Learning</title>
      <link>http://arxiv.org/abs/2508.03283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了图上在线持续学习的通用公式，建立了基准测试，为系统评估提供了明确定义的设置。&lt;h4&gt;背景&lt;/h4&gt;持续学习旨在增量学习新任务同时避免灾难性遗忘；在线持续学习关注从连续数据流中高效学习，这些数据流具有变化的分布。尽管有研究探索使用图神经网络进行持续学习，但只有少数关注流式设置，而现实中的图数据通常会随时间演变，需要及时和在线的预测。&lt;h4&gt;目的&lt;/h4&gt;提出图上在线持续学习的通用公式，强调在图拓扑上进行批处理的效率要求，并为系统模型评估提供明确定义的设置。&lt;h4&gt;方法&lt;/h4&gt;引入一组基准测试，并报告持续学习文献中几种方法在所提设置下的性能表现。&lt;h4&gt;主要发现&lt;/h4&gt;当前方法与标准的在线持续学习设置不太一致，部分原因是缺乏对图上在线持续学习的明确定义。&lt;h4&gt;结论&lt;/h4&gt;通过提出的通用公式和基准测试，为图上的在线持续学习研究提供了更清晰的框架和评估标准。&lt;h4&gt;翻译&lt;/h4&gt;持续学习的目标是在避免灾难性遗忘的同时增量学习新任务。在线持续学习特别关注从连续数据流中高效学习，这些数据流具有变化的分布。虽然最近有研究探索使用图神经网络进行持续学习，但只有少数研究关注流式设置。然而，现实中的图数据通常会随时间演变，往往需要及时和在线的预测。然而，当前方法与标准的在线持续学习设置不太一致，部分原因是缺乏对图上在线持续学习的明确定义。在这项工作中，我们提出了图上在线持续学习的通用公式，强调在图拓扑上进行批处理的效率要求，并为系统模型评估提供明确定义的设置。最后，我们引入一组基准测试，并报告了持续学习文献中几种方法在我们设置下的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of Continual Learning (CL) is to learn new tasks incrementally whileavoiding catastrophic forgetting. Online Continual Learning (OCL) specificallyfocuses on learning efficiently from a continuous stream of data with shiftingdistribution. While recent studies explore Continual Learning on graphsexploiting Graph Neural Networks (GNNs), only few of them focus on a streamingsetting. Yet, many real-world graphs evolve over time, often requiring timelyand online predictions. Current approaches, however, are not well aligned withthe standard OCL setting, partly due to the lack of a clear definition ofonline Continual Learning on graphs. In this work, we propose a generalformulation for online Continual Learning on graphs, emphasizing the efficiencyrequirements on batch processing over the graph topology, and providing awell-defined setting for systematic model evaluation. Finally, we introduce aset of benchmarks and report the performance of several methods in the CLliterature, adapted to our setting.</description>
      <author>example@mail.com (Giovanni Donghi, Luca Pasa, Daniele Zambon, Cesare Alippi, Nicolò Navarin)</author>
      <guid isPermaLink="false">2508.03283v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Embedding Models on Hyper-relational Knowledge Graph</title>
      <link>http://arxiv.org/abs/2508.03280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究质疑了超关系知识图谱嵌入(HKGE)模型的优越性来源，通过将超关系知识图谱转换为传统知识图谱格式测试经典KGE模型，发现部分经典模型性能相当。分析表明分解方法改变了原始HKG拓扑且无法完全保留信息，且当前HKGE模型在捕捉长距离依赖或整合主三元组和限定词信息方面存在不足。为此，作者提出了FormerGNN框架，通过限定词整合器保留原始拓扑，基于GNN的图编码器捕捉长距离依赖，并改进信息整合方法。实验证明FormerGNN优于现有HKGE模型。&lt;h4&gt;背景&lt;/h4&gt;超关系知识图谱(HKGs)是传统知识图谱(KGs)的扩展，通过额外的限定词(qualifiers)更好地表示现实世界事实。研究人员尝试通过设计额外的限定词处理模块来适应经典知识图谱嵌入(KGE)模型以应用于HKGs。&lt;h4&gt;目的&lt;/h4&gt;确定HKGE模型的优越性能是来自于其基础KGE模型还是专门设计的扩展模块，并探索改进HKGE模型的方法。&lt;h4&gt;方法&lt;/h4&gt;作者采用三种分解方法将超关系知识图谱转换为传统知识图谱格式，评估了多个经典KGE模型在HKGs上的性能。基于发现的问题，提出了FormerGNN框架，包含限定词整合器保留原始HKG拓扑，基于GNN的图编码器捕捉长距离依赖，以及改进的主三元组和限定词信息整合方法。&lt;h4&gt;主要发现&lt;/h4&gt;1)一些经典KGE模型在HKGs上可以达到与HKGE模型相当的性能；2)分解方法改变了原始HKG拓扑且无法完全保留HKG信息；3)当前HKGE模型在捕捉图的长期依赖或整合主三元组和限定词信息方面存在不足，部分原因是信息压缩问题。&lt;h4&gt;结论&lt;/h4&gt;FormerGNN框架通过保留原始HKG拓扑、捕捉长距离依赖和改进信息整合方法，能够有效提升HKGE模型的性能，优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;最近，超关系知识图谱(HKGs)被提出作为传统知识图谱(KGs)的扩展，通过额外的限定词更好地表示现实世界事实。因此，研究人员尝试通过设计额外的限定词处理模块来适应经典知识图谱嵌入(KGE)模型以应用于HKGs。然而，目前尚不清楚超关系KGE(HKGE)模型的优越性能是来自于其基础KGE模型还是专门设计的扩展模块。因此，在本文中，我们使用三种分解方法将HKG数据转换为KG格式，然后评估了几个经典KGE模型在HKGs上的性能。我们的结果表明，一些KGE模型能够达到与HKGE模型相当的性能。进一步分析发现，分解方法改变了原始HKG拓扑且无法完全保留HKG信息。此外，我们观察到当前HKGE模型在捕捉图的长期依赖方面不足，或由于信息压缩问题而难以整合主三元组和限定词信息。为进一步验证我们的发现并为未来HKGE研究提供潜在方向，我们提出了FormerGNN框架。该框架采用限定词整合器保留原始HKG拓扑，基于GNN的图编码器捕捉图的长距离依赖，然后采用改进的方法整合主三元组和限定词信息以缓解压缩问题。我们的实验结果表明FormerGNN优于现有HKGE模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as anextension of traditional Knowledge Graphs (KGs) to better represent real-worldfacts with additional qualifiers. As a result, researchers have attempted toadapt classical Knowledge Graph Embedding (KGE) models for HKGs by designingextra qualifier processing modules. However, it remains unclear whether thesuperior performance of Hyper-relational KGE (HKGE) models arises from theirbase KGE model or the specially designed extension module. Hence, in thispaper, we data-wise convert HKGs to KG format using three decomposition methodsand then evaluate the performance of several classical KGE models on HKGs. Ourresults show that some KGE models achieve performance comparable to that ofHKGE models. Upon further analysis, we find that the decomposition methodsalter the original HKG topology and fail to fully preserve HKG information.Moreover, we observe that current HKGE models are either insufficient incapturing the graph's long-range dependency or struggle to integratemain-triple and qualifier information due to the information compression issue.To further justify our findings and offer a potential direction for future HKGEresearch, we propose the FormerGNN framework. This framework employs aqualifier integrator to preserve the original HKG topology, and a GNN-basedgraph encoder to capture the graph's long-range dependencies, followed by animproved approach for integrating main-triple and qualifier information tomitigate compression issues. Our experimental results demonstrate thatFormerGNN outperforms existing HKGE models.</description>
      <author>example@mail.com (Yubo Wang, Shimin Di, Zhili Wang, Haoyang Li, Fei Teng, Hao Xin, Lei Chen)</author>
      <guid isPermaLink="false">2508.03280v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks</title>
      <link>http://arxiv.org/abs/2508.03132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  in Proc. 75th Int. Astronautical Congress (IAC-24), Milan, Italy,  Oct. 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为COFFEE的实时姿态估计框架，用于解决太空天体精确状态估计中的挑战，特别是处理高不透明自投射阴影导致的偏差问题。&lt;h4&gt;背景&lt;/h4&gt;太空中的未知天体精确状态估计是一个关键挑战，应用包括空间碎片跟踪和小天体形状估计。现有方法在实时性和准确性之间存在权衡，且都不太能处理天体自投射阴影带来的偏差问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种实时、准确且无偏差的姿态估计方法，能够处理天体自投射阴影导致的偏差问题，适用于航天级硬件环境。&lt;h4&gt;方法&lt;/h4&gt;提出COFFEE框架，利用太阳跟踪传感器提供的太阳相位角先验信息，通过将显著轮廓与其投射的阴影关联来检测特征稀疏集，然后联合训练稀疏神经网络和基于注意力的图神经网络特征匹配模型，提供连续帧之间的对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;当天体旋转时，自投射阴影会导致姿态估计产生很大偏差，特别是对于经历混沌翻滚运动的天体；所提出的COFFEE框架无偏差，比传统方法更准确，比其他深度学习方法快一个数量级。&lt;h4&gt;结论&lt;/h4&gt;COFFEE框架解决了太空天体姿态估计中的实时性、准确性和鲁棒性问题，特别适用于处理高不透明自投射阴影带来的挑战，为航天任务提供了可靠的天体状态估计解决方案。&lt;h4&gt;翻译&lt;/h4&gt;太空中的未知天体精确状态估计是一个关键挑战，应用范围从空间碎片跟踪到小天体形状估计。实现这一能力的一个必要前提是在连续的图像流中找到并跟踪特征。现有方法如SIFT、ORB和AKAZE实现了实时但不准确姿态估计，而现代深度学习方法在提供更高质量特征的同时需要更多计算资源，而这些资源在航天级硬件上可能不可用。此外，传统方法和数据驱动方法对目标物体上的高不透明自投射阴影都不鲁棒。我们表明，当目标天体旋转时，这些阴影可能导致姿态估计产生很大偏差。对于这些天体，实时姿态估计算法中的偏差可能会误导航天器的状态估计器并导致任务失败，特别是当天体经历混沌翻滚运动时。我们提出了COFFEE（天体遮挡快速特征提取器），这是一种为小行星设计的实时姿态估计框架，旨在利用航天器上常用的太阳跟踪传感器提供的太阳相位角先验信息。通过将显著轮廓与其投射的阴影关联，检测一组对阴影运动不变的特征稀疏集。然后联合训练稀疏神经网络和基于注意力的图神经网络特征匹配模型，以提供连续帧之间的对应关系集合。结果表明，所提出的姿态估计管道无偏差，比传统姿态估计管道更准确，在合成数据以及旋转小行星Apophis的渲染上比其他最先进的深度学习管道快一个数量级。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决对未知太空小行星（尤其是具有混沌翻滚运动的小行星）进行实时、准确的姿态估计问题。这个问题在现实中非常重要，因为准确估计小行星姿态是航天器与小行星轨道同步、最终接近和着陆的关键前提。传统方法实时但不准确，深度学习方法准确但计算资源需求高，且小行星表面的高度均匀性和自影会导致姿态估计产生很大偏差，可能误导航天器状态估计器，导致任务失败，特别是对于像Apophis这样进行混沌翻滚运动的小行星。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了太空小行星姿态估计的特殊挑战，认识到小行星上的高对比度边缘实际上是巨石和陨石坑投射的自影。他们提出利用航天器上常见的太阳跟踪传感器提供的太阳相位角先验信息，来区分几何特征和投射阴影。方法设计包括关键点检测（利用太阳光线方向信息）、特征描述（使用稀疏子流形CNN）、特征匹配（基于注意力的图神经网络）和姿态估计（5点算法和RANSAC）。作者借鉴了传统特征检测器的基本思想，但针对太空环境进行了改进；使用了稀疏卷积神经网络处理稀疏关键点；借鉴了Lightglue架构用于特征匹配；并使用了计算机视觉中的经典姿态估计方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用太阳相位角信息来区分小行星表面的真实几何特征和由这些特征投射的阴影，通过分析太阳光线的投影，找到不会随着小行星旋转而移动的几何特征。整体流程包括：1)关键点检测：利用太阳光线方向计算消失点，沿指向消失点的光线应用边缘滤波，保留负边缘并编码阴影大小；2)特征描述：使用稀疏子流形CNN将关键点转换为256维特征向量；3)特征匹配：结合图神经网络和注意力机制进行自注意力和交叉注意力处理；4)姿态估计：使用5点算法和RANSAC从对应关键点恢复相对姿态，并利用测距传感器获取深度信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)针对太空环境的特征检测，利用太阳相位角区分几何特征和阴影；2)使用稀疏子流形CNN处理关键点而非整个图像；3)结合图神经网络和注意力机制进行特征匹配；4)实现对自影变化的鲁棒性，消除传统方法中的姿态估计偏差。相比传统方法，COFFEE更准确且对阴影变化鲁棒；相比深度学习方法，计算效率更高且专门针对太空小行星场景设计；相比其他考虑光照条件的方法，明确将太阳相位角作为先验信息整合到状态估计管道中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; COFFEE通过利用太阳相位角信息结合稀疏神经网络，实现了对未知翻滚小行星实时、准确且对阴影变化具有鲁棒性的姿态估计，解决了太空任务中小行星轨道同步和接近的关键挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate state estimation of unknown bodies in space is a criticalchallenge with applications ranging from the tracking of space debris to theshape estimation of small bodies. A necessary enabler to this capability is tofind and track features on a continuous stream of images. Existing methods,such as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates,whereas modern deep learning methods yield higher quality features at the costof more demanding computational resources which might not be available onspace-qualified hardware. Additionally, both classical and data-driven methodsare not robust to the highly opaque self-cast shadows on the object ofinterest. We show that, as the target body rotates, these shadows may lead tolarge biases in the resulting pose estimates. For these objects, a bias in thereal-time pose estimation algorithm may mislead the spacecraft's stateestimator and cause a mission failure, especially if the body undergoes achaotic tumbling motion. We present COFFEE, the Celestial Occlusion FastFEature Extractor, a real-time pose estimation framework for asteroids designedto leverage prior information on the sun phase angle given by sun-trackingsensors commonly available onboard spacecraft. By associating salient contoursto their projected shadows, a sparse set of features are detected, invariant tothe motion of the shadows. A Sparse Neural Network followed by anattention-based Graph Neural Network feature matching model are then jointlytrained to provide a set of correspondences between successive frames. Theresulting pose estimation pipeline is found to be bias-free, more accurate thanclassical pose estimation pipelines and an order of magnitude faster than otherstate-of-the-art deep learning pipelines on synthetic data as well as onrenderings of the tumbling asteroid Apophis.</description>
      <author>example@mail.com (Arion Zimmermann, Soon-Jo Chung, Fred Hadaegh)</author>
      <guid isPermaLink="false">2508.03132v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>GEDAN: Learning the Edit Costs for Graph Edit Distance</title>
      <link>http://arxiv.org/abs/2508.03111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新颖的图神经网络框架，通过监督和无监督训练来近似图编辑距离(GED)，该方法集成了广义加性模型以学习上下文感知的编辑成本，显著提高了适应性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;图编辑距离(GED)是将一个图转换为另一个图的最小成本变换，是衡量图之间差异的广泛采用的度量标准。然而，GED的计算是NP难的，这促使了各种近似方法的发展，包括基于神经网络的方法。大多数现有方法假设单位成本编辑操作，这在现实应用中不太现实。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决现有基于神经网络的GED近似方法中单位成本编辑操作的不现实约束问题，提出一种能够学习灵活且可解释的上下文感知编辑成本的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种新颖的图神经网络框架，使用监督和无监督训练来近似GED。在无监督设置中，它采用仅基于梯度的自组织机制，无需真实距离即可进行优化。架构的核心组件是广义加性模型的集成，这允许灵活且可解释地学习上下文感知的编辑成本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法与最先进的参考方法取得了相似的结果，但显著提高了适应性和可解释性。学习到的成本函数为复杂的图结构提供了见解。&lt;h4&gt;结论&lt;/h4&gt;该方法通过学习上下文感知的编辑成本，不仅实现了与现有方法相当的性能，还提高了模型的适应性和可解释性，使其在分子分析和结构模式发现等领域特别有价值。&lt;h4&gt;翻译&lt;/h4&gt;图编辑距离(GED)被定义为将一个图转换为另一个图的最小成本变换，是衡量图之间差异的广泛采用的度量标准。GED的主要问题是其计算是NP难的，这反过来又促使了各种近似方法的发展，包括基于神经网络(NN)的方法。大多数这些基于神经网络的模型通过假设单位成本编辑操作来简化GED问题，这在现实应用中是一个相当不现实的约束。在这项工作中，我们提出了一种新颖的图神经网络框架，使用监督和无监督训练来近似GED。在无监督设置中，它采用仅基于梯度的自组织机制，使无需真实距离即可进行优化。此外，我们架构的一个核心组件是广义加性模型的集成，这允许灵活且可解释地学习上下文感知的编辑成本。实验结果表明，所提出的方法与最先进的参考方法取得了相似的结果，但显著提高了适应性和可解释性。也就是说，学习到的成本函数为复杂的图结构提供了见解，使其在分子分析和结构模式发现等领域特别有价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Edit Distance (GED) is defined as the minimum cost transformation ofone graph into another and is a widely adopted metric for measuring thedissimilarity between graphs. The major problem of GED is that its computationis NP-hard, which has in turn led to the development of various approximationmethods, including approaches based on neural networks (NN). Most of theseNN-based models simplify the problem of GED by assuming unit-cost editoperations, a rather unrealistic constraint in real-world applications. In thiswork, we present a novel Graph Neural Network framework that approximates GEDusing both supervised and unsupervised training. In the unsupervised setting,it employs a gradient-only self-organizing mechanism that enables optimizationwithout ground-truth distances. Moreover, a core component of our architectureis the integration of a Generalized Additive Model, which allows the flexibleand interpretable learning of context-aware edit costs. Experimental resultsshow that the proposed method achieves similar results as state-of-the-artreference methods, yet significantly improves both adaptability andinterpretability. That is, the learned cost function offers insights intocomplex graph structures, making it particularly valuable in domains such asmolecular analysis and structural pattern discovery.</description>
      <author>example@mail.com (Francesco Leonardi, Markus Orsi, Jean-Louis Reymond, Kaspar Riesen)</author>
      <guid isPermaLink="false">2508.03111v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Considering Spatial Structure of the Road Network in Pavement Deterioration Modeling</title>
      <link>http://arxiv.org/abs/2508.02749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过图神经网络将道路网络的空间依赖性纳入路面退化建模，以提高预测性能&lt;h4&gt;背景&lt;/h4&gt;路面退化建模对于提供道路网络未来状态信息和确定预防性维护或修复需求非常重要&lt;h4&gt;目的&lt;/h4&gt;探索考虑道路网络的空间结构是否能提高退化模型的预测性能&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)将道路网络的空间依赖性纳入路面退化建模，数据来自德克萨斯州交通部维护的路面管理信息系统(PMIS)，包含超过50万个观测值&lt;h4&gt;主要发现&lt;/h4&gt;比较结果表明，当考虑空间关系时，路面退化预测模型的性能更好&lt;h4&gt;结论&lt;/h4&gt;将空间关系考虑在内的路面退化预测模型表现更好，证明了在建模中考虑道路网络空间结构的重要性&lt;h4&gt;翻译&lt;/h4&gt;路面退化建模对于提供道路网络未来状态信息和确定预防性维护或修复需求非常重要。本研究通过图神经网络(GNN)将道路网络的空间依赖性纳入路面退化建模。使用图神经网络进行路面性能建模的关键动机是能够轻松直接地利用网络中的丰富结构信息。本文探讨了考虑道路网络的空间结构是否能提高退化模型的预测性能。本研究使用的数据集来自德克萨斯州交通部维护的路面管理信息系统(PMIS)，包含超过50万个观测值的有大路面状况数据集。有前景的比较结果表明，当考虑空间关系时，路面退化预测模型的性能更好&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1177/03611981231188373&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pavement deterioration modeling is important in providing informationregarding the future state of the road network and in determining the needs ofpreventive maintenance or rehabilitation treatments. This research incorporatedspatial dependence of road network into pavement deterioration modeling througha graph neural network (GNN). The key motivation of using a GNN for pavementperformance modeling is the ability to easily and directly exploit the richstructural information in the network. This paper explored if consideringspatial structure of the road network will improve the prediction performanceof the deterioration models. The data used in this research comprises a largepavement condition data set with more than a half million observations takenfrom the Pavement Management Information System (PMIS) maintained by the TexasDepartment of Transportation. The promising comparison results indicates thatpavement deterioration prediction models perform better when spatialrelationship is considered.</description>
      <author>example@mail.com (Lu Gao, Ke Yu, Pan Lu)</author>
      <guid isPermaLink="false">2508.02749v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes</title>
      <link>http://arxiv.org/abs/2508.03484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SmartGen，一个基于大型语言模型(LLM)的框架，用于生成情境感知的用户行为数据，以支持智能家居模型在行为漂移情况下的持续适应。该框架包含四个关键组件：时间和语义感知分割模块、语义感知序列压缩、图引导序列合成和两阶段异常过滤器。实验表明，SmartGen显著提高了异常检测和行为预测任务的性能。&lt;h4&gt;背景&lt;/h4&gt;随着智能家居的普及，智能模型被广泛用于异常检测和行为预测等任务。这些模型通常在静态数据集上训练，对由季节变化、生活方式转变或日常习惯演变引起的行为漂移较为脆弱。然而，重新收集行为数据进行再训练往往不切实际，因为数据收集缓慢、成本高且存在隐私问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于LLM的框架SmartGen，用于合成情境感知的用户行为数据，以支持智能家居模型在行为漂移情况下的持续适应。&lt;h4&gt;方法&lt;/h4&gt;SmartGen包含四个关键组件：1)时间和语义感知分割模块，在双时间跨度约束下将长行为序列分割为语义连贯的子序列；2)语义感知序列压缩，通过在潜在空间中对行为映射进行聚类来减少输入长度同时保留代表性语义；3)图引导序列合成，构建行为关系图并将频繁转换编码为提示，指导LLM生成与情境变化一致的数据；4)两阶段异常过滤器，识别并移除不合理或语义不一致的输出。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实验表明，SmartGen在行为漂移情况下显著提高了模型性能，异常检测任务平均提高85.43%，行为预测任务平均提高70.51%。&lt;h4&gt;结论&lt;/h4&gt;SmartGen是一个有效的框架，能够通过合成情境感知的用户行为数据来支持智能家居模型在行为漂移情况下的持续适应，显著提升了模型在异常检测和行为预测任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;随着智能家居变得越来越普遍，智能模型被广泛用于异常检测和行为预测等任务。这些模型通常在静态数据集上训练，对由季节变化、生活方式转变或日常习惯演变引起的行为漂移较为脆弱。然而，由于数据收集缓慢、成本高和隐私问题，重新收集行为数据进行再训练往往不切实际。在本文中，我们提出了SmartGen，一个基于LLM的框架，用于合成情境感知的用户行为数据，以支持下游智能家居模型的持续适应。SmartGen包含四个关键组件：时间和语义感知分割模块、语义感知序列压缩、图引导序列合成和两阶段异常过滤器。在三个真实世界数据集上的实验表明，SmartGen在行为漂移情况下显著提高了异常检测和行为预测任务的性能，异常检测平均提高85.43%，行为预测平均提高70.51%。代码可在https://github.com/horizonsinzqs/SmartGen获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As smart homes become increasingly prevalent, intelligent models are widelyused for tasks such as anomaly detection and behavior prediction. These modelsare typically trained on static datasets, making them brittle to behavioraldrift caused by seasonal changes, lifestyle shifts, or evolving routines.However, collecting new behavior data for retraining is often impractical dueto its slow pace, high cost, and privacy concerns. In this paper, we proposeSmartGen, an LLM-based framework that synthesizes context-aware user behaviordata to support continual adaptation of downstream smart home models. SmartGenconsists of four key components. First, we design a Time and Semantic-awareSplit module to divide long behavior sequences into manageable, semanticallycoherent subsequences under dual time-span constraints. Second, we proposeSemantic-aware Sequence Compression to reduce input length while preservingrepresentative semantics by clustering behavior mapping in latent space. Third,we introduce Graph-guided Sequence Synthesis, which constructs a behaviorrelationship graph and encodes frequent transitions into prompts, guiding theLLM to generate data aligned with contextual changes while retaining corebehavior patterns. Finally, we design a Two-stage Outlier Filter to identifyand remove implausible or semantically inconsistent outputs, aiming to improvethe factual coherence and behavioral validity of the generated sequences.Experiments on three real-world datasets demonstrate that SmartGensignificantly enhances model performance on anomaly detection and behaviorprediction tasks under behavioral drift, with anomaly detection improving by85.43% and behavior prediction by 70.51% on average. The code is available athttps://github.com/horizonsinzqs/SmartGen.</description>
      <author>example@mail.com (Zhiyao Xu, Dan Zhao, Qingsong Zou, Qing Li, Yong Jiang, Yuhang Wang, Jingyu Xiao)</author>
      <guid isPermaLink="false">2508.03484v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio</title>
      <link>http://arxiv.org/abs/2508.02944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page at https://byteaigc.github.io/X-Actor/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;X-Actor是一种新颖的音频驱动肖像动画框架，可以从单个参考图像和输入音频生成逼真、富有情感表现力的说话头部视频。&lt;h4&gt;背景&lt;/h4&gt;先前的方法强调在有限说话场景中的口型同步和短距离视觉保真度，无法实现长形式的、情感丰富的肖像表演。&lt;h4&gt;目的&lt;/h4&gt;开发能够实现演员质量、长形式的肖像表演系统，捕捉与言语节奏和内容流畅连贯的细微动态情感变化。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段解耦生成管道：首先使用音频条件自回归扩散模型在长时间上下文窗口内预测表情丰富但身份无关的面部运动潜在标记；然后通过基于扩散的视频合成模块将这些动作转换为高保真视频动画。&lt;h4&gt;主要发现&lt;/h4&gt;在紧凑的面部运动潜在空间中操作，与视觉和身份线索解耦，通过扩散强制训练范式有效捕捉音频和面部动力学之间的长程相关性，实现无限长度的情感丰富动作预测而不会出现误差累积。&lt;h4&gt;结论&lt;/h4&gt;X-Actor产生了引人入胜的电影风格表演，超越了标准的说话头部动画，在长范围、音频驱动的情感肖像表演方面取得了最先进的结果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了X-Actor，一个新颖的音频驱动肖像动画框架，可以从单个参考图像和输入音频片段生成逼真、富有情感表现力的说话头部视频。与之前强调在有限说话场景中口型同步和短距离视觉保真度的方法不同，X-Actor能够实现演员质量的长形式肖像表演，捕捉与言语节奏和内容流畅连贯的细微动态情感变化。我们方法的核心是一种两阶段解耦生成管道：一个音频条件自回归扩散模型，在长时间上下文窗口内预测表情丰富但身份无关的面部运动潜在标记；随后是一个基于扩散的视频合成模块，将这些动作转换为高保真视频动画。在紧凑的面部运动潜在空间中操作，与视觉和身份线索解耦，我们的自回归扩散模型通过扩散强制训练范式有效捕捉音频和面部动力学之间的长程相关性，实现无限长度的情感丰富动作预测而不会出现误差累积。大量实验表明X-Actor产生了引人入胜的电影风格表演，超越了标准的说话头部动画，在长范围、音频驱动的情感肖像表演方面取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present X-Actor, a novel audio-driven portrait animation framework thatgenerates lifelike, emotionally expressive talking head videos from a singlereference image and an input audio clip. Unlike prior methods that emphasizelip synchronization and short-range visual fidelity in constrained speakingscenarios, X-Actor enables actor-quality, long-form portrait performancecapturing nuanced, dynamically evolving emotions that flow coherently with therhythm and content of speech. Central to our approach is a two-stage decoupledgeneration pipeline: an audio-conditioned autoregressive diffusion model thatpredicts expressive yet identity-agnostic facial motion latent tokens within along temporal context window, followed by a diffusion-based video synthesismodule that translates these motions into high-fidelity video animations. Byoperating in a compact facial motion latent space decoupled from visual andidentity cues, our autoregressive diffusion model effectively captureslong-range correlations between audio and facial dynamics through adiffusion-forcing training paradigm, enabling infinite-length emotionally-richmotion prediction without error accumulation. Extensive experiments demonstratethat X-Actor produces compelling, cinematic-style performances that go beyondstandard talking head animations and achieves state-of-the-art results inlong-range, audio-driven emotional portrait acting.</description>
      <author>example@mail.com (Chenxu Zhang, Zenan Li, Hongyi Xu, You Xie, Xiaochen Zhao, Tianpei Gu, Guoxian Song, Xin Chen, Chao Liang, Jianwen Jiang, Linjie Luo)</author>
      <guid isPermaLink="false">2508.02944v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition</title>
      <link>http://arxiv.org/abs/2508.03695v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025; First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Trokens的新方法，通过将轨迹点转换为语义感知的关系标记，结合自适应采样策略和运动建模框架，有效结合了运动信息和外观特征，在六个少样本动作识别基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;视频理解需要有效建模运动和外观信息，特别是在少样本动作识别任务中。尽管最近的点跟踪技术已显示出对少样本动作识别的改进，但仍然存在两个基本挑战：选择有信息量的跟踪点和有效建模它们的运动模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，解决少样本动作识别中的两个基本挑战：选择有信息量的跟踪点和有效建模运动模式，从而提高动作识别的性能。&lt;h4&gt;方法&lt;/h4&gt;Trokens方法包括两个主要部分：1) 语义感知的采样策略，根据对象尺度和语义相关性自适应分布跟踪点；2) 运动建模框架，通过方向位移直方图(HoD)捕获轨迹内动态，并通过轨迹间关系建模复杂动作模式。该方法将这些轨迹标记与语义特征结合，用运动信息增强外观特征。&lt;h4&gt;主要发现&lt;/h4&gt;Trokens方法在六个不同的少样本动作识别基准测试上取得了最先进的性能，包括Something-Something-V2（完整和小型分割）、Kinetics、UCF101、HMDB51和FineGym。&lt;h4&gt;结论&lt;/h4&gt;通过将轨迹点转换为语义感知的关系标记，并采用自适应采样和运动建模策略，Trokens方法有效解决了少样本动作识别中的关键挑战，显著提升了性能。&lt;h4&gt;翻译&lt;/h4&gt;视频理解需要对运动和外观信息进行有效建模，特别是在少样本动作识别方面。尽管最近在点跟踪方面的进展已被证明可以改善少样本动作识别，但两个基本挑战仍然存在：选择信息丰富的跟踪点和有效建模它们的运动模式。我们提出了Trokens，一种新颖的方法，将轨迹点转换为用于动作识别的语义感知关系标记。首先，我们引入了一种语义感知的采样策略，根据对象尺度和语义相关性自适应分布跟踪点。其次，我们开发了一个运动建模框架，通过方向位移直方图(HoD)捕获轨迹内动态，并通过轨迹间关系建模复杂动作模式。我们的方法将这些轨迹标记与语义特征有效结合，用运动信息增强外观特征，在六个不同的少样本动作识别基准测试上实现了最先进的性能：Something-Something-V2（完整和小型分割）、Kinetics、UCF101、HMDB51和FineGym。项目页面请见https://trokens-iccv25.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding requires effective modeling of both motion and appearanceinformation, particularly for few-shot action recognition. While recentadvances in point tracking have been shown to improve few-shot actionrecognition, two fundamental challenges persist: selecting informative pointsto track and effectively modeling their motion patterns. We present Trokens, anovel approach that transforms trajectory points into semantic-aware relationaltokens for action recognition. First, we introduce a semantic-aware samplingstrategy to adaptively distribute tracking points based on object scale andsemantic relevance. Second, we develop a motion modeling framework thatcaptures both intra-trajectory dynamics through the Histogram of OrientedDisplacements (HoD) and inter-trajectory relationships to model complex actionpatterns. Our approach effectively combines these trajectory tokens withsemantic features to enhance appearance features with motion information,achieving state-of-the-art performance across six diverse few-shot actionrecognition benchmarks: Something-Something-V2 (both full and small splits),Kinetics, UCF101, HMDB51, and FineGym. For project page seehttps://trokens-iccv25.github.io</description>
      <author>example@mail.com (Pulkit Kumar, Shuaiyi Huang, Matthew Walmer, Sai Saketh Rambhatla, Abhinav Shrivastava)</author>
      <guid isPermaLink="false">2508.03695v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations</title>
      <link>http://arxiv.org/abs/2508.03420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025. 11 pages, 4 figures. Code:  https://github.com/wangbing1416/MISDER&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MISDER的新型虚假信息检测框架，该框架考虑了社会环境的动态变化，通过学习每个时期的社会环境表征并预测未来表征来提高虚假信息检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;虚假信息在各类社交媒体平台上的广泛传播引起了学术界和工业界的关注，自动区分虚假信息（MD）已成为一个活跃的研究领域。&lt;h4&gt;目的&lt;/h4&gt;解决主流虚假信息检测方法中静态学习范式的局限性，考虑新闻真实性在动态变化的社会环境中的波动性。&lt;h4&gt;方法&lt;/h4&gt;提出MISDER框架，学习每个时期的社会环境表征，并使用时间模型预测未来时期的表征。具体包括三种变体：MISDER-LSTM（使用LSTM模型）、MISDER-ODE（使用连续动力学方程）和MISDER-PT（使用预训练动力学系统）。&lt;h4&gt;主要发现&lt;/h4&gt;在两个流行数据集上的实验结果表明，所提出的MISDER模型比各种MD基线方法更有效，证明了考虑社会环境动态变化对虚假信息检测的重要性。&lt;h4&gt;结论&lt;/h4&gt;通过引入动态环境表征，MISDER框架能够更好地捕捉虚假信息在动态社会环境中的变化特性，提高了虚假信息检测的准确性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;虚假信息在各类社交媒体平台上的广泛传播由于其有害效应而引起了学术界和工业界的显著关注。因此，自动区分虚假信息（被称为虚假信息检测MD）已成为一个日益活跃的研究课题。主流方法将MD构建为静态学习范式，学习新闻文章的内容、链接和传播与相应的人工真实性标签之间的映射。然而，在现实场景中，静态假设经常被违反，因为新闻文章的真实性可能在动态演变的社会环境中波动。为了解决这个问题，我们提出了一个新框架，即带有动态环境表征的虚假信息检测（MISDER）。MISDER的基本思想在于为每个时期学习一个社会环境表征，并使用时间模型来预测未来时期的表征。在这项工作中，我们将时间模型指定为LSTM模型、连续动力学方程和预训练动力学系统，分别提出了MISDER的三种变体，即MISDER-LSTM、MISDER-ODE和MISDER-PT。为了评估MISDER的性能，我们在两个流行数据集上将其与各种MD基线方法进行了比较，实验结果可以证明我们提出的模型的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of misinformation across diverse social media platforms hasdrawn significant attention from both academic and industrial communities dueto its detrimental effects. Accordingly, automatically distinguishingmisinformation, dubbed as Misinformation Detection (MD), has become anincreasingly active research topic. The mainstream methods formulate MD as astatic learning paradigm, which learns the mapping between the content, links,and propagation of news articles and the corresponding manual veracity labels.However, the static assumption is often violated, since in real-worldscenarios, the veracity of news articles may vacillate within the dynamicallyevolving social environment. To tackle this problem, we propose a novelframework, namely Misinformation detection with Dynamic EnvironmentalRepresentations (MISDER). The basic idea of MISDER lies in learning a socialenvironmental representation for each period and employing a temporal model topredict the representation for future periods. In this work, we specify thetemporal model as the LSTM model, continuous dynamics equation, and pre-traineddynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,we compare it to various MD baselines across 2 prevalent datasets, and theexperimental results can indicate the effectiveness of our proposed model.</description>
      <author>example@mail.com (Bing Wang, Ximing Li, Yiming Wang, Changchun Li, Jiaxu Cui, Renchu Guan, Bo Yang)</author>
      <guid isPermaLink="false">2508.03420v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2508.03251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  European Conference of Artificial Intelligence 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种全历史图表示方法和边类型解耦网络(ETDNet)，用于建模实体间随时间演化的交互关系，在驾驶员意图预测和比特币欺诈检测任务上超越了现有基线方法。&lt;h4&gt;背景&lt;/h4&gt;在许多现实世界任务中，建模实体间随时间演化的交互关系至关重要，如预测驾驶员操作需要跟踪相邻车辆的相对运动，检测金融欺诈需要追踪资金在网络中的传播路径。这些任务不同于经典时间序列预测，需要推理谁与谁交互以及何时交互。&lt;h4&gt;目的&lt;/h4&gt;提出一种时间图表示方法，使关系及其演化都显式化，以解决需要推理实体间交互关系及其随时间变化的问题。&lt;h4&gt;方法&lt;/h4&gt;提出全历史图，为每个实体在每个时间步创建节点，并分离两种边集：时间内步边捕获单个帧内的关系，时间间步边连接实体在连续步骤中的自身。设计边类型解耦网络，包含图注意力模块、多头时间注意力模块和融合模块。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo驾驶员意图预测和Elliptic++比特币欺诈检测任务上，ETDNet超越强基线：将Waymo联合准确率提升至75.6%(对比74.1%)，将Elliptic++非法类别F1值提升至88.1%(对比60.4%)。&lt;h4&gt;结论&lt;/h4&gt;这些性能提升证明了在单一图中将结构关系和时间关系表示为不同边的好处。&lt;h4&gt;翻译&lt;/h4&gt;建模实体间不断演化的交互关系在许多现实世界任务中至关重要。例如，预测交通中的驾驶员操作需要跟踪相邻车辆在连续帧中如何加速、刹车和变道。同样，检测金融欺诈依赖于追踪资金通过连续交易在网络中传播的路径。与经典时间序列预测不同，这些场景需要推理谁与谁交互以及何时交互，这需要一种时间图表示方法，使关系及其演化都显式化。现有的时间图方法通常使用快照图来编码时间演化。我们引入了一种全历史图，为每个实体在每个时间步实例化一个节点，并分离两种边集：(i)时间内步边捕获单个帧内的关系，(ii)时间间步边将实体连接到其连续步骤中的自身。为了在此图上进行学习，我们设计了边类型解耦网络(ETDNet)，包含并行模块：图注意力模块沿时间内步边聚合信息，多头时间注意力模块关注实体的时间间步历史，融合模块在每一层后组合这两种消息。在驾驶员意图预测(Waymo)和比特币欺诈检测(Elliptic++)上评估，ETDNet一致超越强基线，将Waymo联合准确率提升至75.6%(对比74.1%)，将Elliptic++非法类别F1提升至88.1%(对比60.4%)。这些增益证明了在单一图中将结构关系和时间关系表示为不同边的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling evolving interactions among entities is critical in many real-worldtasks. For example, predicting driver maneuvers in traffic requires trackinghow neighboring vehicles accelerate, brake, and change lanes relative to oneanother over consecutive frames. Likewise, detecting financial fraud hinges onfollowing the flow of funds through successive transactions as they propagatethrough the network. Unlike classic time-series forecasting, these settingsdemand reasoning over who interacts with whom and when, calling for atemporal-graph representation that makes both the relations and their evolutionexplicit. Existing temporal-graph methods typically use snapshot graphs toencode temporal evolution. We introduce a full-history graph that instantiatesone node for every entity at every time step and separates two edge sets: (i)intra-time-step edges that capture relations within a single frame and (ii)inter-time-step edges that connect an entity to itself at consecutive steps. Tolearn on this graph we design an Edge-Type Decoupled Network (ETDNet) withparallel modules: a graph-attention module aggregates information alongintra-time-step edges, a multi-head temporal-attention module attends over anentity's inter-time-step history, and a fusion module combines the two messagesafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoinfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,lifting Waymo joint accuracy to 75.6\% (vs. 74.1\%) and raising Elliptic++illicit-class F1 to 88.1\% (vs. 60.4\%). These gains demonstrate the benefit ofrepresenting structural and temporal relations as distinct edges in a singlegraph.</description>
      <author>example@mail.com (Osama Mohammed, Jiaxin Pan, Mojtaba Nayyeri, Daniel Hernández, Steffen Staab)</author>
      <guid isPermaLink="false">2508.03251v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering</title>
      <link>http://arxiv.org/abs/2508.03039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VideoForest框架，通过以人为中心的分层推理解决跨视频问答的挑战，利用人物级特征作为视频间的自然桥梁，无需端到端训练即可实现有效的跨视频理解。&lt;h4&gt;背景&lt;/h4&gt;跨视频问答面临显著挑战，超越了传统的单视频理解，特别是在建立视频流之间的有意义连接和管理多源信息检索的复杂性方面。&lt;h4&gt;目的&lt;/h4&gt;引入VideoForest框架解决跨视频理解挑战，并开发CrossVideoQA基准数据集专门用于以人为中心的跨视频分析。&lt;h4&gt;方法&lt;/h4&gt;VideoForest整合三个关键创新：1)以人为中心特征提取机制，使用ReID和跟踪算法建立多视频源间的时空关系；2)多粒度生成树结构，围绕人物级轨迹分层组织视觉内容；3)多智能体推理框架，有效遍历分层结构回答复杂跨视频查询。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明VideoForest在跨视频推理任务中表现优越：人物识别准确率71.93%，行为分析83.75%，总结和推理51.67%，显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过以人物级特征统一多个视频流，VideoForest建立了跨视频理解的新范式，能够在保持计算效率的同时实现分布式视觉信息的高级推理。&lt;h4&gt;翻译&lt;/h4&gt;跨视频问答提出了超越传统单视频理解的重大挑战，特别是在建立视频流之间的有意义连接和管理多源信息检索的复杂性方面。我们引入了VideoForest，这是一个新颖的框架，通过以人为中心的分层推理来解决这些挑战。我们的方法利用人物级特征作为视频之间的自然桥梁点，实现了有效的跨视频理解，而无需端到端训练。VideoForest整合了三个关键创新：1)采用ReID和跟踪算法的以人为中心特征提取机制，在多个视频源之间建立强大的时空关系；2)多粒度生成树结构，围绕人物级轨迹分层组织视觉内容；3)多智能体推理框架，有效遍历此分层结构以回答复杂的跨视频查询。为了评估我们的方法，我们开发了CrossVideoQA，这是一个专门设计用于以人为中心的跨视频分析的综合基准数据集。实验结果表明VideoForest在跨视频推理任务中具有卓越性能，在人物识别方面达到71.93%的准确率，在行为分析方面达到83.75%，在总结和推理方面达到51.67%，显著优于现有方法。我们的工作通过以人物级特征统一多个视频流，为跨视频理解建立了新范式，能够在保持计算效率的同时实现对分布式视觉信息的高级推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3754573&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-video question answering presents significant challenges beyondtraditional single-video understanding, particularly in establishing meaningfulconnections across video streams and managing the complexity of multi-sourceinformation retrieval. We introduce VideoForest, a novel framework thataddresses these challenges through person-anchored hierarchical reasoning. Ourapproach leverages person-level features as natural bridge points betweenvideos, enabling effective cross-video understanding without requiringend-to-end training. VideoForest integrates three key innovations: 1) ahuman-anchored feature extraction mechanism that employs ReID and trackingalgorithms to establish robust spatiotemporal relationships across multiplevideo sources; 2) a multi-granularity spanning tree structure thathierarchically organizes visual content around person-level trajectories; and3) a multi-agent reasoning framework that efficiently traverses thishierarchical structure to answer complex cross-video queries. To evaluate ourapproach, we develop CrossVideoQA, a comprehensive benchmark datasetspecifically designed for person-centric cross-video analysis. Experimentalresults demonstrate VideoForest's superior performance in cross-video reasoningtasks, achieving 71.93% accuracy in person recognition, 83.75% in behavioranalysis, and 51.67% in summarization and reasoning, significantlyoutperforming existing methods. Our work establishes a new paradigm forcross-video understanding by unifying multiple video streams throughperson-level features, enabling sophisticated reasoning across distributedvisual information while maintaining computational efficiency.</description>
      <author>example@mail.com (Yiran Meng, Junhong Ye, Wei Zhou, Guanghui Yue, Xudong Mao, Ruomei Wang, Baoquan Zhao)</author>
      <guid isPermaLink="false">2508.03039v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Long Video Question Answering with Scene-Localized Frame Grouping</title>
      <link>http://arxiv.org/abs/2508.03009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SLFG的新方法，通过场景帧重组机制增强多模态大语言模型在长视频理解中的表现，无需修改原始模型架构即可实现即插即用。&lt;h4&gt;背景&lt;/h4&gt;当前多模态大语言模型在长视频理解方面表现不佳，主要受限于资源无法处理所有视频帧。现有评估方法专注于识别特定帧而非整体场景理解，与实际应用需求不符。&lt;h4&gt;目的&lt;/h4&gt;提出新的视频问答任务场景SceneQA，强调基于场景的细节感知和推理能力，并开发LVSQA数据集以支持更公平的模型评估。&lt;h4&gt;方法&lt;/h4&gt;受人类认知启发，SLFG方法将单个帧组合成语义连贯的场景帧，利用场景定位和动态帧重组机制增强模型理解能力，无需修改原始架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SLFG方法在多个长视频基准测试中表现优异，代码和数据集将在指定网址发布。&lt;h4&gt;结论&lt;/h4&gt;SLFG方法有效解决了长视频理解中的资源限制问题，通过场景重组提高了模型效率，同时保持了良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;当前多模态大语言模型通常在长视频理解中表现不佳，主要由于资源限制导致无法处理所有视频帧及其相关信息。高效提取相关信息成为一项挑战。现有框架和评估任务专注于从大量无关帧中识别包含核心对象的特定帧，这与现实世界应用的实际需求不符。为解决这一问题，我们在视频问答任务中提出了一种新场景SceneQA，强调基于场景的细节感知和推理能力。我们开发了LVSQA数据集来支持SceneQA任务，该数据集基于从LVBench中精心挑选的视频构建，包含新的问答对集合，以促进对MLLMs在长视频中场景感知能力的更公平评估。受人类认知启发，我们引入了一种名为SLFG的新方法。SLFG的核心思想是将单个帧组合成语义连贯的场景帧。通过利用场景定位方法和动态帧重组机制，SLFG显著增强了现有MLLMs对长视频的理解能力。SLFG无需修改原始模型架构，并具有出色的即插即用可用性。实验结果表明，该方法在多个长视频基准测试中表现异常出色。代码和数据集将在http://www.slfg.pkuzwh.cn发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型在长视频理解中表现不佳的问题。当前模型因资源限制无法处理视频所有帧，现有方法侧重于从大量帧中识别特定对象帧，这与现实应用需求不符。这个问题很重要，因为长视频理解对于视频内容分析、自动摘要、智能监控等应用至关重要，且现有方法无法满足对场景细节感知和推理能力的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类认知过程中获取灵感，指出人类处理长视频时不是逐帧分析，而是浏览视频并对场景转换保持敏感，然后专注于相关场景内的细节。基于这一认知，作者认为模型分析的基本单元应从单个帧转向语义连贯的场景。作者借鉴了现有工作中的视频帧采样、分组技术、大语言模型生成场景表示、语义相似度计算等方法，但将这些技术组合成一个专注于场景级别理解的新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将单个帧组合成语义连贯的场景帧，通过场景定位方法和动态帧重组机制增强模型对长视频的理解能力。整体流程包括四个阶段：1）帧分组描述：密集采样视频帧并分组，用多模态大语言模型提取视觉描述；2）场景生成：用大语言模型将视觉描述抽象为场景级表示；3）场景定位：计算问题场景与各帧组场景的语义相似度；4）帧重组：根据相似度分数调整帧组结构，合并高相关性组，扩展关键片段时间窗口。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出SceneQA任务场景，强调基于场景的细节感知和推理能力；2）构建LVSQA数据集，支持对MLLMs长视频场景感知能力的评估；3）提出SLFG方法，将帧组织成语义连贯的场景单元，无需修改原始模型架构。相比之前工作，本文实现了分析单元从单个帧到语义连贯场景的转变，评估方法从识别特定帧转向理解整个场景，通过场景级信息压缩提升了处理效率，并增强了方法的通用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了基于场景定位的帧分组方法(SLFG)，通过将视频帧组织成语义连贯的场景单元，显著提升了多模态大语言模型对长视频的理解能力，并构建了新的评估基准LVSQA以促进长视频理解研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Multimodal Large Language Models (MLLMs) often perform poorly in longvideo understanding, primarily due to resource limitations that prevent themfrom processing all video frames and their associated information. Efficientlyextracting relevant information becomes a challenging task. Existing frameworksand evaluation tasks focus on identifying specific frames containing coreobjects from a large number of irrelevant frames, which does not align with thepractical needs of real-world applications. To address this issue, we propose anew scenario under the video question-answering task, SceneQA, which emphasizesscene-based detail perception and reasoning abilities. And we develop the LVSQAdataset to support the SceneQA task, which is built upon carefully selectedvideos from LVBench and contains a new collection of question-answer pairs topromote a more fair evaluation of MLLMs' scene perception abilities in longvideos. Inspired by human cognition, we introduce a novel method called SLFG.The core idea of SLFG is to combine individual frames into semanticallycoherent scene frames. By leveraging scene localization methods and dynamicframe reassembly mechanisms, SLFG significantly enhances the understandingcapabilities of existing MLLMs in long videos. SLFG requires no modification tothe original model architecture and boasts excellent plug-and-play usability.Experimental results show that this method performs exceptionally well inseveral long video benchmark tests. Code and dataset will be released athttp://www.slfg.pkuzwh.cn.</description>
      <author>example@mail.com (Xuyi Yang, Wenhao Zhang, Hongbo Jin, Lin Liu, Hongbo Xu, Yongwei Nie, Fei Yu, Fei Ma)</author>
      <guid isPermaLink="false">2508.03009v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion</title>
      <link>http://arxiv.org/abs/2508.03252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RSDNet的单阶段全稀疏3D目标检测网络，通过可分离的潜在框架改进了去噪扩散概率模型，实现了高效且鲁棒的3D目标检测。&lt;h4&gt;背景&lt;/h4&gt;DDPMs在鲁棒的3D目标检测任务中显示出成功，但现有方法通常依赖于3D框的分数匹配或预训练的扩散先验，且需要在推理时进行多步迭代，限制了效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法效率低下的问题，提出一种单阶段、全稀疏的3D目标检测网络，实现高效且鲁棒的检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出RSDNet，通过轻量级去噪网络在潜在特征空间学习去噪过程；重构DDPMs的加噪和去噪机制，构建多类型和多级别的噪声样本和目标；引入语义-几何条件引导感知物体边界和形状；设计可分离的去噪网络实现单步检测。&lt;h4&gt;主要发现&lt;/h4&gt;RSDNet能在多级扰动下有效理解场景分布，实现鲁棒可靠的检测；在全稀疏检测流程中表现良好；可分离的去噪网络设计提高了检测效率。&lt;h4&gt;结论&lt;/h4&gt;在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测性能。&lt;h4&gt;翻译&lt;/h4&gt;去噪扩散概率模型已在鲁棒的3D目标检测任务中显示出成功。现有方法通常依赖于3D框的分数匹配或预训练的扩散先验。然而，它们通常需要在推理时进行多步迭代，这限制了效率。为此，我们提出了一种具有DDPMs可分离潜在框架的鲁棒单阶段全稀疏3D目标检测网络，命名为RSDNet。具体来说，RSDNet通过轻量级去噪网络在潜在特征空间中学习去噪过程。这使得RSDNet能够在多级扰动下有效理解场景分布，实现鲁棒可靠的检测。同时，我们重构了DDPMs的加噪和去噪机制，使DLF能够构建多类型和多级别的噪声样本和目标，增强RSDNet对多种扰动的鲁棒性。此外，引入了语义-几何条件引导以感知物体边界和形状，缓解了稀疏表示中的中心特征缺失问题，使RSDNet能够在全稀疏检测流程中运行。而且，DLF的可分离去噪网络设计使RSDNet能够在推理时进行单步检测，进一步提高检测效率。在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D目标检测中的多步推理效率和噪声鲁棒性问题。现有基于DDPMs的3D目标检测方法需要多步迭代推理，限制了检测效率，同时难以应对点云数据中存在的多种扰动(如点级随机噪声和全局几何失真)。这个问题在自动驾驶、AR/VR和机器人等实时应用中至关重要，因为这些应用需要高效且可靠的3D目标检测，而实际场景中的传感器数据常常受到各种噪声影响。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了DDPMs的鲁棒性来源，发现其鲁棒性来自训练阶段构建的噪声样本和目标，而非推理过程本身。然后重新思考了DDPMs的加噪和解噪机制，提出'样本拟合'概念，将复杂的分布匹配问题简化为简单的样本拟合问题。基于这些洞察，设计了可分离的潜在框架(DLF)。作者借鉴了DDPMs的基本原理、单阶段全稀疏3D检测管道(如CenterPoint、PillarNeXt)以及去噪自编码器(DAEs)的设计，将其作为轻量级去噪网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个可分离的潜在框架(DLF)，将去噪网络作为辅助分支，在潜在特征空间中指导主干网络学习多类型和多级别的去噪过程，训练阶段构建多类型噪声样本和目标以增强鲁棒性，推理阶段分离去噪网络实现单步检测。整体流程包括：1)全稀疏管道(FSP)使用3D和2D稀疏主干提取特征；2)噪声构建模块(NCM)构建多类型噪声样本；3)语义-几何条件层(SGCL)嵌入对象边界和形状的先验知识；4)去噪U-Net(DUNet)包括3DDU和2DDU两个轻量级网络指导去噪学习。训练时计算扩散损失和任务损失，推理时只需单步即可完成检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)可分离的潜在框架(DLF)，实现单步推理同时保持多噪声鲁棒性；2)多类型和多级别的噪声建模，使模型能应对点级随机噪声和全局几何失真等多种扰动；3)语义-几何条件引导，缓解下采样导致的中心特征缺失问题；4)基于DLF的RSDNet实现单步全稀疏检测。与传统DDPMs方法不同，DLF不是通过迭代去噪估计边界框分数，而是在潜在特征空间学习去噪过程；与只能处理高斯噪声的传统方法不同，DLF能处理多种扰动；与现有全稀疏检测方法相比，RSDNet特别强调了鲁棒性而不仅仅是效率和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种可分离的潜在框架(DLF)，通过在潜在特征空间中学习多类型和多级别的去噪过程，实现了单步推理的鲁棒3D目标检测，同时保持了对多种扰动的强鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust3D object detection tasks. Existing methods often rely on the score matchingfrom 3D boxes or pre-trained diffusion priors. However, they typically requiremulti-step iterations in inference, which limits efficiency. To address this,we propose a \textbf{R}obust single-stage fully \textbf{S}parse 3D object\textbf{D}etection \textbf{Net}work with a Detachable Latent Framework (DLF) ofDDPMs, named RSDNet. Specifically, RSDNet learns the denoising process inlatent feature spaces through lightweight denoising networks like multi-leveldenoising autoencoders (DAEs). This enables RSDNet to effectively understandscene distributions under multi-level perturbations, achieving robust andreliable detection. Meanwhile, we reformulate the noising and denoisingmechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noisesamples and targets, enhancing RSDNet robustness to multiple perturbations.Furthermore, a semantic-geometric conditional guidance is introduced toperceive the object boundaries and shapes, alleviating the center featuremissing problem in sparse representations, enabling RSDNet to perform in afully sparse detection pipeline. Moreover, the detachable denoising networkdesign of DLF enables RSDNet to perform single-step detection in inference,further enhancing detection efficiency. Extensive experiments on publicbenchmarks show that RSDNet can outperform existing methods, achievingstate-of-the-art detection.</description>
      <author>example@mail.com (Wentao Qu, Guofeng Mei, Jing Wang, Yujiao Wu, Xiaoshui Huang, Liang Xiao)</author>
      <guid isPermaLink="false">2508.03252v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Model Semantics in Representation Learning</title>
      <link>http://arxiv.org/abs/2508.03649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了结构约束如何影响不同架构间内部表示的兼容性，发现结构规律性使表示几何结构在架构变化下更稳定，某些归纳偏差能提高跨模型特征的互操作性。&lt;h4&gt;背景&lt;/h4&gt;深度网络学习到的内部表示通常对架构特定选择敏感，引发了对学习结构在不同模型间稳定性、对齐性和可转移性的疑问。&lt;h4&gt;目的&lt;/h4&gt;研究结构约束（如线性整形算子和校正路径）如何影响不同架构间内部表示的兼容性。&lt;h4&gt;方法&lt;/h4&gt;基于对结构化变换和收敛的先前研究见解，开发测量和分析不同架构网络间表示对齐的框架，结合理论分析、经验探针和受控转移实验。&lt;h4&gt;主要发现&lt;/h4&gt;结构规律性诱导的表示几何结构在架构变化下更加稳定，某些形式的归纳偏差不仅支持模型内的泛化，还提高了跨模型学习特征的互操作性。&lt;h4&gt;结论&lt;/h4&gt;表示可转移性对模型蒸馏、模块化学习和鲁棒学习系统的原则性设计具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;深度网络学习到的内部表示通常对架构特定选择敏感，引发了对学习结构在不同模型间稳定性、对齐性和可转移性的问题。在本文中，我们研究结构约束（如线性整形算子和校正路径）如何影响不同架构间内部表示的兼容性。基于对结构化变换和收敛的先前研究的见解，我们开发了一个框架，用于测量和分析具有不同但相关架构先验的网络之间的表示对齐。通过结合理论见解、经验探针和受控的转移实验，我们证明结构规律性诱导的表示几何结构在架构变化下更加稳定。这表明某些形式的归纳偏差不仅支持模型内的泛化，还提高了跨模型学习特征的互操作性。最后，我们讨论了表示可转移性对模型蒸馏、模块化学习和鲁棒学习系统原则性设计的意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The internal representations learned by deep networks are often sensitive toarchitecture-specific choices, raising questions about the stability,alignment, and transferability of learned structure across models. In thispaper, we investigate how structural constraints--such as linear shapingoperators and corrective paths--affect the compatibility of internalrepresentations across different architectures. Building on the insights fromprior studies on structured transformations and convergence, we develop aframework for measuring and analyzing representational alignment acrossnetworks with distinct but related architectural priors. Through a combinationof theoretical insights, empirical probes, and controlled transfer experiments,we demonstrate that structural regularities induce representational geometrythat is more stable under architectural variation. This suggests that certainforms of inductive bias not only support generalization within a model, butalso improve the interoperability of learned features across models. Weconclude with a discussion on the implications of representationaltransferability for model distillation, modular learning, and the principleddesign of robust learning systems.</description>
      <author>example@mail.com (Saleh Nikooroo, Thomas Engel)</author>
      <guid isPermaLink="false">2508.03649v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Single Collaborative Branch for Recommendation</title>
      <link>http://arxiv.org/abs/2508.03518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于权重共享的新型推荐系统CoBraR，通过减少参数数量并提高准确性以外的方面，在不损害准确性的情况下提升推荐性能。&lt;h4&gt;背景&lt;/h4&gt;推荐系统通常依赖于用户和物品在联合嵌入空间中的表示以及相似度度量来计算相关性得分。在现代推荐系统中，获取用户和物品表示的模块由两个独立且分离的神经网络组成。&lt;h4&gt;目的&lt;/h4&gt;受多模态表示学习中权重共享方法的启发，提出一种利用用户和物品神经网络模块之间权重共享的新型推荐系统，以获取共享嵌入空间中的潜在表示。&lt;h4&gt;方法&lt;/h4&gt;提出的框架由一个单一的推荐协作分支(CoBraR)组成，该框架通过在用户和物品神经网络模块之间实现权重共享来减少参数数量并提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过在电子商务和电影推荐方面的定量实验，发现CoBraR能够减少参数数量，在不损害准确性的情况下提高准确性以外的方面。&lt;h4&gt;结论&lt;/h4&gt;CoBraR有潜力被应用于和扩展到实际场景中。&lt;h4&gt;翻译&lt;/h4&gt;推荐系统(RS)通常依赖于用户和物品在联合嵌入空间中的表示以及相似度度量来计算相关性得分。在现代RS中，获取用户和物品表示的模块由两个独立且分离的神经网络(NN)组成。在多模态表示学习中，权重共享已被证明在减少同一物品多个模态之间的距离方面是有效的。受这些方法的启发，我们提出了一种新型RS，它利用用户和物品神经网络模块之间的权重共享，以获取共享嵌入空间中的潜在表示。所提出的框架由一个单一的推荐协作分支(CoBraR)组成。我们通过电子商务和电影推荐的定量实验来评估CoBraR。我们的实验表明，通过减少参数数量并提高准确性以外的方面而不损害准确性，CoBraR有潜力被应用于和扩展到实际场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3705328.3759302&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommender Systems (RS) often rely on representations of users and items ina joint embedding space and on a similarity metric to compute relevance scores.In modern RS, the modules to obtain user and item representations consist oftwo distinct and separate neural networks (NN). In multimodal representationlearning, weight sharing has been proven effective in reducing the distancebetween multiple modalities of a same item. Inspired by these approaches, wepropose a novel RS that leverages weight sharing between the user and item NNmodules used to obtain the latent representations in the shared embeddingspace. The proposed framework consists of a single Collaborative Branch forRecommendation (CoBraR). We evaluate CoBraR by means of quantitativeexperiments on e-commerce and movie recommendation. Our experiments show thatby reducing the number of parameters and improving beyond-accuracy aspectswithout compromising accuracy, CoBraR has the potential to be applied andextended for real-world scenarios.</description>
      <author>example@mail.com (Marta Moscati, Shah Nawaz, Markus Schedl)</author>
      <guid isPermaLink="false">2508.03518v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Imputation Drives Cross-Domain Alignment for EEG Classification</title>
      <link>http://arxiv.org/abs/2508.03437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACMMM 2025 poster&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IMAC是一种创新的通道依赖掩码和插值自监督框架，通过将跨域EEG数据偏移对齐表述为时空序列插值任务，解决了因异构电极配置、采集协议和硬件差异导致的数据分布偏移问题，在多个EEG数据集上实现了最先进的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)信号分类面临重大挑战，由于不同领域间的异构电极配置、采集协议和硬件差异导致数据分布偏移，影响了分类性能。&lt;h4&gt;目的&lt;/h4&gt;开发IMAC框架，将跨域EEG数据偏移的对齐表述为时空序列插值任务，以解决异构电极配置带来的数据分布偏移问题。&lt;h4&gt;方法&lt;/h4&gt;IMAC采用3D到2D位置统一映射策略标准化不同电极布局，建立统一空间表示；引入时空信号对齐，构建通道依赖掩码和重建任务，模拟跨域变化；采用解耦结构分别建模EEG信号的时间和空间信息，降低计算复杂度并提高灵活性。&lt;h4&gt;主要发现&lt;/h4&gt;在10个公开EEG数据集上全面评估，IMAC在跨subject和跨中心验证场景中实现最先进分类准确率；在模拟和真实世界分布偏移下表现出强大鲁棒性，完整性得分比基线方法高35%，同时保持一致的分类准确率。&lt;h4&gt;结论&lt;/h4&gt;IMAC能有效处理跨域EEG数据分布偏移问题，通过创新的插值和解耦结构实现高性能信号分类，展现出强大的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)信号分类由于跨领域异构电极配置、采集协议和硬件差异导致的数据分布偏移而面临重大挑战。本文介绍了IMAC，一种新颖的通道依赖掩码和插值自监督框架，将跨域EEG数据偏移的对齐表述为时空序列插值任务。为解决跨域场景中的异构电极配置问题，IMAC首先使用3D到2D位置统一映射策略标准化不同电极布局，建立统一空间表示。与以往基于掩码的自监督表示学习方法不同，IMAC引入了时空信号对齐。这包括构建通道依赖掩码和重建任务，表述为低到高分辨率的EEG空间插值问题。因此，该方法模拟了通道缺失和时间不稳定性等跨域变化，使模型能够在推理过程中利用提出的插值器进行稳健信号对齐。此外，IMAC采用解耦结构，分别建模EEG信号的时间和空间信息，降低计算复杂度同时提高灵活性和适应性。在10个公开可用的EEG数据集上的全面评估表明IMAC的卓越性能，在跨subject和跨中心验证场景中均实现了最先进的分类准确率。值得注意的是，IMAC在模拟和真实世界分布偏移下表现出强大的鲁棒性，完整性得分比基线方法高出35%，同时保持一致的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755582&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalogram (EEG) signal classification faces significant challengesdue to data distribution shifts caused by heterogeneous electrodeconfigurations, acquisition protocols, and hardware discrepancies acrossdomains. This paper introduces IMAC, a novel channel-dependent mask andimputation self-supervised framework that formulates the alignment ofcross-domain EEG data shifts as a spatial time series imputation task. Toaddress heterogeneous electrode configurations in cross-domain scenarios, IMACfirst standardizes different electrode layouts using a 3D-to-2D positionalunification mapping strategy, establishing unified spatial representations.Unlike previous mask-based self-supervised representation learning methods,IMAC introduces spatio-temporal signal alignment. This involves constructing achannel-dependent mask and reconstruction task framed as a low-to-highresolution EEG spatial imputation problem. Consequently, this approachsimulates cross-domain variations such as channel omissions and temporalinstabilities, thus enabling the model to leverage the proposed imputer forrobust signal alignment during inference. Furthermore, IMAC incorporates adisentangled structure that separately models the temporal and spatialinformation of the EEG signals separately, reducing computational complexitywhile enhancing flexibility and adaptability. Comprehensive evaluations across10 publicly available EEG datasets demonstrate IMAC's superior performance,achieving state-of-the-art classification accuracy in both cross-subject andcross-center validation scenarios. Notably, IMAC shows strong robustness underboth simulated and real-world distribution shifts, surpassing baseline methodsby up to $35$\% in integrity scores while maintaining consistent classificationaccuracy.</description>
      <author>example@mail.com (Hongjun Liu, Chao Yao, Yalan Zhang, Xiaokun wang, Xiaojuan Ban)</author>
      <guid isPermaLink="false">2508.03437v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN</title>
      <link>http://arxiv.org/abs/2508.03415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is currently under review for publication in an IEEE  Transactions. If accepted, the copyright will be transferred to IEEE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Fd-CycleGAN，一个图像到图像的翻译框架，通过增强潜在表示学习来近似真实数据分布。该方法基于CycleGAN进行改进，整合了局部邻域编码和频率感知监督，在多个数据集上展示了优于基线方法的性能。&lt;h4&gt;背景&lt;/h4&gt;图像到图像翻译是计算机视觉的重要任务，CycleGAN是该领域的基础框架，但在捕获细粒度局部语义和保持结构连贯性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;增强潜在表示学习以近似真实数据分布，提高图像翻译的感知质量、收敛速度和模式多样性，特别是在低数据情况下。&lt;h4&gt;方法&lt;/h4&gt;提出Fd-CycleGAN框架，集成局部邻域编码和频率感知监督；使用基于分布的损失指标包括KL/JS散度和基于对数的相似性度量；在空间和频率域上量化真实和生成图像分布的对齐情况；在Horse2Zebra、Monet2Photo和合成的Strike-off数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;Fd-CycleGAN相比基线方法具有更好的感知质量、更快的收敛速度和改进的模式多样性；在低数据情况下表现尤为突出；通过有效捕获局部和全局分布特征，实现了视觉上更连贯和语义上更一致的翻译；频率引导的潜在学习显著提高了图像翻译任务的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;频率引导的潜在学习可以显著提高图像翻译任务的泛化能力，有希望应用于文档恢复、艺术风格迁移和医学图像合成等领域；与基于扩散的生成模型相比，这种轻量级对抗性方法在训练效率和输出质量方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;Fd-CycleGAN: 一种增强潜在表示学习的图像到图像翻译框架&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents Fd-CycleGAN, an image-to-image (I2I) translationframework that enhances latent representation learning to approximate real datadistributions. Building upon the foundation of CycleGAN, our approachintegrates Local Neighborhood Encoding (LNE) and frequency-aware supervision tocapture fine-grained local pixel semantics while preserving structuralcoherence from the source domain. We employ distribution-based loss metrics,including KL/JS divergence and log-based similarity measures, to explicitlyquantify the alignment between real and generated image distributions in bothspatial and frequency domains. To validate the efficacy of Fd-CycleGAN, weconduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and asynthetically augmented Strike-off dataset. Compared to baseline CycleGAN andother state-of-the-art methods, our approach demonstrates superior perceptualquality, faster convergence, and improved mode diversity, particularly inlow-data regimes. By effectively capturing local and global distributioncharacteristics, Fd-CycleGAN achieves more visually coherent and semanticallyconsistent translations. Our results suggest that frequency-guided latentlearning significantly improves generalization in image translation tasks, withpromising applications in document restoration, artistic style transfer, andmedical image synthesis. We also provide comparative insights withdiffusion-based generative models, highlighting the advantages of ourlightweight adversarial approach in terms of training efficiency andqualitative output.</description>
      <author>example@mail.com (Shivangi Nigam, Adarsh Prasad Behera, Shekhar Verma, P. Nagabhushan)</author>
      <guid isPermaLink="false">2508.03415v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement</title>
      <link>http://arxiv.org/abs/2508.03338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出CIVQLLIE框架，通过离散表示学习和因果推理解决夜间低光图像增强问题，克服了现有方法在极暗条件下表现不佳和依赖简化假设的局限性。&lt;h4&gt;背景&lt;/h4&gt;夜间场景拍摄的图像存在严重可见度降低问题，阻碍有效内容感知。当前低光图像增强方法面临挑战：数据驱动的端到端映射网络缺乏可解释性或依赖不可靠先验指导，在极暗条件下表现不佳；而基于物理的方法依赖简化假设，往往在复杂现实场景中失效。&lt;h4&gt;目的&lt;/h4&gt;解决现有低光图像增强方法的局限性，提出一种利用离散表示学习和因果推理的新型框架CIVQLLIE。&lt;h4&gt;方法&lt;/h4&gt;通过向量量化将连续图像特征映射到从大规模高质量图像学习到的离散视觉标记码本，该码本作为可靠先验编码标准化亮度和颜色模式。针对分布偏移问题，提出多级因果干预：1)像素级因果干预模块对齐低级特征与码本期望的分布；2)特征感知因果干预机制结合低频选择性注意力门控识别和增强受光照退化影响最大的通道；3)高频细节重建模块利用匹配码本中的结构信息重建精细细节。&lt;h4&gt;主要发现&lt;/h4&gt;直接将向量量化应用于低光图像会因退化输入与学习码本间的分布偏移而失败，所提出的多级因果干预方法能有效纠正这些偏移。&lt;h4&gt;结论&lt;/h4&gt;CIVQLLIE框架通过离散表示学习和因果推理有效解决了低光图像增强问题，能够在复杂现实场景中提升图像质量，增强内容感知能力。&lt;h4&gt;翻译&lt;/h4&gt;夜间场景拍摄的图像存在严重可见度降低问题，阻碍有效内容感知。当前低光图像增强方法面临重大挑战：数据驱动的端到端映射网络缺乏可解释性或依赖不可靠的先验指导，在极暗条件下表现不佳，而基于物理的方法依赖简化的假设，往往在复杂现实场景中失效。为解决这些局限性，我们提出CIVQLLIE，一种利用离散表示学习和因果推理力量的新框架。我们通过向量量化实现这一点，它将连续图像特征映射到从大规模高质量图像学习到的离散视觉标记码本。该码本作为可靠先验，编码独立于退化的标准化亮度和颜色模式。然而，由于退化输入与学习码本之间的分布偏移，直接将VQ应用于低光图像会失败。因此，我们提出多级因果干预方法来系统纠正这些偏移。首先，在编码过程中，我们的像素级因果干预模块干预以使低级特征与码本期望的亮度和颜色分布对齐。其次，具有低频选择性注意力门控的特征感知因果干预机制识别和增强受光照退化影响最大的通道，促进准确的码本标记匹配，同时通过灵活的特征级干预增强编码器的泛化性能。最后，在解码过程中，高频细节重建模块利用匹配码本表示中保留的结构信息，使用可变形卷积技术重建精细细节。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Images captured in nighttime scenes suffer from severely reduced visibility,hindering effective content perception. Current low-light image enhancement(LLIE) methods face significant challenges: data-driven end-to-end mappingnetworks lack interpretability or rely on unreliable prior guidance, strugglingunder extremely dark conditions, while physics-based methods depend onsimplified assumptions that often fail in complex real-world scenarios. Toaddress these limitations, we propose CIVQLLIE, a novel framework thatleverages the power of discrete representation learning through causalreasoning. We achieve this through Vector Quantization (VQ), which mapscontinuous image features to a discrete codebook of visual tokens learned fromlarge-scale high-quality images. This codebook serves as a reliable prior,encoding standardized brightness and color patterns that are independent ofdegradation. However, direct application of VQ to low-light images fails due todistribution shifts between degraded inputs and the learned codebook.Therefore, we propose a multi-level causal intervention approach tosystematically correct these shifts. First, during encoding, our Pixel-levelCausal Intervention (PCI) module intervenes to align low-level features withthe brightness and color distributions expected by the codebook. Second, aFeature-aware Causal Intervention (FCI) mechanism with Low-frequency SelectiveAttention Gating (LSAG) identifies and enhances channels most affected byillumination degradation, facilitating accurate codebook token matching whileenhancing the encoder's generalization performance through flexiblefeature-level intervention. Finally, during decoding, the High-frequency DetailReconstruction Module (HDRM) leverages structural information preserved in thematched codebook representations to reconstruct fine details using deformableconvolution techniques.</description>
      <author>example@mail.com (Tongshun Zhang, Pingping Liu, Zhe Zhang, Qiuzhan Zhou)</author>
      <guid isPermaLink="false">2508.03338v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices</title>
      <link>http://arxiv.org/abs/2508.03313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了BaroPoser，一种结合智能手机和智能手表记录的IMU和气压数据来实时估计人体姿态和全局位移的方法。通过利用气压数据估计传感器高度变化，该方法提高了人体姿态估计精度并支持非平坦地形上的运动追踪，同时提出的局部大腿坐标系有助于更好地学习姿态表示。实验证明该方法在相同硬件配置下优于仅使用IMU的最先进方法。&lt;h4&gt;背景&lt;/h4&gt;近年来，使用智能手机和智能手表等日常设备的IMU来追踪人体运动变得越来越流行。然而，由于传感器测量的稀疏性以及缺乏在不平地形上捕捉人体运动的数据集，现有方法在姿态估计准确性方面常常遇到困难，并且通常仅限于恢复平坦地形上的运动。&lt;h4&gt;目的&lt;/h4&gt;提出BaroPoser，第一个结合智能手机和智能手表记录的IMU和气压数据来实时估计人体姿态和全局位移的方法。&lt;h4&gt;方法&lt;/h4&gt;利用气压读数估计传感器高度变化，为提高人体姿态估计精度和预测非平坦地形上的全局位移提供线索；提出一个局部大腿坐标系，用于分离局部和全局运动输入，以便更好地进行姿态表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准数据集和真实世界记录上的评估表明，该方法在相同硬件配置下优于仅使用IMU的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;BaroPoser方法通过结合IMU和气压数据，解决了现有方法在不平地形上姿态估计精度不足的问题，扩展了IMU在复杂地形下人体运动追踪的应用能力。&lt;h4&gt;翻译&lt;/h4&gt;近年来，使用智能手机和智能手表等日常设备的IMU来追踪人体运动越来越受欢迎。然而，由于传感器测量的稀疏性以及缺乏在不平地形上捕捉人体运动的数据集，现有方法在姿态估计准确性方面常常遇到困难，并且通常仅限于恢复平坦地形上的运动。为此，我们提出了BaroPoser，这是第一个结合智能手机和智能手表记录的IMU和气压数据来实时估计人体姿态和全局位移的方法。通过利用气压读数，我们估计传感器高度变化，这为提高人体姿态估计精度和预测非平坦地形上的全局位移提供了有价值的线索。此外，我们提出了一个局部大腿坐标系，用于分离局部和全局运动输入，以便更好地进行姿态表示学习。我们在公共基准数据集和真实世界记录上评估了我们的方法。定量和定性结果表明，我们的方法优于使用相同硬件配置的仅使用IMU的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746059.3747731&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, tracking human motion using IMUs from everyday devices suchas smartphones and smartwatches has gained increasing popularity. However, dueto the sparsity of sensor measurements and the lack of datasets capturing humanmotion over uneven terrain, existing methods often struggle with poseestimation accuracy and are typically limited to recovering movements on flatterrain only. To this end, we present BaroPoser, the first method that combinesIMU and barometric data recorded by a smartphone and a smartwatch to estimatehuman pose and global translation in real time. By leveraging barometricreadings, we estimate sensor height changes, which provide valuable cues forboth improving the accuracy of human pose estimation and predicting globaltranslation on non-flat terrain. Furthermore, we propose a local thighcoordinate frame to disentangle local and global motion input for better poserepresentation learning. We evaluate our method on both public benchmarkdatasets and real-world recordings. Quantitative and qualitative resultsdemonstrate that our approach outperforms the state-of-the-art (SOTA) methodsthat use IMUs only with the same hardware configuration.</description>
      <author>example@mail.com (Libo Zhang, Xinyu Yi, Feng Xu)</author>
      <guid isPermaLink="false">2508.03313v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Dual-disentangle Framework for Diversified Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2508.03172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种模型无关的双重解耦框架(DDSRec)用于多样化顺序推荐，通过解耦用户兴趣和意图建模，平衡准确性和多样性。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐预测用户随时间的偏好并取得了显著成功，但随着用户交互序列长度增加以及用户兴趣和意图的复杂交织，多样性面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种模型无关的双重解耦框架用于多样化顺序推荐(DDSRec)，以解决顺序推荐中的多样性挑战。&lt;h4&gt;方法&lt;/h4&gt;该框架通过在交互建模和表示学习中采用解耦视角来改进用户兴趣和意图建模，从而平衡顺序推荐中的准确性和多样性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集上的大量实验证明了DDSRec在顺序推荐的准确性和多样性方面的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DDSRec框架能够有效解决顺序推荐中面临的多样性挑战，提高推荐的准确性和多样性。&lt;h4&gt;翻译&lt;/h4&gt;顺序推荐预测用户随时间的偏好并取得了显著成功。然而，用户交互序列的不断增长以及不断演变的用户兴趣和意图的复杂交织给多样性带来了重大挑战。为解决这些问题，我们提出了一种模型无关的双重解耦框架用于多样化顺序推荐(DDSRec)。该框架通过在交互建模和表示学习中采用解耦视角来完善用户兴趣和意图建模，从而平衡顺序推荐中的准确性和多样性。在多个公共数据集上的大量实验证明了DDSRec在顺序推荐的准确性和多样性方面的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation predicts user preferences over time and hasachieved remarkable success. However, the growing length of user interactionsequences and the complex entanglement of evolving user interests andintentions introduce significant challenges to diversity. To address these, wepropose a model-agnostic Dual-disentangle framework for Diversified SequentialRecommendation (DDSRec). The framework refines user interest and intentionmodeling by adopting disentangling perspectives in interaction modeling andrepresentation learning, thereby balancing accuracy and diversity in sequentialrecommendations. Extensive experiments on multiple public datasets demonstratethe effectiveness and superiority of DDSRec in terms of accuracy and diversityfor sequential recommendations.</description>
      <author>example@mail.com (Haoran Zhang, Jingtong Liu, Jiangzhou Deng, Junpeng Guo)</author>
      <guid isPermaLink="false">2508.03172v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Pseudo-label Induced Subspace Representation Learning for Robust Out-of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2508.03108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于伪标签诱导子空间表示的OOD检测框架，通过结合交叉熵分类损失和子空间距离正则化损失来增强ID-OOD可分离性，实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;OOD检测是稳健人工智能的核心，旨在识别训练集之外的新分布样本。&lt;h4&gt;目的&lt;/h4&gt;开发一种在更宽松和自然假设下工作的OOD检测方法，解决现有方法对特征空间限制性假设的依赖问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于伪标签诱导的子空间表示的OOD检测框架，并引入结合交叉熵ID分类损失和子空间距离正则化损失的学习准则。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在更宽松和自然的假设下工作，能够有效增强ID和OOD样本之间的可分离性。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了所提出的OOD检测框架的有效性，表明该方法在识别训练集外新分布样本方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;分布外(OOD)检测是稳健人工智能(AI)的核心，旨在识别来自训练集之外的新分布的样本。最近的方法利用特征表示作为OOD检测的区分性特征。然而，大多数现有方法依赖于特征空间的限制性假设，这限制了训练集内(ID)和分布外(OOD)样本之间的可分离性。在这项工作中，我们提出了一种基于伪标签诱导的子空间表示的新型OOD检测框架，与现有的基于特征的技术相比，它在更宽松和自然的假设下工作。此外，我们引入了一个简单而有效的学习准则，将基于交叉熵的ID分类损失与基于子空间距离的正则化损失相结合，以增强ID-OOD的可分离性。大量的实验验证了我们框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution (OOD) detection lies at the heart of robust artificialintelligence (AI), aiming to identify samples from novel distributions beyondthe training set. Recent approaches have exploited feature representations asdistinguishing signatures for OOD detection. However, most existing methodsrely on restrictive assumptions on the feature space that limit theseparability between in-distribution (ID) and OOD samples. In this work, wepropose a novel OOD detection framework based on a pseudo-label-inducedsubspace representation, that works under more relaxed and natural assumptionscompared to existing feature-based techniques. In addition, we introduce asimple yet effective learning criterion that integrates a cross-entropy-basedID classification loss with a subspace distance-based regularization loss toenhance ID-OOD separability. Extensive experiments validate the effectivenessof our framework.</description>
      <author>example@mail.com (Tarhib Al Azad, Faizul Rakib Sayem, Shahana Ibrahim)</author>
      <guid isPermaLink="false">2508.03108v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Elucidating the Role of Feature Normalization in IJEPA</title>
      <link>http://arxiv.org/abs/2508.02829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了在图像联合嵌入预测架构(IJEPA)中，层归一化(LN)对特征处理的影响，并提出用DynTanh激活替代LN，以保留视觉标记的自然能量层次结构，从而提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;在标准的IJEPA架构中，教师编码器输出的特征在作为学生编码器和预测器的蒸馏目标之前会进行层归一化处理。这种归一化被认为可能破坏视觉标记的自然能量层次结构。&lt;h4&gt;目的&lt;/h4&gt;研究的目的是探索特征归一化对IJEPA模型性能的影响，并提出一种改进方法，以更好地保留视觉标记的自然能量层次结构，提高模型的自监督视觉表征学习能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出用DynTanh激活替换特征层归一化(LN)，以更好地保留标记能量并允许高能量标记对预测损失做出更大贡献。他们比较了使用LN和DynTanh激活的IJEPA模型在ImageNet线性探针准确率和NYU Depth V2单目深度估计任务上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;特征LN破坏了视觉标记的自然能量层次结构，阻止模型优先考虑语义丰富的区域；使用特征LN训练的IJEPA模型在损失图中表现出显著的棋盘状伪影；使用DynTanh激活替代LN可以修复损失图中的棋盘状伪影并使IJEPA表现出更长尾的损失分布；使用DynTanh激活将ViT-Small在ImageNet线性探针准确率从38%提高到42.7%；在NYU Depth V2单目深度估计任务中，RMSE降低了0.08。&lt;h4&gt;结论&lt;/h4&gt;保留自然标记能量对于有效的自监督视觉表征学习至关重要。用DynTanh激活替代特征层归一化是一种简单而有效的改进方法，可以显著提高IJEPA模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;在标准的图像联合嵌入预测架构(IJEPA)中，教师编码器输出的特征在作为学生编码器和预测器的蒸馏目标之前会被层归一化(LN)处理。我们提出这种特征归一化会破坏视觉标记的自然能量层次结构，其中高能量标记(具有较大L2范数的标记)编码了语义重要的图像区域。LN强制所有特征具有相同的L2范数，有效地平衡了它们的能量，并阻止了模型优先考虑语义丰富的区域。我们发现，使用特征LN训练的IJEPA模型在损失图中表现出显著的棋盘状伪影。我们提出用DynTanh激活替换特征LN，因为后者更好地保留了标记能量，并允许高能量标记对预测损失做出更大贡献。研究表明，使用特征DynTanh训练的IJEPA表现出更长尾的损失分布，并修复了损失图中的棋盘状伪影。我们的实证结果显示，这种简单的修改将ViT-Small在ImageNet线性探针准确率从38%提高到42.7%，并在NYU Depth V2单目深度估计中将RMSE降低了0.08。这些结果表明，保留自然标记能量对于有效的自监督视觉表征学习至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the standard image joint embedding predictive architecture (IJEPA),features at the output of the teacher encoder are layer normalized (LN) beforeserving as a distillation target for the student encoder and predictor. Wepropose that this feature normalization disrupts the natural energy hierarchyof visual tokens, where high-energy tokens (those with larger L2 norms) encodesemantically important image regions. LN forces all features to have identicalL2 norms, effectively equalizing their energies and preventing the model fromprioritizing semantically rich regions. We find that IJEPA models trained withfeature LN exhibit loss maps with significant checkerboard-like artifacts. Wepropose that feature LN be replaced with a DynTanh activation as the latterbetter preserves token energies and allows high-energy tokens to greatercontribute to the prediction loss. We show that IJEPA trained with featureDynTanh exhibits a longer-tailed loss distribution and fixes the checkerboardartifacts in the loss map. Our empirical results show that our simplemodification improves ImageNet linear probe accuracy from 38% to 42.7% forViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.These results suggest that preserving natural token energies is crucial foreffective self-supervised visual representation learning.</description>
      <author>example@mail.com (Adam Colton)</author>
      <guid isPermaLink="false">2508.02829v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy</title>
      <link>http://arxiv.org/abs/2508.03596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 (Highlight); Project Page:  https://cuhk-aim-group.github.io/MetaScope/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出MetaScope，一种专为金属透镜内窥镜设计的光学驱动神经网络，通过光学信息强度调整和色差校正技术，解决了金属透镜内窥镜领域的挑战，在分割、恢复和泛化能力上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;微型内窥镜技术已发展，但传统相机使用的凸透镜受限于毫米级厚度，阻碍了微型临床应用。基于金属透镜（微米级）的超微型成像成为有前景的解决方案，但由于金属透镜的物理特性差异，数据获取和算法研究存在巨大差距。&lt;h4&gt;目的&lt;/h4&gt;弥合金属透镜内窥镜领域未被探索的差距，推动新型金属透镜内窥镜的发展。&lt;h4&gt;方法&lt;/h4&gt;建立金属透镜内窥镜数据集并进行初步光学模拟；提出MetaScope网络，包含光学信息强度调整(OIA)和光学信息色差校正(OCC)两个创新设计；采用梯度引导的蒸馏方法增强联合学习，自适应转移知识。&lt;h4&gt;主要发现&lt;/h4&gt;MetaScope在金属透镜分割和恢复方面优于最先进的方法，在真实生物医学场景中实现了令人印象深刻的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MetaScope代表了金属透镜内窥镜领域的重要进展，解决了该领域的关键挑战，展示了在临床应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;微型内窥镜的进步已在人体内实现了准确的视觉感知。现有研究仍局限于使用凸透镜的传统相机，其毫米级厚度的物理限制对微型临床应用构成严重阻碍。最近，随着超表面光学技术的出现，基于金属透镜（微米级）的超微型成像受到广泛关注，成为有前景的解决方案。然而，由于金属透镜的物理差异，在数据获取和算法研究方面存在巨大差距。鉴于此，我们旨在弥合这一未被探索的差距，推动新型金属透镜内窥镜的发展。首先，我们建立金属透镜内窥镜数据集并进行初步光学模拟，确定了两个符合强光学先验的衍生光学问题。其次，我们提出MetaScope，一种专为金属透镜内窥镜设计的新型光学驱动神经网络。MetaScope包含两个新颖设计：光学信息强度调整(OIA)，通过学习光学嵌入来校正强度衰减；光学信息色差校正(OCC)，通过学习由学习到的点扩散函数(PSF)分布提供信息的空间变形来减轻色差。为了增强联合学习，我们进一步部署了梯度引导的蒸馏方法，自适应地将知识从基础模型转移过来。大量实验表明，MetaScope不仅在金属透镜分割和恢复方面优于最先进的方法，而且在真实的生物医学场景中实现了令人印象深刻的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Miniaturized endoscopy has advanced accurate visual perception within thehuman body. Prevailing research remains limited to conventional camerasemploying convex lenses, where the physical constraints with millimetre-scalethickness impose serious impediments on the micro-level clinical. Recently,with the emergence of meta-optics, ultra-micro imaging based on metalenses(micron-scale) has garnered great attention, serving as a promising solution.However, due to the physical difference of metalens, there is a large gap indata acquisition and algorithm research. In light of this, we aim to bridgethis unexplored gap, advancing the novel metalens endoscopy. First, weestablish datasets for metalens endoscopy and conduct preliminary opticalsimulation, identifying two derived optical issues that physically adhere tostrong optical priors. Second, we propose MetaScope, a novel optics-drivenneural network tailored for metalens endoscopy driven by physical optics.MetaScope comprises two novel designs: Optics-informed Intensity Adjustment(OIA), rectifying intensity decay by learning optical embeddings, andOptics-informed Chromatic Correction (OCC), mitigating chromatic aberration bylearning spatial deformations informed by learned Point Spread Function (PSF)distributions. To enhance joint learning, we further deploy a gradient-guideddistillation to transfer knowledge from the foundational model adaptively.Extensive experiments demonstrate that MetaScope not only outperformsstate-of-the-art methods in both metalens segmentation and restoration but alsoachieves impressive generalized ability in real biomedical scenes.</description>
      <author>example@mail.com (Wuyang Li, Wentao Pan, Xiaoyuan Liu, Zhendong Luo, Chenxin Li, Hengyu Liu, Din Ping Tsai, Mu Ku Chen, Yixuan Yuan)</author>
      <guid isPermaLink="false">2508.03596v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks</title>
      <link>http://arxiv.org/abs/2508.03566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAM2-UNeXT的先进框架，通过整合辅助DINOv2编码器扩展SAM2的表示能力，采用双分辨率策略和密集粘合层实现更准确的分割，同时简化架构设计，无需复杂解码器。在四个基准测试中展示了优越性能。&lt;h4&gt;背景&lt;/h4&gt;最近的研究已强调将SegmentAnything Model (SAM) 适应各种下游任务的潜力，但构建更强大和可推广的编码器以进一步增强性能仍是一个开放挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个先进框架SAM2-UNeXT，基于SAM2-UNet核心原则，通过整合辅助DINOv2编码器扩展SAM2的表示能力，实现更准确的分割，同时简化架构设计。&lt;h4&gt;方法&lt;/h4&gt;提出SAM2-UNeXT框架，整合辅助DINOv2编码器到SAM2中，采用双分辨率策略和密集粘合层，实现更准确的分割，简化架构设计，无需复杂解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在二值图像分割、伪装物体检测、海洋动物分割和遥感显著性检测四个基准测试上进行的广泛实验，证明了所提出方法的优越性能。&lt;h4&gt;结论&lt;/h4&gt;SAM2-UNeXT框架通过整合辅助DINOv2编码器及采用双分辨率策略和密集粘合层，实现了更准确的分割性能，同时简化了架构设计。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经强调了将SegmentAnything Model (SAM) 适应各种下游任务的潜力。然而，构建一个更强大和可推广的编码器以进一步增强性能仍然是一个开放的挑战。在这项工作中，我们提出了SAM2-UNeXT，一个基于SAM2-UNet核心原则构建的先进框架，通过整合辅助DINOv2编码器扩展了SAM2的表示能力。通过采用双分辨率策略和密集粘合层，我们的方法实现了更准确的分割，同时采用简单架构，减少了对复杂解码器设计的需求。在二值图像分割、伪装物体检测、海洋动物分割和遥感显著性检测四个基准测试上进行的广泛实验，证明了我们提出方法的优越性能。代码可在https://github.com/WZH0120/SAM2-UNeXT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have highlighted the potential of adapting the SegmentAnything Model (SAM) for various downstream tasks. However, constructing a morepowerful and generalizable encoder to further enhance performance remains anopen challenge. In this work, we propose SAM2-UNeXT, an advanced framework thatbuilds upon the core principles of SAM2-UNet while extending therepresentational capacity of SAM2 through the integration of an auxiliaryDINOv2 encoder. By incorporating a dual-resolution strategy and a dense gluelayer, our approach enables more accurate segmentation with a simplearchitecture, relaxing the need for complex decoder designs. Extensiveexperiments conducted on four benchmarks, including dichotomous imagesegmentation, camouflaged object detection, marine animal segmentation, andremote sensing saliency detection, demonstrate the superior performance of ourproposed method. The code is available athttps://github.com/WZH0120/SAM2-UNeXT.</description>
      <author>example@mail.com (Xinyu Xiong, Zihuang Wu, Lei Zhang, Lei Lu, Ming Li, Guanbin Li)</author>
      <guid isPermaLink="false">2508.03566v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models</title>
      <link>http://arxiv.org/abs/2508.03533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了EmbedGrad框架，通过基于梯度的微调优化文本提示嵌入，解决了现有提示工程和参数适应方法的局限性，实现了任务适应的新范式。&lt;h4&gt;背景&lt;/h4&gt;有效地将强大的预训练基础模型适应到各种任务中仍然是AI部署中的一个关键挑战。当前方法主要有两种：离散优化文本提示或通过额外参数连续适应，但前者缺乏精确性，后者增加复杂性和降低可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决现有提示工程和参数适应方法的局限性，实现更精确、高效且可解释的任务适应。&lt;h4&gt;方法&lt;/h4&gt;EmbedGrad是一种通过基于梯度的微调来优化文本提示嵌入的新框架。该方法将训练与部署解耦：优化过程中，标记示例引导精确的嵌入调整；推理过程中，只有优化后的嵌入与用户查询集成，实现了文本空间中不可能实现的精细校准。&lt;h4&gt;主要发现&lt;/h4&gt;在数学推理、情感分析和因果判断任务上的全面评估显示了EmbedGrad的有效性。优化推理提示使Qwen2.5-Math-1.5B在数学问题上的准确率从14.74%提高到58.96%。在模型规模(0.5B-14B)和所有任务上都观察到一致的改进，小型模型在复杂问题上获得了特别显著的提升。&lt;h4&gt;结论&lt;/h4&gt;通过桥接提示工程和参数效率，无需架构更改，EmbedGrad建立了嵌入微调作为任务适应的一个强大新范式。&lt;h4&gt;翻译&lt;/h4&gt;有效地将强大的预训练基础模型适应到各种任务仍然是AI部署中的一个关键挑战。当前方法主要遵循两种范式：通过提示工程进行文本提示的离散优化，或通过额外的可训练参数进行连续适应。两种方法都存在局限性——离散方法缺乏精细的精确度，而基于参数的技术增加了复杂性和降低了可解释性。为了解决这些限制，我们提出了EmbedGrad，一种通过基于梯度的微调来优化文本提示嵌入的新框架。我们的方法独特地将训练与部署解耦：在优化过程中，标记示例引导精确的嵌入调整同时保持语义意义；在推理过程中，只有优化后的嵌入与用户查询集成。这实现了在文本空间中不可能实现的精细校准，例如增强"请逐步推理"等提示的推理能力。在数学推理、情感分析和因果判断任务上的全面评估证明了EmbedGrad的有效性：为Qwen2.5-Math-1.5B优化这个推理提示使数学问题的准确率从14.74%提高到58.96%。在模型规模(0.5B-14B)和所有任务上都观察到一致的改进，小型模型在因果判断等复杂问题上获得了特别显著的提升。通过桥接提示工程和参数效率而无需架构更改，我们的工作建立了嵌入微调作为任务适应的一个强大新范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effectively adapting powerful pretrained foundation models to diverse tasksremains a key challenge in AI deployment. Current approaches primarily followtwo paradigms:discrete optimization of text prompts through prompt engineering,or continuous adaptation via additional trainable parameters. Both exhibitlimitations-discrete methods lack refinement precision while parameter-basedtechniques increase complexity and reduce interpretability. To address theseconstraints, we propose EmbedGrad, a novel framework that optimizes text promptembeddings through gradient-based refinement. Our approach uniquely decouplestraining from deployment:during optimization,labeled examples guide preciseembedding adjustments while preserving semantic meaning; during inference, onlyoptimized embeddings integrate with user queries. This enables fine-grainedcalibration impossible in text space, such as enhancing the reasoningcapability of prompts like please reason step by step. Comprehensiveevaluations across mathematical reasoning, sentiment analysis, and causaljudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoningprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% onmathematical problems. Consistent improvements were observed across modelscales (0.5B-14B) and all tasks, with particularly significant gains forsmaller models on complex problems like causal judgment. By bridging promptengineering and parameter efficiency without architectural changes, our workestablishes embedding refinement as a powerful new paradigm for taskadaptation.</description>
      <author>example@mail.com (Xiaoming Hou, Jiquan Zhang, Zibin Lin, DaCheng Tao, Shengli Zhang)</author>
      <guid isPermaLink="false">2508.03533v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models</title>
      <link>http://arxiv.org/abs/2508.03524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SemanticStitcher的新型组织病理学图像拼接方法，使用视觉病理学基础模型的潜在特征表示来识别不同组织片段中的相邻区域，并通过鲁棒姿态估计将多个片段拼接成全载玻片图像(WMS)。&lt;h4&gt;背景&lt;/h4&gt;在组织病理学中，组织样本通常大于标准显微镜载玻片，需要拼接多个片段来处理整个结构如肿瘤。自动化拼接是扩展分析的前提，但面临多种挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服传统方法限制的新型组织病理学图像拼接技术，用于重建人工全载玻片图像(WMS)。&lt;h4&gt;方法&lt;/h4&gt;SemanticStitcher利用视觉病理学基础模型推导的潜在特征表示来识别不同组织片段中的相邻区域，基于大量语义匹配候选者进行鲁棒姿态估计，将多个片段拼接成马赛克形成完整的WMS。&lt;h4&gt;主要发现&lt;/h4&gt;在三个不同的组织病理学数据集上的实验表明，SemanticStitcher能够产生鲁棒的WMS拼接，并在正确的边界匹配方面始终优于现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;SemanticStitcher有效解决了传统组织病理学图像拼接中的挑战，能够克服组织丢失、形态畸变、染色不一致等问题，为组织病理学分析提供了可靠的图像拼接解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在组织病理学中，组织样本通常比标准显微镜载玻片大，因此需要拼接多个片段来处理整个结构如肿瘤。自动化拼接是扩展分析的前提，但由于制备过程中可能发生组织丢失、形态学畸变不均匀、染色不一致、载玻片上错位导致的缺失区域或组织边缘破碎等问题，这一过程具有挑战性。这限制了使用边界形状匹配算法的最先进拼接方法重建人工全载玻片图像(WMS)的能力。在此，我们引入了SemanticStitcher，它使用视觉病理学基础模型推导的潜在特征表示来识别不同片段中的相邻区域。基于大量语义匹配候选者的鲁棒姿态估计，将多个片段拼接成马赛克形成WMS。在三个不同组织病理学数据集上的实验表明，SemanticStitcher能够产生鲁棒的WMS拼接，并在正确的边界匹配方面始终优于现有最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In histopathology, tissue samples are often larger than a standard microscopeslide, making stitching of multiple fragments necessary to process entirestructures such as tumors. Automated stitching is a prerequisite for scalinganalysis, but is challenging due to possible tissue loss during preparation,inhomogeneous morphological distortion, staining inconsistencies, missingregions due to misalignment on the slide, or frayed tissue edges. This limitsstate-of-the-art stitching methods using boundary shape matching algorithms toreconstruct artificial whole mount slides (WMS). Here, we introduceSemanticStitcher using latent feature representations derived from a visualhistopathology foundation model to identify neighboring areas in differentfragments. Robust pose estimation based on a large number of semantic matchingcandidates derives a mosaic of multiple fragments to form the WMS. Experimentson three different histopathology datasets demonstrate that SemanticStitcheryields robust WMS mosaicing and consistently outperforms the state of the artin correct boundary matches.</description>
      <author>example@mail.com (Stefan Brandstätter, Maximilian Köller, Philipp Seeböck, Alissa Blessing, Felicitas Oberndorfer, Svitlana Pochepnia, Helmut Prosch, Georg Langs)</author>
      <guid isPermaLink="false">2508.03524v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.03511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAUP（多中心自适应不确定性感知提示）的无训练跨领域少样本医学图像分割方法，通过将自然图像预训练的Segment Anything Model适应到医学图像分割任务中，实现了在三个医学数据集上的精确分割结果，无需额外训练。&lt;h4&gt;背景&lt;/h4&gt;当前跨领域少样本医学图像分割模型在其他医学源域上的重度训练依赖限制了模型的通用性和部署便利性。随着大型视觉模型的发展，特别是自然图像领域的Segment Anything Model的出现，为解决这个问题提供了新思路。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需训练的CD-FSMIS模型，利用MAUP策略将自然图像预训练的SAM模型适应到跨领域少样本医学图像分割任务中，避免对其他源医学域的重度训练，提高模型的通用性和部署便利性。&lt;h4&gt;方法&lt;/h4&gt;MAUP策略包含三个关键创新：(1)基于K-means聚类的多中心提示生成，实现全面的空间覆盖；(2)不确定性感知的提示选择，专注于困难区域；(3)自适应提示优化，能根据目标区域复杂度动态调整。&lt;h4&gt;主要发现&lt;/h4&gt;使用预训练的DINOv2特征编码器，MAUP在三个医学数据集上实现了精确的分割结果，与传统的CD-FSMIS模型和无训练的FSMIS模型相比，无需任何额外训练就取得了良好效果。&lt;h4&gt;结论&lt;/h4&gt;MAUP策略成功解决了传统CD-FSMIS模型需要重度训练其他源医学域的问题，通过将自然图像预训练的SAM模型适应到医学图像分割任务中，实现了高效准确的跨领域少样本医学图像分割，提高了模型的通用性和部署便利性。&lt;h4&gt;翻译&lt;/h4&gt;跨领域少样本医学图像分割(CD-FSMIS)是利用其他领域的知识来分割标注有限的医学图像的一种潜在解决方案。当前CD-FSMIS模型的显著性能依赖于在其他医学源域上的重度训练过程，这降低了模型的通用性和部署便利性。随着大型自然图像视觉模型的发展，我们提出了一种无需训练的CD-FSMIS模型，引入了多中心自适应不确定性感知提示(MAUP)策略，将自然图像训练的基础模型Segment Anything Model适应到CD-FSMIS任务中。具体而言，MAUP包含三个关键创新：(1)基于K-means聚类的多中心提示生成，实现全面的空间覆盖；(2)不确定性感知的提示选择，专注于困难区域；(3)自适应提示优化，能根据目标区域复杂度动态调整。使用预训练的DINOv2特征编码器，MAUP在三个医学数据集上实现了精确的分割结果，与几个传统的CD-FSMIS模型和无训练的FSMIS模型相比，无需任何额外训练。源代码可在以下网址获取：https://github.com/YazhouZhu19/MAUP。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potentialsolution for segmenting medical images with limited annotation using knowledgefrom other domains. The significant performance of current CD-FSMIS modelsrelies on the heavily training procedure over other source medical domains,which degrades the universality and ease of model deployment. With thedevelopment of large visual models of natural images, we propose atraining-free CD-FSMIS model that introduces the Multi-center AdaptiveUncertainty-aware Prompting (MAUP) strategy for adapting the foundation modelSegment Anything Model (SAM), which is trained with natural images, into theCD-FSMIS task. To be specific, MAUP consists of three key innovations: (1)K-means clustering based multi-center prompts generation for comprehensivespatial coverage, (2) uncertainty-aware prompts selection that focuses on thechallenging regions, and (3) adaptive prompt optimization that can dynamicallyadjust according to the target region complexity. With the pre-trained DINOv2feature encoder, MAUP achieves precise segmentation results across threemedical datasets without any additional training compared with severalconventional CD-FSMIS models and training-free FSMIS model. The source code isavailable at: https://github.com/YazhouZhu19/MAUP.</description>
      <author>example@mail.com (Yazhou Zhu, Haofeng Zhang)</author>
      <guid isPermaLink="false">2508.03511v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes</title>
      <link>http://arxiv.org/abs/2508.03490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures. Accepted for presentation at EUSIPCO 2025,  September 8-12, 2025. List of accepted papers available at  http://cmsworkshops.com/EUSIPCO2025/papers/accepted_papers.php&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对建筑行业资源消耗问题，提出了一种名为ParticleSAM的视觉分割方法，用于建筑材料颗粒的质量监测，并创建了相应的数据集作为基准。&lt;h4&gt;背景&lt;/h4&gt;建筑行业是资源消耗的主要部门，回收建筑材料具有高再利用潜力，但骨料质量监测通常仍采用人工方法进行。基于视觉的机器学习方法可能提供更高效的解决方案，但现有分割方法不适用于包含数百个小颗粒的图像。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于小而密集物体图像的分割方法，特别是建筑材料颗粒图像，以实现自动化的材料质量控制。&lt;h4&gt;方法&lt;/h4&gt;提出了ParticleSAM，一种针对小而密集物体的分割基础模型适配；创建了新的密集多颗粒数据集，通过自动数据生成和标记流程从孤立的颗粒图像模拟而来；进行了定量和定性实验验证方法优势。&lt;h4&gt;主要发现&lt;/h4&gt;ParticleSAM在处理小颗粒分割任务上优于原始SAM方法；创建的数据集可作为视觉材料质量控制自动化的基准；该方法在建筑以外的领域也具有应用潜力。&lt;h4&gt;结论&lt;/h4&gt;ParticleSAM为建筑材料质量监测提供了一种更快、更高效的解决方案，通过自动化数据生成和标记流程创建的数据集为该领域提供了基准，方法具有广泛的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;建筑行业在资源消耗方面代表了一个主要部门。回收建筑材料具有高再利用潜力，但骨料的质量监测通常仍采用人工方法进行。基于视觉的机器学习方法可以为这一问题提供更快、更高效的解决方案，但现有的分割方法在设计上不适用于包含数百个小颗粒的图像。在本文中，我们提出了ParticleSAM，这是一种分割基础模型针对建筑材料颗粒等常见的小而密集物体图像的适配。此外，我们借助自动数据生成和标记流程，从孤立的颗粒图像创建了一个新的密集多颗粒数据集。该数据集作为视觉材料质量控制自动化的基准，而我们的分割方法在需要小颗粒分割的建筑以外的应用领域也具有潜在价值。我们的实验结果通过定量和定性实验与原始SAM方法进行比较，验证了我们方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The construction industry represents a major sector in terms of resourceconsumption. Recycled construction material has high reuse potential, butquality monitoring of the aggregates is typically still performed with manualmethods. Vision-based machine learning methods could offer a faster and moreefficient solution to this problem, but existing segmentation methods are bydesign not directly applicable to images with hundreds of small particles. Inthis paper, we propose ParticleSAM, an adaptation of the segmentationfoundation model to images with small and dense objects such as the ones oftenencountered in construction material particles. Moreover, we create a new densemulti-particle dataset simulated from isolated particle images with theassistance of an automated data generation and labeling pipeline. This datasetserves as a benchmark for visual material quality control automation while oursegmentation approach has the potential to be valuable in application areasbeyond construction where small-particle segmentation is needed. Ourexperimental results validate the advantages of our method by comparing to theoriginal SAM method both in quantitative and qualitative experiments.</description>
      <author>example@mail.com (Yu Zhou, Pelle Thielmann, Ayush Chamoli, Bruno Mirbach, Didier Stricker, Jason Rambach)</author>
      <guid isPermaLink="false">2508.03490v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis</title>
      <link>http://arxiv.org/abs/2508.03441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 6 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MedCAL-Bench，首个基于基础模型的医学图像分析冷启动主动学习(CSAL)基准，评估了14个基础模型和7种CSAL策略在7个数据集上的表现，覆盖分类和分割任务。&lt;h4&gt;背景&lt;/h4&gt;在医学图像分析中，冷启动主动学习(CSAL)旨在没有先验知识的情况下选择有信息量的样本进行标注，以提高标注效率和模型性能。现有方法主要依赖自监督学习进行特征提取，但这种方法效率低下且特征表示不足。&lt;h4&gt;目的&lt;/h4&gt;填补预训练基础模型在冷启动主动学习任务中的研究空白，并建立一个全面的基准来比较不同基础模型和CSAL策略在医学图像分析中的性能。&lt;h4&gt;方法&lt;/h4&gt;构建MedCAL-Bench基准，评估14个基础模型和7种CSAL策略在7个数据集上的表现，覆盖不同标注预算，包括分类和分割任务，来自多种医学模态。该基准首次同时评估特征提取和样本选择两个阶段。&lt;h4&gt;主要发现&lt;/h4&gt;1) 大多数基础模型是CSAL的有效特征提取器，DINO系列在分割任务中表现最佳；2) 这些基础模型在分割任务中的性能差异较大，而在分类任务中差异较小；3) 在不同数据集上应考虑不同的样本选择策略，ALPS在分割任务中表现最佳，而RepDiv在分类任务中领先。&lt;h4&gt;结论&lt;/h4&gt;基础模型在冷启动主动学习中具有巨大潜力，但不同任务和数据集需要选择合适的基础模型和样本选择策略。MedCAL-Bench为这一领域提供了重要的基准和参考。&lt;h4&gt;翻译&lt;/h4&gt;Cold-Start Active Learning (CSAL)旨在在没有先验知识的情况下选择有信息量的样本进行标注，这对于在有限的标注预算下提高医学图像分析中的标注效率和模型性能很重要。大多数现有的CSAL方法依赖在目标数据集上进行自监督学习(SSL)进行特征提取，这种方法效率低下且受限于特征表示不足。最近，预训练基础模型(FMs)已显示出强大的特征提取能力，并有潜力用于更好的CSAL。然而，这种范式很少被研究，缺乏在CSAL任务中比较FMs的基准。为此，我们提出了MedCAL-Bench，这是第一个基于FM的医学图像分析CSAL基准。我们在7个数据集上评估了14个FMs和7种CSAL策略，涵盖不同标注预算，包括来自多种医学模态的分类和分割任务。它也是首个同时评估特征提取和样本选择两个阶段的CSAL基准。我们的实验结果表明：1) 大多数FMs是CSAL的有效特征提取器，DINO系列在分割任务中表现最佳；2) 这些FMs在分割任务中的性能差异较大，而在分类任务中差异较小；3) 在不同数据集上的CSAL应考虑不同的样本选择策略，ALPS在分割任务中表现最佳，而RepDiv在分类任务中领先。代码可在https://github.com/HiLab-git/MedCAL-Bench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cold-Start Active Learning (CSAL) aims to select informative samples forannotation without prior knowledge, which is important for improving annotationefficiency and model performance under a limited annotation budget in medicalimage analysis. Most existing CSAL methods rely on Self-Supervised Learning(SSL) on the target dataset for feature extraction, which is inefficient andlimited by insufficient feature representation. Recently, pre-trainedFoundation Models (FMs) have shown powerful feature extraction ability with apotential for better CSAL. However, this paradigm has been rarely investigated,with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, wepropose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medicalimage analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasetsunder different annotation budgets, covering classification and segmentationtasks from diverse medical modalities. It is also the first CSAL benchmark thatevaluates both the feature extraction and sample selection stages. Ourexperimental results reveal that: 1) Most FMs are effective feature extractorsfor CSAL, with DINO family performing the best in segmentation; 2) Theperformance differences of these FMs are large in segmentation tasks, whilesmall for classification; 3) Different sample selection strategies should beconsidered in CSAL on different datasets, with Active Learning by ProcessingSurprisal (ALPS) performing the best in segmentation while RepDiv leading forclassification. The code is available athttps://github.com/HiLab-git/MedCAL-Bench.</description>
      <author>example@mail.com (Ning Zhu, Xiaochuan Ma, Shaoting Zhang, Guotai Wang)</author>
      <guid isPermaLink="false">2508.03441v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation</title>
      <link>http://arxiv.org/abs/2508.03426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大规模多模态医学知识图谱的X光医学报告生成方法，解决了现有方法中存在的幻觉和疾病诊断能力弱等问题。&lt;h4&gt;背景&lt;/h4&gt;X光医学报告生成是人工智能在医疗领域的重要应用，基于大型基础模型的支持，医学报告生成的质量已显著提高，但仍存在幻觉和疾病诊断能力弱等挑战。&lt;h4&gt;目的&lt;/h4&gt;构建一个大规模多模态医学知识图谱，并设计一个有效的X光报告生成框架，提高生成质量和疾病诊断能力。&lt;h4&gt;方法&lt;/h4&gt;1) 构建基于真实医学报告的大规模多模态医学知识图谱(M3KG)，包含2477个实体、3种关系、37424个三元组和6943个疾病感知视觉标记；2) 对知识图谱进行采样获得多粒度语义图，使用R-GCN编码器进行特征提取；3) 采用Swin-Transformer提取X光图像视觉特征，通过交叉注意力与知识交互；4) 将视觉标记输入Q-former，使用交叉注意力检索疾病感知视觉标记；5) 采用大型语言模型将语义知识图谱、输入X光图像和疾病感知视觉标记映射为语言描述。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的大量实验充分验证了所提出的知识图谱和X光报告生成框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效解决X光医学报告生成中的问题，提高生成质量和诊断能力，源代码将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;X光医学报告生成是人工智能在医疗领域的重要应用之一。在大型基础模型的支持下，医学报告生成的质量已显著提高。然而，诸如幻觉和疾病诊断能力弱等挑战仍然存在。在本文中，我们首先基于真实医学报告使用GPT-4o构建了一个大规模多模态医学知识图谱（称为M3KG）。它包含2477个实体、3种关系、37424个三元组和6943个针对CheXpert Plus数据集的疾病感知视觉标记。然后，我们对知识图谱进行采样以获得多粒度语义图，并使用R-GCN编码器进行特征提取。对于输入的X光图像，我们采用Swin-Transformer提取视觉特征，并通过交叉注意力与知识交互。视觉标记被输入Q-former，并使用另一个交叉注意力检索疾病感知视觉标记。最后，我们采用大型语言模型将语义知识图谱、输入X光图像和疾病感知视觉标记映射为语言描述。在多个数据集上的大量实验充分验证了我们提出的知识图谱和X光报告生成框架的有效性。本文的源代码将在https://github.com/Event-AHU/Medical_Image_Analysis上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray medical report generation is one of the important applications ofartificial intelligence in healthcare. With the support of large foundationmodels, the quality of medical report generation has significantly improved.However, challenges such as hallucination and weak disease diagnosticcapability still persist. In this paper, we first construct a large-scalemulti-modal medical knowledge graph (termed M3KG) based on the ground truthmedical report using the GPT-4o. It contains 2477 entities, 3 kinds ofrelations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpertPlus dataset. Then, we sample it to obtain multi-granularity semantic graphsand use an R-GCN encoder for feature extraction. For the input X-ray image, weadopt the Swin-Transformer to extract the vision features and interact with theknowledge using cross-attention. The vision tokens are fed into a Q-former andretrieved the disease-aware vision tokens using another cross-attention.Finally, we adopt the large language model to map the semantic knowledge graph,input X-ray image, and disease-aware vision tokens into language descriptions.Extensive experiments on multiple datasets fully validated the effectiveness ofour proposed knowledge graph and X-ray report generation framework. The sourcecode of this paper will be released onhttps://github.com/Event-AHU/Medical_Image_Analysis.</description>
      <author>example@mail.com (Futian Wang, Yuhan Qiao, Xiao Wang, Fuling Wang, Yuxiang Zhang, Dengdi Sun)</author>
      <guid isPermaLink="false">2508.03426v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models</title>
      <link>http://arxiv.org/abs/2508.03356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages (main document) + 12 pages (appendix), 3 figures (main) + 12  figures (appendix), 5 tables (main) + 6 tables (appendix), submitted to AAAI  2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FedPromo是一种新型框架，通过两阶段过程和知识蒸馏技术，使大规模基础模型能够在资源有限的客户端设备上有效适应新领域，同时保持隐私保护和计算效率。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(Federated Learning)是一种在分散数据上训练深度学习模型的成熟范式。然而，随着模型规模的增长，传统联邦学习方法通常需要客户端设备大量计算资源，这可能不切实际。&lt;h4&gt;目的&lt;/h4&gt;引入FedPromo框架，使存储在中央服务器上的大规模基础模型能够有效适应远程客户端遇到的新领域，同时减少计算开销并保持隐私保护。&lt;h4&gt;方法&lt;/h4&gt;FedPromo通过两阶段过程实现：首先，服务器端知识蒸馏将大规模基础模型(如transformer)的表示与紧凑模型(如CNN)的表示对齐；然后，紧凑模型编码器部署到客户端设备，在本地学习可训练分类器。这些分类器随后被聚合并无缝传输回基础模型，促进个性化适应而无需直接访问用户数据。通过新颖的正则化策略，框架实现分散式多领域学习，平衡性能、隐私和资源效率。&lt;h4&gt;主要发现&lt;/h4&gt;在五个图像分类基准上的大量实验表明，FedPromo在资源有限的客户端情况下优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;FedPromo提供了一种有效的方法，使大规模基础模型能够在联邦学习环境中适应新领域，同时减少计算开销并保持隐私保护。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(Federated Learning)是一种在分散数据上训练深度学习模型的成熟范式。然而，随着模型规模的增长，传统的联邦学习方法通常需要客户端设备大量计算资源，这可能不切实际。我们引入了FedPromo，一种新型框架，使存储在中央服务器上的大规模基础模型能够有效适应远程客户端遇到的新领域。FedPromo不是直接在客户端设备上训练大模型，而是通过联邦学习优化轻量级代理模型，显著减少计算开销同时保持隐私。我们的方法遵循两阶段过程：首先，服务器端知识蒸馏将大规模基础模型(如transformer)的表示与紧凑模型(如CNN)的表示对齐。然后，紧凑模型编码器部署到客户端设备，在本地学习可训练分类器。这些分类器随后被聚合并无缝传输回基础模型，促进个性化适应而无需直接访问用户数据。通过新颖的正则化策略，我们的框架能够实现分散式多领域学习，平衡性能、隐私和资源效率。在五个图像分类基准上的大量实验表明，FedPromo在资源有限的客户端情况下优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) is an established paradigm for training deep learningmodels on decentralized data. However, as the size of the models grows,conventional FL approaches often require significant computational resources onclient devices, which may not be feasible. We introduce FedPromo, a novelframework that enables efficient adaptation of large-scale foundation modelsstored on a central server to new domains encountered only by remote clients.Instead of directly training the large model on client devices, FedPromooptimizes lightweight proxy models via FL, significantly reducing computationaloverhead while maintaining privacy. Our method follows a two-stage process:first, server-side knowledge distillation aligns the representations of alarge-scale foundation model (e.g., a transformer) with those of a compactcounterpart (e.g., a CNN). Then, the compact model encoder is deployed toclient devices, where trainable classifiers are learned locally. Theseclassifiers are subsequently aggregated and seamlessly transferred back to thefoundation model, facilitating personalized adaptation without requiring directaccess to user data. Through novel regularization strategies, our frameworkenables decentralized multi-domain learning, balancing performance, privacy,and resource efficiency. Extensive experiments on five image classificationbenchmarks demonstrate that FedPromo outperforms existing methods whileassuming limited-resource clients.</description>
      <author>example@mail.com (Matteo Caligiuri, Francesco Barbato, Donald Shenaj, Umberto Michieli, Pietro Zanuttigh)</author>
      <guid isPermaLink="false">2508.03356v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools</title>
      <link>http://arxiv.org/abs/2508.03284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出ToolVQA数据集和ToolEngine数据生成管道，旨在提高大型基础模型在真实世界多模态工具使用场景中的问题解决能力，特别是在需要多步推理的复杂任务中。&lt;h4&gt;背景&lt;/h4&gt;现有研究在工具增强的视觉问答方面已显示出强大性能，但最近的基准测试显示，在需要多步推理的真实世界工具使用能力方面存在显著差距，特别是在功能多样的多模态环境中。&lt;h4&gt;目的&lt;/h4&gt;介绍ToolVQA，一个大规模多模态数据集，包含23K个实例，旨在弥合大型基础模型在真实世界工具使用能力方面的差距。&lt;h4&gt;方法&lt;/h4&gt;提出ToolEngine，一种新颖的数据生成管道，采用带有动态上下文示例匹配机制的深度优先搜索来模拟类人工具使用推理，构建了包含10种多模态工具跨越7个不同任务领域的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在ToolVQA上微调的70亿参数大型基础模型不仅在测试集上取得了令人印象深刻的性能，而且在各种分布外数据集上超越了大型闭源模型GPT-3.5-turbo，显示出对真实世界工具使用场景的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ToolVQA数据集和ToolEngine管道能够有效提高大型基础模型在真实世界多模态工具使用场景中的表现和泛化能力，特别是在需要多步推理的复杂任务中。&lt;h4&gt;翻译&lt;/h4&gt;将外部工具集成到大型基础模型中已成为提高其问题解决能力的一种有前景的方法。虽然现有研究已在工具增强的视觉问答方面展示了强大性能，但最近的基准测试揭示了在真实世界工具使用能力方面的显著差距，特别是在功能多样的多模态环境中需要多步推理。在这项工作中，我们介绍了ToolVQA，一个包含23K个实例的大规模多模态数据集，旨在弥合这一差距。与依赖合成场景和简化查询的先前数据集不同，ToolVQA具有真实世界的视觉上下文和具有挑战性的隐式多步推理任务，更好地与真实用户交互保持一致。为构建此数据集，我们提出了ToolEngine，一种新颖的数据生成管道，它采用带有动态上下文示例匹配机制的深度优先搜索来模拟类人工具使用推理。ToolVQA涵盖7个不同任务领域的10种多模态工具，每个实例平均推理长度为2.78步。在ToolVQA上微调的70亿参数大型基础模型不仅在测试集上取得了令人印象深刻的性能，还在各种分布外数据集上超越了大型闭源模型GPT-3.5-turbo，显示出对真实世界工具使用场景的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating external tools into Large Foundation Models (LFMs) has emerged asa promising approach to enhance their problem-solving capabilities. Whileexisting studies have demonstrated strong performance in tool-augmented VisualQuestion Answering (VQA), recent benchmarks reveal significant gaps inreal-world tool-use proficiency, particularly in functionally diversemultimodal settings requiring multi-step reasoning. In this work, we introduceToolVQA, a large-scale multimodal dataset comprising 23K instances, designed tobridge this gap. Unlike previous datasets that rely on synthetic scenarios andsimplified queries, ToolVQA features real-world visual contexts and challengingimplicit multi-step reasoning tasks, better aligning with real userinteractions. To construct this dataset, we propose ToolEngine, a novel datageneration pipeline that employs Depth-First Search (DFS) with a dynamicin-context example matching mechanism to simulate human-like tool-usereasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse taskdomains, with an average inference length of 2.78 reasoning steps per instance.The fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance onour test set but also surpass the large close-sourced model GPT-3.5-turbo onvarious out-of-distribution (OOD) datasets, demonstrating stronggeneralizability to real-world tool-use scenarios.</description>
      <author>example@mail.com (Shaofeng Yin, Ting Lei, Yang Liu)</author>
      <guid isPermaLink="false">2508.03284v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.03235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了一种基于视觉基础模型的零样本分类流程，用于扫描电子显微镜图像中纳米颗粒形态的高效准确表征，无需大量标记数据和参数微调。&lt;h4&gt;背景&lt;/h4&gt;在纳米材料合成中，准确高效地表征纳米颗粒形态对确保产品质量和加速开发至关重要，但传统深度学习方法受限于大量标记数据需求和计算密集型训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需大量标记数据和参数微调的纳米颗粒形态分类方法，使其对研究及工业环境中的纳米颗粒从业者更易访问和使用。&lt;h4&gt;方法&lt;/h4&gt;结合Segment Anything Model (SAM)用于目标分割和DINOv2用于特征嵌入，与轻量级分类器构建零样本分类流程，应用于三个形态多样化的纳米颗粒数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在纳米颗粒形状分类上优于微调的YOLOv11和ChatGPT o4-mini-high基线，对小数据集、细微形态变化和域转移具有鲁棒性，且DINOv2特征的PCA聚类指标可用于评估化学合成进展。&lt;h4&gt;结论&lt;/h4&gt;基础模型在自动化显微镜图像分析方面具有巨大潜力，为纳米颗粒研究中的传统深度学习流程提供了更高效、更用户友好的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;准确高效地表征扫描电子显微镜(SEM)图像中的纳米颗粒形态对于确保纳米材料合成产品质量和加速开发至关重要。然而，传统的用于形状分类的深度学习方法需要大量标记的数据集和计算密集型训练，限制了其在研究和工业环境中典型纳米颗粒从业者中的应用。在本研究中，我们引入了一种零样本分类流程，利用两个视觉基础模型：Segment Anything Model (SAM)用于目标分割，DINOv2用于特征嵌入。将这些模型与轻量级分类器结合，我们在三个形态多样化的纳米颗粒数据集上实现了高精度的形状分类，无需大量参数微调。我们的方法优于微调的YOLOv11和ChatGPT o4-mini-high基线，展示了对小数据集、细微形态变化和从自然图像到科学成像的域转移的鲁棒性。讨论了DINOv2特征的PCA图上的定量聚类指标，作为评估化学合成进展的一种手段。这项工作强调了基础模型在推进自动化显微镜图像分析方面的潜力，为纳米颗粒研究中的传统深度学习流程提供了一种更高效、更易于用户使用的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient characterization of nanoparticle morphology inScanning Electron Microscopy (SEM) images is critical for ensuring productquality in nanomaterial synthesis and accelerating development. However,conventional deep learning methods for shape classification require extensivelabeled datasets and computationally demanding training, limiting theiraccessibility to the typical nanoparticle practitioner in research andindustrial settings. In this study, we introduce a zero-shot classificationpipeline that leverages two vision foundation models: the Segment AnythingModel (SAM) for object segmentation and DINOv2 for feature embedding. Bycombining these models with a lightweight classifier, we achieve high-precisionshape classification across three morphologically diverse nanoparticle datasets- without the need for extensive parameter fine-tuning. Our methodologyoutperforms a fine-tuned YOLOv11 and ChatGPT o4-mini-high baselines,demonstrating robustness to small datasets, subtle morphological variations,and domain shifts from natural to scientific imaging. Quantitative clusteringmetrics on PCA plots of the DINOv2 features are discussed as a means ofassessing the progress of the chemical synthesis. This work highlights thepotential of foundation models to advance automated microscopy image analysis,offering an alternative to traditional deep learning pipelines in nanoparticleresearch which is both more efficient and more accessible to the user.</description>
      <author>example@mail.com (Freida Barnatan, Emunah Goldstein, Einav Kalimian, Orchen Madar, Avi Huri, David Zitoun, Ya'akov Mandelbaum, Moshe Amitay)</author>
      <guid isPermaLink="false">2508.03235v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2508.03118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出H3R混合框架，通过集成体积潜在融合与基于注意力的特征聚合，解决了3D重建中多视图对应建模的挑战，实现了比现有方法快2倍的收敛速度，并在多个基准测试上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;尽管前馈3D高斯喷溅技术最近有所进展，通用3D重建仍然具有挑战性，特别是在多视图对应建模方面。现有方法面临基本权衡：显式方法具有几何精度但在模糊区域表现不佳，隐式方法提供鲁棒性但收敛速度慢。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提高通用性，加快收敛速度，并解决语义表示与空间重建需求之间的不匹配问题。&lt;h4&gt;方法&lt;/h4&gt;提出H3R混合框架，集成体积潜在融合与基于注意力的特征聚合。框架包含两个互补组件：1) 高效的潜在体积，通过极线约束强制几何一致性；2) 相机感知Transformer，利用普吕克坐标进行自适应对应细化。同时探索空间对齐的基础模型（如SD-VAE）比语义对齐的模型（如DINOv2）表现更好。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法提高了通用性，收敛速度比现有方法快2倍；空间对齐的基础模型比语义对齐的模型表现更好；支持可变数量和高分辨率输入视图；展示了强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;在多个基准测试上取得了最先进的性能，在RealEstate10K、ACID和DTU数据集上分别有0.59 dB、1.06 dB和0.22 dB的显著PSNR改进。&lt;h4&gt;翻译&lt;/h4&gt;尽管前馈3D高斯喷溅技术最近有所进展，通用3D重建仍然具有挑战性，特别是在多视图对应建模方面。现有方法面临基本权衡：显式方法实现几何精度但在模糊区域表现不佳，而隐式方法提供鲁棒性但收敛缓慢。我们提出H3R，一个通过集成体积潜在融合与基于注意力的特征聚合来解决这一局限性的混合框架。我们的框架包含两个互补组件：一个通过极线约束强制几何一致性的高效潜在体积，以及一个利用普吕克坐标进行自适应对应细化的相机感知Transformer。通过集成这两种范式，我们的方法提高了通用性，同时收敛速度比现有方法快2倍。此外，我们表明空间对齐的基础模型（如SD-VAE）明显优于语义对齐的模型（如DINOv2），解决了语义表示与空间重建需求之间的不匹配问题。我们的方法支持可变数量和高分辨率输入视图，同时展示了强大的跨数据集泛化能力。大量实验表明，我们的方法在多个基准测试上取得了最先进的性能，在RealEstate10K、ACID和DTU数据集上分别有0.59 dB、1.06 dB和0.22 dB的显著PSNR改进。代码可在https://github.com/JiaHeng-DLUT/H3R获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决通用化3D重建中的多视角对应关系建模挑战。现有方法在显式方法(几何精确但处理模糊区域能力弱)和隐式方法(鲁棒性强但收敛慢)之间存在权衡。这个问题很重要，因为3D重建在虚拟现实、自动驾驶等领域有广泛应用，而传统方法需要针对每个场景进行昂贵训练，限制了实用性。通用化3D重建方法不需要场景特定优化，能快速应用于各种场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法在显式和隐式对应关系建模间的权衡问题，然后设计混合框架结合两者优势。方法借鉴了多视角立体中的平面扫描立体技术构建潜在体积，采用Transformer架构进行跨视角对应关系聚合，并使用3D高斯溅射作为场景表示。此外，作者系统评估不同视觉基础模型，发现空间对齐模型(如SD-VAE)比语义对齐模型更适合3D重建。为适应实际应用，还开发了H3R-α(处理多视图)和H3R-β(高分辨率重建)两个扩展版本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合显式和隐式多视角对应关系建模的优点，通过混合框架解决两者间的权衡。具体包括：使用显式体积潜在融合捕获几何精确对应关系，基于相机的Transformer处理模糊场景，以及利用空间对齐视觉表示增强空间保真度。整体流程分为五步：1)视觉编码从上下文视图提取特征；2)极线匹配使用几何约束对齐特征并构建潜在体积；3)相机嵌入集成参数并通过多视角注意力细化特征；4)高斯解码将特征转换为像素对齐的高斯参数；5)实时渲染实现新视角合成。多视角重建还支持可变输入视图和目标视图集成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合多视角对应关系建模，统一显式体积融合和隐式Transformer，解决方法间权衡；2)空间对齐的视觉表示，证明SD-VAE等模型比DINOv2等语义对齐模型更适合3D重建；3)全面的性能改进，解决三个核心挑战并实现最先进性能。相比之前工作，不同之处在于：结合了单一方法通常不具备的显式和隐式优势；采用更合适的视觉表示；支持可变输入视图和高分辨率重建；在多个基准测试上实现显著性能提升(PSNR提高0.22-1.06 dB)且训练速度快2倍。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; H3R通过结合显式体积融合和隐式基于相机的Transformer，并采用空间对齐的视觉表示，解决了通用化3D重建中多视角对应关系建模的权衡问题，实现了更高质量、更快收敛且具有强大泛化能力的3D重建方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable3D reconstruction remains challenging, particularly in multi-viewcorrespondence modeling. Existing approaches face a fundamental trade-off:explicit methods achieve geometric precision but struggle with ambiguousregions, while implicit methods provide robustness but suffer from slowconvergence. We present H3R, a hybrid framework that addresses this limitationby integrating volumetric latent fusion with attention-based featureaggregation. Our framework consists of two complementary components: anefficient latent volume that enforces geometric consistency through epipolarconstraints, and a camera-aware Transformer that leverages Pl\"uckercoordinates for adaptive correspondence refinement. By integrating bothparadigms, our approach enhances generalization while converging 2$\times$faster than existing methods. Furthermore, we show that spatial-alignedfoundation models (e.g., SD-VAE) substantially outperform semantic-alignedmodels (e.g., DINOv2), resolving the mismatch between semantic representationsand spatial reconstruction requirements. Our method supports variable-numberand high-resolution input views while demonstrating robust cross-datasetgeneralization. Extensive experiments show that our method achievesstate-of-the-art performance across multiple benchmarks, with significant PSNRimprovements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, andDTU datasets, respectively. Code is available athttps://github.com/JiaHeng-DLUT/H3R.</description>
      <author>example@mail.com (Heng Jia, Linchao Zhu, Na Zhao)</author>
      <guid isPermaLink="false">2508.03118v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping</title>
      <link>http://arxiv.org/abs/2508.03099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Point2Act是一种直接检索与上下文描述任务相关的3D动作点的方法，利用多模态大语言模型(MLLMs)。该方法绕过高维特征，而是高效地注入轻量级2D点级指导，专门针对特定任务的动作。多视图聚合有效补偿了几何歧义和语义不确定性带来的错位。完整流程在20秒内生成空间响应，促进实际操作任务。&lt;h4&gt;背景&lt;/h4&gt;基础模型为通用机器人提供了可能性，使其能够在未知环境中执行零样本任务，遵循自然语言描述。然而，从大规模图像和语言数据集获得的语义虽然在2D图像中提供上下文理解，但其丰富而细微的特征只能推断出模糊的2D区域，难以找到精确的3D动作位置。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接检索与上下文描述任务相关的3D动作点的方法，解决从2D语义理解到精确3D动作定位的挑战，使机器人能够在未知环境中执行实际操作任务。&lt;h4&gt;方法&lt;/h4&gt;提出3D相关性场，绕过高维特征，而是高效地注入针对特定任务动作的轻量级2D点级指导。使用多视图聚合有效补偿了几何歧义（如遮挡）或语言描述中固有的语义不确定性带来的错位。完整流程包括捕获、MLLM查询、3D重建和抓取姿态提取。&lt;h4&gt;主要发现&lt;/h4&gt;输出区域高度局部化，能够推理细粒度的3D空间上下文，可以直接转移到场景即时重建中的物理动作明确位置。完整流程在20秒内生成空间响应，促进实际操作任务。&lt;h4&gt;结论&lt;/h4&gt;Point2Act成功解决了从2D语义理解到精确3D动作定位的挑战，通过轻量级点级指导和多视图聚合，实现了快速的空间响应生成，为机器人在未知环境中执行实际操作任务提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Point2Act，它直接检索与上下文描述任务相关的3D动作点，利用多模态大语言模型(MLLMs)。基础模型为通用机器人打开了可能性，使其能够在未知环境中执行零样本任务，遵循自然语言描述。虽然从大规模图像和语言数据集获得的语义在2D图像中提供上下文理解，但其丰富而细微的特征只能推断出模糊的2D区域，难以找到精确的3D动作位置。我们提出的3D相关性场绕过高维特征，而是高效地注入针对特定任务动作的轻量级2D点级指导。多视图聚合有效补偿了几何歧义（如遮挡）或语言描述中固有的语义不确定性带来的错位。输出区域高度局部化，能够推理细粒度的3D空间上下文，可以直接转移到场景即时重建中的物理动作明确位置。我们的完整流程，包括捕获、MLLM查询、3D重建和抓取姿态提取，在20秒内生成空间响应，促进实际操作任务。项目页面：https://sangminkim-99.github.io/point2act/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人如何根据自然语言描述在3D场景中精确定位并执行抓取动作的问题，特别是在零样本和上下文感知的场景下。这个问题很重要，因为它能让机器人理解人类自然语言指令，处理新环境中的新任务，并根据任务需求选择合适的抓取位置，而不仅仅是稳定抓取，这对机器人在现实世界中的应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法（如LERF和F3RM）使用高维特征进行3D场景表示存在计算效率低和定位不精确的问题。他们提出使用MLLM直接预测2D点，然后蒸馏成3D相关性场的方法，避免了高维特征的计算负担。该方法借鉴了多视角图像3D重建（如NeRF）、MLLM语言理解（如Molmo）和现有抓取姿态生成方法（如AnyGrasp），并通过多视角聚合处理遮挡和视角变化问题，最后采用流水线处理优化整个流程减少延迟。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用MLLM直接预测多视角图像中的2D相关点，然后蒸馏成一个轻量级的3D相关性场，避免高维特征的计算负担，同时提供精确的空间定位。整体流程包括：1) 从多视角捕获场景图像；2) 将图像和指令输入MLLM获取2D相关点；3) 使用NeRF重建3D场景并学习相关性分数；4) 将多视角2D点预测蒸馏成3D相关性场；5) 将相关性场转换为点云并生成抓取姿态候选；6) 根据相关性场选择最符合指令的抓取姿态。整个流程采用流水线设计，可在约20秒内完成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 使用MLLM直接预测2D点作为中间表示，降低计算复杂度；2) 提出轻量级3D相关性场编码单通道相关性信息；3) 通过多视角聚合处理遮挡和视角变化；4) 设计高效流水线系统实现20秒内完成全流程；5) 支持零样本上下文感知抓取，处理多种语言查询。相比之前工作，Point2Act避免了高维特征的计算负担，提高了定位精度和鲁棒性，不需要大量机器人演示数据训练，能处理更复杂的组合描述和上下文差异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point2Act通过将多模态大语言模型的2D点预测蒸馏成高效的3D相关性场，实现了零样本上下文感知的机器人抓取，在保持高精度的同时显著提高了计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Point2Act, which directly retrieves the 3D action point relevantfor a contextually described task, leveraging Multimodal Large Language Models(MLLMs). Foundation models opened the possibility for generalist robots thatcan perform a zero-shot task following natural language descriptions within anunseen environment. While the semantics obtained from large-scale image andlanguage datasets provide contextual understanding in 2D images, the rich yetnuanced features deduce blurry 2D regions and struggle to find precise 3Dlocations for actions. Our proposed 3D relevancy fields bypass thehigh-dimensional features and instead efficiently imbue lightweight 2Dpoint-level guidance tailored to the task-specific action. The multi-viewaggregation effectively compensates for misalignments due to geometricambiguities, such as occlusion, or semantic uncertainties inherent in thelanguage descriptions. The output region is highly localized, reasoningfine-grained 3D spatial context that can directly transfer to an explicitposition for physical action at the on-the-fly reconstruction of the scene. Ourfull-stack pipeline, which includes capturing, MLLM querying, 3Dreconstruction, and grasp pose extraction, generates spatially groundedresponses in under 20 seconds, facilitating practical manipulation tasks.Project page: https://sangminkim-99.github.io/point2act/</description>
      <author>example@mail.com (Sang Min Kim, Hyeongjun Heo, Junho Kim, Yonghyeon Lee, Young Min Kim)</author>
      <guid isPermaLink="false">2508.03099v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.03007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为多粒度特征校准(MGFC)的新框架，用于增强领域泛化语义分割任务中的模型泛化能力，通过层次化和粒度感知的特征校准，有效将视觉基础模型的泛化能力转移到领域特定任务中。&lt;h4&gt;背景&lt;/h4&gt;领域泛化语义分割(DGSS)旨在提高模型在未见领域上的泛化能力，训练过程中无法访问目标数据。近期DGSS进展越来越多地利用视觉基础模型(VFMs)并通过参数高效微调策略，但大多数方法专注于全局特征微调，忽略了跨特征层级的层次化适应，这对精确密集预测至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，对视觉基础模型特征进行从粗到细的对齐，增强在领域变化下的鲁棒性，解决现有方法忽略层次化适应的问题。&lt;h4&gt;方法&lt;/h4&gt;提出多粒度特征校准(MGFC)框架，首先校准粗粒度特征捕获全局上下文语义和场景级结构，然后通过提高类别级特征判别能力细化中等粒度特征，最后通过高频空间细节增强校准细粒度特征，实现层次化和粒度感知的特征校准。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的广泛实验表明，该方法优于最先进的DGSS方法，多粒度适应对于领域泛化的语义分割任务是有效的。&lt;h4&gt;结论&lt;/h4&gt;多粒度特征校准(MGFC)框架能够有效解决DGSS任务中的领域泛化问题，层次化和粒度感知的特征校准对于视觉基础模型在语义分割任务中的有效迁移至关重要。&lt;h4&gt;翻译&lt;/h4&gt;领域泛化语义分割(DGSS)旨在提高模型在未见领域的泛化能力，且在训练过程中无法访问目标数据。近期DGSS进展越来越多地利用视觉基础模型(VFMs)并通过参数高效的微调策略。然而，大多数现有方法专注于全局特征微调，而忽略了跨特征层级的层次化适应，这对精确的密集预测至关重要。本文提出多粒度特征校准(MGFC)，一种新框架，对VFM特征进行从粗到细的对齐以增强在领域变化下的鲁棒性。具体而言，MGFC首先校准粗粒度特征以捕获全局上下文语义和场景级结构。然后，通过提高类别级特征判别能力来细化中等粒度特征。最后，通过高频空间细节增强来校准细粒度特征。通过层次化和粒度感知的校准，MGFC有效地将VFMs的泛化能力转移到DGSS的领域特定任务中。在基准数据集上的广泛实验表明，我们的方法优于最先进的DGSS方法，突显了多粒度适应对领域泛化语义分割任务的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain Generalized Semantic Segmentation (DGSS) aims to improve thegeneralization ability of models across unseen domains without access to targetdata during training. Recent advances in DGSS have increasingly exploitedvision foundation models (VFMs) via parameter-efficient fine-tuning strategies.However, most existing approaches concentrate on global feature fine-tuning,while overlooking hierarchical adaptation across feature levels, which iscrucial for precise dense prediction. In this paper, we proposeMulti-Granularity Feature Calibration (MGFC), a novel framework that performscoarse-to-fine alignment of VFM features to enhance robustness under domainshifts. Specifically, MGFC first calibrates coarse-grained features to captureglobal contextual semantics and scene-level structure. Then, it refinesmedium-grained features by promoting category-level feature discriminability.Finally, fine-grained features are calibrated through high-frequency spatialdetail enhancement. By performing hierarchical and granularity-awarecalibration, MGFC effectively transfers the generalization strengths of VFMs tothe domain-specific task of DGSS. Extensive experiments on benchmark datasetsdemonstrate that our method outperforms state-of-the-art DGSS approaches,highlighting the effectiveness of multi-granularity adaptation for the semanticsegmentation task of domain generalization.</description>
      <author>example@mail.com (Xinhui Li, Xiaojie Guo)</author>
      <guid isPermaLink="false">2508.03007v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Real-time speech enhancement in noise for throat microphone using neural audio codec as foundation model</title>
      <link>http://arxiv.org/abs/2508.02974v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了一个使用喉部麦克风捕获语音的实时语音增强演示系统，解决了喉部麦克风带宽受限的问题，并提供了完整的处理流程。&lt;h4&gt;背景&lt;/h4&gt;喉部麦克风记录皮肤振动，能自然衰减外部噪声，但代价是音频带宽减少，这限制了语音质量。&lt;h4&gt;目的&lt;/h4&gt;展示在嘈杂环境中使用体导麦克风捕获语音的完整处理流程，并解决喉部麦克风带宽受限的问题。&lt;h4&gt;方法&lt;/h4&gt;微调Kyutai的Mimi（一种支持实时推理的神经音频编解码器）在Vibravox数据集上，该数据集包含空气传导和喉部麦克风的配对录音。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进的模型相比，所提出的增强策略展现出优越的性能。&lt;h4&gt;结论&lt;/h4&gt;该实时语音增强系统有效解决了喉部麦克风带宽受限的问题，并通过交互式界面提供了良好的用户体验。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了一个使用喉部麦克风捕获语音的实时语音增强演示。这个演示旨在展示完整流程，从录制到基于深度学习的后处理，用于在嘈杂环境中使用体导麦克风捕获的语音。喉部麦克风记录皮肤振动，自然衰减外部噪声，但这种稳健性是以减少音频带宽为代价的。为应对这一挑战，我们在Vibravox数据集上微调了Kyutai的Mimi——一种支持实时推理的神经音频编解码器，该数据集包含空气传导和喉部麦克风的配对录音。我们将这种增强策略与最先进的模型进行比较，展示了其优越性能。推理在交互式界面中运行，允许用户切换增强、可视化频谱图和监控处理延迟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a real-time speech enhancement demo using speech captured with athroat microphone. This demo aims to showcase the complete pipeline, fromrecording to deep learning-based post-processing, for speech captured in noisyenvironments with a body-conducted microphone. The throat microphone recordsskin vibrations, which naturally attenuate external noise, but this robustnesscomes at the cost of reduced audio bandwidth. To address this challenge, wefine-tune Kyutai's Mimi--a neural audio codec supporting real-timeinference--on Vibravox, a dataset containing paired air-conducted and throatmicrophone recordings. We compare this enhancement strategy againststate-of-the-art models and demonstrate its superior performance. The inferenceruns in an interactive interface that allows users to toggle enhancement,visualize spectrograms, and monitor processing latency.</description>
      <author>example@mail.com (Julien Hauret, Thomas Joubaud, Éric Bavu)</author>
      <guid isPermaLink="false">2508.02974v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow</title>
      <link>http://arxiv.org/abs/2508.02959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 12 figures, under review for AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Polymath的自优化代理，具有动态层次工作流程，能够解决现实世界的动态问题，无需标记数据即可优化工作流程。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型通过代理工作流程解决复杂任务，但通过文本接口手动构建通用代理系统存在可扩展性和效率限制。现有基于代码的工作流程优化方法依赖标记数据，无法有效处理标记数据不可用的现实世界动态问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记数据即可自动生成和优化工作流程的代理系统，以解决现实世界的动态问题。&lt;h4&gt;方法&lt;/h4&gt;Polymath结合任务流图的灵活性和代码表示工作流程的表现力，采用多网格启发的图优化与自反思引导的进化算法相结合的优化方法，实现无标记数据的工作流程优化。&lt;h4&gt;主要发现&lt;/h4&gt;在编码、数学和多轮问答任务的六个基准数据集上，Polymath比最先进的基线平均提高了8.1%的性能。&lt;h4&gt;结论&lt;/h4&gt;Polymath通过动态层次工作流程和无标记数据优化方法，有效解决了现有代理系统在处理现实世界动态问题时的局限性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型通过执行由详细指令和结构化操作组成的代理工作流程，在解决复杂任务方面表现出色。然而，通过文本接口将基础模型手动嵌入到代理系统(如Chain-of-Thought、Self-Reflection和ReACT)中构建通用代理，限制了可扩展性和效率。最近，许多研究人员试图通过基于代码的表示来自动生成和优化这些工作流程。然而，现有方法通常依赖标记数据集来训练和优化工作流程，使得它们在解决标记数据不可用的现实世界动态问题时效果不佳且不够灵活。为解决这一挑战，我们引入了Polymath，这是一个具有动态层次工作流程的自优化代理，利用任务流图的灵活性和代码表示工作流程的表现力来解决各种现实世界的动态问题。所提出的优化方法将多网格启发的图优化与自反思引导的进化算法相结合，以在没有标记数据的情况下优化工作流程。在编码、数学和多轮问答任务的六个基准数据集上的实验结果表明，Polymath比最先进的基线平均提高了8.1%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) excel at solving complex tasks by executingagentic workflows composed of detailed instructions and structured operations.Yet, building general-purpose agents by manually embedding foundation modelsinto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACTthrough text interfaces limits scalability and efficiency. Recently, manyresearchers have sought to automate the generation and optimization of theseworkflows through code-based representations. However, existing methods oftenrely on labeled datasets to train and optimize workflows, making themineffective and inflexible for solving real-world, dynamic problems wherelabeled data is unavailable. To address this challenge, we introduce Polymath,a self-optimizing agent with dynamic hierarchical workflow that leverages theflexibility of task flow graphs and the expressiveness of code-representedworkflows to solve a wide range of real-world, dynamic problems. The proposedoptimization methodology integrates multi-grid-inspired graph optimization witha self-reflection-guided evolutionary algorithm to refine workflows withoutlabeled data. Experimental results on six benchmark datasets across coding,math, and multi-turn QA tasks show that Polymath achieves 8.1% averageimprovement over state-of-the-art baselines.</description>
      <author>example@mail.com (Chia-Tung Ho, Jing Gong, Xufeng Yao, Yunsheng Bai, Abhishek B Akkur, Haoxing Ren)</author>
      <guid isPermaLink="false">2508.02959v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game</title>
      <link>http://arxiv.org/abs/2508.02900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于Countdown游戏的规划基准测试，解决了现有基准测试无法有效衡量基础模型和代理规划能力的问题。Countdown游戏要求玩家通过算术运算从输入数字形成目标数字，该问题具有直观的自然语言描述、计算复杂度高(NP完全)、实例空间丰富等特点，能有效防止记忆问题。评估显示该基准测试对现有LLM辅助规划方法极具挑战性。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型和代理无法制定长期计划是一个关键局限，而现有规划基准测试存在严重不足：要么专注于难以形式化和验证的松散定义任务(如旅行规划)，要么利用国际规划竞赛中专门用于测试现有自动规划器弱点的现有问题。&lt;h4&gt;目的&lt;/h4&gt;创建一种以Countdown游戏为中心的规划基准测试，该游戏要求玩家通过算术运算从输入数字列表中形成目标数字，以有效评估基础模型和代理的规划能力。&lt;h4&gt;方法&lt;/h4&gt;提出Countdown规划基准测试并进行广泛理论分析，建立计算复杂性结果；展示实例生成程序优于公共基准测试的优势；使用程序生成的实例评估各种现有LLM辅助规划方法。&lt;h4&gt;主要发现&lt;/h4&gt;Countdown问题领域允许对每个问题实例进行直观的、自然语言的描述；该问题在计算上具有挑战性(NP完全)；实例空间足够丰富，无需担心记忆问题；与24点游戏等其他领域不同，所提出的动态基准测试对现有基于LLM的方法仍然极具挑战性。&lt;h4&gt;结论&lt;/h4&gt;Countdown基准测试满足了理想规划能力评估基准的多个要求，是一个有效的规划能力评估工具，能有效区分现有方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;普遍认为，无法制定长期计划是当前基础模型和代理的关键局限之一。然而，现有的规划基准测试远远不足以真正衡量它们的规划能力。大多数现有基准测试要么专注于难以形式化和验证的松散定义任务(如旅行规划)，要么最终利用国际规划竞赛中现有的领域和问题，而这些问题专门设计用于测试和挑战现有自动规划器的弱点。为解决这些不足，我们提出了一种创建以名为Countdown的游戏为中心的规划基准测试的程序，在该游戏中，玩家需要通过算术运算从输入数字列表中形成一个目标数字。我们讨论了这个问题如何满足与理想的规划能力评估基准相关的许多期望特性。具体来说，该领域允许对每个问题实例进行直观的、自然语言的描述，它在计算上具有挑战性(NP完全)，并且实例空间足够丰富，我们不必担心记忆问题。我们进行了广泛的理论分析，建立了计算复杂性结果，并展示了我们的实例生成程序优于公共基准测试的优势。我们使用我们的程序生成的实例评估了各种现有的LLM辅助规划方法。我们的结果表明，与其他领域(如24点游戏，Countdown的特殊情况)不同，我们提出的动态基准测试对现有的基于LLM的方法仍然极具挑战性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There is a broad consensus that the inability to form long-term plans is oneof the key limitations of current foundational models and agents. However, theexisting planning benchmarks remain woefully inadequate to truly measure theirplanning capabilities. Most existing benchmarks either focus on loosely definedtasks like travel planning or end up leveraging existing domains and problemsfrom international planning competitions. While the former tasks are hard toformalize and verify, the latter were specifically designed to test andchallenge the weaknesses of existing automated planners. To address theseshortcomings, we propose a procedure for creating a planning benchmark centeredaround the game called Countdown, where a player is expected to form a targetnumber from a list of input numbers through arithmetic operations. We discusshow this problem meets many of the desiderata associated with an idealbenchmark for planning capabilities evaluation. Specifically, the domain allowsfor an intuitive, natural language description for each problem instance, it iscomputationally challenging (NP-complete), and the instance space is richenough that we do not have to worry about memorization. We perform an extensivetheoretical analysis, establishing the computational complexity result anddemonstrate the advantage of our instance generation procedure over publicbenchmarks. We evaluate a variety of existing LLM-assisted planning methods oninstances generated using our procedure. Our results show that, unlike otherdomains like 24 Game (a special case of Countdown), our proposed dynamicbenchmark remains extremely challenging for existing LLM-based approaches.</description>
      <author>example@mail.com (Michael Katz, Harsha Kokel, Sarath Sreedharan)</author>
      <guid isPermaLink="false">2508.02900v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CauKer: classification time series foundation models can be pretrained on synthetic data only</title>
      <link>http://arxiv.org/abs/2508.02879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了CauKer，一种新型算法，用于生成多样且因果一致的人工时间序列，以实现时间序列基础模型(TSFMs)的高效样本预训练。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)因其强大的零样本能力和广泛的应用而受到关注，但通常需要在大规模、精心收集的真实序列上进行计算成本高昂的预训练。&lt;h4&gt;目的&lt;/h4&gt;提出一种算法，允许时间序列基础模型以更高效的样本方式进行预训练，减少对大规模真实数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;CauKer算法结合高斯过程(GP)核组合与结构因果模型(SCM)，生成具有真实趋势、季节性和非线性交互的多样因果一致人工时间序列数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CauKer生成的数据集在数据集规模(10K至10M样本)和模型容量(1M至783M参数)方面显示出明确的缩放定律，而真实世界数据集则表现出不规则的缩放行为。&lt;h4&gt;结论&lt;/h4&gt;CauKer为时间序列基础模型提供了一种样本高效的预训练方法，通过生成高质量的人工时间序列数据，减少了对大规模真实数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)最近因其强大的零样本能力和广泛的应用而受到广泛关注。这类模型通常需要在大规模、精心收集的真实序列上进行计算成本高昂的预训练。为了实现TSFMs的样本高效预训练，我们提出了CauKer，一种新颖的算法，旨在生成具有真实趋势、季节性和非线性交互的多样、因果一致的人工时间序列。CauKer将高斯过程(GP)核组合与结构因果模型(SCM)相结合，为具有不同架构和遵循不同预训练方法的最先进分类TSFMs生成数据，以实现样本高效预训练。此外，我们的实验揭示，与表现出不规则缩放行为的真实世界数据集不同，CauKer生成的数据集在数据集规模(10K至10M样本)和模型容量(1M至783M参数)方面显示出明确的缩放定律。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (TSFMs) have recently gained significantattention due to their strong zero-shot capabilities and widespread real-worldapplications. Such models typically require a computationally costlypretraining on large-scale, carefully curated collections of real-worldsequences. To allow for a sample-efficient pretraining of TSFMs, we proposeCauKer, a novel algorithm designed to generate diverse, causally coherentsynthetic time series with realistic trends, seasonality, and nonlinearinteractions. CauKer combines Gaussian Process (GP) kernel composition withStructural Causal Models (SCM) to produce data for sample-efficient pretrainingof state-of-the-art classification TSFMs having different architectures andfollowing different pretraining approaches. Additionally, our experimentsreveal that CauKer-generated datasets exhibit clear scaling laws for bothdataset size (10K to 10M samples) and model capacity (1M to 783M parameters),unlike real-world datasets, which display irregular scaling behavior.</description>
      <author>example@mail.com (Shifeng Xie, Vasilii Feofanov, Marius Alonso, Ambroise Odonnat, Jianfeng Zhang, Themis Palpanas, Ievgen Redko)</author>
      <guid isPermaLink="false">2508.02879v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows</title>
      <link>http://arxiv.org/abs/2508.02866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper under peer-reviewed evaluation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PROV-AGENT，一种专为AI代理工作流设计的溯源模型，扩展了W3C PROV并利用模型上下文协议(MCP)来集成代理交互到端到端工作流溯源中。该系统解决了现有技术在捕获以代理为中心的元数据方面的不足，提供了近实时、开源的代理溯源解决方案，并在多种环境中进行了评估。&lt;h4&gt;背景&lt;/h4&gt;基础模型（如大型语言模型）正越来越多地被用作复杂、大规模工作流中AI代理的核心组件，这些工作流跨越联邦和异构环境。在代理工作流中，自主代理规划任务、与人类和同行互动，并塑造科学成果，这使得透明度、可追溯性、可重复性和可靠性变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决AI代理可能产生的幻觉或推理错误及其在工作流中传播的问题，通过引入细粒度溯源来链接代理决策、端到端上下文和下游影响。&lt;h4&gt;方法&lt;/h4&gt;介绍PROV-AGENT，一种扩展W3C PROV并利用模型上下文协议(MCP)的溯源模型，将代理交互集成到端到端工作流溯源中。&lt;h4&gt;主要发现&lt;/h4&gt;现有溯源技术无法捕获和关联以代理为中心的元数据（提示、响应和决策）与工作流的其余部分；PROV-AGENT提供了近实时、开源的代理溯源系统，并在边缘、云和HPC环境中展示了支持关键溯源查询和代理可靠性分析的能力。&lt;h4&gt;结论&lt;/h4&gt;PROV-AGENT解决了现有技术在捕获代理元数据方面的不足，提供了在多种环境中工作的系统，支持关键溯源查询和代理可靠性分析，有助于提高AI代理工作流的透明度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;基础模型，如大型语言模型，正越来越多地被用作复杂、大规模工作流中AI代理的核心组件，这些工作流跨越联邦和异构环境。在代理工作流中，自主代理规划任务、与人类和同行互动，并塑造科学成果。这使得透明度、可追溯性、可重复性和可靠性变得至关重要。然而，基于AI的代理可能产生幻觉或推理错误，并且它们的决策可能会在工作流中传播错误，特别是当一个代理的输出成为另一个代理的输入时。因此，细粒度溯源对于链接代理决策、它们的端到端上下文和下游影响至关重要。虽然溯源技术长期以来支持可重复性和工作流数据理解，但它们无法捕获并将以代理为中心的元数据（提示、响应和决策）与工作流的其余部分关联起来。在本文中，我们介绍了PROV-AGENT，一种扩展W3C PROV并利用模型上下文协议(MCP)将代理交互集成到端到端工作流溯源中的溯源模型。我们的贡献包括：(1)专为代理工作流定制的溯源模型，(2)用于捕获代理溯源的近实时、开源系统，以及(3)跨越边缘、云和HPC环境的跨设施评估，展示了支持关键溯源查询和代理可靠性分析的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, such as Large Language Models (LLMs), are increasinglyused as core components of AI agents in complex, large-scale workflows acrossfederated and heterogeneous environments. In agentic workflows, autonomousagents plan tasks, interact with humans and peers, and shape scientificoutcomes. This makes transparency, traceability, reproducibility, andreliability essential. However, AI-based agents can hallucinate or reasonincorrectly, and their decisions may propagate errors through the workflow,especially when one agent's output feeds into another's input. Therefore,fine-grained provenance is essential to link agent decisions, their end-to-endcontext, and downstream impacts. While provenance techniques have longsupported reproducibility and workflow data understanding, they fail to captureand relate agent-centric metadata (prompts, responses, and decisions) with therest of the workflow. In this paper, we introduce PROV-AGENT, a provenancemodel that extends W3C PROV and leverages the Model Context Protocol (MCP) tointegrate agent interactions into end-to-end workflow provenance. Ourcontributions include: (1) a provenance model tailored for agentic workflows,(2) a near real-time, open-source system for capturing agentic provenance, and(3) a cross-facility evaluation spanning edge, cloud, and HPC environments,demonstrating support for critical provenance queries and agent reliabilityanalysis.</description>
      <author>example@mail.com (Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva)</author>
      <guid isPermaLink="false">2508.02866v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation</title>
      <link>http://arxiv.org/abs/2508.02808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ICARE的可解释评估框架，用于评估自动生成的放射学报告。该框架利用大型语言模型代理和动态多选题回答方法，通过让两个代理基于不同报告相互提问来评估报告的临床准确性和一致性。研究表明，这种方法与专家判断更为一致，且具有良好的可解释性。&lt;h4&gt;背景&lt;/h4&gt;放射学成像在诊断、治疗计划和临床决策中至关重要。视觉-语言基础模型推动了自动放射学报告生成的发展，但安全部署需要可靠的临床评估。现有评估方法往往依赖表面相似度或作为黑盒操作，缺乏可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种可解释且基于临床的放射学报告生成评估框架，解决现有评估指标的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入ICARE（可解释且基于临床的代理报告评估）框架，利用两个大型语言模型代理分别基于真实报告或生成报告生成临床问题并相互提问。答案的一致性作为临床精确度和召回率的可解释指标，通过将分数与问题-答案对关联实现透明评估。&lt;h4&gt;主要发现&lt;/h4&gt;临床研究表明ICARE与专家判断的一致性显著高于先前指标；扰动分析证实了其对临床内容的敏感性和可重复性；模型比较揭示了可解释的错误模式。&lt;h4&gt;结论&lt;/h4&gt;ICARE提供了一种更可靠、更可解释的放射学报告生成评估方法，能够更好地与临床专家判断保持一致。&lt;h4&gt;翻译&lt;/h4&gt;放射学成像在诊断、治疗计划和临床决策中至关重要。视觉-语言基础模型激发了人们对自动放射学报告生成(RRG)的兴趣，但安全部署需要对生成的报告进行可靠的临床评估。现有指标通常依赖于表面相似度或表现为黑盒，缺乏可解释性。我们引入ICARE（可解释且基于临床的代理报告评估），这是一个利用大型语言模型代理和动态多选题回答(MCQA)的可解释评估框架。两个代理分别基于真实报告或生成报告，生成有临床意义的问题并相互提问。答案上的一致性捕捉了发现结果的保留性和一致性，作为临床精确度和召回率的可解释代理。通过将分数与问题-答案对关联，ICARE实现了透明且可解释的评估。临床研究表明，ICARE与专家判断的一致性显著高于先前的指标。扰动分析证实了对临床内容的敏感性和可重复性，而模型比较则揭示了可解释的错误模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radiological imaging is central to diagnosis, treatment planning, andclinical decision-making. Vision-language foundation models have spurredinterest in automated radiology report generation (RRG), but safe deploymentrequires reliable clinical evaluation of generated reports. Existing metricsoften rely on surface-level similarity or behave as black boxes, lackinginterpretability. We introduce ICARE (Interpretable and Clinically-groundedAgent-based Report Evaluation), an interpretable evaluation frameworkleveraging large language model agents and dynamic multiple-choice questionanswering (MCQA). Two agents, each with either the ground-truth or generatedreport, generate clinically meaningful questions and quiz each other. Agreementon answers captures preservation and consistency of findings, serving asinterpretable proxies for clinical precision and recall. By linking scores toquestion-answer pairs, ICARE enables transparent, and interpretable assessment.Clinician studies show ICARE aligns significantly more with expert judgmentthan prior metrics. Perturbation analyses confirm sensitivity to clinicalcontent and reproducibility, while model comparisons reveal interpretable errorpatterns.</description>
      <author>example@mail.com (Radhika Dua, Young Joon, Kwon, Siddhant Dogra, Daniel Freedman, Diana Ruan, Motaz Nashawaty, Danielle Rigau, Daniel Alexander Alber, Kang Zhang, Kyunghyun Cho, Eric Karl Oermann)</author>
      <guid isPermaLink="false">2508.02808v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision</title>
      <link>http://arxiv.org/abs/2508.03177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对大型视觉-语言模型在风格化图像中出现的幻觉问题，提出了SAVER机制，通过基于标记级视觉注意力模式的动态调整，有效减轻了风格化图像引发的幻觉现象。&lt;h4&gt;背景&lt;/h4&gt;大型视觉-语言模型在理解复杂的视觉-文本上下文方面取得了重大突破，但幻觉问题仍然限制了它们的实际应用能力。现有方法主要关注照片图像中的幻觉问题，忽视了风格化图像在游戏场景理解、艺术教育和医疗分析等关键场景中的潜在风险。&lt;h4&gt;目的&lt;/h4&gt;研究风格化图像对LVLMs幻觉现象的影响，并提出有效的缓解方法。&lt;h4&gt;方法&lt;/h4&gt;首先构建了一个包含照片图像及其对应风格化版本的标注数据集，然后对13个先进的LVLMs进行判别性和生成性任务的比较测试。基于研究结果，提出了Style-Aware Visual Early Revision (SAVER)机制，该机制基于标记级别的视觉注意力模式动态调整LVLMs的最终输出，利用早期层的反馈来减轻风格化图像引起的幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;风格化图像比其照片对应图像更容易引发幻觉现象。SAVER机制在各种模型、数据集和任务中实现了最先进的幻觉缓解性能。&lt;h4&gt;结论&lt;/h4&gt;SAVER作为一种新颖的幻觉缓解机制，能够有效处理风格化图像引发的幻觉问题，提高了LVLMs在关键场景中的可靠性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉-语言模型最近在理解复杂的视觉-文本上下文方面取得了重大突破。然而，幻觉问题仍然限制了它们的实际应用能力。尽管先前的缓解方法能有效减少照片图像中的幻觉，但它们 largely 忽视了风格化图像带来的潜在风险，而这些图像在游戏场景理解、艺术教育和医疗分析等关键场景中扮演着重要角色。在本工作中，我们首先构建了一个包含照片图像及其对应风格化版本的标注数据集。然后，通过在收集的数据集上对13个先进的LVLMs进行判别性和生成性任务的比较测试。我们的研究结果表明，风格化图像比其照片对应图像更容易引发幻觉。为了解决这个问题，我们提出了Style-Aware Visual Early Revision，一种新颖的机制，它基于标记级别的视觉注意力模式动态调整LVLMs的最终输出，利用早期层的反馈来减轻风格化图像引起的幻觉。大量实验表明，SAVER在各种模型、数据集和任务中实现了最先进的幻觉缓解性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision-Language Models (LVLMs) recently achieve significantbreakthroughs in understanding complex visual-textual contexts. However,hallucination issues still limit their real-world applicability. Althoughprevious mitigation methods effectively reduce hallucinations in photographicimages, they largely overlook the potential risks posed by stylized images,which play crucial roles in critical scenarios such as game sceneunderstanding, art education, and medical analysis. In this work, we firstconstruct a dataset comprising photographic images and their correspondingstylized versions with carefully annotated caption labels. We then conducthead-to-head comparisons on both discriminative and generative tasks bybenchmarking 13 advanced LVLMs on the collected datasets. Our findings revealthat stylized images tend to induce significantly more hallucinations thantheir photographic counterparts. To address this issue, we propose Style-AwareVisual Early Revision SAVER, a novel mechanism that dynamically adjusts LVLMs'final outputs based on the token-level visual attention patterns, leveragingearly-layer feedback to mitigate hallucinations caused by stylized images.Extensive experiments demonstrate that SAVER achieves state-of-the-artperformance in hallucination mitigation across various models, datasets, andtasks.</description>
      <author>example@mail.com (Zhaoxu Li, Chenqi Kong, Yi Yu, Qiangqiang Wu, Xinghao Jiang, Ngai-Man Cheung, Bihan Wen, Alex Kot, Xudong Jiang)</author>
      <guid isPermaLink="false">2508.03177v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions</title>
      <link>http://arxiv.org/abs/2508.03173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Geoint-R1多模态推理框架和Geoint基准测试，解决了现有模型在形式几何推理中的困难，特别是在动态构建和验证辅助几何元素方面。Geoint-R1能够从文本描述和视觉图表生成可形式验证的几何解决方案，并在测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;数学几何推理对科学发现和教育发展至关重要，需要精确的逻辑和严格的正式验证。尽管多模态大语言模型(MLLMs)的最新进展改善了推理任务，但现有模型在形式几何推理方面通常表现不佳，特别是在动态构建和验证辅助几何元素时。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型在形式几何推理方面的挑战，特别是动态构建和验证辅助几何元素方面的困难，开发能够生成可形式验证几何解决方案的框架。&lt;h4&gt;方法&lt;/h4&gt;引入Geoint-R1多模态推理框架，整合辅助元素构建、通过Lean4表示的形式推理和交互式可视化；提出Geoint基准测试，包含1,885个跨平面几何、空间几何和立体几何等不同主题的严格注释几何问题，每个问题包括结构化文本注释、精确Lean4代码和专家验证的详细解决方案步骤。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，Geoint-R1显著优于现有的多模态和特定数学推理模型，特别是在需要明确辅助元素构建的挑战性问题上。&lt;h4&gt;结论&lt;/h4&gt;Geoint-R1框架和Geoint基准为形式几何推理提供了有效的解决方案和评估工具，推动了该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;数学几何推理对于科学发现和教育发展至关重要，需要精确的逻辑和严格的正式验证。虽然多模态大语言模型(MLLMs)的最新进展已经改善了推理任务，但现有模型通常在形式几何推理方面表现不佳，特别是在动态构建和验证辅助几何元素时。为了解决这些挑战，我们引入了Geoint-R1，这是一个多模态推理框架，旨在从文本描述和视觉图表中生成可形式验证的几何解决方案。Geoint-R1独特地集成了辅助元素构建、通过Lean4表示的形式推理和交互式可视化。为了系统评估和推进形式几何推理，我们提出了Geoint基准，包含1,885个跨平面几何、空间几何和立体几何等不同主题的严格注释几何问题。每个问题包括结构化文本注释、用于辅助构造的精确Lean4代码以及专家验证的详细解决方案步骤。大量实验表明，Geoint-R1显著优于现有的多模态和特定数学推理模型，特别是在需要明确辅助元素构建的挑战性问题上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mathematical geometric reasoning is essential for scientific discovery andeducational development, requiring precise logic and rigorous formalverification. While recent advances in Multimodal Large Language Models (MLLMs)have improved reasoning tasks, existing models typically struggle with formalgeometric reasoning, particularly when dynamically constructing and verifyingauxiliary geometric elements. To address these challenges, we introduceGeoint-R1, a multimodal reasoning framework designed to generate formallyverifiable geometric solutions from textual descriptions and visual diagrams.Geoint-R1 uniquely integrates auxiliary elements construction, formal reasoningrepresented via Lean4, and interactive visualization. To systematicallyevaluate and advance formal geometric reasoning, we propose the Geointbenchmark, comprising 1,885 rigorously annotated geometry problems acrossdiverse topics such as plane, spatial, and solid geometry. Each problemincludes structured textual annotations, precise Lean4 code for auxiliaryconstructions, and detailed solution steps verified by experts. Extensiveexperiments demonstrate that Geoint-R1 significantly surpasses existingmultimodal and math-specific reasoning models, particularly on challengingproblems requiring explicit auxiliary element constructions.</description>
      <author>example@mail.com (Jingxuan Wei, Caijun Jia, Qi Chen, Honghao He, Linzhuang Sun, Conghui He, Lijun Wu, Bihui Yu, Cheng Tan)</author>
      <guid isPermaLink="false">2508.03173v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.03060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CHARM框架，一种新颖的互补学习框架，用于实现跨模态的语义分割。CHARM通过隐式对齐内容同时保留各模态的独特优势，避免了传统方法中模态同质化的问题。框架包含相互感知单元(MPU)和双路径优化策略两个核心组件，在多个数据集和骨干网络上均表现出色，特别是在脆弱模态上有显著提升。&lt;h4&gt;背景&lt;/h4&gt;现有的模态无关语义分割方法通常依赖显式特征对齐来实现模态同质化，这种方法削弱了各模态的独特优势并破坏了它们内在的互补性。&lt;h4&gt;目的&lt;/h4&gt;实现跨模态的协同和谐化而非同质化，保留各模态的特定优势，同时实现内容层面的隐式对齐。&lt;h4&gt;方法&lt;/h4&gt;提出CHARM框架，包含两个核心组件：(1)相互感知单元(MPU)，通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文；(2)双路径优化策略，将训练解耦为互补融合学习的协作学习策略(CoL)和保护模态特定优化的个体增强策略(InE)。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集和骨干网络上的实验表明，CHARM框架始终优于基线方法，特别是在脆弱模态上有显著提升。&lt;h4&gt;结论&lt;/h4&gt;本研究将重点从模型同质化转向和谐化，实现了真正的跨模态互补，为多样化模态的真正和谐提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;模态无关的语义分割(MaSS)旨在实现跨越任意输入模态组合的鲁棒场景理解。现有方法通常依赖显式特征对齐来实现模态同质化，这削弱了各模态的独特优势并破坏了它们内在的互补性。为了实现协同和谐化而非同质化，我们提出了CHARM，一种新颖的互补学习框架，通过两个组件设计来隐式对齐内容同时保留模态特定优势：(1)相互感知单元(MPU)，通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文，以发现模态交互对应关系；(2)双路径优化策略，将训练解耦为互补融合学习的协作学习策略(CoL)和保护模态特定优化的个体增强策略(InE)。在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，在脆弱模态上有显著提升。本研究将重点从模型同质化转向和谐化，实现了真正的跨模态互补，实现了多样化中的真正和谐。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决模态无关语义分割（MaSS）中现有方法通过显式特征对齐实现模态同质化的问题，这种方法会稀释各模态的独特优势并破坏其互补性。这个问题在现实中非常重要，因为不同传感器模态（如RGB、LiDAR、深度等）在恶劣环境（如雨天、夜间）下各有优势和局限性，保留各模态独特特性并实现有效互补对于自动驾驶、监控等安全关键应用中的鲁棒场景理解至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有MaSS方法的局限性：显式特征对齐导致模态同质化，削弱了各模态独特优势。基于两个设计原则（模态应相互理解协调；应积极强化互补特征而非抑制），作者设计了CHARM框架，包含相互感知单元（MPU）实现隐式对齐，以及双路径优化策略平衡协作与个体增强。作者借鉴了现有多模态分割工作（如Swin Transformer的窗口注意力设计）和MaSS方法（如MAGIC、Any2Seg），但针对其局限性提出了新的调和范式而非简单同质化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是实现'协同调和'而非'同质化'，保留模态独特性同时实现互补融合。整体流程包括：1)共享编码器提取各模态特征；2)双路径处理（协作学习CoL和个体增强InE）；3)CoL路径通过鲁棒性加权融合和MPU实现模态互补；4)InE路径通过脆弱模态偏向采样和MPU增强个体模态；5)双路径输出通过分割头预测并计算损失；6)推理时类似CoL路径但直接使用模态特征作为上下文。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从'同质化'到'调和'的范式转变；2)相互感知单元（MPU）实现隐式对齐；3)双路径优化策略（CoL和InE）；4)鲁棒性引导的模态处理。相比之前工作，CHARM不使用显式对齐约束强制特征一致性，而是通过MPU实现隐式对齐；不倾向于将辅助模态向主模态对齐，而是实现真正的模态间互补；采用双路径优化而非单一路径，在保持协作学习的同时增强个体模态；在脆弱模态组合上性能显著提升（Last-1 mIoU提高超过28%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出CHARM框架，通过相互感知单元和双路径优化策略实现了跨模态的协同调和而非简单同质化，显著提升了模态无关语义分割在任意模态组合下的性能，特别是在脆弱模态上的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust sceneunderstanding across arbitrary combinations of input modality. Existing methodstypically rely on explicit feature alignment to achieve modal homogenization,which dilutes the distinctive strengths of each modality and destroys theirinherent complementarity. To achieve cooperative harmonization rather thanhomogenization, we propose CHARM, a novel complementary learning frameworkdesigned to implicitly align content while preserving modality-specificadvantages through two components: (1) Mutual Perception Unit (MPU), enablingimplicit alignment through window-based cross-modal interaction, wheremodalities serve as both queries and contexts for each other to discovermodality-interactive correspondences; (2) A dual-path optimization strategythat decouples training into Collaborative Learning Strategy (CoL) forcomplementary fusion learning and Individual Enhancement Strategy (InE) forprotected modality-specific optimization. Experiments across multiple datasetsand backbones indicate that CHARM consistently outperform the baselines, withsignificant increment on the fragile modalities. This work shifts the focusfrom model homogenization to harmonization, enabling cross-modalcomplementarity for true harmony in diversity.</description>
      <author>example@mail.com (Lekang Wen, Jing Xiao, Liang Liao, Jiajun Chen, Mi Wang)</author>
      <guid isPermaLink="false">2508.03060v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Taking Language Embedded 3D Gaussian Splatting into the Wild</title>
      <link>http://arxiv.org/abs/2507.19830v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Visit our project page at  https://yuzewang1998.github.io/takinglangsplatw/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架，用于从不受约束的照片集合中进行开放词汇场景理解，扩展了语言嵌入3D高斯溅射技术，并通过多外观CLIP特征和语言特征不确定性图指导优化过程。作者还提出了临时不确定性感知自动编码器、多外观语言场3DGS表示和后集成策略，并引入了PT-OVS基准数据集进行评估。实验结果表明该方法优于现有方法，支持多种应用。&lt;h4&gt;背景&lt;/h4&gt;利用大规模互联网照片集合进行3D重建的最新进展使得全球地标和历史遗迹的沉浸式虚拟探索成为可能。然而，建筑风格和结构知识的沉浸式理解很少受到关注，主要局限于浏览静态文本图像对，对建筑组件3D结构的沉浸式理解研究不足。&lt;h4&gt;目的&lt;/h4&gt;从3D野外重建技术中汲取灵感，利用不受约束的照片集合创建一种沉浸式方法，用于理解建筑组件的3D结构，开发能够从不受约束照片集合中进行开放词汇场景理解的方法。&lt;h4&gt;方法&lt;/h4&gt;1) 从重建的辐射场中渲染与不受约束图像相同视点的多个外观图像；2) 提取多外观CLIP特征和两种语言特征不确定性图（临时不确定性和外观不确定性）；3) 提出临时不确定性感知自动编码器、多外观语言场3DGS表示和后集成策略；4) 引入PT-OVS基准数据集用于评估不受约束照片集合上的开放词汇分割性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的方法优于现有方法，能够实现准确的开放词汇分割，并支持开放词汇查询的交互式漫游、建筑风格模式识别和3D场景编辑等应用。&lt;h4&gt;结论&lt;/h4&gt;通过扩展语言嵌入3D高斯溅射技术并引入新框架和组件，成功实现了从不受约束照片集合中进行开放词汇场景理解，为建筑组件的3D结构理解提供了新的沉浸式方法。&lt;h4&gt;翻译&lt;/h4&gt;利用大规模互联网照片集合进行3D重建的最新进展使得全球地标和历史遗迹的沉浸式虚拟探索成为可能。然而，很少有人关注建筑风格和结构知识的沉浸式理解，这部分内容主要局限于浏览静态文本图像对。因此，我们能否从3D野外重建技术中汲取灵感，利用不受约束的照片集合创建一种沉浸式方法，用于理解建筑组件的3D结构？为此，我们扩展了语言嵌入3D高斯溅射技术，并提出了一个用于从不受约束照片集合中进行开放词汇场景理解的新框架。具体来说，我们首先从重建的辐射场中渲染与不受约束图像相同视点的多个外观图像，然后提取多外观CLIP特征和两种类型的语言特征不确定性图——临时不确定性和外观不确定性——这些特征来自多外观特征，用于指导后续的优化过程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从不受约束的照片集合(如互联网收集的照片)中实现开放词汇的场景理解，特别是对建筑结构和风格的沉浸式理解。这个问题在现实中很重要，因为它能让人们以更直观、沉浸式的方式探索历史建筑和地标的结构、风格和历史背景，而不仅仅是浏览静态的文本-图像对。在研究中，这是一个挑战性问题，因为不受约束的照片集合包含各种条件下的图像(不同年份、光照、视角)和临时遮挡物，会导致多视图语言特征不一致，影响3D场景理解的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：现有3D开放词汇场景理解方法依赖高质量受控拍摄的照片集合，而不受约束照片集合中的外观变化和临时遮挡物会导致多视图CLIP特征不一致；同时CLIP特征的非加性性质使得直接适应野外辐射场重建方法困难。作者借鉴了现有工作，包括3D高斯飞溅(3DGS)技术用于场景表示、野外3D场景重建方法(如WE-GS)处理不受约束照片集合、以及开放词汇场景理解方法(如LangSplat)结合自然语言与3D表示。基于这些，作者设计了一个新框架，通过多外观CLIP特征和不确定性图来处理外观变化和临时遮挡物问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多外观CLIP特征和不确定性图来处理不受约束照片集合中的外观变化和临时遮挡物，实现开放词汇场景理解。具体通过渲染多外观图像并提取对应的CLIP特征，计算外观不确定性和瞬时不确定性图来指导优化过程。整体流程包括：1)多外观像素级语言特征提取：渲染多外观图像并提取CLIP特征，计算不确定性图；2)瞬时不确定性感知自动编码器：压缩CLIP特征到低维表示；3)多外观语言嵌入3DGS：为每个3D高斯分配多个语言特征并优化；4)后融合和开放词汇查询：融合多外观CLIP特征，应用背景过滤和加权融合，执行开放词汇分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多外观CLIP特征增强策略，通过渲染多外观图像增强语言特征鲁棒性；2)两种不确定性图(外观和瞬时不确定性图)量化语义不确定性；3)瞬时不确定性感知自动编码器有效压缩语言特征；4)多外观语言嵌入3DGS学习多外观CLIP特征；5)后融合策略支持开放词汇查询；6)PT-OVS基准数据集评估不受约束照片集合上的开放词汇分割性能。相比之前工作，不同之处在于：之前的开放词汇3D场景理解方法依赖高质量受控拍摄的照片集合，而野外3D场景重建方法主要关注辐射场重建而非语言场；本文首次将语言嵌入3D高斯飞溅扩展到不受约束照片集合，实现了开放词汇场景理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一个从不受约束照片集合中构建语言嵌入3D高斯飞溅的框架，实现了对建筑结构和风格的沉浸式开放词汇理解，并引入了新的基准数据集PT-OVS进行评估。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in leveraging large-scale Internet photo collections for 3Dreconstruction have enabled immersive virtual exploration of landmarks andhistoric sites worldwide. However, little attention has been given to theimmersive understanding of architectural styles and structural knowledge, whichremains largely confined to browsing static text-image pairs. Therefore, can wedraw inspiration from 3D in-the-wild reconstruction techniques and useunconstrained photo collections to create an immersive approach forunderstanding the 3D structure of architectural components? To this end, weextend language embedded 3D Gaussian splatting (3DGS) and propose a novelframework for open-vocabulary scene understanding from unconstrained photocollections. Specifically, we first render multiple appearance images from thesame viewpoint as the unconstrained image with the reconstructed radiancefield, then extract multi-appearance CLIP features and two types of languagefeature uncertainty maps-transient and appearance uncertainty-derived from themulti-appearance features to guide the subsequent optimization process. Next,we propose a transient uncertainty-aware autoencoder, a multi-appearancelanguage field 3DGS representation, and a post-ensemble strategy to effectivelycompress, learn, and fuse language features from multiple appearances. Finally,to quantitatively evaluate our method, we introduce PT-OVS, a new benchmarkdataset for assessing open-vocabulary segmentation performance on unconstrainedphoto collections. Experimental results show that our method outperformsexisting methods, delivering accurate open-vocabulary segmentation and enablingapplications such as interactive roaming with open-vocabulary queries,architectural style pattern recognition, and 3D scene editing.</description>
      <author>example@mail.com (Yuze Wang, Yue Qi)</author>
      <guid isPermaLink="false">2507.19830v2</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Entity Representation Learning Through Onsite-Offsite Graph for Pinterset Ads</title>
      <link>http://arxiv.org/abs/2508.02609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用图神经网络和知识图谱嵌入技术改进广告推荐系统，构建了基于用户线上广告互动和线下转化活动的大规模异构图，提出TransRA模型和基于注意力的KGE微调方法，显著提高了CTR和CVR预测模型性能，并在Pinterest广告系统中实现了2.69%的CTR提升和1.34%的CPC降低。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已广泛应用于工业推荐系统，如GraphSage、TwHIM和LiGNN等模型，这些模型基于用户平台活动构建图结构学习节点嵌入。除线上活动外，用户的线下转化数据对捕捉购物兴趣同样重要。&lt;h4&gt;目的&lt;/h4&gt;更好地利用线下转化数据，探索线上和线下活动间的联系，并将知识图谱嵌入有效整合到广告排序模型中，提高点击率和转化率预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;1) 构建基于用户线上广告互动和线下转化活动的大规模异构图；2) 引入TransRA(带锚点的TransR)模型作为新型知识图谱嵌入方法；3) 采用大型ID嵌入表技术；4) 创新基于注意力的KGE微调方法应用于广告排序模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 广告排序模型最初难以直接整合知识图谱嵌入，仅观察到适度提升；2) 通过新技术显著提高了CTR和CVR预测模型的AUC；3) 在Pinterest广告系统中部署后，实现了2.69%的CTR提升和1.34%的CPC降低。&lt;h4&gt;结论&lt;/h4&gt;提出的技术可有效应用于大规模工业广告推荐系统，显著提升广告效果，具有广泛的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已广泛应用于工业推荐系统，如GraphSage、TwHIM、LiGNN等模型。在这些工作中，图是基于用户在平台上的活动构建的，各种图模型被开发用于有效学习节点嵌入。除了用户的线上活动外，他们的线下转化对广告模型捕捉购物兴趣也至关重要。为了更好地利用线下转化数据并探索线上和线下活动之间的联系，我们基于用户的线上广告互动和选择参与的线下转化活动构建了一个大规模异构图。此外，我们引入了TransRA(带锚点的TransR)，这是一种新的知识图谱嵌入模型，可以更有效地将图嵌入整合到广告排序模型中。然而，我们的广告排序模型最初难以直接整合知识图谱嵌入，离线实验中只观察到适度的提升。为了解决这一挑战，我们采用了大型ID嵌入表技术，并在广告排序模型中创新了一种基于注意力的KGE微调方法。结果，我们在点击率和转化率预测模型中观察到了显著的AUC提升。此外，该框架已在Pinterest的广告参与模型中部署，并为CTR提升2.69%和CPC降低1.34%做出了贡献。我们相信本文提出的技术可以被其他大规模工业模型所利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNN) have been extensively applied to industryrecommendation systems, as seen in models like GraphSage\cite{GraphSage},TwHIM\cite{TwHIM}, LiGNN\cite{LiGNN} etc. In these works, graphs wereconstructed based on users' activities on the platforms, and various graphmodels were developed to effectively learn node embeddings. In addition tousers' onsite activities, their offsite conversions are crucial for Ads modelsto capture their shopping interest. To better leverage offsite conversion dataand explore the connection between onsite and offsite activities, weconstructed a large-scale heterogeneous graph based on users' onsite adinteractions and opt-in offsite conversion activities. Furthermore, weintroduced TransRA (TransR\cite{TransR} with Anchors), a novel Knowledge GraphEmbedding (KGE) model, to more efficiently integrate graph embeddings into Adsranking models. However, our Ads ranking models initially struggled to directlyincorporate Knowledge Graph Embeddings (KGE), and only modest gains wereobserved during offline experiments. To address this challenge, we employed theLarge ID Embedding Table technique and innovated an attention based KGEfinetuning approach within the Ads ranking models. As a result, we observed asignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)prediction models. Moreover, this framework has been deployed in Pinterest'sAds Engagement Model and contributed to $2.69\%$ CTR lift and $1.34\%$ CPCreduction. We believe the techniques presented in this paper can be leveragedby other large-scale industrial models.</description>
      <author>example@mail.com (Jiayin Jin, Zhimeng Pan, Yang Tang, Jiarui Feng, Kungang Li, Chongyuan Xiang, Jiacheng Li, Runze Su, Siping Ji, Han Sun, Ling Leng, Prathibha Deshikachar)</author>
      <guid isPermaLink="false">2508.02609v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
  <item>
      <title>Adaptive Riemannian Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.02600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了自适应黎曼图神经网络(ARGNN)框架，学习连续且各向异性的黎曼度量张量场，使图神经网络能够适应图数据中复杂的几何异质性，包括树状层次结构和密集社区等不同局部曲率的结构。&lt;h4&gt;背景&lt;/h4&gt;图数据通常表现出复杂的几何异质性，其中具有不同局部曲率的结构（如树状层次结构和密集社区）共存于单个网络中。现有的几何图神经网络将图嵌入到单一固定曲率流形或离散乘积空间中，难以捕捉这种多样性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自适应处理图数据中复杂几何异质性的图神经网络框架，使模型能够根据节点的局部特性确定最优几何表示，从而更好地捕捉图的结构多样性。&lt;h4&gt;方法&lt;/h4&gt;提出自适应黎曼图神经网络(ARGNN)，学习一个连续且各向异性的黎曼度量张量场在图上。核心创新是节点级度量张量的高效参数化，采用可学习的对角形式，在保持计算可处理性的同时捕捉方向几何信息。同时，集成了受里奇流启发的正则化，确保几何规律性和稳定训练。&lt;h4&gt;主要发现&lt;/h4&gt;ARGNN在同类和异类基准数据集上表现出优越性能，能够自适应地捕捉多样化结构。学习到的几何为底层图结构提供了可解释的见解，并 empirically corroborate 理论分析。理论上建立了ARGNN的严格几何演化收敛保证，并提供了统一先前固定或混合曲率GNNs的连续泛化。&lt;h4&gt;结论&lt;/h4&gt;自适应黎曼图神经网络(ARGNN)通过学习连续且各向异性的黎曼度量张量场，有效解决了图数据中复杂几何异质性的建模问题，为图神经网络提供了一种灵活且强大的表示学习方法，具有理论和实验上的优势。&lt;h4&gt;翻译&lt;/h4&gt;图数据通常表现出复杂的几何异质性，其中具有不同局部曲率的结构（如树状层次结构和密集社区）共存于单个网络中。现有的几何图神经网络将图嵌入到单一固定曲率流形或离散乘积空间中，难以捕捉这种多样性。我们引入了自适应黎曼图神经网络(ARGNN)，一种新颖的框架，它在图上学习连续且各向异性的黎曼度量张量场。它允许每个节点确定其最优局部几何，使模型能够流畅地适应图的结构景观。我们的核心创新是节点级度量张量的高效参数化，专门化为可学习的对角形式，在保持计算可处理性的同时捕捉方向几何信息。为确保几何规律性和稳定训练，我们集成了受里奇流启发的正则化，平滑学习到的流形。理论上，我们建立了ARGNN的严格几何演化收敛保证，并提供了统一先前固定或混合曲率GNNs的连续泛化。实验上，我们的方法在同类和异类基准数据集上表现出优越性能，能够自适应地捕捉多样化结构。此外，学习到的几何为底层图结构提供了可解释的见解，并 empirically corroborate 我们的理论分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph data often exhibits complex geometric heterogeneity, where structureswith varying local curvature, such as tree-like hierarchies and densecommunities, coexist within a single network. Existing geometric GNNs, whichembed graphs into single fixed-curvature manifolds or discrete product spaces,struggle to capture this diversity. We introduce Adaptive Riemannian GraphNeural Networks (ARGNN), a novel framework that learns a continuous andanisotropic Riemannian metric tensor field over the graph. It allows each nodeto determine its optimal local geometry, enabling the model to fluidly adapt tothe graph's structural landscape. Our core innovation is an efficientparameterization of the node-wise metric tensor, specializing to a learnablediagonal form that captures directional geometric information while maintainingcomputational tractability. To ensure geometric regularity and stable training,we integrate a Ricci flow-inspired regularization that smooths the learnedmanifold. Theoretically, we establish the rigorous geometric evolutionconvergence guarantee for ARGNN and provide a continuous generalization thatunifies prior fixed or mixed-curvature GNNs. Empirically, our methoddemonstrates superior performance on both homophilic and heterophilic benchmarkdatasets with the ability to capture diverse structures adaptively. Moreover,the learned geometries both offer interpretable insights into the underlyinggraph structure and empirically corroborate our theoretical analysis.</description>
      <author>example@mail.com (Xudong Wang, Tongxin Li, Chris Ding, Jicong Fan)</author>
      <guid isPermaLink="false">2508.02600v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction</title>
      <link>http://arxiv.org/abs/2508.02532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为上下文图Transformer (CGT)的混合神经架构，结合图神经网络和Transformer技术，专门用于处理技术、工程文档中的细粒度语法和实体关系问题。该模型在保持参数效率的同时，在检索增强生成管道中表现出色，准确率比GPT-2提高24.7%，参数减少62.4%。&lt;h4&gt;背景&lt;/h4&gt;标准基于Transformer的语言模型虽然对普通文本处理能力强大，但在处理复杂技术、工程文档中的细粒度语法和实体关系方面存在明显局限性。技术领域通常需要具有更强上下文感知和结构感知能力的专业语言模型，而非通用大型模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理技术文档中细粒度语法和实体关系的专业语言模型，同时保持参数效率，适用于特定领域的问答任务。&lt;h4&gt;方法&lt;/h4&gt;提出上下文图Transformer (CGT)，一种混合神经架构，使用顺序、skip-gram和语义相似性边在输入token上构建动态图，通过GATv2Conv层处理局部结构学习，然后将增强的嵌入传递给Transformer编码器以捕获全局依赖。模型采用两阶段训练方法：首先在普通文本上预训练，然后在特定领域的手册上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在检索增强生成(RAG)管道中集成后，CGT在准确率上比GPT-2高24.7%，同时参数减少了62.4%。这种提升来自于CGT能够联合建模结构token交互和长距离语义连贯性的能力。该模型能够很好地适应技术语言，实现更好的基础、实体跟踪和检索增强响应。&lt;h4&gt;结论&lt;/h4&gt;CGT为技术文档处理提供了一种参数高效的解决方案，通过结合图神经网络和Transformer的优势，能够有效捕获技术文档中的细粒度语法和实体关系，优于通用大型语言模型。&lt;h4&gt;翻译&lt;/h4&gt;标准基于Transformer的语言模型虽然对普通文本很强大，但在处理复杂技术、工程文档中的细粒度语法和实体关系方面往往存在困难。为解决这一问题，我们提出了上下文图Transformer (CGT)，这是一种混合神经架构，结合了图神经网络(GNNs)和Transformer，用于特定领域的问答。CGT使用顺序、skip-gram和语义相似性边在输入token上构建动态图，通过GATv2Conv层处理局部结构学习。然后，这些增强的嵌入被传递给Transformer编码器以捕获全局依赖。与通用大型模型不同，技术领域通常需要具有更强上下文感知和结构感知能力的专业语言模型。CGT为这类用例提供了参数高效的解决方案。集成到检索增强生成(RAG)管道中后，CGT优于GPT-2和BERT等基线模型，准确率比GPT-2高24.7%，同时参数减少62.4%。这种提升来自于CGT能够联合建模结构token交互和长距离语义连贯性的能力。该模型采用两阶段方法从头开始训练：首先在普通文本上预训练，然后在特定领域的手册上进行微调。这突显了CGT对技术语言的适应性，能够在实际应用中实现更好的基础、实体跟踪和检索增强响应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Standard transformer-based language models, while powerful for general text,often struggle with the fine-grained syntax and entity relationships in complextechnical, engineering documents. To address this, we propose the ContextualGraph Transformer (CGT), a hybrid neural architecture that combines GraphNeural Networks (GNNs) and Transformers for domain-specific question answering.CGT constructs a dynamic graph over input tokens using sequential, skip-gram,and semantic similarity edges, which is processed by GATv2Conv layers for localstructure learning. These enriched embeddings are then passed to a Transformerencoder to capture global dependencies. Unlike generic large models, technicaldomains often require specialized language models with strongercontextualization and structure awareness. CGT offers a parameter-efficientsolution for such use cases. Integrated into a Retrieval-Augmented Generation(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems fromCGTs ability to jointly model structural token interactions and long-rangesemantic coherence. The model is trained from scratch using a two-phaseapproach: pretraining on general text followed by fine-tuning ondomain-specific manuals. This highlights CGTs adaptability to technicallanguage, enabling better grounding, entity tracking, and retrieval-augmentedresponses in real-world applications.</description>
      <author>example@mail.com (Karan Reddy, Mayukha Pal)</author>
      <guid isPermaLink="false">2508.02532v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>On Effectiveness of Graph Neural Network Architectures for Network Digital Twins (NDTs)</title>
      <link>http://arxiv.org/abs/2508.02373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于人工智能的网络数字孪生（AI-NDT）解决方案，利用多层知识图谱架构和图神经网络来预测直接影响用户体验的网络指标。研究评估了四种图神经网络架构，发现GraphTransformer性能最佳，同时其他架构在需要较短训练时间的场景中也能提供可接受的结果。&lt;h4&gt;背景&lt;/h4&gt;未来网络如6G需要支持大量多样化的互联设备和应用，每种都有特定需求。传统网络管理方法已不足够，自动化解决方案变得必要。然而，网络自动化框架容易出错，通常采用需要训练的机器学习技术来优化网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于人工智能的网络数字孪生系统，用于在不影响真实网络和用户的情况下模拟、测试和训练AI模型，并预测直接影响用户体验的网络指标。&lt;h4&gt;方法&lt;/h4&gt;采用多层知识图谱架构和图神经网络技术，对四种最突出的图神经网络架构进行评估，并使用RIPE Atlas的公开可用测量数据训练数字孪生系统。&lt;h4&gt;主要发现&lt;/h4&gt;在评估的四种架构中，GraphTransformer表现最佳；在需要较短训练时间的场景中，其他架构可能更适合，同时也能提供可接受的结果；研究结果接近实际应用中的预期效果。&lt;h4&gt;结论&lt;/h4&gt;这项工作预示了主动网络管理可能成为常见实践，提供了一种可扩展且准确的解决方案，符合下一代网络的需求。&lt;h4&gt;翻译&lt;/h4&gt;未来的网络，如6G，将需要支持大量多样的互联设备和应用，每种都有其特定的需求。虽然传统的网络管理方法将足够，但自动化解决方案正变得必不可少。然而，网络自动化框架容易出错，并且通常采用基于机器学习的技术，这些技术需要训练来学习如何优化网络。从这个意义上说，网络数字孪生是一种有用的工具，它允许在不影响真实网络和用户的情况下模拟、测试和训练AI模型。本文提出了一种基于人工智能的网络数字孪生（AI-NDT），它利用多层知识图谱架构和图神经网络来预测直接影响用户体验的网络指标。对四种最突出的图神经网络架构进行了评估，以评估它们在开发网络数字孪生方面的有效性。我们在RIPE Atlas的公开可用测量数据上训练了数字孪生，因此获得了接近实际应用预期结果的结果。结果表明，在评估的四种架构中，GraphTransformer表现最佳。然而，在较短训练时间更重要的场景中，其他架构可能更适合，同时也能提供可接受的结果。这项工作的结果表明，主动网络管理可能成为常见做法，提供了一种符合下一代网络需求的可扩展且准确的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Future networks, such as 6G, will need to support a vast and diverse range ofinterconnected devices and applications, each with its own set of requirements.While traditional network management approaches will suffice, an automatedsolutions are becoming a must. However, network automation frameworks are proneto errors, and often they employ ML-based techniques that require training tolearn how the network can be optimized. In this sense, network digital twinsare a useful tool that allows for the simulation, testing, and training of AImodels without affecting the real-world networks and users. This paper presentsan AI-based Network Digital Twin (AI-NDT) that leverages a multi-layeredknowledge graph architecture and graph neural networks to predict networkmetrics that directly affect the quality of experience of users. An evaluationof the four most prominent Graph Neural Networks (GNN) architectures wasconducted to assess their effectiveness in developing network digital twins. Wetrained the digital twin on publicly available measurement data from RIPEAtlas, therefore obtaining results close to what is expected in real-worldapplications. The results show that among the four architectures evaluated,GraphTransformer presents the best performance. However, other architecturesmight fit better in scenarios where shorter training time is important, whilealso delivering acceptable results. The results of this work are indicative ofwhat might become common practice for proactive network management, offering ascalable and accurate solution aligned with the requirements of thenext-generation networks.</description>
      <author>example@mail.com (Iulisloi Zacarias, Oussama Ben Taarit, Admela Jukan)</author>
      <guid isPermaLink="false">2508.02373v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Skeleton-Guided Learning for Shortest Path Search</title>
      <link>http://arxiv.org/abs/2508.02270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的基于学习的框架，用于在通用图上进行最短路径搜索，无需特定领域特征。该框架通过构建骨架图和骨架图神经网络(SGNN)来学习节点嵌入并预测距离，结合LSearch算法实现高效搜索，并通过分层训练策略扩展到大型图，形成HLSearch方法。&lt;h4&gt;背景&lt;/h4&gt;最短路径搜索是图应用的核心操作，但现有方法存在局限性：经典算法(如Dijkstra和A*)在复杂图中效率低下；基于索引的技术需要大量预处理和存储；基于学习的方法通常局限于空间图并依赖特定领域特征，限制了通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的基于学习的框架，用于在通用图上进行最短路径搜索，不依赖特定领域特征。&lt;h4&gt;方法&lt;/h4&gt;构建骨架图捕获多级距离和跳数信息；使用骨架图神经网络(SGNN)学习节点嵌入并预测节点对间的距离和跳长；开发LSearch算法利用模型驱动剪枝减少搜索空间；引入分层训练策略处理大图，形成HLSearch方法在图分区间进行高效搜索。&lt;h4&gt;主要发现&lt;/h4&gt;在五个不同真实图上的实验表明，该框架在各种图类型上实现了强大的性能，为基于学习的最短路径搜索提供了灵活且有效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;该框架为基于学习的最短路径搜索提供了灵活且有效的解决方案，适用于多种图类型。&lt;h4&gt;翻译&lt;/h4&gt;最短路径搜索是基于图的应用中的核心操作，然而现有方法面临重要的局限性。经典算法如Dijkstra和A*随着图变得更复杂而变得效率低下，而基于索引的技术通常需要大量的预处理和存储。最近的基于学习的方法通常专注于空间图，并依赖于特定于上下文的特征如地理坐标，限制了它们的通用适用性。我们提出了一种通用的基于学习的框架，用于在通用图上进行最短路径搜索，不需要特定领域的特征。我们方法的核心是构建一个骨架图，以紧凑的形式捕获多级距离和跳数信息。一个骨架图神经网络(SGNN)在该结构上操作，学习节点嵌入并预测节点对之间的距离和跳长。这些预测支持LSearch，一种使用模型驱动的剪枝来减少搜索空间同时保持准确性的引导搜索算法。为了处理更大的图，我们引入了一种分层训练策略，将图划分为具有单独训练的SGNN的子图。这种结构支持HLSearch，我们方法的扩展，用于在图分区之间进行高效的路径搜索。在五个不同的真实图上的实验表明，我们的框架在图类型方面实现了强大的性能，为基于学习的最短路径搜索提供了一个灵活且有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shortest path search is a core operation in graph-based applications, yetexisting methods face important limitations. Classical algorithms such asDijkstra's and A* become inefficient as graphs grow more complex, whileindex-based techniques often require substantial preprocessing and storage.Recent learning-based approaches typically focus on spatial graphs and rely oncontext-specific features like geographic coordinates, limiting their generalapplicability. We propose a versatile learning-based framework for shortestpath search on generic graphs, without requiring domain-specific features. Atthe core of our approach is the construction of a skeleton graph that capturesmulti-level distance and hop information in a compact form. A Skeleton GraphNeural Network (SGNN) operates on this structure to learn node embeddings andpredict distances and hop lengths between node pairs. These predictions supportLSearch, a guided search algorithm that uses model-driven pruning to reduce thesearch space while preserving accuracy. To handle larger graphs, we introduce ahierarchical training strategy that partitions the graph into subgraphs withindividually trained SGNNs. This structure enables HLSearch, an extension ofour method for efficient path search across graph partitions. Experiments onfive diverse real-world graphs demonstrate that our framework achieves strongperformance across graph types, offering a flexible and effective solution forlearning-based shortest path search.</description>
      <author>example@mail.com (Tiantian Liu, Xiao Li, Huan Li, Hua Lu, Christian S. Jensen, Jianliang Xu)</author>
      <guid isPermaLink="false">2508.02270v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration</title>
      <link>http://arxiv.org/abs/2508.02069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了一种新的脉冲神经网络架构，首次将图结构学习与基于脉冲的时间处理无缝集成，用于多元时间序列预测，显著提高了模型在长期序列数据集上的准确性。&lt;h4&gt;背景&lt;/h4&gt;脉冲神经网络(SNNs)受生物神经元脉冲行为启发，是捕捉时间数据复杂性的独特方法，但其多元时间序列预测中的空间建模潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;填补SNNs在多元时间序列预测中空间建模研究的空白，引入一种新的SNN架构，将图结构学习与基于脉冲的时间处理相结合。&lt;h4&gt;方法&lt;/h4&gt;1) 嵌入时间特征和自适应矩阵，消除预定义图结构需求；2) 通过观察(OBS)块学习序列特征；3) 利用脉冲SAGE层分层聚合邻域信息，实现多跳特征提取且无需浮点运算；4) 提出双路径脉冲融合(DSF)块，通过脉冲门控机制整合空间图特征和时间动态，结合LSTM处理序列与脉冲自注意力输出。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该模型在所有数据集上超越最先进的基于SNN的iSpikformer，在长期预测中优于传统时间模型，为高效时空建模建立了新范式。&lt;h4&gt;结论&lt;/h4&gt;该模型为多元时间序列预测中的空间建模提供了新方法，通过脉冲神经网络和图结构学习的结合，有效提高了长期序列数据集的模型准确性。&lt;h4&gt;翻译&lt;/h4&gt;脉冲神经网络(SNNs)受生物神经元的脉冲行为启发，为捕捉时间数据的复杂性提供了一种独特方法。然而，SNNs在多元时间序列预测中的空间建模潜力在很大程度上仍未被探索。为了弥补这一空白，我们引入了一种全新的SNN架构，这是首批将图结构学习与基于脉冲的时间处理无缝集成用于多元时间序列预测的架构之一。具体来说，我们首先嵌入时间特征和一个自适应矩阵，消除了对预定义图结构的需求。然后，我们通过观察(OBS)块进一步学习序列特征。在此基础上，我们的多尺度脉冲聚合(MSSA)通过脉冲SAGE层分层聚合邻域信息，实现多跳特征提取，同时消除了对浮点运算的需求。最后，我们提出双路径脉冲融合(DSF)块，通过脉冲门控机制整合空间图特征和时间动态，将LSTM处理的序列与脉冲自注意力输出相结合，有效提高了长期序列数据集的模型准确性。实验表明，我们的模型在所有数据集上都超越了最先进的基于SNN的iSpikformer，并在长期预测中优于传统时间模型，从而为高效的时空建模建立了新范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking neural networks (SNNs), inspired by the spiking behavior ofbiological neurons, offer a distinctive approach for capturing the complexitiesof temporal data. However, their potential for spatial modeling in multivariatetime-series forecasting remains largely unexplored. To bridge this gap, weintroduce a brand new SNN architecture, which is among the first to seamlesslyintegrate graph structural learning with spike-based temporal processing formultivariate time-series forecasting. Specifically, we first embed timefeatures and an adaptive matrix, eliminating the need for predefined graphstructures. We then further learn sequence features through the Observation(OBS) Block. Building upon this, our Multi-Scale Spike Aggregation (MSSA)hierarchically aggregates neighborhood information through spiking SAGE layers,enabling multi-hop feature extraction while eliminating the need forfloating-point operations. Finally, we propose a Dual-Path Spike Fusion (DSF)Block to integrate spatial graph features and temporal dynamics via aspike-gated mechanism, combining LSTM-processed sequences with spikingself-attention outputs, effectively improve the model accuracy of long sequencedatasets. Experiments show that our model surpasses the state-of-the-artSNN-based iSpikformer on all datasets and outperforms traditional temporalmodels at long horizons, thereby establishing a new paradigm for efficientspatial-temporal modeling.</description>
      <author>example@mail.com (Bang Hu, Changze Lv, Mingjie Li, Yunpeng Liu, Xiaoqing Zheng, Fengzhe Zhang, Wei cao, Fan Zhang)</author>
      <guid isPermaLink="false">2508.02069v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph Unlearning via Embedding Reconstruction -- A Range-Null Space Decomposition Approach</title>
      <link>http://arxiv.org/abs/2508.02044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的图神经网络节点遗忘方法，通过嵌入重建和范围-零空间分解实现高效节点遗忘，实验证明该方法达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;图遗忘是专门为图神经网络设计的，用于处理广泛且多样的图结构遗忘请求，但这一领域在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;避免重新训练的开销，并实现遗忘的模型效用，解决现有方法在节点遗忘方面的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过嵌入重建来反转GNN中的聚合过程，并采用范围-零空间分解进行节点交互学习。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性数据集上的实验结果证明了所提出方法的SOTA性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的节点遗忘方法有效解决了现有方法在处理节点遗忘时的局限性，达到了最先进的性能水平。&lt;h4&gt;翻译&lt;/h4&gt;图遗忘专门针对图神经网络设计，用于处理广泛且多样的图结构遗忘请求，但这一领域在很大程度上尚未被探索。GIF（图影响函数）在部分边遗忘方面有效，但在处理更具挑战性的节点遗忘时面临挑战。为了避免重新训练的开销并实现遗忘的模型效用，我们提出了一种新颖的节点遗忘方法，通过嵌入重建来反转GNN中的聚合过程，并采用范围-零空间分解进行节点交互学习。在多个代表性数据集上的实验结果证明了我们提出方法的SOTA性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph unlearning is tailored for GNNs to handle widespread and various graphstructure unlearning requests, which remain largely unexplored. The GIF (graphinfluence function) achieves validity under partial edge unlearning, but faceschallenges in dealing with more disturbing node unlearning. To avoid theoverhead of retraining and realize the model utility of unlearning, we proposeda novel node unlearning method to reverse the process of aggregation in GNN byembedding reconstruction and to adopt Range-Null Space Decomposition for thenodes' interaction learning. Experimental results on multiple representativedatasets demonstrate the SOTA performance of our proposed approach.</description>
      <author>example@mail.com (Hang Yin, Zipeng Liu, Xiaoyong Peng, Liyao Xiang)</author>
      <guid isPermaLink="false">2508.02044v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Aware GNN for Transmission Network Reconfiguration via Substation Breaker Optimization</title>
      <link>http://arxiv.org/abs/2508.01951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OptiGridML，一个用于电力网格离散拓扑优化的机器学习框架，通过两阶段神经网络架构替代传统混合整数规划方法，实现了电力出口优化和计算效率的大幅提升。&lt;h4&gt;背景&lt;/h4&gt;电力网格拓扑优化涉及选择变电站断路器配置以最大化跨区域电力出口，传统方法将其表述为混合整数规划问题，但对于大型网络来说，这类问题是NP-hard且计算上难以处理的。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的机器学习方法来解决电力网格离散拓扑优化问题，克服传统混合整数规划方法在大规模网络中的计算瓶颈。&lt;h4&gt;方法&lt;/h4&gt;OptiGridML采用两阶段神经网络架构：1)线图神经网络(LGNN)近似给定网络拓扑的直流电力流；2)异构GNN(HeteroGNN)在结构和物理约束下预测断路器状态；并通过物理信息一致性损失连接这些组件，强制执行基尔霍夫定律。&lt;h4&gt;主要发现&lt;/h4&gt;在具有多达1,000个断路器的合成网络上，OptiGridML实现了比基线拓扑高18%的电力出口改进，同时将推理时间从小时级别减少到毫秒级别。&lt;h4&gt;结论&lt;/h4&gt;结构化、感知流的图神经网络在加速物理网络系统中的组合优化方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了OptiGridML，一个用于电力网格离散拓扑优化的机器学习框架。该任务涉及选择变电站断路器配置以最大化跨区域电力出口，这通常被表述为混合整数规划问题，对于大型网络来说是NP-hard且计算上难以处理的。OptiGridML用两阶段神经网络架构替代了重复的MIP求解：一个近似给定网络拓扑直流电力流的线图神经网络(LGNN)，和一个在结构和物理约束下预测断路器状态的异构GNN(HeteroGNN)。物理信息一致性损失通过在预测流上强制执行基尔霍夫定律连接这些组件。在具有多达1,000个断路器的合成网络上的实验表明，OptiGridML实现了比基线拓扑高18%的电力出口改进，同时将推理时间从小时减少到毫秒。这些结果展示了结构化、感知流的GNN在加速物理网络系统中的组合优化的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces OptiGridML, a machine learning framework for discretetopology optimization in power grids. The task involves selecting substationbreaker configurations that maximize cross-region power exports, a problemtypically formulated as a mixed-integer program (MIP) that is NP-hard andcomputationally intractable for large networks. OptiGridML replaces repeatedMIP solves with a two-stage neural architecture: a line-graph neural network(LGNN) that approximates DC power flows for a given network topology, and aheterogeneous GNN (HeteroGNN) that predicts breaker states under structural andphysical constraints. A physics-informed consistency loss connects thesecomponents by enforcing Kirchhoff's law on predicted flows. Experiments onsynthetic networks with up to 1,000 breakers show that OptiGridML achievespower export improvements of up to 18% over baseline topologies, while reducinginference time from hours to milliseconds. These results demonstrate thepotential of structured, flow-aware GNNs for accelerating combinatorialoptimization in physical networked systems.</description>
      <author>example@mail.com (Dekang Meng, Rabab Haider, Pascal van Hentenryck)</author>
      <guid isPermaLink="false">2508.01951v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation</title>
      <link>http://arxiv.org/abs/2508.01925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的迭代解释框架，用于解决图神经网络(GNNs)预测解释中的挑战。通过将模型训练数据分布与解释过程中的加权图分布对齐，该方法提高了解释的鲁棒性，并在多个基准数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在多种图相关学习任务中表现出色，但解释其预测仍然是一个挑战。现有方法主要在加权图上优化软边缘掩码来突出重要子结构，但这些图与GNN训练时使用的无权图存在差异，导致分布偏移，产生不可靠的梯度和低质量的解释，特别是在生成小型稀疏子图时。&lt;h4&gt;目的&lt;/h4&gt;解决训练图和解释图之间的分布不匹配问题，提高GNN预测解释的鲁棒性和质量，同时保持方法的灵活性和通用性。&lt;h4&gt;方法&lt;/h4&gt;提出一种迭代解释框架，交替进行两个阶段：解释子图识别和模型适应。从较大的解释子图开始，在该子图上进行软掩码优化是可靠的。基于此子图，为解释性和非解释性边分配重要性感知的边权重，并在这些加权图上重新训练GNN。这个过程使用逐渐变小的子图重复进行，形成迭代细化过程。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个基准数据集上使用不同的GNN骨干网络和解释方法时，一致提高了解释质量。该方法可以灵活地集成到不同的架构中，证明了其通用性和有效性。&lt;h4&gt;结论&lt;/h4&gt;通过迭代对齐训练数据分布和解释过程中的加权图分布，该方法有效解决了GNN解释中的分布偏移问题，提高了解释的鲁棒性和质量，为GNN的解释提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在广泛的图相关学习任务中取得了显著性能。然而，解释它们的预测仍然是一个具有挑战性的问题，特别是在训练期间使用的图与解释过程中遇到的图之间存在不匹配的情况下。大多数现有方法在加权图上优化软边缘掩码以突出重要子结构，但这些图与GNN训练所用的无权图不同。这种分布偏移导致不可靠的梯度和退化的解释质量，特别是在生成小型稀疏子图时。为了解决这个问题，我们提出了一种新颖的迭代解释框架，通过将模型的训练数据分布与解释过程中出现的加权图分布对齐来提高解释的鲁棒性。我们的方法在两个阶段之间交替：解释子图识别和模型适应。它从一个相对较大的解释子图开始，在该子图上软掩码优化是可靠的。基于此子图，我们为解释性和非解释性边分配重要性感知的边权重，并在这些加权图上重新训练GNN。这个过程使用逐渐变小的子图重复进行，形成迭代细化过程。我们使用不同的GNN骨干网络和解释方法在多个基准数据集上评估了我们的方法。实验结果表明，我们的方法一致提高了解释质量，并且可以灵活地集成到不同的架构中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved remarkable performance in a widerange of graph-related learning tasks. However, explaining their predictionsremains a challenging problem, especially due to the mismatch between thegraphs used during training and those encountered during explanation. Mostexisting methods optimize soft edge masks on weighted graphs to highlightimportant substructures, but these graphs differ from the unweighted graphs onwhich GNNs are trained. This distributional shift leads to unreliable gradientsand degraded explanation quality, especially when generating small, sparsesubgraphs. To address this issue, we propose a novel iterative explanationframework which improves explanation robustness by aligning the model'straining data distribution with the weighted graph distribution appeared duringexplanation. Our method alternates between two phases: explanation subgraphidentification and model adaptation. It begins with a relatively largeexplanation subgraph where soft mask optimization is reliable. Based on thissubgraph, we assign importance-aware edge weights to explanatory andnon-explanatory edges, and retrain the GNN on these weighted graphs. Thisprocess is repeated with progressively smaller subgraphs, forming an iterativerefinement procedure. We evaluate our method on multiple benchmark datasetsusing different GNN backbones and explanation methods. Experimental resultsshow that our method consistently improves explanation quality and can beflexibly integrated with different architectures.</description>
      <author>example@mail.com (Zhuomin Chen, Jingchao Ni, Hojat Allah Salehi, Xu Zheng, Dongsheng Luo)</author>
      <guid isPermaLink="false">2508.01925v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Pi-SAGE: Permutation-invariant surface-aware graph encoder for binding affinity prediction</title>
      <link>http://arxiv.org/abs/2508.01924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Pi-SAGE，一种新型的置换不变表面感知图编码器，通过明确结合蛋白质表面的局部、上下文感知的化学性质，提高了全原子图神经网络在预测蛋白质结合亲和力变化方面的能力。&lt;h4&gt;背景&lt;/h4&gt;当前预测蛋白质结合亲和力变化的最先进模型（如GearBind）都是基于蛋白质结构衍生的全原子几何模型。虽然表面性质可以从蛋白质结构中隐式学习，但明确了解蛋白质表面可能改进预测能力。&lt;h4&gt;目的&lt;/h4&gt;引入Pi-SAGE模型，通过明确利用蛋白质表面特征，提高预测蛋白质复合物之间结合亲和力变化的准确性。&lt;h4&gt;方法&lt;/h4&gt;首先训练Pi-SAGE从蛋白质结构创建表面码本并为表面残基分配标记；然后使用领域自适应的Pi-SAGE表面特征增强GearBind模型的节点特征，在SKEMPI数据集上进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;明确结合残基的局部、上下文感知的化学性质，显著增强了全原子图神经网络在建模野生型和突变体蛋白质间结合亲和力变化方面的预测能力。&lt;h4&gt;结论&lt;/h4&gt;通过明确整合蛋白质表面的局部、上下文感知化学性质，可以有效改进基于结构的模型预测蛋白质结合亲和力变化的能力。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质表面指纹编码了控制蛋白质相互作用的化学和几何特征，可用于预测两个蛋白质复合物之间结合亲和力的变化。当前预测结合亲和力变化的最先进模型（如GearBind）都是基于蛋白质结构衍生的全原子几何模型。虽然表面性质可以从蛋白质结构中隐式学习，但我们假设明确了解蛋白质表面可以改进基于结构的模型预测结合亲和力变化的能力。为此，我们引入了Pi-SAGE，一种新型的置换不变表面感知图编码器。我们首先训练Pi-SAGE直接从结构创建蛋白质表面码本，并为每个表面暴露的残基分配一个标记。接下来，我们使用领域自适应的Pi-SAGE中的表面特征增强GearBind模型的节点特征，以在SKEMPI数据集上预测结合亲和力的变化。我们表明，明确地结合残基的局部、上下文感知的化学性质增强了全原子图神经网络在建模野生型和突变体蛋白质之间结合亲和力变化方面的预测能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein surface fingerprint encodes chemical and geometric features thatgovern protein-protein interactions and can be used to predict changes inbinding affinity between two protein complexes. Current state-of-the-art modelsfor predicting binding affinity change, such as GearBind, are all-atom basedgeometric models derived from protein structures. Although surface propertiescan be implicitly learned from the protein structure, we hypothesize thatexplicit knowledge of protein surfaces can improve a structure-based model'sability to predict changes in binding affinity. To this end, we introducePi-SAGE, a novel Permutation-Invariant Surface-Aware Graph Encoder. We firsttrain Pi-SAGE to create a protein surface codebook directly from the structureand assign a token for each surface-exposed residue. Next, we augment the nodefeatures of the GearBind model with surface features from domain-adaptedPi-SAGE to predict binding affinity change on the SKEMPI dataset. We show thatexplicitly incorporating local, context-aware chemical properties of residuesenhances the predictive power of all-atom graph neural networks in modelingbinding affinity changes between wild-type and mutant proteins.</description>
      <author>example@mail.com (Sharmi Banerjee, Mostafa Karimi, Melih Yilmaz, Tommi Jaakkola, Bella Dubrov, Shang Shang, Ron Benson)</author>
      <guid isPermaLink="false">2508.01924v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Unified System Representations for Microservice Tail Latency Prediction</title>
      <link>http://arxiv.org/abs/2508.01635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为USRFNet的深度学习网络，用于解决微服务架构中性能监控和资源管理的挑战。该网络通过分离和建模流量端与资源端特征，结合图神经网络和gMLP模块，实现了对窗口级P95延迟的高精度预测，在真实微服务基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;微服务架构已成为构建可扩展云原生应用的标准，但其分布式特性在性能监控和资源管理方面带来了重大挑战。传统方法依赖每个请求的延迟指标，这些指标对瞬时噪声高度敏感，无法反映复杂并发工作负载的整体行为。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在处理异构数据和整合互补模态方面的不足，开发一种能够准确预测窗口级P95延迟的系统，以捕捉系统整体趋势和用户感知的性能退化。&lt;h4&gt;方法&lt;/h4&gt;提出USRFNet，一种深度学习网络，使用图神经网络(GNNs)捕获服务交互和请求传播模式，同时使用gMLP模块独立建模集群资源动态。这些表示被融合为统一的系统嵌入，用于预测窗口级P95延迟。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法在处理异构数据（流量端特征传播和资源端局部瓶颈）和整合互补模态方面存在不足。USRFNet通过明确分离和建模流量端与资源端特征，实现了比现有基线更准确的预测性能。&lt;h4&gt;结论&lt;/h4&gt;USRFNet有效解决了微服务架构中性能监控的挑战，在大规模压力测试条件下展现出显著的预测准确性提升，为复杂分布式系统的性能优化提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;微服务架构已成为构建可扩展云原生应用的事实标准，但其分布式特性在性能监控和资源管理方面带来了重大挑战。传统方法通常依赖每个请求的延迟指标，这些指标对瞬时噪声高度敏感，无法反映复杂并发工作负载的整体行为。相比之下，窗口级P95尾部延迟提供了稳定且有意义的信号，既能捕捉系统级趋势，又能反映用户感知的性能退化。我们确定了现有方法的两个关键缺陷：(i) 对异构数据处理不足，其中流量端特征在服务依赖关系间传播，资源端信号反映局部瓶颈；(ii) 缺乏有原则的架构设计，无法有效区分和整合这些互补模态。为解决这些挑战，我们提出了USRFNet，一种深度学习网络，明确分离并建模流量端和资源端特征。USRFNet使用图神经网络捕获服务交互和请求传播模式，同时使用gMLP模块独立建模集群资源动态。这些表示随后被融合为统一的系统嵌入，以高精度预测窗口级P95延迟。我们在大规模压力测试条件下的真实微服务基准上评估了USRFNet，展示了与最先进基线相比预测准确性的显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Microservice architectures have become the de facto standard for buildingscalable cloud-native applications, yet their distributed nature introducessignificant challenges in performance monitoring and resource management.Traditional approaches often rely on per-request latency metrics, which arehighly sensitive to transient noise and fail to reflect the holistic behaviorof complex, concurrent workloads. In contrast, window-level P95 tail latencyprovides a stable and meaningful signal that captures both system-wide trendsand user-perceived performance degradation. We identify two key shortcomings inexisting methods: (i) inadequate handling of heterogeneous data, wheretraffic-side features propagate across service dependencies and resource-sidesignals reflect localized bottlenecks, and (ii) the lack of principledarchitectural designs that effectively distinguish and integrate thesecomplementary modalities. To address these challenges, we propose USRFNet, adeep learning network that explicitly separates and models traffic-side andresource-side features. USRFNet employs GNNs to capture service interactionsand request propagation patterns, while gMLP modules independently modelcluster resource dynamics. These representations are then fused into a unifiedsystem embedding to predict window-level P95 latency with high accuracy. Weevaluate USRFNet on real-world microservice benchmarks under large-scale stresstesting conditions, demonstrating substantial improvements in predictionaccuracy over state-of-the-art baselines.</description>
      <author>example@mail.com (Wenzhuo Qian, Hailiang Zhao, Tianlv Chen, Jiayi Chen, Ziqi Wang, Kingsum Chow, Shuiguang Deng)</author>
      <guid isPermaLink="false">2508.01635v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for the Elementary Shortest Path Problem</title>
      <link>http://arxiv.org/abs/2508.01557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于概率方法和无监督图神经网络的解决方案，用于解决存在负成本环时的基本最短路径问题(ESPP)，该方法在实验中表现出色并具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;基本最短路径问题(ESPP)旨在寻找从起点s到终点t的最小成本路径，且每个顶点最多访问一次。当图中存在负成本环时，该问题变为NP难问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效求解存在负成本环的ESPP问题的方法，特别是寻找近似最优解。&lt;h4&gt;方法&lt;/h4&gt;提出一种概率方法，使用无监督图神经网络联合学习节点价值估计和边选择概率，通过代理损失函数实现，该损失函数可减少负成本环并保证算法一致性。在推理阶段，使用解码算法将学习到的边概率转换为基本路径。&lt;h4&gt;主要发现&lt;/h4&gt;在最多100个节点的图上进行的实验表明，该方法优于无监督基线和经典启发式算法，并且在未见过的合成图上跨尺寸和跨拓扑泛化方面表现出高性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于图神经网络的概率方法为解决NP难的基本最短路径问题提供了一种有效途径，特别是在处理负成本环时，具有良好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;基本最短路径问题(ESPP)寻求从s到t的最小成本路径，且每个顶点最多访问一次。负成本环的存在使问题变得NP难。我们提出了一种寻找近似最优ESPP的概率方法，该方法通过一个无监督图神经网络实现，该网络通过代理损失函数联合学习节点价值估计和边选择概率。该损失函数通过同时减少负成本环和嵌入所需的算法一致性，为找到近似最优ESPP解提供了高概率保证。在推理时，解码算法将学习到的边概率转换为基本路径。在最多100个节点的图上的实验表明，所提出的方法优于无监督基线和经典启发式算法，并且在未见过的合成图上跨尺寸和跨拓扑泛化方面表现出高性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Elementary Shortest-Path Problem(ESPP) seeks a minimum cost path from sto t that visits each vertex at most once. The presence of negative-cost cyclesrenders the problem NP-hard. We present a probabilistic method for findingnear-optimal ESPP, enabled by an unsupervised graph neural network that jointlylearns node value estimates and edge-selection probabilities via a surrogateloss function. The loss provides a high probability certificate of findingnear-optimal ESPP solutions by simultaneously reducing negative-cost cycles andembedding the desired algorithmic alignment. At inference time, a decodingalgorithm transforms the learned edge probabilities into an elementary path.Experiments on graphs of up to 100 nodes show that the proposed methodsurpasses both unsupervised baselines and classical heuristics, whileexhibiting high performance in cross-size and cross-topology generalization onunseen synthetic graphs.</description>
      <author>example@mail.com (Jingyi Chen, Xinyuan Zhang, Xinwu Qian)</author>
      <guid isPermaLink="false">2508.01557v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Sensitivity to $HH\to b\bar{b} γγ$ with Graph Neural Networks and XGBoost</title>
      <link>http://arxiv.org/abs/2508.01449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用先进机器学习技术提高双希格斯玻色子搜索的灵敏度，比较了XGBoost和GNN两种模型，发现GNN模型显著提高了双希格斯玻色子产生截面的上限约束。&lt;h4&gt;背景&lt;/h4&gt;在希格斯玻色子物理研究中，双希格斯玻色子搜索对于理解希格斯自耦合至关重要，但传统方法在HH → b̄bγγ衰变通道中的灵敏度有限。&lt;h4&gt;目的&lt;/h4&gt;提高双希格斯玻色子搜索的灵敏度，特别是在HH → b̄bγγ衰变通道中，以改善对双希格斯玻色子产生截面上限的约束和对希格斯玻色子自耦合(κλ)的测量精度。&lt;h4&gt;方法&lt;/h4&gt;实现了两种机器学习模型：基于树的XGBoost分类器和基于几何的图神经网络分类器(GNN)，并在√s = 13.6 TeV条件下进行性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;几何模型(GNN)优于传统的XGBoost分类器，将双希格斯玻色子产生截面的95%置信水平上限提高了28%，与最新的ATLAS实验结果相比，在上限和希格斯玻色子自耦合(κλ)约束方面均有显著改进。&lt;h4&gt;结论&lt;/h4&gt;基于几何的图神经网络模型在双希格斯玻色子搜索中表现优异，能有效提高实验数据的分析灵敏度，为粒子物理研究提供更精确的测量结果。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们探讨了使用先进的机器学习技术来提高在13.6 TeV条件下，双希格斯玻色子在HH → b̄bγγ衰变通道中的搜索灵敏度。实现了并比较了两种机器学习模型：使用XGBoost的基于树的分类器和使用几何方法的图神经网络分类器。我们证明几何模型优于传统的XGBoost分类器，将双希格斯玻色子产生截面的95%置信水平上限提高了28%。我们的结果与最新的ATLAS实验结果进行了比较，显示出在上限和希格斯玻色子自耦合约束方面的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore the use of advanced machine learning (ML)techniques to enhance the sensitivity of double Higgs boson searches in the \(HH \to b\bar{b}\gamma\gamma \) decay channel at $\sqrt{s} = $ 13.6 TeV. Two MLmodels are implemented and compared: a tree-based classifier using XGBoost, anda geometrical-based graph neural network classifier (GNN). We show that thegeometrical model outperform the traditional XGBoost classifier improving theexpected 95\% CL upper limit on the double Higgs boson production cross-sectionby 28\%. Our results are compared to the latest ATLAS experiment results,showing significant improvement of both upper limit and Higgs bosonself-coupling ($\kappa_{\lambda}$) constraints.</description>
      <author>example@mail.com (Mohamed Belfkir, Mohamed Amin Loualidi, Salah Nasri)</author>
      <guid isPermaLink="false">2508.01449v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A graph neural network based on feature network for identifying influential nodes</title>
      <link>http://arxiv.org/abs/2508.01278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FNGCN的新型图卷积网络框架，用于解决复杂网络中重要节点识别问题，通过特征网络表示局部中心性间关系，实验证明其性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;识别复杂网络中的重要节点非常重要，在电子商务、计算机信息系统等领域有广泛应用。现有方法要么只考虑网络结构的一个方面，要么使用计算成本高的全局中心性作为节点特征，且没有考虑不同中心性之间的关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一种更准确识别复杂网络中重要节点的框架。&lt;h4&gt;方法&lt;/h4&gt;提出基于特征网络的图卷积网络框架FNGCN，利用特征网络表示局部中心性间的复杂关系，确定最适合的局部中心性。开发了浅层和深层两种FNGCN模型，使用SIR模型获得的真实数据在多个真实网络中与最先进方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，两种FNGCN模型比比较的方法能更准确地识别重要节点，证明所提框架在识别复杂网络重要节点方面有效。&lt;h4&gt;结论&lt;/h4&gt;FNGCN框架通过考虑局部中心性之间的关系，解决了现有方法的局限性，能够更准确地识别复杂网络中的重要节点。&lt;h4&gt;翻译&lt;/h4&gt;识别复杂网络中的重要节点非常重要，并且在实践中有很多应用。例如，在电子商务网络中找到重要节点可以为商家提供购买意愿强的客户；在计算机信息系统中识别重要节点可以帮助定位导致系统崩溃的组件；识别这些网络中的重要节点可以加速网络中的信息流动。因此，人们已经做了大量努力来解决识别重要节点的问题。然而，之前的研究要么只考虑网络结构的一个方面，要么使用计算成本高的全局中心性作为节点特征来识别重要节点，且现有方法没有考虑不同中心性之间的关系。为了解决这些问题，我们提出了一个基于特征网络的图卷积网络框架，简称为FNGCN。此外，为了排除噪声和减少冗余，FNGCN利用特征网络来表示局部中心性之间的复杂关系，基于此确定最适合的局部中心性。通过将浅层GCN和深层GCN纳入FNGCN框架，开发了两种FNGCN。使用广泛使用的易感染-感染-恢复模型获得的真实数据，在几个真实网络中将这两种FNGCN与最先进的方法进行比较。实验结果表明，这两种FNGCN比比较的方法能更准确地识别重要节点，这表明所提出的框架在识别复杂网络中的重要节点方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying influential nodes in complex networks is of great importance, andhas many applications in practice. For example, finding influential nodes ine-commerce network can provide merchants with customers with strong purchaseintent; identifying influential nodes in computer information system can helplocating the components that cause the system break down and identifyinginfluential nodes in these networks can accelerate the flow of information innetworks. Thus, a lot of efforts have been made on the problem of indentifyinginfluential nodes. However, previous efforts either consider only one aspect ofthe network structure, or using global centralities with high time consuming asnode features to identify influential nodes, and the existing methods do notconsider the relationships between different centralities. To solve theseproblems, we propose a Graph Convolutional Network Framework based on FeatureNetwork, abbreviated as FNGCN (graph convolutional network is abbreviated asGCN in the following text). Further, to exclude noises and reduce redundency,FNGCN utilizes feature network to represent the complicated relationships amongthe local centralities, based on which the most suitable local centralities aredetermined. By taking a shallow GCN and a deep GCN into the FNGCN framework,two FNGCNs are developed. With ground truth obtained from the widely usedSusceptible Infected Recovered (SIR) model, the two FNGCNs are compared withthe state-of-art methods on several real-world networks. Experimental resultsshow that the two FNGCNs can identify the influential nodes more accuratelythan the compared methods, indicating that the proposed framework is effectivein identifying influential nodes in complex networks.</description>
      <author>example@mail.com (Yanmei Hu, Siyuan Yin, Yihang Wu, Xue Yue, Yue Liu)</author>
      <guid isPermaLink="false">2508.01278v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation</title>
      <link>http://arxiv.org/abs/2508.01240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 14 figures, paper accepted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种新的空间插值流程，结合图神经网络和特殊的编码技术，能够产生可靠的结果并有效传达不确定性信息。&lt;h4&gt;背景&lt;/h4&gt;准确可靠地可视化时空传感器数据（如环境参数和气象条件）对于决策制定至关重要。然而，传统空间插值方法由于传感器覆盖有限且不规则，往往无法产生可靠的插值结果。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的空间插值流程，实现可靠的插值结果，并生成一种包含不确定性信息的新型热图表示。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络(GNNs)的插值参考数据提高可视化可靠性和时间分辨率；集成主邻域聚合(PNA)和地理位置编码(GPE)使模型有效学习时空依赖关系；提出一种基于插值热图的外部静态可视化技术，有效传达插值图中各种来源的不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列用例、真实世界数据集的广泛评估和用户研究，证明了模型在数据插值方面的优越性能；参考数据改善了插值结果；可视化设计在传达不确定性方面有效。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效解决传统空间插值方法的局限性，提供更可靠的数据可视化和不确定性表达。&lt;h4&gt;翻译&lt;/h4&gt;准确可靠地可视化时空传感器数据（如环境参数和气象条件）对于明智的决策制定至关重要。然而，由于传感器覆盖有限且不规则，传统的空间插值方法往往无法产生可靠的插值结果。本文介绍了一种新颖的空间插值流程，能够实现可靠的插值结果，并生成一种编码了不确定性信息的新型热图表示。我们利用图神经网络(GNNs)的插值参考数据来提高可视化可靠性和时间分辨率。通过集成主邻域聚合(PNA)和地理位置编码(GPE)，我们的模型能够有效学习时空依赖关系。此外，我们提出了一种基于插值热图的外部静态可视化技术，能够有效传达插值图中各种来源产生的不确定性。通过一系列用例、真实世界数据集的广泛评估和用户研究，我们证明了模型在数据插值方面的优越性能，参考数据对插值的改进效果，以及我们的可视化设计在传达不确定性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and reliable visualization of spatiotemporal sensor data such asenvironmental parameters and meteorological conditions is crucial for informeddecision-making. Traditional spatial interpolation methods, however, often fallshort of producing reliable interpolation results due to the limited andirregular sensor coverage. This paper introduces a novel spatial interpolationpipeline that achieves reliable interpolation results and produces a novelheatmap representation with uncertainty information encoded. We leverageimputation reference data from Graph Neural Networks (GNNs) to enhancevisualization reliability and temporal resolution. By integrating PrincipalNeighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), ourmodel effectively learns the spatiotemporal dependencies. Furthermore, wepropose an extrinsic, static visualization technique for interpolation-basedheatmaps that effectively communicates the uncertainties arising from varioussources in the interpolated map. Through a set of use cases, extensiveevaluations on real-world datasets, and user studies, we demonstrate ourmodel's superior performance for data imputation, the improvements to theinterpolant with reference data, and the effectiveness of our visualizationdesign in communicating uncertainties.</description>
      <author>example@mail.com (Juntong Chen, Huayuan Ye, He Zhu, Siwei Fu, Changbo Wang, Chenhui Li)</author>
      <guid isPermaLink="false">2508.01240v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Oldie but Goodie: Re-illuminating Label Propagation on Graphs with Partially Observed Features</title>
      <link>http://arxiv.org/abs/2508.01209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GOODIE是一种结合标签传播和特征传播的混合框架，通过GNN解码器、结构-特征注意力和伪标签对比学习，有效解决了现实世界中图数据特征缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的图中，经常遇到节点特征缺失的情况，如敏感信息缺失。直接使用图神经网络会导致下游任务次优结果，而当只有少量特征可用时，现有的基于GNN的方法表现甚至不如传统基于结构的模型。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，进一步挖掘经典标签传播的潜力，特别是在只有部分特征可用时，有效利用特征传播。&lt;h4&gt;方法&lt;/h4&gt;GOODIE框架采用混合方法从标签传播分支和特征传播分支获取嵌入：设计基于GNN的解码器使两分支输出对齐；利用结构-特征注意力自动捕获结构和特征信息重要性；采用伪标签对比学习区分不同正样本贡献；最终输出未标记节点的预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明，GOODIE模型不仅只在少量特征可用时表现优异，而且在特征丰富的情况下也优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;GOODIE模型能有效处理图数据中的节点特征缺失问题，特别是在特征有限的情况下表现突出。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的图中，我们经常遇到特征缺失的情况，其中少数或大部分节点特征（如敏感信息）缺失。在这种情况下，直接使用图神经网络会导致下游任务（如节点分类）的次优结果。尽管已经出现了一些基于GNN的方法试图缓解这种缺失情况，但当只有少量特征可用时，它们的表现反而不如传统的基于结构的模型。为此，我们提出了一个新框架，进一步挖掘经典标签传播的潜力，利用特征传播，特别是在只有部分特征可用的情况下。现在称为GOODIE，它采用混合方法从标签传播分支和特征传播分支获取嵌入。为此，我们首先设计了一个基于GNN的解码器，使标签传播分支能够输出与特征传播分支对齐的隐藏嵌入。然后，GOODIE通过新设计的结构-特征注意力自动捕获结构和特征信息的重要性。接着是一种新的伪标签对比学习，它区分来自标签传播分支的伪标签中每对正样本的贡献，GOODIE输出未标记节点的最终预测。通过大量实验，我们证明所提出的GOODIE模型不仅在只有少量特征可用时优于现有的最先进方法，而且在特征丰富的情况下也表现更好。GOODIE的源代码可在以下网址获取：https://github.com/SukwonYun/GOODIE。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world graphs, we often encounter missing feature situations where afew or the majority of node features, e.g., sensitive information, are missed.In such scenarios, directly utilizing Graph Neural Networks (GNNs) would yieldsub-optimal results in downstream tasks such as node classification. Despitethe emergence of a few GNN-based methods attempting to mitigate its missingsituation, when only a few features are available, they rather perform worsethan traditional structure-based models. To this end, we propose a novelframework that further illuminates the potential of classical Label Propagation(Oldie), taking advantage of Feature Propagation, especially when only apartial feature is available. Now called by GOODIE, it takes a hybrid approachto obtain embeddings from the Label Propagation branch and Feature Propagationbranch. To do so, we first design a GNN-based decoder that enables the LabelPropagation branch to output hidden embeddings that align with those of the FPbranch. Then, GOODIE automatically captures the significance of structure andfeature information thanks to the newly designed Structure-Feature Attention.Followed by a novel Pseudo-Label contrastive learning that differentiates thecontribution of each positive pair within pseudo-labels originating from the LPbranch, GOODIE outputs the final prediction for the unlabeled nodes. Throughextensive experiments, we demonstrate that our proposed model, GOODIE,outperforms the existing state-of-the-art methods not only when only a fewfeatures are available but also in abundantly available situations. Source codeof GOODIE is available at: https://github.com/SukwonYun/GOODIE.</description>
      <author>example@mail.com (Sukwon Yun, Xin Liu, Yunhak Oh, Junseok Lee, Tianlong Chen, Tsuyoshi Murata, Chanyoung Park)</author>
      <guid isPermaLink="false">2508.01209v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>BSL: A Unified and Generalizable Multitask Learning Platform for Virtual Drug Discovery from Design to Synthesis</title>
      <link>http://arxiv.org/abs/2508.01195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Baishenglai(BSL)平台，一个深度学习增强的开放平台，专为虚拟药物发现设计。该平台整合了七个核心任务，采用生成模型和图神经网络等先进技术，在多个基准数据集上达到最先进性能，并对分布外数据有良好泛化能力。实验证明BSL能有效发现具有生物活性的化合物，是加速药物发现的有力工具。&lt;h4&gt;背景&lt;/h4&gt;药物发现对保障人类健康、延长寿命和应对重大疾病挑战具有重要意义。近年来，人工智能在生物信息学和药理学关键任务中展现出显著优势，但现有计算平台存在工作流程碎片化、效率低下、缺乏算法创新以及对分布外数据泛化能力差等问题，阻碍了药物发现的进展。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面、高效且算法创新的平台，解决现有药物发现计算平台的局限性，提高虚拟药物发现的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Baishenglai(BSL)平台，这是一个深度学习增强的开放平台。在一个统一和模块化的框架内整合七个核心任务，采用生成模型和图神经网络等先进技术，并强调对OOD分子结构泛化的评估机制。&lt;h4&gt;主要发现&lt;/h4&gt;1) BSL在多个基准数据集上达到了最先进的性能；2) 与现有平台和基线方法的比较实验表明，BSL为虚拟药物发现提供了全面、可扩展且有效的解决方案；3) BSL成功发现了GluN1/GluN3A NMDA受体的新型调节剂，识别出三种具有明确生物活性的化合物，证明了其实用价值。&lt;h4&gt;结论&lt;/h4&gt;BSL是一个有前途的综合平台，既具有算法创新性，又能提供高精度预测，为加速生物医学研究和药物发现提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;药物发现对于保障人类健康、延长寿命以及应对重大疾病挑战具有重大的社会意义。近年来，由于其高效的数据处理和数据表示能力，人工智能在生物信息学和药理学的关键任务中展现出了显著优势。然而，大多数现有计算平台仅涵盖部分核心任务，导致工作流程碎片化和效率低下。此外，它们通常缺乏算法创新，对分布外(OOD)数据的泛化能力差，这严重阻碍了药物发现的进展。为解决这些局限性，我们提出了Baishenglai(BSL)，这是一个深度学习增强的开放平台，专为虚拟药物发现设计。BSL在一个统一和模块化的框架内整合了七个核心任务，融入了生成模型和图神经网络等先进技术。除了在多个基准数据集上达到最先进的性能外，该平台还强调关注对OOD分子结构泛化的评估机制。与现有平台和基线方法的比较实验表明，BSL为虚拟药物发现提供了一个全面、可扩展且有效的解决方案，既提供了算法创新，又为现实世界的研究提供了高精度预测。此外，BSL通过发现GluN1/GluN3A NMDA受体的新型调节剂证明了其实用性，在体外电生理学实验中成功识别出三种具有明确生物活性的化合物。这些结果突显了BSL作为加速生物医学研究和药物发现的有前途且全面的平台。平台网址为https://www.baishenglai.net。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug discovery is of great social significance in safeguarding human health,prolonging life, and addressing the challenges of major diseases. In recentyears, artificial intelligence has demonstrated remarkable advantages in keytasks across bioinformatics and pharmacology, owing to its efficient dataprocessing and data representation capabilities. However, most existingcomputational platforms cover only a subset of core tasks, leading tofragmented workflows and low efficiency. In addition, they often lackalgorithmic innovation and show poor generalization to out-of-distribution(OOD) data, which greatly hinders the progress of drug discovery. To addressthese limitations, we propose Baishenglai (BSL), a deep learning-enhanced,open-access platform designed for virtual drug discovery. BSL integrates sevencore tasks within a unified and modular framework, incorporating advancedtechnologies such as generative models and graph neural networks. In additionto achieving state-of-the-art (SOTA) performance on multiple benchmarkdatasets, the platform emphasizes evaluation mechanisms that focus ongeneralization to OOD molecular structures. Comparative experiments withexisting platforms and baseline methods demonstrate that BSL provides acomprehensive, scalable, and effective solution for virtual drug discovery,offering both algorithmic innovation and high-precision prediction forreal-world pharmaceutical research. In addition, BSL demonstrated its practicalutility by discovering novel modulators of the GluN1/GluN3A NMDA receptor,successfully identifying three compounds with clear bioactivity in in-vitroelectrophysiological assays. These results highlight BSL as a promising andcomprehensive platform for accelerating biomedical research and drug discovery.The platform is accessible at https://www.baishenglai.net.</description>
      <author>example@mail.com (Kun Li, Zhennan Wu, Yida Xiong, Hongzhi Zhang, Longtao Hu, Zhonglie Liu, Junqi Zeng, Wenjie Wu, Mukun Chen, Jiameng Chen, Wenbin Hu)</author>
      <guid isPermaLink="false">2508.01195v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Explaining GNN Explanations with Edge Gradients</title>
      <link>http://arxiv.org/abs/2508.01048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNN)解释方法的理论基础，建立了不同解释方法之间的联系，并在输入级和分层设置下进行了分析。&lt;h4&gt;背景&lt;/h4&gt;近年来，图神经网络在图结构数据上取得显著成功，促使了大量解释GNN预测方法的出现。然而，GNN可解释性的最先进方法仍在不断变化，不同比较方法结果不一，许多解释方法在处理复杂GNN架构和任务时存在困难。&lt;h4&gt;目的&lt;/h4&gt;对竞争性的GNN解释方法进行更仔细的理论分析，建立不同解释方法之间的理论联系。&lt;h4&gt;方法&lt;/h4&gt;在两种不同设置下研究GNN解释：输入级解释(产生输入图的解释子图)和分层解释(产生计算图的解释子图)。建立基于扰动的方法和基于梯度的方法之间的理论联系，并分析其他最近提出的方法。&lt;h4&gt;主要发现&lt;/h4&gt;1) 建立了基于扰动和基于梯度的方法之间的第一个理论联系；2) 在输入级，证明了GNNExplainer在某些条件下可以通过基于边梯度符号的简单启发式方法近似；3) 在分层设置中，指出边梯度等价于线性GNN的遮挡搜索；4) 通过实验展示了理论结果在实际数据上的表现。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了对GNN解释方法更深入的理论理解，连接了不同的解释方法，并展示了理论结果在实际中的应用。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络在图结构数据上的显著成功促使了大量解释GNN预测方法的涌现。然而，GNN可解释性的最先进方法仍在不断变化。不同的比较方法对不同方法的结果不一，许多解释方法在处理更复杂的GNN架构和任务时存在困难。这迫切需要对竞争性的GNN解释方法进行更仔细的理论分析。在本工作中，我们更仔细地研究了两种不同设置下的GNN解释：输入级解释，产生输入图的解释子图；以及分层解释，产生计算图的解释子图。我们建立了流行的基于扰动的方法和经典的基于梯度的方法之间的第一个理论联系，并指出了与其他最近提出的方法之间的联系。在输入级，我们证明了在某些条件下，GNNExplainer可以通过基于边梯度符号的简单启发式方法来近似。在分层设置中，我们指出边梯度等价于线性GNN的遮挡搜索。最后，我们通过在合成和真实数据集上的实验展示了我们的理论结果如何在实际中体现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3711896.3736947&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the remarkable success of graph neural networks (GNNs) ongraph-structured data has prompted a surge of methods for explaining GNNpredictions. However, the state-of-the-art for GNN explainability remains influx. Different comparisons find mixed results for different methods, with manyexplainers struggling on more complex GNN architectures and tasks. Thispresents an urgent need for a more careful theoretical analysis of competingGNN explanation methods. In this work we take a closer look at GNN explanationsin two different settings: input-level explanations, which produce explanatorysubgraphs of the input graph, and layerwise explanations, which produceexplanatory subgraphs of the computation graph. We establish the firsttheoretical connections between the popular perturbation-based and classicalgradient-based methods, as well as point out connections between other recentlyproposed methods. At the input level, we demonstrate conditions under whichGNNExplainer can be approximated by a simple heuristic based on the sign of theedge gradients. In the layerwise setting, we point out that edge gradients areequivalent to occlusion search for linear GNNs. Finally, we demonstrate how ourtheoretical results manifest in practice with experiments on both synthetic andreal datasets.</description>
      <author>example@mail.com (Jesse He, Akbar Rafiey, Gal Mishne, Yusu Wang)</author>
      <guid isPermaLink="false">2508.01048v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions</title>
      <link>http://arxiv.org/abs/2508.02329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CLIP-IN框架，通过两种核心创新增强了CLIP模型的细粒度视觉感知能力，包括利用指令编辑数据集作为困难负图像-文本对来源，以及整合长描述性字幕并使用旋转位置编码捕获丰富语义上下文。&lt;h4&gt;背景&lt;/h4&gt;尽管视觉语言模型（如CLIP）在视觉和语言对齐方面取得了成功，但它们在详细、细粒度的视觉理解方面仍面临关键挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新框架来增强CLIP模型的细粒度感知能力，提升其对细微视觉差异的理解。&lt;h4&gt;方法&lt;/h4&gt;首先利用指令编辑数据集作为困难负图像-文本对来源，结合对称困难负对比损失使模型能区分细微的视觉-语义差异；其次整合长描述性字幕，使用旋转位置编码捕获标准CLIP经常错过的丰富语义上下文。&lt;h4&gt;主要发现&lt;/h4&gt;CLIP-IN在MMVP基准和各种细粒度视觉识别任务上取得显著提升，同时不损害在更广泛分类和检索任务上的零样本性能；将CLIP-IN的视觉表示整合到多模态大语言模型中，显著减少了视觉幻觉并提高了推理能力。&lt;h4&gt;结论&lt;/h4&gt;将目标化的指令对比学习与全面的描述信息相结合，对于提升视觉语言模型的细粒度理解具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;尽管视觉语言模型（如CLIP）在视觉和语言对齐方面取得了成功，但它们在详细、细粒度的视觉理解方面仍面临关键挑战。我们提出了CLIP-IN，一个通过两项核心创新增强CLIP细粒度感知能力的新框架。首先，我们利用原本用于图像操作的指令编辑数据集作为困难负图像-文本对的独特来源。结合对称困难负对比损失，这使模型能够有效区分细微的视觉-语义差异。其次，CLIP-IN整合了长描述性字幕，使用旋转位置编码来捕获标准CLIP经常错过的丰富语义上下文。我们的实验表明，CLIP-IN在MMVP基准和各种细粒度视觉识别任务上取得了显著提升，同时不损害在更广泛的分类和检索任务上的零样本鲁棒性能。关键的是，将CLIP-IN的视觉表示整合到多模态大语言模型中显著减少了视觉幻觉并增强了推理能力。这项工作强调了将目标化的指令对比学习与全面的描述信息相结合以提升VLM细粒度理解的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the success of Vision-Language Models (VLMs) like CLIP in aligningvision and language, their proficiency in detailed, fine-grained visualcomprehension remains a key challenge. We present CLIP-IN, a novel frameworkthat bolsters CLIP's fine-grained perception through two core innovations.Firstly, we leverage instruction-editing datasets, originally designed forimage manipulation, as a unique source of hard negative image-text pairs.Coupled with a symmetric hard negative contrastive loss, this enables the modelto effectively distinguish subtle visual-semantic differences. Secondly,CLIP-IN incorporates long descriptive captions, utilizing rotary positionalencodings to capture rich semantic context often missed by standard CLIP. Ourexperiments demonstrate that CLIP-IN achieves substantial gains on the MMVPbenchmark and various fine-grained visual recognition tasks, withoutcompromising robust zero-shot performance on broader classification andretrieval tasks. Critically, integrating CLIP-IN's visual representations intoMultimodal Large Language Models significantly reduces visual hallucinationsand enhances reasoning abilities. This work underscores the considerablepotential of synergizing targeted, instruction-based contrastive learning withcomprehensive descriptive information to elevate the fine-grained understandingof VLMs.</description>
      <author>example@mail.com (Ziteng Wang, Siqi Yang, Limeng Qiao, Lin Ma)</author>
      <guid isPermaLink="false">2508.02329v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation</title>
      <link>http://arxiv.org/abs/2508.02265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Hermes是一种创新的半监督双阈值对比学习策略，用于超声图像分类和分割，解决了传统方法中伪标签不准确和任务间信息利用不足的问题。&lt;h4&gt;背景&lt;/h4&gt;基于置信度的伪标签选择通常产生过于自信但不正确的预测，这是由于模型早期的误导性和学习过程中对不准确伪标签的过拟合，严重降低了半监督对比学习的性能。此外，分割和分类任务被独立处理，亲和力无法被充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决现有半监督对比学习在超声图像处理中的局限性，提出一种能够有效利用任务间信息的新方法。&lt;h4&gt;方法&lt;/h4&gt;结合对比学习和半监督学习的优势，让伪标签通过提供额外指导来辅助对比学习；开发任务间注意力和显著性模块促进分割和分类任务间的信息共享；设计任务间一致性学习策略对齐两个任务中的肿瘤特征，避免负迁移以减少特征差异；收集了SZ-TUS甲状腺超声图像数据集解决数据集缺乏问题。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共超声数据集和一个私有数据集上的大量实验表明，Hermes在各种半监督设置下始终优于多种最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;Hermes通过有效解决伪标签不准确和任务间信息利用不足的问题，显著提升了半监督超声图像分类和分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于置信度的伪标签选择通常会产生过于自信但不正确的预测，这是由于模型早期的误导性和学习过程中对不准确伪标签的过拟合，这严重降低了半监督对比学习的性能。此外，分割和分类任务被独立处理，亲和力无法被充分探索。为解决这些问题，我们提出了一种用于超声图像分类和分割的新型半监督双阈值对比学习策略，名为Hermes。该策略结合了对比学习和半监督学习的优势，其中伪标签通过提供额外指导来辅助对比学习。具体而言，还开发了任务间注意力和显著性模块，以促进分割和分类任务之间的信息共享。此外，还设计了任务间一致性学习策略，对齐两个任务中的肿瘤特征，避免负迁移以减少特征差异。为解决公开可用超声数据集缺乏的问题，我们收集了SZ-TUS数据集，一个甲状腺超声图像数据集。在两个公共超声数据集和一个私有数据集上的大量实验表明，Hermes在各种半监督设置下始终优于多种最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Confidence-based pseudo-label selection usually generates overly confidentyet incorrect predictions, due to the early misleadingness of model andoverfitting inaccurate pseudo-labels in the learning process, which heavilydegrades the performance of semi-supervised contrastive learning. Moreover,segmentation and classification tasks are treated independently and theaffinity fails to be fully explored. To address these issues, we propose anovel semi-supervised dual-threshold contrastive learning strategy forultrasound image classification and segmentation, named Hermes. This strategycombines the strengths of contrastive learning with semi-supervised learning,where the pseudo-labels assist contrastive learning by providing additionalguidance. Specifically, an inter-task attention and saliency module is alsodeveloped to facilitate information sharing between the segmentation andclassification tasks. Furthermore, an inter-task consistency learning strategyis designed to align tumor features across both tasks, avoiding negativetransfer for reducing features discrepancy. To solve the lack of publiclyavailable ultrasound datasets, we have collected the SZ-TUS dataset, a thyroidultrasound image dataset. Extensive experiments on two public ultrasounddatasets and one private dataset demonstrate that Hermes consistentlyoutperforms several state-of-the-art methods across various semi-supervisedsettings.</description>
      <author>example@mail.com (Peng Zhang, Zhihui Lai, Heng Kong)</author>
      <guid isPermaLink="false">2508.02265v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Marco-Voice Technical Report</title>
      <link>http://arxiv.org/abs/2508.02038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个多功能语音合成系统Marco-Voice，集成了声音克隆和情感控制功能，在一个统一框架内实现高度表达性、可控性和自然的语音生成。&lt;h4&gt;背景&lt;/h4&gt;长期以来，在实现高度表达性、可控性和自然的语音生成方面存在挑战，特别是在不同语言和情感背景下保持说话人身份的一致性。&lt;h4&gt;目的&lt;/h4&gt;解决在高度表达性、可控性和自然语音生成方面的长期挑战，使生成的语音能在不同语言和情感背景下忠实保留说话人身份。&lt;h4&gt;方法&lt;/h4&gt;引入了有效的说话人-情感解耦机制，使用批次内对比学习实现说话人身份和情感风格的独立操作；开发了旋转情感嵌入集成方法用于平滑情感控制；构建了CSEMOTIONS数据集，包含6位专业说话人的10小时普通话语音，涵盖7种情感类别。&lt;h4&gt;主要发现&lt;/h4&gt;提出的系统Marco-Voice在客观和主观指标上都有显著改进；在语音清晰度和情感丰富度方面具有竞争力；代表了表达性神经语音合成领域的重大进展。&lt;h4&gt;结论&lt;/h4&gt;Marco-Voice系统能够在保持说话人身份的同时，实现高度表达性、可控性和自然的语音生成，是表达性神经语音合成领域的重要突破。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了一个多功能语音合成系统，该系统在一个统一的框架内集成了声音克隆和情感控制语音合成。这项工作的目标是解决在实现高度表达性、可控性和自然的语音生成方面的长期挑战，使生成的语音能在不同语言和情感背景下忠实保留说话人身份。我们的方法引入了一种有效的说话人-情感解耦机制，使用批次内对比学习，能够独立操作说话人身份和情感风格，以及用于平滑情感控制的旋转情感嵌入集成方法。为了支持全面的训练和评估，我们构建了CSEMOTIONS，一个高质量的情感语音数据集，包含来自六位专业说话人的10小时普通话语音，涵盖七种情感类别。大量实验表明，我们的系统Marco-Voice在客观和主观指标上都取得了显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a multifunctional speech synthesis system that integratesvoice cloning and emotion control speech synthesis within a unified framework.The goal of this work is to address longstanding challenges in achieving highlyexpressive, controllable, and natural speech generation that faithfullypreserves speaker identity across diverse linguistic and emotional contexts.Our approach introduces an effective speaker-emotion disentanglement mechanismwith in-batch contrastive learning, enabling independent manipulation ofspeaker identity and eemotional style, as well as rotational emotionalembedding integration method for smooth emotion control. To supportcomprehensive training and evaluation, we construct CSEMOTIONS, a high-qualityemotional speech dataset containing 10 hours of Mandarin speech from sixprofessional speakers across seven emotional categories. Extensive experimentsdemonstrate that our system, Marco-Voice, achieves substantial improvements inboth objective and subjective metrics. Comprehensive evaluations and analysiswere conducted, results show that MarcoVoice delivers competitive performancein terms of speech clarity and emotional richness, representing a substantialadvance in the field of expressive neural speech synthesis.</description>
      <author>example@mail.com (Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang)</author>
      <guid isPermaLink="false">2508.02038v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection</title>
      <link>http://arxiv.org/abs/2508.01966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对比自监督学习(SSL)如何减少单阶段目标检测器对大规模标注数据的依赖，通过SimCLR框架预训练YOLO骨干网络，在有限标注数据场景下提升检测性能。&lt;h4&gt;背景&lt;/h4&gt;单阶段目标检测器如YOLO系列在实时视觉应用中取得最先进性能，但训练过程严重依赖大规模标注数据集，限制了其在标注数据有限场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;研究对比自监督学习作为减少对标注数据依赖的方法，通过在未标注图像上预训练YOLO检测器骨干网络，提高模型在标注数据有限情况下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单有效的流程：将YOLO卷积骨干网络作为编码器，使用全局池化和投影头，利用COCO未标注数据集(12万张图像)的增强图像优化对比损失，然后在标注有限的自行车骑行者检测任务上微调预训练骨干网络。&lt;h4&gt;主要发现&lt;/h4&gt;SSL预训练带来更高的mAP，加快收敛速度，改善精确率-召回率性能，特别是在低标注数据条件下；SimCLR预训练的YOLOv8达到0.7663的mAP@50:95，超过监督学习对应模型，尽管预训练过程中未使用任何标注数据。&lt;h4&gt;结论&lt;/h4&gt;这些发现为将对比SSL应用于单阶段检测器建立了强有力基准，突出了未标注数据作为标注高效目标检测的可扩展资源的潜力。&lt;h4&gt;翻译&lt;/h4&gt;单阶段目标检测器如YOLO系列在实时视觉应用中取得了最先进的性能，但在训练过程中仍然严重依赖大规模标注数据集。在这项工作中，我们提出了对对比自监督学习(SSL)的系统性研究，作为一种通过使用SimCLR框架在未标注图像上预训练YOLOv5和YOLOv8骨干网络来减少这种依赖的方法。我们的方法引入了一个简单而有效的流程，将YOLO的卷积骨干网络作为编码器，采用全局池化和投影头，并使用COCO未标注数据集(12万张图像)的增强图像优化对比损失。然后在标注有限的自行车骑行者检测任务上对预训练骨干网络进行微调。实验结果表明，SSL预训练 consistently带来更高的mAP，更快的收敛速度，以及改善的精确率-召回率性能，特别是在低标注数据条件下。例如，我们的SimCLR预训练的YOLOv8达到了0.7663的mAP@50:95，超过了其监督学习对应模型，尽管在预训练过程中没有使用任何标注数据。这些发现为将对比SSL应用于单阶段检测器建立了强有力的基准，并突出了未标注数据作为标注高效目标检测的可扩展资源的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One-stage object detectors such as the YOLO family achieve state-of-the-artperformance in real-time vision applications but remain heavily reliant onlarge-scale labeled datasets for training. In this work, we present asystematic study of contrastive self-supervised learning (SSL) as a means toreduce this dependency by pretraining YOLOv5 and YOLOv8 backbones on unlabeledimages using the SimCLR framework. Our approach introduces a simple yeteffective pipeline that adapts YOLO's convolutional backbones as encoders,employs global pooling and projection heads, and optimizes a contrastive lossusing augmentations of the COCO unlabeled dataset (120k images). The pretrainedbackbones are then fine-tuned on a cyclist detection task with limited labeleddata. Experimental results show that SSL pretraining leads to consistentlyhigher mAP, faster convergence, and improved precision-recall performance,especially in low-label regimes. For example, our SimCLR-pretrained YOLOv8achieves a mAP@50:95 of 0.7663, outperforming its supervised counterpartdespite using no annotations during pretraining. These findings establish astrong baseline for applying contrastive SSL to one-stage detectors andhighlight the potential of unlabeled data as a scalable resource forlabel-efficient object detection.</description>
      <author>example@mail.com (Manikanta Kotthapalli, Reshma Bhatia, Nainsi Jain)</author>
      <guid isPermaLink="false">2508.01966v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery</title>
      <link>http://arxiv.org/abs/2508.01799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型预训练方法，通过整合不同溶剂条件下生成的配体构象系综作为增强输入，显著提升了蛋白质-配体相互作用预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确的蛋白质-配体相互作用预测对计算机辅助药物发现至关重要，但现有方法无法捕捉溶剂依赖的构象变化，且缺乏联合学习多个相关任务的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，开发一种能够同时学习结构灵活性和环境背景的预训练方法。&lt;h4&gt;方法&lt;/h4&gt;引入预训练方法，将不同溶剂条件下生成的配体构象系综作为增强输入。训练过程整合了分子重建以捕获局部几何结构、原子间距离预测以建模空间关系、以及对比学习以构建溶剂不变的分子表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法带来显著改进：结合亲和力预测提高3.7%、在PoseBusters Astex对接基准测试中达到82%成功率、虚拟筛选中曲线下面积为97.1%。框架支持溶剂感知的多任务建模，并在基准测试中产生一致结果。案例研究展示亚埃级对接精度，均方根偏差为0.157埃。&lt;h4&gt;结论&lt;/h4&gt;该框架为结合机制提供了原子级见解，并推动了基于结构的药物设计。&lt;h4&gt;翻译&lt;/h4&gt;准确的蛋白质-配体相互作用预测对计算机辅助药物发现至关重要。然而，现有方法通常无法捕捉溶剂依赖的构象变化，并且缺乏联合学习多个相关任务的能力。为解决这些局限性，我们引入了一种预训练方法，该方法将不同溶剂条件下生成的配体构象系综作为增强输入。这种设计使模型能够以统一方式学习结构灵活性和环境背景。训练过程整合了分子重建以捕获局部几何结构、原子间距离预测以建模空间关系、以及对比学习以构建溶剂不变的分子表示。这些组件共同带来了显著改进，包括结合亲和力预测提高3.7%、在PoseBusters Astex对接基准测试中达到82%的成功率、以及在虚拟筛选中97.1%的曲线下面积。该框架支持溶剂感知的多任务建模，并在基准测试中产生一致的结果。案例研究进一步展示了亚埃级对接精度，均方根偏差为0.157埃，为结合机制提供了原子级见解，并推动了基于结构的药物设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of protein-ligand interactions is essential forcomputer-aided drug discovery. However, existing methods often fail to capturesolvent-dependent conformational changes and lack the ability to jointly learnmultiple related tasks. To address these limitations, we introduce apre-training method that incorporates ligand conformational ensembles generatedunder diverse solvent conditions as augmented input. This design enables themodel to learn both structural flexibility and environmental context in aunified manner. The training process integrates molecular reconstruction tocapture local geometry, interatomic distance prediction to model spatialrelationships, and contrastive learning to build solvent-invariant molecularrepresentations. Together, these components lead to significant improvements,including a 3.7% gain in binding affinity prediction, an 82% success rate onthe PoseBusters Astex docking benchmarks, and an area under the curve of 97.1%in virtual screening. The framework supports solvent-aware, multi-task modelingand produces consistent results across benchmarks. A case study furtherdemonstrates sub-angstrom docking accuracy with a root-mean-square deviation of0.157 angstroms, offering atomic-level insight into binding mechanisms andadvancing structure-based drug design.</description>
      <author>example@mail.com (Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Ng Nga Chun, Gerald W. Y. Cheng, Zongxi Li, Jing Cai, Liang-ting Lin, Jung Sun Yoo)</author>
      <guid isPermaLink="false">2508.01799v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation</title>
      <link>http://arxiv.org/abs/2508.01600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in Proceedings of the Conference on Robot Learning (CoRL)  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLASS(Contrastive Learning via Action Sequence Supervision)的新方法，用于解决行为克隆在异构数据集上的泛化问题，特别是在视觉变化显著的情况下。&lt;h4&gt;背景&lt;/h4&gt;行为克隆在机器人操作领域因表现力强的模型、动作序列建模和大规模演示数据而取得显著进展，但在应用于异构数据集(如不同相机姿态或物体外观导致的视觉变化)时性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法使行为克隆能够从异构演示数据中学习共享结构，提高在视觉变化环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出CLASS方法，使用监督式对比学习从演示中学习行为表征，利用动态时间规整识别相似动作序列提供的弱监督，并优化具有相似度加权正对的软InfoNCE损失。&lt;h4&gt;主要发现&lt;/h4&gt;CLASS在5个模拟基准和3个真实世界任务上实现了具有竞争力的结果；对于在显著视觉变化下的下游策略学习，使用CLASS预训练的扩散策略取得了平均75%的成功率，而所有其他基线方法无法竞争性地执行。&lt;h4&gt;结论&lt;/h4&gt;CLASS方法有效解决了行为克隆在异构数据集上的泛化问题，特别是在视觉变化显著的情况下表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近的 Behavior Cloning (BC) 进展在机器人操作领域取得了强大性能，这得益于表现力强的模型、动作序列建模和大规模演示数据。然而，当应用于异构数据集(如不同相机姿态或物体外观导致的视觉变化)时，BC面临重大挑战，尽管有大规模学习的好处，性能仍然下降。这是因为BC倾向于过度拟合单个演示而不是捕获共享结构，限制了泛化能力。为解决这一问题，我们引入了通过动作序列监督进行对比学习(Contrastive Learning via Action Sequence Supervision, CLASS)，一种使用监督式对比学习从演示中学习行为表征的方法。CLASS利用通过动态时间规整(DTW)识别的相似动作序列提供的弱监督，并优化具有相似度加权正对的软InfoNCE损失。我们在5个模拟基准和3个真实世界任务上评估CLASS，仅使用基于检索的控制和表征就实现了具有竞争力的结果。最值得注意的是，对于在显著视觉变化下的下游策略学习，使用CLASS预训练的扩散策略取得了平均75%的成功率，而所有其他基线方法无法竞争性地执行。项目网页：https://class-robot.github.io。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Behavior Cloning (BC) have led to strong performance inrobotic manipulation, driven by expressive models, sequence modeling ofactions, and large-scale demonstration data. However, BC faces significantchallenges when applied to heterogeneous datasets, such as visual shift withdifferent camera poses or object appearances, where performance degradesdespite the benefits of learning at scale. This stems from BC's tendency tooverfit individual demonstrations rather than capture shared structure,limiting generalization. To address this, we introduce Contrastive Learning viaAction Sequence Supervision (CLASS), a method for learning behavioralrepresentations from demonstrations using supervised contrastive learning.CLASS leverages weak supervision from similar action sequences identified viaDynamic Time Warping (DTW) and optimizes a soft InfoNCE loss withsimilarity-weighted positive pairs. We evaluate CLASS on 5 simulationbenchmarks and 3 real-world tasks to achieve competitive results usingretrieval-based control with representations only. Most notably, for downstreampolicy learning under significant visual shifts, Diffusion Policy with CLASSpre-training achieves an average success rate of 75%, while all other baselinemethods fail to perform competitively. Project webpage:https://class-robot.github.io.</description>
      <author>example@mail.com (Sung-Wook Lee, Xuhui Kang, Brandon Yang, Yen-Ling Kuo)</author>
      <guid isPermaLink="false">2508.01600v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.01569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LetheViT的对比遗忘方法，专门用于解决Vision Transformers(ViTs)中的随机数据遗忘问题。通过选择性掩码实验揭示ViTs核心特征，LetheViT能够有效引导模型忘记特定数据样本的影响，同时保留其一般识别能力，达到隐私保护与模型效能的平衡。&lt;h4&gt;背景&lt;/h4&gt;Vision Transformers在计算机视觉任务中表现出色，但随着GDPR和CCPA等隐私法规的实施，用户有权撤回其数据，这要求不仅删除数据，还需从训练模型中完全移除数据影响。精确遗忘方法计算成本过高，而近似方法提供了更实用的解决方案。&lt;h4&gt;目的&lt;/h4&gt;解决ViTs中随机数据遗忘的特别挑战场景，使模型能够忘记特定样本同时保留其他样本，即使在同一类别中也是如此，同时平衡隐私合规性与模型效能。&lt;h4&gt;方法&lt;/h4&gt;通过选择性掩码实验揭示ViTs的核心特征，提出LetheViT对比遗忘方法。该方法使用掩码图像输入生成正logits，原始图像输入生成负logits，引导模型忘记特定细节同时保留一般类别轮廓。&lt;h4&gt;主要发现&lt;/h4&gt;1) 当ViTs的高注意力区域被掩码时，模型保留识别能力但显著削弱记忆能力；2) 基于这一发现提出的LetheViT方法能有效平衡隐私保护与模型性能；3) LetheViT在随机数据遗忘场景中达到了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;LetheViT作为一种专为ViTs设计的对比遗忘方法，通过选择性掩码和对比学习策略，成功解决了随机数据遗忘问题，实现了隐私合规与模型效能的有效平衡，为满足隐私法规要求提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉变换器(ViTs)凭借其卓越性能彻底改变了计算机视觉任务。然而，GDPR和CCPA等隐私法规的引入给它们带来了新的挑战。这些法律授予用户撤回其数据的权利，不仅需要删除数据，还需要从训练好的模型中完全移除其影响。机器遗忘成为一种关键解决方案，其中精确遗忘在计算上过于昂贵，而近似方法提供了更实用的方法。这项工作解决了ViTs中随机数据遗忘的特别具有挑战性的场景，模型必须忘记特定样本同时保留其他样本，即使在同一类别中也是如此。我们首先通过选择性掩码实验揭示了ViTs的核心特征：当高注意力区域被掩码时，模型保留其识别能力但显著削弱其记忆能力。基于上述见解，我们提出了LetheViT，一种专为ViTs设计的对比遗忘方法。LetheViT使用掩码图像输入生成正logits，原始图像输入生成负logits，引导模型忘记特定细节同时保留一般的类别轮廓。实验结果表明，LetheViT达到了最先进的性能，有效平衡了隐私合规与模型效能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Transformers (ViTs) have revolutionized computer vision tasks withtheir exceptional performance. However, the introduction of privacy regulationssuch as GDPR and CCPA has brought new challenges to them. These laws grantusers the right to withdraw their data, necessitating not only the deletion ofdata but also the complete removal of its influence from trained models.Machine unlearning emerges as a critical solution, with exact unlearning beingcomputationally prohibitive and approximate methods offering a more practicalapproach. This work addresses the particularly challenging scenario of randomdata forgetting in ViTs, where the model must forget specific samples whileretaining others, even within the same class. We first reveal the corecharacteristics of ViTs through selective masking experiments: whenhigh-attention areas are masked, the model retains its recognition capabilitybut significantly weakens its memorization ability. Based on the aboveinsights, we propose LetheViT, a contrastive unlearning method tailored forViTs. LetheViT uses masked image inputs to generate positive logits andoriginal image inputs to generate negative logits, guiding the model to forgetspecific details while retaining the general cl category outlines. Experimentalresults demonstrate that LetheViT achieves state-of-the-art performance,effectively balancing privacy compliance with model efficacy.</description>
      <author>example@mail.com (Yujia Tong, Tian Zhang, Jingling Yuan, Yuze Wang, Chuang Hu)</author>
      <guid isPermaLink="false">2508.01569v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection</title>
      <link>http://arxiv.org/abs/2508.01248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NS-Net的新型检测框架，通过NULL-Space投影解耦CLIP视觉特征中的语义信息，并结合对比学习捕获真实与生成图像间的分布差异，同时设计了Patch Selection策略保留精细痕迹，显著提升了AI生成图像检测的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;生成模型如GANs和扩散模型的快速发展能够创建高度逼真的图像，引发安全敏感领域被滥用的担忧。现有检测器在已知生成设置下表现良好，但难以推广到未知生成模型，尤其当真实与伪造图像语义内容紧密对齐时。&lt;h4&gt;目的&lt;/h4&gt;重新审视CLIP特征在AI生成图像检测中的应用，解决CLIP视觉特征中嵌入的高层次语义信息阻碍有效区分的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NS-Net检测框架，利用NULL-Space投影将CLIP视觉特征中的语义信息解耦，使用对比学习捕获真实和生成图像间的内在分布差异，并设计Patch Selection策略减轻全局图像结构引起的语义偏差以保留精细人工痕迹。&lt;h4&gt;主要发现&lt;/h4&gt;CLIP特征中的高层次语义信息阻碍了有效区分真实和AI生成图像；NS-Net在包含40种不同生成模型的开世界基准测试中表现优异，检测准确率比现有最先进方法提高7.4%。&lt;h4&gt;结论&lt;/h4&gt;NS-Net在GAN和基于扩散的图像生成技术方面展现出强大的泛化能力，为AI生成图像检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;生成模型（如GANs和扩散模型）的快速发展促进了高度逼真图像的创建，引发了在安全敏感领域被滥用的日益增长的担忧。虽然现有检测器在已知的生成设置下表现良好，但它们往往难以推广到未知的生成模型，特别是当真实和伪造图像之间的语义内容紧密对齐时。在本文中，我们重新审视了CLIP特征在AI生成图像检测中的应用，并发现了一个关键局限：CLIP视觉特征中嵌入的高层次语义信息阻碍了有效区分。为解决这一问题，我们提出了NS-Net，一种新的检测框架，利用NULL-Space投影将CLIP视觉特征中的语义信息解耦，然后通过对比学习捕获真实和生成图像之间的内在分布差异。此外，我们设计了一种Patch Selection策略，通过减轻全局图像结构引起的语义偏差来保留精细的人工痕迹。在对包含40种不同生成模型生成图像的开世界基准进行的广泛实验中，NS-Net优于现有最先进的方法，检测准确率提高了7.4%，从而证明了它在基于GAN和基于扩散的图像生成技术方面具有强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid progress of generative models, such as GANs and diffusion models,has facilitated the creation of highly realistic images, raising growingconcerns over their misuse in security-sensitive domains. While existingdetectors perform well under known generative settings, they often fail togeneralize to unknown generative models, especially when semantic contentbetween real and fake images is closely aligned. In this paper, we revisit theuse of CLIP features for AI-generated image detection and uncover a criticallimitation: the high-level semantic information embedded in CLIP's visualfeatures hinders effective discrimination. To address this, we propose NS-Net,a novel detection framework that leverages NULL-Space projection to decouplesemantic information from CLIP's visual features, followed by contrastivelearning to capture intrinsic distributional differences between real andgenerated images. Furthermore, we design a Patch Selection strategy to preservefine-grained artifacts by mitigating semantic bias caused by global imagestructures. Extensive experiments on an open-world benchmark comprising imagesgenerated by 40 diverse generative models show that NS-Net outperforms existingstate-of-the-art methods, achieving a 7.4\% improvement in detection accuracy,thereby demonstrating strong generalization across both GAN- anddiffusion-based image generation techniques.</description>
      <author>example@mail.com (Jiazhen Yan, Fan Wang, Weiwei Jiang, Ziqiang Li, Zhangjie Fu)</author>
      <guid isPermaLink="false">2508.01248v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CM$^3$: Calibrating Multimodal Recommendation</title>
      <link>http://arxiv.org/abs/2508.01226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Working Paper: https://github.com/enoche/CM3&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对多模态推荐系统中的对齐和均匀性原则，提出了一种校准的均匀性损失函数和球形贝塞尔方法，解决了现有模型过度强调均匀性而忽视对齐的问题，提升了推荐性能。&lt;h4&gt;背景&lt;/h4&gt;在对比学习中，对齐和均匀性是基本原则。在推荐系统中，优化贝叶斯个性化排序(BPR)损失有助于实现对齐和均匀性目标，其中对齐旨在拉近交互用户和项目的表示，均匀性要求用户和项目嵌入在单位超球面上均匀分布。&lt;h4&gt;目的&lt;/h4&gt;重新审视多模态推荐系统中的对齐和均匀性属性，挑战通过均匀性损失对项目进行公平处理的传统假设，提出使具有相似多模态属性的项目在超球面流形上收敛到相近表示的更细致方法。&lt;h4&gt;方法&lt;/h4&gt;利用项目多模态数据之间的相似性校准均匀性分布，增强不相似实体间的排斥力；引入球形贝塞尔方法融合多模态特征，确保融合后的特征被约束在相同的超球面流形上；进行理论分析阐明校准的均匀性损失与常规均匀性函数的关系。&lt;h4&gt;主要发现&lt;/h4&gt;现有多模态推荐模型倾向于优先考虑均匀性而损害对齐；通过整合MLLM提取的特征，所提方法可实现NDCG@20性能高达5.4%的提升；校准的均匀性损失能在嵌入空间中产生更强的排斥力。&lt;h4&gt;结论&lt;/h4&gt;通过校准均匀性损失和引入球形贝塞尔方法，所提方法在多模态推荐系统中优于竞争基线，实现了更好的推荐性能，源代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;对齐和均匀性是对比学习领域的基本原则。在推荐系统中，先前的工作已经证明优化贝叶斯个性化排序(BPR)损失有助于实现对齐和均匀性的目标。具体来说，对齐旨在拉近交互用户和项目的表示，而均匀性要求用户和项目嵌入在单位超球面上均匀分布。本研究重新审视了多模态推荐系统背景下的对齐和均匀性属性，揭示了现有模型倾向于优先考虑均匀性而损害对齐的倾向。我们的假设挑战了通过均匀性损失对项目进行公平处理的传统假设，提出了一种更细致的方法，即具有相似多模态属性的项目在超球面流形上收敛到相近的表示。具体来说，我们利用项目多模态数据之间的相似性来校准它们的均匀性分布，从而在嵌入空间中诱导不相似实体之间更明显的排斥力。理论分析阐明了这种校准的均匀性损失与常规均匀性函数之间的关系。此外，为了增强多模态特征的融合，我们引入了一种球形贝塞尔方法，旨在融合任意数量的模态，同时确保融合后的特征被约束在相同的超球面流形上。在五个真实世界数据集上进行经验评估，证实了我们的方法优于竞争基线。我们还表明，通过整合MLLM提取的特征，所提出的方法可以实现NDCG@20性能高达5.4%的提升。源代码可在以下网址获取：https://github.com/enoche/CM3。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alignment and uniformity are fundamental principles within the domain ofcontrastive learning. In recommender systems, prior work has established thatoptimizing the Bayesian Personalized Ranking (BPR) loss contributes to theobjectives of alignment and uniformity. Specifically, alignment aims to drawtogether the representations of interacting users and items, while uniformitymandates a uniform distribution of user and item embeddings across a unithypersphere. This study revisits the alignment and uniformity properties withinthe context of multimodal recommender systems, revealing a proclivity amongextant models to prioritize uniformity to the detriment of alignment. Ourhypothesis challenges the conventional assumption of equitable item treatmentthrough a uniformity loss, proposing a more nuanced approach wherein items withsimilar multimodal attributes converge toward proximal representations withinthe hyperspheric manifold. Specifically, we leverage the inherent similaritybetween items' multimodal data to calibrate their uniformity distribution,thereby inducing a more pronounced repulsive force between dissimilar entitieswithin the embedding space. A theoretical analysis elucidates the relationshipbetween this calibrated uniformity loss and the conventional uniformityfunction. Moreover, to enhance the fusion of multimodal features, we introducea Spherical B\'ezier method designed to integrate an arbitrary number ofmodalities while ensuring that the resulting fused features are constrained tothe same hyperspherical manifold. Empirical evaluations conducted on fivereal-world datasets substantiate the superiority of our approach over competingbaselines. We also shown that the proposed methods can achieve up to a 5.4%increase in NDCG@20 performance via the integration of MLLM-extracted features.Source code is available at: https://github.com/enoche/CM3.</description>
      <author>example@mail.com (Xin Zhou, Yongjie Wang, Zhiqi Shen)</author>
      <guid isPermaLink="false">2508.01226v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Pivoting Manipulation with Force and Vision Feedback Using Optimization-based Demonstrations</title>
      <link>http://arxiv.org/abs/2508.01082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合模型方法和学习方法的框架，用于学习闭环摆动操作，通过接触隐式轨迹优化和演示引导的深度强化学习实现样本高效学习，并采用特权训练策略实现仿真到现实迁移。&lt;h4&gt;背景&lt;/h4&gt;非抓取操作具有挑战性，因为物体、环境和机器人之间复杂的接触相互作用使得操作困难。&lt;h4&gt;目的&lt;/h4&gt;结合基于模型和基于学习方法的优势，提出一种框架，使机器人能够仅使用本体感觉、视觉和力传感执行摆动操作，无需特权信息。&lt;h4&gt;方法&lt;/h4&gt;利用计算高效的接触隐式轨迹优化（CITO）设计演示引导的深度强化学习（RL），采用特权训练策略实现仿真到现实迁移。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多个摆动任务上成功实现了仿真到现实迁移，机器人能够仅使用本体感觉、视觉和力传感执行摆动操作，无需特权信息。&lt;h4&gt;结论&lt;/h4&gt;结合模型方法和学习方法可以克服各自的局限性，实现样本高效的学习和成功的仿真到现实迁移。&lt;h4&gt;翻译&lt;/h4&gt;非抓取操作由于物体、环境和机器人之间复杂的接触相互作用而具有挑战性。基于模型的方法可以在接触约束下高效生成机器人和物体的复杂轨迹，但它们往往对模型不准确敏感，并且需要访问特权信息（如物体质量、大小、姿态），使其不太适合新物体。相比之下，基于学习的方法通常对建模错误更鲁棒，但需要大量数据。在本文中，我们结合这两种方法，提出了一种学习闭环摆动操作的框架。通过利用计算高效的接触隐式轨迹优化（CITO），我们设计了演示引导的深度强化学习（RL），实现了样本高效学习。我们还提出了使用特权训练策略的仿真到现实迁移方法，使机器人能够仅使用本体感觉、视觉和力传感执行摆动操作，无需访问特权信息。我们的方法在几个摆动任务上进行了评估，证明它能够成功实现仿真到现实迁移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-prehensile manipulation is challenging due to complex contactinteractions between objects, the environment, and robots. Model-basedapproaches can efficiently generate complex trajectories of robots and objectsunder contact constraints. However, they tend to be sensitive to modelinaccuracies and require access to privileged information (e.g., object mass,size, pose), making them less suitable for novel objects. In contrast,learning-based approaches are typically more robust to modeling errors butrequire large amounts of data. In this paper, we bridge these two approaches topropose a framework for learning closed-loop pivoting manipulation. Byleveraging computationally efficient Contact-Implicit Trajectory Optimization(CITO), we design demonstration-guided deep Reinforcement Learning (RL),leading to sample-efficient learning. We also present a sim-to-real transferapproach using a privileged training strategy, enabling the robot to performpivoting manipulation using only proprioception, vision, and force sensingwithout access to privileged information. Our method is evaluated on severalpivoting tasks, demonstrating that it can successfully perform sim-to-realtransfer.</description>
      <author>example@mail.com (Yuki Shirai, Kei Ota, Devesh K. Jha, Diego Romeres)</author>
      <guid isPermaLink="false">2508.01082v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph Embedding in the Graph Fractional Fourier Transform Domain</title>
      <link>http://arxiv.org/abs/2508.02383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图分数傅里叶变换的广义分数滤波嵌入方法(GEFRFE)，通过将现有方法扩展到分数域来增强嵌入信息量，并在多个基准数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;谱图嵌入在图表示学习中起着关键作用，它通过从图谱信息生成低维向量表示来实现。然而，传统谱嵌入方法的嵌入空间往往表现出有限的表达能力，无法在可替代的变换域中充分捕获潜在的结构特征。&lt;h4&gt;目的&lt;/h4&gt;解决传统谱嵌入方法表达能力有限的问题，使其能够在可替代的变换域中充分捕获潜在的结构特征。&lt;h4&gt;方法&lt;/h4&gt;使用图分数傅里叶变换将现有的广义频率滤波嵌入(GEFFE)扩展到分数域，产生GEFRFE；利用图分数域滤波和来自分数化图拉普拉斯矩阵的特征向量分量的非线性组合；引入两种并行策略动态确定分数阶：基于搜索的优化和基于ResNet18的自适应学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准数据集上的实验表明，GEFRFE捕获了更丰富的结构特征并显著提高了分类性能，同时保持了与GEFFE方法相当的计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;GEFRFE方法通过扩展到分数域有效增强了嵌入的表达能力，提高了图表示学习的效果。&lt;h4&gt;翻译&lt;/h4&gt;谱图嵌入通过从图谱信息生成低维向量表示，在图表示学习中起着关键作用。然而，传统谱嵌入方法的嵌入空间往往表现出有限的表达能力，无法在可替代的变换域中充分捕获潜在的结构特征。为解决这个问题，我们使用图分数傅里叶变换将现有的最先进广义频率滤波嵌入(GEFFE)扩展到分数域，产生了广义分数滤波嵌入(GEFRFE)，该方法通过图分数域增强了嵌入信息量。GEFRFE利用图分数域滤波和来自分数化图拉普拉斯矩阵导出的特征向量分量的非线性组合。为了动态确定分数阶，引入了两种并行策略：基于搜索的优化和基于ResNet18的自适应学习。在六个基准数据集上的大量实验表明，GEFRFE捕获了更丰富的结构特征并显著提高了分类性能。值得注意的是，所提出的方法保持了与GEFFE方法相当的计算复杂度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral graph embedding plays a critical role in graph representationlearning by generating low-dimensional vector representations from graphspectral information. However, the embedding space of traditional spectralembedding methods often exhibit limited expressiveness, failing to exhaustivelycapture latent structural features across alternative transform domains. Toaddress this issue, we use the graph fractional Fourier transform to extend theexisting state-of-the-art generalized frequency filtering embedding (GEFFE)into fractional domains, giving birth to the generalized fractional filteringembedding (GEFRFE), which enhances embedding informativeness via the graphfractional domain. The GEFRFE leverages graph fractional domain filtering and anonlinear composition of eigenvector components derived from a fractionalizedgraph Laplacian. To dynamically determine the fractional order, two parallelstrategies are introduced: search-based optimization and a ResNet18-basedadaptive learning. Extensive experiments on six benchmark datasets demonstratethat the GEFRFE captures richer structural features and significantly enhanceclassification performance. Notably, the proposed method retains computationalcomplexity comparable to GEFFE approaches.</description>
      <author>example@mail.com (Changjie Sheng, Zhichao Zhang, Wei Yao)</author>
      <guid isPermaLink="false">2508.02383v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment</title>
      <link>http://arxiv.org/abs/2508.02307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种全身自监督表示学习方法，用于竞争风险建模下的临床前疾病风险评估，在多种疾病预测中优于传统放射组学方法，并展示了其在临床工作流程中的转化潜力。&lt;h4&gt;背景&lt;/h4&gt;可靠的临床前疾病风险评估对将公共医疗从反应性治疗转变为主动识别和预防至关重要，但现有基于图像的风险预测算法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种全身自监督表示学习方法，用于临床前疾病风险评估，以克服现有方法仅考虑单一疾病和依赖手工制作特征的不足。&lt;h4&gt;方法&lt;/h4&gt;采用全身自监督表示学习方法在竞争风险建模框架下进行临床前疾病风险评估，并结合心脏MRI提高心血管疾病亚组的预测能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在心血管疾病、2型糖尿病、慢性阻塞性肺病和慢性肾病等多种疾病预测中优于全身放射组学；与心脏MRI结合后，进一步提高了缺血性心脏病、高血压疾病和中风等心血管疾病亚组的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;全身表示作为独立筛查方式和临床工作流程中多模态框架的一部分，在早期个性化风险分层方面具有重要转化潜力，相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;可靠的临床前疾病风险评估对于将公共医疗从反应性治疗转变为主动识别和预防至关重要。然而，基于图像的风险预测算法通常一次只考虑一种情况，并依赖于通过分割工具获得的手工制作特征。我们提出了一种在竞争风险建模下的临床前疾病风险评估的全局自监督表示学习方法。这种方法在多种疾病（包括心血管疾病、2型糖尿病、慢性阻塞性肺病和慢性肾病）中优于全身放射组学。模拟临床前筛查场景并与心脏MRI结合后，进一步提高了心血管疾病亚组（缺血性心脏病、高血压疾病和中风）的预测能力。结果表明，全身表示作为独立筛查方式和临床工作流程中多模态框架的一部分，在早期个性化风险分层方面具有转化潜力。代码可在https://github.com/yayapa/WBRLforCR/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何进行更准确的全身性疾病风险预测问题，特别是在疾病尚未出现症状的'临床前'阶段。这个问题很重要，因为心血管疾病、2型糖尿病等慢性疾病每年导致超过2600万人死亡，这些疾病经常共存，而传统算法通常单独处理，无法捕捉疾病间的相互作用。早期风险识别可以将医疗从被动治疗转向主动预防，而全身MRI在一次筛查中提供多器官视图，但这种方法在风险预测中尚未得到充分利用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有风险预测方法的局限性，特别是无法处理竞争风险和依赖手工特征的问题。他们使用UK Biobank数据集作为基础，借鉴了Zhang等人在心脏MRI表示学习中的MAE架构，以及Bai等人的心脏特征提取方法。在竞争风险建模方面，他们采用了现有的DSM、NFG和DeepHit模型。作者设计了一个基于MAE的自监督学习框架，从全身MRI中学习表示，然后将其输入竞争风险模型中进行疾病风险预测，并评估了单独使用全身表示以及与心脏特征结合的多模态方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用自监督学习从全身MRI中学习有意义的表示，这些表示能够捕捉全身多个器官和系统之间的相互作用，并在竞争风险框架下使用它们，同时考虑多种疾病之间的相互关系。整体流程包括：1)从UK Biobank获取全身MRI和心脏MRI数据并确定疾病组；2)开发基于MAE的自监督学习模型训练全身表示；3)提取心脏结构和功能特征；4)使用竞争风险模型(DSM、NFG、DeepHit)进行风险预测；5)使用时间依赖一致性指数评估不同模态和模型的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开发了专门针对全身MRI的自监督表示学习方法；2)在单一框架中同时评估多种疾病风险，考虑疾病间的竞争关系；3)提出将全身MRI筛查整合到临床工作流程中的方法；4)证明全身表示可作为独立筛查工具，也可与器官特异性成像结合。相比之前的工作，这篇论文不依赖手工分割和特征提取，而是使用端到端的表示学习；同时考虑多种疾病风险而非单一疾病；将自监督学习应用于全身MRI，并结合表示学习和竞争风险建模，这些都是医学影像分析中的新方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文开发了一种基于全身自监督表示学习的竞争风险预测方法，能够在疾病出现症状前准确评估多种疾病风险，为早期预防和个性化医疗提供了新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable preclinical disease risk assessment is essential to move publichealthcare from reactive treatment to proactive identification and prevention.However, image-based risk prediction algorithms often consider one condition ata time and depend on hand-crafted features obtained through segmentation tools.We propose a whole-body self-supervised representation learning method for thepreclinical disease risk assessment under a competing risk modeling. Thisapproach outperforms whole-body radiomics in multiple diseases, includingcardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructivepulmonary disease (COPD), and chronic kidney disease (CKD). Simulating apreclinical screening scenario and subsequently combining with cardiac MRI, itsharpens further the prediction for CVD subgroups: ischemic heart disease(IHD), hypertensive diseases (HD), and stroke. The results indicate thetranslational potential of whole-body representations as a standalone screeningmodality and as part of a multi-modal framework within clinical workflows forearly personalized risk stratification. The code is available athttps://github.com/yayapa/WBRLforCR/</description>
      <author>example@mail.com (Dmitrii Seletkov, Sophie Starck, Ayhan Can Erdur, Yundi Zhang, Daniel Rueckert, Rickmer Braren)</author>
      <guid isPermaLink="false">2508.02307v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2508.02190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedVLA，第一个联邦Vision-language-action学习框架，旨在解决VLA模型训练中的隐私安全问题，同时保持模型性能。&lt;h4&gt;背景&lt;/h4&gt;Vision-language-action (VLA)模型通过使机器人能够解释语言指令来执行任务，显著推进了机器人操作。然而，训练这些模型通常依赖于大规模用户特定数据，引发隐私和安全问题，限制了其广泛采用。&lt;h4&gt;目的&lt;/h4&gt;提出FedVLA框架，实现分布式模型训练，保护数据隐私而不损害性能，促进VLA模型的广泛采用。&lt;h4&gt;方法&lt;/h4&gt;整合任务感知表征学习、自适应专家选择和专家驱动的联邦聚合；引入指令导向场景解析机制，基于任务指令分解和增强对象级特征；设计双门控专家混合(DGMoE)机制，使输入标记和自感知专家自适应决定激活；在联邦服务器上提出专家驱动聚合策略，由激活专家引导模型聚合。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的仿真和真实机器人实验证明了提案的有效性；DGMoE与其基础版本相比显著提高了计算效率；FedVLA实现了与集中式训练相当的任务成功率，同时有效保护数据隐私。&lt;h4&gt;结论&lt;/h4&gt;FedVLA框架能够在保护数据隐私的同时有效训练VLA模型，有望解决VLA模型广泛采用中的隐私问题。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-行动(VLA)模型通过使机器人能够解释语言指令来执行任务，显著推进了机器人操作。然而，训练这些模型通常依赖于大规模用户特定数据，引发隐私和安全问题，进而限制了它们的广泛采用。为此，我们提出了FedVLA，第一个联邦VLA学习框架，能够实现分布式模型训练，在保护数据隐私的同时不损害性能。我们的框架整合了任务感知表征学习、自适应专家选择和专家驱动的联邦聚合，实现了VLA模型的高效且隐私保护的训练。具体来说，我们引入了指令导向场景解析机制，基于任务指令分解和增强对象级特征，提高上下文理解能力。为了有效学习多样化的任务模式，我们设计了双门控专家混合(DGMoE)机制，其中不仅输入标记，而且自感知专家也能自适应决定其激活。最后，我们在联邦服务器上提出了专家驱动聚合策略，模型聚合由激活专家引导，确保有效的跨客户端知识转移。广泛的仿真和真实机器人实验证明了我们提案的有效性。值得注意的是，与基础版本相比，DGMoE显著提高了计算效率，而FedVLA实现了与集中式训练相当的任务成功率，同时有效保护数据隐私。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉-语言-动作模型训练中的隐私安全问题以及联邦学习环境下机器人操作任务的异构性问题。这些问题很重要，因为训练VLA模型需要大量用户特定数据，会引发隐私泄露风险，限制了技术的广泛应用；同时，传统联邦学习方法难以处理多模态环境下的任务异构性，而现有混合专家模型又缺乏对任务复杂性的适应性，导致在资源受限的客户端上效率低下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到VLA模型训练中的隐私泄露风险和联邦学习中的任务异构性问题，然后借鉴了联邦学习的基本框架保护数据隐私，基于现有VLA模型架构如RoboFlamingo和RT-2，扩展了混合专家模型架构并增加了自适应选择机制。创新性地设计了指令导向场景解析模块增强特征提取，提出双重门控混合专家机制实现自适应知识路由，以及专家驱动聚合策略处理任务异构性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在保护数据隐私的前提下，通过联邦学习训练VLA模型，利用双重门控混合专家机制提高计算效率和模型性能，基于专家相似性进行模型聚合处理任务异构性。整体流程是：客户端使用IOSP模块处理输入图像，通过DGMoE机制自适应选择专家处理输入并记录专家选择统计信息；服务器接收客户端的模型更新和专家选择统计信息，使用EDA策略基于专家相似性计算聚合权重并更新全局模型；重复迭代直到模型收敛。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) FedVLA框架，首个用于VLA训练的隐私保护联邦学习框架；2) 双重门控混合专家(DGMoE)机制，引入自我感知专家实现双向选择；3) 专家驱动聚合(EDA)策略，基于专家相似性动态分配权重；4) 指令导向场景解析(IOSP)模块，提高上下文理解能力。相比之前的工作，FedVLA能够处理任务异构性，自适应选择专家而非固定数量，并专注于多模态环境下的隐私保护训练，而非单模态任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了FedVLA框架，结合双重门控混合专家机制，在保护用户数据隐私的同时，通过自适应专家选择和基于专家相似性的智能聚合，实现了与集中式训练相当的机器人操作性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have significantly advanced roboticmanipulation by enabling robots to interpret language instructions for taskexecution. However, training these models often relies on large-scaleuser-specific data, raising concerns about privacy and security, which in turnlimits their broader adoption. To address this, we propose FedVLA, the firstfederated VLA learning framework, enabling distributed model training thatpreserves data privacy without compromising performance. Our frameworkintegrates task-aware representation learning, adaptive expert selection, andexpert-driven federated aggregation, enabling efficient and privacy-preservingtraining of VLA models. Specifically, we introduce an Instruction OrientedScene-Parsing mechanism, which decomposes and enhances object-level featuresbased on task instructions, improving contextual understanding. To effectivelylearn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE)mechanism, where not only input tokens but also self-aware experts adaptivelydecide their activation. Finally, we propose an Expert-Driven Aggregationstrategy at the federated server, where model aggregation is guided byactivated experts, ensuring effective cross-client knowledge transfer.Extensivesimulations and real-world robotic experiments demonstrate the effectiveness ofour proposals. Notably, DGMoE significantly improves computational efficiencycompared to its vanilla counterpart, while FedVLA achieves task success ratescomparable to centralized training, effectively preserving data privacy.</description>
      <author>example@mail.com (Cui Miao, Tao Chang, Meihan Wu, Hongbin Xu, Chun Li, Ming Li, Xiaodong Wang)</author>
      <guid isPermaLink="false">2508.02190v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.02172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, accepted by MM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GaussianCross，一种新的跨模态自监督3D表示学习架构，通过整合3D高斯溅射技术解决3D场景理解中的模型坍塌和结构信息不足问题，实现了优越的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;3D场景理解中，信息丰富且鲁棒的点表示的重要性已被广泛认可。尽管现有的自监督预训练方法显示出有前景的性能，但由于点区分难度不足，模型坍塌和结构信息不足的问题仍然普遍存在，导致不可靠的表达和次优性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态自监督3D表示学习架构，以解决当前3D场景理解中的模型坍塌和结构信息不足问题。&lt;h4&gt;方法&lt;/h4&gt;提出了GaussianCross，整合前馈3D高斯溅射技术，将尺度不一致的3D点云转换为统一的立方体归一化高斯表示，并加入三属性自适应蒸馏溅射模块构建3D特征场，促进外观、几何和语义线索的协同特征捕获。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，GaussianCross显示出显著的参数和数据效率，通过线性 probing（使用少于0.1%的参数）和有限数据训练（仅使用1%的场景）实现了优越性能；在ScanNet200的语义和实例分割任务上，完整微调将mIoU提高了9.3%，AP50提高了6.1%。&lt;h4&gt;结论&lt;/h4&gt;GaussianCross是一种有效的3D表示学习方法，能够解决现有方法中的模型坍塌和结构信息不足问题，并提供强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;信息丰富且鲁棒的点表示在3D场景理解中的重要性已被广泛认可。尽管现有的自监督预训练对应方法显示出有前景的性能，但由于点区分难度不足，模型坍塌和结构信息不足仍然普遍存在，导致不可靠的表达和次优性能。在本文中，我们提出了GaussianCross，一种新的跨模态自监督3D表示学习架构，整合前馈3D高斯溅射技术以解决当前挑战。GaussianCross无缝地将尺度不一致的3D点云转换为统一的立方体归一化高斯表示而不丢失细节，实现稳定且可泛化的预训练。随后，集成了一个三属性自适应蒸馏溅射模块来构建3D特征场，促进外观、几何和语义线索的协同特征捕获，以保持跨模态一致性。为了验证GaussianCross，我们在各种基准上进行了广泛评估，包括ScanNet、ScanNet200和S3DIS。特别是，GaussianCross显示出显著的参数和数据效率，通过线性 probing（使用少于0.1%的参数）和有限数据训练（仅使用1%的场景）与最先进的方法相比实现了优越性能。此外，GaussianCross展示了强大的泛化能力，在ScanNet200的语义和实例分割任务上，通过完整微调将mIoU提高了9.3%，AP50提高了6.1%，支持我们方法的有效性。代码、权重和可视化可在https://rayyoh.github.io/GaussianCross/公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景理解中自监督表示学习的挑战，特别是模型崩溃和结构信息不足的问题。这个问题在研究中很重要，因为3D数据稀缺且具有复杂空间结构，限制了有效表示学习策略的设计；在现实中，解决这一问题将减少对大量标注3D数据的依赖，提高3D场景理解的鲁棒性，推动自动驾驶、机器人导航和虚拟现实等领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自监督3D表示学习方法的局限性，包括对比学习方法的模型崩溃问题和神经渲染方法的训练效率低、忽略几何语义关系的问题。他们选择3D高斯溅射(3DGS)作为基础技术，但注意到其泛化能力不足。为此，他们设计了立方体归一化高斯初始化解决尺度不一致问题，并引入三属性自适应蒸馏溅射模块同时捕获外观、几何和语义信息。该方法借鉴了Ponder的NeRF预训练范式、GS3的3D高斯溅射技术、PointContrast的对比学习思想，以及视觉基础模型的知识蒸馏技术，但避免了这些方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云转换为结构化的3D高斯表示，实现尺度一致的场景表示，通过多模态自监督学习同时捕获外观、几何和语义信息，并利用视觉基础模型的知识蒸馏增强语义感知能力。整体流程包括：1)立方体归一化高斯初始化，将点云转换到单位立方体空间并体素化；2)三属性自适应蒸馏溅射，预测高斯属性并引入偏移量细化位置；3)多目标渲染与损失计算，同时优化光度重建、几何一致性和语义对齐；4)下游任务评估，包括线性探测、数据效率和完整微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)立方体归一化高斯初始化，解决不同场景间的尺度不一致问题；2)三属性自适应蒸馏溅射模块，同时捕获外观、几何和语义属性；3)跨模态知识蒸馏，利用预训练2D视觉基础模型的知识；4)多目标渲染策略，组合多种损失函数提供全面监督。相比之前的工作，该方法避免了对比学习方法的模型崩溃问题，提高了神经渲染方法的训练效率，扩展了现有3DGS方法仅关注光度重建的局限性，并在有限数据情况下也能取得较好性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GaussianCross通过创新的跨模态自监督学习框架，利用3D高斯溅射技术实现了对3D场景外观、几何和语义信息的联合建模，在多种3D场景理解任务上取得了最先进的性能，同时展现出卓越的参数和数据效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The significance of informative and robust point representations has beenwidely acknowledged for 3D scene understanding. Despite existingself-supervised pre-training counterparts demonstrating promising performance,the model collapse and structural information deficiency remain prevalent dueto insufficient point discrimination difficulty, yielding unreliableexpressions and suboptimal performance. In this paper, we presentGaussianCross, a novel cross-modal self-supervised 3D representation learningarchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniquesto address current challenges. GaussianCross seamlessly convertsscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussianrepresentation without missing details, enabling stable and generalizablepre-training. Subsequently, a tri-attribute adaptive distillation splattingmodule is incorporated to construct a 3D feature field, facilitating synergeticfeature capturing of appearance, geometry, and semantic cues to maintaincross-modal consistency. To validate GaussianCross, we perform extensiveevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. Inparticular, GaussianCross shows a prominent parameter and data efficiency,achieving superior performance through linear probing (&lt;0.1% parameters) andlimited data training (1% of scenes) compared to state-of-the-art methods.Furthermore, GaussianCross demonstrates strong generalization capabilities,improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ onScanNet200 semantic and instance segmentation tasks, respectively, supportingthe effectiveness of our approach. The code, weights, and visualizations arepublicly available at\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.</description>
      <author>example@mail.com (Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau)</author>
      <guid isPermaLink="false">2508.02172v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Structure Maintained Representation Learning Neural Network for Causal Inference</title>
      <link>http://arxiv.org/abs/2508.01865v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结构保持表征学习方法，通过引入结构保持器来提高表征学习和对抗网络在估计个体处理效应时的预测准确性。&lt;h4&gt;背景&lt;/h4&gt;因果推断的最新发展使研究兴趣从估计平均处理效应转向个体处理效应。&lt;h4&gt;目的&lt;/h4&gt;提高表征学习和对抗网络在估计个体处理效应时的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;在表征层末端训练一个判别器以平衡表征平衡性和信息损失，提出的方法最小化了处理估计误差的上界，通过考虑学习表征空间与原始协变量特征空间之间的相关性来解决分布平衡与信息损失之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;提出的结构保持表征学习算法在模拟和真实世界观察数据上的实验中优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;该算法在MIMIC-III数据库的真实电子健康记录数据上得到了验证。&lt;h4&gt;翻译&lt;/h4&gt;最近因果推断的发展极大地改变了研究兴趣，从估计平均处理效应转向个体处理效应。在本文中，我们通过引入结构保持器来提高表征学习和对抗网络在估计个体处理效应时的预测准确性，该结构保持器保持了基线协变量及其在高维空间中对应表征之间的相关性。我们在表征层末端训练一个判别器来平衡表征平衡性和信息损失。我们表明，所提出的判别器最小化了处理估计误差的上界。通过考虑学习表征空间与原始协变量特征空间之间的相关性，我们可以解决分布平衡与信息损失之间的权衡。我们使用模拟和真实世界观察数据进行了广泛的实验，表明我们提出的结构保持表征学习算法优于最先进的方法。我们还在MIMIC-III数据库的真实电子健康记录数据上展示了该算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments in causal inference have greatly shifted the interestfrom estimating the average treatment effect to the individual treatmenteffect. In this article, we improve the predictive accuracy of representationlearning and adversarial networks in estimating individual treatment effects byintroducing a structure keeper which maintains the correlation between thebaseline covariates and their corresponding representations in the highdimensional space. We train a discriminator at the end of representation layersto trade off representation balance and information loss. We show that theproposed discriminator minimizes an upper bound of the treatment estimationerror. We can address the tradeoff between distribution balance and informationloss by considering the correlations between the learned representation spaceand the original covariate feature space. We conduct extensive experiments withsimulated and real-world observational data to show that our proposed StructureMaintained Representation Learning (SMRL) algorithm outperformsstate-of-the-art methods. We also demonstrate the algorithms on real electronichealth record data from the MIMIC-III database.</description>
      <author>example@mail.com (Yang Sun, Wenbin Lu, Yi-Hui Zhou)</author>
      <guid isPermaLink="false">2508.01865v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>OmniEvent: Unified Event Representation Learning</title>
      <link>http://arxiv.org/abs/2508.01842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OmniEvent，第一个统一的事件表征学习框架，能够在多样化任务上实现最先进性能，完全消除了对特定任务设计的需求。&lt;h4&gt;背景&lt;/h4&gt;事件相机因其超高的动态范围和时间分辨率在计算机视觉中日益流行，但事件网络严重依赖特定任务的设计，因为数据分布是非结构化的，且存在时空不均匀性，这使得难以重用现有架构来完成新任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的事件表征学习框架，能够在多样化任务上实现最先进性能，完全消除对特定任务设计的需求。&lt;h4&gt;方法&lt;/h4&gt;提出解耦-增强-融合范式，在空间和时间域上独立进行局部特征聚合和增强以避免不均匀性问题；应用空间填充曲线实现大感受野同时提高内存和计算效率；使用注意力机制融合各域特征学习时空相互作用；输出网格形状张量使标准视觉模型能处理事件数据无需架构改变。&lt;h4&gt;主要发现&lt;/h4&gt;使用统一框架和相似超参数，OmniEvent在3个代表性任务和10个数据集上，比特定任务的SOTA方法性能提高高达68.2%。&lt;h4&gt;结论&lt;/h4&gt;OmniEvent解决了事件相机数据处理的时空不均匀性问题，提供了一种通用方法，无需针对不同任务进行专门设计，代码将在https://github.com/Wickyan/OmniEvent上提供。&lt;h4&gt;翻译&lt;/h4&gt;事件相机由于其超高的动态范围和时间分辨率，在计算机视觉中获得了越来越多的关注。然而，由于事件数据的非结构化分布和时空(S-T)不均匀性，事件网络严重依赖特定任务的设计，这使得难以重用现有架构来完成新任务。我们提出了OmniEvent，这是第一个统一的事件表征学习框架，能够在多样化任务上实现最先进性能，完全消除对特定任务设计的需求。与以往将事件数据视为具有手动调整时空缩放权重的3D点云的方法不同，OmniEvent提出了解耦-增强-融合范式，在空间和时间域上独立进行局部特征聚合和增强，以避免不均匀性问题。应用空间填充曲线能够实现大感受野，同时提高内存和计算效率。然后通过注意力机制融合各个域的特征，学习时空相互作用。OmniEvent的输出是一个网格形状的张量，使标准视觉模型能够处理事件数据而无需改变架构。使用统一的框架和相似的超参数，OmniEvent在3个代表性任务和10个数据集上，比特定任务的SOTA方法性能提高高达68.2%(图1)。代码将在https://github.com/Wickyan/OmniEvent上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras have gained increasing popularity in computer vision due totheir ultra-high dynamic range and temporal resolution. However, event networksheavily rely on task-specific designs due to the unstructured data distributionand spatial-temporal (S-T) inhomogeneity, making it hard to reuse existingarchitectures for new tasks. We propose OmniEvent, the first unified eventrepresentation learning framework that achieves SOTA performance across diversetasks, fully removing the need of task-specific designs. Unlike previousmethods that treat event data as 3D point clouds with manually tuned S-Tscaling weights, OmniEvent proposes a decouple-enhance-fuse paradigm, where thelocal feature aggregation and enhancement is done independently on the spatialand temporal domains to avoid inhomogeneity issues. Space-filling curves areapplied to enable large receptive fields while improving memory and computeefficiency. The features from individual domains are then fused by attention tolearn S-T interactions. The output of OmniEvent is a grid-shaped tensor, whichenables standard vision models to process event data without architecturechange. With a unified framework and similar hyper-parameters, OmniEventout-performs (tasks-specific) SOTA by up to 68.2% across 3 representative tasksand 10 datasets (Fig.1). Code will be ready inhttps://github.com/Wickyan/OmniEvent .</description>
      <author>example@mail.com (Weiqi Yan, Chenlu Lin, Youbiao Wang, Zhipeng Cai, Xiuhong Lin, Yangyang Shi, Weiquan Liu, Yu Zang)</author>
      <guid isPermaLink="false">2508.01842v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>ModFus-DM: Explore the Representation in Modulated Signal Diffusion Generated Models</title>
      <link>http://arxiv.org/abs/2508.01719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ModFus-DM的新型无监督AMC框架，通过扩散模型进行调制特征融合，解决了现有方法在大规模标记信号需求、非固定信号长度、分布偏移和标记信号有限等方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;自动调制分类(AMC)在军事和民用无线通信系统中都至关重要，但现有基于深度学习的方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在有限标记信号条件下处理非固定信号长度、分布偏移等挑战的AMC方法。&lt;h4&gt;方法&lt;/h4&gt;提出调制驱动特征融合的扩散模型(ModFus-DM)，包括调制信号扩散生成模型(MSDGM)和扩散感知特征融合(DAFFus)模块，通过渐进去噪过程捕获信号结构和语义信息，并自适应聚合多尺度扩散特征。&lt;h4&gt;主要发现&lt;/h4&gt;在RML2016.10A、RML2016.10B、RML2018.01A和RML2022数据集上的实验表明，ModFus-DM在有限标记设置、分布偏移、可变长度信号识别和信道衰减等场景下显著优于现有方法，在24类识别任务中，当信噪比≥12dB且每类只有10个标记信号时，准确率达到88.27%以上。&lt;h4&gt;结论&lt;/h4&gt;ModFus-DM通过利用扩散模型的生成能力，有效解决了AMC中的关键挑战，为实际应用提供了更稳健的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;自动调制分类(AMC)对于军事和民用应用中的无线通信系统至关重要。然而，现有的基于深度学习的AMC方法通常需要大量标记信号，并且难以处理非固定信号长度、分布偏移和标记信号有限等问题。为应对这些挑战，我们提出了通过扩散模型进行调制驱动的特征融合(ModFus-DM)，这是一种新颖的无监督AMC框架，它利用扩散模型的生成能力进行稳健的调制表征学习。我们设计了一个调制信号扩散生成模型(MSDGM)，通过渐进去噪过程隐式捕获结构和语义信息。此外，我们提出了扩散感知特征融合(DAFFus)模块，该模块自适应地聚合多尺度扩散特征以增强判别性表征。在RML2016.10A、RML2016.10B、RML2018.01A和RML2022数据集上的广泛实验表明，ModFus-DM在各种具有挑战性的场景下显著优于现有方法，例如有限标记设置、分布偏移、可变长度信号识别和信道衰减场景。值得注意的是，在24类识别任务中，当信噪比≥12dB且每类只有10个标记信号时，ModFus-DM的准确率超过88.27%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic modulation classification (AMC) is essential for wirelesscommunication systems in both military and civilian applications. However,existing deep learning-based AMC methods often require large labeled signalsand struggle with non-fixed signal lengths, distribution shifts, and limitedlabeled signals. To address these challenges, we propose a modulation-drivenfeature fusion via diffusion model (ModFus-DM), a novel unsupervised AMCframework that leverages the generative capacity of diffusion models for robustmodulation representation learning. We design a modulated signal diffusiongeneration model (MSDGM) to implicitly capture structural and semanticinformation through a progressive denoising process. Additionally, we proposethe diffusion-aware feature fusion (DAFFus) module, which adaptively aggregatesmulti-scale diffusion features to enhance discriminative representation.Extensive experiments on RML2016.10A, RML2016.10B, RML2018.01A and RML2022datasets demonstrate that ModFus-DM significantly outperforms existing methodsin various challenging scenarios, such as limited-label settings, distributionshifts, variable-length signal recognition and channel fading scenarios.Notably, ModFus-DM achieves over 88.27% accuracy in 24-type recognition tasksat SNR $\geq $ 12dB with only 10 labeled signals per type.</description>
      <author>example@mail.com (Haoyue Tan, Yu Li, Zhenxi Zhang, Xiaoran Shi, Feng Zhou)</author>
      <guid isPermaLink="false">2508.01719v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition</title>
      <link>http://arxiv.org/abs/2508.01644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in ACM Multimedia 2025. 10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DRKF(解耦表示与知识融合)的多模态情感识别方法，通过优化表示学习和知识融合两个主要模块解决了模态异质性和情感线索不一致性的挑战，在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态情感识别(MER)旨在通过整合和分析多种模态的信息来识别情感状态，但模态异质性和情感线索不一致性是阻碍性能提升的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决多模态情感识别中存在的模态异质性和情感线索不一致性问题，提高情感识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出DRKF方法，包含两个主要模块：1)优化表示学习(ORL)模块，使用对比互信息估计方法和渐进模态增强来解耦任务相关的共享表示和模态特定特征；2)知识融合(KF)模块，包含基于轻量级自注意力的融合编码器(FE)和情感判别子模块(ED)，用于识别主导模态、整合情感信息并处理情感不一致情况。&lt;h4&gt;主要发现&lt;/h4&gt;DRKF在IEMOCAP、MELD和M3ED数据集上实现了最先进的(SOTA)性能，有效解决了多模态情感识别中的关键挑战。&lt;h4&gt;结论&lt;/h4&gt;DRKF方法通过解耦表示和知识融合有效处理了模态异质性和情感不一致性问题，即使在不正确选择主导模态的情况下仍能保持准确预测，源代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;多模态情感识别(MER)旨在通过整合和分析多种模态的信息来识别情感状态。然而，固有的模态异质性和情感线索不一致性仍然是阻碍性能的关键挑战。为解决这些问题，我们为MER提出了一种解耦表示与知识融合(DRKF)方法。DRKF包含两个主要模块：优化表示学习(ORL)模块和知识融合(KF)模块。ORL采用对比互信息估计方法与渐进模态增强来解耦任务相关的共享表示和模态特定特征，同时减轻模态异质性。KF包含一个基于轻量级自注意力的融合编码器(FE)，该编码器识别主导模态并整合其他模态的情感信息以增强融合表示。为处理情感不一致条件下错误选择主导模态的潜在问题，我们引入了情感判别子模块(ED)，强制融合表示保留情感不一致性的判别线索。这确保即使FE选择了不适当的主导模态，情感分类子模块(EC)仍能通过利用保留的不一致性信息做出准确预测。实验表明，DRKF在IEMOCAP、MELD和M3ED上达到了最先进的性能。源代码已在https://github.com/PANPANKK/DRKF公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3754758&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal emotion recognition (MER) aims to identify emotional states byintegrating and analyzing information from multiple modalities. However,inherent modality heterogeneity and inconsistencies in emotional cues remainkey challenges that hinder performance. To address these issues, we propose aDecoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKFconsists of two main modules: an Optimized Representation Learning (ORL) Moduleand a Knowledge Fusion (KF) Module. ORL employs a contrastive mutualinformation estimation method with progressive modality augmentation todecouple task-relevant shared representations and modality-specific featureswhile mitigating modality heterogeneity. KF includes a lightweightself-attention-based Fusion Encoder (FE) that identifies the dominant modalityand integrates emotional information from other modalities to enhance the fusedrepresentation. To handle potential errors from incorrect dominant modalityselection under emotionally inconsistent conditions, we introduce an EmotionDiscrimination Submodule (ED), which enforces the fused representation toretain discriminative cues of emotional inconsistency. This ensures that evenif the FE selects an inappropriate dominant modality, the EmotionClassification Submodule (EC) can still make accurate predictions by leveragingpreserved inconsistency information. Experiments show that DRKF achievesstate-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source codeis publicly available at https://github.com/PANPANKK/DRKF.</description>
      <author>example@mail.com (Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, Daibing Yao)</author>
      <guid isPermaLink="false">2508.01644v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection</title>
      <link>http://arxiv.org/abs/2508.01525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACMMM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MiraGe的AI生成图像检测方法，通过学习生成器不变特征，实现了对已知和未知生成器的高效检测。&lt;h4&gt;背景&lt;/h4&gt;生成式模型的快速发展使得需要能够区分真实图像和AI生成图像的鲁棒检测器，现有方法在已知生成器上表现良好，但在面对新兴或未见过的生成模型时性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效区分真实图像和AI生成图像的检测方法，特别是在面对未知生成器时仍能保持高性能。&lt;h4&gt;方法&lt;/h4&gt;提出MiraGe方法，学习生成器不变特征，通过最小化类内变化和最大化类间分离来增强特征判别能力，并应用多模态提示学习将原则提炼到CLIP中，利用文本嵌入作为语义锚点。&lt;h4&gt;主要发现&lt;/h4&gt;MiraGe在多个基准测试上取得了最先进的性能，即使在对像Sora这样的未见过的生成器时也能保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MiraGe通过学习生成器不变特征和利用多模态提示学习，显著提高了AI生成图像检测的泛化能力和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;最近的生成模型进展凸显了对能够区分真实图像和AI生成图像的鲁棒检测器的需求。虽然现有方法在已知生成器上表现良好，但当用新兴或未见的生成模型测试时，其性能往往会下降，这是由于重叠的特征嵌入阻碍了准确的跨生成器分类。在本文中，我们提出了用于通用AI生成图像检测的多模态判别表示学习(MiraGe)，这是一种旨在学习生成器不变特征的方法。受最小化类内变化和最大化类间分离的理论见解启发，MiraGe紧密对齐同一类别内的特征，同时最大化类别之间的分离，增强特征判别能力。此外，我们将多模态提示学习应用于将这些原则进一步提炼到CLIP中，利用文本嵌入作为语义锚点进行有效的判别表示学习，从而提高泛化能力。在多个基准测试上的综合实验表明，MiraGe实现了最先进的性能，即使对像Sora这样的未见过的生成器也能保持鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generative models have highlighted the need for robustdetectors capable of distinguishing real images from AI-generated images. Whileexisting methods perform well on known generators, their performance oftendeclines when tested with newly emerging or unseen generative models due tooverlapping feature embeddings that hinder accurate cross-generatorclassification. In this paper, we propose Multimodal DiscriminativeRepresentation Learning for Generalizable AI-generated Image Detection(MiraGe), a method designed to learn generator-invariant features. Motivated bytheoretical insights on intra-class variation minimization and inter-classseparation, MiraGe tightly aligns features within the same class whilemaximizing separation between classes, enhancing feature discriminability.Moreover, we apply multimodal prompt learning to further refine theseprinciples into CLIP, leveraging text embeddings as semantic anchors foreffective discriminative representation learning, thereby improvinggeneralizability. Comprehensive experiments across multiple benchmarks showthat MiraGe achieves state-of-the-art performance, maintaining robustness evenagainst unseen generators like Sora.</description>
      <author>example@mail.com (Kuo Shi, Jie Lu, Shanshan Ye, Guangquan Zhang, Zhen Fang)</author>
      <guid isPermaLink="false">2508.01525v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians</title>
      <link>http://arxiv.org/abs/2508.01464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Can3Tok是首个3D场景级别的变分自编码器，能够将大量高斯基元编码为低维潜在嵌入，有效捕获输入的语义和空间信息，并通过通用数据处理流程解决了3D场景中的尺度不一致问题。&lt;h4&gt;背景&lt;/h4&gt;3D生成已取得显著进展，但仍主要停留在物体级别；前馈3D场景级别生成很少被探索，因为缺乏能够扩展3D场景级别数据潜在表示学习的模型；与物体级别生成模型不同，3D场景级别生成面临无边界的场景和不同场景间尺度不一致的挑战，使得统一的潜在表示学习变得极其困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种3D场景级别的变分自编码器(VAE)，能够将大量高斯基元编码为低维潜在嵌入，同时解决3D场景数据中的尺度不一致性问题。&lt;h4&gt;方法&lt;/h4&gt;提出Can3Tok，第一个3D场景级别的变分自编码器，设计了一种通用的3D场景数据处理流程来解决尺度不一致问题，使用3D高斯溅射(3DGS)表示3D场景。&lt;h4&gt;主要发现&lt;/h4&gt;在DL3DV-10K数据集上验证，只有Can3Tok成功推广到新的3D场景，而其他方法在训练几百个场景输入时无法收敛，并且在推理时表现出零泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Can3Tok能够有效捕获输入的语义和空间信息，展示了图像到3DGS和文本到3DGS生成的应用，证明了其促进下游生成任务的能力。&lt;h4&gt;翻译&lt;/h4&gt;三维生成已取得显著进展，然而，它仍然主要停留在物体级别。由于缺乏能够扩展三维场景级别数据潜在表示学习的模型，前馈三维场景级别生成很少被探索。与在有限规范空间中使用良好标记的三维数据进行训练的物体级别生成模型不同，使用三维高斯溅射表示的三维场景级别生成是无边界的，并且在不同场景之间存在尺度不一致性，这使得用于生成目的的统一潜在表示学习变得极其困难。在本文中，我们引入了Can3Tok，这是第一个三维场景级别的变分自编码器(VAE)，能够将大量高斯基元编码为低维潜在嵌入，有效捕获输入的语义和空间信息。除了模型设计外，我们还提出了一个通用的三维场景数据处理流程来解决尺度不一致问题。我们在最近的三维场景级别数据集DL3DV-10K上验证了我们的方法，发现只有Can3Tok成功推广到新的三维场景，而相比之下，其他方法在训练几百个场景输入时无法收敛，并且在推理时表现出零泛化能力。最后，我们展示了图像到三维高斯溅射和文本到三维高斯溅射的生成作为应用，以证明其促进下游生成任务的能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景级别的生成和表示学习问题。当前3D生成技术主要集中在物体级别，而完整的场景级别3D生成很少被探索，缺乏能在大规模3D场景数据上进行潜在表示学习的模型。这个问题很重要，因为它能推动AR/VR等沉浸式应用的发展，为游戏、电影制作等领域提供更高效的3D内容创建工具，而现有方法要么训练时间过长，要么缺乏3D一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D生成方法的局限性，识别出两个主要挑战：3DGS数据的高度无结构性和不同场景间的尺度不一致性。他们借鉴了变分自编码器(VAE)的思想、PerceiverIO的交叉注意力架构、图像处理中的归一化技术，以及文本分割模型(Segment Anything)来过滤噪声。设计上，作者创建了Can3Tok架构，结合了交叉注意力处理无结构数据、3DGS归一化解决尺度问题，以及语义过滤提高质量，这些都是对现有技术的创新应用和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将无结构的3D高斯溅射(3DGS)转换为结构化的3D标记，通过VAE框架学习低维潜在空间表示，同时捕捉语义和空间信息。整体流程包括：1)输入处理(归一化和语义过滤)；2)编码器(傅里叶位置编码、交叉注意力、自注意力)；3)潜在空间采样(使用VAE重参数化)；4)解码器(线性层、自注意力、多层感知器映射回3D空间)；5)训练目标(重建损失和KL散度)；6)应用(文本到3DGS和图像到3DGS生成)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Can3Tok架构，首个场景级别3DGS VAE，使用交叉注意力处理无结构数据；2)3DGS数据归一化方法，解决尺度不一致问题；3)语义感知过滤机制，提高重建质量；4)结构化潜在空间，同时保留语义和空间关系。相比之前工作，Can3Tok能处理数千个场景(其他方法仅能处理数百个)，在测试新场景时表现良好，而其他方法完全失效；它专门针对高度无结构的3DGS数据设计，而非传统点云；专注于整个场景而非单个物体的表示和生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Can3Tok首次实现了场景级别3D高斯溅射的标准化3D标记化和潜在建模，解决了大规模3D场景表示学习的尺度不一致问题，并成功应用于文本到3DGS和图像到3DGS的生成任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D generation has made significant progress, however, it still largelyremains at the object-level. Feedforward 3D scene-level generation has beenrarely explored due to the lack of models capable of scaling-up latentrepresentation learning on 3D scene-level data. Unlike object-level generativemodels, which are trained on well-labeled 3D data in a bounded canonical space,scene-level generations with 3D scenes represented by 3D Gaussian Splatting(3DGS) are unbounded and exhibit scale inconsistency across different scenes,making unified latent representation learning for generative purposes extremelychallenging. In this paper, we introduce Can3Tok, the first 3D scene-levelvariational autoencoder (VAE) capable of encoding a large number of Gaussianprimitives into a low-dimensional latent embedding, which effectively capturesboth semantic and spatial information of the inputs. Beyond model design, wepropose a general pipeline for 3D scene data processing to address scaleinconsistency issue. We validate our method on the recent scene-level 3Ddataset DL3DV-10K, where we found that only Can3Tok successfully generalizes tonovel 3D scenes, while compared methods fail to converge on even a few hundredscene inputs during training and exhibit zero generalization ability duringinference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation asour applications to demonstrate its ability to facilitate downstream generationtasks.</description>
      <author>example@mail.com (Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon)</author>
      <guid isPermaLink="false">2508.01464v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification</title>
      <link>http://arxiv.org/abs/2508.01427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SPECTRUM模型，一个时间-频率协同模型，用于在线手写验证(OHV)，通过多领域表示学习提高验证性能，包含三个核心组件：多尺度交互器、自门控融合模块和多域距离验证器。&lt;h4&gt;背景&lt;/h4&gt;在线手写验证领域需要更有效的特征表示方法来区分真实和伪造的手写，传统的基于时间的方法可能不够充分，而多领域学习可能未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出SPECTRUM模型，解锁多领域表示学习在在线手写验证中的未开发潜力，提高真实与伪造手写之间的区分度。&lt;h4&gt;方法&lt;/h4&gt;SPECTRUM包含三个核心组件：(1)多尺度交互器：通过双模态序列交互和多尺度聚合精细结合时间和频率特征；(2)自门控融合模块：通过自驱动平衡动态整合全局时间和频率特征；(3)多域距离验证器：利用时间和频率表示来提高真实和伪造手写之间的区分度。&lt;h4&gt;主要发现&lt;/h4&gt;SPECTRUM在OHV任务上表现优于现有方法；时间-频率多领域学习是有效的；整合多种手写生物特征可以增强手写表示的区分能力；多领域学习在OHV中有效，并为特征和生物识别领域的多领域方法研究铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;SPECTRUM模型通过时间-频率多领域协同学习显著提高了在线手写验证的性能，验证了多领域学习的有效性，并为未来研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了SPECTRUM，一个时间-频率协同模型，该模型解锁了多领域表示学习在在线手写验证中的未开发潜力。SPECTRUM包含三个核心组件：(1) 一个多尺度交互器，通过双模态序列交互和多尺度聚合精细结合时间和频率特征；(2) 一个自门控融合模块，通过自驱动平衡动态整合全局时间和频率特征。这两个组件协同工作，实现从微观到宏观的光谱-时间整合。(3) 一个基于多域距离的验证器随后利用时间和频率表示来提高真实和伪造手写之间的区分度，超越了传统仅基于时间的方法。大量实验证明了SPECTRUM相对于现有OHV方法的优越性能，强调了时间-频率多领域学习的有效性。此外，我们揭示整合多种手写生物特征可以从根本上增强手写表示的区分能力并促进验证。这些发现不仅验证了多领域学习在OHV中的有效性，也为未来在特征和生物识别领域的多领域方法研究铺平了道路。代码可在公开链接获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose SPECTRUM, a temporal-frequency synergistic modelthat unlocks the untapped potential of multi-domain representation learning foronline handwriting verification (OHV). SPECTRUM comprises three corecomponents: (1) a multi-scale interactor that finely combines temporal andfrequency features through dual-modal sequence interaction and multi-scaleaggregation, (2) a self-gated fusion module that dynamically integrates globaltemporal and frequency features via self-driven balancing. These two componentswork synergistically to achieve micro-to-macro spectral-temporal integration.(3) A multi-domain distance-based verifier then utilizes both temporal andfrequency representations to improve discrimination between genuine and forgedhandwriting, surpassing conventional temporal-only approaches. Extensiveexperiments demonstrate SPECTRUM's superior performance over existing OHVmethods, underscoring the effectiveness of temporal-frequency multi-domainlearning. Furthermore, we reveal that incorporating multiple handwrittenbiometrics fundamentally enhances the discriminative power of handwritingrepresentations and facilitates verification. These findings not only validatethe efficacy of multi-domain learning in OHV but also pave the way for futureresearch in multi-domain approaches across both feature and biometric domains.Code is publicly available at https://github.com/NiceRingNode/SPECTRUM.</description>
      <author>example@mail.com (Peirong Zhang, Kai Ding, Lianwen Jin)</author>
      <guid isPermaLink="false">2508.01427v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis</title>
      <link>http://arxiv.org/abs/2508.01292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoCoLIT的扩散模型框架，用于从结构MRI合成淀粉样蛋白PET扫描图像，为阿尔茨海默病的大规模筛查提供了一种经济有效的方法。&lt;h4&gt;背景&lt;/h4&gt;淀粉样蛋白PET扫描是阿尔茨海默病筛查的重要工具，但不如结构MRI广泛可用且成本高。虽然MRI不直接检测淀粉样蛋白病理，但它可能编码与淀粉样蛋白沉积相关的信息。现有的MRI到PET翻译方法面临3D神经影像数据高维度和结构复杂性的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从MRI合成高质量淀粉样蛋白PET图像的方法，用于大规模阿尔茨海默病筛查，同时提高合成质量和分类准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CoCoLIT（ControlNet-Conditioned Latent Image Translation），一种基于扩散的潜在生成框架，包含三个主要创新：加权图像空间损失（WISL）改善潜在表示学习和合成质量；对潜在平均稳定化（LAS）的理论和实证分析；基于ControlNet的条件化方法用于MRI到PET翻译。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集上评估，CoCoLIT在图像相关和淀粉样蛋白相关指标上都显著优于最先进的方法。在淀粉样蛋白阳性分类中，CoCoLIT比第二好的方法在内部分别集上提高了10.5%，在外部分别集上提高了23.7%。&lt;h4&gt;结论&lt;/h4&gt;CoCoLIT提供了一种有效且经济的方法，可以从MRI合成高质量的淀粉样蛋白PET图像，有助于阿尔茨海默病的大规模筛查。&lt;h4&gt;翻译&lt;/h4&gt;论文的代码和模型可在https://github.com/brAIn-science/CoCoLIT获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从广泛可用的结构MRI图像合成淀粉样蛋白PET图像的问题。这个问题很重要，因为淀粉样蛋白PET是早期阿尔茨海默病诊断的关键工具，但成本高、获取有限且有辐射暴露。而MRI更经济、无创且广泛使用，但对早期AD诊断效果较差。从MRI合成PET可以 enabling大规模、经济高效的AD筛查，特别是在资源有限地区。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有GAN方法存在训练不稳定和模式崩溃问题，而扩散模型虽能生成高质量图像但计算需求大。作者借鉴了潜在扩散模型(LDMs)、ControlNet调节机制和潜在平均稳定化(LAS)技术，设计了一个结合三个创新的框架：加权图像空间损失(WISL)、LAS的理论分析以及ControlNet条件用于MRI-PET转换。作者在低维潜在空间操作以简化学习任务，并利用ControlNet实现精确条件控制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型在潜在空间进行MRI到PET的转换，结合ControlNet实现精确条件控制，并使用加权图像空间损失和潜在平均稳定化技术提高合成质量。流程包括：(1)独立训练MRI和PET的VAE进行表示学习；(2)训练LDM学习潜在分布；(3)添加ControlNet模块学习条件分布；(4)引入WISL损失提高合成质量；(5)在推理中使用LAS技术，通过平均多个潜在样本提高效率而不损失质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)加权图像空间损失(WISL)，使用时间步长相关加权优先考虑不同频率特征的合成；(2)提供潜在平均稳定化(LAS)的理论分析和实证验证，证明其在充分训练模型中的有效性；(3)首次成功将ControlNet应用于MRI到PET转换。相比之前工作，CoCoLIT在潜在空间操作而非图像空间，减少计算需求；使用WISL而非简单图像损失，更好地与扩散过程保持一致；通过LAS提高推理效率；在图像质量和临床指标上均显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoCoLIT通过创新的ControlNet条件潜在图像转换框架，结合加权图像空间损失和潜在平均稳定化技术，实现了从MRI到淀粉样蛋白PET的高质量合成，显著提高了阿尔茨海默病早期筛查的准确性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthesizing amyloid PET scans from the more widely available and accessiblestructural MRI modality offers a promising, cost-effective approach forlarge-scale Alzheimer's Disease (AD) screening. This is motivated by evidencethat, while MRI does not directly detect amyloid pathology, it may nonethelessencode information correlated with amyloid deposition that can be uncoveredthrough advanced modeling. However, the high dimensionality and structuralcomplexity of 3D neuroimaging data pose significant challenges for existingMRI-to-PET translation methods. Modeling the cross-modality relationship in alower-dimensional latent space can simplify the learning task and enable moreeffective translation. As such, we present CoCoLIT (ControlNet-ConditionedLatent Image Translation), a diffusion-based latent generative framework thatincorporates three main innovations: (1) a novel Weighted Image Space Loss(WISL) that improves latent representation learning and synthesis quality; (2)a theoretical and empirical analysis of Latent Average Stabilization (LAS), anexisting technique used in similar generative models to enhance inferenceconsistency; and (3) the introduction of ControlNet-based conditioning forMRI-to-PET translation. We evaluate CoCoLIT's performance on publicly availabledatasets and find that our model significantly outperforms state-of-the-artmethods on both image-based and amyloid-related metrics. Notably, inamyloid-positivity classification, CoCoLIT outperforms the second-best methodwith improvements of +10.5% on the internal dataset and +23.7% on the externaldataset. The code and models of our approach are available athttps://github.com/brAIn-science/CoCoLIT.</description>
      <author>example@mail.com (Alec Sargood, Lemuel Puglisi, James H. Cole, Neil P. Oxtoby, Daniele Ravì, Daniel C. Alexander)</author>
      <guid isPermaLink="false">2508.01292v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Bioacoustics -- a Comparative Review</title>
      <link>http://arxiv.org/abs/2508.01277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对大规模预训练的生物声学基础模型进行了全面综述，并系统研究了它们在多个生物声学分类任务上的可迁移性。研究评估了不同模型在各种基准测试上的表现，并提供了模型选择和适应新任务的有价值指导。&lt;h4&gt;背景&lt;/h4&gt;自动生物声学分析对生物多样性监测和保护至关重要，需要先进的深度学习模型来适应各种生物声学任务。&lt;h4&gt;目的&lt;/h4&gt;对大规模预训练的生物声学基础模型进行全面综述，系统研究这些模型在多个生物声学分类任务上的可迁移性。&lt;h4&gt;方法&lt;/h4&gt;概述生物声学表示学习包括主要预训练数据来源和基准；通过分析模型架构、预训练方案和训练范式等设计决策回顾生物声学基础模型；在BEANS和BirdSet基准的分类任务上评估选定模型；比较在线性和注意力探测策略下学习表示的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;BirdMAE在大规模鸟类歌曲数据上以自监督目标训练，在BirdSet基准上表现最佳；在BEANS上，BEATs_NLM表现稍好；基于Transformer的模型需要注意力探测来提取其表示的全部性能；ConvNext_BS和Perch模型在BirdSet的被动声学监测分类任务中保持竞争力；训练新的线性分类器比不进一步训练评估模型有明显优势；在BEANS上，BEATs基线模型在使用注意力探测评估时优于鸟类特定模型。&lt;h4&gt;结论&lt;/h4&gt;这些发现为从业者选择适当的模型并通过探测将其适应新的生物声学分类任务提供了有价值的指导。&lt;h4&gt;翻译&lt;/h4&gt;自动生物声学分析对生物多样性监测和保护至关重要，需要能够适应各种生物声学任务的高级深度学习模型。本文对大规模预训练的生物声学基础模型进行了全面综述，并系统研究了它们在多个生物声学分类任务上的可迁移性。我们概述了生物声学表示学习，包括主要的预训练数据来源和基准。在此基础上，我们通过深入分析模型架构、预训练方案和训练范式等设计决策来回顾生物声学基础模型。此外，我们在BEANS和BirdSet基准的分类任务上评估了选定的基础模型，比较了在线性和注意力探测策略下学习表示的泛化能力。我们全面的实验分析表明，在大规模鸟类歌曲数据上以自监督目标训练的BirdMAE在BirdSet基准上取得了最佳性能。在BEANS上，NatureLM-audio大型音频模型的提取编码器BEATs_NLM表现稍好。两种基于Transformer的模型都需要注意力探测来提取其表示的全部性能。在大规模鸟类歌曲数据上监督训练的ConvNext_BS和Perch模型在BirdSet的线性探测设置中对于被动声学监测分类任务仍然具有竞争力。训练新的线性分类器比在不进一步训练的情况下评估这些模型有明显优势。而在BEANS上，当使用注意力探测评估时，在AudioSet上以自监督训练的基线模型BEATs优于鸟类特定模型。这些发现为从业者选择适当的模型并通过探测将其适应新的生物声学分类任务提供了有价值的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated bioacoustic analysis is essential for biodiversity monitoring andconservation, requiring advanced deep learning models that can adapt to diversebioacoustic tasks. This article presents a comprehensive review of large-scalepretrained bioacoustic foundation models and systematically investigates theirtransferability across multiple bioacoustic classification tasks. We overviewbioacoustic representation learning including major pretraining data sourcesand benchmarks. On this basis, we review bioacoustic foundation models bythoroughly analysing design decisions such as model architecture, pretrainingscheme, and training paradigm. Additionally, we evaluate selected foundationmodels on classification tasks from the BEANS and BirdSet benchmarks, comparingthe generalisability of learned representations under both linear and attentiveprobing strategies. Our comprehensive experimental analysis reveals thatBirdMAE, trained on large-scale bird song data with a self-supervisedobjective, achieves the best performance on the BirdSet benchmark. On BEANS,BEATs$_{NLM}$, the extracted encoder of the NatureLM-audio large audio model,is slightly better. Both transformer-based models require attentive probing toextract the full performance of their representations. ConvNext$_{BS}$ andPerch models trained with supervision on large-scale bird song data remaincompetitive for passive acoustic monitoring classification tasks of BirdSet inlinear probing settings. Training a new linear classifier has clear advantagesover evaluating these models without further training. While on BEANS, thebaseline model BEATs trained with self-supervision on AudioSet outperformsbird-specific models when evaluated with attentive probing. These findingsprovide valuable guidance for practitioners selecting appropriate models toadapt them to new bioacoustic classification tasks via probing.</description>
      <author>example@mail.com (Raphael Schwinger, Paria Vali Zadeh, Lukas Rauch, Mats Kurz, Tom Hauschild, Sam Lapp, Sven Tomforde)</author>
      <guid isPermaLink="false">2508.01277v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2508.01251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Soft Separation and Distillation (SSD)的新方法，用于解决联邦无监督学习中的全局均匀性问题。该方法通过鼓励客户端表示向不同方向扩展来减少模型聚合时的干扰，从而提高全局均匀性同时保持局部表示的表现力。&lt;h4&gt;背景&lt;/h4&gt;联邦无监督学习(FUL)旨在在联邦和自监督设置下学习表现力强的表示。现有方法在实现客户端(局部)均匀性方面表现良好，但由于非独立同分布数据分布和FUL的去中心化特性，在聚合后无法实现客户端间(全局)均匀性。&lt;h4&gt;目的&lt;/h4&gt;解决FUL中局部均匀性良好但全局均匀性不足的问题，提出一种能够保持客户端间均匀性的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出Soft Separation and Distillation (SSD)方法，通过鼓励客户端表示向不同方向扩展来保持客户端间的均匀性。此外，引入了投影蒸馏模块来解决损失优化与表示质量之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;在跨设施和跨设备的联邦设置下评估SSD，展示了在各种训练场景中表示质量和任务性能的一致性改进。&lt;h4&gt;结论&lt;/h4&gt;强调了客户端间均匀性在FUL中的重要性，并将SSD确立为解决这一挑战的有效方案。&lt;h4&gt;翻译&lt;/h4&gt;联邦无监督学习(FUL)旨在在联邦和自监督设置下学习表现力强的表示。FUL中学习到的表示质量通常由均匀性决定，即表示在嵌入空间中分布的均匀程度。然而，现有解决方案在实现客户端(局部)均匀性方面表现良好，但由于非独立同分布数据分布和FUL的去中心化特性，在聚合后无法实现客户端间(全局)均匀性。为了解决这个问题，我们提出了Soft Separation and Distillation (SSD)，一种新颖的方法，通过鼓励客户端表示向不同方向扩展来保持客户端间均匀性。这种设计减少了客户端模型聚合时的干扰，从而提高了全局均匀性，同时保持了局部表示的表现力。我们通过引入投影蒸馏模块进一步增强了这一效果，以解决损失优化与表示质量之间的差异。我们在跨设施和跨设备的联邦设置下评估了SSD，展示了在各种训练场景中表示质量和任务性能的一致性改进。我们的结果强调了客户端间均匀性在FUL中的重要性，并将SSD确立为解决这一挑战的有效方案。项目页面：https://ssd-uniformity.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Unsupervised Learning (FUL) aims to learn expressiverepresentations in federated and self-supervised settings. The quality ofrepresentations learned in FUL is usually determined by uniformity, a measureof how uniformly representations are distributed in the embedding space.However, existing solutions perform well in achieving intra-client (local)uniformity for local models while failing to achieve inter-client (global)uniformity after aggregation due to non-IID data distributions and thedecentralized nature of FUL. To address this issue, we propose Soft Separationand Distillation (SSD), a novel approach that preserves inter-client uniformityby encouraging client representations to spread toward different directions.This design reduces interference during client model aggregation, therebyimproving global uniformity while preserving local representationexpressiveness. We further enhance this effect by introducing a projectordistillation module to address the discrepancy between loss optimization andrepresentation quality. We evaluate SSD in both cross-silo and cross-devicefederated settings, demonstrating consistent improvements in representationquality and task performance across various training scenarios. Our resultshighlight the importance of inter-client uniformity in FUL and establish SSD asan effective solution to this challenge. Project page:https://ssd-uniformity.github.io/</description>
      <author>example@mail.com (Hung-Chieh Fang, Hsuan-Tien Lin, Irwin King, Yifei Zhang)</author>
      <guid isPermaLink="false">2508.01251v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning</title>
      <link>http://arxiv.org/abs/2508.01184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，通过学习可供性感知的3D表示和采用分阶段推理策略来解决Embodied AI中物体操作学习的问题，实验证明该方法在可供性定位和分类方面都有更好的性能。&lt;h4&gt;背景&lt;/h4&gt;Embodied AI的一个核心问题是像人类一样通过观察学习物体操作。这需要通过观察（如图像）来定位3D物体的可供性区域并理解其功能。&lt;h4&gt;目的&lt;/h4&gt;解决先前方法将可供性定位和分类任务分开处理导致预测不一致的问题，以及无法预测完整可供性区域和在固定尺度下操作难以处理尺度变化的可供性的问题。&lt;h4&gt;方法&lt;/h4&gt;开发跨模态3D表示通过高效融合和多尺度几何特征传播，能够在适当区域尺度上推断完整潜在可供性区域；采用两阶段预测机制有效耦合定位和分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了所提出方法的有效性，在可供性定位和分类方面都显示出改进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过结合跨模态3D表示和两阶段预测机制，所提出的方法解决了先前方法在处理可供性任务时面临的挑战，实现了更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Embodied AI的一个核心问题是像人类一样通过观察学习物体操作。为了实现这一点，通过观察如图像等来定位3D物体的可供性区域（3D可供性定位）并理解其功能（可供性分类）非常重要。先前的尝试通常分别处理这两个任务，由于缺乏对它们之间依赖关系的适当建模，导致预测不一致。此外，这些方法通常只定位图像中描绘的部分可供性区域，无法预测完整的潜在可供性区域，并且在固定尺度下操作，难以处理相对于整个物体尺度显著变化的可供性。为了解决这些问题，我们提出了一种新方法，学习可供性感知的3D表示，并利用定位和分类任务之间的依赖关系采用分阶段推理策略。具体来说，我们首先通过高效融合和多尺度几何特征传播开发跨模态3D表示，使得能够在适当的区域尺度上推断完整的潜在可供性区域。此外，我们采用简单的两阶段预测机制，有效耦合定位和分类，以更好地理解可供性。实验证明了我们方法的有效性，显示在可供性定位和分类方面都有性能提升。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身AI中的核心问题：如何从观察中学习物体操作，具体包括3D物体可供性区域定位（通过图像等观察定位3D物体的交互区域）和可供性分类（理解这些区域的功能）。这个问题在现实中非常重要，因为它使机器人能够像人类一样理解物体的交互方式和功能，对于机器人操作、人机交互、智能家居等领域至关重要，是实现智能体与物理世界有效交互的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的三大局限性：1）将定位和分类任务分开处理导致预测不一致；2）只能定位图像中显示的不完整可供性区域，无法预测完整区域；3）在固定尺度上操作，难以处理尺度变化大的可供性。基于这些分析，作者借鉴了跨模态学习、图神经网络和多尺度特征提取等现有技术，但创新性地将它们组合起来，设计了一个能同时处理两个任务、预测完整区域并适应多尺度的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个可供性感知的3D表示，并采用分阶段推理策略利用定位和分类任务间的依赖关系。整体流程分为四步：1）多模态特征提取：从图像提取上下文感知特征，从点云提取多尺度几何特征；2）跨模态融合：通过多头交叉注意力融合两个模态特征；3）传播和选择：用图神经网络传播特征并选择相关尺度；4）可供性预测：先预测定位概率掩码，再融合全局和局部特征预测类别。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）新的跨模态多尺度可供性表示，提高区域覆盖率；2）简单有效的级联策略，耦合定位和分类任务；3）几何特征传播模块，利用几何相似性传播特征；4）多尺度特征选择，适应不同大小的可供性区域。相比之前工作，本文同时处理两个任务并利用其依赖关系，预测完整而非部分可供性区域，使用多尺度特征处理不同尺度需求，并利用几何相似性提高定位准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于多尺度跨模态表示学习的创新方法，通过融合图像和点云信息并利用几何特征传播，实现了更准确的3D物体可供性区域定位和分类，显著提升了机器从观察中学习物体操作的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A core problem of Embodied AI is to learn object manipulation fromobservation, as humans do. To achieve this, it is important to localize 3Dobject affordance areas through observation such as images (3D affordancegrounding) and understand their functionalities (affordance classification).Previous attempts usually tackle these two tasks separately, leading toinconsistent predictions due to lacking proper modeling of their dependency. Inaddition, these methods typically only ground the incomplete affordance areasdepicted in images, failing to predict the full potential affordance areas, andoperate at a fixed scale, resulting in difficulty in coping with affordancessignificantly varying in scale with respect to the whole object. To addressthese issues, we propose a novel approach that learns an affordance-aware 3Drepresentation and employs a stage-wise inference strategy leveraging thedependency between grounding and classification tasks. Specifically, we firstdevelop a cross-modal 3D representation through efficient fusion andmulti-scale geometric feature propagation, enabling inference of full potentialaffordance areas at a suitable regional scale. Moreover, we adopt a simpletwo-stage prediction mechanism, effectively coupling grounding andclassification for better affordance understanding. Experiments demonstrate theeffectiveness of our method, showing improved performance in both affordancegrounding and classification.</description>
      <author>example@mail.com (Xinhang Wan, Dongqiang Gou, Xinwang Liu, En Zhu, Xuming He)</author>
      <guid isPermaLink="false">2508.01184v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Interaction Augmentation Network for Robust Multimodal Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2508.01168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的新型框架，用于解决多模态情感分析中因模态不完整性导致的挑战，通过利用模态内部和跨模态的交互作用，使模型能够从不完整样本中获取缺失语义。&lt;h4&gt;背景&lt;/h4&gt;真实场景中不可避免的模态不完整性对多模态情感分析构成重大挑战。现有方法通过重建或联合表示学习来恢复缺失语义，但往往忽略了模态内部和跨模态的复杂依赖关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用模态内部和跨模态交互作用的框架，使不完整样本能够从互补部分获取缺失语义，从而实现更稳健的多模态情感分析。&lt;h4&gt;方法&lt;/h4&gt;1) 设计可学习超图建模模态内部时间依赖关系，利用各模态内上下文信息；2) 采用基于注意力机制的有向图探索跨模态相关性，捕捉模态间互补信息；3) 整合完美样本知识监督交互过程，引导模型学习可靠稳健的联合表示。&lt;h4&gt;主要发现&lt;/h4&gt;在MOSI和MOSEI数据集上的大量实验证明了该方法的有效性，能够更好地处理模态不完整性问题并提升多模态情感分析的稳健性。&lt;h4&gt;结论&lt;/h4&gt;所提出的图框架能够有效处理模态不完整性，通过建模复杂的模态内部和跨模态交互关系，使模型能够从不完整数据中学习更全面、更鲁棒的表示。&lt;h4&gt;翻译&lt;/h4&gt;真实场景中不可避免的模态不完整性对多模态情感分析(MSA)构成了重大挑战。虽然现有方法通过调整重建或联合表示学习策略来恢复缺失的语义，但它们往往忽略了模态内部和跨模态的复杂依赖关系。因此，它们无法充分利用可用模态来捕捉互补语义。为此，本文提出了一种基于图的新型框架，利用模态内部和跨模态的交互作用，使不完整的样本能够从互补部分中获取缺失语义，从而实现稳健的MSA。具体来说，我们首先设计了一个可学习的超图来建模模态内部的时间依赖关系，以利用每个模态内的上下文信息。然后，采用有向图基于注意力机制探索跨模态相关性，捕捉不同模态间的互补信息。最后，将完美样本中的知识整合起来监督我们的交互过程，引导模型学习可靠且稳健的联合表示。在MOSI和MOSEI数据集上的大量实验证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The inevitable modality imperfection in real-world scenarios posessignificant challenges for Multimodal Sentiment Analysis (MSA). While existingmethods tailor reconstruction or joint representation learning strategies torestore missing semantics, they often overlook complex dependencies within andacross modalities. Consequently, they fail to fully leverage availablemodalities to capture complementary semantics. To this end, this paper proposesa novel graph-based framework to exploit both intra- and inter-modalityinteractions, enabling imperfect samples to derive missing semantics fromcomplementary parts for robust MSA. Specifically, we first devise a learnablehypergraph to model intra-modality temporal dependencies to exploit contextualinformation within each modality. Then, a directed graph is employed to exploreinter-modality correlations based on attention mechanism, capturingcomplementary information across different modalities. Finally, the knowledgefrom perfect samples is integrated to supervise our interaction processes,guiding the model toward learning reliable and robust joint representations.Extensive experiments on MOSI and MOSEI datasets demonstrate the effectivenessof our method.</description>
      <author>example@mail.com (Hu Zhangfeng, Shi mengxin)</author>
      <guid isPermaLink="false">2508.01168v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Dataset Condensation with Color Compensation</title>
      <link>http://arxiv.org/abs/2508.01139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了DC3框架，通过颜色补偿解决数据集压缩中的性能与保真度权衡问题，利用潜在扩散模型增强图像颜色多样性，实验证明其优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;数据集压缩面临在极端压缩下平衡性能和保真度的固有挑战。现有方法存在两个瓶颈：图像级选择方法（如Coreset Selection、Dataset Quantization）压缩效率低，像素级优化（如Dataset Distillation）因过参数化导致语义失真。&lt;h4&gt;目的&lt;/h4&gt;解决数据集压缩中的性能与保真度权衡问题，提高压缩图像的色彩度以增强表示学习。&lt;h4&gt;方法&lt;/h4&gt;提出DC3（带颜色补偿的数据集压缩框架），采用校准的选择策略，并利用潜在扩散模型增强图像颜色多样性而非创建全新图像。&lt;h4&gt;主要发现&lt;/h4&gt;颜色在数据集压缩中扮演双重角色（信息载体和基本语义表示单元）；提高压缩图像色彩度有利于表示学习；DC3在多个基准测试中优于最先进方法。&lt;h4&gt;结论&lt;/h4&gt;DC3框架有效解决了数据集压缩中的性能与保真度权衡问题；使用压缩数据集微调预训练扩散模型是可行的，不会导致模型崩溃或退化问题。&lt;h4&gt;翻译&lt;/h4&gt;数据集压缩总是面临一个固有的权衡：在极端压缩下平衡性能和保真度。现有方法面临两个瓶颈：图像级选择方法（Coreset Selection、Dataset Quantization）在压缩效率方面表现不佳，而像素级优化（Dataset Distillation）由于过参数化导致语义失真。通过经验观察，我们发现数据集压缩中的一个关键问题是忽视了颜色的双重作用：作为信息载体和基本语义表示单元。我们认为提高压缩图像的色彩度有利于表示学习。受此启发，我们提出了DC3：一种带颜色补偿的数据集压缩框架。经过校准的选择策略后，DC3利用潜在扩散模型增强图像的颜色多样性，而不是创建全新的图像。大量实验证明了DC3的优越性能和泛化能力，在多个基准测试中优于最先进的方法。据我们所知，除了关注下游任务外，DC3是第一个使用压缩数据集微调预训练扩散模型的研究。FID结果表明，使用我们的高质量数据集训练网络是可行的，不会出现模型崩溃或其他退化问题。代码和生成数据即将发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset condensation always faces a constitutive trade-off: balancingperformance and fidelity under extreme compression. Existing methods strugglewith two bottlenecks: image-level selection methods (Coreset Selection, DatasetQuantization) suffer from inefficiency condensation, while pixel-leveloptimization (Dataset Distillation) introduces semantic distortion due toover-parameterization. With empirical observations, we find that a criticalproblem in dataset condensation is the oversight of color's dual role as aninformation carrier and a basic semantic representation unit. We argue thatimproving the colorfulness of condensed images is beneficial for representationlearning. Motivated by this, we propose DC3: a Dataset Condensation frameworkwith Color Compensation. After a calibrated selection strategy, DC3 utilizesthe latent diffusion model to enhance the color diversity of an image ratherthan creating a brand-new one. Extensive experiments demonstrate the superiorperformance and generalization of DC3 that outperforms SOTA methods acrossmultiple benchmarks. To the best of our knowledge, besides focusing ondownstream tasks, DC3 is the first research to fine-tune pre-trained diffusionmodels with condensed datasets. The FID results prove that training networkswith our high-quality datasets is feasible without model collapse or otherdegradation issues. Code and generated data will be released soon.</description>
      <author>example@mail.com (Huyu Wu, Duo Su, Junjie Hou, Guang Li)</author>
      <guid isPermaLink="false">2508.01139v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation</title>
      <link>http://arxiv.org/abs/2508.01064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM Multimedia 2025. Code:  https://github.com/FengheTan9/Mobile-U-ViT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mobile U-shaped Vision Transformer (Mobile U-ViT)的轻量级移动模型，专门用于医疗图像分割，在资源受限设备上实现了高效且高性能的医疗图像分析。&lt;h4&gt;背景&lt;/h4&gt;临床实践中医疗图像分析需要在资源受限的移动设备上高效执行，但现有针对自然图像优化的移动模型在医疗任务上表现不佳，因为自然图像和医疗领域之间存在显著的信息密度差距。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、通用且高性能的网络，结合计算效率和医疗成像特有的架构优势，解决医疗图像在移动设备上的分析挑战。&lt;h4&gt;方法&lt;/h4&gt;提出Mobile U-ViT模型，采用ConvUtr作为分层块嵌入，引入大核局部-全局-局部(LGL)块促进信息交换，使用浅层轻量级transformer瓶颈进行长距离建模，并采用具有下采样跳跃连接的级联解码器进行密集预测。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在八个涵盖多种成像模态的公共2D和3D数据集上实现了最先进性能，包括在四个未见数据集上的零样本测试，证明了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Mobile U-ViT成为移动医疗图像分析的一种高效、强大且具有泛化能力的解决方案，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;在临床实践中，医疗图像分析通常需要在资源受限的移动设备上高效执行。然而，现有的移动模型主要针对自然图像优化，在医疗任务上表现不佳，因为自然图像和医疗领域之间存在显著的信息密度差距。开发结合计算效率与医疗成像特定架构优势的轻量级、通用且高性能网络仍然是一个挑战。为此，我们提出了一种名为Mobile U-shaped Vision Transformer (Mobile U-ViT)的移动模型，专为医疗图像分割而设计。具体而言，我们采用新提出的ConvUtr作为分层块嵌入，这是一种具有倒置瓶颈融合的参数高效大核CNN。该设计具有类似transformer的表示学习能力，同时更轻更快。为了实现高效的局部-全局信息交换，我们引入了一种新颖的大核局部-全局-局部(LGL)块，有效平衡了医疗图像的低信息密度和高层次语义差异。最后，我们采用浅层轻量级transformer瓶颈进行长距离建模，并使用具有下采样跳跃连接的级联解码器进行密集预测。尽管计算需求减少，我们的医疗优化架构在涵盖多种成像模态的八个公共2D和3D数据集上实现了最先进性能，包括在四个未见数据集上的零样本测试。这些结果确立了它作为移动医疗图像分析的一种高效且强大且具有泛化能力的解决方案。代码可在https://github.com/FengheTan9/Mobile-U-ViT获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何在资源受限的移动设备上高效执行医学图像分割任务，同时保持高准确性的问题。这个问题在现实中非常重要，因为临床实践中医学图像分析常需要在移动设备上实时执行，而现有为自然图像优化的移动模型在医学任务上表现不佳。医学图像分割对提高诊断效率至关重要，能为医生提供客观精确的参考，尤其在床旁成像干预和实时诊断等场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了医学图像的两个关键特性：稀疏局部信息和模糊边界高噪声水平。他们借鉴了CNN的高效性和Transformer的全局建模能力，结合深度可分离卷积(DSConv)等轻量级技术。针对医学图像特点，作者设计了ConvUtr块扩大感受野，并创建了LKLGL模块促进局部和全局信息交换。整个方法是基于对现有医学分割模型的批判性分析，结合CNN和Transformer优势，并针对医学图像特殊特性进行的专门优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合CNN的高效性和Transformer的全局建模能力，同时针对医学图像的特殊特性（稀疏局部信息、模糊边界和高噪声）进行专门优化，设计一个轻量级但高效的医学图像分割网络。整体流程包括：1)输入图像经过ConvUtr块进行特征提取；2)通过LKLGL块进行局部和全局信息交互；3)使用轻量级Transformer进行长程依赖建模；4)最后通过级联解码器生成分割结果，解码过程中使用下采样跳跃连接融合不同层次特征，过滤冗余信息，突出边界信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ConvUtr块 - 一种轻量级分层补丁嵌入，结合大核CNN和Transformer学习模式；2)LKLGL块 - 促进局部和全局信息交换的新型模块；3)级联解码器与下采样跳跃连接 - 特殊解码器结构，过滤冗余信息；4)针对医学图像特性的整体架构优化。相比之前工作，Mobile U-ViT专门针对医学图像特性优化，而非简单应用自然图像模型；巧妙结合CNN和Transformer优势；引入新组件解决特定挑战；整体设计更轻量，适合移动设备部署。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Mobile U-ViT通过创新的ConvUtr块和LKLGL模块，成功地将Transformer的全局建模能力与CNN的计算效率相结合，为医学图像分割提供了一个既轻量又高效的解决方案，特别适合在资源受限的移动设备上部署。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In clinical practice, medical image analysis often requires efficientexecution on resource-constrained mobile devices. However, existing mobilemodels-primarily optimized for natural images-tend to perform poorly on medicaltasks due to the significant information density gap between natural andmedical domains. Combining computational efficiency with medicalimaging-specific architectural advantages remains a challenge when developinglightweight, universal, and high-performing networks. To address this, wepropose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT)tailored for medical image segmentation. Specifically, we employ the newlypurposed ConvUtr as a hierarchical patch embedding, featuring aparameter-efficient large-kernel CNN with inverted bottleneck fusion. Thisdesign exhibits transformer-like representation learning capacity while beinglighter and faster. To enable efficient local-global information exchange, weintroduce a novel Large-kernel Local-Global-Local (LGL) block that effectivelybalances the low information density and high-level semantic discrepancy ofmedical images. Finally, we incorporate a shallow and lightweight transformerbottleneck for long-range modeling and employ a cascaded decoder withdownsample skip connections for dense prediction. Despite its reducedcomputational demands, our medical-optimized architecture achievesstate-of-the-art performance across eight public 2D and 3D datasets coveringdiverse imaging modalities, including zero-shot testing on four unseendatasets. These results establish it as an efficient yet powerful andgeneralization solution for mobile medical image analysis. Code is available athttps://github.com/FengheTan9/Mobile-U-ViT.</description>
      <author>example@mail.com (Fenghe Tang, Bingkun Nian, Jianrui Ding, Wenxin Ma, Quan Quan, Chengqi Dong, Jie Yang, Wei Liu, S. Kevin Zhou)</author>
      <guid isPermaLink="false">2508.01064v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning</title>
      <link>http://arxiv.org/abs/2508.01010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了van der Put神经网络(v-PuNNs)，一种基于p-adic数的新型深度学习架构，专门用于处理严格分层数据，在多个基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;传统深度学习模型将数据嵌入在欧几里得空间中，不适合处理如分类单元、词义或文件系统等严格分层的对象。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够精确表示和处理分层数据的深度学习架构，提供可解释且高效的模型。&lt;h4&gt;方法&lt;/h4&gt;提出van der Put神经网络(v-PuNNs)，其神经元是p-adic球的特征函数；在透明超度量表示学习(TURL)原则下，每个权重都是p-adic数；开发值自适应扰动优化(VAPO)方法解决离散空间中的梯度消失问题。&lt;h4&gt;主要发现&lt;/h4&gt;深度K的v-PuNN可普遍表示任何K级树；在WordNet名词、GO分子功能和NCBI哺乳动物分类等任务上取得最先进性能；学习到的度量是完全超度量的，具有特定的分形和信息论特性。&lt;h4&gt;结论&lt;/h4&gt;v-PuNNs连接了数论和深度学习，为分层数据提供了精确、可解释和高效的模型，并已扩展到量子系统和表格数据生成等应用。&lt;h4&gt;翻译&lt;/h4&gt;传统深度学习模型将数据嵌入在欧几里得空间中，对于严格分层的对象如分类单元、词义或文件系统来说，这不是一个好的拟合。我们引入了van der Put神经网络(v-PuNNs)，这是第一个神经元是p-adic球中的特征函数的架构。在我们的透明超度量表示学习(TURL)原则下，每个权重本身就是一个p-adic数，给出精确的子树语义。一个新的有限层次逼近定理表明，具有特定数量神经元的深度K的v-PuNN可以普遍表示任何K级树。由于梯度在离散空间中消失，我们提出了值自适应扰动优化(VAPO)，包括一个快速确定性变体和一个基于矩的变体。在三个标准基准测试中，我们的仅CPU实现设置了新的最先进水平：WordNet名词在短时间内达到高准确率；GO分子功能高准确率；NCBI哺乳动物与真实分类距离高度相关。学习到的度量是完全超度量的，并分析了其分形和信息论特性。除了分类外，作者还为量子系统推导了结构不变量和表格数据的可控生成代码。因此，v-PuNNs连接了数论和深度学习，为分层数据提供了精确、可解释和高效的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional deep learning models embed data in Euclidean space$\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, wordsenses, or file systems. We introduce van der Put Neural Networks (v-PuNNs),the first architecture whose neurons are characteristic functions of p-adicballs in $\mathbb{Z}_p$. Under our Transparent Ultrametric RepresentationLearning (TURL) principle every weight is itself a p-adic number, giving exactsubtree semantics. A new Finite Hierarchical Approximation Theorem shows that adepth-K v-PuNN with $\sum_{j=0}^{K-1}p^{\,j}$ neurons universally representsany K-level tree. Because gradients vanish in this discrete space, we proposeValuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministicvariant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On threecanonical benchmarks our CPU-only implementation sets new state-of-the-art:WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GOmolecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\rho= -0.96$ with true taxonomic distance. The learned metric is perfectlyultrametric (zero triangle violations), and its fractal andinformation-theoretic properties are analyzed. Beyond classification we derivestructural invariants for quantum systems (HiPaQ) and controllable generativecodes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory anddeep learning, offering exact, interpretable, and efficient models forhierarchical data.</description>
      <author>example@mail.com (Gnankan Landry Regis N'guessan)</author>
      <guid isPermaLink="false">2508.01010v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles</title>
      <link>http://arxiv.org/abs/2508.00969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MORPHEUS是一个统一的基于Transformer的预训练框架，能够将组织病理学和多组学数据编码到共享的潜在空间中，实现了跨模态学习和任意组学生成功能。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在计算病理学中取得重大进展，使模型能从H&amp;E染色癌症组织中学习丰富表征。然而，仅靠组织病理学不足以进行分子表征和理解临床结果，重要信息包含在转录组学、甲基化组学或基因组学等高维组学谱中。&lt;h4&gt;目的&lt;/h4&gt;引入MORPHEUS框架，整合组织病理学和多组学数据，实现跨模态学习和预测功能。&lt;h4&gt;方法&lt;/h4&gt;MORPHEUS应用掩码建模目标于随机选择的组学部分，鼓励模型学习有生物学意义的跨模态关系。同一预训练网络可单独或组合使用不同模态，并支持从一种或多种组学推断其他组学数据。&lt;h4&gt;主要发现&lt;/h4&gt;在大型泛癌队列上预训练后，MORPHEUS在各种模态组合和任务中始终优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;MORPHEUS是肿瘤学中开发多模态基础模型的有前景框架，其代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习通过使模型能够从苏木精-伊红(H&amp;E)染色的癌症组织中学习丰富的表征，推动了计算病理学的主要进展。然而，仅靠组织病理学往往不足以进行分子表征和理解临床结果，因为重要信息包含在转录组学、甲基化组学或基因组学等高维组学谱中。在这项工作中，我们引入了MORPHEUS，一个统一的基于Transformer的预训练框架，将组织病理学和多组学数据编码到共享的潜在空间中。其核心是MORPHEUS依赖于应用于随机选择的组学部分的掩码建模目标，鼓励模型学习有生物学意义的跨模态关系。相同的预训练网络可以单独应用于组织病理学或与任何组学模态子集结合应用，无缝适应可用输入。此外，MORPHEUS支持任意到任意组学生成，能够从一个或多个组学谱推断任何模态子集，包括仅从H&amp;E推断。在大型泛癌队列上预训练后，MORPHEUS在各种模态组合和任务中始终优于最先进的方法，将自己定位为肿瘤学中开发多模态基础模型的有前景框架。代码可在以下网址获取：https://github.com/Lucas-rbnt/MORPHEUS&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has driven major advances in computational pathologyby enabling models to learn rich representations from hematoxylin and eosin(H&amp;E)-stained cancer tissue. However, histopathology alone often falls shortfor molecular characterization and understanding clinical outcomes, asimportant information is contained in high-dimensional omics profiles liketranscriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS,a unified transformer-based pre-training framework that encodes bothhistopathology and multi-omics data into a shared latent space. At its core,MORPHEUS relies on a masked modeling objective applied to randomly selectedomics portions, encouraging the model to learn biologically meaningfulcross-modal relationships. The same pre-trained network can be applied tohistopathology alone or in combination with any subset of omics modalities,seamlessly adapting to the available inputs. Additionally, MORPHEUS enablesany-to-any omics generation, enabling one or more omics profiles to be inferredfrom any subset of modalities, including H&amp;E alone. Pre-trained on a largepan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methodsacross diverse modality combinations and tasks, positioning itself as apromising framework for developing multimodal foundation models in oncology.The code is available at: https://github.com/Lucas-rbnt/MORPHEUS</description>
      <author>example@mail.com (Lucas Robinet, Ahmad Berjaoui, Elizabeth Cohen-Jonathan Moyal)</author>
      <guid isPermaLink="false">2508.00969v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Unified User Quantized Tokenizers for User Representation</title>
      <link>http://arxiv.org/abs/2508.00956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为U²QT的新型框架，用于多源用户表示学习，通过整合跨领域知识转移和异构领域的早期融合，解决了现有方法在统一表示框架、数据压缩的扩展性和存储问题以及跨任务泛化灵活性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;多源用户表示学习在为网络平台（如支付宝）提供个性化服务方面起着关键作用。先前的研究采用后期融合策略组合异构数据源，但存在三个主要局限：缺乏统一的表示框架、数据压缩中的扩展性和存储问题，以及不灵活的跨任务泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有多源用户表示学习方法中的三个关键局限：缺乏统一表示框架、数据压缩的扩展性和存储问题、以及不灵活的跨任务泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出U²QT框架，采用两阶段架构：首先，因果Q-Former将领域特定特征投影到共享的因果表示空间，保留模态间依赖关系；其次，多视图RQ-VAE通过共享和特定于源的码本将因果嵌入离散化为紧凑的标记，实现高效存储同时保持语义连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，U²QT在各种下游任务中具有优势，在未来的行为预测和推荐任务中优于特定任务的基线，同时在存储和计算方面实现了效率提升。&lt;h4&gt;结论&lt;/h4&gt;统一的标记化框架能够与语言模型无缝集成，并支持工业规模的应用。&lt;h4&gt;翻译&lt;/h4&gt;多源用户表示学习在网络平台（例如支付宝）上实现个性化服务方面起着关键作用。虽然先前的研究采用了后期融合策略来组合异构数据源，但它们存在三个主要局限：缺乏统一的表示框架、数据压缩中的扩展性和存储问题，以及不灵活的跨任务泛化能力。为了解决这些挑战，我们提出了U²QT，一种将跨领域知识转移与异构领域早期融合相结合的新框架。我们的框架采用两阶段架构：首先，因果Q-Former将领域特定特征投影到共享的因果表示空间，以保留模态间依赖关系；其次，多视图RQ-VAE通过共享和特定于源的码本将因果嵌入离散化为紧凑的标记，实现高效存储同时保持语义连贯性。实验结果展示了U²QT在各种下游任务中的优势，在未来的行为预测和推荐任务中优于特定任务的基线，同时在存储和计算方面实现了效率提升。统一的标记化框架能够与语言模型无缝集成，并支持工业规模的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-source user representation learning plays a critical role in enablingpersonalized services on web platforms (e.g., Alipay). While prior works haveadopted late-fusion strategies to combine heterogeneous data sources, theysuffer from three key limitations: lack of unified representation frameworks,scalability and storage issues in data compression, and inflexible cross-taskgeneralization. To address these challenges, we propose U^2QT (Unified UserQuantized Tokenizers), a novel framework that integrates cross-domain knowledgetransfer with early fusion of heterogeneous domains. Our framework employs atwo-stage architecture: first, a causal Q-Former projects domain-specificfeatures into a shared causal representation space to preserve inter-modalitydependencies; second, a multi-view RQ-VAE discretizes causal embeddings intocompact tokens through shared and source-specific codebooks, enabling efficientstorage while maintaining semantic coherence. Experimental results showcaseU^2QT's advantages across diverse downstream tasks, outperforming task-specificbaselines in future behavior prediction and recommendation tasks whileachieving efficiency gains in storage and computation. The unified tokenizationframework enables seamless integration with language models and supportsindustrial-scale applications.</description>
      <author>example@mail.com (Chuan He, Yang Chen, Wuliang Huang, Tianyi Zheng, Jianhu Chen, Bin Dou, Yice Luo, Yun Zhu, Baokun Wang, Yongchao Liu, Xing Fu, Yu Cheng, Chuntao Hong, Weiqiang Wang, Xin-Wei Yao)</author>
      <guid isPermaLink="false">2508.00956v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model</title>
      <link>http://arxiv.org/abs/2508.00955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的多模态嵌入框架，通过分层嵌入提示模板和自觉困难负样本采样两个组件，成功将多模态大语言模型的生成性质适应到判别表示学习中，无需大规模对比预训练即可达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已成为通用嵌入任务的解决方案，但将其生成性质适应判别表示学习仍面临挑战。大规模对比预训练存在计算成本高和无法利用MLLM固有指令跟随能力等关键低效率问题。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一个高效框架用于通用多模态嵌入，解决多模态大语言模型在判别表示学习中的适应问题。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个协同组件的框架：1) 分层嵌入提示模板，采用两级指令架构强制模型生成判别性表示；2) 自觉困难负样本采样，利用模型自身理解高效挖掘挑战性负样本并过滤潜在假负样本。&lt;h4&gt;主要发现&lt;/h4&gt;1) 分层提示实现了与对比训练基线相当的零样本性能；2) 在MMEB基准测试上，通过简单的批次内负样本基线提升了4.8个百分点；3) 结合自觉困难负样本采样，无需对比预训练就达到了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;该工作为将多模态大语言模型适应通用嵌入任务提供了有效且高效的途径，显著减少了训练时间。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型已成为通用嵌入任务的解决方案，但将其生成性质适应判别表示学习仍然是一个重大挑战。大规模对比预训练主导范式存在关键低效率问题，包括难以承受的计算成本和无法利用多模态大语言模型的固有指令跟随能力。为了克服这些限制，我们提出了一个用于通用多模态嵌入的高效框架，通过聚焦两个协同组件弥合这一差距。首先，我们的分层嵌入提示模板采用两级指令架构，强制模型生成判别性表示。基于这一坚实基础，我们的第二个组件——自觉困难负样本采样，通过利用模型自身的理解重新定义了微调过程，高效挖掘挑战性负样本同时主动过滤潜在假负样本。我们的全面实验表明，我们的分层提示实现了与对比训练基线相当的零样本性能，并通过在MMEB基准测试上将简单的批次内负样本基线提升4.8个百分点来增强微调过程。我们通过自觉困难负样本采样进一步提升了性能，无需对比预训练就达到了最先进性能。我们的工作为将多模态大语言模型适应通用嵌入任务提供了一条有效且高效的途径，显著减少了训练时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have emerged as a promising solutionfor universal embedding tasks, yet adapting their generative nature fordiscriminative representation learning remains a significant challenge. Thedominant paradigm of large-scale contrastive pre-training suffers from criticalinefficiencies, including prohibitive computational costs and a failure toleverage the intrinsic, instruction-following capabilities of MLLMs. Toovercome these limitations, we propose an efficient framework for universalmultimodal embeddings, which bridges this gap by centering on two synergisticcomponents. First, our hierarchical embedding prompt template employs atwo-level instruction architecture that forces the model to producediscriminative representations. Building on this strong foundation, our secondcomponent, self-aware hard negative sampling, redefines the fine-tuning processby leveraging the model's own understanding to efficiently mine challengingnegatives while actively filtering out potential false negatives. Ourcomprehensive experiments show that our hierarchical prompt achieves zero-shotperformance competitive with contrastively trained baselines and enhances thefine-tuning process by lifting a simple in-batch negative baseline by 4.8points on the MMEB benchmark. We further boost the performance via ourself-aware hard negative sampling, achieving the state-of-the-art performancewithout the contrative pre-training. Our work presents an effective andefficient pathway to adapt MLLMs for universal embedding tasks, significantlyreducing training time.</description>
      <author>example@mail.com (Yeong-Joon Ju, Seong-Whan Lee)</author>
      <guid isPermaLink="false">2508.00955v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2508.02187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于矩匹配的点云配准框架，在稀疏点和重噪声条件下实现了比传统方法更高的准确性和鲁棒性，并成功应用于4D雷达SLAM系统。&lt;h4&gt;背景&lt;/h4&gt;点云配准是机器人感知任务中的关键步骤，如同时定位与建图（SLAM）。在稀疏点和重噪声条件下，传统配准方法如迭代最近点（ICP）和正态分布变换（NDT）难以实现鲁棒且准确的配准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理稀疏点和重噪声条件的鲁棒点云配准方法，提高SLAM系统在挑战性环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;将点云视为从源帧和目标帧中观察到的同一分布的独立同分布样本，通过匹配广义高斯径向基矩来估计两帧之间的刚体变换，无需显式点对点对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;合成和真实数据集实验表明，该方法比现有方法具有更高的准确性和鲁棒性；集成到4D雷达SLAM系统后显著提高了定位性能，结果与基于激光雷达的系统相当。&lt;h4&gt;结论&lt;/h4&gt;矩匹配技术在稀疏和噪声场景中的鲁棒点云配准具有潜力，可为机器人感知任务提供有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;点云配准是机器人感知任务中的关键步骤，如同时定位与建图（SLAM）。在稀疏点和重噪声条件下，点云配准特别具有挑战性。传统配准方法，如迭代最近点（ICP）和正态分布变换（NDT），通常难以在这些条件下实现鲁棒且准确的配准。在本文中，我们提出了一种基于矩匹配的配准框架。特别是，点云被视为从源帧和目标帧中观察到的同一分布的独立同分布样本。然后，我们匹配从点云计算出的广义高斯径向基矩来估计两帧之间的刚体变换。此外，该方法不需要点云之间的显式点对点对应关系。我们进一步证明了所提出方法的一致性。在合成和真实世界数据集上的实验表明，我们的方法比现有方法实现了更高的准确性和鲁棒性。此外，我们将我们的框架集成到4D雷达SLAM系统中。所提出的方法显著提高了定位性能，并实现了与基于激光雷达的系统相当的结果。这些发现证明了矩匹配技术在稀疏和噪声场景中鲁棒点云配准的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准在稀疏点和噪声条件下的挑战问题。这个问题很重要，因为点云配准是机器人感知和自动驾驶中的关键步骤，而实际传感器数据常常是稀疏和含噪声的，特别是在使用4D雷达等传感器时。解决这一问题可以显著改善定位性能，使基于雷达的系统能够达到与基于LiDAR的系统相当的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法如ICP和NDT在稀疏和噪声条件下的局限性，然后提出不依赖点对点对应关系或局部密度估计的配准框架。他们将点云视为从同一分布中抽取的样本，通过匹配广义矩来估计变换。作者借鉴了统计学中的矩匹配方法、广义矩估计理论、机器学习中的径向基函数、数值优化中的BFGS方法以及GPU并行计算技术，但将这些元素创新性地组合应用于点云配准问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云视为从同一环境对象分布中抽取的独立同分布样本，通过计算和匹配点云的广义矩(特别是使用高斯径向基函数构造的矩)来估计两个视图之间的刚性变换，不依赖于显式的点对点对应关系。整体流程包括：1)根据点云密度自适应选择高斯RBF核的中心点；2)计算目标点云的各阶矩；3)初始化变换参数；4)计算变换后源点云的矩并与目标点云矩比较；5)使用BFGS方法优化变换参数以最小化矩差异；6)迭代直至收敛并输出最终变换参数。整个流程通过CUDA并行计算实现加速。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于矩匹配的全新配准框架，不依赖点对点对应关系；2)首次证明了算法的统计一致性，提供了对噪声的稳健保证；3)设计了自适应核函数选择策略，根据点云密度选择中心点；4)实现了CUDA并行化加速。相比传统方法，它不依赖点对应关系和局部统计估计；相比基于特征的方法，它不依赖局部特征提取且计算效率更高；相比基于学习的方法，它不需要大规模训练数据且泛化能力更强；相比其他矩匹配方法，它专门针对点云设计并提供理论保证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于矩匹配的点云配准方法，通过匹配高斯径向基函数构造的广义矩，实现了在稀疏和噪声条件下的稳健配准，无需显式的点对点对应关系，并在理论和实验上证明了其优越性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a key step in robotic perception tasks, such asSimultaneous Localization and Mapping (SLAM). It is especially challenging inconditions with sparse points and heavy noise. Traditional registrationmethods, such as Iterative Closest Point (ICP) and Normal DistributionsTransform (NDT), often have difficulties in achieving a robust and accuratealignment under these conditions. In this paper, we propose a registrationframework based on moment matching. In particular, the point clouds areregarded as i.i.d. samples drawn from the same distribution observed in thesource and target frames. We then match the generalized Gaussian Radial Basismoments calculated from the point clouds to estimate the rigid transformationbetween two frames. Moreover, such method does not require explicitpoint-to-point correspondences among the point clouds. We further show theconsistency of the proposed method. Experiments on synthetic and real-worlddatasets show that our approach achieves higher accuracy and robustness thanexisting methods. In addition, we integrate our framework into a 4D Radar SLAMsystem. The proposed method significantly improves the localization performanceand achieves results comparable to LiDAR-based systems. These findingsdemonstrate the potential of moment matching technique for robust point cloudregistration in sparse and noisy scenarios.</description>
      <author>example@mail.com (Xingyi Li, Han Zhang, Ziliang Wang, Yukai Yang, Weidong Chen)</author>
      <guid isPermaLink="false">2508.02187v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference</title>
      <link>http://arxiv.org/abs/2508.02134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  published in ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Free-MoRef的无训练方法，用于解决视频多模态大型语言模型在长视频理解中的上下文长度限制问题，通过分割和融合长视觉token序列，实现了高效的长视频理解。&lt;h4&gt;背景&lt;/h4&gt;Video-MLLM在视频理解任务中取得了显著进展，但受限于底层LLM的上下文长度，现有模型在长视频场景中表现不佳。常见解决方案如token压缩和流推理技术会牺牲特征粒度或推理效率。&lt;h4&gt;目的&lt;/h4&gt;高效实现对更长帧输入的全面理解，在不牺牲特征粒度和推理效率的情况下处理长视频场景。&lt;h4&gt;方法&lt;/h4&gt;受MoE启发，提出Free-MoRef方法：1)将视觉token重构为几个短序列作为多参考；2)引入MoRef-attention并行收集多参考块线索；3)在LLM阴影层后添加参考融合步骤，组合关键token形成最终混合推理序列。&lt;h4&gt;主要发现&lt;/h4&gt;在VideoMME、MLVU、LongVideoBench等数据集上，Free-MoRef能够在不压缩的情况下处理2-8倍更长的输入帧，在单个A100 GPU上保持即时响应，性能甚至超过专门训练的长视频MLLM。&lt;h4&gt;结论&lt;/h4&gt;Free-MoRef通过分割和融合长视觉token序列，在更低的计算成本下实现了显著性能提升，为长视频理解提供了高效有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视频多模态大型语言模型(Video-MLLM)在视频理解任务中取得了显著进展。然而，受限于底层LLM的上下文长度限制，现有Video-MLLM通常在长视频场景中表现不佳。为了理解扩展的输入帧，常见的解决方案包括token压缩和流推理技术，这些方法牺牲了特征粒度或推理效率。不同地，为了高效实现对更长帧输入的全面理解，我们从MoE中汲取灵感，提出了一种无训练方法Free-MoRef，该方法在一个推理过程中即时复用Video-MLLM的上下文感知能力。具体而言，Free-MoRef将视觉token重构为几个短序列作为多参考。随后，我们引入MoRef-attention，并行从多参考块中收集线索以总结统一的查询激活。在LLM的阴影层后，推导出一个参考融合步骤，组合来自并行块的关键token构成最终混合推理序列，这补偿了MoRef-attention中被忽略的跨参考视觉交互。通过分割和融合长视觉token序列，Free-MoRef在推理复用上下文长度方面以更低的计算成本实现了改进的性能，展现出强大的效率和有效性。在VideoMME、MLVU、LongVideoBench上的实验表明，Free-MoRef能够在不进行压缩的情况下，在单个A100 GPU上完全感知2倍到8倍更长的输入帧，同时保持即时响应，从而带来显著的性能提升，甚至超过了专门训练的长视频MLLM。代码可在https://github.com/wkfdb/Free-MoRef获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkableadvancements in video understanding tasks. However, constrained by the contextlength limitation in the underlying LLMs, existing Video-MLLMs typicallyexhibit suboptimal performance on long video scenarios. To understand extendedinput frames, common solutions span token compression and streaming inferencetechniques, which sacrifice feature granularity or inference efficiency.Differently, to efficiently achieve comprehensive understanding of longer frameinputs, we draw ideas from MoE and propose a training-free approach\textbf{Free-MoRef}, which instantly multiplexes the context perceptioncapabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRefreconstructs the vision tokens into several short sequences asmulti-references. Subsequently, we introduce MoRef-attention, which gathersclues from the multi-reference chunks in parallel to summarize unified queryactivations. After the shadow layers in LLMs, a reference fusion step isderived to compose a final mixed reasoning sequence with key tokens fromparallel chunks, which compensates the cross-reference vision interactions thatare neglected in MoRef-attention. By splitting and fusing the long vision tokensequences, Free-MoRef achieves improved performance under much lower computingcosts in reasoning multiplexed context length, demonstrating strong efficiencyand effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show thatFree-MoRef achieves full perception of 2$\times$ to 8$\times$ longer inputframes without compression on a single A100 GPU while keeping instantresponses, thereby bringing significant performance gains, even surpassingdedicatedly trained long-video-MLLMs. Codes are available athttps://github.com/wkfdb/Free-MoRef</description>
      <author>example@mail.com (Kuo Wang, Quanlong Zheng, Junlin Xie, Yanhao Zhang, Jinguo Luo, Haonan Lu, Liang Lin, Fan Zhou, Guanbin Li)</author>
      <guid isPermaLink="false">2508.02134v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding</title>
      <link>http://arxiv.org/abs/2508.01875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为StreamAgent的实时流视频理解方法，通过预测未来任务相关信息和优化记忆机制，提升了响应准确性和实时效率。&lt;h4&gt;背景&lt;/h4&gt;实时流视频理解在自动驾驶和智能监控等领域带来了挑战，需要基于动态演化的视觉内容进行持续感知、主动决策和响应式交互，这超越了传统离线视频处理的范畴。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法缺乏任务驱动规划和未来预测的问题，本文提出StreamAgent以实现主动和目标驱动的响应。&lt;h4&gt;方法&lt;/h4&gt;StreamAgent通过预期代理整合问题语义和历史观察，预测关键事件的时间进展，并将当前观察与预期未来证据对齐，同时调整感知动作。此外，设计了流式KV缓存记忆机制，构建分层记忆结构实现高效语义检索，减少存储开销。&lt;h4&gt;主要发现&lt;/h4&gt;在流式和长视频理解任务上的实验表明，该方法在响应准确性和实时效率方面优于现有方法，具有现实世界流式场景的实用价值。&lt;h4&gt;结论&lt;/h4&gt;StreamAgent通过预测未来时空区间和优化记忆机制，有效提升了实时流视频理解中的响应准确性和实时效率。&lt;h4&gt;翻译&lt;/h4&gt;实时流视频理解在自动驾驶和智能监控等领域带来了超越传统离线视频处理的挑战，需要基于动态演化的视觉内容进行持续感知、主动决策和响应式交互。然而，现有方法依赖于交替的感知-反应或异步触发，缺乏任务驱动的规划和未来预测，这限制了它们在演化视频流中的实时响应性和主动决策能力。为此，我们提出了StreamAgent，它可以预测包含未来任务相关信息的时空区间，以实现主动和目标驱动的响应。具体来说，我们通过提示预期代理来整合问题语义和历史观察，以预测关键事件的时间进展，将当前观察与预期的未来证据对齐，并随后调整感知动作（例如，关注任务相关区域或在后续帧中持续跟踪）。为了实现高效推理，我们设计了一个流式KV缓存记忆机制，它构建了一个分层记忆结构，用于选择性回忆相关标记，实现高效的语义检索，同时减少了传统KV缓存中存储所有标记的开销。在流式和长视频理解任务上的广泛实验表明，我们的方法在响应准确性和实时效率方面优于现有方法，突显了其在现实世界流式场景中的实际价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time streaming video understanding in domains such as autonomous drivingand intelligent surveillance poses challenges beyond conventional offline videoprocessing, requiring continuous perception, proactive decision making, andresponsive interaction based on dynamically evolving visual content. However,existing methods rely on alternating perception-reaction or asynchronoustriggers, lacking task-driven planning and future anticipation, which limitstheir real-time responsiveness and proactive decision making in evolving videostreams. To this end, we propose a StreamAgent that anticipates the temporalintervals and spatial regions expected to contain future task-relevantinformation to enable proactive and goal-driven responses. Specifically, weintegrate question semantics and historical observations through prompting theanticipatory agent to anticipate the temporal progression of key events, aligncurrent observations with the expected future evidence, and subsequently adjustthe perception action (e.g., attending to task-relevant regions or continuouslytracking in subsequent frames). To enable efficient inference, we design astreaming KV-cache memory mechanism that constructs a hierarchical memorystructure for selective recall of relevant tokens, enabling efficient semanticretrieval while reducing the overhead of storing all tokens in the traditionalKV-cache. Extensive experiments on streaming and long video understanding tasksdemonstrate that our method outperforms existing methods in response accuracyand real-time efficiency, highlighting its practical value for real-worldstreaming scenarios.</description>
      <author>example@mail.com (Haolin Yang, Feilong Tang, Linxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Boqian Wang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak)</author>
      <guid isPermaLink="false">2508.01875v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>T-GRAG: A Dynamic GraphRAG Framework for Resolving Temporal Conflicts and Redundancy in Knowledge Retrieval</title>
      <link>http://arxiv.org/abs/2508.01680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为T-GRAG（Temporal GraphRAG）的新框架，解决了现有GraphRAG方法在处理知识时间动态性方面的局限性，通过建模知识随时间的演变来提高检索增强生成的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言生成方面表现出色，但在知识密集型任务中受到内部知识过时或不完整的限制。检索增强生成（RAG）通过整合外部检索解决了这一问题，而GraphRAG通过结构化知识图谱和多跳推理进一步提高了性能。然而，现有的GraphRAG方法大多忽略了知识的时间动态性，导致时间模糊性、时间不敏感的检索和语义冗余等问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有GraphRAG方法在处理知识时间动态性方面的局限性，作者提出了T-GRAG，一个动态的、时间感知的检索增强生成框架，用于建模知识随时间的演变。&lt;h4&gt;方法&lt;/h4&gt;T-GRAG包含五个关键组件：1）时间知识图谱生成器，创建带时间戳的、演化的图结构；2）时间查询分解机制，将复杂的时间查询分解为可管理的子查询；3）三层交互检索器，在时间子图上逐步过滤和优化检索；4）源文本提取器，减少噪声；5）基于LLM的生成器，合成具有上下文和时间准确性的响应。作者还引入了Time-LongQA基准数据集，基于真实世界公司年报，用于测试在演化知识上的时间推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，在时间约束下，T-GRAG在检索准确性和响应相关性方面显著优于之前的RAG和GraphRAG基线，突显了为健壮的长文本问答建模知识演化的必要性。&lt;h4&gt;结论&lt;/h4&gt;T-GRAG通过考虑知识的时间动态性，有效解决了现有GraphRAG方法在处理时间相关知识时的局限性，显著提高了检索增强生成的性能。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在自然语言生成方面已展现出强大的性能，但由于内部知识过时或不完整，在知识密集型任务中仍然存在局限。检索增强生成通过整合外部检索解决了这一问题，而GraphRAG通过结构化知识图谱和多跳推理进一步增强了性能。然而，现有的GraphRAG方法大多忽略了知识的时间动态性，导致时间模糊性、时间不敏感的检索和语义冗余等问题。为了克服这些局限性，我们提出了T-GRAG，这是一个动态的、时间感知的RAG框架，用于建模知识随时间的演变。T-GRAG包含五个关键组件：时间知识图谱生成器，创建带时间戳的、演化的图结构；时间查询分解机制，将复杂的时间查询分解为可管理的子查询；三层交互检索器，在时间子图上逐步过滤和优化检索；源文本提取器，减少噪声；基于LLM的生成器，合成具有上下文和时间准确性的响应。我们还引入了Time-LongQA，这是一个基于真实世界公司年报的新颖基准数据集，用于测试在演化知识上的时间推理能力。大量实验表明，在时间约束下，T-GRAG在检索准确性和响应相关性方面显著优于之前的RAG和GraphRAG基线，突显了为健壮的长文本问答建模知识演化的必要性。我们的代码已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated strong performance in naturallanguage generation but remain limited in knowle-  dge-intensive tasks due to outdated or incomplete internal knowledge.Retrieval-Augmented Generation (RAG) addresses this by incorporating externalretrieval, with GraphRAG further enhancing performance through structuredknowledge graphs and multi-hop reasoning. However, existing GraphRAG methodslargely ignore the temporal dynamics of knowledge, leading to issues such astemporal ambiguity, time-insensitive retrieval, and semantic redundancy. Toovercome these limitations, we propose Temporal GraphRAG (T-GRAG), a dynamic,temporally-aware RAG framework that models the evolution of knowledge overtime. T-GRAG consists of five key components: (1) a Temporal Knowledge GraphGenerator that creates time-stamped, evolving graph structures; (2) a TemporalQuery Decomposition mechanism that breaks complex temporal queries intomanageable sub-queries; (3) a Three-layer Interactive Retriever thatprogressively filters and refines retrieval across temporal subgraphs; (4) aSource Text Extractor to mitigate noise; and (5) a LLM-based Generator thatsynthesizes contextually and temporally accurate responses. We also introduceTime-LongQA, a novel benchmark dataset based on real-world corporate annualreports, designed to test temporal reasoning across evolving knowledge.Extensive experiments show that T-GRAG significantly outperforms prior RAG andGraphRAG baselines in both retrieval accuracy and response relevance undertemporal constraints, highlighting the necessity of modeling knowledgeevolution for robust long-text question answering. Our code is publiclyavailable on the T-GRAG</description>
      <author>example@mail.com (Dong Li, Yichen Niu, Ying Ai, Xiang Zou, Biqing Qi, Jianxing Liu)</author>
      <guid isPermaLink="false">2508.01680v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>E-VRAG: Enhancing Long Video Understanding with Resource-Efficient Retrieval Augmented Generation</title>
      <link>http://arxiv.org/abs/2508.01546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;E-VRAG是一种新颖高效的视频检索增强生成(RAG)框架，通过帧预过滤、轻量级VLM评分、全局统计分布的帧检索策略和多视角问答方案，显著降低了计算成本并提高了视频理解的准确性。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型(VLMs)通过利用跨模态推理能力在视频理解方面取得了实质性进展。然而，它们的有效性受到上下文窗口的限制以及处理包含数千帧的长视频所需的高计算成本的限制。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频RAG方法在检索效率和准确性之间难以平衡的问题，特别是在处理多样化和复杂视频内容时。&lt;h4&gt;方法&lt;/h4&gt;提出E-VRAG框架，包含：1)基于分层查询分解的帧预过滤方法；2)使用轻量级VLM进行帧评分；3)利用帧间分数全局统计分布的帧检索策略；4)多视角问答方案增强VLM对长视频上下文的理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;E-VRAG在四个公共基准测试上实现了约70%的计算成本降低，同时比基线方法获得更高的准确性，且无需额外训练。&lt;h4&gt;结论&lt;/h4&gt;E-VRAG在提高视频RAG任务的效率和准确性方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)通过利用跨模态推理能力在视频理解方面取得了实质性进展。然而，它们的有效性受到上下文窗口的限制以及处理包含数千帧的长视频所需的高计算成本的限制。检索增强生成(RAG)通过仅选择最相关的帧作为输入来应对这一挑战，从而降低了计算负担。然而，现有的视频RAG方法难以平衡检索效率和准确性，特别是在处理多样化和复杂的视频内容时。为解决这些局限性，我们提出了E-VRAG，一种用于视频理解的新型高效视频RAG框架。我们首先应用基于分层查询分解的帧预过滤方法来消除无关帧，从而在数据层面降低计算成本。然后，我们使用轻量级VLM进行帧评分，进一步在模型层面降低计算成本。此外，我们提出了一种利用帧间分数全局统计分布的帧检索策略，以减轻使用轻量级VLM可能导致的性能下降。最后，我们为检索到的帧引入了多视角问答方案，增强了VLM从长视频上下文中提取和理解信息的能力。在四个公共基准上的实验表明，E-VRAG实现了比基线方法约70%的计算成本降低和更高的准确性，且无需额外训练。这些结果证明了E-VRAG在提高视频RAG任务的效率和准确性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have enabled substantial progress in videounderstanding by leveraging cross-modal reasoning capabilities. However, theireffectiveness is limited by the restricted context window and the highcomputational cost required to process long videos with thousands of frames.Retrieval-augmented generation (RAG) addresses this challenge by selecting onlythe most relevant frames as input, thereby reducing the computational burden.Nevertheless, existing video RAG methods struggle to balance retrievalefficiency and accuracy, particularly when handling diverse and complex videocontent. To address these limitations, we propose E-VRAG, a novel and efficientvideo RAG framework for video understanding. We first apply a framepre-filtering method based on hierarchical query decomposition to eliminateirrelevant frames, reducing computational costs at the data level. We thenemploy a lightweight VLM for frame scoring, further reducing computationalcosts at the model level. Additionally, we propose a frame retrieval strategythat leverages the global statistical distribution of inter-frame scores tomitigate the potential performance degradation from using a lightweight VLM.Finally, we introduce a multi-view question answering scheme for the retrievedframes, enhancing the VLM's capability to extract and comprehend informationfrom long video contexts. Experiments on four public benchmarks show thatE-VRAG achieves about 70% reduction in computational cost and higher accuracycompared to baseline methods, all without additional training. These resultsdemonstrate the effectiveness of E-VRAG in improving both efficiency andaccuracy for video RAG tasks.</description>
      <author>example@mail.com (Zeyu Xu, Junkang Zhang, Qiang Wang, Yi Liu)</author>
      <guid isPermaLink="false">2508.01546v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models</title>
      <link>http://arxiv.org/abs/2508.01533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ReasonAct方法，通过三阶段训练过程增强小模型在视频理解中的细粒度时间推理能力，并在多个数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;近期多模态模型在视觉-语言任务上取得了进展，但小规模模型在视频理解所需的细粒度时间推理方面仍然存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法（ReasonAct）来增强小模型中的视频推理能力，使其能够更好地处理视频理解中的时间推理问题。&lt;h4&gt;方法&lt;/h4&gt;采用三阶段训练过程：首先仅使用文本进行基础推理训练，然后在视频上进行微调，最后使用时间感知的强化学习进行优化；基于时间组相对策略优化（T-GRPO）构建，融入时间一致性建模；提出生物力学启发的子动作分解机制，为组成动作阶段提供渐进式奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在HMDB51、UCF-101和Kinetics-400数据集上，30亿参数模型分别达到67.2%、94.1%和78.9%的准确率，比基线分别提高了17.9、15.8和12.3个百分点；渐进式训练方法使小模型能够实现竞争性的视频推理性能，同时保持计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的ReasonAct方法使小规模模型能够在视频理解任务上取得显著改进，通过三阶段训练和特定优化技术有效解决了小模型在时间推理方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近的多模态模型在视觉-语言任务上取得了进展，但小规模变体在视频理解所需的细粒度时间推理方面仍然存在困难。我们引入了ReasonAct，一种通过三阶段训练过程增强小模型视频推理的方法：首先仅使用文本推理建立基础，然后在视频上进行微调，最后使用时间感知的强化学习进行优化。我们在时间组相对策略优化（T-GRPO）的基础上构建，通过在策略优化中融入时间一致性建模。我们还提出了一个生物力学启发的子动作分解机制，为组成动作阶段提供渐进式奖励。在HMDB51、UCF-101和Kinetics-400上的实验表明，我们的30亿参数模型分别实现了67.2%、94.1%和78.9%的准确率，比基线分别提高了17.9、15.8和12.3个百分点。消融研究验证了我们的渐进式训练方法使小模型能够实现竞争性的视频推理性能，同时保持计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent multimodal models have shown progress in vision-language tasks,small-scale variants still struggle with the fine-grained temporal reasoningrequired for video understanding. We introduce ReasonAct, a method thatenhances video reasoning in smaller models through a three-stage trainingprocess: first building a foundation with text-only reasoning, then fine-tuningon video, and finally refining with temporal-aware reinforcement learning. Webuild upon Temporal Group Relative Policy Optimization (T-GRPO) byincorporating temporal consistency modeling into policy optimization. We alsopropose a biomechanically-motivated sub-action decomposition mechanism thatprovides graduated rewards for constituent action phases. Through experimentson HMDB51, UCF-101, and Kinetics-400, our 3B-parameter model achieves 67.2%,94.1%, and 78.9% accuracy respectively, demonstrating improvements of 17.9,15.8, and 12.3 points over baselines. Ablation studies validate that ourprogressive training methodology enables smaller models to achieve competitivevideo reasoning performance while maintaining computational efficiency.</description>
      <author>example@mail.com (Jiaxin Liu, Zhaolu Kang)</author>
      <guid isPermaLink="false">2508.01533v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Frequency-Constrained Learning for Long-Term Forecasting</title>
      <link>http://arxiv.org/abs/2508.01508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种简单而有效的方法，通过频谱初始化和频率约束优化来增强深度学习模型的长期预测能力，特别关注捕捉时间序列中的周期性模式。&lt;h4&gt;背景&lt;/h4&gt;现实世界的时间序列通常表现出由物理规律、人类习惯或季节性周期引起的强周期性结构。然而，现代深度预测模型由于频谱偏差和缺乏频率感知的归纳先验，往往无法捕捉这些重复出现的模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够明确建模周期性的方法，通过注入频谱先验来增强深度时间模型的长期预测能力，使其能够更准确地捕捉时间序列中的重复模式。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种通过频谱初始化和频率约束优化来明确建模周期性的方法。具体包括：通过快速傅里叶变换(FFT)引导的坐标下降法提取主导低频分量；用这些分量初始化正弦嵌入；采用双速学习计划来训练期间保留有意义的频率结构。该方法与模型无关，可以无缝集成到现有的基于Transformer的架构中。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的现实世界基准上的广泛实验展示了持续的性能提升，特别是在长期预测方面。此外，在合成数据上，该方法能准确恢复真实频率，验证了其在捕捉潜在周期性模式方面的可解释性和有效性。&lt;h4&gt;结论&lt;/h4&gt;将频谱先验注入深度时间模型可以带来稳健和可解释的长期预测性能提升。该方法通过明确建模周期性，有效解决了现代深度预测模型在捕捉重复模式方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的许多时间序列表现出由物理规律、人类习惯或季节性周期引起的强周期性结构。然而，现代深度预测模型往往由于频谱偏差和缺乏频率感知的归纳先验而无法捕捉这些重复出现的模式。受此差距的启发，我们提出了一种简单而有效的方法，通过频谱初始化和频率约束优化来明确建模周期性，从而增强长期预测能力。具体而言，我们通过快速傅里叶变换(FFT)引导的坐标下降法提取主导低频分量，用这些分量初始化正弦嵌入，并采用双速学习计划来训练期间保留有意义的频率结构。我们的方法与模型无关，可以无缝集成到现有的基于Transformer的架构中。在多样化的现实世界基准上的广泛实验展示了持续的性能提升——特别是在长期预测方面——突显了将频谱先验注入深度时间模型对稳健和可解释的长期预测的好处。此外，在合成数据上，我们的方法能准确恢复真实频率，进一步验证了其在捕捉潜在周期性模式方面的可解释性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many real-world time series exhibit strong periodic structures arising fromphysical laws, human routines, or seasonal cycles. However, modern deepforecasting models often fail to capture these recurring patterns due tospectral bias and a lack of frequency-aware inductive priors. Motivated by thisgap, we propose a simple yet effective method that enhances long-termforecasting by explicitly modeling periodicity through spectral initializationand frequency-constrained optimization. Specifically, we extract dominantlow-frequency components via Fast Fourier Transform (FFT)-guided coordinatedescent, initialize sinusoidal embeddings with these components, and employ atwo-speed learning schedule to preserve meaningful frequency structure duringtraining. Our approach is model-agnostic and integrates seamlessly intoexisting Transformer-based architectures. Extensive experiments across diversereal-world benchmarks demonstrate consistent performance gains--particularly atlong horizons--highlighting the benefits of injecting spectral priors into deeptemporal models for robust and interpretable long-range forecasting. Moreover,on synthetic data, our method accurately recovers ground-truth frequencies,further validating its interpretability and effectiveness in capturing latentperiodic patterns.</description>
      <author>example@mail.com (Menglin Kong, Vincent Zhihao Zheng, Lijun Sun)</author>
      <guid isPermaLink="false">2508.01508v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>WinkTPG: An Execution Framework for Multi-Agent Path Finding Using Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2508.01495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WinkTPG的多智能体路径规划执行框架，能够高效地为大量智能体生成无碰撞的可行速度曲线。&lt;h4&gt;背景&lt;/h4&gt;为大量智能体规划无碰撞路径是一个具有众多实际应用前景的挑战性问题。尽管多智能体路径查找(MAPF)的最新进展显示出 promising 的进展，但标准MAPF算法依赖于简化的运动动力学模型，导致智能体无法直接遵循生成的MAPF计划。&lt;h4&gt;目的&lt;/h4&gt;弥合MAPF算法生成的计划与实际智能体运动之间的差距，提出一种能够将MAPF计划转化为运动动力学可行计划的方法，同时考虑不确定性并保持无碰撞特性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两种方法：运动动力学时序计划图规划(kTPG)，一种多智能体速度优化算法，能够高效地将MAPF计划转化为运动动力学可行的计划；以及基于窗口的kTPG(WinkTPG)，一种MAPF执行框架，使用基于窗口的机制增量式地优化MAPF计划，在执行过程中动态整合智能体信息以减少不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，WinkTPG可以在1秒内为多达1,000个智能体生成速度曲线，并且相比现有的MAPF执行方法，解决方案质量提高了高达51.7%。&lt;h4&gt;结论&lt;/h4&gt;WinkTPG是一种高效的多智能体路径规划执行框架，能够解决大规模智能体路径规划问题，显著提高解决方案质量。&lt;h4&gt;翻译&lt;/h4&gt;为大量智能体规划无碰撞路径是一个具有众多实际应用前景的挑战性问题。尽管多智能体路径查找(MAPF)的最新进展显示出 promising 的进展，但标准MAPF算法依赖于简化的运动动力学模型，这导致智能体无法直接遵循生成的MAPF计划。为了弥合这一差距，我们提出了运动动力学时序计划图规划(kTPG)，这是一种多智能体速度优化算法，能够高效地将MAPF计划转化为运动动力学可行的计划，同时考虑不确定性并保持无碰撞特性。基于kTPG，我们提出了基于窗口的kTPG(WinkTPG)，这是一种MAPF执行框架，使用基于窗口的机制增量式地优化MAPF计划，在执行过程中动态整合智能体信息以减少不确定性。实验表明，WinkTPG可以在1秒内为多达1,000个智能体生成速度曲线，并且相比现有的MAPF执行方法，解决方案质量提高了高达51.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning collision-free paths for a large group of agents is a challengingproblem with numerous real-world applications. While recent advances inMulti-Agent Path Finding (MAPF) have shown promising progress, standard MAPFalgorithms rely on simplified kinodynamic models, preventing agents fromdirectly following the generated MAPF plan. To bridge this gap, we proposekinodynamic Temporal Plan Graph Planning (kTPG), a multi-agent speedoptimization algorithm that efficiently refines a MAPF plan into akinodynamically feasible plan while accounting for uncertainties and preservingcollision-freeness. Building on kTPG, we propose Windowed kTPG (WinkTPG), aMAPF execution framework that incrementally refines MAPF plans using awindow-based mechanism, dynamically incorporating agent information duringexecution to reduce uncertainty. Experiments show that WinkTPG can generatespeed profiles for up to 1,000 agents in 1 second and improves solution qualityby up to 51.7% over existing MAPF execution methods.</description>
      <author>example@mail.com (Jingtian Yan, Stephen F. Smith, Jiaoyang Li)</author>
      <guid isPermaLink="false">2508.01495v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Modeling high and low extremes with a novel dynamic spatio-temporal model</title>
      <link>http://arxiv.org/abs/2508.01481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型动态时空模型，能够有效捕捉环境系统中的高值和低值极端事件，解决了现有模型在处理极端情况时的局限性。&lt;h4&gt;背景&lt;/h4&gt;近年来，极端环境事件（如严重风暴、干旱、热浪、山洪和物种突然崩溃）在地球-大气动态系统中变得更加普遍。&lt;h4&gt;目的&lt;/h4&gt;为了充分理解极端环境事件的潜在机制并增强相关决策的科学性，需要开发能够容纳极端情况的灵活模型。&lt;h4&gt;方法&lt;/h4&gt;引入了一类新的动态时空模型，使用具有不同尾部指数的重尾和轻尾分布混合来捕捉高值和低值极端情况，该框架能够灵活识别时空上的极端依赖性和独立性，并进行不确定性量化，同时支持缺失数据预测。&lt;h4&gt;主要发现&lt;/h4&gt;现有动态时空统计模型在假设高斯误差分布时难以捕捉极端情况，而当前空间极端模型大多假设时间独立性且仅关注多个位置的联合上尾。新模型通过混合分布有效解决了这些问题。&lt;h4&gt;结论&lt;/h4&gt;该新型动态时空模型能够更好地捕捉环境系统中的极端事件，通过美国中部每小时颗粒物再分析数据集验证了其有效性，为理解和应对极端环境事件提供了更好的工具。&lt;h4&gt;翻译&lt;/h4&gt;近年来，极端环境事件如严重风暴、干旱、热浪、山洪和物种突然崩溃在地球-大气动态系统中变得更加普遍。为了充分理解潜在机制并增强知情决策，需要一个能够容纳极端情况的灵活模型。现有的动态时空统计模型在假设高斯误差分布时，在捕捉极端情况方面存在局限性，而当前的空间极端模型大多假设时间独立性，并专注于两个或多个位置的联合上尾。在此，我们引入了一类新的动态时空模型，使用具有不同尾部指数的重尾和轻尾分布混合来捕捉高值和低值极端情况。我们的框架灵活地识别时空上的极端依赖性和独立性，进行不确定性量化，并支持缺失数据预测，与其他动态时空模型类似。我们使用美国中部每小时颗粒物再分析数据集的大数据集证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extreme environmental events such as severe storms, drought, heat waves,flash floods, and abrupt species collapse have become more prevalent in theearth-atmosphere dynamic system in recent years. In order to fully understandthe underlying mechanisms and enhance informed decision-making, a flexiblemodel capable of accommodating extremes is necessary. Existing dynamicspatio-temporal statistical models exhibit limitations in capturing extremeswhen assuming Gaussian error distributions, whereas the current models forspatial extremes mostly assume temporal independence and are focused on jointupper tails at two or more locations. Here, we introduce a new class of dynamicspatio-temporal models that capture both high and low extremes using a mixtureof heavy- and light-tailed distributions with varying tail indices. Ourframework flexibly identifies extremal dependence and independence in bothspace and time with uncertainty quantification and supports missing dataprediction, as in other dynamic spatio-temporal models. We demonstrate itseffectiveness using a large reanalysis dataset of hourly particulate matter inthe Central United States.</description>
      <author>example@mail.com (Myungsoo Yoo, Likun Zhang, Christopher K. Wikle, Thomas Opitz)</author>
      <guid isPermaLink="false">2508.01481v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Toward Using Machine Learning as a Shape Quality Metric for Liver Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2508.02482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用机器学习方法评估生成肝脏形状的质量，作为专家评估的替代方案，提供可解释的反馈。&lt;h4&gt;背景&lt;/h4&gt;3D医学形状生成模型（如扩散模型）在合成多样化且解剖学合理的结构方面显示出潜力，但由于缺乏真实数据，质量评估具有挑战性。现有评估指标通常测量训练集和生成集之间的分布距离，而医学领域需要对每个生成的形状进行个体级别的质量评估，这需要大量的人工专家审查。&lt;h4&gt;目的&lt;/h4&gt;研究使用传统机器学习方法和PointNet作为替代的、可解释的方法来评估生成肝脏形状的质量。&lt;h4&gt;方法&lt;/h4&gt;从生成肝脏形状的表面采样点云，提取手工制作的几何特征，并训练一组监督机器学习和PointNet模型将肝脏形状分类为好或坏。然后训练这些模型作为代理判别器来评估生成模型产生的合成肝脏形状的质量。&lt;h4&gt;主要发现&lt;/h4&gt;基于机器学习的形状分类器不仅提供可解释的反馈，而且与专家评估相比还能提供补充性的见解。&lt;h4&gt;结论&lt;/h4&gt;机器学习分类器可以作为3D器官形状生成中的轻量级、任务相关的质量指标，支持医学形状建模中更透明和临床一致的评估协议。&lt;h4&gt;翻译&lt;/h4&gt;虽然3D医学形状生成模型（如扩散模型）在合成多样化且解剖学合理的结构方面显示出前景，但缺乏真实数据使得质量评估具有挑战性。现有的评估指标通常测量训练集和生成集之间的分布距离，而医学领域需要对每个生成的形状进行个体级别的质量评估，这需要大量的人工专家审查。在本文中，我们研究了使用传统机器学习方法和PointNet作为替代的、可解释的方法来评估生成肝脏形状的质量。我们从生成肝脏形状的表面采样点云，提取手工制作的几何特征，并训练一组监督机器学习和PointNet模型将肝脏形状分类为好或坏。然后训练这些模型作为代理判别器来评估生成模型产生的合成肝脏形状的质量。我们的结果表明，基于机器学习的形状分类器不仅提供可解释的反馈，而且与专家评估相比还能提供补充性的见解。这表明机器学习分类器可以作为3D器官形状生成中的轻量级、任务相关的质量指标，支持医学形状建模中更透明和临床一致的评估协议。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D医学生成肝脏形状的质量评估问题。由于缺乏真实值（ground truth），现有评估方法主要测量分布距离，而医学领域需要在个体层面评估每个生成形状，这需要劳动密集型的专家审查。这个问题很重要，因为高质量肝脏形状模型对手术规划、医学教育和研究至关重要，专家评估耗时且成本高，限制了生成模型的开发和迭代速度，自动化质量评估能加速医学形状建模流程并确保生成模型的临床适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别专家评估是肝脏生成工作中的瓶颈，然后寻找机器学习替代方案。他们设计了两种方法：传统机器学习方法（提取几何特征后使用分类器）和深度学习方法（使用PointNet直接处理点云）。作者借鉴了现有工作：使用了现有的机器学习算法（如随机森林、SVM等）；应用了PointNet和PointNet++等点云处理经典架构；使用了计算机视觉和医学图像分析中常用的基本几何特征；采用了机器学习领域的标准评估方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用机器学习分类器作为肝脏形状质量的代理评估工具，替代或辅助专家评估。整体流程包括：1)数据准备（获取肝脏对象并由专家分类为'好'/'坏'）；2)点云创建（在每个肝脏表面采样20,000个点）；3)特征提取（传统ML方法提取14维几何特征）；4)模型训练（传统ML方法和PointNet/PointNet++分别训练）；5)模型评估（在测试集和生成集上评估）；6)可解释性分析（使用SHAP分析理解决策依据）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将机器学习分类器应用于3D医学生成形状的质量评估；提供了可解释的质量评估方法（通过SHAP分析）；展示了机器学习评估与专家评估的互补性；提出了轻量级、任务相关的质量指标。相比之前的工作，该方法专注于个体层面的质量评估而非分布距离；能学习比纯几何指标更复杂的形状特征；可以自动化评估过程而非依赖专家；传统ML方法提供了可解释的决策依据而非黑盒预测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一种基于机器学习的可解释方法，用于评估3D生成肝脏形状的质量，能够替代部分专家评估工作并提供与专家判断互补的见解，加速了医学形状生成模型的开发和评估流程。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While 3D medical shape generative models such as diffusion models have shownpromise in synthesizing diverse and anatomically plausible structures, theabsence of ground truth makes quality evaluation challenging. Existingevaluation metrics commonly measure distributional distances between trainingand generated sets, while the medical field requires assessing quality at theindividual level for each generated shape, which demands labor-intensive expertreview.  In this paper, we investigate the use of classical machine learning (ML)methods and PointNet as an alternative, interpretable approach for assessingthe quality of generated liver shapes. We sample point clouds from the surfacesof the generated liver shapes, extract handcrafted geometric features, andtrain a group of supervised ML and PointNet models to classify liver shapes asgood or bad. These trained models are then used as proxy discriminators toassess the quality of synthetic liver shapes produced by generative models.  Our results show that ML-based shape classifiers provide not onlyinterpretable feedback but also complementary insights compared to expertevaluation. This suggests that ML classifiers can serve as lightweight,task-relevant quality metrics in 3D organ shape generation, supporting moretransparent and clinically aligned evaluation protocols in medical shapemodeling.</description>
      <author>example@mail.com (Khoa Tuan Nguyen, Gaeun Oh, Ho-min Park, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Niki Rashidian, Wesley De Neve)</author>
      <guid isPermaLink="false">2508.02482v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction</title>
      <link>http://arxiv.org/abs/2508.02408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GR-Gaussian是一种基于图的3D高斯飞溅框架，通过两个关键创新策略有效解决了稀疏视图CT重建中的针状伪影问题，提高了重建准确性。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅(3DGS)已成为CT重建的一种有前景的方法，但现有方法依赖于视图内点的平均梯度幅度，这在稀疏视图条件下常导致严重的针状伪影。&lt;h4&gt;目的&lt;/h4&gt;解决稀疏视图条件下的针状伪影问题，提高重建准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了GR-Gaussian框架，包含两个关键创新：(1)去噪点云初始化策略，减少初始化误差并加速收敛；(2)像素图感知梯度策略，使用基于图的密度差异改进梯度计算，提高分割精度和密度表示。&lt;h4&gt;主要发现&lt;/h4&gt;在X-3D和真实世界数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提高0.67 dB和0.92 dB，SSIM提高0.011和0.021。&lt;h4&gt;结论&lt;/h4&gt;GR-Gaussian适用于在具有挑战性的稀疏视图条件下进行准确的CT重建。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅(3DGS)已成为一种有前景的CT重建方法。然而，现有方法依赖于视图内点的平均梯度幅度，常导致稀疏视图条件下出现严重的针状伪影。为应对这一挑战，我们提出了GR-Gaussian，一种基于图的3D高斯飞溅框架，可在稀疏视图条件下抑制针状伪影并提高重建准确性。我们的框架引入了两个关键创新：(1)去噪点云初始化策略，减少初始化误差并加速收敛；(2)像素图感知梯度策略，利用基于图的密度差异改进梯度计算，提高分割精度和密度表示。在X-3D和真实世界数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提高0.67 dB和0.92 dB，SSIM提高0.011和0.021。这些结果突显了GR-Gaussian在具有挑战性的稀疏视图条件下进行准确CT重建的适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决稀疏视角CT重建中的针状伪影问题。这个问题在现实中非常重要，因为在医学CT等领域减少X射线投影角度可以降低患者辐射风险，在电子断层扫描中受限于旋转角度，而传统方法在这些条件下会产生严重伪影，影响诊断准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析发现现有3D高斯飞溅方法在CT重建中产生针状伪影的原因是保留了小梯度的大高斯核，缺乏对点间关系的考虑。因此，他们引入图结构建模点间关系，并借鉴了3D高斯飞溅技术、图神经网络思想，同时改进了传统FDK方法和像素感知梯度概念，设计了去噪点云初始化和像素图感知梯度策略两个核心创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图结构表示对象点间关系，通过密度差异改进梯度计算，并在初始化阶段就去噪。流程包括：1)初始化阶段：用改进FDK生成初始体积，高斯滤波去噪，随机采样点位置，用KNN构建图结构；2)训练阶段：稀疏视角投影光栅化计算损失，应用像素图感知梯度策略优化高斯核分裂，使用包含光度损失、正则化的总损失函数进行优化；3)重建阶段：优化后的图结构辐射高斯表示进行体素化获取最终密度体积。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)图结构辐射高斯表示；2)去噪点云初始化策略；3)像素图感知梯度策略。相比传统方法，GR-Gaussian能抑制条纹伪影；相比迭代优化方法，更高效且保留更多细节；相比深度学习方法，无需大量标记数据；相比NeRF方法，渲染速度更快；特别相比现有3DGS方法，有效解决了稀疏视角CT中的针状伪影问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了GR-Gaussian框架，通过图结构和改进的初始化与梯度策略，有效解决了稀疏视角CT重建中的针状伪影问题，显著提高了重建质量和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CTreconstruction. However, existing methods rely on the average gradientmagnitude of points within the view, often leading to severe needle-likeartifacts under sparse-view conditions. To address this challenge, we proposeGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppressesneedle-like artifacts and improves reconstruction accuracy under sparse-viewconditions. Our framework introduces two key innovations: (1) a Denoised PointCloud Initialization Strategy that reduces initialization errors andaccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy thatrefines gradient computation using graph-based density differences, improvingsplitting accuracy and density representation. Experiments on X-3D andreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNRimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. Theseresults highlight the applicability of GR-Gaussian for accurate CTreconstruction under challenging sparse-view conditions.</description>
      <author>example@mail.com (Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang)</author>
      <guid isPermaLink="false">2508.02408v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera</title>
      <link>http://arxiv.org/abs/2508.02348v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新框架，通过相机推断的道路布局来解释雷达点云，实现非视距区域行人的定位。&lt;h4&gt;背景&lt;/h4&gt;在城市环境中，非视距区域的行人定位对自动驾驶系统构成重大挑战。毫米波雷达容易受多径反射影响而失真，相机图像则缺乏深度感知能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，通过从相机推断的道路布局来解释雷达点云，实现NLoS区域行人的定位。&lt;h4&gt;方法&lt;/h4&gt;利用相机的视觉信息来解释2D雷达点云，实现空间场景重建。&lt;h4&gt;主要发现&lt;/h4&gt;通过在真实车辆上安装的雷达-相机系统进行实验验证，使用在户外NLoS驾驶环境中收集的数据集评估定位性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在NLoS行人定位中具有实际适用性。&lt;h4&gt;翻译&lt;/h4&gt;在城市环境中，非视距区域的行人定位对自动驾驶系统构成重大挑战。虽然毫米波雷达已显示出在这种场景中检测物体的潜力，但2D雷达点云数据容易受多径反射引起的失真影响，使得准确的空间推断变得困难。此外，尽管相机图像能提供高分辨率的视觉信息，但它们缺乏深度感知能力，无法直接观察到非视距区域的物体。在本文中，我们提出了一种新框架，该框架通过从相机推断的道路布局来解释雷达点云，以实现非视距行人的定位。所提出的方法利用相机的视觉信息来解释2D雷达点云，从而实现空间场景重建。通过在真实车辆上安装的雷达-相机系统进行的实验验证了所提出方法的有效性。使用在户外非视距驾驶环境中收集的数据集评估了定位性能，证明了该方法的实际适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在城市环境中非视距(NLoS)区域内行人定位的问题。这个问题在现实中非常重要，因为根据美国交通部数据，2022年交叉口交通事故导致12,036人死亡。传统的传感器只能检测可见区域(LoS)的物体，无法被建筑物或围栏遮挡的行人，这对自动驾驶系统构成重大安全风险。现有的V2X通信解决方案又缺乏基础设施支持且成本高昂。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到需要能反射和衍射信号的传感器如毫米波雷达来解决NLoS检测问题。他们意识到单独使用雷达点云存在稀疏、噪声大和多路径反射导致位置失真等挑战；而相机虽提供高分辨率视觉信息但缺乏深度感知。因此，他们设计了一种融合方法，结合雷达的精确距离测量和相机的视觉信息。该方法借鉴了现有的相机BEV变换技术、多路径反射模型和雷达-相机融合技术，但进行了创新改进以适应NLoS场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是融合毫米波雷达和相机图像的优势，利用相机提供的道路布局信息来解释雷达点云数据，从而推断遮挡空间的结构并定位NLoS行人。整体流程分为两部分：1)空间配置推断：从相机提取道路布局，对齐雷达与相机数据，对静态点进行分类和射线追踪，重建空间环境；2)NLoS行人定位：对动态点进行射线追踪，通过过滤和聚类处理噪声，使用DBSCAN算法估计行人实际位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出毫米波雷达与相机融合的新框架用于T型路口NLoS行人定位；2)利用相机图像解释2D雷达点云实现准确空间推理；3)在真实世界室外NLoS环境中验证方法有效性。相比之前工作，本文方法不仅适用于室外复杂场景，还能估计反射器位置并利用多反射生成的雷达点，而之前的方法或仅适用于室内环境，或无法重建空间结构，或不考虑多反射情况，导致定位准确性受限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的毫米波雷达与相机融合方法，通过利用相机推断的道路布局解释雷达点云，实现了在T型路口非视距区域内行人的准确定位，显著提高了自动驾驶系统在复杂城市环境中的感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urbanenvironments poses a significant challenge for autonomous driving systems.While mmWave radar has demonstrated potential for detecting objects in suchscenarios, the 2D radar point cloud (PCD) data is susceptible to distortionscaused by multipath reflections, making accurate spatial inference difficult.Additionally, although camera images provide high-resolution visualinformation, they lack depth perception and cannot directly observe objects inNLoS regions. In this paper, we propose a novel framework that interprets radarPCD through road layout inferred from camera for localization of NLoSpedestrians. The proposed method leverages visual information from the camerato interpret 2D radar PCD, enabling spatial scene reconstruction. Theeffectiveness of the proposed approach is validated through experimentsconducted using a radar-camera system mounted on a real vehicle. Thelocalization performance is evaluated using a dataset collected in outdoor NLoSdriving environments, demonstrating the practical applicability of the method.</description>
      <author>example@mail.com (Byeonggyu Park, Hee-Yeun Kim, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seong-Woo Kim)</author>
      <guid isPermaLink="false">2508.02348v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>AID4AD: Aerial Image Data for Automated Driving Perception</title>
      <link>http://arxiv.org/abs/2508.02140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出AID4AD数据集，将高分辨率航空图像精确对齐到nuScenes数据集的局部坐标系，展示了航空图像在自动驾驶车辆感知任务中的实用价值，实验证明其可提高地图构建准确度和轨迹预测性能。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆需要准确的环境感知和地图信息，但高清地图在某些场景下可能不可用、过时或维护成本高，需要寻找替代或补充的环境信息源。&lt;h4&gt;目的&lt;/h4&gt;开发一种将航空图像与地面车辆数据集精确对齐的方法，创建公开可用的数据集(AID4AD)，并验证航空图像在自动驾驶感知任务中的实用价值。&lt;h4&gt;方法&lt;/h4&gt;使用基于SLAM的点云地图将航空图像与nuScenes局部坐标系对齐，提出对齐工作流程校正定位和投影失真，通过手动质量控制识别高质量对齐作为地面真相，并在在线地图构建和运动预测两个任务中评估数据集。&lt;h4&gt;主要发现&lt;/h4&gt;航空图像作为补充输入可提高地图构建过程；作为结构化环境表示可替代高清地图；使地图构建准确度提高15-23%；使轨迹预测性能提高2%。&lt;h4&gt;结论&lt;/h4&gt;航空图像是自动驾驶系统中一种可扩展和适应性强的环境上下文来源，特别是在高清地图不可用、过时或维护成本高的场景中具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;本研究研究了将空间对齐的航空图像集成到自动驾驶车辆(AVs)的感知任务中。作为核心贡献，我们提出了AID4AD，一个公开可用的数据集，通过将高分辨率航空图像精确对齐到nuScenes数据集的局部坐标系来增强该数据集。对齐是使用nuScenes提供的基于SLAM的点云地图执行的，建立了航空数据与nuScenes局部坐标系之间的直接联系。为确保空间保真度，我们提出了一种对齐工作流程，校正定位和投影失真。手动质量控制过程进一步通过识别一组高质量对齐来完善数据集，我们将其作为地面真相发布，以支持未来关于自动注册的研究。我们在两个代表性任务中展示了AID4AD的实际价值：在在线地图构建中，航空图像作为补充输入改善了地图构建过程；在运动预测中，它作为结构化环境表示替代了高清地图。实验表明，航空图像使地图构建准确度提高了15-23%，轨迹预测性能提高了2%。这些结果突显了航空图像作为自动驾驶系统中可扩展和适应性强的环境上下文来源的潜力，特别是在高清地图不可用、过时或维护成本高的场景中。AID4AD以及评估代码和预训练模型已公开发布，以促进该方向的进一步研究：https://github.com/DriverlessMobility/AID4AD。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将航空影像有效集成到自动驾驶汽车感知系统中的问题，作为高清地图的替代或补充方案。这个问题很重要，因为当前自动驾驶系统依赖的高清地图维护成本高、更新困难，特别是在动态变化的城市环境中，而航空影像提供了一种可扩展、近乎实时更新的环境上下文来源，可用于高清地图不可用或过时的场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有数据集如SatforHDMap和OpenSatMap在空间对齐和时间匹配方面的局限性，缺乏公开的精确对齐航空影像数据集。他们选择nuScenes作为基础数据集，因为它在规模、传感器覆盖和定位精度间提供了最佳平衡。方法设计上，他们开发了专门的航空影像对齐工作流程，使用基于SLAM的点云地图解决空间不匹配问题，并借鉴了现有的地图构建和运动预测算法，但解决了之前工作的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是航空影像可作为结构化、可扩展的环境参考源，通过精确空间对齐可补充或替代高清地图。整体流程包括：1)收集与nuScenes时期匹配的高分辨率航空影像；2)开发对齐工作流程，计算偏移网格解决空间差异；3)使用互信息最大化自动估计偏移量；4)应用预处理提高结构一致性；5)进行手动质量控制；6)生成车辆为中心的影像裁剪；7)将偏移量聚合到空间网格并插值；8)在地图构建和运动预测任务中评估效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AID4AD数据集-首个公开精确对齐的航空影像数据集；2)对齐工作流程-使用SLAM点云地图解决定位和投影畸变；3)应用评估-证明航空影像在地图构建提升15-23%精度，运动预测提升2%性能。相比之前工作，AID4AD实现了更高空间对齐精度(0.16m vs 1.46m/2.80m)，更高分辨率(0.15m/像素)，更好的时间匹配，并提供地面真实对齐数据，解决了现有数据集的空间不匹配和时间不一致问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AID4AD数据集通过将高分辨率航空影像与nuScenes精确对齐，为自动驾驶感知提供了一种可扩展、适应性强的环境上下文来源，实验证明其在地图构建和运动预测任务中显著提升了性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates the integration of spatially aligned aerial imageryinto perception tasks for automated vehicles (AVs). As a central contribution,we present AID4AD, a publicly available dataset that augments the nuScenesdataset with high-resolution aerial imagery precisely aligned to its localcoordinate system. The alignment is performed using SLAM-based point cloud mapsprovided by nuScenes, establishing a direct link between aerial data andnuScenes local coordinate system. To ensure spatial fidelity, we propose analignment workflow that corrects for localization and projection distortions. Amanual quality control process further refines the dataset by identifying a setof high-quality alignments, which we publish as ground truth to support futureresearch on automated registration. We demonstrate the practical value ofAID4AD in two representative tasks: in online map construction, aerial imageryserves as a complementary input that improves the mapping process; in motionprediction, it functions as a structured environmental representation thatreplaces high-definition maps. Experiments show that aerial imagery leads to a15-23% improvement in map construction accuracy and a 2% gain in trajectoryprediction performance. These results highlight the potential of aerial imageryas a scalable and adaptable source of environmental context in automatedvehicle systems, particularly in scenarios where high-definition maps areunavailable, outdated, or costly to maintain. AID4AD, along with evaluationcode and pretrained models, is publicly released to foster further research inthis direction: https://github.com/DriverlessMobility/AID4AD.</description>
      <author>example@mail.com (Daniel Lengerer, Mathias Pechinger, Klaus Bogenberger, Carsten Markgraf)</author>
      <guid isPermaLink="false">2508.02140v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>On-the-Fly Object-aware Representative Point Selection in Point Cloud</title>
      <link>http://arxiv.org/abs/2508.01980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于点云降采样的代表性点选择框架，能够在自动驾驶车辆应用中有效存储和处理大量点云数据，同时保留关键物体相关信息并过滤无关背景点。&lt;h4&gt;背景&lt;/h4&gt;点云对物体建模和自动驾驶车辆的驾驶任务辅助至关重要。然而，自动驾驶车辆产生的大量数据给存储、带宽和处理成本带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种点云降采样方法，能够在减少数据量的同时保留关键物体相关信息，有效过滤无关背景点，解决自动驾驶车辆数据处理中的存储、带宽和处理成本问题。&lt;h4&gt;方法&lt;/h4&gt;该方法包含两个步骤：1) 物体存在检测：引入无监督的基于密度峰值的分类器和监督的朴素贝叶斯分类器来处理不同场景；2) 采样预算分配：提出一种选择物体相关点同时保持高物体信息保留率的策略。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和nuScenes数据集上的大量实验表明，该方法在不同采样率下，在效率和效果方面均持续优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;作为一种与模型无关的解决方案，该方法能够与各种下游模型无缝集成，成为自动驾驶应用中3D点云降采样工具包中有价值且可扩展的补充。&lt;h4&gt;翻译&lt;/h4&gt;点云对于物体建模至关重要，并在辅助自动驾驶车辆(AVs)的驾驶任务中发挥关键作用。然而，自动驾驶车辆产生的大量数据给存储、带宽和处理成本带来了挑战。为了应对这些挑战，我们提出了一种用于点云降采样的代表性点选择框架，该方法在保留关键物体相关信息的同时，有效过滤了无关的背景点。我们的方法包括两个步骤：(1)物体存在检测，我们引入了一种无监督的基于密度峰值的分类器和一种监督的朴素贝叶斯分类器来处理不同场景；(2)采样预算分配，我们提出了一种选择物体相关点同时保持高物体信息保留率的策略。在KITTI和nuScenes数据集上的大量实验表明，我们的方法在不同采样率下，在效率和效果方面均持续优于最先进的基线方法。作为一种与模型无关的解决方案，我们的方法能够与各种下游模型无缝集成，使其成为自动驾驶应用中3D点云降采样工具包中有价值且可扩展的补充。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶车辆生成的点云数据量过大带来的存储、带宽和处理成本挑战。每小时约5TB的数据量超过了5G网络的处理能力，且高性能GPU处理成本高昂。这个问题在现实中很重要，因为它直接影响自动驾驶系统的实时性能和部署成本，同时限制了大规模数据处理和模型训练的可行性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析点云中物体点比背景点更重要的特性，设计了两阶段方法：物体存在检测和采样预算分配。借鉴了密度峰值聚类思想用于物体检测，以及朴素贝叶斯分类器进行监督学习。同时结合了随机采样和FPS（最远点采样）的优点，但做了改进以提高效率。作者特别关注了方法的泛化能力，使其能适用于不同的下游检测模型，而非仅针对特定模型优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是物体感知的点云降采样，优先保留物体相关的点，过滤掉不相关的背景点。整体流程分为两步：1)物体存在检测：使用无监督的密度峰值分类器或监督的朴素贝叶斯分类器将点分为物体点和背景点；2)采样预算分配：为物体点和背景点分配不同采样比例（如7:3或8:2），对物体点使用更精细的采样策略，对背景点使用高效随机采样。这种方法在保持物体检测精度的同时，显著减少了数据量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)物体感知的点云降采样框架，明确区分物体点和背景点；2)两阶段处理方法，结合无监督和监督两种物体检测方案；3)基于统计特征的物体检测，无需深度学习特征提取；4)分层过滤策略，平衡效率和效果。相比之前工作，本方法泛化能力强（可与各种下游模型集成），计算效率更高（比FPS快约50倍），且在低采样率下对小物体检测效果更好，解决了现有方法要么效率低、要么泛化能力差的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种高效且通用的物体感知点云降采样方法，通过两阶段处理在保持物体检测精度的同时显著减少了点云数据量，解决了自动驾驶系统中大规模点云数据的存储、传输和处理挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are essential for object modeling and play a critical role inassisting driving tasks for autonomous vehicles (AVs). However, the significantvolume of data generated by AVs creates challenges for storage, bandwidth, andprocessing cost. To tackle these challenges, we propose a representative pointselection framework for point cloud downsampling, which preserves criticalobject-related information while effectively filtering out irrelevantbackground points. Our method involves two steps: (1) Object PresenceDetection, where we introduce an unsupervised density peak-based classifier anda supervised Na\"ive Bayes classifier to handle diverse scenarios, and (2)Sampling Budget Allocation, where we propose a strategy that selectsobject-relevant points while maintaining a high retention rate of objectinformation. Extensive experiments on the KITTI and nuScenes datasetsdemonstrate that our method consistently outperforms state-of-the-art baselinesin both efficiency and effectiveness across varying sampling rates. As amodel-agnostic solution, our approach integrates seamlessly with diversedownstream models, making it a valuable and scalable addition to the 3D pointcloud downsampling toolkit for AV applications.</description>
      <author>example@mail.com (Xiaoyu Zhang, Ziwei Wang, Hai Dong, Zhifeng Bao, Jiajun Liu)</author>
      <guid isPermaLink="false">2508.01980v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Rate-distortion Optimized Point Cloud Preprocessing for Geometry-based Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2508.01633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的预处理框架，结合面向压缩的体素化网络和可微的G-PCC代理模型，显著提高了基于几何的点云压缩标准的效率，同时保持其互操作性和计算灵活性，实现了38.84%的平均BD-rate降低。&lt;h4&gt;背景&lt;/h4&gt;基于几何的点云压缩(G-PCC)是由MPEG设计的国际标准，为压缩各种类型的点云提供了通用框架，并确保了应用和设备之间的互操作性。然而，与最近的基于深度学习的点云压缩方法相比，G-PCC的性能较差，尽管它的计算功耗较低。&lt;h4&gt;目的&lt;/h4&gt;为了提高G-PCC的效率而不牺牲其互操作性或计算灵活性，研究人员提出了一种新的预处理框架，旨在将深度学习与传统的G-PCC标准相结合，实现性能提升的同时保持向后兼容性。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种结合压缩导向的体素化网络和可微的G-PCC代理模型的预处理框架。这两个组件在训练阶段进行联合优化。代理模型模仿非可微G-PCC编解码器的率失真行为，实现端到端的梯度传播。多功能的体素化网络使用基于学习的体素化自适应转换输入点云，并通过全局缩放、细粒度剪枝和点级编辑有效操作点云，以实现率失真权衡。在推理阶段，只有轻量级的体素化网络被附加到G-PCC编码器上，不需要修改解码器。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，该方法比G-PCC实现了38.84%的平均BD-rate降低。通过将传统编解码器与深度学习相结合，这项工作为增强传统压缩标准同时保持其向后兼容性提供了实际途径。&lt;h4&gt;结论&lt;/h4&gt;这项研究成功地将深度学习与传统G-PCC标准相结合，通过创新的预处理框架实现了显著的性能提升，同时保持了标准的互操作性和计算灵活性。这种方法为增强现有压缩标准提供了实用途径，特别适合实际部署场景。&lt;h4&gt;翻译&lt;/h4&gt;基于几何的点云压缩(G-PCC)是由MPEG设计的国际标准，为压缩各种类型的点云提供了通用框架，同时确保了应用和设备之间的互操作性。然而，与最近的基于深度学习的点云压缩方法相比，G-PCC的性能较差，尽管它的计算功耗较低。为了提高G-PCC的效率而不牺牲其互操作性或计算灵活性，我们提出了一种新颖的预处理框架，该框架集成了面向压缩的体素化网络和可微的G-PCC代理模型，在训练阶段进行联合优化。代理模型模仿非可微G-PCC编解码器的率失真行为，实现端到端的梯度传播。多功能的体素化网络使用基于学习的体素化自适应转换输入点云，并通过全局缩放、细粒度剪枝和点级编辑有效操作点云，以实现率失真权衡。在推理阶段，只有轻量级的体素化网络被附加到G-PCC编码器上，不需要修改解码器，因此不会给最终用户带来计算开销。广泛的实验表明，该方法比G-PCC实现了38.84%的平均BD-rate降低。通过将传统编解码器与深度学习相结合，这项工作为增强传统压缩标准同时保持其向后兼容性提供了实际途径，使其非常适合实际部署。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决G-PCC（基于几何的点云压缩）国际标准压缩性能不足的问题。在自动驾驶、虚拟现实和3D重建等领域，点云数据应用广泛但体积庞大，需要高效压缩技术。G-PCC虽然具有互操作性和计算效率优势，但相比基于深度学习的压缩方法性能较差。而深度学习方法虽性能好但计算复杂且存在可重现性问题。因此，在不牺牲G-PCC优势的前提下提升其效率，对推动点云技术的实际应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了G-PCC和基于深度学习的PCC方法的优缺点，提出结合两者优势的方案。他们设计了一个预处理框架，整合了面向压缩的体素化网络和可微分G-PCC代理模型。在训练阶段进行联合优化，使代理模型模仿G-PCC的率失真行为。推理阶段只保留轻量级体素化网络附加到G-PCC编码器。该方法借鉴了现有工作：体素化网络采用稀疏张量多尺度表示；代理模型设计参考了图像压缩中的可微分JPEG代理；损失函数结合了率失真优化框架；网络架构借鉴了InceptionResNet等结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习型预处理优化点云分布，使其能被G-PCC以最优的率失真权衡进行压缩，同时保持G-PCC的兼容性和效率。实现流程：1)输入原始点云；2)体素化网络进行全局缩放、局部剪枝和点级编辑；3)使用稀疏卷积处理多尺度稀疏张量表示；4)通过分类机制优化节点占用状态；5)可微分代理模型模拟G-PCC编码行为；6)训练阶段联合优化率失真损失；7)推理阶段仅保留轻量级体素化网络附加到G-PCC编码器，解码器保持不变。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)轻量级通用点云体素化网络，通过多种操作优化点云分布；2)可微分G-PCC代理模型实现梯度传播；3)后加载上采样模块减少计算量；4)实用范例增强传统标准而不修改解码端。相比之前工作：与传统G-PCC相比显著提升压缩性能(38.84% BD-rate减少)；与深度学习方法相比避免解码端使用神经网络，解决可重现性问题；与现有预处理相比针对压缩任务联合优化；与图像压缩方法相比专门针对3D点云特性设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合学习型预处理和传统G-PCC框架的混合方法，通过可微分代理模型实现端到端优化，显著提升了点云压缩性能同时保持标准的互操作性和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometry-based point cloud compression (G-PCC), an international standarddesigned by MPEG, provides a generic framework for compressing diverse types ofpoint clouds while ensuring interoperability across applications and devices.However, G-PCC underperforms compared to recent deep learning-based PCC methodsdespite its lower computational power consumption. To enhance the efficiency ofG-PCC without sacrificing its interoperability or computational flexibility, wepropose a novel preprocessing framework that integrates a compression-orientedvoxelization network with a differentiable G-PCC surrogate model, jointlyoptimized in the training phase. The surrogate model mimics the rate-distortionbehaviour of the non-differentiable G-PCC codec, enabling end-to-end gradientpropagation. The versatile voxelization network adaptively transforms inputpoint clouds using learning-based voxelization and effectively manipulatespoint clouds via global scaling, fine-grained pruning, and point-level editingfor rate-distortion trade-offs. During inference, only the lightweightvoxelization network is appended to the G-PCC encoder, requiring nomodifications to the decoder, thus introducing no computational overhead forend users. Extensive experiments demonstrate a 38.84% average BD-rate reductionover G-PCC. By bridging classical codecs with deep learning, this work offers apractical pathway to enhance legacy compression standards while preservingtheir backward compatibility, making it ideal for real-world deployment.</description>
      <author>example@mail.com (Wanhao Ma, Wei Zhang, Shuai Wan, Fuzheng Yang)</author>
      <guid isPermaLink="false">2508.01633v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor</title>
      <link>http://arxiv.org/abs/2508.01311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  We have provided the code for C3D-AD with checkpoints and BASELINE at  this link: https://github.com/hzzzzzhappy/CL3AD&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为C3D-AD的持续学习框架，用于3D异常检测，通过三个关键模块实现了对多类别点云的通用表示学习，并能处理随时间出现的新类别，在三个公共数据集上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;3D异常检测在检测高精度工业产品的异常或缺陷方面显示出巨大潜力。然而，现有方法通常以类别特定的方式训练，并且缺乏从新兴类别中学习的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个持续学习框架，能够学习多类别点云的通用表示，并处理随时间出现的新类别。&lt;h4&gt;方法&lt;/h4&gt;提出了C3D-AD框架，包含三个主要模块：1)引入带有随机特征层的核注意力(KAL)来高效提取多样化产品类型的通用局部特征并规范化特征空间；2)提出带有可学习顾问的核注意力(KAA)机制，在编码器和解码器中从新类别学习信息同时丢弃冗余旧信息；3)设计带有参数扰动的重建(RPP)模块，通过表示重放损失函数保持跨任务的表示一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的广泛实验证明了所提出方法的有效性，在Real3D-AD、Anomaly-ShapeNet和MulSen-AD上分别实现了66.4%、83.1%和63.4%的平均AUROC性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的C3D-AD框架能够有效处理3D点云数据的持续学习异常检测问题，能够在处理新类别的同时保持对旧类别的记忆。&lt;h4&gt;翻译&lt;/h4&gt;3D异常检测在检测高精度工业产品的异常或缺陷方面显示出巨大潜力。然而，现有方法通常以类别特定的方式训练，并且缺乏从新兴类别中学习的能力。在本研究中，我们提出了一个名为持续3D异常检测的持续学习框架，它不仅能学习多类别点云的通用表示，还能处理随时间出现的新类别。具体来说，在特征提取模块中，为了高效地从不同任务的各种产品类型中提取通用局部特征，引入了带有随机特征层的核注意力，它规范化了特征空间。然后，为了正确且持续地重建数据，提出了一个高效的带有可学习顾问的核注意力机制，它在编码器和解码器中从新类别学习信息，同时丢弃冗余的旧信息。最后，为了保持跨任务的表示一致性，设计了带有参数扰动的重建模块，通过设计表示重放损失函数，确保模型记住之前的类别信息并返回类别自适应的表示。在三个公共数据集上的广泛实验证明了所提出方法的有效性，在Real3D-AD、Anomaly-ShapeNet和MulSen-AD上分别实现了66.4%、83.1%和63.4%的平均AUROC性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D异常检测在持续学习环境下的挑战，即当新类别产品不断出现时，现有方法需要完全重新训练的问题。这个问题在工业现实中非常重要，因为生产线会不断更新产品类型，需要能够持续学习并适应新类别的异常检测系统，同时保持对之前学习类别的检测能力，避免灾难性遗忘。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D异常检测方法的局限性，指出它们通常针对特定类别训练且缺乏从新兴类别学习的能力。然后，作者借鉴了持续学习领域的方法，如基于正则化和基于重放的技术，以及3D异常检测中的特征嵌入和重建方法。但由于3D点云数据的高分辨率特性和类别特定模型限制，这些方法无法直接应用于3D场景，因此作者设计了专门针对3D点云的持续学习框架，包括KAL、KAA和RPP三个核心组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过三个组件实现持续学习：1) KAL模块提取通用特征并归一化特征空间；2) KAA机制在编码器-解码器中学习新信息同时丢弃冗余旧信息；3) RPP模块保持跨任务表示一致性。整体流程是：首先使用KAL从点云提取特征；然后通过KAA处理特征，学习新类别信息；最后使用RPP重建特征，保持表示一致性；通过比较特征令牌和重建令牌差异计算异常分数进行检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) KAL层归一化特征空间并提取跨组点云的通用非线性结构信息；2) KAA机制具有线性O(n)复杂度，能减少旧知识冗余同时学习新知识；3) RPP模块通过参数扰动保持表示一致性。相比之前工作，C3D-AD首次以类别增量方式解决3D异常检测问题，提供了多类别和持续异常检测能力，避免了传统方法的完全重新训练需求，同时解决了统一模型中的灾难性遗忘问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; C3D-AD首次提出了一种持续学习框架，通过核注意力和可学习顾问机制，使3D异常检测模型能够从新兴类别中学习同时避免灾难性遗忘，实现了高效的多类别持续异常检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Anomaly Detection (AD) has shown great potential in detecting anomalies ordefects of high-precision industrial products. However, existing methods aretypically trained in a class-specific manner and also lack the capability oflearning from emerging classes. In this study, we proposed a continual learningframework named Continual 3D Anomaly Detection (C3D-AD), which can not onlylearn generalized representations for multi-class point clouds but also handlenew classes emerging over time.Specifically, in the feature extraction module,to extract generalized local features from diverse product types of differenttasks efficiently, Kernel Attention with random feature Layer (KAL) isintroduced, which normalizes the feature space. Then, to reconstruct datacorrectly and continually, an efficient Kernel Attention with learnable Advisor(KAA) mechanism is proposed, which learns the information from new categorieswhile discarding redundant old information within both the encoder and decoder.Finally, to keep the representation consistency over tasks, a Reconstructionwith Parameter Perturbation (RPP) module is proposed by designing arepresentation rehearsal loss function, which ensures that the model remembersprevious category information and returns category-adaptiverepresentation.Extensive experiments on three public datasets demonstrate theeffectiveness of the proposed method, achieving an average performance of66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD,respectively.</description>
      <author>example@mail.com (Haoquan Lu, Hanzhe Liang, Jie Zhang, Chenxi Hu, Jinbao Wang, Can Gao)</author>
      <guid isPermaLink="false">2508.01311v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2508.01269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了ModelNet40-E基准数据集，用于评估点云分类模型在合成激光雷达噪声下的鲁棒性和校准能力。该数据集提供带噪声的点云和点级别的不确定性标注，使研究者能够进行更细致的不确定性建模评估。通过评估三种模型，发现Point Transformer v3在噪声环境下表现出更好的校准能力。&lt;h4&gt;背景&lt;/h4&gt;现有的点云分类模型评估基准缺乏对模型在噪声环境下鲁棒性和校准能力的全面评估，特别是缺乏点级别的不确定性标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的基准数据集ModelNet40-E，用于评估点云分类模型在合成激光雷达噪声环境下的鲁棒性和校准能力，并提供点级别的不确定性标注以支持细粒度的不确定性建模评估。&lt;h4&gt;方法&lt;/h4&gt;构建ModelNet40-E基准数据集，包含噪声损坏的点云和通过高斯噪声参数提供的不确定性标注。评估了三种流行的点云分类模型：PointNet、DGCNN和Point Transformer v3，使用分类准确率、校准指标和不确定性感知能力等多个指标在不同噪声水平下进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有模型在噪声水平增加时性能都会下降，但Point Transformer v3表现出更好的校准能力，其预测的不确定性更接近底层测量不确定性。&lt;h4&gt;结论&lt;/h4&gt;ModelNet40-E基准数据集为点云分类模型在噪声环境下的评估提供了新的工具，特别是不确定性建模的评估。Point Transformer v3在噪声环境下的校准表现优于其他评估的模型，这表明它在需要可靠不确定性估计的应用中具有优势。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了ModelNet40-E，这是一个新的基准，旨在评估点云分类模型在合成激光雷达类似噪声下的鲁棒性和校准能力。与现有基准不同，ModelNet40-E通过高斯噪声参数提供了噪声损坏的点云和点级别的不确定性标注，使不确定性建模能够进行细粒度评估。我们使用分类准确率、校准指标和不确定性感知能力，在多个噪声水平下评估了三种流行模型：PointNet、DGCNN和Point Transformer v3。虽然所有模型在噪声增加时性能都会下降，但Point Transformer v3表现出更好的校准能力，其预测的不确定性更接近底层测量不确定性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有点云分类基准缺乏真实感的问题，即现有数据集（如ModelNet40）使用干净、理想化的点云，无法反映真实传感器数据中的噪声和不确定性。这一问题在自动驾驶、机器人等安全关键应用中尤为重要，因为这些应用不仅需要模型做出准确预测，还需要评估预测的置信度，避免过于自信但错误的预测导致灾难性后果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有基准的局限性，包括缺乏真实噪声和不确定性标注。他们借鉴了激光雷达测量误差的实证研究（Gschwandtner et al. 2011），设计了包含范围相关噪声、角度相关噪声、系统偏差和随机异常值的激光雷达噪声模拟方法。基于ModelNet40数据集，他们添加了三种严重程度的噪声（轻度、中度、重度），并为每个点提供噪声统计信息（σ和μ），从而创建了一个更贴近真实场景的评估基准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个包含真实激光雷达样式噪声和不确定性标注的点云分类基准，用于评估模型在噪声环境下的分类性能和不确定性估计能力。整体流程包括：1) 基于原始ModelNet40数据集；2) 添加三种严重程度的激光雷达噪声；3) 为每个点计算噪声参数（包括范围相关噪声、角度相关噪声、系统偏差和异常值）；4) 评估三种模型（PointNet、DGCNN、Point Transformer v3）的性能；5) 使用分类准确率、校准误差和不确定性相关性作为评估指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出ModelNet40-E，一个不确定性感知的点云分类基准；2) 引入真实的激光雷达样式噪声而非随机扰动；3) 提供逐点噪声标注（σ和μ），支持细粒度不确定性评估；4) 全面评估模型在不同噪声水平下的分类准确率、校准和不确定性感知能力。相比之前工作，ModelNet40-E与ModelNet40相比添加了真实噪声和不确定性标注；与ModelNet-C相比使用更真实的激光雷达噪声并提供不确定性标注；评估方面不仅关注准确率，还关注校准和不确定性感知。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了ModelNet40-E，一个包含真实激光雷达噪声和不确定性标注的点云分类基准，用于评估点云分类模型在噪声环境下的鲁棒性和不确定性估计能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ModelNet40-E, a new benchmark designed to assess the robustnessand calibration of point cloud classification models under synthetic LiDAR-likenoise. Unlike existing benchmarks, ModelNet40-E provides both noise-corruptedpoint clouds and point-wise uncertainty annotations via Gaussian noiseparameters ({\sigma}, {\mu}), enabling fine-grained evaluation of uncertaintymodeling. We evaluate three popular models-PointNet, DGCNN, and PointTransformer v3-across multiple noise levels using classification accuracy,calibration metrics, and uncertainty-awareness. While all models degrade underincreasing noise, Point Transformer v3 demonstrates superior calibration, withpredicted uncertainties more closely aligned with the underlying measurementuncertainty.</description>
      <author>example@mail.com (Pedro Alonso, Tianrui Li, Chongshou Li)</author>
      <guid isPermaLink="false">2508.01269v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Point-wise Diffusion Models for Physical Systems with Shape Variations: Application to Spatio-temporal and Large-scale system</title>
      <link>http://arxiv.org/abs/2508.01230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型的逐点扩散模型，能够独立处理时空点，有效预测具有形状变化的复杂物理系统。该方法在各个时空点上应用前向和后向扩散过程，结合逐点扩散Transformer架构进行去噪，可直接处理网格和点云等数据格式，同时保持几何保真度。&lt;h4&gt;背景&lt;/h4&gt;传统基于图像的扩散模型在处理复杂物理系统预测时存在局限性，特别是在处理非结构化数据格式和实时应用方面。现有方法在训练效率、参数量和预测准确性方面仍有提升空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效处理复杂物理系统预测的逐点扩散模型，提高预测精度，减少训练时间和参数量，并实现实时预测能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种逐点扩散模型，在各个时空点上应用前向和后向扩散过程，结合逐点扩散Transformer架构进行去噪。采用去噪扩散隐式模型(DDIM)进行高效确定性采样，仅需5-10步。在三个物理领域验证该方法：2D时空系统(圆柱体流体和OLED液滴冲击测试)和3D大规模系统(汽车外空气动力学)。&lt;h4&gt;主要发现&lt;/h4&gt;1) 相比传统基于图像的扩散模型，训练时间减少94.4%，参数减少89.0%，预测准确率提高28%以上；2) 采用DDIM仅需5-10步采样，实现100-200倍的计算加速，同时不牺牲准确性；3) 与DeepONet和Meshgraphnet等代理模型相比，在所有测试物理系统中表现优越；4) 该方法能够直接处理网格和点云等数据格式，保持几何保真度。&lt;h4&gt;结论&lt;/h4&gt;逐点扩散模型在复杂物理系统预测中表现出优越的性能，具有高效性、准确性和灵活性，适用于实时预测应用。该方法通过减少训练时间、参数量和计算步骤，同时提高预测精度，为物理系统模拟提供了新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了一种新型的逐点扩散模型，它独立处理时空点以高效预测具有形状变化的复杂物理系统。该方法的主要贡献在于在各个时空点上应用前向和后向扩散过程，并结合逐点扩散Transformer架构进行去噪。与操作在结构化数据表示上的传统基于图像的扩散模型不同，该框架能够直接处理任何数据格式，包括网格和点云，同时保持几何保真度。我们在三个具有复杂几何配置的不同物理领域验证了我们的方法：包括圆柱体流体和OLED液滴冲击测试的2D时空系统，以及用于汽车外空气动力学的3D大规模系统。为了证明我们的逐点方法在实时预测应用中的必要性，我们采用去噪扩散隐式模型(DDIM)进行高效确定性采样，仅需5-10步，相比传统的1000步，在推理过程中实现了100到200倍的计算加速，同时不牺牲准确性。此外，与基于图像的扩散模型相比，我们提出的模型在训练时间上减少了94.4%，参数减少了89.0%，同时预测准确率提高了28%以上。与DeepONet和Meshgraphnet等数据灵活的代理模型的全面比较表明，我们的方法在所有三个物理系统中都表现出一致的优势。为了进一步完善提出的模型，我们研究了两个关键方面：1) 最终物理状态预测或增量变化预测的比较，以及2) 在不同下采样比率(10%-100%)下的计算效率评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何高效、准确地模拟具有复杂几何形状变化的物理系统的问题。这个问题在现实中非常重要，因为许多工程应用（如流体动力学、结构力学、气候建模等）需要快速模拟具有不同几何形状的物理系统，而传统数值模拟方法计算成本高，难以实时处理；在研究中，现有机器学习方法在处理不规则几何形状和保持几何保真度方面存在局限，无法满足形状设计过程中需要快速迭代评估不同几何配置性能的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法在处理形状变化物理系统时的局限性进行思考，发现基于图像的方法难以准确捕捉不规则几何形状，而网格和点云方法虽灵活但计算开销大。作者借鉴了扩散模型（如DDPM、DDIM）的基本框架，Transformer架构（如DiT）的思想，以及科学机器学习领域的成果（如Meshgraphnet和DeepONet），同时创新性地将扩散过程从全局图像处理改为点级独立处理，设计了点扩散变换器架构，采用DDIM进行高效确定性采样，并使用位置编码和时间嵌入增强模型的空间和时间感知能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在点级别独立执行扩散过程，而非传统的全局图像级别处理，使模型能够直接处理任何数据格式（包括网格和点云）并保持几何保真度，同时利用DDIM算法实现高效确定性采样。整体实现流程包括：1）数据准备，收集物理系统模拟数据并提取时空点坐标和物理量；2）前向扩散过程，对每个点逐步添加高斯噪声；3）模型训练，使用点扩散变换器架构学习去噪过程；4）后向扩散过程，采用DDIM算法仅需5-10步从噪声恢复原始物理量；5）预测与应用，对未见过的几何形状进行预测；6）模型优化，通过比较不同预测策略和采样比例优化性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）点级扩散框架，首次将扩散模型应用于点级独立处理；2）几何保真度保持，直接在原始几何结构上操作避免信息损失；3）高效确定性采样，将1000步采样减少到5-10步；4）非自回归时间建模，避免误差累积；5）条件注入机制，有效注入物理和几何条件。相比之前的工作，不同之处在于：相比图像扩散模型，不需规则网格表示且精度更高；相比网格图神经网络，消除计算开销和误差累积；相比基于坐标的方法，克服频谱偏差问题；相比其他扩散模型，避免几何信息损失和自然语言歧义影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的点扩散模型，通过在点级别独立执行扩散过程，实现了对具有复杂形状变化的物理系统的高效、高精度预测，同时保持了几何保真度和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a novel point-wise diffusion model that processesspatio-temporal points independently to efficiently predict complex physicalsystems with shape variations. This methodological contribution lies inapplying forward and backward diffusion processes at individual spatio-temporalpoints, coupled with a point-wise diffusion transformer architecture fordenoising. Unlike conventional image-based diffusion models that operate onstructured data representations, this framework enables direct processing ofany data formats including meshes and point clouds while preserving geometricfidelity. We validate our approach across three distinct physical domains withcomplex geometric configurations: 2D spatio-temporal systems including cylinderfluid flow and OLED drop impact test, and 3D large-scale system for road-carexternal aerodynamics. To justify the necessity of our point-wise approach forreal-time prediction applications, we employ denoising diffusion implicitmodels (DDIM) for efficient deterministic sampling, requiring only 5-10 stepscompared to traditional 1000-step and providing computational speedup of 100 to200 times during inference without compromising accuracy. In addition, ourproposed model achieves superior performance compared to image-based diffusionmodel: reducing training time by 94.4% and requiring 89.0% fewer parameterswhile achieving over 28% improvement in prediction accuracy. Comprehensivecomparisons against data-flexible surrogate models including DeepONet andMeshgraphnet demonstrate consistent superiority of our approach across allthree physical systems. To further refine the proposed model, we investigatetwo key aspects: 1) comparison of final physical states prediction orincremental change prediction, and 2) computational efficiency evaluationacross varying subsampling ratios (10%-100%).</description>
      <author>example@mail.com (Jiyong Kim, Sunwoong Yang, Namwoo Kang)</author>
      <guid isPermaLink="false">2508.01230v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding</title>
      <link>http://arxiv.org/abs/2508.01197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IROS 2025 Accepted Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了3D占用目标定位的新基准和模型，解决了传统边界框表示物体不准确的问题，通过结合自然语言与体素级注释提供了更精确的物体感知方法。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉目标定位任务通常依赖于边界框，但边界框往往无法捕捉细粒度细节，因为边界框内的所有体素并非都被占用，导致物体表示不准确。&lt;h4&gt;目的&lt;/h4&gt;引入具有挑战性的户外场景3D占用目标定位基准，并提出一种端到端模型GroundingOcc，用于更精确的3D占用目标定位。&lt;h4&gt;方法&lt;/h4&gt;提出GroundingOcc模型，通过多模态学习结合视觉、文本和点云特征，从粗到细预测物体位置和占用信息。该模型包含多模态编码器、占用头、定位头，以及增强几何理解的2D目标定位模块和深度估计模块。&lt;h4&gt;主要发现&lt;/h4&gt;在基准上的大量实验表明，提出的方法在3D占用目标定位上优于现有的基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过新的3D占用目标定位基准和GroundingOcc模型，研究提供了比传统目标定位任务更精确的物体感知方法，有助于自动驾驶中的空间感知。&lt;h4&gt;翻译&lt;/h4&gt;视觉目标定位旨在根据自然语言描述识别场景中的物体或区域，对自动驾驶中的空间感知至关重要。然而，现有的视觉目标定位任务通常依赖于边界框，这些边界框往往无法捕捉细粒度细节。边界框内的并非所有体素都被占用，导致物体表示不准确。为了解决这个问题，我们引入了一个具有挑战性的户外场景3D占用目标定位基准。该基准基于nuScenes数据集构建，将自然语言与体素级占用注释相结合，与传统目标定位任务相比，提供了更精确的物体感知。此外，我们提出了GroundingOcc，这是一个专为3D占用目标定位设计的端到端模型，通过多模态学习实现。它结合视觉、文本和点云特征，从粗到细预测物体位置和占用信息。具体而言，GroundingOcc包含一个用于特征提取的多模态编码器，一个用于体素级预测的占用头，以及一个用于精确定位的定位头。此外，2D目标定位模块和深度估计模块增强了几何理解，从而提高了模型性能。在基准上的大量实验表明，我们的方法在3D占用目标定位上优于现有的基线方法。该数据集可在https://github.com/RONINGOD/GroundingOcc获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D占用定位问题，即根据自然语言描述在3D场景中精确定位物体并预测其占用状态。这个问题在自动驾驶领域非常重要，因为传统的边界框定位无法准确表示复杂或不规则形状的物体，而精确的物体表示对于自动驾驶系统做出正确决策至关重要，特别是在处理不规则形状或部分遮挡的物体时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视觉定位任务的局限性，特别是边界框表示的不足。他们注意到现有数据集主要关注基于边界框的理解，缺乏细粒度的体素级占用预测。作者借鉴了多模态学习思想，结合视觉、文本和点云特征，设计了GroundingOcc模型。该方法整合了2D视觉定位、3D视觉定位和多模态占用感知等领域的研究成果，但将其扩展到新的3D占用定位任务，并通过添加2D定位模块和深度估计模块来增强几何理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态学习，结合视觉、文本和点云特征，从粗到细地预测物体的位置和占用信息，克服传统边界框表示的局限性。整体流程包括：1)输入多模态数据(多视角图像、点云和自然语言)；2)使用编码器提取各模态特征；3)通过Vision-Language PAN模块增强视觉和文本表示间的交互；4)利用2D定位和深度估计等辅助任务增强几何理解；5)通过体素编码和融合生成最终特征；6)输出3D占用预测和可选的3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将占用预测引入3D视觉定位；2)提出新的基准数据集Talk2Occ；3)设计端到端多分支网络GroundingOcc；4)利用2D定位和深度估计提高准确性；5)引入几何监督而非仅依赖语义监督。相比之前工作，本文从边界框定位转向体素级占用定位，结合多模态信息，引入辅助任务增强几何理解，使用几何监督提高模型性能，并提出了专门用于评估3D占用定位任务的新基准数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过提出GroundingOcc模型和Talk2Occ基准数据集，将多模态学习与体素级占用预测相结合，实现了比传统边界框定位更精确的3D物体表示，为自动驾驶中的细粒度场景理解提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding aims to identify objects or regions in a scene based onnatural language descriptions, essential for spatially aware perception inautonomous driving. However, existing visual grounding tasks typically dependon bounding boxes that often fail to capture fine-grained details. Not allvoxels within a bounding box are occupied, resulting in inaccurate objectrepresentations. To address this, we introduce a benchmark for 3D occupancygrounding in challenging outdoor scenes. Built on the nuScenes dataset, itintegrates natural language with voxel-level occupancy annotations, offeringmore precise object perception compared to the traditional grounding task.Moreover, we propose GroundingOcc, an end-to-end model designed for 3Doccupancy grounding through multi-modal learning. It combines visual, textual,and point cloud features to predict object location and occupancy informationfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoderfor feature extraction, an occupancy head for voxel-wise predictions, and agrounding head to refine localization. Additionally, a 2D grounding module anda depth estimation module enhance geometric understanding, thereby boostingmodel performance. Extensive experiments on the benchmark demonstrate that ourmethod outperforms existing baselines on 3D occupancy grounding. The dataset isavailable at https://github.com/RONINGOD/GroundingOcc.</description>
      <author>example@mail.com (Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu)</author>
      <guid isPermaLink="false">2508.01197v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception</title>
      <link>http://arxiv.org/abs/2508.01062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CP-FREEZER的新型延迟攻击，通过V2V消息注入对抗性扰动，显著增加合作感知系统的计算延迟，威胁自动驾驶系统的及时性和可用性。&lt;h4&gt;背景&lt;/h4&gt;合作感知通过交换和组合多个智能体的消息来增强联网和自动驾驶车辆的环境感知能力。先前研究主要关注针对感知准确性的对抗攻击，但对CP系统在及时性方面的鲁棒性研究不足，而及时性是自动驾驶的安全关键要求。&lt;h4&gt;目的&lt;/h4&gt;研究合作感知系统在面对针对及时性(可用性)的攻击时的脆弱性，并开发一种能够最大化CP算法计算延迟的攻击方法。&lt;h4&gt;方法&lt;/h4&gt;提出CP-FREEZER攻击方法，通过V2V消息注入对抗性扰动来增加延迟。该方法解决了点云预处理非可微性、传输延迟导致的受害者输入异步知识等挑战，并使用新型损失函数有效最大化CP管道的执行时间。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明CP-FREEZER将端到端CP延迟增加了90倍以上，在真实车辆测试平台上以100%的成功率将每帧处理时间推至3秒以上，揭示了CP系统可用性的关键威胁。&lt;h4&gt;结论&lt;/h4&gt;合作感知系统在面对可用性攻击时存在严重威胁，突显了开发鲁棒防御措施的迫切需求。&lt;h4&gt;翻译&lt;/h4&gt;合作感知(CP)通过交换和组合多个智能体的消息来增强联网和自动驾驶车辆的环境感知能力。虽然先前工作已经探索了降低感知准确性的对抗完整性攻击，但CP系统针对及时性(或可用性)攻击的鲁棒性仍知之甚少，这对自动驾驶是一项安全关键要求。在本文中，我们提出了CP-FREEZER，这是第一种通过V2V消息注入对抗性扰动来最大化CP算法计算延迟的延迟攻击。我们的攻击解决了几个独特挑战，包括点云预处理的非可微性、由于传输延迟导致的受害者输入异步知识，并使用了一种新的损失函数，有效最大化CP管道的执行时间。大量实验表明，CP-FREEZER将端到端CP延迟增加了90倍以上，在我们的真实车辆测试平台上以100%的成功率将每帧处理时间推至3秒以上。我们的发现揭示了CP系统可用性的关键威胁，突显了开发鲁棒防御措施的迫切需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cooperative perception (CP) enhances situational awareness of connected andautonomous vehicles by exchanging and combining messages from multiple agents.While prior work has explored adversarial integrity attacks that degradeperceptual accuracy, little is known about CP's robustness against attacks ontimeliness (or availability), a safety-critical requirement for autonomousdriving. In this paper, we present CP-FREEZER, the first latency attack thatmaximizes the computation delay of CP algorithms by injecting adversarialperturbation via V2V messages. Our attack resolves several unique challenges,including the non-differentiability of point cloud preprocessing, asynchronousknowledge of the victim's input due to transmission delays, and uses a novelloss function that effectively maximizes the execution time of the CP pipeline.Extensive experiments show that CP-FREEZER increases end-to-end CP latency byover $90\times$, pushing per-frame processing time beyond 3 seconds with a 100%success rate on our real-world vehicle testbed. Our findings reveal a criticalthreat to the availability of CP systems, highlighting the urgent need forrobust defenses.</description>
      <author>example@mail.com (Chenyi Wang, Ruoyu Song, Raymond Muller, Jean-Philippe Monteuuis, Z. Berkay Celik, Jonathan Petit, Ryan Gerdes, Ming Li)</author>
      <guid isPermaLink="false">2508.01062v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion</title>
      <link>http://arxiv.org/abs/2508.01778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DiffSemanticFusion是一种融合框架，用于多模态轨迹预测和规划，结合了基于光栅和基于图表示方法的优点，提高了在线高清地图的稳定性和表现力。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶需要准确理解场景，包括道路几何形状、交通参与者及其语义关系。在线高清地图生成场景中，基于光栅的表示适合视觉模型但缺乏几何精度，而基于图的表示保留了结构细节但没有精确地图时会变得不稳定。&lt;h4&gt;目的&lt;/h4&gt;利用基于光栅和基于图两种表示方法的互补优势，提出DiffSemanticFusion融合框架，用于多模态轨迹预测和规划。&lt;h4&gt;方法&lt;/h4&gt;在语义光栅融合BEV空间上推理，增强地图扩散模块，提高在线高清地图表示的稳定性和表现力。在轨迹预测和面向规划的全栈自动驾驶两个下游任务上验证框架。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和NAVSIM真实自动驾驶基准测试上，性能优于几种最先进方法。在nuScenes预测任务中，与QCNet集成实现5.1%性能提升；在NAVSIM全栈自动驾驶中，NavHard场景实现15%性能提升。地图扩散模块可无缝集成到其他基于向量的方法中提高性能。&lt;h4&gt;结论&lt;/h4&gt;DiffSemanticFusion有效结合了基于光栅和基于图表示方法的优点，提高了在线高清地图的稳定性和表现力，在多个任务和基准测试上取得显著性能提升。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶需要准确理解场景，包括道路几何形状、交通参与者及其语义关系。在线高清地图生成场景中，基于光栅的表示适合视觉模型但缺乏几何精度，而基于图的表示保留了结构细节但没有精确地图时会变得不稳定。为了利用这两种方法的互补优势，我们提出了DiffSemanticFusion——一个用于多模态轨迹预测和规划的融合框架。我们的方法在语义光栅融合的BEV空间上进行推理，并通过增强地图扩散模块提高了在线高清地图表示的稳定性和表现力。我们在两个下游任务上验证了我们的框架：轨迹预测和面向规划的全栈自动驾驶。在nuScenes和NAVSIM真实自动驾驶基准测试上的实验表明，性能优于几种最先进方法。对于nuScenes上的预测任务，我们将DiffSemanticFusion与在线高清地图信息QCNet集成，实现了5.1%的性能提升。在NAVSIM的全栈自动驾驶中，DiffSemanticFusion实现了最先进的结果，在NavHard场景中实现了15%的性能提升。此外，大量的消融和敏感性研究表明，我们的地图扩散模块可以无缝集成到其他基于向量的方法中以提高性能。所有代码和资源可在https://github.com/SunZhigang7/DiffSemanticFusion获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中在线高清地图(online HD map)在噪声、不完整或错位条件下的鲁棒性问题，以及如何有效融合栅格表示和图表示两种场景表示方法的优势。这个问题在现实中非常重要，因为自动驾驶需要准确理解道路几何、交通参与者及其语义关系，而地图质量问题会显著影响下游任务如运动预测和规划，特别是在动态或未见过的环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶中场景表示的两种主要方法(栅格表示和图表示)的优缺点，认识到需要结合两者的优势。他们借鉴了扩散模型在图像生成和轨迹预测中的应用(如MotionDiffuser)，参考了BEV表示方法(如BEVDet)和图神经网络在交通场景表示中的应用(如SemanticFormer)，以及端到端自动驾驶框架(如DiffusionDrive)。在此基础上，作者创新性地设计了在线高清地图扩散模块和语义栅格BEV融合框架，通过扩散模型增强地图表示，并在统一的BEV空间中融合多种表示方法的优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型增强在线高清地图的表示，提高其在噪声或不完整条件下的鲁棒性，同时融合栅格表示、图表示和BEV特征表示的优点。整体实现流程包括：1)混合感知模块提取稀疏和密集场景表示；2)在线高清地图扩散模块对稀疏车道向量添加噪声并预测去噪结果；3)语义栅格图像融合模块将不同表示投影到统一空间；4)DiffSemanticFusion架构融合多模态特征并通过扩散模型解码轨迹；5)端到端训练整个模型，优化扩散损失和原始任务损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)在线高清地图扩散模块，首次将扩散模型应用于地图表示增强而非直接轨迹生成；2)语义栅格BEV融合框架，首个统一稀疏场景表示和密集BEV特征的端到端自动驾驶框架；3)多模态融合方法，有效结合栅格、图和BEV特征的互补特性。相比之前工作，本文创新性地使用扩散模型增强地图表示而非直接生成轨迹，首次统一了稀疏和密集表示方法，并将地图视为需要动态优化的表示而非固定输入。实验表明，该方法在nuScenes预测任务上提升5.1%，在NAVSIM NavHard场景上提升15%性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了DiffSemanticFusion，一种结合在线高清地图扩散和语义栅格BEV融合的创新框架，有效解决了自动驾驶中地图不完整和表示方法单一的问题，显著提升了轨迹预测和规划的准确性与鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving requires accurate scene understanding, including roadgeometry, traffic agents, and their semantic relationships. In online HD mapgeneration scenarios, raster-based representations are well-suited to visionmodels but lack geometric precision, while graph-based representations retainstructural detail but become unstable without precise maps. To harness thecomplementary strengths of both, we propose DiffSemanticFusion -- a fusionframework for multimodal trajectory prediction and planning. Our approachreasons over a semantic raster-fused BEV space, enhanced by a map diffusionmodule that improves both the stability and expressiveness of online HD maprepresentations. We validate our framework on two downstream tasks: trajectoryprediction and planning-oriented end-to-end autonomous driving. Experiments onreal-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrateimproved performance over several state-of-the-art methods. For the predictiontask on nuScenes, we integrate DiffSemanticFusion with the online HD mapinformed QCNet, achieving a 5.1\% performance improvement. For end-to-endautonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-artresults, with a 15\% performance gain in NavHard scenarios. In addition,extensive ablation and sensitivity studies show that our map diffusion modulecan be seamlessly integrated into other vector-based approaches to enhanceperformance. All artifacts are available athttps://github.com/SunZhigang7/DiffSemanticFusion.</description>
      <author>example@mail.com (Zhigang Sun, Yiru Wang, Anqing Jiang, Shuo Wang, Yu Gao, Yuwen Heng, Shouyi Zhang, An He, Hao Jiang, Jinhao Chai, Zichong Gu, Wang Jijun, Shichen Tang, Lavdim Halilaj, Juergen Luettin, Hao Sun)</author>
      <guid isPermaLink="false">2508.01778v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing</title>
      <link>http://arxiv.org/abs/2508.01740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AG$^2$aussian的新型框架，利用锚图结构组织语义特征并调节基本高斯，实现了干净准确的实例级高斯选择，在四个应用场景中验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting (3DGS)已在不同领域获得广泛应用，对语义感知的3D高斯表示的需求日益增长，以支持场景理解和编辑任务。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中语义特征附加到自由高斯上导致的嘈杂分割和杂乱高斯选择问题，实现干净准确的实例级高斯选择。&lt;h4&gt;方法&lt;/h4&gt;提出AG$^2$aussian框架，利用锚图结构组织语义特征和调节基本高斯，促进紧凑和实例感知的高斯分布分布，支持基于图的传播。&lt;h4&gt;主要发现&lt;/h4&gt;在四个应用场景（基于点击的交互式查询、开放词汇文本驱动查询、物体移除编辑和物理模拟）中验证了该方法的优势，实验和消融研究证实了关键设计的有效性。&lt;h4&gt;结论&lt;/h4&gt;AG$^2$aussian框架在各种应用中显示出优势，能够实现干净准确的实例级高斯选择，为语义感知的3D高斯表示提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;三维高斯散射(3DGS)已在不同应用中见证指数级增长，推动了语义感知的3D高斯表示的迫切需求，以实现场景理解和编辑任务。现有方法通常将语义特征附加到一组自由高斯上，并通过可微分渲染来蒸馏这些特征，导致嘈杂的分割和杂乱的高斯选择。在本文中，我们引入了AG$^2$aussian，一种利用锚图结构来组织语义特征和调节基本高斯的新型框架。我们的锚图结构不仅促进了紧凑和实例感知的高斯分布，而且支持基于图的传播，实现了干净准确的实例级高斯选择。在四个应用（即基于点击的交互式查询、开放词汇文本驱动查询、物体移除编辑和物理模拟）上的广泛验证证明了我们方法的优势及其对各种应用的益处。实验和消融研究进一步评估了我们方法关键设计的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景理解和编辑中的实例级高斯选择不准确问题。现有方法将语义特征直接分配给自由高斯集合，导致分割结果有噪声，选择时包含周围多余高斯而内部高斯被遗漏。这个问题很重要，因为准确的对象选择是3D场景编辑和物理模拟等应用的基础，不准确的选择会严重影响后续应用效果，限制了3D场景理解和编辑的能力发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：自由高斯过度扩展、特征分配歧义性以及忽略3D空间约束。基于这些分析，作者构思出构建锚点图结构来组织语义特征并约束高斯分布。设计过程中借鉴了Scaffold-GS的结构化思想、OpenGaussian的对比学习策略、SAM的实例分割和CLIP的语言特征提取等技术，但创新性地将它们整合到锚点图框架中，形成了一个三阶段流程：锚点-高斯生长、锚点图传播和语言特征附加。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建锚点图结构来组织语义特征并约束高斯分布，每个锚点附着语义特征和一小组高斯，从而实现紧凑且实例感知的高斯分布，并通过图传播提高特征准确性。整体流程分为三阶段：1)锚点-高斯生长：初始化语义锚点和相关高斯，通过体素化定位锚点，优化高斯参数并进行语义对比学习；2)锚点图传播：构建基于体素邻域的锚点图，使用图拉普拉斯传播细化语义特征；3)语言特征附加：通过图聚类定位对象实例，匹配实例图与掩码，附加CLIP语言特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)锚点图结构化的3D高斯表示，将语义特征组织成更高层次结构；2)锚点图特征传播算法，通过图拉普拉斯传播提高特征准确性；3)应用于多种3D场景理解和编辑任务。相比之前工作，不同之处在于：与SAGA等方法相比产生更清洁的语义特征和更精确的查询结果；与OpenGaussian相比显式调节高斯并采用图传播；与Scaffold-GS相比限制高斯在锚点体素内并使用图传播；与SuperGSeg相比特征细化更有效且内存成本更低。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了AG2aussian，一种通过锚点图结构化高斯飞溅的方法，实现了干净准确的实例级3D场景理解和编辑，显著提高了对象相关高斯选择的精确度并支持多种应用场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has witnessed exponential adoption acrossdiverse applications, driving a critical need for semantic-aware 3D Gaussianrepresentations to enable scene understanding and editing tasks. Existingapproaches typically attach semantic features to a collection of free Gaussiansand distill the features via differentiable rendering, leading to noisysegmentation and a messy selection of Gaussians. In this paper, we introduceAG$^2$aussian, a novel framework that leverages an anchor-graph structure toorganize semantic features and regulate Gaussian primitives. Our anchor-graphstructure not only promotes compact and instance-aware Gaussian distributions,but also facilitates graph-based propagation, achieving a clean and accurateinstance-level Gaussian selection. Extensive validation across fourapplications, i.e. interactive click-based query, open-vocabulary text-drivenquery, object removal editing, and physics simulation, demonstrates theadvantages of our approach and its benefits to various applications. Theexperiments and ablation studies further evaluate the effectiveness of the keydesigns of our approach.</description>
      <author>example@mail.com (Zhaonan Wang, Manyi Li, Changhe Tu)</author>
      <guid isPermaLink="false">2508.01740v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.01713v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了TOPICS+方法，专门针对机器人辅助手术场景的鲁棒分割问题，通过引入Dice损失、层次伪标记和定制标签分类法，解决了传统分割模型在动态手术环境中的局限性。&lt;h4&gt;背景&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全引导手术器械，但静态数据集上训练的分割模型在部署到动态演变的手术环境中时面临关键限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强的TOPICS+方法，专门针对手术场景的鲁棒分割，使模型能够持续适应新类别而避免灾难性遗忘。&lt;h4&gt;方法&lt;/h4&gt;基于TOPICS方法提出增强版本TOPICS+，将Dice损失合并到层次损失公式中处理类别不平衡，引入层次伪标记，为机器人手术环境设计定制标签分类法，提出六个新的CISS基准测试，并在Syn-Mediverse数据集上创建包含144多个类别的标签集。&lt;h4&gt;主要发现&lt;/h4&gt;TOPICS+方法能够有效处理手术环境中的强类别不平衡问题，通过层次伪标记和定制标签分类法提高了分割性能，六个新的CISS基准测试为评估提供了标准。&lt;h4&gt;结论&lt;/h4&gt;TOPICS+方法为机器人辅助手术提供了更有效的场景理解解决方案，通过公开代码和训练模型促进了该领域的研究发展。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全地引导手术器械。然而，在静态数据集上训练的分割模型在部署到这些动态和不断演变的手术环境时面临关键限制。类增量语义分割(CISS)允许模型持续适应新类别，同时避免对先前知识的灾难性遗忘，而无需在先前数据上训练。在这项工作中，我们基于最近引入的面向分类学的庞加莱正则化增量类别分割(TOPICS)方法，提出了一个增强版本，称为TOPICS+，专门针对手术场景的鲁棒分割。具体来说，我们将Dice损失合并到层次损失公式中以处理强类别不平衡，引入层次伪标记，并为机器人手术环境设计定制的标签分类法。我们还提出了六个专为机器人手术环境设计的新型CISS基准，包括多个增量步骤和几个语义类别，以模拟手术环境中真实的类增量设置。此外，我们在Syn-Mediverse合成数据集上引入了一个包含144多个类别的 refined 标签集，在线托管作为评估基准。我们在http://topics.cs.uni-freiburg.de公开了代码和训练模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人辅助手术中，当手术环境动态变化时，模型如何持续学习新类别而不忘记已学知识的问题（即类增量语义分割问题）。这个问题非常重要，因为机器人手术在全球快速增长，手术环境复杂多变，医院需要快速适应新设备和患者群体，而传统方法存在'灾难性遗忘'问题，且临床场景因隐私限制无法无限制访问历史数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了机器人手术场景的特殊需求和挑战，借鉴了TOPICS（Taxonomy-Oriented Poincaré-regularized Incremental Class Segmentation）方法作为基础，并针对手术场景进行了改进。作者还参考了双曲空间中树状结构建模的知识表示方法，以及医学图像分类中的持续学习技术。通过分析现有工作的局限性，作者设计了专门针对手术场景的层次化Dice损失、层次化伪标记和定制标签分类法，以处理类别不平衡和背景复杂度多样等问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在双曲空间中构建层次化的类别表示，使模型能够在新旧类别间保持知识连续性，同时处理手术场景中的类别不平衡和背景复杂度问题。整体实现流程包括：1)使用DeepLabV3和ResNet-101构建基础模型；2)在双曲空间中编码类别层次关系；3)应用层次化Dice损失处理类别不平衡；4)使用层次化伪标记维持旧类知识；5)设计适合手术场景的标签分类法；6)在不同数据集和增量设置下训练评估模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)TOPICS+方法，专门针对机器人手术场景优化的不基于回放的类增量分割；2)层次化Dice损失，处理手术中的强烈类别不平衡；3)层次化伪标记，改善不同背景复杂度下的准确性；4)定制标签分类法，防止知识遗忘；5)六个新的CISS基准测试，模拟真实手术环境；6)扩展Syn-Mediverse数据集，创建144+手术环境细分类别。相比之前工作，TOPICS+专门针对手术场景优化，使用双曲空间层次编码，结合了层次化Dice损失和伪标记，提供了更全面的评估基准，并解决了背景偏移等特定问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了TOPICS+方法，使机器人辅助手术系统能够在动态变化的环境中持续学习新的手术工具和组织类型，同时保持对已学知识的记忆，无需访问历史数据，从而提高了手术的精确性和安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot-assisted surgeries rely on accurate and real-time scene understandingto safely guide surgical instruments. However, segmentation models trained onstatic datasets face key limitations when deployed in these dynamic andevolving surgical environments. Class-incremental semantic segmentation (CISS)allows models to continually adapt to new classes while avoiding catastrophicforgetting of prior knowledge, without training on previous data. In this work,we build upon the recently introduced Taxonomy-Oriented Poincar\'e-regularizedIncremental Class Segmentation (TOPICS) approach and propose an enhancedvariant, termed TOPICS+, specifically tailored for robust segmentation ofsurgical scenes. Concretely, we incorporate the Dice loss into the hierarchicalloss formulation to handle strong class imbalances, introduce hierarchicalpseudo-labeling, and design tailored label taxonomies for robotic surgeryenvironments. We also propose six novel CISS benchmarks designed for roboticsurgery environments including multiple incremental steps and several semanticcategories to emulate realistic class-incremental settings in surgicalenvironments. In addition, we introduce a refined set of labels with more than144 classes on the Syn-Mediverse synthetic dataset, hosted online as anevaluation benchmark. We make the code and trained models publicly available athttp://topics.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Julia Hindel, Ema Mekic, Enamundram Naga Karthik, Rohit Mohan, Daniele Cattaneo, Maria Kalweit, Abhinav Valada)</author>
      <guid isPermaLink="false">2508.01713v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding</title>
      <link>http://arxiv.org/abs/2508.01150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了OpenGS-Fusion，一种创新的开词汇密集映射框架，结合3D高斯表示和截断符号距离场，实现语义特征的实时无损融合，并通过多模态语言引导的自适应阈值方法显著提高了3D对象分割效果。&lt;h4&gt;背景&lt;/h4&gt;尽管3D场景理解在VR/AR和机器人应用方面取得了显著进展，但现有方法受限于僵化的离线流程，无法根据开放查询提供精确的3D对象级理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够提供精确3D对象级理解并支持开放词汇查询交互的框架，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出OpenGS-Fusion框架，结合3D高斯表示与截断符号距离场实现语义特征实时无损融合；引入MLLM-Assisted Adaptive Thresholding方法，通过自适应调整相似度阈值改进3D对象分割。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，该方法在3D对象理解、场景重建质量方面优于现有方法；与固定阈值策略相比，自适应阈值方法使3D mIoU提升了17%；该方法在语言引导的场景交互中表现出色。&lt;h4&gt;结论&lt;/h4&gt;OpenGS-Fusion框架显著提升了3D场景理解和交互能力，为VR/AR和机器人应用提供了更精确的对象级理解工具。&lt;h4&gt;翻译&lt;/h4&gt;最近3D场景理解的进展使得使用开放词汇查询与场景进行交互取得了显著进步，特别是在VR/AR和机器人应用方面。然而，现有方法受限于僵化的离线流程，无法根据开放查询提供精确的3D对象级理解。在本文中，我们提出了OpenGS-Fusion，一种创新的开词汇密集映射框架，改进了语义建模并优化了对象级理解。OpenGS-Fusion将3D高斯表示与截断符号距离场相结合，实现语义特征的实时无损融合。此外，我们引入了一种名为MLLM-Assisted Adaptive Thresholding的新型多模态语言引导方法，通过自适应调整相似度阈值来改进3D对象分割，与固定阈值策略相比，3D mIoU提高了17%。大量实验证明，我们的方法在3D对象理解和场景重建质量方面优于现有方法，同时展示了其在语言引导场景交互中的有效性。代码可在https://young-bit.github.io/opengs-fusion.github.io/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个问题：一是传统3D场景理解方法的刚性离线流水线问题，无法支持在线感知；二是现有方法在3D对象级理解上的局限性，无法提供精确的3D对象模型且使用固定阈值导致对象边界划分不准确。这些问题对于VR/AR应用、机器人导航和物体操作等现实应用至关重要，因为这些应用需要通过开放词汇查询与场景实时交互，并获取精确的3D对象信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：点云和体素方法难以高保真重建，NeRF方法效率低下，基于高斯溅射的方法存在离线流水线和3D对象理解不足的问题。在此基础上，作者借鉴了3D高斯溅射的训练效率和显式表示优势，结合截断符号距离场(TSDF)辅助初始化和语义融合，并利用视觉语言模型(如CLIP)提取语义特征。同时，作者创新性地设计了混合场景表示和MLLM辅助的自适应阈值调整机制，解决了现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用混合场景表示(TSDF-3DGS)同时建模场景的外观、几何和语义特征，并利用多模态大语言模型辅助的自适应阈值调整提高3D对象定位精度。整体流程包括：1)接收RGB-D输入并提取2D语义特征；2)使用基于VDB的TSDF融合更新全局几何；3)在高密度区域初始化高斯原始体并融合语义特征；4)应用场景优化策略提升映射质量；5)通过开放词汇查询和AT-MLLM机制实现精确的3D对象定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)OpenGS-Fusion框架，支持在线建模的开放词汇密集映射；2)混合3D高斯场景表示，结合TSDF和3DGS的优势；3)MLLM辅助的自适应阈值调整机制，显著提高对象定位精度。相比之前的工作，OpenGS-Fusion不再依赖离线流水线，支持实时更新；提供精确的3D对象级理解而非仅2D渲染；使用自适应阈值而非固定阈值提高边界划分准确性；同时支持高质量场景重建和对象定位，更适合机器人导航和交互式任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenGS-Fusion通过结合3D高斯溅射与截断符号距离场的混合表示以及多模态大语言模型辅助的自适应阈值调整，实现了支持在线更新的开放词汇3D场景理解，显著提高了3D对象定位精度和场景重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in 3D scene understanding have made significant stridesin enabling interaction with scenes using open-vocabulary queries, particularlyfor VR/AR and robotic applications. Nevertheless, existing methods are hinderedby rigid offline pipelines and the inability to provide precise 3D object-levelunderstanding given open-ended queries. In this paper, we presentOpenGS-Fusion, an innovative open-vocabulary dense mapping framework thatimproves semantic modeling and refines object-level understanding.OpenGS-Fusion combines 3D Gaussian representation with a Truncated SignedDistance Field to facilitate lossless fusion of semantic features on-the-fly.Furthermore, we introduce a novel multimodal language-guided approach namedMLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3Dobjects by adaptively adjusting similarity thresholds, achieving an improvement17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experimentsdemonstrate that our method outperforms existing methods in 3D objectunderstanding and scene reconstruction quality, as well as showcasing itseffectiveness in language-guided scene interaction. The code is available athttps://young-bit.github.io/opengs-fusion.github.io/ .</description>
      <author>example@mail.com (Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang)</author>
      <guid isPermaLink="false">2508.01150v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>3D Reconstruction via Incremental Structure From Motion</title>
      <link>http://arxiv.org/abs/2508.01019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures, proceedings in International Bhurban Conference  on Applied Sciences &amp; Technology (IBCAST) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种增量式结构运动（SfM）方法的详细实现，该方法通过逐步整合新视图到三维重建中，能够在稀疏或部分重叠的数据集中恢复场景结构和相机运动。&lt;h4&gt;背景&lt;/h4&gt;从非结构化图像集合中进行精确的三维重建是机器人技术、地图绘制和场景理解等应用中的关键需求。全局式SfM技术依赖完整的图像连接性，对噪声或缺失数据较为敏感。&lt;h4&gt;目的&lt;/h4&gt;展示增量式SfM流程的详细实现，重点关注几何估计的一致性和通过捆集调整进行迭代优化的效果，并评估其在实际应用中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;实现增量式SfM流程，通过逐步整合新视图到重建中，使用捆集调整进行迭代优化，并通过重投影误差和相机轨迹一致性评估重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;增量式SfM能够在稀疏或部分重叠的数据集中恢复场景结构和相机运动，实验验证了其在视觉结构化环境中进行稀疏三维重建的有效性。&lt;h4&gt;结论&lt;/h4&gt;增量式SfM是一种可靠的稀疏三维重建方法，在视觉结构化环境中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;从非结构化图像集合中进行精确的三维重建是机器人技术、地图绘制和场景理解等应用中的关键需求。虽然全局式结构运动（SfM）技术依赖完整的图像连接性，并且可能对噪声或缺失数据敏感，但增量式SfM提供了更灵活的替代方案。通过逐步将新视图整合到重建中，它使系统能够在稀疏或部分重叠的数据集中恢复场景结构和相机运动。在本文中，我们详细介绍了增量式SfM流程的实现，重点关注几何估计的一致性和通过捆集调整进行迭代优化的效果。我们使用真实数据集演示了该方法，并通过重投影误差和相机轨迹一致性评估了重建质量。结果支持了增量式SfM作为在视觉结构化环境中进行稀疏三维重建的可靠方法的实际效用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从非结构化图像集合中进行准确的3D重建问题。这个问题在现实中非常重要，因为它在机器人导航、地图绘制、场景理解、文化遗产保护和智慧城市发展等多个领域都有广泛应用。相比全局SfM技术，增量式SfM对噪声和缺失数据更具鲁棒性，能够处理稀疏或部分重叠的数据集，这对于实际应用场景中的图像收集尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者重新审视了传统几何方法，强调方法的模块化和可解释性，关注几何估计的一致性和通过捆绑调整进行迭代细化的效果。他们借鉴了多项现有工作：增量SfM的基本框架、捆绑调整优化技术、SIFT特征提取算法、RANSAC几何验证方法、本质矩阵分解和PnP姿态估计算法。作者还与现有的COLMAP工具进行了比较，以验证自己方法的性能和优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增量式重建从初始图像对开始，逐步添加新图像，而不是一次性处理所有图像，同时通过捆绑调整不断优化相机姿态和3D点位置以保持几何一致性。整体流程包括：1)使用SIFT算法检测图像特征并生成描述符；2)通过暴力匹配和RANSAC进行特征匹配和几何验证；3)选择最佳初始图像对估计相对姿态并三角测量初始3D点；4)使用PnP算法逐步添加新图像并估计其姿态；5)三角测量新的3D点并加入点云；6)通过捆绑调整联合优化所有相机参数和3D点位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了模块化、可解释的增量SfM流水线；2)特别关注了几何估计的一致性和迭代细化效果；3)通过真实数据集和重投影误差分析验证了实用性。相比之前的工作，不同之处在于：相比全局SfM，增量式方法对噪声和缺失数据更鲁棒；相比深度学习方法，重新强调了传统几何方法的透明度和灵活性；相比现有工具如COLMAP，虽然性能相当，但提供了更大的透明度和灵活性，更适合研究和实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种模块化、可解释的增量式结构从运动流水线，展示了传统几何方法在从无序图像集进行准确且一致的稀疏3D重建方面的有效性，同时提供了比现有工具更大的透明度和灵活性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D reconstruction from unstructured image collections is a keyrequirement in applications such as robotics, mapping, and scene understanding.While global Structure from Motion (SfM) techniques rely on full imageconnectivity and can be sensitive to noise or missing data, incremental SfMoffers a more flexible alternative. By progressively incorporating new viewsinto the reconstruction, it enables the system to recover scene structure andcamera motion even in sparse or partially overlapping datasets. In this paper,we present a detailed implementation of the incremental SfM pipeline, focusingon the consistency of geometric estimation and the effect of iterativerefinement through bundle adjustment. We demonstrate the approach using a realdataset and assess reconstruction quality through reprojection error and cameratrajectory coherence. The results support the practical utility of incrementalSfM as a reliable method for sparse 3D reconstruction in visually structuredenvironments.</description>
      <author>example@mail.com (Muhammad Zeeshan, Umer Zaki, Syed Ahmed Pasha, Zaar Khizar)</author>
      <guid isPermaLink="false">2508.01019v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF</title>
      <link>http://arxiv.org/abs/2508.00967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS  2024 template&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种创新的无人机集群感知系统，通过联邦学习和轻量级模型解决计算限制、低带宽通信和实时场景重建问题&lt;h4&gt;背景&lt;/h4&gt;无人机集群系统面临计算资源有限、通信带宽低以及实时场景重建困难等挑战&lt;h4&gt;目的&lt;/h4&gt;开发一个创新的无人机集群感知系统，解决计算限制、低带宽通信和实时场景重建问题&lt;h4&gt;方法&lt;/h4&gt;使用联邦学习共享扩散模型、采用YOLOv12轻量级语义提取、实施局部NeRF更新、重新设计生成扩散模型用于联合场景重建、添加语义感知压缩协议&lt;h4&gt;主要发现&lt;/h4&gt;该框架能实现高效的多智能体3D/4D场景合成，在保持隐私和可扩展性的同时提升协作场景理解能力&lt;h4&gt;结论&lt;/h4&gt;该提案代表了多智能体AI在自主系统领域的颠覆性进步，具有实际应用潜力&lt;h4&gt;翻译&lt;/h4&gt;该提案引入了一种创新的无人机集群感知系统，旨在解决与计算限制、低带宽通信和实时场景重建相关的问题。该框架通过共享扩散模型的联邦学习、YOLOv12轻量级语义提取和局部NeRF更新，实现了高效的多智能体3D/4D场景合成，同时保持隐私和可扩展性。该框架重新设计了生成扩散模型，用于联合场景重建，并改进了协作场景理解，同时添加了语义感知压缩协议。该方法可以通过模拟和无人机测试平台上的潜在实际部署进行验证，使其成为自主系统多智能体AI的颠覆性进步。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多无人机系统中的'三难困境'：通信瓶颈、计算负担和隐私/可扩展性问题。这个问题在现实中非常重要，因为单个无人机的感知能力受限于物理视角，导致遮挡和视野局限，在复杂场景如城市交通、搜索救援中成为安全瓶颈。现有方法要么丢失重要低级数据，要么通信成本过高，无法满足实时应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有合作感知方法的局限性（后期融合丢失信息、早期融合通信成本高、中期融合缺乏可扩展性）来设计方法。他们借鉴了多个领域的现有工作：NeRF用于3D场景表示、扩散模型用于图像生成、联邦学习保护隐私、多终端编码理论提供压缩基础、语义压缩重新定义压缩目标，以及YOLOv12用于轻量级语义提取。这些技术被创新地整合成一个新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是无人机只交换压缩的语义信息和姿态数据，而非原始数据或特征。使用联邦学习训练共享的生成扩散模型，根据语义提示和姿态生成逼真2D图像，这些图像用于训练或增强局部NeRF模型。整体流程包括：1)请求阶段-目标无人机广播需求；2)数据共享阶段-源无人机发送语义和姿态；3)幻觉阶段-目标无人机生成图像；4)NeRF更新阶段-用生成图像训练局部模型；5)反馈迭代-持续改进模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)生成式扩散与NeRF结合的3D/4D场景合成；2)YOLO增强的语义提取；3)语义感知压缩减少90%以上通信开销；4)在真实无人机平台上的验证。相比之前工作，该方法通信效率更高（&lt;1MB vs &gt;10MB），计算负载更低（源无人机只做轻量级语义提取），隐私保护更好（联邦学习无需共享原始数据），并且整合了多种技术形成一个完整系统。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的资源高效合作感知框架，通过联邦学习和条件扩散模型结合神经辐射场，使无人机群能够在低带宽和计算受限条件下实时构建高保真3D/4D场景，同时保护隐私并确保系统可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proposal introduces an innovative drone swarm perception system that aimsto solve problems related to computational limitations and low-bandwidthcommunication, and real-time scene reconstruction. The framework enablesefficient multi-agent 3D/4D scene synthesis through federated learning ofshared diffusion model and YOLOv12 lightweight semantic extraction and localNeRF updates while maintaining privacy and scalability. The framework redesignsgenerative diffusion models for joint scene reconstruction, and improvescooperative scene understanding, while adding semantic-aware compressionprotocols. The approach can be validated through simulations and potentialreal-world deployment on drone testbeds, positioning it as a disruptiveadvancement in multi-agent AI for autonomous systems.</description>
      <author>example@mail.com (Massoud Pourmandi)</author>
      <guid isPermaLink="false">2508.00967v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning</title>
      <link>http://arxiv.org/abs/2507.23318v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision-Language-Action (VLA)模型在复杂场景理解和动作推理方面显示出巨大潜力，但视觉token长度增加了计算成本。针对自动驾驶场景，本文提出了FastDriveVLA框架，通过MAE风格的像素重建优先考虑前景信息，并设计了对抗性前景-背景重建策略训练ReconPruner剪枝器。该方法在不同剪枝比例下取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;VLA模型在自动驾驶系统中应用广泛，但其视觉token长度导致计算成本高昂。现有视觉token剪枝方法在自动驾驶场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对自动驾驶场景的视觉token剪枝框架，减少VLA模型的计算成本同时保持性能。&lt;h4&gt;方法&lt;/h4&gt;提出FastDriveVLA框架，包含ReconPruner剪枝器，通过MAE风格像素重建和对抗性前景-背景重建策略训练，并引入nuScenes-FG数据集(241K图像-掩码对)。&lt;h4&gt;主要发现&lt;/h4&gt;人类驾驶者关注前景区域，保留前景信息对有效决策至关重要；FastDriveVLA在不同剪枝比例下取得最先进结果。&lt;h4&gt;结论&lt;/h4&gt;FastDriveVLA有效解决了VLA模型在自动驾驶中的计算效率问题，通过优先保留前景信息实现高性能决策。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA)模型在复杂场景理解和动作推理方面显示出巨大潜力，导致它们在端到端自动驾驶系统中被越来越多地采用。然而，VLA模型的视觉token长度大大增加了计算成本。当前视觉语言模型(VLM)中的视觉token剪枝方法依赖于视觉token相似性或视觉-文本注意力，但在自动驾驶场景中表现不佳。鉴于人类驾驶者在驾驶时会专注于相关的前景区域，我们认为保留包含这种前景信息的视觉token对有效决策至关重要。受此启发，我们提出了FastDriveVLA，一种专为自动驾驶设计的基于重建的视觉token剪枝框架。FastDriveVLA包含一个即插即用的视觉token剪枝器ReconPruner，它通过MAE风格的像素重建优先考虑前景信息。设计了一种新的对抗性前景-背景重建策略来训练VLA模型视觉编码器的ReconPruner。一旦训练完成，ReconPruner可以无缝应用于具有相同视觉编码器的不同VLA模型，无需重新训练。为了训练ReconPruner，我们还引入了一个名为nuScenes-FG的大规模数据集，包含241K带有注释前景区域的图像-掩码对。我们的方法在不同剪裁比例下的nuScenes开环规划基准测试中取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have demonstrated significant potentialin complex scene understanding and action reasoning, leading to theirincreasing adoption in end-to-end autonomous driving systems. However, the longvisual tokens of VLA models greatly increase computational costs. Currentvisual token pruning methods in Vision-Language Models (VLM) rely on eithervisual token similarity or visual-text attention, but both have shown poorperformance in autonomous driving scenarios. Given that human driversconcentrate on relevant foreground areas while driving, we assert thatretaining visual tokens containing this foreground information is essential foreffective decision-making. Inspired by this, we propose FastDriveVLA, a novelreconstruction-based vision token pruning framework designed specifically forautonomous driving. FastDriveVLA includes a plug-and-play visual token prunercalled ReconPruner, which prioritizes foreground information through MAE-stylepixel reconstruction. A novel adversarial foreground-background reconstructionstrategy is designed to train ReconPruner for the visual encoder of VLA models.Once trained, ReconPruner can be seamlessly applied to different VLA modelswith the same visual encoder without retraining. To train ReconPruner, we alsointroduce a large-scale dataset called nuScenes-FG, consisting of 241Kimage-mask pairs with annotated foreground regions. Our approach achievesstate-of-the-art results on the nuScenes open-loop planning benchmark acrossdifferent pruning ratios.</description>
      <author>example@mail.com (Jiajun Cao, Qizhe Zhang, Peidong Jia, Xuhui Zhao, Bo Lan, Xiaoan Zhang, Xiaobao Wei, Sixiang Chen, Zhuo Li, Yang Wang, Liyun Li, Xianming Liu, Ming Lu, Shanghang Zhang)</author>
      <guid isPermaLink="false">2507.23318v2</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Variance in Visual Question Answering Benchmarks</title>
      <link>http://arxiv.org/abs/2508.02645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ICCV 2025 Workshop on What's Next in Multimodal  Foundational Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多模态大语言模型在视觉问答任务评估中的性能变异性问题，指出当前评估方法存在局限性，并提出差异感知的评估方法。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已成为视觉问答的强大工具，能够实现视觉和文本模态间的推理和上下文理解。&lt;h4&gt;目的&lt;/h4&gt;批判性审视当前多模态大语言模型在视觉问答基准测试中评估的局限性，分析导致性能变异的因素，并提出改进的评估方法。&lt;h4&gt;方法&lt;/h4&gt;分析14个广泛使用的VQA基准测试中的性能差异，研究训练种子、框架非确定性、模型规模和扩展指令微调对性能变异性的影响，并探索填空式评估作为替代评估策略。&lt;h4&gt;主要发现&lt;/h4&gt;当前视觉问答基准测试的评估实践存在显著局限性，忽视了随机模型输出、训练种子敏感性和超参数配置等因素导致的性能差异。&lt;h4&gt;结论&lt;/h4&gt;应采用差异感知的评估方法来促进多模态大语言模型更强大和可靠的发展，以提高评估的全面性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型已成为视觉问答的强大工具，能够实现视觉和文本模态间的推理和上下文理解。尽管取得了进步，但在VQA基准测试中对MLLMs的评估通常依赖于点估计，忽视了随机模型输出、训练种子敏感性和超参数配置等因素导致的显著性能差异。本文通过分析14个广泛使用的VQA基准测试中的性能差异，批判性地审视这些问题，这些基准测试涵盖了视觉推理、文本理解和常识推理等多样化任务。我们系统地研究了训练种子、框架非确定性、模型规模和扩展指令微调对性能变异性的影响。此外，我们还探索了填空式评估作为替代评估策略，研究了其在减少随机性和提高基准测试可靠性方面的有效性。研究结果突显了当前评估实践的局限性，并倡导采用差异感知的方法来促进MLLMs更强大和可靠的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have emerged as powerful tools forvisual question answering (VQA), enabling reasoning and contextualunderstanding across visual and textual modalities. Despite their advancements,the evaluation of MLLMs on VQA benchmarks often relies on point estimates,overlooking the significant variance in performance caused by factors such asstochastic model outputs, training seed sensitivity, and hyperparameterconfigurations. This paper critically examines these issues by analyzingvariance across 14 widely used VQA benchmarks, covering diverse tasks such asvisual reasoning, text understanding, and commonsense reasoning. Wesystematically study the impact of training seed, framework non-determinism,model scale, and extended instruction finetuning on performance variability.Additionally, we explore Cloze-style evaluation as an alternate assessmentstrategy, studying its effectiveness in reducing stochasticity and improvingreliability across benchmarks. Our findings highlight the limitations ofcurrent evaluation practices and advocate for variance-aware methodologies tofoster more robust and reliable development of MLLMs.</description>
      <author>example@mail.com (Nikitha SR)</author>
      <guid isPermaLink="false">2508.02645v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.02464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SAMPO（Segment Anything Model with Preference Optimization），一种新型框架，用于教会视觉基础模型从稀疏视觉交互中推断高层级分类意图，解决了视觉基础模型在提示分割中存在的意图差距问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型如Segment Anything Model (SAM)在提示分割方面表现出色，但存在意图差距：它们只能分割明确提示的对象，无法泛化到用户隐含期望的语义相关实例。这种限制在具有密集同质对象的领域（例如生物医学细胞核分割）中尤为关键，因为稀疏视觉提示通常会导致不完整的结果，而密集标注则因成本过高而不切实际。&lt;h4&gt;目的&lt;/h4&gt;弥合视觉基础模型中的意图差距，使模型能够从稀疏提示中推断用户的隐含意图，实现更完整和准确的分割，特别是在密集同质对象的场景中。&lt;h4&gt;方法&lt;/h4&gt;SAMPO通过偏好优化（preference optimization）来训练视觉基础模型，使其能够从稀疏视觉交互中推断高层级分类意图。与传统的像素级微调不同，SAMPO优化模型以隐式地捕获目标类特征，这种方法不依赖于语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;SAMPO能够在稀疏提示的情况下实现稳健的多目标分割，并在微调过程中表现出卓越的数据效率。在三个医学分割任务上的验证表明，SAMPO达到了最先进的性能：在像PanNuke-T2这样具有挑战性的任务上，仅使用10%的训练数据进行微调，就显著优于在完整100%数据集上训练的所有现有方法，比最佳基线提高了9个百分点以上。&lt;h4&gt;结论&lt;/h4&gt;该研究为视觉基础模型中的意图感知对齐建立了新范式，消除了对辅助提示生成器或语言模型辅助偏好学习的依赖。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如Segment Anything Model (SAM)在提示分割方面表现出色，但存在意图差距：它们只能分割明确提示的对象，无法泛化到用户隐含期望的语义相关实例。这种限制在具有密集同质对象的领域（例如生物医学细胞核分割）中尤为关键，因为稀疏视觉提示通常会导致不完整的结果，而密集标注则因成本过高而不切实际。为了弥合这一差距，我们引入了SAMPO（Segment Anything Model with Preference Optimization），一种新型框架，它教会视觉基础模型从稀疏视觉交互中推断高层级分类意图。与传统的像素级微调不同，SAMPO优化模型以通过偏好优化隐式地捕获目标类特征。这种方法不依赖于语言模型，使得即使在稀疏提示的情况下也能实现稳健的多目标分割，并在微调过程中表现出卓越的数据效率。在三个医学分割任务上的验证表明，SAMPO达到了最先进的性能：在像PanNuke-T2这样具有挑战性的任务上，我们的方法仅使用10%的训练数据进行微调，就显著优于在完整100%数据集上训练的所有现有方法，比最佳基线提高了9个百分点以上。我们的研究为视觉基础模型中的意图感知对齐建立了一种新范式，消除对辅助提示生成器或语言模型辅助偏好学习的依赖。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models like Segment Anything Model (SAM) excel in promptablesegmentation but suffer from an intent gap: they segment only explicitlyprompted objects, failing to generalize to semantically related instancesimplicitly desired by users. This limitation is critical in domains with densehomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visualprompts typically yield incomplete results, rendering dense annotationsimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO(Segment Anything Model with Preference Optimization), a novel framework thatteaches visual foundation models to infer high-level categorical intent fromsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPOoptimizes models to implicitly capture target-class characteristics throughpreference optimization. This approach, which operates without dependency onlanguage models, enables robust multi-object segmentation even under sparseprompting and demonstrates superior data efficiency during fine-tuning.Validated on three medical segmentation tasks, SAMPO achieves state-of-the-artperformance: on challenging tasks like PanNuke-T2, our method, when fine-tunedwith only 10% of the training data, significantly outperforms all existingmethods trained on the full 100% dataset, achieving an improvement of over 9percentage points compared to the best baseline. Our work establishes a newparadigm for intent-aware alignment in visual foundation models, removingdependencies on auxiliary prompt generators or language-model-assistedpreference learning.</description>
      <author>example@mail.com (Yonghuang Wu, Wenwen Zeng, Xuan Xie, Chengqian Zhao, Guoqing Wu, Jinhua Yu)</author>
      <guid isPermaLink="false">2508.02464v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems</title>
      <link>http://arxiv.org/abs/2508.02344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Traffic-R1，一个具有人类推理能力的基础模型，用于交通信号控制系统。该模型通过在模拟环境中使用专家指导的强化大型语言模型进行自我探索和迭代开发，相比传统方法和现有LLM方法具有显著优势，已在实际部署中显示出积极效果。&lt;h4&gt;背景&lt;/h4&gt;交通信号控制对于缓解交通拥堵和维持城市交通流动性至关重要。随着城市交通需求的增长，开发更智能、更高效的交通信号控制系统变得日益重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个具有人类推理能力的基础模型(Traffic-R1)，用于交通信号控制系统，以解决传统方法和现有基于LLM方法的局限性，提高交通信号控制的效率和适应性。&lt;h4&gt;方法&lt;/h4&gt;通过在模拟交通环境中使用专家指导的强化大型语言模型进行自我探索和迭代开发来构建Traffic-R1模型。这种方法结合了强化学习和大型语言模型的优势，使模型能够学习类似人类的交通控制策略。&lt;h4&gt;主要发现&lt;/h4&gt;Traffic-R1相比传统强化学习和近期基于LLM的方法有三个显著优势：1) 实现零样本泛化，能够直接应用于新的道路网络和分布外事件；2) 30亿参数的轻量级架构，支持在移动级芯片上进行实时推理，便于大规模边缘部署；3) 提供可解释的交通信号控制过程，并通过自我迭代和新的同步通信网络促进多路口通信。&lt;h4&gt;结论&lt;/h4&gt;Traffic-R1在基准测试中建立了新的最先进状态，超越了强大的基线和训练密集型强化学习控制器。在实际应用中，该模型已成功管理超过55,000名驾驶员的交通信号，将平均排队时间缩短5%以上，并将操作员工作量减少一半。模型检查点已公开发布，可供进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;交通信号控制对于缓解交通拥堵和维持城市交通流动性至关重要。在本文中，我们介绍了Traffic-R1，一个具有人类推理能力的基础模型，用于交通信号控制系统。我们的模型通过在模拟交通环境中使用专家指导的强化大型语言模型进行自我探索和迭代开发而成。与传统强化学习和近期基于LLM的方法相比，Traffic-R1提供了三个显著优势。首先，Traffic-R1实现了零样本泛化，通过利用其内部交通控制策略和类似人类的推理能力，无需修改即可应用于新的道路网络和分布外事件。其次，其30亿参数的架构足够轻量，可以在移动级芯片上进行实时推理，支持大规模边缘部署。第三，Traffic-R1通过自我迭代和新的同步通信网络，提供可解释的交通信号控制过程，并促进多路口通信。广泛的基准测试表明，Traffic-R1建立了新的最先进状态，超越了强大的基线和训练密集型强化学习控制器。在实际应用中，该模型现在每天为超过55,000名驾驶员管理信号，将平均排队时间缩短5%以上，并将操作员工作量减少一半。我们的模型检查点可在https://huggingface.co/Season998/Traffic-R1获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic signal control (TSC) is vital for mitigating congestion andsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundationmodel with human-like reasoning for TSC systems. Our model is developed throughself-exploration and iteration of reinforced large language models (LLMs) withexpert guidance in a simulated traffic environment. Compared to traditionalreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offersthree significant advantages. First, Traffic-R1 delivers zero-shotgeneralisation, transferring unchanged to new road networks andout-of-distribution incidents by utilizing its internal traffic controlpolicies and human-like reasoning. Second, its 3B-parameter architecture islightweight enough for real-time inference on mobile-class chips, enablinglarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSCprocess and facilitates multi-intersection communication through itsself-iteration and a new synchronous communication network. Extensivebenchmarks demonstrate that Traffic-R1 sets a new state of the art,outperforming strong baselines and training-intensive RL controllers. Inpractice, the model now manages signals for more than 55,000 drivers daily,shortening average queues by over 5% and halving operator workload. Ourcheckpoint is available at https://huggingface.co/Season998/Traffic-R1.</description>
      <author>example@mail.com (Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang)</author>
      <guid isPermaLink="false">2508.02344v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Qwen-Image Technical Report</title>
      <link>http://arxiv.org/abs/2508.02324v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/QwenLM/Qwen-Image&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了Qwen-Image图像生成基础模型，该模型在复杂文本渲染和精确图像编辑方面取得了显著进展。通过全面的数据管道设计和渐进式训练策略，模型在字母语言和表意语言上都表现出色。同时，通过改进的多任务训练范式和双编码机制，有效提高了图像编辑的一致性和质量。&lt;h4&gt;背景&lt;/h4&gt;图像生成技术在文本渲染和图像编辑方面仍面临挑战，特别是在复杂文本处理和保持编辑一致性方面。现有方法在处理不同语言类型和保持语义一致性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在复杂文本渲染和精确图像编辑方面取得显著进展的图像生成基础模型，提高模型在多种语言上的表现，并增强图像编辑的一致性和质量。&lt;h4&gt;方法&lt;/h4&gt;1. 设计全面的数据管道，包括大规模数据收集、过滤、标注、合成和平衡2. 采用渐进式训练策略，从简单到复杂逐步提升模型能力3. 引入改进的多任务训练范式，整合T2I、TI2I和I2I任务4. 实施双编码机制，分别获取语义和重建表示5. 对齐Qwen2.5-VL和MMDiT之间的潜在表示&lt;h4&gt;主要发现&lt;/h4&gt;1. Qwen-Image在复杂文本渲染方面取得了显著进展2. 模型在英语等字母语言和中文等表意语言上都表现出色3. 改进的多任务训练范式有效提高了图像编辑的一致性4. 双编码机制使编辑模块能够在语义一致性和视觉保真度之间取得平衡5. 模型在多个基准测试中达到了最先进的性能&lt;h4&gt;结论&lt;/h4&gt;Qwen-Image是一个强大的图像生成基础模型，通过创新的数据处理方法、训练策略和技术架构，在复杂文本渲染和精确图像编辑方面取得了突破性进展，为图像生成领域的发展提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Qwen-Image，这是Qwen系列中的一个图像生成基础模型，在复杂文本渲染和精确图像编辑方面取得了显著进展。为解决复杂文本渲染的挑战，我们设计了一个全面的数据管道，包括大规模数据收集、过滤、标注、合成和平衡。此外，我们采用了一种渐进式训练策略，从非文本到文本渲染开始，从简单到复杂的文本输入逐步演变，最终扩展到段落级描述。这种课程学习方法显著增强了模型的原生文本渲染能力。因此，Qwen-Image不仅在英语等字母语言上表现出色，还在中文等更困难的表意语言上取得了显著进展。为提高图像编辑的一致性，我们引入了改进的多任务训练范式，不仅包括传统的文本到图像(T2I)和文本图像到图像(TI2I)任务，还包括图像到图像(I2I)重建，有效对齐了Qwen2.5-VL和MMDiT之间的潜在表示。此外，我们将原始图像分别输入Qwen2.5-VL和VAE编码器，以获取语义和重建表示。这种双编码机制使编辑模块能够在保持语义一致性和维护视觉保真度之间取得平衡。Qwen-Image达到了最先进的性能，展示了在多个基准测试中图像生成和编辑的强大能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Qwen-Image, an image generation foundation model in the Qwenseries that achieves significant advances in complex text rendering and preciseimage editing. To address the challenges of complex text rendering, we design acomprehensive data pipeline that includes large-scale data collection,filtering, annotation, synthesis, and balancing. Moreover, we adopt aprogressive training strategy that starts with non-text-to-text rendering,evolves from simple to complex textual inputs, and gradually scales up toparagraph-level descriptions. This curriculum learning approach substantiallyenhances the model's native text rendering capabilities. As a result,Qwen-Image not only performs exceptionally well in alphabetic languages such asEnglish, but also achieves remarkable progress on more challenging logographiclanguages like Chinese. To enhance image editing consistency, we introduce animproved multi-task training paradigm that incorporates not only traditionaltext-to-image (T2I) and text-image-to-image (TI2I) tasks but alsoimage-to-image (I2I) reconstruction, effectively aligning the latentrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feedthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic andreconstructive representations, respectively. This dual-encoding mechanismenables the editing module to strike a balance between preserving semanticconsistency and maintaining visual fidelity. Qwen-Image achievesstate-of-the-art performance, demonstrating its strong capabilities in bothimage generation and editing across multiple benchmarks.</description>
      <author>example@mail.com (Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu)</author>
      <guid isPermaLink="false">2508.02324v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.02281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, Third International Workshop on Data Engineering  in Medical Imaging (DEMI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了边缘增强预训练对医学图像分割模型性能的影响，并提出了一种元学习策略来优化预训练方法的选择，显著提高了跨模态医学图像分割的总体性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割对疾病诊断和治疗规划至关重要，但开发强大的分割模型需要大量计算资源和大型数据集。虽然预训练和微调基础模型可以提高分割性能，但特定图像预处理步骤如何影响不同医学成像模态的分割性能仍不清楚。边缘作为像素强度的急剧变化，被广泛认为是物体边界的重要线索，但在基础模型的预训练中尚未得到系统性研究。&lt;h4&gt;目的&lt;/h4&gt;研究使用计算高效的边缘核（如kirsch）处理数据进行预训练，能在多大程度上提高基础模型的跨模态分割能力。&lt;h4&gt;方法&lt;/h4&gt;在多个医学成像模态上训练两个版本的基础模型：一个使用原始数据，一个使用边缘增强数据，然后在针对特定医学模态选择的原始数据子集上进行微调。在Dermoscopy、Fundus、Mammography、Microscopy、OCT、US和XRay等医学领域进行系统研究。提出一种元学习策略，使用原始图像的标准差和图像熵来选择在边缘增强数据或原始数据上预训练的模型。&lt;h4&gt;主要发现&lt;/h4&gt;使用边缘聚焦预训练在不同模态上显示出提高和降低的分割性能，表明需要选择性应用这种方法。集成元学习层相比仅在边缘增强数据上预训练的模型，在各种医学成像任务中总体分割性能提高了16.42%；相比仅在原始数据上预训练的模型，总体分割性能提高了19.30%。&lt;h4&gt;结论&lt;/h4&gt;边缘增强预训练可以提高基础模型的跨模态分割能力，但需要根据具体情况选择性应用。提出的元学习策略可以有效指导选择适当的预训练方法，显著提高分割性能。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割对疾病诊断和治疗规划至关重要，然而开发强大的分割模型通常需要大量计算资源和大型数据集。现有研究表明，预训练和微调的基础模型可以提高分割性能。然而，关于特定图像预处理步骤如何影响不同医学成像模态的分割性能的问题仍然存在。特别是，边缘（像素强度的急剧变化）被广泛认为是物体边界的重要线索，但在基础模型的预训练中尚未得到系统性研究。我们通过研究使用计算高效的边缘核（如kirsch）处理数据进行预训练能在多大程度上提高基础模型的跨模态分割能力来填补这一空白。首先在多个医学成像模态上使用原始数据或边缘增强数据训练基础模型的两个版本，然后在针对特定医学模态选择的原始数据子集上进行微调。在使用Dermoscopy、Fundus、Mammography、Microscopy、OCT、US和XRay等医学领域进行系统研究后，我们发现使用边缘聚焦预训练在不同模态上显示出提高和降低的分割性能，表明需要选择性应用这种方法。为指导此类选择性应用，我们提出了一种元学习策略，它使用原始图像的标准差和图像熵来选择在边缘增强数据或原始数据上预训练的模型以获得最佳性能。我们的实验表明，与仅在边缘增强数据上预训练的模型相比，集成这种元学习层在各种医学成像任务中总体分割性能提高了16.42%；与仅在原始数据上预训练的模型相比，提高了19.30%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image segmentation is crucial for disease diagnosis and treatmentplanning, yet developing robust segmentation models often requires substantialcomputational resources and large datasets. Existing research shows thatpre-trained and finetuned foundation models can boost segmentation performance.However, questions remain about how particular image preprocessing steps mayinfluence segmentation performance across different medical imaging modalities.In particular, edges-abrupt transitions in pixel intensity-are widelyacknowledged as vital cues for object boundaries but have not beensystematically examined in the pre-training of foundation models. We addressthis gap by investigating to which extend pre-training with data processedusing computationally efficient edge kernels, such as kirsch, can improvecross-modality segmentation capabilities of a foundation model. Two versions ofa foundation model are first trained on either raw or edge-enhanced data acrossmultiple medical imaging modalities, then finetuned on selected raw subsetstailored to specific medical modalities. After systematic investigation usingthe medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, andXRay, we discover both increased and reduced segmentation performance acrossmodalities using edge-focused pre-training, indicating the need for a selectiveapplication of this approach. To guide such selective applications, we proposea meta-learning strategy. It uses standard deviation and image entropy of theraw image to choose between a model pre-trained on edge-enhanced or on raw datafor optimal performance. Our experiments show that integrating thismeta-learning layer yields an overall segmentation performance improvementacross diverse medical imaging tasks by 16.42% compared to models pre-trainedon edge-enhanced data only and 19.30% compared to models pre-trained on rawdata only.</description>
      <author>example@mail.com (Paul Zaha, Lars Böcking, Simeon Allmendinger, Leopold Müller, Niklas Kühl)</author>
      <guid isPermaLink="false">2508.02281v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2508.02107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的框架，通过语义驱动的LoRA检索和动态聚合，解决了分布式开源LoRA模块在图像生成模型中的有效利用问题，实现了图像生成性能的显著提升。&lt;h4&gt;背景&lt;/h4&gt;尽管FLUX和Stable Diffusion v3等大规模模型在照片级真实感图像生成方面取得了进展，但这些架构的实际部署受到参数微调复杂性的限制。低秩适应（LoRA）虽然能以最小参数开销实现模型定制，但分布式开源LoRA模块的有效利用面临三个关键挑战：稀疏的元数据标注、零样本适应能力的需求，以及多LoRA融合策略的次优融合策略。&lt;h4&gt;目的&lt;/h4&gt;解决分布式开源LoRA模块有效利用面临的三个关键挑战，实现语义驱动的LoRA检索和动态聚合，从而提高图像生成性能，促进基础模型的可扩展和数据高效增强。&lt;h4&gt;方法&lt;/h4&gt;作者引入了一个包含两个关键组件的框架：(1) 基于权重编码的LoRA检索器，在LoRA参数矩阵和文本提示之间建立共享语义空间，消除对原始训练数据的依赖；(2) 细粒度门控融合机制，在网络层和扩散时间步上计算上下文特定的融合权重，以在生成过程中最优地集成多个LoRA模块。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在图像生成性能上取得了显著改进，实现了基础模型的可扩展和数据高效增强。这项工作在社区开发的碎片化LoRA景观与实际部署需求之间建立了关键桥梁。&lt;h4&gt;结论&lt;/h4&gt;通过标准化适配器集成，该方法实现了协作模型演进，解决了LoRA模块在图像生成模型中的有效利用问题。&lt;h4&gt;翻译&lt;/h4&gt;尽管通过FLUX和Stable Diffusion v3等大规模模型在照片级真实感图像生成方面取得了最近进展，但这些架构的实际部署仍然受到其参数微调固有复杂性的限制。虽然低秩适应（LoRA）已经证明在以最小参数开销实现模型定制方面有效，但分布式开源LoRA模块的有效利用面临三个关键挑战：稀疏的元数据标注、零样本适应能力的需求，以及多LoRA融合策略的次优融合策略。为了解决这些限制，我们引入了一个新颖的框架，通过两个关键组件实现语义驱动的LoRA检索和动态聚合：(1) 基于权重编码的LoRA检索器，在LoRA参数矩阵和文本提示之间建立共享语义空间，消除对原始训练数据的依赖；(2) 细粒度门控融合机制，在网络层和扩散时间步上计算上下文特定的融合权重，以在生成过程中最优地集成多个LoRA模块。我们的方法在图像生成性能上取得了显著改进，从而促进基础模型的可扩展和数据高效增强。这项工作在社区开发的碎片化LoRA景观与实际部署需求之间建立了关键桥梁，通过标准化适配器集成实现协作模型演进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in photorealistic image generation throughlarge-scale models like FLUX and Stable Diffusion v3, the practical deploymentof these architectures remains constrained by their inherent intractability toparameter fine-tuning. While low-rank adaptation (LoRA) have demonstratedefficacy in enabling model customization with minimal parameter overhead, theeffective utilization of distributed open-source LoRA modules faces threecritical challenges: sparse metadata annotation, the requirement for zero-shotadaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusionstrategies. To address these limitations, we introduce a novel framework thatenables semantic-driven LoRA retrieval and dynamic aggregation through two keycomponents: (1) weight encoding-base LoRA retriever that establishes a sharedsemantic space between LoRA parameter matrices and text prompts, eliminatingdependence on original training data, and (2) fine-grained gated fusionmechanism that computes context-specific fusion weights across network layersand diffusion timesteps to optimally integrate multiple LoRA modules duringgeneration. Our approach achieves significant improvement in image generationperfermance, thereby facilitating scalable and data-efficient enhancement offoundational models. This work establishes a critical bridge between thefragmented landscape of community-developed LoRAs and practical deploymentrequirements, enabling collaborative model evolution through standardizedadapter integration.</description>
      <author>example@mail.com (Zhiwen Li, Zhongjie Duan, Die Chen, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen)</author>
      <guid isPermaLink="false">2508.02107v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2508.02062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning 2025 (CoRL 2025), 17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为RICL(为上下文学习重新训练)的方法，能够向预训练的视觉-语言-动作(VLA)模型注入上下文学习(ICL)能力，使机器人模型能够通过少量演示学习新任务，无需参数更新。&lt;h4&gt;背景&lt;/h4&gt;多任务'视觉-语言-动作'(VLA)模型作为机器人领域的通用基础模型表现出色，但终端用户缺乏简单的方法来改进这些模型。语言和视觉模型已通过上下文学习(ICL)实现了无需参数微调的任务教学，但通过模仿学习预训练的VLA模型不具备这种自然获得的ICL能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，使预训练的VLA模型能够获得上下文学习(ICL)能力，允许终端用户通过少量演示轻松教授新任务，无需进行参数微调。&lt;h4&gt;方法&lt;/h4&gt;通过特定的微调方法和一个小型机器人演示数据集，对预训练的VLA模型进行重新训练(RICL)，以注入上下文适应能力。RICL能够从用户提供的少量演示(10-20个)中提取最相关部分，并将其注入到VLA的上下文中，利用ICL执行新任务。&lt;h4&gt;主要发现&lt;/h4&gt;将RICL应用于π0-FAST VLA模型后，仅通过每个任务20个演示且无需参数更新，就能显著提高各种新操作任务的性能。当允许对目标任务演示进行参数更新时，RICL微调可以进一步提升性能。&lt;h4&gt;结论&lt;/h4&gt;RICL方法成功地向预训练的VLA模型注入了上下文学习能力，为机器人操作任务提供了一种简单而有效的接口，使用户能够通过少量演示教授新任务，无需复杂的参数调整。&lt;h4&gt;翻译&lt;/h4&gt;多任务'视觉-语言-动作'(VLA)模型最近显示出作为机器人通用基础模型的巨大潜力，能够在全新环境中的新任务上取得非平凡的初始性能。然而，为了使这类模型真正实用，终端用户必须拥有简单的方法来教会它们改进。对于语言和视觉模型，涌现出的上下文学习(ICL)能力已被证明是一种多功能且非常有用的接口，可以通过无需参数微调的方式轻松教授新任务。不幸的是，通过模仿学习目标预训练的VLA模型不会自然获得ICL能力。在本文中，我们证明，通过正确的微调方法和一个小型机器人演示数据集，可以在事后向这样的VLA注入上下文适应性。在为上下文学习重新训练(RICL)后，我们的系统允许终端用户为新的任务提供少量(10-20个)演示。RICL随后将这些演示中最相关的部分提取到VLA上下文中，以利用ICL执行新任务并提高任务性能。我们将RICL应用于向π0-FAST VLA注入ICL，并表明仅通过每个任务20个演示，无需任何参数更新，就能为各种新的操作任务带来大幅的上下文改进。当可以对目标任务演示进行参数更新时，RICL微调可以进一步提高性能。我们随论文一起发布了RICL-π0-FAST的代码和模型权重，首次为新的操作任务提供了一个简单的上下文学习接口。网站：https://ricl-vla.github.io。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-task ``vision-language-action'' (VLA) models have recently demonstratedincreasing promise as generalist foundation models for robotics, achievingnon-trivial performance out of the box on new tasks in new environments.However, for such models to be truly useful, an end user must have easy meansto teach them to improve. For language and vision models, the emergent abilityto perform in-context learning (ICL) has proven to be a versatile and highlyuseful interface to easily teach new tasks with no parameter finetuning.Unfortunately, VLAs pre-trained with imitation learning objectives do notnaturally acquire ICL abilities. In this paper, we demonstrate that, with theright finetuning recipe and a small robot demonstration dataset, it is possibleto inject in-context adaptability post hoc into such a VLA. After retrainingfor in-context learning (RICL), our system permits an end user to provide asmall number (10-20) of demonstrations for a new task. RICL then fetches themost relevant portions of those demonstrations into the VLA context to exploitICL, performing the new task and boosting task performance. We apply RICL toinject ICL into the $\pi_{0}$-FAST VLA, and show that it permits largein-context improvements for a variety of new manipulation tasks with only 20demonstrations per task, without any parameter updates. When parameter updateson the target task demonstrations is possible, RICL finetuning further boostsperformance. We release code and model weights for RICL-$\pi_{0}$-FASTalongside the paper to enable, for the first time, a simple in-context learninginterface for new manipulation tasks. Website: https://ricl-vla.github.io.</description>
      <author>example@mail.com (Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, Insup Lee)</author>
      <guid isPermaLink="false">2508.02062v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</title>
      <link>http://arxiv.org/abs/2508.02002v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GRAD模型，一种结合动作专家混合模块和因果变换器价值估计器的可扩展自动出价基础模型，解决了生成方法面临的分布偏移、探索有限和约束满足等挑战，显著提高了平台收入。&lt;h4&gt;背景&lt;/h4&gt;现代自动出价系统需要平衡整体性能、广告商目标和现实约束，条件生成模型如transformers和diffusers的最新进展为传统基于马尔可夫决策过程的方法提供了替代方案。&lt;h4&gt;目的&lt;/h4&gt;解决生成方法在自动出价系统中面临的离线和在线环境分布偏移、动作空间探索有限以及满足CPM和ROI等约束的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出GRAD（基于生成奖励的自动出价与专家混合模型），结合动作专家混合模块用于多样化出价动作探索，以及因果变换器价值估计器用于约束感知优化。&lt;h4&gt;主要发现&lt;/h4&gt;大量离线和在线实验证明GRAD显著提高了平台收入；在美团的实施导致GMV增加2.18%，ROI增加10.68%。&lt;h4&gt;结论&lt;/h4&gt;GRAD能有效应对现代广告商不断变化和多样化的需求，是解决自动出价系统复杂挑战的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现代自动出价系统需要在整体性能、多样化的广告商目标和现实约束之间取得平衡，反映了行业动态发展的需求。条件生成模型（如transformers和diffusers）的最新进展使得能够根据广告商偏好直接生成轨迹，这为传统基于马尔可夫决策过程的方法提供了有希望的替代方案。然而，这些生成方法面临重大挑战，如离线和在线环境之间的分布偏移、动作空间的探索有限，以及满足边际千次展示成本(CPM)和投资回报率(ROI)等约束的必要性。为了应对这些挑战，我们提出了GRAD（基于生成奖励的自动出价与专家混合模型），这是一种可扩展的基础模型，它结合了用于多样化出价动作探索的'动作专家混合'模块和用于约束感知优化的'因果变换器价值估计器'。大量的离线和在线实验证明，GRAD显著提高了平台收入，突显了其在应对现代广告商不断变化和多样化需求方面的有效性。此外，GRAD已在美团（全球最大的在线食品配送平台之一）的多个营销场景中实施，导致商品交易总额(GMV)增加2.18%，投资回报率(ROI)增加10.68%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern auto-bidding systems are required to balance overall performance withdiverse advertiser goals and real-world constraints, reflecting the dynamic andevolving needs of the industry. Recent advances in conditional generativemodels, such as transformers and diffusers, have enabled direct trajectorygeneration tailored to advertiser preferences, offering a promising alternativeto traditional Markov Decision Process-based methods. However, these generativemethods face significant challenges, such as the distribution shift betweenoffline and online environments, limited exploration of the action space, andthe necessity to meet constraints like marginal Cost-per-Mille (CPM) and Returnon Investment (ROI). To tackle these challenges, we propose GRAD (GenerativeReward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation modelfor auto-bidding that combines an Action-Mixture-of-Experts module for diversebidding action exploration with the Value Estimator of Causal Transformer forconstraint-aware optimization. Extensive offline and online experimentsdemonstrate that GRAD significantly enhances platform revenue, highlighting itseffectiveness in addressing the evolving and diverse requirements of modernadvertisers. Furthermore, GRAD has been implemented in multiple marketingscenarios at Meituan, one of the world's largest online food deliveryplatforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and10.68% increase in ROI.</description>
      <author>example@mail.com (Yu Lei, Jiayang Zhao, Yilei Zhao, Zhaoqi Zhang, Linyou Cai, Qianlong Xie, Xingxing Wang)</author>
      <guid isPermaLink="false">2508.02002v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models</title>
      <link>http://arxiv.org/abs/2508.01731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpectralX是一种创新的参数高效微调框架，通过两阶段训练方法使现有遥感基础模型能够适应处理各种光谱模态，无需大量光谱预训练，从而提高领域泛化性能并能够解释来自新区域或季节的光谱图像。&lt;h4&gt;背景&lt;/h4&gt;遥感基础模型(RSFMs)最近取得了显著突破，但许多模型使用大量光学图像进行预训练，而多光谱/高光谱数据缺乏相应的基础模型。&lt;h4&gt;目的&lt;/h4&gt;探索现有RSFMs是否可以有效适应处理各种光谱模态而无需大量光谱预训练，以利用光谱图像在地球观测中的优势。&lt;h4&gt;方法&lt;/h4&gt;提出SpectralX框架，使用现有RSFMs作为骨干网络，采用两阶段训练方法：第一阶段使用掩码重建任务和专门的超标记器(HyperT)提取属性令牌，开发面向属性的适配器混合(AoMoA)；第二阶段以语义分割为下游任务，插入属性细化适配器(Are-adapter)，通过迭代查询低层语义特征与高层表示，使模型关注任务有益属性。&lt;h4&gt;主要发现&lt;/h4&gt;SpectralX能够显著提高领域泛化性能，并能够解释来自新区域或季节的光谱图像。&lt;h4&gt;结论&lt;/h4&gt;通过两阶段适应过程，SpectralX使现有遥感基础模型能够有效处理各种光谱模态，无需大量光谱预训练。&lt;h4&gt;翻译&lt;/h4&gt;遥感基础模型(RSFMs)的最新进展在该领域取得了重大突破。虽然许多RSFMs使用大量光学图像进行了预训练，但更多多光谱/高光谱数据仍缺乏相应的基础模型。为了利用光谱图像在地球观测中的优势，我们探索了现有RSFMs是否可以有效地适应处理各种光谱模态，而无需大量光谱预训练。为应对这一挑战，我们提出了SpectralX，一种创新的参数高效微调框架，它将现有RSFMs作为骨干网络，同时引入两阶段训练方法来处理各种光谱输入，从而显著提高了领域泛化性能。在第一阶段，我们采用掩码重建任务，并设计了一个专门的超标记器(HyperT)来从空间和光谱维度提取属性令牌。同时，我们开发了一个面向属性的适配器混合(AoMoA)，在逐层微调的同时动态聚合多属性专家知识。在第二阶段，以语义分割作为下游任务，我们在第一阶段框架中插入了一个属性细化适配器(Are-adapter)。通过用高层表示迭代查询低层语义特征，模型学会关注任务有益的属性，从而实现对RSFMs的定制调整。经过这两阶段适应过程，SpectralX能够解释来自新区域或季节的光谱图像。代码将在网站https://github.com/YuxiangZhang-BIT上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Remote Sensing Foundation Models (RSFMs) have led tosignificant breakthroughs in the field. While many RSFMs have been pretrainedwith massive optical imagery, more multispectral/hyperspectral data remain lackof the corresponding foundation models. To leverage the advantages of spectralimagery in earth observation, we explore whether existing RSFMs can beeffectively adapted to process diverse spectral modalities without requiringextensive spectral pretraining. In response to this challenge, we proposedSpectralX, an innovative parameter-efficient fine-tuning framework that adaptexisting RSFMs as backbone while introducing a two-stage training approach tohandle various spectral inputs, thereby significantly improving domaingeneralization performance. In the first stage, we employ amasked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) toextract attribute tokens from both spatial and spectral dimensions.Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA)that dynamically aggregates multi-attribute expert knowledge while performinglayer-wise fine-tuning. With semantic segmentation as downstream task in thesecond stage, we insert an Attribute-refined Adapter (Are-adapter) into thefirst stage framework. By iteratively querying low-level semantic features withhigh-level representations, the model learns to focus on task-beneficialattributes, enabling customized adjustment of RSFMs. Following this two-phaseadaptation process, SpectralX is capable of interpreting spectral imagery fromnew regions or seasons. The codes will be available from the website:https://github.com/YuxiangZhang-BIT.</description>
      <author>example@mail.com (Yuxiang Zhang, Wei Li, Mengmeng Zhang, Jiawei Han, Ran Tao, Shunlin Liang)</author>
      <guid isPermaLink="false">2508.01731v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>OccamVTS: Distilling Vision Models to 1% Parameters for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2508.01727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为OccamVTS的知识蒸馏框架，能够从大型视觉模型中仅提取1%的必要参数用于时间序列预测，在保持或提高预测精度的同时大幅减少了模型复杂度。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测是多种应用的基础，近期研究利用大型视觉模型(LVMs)通过视觉表示来捕获时间模式。&lt;h4&gt;目的&lt;/h4&gt;解决大型视觉模型应用于时间序列预测时参数冗余的问题，提取关键预测信息并构建轻量级网络。&lt;h4&gt;方法&lt;/h4&gt;提出OccamVTS知识蒸馏框架，使用预训练LVMs作为教师模型，采用金字塔式特征对齐结合相关性和特征蒸馏，转移有益模式同时过滤语义噪声。&lt;h4&gt;主要发现&lt;/h4&gt;时间序列与低级纹理特征对齐而非高级语义，99%的LVMs参数对时间序列任务是不必要的；通过减少参数可以消除对无关视觉特征的过拟合，提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;OccamVTS在多个基准数据集上实现了最先进性能，仅使用原始参数的1%，尤其在少样本和零样本场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测是多种应用的基础，近期方法利用大型视觉模型(LVMs)通过视觉表示来捕获时间模式。研究表明，虽然视觉模型提高了预测性能，但99%的参数对时间序列任务是不必要的。通过跨模态分析，我们发现时间序列与低级纹理特征对齐而非高级语义，这可能损害预测准确性。我们提出了OccamVTS，一个知识蒸馏框架，只从LVMs中提取必要的1%预测信息到轻量级网络中。使用预训练LVMs作为特权教师，OccamVTS采用金字塔式特征对齐结合相关性和特征蒸馏，转移有益模式同时过滤语义噪声。反直觉的是，这种激进的参数减少通过消除对无关视觉特征的过拟合并保留基本时间模式提高了准确性。在多个基准数据集上的广泛实验表明，OccamVTS仅使用原始参数的1%就持续实现了最先进的性能，尤其在少样本和零样本场景中表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting is fundamental to diverse applications, with recentapproaches leverage large vision models (LVMs) to capture temporal patternsthrough visual representations. We reveal that while vision models enhanceforecasting performance, 99% of their parameters are unnecessary for timeseries tasks. Through cross-modal analysis, we find that time series align withlow-level textural features but not high-level semantics, which can impairforecasting accuracy. We propose OccamVTS, a knowledge distillation frameworkthat extracts only the essential 1% of predictive information from LVMs intolightweight networks. Using pre-trained LVMs as privileged teachers, OccamVTSemploys pyramid-style feature alignment combined with correlation and featuredistillation to transfer beneficial patterns while filtering out semanticnoise. Counterintuitively, this aggressive parameter reduction improvesaccuracy by eliminating overfitting to irrelevant visual features whilepreserving essential temporal patterns. Extensive experiments across multiplebenchmark datasets demonstrate that OccamVTS consistently achievesstate-of-the-art performance with only 1% of the original parameters,particularly excelling in few-shot and zero-shot scenarios.</description>
      <author>example@mail.com (Sisuo Lyu, Siru Zhong, Weilin Ruan, Qingxiang Liu, Qingsong Wen, Hui Xiong, Yuxuan Liang)</author>
      <guid isPermaLink="false">2508.01727v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Zero-Shot Terrain Traversability Estimation: Challenges and Opportunities</title>
      <link>http://arxiv.org/abs/2508.01715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and presented at the 1st German Robotics Conference (GRC);  March 13-15, 2025, Nuremberg, Germany  https://ras.papercept.net/conferences/conferences/GRC25/program/GRC25_ContentListWeb_3.html#sada_48&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用视觉语言模型进行地形可通行性估计的可能性，特别是在非结构化环境中。研究引入了一个人工标注的水域可通行性数据集，并提出了一种整合视觉语言模型的简单管道进行零样本估计。实验结果表明当前基础模型在实际应用中尚不成熟，但为未来研究提供了方向。&lt;h4&gt;背景&lt;/h4&gt;地形可通行性估计对自主机器人在非结构化环境中至关重要，视觉线索和推理在其中起关键作用。虽然视觉语言模型具有零样本估计的潜力，但这一问题本质上是不适定的。&lt;h4&gt;目的&lt;/h4&gt;探索视觉语言模型在零样本地形可通行性估计中的应用潜力，并评估当前基础模型在这一任务上的适用性。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一个包含人类对水域可通行性评分的人工标注数据集，并提出了一个整合视觉语言模型的简单管道进行零样本可通行性估计。通过实验评估了该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;尽管地形可通行性估计是主观的，但人类评分者之间仍表现出一定的一致性。实验结果喜忧参半，表明当前基础模型在实际部署中尚未成熟。&lt;h4&gt;结论&lt;/h4&gt;当前基础模型尚不适合实际部署，但研究结果为可通行性估计领域的进一步研究提供了有价值的见解和方向。&lt;h4&gt;翻译&lt;/h4&gt;地形可通行性估计对自主机器人至关重要，特别是在非结构化环境中，视觉线索和推理起着关键作用。虽然视觉语言模型为零样本估计提供了可能性，但该问题本质上是不适定的。为了探索这一点，我们引入了一个包含人工标注的水域可通行性评分的小型数据集，揭示尽管估计是主观的，但人工评分者仍表现出一定的一致性。此外，我们提出了一种整合视觉语言模型进行零样本可通行性估计的简单管道。我们的实验结果喜忧参半，表明当前基础模型尚不适合实际部署，但为进一步研究提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Terrain traversability estimation is crucial for autonomous robots,especially in unstructured environments where visual cues and reasoning play akey role. While vision-language models (VLMs) offer potential for zero-shotestimation, the problem remains inherently ill-posed. To explore this, weintroduce a small dataset of human-annotated water traversability ratings,revealing that while estimations are subjective, human raters still show someconsensus. Additionally, we propose a simple pipeline that integrates VLMs forzero-shot traversability estimation. Our experiments reveal mixed results,suggesting that current foundation models are not yet suitable for practicaldeployment but provide valuable insights for further research.</description>
      <author>example@mail.com (Ida Germann, Mark O. Mints, Peer Neubert)</author>
      <guid isPermaLink="false">2508.01715v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe</title>
      <link>http://arxiv.org/abs/2508.01691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Voxlect是一个用于建模世界范围内方言和区域语言的语音基础模型的新型基准测试，涵盖了英语、阿拉伯语、汉语、藏语、印度语言等多种语言的方言评估。&lt;h4&gt;背景&lt;/h4&gt;缺乏针对全球范围内方言和区域语言变体的系统性基准测试，现有语音模型在处理方言差异方面表现不足。&lt;h4&gt;目的&lt;/h4&gt;创建一个全面的基准测试来评估语音基础模型在识别和分类不同方言和区域语言变体方面的性能，并探索其下游应用。&lt;h4&gt;方法&lt;/h4&gt;使用来自30个公开可用的语音语料库的超过200万个训练语句，评估多种语音基础模型在方言分类任务上的表现，测试模型在嘈杂环境下的鲁棒性，并进行错误分析。&lt;h4&gt;主要发现&lt;/h4&gt;语音基础模型在不同方言上的分类性能存在差异，模型在嘈杂条件下的鲁棒性有限，错误分析显示建模结果与地理连续性一致。&lt;h4&gt;结论&lt;/h4&gt;Voxlect为方言和区域语言建模提供了重要基准，可用于增强现有语音识别数据集、分析ASR性能在不同方言中的表现，以及评估语音生成系统。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Voxlect，这是一个用于使用语音基础模型建模全球范围内方言和区域语言的新型基准测试。具体来说，我们报告了在英语、阿拉伯语、普通话和粤语、藏语、印度语言、泰语、西班牙语、法语、德语、巴西葡萄牙语和意大利语的方言和区域语言变体上的全面基准评估。我们的研究使用了来自30个公开可用的语音语料库的超过200万个训练语句，这些语料库都提供了方言信息。我们评估了几种广泛使用的语音基础模型在分类语音方言方面的性能。我们评估了方言模型在嘈杂条件下的鲁棒性，并进行了错误分析，突出了与地理连续性一致的结果。除了基准测试方言分类外，我们还展示了Voxlect启用的几种下游应用。具体来说，我们展示了Voxlect可以应用于用方言信息增强现有的语音识别数据集，从而能够对ASR性能在不同方言变化中的表现进行更详细的分析。Voxlect也被用作评估语音生成系统性能的工具。Voxlect以RAIL系列的许可证公开发布，地址为：https://github.com/tiantiaf0627/voxlect。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Voxlect, a novel benchmark for modeling dialects and regionallanguages worldwide using speech foundation models. Specifically, we reportcomprehensive benchmark evaluations on dialects and regional language varietiesin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over2 million training utterances from 30 publicly available speech corpora thatare provided with dialectal information. We evaluate the performance of severalwidely used speech foundation models in classifying speech dialects. We assessthe robustness of the dialectal models under noisy conditions and present anerror analysis that highlights modeling results aligned with geographiccontinuity. In addition to benchmarking dialect classification, we demonstrateseveral downstream applications enabled by Voxlect. Specifically, we show thatVoxlect can be applied to augment existing speech recognition datasets withdialect information, enabling a more detailed analysis of ASR performanceacross dialectal variations. Voxlect is also used as a tool to evaluate theperformance of speech generation systems. Voxlect is publicly available withthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.</description>
      <author>example@mail.com (Tiantian Feng, Kevin Huang, Anfeng Xu, Xuan Shi, Thanathai Lertpetchpun, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan)</author>
      <guid isPermaLink="false">2508.01691v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Innovative tokenisation of structured data for LLM training</title>
      <link>http://arxiv.org/abs/2508.01685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合标记化方法，将表格数据转换为适合大语言模型训练的统一序列格式，应用于网络入侵检测系统基础模型的训练。&lt;h4&gt;背景&lt;/h4&gt;机器学习中数据表示是一个基本挑战，特别是当将Transformers和大语言模型等基于序列的架构应用于结构化表格数据时。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的混合标记化方法，将表格数据转换为适合LLM训练的统一序列格式，解决现有方法无法统一编码特征混合或保留表格结构的问题。&lt;h4&gt;方法&lt;/h4&gt;结合预定义的固定标记表示结构元素和低基数分类特征，使用字节对编码(BPE)学习子词词汇表来处理高基数和连续值。&lt;h4&gt;主要发现&lt;/h4&gt;该方法高效处理了超过3100万网络流量，用时不到5小时，实现了6.18:1的显著数据压缩比，生成了超过10亿个标记的计算可管理语料库。&lt;h4&gt;结论&lt;/h4&gt;为在结构化数据上训练基础模型建立了可行且可推广的途径。&lt;h4&gt;翻译&lt;/h4&gt;数据表示在机器学习中仍然是一个基本挑战，特别是当将Transformers和大语言模型等基于序列的架构应用于结构化表格数据时。现有方法通常无法统一编码数值和分类特征的混合，或者保留表格的固有结构。本文引入了一种新的混合标记化方法，旨在将表格数据转换为适合LLM训练的统一序列格式。我们的方法结合了预定义的固定标记来表示结构元素和低基数分类特征，并使用字节对编码(BPE)学习子词词汇表来处理高基数和连续值。我们通过将此技术应用于大规模NetFlow数据集(CIDDS-001)，为网络入侵检测系统基础模型准备语料库，证明了该技术的有效性。评估显示，我们的方法效率很高，在不到五小时内处理了超过3100万网络流量，并实现了6.18:1的显著数据压缩比。这一过程生成了超过10亿个标记的计算可管理语料库，为在结构化数据上训练基础模型建立了可行且可推广的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data representation remains a fundamental challenge in machine learning,particularly when adapting sequence-based architectures like Transformers andLarge Language Models (LLMs) for structured tabular data. Existing methodsoften fail to cohesively encode the mix of numerical and categorical featuresor preserve the inherent structure of tables. This paper introduces a novel,hybrid tokenisation methodology designed to convert tabular data into aunified, sequential format suitable for LLM training. Our approach combinespredefined fixed tokens to represent structural elements and low-cardinalitycategorical features, with a learned subword vocabulary using Byte-PairEncoding (BPE) for high-cardinality and continuous values. We demonstrate theefficacy of this technique by applying it to a large-scale NetFlow dataset(CIDDS-001), preparing a corpus for a Network Intrusion Detection System (NIDS)foundation model. The evaluation shows that our method is highly efficient,processing over 31 million network flows in under five hours and achieving asignificant data compression ratio of 6.18:1. This process resulted in acomputationally manageable corpus of over one billion tokens, establishing aviable and generalisable pathway for training foundation models on structureddata.</description>
      <author>example@mail.com (Kayvan Karim, Hani Ragab Hassen. Hadj Batatia)</author>
      <guid isPermaLink="false">2508.01685v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.01667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Rein++是一种高效的基于视觉基础模型(VFM)的语义分割框架，能够从有限数据中实现优越的泛化，并有效适应多样化的无标记场景。该框架结合了领域泛化解决方案Rein-G和领域适应解决方案Rein-A，通过参数高效的方法微调不到主干网络1%的参数，实现鲁棒泛化，并通过无监督领域适应和语义转移模块减轻领域偏移。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFMs)已在各种计算机视觉任务中取得显著成功，但在语义分割应用中面临两个主要挑战：一是分割数据集通常远小于VFM预训练使用的数据集，存在数据规模差异；二是现实世界分割场景多样，但在预训练中常常代表性不足，存在领域分布偏移问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的VFM分割框架，克服数据规模差异和领域分布偏移的挑战，实现从有限数据中学习可泛化模型，并能够有效适应多样化的无标记目标场景。&lt;h4&gt;方法&lt;/h4&gt;Rein++框架包含两个主要组件：1) Rein-G引入一组可训练的、感知实例的令牌，有效优化VFM特征用于分割任务，采用参数高效方法微调不到1%的主干参数；2) Rein-A在Rein-G基础上，在实例和logit层面执行无监督领域适应，并整合语义转移模块，利用Segment Anything模型的类无关能力增强目标域边界细节。该框架首先在源域学习可泛化模型，然后无需目标域标签即可适应多样化目标域。&lt;h4&gt;主要发现&lt;/h4&gt;综合实验表明，Rein++以高效的训练显著优于最先进方法，证明了其作为VFM的高效、可泛化和适应性强分割解决方案的有效性，即使对于具有数十亿参数的大模型也同样适用。&lt;h4&gt;结论&lt;/h4&gt;Rein++成功解决了VFMs在语义分割应用中的关键挑战，提供了一个实用的框架，使VFMs能够有效处理数据规模有限和领域分布多样的实际场景，为计算机视觉领域特别是语义分割任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)已在各种计算机视觉任务中取得显著成功。然而，它们在语义分割中的应用受到两个重大挑战的阻碍：(1)数据规模差异，因为分割数据集通常远小于用于VFM预训练的数据集，以及(2)领域分布偏移，现实世界的分割场景多样，但在预训练中常常代表性不足。为了克服这些局限性，我们提出了Rein++，一种高效的基于VFM的分割框架，展示了从有限数据中优越的泛化能力，并能够有效适应多样化的无标记场景。具体来说，Rein++包含一个领域泛化解决方案Rein-G和一个领域适应解决方案Rein-A。Rein-G引入一组可训练的、感知实例的令牌，有效优化VFM特征用于分割任务。这种参数高效的方法微调了不到主干网络1%的参数，实现鲁棒泛化。基于Rein-G，Rein-A在实例和logit层面执行无监督领域适应，以减轻领域偏移。此外，它整合了语义转移模块，利用分割任何模型的类无关能力增强目标域的边界细节。集成的Rein++流程首先在源域学习可泛化模型，然后无需任何目标标签将其适应到多样化的目标域。综合实验证明，Rein++以高效的训练显著优于最先进方法，强调了其作为VFM的高效、可泛化和适应性强分割解决方案的作用，即使对于具有数十亿参数的大模型也是如此。代码可在https://github.com/wloves/Rein获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Foundation Models(VFMs) have achieved remarkable success in variouscomputer vision tasks. However, their application to semantic segmentation ishindered by two significant challenges: (1) the disparity in data scale, assegmentation datasets are typically much smaller than those used for VFMpre-training, and (2) domain distribution shifts, where real-world segmentationscenarios are diverse and often underrepresented during pre-training. Toovercome these limitations, we present Rein++, an efficient VFM-basedsegmentation framework that demonstrates superior generalization from limiteddata and enables effective adaptation to diverse unlabeled scenarios.Specifically, Rein++ comprises a domain generalization solution Rein-G and adomain adaptation solution Rein-A. Rein-G introduces a set of trainable,instance-aware tokens that effectively refine the VFM's features for thesegmentation task. This parameter-efficient approach fine-tunes less than 1% ofthe backbone's parameters, enabling robust generalization. Building on theRein-G, Rein-A performs unsupervised domain adaptation at both the instance andlogit levels to mitigate domain shifts. In addition, it incorporates a semantictransfer module that leverages the class-agnostic capabilities of the segmentanything model to enhance boundary details in the target domain. The integratedRein++ pipeline first learns a generalizable model on a source domain (e.g.,daytime scenes) and subsequently adapts it to diverse target domains (e.g.,nighttime scenes) without any target labels. Comprehensive experimentsdemonstrate that Rein++ significantly outperforms state-of-the-art methods withefficient training, underscoring its roles an efficient, generalizable, andadaptive segmentation solution for VFMs, even for large models with billions ofparameters. The code is available at https://github.com/wloves/Rein.</description>
      <author>example@mail.com (Zhixiang Wei, Xiaoxiao Ma, Ruishen Yan, Tao Tu, Huaian Chen, Jinjin Zheng, Yi Jin, Enhong Chen)</author>
      <guid isPermaLink="false">2508.01667v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>TCDiff: Triplex Cascaded Diffusion for High-fidelity Multimodal EHRs Generation with Incomplete Clinical Data</title>
      <link>http://arxiv.org/abs/2508.01615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TCDiff（三重级联扩散网络），一种新型电子健康记录生成框架，解决了大规模高质量EHR数据稀缺问题，特别是在中医领域。&lt;h4&gt;背景&lt;/h4&gt;大规模高质量电子健康记录(EHRs)的稀缺性是生物医学研究的主要瓶颈，随着大型基础模型变得越来越需要大量数据，这一问题更加突出。从现有数据集中合成去标识化和高保真数据是一个有前景的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理异构多模态EHR数据、捕捉复杂依赖关系并稳健处理数据不完整性的生成框架，特别针对中医领域的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出TCDiff框架，级联三个扩散网络（参考模态扩散、跨模态桥接、目标模态扩散）来学习真实EHR数据特征。同时构建了TCM-SZ1多模态EHR数据集用于基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种缺失率下，TCDiff在数据保真度方面平均比最先进的基线高出10%，同时保持了有竞争力的隐私保证。&lt;h4&gt;结论&lt;/h4&gt;TCDiff在现实医疗场景中展现出有效性、鲁棒性和通用性，为解决EHR数据稀缺问题提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;大规模高质量电子健康记录(EHRs)的稀缺性仍然是生物医学研究的主要瓶颈，特别是随着大型基础模型变得越来越需要大量数据时。从现有数据集中合成大量去标识化和高保真数据已成为一个有前景的解决方案。然而，现有方法存在一系列局限性：它们难以建模异构多模态EHR数据的内在特性（如连续、离散和文本模态），难以捕捉它们之间的复杂依赖关系，以及难以稳健地处理普遍存在的数据不完整性。这些挑战在中医(TCM)领域尤为突出。为此，我们提出了TCDiff（三重级联扩散网络），一种新颖的EHR生成框架，通过级联三个扩散网络来学习真实世界EHR数据的特征，形成多阶段生成过程：参考模态扩散、跨模态桥接和目标模态扩散。此外，为了验证我们提出的框架，除了两个公共数据集外，我们还构建并引入了TCM-SZ1，这是一个用于基准测试的新型多模态EHR数据集。实验结果表明，在各种缺失率下，TCDiff在数据保真度方面平均比最先进的基线高出10%，同时保持了有竞争力的隐私保证。这突显了我们的方法在现实医疗场景中的有效性、鲁棒性和通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of large-scale and high-quality electronic health records (EHRs)remains a major bottleneck in biomedical research, especially as largefoundation models become increasingly data-hungry. Synthesizing substantialvolumes of de-identified and high-fidelity data from existing datasets hasemerged as a promising solution. However, existing methods suffer from a seriesof limitations: they struggle to model the intrinsic properties ofheterogeneous multimodal EHR data (e.g., continuous, discrete, and textualmodalities), capture the complex dependencies among them, and robustly handlepervasive data incompleteness. These challenges are particularly acute inTraditional Chinese Medicine (TCM). To this end, we propose TCDiff (TriplexCascaded Diffusion Network), a novel EHR generation framework that cascadesthree diffusion networks to learn the features of real-world EHR data,formatting a multi-stage generative process: Reference Modalities Diffusion,Cross-Modal Bridging, and Target Modality Diffusion. Furthermore, to validateour proposed framework, besides two public datasets, we also construct andintroduce TCM-SZ1, a novel multimodal EHR dataset for benchmarking.Experimental results show that TCDiff consistently outperforms state-of-the-artbaselines by an average of 10% in data fidelity under various missing rate,while maintaining competitive privacy guarantees. This highlights theeffectiveness, robustness, and generalizability of our approach in real-worldhealthcare scenarios.</description>
      <author>example@mail.com (Yandong Yan, Chenxi Li, Yu Huang, Dexuan Xu, Jiaqi Zhu, Zhongyan Chai, Huamin Zhang)</author>
      <guid isPermaLink="false">2508.01615v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning</title>
      <link>http://arxiv.org/abs/2508.01603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Image-Adaptive Prompt Learning (IAPL)的新框架，通过两个自适应模块增强AI生成图像检测模型对未知生成器的泛化能力，在两个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;AI生成图像检测的主要挑战是识别来自未知生成器的假图像。现有方法通常通过部分参数微调预训练基础模型，但这些在有限范围生成器上训练的参数难以泛化到未知来源。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够灵活处理多样化测试图像的新框架，解决现有方法在未知生成器上泛化能力差的问题。&lt;h4&gt;方法&lt;/h4&gt;IAPL框架包含两个自适应模块：1) 条件信息学习器，使用基于CNN的特征提取器学习伪造特定和图像特定条件，通过门控机制传播到可学习标记；2) 置信度驱动的自适应预测，基于单个测试样本优化最浅层可学习标记，选择最高预测置信度的裁剪视图进行最终检测。这些模块使提示能根据输入图像自动调整。&lt;h4&gt;主要发现&lt;/h4&gt;在UniversalFakeDetect和GenImage两个广泛使用的数据集上，IAPL分别实现了95.61%和96.7%的平均准确率，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;IAPL通过自适应提示学习显著增强了模型对各种伪造图像的适应能力，解决了现有AI生成图像检测方法在未知生成器上泛化能力不足的问题。&lt;h4&gt;翻译&lt;/h4&gt;AI生成图像检测的一个主要挑战是识别来自未知生成器的假图像。现有的最先进方法通常通过部分参数微调来定制预训练的基础模型。然而，这些在有限范围生成器上训练的参数可能无法泛化到未知来源。鉴于此，我们提出了一种名为Image-Adaptive Prompt Learning (IAPL)的新颖框架，增强了处理多样化测试图像的灵活性。它包含两个自适应模块，即条件信息学习器和置信度驱动的自适应预测。前者使用基于CNN的特征提取器学习伪造特定和图像特定的条件，这些条件通过门控机制传播到可学习标记。后者基于单个测试样本优化最浅层的可学习标记，并选择具有最高预测置信度的裁剪视图进行最终检测。这两个模块使输入基础模型的提示能够根据输入图像自动调整，而不是在训练后固定，从而增强模型对各种伪造图像的适应能力。大量实验表明，IAPL实现了最先进的性能，在两个广泛使用的UniversalFakeDetect和GenImage数据集上分别达到95.61%和96.7%的平均准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major struggle for AI-generated image detection is identifying fake imagesfrom unseen generators. Existing cutting-edge methods typically customizepre-trained foundation models to this task via partial-parameter fine-tuning.However, these parameters trained on a narrow range of generators may fail togeneralize to unknown sources. In light of this, we propose a novel frameworknamed Image-Adaptive Prompt Learning (IAPL), which enhances flexibility inprocessing diverse testing images. It consists of two adaptive modules, i.e.,the Conditional Information Learner and the Confidence-Driven AdaptivePrediction. The former employs CNN-based feature extractors to learnforgery-specific and image-specific conditions, which are then propagated tolearnable tokens via a gated mechanism. The latter optimizes the shallowestlearnable tokens based on a single test sample and selects the cropped viewwith the highest prediction confidence for final detection. These two modulesenable the prompts fed into the foundation model to be automatically adjustedbased on the input image, rather than being fixed after training, therebyenhancing the model's adaptability to various forged images. Extensiveexperiments show that IAPL achieves state-of-the-art performance, with 95.61%and 96.7% mean accuracy on two widely used UniversalFakeDetect and GenImagedatasets, respectively.</description>
      <author>example@mail.com (Yiheng Li, Zichang Tan, Zhen Lei, Xu Zhou, Yang Yang)</author>
      <guid isPermaLink="false">2508.01603v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.01582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次引入集合枢轴学习(SPL)概念，这是一种基于视觉基础模型(VFMs)重新定义领域泛化(DG)的范式转变。作者提出了一种新的领域迁移任务定义，并开发了动态提示微调方法，结合动态类感知提示器和提示引导的特征聚焦器，以提高VFMs在特定场景中的性能。&lt;h4&gt;背景&lt;/h4&gt;传统领域泛化(DG)假设在训练过程中目标领域不可访问，但随着视觉基础模型(VFMs)的出现，这些模型在大量多样化数据上训练，使得这一假设变得不明确和过时。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一挑战，作者提出集合枢轴学习(SPL)，这是一种基于VFMs的新领域迁移任务定义，更适合当前研究和应用需求。&lt;h4&gt;方法&lt;/h4&gt;SPL具有两个关键属性：1)动态适应，从静态领域对齐转向灵活的、任务驱动的特征优化，使模型能够随着下游场景而演变；2)VFM为中心的调优，利用预训练知识作为枢轴，同时保留跨领域鲁棒性。基于SPL，提出了一种动态提示微调方法，结合了动态类感知提示器和提示引导的特征聚焦器。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，该方法有效，并且优于最先进的方法，特别是在通用分割方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;集合枢轴学习和动态提示微调方法能够有效提升视觉基础模型在特定场景中的性能，特别是在通用分割任务上具有优越性。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们首次引入了集合枢轴学习的概念，这是一种基于视觉基础模型重新定义领域泛化的范式转变。传统领域泛化假设在训练过程中目标领域不可访问，但视觉基础模型的出现，这些模型在大量多样化数据上训练，使得这一假设变得不明确和过时。为了应对这一挑战，我们提出了基于VFMs的集合枢轴学习(SPL)，这是一种更适合当前研究和应用需求的领域迁移任务新定义。与传统DG方法不同，SPL优先考虑适应性调整而非严格的领域迁移，确保与不断变化的真实世界条件保持持续对齐。具体来说，SPL具有两个关键属性：(i)动态适应，从静态领域对齐转向灵活的、任务驱动的特征优化，使模型能够随着下游场景而演变；(ii)VFM为中心的调优，利用预训练知识作为枢轴，同时保留跨领域鲁棒性。基于SPL，我们提出了一种动态提示微调方法，结合了动态类感知提示器和提示引导的特征聚焦器，以提高VFMs在目标场景中的性能。在基准数据集上的大量实验表明了我们方法的有效性，突显了其优于最先进方法的优越性，特别是在通用分割方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce, for the first time, the concept of Set PivotLearning, a paradigm shift that redefines domain generalization (DG) based onVision Foundation Models (VFMs). Traditional DG assumes that the target domainis inaccessible during training, but the emergence of VFMs, trained on vast anddiverse data, renders this assumption unclear and obsolete. Traditional DGassumes that the target domain is inaccessible during training, but theemergence of VFMs, which are trained on vast and diverse datasets, renders thisassumption unclear and obsolete. To address this challenge, we propose SetPivot Learning (SPL), a new definition of domain migration task based on VFMs,which is more suitable for current research and application requirements.Unlike conventional DG methods, SPL prioritizes adaptive refinement over rigiddomain transfer, ensuring continuous alignment with evolving real-worldconditions. Specifically, SPL features two key attributes: (i) Dynamicadaptation, transitioning from static domain alignment to flexible, task-drivenfeature optimization, enabling models to evolve with downstream scenarios; (ii)VFM-centric tuning, leveraging pretrained knowledge as a pivot to honetask-specific representations while preserving cross-domain robustness.Building on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combinesa Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevateVFM performance in targeted scenarios. Extensive experiments on benchmarkdatasets show the effectiveness of our method, highlighting its superiorityover state-of-the-art methods, particularly in generalized segmentation.</description>
      <author>example@mail.com (Xinhui Li, Xinyu He, Qiming Hu, Xiaojie Guo)</author>
      <guid isPermaLink="false">2508.01582v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned Graph-Augmented Transformer</title>
      <link>http://arxiv.org/abs/2508.01465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EfficientGFormer，一种结合预训练基础模型、基于图的推理和轻量级效率机制的新型架构，用于鲁棒的3D脑肿瘤分割。&lt;h4&gt;背景&lt;/h4&gt;准确高效的脑肿瘤分割在神经影像学中仍是一个关键挑战，主要由于肿瘤亚区域的异质性和体积推断的高计算成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现准确且高效脑肿瘤分割的方法，同时保持临床实用性。&lt;h4&gt;方法&lt;/h4&gt;EfficientGFormer使用nnFormer作为模态感知编码器将多模态MRI转换为补丁级嵌入，构建捕捉空间邻接和语义相似性的双边图，采用修剪的边类型感知图注意力网络进行关系推理，并通过蒸馏模块实现知识转移以支持实时部署。&lt;h4&gt;主要发现&lt;/h4&gt;在MSD Task01和BraTS 2021数据集上，EfficientGFormer实现了最先进的准确性，同时显著减少了内存使用和推理时间，性能优于最近的基于transformer和基于图的方法。&lt;h4&gt;结论&lt;/h4&gt;EfficientGFormer为快速准确的体积肿瘤分割提供了临床可行的解决方案，结合了可扩展性、可解释性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确高效的脑肿瘤分割由于肿瘤亚区域的异质性和体积推断的高计算成本，在神经影像学中仍然是一个关键挑战。在本文中，我们提出了EfficientGFormer，一种结合预训练基础模型、基于图的推理和轻量级效率机制的新型架构，用于鲁棒的3D脑肿瘤分割。我们的框架利用nnFormer作为模态感知编码器，将多模态MRI体积转换为补丁级嵌入。这些特征被结构化为双边图，同时捕捉空间邻接和语义相似性。修剪的、边类型感知的图注意力网络(GAT)实现了肿瘤亚区域间的高效关系推理，而蒸馏模块将全容量教师模型的知识转移到紧凑的学生模型以实现实时部署。在MSD Task01和BraTS 2021数据集上的实验表明，EfficientGFormer以显著减少的内存和推理时间实现了最先进的准确性，优于最近的基于transformer和基于图的方法。这项工作为快速准确的体积肿瘤分割提供了临床可行的解决方案，结合了可扩展性、可解释性和泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决脑肿瘤分割中准确性和效率的平衡问题。脑肿瘤具有异质性，包含不同亚区域（如增强肿瘤、肿瘤核心和周围水肿），这些区域有不同的强度特征、形态和变异性，使得分割变得困难。同时，现有的3D分割方法计算成本高，难以在实际临床环境中部署。这个问题很重要，因为准确的脑肿瘤分割直接影响诊断、预后和治疗计划，而高效性则决定了该方法能否在临床实践中得到应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到transformer模型在3D分割任务中表现出色但存在计算瓶颈，同时看到图神经网络在建模医学图像关系结构方面有潜力。因此，作者思考如何将两者的优势结合起来。他们借鉴了nnFormer作为模态感知编码器，利用图神经网络进行关系建模，并引入知识蒸馏技术来提高效率。这种方法还受到了视频理解领域结合图建模和基础模型的启发，将其整合到医学成像的结构化和多模态推理中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将预训练的基础模型编码器与图推理和轻量级效率机制相结合，使用双重边图结构捕获空间邻接和语义相似性，通过剪枝的边类型感知图注意力网络实现高效的关系推理，并利用知识蒸馏将全容量教师模型的知识转移到紧凑的学生模型以实现实时部署。整体流程包括：1)使用预训练nnFormer处理多模态MRI数据提取特征；2)构建双重边图，连接空间邻近和语义相似的图像补丁；3)应用剪枝的边类型感知GAT进行关系推理；4)通过知识蒸馏压缩模型；5)使用分割头生成体素级肿瘤预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出EfficientGFormer架构，结合基础模型编码器、图推理和效率模块；2)引入双重边图构建策略，捕获空间连续性和语义相似性；3)设计剪枝的边类型感知GAT，减少计算开销；4)实现知识蒸馏模块，压缩模型同时保持精度；5)在多个数据集上展示最先进性能。相比之前的工作，这种方法不仅结合了transformer和GNN的优势，还通过双重边图和边类型感知注意力更有效地建模空间和语义关系，并通过知识蒸馏实现了模型压缩，使其更适合临床应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EfficientGFormer通过结合基础模型编码器、双重边图推理和知识蒸馏，实现了准确且高效的脑肿瘤分割，为临床应用提供了兼具高精度和实时性能的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient brain tumor segmentation remains a critical challengein neuroimaging due to the heterogeneous nature of tumor subregions and thehigh computational cost of volumetric inference. In this paper, we proposeEfficientGFormer, a novel architecture that integrates pretrained foundationmodels with graph-based reasoning and lightweight efficiency mechanisms forrobust 3D brain tumor segmentation. Our framework leverages nnFormer as amodality-aware encoder, transforming multi-modal MRI volumes into patch-levelembeddings. These features are structured into a dual-edge graph that capturesboth spatial adjacency and semantic similarity. A pruned, edge-type-aware GraphAttention Network (GAT) enables efficient relational reasoning across tumorsubregions, while a distillation module transfers knowledge from afull-capacity teacher to a compact student model for real-time deployment.Experiments on the MSD Task01 and BraTS 2021 datasets demonstrate thatEfficientGFormer achieves state-of-the-art accuracy with significantly reducedmemory and inference time, outperforming recent transformer-based andgraph-based baselines. This work offers a clinically viable solution for fastand accurate volumetric tumor delineation, combining scalability,interpretability, and generalization.</description>
      <author>example@mail.com (Fatemeh Ziaeetabar)</author>
      <guid isPermaLink="false">2508.01465v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting</title>
      <link>http://arxiv.org/abs/2508.01426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 80 figures, submitted to ACM KDD 2026 conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了UniExtreme，一个通用的极端天气预测基础模型，通过整合自适应频率调制和事件先验增强两个模块，有效解决了现有方法在预测多样化极端天气事件方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;深度学习的进步已促成天气预测基础模型的发展，但这些模型预测极端天气事件的能力仍然有限。现有方法要么专注于一般天气条件，要么专门针对特定类型的极端天气，忽略了多样化极端事件的现实大气模式。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效预测多样化极端天气事件的基础模型，解决现有方法在处理极端天气事件频谱差异和层次驱动因素方面的不足。&lt;h4&gt;方法&lt;/h4&gt;UniExtreme模型包含两个关键模块：(1)自适应频率调制(AFM)模块，通过可学习的Beta分布滤波器和多粒度频谱聚合，捕捉区域性的正常与极端天气之间的频谱差异；(2)事件先验增强(EPA)模块，通过双级记忆融合网络，整合区域特定的极端事件先验，解决极端多样性和复合极端模式问题。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验证明，UniExtreme在极端天气和一般天气预测方面都优于最先进的基线方法，展现出在多样化极端场景下的卓越适应性。&lt;h4&gt;结论&lt;/h4&gt;UniExtreme模型通过整合自适应频率调制和事件先验增强两个关键模块，有效解决了极端天气预测中的频谱差异和层次驱动因素问题，为极端天气预测提供了一个通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的最新进展已经促成了天气预测基础模型(FMs)的发展，但它们预测极端天气事件的能力仍然有限。现有方法要么专注于一般天气条件，要么专门针对特定类型的极端天气，忽略了多样化极端事件的现实大气模式。在这项工作中，我们确定了极端事件的两个关键特征：(1)与正常天气系统的频谱差异，以及(2)多样化极端事件的层次驱动因素和地理融合。基于此，我们提出了UniExtreme，一个通用的极端天气预测基础模型，它整合了(1)一个自适应频率调制(AFM)模块，通过可学习的Beta分布滤波器和多粒度频谱聚合，捕捉区域性的正常与极端天气之间的频谱差异，以及(2)一个事件先验增强(EPA)模块，通过双级记忆融合网络，整合区域特定的极端事件先验，以解决极端多样性和复合极端模式问题。大量实验证明，UniExtreme在极端天气和一般天气预测方面都优于最先进的基线方法，展现出在多样化极端场景下的卓越适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in deep learning have led to the development ofFoundation Models (FMs) for weather forecasting, yet their ability to predictextreme weather events remains limited. Existing approaches either focus ongeneral weather conditions or specialize in specific-type extremes, neglectingthe real-world atmospheric patterns of diversified extreme events. In thiswork, we identify two key characteristics of extreme events: (1) the spectraldisparity against normal weather regimes, and (2) the hierarchical drivers andgeographic blending of diverse extremes. Along this line, we proposeUniExtreme, a universal extreme weather forecasting foundation model thatintegrates (1) an Adaptive Frequency Modulation (AFM) module that capturesregion-wise spectral differences between normal and extreme weather, throughlearnable Beta-distribution filters and multi-granularity spectral aggregation,and (2) an Event Prior Augmentation (EPA) module which incorporatesregion-specific extreme event priors to resolve hierarchical extreme diversityand composite extreme schema, via a dual-level memory fusion network. Extensiveexperiments demonstrate that UniExtreme outperforms state-of-the-art baselinesin both extreme and general weather forecasting, showcasing superioradaptability across diverse extreme scenarios.</description>
      <author>example@mail.com (Hang Ni, Weijia Zhang, Hao Liu)</author>
      <guid isPermaLink="false">2508.01426v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>VLH: Vision-Language-Haptics Foundation Model</title>
      <link>http://arxiv.org/abs/2508.01361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了VLH，一种新型的视觉-语言-触觉基础模型，统一了空中机器人和虚拟现实中的感知、语言和触觉反馈。&lt;h4&gt;背景&lt;/h4&gt;先前的研究将触觉作为次要的、反应性的通道，而VLH将空中力和振动提示作为视觉理解上下文和自然语言命令的直接结果。&lt;h4&gt;目的&lt;/h4&gt;创建一个能够统一视觉、语言和触觉反馈的基础模型，以增强人机交互的表达性和沉浸感。&lt;h4&gt;方法&lt;/h4&gt;平台由8英寸四轴飞行器组成，配备双反向五连杆阵列用于局部触觉驱动，第一人称VR摄像头和自上下的外部视角；使用微调的OpenVLA主干网络，通过在450个多模态场景的自定义数据集上使用LoRA进行适应，输出7维动作向量；采用INT8量化和高性能服务器确保4-5Hz的实时操作。&lt;h4&gt;主要发现&lt;/h4&gt;人机交互实验（90次飞行）中，目标获取成功率为56.7%（平均到达时间21.3秒，姿态误差0.24米），纹理辨别准确率为100%；泛化测试在新任务上的表现：视觉70.0%，运动54.4%，物理40.0%，语义35.0%。&lt;h4&gt;结论&lt;/h4&gt;VLH能够将触觉反馈与感知推理和意图共同发展，推进了表达性强、沉浸式的人机交互。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了VLH，一种新型的视觉-语言-触觉基础模型，统一了空中机器人和虚拟现实中的感知、语言和触觉反馈。与先前将触觉作为次要、反应性通道的研究不同，VLH将空中力和振动提示作为视觉理解上下文和自然语言命令的直接结果。我们的平台包括一个配备双反向五连杆阵列用于局部触觉驱动的8英寸四轴飞行器，一个第一人称VR摄像头，以及一个自上而下的外部视角。视觉输入和语言指令由微调的OpenVLA主干网络处理——通过在450个多模态场景的自定义数据集上使用LoRA进行适应——输出一个7维动作向量（Vx, Vy, Vz, Hx, Hy, Hz, Hv）。INT8量化和高性能服务器确保4-5Hz的实时操作。在人机交互实验（90次飞行）中，VLH在目标获取方面实现了56.7%的成功率（平均到达时间21.3秒，姿态误差0.24米），纹理辨别准确率为100%。泛化测试在新任务上取得了70.0%（视觉）、54.4%（运动）、40.0%（物理）和35.0%（语义）的性能。这些结果证明了VLH能够将触觉反馈与感知推理和意图共同发展，推进了表达性强、沉浸式的人机交互。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在虚拟现实和空中机器人领域中，如何将触觉反馈作为一种生成性、表达性的模态，与视觉感知和自然语言处理无缝集成的问题。这个问题很重要，因为当前触觉技术主要局限于静态、预编程的反馈，无法捕捉触觉动态、上下文敏感的特性，这限制了沉浸式人机交互的发展。触觉作为人类交流的基本方面，能够传达情感、意图和物理互动，具有无与伦比的细微差别，在动态环境如空中机器人中，缺乏上下文感知的触觉反馈系统限制了人机交互的自然性和有效性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的设计思路是将触觉视为感知和意图的直接结果，而非次要的反应性通道。他们设计了一个配备双倒五杆联动阵列的空中触觉平台，结合机械驱动与感知推理，使用视觉数据估计物体几何和表面特性。作者借鉴了多项现有工作：基于斯坦福AI实验室的OpenVLA模型进行微调；使用低秩适应(LoRA)方案更新权重；参考了RaceVLA和CognitiveDrone等视觉-语言-动作框架的思想，但增加了触觉模态；受到HapticGen等文本到振动模型的启发；参考了VRHapticDrones等利用四旋翼作为触觉反馈代理的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将触觉反馈视为一种生成性、表达性的模态，与视觉感知和自然语言处理无缝集成，实现上下文感知的实时触觉渲染和交互。整体实现流程包括：1)数据采集：收集真实世界与VR环境的交互数据，创建450个多模态场景的自定义数据集；2)模型训练：基于OpenVLA模型进行微调，使用LoRA方案更新权重；3)系统架构：包括双视觉系统、适应的VLA模型、意图推断和触觉映射模块、空中触觉接口；4)实时操作：使用INT8量化和高性能服务器实现4-5Hz的更新率；5)输出与反馈：模型输出三维速度控制和三维方向力及振动强度，无人机执行飞行控制并产生触觉反馈。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将触觉作为生成性模态，而非仅是反应性通道；2)开发配备双倒五杆联动阵列的空中触觉平台；3)结合第一人称视角和俯视外部视角的双视觉系统；4)适应的VLA模型，输出7维动作向量；5)实时性能优化，实现4-5Hz的操作频率。相比之前工作的不同：传统触觉系统提供静态反馈，VLH提供动态上下文感知反馈；现有VLA框架如RaceVLA专注于导航但忽略触觉，VLH集成了触觉实现多模态交互；VRHapticDrones提供被动触觉表面，VLH主动合成与飞行命令协调的触觉模式；HapticGen等从文本生成触觉，VLH结合视觉、语言和触觉实现全面交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VLH开创了一种将触觉反馈作为生成性、表达性模态与视觉感知和自然语言处理无缝集成的新方法，通过创新的空中触觉平台和适应的视觉-语言-动作模型，实现了上下文感知的实时触觉渲染和交互，为沉浸式人机交互树立了新标杆。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present VLH, a novel Visual-Language-Haptic Foundation Model that unifiesperception, language, and tactile feedback in aerial robotics and virtualreality. Unlike prior work that treats haptics as a secondary, reactivechannel, VLH synthesizes mid-air force and vibration cues as a directconsequence of contextual visual understanding and natural language commands.Our platform comprises an 8-inch quadcopter equipped with dual inverse five-barlinkage arrays for localized haptic actuation, an egocentric VR camera, and anexocentric top-down view. Visual inputs and language instructions are processedby a fine-tuned OpenVLA backbone - adapted via LoRA on a bespoke dataset of 450multimodal scenarios - to output a 7-dimensional action vector (Vx, Vy, Vz, Hx,Hy, Hz, Hv). INT8 quantization and a high-performance server ensure real-timeoperation at 4-5 Hz. In human-robot interaction experiments (90 flights), VLHachieved a 56.7% success rate for target acquisition (mean reach time 21.3 s,pose error 0.24 m) and 100% accuracy in texture discrimination. Generalizationtests yielded 70.0% (visual), 54.4% (motion), 40.0% (physical), and 35.0%(semantic) performance on novel tasks. These results demonstrate VLH's abilityto co-evolve haptic feedback with perceptual reasoning and intent, advancingexpressive, immersive human-robot interactions.</description>
      <author>example@mail.com (Luis Francisco Moreno Fuentes, Muhammad Haris Khan, Miguel Altamirano Cabrera, Valerii Serpiva, Dmitri Iarchuk, Yara Mahmoud, Issatay Tokmurziyev, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2508.01361v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study</title>
      <link>http://arxiv.org/abs/2508.01352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度学习的框架，用于从常规病理切片中预测EGFR突变状态，结合视觉变换器基础模型和基于注意力的多实例学习架构，在印度队列和TCGA数据集上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;肺腺癌是非小细胞肺癌的一种亚型，约46%的肺腺癌患者存在EGFR基因突变，这类患者可使用特异性酪氨酸激酶抑制剂治疗。东南亚人群的EGFR突变率显著高于高加索人（39-64%对比7-22%）。H&amp;E染色的全切片成像是癌症分期和亚型的常规筛查程序，近期AI模型在癌症检测和分类方面展现出良好前景。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度学习框架，从H&amp;E全切片成像中预测EGFR突变状态，以辅助临床决策制定。&lt;h4&gt;方法&lt;/h4&gt;构建基于视觉变换器病理基础模型和基于注意力的多实例学习架构的深度学习框架。使用印度队列（170个全切片成像）进行训练，并在两个独立数据集上评估：内部测试集（印度队列中的30个全切片成像）和TCGA的外部测试集（86个全切片成像）。&lt;h4&gt;主要发现&lt;/h4&gt;模型在两个数据集上均表现出一致的性能，内部测试集的AUC为0.933（±0.010），外部测试集的AUC为0.965（±0.015）。该框架可以在小数据集上有效训练，与先前研究相比，无论训练领域如何，都取得了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了使用常规病理切片准确预测EGFR突变状态的可行性，特别是在资源有限的环境中，使用基础模型和基于注意力的多实例学习可有效实现这一目标。&lt;h4&gt;翻译&lt;/h4&gt;肺腺癌是非小细胞肺癌的一种亚型。EGFR基因突变的肺腺癌约占肺腺癌病例的46%。携带EGFR突变的患者可以用特异性酪氨酸激酶抑制剂治疗。因此，预测EGFR突变状态有助于临床决策制定。H&amp;E染色的全切片成像是常规执行的癌症分期和亚型筛查程序，特别是在东南亚人群中，与高加索人相比，该人群的突变发生率显著更高（39-64%对比7-22%）。近期AI模型的进展在癌症检测和分类方面显示出有前景的结果。在本研究中，我们提出了一种深度学习框架，基于视觉变换器的病理基础模型和基于注意力的多实例学习架构，用于从H&amp;E全切片成像中预测EGFR突变状态。开发的管道使用印度队列（170个全切片成像）的数据进行训练，并在两个独立数据集上评估：内部测试集（来自印度队列的30个全切片成像）和来自TCGA的外部测试集（86个全切片成像）。模型在两个数据集上都表现出一致的性能，内部测试集和外部测试集的AUC分别为0.933（±0.010）和0.965（±0.015）。该框架可以在小数据集上有效训练，与先前研究相比，无论训练领域如何，都取得了优越的性能。当前研究证明了使用常规病理切片准确预测EGFR突变状态的可行性，特别是在资源有限的环境中，使用基础模型和基于注意力的多实例学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung adenocarcinoma (LUAD) is a subtype of non-small cell lung cancer(NSCLC). LUAD with mutation in the EGFR gene accounts for approximately 46% ofLUAD cases. Patients carrying EGFR mutations can be treated with specifictyrosine kinase inhibitors (TKIs). Hence, predicting EGFR mutation status canhelp in clinical decision making. H&amp;E-stained whole slide imaging (WSI) is aroutinely performed screening procedure for cancer staging and subtyping,especially affecting the Southeast Asian populations with significantly higherincidence of the mutation when compared to Caucasians (39-64% vs 7-22%). Recentprogress in AI models has shown promising results in cancer detection andclassification. In this study, we propose a deep learning (DL) framework builton vision transformers (ViT) based pathology foundation model andattention-based multiple instance learning (ABMIL) architecture to predict EGFRmutation status from H&amp;E WSI. The developed pipeline was trained using datafrom an Indian cohort (170 WSI) and evaluated across two independent datasets:Internal test (30 WSI from Indian cohort) set, and an external test set fromTCGA (86 WSI). The model shows consistent performance across both datasets,with AUCs of 0.933 (+/-0.010), and 0.965 (+/-0.015) for the internal andexternal test sets respectively. This proposed framework can be efficientlytrained on small datasets, achieving superior performance as compared toseveral prior studies irrespective of training domain. The current studydemonstrates the feasibility of accurately predicting EGFR mutation statususing routine pathology slides, particularly in resource-limited settings usingfoundation models and attention-based multiple instance learning.</description>
      <author>example@mail.com (Sagar Singh Gwal, Rajan, Suyash Devgan, Shraddhanjali Satapathy, Abhishek Goyal, Nuruddin Mohammad Iqbal, Vivaan Jain, Prabhat Singh Mallik, Deepali Jain, Ishaan Gupta)</author>
      <guid isPermaLink="false">2508.01352v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Advancing the Foundation Model for Music Understanding</title>
      <link>http://arxiv.org/abs/2508.01178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为MuFun的统一基础模型，用于整体音乐理解，挑战了音乐信息检索领域碎片化的现状。&lt;h4&gt;背景&lt;/h4&gt;音乐信息检索(MIR)领域是碎片化的，专业模型在孤立任务上表现优异。&lt;h4&gt;目的&lt;/h4&gt;挑战现有范式，引入一个统一的音乐理解基础模型。&lt;h4&gt;方法&lt;/h4&gt;MuFun模型具有新颖的架构，能够同时处理乐器内容和歌词内容，并在覆盖多种任务(如流派分类、音乐标记和问答)的大规模数据集上进行训练。同时提出了新的多方面音乐理解基准测试MuCUE。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MuFun模型在MuCUE任务上显著优于现有的音频大语言模型。&lt;h4&gt;结论&lt;/h4&gt;MuFun模型展示了最先进的有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;音乐信息检索(MIR)领域是碎片化的，专业模型在孤立任务上表现优异。在这项工作中，我们通过引入一个名为MuFun的统一基础模型来挑战这一范式，用于整体音乐理解。我们的模型具有新颖的架构，能够同时处理乐器内容和歌词内容，并在覆盖多种任务(如流派分类、音乐标记和问答)的大规模数据集上进行训练。为了促进稳健评估，我们还提出了一个名为MuCUE(音乐综合理解评估)的多方面音乐理解新基准。实验表明，我们的模型在MuCUE任务上显著优于现有的音频大语言模型，展示了其最先进的有效性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of Music Information Retrieval (MIR) is fragmented, withspecialized models excelling at isolated tasks. In this work, we challenge thisparadigm by introducing a unified foundation model named MuFun for holisticmusic understanding. Our model features a novel architecture that jointlyprocesses instrumental and lyrical content, and is trained on a large-scaledataset covering diverse tasks such as genre classification, music tagging, andquestion answering. To facilitate robust evaluation, we also propose a newbenchmark for multi-faceted music understanding called MuCUE (MusicComprehensive Understanding Evaluation). Experiments show our modelsignificantly outperforms existing audio large language models across the MuCUEtasks, demonstrating its state-of-the-art effectiveness and generalizationability.</description>
      <author>example@mail.com (Yi Jiang, Wei Wang, Xianwen Guo, Huiyun Liu, Hanrui Wang, Youri Xu, Haoqi Gu, Zhongqian Xie, Chuanjiang Luo)</author>
      <guid isPermaLink="false">2508.01178v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions</title>
      <link>http://arxiv.org/abs/2508.02477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多类图像异常检测中类信息使用和检测阈值定义的问题，提出了Hierarchical Coreset (HierCore)框架，该框架能够在不同条件下有效工作，即使在没有类标签的情况下也能保持稳定性能。&lt;h4&gt;背景&lt;/h4&gt;图像异常检测已从单类扩展到多类框架以提高效率，但单模型处理多类时每类检测准确性通常低于特定类别模型。以往研究主要关注缩小性能差距，而类信息如何影响检测阈值定义相对未被研究。&lt;h4&gt;目的&lt;/h4&gt;确定并形式化多类图像异常检测模型在不同条件（训练和评估是否有类标签）下必须满足的要求，并据此提出新框架。&lt;h4&gt;方法&lt;/h4&gt;提出Hierarchical Coreset (HierCore)框架，利用分层存储库估计类决策标准，即使在无类标签情况下也能有效工作。在四种由训练和评估阶段类标签存在与否决定的不同场景下验证方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明HierCore在各种设置下均满足所有要求，保持强大且稳定的性能，显示出在实际多类异常检测任务中的潜力。&lt;h4&gt;结论&lt;/h4&gt;HierCore框架能有效处理多类图像异常检测问题，无论训练和评估阶段是否有类标签，都能保持良好性能，为实际应用提供了有价值的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图像异常检测的最新进展已经将基于无监督学习的模型从单类设置扩展到多类框架，旨在提高训练时间和模型存储效率。当单个模型被训练处理多个类别时，其在每类检测准确性方面通常不如特定类别的模型。因此，以往的研究主要关注缩小这种性能差距。然而，类信息如何使用或未使用仍然是一个相对未被研究的因素，这可能影响多类图像异常检测中检测阈值的定义。这些阈值，无论是特定类别的还是类别无关的，都会显著影响检测结果。在本研究中，我们确定了并形式化了多类图像异常检测模型在不同条件下必须满足的要求，这些条件取决于训练和评估期间是否有类标签可用。然后，我们根据这些标准重新审视了现有方法。为了应对这些挑战，我们提出了Hierarchical Coreset (HierCore)，一个设计用于满足所有定义要求的新颖框架。HierCore即使在没有类标签的情况下也能有效工作，利用分层存储库来估计用于异常检测的类决策标准。我们在四种不同场景下实证验证了现有方法和HierCore的适用性和鲁棒性，这些场景由训练和评估阶段是否存在类标签决定。实验结果表明，HierCore始终满足所有要求，并在所有设置中保持强大且稳定的性能，突显了其在现实世界多类异常检测任务中的实际潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in image anomaly detection have extended unsupervisedlearning-based models from single-class settings to multi-class frameworks,aiming to improve efficiency in training time and model storage. When a singlemodel is trained to handle multiple classes, it often underperforms compared toclass-specific models in terms of per-class detection accuracy. Accordingly,previous studies have primarily focused on narrowing this performance gap.However, the way class information is used, or not used, remains a relativelyunderstudied factor that could influence how detection thresholds are definedin multi-class image anomaly detection. These thresholds, whetherclass-specific or class-agnostic, significantly affect detection outcomes. Inthis study, we identify and formalize the requirements that a multi-class imageanomaly detection model must satisfy under different conditions, depending onwhether class labels are available during training and evaluation. We thenre-examine existing methods under these criteria. To meet these challenges, wepropose Hierarchical Coreset (HierCore), a novel framework designed to satisfyall defined requirements. HierCore operates effectively even without classlabels, leveraging a hierarchical memory bank to estimate class-wise decisioncriteria for anomaly detection. We empirically validate the applicability androbustness of existing methods and HierCore under four distinct scenarios,determined by the presence or absence of class labels in the training andevaluation phases. The experimental results demonstrate that HierCoreconsistently meets all requirements and maintains strong, stable performanceacross all settings, highlighting its practical potential for real-worldmulti-class anomaly detection tasks.</description>
      <author>example@mail.com (Jaehyuk Heo, Pilsung Kang)</author>
      <guid isPermaLink="false">2508.02477v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Efforts in Modeling the Mechanics and Chemistry of Energetic Materials Across Scales</title>
      <link>http://arxiv.org/abs/2508.02141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了含能分子晶体多尺度力学和化学本构定律的最新发展，包括分子动力学模拟工具的开发、连续介质模型的构建、化学分解动力学的识别与校准，以及在中尺度水平上模拟冲击到爆轰转变的研究。&lt;h4&gt;背景&lt;/h4&gt;含能分子晶体的多尺度建模需要将原子尺度的信息传递到连续介质尺度，以准确描述材料的力学和化学行为。&lt;h4&gt;目的&lt;/h4&gt;开发综合的多尺度建模方法，用于模拟含能分子晶体的力学行为和化学分解，特别是冲击到爆轰的转变过程。&lt;h4&gt;方法&lt;/h4&gt;在分子动力学代码中集成特定工具，增强原子模拟的变形跟踪能力，构建非线性超弹性连续介质模型，使用无监督学习算法分析反应分子动力学模拟，并在有限元代码中实现这些模型。&lt;h4&gt;主要发现&lt;/h4&gt;成功构建了TATB的晶体塑性模型，识别和校准了RDX和TATB单晶的化学分解动力学，并将方法扩展到β-HMX和多组分状态方程，能够模拟中尺度水平的冲击到爆轰转变。&lt;h4&gt;结论&lt;/h4&gt;多尺度建模方法为含能分子晶体的研究提供了有力工具，但也需要新的前瞻性实验来验证和进一步发展这些模型。&lt;h4&gt;翻译&lt;/h4&gt;本文致力于构建含能分子晶体的多尺度力学和化学本构定律的最新发展进行了阐述和讨论。特别是，各种工具已被专门整合到分子动力学代码中，以促进随后向连续介质（即有限元模拟代码）的信息传递。原子模拟已增强跟踪特定变形路径以及局部拉格朗日力学度量的能力，使材料流动应力面的计算成为可能。这种机制库允许构建一个包括TATB晶体塑性和孪生在内的全面非线性超弹性连续介质模型。此外，使用无监督学习算法分析反应分子动力学模拟的最新进展，使RDX和TATB单晶的化学分解动力学的识别和校准成为可能。在本工作中，该程序被应用于β-HMX，并扩展到多组分状态方程的校准。这两个要素被实现在有限元代码中，以便在介观尺度上模拟冲击到爆轰的转变，并研究准静态热点中的尺寸效应。最后，这些针对含能材料综合多尺度建模的努力也带来了对新的前瞻性实验的需求，本文对此进行了讨论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments dedicated to the building of multiscale mechanical andchemical constitutive laws for energetic molecular crystals are presented anddiscussed. In particular, various tools have been specifically incorporated inmolecular dynamics codes to facilitate the subsequent information transfer tothe continuum, i.e. finite elements simulation codes. Atomistic simulationshave been augmented with the capability to follow specific deformation paths aswell as local Lagrangian mechanical metrics, enabling the computation ofmaterials flow stress surface. This mechanistic library allowed theconstruction of a comprehensive non-linear hyperelastic continuum modelincluding crystal plasticity and twinning for TATB. Besides, recent advances inanalyzing reactive molecular dynamics simulations with unsupervised learningalgorithms has enabled the identification and calibration of chemicaldecomposition kinetics for RDX and TATB single crystal. In the present work,the procedure is applied to $\beta$-HMX and extended with the calibration of amulti-components equation of state. These two ingredients are implemented in afinite-element code in order to model the shock-to-detonation transition at themesoscale level and to study dimensionality effects in quasi-static hotspots.Finally, these dedicated efforts towards a comprehensive multiscale modeling ofexplosives has also given rise to the need for new prospective experiments,discussed throughout the paper.</description>
      <author>example@mail.com (Paul Lafourcade, Nicolas Bruzy, Paul Bouteiller, Jean-Bernard Maillet, Christophe Denoual)</author>
      <guid isPermaLink="false">2508.02141v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Bayesian approach to model uncertainty in single-cell genomic data</title>
      <link>http://arxiv.org/abs/2508.02061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种基于变分贝叶斯框架的单细胞基因组数据聚类分析方法，通过贝叶斯高斯混合模型估计细胞与不同簇的概率关联，能够捕捉细胞转换状态，并在神经发生和乳腺癌进展研究中获得了生物学上有意义的见解。&lt;h4&gt;背景&lt;/h4&gt;网络模型为分析单细胞计数数据提供了强大框架，有助于表征细胞身份、疾病机制和发育轨迹。然而，在基因组数据无监督学习中的不确定性建模仍未得到充分探索。传统聚类方法为每个细胞分配单一身份，可能掩盖分化或突变过程中的过渡状态。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉细胞转换状态的不确定性建模方法，用于单细胞基因组数据的聚类和分析，以更准确地揭示发育和疾病过程中的动态细胞身份。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一种变分贝叶斯框架用于聚类和分析单细胞基因组数据，采用贝叶斯高斯混合模型来估计细胞与不同簇的概率关联。此外，提出了使用误分簇率和曲线下面积作为评估单细胞RNA测序数据聚类性能的创新指标。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够捕捉细胞转换状态，在神经发生和乳腺癌进展研究中获得了生物学上有意义的见解。推断的聚类概率可以进行进一步分析，包括差异表达分析和伪时间分析。&lt;h4&gt;结论&lt;/h4&gt;这种方法学进展增强了单细胞数据分析的分辨率，能够对发育和疾病中的动态细胞身份进行更细致的表征，为理解复杂的生物过程提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;网络模型为分析单细胞计数数据提供了强大的框架，有助于表征细胞身份、疾病机制和发育轨迹。然而，在基因组数据无监督学习中的不确定性建模仍未得到充分探索。传统的聚类方法为每个细胞分配单一的身份，可能掩盖分化或突变过程中的过渡状态。本研究引入了一种用于聚类和分析单细胞基因组数据的变分贝叶斯框架，采用贝叶斯高斯混合模型来估计细胞与不同簇的概率关联。这种方法能够捕捉细胞转换状态，在神经发生和乳腺癌进展研究中获得了生物学上有意义的见解。推断的聚类概率可以进行进一步分析，包括差异表达分析和伪时间分析。此外，我们提出使用误分簇率和曲线下面积作为评估单细胞RNA测序数据聚类性能的创新指标，以定量评估整体聚类性能。这种方法学进展增强了单细胞数据分析的分辨率，能够对发育和疾病中的动态细胞身份进行更细致的表征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network models provide a powerful framework for analysing single-cell countdata, facilitating the characterisation of cellular identities, diseasemechanisms, and developmental trajectories. However, uncertainty modeling inunsupervised learning with genomic data remains insufficiently explored.Conventional clustering methods assign a singular identity to each cell,potentially obscuring transitional states during differentiation or mutation.This study introduces a variational Bayesian framework for clustering andanalysing single-cell genomic data, employing a Bayesian Gaussian mixture modelto estimate the probabilistic association of cells with distinct clusters. Thisapproach captures cellular transitions, yielding biologically coherent insightsinto neurogenesis and breast cancer progression. The inferred clusteringprobabilities enable further analyses, including Differential ExpressionAnalysis and pseudotime analysis. Furthermore, we propose utilising themisclustering rate and Area Under the Curve in clustering scRNA-seq data as aninnovative metric to quantitatively evaluate overall clustering performance.This methodological advancement enhances the resolution of single-cell dataanalysis, enabling a more nuanced characterisation of dynamic cellularidentities in development and disease.</description>
      <author>example@mail.com (Shanshan Ren, Thomas E. Bartlett, Lina Gerontogianni, Swati Chandna)</author>
      <guid isPermaLink="false">2508.02061v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.02039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种模型回收框架，用于源数据自由的迁移学习，通过识别和重用相关源模型的子集，实现了参数高效的模型训练，使模型即服务提供商能够构建高效的预训练模型库。&lt;h4&gt;背景&lt;/h4&gt;数据隐私问题和获取源数据用于模型训练的困难，使得源数据自由的迁移学习变得必要，在这种设置下只能访问预训练模型而无法获取原始源域数据。&lt;h4&gt;目的&lt;/h4&gt;解决源数据不可用时迁移学习的挑战，提出一种模型回收框架用于参数高效训练，识别相关源模型的子集进行重用。&lt;h4&gt;方法&lt;/h4&gt;提出模型回收框架，能够在白盒和黑盒设置下重用源模型子集，实现参数高效的模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;现有迁移学习方法通常依赖源数据访问，限制了其在源数据不可用场景的直接应用；没有源数据信息的情况下难以高效选择用于迁移的模型；无法完全访问源模型的情况下进行迁移也存在困难。&lt;h4&gt;结论&lt;/h4&gt;该框架使模型即服务提供商能够构建高效的预训练模型库，创造了多源数据自由监督迁移学习的机会。&lt;h4&gt;翻译&lt;/h4&gt;对数据隐私日益增长的担忧以及检索用于模型训练的源数据所面临的其他困难，使得对源数据自由的迁移学习变得必要，在这种学习中，人们只能访问预训练模型，而无法获取原始源域的数据。这种设置带来了许多挑战，因为现有的迁移学习方法通常依赖于对源数据的访问，这限制了它们在源数据不可用场景中的直接应用。此外，实际问题使其变得更加困难，例如在没有源数据信息的情况下高效选择用于迁移的模型，以及在无法完全访问源模型的情况下进行迁移。因此，我们提出了一种模型回收框架，用于模型的参数高效训练，该框架能够识别要在白盒和黑盒设置中重用的相关源模型的子集。因此，我们的框架使得模型即服务提供商能够构建高效的预训练模型库，从而创造了多源数据自由监督迁移学习的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increasing concerns for data privacy and other difficulties associated withretrieving source data for model training have created the need for source-freetransfer learning, in which one only has access to pre-trained models insteadof data from the original source domains. This setting introduces manychallenges, as many existing transfer learning methods typically rely on accessto source data, which limits their direct applicability to scenarios wheresource data is unavailable. Further, practical concerns make it more difficult,for instance efficiently selecting models for transfer without information onsource data, and transferring without full access to the source models. Somotivated, we propose a model recycling framework for parameter-efficienttraining of models that identifies subsets of related source models to reuse inboth white-box and black-box settings. Consequently, our framework makes itpossible for Model as a Service (MaaS) providers to build libraries ofefficient pre-trained models, thus creating an opportunity for multi-sourcedata-free supervised transfer learning.</description>
      <author>example@mail.com (Sijia Wang, Ricardo Henao)</author>
      <guid isPermaLink="false">2508.02039v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2508.01916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为邻居距离最小化(NDM)的无监督学习方法，能够在神经网络模型中找到可解释的子空间，这些子空间类似于模型使用的变量，为理解模型内部机制提供了新视角。&lt;h4&gt;背景&lt;/h4&gt;理解神经模型的内部表征是机制可解释性的核心兴趣点。由于表征空间的高维性，它可以编码关于输入的各种方面，但这些不同方面在多大程度上被组织和编码在独立的子空间中尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;探索是否可以纯粹以无监督的方式找到神经网络中编码不同方面的'自然'子空间，并验证这些子空间的可解释性和与模型内部变量的关联性。&lt;h4&gt;方法&lt;/h4&gt;提出邻居距离最小化(NDM)方法，以无监督方式学习非基对齐的子空间，并在GPT-2模型上进行实验，验证该方法在大型模型(20亿参数)上的可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 定性分析显示NDM找到的子空间在许多情况下是可解释的；2) 这些子空间中编码的信息在不同输入间往往共享相同的抽象概念；3) 定量实验表明子空间与GPT-2中的电路变量有强联系；4) 该方法可扩展到20亿模型，能够发现分别介导上下文和参数知识路由的独立子空间。&lt;h4&gt;结论&lt;/h4&gt;NDM方法为理解神经网络模型内部表征和构建计算电路提供了新视角，证明了可以通过看似无关的训练目标在无监督方式下发现具有语义意义的子空间结构。&lt;h4&gt;翻译&lt;/h4&gt;理解神经模型的内部表征是机制可解释性的核心兴趣。由于其高维性，表征空间可以编码关于输入的各个方面。不同方面在多大程度上被组织和编码在独立的子空间中？是否可以纯粹以无监督的方式找到这些'自然'子空间？有些令人惊讶的是，我们确实可以通过一个看似无关的训练目标实现这一点，并找到可解释的子空间。我们的方法，邻居距离最小化(NDM)，以无监督方式学习非基对齐的子空间。定性分析显示子空间在许多情况下是可解释的，并且获得的子空间中编码的信息往往在不同输入间共享相同的抽象概念，使这些子空间类似于模型使用的'变量'。我们还使用GPT-2中的已知电路进行了定量实验；结果显示子空间与电路变量之间存在强联系。我们还提供了证据表明该方法可扩展到20亿模型，通过发现分别介导上下文和参数知识路由的独立子空间。更广泛地看，我们的发现为理解模型内部和构建电路提供了新视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding internal representations of neural models is a core interest ofmechanistic interpretability. Due to its large dimensionality, therepresentation space can encode various aspects about inputs. To what extentare different aspects organized and encoded in separate subspaces? Is itpossible to find these ``natural'' subspaces in a purely unsupervised way?Somewhat surprisingly, we can indeed achieve this and find interpretablesubspaces by a seemingly unrelated training objective. Our method, neighbordistance minimization (NDM), learns non-basis-aligned subspaces in anunsupervised manner. Qualitative analysis shows subspaces are interpretable inmany cases, and encoded information in obtained subspaces tends to share thesame abstract concept across different inputs, making such subspaces similar to``variables'' used by the model. We also conduct quantitative experiments usingknown circuits in GPT-2; results show a strong connection between subspaces andcircuit variables. We also provide evidence showing scalability to 2B models byfinding separate subspaces mediating context and parametric knowledge routing.Viewed more broadly, our findings offer a new perspective on understandingmodel internals and building circuits.</description>
      <author>example@mail.com (Xinting Huang, Michael Hahn)</author>
      <guid isPermaLink="false">2508.01916v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation</title>
      <link>http://arxiv.org/abs/2508.01772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了使用迁移学习方法改善动脉瘤性蛛网膜下腔出血(SAH)的医学图像分割。研究团队基于Unet架构实现了从创伤性脑损伤患者到动脉瘤性SAH患者的迁移学习，并开发了创新的LoRA变体方法。结果显示，LoRA方法显著优于传统Unet微调，且CP-LoRA在性能相当的情况下参数更少。&lt;h4&gt;背景&lt;/h4&gt;动脉瘤性蛛网膜下腔出血是一种危及生命的神经科急症，死亡率超过30%。从相关血肿类型进行迁移学习是一种有潜力但尚未充分探索的方法。虽然Unet架构因其在小数据集上的有效性仍然是医学图像分割的金标准，但LoRA方法在医学成像的卷积神经网络中很少被应用。&lt;h4&gt;目的&lt;/h4&gt;研究目的是探索迁移学习方法在动脉瘤性SAH图像分割中的应用，并评估LoRA及其变体方法与传统微调策略的性能比较。&lt;h4&gt;方法&lt;/h4&gt;研究团队实现了基于Unet的架构，该架构在来自多个机构的124名创伤性脑损伤患者的CT扫描上进行了预训练，然后在密歇根大学健康系统的30名动脉瘤性SAH患者上使用3折交叉验证进行了微调。研究团队开发了基于张量CP分解的CP-LoRA方法，并引入了将权重矩阵分解为幅度和方向分量的DoRA变体(DoRA-C, convDoRA, CP-DoRA)。研究团队将这些方法与现有的LoRA方法(LoRA-C, convLoRA)和在不同模块上的标准微调策略进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA方法持续优于标准Unet微调。性能因出血量而异，所有方法在大体积出血中显示出更高的准确性。CP-LoRA在性能与现有方法相当的情况下使用了显著更少的参数。更高秩的过度参数化比严格的低秩适应持续产生更好的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究表明血肿类型之间的迁移学习是可行的，并且LoRA方法在动脉瘤性SAH分割中显著优于传统的Unet微调。&lt;h4&gt;翻译&lt;/h4&gt;动脉瘤性蛛网膜下腔出血是一种危及生命的神经科急症，死亡率超过30%。从相关血肿类型进行迁移学习是一种有潜力但尚未充分探索的方法。虽然Unet架构因其在小数据集上的有效性仍然是医学图像分割的金标准，但LoRA方法在医学成像的卷积神经网络中很少被应用。研究团队实现了基于Unet的架构，该架构在来自多个机构的124名创伤性脑损伤患者的CT扫描上进行了预训练，然后在密歇根大学健康系统的30名动脉瘤性SAH患者上使用3折交叉验证进行了微调。研究团队开发了基于张量CP分解的CP-LoRA方法，并引入了将权重矩阵分解为幅度和方向分量的DoRA变体(DoRA-C, convDoRA, CP-DoRA)。研究团队将这些方法与现有的LoRA方法(LoRA-C, convLoRA)和在不同模块上的标准微调策略进行了比较。LoRA方法持续优于标准Unet微调。性能因出血量而异，所有方法在大体积出血中显示出更高的准确性。CP-LoRA在性能与现有方法相当的情况下使用了显著更少的参数。更高秩的过度参数化比严格的低秩适应持续产生更好的性能。该研究表明血肿类型之间的迁移学习是可行的，并且LoRA方法在动脉瘤性SAH分割中显著优于传统的Unet微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurologicalemergency with mortality rates exceeding 30%. Transfer learning from relatedhematoma types represents a potentially valuable but underexplored approach.Although Unet architectures remain the gold standard for medical imagesegmentation due to their effectiveness on limited datasets, Low-RankAdaptation (LoRA) methods for parameter-efficient transfer learning have beenrarely applied to convolutional neural networks in medical imaging contexts. Weimplemented a Unet architecture pre-trained on computed tomography scans from124 traumatic brain injury patients across multiple institutions, thenfine-tuned on 30 aneurysmal SAH patients from the University of Michigan HealthSystem using 3-fold cross-validation. We developed a novel CP-LoRA method basedon tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA,CP-DoRA) that decompose weight matrices into magnitude and directionalcomponents. We compared these approaches against existing LoRA methods (LoRA-C,convLoRA) and standard fine-tuning strategies across different modules on amulti-view Unet model. LoRA-based methods consistently outperformed standardUnet fine-tuning. Performance varied by hemorrhage volume, with all methodsshowing improved accuracy for larger volumes. CP-LoRA achieved comparableperformance to existing methods while using significantly fewer parameters.Over-parameterization with higher ranks consistently yielded better performancethan strictly low-rank adaptations. This study demonstrates that transferlearning between hematoma types is feasible and that LoRA-based methodssignificantly outperform conventional Unet fine-tuning for aneurysmal SAHsegmentation.</description>
      <author>example@mail.com (Cristian Minoccheri, Matthew Hodgman, Haoyuan Ma, Rameez Merchant, Emily Wittrup, Craig Williamson, Kayvan Najarian)</author>
      <guid isPermaLink="false">2508.01772v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Classification of Brain Tumors using Hybrid Deep Learning Models</title>
      <link>http://arxiv.org/abs/2508.01350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究应用迁移学习使用较少训练样本实现强分类性能，比较了EfficientNetV2与EfficientNet、ResNet50在脑肿瘤分类中的表现。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络(CNNs)已大大改善医学图像解释，但传统CNN通常需要大量计算资源和大型训练数据集。&lt;h4&gt;目的&lt;/h4&gt;解决传统CNN对计算资源和训练数据的高需求限制，通过迁移学习实现使用较少训练样本的强分类性能。&lt;h4&gt;方法&lt;/h4&gt;比较EfficientNetV2与其前代EfficientNet以及ResNet50在将脑肿瘤分为胶质瘤、脑膜瘤和垂体瘤三种类型的分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;EfficientNetV2与其他模型相比提供了更好的分类性能。&lt;h4&gt;结论&lt;/h4&gt;EfficientNetV2的性能提升是以训练时间增加为代价的，这可能是由于模型复杂度更高所致。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络(CNNs)的使用极大地改善了医学图像的解释。然而，传统的CNN通常需要大量的计算资源和大型训练数据集。为了解决这些限制，本研究应用迁移学习，使用较少的训练样本实现强分类性能。具体而言，研究比较了EfficientNetV2与其前代EfficientNet以及ResNet50在将脑肿瘤分为三种类型(胶质瘤、脑膜瘤和垂体瘤)方面的分类能力。结果表明，与其他模型相比，EfficientNetV2提供了更优的性能。然而，这种改进是以训练时间增加为代价的，可能是由于模型复杂度更高所致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of Convolutional Neural Networks (CNNs) has greatly improved theinterpretation of medical images. However, conventional CNNs typically demandextensive computational resources and large training datasets. To address theselimitations, this study applied transfer learning to achieve strongclassification performance using fewer training samples. Specifically, thestudy compared EfficientNetV2 with its predecessor, EfficientNet, and withResNet50 in classifying brain tumors into three types: glioma, meningioma, andpituitary tumors. Results showed that EfficientNetV2 delivered superiorperformance compared to the other models. However, this improvement came at thecost of increased training time, likely due to the model's greater complexity.</description>
      <author>example@mail.com (Neerav Nemchand Gala)</author>
      <guid isPermaLink="false">2508.01350v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Disparity Confidence Estimation into Relative Depth Prior-Guided Unsupervised Stereo Matching</title>
      <link>http://arxiv.org/abs/2508.01275v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的无监督立体匹配学习框架，通过立体置信度估计算法和深度先验引导的损失函数解决传统方法中的立体匹配歧义问题，在KITTI基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督立体匹配因不需要昂贵的视差标注而受到广泛关注。典型方法依赖多视图一致性假设，但在处理重复模式和纹理less区域等歧义问题时表现不佳。现有知识转移方法从随机稀疏对应关系学习深度排序，导致3D几何知识利用效率低下并引入噪声。&lt;h4&gt;目的&lt;/h4&gt;解决无监督立体匹配中的立体匹配歧义问题，提高3D几何知识利用效率，减少错误视差估计带来的噪声，提升立体匹配准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含三部分的无监督学习框架：1)即插即用的视差置信度估计算法，检查相邻视差与相对深度的局部相干一致性；2)使用高置信度视差估计构建准密集对应关系促进深度排序学习；3)双重视差平滑损失函数提高视差不连续处的匹配性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在所有无监督立体匹配方法中，于KITTI立体匹配基准测试上实现了最先进的立体匹配精度。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过有效利用3D几何知识并解决立体匹配中的歧义问题，显著提升了无监督立体匹配的性能，为该领域研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;无监督立体匹配因其独立于昂贵的视差标注而受到广泛关注。典型的无监督方法依赖于多视图一致性假设来训练网络，但这种方法在处理重复模式和纹理less区域等立体匹配歧义问题时表现不佳。一个可行的解决方案是将3D几何知识从相对深度图转移到立体匹配网络中。然而，现有的知识转移方法从随机构建的稀疏对应关系中学习深度排序信息，这使得3D几何知识的利用效率低下，并引入了错误视差估计带来的噪声。这项工作提出了一个新的无监督学习框架来解决这些挑战，该框架包括一个即插即用的视差置信度估计算法和两个深度先验引导的损失函数。具体来说，首先检查相邻视差及其对应相对深度之间的局部相干一致性以获得视差置信度。然后，仅使用高置信度的视差估计构建准密集对应关系，以促进高效的深度排序学习。最后，提出了双重视差平滑损失函数，以提高视差不连续处的立体匹配性能。实验结果表明，我们的方法在所有无监督立体匹配方法中，于KITTI立体匹配基准测试上实现了最先进的立体匹配精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决无监督立体匹配方法在存在立体匹配模糊性区域（如重复模式和纹理less区域）表现不佳的问题。这个问题很重要，因为立体匹配在3D几何重建、自动驾驶、增强现实等领域有广泛应用，而获取大规模真实世界视差标注数据非常困难，限制了监督学习的发展。无监督方法可以避免对标注数据的依赖，但面临立体匹配模糊性的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了无监督立体匹配在多视图一致性假设下的局限性，然后受到基于视觉基础模型的监督立体匹配方法的启发，考虑将单目深度模型的几何约束整合到无监督框架中。作者分析了现有知识转移方法的三大局限性：随机对应引入噪声、稀疏对应导致知识利用效率低、简单发散策略缺乏精确指导。作者借鉴了基于马尔可夫随机场的立体匹配方法中的局部相干性概念，以及SC-DepthV3中的置信深度排序损失，设计了选择性构建准密集对应关系的方法，仅使用可靠的视差估计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过视差-深度一致性投票算法估计视差置信度，使用高置信度的视差估计构建准密集对应关系，并设计两个基于相对深度先验的损失函数。整体流程是：1)使用ViTAStereo网络生成视差图和相对深度图；2)应用DDCV算法计算视差置信度；3)基于置信度选择高置信像素作为参考点；4)建立像素与参考点的对应关系；5)计算LDR损失惩罚不一致的视差对应；6)计算DDS损失增强视差与深度的一致性；7)结合传统损失和新的损失函数进行网络训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DDCV算法，第一个无监督的相对深度先导视差置信度估计方法；2)LDR损失，基于高置信视差构建准密集对应关系；3)DDS损失，增强视差与深度在平滑性和不连续性上的一致性。相比之前的工作不同之处在于：提高了知识转移效率，通过选择可靠视差抑制噪声，设计了更精确的损失函数，以及提出了无监督的视差置信度估计方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于视差置信度估计和相对深度先导损失函数的无监督立体匹配方法，显著提高了在立体匹配模糊性区域的准确性，并在KITTI立体匹配基准测试上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised stereo matching has garnered significant attention for itsindependence from costly disparity annotations. Typical unsupervised methodsrely on the multi-view consistency assumption for training networks, whichsuffer considerably from stereo matching ambiguities, such as repetitivepatterns and texture-less regions. A feasible solution lies in transferring 3Dgeometric knowledge from a relative depth map to the stereo matching networks.However, existing knowledge transfer methods learn depth ranking informationfrom randomly built sparse correspondences, which makes inefficient utilizationof 3D geometric knowledge and introduces noise from mistaken disparityestimates. This work proposes a novel unsupervised learning framework toaddress these challenges, which comprises a plug-and-play disparity confidenceestimation algorithm and two depth prior-guided loss functions. Specifically,the local coherence consistency between neighboring disparities and theircorresponding relative depths is first checked to obtain disparity confidence.Afterwards, quasi-dense correspondences are built using only confidentdisparity estimates to facilitate efficient depth ranking learning. Finally, adual disparity smoothness loss is proposed to boost stereo matching performanceat disparity discontinuities. Experimental results demonstrate that our methodachieves state-of-the-art stereo matching accuracy on the KITTI Stereobenchmarks among all unsupervised stereo matching methods.</description>
      <author>example@mail.com (Chuang-Wei Liu, Mingjian Sun, Cairong Zhao, Hanli Wang, Alexander Dvorkovich, Rui Fan)</author>
      <guid isPermaLink="false">2508.01275v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?</title>
      <link>http://arxiv.org/abs/2508.01216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to AAAI 2026. arXiv admin note: text overlap with  arXiv:2507.18881&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用视觉场景上下文增强视觉楼层平面定位的方法，通过无监督学习技术预训练房间判别器，提取房间类型信息，并将其注入到FLoc算法中以提高定位的鲁棒性和准确性。&lt;h4&gt;背景&lt;/h4&gt;建筑物的楼层平面图随时间保持一致且对视觉外观变化具有鲁棒性，因此视觉楼层平面定位受到研究者关注。然而，楼层平面图包含许多重复结构，容易导致定位模糊。&lt;h4&gt;目的&lt;/h4&gt;使用更广泛的视觉场景上下文增强FLoc算法，利用场景布局先验来消除定位不确定性。&lt;h4&gt;方法&lt;/h4&gt;提出带有聚类约束的无监督学习技术，在未标记房间图像上预训练房间判别器，提取隐藏房间类型并区分不同房间类型，将判别器总结的场景上下文信息注入到FLoc算法中。&lt;h4&gt;主要发现&lt;/h4&gt;在两个标准的视觉FLoc基准上进行的实验表明，该方法优于最先进的方法，在鲁棒性和准确性方面取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;通过注入场景上下文信息，有效利用了房间风格知识来指导明确的视觉楼层平面定位，提高了定位性能。&lt;h4&gt;翻译&lt;/h4&gt;由于建筑物的楼层平面图随时间保持一致且对视觉外观变化具有内在鲁棒性，视觉楼层平面定位已受到研究者的越来越多的关注。然而，作为建筑物布局的紧凑和极简表示，楼层平面图包含许多重复结构（例如走廊和角落），因此容易导致定位模糊。现有方法要么寄希望于匹配楼层平面图中的二维结构线索，要么依赖于3D几何约束的视觉预训练，忽略了视觉图像提供的更丰富的上下文信息。在本文中，我们建议使用更广泛的视觉场景上下文来增强FLoc算法，利用场景布局先验来消除定位不确定性。特别是，我们提出了一种带有聚类约束的无监督学习技术，在自行收集的未标记房间图像上预训练一个房间判别器。这种判别器可以经验性地提取观察图像的隐藏房间类型，并将其与其他房间类型区分开。通过将判别器总结的场景上下文信息注入到FLoc算法中，房间风格知识被有效利用来指导明确的视觉FLoc。我们在两个标准的视觉FLoc基准上进行了充分的比较研究。我们的实验表明，我们的方法优于最先进的方法，在鲁棒性和准确性方面取得了显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉平面定位（Visual Floorplan Localization, FLoc）中的歧义性问题。由于平面图作为建筑的紧凑表示包含许多重复结构（如走廊和角落），现有方法容易产生错误的定位结果。这个问题在现实中很重要，因为室内视觉定位广泛应用于3D重建、AR/VR和机器人导航等领域，而平面图是轻量级、易于获取且随时间保持一致的定位辅助工具，能有效解决室内场景缺乏卫星信号的问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到不同类型房间（如卧室、浴室）具有特定视觉风格，这些风格可作为上下文信息解决定位歧义。他们借鉴了F3Loc框架的架构（前端观察模型和后端贝叶斯滤波器），并设计了聚类约束的无监督学习技术来训练房间判别器。这种方法不需要人工标注的房间类别信息，而是利用导航任务的难度信息构建约束矩阵，指导模型学习房间的风格表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用房间风格知识解决平面图定位中的歧义性。整体流程包括：1) 自动收集室内场景RGB图像并过滤掉缺乏房间风格信息的图像；2) 使用聚类约束的无监督学习技术训练房间判别器，提取房间风格特征；3) 将预训练的房间风格编码器转移到视觉FLoc任务中；4) 在F3Loc框架基础上，将房间风格知识注入观察模型，指导定位过程。方法支持单帧、多帧和自适应三种定位模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次利用房间视觉风格解决平面图定位歧义；2) 提出聚类约束的无监督学习技术训练房间判别器；3) 设计自动化数据收集和处理流程。相比之前的工作，不同之处在于：不依赖2D结构线索或3D几何约束，而是提供场景级别的上下文信息；不要求人工标注的语义信息；能在视觉外观相似情况下也能提供准确定位，而不仅仅是依赖视觉外观显著变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种利用房间风格知识的视觉平面定位方法，通过无监督学习技术训练房间判别器将场景上下文信息注入定位算法，有效解决了由平面图中重复结构引起的定位歧义问题，显著提高了定位的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since a building's floorplan remains consistent over time and is inherentlyrobust to changes in visual appearance, visual Floorplan Localization (FLoc)has received increasing attention from researchers. However, as a compact andminimalist representation of the building's layout, floorplans contain manyrepetitive structures (e.g., hallways and corners), thus easily result inambiguous localization. Existing methods either pin their hopes on matching 2Dstructural cues in floorplans or rely on 3D geometry-constrained visualpre-trainings, ignoring the richer contextual information provided by visualimages. In this paper, we suggest using broader visual scene context to empowerFLoc algorithms with scene layout priors to eliminate localization uncertainty.In particular, we propose an unsupervised learning technique with clusteringconstraints to pre-train a room discriminator on self-collected unlabeled roomimages. Such a discriminator can empirically extract the hidden room type ofthe observed image and distinguish it from other room types. By injecting thescene context information summarized by the discriminator into an FLocalgorithm, the room style knowledge is effectively exploited to guide definitevisual FLoc. We conducted sufficient comparative studies on two standard visualFloc benchmarks. Our experiments show that our approach outperformsstate-of-the-art methods and achieves significant improvements in robustnessand accuracy.</description>
      <author>example@mail.com (Bolei Chen, Shengsheng Yan, Yongzheng Cui, Jiaxu Kang, Ping Zhong, Jianxin Wang)</author>
      <guid isPermaLink="false">2508.01216v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning physics-informed neural networks for cavity flows using coordinate transformation</title>
      <link>http://arxiv.org/abs/2508.01122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于坐标变换的物理信息神经网络微调方法，用于解决不同形状腔驱动流动问题，通过预训练模型和变形梯度张量提高训练效率。&lt;h4&gt;背景&lt;/h4&gt;物理信息神经网络(PINNs)作为求解偏微分方程的替代方法受到关注，但其训练成本高。使用迁移学习或微调预训练模型是潜在解决方案，但预训练模型与目标几何形状和流动条件不匹配时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种微调方法，使预训练的PINNs能够适应不同几何形状的腔驱动流动问题，提高训练效率并减少训练成本。&lt;h4&gt;方法&lt;/h4&gt;提出结合坐标变换的PINNs微调方法：公式化逆问题给定域内参考数据和壁面边界条件；使用任意雷诺数和形状预训练的PINN初始化目标DNN；通过变形梯度张量进行坐标变换，将控制方程作为PINNs损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;数值实验表明，对于方形、矩形和剪切变形几何形状的各种腔流动，所提方法比未训练模型提高了训练收敛性；使用与目标几何形状相似的模型预训练可进一步提高训练效率。&lt;h4&gt;结论&lt;/h4&gt;基于坐标变换的PINNs微调方法有效解决了不同几何形状腔驱动流动问题，提高了训练效率，对颅内动脉瘤血流建模等临床应用具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;物理信息神经网络(PINNs)作为一种使用深度神经网络(DNN)求解偏微分方程的替代方法已受到关注。它们的简单性和能力使它们能够解决许多应用的逆问题。尽管PINNs具有多功能性，但降低其训练成本仍然具有挑战性。使用通过迁移学习或微调使用任意数据集预训练的DNN是一种潜在的解决方案。然而，使用与目标不同几何形状和流动条件预训练的模型可能不会产生合适的结果。本文提出了一种结合坐标变换的PINNs微调方法，用于模拟各种形状的腔驱动流动。我们将逆问题公式化，给定域内的参考数据和壁面边界条件。使用任意雷诺数和形状预训练的PINN模型来初始化目标DNN。为了使参考形状与不同目标相协调，使用变形梯度张量进行坐标变换，并将控制方程作为PINNs的损失函数。针对方形、矩形和剪切变形几何形状的各种腔流动的数值示例表明，与未训练模型相比，所提出的微调方法提高了训练收敛性。使用与目标几何形状相似的模型预训练可以进一步提高训练效率。这些发现对于临床应用中的颅内动脉瘤血流建模等实际应用是有用的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physics-informed neural networks (PINNs) have attracted attention as analternative approach to solve partial differential equations using a deepneural network (DNN). Their simplicity and capability allow them to solveinverse problems for many applications. Despite the versatility of PINNs, itremains challenging to reduce their training cost. Using a DNN pre-trained withan arbitrary dataset with transfer learning or fine-tuning is a potentialsolution. However, a pre-trained model using a different geometry and flowcondition than the target may not produce suitable results. This paper proposesa fine-tuning approach for PINNs with coordinate transformation, modellinglid-driven cavity flows with various shapes. We formulate the inverse problem,where the reference data inside the domain and wall boundary conditions aregiven. A pre-trained PINN model with an arbitrary Reynolds number and shape isused to initialize a target DNN. To reconcile the reference shape withdifferent targets, governing equations as a loss of the PINNs are given withcoordinate transformation using a deformation gradient tensor. Numericalexamples for various cavity flows with square, rectangular, and shear deformedgeometries demonstrate that the proposed fine-tuning approach improves thetraining convergence compared with an un-trained model. A pre-trained modelwith a similar geometry to the target further increases training efficiency.These findings are useful for real-world applications such as modellingintra-aneurysmal blood flows in clinical use.</description>
      <author>example@mail.com (Ryuta Takao, Satoshi Ii)</author>
      <guid isPermaLink="false">2508.01122v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables</title>
      <link>http://arxiv.org/abs/2508.00959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究改进了物理引导神经网络与内部变量（PGNNIV）框架，通过降阶建模技术解决其在高维数据应用中的可扩展性挑战，并引入了模型重用策略以提高效率。&lt;h4&gt;背景&lt;/h4&gt;物理引导神经网络与内部变量（PGNNIV）是一种仅使用可观测数据进行训练的科学机器学习工具，能够揭示内部状态关系。然而，当应用于高维数据（如细网格空间场或时间演化系统）时，这些模型面临可扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;提出对PGNNIV框架的增强方法，解决其在高维数据应用中的可扩展性限制，提高计算效率、准确性、噪声容忍度和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过降阶建模技术改进PGNNIV框架，引入使用谱分解、POD和预训练自编码器映射的替代原始解码器结构；集成模型重用策略，包括迁移学习和微调技术，以利用先前获得的知识。&lt;h4&gt;主要发现&lt;/h4&gt;增强的PGNNIV框架成功识别了潜在的构成状态方程，同时保持了高预测准确性；提高了对噪声的鲁棒性，减轻了过拟合，并降低了计算需求；模型重用策略显著减少了训练时间，同时保持或提高了模型性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的技术可以根据数据可用性、资源和特定建模目标进行调整，克服各种场景中的可扩展性挑战，使PGNNIV能够更有效地应用于高维数据。&lt;h4&gt;翻译&lt;/h4&gt;物理引导神经网络与内部变量是科学机器学习工具，它们仅使用可观测数据进行训练，并有能力揭示内部状态关系。它们通过规定模型架构和使用损失正则化来整合物理知识，从而使某些特定神经元具有作为内部状态变量的物理意义。尽管有这些潜力，但当这些模型应用于高维数据（如细网格空间场或时间演化系统）时，它们面临可扩展性挑战。在这项工作中，我们提出了一些对PGNNIV框架的增强，通过降阶建模技术解决这些可扩展性限制。具体来说，我们引入了使用谱分解、POD和预训练自编码器映射的替代原始解码器结构。这些替代解码器在计算效率、准确性、噪声容忍度和泛化能力之间提供了不同的权衡，同时显著提高了可扩展性。此外，我们通过迁移学习和微调策略集成模型重用，利用先前获得的知识，支持对新材料或配置的高效适应，同时显著减少训练时间，同时保持或提高模型性能。为了说明这些技术，我们使用了一个由非线性扩散方程控制的代表性案例，仅使用可观测数据。结果表明，增强的PGNNIV框架成功识别了潜在的构成状态方程，同时保持了高预测准确性。它还提高了对噪声的鲁棒性，减轻了过拟合，并降低了计算需求。所提出的技术可以根据数据可用性、资源和特定建模目标进行调整，克服所有场景中的可扩展性挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physically Guided Neural Networks with Internal Variables are SciML toolsthat use only observable data for training and and have the capacity to unravelinternal state relations. They incorporate physical knowledge both byprescribing the model architecture and using loss regularization, thus endowingcertain specific neurons with a physical meaning as internal state variables.Despite their potential, these models face challenges in scalability whenapplied to high-dimensional data such as fine-grid spatial fields ortime-evolving systems. In this work, we propose some enhancements to the PGNNIVframework that address these scalability limitations through reduced-ordermodeling techniques. Specifically, we introduce alternatives to the originaldecoder structure using spectral decomposition, POD, and pretrainedautoencoder-based mappings. These surrogate decoders offer varying trade-offsbetween computational efficiency, accuracy, noise tolerance, andgeneralization, while improving drastically the scalability. Additionally, weintegrate model reuse via transfer learning and fine-tuning strategies toexploit previously acquired knowledge, supporting efficient adaptation to novelmaterials or configurations, and significantly reducing training time whilemaintaining or improving model performance. To illustrate these varioustechniques, we use a representative case governed by the nonlinear diffusionequation, using only observable data. Results demonstrate that the enhancedPGNNIV framework successfully identifies the underlying constitutive stateequations while maintaining high predictive accuracy. It also improvesrobustness to noise, mitigates overfitting, and reduces computational demands.The proposed techniques can be tailored to various scenarios depending on dataavailability, resources, and specific modeling objectives, overcomingscalability challenges in all the scenarios.</description>
      <author>example@mail.com (Rubén Muñoz-Sierra, Manuel Doblaré, Jacobo Ayensa-Jiménez)</author>
      <guid isPermaLink="false">2508.00959v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.02288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅基于事件摄像头的立体3D物体检测框架，解决了传统传感器在高速场景下感知间隔的问题，通过双滤波机制和边界框对齐方法提高了检测性能。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测对自主系统至关重要，可精确定位和估计物体尺寸。传统LiDAR和RGB摄像头因固定帧率在高速场景下会产生感知间隔。事件摄像头具有异步特性和高时间分辨率，能连续捕捉运动。但现有结合事件摄像头与传统传感器的方法依赖同步传感器，在快速运动场景下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅依赖事件摄像头的立体3D物体检测框架，消除对传统3D传感器的需求，解决高速场景下的感知间隔问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种纯事件摄像头驱动的立体3D物体检测框架；引入双滤波机制提取事件数据中的语义和几何信息；通过将边界框与以物体为中心的信息对齐来增强回归效果。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在动态环境中优于先前的方法；验证了事件摄像头在鲁棒、连续时间3D感知方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;事件摄像头可作为传统传感器的有效替代，特别是在高速场景下；所提出的方法为基于事件的3D物体检测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D物体检测对自主系统至关重要，能够实现精确定位和尺寸估计。虽然LiDAR和RGB摄像头被广泛使用，但它们的固定帧率在高速场景下会产生感知间隔。事件摄像头以其异步特性和高时间分辨率，通过连续捕捉运动提供了解决方案。最近将事件摄像头与传统传感器集成用于连续时间检测的方法，由于其依赖于同步传感器，在快速运动场景下表现不佳。我们提出了一种新颖的立体3D物体检测框架，仅依赖事件摄像头，消除了对传统3D传感器的需求。为了弥补事件数据中语义和几何信息的不足，我们引入了双滤波机制来提取这两类信息。此外，我们通过将边界框与以物体为中心的信息对齐来增强回归效果。实验表明，我们的方法在动态环境中优于先前的方法，展示了事件摄像头在鲁棒、连续时间3D感知方面的潜力。代码可在https://github.com/mickeykang16/Ev-Stereo3D获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是在高速动态场景下3D物体检测的感知延迟问题。传统传感器如LiDAR和RGB相机由于固定帧率限制，在高动态场景下会产生感知间隙，导致检测延迟。这一问题在自动驾驶系统中尤为重要，因为在高速场景下，即使是微小的检测延迟也可能导致严重的安全事故。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有连续时间3D物体检测方法（如Ev-3DOD）的局限性，即它们仍依赖于同步传感器（LiDAR和RGB相机），在快速运动场景下表现不佳。因此，作者提出仅使用事件相机来实现3D物体检测，避免对同步传感器的依赖。方法设计上借鉴了PSMNet的特征提取器设计，参考了Ev-3DOD的连续时间检测框架，并利用了立体视觉和Transformer机制来增强特征对齐。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是仅使用异步事件相机实现连续时间的3D物体检测，通过双重语义-几何滤波机制增强事件数据的语义和几何信息，并利用以物体为中心的ROI对齐来提高回归性能。整体流程包括：1)几何平面扫描体积构建深度估计；2)双重语义-几何滤波器增强语义和几何信息；3)全局3D检测器预测3D边界框；4)以物体为中心的ROI对齐细化边界框预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)第一个完全异步的3D物体检测系统，完全基于事件相机；2)双重语义-几何滤波机制协同增强信息；3)以物体为中心的ROI对齐提高回归准确性；4)连续时间3D检测框架。相比之前的工作（如Ev-3DOD），本文方法无需同步传感器，在高速动态场景下表现更好，时间分辨率更高，且有专门的语义-几何交互机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种仅使用立体事件相机的完全异步3D物体检测方法，通过双重语义-几何滤波和以物体为中心的ROI对齐机制，实现了在高速动态场景下连续时间的精确3D物体检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is essential for autonomous systems, enabling preciselocalization and dimension estimation. While LiDAR and RGB cameras are widelyused, their fixed frame rates create perception gaps in high-speed scenarios.Event cameras, with their asynchronous nature and high temporal resolution,offer a solution by capturing motion continuously. The recent approach, whichintegrates event cameras with conventional sensors for continuous-timedetection, struggles in fast-motion scenarios due to its dependency onsynchronized sensors. We propose a novel stereo 3D object detection frameworkthat relies solely on event cameras, eliminating the need for conventional 3Dsensors. To compensate for the lack of semantic and geometric information inevent data, we introduce a dual filter mechanism that extracts both.Additionally, we enhance regression by aligning bounding boxes withobject-centric information. Experiments show that our method outperforms priorapproaches in dynamic environments, demonstrating the potential of eventcameras for robust, continuous-time 3D perception. The code is available athttps://github.com/mickeykang16/Ev-Stereo3D.</description>
      <author>example@mail.com (Jae-Young Kang, Hoonhee Cho, Kuk-Jin Yoon)</author>
      <guid isPermaLink="false">2508.02288v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive LiDAR Scanning: Harnessing Temporal Cues for Efficient 3D Object Detection via Multi-Modal Fusion</title>
      <link>http://arxiv.org/abs/2508.01562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种预测性历史感知的自适应扫描框架，通过智能选择LiDAR扫描区域，显著降低能耗同时保持3D目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;传统LiDAR传感器进行密集、无状态扫描，忽略了现实场景中的时间连续性，导致感知冗余和高能耗，限制了在资源受限平台上的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少LiDAR传感器的能耗，同时保持或提高3D目标检测的性能，使其更适合资源受限平台。&lt;h4&gt;方法&lt;/h4&gt;提出一种包含轻量级预测器网络和可微分掩码生成器网络的框架，预测器网络将历史时空上下文转化为查询嵌入，掩码生成器网络利用Gumbel-Softmax采样生成二进制掩码，识别下一帧的关键ROI，实现自适应扫描。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Lyft基准上的实验表明，该方法将LiDAR能耗降低了65%以上，同时与传统密集LiDAR扫描方法相比，保持了具有竞争力甚至更优的3D目标检测性能。&lt;h4&gt;结论&lt;/h4&gt;通过预测性历史感知的自适应扫描策略，可以在显著降低能耗的同时保持甚至提高3D目标检测性能，为资源受限平台上的多传感器融合应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用LiDAR和RGB相机的多传感器融合显著增强了3D目标检测任务。然而，传统的LiDAR传感器执行密集、无状态的扫描，忽略了现实场景中的强时间连续性。这导致了大量的感知冗余和过高的能耗，限制了它们在资源受限平台上的实用性。为了解决这种低效率问题，我们提出了一种预测性的、历史感知的自适应扫描框架，基于过去的观察来预测感兴趣区域。我们的方法引入了一个轻量级的预测器网络，将历史空间和时间上下文提炼为精细的查询嵌入。这些嵌入指导一个可微分的掩码生成器网络，利用Gumbel-Softmax采样生成二进制掩码，识别下一帧的关键ROI。我们的方法通过仅在这些ROI内进行密集LiDAR扫描，其他地方稀疏采样，显著减少了不必要的数据采集。在nuScenes和Lyft基准上的实验表明，我们的自适应扫描策略将LiDAR能耗降低了65%以上，同时与传统使用密集LiDAR扫描的LiDAR-相机融合方法相比，保持了具有竞争力甚至更优的3D目标检测性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统激光雷达(LiDAR)传感器在3D物体检测中的效率低下问题。传统LiDAR执行密集、无状态的扫描，忽略了场景中的时序连续性，导致大量感知冗余和过度能源消耗。这个问题很重要，因为LiDAR作为主动传感器能耗远高于相机(10-100W vs 1-5W)，高能耗限制了LiDAR在资源受限平台上的实际应用，增加了自动驾驶系统的运行成本和可持续性挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到传统LiDAR扫描是'无状态'的，每次帧都执行均匀角度密集扫描，忽略了真实环境中世界不会每十分之一秒就完全重新排列的事实。基于这一观察，作者设计了预测性、历史感知的自适应扫描框架，利用过去帧预测下一帧的感兴趣区域(ROI)。该方法借鉴了现有的多模态融合技术、QTNet中的运动引导时序模块(MTM)进行查询预测，以及Gumbel-Softmax采样技术生成二进制掩码，同时改进了传统体素化方法使其可微分以支持端到端训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用历史帧信息预测下一帧中的感兴趣区域(ROI)，只在ROI区域进行密集LiDAR扫描，而在非ROI区域进行稀疏扫描，从而减少不必要的数据采集，降低能耗。整体流程包括：1)历史感知查询预测模块使用过去帧信息预测当前帧的物体位置；2)可微分掩码生成器将预测转换为扫描策略；3)自适应执行扫描，ROI区域密集扫描，非ROI区域稀疏采样；4)可微分体素化处理点云数据；5)结合多种损失函数进行模型训练，包括3D检测损失、蒸馏损失、掩码损失和条件风险价值(CVaR)损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)历史驱动的自适应LiDAR范式，从无记忆转向利用过去预测未来ROI；2)两阶段架构结合历史感知查询预测和可微分掩码生成；3)可微分体素化层实现端到端训练；4)条件风险价值(CVaR)损失提高对小物体的检测鲁棒性。相比之前工作，不同之处在于：传统多模态融合方法假设每帧都有密集扫描，不利用时序信息；现有自适应推理主要关注计算资源而非感知资源；传统点云下采样是后处理技术，而本文在捕获前控制扫描密度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种利用历史信息预测感兴趣区域的自适应LiDAR扫描方法，能够在减少65%以上LiDAR能耗的同时，保持或提升3D物体检测性能，为自动驾驶系统提供了一种更高效、更可持续的感知方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-sensor fusion using LiDAR and RGB cameras significantly enhances 3Dobject detection task. However, conventional LiDAR sensors perform dense,stateless scans, ignoring the strong temporal continuity in real-world scenes.This leads to substantial sensing redundancy and excessive power consumption,limiting their practicality on resource-constrained platforms. To address thisinefficiency, we propose a predictive, history-aware adaptive scanningframework that anticipates informative regions of interest (ROI) based on pastobservations. Our approach introduces a lightweight predictor network thatdistills historical spatial and temporal contexts into refined queryembeddings. These embeddings guide a differentiable Mask Generator network,which leverages Gumbel-Softmax sampling to produce binary masks identifyingcritical ROIs for the upcoming frame. Our method significantly reducesunnecessary data acquisition by concentrating dense LiDAR scanning only withinthese ROIs and sparsely sampling elsewhere. Experiments on nuScenes and Lyftbenchmarks demonstrate that our adaptive scanning strategy reduces LiDAR energyconsumption by over 65% while maintaining competitive or even superior 3Dobject detection performance compared to traditional LiDAR-camera fusionmethods with dense LiDAR scanning.</description>
      <author>example@mail.com (Sara Shoouri, Morteza Tavakoli Taba, Hun-Seok Kim)</author>
      <guid isPermaLink="false">2508.01562v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy</title>
      <link>http://arxiv.org/abs/2507.21358v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The manuscript has been accepted by ICONIP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为协作感知器(CoP)的多任务学习框架，通过整合空间占用信息作为辅助任务，显著提升了基于视觉的鸟瞰图3D物体检测性能，在nuScenes测试集上实现了49.5%的mAP和59.2%的NDS的优异成绩。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图(BEV)3D物体检测在自动驾驶领域取得了显著进展，具有成本效益和丰富的上下文信息。然而，现有方法通常通过折叠提取的物体特征来构建BEV表示，忽略了内在的环境上下文（如道路和人行道），这阻碍了检测器全面感知物理世界特征的能力。&lt;h4&gt;目的&lt;/h4&gt;引入一个多任务学习框架，利用空间占用率作为辅助信息，挖掘3D物体检测和占用预测任务之间一致的结构和概念相似性，弥补空间表示和特征细化方面的差距。&lt;h4&gt;方法&lt;/h4&gt;提出一个管道生成密集占用率真实值（包含局部密度信息LDO）；采用体素高度引导的采样(VHS)策略根据不同物体特性提炼细粒度局部特征；开发全局-局部协作特征融合(CFF)模块无缝整合两个任务间的互补知识，构建更强大的BEV表示。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准测试上进行了大量实验，CoP优于现有基于视觉的框架，在测试集上达到49.5%的mAP和59.2%的NDS。&lt;h4&gt;结论&lt;/h4&gt;协作感知器(CoP)框架有效提升了基于视觉的BEV 3D物体检测性能，通过整合空间占用信息作为辅助任务，能够更好地理解环境上下文，从而构建更准确的BEV表示。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图(BEV)3D物体检测在自动驾驶领域已取得显著进展，提供了成本效益和丰富的上下文信息。然而，现有方法通常通过折叠提取的物体特征来构建BEV表示，忽略了内在的环境上下文，如道路和人行道。这阻碍了检测器全面感知物理世界特征的能力。为缓解这一问题，我们引入了一个多任务学习框架——协作感知器(CoP)，它利用空间占用率作为辅助信息，挖掘3D物体检测和占用预测任务之间一致的结构和概念相似性，弥补了空间表示和特征细化方面的差距。为此，我们首先提出一个管道来生成包含局部密度信息(LDO)的密集占用率真实值，用于重建详细的环境信息。接着，我们采用体素高度引导的采样(VHS)策略，根据不同的物体特性提炼细粒度的局部特征。此外，我们还开发了一个全局-局部协作特征融合(CFF)模块，无缝整合两个任务之间的互补知识，从而构建更强大的BEV表示。在nuScenes基准测试上的大量实验表明，CoP优于现有的基于视觉的框架，在测试集上实现了49.5%的mAP和59.2%的NDS。代码和补充材料可在以下链接获取：https://github.com/jichengyuan/Collaborative-Perceiver。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于鸟瞰图(BEV)的3D物体检测方法中忽略环境内在上下文信息的问题。现有方法通过压缩多视图特征构建BEV表示，无法全面感知物理世界特征，特别是在识别具有独特或不规则几何形状的物体时表现不佳。这个问题在自动驾驶领域至关重要，因为准确感知周围环境对确保安全至关重要，而现有方法难以处理复杂交通场景，无法满足自动驾驶的安全需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过引入多任务学习框架Collaborative Perceiver (CoP)，利用空间占用作为辅助信息来挖掘3D物体检测和占用预测任务之间的一致结构和概念相似性。作者借鉴了现有BEV-based方法如BEVDet的Lift-Splat-Shoot视图变换，以及3D占用预测方法如SurroundOcc的概念，但改进了它们在处理点云密度不均匀问题上的不足。同时借鉴了FlashOcc的通道到高度(C2H)模块，用于特征转换。作者设计了一个包含局部密度信息的密集占用生成管道、体素高度引导的采样策略和全局-局部协作特征融合模块的创新组合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务学习框架将3D物体检测和3D占用预测任务结合起来，利用两个任务之间的互补和一致知识，结合局部密度感知的空间占用信息作为辅助监督，帮助模型更好地理解环境结构。整体流程包括：1)输入多摄像头图像并提取特征；2)使用Lift-Splat-Shoot将2D特征转换为3D体素特征；3)采用体素高度引导的采样提取细粒度局部特征；4)使用全局池化提取高度无关的全局特征；5)通过全局-局部协作特征融合模块整合特征；6)将统一BEV特征转换回体素特征；7)分别执行3D检测和占用预测；8)使用局部密度矩阵优化多任务学习目标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)局部密度感知的空间占用生成(LDO)，考虑点云非均匀分布为每个体素分配局部密度因子；2)体素高度引导的采样(VHS)策略，根据占用体素高度分布进行分层采样；3)全局-局部协作特征融合(CFF)模块，通过自适应参数融合全局和局部特征。相比之前工作，CoP保留了传统方法忽略的不同高度范围内的细粒度信息，考虑了局部密度作为表示细粒度物体结构的关键因素，并通过多任务学习整合了两个任务的互补知识，而不是仅依赖任务特定的局部或全局信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Collaborative Perceiver通过引入局部密度感知的空间占用作为辅助信息，结合体素高度引导的采样和全局-局部协作特征融合，有效提升了基于视觉的3D物体检测性能，特别是在复杂交通场景中识别具有不规则几何形状的物体方面取得了显著进步。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based bird's-eye-view (BEV) 3D object detection has advancedsignificantly in autonomous driving by offering cost-effectiveness and richcontextual information. However, existing methods often construct BEVrepresentations by collapsing extracted object features, neglecting intrinsicenvironmental contexts, such as roads and pavements. This hinders detectorsfrom comprehensively perceiving the characteristics of the physical world. Toalleviate this, we introduce a multi-task learning framework, CollaborativePerceiver (CoP), that leverages spatial occupancy as auxiliary information tomine consistent structural and conceptual similarities shared between 3D objectdetection and occupancy prediction tasks, bridging gaps in spatialrepresentations and feature refinement. To this end, we first propose apipeline to generate dense occupancy ground truths incorporating local densityinformation (LDO) for reconstructing detailed environmental information. Next,we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grainedlocal features according to distinct object properties. Furthermore, we developa global-local collaborative feature fusion (CFF) module that seamlesslyintegrates complementary knowledge between both tasks, thus composing morerobust BEV representations. Extensive experiments on the nuScenes benchmarkdemonstrate that CoP outperforms existing vision-based frameworks, achieving49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials areavailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.</description>
      <author>example@mail.com (Jicheng Yuan, Manh Nguyen Duc, Qian Liu, Manfred Hauswirth, Danh Le Phuoc)</author>
      <guid isPermaLink="false">2507.21358v4</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A survey on proximity monitoring and warning in construction</title>
      <link>http://arxiv.org/abs/2508.00862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究全面回顾了2010-2024年间发表的97篇相关文章，对接近度监测和预警(PMW)进行了文献计量分析和定性审查，指出了当前研究的局限性和未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;各种技术被应用于监测两个施工实体之间的距离，防止撞击事故，从而提高现场安全性。&lt;h4&gt;目的&lt;/h4&gt;全面回顾接近度监测和预警(PMW)的相关研究工作，呈现当前研究状况、局限性和未来方向。&lt;h4&gt;方法&lt;/h4&gt;通过文献计量分析揭示技术路线图和研究网络，从四个角度进行定性审查：影响因素研究、危险等级定义和确定、接近度感知、警报发布和接收。&lt;h4&gt;主要发现&lt;/h4&gt;当前接近度感知存在局限性和挑战，未来研究方向包括端到端的三维物体检测、动态施工场景的实时三维重建和更新、多模态融合。&lt;h4&gt;结论&lt;/h4&gt;本综述呈现了PMW的当前研究状况、局限性和未来方向，指导PMW系统的未来发展。&lt;h4&gt;翻译&lt;/h4&gt;Various technologies have been applied to monitor the proximity between two construction entities, preventing struck-by accidents and thereby enhancing onsite safety. This study comprehensively reviews related efforts dedicated to proximity monitoring and warning (PMW) based on 97 relevant articles published between 2010 and 2024. The bibliometric analysis reveals the technical roadmap over time, as well as the five most influential leaders and the two largest research networks they have established. The qualitative review is then conducted from four perspectives: influencing factor study, hazard level definition and determination, proximity perception, and alarm issuing and receiving. Finally, the limitations and challenges of current proximity perception are discussed, along with corresponding future research directions, including end-to-end three-dimensional (3D) object detection, real-time 3D reconstruction and updating for dynamic construction scenes, and multimodal fusion. This review presents the current research status, limitations, and future directions of PMW, guiding the future development of PMW systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Various technologies have been applied to monitor the proximity between twoconstruction entities, preventing struck-by accidents and thereby enhancingonsite safety. This study comprehensively reviews related efforts dedicated toproximity monitoring and warning (PMW) based on 97 relevant articles publishedbetween 2010 and 2024. The bibliometric analysis reveals the technical roadmapover time, as well as the five most influential leaders and the two largestresearch networks they have established. The qualitative review is thenconducted from four perspectives: influencing factor study, hazard leveldefinition and determination, proximity perception, and alarm issuing andreceiving. Finally, the limitations and challenges of current proximityperception are discussed, along with corresponding future research directions,including end-to-end three-dimensional (3D) object detection, real-time 3Dreconstruction and updating for dynamic construction scenes, and multimodalfusion. This review presents the current research status, limitations, andfuture directions of PMW, guiding the future development of PMW systems.</description>
      <author>example@mail.com (Yuexiong Ding, Qiong Liu, Ankang Ji, Xiaowei Luo, Wen Yi, Albert P. C. Chan)</author>
      <guid isPermaLink="false">2508.00862v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Distinct inhibitory connectivity motifs trigger distinct forms of anticipation in the retinal network</title>
      <link>http://arxiv.org/abs/2508.02436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Figures, 28 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了视网膜中两种不同的抑制性连接模式（前馈抑制和循环反馈抑制）如何影响视网膜对移动物体的响应，特别是运动预测机制及其速度依赖性差异。&lt;h4&gt;背景&lt;/h4&gt;运动是视觉场景的重要特征，视网膜神经元回路会选择性地传递不同的运动特征。视网膜可以外推移动物体的位置，从而补偿感觉传导延迟，实现实时信号处理。无长突细胞作为视网膜的抑制性中间神经元在这种计算中发挥重要作用，但其精确功能尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;探索两种不同的抑制性连接模式（前馈抑制和循环反馈抑制）对视网膜响应移动物体的影响，以及它们如何实现运动预测。&lt;h4&gt;方法&lt;/h4&gt;通过计算建模方法研究前馈抑制和循环反馈抑制对视网膜运动响应的影响。&lt;h4&gt;主要发现&lt;/h4&gt;两种机制都可以实现运动预测，但机制不同。前馈抑制通过减法抑制截断运动响应并将峰值响应前移；循环反馈耦合通过除法抑制引起不同相位的兴奋性和抑制性波，它们相加并使响应峰值偏移。两种机制的主要区别在于峰值响应如何随移动物体速度变化。使用前馈电路的运动预测随速度增加而单调下降，而循环反馈耦合则诱导出具有最优速度的调谐曲线，在该速度下运动预测最大。&lt;h4&gt;结论&lt;/h4&gt;视网膜中存在多种机制可以实现运动预测，前馈抑制和循环反馈抑制是两种不同的机制，它们在速度依赖性方面表现出显著差异。&lt;h4&gt;翻译&lt;/h4&gt;运动是视觉场景的重要特征，视网膜神经元回路会选择性地传递不同的运动特征。研究表明，视网膜可以外推移动物体的位置，从而补偿感觉传导延迟，实现实时信号处理。无长突细胞作为视网膜的抑制性中间神经元，在这种计算中发挥重要作用，但其精确功能仍不清楚。本文通过计算探索了两种不同的抑制性连接模式对视网膜响应移动物体的影响：前馈抑制和循环反馈抑制。我们证明，两种机制都可以通过不同的方式实现运动预测。前馈抑制通过减法抑制截断运动响应并将峰值响应前移，而循环反馈耦合通过除法抑制引起不同相位的兴奋性和抑制性波，它们相加并使响应峰值偏移。两种机制的主要区别在于峰值响应如何随移动物体速度变化。使用前馈电路的运动预测随速度增加而单调下降，而循环反馈耦合则诱导出具有最优速度的调谐曲线，在该速度下运动预测达到最大。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion is an important feature of visual scenes and retinal neuronal circuitsselectively signal different motion features. It has been shown that the retinacan extrapolate the position of a moving object, thereby compensating sensorytransmission delays and enabling signal processing in real-time. Amacrinecells, the inhibitory interneurons of the retina, play essential roles in suchcomputations although their precise function remain unclear. Here, wecomputationally explore the effect of two different inhibitory connectivitymotifs on the retina's response to moving objects: feed-forward and recurrentfeed-back inhibition. We show that both can account for motion anticipationwith two different mechanisms. Feed-forward inhibition truncates motionresponses and shifts peak responses forward via subtractive inhibition, whereasrecurrent feedback coupling evokes, via divisive inhibition, excitatory andinhibitory waves with different phases that add up and shift the response peak.A key difference between the two mechanisms is how the peak response scaleswith the speed of a moving object. Motion prediction with feedforward circuitsmonotonically decreases with increasing speeds, while recurrent feedbackcoupling induces tuning curves that exhibit a preferred speed for which motionprediction is maximal.</description>
      <author>example@mail.com (S. Ebert, B. Cessac)</author>
      <guid isPermaLink="false">2508.02436v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering</title>
      <link>http://arxiv.org/abs/2508.02362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Text2Lip是一种以视素为中心的框架，通过将文本输入转换为结构化视素序列，构建语音-视觉桥梁，实现语义连贯且视觉准确的说话人脸生成，解决了现有音频驱动方法在可扩展性和鲁棒性方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;生成语义连贯且视觉准确的说话人脸需要弥合语言意义和面部发音之间的差距。基于音频驱动的方法虽然普遍，但它们依赖于高质量的成对视听数据，并且将声学映射到唇部运动存在固有歧义，这给可扩展性和鲁棒性带来了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有音频驱动方法在可扩展性和鲁棒性方面的问题，提出一种新的可控且灵活的说话人脸生成范式。&lt;h4&gt;方法&lt;/h4&gt;提出Text2Lip框架，将文本输入嵌入到结构化视素序列中构建可解释的语音-视觉桥梁；设计基于课程学习的渐进式视素-音频替换策略，使模型能从真实音频过渡到通过跨模态注意力从增强视素特征重建的伪音频；使用标志引导的渲染器合成具有精确唇部同步的逼真面部视频。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估表明，Text2Lip在语义保真度、视觉真实性和模态鲁棒性方面优于现有方法，能够在有音频和无音频场景中都能鲁棒生成。&lt;h4&gt;结论&lt;/h4&gt;Text2Lip为可控且灵活的说话人脸生成建立了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;生成语义连贯且视觉准确的说话人脸需要弥合语言意义和面部发音之间的差距。虽然基于音频驱动的方法仍然普遍，但它们依赖于高质量的成对视听数据，并且将声学映射到唇部运动存在固有歧义，这给可扩展性和鲁棒性带来了重大挑战。为解决这些问题，我们提出了Text2Lip，一个以视素为中心的框架，通过将文本输入嵌入到结构化的视素序列中，构建了一个可解释的语音-视觉桥梁。这些中间单元作为唇部运动预测的语言学基础先验。此外，我们设计了一种基于课程学习的渐进式视素-音频替换策略，使模型能够逐渐从真实音频过渡到通过跨模态注意力从增强的视素特征重建的伪音频。这使得模型能够在有音频和无音频场景中都能鲁棒生成。最后，一个由标志引导的渲染器合成具有精确唇部同步的逼真面部视频。广泛的评估表明，Text2Lip在语义保真度、视觉真实性和模态鲁棒性方面优于现有方法，为可控且灵活的说话人脸生成建立了新的范式。我们的项目主页是https://plyon1.github.io/Text2Lip/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating semantically coherent and visually accurate talking faces requiresbridging the gap between linguistic meaning and facial articulation. Althoughaudio-driven methods remain prevalent, their reliance on high-quality pairedaudio visual data and the inherent ambiguity in mapping acoustics to lip motionpose significant challenges in terms of scalability and robustness. To addressthese issues, we propose Text2Lip, a viseme-centric framework that constructsan interpretable phonetic-visual bridge by embedding textual input intostructured viseme sequences. These mid-level units serve as a linguisticallygrounded prior for lip motion prediction. Furthermore, we design a progressiveviseme-audio replacement strategy based on curriculum learning, enabling themodel to gradually transition from real audio to pseudo-audio reconstructedfrom enhanced viseme features via cross-modal attention. This allows for robustgeneration in both audio-present and audio-free scenarios. Finally, alandmark-guided renderer synthesizes photorealistic facial videos with accuratelip synchronization. Extensive evaluations show that Text2Lip outperformsexisting approaches in semantic fidelity, visual realism, and modalityrobustness, establishing a new paradigm for controllable and flexible talkingface generation. Our project homepage is https://plyon1.github.io/Text2Lip/.</description>
      <author>example@mail.com (Xu Wang, Shengeng Tang, Fei Wang, Lechao Cheng, Dan Guo, Feng Xue, Richang Hong)</author>
      <guid isPermaLink="false">2508.02362v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2508.01585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STCN是一种创新的随机和连续人类运动预测方法，通过时空连续网络和锚集机制解决了现有方法的局限性，在多个数据集上表现出色&lt;h4&gt;背景&lt;/h4&gt;随机人类运动预测（HMP）因广泛应用而受到越来越多的关注。尽管生成领域取得了快速进展，但现有方法在学习连续时间动态和预测随机运动序列方面面临挑战。这些方法往往忽视了复杂人类运动中固有的灵活性，并且容易出现模式崩溃。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在学习连续时间动态和预测随机运动序列方面的挑战，解决方法忽视复杂人类运动中固有灵活性的问题，缓解模式崩溃问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为STCN的新方法，用于随机和连续的人类运动预测，包含两个阶段：第一阶段提出时空连续网络生成更平滑的人类运动序列，并创新地将锚集引入随机HMP任务以防止模式崩溃；第二阶段STCN借助锚集获取观测运动序列的高斯混合分布，关注每个锚相关的概率，并采用从每个锚采样多个序列的策略来缓解人类运动中的类内差异。&lt;h4&gt;主要发现&lt;/h4&gt;在两个广泛使用的数据集（Human3.6M和HumanEva-I）上的实验结果表明，该模型在多样性和准确性方面都获得了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;STCN方法有效解决了随机人类运动预测中的连续时间动态学习和模式崩溃问题，通过引入锚集和采用多序列采样策略，提高了预测的多样性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;随机人类运动预测（HMP）由于其广泛的应用而受到越来越多的关注。尽管生成领域取得了快速进展，但现有方法通常在学习连续时间动态和预测随机运动序列方面面临挑战。它们往往忽视了复杂人类运动中固有的灵活性，并且容易出现模式崩溃。为了缓解这些问题，我们提出了一种名为STCN的新方法，用于随机和连续的人类运动预测，它包含两个阶段。具体来说，在第一阶段，我们提出了一个时空连续网络来生成更平滑的人类运动序列。此外，创新地将锚集引入随机HMP任务以防止模式崩溃，锚集代表了潜在的人类运动模式。在第二阶段，STCN借助锚集获取观测运动序列的高斯混合分布（GMM）。它还关注与每个锚相关的概率，并采用从每个锚采样多个序列的策略来缓解人类运动中的类内差异。在两个广泛使用的数据集（Human3.6M和HumanEva-I）上的实验结果表明，我们的模型在多样性和准确性方面都获得了具有竞争力的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决随机3D人体运动预测中的两个关键问题：一是现有方法难以学习连续时间动态，无法灵活处理不同速度和频率的复杂人体运动；二是容易出现模式崩溃问题，忽略了运动中的多样性和类内差异。这个问题在现实中非常重要，因为人体运动预测在虚拟现实、人机交互、游戏开发等领域有广泛应用，而准确预测多种可能的未来运动对实现自然、安全的人机交互至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有随机人体运动预测方法的局限性，包括VAEs生成模糊运动、GANs容易模式崩溃、扩散模型训练复杂等问题。他们借鉴了神经ODE用于连续建模、VQVAE作为骨干网络、锚集概念用于聚类等现有技术，但创新性地将这些技术组合应用于人体运动预测。作者设计了两阶段方法：第一阶段学习连续表示和锚集，第二阶段预测随机运动序列，整体上通过时空连续网络和锚集机制来解决现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时空连续网络捕捉人体运动的连续时间动态，引入锚集表示潜在运动模式防止模式崩溃，并通过高斯混合模型建模每个锚的分布，同时从每个锚采样多个序列来捕捉同一运动模式内的类内差异。整体流程分为两个阶段：第一阶段是人体运动重建，将输入序列编码到潜在空间，使用ODE网络建模连续动态，并通过K-means学习锚集；第二阶段是随机运动预测，将观察序列与锚集匹配，学习每个锚的概率和分布，最后从分布中采样多个序列生成多样化预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于ODE的时空连续网络，能捕捉连续时间动态提高预测平滑性；2) 创新引入锚集机制防止模式崩溃；3) 使用高斯混合模型建模每个锚的分布；4) 从每个锚采样多个序列缓解类内差异。相比之前工作，STCN解决了VAEs生成模糊、GANs模式崩溃、扩散模型训练复杂等问题，同时克服了离散时间模型不够灵活的局限，能够生成更平滑、更多样化的人体运动预测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; STCN通过时空连续网络和锚集机制，有效解决了随机3D人体运动预测中的模式崩溃和连续动态建模问题，显著提高了预测序列的多样性和平滑性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stochastic Human Motion Prediction (HMP) has received increasing attentiondue to its wide applications. Despite the rapid progress in generative fields,existing methods often face challenges in learning continuous temporal dynamicsand predicting stochastic motion sequences. They tend to overlook theflexibility inherent in complex human motions and are prone to mode collapse.To alleviate these issues, we propose a novel method called STCN, forstochastic and continuous human motion prediction, which consists of twostages. Specifically, in the first stage, we propose a spatio-temporalcontinuous network to generate smoother human motion sequences. In addition,the anchor set is innovatively introduced into the stochastic HMP task toprevent mode collapse, which refers to the potential human motion patterns. Inthe second stage, STCN endeavors to acquire the Gaussian mixture distribution(GMM) of observed motion sequences with the aid of the anchor set. It alsofocuses on the probability associated with each anchor, and employs thestrategy of sampling multiple sequences from each anchor to alleviateintra-class differences in human motions. Experimental results on twowidely-used datasets (Human3.6M and HumanEva-I) demonstrate that our modelobtains competitive performance on both diversity and accuracy.</description>
      <author>example@mail.com (Hua Yu, Yaqing Hou, Xu Gui, Shanshan Feng, Dongsheng Zhou, Qiang Zhang)</author>
      <guid isPermaLink="false">2508.01585v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future</title>
      <link>http://arxiv.org/abs/2507.22792v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  45 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述文章提供了基于SAM/SAM2的视频对象分割和跟踪(VOST)方法的全面回顾，从过去、现在和未来三个时间维度分析了相关技术发展，指出了从早期基于内存的架构到SAM2的流式内存和实时分割能力的演变，讨论了最新的创新，并确定了剩余挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;视频对象分割和跟踪(VOST)在计算机视觉中是一个复杂而关键的挑战，需要稳健地整合跨时间动态帧的分割和跟踪功能。传统方法在领域泛化、时间一致性和计算效率方面存在困难。基础模型(如Segment Anything Model及其后续版本SAM2)的出现带来了范式转变，实现了具有强大泛化能力的提示驱动分割。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在提供一个及时且结构化的VOST领域概述，通过基础模型的视角，指导研究人员和实践者推进VOST的发展状态。&lt;h4&gt;方法&lt;/h4&gt;综述沿着三个时间维度对SAM/SAM2方法进行了结构化分析：过去(保留和更新历史信息的策略)，现在(从当前帧提取和优化判别特征的方法)，未来(预测对象动态和轨迹估计的机制)。&lt;h4&gt;主要发现&lt;/h4&gt;从早期基于内存的架构发展到SAM2的流式内存和实时分割能力；最近的创新包括感知内存的选择和轨迹引导的提示，旨在提高准确性和效率；基础模型如SAM和SAM2为VOST带来了范式转变，实现了具有强大泛化能力的提示驱动分割。&lt;h4&gt;结论&lt;/h4&gt;剩余挑战包括内存冗余、错误累积和提示效率低下。未来研究有希望的方向包括优化内存使用、减少错误传播、提高提示效率等。&lt;h4&gt;翻译&lt;/h4&gt;视频对象分割和跟踪(VOST)在计算机视觉中提出了一个复杂而至关重要的挑战，需要稳健地整合跨时间动态帧的分割和跟踪。传统方法在领域泛化、时间一致性和计算效率方面一直存在困难。基础模型(如Segment Anything Model及其后续版本SAM2)的出现引入了范式转变，实现了具有强大泛化能力的提示驱动分割。基于这些进展，这篇综述对基于SAM/SAM2的VOST方法进行了全面回顾，沿着过去、现在和未来三个时间维度进行结构化分析。我们检查了保留和更新历史信息(过去)的策略，从当前帧提取和优化判别特征(现在)的方法，以及预测后续帧中对象动态和轨迹估计(未来)的机制。通过这样做，我们突显了从早期基于内存的架构到SAM2的流式内存和实时分割能力的演变。我们还讨论了最近的创新，如感知内存的选择和轨迹引导的提示，这些创新旨在提高准确性和效率。最后，我们确定了剩余的挑战，包括内存冗余、错误累积和提示效率低下，并建议了未来研究的有希望方向。这篇综述提供了该领域及时且结构化的概述，旨在通过基础模型的视角指导研究人员和实践者推进VOST的发展状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Object Segmentation and Tracking (VOST) presents a complex yet criticalchallenge in computer vision, requiring robust integration of segmentation andtracking across temporally dynamic frames. Traditional methods have struggledwith domain generalization, temporal consistency, and computational efficiency.The emergence of foundation models like the Segment Anything Model (SAM) andits successor, SAM2, has introduced a paradigm shift, enabling prompt-drivensegmentation with strong generalization capabilities. Building upon theseadvances, this survey provides a comprehensive review of SAM/SAM2-based methodsfor VOST, structured along three temporal dimensions: past, present, andfuture. We examine strategies for retaining and updating historical information(past), approaches for extracting and optimizing discriminative features fromthe current frame (present), and motion prediction and trajectory estimationmechanisms for anticipating object dynamics in subsequent frames (future). Indoing so, we highlight the evolution from early memory-based architectures tothe streaming memory and real-time segmentation capabilities of SAM2. We alsodiscuss recent innovations such as motion-aware memory selection andtrajectory-guided prompting, which aim to enhance both accuracy and efficiency.Finally, we identify remaining challenges including memory redundancy, erroraccumulation, and prompt inefficiency, and suggest promising directions forfuture research. This survey offers a timely and structured overview of thefield, aiming to guide researchers and practitioners in advancing the state ofVOST through the lens of foundation models.</description>
      <author>example@mail.com (Guoping Xu, Jayaram K. Udupa, Yajun Yu, Hua-Chieh Shao, Songlin Zhao, Wei Liu, You Zhang)</author>
      <guid isPermaLink="false">2507.22792v2</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2508.00917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2303.01788,  arXiv:2304.01168 by other authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是关于多任务学习(MTL)在连接自动驾驶汽车(CAVs)中应用的首次全面综述研究。CAVs需要同时执行多项任务以确保安全导航，而传统方法使用独立模型导致高成本和计算开销。MTL通过单一模型联合学习多个任务，提高了效率和资源利用率。&lt;h4&gt;背景&lt;/h4&gt;CAVs必须同时执行多项任务(如目标检测、语义分割、深度估计、轨迹预测等)以确保在复杂环境中的安全导航。V2X通信使CAVs能够协同驾驶，减轻单个传感器限制。传统方法使用不同模型处理不同任务，导致高部署成本、增加计算开销和实时性能挑战。&lt;h4&gt;目的&lt;/h4&gt;提供首个专注于CAVs背景下MTL的全面综述，探讨MTL在关键功能模块中的应用，讨论现有方法的优缺点，确定研究差距，并提供未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;本文首先概述CAVs和MTL提供基础背景，然后探索MTL在关键功能模块(包括感知、预测、规划、控制和多智能体协作)中的应用，最后讨论现有方法的优缺点并确定未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;MTL作为一种有前景的解决方案，可以在单一统一模型中联合学习多个任务，提高了效率和资源利用率。V2X通信使CAVs能够协同驾驶，减轻了单个传感器的限制，减少了遮挡，并提高了长距离感知能力。&lt;h4&gt;结论&lt;/h4&gt;MTL为CAVs系统提供了更高效和资源优化的解决方案，但仍有研究需要开展以进一步改进MTL方法。未来的研究方向应集中在解决现有方法的局限性，填补研究空白，并推进CAV系统的MTL方法论。&lt;h4&gt;翻译&lt;/h4&gt;连接自动驾驶汽车(CAVs)必须同时执行多项任务，如目标检测、语义分割、深度估计、轨迹预测、运动预测和行为预测，以确保在复杂环境中的安全导航。车对万物(V2X)通信使CAVs能够实现协同驾驶，从而减轻单个传感器的限制，减少遮挡，并提高长距离感知能力。传统上，这些任务使用不同的模型来解决，这导致高部署成本、增加的计算开销以及实现实时性能的挑战。多任务学习(MTL)最近出现了一种有前景的解决方案，它可以在单一统一模型中实现多个任务的联合学习。这提高了效率和资源利用率。据我们所知，这是首个专注于CAVs背景下MTL的全面综述。我们从概述CAVs和MTL开始，以提供基础背景。然后，我们探讨了MTL在关键功能模块中的应用，包括感知、预测、规划、控制和多智能体协作。最后，我们讨论了现有方法的优缺点，确定了关键研究差距，并提供了旨在推进CAV系统MTL方法论的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Connected autonomous vehicles (CAVs) must simultaneously perform multipletasks, such as object detection, semantic segmentation, depth estimation,trajectory prediction, motion prediction, and behaviour prediction, to ensuresafe and reliable navigation in complex environments. Vehicle-to-everything(V2X) communication enables cooperative driving among CAVs, thereby mitigatingthe limitations of individual sensors, reducing occlusions, and improvingperception over long distances. Traditionally, these tasks are addressed usingdistinct models, which leads to high deployment costs, increased computationaloverhead, and challenges in achieving real-time performance. Multi-tasklearning (MTL) has recently emerged as a promising solution that enables thejoint learning of multiple tasks within a single unified model. This offersimproved efficiency and resource utilization. To the best of our knowledge,this survey is the first comprehensive review focused on MTL in the context ofCAVs. We begin with an overview of CAVs and MTL to provide foundationalbackground. We then explore the application of MTL across key functionalmodules, including perception, prediction, planning, control, and multi-agentcollaboration. Finally, we discuss the strengths and limitations of existingmethods, identify key research gaps, and provide directions for future researchaimed at advancing MTL methodologies for CAV systems.</description>
      <author>example@mail.com (Jiayuan Wang, Farhad Pourpanah, Q. M. Jonathan Wu, Ning Zhang)</author>
      <guid isPermaLink="false">2508.00917v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Amortized Clustering Assistant Classification of Anomalous Hybrid Floquet Modes in a Periodically Driven non-Hermitian Lattice</title>
      <link>http://arxiv.org/abs/2508.00571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了Floquet周期驱动与非厄米性相互作用在有限尺寸紧束缚晶格模型中产生的异常Floquet拓扑相现象。作者为双晶格系统设计了复杂驱动协议，发现了支持Floquet π模的两个非平凡拓扑相，并引入无监督学习方法分析系统本征函数在不同能量增益/损失条件下的分布特征。通过构建能动态升级的算法选择器，研究揭示了二维晶格中动态局域化的调控机制，为Floquet模的分类提供了基于机器学习的可行方法论。&lt;h4&gt;背景&lt;/h4&gt;Floquet周期驱动与非厄米性的相互作用可以产生有趣的异常Floquet拓扑相现象。然而，如何有效研究具有复杂驱动协议的非厄米Floquet系统的准能量和本征场仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;研究具有复杂驱动协议的双晶格系统中的Floquet拓扑相，探索系统本征函数在不同能量增益/损失条件下的分布特征，揭示动态局域化的调控机制，并提供一种通过机器学习方法辅助Floquet模分类的可行方法论。&lt;h4&gt;方法&lt;/h4&gt;定义复杂的双晶格系统驱动协议；引入无监督学习方法探索系统本征函数在不同能量增益/损失幅度下的分布特征；利用分摊聚类思想构建能随输入参数增加而动态升级的算法选择器；通过机器学习方法对Floquet模进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;发现了支持Floquet π模的两个非平凡拓扑相；算法选择器的适当应用能够从二维晶格中丰富的波函数分布中有效地揭示动态局域化的调控机制；提供了一种通过机器学习方法辅助Floquet模分类的可行方法论。&lt;h4&gt;结论&lt;/h4&gt;本研究通过结合Floquet拓扑相理论与机器学习方法，有效地揭示了复杂驱动协议下非厄米Floquet系统的动态局域化调控机制，为Floquet模的分类提供了一种新的可行途径。&lt;h4&gt;翻译&lt;/h4&gt;Floquet周期驱动与非厄米性之间的相互作用可以在有限尺寸的紧束缚晶格模型中带来有趣的异常Floquet拓扑相现象。如何有效地研究具有复杂驱动协议的非厄米Floquet系统的准能量和本征场仍然是一个具有挑战性的任务。在这项工作中，我们为双晶格系统定义了一种复杂的驱动协议，并发现了支持Floquet π模的两个非平凡拓扑相。随后，我们引入无监督学习方法来探索系统本征函数在不同能量增益/损失幅度下的分布特征。我们利用分摊聚类的思想构建了一个算法选择器，该选择器可以随着增益/损失作为输入参数的增加而动态升级。选择器的适当应用使我们能够以另一种高效的方式从二维晶格中丰富的波函数分布中揭示动态局域化的调控机制。此外，我们的工作通过机器学习方法提供了一种可行的辅助Floquet模分类的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The interplay between Floquet periodically driving and non-Hermiticity couldbring about intriguing novel phenomena with anomalous Floquet topologicalphases of a finite-size, tight-binding lattice model. How to efficientlyinvestigate on quasi-energy and eigenfield of a non-Hermitian Floquet systemwith complicated driving protocol remains a challenging task. In this work, wedefine a somewhat complex driving protocol for a bipartite lattice system anddiscover two nontrivial topological phases that support Floquet {\pi} mode.Thereafter, we introduce unsupervised learning method in order to exploredistribution features of system eigenfunctions under different magnitude ofsystem energy gain/loss. We utilize the idea of amortized clustering andconstruct an algorithm selector that could dynamically upgrade with increasinggain/loss as input parameter. Proper employment of the selector enables us toreveal the regulation of dynamic localization from abundant possible wavefunction distribution in two-dimension lattice in another efficient way. Inaddition, our work provides a feasible methodology via machine learning methodto assist in classification of Floquet modes.</description>
      <author>example@mail.com (Yifei Xia, Xiumei Wang, Yali Li, Xingping Zhou)</author>
      <guid isPermaLink="false">2508.00571v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
  <item>
      <title>Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning</title>
      <link>http://arxiv.org/abs/2508.00822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了异构标记点云数据集上的语义分割性能，这些数据集与公共安全应用相关。研究使用NIST点云城市数据集，采用分级模式和KPConv架构，通过IoU指标评估性能。研究发现大型物体分割性能较高，而小型安全关键特征识别率较低，主要受类别不平衡和物体几何区分度有限的影响。研究指出当前方法在检测某些安全特征方面存在局限性，并提出需要标准化注释协议和改进标记技术。&lt;h4&gt;背景&lt;/h4&gt;研究针对公共安全应用中的点云语义分割，包括基于激光雷达扫描的预先事件规划系统。面临的主要挑战是如何统一不同标记的3D数据集。&lt;h4&gt;目的&lt;/h4&gt;研究异构标记点云数据集上的语义分割性能，评估统一不同标记3D数据所面临的挑战，以及点云语义分割在公共安全应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用NIST的点云城市数据集（Enfield和Memphis收藏），采用分级模式和KPConv架构，通过IoU指标评估与安全相关特征的性能。&lt;h4&gt;主要发现&lt;/h4&gt;几何上较大的物体（如楼梯、窗户）实现了较高的分割性能，可能用于导航上下文；较小的安全关键特征识别率较低。性能受到类别不平衡和典型激光雷达扫描中小物体几何区分度有限的影响，表明当前点云方法在检测某些安全相关特征方面存在局限性。&lt;h4&gt;结论&lt;/h4&gt;公共安全可靠的点云语义分割需要标准化的注释协议和改进的标记技术，以解决数据异质性和检测小安全关键元素的问题。&lt;h4&gt;翻译&lt;/h4&gt;本研究分析了与公共安全应用相关的异构标记点云数据集上的语义分割性能，包括从激光雷达扫描中获得的预先事件规划系统。使用NIST的点云城市数据集（Enfield和Memphis收藏），我们研究了统一不同标记的3D数据所面临的挑战。我们的方法采用分级模式和KPConv架构，通过IoU指标评估与安全相关特征的性能。结果表明性能存在变异性：几何上较大的物体（如楼梯、窗户）实现了较高的分割性能，表明可能用于导航上下文，而较小的安全关键特征表现出较低的识别率。性能受到类别不平衡和典型激光雷达扫描中小物体几何区分度有限的影响，表明当前点云方法在检测某些安全相关特征方面存在局限性。确定的主要挑战包括标记数据不足、难以统一不同数据集中的类别标签以及标准化的需求。潜在方向包括自动化标记和多数据集学习策略。我们得出结论，公共安全可靠的点云语义分割需要标准化的注释协议和改进的标记技术，以解决数据异质性和检测小安全关键元素的问题。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决不同标注方式下的点云数据集整合问题，特别是在公共安全应用场景中如何统一NIST点云城市数据集用于3D深度学习。这个问题在现实中非常重要，因为公共安全应用（如紧急响应预案系统）需要可靠的3D点云语义分割来识别安全关键特征，而现有点云数据集通常缺乏针对公共安全应用的专门标注，不同数据集使用不同的标注方法导致难以整合利用，限制了深度学习模型在公共安全领域的应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到公共安全领域缺乏统一的点云数据集和标注标准，意识到不同数据集（Enfield和Memphis）虽然都关注公共安全特征但使用了不同的标注方法。他们借鉴了现有工作，包括使用SemanticKITTI数据集结构作为转换目标格式，采用KPConv（可变形卷积）架构直接处理点云数据，利用Open3D-ML软件管道处理数据，并参考了Point Prompt Training等新兴方法。作者设计了一种统一的标注模式（graded schema）和转换管道来解决数据异质性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的标注模式将不同数据集的异构标注整合为一致的语义类别，开发转换管道将NIST点云城市数据集转换为与SemanticKITTI兼容的格式，并评估不同训练配置下模型的性能。整体流程包括：1) 数据预处理和格式统一，选择Enfield和Memphis数据集并转换为SemanticKITTI格式；2) 开发PCC-SKITTI映射为语义类分配一致标识符；3) 使用Open3D-ML实现KPConv架构；4) 设置三种训练配置（合并数据集、仅Enfield、仅Memphis）；5) 使用IoU和准确率指标评估性能，分析几何特征与性能的关系及类别不平衡的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 系统分析公共安全导向点云数据集的跨数据集语义分割性能；2) 提出统一异构标注数据集的方法并保留语义意义；3) 提供不同标注方法对模型性能影响的定量见解。相比之前工作，本研究专注于公共安全应用而非通用场景，使用专门为公共安全设计的NIST点云城市数据集，直接解决跨数据集标注不一致问题，分析了几何特征与分割性能的关系，并提出了为公共安全领域点云数据集标准化提供基础的统一框架（PCC-SKITTI）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过开发统一异构标注点云数据集的方法并系统评估跨数据集语义分割性能，为公共安全应用中的3D场景理解提供了重要框架和实用见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study analyzes semantic segmentation performance across heterogeneouslylabeled point-cloud datasets relevant to public safety applications, includingpre-incident planning systems derived from lidar scans. Using NIST's PointCloud City dataset (Enfield and Memphis collections), we investigate challengesin unifying differently labeled 3D data. Our methodology employs a gradedschema with the KPConv architecture, evaluating performance through IoU metricson safety-relevant features. Results indicate performance variability:geometrically large objects (e.g. stairs, windows) achieve higher segmentationperformance, suggesting potential for navigational context, while smallersafety-critical features exhibit lower recognition rates. Performance isimpacted by class imbalance and the limited geometric distinction of smallerobjects in typical lidar scans, indicating limitations in detecting certainsafety-relevant features using current point-cloud methods. Key identifiedchallenges include insufficient labeled data, difficulties in unifying classlabels across datasets, and the need for standardization. Potential directionsinclude automated labeling and multi-dataset learning strategies. We concludethat reliable point-cloud semantic segmentation for public safety necessitatesstandardized annotation protocols and improved labeling techniques to addressdata heterogeneity and the detection of small, safety-critical elements.</description>
      <author>example@mail.com (Alexander Nikitas Dimopoulos, Joseph Grasso)</author>
      <guid isPermaLink="false">2508.00822v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR</title>
      <link>http://arxiv.org/abs/2508.00744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at the Embedded Vision Workshop ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Dense Backbone的轻量级主干网络，专为基于点云的3D目标检测设计，能够在保持高检测性能的同时显著降低模型复杂度和计算成本。&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D目标检测技术已取得显著进展，推动了自动驾驶的实现。然而，现有方法大多仍依赖VGG或ResNet作为主干网络，增加了模型复杂度。轻量级主干设计在2D目标检测中研究较多，但在3D目标检测领域研究仍有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合高处理速度、轻量级架构和稳健检测准确性的轻量级主干网络，用于3D目标检测。&lt;h4&gt;方法&lt;/h4&gt;提出Dense Backbone，一种密集层主干网络，适配了多个最先进的3D目标检测器（如PillarNet）。该设计为即插即用，可轻松集成到现有架构中，无需修改其他网络组件。&lt;h4&gt;主要发现&lt;/h4&gt;DensePillarNet在nuScenes测试集上实现了模型参数减少29%，延迟降低28%，而检测精度仅下降2%。使用Dense Backbone的模型在显著降低计算成本的情况下保留了大部分检测能力。&lt;h4&gt;结论&lt;/h4&gt;Dense Backbone是首个专门为点云数据3D目标检测设计的密集层主干网络，能够在保持高检测性能的同时显著减少模型复杂度和计算成本。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D目标检测的最新进展显著加速了在现实环境中实现完全自动驾驶的进程。尽管取得了高检测性能，但大多数方法仍依赖于基于VGG或ResNet的主干网络进行特征探索，这增加了模型复杂度。轻量级主干设计在2D目标检测中已被充分研究，但在3D目标检测方面的研究仍然有限。在这项工作中，我们引入了Dense Backbone，一种轻量级主干网络，结合了高处理速度、轻量级架构和稳健检测准确性的优势。我们适配了多个最先进的3D目标检测器（如PillarNet）与我们的主干网络，并表明使用我们的主干网络，这些模型在显著降低计算成本的情况下保留了大部分检测能力。据我们所知，这是首个专门为点云数据3D目标检测设计的密集层主干网络。我们的PillarNet适配版本DensePillarNet在nuScenes测试集上实现了模型参数减少29%，延迟降低28%，而检测精度仅下降2%。此外，Dense Backbone的即插即用设计允许轻松集成到现有架构中，无需修改其他网络组件。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是当前基于LiDAR的3D目标检测模型大多依赖VGG或ResNet等复杂骨干网络，导致模型计算量大、难以在资源有限的自动驾驶车辆上实时运行。这个问题在现实中非常重要，因为自动驾驶车辆需要在边缘设备上实时执行多项任务（如目标检测、路径规划等），而车载计算资源有限，无法部署大型模型；同时将计算卸载到云端又受网络限制且存在安全隐私问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到点云数据本质上是稀疏且缺乏结构的，因此高效的特征重用对于从点云学习表示至关重要。他们认为现有模型检测精度下降的主要原因在于使用了未针对点云数据优化的图像检测骨干网络。作者借鉴了DenseNet的特征重用思想、PeleeNet的双向密集层设计以及VovNet的一次性聚合策略，设计了专门针对点云数据的密集连接骨干网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过密集连接和高效的特征重用，在减少模型参数和计算成本的同时保持检测精度。整体实现流程包括：1) Dense Block作为基本构建块，使用一系列卷积层后进行特征连接；2) Transition Layer作为密集块间的中间层，通过点卷积聚合特征并加入平均池化；3) 采用渐进式增长率策略，初始增长率为32，后续每个密集块中翻倍；4) 设计即插即用架构，可无缝集成到现有3D目标检测框架中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了首个专为LiDAR点云3D目标设计的密集层骨干网络；2) 通过密集连接和一次性聚合实现高效特征重用；3) 设计了完全即插即用的骨干网络，无需修改其他组件；4) 在多种现有模型上验证了有效性。相比之前的工作，不同之处在于：不使用通用图像检测骨干网络（如ResNet），而是专门针对点云稀疏特性设计；与需要修改编码器的其他方法不同，Dense Backbone完全即插即用；通过渐进式增长率策略解决了传统密集网络内存访问成本高的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Dense Backbone是一种专为LiDAR点云3D目标检测设计的轻量级、即插即用的密集层骨干网络，通过高效特征重用显著减少了模型参数和计算成本，同时保持了 competitive的检测精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in LiDAR-based 3D object detection have significantlyaccelerated progress toward the realization of fully autonomous driving inreal-world environments. Despite achieving high detection performance, most ofthe approaches still rely on a VGG-based or ResNet-based backbone for featureexploration, which increases the model complexity. Lightweight backbone designis well-explored for 2D object detection, but research on 3D object detectionstill remains limited. In this work, we introduce Dense Backbone, a lightweightbackbone that combines the benefits of high processing speed, lightweightarchitecture, and robust detection accuracy. We adapt multiple SoTA 3d objectdetectors, such as PillarNet, with our backbone and show that with ourbackbone, these models retain most of their detection capability at asignificantly reduced computational cost. To our knowledge, this is the firstdense-layer-based backbone tailored specifically for 3D object detection frompoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%reduction in model parameters and a 28% reduction in latency with just a 2%drop in detection accuracy on the nuScenes test set. Furthermore, DenseBackbone's plug-and-play design allows straightforward integration intoexisting architectures, requiring no modifications to other network components.</description>
      <author>example@mail.com (Adwait Chandorkar, Hasan Tercan, Tobias Meisen)</author>
      <guid isPermaLink="false">2508.00744v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints</title>
      <link>http://arxiv.org/abs/2508.00558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the IEEE/CVF International Conference on  Computer Vision (ICCV), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PhysNAP，一种基于扩散模型的新方法，用于生成与部分点云对齐且物理合理的铰接对象。&lt;h4&gt;背景&lt;/h4&gt;铰接对象是日常生活中重要的可交互对象类型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够与部分点云对齐并提高物理合理性的铰接对象生成方法。&lt;h4&gt;方法&lt;/h4&gt;使用符号距离函数表示部分形状；通过点云对齐损失引导反向扩散过程；施加非穿透性和移动性约束；使方法具有类别感知能力以提高点云对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在PartNet-Mobility数据集上评估显示，PhysNAP能提高约束一致性，并在生成能力方面与无引导基线模型提供权衡。&lt;h4&gt;结论&lt;/h4&gt;PhysNAP是一种有效的扩散模型方法，能够生成与部分点云对齐且物理合理的铰接对象。&lt;h4&gt;翻译&lt;/h4&gt;铰接对象是日常生活中重要的可交互对象类型。在本文中，我们提出PhysNAP，一种基于扩散模型的新方法，用于生成与部分点云对齐并提高其物理合理性的铰接对象。该模型使用符号距离函数表示部分形状。我们使用预测的SDFs计算点云对齐损失来引导反向扩散过程。此外，我们基于部分SDFs施加非穿透性和移动性约束，引导模型生成更物理合理的对象。我们还使扩散方法具有类别感知能力，如果在有类别信息的情况下可进一步提高点云对齐。我们使用PartNet-Mobility数据集评估了使用PhysNAP生成的样本的生成能力和约束一致性。我们还将其与无引导的基线扩散模型进行比较，证明PhysNAP可以提高约束一致性，并在生成能力方面提供权衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成符合物理规律且能与部分点云对齐的关节式物体问题。这个问题在虚拟现实应用和机器人技术中非常重要，因为这些应用需要创建日常物品（如抽屉、电器、笔记本电脑等）的数字孪生体。生成高质量的关节式物体对于构建交互式环境、物体识别和机器人操作等任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于NAP扩散模型进行扩展，借鉴了PhysPart的物理约束思想。他们设计了一个训练自由引导框架，在反向扩散过程中添加点云对齐和物理可行性约束。作者考虑了关节式物体的复杂性，设计了多种可微损失函数，并确保这些损失能够调整物体的各种属性，如位置、形状和关节参数。他们还引入了类别条件信息，以提高点云对齐的准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型生成关节式物体，同时确保生成的物体符合物理规律（无部分穿透）且能与输入的部分点云对齐。整体流程包括：1) 使用预训练SDF表示部分形状；2) 前向扩散添加噪声，反向扩散逐步重建；3) 在反向扩散最后阶段添加引导损失（点云对齐、穿透损失和移动性损失）；4) 后处理提取网格和确定关节连接；5) 评估生成的物体质量和对齐程度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) PhysNAP方法首次将扩散模型与点云对齐和物理约束结合，无需预知关节结构；2) 设计了基于SDF的可微点云对齐损失；3) 引入非穿透和移动性物理约束；4) 实现类别感知生成。相比之前工作，PhysNAP不需要完整的点云输入（不同于PhysPart），不需要预知关节结构（不同于CAGE），同时生成关节图和形状（不同于MIDGaRD），并考虑了所有部分之间的穿透问题（不同于PhysPart只考虑基物体与单个部分）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PhysNAP是一种创新的扩散模型方法，通过点云对齐和物理可行性约束引导，能够生成符合物理规律且与部分点云对齐的关节式物体，无需预先知道关节图结构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Articulated objects are an important type of interactable objects in everydayenvironments. In this paper, we propose PhysNAP, a novel diffusion model-basedapproach for generating articulated objects that aligns them with partial pointclouds and improves their physical plausibility. The model represents partshapes by signed distance functions (SDFs). We guide the reverse diffusionprocess using a point cloud alignment loss computed using the predicted SDFs.Additionally, we impose non-penetration and mobility constraints based on thepart SDFs for guiding the model to generate more physically plausible objects.We also make our diffusion approach category-aware to further improve pointcloud alignment if category information is available. We evaluate thegenerative ability and constraint consistency of samples generated with PhysNAPusing the PartNet-Mobility dataset. We also compare it with an unguidedbaseline diffusion model and demonstrate that PhysNAP can improve constraintconsistency and provides a tradeoff with generative ability.</description>
      <author>example@mail.com (Jens U. Kreber, Joerg Stueckler)</author>
      <guid isPermaLink="false">2508.00558v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.00473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于3D点云视频异常检测的新型双曲时空变换器HyPCV-Former，通过在洛伦兹双曲空间中嵌入特征和使用双曲多头自注意力机制，有效捕获了事件的分层结构和时空连续性，实验证明其在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;视频异常检测是视频监控中的基本任务，在公共安全和智能监控系统中有广泛应用。之前的方法利用RGB或深度域中的欧几里得表示，但这些嵌入在捕获分层事件结构和时空连续性方面存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在捕获分层事件结构和时空连续性方面的局限性，提出一种新的用于3D点云视频异常检测的双曲时空变换器。&lt;h4&gt;方法&lt;/h4&gt;提出HyPCV-Former，首先通过点云提取器从点云序列中提取每帧空间特征，然后将这些特征嵌入到洛伦兹双曲空间中。引入双曲多头自注意力(HMHA)机制来建模时间动态，利用洛伦兹内积和曲率感知的softmax在非欧几里得几何下学习时间依赖性。方法在完整的洛伦兹空间内直接执行所有特征变换和异常评分，而不是通过切线空间近似。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，HyPCV-Former在多个异常类别上达到了最先进的性能，在TIMo数据集上比基准方法提高了7%，在DAD数据集上提高了5.6%。&lt;h4&gt;结论&lt;/h4&gt;HyPCV-Former是一种有效的视频异常检测方法，特别是在处理3D点云视频时。代码将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;视频异常检测是视频监控中的一个基本任务，在公共安全和智能监控系统中有广泛的应用。尽管先前的方法利用RGB或深度域中的欧几里得表示，但这类嵌入本质上在捕获分层事件结构和时空连续性方面存在局限性。为了解决这些局限性，我们提出了HyPCV-Former，一种用于3D点云视频异常检测的新型双曲时空变换器。我们的方法首先通过点云提取器从点云序列中提取每帧空间特征，然后将它们嵌入到洛伦兹双曲空间中，这更好地捕获了事件的潜在分层结构。为了建模时间动态，我们引入了一种双曲多头自注意力(HMHA)机制，该机制利用洛伦兹内积和曲率感知的softmax，在非欧几里得几何下学习时间依赖性。我们的方法在完整的洛伦兹空间内直接执行所有特征变换和异常评分，而不是通过切线空间近似。大量实验证明，HyPCV-Former在多个异常类别上实现了最先进的性能，在TIMo数据集上比基准方法提高了7%，在DAD数据集上提高了5.6%。代码将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云视频中的异常检测问题。这个问题在现实和研究中非常重要，因为视频异常检测是视频监控的基础任务，在公共安全和智能监控系统中有着广泛应用。现有的方法主要在欧几里得空间处理RGB或深度图像，难以有效捕捉异常事件的层次结构和时空连续性。而3D点云能提供准确的3D空间信息，同时保护个人隐私，是一种很有前景的数据表示方式。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D点云数据的优势，能够提供精确的几何信息同时保护隐私。然后发现现有方法在欧几里得空间处理层次结构数据存在局限，而超几何空间特别适合表示层次结构。作者借鉴了PointNet等点云处理方法用于特征提取，Transformer架构用于处理序列数据，以及超几何学习中的Lorentz模型。创新性地将这些技术结合，设计出完全在超几何空间中操作的模型，而不是像之前工作那样通过切线空间近似。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用超几何空间更好地表示层次事件结构，设计超几何多头自注意力机制建模时间依赖关系，并在完整的超几何空间中执行所有特征变换和异常评分。整体流程分为四个阶段：首先使用PointNet等点云提取器提取每帧空间特征；然后将特征嵌入到Lorentzian超几何空间；接着使用超几何时空Transformer建模时空依赖关系；最后使用Lorentzian内距离作为异常评分，直接在超几何空间中计算。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出超几何时空Transformer用于3D点云视频异常检测；引入超几何多头自注意力机制利用Lorentzian内积和曲率感知softmax；在完整Lorentzian空间中执行所有特征变换和异常评分；首次将超几何几何应用于3D点云视频异常检测。相比之前工作，不同之处在于：使用超几何空间而非欧几里得空间；直接在超几何空间操作而非切线空间近似；处理3D点云数据而非RGB或深度图像；引入专门设计的超几何多头自注意力机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了HyPCV-Former，一种基于超几何时空Transformer的3D点云视频异常检测方法，通过在Lorentzian超几何空间中直接操作特征，有效捕获了层次事件结构，实现了比现有方法更优的异常检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video anomaly detection is a fundamental task in video surveillance, withbroad applications in public safety and intelligent monitoring systems.Although previous methods leverage Euclidean representations in RGB or depthdomains, such embeddings are inherently limited in capturing hierarchical eventstructures and spatio-temporal continuity. To address these limitations, wepropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer foranomaly detection in 3D point cloud videos. Our approach first extractsper-frame spatial features from point cloud sequences via point cloudextractor, and then embeds them into Lorentzian hyperbolic space, which bettercaptures the latent hierarchical structure of events. To model temporaldynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanismthat leverages Lorentzian inner products and curvature-aware softmax to learntemporal dependencies under non-Euclidean geometry. Our method performs allfeature transformations and anomaly scoring directly within full Lorentzianspace rather than via tangent space approximation. Extensive experimentsdemonstrate that HyPCV-Former achieves state-of-the-art performance acrossmultiple anomaly categories, with a 7\% improvement on the TIMo dataset and a5.6\% gain on the DAD dataset compared to benchmarks. The code will be releasedupon paper acceptance.</description>
      <author>example@mail.com (Jiaping Cao, Kangkang Zhou, Juan Du)</author>
      <guid isPermaLink="false">2508.00473v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.00259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointGauss的新颖点云引导框架，用于高斯泼溅表示中的实时多目标分割，以及一个名为DesktopObjects-360的全面3D分割数据集。&lt;h4&gt;背景&lt;/h4&gt;现有方法存在初始化时间长和多视角一致性有限的问题，当前基准测试也有局限性，如只关注单个对象、3D评估不一致、规模小和覆盖不全面。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的3D分割方法，解决现有方法的局限性，并提供一个全面的3D分割数据集。&lt;h4&gt;方法&lt;/h4&gt;提出两个关键创新：(1) 基于点云的高斯基元解码器，可在1分钟内生成3D实例掩码；(2) GPU加速的2D掩码渲染系统，确保多视角一致性。同时提出DesktopObjects-360数据集，具有复杂多对象场景、全局一致的2D注释、大规模训练数据、完整的360度覆盖和3D评估掩码。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与之前的最先进方法相比有显著改进，在多视角mIoU上实现了1.89%到31.78%的性能提升，同时保持卓越的计算效率。&lt;h4&gt;结论&lt;/h4&gt;PointGauss框架有效解决了现有方法在初始化时间和多视角一致性方面的局限性，而DesktopObjects-360数据集解决了当前基准测试的局限性，为3D分割研究提供了更全面的资源。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了PointGauss，一种新颖的点云引导框架，用于高斯泼溅表示中的实时多目标分割。与现有方法相比，这些方法存在初始化时间长和多视角一致性有限的问题，我们的方法通过点云分割驱动的管道直接解析高斯基元，实现了高效的3D分割。关键创新在于两个方面：(1) 基于点云的高斯基元解码器，可在1分钟内生成3D实例掩码，以及(2) GPU加速的2D掩码渲染系统，确保多视角一致性。大量实验表明，与之前的最先进方法相比有显著改进，在多视角mIoU上实现了1.89%到31.78%的性能提升，同时保持卓越的计算效率。为了解决当前基准测试的局限性（单一对象关注、不一致的3D评估、规模小和部分覆盖），我们提出了DesktopObjects-360，这是一个用于辐射场中3D分割的新型综合数据集，具有以下特点：(1) 复杂的多对象场景，(2) 全局一致的2D注释，(3) 大规模训练数据（超过27千个2D掩码），(4) 完整的360度覆盖，以及(5) 3D评估掩码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D高斯溅射表示中的多目标分割问题，具体包括现有方法初始化时间过长（通常需要32-45分钟）和多视角一致性不足的问题。这个问题在现实中非常重要，因为3D场景分割是实现3D环境理解和交互的关键步骤，对于增强现实、自动驾驶、机器人导航等领域至关重要。现有方法无法满足实时交互需求，限制了这些技术的实际应用，且多目标分割能力对于复杂场景的理解必不可少。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性（3D几何利用不足、架构复杂、依赖2D模型）发现可以直接操作高斯原始体实现3D分割。他们借鉴了点云分割技术（特别是PointTransformerV3作为骨干网络）和交互式分割思想，允许用户通过点击指定分割目标。同时利用了3D高斯溅射的渲染机制，但创新性地将其用于分割而非仅用于渲染。整体设计思路是绕过2D转换步骤，直接在3D空间中进行分割，从而提高效率并保证多视角一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点云分割模型直接对3D高斯原始体进行分割，充分利用3D几何结构实现快速初始化和多视角一致的分割结果。整体流程分为三部分：1) 提示编码器：将用户点击转换为3D参考点，计算每个高斯原始体的空间相关性权重，并将其与原始属性结合；2) 高斯解码器：通过区域裁剪、自适应批处理、PointTransformerV3网络分类和实例标签分配四阶段处理，生成3D实例标签；3) 溅射投影：将3D标签投影到2D视图生成分割掩码，并应用后处理优化质量。整个流程在1分钟内完成3D实例分割，并能实时渲染多视角一致的分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 点云引导的高斯原始体解码器，直接在3D空间分割，1分钟内生成掩码，比现有方法快200-300倍；2) GPU加速的2D掩码渲染系统，确保多视角一致性；3) DesktopObjects-360数据集，提供复杂多目标场景、全局一致的2D标注、大规模训练数据、完整360°覆盖和3D评估掩码。相比之前工作，PointGauss简化了架构（消除多阶段蒸馏），显著提高了效率（初始化从32-45分钟减至8秒），提升了性能（多视角mIoU提高1.89-31.78%），并专门针对多目标场景设计，能更好处理复杂场景中的遮挡和交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointGauss通过直接操作3D高斯原始体实现了快速、多视角一致的多目标分割，显著提高了分割效率和准确性，并提供了新的基准数据集推动领域发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce PointGauss, a novel point cloud-guided framework for real-timemulti-object segmentation in Gaussian Splatting representations. Unlikeexisting methods that suffer from prolonged initialization and limitedmulti-view consistency, our approach achieves efficient 3D segmentation bydirectly parsing Gaussian primitives through a point cloud segmentation-drivenpipeline. The key innovation lies in two aspects: (1) a point cloud-basedGaussian primitive decoder that generates 3D instance masks within 1 minute,and (2) a GPU-accelerated 2D mask rendering system that ensures multi-viewconsistency. Extensive experiments demonstrate significant improvements overprevious state-of-the-art methods, achieving performance gains of 1.89 to31.78% in multi-view mIoU, while maintaining superior computational efficiency.To address the limitations of current benchmarks (single-object focus,inconsistent 3D evaluation, small scale, and partial coverage), we presentDesktopObjects-360, a novel comprehensive dataset for 3D segmentation inradiance fields, featuring: (1) complex multi-object scenes, (2) globallyconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2Dmasks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.</description>
      <author>example@mail.com (Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li)</author>
      <guid isPermaLink="false">2508.00259v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Robust Model Reconstruction Based on the Topological Understanding of Point Clouds Using Persistent Homology</title>
      <link>http://arxiv.org/abs/2508.00251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于持久同调和Loop细分技术的自动方法，用于从无组织的噪声点云中重建多组件模型，能够有效区分和分离具有共享区域的多个封闭表面，生成高质量最终表面。&lt;h4&gt;背景&lt;/h4&gt;从无组织的点云重建模型是一个重大挑战，特别是当模型由多个组件并由其表面点云表示时。这些模型通常包含噪声的点云，代表具有共享区域的多个封闭表面，使得它们的自动识别和分离本质上很复杂。&lt;h4&gt;目的&lt;/h4&gt;提出一种自动方法，利用持久同调的拓扑理解和代表2-循环，有效区分和分离每个封闭表面，并生成高质量的最终表面实现完整的模型重建。&lt;h4&gt;方法&lt;/h4&gt;使用持久同调提供的拓扑理解，结合持久同调群的代表2-循环来区分表面，采用Loop细分和最小二乘渐进迭代逼近(LSPIA)技术生成高质量最终表面。&lt;h4&gt;主要发现&lt;/h4&gt;该方法对点云中的噪声具有鲁棒性，适合从包含噪声的数据重建模型，实验结果证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了实际应用的潜力，能够有效处理多组件模型的重建问题。&lt;h4&gt;翻译&lt;/h4&gt;从无组织的点云重建模型是一个重大挑战，特别是当模型由多个组件并由其表面点云表示时。这类模型通常涉及表示具有共享区域的多个封闭表面的含噪点云，使得它们的自动识别和分离本质上很复杂。在本文中，我们提出了一种自动方法，利用持久同调提供的拓扑理解以及持久同调群的代表2-循环，有效地区分和分离每个封闭表面。此外，我们采用Loop细分和最小二乘渐进迭代逼近(LSPIA)技术来生成高质量的最终表面，实现完整的模型重建。我们的方法对点云中的噪声具有鲁棒性，使其适合从这类数据重建模型。实验结果证明了我们方法的有效性，并突显了其在实际应用中的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从无组织的点云重建多组件模型的问题，特别是当这些组件的表面点云共享区域且包含噪声时。这个问题在计算机图形学、几何建模和逆向工程中非常重要，因为现实世界中的扫描数据通常包含噪声，而复杂模型（如工业机制、雕塑等）往往由多个组件组成。传统方法在这种情况下难以自动识别和分离每个封闭表面，限制了这些技术在CAD、医学成像、计算机动画和虚拟现实等领域的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统表面重建方法的局限性，包括基于三角剖分的方法对参数敏感、基于平滑先验的方法会过度平滑细节、基于模板的方法缺乏通用性，以及深度学习方法需要大量训练数据。然后，作者借鉴了持续同调用于分析点云拓扑特征的工作，代表性循环作为表面初始近似的方法，以及Loop细化和LSPIA优化技术。通过整合拓扑分析与几何处理技术，作者设计了一种能够自动识别和分离多组件模型封闭表面的方法，同时保持对噪声的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用持续同调的拓扑理解来识别和分离点云中的多个封闭表面，使用代表性2-循环作为初始控制网格，并通过Loop细分和LSPIA优化生成高质量表面。整体流程包括：1)从点云构建alpha过滤并计算持续同调和2-PD；2)聚类2-PD中的点找出显著点；3)为每个显著点计算体积最优循环；4)移除非流形顶点和边；5)为每个2-循环计算邻近点；6)简化网格并作为初始控制网格；7)应用Loop细分和LSPIA优化直到满足停止准则。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用持续同调的拓扑理解来确定重建的封闭表面数量；2)使用持续同调群的有代表性2-循环作为表面重建的初始控制网格；3)采用LSPIA方法生成更好地近似给定点云的重建表面。与传统方法相比，这种方法能够处理多组件共享区域的复杂模型，对噪声具有鲁棒性，能够自动识别和分离每个封闭表面，同时生成高质量的重建表面，解决了传统方法在处理此类复杂场景时的局限性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于持续同调拓扑理解和代表性2-循环的鲁棒模型重建方法，能够从包含噪声的无组织点云中自动识别、分离并高质量重建多组件模型的封闭表面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing models from unorganized point clouds presents a significantchallenge, especially when the models consist of multiple componentsrepresented by their surface point clouds. Such models often involve pointclouds with noise that represent multiple closed surfaces with shared regions,making their automatic identification and separation inherently complex. Inthis paper, we propose an automatic method that uses the topologicalunderstanding provided by persistent homology, along with representative2-cycles of persistent homology groups, to effectively distinguish and separateeach closed surface. Furthermore, we employ Loop subdivision and least squaresprogressive iterative approximation (LSPIA) techniques to generate high-qualityfinal surfaces and achieve complete model reconstruction. Our method is robustto noise in the point cloud, making it suitable for reconstructing models fromsuch data. Experimental results demonstrate the effectiveness of our approachand highlight its potential for practical applications.</description>
      <author>example@mail.com (Yu Chen, Hongwei Lin)</author>
      <guid isPermaLink="false">2508.00251v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs</title>
      <link>http://arxiv.org/abs/2508.00169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了概率点云(PPC)作为一种新的3D场景表示方法，通过为每个点增加概率属性来封装原始LiDAR测量中的不确定性信息，并展示了其在鲁棒3D目标检测中的优势。&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D传感器提供点云，这是各种场景理解任务中的标准3D表示。然而，现代LiDAR在远距离或低反照率物体等现实场景中面临挑战，导致产生稀疏或有误的点云。这些错误源于原始LiDAR测量的噪声，并传播到下游感知模型中，导致准确性严重下降，因为传统3D处理管道在构建点云时不会保留原始测量中的任何不确定性信息。&lt;h4&gt;目的&lt;/h4&gt;作者旨在解决传统LiDAR点云表示中缺乏不确定性信息的问题，提出一种新的3D场景表示方法，能够提高3D目标检测在具有挑战性场景中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了概率点云(PPC)，一种新的3D场景表示方法，其中每个点都增加了概率属性，封装原始数据中的测量不确定性(或置信度)。他们还引入了利用PPC进行鲁棒3D目标检测的推理方法，这些方法可作为计算量小的即插即用模块集成到3D推理管道中。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和真实捕获的实验表明，基于PPC的3D推理方法在具有挑战性的室内和室外场景中优于几种基线方法，这些场景涉及小型、远距离和低反照率物体以及强环境光，包括仅使用LiDAR的模型和相机-LiDAR融合模型。&lt;h4&gt;结论&lt;/h4&gt;概率点云(PPC)作为一种新的3D场景表示方法，通过为点云增加不确定性信息，能够显著提高3D目标检测在具有挑战性场景中的性能，特别是在处理小型、远距离和低反照率物体以及强环境光条件时。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D传感器提供点云，这是各种场景理解任务中的标准3D表示。现代LiDAR在远距离或低反照率物体等现实场景中面临关键挑战，产生稀疏或有误的点云。这些错误源于原始LiDAR测量的噪声，并传播到下游感知模型中，可能导致准确性严重损失。这是因为传统3D处理管道在构建点云时不会保留原始测量中的任何不确定性信息。我们提出了概率点云(PPC)，这是一种新的3D场景表示方法，其中每个点都增加了概率属性，封装原始数据中的测量不确定性(或置信度)。我们进一步引入了利用PPC进行鲁棒3D目标检测的推理方法；这些方法具有通用性，可作为计算量小的即插即用模块用于3D推理管道。通过模拟和真实捕获的实验，我们证明了基于PPC的3D推理方法在具有挑战性的室内和室外场景中优于几种基线方法，这些场景涉及小型、远距离和低反照率物体以及强环境光，包括使用LiDAR和相机-LiDAR融合模型的基线方法。我们的项目网页位于https://bhavyagoyal.github.io/ppc。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现代LiDAR在挑战性现实场景（如长距离、低反照率物体、强环境光）下产生稀疏或错误点云的问题。这个问题很重要，因为LiDAR是自动驾驶、监控等应用的核心传感器，而这些应用需要在各种条件下可靠工作。传统3D处理管道在构建点云时忽略了原始测量中的不确定性信息，导致错误传播到下游感知模型，严重影响系统可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到传统LiDAR信号处理仅依靠峰值位置估计点云，忽略了原始SPAD直方图中的丰富信息。他们决定不采用确定性过滤方法，而是设计轻量级置信度特征来编码传感器不确定性。作者借鉴了点云网络中的关键点采样方法，参考了现有点云去噪方法但指出其计算成本高，也参考了3D重建技术但避免了其高带宽需求。设计时特别考虑了在资源受限的传感器上实现的可能性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出概率点云(PPC)，一种新的3D场景表示，其中每个点都增加了概率属性，封装了原始测量中的不确定性。整体流程包括：1)从原始SPAD直方图中提取概率特征；2)创建带概率属性的点云；3)使用两种方法进行鲁棒推理：邻域概率密度(NPD)过滤去除低置信度噪声点，最远概率点采样(FPPS)确保采样点更可靠。这些方法可作为即插即用模块集成到现有3D感知模型中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计轻量级物理置信度特征；2)开发PPC表示传播传感器不确定性；3)设计低计算成本的鲁棒推理方法；4)在模拟和真实数据上验证效果。不同之处在于：PPC保留而非丢弃不确定性信息；避免额外计算密集型去噪步骤；使用紧凑表示而非处理完整直方图；是首个为3D LiDAR设计和传播置信度属性的工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出概率点云表示和相应的推理方法，显著提高了在挑战性现实场景下3D物体检测的鲁棒性和准确性，同时保持了计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D sensors provide point clouds, a canonical 3D representationused in various scene understanding tasks. Modern LiDARs face key challenges inseveral real-world scenarios, such as long-distance or low-albedo objects,producing sparse or erroneous point clouds. These errors, which are rooted inthe noisy raw LiDAR measurements, get propagated to downstream perceptionmodels, resulting in potentially severe loss of accuracy. This is becauseconventional 3D processing pipelines do not retain any uncertainty informationfrom the raw measurements when constructing point clouds.  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representationwhere each point is augmented with a probability attribute that encapsulatesthe measurement uncertainty (or confidence) in the raw data. We furtherintroduce inference approaches that leverage PPC for robust 3D objectdetection; these methods are versatile and can be used as computationallylightweight drop-in modules in 3D inference pipelines. We demonstrate, via bothsimulations and real captures, that PPC-based 3D inference methods outperformseveral baselines using LiDAR as well as camera-LiDAR fusion models, acrosschallenging indoor and outdoor scenarios involving small, distant, andlow-albedo objects, as well as strong ambient light.  Our project webpage is at https://bhavyagoyal.github.io/ppc .</description>
      <author>example@mail.com (Bhavya Goyal, Felipe Gutierrez-Barragan, Wei Lin, Andreas Velten, Yin Li, Mohit Gupta)</author>
      <guid isPermaLink="false">2508.00169v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation</title>
      <link>http://arxiv.org/abs/2508.00428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing,  Visualization, Visualization design and evaluation methods&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Sel3DCraft，一个用于Text-to-3D生成的视觉提示工程系统，通过三个关键创新解决了传统方法中的盲目试错问题，将无结构的探索转变为引导式视觉过程。&lt;h4&gt;背景&lt;/h4&gt;Text-to-3D生成已经改变了数字内容创作，但仍受到盲目试错提示过程的限制，导致结果不可预测。虽然视觉提示工程在文本到图像领域已经取得进展，但在3D生成中的应用面临独特挑战，需要多视图一致性和空间理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够将无结构的探索转变为引导式视觉过程的系统，以支持设计师在Text-to-3D生成中的创造力，提高生成结果的可预测性。&lt;h4&gt;方法&lt;/h4&gt;Sel3DCraft采用了三个关键创新：1) 双分支结构，结合检索和生成，用于多样化候选探索；2) 多视图混合评分方法，利用MLLMs和创新的高层次指标，以人类专家一致性评估3D模型；3) 提示驱动的可视化分析套件，实现直观的缺陷识别和改进。&lt;h4&gt;主要发现&lt;/h4&gt;大量测试和用户研究表明，Sel3DCraft在支持设计师创造力方面优于其他Text-to-3D系统。&lt;h4&gt;结论&lt;/h4&gt;Sel3DCraft通过创新的视觉提示工程方法，有效解决了Text-to-3D生成中的盲目试错问题，提高了生成结果的可预测性和设计师的工作效率。&lt;h4&gt;翻译&lt;/h4&gt;文本到3D生成已经改变了数字内容创作，但仍受到盲目试错提示过程的限制，导致结果不可预测。虽然视觉提示工程在文本到图像领域已经取得进展，但在3D生成中的应用面临独特挑战，需要多视图一致性和空间理解。我们提出了Sel3DCraft，一个用于T23D的视觉提示工程系统，将无结构的探索转变为引导式视觉过程。我们的方法引入了三个关键创新：结合检索和生成的双分支结构，用于多样化候选探索；利用MLLMs和创新高层次指标的多视图混合评分方法，以人类专家一致性评估3D模型；以及提示驱动的可视化分析套件，实现直观的缺陷识别和改进。大量测试和用户研究表明，Sel3DCraft在支持设计师创造力方面优于其他T23D系统。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决文本到3D生成(T23D)中的提示词工程挑战，当前T23D工具存在盲目试错、用户期望与生成质量差距大、缺乏有效提示词推荐机制等问题。这个问题很重要，因为T23D已改变虚拟现实、游戏等行业的数字内容创作，但现有工具以'黑盒'方式运行，缺乏用户控制，导致专业设计师难以高效创建符合预期的3D模型，限制了技术的实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过与专业设计师进行形成性研究，识别了当前T23D工具的四个关键低效问题，并据此推导出四个设计要求：生成足够图像用于探索、支持3D感知的多视图结果、提供与人类一致的多级评估、推荐提示词迭代改进。方法借鉴了视觉提示工程(如PromptMagician)、创意支持系统(如TaleBrush)、形状探索技术和文本到3D生成技术，但创新性地将这些技术整合应用于3D生成领域，设计了结合检索和生成的双分支结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将T23D的无结构探索转变为引导式视觉过程，通过结合检索与生成、多视图混合评分和可视化分析工具，使提示词工程更加直观高效。整体流程包括：1)多模态统一视觉表示，通过预训练模型检索和多模态合成生成候选；2)多视图混合级评分函数，结合低级计算指标和基于MLLM的高级语义评估；3)评分引导的提示推荐，通过树图词云和关键词贡献图帮助用户迭代优化提示词；4)用户交互流程，用户浏览候选、评估质量、选择关键词并迭代生成新模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：1)首次引入T23D视觉提示工程作为新任务，利用MLLMs评估3D模型多视图图像；2)提出跨模态多视图视觉表示方法，包括多视图卫星图、混合级评分热图和提示驱动的树图词云；3)设计统一的检索和生成交互式系统Sel3DCraft。相比之前工作，本文将2D视觉提示工程扩展到3D领域，强调人类在环路的探索体验而非仅关注生成算法，结合多维度评估方法提供直观的缺陷识别和改进，并通过双分支结构提供更丰富的候选选择。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Sel3DCraft通过交互式视觉提示工程方法结合检索与生成的双分支结构、多视图混合评分和可视化分析工具，显著提高了文本到3D生成的效率、质量和用户体验，使专业设计师能够更直观、高效地创建符合预期的3D模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-3D (T23D) generation has transformed digital content creation, yetremains bottlenecked by blind trial-and-error prompting processes that yieldunpredictable results. While visual prompt engineering has advanced intext-to-image domains, its application to 3D generation presents uniquechallenges requiring multi-view consistency evaluation and spatialunderstanding. We present Sel3DCraft, a visual prompt engineering system forT23D that transforms unstructured exploration into a guided visual process. Ourapproach introduces three key innovations: a dual-branch structure combiningretrieval and generation for diverse candidate exploration; a multi-view hybridscoring approach that leverages MLLMs with innovative high-level metrics toassess 3D models with human-expert consistency; and a prompt-driven visualanalytics suite that enables intuitive defect identification and refinement.Extensive testing and user studies demonstrate that Sel3DCraft surpasses otherT23D systems in supporting creativity for designers.</description>
      <author>example@mail.com (Nan Xiang, Tianyi Liang, Haiwen Huang, Shiqi Jiang, Hao Huang, Yifei Huang, Liangyu Chen, Changbo Wang, Chenhui Li)</author>
      <guid isPermaLink="false">2508.00428v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Analysing Temporal Reasoning in Description Logics Using Formal Grammars</title>
      <link>http://arxiv.org/abs/2508.00575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is an extended version of a paper appearing at the 28th European  Conference on Artificial Intelligence (ECAI 2025). 20 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文建立了一种时间扩展描述逻辑的片段与特定形式文法（特别是合取文法）之间的对应关系。&lt;h4&gt;背景&lt;/h4&gt;这种描述逻辑（具有LTL算子的k次方操作特性）自提出以来，其模型是否具有终极周期性以及查询回答是否可判定一直是一个未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;建立该描述逻辑与合取文法之间的联系，从而解决该逻辑的模型周期性和查询回答可判定性问题。&lt;h4&gt;方法&lt;/h4&gt;通过建立该描述逻辑与合取文法之间的对应关系，利用已有的合取文法工具和算法来解决该逻辑的相关问题。&lt;h4&gt;主要发现&lt;/h4&gt;发现该描述逻辑不具有模型的终极周期性；证明了该描述逻辑中查询回答的不可判定性；为该描述逻辑的一些新的有趣片段建立了查询回答的可判定性。&lt;h4&gt;结论&lt;/h4&gt;通过建立该描述逻辑与合取文法之间的对应关系，解决了该描述逻辑的一些重要理论问题，并为后续研究提供了新的方法和工具。&lt;h4&gt;翻译&lt;/h4&gt;我们建立了一种时间扩展的EL描述逻辑片段，该逻辑带有LTL算子的k次方操作，与特定形式文法（特别是合取文法，即配备了交运算的上下文无关文法）之间的对应关系。这种联系意味着该描述逻辑不具有模型的终极周期性，并进一步导致该描述逻辑中查询回答的不可判定性，解决了一个自该描述逻辑提出以来一直悬而未决的问题。此外，它还为该描述逻辑的一些新的有趣片段建立了查询回答的可判定性，并为此目的重用了现有的合取文法工具和算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We establish a correspondence between (fragments of)$\mathcal{TEL}^\bigcirc$, a temporal extension of the $\mathcal{EL}$description logic with the LTL operator $\bigcirc^k$, and some specific kindsof formal grammars, in particular, conjunctive grammars (context-free grammarsequipped with the operation of intersection). This connection implies that$\mathcal{TEL}^\bigcirc$ does not possess the property of ultimate periodicityof models, and further leads to undecidability of query answering in$\mathcal{TEL}^\bigcirc$, closing a question left open since the introductionof $\mathcal{TEL}^\bigcirc$. Moreover, it also allows to establish decidabilityof query answering for some new interesting fragments of$\mathcal{TEL}^\bigcirc$, and to reuse for this purpose existing tools andalgorithms for conjunctive grammars.</description>
      <author>example@mail.com (Camille Bourgaux, Anton Gnatenko, Michaël Thomazo)</author>
      <guid isPermaLink="false">2508.00575v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Fine-grained Spatiotemporal Grounding on Egocentric Videos</title>
      <link>http://arxiv.org/abs/2508.00518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对第一人称视角视频中的时空定位问题，提出了EgoMask基准和EgoMask-Train数据集，解决了第一人称视频中目标持续时间短、轨迹稀疏、目标尺寸小和位置变化大等挑战。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要在外视点视频中取得了显著进展，但在第一人称视角视频中的研究相对较少，尽管在增强现实和机器人应用中越来越重要。&lt;h4&gt;目的&lt;/h4&gt;分析第一人称视角和外视点视频之间的差异，解决第一人称视角视频中的关键挑战，为第一人称视频理解提供资源和见解。&lt;h4&gt;方法&lt;/h4&gt;系统分析第一人称视角和外视点视频之间的差异；引入EgoMask，第一个用于第一人称视频中细粒度时空定位的像素级基准；通过提出的自动标注流程构建EgoMask；创建EgoMask-Train大规模训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;第一人称视角视频中的关键挑战包括：目标持续时间更短、轨迹更稀疏、目标尺寸更小、位置变化更大；最先进的时空定位模型在EgoMask基准上表现不佳；在EgoMask-Train上进行微调可以显著提高性能，同时保持在外视点数据集上的表现。&lt;h4&gt;结论&lt;/h4&gt;该工作为推进第一人称视频理解提供了必要的资源和见解，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;时空视频定位旨在根据文本查询在视频中定位目标实体。虽然现有研究在外视点视频中取得了显著进展，但第一人称视角设置的研究相对较少，尽管它在增强现实和机器人等应用中的重要性日益增长。在这项工作中，我们对第一人称视角和外视点视频之间的差异进行了系统分析，揭示了关键挑战，如更短的目标持续时间、更稀疏的轨迹、更小的目标尺寸和更大的位置变化。为应对这些挑战，我们引入了EgoMask，这是第一个用于第一人称视频中细粒度时空定位的像素级基准。它由我们提出的自动标注流程构建，该流程标注了短、中、长期视频中的指代表达式和对象掩码。此外，我们创建了EgoMask-Train，一个大规模训练数据集，以促进模型开发。实验表明，最先进的时空定位模型在我们的EgoMask基准上表现不佳，但在EgoMask-Train上进行微调可以显著提高性能，同时保持在外视点数据集上的表现。我们的工作因此为推进第一人称视频理解提供了必要的资源和见解。我们的代码可在https://github.com/LaVi-Lab/EgoMask获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatiotemporal video grounding aims to localize target entities in videosbased on textual queries. While existing research has made significant progressin exocentric videos, the egocentric setting remains relatively underexplored,despite its growing importance in applications such as augmented reality androbotics. In this work, we conduct a systematic analysis of the discrepanciesbetween egocentric and exocentric videos, revealing key challenges such asshorter object durations, sparser trajectories, smaller object sizes, andlarger positional shifts. To address these challenges, we introduce EgoMask,the first pixel-level benchmark for fine-grained spatiotemporal grounding inegocentric videos. It is constructed by our proposed automatic annotationpipeline, which annotates referring expressions and object masks across short-,medium-, and long-term videos. Additionally, we create EgoMask-Train, alarge-scale training dataset to facilitate model development. Experimentsdemonstrate that the state-of-the-art spatiotemporal grounding models performpoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yieldssignificant improvements, while preserving performance on exocentric datasets.Our work thus provides essential resources and insights for advancingegocentric video understanding. Our code is available athttps://github.com/LaVi-Lab/EgoMask .</description>
      <author>example@mail.com (Shuo Liang, Yiwu Zhong, Zi-Yuan Hu, Yeyao Tao, Liwei Wang)</author>
      <guid isPermaLink="false">2508.00518v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>iSafetyBench: A video-language benchmark for safety in industrial environment</title>
      <link>http://arxiv.org/abs/2508.00399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to VISION'25 - ICCV 2025 workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队创建了iSafetyBench基准测试，评估视觉语言模型在工业环境中的表现，特别是在识别常规操作和危险场景方面的能力。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)在零样本设置下已能在多种视频理解任务中实现令人印象深刻的泛化能力，但在高风险工业领域（需要识别常规操作和安全关键异常）的能力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决这一差距，引入iSafetyBench，一个专门用于评估模型在工业环境中正常和危险场景下性能的新视频语言基准。&lt;h4&gt;方法&lt;/h4&gt;iSafetyBench包含1,100个来自真实工业场景的视频片段，标注了98个常规和67个危险动作类别的开放词汇、多标签动作标签。每个片段都配有单标签和多标签评估的多项选择题，能够在零样本条件下评估八种最先进的视频语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;尽管这些模型在现有视频基准上表现强劲，但在iSafetyBench上表现不佳，特别是在识别危险活动和多标签场景方面。结果显示了显著的性能差距。&lt;h4&gt;结论&lt;/h4&gt;需要更强大、安全感知的多模态模型用于工业应用，iSafetyBench提供了首个测试平台来推动这一方向的发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)的最新进展使其能够在零样本设置下跨多种视频理解任务实现令人印象深刻的泛化能力。然而，它们在高风险工业领域的能力仍然很大程度上未被探索，在这些领域中，识别常规操作和安全关键异常至关重要。为解决这一差距，我们引入了iSafetyBench，这是一个新的视频语言基准，专门设计用于评估模型在工业环境中正常和危险场景下的性能。iSafetyBench包含1,100个来自真实工业场景的视频片段，标注了98个常规和67个危险动作类别的开放词汇、多标签动作标签。每个片段都配有单标签和多标签评估的多项选择题，能够细粒度评估VLMs在标准和安全关键环境中的性能。我们在零样本条件下评估了八种最先进的视频语言模型。尽管它们在现有视频基准上表现强劲，但这些模型在iSafetyBench上表现不佳，特别是在识别危险活动和多标签场景方面。我们的结果显示了显著的性能差距，强调了工业应用需要更强大、安全感知的多模态模型。iSafetyBench提供了首个测试平台来推动这一方向的进展。数据集可在以下网址获取：https://github.com/raiyaan-abdullah/iSafety-Bench。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision-language models (VLMs) have enabled impressivegeneralization across diverse video understanding tasks under zero-shotsettings. However, their capabilities in high-stakes industrial domains-whererecognizing both routine operations and safety-critical anomalies isessential-remain largely underexplored. To address this gap, we introduceiSafetyBench, a new video-language benchmark specifically designed to evaluatemodel performance in industrial environments across both normal and hazardousscenarios. iSafetyBench comprises 1,100 video clips sourced from real-worldindustrial settings, annotated with open-vocabulary, multi-label action tagsspanning 98 routine and 67 hazardous action categories. Each clip is pairedwith multiple-choice questions for both single-label and multi-labelevaluation, enabling fine-grained assessment of VLMs in both standard andsafety-critical contexts. We evaluate eight state-of-the-art video-languagemodels under zero-shot conditions. Despite their strong performance on existingvideo benchmarks, these models struggle with iSafetyBench-particularly inrecognizing hazardous activities and in multi-label scenarios. Our resultsreveal significant performance gaps, underscoring the need for more robust,safety-aware multimodal models for industrial applications. iSafetyBenchprovides a first-of-its-kind testbed to drive progress in this direction. Thedataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.</description>
      <author>example@mail.com (Raiyaan Abdullah, Yogesh Singh Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.00399v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Punching Bag vs. Punching Person: Motion Transferability in Videos</title>
      <link>http://arxiv.org/abs/2508.00085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025 main conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了动作识别模型在不同上下文中传递高级运动概念的能力，特别是面对未见过的变化时的表现。作者提出了一个动作可迁移性框架，包含三个数据集，并评估了13个最先进模型，发现模型在识别新上下文中的高级动作时性能显著下降。&lt;h4&gt;背景&lt;/h4&gt;动作识别模型展现出强大的泛化能力，但它们是否能够在不同上下文（即使在相似分布内）有效地传递高级运动概念尚不清楚。例如，模型能否识别出未见过的'punching person'这样的变化，而不仅仅是基本的'punching'动作。&lt;h4&gt;目的&lt;/h4&gt;探索动作识别模型在不同上下文中传递高级运动概念的能力，特别是面对未见过的变化时的表现，并建立一个评估动作可迁移性的重要基准。&lt;h4&gt;方法&lt;/h4&gt;作者引入了一个动作可迁移性框架，包含三个数据集：Syn-TA（包含3D物体运动的合成数据集）、Kinetics400-TA和Something-Something-v2-TA（两者都改编自自然视频数据集）。作者评估了13个最先进的模型在这些基准上的表现，并进行了详细分析。&lt;h4&gt;主要发现&lt;/h4&gt;1) 多模态模型在细粒度未知动作上比粗粒度动作表现更差；2) 无偏的Syn-TA数据集与真实世界数据集一样具有挑战性，模型在受控环境中表现出更大的性能下降；3) 当空间线索占主导时，较大的模型提高了可迁移性，但在密集的时间推理方面表现不佳，而对物体和背景线索的依赖阻碍了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项研究建立了评估动作识别中运动可迁移性的重要基准，并揭示了当前模型在处理新上下文中的高级动作时的局限性。作者还探讨了分离粗粒度和细粒度运动如何改善在时间上有挑战性的数据集中的识别。&lt;h4&gt;翻译&lt;/h4&gt;动作识别模型展现出强大的泛化能力，但它们是否能够在不同上下文（即使在相似分布内）有效地传递高级运动概念？例如，当面对'punching person'这样的未见过的变化时，模型能否识别出广义的'punching'动作？为了探索这一点，我们引入了一个动作可迁移性框架，包含三个数据集：(1) Syn-TA，一个包含3D物体运动的合成数据集；(2) Kinetics400-TA；以及(3) Something-Something-v2-TA，两者都改编自自然视频数据集。我们在这些基准上评估了13个最先进的模型，并观察到模型在识别新上下文中的高级动作时性能显著下降。我们的分析揭示：1) 多模态模型在细粒度未知动作上比粗粒度动作表现更差；2) 无偏的Syn-TA数据集与真实世界数据集一样具有挑战性，模型在受控环境中表现出更大的性能下降；3) 当空间线索占主导时，较大的模型提高了可迁移性，但在密集的时间推理方面表现不佳，而对物体和背景线索的依赖阻碍了泛化能力。我们进一步探讨了分离粗粒度和细粒度运动如何改善在时间上有挑战性的数据集中的识别。我们认为这项研究建立了评估动作识别中运动可迁移性的重要基准。数据集和相关代码：https://github.com/raiyaan-abdullah/Motion-Transfer。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Action recognition models demonstrate strong generalization, but can theyeffectively transfer high-level motion concepts across diverse contexts, evenwithin similar distributions? For example, can a model recognize the broadaction "punching" when presented with an unseen variation such as "punchingperson"? To explore this, we introduce a motion transferability framework withthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from naturalvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks andobserve a significant drop in performance when recognizing high-level actionsin novel contexts. Our analysis reveals: 1) Multimodal models struggle morewith fine-grained unknown actions than with coarse ones; 2) The bias-freeSyn-TA proves as challenging as real-world datasets, with models showinggreater performance drops in controlled settings; 3) Larger models improvetransferability when spatial cues dominate but struggle with intensive temporalreasoning, while reliance on object and background cues hinders generalization.We further explore how disentangling coarse and fine motions can improverecognition in temporally challenging datasets. We believe this studyestablishes a crucial benchmark for assessing motion transferability in actionrecognition. Datasets and relevant code:https://github.com/raiyaan-abdullah/Motion-Transfer.</description>
      <author>example@mail.com (Raiyaan Abdullah, Jared Claypoole, Michael Cogswell, Ajay Divakaran, Yogesh Rawat)</author>
      <guid isPermaLink="false">2508.00085v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly</title>
      <link>http://arxiv.org/abs/2507.19924v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://dejian-lc.github.io/humansam/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HumanSAM是一个新型框架，旨在解决以人为中心的伪造视频检测问题，通过将伪造分为空间、外观和运动异常三种类型，并采用视频理解和空间深度融合的方法，以及基于排序的置信度增强策略。研究团队还构建了第一个公共基准数据集HFV，实验表明HumanSAM在二元和多类伪造分类中表现优异。&lt;h4&gt;背景&lt;/h4&gt;生成式模型（特别是模拟真实人类行为的以人为中心的视频）对人类信息安全和真实性构成重大威胁。尽管二元伪造视频检测取得进展，但对伪造类型的细粒度理解不足，影响可靠性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出HumanSAM框架，解决视频生成模型的基本挑战，将人为中心的伪造分为三种常见类型：空间异常、外观异常和运动异常，提高检测的可靠性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;1) 通过融合视频理解和空间深度两个分支生成人类伪造表示；2) 采用基于排序的置信度增强策略，引入三个先验分数学习更强大的表示；3) 构建第一个公共基准数据集HFV，所有类型的伪造都经过半自动仔细注释。&lt;h4&gt;主要发现&lt;/h4&gt;HumanSAM在二元和多类伪造分类任务中与最先进方法相比取得了有希望的结果，表明该方法在处理不同类型伪造视频方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;HumanSAM框架为以人为中心的伪造视频检测提供了新的解决方案，通过细粒度分类和多特征融合方法提高了检测的准确性和可靠性，为实际应用提供了更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;许多来自生成模型的合成视频，特别是模拟真实人类行为的以人为中心的视频，对人类信息安全和真实性构成重大威胁。虽然在二元伪造视频检测方面取得了进展，但对伪造类型的细粒度理解不足，这关系到可靠性和可解释性，而这些对于实际应用至关重要。为解决这一局限，我们提出了HumanSAM，一个构建于视频生成模型基本挑战之上的新框架。具体来说，HumanSAM旨在将人为中心的伪造分为生成内容中常见的三种不同类型的伪影：空间异常、外观异常和运动异常。为了更好地捕捉几何、语义和时空一致性的特征，我们提出通过融合视频理解和空间深度两个分支来生成人类伪造表示。在训练过程中，我们还采用基于排序的置信度增强策略，通过引入三个先验分数来学习更强大的表示。为了训练和评估，我们构建了第一个公共基准——以人为中心的伪造视频(HFV)数据集，所有类型的伪造都经过半自动仔细注释。在我们的实验中，HumanSAM在二元和多类伪造分类中与最先进方法相比取得了有希望的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous synthesized videos from generative models, especially human-centricones that simulate realistic human actions, pose significant threats to humaninformation security and authenticity. While progress has been made in binaryforgery video detection, the lack of fine-grained understanding of forgerytypes raises concerns regarding both reliability and interpretability, whichare critical for real-world applications. To address this limitation, wepropose HumanSAM, a new framework that builds upon the fundamental challengesof video generation models. Specifically, HumanSAM aims to classifyhuman-centric forgeries into three distinct types of artifacts commonlyobserved in generated content: spatial, appearance, and motion anomaly. Tobetter capture the features of geometry, semantics and spatiotemporalconsistency, we propose to generate the human forgery representation by fusingtwo branches of video understanding and spatial depth. We also adopt arank-based confidence enhancement strategy during the training process to learnmore robust representation by introducing three prior scores. For training andevaluation, we construct the first public benchmark, the Human-centric ForgeryVideo (HFV) dataset, with all types of forgeries carefully annotatedsemi-automatically. In our experiments, HumanSAM yields promising results incomparison with state-of-the-art methods, both in binary and multi-classforgery classification.</description>
      <author>example@mail.com (Chang Liu, Yunfan Ye, Fan Zhang, Qingyang Zhou, Yuchuan Luo, Zhiping Cai)</author>
      <guid isPermaLink="false">2507.19924v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Can Large Pretrained Depth Estimation Models Help With Image Dehazing?</title>
      <link>http://arxiv.org/abs/2508.00698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于预训练深度表示的图像去雾方法，通过RGB-D融合模块增强现有去雾架构的适应性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;真实场景中的图像去雾具有挑战性，因为雾霾具有空间变化特性。现有方法虽然在大规模预训练模型上表现出色，但其架构特定设计限制了在不同场景下的适应能力。&lt;h4&gt;目的&lt;/h4&gt;系统研究预训练深度表示在图像去雾任务中的泛化能力，并开发一种适用于多种去雾架构的通用方法。&lt;h4&gt;方法&lt;/h4&gt;分析从数百万张不同图像中学习的预训练深度表示的去雾泛化能力，并基于深度特征在不同雾霾水平下保持一致的发现，提出一个即插即用的RGB-D融合模块，可无缝集成到各种去雾架构中。&lt;h4&gt;主要发现&lt;/h4&gt;学习的深度深度特征在不同雾霾水平下表现出显著的一致性，这为跨场景去雾提供了基础。&lt;h4&gt;结论&lt;/h4&gt;在多个基准测试上的广泛实验验证了该方法的有效性和广泛适用性，表明它能够适应不同准确性和效率要求的场景。&lt;h4&gt;翻译&lt;/h4&gt;图像去雾由于真实场景中雾霾的空间变化特性而仍然是一个具有挑战性的问题。虽然现有方法已经展示了大规模预训练模型在图像去雾方面的潜力，但其架构特定设计限制了在不同准确性和效率要求场景下的适应性。在这项工作中，我们系统地研究了从数百万张不同图像中学习的预训练深度表示在图像去雾中的泛化能力。我们的经验分析表明，学习的深度深度特征在不同雾霾水平下保持显著的一致性。基于这一见解，我们提出一个即插即用的RGB-D融合模块，可以与各种去雾架构无缝集成。在多个基准测试上的广泛实验验证了我们方法的有效性和广泛适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image dehazing remains a challenging problem due to the spatially varyingnature of haze in real-world scenes. While existing methods have demonstratedthe promise of large-scale pretrained models for image dehazing, theirarchitecture-specific designs hinder adaptability across diverse scenarios withdifferent accuracy and efficiency requirements. In this work, we systematicallyinvestigate the generalization capability of pretrained depthrepresentations-learned from millions of diverse images-for image dehazing. Ourempirical analysis reveals that the learned deep depth features maintainremarkable consistency across varying haze levels. Building on this insight, wepropose a plug-and-play RGB-D fusion module that seamlessly integrates withdiverse dehazing architectures. Extensive experiments across multiplebenchmarks validate both the effectiveness and broad applicability of ourapproach.</description>
      <author>example@mail.com (Hongfei Zhang, Kun Zhou, Ruizheng Wu, Jiangbo Lu)</author>
      <guid isPermaLink="false">2508.00698v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text</title>
      <link>http://arxiv.org/abs/2508.00447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究者提出了CLIPTime，一个基于CLIP架构的多模态、多任务框架，能够从图像和文本输入中预测真菌生长的发育阶段和时间戳，有效解决了现有视觉-语言模型在捕捉时间进展方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;理解生物生长的时间动态在微生物学、农业和生物降解研究等多个领域至关重要，但现有的视觉-语言模型如CLIP在联合视觉-文本推理方面表现出色，但在捕捉时间进展方面的效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从图像和文本输入中预测真菌生长发育阶段和相应时间戳的多模态、多任务框架。&lt;h4&gt;方法&lt;/h4&gt;基于CLIP架构构建，学习联合视觉-文本嵌入，实现时间感知推理；引入带有对齐时间戳和分类阶段标签的合成真菌生长数据集；联合执行分类和回归任务；提出自定义评估指标包括时间准确性和回归误差。&lt;h4&gt;主要发现&lt;/h4&gt;CLIPTime能够有效建模生物进展，产生可解释的、基于时间的输出。&lt;h4&gt;结论&lt;/h4&gt;视觉-语言模型在现实世界生物监测应用中具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;理解生物生长的时间动态在微生物学、农业和生物降解研究等不同领域至关重要。尽管像对比语言图像预训练(CLIP)这样的视觉-语言模型在联合视觉-文本推理方面表现出强大能力，但它们在捕捉时间进展方面的有效性仍然有限。为解决这一问题，我们提出了CLIPTime，一个多模态、多任务框架，旨在从图像和文本输入中预测真菌生长的发育阶段和相应时间戳。基于CLIP架构构建，我们的模型学习联合视觉-文本嵌入，并能够在测试时不需要明确的时间输入的情况下实现时间感知推理。为促进训练和评估，我们引入了一个带有对齐时间戳和分类阶段标签的合成真菌生长数据集。CLIPTime联合执行分类和回归，预测离散生长阶段以及连续时间戳。我们还提出了自定义评估指标，包括时间准确性和回归误差，以评估时间感知预测的精确度。实验结果表明，CLIPTime有效地建模了生物进展，并产生了可解释的、基于时间的输出，突显了视觉-语言模型在现实世界生物监测应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the temporal dynamics of biological growth is critical acrossdiverse fields such as microbiology, agriculture, and biodegradation research.Although vision-language models like Contrastive Language Image Pretraining(CLIP) have shown strong capabilities in joint visual-textual reasoning, theireffectiveness in capturing temporal progression remains limited. To addressthis, we propose CLIPTime, a multimodal, multitask framework designed topredict both the developmental stage and the corresponding timestamp of fungalgrowth from image and text inputs. Built upon the CLIP architecture, our modellearns joint visual-textual embeddings and enables time-aware inference withoutrequiring explicit temporal input during testing. To facilitate training andevaluation, we introduce a synthetic fungal growth dataset annotated withaligned timestamps and categorical stage labels. CLIPTime jointly performsclassification and regression, predicting discrete growth stages alongsidecontinuous timestamps. We also propose custom evaluation metrics, includingtemporal accuracy and regression error, to assess the precision of time-awarepredictions. Experimental results demonstrate that CLIPTime effectively modelsbiological progression and produces interpretable, temporally grounded outputs,highlighting the potential of vision-language models in real-world biologicalmonitoring applications.</description>
      <author>example@mail.com (Anju Rani, Daniel Ortiz-Arroyo, Petar Durdevic)</author>
      <guid isPermaLink="false">2508.00447v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment</title>
      <link>http://arxiv.org/abs/2508.00332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出MCSEO方法，通过细粒度的对象-短语对齐增强多模态句子嵌入，解决了图像-字幕对中存在的噪声问题&lt;h4&gt;背景&lt;/h4&gt;多模态句子嵌入模型通常在训练中结合图像-字幕对和文本数据，但这些对常包含冗余或无关信息等噪声&lt;h4&gt;目的&lt;/h4&gt;减轻图像-字幕对中的噪声问题，提高多模态句子嵌入的质量&lt;h4&gt;方法&lt;/h4&gt;MCSEO方法结合传统图像-字幕对齐与细粒度对象-短语对齐，利用现有分割和目标检测模型提取准确对象-短语对，优化针对对象-短语对应的对比学习目标&lt;h4&gt;主要发现&lt;/h4&gt;在不同骨干模型上的语义文本相似性任务中，MCSEO持续超越强大基线方法，证明精确的对象-短语对齐对多模态表示学习至关重要&lt;h4&gt;结论&lt;/h4&gt;精确的对象-短语对齐在多模态表示学习中具有重要意义，MCSEO方法能有效提升多模态句子嵌入性能&lt;h4&gt;翻译&lt;/h4&gt;多模态句子嵌入模型通常在训练过程中除了使用文本数据外，还会利用图像-字幕对。然而，这些对常常包含噪声，包括图像或字幕一侧的冗余或无关信息。为缓解这一问题，我们提出了MCSEO，一种通过结合细粒度的对象-短语对齐和传统的图像-字幕对齐来增强多模态句子嵌入的方法。具体而言，MCSEO利用现有的分割和目标检测模型提取准确的对象-短语对，然后使用这些对来优化针对对象-短语对应的对比学习目标。在不同骨干模型上的语义文本相似性任务上的实验结果表明，MCSEO始终优于强大的基线方法，突显了精确的对象-短语对齐在多模态表示学习中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sentence embedding models typically leverage image-caption pairsin addition to textual data during training. However, such pairs often containnoise, including redundant or irrelevant information on either the image orcaption side. To mitigate this issue, we propose MCSEO, a method that enhancesmultimodal sentence embeddings by incorporating fine-grained object-phrasealignment alongside traditional image-caption alignment. Specifically, MCSEOutilizes existing segmentation and object detection models to extract accurateobject-phrase pairs, which are then used to optimize a contrastive learningobjective tailored to object-phrase correspondence. Experimental results onsemantic textual similarity (STS) tasks across different backbone modelsdemonstrate that MCSEO consistently outperforms strong baselines, highlightingthe significance of precise object-phrase alignment in multimodalrepresentation learning.</description>
      <author>example@mail.com (Kaiyan Zhao, Zhongtao Miao, Yoshimasa Tsuruoka)</author>
      <guid isPermaLink="false">2508.00332v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.00267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于邻居采样的Adam型随机方法来解决非凸GCN训练问题，利用控制变量技术减少随机误差，并在标准假设下证明了最优收敛速率。数值实验表明，这些方法在节点分类任务上优于经典方法，特别是在大规模图数据集上。&lt;h4&gt;背景&lt;/h4&gt;图卷积网络(GCN)是图表示学习的强大工具，但由于其递归邻域聚合的特性，现有高效训练方法缺乏理论保证或缺少现代深度学习算法中的重要实用元素，如自适应性和动量。&lt;h4&gt;目的&lt;/h4&gt;开发基于邻居采样的Adam型随机方法，解决非凸GCN训练问题，同时提供理论保证和现代深度学习算法的实用优势。&lt;h4&gt;方法&lt;/h4&gt;提出基于邻居采样的Adam型随机方法；利用控制变量技术减少邻居采样引起的随机误差；在标准Adam型方法假设下证明收敛性；在多个基准数据集上进行节点分类任务数值实验。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在标准Adam型方法假设下享有最优收敛速率；在节点分类任务上优于经典基于邻居采样的SGD方法；在大规模图数据集上性能优势尤其明显。&lt;h4&gt;结论&lt;/h4&gt;基于邻居采样的Adam型方法结合控制变量技术可有效解决非凸GCN训练问题，提供理论保证和实际性能优势，特别是在处理大规模图数据时。&lt;h4&gt;翻译&lt;/h4&gt;图卷积网络(GCN)是图表示学习的强大工具。由于GCN采用的递归邻域聚合，现有的高效训练方法要么缺乏理论保证，要么缺少现代深度学习算法中的重要实用元素，如自适应性和动量。在本文中，我们提出了几种基于邻居采样的Adam型随机方法来解决非凸GCN训练问题。我们利用[1]提出的控制变量技术来减少邻居采样引起的随机误差。在Adam型方法的标准假设下，我们证明了我们的方法享有最优收敛速率。此外，我们在几个基准数据集上进行了广泛的节点分类任务数值实验。结果表明，我们的方法优于同样使用控制变量技术的经典基于邻居采样的SGD方法，特别是在大规模图数据集上。我们的代码可在https://github.com/RPI-OPT/CV-ADAM-GNN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph convolutional networks (GCNs) are a powerful tool for graphrepresentation learning. Due to the recursive neighborhood aggregationsemployed by GCNs, efficient training methods suffer from a lack of theoreticalguarantees or are missing important practical elements from modern deeplearning algorithms, such as adaptivity and momentum. In this paper, we presentseveral neighbor-sampling (NS) based Adam-type stochastic methods for solving anonconvex GCN training problem. We utilize the control variate techniqueproposed by [1] to reduce the stochastic error caused by neighbor sampling.Under standard assumptions for Adam-type methods, we show that our methodsenjoy the optimal convergence rate. In addition, we conduct extensive numericalexperiments on node classification tasks with several benchmark datasets. Theresults demonstrate superior performance of our methods over classic NS-basedSGD that also uses the control-variate technique, especially for large-scalegraph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .</description>
      <author>example@mail.com (Molly Noel, Gabriel Mancino-Ball, Yangyang Xu)</author>
      <guid isPermaLink="false">2508.00267v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Melody-Lyrics Matching with Contrastive Alignment Loss</title>
      <link>http://arxiv.org/abs/2508.00123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, 3 tables. This work has been submitted to the  IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了旋律-歌词匹配(MLM)任务，探索音乐和歌词之间的关系，通过自监督表示学习框架实现旋律与歌词的匹配，并展示了将旋律与连贯且可唱的歌词匹配的能力。&lt;h4&gt;背景&lt;/h4&gt;音乐和歌词之间的联系超越了语义层面，包括节奏与韵律、音符时长与音节重音、结构对应等概念对关系，这在音乐信息检索领域是一个引人注目但很少被探索的方向。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的旋律-歌词匹配(MLM)任务，旨在从文本源中检索给定符号旋律的潜在歌词，而非从头开始生成歌词。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自监督的表示学习框架，使用对比对齐损失处理旋律和歌词；引入了'音节'(sylphone)这一基于音素身份和元音重音激活的歌词音节级别的新表示方法；无需对齐注释即可利用现有的大量配对旋律和歌词的歌曲。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验结果和直观示例，证明了该方法能够将旋律与连贯且可唱的歌词有效匹配。&lt;h4&gt;结论&lt;/h4&gt;通过开源代码和提供匹配示例，展示了旋律-歌词匹配方法的有效性和实用性，为音乐信息检索领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;音乐和歌词之间的联系远超语义联系。两种模态中的概念对，如节奏和韵律、音符时长和音节重音、结构对应关系等，在音乐信息检索领域提出了一个引人注目但很少被探索的方向。在本文中，我们提出了旋律-歌词匹配(MLM)，一个新任务，可以从文本源中检索给定符号旋律的潜在歌词。MLM本质上是利用旋律和歌词之间的关系，而非从头开始生成歌词。我们提出了一个带有对比对齐损失的自监督表示学习框架，用于处理旋律和歌词。这有可能利用现有的大量配对旋律和歌词的歌曲，无需对齐注释。此外，我们引入了'音节'，这是一种基于音素身份和元音重音激活的歌词音节级别的新表示方法。我们通过实验结果和直观示例证明了我们的方法可以将旋律与连贯且可唱的歌词相匹配。我们开源了代码，并在配套网页上提供了匹配示例：https://github.com/changhongw/mlm。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The connection between music and lyrics is far beyond semantic bonds.Conceptual pairs in the two modalities such as rhythm and rhyme, note durationand syllabic stress, and structure correspondence, raise a compelling yetseldom-explored direction in the field of music information retrieval. In thispaper, we present melody-lyrics matching (MLM), a new task which retrievespotential lyrics for a given symbolic melody from text sources. Rather thangenerating lyrics from scratch, MLM essentially exploits the relationshipsbetween melody and lyrics. We propose a self-supervised representation learningframework with contrastive alignment loss for melody and lyrics. This has thepotential to leverage the abundance of existing songs with paired melody andlyrics. No alignment annotations are required. Additionally, we introducesylphone, a novel representation for lyrics at syllable-level activated byphoneme identity and vowel stress. We demonstrate that our method can matchmelody with coherent and singable lyrics with empirical results and intuitiveexamples. We open source code and provide matching examples on the companionwebpage: https://github.com/changhongw/mlm.</description>
      <author>example@mail.com (Changhong Wang, Michel Olvera, Gaël Richard)</author>
      <guid isPermaLink="false">2508.00123v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity</title>
      <link>http://arxiv.org/abs/2508.00043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了两种地形神经网络约束方法(权重相似性和激活相似性)对模型性能的影响，发现权重相似性约束能产生更鲁棒的表示，提高对噪声和权重扰动的抵抗力，并增强功能定位。&lt;h4&gt;背景&lt;/h4&gt;地形神经网络是能够模拟大脑空间和功能组织的计算模型。神经网络中的地形约束可以通过多种方式实现，可能对网络学习的表示产生不同影响，但这些不同实现方式的影响尚未得到系统性研究。&lt;h4&gt;目的&lt;/h4&gt;比较两种空间约束下的地形卷积神经网络(权重相似性和激活相似性)对模型性能的影响，评估它们在分类准确率、鲁棒性和表示空间组织方面的差异。&lt;h4&gt;方法&lt;/h4&gt;使用两种空间约束训练地形卷积神经网络：权重相似性推动相邻单元发展相似的输入权重，激活相似性强制单元激活相似性。评估这些模型在分类准确率、对权重扰动和输入退化的鲁棒性以及学习表示的空间组织方面的表现。&lt;h4&gt;主要发现&lt;/h4&gt;与激活相似性和标准CNN相比，权重相似性提供了三个主要优势：提高了对噪声的鲁棒性；更高的输入敏感性，反映在更高的激活方差上；更强的功能定位，具有相似激活的单元位于更近的距离。此外，权重相似性还产生了单元方向调谐、对称敏感性和离心率剖面的差异，表明这种空间约束对网络表示几何形状的影响。&lt;h4&gt;结论&lt;/h4&gt;在端到端训练过程中，权重相似性约束比激活相似性或非地形CNN产生更鲁棒的表示。这些发现还表明，基于权重的空间约束可以在生物物理启发的模型中塑造特征学习和功能组织。&lt;h4&gt;翻译&lt;/h4&gt;地形神经网络是能够模拟大脑空间和功能组织的计算模型。神经网络中的地形约束可以通过多种方式实现，可能对网络学习的表示产生不同影响。然而，这些不同实现方式的影响尚未得到系统性研究。为此，我们比较了使用两种空间约束训练的地形卷积神经网络：权重相似性，它推动相邻单元发展相似的输入权重；和激活相似性，它强制单元激活相似性。我们评估了这些模型在分类准确率、对权重扰动和输入退化的鲁棒性以及学习表示的空间组织方面的表现。与激活相似性和标准CNN相比，权重相似性提供了三个主要优势：i) 提高了对噪声的鲁棒性，在权重损坏情况下也显示出更高的准确率；ii) 更高的输入敏感性，反映在更高的激活方差上；iii) 更强的功能定位，具有相似激活的单元位于更近的距离。此外，权重相似性还产生了单元方向调谐、对称敏感性和离心率剖面的差异，表明这种空间约束对网络表示几何形状的影响。我们的研究结果表明，在端到端训练过程中，权重相似性约束比激活相似性或非地形CNN产生更鲁棒的表示。这些发现还表明，基于权重的空间约束可以在生物物理启发的模型中塑造特征学习和功能组织。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Topographic neural networks are computational models that can simulate thespatial and functional organization of the brain. Topographic constraints inneural networks can be implemented in multiple ways, with potentially differentimpacts on the representations learned by the network. The impact of suchdifferent implementations has not been systematically examined. To this end,here we compare topographic convolutional neural networks trained with twospatial constraints: Weight Similarity (WS), which pushes neighboring units todevelop similar incoming weights, and Activation Similarity (AS), whichenforces similarity in unit activations. We evaluate the resulting models onclassification accuracy, robustness to weight perturbations and inputdegradation, and the spatial organization of learned representations. Comparedto both AS and standard CNNs, WS provided three main advantages: i) improvedrobustness to noise, also showing higher accuracy under weight corruption; ii)greater input sensitivity, reflected in higher activation variance; and iii)stronger functional localization, with units showing similar activationspositioned at closer distances. In addition, WS produced differences inorientation tuning, symmetry sensitivity, and eccentricity profiles of units,indicating an influence of this spatial constraint on the representationalgeometry of the network. Our findings suggest that during end-to-end training,WS constraints produce more robust representations than AS or non-topographicCNNs. These findings also suggest that weight-based spatial constraints canshape feature learning and functional organization in biophysical inspiredmodels.</description>
      <author>example@mail.com (Nhut Truong, Uri Hasson)</author>
      <guid isPermaLink="false">2508.00043v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items</title>
      <link>http://arxiv.org/abs/2507.22268v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MMSC的自监督多模态关系物品表征学习框架，用于推断可替代和互补物品，解决了用户行为数据噪声和数据稀疏性问题，显著提升了推荐性能。&lt;h4&gt;背景&lt;/h4&gt;现有方法主要使用图神经网络建模物品关联或利用物品内容信息，但忽略了用户行为数据噪声和长尾分布导致的数据稀疏性问题。&lt;h4&gt;目的&lt;/h4&gt;提出MMSC框架以解决用户行为数据噪声和数据稀疏性问题，提高可替代和互补物品推荐的准确性。&lt;h4&gt;方法&lt;/h4&gt;MMSC包含三个主要组件：多模态物品表征学习模块、自监督行为表征学习模块和层次化表征聚合机制，并利用大语言模型生成增强训练数据进行去噪。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上实验表明，MMSC在可替代推荐方面比现有基线方法高出26.1%，在互补推荐方面高出39.2%，且能有效建模冷启动物品。&lt;h4&gt;结论&lt;/h4&gt;MMSC框架通过结合多模态表征学习、自监督行为表征学习和层次化表征聚合，有效解决了用户行为数据噪声和数据稀疏性问题，显著提升了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种新颖的自监督多模态关系物品表征学习框架，旨在推断可替代和互补物品。现有方法主要关注使用图神经网络(GNN)建模从用户行为推断的物品关联，或利用物品内容信息。然而，这些方法经常忽略关键挑战，如嘈杂的用户行为数据以及由于这些行为的长尾分布导致的数据稀疏性。在本文中，我们提出了MMSC，一个自监督多模态关系物品表征学习框架，以应对这些挑战。具体而言，MMSC包含三个主要组件：(1) 多模态物品表征学习模块，利用多模态基础模型并从物品元数据中学习；(2) 自监督行为表征学习模块，对用户行为数据进行去噪并从中学习；(3) 层次化表征聚合机制，在语义和任务两个级别整合物品表征。此外，我们利用大语言模型生成增强训练数据，进一步训练过程中的去噪。我们在五个真实世界数据集上进行了广泛实验，表明MMSC在可替代推荐方面比现有基线方法高出26.1%，在互补推荐方面高出39.2%。此外，我们经验性地证明MMSC在建模冷启动物品方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel self-supervised multi-modal relational itemrepresentation learning framework designed to infer substitutable andcomplementary items. Existing approaches primarily focus on modeling item-itemassociations deduced from user behaviors using graph neural networks (GNNs) orleveraging item content information. However, these methods often overlookcritical challenges, such as noisy user behavior data and data sparsity due tothe long-tailed distribution of these behaviors. In this paper, we proposeMMSC, a self-supervised multi-modal relational item representation learningframework to address these challenges. Specifically, MMSC consists of threemain components: (1) a multi-modal item representation learning module thatleverages a multi-modal foundational model and learns from item metadata, (2) aself-supervised behavior-based representation learning module that denoises andlearns from user behavior data, and (3) a hierarchical representationaggregation mechanism that integrates item representations at both the semanticand task levels. Additionally, we leverage LLMs to generate augmented trainingdata, further enhancing the denoising process during training. We conductextensive experiments on five real-world datasets, showing that MMSCoutperforms existing baselines by 26.1% for substitutable recommendation and39.2% for complementary recommendation. In addition, we empirically show thatMMSC is effective in modeling cold-start items.</description>
      <author>example@mail.com (Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram)</author>
      <guid isPermaLink="false">2507.22268v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering</title>
      <link>http://arxiv.org/abs/2507.20980v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为LargeMvC-Net的新型深度网络架构，用于解决大规模多视图聚类问题，通过将优化问题展开为专门的模块实现更好的结构和优化可追溯性。&lt;h4&gt;背景&lt;/h4&gt;基于深度锚点的多视图聚类方法利用代表性锚点减少大规模聚类的计算复杂度，但现有方法通常以启发式或任务无关方式整合锚点结构，忽略了基于锚点的聚类的核心结构需求和优化原则。&lt;h4&gt;目的&lt;/h4&gt;重新审视大规模基于锚点的多视图聚类的底层优化问题，并将其迭代解展开为一种新颖的深度网络架构，以弥合现有方法的设计缺陷。&lt;h4&gt;方法&lt;/h4&gt;提出的模型将基于锚点的聚类过程分解为三个模块：RepresentModule（表示学习）、NoiseModule（噪声抑制）和AnchorModule（锚点指标估计）。每个模块通过将原始优化过程的步骤展开到专用网络组件中导出，并提供了一种无监督重构损失来对齐视图与锚点诱导的潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;在多个大规模多视图基准上的广泛实验表明，LargeMvC-Net在有效性和扩展性方面持续优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;LargeMvC-Net通过优化问题展开的方式提供了更好的结构和优化可追溯性，是一种有效的大规模多视图聚类解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于深度锚点的多视图聚类方法通过利用代表性锚点来减少大规模聚类的计算复杂度，从而提高神经网络的扩展性。尽管它们具有扩展性优势，但现有方法通常以启发式或任务无关的方式整合锚点结构，通过事后图构建或作为消息传递的辅助组件。此类设计忽略了基于锚点的聚类的核心结构需求，忽视了关键的优化原则。为了弥合这一差距，我们重新审视了大规模基于锚点的多视图聚类的底层优化问题，并将其迭代解展开为一种新颖的深度网络架构，称为LargeMvC-Net。所提出的模型将基于锚点的聚类过程分解为三个模块：RepresentModule、NoiseModule和AnchorModule，分别对应表示学习、噪声抑制和锚点指标估计。每个模块都是通过将原始优化过程的步骤展开到专用的网络组件中导出的，提供了结构清晰性和优化可追溯性。此外，一种无监督的重构损失将每个视图与锚点诱导的潜在空间对齐，鼓励跨视图保持一致的聚类结构。在几个大规模多视图基准上的广泛实验表明，LargeMvC-Net在有效性和扩展性方面都持续优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep anchor-based multi-view clustering methods enhance the scalability ofneural networks by utilizing representative anchors to reduce the computationalcomplexity of large-scale clustering. Despite their scalability advantages,existing approaches often incorporate anchor structures in a heuristic ortask-agnostic manner, either through post-hoc graph construction or asauxiliary components for message passing. Such designs overlook the corestructural demands of anchor-based clustering, neglecting key optimizationprinciples. To bridge this gap, we revisit the underlying optimization problemof large-scale anchor-based multi-view clustering and unfold its iterativesolution into a novel deep network architecture, termed LargeMvC-Net. Theproposed model decomposes the anchor-based clustering process into threemodules: RepresentModule, NoiseModule, and AnchorModule, corresponding torepresentation learning, noise suppression, and anchor indicator estimation.Each module is derived by unfolding a step of the original optimizationprocedure into a dedicated network component, providing structural clarity andoptimization traceability. In addition, an unsupervised reconstruction lossaligns each view with the anchor-induced latent space, encouraging consistentclustering structures across views. Extensive experiments on severallarge-scale multi-view benchmarks show that LargeMvC-Net consistentlyoutperforms state-of-the-art methods in terms of both effectiveness andscalability.</description>
      <author>example@mail.com (Shide Du, Chunming Wu, Zihan Fang, Wendi Zhao, Yilin Wu, Changwei Wang, Shiping Wang)</author>
      <guid isPermaLink="false">2507.20980v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Three-dimentional reconstruction of complex, dynamic population canopy architecture for crops with a novel point cloud completion model: A case study in Brassica napus rapeseed</title>
      <link>http://arxiv.org/abs/2506.18292v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CP-PCN的方法，用于油菜作物复杂动态群体冠层架构的三维重建，通过点云补全模型解决冠层内部遮挡问题，实现了高精度的冠层架构描述，提高了产量预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确描述完整冠层架构对于评估作物光合作用和产量性能以及指导理想株型设计至关重要。尽管已有各种传感技术用于单株和冠层的三维重建，但由于复杂冠层架构之间的严重遮挡，这些技术未能获得准确的冠层架构描述。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法，用于油菜作物复杂动态群体冠层架构的三维重建，以解决冠层内部遮挡问题，从而更准确地评估作物光合作用和产量性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种点云补全模型CP-PCN（作物群体点云补全网络）；开发了完整的点云生成框架，用于自动标注训练数据集，区分冠层内的表面点和遮挡点；设计了包含多分辨率动态图卷积编码器(MRDG)和点金字塔解码器(PPD)的CP-PCN网络；提出了动态图卷积特征提取器(DGCFE)模块，以捕获整个油菜生长过程中的结构变化。&lt;h4&gt;主要发现&lt;/h4&gt;CP-PCN在四个生长阶段上的查普距离(CD)值达到3.35厘米-4.51厘米，优于基于Transformer的最先进方法(PoinTr)；消融研究证实了MRDG和DGCFE模块的有效性；使用CP-PCN开发的角果效率指数相比使用不完整点云，提高了油菜产量预测的整体准确性11.2%。&lt;h4&gt;结论&lt;/h4&gt;CP-PCN流程有潜力扩展到其他作物，显著推进田间群体冠层架构的定量分析，为作物光合作用评估和产量预测提供了更准确的方法。&lt;h4&gt;翻译&lt;/h4&gt;完整冠层架构的定量描述对于准确评估作物光合作用和产量性能以及指导理想株型设计至关重要。尽管已经开发了各种传感技术用于单株和冠层的三维重建，但由于复杂冠层架构之间的严重遮挡，它们未能获得准确的冠层架构描述。我们提出了一种有效的方法，使用新颖的点云补全模型进行油菜作物复杂动态群体冠层架构的三维重建。开发了完整的点云生成框架，用于通过区分冠层内的表面点和遮挡点来自动标注训练数据集。随后设计了作物群体点云补全网络(CP-PCN)，采用多分辨率动态图卷积编码器(MRDG)和点金字塔解码器(PPD)来预测遮挡点。为了进一步增强特征提取，提出了动态图卷积特征提取器(DGCFE)模块，以捕获整个油菜生长过程中的结构变化。结果表明，CP-PCN在四个生长阶段上的查普距离(CD)值达到3.35厘米-4.51厘米，优于基于Transformer的最先进方法(PoinTr)。消融研究证实了MRDG和DGCFE模块的有效性。此外，验证实验表明，从CP-PCN开发的角果效率指数相比使用不完整点云，提高了油菜产量预测的整体准确性11.2%。CP-PCN流程有潜力扩展到其他作物，显著推进田间群体冠层架构的定量分析。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何准确重建复杂、动态的作物群体冠层三维结构的问题。由于冠层内部严重遮挡，现有三维重建技术无法获取完整的冠层信息，特别是内部被遮挡部分。这个问题很重要，因为完整的冠层结构定量描述对于准确评估作物光合作用和产量性能至关重要，直接影响光分布和截获效率，进而影响作物产量。理想冠层结构设计对于提高作物产量潜力，特别是对于油菜这种生长周期中冠层结构多样性大的作物尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有三维重建技术的局限性：深度相机和激光扫描仅适用于简单目标；LiDAR成本高且存在光吸收问题；多视图成像仍无法获取完整冠层结构。同时，现有点云补全方法主要针对刚性物体设计，难以处理复杂植物冠层。基于这些分析，作者借鉴了生成对抗网络(GAN)框架和图神经网络(GNN)处理点云数据的思想，设计了专门针对作物群体冠层的CP-PCN网络。作者还创新性地引入多分辨率特征提取和动态图卷积策略，以适应不同生长阶段的冠层结构变化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用虚拟-真实集成(VRI)方法生成高质量训练数据集，设计专门针对作物群体冠层的点云补全网络CP-PCN，利用多分辨率动态图卷积编码器(MRDG)和点金字塔解码器(PPD)预测被遮挡点，并通过动态图卷积特征提取器(DGCFE)捕捉生长周期中的结构变化。整体流程包括：1)收集个体植物3D数据并基于种植模式生成完整群体冠层；2)使用遮挡点检测算法区分表面点和被遮挡点；3)训练CP-PCN模型，输入表面点云预测被遮挡点；4)将模型应用于田间采集的不完整点云，补全冠层结构；5)从完整点云提取特征并用于产量估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出虚拟-真实集成(VRI)方法和遮挡点检测算法，生成更真实的训练数据集；2)设计CP-PCN网络，提出多分辨率动态图卷积编码器(MRDG)和动态图卷积特征提取器(DGCFE)；3)提出角果效率指数(SEI)作为产量评估指标。相比之前工作，本文专注于作物群体而非单株冠层重建，考虑了生长过程中的结构动态变化，解决了冠层内部严重遮挡问题，并将完整冠层结构与产量预测相结合，方法可扩展到其他作物，推动了田间群体冠层结构的定量分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出创新的点云补全模型和虚拟-真实集成数据生成方法，实现了复杂作物群体冠层结构的高精度三维重建，显著提高了产量预测精度，为作物理想株型设计提供了新的技术手段。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-06-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantitative descriptions of the complete canopy architecture are essentialfor accurately evaluating crop photosynthesis and yield performance to guideideotype design. Although various sensing technologies have been developed forthree-dimensional (3D) reconstruction of individual plants and canopies, theyfailed to obtain an accurate description of canopy architectures due to severeocclusion among complex canopy architectures. We proposed an effective methodfor 3D reconstruction of complex, dynamic population canopy architecture forrapeseed crops with a novel point cloud completion model. A complete pointcloud generation framework was developed for automated annotation of thetraining dataset by distinguishing surface points from occluded points withincanopies. The crop population point cloud completion network (CP-PCN) was thendesigned with a multi-resolution dynamic graph convolutional encoder (MRDG) anda point pyramid decoder (PPD) to predict occluded points. To further enhancefeature extraction, a dynamic graph convolutional feature extractor (DGCFE)module was proposed to capture structural variations over the whole rapeseedgrowth period. The results demonstrated that CP-PCN achieved chamfer distance(CD) values of 3.35 cm -4.51 cm over four growth stages, outperforming thestate-of-the-art transformer-based method (PoinTr). Ablation studies confirmedthe effectiveness of the MRDG and DGCFE modules. Moreover, the validationexperiment demonstrated that the silique efficiency index developed from CP-PCNimproved the overall accuracy of rapeseed yield prediction by 11.2% compared tothat of using incomplete point clouds. The CP-PCN pipeline has the potential tobe extended to other crops, significantly advancing the quantitatively analysisof in-field population canopy architectures.</description>
      <author>example@mail.com (Ziyue Guo, Xin Yang, Yutao Shen, Yang Zhu, Lixi Jiang, Haiyan Cen)</author>
      <guid isPermaLink="false">2506.18292v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>GECO: Geometrically Consistent Embedding with Lightspeed Inference</title>
      <link>http://arxiv.org/abs/2508.00746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GECO是一种基于最优传输的自监督视觉框架，能够产生几何一致的特征，有效区分不同部分的几何特性，同时实现高效运行。&lt;h4&gt;背景&lt;/h4&gt;自监督视觉基础模型能够捕捉语义对应关系，但通常缺乏对底层3D几何结构的感知能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法缺乏对底层3D几何结构感知的问题，开发能基于几何区分不同部分的视觉特征表示方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于最优传输的训练框架GECO，超越关键点监督，支持在遮挡和遮挡解除情况下的训练，采用轻量级架构实现高效运行。&lt;h4&gt;主要发现&lt;/h4&gt;GECO比之前的方法快98.2%，在PFPascal、APK和CUB数据集上达到最先进性能，PCK指标分别提高了6.0%、6.2%和4.1%。同时发现PCK指标不足以捕捉几何质量。&lt;h4&gt;结论&lt;/h4&gt;GECO成功实现了几何一致的特征学习，为更几何感知的特征学习提供了新指标和见解。&lt;h4&gt;翻译&lt;/h4&gt;最近的特征学习进展表明，自监督视觉基础模型能够捕捉语义对应关系，但通常缺乏对底层3D几何结构的感知能力。GECO通过产生几何一致的特征来填补这一空白，这些特征能够基于几何语义区分不同部分（如左右眼、前后腿）。我们提出了一种基于最优传输的训练框架，使得监督可以超越关键点，即使在遮挡和遮挡解除的情况下也能工作。凭借轻量级架构，GECO以30fps的速度运行，比之前的方法快98.2%，同时在PFPascal、APK和CUB上取得了最先进的性能，分别将PCK提高了6.0%、6.2%和4.1%。最后，我们表明单独的PCK不足以捕捉几何质量，并为更几何感知的特征学习引入了新的指标和见解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉基础模型缺乏对底层3D几何认识的问题，特别是难以区分对称部分（如左右眼、前后腿）的几何属性。这个问题很重要，因为缺乏几何意识会导致实际应用中出现严重的人工制品，例如在3D重建中错误重建动物或家具，而现有几何感知方法速度太慢难以实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：它们依赖基于argmax的分配，无法处理遮挡情况，只提供稀疏监督信号。借鉴了最优传输（特别是Sinkhorn算法）的概念来实现可微的软分配公式，处理部分分配问题并提供更强监督。同时使用了DINOv2作为预训练模型和LoRA适配技术进行微调，这些都是对现有工作的合理借鉴。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用最优传输学习几何一致的特征表示，处理部分分配问题并提供更强监督。流程：1)使用DINOv2-B模型和LoRA适配器；2)计算图像特征间的余弦相似度矩阵；3)增强矩阵添加bin；4)通过可微分OT层计算最优传输计划；5)使用KL正则化软分配；6)应用二元交叉熵损失训练模型预测正确分配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)基于最优传输的软分配损失函数和轻量级架构；2)不同的最优传输模块边缘分布创建方式，无需可训练参数；3)作为特征编码器而非特征匹配器；4)提出新评估指标PGCK补充传统PCK。相比Geo方法，GECO速度更快(30fps，快98.2%)、模型更小(332MB vs 9GB)、性能更优，且不依赖交叉注意力机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GECO通过基于最优传输的软分配损失函数和轻量级架构，实现了高效且几何一致的视觉特征学习，在保持实时性能的同时显著提升了关键点匹配的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in feature learning have shown that self-supervised visionfoundation models can capture semantic correspondences but often lack awarenessof underlying 3D geometry. GECO addresses this gap by producing geometricallycoherent features that semantically distinguish parts based on geometry (e.g.,left/right eyes, front/back legs). We propose a training framework based onoptimal transport, enabling supervision beyond keypoints, even under occlusionsand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%faster than prior methods, while achieving state-of-the-art performance onPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.Finally, we show that PCK alone is insufficient to capture geometric qualityand introduce new metrics and insights for more geometry-aware featurelearning. Link to project page:https://reginehartwig.github.io/publications/geco/</description>
      <author>example@mail.com (Regine Hartwig, Dominik Muhle, Riccardo Marin, Daniel Cremers)</author>
      <guid isPermaLink="false">2508.00746v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems</title>
      <link>http://arxiv.org/abs/2508.00721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FMPlug是一种新型插件框架，用于增强基础流匹配(FM)先验以解决病态逆问题。它利用观测对象与期望对象间的相似性以及生成流的高斯性，通过时间自适应预热策略和强化高斯性正则化，释放了领域不可知基础模型的潜力，并在图像处理任务上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;传统解决病态逆问题的方法依赖于领域特定或未训练的先验，存在局限性。&lt;h4&gt;目的&lt;/h4&gt;增强基础流匹配(FM)先验，解决病态逆问题，并释放领域不可知基础模型的真正潜力。&lt;h4&gt;方法&lt;/h4&gt;开发FMPlug插件框架，利用观测对象与期望对象之间的相似性以及生成流的高斯性两个关键洞见；引入时间自适应预热策略和强化高斯性正则化技术。&lt;h4&gt;主要发现&lt;/h4&gt;FMPlug在图像超分辨率和高斯去模糊任务上，以显著优势击败了使用基础FM先验的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;FMPlug通过巧妙利用简单而强大的洞见，结合时间自适应预热策略和强化高斯性正则化，成功释放了领域不可知基础模型的真正潜力，有效解决了病态逆问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了FMPlug，一种新颖的插件框架，用于增强基础流匹配(FM)先验以解决病态逆问题。与传统依赖于领域特定或未训练先验的方法不同，FMPlug巧妙地利用了两个简单而强大的洞见：观测对象与期望对象之间的相似性以及生成流的高斯性。通过引入时间自适应预热策略和强化的高斯性正则化，FMPlug释放了领域不可知基础模型的真正潜力。我们的方法在图像超分辨率和高斯去模糊任务上，以显著优势击败了使用基础FM先验的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present FMPlug, a novel plug-in framework that enhances foundationflow-matching (FM) priors for solving ill-posed inverse problems. Unliketraditional approaches that rely on domain-specific or untrained priors, FMPlugsmartly leverages two simple but powerful insights: the similarity betweenobserved and desired objects and the Gaussianity of generative flows. Byintroducing a time-adaptive warm-up strategy and sharp Gaussianityregularization, FMPlug unlocks the true potential of domain-agnostic foundationmodels. Our method beats state-of-the-art methods that use foundation FM priorsby significant margins, on image super-resolution and Gaussian deblurring.</description>
      <author>example@mail.com (Yuxiang Wan, Ryan Devera, Wenjie Zhang, Ju Sun)</author>
      <guid isPermaLink="false">2508.00721v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources</title>
      <link>http://arxiv.org/abs/2508.00627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IAMAP是一个用户友好的QGIS插件，解决了深度学习在遥感应用中的三个主要限制：需要大量数据集、计算资源和编程技能。它利用自监督学习策略提供的特征提取器，使非AI专业人员能够高效使用深度学习方法。&lt;h4&gt;背景&lt;/h4&gt;遥感技术随着人工智能方法的快速发展已进入新时代，但深度学习的实施主要局限于专业人士且不实用，因为它需要大量参考数据集、计算资源和编程技能。&lt;h4&gt;目的&lt;/h4&gt;开发IAMAP插件，以一种简单而灵活的方式解决深度学习在遥感应用中的三个主要挑战，使非AI专业人员也能利用这些先进技术。&lt;h4&gt;方法&lt;/h4&gt;IAMAP基于自监督学习策略提供的基础模型，提供界面让用户执行：(i)使用多种深度学习架构提取图像特征；(ii)使用内置算法降维；(iii)对特征或其降维表示进行聚类；(iv)生成特征相似性地图；(v)校准和验证监督机器学习模型进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过使非AI专业人员能够利用最新深度学习方法提供的高质量特征，而无需GPU容量或大量参考数据集，IAMAP实现了计算高效且节能的深度学习方法。&lt;h4&gt;结论&lt;/h4&gt;IAMAP推动了计算高效且节能的深度学习方法的普及化，降低了遥感技术中深度学习的应用门槛。&lt;h4&gt;翻译&lt;/h4&gt;遥感技术随着人工智能方法的快速发展已进入新时代。然而，深度学习的实施主要局限于专业人士，且不实用，因为它通常需要：(i)大量的参考数据集用于模型训练和验证；(ii)大量的计算资源；(iii)强大的编程技能。在这里，我们介绍IAMAP，一个用户友好的QGIS插件，以一种简单而灵活的方式解决这三个挑战。IAMAP建立在最新的自监督学习策略进展基础上，这些策略提供了强大的特征提取器，通常被称为基础模型。这些通用模型可以在少样本或零样本场景中可靠使用（即很少或不需要微调）。IAMAP的界面允许用户简化遥感图像分析中的几个关键步骤：(i)使用各种深度学习架构提取图像特征；(ii)使用内置算法降维；(iii)对特征或其降维表示进行聚类；(iv)生成特征相似性地图；(v)校准和验证监督机器学习模型以进行预测。通过使非AI专业人员能够利用最新深度学习方法提供的高质量特征，而无需GPU容量或大量参考数据集，IAMAP有助于实现计算高效且节能的深度学习方法的普及化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing has entered a new era with the rapid development of artificialintelligence approaches. However, the implementation of deep learning haslargely remained restricted to specialists and has been impractical because itoften requires (i) large reference datasets for model training and validation;(ii) substantial computing resources; and (iii) strong coding skills. Here, weintroduce IAMAP, a user-friendly QGIS plugin that addresses these threechallenges in an easy yet flexible way. IAMAP builds on recent advancements inself-supervised learning strategies, which now provide robust featureextractors, often referred to as foundation models. These generalist models canoften be reliably used in few-shot or zero-shot scenarios (i.e., with little tono fine-tuning). IAMAP's interface allows users to streamline several key stepsin remote sensing image analysis: (i) extracting image features using a widerange of deep learning architectures; (ii) reducing dimensionality withbuilt-in algorithms; (iii) performing clustering on features or their reducedrepresentations; (iv) generating feature similarity maps; and (v) calibratingand validating supervised machine learning models for prediction. By enablingnon-AI specialists to leverage the high-quality features provided by recentdeep learning approaches without requiring GPU capacity or extensive referencedatasets, IAMAP contributes to the democratization of computationally efficientand energy-conscious deep learning methods.</description>
      <author>example@mail.com (Paul Tresson, Pierre Le Coz, Hadrien Tulet, Anthony Malkassian, Maxime Réjou Méchain)</author>
      <guid isPermaLink="false">2508.00627v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications</title>
      <link>http://arxiv.org/abs/2508.00583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了大型视觉模型(LVMs)在视觉智能中的基础性作用及其在物联网场景中的应用。作者研究了LVM的功能和核心架构，探索了其在无线通信各层的应用，并提出了一种渐进式微调框架，通过案例研究证明了其在无人机互联网任务中优于传统CNNs。&lt;h4&gt;背景&lt;/h4&gt;大型视觉模型已成为视觉智能的基础性范式，在多样化视觉任务中取得最先进性能。LVMs的最新进展促进了它们在物联网场景中的集成，为视觉辅助的网络优化提供了更好的泛化能力和适应性。&lt;h4&gt;目的&lt;/h4&gt;研究LVM的功能和核心架构，探索其在无线通信中的应用，针对LVM模型大和无线领域重新训练挑战提出渐进式微调框架，并通过案例研究验证框架有效性。&lt;h4&gt;方法&lt;/h4&gt;分析LVM在分类、分割、生成和多模态视觉处理方面的能力；探索LVM在无线通信物理层、网络层和应用层的应用；提出渐进式微调框架逐步调整预训练LVMs以优化多个IoT任务；在低空经济网络中进行案例研究比较与传统CNNs的性能差异。&lt;h4&gt;主要发现&lt;/h4&gt;大型视觉模型在视觉智能领域表现出色且适用性广；LVMs能有效集成到物联网场景提供更好的泛化能力和适应性；所提出的渐进式微调框架能有效适应预训练LVMs用于多任务优化；在波束成形和定位任务中，所提框架优于传统CNNs。&lt;h4&gt;结论&lt;/h4&gt;将大型视觉模型集成到智能无线系统是一个有前景的方向，特别是在处理多任务优化时。渐进式微调框架为在资源受限的无线环境中有效利用LVMs提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉模型已成为视觉智能的基础性范式，在多样化的视觉任务中取得了最先进的性能。LVMs的最新进展促进了它们在物联网场景中的集成，为视觉辅助的网络优化提供了更好的泛化能力和适应性。在本文中，我们首先研究了LVM的功能和核心架构，强调了它们在分类、分割、生成和多模态视觉处理方面的能力。然后，我们探索了LVM在无线通信中的各种应用，涵盖了物理层、网络层和应用层的代表性任务。此外，鉴于LVM的模型规模大以及无线领域模型重新训练的挑战，我们提出了一种渐进式微调框架，逐步调整预训练的LVMs，以优化多个IoT任务。在低空经济网络中的案例研究证明了所提框架在无人机互联网的波束成形和定位任务中优于传统CNNs，强调了将LVMs集成到智能无线系统中的有前景方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision models (LVMs) have emerged as a foundational paradigm in visualintelligence, achieving state-of-the-art performance across diverse visualtasks. Recent advances in LVMs have facilitated their integration into Internetof Things (IoT) scenarios, offering superior generalization and adaptabilityfor vision-assisted network optimization. In this paper, we first investigatethe functionalities and core architectures of LVMs, highlighting theircapabilities across classification, segmentation, generation, and multimodalvisual processing. We then explore a variety of LVM applications in wirelesscommunications, covering representative tasks across the physical layer,network layer, and application layer. Furthermore, given the substantial modelsize of LVMs and the challenges of model retraining in wireless domains, wepropose a progressive fine-tuning framework that incrementally adaptspretrained LVMs for joint optimization of multiple IoT tasks. A case study inlow-altitude economy networks (LAENets) demonstrates the effectiveness of theproposed framework over conventional CNNs in joint beamforming and positioningtasks for Internet of drones, underscoring a promising direction forintegrating LVMs into intelligent wireless systems.</description>
      <author>example@mail.com (Yunting Xu, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Deepu Rajan, Liang Yu, Haibo Zhou, Abbas Jamalipour, Xianbin Wang)</author>
      <guid isPermaLink="false">2508.00583v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer</title>
      <link>http://arxiv.org/abs/2508.00477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LAMIC是一种布局感知的多图像合成框架，首次以无需训练的方式将单参考扩散模型扩展到多参考场景。它通过两种即插即用的注意力机制实现了身份保持、背景保留、布局控制和提示跟随等能力，无需任何训练或微调，展示了强大的零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在可控图像合成领域，从多个参考图像生成连贯且一致的图像，同时保持空间布局意识仍然是一个开放的挑战。现有的多参考基线方法在性能上存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练的框架，能够从多个参考图像生成连贯且一致的图像，同时保持空间布局意识，并超越现有多参考基线方法的性能。&lt;h4&gt;方法&lt;/h4&gt;LAMIC基于MMDiT模型，引入了两种即插即用的注意力机制：1）组隔离注意力（GIA）以增强实体解耦；2）区域调制注意力（RMA）以实现布局感知的生成。同时，引入了三个评估指标：包含比率（IN-R）和填充比率（FI-R）用于评估布局控制；背景相似度（BG-S）用于测量背景一致性。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，LAMIC在大多数主要指标上达到了最先进的性能：在所有设置中，它在ID-S、BG-S、IN-R和平均分数方面持续优于现有的多参考基线，并在复杂合成任务中取得了最佳的DPG。这些结果证明了LAMIC在身份保持、背景保留、布局控制和提示跟随方面的优越能力。&lt;h4&gt;结论&lt;/h4&gt;通过继承先进单参考模型的优势并实现向多图像场景的无缝扩展，LAMIC为可控多图像合成建立了一种新的无需训练的范式。随着基础模型的不断发展，LAMIC的性能有望相应提升。该研究的实现可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;在可控图像合成中，从多个参考图像生成具有空间布局意识的连贯且一致的图像仍然是一个开放的挑战。我们提出了LAMIC，一种布局感知的多图像合成框架，首次以无需训练的方式将单参考扩散模型扩展到多参考场景。基于MMDiT模型，LAMIC引入了两种即插即用的注意力机制：1）组隔离注意力（GIA）以增强实体解耦；2）区域调制注意力（RMA）以实现布局感知的生成。为了全面评估模型能力，我们进一步引入了三个指标：1）包含比率（IN-R）和填充比率（FI-R）用于评估布局控制；2）背景相似度（BG-S）用于测量背景一致性。广泛的实验表明，LAMIC在大多数主要指标上达到了最先进的性能：在所有设置中，它在ID-S、BG-S、IN-R和平均分数方面持续优于现有的多参考基线，并在复杂合成任务中取得了最佳的DPG。这些结果证明了LAMIC在身份保持、背景保留、布局控制和提示跟随方面的优越能力，均无需任何训练或微调，展示了强大的零样本泛化能力。通过继承先进单参考模型的优势并实现向多图像场景的无缝扩展，LAMIC为可控多图像合成建立了一种新的无需训练的范式。随着基础模型的不断发展，LAMIC的性能有望相应提升。我们的实现在https://github.com/Suchenl/LAMIC上可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In controllable image synthesis, generating coherent and consistent imagesfrom multiple references with spatial layout awareness remains an openchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition frameworkthat, for the first time, extends single-reference diffusion models tomulti-reference scenarios in a training-free manner. Built upon the MMDiTmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) GroupIsolation Attention (GIA) to enhance entity disentanglement; and 2)Region-Modulated Attention (RMA) to enable layout-aware generation. Tocomprehensively evaluate model capabilities, we further introduce threemetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layoutcontrol; and 2) Background Similarity (BG-S) for measuring backgroundconsistency. Extensive experiments show that LAMIC achieves state-of-the-artperformance across most major metrics: it consistently outperforms existingmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across allsettings, and achieves the best DPG in complex composition tasks. These resultsdemonstrate LAMIC's superior abilities in identity keeping, backgroundpreservation, layout control, and prompt-following, all achieved without anytraining or fine-tuning, showcasing strong zero-shot generalization ability. Byinheriting the strengths of advanced single-reference models and enablingseamless extension to multi-image scenarios, LAMIC establishes a newtraining-free paradigm for controllable multi-image composition. As foundationmodels continue to evolve, LAMIC's performance is expected to scaleaccordingly. Our implementation is available at:https://github.com/Suchenl/LAMIC.</description>
      <author>example@mail.com (Yuzhuo Chen, Zehua Ma, Jianhua Wang, Kai Kang, Shunyu Yao, Weiming Zhang)</author>
      <guid isPermaLink="false">2508.00477v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</title>
      <link>http://arxiv.org/abs/2508.00414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Cognitive Kernel-Pro，一个完全开源且尽可能免费的模块化AI智能体框架，旨在促进先进AI智能体的开发和评估民主化。该框架在GAIA基准测试中取得了最先进的结果，其8B参数模型超越了之前的领先系统。&lt;h4&gt;背景&lt;/h4&gt;通用AI智能体被认为是下一代人工智能的基础框架，具有复杂推理、网络交互、编码和自主研究能力。然而，当前智能体系统要么是闭源的，要么严重依赖各种付费API和专有工具，限制了研究社区的访问性和可复现性。&lt;h4&gt;目的&lt;/h4&gt;开发一个完全开源且尽可能免费的模块化智能体框架，促进先进AI智能体的开发和评估民主化。&lt;h4&gt;方法&lt;/h4&gt;系统性地研究智能体基础模型的高质量训练数据整理，重点关注四个关键领域（网络、文件、代码和通用推理）中查询、轨迹和可验证答案的构建；探索新颖的智能体测试时反思和投票策略，以增强智能体的鲁棒性和性能。&lt;h4&gt;主要发现&lt;/h4&gt;在GAIA上评估Cognitive Kernel-Pro，在开源和免费智能体中取得了最先进的结果；8B参数开源模型超越了之前的领先系统如WebDancer和WebSailor，为可访问的高能力AI智能体建立了新的性能标准。&lt;h4&gt;结论&lt;/h4&gt;Cognitive Kernel-Pro为研究社区提供了一个可访问、高性能的AI智能体框架，促进了AI智能体研究和开发的民主化。&lt;h4&gt;翻译&lt;/h4&gt;通用AI智能体正越来越多地被视为下一代人工智能的基础框架，使复杂推理、网络交互、编码和自主研究成为可能。然而，当前的智能体系统要么是闭源的，要么严重依赖各种付费API和专有工具，这限制了研究社区的访问性和可复现性。在本工作中，我们提出了Cognitive Kernel-Pro，这是一个完全开源且尽可能免费的多模块智能体框架，旨在促进先进AI智能体的开发和评估民主化。在Cognitive Kernel-Pro中，我们系统性地研究了智能体基础模型的高质量训练数据的整理，重点关注四个关键领域（网络、文件、代码和通用推理）中查询、轨迹和可验证答案的构建。此外，我们探索了新颖的智能体测试时反思和投票策略，以增强智能体的鲁棒性和性能。我们在GAIA上评估了Cognitive Kernel-Pro，在开源和免费智能体中取得了最先进的结果。值得注意的是，我们的8B参数开源模型超越了之前的领先系统如WebDancer和WebSailor，为可访问的高能力AI智能体建立了新的性能标准。代码可在https://github.com/Tencent/CognitiveKernel-Pro获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General AI Agents are increasingly recognized as foundational frameworks forthe next generation of artificial intelligence, enabling complex reasoning, webinteraction, coding, and autonomous research capabilities. However, currentagent systems are either closed-source or heavily reliant on a variety of paidAPIs and proprietary tools, limiting accessibility and reproducibility for theresearch community. In this work, we present \textbf{Cognitive Kernel-Pro}, afully open-source and (to the maximum extent) free multi-module agent frameworkdesigned to democratize the development and evaluation of advanced AI agents.Within Cognitive Kernel-Pro, we systematically investigate the curation ofhigh-quality training data for Agent Foundation Models, focusing on theconstruction of queries, trajectories, and verifiable answers across four keydomains: web, file, code, and general reasoning. Furthermore, we explore novelstrategies for agent test-time reflection and voting to enhance agentrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achievingstate-of-the-art results among open-source and free agents. Notably, our8B-parameter open-source model surpasses previous leading systems such asWebDancer and WebSailor, establishing a new performance standard foraccessible, high-capability AI agents. Code is available athttps://github.com/Tencent/CognitiveKernel-Pro</description>
      <author>example@mail.com (Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu)</author>
      <guid isPermaLink="false">2508.00414v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition</title>
      <link>http://arxiv.org/abs/2508.00391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Cued-Agent的协作多智能体系统，用于自动Cued Speech识别，解决了传统方法中因数据有限导致的多模态融合性能不佳问题。&lt;h4&gt;背景&lt;/h4&gt;Cued Speech是一种结合唇读和手部编码的视觉沟通系统，用于帮助听力障碍人士交流。传统自动CS识别方法因手部和唇部动作的时间异步性需要复杂模块，但受限于数据不足，导致融合机制训练不充分，性能不佳。&lt;h4&gt;目的&lt;/h4&gt;开发首个用于自动Cued Speech识别的协作多智能体系统，克服数据限制，提高识别性能。&lt;h4&gt;方法&lt;/h4&gt;Cued-Agent整合四个专门子智能体：基于多模态大型语言模型的手部识别智能体、基于预训练Transformer的唇部识别智能体、手部提示解码智能体和自我校正音素到词智能体。同时扩展了现有普通话CS数据集，收集了八名听力障碍者的数据，建立了包含十四名受试者的混合数据集。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，与最先进方法相比，Cued-Agent在正常和听力障碍场景下均表现出色。&lt;h4&gt;结论&lt;/h4&gt;Cued-Agent通过多智能体协作有效解决了数据有限条件下的多模态融合问题，显著提升了自动Cued Speech识别性能。&lt;h4&gt;翻译&lt;/h4&gt;Cued Speech (CS) 是一种结合唇读和手部编码的视觉沟通系统，旨在帮助听力障碍人士进行交流。自动CS识别(ACSR)的目标是通过AI驱动的方法将CS手势和唇部动作转换为文本。传统上，手部和唇部动作的时间异步性需要设计复杂模块以实现有效的多模态融合。然而，受限于数据可用性不足，当前方法在训练这些融合机制时能力不够，导致性能不佳。最近，多智能体系统在处理数据有限条件下的复杂任务方面显示出良好的能力。为此，我们提出了首个用于ACSR的协作多智能体系统，名为Cued-Agent。它整合了四个专门的子智能体：一个基于多模态大型语言模型的手部识别智能体，采用关键帧筛选和CS专家提示策略解码手部动作；一个基于预训练Transformer的唇部识别智能体，从输入视频中提取唇部特征；一个手部提示解码智能体，以无需训练的方式在推理过程中动态整合手部提示和唇部特征；以及一个自我校正音素到词智能体，首次通过语义优化实现音素序列到自然语言句子的后处理和端到端转换。为支持本研究，我们从八名听力障碍的手语使用者那里收集数据，扩展了现有的普通话CS数据集，建立了包含十四名受试者的混合数据集。大量实验表明，与最先进方法相比，我们的Cued-Agent在正常和听力障碍场景下均表现出色。实现可在https://github.com/DennisHgj/Cued-Agent获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cued Speech (CS) is a visual communication system that combines lip-readingwith hand coding to facilitate communication for individuals with hearingimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gesturesand lip movements into text via AI-driven methods. Traditionally, the temporalasynchrony between hand and lip movements requires the design of complexmodules to facilitate effective multimodal fusion. However, constrained bylimited data availability, current methods demonstrate insufficient capacityfor adequately training these fusion mechanisms, resulting in suboptimalperformance. Recently, multi-agent systems have shown promising capabilities inhandling complex tasks with limited data availability. To this end, we proposethe first collaborative multi-agent system for ACSR, named Cued-Agent. Itintegrates four specialized sub-agents: a Multimodal Large Language Model-basedHand Recognition agent that employs keyframe screening and CS expert promptstrategies to decode hand movements, a pretrained Transformer-based LipRecognition agent that extracts lip features from the input video, a HandPrompt Decoding agent that dynamically integrates hand prompts with lipfeatures during inference in a training-free manner, and a Self-CorrectionPhoneme-to-Word agent that enables post-process and end-to-end conversion fromphoneme sequences to natural language sentences for the first time throughsemantic refinement. To support this study, we expand the existing Mandarin CSdataset by collecting data from eight hearing-impaired cuers, establishing amixed dataset of fourteen subjects. Extensive experiments demonstrate that ourCued-Agent performs superbly in both normal and hearing-impaired scenarioscompared with state-of-the-art methods. The implementation is available athttps://github.com/DennisHgj/Cued-Agent.</description>
      <author>example@mail.com (Guanjie Huang, Danny H. K. Tsang, Shan Yang, Guangzhi Lei, Li Liu)</author>
      <guid isPermaLink="false">2508.00391v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.00383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted (Oral) in MICCAI 2025 COMPAYL Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MVHybrid的新型混合主干架构，结合状态空间模型与Vision Transformer，用于从组织病理学图像预测空间基因表达，在多项评估中表现优于传统ViT模型。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学可在组织环境中揭示基因表达模式，支持精准肿瘤学应用，但高成本和技术复杂性限制了临床应用。从常规组织病理学图像预测空间基因表达是一种替代方法，但当前基于Vision Transformer的病理学视觉基础模型表现低于临床标准。&lt;h4&gt;目的&lt;/h4&gt;探索超越ViT架构的创新，以更好地捕捉与分子表型相关的低频、细微形态模式，并开发一种新的混合主干架构。&lt;h4&gt;方法&lt;/h4&gt;展示具有负实特征值初始化的状态空间模型表现出强烈的低频偏差；引入MVHybrid混合架构；比较五种不同主干架构；所有模型在相同结直肠癌数据集上使用DINOv2自监督学习预训练；通过随机分割和留一研究法(LOSO)评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;在LOSO评估中，MVHybrid比最佳ViT高57%的相关性；在基因表达预测中，MVHybrid比随机分割显示43%更小的性能下降；MVHybrid在分类、块检索和生存预测等下游任务中表现与ViT相当或更好。&lt;h4&gt;结论&lt;/h4&gt;MVHybrid作为下一代病理学视觉基础模型主干具有广阔前景，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;空间转录组学揭示了组织环境中的基因表达模式，使精准肿瘤学应用如治疗反应预测成为可能，但其高成本和技术复杂性限制了临床应用。从常规组织病理学图像预测空间基因表达（生物标志物）提供了一种实用的替代方案，然而，当前基于Vision Transformer主干架构的病理学视觉基础模型表现低于临床标准。鉴于视觉基础模型已在数百万种不同的全幻灯片图像上进行了训练，我们假设超越ViT的架构创新可能更好地捕捉与分子表型相关的低频、细微形态模式。通过展示具有负实特征值初始化的状态空间模型表现出强烈的低频偏差，我们引入了MVHybrid，这是一种结合状态空间模型与ViT的混合主干架构。我们比较了其他五种不同的病理学视觉基础模型主干架构，所有架构都使用DINOv2自监督学习方法在相同的结直肠癌数据集上进行了预训练。我们使用相同的生物标志物数据集的随机分割和留一研究法(LOSO)设置评估了所有预训练模型。在LOSO评估中，MVHybrid比表现最佳的ViT高57%的相关性，并且在基因表达预测中显示出比随机分割小43%的性能下降，分别证明了其优越的性能和鲁棒性。此外，与ViT相比，MVHybrid在分类、块检索和生存预测等下游任务中显示出相等或更好的性能，显示了其作为下一代病理学视觉基础模型主干的潜力。我们的代码已在GitHub上公开：https://github.com/deepnoid-ai/MVHybrid。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics reveals gene expression patterns within tissuecontext, enabling precision oncology applications such as treatment responseprediction, but its high cost and technical complexity limit clinical adoption.Predicting spatial gene expression (biomarkers) from routine histopathologyimages offers a practical alternative, yet current vision foundation models(VFMs) in pathology based on Vision Transformer (ViT) backbones perform belowclinical standards. Given that VFMs are already trained on millions of diversewhole slide images, we hypothesize that architectural innovations beyond ViTsmay better capture the low-frequency, subtle morphological patterns correlatingwith molecular phenotypes. By demonstrating that state space models initializedwith negative real eigenvalues exhibit strong low-frequency bias, we introduce$MV_{Hybrid}$, a hybrid backbone architecture combining state space models(SSMs) with ViT. We compare five other different backbone architectures forpathology VFMs, all pretrained on identical colorectal cancer datasets usingthe DINOv2 self-supervised learning method. We evaluate all pretrained modelsusing both random split and leave-one-study-out (LOSO) settings of the samebiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% highercorrelation than the best-performing ViT and shows 43% smaller performancedegradation compared to random split in gene expression prediction,demonstrating superior performance and robustness, respectively. Furthermore,$MV_{Hybrid}$ shows equal or better downstream performance in classification,patch retrieval, and survival prediction tasks compared to that of ViT, showingits promise as a next-generation pathology VFM backbone. Our code is publiclyavailable at: https://github.com/deepnoid-ai/MVHybrid.</description>
      <author>example@mail.com (Won June Cho, Hongjun Yoon, Daeky Jeong, Hyeongyeol Lim, Yosep Chong)</author>
      <guid isPermaLink="false">2508.00383v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer</title>
      <link>http://arxiv.org/abs/2508.00298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2412.00837&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了AniMer+，一个统一理解不同动态对象的网络框架，特别用于哺乳动物和鸟类的姿态与形状估计。通过高容量的Vision Transformer架构和混合专家设计，结合扩散条件图像生成技术创建大规模合成数据集，解决了多物种3D训练数据稀缺的问题，并在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;在基础模型时代，通过单一网络实现对不同动态对象的统一理解有望增强空间智能。准确估计不同物种动物的姿态和形状对生物研究中的定量分析至关重要。然而，由于先前方法的网络容量有限以及全面的多物种数据集稀缺，这一领域研究不足。&lt;h4&gt;目的&lt;/h4&gt;解决网络容量限制和多物种数据集稀缺的问题，开发一个统一的网络框架，用于重建哺乳动物和鸟类，并提高动物姿态和形状估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了AniMer+，可扩展的AniMer框架的扩展版本；2. 设计了高容量的、家族感知的Vision Transformer (ViT)，采用混合专家(MoE)设计；3. 架构将网络层分为特定分类组件(哺乳动物和鸟类)和共享分类组件；4. 引入基于扩散的条件图像生成管道，创建两个大规模合成数据集：CtrlAni3D(四足动物)和CtrlAVES3D(鸟类)；5. 在41.3k张哺乳动物和12.4k张鸟类图像(真实和合成数据组合)上训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. AniMer+在广泛的基准测试中表现优于现有方法，包括具有挑战性的域外Animal Kingdom数据集；2. 消融研究验证了新型网络架构和生成的合成数据集在提高实际应用性能方面的有效性；3. CtrlAVES3D是首个大规模3D注释的鸟类数据集，对于解决单视图深度歧义至关重要。&lt;h4&gt;结论&lt;/h4&gt;AniMer+通过统一网络架构和合成数据生成技术，有效解决了多物种动物姿态和形状估计的挑战，为生物研究和空间智能提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;在基础模型时代，通过单一网络实现对不同动态对象的统一理解有望增强强大的空间智能。此外，准确估计不同物种动物的姿态和形状对生物研究中的定量分析至关重要。然而，由于先前方法的网络容量有限以及全面的多物种数据集稀缺，这一主题仍未得到充分探索。为解决这些限制，我们引入了AniMer+，即可扩展AniMer框架的扩展版本。在本文中，我们专注于统一重建哺乳动物(mammalia)和鸟类(aves)的方法。AniMer+的一个关键创新是其高容量的、家族感知的Vision Transformer (ViT)，采用混合专家(MoE)设计。其架构将网络层分为特定分类组件(针对哺乳动物和鸟类)和共享分类组件，使单一模型能够高效学习独特的和共同的解剖特征。为克服3D训练数据的严重短缺，特别是鸟类数据，我们引入了基于扩散的条件图像生成管道。该管道产生两个大规模合成数据集：用于四足动物的CtrlAni3D和用于鸟类的CtrlAVES3D。值得注意的是，CtrlAVES3D是首个大规模3D注释的鸟类数据集，对于解决单视图深度歧义至关重要。在41.3k张哺乳动物和12.4k张鸟类图像(结合真实和合成数据)的集合上训练后，我们的方法在广泛的基准测试中表现出优于现有方法的性能，包括具有挑战性的域外Animal Kingdom数据集。消融研究验证了我们的新型网络架构和生成的合成数据集在提高实际应用性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决统一估计不同动物（特别是哺乳动物和鸟类）的姿态和形状的问题。这个问题在现实中非常重要，因为准确估计跨物种动物姿态和形状对生物研究中的定量分析至关重要，能够帮助研究人员更好地理解动物行为、生物力学和与环境相互作用的关系，为动物福利、农业、生态学和生命科学等多个领域提供重要见解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出现有方法的局限性：网络容量有限和多物种数据集稀缺。他们借鉴了人类网格恢复领域的工作，特别是HMR2.0和HaMeR等使用Vision Transformer骨干网络的方法。作者扩展了他们之前的工作AniMer，引入了专家混合(MoE)设计来处理不同动物分类群。为了解决3D训练数据不足的问题，特别是鸟类数据，作者引入了基于扩散的条件图像生成管道。借鉴的现有工作包括Vision Transformer架构、专家混合设计、ControlNet图像生成、SMAL和AVES参数模型以及监督对比学习技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用高容量的家族感知Vision Transformer网络，结合专家混合(MoE)设计，将网络层分为特定分类群组件（哺乳动物和鸟类）和分类群共享组件，在单一模型中高效学习不同和共同的解剖特征，并利用合成数据解决3D训练数据稀缺问题。整体实现流程包括：1)收集和生成数据（聚合现有数据集并使用ControlNet生成CtrlAni3D和CtrlAVES3D合成数据集）；2)构建模型架构（ViT-MoE编码器、Transformer解码器、预测头和回归头）；3)采用两阶段训练策略；4)使用多种损失函数进行训练；5)在多个基准数据集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AniMer+网络架构，首个统一预测不同动物分类群的方法；2)动物家族监督对比学习方案，增强模型区分不同动物家族独特形状的能力；3)合成数据集生成管道，创建了首个带有3D注释的大规模鸟类数据集CtrlAVES3D；4)统一框架，首次在单一网络中同时处理哺乳动物和鸟类的网格恢复。相比之前的工作，AniMer+使用高容量Transformer骨干网络而非传统CNN，通过MoE设计处理不同解剖结构，使用ControlNet生成更高质量合成图像，并首次引入3D注释的大规模鸟类数据集解决深度歧义问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AniMer+通过引入家族感知Transformer和专家混合架构，结合大规模合成数据集，首次实现了在单一网络中统一估计哺乳动物和鸟类的姿态和形状，为动物网格恢复研究提供了新的基础模型范式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of foundation models, achieving a unified understanding ofdifferent dynamic objects through a single network has the potential to empowerstronger spatial intelligence. Moreover, accurate estimation of animal pose andshape across diverse species is essential for quantitative analysis inbiological research. However, this topic remains underexplored due to thelimited network capacity of previous methods and the scarcity of comprehensivemulti-species datasets. To address these limitations, we introduce AniMer+, anextended version of our scalable AniMer framework. In this paper, we focus on aunified approach for reconstructing mammals (mammalia) and birds (aves). A keyinnovation of AniMer+ is its high-capacity, family-aware Vision Transformer(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecturepartitions network layers into taxa-specific components (for mammalia and aves)and taxa-shared components, enabling efficient learning of both distinct andcommon anatomical features within a single model. To overcome the criticalshortage of 3D training data, especially for birds, we introduce adiffusion-based conditional image generation pipeline. This pipeline producestwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D forbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset forbirds, which is crucial for resolving single-view depth ambiguities. Trained onan aggregated collection of 41.3k mammalian and 12.4k avian images (combiningreal and synthetic data), our method demonstrates superior performance overexisting approaches across a wide range of benchmarks, including thechallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm theeffectiveness of both our novel network architecture and the generatedsynthetic datasets in enhancing real-world application performance.</description>
      <author>example@mail.com (Jin Lyu, Liang An, Li Lin, Pujin Cheng, Yebin Liu, Xiaoying Tang)</author>
      <guid isPermaLink="false">2508.00298v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models</title>
      <link>http://arxiv.org/abs/2508.00202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, under review at CAMSAP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在标签噪声环境下提高基础模型鲁棒性的两阶段框架，无需重新训练模型。通过引入几何信息和使用非负核邻域构建，改进了现有的kNN方法，在多个数据集上表现出更好的性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大数据集上预训练后，已成为各种下游机器学习任务的基础，特别是在获取完美标记数据成本极高的场景中。然而，当基础模型需要用噪声数据进行微调时，如何确保鲁棒分类成为一个挑战。最近的kNN方法虽然能在严重标签噪声下表现良好，但仅利用了局部几何信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在标签噪声环境下确保鲁棒分类的两阶段框架，无需重新训练模型。通过引入几何信息，进一步提高现有方法的性能。&lt;h4&gt;方法&lt;/h4&gt;提出一个两阶段框架：首先是可靠性估计，其次是可靠性加权推理。对于给定实例，使用非负核邻域构建获取训练数据的局部邻域。作者还提出了几种可靠性估计方法，这些方法随着标签噪声的增加可以减少对距离和局部邻域的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10和DermaMNIST上的评估表明，所提出的方法在各种噪声条件下都能提高鲁棒性，超越了标准的K-NN方法和最近的自适应邻域基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何信息并使用非负核邻域构建，提出的方法能够在标签噪声环境下提供更鲁棒的性能，且无需重新训练模型。&lt;h4&gt;翻译&lt;/h4&gt;在大数据集上预训练的基础模型已成为各种下游机器学习任务的基础，特别是在获取完美标记数据成本极高的场景中。在本文中，我们假设一个基础模型必须用噪声数据进行微调，并提出一个两阶段框架，以确保在存在标签噪声的情况下进行鲁棒分类，而无需重新训练模型。最近的工作表明，即使存在严重的标签噪声，使用从基础模型派生的嵌入的简单k最近邻方法也能取得良好的性能。我们的工作动机是这些方法利用了局部几何信息。在本文中，遵循类似的两阶段过程，即可靠性估计后接可靠性加权推理，我们表明通过引入几何信息可以实现改进的性能。对于给定的实例，我们提出的推理使用使用非负核邻域构建获得的训练数据的局部邻域。我们提出了几种可靠性估计方法，这些方法随着标签噪声的增加可以减少对距离和局部邻域的依赖。我们在CIFAR-10和DermaMNIST上的评估表明，我们的方法在各种噪声条件下都能提高鲁棒性，超越了标准的K-NN方法和最近的自适应邻域基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) pretrained on large datasets have become fundamentalfor various downstream machine learning tasks, in particular in scenarios whereobtaining perfectly labeled data is prohibitively expensive. In this paper, weassume an FM has to be fine-tuned with noisy data and present a two-stageframework to ensure robust classification in the presence of label noisewithout model retraining. Recent work has shown that simple k-nearest neighbor(kNN) approaches using an embedding derived from an FM can achieve goodperformance even in the presence of severe label noise. Our work is motivatedby the fact that these methods make use of local geometry. In this paper,following a similar two-stage procedure, reliability estimation followed byreliability-weighted inference, we show that improved performance can beachieved by introducing geometry information. For a given instance, ourproposed inference uses a local neighborhood of training data, obtained usingthe non-negative kernel (NNK) neighborhood construction. We propose severalmethods for reliability estimation that can rely less on distance and localneighborhood as the label noise increases. Our evaluation on CIFAR-10 andDermaMNIST shows that our methods improve robustness across various noiseconditions, surpassing standard K-NN approaches and recentadaptive-neighborhood baselines.</description>
      <author>example@mail.com (Ecem Bozkurt, Antonio Ortega)</author>
      <guid isPermaLink="false">2508.00202v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2507.23523v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出H-RDT（Human to Robotics Diffusion Transformer），一种利用人类操作数据增强机器人操作能力的新方法，采用两阶段训练范式，在模拟和真实世界实验中显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;机器人操作的模仿学习面临大规模高质量演示数据稀缺的挑战，现有机器人基础模型虽在跨形态数据集上预训练以增加数据规模，但因不同机器人形态和动作空间多样性导致统一训练困难。&lt;h4&gt;目的&lt;/h4&gt;利用大规模第一人称视角人类操作数据及其3D手部姿态注释，为机器人操作学习提供丰富的行为先验，解决数据稀缺和跨形态训练问题。&lt;h4&gt;方法&lt;/h4&gt;H-RDT采用两阶段训练范式：1)在大量第一人称视角人类操作数据上预训练；2)在机器人特定数据上跨形态微调，使用模块化动作编码器和解码器。基于20亿参数的扩散变压器架构，使用流匹配建模复杂动作分布。&lt;h4&gt;主要发现&lt;/h4&gt;广泛评估表明H-RDT在模拟和真实世界实验中分别比从头训练提高13.9%和40.5%，优于现有最先进方法包括Pi0和RDT，在单任务、多任务、少样本学习和鲁棒性评估中均表现优异。&lt;h4&gt;结论&lt;/h4&gt;人类操作数据可作为学习双臂机器人操作策略的强大基础，验证了利用人类行为先验增强机器人能力的核心假设。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作的模仿学习面临一个基本挑战：大规模、高质量的机器人演示数据稀缺。最近的机器人基础模型通常在跨形态机器人数据集上进行预训练以增加数据规模，而它们面临着显著的限制，因为不同机器人形态和动作空间的多样性使得统一训练具有挑战性。在本文中，我们提出了H-RDT（Human to Robotics Diffusion Transformer），一种利用人类操作数据增强机器人操作能力的新方法。我们的关键见解是，大规模第一人称视角人类操作视频与配对的3D手部姿态注释提供了丰富的行为先验，这些先验捕获了自然操作策略，并且可以受益于机器人策略学习。我们引入了一种两阶段训练范式：（1）在大量第一人称视角人类操作数据上进行预训练，以及（2）使用模块化动作编码器和解码器在机器人特定数据上进行跨形态微调。基于具有20亿参数的扩散变压器架构，H-RDT使用流匹配来建模复杂的动作分布。涵盖模拟和真实世界实验、单任务和多任务场景以及少样本学习和鲁棒性评估的广泛评估表明，H-RDT优于从头训练和现有的最先进方法，包括Pi0和RDT，在模拟和真实世界实验中分别比从头训练实现了13.9%和40.5%的显著改进。结果验证了我们的核心假设，即人类操作数据可以作为学习双臂机器人操作策略的强大基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning for robotic manipulation faces a fundamental challenge:the scarcity of large-scale, high-quality robot demonstration data. Recentrobotic foundation models often pre-train on cross-embodiment robot datasets toincrease data scale, while they face significant limitations as the diversemorphologies and action spaces across different robot embodiments make unifiedtraining challenging. In this paper, we present H-RDT (Human to RoboticsDiffusion Transformer), a novel approach that leverages human manipulation datato enhance robot manipulation capabilities. Our key insight is that large-scaleegocentric human manipulation videos with paired 3D hand pose annotationsprovide rich behavioral priors that capture natural manipulation strategies andcan benefit robotic policy learning. We introduce a two-stage trainingparadigm: (1) pre-training on large-scale egocentric human manipulation data,and (2) cross-embodiment fine-tuning on robot-specific data with modular actionencoders and decoders. Built on a diffusion transformer architecture with 2Bparameters, H-RDT uses flow matching to model complex action distributions.Extensive evaluations encompassing both simulation and real-world experiments,single-task and multitask scenarios, as well as few-shot learning androbustness assessments, demonstrate that H-RDT outperforms training fromscratch and existing state-of-the-art methods, including Pi0 and RDT, achievingsignificant improvements of 13.9% and 40.5% over training from scratch insimulation and real-world experiments, respectively. The results validate ourcore hypothesis that human manipulation data can serve as a powerful foundationfor learning bimanual robotic manipulation policies.</description>
      <author>example@mail.com (Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, Jun Zhu)</author>
      <guid isPermaLink="false">2507.23523v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data</title>
      <link>http://arxiv.org/abs/2508.00758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 16 figures, 7 tables, preprint version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种扩散调度去噪自编码器(DDAE)框架，通过整合基于扩散的噪声调度和对比学习来改进表格数据异常检测，在半监督和无监督设置中均取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;表格数据异常检测面临特征交互复杂和异常样本稀少的挑战；传统去噪自编码器使用固定幅度噪声，适应性有限；扩散模型虽有计划噪声和迭代去噪机制，但缺乏明确的重建映射。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合扩散模型噪声调度和对比学习的新框架，提高表格数据异常检测的准确性和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出扩散调度去噪自编码器(DDAE)，将基于扩散的噪声调度机制和对比学习整合到编码过程中，形成一种新的异常检测框架。&lt;h4&gt;主要发现&lt;/h4&gt;在57个ADBench数据集上评估显示，DDAE在半监督设置中优于现有方法，在无监督设置中具有竞争力；与最先进基线相比，PR-AUC提高最多65%(9%)，ROC-AUC提高16%(6%)；较高噪声水平有利于无监督训练，较低噪声配合线性调度在半监督设置中最优。&lt;h4&gt;结论&lt;/h4&gt;合理的噪声策略对表格异常检测至关重要，DDAE框架通过整合扩散模型的噪声调度和对比学习显著提升了异常检测性能。&lt;h4&gt;翻译&lt;/h4&gt;表格数据中的异常检测由于复杂的特征交互和异常样本的稀少性而仍然具有挑战性。去噪自编码器依赖于固定幅度的噪声，限制了其对不同数据分布的适应性。扩散模型引入了计划噪声和迭代去噪，但缺乏明确的重建映射。我们提出了扩散调度去噪自编码器(DDAE)，这是一个将基于扩散的噪声调度和对比学习整合到编码过程中的框架，以改进异常检测。我们在ADBench的57个数据集上评估了DDAE。我们的方法在半监督设置中优于现有方法，并在无监督设置中取得了具有竞争力的结果，将PR-AUC比最先进的自编码器(扩散)模型基线提高了最多65%(9%)，ROC-AUC提高了16%(6%)。我们观察到，较高的噪声水平有利于无监督训练，而较低的噪声配合线性调度在半监督设置中是最优的。这些发现强调了在表格异常检测中，合理的噪声策略的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3711896.3736910&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in tabular data remains challenging due to complex featureinteractions and the scarcity of anomalous examples. Denoising autoencodersrely on fixed-magnitude noise, limiting adaptability to diverse datadistributions. Diffusion models introduce scheduled noise and iterativedenoising, but lack explicit reconstruction mappings. We propose theDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integratesdiffusion-based noise scheduling and contrastive learning into the encodingprocess to improve anomaly detection. We evaluated DDAE on 57 datasets fromADBench. Our method outperforms in semi-supervised settings and achievescompetitive results in unsupervised settings, improving PR-AUC by up to 65%(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)model baselines. We observed that higher noise levels benefit unsupervisedtraining, while lower noise with linear scheduling is optimal insemi-supervised settings. These findings underscore the importance ofprincipled noise strategies in tabular anomaly detection.</description>
      <author>example@mail.com (Timur Sattarov, Marco Schreyer, Damian Borth)</author>
      <guid isPermaLink="false">2508.00758v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction</title>
      <link>http://arxiv.org/abs/2508.00657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MLHC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了TrajSurv模型，通过学习纵向电子健康记录中的连续潜在轨迹，实现可信的生存预测，并提高模型的可解释性。&lt;h4&gt;背景&lt;/h4&gt;可信的生存预测对临床决策至关重要，纵向电子健康记录提供了独特的预测机会，但准确建模不规则采样数据下的连续临床进展并透明地将其与生存结果联系起来具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从纵向EHR数据中学习连续潜在轨迹的模型，以实现可信且透明的生存预测。&lt;h4&gt;方法&lt;/h4&gt;TrajSurv使用神经控制微分方程从不规则采样数据中提取连续时间潜在状态，形成连续轨迹；通过时间感知对比学习确保轨迹反映临床进展；使用两步解释过程将临床进展与生存结果透明链接。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-III和eICU数据集上评估显示，TrajSurv具有比现有深度学习方法更具竞争力的准确性和优越的透明度。&lt;h4&gt;结论&lt;/h4&gt;TrajSurv模型能够有效处理不规则采样的纵向EHR数据，生成可信的生存预测，同时提供透明的临床进展解释，有助于临床决策。&lt;h4&gt;翻译&lt;/h4&gt;可信的生存预测对临床决策至关重要。纵向电子健康记录(EHRs)为预测提供了独特而强大的机会。然而，准确建模不规则采样临床特征下患者潜在的连续临床进展，并透明地将进展与生存结果联系起来具有挑战性。为应对这些挑战，我们开发了TrajSurv，一个从纵向EHR数据学习连续潜在轨迹以实现可信生存预测的模型。TrajSurv采用神经控制微分方程(NCDE)从不规则采样数据中提取连续时间潜在状态，形成连续潜在轨迹。为确保潜在轨迹反映临床进展，TrajSurv通过时间感知对比学习方法将潜在状态空间与患者状态空间对齐。为透明地连接临床进展与生存结果，TrajSurv在两步分而治之的解释过程中使用潜在轨迹。首先，它使用学习到的向量场解释临床特征变化如何转化为潜在轨迹的演变。其次，它对这些潜在轨迹进行聚类，以识别与不同生存结果相关的关键临床进展模式。在MIMIC-III和eICU两个真实医疗数据集上的评估显示，TrajSurv相比现有深度学习方法具有竞争性的准确性和优越的透明度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trustworthy survival prediction is essential for clinical decision making.Longitudinal electronic health records (EHRs) provide a uniquely powerfulopportunity for the prediction. However, it is challenging to accurately modelthe continuous clinical progression of patients underlying the irregularlysampled clinical features and to transparently link the progression to survivaloutcomes. To address these challenges, we develop TrajSurv, a model that learnscontinuous latent trajectories from longitudinal EHR data for trustworthysurvival prediction. TrajSurv employs a neural controlled differential equation(NCDE) to extract continuous-time latent states from the irregularly sampleddata, forming continuous latent trajectories. To ensure the latent trajectoriesreflect the clinical progression, TrajSurv aligns the latent state space withpatient state space through a time-aware contrastive learning approach. Totransparently link clinical progression to the survival outcome, TrajSurv useslatent trajectories in a two-step divide-and-conquer interpretation process.First, it explains how the changes in clinical features translate into thelatent trajectory's evolution using a learned vector field. Second, it clustersthese latent trajectories to identify key clinical progression patternsassociated with different survival outcomes. Evaluations on two real-worldmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy andsuperior transparency over existing deep learning methods.</description>
      <author>example@mail.com (Sihang Zeng, Lucas Jing Liu, Jun Wen, Meliha Yetisgen, Ruth Etzioni, Gang Luo)</author>
      <guid isPermaLink="false">2508.00657v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.00513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CMUCL，一种用于文本属性图异常检测的新型端到端范式，通过联合训练文本和图编码器并利用跨模态和单模态多尺度一致性来提高异常检测性能，平均精度比次优方法提高11.13%。&lt;h4&gt;背景&lt;/h4&gt;图数据在各种高风险场景中的广泛应用增加了对图异常检测(GAD)的关注。现实世界中的图通常带有以原始文本序列形式存在的节点描述，称为文本属性图(TAGs)。&lt;h4&gt;目的&lt;/h4&gt;解决如何无缝整合原始文本和图拓扑，以释放TAGs中跨模态数据在异常检测方面的巨大潜力这一挑战性问题。&lt;h4&gt;方法&lt;/h4&gt;提出名为CMUCL的新型端到端范式，同时建模文本和图结构的数据，通过利用跨模态和单模态多尺度一致性联合训练文本和图编码器，设计基于不一致性挖掘的异常评分估计器得出节点特定的异常评分。&lt;h4&gt;主要发现&lt;/h4&gt;大量评估表明，CMUCL在文本属性图异常检测方面取得了显著进展，平均精度(AP)比次优方法提高了11.13%。&lt;h4&gt;结论&lt;/h4&gt;CMUCL通过整合文本和图信息，有效提高了文本属性图异常检测的性能，并发布了8个专门针对TAGs异常检测的基准数据集以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;图数据在各种高风险场景中的广泛应用增加了对图异常检测(GAD)的关注。面对现实世界中通常携带以原始文本序列形式存在的节点描述的图(称为文本属性图(TAGs))，现有的图异常检测流程通常涉及浅层嵌入技术将此类文本信息编码为特征，然后依赖图域内的复杂自监督任务来检测异常。然而，这种文本编码过程与图域中的异常检测训练目标分离，难以确保提取的文本特征专注于GAD相关信息，严重限制了检测能力。如何无缝整合原始文本和图拓扑，以释放TAGs中跨模态数据在异常检测方面的巨大潜力，构成了一个具有挑战性的问题。本文提出了一个用于文本属性图异常检测的新型端到端范式，名为CMUCL。我们同时建模来自文本和图结构的数据，并通过利用跨模态和单模态多尺度一致性联合训练文本和图编码器，以发现潜在的异常相关信息。据此，我们设计了一个基于不一致性挖掘的异常评分估计器，以得出节点特定的异常评分。考虑到缺乏专门针对TAGs异常检测的基准数据集，我们发布了8个数据集以促进未来研究。大量评估表明，CMUCL在文本属性图异常检测方面取得了显著进展，平均精度(AP)比次优方法提高了11.13%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread application of graph data in various high-risk scenarios hasincreased attention to graph anomaly detection (GAD). Faced with real-worldgraphs that often carry node descriptions in the form of raw text sequences,termed text-attributed graphs (TAGs), existing graph anomaly detectionpipelines typically involve shallow embedding techniques to encode such textualinformation into features, and then rely on complex self-supervised taskswithin the graph domain to detect anomalies. However, this text encodingprocess is separated from the anomaly detection training objective in the graphdomain, making it difficult to ensure that the extracted textual features focuson GAD-relevant information, seriously constraining the detection capability.How to seamlessly integrate raw text and graph topology to unleash the vastpotential of cross-modal data in TAGs for anomaly detection poses a challengingissue. This paper presents a novel end-to-end paradigm for text-attributedgraph anomaly detection, named CMUCL. We simultaneously model data from bothtext and graph structures, and jointly train text and graph encoders byleveraging cross-modal and uni-modal multi-scale consistency to uncoverpotential anomaly-related information. Accordingly, we design an anomaly scoreestimator based on inconsistency mining to derive node-specific anomaly scores.Considering the lack of benchmark datasets tailored for anomaly detection onTAGs, we release 8 datasets to facilitate future research. Extensiveevaluations show that CMUCL significantly advances in text-attributed graphanomaly detection, delivering an 11.13% increase in average accuracy (AP) overthe suboptimal.</description>
      <author>example@mail.com (Yiming Xu, Xu Hua, Zhen Peng, Bin Shi, Jiarun Chen, Xingbo Fu, Song Wang, Bo Dong)</author>
      <guid isPermaLink="false">2508.00513v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework</title>
      <link>http://arxiv.org/abs/2508.00456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉-语言模型的对比学习多模态波束预测框架，通过整合多模态数据并采用对比预训练策略，提高了在复杂动态环境下的毫米波波束预测准确性。&lt;h4&gt;背景&lt;/h4&gt;现实传播环境日益复杂和动态，传统依赖实时信道状态信息的方法计算成本高且在复杂环境下难以保持准确性，毫米波波束预测面临巨大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于视觉-语言模型(VLM)的对比学习多模态波束预测框架，提高复杂动态环境下的波束预测性能。&lt;h4&gt;方法&lt;/h4&gt;通过特定模态编码器整合多模态数据，采用对比预训练策略对齐图像和LiDAR特征，使用位置信息作为文本提示引入语言模态。&lt;h4&gt;主要发现&lt;/h4&gt;在DeepSense-6G数据集上实验表明，VLM主干提供了额外的语义基础，与现有方法相比，基于距离的整体准确度分数达到0.9016，平均提高了1.46%。&lt;h4&gt;结论&lt;/h4&gt;VLM驱动的对比学习方法能有效提高毫米波波束预测在复杂动态环境下的准确性和性能。&lt;h4&gt;翻译&lt;/h4&gt;随着实际传播环境变得越来越复杂和动态，毫米波波束预测面临巨大挑战。然而，视觉-语言模型(VLM)强大的跨模态表示能力提供了一种有前景的方法。传统依赖实时信道状态信息(CSI)的方法计算量大，且在这些环境中往往难以保持准确性。本文提出了一种VLM驱动的基于对比学习的多模态波束预测框架，通过特定模态编码器整合多模态数据。为了强制跨模态一致性，我们采用对比预训练策略对齐潜在空间中的图像和LiDAR特征。我们使用位置信息作为文本提示并将其连接到文本编码器以引入语言模态，这进一步提高了跨模态一致性。在DeepSense-6G数据集上的实验表明，我们的VLM主干提供了额外的语义基础。与现有方法相比，0.9016的整体基于距离的准确度分数(DBA-Score)对应于1.46%的平均改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决毫米波通信系统中在复杂动态环境下的波束预测问题。传统方法依赖实时信道状态信息(CSI)，计算成本高且在动态环境中难以保持准确性。这个问题很重要，因为随着实际环境日益复杂，可靠的波束预测是毫米波(特别是车对基础设施V2I通信)高效通信的基础，而实时CSI获取在快速变化环境中开销巨大，需要更高效的替代方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有单模态方法(如雷达、LiDAR、GPS)的局限性，它们性能不稳定且对环境变化敏感。然后借鉴了多模态融合的潜力，但发现跨模态对齐仍是挑战。作者创新性地将视觉-语言模型(VLM)引入波束预测，借鉴了对比学习框架来对齐不同模态特征，同时改进了现有方法中直接将位置作为数值输入的做法，转而将其文本化输入文本编码器。整体设计融合了现有技术但提出了新的架构和训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉-语言模型的跨模态表征能力，通过对比学习对齐不同模态特征，结合图像、LiDAR和GPS数据提高波束预测准确性。整体流程包括：1)使用模态特定编码器提取各模态特征；2)通过对比预训练对齐图像和LiDAR特征；3)将GPS位置转化为文本提示引入语言模态；4)使用动态门控和跨模态注意力融合多模态特征；5)通过分类器预测最优波束。训练分为对比预训练和微调两个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将视觉-语言模型应用于毫米波波束预测；2)提出自监督对比预训练策略对齐跨模态特征；3)设计GPS-Text分支将位置信息文本化引入语言模态；4)采用动态门控和跨模态注意力实现自适应特征融合。相比之前工作，不同之处在于：位置信息处理从数值输入转为文本提示；特征对齐从简单拼接转为对比学习；融合机制从固定转为动态自适应；训练流程从端到端转为两阶段(预训练+微调)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于视觉-语言模型和对比学习的多模态框架，通过有效对齐图像、LiDAR和文本特征，显著提高了复杂环境下毫米波波束预测的准确性，同时降低了对实时信道状态信息的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the real propagation environment becomes in creasingly complex anddynamic, millimeter wave beam prediction faces huge challenges. However, thepowerful cross modal representation capability of vision-language model (VLM)provides a promising approach. The traditional methods that rely on real-timechannel state information (CSI) are computationally expensive and often fail tomaintain accuracy in such environments. In this paper, we present a VLM-drivencontrastive learning based multimodal beam prediction framework that integratesmultimodal data via modality-specific encoders. To enforce cross-modalconsistency, we adopt a contrastive pretraining strategy to align image andLiDAR features in the latent space. We use location information as text promptsand connect it to the text encoder to introduce language modality, whichfurther improves cross-modal consistency. Experiments on the DeepSense-6Gdataset show that our VLM backbone provides additional semantic grounding.Compared with existing methods, the overall distance-based accuracy score(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.</description>
      <author>example@mail.com (Ji Wang, Bin Tang, Jian Xiao, Qimei Cui, Xingwang Li, Tony Q. S. Quek)</author>
      <guid isPermaLink="false">2508.00456v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation</title>
      <link>http://arxiv.org/abs/2508.00452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M^2VAE的多模态多视图变分自编码器模型，用于解决冷启动项目推荐问题，通过处理多模态特征的公共视图和独特视图，以及用户偏好建模，有效提升了新项目的推荐效果。&lt;h4&gt;背景&lt;/h4&gt;冷启动项目推荐是推荐系统中的重大挑战，特别是在引入没有历史交互数据的新项目时。现有方法虽利用多模态内容缓解此问题，但往往忽略了模态固有的多视图结构以及共享特征与模态特定特征的区别。&lt;h4&gt;目的&lt;/h4&gt;开发一种生成模型，能够处理属性和多模态特征中的公共视图和独特视图建模挑战，以及对用户在单类型项目特征上的偏好进行建模。&lt;h4&gt;方法&lt;/h4&gt;提出M^2VAE模型，为项目ID、分类属性和图像特征生成类型特定的潜在变量，使用专家乘积导出公共表示，采用解耦对比损失分离公共视图与独特视图，利用偏好引导的专家混合自适应融合表示，并通过对比学习结合共现信号实现无需预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的大量实验验证了M^2VAE方法的有效性，能够有效解决冷启动项目推荐问题。&lt;h4&gt;结论&lt;/h4&gt;M^2VAE模型通过多模态多视图的变分自编码方法，成功解决了冷启动项目推荐中的关键挑战，为新项目的推荐提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;冷启动项目推荐是推荐系统中的一个重大挑战，特别是当引入没有任何历史交互数据的新项目时。虽然现有方法利用多模态内容来缓解冷启动问题，但它们常常忽略了模态固有的多视图结构以及共享特征和模态特定特征之间的区别。在本文中，我们提出了多模态多视图变分自编码器(M^2VAE)，这是一种生成模型，它解决了在属性和多模态特征中建模公共视图和独特视图的挑战，以及对用户在单类型项目特征上的偏好建模的问题。具体来说，我们为项目ID、分类属性和图像特征生成类型特定的潜在变量，并使用专家乘积(PoE)导出公共表示。解耦对比损失将公共视图与独特视图分离，同时保持特征信息量。为了建模用户倾向，我们采用偏好引导的专家混合(MoE)来自适应融合表示。我们还通过对比学习结合共现信号，消除了对预训练的需求。在真实世界数据集上的大量实验验证了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cold-start item recommendation is a significant challenge in recommendationsystems, particularly when new items are introduced without any historicalinteraction data. While existing methods leverage multi-modal content toalleviate the cold-start issue, they often neglect the inherent multi-viewstructure of modalities, the distinction between shared and modality-specificfeatures. In this paper, we propose Multi-Modal Multi-View VariationalAutoEncoder (M^2VAE), a generative model that addresses the challenges ofmodeling common and unique views in attribute and multi-modal features, as wellas user preferences over single-typed item features. Specifically, we generatetype-specific latent variables for item IDs, categorical attributes, and imagefeatures, and use Product-of-Experts (PoE) to derive a common representation. Adisentangled contrastive loss decouples the common view from unique views whilepreserving feature informativeness. To model user inclinations, we employ apreference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.We further incorporate co-occurrence signals via contrastive learning,eliminating the need for pretraining. Extensive experiments on real-worlddatasets validate the effectiveness of our approach.</description>
      <author>example@mail.com (Chuan He, Yongchao Liu, Qiang Li, Wenliang Zhong, Chuntao Hong, Xinwei Yao)</author>
      <guid isPermaLink="false">2508.00452v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA</title>
      <link>http://arxiv.org/abs/2508.00719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DAMR框架，结合符号搜索和自适应路径评估，解决现有KGQA方法在适应性和计算效率方面的局限性，实现高效且上下文感知的知识图谱问答。&lt;h4&gt;背景&lt;/h4&gt;知识图谱问答旨在解释自然语言查询并利用知识图谱的关系和语义结构进行推理。现有方法主要分为两类：基于静态路径提取的检索-推理范式和基于大型语言模型的动态路径生成策略，但前者适应性有限，后者计算成本高且难以准确评估路径。&lt;h4&gt;目的&lt;/h4&gt;解决现有KGQA方法的局限性，提出一种高效且上下文感知的知识图谱问答框架。&lt;h4&gt;方法&lt;/h4&gt;提出DAMR框架，使用基于LLM的规划器引导的蒙特卡洛树搜索主干，每步选择top-k相关关系减少搜索空间；引入基于Transformer的评分器通过交叉注意力编码问题和关系序列；包含动态伪路径优化机制，从搜索过程中生成训练信号。&lt;h4&gt;主要发现&lt;/h4&gt;在多个KGQA基准测试上，DAMR显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;DAMR框架有效解决了现有KGQA方法在适应性和计算效率方面的问题，实现了更高效准确的知识图谱问答。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱问答(KGQA)旨在解释自然语言查询并通过利用知识图谱的关系和语义结构进行结构化推理来检索准确答案。最近的KGQA方法主要遵循检索-推理范式，依赖GNN或启发式规则进行静态路径提取，或使用大型语言模型(LLM)通过提示进行动态路径生成策略，共同执行检索和推理。然而，前者由于静态路径提取和缺乏上下文优化而适应性有限，后者由于依赖固定评分函数和大量LLM调用导致计算成本高且难以准确评估路径。为解决这些问题，本文提出了基于动态自适应蒙特卡洛树搜索的推理(DAMR)，这是一个结合符号搜索和自适应路径评估的新型框架，用于高效且上下文感知的KGQA。DAMR采用基于LLM的规划器引导的蒙特卡洛树搜索(MCTS)主干，在每一步选择top-k相关关系以减少搜索空间。为提高路径评估准确性，我们引入了一个基于轻量级Transformer的评分器，通过交叉注意力共同编码问题和关系序列，执行上下文感知的合理性估计，使模型能够捕获多跳推理过程中的细粒度语义变化。此外，为缓解高质量监督数据的稀缺性，DAMR纳入了动态伪路径优化机制，定期从搜索过程中探索的部分路径生成训练信号，使评分器能够持续适应推理轨迹的演变分布。在多个KGQA基准上的广泛实验表明，DAMR显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graph Question Answering (KGQA) aims to interpret natural languagequeries and perform structured reasoning over knowledge graphs by leveragingtheir relational and semantic structures to retrieve accurate answers. RecentKGQA methods primarily follow either retrieve-then-reason paradigm, relying onGNNs or heuristic rules for static paths extraction, or dynamic path generationstrategies that use large language models (LLMs) with prompting to jointlyperform retrieval and reasoning. However, the former suffers from limitedadaptability due to static path extraction and lack of contextual refinement,while the latter incurs high computational costs and struggles with accuratepath evaluation due to reliance on fixed scoring functions and extensive LLMcalls. To address these issues, this paper proposes Dynamically AdaptiveMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic searchwith adaptive path evaluation for efficient and context-aware KGQA. DAMRemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-basedplanner, which selects top-$k$ relevant relations at each step to reduce searchspace. To improve path evaluation accuracy, we introduce a lightweightTransformer-based scorer that performs context-aware plausibility estimation byjointly encoding the question and relation sequence through cross-attention,enabling the model to capture fine-grained semantic shifts during multi-hopreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,DAMR incorporates a dynamic pseudo-path refinement mechanism that periodicallygenerates training signals from partial paths explored during search, allowingthe scorer to continuously adapt to the evolving distribution of reasoningtrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMRsignificantly outperforms state-of-the-art methods.</description>
      <author>example@mail.com (Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu)</author>
      <guid isPermaLink="false">2508.00719v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Network Dismantling without Handcrafted Inputs</title>
      <link>http://arxiv.org/abs/2508.00706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MIND的消息迭代网络拆除器模型，通过引入注意力机制和利用消息迭代配置文件，消除了对手工制作结构特征的需求，实现了在大型真实网络上高效解决网络拆除问题。&lt;h4&gt;背景&lt;/h4&gt;消息传递图神经网络在网络科学问题上取得了突破性进展，但其竞争性能通常依赖于手工制作的特征作为输入，这增加了计算成本并引入了偏差。&lt;h4&gt;目的&lt;/h4&gt;消除对手工制作特征的需求，构建表达性强的消息传递框架，高效解决网络拆除(NP难问题)及其相关的关键节点识别问题。&lt;h4&gt;方法&lt;/h4&gt;引入注意力机制和消息迭代配置文件，采用有效算法生成结构多样化的合成训练网络集，提出MIND模型，仅使用多样化合成网络进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;MIND模型能够在仅使用多样化合成网络训练的情况下，成功推广到包含数百万节点的大型未见真实网络，性能优于现有最先进的网络拆除方法。&lt;h4&gt;结论&lt;/h4&gt;所提出模型的高效率和泛化能力不仅限于网络拆除问题，还可以在一系列复杂网络问题中得到广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;消息传递图神经网络的应用已成为网络科学重要问题的一个突破。然而，其竞争性能通常依赖于使用手工制作的特征作为输入，这增加了计算成本，并对原本纯数据驱动的网络表示引入了偏差。在此，我们通过引入注意力机制和利用消息迭代配置文件，消除了对手工制作特征的需求，同时采用有效的算法方法生成结构多样化的小型合成训练网络集。由此，我们构建了一个表达性强的消息传递框架，并利用它高效解决网络拆除这一NP难问题，该问题几乎等同于关键节点识别，具有重要的实际应用。仅使用多样化的合成网络进行训练，我们提出的模型——MIND：消息迭代网络拆除器——能够推广到包含数百万节点的大型未见真实网络，性能优于最先进的网络拆除方法。所提出模型在效率和泛化能力方面的提升可以在拆除之外的一系列复杂网络问题中得到利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of message-passing Graph Neural Networks has been abreakthrough for important network science problems. However, the competitiveperformance often relies on using handcrafted structural features as inputs,which increases computational cost and introduces bias into the otherwisepurely data-driven network representations. Here, we eliminate the need forhandcrafted features by introducing an attention mechanism and utilizingmessage-iteration profiles, in addition to an effective algorithmic approach togenerate a structurally diverse training set of small synthetic networks.Thereby, we build an expressive message-passing framework and use it toefficiently solve the NP-hard problem of Network Dismantling, virtuallyequivalent to vital node identification, with significant real-worldapplications. Trained solely on diversified synthetic networks, our proposedmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,unseen real networks with millions of nodes, outperforming state-of-the-artnetwork dismantling methods. Increased efficiency and generalizability of theproposed model can be leveraged beyond dismantling in a range of complexnetwork problems.</description>
      <author>example@mail.com (Haozhe Tian, Pietro Ferraro, Robert Shorten, Mahdi Jalili, Homayoun Hamedmoghadam)</author>
      <guid isPermaLink="false">2508.00706v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data</title>
      <link>http://arxiv.org/abs/2508.00615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于相似性的自构建图模型(SBSCGM)和混合图神经网络架构(HybridGraphMedGNN)，用于预测ICU患者的临界状态。该模型利用多模态电子健康记录数据构建患者相似性图，并通过图神经网络预测患者死亡风险和连续临界评分，实验证明其性能达到最先进水平且具有可解释性。&lt;h4&gt;背景&lt;/h4&gt;准确预测ICU患者的临界状态(如ICU内死亡风险)对于重症监护的早期干预至关重要。然而，传统模型通常将每个患者视为独立个体，难以利用电子健康记录(EHR)中的关系结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用电子健康记录中关系结构的模型，以提高ICU患者临界状态预测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出相似性自构建图模型(SBSCGM)，从多模态EHR数据动态构建患者相似性图；2. 开发混合图神经网络架构(HybridGraphMedGNN)，整合图卷积网络(GCN)、GraphSAGE和图注意力网络(GAT)层；3. 使用混合相似度度量实时连接具有相似临床特征的患者；4. 利用局部和全局图模式学习鲁棒的患者表示。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在MIMIC-III数据集的6,000次ICU住院实验中，模型达到AUC-ROC 0.94的最先进性能；2. 优于基线分类器和单类型GNN模型；3. 提高了精确度和召回率；4. 注意力机制为模型预测提供了可解释的见解。&lt;h4&gt;结论&lt;/h4&gt;该框架为重症监护风险预测提供了可扩展和可解释的解决方案，有潜力支持临床医生在现实ICU环境中部署使用。&lt;h4&gt;翻译&lt;/h4&gt;准确预测ICU患者的临界状态(如ICU内死亡风险)对于重症监护的早期干预至关重要。然而，传统模型通常将每个患者视为独立个体，难以利用电子健康记录(EHR)中的关系结构。我们提出了一种基于相似性的自构建图模型(SBSCGM)，该模型从多模态EHR数据动态构建患者相似性图，以及一种混合图神经网络架构(HybridGraphMedGNN)，在该图上操作以预测患者死亡和连续临界评分。SBSCGM使用混合相似度度量(结合基于特征和结构相似度)实时连接具有相似临床特征的患者。HybridGraphMedGNN整合了图卷积网络(GCN)、GraphSAGE和图注意力网络(GAT)层，以学习鲁棒的患者表示，利用局部和全局图模式。在MIMIC-III数据集的6,000次ICU住院实验中，我们的模型达到了最先进的性能(AUC-ROC 0.94)，优于基线分类器和单类型GNN模型。我们还展示了改进的精确度/召回率，并证明注意力机制为模型预测提供了可解释的见解。我们的框架为重症监护风险预测提供了可扩展和可解释的解决方案，有潜力支持临床医生在现实ICU环境中部署使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the criticalness of ICU patients (such as in-ICUmortality risk) is vital for early intervention in critical care. However,conventional models often treat each patient in isolation and struggle toexploit the relational structure in Electronic Health Records (EHR). We proposea Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically buildsa patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNNarchitecture that operates on this graph to predict patient mortality and acontinuous criticalness score. SBSCGM uses a hybrid similarity measure(combining feature-based and structural similarities) to connect patients withanalogous clinical profiles in real-time. The HybridGraphMedGNN integratesGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)layers to learn robust patient representations, leveraging both local andglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-IIIdataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)outperforming baseline classifiers and single-type GNN models. We alsodemonstrate improved precision/recall and show that the attention mechanismprovides interpretable insights into model predictions. Our framework offers ascalable and interpretable solution for critical care risk prediction, withpotential to support clinicians in real-world ICU deployment.</description>
      <author>example@mail.com (Mukesh Kumar Sahu, Pinki Roy)</author>
      <guid isPermaLink="false">2508.00615v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides</title>
      <link>http://arxiv.org/abs/2508.00578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 12 figures, and 4 tables (references and SI included)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于机器学习的势能面学习方法，用于模拟氢原子转移反应，在生物相关系统中实现了量子化学准确性。&lt;h4&gt;背景&lt;/h4&gt;氢原子转移反应在生物过程中至关重要，如受损蛋白质中的自由基迁移，但其机制尚未完全理解。模拟HAT具有挑战性，因为需要在生物相关尺度上达到量子化学准确性，而经典力场和基于DFT的分子动力学都不适用。&lt;h4&gt;目的&lt;/h4&gt;系统生成肽中的HAT构型构建大型数据集，基准测试图神经网络架构学习HAT势能面的能力，将ML势能整合到大规模胶原蛋白模拟中，计算反应速率，推进对HAT和自由基迁移机制的理解。&lt;h4&gt;方法&lt;/h4&gt;使用半经验方法和DFT系统生成肽中的HAT构型构建数据集，基准测试三种图神经网络架构(SchNet, Allegro, 和 MACE)，分析标度律、模型可转移性和成本-性能权衡，探索结合ML势能和过渡态搜索算法以及主动学习的改进策略。&lt;h4&gt;主要发现&lt;/h4&gt;MACE在能量、力和势垒预测方面始终优于其他模型，在分布外DFT势垒预测上达到1.13千卡/摩尔的平均绝对误差，这种准确性使得能够将ML势能整合到大规模胶原蛋白模拟中。&lt;h4&gt;结论&lt;/h4&gt;该方法可推广到其他生物分子系统，能够在复杂环境中实现化学反应的量子精确模拟。&lt;h4&gt;翻译&lt;/h4&gt;氢原子转移反应在许多生物过程中至关重要，如受损蛋白质中的自由基迁移，但其机制途径尚未完全理解。模拟HAT具有挑战性，因为需要在生物相关尺度上达到量子化学准确性；因此，经典力场和基于DFT的分子动力学都不适用。机器学习势能提供了一种替代方案，能够以接近量子的准确性学习势能面。然而，训练这些模型以在多样化的HAT构型中泛化，特别是在蛋白质的自由基位置，需要量身定制的数据生成和仔细的模型选择。在此，我们使用半经验方法和DFT系统生成肽中的HAT构型来构建大型数据集。我们基准测试了三种图神经网络架构学习HAT势能面的能力，并从能量预测间接预测反应势垒的能力。MACE在能量、力和势垒预测方面始终优于其他模型，在分布外DFT势垒预测上达到1.13千卡/摩尔的平均绝对误差。这种准确性使得能够将ML势能整合到大规模胶原蛋白模拟中，从预测的势垒计算反应速率，推进对HAT和肽中自由基迁移的机制理解。我们分析了标度律、模型可转移性和成本-性能权衡，并概述了通过结合ML势能和过渡态搜索算法以及主动学习来改进的策略。我们的方法可推广到其他生物分子系统，能够在复杂环境中实现化学反应的量子精确模拟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hydrogen atom transfer (HAT) reactions are essential in many biologicalprocesses, such as radical migration in damaged proteins, but their mechanisticpathways remain incompletely understood. Simulating HAT is challenging due tothe need for quantum chemical accuracy at biologically relevant scales; thus,neither classical force fields nor DFT-based molecular dynamics are applicable.Machine-learned potentials offer an alternative, able to learn potential energysurfaces (PESs) with near-quantum accuracy. However, training these models togeneralize across diverse HAT configurations, especially at radical positionsin proteins, requires tailored data generation and careful model selection.Here, we systematically generate HAT configurations in peptides to build largedatasets using semiempirical methods and DFT. We benchmark three graph neuralnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HATPESs and indirectly predict reaction barriers from energy predictions. MACEconsistently outperforms the others in energy, force, and barrier prediction,achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFTbarrier predictions. This accuracy enables integration of ML potentials intolarge-scale collagen simulations to compute reaction rates from predictedbarriers, advancing mechanistic understanding of HAT and radical migration inpeptides. We analyze scaling laws, model transferability, and cost-performancetrade-offs, and outline strategies for improvement by combining ML potentialswith transition state search algorithms and active learning. Our approach isgeneralizable to other biomolecular systems, enabling quantum-accuratesimulations of chemical reactivity in complex environments.</description>
      <author>example@mail.com (Marlen Neubert, Patrick Reiser, Frauke Gräter, Pascal Friederich)</author>
      <guid isPermaLink="false">2508.00578v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.00507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM Multimedia 2025 (MM '25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoLL的新型框架，结合大型语言模型和图神经网络，用于文本属性图(TAG)中的异常检测，实现了平均13.37%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;文本属性图(TAGs)结合了复杂的拓扑结构和丰富的文本信息，为图异常检测(GAD)提供了新视角。然而，现有方法主要关注图域内的复杂优化目标，忽视了文本模态的补充价值，且文本特征通常由浅层嵌入技术编码，可能导致与异常相关的语义上下文被遗漏。&lt;h4&gt;目的&lt;/h4&gt;释放文本模态的巨大潜力，解决大型语言模型在TAG异常检测应用中的局限性，并克服LLMs因输入长度限制而难以编码图中固有高阶结构信息的问题。&lt;h4&gt;方法&lt;/h4&gt;提出CoLL框架，结合大型语言模型(LLMs)和图神经网络(GNNs)的互补优势。CoLL采用多LLM协作进行证据增强生成，捕获与异常相关的上下文并提供人类可读的解释；同时集成带有门控机制的GNN，自适应融合文本特征和证据，保留高阶拓扑信息。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明CoLL具有优越性，在AP指标上平均提高了13.37%。&lt;h4&gt;结论&lt;/h4&gt;这项研究为将大型语言模型整合到图异常检测的进展中开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图(TAGs)中复杂的拓扑结构与丰富的文本信息的自然结合，为图异常检测(GAD)开辟了新视角。然而，现有的GAD方法主要专注于在图域内设计复杂的优化目标，忽视了文本模态的补充价值，其特征通常由bag-of-words或skip-gram等浅层嵌入技术编码，因此可能与异常相关的语义上下文被遗漏。为了释放文本模态的巨大潜力，大型语言模型(LLMs)由于其强大的语义理解和推理能力，已成为有希望的替代方案。然而，它们在TAG异常检测中的应用仍处于起步阶段，并且由于输入长度限制，它们难以编码图中固有高阶结构信息。为了在TAGs中进行高质量的异常检测，我们提出了CoLL，这是一个结合LLMs和图神经网络(GNNs)以利用它们互补优势的新型框架。CoLL采用多LLM协作进行证据增强生成，以捕获与异常相关的上下文，同时为检测到的异常提供人类可读的解释。此外，CoLL集成了一个带有门控机制的GNN，以自适应地融合文本特征和证据，同时保留高阶拓扑信息。大量实验证明了CoLL的优越性，在AP上平均提高了13.37%。这项研究为将LLMs整合到推进GAD中开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The natural combination of intricate topological structures and rich textualinformation in text-attributed graphs (TAGs) opens up a novel perspective forgraph anomaly detection (GAD). However, existing GAD methods primarily focus ondesigning complex optimization objectives within the graph domain, overlookingthe complementary value of the textual modality, whose features are oftenencoded by shallow embedding techniques, such as bag-of-words or skip-gram, sothat semantic context related to anomalies may be missed. To unleash theenormous potential of textual modality, large language models (LLMs) haveemerged as promising alternatives due to their strong semantic understandingand reasoning capabilities. Nevertheless, their application to TAG anomalydetection remains nascent, and they struggle to encode high-order structuralinformation inherent in graphs due to input length constraints. Forhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework thatcombines LLMs and graph neural networks (GNNs) to leverage their complementarystrengths. CoLL employs multi-LLM collaboration for evidence-augmentedgeneration to capture anomaly-relevant contexts while delivering human-readablerationales for detected anomalies. Moreover, CoLL integrates a GNN equippedwith a gating mechanism to adaptively fuse textual features with evidence whilepreserving high-order topological information. Extensive experimentsdemonstrate the superiority of CoLL, achieving an average improvement of 13.37%in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.</description>
      <author>example@mail.com (Yiming Xu, Jiarun Chen, Zhen Peng, Zihan Chen, Qika Lin, Lan Ma, Bin Shi, Bo Dong)</author>
      <guid isPermaLink="false">2508.00507v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool</title>
      <link>http://arxiv.org/abs/2508.00506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Video supplement demonstrating feature-space exploration and  interactive labelling is available at: https://youtu.be/GZl1ebZJgEA and is  archived at https://doi.org/10.5281/zenodo.16676591&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于无监督学习的遥感图像标记方法，利用卷积神经网络和图神经网络分割和编码Sentinel-2卫星图像，实现地理区域的自动标记&lt;h4&gt;背景&lt;/h4&gt;机器学习在遥感成像中的应用依赖于最新和准确的标签进行模型训练和测试，但遥感图像标记需要大量时间和成本，需要专家分析&lt;h4&gt;目的&lt;/h4&gt;定义一个无监督管道，用于在Sentinel-2卫星图像中寻找和标记具有相似上下文和内容的地理区域&lt;h4&gt;方法&lt;/h4&gt;利用分割技术与卷积神经网络和图神经网络相结合，为图像比较编码更强大的特征空间；将图像分割成基于颜色和空间相似性分组的同质像素区域；使用图神经网络聚合周围片段的信息&lt;h4&gt;主要发现&lt;/h4&gt;该方法减少了标记工具中的异常值；允许用户在细粒度级别进行标记；允许在编码空间内形成图像级别的旋转不变语义关系&lt;h4&gt;结论&lt;/h4&gt;该方法通过去除先前方法的局限性，提供了一种更有效的遥感图像标记方法&lt;h4&gt;翻译&lt;/h4&gt;Machine learning for remote sensing imaging relies on up-to-date and accurate labels for model training and testing. Labelling remote sensing imagery is time and cost intensive, requiring expert analysis. Previous labelling tools rely on pre-labelled data for training in order to label new unseen data. In this work, we define an unsupervised pipeline for finding and labelling geographical areas of similar context and content within Sentinel-2 satellite imagery. Our approach removes limitations of previous methods by utilising segmentation with convolutional and graph neural networks to encode a more robust feature space for image comparison. Unlike previous approaches we segment the image into homogeneous regions of pixels that are grouped based on colour and spatial similarity. Graph neural networks are used to aggregate information about the surrounding segments enabling the feature representation to encode the local neighbourhood whilst preserving its own local information. This reduces outliers in the labelling tool, allows users to label at a granular level, and allows a rotationally invariant semantic relationship at the image level to be formed within the encoding space.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning for remote sensing imaging relies on up-to-date and accuratelabels for model training and testing. Labelling remote sensing imagery is timeand cost intensive, requiring expert analysis. Previous labelling tools rely onpre-labelled data for training in order to label new unseen data. In this work,we define an unsupervised pipeline for finding and labelling geographical areasof similar context and content within Sentinel-2 satellite imagery. Ourapproach removes limitations of previous methods by utilising segmentation withconvolutional and graph neural networks to encode a more robust feature spacefor image comparison. Unlike previous approaches we segment the image intohomogeneous regions of pixels that are grouped based on colour and spatialsimilarity. Graph neural networks are used to aggregate information about thesurrounding segments enabling the feature representation to encode the localneighbourhood whilst preserving its own local information. This reducesoutliers in the labelling tool, allows users to label at a granular level, andallows a rotationally invariant semantic relationship at the image level to beformed within the encoding space.</description>
      <author>example@mail.com (Tulsi Patel, Mark W. Jones, Thomas Redfern)</author>
      <guid isPermaLink="false">2508.00506v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network</title>
      <link>http://arxiv.org/abs/2508.00429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ReaGAN的新型图神经网络框架，通过智能体规划和检索增强生成技术解决了传统GNN在节点信息不平衡和全局语义关系捕获方面的局限性，实现了在少样本场景下的高性能表现。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)通过在邻居节点之间传播信息在基于图的学习中取得了显著成功，主要通过预定义的聚合机制实现。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN的两个关键局限性：一是无法处理节点信息不平衡问题，二是预定义消息传递主要利用局部结构相似性而忽略全局语义关系，限制了模型捕获远距离相关信息的能力。&lt;h4&gt;方法&lt;/h4&gt;提出Retrieval-augmented Graph Agentic Network (ReaGAN)，这是一个基于智能体的框架，使每个节点能够自主进行节点级决策。每个节点作为智能体基于内部记忆独立规划下一个动作，实现节点级规划和自适应消息传播。同时，使用检索增强生成(RAG)技术使节点能够访问语义相关内容并在图中建立全局关系。&lt;h4&gt;主要发现&lt;/h4&gt;ReaGAN在少样本上下文设置下使用冻结的LLM主干架构无需微调即可实现有竞争力的性能，展示了智能体规划和局部-全局检索在图学习中的潜力。&lt;h4&gt;结论&lt;/h4&gt;智能体规划和检索增强技术可以有效解决传统图神经网络在处理节点信息不平衡和捕获全局语义关系方面的局限性，为图学习领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通过在邻居节点之间通过预定义的聚合机制传播信息，在基于图的学习中取得了显著成功。然而，这种固定方案通常面临两个关键局限性。首先，它们无法处理节点信息不平衡的问题——一些节点信息丰富，而另一些则保持稀疏。其次，预定义的消息传递主要利用局部结构相似性，而忽略了图中的全局语义关系，限制了模型捕获远距离但相关信息的能力。我们提出了检索增强图智能体网络(ReaGAN)，这是一个基于智能体的框架，使每个节点具备自主的节点级决策能力。每个节点作为一个智能体，基于其内部记忆独立规划下一个行动，实现节点级规划和自适应消息传播。此外，检索增强生成(RAG)使节点能够访问语义相关内容并在图中建立全局关系。ReaGAN在少样本上下文设置下使用冻结的LLM主干架构无需微调即可实现有竞争力的性能，展示了智能体规划和局部-全局检索在图学习中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved remarkable success in graph-basedlearning by propagating information among neighbor nodes via predefinedaggregation mechanisms. However, such fixed schemes often suffer from two keylimitations. First, they cannot handle the imbalance in node informativeness --some nodes are rich in information, while others remain sparse. Second,predefined message passing primarily leverages local structural similaritywhile ignoring global semantic relationships across the graph, limiting themodel's ability to capture distant but relevant information. We proposeRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based frameworkthat empowers each node with autonomous, node-level decision-making. Each nodeacts as an agent that independently plans its next action based on its internalmemory, enabling node-level planning and adaptive message propagation.Additionally, retrieval-augmented generation (RAG) allows nodes to accesssemantically relevant content and build global relationships in the graph.ReaGAN achieves competitive performance under few-shot in-context settingsusing a frozen LLM backbone without fine-tuning, showcasing the potential ofagentic planning and local-global retrieval in graph learning.</description>
      <author>example@mail.com (Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang)</author>
      <guid isPermaLink="false">2508.00429v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization</title>
      <link>http://arxiv.org/abs/2508.00357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SGPC的新型图神经网络架构，通过结合胞层消息传递、最优传输提升、方差减少扩散和PAC-Bayes谱正则化机制，有效解决了图神经网络中的过平滑问题，并在多个基准测试上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络中的过平滑问题导致节点特征崩溃，尤其在异质图上问题更严重，相邻节点通常具有不同标签。现有的层神经网络虽部分缓解此问题，但依赖静态或高度参数化的层结构，阻碍了泛化和可扩展性，且无法提供严格的稳定性保证。&lt;h4&gt;目的&lt;/h4&gt;引入SGPC(基于PAC-Bayes校准的层神经网络)方案，创建统一架构，结合多种机制实现稳健的半监督节点分类，同时提供理论性能边界和认证置信区间。&lt;h4&gt;方法&lt;/h4&gt;SGPC结合胞层消息传递与三种机制：基于最优传输的提升、方差减少的扩散和PAC-Bayes谱正则化。通过理论建立性能边界，证明可通过端到端训练实现边界感知目标，计算复杂度为线性。&lt;h4&gt;主要发现&lt;/h4&gt;在九个同质和异质基准测试上，SGPC性能优于最先进的谱和基于层的GNN，同时能为未见过的节点提供认证的置信区间。&lt;h4&gt;结论&lt;/h4&gt;SGPC是一种创新的层神经网络方法，有效解决了GNN中的过平滑问题，在理论保证和实际性能之间取得了良好平衡，具有更好的泛化能力和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络中的过平滑问题导致节点特征崩溃，特别是在相邻节点通常具有不同标签的异质图上。虽然层神经网络部分缓解了这个问题，但它们通常依赖静态或高度参数化的层结构，阻碍了泛化和可扩展性。现有的基于层的方法要么预定义限制映射，要么引入过多复杂性，但未能提供严格的稳定性保证。在本文中，我们引入了一种名为SGPC(基于PAC-Bayes校准的层神经网络)的新方案，这是一种统一架构，结合了胞层消息传递和多种机制，包括基于最优传输的提升、方差减少的扩散和PAC-Bayes谱正则化，用于稳健的半监督节点分类。我们从理论上建立了性能边界，并证明可以通过线性计算复杂度的端到端训练实现边界感知目标。在九个同质和异质基准测试上的实验表明，SGPC优于最先进的谱和基于层的GNN，同时为未见过的节点提供认证的置信区间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinctnode features, particularly on heterophilic graphs where adjacent nodes oftenhave dissimilar labels. Although sheaf neural networks partially mitigate thisproblem, they typically rely on static or heavily parameterized sheafstructures that hinder generalization and scalability. Existing sheaf-basedmodels either predefine restriction maps or introduce excessive complexity, yetfail to provide rigorous stability guarantees. In this paper, we introduce anovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unifiedarchitecture that combines cellular-sheaf message passing with severalmechanisms, including optimal transport-based lifting, variance-reduceddiffusion, and PAC-Bayes spectral regularization for robust semi-supervisednode classification. We establish performance bounds theoretically anddemonstrate that the resulting bound-aware objective can be achieved viaend-to-end training in linear computational complexity. Experiments on ninehomophilic and heterophilic benchmarks show that SGPC outperformsstate-of-the-art spectral and sheaf-based GNNs while providing certifiedconfidence intervals on unseen nodes.</description>
      <author>example@mail.com (Yoonhyuk Choi, Jiho Choi, Chong-Kwon Kim)</author>
      <guid isPermaLink="false">2508.00357v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition</title>
      <link>http://arxiv.org/abs/2508.00205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的真实性格识别(RPR)方法，通过模拟与表达行为相关的个性化内部认知，使用二维图神经网络(2D-GNN)来准确推断真实性格特质。&lt;h4&gt;背景&lt;/h4&gt;现有的自动真实性格识别方法通常作为外部观察者，基于目标个体的表达行为来推断观察者对他们的性格印象，这种方法与真实性格有显著偏差，导致识别性能不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从目标个体容易获取的外部短音频-视觉行为中有效模拟个性化内部认知，并准确推断真实性格特质的方法。&lt;h4&gt;方法&lt;/h4&gt;1) 模拟个性化内部认知，表示为使个性化网络重现个体特定面部反应的网络权重；2) 将模拟的认知编码为包含二维节点和边特征矩阵的新型图结构；3) 使用二维图神经网络(2D-GNN)从该图中推断真实性格特质；4) 设计端到端策略联合训练认知模拟、图构建和性格识别模块。&lt;h4&gt;主要发现&lt;/h4&gt;真实性格与生成表达行为的人类内部认知之间存在关联，这种关联可用于改进真实性格识别性能。&lt;h4&gt;结论&lt;/h4&gt;通过模拟与真实性格相关的认知并使用2D-GNN进行推断，可以更准确地识别真实性格特质，优于传统的外部观察者方法。&lt;h4&gt;翻译&lt;/h4&gt;自动真实性格识别(RPR)旨在从人们的表达行为中评估真实性格特质。然而，大多数现有解决方案通常作为外部观察者，基于目标个体的表达行为来推断观察者对他们的性格印象，这与他们的真实性格有显著偏差，并持续导致识别性能不佳。受真实性格与生成表达行为的人类内部认知之间关联的启发，我们提出了一种新的RPR方法，能够从目标个体容易获取的外部短音频-视觉行为中有效模拟个性化的内部认知。模拟的个性化认知表示为一组网络权重，这些权重使个性化网络能够重现个体特定的面部反应，进一步编码为包含二维节点和边特征矩阵的新型图结构，并提出了一种新的二维图神经网络(2D-GNN)用于从中推断真实性格特质。为了模拟与真实性格相关的认知，我们设计了一种端到端策略，联合训练认知模拟、2D图构建和性格识别模块。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic real personality recognition (RPR) aims to evaluate human realpersonality traits from their expressive behaviours. However, most existingsolutions generally act as external observers to infer observers' personalityimpressions based on target individuals' expressive behaviours, whichsignificantly deviate from their real personalities and consistently lead toinferior recognition performance. Inspired by the association between realpersonality and human internal cognition underlying the generation ofexpressive behaviours, we propose a novel RPR approach that efficientlysimulates personalised internal cognition from easy-accessible external shortaudio-visual behaviours expressed by the target individual. The simulatedpersonalised cognition, represented as a set of network weights that enforcethe personalised network to reproduce the individual-specific facial reactions,is further encoded as a novel graph containing two-dimensional node and edgefeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed forinferring real personality traits from it. To simulate real personality-relatedcognition, an end-to-end strategy is designed to jointly train our cognitionsimulation, 2D graph construction, and personality recognition modules.</description>
      <author>example@mail.com (Xiangyu Kong, Hengde Zhu, Haoqin Sun, Zhihao Guo, Jiayan Gu, Xinyi Ni, Wei Zhang, Shizhe Liu, Siyang Song)</author>
      <guid isPermaLink="false">2508.00205v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.00141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了INSPIRE-GNN框架，一种结合强化学习和图神经网络的混合模型，用于优化传感器放置和提高数据稀疏环境下的自行车流量估计&lt;h4&gt;背景&lt;/h4&gt;准确的链路级自行车流量估计对可持续城市交通规划至关重要，但许多城市面临自行车计数传感器覆盖有限导致的数据稀疏性挑战&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来优化传感器放置并提高数据稀疏环境下的链路级自行车流量估计&lt;h4&gt;方法&lt;/h4&gt;INSPIRE-GNN整合了图卷积网络、图注意力网络和基于深度Q网络的强化学习代理，实现数据驱动的传感器位置选择以最大化估计性能&lt;h4&gt;主要发现&lt;/h4&gt;在墨尔本自行车网络（15,933个路段中仅141个有传感器覆盖，99%稀疏性）上应用INSPIRE-GNN，通过战略性地选择额外传感器位置，在50、100、200和500个传感器的部署中显著提高了流量估计准确性，并在均方误差、均方根误差和平均绝对误差等指标上优于传统启发式方法&lt;h4&gt;结论&lt;/h4&gt;该框架为交通规划者提供了可行的见解，可有效扩展传感器网络、优化传感器放置，并最大化自行车数据的流量估计准确性和可靠性，支持知情交通规划决策&lt;h4&gt;翻译&lt;/h4&gt;准确的链路级自行车流量估计对可持续城市交通规划至关重要。然而，许多城市由于自行车计数传感器覆盖有限而面临严重的稀疏数据挑战。为解决这个问题，我们提出了INSPIRE-GNN，一种新颖的强化学习增强混合图神经网络框架，旨在优化传感器放置并提高数据稀疏环境下的链路级自行车流量估计。INSPIRE-GNN将图卷积网络和图注意力网络与基于深度Q网络的强化学习代理相结合，实现了数据驱动的传感器位置战略选择，以最大化估计性能。应用于墨尔本的自行车网络（包含15,933个路段，仅141个路段有传感器覆盖，99%稀疏性），INSPIRE-GNN通过战略性地选择额外传感器位置，在50、100、200和500个传感器的部署中显著提高了流量估计。我们的框架在均方误差、均方根误差和平均绝对误差等关键指标上，优于介数中心性、接近中心性、观察到的自行车活动和随机放置等传统启发式传感器放置方法。此外，我们的实验将INSPIRE-GNN与标准机器学习和深度学习模型在自行车流量估计性能上进行基准测试，证明了其有效性。我们提出的框架为交通规划者提供了可行的见解，以有效扩展传感器网络、优化传感器放置，并最大化自行车数据的流量估计准确性和可靠性，为知情交通规划决策提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate link-level bicycling volume estimation is essential for sustainableurban transportation planning. However, many cities face significant challengesof high data sparsity due to limited bicycling count sensor coverage. Toaddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimizesensor placement and improve link-level bicycling volume estimation indata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RLagent, enabling a data-driven strategic selection of sensor locations tomaximize estimation performance. Applied to Melbourne's bicycling network,comprising 15,933 road segments with sensor coverage on only 141 road segments(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volumeestimation by strategically selecting additional sensor locations indeployments of 50, 100, 200 and 500 sensors. Our framework outperformstraditional heuristic methods for sensor placement such as betweennesscentrality, closeness centrality, observed bicycling activity and randomplacement, across key metrics such as Mean Squared Error (MSE), Root MeanSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, ourexperiments benchmark INSPIRE-GNN against standard machine learning and deeplearning models in the bicycle volume estimation performance, underscoring itseffectiveness. Our proposed framework provides transport planners actionableinsights to effectively expand sensor networks, optimize sensor placement andmaximize volume estimation accuracy and reliability of bicycling data forinformed transportation planning decisions.</description>
      <author>example@mail.com (Mohit Gupta, Debjit Bhowmick, Rhys Newbury, Meead Saberi, Shirui Pan, Ben Beck)</author>
      <guid isPermaLink="false">2508.00141v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion</title>
      <link>http://arxiv.org/abs/2508.00037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE Transactions on Industrial Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ScaleSTF的可扩展时空Transformer模型，解决了大规模城市网络中预测模型的效能与效率权衡问题，基于物理定律启发设计，具有线性复杂度，在大规模城市系统中表现出色。&lt;h4&gt;背景&lt;/h4&gt;网络化城市系统促进人流、资源和服务的流动，对经济和社会互动至关重要。这些系统涉及具有未知规则的复杂过程，通过传感器时间序列观测。当前预测模型如图神经网络在大规模应用中面临效能与效率的权衡。&lt;h4&gt;目的&lt;/h4&gt;解决预测模型在大规模网络应用中面临的效能与效率权衡挑战，设计一种高性能且计算效率高的模型。&lt;h4&gt;方法&lt;/h4&gt;从物理定律获取灵感，指导基本模型设计，避免架构冗余。提出基于Transformer结构的可解释神经扩散方案，其注意层由低维嵌入诱导，开发出具有线性复杂度的ScaleSTF模型。&lt;h4&gt;主要发现&lt;/h4&gt;ScaleSTF模型在交通流量、太阳能和智能电表等大规模城市系统验证中，展示了最先进的性能和显著的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;研究结果为大规模城市网络的动态预测提供了新的视角和方法。&lt;h4&gt;翻译&lt;/h4&gt;网络化城市系统促进了人流、资源和服务的流动，对经济和社会互动至关重要。这些系统通常涉及具有未知规则的复杂过程，通过基于传感器的时间序列进行观测。为了辅助工业和工程环境中的决策制定，数据驱动的预测模型被用于预测城市系统的时空动态。当前模型如图神经网络已显示出前景，但由于计算需求，它们在效能和效率之间面临权衡。因此，它们在大规模网络中的应用仍需进一步努力。本文通过从物理定律中获取灵感，解决了这一权衡挑战，指导了符合基本原理且避免架构冗余的基本模型设计。通过理解微观和宏观过程，我们提出了一种基于Transformer结构的可解释神经扩散方案，其注意层由低维嵌入诱导。所提出的具有线性复杂度的可扩展时空Transformer（ScaleSTF）在包括交通流量、太阳能和智能电表在内的大规模城市系统中得到了验证，展示了最先进的性能和显著的可扩展性。我们的结果为大规模城市网络的动态预测提供了新的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TII.2025.3588614&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Networked urban systems facilitate the flow of people, resources, andservices, and are essential for economic and social interactions. These systemsoften involve complex processes with unknown governing rules, observed bysensor-based time series. To aid decision-making in industrial and engineeringcontexts, data-driven predictive models are used to forecast spatiotemporaldynamics of urban systems. Current models such as graph neural networks haveshown promise but face a trade-off between efficacy and efficiency due tocomputational demands. Hence, their applications in large-scale networks stillrequire further efforts. This paper addresses this trade-off challenge bydrawing inspiration from physical laws to inform essential model designs thatalign with fundamental principles and avoid architectural redundancy. Byunderstanding both micro- and macro-processes, we present a principledinterpretable neural diffusion scheme based on Transformer-like structureswhose attention layers are induced by low-dimensional embeddings. The proposedscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, isvalidated on large-scale urban systems including traffic flow, solar power, andsmart meters, showing state-of-the-art performance and remarkable scalability.Our results constitute a fresh perspective on the dynamics prediction inlarge-scale urban networks.</description>
      <author>example@mail.com (Tong Nie, Jian Sun, Wei Ma)</author>
      <guid isPermaLink="false">2508.00037v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Formal Bayesian Transfer Learning via the Total Risk Prior</title>
      <link>http://arxiv.org/abs/2507.23768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在源数据集也有限且与目标数据集不一定对齐的情况下进行迁移学习的新方法，通过贝叶斯框架进行不确定性量化和模型平均，显著提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;在数据严重受限的分析中，通过辅助数据集增强目标数据集可改进统计程序，但现有迁移学习方法难以处理源数据集有限且与目标数据集不一定对齐的情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在源数据集有限且与目标数据集不一定对齐的情况下进行有效迁移学习的方法。&lt;h4&gt;方法&lt;/h4&gt;提出使用基于源参数的条件风险最小化器而非经验损失最小化器作为目标参数的先验均值，构建单个联合先验分布；通过Gibbs采样对指示变量进行模型平均，实现完整的贝叶斯不确定性量化；讨论了特定先验实例在变换坐标系中导致贝叶斯Lasso的实现，以及扩展到中等规模数据集的计算技术。&lt;h4&gt;主要发现&lt;/h4&gt;最近提出的极小化-频率论迁移学习技术可被视为该模型的最大后验概率近似方法；在遗传学应用中，特别是在源数据有限的情况下，与频率论基线相比显示出优越的预测性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够在源数据有限的情况下，通过贝叶斯框架进行不确定性量化和模型平均，提高预测性能，为数据受限环境下的迁移学习提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;在数据严重受限的分析中，通过应用领域内的辅助数据集（源数据集）来增强目标数据集，可以显著改进统计程序。然而，现有的迁移学习方法难以处理源数据集也有限且与目标数据集不一定对齐的情况。典型策略是使用源数据上的经验损失最小化器作为目标参数的先验均值，这使得源参数的估计脱离贝叶斯框架。我们的关键概念贡献是使用基于源参数的条件风险最小化器。这使我们能够为源数据集和目标数据集的所有参数构建单个联合先验分布。因此，我们受益于完整的贝叶斯不确定性量化，并且可以通过Gibbs采样对指示变量进行模型平均，这些指示变量控制每个源数据集的包含。我们展示了特定先验实例如何在变换坐标系中导致贝叶斯Lasso，并讨论了将该方法扩展到中等规模数据集的计算技术。我们还证明了最近提出的极小化-频率论迁移学习技术可以被视为我们模型的最大后验概率近似方法。最后，我们在遗传学应用中展示了相对于频率论基线的优越预测性能，特别是在源数据有限的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In analyses with severe data-limitations, augmenting the target dataset withinformation from ancillary datasets in the application domain, called sourcedatasets, can lead to significantly improved statistical procedures. However,existing methods for this transfer learning struggle to deal with situationswhere the source datasets are also limited and not guaranteed to bewell-aligned with the target dataset. A typical strategy is to use theempirical loss minimizer on the source data as a prior mean for the targetparameters, which places the estimation of source parameters outside of theBayesian formalism. Our key conceptual contribution is to use a risk minimizerconditional on source parameters instead. This allows us to construct a singlejoint prior distribution for all parameters from the source datasets as well asthe target dataset. As a consequence, we benefit from full Bayesian uncertaintyquantification and can perform model averaging via Gibbs sampling overindicator variables governing the inclusion of each source dataset. We show howa particular instantiation of our prior leads to a Bayesian Lasso in atransformed coordinate system and discuss computational techniques to scale ourapproach to moderately sized datasets. We also demonstrate that recentlyproposed minimax-frequentist transfer learning techniques may be viewed as anapproximate Maximum a Posteriori approach to our model. Finally, we demonstratesuperior predictive performance relative to the frequentist baseline on agenetics application, especially when the source data are limited.</description>
      <author>example@mail.com (Nathan Wycoff, Ali Arab, Lisa O. Singh)</author>
      <guid isPermaLink="false">2507.23768v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
  <item>
      <title>Text-to-SQL Task-oriented Dialogue Ontology Construction</title>
      <link>http://arxiv.org/abs/2507.23358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TeQoDO是一种无需监督的面向任务对话本体构建方法，利用LLM的SQL编程能力和对话理论自主构建本体，优于迁移学习方法，且构建的本体在下游任务中表现良好，有助于提高LLM的可解释性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型被广泛用作通用知识源，但它们依赖参数化知识，限制了可解释性和可信度。在面向任务的对话系统中，通常使用由显式本体结构化的外部数据库来确保可解释性和可控性。&lt;h4&gt;目的&lt;/h4&gt;引入TeQoDO：一种文本到SQL的面向任务对话本体构建方法，解决构建本体需要手动标签或监督训练的问题。&lt;h4&gt;方法&lt;/h4&gt;LLM利用其固有的SQL编程能力，结合提示中提供的对话理论，自主从头构建面向任务对话本体，无需监督。&lt;h4&gt;主要发现&lt;/h4&gt;TeQoDO优于迁移学习方法；其构建的本体在下游对话状态跟踪任务上具有竞争力；消融研究证明对话理论的关键作用；TeQoDO能够扩展构建更大的本体，在Wikipedia和ArXiv数据集上进行了验证。&lt;h4&gt;结论&lt;/h4&gt;这被视为朝着更广泛地将本体应用于提高LLM可解释性迈出的一步。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)被广泛用作通用知识源，但它们依赖于参数化知识，这限制了可解释性和可信度。在面向任务的对话(TOD)系统中，这种分离是明确的，使用由显式本体结构化的外部数据库来确保可解释性和可控性。然而，构建这样的本体需要手动标签或监督训练。我们引入TeQoDO：一种文本到SQL的面向任务对话本体构建方法。在这里，LLM利用其固有的SQL编程能力，结合提示中提供的对话理论，自主从头构建TOD本体，无需监督。我们证明TeQoDO优于迁移学习方法，其构建的本体在下游对话状态跟踪任务上具有竞争力。消融研究证明了对话理论的关键作用。TeQoDO还能够扩展以构建更大的本体，我们在Wikipedia和ArXiv数据集上对此进行了研究。我们认为这是朝着更广泛地将本体应用于提高LLM可解释性迈出的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are widely used as general-purpose knowledgesources, but they rely on parametric knowledge, limiting explainability andtrustworthiness. In task-oriented dialogue (TOD) systems, this separation isexplicit, using an external database structured by an explicit ontology toensure explainability and controllability. However, building such ontologiesrequires manual labels or supervised training. We introduce TeQoDO: aText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLMautonomously builds a TOD ontology from scratch without supervision using itsinherent SQL programming capabilities combined with dialogue theory provided inthe prompt. We show that TeQoDO outperforms transfer learning approaches, andits constructed ontology is competitive on a downstream dialogue state trackingtask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO alsoscales to allow construction of much larger ontologies, which we investigate ona Wikipedia and ArXiv dataset. We view this as a step towards broaderapplication of ontologies to increase LLM explainability.</description>
      <author>example@mail.com (Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic)</author>
      <guid isPermaLink="false">2507.23358v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Search for $t\bar tt\bar tW$ Production at $\sqrt{s} = 13$ TeV Using a Modified Graph Neural Network at the LHC</title>
      <link>http://arxiv.org/abs/2507.23723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究在大型强子对撞机上同时产生四个顶夸克和一个W玻色子的稀有标准模型过程，提出了一种改进的物理信息神经网络方法，在完全强子衰变通道中有效识别这一过程，克服了巨大本底干扰的挑战。&lt;h4&gt;背景&lt;/h4&gt;在质心能量为13 TeV时，同时产生四个顶夸克和一个W玻色子的过程是标准模型中的稀有过程，其截面为6.6加上2.4减去2.6 ab。在完全强子衰变通道中识别这一过程面临来自tar{t}、tar{t}W、tar{t}Z和三顶夸克产生过程的巨大本底干扰，极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法来识别大型强子对撞机上同时产生四个顶夸克和一个W玻色子的稀有过程，特别是在完全强子衰变通道中，以提高信号显著性并克服本底干扰。&lt;h4&gt;方法&lt;/h4&gt;提出了一种改进的物理信息神经网络，这是一种混合图神经网络(GNN)，结合了用于粒子级特征的图层、基于多层感知器(MLP)的自定义全局流(包含量子电路)以及交叉注意力融合机制来结合局部和全局表示。使用物理信息损失函数强制执行源自事件衰变动力学的喷注多重数约束。模型在蒙特卡洛模拟上进行训练，并使用基于截面的重加权对事件进行归一化。&lt;h4&gt;主要发现&lt;/h4&gt;与传统方法相比，所提出的GNN模型在信号显著性方面达到0.174，ROC-AUC达到0.974，显著优于BDT(0.138显著性和0.913 ROC)和Xgboost(0.149显著性和0.920 ROC)。该方法为在350逆飞秒平方积分亮度的数据集中精确选择此类事件提供了有效框架。&lt;h4&gt;结论&lt;/h4&gt;这种增强的方法为大型强子对撞机上的精确事件选择提供了框架，利用高维统计学习和物理信息推断来解决高能物理学的基本挑战，与机器学习的发展相一致。所提出的混合图神经网络方法在识别稀有物理过程方面表现出色，克服了传统方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;在质心能量为13 TeV时，同时产生四个顶夸克和一个W玻色子是标准模型中的稀有过程，其截面为6.6加上2.4减去2.6 ab。在完全强子衰变通道中识别这一过程特别具有挑战性，因为存在来自tar{t}、tar{t}W、tar{t}Z和三顶夸克产生过程的巨大本底。本研究引入了一种改进的物理信息神经网络，一种增强事件分类的混合图神经网络(GNN)。所提出的模型结合了用于粒子级特征的图层，一个基于多层感知器(MLP)的自定义全局流(包含量子电路)以及交叉注意力融合，以结合局部和全局表示。物理信息损失函数强制执行源自事件衰变动力学的喷注多重数约束。与传统方法相比，GNN达到了0.174的信号显著性(信号除以信号加本底的平方根)和0.974的ROC-AUC，优于BDT的0.138显著性和0.913的ROC，而Xgboost达到了0.149的显著性和0.920的ROC。分类模型在蒙特卡洛模拟上进行训练，使用基于截面的重加权对事件进行归一化，以反映在350逆飞秒平方积分亮度数据集中的预期贡献。这种增强的方法为大型强子对撞机上的精确事件选择提供了框架，利用高维统计学习和物理信息推断来解决高能物理学的基本挑战，与机器学习的发展相一致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The simultaneous production of four top quarks in association with a ($W$)boson at $(\sqrt{s} = 13)$ TeV is an rare SM process with anext-to-leading-order (NLO) cross-section of $(6.6^{+2.4}_{-2.6}{ab})$\cite{saiel}. Identifying this process in the fully hadronic decaychannel is particularly challenging due to overwhelming backgrounds from$t\bar{t}, t\bar{t}W, t\bar{t}Z$, and triple-top production processes. Thisstudy introduces a modified physics informed Neural Network, a hybrid graphneural network (GNN) enhancing event classification. The proposed modelintegrates Graph layers for particle-level features, a custom Multi LayerPerceptron(MLP) based global stream with a quantum circuit and cross-attentionfusion to combine local and global representations. Physics-informed Lossfunction enforce jet multiplicity constraints, derived from event decaydynamics. Benchmarked against conventional methods, the GNN achieves a signalsignificance $(S/\sqrt{S+B})$ of $0.174$ and ROC-AUC of 0.974, surpassing BDT'ssignificance of $0.148$ and ROC of $0.913$, while Xgboost achieves asignificance of $0.149$ and ROC of $0.920$. The classification models aretrained on Monte Carlo (MC) simulations, with events normalized usingcross-section-based reweighting to reflect their expected contributions in adataset corresponding to $350\;$fb$^{-1}$ of integrated luminosity. Thisenhanced approach offers a framework for precision event selection at the LHC,leveraging high dimensional statistical learning and physics informed inferenceto tackle fundamental HEP challenges, aligning with ML developments.</description>
      <author>example@mail.com (Syed Haider Ali, Ashfaq Ahmad, Muhammad Saiel, Nadeem Shaukat)</author>
      <guid isPermaLink="false">2507.23723v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FMIP: Multimodal Flow Matching for Mixed Integer Linear Programming</title>
      <link>http://arxiv.org/abs/2507.23390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A Generative Model based Method for Mixed Integer Linear Programming&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FMIP的新型多模态流匹配框架，用于解决混合整数线性规划(MILP)问题。该框架能够对MILP混合解空间中的整数和连续变量进行联合分布建模，并通过引导机制提高解决方案质量。&lt;h4&gt;背景&lt;/h4&gt;混合整数线性规划(MILP)是数学优化的基石，能够同时处理整数和连续变量的复杂决策问题。然而，大多数MILP问题是NP完全的，在实际求解中具有挑战性。现有的基于图神经网络(GNN)的启发式方法仅预测整数变量的解，难以捕捉连续变量和整数变量之间的复杂相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决现有GNN基方法在MILP问题求解中的局限性，能够更好地捕捉整数和连续变量之间的相互作用，提高解决方案的质量和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了FMIP，一种新颖的多模态流匹配框架，该框架对MILP混合解空间中的整数和连续变量的联合分布进行建模。FMIP集成了一个引导机制，用于在目标函数优化和约束满足条件下指导解的采样。&lt;h4&gt;主要发现&lt;/h4&gt;在七个标准MILP基准上评估FMIP，实验表明FMIP平均比现有的基于GNN的预测基线提高了50.04%的解决方案质量。&lt;h4&gt;结论&lt;/h4&gt;FMIP作为开发基于学习的MILP解决方案策略的有力新方法具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;混合整数线性规划(MILP)是数学优化的基石，能够对涉及整数和连续变量的复杂决策问题进行建模。尽管其通用性很强，但大多数MILP问题是NP完全的，因此在实践中难以求解。现有的基于图神经网络(GNN)的启发式方法旨在通过仅预测给定实例中整数变量的解来减少问题规模，但难以捕捉连续变量和整数变量之间的复杂相互作用，且缺乏足够的表示能力。为了解决这些局限性，我们提出了FMIP，一种新颖的多模态流匹配框架，该框架对MILP混合解空间中整数和连续变量的联合分布进行建模。为了实现更准确和可扩展的启发式方法，FMIP集成了一个引导机制，用于在目标函数优化和约束满足条件下指导解的采样。我们在七个标准MILP基准上评估了FMIP。实验表明，FMIP平均比现有的基于GNN的预测基线提高了50.04%的解决方案质量。这些结果突显了FMIP作为开发基于学习的MILP解决方案策略的有力新方法的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mixed-Integer Linear Programming (MILP) is a cornerstone of mathematicaloptimization, enabling the modeling of complex decision-making problemsinvolving both integer and continuous variables. Despite its versatility, mostMILP problems are NP-complete, making them challenging to solve in practice.Existing graph neural network (GNN)-based heuristics aim to reduce problemscale by predicting only the solutions on integer variables for a giveninstance, struggling to capture the intricate interplay between continuous andinteger variables and lack sufficient representational power. To address theselimitations, we propose FMIP, a novel multimodal flow-matching framework thatmodels the joint distribution over integer and continuous variables in themixed solution space of MILP. To enable more accurate and scalable heuristics,FMIP integrates a guidance mechanism to guide solution sampling under bothobjective function optimization and constraint satisfaction. We evaluate FMIPon seven standard MILP benchmarks. Our experiments show that FMIP improvessolution quality by 50.04% on average over existing GNN-based predictivebaselines. These results highlight FMIP's potential as a powerful new approachfor developing learning based MILP solution strategy.</description>
      <author>example@mail.com (Hongpei Li, Hui Yuan, Han Zhang, Dongdong Ge, Mengdi Wang, Yinyu Ye)</author>
      <guid isPermaLink="false">2507.23390v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Causal-Inspired Multi-Agent Decision-Making via Graph Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2507.23080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将因果学习与强化学习相结合，通过因果解缠表示学习提取影响自动驾驶决策的因果特征，并融入图神经网络强化学习算法中，优化无信号灯交叉口的车辆行为，实验表明该方法在多个关键指标上优于现有学习方法。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶技术在过去十年取得了显著进展，但现有研究仍难以解决多车辆环境下的无缝交互挑战。&lt;h4&gt;目的&lt;/h4&gt;通过整合因果学习与强化学习方法，识别和提取影响自动驾驶车辆最优决策的因果特征，提高复杂交通场景下的决策能力。&lt;h4&gt;方法&lt;/h4&gt;利用因果解缠表示学习(CDRL)识别和提取因果特征，将这些特征融入基于图神经网络的强化学习算法，优化无信号灯交叉口的车辆行为。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在训练过程中获得最高平均奖励，并在碰撞率和平均累积奖励等关键指标上显著优于其他学习方法。&lt;h4&gt;结论&lt;/h4&gt;该研究为推进多智能体自动驾驶系统提供了有前景的方向，使自动驾驶车辆在复杂交通环境中导航更安全高效。&lt;h4&gt;翻译&lt;/h4&gt;自从自动驾驶技术问世以来，在过去十年中经历了显著的发展。然而，大多数现有研究仍然难以解决多车辆必须无缝交互的环境所带来的挑战。本研究旨在通过利用因果解缠表示学习(CDRL)将因果学习与基于强化学习的方法相结合，以识别和提取影响自动驾驶车辆最优决策的因果特征。然后将这些特征融入基于图神经网络的强化学习算法中，以增强复杂交通场景下的决策能力。通过使用因果特征作为输入，所提出的方法能够在无信号灯交叉口优化车辆行为。实验结果表明，我们提出的方法在训练过程中实现了最高的平均奖励，并且在测试过程中的碰撞率和平均累积奖励等几个关键指标上显著优于其他基于学习的方法。该研究为推进多智能体自动驾驶系统提供了有前景的方向，使自动驾驶车辆在复杂交通环境中导航更安全、更高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the advent of autonomous driving technology, it has experiencedremarkable progress over the last decade. However, most existing research stillstruggles to address the challenges posed by environments where multiplevehicles have to interact seamlessly. This study aims to integrate causallearning with reinforcement learning-based methods by leveraging causaldisentanglement representation learning (CDRL) to identify and extract causalfeatures that influence optimal decision-making in autonomous vehicles. Thesefeatures are then incorporated into graph neural network-based reinforcementlearning algorithms to enhance decision-making in complex traffic scenarios. Byusing causal features as inputs, the proposed approach enables the optimizationof vehicle behavior at an unsignalized intersection. Experimental resultsdemonstrate that our proposed method achieves the highest average reward duringtraining and our approach significantly outperforms other learning-basedmethods in several key metrics such as collision rate and average cumulativereward during testing. This study provides a promising direction for advancingmulti-agent autonomous driving systems and make autonomous vehicles' navigationsafer and more efficient in complex traffic environments.</description>
      <author>example@mail.com (Jing Wang, Yan Jin, Fei Ding, Chongfeng Wei)</author>
      <guid isPermaLink="false">2507.23080v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding</title>
      <link>http://arxiv.org/abs/2507.23478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了3D-R1，一个增强三维视觉-语言模型推理能力的基础模型，通过高质量数据集构建、强化学习训练策略和动态视角选择策略，显著提高了三维场景理解的推理和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;大型视觉-语言模型在二维视觉理解任务中取得了显著进展，但当前的三维VLMs由于高质量空间数据有限和视角假设的静态性质，在推理和泛化方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决当前3D VLMs在推理和泛化方面的挑战，提高三维场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;构建高质量合成数据集Scene-30K，使用思维链(CoT)和基于Gemini 2.5 Pro的数据引擎；利用RLHF策略(如GRPO)增强推理能力；引入三个奖励函数：感知奖励、语义相似性奖励和格式奖励；实施动态视角选择策略，自适应选择最有信息量的视角。&lt;h4&gt;主要发现&lt;/h4&gt;3D-R1在各种三维场景基准测试中平均提高了10%，有效增强了三维场景理解的推理和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;3D-R1通过综合运用数据集构建、强化学习训练和动态视角选择策略，成功解决了3D VLMs在推理和泛化方面的挑战，显著提高了三维场景理解能力。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉-语言模型在二维视觉理解任务中取得了显著进展，引发了将这些能力扩展到三维场景理解领域的兴趣。然而，由于高质量空间数据的限制和视角假设的静态性质，当前的三维VLMs在推理和泛化方面常常遇到困难。为应对这些挑战，我们提出了3D-R1，这是一个增强三维VLMs推理能力的基础模型。具体而言，我们首先构建了一个高质量的包含思维链(CoT)的合成数据集，名为Scene-30K，利用现有的3D-VL数据集和基于Gemini 2.5 Pro的数据引擎。它作为3D-R1的冷启动初始化数据。此外，我们在强化学习训练过程中利用了RLHF策略，如GRPO，以增强推理能力，并引入了三个奖励函数：感知奖励、语义相似性奖励和格式奖励，以保持检测精度和答案语义精确性。此外，我们还引入了一种动态视角选择策略，能够自适应地选择对三维场景理解最有信息量的视角。大量实验表明，3D-R1在各种三维场景基准测试中平均提高了10%，突显了其在增强三维场景理解推理和泛化方面的有效性。代码：https://github.com/AIGeeksGroup/3D-R1。网站：https://aigeeksgroup.github.io/3D-R1。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (VLMs) have made significant strides in 2Dvisual understanding tasks, sparking interest in extending these capabilitiesto 3D scene understanding. However, current 3D VLMs often struggle with robustreasoning and generalization due to limitations in high-quality spatial dataand the static nature of viewpoint assumptions. To address these challenges, wepropose 3D-R1, a foundation model that enhances the reasoning capabilities of3D VLMs. Specifically, we first construct a high-quality synthetic dataset withCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data enginebased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.Moreover, we leverage RLHF policy such as GRPO in the reinforcement learningtraining process to enhance reasoning capabilities and introduce three rewardfunctions: a perception reward, a semantic similarity reward and a formatreward to maintain detection accuracy and answer semantic precision.Furthermore, we introduce a dynamic view selection strategy that adaptivelychooses the most informative perspectives for 3D scene understanding. Extensiveexperiments demonstrate that 3D-R1 delivers an average improvement of 10%across various 3D scene benchmarks, highlighting its effectiveness in enhancingreasoning and generalization in 3D scene understanding. Code:https://github.com/AIGeeksGroup/3D-R1. Website:https://aigeeksgroup.github.io/3D-R1.</description>
      <author>example@mail.com (Ting Huang, Zeyu Zhang, Hao Tang)</author>
      <guid isPermaLink="false">2507.23478v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models</title>
      <link>http://arxiv.org/abs/2507.23325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FASTopoWM是一种结合潜在世界模型的新型快速-慢速车道线段拓扑推理框架，通过并行监督历史和新初始化查询，以及引入基于动作潜力的潜在查询和BEV世界模型，显著提高了自动驾驶系统的车道线段检测和中心线感知性能。&lt;h4&gt;背景&lt;/h4&gt;车道线段拓扑推理提供全面的鸟瞰道路场景理解，可作为端到端自动驾驶系统的关键感知模块，但现有方法难以有效利用时间信息提升性能。&lt;h4&gt;目的&lt;/h4&gt;克服现有车道拓扑推理方法的局限性，包括过度依赖历史查询、易受姿态估计失败影响以及时间传播不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出FASTopoWM框架，通过并行监督历史和新初始化查询减少姿态估计失败影响；引入基于动作潜力的潜在查询和BEV世界模型，将状态表示从过去观测传播到当前时间步，提高慢速流程中的时间感知性能。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenLane-V2基准测试上，FASTopoWM在车道线段检测方面mAP达到37.4%(优于最先进方法的33.6%)，在中心线感知方面OLS达到46.3%(优于最先进方法的41.5%)。&lt;h4&gt;结论&lt;/h4&gt;FASTopoWM显著提升了自动驾驶系统的车道线段检测和中心线感知性能，是有效的车道线段拓扑推理框架。&lt;h4&gt;翻译&lt;/h4&gt;车道线段拓扑推理提供了全面的鸟瞰(BEV)道路场景理解，可以作为面向规划的端到端自动驾驶系统中的关键感知模块。现有的车道拓扑推理方法通常无法有效利用时间信息来提升检测和推理性能。最近，基于流的时序传播方法通过在查询和BEV级别整合时序线索展示了有前景的结果。然而，它仍然受限于过度依赖历史查询、容易受姿态估计失败影响以及时间传播不足等问题。为了克服这些限制，我们提出了FASTopoWM，一种结合潜在世界模型的新型快速-慢速车道线段拓扑推理框架。为减少姿态估计失败的影响，这个统一框架实现了历史查询和新初始化查询的并行监督，促进快速和慢速系统之间的相互强化。此外，我们引入了基于动作潜力的潜在查询和BEV世界模型，将状态表示从过去观测传播到当前时间步。这种设计显著提高了慢速流程中的时间感知性能。在OpenLane-V2基准测试上的大量实验表明，FASTopoWM在车道线段检测(37.4%对比33.6%的mAP)和中心线感知(46.3%对比41.5%的OLS)方面均优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决车道线段拓扑推理中时间信息利用不足的问题。现有方法在有效利用时间信息增强检测和推理性能方面存在局限，特别是在姿态估计失败时性能显著下降。这个问题在自动驾驶领域非常重要，因为车道拓扑推理作为关键感知模块，为端到端自动驾驶系统提供鸟瞰图道路场景理解，直接关系到自动驾驶系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于流的时间传播方法的三大局限性：过度依赖历史查询、易受姿态估计失败影响、时间传播不足。受视觉语言模型(VLMs)的启发，作者设计了双路径系统(快速-慢速管道)；借鉴自动驾驶中世界模型的概念，设计了两个潜在世界模型来捕获时间动态；同时引入统一框架实现并行监督，使两个系统相互强化而非完全独立。这些设计都建立在批判性分析现有工作和吸收相关领域创新的基础上。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建快速-慢速双系统架构，结合潜在世界模型增强时间传播能力。快速系统负责单帧感知确保基本功能，慢速系统利用时间信息提升性能；通过统一框架实现两个系统的相互强化；潜在世界模型基于'现在是过去的延续'原则，利用相对姿态将历史状态传播到当前时间步。整体流程分为编码器和解码器：编码器处理多视角图像提取BEV特征，并通过世界模型生成时间感知的流查询和流BEV特征；解码器分为快速和慢速两个分支，共享权重但分别处理不同信息，训练时联合监督，推理时根据姿态估计可靠性选择输出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)统一的快速-慢速框架，实现并行监督和相互强化；(2)两个潜在世界模型(查询世界模型和BEV世界模型)有效捕获时间动态；(3)改进的时间传播机制，避免传统warping导致的信息丢失。相比之前工作，该方法解决了现有流式方法过度依赖历史查询、姿态估计失败时性能下降、时间传播不足的问题；与其他快速-慢速系统不同，它无需额外模型，通过共享权重实现相互强化；相比现有世界模型方法，它专注于时间推理而非未来预测，更适合当前车道拓扑任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FASTopoWM通过创新的快速-慢双系统架构和潜在世界模型，显著提升了车道拓扑推理在时间感知和姿态估计失败场景下的鲁棒性和性能，实现了自动驾驶系统道路理解的重大突破。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)road scene understanding, which can serve as a key perception module inplanning-oriented end-to-end autonomous driving systems. Existing lane topologyreasoning methods often fall short in effectively leveraging temporalinformation to enhance detection and reasoning performance. Recently,stream-based temporal propagation method has demonstrated promising results byincorporating temporal cues at both the query and BEV levels. However, itremains limited by over-reliance on historical queries, vulnerability to poseestimation failures, and insufficient temporal propagation. To overcome theselimitations, we propose FASTopoWM, a novel fast-slow lane segment topologyreasoning framework augmented with latent world models. To reduce the impact ofpose estimation failures, this unified framework enables parallel supervisionof both historical and newly initialized queries, facilitating mutualreinforcement between the fast and slow systems. Furthermore, we introducelatent query and BEV world models conditioned on the action latent to propagatethe state representations from past observations to the current timestep. Thisdesign substantially improves the performance of temporal perception within theslow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstratethat FASTopoWM outperforms state-of-the-art methods in both lane segmentdetection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%on OLS).</description>
      <author>example@mail.com (Yiming Yang, Hongbin Lin, Yueru Luo, Suzhong Fu, Chao Zheng, Xinrui Yan, Shuqi Mei, Kun Tang, Shuguang Cui, Zhen Li)</author>
      <guid isPermaLink="false">2507.23325v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning</title>
      <link>http://arxiv.org/abs/2507.23318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FastDriveVLA的新型视觉标记修剪框架，专门针对自动驾驶场景中的Vision-Language-Action模型设计，通过保留关键前景信息来降低计算成本同时保持性能。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action模型在复杂场景理解和动作推理方面表现出色，被越来越多地应用于端到端自动驾驶系统。然而，这些模型的长视觉标记导致计算成本高昂。现有的视觉标记修剪方法在自动驾驶场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效减少VLA模型计算负担同时保持决策性能的视觉标记修剪方法，特别针对自动驾驶场景优化。&lt;h4&gt;方法&lt;/h4&gt;提出FastDriveVLA框架，包含一个名为ReconPruner的即插即用视觉标记修剪器，通过MAE风格像素重建优先保留前景信息。设计对抗性前景-背景重建策略训练ReconPruner，适用于VLA模型的视觉编码器。创建nuScenes-FG数据集，包含24.1万张带前景标注的图像-掩码对。&lt;h4&gt;主要发现&lt;/h4&gt;人类驾驶员驾驶时专注于前景区域，保留这些区域的视觉标记对有效决策至关重要。ReconPruner能够无缝应用于不同VLA模型而无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;FastDriveVLA在不同修剪比例下于nuScenes闭环规划基准测试上取得了最先进的结果，证明其在自动驾驶场景中有效降低了VLA模型的计算成本同时保持了性能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action模型在复杂场景理解和动作推理方面显示出巨大潜力，导致它们在端到端自动驾驶系统中的采用率不断增加。然而，VLA模型的长视觉标记大大增加了计算成本。当前视觉语言模型中的视觉标记修剪方法依赖于视觉标记相似性或视觉-文本注意力，但两者在自动驾驶场景中都表现不佳。鉴于人类驾驶员驾驶时专注于相关前景区域，我们认为保留包含这些前景信息的视觉标记对有效决策至关重要。受此启发，我们提出了FastDriveVLA，这是一个专门为自动驾驶设计的基于重建的视觉标记修剪框架。FastDriveVLA包括一个即插即用的视觉标记修剪器ReconPruner，它通过MAE风格的像素重建优先考虑前景信息。设计了一种新的对抗性前景-背景重建策略来训练ReconPruner，用于VLA模型的视觉编码器。一旦训练完成，ReconPruner可以无缝应用于具有相同视觉编码器的不同VLA模型而无需重新训练。为了训练ReconPruner，我们还引入了一个名为nuScenes-FG的大规模数据集，包含24.1万张图像-掩码对，并标注了前景区域。我们的方法在不同修剪比例下于nuScenes闭环规划基准测试上取得了最先进的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Vision-Language-Action (VLA) 模型在自动驾驶应用中的计算效率问题。VLA模型将视觉输入转换为大量视觉标记，大大增加了计算成本和推理延迟，使得在真实场景中部署受限。这个问题在现实中非常重要，因为自动驾驶系统需要实时处理大量视觉数据，计算资源有限，且推理速度对安全性至关重要。提高VLA模型的计算效率可以降低硬件要求，提高实时性，使自动驾驶技术更接近实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类驾驶员主要关注前景区域（如道路、车辆、行人等），而背景区域对驾驶决策影响很小。这启发他们思考保留包含前景信息的视觉标记对自动驾驶决策至关重要。作者批判了现有的视觉标记修剪方法（基于相似性和基于注意力）在自动驾驶场景中的局限性。他们设计了基于重建的视觉标记修剪框架FastDriveVLA，其中包括一个即插即用的视觉标记修剪器ReconPruner。作者借鉴了MAE（Masked Autoencoders）的像素重建思想和GAN（生成对抗网络）的对抗训练思想，并使用Grounded-SAM进行前景分割，构建了nuScenes-FG数据集来训练模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：人类驾驶员主要关注前景区域，因此保留包含前景信息的视觉标记对自动驾驶决策至关重要；通过像素重建来训练模型识别和保留重要的前景视觉标记；使用对抗性训练策略确保模型能够区分前景和背景。整体实现流程包括：1) 设计轻量级的ReconPruner，由PrunerLayer和Scorer组成；2) 训练过程中引入可学习的查询标记，通过MAE风格的像素重建和对抗性前景-背景重建策略训练模型；3) 推理时根据显著性分数选择保留重要视觉标记；4) 构建nuScenes-FG数据集提供训练支持。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了基于重建的视觉标记修剪框架FastDriveVLA；2) 设计了即插即用的视觉标记修剪器ReconPruner；3) 引入了对抗性前景-背景重建策略；4) 构建了nuScenes-FG大规模数据集。相比之前的工作，这种方法不依赖于文本-视觉对齐（区别于基于注意力的方法），也不强调标记相似性（区别于基于相似性的方法），更适合自动驾驶场景中前景区域明确的情况，能够更好地保留与驾驶相关的关键信息，在各种修剪比例下都取得了SOTA结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了FastDriveVLA，一种基于重建的视觉标记修剪框架，通过即插即用的ReconPruner和对抗性训练策略，显著提高了自动驾驶VLA模型的计算效率，同时保持了甚至超越了原始模型的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have demonstrated significant potentialin complex scene understanding and action reasoning, leading to theirincreasing adoption in end-to-end autonomous driving systems. However, the longvisual tokens of VLA models greatly increase computational costs. Currentvisual token pruning methods in Vision-Language Models (VLM) rely on eithervisual token similarity or visual-text attention, but both have shown poorperformance in autonomous driving scenarios. Given that human driversconcentrate on relevant foreground areas while driving, we assert thatretaining visual tokens containing this foreground information is essential foreffective decision-making. Inspired by this, we propose FastDriveVLA, a novelreconstruction-based vision token pruning framework designed specifically forautonomous driving. FastDriveVLA includes a plug-and-play visual token prunercalled ReconPruner, which prioritizes foreground information through MAE-stylepixel reconstruction. A novel adversarial foreground-background reconstructionstrategy is designed to train ReconPruner for the visual encoder of VLA models.Once trained, ReconPruner can be seamlessly applied to different VLA modelswith the same visual encoder without retraining. To train ReconPruner, we alsointroduce a large-scale dataset called nuScenes-FG, consisting of 241Kimage-mask pairs with annotated foreground regions. Our approach achievesstate-of-the-art results on the nuScenes closed-loop planning benchmark acrossdifferent pruning ratios.</description>
      <author>example@mail.com (Jiajun Cao, Qizhe Zhang, Peidong Jia, Xuhui Zhao, Bo Lan, Xiaoan Zhang, Xiaobao Wei, Sixiang Chen, Zhuo Li, Yang Wang, Liyun Li, Xianming Liu, Ming Lu, Shanghang Zhang)</author>
      <guid isPermaLink="false">2507.23318v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>AGA: An adaptive group alignment framework for structured medical cross-modal representation learning</title>
      <link>http://arxiv.org/abs/2507.23402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为自适应分组对齐(AGA)的新框架，用于从配对的医学图像和报告中学习结构化语义，解决了当前医学视觉语言预训练中忽略报告结构和难以获取硬负样本的问题。&lt;h4&gt;背景&lt;/h4&gt;学习医学视觉表征是表征学习的有前途方向，但当前医学领域的视觉语言预训练方法常将临床报告简化为单一实体或碎片化标记，忽略了其固有结构。此外，对比学习框架通常依赖大量硬负样本，这在小规模医学数据集上不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获医学图像和报告间结构化语义的框架，无需依赖大量硬负样本，同时保留临床报告的固有结构信息。&lt;h4&gt;方法&lt;/h4&gt;提出AGA框架，包含：1)基于稀疏相似矩阵的双向分组机制，计算文本标记和图像块间相似性并形成视觉组和语言组；2)语言分组阈值门和视觉分组阈值门两个阈值门控模块，动态学习分组阈值；3)基于相似度分数计算组表示；4)实例感知组对齐损失，无需外部负样本；5)双向跨模态分组对齐模块增强细粒度对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在公共和私有数据集上的实验表明，该方法在微调和零样本设置下的图像-文本检索和分类任务中取得了强大性能。&lt;h4&gt;结论&lt;/h4&gt;AGA框架有效解决了医学视觉语言预训练中的结构忽略和负样本获取困难问题，通过自适应分组和双向对齐机制实现了更好的医学图像和报告之间的对齐。&lt;h4&gt;翻译&lt;/h4&gt;从配对的图像和报告中学习医学视觉表征是表征学习中的一个有前途的方向。然而，当前医学领域的视觉语言预训练方法通常将临床报告简化为单一实体或碎片化标记，忽略了它们的固有结构。此外，对比学习框架通常依赖于大量硬负样本，这在小规模医学数据集上是不切实际的。为了应对这些挑战，我们提出了自适应分组对齐(AGA)，一个新框架，能够从配对的医学图像和报告中捕获结构化语义。AGA引入了基于稀疏相似矩阵的双向分组机制。对于每个图像-报告对，我们计算文本标记和图像块之间的细粒度相似性。每个标记选择其最匹配的图像块形成视觉组，每个图像块选择其最相关的标记形成语言组。为了实现自适应分组，我们设计了两个阈值门控模块，称为语言分组阈值门和视觉分组阈值门，它们动态学习分组阈值。组表示基于相似度分数计算为加权平均值。为了将每个标记与其组表示对齐，我们引入了实例感知组对齐损失，它在每个图像-文本对内操作，无需外部负样本。最后，应用双向跨模态分组对齐模块，以增强视觉和语言组表示之间的细粒度对齐。在公共和私有数据集上的广泛实验表明，我们的方法在微调和零样本设置下的图像-文本检索和分类任务中取得了强大的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning medical visual representations from paired images and reports is apromising direction in representation learning. However, currentvision-language pretraining methods in the medical domain often simplifyclinical reports into single entities or fragmented tokens, ignoring theirinherent structure. In addition, contrastive learning frameworks typicallydepend on large quantities of hard negative samples, which is impractical forsmall-scale medical datasets. To tackle these challenges, we propose AdaptiveGrouped Alignment (AGA), a new framework that captures structured semanticsfrom paired medical images and reports. AGA introduces a bidirectional groupingmechanism based on a sparse similarity matrix. For each image-report pair, wecompute fine-grained similarities between text tokens and image patches. Eachtoken selects its top-matching patches to form a visual group, and each patchselects its most related tokens to form a language group. To enable adaptivegrouping, we design two threshold gating modules, called Language GroupedThreshold Gate and Vision Grouped Threshold Gate, which learn groupingthresholds dynamically. Group representations are computed as weighted averagesbased on similarity scores. To align each token with its group representation,we introduce an Instance Aware Group Alignment loss that operates within eachimage-text pair, removing the need for external negatives. Finally, aBidirectional Cross-modal Grouped Alignment module is applied to enhancefine-grained alignment between visual and linguistic group representations.Extensive experiments on public and private datasets show that our methodachieves strong performance on image-text retrieval and classification tasksunder both fine-tuning and zero-shot settings.</description>
      <author>example@mail.com (Wei Li, Xun Gong, Jiao Li, Xiaobin Sun)</author>
      <guid isPermaLink="false">2507.23402v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision</title>
      <link>http://arxiv.org/abs/2507.23331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合开放词汇检测和跨模态学习的两阶段框架，用于解决交通标志识别中的长尾分布和小目标检测问题，在TT100K数据集上取得了78.4% mAP的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;交通标志识别作为自动驾驶感知系统的核心组成部分，直接影响车辆的环境感知和驾驶安全。&lt;h4&gt;目的&lt;/h4&gt;解决当前交通标志识别技术面临的两个主要挑战：数据集的长尾分布导致传统网络对低频类别识别性能下降，以及真实场景中小目标、多尺度交通标志的特征提取困难。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段框架：1) 交通标志检测采用NanoVerse YOLO模型，集成RepVL-PAN网络和SPD-Conv模块增强小目标和多尺度特征提取；2) 交通标志分类设计TSR-MCL模型，通过对比视觉Transformer的视觉特征和BERT的语义特征学习鲁棒的频率无关表示。&lt;h4&gt;主要发现&lt;/h4&gt;在TT100K数据集上，方法在长尾检测任务中达到78.4% mAP，分类准确率和召回率分别达到91.8%和88.9%，显著优于主流算法。&lt;h4&gt;结论&lt;/h4&gt;该方法有效缓解了数据不平衡引起的类别混淆问题，在复杂开放世界场景中展现出更高的准确性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;交通标志识别作为自动驾驶感知系统的核心组成部分，直接影响车辆的环境感知和驾驶安全。当前技术面临两个重大挑战：首先，交通标志数据集呈现明显的长尾分布，导致传统卷积网络在处理低频和分布外类别时识别性能显著下降；其次，真实场景中的交通标志主要是小目标且尺度变化大，难以提取多尺度特征。为解决这些问题，我们提出了一种结合开放词汇检测和跨模态学习的新型两阶段框架。对于交通标志检测，我们的NanoVerse YOLO模型集成了可重参数化的视觉语言路径聚合网络(RepVL-PAN)和SPD-Conv模块，专门针对小目标和多尺度目标增强特征提取。对于交通标志分类，我们设计了交通标志识别多模态对比学习模型(TSR-MCL)。通过对比视觉Transformer的视觉特征和基于规则的BERT的语义特征，TSR-MCL学习鲁棒的、频率无关的表示，有效缓解了数据不平衡引起的类别混淆。在TT100K数据集上，我们的方法在长尾检测任务的全类别识别中取得了最先进的78.4% mAP。该模型还获得了91.8%的准确率和88.9%的召回率，显著优于主流算法，在复杂开放世界场景中展现出更高的准确性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic sign recognition, as a core component of autonomous drivingperception systems, directly influences vehicle environmental awareness anddriving safety. Current technologies face two significant challenges: first,the traffic sign dataset exhibits a pronounced long-tail distribution,resulting in a substantial decline in recognition performance of traditionalconvolutional networks when processing low-frequency and out-of-distributionclasses; second, traffic signs in real-world scenarios are predominantly smalltargets with significant scale variations, making it difficult to extractmulti-scale features.To overcome these issues, we propose a novel two-stageframework combining open-vocabulary detection and cross-modal learning. Fortraffic sign detection, our NanoVerse YOLO model integrates a reparameterizablevision-language path aggregation network (RepVL-PAN) and an SPD-Conv module tospecifically enhance feature extraction for small, multi-scale targets. Fortraffic sign classification, we designed a Traffic Sign Recognition MultimodalContrastive Learning model (TSR-MCL). By contrasting visual features from aVision Transformer with semantic features from a rule-based BERT, TSR-MCLlearns robust, frequency-independent representations, effectively mitigatingclass confusion caused by data imbalance. On the TT100K dataset, our methodachieves a state-of-the-art 78.4% mAP in the long-tail detection task forall-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,significantly outperforming mainstream algorithms and demonstrating superioraccuracy and generalization in complex, open-world scenarios.</description>
      <author>example@mail.com (Qiang Lu, Waikit Xiu, Xiying Li, Shenyu Hu, Shengbo Sun)</author>
      <guid isPermaLink="false">2507.23331v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space</title>
      <link>http://arxiv.org/abs/2507.23188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TMM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态动作检索框架，整合文本、音频、视频和动作四种模态，通过细粒度联合嵌入空间和序列级对比学习方法，显著提升了动作检索性能。&lt;h4&gt;背景&lt;/h4&gt;动作检索对动作获取至关重要，相比动作生成具有更高精确度、真实感、可控性和可编辑性。现有方法利用对比学习构建统一嵌入空间进行文本或视觉模态的动作检索，但缺乏直观友好的交互模式，且常忽略模态的序列表示以提升检索性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出将文本、音频、视频和动作四种模态对齐在细粒度联合嵌入空间中的框架，首次在动作检索中引入音频以增强用户沉浸感和便利性。&lt;h4&gt;方法&lt;/h4&gt;通过序列级对比学习方法实现细粒度空间，捕捉跨模态关键细节以实现更好对齐。通过合成多样化音频记录扩展现有文本-动作数据集，创建两个多模态动作检索数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多个子任务上优于最先进方法，包括在HumanML3D数据集上文本到动作检索的R@10提高10.16%，视频到动作检索的R@1提高25.43%。四模态框架显著优于三模态对应框架，突显多模态动作检索的潜力。&lt;h4&gt;结论&lt;/h4&gt;多模态动作检索框架通过整合四种模态并在细粒度联合嵌入空间中对齐，显著提升了动作检索性能，为动作获取领域带来新可能性。&lt;h4&gt;翻译&lt;/h4&gt;动作检索对于动作获取至关重要，相比动作生成具有更高的精确度、真实感、可控性和可编辑性。现有方法利用对比学习构建统一嵌入空间进行文本或视觉模态的动作检索，但缺乏更直观和用户友好的交互模式，并且常常忽略了大多数模态的序列表示以提升检索性能。为了解决这些限制，我们提出了一种将文本、音频、视频和动作四种模态对齐在细粒度联合嵌入空间中的框架，首次在动作检索中引入音频以增强用户沉浸感和便利性。这种细粒度空间是通过序列级对比学习方法实现的，捕捉跨模态的关键细节以实现更好的对齐。为了评估我们的框架，我们通过合成但多样化的音频记录扩展现有的文本-动作数据集，创建了两个多模态动作检索数据集。实验结果表明，在多个子任务上，我们的方法优于最先进的方法，包括在HumanML3D数据集上文本到动作检索的R@10提高了10.16%，视频到动作检索的R@1提高了25.43%。此外，我们的结果显示四模态框架显著优于三模态对应框架，突显了多模态动作检索在推进动作获取方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion retrieval is crucial for motion acquisition, offering superiorprecision, realism, controllability, and editability compared to motiongeneration. Existing approaches leverage contrastive learning to construct aunified embedding space for motion retrieval from text or visual modality.However, these methods lack a more intuitive and user-friendly interaction modeand often overlook the sequential representation of most modalities forimproved retrieval performance. To address these limitations, we propose aframework that aligns four modalities -- text, audio, video, and motion --within a fine-grained joint embedding space, incorporating audio for the firsttime in motion retrieval to enhance user immersion and convenience. Thisfine-grained space is achieved through a sequence-level contrastive learningapproach, which captures critical details across modalities for betteralignment. To evaluate our framework, we augment existing text-motion datasetswith synthetic but diverse audio recordings, creating two multi-modal motionretrieval datasets. Experimental results demonstrate superior performance overstate-of-the-art methods across multiple sub-tasks, including an 10.16%improvement in R@10 for text-to-motion retrieval and a 25.43% improvement inR@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, ourresults show that our 4-modal framework significantly outperforms its 3-modalcounterpart, underscoring the potential of multi-modal motion retrieval foradvancing motion acquisition.</description>
      <author>example@mail.com (Shiyao Yu, Zi-An Wang, Kangning Yin, Zheng Tian, Mingyuan Zhang, Weixin Si, Shihao Zou)</author>
      <guid isPermaLink="false">2507.23188v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network</title>
      <link>http://arxiv.org/abs/2507.19095v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code for this study is available at  https://github.com/YF-W/GCL-GCN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GCL-GCN的新型深度图聚类模型，通过引入Graphormer模块和对比学习模块，有效解决了现有模型在处理稀疏和异构图数据时的局限性，显著提升了聚类质量和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;带属性图聚类在现代数据分析中具有重要意义，但由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够有效捕获局部依赖性和复杂结构的图聚类模型，以处理稀疏和异构图数据。&lt;h4&gt;方法&lt;/h4&gt;提出GCL-GCN模型，包含创新的Graphormer模块(结合中心性编码和空间关系)和对比学习模块(通过对比学习增强特征判别能力)。&lt;h4&gt;主要发现&lt;/h4&gt;在六个数据集上的实验表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法；在Cora数据集上，ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。&lt;h4&gt;结论&lt;/h4&gt;GCL-GCN通过有效捕获节点间的全局和局部信息，显著提升了节点表示质量，是一种先进的图聚类方法。&lt;h4&gt;翻译&lt;/h4&gt;带属性图聚类在现代数据分析中具有重要意义。然而，由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍然具有挑战性。为此，我们提出了一种新型深度图聚类模型GCL-GCN，专门设计用于解决现有模型在处理稀疏和异构图数据时捕获局部依赖性和复杂结构方面的局限性。GCL-GCN引入了一个创新的Graphormer模块，该模块结合了中心性编码和空间关系，有效捕获节点之间的全局和局部信息，从而增强节点表示的质量。此外，我们还提出了一种新颖的对比学习模块，显著增强了特征表示的判别能力。在预训练阶段，该模块通过对原始特征矩阵进行对比学习来增加特征区分度，确保随后的图卷积和聚类任务具有更可识别的初始表示。在六个数据集上的大量实验结果表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法。特别是在Cora数据集上，与主要对比方法MBN相比，ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attributed graph clustering holds significant importance in modern dataanalysis. However, due to the complexity of graph data and the heterogeneity ofnode attributes, leveraging graph information for clustering remainschallenging. To address this, we propose a novel deep graph clustering model,GCL-GCN, specifically designed to address the limitations of existing models incapturing local dependencies and complex structures when dealing with sparseand heterogeneous graph data. GCL-GCN introduces an innovative Graphormermodule that combines centrality encoding and spatial relationships, effectivelycapturing both global and local information between nodes, thereby enhancingthe quality of node representations. Additionally, we propose a novelcontrastive learning module that significantly enhances the discriminativepower of feature representations. In the pre-training phase, this moduleincreases feature distinction through contrastive learning on the originalfeature matrix, ensuring more identifiable initial representations forsubsequent graph convolution and clustering tasks. Extensive experimentalresults on six datasets demonstrate that GCL-GCN outperforms 14 advancedmethods in terms of clustering quality and robustness. Specifically, on theCora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,respectively, compared to the primary comparison method MBN.</description>
      <author>example@mail.com (Binxiong Li, Xu Xiang, Xue Li, Quanzhou Lou, Binyu Zhao, Yujie Liu, Huijie Tang, Benhan Yang)</author>
      <guid isPermaLink="false">2507.19095v2</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language Translation</title>
      <link>http://arxiv.org/abs/2507.23575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BeyondGloss是一种新型的无词汇表手语翻译框架，利用视频大语言模型的时空推理能力，通过生成手部运动的细粒度文本描述、对比对齐模块、特征提炼和对比损失等方法，有效弥合了视觉和语言信息之间的模态差距，在基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;手语翻译是一个具有挑战性的任务，需要弥合视觉和语言信息之间的模态差距，同时捕捉手部形状和运动的细微变化。现有视频大语言模型难以详细建模长视频。&lt;h4&gt;目的&lt;/h4&gt;介绍BeyondGloss框架，解决现有VideoLLMs的局限性，提高手语翻译的准确性和效果。&lt;h4&gt;方法&lt;/h4&gt;提出生成手部运动的细粒度、时间感知文本描述的方法；通过对比对齐模块将描述与视频特征对齐；从HaMeR中提炼细粒度特征；应用手语视频表示与目标语言嵌入之间的对比损失以减少模态差距。&lt;h4&gt;主要发现&lt;/h4&gt;BeyondGloss在Phoenix14T和CSL-Daily基准测试上取得了最先进的性能，证明了所提出框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;BeyondGloss框架在手语翻译任务上表现出色，作者将在论文被接受后发布代码。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译是一项具有挑战性的任务，需要弥合视觉和语言信息之间的模态差距，同时捕捉手部形状和运动的细微变化。为应对这些挑战，我们介绍了BeyondGloss，一种新型的无词汇表手语翻译框架，利用视频大语言模型的时空推理能力。由于现有VideoLLMs难以详细建模长视频，我们提出了一种生成手部运动的细粒度、时间感知文本描述的新方法。对比对齐模块在预训练期间将这些描述与视频特征对齐，鼓励模型专注于以手为中心的时间动态，更有效地区分手语。此外，我们从HaMeR中提炼细粒度特征，以丰富手部特定表示。我们还应用了手语视频表示与目标语言嵌入之间的对比损失，以在预训练期间减少模态差距。BeyondGloss在Phoenix14T和CSL-Daily基准测试上取得了最先进的性能，证明了所提出框架的有效性。我们将在论文被接受后发布代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign Language Translation (SLT) is a challenging task that requires bridgingthe modality gap between visual and linguistic information while capturingsubtle variations in hand shapes and movements. To address these challenges, weintroduce \textbf{BeyondGloss}, a novel gloss-free SLT framework that leveragesthe spatio-temporal reasoning capabilities of Video Large Language Models(VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail,we propose a novel approach to generate fine-grained, temporally-aware textualdescriptions of hand motion. A contrastive alignment module aligns thesedescriptions with video features during pre-training, encouraging the model tofocus on hand-centric temporal dynamics and distinguish signs more effectively.To further enrich hand-specific representations, we distill fine-grainedfeatures from HaMeR. Additionally, we apply a contrastive loss between signvideo representations and target language embeddings to reduce the modality gapin pre-training. \textbf{BeyondGloss} achieves state-of-the-art performance onthe Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of theproposed framework. We will release the code upon acceptance of the paper.</description>
      <author>example@mail.com (Sobhan Asasi, Mohamed Ilyas Lakhal, Ozge Mercanoglu Sincan, Richard Bowden)</author>
      <guid isPermaLink="false">2507.23575v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>The Effect of Prior Parameters on Standardized Kalman Filter-Based EEG Source Localization</title>
      <link>http://arxiv.org/abs/2507.23450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发和优化了标准卡尔曼滤波器(SKF)框架内的高斯先验模型，用于同时检测皮层和皮层下活动。通过合成数据测试，发现提高标准化指数并结合平滑技术能显著改善深度定位准确性。&lt;h4&gt;背景&lt;/h4&gt;EEG源定位是神经科学的关键工具，用于解决不适定的逆问题。贝叶斯框架可通过先验知识约束提供解决方案。标准卡尔曼滤波器(SKF)是一种动态贝叶斯方法，结合时间建模与高斯先验结构，通过后标准化步骤解决深度偏差问题。&lt;h4&gt;目的&lt;/h4&gt;开发和优化SKF框架内的高斯先验模型，用于同时检测皮层和皮层下活动，确定在不同噪声水平下重建深部和浅表源的有效先验参数配置，并研究RTS平滑在增强源可分离性方面的作用。&lt;h4&gt;方法&lt;/h4&gt;使用类似体感诱发电位(SEP)的P20/N20组分的合成数据，测试不同先验参数配置下深部和浅表源的重建效果，并应用RTS平滑技术增强源可分离性。&lt;h4&gt;主要发现&lt;/h4&gt;将标准化指数提高到1.25并结合平滑技术，在低噪声水平下显著提高了深度定位的准确性。&lt;h4&gt;结论&lt;/h4&gt;优化后的高斯先验模型和标准化参数配置能有效改善EEG源定位中的深度偏差问题，特别是对于低噪声环境下的深部活动检测。&lt;h4&gt;翻译&lt;/h4&gt;EEG源定位是神经科学中的关键工具，应用范围从癫痫诊断到认知研究。它涉及解决一个不适定的逆问题，除非有先验知识的约束，否则没有唯一解。贝叶斯框架能够整合这些先验知识，通常通过先验模型进行编码。已提出了各种源定位算法，它们在如何整合先验知识方面存在显著差异。一些方法依赖于解剖或功能约束，而其他方法使用统计分布或基于采样的技术。在这种背景下，标准卡尔曼滤波器(SKF)代表了一种动态贝叶斯方法，它将时间建模与高斯先验结构相结合。通过一个后标准化步骤解决了源定位中的深度偏差问题，该步骤使皮层深度之间的敏感性相等，并使深部活动检测成为可能。本研究专注于在SKF框架内开发和优化高斯先验模型，用于同时检测皮层和皮层下活动。使用类似体感诱发电位(SEP)的P20/N20组分的合成数据，来确定在不同噪声水平下重建深部和浅表源的有效先验参数配置。还研究了RTS平滑在增强源可分离性方面的作用。结果表明，将标准化指数提高到1.25并结合平滑技术，在低噪声水平下显著提高了深度定位的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; EEG Source localization is a critical tool in neuroscience, with applicationsranging from epilepsy diagnosis to cognitive research. It involves solving anill-posed inverse problem that lacks a unique solution unless constrained byprior knowledge. The Bayesian framework enables the incorporation of suchknowledge, typically encoded through prior models. Various algorithms have beenproposed for source localization, and they differ significantly in how priorknowledge is incorporated. Some approaches rely on anatomical or functionalconstraints, while others use statistical distributions or sampling-basedtechniques. In this landscape, the Standardized Kalman Filter (SKF) representsa dynamic Bayesian approach that integrates temporal modeling with a Gaussianprior structure. It addresses the depth bias, a common limitation in sourcelocalization, through a post-hoc standardization step that equalizessensitivity across cortical depths and makes deep activity detection feasible.  This study focuses on the development and optimization of Gaussian priormodels within the SKF framework for simultaneous cortical and sub-corticalactivity detection. Synthetic data similar to the P20 / N20 component of thesomatosensory evoked potentials (SEP) was used to identify effective priorparameter configurations for reconstructing both deep and superficial sourcesunder different noise levels. We also investigated the role of RTS smoothing inenhancing source separability. Our results indicate that raising thestandardization exponent to 1.25, along with smoothing, significantly improvesdepth localization accuracy at low noise levels.</description>
      <author>example@mail.com (Dilshanie Prasikala, Joonas Lahtinen, Alexandra Koulouri, Sampsa Pursiainen)</author>
      <guid isPermaLink="false">2507.23450v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>A Deep Dive into Generic Object Tracking: A Survey</title>
      <link>http://arxiv.org/abs/2507.23251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  55 pages, 29 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对通用目标跟踪领域进行了全面综述，涵盖基于Siamese的跟踪器、判别式跟踪器和基于Transformer的方法三种主要范式，特别强调了快速发展的基于Transformer的方法。&lt;h4&gt;背景&lt;/h4&gt;通用目标跟踪在计算机视觉中是一项重要且具有挑战性的任务，主要面临复杂的空间-时间动态特性，特别是在存在遮挡、相似干扰物和外观变化的情况下。&lt;h4&gt;目的&lt;/h4&gt;提供一个对所有三种跟踪类别的全面回顾，特别关注基于Transformer的方法，通过定性和定量比较分析各类方法的核心设计原则、创新点和局限性。&lt;h4&gt;方法&lt;/h4&gt;采用新的分类方法，对代表性方法进行统一的视觉和表格比较，从多视角组织现有跟踪器，并总结主要评估基准。&lt;h4&gt;主要发现&lt;/h4&gt;基于Transformer的跟踪方法因其强大的空间-时间建模能力而取得了快速发展，成为目标跟踪领域的重要进展。&lt;h4&gt;结论&lt;/h4&gt;通过对三类跟踪范式的全面回顾和比较，深入理解了目标跟踪领域的当前状态，并突显了基于Transformer方法的发展趋势。&lt;h4&gt;翻译&lt;/h4&gt;通用目标跟踪在计算机视觉中仍然是一个重要且具有挑战性的任务，由于复杂的空间-时间动态特性，特别是在存在遮挡、相似干扰物和外观变化的情况下。在过去的二十年里，已经引入了多种跟踪范式，包括基于Siamese的跟踪器、判别式跟踪器，以及最近突出的基于Transformer的方法，来解决这些挑战。虽然该领域已有一些综述论文要么集中在单一类别上，要么广泛涵盖多个类别以捕捉进展，但我们的论文对所有三个类别进行了全面回顾，特别强调了快速发展的基于Transformer的方法。我们通过定性和定量比较分析了每种方法的核心设计原则、创新点和局限性。我们的研究引入了一种新的分类方法，并对代表性方法提供了统一的视觉和表格比较。此外，我们从多个视角组织了现有的跟踪器，总结了主要的评估基准，突显了基于Transformer的跟踪方法由于其强大的空间-时间建模能力而取得的快速发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generic object tracking remains an important yet challenging task in computervision due to complex spatio-temporal dynamics, especially in the presence ofocclusions, similar distractors, and appearance variations. Over the past twodecades, a wide range of tracking paradigms, including Siamese-based trackers,discriminative trackers, and, more recently, prominent transformer-basedapproaches, have been introduced to address these challenges. While a fewexisting survey papers in this field have either concentrated on a singlecategory or widely covered multiple ones to capture progress, our paperpresents a comprehensive review of all three categories, with particularemphasis on the rapidly evolving transformer-based methods. We analyze the coredesign principles, innovations, and limitations of each approach through bothqualitative and quantitative comparisons. Our study introduces a novelcategorization and offers a unified visual and tabular comparison ofrepresentative methods. Additionally, we organize existing trackers frommultiple perspectives and summarize the major evaluation benchmarks,highlighting the fast-paced advancements in transformer-based tracking drivenby their robust spatio-temporal modeling capabilities.</description>
      <author>example@mail.com (Fereshteh Aghaee Meibodi, Shadi Alijani, Homayoun Najjaran)</author>
      <guid isPermaLink="false">2507.23251v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</title>
      <link>http://arxiv.org/abs/2507.23567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3D-MOOD，第一个端到端的开放场景单目3D物体检测器，解决了现有方法在封闭场景设置中的局限性，通过创新方法实现了在封闭和开放场景下的高性能表现。&lt;h4&gt;背景&lt;/h4&gt;单目3D物体检测在机器人和AR/VR等多种应用中具有重要价值。然而，现有方法仅限于封闭场景设置，即训练和测试集包含相同的场景和/或物体类别。现实世界的应用常常引入新的环境和新颖的物体类别，这些方法难以应对。&lt;h4&gt;目的&lt;/h4&gt;解决开放场景设置下的单目3D物体检测问题，并提出第一个端到端的3D单目开放场景物体检测器（3D-MOOD），以提高在多样化场景和物体类别下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;设计了3D边界框头部，将开放场景的2D检测提升到3D空间，实现2D和3D任务的端到端联合训练；使用几何先验条件处理物体查询，克服3D估计在多样化场景中的泛化问题；设计了规范图像空间，以实现更高效的跨数据集训练。&lt;h4&gt;主要发现&lt;/h4&gt;3D-MOOD在封闭场景（Omni3D）和开放场景（从Omni3D到Argoverse 2、ScanNet）的评估中都取得了新的最先进结果，证明了其在不同场景下的有效性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;3D-MOOD成功解决了开放场景下单目3D物体检测的挑战，通过创新的方法设计，实现了在封闭和开放场景下的高性能表现，为实际应用提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;单目3D物体检测在机器人学和AR/VR等众多应用中具有重要价值。现有方法仅限于封闭场景设置，其中训练和测试集包含相同的场景和/或物体类别。然而，现实世界的应用常常引入新的环境和新颖的物体类别，对这些方法构成了挑战。在本文中，我们解决了开放场景设置下的单目3D物体检测问题，并引入了第一个端到端的3D单目开放场景物体检测器（3D-MOOD）。我们提出通过设计的3D边界框头部将开放场景的2D检测提升到3D空间，使2D和3D任务能够进行端到端的联合训练，从而获得更好的整体性能。我们使用几何先验条件处理物体查询，克服了3D估计在多样化场景中的泛化问题。为了进一步提高性能，我们设计了规范图像空间以实现更高效的跨数据集训练。我们在封闭场景（Omni3D）和开放场景（从Omni3D到Argoverse 2、ScanNet）上评估了3D-MOOD，并取得了新的最先进结果。代码和模型可在royyang0714.github.io/3D-MOOD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D目标检测在开放环境下的挑战，即现有方法只能在训练和测试数据集包含相同场景和物体类别的封闭集设置下工作。这个问题在现实中非常重要，因为真实世界应用（如机器人技术和AR/VR）经常需要面对新的环境和未知物体类别，封闭集方法无法适应这些变化，限制了实际应用效果。开放集检测能使模型识别和定位训练时未见过的物体，这在实际场景中非常实用且必要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出开放集单目3D目标检测的两个主要障碍：跨模态学习困难（3D数据缺乏丰富视觉-语言对）和单目深度估计在不同场景间的泛化挑战。针对这些问题，他们提出利用通用单目深度估计方法的良好泛化能力，设计3D边界框头部将2D检测结果提升到3D空间。作者借鉴了G-DINO作为2D开放集检测器基础，采用检测Transformer架构，并参考了Cube R-CNN和Uni-MODE在统一模型方面的思路，以及UniDepth在相机条件下的深度估计方法，最终形成了3D-MOOD的完整框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过设计的3D边界框头部将开放集2D检测提升到3D空间，实现2D和3D任务的端到端联合训练，利用几何先验增强模型在不同场景间的泛化能力。整体流程为：1)接收单目图像和语言提示；2)提取图像和文本特征；3)通过transformer进行视觉-语言融合；4)生成2D检测结果；5)使用3D边界框头部将2D结果提升为3D边界框；6)利用几何感知查询生成增强模型泛化能力；7)通过辅助深度估计提供场景理解；8)输出包含位置、尺寸和方向的3D检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个端到端的开放集单目3D目标检测器；2)创新的3D边界框头部设计，实现可微分提升；3)几何感知的3D查询生成，利用相机内参和深度估计增强泛化；4)规范图像空间解决图像、相机内参和深度间的歧义。相比之前工作，3D-MOOD最大的不同是实现了真正的开放集3D检测，能够识别未知物体类别；而之前的方法如Cube R-CNN和Uni-MODE只能在封闭集下工作。此外，3D-MOOD可通过可微分参数实现端到端训练，而OVM3D-Det等方法无法使用3D数据进行端到端训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3D-MOOD首次实现了端到端的单目开放集3D目标检测，通过创新的2D到3D提升机制和几何感知查询生成，显著提升了模型在未知场景和未知物体类别上的检测性能，为实际应用中的开放环境视觉理解提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detection is valuable for various applications such asrobotics and AR/VR. Existing methods are confined to closed-set settings, wherethe training and testing sets consist of the same scenes and/or objectcategories. However, real-world applications often introduce new environmentsand novel object categories, posing a challenge to these methods. In thispaper, we address monocular 3D object detection in an open-set setting andintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).We propose to lift the open-set 2D detection into 3D space through our designed3D bounding box head, enabling end-to-end joint training for both 2D and 3Dtasks to yield better overall performance. We condition the object queries withgeometry prior and overcome the generalization for 3D estimation across diversescenes. To further improve performance, we design the canonical image space formore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-setsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), andachieve new state-of-the-art results. Code and models are available atroyyang0714.github.io/3D-MOOD.</description>
      <author>example@mail.com (Yung-Hsu Yang, Luigi Piccinelli, Mattia Segu, Siyuan Li, Rui Huang, Yuqian Fu, Marc Pollefeys, Hermann Blum, Zuria Bauer)</author>
      <guid isPermaLink="false">2507.23567v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy</title>
      <link>http://arxiv.org/abs/2507.21358v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The manuscript has been accepted by ICONIP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Collaborative Perceiver (CoP)的多任务学习框架，通过利用空间占用作为辅助信息，改进了基于视觉的鸟瞰图3D目标检测方法。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图3D目标检测在自动驾驶领域取得了显著进展，具有成本效益和丰富的上下文信息。然而，现有方法通常通过折叠提取的目标特征来构建BEV表示，忽略了道路和人行道等内在环境上下文。&lt;h4&gt;目的&lt;/h4&gt;为了缓解现有方法无法全面感知物理世界特征的问题，作者引入了CoP框架，挖掘3D目标检测和占用预测任务之间的一致结构和概念相似性，弥合空间表示和特征精化的差距。&lt;h4&gt;方法&lt;/h4&gt;1) 提出了一种管道生成包含局部密度信息的密集占用真实值；2) 采用基于体素高度引导的采样策略提炼细粒度局部特征；3) 开发全局-局部协作特征融合模块集成两个任务的互补知识。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准上的实验表明，CoP优于现有基于视觉的框架，在测试集上实现了49.5%的平均精度和59.2%的检测分数。&lt;h4&gt;结论&lt;/h4&gt;CoP通过多任务学习和特征融合有效改进了BEV表示，使检测器能够更全面地感知物理世界的特征。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图3D目标检测在自动驾驶领域通过提供成本效益和丰富的上下文信息取得了显著进展。然而，现有方法通常通过折叠提取的目标特征来构建BEV表示，忽略了道路和人行道等内在环境上下文。这阻碍了检测器全面感知物理世界的特征。为了缓解这一问题，我们引入了Collaborative Perceiver (CoP)多任务学习框架，利用空间占用作为辅助信息，挖掘3D目标检测和占用预测任务之间共享的一致结构和概念相似性，弥合空间表示和特征精化的差距。为此，我们首先提出了一种管道来生成包含局部密度信息的密集占用真实值，用于重建详细的环境信息。接下来，我们采用基于体素高度引导的采样策略，根据不同的物体属性提炼细粒度的局部特征。此外，我们还开发了一种全局-局部协作特征融合模块，无缝集成两个任务之间的互补知识，从而组成更强大的BEV表示。在nuScenes基准上的大量实验表明，CoP优于现有的基于视觉的框架，在测试集上实现了49.5%的平均精度和59.2%的检测分数。代码和补充材料可在提供的链接获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉3D物体检测方法无法全面感知物理世界特征的问题，特别是在识别具有独特或不规则几何形状物体方面的局限性。这个问题在自动驾驶领域至关重要，因为准确的物体识别和环境理解是确保系统在复杂交通场景中安全运行的基础，同时环境上下文信息（如道路、人行道等）对于导航和决策也必不可少。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D检测方法的不足，然后提出利用多任务学习框架，将3D物体检测和占用预测相结合，挖掘两个任务之间的互补知识。方法借鉴了现有工作中的视图转换技术（如Lift-Splat-Shoot）、体素化方法和注意力机制，但创新性地引入了局部密度感知的密集占用生成管道、体素高度引导的采样策略和全局-局部协作特征融合模块，以解决传统方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务协作学习，结合3D物体检测和3D占用预测两个任务的互补知识，利用局部密度信息捕获物体的细粒度结构，并通过全局-局部特征融合构建更鲁棒的BEV表示。整体流程包括：1)输入多视图图像并提取特征；2)通过LSS将2D特征转换为3D体素特征；3)分别提取全局特征和通过VHS策略提取局部特征；4)通过CFF模块融合全局和局部特征；5)将融合后的特征分别送入3D占用预测头和3D物体检测头；6)结合两个任务的损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)局部密度感知的密集占用(LDO)生成管道，从多帧激光雷达数据生成包含局部密度信息的密集占用地面真实值；2)体素高度引导的采样(VHS)策略，根据占用预测的高度分布提取细粒度局部特征；3)全局-局部协作特征融合(CFF)模块，自适应融合全局和局部特征。相比之前的工作，CoP不是单一任务方法，而是通过多任务学习利用互补信息；考虑了非均匀点密度分布，更好地表示物体细粒度结构；避免了传统方法中沿高度维度压缩特征导致的信息丢失问题，在nuScenes基准测试上实现了最先进的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了协作感知器(CoP)，一个通过局部密度感知的空间占用信息和全局-局部特征融合来提升视觉3D物体检测性能的多任务学习框架，在nuScenes基准测试上实现了49.5%的mAP和59.2%的NDS，超过了现有视觉框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based bird's-eye-view (BEV) 3D object detection has advancedsignificantly in autonomous driving by offering cost-effectiveness and richcontextual information. However, existing methods often construct BEVrepresentations by collapsing extracted object features, neglecting intrinsicenvironmental contexts, such as roads and pavements. This hinders detectorsfrom comprehensively perceiving the characteristics of the physical world. Toalleviate this, we introduce a multi-task learning framework, CollaborativePerceiver (CoP), that leverages spatial occupancy as auxiliary information tomine consistent structural and conceptual similarities shared between 3D objectdetection and occupancy prediction tasks, bridging gaps in spatialrepresentations and feature refinement. To this end, we first propose apipeline to generate dense occupancy ground truths incorporating local densityinformation (LDO) for reconstructing detailed environmental information. Next,we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grainedlocal features according to distinct object properties. Furthermore, we developa global-local collaborative feature fusion (CFF) module that seamlesslyintegrates complementary knowledge between both tasks, thus composing morerobust BEV representations. Extensive experiments on the nuScenes benchmarkdemonstrate that CoP outperforms existing vision-based frameworks, achieving49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials areavailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.</description>
      <author>example@mail.com (Jicheng Yuan, Manh Nguyen Duc, Qian Liu, Manfred Hauswirth, Danh Le Phuoc)</author>
      <guid isPermaLink="false">2507.21358v3</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios</title>
      <link>http://arxiv.org/abs/2507.23660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DuLoc的鲁棒且准确的LiDAR定位方法，通过紧密耦合LiDAR-惯性里程计与基于离线地图的定位，并集成动态实时地图，解决了传统方法在长期环境变化中的局限性，在大型变化的户外环境中优于其他最先进的LiDAR定位系统。&lt;h4&gt;背景&lt;/h4&gt;LiDAR定位是自主系统中的关键组成部分，但现有方法在平衡重复性、准确性和环境适应性方面面临持续挑战。传统仅依赖离线地图的点云注册方法在面对长期环境变化时往往表现出有限的鲁棒性，导致在动态现实场景中出现定位漂移和可靠性下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有LiDAR定位方法在平衡重复性、准确性和环境适应性方面的挑战，特别是在处理长期环境变化和动态现实场景中的定位漂移和可靠性下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出DuLoc方法，紧密耦合LiDAR-惯性里程计与基于离线地图的定位，引入恒定速度运动模型减轻现实场景中的异常噪声，开发将先验全局地图与动态实时局部地图无缝集成的LiDAR定位框架，实现无界和变化环境中的鲁棒定位。&lt;h4&gt;主要发现&lt;/h4&gt;在超大型港口进行的实验中，涉及32辆智能引导车(IGV)的2856小时运行数据，结果表明该系统在大型变化的户外环境中优于其他最先进的LiDAR定位系统。&lt;h4&gt;结论&lt;/h4&gt;DuLoc方法通过紧密耦合LiDAR-惯性里程计与离线地图定位，并集成动态实时地图，有效解决了传统方法在长期环境变化和动态场景中的局限性，实现了更准确、更鲁棒的定位性能。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的定位是自主系统中的关键组成部分，然而现有方法在平衡重复性、准确性和环境适应性方面面临持续挑战。传统仅依赖离线地图的点云注册方法在面对长期环境变化时往往表现出有限的鲁棒性，导致在动态现实场景中出现定位漂移和可靠性下降。为解决这些挑战，本文提出了DuLoc，一种鲁棒且准确的定位方法，该方法将LiDAR-惯性里程计与基于离线地图的定位紧密耦合，并采用恒定速度运动模型来减轻现实场景中的异常噪声。具体而言，我们开发了一种基于LiDAR的定位框架，该框架将先验全局地图与动态实时局部地图无缝集成，使得在无界和变化环境中实现鲁棒定位成为可能。本研究在涉及32辆智能引导车(IGV)的2856小时运行数据的超大型港口进行了大量现实实验。结果表明，我们的系统在大型变化的户外环境中优于其他最先进的LiDAR定位系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based localization serves as a critical component in autonomoussystems, yet existing approaches face persistent challenges in balancingrepeatability, accuracy, and environmental adaptability. Traditional pointcloud registration methods relying solely on offline maps often exhibit limitedrobustness against long-term environmental changes, leading to localizationdrift and reliability degradation in dynamic real-world scenarios. To addressthese challenges, this paper proposes DuLoc, a robust and accurate localizationmethod that tightly couples LiDAR-inertial odometry with offline map-basedlocalization, incorporating a constant-velocity motion model to mitigateoutlier noise in real-world scenarios. Specifically, we develop a LiDAR-basedlocalization framework that seamlessly integrates a prior global map withdynamic real-time local maps, enabling robust localization in unbounded andchanging environments. Extensive real-world experiments in ultra unbounded portthat involve 2,856 hours of operational data across 32 Intelligent GuidedVehicles (IGVs) are conducted and reported in this study. The results attaineddemonstrate that our system outperforms other state-of-the-art LiDARlocalization systems in large-scale changing outdoor environments.</description>
      <author>example@mail.com (Haoxuan Jiang, Peicong Qian, Yusen Xie, Xiaocong Li, Ming Liu, Jun Ma)</author>
      <guid isPermaLink="false">2507.23660v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization</title>
      <link>http://arxiv.org/abs/2507.23569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于3D高斯溅射(3DGS)的视觉定位方法，通过结合显式几何模型和隐式特征场，实现了准确且保护隐私的视觉定位。&lt;h4&gt;背景&lt;/h4&gt;视觉定位是在已知环境中估计相机姿态的任务，传统方法可能在准确性和隐私保护方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持高精度又能保护隐私的视觉定位方法。&lt;h4&gt;方法&lt;/h4&gt;提出高斯溅射特征场(GSFFs)，结合3DGS的密集几何信息和可微分光栅化算法学习鲁棒特征表示；通过对比框架对齐3D尺度感知特征场和2D特征编码器；利用3D结构感知聚类过程正则化表示学习；将特征转换为分割用于隐私保护定位；通过查询图像与GSFFs渲染特征的对齐实现姿态精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界数据集上评估显示，所提出的隐私和非隐私保护定位流程达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;基于3DGS的视觉定位方法在准确性和隐私保护方面都表现出色，是一种有效的视觉定位解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位是在已知环境中估计相机姿态的任务。在本文中，我们利用基于3D高斯溅射(3DGS)的表示方法进行准确且保护隐私的视觉定位。我们提出了高斯溅射特征场(GSFFs)，这是一种用于视觉定位的场景表示方法，结合了显式几何模型(3DGS)和隐式特征场。我们利用3DGS的密集几何信息和可微分光栅化算法来学习基于3D的鲁棒特征表示。特别是，我们通过对比框架将3D尺度感知特征场和2D特征编码器对齐到共同的嵌入空间。使用3D结构感知聚类过程，我们进一步正则化表示学习，并将特征无缝转换为分割，可用于保护隐私的视觉定位。姿态精炼涉及将查询图像的特征图或分割与从GSFFs场景表示渲染的特征图进行对齐，用于实现定位。在多个真实世界数据集上评估的隐私和非隐私保护定位流程显示出最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉定位任务中的隐私保护问题。视觉定位是估计相机在已知环境中的姿态，是自动驾驶和自主机器人的核心技术。然而，传统基于特征匹配的方法虽然准确，但存在隐私泄露风险，因为图像细节可能从特征描述符中恢复。随着视觉定位系统越来越多地通过云端部署，保护用户上传图像和场景的隐私变得至关重要，这关系到用户数据安全和隐私保护法规的遵守。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点：基于特征匹配的方法准确但存在隐私问题，SegLoc方法通过分割标签提高隐私但缺乏3D一致性，而NeRF-based方法虽有3D一致性但不提供隐私保护。作者选择3D高斯泼溅(3DGS)作为基础，因其高效渲染特性，结合显式几何模型和隐式特征场，提出高斯泼溅特征场(GSFFs)。借鉴了SegLoc的隐私保护思路和NeRF的多视角一致性思想，但通过显式表示提高了隐私保护能力。设计过程中，作者利用3DGS的密集几何信息和可微分光栅化算法，通过对比框架对齐3D和2D特征，并使用聚类将特征转换为分割标签实现隐私保护。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合3D高斯泼溅与特征场，创建一种新型场景表示方法，通过自监督学习特征表示，使2D图像特征与从3D渲染的特征能够对齐，并利用聚类将特征转换为分割标签实现隐私保护。整体流程包括：1)场景表示：采用高斯不透明场模型，使用三平面网格参数化特征场；2)自监督训练：通过对比损失对齐渲染的3D特征和提取的2D特征，使用原型对比损失改进特征区分度；3)特征定位：从查询图像提取特征，检索初始姿态，通过最小化特征误差细化姿态；4)隐私保护定位：将特征转换为分割标签，移除敏感信息，通过最小化分割图差异细化姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)高斯泼溅特征场(GSFFs)：结合显式几何模型和隐式特征场；2)自监督特征学习：通过对比框架和多视角一致性正则化学习特征；3)原型特征正则化：利用3D高斯云空间结构聚类构建特征原型；4)隐私保护转换：将高维特征转换为低维分割标签。相比之前工作的不同：与SegLoc相比，GSFFs提供3D一致的分割标签和更高精度；与基于NeRF的方法相比，GSFFs使用显式表示更适合隐私保护且渲染更快；与其他基于3DGS的方法相比，GSFFs自监督学习特征且直接通过光栅器反向传播姿态误差。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了高斯泼溅特征场(GSFFs)，一种结合3D高斯泼溅与特征场的新型场景表示方法，实现了准确且隐私保护的视觉定位，在多个真实世界数据集上取得了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization is the task of estimating a camera pose in a knownenvironment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-basedrepresentations for accurate and privacy-preserving visual localization. Wepropose Gaussian Splatting Feature Fields (GSFFs), a scene representation forvisual localization that combines an explicit geometry model (3DGS) with animplicit feature field. We leverage the dense geometric information anddifferentiable rasterization algorithm from 3DGS to learn robust featurerepresentations grounded in 3D. In particular, we align a 3D scale-awarefeature field and a 2D feature encoder in a common embedding space through acontrastive framework. Using a 3D structure-informed clustering procedure, wefurther regularize the representation learning and seamlessly convert thefeatures to segmentations, which can be used for privacy-preserving visuallocalization. Pose refinement, which involves aligning either feature maps orsegmentations from a query image with those rendered from the GSFFs scenerepresentation, is used to achieve localization. The resulting privacy- andnon-privacy-preserving localization pipelines, evaluated on multiple real-worlddatasets, show state-of-the-art performances.</description>
      <author>example@mail.com (Maxime Pietrantoni, Gabriela Csurka, Torsten Sattler)</author>
      <guid isPermaLink="false">2507.23569v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Your Spending Needs Attention: Modeling Financial Habits with Transformers</title>
      <link>http://arxiv.org/abs/2507.23267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为nuFormer的新方法，将基于Transformer的自监督学习应用于金融交易数据，通过结合文本和结构化属性，并使用端到端微调，在Nubank的推荐系统中实现了性能提升。&lt;h4&gt;背景&lt;/h4&gt;预测模型在金融行业中至关重要，用于风险预测、欺诈检测和个性化推荐。金融机构拥有大量用户数据，但有效利用这些数据具有挑战性。目前大多数生产模型采用传统机器学习方法，而其他领域已有效利用自监督学习从原始数据中学习丰富表示。&lt;h4&gt;目的&lt;/h4&gt;研究基于Transformer的表示学习模型处理交易数据，探索这些在大规模数据上训练的模型是否可以提供理解和理解客户行为的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出一种新方法使自监督学习能够应用于交易数据，通过调整基于Transformer的模型来处理文本和结构化属性。提出了名为nuFormer的方法，包括一种将用户嵌入与现有表格特征集成的端到端微调方法。&lt;h4&gt;主要发现&lt;/h4&gt;在Nubank的大规模推荐问题上显示出改进，这些改进仅通过增强表示学习实现，而非整合新数据源。&lt;h4&gt;结论&lt;/h4&gt;基于Transformer的表示学习模型可以有效地应用于金融交易数据，替代传统的手动特征工程，提供更强大的客户行为理解。&lt;h4&gt;翻译&lt;/h4&gt;预测模型在金融行业中起着关键作用，能够进行风险预测、欺诈检测和个性化推荐，其中核心模型性能的微小变化可能导致数十亿美元的收益或损失。虽然金融机构可以访问大量用户数据（例如银行交易、应用内事件和客户支持日志），但由于其复杂性和规模，有效利用这些数据仍然具有挑战性。因此，在许多金融机构中，大多数生产模型采用传统机器学习方法，将非结构化数据转换为手动设计的表格特征。相反，其他领域（如自然语言处理）已有效利用自监督学习从原始数据中学习丰富表示，无需手动特征提取。在本文中，我们研究使用基于Transformer的表示学习模型处理交易数据，假设这些在大规模数据上训练的模型可以提供理解和理解客户行为的新方法。我们提出了一种新方法，通过调整基于Transformer的模型来处理文本和结构化属性，使自监督学习能够应用于交易数据。我们的方法称为nuFormer，包括一种将用户嵌入与现有表格特征集成的端到端微调方法。我们的实验证明了在Nubank的大规模推荐问题上有所改进。值得注意的是，这些改进仅通过增强表示学习实现，而非整合新数据源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive models play a crucial role in the financial industry, enablingrisk prediction, fraud detection, and personalized recommendations, whereslight changes in core model performance can result in billions of dollars inrevenue or losses. While financial institutions have access to enormous amountsof user data (e.g., bank transactions, in-app events, and customer supportlogs), leveraging this data effectively remains challenging due to itscomplexity and scale. Thus, in many financial institutions, most productionmodels follow traditional machine learning (ML) approaches by convertingunstructured data into manually engineered tabular features. Conversely, otherdomains (e.g., natural language processing) have effectively utilizedself-supervised learning (SSL) to learn rich representations from raw data,removing the need for manual feature extraction. In this paper, we investigateusing transformer-based representation learning models for transaction data,hypothesizing that these models, trained on massive data, can provide a noveland powerful approach to understanding customer behavior. We propose a newmethod enabling the use of SSL with transaction data by adaptingtransformer-based models to handle both textual and structured attributes. Ourapproach, denoted nuFormer, includes an end-to-end fine-tuning method thatintegrates user embeddings with existing tabular features. Our experimentsdemonstrate improvements for large-scale recommendation problems at Nubank.Notably, these gains are achieved solely through enhanced representationlearning rather than incorporating new data sources.</description>
      <author>example@mail.com (D. T. Braithwaite, Misael Cavalcanti, R. Austin McEver, Hiroto Udagawa, Daniel Silva, Rohan Ramanath, Felipe Meneses, Arissa Yoshida, Evan Wingert, Matheus Ramos, Brian Zanfelice, Aman Gupta)</author>
      <guid isPermaLink="false">2507.23267v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Balancing Information Preservation and Disentanglement in Self-Supervised Music Representation Learning</title>
      <link>http://arxiv.org/abs/2507.22995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In proceedings of WASPAA 2025. 4 pages, 4 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种统一的多视图自监督学习框架，结合了对比和重建目标，用于解耦音乐音频表示。通过广泛评估发现，这两种方法虽然存在权衡，但可以相互补充，最终能够在不损害信息完整性的情况下解耦音乐属性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习(SSL)方法在音乐音频领域取得了进展，能够无需标记数据就能捕获有用的音乐表示。现有方法分为两类：一类通过重建来保留全面细节，另一类通过对比目标来保留语义结构。很少有研究在一个统一的SSL框架中检查这些范式之间的相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一个多视图SSL框架，用于解耦音乐音频表示，结合对比和重建目标，促进解耦子空间中信息保真度和结构语义。&lt;h4&gt;方法&lt;/h4&gt;设计了一个结合对比和重建目标的架构，在受控环境中对对比策略的设计选择进行了广泛评估，使用音乐音频表示进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;重建和对比策略表现出一致性的权衡，当有效结合时，它们可以相互补充，能够在不损害信息完整性的情况下解耦音乐属性。&lt;h4&gt;结论&lt;/h4&gt;多视图SSL框架能够有效结合对比和重建方法，这种结合可以解耦音乐属性同时保持信息完整性。&lt;h4&gt;翻译&lt;/h4&gt;最近的自监督学习(SSL)方法进展提供了多种策略，可以从音乐音频中捕获有用的表示，而无需标记数据。虽然一些技术通过重建来保留全面细节，但其他技术则通过对比目标来保留语义结构。很少有研究在统一的SSL框架中检查这些范式之间的相互作用。在这项工作中，我们提出了一个多视图SSL框架，用于解耦音乐音频表示，该框架结合了对比和重建目标。该架构旨在促进解耦子空间中因素的信息保真度和结构语义。我们在受控环境中使用音乐音频表示对对比策略的设计选择进行了广泛评估。我们发现，虽然重建和对比策略表现出一致性的权衡，但当有效结合时，它们可以相互补充；这能够在不损害信息完整性的情况下解耦音乐属性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in self-supervised learning (SSL) methods offer a range ofstrategies for capturing useful representations from music audio without theneed for labeled data. While some techniques focus on preserving comprehensivedetails through reconstruction, others favor semantic structure via contrastiveobjectives. Few works examine the interaction between these paradigms in aunified SSL framework. In this work, we propose a multi-view SSL framework fordisentangling music audio representations that combines contrastive andreconstructive objectives. The architecture is designed to promote bothinformation fidelity and structured semantics of factors in disentangledsubspaces. We perform an extensive evaluation on the design choices ofcontrastive strategies using music audio representations in a controlledsetting. We find that while reconstruction and contrastive strategies exhibitconsistent trade-offs, when combined effectively, they complement each other;this enables the disentanglement of music attributes without compromisinginformation integrity.</description>
      <author>example@mail.com (Julia Wilkins, Sivan Ding, Magdalena Fuentes, Juan Pablo Bello)</author>
      <guid isPermaLink="false">2507.22995v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2507.23772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SeqSplatNet框架，用于解决3D功能区域推理中的长期、多物体任务挑战，通过引入序列3D高斯功能区域推理任务和SeqAffordSplat基准测试，成功将功能区域推理从单步交互提升到场景级别的复杂序列任务。&lt;h4&gt;背景&lt;/h4&gt;3D功能区域推理是具身智能体的关键能力，但当前基于3D Gaussian Splatting的方法仅限于单物体、单步交互，无法满足复杂现实应用所需的长期、多物体任务需求。&lt;h4&gt;目的&lt;/h4&gt;弥补现有方法的不足，引入序列3D高斯功能区域推理新任务，并建立SeqAffordSplat基准测试，支持复杂3DGS环境中的长期功能区域理解研究。&lt;h4&gt;方法&lt;/h4&gt;提出SeqSplatNet端到端框架，使用大型语言模型自回归生成交织文本与分割标记，引入条件几何重建预训练策略建立几何先验，设计特征注入机制融合2D视觉基础模型的语义特征到3D解码器。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明，该方法在具有挑战性的基准测试上达到新的最先进水平，有效推动功能区域推理从单步交互发展到场景级别的复杂序列任务。&lt;h4&gt;结论&lt;/h4&gt;该方法成功解决了3D功能区域推理中的长期、多物体任务挑战，为具身智能体在复杂环境中的交互提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;3D功能区域推理是将人类指令与3D物体功能区域关联的任务，是具身智能体的关键能力。当前基于3D高斯溅射的方法根本局限于单物体、单步交互，这种范式无法解决复杂现实应用所需的长期、多物体任务。为弥合这一差距，我们引入了序列3D高斯功能区域推理这一新颖任务，并建立了SeqAffordSplat基准测试，包含1800多个场景，支持复杂3DGS环境中的长期功能区域理解研究。随后我们提出了SeqSplatNet，一个端到端框架，直接将指令映射为3D功能区域掩码序列。SeqSplatNet采用大型语言模型自回归生成与特殊分割标记交织的文本，引导条件解码器生成相应的3D掩码。为处理复杂场景几何，我们引入了条件几何重建预训练策略，让模型从已知的几何观测中重建完整的功能区域掩码，从而建立强大的几何先验。此外，为解决语义歧义，我们设计了一个特征注入机制，从2D视觉基础模型中提取丰富的语义特征，并将它们多尺度融合到3D解码器中。大量实验证明，我们的方法在具有挑战性的基准测试上设定了新的最先进水平，有效将功能区域推理从单步交互推进到场景级别的复杂任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前基于3D高斯溅射(3DGS)的功能推理方法只能处理单对象、单步交互的局限性，无法满足现实世界中长视距、多对象的复杂任务需求。这个问题在现实中很重要，因为3D功能推理是机器人操作、增强现实和虚拟现实等应用中具身智能体的核心能力，能够将人类指令与3D对象的功能区域相关联，使智能体能够理解并执行复杂的多步骤任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：点云方法存在稀疏性问题，而3DGS方法仅限于单步交互。他们借鉴了3DGS的高保真表示能力、大型语言模型的层次规划能力以及2D视觉基础模型的语义特征提取能力。设计上，他们创建了SeqSplatNet框架，整合了3DGS编码器、大型语言模型和条件功能解码器，并设计了条件几何重建预训练策略和语义特征注入机制来解决复杂场景中的几何和语义挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将大型语言模型的序列规划能力与3DGS的高保真表示能力统一到端到端架构中，直接将语言指令映射为一系列有序的3D功能掩码。整体流程是：1)接收复杂指令和3DGS场景；2)3DGS编码器提取几何信息，LLM处理指令并自回归生成包含特殊&lt;SEG&gt;标记的序列；3)每个&lt;SEG&gt;标记触发条件功能解码器，结合几何特征和注入的语义特征生成对应的3D功能掩码；4)输出完成复杂指令所需的有序功能掩码序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出场景级顺序3D高斯功能推理新任务；2)创建SeqAffordSplat大规模基准数据集(1800+场景、14000+掩码、8000+指令)；3)设计SeqSplatNet统一框架；4)提出条件几何重建预训练策略；5)实现语义特征注入机制。相比之前工作，不同之处在于：现有点云方法无法提供精细定位，而现有3DGS方法仅限于单步交互，本文首次同时实现了精细功能定位和长视距顺序推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SeqAffordSplat通过引入场景级顺序3D高斯功能推理任务和SeqSplatNet框架，首次将长视距序列推理与高保真3D表示相结合，显著提升了具身智能体在复杂环境中理解和执行多步骤指令的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D affordance reasoning, the task of associating human instructions with thefunctional regions of 3D objects, is a critical capability for embodied agents.Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limitedto single-object, single-step interactions, a paradigm that falls short ofaddressing the long-horizon, multi-object tasks required for complex real-worldapplications. To bridge this gap, we introduce the novel task of Sequential 3DGaussian Affordance Reasoning and establish SeqAffordSplat, a large-scalebenchmark featuring 1800+ scenes to support research on long-horizon affordanceunderstanding in complex 3DGS environments. We then propose SeqSplatNet, anend-to-end framework that directly maps an instruction to a sequence of 3Daffordance masks. SeqSplatNet employs a large language model thatautoregressively generates text interleaved with special segmentation tokens,guiding a conditional decoder to produce the corresponding 3D mask. To handlecomplex scene geometry, we introduce a pre-training strategy, ConditionalGeometric Reconstruction, where the model learns to reconstruct completeaffordance region masks from known geometric observations, thereby building arobust geometric prior. Furthermore, to resolve semantic ambiguities, we designa feature injection mechanism that lifts rich semantic features from 2D VisionFoundation Models (VFM) and fuses them into the 3D decoder at multiple scales.Extensive experiments demonstrate that our method sets a new state-of-the-arton our challenging benchmark, effectively advancing affordance reasoning fromsingle-step interactions to complex, sequential tasks at the scene level.</description>
      <author>example@mail.com (Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Yuhui Zheng, Mingtao Feng, Guangming Shi)</author>
      <guid isPermaLink="false">2507.23772v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2507.23673v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAMSA是一种结合RGB基础模型和光谱分析的交互式分割框架，有效解决了高光谱医学成像中的数据限制和硬件差异挑战。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像为医学成像提供丰富光谱信息，但面临数据限制和硬件差异带来的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的交互式分割框架，结合RGB基础模型与光谱分析，提高高光谱医学图像分割效果。&lt;h4&gt;方法&lt;/h4&gt;SAMSA利用用户点击指导RGB分割和光谱相似度计算，采用独特的光谱特征融合策略，独立于光谱波段数量和分辨率运作。&lt;h4&gt;主要发现&lt;/h4&gt;在神经外科高光谱数据集上达到八十一点零的1点击和九十三点四的5点击DICE分数，在术中猪高光谱数据集上达到八十一点一的1点击和八十九点二的5点击DICE分数。&lt;h4&gt;结论&lt;/h4&gt;SAMSA在小样本和零样本学习场景中表现有效，能够无缝集成不同光谱特性的数据集，为高光谱医学图像分析提供灵活框架。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像为医学成像提供丰富的光谱信息，但由于数据限制和硬件差异而遇到重大挑战。我们引入了SAMSA，一种结合RGB基础模型与光谱分析的新型交互式分割框架。SAMSA有效利用用户点击来指导RGB分割和光谱相似度计算。该方法通过一种独特的光谱特征融合策略解决了HSI分割的关键限制，该策略独立于光谱波段数量和分辨率。在公开可用数据集上的性能评估显示，在神经外科和术中猪高光谱数据集上分别达到八十一点零的1点击和九十三点四的5点击DICE，以及八十一点一的1点击和八十九点二的5点击DICE。实验结果表明SAMSA在小样本和零样本学习场景以及使用最少训练示例时的有效性。我们的方法能够无缝集成具有不同光谱特性的数据集，为高光谱医学图像分析提供了灵活的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) provides rich spectral information for medicalimaging, yet encounters significant challenges due to data limitations andhardware variations. We introduce SAMSA, a novel interactive segmentationframework that combines an RGB foundation model with spectral analysis. SAMSAefficiently utilizes user clicks to guide both RGB segmentation and spectralsimilarity computations. The method addresses key limitations in HSIsegmentation through a unique spectral feature fusion strategy that operatesindependently of spectral band count and resolution. Performance evaluation onpublicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on aneurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperativeporcine hyperspectral dataset. Experimental results demonstrate SAMSA'seffectiveness in few-shot and zero-shot learning scenarios and using minimaltraining examples. Our approach enables seamless integration of datasets withdifferent spectral characteristics, providing a flexible framework forhyperspectral medical image analysis.</description>
      <author>example@mail.com (Alfie Roddan, Tobias Czempiel, Chi Xu, Daniel S. Elson, Stamatia Giannarou)</author>
      <guid isPermaLink="false">2507.23673v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2507.23523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了H-RDT（Human to Robotics Diffusion Transformer），一种利用大规模人类操作视频数据增强机器人操作能力的新方法，通过两阶段训练范式和扩散Transformer架构，显著提升了机器人操作性能。&lt;h4&gt;背景&lt;/h4&gt;机器人模仿学习面临大规模高质量演示数据稀缺的挑战，而现有的机器人基础模型在跨形态机器人数据集上预训练时，由于不同机器人形态和动作空间的多样性，统一训练面临重大限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用人类操作数据增强机器人操作能力的新方法，解决机器人操作数据稀缺和跨形态训练困难的问题。&lt;h4&gt;方法&lt;/h4&gt;提出H-RDT方法，基于20亿参数的扩散Transformer架构，使用流匹配建模复杂动作分布；采用两阶段训练范式：首先在大规模以自我为中心的人类操作数据上进行预训练，然后在机器人特定数据上进行跨形态微调，使用模块化动作编码器和解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验、单任务和多任务场景、少样本学习和鲁棒性评估中，H-RDT优于从头训练和现有最先进方法；与从头训练相比，在模拟和真实世界实验中分别实现了13.9%和40.5%的显著改进。&lt;h4&gt;结论&lt;/h4&gt;人类操作数据可以作为学习双臂机器人操作策略的强大基础，验证了利用人类行为先验增强机器人操作能力的核心假设。&lt;h4&gt;翻译&lt;/h4&gt;机器人模仿学习面临一个基本挑战：大规模、高质量的机器人演示数据稀缺。最近的机器人基础模型通常在跨形态机器人数据集上进行预训练以增加数据规模，但由于不同机器人形态和动作空间的多样性，统一训练面临重大限制。在本文中，我们提出了H-RDT（Human to Robotics Diffusion Transformer），一种利用人类操作数据增强机器人操作能力的新方法。我们的核心见解是，大规模带有配对3D手部姿态注释的以自我为中心的人类操作视频提供了丰富的行为先验，可以捕捉自然操作策略并有益于机器人策略学习。我们引入了两阶段训练范式：（1）在大规模以自我为中心的人类操作数据上进行预训练，（2）在机器人特定数据上进行跨形态微调，使用模块化动作编码器和解码器。基于具有20亿参数的扩散Transformer架构构建，H-RDT使用流匹配来建模复杂的动作分布。在模拟和真实世界实验、单任务和多任务场景、少样本学习和鲁棒性评估的广泛评估中，H-RDT优于从头训练和现有的最先进方法，包括Pi0和RDT，在模拟和真实世界实验中分别实现了比从头训练高13.9%和40.5%的显著改进。结果验证了我们的核心假设：人类操作数据可以作为学习双臂机器人操作策略的强大基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning for robotic manipulation faces a fundamental challenge:the scarcity of large-scale, high-quality robot demonstration data. Recentrobotic foundation models often pre-train on cross-embodiment robot datasets toincrease data scale, while they face significant limitations as the diversemorphologies and action spaces across different robot embodiments make unifiedtraining challenging. In this paper, we present H-RDT (Human to RoboticsDiffusion Transformer), a novel approach that leverages human manipulation datato enhance robot manipulation capabilities. Our key insight is that large-scaleegocentric human manipulation videos with paired 3D hand pose annotationsprovide rich behavioral priors that capture natural manipulation strategies andcan benefit robotic policy learning. We introduce a two-stage trainingparadigm: (1) pre-training on large-scale egocentric human manipulation data,and (2) cross-embodiment fine-tuning on robot-specific data with modular actionencoders and decoders. Built on a diffusion transformer architecture with 2Bparameters, H-RDT uses flow matching to model complex action distributions.Extensive evaluations encompassing both simulation and real-world experiments,single-task and multitask scenarios, as well as few-shot learning androbustness assessments, demonstrate that H-RDT outperforms training fromscratch and existing state-of-the-art methods, including Pi0 and RDT, achievingsignificant improvements of 13.9% and 40.5% over training from scratch insimulation and real-world experiments, respectively. The results validate ourcore hypothesis that human manipulation data can serve as a powerful foundationfor learning bimanual robotic manipulation policies.</description>
      <author>example@mail.com (Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, Jun Zhu)</author>
      <guid isPermaLink="false">2507.23523v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2</title>
      <link>http://arxiv.org/abs/2507.23272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the 28th International Conference on  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep  Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in  Breast Care (DeepBreath), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何将Segment Anything Model 2 (SAM2)模型适应用于低成本、低输入的3D乳腺MRI肿瘤分割，发现使用单一切片边界框注释结合从中心向外传播策略能提供最佳分割效果，为资源受限环境提供了经济实惠的医学图像分析解决方案。&lt;h4&gt;背景&lt;/h4&gt;乳腺MRI提供高分辨率体积成像对肿瘤评估和治疗规划至关重要，但3D扫描的手动解释劳动密集且主观。商业医疗AI产品在低收入和中等收入国家采用有限，主要受制于高昂的许可费用、专有软件和基础设施需求。&lt;h4&gt;目的&lt;/h4&gt;调查Segment Anything Model 2 (SAM2)是否可以适应用于低成本、低输入的3D乳腺MRI肿瘤分割。&lt;h4&gt;方法&lt;/h4&gt;使用单一切片上的单个边界框注释，通过三种不同的切片跟踪策略（从上到下、从下到上、从中心向外）将分割预测传播到整个3D体积，并在大量患者队列中评估这些策略。&lt;h4&gt;主要发现&lt;/h4&gt;1. 从中心向外传播策略提供了最一致和准确的分割结果；2. 尽管SAM2是为通用目的设计的零样本模型，没有针对体积医学数据进行训练，但在最少监督下仍能实现强大的分割性能；3. 研究分析了分割性能与肿瘤大小、位置和形状的关系，确定了关键的失败模式。&lt;h4&gt;结论&lt;/h4&gt;通用基础模型如SAM2可以在最少监督的情况下支持3D医学图像分析，为资源受限环境提供了一种可访问且经济实惠的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;乳腺MRI提供高分辨率体积成像，对肿瘤评估和治疗规划至关重要，但3D扫描的手动解释仍然劳动密集且主观。虽然人工智能工具有望加速医学图像分析，但由于高昂的许可费用、专有软件和基础设施需求，商业医疗AI产品在低收入和中等收入国家的采用仍然有限。在这项工作中，我们研究了Segment Anything Model 2 (SAM2)是否可以适应用于低成本、低输入的3D乳腺MRI肿瘤分割。使用一个切片上的单个边界框注释，我们通过三种不同的切片跟踪策略将分割预测传播到整个3D体积：从上到下、从下到上和从中心向外。我们在大量患者队列中评估了这些策略，发现从中心向外传播提供了最一致和准确的分割结果。尽管SAM2是一个没有针对体积医学数据进行训练的零样本模型，但在最少监督下仍能实现强大的分割性能。我们进一步分析了分割性能与肿瘤大小、位置和形状的关系，确定了关键的失败模式。我们的结果表明，像SAM2这样的通用基础模型可以在最少监督的情况下支持3D医学图像分析，为资源受限环境提供了一种可访问且经济实惠的替代方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何开发低成本、低资源需求的3D乳腺MRI肿瘤分割和可视化方法。这个问题很重要，因为乳腺癌是全球最常见的癌症之一，早期准确检测对改善患者预后至关重要。然而，3D乳腺MRI包含数百张切片，人工解读耗时且主观，而现有AI工具因成本高、需要专业基础设施，在资源有限的医疗环境中难以普及。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有AI医疗工具在资源有限地区的应用障碍，以及传统3D肿瘤分割方法需要大量标注数据的局限性。他们借鉴了视频对象跟踪技术，将SAM2这一开源基础模型应用于医学图像领域。SAM2原本是为图像和视频分割设计的，作者创新性地将其用于3D医学数据的切片级处理，并设计了三种不同的传播策略来遍历3D体积。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D乳腺MRI视为2D切片序列，利用SAM2模型的跟踪功能，通过最小人工输入（单个边界框）实现肿瘤分割。流程包括：1)从3D MRI中提取2D切片；2)在一个切片上提供边界框作为初始输入；3)使用SAM2的跟踪功能在相邻切片间传播分割；4)评估三种传播策略（从下到上、从上到下、从中心向外）；5)使用体积Dice系数评估分割效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)利用SAM2这一零样本基础模型实现最小监督的3D肿瘤分割；2)发现并验证了'从中心向外'的传播策略最有效；3)完全基于开源工具，无需专业基础设施；4)只需单一切片的边界框输入，大幅减少人工标注；5)分析失败模式，提供对模型局限性的深入理解。相比传统方法，这种方法不需要大量标注数据和密集训练，更适合资源有限的医疗环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文成功将SAM2开源模型应用于3D乳腺MRI肿瘤分割，通过最小人工输入实现了准确分割，为资源有限的医疗环境提供了一种可负担且可访问的AI解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast MRI provides high-resolution volumetric imaging critical for tumorassessment and treatment planning, yet manual interpretation of 3D scansremains labor-intensive and subjective. While AI-powered tools hold promise foraccelerating medical image analysis, adoption of commercial medical AI productsremains limited in low- and middle-income countries due to high license costs,proprietary software, and infrastructure demands. In this work, we investigatewhether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,minimal-input 3D tumor segmentation in breast MRI. Using a single bounding boxannotation on one slice, we propagate segmentation predictions across the 3Dvolume using three different slice-wise tracking strategies: top-to-bottom,bottom-to-top, and center-outward. We evaluate these strategies across a largecohort of patients and find that center-outward propagation yields the mostconsistent and accurate segmentations. Despite being a zero-shot model nottrained for volumetric medical data, SAM2 achieves strong segmentationperformance under minimal supervision. We further analyze how segmentationperformance relates to tumor size, location, and shape, identifying key failuremodes. Our results suggest that general-purpose foundation models such as SAM2can support 3D medical image analysis with minimal supervision, offering anaccessible and affordable alternative for resource-constrained settings.</description>
      <author>example@mail.com (Solha Kang, Eugene Kim, Joris Vankerschaver, Utku Ozbulak)</author>
      <guid isPermaLink="false">2507.23272v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs</title>
      <link>http://arxiv.org/abs/2507.23227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为TAP-GPT的新型框架，利用大型语言模型进行阿尔茨海默病的早期准确诊断。该框架通过整合多种生物标志物数据，在小样本情况下实现了优于现有方法的诊断性能。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病是一种复杂的神经退行性疾病，其早期准确诊断需要分析包括神经影像、遗传风险因素、认知测试和脑脊液蛋白在内的多种异质性生物标志物，这些数据通常以表格形式呈现。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用结构化生物标志数据进行阿尔茨海默病诊断的框架，解决小样本量下的诊断挑战。&lt;h4&gt;方法&lt;/h4&gt;提出TAP-GPT框架，将TableGPT2(一种商业智能任务开发的多模态表格专用LLM)适应用于AD诊断。通过构建少量样本表格提示和使用qLoRA参数高效适配技术对TableGPT2进行微调，实现AD与认知正常的临床二分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;TAP-GPT框架利用TableGPT2的表格理解能力和LLMs的先验知识，在预测任务上表现优于先进的通用LLMs和专为预测任务开发的表格基础模型(TFM)。这是首次将LLMs应用于表格生物标志数据预测任务。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来生物信息学中的LLM驱动多智能体框架铺平了道路，展示了大型语言模型在医疗诊断特别是神经退行性疾病诊断中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病(AD)是一种复杂的神经退行性疾病，其早期准确诊断需要分析异质性生物标志物(如神经影像、遗传风险因素、认知测试和脑脊液蛋白)，这些数据通常以表格形式呈现。凭借灵活的少样本推理、多模态集成和基于自然语言的可解释性，大型语言模型(LLMs)为结构化生物医学数据的预测提供了前所未有的机会。我们提出了一种名为TAP-GPT(表格阿尔茨海默病预测GPT)的新框架，该框架将TableGPT2(一种最初为商业智能任务开发的多模态表格专用LLM)适应用于使用小样本量的结构化生物标志数据进行AD诊断。我们的方法使用结构化生物医学数据中的上下文学习示例构建少量样本表格提示，并使用参数高效的qLoRA适配对TableGPT2进行微调，用于AD或认知正常(CN)的临床二分类任务。TAP-GPT框架利用TableGPT2的强大表格理解能力和LLMs的编码先验知识，在性能上超越了更先进的通用LLMs和专为预测任务开发的表格基础模型(TFM)。据我们所知，这是首次将LLMs应用于使用表格生物标志数据的预测任务，为未来生物信息学中的LLM驱动多智能体框架铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early and accurate diagnosis of Alzheimer's disease (AD), a complexneurodegenerative disorder, requires analysis of heterogeneous biomarkers(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinalfluid proteins) typically represented in a tabular format. With flexiblefew-shot reasoning, multimodal integration, and natural-language-basedinterpretability, large language models (LLMs) offer unprecedentedopportunities for prediction with structured biomedical data. We propose anovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adaptsTableGPT2, a multimodal tabular-specialized LLM originally developed forbusiness intelligence tasks, for AD diagnosis using structured biomarker datawith small sample sizes. Our approach constructs few-shot tabular prompts usingin-context learning examples from structured biomedical data and finetunesTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binaryclassification task of AD or cognitively normal (CN). The TAP-GPT frameworkharnesses the powerful tabular understanding ability of TableGPT2 and theencoded prior knowledge of LLMs to outperform more advanced general-purposeLLMs and a tabular foundation model (TFM) developed for prediction tasks. Toour knowledge, this is the first application of LLMs to the prediction taskusing tabular biomarker data, paving the way for future LLM-driven multi-agentframeworks in biomedical informatics.</description>
      <author>example@mail.com (Sophie Kearney, Shu Yang, Zixuan Wen, Bojian Hou, Duy Duong-Tran, Tianlong Chen, Jason Moore, Marylyn Ritchie, Li Shen)</author>
      <guid isPermaLink="false">2507.23227v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Clean Energy Forecasting: A Comprehensive Review</title>
      <link>http://arxiv.org/abs/2507.23147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is currently under review at the journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了基础模型(FMs)在可再生能源预测领域的应用，特别是风能和太阳能预测。论文涵盖了模型架构、预训练策略、微调方法和数据类型，强调了大规模训练模型和领域特定Transformer架构的重要性，并评估了预测准确性方面的最新进展和存在的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着全球能源系统向清洁能源转型，准确的可再生能源发电量和需求预测对有效的电网管理至关重要。基础模型能够快速处理复杂的高维时间序列数据，从而提高预测准确性。&lt;h4&gt;目的&lt;/h4&gt;这篇综述论文旨在概述基础模型在可再生能源预测领域的应用，评估其在预测准确性方面的最新进展，讨论现有挑战和改进领域，并提出未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;论文通过综述现有研究，分析了基础模型的架构、预训练策略、微调方法和数据类型，特别关注了大规模训练模型、领域特定Transformer架构、时空相关性、领域知识嵌入以及可再生能源发电的短暂性和间歇性。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在可再生能源预测中表现出色，特别是在多时间尺度预测协调和不确定性量化方面。论文区分了理论和实践，批判性评估了FMs的优缺点，并确定了长期和多变量时间序列预测中的挑战和改进领域。&lt;h4&gt;结论&lt;/h4&gt;基础模型为可再生能源预测提供了新的可能性，但仍存在理论和实践上的差距。未来研究应关注解决这些差距，进一步提高预测准确性，特别是在长期和多变量时间序列预测方面。&lt;h4&gt;翻译&lt;/h4&gt;随着全球能源系统向清洁能源转型，准确的可再生能源发电和需求预测对有效的电网管理至关重要。基础模型(FMs)可以帮助提高可再生能源发电和需求的预测，因为FMs能够快速处理复杂的高维时间序列数据。这篇综述论文专注于可再生能源预测领域的基础模型，主要关注风能和太阳能。我们概述了可再生能源预测背景下的架构、预训练策略、微调方法和数据类型。我们强调了大规模训练模型、领域特定Transformer架构的作用，这些架构关注时空相关性、领域知识的嵌入，以及可再生能源发电的短暂性和间歇性。我们评估了基于FMs在预测准确性方面的最新进展，如协调多时间尺度的预测和量化可再生能源预测中的不确定性。我们还回顾了长期和多变量时间序列预测中现有的挑战和改进领域。在本综述中，我们建立了在清洁能源预测领域使用FMs时理论与实践之间的区别。此外，它批判性地评估了FMs的优缺点，同时推进了这一新兴而令人兴奋的预测领域的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As global energy systems transit to clean energy, accurate renewablegeneration and renewable demand forecasting is imperative for effective gridmanagement. Foundation Models (FMs) can help improve forecasting of renewablegeneration and demand because FMs can rapidly process complex, high-dimensionaltime-series data. This review paper focuses on FMs in the realm of renewableenergy forecasting, primarily focusing on wind and solar. We present anoverview of the architectures, pretraining strategies, finetuning methods, andtypes of data used in the context of renewable energy forecasting. We emphasizethe role of models that are trained at a large scale, domain specificTransformer architectures, where attention is paid to spatial temporalcorrelations, the embedding of domain knowledge, and also the brief andintermittent nature of renewable generation. We assess recent FM basedadvancements in forecast accuracy such as reconciling predictions over multipletime scales and quantifying uncertainty in renewable energy forecasting. Wealso review existing challenges and areas of improvement in long-term andmultivariate time series forecasting. In this survey, a distinction betweentheory and practice is established regarding the use of FMs in the clean energyforecasting domain. Additionally, it critically assesses the strengths andweaknesses of FMs while advancing future research direction in this new andexciting area of forecasting.</description>
      <author>example@mail.com (Md Meftahul Ferdaus, Tanmoy Dam, Md Rasel Sarkar, Moslem Uddin, Sreenatha G. Anavatti)</author>
      <guid isPermaLink="false">2507.23147v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance</title>
      <link>http://arxiv.org/abs/2507.23088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型的感知代理(Perception Agent)，通过结合语音集成的提示工程大型语言模型、分割任何模型和任意点跟踪基础模型，实现了更自然的人机交互，用于实时术中外科手术辅助。该代理包含记忆库和两种分割未见元素的新机制，能够记忆新元素供未来手术使用，解决了当前AI驱动解决方案的僵化问题。&lt;h4&gt;背景&lt;/h4&gt;外科手术数据科学和机器人解决方案需要自然的人机界面来充分发挥其潜力，但当代AI驱动解决方案本质上仍然僵化，灵活性有限，限制了动态手术环境中的自然人机交互。这些解决方案依赖于大量特定任务预训练、固定物体类别和明确的手动提示。&lt;h4&gt;目的&lt;/h4&gt;引入一种新型感知代理，实现更自然的人机交互，用于实时术中外科手术辅助，克服当前AI驱动解决方案的僵化问题。&lt;h4&gt;方法&lt;/h4&gt;利用语音集成的提示工程大型语言模型、分割任何模型和任意点跟踪基础模型；包含记忆库和两种分割未见元素的新机制；能够通过直观交互分割手术场景中已知和未见的元素；具备记忆新元素供未来手术使用的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上的定量分析表明，代理性能与劳动强度大得多的手动提示策略相当；在自定义数据集上的定性分析展示了代理在分割新元素（器械、移植物纱布）方面的灵活性。&lt;h4&gt;结论&lt;/h4&gt;通过提供自然的人机交互和克服僵化，感知代理可能使基于AI的实时辅助在动态手术环境中更接近现实，在外科手术中人机共生方面迈出了显著的一步。&lt;h4&gt;翻译&lt;/h4&gt;新兴的外科手术数据科学和机器人解决方案，特别是那些旨在现场提供帮助的解决方案，需要自然的人机界面来充分发挥其在提供自适应和直观帮助方面的潜力。当代的AI驱动解决方案本质上仍然僵化，灵活性有限，限制了动态手术环境中的自然人机交互。这些解决方案严重依赖于大量特定任务的预训练、固定的物体类别和明确的手动提示。本文介绍了一种新型的感知代理，它利用语音集成的提示工程大型语言模型、分割任何模型和任意点跟踪基础模型，以实现更自然的人机交互，用于实时术中外科手术辅助。通过包含记忆库和两种用于分割未见元素的新机制，感知代理能够通过直观的交互来分割手术场景中已知和未见的元素。通过具备记忆新元素以供未来手术使用的能力，这项工作在外科手术中人机共生方面迈出了显著的一步。通过对公共数据集的定量分析，我们展示了代理的性能与劳动强度大得多的手动提示策略相当。在定性方面，我们在自定义数据集中展示了代理在分割新元素（器械、移植物纱布）方面的灵活性。通过提供自然的人机交互和克服僵化，我们的感知代理可能使基于AI的实时辅助在动态手术环境中更接近现实。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决当前AI手术辅助系统的'刚性'问题。现有AI系统缺乏灵活性，依赖大量预训练、固定对象类别和手动提示，无法自然适应动态手术环境。这个问题很重要，因为随着手术越来越复杂，需要能自然集成并适应变化的智能辅助系统。灵活的AI系统可以增强决策制定和程序效率，提高手术精度，减少并发症风险，最终改善患者治疗效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前手术AI系统的局限性，特别是它们无法处理新颖手术元素和缺乏自然交互能力。他们设计了一个名为'Perception Agent'的系统，整合语音交互、记忆存储和新型分割机制。作者确实借鉴了现有基础模型如SAM2（分割模型）和CoTracker3（点跟踪模型），以及大语言模型，但将这些技术以创新方式组合，并添加了记忆存储库和新的分割方法，创造出更加灵活的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个能通过自然语言和手势与外科医生交互的AI系统，实时分割和跟踪手术场景中的已知和新元素，并能'学习'新元素供未来使用。整体流程包括：1) 语音指令转换为文本并提取任务和元素；2) 对于已知元素，从记忆库中检索并注入SAM2进行分割；3) 对于新元素，采用两种方法：以对象为中心的方法通过分析运动模式识别新仪器，或基于参考的方法通过已知对象关系定位新元素；4) 将新元素存入记忆库供未来使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 记忆存储库，能存储和检索手术元素的记忆；2) 语音集成的大语言模型，理解自然语言指令；3) 以对象为中心的分割方法，通过运动识别新仪器；4) 基于参考的分割方法，通过对象关系定位新元素；5) 人机共生能力，系统可不断学习适应。相比之前工作，不同之处在于：之前的系统只能处理预定义类别，需要手动标记，无法学习新元素；而这个系统能理解自然语言，分割已知和新元素，并能记忆新元素供未来使用，实现了更自然、更灵活的人机交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文引入了Perception Agent，一个创新的AI系统，通过整合语音交互、记忆存储和新型分割机制，实现了手术中自然、灵活的人机协作，使AI能够实时分割已知和新元素，并学习记忆新元素以供未来使用，朝着手术中人机共生的方向迈出了重要一步。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emerging surgical data science and robotics solutions, especially thosedesigned to provide assistance in situ, require natural human-machineinterfaces to fully unlock their potential in providing adaptive and intuitiveaid. Contemporary AI-driven solutions remain inherently rigid, offering limitedflexibility and restricting natural human-machine interaction in dynamicsurgical environments. These solutions rely heavily on extensive task-specificpre-training, fixed object categories, and explicit manual-prompting. This workintroduces a novel Perception Agent that leverages speech-integratedprompt-engineered large language models (LLMs), segment anything model (SAM),and any-point tracking foundation models to enable a more natural human-machineinteraction in real-time intraoperative surgical assistance. Incorporating amemory repository and two novel mechanisms for segmenting unseen elements,Perception Agent offers the flexibility to segment both known and unseenelements in the surgical scene through intuitive interaction. Incorporating theability to memorize novel elements for use in future surgeries, this work takesa marked step towards human-machine symbiosis in surgical procedures. Throughquantitative analysis on a public dataset, we show that the performance of ouragent is on par with considerably more labor-intensive manual-promptingstrategies. Qualitatively, we show the flexibility of our agent in segmentingnovel elements (instruments, phantom grafts, and gauze) in a custom-curateddataset. By offering natural human-machine interaction and overcoming rigidity,our Perception Agent potentially brings AI-based real-time assistance indynamic surgical environments closer to reality.</description>
      <author>example@mail.com (Lalithkumar Seenivasan, Jiru Xu, Roger D. Soberanis Mukul, Hao Ding, Grayson Byrd, Yu-Chun Ku, Jose L. Porras, Masaru Ishii, Mathias Unberath)</author>
      <guid isPermaLink="false">2507.23088v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for Material Fracture Prediction</title>
      <link>http://arxiv.org/abs/2507.23077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种数据驱动的断裂预测基础模型，该模型基于Transformer架构，能够跨模拟器、多种材料和不同加载条件进行断裂预测，显著减少了对数据量的需求。&lt;h4&gt;背景&lt;/h4&gt;准确预测材料失效对设计安全可靠的结构至关重要，但断裂行为在多样化条件下难以建模。现有方法中，机器学习模型缺乏泛化能力，而基于物理的模拟器则分散且计算资源密集。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的断裂预测模型，解决现有方法的局限性，实现跨材料、跨模拟器的断裂预测，并减少对大量训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;构建基于Transformer的架构，支持结构化和非结构化网格，结合大型语言模型嵌入的文本输入（材料属性、边界条件和求解器设置），实现多模态输入设计，使模型能够灵活适应各种模拟场景。&lt;h4&gt;主要发现&lt;/h4&gt;该模型可用最少数据在多种下游任务上微调，包括失效时间估计和断裂演化建模；能推广至未见材料如钛和混凝土，仅需单个样本；相比标准机器学习方法大幅减少数据需求。&lt;h4&gt;结论&lt;/h4&gt;断裂预测可在单一模型架构下统一，为特定模拟器工作流程提供了可扩展、可扩展的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;准确预测材料何时以及如何失效对于设计在应力下运行的安全、可靠结构、机械系统和工程部件至关重要。然而，断裂行为在现实应用的多样化材料、几何形状和加载条件下仍然难以建模。虽然机器学习方法显示出前景，但大多数模型在狭窄数据集上训练，缺乏鲁棒性，并且难以泛化。同时，基于物理的模拟器提供高保真预测，但分散在各种专门方法中，需要大量高性能计算资源来探索输入空间。为了解决这些限制，我们提出了一个用于断裂预测的数据驱动基础模型，这是一个基于Transformer的架构，可以在模拟器、多种材料（包括塑料粘结炸药、钢、铝、页岩和钨）和不同加载条件下运行。该模型支持结构化和非结构化网格，将它们与大型语言模型嵌入的文本输入相结合，指定材料属性、边界条件和求解器设置。这种多模态输入设计使模型能够灵活适应各种模拟场景，而无需更改模型架构。训练好的模型可以用最少的数据在各种下游任务上进行微调，包括失效时间估计、建模断裂演化和适应组合有限元-离散元方法模拟。它还可以推广到未见过的材料，如钛和混凝土，只需要单个样本，与标准机器学习方法相比，显著减少了数据需求。我们的结果表明，断裂预测可以在单一模型架构下统一，为特定模拟器的工作流程提供了一种可扩展、可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting when and how materials fail is critical to designingsafe, reliable structures, mechanical systems, and engineered components thatoperate under stress. Yet, fracture behavior remains difficult to model acrossthe diversity of materials, geometries, and loading conditions in real-worldapplications. While machine learning (ML) methods show promise, most models aretrained on narrow datasets, lack robustness, and struggle to generalize.Meanwhile, physics-based simulators offer high-fidelity predictions but arefragmented across specialized methods and require substantial high-performancecomputing resources to explore the input space. To address these limitations,we present a data-driven foundation model for fracture prediction, atransformer-based architecture that operates across simulators, a wide range ofmaterials (including plastic-bonded explosives, steel, aluminum, shale, andtungsten), and diverse loading conditions. The model supports both structuredand unstructured meshes, combining them with large language model embeddings oftextual input decks specifying material properties, boundary conditions, andsolver settings. This multimodal input design enables flexible adaptationacross simulation scenarios without changes to the model architecture. Thetrained model can be fine-tuned with minimal data on diverse downstream tasks,including time-to-failure estimation, modeling fracture evolution, and adaptingto combined finite-discrete element method simulations. It also generalizes tounseen materials such as titanium and concrete, requiring as few as a singlesample, dramatically reducing data needs compared to standard ML. Our resultsshow that fracture prediction can be unified under a single model architecture,offering a scalable, extensible alternative to simulator-specific workflows.</description>
      <author>example@mail.com (Agnese Marcato, Aleksandra Pachalieva, Ryley G. Hill, Kai Gao, Xiaoyu Wang, Esteban Rougier, Zhou Lei, Vinamra Agrawal, Janel Chua, Qinjun Kang, Jeffrey D. Hyman, Abigail Hunter, Nathan DeBardeleben, Earl Lawrence, Hari Viswanathan, Daniel O'Malley, Javier E. Santos)</author>
      <guid isPermaLink="false">2507.23077v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation</title>
      <link>http://arxiv.org/abs/2507.23058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A dissertation submitted to The University of Manchester for the  degree of Bachelor of Science in Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了两种新的合成数据生成方法MObI和AnydoorMed，分别用于自动驾驶和医学图像分析，通过扩散模型实现高度真实且可控的多模态对象修复。&lt;h4&gt;背景&lt;/h4&gt;安全关键应用需要大量多模态数据进行严格测试，但真实世界数据收集成本高、复杂，合成数据方法需要高度真实性和可控性才能有效。&lt;h4&gt;目的&lt;/h4&gt;开发适用于自动驾驶和医学图像分析的合成数据生成方法，提供高度真实且可控的多模态数据。&lt;h4&gt;方法&lt;/h4&gt;MObI是首个多模态对象修复框架，利用扩散模型在相机和激光雷达数据中生成对象修复；AnydoorMed将此范式扩展到医学成像，专注于乳腺X光扫描的参考引导修复。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可以轻松适应不同感知模态，实现高度真实且可控的多模态对象修复。&lt;h4&gt;结论&lt;/h4&gt;这些方法为构建高度真实、可控和多模态反事实情景的下一代系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;该研究介绍了自动驾驶和医学图像分析领域中的两种新型合成数据生成方法：MObI和AnydoorMed。MObI是一种首创的多模态对象修复框架，利用扩散模型在感知模态（同时适用于相机和激光雷达）中生成真实且可控的对象修复。给定单个参考RGB图像，MObI能够在指定的3D位置（由边界框引导）将对象无缝插入到现有多模态场景中，同时保持语义一致性和多模态一致性。与仅依赖编辑掩码的传统修复方法不同，此方法使用3D边界框条件确保准确的空间定位和真实的缩放。AnydoorMed将此范式扩展到医学成像领域，专注于乳腺X光扫描的参考引导修复。它利用基于扩散的模型以令人印象深刻的细节保留修复异常，同时保持参考异常的结构完整性，并在语义上将其与周围组织融合。这些方法共同证明，自然图像中参考引导修复的基础模型可以轻松适应不同的感知模态，为能够构建高度真实、可控和多模态反事实情景的下一代系统铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成高质量、可控的多模态反事实数据问题。在自动驾驶领域，需要生成逼真的相机和激光雷达数据来测试感知系统；在医学领域，需要生成真实的异常图像来增强数据集。这个问题很重要，因为安全关键应用需要大量数据进行严格测试，但收集真实世界数据成本高、复杂，且难以覆盖罕见但重要的场景。现有合成数据方法往往缺乏可控性或真实感。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于反事实推理是人类智能核心的理念，认为构建能够生成反事实的系统是人工智能发展的重要方向。他们利用了生成模型的优势，特别是潜在扩散模型能够在学习的潜在空间内捕获底层语义结构。作者借鉴了多项现有工作，包括扩散模型（DDPM、DDIM、LDM）、参考引导修复方法（Paint-by-Example）、多模态数据生成技术和3D条件控制方法，并将这些技术整合并扩展到多模态反事实生成任务中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用参考引导的扩散修复技术在多模态数据中生成逼真的反事实示例，并通过3D边界框条件确保准确的定位和缩放。整体流程包括：1)数据处理和编码（图像、激光雷达或医学图像的预处理和VAE编码）；2)条件编码（场景上下文、参考对象、3D边界框和编辑掩码）；3)多模态生成（在潜在空间中使用扩散模型预测噪声，确保多模态一致性）；4)推理和合成（生成新数据并解码回原始空间）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MObI是首个用于相机-激光雷达多模态物体修复的框架，使用3D边界框条件确保准确空间定位；2)AnydoorMed将参考引导修复扩展到医学图像领域，能合成逼真的医学异常；3)端到端方法，联合生成多模态数据；4)保持语义一致性和多模态一致性。相比之前的工作，这种方法不依赖编辑掩码或高质量3D资产，能处理部分可见的对象，并且是端到端的解决方案，而非多阶段管道。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了两种创新方法（MObI和AnydoorMed），利用参考引导的扩散修复技术在自动驾驶和医学图像领域生成高质量、可控的多模态反事实数据，解决了安全关键应用中数据收集困难和现有合成方法缺乏可控性或真实感的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety-critical applications, such as autonomous driving and medical imageanalysis, require extensive multimodal data for rigorous testing. Syntheticdata methods are gaining prominence due to the cost and complexity of gatheringreal-world data, but they demand a high degree of realism and controllabilityto be useful. This work introduces two novel methods for synthetic datageneration in autonomous driving and medical image analysis, namely MObI andAnydoorMed, respectively. MObI is a first-of-its-kind framework for MultimodalObject Inpainting that leverages a diffusion model to produce realistic andcontrollable object inpaintings across perceptual modalities, demonstratedsimultaneously for camera and lidar. Given a single reference RGB image, MObIenables seamless object insertion into existing multimodal scenes at aspecified 3D location, guided by a bounding box, while maintaining semanticconsistency and multimodal coherence. Unlike traditional inpainting methodsthat rely solely on edit masks, this approach uses 3D bounding box conditioningto ensure accurate spatial positioning and realistic scaling. AnydoorMedextends this paradigm to the medical imaging domain, focusing onreference-guided inpainting for mammography scans. It leverages adiffusion-based model to inpaint anomalies with impressive detail preservation,maintaining the reference anomaly's structural integrity while semanticallyblending it with the surrounding tissue. Together, these methods demonstratethat foundation models for reference-guided inpainting in natural images can bereadily adapted to diverse perceptual modalities, paving the way for the nextgeneration of systems capable of constructing highly realistic, controllableand multimodal counterfactual scenarios.</description>
      <author>example@mail.com (Alexandru Buburuzan)</author>
      <guid isPermaLink="false">2507.23058v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Data Readiness for Scientific AI at Scale</title>
      <link>http://arxiv.org/abs/2507.23018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 1 figure, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了人工智能数据准备原则在大规模科学数据集训练基础模型中的应用，分析了四个代表性领域的典型工作流程，并提出了一个二维数据准备框架。&lt;h4&gt;背景&lt;/h4&gt;科学数据用于训练基础模型时面临数据准备的挑战，需要针对高性能计算环境特定的解决方案。&lt;h4&gt;目的&lt;/h4&gt;识别科学数据预处理中的通用模式和领域特定约束，建立评估科学数据准备就绪程度的框架，指导基础设施开发。&lt;h4&gt;方法&lt;/h4&gt;分析气候、核聚变、生物/健康和材料四个领域的典型工作流程，识别预处理模式和约束，构建二维数据准备框架。&lt;h4&gt;主要发现&lt;/h4&gt;科学数据转换为可扩展AI训练数据存在关键挑战，基于Transformer的生成模型需要特定的数据准备策略。&lt;h4&gt;结论&lt;/h4&gt;提出的二维框架形成了概念成熟度矩阵，可表征科学数据准备情况，指导开发标准化、跨域支持的科学AI基础设施，促进可扩展和可重复的科学AI。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了人工智能数据准备(DRAI)原则如何应用于用于训练基础模型的大规模科学数据集。我们分析了气候、核聚变、生物/健康和材料四个代表性领域的典型工作流程，以识别通用的预处理模式和领域特定约束。我们提出了一个由数据准备级别(从原始数据到AI就绪数据)和数据处理阶段(从摄取到分片)组成的二维框架，这两个维度都针对高性能计算(HPC)环境定制。该框架概述了将科学数据转换为可扩展AI训练数据的关键挑战，强调基于Transformer的生成模型。这两个维度共同形成了一个概念成熟度矩阵，用于表征科学数据准备情况，并指导基础设施开发朝着标准化、跨域支持方向发展，以实现科学AI的可扩展性和可重复性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3750720.3757282&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper examines how Data Readiness for AI (DRAI) principles apply toleadership-scale scientific datasets used to train foundation models. Weanalyze archetypal workflows across four representative domains - climate,nuclear fusion, bio/health, and materials - to identify common preprocessingpatterns and domain-specific constraints. We introduce a two-dimensionalreadiness framework composed of Data Readiness Levels (raw to AI-ready) andData Processing Stages (ingest to shard), both tailored to high performancecomputing (HPC) environments. This framework outlines key challenges intransforming scientific data for scalable AI training, emphasizingtransformer-based generative models. Together, these dimensions form aconceptual maturity matrix that characterizes scientific data readiness andguides infrastructure development toward standardized, cross-domain support forscalable and reproducible AI for science.</description>
      <author>example@mail.com (Wesley Brewer, Patrick Widener, Valentine Anantharaj, Feiyi Wang, Tom Beck, Arjun Shankar, Sarp Oral)</author>
      <guid isPermaLink="false">2507.23018v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity</title>
      <link>http://arxiv.org/abs/2507.22633v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;H2Tune是一种处理混合异构联邦微调场景的新方法，解决了客户端在模型架构和下游任务上表现出的双重异构性问题。该方法通过三个关键组件有效解决了异构矩阵聚合和多任务知识干扰两个主要挑战，实现了高达15.4%的准确率提升。&lt;h4&gt;背景&lt;/h4&gt;现有的联邦微调方法主要针对基础模型，而混合异构联邦微调（HHFFT）是一个尚未充分探索的场景，其中客户端在模型架构和下游任务上表现出双重异构性。&lt;h4&gt;目的&lt;/h4&gt;解决混合异构联邦微调中的两个主要挑战：异构矩阵聚合和多任务知识干扰，提高联邦微调的效率和效果。&lt;h4&gt;方法&lt;/h4&gt;提出了H2Tune框架，包含三个关键组件：(1)稀疏三重矩阵分解，通过构建秩一致的中介矩阵对齐不同客户端的隐藏维度；(2)关系引导的矩阵层对齐，处理异构层结构和表示能力；(3)交替任务知识解纠缠机制，通过交替优化解耦本地模型参数的共享和特定知识。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析证明了收敛率为O(1/√T)，实验结果表明与最先进的基线方法相比，准确率提高了高达15.4%。&lt;h4&gt;结论&lt;/h4&gt;H2Tune框架有效解决了混合异构联邦微调中的双重异构性问题，为处理不同架构模型和不同任务下的联邦学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;与现有的基础模型联邦微调方法不同，混合异构联邦微调（HHFFT）是一个尚未充分探索的场景，其中客户端在模型架构和下游任务上表现出双重异构性。这种混合异构性带来了两个重大挑战：1)异构矩阵聚合，客户端根据任务需求和资源限制采用不同的大规模基础模型，导致LoRA参数聚合时出现维度不匹配；2)多任务知识干扰，本地共享参数同时使用任务共享知识和任务特定知识进行训练，无法确保只有任务共享知识在客户端之间传递。为解决这些挑战，我们提出了H2Tune，一种处理混合异构性的联邦基础模型微调方法。我们的框架H2Tune包含三个关键组件：(i)稀疏三重矩阵分解，通过构建秩一致的中介矩阵对齐不同客户端的隐藏维度，并根据客户端资源进行自适应稀疏化；(ii)关系引导的矩阵层对齐，处理异构层结构和表示能力；(iii)交替任务知识解纠缠机制，通过交替优化解耦本地模型参数的共享和特定知识。理论分析证明了收敛率为O(1/√T)。大量实验表明，我们的方法与最先进的基线方法相比，准确率提高了高达15.4%。我们的代码可在https://anonymous.4open.science/r/H2Tune-1407获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Different from existing federated fine-tuning (FFT) methods for foundationmodels, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-exploredscenario where clients exhibit double heterogeneity in model architectures anddownstream tasks. This hybrid heterogeneity introduces two significantchallenges: 1) heterogeneous matrix aggregation, where clients adopt differentlarge-scale foundation models based on their task requirements and resourcelimitations, leading to dimensional mismatches during LoRA parameteraggregation; and 2) multi-task knowledge interference, where local sharedparameters, trained with both task-shared and task-specific knowledge, cannotensure only task-shared knowledge is transferred between clients. To addressthese challenges, we propose H2Tune, a federated foundation model fine-tuningwith hybrid heterogeneity. Our framework H2Tune consists of three keycomponents: (i) sparsified triple matrix decomposition to align hiddendimensions across clients through constructing rank-consistent middle matrices,with adaptive sparsification based on client resources; (ii) relation-guidedmatrix layer alignment to handle heterogeneous layer structures andrepresentation capabilities; and (iii) alternating task-knowledgedisentanglement mechanism to decouple shared and specific knowledge of localmodel parameters through alternating optimization. Theoretical analysis provesa convergence rate of O(1/\sqrt{T}). Extensive experiments show our methodachieves up to 15.4% accuracy improvement compared to state-of-the-artbaselines. Our code is available athttps://anonymous.4open.science/r/H2Tune-1407.</description>
      <author>example@mail.com (Wei Guo, Siyuan Lu, Yiqi Tong, Zhaojun Hu, Fuzhen Zhuang, Xiao Zhang, Tao Fan, Jin Dong)</author>
      <guid isPermaLink="false">2507.22633v2</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Automated Label Placement on Maps via Large Language Models</title>
      <link>http://arxiv.org/abs/2507.22952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Workshop on AI for Data Editing (AI4DE) at KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的自动标签放置(ALP)范式，将其作为数据编辑问题，并利用大型语言模型(LLMs)进行上下文感知的空间标注。作者创建了名为MAPLE的基准数据集，评估了四种开源LLMs的性能，结果表明在结构化提示和领域特定检索的引导下，LLMs能够执行准确的空间编辑，生成的输出符合专家制图标准。这项工作为AI辅助地图 finishing提供了可扩展的框架，展示了基础模型在结构化数据编辑任务中的潜力。&lt;h4&gt;背景&lt;/h4&gt;标签放置是地图设计的关键方面，作为空间标注直接影响地图的清晰度和可解释性。尽管其重要性，标签放置仍然主要依赖手动操作，难以规模化，因为现有自动化系统难以整合制图惯例、适应上下文或解释标注指令。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的自动标签放置(ALP)范式，将其作为数据编辑问题，并利用大型语言模型(LLMs)进行上下文感知的空间标注。为此，创建了MAPLE基准数据集，用于评估真实世界地图上的ALP性能。&lt;h4&gt;方法&lt;/h4&gt;方法包括检索与每个地标类型相关的标注指南，利用检索增强生成(RAG)将其整合到提示中，并使用指令微调的LLMs生成理想的标签坐标。作者在MAPLE上评估了四种开源LLMs，分析了整体性能以及在不同类型地标上的泛化能力，包括零样本和指令微调性能。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型在结构化提示和领域特定检索的引导下，能够学习执行准确的空间编辑，使生成的输出与专家制图标准保持一致。&lt;h4&gt;结论&lt;/h4&gt;该工作为AI辅助地图 finishing提供了可扩展的框架，展示了基础模型在结构化数据编辑任务中的潜力。代码和数据可在https://github.com/HarryShomer/MAPLE获取。&lt;h4&gt;翻译&lt;/h4&gt;标签放置是地图设计的关键方面，作为一种空间标注直接影响清晰度和可解释性。尽管其重要性，标签放置仍然主要依赖手动操作，难以规模化，因为现有自动化系统难以整合制图惯例、适应上下文或解释标注指令。在这项工作中，我们引入了一种用于自动标签放置(ALP)的新范式，将任务表述为数据编辑问题，并利用大型语言模型(LLMs)进行上下文感知的空间标注。为支持这一方向，我们创建了MAPLE，这是第一个已知的用于评估真实世界地图上ALP的基准数据集，包含来自开源数据的不同地标类型和标签放置标注。我们的方法利用检索增强生成(RAG)检索与每个地标类型相关的标注指南，将其整合到提示中，并使用指令微调的LLMs生成理想的标签坐标。我们在MAPLE上评估了四种开源LLMs，分析了整体性能以及在不同类型地标上的泛化能力，包括零样本和指令微调性能。我们的结果表明，大型语言模型在结构化提示和领域特定检索的引导下，能够学习执行准确的空间编辑，使生成的输出与专家制图标准保持一致。总体而言，我们的工作为AI辅助地图 finishing提供了可扩展的框架，展示了基础模型在结构化数据编辑任务中的潜力。代码和数据可在https://github.com/HarryShomer/MAPLE找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label placement is a critical aspect of map design, serving as a form ofspatial annotation that directly impacts clarity and interpretability. Despiteits importance, label placement remains largely manual and difficult to scale,as existing automated systems struggle to integrate cartographic conventions,adapt to context, or interpret labeling instructions. In this work, weintroduce a new paradigm for automatic label placement (ALP) that formulatesthe task as a data editing problem and leverages large language models (LLMs)for context-aware spatial annotation. To support this direction, we curateMAPLE, the first known benchmarking dataset for evaluating ALP on real-worldmaps, encompassing diverse landmark types and label placement annotations fromopen-source data. Our method retrieves labeling guidelines relevant to eachlandmark type leveraging retrieval-augmented generation (RAG), integrates theminto prompts, and employs instruction-tuned LLMs to generate ideal labelcoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overallperformance and generalization across different types of landmarks. Thisincludes both zero-shot and instruction-tuned performance. Our resultsdemonstrate that LLMs, when guided by structured prompts and domain-specificretrieval, can learn to perform accurate spatial edits, aligning the generatedoutputs with expert cartographic standards. Overall, our work presents ascalable framework for AI-assisted map finishing and demonstrates the potentialof foundation models in structured data editing tasks. The code and data can befound at https://github.com/HarryShomer/MAPLE.</description>
      <author>example@mail.com (Harry Shomer, Jiejun Xu)</author>
      <guid isPermaLink="false">2507.22952v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Meta CLIP 2: A Worldwide Scaling Recipe</title>
      <link>http://arxiv.org/abs/2507.22062v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Meta CLIP 2，这是第一个从零开始在全球网络规模图像-文本对上训练的CLIP模型，成功解决了多语言CLIP训练中的两大挑战：处理非英语数据点和克服'多语言诅咒'现象。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一种流行的基础模型，支持从零样本分类、检索到多模态大语言模型的编码器。虽然CLIP成功在英语世界的十亿级图像-文本对上训练，但扩展到全球网络数据仍面临挑战：缺乏处理非英语数据的方法，以及现有多语言CLIP的英语性能比仅英语版本差。&lt;h4&gt;目的&lt;/h4&gt;开发一种训练方法，使CLIP能够从全球网络规模的图像-文本对中学习，同时克服多语言训练中的挑战，实现英语和非英语数据的互利共赢。&lt;h4&gt;方法&lt;/h4&gt;提出Meta CLIP 2，从零开始在全球网络规模图像-文本对上训练CLIP。通过严格的消融研究，采用最小必要的变化解决挑战，提出使英语和非英语世界数据互利共赢的方案。&lt;h4&gt;主要发现&lt;/h4&gt;在零样本ImageNet分类中，Meta CLIP 2 ViT-H/14比仅英语版本高0.8%，比mSigLIP高0.7%；在多语言基准测试上设置新最先进水平：CVQA达57.4%，Babel-ImageNet达50.2%，XM3600图像到文本检索达64.3%。&lt;h4&gt;结论&lt;/h4&gt;Meta CLIP 2成功解决了CLIP模型扩展到全球网络数据训练的关键挑战，特别是在多语言环境下的性能优化，为多模态基础模型的进一步发展提供了重要参考。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练(CLIP)是一种流行的基础模型，支持从零样本分类、检索到多模态大语言模型的编码器。尽管CLIP成功在英语世界的十亿级图像-文本对上训练，但进一步扩展训练以从全球网络数据中学习仍面临挑战：(1)缺乏处理非英语数据的方法；(2)现有多语言CLIP的英语性能比仅英语版本差，即'多语言诅咒'现象。我们提出Meta CLIP 2，这是首个从零开始在全球网络规模图像-文本对上训练CLIP的方案。通过严格的消融研究，采用最小必要变化解决上述挑战，提出使英语和非英语数据互利共赢的方案。在零样本ImageNet分类中，Meta CLIP 2 ViT-H/14比仅英语版本高0.8%，比mSigLIP高0.7%，并在多语言基准测试上设置新最先进水平，无系统混淆因素，如CVQA达57.4%，Babel-ImageNet达50.2%，XM3600图像到文本检索达64.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,supporting from zero-shot classification, retrieval to encoders for multimodallarge language models (MLLMs). Although CLIP is successfully trained onbillion-scale image-text pairs from the English world, scaling CLIP's trainingfurther to learning from the worldwide web data is still challenging: (1) nocuration method is available to handle data points from non-English world; (2)the English performance from existing multilingual CLIP is worse than itsEnglish-only counterpart, i.e., "curse of multilinguality" that is common inLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratchon worldwide web-scale image-text pairs. To generalize our findings, we conductrigorous ablations with minimal changes that are necessary to address the abovechallenges and present a recipe enabling mutual benefits from English andnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,and surprisingly sets new state-of-the-art without system-level confoundingfactors (e.g., translation, bespoke architecture changes) on multilingualbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with64.3% on image-to-text retrieval.</description>
      <author>example@mail.com (Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu)</author>
      <guid isPermaLink="false">2507.22062v2</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model</title>
      <link>http://arxiv.org/abs/2507.23595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV25 poster&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MamV2XCalib的基于V2X的基础设施摄像头自动标定方法，利用车载激光雷达实现高效、精确的标定，无需特定参考物体或人工干预。&lt;h4&gt;背景&lt;/h4&gt;随着利用路边摄像头辅助自动驾驶车辆感知的协作系统日益普及，大规模精确标定基础设施摄像头已成为关键问题。传统手动标定方法通常耗时、费力，且可能需要封闭道路。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于V2X的基础设施摄像头自动标定方法，减少对人工干预和特定参考物体的依赖，提高标定效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出MamV2XCalib，首个基于V2X并借助车载激光雷达的基础设施摄像头标定方法。引入一种新的无目标激光雷达-摄像头标定方法，结合多尺度特征和4D相关体积估计车载点云与路边图像的关联性。使用Mamba建模时间信息并估计旋转角度，解决V2X场景中标定失败问题。&lt;h4&gt;主要发现&lt;/h4&gt;在V2X-Seq和TUMTraf-V2X真实世界数据集上评估表明，该方法具有有效性和鲁棒性。与之前为单车标定设计的激光雷达-摄像头方法相比，在V2X场景下用更少参数实现了更好且更稳定的标定性能。&lt;h4&gt;结论&lt;/h4&gt;MamV2XCalib是一种创新的V2X-based基础设施摄像头标定解决方案，仅需配备激光雷达的自动驾驶车辆在待标定摄像头附近行驶即可完成标定，无需特定参考物体或人工干预。&lt;h4&gt;翻译&lt;/h4&gt;随着利用路边摄像头辅助自动驾驶车辆感知的协作系统日益普及，大规模精确标定基础设施摄像头已成为关键问题。传统手动标定方法通常耗时、费力，且可能需要封闭道路。本文提出了MamV2XCalib，这是首个基于V2X并借助车载激光雷达的基础设施摄像头标定方法。MamV2XCalib仅需配备激光雷达的自动驾驶车辆在基础设施中待标定的摄像头附近行驶，无需特定参考物体或人工干预。我们还引入了一种新的无目标激光雷达-摄像头标定方法，结合多尺度特征和4D相关体积来估计车载点云与路边图像之间的相关性。我们使用Mamba建模时间信息并估计旋转角度，有效解决了V2X场景中标定失败问题，这些问题由车载数据缺陷（如遮挡）和视点差异大引起。我们在V2X-Seq和TUMTraf-V2X真实世界数据集上评估了MamV2XCalib，证明了我们基于V2X的自动标定方法的有效性和鲁棒性。与之前为单车标定设计的激光雷达-摄像头方法相比，我们的方法在V2X场景下用更少的参数实现了更好且更稳定的标定性能。代码可在https://github.com/zhuyaoye/MamV2XCalib获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大规模基础设施摄像头的精确标定问题。随着利用路边摄像头辅助自动驾驶车辆感知的协作系统越来越普及，这些摄像头在户外部署容易受到天气和振动影响导致位置偏移，如果不重新标定会显著降低感知系统准确性甚至导致完全失效。高效可扩展的标定方法对稳定的智能交通系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统标定方法耗时耗力和现有自动标定方法依赖特定检测或参考图像的不足，然后创新性地提出反转V2X过程，利用车辆感知数据将整个环境作为参考来标定摄像头。方法借鉴了光流估计(RAFT模型)构建4D相关体积，参考了激光雷达-摄像头标定任务中的特征匹配方法，并利用Mamba架构处理时序信息，但针对V2X场景进行了专门改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用车辆移动过程中收集的多帧激光雷达点云和路边摄像头图像数据，通过时空建模估计路边摄像头的旋转偏差，无需特定参考物体。整体流程包括：1)输入处理，将激光雷达点云投影到摄像头平面生成深度图；2)多尺度特征提取，使用三个分支分别提取图像、深度图和上下文特征；3)特征匹配和迭代细化，构建4D相关体积并迭代更新流场；4)使用Mamba架构进行时空信息聚合；5)回归旋转偏差；6)损失函数计算和模型训练；7)多网络协同推理提高标定精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个基于V2X的无目标基础设施摄像头标定解决方案；2)结合多尺度处理、迭代更新和Mamba架构的新型标定网络；3)在真实数据集上验证了方法有效性。相比传统方法，它完全利用环境数据且无需预准备参考物；相比现有单车辆标定算法，它解决了车辆点云在路边视角下覆盖区域小且不确定的问题，用更少参数实现了更稳定的标定性能；通过时序信息解决了V2X场景中的特殊挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MamV2XCalib首次提出了一种基于V2X的无目标基础设施摄像头标定方法，通过融合车载激光雷达点云与路边摄像头图像，并利用Mamba状态空间模型进行时空建模，实现了高效、自动、鲁棒的路边摄像头旋转偏差标定。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As cooperative systems that leverage roadside cameras to assist autonomousvehicle perception become increasingly widespread, large-scale precisecalibration of infrastructure cameras has become a critical issue. Traditionalmanual calibration methods are often time-consuming, labor-intensive, and mayrequire road closures. This paper proposes MamV2XCalib, the first V2X-basedinfrastructure camera calibration method with the assistance of vehicle-sideLiDAR. MamV2XCalib only requires autonomous vehicles equipped with LiDAR todrive near the cameras to be calibrated in the infrastructure, without the needfor specific reference objects or manual intervention. We also introduce a newtargetless LiDAR-camera calibration method, which combines multi-scale featuresand a 4D correlation volume to estimate the correlation between vehicle-sidepoint clouds and roadside images. We model the temporal information andestimate the rotation angles with Mamba, effectively addressing calibrationfailures in V2X scenarios caused by defects in the vehicle-side data (such asocclusions) and large differences in viewpoint. We evaluate MamV2XCalib on theV2X-Seq and TUMTraf-V2X real-world datasets, demonstrating the effectivenessand robustness of our V2X-based automatic calibration approach. Compared toprevious LiDAR-camera methods designed for calibration on one car, our approachachieves better and more stable calibration performance in V2X scenarios withfewer parameters. The code is available athttps://github.com/zhuyaoye/MamV2XCalib.</description>
      <author>example@mail.com (Yaoye Zhu, Zhe Wang, Yan Wang)</author>
      <guid isPermaLink="false">2507.23595v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction</title>
      <link>http://arxiv.org/abs/2507.23480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FastPoint的新型软件加速技术，用于高效处理大型和不规则的3D点云，通过预测距离曲线加速最远点采样和邻域搜索操作，在保持采样质量和模型性能的同时实现了显著的加速效果。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络已经革新了3D点云处理，但高效处理大型和不规则点云仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决在3D点云处理中高效处理大型和不规则点云的问题。&lt;h4&gt;方法&lt;/h4&gt;引入FastPoint，一种基于软件的加速技术，利用最远点采样过程中采样点之间的可预测距离趋势，通过预测距离曲线来高效识别后续样本点，避免穷举计算所有点对距离。&lt;h4&gt;主要发现&lt;/h4&gt;FastPoint显著加速了最远点采样和邻域搜索操作，同时保持采样质量和模型性能；集成到最先进的3D点云模型中，在NVIDIA RTX 3090 GPU上实现了2.55倍的端到端加速，而不牺牲准确性。&lt;h4&gt;结论&lt;/h4&gt;FastPoint是一种有效的加速技术，可以在不牺牲质量的情况下提高3D点云处理的效率。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络已经革新了3D点云处理，但高效处理大型和不规则点云仍然具有挑战性。为解决这个问题，我们引入了FastPoint，一种新颖的软件加速技术，它利用最远点采样过程中采样点之间的可预测距离趋势。通过预测距离曲线，我们可以有效地识别后续样本点，而不需要穷举计算所有点对距离。我们的提议显著加速了最远点采样和邻域搜索操作，同时保持采样质量和模型性能。通过将FastPoint集成到最先进的3D点云模型中，我们在NVIDIA RTX 3090 GPU上实现了2.55倍的端到端加速，而不会牺牲准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云模型推理过程中的计算效率问题，特别是最远点采样(FPS)和邻域搜索操作的性能瓶颈。这个问题在现实中非常重要，因为3D点云在机器人、自动驾驶等领域变得越来越重要，用于表示和理解3D场景。随着点云数据量的增长和不规则性的增加，计算挑战变得更加显著，限制了实时应用场景的可行性。现有的硬件加速器开发成本高且复杂，而软件解决方案通常以显著精度损失为代价，限制了实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D点云模型中的性能瓶颈，发现FPS和邻域搜索操作占用了大部分计算时间。他们观察到FPS算法有两个可预测的趋势：最小距离递减（随着采样进行，采样点间的最小距离平滑递减）和早期结构捕获（初始采样点往往是点云的极值点）。基于这些观察，作者提出可以通过仅使用几个初始FPS迭代来准确估计距离曲线，从而高效识别后续采样点。作者借鉴了现有硬件加速器（如QuickFPS）和软件加速技术（如RandLA-Net、Grid采样）的思想，但通过预测距离曲线的方式避免了精度损失，同时实现了显著的加速。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; FastPoint的核心思想是利用最远点采样(FPS)过程中采样点之间的可预测距离趋势，通过预测距离曲线来高效识别后续采样点，而无需全面计算所有点对距离。整体实现流程包括：1)最小距离预测采样(MDPS)：使用前10%的FPS迭代结果训练MLP预测器预测剩余距离曲线，将曲线分段，基于排除列表进行采样，并实现提前终止；2)无冗余邻域搜索：直接使用排除列表进行球查询，利用排除列表限制k-NN的搜索空间。这种方法不仅加速了FPS本身，还通过预计算的距离信息使后续的邻域搜索操作受益。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)最小距离预测采样(MDPS)：首次利用FPS过程中的可预测距离趋势，通过MLP预测器估计距离曲线，实现采样和距离计算的解耦；2)无冗余邻域搜索：重用排除列表优化后续操作；3)纯软件加速方案：无需专用硬件即可实现高效推理。相比之前的工作，FastPoint保持了与原始FPS相近的采样质量（超过98%），而其他软件方法通常导致显著精度损失；作为纯软件解决方案，它可以在普通GPU上运行，且可与硬件加速方法结合使用，实现更大的加速。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FastPoint通过预测最远点采样过程中的距离曲线，实现了3D点云模型的高效推理，在保持模型准确性的同时实现了2.55倍的端到端加速，为点云处理提供了一种实用且高效的软件解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks have revolutionized 3D point cloud processing, yetefficiently handling large and irregular point clouds remains challenging. Totackle this problem, we introduce FastPoint, a novel software-basedacceleration technique that leverages the predictable distance trend betweensampled points during farthest point sampling. By predicting the distancecurve, we can efficiently identify subsequent sample points withoutexhaustively computing all pairwise distances. Our proposal substantiallyaccelerates farthest point sampling and neighbor search operations whilepreserving sampling quality and model performance. By integrating FastPointinto state-of-the-art 3D point cloud models, we achieve 2.55x end-to-endspeedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.</description>
      <author>example@mail.com (Donghyun Lee, Dawoon Jeong, Jae W. Lee, Hongil Yoon)</author>
      <guid isPermaLink="false">2507.23480v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors</title>
      <link>http://arxiv.org/abs/2507.21872v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MultiEditor是一种双分支潜在扩散框架，用于共同编辑驾驶场景中的图像和LiDAR点云，通过引入3D高斯溅射作为先验，并设计多级别外观控制机制和深度引导的可变形跨模态条件模块，有效解决了自动驾驶系统中多模态感知的长尾分布问题，提高了对稀有车辆类别的感知能力。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统依赖多模态感知数据来理解复杂环境，但真实世界数据的长尾分布阻碍了泛化能力，特别是对于稀有但安全关键的车辆类别。&lt;h4&gt;目的&lt;/h4&gt;解决数据长尾分布导致的泛化问题，提高对稀有但安全关键的车辆类别的感知能力。&lt;h4&gt;方法&lt;/h4&gt;提出MultiEditor双分支潜在扩散框架，引入3D高斯溅射作为目标对象的结构和外观先验，设计多级别外观控制机制（包括像素级粘贴、语义级指导和多分支细化），以及深度引导的可变形跨模态条件模块，实现图像和LiDAR点云的联合编辑。&lt;h4&gt;主要发现&lt;/h4&gt;MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现出色；使用MultiEditor生成稀有类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;结论&lt;/h4&gt;MultiEditor能够有效解决自动驾驶系统中多模态感知的长尾分布问题，通过联合编辑图像和LiDAR点云，提高了对稀有车辆类别的感知能力。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶系统严重依赖多模态感知数据来理解复杂环境。然而，真实世界数据的长尾分布阻碍了泛化能力，特别是对于稀有但安全关键的车辆类别。为应对这一挑战，我们提出了MultiEditor，一种双分支潜在扩散框架，专为共同编辑驾驶场景中的图像和LiDAR点云而设计。我们方法的核心是引入3D高斯溅射作为目标对象的结构和外观先验。利用这一先验，我们设计了一个多级别外观控制机制，包括像素级粘贴、语义级指导和多分支细化，以实现跨模态的高保真重建。我们进一步提出了一个深度引导的可变形跨模态条件模块，使用3DGS渲染的深度自适应地实现模态间的相互引导，显著增强了跨模态一致性。大量实验表明，MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现出色。此外，使用MultiEditor生成稀有类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶系统中多模态数据（图像和LiDAR点云）编辑的长尾分布问题，特别是罕见但安全关键的车辆类别（如压路机、挖掘机）数据不足的问题。这个问题在现实中很重要，因为数据不平衡导致感知模型在边缘情况下的鲁棒性不足，而自动驾驶系统需要准确识别各种类型的车辆以确保安全性。现有方法大多是单模态编辑，缺乏跨模态一致性，难以精确控制对象姿态和罕见类别编辑。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶场景中多模态数据编辑的挑战，特别是长尾数据分布问题。他们认识到现有单模态方法的局限性，需要一种能同时编辑图像和LiDAR点云并保持一致性的方法。设计上，作者借鉴了潜在扩散模型(LDMs)、3D高斯散射(3DGS)、图像编辑技术(如Paint-by-Example和AnyDoor)、点云处理方法(如RangeLDM)以及跨模态交互机制(如X-Drive)。核心思路是利用3DGS作为统一先验，设计双分支扩散框架，通过多层次外观控制和跨模态条件模块确保一致性和高质量编辑。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D高斯散射(3DGS)作为目标对象的结构和外观统一先验，设计双分支潜在扩散框架同时处理图像和LiDAR点云，并通过多层次外观控制和跨模态条件模块确保一致性和高质量。整体流程包括：1)数据准备(从KITTI构建训练数据，处理阴影问题)；2)双分支框架(图像分支使用像素级保留、语义级引导和多分支优化；点云分支针对范围图像特性定制)；3)跨模态条件模块(利用3DGS渲染的深度图建立对齐，通过可变形交叉注意力实现双向条件传递)；4)五阶段训练策略；5)推理过程(给定目标对象和场景掩码，生成编辑后的多模态数据)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合3DGS作为统一先验的多模态编辑框架；2)多层次外观控制机制(像素级保留、语义级引导、多分支优化)；3)深度引导的可变形跨模态条件模块；4)支持罕见车辆类型的编辑。相比之前工作的不同：1)实现了图像和LiDAR点云的联合编辑而非单模态；2)使用3DGS而非数据集派生先验；3)通过专门模块确保跨模态一致性；4)提供更高的编辑可控性和罕见类别处理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MultiEditor通过整合3D高斯散射作为统一先验，实现了自动驾驶场景中图像和LiDAR点云的高保真、一致且可控的联合编辑，显著提升了感知模型在罕见车辆类别上的检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving systems rely heavily on multimodal perception data tounderstand complex environments. However, the long-tailed distribution ofreal-world data hinders generalization, especially for rare but safety-criticalvehicle categories. To address this challenge, we propose MultiEditor, adual-branch latent diffusion framework designed to edit images and LiDAR pointclouds in driving scenarios jointly. At the core of our approach is introducing3D Gaussian Splatting (3DGS) as a structural and appearance prior for targetobjects. Leveraging this prior, we design a multi-level appearance controlmechanism--comprising pixel-level pasting, semantic-level guidance, andmulti-branch refinement--to achieve high-fidelity reconstruction acrossmodalities. We further propose a depth-guided deformable cross-modalitycondition module that adaptively enables mutual guidance between modalitiesusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.Extensive experiments demonstrate that MultiEditor achieves superiorperformance in visual and geometric fidelity, editing controllability, andcross-modality consistency. Furthermore, generating rare-category vehicle datawith MultiEditor substantially enhances the detection accuracy of perceptionmodels on underrepresented classes.</description>
      <author>example@mail.com (Shouyi Lu, Zihan Lin, Chao Lu, Huanran Wang, Guirong Zhuo, Lianqing Zheng)</author>
      <guid isPermaLink="false">2507.21872v3</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings</title>
      <link>http://arxiv.org/abs/2507.22802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the MICCAI 2025 MIRASOL Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于FetalCLIP模型的自动化胎儿超声图像质量评估方法，帮助解决资源有限地区产前护理中超声医师缺乏的问题。&lt;h4&gt;背景&lt;/h4&gt;准确的胎儿生物测量（如腹围）在产前护理中至关重要，但高质量超声图像获取高度依赖超声医师专业技能，这在低收入国家构成挑战，因缺乏训练有素的人员。&lt;h4&gt;目的&lt;/h4&gt;开发自动化胎儿超声图像质量评估系统，以解决资源有限地区产前护理中专业超声医师不足的问题。&lt;h4&gt;方法&lt;/h4&gt;利用在21万对胎儿超声图像-标题数据集上预训练的FetalCLIP视觉语言模型，使用低秩适应(LoRA)技术开发FetalCLIP$_{CLS}$模型，在ACOUSLIC-AI数据集上与六个CNN和Transformer基线进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;FetalCLIP$_{CLS}$模型达到0.757的最高F1分数，而改编的分割模型用于分类时进一步提高性能，达到0.771的F1分数。&lt;h4&gt;结论&lt;/h4&gt;胎儿超声基础模型的参数高效微调可实现任务特定适应，推进资源有限环境中的产前护理。&lt;h4&gt;翻译&lt;/h4&gt;准确的胎儿生物测量，如腹围，在产前护理中起着至关重要的作用。然而，获取这些测量所需的高质量超声图像高度依赖于超声医师的专业技能，这在低收入国家构成了重大挑战，因为缺乏训练有素的人员。为解决这个问题，我们利用FetalCLIP，一种在精选的21万多对胎儿超声图像-标题数据集上预训练的视觉语言模型，对盲扫超声数据进行自动化的胎儿超声图像质量评估(IQA)。我们引入了FetalCLIP$_{CLS}$，这是使用低秩适应(LoRA)从FetalCLIP改编的IQA模型，并在ACOUSLIC-AI数据集上与六个CNN和Transformer基线进行了评估。FetalCLIP$_{CLS}$获得了最高的0.757 F1分数。此外，我们表明改编的分割模型在重新用于分类时可以进一步提高性能，达到0.771的F1分数。我们的工作展示了如何通过胎儿超声基础模型的参数高效微调实现任务特定的适应，从而推进资源有限环境中的产前护理。实验代码可在以下网址获取：https://github.com/donglihe-hub/FetalCLIP-IQA。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate fetal biometric measurements, such as abdominal circumference, playa vital role in prenatal care. However, obtaining high-quality ultrasoundimages for these measurements heavily depends on the expertise of sonographers,posing a significant challenge in low-income countries due to the scarcity oftrained personnel. To address this issue, we leverage FetalCLIP, avision-language model pretrained on a curated dataset of over 210,000 fetalultrasound image-caption pairs, to perform automated fetal ultrasound imagequality assessment (IQA) on blind-sweep ultrasound data. We introduceFetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-RankAdaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNNand Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of0.757. Moreover, we show that an adapted segmentation model, when repurposedfor classification, further improves performance, achieving an F1 score of0.771. Our work demonstrates how parameter-efficient fine-tuning of fetalultrasound foundation models can enable task-specific adaptations, advancingprenatal care in resource-limited settings. The experimental code is availableat: https://github.com/donglihe-hub/FetalCLIP-IQA.</description>
      <author>example@mail.com (Dongli He, Hu Wang, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2507.22802v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
  <item>
      <title>Quantifying surprise in clinical care: Detecting highly informative events in electronic health records with foundation models</title>
      <link>http://arxiv.org/abs/2507.22798v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于基础模型的方法，用于识别电子健康记录中信息量大的标记和事件，该方法能够考虑患者住院的整体背景，标记出基于规则方法可能忽略的异常事件。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录中包含大量数据，需要有效识别信息量大的标记和事件，而传统基于规则的方法可能无法捕捉到在整体背景下的异常情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于基础模型的方法，能够识别电子健康记录中高度信息化的标记和事件，并提高对预后模型预测结果的理解。&lt;h4&gt;方法&lt;/h4&gt;提出了一种考虑患者住院整体背景的基础模型衍生方法，能够标记出基于规则方法可能认为是正常范围内的异常事件。&lt;h4&gt;主要发现&lt;/h4&gt;模型标记的事件对预测患者后续结果具有重要意义；被认为信息量小的一部分事件可以安全地被丢弃；信息量指标有助于解释在基础模型派生表示上训练的预后模型的预测结果。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的方法能够有效识别电子健康记录中的重要信息，同时提高预后模型的解释性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于基础模型的方法，用于识别电子健康记录中信息量大的标记和事件。我们的方法考虑患者住院的整体背景，因此可以标记出基于规则方法可能认为是正常范围内的异常事件。我们证明了模型标记的事件对预测患者后续结果具有重要意义，并且被识别为信息量小的一部分事件可以安全地被丢弃。此外，我们还展示了信息量如何有助于解释在基础模型派生表示上训练的预后模型的预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a foundation model-derived method to identify highly informativetokens and events in electronic health records. Our approach considers incomingdata in the entire context of a patient's hospitalization and so can flaganomalous events that rule-based approaches would consider within a normalrange. We demonstrate that the events our model flags are significant forpredicting downstream patient outcomes and that a fraction of events identifiedas carrying little information can safely be dropped. Additionally, we show howinformativeness can help interpret the predictions of prognostic models trainedon foundation model-derived representations.</description>
      <author>example@mail.com (Michael C. Burkhart, Bashar Ramadan, Luke Solo, William F. Parker, Brett K. Beaulieu-Jones)</author>
      <guid isPermaLink="false">2507.22798v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future</title>
      <link>http://arxiv.org/abs/2507.22792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  45 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了基于SAM/SAM2基础模型在视频对象分割与跟踪(VOST)领域的应用，从过去、现在和未来三个时间维度分析了相关方法的发展。&lt;h4&gt;背景&lt;/h4&gt;视频对象分割与跟踪是计算机视觉中复杂而关键的问题，传统方法在领域泛化、时间一致性和计算效率方面存在困难。基础模型如SAM和SAM2的出现带来了范式转变，实现了提示驱动的分割和强大的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提供对基于SAM/SAM2的VOST方法的全面综述，按过去、现在和未来三个时间维度结构化组织内容，指导研究人员和实践者推进VOST领域的发展。&lt;h4&gt;方法&lt;/h4&gt;分析保留和更新历史信息的策略(过去)，研究从当前帧提取和优化判别性特征的方法(现在)，探索预测对象动态和轨迹估计的机制(未来)，并讨论最近的创新如运动感知内存选择和轨迹引导提示。&lt;h4&gt;主要发现&lt;/h4&gt;从早期基于内存的架构到SAM2的流式内存和实时分割能力展示了技术进步；创新方法如运动感知内存选择和轨迹引导提示提高了准确性和效率；基础模型为VOST带来了新的可能性。&lt;h4&gt;结论&lt;/h4&gt;当前VOST领域仍面临内存冗余、误差累积和提示效率低下等挑战；论文提出了未来研究的方向；该综述提供了该领域的及时和结构化概述，旨在通过基础模型的视角推进VOST的发展。&lt;h4&gt;翻译&lt;/h4&gt;视频对象分割与跟踪(VOST)在计算机视觉中提出了复杂而关键的挑战，需要在时间动态帧上稳健地整合分割和跟踪。传统方法在领域泛化、时间一致性和计算效率方面一直存在困难。基础模型如Segment Anything Model (SAM)及其后续版本SAM2的出现引入了范式转变，实现了具有强大泛化能力的提示驱动分割。基于这些进展，本综述对基于SAM/SAM2的VOST方法进行了全面回顾，沿着过去、现在和未来三个时间维度进行结构化组织。我们检验了保留和更新历史信息的策略(过去)，从当前帧提取和优化判别性特征的方法(现在)，以及预测后续帧中对象动态和轨迹估计的机制(未来)。通过这样做，我们突出了从早期基于内存的架构到SAM2的流式内存和实时分割能力的演变。我们还讨论了最近的创新，如运动感知内存选择和轨迹引导提示，这些创新旨在提高准确性和效率。最后，我们确定了剩余的挑战，包括内存冗余、误差累积和提示效率低下，并建议了未来研究的有前途方向。本综述及时且结构化地概述了该领域，旨在指导研究人员和实践者通过基础模型的视角推进VOST的当前状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Object Segmentation and Tracking (VOST) presents a complex yet criticalchallenge in computer vision, requiring robust integration of segmentation andtracking across temporally dynamic frames. Traditional methods have struggledwith domain generalization, temporal consistency, and computational efficiency.The emergence of foundation models like the Segment Anything Model (SAM) andits successor, SAM2, has introduced a paradigm shift, enabling prompt-drivensegmentation with strong generalization capabilities. Building upon theseadvances, this survey provides a comprehensive review of SAM/SAM2-based methodsfor VOST, structured along three temporal dimensions: past, present, andfuture. We examine strategies for retaining and updating historical information(past), approaches for extracting and optimizing discriminative features fromthe current frame (present), and motion prediction and trajectory estimationmechanisms for anticipating object dynamics in subsequent frames (future). Indoing so, we highlight the evolution from early memory-based architectures tothe streaming memory and real-time segmentation capabilities of SAM2. We alsodiscuss recent innovations such as motion-aware memory selection andtrajectory-guided prompting, which aim to enhance both accuracy and efficiency.Finally, we identify remaining challenges including memory redundancy, erroraccumulation, and prompt inefficiency, and suggest promising directions forfuture research. This survey offers a timely and structured overview of thefield, aiming to guide researchers and practitioners in advancing the state ofVOST through the lens of foundation models.</description>
      <author>example@mail.com (Guoping Xu, Jayaram K. Udupa, Yajun Yu, Hua-Chieh Shao, Songlin Zhao, Wei Liu, You Zhang)</author>
      <guid isPermaLink="false">2507.22792v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Image Anomaly Detection Using Generative Foundation Models</title>
      <link>http://arxiv.org/abs/2507.22692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the workshop of Anomaly Detection with Foundation Models,  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于扩散模型的分布外检测新方法，利用去噪扩散模型作为通用感知模板，通过分析Stein分数误差实现无需重新训练的异常检测。&lt;h4&gt;背景&lt;/h4&gt;在开放世界环境中部署安全的视觉系统需要有效的分布外输入检测能力。&lt;h4&gt;目的&lt;/h4&gt;探索扩散模型作为分布外检测工具的可能性，开发一种无需针对每个目标数据集重新训练的异常检测方法。&lt;h4&gt;方法&lt;/h4&gt;利用去噪扩散模型（DDMs）的去噪轨迹作为纹理和语义信息来源，通过分析通过结构相似性指数度量（SSIM）放大的Stein分数误差来识别异常样本。&lt;h4&gt;主要发现&lt;/h4&gt;仅在CelebA数据集上训练的单个模型即可实现优异性能，甚至超过在ImageNet等更常用数据集上的训练效果；在某些基准测试上接近完美性能，同时显示有进一步改进的空间。&lt;h4&gt;结论&lt;/h4&gt;生成基础模型在异常检测领域具有显著优势和巨大潜力，为开放世界环境中的安全视觉系统提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;检测分布外（OOD）输入对于在开放世界环境中部署安全的视觉系统至关重要。我们重新审视扩散模型，不是作为生成器，而是作为OOD检测的通用感知模板。本研究探索了使用基于分数的生成模型作为在未见过的数据集上进行语义异常检测的基础工具。具体来说，我们利用去噪扩散模型（DDMs）的去噪轨迹作为丰富的纹理和语义信息来源。通过分析通过结构相似性指数度量（SSIM）放大的Stein分数误差，我们引入了一种无需在每个目标数据集上重新训练即可识别异常样本的新方法。我们的方法优于最先进的技术，只需要在一个数据集（CelebA）上训练单个模型，我们发现这是一个有效的基础分布，甚至在某些设置中优于更常用的ImageNet数据集。实验结果显示在一些基准测试上接近完美性能，而在其他测试上有显著提升空间，突显了生成基础模型在异常检测中的优势和未来潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting out-of-distribution (OOD) inputs is pivotal for deploying safevision systems in open-world environments. We revisit diffusion models, not asgenerators, but as universal perceptual templates for OOD detection. Thisresearch explores the use of score-based generative models as foundationaltools for semantic anomaly detection across unseen datasets. Specifically, weleverage the denoising trajectories of Denoising Diffusion Models (DDMs) as arich source of texture and semantic information. By analyzing Stein scoreerrors, amplified through the Structural Similarity Index Metric (SSIM), weintroduce a novel method for identifying anomalous samples without requiringre-training on each target dataset. Our approach improves over state-of-the-artand relies on training a single model on one dataset -- CelebA -- which we findto be an effective base distribution, even outperforming more commonly useddatasets like ImageNet in several settings. Experimental results shownear-perfect performance on some benchmarks, with notable headroom on others,highlighting both the strength and future potential of generative foundationmodels in anomaly detection.</description>
      <author>example@mail.com (Lemar Abdi, Amaan Valiuddin, Francisco Caetano, Christiaan Viviers, Fons van der Sommen)</author>
      <guid isPermaLink="false">2507.22692v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2507.22675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MergeSAM的创新性无监督变化检测方法，基于Segment Anything Model (SAM)，用于高分辨率遥感影像的变化检测。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型在大量数据集上训练，在特征提取和通用特征表示方面表现出色；深度学习驱动的大型模型不断发展，为加速无监督变化检测方法提供了巨大潜力，增强了变化检测技术的实际应用性。&lt;h4&gt;目的&lt;/h4&gt;引入MergeSAM，一种基于Segment Anything Model (SAM)的创新性无监督变化检测方法，用于高分辨率遥感影像的变化检测。&lt;h4&gt;方法&lt;/h4&gt;设计了两种新策略：Mask Matching和Mask Splitting，解决现实世界中的复杂性问题如对象分割、合并等；充分利用SAM的对象分割能力构建多时相掩膜，捕获复杂变化，并将土地覆盖的空间结构嵌入变化检测过程。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体研究结果或发现。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及结论。&lt;h4&gt;翻译&lt;/h4&gt;最近，在大量数据集上训练的大型基础模型在特征提取和通用特征表示方面表现出色。深度学习驱动的大型模型的持续进展在加速无监督变化检测方法方面显示出巨大潜力，从而增强了变化检测技术的实际应用性。基于这一进展，本文引入了MergeSAM，这是一种基于Segment Anything Model (SAM)的高分辨率遥感影像的创新性无监督变化检测方法。设计了两种新策略：Mask Matching和Mask Splitting，以解决现实世界中的复杂性，如对象分割、合并和其他复杂变化。所提出的方法充分利用了SAM的对象分割能力来构建捕获复杂变化的多时相掩膜，将土地覆盖的空间结构嵌入到变化检测过程中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, large foundation models trained on vast datasets have demonstratedexceptional capabilities in feature extraction and general featurerepresentation. The ongoing advancements in deep learning-driven large modelshave shown great promise in accelerating unsupervised change detection methods,thereby enhancing the practical applicability of change detection technologies.Building on this progress, this paper introduces MergeSAM, an innovativeunsupervised change detection method for high-resolution remote sensingimagery, based on the Segment Anything Model (SAM). Two novel strategies,MaskMatching and MaskSplitting, are designed to address real-world complexitiessuch as object splitting, merging, and other intricate changes. The proposedmethod fully leverages SAM's object segmentation capabilities to constructmultitemporal masks that capture complex changes, embedding the spatialstructure of land cover into the change detection process.</description>
      <author>example@mail.com (Meiqi Hu, Lingzhi Lu, Chengxi Han, Xiaoping Liu)</author>
      <guid isPermaLink="false">2507.22675v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity</title>
      <link>http://arxiv.org/abs/2507.22633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为H2Tune的联邦基础模型微调方法，用于解决混合异构联邦微调场景下的双重异构性问题，通过创新的矩阵处理和知识解耦机制，显著提升了联邦学习中的模型性能。&lt;h4&gt;背景&lt;/h4&gt;现有的联邦微调方法主要针对基础模型，但混合异构联邦微调是一个尚未充分探索的场景，其中客户端在模型架构和下游任务上表现出双重异构性。这种混合异构性带来了异构矩阵聚合和多任务知识干扰两个主要挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够处理客户端在模型架构和下游任务上双重异构性的联邦基础模型微调方法，解决异构矩阵聚合和多任务知识干扰问题，提高联邦学习中的模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出H2Tune框架，包含三个关键组件：1)稀疏三重矩阵分解，通过构建秩一致的中介矩阵来对齐不同客户端的隐藏维度，并根据客户端资源进行自适应稀疏化；2)关系引导的矩阵层对齐，处理异构层结构和表示能力；3)交替任务知识解缠机制，通过交替优化来解耦本地模型参数的共享和特定知识。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析证明了方法的收敛率为O(1/√T)。大量实验表明，与最先进的基线方法相比，该方法实现了高达15.4%的准确率提升。&lt;h4&gt;结论&lt;/h4&gt;H2Tune有效解决了混合异构联邦微调场景下的双重异构性问题，通过创新的矩阵处理和知识解耦机制，显著提升了联邦学习中的模型性能。&lt;h4&gt;翻译&lt;/h4&gt;与现有的基础模型联邦微调方法不同，混合异构联邦微调是一个尚未充分探索的场景，其中客户端在模型架构和下游任务上表现出双重异构性。这种混合异构性带来了两个重大挑战：1)异构矩阵聚合，客户端根据其任务需求和资源限制采用不同的大规模基础模型，导致在LoRA参数聚合过程中出现维度不匹配；2)多任务知识干扰，本地共享参数同时使用任务共享和任务特定知识进行训练，无法确保只在客户端之间传输任务共享知识。为解决这些挑战，我们提出了H2Tune，一种处理混合异构性的联邦基础模型微调方法。我们的框架H2Tune包含三个关键组件：(i)稀疏三重矩阵分解，通过构建秩一致的中介矩阵来对齐不同客户端的隐藏维度，并根据客户端资源进行自适应稀疏化；(ii)关系引导的矩阵层对齐，处理异构层结构和表示能力；(iii)交替任务知识解缠机制，通过交替优化来解耦本地模型参数的共享和特定知识。理论分析证明了O(1/√T)的收敛率。大量实验表明，与最先进的基线方法相比，我们的方法实现了高达15.4%的准确率提升。我们的代码可在https://anonymous.4open.science/r/H2Tune-1407获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Different from existing federated fine-tuning (FFT) methods for foundationmodels, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-exploredscenario where clients exhibit double heterogeneity in model architectures anddownstream tasks. This hybrid heterogeneity introduces two significantchallenges: 1) heterogeneous matrix aggregation, where clients adopt differentlarge-scale foundation models based on their task requirements and resourcelimitations, leading to dimensional mismatches during LoRA parameteraggregation; and 2) multi-task knowledge interference, where local sharedparameters, trained with both task-shared and task-specific knowledge, cannotensure only task-shared knowledge is transferred between clients. To addressthese challenges, we propose H2Tune, a federated foundation model fine-tuningwith hybrid heterogeneity. Our framework H2Tune consists of three keycomponents: (i) sparsified triple matrix decomposition to align hiddendimensions across clients through constructing rank-consistent middle matrices,with adaptive sparsification based on client resources; (ii) relation-guidedmatrix layer alignment to handle heterogeneous layer structures andrepresentation capabilities; and (iii) alternating task-knowledgedisentanglement mechanism to decouple shared and specific knowledge of localmodel parameters through alternating optimization. Theoretical analysis provesa convergence rate of O(1/\sqrt{T}). Extensive experiments show our methodachieves up to 15.4% accuracy improvement compared to state-of-the-artbaselines. Our code is available athttps://anonymous.4open.science/r/H2Tune-1407.</description>
      <author>example@mail.com (Wei Guo, Siyuan Lu, Yiqi Tong, Zhaojun Hu, Fuzhen Zhuang, Xiao Zhang, Tao Fan, Jin Dong)</author>
      <guid isPermaLink="false">2507.22633v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning</title>
      <link>http://arxiv.org/abs/2507.22604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为基于快捷方式的微调(ShortFT)的高效策略，通过利用较短的去噪链来改进扩散模型与奖励函数的对齐效果。&lt;h4&gt;背景&lt;/h4&gt;基于反向传播的方法试图通过去噪链中的端到端奖励梯度反向传播使扩散模型与奖励函数保持一致，这种方法有前景但面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法因计算成本高和去噪链过长导致的梯度爆炸问题，实现更完整的梯度反向传播。&lt;h4&gt;方法&lt;/h4&gt;采用轨迹保持少步扩散模型构建基于快捷方式的较短去噪链，并通过在这个链上进行优化来提高基础模型微调的效率和效果。&lt;h4&gt;主要发现&lt;/h4&gt;ShortFT方法经过严格测试，可有效应用于各种奖励函数，显著提高对齐性能，并超越现有的最先进替代方法。&lt;h4&gt;结论&lt;/h4&gt;基于快捷方式的微调策略解决了传统方法中的计算和梯度问题，为扩散模型与奖励函数的高效对齐提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;基于反向传播的方法旨在通过去噪链中的端到端奖励梯度反向传播，使扩散模型与奖励函数保持一致，提供了有前景的视角。然而，由于去噪链过长带来的计算成本和梯度爆炸风险，现有方法难以实现完整的梯度反向传播，导致次优结果。在本文中，我们引入了基于快捷方式的微调(ShortFT)，一种利用较短去噪链的高效微调策略。更具体地说，我们采用最近研究的轨迹保持少步扩散模型，它可以在原始去噪链上创建快捷方式，并构建了基于快捷方式的较短去噪链。在这个链上的优化显著提高了基础模型微调的效率和效果。我们的方法经过严格测试，可有效应用于各种奖励函数，显著提高了对齐性能，超越了最先进的替代方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backpropagation-based approaches aim to align diffusion models with rewardfunctions through end-to-end backpropagation of the reward gradient within thedenoising chain, offering a promising perspective. However, due to thecomputational costs and the risk of gradient explosion associated with thelengthy denoising chain, existing approaches struggle to achieve completegradient backpropagation, leading to suboptimal results. In this paper, weintroduce Shortcut-based Fine-Tuning (ShortFT), an efficient fine-tuningstrategy that utilizes the shorter denoising chain. More specifically, weemploy the recently researched trajectory-preserving few-step diffusion model,which enables a shortcut over the original denoising chain, and construct ashortcut-based denoising chain of shorter length. The optimization on thischain notably enhances the efficiency and effectiveness of fine-tuning thefoundational model. Our method has been rigorously tested and can beeffectively applied to various reward functions, significantly improvingalignment performance and surpassing state-of-the-art alternatives.</description>
      <author>example@mail.com (Xiefan Guo, Miaomiao Cui, Liefeng Bo, Di Huang)</author>
      <guid isPermaLink="false">2507.22604v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework</title>
      <link>http://arxiv.org/abs/2507.22481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, accepted by ACMMM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种盲比特流损坏视频恢复框架，集成了视觉基础模型和恢复模型，能够适应不同类型的损坏和比特流级别的提示。同时提出了检测任何损坏(DAC)模型和损坏感知特征完成(CFC)模块，实现了无需手动标注掩码序列的高效视频恢复。&lt;h4&gt;背景&lt;/h4&gt;视频信号在多媒体通信和存储系统中容易受到损坏，即使轻微的比特流域损坏也会导致显著的像素域退化。现有方法需要对每个损坏的视频帧进行耗时且繁琐的损坏区域标注，造成大量工作负担。此外，损坏帧中的部分局部残差信息可能会误导特征完成和后续内容恢复，使得高质量恢复仍然困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需手动标注损坏区域掩码序列的比特流损坏视频恢复方法，解决现有方法需要大量标注工作以及高质量恢复困难的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了第一个盲比特流损坏视频恢复框架，集成了视觉基础模型和恢复模型；2. 提出了检测任何损坏(DAC)模型，利用视觉基础模型的丰富先验知识增强损坏定位和盲恢复；3. 引入了损坏感知特征完成(CFC)模块，基于高级损坏理解自适应处理残差贡献；4. 使用视觉基础模型引导的分层特征增强和混合残差专家(MoRE)结构抑制伪影并增强信息残差。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所提出的方法在比特流损坏视频恢复任务中取得了优异的性能；2. 该方法不需要手动标注的掩码序列；3. 该方法的有效性有助于改善用户体验，扩大应用场景，并实现更可靠的多媒体通信和存储系统。&lt;h4&gt;结论&lt;/h4&gt;通过整合视觉基础模型与专门的恢复模型，结合创新的DAC和CFC模块，所提出的框架能够有效地处理各种类型的比特流损坏，实现高质量的视频内容恢复，同时显著减少了人工标注的需求。&lt;h4&gt;翻译&lt;/h4&gt;视频信号在多媒体通信和存储系统中容易受到损坏，因为即使是轻微的比特流域损坏也会导致显著的像素域退化。为了从损坏的输入中恢复忠实的时间和空间内容，比特流损坏视频恢复最近已成为一个具有挑战性且研究不足的任务。然而，现有方法需要对每个损坏的视频帧进行耗时且繁琐的损坏区域标注，导致在实际应用中工作量很大。此外，由于损坏帧中的一部分局部残差信息可能会误导特征完成和后续内容恢复，高质量恢复仍然困难。在本文中，我们提出了第一个盲比特流损坏视频恢复框架，该框架集成了视觉基础模型和恢复模型，该模型适用于不同类型的损坏和比特流级别的提示。在该框架内，所提出的检测任何损坏(DAC)模型利用视觉基础模型的丰富先验，同时结合比特流和损坏知识来增强损坏定位和盲恢复。此外，我们引入了一种新的损坏感知特征完成(CFC)模块，该模块基于高级损坏理解自适应处理残差贡献。通过视觉基础模型引导的分层特征增强和混合残差专家(MoRE)结构中的高级协调，我们的方法抑制了伪影并增强了信息残差。全面的评估表明，所提出的方法在比特流损坏视频恢复中取得了出色的性能，而无需手动标注的掩码序列。所展示的有效性将有助于实现改善的用户体验、更广泛的应用场景和更可靠的多媒体通信和存储系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video signals are vulnerable in multimedia communication and storage systems,as even slight bitstream-domain corruption can lead to significant pixel-domaindegradation. To recover faithful spatio-temporal content from corrupted inputs,bitstream-corrupted video recovery has recently emerged as a challenging andunderstudied task. However, existing methods require time-consuming andlabor-intensive annotation of corrupted regions for each corrupted video frame,resulting in a large workload in practice. In addition, high-quality recoveryremains difficult as part of the local residual information in corrupted framesmay mislead feature completion and successive content recovery. In this paper,we propose the first blind bitstream-corrupted video recovery framework thatintegrates visual foundation models with a recovery model, which is adapted todifferent types of corruption and bitstream-level prompts. Within theframework, the proposed Detect Any Corruption (DAC) model leverages the richpriors of the visual foundation model while incorporating bitstream andcorruption knowledge to enhance corruption localization and blind recovery.Additionally, we introduce a novel Corruption-aware Feature Completion (CFC)module, which adaptively processes residual contributions based on high-levelcorruption understanding. With VFM-guided hierarchical feature augmentation andhigh-level coordination in a mixture-of-residual-experts (MoRE) structure, ourmethod suppresses artifacts and enhances informative residuals. Comprehensiveevaluations show that the proposed method achieves outstanding performance inbitstream-corrupted video recovery without requiring a manually labeled masksequence. The demonstrated effectiveness will help to realize improved userexperience, wider application scenarios, and more reliable multimediacommunication and storage systems.</description>
      <author>example@mail.com (Tianyi Liu, Kejun Wu, Chen Cai, Yi Wang, Kim-Hui Yap, Lap-Pui Chau)</author>
      <guid isPermaLink="false">2507.22481v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Visual Language Models as Zero-Shot Deepfake Detectors</title>
      <link>http://arxiv.org/abs/2507.22469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the ICML 2025 Workshop on Reliable and Responsible  Foundation Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于视觉语言模型（VLM）的新型深度伪造检测方法，利用VLM的零样本能力，在包含60,000张图像的高质量数据集上取得了优于传统方法的性能。&lt;h4&gt;背景&lt;/h4&gt;深度伪造技术利用GAN或扩散模型进行面部替换，对数字媒体、身份验证等多个领域构成重大威胁。现有检测方法主要依赖训练专门分类器区分真实和操纵图像，仅关注图像域且缺乏辅助任务以提高鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;受视觉语言模型零样本能力启发，提出一种新型VLM-based图像分类方法并评估其在深度伪造检测中的应用效果。&lt;h4&gt;方法&lt;/h4&gt;利用包含60,000张图像的高质量深度伪造数据集，评估零样本模型的性能；比较最佳架构InstructBLIP在流行数据集DFDC-P上的表现，分别在零样本和领域内微调两种场景下与传统方法进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;零样本模型在60,000张图像的数据集上表现出优于几乎所有现有方法的性能；VLMs在深度伪造检测中显著优于传统分类器。&lt;h4&gt;结论&lt;/h4&gt;视觉语言模型在深度伪造检测中展现出优越性能，无论是在零样本场景还是领域内微调场景下都优于传统分类方法。&lt;h4&gt;翻译&lt;/h4&gt;当代深度伪造现象利用GAN或扩散模型进行面部替换，对数字媒体、身份验证和众多其他系统构成了重大且不断演变的威胁。大多数现有的深度伪造检测方法依赖于训练专门的分类器来区分真实和操纵的图像，仅专注于图像域而不包含任何能提高鲁棒性的辅助任务。在本文中，受视觉语言模型的零样本能力启发，我们提出了一种新型的基于VLM的图像分类方法，并评估了其在深度伪造检测中的应用。具体而言，我们利用了一个包含60,000张图像的新高质量深度伪造数据集，我们的零样本模型在该数据集上表现出优于几乎所有现有方法的性能。随后，我们在流行的深度伪造数据集DFDC-P上比较了最佳架构InstructBLIP在两种场景下的性能：零样本和领域内微调，与传统方法进行比较。我们的结果证明了VLMs优于传统分类器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The contemporary phenomenon of deepfakes, utilizing GAN or diffusion modelsfor face swapping, presents a substantial and evolving threat in digital media,identity verification, and a multitude of other systems. The majority ofexisting methods for detecting deepfakes rely on training specializedclassifiers to distinguish between genuine and manipulated images, focusingonly on the image domain without incorporating any auxiliary tasks that couldenhance robustness. In this paper, inspired by the zero-shot capabilities ofVision Language Models, we propose a novel VLM-based approach to imageclassification and then evaluate it for deepfake detection. Specifically, weutilize a new high-quality deepfake dataset comprising 60,000 images, on whichour zero-shot models demonstrate superior performance to almost all existingmethods. Subsequently, we compare the performance of the best-performingarchitecture, InstructBLIP, on the popular deepfake dataset DFDC-P againsttraditional methods in two scenarios: zero-shot and in-domain fine-tuning. Ourresults demonstrate the superiority of VLMs over traditional classifiers.</description>
      <author>example@mail.com (Viacheslav Pirogov)</author>
      <guid isPermaLink="false">2507.22469v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items</title>
      <link>http://arxiv.org/abs/2507.22268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了MMSC框架，一个自监督多模态关系物品表征学习系统，用于推断可替代和互补物品，解决了现有方法中噪声数据和稀疏性问题，实验显示性能显著提升。&lt;h4&gt;背景&lt;/h4&gt;现有方法主要使用图神经网络或物品内容信息建模物品关联，但忽略了用户行为数据中的噪声和数据稀疏性问题，这些问题源于用户行为的长尾分布。&lt;h4&gt;目的&lt;/h4&gt;开发一个自监督多模态关系物品表征学习框架，解决噪声用户行为数据和数据稀疏性问题，提高可替代和互补物品推荐的准确性。&lt;h4&gt;方法&lt;/h4&gt;MMSC框架包含三个主要组件：多模态物品表征学习模块、自监督行为表征学习模块和分层表征聚合机制，并利用大型语言模型生成增强训练数据以增强去噪效果。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上的实验表明，MMSC在可替代推荐方面比基线方法高26.1%，在互补推荐方面高39.2%，且能有效处理冷启动物品。&lt;h4&gt;结论&lt;/h4&gt;MMSC通过整合多模态表征学习、自监督学习和分层聚合机制，有效解决了推荐系统中的数据噪声和稀疏性问题，特别是在处理复杂物品关系方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了一种新颖的自监督多模态关系物品表征学习框架，旨在推断可替代和互补物品。现有方法主要关注使用图神经网络建模从用户行为推断的物品-物品关联，或利用物品内容信息。然而，这些方法通常忽略了关键挑战，如嘈杂的用户行为数据和由于这些行为长尾分布导致的数据稀疏性。在本文中，我们提出了MMSC，一个自监督多模态关系物品表征学习框架，以解决这些挑战。具体来说，MMSC包含三个主要组件：(1)多模态物品表征学习模块，利用多模态基础模型并从物品元数据中学习；(2)自监督行为表征学习模块，对用户行为数据进行去噪并从中学习；(3)分层表征聚合机制，在语义和任务级别整合物品表征。此外，我们利用大型语言模型生成增强的训练数据，进一步增强了训练过程中的去噪效果。我们在五个真实世界数据集上进行了广泛的实验，表明MMSC在可替代推荐方面比现有基线方法高出26.1%，在互补推荐方面高出39.2%。此外，我们实证表明MMSC在建模冷启动物品方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel self-supervised multi-modal relational itemrepresentation learning framework designed to infer substitutable andcomplementary items. Existing approaches primarily focus on modeling item-itemassociations deduced from user behaviors using graph neural networks (GNNs) orleveraging item content information. However, these methods often overlookcritical challenges, such as noisy user behavior data and data sparsity due tothe long-tailed distribution of these behaviors. In this paper, we proposeMMSC, a self-supervised multi-modal relational item representation learningframework to address these challenges. Specifically, MMSC consists of threemain components: (1) a multi-modal item representation learning module thatleverages a multi-modal foundational model and learns from item metadata, (2) aself-supervised behavior-based representation learning module that denoises andlearns from user behavior data, and (3) a hierarchical representationaggregation mechanism that integrates item representations at both the semanticand task levels. Additionally, we leverage LLMs to generate augmented trainingdata, further enhancing the denoising process during training. We conductextensive experiments on five real-world datasets, showing that MMSCoutperforms existing baselines by 26.1% for substitutable recommendation and39.2% for complementary recommendation. In addition, we empirically show thatMMSC is effective in modeling cold-start items.</description>
      <author>example@mail.com (Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram)</author>
      <guid isPermaLink="false">2507.22268v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Using Scaling Laws for Data Source Utility Estimation in Domain-Specific Pre-Training</title>
      <link>http://arxiv.org/abs/2507.22250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种优化基础模型训练中特定领域数据集构建的框架，通过估计不同数据源的质量来实现成本效益最优的资源分配。&lt;h4&gt;背景&lt;/h4&gt;在基础模型训练的第二阶段预训练（退火）阶段，需要做出关于如何从不同数据源获取资源的决策，以使通用预训练模型专门化到特定领域。先前的研究依赖于点估计方法进行数据扩展决策，但这种方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;寻找一种成本效益高的方法来估计数据源质量，以便在第二阶段预训练阶段做出最优的资源分配决策，使通用预训练模型专门化到特定领域。&lt;h4&gt;方法&lt;/h4&gt;扩展了传统的点估计方法（微退火），通过执行多次计算量不同的退火运行来估计扩展定律。通过系统分析相对于获取成本的性能提升，为不同数据源估计扩展曲线，从而指导资源分配。&lt;h4&gt;主要发现&lt;/h4&gt;不同计算规模之间缺乏秩不变性，使得基于点估计的数据扩展决策可能具有误导性；通过系统分析可以估计不同数据源的扩展曲线；使用多次退火运行可以高效估计数据源的扩展行为，得出与传统微退火技术不同的结论。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法使数据驱动的决策成为可能，用于选择和优化数据源，从而在特定领域模型专门化过程中实现成本效益最优的资源分配。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个用于优化基础模型训练中特定领域数据集构建的框架。具体来说，我们寻求一种成本效益高的方法来估计数据源（例如合成生成的或过滤的网络数据等）的质量，以便在第二阶段预训练（即退火）阶段做出关于从这些数据源获取资源的最优决策，目标是使通用预训练模型专门化到特定领域。我们的方法扩展了通常的点估计方法（即微退火），通过执行多次计算量不同的退火运行来估计扩展定律。这解决了先前工作中的一个关键限制，即由于不同计算规模之间缺乏秩不变性，对数据扩展决策的点估计可能具有误导性——这种现象在实验中得到了证实。通过系统分析相对于获取成本的性能提升，我们发现可以为不同的数据源估计扩展曲线。这样的扩展定律可以告知不同数据获取方法（例如合成数据）、数据源（例如用户或网络数据）和可用计算资源之间的成本效益资源分配。我们在具有70亿参数的预训练模型上通过实验验证了我们的方法。我们将该方法应用于：预训练数据中具有良好代表性的领域——医疗领域，以及预训练语料库中代表性不足的领域——数学领域。我们表明，通过运行多次退火运行，可以高效估计数据源的扩展行为，如果使用通常的微退火技术进行点估计，可能会导致不同的结论。这使得能够进行数据驱动的决策来选择和优化数据源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a framework for optimizing domain-specific dataset constructionin foundation model training. Specifically, we seek a cost-efficient way toestimate the quality of data sources (e.g. synthetically generated or filteredweb data, etc.) in order to make optimal decisions about resource allocationfor data sourcing from these sources for the stage two pre-training phase, akaannealing, with the goal of specializing a generalist pre-trained model tospecific domains. Our approach extends the usual point estimate approaches, akamicro-annealing, to estimating scaling laws by performing multiple annealingruns of varying compute spent on data curation and training. This addresses akey limitation in prior work, where reliance on point estimates for datascaling decisions can be misleading due to the lack of rank invariance acrosscompute scales -- a phenomenon we confirm in our experiments. By systematicallyanalyzing performance gains relative to acquisition costs, we find that scalingcurves can be estimated for different data sources. Such scaling laws caninform cost effective resource allocation across different data acquisitionmethods (e.g. synthetic data), data sources (e.g. user or web data) andavailable compute resources. We validate our approach through experiments on apre-trained model with 7 billion parameters. We adapt it to: a domainwell-represented in the pre-training data -- the medical domain, and a domainunderrepresented in the pretraining corpora -- the math domain. We show thatone can efficiently estimate the scaling behaviors of a data source by runningmultiple annealing runs, which can lead to different conclusions, had one usedpoint estimates using the usual micro-annealing technique instead. This enablesdata-driven decision-making for selecting and optimizing data sources.</description>
      <author>example@mail.com (Oleksiy Ostapenko, Charles Guille-Escuret, Luke Kumar, Max Tian, Denis Kocetkov, Gopeshh Subbaraj, Raymond Li, Joel Lamy-Poirier, Sebastien Paquet, Torsten Scholak)</author>
      <guid isPermaLink="false">2507.22250v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction</title>
      <link>http://arxiv.org/abs/2507.22229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了TRIBE模型，首个能跨多种模态、皮层区域和个体预测脑反应的深度神经网络，在脑编码竞赛中表现优异，为构建统一认知模型奠定基础。&lt;h4&gt;背景&lt;/h4&gt;神经科学历史上是分裂成专门领域的，每个领域专注于分离的模态、任务或脑区，这种方法阻碍了统一认知模型的发展。&lt;h4&gt;目的&lt;/h4&gt;引入TRIBE模型，预测跨多种模态、皮层区域和个体的刺激引起的脑反应，构建人类大脑中表征的整合模型。&lt;h4&gt;方法&lt;/h4&gt;结合预训练的文本、音频和视频基础模型的表征，使用transformer处理它们随时间演变的特性，精确建模视频的fMRI空间和时间响应。&lt;h4&gt;主要发现&lt;/h4&gt;在Algonauts 2025脑编码竞赛中以显著优势获得第一名；单模态模型可预测相应皮层网络，但在高级联合皮层中，多模态模型表现更优。&lt;h4&gt;结论&lt;/h4&gt;目前应用于感知和理解，为构建人类大脑中表征的整合模型铺平了道路，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;历史上，神经科学通过分裂成专门领域而取得进展，每个领域专注于分离的模态、任务或脑区。虽然这种方法富有成效，但它阻碍了统一认知模型的发展。在这里，我们引入TRIBE，这是第一个深度神经网络，经过训练可以预测跨多种模态、皮层区域和个体的刺激引起的脑反应。通过结合预训练的文本、音频和视频基础模型的表征，并使用transformer处理它们随时间演变的特性，我们的模型可以精确建模视频的fMRI空间和时间响应，在Algonauts 2025脑编码竞赛中以显著优势获得第一名。消融实验显示，虽然单模态模型可以可靠地预测相应的皮层网络（如视觉或听觉网络），但在高级联合皮层中，它们被我们的多模态模型系统性地超越。目前应用于感知和理解，我们的方法为构建人类大脑中表征的整合模型铺平了道路。我们的代码可在https://github.com/facebookresearch/algonauts-2025获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Historically, neuroscience has progressed by fragmenting into specializeddomains, each focusing on isolated modalities, tasks, or brain regions. Whilefruitful, this approach hinders the development of a unified model ofcognition. Here, we introduce TRIBE, the first deep neural network trained topredict brain responses to stimuli across multiple modalities, cortical areasand individuals. By combining the pretrained representations of text, audio andvideo foundational models and handling their time-evolving nature with atransformer, our model can precisely model the spatial and temporal fMRIresponses to videos, achieving the first place in the Algonauts 2025 brainencoding competition with a significant margin over competitors. Ablations showthat while unimodal models can reliably predict their corresponding corticalnetworks (e.g. visual or auditory networks), they are systematicallyoutperformed by our multimodal model in high-level associative cortices.Currently applied to perception and comprehension, our approach paves the waytowards building an integrative model of representations in the human brain.Our code is available at https://github.com/facebookresearch/algonauts-2025.</description>
      <author>example@mail.com (Stéphane d'Ascoli, Jérémy Rapin, Yohann Benchetrit, Hubert Banville, Jean-Rémi King)</author>
      <guid isPermaLink="false">2507.22229v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception</title>
      <link>http://arxiv.org/abs/2507.22194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Frontier-Seg是一种用于从移动机器人视频流中进行时间一致的无监督地形分割的方法，克服了传统监督方法和现有零样本无监督方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;地形感知自主地面导航的进步依赖于监督语义分割，但这些方法需要昂贵的数据收集和人工标注。自主系统越来越多地部署在没有标记数据的非结构化环境中，语义类别可能模糊或特定于领域。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本无监督分割方法缺乏时间一致性的问题，实现从移动机器人视频流中地形的时间一致的无监督分割，无需人工监督。&lt;h4&gt;方法&lt;/h4&gt;Frontier-Seg通过对从基础模型骨干网络（特别是DINOv2）提取的超像素级特征进行聚类，并强制执行跨帧的时间一致性，以识别持久的地形边界或前沿。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的基准数据集（包括RUGD和RELLIS-3D）上的评估表明，Frontier-Seg能够在非结构化越野环境中执行无监督分割。&lt;h4&gt;结论&lt;/h4&gt;Frontier-Seg成功地实现了时间一致的无监督地形分割，解决了自主系统在非结构化环境中面临的感知挑战。&lt;h4&gt;翻译&lt;/h4&gt;地形感知自主地面导航的快速发展得益于监督语义分割的进步。然而，这些方法依赖昂贵的数据收集和劳动密集型的真实标签来训练深度模型。此外，自主系统越来越多地部署在没有标记数据存在的排练过的、非结构化的环境中，语义类别可能模糊或特定于领域。最近用于此类环境的零样本无监督分割方法显示出前景，但通常在单个帧上操作，缺乏时间一致性——这是非结构化环境中鲁棒感知的关键属性。为了解决这一差距，我们引入了Frontier-Seg，一种用于从移动机器人视频流中进行时间一致的无监督地形分割的方法。Frontier-Seg对从基础模型骨干网络（特别是DINOv2）提取的超像素级特征进行聚类，并强制执行跨帧的时间一致性，以识别持久的地形边界或前沿，无需人工监督。我们在多样化的基准数据集（包括RUGD和RELLIS-3D）上评估了Frontier-Seg，证明了其在非结构化越野环境中执行无监督分割的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid progress in terrain-aware autonomous ground navigation has been drivenby advances in supervised semantic segmentation. However, these methods rely oncostly data collection and labor-intensive ground truth labeling to train deepmodels. Furthermore, autonomous systems are increasingly deployed inunrehearsed, unstructured environments where no labeled data exists andsemantic categories may be ambiguous or domain-specific. Recent zero-shotapproaches to unsupervised segmentation have shown promise in such settings buttypically operate on individual frames, lacking temporal consistency-a criticalproperty for robust perception in unstructured environments. To address thisgap we introduce Frontier-Seg, a method for temporally consistent unsupervisedsegmentation of terrain from mobile robot video streams. Frontier-Seg clusterssuperpixel-level features extracted from foundation modelbackbones-specifically DINOv2-and enforces temporal consistency across framesto identify persistent terrain boundaries or frontiers without humansupervision. We evaluate Frontier-Seg on a diverse set of benchmarkdatasets-including RUGD and RELLIS-3D-demonstrating its ability to performunsupervised segmentation across unstructured off-road environments.</description>
      <author>example@mail.com (Christian Ellis, Maggie Wigness, Craig Lennon, Lance Fiondella)</author>
      <guid isPermaLink="false">2507.22194v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Measuring Time-Series Dataset Similarity using Wasserstein Distance</title>
      <link>http://arxiv.org/abs/2507.22189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于分布的时间序列数据集相似性测量方法，利用Wasserstein距离计算时间序列数据集对应多元正态分布之间的距离，实验证明该方法能有效识别相似数据集并估计基础模型性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型研究正在兴起，需要衡量时间序列数据集的(不)相似性，这种相似性测量在模型选择、微调和可视化等多个方面有助于研究。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于分布的时间序列数据集相似性测量方法，利用Wasserstein距离来测量相似性。&lt;h4&gt;方法&lt;/h4&gt;将时间序列数据集视为基础多元正态分布的经验实例，两个时间序列数据集之间的相似性计算为它们对应多元正态分布之间的Wasserstein距离。&lt;h4&gt;主要发现&lt;/h4&gt;全面的实验和可视化证明了该方法的有效性；Wasserstein距离有助于识别相似的时间序列数据集；该方法有助于估计基础模型在分布外和迁移学习评估中的推理性能；所提出的测量与推理损失之间具有高相关性(&gt;0.60)。&lt;h4&gt;结论&lt;/h4&gt;基于Wasserstein距离的分布方法是测量时间序列数据集相似性的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型研究的兴起提升了对测量时间序列数据集(不)相似性的日益增长的需求。时间序列数据集相似性测量在多个方面有助于研究，包括模型选择、微调和可视化。在本文中，我们提出了一种基于分布的方法，通过利用Wasserstein距离来测量时间序列数据集相似性。我们将时间序列数据集视为基础多元正态分布的经验实例。因此，两个时间序列数据集之间的相似性计算为它们对应多元正态分布之间的Wasserstein距离。全面的实验和可视化展示了我们方法的有效性。具体来说，我们展示了Wasserstein距离如何帮助识别相似的时间序列数据集，并促进基础模型在分布外和迁移学习评估中的推理性能估计，我们提出的测量与推理损失之间具有高相关性(&gt;0.60)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of time-series foundation model research elevates the growingneed to measure the (dis)similarity of time-series datasets. A time-seriesdataset similarity measure aids research in multiple ways, including modelselection, finetuning, and visualization. In this paper, we propose adistribution-based method to measure time-series dataset similarity byleveraging the Wasserstein distance. We consider a time-series dataset anempirical instantiation of an underlying multivariate normal distribution(MVN). The similarity between two time-series datasets is thus computed as theWasserstein distance between their corresponding MVNs. Comprehensiveexperiments and visualization show the effectiveness of our approach.Specifically, we show how the Wasserstein distance helps identify similartime-series datasets and facilitates inference performance estimation offoundation models in both out-of-distribution and transfer learning evaluation,with high correlations between our proposed measure and the inference loss(&gt;0.60).</description>
      <author>example@mail.com (Hongjie Chen, Akshay Mehra, Josh Kimball, Ryan A. Rossi)</author>
      <guid isPermaLink="false">2507.22189v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock</title>
      <link>http://arxiv.org/abs/2507.22101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述系统回顾了农业领域超过200项研究工作，涵盖了传统机器学习方法、先进深度学习技术和视觉-语言基础模型，应用于作物疾病检测、牲畜健康管理和水生物种监测等任务，并讨论了实施挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;农业、渔业和畜牧业是全球粮食生产的支柱，对于养活不断增长的人口至关重要。然而，这些领域面临着气候多变、资源限制和可持续管理需求等重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决农业领域面临的挑战需要高效、准确和可扩展的技术解决方案，突显了人工智能的重要性。这篇综述旨在系统回顾农业AI领域的最新研究进展。&lt;h4&gt;方法&lt;/h4&gt;综述对农业领域的传统机器学习方法、先进的深度学习技术（如视觉变换器）和最近的视觉-语言基础模型（如CLIP）进行了系统回顾，重点关注作物疾病检测、牲畜健康管理和水生物种监测等多样化任务。&lt;h4&gt;主要发现&lt;/h4&gt;综述涵盖了主要实施挑战，如数据可变性和实验方面：数据集、性能评估指标和地理重点。同时强调了需要多模态数据集成、高效的边缘设备部署以及适应不同农业环境的领域自适应AI模型。&lt;h4&gt;结论&lt;/h4&gt;农业AI领域正在快速发展，未来研究方向包括多模态数据集成、高效的边缘设备部署和领域自适应AI模型，以适应多样化的农业环境。&lt;h4&gt;翻译&lt;/h4&gt;农作物、渔业和畜牧业构成了全球粮食生产的支柱，对于养活不断增长的世界人口至关重要。然而，这些领域面临着相当大的挑战，包括气候多变、资源限制和可持续管理的需求。解决这些问题需要高效、准确和可扩展的技术解决方案，突显了人工智能的重要性。本综述对农业领域的200多项研究工作进行了系统而全面的回顾，涵盖了传统机器学习方法、先进的深度学习技术（如视觉变换器）和最近的视觉-语言基础模型（如CLIP），重点关注作物疾病检测、牲畜健康管理和水生物种监测等多样化任务。我们进一步涵盖了主要实施挑战，如数据可变性和实验方面：数据集、性能评估指标和地理重点。我们通过讨论潜在的开研究方向结束综述，强调了需要多模态数据集成、高效的边缘设备部署和适应不同农业环境的领域自适应AI模型。该领域的快速发展演变可以在我们的项目页面上积极跟踪：https://github.com/umair1221/AI-in-Agriculture&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crops, fisheries and livestock form the backbone of global food production,essential to feed the ever-growing global population. However, these sectorsface considerable challenges, including climate variability, resourcelimitations, and the need for sustainable management. Addressing these issuesrequires efficient, accurate, and scalable technological solutions,highlighting the importance of artificial intelligence (AI). This surveypresents a systematic and thorough review of more than 200 research workscovering conventional machine learning approaches, advanced deep learningtechniques (e.g., vision transformers), and recent vision-language foundationmodels (e.g., CLIP) in the agriculture domain, focusing on diverse tasks suchas crop disease detection, livestock health management, and aquatic speciesmonitoring. We further cover major implementation challenges such as datavariability and experimental aspects: datasets, performance evaluation metrics,and geographical focus. We finish the survey by discussing potential openresearch directions emphasizing the need for multimodal data integration,efficient edge-device deployment, and domain-adaptable AI models for diversefarming environments. Rapid growth of evolving developments in this field canbe actively tracked on our project page:https://github.com/umair1221/AI-in-Agriculture</description>
      <author>example@mail.com (Umair Nawaz, Muhammad Zaigham Zaheer, Fahad Shahbaz Khan, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer)</author>
      <guid isPermaLink="false">2507.22101v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss</title>
      <link>http://arxiv.org/abs/2507.22092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted (Oral) in MedAGI 2025 International Workshop at MICCAI  Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;计算病理学中的扫描仪偏差问题影响模型可信度和临床应用，提出的ScanGen方法通过对比损失函数有效减轻了这种偏差，提高了跨扫描仪的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;计算病理学能从全切片图像中挖掘可操作见解，深度学习虽性能强大但可能受扫描仪引入的不相关细节影响，导致扫描仪偏差，阻碍临床医生对CPath工具的信任及临床应用。&lt;h4&gt;目的&lt;/h4&gt;基准测试病理学基础模型使用多扫描仪数据集，解决扫描仪偏差问题，提高模型对扫描仪变化的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出ScanGen，一种在任务特定微调期间应用的对比损失函数，应用于肺癌中EGFR突变预测的多实例学习任务，使用H&amp;E染色的全切片图像。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型仍受扫描仪偏差影响，ScanGen显著提高了模型跨扫描仪泛化能力，同时保持或提高了EGFR突变预测性能。&lt;h4&gt;结论&lt;/h4&gt;ScanGen是有效减轻扫描仪偏差的方法，提高了计算病理学模型在临床实践中的可靠性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学（CPath）在从全切片图像（WSIs）中挖掘可操作的见解方面显示出巨大潜力。深度学习（DL）一直是现代CPath的核心，虽然它提供了前所未有的性能，但众所周知，DL可能受到不相关细节的影响，例如由不同商业可用扫描仪引入的细节。这可能导致扫描仪偏差，即由不同扫描仪获取的相同组织的模型输出可能有所不同。反过来，这阻碍了临床医生对基于CPath的工具的信任及其在现实世界临床实践中的部署。最近的病理学基础模型（FMs）有望提供更好的领域泛化能力。在本文中，我们使用多扫描仪数据集对FMs进行基准测试，并表明FMs仍然受到扫描仪偏差的影响。基于这一观察，我们提出了ScanGen，一种在任务特定微调期间应用的对比损失函数，可减轻扫描仪偏差，从而增强模型对扫描仪变化的鲁棒性。我们的方法应用于肺癌中表皮生长因子受体（EGFR）突变预测的多实例学习任务，使用H&amp;E染色的WSIs。我们观察到，ScanGen显著提高了跨扫描仪泛化的能力，同时保持或提高了EGFR突变预测的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational pathology (CPath) has shown great potential in miningactionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has beenat the center of modern CPath, and while it delivers unprecedented performance,it is also known that DL may be affected by irrelevant details, such as thoseintroduced during scanning by different commercially available scanners. Thismay lead to scanner bias, where the model outputs for the same tissue acquiredby different scanners may vary. In turn, it hinders the trust of clinicians inCPath-based tools and their deployment in real-world clinical practices. Recentpathology Foundation Models (FMs) promise to provide better domaingeneralization capabilities. In this paper, we benchmark FMs using amulti-scanner dataset and show that FMs still suffer from scanner bias.Following this observation, we propose ScanGen, a contrastive loss functionapplied during task-specific fine-tuning that mitigates scanner bias, therebyenhancing the models' robustness to scanner variations. Our approach is appliedto the Multiple Instance Learning task of Epidermal Growth Factor Receptor(EGFR) mutation prediction from H\&amp;E-stained WSIs in lung cancer. We observethat ScanGen notably enhances the ability to generalize across scanners, whileretaining or improving the performance of EGFR mutation prediction.</description>
      <author>example@mail.com (Gianluca Carloni, Biagio Brattoli, Seongho Keum, Jongchan Park, Taebum Lee, Chang Ho Ahn, Sergio Pereira)</author>
      <guid isPermaLink="false">2507.22092v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>PINN and GNN-based RF Map Construction for Wireless Communication Systems</title>
      <link>http://arxiv.org/abs/2507.22513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合物理信息神经网络(PINN)和图神经网络(GNN)的新型RF地图构建方法，能够在稀疏采样条件下实现高精度预测，在室内外复杂环境中表现优异。&lt;h4&gt;背景&lt;/h4&gt;射频(RF)地图是捕获多径信号传播特性的有前景技术，为无线通信网络中的信道建模、覆盖分析和波束成形提供关键支持。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的RF地图构建方法，解决在稀疏采样条件下进行高精度RF地图构建的问题。&lt;h4&gt;方法&lt;/h4&gt;结合物理信息神经网络(PINN)和图神经网络(GNN)；PINN整合电磁传播定律的物理约束指导学习；GNN建模接收器位置间的空间相关性；将多径信号参数化为接收功率、延迟和到达角度；整合物理先验和空间依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;方法能在稀疏采样条件下实现高精度RF地图构建；在室内和复杂室外环境中提供稳健性能；在泛化性和准确性方面优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出方法结合物理约束和空间相关性，能准确预测多径参数，为无线通信网络中的RF地图构建提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;射频（RF）地图是一种捕获多径信号传播特性的有前景技术，为无线通信网络中的信道建模、覆盖分析和波束成形提供关键支持。本文提出了一种基于物理信息神经网络（PINN）和图神经网络（GNN）相结合的新型RF地图构建方法。PINN整合了源自电磁传播定律的物理约束来指导学习过程，而GNN则建模接收器位置之间的空间相关性。通过将多径信号参数化为接收功率、延迟和到达角度（AoA），并整合物理先验和空间依赖性，所提出的方法实现了多径参数的准确预测。实验结果表明，该方法能够在稀疏采样条件下实现高精度RF地图构建，并在室内和复杂室外环境中都提供稳健的性能，在泛化性和准确性方面优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radio frequency (RF) map is a promising technique for capturing thecharacteristics of multipath signal propagation, offering critical support forchannel modeling, coverage analysis, and beamforming in wireless communicationnetworks. This paper proposes a novel RF map construction method based on acombination of physics-informed neural network (PINN) and graph neural network(GNN). The PINN incorporates physical constraints derived from electromagneticpropagation laws to guide the learning process, while the GNN models spatialcorrelations among receiver locations. By parameterizing multipath signals intoreceived power, delay, and angle of arrival (AoA), and integrating bothphysical priors and spatial dependencies, the proposed method achieves accurateprediction of multipath parameters. Experimental results demonstrate that themethod enables high-precision RF map construction under sparse samplingconditions and delivers robust performance in both indoor and complex outdoorenvironments, outperforming baseline methods in terms of generalization andaccuracy.</description>
      <author>example@mail.com (Lizhou Liu, Xiaohui Chen, Zihan Tang, Mengyao Ma, Wenyi Zhang)</author>
      <guid isPermaLink="false">2507.22513v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2507.22454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IROS 2025. Code:https://github.com/IRMVLab/TopoLiDM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoLiDM是一个创新的LiDAR场景生成框架，结合图神经网络和扩散模型，通过拓扑正则化实现高保真生成，在保持几何真实性和全局拓扑一致性方面优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;LiDAR场景生成对于降低真实世界LiDAR数据收集成本和提高自动驾驶下游感知任务的鲁棒性至关重要。现有方法难以捕捉几何真实性和全局拓扑一致性，而最近的LiDAR扩散模型将点嵌入潜在空间以提高生成效率，这限制了它们对详细几何结构建模的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在捕捉几何真实性和全局拓扑一致性方面的挑战，提出一个能够高保真生成LiDAR数据的新框架。&lt;h4&gt;方法&lt;/h4&gt;提出TopoLiDM框架，首先训练拓扑保持的VAE提取潜在图表示，然后冻结VAE通过潜在扩散模型生成新的潜在拓扑图，并引入0维持久同调约束确保生成场景遵循真实世界全局拓扑结构。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI-360数据集上实验证明，TopoLiDM实现了22.6%的Frechet范围图像距离降低和9.2%的最小匹配距离降低，平均推理速度达1.68样本/秒，展示了实际应用的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;TopoLiDM是一个有效的LiDAR场景生成框架，能够保持几何真实性和全局拓扑一致性，相关代码将在https://github.com/IRMVLab/TopoLiDM发布。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR场景生成对于降低真实世界LiDAR数据收集成本和提高自动驾驶下游感知任务的鲁棒性至关重要。然而，现有方法通常难以捕捉几何真实性和全局拓扑一致性。最近的LiDAR扩散模型主要将LiDAR点嵌入到潜在空间以提高生成效率，这限制了它们对详细几何结构建模的可解释能力和保持全局拓扑一致性的能力。为解决这些挑战，我们提出了TopoLiDM，一个将图神经网络与扩散模型相结合并在拓扑正则化下进行高保真LiDAR生成的新框架。我们的方法首先训练一个拓扑保持的VAE，通过图构造和多个图卷积层提取潜在图表示。然后我们冻结VAE并通过潜在扩散模型生成新的潜在拓扑图。我们还引入了0维持久同调约束，确保生成的LiDAR场景遵循真实世界的全局拓扑结构。在KITTI-360数据集上的大量实验证明了TopoLiDM优于最先进的方法，实现了22.6%的Frechet范围图像距离降低和9.2%的最小匹配距离降低。值得注意的是，我们的模型还实现了快速生成速度，平均推理时间为1.68样本/秒，展示了其在实际应用中的可扩展性。我们将在https://github.com/IRMVLab/TopoLiDM发布相关代码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决LiDAR点云生成中的几何真实性和全局拓扑一致性问题。这个问题在现实中非常重要，因为真实LiDAR数据收集成本高昂，在极端天气条件下性能会下降，而自动驾驶、机器人定位等下游任务需要大量高质量数据来增强感知系统的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有LiDAR扩散模型的局限性，特别是它们在保持全局拓扑结构和几何细节方面的不足。他们借鉴了图神经网络来建模点间关系，结合拓扑数据分析和持久同调技术来保持全局拓扑结构。设计了两阶段训练范式：先训练拓扑保持的VAE，然后冻结VAE参数训练扩散模型。他们参考了扩散模型、图神经网络、持久同调以及现有LiDAR生成方法如LiDARGAN、LiDARVAE等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将图神经网络与扩散模型结合，使用拓扑正则化的图结构作为潜在变量，而非传统点嵌入，从而保持全局拓扑结构和几何相关性。整体流程包括：1)将LiDAR距离图像分割并构建图结构；2)训练拓扑保持的VAE，加入持久同调约束；3)冻结VAE参数，用扩散模型生成新的拓扑图；4)将生成的拓扑图解码为3D LiDAR点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)拓扑感知的图表示作为潜在变量；2)拓扑保持的VAE模块，强制执行持久同调约束；3)两阶段训练范式；4)高效与质量的平衡。相比之前工作，它不直接将LiDAR点嵌入潜在空间，而是使用图结构，更好地保持了全局拓扑一致性和几何相关性，在多个评估指标上表现更优(FRID降低22.6%，MMD降低9.2%)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TopoLiDM通过结合图神经网络与扩散模型并在拓扑正则化下进行训练，实现了高效、可解释且高保真的LiDAR点云生成，显著提升了生成场景的几何真实性和全局拓扑一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR scene generation is critical for mitigating real-world LiDAR datacollection costs and enhancing the robustness of downstream perception tasks inautonomous driving. However, existing methods commonly struggle to capturegeometric realism and global topological consistency. Recent LiDAR DiffusionModels (LiDMs) predominantly embed LiDAR points into the latent space forimproved generation efficiency, which limits their interpretable ability tomodel detailed geometric structures and preserve global topologicalconsistency. To address these challenges, we propose TopoLiDM, a novelframework that integrates graph neural networks (GNNs) with diffusion modelsunder topological regularization for high-fidelity LiDAR generation. Ourapproach first trains a topological-preserving VAE to extract latent graphrepresentations by graph construction and multiple graph convolutional layers.Then we freeze the VAE and generate novel latent topological graphs through thelatent diffusion models. We also introduce 0-dimensional persistent homology(PH) constraints, ensuring the generated LiDAR scenes adhere to real-worldglobal topological structures. Extensive experiments on the KITTI-360 datasetdemonstrate TopoLiDM's superiority over state-of-the-art methods, achievingimprovements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lowerMinimum Matching Distance (MMD). Notably, our model also enables fastgeneration speed with an average inference time of 1.68 samples/s, showcasingits scalability for real-world applications. We will release the related codesat https://github.com/IRMVLab/TopoLiDM.</description>
      <author>example@mail.com (Jiuming Liu, Zheng Huang, Mengmeng Liu, Tianchen Deng, Francesco Nex, Hao Cheng, Hesheng Wang)</author>
      <guid isPermaLink="false">2507.22454v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic</title>
      <link>http://arxiv.org/abs/2507.22174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合图神经网络和循环神经网络的时空强化学习方法，用于优化通信网络中的数据包路由，解决了传统强化学习方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;强化学习已成为优化通信网络中数据包路由的成熟方法，但标准强化学习算法基于马尔可夫决策过程，该假设在许多实际场景中不成立。传统强化学习算法无法明确捕捉复杂网络拓扑中的空间关系，而通信网络的动态流量模式和任意节点链路数量进一步增加了决策复杂性。&lt;h4&gt;目的&lt;/h4&gt;解决传统强化学习方法在通信网络路由优化中的局限性，特别是马尔可夫假设不成立和无法有效捕捉网络空间关系的问题，提出一种能够更好地捕捉网络拓扑空间动态和流量时间模式的强化学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出一种时空强化学习方法，将图神经网络(GNNs)用于捕捉网络拓扑的空间动态，循环神经网络(RNNs)用于捕捉时间流量模式，两者结合以增强路由决策。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，与传统强化学习技术相比，所提出的方法在性能上更优，并且对网络拓扑的变化具有更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过整合图神经网络和循环神经网络，所提出的时空强化学习方法能有效解决传统强化学习在通信网络路由优化中的局限性，特别是在处理非马尔可夫环境和复杂网络拓扑方面表现出色，且对网络变化具有更好的适应性。&lt;h4&gt;翻译&lt;/h4&gt;强化学习已成为优化通信网络中数据包路由的成熟方法。标准强化学习算法通常基于马尔可夫决策过程，该过程假设当前环境状态为系统演化和决策提供了所有必要信息。然而，在许多实际场景中，这种马尔可夫假设是无效的，使得MDP和强化学习框架无法产生最优解。此外，传统强化学习算法通常采用函数近似，无法明确捕捉具有复杂网络拓扑的环境中的固有空间关系。通信网络的特点是动态流量模式和任意数量的节点与链路，这进一步增加了决策过程的复杂性。为应对这些挑战，我们提出了一种时空强化学习方法，将图神经网络和循环神经网络相结合，以适当捕捉网络拓扑的空间动态和流量时间模式，从而增强路由决策。我们的评估表明，与传统强化学习技术相比，所提出的方法在性能上更优，并且对网络拓扑的变化具有更强的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) has become a well-established approach foroptimizing packet routing in communication networks. Standard RL algorithmstypically are based on the Markov Decision Process (MDP), which assumes thatthe current state of the environment provides all the necessary information forsystem evolution and decision-making. However, this Markovian assumption isinvalid in many practical scenarios, making the MDP and RL frameworksinadequate to produce the optimal solutions. Additionally, traditional RLalgorithms often employ function approximations (e.g., by neural networks) thatdo not explicitly capture the spatial relationships inherent in environmentswith complex network topologies. Communication networks are characterized bydynamic traffic patterns and arbitrary numbers of nodes and links, whichfurther complicate the decision-making process. To address these challenges, wepropose a spatial-temporal RL approach that integrates Graph Neural Networks(GNNs) and Recurrent Neural Networks (RNNs) to adequately capture the spatialdynamics regarding network topology and temporal traffic patterns,respectively, to enhance routing decisions. Our evaluation demonstrates thatthe proposed method outperforms and is more robust to changes in the networktopology when compared with traditional RL techniques.</description>
      <author>example@mail.com (Molly Wang)</author>
      <guid isPermaLink="false">2507.22174v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Mesh based segmentation for automated margin line generation on incisors receiving crown treatment</title>
      <link>http://arxiv.org/abs/2507.22859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种使用深度学习自动确定牙冠边缘线的新框架，解决了传统方法中手动定义边缘线不可重复且不一致的问题。研究通过基于网格的神经网络、集成模型和图割方法优化分割结果，提高了牙冠设计的自动化水平和准确性。&lt;h4&gt;背景&lt;/h4&gt;牙冠是修复患者受损或缺失牙齿的重要治疗方法。目前的设计方法使用商业牙科设计软件，需要牙科技师在预备体表面上手动定义精确的边缘线，这是一个不可重复且不一致的过程。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动且准确地确定牙冠边缘线的新框架，以替代传统的人工方法，提高牙冠设计的一致性和效率。&lt;h4&gt;方法&lt;/h4&gt;使用合作牙科实验室提供的门牙数据集训练深度学习分割模型；修改基于网格的神经网络输入通道，将预备体分割为两个区域；使用k折交叉验证训练5个模型并采用投票分类器技术组合结果；应用边界平滑和图割方法优化分割结果；选择分离两个区域的边界面代表边缘线面；使用样条曲线拟合边界面的中心点来预测边缘线。&lt;h4&gt;主要发现&lt;/h4&gt;集成模型结合最大概率预测成功率最高（13个测试案例中7个成功）；预测边缘线与真实边缘线之间的最大距离阈值为200微米（代表人为误差）；预备体质量越好，预测边缘线与真实边缘线之间的差异越小（斯皮尔曼等级相关系数为-0.683）。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发了一种使用深度学习自动确定牙冠边缘线的方法，可以提高牙冠设计的一致性和效率，且预备体质量与预测准确性呈正相关。&lt;h4&gt;翻译&lt;/h4&gt;牙冠是修复患者受损或缺失牙齿的重要牙科治疗方法。最近的牙冠设计方法使用商业牙科设计软件完成。一旦预备体的扫描上传到软件中，牙科技师需要手动在预备体表面定义精确的边缘线，这是一个不可重复且不一致的过程。这项工作提出了一种新的框架，使用深度学习自动且准确地确定边缘线。合作牙科实验室提供了门牙数据集来训练深度学习分割模型。通过更改输入通道修改了基于网格的神经网络，用于将预备体分割为两个区域，使边缘线包含在分离两个区域的边界面上。接下来，使用k折交叉验证训练了5个模型，并采用投票分类器技术组合它们的结果以提高分割效果。之后，使用图割方法应用边界平滑和优化来细化分割结果。然后，选择分离两个区域的边界面代表边缘线面。使用样条曲线近似拟合边界面的中心点来预测边缘线。我们的结果表明，结合最大概率的集成模型预测了最多成功的测试案例（13个中7个），基于预测真实点云之间最大距离阈值为200微米（代表人为误差）。同时证明，预备体质量越好，预测边缘线与真实边缘线之间的差异越小（斯皮尔曼等级相关系数为-0.683）。我们为社区提供了训练和测试数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dental crowns are essential dental treatments for restoring damaged ormissing teeth of patients. Recent design approaches of dental crowns arecarried out using commercial dental design software. Once a scan of apreparation is uploaded to the software, a dental technician needs to manuallydefine a precise margin line on the preparation surface, which constitutes anon-repeatable and inconsistent procedure. This work proposes a new frameworkto determine margin lines automatically and accurately using deep learning. Adataset of incisor teeth was provided by a collaborating dental laboratory totrain a deep learning segmentation model. A mesh-based neural network wasmodified by changing its input channels and used to segment the prepared toothinto two regions such that the margin line is contained within the boundaryfaces separating the two regions. Next, k-fold cross-validation was used totrain 5 models, and a voting classifier technique was used to combine theirresults to enhance the segmentation. After that, boundary smoothing andoptimization using the graph cut method were applied to refine the segmentationresults. Then, boundary faces separating the two regions were selected torepresent the margin line faces. A spline was approximated to best fit thecenters of the boundary faces to predict the margin line. Our results show thatan ensemble model combined with maximum probability predicted the highestnumber of successful test cases (7 out of 13) based on a maximum distancethreshold of 200 m (representing human error) between the predicted and groundtruth point clouds. It was also demonstrated that the better the quality of thepreparation, the smaller the divergence between the predicted and ground truthmargin lines (Spearman's rank correlation coefficient of -0.683). We providethe train and test datasets for the community.</description>
      <author>example@mail.com (Ammar Alsheghri, Ying Zhang, Farnoosh Ghadiri, Julia Keren, Farida Cheriet, Francois Guibault)</author>
      <guid isPermaLink="false">2507.22859v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques</title>
      <link>http://arxiv.org/abs/2507.22791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面回顾了基于模态的特征匹配技术，从传统手工方法到当代深度学习方法，涵盖多种数据模态的应用与发展。&lt;h4&gt;背景&lt;/h4&gt;特征匹配是计算机视觉的基础任务，对图像检索、立体匹配、3D重建和SLAM等应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;探索并总结不同模态下的特征匹配方法，分析传统方法与深度学习方法的优缺点，以及跨模态应用的最新进展。&lt;h4&gt;方法&lt;/h4&gt;分析了RGB图像、深度图像、3D点云、LiDAR扫描、医学图像和视觉语言交互等多种模态下的特征匹配方法，包括传统手工方法和深度学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;传统方法如Harris角点检测器和SIFT、ORB描述符在适度的模态内变化下表现鲁棒，但难以处理显著模态差距；深度学习方法如SuperPoint和LoFTR显著提高了跨模态鲁棒性；模态特定解决方案如深度图像的几何描述符、3D点云的稀疏和密集学习方法、LiDAR的注意力增强网络以及医学图像的MIND描述符等展现了专业领域的进步。&lt;h4&gt;结论&lt;/h4&gt;特征匹配技术已从传统方法发展到深度学习方法，能够处理日益多样化的数据交互和模态差距，特别是在医学图像配准和视觉语言任务中展现出强大潜力。&lt;h4&gt;翻译&lt;/h4&gt;特征匹配是计算机视觉中的基石任务，对图像检索、立体匹配、3D重建和SLAM等应用至关重要。本综述全面回顾了基于模态的特征匹配，探索了传统手工方法和当代深度学习方法，涵盖RGB图像、深度图像、3D点云、LiDAR扫描、医学图像和视觉语言交互等多种模态。传统方法利用Harris角点等检测器和SIFT、ORB等描述符，在适度的模态内变化下表现出鲁棒性，但在处理显著的模态差距时存在困难。当代基于深度学习的方法，如基于CNN的SuperPoint和基于Transformer的LoFTR等无检测器策略，显著提高了跨模态的鲁棒性和适应性。我们强调了模态感知的进步，如深度图像的几何和特定深度描述符、3D点云的稀疏和密集学习方法、LiDAR扫描的注意力增强神经网络，以及医学图像匹配的MIND描述符等专门解决方案。跨模态应用，特别是在医学图像配准和视觉语言任务中，突显了特征匹配处理日益多样化数据交互的演变。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决特征匹配技术在处理不同模态数据时的适应性和有效性问题。特征匹配是计算机视觉的基础任务，对图像检索、立体匹配、3D重建和SLAM等应用至关重要。这个问题在现实中非常重要，因为现实世界中的数据往往来自不同传感器或模态，需要有效匹配这些不同模态的特征；不同模态数据之间存在差异（如RGB图像与深度图像、不同医学成像技术）；传统特征匹配方法在处理跨模态数据时效果有限；随着深度学习的发展，需要系统性地总结和评估不同模态下的特征匹配方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这是一篇综述性论文，作者的主要工作是系统性地组织和回顾现有的单模态和跨模态特征匹配技术，并按照数据模态（RGB图像、3D数据、医学图像、视觉语言）对方法进行分类，分析不同方法的优缺点和适用场景。作者借鉴了大量现有工作，包括传统的手工设计特征方法（如SIFT、SURF、ORB等）、基于深度学习的特征匹配方法（如SuperPoint、LoFTR等）、3D数据特征匹配方法（如Spin Images、FPFH、3DMatch等）、医学图像配准方法（如MI、NMI、VoxelMorph等）以及视觉语言匹配方法（如CLIP、ALIGN等）。作者对这些现有工作进行了系统性的整理和比较，但没有提出全新的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为一篇综述论文，没有单一的核心思想和实现流程。论文总结了不同模态下特征匹配的核心思想和实现流程：对于RGB图像特征匹配，传统方法遵循特征检测→特征描述→特征匹配的流程，而深度学习方法则学习特征检测器和描述器，然后使用注意力机制进行匹配或直接进行端到端的匹配；对于3D数据特征匹配，传统方法基于几何描述符，而深度学习方法使用点云网络或稀疏3D CNN学习特征描述；对于医学图像特征匹配，传统方法基于强度或特征，而深度学习方法使用空间变换网络或变形场预测；对于视觉语言特征匹配，使用双编码器模型将图像和文本映射到共享空间，或使用Transformer模型进行视觉和语言的精确对齐。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 作为一篇综述论文，其创新点在于组织和呈现方式，而非技术方法创新。论文的关键创新点包括：1) 按模态分类的组织方式，不同于以往按算法框架分类；2) 全面覆盖视觉语言匹配这一快速发展领域；3) 提供深入的跨模态比较评估，强调从传统手工技术到复杂深度学习解决方案的演变过程。相比之前的工作，本文的独特之处在于更全面的模态覆盖、对新兴的视觉语言匹配领域的专门关注、系统性地分析不同模态下的特征匹配技术演进，以及提供了丰富的基准测试数据集和评估协议的概述。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文系统地综述和比较了不同数据模态下的单模态和跨模态特征匹配技术，从传统手工方法到现代深度学习解决方案，为特征匹配领域提供了全面的视角和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature matching is a cornerstone task in computer vision, essential forapplications such as image retrieval, stereo matching, 3D reconstruction, andSLAM. This survey comprehensively reviews modality-based feature matching,exploring traditional handcrafted methods and emphasizing contemporary deeplearning approaches across various modalities, including RGB images, depthimages, 3D point clouds, LiDAR scans, medical images, and vision-languageinteractions. Traditional methods, leveraging detectors like Harris corners anddescriptors such as SIFT and ORB, demonstrate robustness under moderateintra-modality variations but struggle with significant modality gaps.Contemporary deep learning-based methods, exemplified by detector-freestrategies like CNN-based SuperPoint and transformer-based LoFTR, substantiallyimprove robustness and adaptability across modalities. We highlightmodality-aware advancements, such as geometric and depth-specific descriptorsfor depth images, sparse and dense learning methods for 3D point clouds,attention-enhanced neural networks for LiDAR scans, and specialized solutionslike the MIND descriptor for complex medical image matching. Cross-modalapplications, particularly in medical image registration and vision-languagetasks, underscore the evolution of feature matching to handle increasinglydiverse data interactions.</description>
      <author>example@mail.com (Weide Liu, Wei Zhou, Jun Liu, Ping Hu, Jun Cheng, Jungong Han, Weisi Lin)</author>
      <guid isPermaLink="false">2507.22791v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation</title>
      <link>http://arxiv.org/abs/2507.22668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 11 figures, to be published in ACMMM 2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种图引导的数据增强框架，具有双级约束用于真实的3D场景合成，解决了现有方法缺乏对场景全局结构依赖性考虑的问题&lt;h4&gt;背景&lt;/h4&gt;3D点云分割旨在为场景中的各个点分配语义标签，以实现细粒度的空间理解。现有方法通常采用数据增强来减轻大规模标注的负担&lt;h4&gt;目的&lt;/h4&gt;解决现有数据增强策略只关注局部变换或语义重组，缺乏对场景内全局结构依赖性考虑的局限性&lt;h4&gt;方法&lt;/h4&gt;提出图引导的数据增强框架，从真实世界数据学习对象关系统计构建引导图，应用局部级约束确保对象间几何合理性和语义一致性，使用全局级约束保持场景拓扑结构&lt;h4&gt;主要发现&lt;/h4&gt;在室内和室外数据集上的实验表明，该框架生成了多样且高质量的增强场景，在各种模型的点云分割性能上取得了一致的改进&lt;h4&gt;结论&lt;/h4&gt;所提出的图引导数据增强框架能有效解决现有方法在考虑场景全局结构依赖性方面的不足，显著提高3D点云分割的性能&lt;h4&gt;翻译&lt;/h4&gt;三维点云分割旨在为场景中的单个点分配语义标签，以实现细粒度的空间理解。现有方法通常采用数据增强来减轻大规模标注的负担。然而，大多数增强策略只关注局部变换或语义重组，缺乏对场景内全局结构依赖性的考虑。为解决这一局限性，我们提出了一种具有双级约束的图引导数据增强框架，用于真实的三维场景合成。我们的方法从真实世界数据中学习对象关系统计，构建场景生成的引导图。局部级约束强制执行对象之间的几何合理性和语义一致性，而全局级约束通过将生成的布局与引导图对齐来保持场景的拓扑结构。在室内和室外数据集上的大量实验表明，我们的框架生成了多样且高质量的增强场景，在各种模型的点云分割性能上取得了持续的改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云分割中数据增强方法的局限性，即现有增强策略只关注局部变换或语义重组，而忽略了场景内部的全局结构依赖关系。这个问题很重要，因为3D点云分割是机器人导航、自动驾驶等应用的基础任务，而获取大规模标注数据成本高昂；同时，真实世界的3D场景遵循特定的物理和语义规律，缺乏全局结构考虑会导致增强样本与现实场景不符，影响模型在实际应用中的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云增强技术的局限性，发现几何级方法无法生成新场景布局，语义级方法难以保持一致性和真实性。核心洞察是真实3D场景中物体间的空间关系遵循特定规律。方法设计包括构建对象关系图(ORG)建模物体共现统计，设计双层约束机制(局部几何语义约束和全局拓扑约束)，并使用图神经网络对齐场景结构。该方法借鉴了场景图概念、JS散度、图神经网络和现有点云分割模型架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双层约束机制生成真实且多样化的3D点云场景，其中局部级约束确保物体间的几何合理性和语义一致性，全局级约束保持场景拓扑结构。整体流程包括：1)场景分解和物体提取，将场景分为背景和前景组件；2)构建对象关系图(ORG)建模物体关系；3)应用局部几何约束(碰撞避免和表面对齐)；4)应用局部语义约束(七种空间关系的损失函数)；5)应用全局拓扑约束(图神经网络嵌入和GGCL损失)；6)迭代优化物体姿态，最小化总损失函数，生成最终场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)图引导的双层约束框架，结合局部和全局约束；2)对象关系图(ORG)建模物体共现统计和空间关系；3)图全局约束损失(GGCL)对齐生成场景与引导图结构。相比之前工作，本文超越了简单的局部变换，能生成全新场景布局；超越了语义级重组，确保语义一致性和物理真实性；考虑了全局结构依赖，保持场景拓扑关系；并建模了更丰富的七种空间关系，实现更精确的约束。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种图引导的双层约束框架，通过结合局部几何语义约束和全局拓扑结构对齐，生成真实且多样化的3D点云场景，显著提升了点云分割模型的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755547&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D point cloud segmentation aims to assign semantic labels to individualpoints in a scene for fine-grained spatial understanding. Existing methodstypically adopt data augmentation to alleviate the burden of large-scaleannotation. However, most augmentation strategies only focus on localtransformations or semantic recomposition, lacking the consideration of globalstructural dependencies within scenes. To address this limitation, we propose agraph-guided data augmentation framework with dual-level constraints forrealistic 3D scene synthesis. Our method learns object relationship statisticsfrom real-world data to construct guiding graphs for scene generation.Local-level constraints enforce geometric plausibility and semantic consistencybetween objects, while global-level constraints maintain the topologicalstructure of the scene by aligning the generated layout with the guiding graph.Extensive experiments on indoor and outdoor datasets demonstrate that ourframework generates diverse and high-quality augmented scenes, leading toconsistent improvements in point cloud segmentation performance across variousmodels.</description>
      <author>example@mail.com (Hongbin Lin, Yifan Jiang, Juangui Xu, Jesse Jiaxi Xu, Yi Lu, Zhengyu Hu, Ying-Cong Chen, Hao Wang)</author>
      <guid isPermaLink="false">2507.22668v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Recognizing Actions from Robotic View for Natural Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2507.22522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, Accepted to ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了ACTIVE数据集和ACTIVE-PC方法，专门针对自然人机交互(N-HRI)中的动作识别挑战。ACTIVE是一个大规模数据集，包含30个复合动作类别、80名参与者和46,868个带注释的视频实例，覆盖RGB和点云两种模态。ACTIVE-PC方法通过多层次邻域采样、分层识别器、弹性椭圆查询以及精确解耦运动学干扰来实现远距离人类动作的准确感知。&lt;h4&gt;背景&lt;/h4&gt;自然人机交互(N-HRI)要求机器人能够识别不同距离和状态下的人类动作，无论机器人本身是运动还是静止的，这比传统的人类动作识别任务更灵活实用。然而，现有的传统动作识别基准数据集无法解决N-HRI中的独特复杂性，存在数据量、模态、任务类别以及受试者和环境多样性方面的限制。&lt;h4&gt;目的&lt;/h4&gt;为了解决N-HRI中数据集的不足，作者旨在创建一个专门为移动服务机器人感知视角设计的大规模数据集，并开发一种能够准确感知远距离人类动作的方法。&lt;h4&gt;方法&lt;/h4&gt;构建了ACTIVE数据集，参与者在不同距离(3米至50米)的多样化环境中执行各种人类动作，同时相机平台也是移动的，模拟了由于地面不平导致的相机高度变化。提出了ACTIVE-PC方法，使用多层次邻域采样、分层识别器、弹性椭圆查询以及精确解耦人类动作的运动学干扰来提高远距离动作识别的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明了ACTIVE-PC方法在远距离人类动作识别方面的有效性。ACTIVE数据集作为一个全面且具有挑战性的基准，为N-HRI中的动作和属性识别研究提供了新的资源。&lt;h4&gt;结论&lt;/h4&gt;ACTIVE数据集和ACTIVE-PC方法为N-HRI中的动作识别研究提供了新的基准和解决方案，有助于推进该领域的发展，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;自然人机交互(N-HRI)要求机器人能够识别不同距离和状态下的人类动作，无论机器人本身是运动还是静止的。这种设置比传统的人类动作识别任务更灵活和实用。然而，现有的为传统动作识别设计的基准数据集无法解决N-HRI中的独特复杂性，因为这些数据集在数据量、模态、任务类别以及受试者和环境的多样性方面存在限制。为了解决这些挑战，我们引入了ACTIVE（机器人视角中的动作），这是一个专门为移动服务机器人中普遍存在的以感知为中心的机器人视角量身定制的大规模数据集。ACTIVE包含30个复合动作类别、80名参与者和46,868个带注释的视频实例，覆盖RGB和点云两种模态。参与者在3米到50米的不同距离的多样化环境中执行各种人类动作，同时相机平台也是移动的，模拟了由于地面不平而导致的相机高度变化的现实世界场景。这个全面且具有挑战性的基准旨在推动N-HRI中的动作和属性识别研究。此外，我们提出了ACTIVE-PC方法，使用多层次邻域采样、分层识别器、弹性椭圆查询以及精确解耦人类动作的运动学干扰来准确感知远距离的人类动作。实验结果证明了ACTIVE-PC方法的有效性。我们的代码可在以下网址获取：https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是自然人机交互(N-HRI)场景中的人体动作识别问题。现有数据集和方法无法满足N-HRI的需求，因为它们主要针对固定摄像头、固定距离和背景的场景，而N-HRI需要机器人识别不同距离(3-50米)和状态下的人体动作，无论机器人自身是移动还是静止的。这个问题很重要，因为N-HRI在服务机器人、公共安全监控和医疗诊断等领域有广泛应用，机器人需要实时适应变化，理解人体动作并考虑人类位置变化和自身运动。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集和方法在N-HRI场景中的局限性，包括样本量有限、固定捕获模式、受限场景、有限距离和有限模态。针对这些问题，作者设计了ACTIVE数据集，包含30个复合动作类别、80名参与者和46,868个视频实例，覆盖RGB和点云两种模态。方法设计上，作者借鉴了现有点云处理技术如P4Transformer和PST-Transformer，但进行了改进和创新，包括多级邻域采样(MNS)、分层识别器(LR)和弹性椭圆查询(EEQ)，以适应N-HRI场景的特殊需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分层处理和多级采样解决N-HRI场景中的两个主要挑战：长距离下人体动作细微且快速难以捕捉，以及动态交互场景中大规模运动干扰增加识别难度。整体流程是：1)输入点云视频序列；2)应用多级邻域采样生成不同密度的点云；3)对每个点云应用弹性椭圆查询进行自适应邻域查询，得到特征图；4)将低密度点云输入运动解释器处理人体-机器人状态理解；5)将高密度点云输入动作识别器处理人体动作识别；6)融合两个分支的输出得到最终的动作分数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ACTIVE数据集：首个专注于N-HRI的大规模数据集，包含30个复合动作类别、80名参与者和46,868个视频实例，覆盖RGB和点云两种模态；2)多级邻域采样(MNS)：基于局部邻域约束构建层次依赖关系，同时确保全局覆盖和保留局部细节；3)分层识别器(LR)：将不同层次特征发送到专门子网络，分别处理人体动作和运动干扰；4)弹性椭圆查询(EEQ)：引入轴特定自适应缩放处理不同交互距离。相比之前工作，这篇论文专注于N-HRI场景的特殊需求，同时处理长距离识别和运动干扰两个挑战，结合RGB和点云两种模态优势，提出了专门针对机器人视角的数据集和方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了首个专注于自然人机交互场景的大规模动作识别数据集ACTIVE，并开发了能有效处理长距离识别和运动干扰的ACTIVE-PC方法，显著提升了机器人视角下的人体动作识别性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Natural Human-Robot Interaction (N-HRI) requires robots to recognize humanactions at varying distances and states, regardless of whether the robot itselfis in motion or stationary. This setup is more flexible and practical thanconventional human action recognition tasks. However, existing benchmarksdesigned for traditional action recognition fail to address the uniquecomplexities in N-HRI due to limited data, modalities, task categories, anddiversity of subjects and environments. To address these challenges, weintroduce ACTIVE (Action from Robotic View), a large-scale dataset tailoredspecifically for perception-centric robotic views prevalent in mobile servicerobots. ACTIVE comprises 30 composite action categories, 80 participants, and46,868 annotated video instances, covering both RGB and point cloud modalities.Participants performed various human actions in diverse environments atdistances ranging from 3m to 50m, while the camera platform was also mobile,simulating real-world scenarios of robot perception with varying camera heightsdue to uneven ground. This comprehensive and challenging benchmark aims toadvance action and attribute recognition research in N-HRI. Furthermore, wepropose ACTIVE-PC, a method that accurately perceives human actions at longdistances using Multilevel Neighborhood Sampling, Layered Recognizers, ElasticEllipse Query, and precise decoupling of kinematic interference from humanactions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Ourcode is available at:https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.</description>
      <author>example@mail.com (Ziyi Wang, Peiming Li, Hong Liu, Zhichao Deng, Can Wang, Jun Liu, Junsong Yuan, Mengyuan Liu)</author>
      <guid isPermaLink="false">2507.22522v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>UAVScenes: A Multi-Modal Dataset for UAVs</title>
      <link>http://arxiv.org/abs/2507.22412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UAVScenes，一个大规模多模态无人机感知数据集，支持2D和3D模态的各种基准任务，填补了现有数据集在高级场景理解任务中的空白。&lt;h4&gt;背景&lt;/h4&gt;多模态感知对无人机操作至关重要，但现有多模态无人机数据集主要偏向定位和3D重建任务，或仅支持地图级语义分割，缺乏相机图像和LiDAR点云的逐帧注释，限制了其在高级场景理解任务中的应用。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白并推动多模态无人机感知发展，作者引入UAVScenes数据集，旨在为2D和3D模态的各种任务提供基准。&lt;h4&gt;方法&lt;/h4&gt;该数据集基于经过校准的多模态无人机数据集MARS-LVIG（原用于同时定位与地图构建SLAM），通过提供逐帧图像和LiDAR点云的手动标注语义注释，以及准确的6自由度姿态信息进行增强。&lt;h4&gt;主要发现&lt;/h4&gt;增强后的数据集支持广泛的无人机感知任务，包括分割、深度估计、6自由度定位、位置识别和新视图合成。&lt;h4&gt;结论&lt;/h4&gt;UAVScenes数据集现已公开，可通过https://github.com/sijieaaa/UAVScenes获取，为无人机多模态感知研究提供了新资源。&lt;h4&gt;翻译&lt;/h4&gt;多模态感知对无人机操作至关重要，因为它能够全面理解无人机周围环境。然而，大多数现有的多模态无人机数据集主要偏向定位和3D重建任务，或由于缺乏相机图像和LiDAR点云的逐帧注释，仅支持地图级语义分割。这一限制阻止了它们在高级场景理解任务中的应用。为了填补这一空白并推动多模态无人机感知发展，我们引入了UAVScenes，一个大规模数据集，旨在为2D和3D模态的各种任务提供基准。我们的基准数据集建立在经过良好校准的多模态无人机数据集MARS-LVIG上，该数据集最初仅用于同时定位与地图构建。我们通过为逐帧图像和LiDAR点云提供手动标注的语义注释，以及准确的6自由度姿态，增强了该数据集。这些新增功能使得广泛的无人机感知任务成为可能，包括分割、深度估计、6自由度定位、位置识别和新视图合成。我们的数据集可在https://github.com/sijieaaa/UAVScenes获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了现有多模态无人机数据集缺乏帧级语义标注的问题，使其无法支持高级场景理解任务。这个问题很重要，因为随着低空经济发展，无人机在空中出租车、物流、农业等领域应用广泛，需要高质量数据集进行可靠的环境感知训练，而现有数据集要么只关注SLAM和3D重建，要么只提供地图级标注，限制了实时场景理解和精确操作的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有无人机数据集的局限性，选择了MARS-LVIG作为基础数据集，因为它提供了多样化的环境数据。他们借鉴了SemanticKITTI的标注方法将数据集分为8个不同分割，使用SfM解决方案重建6-DoF姿态，并采用X-AnyLabeling工具辅助标注。整体设计思路是在保持原始数据集多样性的基础上，添加完整的语义标注和精确姿态信息，以支持更广泛的感知任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过提供大规模、多模态的无人机感知数据集，包含精确对齐的帧级图像和LiDAR点云语义标注以及6-DoF姿态，支持多种无人机感知任务。实现流程包括：1)基于MARS-LVIG构建数据集；2)使用DJI Terra进行3D重建和6-DoF姿态估计；3)对静态场景进行3D地图标注并渲染到2D图像；4)手动标注动态对象；5)利用相机-LiDAR校准将图像标注投影到点云；6)进行一致性检查和修正；7)建立多种感知任务的基准测试。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提供120k帧以上的完整帧级图像和LiDAR点云语义标注；2)重建精确的6-DoF姿态而非仅4-DoF；3)支持至少六种不同的感知任务；4)专注于开源Livox-Avia LiDAR数据。与之前工作不同，UAVScenes是首个同时提供帧级图像和LiDAR标注、6-DoF姿态以及3D地图的数据集，突破了现有数据集在任务覆盖和标注完整性方面的限制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UAVScenes是一个大规模多模态无人机感知数据集，提供了精确对齐的帧级图像和LiDAR点云语义标注以及6-DoF姿态，支持多种无人机感知任务的研究和基准测试。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal perception is essential for unmanned aerial vehicle (UAV)operations, as it enables a comprehensive understanding of the UAVs'surrounding environment. However, most existing multi-modal UAV datasets areprimarily biased toward localization and 3D reconstruction tasks, or onlysupport map-level semantic segmentation due to the lack of frame-wiseannotations for both camera images and LiDAR point clouds. This limitationprevents them from being used for high-level scene understanding tasks. Toaddress this gap and advance multi-modal UAV perception, we introduceUAVScenes, a large-scale dataset designed to benchmark various tasks acrossboth 2D and 3D modalities. Our benchmark dataset is built upon thewell-calibrated multi-modal UAV dataset MARS-LVIG, originally developed onlyfor simultaneous localization and mapping (SLAM). We enhance this dataset byproviding manually labeled semantic annotations for both frame-wise images andLiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.These additions enable a wide range of UAV perception tasks, includingsegmentation, depth estimation, 6-DoF localization, place recognition, andnovel view synthesis (NVS). Our dataset is available athttps://github.com/sijieaaa/UAVScenes</description>
      <author>example@mail.com (Sijie Wang, Siqi Li, Yawei Zhang, Shangshu Yu, Shenghai Yuan, Rui She, Quanjiang Guo, JinXuan Zheng, Ong Kang Howe, Leonrich Chandra, Shrivarshann Srijeyan, Aditya Sivadas, Toshan Aggarwal, Heyuan Liu, Hongming Zhang, Chujie Chen, Junyu Jiang, Lihua Xie, Wee Peng Tay)</author>
      <guid isPermaLink="false">2507.22412v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors</title>
      <link>http://arxiv.org/abs/2507.21872v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MultiEditor，一个用于自动驾驶场景中联合编辑图像和LiDAR点云的双分支潜在扩散框架，解决了现实世界数据长尾分布导致的泛化问题，特别是针对罕见但安全关键的车辆类别。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统严重依赖多模态感知数据来理解复杂环境，但现实世界数据的长尾分布阻碍了系统的泛化能力，特别是对于罕见但对安全至关重要的车辆类别。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来增强自动驾驶系统在处理罕见车辆类别时的感知能力，解决数据长尾分布带来的泛化问题。&lt;h4&gt;方法&lt;/h4&gt;提出MultiEditor框架，核心是引入3D高斯溅射(3DGS)作为目标物体的结构和外观先验，设计多级外观控制机制（像素级粘贴、语义级引导和多分支细化）实现跨模态高保真重建，同时提出深度引导的可变形跨模态条件模块增强模态间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现优越；使用MultiEditor生成罕见类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;结论&lt;/h4&gt;MultiEditor有效解决了自动驾驶系统中因数据长尾分布导致的泛化问题，特别是对罕见安全关键车辆类别的处理，通过联合编辑图像和LiDAR点云数据，增强了系统的感知能力和准确性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶系统严重依赖多模态感知数据来理解复杂环境。然而，现实世界数据的长尾分布阻碍了泛化能力，特别是对于罕见但对安全至关重要的车辆类别。为应对这一挑战，我们提出了MultiEditor，一个专为驾驶场景中联合编辑图像和LiDAR点云而设计的双分支潜在扩散框架。我们方法的核心是引入3D高斯溅射(3DGS)作为目标物体的结构和外观先验。利用这一先验，我们设计了一个多级外观控制机制，包括像素级粘贴、语义级引导和多分支细化，以实现跨模态的高保真重建。我们还提出了一个深度引导的可变形跨模态条件模块，该模块使用3DGS渲染的深度自适应地实现模态间的相互引导，显著增强了跨模态一致性。大量实验证明，MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现出色。此外，使用MultiEditor生成罕见类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中多模态感知数据的编辑问题，特别是针对驾驶场景中图像和LiDAR点云的联合编辑。这个问题在现实中非常重要，因为自动驾驶系统高度依赖多模态数据来理解复杂环境，但真实世界数据存在'长尾分布'问题——常见车辆类别数据过多，而罕见但安全关键的车辆类别（如压路机、挖掘机）数据严重不足，这会阻碍模型的泛化能力，影响感知模型在处理长尾和边缘情况时的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到自动驾驶数据的长尾分布问题难以通过现有方法解决，发现单模态编辑方法无法保证图像和点云之间的一致性。他们借鉴了多种现有技术：3D Gaussian Splatting作为物体结构和外观的统一先验；潜在扩散模型用于场景合成；Segment Anything Model和Grounding DINO用于目标检测和分割；CLIP用于语义特征提取；变分自编码器用于数据压缩；以及变形注意力机制用于跨模态交互。作者将这些技术整合到一个新的框架中，并创新性地设计了多级控制机制和跨模态条件模块来解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D Gaussian Splatting作为目标物体的统一先验，通过双分支潜在扩散框架分别处理图像和点云数据，并引入多级外观控制机制和深度引导的跨模态条件模块来确保编辑质量和模态间一致性。整体流程包括：1)数据准备，使用KITTI数据集构建专用训练数据；2)双分支处理，图像分支和点云分支分别应用扩散模型；3)跨模态条件，利用3DGS渲染的深度图实现模态间相互引导；4)多级控制，包括像素级保留、语义级引导和多分支优化；5)分阶段训练VAE和扩散模型，最后进行端到端联合训练；6)生成编辑后的图像和点云，确保多模态一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个双分支多模态潜在扩散框架，将3DGS作为统一先验；2)深度引导的变形跨模态条件模块，增强模态间一致性；3)多级外观控制机制，实现像素级、语义级和分支级优化。相比之前工作，MultiEditor是一个统一的多模态框架而非简单组合单模态方法；它利用3DGS提供更灵活的物体姿态控制；通过深度引导机制确保跨模态一致性；能够处理罕见车辆类别；并提供更精细的物体级编辑控制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MultiEditor首次将3D高斯溅射作为统一先验，通过双分支潜在扩散框架和多级控制机制，实现了自动驾驶场景中图像和LiDAR点云的高保真、一致且可控的联合编辑，有效解决了罕见车辆类别数据不足的问题，提升了感知模型在长尾场景中的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving systems rely heavily on multimodal perception data tounderstand complex environments. However, the long-tailed distribution ofreal-world data hinders generalization, especially for rare but safety-criticalvehicle categories. To address this challenge, we propose MultiEditor, adual-branch latent diffusion framework designed to edit images and LiDAR pointclouds in driving scenarios jointly. At the core of our approach is introducing3D Gaussian Splatting (3DGS) as a structural and appearance prior for targetobjects. Leveraging this prior, we design a multi-level appearance controlmechanism--comprising pixel-level pasting, semantic-level guidance, andmulti-branch refinement--to achieve high-fidelity reconstruction acrossmodalities. We further propose a depth-guided deformable cross-modalitycondition module that adaptively enables mutual guidance between modalitiesusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.Extensive experiments demonstrate that MultiEditor achieves superiorperformance in visual and geometric fidelity, editing controllability, andcross-modality consistency. Furthermore, generating rare-category vehicle datawith MultiEditor substantially enhances the detection accuracy of perceptionmodels on underrepresented classes.</description>
      <author>example@mail.com (Shouyi Lu, Zihan Lin, Chao Lu, Huanran Wang, Guirong Zhuo, Lianqing Zheng)</author>
      <guid isPermaLink="false">2507.21872v2</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking</title>
      <link>http://arxiv.org/abs/2507.22421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的时空建模框架，通过引入分层注意力机制，实现了高效的动作识别和目标跟踪，在保持实时性能的同时提高了准确率。&lt;h4&gt;背景&lt;/h4&gt;实时视频分析在计算机视觉中面临挑战，需要高效处理时空信息并保持计算效率，现有方法难以在准确性和速度间取得平衡，特别是在资源受限环境下。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架，利用先进的时空建模技术，同时实现动作识别和目标跟踪，并平衡准确性和速度。&lt;h4&gt;方法&lt;/h4&gt;基于并行序列建模的最新进展，引入了一种新的分层注意力机制，能够自适应地关注时间序列中的相关空间区域。&lt;h4&gt;主要发现&lt;/h4&gt;在UCF-101、HMDB-51和MOT17数据集上的实验表明，与现有方法相比，动作识别准确率提高了3.2%，跟踪精度提高了2.8%，推理时间快了40%，同时保持了实时性能。&lt;h4&gt;结论&lt;/h4&gt;提出的统一框架通过分层注意力机制，成功地在保持实时性能的同时提高了动作识别和目标跟踪的准确性，解决了现有方法在准确性和速度间的平衡问题。&lt;h4&gt;翻译&lt;/h4&gt;实时视频分析在计算机视觉中仍然是一个具有挑战性的问题，需要高效处理空间和时间信息，同时保持计算效率。现有方法往往难以平衡准确性和速度，特别是在资源受限的环境中。在这项工作中，我们提出了一个统一框架，利用先进的时空建模技术同时实现动作识别和目标跟踪。我们的方法基于并行序列建模的最新进展，并引入了一种新的分层注意力机制，能够自适应地关注时间序列中的相关空间区域。我们证明，我们的方法在标准基准测试上取得了最先进的性能，同时保持实时推理速度。在UCF-101、HMDB-51和MOT17数据集上的大量实验显示，与现有方法相比，动作识别准确率提高了3.2%，跟踪精度提高了2.8%，推理时间快了40%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time video analysis remains a challenging problem in computer vision,requiring efficient processing of both spatial and temporal information whilemaintaining computational efficiency. Existing approaches often struggle tobalance accuracy and speed, particularly in resource-constrained environments.In this work, we present a unified framework that leverages advancedspatial-temporal modeling techniques for simultaneous action recognition andobject tracking. Our approach builds upon recent advances in parallel sequencemodeling and introduces a novel hierarchical attention mechanism thatadaptively focuses on relevant spatial regions across temporal sequences. Wedemonstrate that our method achieves state-of-the-art performance on standardbenchmarks while maintaining real-time inference speeds. Extensive experimentson UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in actionrecognition accuracy and 2.8% in tracking precision compared to existingmethods, with 40% faster inference time.</description>
      <author>example@mail.com (Shahla John)</author>
      <guid isPermaLink="false">2507.22421v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>ST-GDance: Long-Term and Collision-Free Group Choreography from Music</title>
      <link>http://arxiv.org/abs/2507.21518v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures. Accepted at BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ST-GDance框架，用于从音乐生成群舞，解决了多舞者同步和空间协调问题，通过解耦空间和时间依赖关系，使用轻量级图卷积和加速稀疏注意力，实现了高效且无碰撞的群舞生成。&lt;h4&gt;背景&lt;/h4&gt;群舞生成在电影、游戏和动画制作中有广泛应用，但需要同步多个舞者并保持空间协调。随着舞者数量和序列长度增加，任务面临更高的计算复杂性和运动碰撞风险。现有方法难以建模密集的时空交互，导致可扩展性问题和多舞者碰撞。&lt;h4&gt;目的&lt;/h4&gt;解决群舞生成中的计算复杂性和碰撞问题，特别是处理大量舞者和长序列时的时空交互建模，实现高效、连贯且无碰撞的群舞编排。&lt;h4&gt;方法&lt;/h4&gt;提出了ST-GDance框架，该框架解耦了空间和时间依赖关系。使用轻量级图卷积进行距离感知的空间建模，使用加速稀疏注意力进行高效的时间建模。这种设计显著降低了计算成本，同时确保了流畅且无碰撞的交互。&lt;h4&gt;主要发现&lt;/h4&gt;在AIOZ-GDance数据集上的实验表明，ST-GDance优于最先进的基线方法，特别是在生成长且连贯的群舞序列方面。&lt;h4&gt;结论&lt;/h4&gt;ST-GDance框架通过解耦时空依赖关系，有效解决了群舞生成中的计算复杂性和碰撞问题，为电影、游戏和动画制作提供了更高效的群舞生成解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从音乐生成群舞在电影、游戏和动画制作中有广泛的应用。然而，它需要同步多个舞者同时保持空间协调。随着舞者数量和序列长度的增加，这项任务面临更高的计算复杂性和更大的运动碰撞风险。现有方法往往难以建模密集的时空交互，导致可扩展性问题和多舞者碰撞。为了解决这些挑战，我们提出了ST-GDance，这是一个新颖的框架，它解耦了空间和时间依赖关系，以优化长期且无碰撞的群舞编排。我们使用轻量级图卷积进行距离感知的空间建模，并使用加速稀疏注意力进行高效的时间建模。这种设计显著降低了计算成本，同时确保了流畅且无碰撞的交互。在AIOZ-GDance数据集上的实验表明，ST-GDance优于最先进的基线方法，特别是在生成长且连贯的群舞序列方面。项目页面：https://yilliajing.github.io/ST-GDance-Website/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Group dance generation from music has broad applications in film, gaming, andanimation production. However, it requires synchronizing multiple dancers whilemaintaining spatial coordination. As the number of dancers and sequence lengthincrease, this task faces higher computational complexity and a greater risk ofmotion collisions. Existing methods often struggle to model densespatial-temporal interactions, leading to scalability issues and multi-dancercollisions. To address these challenges, we propose ST-GDance, a novelframework that decouples spatial and temporal dependencies to optimizelong-term and collision-free group choreography. We employ lightweight graphconvolutions for distance-aware spatial modeling and accelerated sparseattention for efficient temporal modeling. This design significantly reducescomputational costs while ensuring smooth and collision-free interactions.Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperformsstate-of-the-art baselines, particularly in generating long and coherent groupdance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.</description>
      <author>example@mail.com (Jing Xu, Weiqiang Wang, Cunjian Chen, Jun Liu, Qiuhong Ke)</author>
      <guid isPermaLink="false">2507.21518v2</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding</title>
      <link>http://arxiv.org/abs/2507.22378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于transformer的模型STDA-SwiFT，通过时空分离注意力和自监督对比学习从大规模fMRI数据集中学习可转移的表示，显著提高了从fMRI数据解码脑活动的性能。&lt;h4&gt;背景&lt;/h4&gt;神经科学中从脑活动解码心理状态是一个基本挑战，fMRI虽能高空间精度捕捉全脑神经动力学，但因其高维度性、低信噪比和有限受试者内数据，从fMRI数据中解码（尤其是任务诱导活动）仍然困难。&lt;h4&gt;目的&lt;/h4&gt;利用大规模fMRI数据集学习可转移的表示，以克服从fMRI数据解码脑活动的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出STDA-SwiFT模型，一种基于transformer的模型，通过时空分离注意力和自监督对比学习从大规模fMRI数据集学习可转移的表示；使用人类连接组计划中995名受试者的预训练体素级表示；采用内存高效的注意力机制实现更大的感受野。&lt;h4&gt;主要发现&lt;/h4&gt;模型显著提高了多个感觉和认知领域任务诱导活动的下游解码性能，即使使用最少的数据预处理；通过内存高效的注意力机制实现更大的感受野带来性能提升；预训练数据中的功能相关性在小样本微调时对性能有重要影响。&lt;h4&gt;结论&lt;/h4&gt;迁移学习是一种可行的方法，可以利用大规模数据集来克服从fMRI数据解码脑活动的挑战。&lt;h4&gt;翻译&lt;/h4&gt;神经科学中的一个基本挑战是从脑活动中解码心理状态。虽然功能性磁共振成像(fMRI)提供了一种非侵入性方法，可以高空间精度地捕捉全脑神经动力学，但由于其高维度性、低信噪比和有限的受试者内数据，从fMRI数据中解码（特别是从任务诱导的活动中）仍然具有挑战性。在这里，我们利用计算机视觉的最新进展，提出了STDA-SwiFT，一种基于transformer的模型，通过时空分离注意力和自监督对比学习从大规模fMRI数据集中学习可转移的表示。使用人类连接组计划(HCP)中995名受试者的预训练体素级表示，我们展示了我们的模型显著提高了多个感觉和认知领域任务诱导活动的下游解码性能，即使使用最少的数据预处理。我们展示了通过内存高效的注意力机制实现更大感受野带来的性能提升，以及在微调小样本时预训练数据中的功能相关性的影响。我们的工作展示了迁移学习作为一种可行的方法，可以利用大规模数据集来克服从fMRI数据解码脑活动的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A fundamental challenge in neuroscience is to decode mental states from brainactivity. While functional magnetic resonance imaging (fMRI) offers anon-invasive approach to capture brain-wide neural dynamics with high spatialprecision, decoding from fMRI data -- particularly from task-evoked activity --remains challenging due to its high dimensionality, low signal-to-noise ratio,and limited within-subject data. Here, we leverage recent advances in computervision and propose STDA-SwiFT, a transformer-based model that learnstransferable representations from large-scale fMRI datasets viaspatial-temporal divided attention and self-supervised contrastive learning.Using pretrained voxel-wise representations from 995 subjects in the HumanConnectome Project (HCP), we show that our model substantially improvesdownstream decoding performance of task-evoked activity across multiple sensoryand cognitive domains, even with minimal data preprocessing. We demonstrateperformance gains from larger receptor fields afforded by our memory-efficientattention mechanism, as well as the impact of functional relevance inpretraining data when fine-tuning on small samples. Our work showcases transferlearning as a viable approach to harness large-scale datasets to overcomechallenges in decoding brain activity from fMRI data.</description>
      <author>example@mail.com (Yueh-Po Peng, Vincent K. M. Cheung, Li Su)</author>
      <guid isPermaLink="false">2507.22378v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Prediction of acoustic field in 1-D uniform duct with varying mean flow and temperature using neural networks</title>
      <link>http://arxiv.org/abs/2507.22370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了基于物理定律约束的神经网络作为替代数值工具的应用，特别关注了一维管道中异质介质内声传播问题的建模与求解。&lt;h4&gt;背景&lt;/h4&gt;基于物理定律约束的神经网络已成为一种替代数值工具，可用于解决复杂的物理问题。&lt;h4&gt;目的&lt;/h4&gt;推导代表一维管道中异质介质内声传播的控制方程，并利用神经网络求解该问题。&lt;h4&gt;方法&lt;/h4&gt;将声传播问题转化为无约束优化问题，使用神经网络进行求解，同时预测和验证声压和粒子速度这两个声学状态变量。&lt;h4&gt;主要发现&lt;/h4&gt;研究了温度梯度对声场的影响，并展示了迁移学习和自动微分等机器学习技术在声学应用中的有效性。&lt;h4&gt;结论&lt;/h4&gt;基于物理定律约束的神经网络可以有效解决声传播问题，为声学应用提供了新的数值方法。&lt;h4&gt;翻译&lt;/h4&gt;受物理定律约束的神经网络作为一种替代数值工具应运而生。在本文中，推导了代表一维管道中异质介质内声传播的控制方程。该问题被转化为无约束优化问题并使用神经网络求解。预测并验证了声学状态变量：声压和粒子速度，并与传统的龙格-库塔求解器进行了比较。研究了温度梯度对声场的影响。展示了迁移学习和自动微分等机器学习技术在声学应用中的使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1142/S2591728524400036&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural networks constrained by the physical laws emerged as an alternatenumerical tool. In this paper, the governing equation that represents thepropagation of sound inside a one-dimensional duct carrying a heterogeneousmedium is derived. The problem is converted into an unconstrained optimizationproblem and solved using neural networks. Both the acoustic state variables:acoustic pressure and particle velocity are predicted and validated with thetraditional Runge-Kutta solver. The effect of the temperature gradient on theacoustic field is studied. Utilization of machine learning techniques such astransfer learning and automatic differentiation for acoustic applications isdemonstrated.</description>
      <author>example@mail.com (D. Veerababu, Prasanta K. Ghosh)</author>
      <guid isPermaLink="false">2507.22370v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Principled Curriculum Learning using Parameter Continuation Methods</title>
      <link>http://arxiv.org/abs/2507.22089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种用于神经网络优化的参数连续方法，该方法与同伦方法和课程学习有密切联系，在理论和实践中都表现出色。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络优化领域需要更有效的优化方法，现有的技术如ADAM等仍有改进空间。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的参数连续方法，用于优化神经网络，提高其泛化性能。&lt;h4&gt;方法&lt;/h4&gt;参数连续方法，这种方法与同伦方法和课程学习有理论联系。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在深度神经网络的多个问题上理论上合理且实践中有效，特别是在监督和无监督学习任务中，比ADAM等最先进的优化技术具有更好的泛化性能。&lt;h4&gt;结论&lt;/h4&gt;参数连续方法是一种有效且理论上合理的神经网络优化方法，优于现有的最先进技术。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了一种用于神经网络优化的参数连续方法。参数连续性、同伦方法和课程学习之间存在密切联系。我们在这里提出的方法在理论上是有依据的，并且在深度神经网络的几个问题上在实践中是有效的。特别是，我们展示了比最先进的优化技术（如ADAM）更好的泛化性能，用于监督和无监督学习任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we propose a parameter continuation method for the optimizationof neural networks. There is a close connection between parameter continuation,homotopies, and curriculum learning. The methods we propose here aretheoretically justified and practically effective for several problems in deepneural networks. In particular, we demonstrate better generalizationperformance than state-of-the-art optimization techniques such as ADAM forsupervised and unsupervised learning tasks.</description>
      <author>example@mail.com (Harsh Nilesh Pathak, Randy Paffenroth)</author>
      <guid isPermaLink="false">2507.22089v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>TurboReg: TurboClique for Robust and Efficient Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2507.01439v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV-2025 Accepted Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TurboReg的快速鲁棒估计器，用于解决基于对应关系的点云配准中的效率问题。该方法通过引入TurboClique和Pivot-Guided Search算法，显著提高了点云配准的速度，同时保持了高召回率。&lt;h4&gt;背景&lt;/h4&gt;在基于对应关系的点云配准中，鲁棒估计至关重要。现有使用不兼容图中最大团搜索的方法虽然能实现高召回率，但存在指数级时间复杂度的缺点，限制了它们在时间敏感应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种既快速又鲁棒的方法，用于点云配准，解决现有方法计算效率低的问题。&lt;h4&gt;方法&lt;/h4&gt;提出TurboReg估计器，基于两种创新：1) TurboClique，定义为高度约束兼容图中的3-clique结构，具有轻量级特性，允许高效并行搜索，并通过高度约束的兼容图确保鲁棒的空间一致性；2) Pivot-Guided Search (PGS)算法，选择高SC²分数的匹配对作为枢轴，有效引导搜索向高内点比例的TurboClique，且具有线性时间复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;1. TurboReg在多个真实数据集上实现了最先进的性能2. 与现有方法相比有显著的加速效果3. 在3DMatch+FCGF数据集上，TurboReg (1K)比3DMAC快208.22倍，同时实现了更高的召回率4. 代码已在GitHub上公开&lt;h4&gt;结论&lt;/h4&gt;TurboReg通过创新的TurboClique结构和PGS算法，成功解决了点云配准中效率与鲁棒性之间的权衡问题，为时间敏感应用提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;鲁棒估计在基于对应关系的点云配准中至关重要。现有使用不兼容图中最大团搜索的方法虽能实现高召回率，但存在指数级时间复杂度的问题，限制了它们在时间敏感应用中的使用。为解决这一挑战，我们提出了一种快速鲁棒估计器TurboReg，它基于一种新的轻量级团TurboClique和一个高度可并行化的Pivot-Guided Search (PGS)算法。首先，我们将TurboClique定义为高度约束兼容图中的一个3-clique。3-clique的轻量级特性允许高效的并行搜索，而高度约束的兼容图确保了鲁棒的空间一致性，用于稳定的变换估计。接下来，PGS选择具有高SC²分数的匹配对作为枢轴，有效地引导搜索向具有更高内点比例的TurboClique。此外，PGS算法具有线性时间复杂度，比具有指数级时间复杂度的最大团搜索效率高得多。大量实验表明，TurboReg在多个真实数据集上实现了最先进的性能，并有显著的加速效果。例如，在3DMatch+FCGF数据集上，TurboReg (1K)比3DMAC快208.22倍，同时实现了更高的召回率。我们的代码可在https://github.com/Laka-3DV/TurboReg获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准(PCR)中基于图的鲁棒估计方法存在的计算效率问题。现有使用最大团搜索的方法虽然能实现高召回率，但具有指数级时间复杂度，限制了它们在时间敏感应用中的使用。这个问题在SLAM、虚拟现实等需要实时处理点云数据的领域尤为重要，因为随着点云规模增长，指数级复杂度会带来严重的性能瓶颈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：RANSAC类方法收敛慢，深度学习方法泛化能力弱且需大量训练，基于图的方法虽提高了鲁棒性但在低内点比率场景下仍有挑战。作者从最大团的稳定性机制获得启发，注意到最大团通过数据缩放和成对兼容性提供稳定性，但数据缩放需要最大化团尺寸导致计算开销大。因此作者设计了固定大小的3-团(TurboClique)替代最大团，并通过严格兼容性阈值弥补稳定性损失。同时借鉴了SC2图的概念和[47]中的策略，设计了有序SC2图避免冗余检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用轻量级的TurboClique(固定大小的3-团)替代计算密集的最大团，并通过Pivot-Guided Search算法高效识别高质量的TurboCliques。整体流程：1)构建有序SC2兼容性图(O2Graph)；2)应用PGS算法识别TurboCliques，包括选择高SC2分数的支点、搜索兼容邻居、聚合权重选择；3)对每个TurboClique估计变换，选择最优变换输出。这种方法在保持鲁棒性的同时显著降低了计算复杂度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)TurboClique：新型轻量级团结构，通过严格兼容性约束保证稳定性；2)Pivot-Guided Search算法：线性时间复杂度的高度并行化搜索策略；3)有序SC2图：避免冗余检测。相比之前工作：与RANSAC方法相比不需随机采样且收敛更快；与基于图的方法(如SC2-PCR)相比解决了指数级复杂度问题；与3DMAC相比效率提升208倍且精度更高；与深度学习方法相比不需大量训练且泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TurboReg通过引入轻量级的TurboClique结构和高效的Pivot-Guided Search算法，在保持点云配准高精度的同时，将计算复杂度从指数级降低到线性，实现了速度和鲁棒性的显著提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust estimation is essential in correspondence-based Point CloudRegistration (PCR). Existing methods using maximal clique search incompatibility graphs achieve high recall but suffer from exponential timecomplexity, limiting their use in time-sensitive applications. To address thischallenge, we propose a fast and robust estimator, TurboReg, built upon a novellightweight clique, TurboClique, and a highly parallelizable Pivot-GuidedSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within ahighly-constrained compatibility graph. The lightweight nature of the 3-cliqueallows for efficient parallel searching, and the highly-constrainedcompatibility graph ensures robust spatial consistency for stabletransformation estimation. Next, PGS selects matching pairs with high SC$^2$scores as pivots, effectively guiding the search toward TurboCliques withhigher inlier ratios. Moreover, the PGS algorithm has linear time complexityand is significantly more efficient than the maximal clique search withexponential time complexity. Extensive experiments show that TurboReg achievesstate-of-the-art performance across multiple real-world datasets, withsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achievinghigher recall. Our code is accessible at\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.</description>
      <author>example@mail.com (Shaocheng Yan, Pengcheng Shi, Zhenjun Zhao, Kaixin Wang, Kuang Cao, Ji Wu, Jiayuan Li)</author>
      <guid isPermaLink="false">2507.01439v3</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy</title>
      <link>http://arxiv.org/abs/2507.21358v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The manuscript has been accepted by ICONIP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Collaborative Perceiver (CoP)的多任务学习框架，通过利用空间占用作为辅助信息，改进基于视觉的鸟瞰图3D目标检测性能，解决了现有方法忽略环境上下文的问题。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图3D目标检测在自动驾驶领域取得了显著进展，但现有方法通常通过折叠提取的目标特征来构建BEV表示，忽略了道路和人行道等内在环境上下文，阻碍了检测器全面感知物理世界的能力。&lt;h4&gt;目的&lt;/h4&gt;引入多任务学习框架CoP，利用空间占用信息挖掘3D目标检测和占用预测任务之间的一致结构和概念相似性，弥合空间表示和特征精化的差距，构建更强大的BEV表示。&lt;h4&gt;方法&lt;/h4&gt;1) 提出包含局部密度信息的密集占用真实值生成管道；2) 采用体素高度引导采样策略根据物体属性提炼细粒度局部特征；3) 开发全局-局部协作特征融合模块集成两个任务的互补知识。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准测试中，CoP优于现有基于视觉的框架，在测试集上达到49.5%的mAP和59.2%的NDS，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过多任务学习框架CoP整合空间占用信息，可以显著提升基于视觉的鸟瞰图3D目标检测性能，使检测器能够更全面地感知物理世界特征。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图(BEV)3D目标检测在自动驾驶领域通过提供成本效益和丰富的上下文信息取得了显著进展。然而，现有方法通常通过折叠提取的目标特征来构建BEV表示，忽略了道路和人行道等内在环境上下文。这阻碍了检测器全面感知物理世界特征的能力。为此，我们引入了一个多任务学习框架Collaborative Perceiver (CoP)，利用空间占用作为辅助信息，挖掘3D目标检测和占用预测任务之间一致的结构和概念相似性，弥合空间表示和特征精化的差距。为此，我们首先提出了一种管道来生成包含局部密度信息(LDO)的密集占用真实值，用于重建详细的环境信息。接下来，我们采用体素高度引导采样(VHS)策略，根据不同的物体属性提炼细粒度的局部特征。此外，我们开发了全局-局部协作特征融合(CFF)模块，无缝集成两个任务之间的互补知识，从而组成更强大的BEV表示。在nuScenes基准上的大量实验表明，CoP优于现有的基于视觉的框架，在测试集上达到49.5%的mAP和59.2%的NDS。代码和补充材料可在提供的链接获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于视觉的鸟瞰图(BEV)3D目标检测中忽略环境内在上下文信息的问题。现有方法通常通过压缩提取的目标特征来构建BEV表示，忽略了道路、人行道等重要环境信息，导致检测器无法全面感知物理世界。这个问题在自动驾驶领域至关重要，因为准确感知周围环境对确保安全行车非常关键，特别是在复杂交通场景中，对具有独特或不规则几何形状物体的识别能力直接影响自动驾驶系统的安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D目标检测方法的三个主要挑战来设计方法：1)难以识别不规则几何形状物体；2)传统BEV方法沿高度维度压缩特征，难以保留不同高度物体的空间特性；3)仅依赖特定任务信息阻碍了通用环境表示构建。作者借鉴了3D占用预测的思想，因为它能提供几何一致的场景描述；参考了BEVDet、BEVDepth等BEV-based方法及LSS视图变换技术；并采用了多任务学习框架。基于此，作者设计了Collaborative Perceiver (CoP)框架，利用空间占用作为辅助信息，挖掘两个任务间的一致结构和概念相似性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务学习框架将3D目标检测和3D占用预测任务结合，利用两个任务间的互补和一致知识构建更强大的BEV表示。整体流程包括：1)输入多摄像头图像并提取特征；2)使用LSS视图变换将2D特征转换为3D体素特征；3)生成局部密度感知的密集占用(LDO)地面真实；4)通过体素高度引导采样(VHS)提取不同高度区间的细粒度局部特征；5)利用全局-局部协作特征融合(CFF)模块整合全局和局部特征；6)将统一的BEV特征发送到任务特定头进行3D占用预测和目标检测；7)结合局部密度矩阵作为辅助监督信号进行端到端训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)局部密度感知的空间占用(LDO)生成，考虑点云非均匀性而非常规方法的均匀密度假设；2)体素高度引导采样(VHS)策略，保留不同高度物体的空间特性而非简单压缩；3)全局-局部协作特征融合(CFF)模块，实现特征提取、增强和交互而非单一特征处理；4)多任务学习框架，结合两个任务知识共享实现全面环境理解。相比之前工作，CoP同时考虑两个任务，引入局部密度机制，设计高度引导采样，并通过协作特征融合构建更鲁棒的BEV表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Collaborative Perceiver多任务学习框架，通过结合3D目标检测和局部密度感知的空间占用预测，实现了对物理世界更全面、准确的感知，显著提升了基于视觉的3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based bird's-eye-view (BEV) 3D object detection has advancedsignificantly in autonomous driving by offering cost-effectiveness and richcontextual information. However, existing methods often construct BEVrepresentations by collapsing extracted object features, neglecting intrinsicenvironmental contexts, such as roads and pavements. This hinders detectorsfrom comprehensively perceiving the characteristics of the physical world. Toalleviate this, we introduce a multi-task learning framework, CollaborativePerceiver (CoP), that leverages spatial occupancy as auxiliary information tomine consistent structural and conceptual similarities shared between 3D objectdetection and occupancy prediction tasks, bridging gaps in spatialrepresentations and feature refinement. To this end, we first propose apipeline to generate dense occupancy ground truths incorporating local densityinformation (LDO) for reconstructing detailed environmental information. Next,we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grainedlocal features according to distinct object properties. Furthermore, we developa global-local collaborative feature fusion (CFF) module that seamlesslyintegrates complementary knowledge between both tasks, thus composing morerobust BEV representations. Extensive experiments on the nuScenes benchmarkdemonstrate that CoP outperforms existing vision-based frameworks, achieving49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials areavailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.</description>
      <author>example@mail.com (Jicheng Yuan, Manh Nguyen Duc, Qian Liu, Manfred Hauswirth, Danh Le Phuoc)</author>
      <guid isPermaLink="false">2507.21358v2</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2507.19856v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RaGS的新型框架，首次利用3D高斯溅射作为表示方法来融合4D雷达和单目图像进行3D目标检测，解决了现有方法缺乏整体场景理解或受限于刚性网格结构的问题。&lt;h4&gt;背景&lt;/h4&gt;4D毫米波雷达已成为自动驾驶领域有前景的传感器，但从4D雷达和单目图像中进行有效的3D目标检测仍面临挑战。现有融合方法通常基于实例提案或密集BEV网格，存在整体场景理解不足或受限于刚性网格结构的问题。&lt;h4&gt;目的&lt;/h4&gt;提出RaGS框架，利用3D高斯溅射作为表示方法，有效融合4D雷达和单目线索进行3D目标检测，克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;RaGS采用级联管道构建和优化高斯场：首先通过基于视锥的定位初始化(FLI)将前景像素反投影初始化3D高斯位置；然后使用迭代多模态聚合(IMA)融合语义和几何信息，将高斯优化到感兴趣区域；最后通过多级高斯融合(MGF)将高斯渲染到多级BEV特征中用于3D目标检测。&lt;h4&gt;主要发现&lt;/h4&gt;3D高斯溅射通过将场景建模为高斯场，自然适合3D目标检测，能够动态分配资源给前景对象，提供灵活、资源高效的解决方案。RaGS通过动态关注场景中的稀疏对象，实现对象集中同时提供全面的场景感知能力。&lt;h4&gt;结论&lt;/h4&gt;在View-of-Delft、TJ4DRadSet和OmniHD-Scenes基准测试上的广泛实验证明了RaGS的最先进性能，相关代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;4D毫米波雷达已成为自动驾驶领域有前景的传感器，但从4D雷达和单目图像中进行有效的3D目标检测仍面临挑战。现有融合方法通常基于实例提案或密集BEV网格，缺乏整体场景理解或受限于刚性网格结构。为此，我们提出RaGS，这是首个利用3D高斯溅射作为表示方法来融合4D雷达和单目线索进行3D目标检测的框架。3D高斯溅射通过将场景建模为高斯场，自然适合3D目标检测，能够动态分配资源给前景对象，提供灵活、资源高效的解决方案。RaGS使用级联管道构建和优化高斯场。首先通过基于视锥的定位初始化(FLI)将前景像素反投影以初始化粗略的3D高斯位置。然后使用迭代多模态聚合(IMA)融合语义和几何信息，将有限的高斯优化到感兴趣区域。最后通过多级高斯融合(MGF)将高斯渲染到多级BEV特征中用于3D目标检测。通过动态关注场景中的稀疏对象，RaGS实现了对象集中同时提供全面的场景感知能力。在View-of-Delft、TJ4DRadSet和OmniHD-Scenes基准测试上的广泛实验证明了其最先进的性能。相关代码将公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效融合4D毫米波雷达和单目图像进行3D物体检测的问题。这个问题在自动驾驶领域非常重要，因为4D雷达在恶劣环境中具有鲁棒性，能捕获速度和高度数据，而相机提供高分辨率语义信息，两者互补性对增强感知至关重要。然而，现有方法要么缺乏整体场景理解能力，要么受限于刚性网格结构，无法高效处理本质上稀疏但仍需全面理解的3D检测任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于实例和BEV两种融合方法的局限性，前者缺乏全局场景理解，后者受限于固定网格结构。然后引入3D高斯泼溅(GS)技术，它提供物理可解释性、稀疏性和灵活性。作者设计了RaGS框架，包含FLI、IMA和MGF三个关键模块。该方法借鉴了GS技术用于神经渲染的工作，以及RCFusion、LXL等雷达相机融合方法，同时使用了ResNet-50骨干网络、FPN和稀疏卷积等技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将场景表示为连续的3D高斯场而非传统密集体素网格，通过迭代动态分配资源专注于稀疏前景物体，同时保持全局场景感知。实现流程包括：1)特征提取，处理图像和雷达数据；2)FLI模块，通过前景像素反投影初始化高斯位置；3)IMA模块，使用3D可变形交叉注意力聚合多模态特征并迭代更新高斯位置；4)MGF模块，将高斯渲染为多级BEV特征并进行融合，最终输出3D检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将3D高斯泼溅应用于4D雷达和单目相机融合的3D物体检测框架；2)设计了FLI、IMA和MGF三个关键模块构建和细化高斯表示；3)实现动态关注稀疏物体同时保持场景感知。相比不同之处在于：与基于实例方法相比，提供更好的全局场景理解；与基于BEV方法相比，避免固定网格限制和背景效率问题；与其他GS工作相比，专注于3D物体检测而非渲染或占用预测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RaGS首次将3D高斯泼溅技术应用于4D雷达和单目相机融合，通过动态分配资源专注于稀疏物体，同时保持全局场景感知，实现了最先进的3D物体检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D millimeter-wave radar has emerged as a promising sensor for autonomousdriving, but effective 3D object detection from both 4D radar and monocularimages remains a challenge. Existing fusion approaches typically rely on eitherinstance-based proposals or dense BEV grids, which either lack holistic sceneunderstanding or are limited by rigid grid structures. To address these, wepropose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) asrepresentation for fusing 4D radar and monocular cues in 3D object detection.3D GS naturally suits 3D object detection by modeling the scene as a field ofGaussians, dynamically allocating resources on foreground objects and providinga flexible, resource-efficient solution. RaGS uses a cascaded pipeline toconstruct and refine the Gaussian field. It starts with the Frustum-basedLocalization Initiation (FLI), which unprojects foreground pixels to initializecoarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA)fuses semantics and geometry, refining the limited Gaussians to the regions ofinterest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussiansinto multi-level BEV features for 3D object detection. By dynamically focusingon sparse objects within scenes, RaGS enable object concentrating whileoffering comprehensive scene perception. Extensive experiments onView-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate itsstate-of-the-art performance. Code will be released.</description>
      <author>example@mail.com (Xiaokai Bai, Chenxu Zhou, Lianqing Zheng, Si-Yuan Cao, Jianan Liu, Xiaohan Zhang, Zhengzhuang Zhang, Hui-liang Shen)</author>
      <guid isPermaLink="false">2507.19856v2</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>Representation biases: will we achieve complete understanding by analyzing representations?</title>
      <link>http://arxiv.org/abs/2507.22216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了神经表征分析中特征表征偏差带来的挑战，以及这些偏差如何影响常见分析方法的结果，并讨论了对神经科学研究的启示。&lt;h4&gt;背景&lt;/h4&gt;神经科学中常见方法是通过研究神经表征来理解系统，越来越多地将神经表征与计算模型学习的内部表征联系起来。然而，最近机器学习研究表明学习到的特征表征可能存在偏差。&lt;h4&gt;目的&lt;/h4&gt;说明特征表征偏差带来的挑战，展示这些偏差如何导致常见分析(如PCA、回归和RSA)产生强烈偏差的推断，并讨论对神经科学研究的启示。&lt;h4&gt;方法&lt;/h4&gt;以同态加密作为简单案例研究，展示表征模式和计算之间可能存在的强烈分离。&lt;h4&gt;主要发现&lt;/h4&gt;学习到的特征表征可能过度表示某些特征而较弱且不一致地表示其他特征，简单特征可能比复杂特征更强且更一致地表征；这些偏差会导致常见分析方法产生强烈偏差的推断；表征模式和计算之间可能存在强烈分离。&lt;h4&gt;结论&lt;/h4&gt;特征表征偏差对系统间的表征比较有重要影响，对神经科学研究有更广泛的启示，需要考虑这些偏差以实现更准确的理解。&lt;h4&gt;翻译&lt;/h4&gt;神经科学中的一种常见方法是研究神经表征作为理解系统的手段——越来越多地通过将神经表征与计算模型学习的内部表征联系起来。然而，机器学习领域最近的一项工作(Lampinen, 2024)表明，学习到的特征表征可能有偏差，过度表示某些特征，而较弱且不一致地表示其他特征。例如，简单(线性)特征可能比复杂(高度非线性)特征更强且更一致地表征。这些偏差可能通过表征分析实现系统完整理解时带来挑战。在这篇观点文章中，我们说明了这些挑战——展示特征表征偏差如何导致常见分析(如PCA、回归和RSA)产生强烈偏差的推断。我们还以同态加密作为简单案例研究，展示表征模式和计算之间可能存在强烈分离的潜力。我们讨论了这些结果对系统间表征比较的影响，以及对神经科学的更广泛影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A common approach in neuroscience is to study neural representations as ameans to understand a system -- increasingly, by relating the neuralrepresentations to the internal representations learned by computationalmodels. However, a recent work in machine learning (Lampinen, 2024) shows thatlearned feature representations may be biased to over-represent certainfeatures, and represent others more weakly and less-consistently. For example,simple (linear) features may be more strongly and more consistently representedthan complex (highly nonlinear) features. These biases could pose challengesfor achieving full understanding of a system through representational analysis.In this perspective, we illustrate these challenges -- showing how featurerepresentation biases can lead to strongly biased inferences from commonanalyses like PCA, regression, and RSA. We also present homomorphic encryptionas a simple case study of the potential for strong dissociation betweenpatterns of representation and computation. We discuss the implications ofthese results for representational comparisons between systems, and forneuroscience more generally.</description>
      <author>example@mail.com (Andrew Kyle Lampinen, Stephanie C. Y. Chan, Yuxuan Li, Katherine Hermann)</author>
      <guid isPermaLink="false">2507.22216v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>MASCA: LLM based-Multi Agents System for Credit Assessment</title>
      <link>http://arxiv.org/abs/2507.22758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACL REALM Workshop. Work in Progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MASCA，一个基于大型语言模型的多代理系统，用于改进信用评估，通过分层架构和对比学习优化决策，并在实验中表现出色。&lt;h4&gt;背景&lt;/h4&gt;最近的金融问题解决进展主要利用大型语言模型和基于代理的系统，专注于交易和金融建模，而信用评估仍是一个探索不足的领域，传统上依赖基于规则的方法和统计模型。&lt;h4&gt;目的&lt;/h4&gt;引入MASCA系统，通过模拟现实世界的决策过程来增强信用评估能力。&lt;h4&gt;方法&lt;/h4&gt;采用分层架构设计，让专门的基于LLM的代理协作处理子任务；集成对比学习进行风险评估和奖励评估；提供分层多代理系统的信号博弈理论视角；进行信用评估中的详细偏差分析以解决公平性问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明MASCA优于基线方法，证明了分层LLM多代理系统在金融应用中的有效性，特别是在信用评分方面。&lt;h4&gt;结论&lt;/h4&gt;分层LLM多代理系统在金融应用中具有显著优势，特别是在信用评分领域。&lt;h4&gt;翻译&lt;/h4&gt;最近的金融问题解决进展利用了大型语言模型和基于代理的系统，主要关注交易和金融建模。然而，信用评估仍然是一个探索不足的挑战，传统上依赖于基于规则的方法和统计模型。在本文中，我们介绍了MASCA，一个由LLM驱动的多代理系统，旨在通过模拟现实世界的决策过程来增强信用评估。该框架采用分层架构，其中专门的基于LLM的代理协作处理子任务。此外，我们集成了对比学习进行风险评估和奖励评估，以优化决策。我们还提出了分层多代理系统的信号博弈理论视角，为其结构和相互作用提供了理论见解。我们的论文还包括了信用评估中的详细偏差分析，解决了公平性问题。实验结果表明MASCA优于基线方法，突出了分层LLM多代理系统在金融应用中的有效性，特别是在信用评分方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in financial problem-solving have leveraged LLMs andagent-based systems, with a primary focus on trading and financial modeling.However, credit assessment remains an underexplored challenge, traditionallydependent on rule-based methods and statistical models. In this paper, weintroduce MASCA, an LLM-driven multi-agent system designed to enhance creditevaluation by mirroring real-world decision-making processes. The frameworkemploys a layered architecture where specialized LLM-based agentscollaboratively tackle sub-tasks. Additionally, we integrate contrastivelearning for risk and reward assessment to optimize decision-making. We furtherpresent a signaling game theory perspective on hierarchical multi-agentsystems, offering theoretical insights into their structure and interactions.Our paper also includes a detailed bias analysis in credit assessment,addressing fairness concerns. Experimental results demonstrate that MASCAoutperforms baseline approaches, highlighting the effectiveness of hierarchicalLLM-based multi-agent systems in financial applications, particularly in creditscoring.</description>
      <author>example@mail.com (Gautam Jajoo, Pranjal A Chitale, Saksham Agarwal)</author>
      <guid isPermaLink="false">2507.22758v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models</title>
      <link>http://arxiv.org/abs/2507.22431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种利用大型视觉-语言模型(LVLMs)提升图像-文本对数据质量的方法，创建了HQ-CLIP模型，在多个视觉-语言任务上实现了最先进的性能，甚至在数据量少10倍的情况下超越了标准CLIP模型。&lt;h4&gt;背景&lt;/h4&gt;大规模但有噪声的图像-文本对数据促进了对比语言-图像预训练(CLI)的成功。CLIP作为基础视觉编码器，又成为大多数大型视觉-语言模型(LVLMs)的基石，两者之间存在相互依赖关系。&lt;h4&gt;目的&lt;/h4&gt;探索能否利用LVLMs来提高图像-文本对数据的质量，从而实现持续改进的自我强化循环。&lt;h4&gt;方法&lt;/h4&gt;提出了一种LVLM驱动的数据精炼流程，利用LVLMs处理图像及其原始替代文本，生成四种互补的文本描述(长正面描述、长负面描述、短正面标签和短负面标签)。将此流程应用于DFN-Large数据集，生成了VLM-150M数据集。基于此数据集，提出了一种扩展传统对比学习的训练范式，将负面描述和短标签作为额外监督信号。&lt;h4&gt;主要发现&lt;/h4&gt;HQ-CLIP模型在多样化的基准测试中表现出显著改进。在相当规模训练数据的情况下，在零样本分类、跨模态检索和细粒度视觉理解任务中实现了最先进的性能。在检索基准测试中，HQ-CLIP超越了在DFN-2B数据集上训练的标准CLIP模型，而该数据集的训练数据量比实验数据集多10倍。&lt;h4&gt;结论&lt;/h4&gt;成功实现了利用LVLMs提升CLIP数据质量的愿景，证明了通过LVLM驱动数据精炼可以形成自我强化循环，持续提升视觉-语言模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;大规模但有噪声的图像-文本对数据为对比语言-图像预训练(CLI)的成功铺平了道路。作为基础视觉编码器，CLIP又成为大多数大型视觉-语言模型(LVLMs)的基石。这种相互依赖关系自然引发了一个有趣的问题：我们能否利用LVLMs来提高图像-文本对数据的质量，从而实现持续改进的自我强化循环的可能性？在本工作中，我们通过引入一个LVLM驱动的数据精炼流程，朝着这一愿景迈出了重要一步。我们的框架利用LVLMs处理图像及其原始替代文本，生成四种互补的文本公式：长正面描述、长负面描述、短正面标签和短负面标签。将此流程应用于精选的DFN-Large数据集，生成了VLM-150M，这是一个具有多粒度注释的精炼数据集。基于此数据集，我们进一步提出了一种训练范式，通过将负面描述和短标签作为额外的监督信号来扩展传统的对比学习。 resulting model(HQ-CLIP)在多样化的基准测试中表现出显著的改进。在相当规模训练数据的情况下，我们的方法在零样本分类、跨模态检索和细粒度视觉理解任务中实现了最先进的性能。在检索基准测试中，HQ-CLIP甚至超越了在DFN-2B数据集上训练的标准CLIP模型，该数据集的训练数据量比我们的多10倍。所有代码、数据和模型可在https://zxwei.site/hqclip获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale but noisy image-text pair data have paved the way for the successof Contrastive Language-Image Pretraining (CLIP). As the foundation visionencoder, CLIP in turn serves as the cornerstone for most large vision-languagemodels (LVLMs). This interdependence naturally raises an interesting question:Can we reciprocally leverage LVLMs to enhance the quality of image-text pairdata, thereby opening the possibility of a self-reinforcing cycle forcontinuous improvement? In this work, we take a significant step toward thisvision by introducing an LVLM-driven data refinement pipeline. Our frameworkleverages LVLMs to process images and their raw alt-text, generating fourcomplementary textual formulas: long positive descriptions, long negativedescriptions, short positive tags, and short negative tags. Applying thispipeline to the curated DFN-Large dataset yields VLM-150M, a refined datasetenriched with multi-grained annotations. Based on this dataset, we furtherpropose a training paradigm that extends conventional contrastive learning byincorporating negative descriptions and short tags as additional supervisedsignals. The resulting model, namely HQ-CLIP, demonstrates remarkableimprovements across diverse benchmarks. Within a comparable training datascale, our approach achieves state-of-the-art performance in zero-shotclassification, cross-modal retrieval, and fine-grained visual understandingtasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP modelstrained on the DFN-2B dataset, which contains 10$\times$ more training datathan ours. All code, data, and models are available athttps://zxwei.site/hqclip.</description>
      <author>example@mail.com (Zhixiang Wei, Guangting Wang, Xiaoxiao Ma, Ke Mei, Huaian Chen, Yi Jin, Fengyun Rao)</author>
      <guid isPermaLink="false">2507.22431v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</title>
      <link>http://arxiv.org/abs/2507.22264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新方法SmartCLIP，解决了CLIP模型在图像-文本数据集中的信息不对齐和表示纠缠问题，通过建立理论条件实现文本和视觉表示在不同粒度级别上的灵活对齐。&lt;h4&gt;背景&lt;/h4&gt;CLIP（对比语言-图像预训练）已成为计算机视觉和多模态学习中的关键模型，通过对比学习实现了对齐视觉和文本表示的最先进性能。&lt;h4&gt;目的&lt;/h4&gt;建立理论条件，使文本和视觉表示能够在不同粒度级别上实现灵活对齐，确保模型既能保留跨模态语义信息的完整性，又能解耦视觉表示以捕获细粒度的文本概念。&lt;h4&gt;方法&lt;/h4&gt;提出SmartCLIP，一种新颖的方法，以模块化方式识别和对齐最相关的视觉和文本表示，确保模型能够保留跨模态语义信息并解耦视觉表示。&lt;h4&gt;主要发现&lt;/h4&gt;在各种任务上的优越性能表明SmartCLIP能够有效处理信息不对齐问题，支持了他们的识别理论。&lt;h4&gt;结论&lt;/h4&gt;SmartCLIP通过解决CLIP模型的信息不对齐和表示纠缠问题，提高了模型在涉及短提示的下游任务上的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练（CLIP）已成为计算机视觉和多模态学习中的关键模型，通过对比学习实现了对齐视觉和文本表示的最先进性能。然而，CLIP在许多图像-文本数据集中存在潜在的信息不对齐问题，并且表示纠缠。一方面，MSCOCO等数据集中单个图像的简短描述可能描述图像中的不连续区域，使模型不确定保留或忽略哪些视觉特征。另一方面，直接将长标题与图像对齐可能导致保留纠缠的细节，使模型无法学习解耦的、原子化的概念，最终限制了其在涉及短提示的某些下游任务上的泛化能力。在本文中，我们建立了理论条件，使文本和视觉表示能够在不同粒度级别上实现灵活对齐。具体来说，我们的框架确保模型不仅可以保留跨模态语义信息的完整性，还可以解耦视觉表示以捕获细粒度的文本概念。基于此，我们引入了SmartCLIP，一种新颖的方法，以模块化方式识别和对齐最相关的视觉和文本表示。在各种任务上的优越性能表明其处理信息不对齐的能力，并支持了我们的识别理论。代码可在https://github.com/Mid-Push/SmartCLIP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning}has emerged as a pivotal model in computer vision and multimodal learning,achieving state-of-the-art performance at aligning visual and textualrepresentations through contrastive learning. However, CLIP struggles withpotential information misalignment in many image-text datasets and suffers fromentangled representation. On the one hand, short captions for a single image indatasets like MSCOCO may describe disjoint regions in the image, leaving themodel uncertain about which visual features to retain or disregard. On theother hand, directly aligning long captions with images can lead to theretention of entangled details, preventing the model from learningdisentangled, atomic concepts -- ultimately limiting its generalization oncertain downstream tasks involving short prompts.  In this paper, we establish theoretical conditions that enable flexiblealignment between textual and visual representations across varying levels ofgranularity. Specifically, our framework ensures that a model can not only\emph{preserve} cross-modal semantic information in its entirety but also\emph{disentangle} visual representations to capture fine-grained textualconcepts. Building on this foundation, we introduce \ours, a novel approachthat identifies and aligns the most relevant visual and textual representationsin a modular manner. Superior performance across various tasks demonstrates itscapability to handle information misalignment and supports our identificationtheory. The code is available at https://github.com/Mid-Push/SmartCLIP.</description>
      <author>example@mail.com (Shaoan Xie, Lingjing Kong, Yujia Zheng, Yu Yao, Zeyu Tang, Eric P. Xing, Guangyi Chen, Kun Zhang)</author>
      <guid isPermaLink="false">2507.22264v1</guid>
      <pubDate>Thu, 31 Jul 2025 14:33:19 +0800</pubDate>
    </item>
    <item>
      <title>A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data</title>
      <link>http://arxiv.org/abs/2507.21873v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the Journal of Artificial Intelligence Research (JAIR);  under revision. 29 pages, 6 figures. Code available at  https://github.com/raffaelepojer/NeSy-for-graph-data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种神经符号框架，将图神经网络(GNNs)与关系贝叶斯网络(RBNs)无缝集成，结合了GNNs的学习能力和RBNs的灵活推理能力，应用于节点分类和环境规划两个不同领域，显著提高了性能并引入了新的基准数据集。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在图结构数据预测任务上表现出色，但往往缺乏整合符号领域知识并进行通用推理的能力。关系贝叶斯网络(RBNs)能够在类图结构上进行完全生成式概率建模，支持丰富的符号知识和概率推理。&lt;h4&gt;目的&lt;/h4&gt;提出一个神经符号框架，将GNNs无缝集成到RBNs中，结合GNNs的学习能力和RBNs的灵活推理能力，弥合学习和推理之间的差距。&lt;h4&gt;方法&lt;/h4&gt;开发了两种集成实现方式：1)将GNNs直接编译为原生RBN语言；2)将GNNs作为外部组件保留。两种方法都保留了GNNs的语义和计算特性，同时完全符合RBN建模范式。还提出了一种针对这些神经符号模型的最大后验概率(MAP)推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;该框架应用于两个不同问题：1)将GNN节点分类模型转换为集体分类模型，明确建模同质和异质标签模式，显著提高准确性；2)在环境规划中引入多目标网络优化问题，其中MAP推理支持复杂决策制定。两个应用都包含新的公开可用基准数据集。&lt;h4&gt;结论&lt;/h4&gt;该工作引入了一种强大且连贯的神经符号方法来处理图数据，弥合了学习和推理之间的差距，使能够实现新应用并在不同任务中提高性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在图结构数据预测任务上表现出色，但往往缺乏整合符号领域知识并进行通用推理的能力。关系贝叶斯网络(RBNs)则能够在类图结构上进行完全生成式概率建模，支持丰富的符号知识和概率推理。本文提出了一个神经符号框架，将GNNs无缝集成到RBNs中，结合了GNNs的学习能力和RBNs的灵活推理能力。我们开发了两种这种集成的实现方式：一种将GNNs直接编译为原生RBN语言，另一种将GNNs作为外部组件保留。两种方法都保留了GNNs的语义和计算特性，同时完全符合RBN建模范式。我们还为这些神经符号模型提出了一种最大后验概率(MAP)推理方法。为展示该框架的通用性，我们将其应用于两个不同问题。首先，我们将一个用于节点分类的GNN转换为集体分类模型，明确建模同质和异质标签模式，显著提高了准确性。其次，我们在环境规划中引入了一个多目标网络优化问题，其中MAP推理支持复杂决策制定。两个应用都包含新的公开可用基准数据集。这项工作引入了一种强大且连贯的神经符号方法来处理图数据，以弥合学习和推理之间的差距，使能够在各种任务中实现新应用并提高性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) excel at predictive tasks on graph-structureddata but often lack the ability to incorporate symbolic domain knowledge andperform general reasoning. Relational Bayesian Networks (RBNs), in contrast,enable fully generative probabilistic modeling over graph-like structures andsupport rich symbolic knowledge and probabilistic inference. This paperpresents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,combining the learning strength of GNNs with the flexible reasoningcapabilities of RBNs.  We develop two implementations of this integration: one compiles GNNsdirectly into the native RBN language, while the other maintains the GNN as anexternal component. Both approaches preserve the semantics and computationalproperties of GNNs while fully aligning with the RBN modeling paradigm. We alsopropose a maximum a-posteriori (MAP) inference method for these neuro-symbolicmodels.  To demonstrate the framework's versatility, we apply it to two distinctproblems. First, we transform a GNN for node classification into a collectiveclassification model that explicitly models homo- and heterophilic labelpatterns, substantially improving accuracy. Second, we introduce amulti-objective network optimization problem in environmental planning, whereMAP inference supports complex decision-making. Both applications include newpublicly available benchmark datasets.  This work introduces a powerful and coherent neuro-symbolic approach to graphdata, bridging learning and reasoning in ways that enable novel applicationsand improved performance across diverse tasks.</description>
      <author>example@mail.com (Raffaele Pojer, Andrea Passerini, Kim G. Larsen, Manfred Jaeger)</author>
      <guid isPermaLink="false">2507.21873v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
  <item>
      <title>LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection</title>
      <link>http://arxiv.org/abs/2507.21756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LiteFat的轻量级时空图学习模型，用于高效检测驾驶员疲劳，同时保持高准确度和低计算需求，适合在资源有限的嵌入式设备上实时运行。&lt;h4&gt;背景&lt;/h4&gt;驾驶员疲劳检测对道路安全至关重要，因为疲劳驾驶是交通事故的主要原因之一。现有解决方案依赖计算密集型的深度学习模型，导致高延迟，不适合资源有限的嵌入式机器人设备（如智能车辆）。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级时空图学习模型，能够高效检测驾驶员疲劳，同时保持高准确度和低计算需求，适合在嵌入式设备上实时运行。&lt;h4&gt;方法&lt;/h4&gt;将流式视频数据转换为时空图（STG），使用面部关键点检测专注于关键运动模式；使用MobileNet提取面部特征并为STG创建特征矩阵；采用轻量级时空图神经网络识别疲劳迹象，实现最小处理和低延迟。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的实验结果表明，LiteFat表现具有竞争力，与当前最先进的方法相比，显著降低了计算复杂度和延迟。&lt;h4&gt;结论&lt;/h4&gt;这项工作促进了实时、资源高效的人类疲劳检测系统的发展，可以在嵌入式机器人设备上实施，提高道路安全性。&lt;h4&gt;翻译&lt;/h4&gt;驾驶员疲劳检测对道路安全至关重要，因为疲劳驾驶仍然是交通事故的主要原因。许多现有解决方案依赖于计算密集型的深度学习模型，导致高延迟，不适合需要快速检测以防止事故的嵌入式机器人设备（如智能车辆/汽车）。本文介绍了LiteFat，一种轻量级时空图学习模型，旨在高效检测驾驶员疲劳，同时保持高准确度和低计算需求。LiteFat涉及使用面部关键点检测将流式视频数据转换为时空图（STG），专注于关键运动模式并减少不必要的数据处理。LiteFat使用MobileNet提取面部特征并为STG创建特征矩阵。然后采用轻量级时空图神经网络以最小处理和低延迟识别疲劳迹象。在基准数据集上的实验结果表明，与当前最先进的方法相比，LiteFat在性能上具有竞争力，同时显著降低了计算复杂度和延迟。这项工作促进了实时、资源高效的人类疲劳检测系统的发展，可以在嵌入式机器人设备上实施。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting driver fatigue is critical for road safety, as drowsy drivingremains a leading cause of traffic accidents. Many existing solutions rely oncomputationally demanding deep learning models, which result in high latencyand are unsuitable for embedded robotic devices with limited resources (such asintelligent vehicles/cars) where rapid detection is necessary to preventaccidents. This paper introduces LiteFat, a lightweight spatio-temporal graphlearning model designed to detect driver fatigue efficiently while maintaininghigh accuracy and low computational demands. LiteFat involves convertingstreaming video data into spatio-temporal graphs (STG) using facial landmarkdetection, which focuses on key motion patterns and reduces unnecessary dataprocessing. LiteFat uses MobileNet to extract facial features and create afeature matrix for the STG. A lightweight spatio-temporal graph neural networkis then employed to identify signs of fatigue with minimal processing and lowlatency. Experimental results on benchmark datasets show that LiteFat performscompetitively while significantly decreasing computational complexity andlatency as compared to current state-of-the-art methods. This work enables thedevelopment of real-time, resource-efficient human fatigue detection systemsthat can be implemented upon embedded robotic devices.</description>
      <author>example@mail.com (Jing Ren, Suyu Ma, Hong Jia, Xiwei Xu, Ivan Lee, Haytham Fayek, Xiaodong Li, Feng Xia)</author>
      <guid isPermaLink="false">2507.21756v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins</title>
      <link>http://arxiv.org/abs/2507.21720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种神经网络扩展对应状态模型，用于预测氢氟烯烃制冷剂的残余热力学性质，结合了理论方法和数据驱动方法的优点，通过图神经网络表征分子结构，显著提高了预测精度。&lt;h4&gt;背景&lt;/h4&gt;氢氟烯烃因其极低的全球变暖潜能值被认为是新一代制冷剂，但缺乏可靠的热力学数据阻碍了新型优质制冷剂的发现和应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测氢氟烯烃制冷剂残余热力学性质的方法，以促进新型制冷剂的发现。&lt;h4&gt;方法&lt;/h4&gt;提出神经网络扩展对应状态模型，结合理论方法和数据驱动方法，使用图神经网络模块表征流体微观分子结构，专门设计模型架构提高泛化能力，使用已知流体精确数据训练，并通过留一法交叉验证评估。&lt;h4&gt;主要发现&lt;/h4&gt;与传统模型相比，新模型在液态和超临界区域对密度和能量性质的预测精度显著提高：液态密度偏差1.49%，超临界密度偏差2.42%，液态残余熵偏差3.37%，超临界残余熵偏差2.50%，液态残余焓偏差1.85%，超临界残余焓偏差1.34%。&lt;h4&gt;结论&lt;/h4&gt;将物理知识嵌入机器学习模型是有效的，所提出的神经网络模型有望显著加速新型氢氟烯烃制冷剂的发现。&lt;h4&gt;翻译&lt;/h4&gt;氢氟烯烃因其极低的全球变暖潜能值，被认为是最有前景的新一代制冷剂，可以有效缓解全球变暖效应。然而，缺乏可靠的热力学数据阻碍了新型优质氢氟烯烃制冷剂的发现和应用。本研究结合理论方法和数据驱动方法的优点，提出了一种神经网络扩展对应状态模型来预测氢氟烯烃制冷剂的残余热力学性质。创新之处在于通过图神经网络模块表征流体的微观分子结构，并专门设计模型架构以提高其泛化能力。所提出的模型使用已知流体的精确数据进行训练，并通过留一法交叉验证进行评估。与传统扩展对应状态模型或立方状态方程相比，所提出的模型在液态和超临界区域对密度和能量性质的预测精度显著提高，液态区域密度的平均绝对偏差为1.49%，超临界区域为2.42%；残余熵的平均绝对偏差分别为3.37%和2.50%；残余焓的平均绝对偏差分别为1.85%和1.34%。这些结果表明将物理知识嵌入机器学习模型是有效的。所提出的神经网络扩展对应状态模型有望显著加速新型氢氟烯烃制冷剂的发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hydrofluoroolefins are considered the most promising next-generationrefrigerants due to their extremely low global warming potential values, whichcan effectively mitigate the global warming effect. However, the lack ofreliable thermodynamic data hinders the discovery and application of newer andsuperior hydrofluoroolefin refrigerants. In this work, integrating thestrengths of theoretical method and data-driven method, we proposed a neuralnetwork extended corresponding state model to predict the residualthermodynamic properties of hydrofluoroolefin refrigerants. The innovation isthat the fluids are characterized through their microscopic molecularstructures by the inclusion of graph neural network module and the specializeddesign of model architecture to enhance its generalization ability. Theproposed model is trained using the highly accurate data of available knownfluids, and evaluated via the leave-one-out cross-validation method. Comparedto conventional extended corresponding state models or cubic equation of state,the proposed model shows significantly improved accuracy for density and energyproperties in liquid and supercritical regions, with average absolute deviationof 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% forresidual entropy, 1.85% and 1.34% for residual enthalpy. These resultsdemonstrate the effectiveness of embedding physics knowledge into the machinelearning model. The proposed neural network extended corresponding state modelis expected to significantly accelerate the discovery of novelhydrofluoroolefin refrigerants.</description>
      <author>example@mail.com (Gang Wang, Peng Hu)</author>
      <guid isPermaLink="false">2507.21720v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Graph Neural Network for Compressed Speech Steganalysis</title>
      <link>http://arxiv.org/abs/2507.21591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次将图神经网络(GraphSAGE架构)应用于压缩语音IP(VoIP)语音流的隐写分析，通过简单图结构构建和GraphSAGE分层信息捕获，实现了高检测准确率和效率。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的隐写分析方法通常面临计算复杂性和跨数据集泛化的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用关系数据的隐写分析方法，提高检测准确性和适应性。&lt;h4&gt;方法&lt;/h4&gt;从VoIP流构建简单图结构，并使用GraphSAGE捕获分层隐写分析信息，包括细粒度细节和高级模式。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在揭示VoIP信号中的量化索引调制(QIM)隐写模式方面表现良好，对于0.5秒的短样本检测准确率超过98%，在低嵌入率条件下达到95.17%的准确率，比最先进方法提高2.8%；同时，模型效率更高，0.5秒样本的平均检测时间低至0.016秒。&lt;h4&gt;结论&lt;/h4&gt;该方法对于在线隐写分析任务非常高效，在短样本和低嵌入率约束下提供了检测准确性和效率之间的优越平衡。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的隐写分析方法通常面临计算复杂性和跨数据集泛化的挑战。将图神经网络(GNN)引入隐写分析方案可以利用关系数据提高检测准确性和适应性。本文首次将图神经网络(GraphSAGE架构)应用于压缩语音IP(VoIP)语音流的隐写分析。该方法从VoIP流构建简单图结构，并使用GraphSAGE捕获分层隐写分析信息，包括细粒度细节和高级模式，从而实现高检测准确率。实验结果表明，开发的方法在揭示VoIP信号中的量化索引调制(QIM)隐写模式方面表现良好。即使对于0.5秒的短样本，检测准确率也超过98%，在低嵌入率的挑战条件下达到95.17%的准确率，比最先进的最佳方法提高了2.8%。此外，模型效率更高，0.5秒样本的平均检测时间低至0.016秒，提高了0.003秒。这使得它对于在线隐写分析任务非常高效，在短样本和低嵌入率约束下提供了检测准确性和效率之间的优越平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Steganalysis methods based on deep learning (DL) often struggle withcomputational complexity and challenges in generalizing across differentdatasets. Incorporating a graph neural network (GNN) into steganalysis schemesenables the leveraging of relational data for improved detection accuracy andadaptability. This paper presents the first application of a Graph NeuralNetwork (GNN), specifically the GraphSAGE architecture, for steganalysis ofcompressed voice over IP (VoIP) speech streams. The method involvesstraightforward graph construction from VoIP streams and employs GraphSAGE tocapture hierarchical steganalysis information, including both fine graineddetails and high level patterns, thereby achieving high detection accuracy.Experimental results demonstrate that the developed approach performs well inuncovering quantization index modulation (QIM)-based steganographic patterns inVoIP signals. It achieves detection accuracy exceeding 98 percent even forshort 0.5 second samples, and 95.17 percent accuracy under challengingconditions with low embedding rates, representing an improvement of 2.8 percentover the best performing state of the art methods. Furthermore, the modelexhibits superior efficiency, with an average detection time as low as 0.016seconds for 0.5-second samples an improvement of 0.003 seconds. This makes itefficient for online steganalysis tasks, providing a superior balance betweendetection accuracy and efficiency under the constraint of short samples withlow embedding rates.</description>
      <author>example@mail.com (Mustapha Hemis, Hamza Kheddar, Mohamed Chahine Ghanem, Bachir Boudraa)</author>
      <guid isPermaLink="false">2507.21591v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Torque-based Graph Surgery:Enhancing Graph Neural Networks with Hierarchical Rewiring</title>
      <link>http://arxiv.org/abs/2507.21422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于扭矩的分层图重连接策略，通过动态调节消息传递来改善图神经网络在异配图中的表示学习能力和对噪声图的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)是学习图结构数据的强大工具，利用消息传递来扩散信息和更新节点表示。然而，图中编码的原始交互可能不利于这个过程，促使了图重连接方法的发展。&lt;h4&gt;目的&lt;/h4&gt;提出一种受经典力学中扭矩概念启发的分层重连接策略，动态调节消息传递，以提高异配图中的表示学习，并增强对噪声图的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;定义了一种感知干扰的扭矩指标，结合结构距离和能量分数量化边引起的扰动，鼓励节点聚合来自低能量邻居的信息。通过修剪高扭矩边和添加低扭矩链接，分层重新配置每层的感受野，抑制传播噪声并增强相关信号。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的广泛评估表明，该方法在异配图和同配图上都优于最先进的方法，并且在噪声图上保持高精度。&lt;h4&gt;结论&lt;/h4&gt;提出的基于扭矩的分层重连接策略能有效改善图神经网络的表示学习能力和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为学习图结构数据的强大工具，利用消息传递来扩散信息和更新节点表示。然而，大多数研究表明图中编码的原始交互可能不利于这一过程，促使了图重连接方法的发展。在这项工作中，我们提出了一种基于扭矩的分层重连接策略，受经典力学中扭矩概念的启发，动态调节消息传递，以提高异配图中的表示学习并增强对噪声图的鲁棒性。具体来说，我们定义了一种感知干扰的扭矩指标，该指标结合结构距离和能量分数来量化边引起的扰动，从而鼓励每个节点聚合来自其最近的低能量邻居的信息。我们使用该指标通过明智地修剪高扭矩边和添加低扭矩链接来分层重新配置每层的感受野，抑制传播噪声并增强相关信号。在基准数据集上的广泛评估表明，我们的方法在异配图和同配图上都优于最先进的方法，并且在噪声图上保持高精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful tools for learning fromgraph-structured data, leveraging message passing to diffuse information andupdate node representations. However, most efforts have suggested that nativeinteractions encoded in the graph may not be friendly for this process,motivating the development of graph rewiring methods. In this work, we proposea torque-driven hierarchical rewiring strategy, inspired by the notion oftorque in classical mechanics, dynamically modulating message passing toimprove representation learning in heterophilous graphs and enhance robustnessagainst noisy graphs. Specifically, we define an interference-aware torquemetric that integrates structural distance and energy scores to quantify theperturbation induced by edges, thereby encouraging each node to aggregateinformation from its nearest low-energy neighbors. We use the metric tohierarchically reconfigure the receptive field of each layer by judiciouslypruning high-torque edges and adding low-torque links, suppressing propagationnoise and boosting pertinent signals. Extensive evaluations on benchmarkdatasets show that our approach surpasses state-of-the-art methods on bothheterophilous and homophilous graphs, and maintains high accuracy on noisygraph.</description>
      <author>example@mail.com (Sujia Huang, Lele Fu, Zhen Cui, Tong Zhang, Na Song, Bo Huang)</author>
      <guid isPermaLink="false">2507.21422v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Graph neural networks for residential location choice: connection to classical logit models</title>
      <link>http://arxiv.org/abs/2507.21334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络(GNN)的离散选择模型(GNN-DCMs)，用于分析居住地选择问题，解决了现有深度学习方法无法捕捉选择备选项之间关系的局限性。&lt;h4&gt;背景&lt;/h4&gt;研究人员采用深度学习进行经典离散选择分析因其能捕捉复杂特征关系并提高预测性能，但现有深度学习方法无法明确捕捉选择备选项之间的关系，这是经典离散选择模型的长期关注点。&lt;h4&gt;目的&lt;/h4&gt;引入图神经网络作为新框架，解决深度学习在离散选择分析中无法捕捉备项关系的问题，同时保持与经典随机效用理论的联系。&lt;h4&gt;方法&lt;/h4&gt;开发GNN-DCMs模型，通过神经网络结构化方法捕捉空间备项依赖关系；理论上证明该模型包含嵌套logit和空间相关logit模型作为特例；通过备选效用间消息传递产生新算法解释；在芝加哥77个社区区域的居住地选择预测中进行实证研究。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-DCMs在预测居住地选择方面优于基准MNL、SCL和前馈神经网络；模型能捕捉个体异质性并表现出空间感知的替代模式。&lt;h4&gt;结论&lt;/h4&gt;GNN-DCMs作为统一且富有表现力的框架，有潜力在复杂空间选择背景下协同离散选择建模和深度学习。&lt;h4&gt;翻译&lt;/h4&gt;研究人员已将深度学习应用于经典离散选择分析，因为它可以捕捉复杂的特征关系并实现更高的预测性能。然而，现有的深度学习方法无法明确捕捉选择备选项之间的关系，这一直是经典离散选择模型的长期关注点。为解决这一差距，本文引入图神经网络(GNN)作为分析居住地选择的新框架。基于GNN的离散选择模型(GNN-DCMs)为神经网络提供了一种结构化方法，用于捕捉空间备选项之间的依赖关系，同时保持与经典随机效用理论的明确联系。理论上，我们证明了GNN-DCMs将嵌套logit(NL)模型和空间相关logit(SCL)模型作为两个特例，通过备选项效用之间的消息传递产生新的算法解释。实证上，GNN-DCMs在预测芝加哥77个社区区域的居住地选择方面优于基准MNL、SCL和前馈神经网络。在模型解释方面，GNN-DCMs可以捕捉个体异质性并表现出空间感知的替代模式。总体而言，这些结果突显了GNN-DCMs作为统一且富有表现力的框架的潜力，用于在复杂空间选择背景下协同离散选择建模和深度学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Researchers have adopted deep learning for classical discrete choice analysisas it can capture complex feature relationships and achieve higher predictiveperformance. However, the existing deep learning approaches cannot explicitlycapture the relationship among choice alternatives, which has been along-lasting focus in classical discrete choice models. To address the gap,this paper introduces Graph Neural Network (GNN) as a novel framework toanalyze residential location choice. The GNN-based discrete choice models(GNN-DCMs) offer a structured approach for neural networks to capturedependence among spatial alternatives, while maintaining clear connections toclassical random utility theory. Theoretically, we demonstrate that theGNN-DCMs incorporate the nested logit (NL) model and the spatially correlatedlogit (SCL) model as two specific cases, yielding novel algorithmicinterpretation through message passing among alternatives' utilities.Empirically, the GNN-DCMs outperform benchmark MNL, SCL, and feedforward neuralnetworks in predicting residential location choices among Chicago's 77community areas. Regarding model interpretation, the GNN-DCMs can captureindividual heterogeneity and exhibit spatially-aware substitution patterns.Overall, these results highlight the potential of GNN-DCMs as a unified andexpressive framework for synergizing discrete choice modeling and deep learningin the complex spatial choice contexts.</description>
      <author>example@mail.com (Zhanhong Cheng, Lingqian Hu, Yuheng Bu, Yuqi Zhou, Shenhao Wang)</author>
      <guid isPermaLink="false">2507.21334v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals</title>
      <link>http://arxiv.org/abs/2507.21190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于图拉普拉斯小波变换的非神经网络学习框架，在图谱域运行，使用结构化多尺度滤波、非线性收缩和符号逻辑，具有高透明度和效率。&lt;h4&gt;背景&lt;/h4&gt;传统图学习依赖于卷积、循环或基于注意力的神经网络架构。&lt;h4&gt;目的&lt;/h4&gt;开发一个原则性、可解释且资源高效的替代深度神经网络架构的图学习框架。&lt;h4&gt;方法&lt;/h4&gt;使用图拉普拉斯小波变换(GLWT)分解图节点信号，通过可解释的非线性调制，并重新组合用于下游任务；支持通过符号领域特定语言进行组合推理。&lt;h4&gt;主要发现&lt;/h4&gt;在合成图去噪和语言标记图上的实验表明，与轻量级图神经网络相比具有竞争力，同时具有更高的透明度和效率。&lt;h4&gt;结论&lt;/h4&gt;该框架为图学习提供了一个有效的、可解释的且资源高效的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种完全基于图拉普拉斯小波变换(GLWT)的非神经网络学习框架。与依赖卷积、循环或基于注意力的神经网络架构不同，我们的模型仅在图谱域运行，使用结构化多尺度滤波、非线性收缩和小波系数上的符号逻辑。定义在图节点上的信号通过GLWT分解，通过可解释的非线性调制，并重新组合用于去噪和标记分类等下游任务。系统支持通过图小波激活上的符号领域特定语言(DSL)进行组合推理。在合成图去噪和语言标记图上的实验表明，与轻量级图神经网络(GNN)相比具有竞争力，同时具有更高的透明度和效率。这项工作为图学习提出了一个原则性、可解释且资源高效的深度神经网络架构替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a fully non neural learning framework based on Graph LaplacianWavelet Transforms (GLWT). Unlike traditional architectures that rely onconvolutional, recurrent, or attention based neural networks, our modeloperates purely in the graph spectral domain using structured multiscalefiltering, nonlinear shrinkage, and symbolic logic over wavelet coefficients.Signals defined on graph nodes are decomposed via GLWT, modulated withinterpretable nonlinearities, and recombined for downstream tasks such asdenoising and token classification. The system supports compositional reasoningthrough a symbolic domain-specific language (DSL) over graph waveletactivations. Experiments on synthetic graph denoising and linguistic tokengraphs demonstrate competitive performance against lightweight GNNs with fargreater transparency and efficiency. This work proposes a principled,interpretable, and resource-efficient alternative to deep neural architecturesfor learning on graphs.</description>
      <author>example@mail.com (Andrew Kiruluta, Andreas Lemos, Priscilla Burity)</author>
      <guid isPermaLink="false">2507.21190v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>MetaCLIP 2: A Worldwide Scaling Recipe</title>
      <link>http://arxiv.org/abs/2507.22062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MetaCLIP 2，这是首个从零开始在全球网络规模的图像-文本对上训练CLIP模型的方案，成功解决了多语言CLIP训练中的'多语言诅咒'问题，并在多语言任务上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一个流行的基础模型，支持零样本分类、检索以及多模态大语言模型的编码器。虽然CLIP已在英语世界的十亿级图像-文本对上成功训练，但扩展到全球网络数据仍面临挑战：缺乏处理非英语数据的方法，以及多语言CLIP的英语性能低于仅英语版本。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从全球网络数据中学习并克服'多语言诅咒'的CLIP训练方法，使英语和非英语数据能够相互受益。&lt;h4&gt;方法&lt;/h4&gt;提出MetaCLIP 2，通过严格的消融实验确定必要的最小改动，设计了一个使英语和非英语数据相互受益的训练配方，从头开始在全球网络规模的图像-文本对上训练CLIP模型。&lt;h4&gt;主要发现&lt;/h4&gt;在零样本ImageNet分类中，MetaCLIP 2 ViT-H/14超越了仅英语版本0.8%，超过了mSigLIP 0.7%；在多语言基准测试上，CVQA达到57.4%，Babel-ImageNet达到50.2%，XM3600达到64.3%，创造了新的最先进水平，且没有使用翻译或特殊架构变更等系统层面的混淆因素。&lt;h4&gt;结论&lt;/h4&gt;MetaCLIP 2成功解决了CLIP模型在多语言场景下的训练挑战，证明了从全球网络数据中学习CLIP模型的可行性，并为多模态学习提供了新的训练范式。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练(CLIP)是一种流行的基础模型，支持从零样本分类、检索到多模态大语言模型(MLLM)的编码器。尽管CLIP已在英语世界的十亿级图像-文本对上成功训练，但进一步扩展其训练以从全球网络数据中学习仍然面临挑战：(1)没有可用方法处理来自非英语世界的数据点；(2)现有多语言CLIP的英语性能比其仅英语版本差，即LLMs中常见的'多语言诅咒'。在此，我们提出了MetaCLIP 2，这是首个从零开始在全球网络规模的图像-文本对上训练CLIP的方案。为了推广我们的发现，我们进行了严格的消融实验，采用了必要的最小改动来解决上述挑战，并提出了一个使英语和非英语世界数据相互受益的配方。在零样本ImageNet分类中，MetaCLIP 2 ViT-H/14超过了其仅英语版本0.8%，超过了mSigLIP 0.7%，并且在多语言基准测试上令人惊讶地创造了新的最先进水平，没有系统层面的混淆因素(如翻译、特殊架构变更)，例如在图像到文本检索任务中，CVQA达到57.4%，Babel-ImageNet达到50.2%，XM3600达到64.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,supporting from zero-shot classification, retrieval to encoders for multimodallarge language models (MLLMs). Although CLIP is successfully trained onbillion-scale image-text pairs from the English world, scaling CLIP's trainingfurther to learning from the worldwide web data is still challenging: (1) nocuration method is available to handle data points from non-English world; (2)the English performance from existing multilingual CLIP is worse than itsEnglish-only counterpart, i.e., "curse of multilinguality" that is common inLLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratchon worldwide web-scale image-text pairs. To generalize our findings, we conductrigorous ablations with minimal changes that are necessary to address the abovechallenges and present a recipe enabling mutual benefits from English andnon-English world data. In zero-shot ImageNet classification, MetaCLIP 2ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,and surprisingly sets new state-of-the-art without system-level confoundingfactors (e.g., translation, bespoke architecture changes) on multilingualbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with64.3% on image-to-text retrieval.</description>
      <author>example@mail.com (Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu)</author>
      <guid isPermaLink="false">2507.22062v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Demand Forecasting via Dual-Strategy Ensembling</title>
      <link>http://arxiv.org/abs/2507.22053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的集成框架，通过层次集成和架构集成两种策略增强基线模型在现实供应链销售预测中的性能，实验证明该方法能有效提高预测准确性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;准确的需求预测对供应链优化至关重要，但由于层次复杂性、领域漂移和外部因素变化，在实践中仍然很困难。最近的基线模型在时间序列预测方面有很大潜力，但存在架构僵化和分布变化下鲁棒性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的集成框架，增强基线模型在现实供应链销售预测中的性能。&lt;h4&gt;方法&lt;/h4&gt;结合两种互补的策略：(1)层次集成(HE)：按语义级别划分训练和推理，捕获局部模式；(2)架构集成(AE)：集成不同模型骨干的预测，减少偏差并提高稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在M5基准测试和三个外部销售数据集上的实验表明，该方法始终优于强基线模型，提高了各个层次级别的准确性，并为复杂预测环境中的泛化能力提升提供了简单有效的机制。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效提升基线模型在供应链销售预测中的性能，特别是在处理复杂预测环境时。&lt;h4&gt;翻译&lt;/h4&gt;准确的需求预测对供应链优化至关重要，但在实践中仍然很困难，这是由于层次复杂性、领域漂移和不断变化的外部因素。虽然最近的基线模型在时间序列预测方面展现出巨大潜力，但它们通常存在架构僵化和在分布变化下鲁棒性有限的问题。在本文中，我们提出了一种统一的集成框架，增强了基线模型在现实供应链销售预测中的性能。我们的方法结合了两种互补的策略：(1)层次集成(HE)，它按语义级别（如商店、类别、部门）划分训练和推理，以捕获局部模式；(2)架构集成(AE)，它集成来自不同模型骨干的预测，以减少偏差并提高稳定性。我们在M5基准测试和三个外部销售数据集上进行了广泛的实验，涵盖了领域内和零样本预测。结果表明，我们的方法始终优于强基线模型，提高了各个层次级别的准确性，并为在复杂预测环境中提高泛化能力提供了一种简单而有效的机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate demand forecasting is critical for supply chain optimization, yetremains difficult in practice due to hierarchical complexity, domain shifts,and evolving external factors. While recent foundation models offer strongpotential for time series forecasting, they often suffer from architecturalrigidity and limited robustness under distributional change. In this paper, wepropose a unified ensemble framework that enhances the performance offoundation models for sales forecasting in real-world supply chains. Our methodcombines two complementary strategies: (1) Hierarchical Ensemble (HE), whichpartitions training and inference by semantic levels (e.g., store, category,department) to capture localized patterns; and (2) Architectural Ensemble (AE),which integrates predictions from diverse model backbones to mitigate bias andimprove stability. We conduct extensive experiments on the M5 benchmark andthree external sales datasets, covering both in-domain and zero-shotforecasting. Results show that our approach consistently outperforms strongbaselines, improves accuracy across hierarchical levels, and provides a simpleyet effective mechanism for boosting generalization in complex forecastingenvironments.</description>
      <author>example@mail.com (Wei Yang, Defu Cao, Yan Liu)</author>
      <guid isPermaLink="false">2507.22053v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2507.22028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出Seeing-to-Experiencing (S2E)框架，通过结合视频预训练和强化学习来增强导航基础模型的交互性和安全性，解决了纯离线训练模型在现实世界导航中的局限性。&lt;h4&gt;背景&lt;/h4&gt;导航基础模型在大量网络规模数据上训练，使智能体能够在不同环境中泛化，但仅基于离线数据训练的模型缺乏对行为后果的推理能力或通过反事实理解进行适应的能力，在需要交互式和安全行为的现实城市导航中面临显著限制。&lt;h4&gt;目的&lt;/h4&gt;引入S2E框架来扩展导航基础模型的能力，通过强化学习解决模型缺乏交互性和适应性的问题，提升在现实世界导航中的表现。&lt;h4&gt;方法&lt;/h4&gt;S2E结合视频预训练和强化学习的优势，保持从大规模真实世界视频获得的泛化能力，同时通过模拟环境中的RL增强交互性；引入锚点引导分布匹配策略稳定学习并建模多样化运动模式；开发残差注意力模块获取反应性行为而不擦除预训练知识；建立基于真实场景3DGS重建的端到端评估基准NavBench-GS。&lt;h4&gt;主要发现&lt;/h4&gt;S2E减轻了仅使用离线数据扩展时常见的收益递减问题；对比分析表明强化学习在机器人学习后训练中比监督微调更有优势；整合交互式在线经验对有效扩展机器人领域基础模型至关重要。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了整合交互式在线经验对有效扩展机器人领域基础模型的关键作用，S2E框架成功提升了导航基础模型在现实世界环境中的交互性和安全性。&lt;h4&gt;翻译&lt;/h4&gt;在大量网络规模数据上训练的导航基础模型使智能体能够跨不同环境和实体泛化。然而，这些仅基于离线数据训练的模型往往缺乏对其行为后果的推理能力或通过反事实理解进行适应的能力。因此，在需要交互式和安全行为的现实世界城市导航中，它们面临显著限制。为应对这些挑战，我们引入了Seeing-to-Experiencing框架，通过强化学习扩展导航基础模型的能力。S2E结合了视频预训练和通过RL后训练的优势。它保持了从大规模真实世界视频获得的泛化能力，同时通过模拟环境中的RL增强交互性。具体来说，我们引入了两项创新：锚点引导分布匹配策略，通过基于锚点的监督稳定学习并建模多样化的运动模式；残差注意力模块，从模拟环境获取反应性行为，同时不擦除模型的预训练知识。此外，我们建立了一个全面的端到端评估基准NavBench-GS，基于真实场景的逼真3DGS重建，包含物理交互。它可以系统评估导航基础模型的泛化能力和安全性。大量实验表明，S2E减轻了仅使用离线数据扩展时常见的收益递减问题。我们对强化学习与监督微调在机器人学习后训练背景下的好处进行了彻底分析。我们的研究结果强调了整合交互式在线经验对有效扩展机器人领域基础模型的关键作用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决导航基础模型仅通过离线数据训练而缺乏交互能力和适应性的问题。在真实世界城市导航中，机器人需要实时决策避开障碍物和行人，而仅通过观察视频学习的模型无法理解行为的后果或适应动态环境。这个问题很重要，因为随着基础模型在机器人领域的应用扩展，提升其交互能力和安全性成为实现可靠自主导航的关键挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了人类学习需要从'看到'到'体验'的过程，认识到仅通过观察视频无法获得真正的适应能力。他们结合了大规模视频预训练和强化学习的优势：预训练提供泛化能力，强化学习提供交互经验。方法设计上借鉴了多个领域：基础模型在大规模数据上预训练的思想、游戏AI中预训练加强化学习的范式、高斯混合模型在多模态表示中的应用，以及残差学习在迁移学习中的有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'Seeing-to-Experiencing'(S2E)框架，结合大规模视频预训练和强化学习微调。整体流程包括：1)预训练阶段：使用锚点引导的高斯混合模型在35小时导航视频上建模多模态行为分布；2)强化学习微调阶段：在模拟环境中使用PPO算法，通过残差注意力模块增强交互能力同时保留预训练知识；3)评估阶段：在NavBench-GS基准和真实机器人上测试性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)锚点引导的分布匹配策略，有效建模多模态行为；2)残差注意力模块，在强化学习中增强交互能力同时保留预训练知识；3)NavBench-GS评估基准，提供逼真的3D交互环境。相比之前工作，S2E不仅使用大规模数据预训练，还通过强化学习获得交互能力；区别于仅在合成环境中训练的方法，S2E能在真实世界泛化；相比简单监督微调，S2E通过强化学习获得更好的样本效率和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的S2E框架通过结合大规模视频预训练与强化学习微调，显著提升了导航基础模型在复杂城市环境中的交互能力和泛化性能，实现了从'看到'到'体验'的关键转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigation foundation models trained on massive webscale data enable agentsto generalize across diverse environments and embodiments. However, thesemodels trained solely on offline data, often lack the capacity to reason aboutthe consequences of their actions or adapt through counterfactualunderstanding. They thus face significant limitations in the real-world urbannavigation where interactive and safe behaviors, such as avoiding obstacles andmoving pedestrians, are critical. To tackle these challenges, we introduce theSeeing-to-Experiencing framework to scale the capability of navigationfoundation models with reinforcement learning. S2E combines the strengths ofpre-training on videos and post-training through RL. It maintains thegeneralizability acquired from large-scale real-world videos while enhancingits interactivity through RL in simulation environments. Specifically, weintroduce two innovations: an Anchor-Guided Distribution Matching strategy,which stabilizes learning and models diverse motion patterns throughanchor-based supervision; and a Residual-Attention Module, which obtainsreactive behaviors from simulation environments without erasing the model'spretrained knowledge. Moreover, we establish a comprehensive end-to-endevaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructionsof real-world scenes that incorporate physical interactions. It cansystematically assess the generalizability and safety of navigation foundationmodels. Extensive experiments show that S2E mitigates the diminishing returnsoften seen when scaling with offline data alone. We perform a thorough analysisof the benefits of Reinforcement Learning compared to Supervised Fine-Tuning inthe context of post-training for robot learning. Our findings emphasize thecrucial role of integrating interactive online experiences to effectively scalefoundation models in Robotics.</description>
      <author>example@mail.com (Honglin He, Yukai Ma, Wayne Wu, Bolei Zhou)</author>
      <guid isPermaLink="false">2507.22028v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT Images</title>
      <link>http://arxiv.org/abs/2507.22024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Cardiac-CLIP，一个专为3D心脏CT图像设计的基础模型，采用两阶段预训练策略开发，结合自监督表示学习和对比学习方法，在心血管诊断任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学领域展现出巨大潜力，但其在复杂心血管诊断中的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对3D心脏CT图像的多模态基础模型，用于支持复杂的心血管诊断任务。&lt;h4&gt;方法&lt;/h4&gt;1. 采用两阶段预训练策略：第一阶段使用3D掩码自编码器进行自监督表示学习；第二阶段引入对比学习对齐视觉和文本表示。2. 收集16641个真实临床CT扫描数据并补充114k个公开数据。3. 将放射学报告标准化为统一模板，构建病理向量生成软标签矩阵。4. 从12个独立机构收集6722个真实临床数据构建评估数据集。&lt;h4&gt;主要发现&lt;/h4&gt;1. Cardiac-CLIP在内部和外部数据的各种下游任务上达到了最先进的性能。2. 在心血管异常分类、信息检索和临床分析等多个任务中均取得优异成绩。3. 在急性冠状动脉综合征的前瞻性预测等复杂临床任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;Cardiac-CLIP是一个有效的心血管诊断工具，能够处理复杂的临床任务，特别是在急性冠状动脉综合征预测方面具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在医学领域已展现出巨大潜力。然而，它们在复杂心血管诊断中的应用仍探索不足。在本文中，我们提出了Cardiac-CLIP，这是一个专为3D心脏CT图像设计的多模态基础模型。Cardiac-CLIP通过两阶段预训练策略开发而成。第一阶段采用3D掩码自编码器进行自监督表示学习，从大规模无标签体积数据中捕获丰富的解剖和上下文特征。第二阶段引入对比学习，对齐视觉和文本表示，促进跨模态理解。为支持预训练，我们收集了16641个真实临床CT扫描数据，并补充了114k个公开可用数据。同时，我们将自由文本放射学报告标准化为统一模板，并根据诊断属性构建病理向量，基于此生成软标签矩阵来监督对比学习过程。另一方面，为了全面评估Cardiac-CLIP的有效性，我们从12个独立机构收集了6722个真实临床数据，并结合开源数据构建了评估数据集。具体而言，Cardiac-CLIP在心血管异常分类、信息检索和临床分析等多个任务上得到了全面评估。实验结果表明，Cardiac-CLIP在内部和外部数据的各种下游任务上均达到了最先进的性能。特别是，Cardiac-CLIP在支持复杂临床任务方面表现出色，如急性冠状动脉综合征的前瞻性预测，这在现实场景中 notoriously 困难。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决心血管疾病诊断中AI模型面临的挑战，包括依赖大量标注数据、泛化能力差、对不同医疗机构数据分布敏感等问题。心血管疾病是全球主要健康威胁，死亡率从1990年的1200万例增加到2021年的1900万例，准确诊断对预防和治疗至关重要。现有AI模型在心脏CT分析中存在局限性，专门针对心脏CT的基础模型可提高诊断准确性，减少对专家标注的依赖，增强临床实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到基础模型在医学领域潜力巨大但心血管诊断应用不足，现有医学视觉-语言模型多针对2D图像设计，心脏CT缺乏专门基础模型。他们设计Cardiac-CLIP采用两阶段预训练策略：第一阶段用3D掩码自编码器进行自监督学习，第二阶段用对比学习对齐视觉和文本表示。借鉴了现有视觉-语言基础模型、MAE自编码器和对比学习框架，但专门针对心脏CT进行了优化，并创新性地使用软标签矩阵替代传统一热矩阵。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建专门针对心脏CT的视觉-语言基础模型，利用大规模无标注数据进行自监督学习，通过对比学习对齐视觉和文本表示，采用软标签矩阵学习细粒度视觉-语言关系。整体流程包括：1)收集16,641个真实临床CT扫描和114k公开数据；2)将放射学报告标准化为统一模板并构建病理向量；3)第一阶段用MAE进行自监督表示学习；4)第二阶段引入文本编码器，用软标签矩阵进行多模态对比学习；5)在心血管异常分类、信息检索和临床分析等任务上评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对心脏CT的视觉-语言基础模型；2)大规模真实临床数据收集(16,641个临床CT扫描)；3)两阶段预训练策略结合MAE和对比学习；4)软标签矩阵设计学习细粒度视觉-语言关系；5)多中心临床评估复杂任务。相比之前工作，Cardiac-CLIP专门针对3D心脏CT而非2D图像，专注于心血管领域知识，使用大规模真实临床数据，采用软标签矩阵，在复杂临床任务如急性冠脉综合征预测上表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Cardiac-CLIP，首个专门针对3D心脏CT图像的大规模视觉-语言基础模型，通过创新的两阶段预训练和软标签设计，在心血管诊断任务上实现了最先进性能，展示了支持临床决策的强大潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have demonstrated remarkable potential in medical domain.However, their application to complex cardiovascular diagnostics remainsunderexplored. In this paper, we present Cardiac-CLIP, a multi-modal foundationmodel designed for 3D cardiac CT images. Cardiac-CLIP is developed through atwo-stage pre-training strategy. The first stage employs a 3D maskedautoencoder (MAE) to perform self-supervised representation learning fromlarge-scale unlabeled volumetric data, enabling the visual encoder to capturerich anatomical and contextual features. In the second stage, contrastivelearning is introduced to align visual and textual representations,facilitating cross-modal understanding. To support the pre-training, we collect16641 real clinical CT scans, supplemented by 114k publicly available data.Meanwhile, we standardize free-text radiology reports into unified templatesand construct the pathology vectors according to diagnostic attributes, basedon which the soft-label matrix is generated to supervise the contrastivelearning process. On the other hand, to comprehensively evaluate theeffectiveness of Cardiac-CLIP, we collect 6,722 real-clinical data from 12independent institutions, along with the open-source data to construct theevaluation dataset. Specifically, Cardiac-CLIP is comprehensively evaluatedacross multiple tasks, including cardiovascular abnormality classification,information retrieval and clinical analysis. Experimental results demonstratethat Cardiac-CLIP achieves state-of-the-art performance across variousdownstream tasks in both internal and external data. Particularly, Cardiac-CLIPexhibits great effectiveness in supporting complex clinical tasks such as theprospective prediction of acute coronary syndrome, which is notoriouslydifficult in real-world scenarios.</description>
      <author>example@mail.com (Yutao Hu, Ying Zheng, Shumei Miao, Xiaolei Zhang, Jiahao Xia, Yaolei Qi, Yiyang Zhang, Yuting He, Qian Chen, Jing Ye, Hongyan Qiao, Xiuhua Hu, Lei Xu, Jiayin Zhang, Hui Liu, Minwen Zheng, Yining Wang, Daimin Zhang, Ji Zhang, Wenqi Shao, Yun Liu, Longjiang Zhang, Guanyu Yang)</author>
      <guid isPermaLink="false">2507.22024v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>The Impact of Foundational Models on Patient-Centric e-Health Systems</title>
      <link>http://arxiv.org/abs/2507.21882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper published in COMPSAC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究评估了AI在医疗应用中的成熟度，发现大多数应用仍处于早期集成阶段，只有少数展示了高级集成。&lt;h4&gt;背景&lt;/h4&gt;人工智能(AI)越来越多地嵌入到医疗技术中，了解以患者为中心的AI应用的成熟度对于评估其可信度、透明度和真实世界影响至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究AI在以患者为中心的医疗应用中的集成和成熟度。&lt;h4&gt;方法&lt;/h4&gt;研究人员调查了116个以患者为中心的医疗应用中的AI功能集成。他们使用大型语言模型(LLMs)提取关键功能特征，然后将这些特征分类为Gartner AI成熟模型的不同阶段。&lt;h4&gt;主要发现&lt;/h4&gt;超过86.21%的应用仍处于AI集成的早期阶段，而只有13.79%展示了先进的AI集成。&lt;h4&gt;结论&lt;/h4&gt;以患者为中心的医疗应用中的AI集成仍然处于早期阶段，需要进一步发展才能实现更高级别的集成。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能(AI)越来越多地嵌入到医疗技术中，了解以患者为中心的AI应用的成熟度对于评估其可信度、透明度和真实世界影响至关重要。在本研究中，我们调查了116个以患者为中心的医疗应用中AI功能集成的成熟度。我们使用大型语言模型(LLMs)提取关键功能特征，然后将这些特征分类为Gartner AI成熟模型的不同阶段。我们的结果显示，超过86.21%的应用仍处于AI集成的早期阶段，而只有13.79%展示了先进的AI集成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Artificial Intelligence (AI) becomes increasingly embedded in healthcaretechnologies, understanding the maturity of AI in patient-centric applicationsis critical for evaluating its trustworthiness, transparency, and real-worldimpact. In this study, we investigate the integration and maturity of AIfeature integration in 116 patient-centric healthcare applications. Using LargeLanguage Models (LLMs), we extracted key functional features, which are thencategorized into different stages of the Gartner AI maturity model. Our resultsshow that over 86.21\% of applications remain at the early stages of AIintegration, while only 13.79% demonstrate advanced AI integration.</description>
      <author>example@mail.com (Elmira Onagh, Alireza Davoodi, Maleknaz Nayebi)</author>
      <guid isPermaLink="false">2507.21882v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking</title>
      <link>http://arxiv.org/abs/2507.21732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMITE的视觉目标跟踪模型，通过原型记忆库和位置提示生成器两个额外模块，解决了现有方法在物体遮挡、干扰和误差传播方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;视觉目标跟踪(VOT)广泛应用于自动驾驶等领域，用于连续跟踪视频中的目标。现有方法分为模板匹配和自回归方法，前者忽略帧间时间依赖性，后者在训练过程中容易偏向目标类别，对未见类别泛化能力弱。&lt;h4&gt;目的&lt;/h4&gt;解决现有VOT方法面临的物体遮挡和干扰挑战，阻止跟踪误差传播，提高对未见类别的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于SAM2视频基础模型构建SAMITE模型，包含两个额外模块：(1)原型记忆库：量化每帧跟踪结果的特征级和位置级正确性，选择最佳帧调节后续帧，过滤遮挡和干扰物体；(2)位置提示生成器：生成位置掩码提示，为目标提供明确位置线索，减少干扰物影响。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准测试上的大量实验表明，SAMITE模型在视觉目标跟踪任务中表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;SAMITE模型有效解决了现有VOT方法面临的挑战，特别是在处理物体遮挡和干扰问题上表现出色，能够有效阻止跟踪误差的传播。&lt;h4&gt;翻译&lt;/h4&gt;视觉目标跟踪(VOT)广泛应用于自动驾驶等应用中，用于连续跟踪视频中的目标。现有方法大致可分为模板匹配和自回归方法，前者通常忽略帧间的时间依赖性，后者在训练过程中容易偏向于目标类别，对未见类别表现出较弱的泛化能力。为解决这些问题，一些方法提议将视频基础模型SAM2适应于VOT，其中每帧的跟踪结果会被编码为记忆，以自回归方式调节其余帧。然而，现有方法未能克服物体遮挡和干扰的挑战，也没有任何措施来拦截跟踪误差的传播。为解决这些问题，我们提出了一个基于SAM2构建的SAMITE模型，包含额外模块：(1)原型记忆库：我们提议量化每帧跟踪结果的特征级和位置级正确性，并选择最佳帧来调节后续帧。由于遮挡和干扰物体的特征在特征级和位置级上不准确，它们的评分自然会较低，因此可以被过滤以拦截误差传播；(2)位置提示生成器：为进一步减少干扰物的影响，我们提议生成位置掩码提示，为目标提供明确的位置线索，实现更准确的跟踪。已在六个基准测试上进行了大量实验，表明SAMITE的优越性。代码可在https://github.com/Sam1224/SAMITE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Object Tracking (VOT) is widely used in applications like autonomousdriving to continuously track targets in videos. Existing methods can beroughly categorized into template matching and autoregressive methods, wherethe former usually neglects the temporal dependencies across frames and thelatter tends to get biased towards the object categories during training,showing weak generalizability to unseen classes. To address these issues, somemethods propose to adapt the video foundation model SAM2 for VOT, where thetracking results of each frame would be encoded as memory for conditioning therest of frames in an autoregressive manner. Nevertheless, existing methods failto overcome the challenges of object occlusions and distractions, and do nothave any measures to intercept the propagation of tracking errors. To tacklethem, we present a SAMITE model, built upon SAM2 with additional modules,including: (1) Prototypical Memory Bank: We propose to quantify thefeature-wise and position-wise correctness of each frame's tracking results,and select the best frames to condition subsequent frames. As the features ofoccluded and distracting objects are feature-wise and position-wise inaccurate,their scores would naturally be lower and thus can be filtered to intercepterror propagation; (2) Positional Prompt Generator: To further reduce theimpacts of distractors, we propose to generate positional mask prompts toprovide explicit positional clues for the target, leading to more accuratetracking. Extensive experiments have been conducted on six benchmarks, showingthe superiority of SAMITE. The code is available athttps://github.com/Sam1224/SAMITE.</description>
      <author>example@mail.com (Qianxiong Xu, Lanyun Zhu, Chenxi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao)</author>
      <guid isPermaLink="false">2507.21732v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic Mutations</title>
      <link>http://arxiv.org/abs/2507.21706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EnTao-GPM是一种创新的突变分类工具，通过跨物种预训练、种系突变专业化和可解释临床框架三种创新方法，提高了致病突变与良性多态性区分的准确性，推动了精准医疗发展。&lt;h4&gt;背景&lt;/h4&gt;在精准医疗中，区分致病突变和良性多态性仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更准确地区分致病突变和良性多态性的方法，以提高基因检测的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;EnTao-GPM由复旦大学和BioMap开发，采用三种创新方法：1)在人类、猪、小鼠等疾病相关哺乳动物基因组上进行跨物种目标预训练，利用进化保守性增强对非编码区域致病基序的解释；2)通过在ClinVar和HGMD数据库上微调实现种系突变专业化，提高SNV和非SNV的准确性；3)整合DNA序列嵌入与基于大语言模型的统计解释，构建可解释的临床框架。&lt;h4&gt;主要发现&lt;/h4&gt;通过与ClinVar验证，EnTao-GPM在突变分类方面表现出更高的准确性。&lt;h4&gt;结论&lt;/h4&gt;EnTao-GPM通过实现更快、更准确、更易获得的遗传变异解释，革新了基因检测，为临床诊断（如变异评估、风险识别、个性化治疗）和研究提供了支持，推进了个性化医学的发展。&lt;h4&gt;翻译&lt;/h4&gt;在精准医疗中，区分致病突变和良性多态性仍然是一个关键挑战。由复旦大学和BioMap开发的EnTao-GPM通过三种创新解决了这一问题：1)在疾病相关的哺乳动物基因组（人类、猪、小鼠）上进行跨物种目标预训练，利用进化保守性来增强致病基序的解释，特别是在非编码区域；2)通过在ClinVar和HGMD上进行微调来实现种系突变专业化，提高SNV和非SNV的准确性；3)整合DNA序列嵌入与基于大语言模型的统计解释，提供可操作见解。通过与ClinVar验证，EnTao-GPM在突变分类方面表现出更高的准确性。它通过实现更快、更准确、更易获得的解释，革新了基因检测，为临床诊断（如变异评估、风险识别、个性化治疗）和研究提供了支持，推进了个性化医学。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distinguishing pathogenic mutations from benign polymorphisms remains acritical challenge in precision medicine. EnTao-GPM, developed by FudanUniversity and BioMap, addresses this through three innovations: (1)Cross-species targeted pre-training on disease-relevant mammalian genomes(human, pig, mouse), leveraging evolutionary conservation to enhanceinterpretation of pathogenic motifs, particularly in non-coding regions; (2)Germline mutation specialization via fine-tuning on ClinVar and HGMD, improvingaccuracy for both SNVs and non-SNVs; (3) Interpretable clinical frameworkintegrating DNA sequence embeddings with LLM-based statistical explanations toprovide actionable insights. Validated against ClinVar, EnTao-GPM demonstratessuperior accuracy in mutation classification. It revolutionizes genetic testingby enabling faster, more accurate, and accessible interpretation for clinicaldiagnostics (e.g., variant assessment, risk identification, personalizedtreatment) and research, advancing personalized medicine.</description>
      <author>example@mail.com (Zekai Lin, Haoran Sun, Yucheng Guo, Yujie Yang, Yanwen Wang, Bozhen Hu, Chonghang Ye, Qirong Yang, Fan Zhong, Xiaoming Zhang, Lei Liu)</author>
      <guid isPermaLink="false">2507.21706v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Segmentation of iPS Cells: Case Study on Model Complexity in Biomedical Imaging</title>
      <link>http://arxiv.org/abs/2507.21608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19th International Conference on Machine Vision Applications MVA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究表明，精心配置的DeepLabv3模型在诱导多能干细胞(iPS)菌落分割任务上表现优异，无需结构修改即可优于大规模基础模型如SAM2和MedSAM2，挑战了模型越大越好的假设。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割不仅需要准确性，还需要在具有挑战性的成像条件下具有鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;评估不同模型在特定生物医学图像分割任务上的性能，并探索模型复杂度与性能之间的关系。&lt;h4&gt;方法&lt;/h4&gt;使用精心配置的DeepLabv3模型进行诱导多能干细胞(iPS)菌落分割，并与SAM2及其医学变种MedSAM2进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;在具有微妙、低对比度边界的专业任务上，精心配置的DeepLabv3模型优于大规模基础模型，表明增加模型复杂度不一定带来更好的性能。&lt;h4&gt;结论&lt;/h4&gt;对于特定领域的生物医学应用，适当调整的简单模型可能比更大更通用的模型提供更好的准确性和实际可靠性。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割不仅需要准确性，还需要在具有挑战性的成像条件下具有鲁棒性。在本研究中，我们展示了一个精心配置的DeepLabv3模型在诱导多能干细胞(iPS)菌落分割方面能够实现高性能，并且在我们的实验条件下，无需结构修改就优于SAM2等大规模基础模型及其医学变种MedSAM2。这些结果表明，对于具有微妙、低对比度边界的专业任务，增加模型复杂度并不一定转化为更好的性能。我们的工作重新审视了越大且越通用的架构总是更好的假设，并提供了证据表明，适当调整的简单模型可能在特定领域的生物医学应用中提供强大的准确性和实际可靠性。我们还提供了开源实现，包括小数据集策略和领域特定编码，旨在支持再生医学及相关领域语义分割的进一步发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image segmentation requires not only accuracy but also robustnessunder challenging imaging conditions. In this study, we show that a carefullyconfigured DeepLabv3 model can achieve high performance in segmenting inducedpluripotent stem (iPS) cell colonies, and, under our experimental conditions,outperforms large-scale foundation models such as SAM2 and its medical variantMedSAM2 without structural modifications. These results suggest that, forspecialized tasks characterized by subtle, low-contrast boundaries, increasedmodel complexity does not necessarily translate to better performance. Our workrevisits the assumption that ever-larger and more generalized architectures arealways preferable, and provides evidence that appropriately adapted, simplermodels may offer strong accuracy and practical reliability in domain-specificbiomedical applications. We also offer an open-source implementation thatincludes strategies for small datasets and domain-specific encoding, with theaim of supporting further advances in semantic segmentation for regenerativemedicine and related fields.</description>
      <author>example@mail.com (Maoquan Zhang, Bisser Raytchev, Xiujuan Sun)</author>
      <guid isPermaLink="false">2507.21608v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models for Wireless Communications: From Adaptation to Autonomy</title>
      <link>http://arxiv.org/abs/2507.21524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了大型语言模型在无线通信领域的应用，通过三个关键方向分析LLMs如何变革无线系统：适应预训练LLMs用于核心通信任务、开发无线专用基础模型、以及实现具有自主推理和协调能力的代理型LLMs。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的出现彻底改变了人工智能，在推理、泛化和零样本学习方面展现出前所未有的能力。无线通信领域日益增长的复杂性和动态性需要智能和自适应的解决方案。&lt;h4&gt;目的&lt;/h4&gt;探索LLMs在变革无线系统方面的作用，分析其应用前景和挑战，为未来智能、自适应和自主的无线网络发展提供方向。&lt;h4&gt;方法&lt;/h4&gt;通过分析三个关键方向：适应预训练LLMs用于核心通信任务、开发无线专用基础模型以平衡通用性和效率、以及使代理型LLMs具备自主推理和协调能力，结合最近的进展和实际案例研究。&lt;h4&gt;主要发现&lt;/h4&gt;LLM-based方法相比传统方法具有独特优势，能够应对无线通信领域的复杂性和动态性需求，为无线系统带来新的可能性。&lt;h4&gt;结论&lt;/h4&gt;LLMs在无线通信领域具有巨大潜力，但仍面临多模态融合、与轻量级模型协作、自我改进能力等开放性挑战，这些挑战也代表了未来的研究机会。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的出现彻底改变了人工智能，在推理、泛化和零样本学习方面提供了前所未有的能力。这些优势为无线通信开辟了新的前沿，无线通信日益增长的复杂性和动态性需要智能和自适应的解决方案。本文探讨了LLMs在变革无线系统方面的作用，重点关注三个关键方向：将预训练的LLMs适应用于核心通信任务，开发无线专用基础模型以平衡通用性和效率，以及使代理型LLMs具备自主推理和协调能力。我们强调了最近的进展、实际案例研究以及基于LLM的方法相对于传统方法的独特优势。最后，我们概述了开放性挑战和研究机会，包括多模态融合、与轻量级模型协作以及自我改进能力，为未来智能、自适应和自主的无线网络绘制了路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of large language models (LLMs) has revolutionized artificialintelligence, offering unprecedented capabilities in reasoning, generalization,and zero-shot learning. These strengths open new frontiers in wirelesscommunications, where increasing complexity and dynamics demand intelligent andadaptive solutions. This article explores the role of LLMs in transformingwireless systems across three key directions: adapting pretrained LLMs for corecommunication tasks, developing wireless-specific foundation models to balanceversatility and efficiency, and enabling agentic LLMs with autonomous reasoningand coordination capabilities. We highlight recent advances, practical casestudies, and the unique benefits of LLM-based approaches over traditionalmethods. Finally, we outline open challenges and research opportunities,including multimodal fusion, collaboration with lightweight models, andself-improving capabilities, charting a path toward intelligent, adaptive, andautonomous wireless networks of the future.</description>
      <author>example@mail.com (Le Liang, Hao Ye, Yucheng Sheng, Ouya Wang, Jiacheng Wang, Shi Jin, Geoffrey Ye Li)</author>
      <guid isPermaLink="false">2507.21524v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Transmission With Machine Language Tokens: A Paradigm for Task-Oriented Agent Communication</title>
      <link>http://arxiv.org/abs/2507.21454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Globecom 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种面向任务的代理通信系统，通过利用大型语言模型学习专业机器语言，并引入联合token和信道编码方案，有效减少了传输开销同时提高了任务准确性。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型的快速发展正在推动各行业范式转变，代理将成为未来生产过程的主要参与者，需要新型AI原生通信系统支持代理间通信。现有LLM输出的人类语言对代理通信而言约束性大且效率低下。&lt;h4&gt;目的&lt;/h4&gt;开发一种面向任务的代理通信系统，解决现有LLM输出不适合代理通信的问题，提高通信效率和任务准确性。&lt;h4&gt;方法&lt;/h4&gt;创新性地提出面向任务的代理通信系统，利用原始LLM学习由token嵌入表示的专业机器语言；同时训练多模态LLM理解应用任务并从多模态输入中提取关键信息，用机器语言token表达；引入联合token和信道编码方案压缩token序列并增强抗噪能力。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法通过学习专业机器语言和联合编码方案，显著提高了通信效率，减少了传输开销，同时与最先进方法相比提高了下游任务的准确性。&lt;h4&gt;结论&lt;/h4&gt;面向任务的代理通信系统有效解决了现有LLM输出不适合代理通信的问题，为未来以代理为主的生产过程提供了高效通信解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型的快速发展正在推动各行业的范式转变。一个显著变化是，代理而非传统机器或人类将成为未来生产过程中的主要参与者，这 consequently 需要一种专门为代理通信设计的新型AI原生通信系统。将大型语言模型的能力与面向任务的语义通信相结合是一种潜在方法。然而，现有LLM的输出是人类语言，对于代理通信而言高度受限且次优。在本文中，我们创新性地提出了一种面向任务的代理通信系统。具体而言，我们利用原始LLM学习由token嵌入表示的专业机器语言。同时，训练多模态LLM以理解应用任务并从多模态输入中提取关键隐含信息，随后用机器语言token表达。这种表示在通过空中接口传输时效率显著提高。此外，为减少传输开销，我们引入了联合token和信道编码方案，通过利用token序列的稀疏性进行压缩，同时增强对信道噪声的鲁棒性。大量实验表明，与最先进的方法相比，我们的方法减少了下游任务的传输开销，同时提高了准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement in large foundation models is propelling the paradigmshifts across various industries. One significant change is that agents,instead of traditional machines or humans, will be the primary participants inthe future production process, which consequently requires a novel AI-nativecommunication system tailored for agent communications. Integrating the abilityof large language models (LLMs) with task-oriented semantic communication is apotential approach. However, the output of existing LLM is human language,which is highly constrained and sub-optimal for agent-type communication. Inthis paper, we innovatively propose a task-oriented agent communication system.Specifically, we leverage the original LLM to learn a specialized machinelanguage represented by token embeddings. Simultaneously, a multi-modal LLM istrained to comprehend the application task and to extract essential implicitinformation from multi-modal inputs, subsequently expressing it using machinelanguage tokens. This representation is significantly more efficient fortransmission over the air interface. Furthermore, to reduce transmissionoverhead, we introduce a joint token and channel coding (JTCC) scheme thatcompresses the token sequence by exploiting its sparsity while enhancingrobustness against channel noise. Extensive experiments demonstrate that ourapproach reduces transmission overhead for downstream tasks while enhancingaccuracy relative to the SOTA methods.</description>
      <author>example@mail.com (Zhuoran Xiao, Chenhui Ye, Yijia Feng, Yunbo Hu, Tianyu Jiao, Liyu Cai, Guangyi Liu)</author>
      <guid isPermaLink="false">2507.21454v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Heterogeneous Ensemble Enables a Universal Uncertainty Metric for Atomistic Foundation Models</title>
      <link>http://arxiv.org/abs/2507.21297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一、可扩展的不确定性度量U，基于异构模型集合和预训练uMLIPs的重用，解决了通用机器学习原子间势(uMLIPs)缺乏可靠不确定性量化的问题。通过不确定性感知的模型蒸馏框架，实现了系统特定势函数的高效开发，显著减少了计算成本，同时提高了精度。&lt;h4&gt;背景&lt;/h4&gt;通用机器学习原子间势(uMLIPs)作为基础模型正在重塑原子尺度模拟，能够以接近从头算的精度但成本更低。然而，缺乏可靠、通用的不确定性量化限制了其安全、大规模使用。&lt;h4&gt;目的&lt;/h4&gt;引入一个统一、可扩展的不确定性度量U，基于异构模型集合和预训练uMLIPs的重用，解决不确定性量化问题，并指导数据选择和微调策略。&lt;h4&gt;方法&lt;/h4&gt;提出基于异构模型集合的不确定性度量U，并利用这个度量构建不确定性感知的模型蒸馏框架，用于产生系统特定的势函数。&lt;h4&gt;主要发现&lt;/h4&gt;1) U在化学和结构多样化的数据集上与真实预测误差有很强的相关性；2) U提供了配置级别风险的稳健排序；3) 对于W，使用仅4%的DFT标签就能达到与完整DFT训练相当的精度；4) 对于MoNbTaW，不需要额外的DFT计算；5) 通过过滤数值标签噪声，蒸馏模型在某些情况下可以超越DFT参考标签的精度。&lt;h4&gt;结论&lt;/h4&gt;不确定性感知方法为uMLIP在部署中的可靠性提供了实际监控，并指导数据选择和微调策略，促进基础模型的构建和安全使用，实现准确、系统特定势函数的成本高效开发。&lt;h4&gt;翻译&lt;/h4&gt;通用机器学习原子间势(uMLIPs)正在重塑原子尺度模拟，作为基础模型，它们能以接近从头算的精度但成本更低。然而，缺乏可靠、通用的不确定性量化限制了其安全、大规模使用。我们在此介绍了一种基于异构模型集合和预训练uMLIPs重用的统一、可扩展的不确定性度量U。在化学和结构多样化的数据集上，U与真实预测误差显示出很强的相关性，并提供了配置级别风险的稳健排序。利用这一度量，我们提出了一个不确定性感知的模型蒸馏框架来产生系统特定的势函数：对于W，使用仅4%的DFT标签就达到了与完整DFT训练相当的精度；对于MoNbTaW，不需要额外的DFT计算。值得注意的是，通过过滤数值标签噪声，蒸馏模型在某些情况下可以超越DFT参考标签的精度。不确定性感知方法为uMLIP在部署中的可靠性提供了实际监控，并指导数据选择和微调策略，从而促进基础模型的构建和安全使用，实现准确、系统特定势函数的成本高效开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal machine learning interatomic potentials (uMLIPs) are reshapingatomistic simulation as foundation models, delivering near \textit{ab initio}accuracy at a fraction of the cost. Yet the lack of reliable, generaluncertainty quantification limits their safe, wide-scale use. Here we introducea unified, scalable uncertainty metric \(U\) based on a heterogeneous modelensemble with reuse of pretrained uMLIPs. Across chemically and structurallydiverse datasets, \(U\) shows a strong correlation with the true predictionerrors and provides a robust ranking of configuration-level risk. Leveragingthis metric, we propose an uncertainty-aware model distillation framework toproduce system-specific potentials: for W, an accuracy comparable to full-DFTtraining is achieved using only \(4\%\) of the DFT labels; for MoNbTaW, noadditional DFT calculations are required. Notably, by filtering numerical labelnoise, the distilled models can, in some cases, surpass the accuracy of the DFTreference labels. The uncertainty-aware approach offers a practical monitor ofuMLIP reliability in deployment, and guides data selection and fine-tuningstrategies, thereby advancing the construction and safe use of foundationmodels and enabling cost-efficient development of accurate, system-specificpotentials.</description>
      <author>example@mail.com (Kai Liu, Zixiong Wei, Wei Gao, Poulumi Dey, Marcel H. F. Sluiter, Fei Shuang)</author>
      <guid isPermaLink="false">2507.21297v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Motion Matters: Motion-guided Modulation Network for Skeleton-based Micro-Action Recognition</title>
      <link>http://arxiv.org/abs/2507.21977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种运动引导调制网络(MMN)用于微动作识别，通过捕获和调制细微运动线索来提高识别准确率。&lt;h4&gt;背景&lt;/h4&gt;微动作(MAs)是社会互动中重要的非语言沟通形式，在人类情感分析中有潜在应用。然而，现有微动作识别方法往往忽略了微动作中固有的细微变化，限制了区分相似微动作的准确性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽视微动作固有细微变化的问题，提高区分细微变化的微动作的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出运动引导调制网络(MMN)，包含运动引导的骨骼调制模块(MSM)在骨骼级别注入运动线索，以及运动引导的时间调制模块(MTM)在帧级别整合运动信息，同时采用运动一致性学习策略从多尺度特征聚合运动线索。&lt;h4&gt;主要发现&lt;/h4&gt;在Micro-Action 52和iMiGUE数据集上的实验结果表明，MMN在基于骨骼的微动作识别中达到了最先进的性能，证明了明确建模细微运动线索的重要性。&lt;h4&gt;结论&lt;/h4&gt;MMN模型通过隐式捕获和调制细微运动线索，有效增强了时空表征学习，显著提高了微动作识别的准确性。&lt;h4&gt;翻译&lt;/h4&gt;微动作(MAs)是社会互动中重要的非语言沟通形式，在人类情感分析中有潜在应用。然而，现有的微动作识别方法往往忽略了微动作中固有的细微变化，这限制了区分具有细微变化的微动作的准确性。为了解决这个问题，我们提出了一种新颖的运动引导调制网络(MMN)，该网络隐式捕获和调制细微运动线索以增强时空表征学习。具体来说，我们引入了运动引导的骨骼调制模块(MSM)，在骨骼级别注入运动线索，作为控制信号来引导空间表征建模。同时，我们设计了运动引导的时间调制模块(MTM)，在帧级别整合运动信息，促进微动作中整体运动模式的建模。最后，我们提出了运动一致性学习策略，从多尺度特征聚合运动线索用于微动作分类。在Micro-Action 52和iMiGUE数据集上的实验结果表明，MMN在基于骨骼的微动作识别中达到了最先进的性能，强调了明确建模细微运动线索的重要性。代码将在https://github.com/momiji-bit/MMN上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Micro-Actions (MAs) are an important form of non-verbal communication insocial interactions, with potential applications in human emotional analysis.However, existing methods in Micro-Action Recognition often overlook theinherent subtle changes in MAs, which limits the accuracy of distinguishing MAswith subtle changes. To address this issue, we present a novel Motion-guidedModulation Network (MMN) that implicitly captures and modulates subtle motioncues to enhance spatial-temporal representation learning. Specifically, weintroduce a Motion-guided Skeletal Modulation module (MSM) to inject motioncues at the skeletal level, acting as a control signal to guide spatialrepresentation modeling. In parallel, we design a Motion-guided TemporalModulation module (MTM) to incorporate motion information at the frame level,facilitating the modeling of holistic motion patterns in micro-actions.Finally, we propose a motion consistency learning strategy to aggregate themotion cues from multi-scale features for micro-action classification.Experimental results on the Micro-Action 52 and iMiGUE datasets demonstratethat MMN achieves state-of-the-art performance in skeleton-based micro-actionrecognition, underscoring the importance of explicitly modeling subtle motioncues. The code will be available at https://github.com/momiji-bit/MMN.</description>
      <author>example@mail.com (Jihao Gu, Kun Li, Fei Wang, Yanyan Wei, Zhiliang Wu, Hehe Fan, Meng Wang)</author>
      <guid isPermaLink="false">2507.21977v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic cosmological inference on HI tomographic data</title>
      <link>http://arxiv.org/abs/2507.21682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures, 2 tables, Accepted for publication in  Astrophysics and Space Science&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了从中间红移的21厘米层析数据中获取宇宙学信息的可能性，通过深度学习方法和似然无关推断实现了对宇宙学参数的精确约束。&lt;h4&gt;背景&lt;/h4&gt;21厘米层析数据包含丰富的宇宙学信息，但如何有效提取这些信息是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法从21厘米数据中提取宇宙学参数，并评估其准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用三维卷积编码器将中性氢三维数据映射到低维潜在空间，然后通过掩码自回归流进行密度估计和似然无关推断，将潜在表示映射到宇宙学参数。&lt;h4&gt;主要发现&lt;/h4&gt;深度编码器学习的表示在潜在空间中可分离；神经密度估计器能以高精度约束宇宙学参数；大多数测试实例的真实值落在不确定性范围内；密度估计器的后验不确定性合理校准；特征提取器对分布外数据集具有鲁棒性；即使在分布外数据集上训练，仍能保持良好的宇宙学约束精度。&lt;h4&gt;结论&lt;/h4&gt;该方法在分析未来各种调查的21厘米数据时将具有广泛应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们探索了从中间红移的21厘米层析数据中获取宇宙学信息的可能性。我们方法的第一步是训练一个由多个三维卷积层组成的编码器，将中性氢三维数据投射到低维潜在空间。一旦预训练完成，特征提取器能够生成三维网格表示，这些表示将通过似然无关推断映射到宇宙学参数。对于后者，它被框架化为一个密度估计问题，我们考虑了一种贝叶斯近似方法，利用掩码自回归流的能力来估计后验。研究发现，深度编码器学习的表示在潜在空间中是可分离的。结果表明，在潜在代码上训练的神经密度估计器能够以高精度约束宇宙学，测试集中大多数实例的真实值落在不确定性范围内。证实了密度估计器的后验不确定性是合理校准的。我们还研究了特征提取器的鲁棒性，通过使用它来压缩分布外数据集，这些数据集要么来自不同的模拟，要么来自同一模拟但不同红移。我们发现，虽然在对应不同类型分布外数据集的潜在代码上训练，概率模型仍能合理地约束宇宙学。这突显了本文考虑的密度估计器的预测能力和编码器检索的潜在代码的意义。我们相信，本概念验证中规定的方法在不久的将来分析各种调查的21厘米数据时将非常有用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从中性氢(HI)的3D层析数据中提取宇宙学参数(Ωm, σ8)及其不确定性。这个问题非常重要，因为随着即将到来的HIRAX、CHIME和SKA等大型HI巡天项目，将产生大量3D层析数据，需要有效方法分析；传统基于统计量的方法会损失信息，特别是非线性尺度上的信息；21厘米信号是非高斯的，利用高阶统计量复杂；而这种方法能处理高维数据并提供不确定性估计，对理解宇宙大尺度结构和暗能量等关键问题有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者设计方法时借鉴了多项现有工作：参考了Bull等人(2015)基于Fisher预测框架的研究；借鉴了Zhao等人(2022, 2024)和Binnie等人(2025)使用3D CNN处理HI数据的工作；参考了Andrianomena &amp; Hassan(2023a, 2023b)使用变分自编码器学习特征表示的方法。作者注意到虽然已有研究使用2D HI地图，但利用3D层析数据的研究较少，因此设计了结合深度学习和贝叶斯推断的两步方法：先用3D卷积网络将高维数据压缩为低维特征，再用基于似然自由推断的贝叶斯方法映射到宇宙学参数。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用深度学习将高维3D HI数据压缩为低维特征表示，保留宇宙学信息，然后使用概率方法从这些特征中提取宇宙学参数及其不确定性。整体流程：1)数据准备：使用CAMELS项目的SIMBA和TNG模拟数据集，进行数据增强；2)编码器训练：构建深度神经网络，编码器由多个3D卷积层组成，将3D网格压缩为1024维向量，回归器预测宇宙学参数；3)潜在表示提取：用训练好的编码器将3D网格转换为潜在表示；4)后验估计器训练：使用自动后验变换(APT)方法，以掩码自回归流(MAF)为密度估计器；5)参数推断：对新的3D网格，提取潜在表示并生成参数样本，计算中位数和不确定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次利用深度学习从低/中等红移(z=1)的21厘米3D层析数据中提取宇宙学参数；2)结合深度学习和贝叶斯方法提供参数不确定性估计；3)使用自动后验变换框架，避免似然函数的显式形式；4)探索方法在不同模拟数据集和不同红移的鲁棒性。相比之前工作：1)相比基于功率谱的传统方法，保留了更多非线性尺度信息；2)相比基于2D地图的方法，利用了3D数据的额外信息；3)相比Zhao等人的工作，使用更深网络架构并提供不确定性估计；4)相比Andrianomena &amp; Hassan的2D工作，扩展到3D数据并提高σ8估计精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合深度学习和贝叶斯推断的新方法，能够从中性氢的3D层析数据中精确提取宇宙学参数及其不确定性，并展示了该方法在不同模拟和红移条件下的鲁棒性，为即将到来的大型HI巡天项目提供了有力的数据分析工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore the possibility of retrieving cosmological information from 21-cmtomographic data at intermediate redshift. The first step in our approachconsists of training an encoder, composed of several three dimensionalconvolutional layers, to cast the neutral hydrogen 3D data into a lowerdimension latent space. Once pre-trained, the featurizer is able to generate 3Dgrid representations which, in turn, will be mapped onto cosmology($\Omega_{\rm m}$, $\sigma_{8}$) via likelihood-free inference. For the latter,which is framed as a density estimation problem, we consider a Bayesianapproximation method which exploits the capacity of Masked Autoregressive Flowto estimate the posterior. It is found that the representations learned by thedeep encoder are separable in latent space. Results show that the neuraldensity estimator, trained on the latent codes, is able to constrain cosmologywith a precision of $R^2 \ge 0.91$ on all parameters and that most of theground truth of the instances in the test set fall within $1\sigma$uncertainty. It is established that the posterior uncertainty from the densityestimator is reasonably calibrated. We also investigate the robustness of thefeature extractor by using it to compress out-of-distribution dataset, that iseither from a different simulation or from the same simulation but at differentredshift. We find that, while trained on the latent codes corresponding todifferent types of out-of-distribution dataset, the probabilistic model isstill reasonably capable of constraining cosmology, with $R^2 \ge 0.80$ ingeneral. This highlights both the predictive power of the density estimatorconsidered in this work and the meaningfulness of the latent codes retrieved bythe encoder. We believe that the approach prescribed in this proof of conceptwill be of great use when analyzing 21-cm data from various surveys in the nearfuture.</description>
      <author>example@mail.com (Sambatra Andrianomena)</author>
      <guid isPermaLink="false">2507.21682v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Genome Embeddings</title>
      <link>http://arxiv.org/abs/2507.21648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 16 figures, 10 tables. Camera-ready version for ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用双曲卷积神经网络（hyperbolic CNNs）进行基因组序列建模的新方法，能够更好地捕捉生物系统的进化结构，生成更具表达力的DNA序列表示，并在大多数基准测试中优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;当前基因组序列建模方法难以将机器学习模型的归纳偏置与生物系统进化形成的结构相匹配，传统欧几里得空间模型无法充分捕捉基因组数据的层次结构和进化关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更好地捕捉基因组序列进化结构的新方法，生成更具表达力的DNA序列表示，避免系统发育映射的需要，并能识别与核心功能和调控行为相关的序列关键特性。&lt;h4&gt;方法&lt;/h4&gt;使用双曲卷积神经网络（hyperbolic CNNs）建模基因组序列，创建转座子基准（Transposable Elements Benchmark）数据集，并开发经验方法来解释数据嵌入的双曲性。&lt;h4&gt;主要发现&lt;/h4&gt;在42个基因组解释基准数据集中的37个上，双曲模型优于欧几里得对应模型；在7个GUE基准数据集上超越了最先进的性能；在使用数量级更少的参数且避免预训练的情况下，持续优于许多DNA语言模型。&lt;h4&gt;结论&lt;/h4&gt;双曲CNN框架为基因组表示学习提供了一种稳健的新范式，能够更好地捕捉基因组数据的进化结构和层次关系，同时保持计算效率。&lt;h4&gt;翻译&lt;/h4&gt;当前基因组序列建模方法往往难以将机器学习模型的归纳偏置与生物系统进化形成的结构相匹配。为此，我们提出了双曲CNNs的一种新颖应用，利用这种结构实现更具表达力的DNA序列表示。我们的策略规避了明确系统发育映射的需要，同时能够识别与核心功能和调控行为相关的序列关键特性。在42个基因组解释基准数据集中的37个上，我们的双曲模型优于其欧几里得对应模型。值得注意的是，我们的方法甚至在7个GUE基准数据集上超越了最先进的性能，在使用数量级更少的参数且避免预训练的情况下，持续优于许多DNA语言模型。我们的结果包括一套新的基准数据集——转座子基准，它探索了基因组中一个重要但研究不足的组成部分，具有深远的进化意义。我们通过探索双曲模型如何在各种数据生成条件下识别基因组信号，以及构建一种解释数据嵌入双曲性的经验方法，进一步论证了我们工作的动机。在这些评估中，我们发现持续的证据强调了我们的双曲框架作为基因组表示学习的稳健范式的潜力。我们的代码和基准数据集可在https://github.com/rrkhan/HGE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current approaches to genomic sequence modeling often struggle to align theinductive biases of machine learning models with the evolutionarily-informedstructure of biological systems. To this end, we formulate a novel applicationof hyperbolic CNNs that exploits this structure, enabling more expressive DNAsequence representations. Our strategy circumvents the need for explicitphylogenetic mapping while discerning key properties of sequences pertaining tocore functional and regulatory behavior. Across 37 out of 42 genomeinterpretation benchmark datasets, our hyperbolic models outperform theirEuclidean equivalents. Notably, our approach even surpasses state-of-the-artperformance on seven GUE benchmark datasets, consistently outperforming manyDNA language models while using orders of magnitude fewer parameters andavoiding pretraining. Our results include a novel set of benchmarkdatasets--the Transposable Elements Benchmark--which explores a major butunderstudied component of the genome with deep evolutionary significance. Wefurther motivate our work by exploring how our hyperbolic models recognizegenomic signal under various data-generating conditions and by constructing anempirical method for interpreting the hyperbolicity of dataset embeddings.Throughout these assessments, we find persistent evidence highlighting thepotential of our hyperbolic framework as a robust paradigm for genomerepresentation learning. Our code and benchmark datasets are available athttps://github.com/rrkhan/HGE.</description>
      <author>example@mail.com (Raiyan R. Khan, Philippe Chlenski, Itsik Pe'er)</author>
      <guid isPermaLink="false">2507.21648v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>GUARD-CAN: Graph-Understanding and Recurrent Architecture for CAN Anomaly Detection</title>
      <link>http://arxiv.org/abs/2507.21640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Comments:12 pages, 3 figures, 3 tables; accepted to the 26th World  Conference on Information Security Applications (WISA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GUARD-CAN，一种结合基于图的表示学习与时间序列建模的车载网络异常检测框架，能够有效检测四种类型的CAN攻击而无需复杂特征工程。&lt;h4&gt;背景&lt;/h4&gt;现代车载网络面临各种网络威胁，原因是控制器局域网（CAN）缺乏加密和认证机制。&lt;h4&gt;目的&lt;/h4&gt;解决CAN总线安全性问题，开发一个能够有效检测异常的框架。&lt;h4&gt;方法&lt;/h4&gt;GUARD-CAN将CAN消息分割为固定长度窗口并转换为保留消息顺序的图，利用过完备自编码器和图卷积网络生成图嵌入向量，通过门控循环单元检测时序异常模式，并在序列级别和窗口级别进行异常检测。&lt;h4&gt;主要发现&lt;/h4&gt;通过基于香农熵的分析验证了窗口大小选择的重要性，模型能有效检测泛洪、模糊、重放和欺骗四种CAN攻击。&lt;h4&gt;结论&lt;/h4&gt;GUARD-CAN无需依赖复杂特征工程，就能有效检测多种类型的CAN攻击。&lt;h4&gt;翻译&lt;/h4&gt;现代车载网络由于控制器局域网（CAN）缺乏加密和认证而面临各种网络威胁。为解决这一安全问题，本文提出了GUARD-CAN，一种结合基于图的表示学习与时间序列建模的异常检测框架。GUARD-CAN将CAN消息分割为固定长度的窗口，并将每个窗口转换为保留消息顺序的图。为了在同一窗口中检测时间和结构感知上下文中的异常，GUARD-CAN利用过完备自编码器（AE）和图卷积网络（GCN）生成图嵌入向量。模型将这些向量分组为序列，并输入到门控循环单元（GRU）中，以检测跨图的时序异常模式。GUARD-CAN在序列级别和窗口级别执行异常检测，这允许多角度性能评估。模型还通过基于香农熵的分析验证了窗口大小选择的重要性。结果表明，所提出的模型无需依赖复杂的特征工程，就能有效检测四种类型的CAN攻击（泛洪、模糊、重放和欺骗攻击）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern in-vehicle networks face various cyber threats due to the lack ofencryption and authentication in the Controller Area Network (CAN). To addressthis security issue, this paper presents GUARD-CAN, an anomaly detectionframework that combines graph-based representation learning with time-seriesmodeling. GUARD-CAN splits CAN messages into fixed-length windows and convertseach window into a graph that preserves message order. To detect anomalies inthe timeaware and structure-aware context at the same window, GUARD-CAN takesadvantage of the overcomplete Autoencoder (AE) and Graph Convolutional Network(GCN) to generate graph embedding vectors. The model groups these vectors intosequences and feeds them into the Gated Recurrent Unit (GRU) to detect temporalanomaly patterns across the graphs. GUARD-CAN performs anomaly detection atboth the sequence level and the window level, and this allows multi-perspectiveperformance evaluation. The model also verifies the importance of window sizeselection through an analysis based on Shannon entropy. As a result, GUARD-CANshows that the proposed model detects four types of CAN attacks (flooding,fuzzing, replay and spoofing attacks) effectively without relying on complexfeature engineering.</description>
      <author>example@mail.com (Hyeong Seon Kim, Huy Kang Kim)</author>
      <guid isPermaLink="false">2507.21640v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning</title>
      <link>http://arxiv.org/abs/2507.21588v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PHP（Progressive Homeostatic and Plastic audio-visual prompt）的三阶段方法，用于解决音频-视觉多任务增量学习中的知识保留与增量学习平衡问题。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉多任务增量学习旨在无需对所有任务进行联合训练，而是从多个音频-视觉任务中持续学习。主要挑战是如何在促进新任务学习的同时保留旧任务知识。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够在音频-视觉多任务增量学习场景中有效平衡知识保留与新任务学习能力。&lt;h4&gt;方法&lt;/h4&gt;PHP方法包含三个阶段：1）浅层阶段：设计任务共享的模态聚合适配器，促进跨任务和跨模态表征学习；2）中层阶段：提出任务特定的模态共享动态生成适配器，构建针对特定任务但跨模态通用的提示；3）深层阶段：引入任务特定的模态无关提示，针对每个任务和模态的特定信息优化理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过三个阶段的结合，PHP能够在保留任务特定提示的同时，为新任务调整共享参数，有效平衡知识共享与特异性。&lt;h4&gt;结论&lt;/h4&gt;该方法在四个不同顺序的任务（AVE、AVVP、AVS和AVQA）上达到了最先进的性能（SOTA）。代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;音频-视觉多任务增量学习旨在无需对所有任务进行联合训练，而是从多个音频-视觉任务中持续学习。该问题的挑战在于如何在促进新任务学习的同时保留旧任务知识。为应对这些挑战，我们引入了一种三阶段的渐进式稳态与可塑音频-视觉提示（PHP）方法。在浅层阶段，我们设计了任务共享的模态聚合适配器，促进跨任务和跨模态的音频-视觉表征学习，以增强任务间的共享理解。在中层阶段，我们提出了任务特定的模态共享动态生成适配器，构建针对单个任务定制但跨模态通用的提示，平衡模型保留知识以避免遗忘的能力及其多任务迁移的潜力。在深层阶段，我们引入了任务特定的模态无关提示，通过针对每个任务和模态的特定信息进一步优化理解能力。通过整合这三个阶段，PHP在为新任务调整共享参数的同时保留任务特定提示，有效平衡了知识共享与特异性。我们的方法在四个任务的不同顺序（AVE、AVVP、AVS和AVQA）上达到了最先进的性能。我们的代码可在https://github.com/ENJOY-Yin-jiong/PHP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-visual multi-task incremental learning aims to continuously learn frommultiple audio-visual tasks without the need for joint training on all tasks.The challenge of the problem is how to preserve the old task knowledge whilefacilitating the learning of new task with previous experiences. To addressthese challenges, we introduce a three-stage Progressive Homeostatic andPlastic audio-visual prompt (PHP) method. In the shallow phase, we design thetask-shared modality aggregating adapter to foster cross-task and cross-modalaudio-visual representation learning to enhance shared understanding betweentasks. In the middle phase, we propose the task-specific modality-shareddynamic generating adapter, which constructs prompts that are tailored toindividual tasks while remaining general across modalities, which balances themodels ability to retain knowledge against forgetting with its potential forversatile multi-task transferability. In the deep phase, we introduce thetask-specific modality-independent prompts to further refine the understandability by targeting individual information for each task and modality. Byincorporating these three phases, PHP retains task-specific prompts whileadapting shared parameters for new tasks to effectively balance knowledgesharing and specificity. Our method achieves SOTA performance in differentorders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available athttps://github.com/ENJOY-Yin-jiong/PHP.</description>
      <author>example@mail.com (Jiong Yin, Liang Li, Jiehua Zhang, Yuhan Gao, Chenggang Yan, Xichun Sheng)</author>
      <guid isPermaLink="false">2507.21588v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Suppressing Gradient Conflict for Generalizable Deepfake Detection</title>
      <link>http://arxiv.org/abs/2507.21530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  V1&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种冲突抑制深度伪造检测(CS-DFD)框架，解决了联合训练原始和在线合成伪造图像导致的性能下降问题，通过两个协同模块(UVS和CGR)有效缓解了梯度冲突，提升了模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度伪造检测模型需要能够泛化到不断发展的操纵技术。在线合成包含广泛可泛化伪影的假图像是一种有前景的训练数据增强策略，但联合训练原始和在线合成伪造品可能导致性能下降，这与'更多源域数据应提高检测准确性'的普遍信念相矛盾。&lt;h4&gt;目的&lt;/h4&gt;解决联合训练原始和在线合成伪造品导致的性能下降问题，克服梯度冲突，解决源域准确性和目标域泛化之间的权衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出冲突抑制深度伪造检测(CS-DFD)框架，包含两个协同模块：1)更新向量搜索(UVS)模块：在初始梯度向量附近寻找替代更新向量，调和原始和在线合成伪造品之间的差异；2)冲突梯度减少(CGR)模块：通过冲突下降损失强制实现低冲突特征嵌入空间，惩罚未对齐的梯度方向，引导学习具有对齐、无冲突梯度的表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过经验分析发现，性能下降的原因是反向传播过程中的梯度冲突，这些冲突迫使模型在源域准确性和目标域泛化之间进行权衡。&lt;h4&gt;结论&lt;/h4&gt;CS-DFD在多个深度伪造基准测试上实现了最先进的性能，在域内检测准确性和跨域泛化方面都表现出色，有效缓解了参数优化和表示学习中的梯度干扰。&lt;h4&gt;翻译&lt;/h4&gt;稳健的深度伪造检测模型必须能够泛化到不断发展的操纵技术，超出训练数据的范围。一种有前景的策略是在训练数据中添加在线合成的包含广泛可泛化伪影的假图像。然而，在深度伪造检测的背景下，令人惊讶的是，同时训练原始和在线合成的伪造品可能导致性能下降。这与'纳入更多源域数据应提高检测准确性'的普遍信念相矛盾。通过经验分析，我们将这种退化追溯到反向传播过程中的梯度冲突，这迫使模型在源域准确性和目标域泛化之间进行权衡。为了解决这个问题，我们提出了一个冲突抑制深度伪造检测(CS-DFD)框架，通过两个协同模块明确缓解梯度冲突。首先，更新向量搜索(UVS)模块在初始梯度向量附近寻找替代更新向量，以调和原始和在线合成伪造品之间的差异。通过将搜索过程转化为极值优化问题，UVS产生唯一的更新向量，最大化每种数据类型的损失同时减少。其次，冲突梯度减少(CGR)模块通过一种新的冲突下降损失强制实现低冲突特征嵌入空间。该损失惩罚未对齐的梯度方向，并引导学习具有对齐、无冲突梯度的表示。UVS和CGR的协同作用减轻了参数优化和表示学习中的梯度干扰。在多个深度伪造基准测试上的实验表明，CS-DFD在域内检测准确性和跨域泛化方面都取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust deepfake detection models must be capable of generalizing toever-evolving manipulation techniques beyond training data. A promisingstrategy is to augment the training data with online synthesized fake imagescontaining broadly generalizable artifacts. However, in the context of deepfakedetection, it is surprising that jointly training on both original and onlinesynthesized forgeries may result in degraded performance. This contradicts thecommon belief that incorporating more source-domain data should enhancedetection accuracy. Through empirical analysis, we trace this degradation togradient conflicts during backpropagation which force a trade-off betweensource domain accuracy and target domain generalization. To overcome thisissue, we propose a Conflict-Suppressed Deepfake Detection (CS-DFD) frameworkthat explicitly mitigates the gradient conflict via two synergistic modules.First, an Update Vector Search (UVS) module searches for an alternative updatevector near the initial gradient vector to reconcile the disparities of theoriginal and online synthesized forgeries. By further transforming the searchprocess into an extremum optimization problem, UVS yields the uniquely updatevector, which maximizes the simultaneous loss reductions for each data type.Second, a Conflict Gradient Reduction (CGR) module enforces a low-conflictfeature embedding space through a novel Conflict Descent Loss. This losspenalizes misaligned gradient directions and guides the learning ofrepresentations with aligned, non-conflicting gradients. The synergy of UVS andCGR alleviates gradient interference in both parameter optimization andrepresentation learning. Experiments on multiple deepfake benchmarksdemonstrate that CS-DFD achieves state-of-the-art performance in both in-domaindetection accuracy and cross-domain generalization.</description>
      <author>example@mail.com (Ming-Hui Liu, Harry Cheng, Xin Luo, Xin-Shun Xu)</author>
      <guid isPermaLink="false">2507.21530v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Class Tokens: LLM-guided Dominant Property Mining for Few-shot Classification</title>
      <link>http://arxiv.org/abs/2507.20511v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BCT-CLIP的新型少样本学习方法，通过探索主导属性而非仅使用类标记，利用对比学习提升少样本学习性能。&lt;h4&gt;背景&lt;/h4&gt;少样本学习(FSL)旨在仅使用少量图像来识别新类别的泛化能力，但因数据稀缺性面临重大挑战。基于对比语言-图像预训练的CLIP类方法通过利用类名的文本表征缓解了这一问题，但简单将视觉表征与类名嵌入对齐会损害新类别判别中的视觉多样性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中视觉表征与类名简单对齐导致的视觉多样性损失问题，通过探索主导属性来提升少样本学习的性能。&lt;h4&gt;方法&lt;/h4&gt;提出BCT-CLIP方法，利用基于大型语言模型(LLM)的先验知识；开发多属性生成器(MPG)生成视觉属性标记；设计基于聚类剪枝的LLM辅助检索程序获取主导属性描述；提出新的属性标记对比学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在11个广泛使用的数据集上表现优异，表明对主导属性的研究有效推进了判别性类别特定表征学习和少样本分类。&lt;h4&gt;结论&lt;/h4&gt;探索主导属性能够改进少样本学习中判别性的类别特定表征学习，为解决数据稀缺问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;少样本学习(FSL)旨在仅使用少量图像来识别新类别的泛化能力，但因数据稀缺性面临重大挑战。最近的基于对比语言-图像预训练的CLIP类方法通过利用类名的文本表征来缓解这一问题，用于未见过的图像发现。尽管取得了成功，但简单地将视觉表征与类名嵌入对齐会损害新类别判别中的视觉多样性。为此，我们提出了一种新型的少样本学习方法(BCT-CLIP)，通过对比学习探索主导属性，而不仅仅是使用类标记。通过利用基于大型语言模型(LLM)的先验知识，我们的方法推动了具有全面结构化图像表征的少样本学习，包括全局类别表征和感知补丁的属性嵌入。特别是，我们提出了一个具有感知补丁交叉注意力的新型多属性生成器(MPG)来生成多个视觉属性标记，一个基于聚类剪枝的LLM辅助检索程序来获取主导属性描述，以及一种新的属性标记对比学习策略。在11个广泛使用的数据集上的优异性能表明，我们对主导属性的研究推进了判别性类别特定表征学习和少样本分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot Learning (FSL), which endeavors to develop the generalizationability for recognizing novel classes using only a few images, facessignificant challenges due to data scarcity. Recent CLIP-like methods based oncontrastive language-image pertaining mitigate the issue by leveraging textualrepresentation of the class name for unseen image discovery. Despite theachieved success, simply aligning visual representations to class nameembeddings would compromise the visual diversity for novel classdiscrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method(BCT-CLIP) that explores \textbf{dominating properties} via contrastivelearning beyond simply using class tokens. Through leveraging LLM-based priorknowledge, our method pushes forward FSL with comprehensive structural imagerepresentations, including both global category representation and thepatch-aware property embeddings. In particular, we presented a novelmulti-property generator (MPG) with patch-aware cross-attentions to generatemultiple visual property tokens, a Large-Language Model (LLM)-assistantretrieval procedure with clustering-based pruning to obtain dominating propertydescriptions, and a new contrastive learning strategy for property-tokenlearning. The superior performances on the 11 widely used datasets demonstratethat our investigation of dominating properties advances discriminativeclass-specific representation learning and few-shot classification.</description>
      <author>example@mail.com (Wei Zhuo, Runjie Luo, Wufeng Xue, Linlin Shen)</author>
      <guid isPermaLink="false">2507.20511v2</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Adaptive Structure Learning for Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2507.21191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Initially submitted this draft at Tiny ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进图卷积网络(GCNs)在异质图上性能的方法，通过结构学习重新连接浅层GCN中的边，以捕捉同类非局部节点之间的长距离依赖关系，避免因过平滑导致的性能下降。&lt;h4&gt;背景&lt;/h4&gt;图卷积网络在图表示学习中获得了广泛关注，最近的研究集中在提高异质图在各种实际应用中的性能。&lt;h4&gt;目的&lt;/h4&gt;解决典型消息传递机制中局部特征聚合无法捕捉同类非局部节点之间长距离依赖关系的问题，以及异质图中固有连接结构与同类远距离节点信息共享之间的冲突。&lt;h4&gt;方法&lt;/h4&gt;通过结构学习重新连接浅层GCN中的边，参数化邻接矩阵来学习非局部节点之间的连接，扩展浅层GCN的跳跃范围，从而捕捉长距离依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够避免下游判别任务中因过平滑导致的性能下降，有助于捕捉长距离依赖关系。&lt;h4&gt;结论&lt;/h4&gt;该方法在异质图上不具有普适性，在节点分类任务中的表现依赖于图结构而不一致。&lt;h4&gt;翻译&lt;/h4&gt;图卷积网络在图表示学习中获得了广泛关注，最近的研究集中在提高异质图在各种实际应用中的性能。典型消息传递机制中的局部特征聚合阻碍了对同类非局部节点之间长距离依赖关系的捕捉。异质图中的固有连接结构常常与同类远距离节点之间的信息共享相冲突。我们提出通过结构学习来重新连接浅层GCN中的边，以避免下游判别任务中因过平滑导致的性能下降。通过参数化邻接矩阵来学习非局部节点之间的连接，并扩展浅层GCN的跳跃范围，从而有助于捕捉长距离依赖关系。然而，我们的方法在异质图上不具有普适性，并且在节点分类任务中的表现依赖于图结构而不一致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Convolutional Networks (GCNs) gained traction for graph representationlearning, with recent attention on improving performance on heterophilic graphsfor various real-world applications. The localized feature aggregation in atypical message-passing paradigm hinders the capturing of long-rangedependencies between non-local nodes of the same class. The inherentconnectivity structure in heterophilic graphs often conflicts with informationsharing between distant nodes of same class. We propose structure learning torewire edges in shallow GCNs itself to avoid performance degradation indownstream discriminative tasks due to oversmoothing. Parameterizing theadjacency matrix to learn connections between non-local nodes and extend thehop span of shallow GCNs facilitates the capturing of long-range dependencies.However, our method is not generalizable across heterophilic graphs andperforms inconsistently on node classification task contingent to the graphstructure.</description>
      <author>example@mail.com (Garv Kaushik)</author>
      <guid isPermaLink="false">2507.21191v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection</title>
      <link>http://arxiv.org/abs/2507.21164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型无监督异常检测方法，通过将表示学习与一类支持向量机(OCSVM)紧密耦合，解决了现有方法中重建过度和特征空间次优的问题。该方法在MNIST-C基准测试和脑部MRI微小病变检测任务上表现出色，能够识别小型非高亮病变，并具有良好的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;无监督异常检测(UAD)在没有标签数据的情况下检测异常，这在许多机器学习应用中是必要的，因为异常样本很少或不可用。&lt;h4&gt;目的&lt;/h4&gt;解决现有无监督异常检测方法的局限性，包括基于重建的方法过度重建异常，以及解耦表示学习方法可能产生次优特征空间的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种通过自定义损失公式将表示学习与可分析求解的一类SVM(OCSVM)紧密耦合的方法，该公式直接将潜在特征与OCSVM决策边界对齐。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在MNIST-C基准测试和脑部MRI微小病变检测任务上表现出色，能够识别小型非高亮病变，同时对域移位具有良好的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在性能和鲁棒性方面表现出色，显示出在通用无监督异常检测和真实世界医学成像应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常检测(UAD)旨在没有标签数据的情况下检测异常，这在许多机器学习应用中是必要的，因为异常样本很少或不可用。大多数最先进的方法分为两类：基于重建的方法，它们通常能很好地重建异常；以及带有密度估计器的解耦表示学习，它们可能遭受次优特征空间的问题。虽然最近的一些方法试图将特征学习和异常检测耦合，但它们通常依赖于代理目标、限制核选择或引入近似，限制了其表达能力和鲁棒性。为了应对这一挑战，我们提出了一种新颖的方法，通过自定义损失公式将表示学习与可分析求解的一类SVM(OCSVM)紧密耦合，该公式直接将潜在特征与OCSVM决策边界对齐。模型在两个任务上进行了评估：基于MNIST-C的新基准测试，以及具有挑战性的脑部MRI微小病变检测任务。与大多数关注图像层面大型高亮病变的方法不同，我们的方法能够针对小型非高亮病变，同时评估体素级指标，解决更相关的临床场景。两个实验都评估了对域移位的某种鲁棒性，包括MNIST-C中的损坏类型和MRI中的扫描仪/年龄变化。结果表明了我们提出模型的性能和鲁棒性，突显了其在通用UAD和真实世界医学成像应用中的潜力。源代码可在https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomaly detection (UAD) aims to detect anomalies without labeleddata, a necessity in many machine learning applications where anomalous samplesare rare or not available. Most state-of-the-art methods fall into twocategories: reconstruction-based approaches, which often reconstruct anomaliestoo well, and decoupled representation learning with density estimators, whichcan suffer from suboptimal feature spaces. While some recent methods attempt tocouple feature learning and anomaly detection, they often rely on surrogateobjectives, restrict kernel choices, or introduce approximations that limittheir expressiveness and robustness. To address this challenge, we propose anovel method that tightly couples representation learning with an analyticallysolvable one-class SVM (OCSVM), through a custom loss formulation that directlyaligns latent features with the OCSVM decision boundary. The model is evaluatedon two tasks: a new benchmark based on MNIST-C, and a challenging brain MRIsubtle lesion detection task. Unlike most methods that focus on large,hyperintense lesions at the image level, our approach succeeds to target small,non-hyperintense lesions, while we evaluate voxel-wise metrics, addressing amore clinically relevant scenario. Both experiments evaluate a form ofrobustness to domain shifts, including corruption types in MNIST-C andscanner/age variations in MRI. Results demonstrate performance and robustnessof our proposed mode,highlighting its potential for general UAD and real-worldmedical imaging applications. The source code is available athttps://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning</description>
      <author>example@mail.com (Nicolas Pinon, Carole Lartizien)</author>
      <guid isPermaLink="false">2507.21164v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation</title>
      <link>http://arxiv.org/abs/2507.22020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于分割的可解释人工智能方法，用于处理点云分类的神经网络。该方法引入了一种新的点平移机制来生成扰动，并通过点云分割模型创建易于人类理解的解释。&lt;h4&gt;背景&lt;/h4&gt;人工智能呈指数级增长，理解AI算法在关键领域的决策过程变得尤为重要。可解释AI方法需要产生易于人类理解的解释，以便人们能够更好地分析AI算法并基于分析做出适当决策。&lt;h4&gt;目的&lt;/h4&gt;生成有意义且易于人类解释的解释，专注于解释点云数据分类的AI算法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的点平移机制来引入点云数据的扰动，利用点云分割模型生成分类模型工作的解释，通过分割来引入输入点云数据的扰动并生成显著性图。点平移机制确保平移的点不再影响分类算法的输出。&lt;h4&gt;主要发现&lt;/h4&gt;与之前的方法相比，该方法使用的分割是有意义的，人类可以轻松解释这些分割的含义，能够产生更有意义的显著性图。与使用经典聚类算法生成解释的方法相比具有优势。&lt;h4&gt;结论&lt;/h4&gt;通过示例输入的显著性图分析，证明了该方法在生成有意义解释方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于分割的可解释人工智能方法，用于处理点云分类的神经网络。作为该方法的一个构建模块，我们提出了一种新的点平移机制来引入点云数据的扰动。最近，人工智能呈指数级增长。因此，当AI算法应用于关键领域时，理解其决策过程很重要。我们的工作专注于解释点云数据分类的AI算法。用于解释AI算法的方法的一个重要方面是它们能够产生易于人类理解的解释。这使得人们能够更好地分析AI算法并基于该分析做出适当决策。因此，在这项工作中，我们打算生成易于人类解释的有意义解释。我们考虑的点云数据代表汽车、吉他、笔记本电脑等三维物体。我们利用点云分割模型来生成分类模型工作的解释。这些分割用于向输入点云数据引入扰动并生成显著性图。扰动使用本文提出的新型点平移机制引入，该机制确保平移的点不再影响分类算法的输出。与之前的方法相比，我们方法使用的分割是有意义的，即人类可以轻松解释分割的含义。因此，我们的方法相对于其他方法的优点是它能够产生更有意义的显著性图。我们将我们的方法与使用经典聚类算法生成解释的方法进行了比较。我们还分析了使用我们的方法为示例输入生成的显著性图，以证明该方法在生成有意义解释方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何解释处理点云数据的神经网络模型的决策过程问题。这个问题在现实中很重要，因为AI算法（尤其是深度学习模型）通常被视为'黑盒'，难以理解其决策过程。在关键领域如自动驾驶、医疗诊断等应用AI时，理解其决策过程对于建立信任、确保安全至关重要。此外，点云数据在3D对象识别等领域应用广泛，但针对点云数据的可解释AI方法研究相对较少，存在明显的研究空白。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云XAI方法的局限性，包括使用无意义分割导致解释难以理解、计算成本高、未考虑点云结构信息等问题。在此基础上，作者设计了基于有意义分割的方法，借鉴了现有扰动式XAI方法（如SHAP、LIME）的基本思想，但改进了扰动引入方式。作者特别改进了Zheng等人提出的点位移机制，解决了原方法中点位移到中心可能引入额外特征的问题，并结合点云分割技术和聚类技术以生成更精细的分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用有意义的分割来引入扰动，生成人类容易理解的显著性图，并通过两种扰动机制分析模型决策过程。整体流程包括四个阶段：1）分类阶段：点云数据通过分类模型预测输出类别；2）分割阶段：根据预测类别选择相应分割模型，对输入进行有意义分割；3）扰动阶段：使用分割结果引入扰动，分为'特征缺失'（将特定分割点移到随机点）和'特征存在'（保留特定分割，移除其他点）；4）显著性映射阶段：比较原始输出和扰动后输出，计算显著性图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）基于有意义分割的XAI方法，使用语义上有意义的分割而非无意义聚类；2）新的点位移机制，将点位移到保留结构中的随机点而非中心点；3）两种扰动机制，从不同角度分析模型决策；4）分割+聚类机制，对相似特征进行进一步细分。相比之前的工作，本文方法生成的显著性图更易理解，计算效率更高，能分析单个特征的影响，且考虑了点云中的结构信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于有意义分割的扰动式可解释人工智能方法，通过新的点位移机制和两种扰动策略，为点云分类模型生成人类易于理解的可解释性结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel segmentation-based explainable artificial intelligence(XAI) method for neural networks working on point cloud classification. As onebuilding block of this method, we propose a novel point-shifting mechanism tointroduce perturbations in point cloud data. Recently, AI has seen anexponential growth. Hence, it is important to understand the decision-makingprocess of AI algorithms when they are applied in critical areas. Our workfocuses on explaining AI algorithms that classify point cloud data. Animportant aspect of the methods used for explaining AI algorithms is theirability to produce explanations that are easy for humans to understand. Thisallows them to analyze the AI algorithms better and make appropriate decisionsbased on that analysis. Therefore, in this work, we intend to generatemeaningful explanations that can be easily interpreted by humans. The pointcloud data we consider represents 3D objects such as cars, guitars, andlaptops. We make use of point cloud segmentation models to generateexplanations for the working of classification models. The segments are used tointroduce perturbations into the input point cloud data and generate saliencymaps. The perturbations are introduced using the novel point-shifting mechanismproposed in this work which ensures that the shifted points no longer influencethe output of the classification algorithm. In contrast to previous methods,the segments used by our method are meaningful, i.e. humans can easilyinterpret the meaning of the segments. Thus, the benefit of our method overother methods is its ability to produce more meaningful saliency maps. Wecompare our method with the use of classical clustering algorithms to generateexplanations. We also analyze the saliency maps generated for example inputsusing our method to demonstrate the usefulness of the method in generatingmeaningful explanations.</description>
      <author>example@mail.com (Raju Ningappa Mulawade, Christoph Garth, Alexander Wiebel)</author>
      <guid isPermaLink="false">2507.22020v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors</title>
      <link>http://arxiv.org/abs/2507.21872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MultiEditor是一种双分支潜在扩散框架，能够联合编辑驾驶场景中的图像和LiDAR点云，通过3D高斯溅射作为先验，实现跨模态的高保真重建，显著提升了自动驾驶系统对稀有车辆类别的感知能力。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统依赖多模态感知数据理解复杂环境，但现实世界数据的长尾分布阻碍了泛化能力，特别是对稀有但安全关键的车辆类别。&lt;h4&gt;目的&lt;/h4&gt;解决自动驾驶系统中多模态感知数据的长尾分布问题，特别是提高对稀有但安全关键的车辆类别的识别能力。&lt;h4&gt;方法&lt;/h4&gt;提出MultiEditor双分支潜在扩散框架，引入3D高斯溅射作为目标对象的结构和外观先验，设计多级外观控制机制（像素级粘贴、语义级引导和多分支细化），并提出深度引导的可变形跨模态条件模块，使用3DGS渲染的深度实现模态间的自适应相互引导。&lt;h4&gt;主要发现&lt;/h4&gt;MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现优越；使用MultiEditor生成稀有类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;结论&lt;/h4&gt;MultiEditor能有效解决自动驾驶系统中多模态感知数据的挑战，特别是在处理稀有但安全关键的车辆类别方面，提升了系统的泛化能力和安全性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶系统严重依赖多模态感知数据来理解复杂环境。然而，现实世界数据的长尾分布阻碍了泛化能力，特别是对稀有但安全关键的车辆类别。为应对这一挑战，我们提出了MultiEditor，一种专为驾驶场景设计的双分支潜在扩散框架，能够联合编辑图像和LiDAR点云。我们方法的核心是引入3D高斯溅射作为目标对象的结构和外观先验。利用这一先验，我们设计了多级外观控制机制，包括像素级粘贴、语义级引导和多分支细化，以实现跨模态的高保真重建。我们还提出了深度引导的可变形跨模态条件模块，使用3DGS渲染的深度自适应地实现模态间的相互引导，显著增强了跨模态一致性。大量实验表明，MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面具有优越性能。此外，使用MultiEditor生成稀有类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中多模态感知数据的生成和编辑问题，特别是真实世界数据中的长尾分布问题。自动驾驶系统依赖图像和LiDAR点云等多模态数据理解环境，但常见车辆类别数据过多，而稀有但安全关键的车辆类别（如压路机、挖掘机）数据严重不足，导致模型对罕见场景识别能力差，影响系统安全性和可靠性。这个问题在现实中非常重要，因为稀有车辆识别对避免交通事故至关重要，且收集各种罕见车辆的真实数据成本高昂且困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有单模态编辑方法的局限性，认识到缺乏统一的框架进行多模态联合编辑。他们选择潜在扩散模型作为基础，创新性地引入3D高斯溅射作为物体结构和外观的统一先验，设计了双分支架构（图像分支和点云分支），并通过3DGS连接两个分支。作者借鉴了多项现有工作：使用CLIP提取语义特征、参考DCI-VTON的双分支优化策略、利用3DGS作为3D表示方法，以及采用可变形注意力机制进行跨模态交互。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D高斯溅射作为统一先验，同时控制图像和LiDAR点云的编辑，确保跨模态一致性。实现流程包括：1)数据准备阶段，从KITTI数据集构建训练集并进行目标检测和分割；2)双分支架构设计，图像分支通过像素级保留、语义级指导和多分支优化处理RGB图像，点云分支处理距离图像表示；3)跨模态条件模块，通过深度引导的可变形注意力实现跨模态相互指导；4)五阶段训练流程，从VAE预训练到端到端联合优化；5)推理与应用，生成稀有类别数据增强训练集，提升下游任务性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个双分支多模态潜在扩散框架，将3DGS作为统一先验；2)深度引导的可变形跨模态条件模块，确保跨模态一致性；3)多级外观控制机制，包含像素级保留、语义级指导和多分支优化；4)阴影处理与无约束编辑能力。相比之前工作，MultiEditor首次实现了图像和点云的联合编辑框架，突破了传统方法依赖数据集先验的限制，解决了跨模态不一致问题，支持任意视角的物体操作，并能处理稀有车辆类别，提供更精确的物体级编辑控制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MultiEditor通过引入3D高斯溅射作为统一先验，实现了首个能够精确、一致且可控地联合编辑自动驾驶场景中图像和LiDAR点云的多模态框架，有效解决了自动驾驶感知系统中的长尾数据不平衡问题，显著提升了模型对罕见类别的识别能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving systems rely heavily on multimodal perception data tounderstand complex environments. However, the long-tailed distribution ofreal-world data hinders generalization, especially for rare but safety-criticalvehicle categories. To address this challenge, we propose MultiEditor, adual-branch latent diffusion framework designed to edit images and LiDAR pointclouds in driving scenarios jointly. At the core of our approach is introducing3D Gaussian Splatting (3DGS) as a structural and appearance prior for targetobjects. Leveraging this prior, we design a multi-level appearance controlmechanism--comprising pixel-level pasting, semantic-level guidance, andmulti-branch refinement--to achieve high-fidelity reconstruction acrossmodalities. We further propose a depth-guided deformable cross-modalitycondition module that adaptively enables mutual guidance between modalitiesusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.Extensive experiments demonstrate that MultiEditor achieves superiorperformance in visual and geometric fidelity, editing controllability, andcross-modality consistency. Furthermore, generating rare-category vehicle datawith MultiEditor substantially enhances the detection accuracy of perceptionmodels on underrepresented classes.</description>
      <author>example@mail.com (Shouyi Lu, Zihan Lin, Chao Lu, Huanran Wang, Guirong Zhuo, Lianqing Zheng)</author>
      <guid isPermaLink="false">2507.21872v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>PC-JND: Subjective Study and Dataset on Just Noticeable Difference for Point Clouds in 6DoF Virtual Reality</title>
      <link>http://arxiv.org/abs/2507.21557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 figures, Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在虚拟现实环境中观看点云时的最小可觉差特性，并创建了一个新的点云最小可觉差数据集，为未来研究提供支持。&lt;h4&gt;背景&lt;/h4&gt;最小可觉差(JND)指人类能感知原始刺激与失真版本差异所需的最小失真程度，已广泛应用于视觉信号处理领域优化媒体体验。点云是包含几何信息和属性的主流体积数据表示，用于虚拟现实等沉浸式3D媒体，但其在VR环境中的JND特性尚未被研究。&lt;h4&gt;目的&lt;/h4&gt;研究在六自由度VR环境中使用头戴式显示器观看点云时的点云最小可觉差(PCJND)特性。&lt;h4&gt;方法&lt;/h4&gt;在六自由度VR环境中使用头戴式显示器进行实验，测量和分析点云的最小可觉差特性。&lt;h4&gt;主要发现&lt;/h4&gt;对于大多数点云，人眼纹理PCJND小于几何PCJND；颜色丰富度与纹理PCJND存在相关性，但与几何PCJND无显著相关性；点的数量与纹理或几何PCJND均无显著相关性。&lt;h4&gt;结论&lt;/h4&gt;为支持未来JND预测和感知驱动的信号处理研究，引入了PC-JND新型点云JND数据集，该数据集将公开提供以促进沉浸式媒体的感知优化发展。&lt;h4&gt;翻译&lt;/h4&gt;最小可觉差(JND)指的是人类能够感知原始刺激与其失真版本之间差异所需的最小失真程度。JND概念已被广泛应用于视觉信号处理任务，包括编码、传输、渲染和质量评估，以优化以人为中心的媒体体验。点云是包含几何信息和属性(如颜色)的主流体积数据表示。点云用于先进的沉浸式3D媒体，如虚拟现实(VR)。然而，在VR中观看点云的JND特性之前尚未被探索。在本文中，我们使用头戴式显示器在六自由度(6DoF)VR环境中研究了点云级的JND(PCJND)特性。我们的研究结果表明，对于大多数点云，人眼的纹理PCJND小于几何PCJND。此外，我们确定了颜色丰富度与纹理PCJND之间的相关性。然而，颜色丰富度与几何PCJND之间没有显著相关性，点的数量与纹理或几何PCJND之间也没有显著相关性。为了支持未来JND预测和感知驱动的信号处理研究，我们引入了PC-JND，这是一种新型的基于点云的JND数据集。该数据集将公开提供，以促进沉浸式媒体的感知优化发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云在六自由度虚拟现实环境中的最小可觉差(JND)特性问题。这个问题很重要，因为随着VR技术的发展，点云作为主流的3D数据表示被广泛应用于训练、工业制造、教育、医疗等领域。然而，点云在数据结构(无序、不规则)、显示设备(HMD)和交互方式(6DoF)上与传统2D媒体有很大不同，现有JND模型不适用于点云。研究点云在VR中的JND特性对于优化VR体验、提高压缩效率、减少带宽需求具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有JND研究的局限性，发现它们主要针对2D图像、视频等传统媒体，不适用于点云。然后借鉴了现有的主观测试方法，特别是PWJND和VWJND研究中的DSIS方法，采用并排显示原始和失真点云对，让受试者判断差异。作者还借鉴了异常值检测方法，包括基于范围和标准差的检测以及Grubbs' test。实验设计上，作者创建了34个高质量参考点云，使用V-PCC生成失真版本，在6DoF VR环境中进行测试，并采用放松的二进制搜索方法确定JND阈值。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是研究点云在六自由度虚拟现实环境中的最小可觉差特性，分别研究纹理JND和几何JND，并通过主观实验收集数据构建点云JND数据集。整体流程包括：1)选择并预处理34个高质量参考点云；2)使用V-PCC生成纹理和几何失真版本；3)在VR环境中设计主观实验，使用DSIS方法和放松的二进制搜索确定JND；4)收集并处理数据，检测异常值；5)分析纹理和几何PCJND的差异，以及它们与色彩丰富度、点数的相关性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次研究点云在6DoF VR环境中的JND特性；2)分别研究纹理PCJND和几何PCJND；3)创建PC-JND数据集，包含34个参考点云及其失真版本；4)在6DoF VR环境中进行主观实验，允许受试者自由移动观察；5)发现纹理PCJND通常小于几何PCJND，且色彩丰富度与纹理PCJND相关但与几何PCJND无关。相比之前工作，这篇论文的研究对象从传统2D媒体转变为3D点云，研究环境从传统显示器转变为6DoF VR环境，并创建了专门的点云JND数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次研究了点云在六自由度虚拟现实环境中的最小可觉差特性，创建了PC-JND数据集，并发现纹理JND通常小于几何JND，为点云压缩和VR体验优化提供了重要的感知基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Just Noticeable Difference (JND) accounts for the minimum distortion atwhich humans can perceive a difference between a pristine stimulus and itsdistorted version. The JND concept has been widely applied in visual signalprocessing tasks, including coding, transmission, rendering, and qualityassessment, to optimize human-centric media experiences. A point cloud is amainstream volumetric data representation consisting of both geometryinformation and attributes (e.g. color). Point clouds are used for advancedimmersive 3D media such as Virtual Reality (VR). However, the JNDcharacteristics of viewing point clouds in VR have not been explored before. Inthis paper, we study the point cloud-wise JND (PCJND) characteristics in a SixDegrees of Freedom (6DoF) VR environment using a head-mounted display. Ourfindings reveal that the texture PCJND of human eyes is smaller than thegeometry PCJND for most point clouds. Furthermore, we identify a correlationbetween colorfulness and texture PCJND. However, there is no significantcorrelation between colorfulness and the geometry PCJND, nor between the numberof points and neither the texture or geometry PCJND. To support future researchin JND prediction and perception-driven signal processing, we introduce PC-JND,a novel point cloud-based JND dataset. This dataset will be made publiclyavailable to facilitate advancements in perceptual optimization for immersivemedia.</description>
      <author>example@mail.com (Chunling Fan, Yun Zhang, Dietmar Saupe, Raouf Hamzaoui, Weisi Lin)</author>
      <guid isPermaLink="false">2507.21557v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Reconstruction with Global Context for 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2507.21555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures, IEEE International Conference on Systems, Man,  and Cybernetics (IEEE SMC), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为多视图重建(MVR)的方法，用于解决高精度3D异常检测中全局信息不足的问题。该方法将高分辨率点云无损转换为多视图图像，并采用基于重建的异常检测框架来增强全局信息学习。&lt;h4&gt;背景&lt;/h4&gt;3D异常检测在工业质量检查中至关重要。然而，现有方法在高精度3D异常检测中的表现不佳，主要原因是它们缺乏足够的全局信息。&lt;h4&gt;目的&lt;/h4&gt;解决高精度3D异常检测中全局信息不足的问题，提高检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出多视图重建(MVR)方法，将高分辨率点云无损转换为多视图图像，并采用基于重建的异常检测框架来增强全局信息学习。&lt;h4&gt;主要发现&lt;/h4&gt;在Real3D-AD基准测试上，MVR方法达到了89.6%的对象级AU-ROC和95.7%的点级AU-ROC。&lt;h4&gt;结论&lt;/h4&gt;MVR方法通过转换点云为多视图图像并使用重建框架，有效解决了高精度3D异常检测中全局信息不足的问题，显著提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;三维异常检测在工业质量检查中至关重要。尽管现有方法取得了显著进展，但由于全局信息不足，它们在高精度三维异常检测中的性能有所下降。为此，我们提出了多视图重建(MVR)方法，该方法将高分辨率点云无损转换为多视图图像，并采用基于重建的异常检测框架来增强全局信息学习。大量实验证明了MVR的有效性，在Real3D-AD基准测试上达到了89.6%的对象级AU-ROC和95.7%的点级AU-ROC。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高精度3D异常检测中全局信息不足的问题。在工业质量检测中，3D异常检测通过高精度扫描识别缺陷至关重要，但现有方法因过度依赖局部特征而无法充分利用全局信息，导致在高精度场景下性能下降，无法满足工业部署的严格要求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，如Reg3D-AD忽略全局特征，Group3AD的特征表示存在负面影响，GLFM未能充分利用全局特征。受此启发，作者设计将无结构点云转换为结构化图像表示，利用预训练视觉变换器的自注意力机制捕获全局特征。借鉴了CPMF的多视图渲染方法，但通过高分辨率投影策略改进了质量问题；借鉴了Dinomaly的重建方法，但采用重建驱动框架而非基于记忆库的机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云转换为多视图2D图像，利用2D视觉变换器的全局特征提取能力，通过重建框架学习点云的正常表示，最后通过重建误差检测异常。整体流程包括：1)多视图投影将点云转换为多个深度图像；2)使用预训练ViT编码器提取特征；3)通过瓶颈和重建网络重建特征；4)融合多视图特征并转换回点云空间；5)使用重建损失训练模型；6)通过余弦相似度计算异常分数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多视图渲染策略，采用高分辨率投影后下采样，减少噪声和伪影；2)重建驱动范式，通过空间结构恢复强制学习全局上下文；3)全局特征提取，利用ViT的全局感受野捕获语义相关性。相比之前工作，MVR不依赖基于记忆库的机制，不同于CPMF的低分辨率渲染，也不同于传统3D方法的局部特征提取，而是通过整体捕获全局语义相关性实现高性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的MVR方法通过将高分辨率点云无损转换为多视图图像并利用基于重建的框架捕获全局上下文，显著提高了3D异常检测的性能，在Real3D-AD基准测试上达到了89.6%的对象级AU-ROC和95.7%的点级AU-ROC。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D anomaly detection is critical in industrial quality inspection. Whileexisting methods achieve notable progress, their performance degrades inhigh-precision 3D anomaly detection due to insufficient global information. Toaddress this, we propose Multi-View Reconstruction (MVR), a method thatlosslessly converts high-resolution point clouds into multi-view images andemploys a reconstruction-based anomaly detection framework to enhance globalinformation learning. Extensive experiments demonstrate the effectiveness ofMVR, achieving 89.6\% object-wise AU-ROC and 95.7\% point-wise AU-ROC on theReal3D-AD benchmark.</description>
      <author>example@mail.com (Yihan Sun, Yuqi Cheng, Yunkang Cao, Yuxin Zhang, Weiming Shen)</author>
      <guid isPermaLink="false">2507.21555v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval</title>
      <link>http://arxiv.org/abs/2507.21489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为Describe, Adapt and Combine (DAC)的框架，用于开放集3D对象检索，仅使用多视图图像结合CLIP模型和多模态大语言模型学习通用3D表示，并引入AB-LoRA增强泛化能力，在四个数据集上平均提升10.01%的mAP。&lt;h4&gt;背景&lt;/h4&gt;开放集3D对象检索是一个新兴任务，旨在检索训练集之外的未见类别3D对象。现有方法使用所有模态（体素、点云、多视图图像）但在3D训练数据不足的情况下难以产生通用表示。&lt;h4&gt;目的&lt;/h4&gt;开发一个简单有效的框架，仅使用多视图图像实现开放集3D对象检索，克服现有方法因3D训练数据不足导致的表示泛化能力有限问题。&lt;h4&gt;方法&lt;/h4&gt;提出DAC框架，结合CLIP模型与多模态大语言模型(MLLM)学习通用3D表示。MLLM用于训练时描述已知类别信息以适应CLIP目标，以及推理时提供未知对象的外部提示。同时引入Additive-Bias Low-Rank adaptation (AB-LoRA)减轻过拟合并增强泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;DAC仅使用多视图图像，在四个开放集3DOR数据集上平均比之前的方法提高了10.01%的mAP。其泛化能力在基于图像和跨数据集设置上得到验证。&lt;h4&gt;结论&lt;/h4&gt;DAC框架通过结合CLIP的通用表示能力和MLLM的描述能力，有效解决了开放集3D对象检索中的泛化问题。AB-LoRA的引入进一步增强了模型泛化能力，且只需多视图图像作为输入，简化了流程。&lt;h4&gt;翻译&lt;/h4&gt;开放集3D对象检索(3DOR)是一个新兴任务，旨在检索训练集之外的未见类别的3D对象。现有方法通常使用所有模态（即体素、点云、多视图图像）并在融合前训练特定的主干网络。然而，由于3D训练数据不足，它们仍然难以产生通用表示。CLIP通过在web规模的图像-文本对上进行对比预训练， inherently 为广泛的下游任务产生通用表示。基于此，我们提出了一个简单而有效的框架，名为Describe, Adapt and Combine (DAC)，仅使用多视图图像进行开放集3DOR。DAC创新性地将CLIP模型与多模态大语言模型(MLLM)相结合来学习通用的3D表示，其中MLLM有两个用途。首先，它在训练过程中描述已知类别信息，以适应CLIP的训练目标。其次，它在推理时提供关于未知对象的外部提示，补充视觉线索。为了增强这种协同效应，我们引入了Additive-Bias Low-Rank adaptation (AB-LoRA)，它减轻了过拟合并进一步增强了对未见类别的泛化能力。仅使用多视图图像，DAC在四个开放集3DOR数据集上平均比之前的方法提高了10.01%的mAP。此外，其在基于图像和跨数据集设置上的泛化能力也得到了验证。代码可在https://github.com/wangzhichuan123/DAC获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放集3D对象检索问题，即如何从大型3D库中检索训练集中未见过的3D对象类别。这个问题在现实中非常重要，因为虚拟现实、计算机辅助设计和3D文化遗产保护等应用经常需要处理未知类别的3D对象。现有方法由于3D训练数据有限，容易对已知类别过拟合，导致对未见类别的泛化能力差，无法满足实际应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：它们使用所有模态（体素、点云、多视图图像）并在融合前训练特定主干网络，但受限于3D训练数据不足，难以产生泛化表示。作者受人类认知过程的启发——面对未知3D对象时，我们会从多角度观察并用语言描述特征。因此，作者借鉴了CLIP的对比预训练能力和MLLM（如InternVL）的描述能力，设计了仅使用多视图图像的三步流程（描述、适应、组合）。方法中借鉴了LoRA进行高效微调，并创新性地添加了加性偏置以防止过拟合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用CLIP的强大泛化能力和MLLM的描述能力，通过AB-LoRA高效适应CLIP到多视图投影，结合视觉和文本特征以提高对未知类别的泛化能力。整体流程分为三步：1）描述阶段：训练时用MLLM生成已知类别的详细描述与CLIP对齐，推理时用MLLM描述多视图图像获取外部知识；2）适应阶段：使用AB-LoRA微调CLIP的文本和视觉编码器，通过对比学习增强多视图图像与文本描述的对齐；3）组合阶段：使用加权融合方案（加权和tanh激活）结合视觉和文本特征，生成最终的3D描述符用于检索。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）DAC框架：首次协同生成式和判别式大VLLMs，仅使用多视图图像且无需测试数据训练；2）AB-LoRA：在LoRA基础上添加加性偏置，有效减轻过拟合并增强泛化能力；3）MLLM的双重用途：训练阶段描述已知类别，推理阶段提供未知对象的外部提示；4）简单的特征融合方法：使用加权和tanh激活函数实现高效特征融合。相比之前的工作，DAC不使用复杂的3D数据格式和所有模态，避免了HGM2R等方法使用测试数据训练的不实用性，性能显著提升（平均+10.01% mAP）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了DAC框架，通过协同CLIP编码器和多模态大语言模型，结合创新的加性偏置低秩适应技术，仅利用多视图图像实现了开放集3D对象检索的显著性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3Dobjects of unseen categories beyond the training set. Existing methodstypically utilize all modalities (i.e., voxels, point clouds, multi-viewimages) and train specific backbones before fusion. However, they stillstruggle to produce generalized representations due to insufficient 3D trainingdata. Being contrastively pre-trained on web-scale image-text pairs, CLIPinherently produces generalized representations for a wide range of downstreamtasks. Building upon it, we present a simple yet effective framework namedDescribe, Adapt and Combine (DAC) by taking only multi-view images for open-set3DOR. DAC innovatively synergizes a CLIP model with a multi-modal largelanguage model (MLLM) to learn generalized 3D representations, where the MLLMis used for dual purposes. First, it describes the seen category information toalign with CLIP's training objective for adaptation during training. Second, itprovides external hints about unknown objects complementary to visual cuesduring inference. To improve the synergy, we introduce an Additive-BiasLow-Rank adaptation (AB-LoRA), which alleviates overfitting and furtherenhances the generalization to unseen categories. With only multi-view images,DAC significantly surpasses prior arts by an average of +10.01\% mAP on fouropen-set 3DOR datasets. Moreover, its generalization is also validated onimage-based and cross-dataset setups. Code is available athttps://github.com/wangzhichuan123/DAC.</description>
      <author>example@mail.com (Zhichuan Wang, Yang Zhou, Zhe Liu, Rui Yu, Song Bai, Yulong Wang, Xinwei He, Xiang Bai)</author>
      <guid isPermaLink="false">2507.21489v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Generating Highly Structured Test Inputs Leveraging Constraint-Guided Graph Refinement</title>
      <link>http://arxiv.org/abs/2507.21271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICSME 2025 Registered Reports&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GRAphRef的基于图的测试输入生成框架，旨在解决现代AI应用中处理高度结构化数据时测试输入生成的效率问题。该框架通过将结构化输入映射到图，应用邻域相似性引导的突变，并使用约束细化阶段来修复无效输入，从而提高输入有效性和语义保留。&lt;h4&gt;背景&lt;/h4&gt;现代AI应用越来越多地处理高度结构化的数据，如3D网格和点云，测试输入生成必须同时保持结构和语义的有效性。然而，现有的模糊测试工具和输入生成器通常是针对特定输入类型手工制作的，经常生成无效输入，导致效率低下和泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;研究是否可以通过基于图的表示方法来统一结构化域的测试输入，实现通用、可重用的突变策略，同时强制执行结构约束，并评估这种方法在八个AI系统中提高输入有效性和语义保留方面的效果。&lt;h4&gt;方法&lt;/h4&gt;开发并评估GRAphRef，一个基于图的测试输入生成框架，支持基于约束的突变和细化。该框架将结构化输入映射到图，应用邻域相似性引导的突变，并使用约束细化阶段来修复无效输入。在八个真实的网格处理AI系统上进行确认性研究，将GRAphRef与AFL、MeshAttack、Saffron和两个变体进行比较。评估指标包括结构有效性、语义保留（通过预测一致性）和性能开销。&lt;h4&gt;主要发现&lt;/h4&gt;研究计划中未明确提及主要发现，但预期GRAphRef将比现有方法生成更有效的测试输入，提高结构有效性并更好地保留语义意义，同时保持合理的性能开销。&lt;h4&gt;结论&lt;/h4&gt;研究计划中未明确提及结论，但预期基于图的测试输入生成框架将为结构化数据提供更通用、更高效的测试方法，提高AI系统的可靠性验证。&lt;h4&gt;翻译&lt;/h4&gt;【背景】现代AI应用越来越多地处理高度结构化的数据，如3D网格和点云，测试输入生成必须同时保持结构和语义的有效性。然而，现有的模糊测试工具和输入生成器通常是针对特定输入类型手工制作的，经常生成无效输入，导致效率低下和泛化能力差。【目标】本研究调查是否可以通过基于图的表示方法来统一结构化域的测试输入，实现通用、可重用的突变策略，同时强制执行结构约束。我们将在八个AI系统中评估这种方法在提高输入有效性和语义保留方面的效果。【方法】我们开发并评估GRAphRef，一个基于图的测试输入生成框架，支持基于约束的突变和细化。GRAphRef将结构化输入映射到图，应用邻域相似性引导的突变，并使用约束细化阶段来修复无效输入。我们将在八个真实的网格处理AI系统上进行确认性研究，将GRAphRef与AFL、MeshAttack、Saffron和两个变体进行比较。评估指标包括结构有效性、语义保留（通过预测一致性）和性能开销。实验数据来自ShapeNetCore网格种子和MeshCNN和HodgeNet等系统的模型输出。将使用统计分析和组件延迟分解来评估每个假设。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现代AI应用处理高度结构化数据（如3D网格和点云）时测试输入生成效率低下的问题。现有模糊测试工具常生成无效输入并被丢弃，导致65%的时间被浪费。这个问题很重要，因为AI系统（如自动驾驶、医疗诊断）的可靠性至关重要，其故障可能导致严重后果，而这些系统又依赖于丰富、高度结构化的测试输入。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于三个核心假设设计方法：1）复杂输入可共享通用图结构；2）通过邻居相似性突变可保留语义；3）约束可在突变期间编码执行。作者借鉴了AFL的覆盖率引导模糊测试、Saffron的语法基础模糊测试、ISLa的约束处理以及图表示方法，但将其创新性地结合，解决了现有方法在处理结构化数据时的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将各种结构化输入统一表示为图，应用邻居感知突变保留语义，并使用约束引导的修复确保输入有效。整体流程包括：1）约束强制执行（分析器和验证器检测违规并纠正）；2）图转换器（将结构化输入转为标准图表示）；3）图突变器（应用20个由邻居相似性引导的突变算子）；4）约束引导的修复（自动修复突变导致的结构违规）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）统一的图表示使单一引擎跨输入类型重用；2）邻居感知突变保留语义结构；3）约束引导的修复自动修复结构违规；4）可扩展框架轻松支持新输入类型。相比不同：与AFL相比处理结构化而非原始数据；与Saffron相比不依赖特定语法；与MeshAttack相比支持多种输入类型；与GraphFuzz相比专为结构化输入设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了GRAPHREF框架，通过统一的图表示、邻居感知突变和约束引导修复，高效生成高度结构化的测试输入，解决了现有模糊测试工具在处理结构化数据时的效率和泛化性问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; [Context] Modern AI applications increasingly process highly structured data,such as 3D meshes and point clouds, where test input generation must preserveboth structural and semantic validity. However, existing fuzzing tools andinput generators are typically handcrafted for specific input types and oftengenerate invalid inputs that are subsequently discarded, leading toinefficiency and poor generalizability. [Objective] This study investigateswhether test inputs for structured domains can be unified through a graph-basedrepresentation, enabling general, reusable mutation strategies while enforcingstructural constraints. We will evaluate the effectiveness of this approach inenhancing input validity and semantic preservation across eight AI systems.[Method] We develop and evaluate GRAphRef, a graph-based test input generationframework that supports constraint-based mutation and refinement. GRAphRef mapsstructured inputs to graphs, applies neighbor-similarity-guided mutations, anduses a constraint-refinement phase to repair invalid inputs. We will conduct aconfirmatory study across eight real-world mesh-processing AI systems,comparing GRAphRef with AFL, MeshAttack, Saffron, and two ablated variants.Evaluation metrics include structural validity, semantic preservation (viaprediction consistency), and performance overhead. Experimental data is derivedfrom ShapeNetCore mesh seeds and model outputs from systems like MeshCNN andHodgeNet. Statistical analysis and component latency breakdowns will be used toassess each hypothesis.</description>
      <author>example@mail.com (Zhaorui Yang, Yuxin Qiu, Haichao Zhu, Qian Zhang)</author>
      <guid isPermaLink="false">2507.21271v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Generating Adversarial Point Clouds Using Diffusion Model</title>
      <link>http://arxiv.org/abs/2507.21163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的黑盒对抗样本生成方法，利用扩散模型提高黑盒设置下的攻击成功率和不可感知性，无需依赖点云分类模型的内部信息来生成对抗样本。&lt;h4&gt;背景&lt;/h4&gt;3D点云分类的对抗攻击方法揭示了点云识别模型的脆弱性，这可能在使用深度学习模型的关键应用（如自动驾驶汽车）中导致安全风险。然而，大多数现有的对抗攻击方法基于白盒攻击，虽然这些方法实现了高攻击成功率和不可感知性，但在实际场景中的适用性有限。更有实际意义的黑盒攻击通常效果较差。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的黑盒对抗样本生成方法，提高黑盒设置下的攻击成功率和不可感知性，同时不需要依赖点云分类模型的内部信息。&lt;h4&gt;方法&lt;/h4&gt;使用3D扩散模型，将点云的压缩特征作为先验知识来指导反向扩散过程，向干净样本添加对抗点。随后，利用其反向过程将其他类别的分布转换为对抗点，然后将这些对抗点添加到点云中。&lt;h4&gt;主要发现&lt;/h4&gt;利用扩散模型可以有效地在黑盒设置下生成对抗样本，提高攻击成功率和不可感知性，而不需要模型的内部信息。&lt;h4&gt;结论&lt;/h4&gt;所提出的黑盒对抗攻击方法为评估点云分类模型的安全性提供了新的途径，有助于发现这些模型的缺陷，从而提高在实际应用中的安全性。&lt;h4&gt;翻译&lt;/h4&gt;3D点云分类的对抗攻击方法揭示了点云识别模型的脆弱性。这种脆弱性可能导致使用深度学习模型的关键应用（如自动驾驶汽车）中的安全风险。为了揭示这些模型的缺陷，研究人员可以通过对抗攻击来评估它们的安全性。然而，大多数现有的对抗攻击方法基于白盒攻击。虽然这些方法实现了高攻击成功率和不可感知性，但它们在实际场景中的适用性有限。在实际场景中更有意义的黑盒攻击通常效果较差。本文提出了一种新型的黑盒对抗样本生成方法，利用扩散模型提高黑盒设置下的攻击成功率和不可感知性，无需依赖点云分类模型的内部信息来生成对抗样本。我们使用3D扩散模型将点云的压缩特征作为先验知识来指导反向扩散过程，向干净样本添加对抗点。随后，利用其反向过程将其他类别的分布转换为对抗点，然后将这些对抗点添加到点云中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效生成对抗性点云样本的问题，特别是在黑盒攻击场景下。这个问题重要是因为点云识别模型在自动驾驶等关键应用中存在安全风险，而现有的白盒攻击方法虽然效果好但在现实场景中难以实施（因为攻击者通常无法获取模型内部信息），黑盒攻击更符合实际应用场景但效果较差，因此需要改进黑盒攻击方法以提高其有效性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从生成模型角度重新思考黑盒对抗样本生成，认识到扩散模型在图像生成领域的成功潜力，将其扩展到3D点云。他们借鉴了扩散模型的前向和反向过程、点云的压缩特征（形状潜势）作为先验知识、以及密度感知的Chamfer距离等现有工作。设计思路是将对抗样本生成视为反向扩散过程，利用其他类别的点云分布指导生成对抗点，并通过特定的损失函数约束噪声添加，提高对抗样本的不可察觉性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将对抗点云的生成过程建模为反向扩散过程，利用点云的压缩特征作为先验知识指导反向扩散，通过将其他类别的分布转化为对抗点并添加到点云中来实现攻击，同时使用密度感知的Chamfer距离约束噪声添加以提高不可察觉性。整体流程包括：1)计算引导点云的潜在表示；2)通过反向扩散生成对抗点云；3)使用密度感知的Chamfer距离和均方误差损失函数抑制扩散模型的多样性，确保生成的对抗样本既有效攻击目标模型又保持与原始点云的高度相似性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的引导黑盒对抗方法，利用其他类别的潜在表示指导反向扩散过程；2)新的损失函数结合密度感知的Chamfer距离和均方误差，提高隐蔽性；3)高效的黑盒攻击性能，面对不同模型攻击成功率可达90%以上。相比之前工作，本文方法无需依赖目标模型内部信息（白盒攻击需要），采用基于生成模型的技术路线而非传统的梯度或优化方法，更接近实际应用场景，同时兼顾了攻击成功率、隐蔽性和跨模型泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于扩散模型的新型黑盒对抗点云生成方法，通过反向扩散过程和密度感知的距离约束，实现了高攻击成功率且隐蔽性强的对抗样本，无需依赖目标模型内部信息，显著提升了黑盒攻击在3D点云识别场景中的实用性和有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adversarial attack methods for 3D point cloud classification reveal thevulnerabilities of point cloud recognition models. This vulnerability couldlead to safety risks in critical applications that use deep learning models,such as autonomous vehicles. To uncover the deficiencies of these models,researchers can evaluate their security through adversarial attacks. However,most existing adversarial attack methods are based on white-box attacks. Whilethese methods achieve high attack success rates and imperceptibility, theirapplicability in real-world scenarios is limited. Black-box attacks, which aremore meaningful in real-world scenarios, often yield poor results. This paperproposes a novel black-box adversarial example generation method that utilizesa diffusion model to improve the attack success rate and imperceptibility inthe black-box setting, without relying on the internal information of the pointcloud classification model to generate adversarial samples. We use a 3Ddiffusion model to use the compressed features of the point cloud as priorknowledge to guide the reverse diffusion process to add adversarial points toclean examples. Subsequently, its reverse process is employed to transform thedistribution of other categories into adversarial points, which are then addedto the point cloud.</description>
      <author>example@mail.com (Ruiyang Zhao, Bingbing Zhu, Chuxuan Tong, Xiaoyi Zhou, Xi Zheng)</author>
      <guid isPermaLink="false">2507.21163v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2507.21971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EIFNet是一种多模态融合网络，结合了事件相机和基于帧输入的优势，通过自适应事件特征细化模块、模态自适应重校准模块和多头注意力门控融合模块，解决了基于事件的语义分割中的主要挑战，在相关数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;事件相机具有高动态范围和高时间分辨率的优点，使其在具有挑战性的环境中实现鲁棒场景理解方面具有潜力。基于事件的语义分割是一个有前景的研究方向。&lt;h4&gt;目的&lt;/h4&gt;解决基于事件的语义分割中的两个主要挑战：从稀疏和嘈杂的事件流中提取可靠特征，以及有效融合结构化表示不同、语义丰富的密集图像数据。&lt;h4&gt;方法&lt;/h4&gt;提出EIFNet，一个多模态融合网络，包含自适应事件特征细化模块(AEFRM)用于提高事件表示，模态自适应重校准模块(MARM)和多头注意力门控融合模块(MGFM)用于对齐和集成跨模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;EIFNet在DDD17-Semantic和DSEC-Semantic数据集上实现了最先进的性能，证明了其在基于事件的语义分割中的有效性。&lt;h4&gt;结论&lt;/h4&gt;EIFNet有效地解决了基于事件的语义分割中的主要挑战，通过多模态融合和特征处理模块，提高了事件相机的利用效率，在具有挑战性的环境中实现了鲁棒的场景理解。&lt;h4&gt;翻译&lt;/h4&gt;基于事件的语义分割探索了事件相机的潜力，事件相机具有高动态范围和高时间分辨率，能够在具有挑战性的环境中实现鲁棒的场景理解。尽管有这些优势，由于两个主要挑战，该任务仍然困难：从稀疏和嘈杂的事件流中提取可靠特征，以及有效融合结构化和表示不同的密集、语义丰富的图像数据。为解决这些问题，我们提出了EIFNet，一个结合了事件输入和基于帧输入优势的多模态融合网络。该网络包括自适应事件特征细化模块(AEFRM)，通过多尺度活动建模和空间注意力提高事件表示。此外，我们引入了模态自适应重校准模块(MARM)和多头注意力门控融合模块(MGFM)，它们使用注意力机制和门控融合策略对齐和集成跨模态特征。在DDD17-Semantic和DSEC-Semantic数据集上的实验表明，EIFNet实现了最先进的性能，证明了其在基于事件的语义分割中的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决事件相机和图像数据融合进行语义分割的问题。具体来说面临两个挑战：从稀疏、噪声的事件流中提取可靠特征，以及有效融合结构不同的事件和图像特征。这个问题在现实中很重要，因为在自动驾驶、机器人等应用中，传统相机在动态模糊、低光照或高速动态等挑战性环境中表现不佳，而事件相机具有高动态范围和微秒级时间分辨率，适合这些场景，通过融合两种数据可以提高环境感知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：固定中间表示无法有效区分噪声和有意义的运动，跨模态融合交互深度不足。他们设计了一个'表示-重校准-融合'的三阶段策略逐步整合多模态信息。该方法借鉴了多模态融合的基本思想和注意力机制，但针对事件数据的稀疏、异步特性进行了专门改进，使用了自适应特征提取和深度跨模态交互技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应事件特征细化增强有用信息并抑制噪声，使用模态自适应重校准生成更清洁的特征表示，利用多头注意力门控融合实现深层次跨模态特征交互。整体流程是：输入同步事件和图像帧→AEFRM处理事件数据增强特征→双分支Transformer编码器分别处理事件和图像特征→MARM进行通道和空间重校准→MGFM进行跨模态融合→聚合多尺度特征→Transformer解码器输出分割图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) AEFRM模块结合多尺度建模和空间注意力增强事件特征；2) MARM模块在通道和空间维度重校准特征，抑制噪声；3) MGFM模块结合交叉注意力和差分注意力实现深度跨模态融合。相比之前工作，该方法不再使用固定中间表示，解决了跨模态交互深度不足的问题，针对事件数据特性专门设计，实现了更深层次的交互和动态门控融合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EIFNet通过创新的注意力机制和门控融合策略，有效解决了事件相机与图像数据融合进行语义分割的挑战，显著提升了在动态模糊、低光照等复杂环境中的场景理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event-based semantic segmentation explores the potential of event cameras,which offer high dynamic range and fine temporal resolution, to achieve robustscene understanding in challenging environments. Despite these advantages, thetask remains difficult due to two main challenges: extracting reliable featuresfrom sparse and noisy event streams, and effectively fusing them with dense,semantically rich image data that differ in structure and representation. Toaddress these issues, we propose EIFNet, a multi-modal fusion network thatcombines the strengths of both event and frame-based inputs. The networkincludes an Adaptive Event Feature Refinement Module (AEFRM), which improvesevent representations through multi-scale activity modeling and spatialattention. In addition, we introduce a Modality-Adaptive Recalibration Module(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align andintegrate features across modalities using attention mechanisms and gatedfusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasetsshow that EIFNet achieves state-of-the-art performance, demonstrating itseffectiveness in event-based semantic segmentation.</description>
      <author>example@mail.com (Zhijiang Li, Haoran He)</author>
      <guid isPermaLink="false">2507.21971v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Multilingual JobBERT for Cross-Lingual Job Title Matching</title>
      <link>http://arxiv.org/abs/2507.21609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the TalentCLEF 2025 Workshop as part of CLEF 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;JobBERT-V3是一个基于对比学习的跨语言职位标题匹配模型，支持英语、德语、西班牙语和中文，通过合成翻译和超过2100万条职位标题的多语言数据集训练而成，保留了前身的效率架构，无需任务特定监督即可实现跨语言稳健对齐。&lt;h4&gt;背景&lt;/h4&gt;基于最先进的单语言JobBERT-V2模型，需要扩展到多语言环境以支持跨语言职位标题匹配。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在多种语言（英语、德语、西班牙语和中文）间进行职位标题匹配的模型，同时保持高效率和无需任务特定监督。&lt;h4&gt;方法&lt;/h4&gt;利用合成翻译和超过2100万条职位标题的平衡多语言数据集，基于对比学习方法训练JobBERT-V3模型，保留了JobBERT-V2的效率架构。&lt;h4&gt;主要发现&lt;/h4&gt;在TalentCLEF2025基准上的广泛评估表明，JobBERT-V3优于强大的多语言基线模型，并在单语言和跨语言设置中实现了一致的性能。此外，该模型还可以有效地用于为给定职位标题排名相关技能。&lt;h4&gt;结论&lt;/h4&gt;JobBERT-V3是一个高效的多语言职位标题匹配模型，在跨语言环境中表现优异，并且具有更广泛的应用价值，特别是在多语言劳动力市场情报领域。&lt;h4&gt;翻译&lt;/h4&gt;模型公开可用，访问链接为：https://huggingface.co/TechWolf/JobBERT-v3。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce JobBERT-V3, a contrastive learning-based model for cross-lingualjob title matching. Building on the state-of-the-art monolingual JobBERT-V2,our approach extends support to English, German, Spanish, and Chinese byleveraging synthetic translations and a balanced multilingual dataset of over21 million job titles. The model retains the efficiency-focused architecture ofits predecessor while enabling robust alignment across languages withoutrequiring task-specific supervision. Extensive evaluations on the TalentCLEF2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingualbaselines and achieves consistent performance across both monolingual andcross-lingual settings. While not the primary focus, we also show that themodel can be effectively used to rank relevant skills for a given job title,demonstrating its broader applicability in multilingual labor marketintelligence. The model is publicly available:https://huggingface.co/TechWolf/JobBERT-v3.</description>
      <author>example@mail.com (Jens-Joris Decorte, Matthias De Lange, Jeroen Van Hautte)</author>
      <guid isPermaLink="false">2507.21609v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation</title>
      <link>http://arxiv.org/abs/2507.21563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用大型语言模型和物品文本描述进行数据增强的新框架，以解决推荐系统中的数据稀疏问题并减轻流行度偏差。&lt;h4&gt;背景&lt;/h4&gt;推荐系统常因用户-物品交互数据稀疏而性能下降，并在实际场景中放大流行度偏差。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的数据增强框架，利用大型语言模型和物品文本描述来丰富交互数据。&lt;h4&gt;方法&lt;/h4&gt;通过少样本提示多次提示大型语言模型来重新排序物品，通过多数投票聚合结果生成高置信度的合成用户-物品交互，并将增强数据集成到图对比学习框架中以缓解分布偏移和减轻流行度偏差。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明该方法提高了准确性并减少了流行度偏差，性能优于强基线方法。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了推荐系统中的数据稀疏问题，同时减轻了流行度偏差。&lt;h4&gt;翻译&lt;/h4&gt;推荐系统常因有限的用户-物品交互而导致数据稀疏，这降低了它们的性能并在实际场景中放大了流行度偏差。本文提出了一种新的数据增强框架，利用大型语言模型和物品文本描述来丰富交互数据。通过少样本提示多次提示大型语言模型来重新排序物品，并通过多数投票聚合结果，我们生成了有高置信度的合成用户-物品交互，这基于测度集中理论提供了理论保证。为了在图推荐系统的背景下有效利用增强数据，我们将其集成到图对比学习框架中，以缓解分布偏移并减轻流行度偏差。大量实验表明，我们的方法提高了准确性并减少了流行度偏差，优于强基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommendation systems often suffer from data sparsity caused by limiteduser-item interactions, which degrade their performance and amplify popularitybias in real-world scenarios. This paper proposes a novel data augmentationframework that leverages Large Language Models (LLMs) and item textualdescriptions to enrich interaction data. By few-shot prompting LLMs multipletimes to rerank items and aggregating the results via majority voting, wegenerate high-confidence synthetic user-item interactions, supported bytheoretical guarantees based on the concentration of measure. To effectivelyleverage the augmented data in the context of a graph recommendation system, weintegrate it into a graph contrastive learning framework to mitigatedistributional shift and alleviate popularity bias. Extensive experiments showthat our method improves accuracy and reduces popularity bias, outperformingstrong baselines.</description>
      <author>example@mail.com (Minh-Anh Nguyen, Bao Nguyen, Ha Lan N. T., Tuan Anh Hoang, Duc-Trong Le, Dung D. Le)</author>
      <guid isPermaLink="false">2507.21563v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning</title>
      <link>http://arxiv.org/abs/2507.21147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in the Proceedings of ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了野火对生态系统和人类健康的影响，以及气候变化如何加剧这些问题，特别是在地中海等干旱地区。研究提出了一种基于形态的课程对比学习方法来解决野火预测中的不平衡数据和高维时空数据挑战，实现了使用更小区块尺寸而不影响性能的效果。&lt;h4&gt;背景&lt;/h4&gt;野火导致生物多样性丧失、水文地质风险增加和有毒物质排放增加。气候变化加剧了这些影响，特别是在温度上升和干旱期延长的地区。数据显示野火事件发生率低于典型情况，且高维时空数据的复杂性给深度学习训练带来挑战。精确的野火预测依赖天气数据，降低计算成本以实现更频繁的更新是有益的。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过采用对比框架来增强区块动态特征的潜在表示，从而解决野火预测中的不平衡数据和高维时空数据复杂性挑战。&lt;h4&gt;方法&lt;/h4&gt;引入一种新的基于形态的课程对比学习方法，减轻不同区域特性差异带来的问题，允许使用更小的区块尺寸而不影响性能，并通过实验分析验证所提出的建模策略的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;基于形态的课程对比学习方法能够有效处理区域特性差异，使用更小区块尺寸不影响预测性能，解决了野火预测中的不平衡数据和高维时空数据复杂性挑战。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的基于形态的课程对比学习方法是一种有效的解决方案，能够解决野火预测中的关键挑战，特别是在处理不平衡数据和高维时空数据方面。&lt;h4&gt;翻译&lt;/h4&gt;野火对自然生态系统和人类健康有重大影响，导致生物多样性丧失、水文地质风险增加和有毒物质排放增加。气候变化加剧了这些影响，特别是在温度上升和干旱期延长的地区，如地中海。这需要开发利用最新技术的先进风险管理策略。然而，在此背景下，数据显示存在不平衡设置的问题，其中野火事件的发生率显著低于典型情况。这种不平衡，加上高维时空数据的固有复杂性，给深度学习架构的训练带来了重大挑战。此外，由于精确的野火预测主要依赖天气数据，找到降低计算成本的方法以实现使用最新天气预报的更频繁更新将是有益的。本文研究了如何通过增强区块动态特征的潜在表示来采用对比框架解决这些挑战。因此，我们引入了一种新的基于形态的课程对比学习方法，它减轻了与不同区域特性相关的问题，并允许使用更小的区块尺寸而不影响性能。进行了实验分析以验证所提出的建模策略的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wildfires significantly impact natural ecosystems and human health, leadingto biodiversity loss, increased hydrogeological risks, and elevated emissionsof toxic substances. Climate change exacerbates these effects, particularly inregions with rising temperatures and prolonged dry periods, such as theMediterranean. This requires the development of advanced risk managementstrategies that utilize state-of-the-art technologies. However, in thiscontext, the data show a bias toward an imbalanced setting, where the incidenceof wildfire events is significantly lower than typical situations. Thisimbalance, coupled with the inherent complexity of high-dimensionalspatio-temporal data, poses significant challenges for training deep learningarchitectures. Moreover, since precise wildfire predictions depend mainly onweather data, finding a way to reduce computational costs to enable morefrequent updates using the latest weather forecasts would be beneficial. Thispaper investigates how adopting a contrastive framework can address thesechallenges through enhanced latent representations for the patch's dynamicfeatures. We thus introduce a new morphology-based curriculum contrastivelearning that mitigates issues associated with diverse regional characteristicsand enables the use of smaller patch sizes without compromising performance. Anexperimental analysis is performed to validate the effectiveness of theproposed modeling strategies.</description>
      <author>example@mail.com (Fabrizio Lo Scudo, Alessio De Rango, Luca Furnari, Alfonso Senatore, Donato D'Ambrosio, Giuseppe Mendicino, Gianluigi Greco)</author>
      <guid isPermaLink="false">2507.21147v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy</title>
      <link>http://arxiv.org/abs/2507.21358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为协作感知器(CoP)的多任务学习框架，通过利用空间占用信息作为辅助信息，改进了基于视觉的鸟瞰图3D物体检测方法，整合了物体检测和占用预测任务之间的相似性，构建了更鲁棒的BEV表示。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图(BEV)3D物体检测在自动驾驶领域已取得显著进展，提供了成本效益和丰富的上下文信息。然而，现有方法通常通过折叠提取的物体特征来构建BEV表示，忽略了内在的环境上下文信息，如道路和人行道。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法忽视环境上下文信息的问题，本研究旨在开发一种多任务学习框架，利用空间占用信息作为辅助信息，挖掘3D物体检测和占用预测任务之间一致的结构性和概念性相似性，从而弥补空间表示和特征细化方面的差距。&lt;h4&gt;方法&lt;/h4&gt;作者首先提出了一种生成密集占用真实值的管道，整合局部密度信息(LDO)以重建详细的环境信息。其次，他们采用了基于体素高度引导的采样(VHS)策略，根据不同的物体属性提取细粒度的局部特征。此外，他们开发了一个全局-局部协作特征融合(CFF)模块，无缝整合两个任务之间的互补知识。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准测试上的大量实验表明，CoP超越了现有的基于视觉的框架，在测试集上达到了49.5%的平均精度(mAP)和59.2%的检测分数(NDS)。&lt;h4&gt;结论&lt;/h4&gt;协作感知器(CoP)框架通过整合空间占用信息和多任务学习，有效解决了现有基于视觉的BEV 3D物体检测方法忽视环境上下文信息的问题，显著提高了检测性能，为自动驾驶领域提供了一种新的、更全面的感知环境的方法。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图(BEV)3D物体检测在自动驾驶领域通过提供成本效益和丰富的上下文信息已取得显著进展。然而，现有方法通常通过折叠提取的物体特征来构建BEV表示，忽略了内在的环境上下文信息，如道路和人行道。这限制了检测器对物理世界特征的全面感知。为了缓解这一问题，我们引入了一种多任务学习框架——协作感知器(CoP)，它利用空间占用作为辅助信息，挖掘3D物体检测和占用预测任务之间一致的结构性和概念性相似性，弥补了空间表示和特征细化方面的差距。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于视觉的鸟瞰图3D目标检测方法在构建表示时忽略环境内在上下文信息的问题。这个问题很重要，因为现有方法难以识别具有独特或不规则几何形状的物体，影响自动驾驶安全性；同时仅依赖特定任务的局部或全局信息会阻碍通用环境表示的构建，导致目标识别不一致和性能不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有3D检测面临的三大挑战（难以识别不规则形状物体、传统BEV方法无法自适应保留空间特性、仅依赖特定任务信息阻碍通用表示）来设计方法。他们借鉴了BEVDet的LSS视图变换框架、3D占用预测方法如SurroundOcc、FlashOcc中的通道到高度模块以及Squeeze-and-Excitation注意力机制，在此基础上创新性地提出了多任务学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务学习框架利用空间占用作为辅助信息，挖掘3D目标检测和占用预测任务间的一致结构和概念相似性，通过全局-局部协作特征融合构建更强大的BEV表示。整体流程包括：多视图图像输入→提取图像特征→LSS视图变换为3D空间特征→生成局部密度感知占用真实值→体素高度引导采样提取局部特征→全局-局部协作特征融合→构建统一BEV表示→转发到任务特定检测和占用预测头。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)局部密度感知的空间占用生成管道，解决均匀点密度假设问题；2)体素高度引导采样策略，根据物体属性提取细粒度局部特征；3)全局-局部协作特征融合模块，无缝集成互补知识。相比之前工作，本文引入占用预测作为辅助任务，通过VHS保留细粒度空间信息，考虑局部密度变化，能更好处理不规则形状物体。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的协作感知器通过局部密度感知的空间占用信息和全局-局部协作特征融合，显著提升了基于视觉的3D目标检测性能，特别是在处理具有独特或不规则几何形状的物体方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based bird's-eye-view (BEV) 3D object detection has advancedsignificantly in autonomous driving by offering cost-effectiveness and richcontextual information. However, existing methods often construct BEVrepresentations by collapsing extracted object features, neglecting intrinsicenvironmental contexts, such as roads and pavements. This hinders detectorsfrom comprehensively perceiving the characteristics of the physical world. Toalleviate this, we introduce a multi-task learning framework, CollaborativePerceiver (CoP), that leverages spatial occupancy as auxiliary information tomine consistent structural and conceptual similarities shared between 3D objectdetection and occupancy prediction tasks, bridging gaps in spatialrepresentations and feature refinement. To this end, we first propose apipeline to generate dense occupancy ground truths incorporating local densityinformation (LDO) for reconstructing detailed environmental information. Next,we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grainedlocal features according to distinct object properties. Furthermore, we developa global-local collaborative feature fusion (CFF) module that seamlesslyintegrates complementary knowledge between both tasks, thus composing morerobust BEV representations. Extensive experiments on the nuScenes benchmarkdemonstrate that CoP outperforms existing vision-based frameworks, achieving49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials areavailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.</description>
      <author>example@mail.com (Jicheng Yuan, Manh Nguyen Duc, Qian Liu, Manfred Hauswirth, Danh Le Phuoc)</author>
      <guid isPermaLink="false">2507.21358v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Multivariate Spatio-temporal Modelling for Completing Cancer Registries and Forecasting Incidence</title>
      <link>http://arxiv.org/abs/2507.21714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  36 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种解决在区域和国家层面数据不完整情况下预测癌症发病率挑战的方法，通过使用多变量时空共享分量模型联合建模死亡数据和可用的癌症发病率数据。&lt;h4&gt;背景&lt;/h4&gt;癌症数据（发病率和死亡率）对于理解癌症负担、设定控制目标和评估政策实施至关重要，但数据处理的复杂性导致癌症发病率数据通常滞后2-3年。此外，不同区域癌症登记处建立时间不同，导致区域间数据在时间上不一致。&lt;h4&gt;目的&lt;/h4&gt;解决在区域和国家层面数据不完整的情况下预测癌症发病率的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出使用多变量时空共享分量模型，联合建模死亡数据和可用的癌症发病率数据。使用英格兰2001-2019年期间的肺癌发病率和死亡人数数据分析模型性能，并通过计算不同的模型预测指标选择最佳模型。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体研究发现。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及研究结论。&lt;h4&gt;翻译&lt;/h4&gt;癌症数据，特别是癌症发病率和死亡率，是理解癌症负担、设定癌症控制目标和评估癌症控制政策实施进展的基础。然而，数据收集、分类、验证和处理的复杂性导致癌症发病率数据通常比实际年份滞后2-3年。因此，国家和区域人口基础癌症登记处(PBCRs)对预测癌症发病率的方法越来越感兴趣。但在许多国家，还存在一个额外困难：区域癌症登记处通常不是在同一年建立的，因此不同区域的癌症发病率数据在时间上不一致。本研究旨在解决在区域和国家层面数据不完整的情况下预测癌症发病率的挑战。为实现这一目标，我们提出使用多变量时空共享分量模型，联合建模死亡数据和可用的癌症发病率数据。使用英格兰2001-2019年期间报告的肺癌发病率和死亡人数数据分析了这些多变量模型的性能。计算了不同的模型预测指标以选择最佳模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cancer data, particularly cancer incidence and mortality, are fundamental tounderstand the cancer burden, to set targets for cancer control and to evaluatethe evolution of the implementation of a cancer control policy. However, thecomplexity of data collection, classification, validation and processing resultin cancer incidence figures often lagging two to three years behind thecalendar year. In response, national or regional population-based cancerregistries (PBCRs) are increasingly interested in methods for forecastingcancer incidence. However, in many countries there is an additional difficultyin projecting cancer incidence as regional registries are usually notestablished in the same year and therefore cancer incidence data series betweendifferent regions of a country are not harmonised over time. This studyaddresses the challenge of forecasting cancer incidence with incomplete data atboth regional and national levels. To achieve our objective, we propose the useof multivariate spatio-temporal shared component models that jointly modelmortality data and available cancer incidence data. The performance of thesemultivariate models are analyzed using lung cancer incidence data, togetherwith the number of deaths reported in England in the period 2001-2019.Different model predictive measures have been calculated to select the bestmodel.</description>
      <author>example@mail.com (Garazi Retegui, Jaione Etxeberria, María Dolores Ugarte)</author>
      <guid isPermaLink="false">2507.21714v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>ST-GDance: Long-Term and Collision-Free Group Choreography from Music</title>
      <link>http://arxiv.org/abs/2507.21518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures. Accepted at BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ST-GDance是一种创新的群体舞蹈生成框架，通过解耦时空依赖关系，使用轻量级图卷积和加速稀疏注意力技术，实现了高效且无碰撞的群体舞蹈生成。&lt;h4&gt;背景&lt;/h4&gt;从音乐生成群体舞蹈在电影、游戏和动画制作中有广泛的应用。然而，随着舞者数量和序列长度的增加，任务面临更高的计算复杂性和动作碰撞风险，现有方法难以建模密集的时空交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在群体舞蹈生成中的局限性，提出一种新框架来优化长期且无碰撞的群体编舞。&lt;h4&gt;方法&lt;/h4&gt;提出了ST-GDance框架，解耦空间和时间依赖关系，使用轻量级图卷积进行距离感知的空间建模，采用加速稀疏注意力进行高效的时间建模，显著降低计算成本同时确保流畅且无碰撞的交互。&lt;h4&gt;主要发现&lt;/h4&gt;在AIOZ-GDance数据集上的实验表明，ST-GDance优于最先进的基线方法，特别是在生成长期且连贯的群体舞蹈序列方面表现突出。&lt;h4&gt;结论&lt;/h4&gt;ST-GDance框架能够有效解决群体舞蹈生成中的时空协调问题，在计算效率和舞蹈质量之间取得了良好的平衡。&lt;h4&gt;翻译&lt;/h4&gt;从音乐生成群体舞蹈在电影、游戏和动画制作中有广泛的应用。然而，它需要同步多个舞者同时保持空间协调。随着舞者数量和序列长度的增加，这项任务面临更高的计算复杂性和更大的动作碰撞风险。现有方法通常难以建模密集的时空交互，导致可扩展性问题和多舞者碰撞。为了解决这些挑战，我们提出了ST-GDance，一个新颖的框架，它解耦了空间和时间依赖关系，以优化长期且无碰撞的群体编舞。我们使用轻量级图卷积进行距离感知的空间建模，并采用加速稀疏注意力进行高效的时间建模。这种设计显著降低了计算成本，同时确保了流畅且无碰撞的交互。在AIOZ-GDance数据集上的实验表明，ST-GDance优于最先进的基线方法，特别是在生成长期且连贯的群体舞蹈序列方面。项目页面：https://yilliajing.github.io/ST-GDance-Website/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Group dance generation from music has broad applications in film, gaming, andanimation production. However, it requires synchronizing multiple dancers whilemaintaining spatial coordination. As the number of dancers and sequence lengthincrease, this task faces higher computational complexity and a greater risk ofmotion collisions. Existing methods often struggle to model densespatial-temporal interactions, leading to scalability issues and multi-dancercollisions. To address these challenges, we propose ST-GDance, a novelframework that decouples spatial and temporal dependencies to optimizelong-term and collision-free group choreography. We employ lightweight graphconvolutions for distance-aware spatial modeling and accelerated sparseattention for efficient temporal modeling. This design significantly reducescomputational costs while ensuring smooth and collision-free interactions.Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperformsstate-of-the-art baselines, particularly in generating long and coherent groupdance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.</description>
      <author>example@mail.com (Jing Xu, Weiqiang Wang, Cunjian Chen, Jun Liu, Qiuhong Ke)</author>
      <guid isPermaLink="false">2507.21518v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes</title>
      <link>http://arxiv.org/abs/2507.21460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的光场极平面结构图像表示方法和角度-时间交互网络(ATINet)，用于光场目标跟踪，特别在低光场景中表现出色。&lt;h4&gt;背景&lt;/h4&gt;高质量的4D光场表示对于场景感知至关重要，因为它能提供区分性的空间-角度线索来识别移动目标。然而，现有方法在时间域内难以提供可靠的角度建模，特别是在复杂低光场景中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理低光场景中光场角度建模的方法，提高目标跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了光场极平面结构图像(ESI)表示，明确定义光场内的几何结构；2. 利用光场极平面内光线角度的突变，增强低光场景视觉表现并减少高维光场冗余；3. 提出了角度-时间交互网络(ATINet)，学习光场的几何结构线索和角度-时间交互线索；4. ATINet可通过自监督方式优化，增强时间域内的几何特征交互；5. 引入了大规模光场低光数据集用于目标跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;1. ATINet在单目标跟踪中取得了最先进的性能；2. 将方法扩展到多目标跟踪也显示了高质量光场角度-时间建模的有效性。&lt;h4&gt;结论&lt;/h4&gt;高质量的光场角度-时间建模对于目标跟踪任务至关重要，特别是在低光场景中。提出的方法和模型能有效解决现有方法在处理复杂低光场景时的局限性。&lt;h4&gt;翻译&lt;/h4&gt;高质量的4D光场表示与高效的角度特征建模对于场景感知至关重要，因为它可以提供区分性的空间-角度线索来识别移动目标。然而，最近的发展在时间域内仍难以提供可靠的角度建模，特别是在复杂的低光场景中。在本文中，我们提出了一种新的光场极平面结构图像(ESI)表示，明确地定义了光场内的几何结构。通过利用光场极平面内光线角度的突变，这种表示可以增强低光场景的视觉表现并减少高维光场的冗余。我们进一步提出了一个用于光场目标跟踪的角度-时间交互网络(ATINet)，从光场的几何结构线索和角度-时间交互线索中学习角度感知表示。此外，ATINet还可以通过自监督方式进行优化，以增强时间域内的几何特征交互。最后，我们引入了一个用于目标跟踪的大规模光场低光数据集。大量的实验证明了ATINet在单目标跟踪中取得了最先进的性能。此外，我们将提出的方法扩展到多目标跟踪，这也显示了高质量光场角度-时间建模的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在低光场景下利用光场数据进行更准确目标跟踪的问题。这个问题很重要，因为在低光条件下，传统RGB图像的强度信息会显著减弱，导致基于RGB的目标跟踪性能下降。光场成像技术同时记录了空间域的强度信息和角度域的几何结构信息，这种额外的几何信息在低光场景下能提供可靠的区分特征，有助于提高目标跟踪的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了光场数据的特性，指出现有方法难以处理高维光场数据中的角度-时间关系建模。然后设计了一种新的光场表示方法(ESI)，通过利用极平面内光射线角度的突变来提取几何结构点。接着设计了角度-时间交互网络(ATINet)，并引入自监督损失函数来优化模型。作者借鉴了EPI(极平面图像)的概念、Transformer架构中的自注意力机制、自监督学习思想以及Trades框架在多目标跟踪中的应用，但都进行了针对性的改进以适应光场数据的特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用光场数据中的几何结构信息(特别是角度域中的突变点)来增强低光场景下的目标跟踪能力。整体流程包括：1)构建光场ESI表示，从高维光场中提取关键几何结构点；2)设计角度-时间交互网络(ATINet)的双流结构，一个流处理外观特征，另一个流处理角度-时间关系；3)使用几何自适应选择(GAS)模块自适应选择几何结构点；4)采用自监督学习策略优化模型，减少对标注数据的依赖；5)扩展到多目标跟踪任务，结合外观和几何特征进行目标关联。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出光场ESI表示方法，通过检测EPI角度空间中的一阶导数最大值提取几何结构点，去除冗余信息；2)设计角度-时间交互网络(ATINet)和几何自适应选择(GAS)模块；3)引入专门针对光场数据的自监督损失函数；4)构建大规模光场低光跟踪数据集。相比之前工作，不同之处在于：ESI表示更专注于几何结构而非冗余外观信息；ATINet专门针对光场的角度-时间特性设计；使用GAS模块处理光场的稀疏结构信息而非密集像素数据；通过自监督学习减少对大量标注数据的依赖。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于光场几何结构的角度-时间交互网络，通过新的ESI表示和自监督学习策略，显著提升了低光场景下的目标跟踪性能，并构建了大规模光场跟踪数据集推动了该领域的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality 4D light field representation with efficient angular featuremodeling is crucial for scene perception, as it can provide discriminativespatial-angular cues to identify moving targets. However, recent developmentsstill struggle to deliver reliable angular modeling in the temporal domain,particularly in complex low-light scenes. In this paper, we propose a novellight field epipolar-plane structure image (ESI) representation that explicitlydefines the geometric structure within the light field. By capitalizing on theabrupt changes in the angles of light rays within the epipolar plane, thisrepresentation can enhance visual expression in low-light scenes and reduceredundancy in high-dimensional light fields. We further propose anangular-temporal interaction network (ATINet) for light field object trackingthat learns angular-aware representations from the geometric structural cuesand angular-temporal interaction cues of light fields. Furthermore, ATINet canalso be optimized in a self-supervised manner to enhance the geometric featureinteraction across the temporal domain. Finally, we introduce a large-scalelight field low-light dataset for object tracking. Extensive experimentationdemonstrates that ATINet achieves state-of-the-art performance in single objecttracking. Furthermore, we extend the proposed method to multiple objecttracking, which also shows the effectiveness of high-quality light fieldangular-temporal modeling.</description>
      <author>example@mail.com (Mianzhao Wang, Fan Shi, Xu Cheng, Feifei Zhang, Shengyong Chen)</author>
      <guid isPermaLink="false">2507.21460v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Generalized few-shot transfer learning architecture for modeling the EDFA gain spectrum</title>
      <link>http://arxiv.org/abs/2507.21728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a preprint of a paper accepted and published in the Journal  of Optical Communications and Networking (JOCN). The final published version  is available at: https://doi.org/10.1364/JOCN.560987&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于半监督自归一化神经网络的少样本迁移学习方法，用于准确建模铒掺杂光纤放大器的增益光谱，减少测量需求并提高预测精度。&lt;h4&gt;背景&lt;/h4&gt;随着网络向多厂商解决方案发展，准确建模EDFAs的增益光谱对优化光网络性能至关重要，但传统方法可能需要大量测量数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够减少测量需求同时提高增益光谱预测准确性的方法，适用于不同类型的EDFA（助推器、前置放大器和ILA EDFAs）。&lt;h4&gt;方法&lt;/h4&gt;提出基于半监督自归一化神经网络(SS-NN)的架构，采用两阶段训练策略（无监督预训练和监督微调），并扩展了迁移学习框架支持同质和异质模型适应，使用协方差匹配损失处理特征不匹配问题。&lt;h4&gt;主要发现&lt;/h4&gt;在26个EDFA上的实验表明，该方法显著减少了系统测量需求，同时实现了比基准方法更低的平均绝对误差和改进的误差分布。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了EDFAs增益光谱建模中的测量效率与准确性问题，适用于多厂商环境下的不同类型EDFA。&lt;h4&gt;翻译&lt;/h4&gt;准确建模铒掺杂光纤放大器(EDFAs)的增益光谱对于优化光网络性能至关重要，特别是随着网络向多厂商解决方案发展。在这项工作中，我们提出了一种基于半监督自归一化神经网络(SS-NN)的广义少样本迁移学习架构，利用EDFA内部特征（如VOA输入或输出功率和衰减）来提高增益光谱预测。我们的SS-NN模型采用两阶段训练策略，包括带噪声增强测量的无监督预训练和带自定义加权MSE损失的监督微调。此外，我们通过迁移学习(TL)技术扩展了该框架，使助推器、前置放大器和ILA EDFAs能够进行同质（相同特征空间）和异质（不同特征集）模型适应。为了解决异质迁移学习中的特征不匹配问题，我们纳入了协方差匹配损失，以对齐源域和目标域之间的二阶特征统计。在COSMOS和Open Ireland测试床中对26个EDFA进行的广泛实验表明，与基准方法相比，所提出的方法显著减少了系统测量需求，同时实现了更低的平均绝对误差和改进的误差分布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1364/JOCN.560987&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers(EDFAs) is essential for optimizing optical network performance, particularlyas networks evolve toward multi-vendor solutions. In this work, we propose ageneralized few-shot transfer learning architecture based on a Semi-SupervisedSelf-Normalizing Neural Network (SS-NN) that leverages internal EDFA features -such as VOA input or output power and attenuation, to improve gain spectrumprediction. Our SS-NN model employs a two-phase training strategy comprisingunsupervised pre-training with noise-augmented measurements and supervisedfine-tuning with a custom weighted MSE loss. Furthermore, we extend theframework with transfer learning (TL) techniques that enable both homogeneous(same-feature space) and heterogeneous (different-feature sets) modeladaptation across booster, preamplifier, and ILA EDFAs. To address featuremismatches in heterogeneous TL, we incorporate a covariance matching loss toalign second-order feature statistics between source and target domains.Extensive experiments conducted across 26 EDFAs in the COSMOS and Open Irelandtestbeds demonstrate that the proposed approach significantly reduces thenumber of measurements requirements on the system while achieving lower meanabsolute errors and improved error distributions compared to benchmark methods.</description>
      <author>example@mail.com (Agastya Raj, Zehao Wang, Tingjun Chen, Daniel C Kilper, Marco Ruffini)</author>
      <guid isPermaLink="false">2507.21728v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation</title>
      <link>http://arxiv.org/abs/2507.21455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自监督数据集蒸馏的新方法，通过学习紧凑的代表性图像和表征集合来减少数据集大小，同时保持模型性能。该方法通过创新的参数化、预增强处理和轻量级网络技术，显著提高了蒸馏效率、跨架构泛化能力和迁移学习性能。&lt;h4&gt;背景&lt;/h4&gt;大型深度模型需要更大的数据集进行训练，但数据集的快速增长带来了显著的训练成本挑战。数据集蒸馏成为一种流行技术，通过学习高度紧凑的代表性样本来减少数据集大小，使模型性能与使用完整数据集训练相当。&lt;h4&gt;目的&lt;/h4&gt;大多数现有数据集蒸馏工作专注于监督数据集，而本文旨在将图像及其自监督训练的表征蒸馏成一个紧凑的集合，实现更高效的数据集压缩，同时保持或提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出自监督数据集蒸馏方法，包含三种创新技术：1) 通过不同低维基对图像和表征进行创新参数化；2) 利用预增强处理解决数据增强的随机性引起的不稳定性；3) 使用轻量级网络建模同一图像增强视图表征间的连接，实现更紧凑的蒸馏对。&lt;h4&gt;主要发现&lt;/h4&gt;在各种数据集上的实验表明，该方法在蒸馏效率、跨架构泛化能力和迁移学习性能方面具有优越性。自监督数据集蒸馏能有效从真实数据集中提取丰富信息，产生具有增强跨架构泛化能力的蒸馏集合。&lt;h4&gt;结论&lt;/h4&gt;自监督数据集蒸馏是一种有效的数据集压缩方法，通过提出的三种新技术，能够在减少数据集大小的同时保持或提高模型性能，特别是在跨架构泛化和迁移学习方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;尽管更大的数据集对于训练大型深度模型至关重要，但数据集大小的快速增长带来了显著的训练成本挑战，甚至导致计算费用过高。数据集蒸馏最近成为一种流行技术，通过学习一组高度紧凑的代表性样本来减少数据集大小，其中使用这些样本训练的模型理应与使用完整数据集训练的模型具有可比性能。然而，大多数现有的数据集蒸馏工作专注于监督数据集，而我们则旨在将图像及其自监督训练的表征蒸馏成一个蒸馏集合。这一过程被称为自监督数据集蒸馏，有效地从真实数据集中提取丰富信息，产生具有增强跨架构泛化能力的蒸馏集合。特别是，为了更忠实和紧凑地保留原始数据集的关键特征，提出了几种新技术：1) 我们通过不同的低维基对图像和表征进行创新参数化，实验表明基选择在参数化中起着关键作用；2) 我们利用预增强处理由数据增强的随机性引起的不稳定性——这是自监督学习的关键组成部分但在先前自监督数据集蒸馏工作中被低估；3) 我们进一步利用轻量级网络建模来自同一图像的增强视图表征之间的连接，从而实现更紧凑的蒸馏对。在各种数据集上进行的广泛实验验证了我们的方法在蒸馏效率、跨架构泛化能力和迁移学习性能方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although larger datasets are crucial for training large deep models, therapid growth of dataset size has brought a significant challenge in terms ofconsiderable training costs, which even results in prohibitive computationalexpenses. Dataset Distillation becomes a popular technique recently to reducethe dataset size via learning a highly compact set of representative exemplars,where the model trained with these exemplars ideally should have comparableperformance with respect to the one trained with the full dataset. While mostof existing works upon dataset distillation focus on supervised datasets, weinstead aim to distill images and their self-supervisedly trainedrepresentations into a distilled set. This procedure, named as Self-SupervisedDataset Distillation, effectively extracts rich information from real datasets,yielding the distilled sets with enhanced cross-architecture generalizability.Particularly, in order to preserve the key characteristics of original datasetmore faithfully and compactly, several novel techniques are proposed: 1) weintroduce an innovative parameterization upon images and representations viadistinct low-dimensional bases, where the base selection for parameterizationis experimentally shown to play a crucial role; 2) we tackle the instabilityinduced by the randomness of data augmentation -- a key component inself-supervised learning but being underestimated in the prior work ofself-supervised dataset distillation -- by utilizing predeterminedaugmentations; 3) we further leverage a lightweight network to model theconnections among the representations of augmented views from the same image,leading to more compact pairs of distillation. Extensive experiments conductedon various datasets validate the superiority of our approach in terms ofdistillation efficiency, cross-architecture generalization, and transferlearning performance.</description>
      <author>example@mail.com (Sheng-Feng Yu, Jia-Jiun Yao, Wei-Chen Chiu)</author>
      <guid isPermaLink="false">2507.21455v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers</title>
      <link>http://arxiv.org/abs/2507.21364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a camera-ready paper at Deep Learning Indaba 2025  (Kigali, Rwanda)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了不同深度学习模型在非洲野生动物图像分类上的性能，发现Vision Transformer ViT-H/14准确率最高(99%)但计算成本高，而DenseNet-201在卷积网络中表现最佳(67%)，并被成功部署用于野外保护工作。&lt;h4&gt;背景&lt;/h4&gt;非洲野生动物种群面临严重威胁，过去五十年脊椎动物数量减少了65%以上。&lt;h4&gt;目的&lt;/h4&gt;研究使用深度学习进行图像分类作为生物多样性监测和保护的工具，比较不同深度学习模型的性能并探索其实际应用。&lt;h4&gt;方法&lt;/h4&gt;使用包含水牛、大象、犀牛和斑马四种动物的公共数据集，评估DenseNet-201、ResNet-152、EfficientNet-B4和Vision Transformer ViT-H/14四种深度学习模型的性能，重点关注使用冻结特征提取器的迁移学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet-201在卷积网络中表现最佳(67%准确率)，ViT-H/14达到最高整体准确率(99%)但计算成本显著更高；实验突显了准确性、资源需求和可部署性之间的权衡。&lt;h4&gt;结论&lt;/h4&gt;将表现最佳的CNN(DenseNet-201)集成到Hugging Face Gradio Space中用于实时野外使用，证明了在保护环境中部署轻量级模型的可行性，为以非洲为基础的AI研究和野生动物保护提供了实用见解。&lt;h4&gt;翻译&lt;/h4&gt;非洲野生动物种群面临严重威胁，过去五十年脊椎动物数量减少了65%以上。作为回应，使用深度学习的图像分类已成为生物多样性监测和保护的有力工具。本文提出了用于自动分类非洲野生动物图像的深度学习模型的比较研究，专注于使用冻结特征提取器的迁移学习。使用包含水牛、大象、犀牛和斑马四种动物的公共数据集，我们评估了DenseNet-201、ResNet-152、EfficientNet-B4和Vision Transformer ViT-H/14的性能。DenseNet-201在卷积网络中取得了最佳性能(67%准确率)，而ViT-H/14达到了最高的整体准确率(99%)，但计算成本显著更高，引发了部署问题。我们的实验突显了准确性、资源需求和可部署性之间的权衡。表现最佳的CNN(DenseNet-201)被集成到Hugging Face Gradio Space中用于实时野外使用，证明了在保护环境中部署轻量级模型的可行性。这项工作通过提供模型选择、数据集准备和深度学习工具负责任部署的实际见解，为以非洲为基础的AI研究做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wildlife populations in Africa face severe threats, with vertebrate numbersdeclining by over 65% in the past five decades. In response, imageclassification using deep learning has emerged as a promising tool forbiodiversity monitoring and conservation. This paper presents a comparativestudy of deep learning models for automatically classifying African wildlifeimages, focusing on transfer learning with frozen feature extractors. Using apublic dataset of four species: buffalo, elephant, rhinoceros, and zebra; weevaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, andVision Transformer ViT-H/14. DenseNet-201 achieved the best performance amongconvolutional networks (67% accuracy), while ViT-H/14 achieved the highestoverall accuracy (99%), but with significantly higher computational cost,raising deployment concerns. Our experiments highlight the trade-offs betweenaccuracy, resource requirements, and deployability. The best-performing CNN(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-timefield use, demonstrating the feasibility of deploying lightweight models inconservation settings. This work contributes to African-grounded AI research byoffering practical insights into model selection, dataset preparation, andresponsible deployment of deep learning tools for wildlife conservation.</description>
      <author>example@mail.com (Lukman Jibril Aliyu, Umar Sani Muhammad, Bilqisu Ismail, Nasiru Muhammad, Almustapha A Wakili, Seid Muhie Yimam, Shamsuddeen Hassan Muhammad, Mustapha Abdullahi)</author>
      <guid isPermaLink="false">2507.21364v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>A Deep Learning Automatic Speech Recognition Model for Shona Language</title>
      <link>http://arxiv.org/abs/2507.21331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种基于深度学习的绍纳语自动语音识别系统的开发，绍纳语是一种具有独特声调和语法复杂性的低资源语言。研究旨在解决训练数据有限、标记数据缺乏以及绍纳语音调细微变化等挑战，并取得了比传统统计模型更好的识别准确率。&lt;h4&gt;背景&lt;/h4&gt;绍纳语是一种低资源语言，具有独特的声调和语法复杂性。对于这类语言，传统统计模型在自动语音识别方面面临诸多挑战，包括有限的训练数据、缺乏标记数据以及复杂的声调变化等。&lt;h4&gt;目的&lt;/h4&gt;探索使用深度学习开发准确的绍纳语ASR系统的可行性；调查设计和实施绍纳语语音识别深度学习架构的具体挑战并提出缓解策略；比较基于深度学习的模型与现有统计模型在准确性方面的性能。&lt;h4&gt;方法&lt;/h4&gt;采用混合架构，包括用于声学建模的卷积神经网络和用于语言建模的长短期记忆网络；采用数据增强技术和迁移学习来克服数据稀缺问题；结合注意力机制以适应绍纳语的声调特性。&lt;h4&gt;主要发现&lt;/h4&gt;开发出的ASR系统取得了显著成果：词错误率为29%，音素错误率为12%，总体准确率为74%。这些指标表明深度学习有潜力提高低资源语言如绍纳语的ASR准确率。&lt;h4&gt;结论&lt;/h4&gt;这项研究促进了绍纳语等低资源语言的ASR技术发展，最终提高了全球绍纳语使用者的可访问性和交流能力。&lt;h4&gt;翻译&lt;/h4&gt;这项研究介绍了一种基于深度学习的绍纳语自动语音识别系统的开发，绍纳语是一种具有独特声调和语法复杂性的低资源语言。研究旨在解决有限训练数据、缺乏标记数据以及绍纳语音调细微变化带来的挑战，目标是相比传统统计模型实现识别准确率的显著提高。研究首先探讨了使用深度学习开发准确的绍纳语ASR系统的可行性。其次，它调查了设计和实施绍纳语语音识别深度学习架构所涉及的具体挑战，并提出了缓解这些挑战的策略。最后，它在准确性方面比较了基于深度学习的模型与现有统计模型的性能。开发的ASR系统采用了混合架构，包括用于声学建模的卷积神经网络和用于语言建模的长短期记忆网络。为了克服数据稀缺问题，采用了数据增强技术和迁移学习。还结合了注意力机制以适应绍纳语的声调特性。最终开发的ASR系统取得了令人印象深刻的结果，词错误率为29%，音素错误率为12%，总体准确率为74%。这些指标表明深度学习有潜力提高绍纳语等低资源语言的ASR准确率。这项研究促进了绍纳语等低资源语言的ASR技术发展，最终提高了全球绍纳语使用者的可访问性和交流能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.15680/IJIRCCE.2024.1206001&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presented the development of a deep learning-based AutomaticSpeech Recognition system for Shona, a low-resource language characterized byunique tonal and grammatical complexities. The research aimed to address thechallenges posed by limited training data, lack of labelled data, and theintricate tonal nuances present in Shona speech, with the objective ofachieving significant improvements in recognition accuracy compared totraditional statistical models. The research first explored the feasibility ofusing deep learning to develop an accurate ASR system for Shona. Second, itinvestigated the specific challenges involved in designing and implementingdeep learning architectures for Shona speech recognition and proposedstrategies to mitigate these challenges. Lastly, it compared the performance ofthe deep learning-based model with existing statistical models in terms ofaccuracy. The developed ASR system utilized a hybrid architecture consisting ofa Convolutional Neural Network for acoustic modelling and a Long Short-TermMemory network for language modelling. To overcome the scarcity of data, dataaugmentation techniques and transfer learning were employed. Attentionmechanisms were also incorporated to accommodate the tonal nature of Shonaspeech. The resulting ASR system achieved impressive results, with a Word ErrorRate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. Thesemetrics indicated the potential of deep learning to enhance ASR accuracy forunder-resourced languages like Shona. This study contributed to the advancementof ASR technology for under-resourced languages like Shona, ultimatelyfostering improved accessibility and communication for Shona speakersworldwide.</description>
      <author>example@mail.com (Leslie Wellington Sirora, Mainford Mutandavari)</author>
      <guid isPermaLink="false">2507.21331v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Addressing High Class Imbalance in Multi-Class Diabetic Retinopathy Severity Grading with Augmentation and Transfer Learning</title>
      <link>http://arxiv.org/abs/2507.17121v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 1 Figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于糖尿病视网膜病变(DR)自动诊断的稳健深度学习框架，在二分类和五分类任务上均取得了优异性能，为DR筛查提供了可扩展且准确的解决方案。&lt;h4&gt;背景&lt;/h4&gt;糖尿病视网膜病变是全球视力丧失的主要原因之一，通过自动视网膜图像分析进行早期诊断可以显著降低失明风险。&lt;h4&gt;目的&lt;/h4&gt;开发一个稳健的深度学习框架，用于二分类和五分类的糖尿病视网膜病变分类，解决类别不平衡和训练数据有限的问题。&lt;h4&gt;方法&lt;/h4&gt;利用迁移学习和大量数据增强技术，在APOTES 2019数据集上评估了多种预训练卷积神经网络架构，包括ResNet和EfficientNet的变体。&lt;h4&gt;主要发现&lt;/h4&gt;二分类任务达到准确率98.9%、精确率98.6%、召回率99.3%、F1分数98.9%和AUC 99.4%；五分类任务获得准确率84.6%和AUC 94.1%，优于现有方法；EfficientNet-B0和ResNet34在准确率和计算效率间提供最佳权衡。&lt;h4&gt;结论&lt;/h4&gt;将类别平衡增强与迁移学习相结合对于高性能DR诊断非常有效，所提框架有潜力在现实临床环境中部署。&lt;h4&gt;翻译&lt;/h4&gt;糖尿病视网膜病变(DR)是全球视力丧失的主要原因之一，通过自动视网膜图像分析进行早期诊断可以显著降低失明风险。本文提出了一种用于二分类和五分类DR分类的稳健深度学习框架，利用迁移学习和大量数据增强来解决类别不平衡和训练数据有限的问题。我们在APOTES 2019数据集上评估了一系列预训练卷积神经网络架构，包括ResNet和EfficientNet的变体。对于二分类，我们的模型达到了最先进的准确率98.9%，精确率98.6%，召回率99.3%，F1分数98.9%和AUC 99.4%。在更具挑战性的五分类严重程度分类任务中，我们的模型获得了有竞争力的准确率84.6%和AUC 94.1%，优于几种现有方法。我们的研究还表明，EfficientNet-B0和ResNet34在两个任务中提供了准确率和计算效率之间的最佳权衡。这些结果证明了将类别平衡增强与迁移学习相结合用于高性能DR诊断的有效性。所提出的框架为DR筛查提供了可扩展且准确的解决方案，有潜力在现实临床环境中部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, andearly diagnosis through automated retinal image analysis can significantlyreduce the risk of blindness. This paper presents a robust deep learningframework for both binary and five-class DR classification, leveraging transferlearning and extensive data augmentation to address the challenges of classimbalance and limited training data. We evaluate a range of pretrainedconvolutional neural network architectures, including variants of ResNet andEfficientNet, on the APTOS 2019 dataset.  For binary classification, our proposed model achieves a state-of-the-artaccuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of98.9%, and an AUC of 99.4%. In the more challenging five-class severityclassification task, our model obtains a competitive accuracy of 84.6% and anAUC of 94.1%, outperforming several existing approaches. Our findings alsodemonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs betweenaccuracy and computational efficiency across both tasks.  These results underscore the effectiveness of combining class-balancedaugmentation with transfer learning for high-performance DR diagnosis. Theproposed framework provides a scalable and accurate solution for DR screening,with potential for deployment in real-world clinical environments.</description>
      <author>example@mail.com (Faisal Ahmed)</author>
      <guid isPermaLink="false">2507.17121v2</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2507.21136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于独立性标准的无监督和监督降维方法，在线性和非线性设置中均表现出优越的性能，为可解释机器学习开辟了新方向。&lt;h4&gt;背景&lt;/h4&gt;无监督和监督学习方法通常使用核函数捕捉数据中的非线性，但专家需确保这些非线性最大化变异性并捕捉数据多样性。&lt;h4&gt;目的&lt;/h4&gt;设计新的无监督和监督降维方法，提高方法的对比度、准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;回顾所有独立性标准，提出3个新的独立性标准，并基于这些标准设计无监督和监督降维方法。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在对比度、准确性和可解释性方面均优于基线方法（tSNE、PCA、正则化LDA、带监督/无监督学习器和层共享的VAE）。&lt;h4&gt;结论&lt;/h4&gt;基于独立性标准的方法为可解释机器学习开辟了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;无监督和监督学习方法传统上使用核函数来捕捉数据结构中固有的非线性。然而，专家必须确保他们提出的非线性最大化变异性并捕捉数据的固有多样性。我们回顾了所有独立性标准来设计无监督学习器。然后我们提出了3个独立性标准，并使用它们来设计无监督和监督降维方法。我们在线性和神经网络非线性设置中评估了这些方法的对比度、准确性和可解释性。结果表明，这些方法优于基线（tSNE、PCA、正则化LDA、带监督/无监督学习器和层共享的VAE），并为研究人员开辟了可解释机器学习（ML）的新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised and supervised learning methods conventionally use kernels tocapture nonlinearities inherent in data structure. However experts have toensure their proposed nonlinearity maximizes variability and capture inherentdiversity of data. We reviewed all independence criteria to design unsupervisedlearners. Then we proposed 3 independence criteria and used them to designunsupervised and supervised dimensionality reduction methods. We evaluatedcontrast, accuracy and interpretability of these methods in both linear andneural nonlinear settings. The results show that the methods have outperformedthe baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner andlayer sharing) and opened a new line of interpretable machine learning (ML) forthe researchers.</description>
      <author>example@mail.com (Mojtaba Moattari)</author>
      <guid isPermaLink="false">2507.21136v1</guid>
      <pubDate>Wed, 30 Jul 2025 14:48:47 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years</title>
      <link>http://arxiv.org/abs/2503.21211v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PTSTnet的新型可解释模型，用于提高厄尔尼诺-南方涛动(ENSO)的长期预测能力，结合动力学过程和跨尺度时空学习，实现了超过24个月提前期的准确预测。&lt;h4&gt;背景&lt;/h4&gt;ENSO对全球气候和社会有重大影响，但超过一年的实时预测仍具挑战性。现有动力学模型存在较大偏差和不确定性，而深度学习方法在可解释性和多尺度动力学处理方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提高ENSO长期预测准确性的可解释模型，解决现有方法的局限性，并深入理解海洋-大气相互作用的物理机制。&lt;h4&gt;方法&lt;/h4&gt;提出PTSTnet模型，这是一种创新的神经网络框架，具有物理编码学习功能，统一了动力学过程和跨尺度时空学习。该模型从稀疏数据中学习具有物理一致性的特征表示，解决海洋-大气过程中的多尺度多物理挑战。&lt;h4&gt;主要发现&lt;/h4&gt;PTSTnet能够产生显著优于现有最先进基准的可解释预测，提前期超过24个月，同时提供了关于海洋-大气相互作用中误差传播的物理见解。&lt;h4&gt;结论&lt;/h4&gt;PTSTnet的成功实现代表了在创新神经海洋建模的可解释见解方面的重要进展，通过固有地增强长期预测技能，有效解决了海洋-大气过程中的固有挑战。&lt;h4&gt;翻译&lt;/h4&gt;厄尔尼诺-南方涛动(ENSO)对全球气候和社会产生影响，但提前期超过一年的实时预测仍然具有挑战性。动力学模型存在较大的偏差和不确定性，而深度学习在可解释性和多尺度动力学方面存在困难。在此，我们介绍了PTSTnet，一种可解释模型，它通过创新的神经网络框架和物理编码学习，统一了动力学过程和跨尺度时空学习。PTSTnet产生的可解释预测显著优于现有最先进的基准，提前期超过24个月，为海洋-大气相互作用中的误差传播提供了物理见解。PTSTnet从稀疏数据中学习具有物理一致性的特征表示，以解决海洋-大气过程固有的多尺度多物理挑战，从而固有地增强了长期预测技能。我们成功的实现标志着在创新神经海洋建模的可解释见解方面迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; El Ni\~no-Southern Oscillation (ENSO) exerts global climate and societalimpacts, but real-time prediction with lead times beyond one year remainschallenging. Dynamical models suffer from large biases and uncertainties, whiledeep learning struggles with interpretability and multi-scale dynamics. Here,we introduce PTSTnet, an interpretable model that unifies dynamical processesand cross-scale spatiotemporal learning in an innovative neural-networkframework with physics-encoding learning. PTSTnet produces interpretablepredictions significantly outperforming state-of-the-art benchmarks with leadtimes beyond 24 months, providing physical insights into error propagation inocean-atmosphere interactions. PTSTnet learns feature representations withphysical consistency from sparse data to tackle inherent multi-scale andmulti-physics challenges underlying ocean-atmosphere processes, therebyinherently enhancing long-term prediction skill. Our successful realizationsmark substantial steps forward in interpretable insights into innovative neuralocean modelling.</description>
      <author>example@mail.com (Rixu Hao, Yuxin Zhao, Shaoqing Zhang, Guihua Wang, Xiong Deng)</author>
      <guid isPermaLink="false">2503.21211v2</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
  <item>
      <title>Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2507.21049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 (Highlight). Project page:  https://jacky1128.github.io/RepMTL/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Rep-MTL方法，通过表征级任务显著性量化任务交互，利用熵惩罚和跨任务对齐减少负迁移并促进互补信息共享。&lt;h4&gt;背景&lt;/h4&gt;现有多任务优化技术主要关注通过优化器策略解决冲突，但未能带来一致改进；共享表征空间包含未被充分利用的丰富信息和潜力。&lt;h4&gt;目的&lt;/h4&gt;探索共享表征空间在促进任务间互补性方面的潜力，开发新的多任务学习方法来减少负迁移并促进互补信息共享。&lt;h4&gt;方法&lt;/h4&gt;Rep-MTL利用表征级任务显著性量化特定任务优化与共享表征学习间的交互，通过基于熵的惩罚和样本级跨任务对齐来引导这些显著性。&lt;h4&gt;主要发现&lt;/h4&gt;Rep-MTL在四个具有挑战性的多任务学习基准上实现了有竞争力的性能提升和良好效率；幂律指数分析证明了其在平衡特定任务学习和跨任务共享方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;共享表征空间为多任务学习提供了丰富的信息和操作潜力，Rep-MTL通过表征级操作实现了优于传统优化方法的效果。&lt;h4&gt;翻译&lt;/h4&gt;尽管多任务学习在利用任务间互补知识方面很有前景，但现有的多任务优化技术仍然专注于通过基于优化器的损失缩放和梯度操作策略来解决冲突，却未能带来一致的改进。在本文中，我们认为共享表征空间（任务交互自然发生的地方）提供了丰富的信息和潜力，可以进行与现有优化器互补的操作，特别是促进任务间互补性，这在多任务优化中很少被探索。这一直觉导致了Rep-MTL的诞生，它利用表征级的任务显著性来量化特定任务优化与共享表征学习之间的交互。通过基于熵的惩罚和样本级的跨任务对齐来引导这些显著性，Rep-MTL旨在通过保持单个任务的有效训练而非纯粹解决冲突来减少负迁移，同时明确促进互补信息共享。在涵盖任务转移和领域转移场景的四个具有挑战性的多任务学习基准上进行了实验。结果表明，即使与基本的等权重策略配对，Rep-MTL也能实现有竞争力的性能提升和良好的效率。除了标准性能指标外，幂律指数分析证明了Rep-MTL在平衡特定任务学习和跨任务共享方面的有效性。项目页面可在HERE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the promise of Multi-Task Learning in leveraging complementaryknowledge across tasks, existing multi-task optimization (MTO) techniquesremain fixated on resolving conflicts via optimizer-centric loss scaling andgradient manipulation strategies, yet fail to deliver consistent gains. In thispaper, we argue that the shared representation space, where task interactionsnaturally occur, offers rich information and potential for operationscomplementary to existing optimizers, especially for facilitating theinter-task complementarity, which is rarely explored in MTO. This intuitionleads to Rep-MTL, which exploits the representation-level task saliency toquantify interactions between task-specific optimization and sharedrepresentation learning. By steering these saliencies through entropy-basedpenalization and sample-wise cross-task alignment, Rep-MTL aims to mitigatenegative transfer by maintaining the effective training of individual tasksinstead pure conflict-solving, while explicitly promoting complementaryinformation sharing. Experiments are conducted on four challenging MTLbenchmarks covering both task-shift and domain-shift scenarios. The resultsshow that Rep-MTL, even paired with the basic equal weighting policy, achievescompetitive performance gains with favorable efficiency. Beyond standardperformance metrics, Power Law exponent analysis demonstrates Rep-MTL'sefficacy in balancing task-specific learning and cross-task sharing. Theproject page is available at HERE.</description>
      <author>example@mail.com (Zedong Wang, Siyuan Li, Dan Xu)</author>
      <guid isPermaLink="false">2507.21049v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Learning Transferable Facial Emotion Representations from Large-Scale Semantically Rich Captions</title>
      <link>http://arxiv.org/abs/2507.21015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于语义丰富自然语言标题的面部情感表征学习方法，解决了传统情感识别系统的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前面部情感识别系统主要训练用于预测预定义类别或抽象维度值，这种有限的监督形式限制了泛化能力和适用性，无法充分表达情感的丰富细微差别。&lt;h4&gt;目的&lt;/h4&gt;探索利用语义丰富的自然语言标题作为监督信号来学习面部情感表征，解决缺乏大规模情感语义标题数据集和有效利用此类监督的框架问题。&lt;h4&gt;方法&lt;/h4&gt;引入EmoCap100K数据集（包含10万+样本，具有丰富和结构化的语义描述），提出EmoCapCLIP方法，结合全局-局部对比学习框架和跨模态引导的正样本挖掘模块，充分利用多级标题信息并处理相关表达间的语义相似性。&lt;h4&gt;主要发现&lt;/h4&gt;在涵盖5个任务的20多个基准测试上，所提方法表现出优越的性能，证明了从大规模语义丰富的标题中学习面部情感表征的潜力。&lt;h4&gt;结论&lt;/h4&gt;自然语言提供的更灵活、更具表现力和可解释性的情感表达方式可以作为面部情感识别的有效监督信号，EmoCapCLIP框架能够有效利用这种丰富的监督信息。&lt;h4&gt;翻译&lt;/h4&gt;当前面部情感识别系统主要被训练来预测预定义的类别或抽象的维度值。这种有限的监督形式限制了泛化能力和适用性，因为它将丰富而细微的情感光谱简化为过度简化的标签或量表。相比之下，自然语言提供了更灵活、更具表现力和可解释性的情感表达方式，提供了更广泛的监督来源。然而，利用语义丰富的自然语言标题作为面部情感表征学习的监督信号仍然相对未被探索，主要由于两个关键挑战：1)缺乏大规模具有丰富情感语义的标题数据集，以及2)缺乏专门针对利用这种丰富监督的有效框架。为此，我们引入了EmoCap100K，这是一个包含超过10万个样本的大规模面部情感标题数据集，具有丰富和结构化的语义描述，能够捕捉全局情感状态和细粒度的局部面部行为。基于此数据集，我们进一步提出了EmoCapCLIP，它结合了全局-局部对比学习框架，并增强了跨模态引导的正样本挖掘模块。这种设计促进了多级标题信息的全面利用，同时适应了相关表达之间的语义相似性。在涵盖5个任务的20多个基准测试上的广泛评估证明了我们方法的优越性能，突显了从大规模语义丰富的标题中学习面部情感表征的前景。代码和数据将在https://github.com/sunlicai/EmoCapCLIP上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current facial emotion recognition systems are predominately trained topredict a fixed set of predefined categories or abstract dimensional values.This constrained form of supervision hinders generalization and applicability,as it reduces the rich and nuanced spectrum of emotions into oversimplifiedlabels or scales. In contrast, natural language provides a more flexible,expressive, and interpretable way to represent emotions, offering a muchbroader source of supervision. Yet, leveraging semantically rich naturallanguage captions as supervisory signals for facial emotion representationlearning remains relatively underexplored, primarily due to two key challenges:1) the lack of large-scale caption datasets with rich emotional semantics, and2) the absence of effective frameworks tailored to harness such richsupervision. To this end, we introduce EmoCap100K, a large-scale facial emotioncaption dataset comprising over 100,000 samples, featuring rich and structuredsemantic descriptions that capture both global affective states andfine-grained local facial behaviors. Building upon this dataset, we furtherpropose EmoCapCLIP, which incorporates a joint global-local contrastivelearning framework enhanced by a cross-modal guided positive mining module.This design facilitates the comprehensive exploitation of multi-level captioninformation while accommodating semantic similarities between closely relatedexpressions. Extensive evaluations on over 20 benchmarks covering five tasksdemonstrate the superior performance of our method, highlighting the promise oflearning facial emotion representations from large-scale semantically richcaptions. The code and data will be available athttps://github.com/sunlicai/EmoCapCLIP.</description>
      <author>example@mail.com (Licai Sun, Xingxun Jiang, Haoyu Chen, Yante Li, Zheng Lian, Biu Liu, Yuan Zong, Wenming Zheng, Jukka M. Leppänen, Guoying Zhao)</author>
      <guid isPermaLink="false">2507.21015v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering</title>
      <link>http://arxiv.org/abs/2507.20980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为LargeMvC-Net的新型深度网络架构，用于大规模基于锚点的多视图聚类。该模型通过将优化过程分解为三个专门模块，解决了现有方法中锚点结构整合的不足，并在多个基准测试中展示了优越的性能。&lt;h4&gt;背景&lt;/h4&gt;基于深度锚点的多视图聚类方法通过利用代表性锚点降低大规模聚类的计算复杂度，提高神经网络扩展性。然而，现有方法通常采用启发式或任务无关的方式整合锚点结构，忽视了基于锚点的聚类的核心结构需求和关键优化原则。&lt;h4&gt;目的&lt;/h4&gt;重新审视大规模基于锚点的多视图聚类的底层优化问题，并将其迭代解展开为一种新颖的深度网络架构，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出的LargeMvC-Net模型将基于锚点的聚类过程分解为三个模块：RepresentModule（表示学习）、NoiseModule（噪声抑制）和AnchorModule（锚点指标估计）。每个模块通过将原始优化过程的一个步骤展开为专用网络组件来推导，并提供无监督重构损失以鼓励跨视图的聚类结构一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在几个大规模多视图基准上的大量实验表明，LargeMvC-Net在有效性和扩展性方面都始终优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;LargeMvC-Net通过重新思考锚点聚制的优化问题，并将其展开为深度网络架构，解决了现有方法的局限性，提供了更好的聚类性能和扩展性。&lt;h4&gt;翻译&lt;/h4&gt;基于深度锚点的多视图聚类方法通过利用代表性锚点来降低大规模聚类的计算复杂度，从而提高神经网络的扩展性。尽管它们具有扩展性优势，但现有方法通常以启发式或任务无关的方式整合锚点结构，通过事后图构建或作为消息传递的辅助组件。此类设计忽略了基于锚点的聚类的核心结构需求，忽视了关键的优化原则。为了填补这一空白，我们重新审视了大规模基于锚点的多视图聚类的底层优化问题，并将其迭代解展开为一种新颖的深度网络架构，称为LargeMvC-Net。所提出的模型将基于锚点的聚类过程分解为三个模块：RepresentModule、NoiseModule和AnchorModule，分别对应表示学习、噪声抑制和锚点指标估计。每个模块都是通过将原始优化过程的一个步骤展开为专用的网络组件来推导的，从而提供结构清晰性和优化可追溯性。此外，一种无监督的重构损失使每个视图与锚点诱导的潜在空间保持一致，从而鼓励跨视图的聚类结构一致性。在几个大规模多视图基准上的大量实验表明，LargeMvC-Net在有效性和扩展性方面都始终优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep anchor-based multi-view clustering methods enhance the scalability ofneural networks by utilizing representative anchors to reduce the computationalcomplexity of large-scale clustering. Despite their scalability advantages,existing approaches often incorporate anchor structures in a heuristic ortask-agnostic manner, either through post-hoc graph construction or asauxiliary components for message passing. Such designs overlook the corestructural demands of anchor-based clustering, neglecting key optimizationprinciples. To bridge this gap, we revisit the underlying optimization problemof large-scale anchor-based multi-view clustering and unfold its iterativesolution into a novel deep network architecture, termed LargeMvC-Net. Theproposed model decomposes the anchor-based clustering process into threemodules: RepresentModule, NoiseModule, and AnchorModule, corresponding torepresentation learning, noise suppression, and anchor indicator estimation.Each module is derived by unfolding a step of the original optimizationprocedure into a dedicated network component, providing structural clarity andoptimization traceability. In addition, an unsupervised reconstruction lossaligns each view with the anchor-induced latent space, encouraging consistentclustering structures across views. Extensive experiments on severallarge-scale multi-view benchmarks show that LargeMvC-Net consistentlyoutperforms state-of-the-art methods in terms of both effectiveness andscalability.</description>
      <author>example@mail.com (Shide Du, Chunming Wu, Zihan Fang, Wendi Zhao, Yilin Wu, Changwei Wang, Shiping Wang)</author>
      <guid isPermaLink="false">2507.20980v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction</title>
      <link>http://arxiv.org/abs/2507.20925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的零样本化合物-蛋白质相互作用预测方法，通过子序列重新排序预训练蛋白质表示，并应用可变长度蛋白质增强技术，有效解决了现有方法面临的挑战，特别是在零样本和数据稀缺场景下表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;化学空间的广阔性和未表征蛋白质的不断出现，使得零样本化合物-蛋白质相互作用预测更好地反映了实际药物开发的挑战和要求。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在CPI任务中面临的两个主要挑战：一是蛋白质序列表示学习忽视了子序列间的复杂依赖关系；二是对大规模或稀缺多模态蛋白质数据集的依赖限制了可扩展性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通过子序列重新排序预训练蛋白质表示的方法，以明确捕获蛋白质子序列之间的依赖关系；并应用可变长度的蛋白质增强技术，确保在小训练数据集上获得优秀的预训练性能。&lt;h4&gt;主要发现&lt;/h4&gt;将该方法与各种基线方法结合，结果表明该方法可以提高基线模型在CPI任务上的性能，特别是在零样本场景下；与现有预训练模型相比，该模型表现出优越的性能，尤其是在训练样本有限的数据稀缺场景中。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了现有CPI预测方法面临的挑战，特别是在零样本和数据稀缺场景下表现优异，为实际药物开发提供了更有效的预测工具。&lt;h4&gt;翻译&lt;/h4&gt;鉴于化学空间的广阔性和先前未表征蛋白质的不断出现，零样本化合物-蛋白质相互作用预测更好地反映了实际药物开发的挑战和要求。尽管现有方法在某些CPI任务中表现良好，但仍面临以下挑战：(1)从局部或完整蛋白质序列的表示学习常常忽略了子序列之间的复杂依赖关系，而这些依赖关系对于预测空间结构和结合特性至关重要。(2)对大规模或稀缺多模态蛋白质数据集的依赖需要大量训练数据和计算资源，限制了可扩展性和效率。为解决这些挑战，我们提出了一种新颖的方法，通过子序列重新排序为CPI预测任务预训练蛋白质表示，明确捕获蛋白质子序列之间的依赖关系。此外，我们应用可变长度蛋白质增强技术，确保在小训练数据集上获得出色的预训练性能。为评估模型的有效性和零样本学习能力，我们将其与各种基线方法结合。结果表明，我们的方法可以提高基线模型在CPI任务上的性能，特别是在具有挑战性的零样本场景下。与现有预训练模型相比，我们的模型表现出优越的性能，尤其是在训练样本有限的数据稀缺场景中。我们的实现可在https://github.com/Hoch-Zhang/PSRP-CPI获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the vastness of chemical space and the ongoing emergence of previouslyuncharacterized proteins, zero-shot compound-protein interaction (CPI)prediction better reflects the practical challenges and requirements ofreal-world drug development. Although existing methods perform adequatelyduring certain CPI tasks, they still face the following challenges: (1)Representation learning from local or complete protein sequences oftenoverlooks the complex interdependencies between subsequences, which areessential for predicting spatial structures and binding properties. (2)Dependence on large-scale or scarce multimodal protein datasets demandssignificant training data and computational resources, limiting scalability andefficiency. To address these challenges, we propose a novel approach thatpretrains protein representations for CPI prediction tasks using subsequencereordering, explicitly capturing the dependencies between protein subsequences.Furthermore, we apply length-variable protein augmentation to ensure excellentpretraining performance on small training datasets. To evaluate the model'seffectiveness and zero-shot learning ability, we combine it with variousbaseline methods. The results demonstrate that our approach can improve thebaseline model's performance on the CPI task, especially in the challengingzero-shot scenario. Compared to existing pre-training models, our modeldemonstrates superior performance, particularly in data-scarce scenarios wheretraining samples are limited. Our implementation is available athttps://github.com/Hoch-Zhang/PSRP-CPI.</description>
      <author>example@mail.com (Hongzhi Zhang, Zhonglie Liu, Kun Meng, Jiameng Chen, Jia Wu, Bo Du, Di Lin, Yan Che, Wenbin Hu)</author>
      <guid isPermaLink="false">2507.20925v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Ensemble Foreground Management for Unsupervised Object Discovery</title>
      <link>http://arxiv.org/abs/2507.20860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV2025 (Highlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UnionCut的新方法，解决了无监督目标发现中的两个主要挑战：确定发现区域是否为前景以及确定何时停止发现过程。UnionCut基于最小割和集成方法提供鲁棒的前景先验，并进一步提出了更高效的UnionSeg模型。&lt;h4&gt;背景&lt;/h4&gt;无监督目标发现（UOD）旨在无需手工标注的情况下检测和分割2D图像中的对象。自监督表征学习的最新进展促进了UOD算法的发展，但缺乏真实标签给现有方法带来两个挑战：确定发现区域是前景还是背景，以及知道还有多少未被发现的对象。&lt;h4&gt;目的&lt;/h4&gt;解决UOD中的两个主要挑战：1) 确定发现区域是前景还是背景；2) 确定何时停止发现过程，避免欠分割或过分割问题。&lt;h4&gt;方法&lt;/h4&gt;提出UnionCut，一种基于最小割和集成方法的鲁棒前景先验，能够检测图像中前景区域的并集；并进一步提出UnionSeg，作为UnionCut的蒸馏变压器，更高效准确地输出前景并集。&lt;h4&gt;主要发现&lt;/h4&gt;通过将UnionCut或UnionSeg与现有最先进的UOD方法结合，在各种基准测试中提高了单目标发现、显著性检测和自监督实例分割的性能。&lt;h4&gt;结论&lt;/h4&gt;UnionCut和UnionSeg为UOD提供了更鲁棒和高效的前景先验，解决了现有方法中前景判断不准确和发现迭代次数固定导致的问题。&lt;h4&gt;翻译&lt;/h4&gt;无监督目标发现（UOD）旨在无需手工标注的情况下检测和分割2D图像中的对象。最近自监督表征学习的进展使UOD算法取得了一些成功。然而，缺乏真实标签给现有UOD方法带来了两个挑战：1) 确定发现的区域是前景还是背景，以及2) 知道还有多少对象尚未被发现。为解决这两个问题，之前的解决方案依赖前景先验来区分发现区域是否为前景，并进行一次或固定次数的发现迭代。然而，现有的前景先验是启发式的且并不总是鲁棒的，而固定数量的发现会导致欠分割或过分割，因为图像中对象的数量各不相同。本文介绍了UnionCut，一种基于最小割和集成方法的鲁棒且有充分依据的前景先验，它能够检测图像前景区域的并集，使UOD算法能够识别前景对象并在分割图像中大部分前景并集后停止发现。此外，我们提出了UnionSeg，作为UnionCut的蒸馏变压器，能更高效准确地输出前景并集。我们的实验表明，通过结合UnionCut或UnionSeg，现有的最先进UOD方法在各种基准测试中的单目标发现、显著性检测和自监督实例分割性能都有所提高。代码可在https://github.com/YFaris/UnionCut获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised object discovery (UOD) aims to detect and segment objects in 2Dimages without handcrafted annotations. Recent progress in self-supervisedrepresentation learning has led to some success in UOD algorithms. However, theabsence of ground truth provides existing UOD methods with two challenges: 1)determining if a discovered region is foreground or background, and 2) knowinghow many objects remain undiscovered. To address these two problems, previoussolutions rely on foreground priors to distinguish if the discovered region isforeground, and conduct one or fixed iterations of discovery. However, theexisting foreground priors are heuristic and not always robust, and a fixednumber of discoveries leads to under or over-segmentation, since the number ofobjects in images varies. This paper introduces UnionCut, a robust andwell-grounded foreground prior based on min-cut and ensemble methods thatdetects the union of foreground areas of an image, allowing UOD algorithms toidentify foreground objects and stop discovery once the majority of theforeground union in the image is segmented. In addition, we propose UnionSeg, adistilled transformer of UnionCut that outputs the foreground union moreefficiently and accurately. Our experiments show that by combining withUnionCut or UnionSeg, previous state-of-the-art UOD methods witness an increasein the performance of single object discovery, saliency detection andself-supervised instance segmentation on various benchmarks. The code isavailable at https://github.com/YFaris/UnionCut.</description>
      <author>example@mail.com (Ziling Wu, Armaghan Moemeni, Praminda Caleb-Solly)</author>
      <guid isPermaLink="false">2507.20860v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Structural-Aware Key Node Identification in Hypergraphs via Representation Learning and Fine-Tuning</title>
      <link>http://arxiv.org/abs/2507.20682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AHGA的新框架，用于在超图中识别关键节点，有效捕捉现实系统中的多元交互关系，实验表明该方法比传统方法性能提升约37.4%。&lt;h4&gt;背景&lt;/h4&gt;评估节点重要性是分析复杂系统的关键环节，在数字营销、谣言抑制和疾病控制等领域有广泛应用。然而，现有方法通常依赖传统网络结构，无法捕捉许多现实系统中固有的多元交互关系。&lt;h4&gt;目的&lt;/h4&gt;研究超图中的关键节点识别问题，解决现有方法无法捕捉多元交互的局限性，提高节点识别的准确性和实用性。&lt;h4&gt;方法&lt;/h4&gt;提出AHGA框架，整合了三个核心组件：用于提取高阶结构特征的自编码器、基于超图神经网络的预训练模块(HGNN)、以及基于主动学习的微调过程。微调步骤有助于弥合合成数据与现实数据差距，增强模型在不同超图拓扑结构上的鲁棒性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在八个经验超图上的大量实验表明，AHGA比经典的基于中心度的基线方法性能提升约37.4%。此外，AHGA识别的节点既具有高影响力，又具有强大的结构破坏能力，在检测多功能节点方面表现出优越性。&lt;h4&gt;结论&lt;/h4&gt;AHGA框架能够有效捕捉超图中的高阶交互关系，显著提升关键节点识别的性能，在实际应用中具有良好的鲁棒性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;评估节点重要性是分析复杂系统的关键方面，在数字营销、谣言抑制和疾病控制等领域有广泛应用。然而，现有方法通常依赖传统网络结构，无法捕捉许多现实系统中固有的多元交互关系。为解决这一局限，我们研究了超图中的关键节点识别问题，其中高阶交互自然地建模为超边。我们提出了一个名为AHGA的新框架，该框架集成了用于提取高阶结构特征的自编码器、基于超图神经网络的预训练模块以及基于主动学习的微调过程。这一微调步骤在弥合合成数据与现实数据差距方面起着重要作用，从而增强模型在不同超图拓扑结构上的鲁棒性和泛化能力。在八个经验超图上的大量实验表明，AHGA比经典的基于中心度的基线方法性能提升约37.4%。此外，AHGA识别的节点既具有高影响力，又具有强大的结构破坏能力，证明了它们在检测多功能节点方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating node importance is a critical aspect of analyzing complex systems,with broad applications in digital marketing, rumor suppression, and diseasecontrol. However, existing methods typically rely on conventional networkstructures and fail to capture the polyadic interactions intrinsic to manyreal-world systems. To address this limitation, we study key nodeidentification in hypergraphs, where higher-order interactions are naturallymodeled as hyperedges. We propose a novel framework, AHGA, which integrates anAutoencoder for extracting higher-order structural features, a HyperGraphneural network-based pre-training module (HGNN), and an Active learning-basedfine-tuning process. This fine-tuning step plays a vital role in mitigating thegap between synthetic and real-world data, thereby enhancing the model'srobustness and generalization across diverse hypergraph topologies. Extensiveexperiments on eight empirical hypergraphs show that AHGA outperforms classicalcentrality-based baselines by approximately 37.4%. Furthermore, the nodesidentified by AHGA exhibit both high influence and strong structural disruptioncapability, demonstrating their superiority in detecting multifunctional nodes.</description>
      <author>example@mail.com (Xiaonan Ni, Guangyuan Mei, Su-Su Zhang, Yang Chen, Xin Xu, Chuang Liu, Xiu-Xiu Zhan)</author>
      <guid isPermaLink="false">2507.20682v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion</title>
      <link>http://arxiv.org/abs/2507.20620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MoCME的多模态知识图谱补全框架，通过互补引导的模态知识融合和熵引导的负采样机制，有效解决了多模态知识图谱中模态分布不均衡的问题，并在五个基准数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态知识图谱补全旨在利用多模态和结构化实体信息揭示隐藏的世界知识，但多模态知识图谱中固有的模态分布不均衡性给利用额外模态数据进行鲁棒实体表示带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有MMKGC方法忽略多模态数据中互补性的问题，提高模型在模态分布不均衡情况下的表现。&lt;h4&gt;方法&lt;/h4&gt;提出MoCME框架，包含互补引导的模态知识融合(CMKF)模块和熵引导的负采样(EGNS)机制。CMKF模块利用模态内和模态间互补性融合多视图和多模态嵌入；EGNS机制动态优先选择信息丰富且不确定的负样本以提高训练效果和模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的实验表明，MoCME方法超越了现有方法，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;MoCME框架有效解决了多模态知识图谱补全中的模态分布不均衡问题，通过充分利用多模态数据的互补性，提高了实体表示的质量和模型的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;多模态知识图谱补全旨在利用多模态和结构化实体信息揭示多模态知识图谱中的隐藏世界知识。然而，多模态知识图谱中固有的不平衡性，其中模态分布在不同实体间存在差异，给利用额外模态数据进行鲁棒实体表示带来了挑战。现有的MMKGC方法通常依赖于注意力机制或基于门控的融合机制，但忽略了多模态数据中包含的互补性。在本文中，我们提出了一种名为互补模态专家混合体(MoCME)的新框架，该框架包含互补引导的模态知识融合(CMKF)模块和熵引导的负采样(EGNS)机制。CMKF模块利用模态内和模态间互补性来融合多视图和多模态嵌入，增强实体表示。此外，我们引入了熵引导的负采样机制，以动态优先选择信息丰富且不确定的负样本，提高训练有效性和模型鲁棒性。在五个基准数据集上的大量实验表明，我们的MoCME达到了最先进的性能，超越了现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden worldknowledge in multimodal knowledge graphs by leveraging both multimodal andstructural entity information. However, the inherent imbalance in multimodalknowledge graphs, where modality distributions vary across entities, poseschallenges in utilizing additional modality data for robust entityrepresentation. Existing MMKGC methods typically rely on attention orgate-based fusion mechanisms but overlook complementarity contained inmulti-modal data. In this paper, we propose a novel framework named Mixture ofComplementary Modality Experts (MoCME), which consists of aComplementarity-guided Modality Knowledge Fusion (CMKF) module and anEntropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploitsboth intra-modal and inter-modal complementarity to fuse multi-view andmulti-modal embeddings, enhancing representations of entities. Additionally, weintroduce an Entropy-guided Negative Sampling mechanism to dynamicallyprioritize informative and uncertain negative samples to enhance trainingeffectiveness and model robustness. Extensive experiments on five benchmarkdatasets demonstrate that our MoCME achieves state-of-the-art performance,surpassing existing approaches.</description>
      <author>example@mail.com (Lijian Li)</author>
      <guid isPermaLink="false">2507.20620v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Distributed Optical Fiber Vibration Recognition via Extreme Lightweight Model and Cross-Domain Distillation</title>
      <link>http://arxiv.org/abs/2507.20587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于FPGA加速的极端轻量级模型和新颖的知识蒸馏框架，解决了分布式光纤振动传感系统在实际部署中的两个主要挑战：动态条件下识别精度下降和海量数据实时处理的计算瓶颈。&lt;h4&gt;背景&lt;/h4&gt;分布式光纤振动传感(DVS)系统在大规模监测和入侵事件识别方面具有前景，但实际部署受到两个主要挑战的限制：动态条件下识别精度下降，以及海量传感数据实时处理的计算瓶颈。&lt;h4&gt;目的&lt;/h4&gt;解决DVS系统在实际部署中面临的动态条件下识别精度下降和实时处理计算瓶颈两大挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于FPGA加速的极端轻量级模型和新颖的知识蒸馏框架，包括一个仅含4141个参数的三层深度可分离卷积网络，以及一种由物理先验引导的跨域蒸馏框架，将频域洞察嵌入到时域模型中，实现时频表征学习而不增加复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;所提模型每样本处理速度达0.019毫秒，可实时处理长达168.68公里的传感光纤；在未见过的环境条件下，识别准确率从51.93%提升至95.72%；缓解了传感范围和实时能力之间的权衡，弥合了理论能力和实际部署需求之间的差距。&lt;h4&gt;结论&lt;/h4&gt;提出的方法为DVS系统及更通用的分布式光纤传感(DOFS)领域提供了关键进展，包括结合可解释信号处理与深度学习的框架，以及实时处理和边缘计算的参考架构，为构建更高效、鲁棒和可解释的DOFS技术人工智能系统指明了新方向。&lt;h4&gt;翻译&lt;/h4&gt;分布式光纤振动传感(DVS)系统为大规模监测和入侵事件识别提供了一种有前景的解决方案。然而，它们的实际部署仍受两个主要挑战的阻碍：动态条件下识别精度下降，以及海量传感数据实时处理的计算瓶颈。本文通过FPGA加速的极端轻量级模型和新提出的知识蒸馏框架，为这些挑战提供了新的解决方案。所提出的三层深度可分离卷积网络仅包含4141个参数，是该领域迄今为止最紧凑的架构，每个样本(覆盖12.5米光纤长度，持续0.256秒)的最大处理速度为0.019毫秒。此性能对应于实时处理长达168.68公里传感光纤的能力。为了提高变化环境下的泛化能力，这里使用了由物理先验引导的跨域蒸馏框架，将频域洞察嵌入到时域模型中。这实现了不增加复杂性的时频表征学习，并在未见环境条件下将识别准确率从51.93%提升到95.72%。所提出的方法提供了关键进展，包括结合可解释信号处理技术与深度学习的框架，以及DVS系统实时处理和边缘计算的参考架构，更通用的分布式光纤传感(DOFS)领域。它缓解了传感范围和实时能力之间的权衡，弥合了理论能力和实际部署需求之间的差距。此外，这项工作为构建更高效、鲁棒和可解释的DOFS技术人工智能系统指明了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributed optical fiber vibration sensing (DVS) systems offer a promisingsolution for large-scale monitoring and intrusion event recognition. However,their practical deployment remains hindered by two major challenges:degradation of recognition accuracy in dynamic conditions, and thecomputational bottleneck of real-time processing for mass sensing data. Thispaper presents a new solution to these challenges, through a FPGA-acceleratedextreme lightweight model along with a newly proposed knowledge distillationframework. The proposed three-layer depthwise separable convolution networkcontains only 4141 parameters, which is the most compact architecture in thisfield to date, and achieves a maximum processing speed of 0.019 ms for eachsample covering a 12.5 m fiber length over 0.256 s. This performancecorresponds to real-time processing capabilities for sensing fibers extendingup to 168.68 km. To improve generalizability under changing environments, theproposed cross-domain distillation framework guided by physical priors is usedhere to embed frequency-domain insights into the time-domain model. This allowsfor time-frequency representation learning without increasing complexity andboosts recognition accuracy from 51.93% to 95.72% under unseen environmentalconditions. The proposed methodology provides key advancements including aframework combining interpretable signal processing technique with deeplearning and a reference architecture for real-time processing andedge-computing in DVS systems, and more general distributed optical fibersensing (DOFS) area. It mitigates the trade-off between sensing range andreal-time capability, bridging the gap between theoretical capabilities andpractical deployment requirements. Furthermore, this work reveals a newdirection for building more efficient, robust and explainable artificialintelligence systems for DOFS technologies.</description>
      <author>example@mail.com (Zhongyao Luo, Hao Wu, Zhao Ge, Ming Tang)</author>
      <guid isPermaLink="false">2507.20587v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Class Tokens: LLM-guided Dominant Property Mining for Few-shot Classification</title>
      <link>http://arxiv.org/abs/2507.20511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BCT-CLIP的新型少样本学习方法，通过探索主导属性而非简单使用类别标记，利用大型语言模型先验知识增强图像表示，在11个数据集上实现了优越性能。&lt;h4&gt;背景&lt;/h4&gt;少样本学习(FSL)因数据稀缺而面临显著挑战。虽然基于对比语言图像预训练的CLIP类方法通过利用类别名称的文本表示缓解了这一问题，但简单地将视觉表示与类别名称嵌入对齐会损害新类别的视觉多样性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的少样本学习方法(BCT-CLIP)，探索主导属性而非仅使用类别标记，利用基于大型语言模型(LLM)的先验知识，推进具有全面结构化图像表示的少样本学习。&lt;h4&gt;方法&lt;/h4&gt;提出多属性生成器(MPG)具有感知补丁的交叉注意力机制来生成多个视觉属性标记；设计基于大型语言模型(LLM)的辅助检索过程，使用基于聚类的剪枝获取主导属性描述；提出新的属性标记对比学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在11个广泛使用的数据集上表现出优越的性能，表明对主导属性的研究促进了判别性类别特定表示学习和少样本分类的发展。&lt;h4&gt;结论&lt;/h4&gt;探索主导属性可以显著提升少样本学习中判别性类别特定表示的学习效果，为少样本分类提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;少样本学习(FSL)致力于仅使用少量图像开发识别新类别的泛化能力，因数据稀缺而面临重大挑战。最近基于对比语言图像预训练的CLIP类方法通过利用类别名称的文本表示来缓解未见图像发现问题。尽管取得了成功，但简单地将视觉表示与类别名称嵌入对齐会损害新类别的视觉多样性鉴别能力。为此，我们提出了一种新型少样本学习方法(BCT-CLIP)，通过对比学习探索主导属性，而不仅仅是使用类别标记。通过利用基于大型语言模型的先验知识，我们的方法推进了具有全面结构化图像表示的少样本学习，包括全局类别表示和感知补丁的属性嵌入。特别是，我们提出了一种新颖的多属性生成器(MPG)，具有感知补丁的交叉注意力机制来生成多个视觉属性标记；一种基于大型语言模型的辅助检索程序，使用基于聚类的剪枝来获取主导属性描述；以及一种用于属性标记学习的新对比学习策略。在11个广泛使用的数据集上的优越性能表明，我们对主导属性的研究促进了判别性类别特定表示学习和少样本分类的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot Learning (FSL), which endeavors to develop the generalizationability for recognizing novel classes using only a few images, facessignificant challenges due to data scarcity. Recent CLIP-like methods based oncontrastive language-image pertaining mitigate the issue by leveraging textualrepresentation of the class name for unseen image discovery. Despite theachieved success, simply aligning visual representations to class nameembeddings would compromise the visual diversity for novel classdiscrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method(BCT-CLIP) that explores \textbf{dominating properties} via contrastivelearning beyond simply using class tokens. Through leveraging LLM-based priorknowledge, our method pushes forward FSL with comprehensive structural imagerepresentations, including both global category representation and thepatch-aware property embeddings. In particular, we presented a novelmulti-property generator (MPG) with patch-aware cross-attentions to generatemultiple visual property tokens, a Large-Language Model (LLM)-assistantretrieval procedure with clustering-based pruning to obtain dominating propertydescriptions, and a new contrastive learning strategy for property-tokenlearning. The superior performances on the 11 widely used datasets demonstratethat our investigation of dominating properties advances discriminativeclass-specific representation learning and few-shot classification.</description>
      <author>example@mail.com (Wei Zhuo, Runjie Luo, Wufeng Xue, Linlin Shen)</author>
      <guid isPermaLink="false">2507.20511v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Value Functions: Single-Loop Bilevel Optimization under Flatness Conditions</title>
      <link>http://arxiv.org/abs/2507.20400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于完全一阶惩罚的高效双层优化算法（PBGD-Free），消除了传统算法中的嵌套循环需求，提高了大型语言模型微调的计算效率，并通过理论证明了算法的收敛性。&lt;h4&gt;背景&lt;/h4&gt;双层优化是一种分层优化范式，在实际应用中受到广泛关注，特别是在生成模型的微调中。然而，由于嵌套问题结构，大多数现有算法需要Hessian向量计算或嵌套循环更新，这在大型语言模型微调中计算效率低下。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的无需值函数（PBGD-Free）算法，消除求解下层问题的循环，并允许完全单循环更新；同时为上层函数提出松弛的平坦性条件，并证明算法的收敛性。&lt;h4&gt;方法&lt;/h4&gt;基于完全一阶惩罚方法构建，提出无需值函数算法，通过松弛的平坦性条件确保收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在各种应用中测试了算法性能，展示了与最先进的双层方法相比具有优越的计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的PBGD-Free算法在计算效率上优于现有方法，适用于大型语言模型微调任务。&lt;h4&gt;翻译&lt;/h4&gt;双层优化，一种分层优化范式，已在多种实际应用中获得显著关注，特别是在生成模型的微调方面。然而，由于嵌套问题结构，大多数现有算法需要Hessian向量计算或嵌套循环更新，这在大型语言模型微调中计算效率低下。在本文中，基于完全一阶惩罚方法，我们提出了一种高效的无需值函数（PBGD-Free）算法，消除了求解下层问题的循环并允许完全单循环更新。受基于表示学习的LLM微调问题景观分析的启发，我们为上层函数提出了一种松弛的平坦性条件，并证明了所提出的无需值函数算法的收敛性。我们在各种应用中测试了所提出算法的性能，并展示了其与最先进的双层方法相比具有优越的计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bilevel optimization, a hierarchical optimization paradigm, has gainedsignificant attention in a wide range of practical applications, notably in thefine-tuning of generative models. However, due to the nested problem structure,most existing algorithms require either the Hessian vector calculation or thenested loop updates, which are computationally inefficient in large languagemodel (LLM) fine-tuning. In this paper, building upon the fully first-orderpenalty-based approach, we propose an efficient value function-free (PBGD-Free)algorithm that eliminates the loop of solving the lower-level problem andadmits fully single-loop updates. Inspired by the landscape analysis ofrepresentation learning-based LLM fine-tuning problem, we propose a relaxedflatness condition for the upper-level function and prove the convergence ofthe proposed value-function-free algorithm. We test the performance of theproposed algorithm in various applications and demonstrate its superiorcomputational efficiency over the state-of-the-art bilevel methods.</description>
      <author>example@mail.com (Liuyuan Jiang, Quan Xiao, Lisha Chen, Tianyi Chen)</author>
      <guid isPermaLink="false">2507.20400v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Wafer Defect Root Cause Analysis with Partial Trajectory Regression</title>
      <link>http://arxiv.org/abs/2507.20357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as K. Miyaguchi, M. Joko, R. Sheraw and T. Id\'e, "Wafer  Defect Root Cause Analysis with Partial Trajectory Regression,'' Proceedings  of the 36th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC  2025), Albany, NY, USA, 2025, pp. 1-6, doi: 10.1109/ASMC64512.2025.11010733&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为部分轨迹回归(PTR)的新框架，用于晶圆缺陷根本原因分析，解决了传统回归模型在处理可变长度加工路线方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;识别导致晶圆缺陷的上游工艺具有挑战性，这是因为工艺流程的组合性质以及加工路线的固有变异性，这种变异性源于返工操作和随机工艺等待时间等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效识别晶圆缺陷根本原因的新框架，特别是处理跨越大量异质物理工艺的可变长度加工路线。&lt;h4&gt;方法&lt;/h4&gt;提出部分轨迹回归(PTR)框架，通过比较来自部分工艺轨迹的两个反事实结果来计算每个工艺的归因分数，并使用新的表示学习方法proc2vec和route2vec实现。&lt;h4&gt;主要发现&lt;/h4&gt;使用来自纽约奥尔巴尼NY CREATES晶圆厂的真实晶圆历史数据证明了所提出框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;PTR框架能够有效处理晶圆缺陷的根本原因分析，克服了传统回归模型的局限性。&lt;h4&gt;翻译&lt;/h4&gt;识别导致晶圆缺陷的上游工艺具有挑战性，这是因为工艺流程的组合性质以及加工路线的固有变异性，这种变异性源于返工操作和随机工艺等待时间等因素。本文提出了一种用于晶圆缺陷根本原因分析的新框架，称为部分轨迹回归(PTR)。提出的框架经过精心设计，以解决传统基于向量的回归模型的局限性，特别是在处理跨越大量异质物理工艺的可变长度加工路线方面。为了计算在特定晶圆上检测到高缺陷密度时每个工艺的归因分数，我们提出了一种新算法，该算法比较来自部分工艺轨迹的两个反事实结果。这通过新的表示学习方法proc2vec和route2vec实现。我们使用来自纽约奥尔巴尼NY CREATES晶圆厂的真实晶圆历史数据证明了所提出框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ASMC64512.2025.11010733&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying upstream processes responsible for wafer defects is challengingdue to the combinatorial nature of process flows and the inherent variabilityin processing routes, which arises from factors such as rework operations andrandom process waiting times. This paper presents a novel framework for waferdefect root cause analysis, called Partial Trajectory Regression (PTR). Theproposed framework is carefully designed to address the limitations ofconventional vector-based regression models, particularly in handlingvariable-length processing routes that span a large number of heterogeneousphysical processes. To compute the attribution score of each process given adetected high defect density on a specific wafer, we propose a new algorithmthat compares two counterfactual outcomes derived from partial processtrajectories. This is enabled by new representation learning methods, proc2vecand route2vec. We demonstrate the effectiveness of the proposed framework usingreal wafer history data from the NY CREATES fab in Albany.</description>
      <author>example@mail.com (Kohei Miyaguchi, Masao Joko, Rebekah Sheraw, Tsuyoshi Idé)</author>
      <guid isPermaLink="false">2507.20357v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Region-based Cluster Discrimination for Visual Representation Learning</title>
      <link>http://arxiv.org/abs/2507.20025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a highlight paper at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了区域感知聚类判别(RICE)方法，解决了现有视觉语言对比模型在密集预测任务中的局限性，通过区域级别的表征学习和统一的学习框架提高了性能。&lt;h4&gt;背景&lt;/h4&gt;视觉表征学习对广泛的下游任务至关重要。最近的视觉语言对比模型(如CLIP和SigLIP)通过大规模视觉语言对齐实现了零样本性能，但其全局表征限制了在密集预测任务(如定位、OCR和分割)中的有效性。&lt;h4&gt;目的&lt;/h4&gt;提高区域级别的视觉和OCR能力，解决现有方法在密集预测任务中的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入区域感知聚类判别(RICE)方法，构建十亿规模的候选区域数据集，提出区域Transformer层提取区域语义，设计统一的区域聚类判别损失在单一分类框架内联合支持对象和OCR学习。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，RICE在分割、密集检测和用于多模态大型语言模型的视觉感知等任务上始终优于以前的方法。&lt;h4&gt;结论&lt;/h4&gt;RICE方法有效解决了现有视觉语言对比模型在密集预测任务中的局限性，通过区域级别的表征学习和统一的学习框架提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;学习视觉表征是广泛的下游任务的基础。尽管最近的视觉语言对比模型，如CLIP和SigLIP，通过大规模视觉语言对齐取得了令人印象深刻的零样本性能，但它们对全局表征的依赖限制了它们在密集预测任务（如定位、OCR和分割）中的有效性。为了解决这一差距，我们引入了区域感知聚类判别（RICE），一种增强区域级别视觉和OCR能力的新方法。我们首先构建了一个十亿规模的候选区域数据集，并提出了一种区域Transformer层来提取丰富的区域语义。我们进一步设计了一个统一的区域聚类判别损失，在单一分类框架内联合支持对象和OCR学习，使在大规模数据上进行高效且可扩展的分布式训练成为可能。大量实验表明，RICE在分割、密集检测以及用于多模态大型语言模型（MLLMs）的视觉感知等任务上始终优于以前的方法。预训练模型已在https://github.com/deepglint/MVT发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning visual representations is foundational for a broad spectrum ofdownstream tasks. Although recent vision-language contrastive models, such asCLIP and SigLIP, have achieved impressive zero-shot performance via large-scalevision-language alignment, their reliance on global representations constrainstheir effectiveness for dense prediction tasks, such as grounding, OCR, andsegmentation. To address this gap, we introduce Region-Aware ClusterDiscrimination (RICE), a novel method that enhances region-level visual and OCRcapabilities. We first construct a billion-scale candidate region dataset andpropose a Region Transformer layer to extract rich regional semantics. Wefurther design a unified region cluster discrimination loss that jointlysupports object and OCR learning within a single classification framework,enabling efficient and scalable distributed training on large-scale data.Extensive experiments show that RICE consistently outperforms previous methodson tasks, including segmentation, dense detection, and visual perception forMultimodal Large Language Models (MLLMs). The pre-trained models have beenreleased at https://github.com/deepglint/MVT.</description>
      <author>example@mail.com (Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, Ismail Elezi, Jiankang Deng)</author>
      <guid isPermaLink="false">2507.20025v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>A mini-batch training strategy for deep subspace clustering networks</title>
      <link>http://arxiv.org/abs/2507.19917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种适用于深度子空间聚类的mini-batch训练策略，通过集成存储库保存全局特征表示，使深度架构能够高效处理高分辨率图像的子空间聚类任务。同时提出了一种无解码器框架，利用对比学习替代自编码进行表示学习，消除了解码器训练的计算开销。&lt;h4&gt;背景&lt;/h4&gt;Mini-batch训练是现代深度学习的基石，提供了计算效率和可扩展性。然而，现有的深度子空间聚类(DSC)方法通常结合自编码器和自表达层，依赖于全批处理，其瓶颈来自于自表达模块需要整个数据集的表示来构建自表示系数矩阵。&lt;h4&gt;目的&lt;/h4&gt;引入一种适用于DSC的mini-batch训练策略，使深度架构能够高效地进行子空间聚类训练，特别是处理高分辨率图像；同时提出一种解码器框架，用于高效微调大规模预训练编码器进行子空间聚类。&lt;h4&gt;方法&lt;/h4&gt;集成一个保存全局特征表示的存储库(memory bank)，提出一种无解码器框架，利用对比学习而非自编码进行表示学习。这种设计消除了解码器训练的计算开销，同时提供了竞争性的性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了与全批处理方法相当的性能，在COIL100和ORL数据集上，通过微调深度网络，优于其他最先进的子空间聚类方法，能够克服之前处理高分辨率图像的限制。&lt;h4&gt;结论&lt;/h4&gt;通过引入存储库和无解码器框架，本文提出的mini-batch训练策略使深度子空间聚类方法能够高效处理大规模高分辨率图像数据，同时保持甚至提升聚类性能。&lt;h4&gt;翻译&lt;/h4&gt;Mini-batch training是现代深度学习的基石，为训练复杂架构提供了计算效率和可扩展性。然而，现有的深度子空间聚类(DSC)方法通常结合自编码器和自表达层，依赖于全批处理。瓶颈来自于自表达模块，它需要整个数据集的表示来构建自表示系数矩阵。在这项工作中，我们通过集成一个保存全局特征表示的存储库，为DSC引入了一种mini-batch训练策略。我们的方法使深度架构能够进行可扩展的子空间聚类训练，处理高分辨率图像，克服了之前的限制。此外，为了高效微调用于子空间聚类的大规模预训练编码器，我们提出了一种无解码器框架，利用对比学习而非自编码进行表示学习。这种设计不仅消除了解码器训练的计算开销，还提供了竞争性的性能。大量实验表明，我们的方法不仅实现了与全批处理方法相当的性能，而且在通过微调深度网络在COIL100和ORL数据集上优于其他最先进的子空间聚类方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mini-batch training is a cornerstone of modern deep learning, offeringcomputational efficiency and scalability for training complex architectures.However, existing deep subspace clustering (DSC) methods, which typicallycombine an autoencoder with a self-expressive layer, rely on full-batchprocessing. The bottleneck arises from the self-expressive module, whichrequires representations of the entire dataset to construct aself-representation coefficient matrix. In this work, we introduce a mini-batchtraining strategy for DSC by integrating a memory bank that preserves globalfeature representations. Our approach enables scalable training of deeparchitectures for subspace clustering with high-resolution images, overcomingprevious limitations. Additionally, to efficiently fine-tune large-scalepre-trained encoders for subspace clustering, we propose a decoder-freeframework that leverages contrastive learning instead of autoencoding forrepresentation learning. This design not only eliminates the computationaloverhead of decoder training but also provides competitive performance.Extensive experiments demonstrate that our approach not only achievesperformance comparable to full-batch methods, but outperforms otherstate-of-the-art subspace clustering methods on the COIL100 and ORL datasets byfine-tuning deep networks.</description>
      <author>example@mail.com (Yuxuan Jiang, Chenwei Yu, Zhi Lin, Xiaolan Liu)</author>
      <guid isPermaLink="false">2507.19917v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation</title>
      <link>http://arxiv.org/abs/2507.19817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IROS 2025, oral presentation. Project page link:  https://ziyin-xiong.github.io/ag2x2.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Ag2x2是一个用于双臂操作的计算框架，通过协调感知的视觉表征实现，这些表征同时编码物体状态和手的运动模式，同时保持与代理无关的特性。该方法在13种双臂任务中达到了73.5%的成功率，优于基线方法和专家设计奖励训练的策略。Ag2x2学到的表征可用于模仿学习，建立无需专家监督的技能获取流程。&lt;h4&gt;背景&lt;/h4&gt;双臂操作是人类日常活动的基础，但由于协调控制的内在复杂性，它仍然是一项具有挑战性的任务。最近的研究进展使得通过从人类视频中获得的与代理无关的视觉表征实现了单臂操作技能的零样本学习。然而，这些方法忽略了双臂协调所需的关键代理特定信息，如末端执行器位置。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为Ag2x2的计算框架，用于通过协调感知的视觉表征实现双臂操作。这些表征需要同时编码物体状态和手的运动模式，同时保持与代理无关的特性。&lt;h4&gt;方法&lt;/h4&gt;Ag2x2是一种计算框架，使用协调感知的视觉表征。这些表征联合编码物体状态和手的运动模式，同时保持与代理无关的特性。&lt;h4&gt;主要发现&lt;/h4&gt;在Bi-DexHands和PerAct2的13种多样化双臂任务中，Ag2x2达到了73.5%的成功率。这些任务包括具有挑战性的场景，如处理绳索等可变形物体。Ag2x2的性能优于基线方法，甚至超过了使用专家设计奖励训练的策略的成功率。通过Ag2x2学到的表征可以有效地用于模仿学习，建立了一个无需专家监督的技能获取可扩展流程。&lt;h4&gt;结论&lt;/h4&gt;Ag2x2在多样化任务中保持稳健性能，无需人类演示或设计奖励。Ag2x2代表了向可扩展学习复杂双臂机器人技能迈出的一步。&lt;h4&gt;翻译&lt;/h4&gt;双臂操作是人类日常活动的基础，由于其固有的协调控制复杂性，仍然是一项具有挑战性的任务。最近的进展使得通过从人类视频中获得的与代理无关的视觉表征实现了单臂操作技能的零样本学习；然而，这些方法忽略了双臂协调所需的关键代理特定信息，例如末端执行器位置。我们提出了Ag2x2，一个用于双臂操作的计算框架，通过协调感知的视觉表征实现，这些表征联合编码物体状态和手的运动模式，同时保持与代理无关的特性。大量实验表明，Ag2x2在Bi-DexHands和PerAct2的13种多样化双臂任务中达到了73.5%的成功率，包括处理绳索等可变形物体的具有挑战性的场景。这一性能优于基线方法，甚至超过了使用专家设计奖励训练的策略的成功率。此外，我们表明通过Ag2x2学到的表征可以有效地用于模仿学习，建立了一个无需专家监督的技能获取可扩展流程。通过在多样化任务中保持稳健性能，无需人类演示或设计奖励，Ag2x2代表了向可扩展学习复杂双臂机器人技能迈出的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bimanual manipulation, fundamental to human daily activities, remains achallenging task due to its inherent complexity of coordinated control. Recentadvances have enabled zero-shot learning of single-arm manipulation skillsthrough agent-agnostic visual representations derived from human videos;however, these methods overlook crucial agent-specific information necessaryfor bimanual coordination, such as end-effector positions. We propose Ag2x2, acomputational framework for bimanual manipulation through coordination-awarevisual representations that jointly encode object states and hand motionpatterns while maintaining agent-agnosticism. Extensive experiments demonstratethat Ag2x2 achieves a 73.5% success rate across 13 diverse bimanual tasks fromBi-DexHands and PerAct2, including challenging scenarios with deformableobjects like ropes. This performance outperforms baseline methods and evensurpasses the success rate of policies trained with expert-engineered rewards.Furthermore, we show that representations learned through Ag2x2 can beeffectively leveraged for imitation learning, establishing a scalable pipelinefor skill acquisition without expert supervision. By maintaining robustperformance across diverse tasks without human demonstrations or engineeredrewards, Ag2x2 represents a step toward scalable learning of complex bimanualrobotic skills.</description>
      <author>example@mail.com (Ziyin Xiong, Yinghan Chen, Puhao Li, Yixin Zhu, Tengyu Liu, Siyuan Huang)</author>
      <guid isPermaLink="false">2507.19817v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>SpecBPP: A Self-Supervised Learning Approach for Hyperspectral Representation and Soil Organic Carbon Estimation</title>
      <link>http://arxiv.org/abs/2507.19781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为光谱带置换预测(SpecBPP)的新型自监督学习框架，利用高光谱图像中的固有光谱连续性，通过恢复被打乱的光谱段顺序来鼓励全局光谱理解，在土壤有机碳估计任务中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在视觉和语言领域的表示学习中已经革命化了，但在高光谱图像领域尚未得到充分探索，高光谱图像的光谱带序列提供了独特的机会。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督学习框架，利用高光谱图像中的固有光谱连续性，通过预测光谱顺序来增强对高光谱数据的理解。&lt;h4&gt;方法&lt;/h4&gt;提出光谱带置换预测(SpecBPP)框架，挑战模型恢复被打乱的光谱段的正确顺序，而非重建掩码带。实施基于课程的训练策略，逐步增加置换难度，以管理置换空间的阶乘复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;应用于使用EnMAP卫星数据的土壤有机碳估计，SpecBPP取得了最先进的结果，优于掩码自编码器和联合嵌入预测基线。在有限的标记样本上微调后，模型获得了优异的性能指标，显著超过了传统和自监督基准。&lt;h4&gt;结论&lt;/h4&gt;光谱顺序预测是理解高光谱图像的有效预训练任务，为遥感及更广泛领域的科学表示学习开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已经革命化了视觉和语言领域的表示学习，但在高光谱图像领域仍然探索不足，高光谱图像的光谱带序列提供了独特的机会。在这项工作中，我们提出了光谱带置换预测，一种新的自监督学习框架，利用高光谱图像中的固有光谱连续性。该框架不是通过重建掩码带，而是挑战模型恢复被打乱的光谱段的正确顺序，鼓励全局光谱理解。我们实施了一种基于课程的训练策略，逐步增加置换难度，以管理置换空间的阶乘复杂性。应用于使用EnMAP卫星数据的土壤有机碳估计，我们的方法取得了最先进的结果，优于掩码自编码器和联合嵌入预测基线。在有限的标记样本上微调后，我们的模型获得了优异的性能指标，显著超过了传统和自监督基准。我们的结果表明，光谱顺序预测是理解高光谱图像的有效预训练任务，为遥感及更广泛领域的科学表示学习开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has revolutionized representation learning in visionand language, but remains underexplored for hyperspectral imagery (HSI), wherethe sequential structure of spectral bands offers unique opportunities. In thiswork, we propose Spectral Band Permutation Prediction (SpecBPP), a novelself-supervised learning framework that leverages the inherent spectralcontinuity in HSI. Instead of reconstructing masked bands, SpecBPP challenges amodel to recover the correct order of shuffled spectral segments, encouragingglobal spectral understanding. We implement a curriculum-based trainingstrategy that progressively increases permutation difficulty to manage thefactorial complexity of the permutation space. Applied to Soil Organic Carbon(SOC) estimation using EnMAP satellite data, our method achievesstate-of-the-art results, outperforming both masked autoencoder (MAE) andjoint-embedding predictive (JEPA) baselines. Fine-tuned on limited labeledsamples, our model yields an $R^2$ of 0.9456, RMSE of 1.1053%, and RPD of 4.19,significantly surpassing traditional and self-supervised benchmarks. Ourresults demonstrate that spectral order prediction is a powerful pretext taskfor hyperspectral understanding, opening new avenues for scientificrepresentation learning in remote sensing and beyond.</description>
      <author>example@mail.com (Daniel La'ah Ayuba, Jean-Yves Guillemaut, Belen Marti-Cardona, Oscar Mendez Maldonado)</author>
      <guid isPermaLink="false">2507.19781v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Self-Guided Masked Autoencoder</title>
      <link>http://arxiv.org/abs/2507.19773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究深入分析了掩码自编码器(MAE)的学习机制，发现其本质上学习基于模式的块级聚类，并提出了自引导掩码自编码器方法，利用聚类进展生成信息丰富的掩码，显著提高了学习效果。&lt;h4&gt;背景&lt;/h4&gt;掩码自编码器(MAE)是一种自监督表示学习方法，广泛应用于计算机视觉的多种下游任务。尽管MAE取得了成功，但对其确切学习内容和方式仍不完全了解。&lt;h4&gt;目的&lt;/h4&gt;深入了解MAE的学习机制，并基于此发现改进MAE的学习过程，同时保持其自监督特性。&lt;h4&gt;方法&lt;/h4&gt;通过深入分析MAE的学习过程，发现MAE本质上学习基于模式的块级聚类。基于这一理解，提出了自引导掩码自编码器，利用模型在块聚类方面的进展生成信息丰富的掩码，替代原始MAE中的随机掩码。&lt;h4&gt;主要发现&lt;/h4&gt;1) MAE本质上学习基于模式的块级聚类，这一现象在预训练的早期阶段就已经出现；2) 利用聚类进度生成信息丰富的掩码可以显著提高学习过程；3) 这种方法不需要依赖任何外部模型或补充信息。&lt;h4&gt;结论&lt;/h4&gt;自引导掩码自编码器方法能够显著提高学习效果，同时保持MAE的自监督特性优势。在各种下游任务上的综合实验验证了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;掩码自编码器(MAE)是一种用于表示学习的自监督方法，广泛应用于计算机视觉的各种下游任务。尽管取得了成功，MAE究竟学习什么以及如何学习仍未完全揭示。在本文中，通过深入分析，我们发现MAE本质上学习基于模式的块级聚类，这一现象在预训练的早期阶段就令人惊讶地出现。基于这一理解，我们提出了自引导掩码自编码器，它利用在块聚类方面的进展生成信息丰富的掩码，替代了原始MAE的简单随机掩码。我们的方法显著提高了学习过程，而不依赖任何外部模型或补充信息，保持了MAE自监督特性的优势。在各种下游任务上的综合实验验证了所提方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked Autoencoder (MAE) is a self-supervised approach for representationlearning, widely applicable to a variety of downstream tasks in computervision. In spite of its success, it is still not fully uncovered what and howMAE exactly learns. In this paper, with an in-depth analysis, we discover thatMAE intrinsically learns pattern-based patch-level clustering from surprisinglyearly stages of pretraining. Upon this understanding, we propose self-guidedmasked autoencoder, which internally generates informed mask by utilizing itsprogress in patch clustering, substituting the naive random masking of thevanilla MAE. Our approach significantly boosts its learning process withoutrelying on any external models or supplementary information, keeping thebenefit of self-supervised nature of MAE intact. Comprehensive experiments onvarious downstream tasks verify the effectiveness of the proposed method.</description>
      <author>example@mail.com (Jeongwoo Shin, Inseo Lee, Junho Lee, Joonseok Lee)</author>
      <guid isPermaLink="false">2507.19773v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning</title>
      <link>http://arxiv.org/abs/2507.19647v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IROS 2025 camera-ready version. First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为GABRIL（基于凝视的正则化模仿学习）的新方法，通过利用人类专家在数据收集过程中的凝视数据来引导模仿学习中的表征学习，从而解决了模仿学习中常见的因果混淆问题。&lt;h4&gt;背景&lt;/h4&gt;模仿学习(IL)是一种广泛采用的方法，它通过将任务构建为监督学习问题，使智能体能够从人类专家演示中学习。然而，IL常常遭受因果混淆的困扰，智能体会将虚假相关性误解为因果关系，导致在测试环境分布发生变化时性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决模仿学习中的因果混淆问题，提高智能体在分布变化环境中的表现，同时增强模型的可解释性。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了GABRIL方法，该方法利用在数据收集阶段收集的人类凝视数据，通过一种正则化损失函数，鼓励模型专注于通过专家凝视识别出的因果相关特征，从而减少混淆变量的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在Atari环境和CARLA的Bench2Drive基准测试中验证了该方法；2. 实验结果表明，在Atari环境中，GABRIL相比行为克隆的改进比其他基线方法高出约179%；3. 在CARLA设置中，改进比其他基线方法高出约76%；4. 与常规IL智能体相比，该方法提供了额外的可解释性。&lt;h4&gt;结论&lt;/h4&gt;GABRIL通过整合人类凝视数据作为正则化信号，有效缓解了模仿学习中的因果混淆问题，显著提升了智能体在分布变化环境中的性能，同时增强了模型的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;模仿学习(IL)是一种广泛采用的方法，它通过将任务构建为监督学习问题，使智能体能够从人类专家演示中学习。然而，IL常常遭受因果混淆的困扰，智能体会将虚假相关性误解为因果关系，导致在测试环境分布发生变化时性能下降。为了解决这个问题，我们引入了GABRIL（模仿学习中的基于凝视的正则化），一种新颖的方法，它利用在数据收集阶段收集的人类凝视数据来引导IL中的表征学习。GABRIL利用一种正则化损失函数，鼓励模型专注于通过专家凝视识别出的因果相关特征，从而减轻混淆变量的影响。我们在Atari环境和CARLA的Bench2Drive基准测试中通过收集人类凝视数据并在这两个领域中应用我们的方法来验证我们的方法。实验结果表明，在Atari环境中，GABRIL相比行为克隆的改进比其他基线方法高出约179%，在CARLA设置中高出约76%。最后，我们表明与常规IL智能体相比，我们的方法提供了额外的可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation Learning (IL) is a widely adopted approach which enables agents tolearn from human expert demonstrations by framing the task as a supervisedlearning problem. However, IL often suffers from causal confusion, where agentsmisinterpret spurious correlations as causal relationships, leading to poorperformance in testing environments with distribution shift. To address thisissue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), anovel method that leverages the human gaze data gathered during the datacollection phase to guide the representation learning in IL. GABRIL utilizes aregularization loss which encourages the model to focus on causally relevantfeatures identified through expert gaze and consequently mitigates the effectsof confounding variables. We validate our approach in Atari environments andthe Bench2Drive benchmark in CARLA by collecting human gaze datasets andapplying our method in both domains. Experimental results show that theimprovement of GABRIL over behavior cloning is around 179% more than the samenumber for other baselines in the Atari and 76% in the CARLA setup. Finally, weshow that our method provides extra explainability when compared to regular ILagents.</description>
      <author>example@mail.com (Amin Banayeeanzade, Fatemeh Bahrani, Yutai Zhou, Erdem Bıyık)</author>
      <guid isPermaLink="false">2507.19647v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Observations Meet Actions: Learning Control-Sufficient Representations for Robust Policy Generalization</title>
      <link>http://arxiv.org/abs/2507.19437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于上下文的强化学习方法，通过双重推理-控制问题框架，将观察充分性和控制充分性分离，并开发了BCPO算法在连续控制基准测试中实现了优越性能。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理在其训练范围之外部署时，捕捉潜在变化('上下文')是关键挑战。&lt;h4&gt;目的&lt;/h4&gt;重新构建基于上下文的强化学习为双重推理-控制问题，并正式表征两个属性及其层次结构。&lt;h4&gt;方法&lt;/h4&gt;将基于上下文的强化学习重新构建为双重推理-控制问题，表征观察充分性和控制充分性，推导上下文ELBO风格目标函数，并使用BCPO算法(在离策略策略学习器前放置变分信息瓶颈编码器)进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在具有变化物理参数的标准连续控制基准测试中，BCPO匹配或超越其他基线方法，同时使用更少的样本，并在训练范围之外保持远超的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架统一了基于上下文的强化学习的理论、诊断和实践。&lt;h4&gt;翻译&lt;/h4&gt;捕捉潜在变化('上下文')是将强化学习代理部署到其训练范围之外的关键。我们将基于上下文的强化学习重新构建为双重推理-控制问题，并正式表征了两个属性及其层次结构：观察充分性(保留所有预测信息)和控制充分性(保留决策相关信息)。利用这种二元性，我们推导出上下文证据下界(ELBO)风格的目标函数，并使用BCPO算法对其进行优化，该算法在任何离策略策略学习器前放置变分信息瓶颈编码器。在具有变化物理参数的标准连续控制基准测试中，BCPO匹配或超越其他基线方法，同时使用更少的样本，并在训练范围之外保持远超的性能。该框架统一了基于上下文的强化学习的理论、诊断和实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Capturing latent variations ("contexts") is key to deployingreinforcement-learning (RL) agents beyond their training regime. We recastcontext-based RL as a dual inference-control problem and formally characterizetwo properties and their hierarchy: observation sufficiency (preserving allpredictive information) and control sufficiency (retaining decision-makingrelevant information). Exploiting this dichotomy, we derive a contextualevidence lower bound(ELBO)-style objective that cleanly separatesrepresentation learning from policy learning and optimizes it with BottleneckedContextual Policy Optimization (BCPO), an algorithm that places a variationalinformation-bottleneck encoder in front of any off-policy policy learner. Onstandard continuous-control benchmarks with shifting physical parameters, BCPOmatches or surpasses other baselines while using fewer samples and retainingperformance far outside the training regime. The framework unifies theory,diagnostics, and practice for context-based RL.</description>
      <author>example@mail.com (Yuliang Gu, Hongpeng Cao, Marco Caccamo, Naira Hovakimyan)</author>
      <guid isPermaLink="false">2507.19437v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction</title>
      <link>http://arxiv.org/abs/2507.19119v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PatchTraj的动态块状轨迹预测框架，通过统一时域和频域表示来改进行人轨迹预测。该方法分解轨迹为原始时间序列和频率分量，使用动态块分割进行多尺度轨迹分割，并采用自适应嵌入层和层次特征聚合来建模精细和长程依赖。实验证明该方法在多个数据集上达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;行人轨迹预测对自动驾驶和机器人技术至关重要。现有的基于点和网格的方法存在两个主要局限：一是未能充分建模人类运动动态，无法平衡局部运动细节与长程时空依赖；二是时间表示在建模轨迹序列时缺乏与频域的交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有行人轨迹预测方法中的两个关键问题：1) 更好地建模人类运动动态，平衡局部细节与长程依赖；2) 在时间表示中引入频域交互，提高轨迹序列建模能力。&lt;h4&gt;方法&lt;/h4&gt;提出PatchTraj框架，一种动态块状轨迹预测方法，统一时域和频域表示。具体包括：1) 将轨迹分解为原始时间序列和频率分量；2) 使用动态块分割进行多尺度轨迹分割；3) 每个块通过自适应嵌入层处理，结合感知尺度的特征提取；4) 进行层次特征聚合；5) 通过跨模态注意力使两个分支输出交互；6) 使用Transformer编码器-解码器整合两种模态，自回归预测未来轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在ETH-UCY、SDD、NBA和JRDB等多个数据集上的大量实验表明，该方法实现了最先进的性能，同时保持高效率。&lt;h4&gt;结论&lt;/h4&gt;PatchTraj框架通过统一时域和频域表示，有效解决了现有行人轨迹预测方法的局限性，提高了预测性能，为自动驾驶和机器人技术提供了更可靠的行人轨迹预测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;行人轨迹预测对自动驾驶和机器人至关重要。虽然现有的基于点和网格的方法暴露了两个关键局限：在建模人类运动动态方面不足，因为它们无法平衡局部运动细节与长程时空依赖，并且时间表示在建模轨迹序列时缺乏与频域的交互。为应对这些挑战，我们提出了PatchTraj，一种动态块状轨迹预测框架，统一了时域和频域表示。具体来说，我们将轨迹分解为原始时间序列和频率分量，采用动态块分割进行多尺度轨迹分割，以捕捉层次化运动模式。每个块通过具有感知尺度特征提取的自适应嵌入层处理，然后进行层次特征聚合，建模精细和长程依赖。两个分支的输出通过跨模态注意力交互，实现时间和频谱线索的互补融合。最后，Transformer编码器-解码器整合两种模态，自回归预测未来轨迹。在ETH-UCY、SDD、NBA和JRDB数据集上的大量实验表明，我们的方法实现了最先进的性能，同时保持高效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pedestrian trajectory prediction is crucial for autonomous driving androbotics. While existing point-based and grid-based methods expose two keylimitations: insufficiently modeling human motion dynamics, as they fail tobalance local motion details with long-range spatiotemporal dependencies, andthe time representation lacks interaction with the frequency domain in modelingtrajectory sequences. To address these challenges, we propose PatchTraj, adynamic patch-based trajectory prediction framework that unifies time-domainand frequency-domain representations. Specifically, we decompose the trajectoryinto raw time sequences and frequency components, employing dynamic patchpartitioning for multi-scale trajectory segmentation to capture hierarchicalmotion patterns. Each patch is processed by an adaptive embedding layer withscale-aware feature extraction, followed by hierarchical feature aggregation tomodel both fine-grained and long-range dependencies. The outputs of twobranches interact via cross-modal attention, enabling complementary fusion oftemporal and spectral cues. Finally, a Transformer encoder-decoder integratesboth modalities to autoregressively predict future trajectories. Extensiveexperiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our methodachieves state-of-the-art performance with high efficiency.</description>
      <author>example@mail.com (Yanghong Liu, Xingping Dong, Ming Li, Weixing Zhang, Yidong Lou)</author>
      <guid isPermaLink="false">2507.19119v2</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2507.18521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GLANCE是一种新型图神经网络框架，通过整合逻辑引导推理、动态图细化和自适应聚类，有效解决了传统GNN在异质图上的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在图结构数据学习方面取得了显著成功，但在异质图(连接节点在特征或类别标签上存在差异的图)上表现不佳，主要源于 indiscriminate neighbor aggregation 和 insufficient incorporation of higher-order structural patterns。&lt;h4&gt;目的&lt;/h4&gt;解决GNN在异质图上的局限性，提出一种能够增强图表示学习的新框架，提供鲁棒且可解释的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出GLANCE (Graph Logic Attention Network with Cluster Enhancement)框架，整合了：1) 逻辑层用于可解释和结构化的嵌入；2) 基于多头注意力的边修剪用于去噪图结构；3) 聚集机制用于捕获全局模式。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集(Cornell, Texas, Wisconsin)上的实验结果表明GLANCE具有竞争力，为异质图场景提供了鲁棒且可解释的解决方案。&lt;h4&gt;结论&lt;/h4&gt;GLANCE框架是轻量级、可适应的，特别适合异质图的挑战，能够有效处理连接节点在特征或类别标签上存在差异的图结构数据。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在从图结构数据中学习方面已经显示出显著的成功，但在异质图上往往表现不佳，在异质图中，连接的节点在特征或类别标签上有所不同。这种局限性源于不加区分的邻居聚合和不足的高阶结构模式整合。为了解决这些挑战，我们提出了GLANCE(具有聚类增强的图逻辑注意力网络)，这是一个新型框架，它整合了逻辑引导推理、动态图细化和自适应聚类以增强图表示学习。GLANCE结合了用于可解释和结构化嵌入的逻辑层、用于去噪图结构的多头注意力边修剪以及用于捕获全局模式的聚集机制。在包括Cornell、Texas和Wisconsin在内的基准数据集上的实验结果表明，GLANCE实现了具有竞争力的性能，为异质图场景提供了鲁棒且可解释的解决方案。所提出的框架是轻量级、可适应的，并且特别适合异质图的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated significant success inlearning from graph-structured data but often struggle on heterophilous graphs,where connected nodes differ in features or class labels. This limitationarises from indiscriminate neighbor aggregation and insufficient incorporationof higher-order structural patterns. To address these challenges, we proposeGLANCE (Graph Logic Attention Network with Cluster Enhancement), a novelframework that integrates logic-guided reasoning, dynamic graph refinement, andadaptive clustering to enhance graph representation learning. GLANCE combines alogic layer for interpretable and structured embeddings, multi-headattention-based edge pruning for denoising graph structures, and clusteringmechanisms for capturing global patterns. Experimental results in benchmarkdatasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCEachieves competitive performance, offering robust and interpretable solutionsfor heterophilous graph scenarios. The proposed framework is lightweight,adaptable, and uniquely suited to the challenges of heterophilous graphs.</description>
      <author>example@mail.com (Zhongtian Sun, Anoushka Harit, Alexandra Cristea, Christl A. Donnelly, Pietro Liò)</author>
      <guid isPermaLink="false">2507.18521v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2507.18519v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对传统双模拟度量在强化学习中的两个主要问题进行了改进，提出了具有更精确奖励差距定义和自适应更新算子的修订版本，并通过理论分析和实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;双模拟度量长期以来被视为强化学习任务中一种有效的控制相关表征学习技术。&lt;h4&gt;目的&lt;/h4&gt;解决传统双模拟度量的两个主要问题：无法表示某些特定场景，以及在递归更新过程中依赖于预定义权重的问题。&lt;h4&gt;方法&lt;/h4&gt;通过引入状态-动作对的度量，提出修订的双模拟度量，具有更精确的奖励差距定义和具有自适应系数的新型更新算子。&lt;h4&gt;主要发现&lt;/h4&gt;第一个问题源于奖励差距的不精确定义，第二个问题源于忽视了不同训练阶段和任务设置中奖励差异和后续状态区分的重要性变化。&lt;h4&gt;结论&lt;/h4&gt;所提出的修订双模拟度量具有更好的表征区分性，并在DeepMind Control和Meta-World两个代表性基准上证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;双模拟度量长期以来被视为强化学习任务中一种有效的控制相关表征学习技术。然而，在本文中，我们确定了传统双模拟度量的两个主要问题：1) 无法表示某些特定场景，以及 2) 在递归更新过程中依赖于预定义的权重来处理奖励和后续状态的差异。我们发现第一个问题源于奖励差距的不精确定义，而第二个问题源于忽视了在不同训练阶段和任务设置中奖励差异和后续状态区分的重要性变化。为解决这些问题，通过引入状态-动作对的度量，我们提出了修订的双模拟度量，具有更精确的奖励差距定义和具有自适应系数的新型更新算子。我们还为所提出的度量和改进的表征区分性提供了收敛性理论保证。除了严格的理论分析外，我们还在DeepMind Control和Meta-World两个代表性基准上进行了广泛实验，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bisimulation metric has long been regarded as an effective control-relatedrepresentation learning technique in various reinforcement learning tasks.However, in this paper, we identify two main issues with the conventionalbisimulation metric: 1) an inability to represent certain distinctivescenarios, and 2) a reliance on predefined weights for differences in rewardsand subsequent states during recursive updates. We find that the first issuearises from an imprecise definition of the reward gap, whereas the second issuestems from overlooking the varying importance of reward difference andnext-state distinctions across different training stages and task settings. Toaddress these issues, by introducing a measure for state-action pairs, wepropose a revised bisimulation metric that features a more precise definitionof reward gap and novel update operators with adaptive coefficient. We alsooffer theoretical guarantees of convergence for our proposed metric and itsimproved representation distinctiveness. In addition to our rigoroustheoretical analysis, we conduct extensive experiments on two representativebenchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness ofour approach.</description>
      <author>example@mail.com (Leiji Zhang, Zeyu Wang, Xin Li, Yao-Hui Li)</author>
      <guid isPermaLink="false">2507.18519v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection</title>
      <link>http://arxiv.org/abs/2507.18260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Neural Networks. We propose the Gaussian Group Squeezer,  leveraging Gaussian sampling and compression with diffusion models for  channel-based data augmentation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种高斯无表示学习方法，用于解决红外小目标检测在数据稀缺情况下的脆弱性问题，通过高斯群压缩器和两阶段扩散模型提高了模型在现实世界挑战中的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;红外小目标检测在众多实际应用中起着重要作用，但当前研究使用大量昂贵的人工标注数据进行表示学习，导致最先进的方法在现实世界挑战中非常脆弱。&lt;h4&gt;目的&lt;/h4&gt;研究在不同数据稀缺性情况下主流ISTD方法的性能变化，挑战现有理论，并提高ISTD模型应对现实世界挑战的能力。&lt;h4&gt;方法&lt;/h4&gt;引入高斯无表示学习，提出高斯群压缩器利用高斯采样和压缩进行非均匀量化，通过多样化训练样本增强模型韧性，并采用两阶段扩散模型进行现实世界重建，使量化信号与现实世界分布对齐。&lt;h4&gt;主要发现&lt;/h4&gt;主流ISTD方法在不同数据稀缺性场景下性能存在显著变化；所提方法能有效应对数据稀缺挑战；通过量化信号与现实分布对齐可提高合成样本质量。&lt;h4&gt;结论&lt;/h4&gt;在各种稀缺性场景下，所提方法与最先进的检测方法相比具有有效性，能显著提高ISTD模型在现实世界应用中的鲁棒性和性能。&lt;h4&gt;翻译&lt;/h4&gt;红外小目标检测(ISTD)在众多实际应用中起着至关重要的作用。为了确定性能边界，研究人员使用大量昂贵的人工标注数据进行表示学习。然而，这种方法导致最先进的ISTD方法在面对现实世界的挑战时非常脆弱。在本文中，我们首先研究在各种稀缺性(即缺乏高质量红外数据)情况下，几种主流方法的检测性能变化，这些情况挑战了关于实际ISTD的主流理论。为解决这一问题，我们引入了高斯无表示学习。具体而言，我们提出了高斯群压缩器，利用高斯采样和压缩进行非均匀量化。通过利用多样化的训练样本，我们增强了ISTD模型应对各种挑战的韧性。然后，我们引入了两阶段扩散模型进行现实世界重建。通过将量化信号与现实世界分布紧密对齐，我们显著提高了合成样本的质量和保真度。在各种稀缺性场景下与最先进的检测方法进行的比较评估证明了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrared small target detection (ISTD) plays a vital role in numerouspractical applications. In pursuit of determining the performance boundaries,researchers employ large and expensive manual-labeling data for representationlearning. Nevertheless, this approach renders the state-of-the-art ISTD methodshighly fragile in real-world challenges. In this paper, we first study thevariation in detection performance across several mainstream methods undervarious scarcity -- namely, the absence of high-quality infrared data -- thatchallenge the prevailing theories about practical ISTD. To address thisconcern, we introduce the Gaussian Agnostic Representation Learning.Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussiansampling and compression for non-uniform quantization. By exploiting a diversearray of training samples, we enhance the resilience of ISTD models againstvarious challenges. Then, we introduce two-stage diffusion models forreal-world reconstruction. By aligning quantized signals closely withreal-world distributions, we significantly elevate the quality and fidelity ofthe synthetic samples. Comparative evaluations against state-of-the-artdetection methods in various scarcity scenarios demonstrate the efficacy of theproposed approach.</description>
      <author>example@mail.com (Junyao Li, Yahao Lu, Xingyuan Guo, Xiaoyu Xian, Tiantian Wang, Yukai Shi)</author>
      <guid isPermaLink="false">2507.18260v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation</title>
      <link>http://arxiv.org/abs/2507.18184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MatSSL是一种简化的自监督学习架构，在骨干网络的每个阶段使用门控特征融合来有效整合多级表示。它解决了金属材料显微图像分析中现有监督方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前金属材料显微图像分析依赖于监督方法，需要针对每个新数据集重新训练，且在只有少量标记样本时表现不一致。虽然自监督学习利用未标记数据提供了替代方案，但大多数现有方法仍依赖大规模数据集才能有效。&lt;h4&gt;目的&lt;/h4&gt;MatSSL旨在克服现有方法的局限性，通过在小规模未标记数据集上进行自监督预训练，然后在多个基准数据集上微调模型，从而实现对金属学领域的有效适应。&lt;h4&gt;方法&lt;/h4&gt;首先在小规模未标记数据集上进行自监督预训练，然后在多个基准数据集上微调模型。MatSSL使用门控特征融合在每个骨干网络阶段整合多级表示。&lt;h4&gt;主要发现&lt;/h4&gt;在MetalDAM上，分割模型实现了69.13%的mIoU，超过了ImageNet预编码器实现的66.73%。在环境屏障涂层基准数据集(EBC)上，与使用MicroNet预训练的模型相比，平均mIoU有近40%的一致性提升。&lt;h4&gt;结论&lt;/h4&gt;MatSSL能够仅使用少量未标记数据实现对金属学领域的有效适应，同时保留了从大规模自然图像预训练中学到的丰富且可转移的特征。&lt;h4&gt;翻译&lt;/h4&gt;MatSSL是一种简化的自监督学习架构，它在骨干网络的每个阶段采用门控特征融合，以有效整合多级表示。当前金属材料显微图像分析依赖于监督方法，这些方法需要针对每个新数据集重新训练，并且通常在只有少量标记样本时表现不一致。虽然自监督学习通过利用未标记数据提供了有前途的替代方案，但大多数现有方法仍然依赖于大规模数据集才能有效。MatSSL旨在克服这一限制。我们首先在小规模未标记数据集上进行自监督预训练，然后在多个基准数据集上微调模型。分割模型在MetalDAM上实现了69.13%的mIoU，超过了ImageNet预编码器实现的66.73%，并且在环境屏障涂层基准数据集(EBC)上，与使用MicroNet预训练的模型相比，平均mIoU有近40%的一致性提升。这表明MatSSL能够仅使用少量未标记数据实现对金属学领域的有效适应，同时保留了从大规模自然图像预训练中学到的丰富且可转移的特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; MatSSL is a streamlined self-supervised learning (SSL) architecture thatemploys Gated Feature Fusion at each stage of the backbone to integratemulti-level representations effectively. Current micrograph analysis ofmetallic materials relies on supervised methods, which require retraining foreach new dataset and often perform inconsistently with only a few labeledsamples. While SSL offers a promising alternative by leveraging unlabeled data,most existing methods still depend on large-scale datasets to be effective.MatSSL is designed to overcome this limitation. We first performself-supervised pretraining on a small-scale, unlabeled dataset and thenfine-tune the model on multiple benchmark datasets. The resulting segmentationmodels achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by anImageNet-pretrained encoder, and delivers consistently up to nearly 40%improvement in average mIoU on the Environmental Barrier Coating benchmarkdataset (EBC) compared to models pretrained with MicroNet. This suggests thatMatSSL enables effective adaptation to the metallographic domain using only asmall amount of unlabeled data, while preserving the rich and transferablefeatures learned from large-scale pretraining on natural images.</description>
      <author>example@mail.com (Hoang Hai Nam Nguyen, Phan Nguyen Duc Hieu, Ho Won Lee)</author>
      <guid isPermaLink="false">2507.18184v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions</title>
      <link>http://arxiv.org/abs/2507.21016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary version; a revised version will be uploaded later&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统评估了经典机器学习与深度学习模型在认知预测方面的性能，发现基于任务的fMRI优于静息态fMRI，结合结构连接和功能连接的GNN表现最佳，强调了选择适当模型架构和特征表示的重要性。&lt;h4&gt;背景&lt;/h4&gt;预测健康个体的认知能力对理解认知神经机制有重要意义，并在精准医疗和神经精神疾病早期检测方面有潜在应用价值。&lt;h4&gt;目的&lt;/h4&gt;系统性地评估经典机器学习(KRR)和先进深度学习模型(GNN和TGNN)在认知预测方面的性能，使用不同类型的fMRI数据。&lt;h4&gt;方法&lt;/h4&gt;使用人类连接组计划青年成人数据集的静息态fMRI、工作记忆任务fMRI和语言任务fMRI数据，比较KRR、GNN和TGNN三种模型，采用R²分数、皮尔逊相关系数和平均绝对误差作为评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基于任务的fMRI在预测认知行为方面优于静息态fMRI；2. 结合结构连接(SC)和功能连接(FC)的GNN在所有fMRI模态中表现最佳；3. 该GNN与仅使用FC的KRR相比优势不显著；4. TGNN在任务fMRI上表现良好，但在静息态数据上表现较差。&lt;h4&gt;结论&lt;/h4&gt;选择合适的模型架构和特征表示对充分利用神经影像数据的空间和时间丰富性至关重要；多模态图感知深度学习模型结合SC和FC进行认知预测具有潜力；基于Transformer的方法在捕捉时间动态方面前景广阔。&lt;h4&gt;翻译&lt;/h4&gt;预测健康个体神经影像数据的认知能力为理解认知能力背后的神经机制提供了见解，并在精准医疗和神经精神疾病的早期检测方面具有潜在应用。本研究使用人类连接组计划青年成人数据集的静息态(RS)、工作记忆和语言任务fMRI数据，系统性地评估了经典机器学习(核脊回归(KRR))和先进深度学习模型(图神经网络(GNN)和Transformer-GNN(TGNN))在认知预测方面的性能。我们的结果基于R²分数、皮尔逊相关系数和平均绝对误差，表明与认知直接相关的任务fMRI在预测认知行为方面优于静息态fMRI。在比较的方法中，结合结构连接(SC)和功能连接(FC)的GNN在所有fMRI模态中始终取得最高性能；然而，其与仅使用FC的KRR相比的优势不具有统计学显著性。设计用于以SC为先验建模时间动态的TGNN在任务fMRI上与基于FC的方法表现相当，但在处理静息态数据时表现不佳，其性能与直接使用fMRI时间序列作为节点特征的较低性能GNN相当。这些发现强调了选择适当的模型架构和特征表示以充分利用神经影像数据的空间和时间丰富性的重要性。本研究突出了多模态图感知深度学习模型结合SC和FC进行认知预测的潜力，以及基于Transformer的方法在捕捉时间动态方面的前景。通过提供全面的模型比较，这项工作为使用fMRI、SC和深度学习推进脑-行为建模提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting cognition from neuroimaging data in healthy individuals offersinsights into the neural mechanisms underlying cognitive abilities, withpotential applications in precision medicine and early detection ofneurological and psychiatric conditions. This study systematically benchmarkedclassical machine learning (Kernel Ridge Regression (KRR)) and advanced deeplearning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN))for cognitive prediction using Resting-state (RS), Working Memory, and Languagetask fMRI data from the Human Connectome Project Young Adult dataset.  Our results, based on R2 scores, Pearson correlation coefficient, and meanabsolute error, revealed that task-based fMRI, eliciting neural responsesdirectly tied to cognition, outperformed RS fMRI in predicting cognitivebehavior. Among the methods compared, a GNN combining structural connectivity(SC) and functional connectivity (FC) consistently achieved the highestperformance across all fMRI modalities; however, its advantage over KRR usingFC alone was not statistically significant. The TGNN, designed to modeltemporal dynamics with SC as a prior, performed competitively with FC-basedapproaches for task-fMRI but struggled with RS data, where its performancealigned with the lower-performing GNN that directly used fMRI time-series dataas node features. These findings emphasize the importance of selectingappropriate model architectures and feature representations to fully leveragethe spatial and temporal richness of neuroimaging data.  This study highlights the potential of multimodal graph-aware DL models tocombine SC and FC for cognitive prediction, as well as the promise ofTransformer-based approaches for capturing temporal dynamics. By providing acomprehensive comparison of models, this work serves as a guide for advancingbrain-behavior modeling using fMRI, SC and DL.</description>
      <author>example@mail.com (Jagruti Patel, Mikkel Schöttner, Thomas A. W. Bolton, Patric Hagmann)</author>
      <guid isPermaLink="false">2507.21016v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network</title>
      <link>http://arxiv.org/abs/2507.20838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于时空图神经网络的多建筑预测方法，通过构建建筑特征和环境因素的图结构，利用带注意力的多级图卷积架构进行能源预测，并提供了图结构解释方法。实验证明该方法优于多种基线方法，具有良好的稳健性、泛化性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;由于操作数据的广泛可用性，数据驱动方法在预测建筑能源负荷方面显示出强大能力。具有相似特征的建筑通常共享能源模式，反映在其操作数据中的空间依赖性，而传统预测方法难以捕捉这种依赖性。&lt;h4&gt;目的&lt;/h4&gt;为了克服传统预测方法难以捕捉建筑间空间依赖性的问题，提出一种使用时空图神经网络的多建筑预测方法。&lt;h4&gt;方法&lt;/h4&gt;首先基于建筑特征和环境因素构建图；其次开发带注意力的多级图卷积架构用于能源预测；最后介绍解释优化图结构的方法。&lt;h4&gt;主要发现&lt;/h4&gt;在Building Data Genome Project 2数据集上的实验证实该方法优于XGBoost、SVR、FCNN、GRU和Naive等基线方法，突显了该方法在捕捉有意义的建筑相似性和空间关系方面的稳健性、泛化性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效捕捉建筑间的空间依赖关系，提高能源预测的准确性，为建筑能源管理提供了新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;由于操作数据的广泛可用性，数据驱动方法在预测建筑能源负荷方面显示出强大能力。具有相似特征的建筑通常共享能源模式，反映在其操作数据中的空间依赖性，而传统预测方法难以捕捉这种依赖性。为了克服这一问题，我们提出了一种使用时空图神经网络的多建筑预测方法，包括图表示、图学习和解释。首先，基于建筑特征和环境因素构建图。其次，开发了一种带注意力的多级图卷积架构用于能源预测。最后，介绍了一种解释优化图结构的方法。在Building Data Genome Project 2数据集上的实验证实了该方法优于XGBoost、SVR、FCNN、GRU和Naive等基线方法，突显了该方法在捕捉有意义的建筑相似性和空间关系方面的稳健性、泛化性和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the extensive availability of operation data, data-driven methods showstrong capabilities in predicting building energy loads. Buildings with similarfeatures often share energy patterns, reflected by spatial dependencies intheir operational data, which conventional prediction methods struggle tocapture. To overcome this, we propose a multi-building prediction approachusing spatio-temporal graph neural networks, comprising graph representation,graph learning, and interpretation. First, a graph is built based on buildingcharacteristics and environmental factors. Next, a multi-level graphconvolutional architecture with attention is developed for energy prediction.Lastly, a method interpreting the optimized graph structure is introduced.Experiments on the Building Data Genome Project 2 dataset confirm superiorperformance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive,highlighting the method's robustness, generalization, and interpretability incapturing meaningful building similarities and spatial relationships.</description>
      <author>example@mail.com (Yongzheng Liu, Yiming Wang, Po Xu, Yingjie Xu, Yuntian Chen, Dongxiao Zhang)</author>
      <guid isPermaLink="false">2507.20838v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning</title>
      <link>http://arxiv.org/abs/2507.20498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MoKGR的知识图谱推理框架，通过专家混合方法实现个性化路径探索，解决了现有图神经网络在推理路径探索上的局限性。&lt;h4&gt;背景&lt;/h4&gt;知识图谱推理在自然语言处理系统中扮演着重要角色，其有效性依赖于构建信息丰富且与上下文相关的推理路径。然而，现有的图神经网络采用僵化、与查询无关的路径探索策略，限制了它们适应不同语言环境和语义细微差别的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有图神经网络在知识图谱推理中的局限性，提出一种能够根据查询特性进行个性化路径探索的框架。&lt;h4&gt;方法&lt;/h4&gt;提出了MoKGR框架，它包含两个互补组件：(1)长度专家混合，根据查询复杂度自适应选择和加权候选路径长度，提供查询特定的推理深度；(2)剪枝专家混合，从互补角度评估候选路径，为每个查询保留信息最丰富的路径。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的基准测试中，MoKGR在直推和归纳设置中都表现出优越的性能，验证了个性化路径探索在知识图谱推理中的有效性。&lt;h4&gt;结论&lt;/h4&gt;个性化路径探索能够显著提升知识图谱推理的性能，MoKGR框架通过根据查询特性自适应调整推理路径，有效解决了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱推理旨在从结构化知识库中推断新事实，在自然语言处理系统中扮演着至关重要的角色。其有效性关键在于构建信息丰富且与上下文相关的推理路径。然而，现有的图神经网络往往采用僵化、与查询无关的路径探索策略，限制了它们适应多样化语言环境和语义细微差别的能力。为解决这些局限性，我们提出了MoKGR，一种专家混合框架，通过两个互补组件实现个性化路径探索：(1)长度专家混合，根据查询复杂度自适应选择和加权候选路径长度，提供查询特定的推理深度；(2)剪枝专家混合，从互补角度评估候选路径，为每个查询保留信息最丰富的路径。在多样化基准上的全面实验表明，MoKGR在直推和归纳设置中均表现出优越性能，验证了知识图谱推理中个性化路径探索的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graph (KG) reasoning, which aims to infer new facts from structuredknowledge repositories, plays a vital role in Natural Language Processing (NLP)systems. Its effectiveness critically depends on constructing informative andcontextually relevant reasoning paths. However, existing graph neural networks(GNNs) often adopt rigid, query-agnostic path-exploration strategies, limitingtheir ability to adapt to diverse linguistic contexts and semantic nuances. Toaddress these limitations, we propose \textbf{MoKGR}, a mixture-of-expertsframework that personalizes path exploration through two complementarycomponents: (1) a mixture of length experts that adaptively selects and weightscandidate path lengths according to query complexity, providing query-specificreasoning depth; and (2) a mixture of pruning experts that evaluates candidatepaths from a complementary perspective, retaining the most informative pathsfor each query. Through comprehensive experiments on diverse benchmark, MoKGRdemonstrates superior performance in both transductive and inductive settings,validating the effectiveness of personalized path exploration in KGs reasoning.</description>
      <author>example@mail.com (Enjun Du, Siyi Liu, Yongqi Zhang)</author>
      <guid isPermaLink="false">2507.20498v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Shapley-Value-Based Graph Sparsification for GNN Inference</title>
      <link>http://arxiv.org/abs/2507.20460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Shapley值的图稀疏化方法，通过评估图的多个子集为节点预测分配正负贡献，从而在保持预测性能的同时显著降低图复杂度，提高GNN推理的可解释性和效率。&lt;h4&gt;背景&lt;/h4&gt;图稀疏化是提高图神经网络推理效率的关键技术，通过移除对预测影响最小的边来改善推理效率。传统的GNN可解释性方法生成局部重要性分数，可聚合为全局分数用于图稀疏化。&lt;h4&gt;目的&lt;/h4&gt;解决现有可解释性方法仅产生非负分数而限制其在稀疏化中应用的问题，开发一种能够保留有影响力边并移除误导性或对抗性连接的图稀疏化方法。&lt;h4&gt;方法&lt;/h4&gt;基于Shapley值的方法为节点预测分配正负贡献，通过评估图的多个子集提供理论上稳健和公平的重要性分配。与基于梯度或扰动的方法相比，Shapley值能够实现更好的剪枝策略。&lt;h4&gt;主要发现&lt;/h4&gt;基于Shapley值的图稀疏化能够在保持预测性能的同时显著降低图复杂度，提高GNN推理的可解释性和效率。&lt;h4&gt;结论&lt;/h4&gt;Shapley值based图稀疏化是一种有效的方法，可以平衡预测性能和计算效率，同时增强模型的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;图稀疏化是通过移除对预测影响最小的边来提高图神经网络推理效率的关键技术。GNN可解释性方法生成局部重要性分数，可聚合为全局分数用于图稀疏化。然而，许多可解释性方法仅产生非负分数，限制了它们在稀疏化中的应用。相比之下，基于Shapley值的方法为节点预测分配正负贡献，通过评估图的多个子集提供理论上稳健和公平的重要性分配。与基于梯度或扰动的方法不同，Shapley值能够实现更好的剪枝策略，保留有影响力的边同时移除误导性或对抗性连接。我们的研究表明，基于Shapley值的图稀疏化在保持预测性能的同时显著降低了图复杂度，提高了GNN推理的可解释性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph sparsification is a key technique for improving inference efficiency inGraph Neural Networks by removing edges with minimal impact on predictions. GNNexplainability methods generate local importance scores, which can beaggregated into global scores for graph sparsification. However, manyexplainability methods produce only non-negative scores, limiting theirapplicability for sparsification. In contrast, Shapley value based methodsassign both positive and negative contributions to node predictions, offering atheoretically robust and fair allocation of importance by evaluating manysubsets of graphs. Unlike gradient-based or perturbation-based explainers,Shapley values enable better pruning strategies that preserve influential edgeswhile removing misleading or adversarial connections. Our approach shows thatShapley value-based graph sparsification maintains predictive performance whilesignificantly reducing graph complexity, enhancing both interpretability andefficiency in GNN inference.</description>
      <author>example@mail.com (Selahattin Akkas, Ariful Azad)</author>
      <guid isPermaLink="false">2507.20460v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction</title>
      <link>http://arxiv.org/abs/2507.20451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为STARN-GAT的多模态时空图注意力网络，用于准确预测交通事故严重程度。该模型能够有效捕捉空间、时间和上下文变量之间的复杂关系，在两个数据集上都表现出色，具有良好的可解释性和实际应用潜力。&lt;h4&gt;背景&lt;/h4&gt;准确预测交通事故严重程度对改善道路安全、优化应急响应策略和设计更安全的交通基础设施至关重要。然而，现有方法往往难以有效建模影响事故结果的空间、时间和上下文变量之间的复杂相互依赖关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模空间、时间和上下文变量之间复杂相互依赖关系的模型，以准确预测交通事故严重程度，从而提高道路安全、优化应急响应和设计更安全的交通基础设施。&lt;h4&gt;方法&lt;/h4&gt;作者提出了STARN-GAT（多模态时空图注意力网络），该方法采用自适应图构建和模态感知注意力机制来捕捉复杂关系。与传统方法不同，STARN-GAT在统一的基于注意力的框架内整合了道路网络拓扑、时间交通模式和上下文环境。&lt;h4&gt;主要发现&lt;/h4&gt;在FARS数据集上，STARN-GAT实现了严重事件的Macro F1分数为85%，ROC-AUC为0.91，召回率为81%；在ARI-BUET交通事故数据集上，实现了Macro F1分数为0.84，召回率为0.78，ROC-AUC为0.89。该模型能够有效识别高风险案例，有潜力部署到实时、安全关键的交通管理系统中。基于注意力的架构增强了可解释性，提供了对影响因素的见解并支持AI辅助决策的信任。&lt;h4&gt;结论&lt;/h4&gt;STARN-GAT弥合了先进图神经网络技术与道路安全分析实际应用之间的差距，为交通事故严重程度预测提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;准确预测交通事故严重程度对于改善道路安全、优化应急响应策略以及设计更安全的交通基础设施至关重要。然而，现有方法往往难以有效建模影响事故结果的空间、时间和上下文变量之间复杂的相互依赖关系。在本研究中，我们引入了STARN-GAT，一种多模态时空图注意力网络，它利用自适应图构建和模态感知注意力机制来捕捉这些复杂关系。与传统方法不同，STARN-GAT在统一的基于注意力的框架内整合了道路网络拓扑、时间交通模式和上下文环境。该模型在FARS数据集上进行了评估，严重事件的Macro F1分数达到85%，ROC-AUC为0.91，召回率为81%。为确保在南亚背景下的泛化能力，STARN-GAT在ARI-BUET交通事故数据集上进一步验证，实现了Macro F1分数为0.84，召回率为0.78，ROC-AUC为0.89。这些结果证明了该模型在识别高风险案例方面的有效性及其在实时、安全关键交通管理系统中部署的潜力。此外，基于注意力的架构增强了可解释性，提供了对影响因素的见解并支持AI辅助决策的信任。总体而言，STARN-GAT弥合了先进图神经网络技术与道路安全分析实际应用之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of traffic accident severity is critical for improvingroad safety, optimizing emergency response strategies, and informing the designof safer transportation infrastructure. However, existing approaches oftenstruggle to effectively model the intricate interdependencies among spatial,temporal, and contextual variables that govern accident outcomes. In thisstudy, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph AttentionNetwork, which leverages adaptive graph construction and modality-awareattention mechanisms to capture these complex relationships. Unlikeconventional methods, STARN-GAT integrates road network topology, temporaltraffic patterns, and environmental context within a unified attention-basedframework. The model is evaluated on the Fatality Analysis Reporting System(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, andrecall of 81 percent for severe incidents. To ensure generalizability withinthe South Asian context, STARN-GAT is further validated on the ARI-BUET trafficaccident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,and ROC-AUC of 0.89. These results demonstrate the model's effectiveness inidentifying high-risk cases and its potential for deployment in real-time,safety-critical traffic management systems. Furthermore, the attention-basedarchitecture enhances interpretability, offering insights into contributingfactors and supporting trust in AI-assisted decision-making. Overall, STARN-GATbridges the gap between advanced graph neural network techniques and practicalapplications in road safety analytics.</description>
      <author>example@mail.com (Pritom Ray Nobin, Imran Ahammad Rifat)</author>
      <guid isPermaLink="false">2507.20451v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data Analysis Tool</title>
      <link>http://arxiv.org/abs/2507.20440v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure, 2 tables; Software available on PyPI as  BioNeuralNet. For documentation, tutorials, and workflows see  https://bioneuralnet.readthedocs.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BioNeuralNet是一个灵活且模块化的Python框架，专门用于端到端的基于网络的多组学数据分析，利用图神经网络学习有生物学意义的低维表示。&lt;h4&gt;背景&lt;/h4&gt;多组学数据为复杂生物系统提供了前所未有的洞察，但这些数据的高维度、稀疏性和复杂相互作用带来了显著的分析挑战。基于网络的方法通过有效捕获分子实体间的生物学相关关系推动了多组学研究。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门设计的工具，能够有效利用网络表示进行多样化的下游分析，填补现有方法的空白。&lt;h4&gt;方法&lt;/h4&gt;BioNeuralNet利用图神经网络(GNNs)从多组学网络中学习有生物学意义的低维表示，将复杂的分子网络转换为通用的嵌入表示。它支持多种网络构建技术、低维表示生成和广泛的下游分析任务，并提供多种GNN架构。&lt;h4&gt;主要发现&lt;/h4&gt;BioNeuralNet支持多组学网络分析的所有主要阶段，与成熟的Python包(如scikit-learn, PyTorch, NetworkX)兼容，增强了可用性并促进了快速采用。&lt;h4&gt;结论&lt;/h4&gt;BioNeuralNet是一个开源、用户友好、有详细文档的框架，设计用于支持精准医学中灵活且可复现的多组学网络分析。&lt;h4&gt;翻译&lt;/h4&gt;多组学数据为复杂生物系统提供了前所未有的洞察，然而它们的高维度、稀疏性和复杂的相互作用带来了显著的分析挑战。基于网络的方法通过有效捕获分子实体之间的生物学相关关系，推动了多组学研究。虽然这些方法在表示分子相互作用方面很强大，但仍然需要专门设计的工具来有效利用这些网络表示进行多样化的下游分析。为了满足这一需求，我们介绍了BioNeuralNet，一个灵活且模块化的Python框架，专为端到端的基于网络的多组学数据分析而设计。BioNeuralNet利用图神经网络(GNNs)从多组学网络中学习有生物学意义的低维表示，将这些复杂的分子网络转换为通用的嵌入表示。BioNeuralNet支持多组学网络分析的所有主要阶段，包括多种网络构建技术、低维表示生成和广泛的下游分析任务。其广泛的实用功能，包括多样的GNN架构，以及对成熟Python包(如scikit-learn, PyTorch, NetworkX)的兼容性，提高了可用性并促进了快速采用。BioNeuralNet是一个开源、用户友好且有详细文档的框架，旨在支持精准医学中灵活且可复现的多组学网络分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-omics data offer unprecedented insights into complex biologicalsystems, yet their high dimensionality, sparsity, and intricate interactionspose significant analytical challenges. Network-based approaches have advancedmulti-omics research by effectively capturing biologically relevantrelationships among molecular entities. While these methods are powerful forrepresenting molecular interactions, there remains a need for toolsspecifically designed to effectively utilize these network representationsacross diverse downstream analyses. To fulfill this need, we introduceBioNeuralNet, a flexible and modular Python framework tailored for end-to-endnetwork-based multi-omics data analysis. BioNeuralNet leverages Graph NeuralNetworks (GNNs) to learn biologically meaningful low-dimensionalrepresentations from multi-omics networks, converting these complex molecularnetworks into versatile embeddings. BioNeuralNet supports all major stages ofmulti-omics network analysis, including several network constructiontechniques, generation of low-dimensional representations, and a broad range ofdownstream analytical tasks. Its extensive utilities, including diverse GNNarchitectures, and compatibility with established Python packages (e.g.,scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quickadoption. BioNeuralNet is an open-source, user-friendly, and extensivelydocumented framework designed to support flexible and reproducible multi-omicsnetwork analysis in precision medicine.</description>
      <author>example@mail.com (Vicente Ramos, Sundous Hussein, Mohamed Abdel-Hafiz, Arunangshu Sarkar, Weixuan Liu, Katerina J. Kechris, Russell P. Bowler, Leslie Lange, Farnoush Banaei-Kashani)</author>
      <guid isPermaLink="false">2507.20440v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery</title>
      <link>http://arxiv.org/abs/2507.20349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的新概率框架，用于从观测数据中进行因果发现，能够学习整个因果图空间的概率分布，而非仅输出单一确定性图。该方法在合成和真实世界数据集上展现出优越的性能，显著提升了因果结构学习能力。&lt;h4&gt;背景&lt;/h4&gt;因果发现从观测数据中具有挑战性，尤其是处理大型数据集和复杂关系时。传统方法在可扩展性和捕获全局结构信息方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;克服传统方法的局限性，引入一种基于图神经网络（GNN）的新概率框架，学习整个因果图空间的概率分布，而不是输出单个确定性图。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络编码节点和边属性为统一的图表示，使模型能够直接从数据中学习复杂的因果结构。在多样化的合成数据集上训练GNN模型，这些数据集通过统计和信息论度量（如互信息和条件熵）进行了增强，捕获局部和全局数据特性。将因果发现作为监督学习问题，直接预测整个图结构。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在准确性和可扩展性方面表现优越，在合成和真实世界数据集上优于传统方法、最近的非GNN方法以及一种基于GNN的方法，无需进一步训练即可获得良好性能。&lt;h4&gt;结论&lt;/h4&gt;这种概率框架显著改善了因果结构学习，对各个领域的决策制定和科学发现有广泛影响。&lt;h4&gt;翻译&lt;/h4&gt;从观测数据中进行因果发现具有挑战性，特别是在处理大型数据集和复杂关系时。传统方法往往难以应对可扩展性和捕获全局结构信息的问题。为了克服这些局限性，我们引入了一种新颖的基于图神经网络（GNN）的概率框架，该方法学习整个因果图空间的概率分布，不同于输出单一确定性图的方法。我们的框架利用了一种能够将节点和边属性编码为统一图表示的GNN，使模型能够直接从数据中学习复杂的因果结构。GNN模型在多样化的合成数据集上进行训练，这些数据集通过统计和信息论度量（如互信息和条件熵）进行了增强，捕获了局部和全局数据特性。我们将因果发现构建为监督学习问题，直接预测整个图结构。我们的方法在准确性和可扩展性方面表现出色，在合成和真实世界数据集上优于传统方法和最近的非GNN方法，以及一种基于GNN的方法，且无需进一步训练。这种概率框架显著改善了因果结构学习，对各个领域的决策制定和科学发现具有广泛意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal discovery from observational data is challenging, especially withlarge datasets and complex relationships. Traditional methods often strugglewith scalability and capturing global structural information. To overcome theselimitations, we introduce a novel graph neural network (GNN)-basedprobabilistic framework that learns a probability distribution over the entirespace of causal graphs, unlike methods that output a single deterministicgraph. Our framework leverages a GNN that encodes both node and edge attributesinto a unified graph representation, enabling the model to learn complex causalstructures directly from data. The GNN model is trained on a diverse set ofsynthetic datasets augmented with statistical and information-theoreticmeasures, such as mutual information and conditional entropy, capturing bothlocal and global data properties. We frame causal discovery as a supervisedlearning problem, directly predicting the entire graph structure. Our approachdemonstrates superior performance, outperforming both traditional and recentnon-GNN-based methods, as well as a GNN-based approach, in terms of accuracyand scalability on synthetic and real-world datasets without further training.This probabilistic framework significantly improves causal structure learning,with broad implications for decision-making and scientific discovery acrossvarious fields.</description>
      <author>example@mail.com (Rezaur Rashid, Gabriel Terejanu)</author>
      <guid isPermaLink="false">2507.20349v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2507.20226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了HFrame，这是首个基于图神经网络的子图同态框架，结合了传统算法与机器学习技术，在保持高准确率的同时大幅提升了计算效率。&lt;h4&gt;背景&lt;/h4&gt;图同态是一种保持图结构的关键映射技术。子图同态问题涉及在给定图中找到一个映射，将模式图中的相邻顶点映射到目标图中的相邻顶点。与子图同构不同，同态允许模式中的多个顶点映射到目标图的同一顶点，使其更为复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的子图同态解决方案，能够处理传统方法难以解决的复杂映射问题，并提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出HFrame框架，这是一种基于图神经网络的子图同态方法，集成了传统算法与机器学习技术，能够区分模式与图不同构的情况。&lt;h4&gt;主要发现&lt;/h4&gt;HFrame能够比标准图神经网络区分更多模式与图不同构的图对；提供了HFrame的泛化误差界限；实验表明HFrame比精确匹配算法快101.91倍，平均准确率达到0.962。&lt;h4&gt;结论&lt;/h4&gt;HFrame作为首个基于图神经网络的子图同态框架，在保持高准确率的同时显著提高了计算效率，为图同态问题提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;同态是图之间保持其结构的一种关键映射技术。给定一个图和一个模式，子图同态问题涉及找到一个从模式到图的映射，确保模式中的相邻顶点映射到图中的相邻顶点。与需要一对一映射的子图同构不同，同态允许模式中的多个顶点映射到图中的同一个顶点，使其更加复杂。我们提出了HFrame，这是第一个基于图神经网络的子图同态框架，它将传统算法与机器学习技术相结合。我们证明HFrame通过能够区分更多模式与图不同构的图对，优于标准图神经网络。此外，我们为HFrame提供了泛化误差界限。通过对真实世界和合成图的实验，我们表明HFrame比精确匹配算法快101.91倍，并达到0.962的平均准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Homomorphism is a key mapping technique between graphs that preserves theirstructure. Given a graph and a pattern, the subgraph homomorphism probleminvolves finding a mapping from the pattern to the graph, ensuring thatadjacent vertices in the pattern are mapped to adjacent vertices in the graph.Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphismallows multiple vertices in the pattern to map to the same vertex in the graph,making it more complex. We propose HFrame, the first graph neural network-basedframework for subgraph homomorphism, which integrates traditional algorithmswith machine learning techniques. We demonstrate that HFrame outperformsstandard graph neural networks by being able to distinguish more graph pairswhere the pattern is not homomorphic to the graph. Additionally, we provide ageneralization error bound for HFrame. Through experiments on both real-worldand synthetic graphs, we show that HFrame is up to 101.91 times faster thanexact matching algorithms and achieves an average accuracy of 0.962.</description>
      <author>example@mail.com (Shuyang Guo, Wenjin Xie, Ping Lu, Ting Deng, Richong Zhang, Jianxin Li, Xiangping Huang, Zhongyi Liu)</author>
      <guid isPermaLink="false">2507.20226v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Integrating LLM-Derived Multi-Semantic Intent into Graph Model for Session-based Recommendation</title>
      <link>http://arxiv.org/abs/2507.20147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LLM-DMsRec的新型会话推荐方法，通过整合大型语言模型衍生的多语义意图与图神经网络的结构意图，显著提升了推荐性能。&lt;h4&gt;背景&lt;/h4&gt;当前主流的会话推荐系统主要基于图神经网络(GNNs)建模用户交互序列，但这些方法主要关注会话序列的ID信息，忽略了其中丰富的语义信息，限制了准确推断用户真实意图的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有GNN-based SBR方法忽视语义信息的问题，通过整合多语义意图提升推荐系统准确捕捉用户意图的能力。&lt;h4&gt;方法&lt;/h4&gt;LLM-DMsRec方法利用预训练GNN模型选择top-k候选项目集，设计提示和大型语言模型(LLM)推断多语义意图，并提出对齐机制将LLM的语义意图与GNN的结构意图有效结合。&lt;h4&gt;主要发现&lt;/h4&gt;在Beauty和ML-1M数据集上的实验表明，该方法能无缝集成到GNN框架中，显著提升推荐性能。&lt;h4&gt;结论&lt;/h4&gt;通过结合LLM的语义理解能力和GNN的结构建模能力，所提出的方法能够更准确地捕捉用户的多维度意图，从而提高会话推荐的准确性。&lt;h4&gt;翻译&lt;/h4&gt;会话推荐(SBR)主要基于匿名用户交互序列来推荐用户最可能点击的下一个项目。目前，最流行且高性能的SBR方法主要利用图神经网络(GNNs)，将会话序列建模为图结构数据以有效捕捉用户意图。然而，大多数基于GNN的SBR方法主要关注会话序列的ID序列信息，而忽略了其中丰富的语义信息。这一限制显著阻碍了模型准确推断用户真实意图的能力。为解决上述挑战，本文提出了一种名为'将LLM衍生的多语义意图集成到图模型中进行会话推荐'(LLM-DMsRec)的新型SBR方法。该方法利用预训练的GNN模型选择前k个项目作为候选项目集，并设计提示和大型语言模型(LLM)从这些候选项目中推断多语义意图。具体来说，我们提出了一种对齐机制，有效地将LLM推断的语义意图与GNN捕获的结构意图结合起来。在Beauty和ML-1M数据集上进行的广泛实验表明，所提出的方法可以无缝集成到GNN框架中，显著提高其推荐性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Session-based recommendation (SBR) is mainly based on anonymous userinteraction sequences to recommend the items that the next user is most likelyto click. Currently, the most popular and high-performing SBR methods primarilyleverage graph neural networks (GNNs), which model session sequences asgraph-structured data to effectively capture user intent. However, mostGNNs-based SBR methods primarily focus on modeling the ID sequence informationof session sequences, while neglecting the rich semantic information embeddedwithin them. This limitation significantly hampers model's ability toaccurately infer users' true intention. To address above challenge, this paperproposes a novel SBR approach called Integrating LLM-Derived Multi-SemanticIntent into Graph Model for Session-based Recommendation (LLM-DMsRec). Themethod utilizes a pre-trained GNN model to select the top-k items as candidateitem sets and designs prompts along with a large language model (LLM) to infermulti-semantic intents from these candidate items. Specifically, we propose analignment mechanism that effectively integrates the semantic intent inferred bythe LLM with the structural intent captured by GNNs. Extensive experimentsconducted on the Beauty and ML-1M datasets demonstrate that the proposed methodcan be seamlessly integrated into GNNs framework, significantly enhancing itsrecommendation performance.</description>
      <author>example@mail.com (Shuo Zhang, Xiao Li, Jiayi Wu, Fan Yang, Xiang Li, Ming Gao)</author>
      <guid isPermaLink="false">2507.20147v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing</title>
      <link>http://arxiv.org/abs/2507.20127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'聚合感知多层感知机'(AMLP)的新型无监督框架，用于解决图神经网络中聚合函数选择不当的问题，通过使MLP适应聚合方式而非直接设计聚合函数，提高了模型在异质图情况下的性能和适用性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习图表示的主导方法，主要归功于其消息传递机制。然而，GNNs通常采用固定的聚合函数(如均值、最大值或总和)，缺乏对这些选择的合理推理。这种固定性在异质图存在的情况下，往往导致较差的、依赖于具体问题的性能。虽然一些方法尝试设计更复杂的聚合函数，但这些方法往往严重依赖标记数据，而现实世界任务中标记数据通常稀缺。&lt;h4&gt;目的&lt;/h4&gt;提出一种无监督框架，解决GNNs中聚合函数选择不当的问题，特别是在异质图情况下，减少对标记数据的依赖，提高模型在图学习任务中的性能和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出名为'聚合感知多层感知机'(AMLP)的无监督框架，包含两个关键步骤：1) 利用图重建方法促进高阶分组效应；2) 使用单层网络编码不同程度的异质性，从而提高模型的容量和适用性。&lt;h4&gt;主要发现&lt;/h4&gt;在节点聚类和分类任务上的大量实验表明，AMLP具有优越的性能，突显了其在多样化图学习场景中的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过将范式从直接设计聚合函数转变为使MLP适应聚合，AMLP提供了一种轻量级解决方案，能够有效处理异质图情况下的图学习问题，且不需要大量标记数据。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图表示的主导方法，主要归功于其消息传递机制。然而，GNNs通常采用固定的聚合函数，如均值、最大值或总和，而没有对选择进行合理的推理。这种固定性，特别是在异质图存在的情况下，往往导致较差的、依赖于具体问题的性能。尽管一些尝试通过设计更复杂的聚合函数来解决这一问题，但这些方法往往严重依赖标记数据，而现实世界任务中标记数据通常稀缺。在这项工作中，我们提出了一种新颖的无监督框架'聚合感知多层感知机'(AMLP)，它将范式从直接设计聚合函数转变为使MLP适应聚合。我们的轻量级方法包含两个关键步骤：首先，我们利用图重建方法促进高阶分组效应；其次，我们使用单层网络编码不同程度的异质性，从而提高模型的容量和适用性。在节点聚类和分类任务上的大量实验证明了AMLP的优越性能，突显了其在多样化图学习场景中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a dominant approach to learninggraph representations, primarily because of their message-passing mechanisms.However, GNNs typically adopt a fixed aggregator function such as Mean, Max, orSum without principled reasoning behind the selection. This rigidity,especially in the presence of heterophily, often leads to poor, problemdependent performance. Although some attempts address this by designing moresophisticated aggregation functions, these methods tend to rely heavily onlabeled data, which is often scarce in real-world tasks. In this work, wepropose a novel unsupervised framework, "Aggregation-aware MultilayerPerceptron" (AMLP), which shifts the paradigm from directly craftingaggregation functions to making MLP adaptive to aggregation. Our lightweightapproach consists of two key steps: First, we utilize a graph reconstructionmethod that facilitates high-order grouping effects, and second, we employ asingle-layer network to encode varying degrees of heterophily, therebyimproving the capacity and applicability of the model. Extensive experiments onnode clustering and classification demonstrate the superior performance ofAMLP, highlighting its potential for diverse graph learning scenarios.</description>
      <author>example@mail.com (Xuanting Xie, Bingheng Li, Erlin Pan, Zhao Kang, Wenyu Chen)</author>
      <guid isPermaLink="false">2507.20127v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Graded Transformers: A Symbolic-Geometric Approach to Structured Learning</title>
      <link>http://arxiv.org/abs/2507.20108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了Graded Transformer框架，一种新型序列模型，通过向量空间的分级变换嵌入代数归纳偏置。基于分级神经网络理论，提出了线性分级Transformer和指数分级Transformer两种架构，提供了严格的理论保证，并在多个领域展示了广泛应用潜力。&lt;h4&gt;背景&lt;/h4&gt;研究基于分级神经网络的理论，旨在克服先前工作中固定分级的局限性，为结构化数据提供更高效的模型。&lt;h4&gt;目的&lt;/h4&gt;创建能够处理结构化数据的高效模型，融合几何和代数原理与注意力机制，提供数据驱动模型的数学替代方案，实现自适应特征优先级。&lt;h4&gt;方法&lt;/h4&gt;应用参数化缩放算子，使用固定或可学习的分级元组和指数因子，引入分级损失函数确保梯度稳定性和与领域先验的一致性，将分级视为可微分参数实现自适应特征优先级。&lt;h4&gt;主要发现&lt;/h4&gt;获得了连续和Sobolev函数的通用逼近定理，通过有效VC维界降低了样本复杂度，证明了分级操作的Lipschitz连续性，展示了模型对对抗性扰动的鲁棒性，以及分级损失函数在优化过程中的梯度稳定性和领域先验对齐能力。&lt;h4&gt;结论&lt;/h4&gt;Graded Transformer为结构化深度学习提供了新方法，融合了几何和代数原理与注意力机制，为复杂领域中的可解释、高效系统铺平了道路，在代数几何、物理、自然语言处理、生物序列分析和图神经网络等领域有广泛应用前景。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了Graded Transformer框架，一种新型的序列模型类别，通过向量空间的分级变换嵌入代数归纳偏置。扩展了分级神经网络的理论，我们提出了两种架构：线性分级Transformer和指数分级Transformer。这些模型应用参数化缩放算子，由固定或可学习的分级元组和指数因子（针对EGT）控制，为注意力层和表示层注入分层结构，增强结构化数据的处理效率。我们推导了严格的理论保证，包括连续和Sobolev函数的通用逼近定理，通过有效VC维界降低样本复杂度，分级操作的Lipschitz连续性，以及对对抗性扰动的鲁棒性。分级损失函数确保了优化过程中的梯度稳定性和与领域先验的一致性。通过将分级视为可微分参数，该框架实现了自适应特征优先级，克服了先前工作中固定分级的局限性。Graded Transformer对分层学习和神经符号推理具有变革潜力，应用范围涵盖代数几何（如模空间和zeta函数）、物理（如多尺度模拟）、自然语言处理（如句法分析）、生物序列分析（如变体预测）以及图神经网络和金融建模等新兴领域。这项工作通过融合几何和代数原理与注意力机制，推动了结构化深度学习的发展，为数据驱动模型提供了数学上合理的替代方案，并为复杂领域中的可解释、高效系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the Graded Transformer framework, a novel class of sequencemodels that embeds algebraic inductive biases through grading transformationson vector spaces. Extending the theory of Graded Neural Networks (GNNs), wepropose two architectures: the Linearly Graded Transformer (LGT) and theExponentially Graded Transformer (EGT). These models apply parameterizedscaling operators-governed by fixed or learnable grading tuples and, for EGT,exponential factors to infuse hierarchical structure into attention andrepresentation layers, enhancing efficiency for structured data.  We derive rigorous theoretical guarantees, including universal approximationtheorems for continuous and Sobolev functions, reduced sample complexity viaeffective VC dimension bounds, Lipschitz continuity of graded operations, androbustness to adversarial perturbations. A graded loss function ensuresgradient stability and alignment with domain priors during optimization. Bytreating grades as differentiable parameters, the framework enables adaptivefeature prioritization, overcoming limitations of fixed grades in prior work.  The Graded Transformer holds transformative potential for hierarchicallearning and neurosymbolic reasoning, with applications spanning algebraicgeometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscalesimulations), natural language processing (e.g., syntactic parsing), biologicalsequence analysis (e.g., variant prediction), and emerging areas like graphneural networks and financial modeling. This work advances structured deeplearning by fusing geometric and algebraic principles with attentionmechanisms, offering a mathematically grounded alternative to data-drivenmodels and paving the way for interpretable, efficient systems in complexdomains.</description>
      <author>example@mail.com (Tony Shaska Sr)</author>
      <guid isPermaLink="false">2507.20108v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2507.19964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了针对联邦图神经网络(FedGNNs)的跨客户端成员推断攻击(CC-MIA)，提出了一种利用FedGNNs聚合行为、梯度更新和嵌入相似性的攻击框架，能够在跨训练轮次中将样本链接到其源客户端。&lt;h4&gt;背景&lt;/h4&gt;图结构数据在社交网络、金融系统和分子生物学等现实世界应用中很普遍，图神经网络(GNNs)因其强大的表示能力已成为学习此类数据的公认标准。随着GNNs在联邦学习环境中的部署越来越多，以保留数据本地性和隐私，新的隐私威胁随之出现。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在对联邦GNNs节点分类任务中的跨客户端成员推断攻击进行系统性研究，揭示联邦环境中特有的样本到客户端归属这一更细粒度的隐私风险。&lt;h4&gt;方法&lt;/h4&gt;设计了一个通用的攻击框架，该框架利用FedGNNs的聚合行为、梯度更新和嵌入相似性来跨训练轮次将样本链接到其源客户端。&lt;h4&gt;主要发现&lt;/h4&gt;在多种图数据集和现实的FL设置下评估显示，该方法在成员推断和所有权识别方面都取得了高性能，表明通过结构和模型级线索会导致客户端身份泄露这一新的隐私威胁。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了联邦图学习中的一种新隐私威胁——通过结构和模型级线索导致的客户端身份泄露，这促使需要设计具有归属鲁棒性的GNN。&lt;h4&gt;翻译&lt;/h4&gt;Graph-structured data is prevalent in many real-world applications, including social networks, financial systems, and molecular biology. Graph Neural Networks (GNNs) have become the de facto standard for learning from such data due to their strong representation capabilities. As GNNs are increasingly deployed in federated learning (FL) settings to preserve data locality and privacy, new privacy threats arise from the interaction between graph structures and decentralized training. In this paper, we present the first systematic study of cross-client membership inference attacks (CC-MIA) against node classification tasks of federated GNNs (FedGNNs), where a malicious client aims to infer which client owns the given data. Unlike prior centralized-focused work that focuses on whether a sample was included in training, our attack targets sample-to-client attribution, a finer-grained privacy risk unique to federated settings. We design a general attack framework that exploits FedGNNs' aggregation behaviors, gradient updates, and embedding proximity to link samples to their source clients across training rounds. We evaluate our attack across multiple graph datasets under realistic FL setups. Results show that our method achieves high performance on both membership inference and ownership identification. Our findings highlight a new privacy threat in federated graph learning-client identity leakage through structural and model-level cues, motivating the need for attribution-robust GNN design.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-structured data is prevalent in many real-world applications, includingsocial networks, financial systems, and molecular biology. Graph NeuralNetworks (GNNs) have become the de facto standard for learning from such datadue to their strong representation capabilities. As GNNs are increasinglydeployed in federated learning (FL) settings to preserve data locality andprivacy, new privacy threats arise from the interaction between graphstructures and decentralized training. In this paper, we present the firstsystematic study of cross-client membership inference attacks (CC-MIA) againstnode classification tasks of federated GNNs (FedGNNs), where a malicious clientaims to infer which client owns the given data. Unlike priorcentralized-focused work that focuses on whether a sample was included intraining, our attack targets sample-to-client attribution, a finer-grainedprivacy risk unique to federated settings. We design a general attack frameworkthat exploits FedGNNs' aggregation behaviors, gradient updates, and embeddingproximity to link samples to their source clients across training rounds. Weevaluate our attack across multiple graph datasets under realistic FL setups.Results show that our method achieves high performance on both membershipinference and ownership identification. Our findings highlight a new privacythreat in federated graph learning-client identity leakage through structuraland model-level cues, motivating the need for attribution-robust GNN design.</description>
      <author>example@mail.com (Kunhao Li, Di Wu, Jun Bai, Jing Xu, Lei Yang, Ziyi Zhang, Yiliao Song, Wencheng Yang, Taotao Cai, Yan Li)</author>
      <guid isPermaLink="false">2507.19964v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>NAICS-Aware Graph Neural Networks for Large-Scale POI Co-visitation Prediction: A Multi-Modal Dataset and Methodology</title>
      <link>http://arxiv.org/abs/2507.19697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了NAICS-aware GraphSAGE，一种新型图神经网络，通过整合商业分类知识来预测大规模共同访问模式，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;了解人们在访问商业场所后的去向对城市规划、零售分析和位置服务至关重要。然而，预测数百万场所间的共同访问模式面临数据稀疏和空间-商业关系复杂性的挑战，传统地理距离方法无法解释不同类型商业场所的客流差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够整合商业分类知识的新型图神经网络，以解决数据稀疏性和空间-商业关系复杂性带来的挑战，提高共同访问模式的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出NAICS-aware GraphSAGE，通过可学习嵌入整合商业分类知识，利用行业代码捕捉商业语义。通过高效状态分解扩展到42亿个场所对的大规模数据，并在端到端框架中结合空间、时间和社会经济特征。&lt;h4&gt;主要发现&lt;/h4&gt;在包含9490万条记录的POI-Graph数据集上评估，R平方值从0.243提高到0.625（增长157%），NDCG@10提升32%，显著优于最先进基线。&lt;h4&gt;结论&lt;/h4&gt;商业语义对预测共同访问模式至关重要，所提出方法能有效处理大规模数据并显著提高预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;了解人们在访问一个商业场所后去哪里对于城市规划、零售分析和基于位置的服务至关重要。然而，由于数据极其稀疏以及空间距离与商业关系之间复杂的相互作用，预测数百万个场所之间的共同访问模式仍然具有挑战性。仅使用地理距离的传统方法无法解释为什么咖啡店和高级餐厅即使位置相近也会吸引不同的客流。我们引入了NAICS-aware GraphSAGE，一种新型图神经网络，通过可学习嵌入整合商业分类知识来预测大规模共同访问模式。我们的关键见解是，通过详细行业代码捕捉的商业语义提供了纯空间模型无法解释的重要信号。该方法通过高效的状态分解扩展到大规模数据集（42亿个可能的场所对），同时在端到端框架中结合空间、时间和经济社会特征。在我们包含92,486个品牌和48个美国州的9490万条共同访问记录的POI-Graph数据集上评估，我们的方法相比最先进基线取得了显著改进：R平方值从0.243提高到0.625（增长了157%），排名质量也有显著提升（NDCG@10提高了32%）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding where people go after visiting one business is crucial forurban planning, retail analytics, and location-based services. However,predicting these co-visitation patterns across millions of venues remainschallenging due to extreme data sparsity and the complex interplay betweenspatial proximity and business relationships. Traditional approaches using onlygeographic distance fail to capture why coffee shops attract different customerflows than fine dining restaurants, even when co-located. We introduceNAICS-aware GraphSAGE, a novel graph neural network that integrates businesstaxonomy knowledge through learnable embeddings to predict population-scaleco-visitation patterns. Our key insight is that business semantics, capturedthrough detailed industry codes, provide crucial signals that pure spatialmodels cannot explain. The approach scales to massive datasets (4.2 billionpotential venue pairs) through efficient state-wise decomposition whilecombining spatial, temporal, and socioeconomic features in an end-to-endframework. Evaluated on our POI-Graph dataset comprising 94.9 millionco-visitation records across 92,486 brands and 48 US states, our methodachieves significant improvements over state-of-the-art baselines: theR-squared value increases from 0.243 to 0.625 (a 157 percent improvement), withstrong gains in ranking quality (32 percent improvement in NDCG at 10).</description>
      <author>example@mail.com (Yazeed Alrubyli, Omar Alomeir, Abrar Wafa, Diána Hidvégi, Hend Alrasheed, Mohsen Bahrami)</author>
      <guid isPermaLink="false">2507.19697v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Gradient-based grand canonical optimization enabled by graph neural networks with fractional atomic existence</title>
      <link>http://arxiv.org/abs/2507.19438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文扩展了消息传递形式化，引入连续变量描述分数原子存在性，提出基于梯度的巨正则优化方法，并展示了其在Cu(110)表面氧化物中的应用。&lt;h4&gt;背景&lt;/h4&gt;机器学习原子势已成为材料科学中不可或缺的工具，能够研究更大系统和更长时间尺度的现象。最先进模型通常是图神经网络，采用消息传递迭代更新原子嵌入，用于预测物质属性。&lt;h4&gt;目的&lt;/h4&gt;扩展消息传递形式化，引入描述分数原子存在性的连续变量，计算吉布斯自由能相对于原子坐标和存在性的梯度，并提出基于梯度的巨正则优化方法。&lt;h4&gt;方法&lt;/h4&gt;通过在消息传递形式化中包含处理分数原子存在性的连续变量，计算吉布斯自由能相对于原子坐标和存在性的梯度，开发基于梯度的巨正则优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;所提出方法能计算吉布斯自由能相对于原子坐标和存在性的梯度，在Cu(110)表面氧化物应用中展示了其能力。&lt;h4&gt;结论&lt;/h4&gt;通过引入描述分数原子存在性的连续变量，扩展的消息传递形式化使基于梯度的巨正则优化成为可能，为材料科学研究提供了新工具。&lt;h4&gt;翻译&lt;/h4&gt;机器学习原子势已成为材料科学中不可或缺的工具，使得能够研究更大系统和更长的时间尺度。最先进的模型通常是图神经网络，采用消息传递来迭代更新原子嵌入，最终用于预测属性。在这项工作中，我们通过包含一个描述分数原子存在的连续变量来扩展消息传递形式化。这使得我们能够计算吉布斯自由能相对于原子笛卡尔坐标和它们存在性的梯度。利用这一点，我们提出了一种基于梯度的巨正则优化方法，并记录了其在Cu(110)表面氧化物方面的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning interatomic potentials have become an indispensable tool formaterials science, enabling the study of larger systems and longer timescales.State-of-the-art models are generally graph neural networks that employ messagepassing to iteratively update atomic embeddings that are ultimately used forpredicting properties. In this work we extend the message passing formalismwith the inclusion of a continuous variable that accounts for fractional atomicexistence. This allows us to calculate the gradient of the Gibbs free energywith respect to both the Cartesian coordinates of atoms and their existence.Using this we propose a gradient-based grand canonical optimization method anddocument its capabilities for a Cu(110) surface oxide.</description>
      <author>example@mail.com (Mads-Peter Verner Christiansen, Bjørk Hammer)</author>
      <guid isPermaLink="false">2507.19438v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Learning Long-Range Representations with Equivariant Messages</title>
      <link>http://arxiv.org/abs/2507.19382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为LOREM的图神经网络架构，通过使用等变电荷处理长程相互作用，解决了机器学习原子势能模型中无法有效建模长程效应的问题，并在多个数据集上展现出竞争力或更优的性能。&lt;h4&gt;背景&lt;/h4&gt;基于第一性原理参考数据训练的机器学习原子势能模型已成为计算物理、生物学和化学的重要工具。等变消息传递神经网络被认为是该任务的最先进方法，但由于需要随系统大小高效扩展，这些模型无法处理全连接原子图，忽略了超过特定截止距离的相互作用，无法建模静电、色散或电子离域等长程效应。&lt;h4&gt;目的&lt;/h4&gt;解决现有机器学习原子势能模型无法有效处理长程相互作用的问题，特别是基于原子间距离反幂律的长程校正方案无法传递高阶几何信息，适用性有限的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出使用等变电荷而非标量电荷来处理长程相互作用，并围绕这种长程消息传递机制设计了一种名为LOREM的图神经网络架构。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个长程数据集上的测试，确认等变电荷能够学习方向依赖的相互作用，LOREM模型与其他方法具有竞争力或更优。此外，LOREM不需要调整相互作用的截止距离或消息传递步骤的数量来建模长程相互作用，使其在不同系统中具有更好的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;LOREM模型有效地解决了长程相互作用建模的问题，不需要针对长程相互作用调整超参数，在不同系统中表现出良好的鲁棒性，代表了机器学习原子势能模型的一个重要进步。&lt;h4&gt;翻译&lt;/h4&gt;基于第一性原理参考数据训练的机器学习原子势能正迅速成为计算物理、生物学和化学中不可或缺的工具。等变消息传递神经网络，包括变换器，被认为是该任务的最先进方法。由于应用需要随系统大小高效扩展，此类模型无法在全连接原子图上操作，因此忽略了超过特定截止距离的相互作用，从而无法建模静电、色散或电子离域等长程效应。虽然已经提出了基于原子间距离反幂律的长程校正方案，但它们无法传递高阶几何信息，因此适用性有限。为了解决这一不足，我们提出使用等变而非标量电荷来处理长程相互作用，并围绕这种长程消息传递机制设计了一种图神经网络架构LOREM。通过在多个长程数据集上的测试，我们确认等变电荷能够学习方向依赖的相互作用，并且所提出的模型与其他方法具有竞争力或更优。此外，LOREM不需要调整相互作用的截止距离或消息传递步骤的数量来建模长程相互作用，这使其在不同系统中具有更好的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器学习原子间势能模型(MLIPs)无法有效模拟长程相互作用的问题。这个问题在现实中非常重要，因为长程相互作用(如静电作用、色散和电子离域)在材料科学、化学和生物系统中起着关键作用，直接影响我们对材料性质、化学反应和生物功能的理解。现有方法要么无法捕获这些长程效应，要么计算效率低下，限制了MLIPs在更广泛科学领域的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有MLIPs在处理长程相互作用时的局限性，包括传统消息传递神经网络受限于截断半径，物理启发的长程修正方案无法传递高阶几何信息，以及全连接图计算效率低等问题。作者借鉴了计算物理学中的Ewald求和方法、Grisafi &amp; Ceriotti的LODE框架以及Kosmala等人的Ewald消息传递工作。基于这些现有方法，作者创新性地提出使用等变电荷而非标量电荷来表示长程相互作用，并设计了LOREM架构，结合短程和长程消息传递，形成了一个统一的处理框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用等变电荷(而非标量电荷)来表示原子的长程相互作用特性，并利用Ewald求和等高效计算方法来处理这些长程相互作用。整体实现流程包括：1)将原子系统表示为几何图并初始化特征；2)通过短程消息传递更新节点特征，考虑邻近原子间的相互作用；3)将特征转换为低维电荷表示；4)使用Ewald求和计算这些电荷的长程相互作用；5)将计算得到的势能与节点特征结合，更新特征表示；6)最终预测每个原子的能量贡献，总和即为总能量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出等变长程消息传递机制，使用等变电荷而非标量电荷；2)设计了LOREM架构，结合短程和长程消息传递；3)利用Ewald求和和PME方法实现高效计算；4)提供统一的处理框架。相比之前的工作，这种方法与纯短程模型相比能处理远距离原子间的相互作用；与标量长程修正模型相比能捕捉方向相关的相互作用；与全连接模型相比计算效率更高且能处理周期系统；与其他物理启发的长程模型相比能传递高阶几何信息且无需预先指定相互作用的幂律指数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合等变电荷和高效计算方法的新型机器学习原子间势能模型(LOREM)，能够在保持计算效率的同时准确模拟物理系统中的长程相互作用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning interatomic potentials trained on first-principles referencedata are quickly becoming indispensable for computational physics, biology, andchemistry. Equivariant message-passing neural networks, including transformers,are considered state-of-the-art for this task. Since applications requireefficient scaling with system size, such models cannot act on fully connectedatomistic graphs and thus neglect interactions beyond a certain cutoff,consequently failing to model long-range effects like electrostatics,dispersion, or electron delocalization. While long-range correction schemesbased on inverse power laws of interatomic distances have been proposed, theyare unable to communicate higher-order geometric information and are thuslimited in applicability. To address this shortcoming, we propose the use ofequivariant, rather than scalar, charges for long-range interactions, anddesign a graph neural network architecture, LOREM, around this long-rangemessage passing mechanism. Through tests on a number of long-range datasets, weconfirm that equivariant charges enable the learning of orientation-dependentinteractions, and that the proposed model is competitive with, or surpasses,other approaches. Moreover, LOREM does not require adapting interaction cutoffsor the number of message passing steps to model long-range interactions, whichcontributes to its robustness across different systems.</description>
      <author>example@mail.com (Egor Rumiantsev, Marcel F. Langer, Tulga-Erdene Sodjargal, Michele Ceriotti, Philip Loche)</author>
      <guid isPermaLink="false">2507.19382v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>EffiComm: Bandwidth Efficient Multi Agent Communication</title>
      <link>http://arxiv.org/abs/2507.19354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at ITSC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为EffiComm的端到端框架，用于车辆间协作感知，通过选择性传输和自适应网格减少技术，仅需传输不到先前方法40%的数据量，同时保持最先进的3D目标检测精度。&lt;h4&gt;背景&lt;/h4&gt;协作感知允许互联车辆交换传感器信息并克服每辆车的盲点。然而，传输原始点云或完整特征图会压垮车对车通信，导致延迟和可扩展性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的通信框架，减少协作感知中的数据传输量，同时保持高精度的3D目标检测能力。&lt;h4&gt;方法&lt;/h4&gt;EffiComm框架操作于任何模态的鸟瞰图特征图，并应用两阶段减少管道：(1) 选择性传输使用置信度掩码修剪低效用区域；(2) 自适应网格减少使用图神经网络根据角色和网络负载分配车辆特定的保持比例。剩余的特征通过软门控专家混合注意力层融合，提供更大的容量和专业化，以实现有效的特征集成。&lt;h4&gt;主要发现&lt;/h4&gt;在OPV2V基准测试中，EffiComm在每帧仅发送平均约1.5MB数据的情况下达到0.84 mAP@0.7的精度，在精度-比特曲线上优于先前方法。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了自适应、学习型通信对于可扩展车对万物感知的价值。&lt;h4&gt;翻译&lt;/h4&gt;协作感知允许互联车辆交换传感器信息并克服每辆车的盲点。然而，传输原始点云或完整特征图会压垮车对车通信，导致延迟和可扩展性问题。我们引入了EffiComm，这是一个端到端框架，传输的数据量不到先前方法所需的40%，同时保持最先进的3D目标检测精度。EffiComm操作于任何模态的鸟瞰图特征图，并应用两阶段减少管道：(1) 选择性传输使用置信度掩码修剪低效用区域；(2) 自适应网格减少使用图神经网络根据角色和网络负载分配车辆特定的保持比例。剩余的特征通过软门控专家混合注意力层融合，提供更大的容量和专业化，以实现有效的特征集成。在OPV2V基准测试中，EffiComm在每帧仅发送平均约1.5MB数据的情况下达到0.84 mAP@0.7的精度，在精度-比特曲线上优于先前方法。这些结果突显了自适应、学习型通信对于可扩展车对万物感知的价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是车辆间通信（V2V）中协作感知的数据传输效率问题。当车辆需要交换传感器信息来克服各自的盲点时，传输原始点云或完整特征图会占用大量带宽，导致延迟和可扩展性问题。这个问题在自动驾驶领域至关重要，因为它直接关系到车辆在复杂环境中的感知能力和安全性，同时也影响着整个车联网系统的效率和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到需要减少传输数据量同时保持检测精度，因此决定直接在BEV特征上操作以保持与任何上游骨干的兼容性。他们设计了一个两阶段压缩管道，并借鉴了多个领域的工作：特征级协作方法（如F-Cooper、AttentiveFusion）交换中间特征而非原始数据；通信感知调度策略（如Where2Comm）决定传输什么信息；注意力融合机制（如Transformer架构）对齐多智能体特征；以及专家混合（MoE）架构在NLP和视觉领域的应用。作者创新性地结合这些概念，创建了EffiComm框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过智能选择和压缩要传输的特征数据，显著减少通信量同时保持高精度检测。整体流程包括：1) 特征编码，使用PointPillars网络处理点云生成BEV特征和置信度图；2) 选择性传输，基于置信度图创建掩码只保留重要区域；3) 自适应网格减少，使用图神经网络根据车辆角色和网络负载动态调整压缩级别；4) 特征融合，通过软门控专家混合注意力层融合接收到的特征；5) 特征解码，将融合特征转换为检测输出。整个系统端到端训练，优化通信效率和检测精度的平衡。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 两阶段压缩管道（选择性传输+自适应网格减少）；2) GNN驱动的动态压缩策略，根据车辆角色和网络负载调整压缩级别；3) MoE-注意力融合头，提供更大容量和专业化处理能力；4) 端到端训练框架。相比之前工作，EffiComm不同之处在于：Where2Comm使用固定空间掩码，而EffiComm采用自适应两阶段压缩；AttentiveFusion等传输完整特征图，而EffiComm只传输精选区域；大多数方法要么关注时间维度决策，要么使用固定掩码，而EffiComm结合了空间和自适应压缩，实现了更高效的通信。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EffiComm通过两阶段自适应压缩和专家混合注意力融合，将车辆间通信数据量减少到先前方法的40%以下，同时保持高精度3D目标检测，为带宽受限的协作感知提供了高效解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception allows connected vehicles to exchange sensorinformation and overcome each vehicle's blind spots. Yet transmitting raw pointclouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications,causing latency and scalability problems. We introduce EffiComm, an end-to-endframework that transmits less than 40% of the data required by prior art whilemaintaining state-of-the-art 3D object detection accuracy. EffiComm operates onBird's-Eye-View (BEV) feature maps from any modality and applies a two-stagereduction pipeline: (1) Selective Transmission (ST) prunes low-utility regionswith a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph NeuralNetwork (GNN) to assign vehicle-specific keep ratios according to role andnetwork load. The remaining features are fused with a soft-gatedMixture-of-Experts (MoE) attention layer, offering greater capacity andspecialization for effective feature integration. On the OPV2V benchmark,EffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve.These results highlight the value of adaptive, learned communication forscalable Vehicle-to-Everything (V2X) perception.</description>
      <author>example@mail.com (Melih Yazgan, Allen Xavier Arasan, J. Marius Zöllner)</author>
      <guid isPermaLink="false">2507.19354v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed Graph Neural Networks for Transverse Momentum Estimation in CMS Trigger Systems</title>
      <link>http://arxiv.org/abs/2507.19205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种物理信息引导的图神经网络框架，用于高能物理中粒子横向动量的实时估计，通过四种图构建策略和新型消息传递层实现了比现有方法更好的精度-效率权衡。&lt;h4&gt;背景&lt;/h4&gt;高能物理中粒子横向动量的实时估计需要在严格硬件约束下同时满足算法效率和准确性要求。静态机器学习模型在高堆积条件下性能下降且缺乏物理感知优化，通用图神经网络忽略了关键领域结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种既高效又准确的算法，用于在严格硬件约束下实时估计粒子横向动量，同时克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出物理信息引导的GNN框架，通过四种图构建策略编码探测器几何和物理观测值：探测器站作为节点、特征作为节点、偏转角为中心和伪快度为中心的表示，并结合新型消息传递层和领域特定损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;CMS触发数据集实验验证：基于探测器站信息的EdgeConv模型实现0.8525的最先进MAE，比深度学习基线减少至少55%参数；以η为中心的MPL配置也显示改进的准确性和可比效率。&lt;h4&gt;结论&lt;/h4&gt;物理引导的GNN在资源受限的触发系统中具有部署前景，为高能物理粒子动量估计提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高能物理中粒子横向动量的实时估计需要算法在严格的硬件约束下既高效又准确。静态机器学习模型在高堆积条件下性能下降，且缺乏物理感知的优化，而通用的图神经网络常常忽略了强健动量回归所必需的领域结构。我们提出了一种物理信息引导的GNN框架，通过四种不同的图构建策略系统地编码探测器几何形状和物理观测值：探测器站作为节点、特征作为节点、偏转角为中心和伪快度为中心的表示。该框架将这些定制的图结构与新型消息传递层相结合，该层具有消息内注意力和门控更新功能，以及结合了动量分布先验的领域特定损失函数。我们的协同设计方法相比现有基线产生了更优的精度-效率权衡。在CMS触发数据集上的广泛实验验证了该方法：一个基于探测器站信息的EdgeConv模型实现了0.8525的最先进平均绝对误差，比深度学习基线减少了至少55%的参数，同时一个以η为中心的MPL配置也显示出改进的准确性和可比的效率。这些结果确立了物理引导的GNN在资源受限的触发系统中部署的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time particle transverse momentum ($p_T$) estimation in high-energyphysics demands algorithms that are both efficient and accurate under stricthardware constraints. Static machine learning models degrade under high pileupand lack physics-aware optimization, while generic graph neural networks (GNNs)often neglect domain structure critical for robust $p_T$ regression. We proposea physics-informed GNN framework that systematically encodes detector geometryand physical observables through four distinct graph construction strategiesthat systematically encode detector geometry and physical observables:station-as-node, feature-as-node, bending angle-centric, and pseudorapidity($\eta$)-centric representations. This framework integrates these tailoredgraph structures with a novel Message Passing Layer (MPL), featuringintra-message attention and gated updates, and domain-specific loss functionsincorporating $p_{T}$-distribution priors. Our co-design methodology yieldssuperior accuracy-efficiency trade-offs compared to existing baselines.Extensive experiments on the CMS Trigger Dataset validate the approach: astation-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with$\ge55\%$ fewer parameters than deep learning baselines, especially TabNet,while an $\eta$-centric MPL configuration also demonstrates improved accuracywith comparable efficiency. These results establish the promise ofphysics-guided GNNs for deployment in resource-constrained trigger systems.</description>
      <author>example@mail.com (Md Abrar Jahin, Shahriar Soudeep, M. F. Mridha, Muhammad Mostafa Monowar, Md. Abdul Hamid)</author>
      <guid isPermaLink="false">2507.19205v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Predictor for Optimal Quantum Hardware Selection</title>
      <link>http://arxiv.org/abs/2507.19093v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于图神经网络(GNN)的预测器，用于自动选择最适合执行特定量子电路的量子硬件平台。该方法通过分析量子电路的有向无环图(DAG)表示来避免传统暴力方法的计算开销，实验结果显示其准确率达到94.4%，F1得分为85.5%。&lt;h4&gt;背景&lt;/h4&gt;随着量子硬件技术的多样化，每种技术都有其独特的特性（如连接性和原生门集），为选择执行特定量子电路的最佳平台带来了挑战。传统的选择过程通常采用暴力方法：在各种设备上编译电路并基于电路深度和门保真度等因素评估性能。然而，这种方法计算成本高，且随着可用量子处理器数量的增加，可扩展性差。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化硬件选择的方法，能够高效预测哪种量子硬件平台最适合执行特定的量子电路，避免传统暴力方法的计算开销，提高选择过程的可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于图神经网络(GNN)的预测器，通过分析量子电路的有向无环图(DAG)表示来自动化硬件选择。研究评估了来自MQT Bench数据集的498个量子电路（最多27个量子比特），使用Qiskit在四种设备上编译：三个超导量子处理器（IBM-Kyiv、IBM-Brisbane、IBM-Sherbrooke）和一个离子阱处理器（IONQ-Forte）。性能估计使用结合电路深度和门保真度的指标。&lt;h4&gt;主要发现&lt;/h4&gt;在评估的数据集中，93个电路在离子阱设备上获得最优编译，其余电路则更适合超导平台。基于图机器学习的方法避免了为模型评估提取电路特征，而是将电路直接嵌入为图，显著加速了最佳目标决策过程并保留了所有信息。实验证明该方法对少数类的准确率达到94.4%，F1得分为85.5%，有效预测了最佳编译目标。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的量子硬件选择预测器能够有效解决传统暴力方法面临的计算成本高和可扩展性差的问题，通过直接分析量子电路的图表示，实现了高效准确的硬件选择决策。&lt;h4&gt;翻译&lt;/h4&gt;随着量子硬件技术的日益多样化，每种技术都有其独特特性，如连接性和原生门集，这为选择执行特定量子电路的最佳平台带来了挑战。这一选择过程通常采用暴力方法：在各种设备上编译电路并根据电路深度和门保真度等因素评估性能。然而，这种方法计算成本高，且随着可用量子处理器数量的增加，可扩展性差。在本工作中，我们提出了一种基于图神经网络(GNN)的预测器，通过分析量子电路的有向无环图(DAG)表示来自动化硬件选择。我们的研究评估了MQT Bench数据集中的498个量子电路（最多27个量子比特），使用Qiskit在四种设备上编译：三个超导量子处理器（IBM-Kyiv、IBM-Brisbane、IBM-Sherbrooke）和一个离子阱处理器（IONQ-Forte）。性能估计使用结合电路深度和门保真度的指标，形成了一个数据集，其中93个电路在离子阱设备上获得最优编译，而其余电路则更适合超导平台。通过利用基于图的机器学习，我们的方法避免了为模型评估提取电路特征，而是将其直接嵌入为图，显著加速了最佳目标决策过程并保留了所有信息。实验结果证明，对于少数类，准确率达到94.4%，F1得分为85.5%，有效预测了最佳编译目标。开发的代码已在GitHub上公开（https://github.com/antotu/GNN-Model-Quantum-Predictor）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing variety of quantum hardware technologies, each with uniquepeculiarities such as connectivity and native gate sets, creates challengeswhen selecting the best platform for executing a specific quantum circuit. Thisselection process usually involves a brute-force approach: compiling thecircuit on various devices and evaluating performance based on factors such ascircuit depth and gate fidelity. However, this method is computationallyexpensive and does not scale well as the number of available quantum processorsincreases. In this work, we propose a Graph Neural Network (GNN)-basedpredictor that automates hardware selection by analyzing the Directed AcyclicGraph (DAG) representation of a quantum circuit. Our study evaluates 498quantum circuits (up to 27 qubits) from the MQT Bench dataset, compiled usingQiskit on four devices: three superconducting quantum processors (IBM-Kyiv,IBM-Brisbane, IBM-Sherbrooke) and one trapped-ion processor (IONQ-Forte).Performance is estimated using a metric that integrates circuit depth and gatefidelity, resulting in a dataset where 93 circuits are optimally compiled onthe trapped-ion device, while the remaining circuits prefer superconductingplatforms. By exploiting graph-based machine learning, our approach avoidsextracting the circuit features for the model evaluation but directly embeds itas a graph, significantly accelerating the optimal target decision-makingprocess and maintaining all the information. Experimental results prove 94.4%accuracy and an 85.5% F1 score for the minority class, effectively predictingthe best compilation target. The developed code is publicly available on GitHub(https://github.com/antotu/GNN-Model-Quantum-Predictor).</description>
      <author>example@mail.com (Antonio Tudisco, Deborah Volpe, Giacomo Orlandi, Giovanna Turvani)</author>
      <guid isPermaLink="false">2507.19093v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems</title>
      <link>http://arxiv.org/abs/2507.19067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PBiLoss的新型损失函数，用于解决推荐系统中的流行度偏差问题。通过惩罚模型对流行物品的倾向，鼓励推荐不太流行但可能更个性化的内容。该方法与模型无关，可轻松集成到现有框架中，实验证明能有效提高公平性而不损害推荐性能。&lt;h4&gt;背景&lt;/h4&gt;推荐系统，特别是基于图神经网络的系统，在捕捉用户-物品交互模式方面取得了显著成功。然而，它们仍然容易受到流行度偏差的影响——即倾向于过度推荐流行物品——这导致内容多样性降低和公平性受损。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为PBiLoss的新型基于正则化的损失函数，明确对抗基于图的推荐模型中的流行度偏差，在保持推荐准确性的同时提高公平性和内容多样性。&lt;h4&gt;方法&lt;/h4&gt;设计PBiLoss损失函数，通过惩罚模型对流行物品的倾向来对抗流行度偏差；引入两种采样策略：流行正样本(PopPos)和流行负样本(PopNeg)，分别调节训练过程中流行正负物品的贡献；探索两种区分流行物品的方法：一种基于固定流行度阈值，另一种不使用阈值；该方法与模型无关，可无缝集成到LightGCN及其变体等框架中。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界数据集上的综合实验表明，PBiLoss显著提高了公平性，具体表现为用户流行度-排名相关性(PRU)和物品流行度-排名相关性(PRI)的降低，同时保持甚至提高了标准的推荐准确性和排名指标。&lt;h4&gt;结论&lt;/h4&gt;直接将公平性目标嵌入优化过程是有效的，为现代推荐系统中平衡准确性和公平内容曝光提供了实用且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;推荐系统，特别是那些基于图神经网络的系统，在捕捉用户-物品交互模式方面取得了显著成功。然而，它们仍然容易受到流行度偏差的影响——即倾向于过度推荐流行物品——这导致内容多样性降低和公平性受损。在本文中，我们提出了PBiLoss，一种新型的基于正则化的损失函数，旨在明确对抗基于图的推荐模型中的流行度偏差。PBiLoss通过惩罚模型对流行物品的倾向来增强传统训练目标，从而鼓励推荐不太流行但可能更个性化的内容。我们引入了两种采样策略：流行正样本(PopPos)和流行负样本(PopNeg)，它们分别调节训练过程中流行正负物品的贡献。我们进一步探索了两种区分流行物品的方法：一种基于固定的流行度阈值，另一种不使用任何阈值，使该方法具有灵活性和适应性。我们提出的方法与模型无关，可以无缝集成到LightGCN及其变体等最先进的基于图的框架中。在多个真实世界数据集上的综合实验表明，PBiLoss显著提高了公平性，如用户流行度-排名相关性(PRU)和物品流行度-排名相关性(PRI)的降低所示，同时保持甚至提高了标准的推荐准确性和排名指标。这些结果突显了将公平性目标直接嵌入优化过程的有效性，为在现代推荐系统中平衡准确性和公平内容曝光提供了实用且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommender systems, especially those based on graph neural networks (GNNs),have achieved remarkable success in capturing user-item interaction patterns.However, they remain susceptible to popularity bias--the tendency toover-recommend popular items--resulting in reduced content diversity andcompromised fairness. In this paper, we propose PBiLoss, a novelregularization-based loss function designed to counteract popularity bias ingraph-based recommender models explicitly. PBiLoss augments traditionaltraining objectives by penalizing the model's inclination toward popular items,thereby encouraging the recommendation of less popular but potentially morepersonalized content. We introduce two sampling strategies: Popular Positive(PopPos) and Popular Negative (PopNeg), which respectively modulate thecontribution of the positive and negative popular items during training. Wefurther explore two methods to distinguish popular items: one based on a fixedpopularity threshold and another without any threshold, making the approachflexible and adaptive. Our proposed method is model-agnostic and can beseamlessly integrated into state-of-the-art graph-based frameworks such asLightGCN and its variants. Comprehensive experiments across multiple real-worlddatasets demonstrate that PBiLoss significantly improves fairness, asdemonstrated by reductions in the Popularity-Rank Correlation for Users (PRU)and Popularity-Rank Correlation for Items (PRI), while maintaining or evenenhancing standard recommendation accuracy and ranking metrics. These resultshighlight the effectiveness of directly embedding fairness objectives into theoptimization process, providing a practical and scalable solution for balancingaccuracy and equitable content exposure in modern recommender systems.</description>
      <author>example@mail.com (Mohammad Naeimi, Mostafa Haghir Chehreghani)</author>
      <guid isPermaLink="false">2507.19067v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs</title>
      <link>http://arxiv.org/abs/2507.19031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProGMLP是一种创新的GNN-to-MLP知识蒸馏方法，通过渐进式框架提供了灵活的推理成本和准确性权衡，适用于资源受限环境。&lt;h4&gt;背景&lt;/h4&gt;GNN-to-MLP(G2M)方法通过将图神经网络(GNN)的知识蒸馏到多层感知器(MLP)中来加速GNN，弥合了GNN表达能力和MLP计算效率之间的差距。然而，现有G2M方法无法灵活动态地调整推理成本和准确性，难以适应现实应用中计算资源和时间约束的变化。&lt;h4&gt;目的&lt;/h4&gt;引入一个渐进式框架(ProGMLP)，为GNN-to-MLP知识蒸馏提供灵活的、按需推理成本和准确性之间的权衡，满足现实应用中资源动态变化的需求。&lt;h4&gt;方法&lt;/h4&gt;ProGMLP采用渐进式训练结构(PTS)，多个MLP学生按顺序训练；结合渐进式知识蒸馏(PKD)迭代精炼蒸馏过程；使用渐进式混合增强(PMA)通过生成更难的混合样本来增强泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在八个真实世界图数据集上的全面实验表明，ProGMLP能够保持高准确性同时动态适应不同的运行时场景。&lt;h4&gt;结论&lt;/h4&gt;ProGMLP对于在多样化的应用环境中部署非常有效，能够根据实际需求灵活调整推理成本和准确性。&lt;h4&gt;翻译&lt;/h4&gt;GNN-to-MLP(G2M)方法作为一种有前途的方法，通过将图神经网络(GNN)的知识蒸馏到更简单的多层感知器(MLP)中来加速GNN。这些方法弥合了GNN的表达能力和MLP的计算效率之间的差距，使其适合资源受限的环境。然而，现有的G2M方法受限于无法灵活地动态调整推理成本和准确性，这是现实应用中的一个关键要求，其中计算资源和时间约束可能差异显著。为了解决这个问题，我们引入了一个渐进式框架(ProGMLP)，旨在为GNN-to-MLP知识蒸馏提供灵活的、按需推理成本和准确性之间的权衡。ProGMLP采用渐进式训练结构(PTS)，其中多个MLP学生按顺序训练，每个都建立在前一个的基础上。此外，ProGMLP结合了渐进式知识蒸馏(PKD)，从GNN到MLP迭代地精炼蒸馏过程，以及渐进式混合增强(PMA)，通过渐进生成更难的混合样本来增强泛化能力。我们在八个真实世界图数据集上进行了全面的实验验证，证明了ProGMLP在保持高准确性的同时能够动态适应不同的运行时场景，使其在多样化的应用环境中部署非常有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerateGraph Neural Networks (GNNs) by distilling their knowledge into simplerMulti-Layer Perceptrons (MLPs). These methods bridge the gap between theexpressive power of GNNs and the computational efficiency of MLPs, making themwell-suited for resource-constrained environments. However, existing G2Mmethods are limited by their inability to flexibly adjust inference cost andaccuracy dynamically, a critical requirement for real-world applications wherecomputational resources and time constraints can vary significantly. To addressthis, we introduce a Progressive framework designed to offer flexible andon-demand trade-offs between inference cost and accuracy for GNN-to-MLPknowledge distillation (ProGMLP). ProGMLP employs a Progressive TrainingStructure (PTS), where multiple MLP students are trained in sequence, eachbuilding on the previous one. Furthermore, ProGMLP incorporates ProgressiveKnowledge Distillation (PKD) to iteratively refine the distillation processfrom GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhancegeneralization by progressively generating harder mixed samples. Our approachis validated through comprehensive experiments on eight real-world graphdatasets, demonstrating that ProGMLP maintains high accuracy while dynamicallyadapting to varying runtime scenarios, making it highly effective fordeployment in diverse application settings.</description>
      <author>example@mail.com (Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang, Yujie Sun, Zheng Liang, Yibing Zhan, Dapeng Tao)</author>
      <guid isPermaLink="false">2507.19031v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling</title>
      <link>http://arxiv.org/abs/2507.18977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个针对时间知识图谱的增量训练框架，通过模型无关的增强层和加权采样策略来解决新实体和稀疏连接实体的挑战，实验证明该方法在多个指标上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;传统的时间知识图谱补全模型假设在训练过程中可以访问整个图谱，忽略了TKGs随时间演变带来的挑战，如模型需要泛化和吸收新知识，以及处理连接稀疏的新实体或未见过实体。&lt;h4&gt;目的&lt;/h4&gt;提出一个专门为TKGs设计的增量训练框架，解决在训练过程中未被观察到或连接稀疏的实体问题。&lt;h4&gt;方法&lt;/h4&gt;结合了与模型无关的增强层和加权采样策略，增强层利用更广泛的实体相似性全局定义，超越基于GNN方法的局部邻域接近性；训练中使用的加权采样策略强调与不常出现实体相连的边。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集上评估，该方法在总链接预测、归纳链接预测和处理长尾实体方面优于现有方法，在MRR指标上实现了10%和15%的提升。&lt;h4&gt;结论&lt;/h4&gt;该方法在减轻灾难性遗忘和增强TKG补全方法的稳健性方面具有潜力，特别是在增量训练背景下效果显著。&lt;h4&gt;翻译&lt;/h4&gt;时间知识图谱补全模型传统上假设在训练过程中可以访问整个图谱。这忽略了源于TKGs演变性质的挑战，例如：(i)模型需要泛化和吸收新知识，以及(ii)管理通常具有稀疏连接的新或未见实体的任务。在本文中，我们提出了一个专门为TKG设计的增量训练框架，旨在解决在训练过程中未被观察到或具有稀疏连接的实体。我们的方法将一个与模型无关的增强层与加权采样策略相结合，可以增强并改进任何现有的TKG补全方法。增强层利用了更广泛的实体相似性全局定义，超越了基于GNN方法的局部邻域接近性。训练中使用的加权采样策略强调与不常出现实体相连的边。我们在两个基准数据集上评估了我们的方法，并证明我们的框架在总链接预测、归纳链接预测以及解决长尾实体方面优于现有方法。值得注意的是，我们的方法在这些数据集上实现了10%的改进和15%的MRR提升。结果强调了我们的方法在减轻灾难性遗忘和增强TKG补全方法稳健性方面的潜力，特别是在增量训练背景下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Knowledge Graph (TKG) completion models traditionally assume accessto the entire graph during training. This overlooks challenges stemming fromthe evolving nature of TKGs, such as: (i) the model's requirement to generalizeand assimilate new knowledge, and (ii) the task of managing new or unseenentities that often have sparse connections. In this paper, we present anincremental training framework specifically designed for TKGs, aiming toaddress entities that are either not observed during training or have sparseconnections. Our approach combines a model-agnostic enhancement layer with aweighted sampling strategy, that can be augmented to and improve any existingTKG completion method. The enhancement layer leverages a broader, globaldefinition of entity similarity, which moves beyond mere local neighborhoodproximity of GNN-based methods. The weighted sampling strategy employed intraining accentuates edges linked to infrequently occurring entities. Weevaluate our method on two benchmark datasets, and demonstrate that ourframework outperforms existing methods in total link prediction, inductive linkprediction, and in addressing long-tail entities. Notably, our method achievesa 10\% improvement and a 15\% boost in MRR for these datasets. The resultsunderscore the potential of our approach in mitigating catastrophic forgettingand enhancing the robustness of TKG completion methods, especially in anincremental training context</description>
      <author>example@mail.com (Mehrnoosh Mirtaheri, Ryan A. Rossi, Sungchul Kim, Kanak Mahadik, Tong Yu, Xiang Chen, Mohammad Rostami)</author>
      <guid isPermaLink="false">2507.18977v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction</title>
      <link>http://arxiv.org/abs/2507.18926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为几何多色消息传递图神经网络（GMC-MPNN）的新型框架，通过整合原子级别的几何特征和长程相互作用，提高了血脑屏障渗透性预测的准确性。该模型基于原子类型构建加权彩色子图，捕捉控制血脑屏障渗透性的空间关系和化学背景。在三个基准数据集上的测试结果表明，GMC-MPNN在分类和回归任务中均优于现有最先进模型，为药物发现流程提供了更准确和可推广的工具。&lt;h4&gt;背景&lt;/h4&gt;血脑屏障渗透性（BBBP）的准确预测对于中枢神经系统（CNS）药物开发至关重要。尽管图神经网络（GNNs）在分子性质预测方面取得了进展，但它们通常依赖于分子拓扑结构，而忽略了建模传输机制所必需的三维几何信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合三维几何信息的图神经网络模型，提高血脑屏障渗透性预测的准确性，为CNS药物开发提供更可靠的工具。&lt;h4&gt;方法&lt;/h4&gt;研究提出了几何多色消息传递图神经网络（GMC-MPNN），这是一种通过显式整合原子级几何特征和长程相互作用来增强标准消息传递架构的新型框架。该方法基于原子类型构建加权彩色子图，以捕捉控制血脑屏障渗透性的空间关系和化学背景。研究使用基于支架的严格分割方法在三个基准数据集上评估了GMC-MPNN在分类和回归任务中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;GMC-MPNN在三个基准数据集上的测试结果一致优于现有最先进模型。在分类任务中，区分可渗透/不可渗透化合物的AUC-ROC分别达到0.9704和0.9685；在回归任务中，连续渗透性值的均方根误差（RMSE）为0.4609，皮尔逊相关系数为0.7759。消融研究进一步量化了特定原子对相互作用的影响，揭示了模型的预测能力来自于能够从常见和罕见但化学上重要的功能基序中学习的能力。&lt;h4&gt;结论&lt;/h4&gt;通过将空间几何整合到图表示中，GMC-MPNN设定了新的性能基准，并为药物发现流程提供了更准确和可推广的工具，有助于中枢神经系统药物的开发。&lt;h4&gt;翻译&lt;/h4&gt;血脑屏障渗透性（BBBP）的准确预测对于中枢神经系统（CNS）药物开发至关重要。尽管图神经网络（GNNs）在分子性质预测方面取得了进展，但它们通常依赖于分子拓扑结构，而忽略了建模传输机制所必需的三维几何信息。本文引入了几何多色消息传递图神经网络（GMC-MPNN），这是一种新型框架，通过显式整合原子级几何特征和长程相互作用来增强标准消息传递架构。我们的模型基于原子类型构建加权彩色子图，以捕捉控制血脑屏障渗透性的空间关系和化学背景。我们在三个基准数据集上使用基于支架的严格分割方法评估了GMC-MPNN在分类和回归任务中的性能，以确保对泛化能力的稳健评估。结果表明，GMC-MPNN始终优于现有的最先进模型，在将化合物分类为可渗透/不可渗透（AUC-ROC分别为0.9704和0.9685）和回归连续渗透性值（RMSE为0.4609，皮尔逊相关系数为0.7759）方面均取得了卓越的性能。消融研究进一步量化了特定原子对相互作用的影响，揭示了模型的预测能力来自于其能够从常见和罕见但化学上重要的功能基序中学习的能力。通过将空间几何整合到图表示中，GMC-MPNN设定了新的性能基准，并为药物发现流程提供了更准确和可推广的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of blood-brain barrier permeability (BBBP) is essentialfor central nervous system (CNS) drug development. While graph neural networks(GNNs) have advanced molecular property prediction, they often rely onmolecular topology and neglect the three-dimensional geometric informationcrucial for modeling transport mechanisms. This paper introduces the geometricmulti-color message-passing graph neural network (GMC-MPNN), a novel frameworkthat enhances standard message-passing architectures by explicitlyincorporating atomic-level geometric features and long-range interactions. Ourmodel constructs weighted colored subgraphs based on atom types to capture thespatial relationships and chemical context that govern BBB permeability. Weevaluated GMC-MPNN on three benchmark datasets for both classification andregression tasks, using rigorous scaffold-based splitting to ensure a robustassessment of generalization. The results demonstrate that GMC-MPNNconsistently outperforms existing state-of-the-art models, achieving superiorperformance in both classifying compounds as permeable/non-permeable (AUC-ROCof 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of0.4609, Pearson correlation of 0.7759). An ablation study further quantifiedthe impact of specific atom-pair interactions, revealing that the model'spredictive power derives from its ability to learn from both common and rare,but chemically significant, functional motifs. By integrating spatial geometryinto the graph representation, GMC-MPNN sets a new performance benchmark andoffers a more accurate and generalizable tool for drug discovery pipelines.</description>
      <author>example@mail.com (Trung Nguyen, Md Masud Rana, Farjana Tasnim Mukta, Chang-Guo Zhan, Duc Duy Nguyen)</author>
      <guid isPermaLink="false">2507.18926v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>ThermoRL:Structure-Aware Reinforcement Learning for Protein Mutation Design to Enhance Thermostability</title>
      <link>http://arxiv.org/abs/2507.18816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ThermoRL是一种基于强化学习的框架，利用图神经网络设计具有增强热稳定性的蛋白质突变，能有效识别稳定突变位点并具有强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;蛋白质热稳定性优化突变设计具有挑战性，因为序列变异、结构动力学和热稳定性之间的关系复杂。现有方法依赖实验随机突变或预定义数据集的预测模型，使用基于序列的启发式方法，将酶设计视为单步过程，限制了设计空间探索和发现。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的蛋白质热稳定性优化突变设计方法，能够更有效地探索设计空间，识别稳定突变位点，并具有更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;ThermoRL框架结合了预训练的基于GNN的编码器和分层Q学习网络，使用代理模型进行奖励反馈，指导强化学习agent在蛋白质的哪个位置以及应用哪种突变氨基酸来增强热稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;ThermoRL在保持计算效率的同时，实现了比基线更高或相当的奖励；能够过滤掉不稳定的突变，识别与实验数据一致稳定突变；准确检测未见蛋白质中的关键突变位点，显示出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;由GNN嵌入驱动的强化学习方法为传统蛋白质突变设计提供了有力的替代方案，ThermoRL框架在蛋白质热稳定性优化方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;设计突变以优化蛋白质热稳定性仍然具有挑战性，因为序列变异、结构动力学和热稳定性之间的关系复杂，通常通过折叠自由能的变化来评估。现有方法依赖于实验随机突变或使用预定义数据集进行测试的预测模型，使用基于序列的启发式方法，并将酶设计视为单步过程而不进行迭代改进，这限制了设计空间的探索并限制了超出已知变异的发现。我们提出了ThermoRL，一种基于强化学习的框架，利用图神经网络来设计具有增强热稳定性的突变。它将预训练的基于GNN的编码器与分层Q学习网络相结合，并采用代理模型进行奖励反馈，指导RL agent在何处（位置）以及应用哪种（突变氨基酸）来增强热稳定性。实验结果表明，ThermoRL在保持计算效率的同时，实现了比基线更高或相当的奖励。它过滤掉不稳定的突变，并识别与实验数据一致的稳定突变。此外，ThermoRL能够准确检测未见蛋白质中的关键突变位点，突出了其强大的泛化能力。这种由GNN嵌入驱动的RL方法为传统的蛋白质突变设计提供了有力的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing mutations to optimize protein thermostability remains challengingdue to the complex relationship between sequence variations, structuraldynamics, and thermostability, often assessed by \delta\delta G  (the change in free energy of unfolding). Existing methods rely onexperimental random mutagenesis or prediction models tested with pre-defineddatasets, using sequence-based heuristics and treating enzyme design as aone-step process without iterative refinement, which limits design spaceexploration and restricts discoveries beyond known variations. We presentThermoRL, a framework based on reinforcement learning (RL) that leverages graphneural networks (GNN) to design mutations with enhanced thermostability. Itcombines a pre-trained GNN-based encoder with a hierarchical Q-learning networkand employs a surrogate model for reward feedback, guiding the RL agent onwhere (the position) and which (mutant amino acid) to apply for enhancedthermostability. Experimental results show that ThermoRL achieves higher orcomparable rewards than baselines while maintaining computational efficiency.It filters out destabilizing mutations and identifies stabilizing mutationsaligned with experimental data. Moreover, ThermoRL accurately detects keymutation sites in unseen proteins, highlighting its strong generalizability.This RL-guided approach powered by GNN embeddings offers a robust alternativeto traditional protein mutation design.</description>
      <author>example@mail.com (Xiangwen Wang, Gaojie Jin, Xiaowei Huang, Ronghui Mu)</author>
      <guid isPermaLink="false">2507.18816v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Ralts: Robust Aggregation for Enhancing Graph Neural Network Resilience on Bit-flip Errors</title>
      <link>http://arxiv.org/abs/2507.18804v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Ralts的通用轻量级解决方案，用于增强图神经网络(GNNs)对硬件位翻转错误的鲁棒性。通过利用图相似性指标过滤异常值和恢复受损图拓扑，Ralts可以直接集成到任何消息传递GNN的聚合函数中，显著提高预测准确性同时保持高效的执行效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已广泛应用于金融和医疗等安全关键领域，现有研究主要关注软件层面的威胁，而对硬件引起的故障和错误探索不足。随着硬件技术进步，系统对瞬时故障更加敏感，可能导致位翻转和静默数据损坏，这是Meta和Google等公司观察到的重要问题。&lt;h4&gt;目的&lt;/h4&gt;首先分析GNN抵抗位翻转错误的能力，揭示系统级优化机会；其次提出一个通用且轻量级的解决方案Ralts，增强GNN对硬件故障的恢复能力。&lt;h4&gt;方法&lt;/h4&gt;Ralts利用各种图相似性指标过滤异常值并恢复受损的图拓扑，将这些保护技术直接整合到聚合函数中，以支持任何消息传递GNN架构。&lt;h4&gt;主要发现&lt;/h4&gt;Ralts有效增强了多种GNN模型、数据集、错误模式以及密集和稀疏架构的鲁棒性。在位错误率为3×10^-5的情况下，当错误出现在模型权重或节点嵌入中时，预测准确性提高至少20%；当错误出现在邻接矩阵中时，提高至少10%。Ralts的执行效率与PyTorch Geometric内置聚合函数相当。&lt;h4&gt;结论&lt;/h4&gt;Ralts是一个有效的解决方案，能够在保持高效执行的同时显著提高GNN对硬件故障的鲁棒性，为构建可靠的GNN系统提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已被广泛应用于安全关键应用中，如金融和医疗网络，其中受损的预测可能导致灾难性后果。虽然现有关于GNN鲁棒性的研究主要集中在软件层面的威胁上，但硬件引起的故障和错误在很大程度上仍未被探索。随着硬件系统向先进技术节点发展以满足高性能和能效需求，它们对瞬时故障变得越来越敏感，这可能导致位翻转和静默数据损坏，这是Meta和Google等主要科技公司观察到的一个突出问题。为此，我们首先呈现了GNN抵抗位翻转错误能力的全面分析，旨在揭示未来可靠高效GNN系统的系统级优化机会。其次，我们提出了Ralts，一个通用且轻量级的解决方案，以增强GNN对位翻转错误的恢复能力。具体而言，Ralts利用各种图相似性指标来过滤异常值并恢复受损的图拓扑，并将这些保护技术直接整合到聚合函数中，以支持任何消息传递GNNs。评估结果表明，Ralts有效地增强了各种GNN模型、图数据集、错误模式以及密集和稀疏架构的GNN鲁棒性。平均而言，在位错误率为3×10^-5的情况下，当错误出现在模型权重或节点嵌入中时，这些鲁棒聚合函数将预测准确性提高了至少20%；当错误出现在邻接矩阵中时，提高了至少10%。Ralts也经过优化，使其执行效率与PyTorch Geometric中内置的聚合函数相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have been widely applied in safety-criticalapplications, such as financial and medical networks, in which compromisedpredictions may cause catastrophic consequences. While existing research on GNNrobustness has primarily focused on software-level threats, hardware-inducedfaults and errors remain largely underexplored. As hardware systems progresstoward advanced technology nodes to meet high-performance and energy efficiencydemands, they become increasingly susceptible to transient faults, which cancause bit flips and silent data corruption, a prominent issue observed by majortechnology companies (e.g., Meta and Google). In response, we first present acomprehensive analysis of GNN robustness against bit-flip errors, aiming toreveal system-level optimization opportunities for future reliable andefficient GNN systems. Second, we propose Ralts, a generalizable andlightweight solution to bolster GNN resilience to bit-flip errors.Specifically, Ralts exploits various graph similarity metrics to filter outoutliers and recover compromised graph topology, and incorporates theseprotective techniques directly into aggregation functions to support anymessage-passing GNNs. Evaluation results demonstrate that Ralts effectivelyenhances GNN robustness across a range of GNN models, graph datasets, errorpatterns, and both dense and sparse architectures. On average, under a BER of$3\times10^{-5}$, these robust aggregation functions improve predictionaccuracy by at least 20\% when errors present in model weights or nodeembeddings, and by at least 10\% when errors occur in adjacency matrices. Raltsis also optimized to deliver execution efficiency comparable to built-inaggregation functions in PyTorch Geometric.</description>
      <author>example@mail.com (Wencheng Zou, Nan Wu)</author>
      <guid isPermaLink="false">2507.18804v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Resolving Indirect Calls in Binary Code via Cross-Reference Augmented Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2507.18801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NeuCall是一种使用图神经网络解决二进制代码中间接调用目标解析问题的创新方法，通过增强控制流图和利用编译器级别类型分析，实现了95.2%的F1分数，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;二进制代码分析在源代码不可用场景中至关重要，广泛应用于安全领域。准确解析间接调用目标一直是静态分析中的长期挑战，因为调用指令的操作数在运行前未知，导致不完整的跨过程控制流图。&lt;h4&gt;目的&lt;/h4&gt;解决二进制代码分析中间接调用目标解析的准确性问题，提高静态分析的完整性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;NeuCall将交叉引用增强到控制流图中保留语义信息，利用编译器级别类型分析生成高质量训练对，并设计基于增强CFG和关系图卷积的图神经网络模型进行目标预测。&lt;h4&gt;主要发现&lt;/h4&gt;在GitHub和Arch User Repository的真实二进制文件(x86_64架构)上评估，NeuCall实现了95.2%的F1分数，显著优于最先进的基于机器学习的方法。&lt;h4&gt;结论&lt;/h4&gt;NeuCall在构建精确的跨过程控制流图方面有效，有望推动下游二进制分析和安全应用的发展。&lt;h4&gt;翻译&lt;/h4&gt;二进制代码分析在源代码不可用的场景中至关重要，在各个安全领域有广泛应用。然而，准确解析间接调用目标一直是保持二进制代码静态分析完整性的长期挑战。这个困难是因为调用指令的操作数(如call rax)在运行前未知，导致不完整的跨过程控制流图(CFG)。先前的方法在准确度和可扩展性方面存在困难。为解决这些限制，近期工作越来越多地转向机器学习(ML)来增强分析。然而，这种ML驱动的方法面临两个重大障碍：低质量的调用点-被调用方训练对和不足的二进制代码表示，这些都削弱了ML模型的准确性。在本文中，我们介绍了NeuCall，一种使用图神经网络解决间接调用的创新方法。该领域的现有ML模型经常忽略数据和代码交叉引用等关键元素，这些元素对于理解程序控制流至关重要。相比之下，NeuCall通过交叉引用增强CFG，保留丰富的语义信息。此外，我们利用先进的编译器级别类型分析生成高质量的调用点-被调用方训练对，提高模型的精度和可靠性。我们进一步设计了一个图神经模型，利用增强的CFG和关系图卷积进行准确的目标预测。在GitHub和Arch User Repository上针对x86_64架构的真实二进制文件进行评估，NeuCall实现了95.2%的F1分数，优于最先进的基于ML的方法。这些结果突显了NeuCall在构建精确的跨过程CFG方面的有效性及其推动下游二进制分析和安全应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Binary code analysis is essential in scenarios where source code isunavailable, with extensive applications across various security domains.However, accurately resolving indirect call targets remains a longstandingchallenge in maintaining the integrity of static analysis in binary code. Thisdifficulty arises because the operand of a call instruction (e.g., call rax)remains unknown until runtime, resulting in an incomplete inter-proceduralcontrol flow graph (CFG). Previous approaches have struggled with low accuracyand limited scalability. To address these limitations, recent work hasincreasingly turned to machine learning (ML) to enhance analysis. However, thisML-driven approach faces two significant obstacles: low-quality callsite-calleetraining pairs and inadequate binary code representation, both of whichundermine the accuracy of ML models. In this paper, we introduce NeuCall, anovel approach for resolving indirect calls using graph neural networks.Existing ML models in this area often overlook key elements such as data andcode cross-references, which are essential for understanding a program'scontrol flow. In contrast, NeuCall augments CFGs with cross-references,preserving rich semantic information. Additionally, we leverage advancedcompiler-level type analysis to generate high-quality callsite-callee trainingpairs, enhancing model precision and reliability. We further design a graphneural model that leverages augmented CFGs and relational graph convolutionsfor accurate target prediction. Evaluated against real-world binaries fromGitHub and the Arch User Repository on x86_64 architecture, NeuCall achieves anF1 score of 95.2%, outperforming state-of-the-art ML-based approaches. Theseresults highlight NeuCall's effectiveness in building precise inter-proceduralCFGs and its potential to advance downstream binary analysis and securityapplications.</description>
      <author>example@mail.com (Haotian Zhang, Kun Liu, Cristian Garces, Chenke Luo, Yu Lei, Jiang Ming)</author>
      <guid isPermaLink="false">2507.18801v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>A comparison of stretched-grid and limited-area modelling for data-driven regional weather forecasting</title>
      <link>http://arxiv.org/abs/2507.18378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了基于图神经网络的区域机器学习天气预测模型中的有限区域模型(LAM)和拉伸网格模型(SGM)两种方法，分析了它们在欧洲区域天气预报中的性能差异和适用场景。&lt;h4&gt;背景&lt;/h4&gt;基于图神经网络的区域机器学习天气预测模型最近展示了显著的预测准确性，比数值天气预报模型计算成本更低。有限区域模型(LAM)和拉伸网格模型(SGM)已出现用于生成高分辨率区域预报，基于区域(再)分析提供的初始条件。LAM使用来自外部全球模型的侧边界，而SGM在较低分辨率下包含全球域。&lt;h4&gt;目的&lt;/h4&gt;理解模型设计差异如何影响相对性能和潜在应用，确定这两种方法在欧洲生成确定性区域预报时的优缺点。&lt;h4&gt;方法&lt;/h4&gt;使用Anemoi框架构建两种类型的模型，通过最小程度地调整共享架构来构建，使用全球和区域再分析进行训练，设置几乎相同。进行多个推理实验以探索它们的相对性能并突出关键差异。&lt;h4&gt;主要发现&lt;/h4&gt;LAM和SGM都是具有竞争力的确定性机器学习天气预测模型，在区域内两者都具有准确且可比较的预测性能。LAM能够成功利用高质量的边界强制进行区域预测，适合在全球数据难以获取的情境中使用。SGM是完全自包含的，更容易操作化，可以利用更多训练数据，在时间泛化能力方面显著优于LAM。&lt;h4&gt;结论&lt;/h4&gt;本文可以作为气象机构开发操作数据驱动预报系统时选择LAM或SGM的起点。&lt;h4&gt;翻译&lt;/h4&gt;基于图神经网络的区域机器学习天气预测模型最近展示了显著的预测准确性，比数值天气预报模型计算成本更低。有限区域模型(LAM)和拉伸网格模型(SGM)方法已出现用于生成高分辨率区域预报，基于区域(再)分析提供的初始条件。LAM使用来自外部全球模型的侧边界，而SGM在较低分辨率下包含全球域。本研究旨在理解模型设计差异如何影响相对性能和潜在应用。具体确定这两种方法在欧洲生成确定性区域预报时的优缺点。使用Anemoi框架，通过最小程度地调整共享架构构建两种类型的模型，使用全球和区域再分析进行几乎相同的设置训练。进行了多个推理实验以探索它们的相对性能并突出关键差异。结果表明，LAM和SGM都是具有竞争力的确定性机器学习天气预测模型，在区域内两者都具有准确且可比较的预测性能。在各种应用中发现了模型性能的差异。LAM能够成功利用高质量的边界强制进行区域预测，适合在全球数据难以获取的情境中使用。SGM是完全自包含的，更容易操作化，可以利用更多训练数据，在时间泛化能力方面显著优于LAM。本文可以作为气象机构开发操作数据驱动预报系统时选择LAM或SGM的起点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Regional machine learning weather prediction (MLWP) models based on graphneural networks have recently demonstrated remarkable predictive accuracy,outperforming numerical weather prediction models at lower computational costs.In particular, limited-area model (LAM) and stretched-grid model (SGM)approaches have emerged for generating high-resolution regional forecasts,based on initial conditions from a regional (re)analysis. While LAM useslateral boundaries from an external global model, SGM incorporates a globaldomain at lower resolution. This study aims to understand how the differencesin model design impact relative performance and potential applications.Specifically, the strengths and weaknesses of these two approaches areidentified for generating deterministic regional forecasts over Europe. Usingthe Anemoi framework, models of both types are built by minimally adapting ashared architecture and trained using global and regional reanalyses in anear-identical setup. Several inference experiments have been conducted toexplore their relative performance and highlight key differences. Results showthat both LAM and SGM are competitive deterministic MLWP models with generallyaccurate and comparable forecasting performance over the regional domain.Various differences were identified in the performance of the models acrossapplications. LAM is able to successfully exploit high-quality boundaryforcings to make predictions within the regional domain and is suitable incontexts where global data is difficult to acquire. SGM is fully self-containedfor easier operationalisation, can take advantage of more training data andsignificantly surpasses LAM in terms of (temporal) generalisability. Our papercan serve as a starting point for meteorological institutes to guide theirchoice between LAM and SGM in developing an operational data-driven forecastingsystem.</description>
      <author>example@mail.com (Jasper S. Wijnands, Michiel Van Ginderachter, Bastien François, Sophie Buurman, Piet Termonia, Dieter Van den Bleeken)</author>
      <guid isPermaLink="false">2507.18378v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Goal-based Trajectory Prediction for improved Cross-Dataset Generalization</title>
      <link>http://arxiv.org/abs/2507.18196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted on IEEE ITSC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的图神经网络，通过结合交通参与者和矢量化路网信息，采用多阶段目标分类方法，提高自动驾驶系统对新场景的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;实现完全自动驾驶需要理解周围环境，特别是预测其他交通参与者的未来状态具有挑战性。当前最先进的模型在真实数据集上表现良好，但在新区域部署时性能显著下降，表明缺乏泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提高自动驾驶模型在新场景中的泛化能力，解决模型在未见过的区域性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;引入一种新的图神经网络(GNN)，利用由交通参与者和矢量化路网组成的异构图，采用多阶段方法对预测轨迹的终点进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;通过跨数据集评估（在Argoverse2上训练，在NuScenes上评估）证明，目标选择过程有效提高了模型对新场景的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;结合交通参与者和路网信息的异构图神经网络，通过多阶段目标分类方法，可以有效提高自动驾驶系统对未见场景的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;要实现完全自动驾驶，必须很好地理解周围环境。特别是预测其他交通参与者的未来状态构成了一个非平凡的挑战。当在真实数据集（如Argoverse2、NuScenes）上训练时，当前最先进的模型已经显示出有希望的结果。然而，当这些模型部署到新的/未见过的区域时会出现问题，性能显著下降，表明模型缺乏泛化能力。在这项工作中，我们引入了一种新的图神经网络(GNN)，该网络利用由交通参与者和矢量化路网组成的异构图。后者采用多阶段方法对目标（即预测轨迹的终点）进行分类，从而提高对未见场景的泛化能力。我们通过跨数据集评估（即在Argoverse2上训练并在NuScenes上评估）证明了目标选择过程的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域中轨迹预测模型在不同数据集之间泛化能力不足的问题。当模型在一种数据集（如Argoverse2）上训练后，在另一种未见过的数据集（如NuScenes）上性能会显著下降。这个问题在现实中至关重要，因为自动驾驶系统需要在各种不同环境中安全运行，性能下降可能导致不准确的轨迹预测，影响行车安全，同时表明模型过度拟合特定训练数据特征，缺乏对新环境的适应能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有模型在跨数据集评估时性能下降的问题，然后借鉴了图神经网络和目标导向的轨迹预测思想。他们参考了TNT、GoRela等目标导向方法，但设计了创新的多阶段目标选择过程，区分道路约束型（车辆）和非道路约束型（行人）交通参与者。对于车辆，采用两阶段目标选择（先选车道，再选车道上的点）；对于行人，直接选择周围人工点作为目标。整体架构采用编码器-解码器结构的GNN，先编码场景信息，再生成查询特征用于目标选择，最后基于目标点生成轨迹。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多阶段目标选择来引导轨迹生成，区分不同类型交通参与者的运动特性，并利用异构图神经网络提高泛化能力。整体流程包括：1) 将输入数据组织为包含四类节点（代理、车道、点、代理查询）的异构图；2) 编码器使用图注意力层处理节点和边信息；3) 解码器生成代理查询特征；4) 多阶段目标选择（车辆先选车道再选点，行人直接选点）；5) 目标回归优化；6) 基于目标点生成轨迹；7) 结合多种损失函数进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多阶段目标选择机制，区分道路约束和非道路约束交通参与者；2) 模块化异构图架构，实现平移和旋转不变性；3) 软绑定轨迹到道路网络，降低偏离道路率。相比之前工作，不同之处在于：不同于TNT和GoRela的简单采样，采用分阶段选择；不同于DenseTNT的密集采样，减少计算复杂度；能同时预测所有代理轨迹而非单个；显著提高了跨数据集泛化能力，使轨迹更符合道路约束和人类习惯。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于多阶段目标选择的图神经网络方法，通过区分不同类型交通参与者，显著提高了轨迹预测模型在不同数据集之间的泛化能力，使预测轨迹更符合道路网络约束和人类驾驶习惯。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To achieve full autonomous driving, a good understanding of the surroundingenvironment is necessary. Especially predicting the future states of othertraffic participants imposes a non-trivial challenge. Current SotA-modelsalready show promising results when trained on real datasets (e.g. Argoverse2,NuScenes). Problems arise when these models are deployed to new/unseen areas.Typically, performance drops significantly, indicating that the models lackgeneralization. In this work, we introduce a new Graph Neural Network (GNN)that utilizes a heterogeneous graph consisting of traffic participants andvectorized road network. Latter, is used to classify goals, i.e. endpoints ofthe predicted trajectories, in a multi-staged approach, leading to a bettergeneralization to unseen scenarios. We show the effectiveness of the goalselection process via cross-dataset evaluation, i.e. training on Argoverse2 andevaluating on NuScenes.</description>
      <author>example@mail.com (Daniel Grimm, Ahmed Abouelazm, J. Marius Zöllner)</author>
      <guid isPermaLink="false">2507.18196v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Logical Characterizations of GNNs with Mean Aggregation</title>
      <link>http://arxiv.org/abs/2507.18145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了以平均值作为聚合函数的图神经网络(GNNs)的表达能力及其与不同逻辑系统的关系&lt;h4&gt;背景&lt;/h4&gt;图神经网络作为处理图结构数据的重要模型，其表达能力取决于使用的聚合函数&lt;h4&gt;目的&lt;/h4&gt;探究平均聚合GNNs的表达能力，并将其与最大值聚合和求和聚合的GNNs进行比较&lt;h4&gt;方法&lt;/h4&gt;在非均匀和均匀两种设置下分析平均GNNs的表达能力，研究其与比例模态逻辑、模态逻辑和分级模态逻辑的关系&lt;h4&gt;主要发现&lt;/h4&gt;在非均匀设置下，平均GNNs与比例模态逻辑表达能力相同，介于max和sum聚合GNNs之间；在均匀设置下，相对于MSO，平均GNNs表达能力弱于sum和max聚合GNNs&lt;h4&gt;结论&lt;/h4&gt;平均聚合GNNs的表达能力受聚合函数类型和设置影响，在放弃某些假设时其表达能力会增加&lt;h4&gt;翻译&lt;/h4&gt;我们研究了以平均值作为聚合函数的图神经网络(GNNs)的表达能力。在非均匀设置下，我们证明这种GNNs与比例模态逻辑具有完全相同的能力，比例模态逻辑有模态算子表达一个顶点的后继中至少有一定比例满足特定属性。非均匀设置下，平均GNNs的表达能力高于最大值聚合的GNNs，但低于求和聚合的GNNs——后者分别由模态逻辑和分级模态逻辑所表征。在均匀设置下，在组合函数是连续的且分类函数是阈值函数的自然假设下，相对于MSO，其表达能力恰好是非交替模态逻辑的能力。这意味着，在均匀设置下，相对于MSO，平均GNNs的表达能力严格弱于求和GNNs和最大值GNNs。当任何假设被放弃时，表达能力会增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the expressive power of graph neural networks (GNNs) with mean asthe aggregation function. In the non-uniform setting, we show that such GNNshave exactly the same expressive power as ratio modal logic, which has modaloperators expressing that at least a certain ratio of the successors of avertex satisfies a specified property. The non-uniform expressive power of meanGNNs is thus higher than that of GNNs with max aggregation, but lower than forsum aggregation--the latter are characterized by modal logic and graded modallogic, respectively. In the uniform setting, we show that the expressive powerrelative to MSO is exactly that of alternation-free modal logic, under thenatural assumptions that combination functions are continuous andclassification functions are thresholds. This implies that, relative to MSO andin the uniform setting, mean GNNs are strictly less expressive than sum GNNsand max GNNs. When any of the assumptions is dropped, the expressive powerincreases.</description>
      <author>example@mail.com (Moritz Schönherr, Carsten Lutz)</author>
      <guid isPermaLink="false">2507.18145v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs</title>
      <link>http://arxiv.org/abs/2507.18668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为双图注意力知识追踪(DGAKT)的新模型，解决了现有知识追踪方法在处理大型图和长学习序列时计算成本增加的问题，通过子图处理方法显著提高了计算效率，同时保持了优异的性能表现。&lt;h4&gt;背景&lt;/h4&gt;在线学习的兴起促进了各种知识追踪(KT)方法的发展，但现有方法在使用大型图和长学习序列时忽视了计算成本增加的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有知识追踪方法在处理大型图和长学习序列时面临的计算效率挑战，降低内存和计算需求。&lt;h4&gt;方法&lt;/h4&gt;提出双图注意力知识追踪(DGAKT)模型，这是一种图神经网络模型，利用表示学生-练习-知识点关系的子图中的高阶信息，通过仅处理每个目标交互的相关子图来提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验结果表明，DGAKT不仅优于现有的KT模型，还在资源效率方面设立了新的标准，解决了先前KT方法 largely忽视的关键需求。&lt;h4&gt;结论&lt;/h4&gt;DGAKT模型通过创新的子图处理方法，成功解决了在线学习环境中知识追踪的计算效率问题，实现了性能与资源利用效率的平衡。&lt;h4&gt;翻译&lt;/h4&gt;在线学习的兴起促使了各种知识追踪(KT)方法的发展。然而，现有方法在使用大型图和长学习序列时忽视了计算成本增加的问题。为解决这一问题，我们引入了双图注意力知识追踪(DGAKT)，这是一种图神经网络模型，旨在利用表示学生-练习-知识点关系的子图中的高阶信息。DGAKT采用基于子图的方法来提高计算效率。通过仅处理每个目标交互的相关子图，DGAKT与完整的全局图模型相比，显著减少了内存和计算需求。大量的实验结果表明，DGAKT不仅优于现有的KT模型，还在资源效率方面设立了新的标准，解决了先前KT方法 largely忽视的关键需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of online learning has led to the development of various knowledgetracing (KT) methods. However, existing methods have overlooked the problem ofincreasing computational cost when utilizing large graphs and long learningsequences. To address this issue, we introduce Dual Graph Attention-basedKnowledge Tracing (DGAKT), a graph neural network model designed to leveragehigh-order information from subgraphs representing student-exercise-KCrelationships. DGAKT incorporates a subgraph-based approach to enhancecomputational efficiency. By processing only relevant subgraphs for each targetinteraction, DGAKT significantly reduces memory and computational requirementscompared to full global graph models. Extensive experimental resultsdemonstrate that DGAKT not only outperforms existing KT models but also sets anew standard in resource efficiency, addressing a critical need that has beenlargely overlooked by prior KT approaches.</description>
      <author>example@mail.com (Donghee Han, Daehee Kim, Minjun Lee, Daeyoung Roh, Keejun Han, Mun Yong Yi)</author>
      <guid isPermaLink="false">2507.18668v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach</title>
      <link>http://arxiv.org/abs/2507.20019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages. PyTorch code for few-shot anomaly detection using  meta-learning is available upon request or can be shared via GitHub&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出一个元学习框架，用于在标记数据有限的情况下检测不同领域的人类语言异常&lt;h4&gt;背景&lt;/h4&gt;语言异常（如垃圾邮件、假新闻、仇恨言论）因其稀疏性和变异性构成重大挑战&lt;h4&gt;目的&lt;/h4&gt;将异常检测视为少样本二元分类问题，利用元学习训练能跨任务泛化的模型&lt;h4&gt;方法&lt;/h4&gt;结合周期性训练、原型网络和领域重采样，以快速适应新的异常检测任务&lt;h4&gt;主要发现&lt;/h4&gt;在F1分数和AUC分数上，该方法优于强基线方法&lt;h4&gt;结论&lt;/h4&gt;元学习框架能有效处理标记数据有限情况下的多领域语言异常检测&lt;h4&gt;翻译&lt;/h4&gt;我们提出一个元学习框架，用于在标记数据有限的情况下检测不同领域的人类语言异常。从垃圾邮件、假新闻到仇恨言论的语言异常因其稀疏性和变异性构成重大挑战。我们将异常检测视为少样本二元分类问题，并利用元学习来训练能跨任务泛化的模型。使用来自SMS垃圾邮件、COVID-19假新闻和仇恨言论等领域的数据集，我们在标记异常极少的情况下评估模型在未见任务上的泛化能力。我们的方法结合了周期性训练、原型网络和领域重采样，以快速适应新的异常检测任务。实验结果表明，我们的方法在F1分数和AUC分数上优于强基线方法。我们还发布了代码和基准，以促进少样本文本异常检测的进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a meta learning framework for detecting anomalies in humanlanguage across diverse domains with limited labeled data. Anomalies inlanguage ranging from spam and fake news to hate speech pose a major challengedue to their sparsity and variability. We treat anomaly detection as a few shotbinary classification problem and leverage meta-learning to train models thatgeneralize across tasks. Using datasets from domains such as SMS spam, COVID-19fake news, and hate speech, we evaluate model generalization on unseen taskswith minimal labeled anomalies. Our method combines episodic training withprototypical networks and domain resampling to adapt quickly to new anomalydetection tasks. Empirical results show that our method outperforms strongbaselines in F1 and AUC scores. We also release the code and benchmarks tofacilitate further research in few-shot text anomaly detection.</description>
      <author>example@mail.com (Saurav Singla, Aarav Singla, Advik Gupta, Parnika Gupta)</author>
      <guid isPermaLink="false">2507.20019v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding</title>
      <link>http://arxiv.org/abs/2507.21037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多源域适应框架，用于解码运动想象脑电图信号，该框架利用预训练的大脑基础模型进行智能源域选择，并采用柯西-施瓦茨散度联合进行特征级和决策级对齐，提高了解码性能同时减少了训练时间。&lt;h4&gt;背景&lt;/h4&gt;运动想象脑电图信号解码是一种重要的非侵入式脑机接口范式，深度学习虽显著推进了该领域，但仍面临受试者间变异性大、标记数据有限以及需要昂贵校准等挑战。现有多源域适应方法存在无差别整合所有源域导致负迁移和计算成本过高，以及忽略特征与决策输出间依赖关系的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有MI-EEG解码方法中的两个主要问题：一是改进源域选择策略避免负迁移和计算成本过高；二是同时考虑特征级和决策级对齐以保持类别判别能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的多源域适应框架，包含两个关键创新：1)利用预训练的大脑基础模型进行动态和智能的源主题选择；2)使用柯西-施瓦茨和条件CS散度联合执行特征级和决策级对齐，增强域不变性同时保持类别判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准MI-EEG数据集上的广泛评估表明，该框架优于一系列最先进的基线方法。使用大型源池进行的额外实验验证了BFM引导选择的可扩展性和效率，显著减少了训练时间而不牺牲性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的MSDA框架通过智能的源域选择和多层次对齐策略，有效解决了MI-EEG解码中的受试者间变性和标记数据有限的问题，为脑机接口应用提供了更高效、更准确的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;解码运动想象脑电图信号是一种用于控制外部系统的非侵入式脑机接口范式，深度学习已显著推动了该领域的发展。然而，由于受试者间存在显著差异和标记的目标数据有限，MI-EEG解码仍然具有挑战性，这需要为新用户进行昂贵的校准。许多现有的多源域适应方法无差别地整合所有可用的源域，忽视了EEG信号中受试者间的大差异，导致负迁移和过高的计算成本。此外，虽然许多方法专注于特征分布对齐，但它们常常忽略特征与决策级输出之间的显式依赖关系，限制了它们保持判别结构的能力。为了解决这些差距，我们提出了一种新的MSDA框架，该框架利用预训练的大脑基础模型进行动态和智能的源主题选择，确保只有相关源域对适应做出贡献。此外，我们采用柯西-施瓦茨和条件CS散度联合执行特征级和决策级对齐，增强域不变性同时保持类别判别能力。在两个基准MI-EEG数据集上的广泛评估表明，我们的框架优于一系列最先进的基线方法。使用大型源池进行的额外实验验证了BFM引导选择的可扩展性和效率，显著减少了训练时间而不牺牲性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decoding motor imagery (MI) electroencephalogram (EEG) signals, a keynon-invasive brain-computer interface (BCI) paradigm for controlling externalsystems, has been significantly advanced by deep learning. However, MI-EEGdecoding remains challenging due to substantial inter-subject variability andlimited labeled target data, which necessitate costly calibration for newusers. Many existing multi-source domain adaptation (MSDA) methodsindiscriminately incorporate all available source domains, disregarding thelarge inter-subject differences in EEG signals, which leads to negativetransfer and excessive computational costs. Moreover, while many approachesfocus on feature distribution alignment, they often neglect the explicitdependence between features and decision-level outputs, limiting their abilityto preserve discriminative structures. To address these gaps, we propose anovel MSDA framework that leverages a pretrained large Brain Foundation Model(BFM) for dynamic and informed source subject selection, ensuring only relevantsources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS)and Conditional CS (CCS) divergences to jointly perform feature-level anddecision-level alignment, enhancing domain invariance while maintaining classdiscriminability. Extensive evaluations on two benchmark MI-EEG datasetsdemonstrate that our framework outperforms a broad range of state-of-the-artbaselines. Additional experiments with a large source pool validate thescalability and efficiency of BFM-guided selection, which significantly reducestraining time without sacrificing performance.</description>
      <author>example@mail.com (Jinzhou Wu, Baoping Tang, Qikang Li, Yi Wang, Cheng Li, Shujian Yu)</author>
      <guid isPermaLink="false">2507.21037v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning Stellar Spectra Foundation Models with LoRA</title>
      <link>http://arxiv.org/abs/2507.20972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures. Accepted to the Machine Learning for Astrophysics  (ML4Astro) Colocated Workshop at ICML 2025. Presented as a spotlight talk&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型结合低秩自适应技术可有效适应不同光谱调查，实现少样本学习，并将预训练知识迁移到新领域。&lt;h4&gt;背景&lt;/h4&gt;基础模型开始影响恒星光谱学领域，其中光谱以结构化、类语言的形式编码丰富的物理信息。然而，将这些模型适应到具有不同分辨率和覆盖范围的异构调查中是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;应用低秩自适应技术微调SpecCLIP模型，使其能够在DESI早期数据发布光谱上有效工作，探索如何将预训练模型扩展到新的仪器和调查领域。&lt;h4&gt;方法&lt;/h4&gt;使用低秩自适应技术微调SpecCLIP模型，这是一个在LAMOST和Gaia XP光谱上对比预训练的模型，并将其应用于DESI早期数据发布光谱的下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA能够在DESI上实现少样本学习，性能因微调模块而异，并且受益于预训练模型中嵌入的Gaia XP知识。&lt;h4&gt;结论&lt;/h4&gt;LoRA为将光谱基础模型扩展到新的仪器和调查领域提供了一种轻量级且有效的策略。&lt;h4&gt;翻译&lt;/h4&gt;基础模型开始影响恒星光谱学，其中光谱以结构化、类语言的形式编码丰富的物理信息。一个关键挑战是使这些模型能够适应具有不同分辨率和覆盖范围的异构调查。我们应用低秩自适应来微调SpecCLIP——一个在LAMOST和Gaia XP光谱上对比预训练的模型——用于DESI早期数据发布光谱的下游任务。我们表明LoRA能够在DESI上实现少样本学习，性能因微调模块而异，并受益于预训练模型中嵌入的Gaia XP知识。我们的研究结果表明，LoRA为将光谱基础模型扩展到新的仪器和调查领域提供了一种轻量级且有效的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are beginning to impact stellar spectroscopy, where spectraencode rich physical information in a structured, language-like form. A keychallenge is adapting these models across heterogeneous surveys with differingresolution and coverage. We apply Low-Rank Adaptation (LoRA) to fine-tuneSpecCLIP--a contrastively pre-trained model on LAMOST and Gaia XP spectra--fordownstream tasks on DESI Early Data Release (EDR) spectra. We show that LoRAenables few-shot learning on DESI, with performance varying by fine-tunedmodule and benefiting from Gaia XP knowledge embedded in the pre-trained model.Our results demonstrate that LoRA provides a lightweight and effective strategyfor extending spectral foundation models to new instruments and survey domains.</description>
      <author>example@mail.com (Xiaosheng Zhao, Yuan-Sen Ting, Alexander S. Szalay, Yang Huang)</author>
      <guid isPermaLink="false">2507.20972v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>First Hallucination Tokens Are Different from Conditional Ones</title>
      <link>http://arxiv.org/abs/2507.20836v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4.5 pages, 3 figures, Dataset, Knowledge Paper, Hallucination,  Trustworthiness&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了基础模型中幻觉的标记级检测问题，发现第一个幻觉标记比后续条件标记携带更强的信号且更易检测。&lt;h4&gt;背景&lt;/h4&gt;幻觉（生成不真实内容）是基础模型的主要关注点之一，标记级幻觉检测对于实时过滤和针对性纠正是至关重要的。&lt;h4&gt;目的&lt;/h4&gt;理解标记序列中幻觉信号的变化，特别是幻觉跨度内标记位置对信号强度的影响。&lt;h4&gt;方法&lt;/h4&gt;利用RAGTruth语料库（包含标记级注释和重现的对数几率）分析幻觉信号与标记位置的关系。&lt;h4&gt;主要发现&lt;/h4&gt;第一个幻觉标记携带更强的信号，比条件标记更容易检测。&lt;h4&gt;结论&lt;/h4&gt;研究结果有助于改进对标记级幻觉的理解，为更精确的幻觉检测提供了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;幻觉，即生成不真实内容，是关于基础模型的主要关注点之一。标记级幻觉检测对于实时过滤和有针对性的纠正是至关重要的，然而，标记序列中幻觉信号的变化尚未被充分理解。利用具有标记级注释和重现对数几率的RAGTruth语料库，我们分析了这些信号如何依赖于幻觉跨度内标记的位置，从而增进对标记级幻觉的理解。我们的结果表明，第一个幻觉标记携带更强的信号，比条件标记更容易检测。我们在https://github.com/jakobsnl/RAGTruth_Xtended发布了我们的分析框架，以及对数几率重现和指标计算的代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hallucination, the generation of untruthful content, is one of the majorconcerns regarding foundational models. Detecting hallucinations at the tokenlevel is vital for real-time filtering and targeted correction, yet thevariation of hallucination signals within token sequences is not fullyunderstood. Leveraging the RAGTruth corpus with token-level annotations andreproduced logits, we analyse how these signals depend on a token's positionwithin hallucinated spans, contributing to an improved understanding oftoken-level hallucination. Our results show that the first hallucinated tokencarries a stronger signal and is more detectable than conditional tokens. Werelease our analysis framework, along with code for logit reproduction andmetric computation at https://github.com/jakobsnl/RAGTruth_Xtended.</description>
      <author>example@mail.com (Jakob Snel, Seong Joon Oh)</author>
      <guid isPermaLink="false">2507.20836v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the Inductive Setting</title>
      <link>http://arxiv.org/abs/2507.20834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究指出CLIP模型在少样本分类评估中存在过度拟合已见数据集的问题，提出使用'遗忘'技术创建真正的归纳学习环境，并在此基础上开发出性能更优的少样本分类方法。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一种在少样本设置下具有可迁移分类性能的基础模型，已有多种方法展示了使用少样本示例提高CLIP性能的能力，但这些技术都使用标准少样本数据集进行基准测试。&lt;h4&gt;目的&lt;/h4&gt;解决现有CLIP少样本分类评估方式不能真实反映归纳泛化能力的问题，提供真正的归纳基线，并开发改进的少样本分类技术。&lt;h4&gt;方法&lt;/h4&gt;提出一个使用'遗忘'技术的管道来去除CLIP模型对数据集的记忆，从而获得真正的归纳学习环境；并在此基础上开发改进的少样本分类技术。&lt;h4&gt;主要发现&lt;/h4&gt;在新的归纳设置中，各种方法性能显著下降（在13个基线和多个数据集上平均下降55%）；使用'遗忘'技术验证了方法的正确性；提出的改进技术在5880次实验中始终优于其他13种最新基线方法。&lt;h4&gt;结论&lt;/h4&gt;识别了CLIP基础少样本分类评估中的问题，使用'遗忘'技术提供了解决方案，提出了新的基准测试，并提供了改进的方法。&lt;h4&gt;翻译&lt;/h4&gt;CLIP是一种在少样本设置下具有可迁移分类性能的基础模型。已有几种方法展示了使用少样本示例提高CLIP性能的能力。然而，迄今为止，所有这些技术都使用标准少样本数据集进行基准测试。我们认为这种评估方式不能真实反映使用少样本示例的归纳泛化能力。由于大多数数据集已被CLIP模型见过，因此这种设置可称为部分归纳设置。为解决此问题，我们提出一个使用'遗忘'技术来获得真正归纳基线的管道。在这种新的归纳设置中，各种方法性能显著下降（在13个基线和多个数据集上平均下降55%）。我们使用'遗忘'技术验证了该方法的有效性。提出了一种改进的少样本分类技术，在5880次实验的综合分析中（包括不同数据集、不同少样本示例数量、遗忘设置和不同随机种子），始终优于其他13种最新基线方法。因此，我们识别了CLIP基础少样本分类评估中的问题，使用'遗忘'技术提供了解决方案，提出了新的基准测试，并提供了改进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CLIP is a foundational model with transferable classification performance inthe few-shot setting. Several methods have shown improved performance of CLIPusing few-shot examples. However, so far, all these techniques have beenbenchmarked using standard few-shot datasets. We argue that this mode ofevaluation does not provide a true indication of the inductive generalizationability using few-shot examples. As most datasets have been seen by the CLIPmodel, the resultant setting can be termed as partially transductive. To solvethis, we propose a pipeline that uses an unlearning technique to obtain trueinductive baselines. In this new inductive setting, the methods show asignificant drop in performance (-55% on average among 13 baselines withmultiple datasets). We validate the unlearning technique using oraclebaselines. An improved few-shot classification technique is proposed thatconsistently obtains state-of-the-art performance over 13 other recent baselinemethods on a comprehensive analysis with 5880 experiments - varying thedatasets, differing number of few-shot examples, unlearning setting, and withdifferent seeds. Thus, we identify the issue with the evaluation of CLIP-basedfew-shot classification, provide a solution using unlearning, propose newbenchmarks, and provide an improved method.</description>
      <author>example@mail.com (Alexey Kravets, Da Chen, Vinay P. Namboodiri)</author>
      <guid isPermaLink="false">2507.20834v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning</title>
      <link>http://arxiv.org/abs/2507.20776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 6 figures, 20 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RingMo-Agent是一种新型遥感视觉语言模型，能够处理来自多种模式和平台的遥感图像数据，通过大规模数据集支持、模态自适应表示学习和统一任务建模，实现了对多样化遥感图像的有效处理，在视觉理解、复杂分析任务以及跨平台和跨模态泛化方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;遥感图像来自多种模式和平台，由于传感器特性和成像视角的差异，表现出多样的细节。现有的遥感领域视觉语言研究主要依赖于相对同质的数据源，且局限于传统的视觉感知任务，如分类或描述，因此无法作为统一且独立的框架有效处理现实应用中来自不同来源的遥感图像。&lt;h4&gt;目的&lt;/h4&gt;提出RingMo-Agent模型，用于处理多模态、多平台数据，并根据用户文本指令执行感知和推理任务，解决现有方法无法有效处理多样化遥感图像来源的问题。&lt;h4&gt;方法&lt;/h4&gt;RingMo-Agent模型具有三个主要特点：1) 由名为RS-VL3M的大规模视觉语言数据集支持，包含超过300万张图像-文本对，涵盖光学、SAR和红外模式，数据来自卫星和无人机平台，覆盖感知和具有挑战性的推理任务；2) 通过集成分离的嵌入层来学习模态自适应表示，为异构模态构建独立特征并减少跨模态干扰；3) 通过引入特定任务令牌和基于令牌的高维隐藏状态解码机制来统一任务建模，该机制专为长距离空间任务设计。&lt;h4&gt;主要发现&lt;/h4&gt;在各种遥感视觉语言任务上的大量实验表明，RingMo-Agent不仅在视觉理解和复杂分析任务中证明有效，而且在不同平台和传感模态上表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;RingMo-Agent是一个能够处理多模态、多平台遥感数据的统一框架，能够执行感知和推理任务，通过大规模数据集支持、模态自适应表示学习和统一任务建模，解决了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;遥感图像来自多种模式和和多平台，由于传感器特性和成像视角的差异，展现出多样的细节。现有的遥感视觉语言研究主要依赖于相对同质的数据源，并且仍然局限于传统的视觉感知任务，如分类或描述。因此，这些方法无法作为统一且独立的框架，有效处理现实应用中来自不同来源的遥感图像。为解决这些问题，我们提出了RingMo-Agent，这是一个设计用于处理多模态和多平台数据的模型，能够根据用户文本指令执行感知和推理任务。与现有模型相比，RingMo-Agent具有以下特点：1) 由名为RS-VL3M的大规模视觉语言数据集支持，包含超过300万张图像-文本对，涵盖光学、SAR和红外模式，数据来自卫星和无人机平台，覆盖感知和具有挑战性的推理任务；2) 通过集成分离的嵌入层学习模态自适应表示，为异构模态构建独立特征并减少跨模态干扰；3) 通过引入特定任务令牌和采用基于令牌的高维隐藏状态解码机制来统一任务建模，该机制专为长距离空间任务设计。在各种遥感视觉语言任务上的大量实验表明，RingMo-Agent不仅在视觉理解和复杂分析任务中证明有效，而且在不同平台和传感模态上表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing (RS) images from multiple modalities and platforms exhibitdiverse details due to differences in sensor characteristics and imagingperspectives. Existing vision-language research in RS largely relies onrelatively homogeneous data sources. Moreover, they still remain limited toconventional visual perception tasks such as classification or captioning. As aresult, these methods fail to serve as a unified and standalone frameworkcapable of effectively handling RS imagery from diverse sources in real-worldapplications. To address these issues, we propose RingMo-Agent, a modeldesigned to handle multi-modal and multi-platform data that performs perceptionand reasoning tasks based on user textual instructions. Compared with existingmodels, RingMo-Agent 1) is supported by a large-scale vision-language datasetnamed RS-VL3M, comprising over 3 million image-text pairs, spanning optical,SAR, and infrared (IR) modalities collected from both satellite and UAVplatforms, covering perception and challenging reasoning tasks; 2) learnsmodality adaptive representations by incorporating separated embedding layersto construct isolated features for heterogeneous modalities and reducecross-modal interference; 3) unifies task modeling by introducing task-specifictokens and employing a token-based high-dimensional hidden state decodingmechanism designed for long-horizon spatial tasks. Extensive experiments onvarious RS vision-language tasks demonstrate that RingMo-Agent not only proveseffective in both visual understanding and sophisticated analytical tasks, butalso exhibits strong generalizability across different platforms and sensingmodalities.</description>
      <author>example@mail.com (Huiyang Hu, Peijin Wang, Yingchao Feng, Kaiwen Wei, Wenxin Yin, Wenhui Diao, Mengyu Wang, Hanbo Bi, Kaiyue Kang, Tong Ling, Kun Fu, Xian Sun)</author>
      <guid isPermaLink="false">2507.20776v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Watermarking Large Language Model-based Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2507.20762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Waltz的新型水印框架，用于保护基于大型语言模型的时间序列预测(LLMTS)模型的输出，解决知识产权保护问题并防止生成误导性或伪造的时间序列数据。&lt;h4&gt;背景&lt;/h4&gt;基于大型语言模型的时间序列预测(LLMTS)在处理复杂多样的时间序列数据方面显示出巨大潜力，代表了时间序列分析基础模型的重要进展。然而，这一新兴范式带来了两个关键挑战：一是巨大的商业潜力和资源密集型开发引发了知识产权保护的紧迫问题；二是其强大的时间序列预测能力可能被滥用来产生误导性或伪造的深度伪造时间序列数据。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，作者探索了对LLMTS模型输出进行水印的方法，即在生成的时间序列数据中嵌入不可见的信号，这些信号可以通过专门算法被检测到。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种新颖的事后水印框架Waltz，该框架与现有的LLMTS模型广泛兼容。Waltz的灵感来自于经验观察：时间序列块嵌入很少与特定的LLM标记集合对齐，作者将这些标记称为'冷标记'。利用这一见解，Waltz通过重新连接块嵌入和冷标记嵌入之间的相似性统计来嵌入水印，并使用相似性z分数检测水印。为了最小化潜在的副作用，作者引入了一种基于相似性的嵌入位置识别策略，并采用投影梯度下降将水印噪声限制在定义的边界内。&lt;h4&gt;主要发现&lt;/h4&gt;使用两个流行的LLMTS模型在七个基准数据集上进行的广泛实验表明，Waltz实现了高水印检测准确性，同时对生成的时间序列质量影响最小。&lt;h4&gt;结论&lt;/h4&gt;Waltz水印框架为LLMTS模型提供了一种有效的知识产权保护手段，同时能够检测和防止可能被滥用来生成误导性数据的时间序列预测。&lt;h4&gt;翻译&lt;/h4&gt;基于大型语言模型的时间序列预测(LLMTS)在处理复杂多样的时间序列数据方面显示出巨大的前景，代表了时间序列分析基础模型的重要进展。然而，这一新兴范式引入了两个关键挑战。首先，巨大的商业潜力和资源密集型开发引发了关于知识产权保护的紧迫问题。其次，它们强大的时间序列预测能力可能被滥用来产生误导性或伪造的深度伪造时间序列数据。为了解决这些问题，我们探索了对LLMTS模型输出进行水印的方法，即在生成的时间序列数据中嵌入不可见的信号，这些信号可以通过专门算法被检测到。我们提出了一种新颖的事后水印框架Waltz，该框架与现有的LLMTS模型广泛兼容。Waltz的灵感来自于经验观察：时间序列块嵌入很少与特定的LLM标记集合对齐，我们称之为'冷标记'。利用这一见解，Waltz通过重新连接块嵌入和冷标记嵌入之间的相似性统计来嵌入水印，并使用相似性z分数检测水印。为了最小化潜在的副作用，我们引入了一种基于相似性的嵌入位置识别策略，并采用投影梯度下降将水印噪声限制在定义的边界内。使用两个流行的LLMTS模型在七个基准数据集上进行的广泛实验表明，Waltz实现了高水印检测准确性，同时对生成的时间序列质量影响最小。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Model-based Time Series Forecasting (LLMTS) has shownremarkable promise in handling complex and diverse temporal data, representinga significant step toward foundation models for time series analysis. However,this emerging paradigm introduces two critical challenges. First, thesubstantial commercial potential and resource-intensive development raiseurgent concerns about intellectual property (IP) protection. Second, theirpowerful time series forecasting capabilities may be misused to producemisleading or fabricated deepfake time series data. To address these concerns,we explore watermarking the outputs of LLMTS models, that is, embeddingimperceptible signals into the generated time series data that remaindetectable by specialized algorithms. We propose a novel post-hoc watermarkingframework, Waltz, which is broadly compatible with existing LLMTS models. Waltzis inspired by the empirical observation that time series patch embeddings arerarely aligned with a specific set of LLM tokens, which we term ``coldtokens''. Leveraging this insight, Waltz embeds watermarks by rewiring thesimilarity statistics between patch embeddings and cold token embeddings, anddetects watermarks using similarity z-scores. To minimize potential sideeffects, we introduce a similarity-based embedding position identificationstrategy and employ projected gradient descent to constrain the watermark noisewithin a defined boundary. Extensive experiments using two popular LLMTS modelsacross seven benchmark datasets demonstrate that Waltz achieves high watermarkdetection accuracy with minimal impact on the quality of the generated timeseries.</description>
      <author>example@mail.com (Wei Yuan, Chaoqun Yang, Yu Xing, Tong Chen, Nguyen Quoc Viet Hung, Hongzhi Yin)</author>
      <guid isPermaLink="false">2507.20762v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>FMimic: Foundation Models are Fine-grained Action Learners from Human Videos</title>
      <link>http://arxiv.org/abs/2507.20622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted to International Journal of Robotics Research(IJRR)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了FMimic，一种利用基础模型直接学习细粒度可泛化技能的新范式，仅需少量人类视频就能在各种任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;视觉模仿学习(VIL)为机器人系统获取新技能提供了高效直观的策略。视觉语言模型(VLMs)等基础模型在VIL任务中表现出色，但现有方法主要利用这些模型学习高层计划，依赖预定义运动基元执行物理交互，这仍是机器人系统的主要瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出FMimic，一种利用基础模型直接在细粒度动作层面学习可泛化技能的新范式，仅使用有限数量的人类视频。&lt;h4&gt;方法&lt;/h4&gt;FMimic是一种新颖的范式，利用基础模型直接学习可泛化技能，可以在细粒度动作层面进行学习，仅需少量人类视频。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用单个人类视频时，FMimic就能提供强大性能；使用五个视频时，性能显著优于所有其他方法。在RLBench多任务实验中性能提高39%以上，在现实世界操作任务中提高29%以上，在高精度任务中超过基线34%以上，在长时程任务中超过基线47%以上。&lt;h4&gt;结论&lt;/h4&gt;FMimic是一种有效的方法，可以仅使用有限数量的人类视频来学习细粒度的可泛化技能，在各种任务中都表现出色，特别是在高精度和长时程任务中。&lt;h4&gt;翻译&lt;/h4&gt;视觉模仿学习(VIL)为机器人系统获取新技能提供了一种高效直观的策略。最近基础模型的进步，特别是视觉语言模型(VLMs)，在VIL任务的视觉和语言推理方面表现出显著能力。尽管如此，现有方法主要利用这些模型从人类演示中学习高层计划，依赖于预定义的运动基元来执行物理交互，这仍然是机器人系统的主要瓶颈。在这项工作中，我们提出了FMimic，一种新颖的范式，利用基础模型直接在细粒度动作层面学习可泛化技能，仅使用有限数量的人类视频。大量实验表明，我们的FMimic仅使用单个人类视频就能提供强大的性能，使用五个视频时显著优于所有其他方法。此外，我们的方法在RLBench多任务实验和现实世界操作任务中分别显示出超过39%和29%的显著改进，在高精度任务中超过基线34%以上，在长时程任务中超过基线47%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual imitation learning (VIL) provides an efficient and intuitive strategyfor robotic systems to acquire novel skills. Recent advancements in foundationmodels, particularly Vision Language Models (VLMs), have demonstratedremarkable capabilities in visual and linguistic reasoning for VIL tasks.Despite this progress, existing approaches primarily utilize these models forlearning high-level plans from human demonstrations, relying on pre-definedmotion primitives for executing physical interactions, which remains a majorbottleneck for robotic systems. In this work, we present FMimic, a novelparadigm that harnesses foundation models to directly learn generalizableskills at even fine-grained action levels, using only a limited number of humanvideos. Extensive experiments demonstrate that our FMimic delivers strongperformance with a single human video, and significantly outperforms all othermethods with five videos. Furthermore, our method exhibits significantimprovements of over 39% and 29% in RLBench multi-task experiments andreal-world manipulation tasks, respectively, and exceeds baselines by more than34% in high-precision tasks and 47% in long-horizon tasks.</description>
      <author>example@mail.com (Guangyan Chen, Meiling Wang, Te Cui, Yao Mu, Haoyang Lu, Zicai Peng, Mengxiao Hu, Tianxing Zhou, Mengyin Fu, Yi Yang, Yufeng Yue)</author>
      <guid isPermaLink="false">2507.20622v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning</title>
      <link>http://arxiv.org/abs/2507.20564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ZSE-Cap系统，一种零样本集成方法，在EVENTA共享任务中获得了第四名，无需对竞赛数据进行微调。&lt;h4&gt;背景&lt;/h4&gt;EVENTA共享任务涉及基于文章的图像检索和描述生成，这是一个需要将文本内容与视觉元素相关联的挑战性任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种零样本方法，能够在不针对特定竞赛数据进行微调的情况下，有效完成图像检索和描述生成任务。&lt;h4&gt;方法&lt;/h4&gt;对于图像检索，集成CLIP、SigLIP和DINOv2模型的相似度分数；对于图像描述，使用精心设计的提示引导Gemma 3模型，将文章中的高级事件与图像视觉内容关联。&lt;h4&gt;主要发现&lt;/h4&gt;ZSE-Cap系统在私有测试集上获得了0.42002的最终分数，排名前四，证明了通过集成和提示技术结合基础模型的有效性。&lt;h4&gt;结论&lt;/h4&gt;零样本方法结合基础模型通过集成和提示技术可以有效地解决图像检索和描述生成任务，无需针对特定数据集进行微调。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了ZSE-Cap（Zero-Shot Ensemble for Captioning），我们在EVENTA（Event-Enriched Image Analysis）共享任务中获得了第四名，该任务涉及基于文章的图像检索和描述生成。我们的零样本方法不需要对竞赛数据进行微调。对于检索，我们集成了CLIP、SigLIP和DINOv2的相似度分数。对于描述生成，我们利用精心设计的提示来指导Gemma 3模型，使其能够将文章中的高级事件与图像中的视觉内容联系起来。我们的系统在私有测试集上获得了0.42002的最终分数，排名前四，证明了通过集成和提示技术结合基础模型的有效性。我们的代码可在https://github.com/ductai05/ZSE-Cap获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place systemin Event-Enriched Image Analysis (EVENTA) shared task on article-grounded imageretrieval and captioning. Our zero-shot approach requires no finetuning on thecompetition's data. For retrieval, we ensemble similarity scores from CLIP,SigLIP, and DINOv2. For captioning, we leverage a carefully engineered promptto guide the Gemma 3 model, enabling it to link high-level events from thearticle to the visual content in the image. Our system achieved a final scoreof 0.42002, securing a top-4 position on the private test set, demonstratingthe effectiveness of combining foundation models through ensembling andprompting. Our code is available at https://github.com/ductai05/ZSE-Cap.</description>
      <author>example@mail.com (Duc-Tai Dinh, Duc Anh Khoa Dinh)</author>
      <guid isPermaLink="false">2507.20564v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>EchoForce: Continuous Grip Force Estimation from Skin Deformation Using Active Acoustic Sensing on a Wristband</title>
      <link>http://arxiv.org/abs/2507.20437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures. Proceedings of the 2025 ACM International  Symposium on Wearable Computers (ISWC '25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EchoForce是一种新型腕带，使用声学传感技术进行低成本、非接触的握力测量，解决了现有方法笨重和用户依赖性的问题。&lt;h4&gt;背景&lt;/h4&gt;握力常被用作老年人的整体健康指标，并可用于跟踪体能训练和康复进展。现有的可穿戴握力测量方法笨重且依赖于用户操作，不足以实现实际的、连续的握力测量。&lt;h4&gt;目的&lt;/h4&gt;开发一种低成本、非接触的握力测量方法，使其能够实际应用于连续握力测量。&lt;h4&gt;方法&lt;/h4&gt;EchoForce是一种新型腕带，通过捕获从前臂屈肌细微皮肤变形反射的声学信号来测量握力。在11名参与者的用户研究中，使用基础模型实现了用户依赖的平均错误率为9.08%，用户独立的平均错误率为12.3%。&lt;h4&gt;主要发现&lt;/h4&gt;EchoForce在不同会话、手部方向和用户之间保持准确，克服了过去力传感系统的一个重要局限性。&lt;h4&gt;结论&lt;/h4&gt;EchoForce使连续握力测量变得实用，为健康监测和新型交互技术提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;握力常被用作老年人的整体健康指标，并可用于跟踪体能训练和康复进展。现有的可穿戴握力测量方法笨重且依赖于用户操作，不足以实现实际的、连续的握力测量。我们介绍了EchoForce，这是一种新型腕带，使用声学传感技术进行低成本、非接触的握力测量。EchoForce通过捕获从前臂屈肌细微皮肤变形反射的声学信号来工作。在11名参与者的用户研究中，EchoForce使用基础模型实现了经过微调的用户依赖平均错误率9.08%和用户独立平均错误率12.3%。我们的系统在不同会话、手部方向和用户之间保持准确，克服了过去力传感系统的一个重要局限性。EchoForce使连续握力测量变得实用，为健康监测和新型交互技术提供了有效工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3715071.3750405&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grip force is commonly used as an overall health indicator in older adultsand is valuable for tracking progress in physical training and rehabilitation.Existing methods for wearable grip force measurement are cumbersome anduser-dependent, making them insufficient for practical, continuous grip forcemeasurement. We introduce EchoForce, a novel wristband using acoustic sensingfor low-cost, non-contact measurement of grip force. EchoForce capturesacoustic signals reflected from subtle skin deformations by flexor muscles onthe forearm. In a user study with 11 participants, EchoForce achieved afine-tuned user-dependent mean error rate of 9.08% and a user-independent meanerror rate of 12.3% using a foundation model. Our system remained accuratebetween sessions, hand orientations, and users, overcoming a significantlimitation of past force sensing systems. EchoForce makes continuous grip forcemeasurement practical, providing an effective tool for health monitoring andnovel interaction techniques.</description>
      <author>example@mail.com (Kian Mahmoodi, Yudong Xie, Tan Gemicioglu, Chi-Jung Lee, Jiwan Kim, Cheng Zhang)</author>
      <guid isPermaLink="false">2507.20437v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Can Foundation Models Predict Fitness for Duty?</title>
      <link>http://arxiv.org/abs/2507.20418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作探讨了如何利用深度学习和基础模型来预测人的警觉性，特别是基于近红外虹膜图像，通过利用自监督模型的泛化能力解决数据集构建挑战。&lt;h4&gt;背景&lt;/h4&gt;生物特征采集设备已被用于通过近红外虹膜图像估计人的警觉性，扩展了其使用范围。然而，创建与酒精摄入、药物使用和睡眠不足相关的图像数据集以训练AI模型存在显著挑战，深度学习方法通常需要大量图像。&lt;h4&gt;目的&lt;/h4&gt;研究深度学习和基础模型在预测'适合工作状态'方面的应用，'适合工作状态'被定义为与确定工作警觉性相关的主体条件。&lt;h4&gt;方法&lt;/h4&gt;利用基于基础模型的大规模图像训练下游模型，利用自监督模型的泛化能力来增强这一领域。&lt;h4&gt;主要发现&lt;/h4&gt;基于基础模型的大规模图像训练为增强这一领域提供了真实机会，自监督模型的泛化能力有助于解决数据集构建的挑战。&lt;h4&gt;结论&lt;/h4&gt;深度学习和基础模型在预测适合工作状态方面具有应用前景，能够有效评估人的警觉性。&lt;h4&gt;翻译&lt;/h4&gt;生物特征采集设备已被用于通过近红外虹膜图像估计人的警觉性，扩展了其使用范围超出了单纯的生物识别。然而，为训练AI模型创建与酒精摄入、药物使用和睡眠不足相关的对应图像数据集构成了重大挑战。通常，有效实施深度学习方法需要大量图像。目前，利用基于基础模型的大规模图像训练下游模型，得益于自监督模型的泛化能力，为增强这一领域提供了真实机会。这项工作研究了深度学习和基础模型在预测适合工作状态方面的应用，适合工作状态被定义为与确定工作警觉性相关的主体条件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biometric capture devices have been utilised to estimate a person's alertnessthrough near-infrared iris images, expanding their use beyond just biometricrecognition. However, capturing a substantial number of corresponding imagesrelated to alcohol consumption, drug use, and sleep deprivation to create adataset for training an AI model presents a significant challenge. Typically, alarge quantity of images is required to effectively implement a deep learningapproach. Currently, training downstream models with a huge number of imagesbased on foundational models provides a real opportunity to enhance this area,thanks to the generalisation capabilities of self-supervised models. This workexamines the application of deep learning and foundational models in predictingfitness for duty, which is defined as the subject condition related todetermining the alertness for work.</description>
      <author>example@mail.com (Juan E. Tapia, Christoph Busch)</author>
      <guid isPermaLink="false">2507.20418v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>MIRepNet: A Pipeline and Foundation Model for EEG-Based Motor Imagery Classification</title>
      <link>http://arxiv.org/abs/2507.20254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MIRepNet的EEG基础模型，专门针对运动想象(MI)范式设计，解决了现有基础模型忽视范式特定神经生理学差异的问题，并在多个公共MI数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;脑机接口(BCI)实现大脑与外部设备之间的直接通信。最近的EEG基础模型旨在学习跨多样本BCI范式的通用表示，但这些方法忽视了基本的范式特定的神经生理学差异，限制了它们的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发第一个专门为运动想象(MI)范式定制的EEG基础模型MIRepNet，解决现有基础模型在特定BCI应用中的泛化能力限制问题。&lt;h4&gt;方法&lt;/h4&gt;MIRepNet包含一个高质量的EEG预处理流程，融入神经生理学信息通道模板，可适应具有任意电极配置的EEG头戴设备。引入混合预训练策略，结合自监督的掩码令牌重建和监督的MI分类，实现对新下游MI任务的快速适应和准确解码(每类少于30次试验)。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共MI数据集上的广泛评估表明，MIRepNet始终达到了最先进的性能，显著优于专门的和通用的EEG模型。&lt;h4&gt;结论&lt;/h4&gt;MIRepNet代表了针对特定BCI范式(如运动想象)定制EEG基础模型的重要进展，解决了现有方法中忽视的神经生理学差异问题，为实际BCI应用提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口(BCI)实现大脑与外部设备之间的直接通信。最近的EEG基础模型旨在学习跨多样本BCI范式的通用表示。然而，这些方法忽视了基本的范式特定的神经生理学差异，限制了它们的泛化能力。重要的是，在实际BCI部署中，特定范式(如用于中风康复或辅助机器人的运动想象(MI))通常在数据采集前就已确定。本文提出了MIRepNet，这是第一个为MI范式定制的EEG基础模型。MIRepNet包含一个高质量的EEG预处理流程，融入神经生理学信息通道模板，可适应具有任意电极配置的EEG头戴设备。此外，我们引入了一种混合预训练策略，结合了自监督的掩码令牌重建和监督的MI分类，促进了对新的下游MI任务的快速适应和准确解码，每类少于30次试验。在五个公共MI数据集上的广泛评估表明，MIRepNet始终达到了最先进的性能，显著优于专门的和通用的EEG模型。我们的代码将在GitHub上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-computer interfaces (BCIs) enable direct communication between thebrain and external devices. Recent EEG foundation models aim to learngeneralized representations across diverse BCI paradigms. However, theseapproaches overlook fundamental paradigm-specific neurophysiologicaldistinctions, limiting their generalization ability. Importantly, in practicalBCI deployments, the specific paradigm such as motor imagery (MI) for strokerehabilitation or assistive robotics, is generally determined prior to dataacquisition. This paper proposes MIRepNet, the first EEG foundation modeltailored for the MI paradigm. MIRepNet comprises a high-quality EEGpreprocessing pipeline incorporating a neurophysiologically-informed channeltemplate, adaptable to EEG headsets with arbitrary electrode configurations.Furthermore, we introduce a hybrid pretraining strategy that combinesself-supervised masked token reconstruction and supervised MI classification,facilitating rapid adaptation and accurate decoding on novel downstream MItasks with fewer than 30 trials per class. Extensive evaluations across fivepublic MI datasets demonstrated that MIRepNet consistently achievedstate-of-the-art performance, significantly outperforming both specialized andgeneralized EEG models. Our code will be available onGitHub\footnote{https://github.com/staraink/MIRepNet}.</description>
      <author>example@mail.com (Dingkun Liu, Zhu Chen, Jingwei Luo, Shijie Lian, Dongrui Wu)</author>
      <guid isPermaLink="false">2507.20254v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>SAMwave: Wavelet-Driven Feature Enrichment for Effective Adaptation of Segment Anything Model</title>
      <link>http://arxiv.org/abs/2507.20186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to BMVC 2025. The first two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMwave的新方法，通过小波变换和复值适配器改进Segment Anything Model在复杂任务上的性能，显著优于现有适应方法。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型的发展推动了各领域进步，图像分割模型SAM就是例证，但这类模型在应用于未训练的复杂任务时往往性能下降。现有方法采用基于适配器的微调策略和傅里叶域高频特征提取，但效果有限。&lt;h4&gt;目的&lt;/h4&gt;解决SAM在复杂任务上的性能下降问题，克服现有方法在特征提取技术上的限制，使SAM能更好地捕获对密集预测更相关的信息。&lt;h4&gt;方法&lt;/h4&gt;提出SAMwave方法，利用小波变换提取多尺度高频特征，引入复值适配器通过复小波变换捕获复值空间频率信息，自适应集成小波系数使SAM编码器捕获更相关信息。&lt;h4&gt;主要发现&lt;/h4&gt;在四个挑战性底层视觉任务上，SAMwave显著优于现有适应方法，这种性能在SAM和SAM2骨干网络上均一致，且适用于实值和复值适配器变体。&lt;h4&gt;结论&lt;/h4&gt;SAMwave通过小波变换和复值适配器有效解决了SAM在复杂任务上的性能下降问题，为适应分割模型提供了一种高效、灵活且可解释的方法。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型的出现推动了各个领域的重大进展。图像分割领域的领先模型SAM体现了这些进步，其性能超过了传统方法。然而，这类基础模型在应用于未训练的复杂任务时，往往会遭受性能下降。现有方法通常采用基于适配器的微调策略来使SAM适应任务，并利用从傅里叶域提取的高频特征。然而，我们的分析表明，由于这些方法在特征提取技术上的限制，它们提供的益处有限。为了克服这一问题，我们提出了SAMwave，这是一种新颖且可解释的方法，它利用小波变换从输入数据中提取更丰富的多尺度高频特征。在此基础上，我们引入了复值适配器，能够通过复小波变换捕获复值空间频率信息。通过自适应地集成这些小波系数，SAMwave使SAM的编码器能够捕获对密集预测更相关的信息。在四个具有挑战性的底层视觉任务上的经验评估表明，SAMwave显著优于现有的适应方法。这种优异性能在SAM和SAM2骨干网络上都一致存在，并且对于实值和复值适配器变体都成立，突显了我们提出方法在适应分割任何模型方面的效率、灵活性和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of large foundation models has propelled significant advancesin various domains. The Segment Anything Model (SAM), a leading model for imagesegmentation, exemplifies these advances, outperforming traditional methods.However, such foundation models often suffer from performance degradation whenapplied to complex tasks for which they are not trained. Existing methodstypically employ adapter-based fine-tuning strategies to adapt SAM for tasksand leverage high-frequency features extracted from the Fourier domain.However, Our analysis reveals that these approaches offer limited benefits dueto constraints in their feature extraction techniques. To overcome this, wepropose \textbf{\textit{SAMwave}}, a novel and interpretable approach thatutilizes the wavelet transform to extract richer, multi-scale high-frequencyfeatures from input data. Extending this, we introduce complex-valued adapterscapable of capturing complex-valued spatial-frequency information via complexwavelet transforms. By adaptively integrating these wavelet coefficients,SAMwave enables SAM's encoder to capture information more relevant for denseprediction. Empirical evaluations on four challenging low-level vision tasksdemonstrate that SAMwave significantly outperforms existing adaptation methods.This superior performance is consistent across both the SAM and SAM2 backbonesand holds for both real and complex-valued adapter variants, highlighting theefficiency, flexibility, and interpretability of our proposed method foradapting segment anything models.</description>
      <author>example@mail.com (Saurabh Yadav, Avi Gupta, Koteswar Rao Jerripothula)</author>
      <guid isPermaLink="false">2507.20186v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Iterative Pretraining Framework for Interatomic Potentials</title>
      <link>http://arxiv.org/abs/2507.20118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为迭代预训练用于原子间势(IPIP)的框架，用于改进机器学习原子间势(MLIPs)模型的预测性能。IPIP通过包含遗忘机制防止迭代训练收敛到次优局部最小值，使用轻量级架构实现了比通用基础模型更高的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;机器学习原子间势(MLIPs)能够实现具有从头算精度的高效分子动力学模拟，已应用于物理科学多个领域。然而，其性能通常依赖于大规模标记训练数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有预训练策略中预训练目标与下游任务不匹配、依赖大量标记数据集以及需要复杂架构实现广泛泛化的问题。&lt;h4&gt;方法&lt;/h4&gt;提出迭代预训练用于原子间势(IPIP)框架，该框架包含遗忘机制防止迭代训练收敛到次优局部最小值，并使用轻量级架构提高准确性和效率。&lt;h4&gt;主要发现&lt;/h4&gt;与通用力场相比，IPIP在Mo-S-O系统中实现了超过80%的预测误差减少和高达4倍的速度提升，能够实现快速准确的模拟。&lt;h4&gt;结论&lt;/h4&gt;IPIP解决了现有MLIPs面临的挑战，通过轻量级架构实现了更高的准确性和效率，使快速准确的分子动力学模拟成为可能。&lt;h4&gt;翻译&lt;/h4&gt;机器学习原子间势(MLIPs)能够实现具有从头算精度的高效分子动力学(MD)模拟，并已应用于物理科学各个领域。然而，它们的性能往往依赖于大规模标记训练数据。虽然现有的预训练策略可以提高模型性能，但它们通常存在预训练目标和下游任务之间的不匹配问题，或依赖大量标记数据集和越来越复杂的架构来实现广泛泛化。为解决这些挑战，我们提出了迭代预训练用于原子间势(IPIP)，这是一个旨在迭代改进MLIP模型预测性能的框架。IPIP包含一个遗忘机制，防止迭代训练收敛到次优局部最小值。与通用基础模型不同，通用基础模型通常在专业任务上表现不佳，因为通用性和系统特定准确性之间存在权衡，而IPIP使用轻量级架构实现了更高的准确性和效率。与通用力场相比，这种方法在具有挑战性的Mo-S-O系统中实现了超过80%的预测误差减少和高达4倍的速度提升，使快速准确的模拟成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning interatomic potentials (MLIPs) enable efficient moleculardynamics (MD) simulations with ab initio accuracy and have been applied acrossvarious domains in physical science. However, their performance often relies onlarge-scale labeled training data. While existing pretraining strategies canimprove model performance, they often suffer from a mismatch between theobjectives of pretraining and downstream tasks or rely on extensive labeleddatasets and increasingly complex architectures to achieve broadgeneralization. To address these challenges, we propose Iterative Pretrainingfor Interatomic Potentials (IPIP), a framework designed to iteratively improvethe predictive performance of MLIP models. IPIP incorporates a forgettingmechanism to prevent iterative training from converging to suboptimal localminima. Unlike general-purpose foundation models, which frequently underperformon specialized tasks due to a trade-off between generality and system-specificaccuracy, IPIP achieves higher accuracy and efficiency using lightweightarchitectures. Compared to general-purpose force fields, this approach achievesover 80% reduction in prediction error and up to 4x speedup in the challengingMo-S-O system, enabling fast and accurate simulations.</description>
      <author>example@mail.com (Taoyong Cui, Zhongyao Wang, Dongzhan Zhou, Yuqiang Li, Lei Bai, Wanli Ouyang, Mao Su, Shufei Zhang)</author>
      <guid isPermaLink="false">2507.20118v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>VAMPIRE: Uncovering Vessel Directional and Morphological Information from OCTA Images for Cardiovascular Disease Risk Factor Prediction</title>
      <link>http://arxiv.org/abs/2507.20017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的CVD风险评估方法，利用OCTA血管成像技术捕捉更详细的血管特征，同时预测CVD风险和相关血液因子条件，提高预测准确性和临床实用性。&lt;h4&gt;背景&lt;/h4&gt;心血管疾病是全球死亡的主要原因，需要开发有效的风险评估方法。当前使用的眼底照相和OCT技术相比OCTA无法捕捉对CVD评估至关重要的血管特征细节，且现有方法仅将CVD风险分类为高或低，缺乏对相关血液因子条件的深入分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多用途CVD风险评估范式，同时执行CVD风险和CVD相关条件预测；创建首个用于CVD风险评估的OCTA数据集；开发基于OCTA enface图像的VAMPIRE预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出VAMPIRE模型，包含两个关键组件：(1)基于Mamba的方向性(MBD)模块，捕捉细粒度血管轨迹特征；(2)信息增强形态学(IEM)模块，整合全面的血管形态学知识。同时创建了OCTA-CVD数据集。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法超越了标准分类骨干网络、基于OCTA的检测方法和眼科基础模型。相关代码和数据集已在GitHub平台公开。&lt;h4&gt;结论&lt;/h4&gt;通过利用OCTA图像提供更详细的血管特征分析，并结合临床经验进行多维度预测，该研究显著提高了CVD风险评估的准确性和临床应用价值。&lt;h4&gt;翻译&lt;/h4&gt;心血管疾病(CVD)仍然是全球死亡的主要原因，需要及时开发有效的风险评估方法进行干预。虽然当前研究已引入使用深度学习模型从视网膜图像预测CVD风险的无创高效方法，但常用的眼底照片和光学相干断层扫描(OCT)相比OCT血管成像(OCTA)图像无法捕捉对CVD评估至关重要的血管特征细节。此外，现有方法通常仅将CVD风险分类为高或低，而不提供对CVD相关血液因子条件的深入分析，从而限制了预测准确性和临床实用性。因此，我们提出了一种新的多用途CVD风险评估范式，同时执行CVD风险和CVD相关条件预测，符合临床经验。基于这一核心理念，我们引入了OCTA-CVD，这是首个用于CVD风险评估的OCTA数据集，以及一个基于OCTA enface图像的Vessel-Aware Mamba-based Prediction model with Informative Enhancement(VAMPIRE)。我们提出的模型旨在通过两个关键组件提取关键血管特征：(1)捕捉细粒度血管轨迹特征的基于Mamba的方向性(MBD)模块，和(2)整合全面血管形态学知识的信息增强形态学(IEM)模块。实验结果表明，我们的方法可以超越标准分类骨干网络、基于OCTA的检测方法和眼科基础模型。我们的代码和收集的OCTA-CVD数据集可在https://github.com/xmed-lab/VAMPIRE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiovascular disease (CVD) remains the leading cause of death worldwide,requiring urgent development of effective risk assessment methods for timelyintervention. While current research has introduced non-invasive and efficientapproaches to predict CVD risk from retinal imaging with deep learning models,the commonly used fundus photographs and Optical Coherence Tomography (OCT)fail to capture detailed vascular features critical for CVD assessment comparedwith OCT angiography (OCTA) images. Moreover, existing methods typicallyclassify CVD risk only as high or low, without providing a deeper analysis onCVD-related blood factor conditions, thus limiting prediction accuracy andclinical utility. As a result, we propose a novel multi-purpose paradigm of CVDrisk assessment that jointly performs CVD risk and CVD-related conditionprediction, aligning with clinical experiences. Based on this core idea, weintroduce OCTA-CVD, the first OCTA dataset for CVD risk assessment, and aVessel-Aware Mamba-based Prediction model with Informative Enhancement(VAMPIRE) based on OCTA enface images. Our proposed model aims to extractcrucial vascular characteristics through two key components: (1) a Mamba-BasedDirectional (MBD) Module that captures fine-grained vascular trajectoryfeatures and (2) an Information-Enhanced Morphological (IEM) Module thatincorporates comprehensive vessel morphology knowledge. Experimental resultsdemonstrate that our method can surpass standard classification backbones,OCTA-based detection methods, and ophthalmologic foundation models. Our codesand the collected OCTA-CVD dataset are available athttps://github.com/xmed-lab/VAMPIRE.</description>
      <author>example@mail.com (Lehan Wang, Hualiang Wang, Chubin Ou, Lushi Chen, Yunyi Liang, Xiaomeng Li)</author>
      <guid isPermaLink="false">2507.20017v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>A Model-Independent Theory of Probabilistic Testing</title>
      <link>http://arxiv.org/abs/2507.19886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种通用的、不依赖于特定模型的概率测试方法，研究了概率并发系统的测试等价性，并将其与概率相似性进行了比较。&lt;h4&gt;背景&lt;/h4&gt;概率并发系统是现代移动计算的基础模型。对于这些系统的测试理论和方法研究具有重要的理论和实践意义。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的、不依赖于特定模型的概率测试方法，研究测试等价性的模型无关表征和外部表征。&lt;h4&gt;方法&lt;/h4&gt;通过引入一种新的基于分布的概率模型语义，以及一种基于进程谓词的概率测试框架，来研究测试等价性的表征。&lt;h4&gt;主要发现&lt;/h4&gt;提出的等价性可以看作是经典公平/应该等价性和可能等价性的推广；这些等价性被证明是协调的；对这些等价性与概率相似性进行了全面的比较；所提出的技术可以容易地扩展到其他概率并发模型。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一种通用的概率测试方法，为概率并发系统的测试理论提供了新的视角，并且所提出的技术具有较好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;概率并发系统是现代移动计算的基础模型。本文提出了一种通用的不依赖于特定模型的概率测试方法。借助一种新的基于分布的概率模型语义和一种基于进程谓词的概率测试框架，研究了测试等价性的模型无关表征和外部表征。后一种表征可以看作是经典公平/应该等价性和可能等价性的推广。这些等价性被证明是协调的。对这些等价性与概率相似性进行了全面的比较。本文介绍的技术可以容易地扩展到其他概率并发模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic concurrent systems are foundational models for modern mobilecomputing. In this paper, a general model-independent approach to probabilistictesting is proposed. With the help of a new distribution-based semantics forprobabilistic models and a probabilistic testing framework with respect toprocess predicates, the model-independent characterization and the externalcharacterization for testing equivalences are studied. The lattercharacterization can be viewed as the generalization of the classicalfair/should equivalence and may equivalence. These equivalences are shown to becongruent. A thorough comparison between these equivalences and probabilisticbisimilarities is carried out. The techniques introduced in this paper can beeasily extended to other probabilistic concurrent models.</description>
      <author>example@mail.com (Weijun Chen, Yuxi Fu, Huan Long, Hao Wu)</author>
      <guid isPermaLink="false">2507.19886v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>EA-ViT: Efficient Adaptation for Elastic Vision Transformer</title>
      <link>http://arxiv.org/abs/2507.19360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的ViT适应框架(EA-ViT)，通过单个适应过程生成不同大小的多个模型，解决了传统方法需要重新训练多个特定大小ViTs的问题，实现了更高效、更节能的模型部署。&lt;h4&gt;背景&lt;/h4&gt;视觉变换器(ViTs)已成为计算机视觉的基础模型，在泛化和适应下游任务方面表现出色。然而，将ViTs部署到支持不同资源约束环境时，通常需要重新训练多个特定大小的ViTs，这既耗时又耗能。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的ViT适应框架，使单个适应过程能够生成不同大小的多个模型，以便部署在各种资源约束平台上。&lt;h4&gt;方法&lt;/h4&gt;方法包含两个阶段：第一阶段通过嵌套弹性架构增强预训练的ViT，在MLP扩展比、注意力头数量、嵌入维度和网络深度方面提供结构灵活性，并采用基于课程的训练策略逐步增加弹性；第二阶段设计一个轻量级路由器，根据计算预算和下游任务需求选择子模型，使用帕累托最优配置初始化后与骨干网络联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的广泛实验证明了EA-ViT框架的有效性和多功能性。&lt;h4&gt;结论&lt;/h4&gt;EA-ViT框架能够通过单个适应过程生成不同大小的多个模型，解决了传统方法需要重新训练多个特定大小ViTs的问题，实现了更高效、更节能的模型部署。&lt;h4&gt;翻译&lt;/h4&gt;视觉变换器(ViTs)已成为计算机视觉的基础模型，在泛化和适应下游任务方面表现出色。然而，将ViTs部署到支持不同资源约束环境时，通常需要重新训练多个特定大小的ViTs，这既耗时又耗能。为了解决这个问题，我们提出了一种高效的ViT适应框架，使单个适应过程能够生成不同大小的多个模型，以便部署在各种资源约束平台上。我们的方法包含两个阶段。在第一阶段，我们通过嵌套弹性架构增强预训练的ViT，该架构在MLP扩展比、注意力头数量、嵌入维度和网络深度方面提供结构灵活性。为了保留预训练知识和确保稳定的适应，我们采用基于课程的训练策略，逐步增加弹性。在第二阶段，我们设计了一个轻量级路由器，根据计算预算和下游任务需求选择子模型。路由器使用通过定制NSGA-II算法导出的帕累托最优配置进行初始化，然后与骨干网络联合优化。在多个基准测试上的广泛实验证明了EA-ViT的有效性和多功能性。代码可在https://github.com/zcxcf/EA-ViT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Transformers (ViTs) have emerged as a foundational model in computervision, excelling in generalization and adaptation to downstream tasks.However, deploying ViTs to support diverse resource constraints typicallyrequires retraining multiple, size-specific ViTs, which is both time-consumingand energy-intensive. To address this issue, we propose an efficient ViTadaptation framework that enables a single adaptation process to generatemultiple models of varying sizes for deployment on platforms with variousresource constraints. Our approach comprises two stages. In the first stage, weenhance a pre-trained ViT with a nested elastic architecture that enablesstructural flexibility across MLP expansion ratio, number of attention heads,embedding dimension, and network depth. To preserve pre-trained knowledge andensure stable adaptation, we adopt a curriculum-based training strategy thatprogressively increases elasticity. In the second stage, we design alightweight router to select submodels according to computational budgets anddownstream task demands. Initialized with Pareto-optimal configurations derivedvia a customized NSGA-II algorithm, the router is then jointly optimized withthe backbone. Extensive experiments on multiple benchmarks demonstrate theeffectiveness and versatility of EA-ViT. The code is available athttps://github.com/zcxcf/EA-ViT.</description>
      <author>example@mail.com (Chen Zhu, Wangbo Zhao, Huiwen Zhang, Samir Khaki, Yuhao Zhou, Weidong Tang, Shuo Wang, Zhihang Yuan, Yuzhang Shang, Xiaojiang Peng, Kai Wang, Dawei Yang)</author>
      <guid isPermaLink="false">2507.19360v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>The Eloquence team submission for task 1 of MLC-SLM challenge</title>
      <link>http://arxiv.org/abs/2507.19308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report for MLC-SLM Challenge of Interspeech2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了为多语言对话语音语言模型挑战和研讨会任务1所进行的研究和实验，专注于通过开发语音语言模型架构推进多语言对话语音识别&lt;h4&gt;背景&lt;/h4&gt;现实世界对话数据对于构建鲁棒的对话系统日益重要&lt;h4&gt;目的&lt;/h4&gt;探索三种多语言自动语音识别方法&lt;h4&gt;方法&lt;/h4&gt;1) 评估基线模型，训练两个不同基础模型的投影器(线性和qformer)；2) 利用SLAM-ASR框架训练自定义多语言线性投影器；3) 研究对比学习和扩展对话上下文在增强识别鲁棒性方面的作用&lt;h4&gt;主要发现&lt;/h4&gt;不同投影器与基础模型的组合表现各异；SLAM-ASR框架可有效应用于多语言场景；对比学习和扩展对话上下文能提高识别的鲁棒性&lt;h4&gt;结论&lt;/h4&gt;多语言对话语音识别可通过创新的模型架构和训练方法得到显著改进&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了为多语言对话语音语言模型挑战和研讨会任务1所进行的研究和实验，专注于通过开发语音语言模型架构推进多语言对话语音识别。鉴于现实世界对话数据对于构建鲁棒的对话系统日益重要，我们探索了三种多语言自动语音识别方法。首先，我们通过训练两个与不同基础模型相连的投影器(线性和qformer)来评估基线模型，以更好地了解其优势和局限性。其次，我们利用SLAM-ASR框架训练自定义多语言线性投影器。最后，我们研究了对比学习和扩展对话上下文在增强识别鲁棒性方面的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present our studies and experiments carried out for thetask 1 of the Challenge and Workshop on Multilingual Conversational SpeechLanguage Model (MLC-SLM), which focuses on advancing multilingualconversational speech recognition through the development of speech languagemodels architectures. Given the increasing relevance of real-worldconversational data for building robust Spoken Dialogue Systems, we explorethree approaches to multilingual ASR. First, we conduct an evaluation of theofficial baseline to better understand its strengths and limitations, bytraining two projectors (linear and qformer) with different foundation models.Second we leverage the SLAM-ASR framework to train a custom multilingual linearprojector. Finally we investigate the role of contrastive learning and theextended conversational context in enhancing the robustness of recognition.</description>
      <author>example@mail.com (Lorenzo Concina, Jordi Luque, Alessio Brutti, Marco Matassoni, Yuchen Zhang)</author>
      <guid isPermaLink="false">2507.19308v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation</title>
      <link>http://arxiv.org/abs/2507.19242v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种利用扩散模型定位未知物体重心的抓取方法，专门针对质量分布不均匀的物体。研究构建了790张图像的数据集，开发了基于基础模型的视觉驱动框架，实验显示该方法比传统关键点方法成功率高49%，比最先进的驱动方法高11%，在未见物体上达到76%的重心定位准确率。&lt;h4&gt;背景&lt;/h4&gt;在机器人抓取中，重心偏差通常导致姿势不稳定，而现有的基于关键点或驱动的方法存在局限性，特别是在处理质量分布不均匀的物体时。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确定位未知物体重心的抓取方法，以提高对质量分布不均匀物体的抓取稳定性和成功率。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含790张质量分布不均匀物体的图像数据集，带有用于重心定位的关键点注释。开发了一个基于基础模型的视觉驱动框架，利用扩散模型来定位物体的重心，实现重心感知抓取。&lt;h4&gt;主要发现&lt;/h4&gt;相比传统关键点方法成功率提高49%，比最先进的驱动方法提高11%，在未见物体上达到76%的重心定位准确率，系统展现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种精确且稳定的抓取任务的新解决方案，特别适用于质量分布不均匀的物体，通过准确识别重心显著提高了抓取成功率。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种针对质量分布不均匀物体的抓取方法，通过利用扩散模型来定位未知物体的重心。在机器人抓取中，重心偏差通常导致姿势不稳定，而现有的基于关键点或驱动的方法存在局限性。我们构建了一个包含790张图像的数据集，这些图像具有质量分布不均匀的物体并带有用于重心定位的关键点注释。开发了一个基于基础模型的视觉驱动框架，以实现重心感知抓取。在现实场景中的实验评估表明，我们的方法相比传统关键点方法成功率提高49%，比最先进的驱动方法提高11%。该系统在未见物体上展现出76%的重心定位准确率，为精确且稳定的抓取任务提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a grasping method for objects with uneven massdistribution by leveraging diffusion models to localize the center of gravity(CoG) on unknown objects. In robotic grasping, CoG deviation often leads topostural instability, where existing keypoint-based or affordance-drivenmethods exhibit limitations. We constructed a dataset of 790 images featuringunevenly distributed objects with keypoint annotations for CoG localization. Avision-driven framework based on foundation models was developed to achieveCoG-aware grasping. Experimental evaluations across real-world scenariosdemonstrate that our method achieves a 49\% higher success rate compared toconventional keypoint-based approaches and an 11\% improvement overstate-of-the-art affordance-driven methods. The system exhibits stronggeneralization with a 76\% CoG localization accuracy on unseen objects,providing a novel solution for precise and stable grasping tasks.</description>
      <author>example@mail.com (Kang Xiangli, Yage He, Xianwu Gong, Zehan Liu, Yuru Bai)</author>
      <guid isPermaLink="false">2507.19242v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>TrinityDNA: A Bio-Inspired Foundational Model for Efficient Long-Sequence DNA Modeling</title>
      <link>http://arxiv.org/abs/2507.19229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TrinityDNA，一种新型DNA基础模型，通过集成生物学信息组件、多尺度注意力和进化训练策略，解决了基因组序列建模中的长距离依赖性和生物学特征捕捉问题。&lt;h4&gt;背景&lt;/h4&gt;基因组序列建模因其长度和结构复杂性而面临独特挑战，传统序列模型难以捕捉DNA中的长距离依赖性和生物学特征。&lt;h4&gt;目的&lt;/h4&gt;开发更准确和高效的基因组序列建模方法，解决传统模型的局限性，提高基因功能预测和调控机制发现的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出TrinityDNA模型，集成Groove Fusion捕捉DNA结构特征，Gated Reverse Complement处理DNA序列对称性，多尺度注意力机制关注不同层次序列依赖性，以及进化训练策略适应原核生物和真核生物基因组；同时引入新的DNA长序列CDS注释基准。&lt;h4&gt;主要发现&lt;/h4&gt;TrinityDNA在基因功能预测、调控机制发现和其他基因组学应用方面提供显著改进，实现了更准确和高效的基因组序列建模。&lt;h4&gt;结论&lt;/h4&gt;TrinityDNA弥合了机器学习技术和生物学见解之间的差距，为更有效地分析基因组数据铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;基因组序列的建模由于其长度和结构复杂性而面临独特挑战。传统序列模型难以捕捉DNA中固有的长距离依赖性和生物学特征。在这项工作中，我们提出了TrinityDNA，一种旨在解决这些挑战的新型DNA基础模型。该模型集成了生物学信息组件，包括用于捕捉DNA结构特征的Groove Fusion和用于处理DNA序列固有对称性的门控反向互补。此外，我们引入了多尺度注意力机制，使模型能够关注不同层次的序列依赖性，以及一种逐步使模型适应原核生物和真核生物基因组的进化训练策略。TrinityDNA为基因组序列建模提供了更准确和高效的方法，在基因功能预测、调控机制发现和其他基因组学应用方面取得了显著改进。我们的模型弥合了机器学习技术和生物学见解之间的差距，为更有效地分析基因组数据铺平了道路。此外，我们引入了一个新的DNA长序列CDS注释基准，使评估更加全面并面向实际应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The modeling of genomic sequences presents unique challenges due to theirlength and structural complexity. Traditional sequence models struggle tocapture long-range dependencies and biological features inherent in DNA. Inthis work, we propose TrinityDNA, a novel DNA foundational model designed toaddress these challenges. The model integrates biologically informedcomponents, including Groove Fusion for capturing DNA's structural features andGated Reverse Complement (GRC) to handle the inherent symmetry of DNAsequences. Additionally, we introduce a multi-scale attention mechanism thatallows the model to attend to varying levels of sequence dependencies, and anevolutionary training strategy that progressively adapts the model to bothprokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate andefficient approach to genomic sequence modeling, offering significantimprovements in gene function prediction, regulatory mechanism discovery, andother genomics applications. Our model bridges the gap between machine learningtechniques and biological insights, paving the way for more effective analysisof genomic data. Additionally, we introduced a new DNA long-sequence CDSannotation benchmark to make evaluations more comprehensive and oriented towardpractical applications.</description>
      <author>example@mail.com (Qirong Yang, Yucheng Guo, Zicheng Liu, Yujie Yang, Qijin Yin, Siyuan Li, Shaomin Ji, Linlin Chao, Xiaoming Zhang, Stan Z. Li)</author>
      <guid isPermaLink="false">2507.19229v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Towards Multimodal Social Conversations with Robots: Using Vision-Language Models</title>
      <link>http://arxiv.org/abs/2507.19196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the workshop "Human - Foundation Models Interaction: A  Focus On Multimodal Information" (FoMo-HRI) at IEEE RO-MAN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型为社交机器人提供了开放域对话能力，但仍缺乏利用社交互动中多种模态的基本社会技能。作者提出了社交机器人进行多模态系统对话的整体需求，并论证视觉-语言模型能够以通用方式处理视觉信息，适用于自主社交机器人。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型使社交机器人能够进行开放域对话，但它们仍然缺乏利用承载社交互动的多种模态的基本社会技能。&lt;h4&gt;目的&lt;/h4&gt;概述社交机器人进行多模态系统对话的整体需求，论证视觉-语言模型能够以足够通用的方式处理广泛的视觉信息，用于自主社交机器人。&lt;h4&gt;方法&lt;/h4&gt;描述如何将视觉-语言模型适应到社交机器人的设置中。&lt;h4&gt;主要发现&lt;/h4&gt;视觉-语言模型能够以足够通用的方式处理广泛的视觉信息，适用于自主社交机器人。&lt;h4&gt;结论&lt;/h4&gt;作者讨论了适应视觉-语言模型到社交机器人设置中仍然存在的技术挑战以及评估实践。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型使社交机器人能够自主进行开放域对话。然而，它们仍然缺乏一项基本社会技能：利用承载社交互动的多种模态。虽然先前的工作主要集中在需要引用环境或社交互动中特定现象（如对话中断）的任务导向型互动，我们概述了社交机器人进行多模态系统对话的整体需求。然后我们论证视觉-语言模型能够以足够通用的方式处理广泛的视觉信息，用于自主社交机器人。我们描述了如何将这些模型适应到社交机器人的设置中，仍然存在哪些技术挑战，并简要讨论了评估实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models have given social robots the ability to autonomouslyengage in open-domain conversations. However, they are still missing afundamental social skill: making use of the multiple modalities that carrysocial interactions. While previous work has focused on task-orientedinteractions that require referencing the environment or specific phenomena insocial interactions such as dialogue breakdowns, we outline the overall needsof a multimodal system for social conversations with robots. We then argue thatvision-language models are able to process this wide range of visualinformation in a sufficiently general manner for autonomous social robots. Wedescribe how to adapt them to this setting, which technical challenges remain,and briefly discuss evaluation practices.</description>
      <author>example@mail.com (Ruben Janssens, Tony Belpaeme)</author>
      <guid isPermaLink="false">2507.19196v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>RegScore: Scoring Systems for Regression Tasks</title>
      <link>http://arxiv.org/abs/2507.19155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for the 28th International Conference on Medical Image  Computing and Computer Assisted Intervention (MICCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为RegScore的新型评分系统，专门用于回归任务。它是一种稀疏且可解释的评分系统，能够整合表格数据和医学图像，实现个性化评分，性能可与最先进的黑盒模型相媲美或更优。&lt;h4&gt;背景&lt;/h4&gt;评分系统在医学应用中广泛采用，因其固有的简单性和透明性，特别适用于涉及表格数据的分类任务。然而，传统的评分系统受限于整数值系数，可能限制了预测性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门用于回归任务的评分系统，克服传统评分系统的限制，提高预测性能，同时保持简单性和透明性，并将其扩展到双模态深度学习，整合表格数据和医学图像。&lt;h4&gt;方法&lt;/h4&gt;引入RegScore评分系统，使用束搜索和k-稀疏岭回归来放松传统评分系统对整数值系数的限制；将RegScore扩展到双模态深度学习，整合表格数据和医学图像；利用TIP转换器的分类令牌生成个性化线性回归参数和个性化RegScore；通过表格数据估计平均肺动脉压力，并结合心脏MRI图像进一步完善估计。&lt;h4&gt;主要发现&lt;/h4&gt;RegScore及其个性化双模态扩展实现了与最先进黑盒模型相当或更好的性能；该方法为临床环境中的回归任务提供了一种透明且可解释的方法；促进更明智和可信的决策制定。&lt;h4&gt;结论&lt;/h4&gt;RegScore是一种有效的、可解释的评分系统，适用于回归任务，特别是在医学应用中。它克服了传统评分系统的限制，并成功地整合了表格数据和医学图像，实现了个性化评分。&lt;h4&gt;翻译&lt;/h4&gt;评分系统因其固有的简单性和透明性在医学应用中被广泛采用，特别适用于涉及表格数据的分类任务。在这项工作中，我们引入了RegScore，一种新型的、稀疏的、可解释的评分系统，专门为回归任务设计。与传统限制为整数值系数的评分系统不同，RegScore利用束搜索和k-稀疏岭回归来放松这些限制，从而提高预测性能。我们将RegScore扩展到双模态深度学习，通过整合表格数据和医学图像。我们利用TIP转换器的分类令牌生成个性化线性回归参数和个性化RegScore，实现个性化评分。我们通过使用表格数据估计平均肺动脉压力来证明RegScore的有效性，并通过结合心脏MRI图像进一步完善这些估计。实验结果表明，RegScore及其个性化双模态扩展实现了与最先进黑盒模型相当或更好的性能。我们的方法为临床环境中的回归任务提供了一种透明且可解释的方法，促进更明智和可信的决策制定。我们提供代码在https://github.com/SanoScience/RegScore。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scoring systems are widely adopted in medical applications for their inherentsimplicity and transparency, particularly for classification tasks involvingtabular data. In this work, we introduce RegScore, a novel, sparse, andinterpretable scoring system specifically designed for regression tasks. Unlikeconventional scoring systems constrained to integer-valued coefficients,RegScore leverages beam search and k-sparse ridge regression to relax theserestrictions, thus enhancing predictive performance. We extend RegScore tobimodal deep learning by integrating tabular data with medical images. Weutilize the classification token from the TIP (Tabular Image Pretraining)transformer to generate Personalized Linear Regression parameters and aPersonalized RegScore, enabling individualized scoring. We demonstrate theeffectiveness of RegScore by estimating mean Pulmonary Artery Pressure usingtabular data and further refine these estimates by incorporating cardiac MRIimages. Experimental results show that RegScore and its personalized bimodalextensions achieve performance comparable to, or better than, state-of-the-artblack-box models. Our method provides a transparent and interpretable approachfor regression tasks in clinical settings, promoting more informed andtrustworthy decision-making. We provide our code athttps://github.com/SanoScience/RegScore.</description>
      <author>example@mail.com (Michal K. Grzeszczyk, Tomasz Szczepański, Pawel Renc, Siyeop Yoon, Jerome Charton, Tomasz Trzciński, Arkadiusz Sitek)</author>
      <guid isPermaLink="false">2507.19155v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution</title>
      <link>http://arxiv.org/abs/2507.19138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RealisVSR是一种高频细节增强的视频扩散模型，通过三个核心创新解决了VSR领域的三个关键挑战：时间动态建模不一致、高频细节恢复有限以及4K超分辨率评估不足。该模型在多个VSR基准上表现出色，特别是在超高清分辨率场景下。&lt;h4&gt;背景&lt;/h4&gt;视频超分辨率（VSR）通过扩散模型取得了显著进展，有效解决了基于GAN的方法中存在的过度平滑问题。然而，VSR领域仍存在三个关键挑战需要解决。&lt;h4&gt;目的&lt;/h4&gt;解决VSR领域中的三个关键挑战：1）基础模型中时间动态建模不一致；2）在复杂真实世界退化条件下高频细节恢复有限；3）细节增强和4K超分辨率评估不足。&lt;h4&gt;方法&lt;/h4&gt;提出RealisVSR，一种高频细节增强的视频扩散模型，包含三个核心创新：1）一致性保留ControlNet（CPC）架构，集成Wan2.1视频扩散；2）高频修正扩散损失（HR-Loss），结合小波分解和HOG特征约束；3）创建RealisVideo-4K，第一个包含1000个高清视频-文本对的公共4K VSR基准。&lt;h4&gt;主要发现&lt;/h4&gt;RealisVSR利用Wan2.1的先进时空引导，只需5-25%的训练数据量即可达到与现有方法相当的性能。在多个VSR基准（REDS, SPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P）上的实验证明了其在超高清分辨率场景下的优越性。&lt;h4&gt;结论&lt;/h4&gt;RealisVSR通过创新的架构和损失函数设计，有效解决了VSR领域的关键挑战，在保持高质量的同时减少了训练数据需求，特别是在4K超分辨率任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;视频超分辨率（VSR）通过扩散模型取得了显著进展，有效解决了基于GAN的方法中固有的过度平滑问题。尽管最近有所进展，VSR领域仍然存在三个关键挑战：1）基础模型中时间动态建模不一致；2）在复杂真实世界退化条件下高频细节恢复有限；3）对细节增强和4K超分辨率的评估不足，因为当前方法主要依赖于细节不足的720P数据集。为解决这些挑战，我们提出了RealisVSR，一种高频细节增强的视频扩散模型，包含三个核心创新：1）集成Wan2.1视频扩散的一致性保留ControlNet（CPC）架构，用于建模平滑复杂的运动并抑制伪影；2）结合小波分解和HOG特征约束的高频修正扩散损失（HR-Loss）用于纹理恢复；3）RealisVideo-4K，第一个包含1000个高清视频-文本对的公共4K VSR基准。利用Wan2.1的先进时空引导，我们的方法与现有方法相比只需要5-25%的训练数据量。在VSR基准（REDS, SPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P）上的大量实验证明了我们的优越性，特别是在超高清分辨率场景下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Super-Resolution (VSR) has achieved significant progress throughdiffusion models, effectively addressing the over-smoothing issues inherent inGAN-based methods. Despite recent advances, three critical challenges persistin VSR community: 1) Inconsistent modeling of temporal dynamics in foundationalmodels; 2) limited high-frequency detail recovery under complex real-worlddegradations; and 3) insufficient evaluation of detail enhancement and 4Ksuper-resolution, as current methods primarily rely on 720P datasets withinadequate details. To address these challenges, we propose RealisVSR, ahigh-frequency detail-enhanced video diffusion model with three coreinnovations: 1) Consistency Preserved ControlNet (CPC) architecture integratedwith the Wan2.1 video diffusion to model the smooth and complex motions andsuppress artifacts; 2) High-Frequency Rectified Diffusion Loss (HR-Loss)combining wavelet decomposition and HOG feature constraints for texturerestoration; 3) RealisVideo-4K, the first public 4K VSR benchmark containing1,000 high-definition video-text pairs. Leveraging the advanced spatio-temporalguidance of Wan2.1, our method requires only 5-25% of the training data volumecompared to existing approaches. Extensive experiments on VSR benchmarks (REDS,SPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P) demonstrate oursuperiority, particularly in ultra-high-resolution scenarios.</description>
      <author>example@mail.com (Weisong Zhao, Jingkai Zhou, Xiangyu Zhu, Weihua Chen, Xiao-Yu Zhang, Zhen Lei, Fan Wang)</author>
      <guid isPermaLink="false">2507.19138v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment</title>
      <link>http://arxiv.org/abs/2507.19004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  We note that the version after peer review of this paper has been  provisionally accepted by The 28th International Conference on Medical Image  Computing and Computer Assisted Intervention (MICCAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MedIQA是首个全面的医学图像质量评估基础模型，通过处理图像尺寸、模态、解剖区域和类型的变异性，显著提升了诊断准确性。&lt;h4&gt;背景&lt;/h4&gt;医学成像技术的快速发展对精确和自动化的图像质量评估提出了关键需求，以确保诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够跨不同模态和临床场景泛化的医学图像质量评估方法。&lt;h4&gt;方法&lt;/h4&gt;构建MedIQA模型，开发大规模多模态数据集，集成显著切片评估模块，采用自动提示策略将上游物理参数预训练与下游专家标注微调对齐。&lt;h4&gt;主要发现&lt;/h4&gt;MedIQA在多个下游任务中显著优于现有基线方法，建立了可扩展的医学IQA框架。&lt;h4&gt;结论&lt;/h4&gt;MedIQA促进了诊断流程和临床决策，为医学图像质量评估提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;医学成像技术的快速发展强调了精确和自动化图像质量评估的迫切需求，以确保诊断准确性。然而，现有的医学IQA方法难以在不同模态和临床场景中泛化。为此，我们推出了MedIQA，这是首个全面的医学IQA基础模型，旨在处理图像尺寸、模态、解剖区域和类型的变异性。我们开发了一个包含大量手动标注质量分数的大规模多模态数据集来支持这一工作。我们的模型集成了一个显著切片评估模块，专注于诊断相关区域特征检索，并采用自动提示策略，将上游物理参数预训练与下游专家标注微调对齐。大量实验表明，MedIQA在多个下游任务中显著优于基线模型，建立了可扩展的医学IQA框架，推动了诊断流程和临床决策的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid advances in medical imaging technology underscore the critical need forprecise and automated image quality assessment (IQA) to ensure diagnosticaccuracy. Existing medical IQA methods, however, struggle to generalize acrossdiverse modalities and clinical scenarios. In response, we introduce MedIQA,the first comprehensive foundation model for medical IQA, designed to handlevariability in image dimensions, modalities, anatomical regions, and types. Wedeveloped a large-scale multi-modality dataset with plentiful manuallyannotated quality scores to support this. Our model integrates a salient sliceassessment module to focus on diagnostically relevant regions feature retrievaland employs an automatic prompt strategy that aligns upstream physicalparameter pre-training with downstream expert annotation fine-tuning. Extensiveexperiments demonstrate that MedIQA significantly outperforms baselines inmultiple downstream tasks, establishing a scalable framework for medical IQAand advancing diagnostic workflows and clinical decision-making.</description>
      <author>example@mail.com (Siyi Xun, Yue Sun, Jingkun Chen, Zitong Yu, Tong Tong, Xiaohong Liu, Mingxiang Wu, Tao Tan)</author>
      <guid isPermaLink="false">2507.19004v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Test-time Offline Reinforcement Learning on Goal-related Experience</title>
      <link>http://arxiv.org/abs/2507.18809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种目标条件测试时训练（GC-TTT）算法，通过自监督数据选择标准从离线数据集中选择与当前状态和评估目标相关的转换，并在评估过程中递归地平方式应用微调，显著提高了高维度本地导航和操作任务中的策略性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型将大量信息压缩在单个大型神经网络中，这与离线目标条件强化学习算法有相似之处。基础模型的研究表明，通过测试时训练可以显著提高性能。&lt;h4&gt;目的&lt;/h4&gt;探索在离线强化学习中应用测试时训练的可能性，以提高策略性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的自监督数据选择标准，根据转换与当前状态的相关性以及对评估目标的质量从离线数据集中选择转换。在评估过程中以递归地平方式应用这种方法的算法称为目标条件测试时训练（GC-TTT）。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛的本地导航和操作任务中，在选定数据上对策略进行几步梯度微调可以比标准离线预训练带来显著的性能提升。在可比的计算成本下，GC-TTT带来的性能提升是无法通过扩大模型规模实现的。&lt;h4&gt;结论&lt;/h4&gt;测试时离线强化学习可以显著提高策略性能，而计算成本最小。&lt;h4&gt;翻译&lt;/h4&gt;基础模型将大量信息压缩在单个大型神经网络中，然后可以针对单个任务进行查询。这种广泛使用的框架与离线目标条件强化学习算法之间有很强的相似性：在大量目标上训练通用价值函数，并在每个测试剧集的单一目标上评估策略。基础模型的广泛研究表明，通过测试时训练可以将性能显著提高，使模型适应当前目标。我们发现，类似地，在与测试目标相关的经验上进行测试时离线强化学习，可以在最小的计算成本下导致显著更好的策略。我们提出了一种新的自监督数据选择标准，它根据转换与当前状态的相关性以及对评估目标的质量从离线数据集中选择转换。我们在广泛的本地导航和操作任务中证明，在选定的数据上对策略进行几步梯度微调可以比标准离线预训练带来显著的性能提升。我们的目标条件测试时训练（GC-TTT）算法在评估过程中以递归地平方式应用此例程，在策略展开时适应当前轨迹。最后，我们研究了推理时的计算分配，证明在可比的成本下，GC-TTT带来的性能提升是无法通过扩大模型规模实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models compress a large amount of information in a single, largeneural network, which can then be queried for individual tasks. There arestrong parallels between this widespread framework and offline goal-conditionedreinforcement learning algorithms: a universal value function is trained on alarge number of goals, and the policy is evaluated on a single goal in eachtest episode. Extensive research in foundation models has shown thatperformance can be substantially improved through test-time training,specializing the model to the current goal. We find similarly that test-timeoffline reinforcement learning on experience related to the test goal can leadto substantially better policies at minimal compute costs. We propose a novelself-supervised data selection criterion, which selects transitions from anoffline dataset according to their relevance to the current state and qualitywith respect to the evaluation goal. We demonstrate across a wide range ofhigh-dimensional loco-navigation and manipulation tasks that fine-tuning apolicy on the selected data for a few gradient steps leads to significantperformance gains over standard offline pre-training. Our goal-conditionedtest-time training (GC-TTT) algorithm applies this routine in areceding-horizon fashion during evaluation, adapting the policy to the currenttrajectory as it is being rolled out. Finally, we study compute allocation atinference, demonstrating that, at comparable costs, GC-TTT induces performancegains that are not achievable by scaling model size.</description>
      <author>example@mail.com (Marco Bagatella, Mert Albaba, Jonas Hübotter, Georg Martius, Andreas Krause)</author>
      <guid isPermaLink="false">2507.18809v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff</title>
      <link>http://arxiv.org/abs/2507.18587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures. Accepted to the IEEE International Symposium on  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer的基础模型用于大规模多输入多输出(mMIMO)系统的预编码，解决了深度学习模型训练中数据收集困难的问题，同时最小化发射器能耗并动态适应用户需求。&lt;h4&gt;背景&lt;/h4&gt;深度学习已成为mMIMO系统中预编码的解决方案，因其能学习传播环境特性，但训练此类模型需要在部署站点收集高质量本地数据集，这通常难以实现。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer的基础模型，用于mMIMO预编码，目标是最小化发射器能耗，同时动态适应每个用户的需求速率。&lt;h4&gt;方法&lt;/h4&gt;提出基于Transformer的基础模型架构；引入数据增强方法，通过计算预训练特征提取器输出间的余弦相似性，找到与目标分布相似的训练样本，解决数据稀少环境下的模型适应问题。&lt;h4&gt;主要发现&lt;/h4&gt;在相同能耗水平下，所提模型的零样本部署显著优于迫零方法，性能接近加权最小均方误差方法，但复杂度降低8倍；解决了数据可用性和训练复杂性问题，使基于深度学习的解决方案可在实际中实施。&lt;h4&gt;结论&lt;/h4&gt;能够动态配置每个用户需求速率的功能可被高级资源分配和调度算法利用，从而更好地控制能源效率、频谱效率和公平性。&lt;h4&gt;翻译&lt;/h4&gt;深度学习(DL)已成为大规模多输入多输出(mMIMO)系统中预编码的解决方案，因其能够学习传播环境的特性。然而，训练此类模型需要在部署站点收集高质量、本地化的数据集，这通常难以实现。我们提出了一种基于Transformer的基础模型用于mMIMO预编码，旨在最小化发射器的能耗，同时动态适应每个用户的需求速率。在相同能耗水平下，所提出的基础模型的零样本部署显著优于迫零方法，性能接近加权最小均方误差方法，但复杂度降低了8倍。为解决数据稀少环境下的模型适应问题，我们引入了一种数据增强方法，通过计算预训练特征提取器输出之间的余弦相似性，找到与目标分布相似的训练样本。我们的工作通过解决数据可用性和训练复杂性的挑战，使基于深度学习的解决方案在实际中得以实施。此外，动态配置每个用户需求速率的能力可被更高级的资源分配和调度算法利用，从而更好地控制能源效率、频谱效率和公平性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning (DL) has emerged as a solution for precoding in massivemultiple-input multiple-output (mMIMO) systems due to its capacity to learn thecharacteristics of the propagation environment. However, training such a modelrequires high-quality, local datasets at the deployment site, which are oftendifficult to collect. We propose a transformer-based foundation model for mMIMOprecoding that seeks to minimize the energy consumption of the transmitterwhile dynamically adapting to per-user rate requirements. At equal energyconsumption, zero-shot deployment of the proposed foundation modelsignificantly outperforms zero forcing, and approaches weighted minimum meansquared error performance with 8x less complexity. To address model adaptationin data-scarce settings, we introduce a data augmentation method that findstraining samples similar to the target distribution by computing the cosinesimilarity between the outputs of the pre-trained feature extractor. Our workenables the implementation of DL-based solutions in practice by addressingchallenges of data availability and training complexity. Moreover, the abilityto dynamically configure per-user rate requirements can be leveraged by higherlevel resource allocation and scheduling algorithms for greater control overenergy efficiency, spectral efficiency and fairness.</description>
      <author>example@mail.com (Jérôme Emery, Ali Hasanzadeh Karkan, Jean-François Frigon, François Leduc-Primeau)</author>
      <guid isPermaLink="false">2507.18587v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface</title>
      <link>http://arxiv.org/abs/2507.18546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GLiNER2是一个统一的框架，在单个高效模型中支持命名实体识别、文本分类和分层结构数据提取，具有竞争性性能且部署便捷。&lt;h4&gt;背景&lt;/h4&gt;信息提取是许多NLP应用的基础，但现有解决方案通常需要不同任务的专用模型或依赖计算昂贵的大型语言模型。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的框架GLiNER2，在单个高效模型中支持多种信息提取任务，提高部署可访问性。&lt;h4&gt;方法&lt;/h4&gt;GLiNER2增强了原始GLiNER架构，基于预训练的transformer编码器构建，保持CPU效率和紧凑大小，并通过基于模式的接口引入多任务组合。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明GLiNER2在提取和分类任务上具有竞争性性能，与基于LLM的替代方案相比，在部署可访问性方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;GLiNER2作为开源库发布，包含预训练模型和文档，可供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;信息提取(IE)是众多NLP应用的基础，然而现有解决方案通常需要不同任务的专用模型或依赖计算昂贵的大型语言模型。我们提出了GLiNER2，这是一个统一框架，增强了原始GLiNER架构，在单个高效模型中支持命名实体识别、文本分类和分层结构数据提取。GLiNER2基于预训练的transformer编码器架构构建，保持CPU效率和紧凑大小，同时通过直观的基于模式的接口引入多任务组合。我们的实验证明，GLiNER2在提取和分类任务上具有竞争性性能，与基于LLM的替代方案相比，在部署可访问性方面有显著改进。我们发布GLiNER2作为开源的可安装库，包含预训练模型和文档，网址为https://github.com/fastino-ai/GLiNER2。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Information extraction (IE) is fundamental to numerous NLP applications, yetexisting solutions often require specialized models for different tasks or relyon computationally expensive large language models. We present GLiNER2, aunified framework that enhances the original GLiNER architecture to supportnamed entity recognition, text classification, and hierarchical structured dataextraction within a single efficient model. Built pretrained transformerencoder architecture, GLiNER2 maintains CPU efficiency and compact size whileintroducing multi-task composition through an intuitive schema-based interface.Our experiments demonstrate competitive performance across extraction andclassification tasks with substantial improvements in deployment accessibilitycompared to LLM-based alternatives. We release GLiNER2 as an open-sourcepip-installable library with pre-trained models and documentation athttps://github.com/fastino-ai/GLiNER2.</description>
      <author>example@mail.com (Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis)</author>
      <guid isPermaLink="false">2507.18546v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs</title>
      <link>http://arxiv.org/abs/2507.18517v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用基础模型在高度杂乱视觉场景中进行语义对象分割的可能性，提出了一种基于注视点的提示生成方法来指导SAM模型，并在第一人称视觉数据上进行了微调，显著提高了分割质量。&lt;h4&gt;背景&lt;/h4&gt;研究背景是视觉引导的上肢神经假体应用，需要在真实世界高度杂乱的场景中进行有效的对象分割。&lt;h4&gt;目的&lt;/h4&gt;调查基础模型是否能够在不针对特定图像进行微调的情况下，在高度杂乱的视觉场景中执行对象分割。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于注视点生成提示的方法来指导分割任何模型，并在第一人称视觉数据上对其进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在Grasping-in-the-Wild语料库的真实世界挑战性数据上，IoU分割质量指标提高了0.51点。&lt;h4&gt;结论&lt;/h4&gt;基础模型通过适当的提示生成和微调，可以在高度杂乱的视觉场景中有效地执行对象分割，对视觉引导的上肢神经假体等应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们解决了使用基础模型进行语义对象分割的问题。我们研究了基础模型，这些模型在大量各种物体上进行了训练，是否能够在不针对包含日常物体的特定图像进行微调的情况下，执行对象分割，特别是在高度杂乱的视觉场景中。'in the wild'背景是由视觉引导上肢神经假体的目标应用驱动的。我们提出了一种基于注视点生成提示的方法，在我们的分割场景中指导分割任何模型，并在第一人称视觉数据上对其进行微调。我们方法的评估结果表明，在RoboFlow平台提供的Grasping-in-the-Wild语料库的真实世界挑战性数据上，IoU分割质量指标提高了0.51点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we address the problem of semantic object segmentation usingfoundation models. We investigate whether foundation models, trained on a largenumber and variety of objects, can perform object segmentation withoutfine-tuning on specific images containing everyday objects, but in highlycluttered visual scenes. The ''in the wild'' context is driven by the targetapplication of vision guided upper limb neuroprostheses. We propose a methodfor generating prompts based on gaze fixations to guide the Segment AnythingModel (SAM) in our segmentation scenario, and fine-tune it on egocentric visualdata. Evaluation results of our approach show an improvement of the IoUsegmentation quality metric by up to 0.51 points on real-world challenging dataof Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform(https://universe.roboflow.com/iwrist/grasping-in-the-wild)</description>
      <author>example@mail.com (Bolutife Atoki, Jenny Benois-Pineau, Renaud Péteri, Fabien Baldacci, Aymar de Rugy)</author>
      <guid isPermaLink="false">2507.18517v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Active Δ-learning with universal potentials for global structure optimization</title>
      <link>http://arxiv.org/abs/2507.18485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种主动学习方案，用于改进通用机器学习原子势(uMLIPs)在全局优化任务中的性能。通过比较不同的优化算法和基础模型，研究发现基于高斯过程回归的Δ模型的主动学习方法在识别全局最小值方面表现稳健，而复制交换公式方法在计算效率方面表现最佳。&lt;h4&gt;背景&lt;/h4&gt;通用机器学习原子势(uMLIPs)最近被提出并显示出良好的泛化能力。然而，当应用于样本外时，可能需要额外的数据收集来改进uMLIPs的性能。&lt;h4&gt;目的&lt;/h4&gt;研究一种主动学习方案，用于在全局优化背景下收集数据来改进uMLIPs，并比较不同全局优化算法和基础模型的效能。&lt;h4&gt;方法&lt;/h4&gt;提出一种主动学习方案，其中逐渐更新的uMLIP指导新结构的寻找，这些结构随后在密度泛函理论(DFT)水平上进行评估。使用局部SOAP描述符、高斯核和稀疏高斯过程回归模型基于新数据增强基础模型。比较了不同的全局优化算法(随机结构搜索、盆跳跃、具有竞争候选的贝叶斯方法和复制交换公式)和几种基础模型(CHGNet、MACE-MP0和MACE-MPA)。测试系统为银-硫簇和Ag(111)及Ag(100)上的硫诱导表面重构。&lt;h4&gt;主要发现&lt;/h4&gt;根据识别全局最小值的保真度判断，基于高斯过程回归的Δ模型的主动学习是一种稳健的方法。根据总CPU时间判断，复制交换公式方法是最有效的。&lt;h4&gt;结论&lt;/h4&gt;基于高斯过程回归的Δ模型的主动学习在识别全局最小值方面表现稳健，而复制交换公式方法在计算效率方面表现最佳。&lt;h4&gt;翻译&lt;/h4&gt;通用机器学习原子势(uMLIPs)最近已被提出并显示出良好的泛化能力。然而，当应用于样本外时，可能需要进一步的数据收集来改进uMLIPs。在本工作中，我们证明，每当MLIPs的预期用途是全局优化时，数据采集可以遵循主动学习方案，其中逐渐更新的uMLIP指导新结构的寻找，这些结构随后在密度泛函理论(DFT)水平上进行评估。在该方案中，我们使用基于新数据的局部SOAP描述符、高斯核和稀疏高斯过程回归模型，通过基于Δ的模型来增强基础模型。我们将该方法与不同的全局优化算法进行比较：随机结构搜索、盆跳跃、具有竞争候选的贝叶斯方法(GOFEE)和复制交换公式(REX)。我们还比较了几种基础模型：CHGNet、MACE-MP0和MACE-MPA。测试系统为银-硫簇和Ag(111)及Ag(100)上的硫诱导表面重构。根据识别全局最小值的保真度判断，基于高斯过程回归的Δ模型的主动学习是一种稳健的方法。根据总CPU时间判断，REX方法是最突出的高效方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal machine learning interatomic potentials (uMLIPs) have recently beenformulated and shown to generalize well. When applied out-of-sample, furtherdata collection for improvement of the uMLIPs may, however, be required. Inthis work we demonstrate that, whenever the envisaged use of the MLIPs isglobal optimization, the data acquisition can follow an active learning schemein which a gradually updated uMLIP directs the finding of new structures, whichare subsequently evaluated at the density functional theory (DFT) level. In thescheme, we augment foundation models using a {\Delta}-model based on this newdata using local SOAP-descriptors, Gaussian kernels, and a sparse GaussianProcess Regression model. We compare the efficacy of the approach withdifferent global optimization algorithms, Random Structure Search, BasinHopping, a Bayesian approach with competitive candidates (GOFEE), and a replicaexchange formulation (REX). We further compare several foundation models,CHGNet, MACE-MP0, and MACE-MPA. The test systems are silver-sulfur clusters andsulfur-induced surface reconstructions on Ag(111) and Ag(100). Judged by thefidelity of identifying global minima, active learning with GPR-based{\Delta}-models appears to be a robust approach. Judged by the total CPU timespent, the REX approach stands out as being the most efficient.</description>
      <author>example@mail.com (Joe Pitfield, Mads-Peter Verner Christiansen, Bjørk Hammer)</author>
      <guid isPermaLink="false">2507.18485v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection</title>
      <link>http://arxiv.org/abs/2507.18481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Q-Former Autoencoder的无监督医学异常检测框架，利用预训练视觉基础模型作为特征提取器，结合Q-Former架构和感知损失，在多个医学图像异常检测基准上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;医学图像异常检测是一个重要但具有挑战性的任务，主要挑战来自于异常的多样性和收集全面标注数据集的实际不可能性。&lt;h4&gt;目的&lt;/h4&gt;解决无监督医学异常检测问题，探索预训练视觉基础模型在医学图像分析中的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出Q-Former Autoencoder框架，直接利用冻结的视觉基础模型（如DINO、DINOv2和Masked Autoencoder）作为特征提取器，使用Q-Former架构作为瓶颈控制重建序列长度并聚合多尺度特征，同时结合预训练Masked Autoencoder计算的特征感知损失引导重建过程。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同的医学异常检测基准测试上评估，在BraTS2021、RESC和RSNA上取得了最先进的结果，表明视觉基础模型编码器在自然图像上预训练后无需进一步微调就能有效泛化到医学图像分析任务。&lt;h4&gt;结论&lt;/h4&gt;视觉基础模型编码器在自然图像上预训练后，无需进一步微调就能有效泛化到医学图像分析任务，为医学异常检测提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;医学图像异常检测是一个重要且具有挑战性的任务，由于异常的多样性和收集全面标注数据集的实际不可能性。在这项工作中，我们通过提出一种现代化的基于自编码器的框架Q-Former Autoencoder来解决无监督医学异常检测问题，该框架利用了最先进的预训练视觉基础模型，如DINO、DINOv2和Masked Autoencoder。我们不是从头开始训练编码器，而是直接使用冻结的视觉基础模型作为特征提取器，能够在没有领域特定微调的情况下实现丰富、多阶段、高层次的表示。我们提出使用Q-Former架构作为瓶颈，这可以控制重建序列的长度，同时有效聚合多尺度特征。此外，我们结合使用预训练Masked Autoencoder计算的特征感知损失，引导重建朝向语义上有意义的结构。我们的框架在四个不同的医学异常检测基准上进行了评估，在BraTS2021、RESC和RSNA上取得了最先进的结果。我们的结果强调了在自然图像上预训练的视觉基础模型编码器无需进一步微调就能有效泛化到医学图像分析任务的潜力。我们在https://github.com/emirhanbayar/QFAE上发布了代码和模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in medical images is an important yet challenging task dueto the diversity of possible anomalies and the practical impossibility ofcollecting comprehensively annotated data sets. In this work, we tackleunsupervised medical anomaly detection proposing a modernized autoencoder-basedframework, the Q-Former Autoencoder, that leverages state-of-the-art pretrainedvision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Insteadof training encoders from scratch, we directly utilize frozen vision foundationmodels as feature extractors, enabling rich, multi-stage, high-levelrepresentations without domain-specific fine-tuning. We propose the usage ofthe Q-Former architecture as the bottleneck, which enables the control of thelength of the reconstruction sequence, while efficiently aggregating multiscalefeatures. Additionally, we incorporate a perceptual loss computed usingfeatures from a pretrained Masked Autoencoder, guiding the reconstructiontowards semantically meaningful structures. Our framework is evaluated on fourdiverse medical anomaly detection benchmarks, achieving state-of-the-artresults on BraTS2021, RESC, and RSNA. Our results highlight the potential ofvision foundation model encoders, pretrained on natural images, to generalizeeffectively to medical image analysis tasks without further fine-tuning. Werelease the code and models at https://github.com/emirhanbayar/QFAE.</description>
      <author>example@mail.com (Francesco Dalmonte, Emirhan Bayar, Emre Akbas, Mariana-Iuliana Georgescu)</author>
      <guid isPermaLink="false">2507.18481v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data</title>
      <link>http://arxiv.org/abs/2507.18442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了AraTable，一个专门用于评估大型语言模型在阿拉伯语表格数据上推理和理解能力的新综合基准。研究发现LLMs在简单表格任务上表现尚可，但在复杂推理和事实验证方面仍面临挑战。研究还提供了一个性能接近人类评判的自动化评估框架。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言处理方面取得了显著进展，但在解释结构化数据特别是表格数据方面表现有限。英语表格数据基准广泛可用，但阿拉伯语由于公共资源有限和独特语言特性，代表性严重不足。&lt;h4&gt;目的&lt;/h4&gt;提出AraTable基准，解决阿拉伯语表格数据评估资源不足的问题，用于评估LLMs在阿拉伯语表格数据上的推理和理解能力。&lt;h4&gt;方法&lt;/h4&gt;AraTable包含直接问答、事实验证和复杂推理等多种评估任务，使用混合管道方法（LLMs生成内容，人类专家验证）确保数据质量，并提出了使用自我反思机制的完全自动化评估框架。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs在简单表格任务（如直接问答）上表现尚可，但在需要更深层次推理和事实验证的任务上仍面临重大认知挑战；自动化评估框架的性能与人类评判几乎相同。&lt;h4&gt;结论&lt;/h4&gt;研究提供了宝贵的公共可用资源和评估框架，有助于加速处理和分析阿拉伯语结构化数据的基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）的认知和推理能力已在自然语言处理领域取得了显著进展。然而，它们在解释结构化数据，特别是表格格式数据方面的表现仍然有限。尽管英语表格数据的基准测试广泛可用，但阿拉伯语由于公共资源有限和其独特的语言特性，仍然代表性不足。为解决这一差距，我们提出了AraTable，这是一个新颖且全面的基准，旨在评估LLMs应用于阿拉伯语表格数据时的推理和理解能力。AraTable包含各种评估任务，如直接问答、事实验证和复杂推理，涉及广泛的阿拉伯语表格数据来源。我们的方法采用混合管道，初始内容由LLMs生成，随后由人类专家过滤和验证，以确保高质量数据集。使用AraTable的初步分析表明，虽然LLMs在简单的表格任务（如直接问答）上表现尚可，但当任务需要更深层次的推理和事实验证时，它们仍然面临重大的认知挑战。这表明未来在提高复杂表格推理任务性能方面有大量工作机会。我们还提出了一个使用自我反思机制的完全自动化评估框架，其性能与人类评判几乎相同。这项研究提供了宝贵的公共可用资源和评估框架，可以帮助加速处理和分析阿拉伯语结构化数据的基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cognitive and reasoning abilities of large language models (LLMs) haveenabled remarkable progress in natural language processing. However, theirperformance in interpreting structured data, especially in tabular formats,remains limited. Although benchmarks for English tabular data are widelyavailable, Arabic is still underrepresented because of the limited availabilityof public resources and its unique language features. To address this gap, wepresent AraTable, a novel and comprehensive benchmark designed to evaluate thereasoning and understanding capabilities of LLMs when applied to Arabic tabulardata. AraTable consists of various evaluation tasks, such as direct questionanswering, fact verification, and complex reasoning, involving a wide range ofArabic tabular sources. Our methodology follows a hybrid pipeline, whereinitial content is generated by LLMs and subsequently filtered and verified byhuman experts to ensure high dataset quality. Initial analyses using AraTableshow that, while LLMs perform adequately on simpler tabular tasks such asdirect question answering, they continue to face significant cognitivechallenges when tasks require deeper reasoning and fact verification. Thisindicates that there are substantial opportunities for future work to improveperformance on complex tabular reasoning tasks. We also propose a fullyautomated evaluation framework that uses a self-deliberation mechanism andachieves performance nearly identical to that of human judges. This researchprovides a valuable, publicly available resource and evaluation framework thatcan help accelerate the development of foundational models for processing andanalysing Arabic structured data.</description>
      <author>example@mail.com (Rana Alshaikh, Israa Alghanmi, Shelan Jeawak)</author>
      <guid isPermaLink="false">2507.18442v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast</title>
      <link>http://arxiv.org/abs/2507.20582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 Accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;M-Net是一种创新的MRI肿瘤分割框架，通过引入Mesh-Cast机制和两阶段训练策略，有效利用相邻MRI切片的空间相关性，提高了分割的连续性和准确性，同时避免了全3D卷积的高计算成本。&lt;h4&gt;背景&lt;/h4&gt;MRI肿瘤分割在医学成像中面临独特挑战，3D数据的复杂性增加了体积分析的计算需求。相邻MRI切片的空间顺序排列包含有价值的信息，可增强分割连续性和准确性，但这一特性在现有模型中未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;设计一个灵活的框架专门用于顺序图像分割，捕获MRI切片间固有的'类时序'空间相关性，利用时序建模技术保持体积上下文信息，同时避免全3D卷积的高计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出M-Net框架，引入Mesh-Cast机制将任意顺序模型集成到通道和时序信息处理中；定义MRI顺序输入模式；设计两阶段顺序训练策略，先学习序列间共同模式，再优化切片特定特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;在BraTS2019和BraTS2023数据集上的实验表明，M-Net在所有关键指标上都优于现有方法，成为一种具有时序感知能力的MRI肿瘤分割的稳健解决方案。&lt;h4&gt;结论&lt;/h4&gt;M-Net通过利用相邻切片的空间相关性，有效解决了MRI肿瘤分割中的挑战，在保持体积上下文信息的同时避免了全3D卷积的高计算成本，提高了顺序分割任务中的泛化能力和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;MRI肿瘤分割在医学成像中仍然是一个关键挑战，其中体积分析由于3D数据的复杂性而面临独特的计算需求。相邻MRI切片的空间顺序排列提供了有价值的信息，增强了分割的连续性和准确性，然而这一特征在许多现有模型中仍未被充分利用。相邻MRI切片之间的空间相关性可以被视为'类时序'数据，类似于视频分割任务中的帧序列。为了弥合这一差距，我们提出了M-Net，一个专门为顺序图像分割设计的灵活框架。M-Net引入了新颖的Mesh-Cast机制，将任意顺序模型无缝集成到通道和时序信息的处理中，从而系统性地捕获MRI切片之间固有的'类时序'空间相关性。此外，我们定义了MRI顺序输入模式，并设计了两阶段顺序训练策略，首先专注于学习序列间的共同模式，然后优化切片特定的特征提取。该方法利用时序建模技术来保持体积上下文信息，同时避免了全3D卷积的高计算成本，从而提高了M-Net在顺序分割任务中的泛化能力和鲁棒性。在BraTS2019和BraTS2023数据集上的实验表明，M-Net在所有关键指标上都优于现有方法，使其成为一种具有时序感知能力的MRI肿瘤分割的稳健解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决MRI脑肿瘤分割中相邻切片空间相关性未被充分利用的问题。这个问题很重要，因为准确的脑肿瘤分割对疾病诊断和治疗规划至关重要，而现有方法（尤其是2D切片方法）难以有效捕捉MRI切片间的'类时序'空间相关性，导致分割性能受限。虽然3D模型可以处理这类数据，但2D切片方法在实际应用中更受欢迎，因为它们对计算资源要求更低。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过将MRI切片序列视为'类时序'数据，借鉴了自然语言处理中的序列处理算法（如LSTM、Transformer和Mamba SSM）以及现有的分割网络架构（如UNet及其变体）。他们创新地设计了M-Net框架，引入了Mesh-Cast机制来整合任意序列模型处理通道和时序信息，并定义了MRI序列输入模式和双阶段训练策略。这种方法在保持2D计算效率的同时，能够捕获3D体积上下文的'类时序'信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将MRI切片序列视为'类时序'数据，通过Mesh-Cast机制整合序列模型来处理通道和时序信息，并使用双阶段训练策略提升模型性能。整体流程包括：1)输入多模态MRI序列并进行预处理；2)使用Vision Sequential Module进行图像级特征提取；3)通过Mesh-Cast Sequential Module捕获时序和通道相关性；4)采用TPS训练策略分两个阶段训练模型；5)使用BCE Loss和Dice Loss的组合损失函数进行优化；6)输出脑肿瘤分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)重新概念化MRI分割，将切片序列视为'类时序'数据；2)创新的Mesh-Cast机制，使传统2D模型能够处理多维数据；3)双阶段序列训练策略，提高模型泛化能力。相比之前的工作，M-Net专门处理MRI切片序列的'类时序'空间相关性，而非单张图像内的序列块；在保持2D计算效率的同时实现了3D模型的性能；提供了灵活的框架适配不同序列处理算法；通过创新训练策略提升了模型鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M-Net通过创新的Mesh-Cast机制和双阶段训练策略，有效将MRI脑肿瘤切片序列视为'类时序'数据，在保持2D计算效率的同时实现了超越现有方法的分割性能，为医学影像分割提供了新的序列分析范式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; MRI tumor segmentation remains a critical challenge in medical imaging, wherevolumetric analysis faces unique computational demands due to the complexity of3D data. The spatially sequential arrangement of adjacent MRI slices providesvaluable information that enhances segmentation continuity and accuracy, yetthis characteristic remains underutilized in many existing models. The spatialcorrelations between adjacent MRI slices can be regarded as "temporal-like"data, similar to frame sequences in video segmentation tasks. To bridge thisgap, we propose M-Net, a flexible framework specifically designed forsequential image segmentation. M-Net introduces the novel Mesh-Cast mechanism,which seamlessly integrates arbitrary sequential models into the processing ofboth channel and temporal information, thereby systematically capturing theinherent "temporal-like" spatial correlations between MRI slices. Additionally,we define an MRI sequential input pattern and design a Two-Phase Sequential(TPS) training strategy, which first focuses on learning common patterns acrosssequences before refining slice-specific feature extraction. This approachleverages temporal modeling techniques to preserve volumetric contextualinformation while avoiding the high computational cost of full 3D convolutions,thereby enhancing the generalizability and robustness of M-Net in sequentialsegmentation tasks. Experiments on the BraTS2019 and BraTS2023 datasetsdemonstrate that M-Net outperforms existing methods across all key metrics,establishing itself as a robust solution for temporally-aware MRI tumorsegmentation.</description>
      <author>example@mail.com (Jiacheng Lu, Hui Ding, Shiyu Zhang, Guoping Huo)</author>
      <guid isPermaLink="false">2507.20582v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>MambaMap: Online Vectorized HD Map Construction using State Space Model</title>
      <link>http://arxiv.org/abs/2507.20224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MambaMap是一种新颖的框架，通过在状态空间中高效融合长程时序特征来构建在线矢量高清地图，解决了现有方法在处理时序信息和计算效率方面的问题。&lt;h4&gt;背景&lt;/h4&gt;高清(HD)地图对自动驾驶至关重要，为下游任务提供精确的道路信息。最近的研究表明时序建模在解决遮挡和扩展感知范围等挑战方面具有潜力，但现有方法要么未能充分利用时序信息，要么在处理长序列时计算开销巨大。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效融合长程时序特征的框架，构建在线矢量高清地图，以解决现有方法在时序信息利用和计算效率方面的不足。&lt;h4&gt;方法&lt;/h4&gt;MambaMap框架引入内存存储库来存储和利用历史帧信息，动态更新BEV特征和实例查询以提高对噪声和遮挡的鲁棒性；在状态空间中引入门控机制，选择性地整合地图元素依赖关系；设计创新的多方向和时空扫描策略，增强BEV和实例级别的特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的策略显著提高了预测准确性，同时确保了强大的时序一致性；在nuScenes和Argoverse2数据集上的大量实验表明，MambaMap在各种分割和感知范围上都优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;MambaMap是一种有效的框架，能够高效地构建在线矢量高清地图，代码将在https://github.com/ZiziAmy/MambaMap上提供。&lt;h4&gt;翻译&lt;/h4&gt;高清(HD)地图对自动驾驶至关重要，因为它们为下游任务提供精确的道路信息。最近的进展突显了时序建模在解决遮挡和扩展感知范围等挑战方面的潜力。然而，现有方法要么未能充分利用时序信息，要么在处理长序列时产生大量计算开销。为了应对这些挑战，我们提出了MambaMap，这是一种新颖的框架，在状态空间中高效融合长程时序特征以构建在线矢量高清地图。具体来说，MambaMap引入了一个内存存储库来存储和利用历史帧信息，动态更新BEV特征和实例查询，以提高对噪声和遮挡的鲁棒性。此外，我们在状态空间中引入了一个门控机制，选择性地整合地图元素的依赖关系，同时保持高计算效率。此外，我们设计了创新的多方向和时空扫描策略，以增强BEV和实例级别的特征提取。这些策略显著提高了我们方法的预测准确性，同时确保了强大的时序一致性。在nuScenes和Argoverse2数据集上的大量实验表明，我们提出的MambaMap方法在各种分割和感知范围内都优于最先进的方法。源代码将在https://github.com/ZiziAmy/MambaMap上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高清地图构建中的遮挡处理和长距离感知范围挑战。现有方法要么无法充分利用时间信息，要么处理长序列时计算开销过大。这个问题在现实中非常重要，因为高清地图对自动驾驶至关重要，提供精确的道路信息供下游任务如轨迹预测和路径规划使用，而遮挡和长距离感知是自动驾驶中的常见挑战，需要有效的时间建模来解决。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的两个关键局限：仅依靠单个循环特征不足以捕获长程时间依赖，以及维护和处理高度冗余的序列会带来大量内存和计算成本。基于这些局限，作者选择状态空间模型(SSM)作为解决方案，因为它擅长以线性复杂度捕获长程依赖，比基于transformer的时间融合方法更高效。作者借鉴了StreamMapNet的流式策略，使用了BEVFormer作为BEV特征编码器，并在SSM实现上参考了DSS（Diagonal State Spaces），虽然受到了Mamba的启发，但选择了DSS而非Mamba的实现。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用状态空间模型高效融合长程时间信息，通过记忆库存储和利用历史帧信息，在BEV和实例级别进行时间融合，并使用门控机制选择性整合地图元素的依赖关系。整体实现流程：1)从多视图图像中提取特征；2)将特征转换为初始BEV表示；3)通过记忆库、BEV Mamba Fusion和Instance Mamba Fusion三个组件进行时间建模；4)使用基于Deformable DETR的地图解码器处理增强的BEV特征；5)输出高清地图预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出MambaMap框架，利用SSM高效融合长程时间信息；2)设计有效的门控机制，在状态空间中进行高效的信息选择和整合；3)设计创新的多向和空间-时间扫描策略；4)使用滑动窗口记忆库动态更新BEV特征和实例查询。相比之前的工作，MambaMap使用SSM而非transformer或简单循环网络，能更高效捕获长程依赖；不维护高度冗余的序列，减少计算和内存开销；多向扫描策略比单方向或双方向扫描更能捕获空间关系；在实例级别同时考虑帧内交互和帧间时间依赖。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MambaMap通过创新的记忆库机制和状态空间模型，实现了高效的长程时间信息融合，显著提升了在线矢量高清地图构建的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-definition (HD) maps are essential for autonomous driving, as theyprovide precise road information for downstream tasks. Recent advanceshighlight the potential of temporal modeling in addressing challenges likeocclusions and extended perception range. However, existing methods either failto fully exploit temporal information or incur substantial computationaloverhead in handling extended sequences. To tackle these challenges, we proposeMambaMap, a novel framework that efficiently fuses long-range temporal featuresin the state space to construct online vectorized HD maps. Specifically,MambaMap incorporates a memory bank to store and utilize information fromhistorical frames, dynamically updating BEV features and instance queries toimprove robustness against noise and occlusions. Moreover, we introduce agating mechanism in the state space, selectively integrating dependencies ofmap elements in high computational efficiency. In addition, we designinnovative multi-directional and spatial-temporal scanning strategies toenhance feature extraction at both BEV and instance levels. These strategiessignificantly boost the prediction accuracy of our approach while ensuringrobust temporal consistency. Extensive experiments on the nuScenes andArgoverse2 datasets demonstrate that our proposed MambaMap approach outperformsstate-of-the-art methods across various splits and perception ranges. Sourcecode will be available at https://github.com/ZiziAmy/MambaMap.</description>
      <author>example@mail.com (Ruizi Yang, Xiaolu Liu, Junbo Chen, Jianke Zhu)</author>
      <guid isPermaLink="false">2507.20224v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion</title>
      <link>http://arxiv.org/abs/2507.20115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DSTF-Diffusion的新型网络流量生成模型，用于解决DDoS攻击检测中高质量标记训练数据集稀缺的问题。该模型基于扩散技术，采用双流架构，能够生成更接近真实网络流量特征的合成数据，从而提高机器学习检测模型的准确性。&lt;h4&gt;背景&lt;/h4&gt;针对分布式拒绝服务(DDoS)攻击的研究越来越多地依赖机器学习(ML)解决方案，但这些方案的有效性很大程度上依赖于标记训练数据集的质量。由于此类数据集稀缺，常使用合成轨迹进行数据增强。然而，当前的合成轨迹生成方法难以捕捉新兴DDoS攻击中表现出的复杂时间模式和空间分布，导致生成的数据与真实轨迹相似度不足，应用于机器学习任务时检测精度不理想。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效生成高质量合成网络流量数据的方法，以解决DDoS攻击检测中标记训练数据集稀缺的问题，提高机器学习检测模型的准确性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion)，这是一种基于扩散模型的多视图、多流网络流量生成模型，具有两个主要流：场景流利用空间映射将网络数据特征与稳定扩散模型的预训练领域连接起来，有效将复杂的网络交互转化为稳定扩散可处理的格式；空间流采用动态时间建模方法，精细捕捉网络流量的内在时间模式。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，与当前最先进的解决方案相比，该模型生成的数据与原始数据具有更高的统计相似性，并能广泛提升下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;DSTF-Diffusion模型通过双流架构有效解决了现有合成轨迹生成方法在捕捉复杂网络流量特征方面的局限性，为DDoS攻击检测提供了更高质量的训练数据，从而提高了机器学习检测模型的准确性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;针对分布式拒绝服务(DDoS)攻击，最近的研究工作越来越依赖基于机器学习(ML)的解决方案，其有效性很大程度上取决于标记训练数据集的质量。为解决此类数据集稀缺的问题，常采用带有合成轨迹的数据增强方法。然而，当前的合成轨迹生成方法难以捕捉新兴DDoS攻击中表现出的复杂时间模式和空间分布。这导致生成的轨迹与真实轨迹相似度不足，应用于机器学习任务时检测精度不理想。在本文中，我们提出了Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion)，这是一种基于扩散模型的多视图、多流网络流量生成模型，具有两个主要流：场景流利用空间映射将网络数据特征与稳定扩散模型的预训练领域连接起来，有效将复杂的网络交互转化为稳定扩散可处理的格式；而空间流采用动态时间建模方法，精细捕捉网络流量的内在时间模式。大量实验表明，与当前最先进的解决方案相比，该模型生成的数据与原始数据具有更高的统计相似性，并能广泛提升下游任务性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In response to Distributed Denial of Service (DDoS) attacks, recent researchefforts increasingly rely on Machine Learning (ML)-based solutions, whoseeffectiveness largely depends on the quality of labeled training datasets. Toaddress the scarcity of such datasets, data augmentation with synthetic tracesis often employed. However, current synthetic trace generation methods struggleto capture the complex temporal patterns and spatial distributions exhibited inemerging DDoS attacks. This results in insufficient resemblance to real tracesand unsatisfied detection accuracy when applied to ML tasks. In this paper, wepropose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view,multi-stream network traffic generative model based on diffusion models,featuring two main streams: The field stream utilizes spatial mapping to bridgenetwork data characteristics with pre-trained realms of stable diffusionmodels, effectively translating complex network interactions into formats thatstable diffusion can process, while the spatial stream adopts a dynamictemporal modeling approach, meticulously capturing the intrinsic temporalpatterns of network traffic. Extensive experiments demonstrate that datagenerated by our model exhibits higher statistical similarity to originalscompared to current state-of-the-art solutions, and enhance performances on awide range of downstream tasks.</description>
      <author>example@mail.com (Gongli Xi, Ye Tian, Yannan Hu, Yuchao Zhang, Yapeng Niu, Xiangyang Gong)</author>
      <guid isPermaLink="false">2507.20115v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly</title>
      <link>http://arxiv.org/abs/2507.19924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://dejian-lc.github.io/humansam/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了HumanSAM框架，用于将以人为中心的伪造视频分为三类：空间异常、外观异常和运动异常。通过融合视频理解和空间深度两个分支，并采用基于排名的置信度增强策略，HumanSAM在二元和多类伪造分类方面取得了有希望的结果。&lt;h4&gt;背景&lt;/h4&gt;生成式模型（特别是模拟真实人类行为的以人为中心的合成视频）对人类信息安全和真实性构成重大威胁。虽然在二元伪造视频检测方面取得了进展，但对伪造类型的细粒度理解不足，这引发了关于可靠性和可解释性的担忧。&lt;h4&gt;目的&lt;/h4&gt;提出HumanSAM框架，旨在将以人为中心的伪造分为三种常见类型：空间异常、外观异常和运动异常，以提高伪造视频检测的可靠性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;通过融合视频理解和空间深度两个分支来生成人类伪造表示；采用基于排名的置信度增强策略，通过引入三个先验分数来学习更强大的表示；构建了第一个公共基准数据集——以人为中心的伪造视频(HFV)数据集。&lt;h4&gt;主要发现&lt;/h4&gt;HumanSAM在二元和多类伪造分类方面与最先进的方法相比取得了有希望的结果。&lt;h4&gt;结论&lt;/h4&gt;HumanSAM框架通过细粒度分类不同类型的伪造视频，提高了伪造检测的可靠性和可解释性，为真实世界应用提供了更好的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大量由生成式模型合成的视频，特别是模拟真实人类行为的以人为中心的视频，对人类信息安全和真实性构成重大威胁。虽然在二元伪造视频检测方面取得了进展，但对伪造类型的细粒度理解不足，这引发了关于可靠性和可解释性的担忧，而这些对于实际应用至关重要。为解决这一局限，我们提出了HumanSAM，一个基于视频生成模型基本挑战的新框架。具体而言，HumanSAM旨在将以人为中心的伪造分为生成内容中常见的三种不同类型的伪影：空间异常、外观异常和运动异常。为更好地捕捉几何、语义和时空一致性的特征，我们提出通过融合视频理解和空间深度两个分支来生成人类伪造表示。在训练过程中，我们还采用基于排名的置信度增强策略，通过引入三个先验分数来学习更强大的表示。为了训练和评估，我们构建了第一个公共基准——以人为中心的伪造视频(HFV)数据集，所有类型的伪造都经过半自动仔细标注。在我们的实验中，HumanSAM在二元和多类伪造分类方面与最先进的方法相比取得了有希望的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous synthesized videos from generative models, especially human-centricones that simulate realistic human actions, pose significant threats to humaninformation security and authenticity. While progress has been made in binaryforgery video detection, the lack of fine-grained understanding of forgerytypes raises concerns regarding both reliability and interpretability, whichare critical for real-world applications. To address this limitation, wepropose HumanSAM, a new framework that builds upon the fundamental challengesof video generation models. Specifically, HumanSAM aims to classifyhuman-centric forgeries into three distinct types of artifacts commonlyobserved in generated content: spatial, appearance, and motion anomaly.Tobetter capture the features of geometry, semantics and spatiotemporalconsistency, we propose to generate the human forgery representation by fusingtwo branches of video understanding and spatial depth. We also adopt arank-based confidence enhancement strategy during the training process to learnmore robust representation by introducing three prior scores. For training andevaluation, we construct the first public benchmark, the Human-centric ForgeryVideo (HFV) dataset, with all types of forgeries carefully annotatedsemi-automatically. In our experiments, HumanSAM yields promising results incomparison with state-of-the-art methods, both in binary and multi-classforgery classification.</description>
      <author>example@mail.com (Chang Liu, Yunfan Ye, Fan Zhang, Qingyang Zhou, Yuchuan Luo, Zhiping Cai)</author>
      <guid isPermaLink="false">2507.19924v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Object-centric Video Question Answering with Visual Grounding and Referring</title>
      <link>http://arxiv.org/abs/2507.19599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的VideoLLM模型，通过引入STOM模块和VideoInfer数据集，实现了对象为中心的多模态视频理解，支持文本和视觉提示交互，在多个基准测试上表现优异。&lt;h4&gt;背景&lt;/h4&gt;Video Large Language Models (VideoLLMs)在通用视频理解方面取得了显著进展，但现有模型主要关注高层次理解且仅限于文本响应，限制了对象为中心的多轮交互灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型的局限性，使模型能够进行对象指代和定位，允许用户使用文本和视觉提示与视频进行交互。&lt;h4&gt;方法&lt;/h4&gt;1) 引入VideoLLM模型，支持视频推理任务中的对象指代和定位；2) 提出STOM(Spatial-Temporal Overlay Module)方法，将任意时间戳的视觉提示传播到视频其他帧；3) 发布VideoInfer数据集，包含需要推理的对象中心视频问答对。&lt;h4&gt;主要发现&lt;/h4&gt;在VideoInfer和其他6个任务的12个基准测试上，所提模型在视频问答和分割任务中均一致优于基线模型，证明了其在多模态、对象为中心的视频和图像理解方面的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的VideoLLM模型结合STOM模块和VideoInfer数据集，显著提升了视频理解能力，特别是在对象为中心的多轮交互场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;视频大型语言模型最近在通用视频理解方面取得了显著进展。然而，现有模型主要关注高层次理解且仅限于文本响应，限制了以对象为中心的多轮交互灵活性。本文做出三项贡献：(i)通过引入VideoLLM模型解决这些局限，该模型能够在视频推理任务中执行输入的对象指代和输出的对象定位，即允许用户使用文本和视觉提示与视频交互；(ii)提出STOM(时空叠加模块)，一种新颖方法，将任意时间戳输入的任意视觉提示传播到视频中的其余帧；(iii)发布VideoInfer，一个人工制作的对象中心视频指令数据集，包含需要推理的问答对。我们在VideoInfer和其他现有基准上进行了全面实验，涵盖视频问答和指代表格分割。在6个任务的12个基准测试上的结果表明，我们提出的模型在视频问答和分割方面一致优于基线，强调了其在多模态、对象为中心的视频和图像理解方面的鲁棒性。项目页面：https://qirui-chen.github.io/RGA3-release/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VideoLLMs) have recently demonstrated remarkableprogress in general video understanding. However, existing models primarilyfocus on high-level comprehension and are limited to text-only responses,restricting the flexibility for object-centric, multiround interactions. Inthis paper, we make three contributions: (i) we address these limitations byintroducing a VideoLLM model, capable of performing both object referring forinput and grounding for output in video reasoning tasks, i.e., allowing usersto interact with videos using both textual and visual prompts; (ii) we proposeSTOM (Spatial-Temporal Overlay Module), a novel approach that propagatesarbitrary visual prompts input at any single timestamp to the remaining frameswithin a video; (iii) we present VideoInfer, a manually curated object-centricvideo instruction dataset featuring questionanswering pairs that requirereasoning. We conduct comprehensive experiments on VideoInfer and otherexisting benchmarks across video question answering and referring objectsegmentation. The results on 12 benchmarks of 6 tasks show that our proposedmodel consistently outperforms baselines in both video question answering andsegmentation, underscoring its robustness in multimodal, object-centric videoand image understanding. Project page:https://qirui-chen.github.io/RGA3-release/.</description>
      <author>example@mail.com (Haochen Wang, Qirui Chen, Cilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, Weidi Xie, Stratis Gavves)</author>
      <guid isPermaLink="false">2507.19599v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding</title>
      <link>http://arxiv.org/abs/2507.18552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages; 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoMind，一个以视频为中心的多模态数据集，包含10.3万个视频样本，每个配有音频和三层文本描述。数据集包含2200万字，平均每样本225词。其特点是提供需要整合整个视频上下文的意图表达，使用思维链方法生成。研究团队建立了3000个人类验证样本的黄金标准基准，并设计混合认知检索实验评估模型性能。&lt;h4&gt;背景&lt;/h4&gt;现有视频数据集在深度认知视频理解方面存在局限，特别是缺乏对整个视频上下文整合的意图表达。这些深度认知表达对于需要深入视频理解的任务（如情感和意图识别）至关重要。&lt;h4&gt;目的&lt;/h4&gt;创建专门用于深度视频内容认知和增强多模态特征表示的视频中心多模态数据集，支持需要深度视频理解的下游识别任务，并建立评估深度认知视频理解的黄金标准基准。&lt;h4&gt;方法&lt;/h4&gt;构建包含10.3万个视频样本的数据集；为每个视频和音频提供三个层次（事实层、抽象层和意图层）的文本描述；使用思维链方法生成深度认知表达；添加主题、地点、时间、事件、动作和意图的标注；建立3000个人类验证样本的黄金标准基准；设计混合认知检索实验和多级检索指标评分系统。&lt;h4&gt;主要发现&lt;/h4&gt;评估了多种模型（如InternVideo、VAST、UMT-L）的性能；VideoMind作为细粒度跨模态对齐的有力基准，推进了需要深入视频理解的研究领域；数据集公开可用，促进了相关研究的发展。&lt;h4&gt;结论&lt;/h4&gt;VideoMind是一个强大的基准数据集，用于细粒度跨模态对齐，并推进了需要深入视频理解的研究领域，如情感和意图识别。该数据集的公开发布为视频理解研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了VideoMind，一个以视频为中心的多模态数据集，专为深度视频内容认知和增强的多模态特征表示而设计。该数据集包含10.3万个视频样本（3000个保留用于测试），每个样本都配有音频和系统详细的文本描述。具体来说，每个视频及其音频在三个层次（事实、抽象和意图）上进行描述，从表面到深度逐步推进。它包含超过2200万字，平均每个样本约225个单词。VideoMind与现有数据集的主要区别在于它提供意图表达，这些表达需要整合整个视频的上下文，不是直接可观察的。这些深度认知表达是使用思维链方法生成的，通过逐步推理提示多语言大语言模型。每个描述包括主题、地点、时间、事件、动作和意图的标注，支持下游识别任务。关键是我们建立了包含3000个人类验证样本的黄金标准基准，用于评估深度认知视频理解。我们设计了混合认知检索实验，通过多级检索指标评分，以适当评估深度视频理解。我们发布了模型（如InternVideo、VAST、UMT-L）的评估结果。VideoMind作为细粒度跨模态对齐的强大基准，推动了需要深入视频理解的研究领域，如情感和意图识别。该数据已在GitHub、HuggingFace和OpenDataLab上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces VideoMind, a video-centric omni-modal dataset designedfor deep video content cognition and enhanced multi-modal featurerepresentation. The dataset comprises 103K video samples (3K reserved fortesting), each paired with audio and systematically detailed textualdescriptions. Specifically, every video and its audio is described across threehierarchical layers (factual, abstract, and intent), progressing from surfaceto depth. It contains over 22 million words, averaging ~225 words per sample.VideoMind's key distinction from existing datasets is its provision of intentexpressions, which require contextual integration across the entire video andare not directly observable. These deep-cognitive expressions are generatedusing a Chain-of-Thought (COT) approach, prompting the mLLM throughstep-by-step reasoning. Each description includes annotations for subject,place, time, event, action, and intent, supporting downstream recognitiontasks. Crucially, we establish a gold-standard benchmark with 3,000 manuallyvalidated samples for evaluating deep-cognitive video understanding. We designhybrid-cognitive retrieval experiments, scored by multi-level retrievalmetrics, to appropriately assess deep video comprehension. Evaluation resultsfor models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as apowerful benchmark for fine-grained cross-modal alignment and advances fieldsrequiring in-depth video understanding, such as emotion and intent recognition.The data is publicly available on GitHub, HuggingFace, and OpenDataLab,https://github.com/cdx-cindy/VideoMind.</description>
      <author>example@mail.com (Baoyao Yang, Wanyun Li, Dixin Chen, Junxiang Chen, Wenbin Yao, Haifeng Lin)</author>
      <guid isPermaLink="false">2507.18552v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>INLA-RF: A Hybrid Modeling Strategy for Spatio-Temporal Environmental Data</title>
      <link>http://arxiv.org/abs/2507.18488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合贝叶斯模型和随机森林的混合时空建模框架INLA-RF，解决了环境过程中复杂非线性模式对传统地统计学建模的挑战。&lt;h4&gt;背景&lt;/h4&gt;环境过程在空间和时间上常表现出复杂、非线性的模式和间断性，这对传统的地统计学建模方法提出了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合时空建模框架，结合贝叶斯模型的可解释性和不确定性量化能力与随机森林的预测能力和灵活性。&lt;h4&gt;方法&lt;/h4&gt;提出两种新算法INLA-RF：INLA-RF1将随机森林预测作为INLA-SPDE模型中的偏移量；INLA-RF2使用随机森林直接校正选定的潜场节点。两种方法都实现了建模阶段间的不确定性传播，并提出了基于Kullback-Leibler散度的停止准则。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟研究发现，混合方法增强了时空预测能力，同时保持了不确定性估计的可解释性和一致性。&lt;h4&gt;结论&lt;/h4&gt;INLA-RF混合方法有效结合了贝叶斯模型和随机森林的优势，提高了时空预测准确性，同时保持了模型的可解释性和不确定性估计的合理性。&lt;h4&gt;翻译&lt;/h4&gt;环境过程通常在空间和时间上表现出复杂、非线性的模式和间断性，这对传统的地统计学建模方法提出了重大挑战。在本文中，我们提出了一种混合时空建模框架，结合了贝叶斯模型的可解释性和不确定性量化能力（使用INLA-SPDE方法估计）与随机森林的预测能力和灵活性。具体来说，我们引入了两种新算法，统称为INLA-RF，它们在迭代的两阶段框架中将统计时空模型与随机森林集成。第一种算法(INLA-RF1)将随机森林预测作为INLA-SPDE模型中的偏移量，而第二种(INLA-RF2)使用随机森林直接校正选定的潜场节点。这两种混合策略都实现了建模阶段之间的不确定性传播，这是现有混合方法中经常被忽视的方面。此外，我们提出了一种基于Kullback-Leibler散度的停止准则。我们通过两项模拟研究评估了所提出算法的预测性能和不确定性量化能力。结果表明，我们的混合方法增强了时空预测，同时保持了不确定性估计的可解释性和一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Environmental processes often exhibit complex, non-linear patterns anddiscontinuities across space and time, posing significant challenges fortraditional geostatistical modeling approaches. In this paper, we propose ahybrid spatio-temporal modeling framework that combines the interpretabilityand uncertainty quantification of Bayesian models -- estimated using theINLA-SPDE approach -- with the predictive power and flexibility of RandomForest (RF). Specifically, we introduce two novel algorithms, collectivelynamed INLA-RF, which integrate a statistical spatio-temporal model with RF inan iterative two-stage framework. The first algorithm (INLA-RF1) incorporatesRF predictions as an offset in the INLA-SPDE model, while the second (INLA-RF2)uses RF to directly correct selected latent field nodes. Both hybrid strategiesenable uncertainty propagation between modeling stages, an aspect oftenoverlooked in existing hybrid approaches. In addition, we propose aKullback-Leibler divergence-based stopping criterion. We evaluate thepredictive performance and uncertainty quantification capabilities of theproposed algorithms through two simulation studies. Results suggest that ourhybrid approach enhances spatio-temporal prediction while maintaininginterpretability and coherence in uncertainty estimates.</description>
      <author>example@mail.com (Mario Figueira, Michela Cameletti, Luca Patelli)</author>
      <guid isPermaLink="false">2507.18488v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs</title>
      <link>http://arxiv.org/abs/2507.18342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了EgoExoBench，第一个用于自我中心-他人中心视频理解和推理的基准测试，评估了多模态大语言模型在跨视角推理任务上的表现，发现这些模型在单视角任务上表现良好，但在跨视角语义对齐、视角关联和时间推理方面存在困难。&lt;h4&gt;背景&lt;/h4&gt;人类智能具有在第一人称和第三人称视角之间转移和整合知识的能力，这是人类能够从他人学习和传达自身经验的基础。尽管多模态大语言模型发展迅速，但它们进行跨视角推理的能力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;开发第一个用于自我中心-他人中心视频理解和推理的基准测试，评估多模态大语言模型在跨视角推理任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;构建EgoExoBench基准测试，基于公开可用数据集，包含超过7,300个问答对，跨越11个子任务，组织成三个核心挑战：语义对齐、视角关联和时间推理。评估了13个最先进的MLLMs。&lt;h4&gt;主要发现&lt;/h4&gt;尽管最先进的MLLMs在单视角任务上表现出色，但它们难以跨视角对齐语义、准确关联视角以及在自我中心-他人中心背景下推断时间动态。&lt;h4&gt;结论&lt;/h4&gt;EgoExoBench可作为研究具身智能体和寻求类人跨视角智能的智能助手的有价值资源。&lt;h4&gt;翻译&lt;/h4&gt;在第一人称(自我中心)和第三人称(他人中心)视角之间转移和整合知识是人类智能的本质特征，使人类能够从他人学习并传达自身经验见解。尽管多模态大语言模型发展迅速，但它们执行此类跨视角推理的能力尚未被探索。为此，我们引入了EgoExoBench，这是第一个用于自我中心-他人中心视频理解和推理的基准测试。基于公开可用数据集构建，EgoExoBench包含超过7,300个问答对，跨越11个子任务，这些子任务组织成三个核心挑战：语义对齐、视角关联和时间推理。我们评估了13个最先进的MLLMs，发现虽然这些模型在单视角任务上表现出色，但它们难以跨视角对齐语义、准确关联视角以及在自我中心-他人中心背景下推断时间动态。我们希望EgoExoBench能够成为研究具身智能体和寻求类人跨视角智能的智能助手的有价值资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferring and integrating knowledge across first-person (egocentric) andthird-person (exocentric) viewpoints is intrinsic to human intelligence,enabling humans to learn from others and convey insights from their ownexperiences. Despite rapid progress in multimodal large language models(MLLMs), their ability to perform such cross-view reasoning remains unexplored.To address this, we introduce EgoExoBench, the first benchmark foregocentric-exocentric video understanding and reasoning. Built from publiclyavailable datasets, EgoExoBench comprises over 7,300 question-answer pairsspanning eleven sub-tasks organized into three core challenges: semanticalignment, viewpoint association, and temporal reasoning. We evaluate 13state-of-the-art MLLMs and find that while these models excel on single-viewtasks, they struggle to align semantics across perspectives, accuratelyassociate views, and infer temporal dynamics in the ego-exo context. We hopeEgoExoBench can serve as a valuable resource for research on embodied agentsand intelligent assistants seeking human-like cross-view intelligence.</description>
      <author>example@mail.com (Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, Jiangmiao Pang)</author>
      <guid isPermaLink="false">2507.18342v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning</title>
      <link>http://arxiv.org/abs/2507.18252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个多模态人机协作框架，用于从眼动追踪数据中增强认知模式的提取，结合了大型语言模型与眼动分析技术，提高了认知分析的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;眼动追踪数据能揭示用户认知状态的有价值信息，但由于其结构化和非语言性质，分析困难。大型语言模型擅长文本推理，但在处理时间和数值数据方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;设计一个多模态人机协作框架，增强从眼动信号中提取认知模式的能力。&lt;h4&gt;方法&lt;/h4&gt;框架包含三个核心组件：(1)多阶段处理管道，使用水平和垂直分割结合LLM推理发现潜在眼动模式；(2)专家-模型共同评分模块，整合专家判断与LLM输出生成行为解释的信任分数；(3)混合异常检测模块，结合基于LSTM的时间建模和LLM驱动的语义分析。&lt;h4&gt;主要发现&lt;/h4&gt;在多个LLMs和提示策略测试下，该方法在一致性、可解释性和性能方面均有提升，在难度预测任务中准确率最高可达50%。&lt;h4&gt;结论&lt;/h4&gt;该框架为认知建模提供了可扩展、可解释的解决方案，在自适应学习、人机交互和教育分析领域具有广泛应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;眼动追踪数据揭示了用户认知状态的有价值见解，但由于其结构化、非语言的性质，难以分析。虽然大型语言模型(LLMs)擅长文本推理，但它们在处理时间和数值数据方面存在困难。本文提出了一个多模态人机协作框架，旨在从眼动信号中增强认知模式的提取。该框架包括：(1)使用水平和垂直分割以及LLM推理的多阶段管道，以揭示潜在的眼动模式；(2)专家-模型共同评分模块，整合专家判断与LLM输出，为行为解释生成信任分数；(3)结合基于LSTM的时间建模和LLM驱动的语义分析的混合异常检测模块。我们在多个LLMs和提示策略下的结果显示，一致性、可解释性和性能均有改进，在难度预测任务中准确率最高可达50%。这种方法为认知建模提供了可扩展、可解释的解决方案，并在自适应学习、人机交互和教育分析方面具有广泛潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Eye-tracking data reveals valuable insights into users' cognitive states butis difficult to analyze due to its structured, non-linguistic nature. Whilelarge language models (LLMs) excel at reasoning over text, they struggle withtemporal and numerical data. This paper presents a multimodal human-AIcollaborative framework designed to enhance cognitive pattern extraction fromeye-tracking signals. The framework includes: (1) a multi-stage pipeline usinghorizontal and vertical segmentation alongside LLM reasoning to uncover latentgaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expertjudgment with LLM output to generate trust scores for behavioralinterpretations; and (3) a hybrid anomaly detection module combining LSTM-basedtemporal modeling with LLM-driven semantic analysis. Our results across severalLLMs and prompt strategies show improvements in consistency, interpretability,and performance, with up to 50% accuracy in difficulty prediction tasks. Thisapproach offers a scalable, interpretable solution for cognitive modeling andhas broad potential in adaptive learning, human-computer interaction, andeducational analytics.</description>
      <author>example@mail.com (Dongyang Guo, Yasmeen Abdrabou, Enkeleda Thaqi, Enkelejda Kasneci)</author>
      <guid isPermaLink="false">2507.18252v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID</title>
      <link>http://arxiv.org/abs/2507.17995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted atIEEE International Joint Conference on Biometrics (IJCB)  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文引入了AG-VPReID.VIR数据集和TCC-VPReID架构，用于解决空中-地面视角和RGB-红外模态之间的跨模态视频行人重识别挑战。&lt;h4&gt;背景&lt;/h4&gt;跨模态(可见光和红外)行人重识别对24小时监控系统至关重要，但现有数据集主要关注地面视角。地面红外系统虽然具备夜间能力，但存在遮挡、覆盖范围有限和易受阻碍物影响的问题，而空中视角可以解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;引入首个空中-地面跨模态视频行人重识别数据集AG-VPReID.VIR，并提出TCC-VPReID架构解决跨平台和跨模态行人重识别的联合挑战。&lt;h4&gt;方法&lt;/h4&gt;创建包含1,837个身份和4,861个轨迹片段(124,855帧)的数据集，使用无人机固定和CCTV摄像头捕获RGB和红外模态数据；提出TCC-VPReID三流架构，包括风格鲁棒特征学习、基于记忆的跨视角适应和中间引导的时间建模。&lt;h4&gt;主要发现&lt;/h4&gt;AG-VPReID.VIR相比现有数据集呈现独特挑战，包括跨视角变化、模态差异和时间动态；TCC-VPReID框架在多种评估协议上取得显著性能提升。&lt;h4&gt;结论&lt;/h4&gt;AG-VPReID.VIR数据集和TCC-VPReID方法有效解决了空中-地面视角和RGB-红外模态之间的行人重识别挑战，数据集和代码已公开提供。&lt;h4&gt;翻译&lt;/h4&gt;可见光和红外模态之间的行人重识别对24小时监控系统至关重要，但现有数据集主要关注地面视角。虽然地面红外系统提供夜间能力，但它们存在遮挡、覆盖范围有限和易受阻碍物影响的问题——这些问题是空中视角独特解决的。为了解决这些限制，我们引入了AG-VPReID.VIR，这是首个空中-地面跨模态视频行人Re-ID数据集。该数据集使用无人机固定和CCTV摄像头在RGB和红外模态下捕获了1,837个身份和4,861个轨迹片段(124,855帧)。AG-VPReID.VIR呈现了独特挑战，包括跨视角变化、模态差异和时间动态。此外，我们提出了TCC-VPReID，一种新颖的三流架构，旨在解决跨平台和跨模态行人Re-ID的联合挑战。我们的方法通过风格鲁棒特征学习、基于记忆的跨视角适应和中间引导的时间建模，桥接了空中-地面视角和RGB-IR模态之间的域差距。实验表明，与现有数据集相比，AG-VPReID.VIR呈现了独特挑战，我们的TCC-VPReID框架在多种评估协议上取得了显著的性能提升。数据集和代码可在指定网址获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (Re-ID) across visible and infrared modalities iscrucial for 24-hour surveillance systems, but existing datasets primarily focuson ground-level perspectives. While ground-based IR systems offer nighttimecapabilities, they suffer from occlusions, limited coverage, and vulnerabilityto obstructions--problems that aerial perspectives uniquely solve. To addressthese limitations, we introduce AG-VPReID.VIR, the first aerial-groundcross-modality video-based person Re-ID dataset. This dataset captures 1,837identities across 4,861 tracklets (124,855 frames) using both UAV-mounted andfixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presentsunique challenges including cross-viewpoint variations, modality discrepancies,and temporal dynamics. Additionally, we propose TCC-VPReID, a novelthree-stream architecture designed to address the joint challenges ofcross-platform and cross-modality person Re-ID. Our approach bridges the domaingaps between aerial-ground perspectives and RGB-IR modalities, throughstyle-robust feature learning, memory-based cross-view adaptation, andintermediary-guided temporal modeling. Experiments show that AG-VPReID.VIRpresents distinctive challenges compared to existing datasets, with ourTCC-VPReID framework achieving significant performance gains across multipleevaluation protocols. Dataset and code are available athttps://github.com/agvpreid25/AG-VPReID.VIR.</description>
      <author>example@mail.com (Huy Nguyen, Kien Nguyen, Akila Pemasiri, Akmal Jahan, Clinton Fookes, Sridha Sridharan)</author>
      <guid isPermaLink="false">2507.17995v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>SV3.3B: A Sports Video Understanding Model for Action Recognition</title>
      <link>http://arxiv.org/abs/2507.17844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SV3.3B，一个轻量级的33亿参数视频理解模型，用于体育视频分析，通过结合时间运动差异采样和自监督学习，实现了高效设备端部署，并在性能上超越了大模型如GPT-4o。&lt;h4&gt;背景&lt;/h4&gt;传统体育视频分析方法受限于计算密集型模型，需要服务器端处理，且缺乏对运动动作的细粒度理解，难以捕捉体育分析中必要的细微生物力学转换和关键阶段。&lt;h4&gt;目的&lt;/h4&gt;解决传统体育视频分析方法的局限性，开发一种能够高效捕捉细微运动阶段、适合设备端部署的轻量级视频理解模型。&lt;h4&gt;方法&lt;/h4&gt;提出SV3.3B模型，采用DWT-VGG16-LDA关键帧提取机制识别最具代表性的16帧，结合V-DWT-JEPA2编码器（通过掩码去噪目标预训练）和针对体育动作描述生成微调的LLM解码器，使用时间运动差异采样和自监督学习技术。&lt;h4&gt;主要发现&lt;/h4&gt;SV3.3B在NSVA篮球数据集子集评估中表现出色，超越了包括GPT-4o变体在内的更大闭源模型，同时保持更低的计算要求；在真实验证指标上比GPT-4o提高29.2%，在信息密度、动作复杂性和测量精度等方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;SV3.3B模型能够生成技术详细和分析丰富的体育描述，为全面运动分析提供了强大工具，同时保持高效性，适合设备端部署。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了自动化体育视频分析的挑战，传统方法受限于计算密集型模型，需要服务器端处理，且缺乏对运动动作的细粒度理解。当前方法难以捕捉体育分析中必要的细微生物力学转换，常常错过准备、执行和跟进等关键阶段。为解决这些限制，我们提出了SV3.3B，这是一个轻量级的33亿参数视频理解模型，结合了新颖的时间运动差异采样和自监督学习，用于高效设备端部署。我们的方法采用基于DWT-VGG16-LDA的关键帧提取机制，智能识别体育序列中最具代表性的16帧，然后是通过掩码去噪目标预训练的V-DWT-JEPA2编码器，以及针对体育动作描述生成微调的LLM解码器。在NSVA篮球数据集子集上的评估显示，SV3.3B在传统文本生成指标和体育特定评估标准上都表现出色，超越了更大的闭源模型（包括GPT-4o变体），同时保持了显著更低的计算要求。该模型在生成技术详细和分析丰富的体育描述方面表现出色，在真实验证指标上比GPT-4o提高了29.2%，在信息密度、动作复杂性和测量精度等指标上有显著提高，这些指标对全面运动分析至关重要。模型可在https://huggingface.co/sportsvision/SV3.3B获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of automated sports video analysis, whichhas traditionally been limited by computationally intensive models requiringserver-side processing and lacking fine-grained understanding of athleticmovements. Current approaches struggle to capture the nuanced biomechanicaltransitions essential for meaningful sports analysis, often missing criticalphases like preparation, execution, and follow-through that occur withinseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3Bparameter video understanding model that combines novel temporal motiondifference sampling with self-supervised learning for efficient on-devicedeployment. Our approach employs a DWT-VGG16-LDA based keyframe extractionmechanism that intelligently identifies the 16 most representative frames fromsports sequences, followed by a V-DWT-JEPA2 encoder pretrained throughmask-denoising objectives and an LLM decoder fine-tuned for sports actiondescription generation. Evaluated on a subset of the NSVA basketball dataset,SV3.3B achieves superior performance across both traditional text generationmetrics and sports-specific evaluation criteria, outperforming largerclosed-source models including GPT-4o variants while maintaining significantlylower computational requirements. Our model demonstrates exceptional capabilityin generating technically detailed and analytically rich sports descriptions,achieving 29.2% improvement over GPT-4o in ground truth validation metrics,with substantial improvements in information density, action complexity, andmeasurement precision metrics essential for comprehensive athletic analysis.Model Available at https://huggingface.co/sportsvision/SV3.3B.</description>
      <author>example@mail.com (Sai Varun Kodathala, Yashwanth Reddy Vutukoori, Rakesh Vunnam)</author>
      <guid isPermaLink="false">2507.17844v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning</title>
      <link>http://arxiv.org/abs/2507.17402v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV'25. 13 pages, 6 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了HLFormer，首个用于部分相关视频检索(PRVR)的超球面建模框架，通过超球面空间学习解决现有方法在欧几里得空间中存在的几何失真问题，优化了视频的层次结构建模。&lt;h4&gt;背景&lt;/h4&gt;部分相关视频检索(PRVR)面临将未修剪视频与仅描述部分内容的文本查询进行匹配的挑战。现有方法在欧几里得空间中存在几何失真，错误表示视频的内在层次结构，忽略某些层次语义，导致次优的时间建模。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在欧几里得空间中存在的几何失真和层次结构建模不足的问题，提高部分相关视频检索的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出HLFormer超球面建模框架，集成洛伦兹注意力块和欧几里得注意力块在混合空间中编码视频嵌入，使用均值引导自适应交互模块动态融合特征，并通过部分序保持损失和洛伦兹锥约束强制执行'文本&lt;视频'的层次结构。&lt;h4&gt;主要发现&lt;/h4&gt;HLFormer通过强化视频内容与文本查询之间的部分相关性增强了跨模态匹配，大量实验表明其性能优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;HLFormer有效解决了部分相关视频检索中的层次结构建模问题，通过超球面空间学习提高了检索性能。&lt;h4&gt;翻译&lt;/h4&gt;部分相关视频检索(PRVR)解决了将未修剪视频与仅描述部分内容的文本查询进行匹配的关键挑战。现有方法在欧几里得空间中存在几何失真，这有时会错误地表示视频的内在层次结构，并忽略某些层次语义，最终导致次优的时间建模。为了解决这个问题，我们提出了首个用于PRVR的超球面建模框架，即HLFormer，它利用超球面空间学习来弥补欧几里得空间中层次建模能力的不足。具体来说，HLFormer集成了洛伦兹注意力块和欧几里得注意力块，在混合空间中对视频嵌入进行编码，使用均值引导自适应交互模块动态融合特征。此外，我们引入了部分序保持损失，通过洛伦兹锥约束强制执行'文本&lt;视频'的层次结构。这种方法通过强化视频内容与文本查询之间的部分相关性，进一步增强了跨模态匹配。大量实验表明，HLFormer优于最先进的方法。代码已发布于https://github.com/lijun2005/ICCV25-HLFormer。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partially Relevant Video Retrieval (PRVR) addresses the critical challenge ofmatching untrimmed videos with text queries describing only partial content.Existing methods suffer from geometric distortion in Euclidean space thatsometimes misrepresents the intrinsic hierarchical structure of videos andoverlooks certain hierarchical semantics, ultimately leading to suboptimaltemporal modeling. To address this issue, we propose the first hyperbolicmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic spacelearning to compensate for the suboptimal hierarchical modeling capabilities ofEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Blockand Euclidean Attention Block to encode video embeddings in hybrid spaces,using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.Additionally, we introduce a Partial Order Preservation Loss to enforce "text &lt;video" hierarchy through Lorentzian cone constraints. This approach furtherenhances cross-modal matching by reinforcing partial relevance between videocontent and text queries. Extensive experiments show that HLFormer outperformsstate-of-the-art methods. Code is released athttps://github.com/lijun2005/ICCV25-HLFormer.</description>
      <author>example@mail.com (Jun Li, Jinpeng Wang, Chaolei Tan, Niu Lian, Long Chen, Yaowei Wang, Min Zhang, Shu-Tao Xia, Bin Chen)</author>
      <guid isPermaLink="false">2507.17402v2</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Multilingual Self-Taught Faithfulness Evaluators</title>
      <link>http://arxiv.org/abs/2507.20752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为'多语言忠实性自教学评估器'的框架，该框架从合成多语言摘要数据中学习，利用跨语言迁移学习，无需大量人工标注数据即可评估多语言场景下大语言模型的信息幻觉问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的使用日益增加，对自动评估系统的需求也随之增长，特别是为了解决信息幻觉问题。现有的忠实性评估方法虽然显示出潜力，但主要集中在英语领域，并且通常需要昂贵的人工标注训练数据来微调专业模型。&lt;h4&gt;目的&lt;/h4&gt;随着LLMs在多语言环境中的采用增加，需要准确的忠实性评估器，这些评估器能够在不依赖大量标注数据的情况下跨语言运作。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'多语言忠实性自教学评估器'的框架，该框架仅从合成多语言摘要数据中学习，同时利用跨语言迁移学习。通过比较特定语言和混合语言的微调方法进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;LLM的一般语言能力与其在特定语言评估任务中的表现之间存在一致的关系。该框架在现有基线方法上显示出改进，包括最先进的英语评估器和基于机器翻译的方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够在不依赖大量人工标注数据的情况下，有效评估多语言场景下大型语言模型的信息幻觉问题，为多语言环境下的LLM评估提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的日益增加的使用增加了对自动评估系统的需求，特别是为了解决信息幻觉的挑战。尽管现有的忠实性评估方法已经显示出潜力，但它们主要集中在英语上，并且通常需要昂贵的人工标注训练数据来微调专业模型。随着LLMs在多语言环境中的采用增加，需要准确的忠实性评估器，这些评估器能够在不依赖大量标注数据的情况下跨语言运作。本文介绍了多语言忠实性自教学评估器，这是一个仅从合成多语言摘要数据中学习并利用跨语言迁移学习的框架。通过比较特定语言和混合语言的微调方法的实验，我们证明了LLM的一般语言能力与其在特定语言评估任务中的表现之间存在一致的关系。我们的框架在现有基线方法上显示出改进，包括最先进的英语评估器和基于机器翻译的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing use of large language models (LLMs) has increased the need forautomatic evaluation systems, particularly to address the challenge ofinformation hallucination. Although existing faithfulness evaluation approacheshave shown promise, they are predominantly English-focused and often requireexpensive human-labeled training data for fine-tuning specialized models. AsLLMs see increased adoption in multilingual contexts, there is a need foraccurate faithfulness evaluators that can operate across languages withoutextensive labeled data. This paper presents Self-Taught Evaluators forMultilingual Faithfulness, a framework that learns exclusively from syntheticmultilingual summarization data while leveraging cross-lingual transferlearning. Through experiments comparing language-specific and mixed-languagefine-tuning approaches, we demonstrate a consistent relationship between anLLM's general language capabilities and its performance in language-specificevaluation tasks. Our framework shows improvements over existing baselines,including state-of-the-art English evaluators and machine translation-basedapproaches.</description>
      <author>example@mail.com (Carlo Alfano, Aymen Al Marjani, Zeno Jonke, Amin Mantrach, Saab Mansour, Marcello Federico)</author>
      <guid isPermaLink="false">2507.20752v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Regularizing Subspace Redundancy of Low-Rank Adaptation</title>
      <link>http://arxiv.org/abs/2507.20745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, Accepted by ACMMM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ReSoRA方法，解决低秩适应(LoRA)及其变体在参数高效迁移学习中的表示冗余问题，通过显式建模映射子空间间冗余并自适应正则化子空间冗余，提高了现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;低秩适应(LoRA)及其变体通过最小化可训练参数和重参数化在参数高效迁移学习中表现出色，但其投影矩阵在训练中不受限制，导致高表示冗余和特征适应效果下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有LoRA方法中投影矩阵不受限制导致的高表示冗余问题，提高特征适应的有效性，并增强方法在不同数据集和架构上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;ReSoRA方法理论上将低秩子矩阵分解为多个等价子空间，系统地对不同投影上的特征分布应用去冗余约束，显式建模映射子空间间的冗余性并自适应正则化。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明ReSoRA在各种主干网络和数据集上，特别是在视觉语言检索和标准视觉分类基准中，持续提升现有最先进PETL方法的性能，且可即插即用，无额外推理成本。&lt;h4&gt;结论&lt;/h4&gt;ReSoRA通过解决LoRA中的表示冗余问题，有效提高了特征适应能力，具有良好的通用性和实用性，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;低秩适应(LoRA)及其变体通过最小化可训练参数和得益于重参数化，在参数高效迁移学习(PETL)中展现了强大的能力。然而，它们的投影矩阵在训练过程中保持不受限制，导致表示冗余度高，并降低了结果子空间中特征适应的有效性。虽然现有方法通过手动调整秩或隐式应用通道级掩码来缓解这一问题，但它们缺乏灵活性，并且在各种数据集和架构上泛化能力差。因此，我们提出了ReSoRA，一种显式建模映射子空间之间冗余性并自适应正则化低秩适应子空间冗余的方法。具体来说，它在理论上将低秩子矩阵分解为多个等价子空间，并系统地对不同投影上的特征分布应用去冗余约束。大量实验验证了我们提出的方法在各种主干网络和数据集上，特别是在视觉语言检索和标准视觉分类基准测试中，持续促进现有的最先进PETL方法。此外，作为训练监督，ReSoRA可以无缝集成到现有方法中，即插即用，且没有额外的推理成本。代码已在https://github.com/Lucenova/ReSoRA公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755359&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-Rank Adaptation (LoRA) and its variants have delivered strong capabilityin Parameter-Efficient Transfer Learning (PETL) by minimizing trainableparameters and benefiting from reparameterization. However, their projectionmatrices remain unrestricted during training, causing high representationredundancy and diminishing the effectiveness of feature adaptation in theresulting subspaces. While existing methods mitigate this by manually adjustingthe rank or implicitly applying channel-wise masks, they lack flexibility andgeneralize poorly across various datasets and architectures. Hence, we proposeReSoRA, a method that explicitly models redundancy between mapping subspacesand adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.Specifically, it theoretically decomposes the low-rank submatrices intomultiple equivalent subspaces and systematically applies de-redundancyconstraints to the feature distributions across different projections.Extensive experiments validate that our proposed method consistentlyfacilitates existing state-of-the-art PETL methods across various backbones anddatasets in vision-language retrieval and standard visual classificationbenchmarks. Besides, as a training supervision, ReSoRA can be seamlesslyintegrated into existing approaches in a plug-and-play manner, with noadditional inference costs. Code is publicly available at:https://github.com/Lucenova/ReSoRA.</description>
      <author>example@mail.com (Yue Zhu, Haiwen Diao, Shang Gao, Jiazuo Yu, Jiawen Zhu, Yunzhi Zhuge, Shuai Hao, Xu Jia, Lu Zhang, Ying Zhang, Huchuan Lu)</author>
      <guid isPermaLink="false">2507.20745v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Angle-distance decomposition based on deep learning for active sonar detection</title>
      <link>http://arxiv.org/abs/2507.20651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于深度学习的主动声纳目标检测方法，将检测过程分解为角度和距离估计任务，解决了传统信号处理方法在复杂水下环境中面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;使用主动声纳进行水下目标检测是海洋科学与工程中的关键研究领域，但传统信号处理方法在复杂水下环境中受噪声、混响和干扰影响面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服水下环境挑战的有效目标检测方法，提高检测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用深度学习模型进行主动声纳目标检测，将检测过程分解为独立的角度和距离估计任务，通过迁移学习和仿真解决水下声学数据有限的问题，最终整合距离和角度估计确定目标位置。&lt;h4&gt;主要发现&lt;/h4&gt;基于深度学习的主动声纳目标检测方法在具有挑战性的条件下能够实现有效且稳健的性能，迁移学习和仿真可以解决水下声学数据有限的问题。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功解决了传统信号处理方法在复杂水下环境中面临的挑战，为水下目标检测提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用主动声纳进行水下目标检测是海洋科学与工程中的一个关键研究领域。然而，由于噪声、混响和干扰，传统的信号处理方法在复杂的水下环境中面临重大挑战。为了解决这些问题，本文提出了一种基于深度学习的主动声纳目标检测方法，将检测过程分解为独立的角度和距离估计任务。主动声纳目标检测使用深度学习模型来预测目标距离和角度，最终目标位置通过整合这些估计来确定。有限的水下声学数据阻碍了有效模型训练，但迁移学习和仿真为这一挑战提供了实用的解决方案。实验结果验证了该方法在具有挑战性的条件下能够实现有效且稳健的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater target detection using active sonar constitutes a criticalresearch area in marine sciences and engineering. However, traditional signalprocessing methods face significant challenges in complex underwaterenvironments due to noise, reverberation, and interference. To address theseissues, this paper presents a deep learning-based active sonar target detectionmethod that decomposes the detection process into separate angle and distanceestimation tasks. Active sonar target detection employs deep learning models topredict target distance and angle, with the final target position determined byintegrating these estimates. Limited underwater acoustic data hinders effectivemodel training, but transfer learning and simulation offer practical solutionsto this challenge. Experimental results verify that the method achieveseffective and robust performance under challenging conditions.</description>
      <author>example@mail.com (Jichao Zhang, Xiao-Lei Zhang, Kunde Yang)</author>
      <guid isPermaLink="false">2507.20651v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Fusing CFD and measurement data using transfer learning</title>
      <link>http://arxiv.org/abs/2507.20576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于神经网络的非线性方法，通过迁移学习结合仿真和测量数据，用于飞机设计中的空气动力学分析。该方法克服了传统线性方法的局限性，在非线性区域产生更物理的解决方案，并可应用于飞行力学设计、结构尺寸设计和认证。&lt;h4&gt;背景&lt;/h4&gt;飞机设计中的空气动力学分析通常涉及不同精度和空间分辨率的方法，这些方法各有优缺点。目前，分布式量的数据融合方法主要依赖于本征正交分解等线性方法，存在局限性。&lt;h4&gt;目的&lt;/h4&gt;创建能够有效结合不同分析方法优势的数据驱动模型，引入一种非线性方法来结合仿真和测量数据，提高空气动力学分析的准确性和适用性。&lt;h4&gt;方法&lt;/h4&gt;基于神经网络的方法，使用迁移学习结合仿真和测量数据。训练过程分两步：首先在仿真数据上训练神经网络学习空间特征；然后在测量数据上进行迁移学习，通过重新训练网络的一小部分来校正系统误差。该方法考虑了数据的异质性，仿真数据空间分辨率高，测量数据稀疏但更准确。&lt;h4&gt;主要发现&lt;/h4&gt;与基于本征正交分解的现有方法相比，该方法在非线性附近产生更物理的解决方案，显示出显著改进。神经网络还可在任意流动条件下提供解决方案，提高了模型的实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的神经网络模型可用于飞行力学设计、结构尺寸设计和认证。由于训练策略非常通用，未来也可应用于更复杂的神经网络架构。&lt;h4&gt;翻译&lt;/h4&gt;飞机设计中的空气动力学分析通常涉及不同精度和空间分辨率的方法，这些方法各有优缺点。因此，创建能够有效结合这些优势的数据驱动模型是理想的。目前，分布式量的数据融合方法主要依赖于本征正交分解，这是一种线性方法。在本文中，我们介绍了一种基于神经网络的非线性方法，通过迁移学习结合仿真和测量数据。网络训练考虑了数据的异质性，因为仿真数据通常具有高空间分辨率，而测量数据稀疏但更准确。首先，神经网络在仿真数据上训练，学习分布式量的空间特征。第二步涉及在测量数据上进行迁移学习，通过仅重新训练整个神经网络模型的一小部分来校正仿真和测量之间的系统误差。该方法应用于多层感知器架构，通过在非线性附近产生更物理的解决方案，比基于本征正交分解的现有方法显示出显著改进。此外，神经网络可在任意流动条件下提供解决方案，从而使该模型对飞行力学设计、结构尺寸设计和认证有用。由于所提出的训练策略非常通用，将来也可以应用于更复杂的神经网络架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic analysis during aircraft design usually involves methods ofvarying accuracy and spatial resolution, which all have their advantages anddisadvantages. It is therefore desirable to create data-driven models whicheffectively combine these advantages. Such data fusion methods for distributedquantities mainly rely on proper orthogonal decomposition as of now, which is alinear method. In this paper, we introduce a non-linear method based on neuralnetworks combining simulation and measurement data via transfer learning. Thenetwork training accounts for the heterogeneity of the data, as simulation datausually features a high spatial resolution, while measurement data is sparsebut more accurate. In a first step, the neural network is trained on simulationdata to learn spatial features of the distributed quantities. The second stepinvolves transfer learning on the measurement data to correct for systematicerrors between simulation and measurement by only re-training a small subset ofthe entire neural network model. This approach is applied to a multilayerperceptron architecture and shows significant improvements over the establishedmethod based on proper orthogonal decomposition by producing more physicalsolutions near nonlinearities. In addition, the neural network providessolutions at arbitrary flow conditions, thus making the model useful for flightmechanical design, structural sizing, and certification. As the proposedtraining strategy is very general, it can also be applied to more complexneural network architectures in the future.</description>
      <author>example@mail.com (Alexander Barklage, Philipp Bekemeyer)</author>
      <guid isPermaLink="false">2507.20576v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning</title>
      <link>http://arxiv.org/abs/2507.20433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE Conference on Games (CoG) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FAST（基于自适应相似性的迁移框架）的方法，通过利用视觉帧和文本描述创建任务动态的潜在表示，来估计环境间的相似性，并据此选择候选策略进行知识迁移，从而加速新任务的学习过程。&lt;h4&gt;背景&lt;/h4&gt;迁移学习(TL)可以通过跨任务转移知识来加速学习，但面临负迁移、领域适应性和选择可靠源策略效率低等挑战。这些问题在游戏开发等动态变化领域尤为突出，因为场景会变化，代理必须适应，而持续发布新代理既昂贵又低效。&lt;h4&gt;目的&lt;/h4&gt;挑战迁移学习中的关键问题，提高知识转移效果，提升代理在跨任务中的性能，并降低计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出名为FAST（Framework for Adaptive Similarity-based Transfer）的方法，利用视觉帧和文本描述创建任务动态的潜在表示，估计环境间相似性，根据相似性分数选择候选策略进行知识迁移，以简化新任务的学习。&lt;h4&gt;主要发现&lt;/h4&gt;在多个赛道上的实验表明，FAST与从头学习的方法相比具有竞争性的最终性能，同时需要显著更少的训练步骤。&lt;h4&gt;结论&lt;/h4&gt;嵌入驱动的任务相似性估计在迁移学习中具有巨大潜力，能够有效提高学习效率并降低计算成本。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习(TL)通过跨任务转移知识提供了加速学习的潜力。然而，它面临关键挑战，如负迁移、领域适应性和选择可靠源策略的效率低下。这些问题在游戏开发等动态变化领域中通常表现为关键问题，即场景会变化，代理必须适应。持续发布新代理既昂贵又低效。在本工作中，我们挑战迁移学习中的关键问题，以提高知识转移效果，提升代理在跨任务中的性能，并降低计算成本。提出的方法称为FAST - 基于自适应相似性的迁移框架，它利用视觉帧和文本描述创建任务动态的潜在表示，利用这些表示来估计环境之间的相似性。相似性分数指导我们的方法选择候选策略，从中转移能力以简化新任务的学习。在多个赛道上的实验结果表明，FAST与从头学习的方法相比具有竞争性的最终性能，同时需要显著更少的训练步骤。这些发现突出了嵌入驱动的任务相似性估计的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer Learning (TL) offers the potential to accelerate learning bytransferring knowledge across tasks. However, it faces critical challenges suchas negative transfer, domain adaptation and inefficiency in selecting solidsource policies. These issues often represent critical problems in evolvingdomains, i.e. game development, where scenarios transform and agents mustadapt. The continuous release of new agents is costly and inefficient. In thiswork we challenge the key issues in TL to improve knowledge transfer, agentsperformance across tasks and reduce computational costs. The proposedmethodology, called FAST - Framework for Adaptive Similarity-based Transfer,leverages visual frames and textual descriptions to create a latentrepresentation of tasks dynamics, that is exploited to estimate similaritybetween environments. The similarity scores guides our method in choosingcandidate policies from which transfer abilities to simplify learning of noveltasks. Experimental results, over multiple racing tracks, demonstrate that FASTachieves competitive final performance compared to learning-from-scratchmethods while requiring significantly less training steps. These findingshighlight the potential of embedding-driven task similarity estimations.</description>
      <author>example@mail.com (Alessandro Capurso, Elia Piccoli, Davide Bacciu)</author>
      <guid isPermaLink="false">2507.20433v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Self-Supervised Neuro-Analytic Visual Servoing for Real-time Quadrotor Control</title>
      <link>http://arxiv.org/abs/2507.19878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Computer Vision Workshops  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种自监督神经分析、成本高效的模型，用于基于视觉的四旋翼控制。通过知识蒸馏，学生模型相比教师模型实现了11倍更快的推理速度，同时保持相似的控制精度，显著降低了计算和内存成本。该方法无需显式几何模型或标志标记，使四旋翼能够在GPS拒止环境中进行方向和移动控制。&lt;h4&gt;背景&lt;/h4&gt;传统视觉伺服控制存在数值不稳定性问题，且计算成本高，难以实时部署在资源受限的无人机平台上。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、低成本的自监督神经分析控制系统，使四旋翼能够在无需显式几何模型或标志标记的情况下进行精确的方向和移动控制。&lt;h4&gt;方法&lt;/h4&gt;研究采用了一种知识蒸馏方法，使用分析型IBVS控制器作为教师，训练一个1.7M参数的小型学生卷积神经网络。系统通过两阶段分割管道(结合YOLOv11和基于U-Net的掩码分割器)进行目标检测和方向估计，并利用模拟到现实的迁移学习技术。&lt;h4&gt;主要发现&lt;/h4&gt;1. 学生模型相比教师IBVS管道实现了11倍更快的推理速度；2. 学生模型在保持相似控制精度的同时，显著降低了计算和内存成本；3. 该方法在GPS拒止的室内环境中有效工作；4. 学生网络性能甚至优于教师模型。&lt;h4&gt;结论&lt;/h4&gt;提出的自监督神经分析控制系统为四旋翼视觉控制提供了一种高效、低成本的解决方案，通过知识蒸馏技术成功将复杂分析模型的能力转移到紧凑的神经网络中，适合实时机载部署。&lt;h4&gt;翻译&lt;/h4&gt;这项工作介绍了一种自监督神经分析、成本高效的、用于基于视觉的四旋翼控制的模型，其中一个小型1.7M参数的学生卷积神经网络从分析型教师(改进的基于图像的视觉伺服(IBVS)控制器)自动学习。我们的IBVS系统通过减少经典视觉伺服方程并实现高效稳定的图像特征检测来解决数值不稳定性问题。通过知识蒸馏，学生模型相比教师IBVS管道实现了11倍更快的推理速度，同时显示出相似的控制精度，计算和内存成本显著降低。我们的仅视觉自监督神经分析控制使四旋翼能够进行方向和移动，而无需显式几何模型或标志标记。提出的方法利用了模拟到现实的迁移学习，并在GPS拒止的室内环境中的小型无人机平台上得到了验证。我们的主要贡献包括：(1) 解决经典方法固有数值不稳定性的分析型IBVS教师；(2) 结合YOLOv11和基于U-Net的掩码分割器的两阶段分割管道，用于稳健的前后车辆分割以正确估计目标方向；(3) 高效的知识蒸馏双路径系统，将几何视觉伺服能力从分析型IBVS教师转移到紧凑的小型学生神经网络，该网络性能优于教师，同时适合实时机载部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces a self-supervised neuro-analytical, cost efficient,model for visual-based quadrotor control in which a small 1.7M parametersstudent ConvNet learns automatically from an analytical teacher, an improvedimage-based visual servoing (IBVS) controller. Our IBVS system solves numericalinstabilities by reducing the classical visual servoing equations and enablingefficient stable image feature detection. Through knowledge distillation, thestudent model achieves 11x faster inference compared to the teacher IBVSpipeline, while demonstrating similar control accuracy at a significantly lowercomputational and memory cost. Our vision-only self-supervised neuro-analyticcontrol, enables quadrotor orientation and movement without requiring explicitgeometric models or fiducial markers. The proposed methodology leveragessimulation-to-reality transfer learning and is validated on a small droneplatform in GPS-denied indoor environments. Our key contributions include: (1)an analytical IBVS teacher that solves numerical instabilities inherent inclassical approaches, (2) a two-stage segmentation pipeline combining YOLOv11with a U-Net-based mask splitter for robust anterior-posterior vehiclesegmentation to correctly estimate the orientation of the target, and (3) anefficient knowledge distillation dual-path system, which transfers geometricvisual servoing capabilities from the analytical IBVS teacher to a compact andsmall student neural network that outperforms the teacher, while being suitablefor real-time onboard deployment.</description>
      <author>example@mail.com (Sebastian Mocanu, Sebastian-Ion Nae, Mihai-Eugen Barbu, Marius Leordeanu)</author>
      <guid isPermaLink="false">2507.19878v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2507.19742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages,9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用近端策略优化(PPO)训练的自适应退化优化智能体(DOA)，用于解决基于粒子滤波的二维SLAM系统在室内环境中(如长直走廊)面临的退化问题。该智能体能动态调整不同传感器对位置优化的贡献，并通过迁移学习增强跨环境泛化能力。&lt;h4&gt;背景&lt;/h4&gt;基于粒子滤波的二维SLAM在室内定位任务中被广泛使用，因其效率高。然而，室内环境如长直走廊会导致SLAM中严重的退化问题。&lt;h4&gt;目的&lt;/h4&gt;解决SLAM系统在室内环境中面临的退化问题，特别是长直走廊等特殊环境导致的退化现象。&lt;h4&gt;方法&lt;/h4&gt;使用近端策略优化(PPO)训练自适应退化优化智能体(DOA)；提出系统化方法解决传统监督学习框架中的三个关键挑战；设计专门的奖励函数指导智能体发展对退化环境的感知能力；利用退化因子作为参考权重动态调整传感器贡献；采用迁移学习模块增强跨环境泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过消融研究证明了模型设计的合理性和迁移学习的作用；与最先进方法相比，所提出的DOA在各种环境中表现出优越的退化检测和优化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的DOA能够有效解决SLAM系统中的退化问题，特别是在室内环境中，通过动态调整传感器贡献和迁移学习实现了更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于粒子滤波的二维SLAM因其效率而在室内定位任务中被广泛使用。然而，室内环境如长直走廊会导致SLAM中严重的退化问题。在本文中，我们使用近端策略优化(PPO)训练一个自适应退化优化智能体(DOA)来解决退化问题。我们提出了一种系统化方法来解决传统监督学习框架中的三个关键挑战：(1)退化数据集中的数据获取瓶颈，(2)训练样本固有的质量下降，以及(3)注释协议设计中的模糊性。我们设计了一个专门的奖励函数来指导智能体发展对退化环境的感知能力。使用输出的退化因子作为参考权重，智能体可以动态调整不同传感器对位置优化的贡献。具体来说，观测分布向运动模型分布移动，步长由与退化因子相关的线性插值公式确定。此外，我们采用迁移学习模块赋予智能体跨环境泛化能力，并解决在退化环境中训练的低效问题。最后，我们进行了消融研究以证明我们模型设计的合理性和迁移学习的作用。我们还比较了所提出的DOA与最先进的方法，以证明其在各种环境中优越的退化检测和优化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Particle filter-based 2D-SLAM is widely used in indoor localization tasks dueto its efficiency. However, indoor environments such as long straight corridorscan cause severe degeneracy problems in SLAM. In this paper, we use ProximalPolicy Optimization (PPO) to train an adaptive degeneracy optimization agent(DOA) to address degeneracy problem. We propose a systematic methodology toaddress three critical challenges in traditional supervised learningframeworks: (1) data acquisition bottlenecks in degenerate dataset, (2)inherent quality deterioration of training samples, and (3) ambiguity inannotation protocol design. We design a specialized reward function to guidethe agent in developing perception capabilities for degenerate environments.Using the output degeneracy factor as a reference weight, the agent candynamically adjust the contribution of different sensors to pose optimization.Specifically, the observation distribution is shifted towards the motion modeldistribution, with the step size determined by a linear interpolation formularelated to the degeneracy factor. In addition, we employ a transfer learningmodule to endow the agent with generalization capabilities across differentenvironments and address the inefficiency of training in degenerateenvironments. Finally, we conduct ablation studies to demonstrate therationality of our model design and the role of transfer learning. We alsocompare the proposed DOA with SOTA methods to prove its superior degeneracydetection and optimization capabilities across various environments.</description>
      <author>example@mail.com (Yanbin Li, Canran Xiao, Hongyang He, Shenghai Yuan, Zong Ke, Jiajie Yu, Zixiong Qin, Zhiguo Zhang, Wenzheng Chi, Wei Zhang)</author>
      <guid isPermaLink="false">2507.19742v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Street network sub-patterns and travel mode</title>
      <link>http://arxiv.org/abs/2507.19648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过系统分类九个美国都市区的城市形态，并分析不同形态与移动行为的关系，发现城市形态对交通选择有显著影响，网格状形态促进公共交通使用并减少汽车依赖，而有机形态则增加汽车使用并减少其他交通方式。研究结果为城市规划和可持续政策提供了重要参考。&lt;h4&gt;背景&lt;/h4&gt;城市形态长期以来被认为是影响人类移动性的因素，但都市区之间城市形态的比较和正式分类仍然有限。&lt;h4&gt;目的&lt;/h4&gt;基于城市结构理论原则和无监督学习的进展，作者系统地使用密度、连通性和空间配置等结构指标对九个美国都市区的建成环境进行分类，并研究不同形态与移动行为的关系。&lt;h4&gt;方法&lt;/h4&gt;通过描述性统计、边际效应估计和事后统计测试，将产生的形态类型与移动模式联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;不同的城市形态系统性地与不同的移动行为相关联。网格状形态与显著更高的公共交通使用和减少的汽车依赖相关，而有机形态则与增加的汽车使用以及公共交通和主动移动性的显著下降相关。这些效应在统计学上是稳健的，突显了城市地区的空间配置在塑造交通选择方面发挥着根本性作用。&lt;h4&gt;结论&lt;/h4&gt;研究结果通过提供可重现的城市形态分类框架扩展了先前的工作，并证明了形态分析在比较城市研究中的附加价值。城市形态应被视为移动规划中的关键变量，为将空间类型学纳入可持续城市政策设计提供了经验支持。&lt;h4&gt;翻译&lt;/h4&gt;城市形态长期以来被认为是影响人类移动性的因素，但都市区之间城市形态的比较和正式分类仍然有限。基于城市结构理论原则和无监督学习的进展，我们系统地使用密度、连通性和空间配置等结构指标对九个美国都市区的建成环境进行了分类。通过描述性统计、边际效应估计和事后统计测试，将产生的形态类型与移动模式联系起来。我们研究表明，不同的城市形态系统性地与不同的移动行为相关联，例如网格状形态与显著更高的公共交通使用和减少的汽车依赖相关，而有机形态则与增加的汽车使用以及公共交通和主动移动性的显著下降相关。这些效应在统计学上是稳健的，突显了城市地区的空间配置在塑造交通选择方面发挥着根本性作用。我们的研究结果通过提供可重现的城市形态分类框架扩展了先前的工作，并证明了形态分析在比较城市研究中的附加价值。这些结果表明，城市形态应被视为移动规划中的关键变量，并为将空间类型学纳入可持续城市政策设计提供了经验支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban morphology has long been recognized as a factor shaping human mobility,yet comparative and formal classifications of urban form across metropolitanareas remain limited. Building on theoretical principles of urban structure andadvances in unsupervised learning, we systematically classified the builtenvironment of nine U.S. metropolitan areas using structural indicators such asdensity, connectivity, and spatial configuration. The resulting morphologicaltypes were linked to mobility patterns through descriptive statistics, marginaleffects estimation, and post hoc statistical testing. Here we show thatdistinct urban forms are systematically associated with different mobilitybehaviors, such as reticular morphologies being linked to significantly higherpublic transport use (marginal effect = 0.49) and reduced car dependence(-0.41), while organic forms are associated with increased car usage (0.44),and substantial declines in public transport (-0.47) and active mobility(-0.30). These effects are statistically robust (p &lt; 1e-19), highlighting thatthe spatial configuration of urban areas plays a fundamental role in shapingtransportation choices. Our findings extend previous work by offering areproducible framework for classifying urban form and demonstrate the addedvalue of morphological analysis in comparative urban research. These resultssuggest that urban form should be treated as a key variable in mobilityplanning and provide empirical support for incorporating spatial typologiesinto sustainable urban policy design.</description>
      <author>example@mail.com (Juan Fernando Riascos Goyes, Michael Lowry, Nicolás Guarín Zapata, Juan Pablo Ospina)</author>
      <guid isPermaLink="false">2507.19648v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Cough Analysis for Non-Small Cell Lung Cancer Detection</title>
      <link>http://arxiv.org/abs/2507.19174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Emilia Ambrosini and Simona Ferrante equally contributed to the work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了使用自动咳嗽分析作为区分非小细胞肺癌患者与健康人群的预筛查工具，通过机器学习和深度学习方法分析咳嗽录音，并评估了模型的性能和公平性。&lt;h4&gt;背景&lt;/h4&gt;非小细胞肺癌的早期检测对改善患者预后至关重要，需要新的方法促进早期诊断。&lt;h4&gt;目的&lt;/h4&gt;探索使用自动咳嗽分析作为预筛查工具来区分非小细胞肺癌患者和健康人群。&lt;h4&gt;方法&lt;/h4&gt;从227名受试者前瞻性采集咳嗽录音，使用支持向量机、XGBoost、卷积神经网络和VGG16迁移学习等方法分析数据，应用SHAP提高模型可解释性，并通过比较不同年龄组和性别群体的模型性能评估公平性。&lt;h4&gt;主要发现&lt;/h4&gt;CNN在测试集上达到最佳性能，准确率为0.83；SVM性能略低（验证集0.76，测试集0.78）但在计算资源有限环境中适用；使用SHAP提高了SVM模型透明度；公平性分析显示年龄差异（0.15）略高于性别差异（0.09）。&lt;h4&gt;结论&lt;/h4&gt;为了加强研究结果的可靠性，需要更大、更多样化和无偏见的数据库，特别是包括非小细胞肺癌风险人群和早期疾病阶段的个体。&lt;h4&gt;翻译&lt;/h4&gt;非小细胞肺癌的早期检测对改善患者预后至关重要，需要新的方法促进早期诊断。在本研究中，我们探索了使用自动咳嗽分析作为区分非小细胞肺癌患者与健康对照组的预筛查工具。从总共227名受试者前瞻性采集咳嗽录音。使用支持向量机、XGBoost等机器学习技术，以及卷积神经网络和使用VGG16的迁移学习等深度学习方法对录音进行分析。为了提高机器学习模型的可解释性，我们使用了Shapley加性解释。通过比较不同年龄组和性别的最佳模型性能，评估了模型在不同人口统计群体中的公平性。结果表明CNN在测试集上达到最佳性能，准确率为0.83。尽管如此，SVM性能略低，使其在计算能力较低的环境中适用。使用SHAP对SVM进行解释进一步提高了模型透明度，使其更适用于临床应用。公平性分析显示，在测试集上年龄差异略高于性别差异。因此，为了加强研究结果的可靠性，需要更大、更多样化和无偏见的数据库，特别是包括非小细胞肺癌风险人群和早期疾病阶段的个体。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection of non-small cell lung cancer (NSCLC) is critical forimproving patient outcomes, and novel approaches are needed to facilitate earlydiagnosis. In this study, we explore the use of automatic cough analysis as apre-screening tool for distinguishing between NSCLC patients and healthycontrols. Cough audio recordings were prospectively acquired from a total of227 subjects, divided into NSCLC patients and healthy controls. The recordingswere analyzed using machine learning techniques, such as support vector machine(SVM) and XGBoost, as well as deep learning approaches, specificallyconvolutional neural networks (CNN) and transfer learning with VGG16. Toenhance the interpretability of the machine learning model, we utilized ShapleyAdditive Explanations (SHAP). The fairness of the models across demographicgroups was assessed by comparing the performance of the best model acrossdifferent age groups (less than or equal to 58y and higher than 58y) and genderusing the equalized odds difference on the test set. The results demonstratethat CNN achieves the best performance, with an accuracy of 0.83 on the testset. Nevertheless, SVM achieves slightly lower performances (accuracy of 0.76in validation and 0.78 in the test set), making it suitable in contexts withlow computational power. The use of SHAP for SVM interpretation furtherenhances model transparency, making it more trustworthy for clinicalapplications. Fairness analysis shows slightly higher disparity across age(0.15) than gender (0.09) on the test set. Therefore, to strengthen ourfindings' reliability, a larger, more diverse, and unbiased dataset is needed-- particularly including individuals at risk of NSCLC and those in earlydisease stages.</description>
      <author>example@mail.com (Chiara Giangregorio, Cristina Maria Licciardello, Vanja Miskovic, Leonardo Provenzano, Alessandra Laura Giulia Pedrocchi, Andra Diana Dumitrascu, Arsela Prelaj, Marina Chiara Garassino, Emilia Ambrosini, Simona Ferrante)</author>
      <guid isPermaLink="false">2507.19174v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Accuracy and Limitations of Machine-Learned Interatomic Potentials for Magnetic Systems: A Case Study on Fe-Cr-C</title>
      <link>http://arxiv.org/abs/2507.18935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究解决了机器学习原子势(MLIPs)在磁性材料中应用的挑战，特别是针对技术重要的Fe-Cr-C系统。研究构建了两种深度机器学习势，并发现非磁性模型能准确预测动态特性，而自旋极化模型能准确预测静态特性。基于这一发现，提出了一种迁移学习方法，显著降低了开发磁性MLIPs的计算成本。&lt;h4&gt;背景&lt;/h4&gt;机器学习原子势已成为原子模拟的标准方法，但将其扩展到磁性材料仍面临挑战，因为需要捕获自旋波动效应，无论是明确还是隐含的方式。&lt;h4&gt;目的&lt;/h4&gt;解决Fe-Cr-C技术重要系统中磁性材料模拟的挑战，开发能准确预测静态和动态特性的机器学习势。&lt;h4&gt;方法&lt;/h4&gt;构建了两种深度机器学习势：一种基于非磁性DFT数据训练(DP-NM)，另一种基于自旋极化DFT数据训练(DP-M)。通过实验验证比较了两种模型的性能，并提出了迁移学习协议，即在非磁性DFT上预训练，再在少量自旋极化数据上微调。&lt;h4&gt;主要发现&lt;/h4&gt;动态集体特性(粘度和熔化温度)被DP-NM准确预测，而DP-M预测错误；静态局部特性(密度和晶格参数)被DP-M准确捕获，特别是在富铁合金中，而DP-NM表现不佳。这种现象可通过顺磁态特性解释：高温下局部磁矩在空间和时间上自平均，因此传输特性不需要明确处理磁效应，但平衡体积需要。迁移学习方法将计算成本降低了一个数量级以上。&lt;h4&gt;结论&lt;/h4&gt;开发在整个成分空间中捕获静态和动态行为的多用途势，需要在DFT计算中适当考虑温度引起的自旋波动，并将自旋自由度正确纳入经典力场中。&lt;h4&gt;翻译&lt;/h4&gt;机器学习的原子势已成为原子模拟的标准，然而它们向磁性材料的扩展仍然具有挑战性，因为自旋波动必须被明确或隐含地捕获。我们通过构建两种在DeePMD实现中的深度机器学习势来解决Fe-Cr-C技术重要系统的这一问题：一种在非磁性DFT数据上训练(DP-NM)，另一种在自旋极化DFT数据上训练(DP-M)。广泛的实验验证揭示了一个显著的对立现象。动态、集体特性，粘度和熔化温度被DP-NM准确再现，却被DP-M错误估计。静态、局部特性，密度和晶格参数被DP-M出色地捕获，特别是在富铁合金中，而DP-NM失败。这种行为可以通过顺磁态的一般性质解释：在高温下，局部磁矩在空间和时间上自平均，因此对于传输特性，它们的明确处理是不必要的，但对于平衡体积是必要的。利用这一见解，我们展示了一种迁移学习协议，在非磁性DFT上预训练，然后在少量自旋极化数据上微调，将开发磁性MLIPs的计算成本降低了一个数量级以上。开发在整个成分空间中捕获静态和动态行为的多用途势，需要在DFT计算中适当考虑温度引起的自旋波动，并将自旋自由度正确地纳入经典力场中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learned interatomic potentials (MLIPs) have become the gold standardfor atomistic simulations, yet their extension to magnetic materials remainschallenging because spin fluctuations must be captured either explicitly orimplicitly. We address this problem for the technologically vital Fe-Cr-Csystem by constructing two deep machine learning potentials in DeePMDrealization: one trained on non-magnetic DFT data (DP-NM) and one onspin-polarised DFT data (DP-M). Extensive validation against experimentsreveals a striking dichotomy. The dynamic, collective properties, viscosity andmelting temperatures are reproduced accurately by DP-NM but are incorrectlyestimated by DP-M. Static, local properties, density, and lattice parametersare captured excellently by DP-M, especially in Fe-rich alloys, whereas DP-NMfails. This behaviour is explained by general properties of paramagnetic state:at high temperature, local magnetic moments self-average in space and time, sotheir explicit treatment is unnecessary for transport properties but essentialfor equilibrium volumes. Exploiting this insight, we show that atransfer-learning protocol, pre-training on non-magnetic DFT and fine-tuning ona small set of spin-polarised data, reduces the computational cost to developmagnetic MLIPs by more than an order of magnitude. Developing general-purposepotentials that capture static and dynamic behaviors throughout the wholecomposition space requires proper accounting for temperature-induced spinfluctuations in DFT calculations and correctly incorporating spin degrees offreedom into classical force fields.</description>
      <author>example@mail.com (E. O. Khazieva, N. M. Chtchelkatchev, R. E. Ryltsev)</author>
      <guid isPermaLink="false">2507.18935v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning</title>
      <link>http://arxiv.org/abs/2507.18743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究构建了SAR-Text大规模高质量数据集，设计了SAR-Narrator框架生成文本描述，并在多个视觉-语言任务上验证了其有效性，显著提升了SAR图像语义理解能力。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)在遥感领域取得显著突破，SAR图像因其全天候能力在遥感中至关重要，但缺乏大规模、高质量的SAR图像-文本数据集阻碍了其语义理解。&lt;h4&gt;目的&lt;/h4&gt;构建一个大规模、高质量的SAR图像-文本数据集，并验证该数据集在视觉-语言任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;构建包含超过13万对SAR图像-文本的SAR-Text数据集；设计SAR-Narrator框架，通过多阶段渐进式迁移学习策略生成文本描述；构建三个代表性模型(SAR-RS-CLIP、SAR-RS-CoCa和SAR-GPT)；在图像-文本检索、图像描述和视觉问答三个任务上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;SAR-RS-CLIP在检索性能上显著提升，平均召回率分别提高16.43%和10.54%；SAR-RS-CoCa在描述任务中的BLEU-4、SPICE和CIDEr分数分别超过原始模型8倍、4倍和10倍；SAR-GPT在多个SAR-VQA数据集上优于基线模型，展现出更强的语义理解和推理能力。&lt;h4&gt;结论&lt;/h4&gt;SAR-Narrator作为一个灵活的描述工具，可以被社区采用构建更大规模的SAR图像-文本数据集。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)近年来在遥感领域取得了显著突破。合成孔径雷达(SAR)图像凭借其全天候能力，在遥感中至关重要，然而缺乏大规模、高质量的SAR图像-文本数据集阻碍了其语义理解。在本文中，我们构建了SAR-Text，一个包含超过13万对SAR图像-文本的大规模高质量数据集。为构建SAR-Text数据集，我们设计了SAR-Narrator框架，通过多阶段渐进式迁移学习策略为SAR图像生成文本描述。为验证SAR-TEXT数据集的有效性，我们在三个典型的视觉-语言任务上进行了实验：图像-文本检索、图像描述和视觉问答(VQA)。具体而言，我们在SAR-TEXT上构建了三个代表性模型：SAR-RS-CLIP、SAR-RS-CoCa和SAR-GPT。SAR-RS-CLIP在检索性能上取得了显著提升，在OSdataset-512和HRSID测试集上分别将平均召回率提高了16.43%和10.54%。在描述任务中，SAR-RS-CoCa的BLEU-4、SPICE和CIDEr分数分别超过了原始CoCa模型的8倍、4倍和10倍。在VQA任务中，SAR-GPT在多个SAR-VQA数据集上优于基线和单阶段模型，展现出更强的语义理解和推理能力，这进一步通过定性结果得到证实。值得注意的是，作为一个灵活的描述工具，SAR-Narrator可以被社区 readily采用构建更大规模的SAR图像-文本数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) have achieved remarkable breakthroughs in thefield of remote sensing in recent years. Synthetic Aperture Radar (SAR)imagery, with its all-weather capability, is essential in remote sensing, yetthe lack of large-scale, high-quality SAR image-text datasets hinders itssemantic understanding. In this paper, we construct SAR-Text, a large-scale andhigh-quality dataset consisting of over 130,000 SAR image-text pairs. Toconstruct the SAR-Text dataset, we design the SAR-Narrator framework, whichgenerates textual descriptions for SAR images through a multi-stage progressivetransfer learning strategy. To verify the effectiveness of the SAR-TEXTdataset, we conduct experiments on three typical vision-language tasks:image-text retrieval, image captioning, and visual question answering (VQA).Specifically, we construct three representative models on SAR-TEXT:SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notableimprovements in retrieval performance, boosting average recall by 16.43% and10.54% on the OSdataset-512 and HRSID test sets, respectively. In thecaptioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceedingthose of the original CoCa model by more than 8x, 4x, and 10x, respectively. Inthe VQA task, SAR-GPT outperforms baseline and single-stage models on multipleSAR-VQA datasets, demonstrating stronger semantic understanding and reasoningability, as further confirmed by qualitative results. It is worth noting that,as a flexible captioning tool, SAR-Narrator can be readily adopted by thecommunity to construct larger-scale SAR image-text datasets.</description>
      <author>example@mail.com (Xinjun Cheng, Yiguo He, Junjie Zhu, Chunping Qiu, Jun Wang, Qiangjuan Huang, Ke Yang)</author>
      <guid isPermaLink="false">2507.18743v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>A Concept for Efficient Scalability of Automated Driving Allowing for Technical, Legal, Cultural, and Ethical Differences</title>
      <link>http://arxiv.org/abs/2507.18326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to be published at 2025 28th IEEE International Conference  on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, November  18-21, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决自动驾驶可扩展性挑战的方法，通过两阶段微调过程实现通用能力向不同系统和环境的适应。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶的高效可扩展性对降低成本、提高安全性、节约资源和最大化影响至关重要，但当前研究局限于特定车辆和上下文。&lt;h4&gt;目的&lt;/h4&gt;解决通用能力向期望系统和环境进行可扩展适应的挑战，以应对不同车辆类型、传感器、执行器、法规、法律、文化和伦理差异。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段微调过程：第一阶段通过特定国家奖励模型进行环境特定微调，作为技术适应与社会政治需求的接口；第二阶段通过车辆特定迁移学习促进系统适应并验证设计决策。&lt;h4&gt;主要发现&lt;/h4&gt;所提出概念提供了一种数据驱动过程，整合技术和社会政治方面，实现跨技术、法律、文化和伦理差异的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效应对自动驾驶在不同环境中的部署挑战，实现高效可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶(AD)的高效可扩展性是降低成本、提高安全性、节约资源和最大化影响的关键。然而，研究专注于特定车辆和上下文，而广泛部署需要在各种配置和环境下的可扩展性。车辆类型、传感器、执行器、交通法规、法律要求、文化动态甚至伦理范式的差异，要求数据驱动开发的能力具有高度灵活性。在本文中，我们解决了通用能力向期望系统和环境进行可扩展适应的挑战。我们的概念遵循两阶段微调过程。第一阶段通过特定国家的奖励模型进行特定环境的微调，该模型作为技术适应与社会政治需求之间的接口。第二阶段，车辆特定的迁移学习促进系统适应并控制设计决策的验证。总之，我们的概念提供了一种整合技术和社會政治方面的数据驱动过程，能够有效跨越技术、法律、文化和伦理差异实现可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient scalability of automated driving (AD) is key to reducing costs,enhancing safety, conserving resources, and maximizing impact. However,research focuses on specific vehicles and context, while broad deploymentrequires scalability across various configurations and environments.Differences in vehicle types, sensors, actuators, but also traffic regulations,legal requirements, cultural dynamics, or even ethical paradigms demand highflexibility of data-driven developed capabilities. In this paper, we addressthe challenge of scalable adaptation of generic capabilities to desired systemsand environments. Our concept follows a two-stage fine-tuning process. In thefirst stage, fine-tuning to the specific environment takes place through acountry-specific reward model that serves as an interface between technologicaladaptations and socio-political requirements. In the second stage,vehicle-specific transfer learning facilitates system adaptation and governsthe validation of design decisions. In sum, our concept offers a data-drivenprocess that integrates both technological and socio-political aspects,enabling effective scalability across technical, legal, cultural, and ethicaldifferences.</description>
      <author>example@mail.com (Lars Ullrich, Michael Buchholz, Jonathan Petit, Klaus Dietmayer, Knut Graichen)</author>
      <guid isPermaLink="false">2507.18326v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection</title>
      <link>http://arxiv.org/abs/2507.19547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用卷积自编码器的深度学习框架，用于从心房颤动患者的心内电图中进行无监督特征提取，以帮助检测AF驱动因素。&lt;h4&gt;背景&lt;/h4&gt;心房颤动是最常见的心律失常，但当前的消融疗法（包括肺静脉隔离）在持续性AF中经常无效，因为涉及到非肺静脉驱动因素。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度学习框架，使用卷积自编码器从单极和双腔心内电图中进行无监督特征提取，实现EGM分析和自动化的表征，有助于检测AF驱动因素。&lt;h4&gt;方法&lt;/h4&gt;使用卷积自编码器框架处理来自291名患者的11,404个采集记录（包含228,080个单极EGM和171,060个双极EGM），进行无监督特征提取和下游分类。&lt;h4&gt;主要发现&lt;/h4&gt;自编码器成功学习了具有低重建损失的潜在表征，保留了形态学特征。提取的嵌入使下游分类器能够以中等性能检测旋转性和局灶性活动，并在识别心房EGM纠缠方面实现了高判别性能。&lt;h4&gt;结论&lt;/h4&gt;提出的方法可以实时运行，能够集成到临床电解剖映射系统中，辅助在消融过程中识别致心律失常区域，无监督学习有助于从心内信号中发现生理上有意义的特征。&lt;h4&gt;翻译&lt;/h4&gt;心房颤动是最常见的心律失常，但当前的消融疗法（包括肺静脉隔离）在持续性AF中经常无效，因为涉及到非肺静脉驱动因素。本研究提出了一种使用卷积自编码器的深度学习框架，用于从AF消融研究期间记录的单极和双腔心内电图中进行无监督特征提取。这些心房电活动的潜在表征能够实现EGM分析和自动化的表征，有助于检测AF驱动因素。数据库包含来自291名患者的11,404个采集记录，包含228,080个单极EGM和171,060个双极EGM。自编码器成功学习了具有低重建损失的潜在表征，保留了形态学特征。提取的嵌入使下游分类器能够以中等性能检测旋转性和局灶性活动，并在识别心房EGM纠缠方面实现了高判别性能。提出的方法可以实时运行，能够集成到临床电解剖映射系统中，辅助在消融过程中识别致心律失常区域。这项工作强调了无监督学习在从心内信号中发现生理上有意义的特征方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yetcurrent ablation therapies, including pulmonary vein isolation, are frequentlyineffective in persistent AF due to the involvement of non-pulmonary veindrivers. This study proposes a deep learning framework using convolutionalautoencoders for unsupervised feature extraction from unipolar and bipolarintracavitary electrograms (EGMs) recorded during AF in ablation studies. Theselatent representations of atrial electrical activity enable thecharacterization and automation of EGM analysis, facilitating the detection ofAF drivers.  The database consisted of 11,404 acquisitions recorded from 291 patients,containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoderssuccessfully learned latent representations with low reconstruction loss,preserving the morphological features. The extracted embeddings alloweddownstream classifiers to detect rotational and focal activity with moderateperformance (AUC 0.73-0.76) and achieved high discriminative performance inidentifying atrial EGM entanglement (AUC 0.93).  The proposed method can operate in real-time and enables integration intoclinical electroanatomical mapping systems to assist in identifyingarrhythmogenic regions during ablation procedures. This work highlights thepotential of unsupervised learning to uncover physiologically meaningfulfeatures from intracardiac signals.</description>
      <author>example@mail.com (Pablo Peiro-Corbacho, Long Lin, Pablo Ávila, Alejandro Carta-Bergaz, Ángel Arenal, Carlos Sevilla-Salcedo, Gonzalo R. Ríos-Muñoz)</author>
      <guid isPermaLink="false">2507.19547v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction</title>
      <link>http://arxiv.org/abs/2507.17924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanPulse是一个可扩展的深度学习框架，通过将每个兴趣点(POI)视为单独节点，提供超细粒度、全市范围的OD流动预测，解决了现有方法在空间假设、跨城市泛化和计算成本方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;准确的人口流动预测对城市规划、交通管理和公共卫生至关重要，但现有方法面临关键挑战：传统模型依赖静态空间假设，深度学习模型难以跨城市泛化，大型语言模型计算成本高且无法捕捉空间结构，同时许多方法通过聚类兴趣点或限制覆盖范围牺牲了分辨率。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的深度学习框架，提供超细粒度、全市范围的OD流动预测，并确保模型在不同城市背景下的稳健泛化能力。&lt;h4&gt;方法&lt;/h4&gt;UrbanPulse框架将每个POI视为单独节点，结合时间图卷积编码器和基于Transformer的解码器建模多尺度时空依赖关系，并采用三阶段迁移学习策略：在大规模城市图上预训练、冷启动适应和强化学习微调，以确保跨城市背景的稳健泛化。&lt;h4&gt;主要发现&lt;/h4&gt;在加利福尼亚三个都市区的超过1.03亿条清理过的GPS记录上评估，UrbanPulse达到了最先进的准确性和可扩展性，通过高效的迁移学习，为高分辨率、AI驱动的城市预测在实际中可在不同城市部署提供了可能。&lt;h4&gt;结论&lt;/h4&gt;UrbanPulse通过创新的架构和迁移学习策略，解决了现有人口流动预测方法的局限性，为实际应用中部署高分辨率、AI驱动的城市预测系统迈出了关键一步。&lt;h4&gt;翻译&lt;/h4&gt;准确的人口流动预测对城市规划、交通管理和公共卫生至关重要。然而，现有方法面临关键局限性：传统模型依赖静态空间假设，深度学习模型难以跨城市泛化，大型语言模型计算成本高且无法捕捉空间结构。此外，许多方法通过聚类兴趣点(POI)或限制覆盖范围来牺牲分辨率，限制了其在全市分析中的实用性。我们提出了UrbanPulse，一个可扩展的深度学习框架，通过将每个POI视为单独节点，提供超细粒度、全市范围的OD流动预测。它结合时间图卷积编码器和基于Transformer的解码器来建模多尺度时空依赖关系。为确保跨城市背景的稳健泛化，UrbanPulse采用三阶段迁移学习策略：在大规模城市图上预训练、冷启动适应和强化学习微调。在加利福尼亚三个都市区的超过1.03亿条清理过的GPS记录上评估，UrbanPulse达到了最先进的准确性和可扩展性。通过高效的迁移学习，UrbanPulse向实现高分辨率、AI驱动的城市预测在实际中可在不同城市部署迈出了关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate population flow prediction is essential for urban planning,transportation management, and public health. Yet existing methods face keylimitations: traditional models rely on static spatial assumptions, deeplearning models struggle with cross-city generalization, and Large LanguageModels (LLMs) incur high computational costs while failing to capture spatialstructure. Moreover, many approaches sacrifice resolution by clustering Pointsof Interest (POIs) or restricting coverage to subregions, limiting theirutility for city-wide analytics. We introduce UrbanPulse, a scalable deeplearning framework that delivers ultra-fine-grained, city-wide OD flowpredictions by treating each POI as an individual node. It combines a temporalgraph convolutional encoder with a transformer-based decoder to modelmulti-scale spatiotemporal dependencies. To ensure robust generalization acrossurban contexts, UrbanPulse employs a three-stage transfer learning strategy:pretraining on large-scale urban graphs, cold-start adaptation, andreinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPSrecords from three metropolitan areas in California, UrbanPulse achievesstate-of-the-art accuracy and scalability. Through efficient transfer learning,UrbanPulse takes a key step toward making high-resolution, AI-powered urbanforecasting deployable in practice across diverse cities.</description>
      <author>example@mail.com (Hongrong Yang, Markus Schlaepfer)</author>
      <guid isPermaLink="false">2507.17924v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>A Sequential Unsupervised Learning Approach for Large, Multicolor, Photometric Surveys</title>
      <link>http://arxiv.org/abs/2507.17882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用长短期记忆自编码器（LSTM-AE）分析多波段测光的新方法，能够有效重建恒星的光谱能量分布（SEDs），提高测光数据质量，并可用于检测稀有恒星类型。&lt;h4&gt;背景&lt;/h4&gt;观测天文学由大规模调查（如Pan-STARRS、SDSS和Gaia任务）推动，这些项目产生大量复杂数据集，对传统分析方法构成挑战，促使应用多种机器学习技术。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多波段测光分析方法，利用长短期记忆自编码器将恒星的光谱能量分布模式编码到二维潜在空间中，实现SED重建和数据去噪。&lt;h4&gt;方法&lt;/h4&gt;使用长短期记忆自编码器（LSTM-AE）模型，将多色测光的平均值视为波长序列，利用Pan-STARRS的grizy平均星等数据，并结合球状星团、SIMBAD标签、Gaia DR3视差和PanSTARRS图像进行辅助分析。&lt;h4&gt;主要发现&lt;/h4&gt;对于北银冠环状区域中的3,112,259颗恒星，99.51%的恒星完整SED形状被重建，每个波段中观测星等和模型预测星等之间的绝对差异小于0.05星等；模型可能对测光数据进行去噪，提高测量质量；通过分析重建效果不佳的测光数据，可以检测稀有恒星类型。&lt;h4&gt;结论&lt;/h4&gt;长短期记忆自编码器为分析多波段测光数据提供了有效工具，能够重建恒星光谱能量分布，提高数据质量，并在恒星类型检测方面具有应用价值。&lt;h4&gt;翻译&lt;/h4&gt;观测天文学在由大规模调查（如Pan-STARRS调查、斯隆数字巡天（SDSS）和Gaia任务）的推动下经历了重大转变。这些项目产生大量复杂数据集，对传统分析方法构成重大挑战，因此许多不同的机器学习技术正在被测试和应用。我们介绍了一种使用长短期记忆自编码器（LSTM-AE）分析多波段测光的新方法。该模型将多色测光的平均值视为波长序列，以便将恒星光谱能量分布（SEDs）中存在的模式编码到二维潜在空间中。我们使用Pan-STARRS的grizy平均星等展示了这一点，并使用球状星团、SIMBAD标签、Gaia DR3视差和PanSTARRS图像来辅助我们对潜在空间的分析和理解。对于北银冠环状区域中的3,112,259颗恒星，99.51%的恒星完整SED形状被重建——即每个波段中观测星等和模型预测星等之间的绝对差异——小于0.05星等。我们表明该模型可能对测光数据进行去噪，从而可能提高测量质量。最后，我们表明通过分析重建效果不佳的测光数据，可以检测稀有恒星类型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Observational astronomy has undergone a significant transformation driven bylarge-scale surveys, such as the Panoramic Survey Telescope and Rapid ResponseSystem (Pan-STARRS) Survey, the Sloan Digital Sky Survey (SDSS), and the GaiaMission. These programs yield large, complex datasets that pose significantchallenges for conventional analysis methods, and as a result, many differentmachine learning techniques are being tested and deployed. We introduce a newapproach to analyzing multiband photometry by using a long-short term memoryautoencoder (LSTM-AE). This model views mean measurements of multicolorphotometry as a sequence in wavelength in order to encode patterns present inthe stars' spectral energy distributions (SEDs) into a two-dimensional latentspace. We showcase this by using Pan-STARRS grizy mean magnitudes, and we useglobular clusters, labels from SIMBAD, Gaia DR3 parallaxes, and PanSTARRSimages to aid our analysis and understanding of the latent space. For 3,112,259stars in an annulus around the North Galactic Cap, 99.51% have their full SEDshape reconstructed--that is the absolute difference between the observed andthe model predicted magnitude in every band--within five hundredths of amagnitude. We show that the model likely denoises photometric data, potentiallyimproving the quality of measurements. Lastly, we show that the detection ofrare stellar types can be performed by analyzing poorly reconstructedphotometry.</description>
      <author>example@mail.com (Bradley D. Hutchinson, Catherine A. Pilachowski, Christian I. Johnson)</author>
      <guid isPermaLink="false">2507.17882v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic</title>
      <link>http://arxiv.org/abs/2507.17876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了'分子任务算术'方法，通过训练模型学习'属性方向'，然后向相反方向移动模型以生成具有理想属性的分子。该方法在20个零样本设计实验中表现出色，能够生成更多样化和成功的分子设计，并且在双目标和少样本设计任务中也能保持设计多样性并维持理想属性。&lt;h4&gt;背景&lt;/h4&gt;具有理想属性（即'正面'分子）的分子稀缺性是生成式分子设计的固有瓶颈。传统方法需要大量正面标记的数据，但这些数据往往难以获取。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需访问任何正面标记数据的方法，解决分子生成设计中理想分子稀缺的问题，提高生成分子的多样性和成功率。&lt;h4&gt;方法&lt;/h4&gt;分子任务算术方法，通过在多样化和丰富的负面样本上训练模型来学习'属性方向'，然后向相反的属性方向移动模型以生成正面分子。&lt;h4&gt;主要发现&lt;/h4&gt;在20个零样本设计实验中，分子任务算术比在正分子上训练的模型生成了更多样化和成功的设计。在双目标和少样本设计任务中，分子任务算术能够持续增加设计的多样性，同时保持理想的属性。&lt;h4&gt;结论&lt;/h4&gt;分子任务算术因其简单性、数据效率和性能表现，有潜力成为从头分子设计的默认迁移学习策略。&lt;h4&gt;翻译&lt;/h4&gt;具有理想属性的分子（即'正面'分子）的稀缺性是生成式分子设计的固有瓶颈。为规避这一障碍，我们提出了分子任务算术：在多样化和丰富的负面样本上训练模型以学习'属性方向'——无需访问任何正面标记的数据——并将模型向相反的属性方向移动以生成正面分子。在20个零样本设计实验中分析显示，分子任务算术比在正分子上训练的模型生成了更多样化和成功的设计。此外，我们在双目标和少样本设计任务中应用了分子任务算术。我们发现分子任务算术能够持续增加设计的多样性，同时保持理想的属性。凭借其简单性、数据效率和性能，分子任务算术有潜力成为从头分子设计的默认迁移学习策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of molecules with desirable properties (i.e., 'positive'molecules) is an inherent bottleneck for generative molecule design. Tosidestep such obstacle, here we propose molecular task arithmetic: training amodel on diverse and abundant negative examples to learn 'property directions'$--$ without accessing any positively labeled data $--$ and moving models inthe opposite property directions to generate positive molecules. When analyzedon 20 zero-shot design experiments, molecular task arithmetic generated morediverse and successful designs than models trained on positive molecules.Moreover, we employed molecular task arithmetic in dual-objective and few-shotdesign tasks. We find that molecular task arithmetic can consistently increasethe diversity of designs while maintaining desirable design properties. Withits simplicity, data efficiency, and performance, molecular task arithmeticbears the potential to become the $\textit{de-facto}$ transfer learningstrategy for de novo molecule design.</description>
      <author>example@mail.com (Rıza Özçelik, Sarah de Ruiter, Francesca Grisoni)</author>
      <guid isPermaLink="false">2507.17876v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains</title>
      <link>http://arxiv.org/abs/2507.17792v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了公共和个体因果机制估计(CICME)，一种新颖的三步方法，用于从多领域收集的异构数据中推断因果机制。&lt;h4&gt;背景&lt;/h4&gt;研究旨在通过因果关系视角深入理解复杂传感器系统，涉及从多个领域收集的异构数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从多领域异构数据中推断因果机制的新方法。&lt;h4&gt;方法&lt;/h4&gt;CICME利用因果迁移学习(CTL)原则，在有足够样本的情况下可靠地检测领域不变的因果机制，然后使用识别出的公共因果机制指导估计每个领域中剩余的因果机制。&lt;h4&gt;主要发现&lt;/h4&gt;CICME在线性高斯模型上的评估表现良好，它结合了在汇集数据和单个领域数据上应用因果发现的优势，在某些情况下优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;CICME是分析复杂传感器系统的有效方法，能够从多领域异构数据中成功推断因果机制。&lt;h4&gt;翻译&lt;/h4&gt;为了通过因果关系的视角更深入地理解复杂传感器系统，我们提出了公共和个体因果机制估计(CICME)，一种新颖的三步方法，用于从多个领域收集的异构数据中推断因果机制。通过利用因果迁移学习(CTL)原则，CICME在有足够样本的情况下能够可靠地检测领域不变的因果机制。识别出的公共因果机制进一步被用来指导估计每个领域中剩余的因果机制。CICME的性能在线性高斯模型上进行了评估，评估场景受到制造过程的启发。基于现有的基于连续优化的因果发现方法，我们表明CICME利用了在汇集数据上应用因果发现以及在单个领域数据上重复应用因果发现的好处，在某些情况下甚至优于两种基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To gain deeper insights into a complex sensor system through the lens ofcausality, we present common and individual causal mechanism estimation(CICME), a novel three-step approach to inferring causal mechanisms fromheterogeneous data collected across multiple domains. By leveraging theprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detectdomain-invariant causal mechanisms when provided with sufficient samples. Theidentified common causal mechanisms are further used to guide the estimation ofthe remaining causal mechanisms in each domain individually. The performance ofCICME is evaluated on linear Gaussian models under scenarios inspired from amanufacturing process. Building upon existing continuous optimization-basedcausal discovery methods, we show that CICME leverages the benefits of applyingcausal discovery on the pooled data and repeatedly on data from individualdomains, and it even outperforms both baseline methods under certain scenarios.</description>
      <author>example@mail.com (Jingyi Yu, Tim Pychynski, Marco F. Huber)</author>
      <guid isPermaLink="false">2507.17792v2</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>CM-UNet: A Self-Supervised Learning-Based Model for Coronary Artery Segmentation in X-Ray Angiography</title>
      <link>http://arxiv.org/abs/2507.17779v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE EMBC 2025, 7 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CM-UNet的新型冠状动脉分割方法，通过自监督预训练和迁移学习，显著减少了对大量标注数据的依赖，提高了分割准确性。&lt;h4&gt;背景&lt;/h4&gt;冠状动脉的准确分割在临床实践中仍面临重大挑战，阻碍了冠状动脉疾病的有效诊断和管理。缺乏大型标注数据集用于模型训练加剧了这一问题，限制了能够协助放射科医生的自动化工具的发展。&lt;h4&gt;目的&lt;/h4&gt;解决缺乏大型标注数据集的问题，引入一种能够减少大量手动标注需求的方法，提高冠状动脉分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CM-UNet模型，利用无标注数据集进行自监督预训练，并在有限的标注数据上进行迁移学习，实现在减少大量手动标注的同时进行准确的疾病检测。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用18张标注图像对CM-UNet进行微调，Dice分数仅下降15.2%，而基线模型在相同情况下下降了46.5%，这表明自监督学习可以提高分割性能并减少对大型数据集的依赖。&lt;h4&gt;结论&lt;/h4&gt;这是首批强调自监督学习在改善X射线血管造影冠状动脉分割中重要性的研究之一。通过提高分割准确性，该方法旨在改善临床工作流程，减少放射科医生工作量，加速疾病检测，最终改善患者预后。&lt;h4&gt;翻译&lt;/h4&gt;冠状动脉的准确分割在临床实践中仍然是一个重大挑战，阻碍了冠状动脉疾病的有效诊断和管理。缺乏用于模型训练的大型标注数据集加剧了这一问题，限制了能够协助放射科医生的自动化工具的开发。为此，我们引入了CM-UNet，它利用无标注数据集进行自监督预训练，并在有限的标注数据上进行迁移学习，实现在减少大量手动标注的同时进行准确的疾病检测。仅用18张标注图像而非500张对CM-UNet进行微调，Dice分数下降了15.2%，而基线模型在没有预训练的情况下下降了46.5%。这表明自监督学习可以提高分割性能并减少对大型数据集的依赖。这是首批强调自监督学习在改善X射线血管造影冠状动脉分割中重要性的研究之一，对提高临床实践中的诊断准确性具有潜在影响。通过提高X射线血管造影图像的分割准确性，所提出的方法旨在改善临床工作流程，减少放射科医生的工作量，并加速疾病检测，最终有助于改善患者预后。源代码可在https://github.com/CamilleChallier/Contrastive-Masked-UNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of coronary arteries remains a significant challenge inclinical practice, hindering the ability to effectively diagnose and managecoronary artery disease. The lack of large, annotated datasets for modeltraining exacerbates this issue, limiting the development of automated toolsthat could assist radiologists. To address this, we introduce CM-UNet, whichleverages self-supervised pre-training on unannotated datasets and transferlearning on limited annotated data, enabling accurate disease detection whileminimizing the need for extensive manual annotations. Fine-tuning CM-UNet withonly 18 annotated images instead of 500 resulted in a 15.2% decrease in Dicescore, compared to a 46.5% drop in baseline models without pre-training. Thisdemonstrates that self-supervised learning can enhance segmentation performanceand reduce dependence on large datasets. This is one of the first studies tohighlight the importance of self-supervised learning in improving coronaryartery segmentation from X-ray angiography, with potential implications foradvancing diagnostic accuracy in clinical practice. By enhancing segmentationaccuracy in X-ray angiography images, the proposed approach aims to improveclinical workflows, reduce radiologists' workload, and accelerate diseasedetection, ultimately contributing to better patient outcomes. The source codeis publicly available athttps://github.com/CamilleChallier/Contrastive-Masked-UNet.</description>
      <author>example@mail.com (Camille Challier, Xiaowu Sun, Thabo Mahendiran, Ortal Senouf, Bernard De Bruyne, Denise Auberson, Olivier Müller, Stephane Fournier, Pascal Frossard, Emmanuel Abbé, Dorina Thanou)</author>
      <guid isPermaLink="false">2507.17779v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2507.19856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RaGS是一种创新的3D物体检测框架，首次利用3D高斯溅射技术融合4D雷达和单目图像，解决了现有方法在场景理解和网格结构限制上的问题。&lt;h4&gt;背景&lt;/h4&gt;4D毫米波雷达作为自动驾驶的有前景传感器，但有效的3D物体检测仍面临挑战，现有融合方法要么缺乏整体场景理解，要么受限于刚性网格结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，能够有效融合4D雷达和单目图像进行3D物体检测，同时克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;RaGS使用3D高斯溅射作为表示方法，通过级联管道构建和优化高斯场：基于视锥体的定位初始化(FLI)初始化3D高斯位置，迭代多模态聚合(IMA)融合语义和几何信息，多级高斯融合(MGF)将高斯渲染成多级BEV特征用于3D物体检测。&lt;h4&gt;主要发现&lt;/h4&gt;通过动态关注场景内的稀疏物体，RaGS能够在实现物体集中的同时提供全面的场景感知，在多个基准测试上展示了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;RaGS成功解决了4D雷达和单目图像融合中的挑战，为自动驾驶中的3D物体检测提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;4D毫米波雷达已成为自动驾驶的一种有前景的传感器，但从4D雷达和单目图像中进行有效的3D物体检测仍然是一个挑战。现有的融合方法通常依赖于基于实例的提案或密集的BEV网格，这些方法要么缺乏整体场景理解，要么受到刚性网格结构的限制。为此，我们提出了RaGS，这是第一个利用3D高斯溅射(GS)作为表示来融合4D雷达和单目线索以进行3D物体检测的框架。3D GS通过将场景建模为高斯场，自然适合3D物体检测，动态分配资源给前景物体，提供灵活、资源高效的解决方案。RaGS使用级联管道来构建和优化高斯场。它首先从基于视锥体的定位初始化(FLI)开始，该步骤将前景像素反投影以初始化粗略的3D高斯位置。然后，迭代多模态聚合(IMA)融合语义和几何信息，将有限的高斯优化到感兴趣区域。最后，多级高斯融合(MGF)将高斯渲染成多级BEV特征用于3D物体检测。通过动态关注场景内的稀疏物体，RaGS能够在实现物体集中的同时提供全面的场景感知。在View-of-Delft、TJ4DRadSet和OmniHD-Scenes基准上的大量实验展示了其最先进的性能。代码将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效融合4D毫米波雷达和单目图像进行3D物体检测的问题。这个问题在自动驾驶领域至关重要，因为自动驾驶需要准确的3D感知来确保安全决策。4D雷达具有在恶劣环境中的鲁棒性和远距离检测能力，而相机提供高分辨率语义信息，两者互补但融合困难。现有方法要么缺乏全局场景理解，要么受限于刚性网格结构，无法高效处理本质上稀疏的3D检测任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：实例级方法专注于稀疏物体但缺乏全局场景理解，BEV方法提供全局推理但受限于固定网格和效率低下。作者注意到3D高斯溅射(3D GS)作为一种紧凑、连续场景表示的潜力，它提供物理可解释性、稀疏性和灵活性。作者借鉴了实例级方法对稀疏物体的关注，BEV方法的全局推理能力，以及GS技术的灵活场景表示，设计了RaGS框架，通过动态高斯分配资源，兼顾物体关注和全局感知。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D高斯溅射作为融合4D雷达和单目视觉线索的表示方法，将场景建模为连续的高斯场，动态分配资源给稀疏前景物体，同时保持全局场景感知。整体流程包含三个主要模块：1)基于视锥的定位初始化(FLI)：通过前景像素和雷达点初始化3D高斯位置；2)迭代多模态聚合(IMA)：使用3D可变形交叉注意力和稀疏卷积聚合雷达和图像特征，迭代更新高斯位置；3)多级高斯融合(MGF)：将高斯渲染为多级BEV特征并进行融合，最终用于3D物体检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次将3D高斯溅射应用于4D雷达和相机融合的3D物体检测；2)设计了FLI、IMA和MGF三个模块构建和优化高斯表示；3)实现动态资源分配，专注于稀疏物体同时保持全局感知。相比不同：与实例级方法相比，RaGS提供了更好的全局场景理解；与BEV方法相比，避免了固定网格限制和过度背景聚合导致的效率低下；与其他GS工作相比，专门针对雷达-相机融合的3D检测任务进行了优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RaGS首次利用3D高斯溅射作为融合框架，通过动态分配资源和迭代更新高斯位置，有效结合了4D雷达和单目视觉的优势，实现了对稀疏前景物体的精准检测同时保持全局场景感知，显著提升了3D物体检测的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D millimeter-wave radar has emerged as a promising sensor for autonomousdriving, but effective 3D object detection from both 4D radar and monocularimages remains a challenge. Existing fusion approaches typically rely on eitherinstance-based proposals or dense BEV grids, which either lack holistic sceneunderstanding or are limited by rigid grid structures. To address these, wepropose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) asrepresentation for fusing 4D radar and monocular cues in 3D object detection.3D GS naturally suits 3D object detection by modeling the scene as a field ofGaussians, dynamically allocating resources on foreground objects and providinga flexible, resource-efficient solution. RaGS uses a cascaded pipeline toconstruct and refine the Gaussian field. It starts with the Frustum-basedLocalization Initiation (FLI), which unprojects foreground pixels to initializecoarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA)fuses semantics and geometry, refining the limited Gaussians to the regions ofinterest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussiansinto multi-level BEV features for 3D object detection. By dynamically focusingon sparse objects within scenes, RaGS enable object concentrating whileoffering comprehensive scene perception. Extensive experiments onView-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate itsstate-of-the-art performance. Code will be released.</description>
      <author>example@mail.com (Xiaokai Bai, Chenxu Zhou, Lianqing Zheng, Si-Yuan Cao, Jianan Liu, Xiaohan Zhang, Zhengzhuang Zhang, Hui-liang Shen)</author>
      <guid isPermaLink="false">2507.19856v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes</title>
      <link>http://arxiv.org/abs/2507.19304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by IEEE/RSJ IROS 2025 for oral  presentation on 19 Oct. 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为多流检测（MuStD）的网络，用于融合LiDAR和RGB数据以提高户外3D目标检测的准确性。该方法通过三流结构从两种数据模态中提取任务相关信息，在KITTI基准测试上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;融合LiDAR和RGB数据有潜力提高户外3D目标检测的准确性。针对户外3D目标检测中的实际挑战，LiDAR和RGB输入的融合开始受到关注。然而，有效整合这些模态以实现精确的目标检测任务仍然是一个 largely 未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决LiDAR和RGB数据有效整合的挑战，作者提出了一个多流检测网络，该网络能够从两种数据模态中精心提取与任务相关的信息，从而提高户外3D目标检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;该网络采用三流结构：1) LiDAR-PillarNet流从LiDAR输入中提取稀疏的2D柱状特征；2) LiDAR-高度压缩流计算鸟瞰图特征；3) 3D多模态流使用UV映射和极坐标索引结合RGB和LiDAR特征。最终，包含全面空间、纹理和几何信息的特征被仔细融合并输入到检测头进行3D目标检测。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的KITTI目标检测基准上进行的广泛评估表明，该方法在不同类别中实现了新的最先进或高度竞争性的结果，同时保持了最高效的方法之一。&lt;h4&gt;结论&lt;/h4&gt;作者提出的多流检测网络（MuStD）有效地融合了LiDAR和RGB数据，提高了户外3D目标检测的准确性，并在KITTI基准测试上取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR和RGB数据的融合有潜力提高户外3D目标检测的准确性。为了应对户外3D目标检测中的实际挑战，LiDAR和RGB输入的融合开始受到关注。然而，有效整合这些模态以实现精确的目标检测任务仍然是一个 largely 未解决的问题。为此，我们提出了一个多流检测（MuStD）网络，该网络精心地从两种数据模态中提取与任务相关的信息。该网络遵循三流结构。其LiDAR-PillarNet流从LiDAR输入中提取稀疏的2D柱状特征，而LiDAR-高度压缩流计算鸟瞰图特征。额外的3D多模态流使用UV映射和极坐标索引结合RGB和LiDAR特征。最终，包含全面空间、纹理和几何信息的特征被仔细融合并输入到检测头进行3D目标检测。我们在具有挑战性的KITTI目标检测基准上的广泛评估，通过在不同类别中实现新的最先进或高度竞争性的结果，同时保持最高效的方法之一，证明了我们方法的有效性。我们的代码将通过MuStD GitHub仓库发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何有效融合LiDAR和RGB相机数据以提高户外3D物体检测精度的问题。这个问题在现实和研究中很重要，因为准确的户外3D物体检测是可靠自主导航的基础，对自动驾驶系统至关重要。单独使用LiDAR存在采样稀疏和遮挡导致测量不完整的问题，而RGB相机能提供高分辨率纹理信息，两者结合可以互补不足，提高检测性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，指出早期方法通常在感兴趣区域或鸟瞰图进行后期融合，而近期方法虽使用深度完成丰富LiDAR数据但仍存在不足。作者注意到当前融合方法多采用孤立策略（如UV映射或极坐标变换），无法有效结合LiDAR的3D空间信息和RGB的语义信息。基于这些分析，作者设计了MuStD多流网络，借鉴了PointPillars等将点云转换为2D伪图像的技术，以及深度完成生成伪点云的方法，但创新性地将它们整合到一个统一的三流架构中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过三个并行的数据流处理不同模态的数据，然后融合这些流中的特征，综合利用LiDAR的几何信息和RGB的纹理信息。特别是在3D多模态流中，创新性地结合UV映射和极坐标索引，同时捕获纹理细节和空间几何信息。整体流程包括：1) LiDAR-PillarNet流将LiDAR转换为2D柱状特征；2) LiDAR-高度压缩流提取3D特征并转换为鸟瞰图；3) 3D多模态流处理RGB和LiDAR数据，利用UV映射和极坐标变换提取特征；4) 最后融合三个流的特征，通过检测头输出检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) MuStD多流检测网络，包含三个并行流结合LiDAR和RGB数据；2) 3D多模态流，创新性地结合UV映射和极坐标索引；3) UV-Polar块，将3D稀疏卷积特征投影到UV图像和极坐标空间。相比之前工作，不同之处在于：之前的融合方法多采用孤立策略，无法有效结合LiDAR的3D空间信息和RGB的语义信息；而本文方法通过三流架构和UV-Polar块，能同时捕获纹理和几何信息，在处理遮挡或远距离物体时表现更佳，且在保持高效的同时取得了最先进的结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了MuStD多流检测网络，通过创新的UV-Polar块和三流架构，有效融合LiDAR和RGB相机数据，显著提高了户外场景中的3D物体检测精度和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D objectdetection accuracy. To address real-world challenges in outdoor 3D objectdetection, fusion of LiDAR and RGB input has started gaining traction. However,effective integration of these modalities for precise object detection taskstill remains a largely open problem. To address that, we propose a MultiStreamDetection (MuStD) network, that meticulously extracts task-relevant informationfrom both data modalities. The network follows a three-stream structure. ItsLiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR inputwhile the LiDAR-Height Compression stream computes Bird's-Eye View features. Anadditional 3D Multimodal stream combines RGB and LiDAR features using UVmapping and polar coordinate indexing. Eventually, the features containingcomprehensive spatial, textural and geometric information are carefully fusedand fed to a detection head for 3D object detection. Our extensive evaluationon the challenging KITTI Object Detection Benchmark using public testing serverathttps://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&amp;result=d162ec699d6992040e34314d19ab7f5c217075e0establishes the efficacy of our method by achieving new state-of-the-art orhighly competitive results in different categories while remaining among themost efficient methods. Our code will be released through MuStD GitHubrepository at https://github.com/IbrahimUWA/MuStD.git</description>
      <author>example@mail.com (Muhammad Ibrahim, Naveed Akhtar, Haitian Wang, Saeed Anwar, Ajmal Mian)</author>
      <guid isPermaLink="false">2507.19304v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols</title>
      <link>http://arxiv.org/abs/2507.18457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种与设备无关的标准化框架，用于物理对抗物体攻击，解决了物理攻击可重复性差的问题，并通过模拟到物理的攻击转移验证了框架的有效性。&lt;h4&gt;背景&lt;/h4&gt;基于激光雷达的3D目标检测对抗鲁棒性是重要研究领域，但数字攻击缺乏物理可实现性，物理对抗物体攻击探索不足且可重复性差。&lt;h4&gt;目的&lt;/h4&gt;提出一个设备无关、标准化的框架，抽象物理对抗物体攻击的关键元素，支持多种方法，并提供开源代码和基准测试协议。&lt;h4&gt;方法&lt;/h4&gt;开发抽象物理对抗物体攻击关键元素的标准化框架，支持多种方法，提供模拟和现实世界环境中的开源代码和基准测试协议。&lt;h4&gt;主要发现&lt;/h4&gt;框架实现了公平比较和研究加速，模拟攻击成功转移到物理激光雷达系统，揭示了影响攻击成功因素，推进了对激光雷达感知对抗鲁棒性的理解。&lt;h4&gt;结论&lt;/h4&gt;标准化框架促进了物理对抗物体攻击的研究，通过公平比较加速了该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;基于激光雷达的3D目标检测对抗鲁棒性是一个重要的研究领域，因为其在现实世界场景中有广泛应用。虽然许多数字攻击操纵点云或网格，但它们通常缺乏物理可实现性，限制了其实际影响。物理对抗物体攻击仍未得到充分探索，并且由于设置不一致和硬件差异而存在可重复性差的问题。为解决这一问题，我们提出了一种与设备无关的标准化框架，该框架抽象了物理对抗物体攻击的关键元素，支持多种方法，并提供模拟和现实世界环境中的开源代码和基准测试协议。我们的框架实现了公平比较，加速了研究，并通过成功将模拟攻击转移到物理激光雷达系统得到验证。除了框架本身，我们还提供了影响攻击成功因素的见解，并推进了对现实世界激光雷达感知中对抗鲁棒性的理解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决物理可实现的对抗对象攻击在LiDAR-based 3D object detection研究中存在的可重复性差、缺乏标准化框架的问题。这个问题很重要，因为LiDAR系统广泛应用于自动驾驶、配送机器人等安全关键领域，对抗鲁棒性对确保AI系统安全至关重要，而现有数字攻击往往缺乏物理可实现性，限制了实际影响，且物理对抗对象攻击研究不足导致难以评估真实世界鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有研究中的不足：物理攻击研究不足、可重复性差、缺乏标准化框架。他们将物理攻击分为spoofing attacks和adversarial objects两类，认为对抗对象攻击构成严重威胁但研究有限。借鉴了PhyAdv、Abdelfattah等人的工作，他们提出一个设备无关的标准化框架，通过调整网格形状、损失约束和对象放置等元素，抽象物理对抗对象攻击的关键组件，并提供开源代码和评估协议。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个设备无关的标准化框架，抽象物理对抗对象攻击的关键组件，支持多种方法并提供开源代码和评估协议。整体流程包括：1)问题形式化，定义对抗网格；2)物理可行约束，通过重新参数化或纳入损失函数实现；3)设计对抗misdetection损失，分为ML、MR和C三类；4)使用梯度下降优化算法；5)评估攻击成功率和不可见性；6)在数据集、模拟和现实环境三个级别进行实验验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出设备无关的标准化框架；2)提供开源代码和基准测试协议；3)验证模拟到现实的攻击转移能力；4)分析影响攻击成功的关键因素；5)提供现实世界LiDAR感知对抗鲁棒性见解。相比之前工作，不同之处在于：1)提供统一评估协议使结果可比；2)全面评估攻击成功率和不可见性；3)分析多种损失函数在不同模型架构上的效果；4)验证跨场景转移能力；5)覆盖更广泛的检测模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出并验证了一个设备无关的标准化框架，用于生成和评估物理可实现的对抗对象攻击，显著提高了LiDAR-based 3D object detection对抗研究的可重复性和可比性，并成功将模拟攻击转移到现实世界。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adversarial robustness in LiDAR-based 3D object detection is a criticalresearch area due to its widespread application in real-world scenarios. Whilemany digital attacks manipulate point clouds or meshes, they often lackphysical realizability, limiting their practical impact. Physical adversarialobject attacks remain underexplored and suffer from poor reproducibility due toinconsistent setups and hardware differences. To address this, we propose adevice-agnostic, standardized framework that abstracts key elements of physicaladversarial object attacks, supports diverse methods, and provides open-sourcecode with benchmarking protocols in simulation and real-world settings. Ourframework enables fair comparison, accelerates research, and is validated bysuccessfully transferring simulated attacks to a physical LiDAR system. Beyondthe framework, we offer insights into factors influencing attack success andadvance understanding of adversarial robustness in real-world LiDAR perception.</description>
      <author>example@mail.com (Luo Cheng, Hanwei Zhang, Lijun Zhang, Holger Hermanns)</author>
      <guid isPermaLink="false">2507.18457v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction</title>
      <link>http://arxiv.org/abs/2507.18331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SGCDet是一种基于自适应3D体积构建的多视角室内3D物体检测框架，通过几何和上下文感知的聚合模块以及稀疏体积构建策略，实现了有效且高效的体积构建，并在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;传统的3D物体检测方法限制体素感受野固定在图像位置上，缺乏对几何和上下文信息的有效整合，且存在冗余计算问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自适应构建3D体积的多视角室内3D物体检测框架，提高检测性能并减少计算冗余。&lt;h4&gt;方法&lt;/h4&gt;提出几何和上下文感知的聚合模块，整合自适应区域内的几何和上下文信息，动态调整不同视角的贡献；设计稀疏体积构建策略，自适应选择高占用概率的体素进行特征细化；仅使用3D边界框进行监督，无需真实场景几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;自适应体积构建方法可以有效提高3D物体检测性能；几何和上下文信息的整合增强了体素特征的表示能力；稀疏体积构建策略减少了自由空间中的冗余计算；仅使用3D边界框进行监督的方法是可行的。&lt;h4&gt;结论&lt;/h4&gt;SGCDet框架通过自适应3D体积构建策略，在室内3D物体检测任务中实现了有效且高效的性能，并在多个数据集上达到了最先进的水平。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了SGCDet，一种基于自适应3D体积构建的新型多视角室内3D物体检测框架。与之前限制体素感受野固定在图像位置的方法不同，我们引入了一个几何和上下文感知的聚合模块，用于整合每个图像自适应区域内的几何和上下文信息，并动态调整不同视角的贡献，增强体素特征的表示能力。此外，我们提出了一种稀疏体积构建策略，自适应识别和选择具有高占用概率的体素进行特征细化，最小化自由空间中的冗余计算。得益于上述设计，我们的框架能够以自适应方式实现有效且高效的体积构建。更好的是，我们的网络仅使用3D边界框就可以进行监督，消除对真实场景几何的依赖。实验结果表明，SGCDet在ScanNet、ScanNet200和ARKitScenes数据集上达到了最先进的性能。源代码可在https://github.com/RM-Zhang/SGCDet获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多视角室内3D物体检测中的效率和精度问题。现有方法要么将体素感受野限制在图像固定位置，忽略了上下文信息；要么构建密集3D体积，不考虑场景稀疏性导致计算效率低下。这个问题很重要，因为室内3D物体检测是机器人、AR/VR和具身AI等应用的基础感知任务，但现有方法要么依赖昂贵3D传感器，要么在精度和效率间难以平衡。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法如ImVoxelNet、ImGeoNet等的局限性，发现它们在特征提取和计算效率上的不足。作者借鉴了DFA3D的3D可变形注意力机制，将其改进以适应室内场景；参考了DETR的稀疏设计思想；还利用了多视图立体视觉技术。在此基础上，作者设计了自适应特征提升和稀疏体积构建策略，解决了现有方法的缺陷。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何和上下文感知聚合模块在每个图像的自适应区域内集成几何和上下文信息，并动态调整不同视图的贡献；同时采用稀疏体积构建策略，只对可能包含物体的体素进行特征细化。整体流程是：输入多视角图像→提取2D特征→估计深度分布→构建粗略3D体积→通过粗到细细化过程→在每个阶段预测体素占用概率→选择高概率体素进行特征细化→使用检测头预测3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)几何和上下文感知聚合模块，在自适应区域内而非固定位置采样特征；2)稀疏体积构建策略，只细化可能包含物体的体素；3)使用3D边界框生成占用伪标签，无需真实场景几何监督。相比之前工作，它解决了特征感受野受限、计算效率低、依赖精确几何信息等问题，在提高精度的同时显著降低了计算成本。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SGCDet通过自适应特征聚合和稀疏体积构建，实现了高效精确的多视角室内3D物体检测，无需真实场景几何信息即可达到最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents SGCDet, a novel multi-view indoor 3D object detectionframework based on adaptive 3D volume construction. Unlike previous approachesthat restrict the receptive field of voxels to fixed locations on images, weintroduce a geometry and context aware aggregation module to integrategeometric and contextual information within adaptive regions in each image anddynamically adjust the contributions from different views, enhancing therepresentation capability of voxel features. Furthermore, we propose a sparsevolume construction strategy that adaptively identifies and selects voxels withhigh occupancy probabilities for feature refinement, minimizing redundantcomputation in free space. Benefiting from the above designs, our frameworkachieves effective and efficient volume construction in an adaptive way. Betterstill, our network can be supervised using only 3D bounding boxes, eliminatingthe dependence on ground-truth scene geometry. Experimental results demonstratethat SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200and ARKitScenes datasets. The source code is available athttps://github.com/RM-Zhang/SGCDet.</description>
      <author>example@mail.com (Runmin Zhang, Zhu Yu, Si-Yuan Cao, Lingyu Zhu, Guangyi Zhang, Xiaokai Bai, Hui-Liang Shen)</author>
      <guid isPermaLink="false">2507.18331v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>GTAD: Global Temporal Aggregation Denoising Learning for 3D Semantic Occupancy Prediction</title>
      <link>http://arxiv.org/abs/2507.20963v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为GTAD的全局时间聚合去噪网络，用于解决自动驾驶和机器人系统中动态环境感知的问题，通过有效利用全局时间信息提高环境理解的连贯性和全面性。&lt;h4&gt;背景&lt;/h4&gt;准确感知动态环境是自动驾驶和机器人系统的基本任务，但现有方法未能充分利用时间信息，主要依赖相邻帧之间的局部时间交互，而未能有效利用全局序列信息。&lt;h4&gt;目的&lt;/h4&gt;研究如何有效聚合时间序列中的全局时间特征，实现能够有效利用历史观测中全局时间信息的占用表示。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为GTAD的全局时间聚合去噪网络，引入了一个全局时间信息聚合框架作为整体3D场景理解的新范式。使用模型内潜在去噪网络聚合当前时刻的局部时间特征和历史序列的全局时间特征，从而有效感知相邻帧的细粒度时间信息和历史观测的全局时间模式。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够提供更连贯和全面的环境理解，在nuScenes和Occ3D-nuScenes基准测试上的大量实验消融研究证明了该方法的优越性。&lt;h4&gt;结论&lt;/h4&gt;GTAD方法在动态环境感知方面优于现有方法，能够有效利用全局时间信息提高环境理解的准确性和全面性。&lt;h4&gt;翻译&lt;/h4&gt;准确感知动态环境是自动驾驶和机器人系统的基本任务。现有方法未能充分利用时间信息，主要依赖相邻帧之间的局部时间交互，而未能有效利用全局序列信息。为解决这一限制，我们研究如何有效聚合时间序列中的全局时间特征，旨在实现能够有效利用历史观测中全局时间信息的占用表示。为此，我们提出了一种名为GTAD的全局时间聚合去噪网络，引入了一个全局时间信息聚合框架作为整体3D场景理解的新范式。我们的方法使用模型内潜在去噪网络聚合当前时刻的局部时间特征和历史序列的全局时间特征。这种方法能够有效感知相邻帧的细粒度时间信息和历史观测的全局时间模式。因此，它提供了对环境更连贯和全面的理解。在nuScenes和Occ3D-nuScenes基准测试上的大量实验和消融研究表明了我们方法的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有方法在3D语义占用预测中未能有效利用全局时间信息的问题。现有方法主要依赖相邻帧间的局部时间交互，无法充分利用整个时间序列的全局信息。这个问题在现实中非常重要，因为准确感知动态环境是自动驾驶和机器人系统的基本任务，全局时间信息对于理解物体运动模式和长期变化至关重要，可以提高系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出当前方法在时间信息利用上的局限性，然后受扩散模型在生成任务中能捕获数据细微细节的启发，将去噪学习应用于3D语义占用预测。考虑到直接应用扩散模型会严重影响计算效率，他们采用了模型内潜在去噪网络来解决这一问题。作者借鉴了BEVFormer的时间信息循环传播、PanoOcc的时间信息对齐、扩散模型的基本思想以及DeTrack的去噪网络设计，结合这些创新提出了GTAD框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D语义占用预测转化为去噪学习过程，通过添加噪声增强模型对全局时间信息的感知能力，并使用模型内潜在去噪网络高效聚合局部和全局时间特征。整体流程包括：1)接收多帧多视角图像输入；2)用图像骨干网络提取多尺度特征；3)通过空间编码器提取当前时刻的体素特征；4)用局部时间编码器聚合邻近帧的时间信息；5)通过全局时间编码器实现历史特征间的全局交互；6)将特征输入去噪网络处理；7)最后用占用解码器生成3D语义占用结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)模型内潜在去噪网络，能同时聚合局部和全局时间特征；2)基于注意力的全局时间信息交互机制，克服了历史特征孤立处理的问题；3)联合训练框架，将检测和分割统一为全景分割。相比之前的工作，GTAD能整合整个时间序列的全局特征而非仅限于局部交互，使用单次前向传播而非多次模型运行降低计算成本，同时结合体素特征的细节和BEV表示的全局视角，兼顾局部感知和全局理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GTAD通过创新的去噪学习框架有效整合了局部和全局时间信息，显著提升了3D语义占用预测在自动驾驶场景中的性能和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately perceiving dynamic environments is a fundamental task forautonomous driving and robotic systems. Existing methods inadequately utilizetemporal information, relying mainly on local temporal interactions betweenadjacent frames and failing to leverage global sequence informationeffectively. To address this limitation, we investigate how to effectivelyaggregate global temporal features from temporal sequences, aiming to achieveoccupancy representations that efficiently utilize global temporal informationfrom historical observations. For this purpose, we propose a global temporalaggregation denoising network named GTAD, introducing a global temporalinformation aggregation framework as a new paradigm for holistic 3D sceneunderstanding. Our method employs an in-model latent denoising network toaggregate local temporal features from the current moment and global temporalfeatures from historical sequences. This approach enables the effectiveperception of both fine-grained temporal information from adjacent frames andglobal temporal patterns from historical observations. As a result, it providesa more coherent and comprehensive understanding of the environment. Extensiveexperiments on the nuScenes and Occ3D-nuScenes benchmark and ablation studiesdemonstrate the superiority of our method.</description>
      <author>example@mail.com (Tianhao Li, Yang Li, Mengtian Li, Yisheng Deng, Weifeng Ge)</author>
      <guid isPermaLink="false">2507.20963v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Compositional Video Synthesis by Temporal Object-Centric Learning</title>
      <link>http://arxiv.org/abs/2507.20855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12+21 pages, submitted to IEEE Transactions on Pattern Analysis and  Machine Intelligence (TPAMI), currently under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新颖的组合视频合成框架，利用时间一致的对象中心表示，将之前的SlotAdapt从图像扩展到视频，实现高质量视频合成和对象级编辑功能。&lt;h4&gt;背景&lt;/h4&gt;现有的对象中心方法要么完全缺乏生成能力，要么将视频序列作为整体处理，忽略了显式的对象级结构，无法保持对象身份一致性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保持对象身份一致性，并支持对象插入、删除或替换等组合编辑功能的视频生成方法。&lt;h4&gt;方法&lt;/h4&gt;通过学习姿态不变的对象中心槽位，并将它们与预训练的扩散模型条件化，来明确地捕获时间动态，实现高质量视频合成。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在视频生成质量和时间一致性方面设定了新基准，优于之前的基于对象的生成方法，同时分割性能与最先进方法接近。&lt;h4&gt;结论&lt;/h4&gt;该方法显著推进了交互式和可控的视频生成，为高级内容创作、语义编辑和动态场景理解开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的组合视频合成框架，利用时间一致的对象中心表示，将我们之前的工作SlotAdapt从图像扩展到视频。虽然现有的对象中心方法要么完全缺乏生成能力，要么将视频序列作为整体处理，从而忽略了显式的对象级结构，但我们的方法通过学习姿态不变的对象中心槽位，并将它们与预训练的扩散模型条件化，明确地捕获时间动态。这种设计实现了高质量、像素级的视频合成，具有卓越的时间一致性，并提供直观的组合编辑功能，如对象插入、删除或替换，同时在帧间保持一致的对象身份。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel framework for compositional video synthesis that leveragestemporally consistent object-centric representations, extending our previouswork, SlotAdapt, from images to video. While existing object-centric approacheseither lack generative capabilities entirely or treat video sequencesholistically, thus neglecting explicit object-level structure, our approachexplicitly captures temporal dynamics by learning pose invariant object-centricslots and conditioning them on pretrained diffusion models. This design enableshigh-quality, pixel-level video synthesis with superior temporal coherence, andoffers intuitive compositional editing capabilities such as object insertion,deletion, or replacement, maintaining consistent object identities acrossframes. Extensive experiments demonstrate that our method sets new benchmarksin video generation quality and temporal consistency, outperforming previousobject-centric generative methods. Although our segmentation performanceclosely matches state-of-the-art methods, our approach uniquely integrates thiscapability with robust generative performance, significantly advancinginteractive and controllable video generation and opening new possibilities foradvanced content creation, semantic editing, and dynamic scene understanding.</description>
      <author>example@mail.com (Adil Kaan Akan, Yucel Yemez)</author>
      <guid isPermaLink="false">2507.20855v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Spatial Reasoning through Visual and Textual Thinking</title>
      <link>http://arxiv.org/abs/2507.20529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SpatialVTS的方法，通过同时进行视觉和文本思考来增强空间推理能力，显著提高了视觉语言模型在空间理解任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;空间推理是关于2D和3D空间中空间关系的推理，是视觉问答和机器人的基本能力。尽管视觉语言模型近年来发展迅速，但在空间推理任务上仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;引入一种能够通过视觉和文本同时增强空间推理的方法（SpatialVTS），以解决视觉语言模型在空间推理任务上的不足。&lt;h4&gt;方法&lt;/h4&gt;SpatialVTS包含两个阶段：1）空间视觉思维阶段，模型自动生成与位置相关的特定标记，处理问题中提到的对象及潜在相关对象；2）空间文本思维阶段，基于视觉线索和对话进行长期思考，逐步推断答案。此外，还对数据集进行人工校正，消除错误标签，重组数据输入格式，并开发具有逻辑推理细节的思考过程。&lt;h4&gt;主要发现&lt;/h4&gt;在不引入额外信息（如掩码或深度）的情况下，该方法在多个空间理解任务上的整体平均水平显著优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;SpatialVTS方法有效提升了视觉语言模型在空间推理任务上的性能，为空间推理问题提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空间推理任务旨在推理2D和3D空间中的空间关系，这是视觉问答和机器人的基本能力。尽管视觉语言模型近年来发展迅速，但它们在空间推理任务上仍然存在困难。在本文中，我们介绍了一种能够通过视觉和文本同时增强空间推理的方法（SpatialVTS）。在空间视觉思维阶段，我们的模型被训练来自动生成与位置相关的特定标记，不仅处理问题中提到的对象，还考虑与推理相关的潜在对象。在空间文本思维阶段，我们的模型基于视觉线索和对话进行长期思考，逐步推断空间推理问题的答案。为有效支持模型训练，我们对现有空间推理数据集进行了人工校正，消除了自动标注导致的许多错误标签，重组了数据输入格式以提高泛化能力，并开发了具有逻辑推理细节的思考过程。在不引入额外信息（如掩码或深度）的情况下，与其它模型相比，我们的模型在多个空间理解任务上的整体平均水平显著提高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型在空间推理任务上的不足。空间推理是指对2D和3D空间中的空间关系进行理解，这对视觉问答和机器人应用至关重要。这一问题很重要，因为准确的空间推理能力不仅能提升通用视觉理解，还能改善具身智能决策、人机交互等下游任务的表现，使AI系统更好地理解和导航物理世界。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到空间推理不只是简单问答，需要结合潜在视觉线索进行分析。他们发现许多空间问题需要参考图像中看似无关但实际有用的目标作为参考，且模型需要进一步思考而非直接给出答案。作者借鉴了思维链（CoT）和基于推理的视觉语言模型方法，以及区域感知的空间推理技术，如SpatialRGPT。基于这些洞察，作者设计了SpatialVTS方法，包含空间视觉思考和空间文本思考两个阶段。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是同时通过视觉和文本思考来增强空间推理能力。空间视觉思考阶段，模型自动生成与问题相关的重要目标的位置标记，包括问题中提到的对象和潜在相关对象。空间文本思考阶段，模型基于视觉线索进行长期思考，逐步推理出答案。整体流程是：首先将图像划分为网格表示目标位置，识别明显和潜在目标；然后将这些目标作为视觉线索输入，设计提示激发模型推理能力，生成理由和最终答案；同时重构数据集，添加边界框注释和推理过程以提高训练效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 同时通过视觉和文本思考增强空间推理；2) 识别潜在相关目标而不仅限于问题中明确提到的对象；3) 重构数据集，修正错误标签并添加推理过程；4) 不依赖额外视觉信息（如深度或掩码）。相比之前工作，不同之处在于：不仅关注直接目标还关注潜在目标；明确分离视觉和文本两个思考阶段；数据集处理更全面；仅使用常规图像和文本训练就能达到接近使用额外视觉信息的方法的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过同时进行视觉和文本思考来增强空间推理能力的方法，在不依赖额外视觉信息的情况下显著提升了模型在多种空间理解任务上的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The spatial reasoning task aims to reason about the spatial relationships in2D and 3D space, which is a fundamental capability for Visual QuestionAnswering (VQA) and robotics. Although vision language models (VLMs) havedeveloped rapidly in recent years, they are still struggling with the spatialreasoning task. In this paper, we introduce a method that can enhance Spatialreasoning through Visual and Textual thinking Simultaneously (SpatialVTS). Inthe spatial visual thinking phase, our model is trained to generatelocation-related specific tokens of essential targets automatically. Not onlyare the objects mentioned in the problem addressed, but also the potentialobjects related to the reasoning are considered. During the spatial textualthinking phase, Our model conducts long-term thinking based on visual cues anddialogues, gradually inferring the answers to spatial reasoning problems. Toeffectively support the model's training, we perform manual corrections to theexisting spatial reasoning dataset, eliminating numerous incorrect labelsresulting from automatic annotation, restructuring the data input format toenhance generalization ability, and developing thinking processes with logicalreasoning details. Without introducing additional information (such as masks ordepth), our model's overall average level in several spatial understandingtasks has significantly improved compared with other models.</description>
      <author>example@mail.com (Xun Liang, Xin Guo, Zhongming Jin, Weihang Pan, Penghui Shang, Deng Cai, Binbin Lin, Jieping Ye)</author>
      <guid isPermaLink="false">2507.20529v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>VESPA: Towards un(Human)supervised Open-World Pointcloud Labeling for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2507.20397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为VESPA的多模态自动标注流水线，融合LiDAR的几何精度和相机图像的语义丰富性，利用视觉语言模型实现开放词汇目标标注，支持新类别发现并生成高质量3D伪标注。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶数据收集加速，但手动标注（特别是3D标注）是主要瓶颈，成本高且劳动强度大。基于LiDAR的自动标注方法利用了几何信息，但面临LiDAR数据固有局限性（稀疏性、遮挡等）且语义粒度有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于LiDAR的自动标注方法的局限性，开发一种支持新类别发现并生成高质量3D伪标注的方法。&lt;h4&gt;方法&lt;/h4&gt;介绍VESPA多模态自动标注流水线，融合LiDAR和相机数据，利用视觉语言模型实现开放词汇目标标注，直接在点云域优化检测质量，无需真实标注或高清地图。&lt;h4&gt;主要发现&lt;/h4&gt;在Nuscenes数据集上，VESPA在物体发现方面实现52.95%的AP，在多类物体检测方面达到46.54%的AP，展示了强大的可扩展3D场景理解能力。&lt;h4&gt;结论&lt;/h4&gt;VESPA通过融合多模态数据和利用视觉语言模型，成功解决了现有自动标注方法的局限性，实现了高质量、可扩展的3D场景理解。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶的数据收集正在迅速加速，但手动标注，特别是3D标注，由于其高成本和劳动强度，仍然是一个主要瓶颈。自动标注已成为一种可扩展的替代方案，允许以最少的人工干预为点云生成标注。虽然基于LiDAR的自动标注方法利用了几何信息，但它们面临LiDAR数据的固有局限性，如稀疏性、遮挡和不完整的物体观测。此外，这些方法通常以类别无关的方式运行，提供的语义粒度有限。为了解决这些挑战，我们引入了VESPA，一种多模态自动标注流水线，它融合了LiDAR的几何精度和相机图像的语义丰富性。我们的方法利用视觉语言模型实现开放词汇目标标注，并直接在点云域中优化检测质量。VESPA支持发现新类别，并在不需要真实标注或高清地图的情况下生成高质量的3D伪标注。在Nuscenes数据集上，VESPA在物体发现方面实现了52.95%的AP，在多类物体检测方面达到了46.54%的AP，展示了在可扩展3D场景理解方面的强大性能。代码将在接受后提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶领域中3D点云标注的高成本和劳动密集问题。这个问题在现实中非常重要，因为准确的3D感知对自动驾驶至关重要，提供关键的空间信息用于路径规划、碰撞避免等，但手动标注3D数据异常耗时且昂贵，严重限制了自动驾驶技术的发展和可扩展性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到激光雷达数据具有几何精度优势但语义信息有限，而相机数据具有丰富语义但缺乏精确3D空间信息，因此设计了融合两种模态的方法。他们借鉴了现有激光雷达自动标注中的几何线索、多模态方法中的传感器融合策略以及视觉-语言模型在开放词汇目标检测中的应用。但VESPA创新性地优先考虑点云空间线索而非图像数据，避免依赖地图信息，且不依赖真实标注标签。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是融合激光雷达的几何精度和相机图像的语义丰富性，利用视觉-语言模型实现开放词汇目标标注。整体流程包括：1)地面去除；2)零样本目标分割；3)激光雷达重投影和掩码关联；4)基于空间聚类的簇去噪；5)多摄像头目标合并；6)外观嵌入提取；7)基于外观的跟踪和运动估计；8)边界框优化和膨胀。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：多模态自动标注流程、开放词汇目标标注、基于DBSCAN的簇去噪方法、基于DINOv2外观嵌入的跟踪技术。相比之前工作，VESPA优先考虑点云空间线索而非图像数据，避免依赖地图信息和真实标注标签，支持开放词汇标注而非固定类别，且不需要多轮自训练即可获得高质量自动标注。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VESPA通过融合激光雷达几何精度和视觉-语言模型语义知识，实现了一种无需人工监督的开放词汇3D点云标注方法，显著提高了自动驾驶领域目标检测的准确性和可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data collection for autonomous driving is rapidly accelerating, but manualannotation, especially for 3D labels, remains a major bottleneck due to itshigh cost and labor intensity. Autolabeling has emerged as a scalablealternative, allowing the generation of labels for point clouds with minimalhuman intervention. While LiDAR-based autolabeling methods leverage geometricinformation, they struggle with inherent limitations of lidar data, such assparsity, occlusions, and incomplete object observations. Furthermore, thesemethods typically operate in a class-agnostic manner, offering limited semanticgranularity. To address these challenges, we introduce VESPA, a multimodalautolabeling pipeline that fuses the geometric precision of LiDAR with thesemantic richness of camera images. Our approach leverages vision-languagemodels (VLMs) to enable open-vocabulary object labeling and to refine detectionquality directly in the point cloud domain. VESPA supports the discovery ofnovel categories and produces high-quality 3D pseudolabels without requiringground-truth annotations or HD maps. On Nuscenes dataset, VESPA achieves an APof 52.95% for object discovery and up to 46.54% for multiclass objectdetection, demonstrating strong performance in scalable 3D scene understanding.Code will be available upon acceptance.</description>
      <author>example@mail.com (Levente Tempfli, Esteban Rivera, Markus Lienkamp)</author>
      <guid isPermaLink="false">2507.20397v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Solving Scene Understanding for Autonomous Navigation in Unstructured Environments</title>
      <link>http://arxiv.org/abs/2507.20389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了在印度驾驶数据集上应用语义分割技术，比较了五种不同模型的性能，实现了最高的平均交并比(MIOU)为0.6496。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶汽车是汽车工业的下一场革命，理解其运行场景对有效运作至关重要。语义分割作为深度学习理解场景的关键部分，在自动驾驶研究中特别有用，用于识别可行驶区域、路边物体等。&lt;h4&gt;目的&lt;/h4&gt;在印度驾驶数据集上执行语义分割，该数据集基于班加罗尔和海得拉巴的城市和农村道路的非结构化驾驶环境，并比较不同模型的性能。&lt;h4&gt;方法&lt;/h4&gt;使用具有四个层次结构的印度驾驶数据集，在第一层次上执行分割。训练了五种模型：UNET、UNET+RESNET50、DeepLabsV3、PSPNet和SegNet，并通过平均交并比评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;实现了最高的MIOU为0.6496，论文详细讨论了数据集、探索性数据分析、模型实现和性能比较过程。&lt;h4&gt;结论&lt;/h4&gt;研究表明在具有挑战性的非结构化驾驶环境数据集上应用语义分割是可行的，不同模型表现各异，为自动驾驶场景理解提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶汽车是汽车工业的下一场革命，预计将彻底改变未来交通。理解自动驾驶车辆运行的场景对其有效运作至关重要。深度学习在迄今为止的进展中发挥了巨大作用。语义分割（标注图像中每个像素的对象类别）是使用深度学习理解场景的关键部分。在自动驾驶研究中特别有用，因为它需要理解可行驶和不可行驶区域、路边物体等。本文在最近基于班加罗尔和海得拉巴城市和农村道路编译的印度驾驶数据集上执行了语义分割。与其他如Cityscapes等数据集相比，此数据集更具挑战性，因为它基于非结构化驾驶环境。它有四个层次结构，本文在第一层次上执行了分割。训练了五种不同的模型，并使用平均交并比比较了它们的性能：UNET、UNET+RESNET50、DeepLabsV3、PSPNet和SegNet。实现了最高的MIOU为0.6496。论文讨论了数据集、探索性数据分析、准备、五种模型的实现，并研究了性能并比较了过程中取得的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles are the next revolution in the automobile industry andthey are expected to revolutionize the future of transportation. Understandingthe scenario in which the autonomous vehicle will operate is critical for itscompetent functioning. Deep Learning has played a massive role in the progressthat has been made till date. Semantic Segmentation, the process of annotatingevery pixel of an image with an object class, is one crucial part of this scenecomprehension using Deep Learning. It is especially useful in AutonomousDriving Research as it requires comprehension of drivable and non-drivableareas, roadside objects and the like. In this paper semantic segmentation hasbeen performed on the Indian Driving Dataset which has been recently compiledon the urban and rural roads of Bengaluru and Hyderabad. This dataset is morechallenging compared to other datasets like Cityscapes, since it is based onunstructured driving environments. It has a four level hierarchy and in thispaper segmentation has been performed on the first level. Five different modelshave been trained and their performance has been compared using the MeanIntersection over Union. These are UNET, UNET+RESNET50, DeepLabsV3, PSPNet andSegNet. The highest MIOU of 0.6496 has been achieved. The paper discusses thedataset, exploratory data analysis, preparation, implementation of the fivemodels and studies the performance and compares the results achieved in theprocess.</description>
      <author>example@mail.com (Naveen Mathews Renji, Kruthika K, Manasa Keshavamurthy, Pooja Kumari, S. Rajarajeswari)</author>
      <guid isPermaLink="false">2507.20389v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks</title>
      <link>http://arxiv.org/abs/2507.20174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个空间评估流程和相应基准测试，将空间理解分为绝对空间理解和3D空间理解两类。研究使用完全合成的数据集测试了最先进的视觉语言模型，发现它们在空间理解能力方面有显著改进空间，特别是在复杂任务上表现远不如人类。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶和人形机器人操作等实际应用需要精确的空间感知能力。然而，视觉语言模型(VLMs)如何识别空间关系和感知空间运动仍是一个研究不足的领域。&lt;h4&gt;目的&lt;/h4&gt;研究旨在填补VLMs在空间理解方面的研究空白，通过引入空间评估流程和构建相应基准来评估VLMs的空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;研究将空间理解分为两类：绝对空间理解(查询图像中物体的绝对空间位置)和3D空间理解(包括运动和旋转)。使用了完全合成的数据集，能够以低成本生成测试样本，同时防止数据集污染。在多个最先进的VLMs上进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;在实验中，人类在所有任务上都达到了接近完美的表现，而当前的VLMs只有在两个最简单的任务上才达到人类水平。对于其余任务，VLMs的表现明显低于人类。事实上，表现最佳的视觉语言模型在多个任务上甚至达到了接近零的分数。&lt;h4&gt;结论&lt;/h4&gt;VLMs在空间理解能力方面有显著的改进空间，特别是在处理复杂空间关系时表现不佳。&lt;h4&gt;翻译&lt;/h4&gt;实际应用，如自动驾驶和人形机器人操作，需要精确的空间感知。然而，视觉语言模型(VLMs)如何识别空间关系和感知空间运动仍是一个研究不足的领域。在这项工作中，我们引入了一个空间评估流程并构建了相应的基准。具体来说，我们将空间理解分为两种主要类型：绝对空间理解，涉及查询图像中物体的绝对空间位置(例如，左、右)，以及3D空间理解，包括运动和旋转。值得注意的是，我们的数据集完全是合成的，能够以低成本生成测试样本，同时防止数据集污染。我们在多个最先进的VLMs上进行了实验，观察到它们的空间理解能力有显著的改进空间。明确地说，在我们的实验中，人类在所有任务上都达到了接近完美的表现，而当前的VLMs只在两个最简单的任务上达到人类水平。对于其余任务，VLMs的表现明显低于人类。事实上，表现最佳的视觉语言模型在多个任务上甚至达到了接近零的分数。数据集和代码可在https://github.com/kong13661/LRR-Bench获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决评估视觉语言模型（VLMs）在空间理解任务上的能力不足问题。这个问题在现实中非常重要，因为空间理解能力对于自动驾驶、机器人操作等安全关键应用至关重要，这些应用需要系统精确感知物体之间的空间关系、位置、方向和运动，以确保安全和可靠运行。研究上，现有基准测试主要关注基本空间关系，而更复杂的空间关系和能力（如运动感知、相对运动）仍被忽视，导致对VLMs真实能力的评估不全面。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先将空间理解分为绝对空间理解和3D空间理解两大类，认识到现有基准测试的局限性（主要关注自然图像中的基本空间关系）。他们设计了一个完全合成的数据集和评估管道，能够低成本生成测试样本并防止数据集污染。在方法设计上，作者借鉴了现有工作：使用扩散模型（如Flux.1-S）生成图像，利用GroundingDINO进行对象检测和过滤；对于3D任务，使用Minecraft游戏生成图像序列。同时参考了现有空间理解基准（如VSR、What'sUP）的设计，但扩展了评估范围。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用完全合成的数据集全面评估VLMs的空间理解能力，将其分为绝对空间理解和3D空间理解两大类，并在2D和3D场景中进行评估。3D空间理解被分解为旋转和运动，从相机和物体两个视角进行测试。整体流程包括：1)使用扩散模型生成绝对位置任务的图像，用Minecraft生成3D任务的图像序列；2)设计9个不同任务（位置、位置组合、位置序列、深度、相机旋转、相机运动、物体方向、物体运动方向、物体运动）；3)使用多个先进VLMs进行评估，测试直接回答和提供推理两种方式；4)计算每个任务的准确率和整体得分。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全合成的空间理解数据集，低成本生成且防止污染；2)全面的评估框架，将空间理解分为绝对和3D两大类，3D类又分解为旋转和运动；3)深入分析发现推理方法不一定提高性能，偏好优化可能有害，参数扩展无效。相比之前工作，不同之处在于：评估更全面（涵盖复杂空间关系）；深入评估3D空间理解（关注相机运动和连续视觉信号）；使用完全合成数据而非自然图像；多角度评估3D空间理解（相机和物体视角，旋转和运动分解）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了LRR-Bench评估框架，通过完全合成数据集揭示了当前VLMs在空间理解（尤其是3D空间理解和相对运动）上的显著不足，为未来改进VLMs的空间理解能力提供了重要基准和方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world applications, such as autonomous driving and humanoid robotmanipulation, require precise spatial perception. However, it remainsunderexplored how Vision-Language Models (VLMs) recognize spatial relationshipsand perceive spatial movement. In this work, we introduce a spatial evaluationpipeline and construct a corresponding benchmark. Specifically, we categorizespatial understanding into two main types: absolute spatial understanding,which involves querying the absolute spatial position (e.g., left, right) of anobject within an image, and 3D spatial understanding, which includes movementand rotation. Notably, our dataset is entirely synthetic, enabling thegeneration of test samples at a low cost while also preventing datasetcontamination. We conduct experiments on multiple state-of-the-art VLMs andobserve that there is significant room for improvement in their spatialunderstanding abilities. Explicitly, in our experiments, humans achievenear-perfect performance on all tasks, whereas current VLMs attain human-levelperformance only on the two simplest tasks. For the remaining tasks, theperformance of VLMs is distinctly lower than that of humans. In fact, thebest-performing Vision-Language Models even achieve near-zero scores onmultiple tasks. The dataset and code are available onhttps://github.com/kong13661/LRR-Bench.</description>
      <author>example@mail.com (Fei Kong, Jinhao Duan, Kaidi Xu, Zhenhua Guo, Xiaofeng Zhu, Xiaoshuang Shi)</author>
      <guid isPermaLink="false">2507.20174v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>FROSS: Faster-than-Real-Time Online 3D Semantic Scene Graph Generation from RGB-D Images</title>
      <link>http://arxiv.org/abs/2507.19993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FROSS是一种创新的在线且超实时3D语义场景图生成方法，通过直接提升2D场景图到3D空间并使用3D高斯分布表示对象，解决了现有方法的高计算需求和非增量处理问题，在保持优越性能的同时显著提高了处理速度。&lt;h4&gt;背景&lt;/h4&gt;将复杂的3D环境抽象为简化和结构化的表示方式在各个领域都很重要。3D语义场景图(SSGs)通过将对象表示为节点，对象间关系表示为边来实现高级场景理解。然而，现有方法面临高计算需求和非增量处理的挑战，限制了它们在实时开放世界应用中的适用性。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D SSG生成方法的高计算需求和非增量处理问题，提出一种在线且超实时3D SSG生成方法。&lt;h4&gt;方法&lt;/h4&gt;提出了FROSS框架，利用2D场景图直接提升到3D空间，将对象表示为3D高斯分布，消除对精确且计算密集的点云处理的依赖。同时扩展了Replica数据集，创建了带有对象间关系注释的ReplicaSSG数据集用于评估。&lt;h4&gt;主要发现&lt;/h4&gt;在ReplicaSSG和3DSSG数据集上的实验结果表明，FROSS在操作速度上明显优于先前的3D SSG生成方法，同时实现了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;FROSS能够实现高性能的3D SSG生成，且运行速度显著快于先前方法，实现和数据集已在GitHub公开可用。&lt;h4&gt;翻译&lt;/h4&gt;将复杂的3D环境抽象为简化和结构化的表示方式在各个领域都很重要。3D语义场景图通过将对象表示为节点，对象间关系表示为边来实现这一点，有助于高级场景理解。然而，现有的3D SSG生成方法面临重大挑战，包括高计算需求和非增量处理，这限制了它们在实时开放世界应用中的适用性。为了解决这个问题，我们提出了FROSS，这是一种创新的在线且超实时3D SSG生成方法，它利用2D场景图直接提升到3D空间，并将对象表示为3D高斯分布。该框架消除对精确且计算密集的点云处理的依赖。此外，我们通过添加对象间关系注释扩展了Replica数据集，创建了ReplicaSSG数据集用于全面评估FROSS。实验结果表明，FROSS在操作速度上明显优于先前的3D SSG生成方法，同时实现了优越的性能。我们的实现和数据集已在GitHub公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D语义场景图生成中的高计算需求和非增量处理问题。这个问题在机器人、计算机视觉和增强现实等领域至关重要，因为这些领域需要系统即时感知和分析环境。在开放世界应用中，环境往往超出已知空间边界，包含以前未见过的空间，因此需要高效的处理方法来生成结构化的环境表示而不创建性能瓶颈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到精确的对象姿态和形状信息对于3D语义场景图不是必需的，即使对象位置近似，其功能含义仍然有效。这促使他们思考通过推断2D场景图并将其提升到3D空间来避免环境映射和点云处理的计算负担。他们借鉴了EGTR 2D场景图生成模型，但将对象检测骨干替换为RT-DETR，并使用了将边界框转换为高斯分布的方法以及Hellinger距离来计算高斯分布之间的相似性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将对象表示为3D高斯分布而非精确点云，直接从2D场景图提升到3D空间，避免复杂的环境映射和点云处理，并使用高效的合并算法整合多视角信息。整体流程包括：1)用RT-DETR检测图像中的对象；2)用EGTR提取对象间关系；3)将边界框转换为2D高斯分布；4)利用深度信息和相机变换将2D高斯分布投影回3D空间；5)通过基于高斯分布的合并算法整合局部3D语义场景图到全局表示中；6)通过加权集成合并高斯分布并累积关系信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出FROSS框架实现在线、更快于实时的3D语义场景图生成；2)提出基于高斯分布的3D对象合并算法，防止随着图像序列扩展出现重复或错误检测对象；3)开发ReplicaSSG数据集，扩展了Replica数据集并添加了对象关系注释。相比之前的工作，FROSS避免了对精确对象定位的需求，不依赖计算密集型的点云处理和环境映射，使用高斯分布而非点云表示对象，能以更快于实时的速度处理数据，而之前的实时方法仍存在显著系统延迟。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FROSS通过将2D场景图直接提升到3D空间并使用高斯分布表示对象，实现了高效、更快于实时的在线3D语义场景图生成，显著提高了处理速度同时保持了优异的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability to abstract complex 3D environments into simplified andstructured representations is crucial across various domains. 3D semantic scenegraphs (SSGs) achieve this by representing objects as nodes and theirinterrelationships as edges, facilitating high-level scene understanding.Existing methods for 3D SSG generation, however, face significant challenges,including high computational demands and non-incremental processing that hindertheir suitability for real-time open-world applications. To address this issue,we propose FROSS (Faster-than-Real-Time Online 3D Semantic Scene GraphGeneration), an innovative approach for online and faster-than-real-time 3D SSGgeneration that leverages the direct lifting of 2D scene graphs to 3D space andrepresents objects as 3D Gaussian distributions. This framework eliminates thedependency on precise and computationally-intensive point cloud processing.Furthermore, we extend the Replica dataset with inter-object relationshipannotations, creating the ReplicaSSG dataset for comprehensive evaluation ofFROSS. The experimental results from evaluations on ReplicaSSG and 3DSSGdatasets show that FROSS can achieve superior performance while operatingsignificantly faster than prior 3D SSG generation methods. Our implementationand dataset are publicly available at https://github.com/Howardkhh/FROSS.</description>
      <author>example@mail.com (Hao-Yu Hou, Chun-Yi Lee, Motoharu Sonogashira, Yasutomo Kawanishi)</author>
      <guid isPermaLink="false">2507.19993v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with Convolution-Compensated ViT Dual SA Block</title>
      <link>http://arxiv.org/abs/2507.19948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025 (International Joint Conference on Artificial  Intelligence)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniCT Depth的事件-图像融合方法，用于单目深度估计。该方法统一了CNN和Transformer架构，有效解决了现有方法在处理遮挡和深度差异时的局限性。&lt;h4&gt;背景&lt;/h4&gt;深度估计在3D场景理解中起着关键作用，并被广泛应用于各种视觉任务。基于图像的方法在具有挑战性的场景中表现不佳，而事件相机虽然具有高动态范围和时间分辨率，但面临数据稀疏的困难。结合事件和图像数据具有显著优势，但有效的融合仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决现有CNN融合方法因感受野有限而在处理遮挡和深度差异时遇到的问题，以及Transformer融合方法通常缺乏深度模态交互的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了UniCT Depth方法，统一了CNN和Transformer来建模局部和全局特征。具体包括：1)提出了针对编码器的卷积补偿ViT双自注意力(CcViT-DA)模块，集成了上下文建模自注意力(CMSA)来捕获空间依赖性，以及模态融合自注意力(MFSA)进行有效的跨模态融合；2)设计了专门的细节补偿卷积(DCC)模块，用于改进纹理细节并增强边缘表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，UniCT Depth在关键指标上优于现有的基于图像、事件和融合的单目深度估计方法。&lt;h4&gt;结论&lt;/h4&gt;UniCT Depth通过统一CNN和Transformer架构，有效解决了事件-图像融合中的挑战，特别是在处理遮挡和深度差异方面，显著提升了单目深度估计的性能。&lt;h4&gt;翻译&lt;/h4&gt;深度估计在3D场景理解中起着关键作用，并被广泛应用于各种视觉任务。基于图像的方法在具有挑战性的场景中表现不佳，而事件相机虽然具有高动态范围和时间分辨率，但面临数据稀疏的困难。结合事件和图像数据具有显著优势，但有效的融合仍然具有挑战性。现有的基于CNN的融合方法因感受野有限而在处理遮挡和深度差异时遇到困难，而基于Transformer的融合方法通常缺乏深度模态交互。为了解决这些问题，我们提出了UniCT Depth，这是一种事件-图像融合方法，统一了CNN和Transformer来建模局部和全局特征。我们提出了针对编码器设计的卷积补偿ViT双自注意力(CcViT-DA)模块，集成了上下文建模自注意力(CMSA)来捕获空间依赖性，以及模态融合自注意力(MFSA)进行有效的跨模态融合。此外，我们设计了专门的细节补偿卷积(DCC)模块，用于改进纹理细节并增强边缘表示。实验表明，UniCT Depth在关键指标上优于现有的基于图像、事件和融合的单目深度估计方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单目深度估计在挑战性场景(如低光照、遮挡)中的准确性问题。基于图像的方法在极端光照条件下表现不佳，而事件相机虽有高动态范围和高时间分辨率但面临数据稀疏挑战。深度估计对3D场景理解至关重要，广泛应用于自动驾驶、医疗成像等领域，准确的深度信息能帮助机器人导航、环境重建和物体识别等任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：CNN方法擅长局部特征但难以建模长距离依赖，Transformer方法擅长全局建模但跨模态交互不足。因此设计了统一的CNN-Transformer架构，结合两者优势。借鉴了U-Net架构作为整体框架，Vision Transformer的基本思想，以及现有的自注意力机制和事件数据的体素网格表示方法。通过创新的双分支自注意力机制和细节补偿卷积块，实现了空间依赖建模和有效跨模态融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合CNN和Transformer的优势，统一建模局部和全局特征，通过双分支自注意力机制同时考虑空间依赖和跨模态融合，使用细节补偿卷积增强局部特征提取。整体流程：1)输入处理(事件编码为体素网格，图像直接输入)；2)预处理(分别提取特征并融合)；3)编码器(使用残差块和CcViT-DA模块逐步下采样)；4)解码器(上采样特征并通过跳跃连接融合)；5)输出(生成像素级深度预测图)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出UniCT Depth统一CNN-Transformer架构；2)设计CcViT-DA块，集成CMSA捕获空间依赖和MFSA实现跨模态融合；3)设计DCC块改善纹理细节和边缘表示；4)统一特征提取和融合设计减少冗余计算。相比之前工作：优于纯CNN方法的长距离建模能力，优于纯Transformer方法的跨模态交互效率和计算效率，优于其他融合方法的空间和通道维度特征交互能力，以及更好的局部特征保留和纹理细节捕捉能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniCT Depth通过创新的CNN-Transformer统一架构和双分支自注意力机制，有效结合事件相机和图像数据的优势，显著提高了在挑战性场景下单目深度估计的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth estimation plays a crucial role in 3D scene understanding and isextensively used in a wide range of vision tasks. Image-based methods strugglein challenging scenarios, while event cameras offer high dynamic range andtemporal resolution but face difficulties with sparse data. Combining event andimage data provides significant advantages, yet effective integration remainschallenging. Existing CNN-based fusion methods struggle with occlusions anddepth disparities due to limited receptive fields, while Transformer-basedfusion methods often lack deep modality interaction. To address these issues,we propose UniCT Depth, an event-image fusion method that unifies CNNs andTransformers to model local and global features. We propose theConvolution-compensated ViT Dual SA (CcViT-DA) Block, designed for the encoder,which integrates Context Modeling Self-Attention (CMSA) to capture spatialdependencies and Modal Fusion Self-Attention (MFSA) for effective cross-modalfusion. Furthermore, we design the tailored Detail Compensation Convolution(DCC) Block to improve texture details and enhances edge representations.Experiments show that UniCT Depth outperforms existing image, event, andfusion-based monocular depth estimation methods across key metrics.</description>
      <author>example@mail.com (Luoxi Jing, Dianxi Shi, Zhe Liu, Songchang Jin, Chunping Qiu, Ziteng Qiao, Yuxian Li, Jianqiang Xia)</author>
      <guid isPermaLink="false">2507.19948v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Taking Language Embedded 3D Gaussian Splatting into the Wild</title>
      <link>http://arxiv.org/abs/2507.19830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Visit our project page at  https://yuzewang1998.github.io/takinglangsplatw/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的框架，利用大规模互联网照片集合实现建筑组件3D结构的沉浸式理解，扩展了语言嵌入3D高斯飞溅技术，并引入了PT-OVS基准数据集进行评估。&lt;h4&gt;背景&lt;/h4&gt;虽然利用大规模互联网照片集合进行3D重建已取得进展，使全球地标和历史遗址的沉浸式虚拟探索成为可能，但对建筑风格和结构知识的沉浸式理解很少受到关注，主要局限于浏览静态文本-图像对。&lt;h4&gt;目的&lt;/h4&gt;探索是否可以从3D野外重建技术中获得灵感，使用不受约束的照片集合创建一种理解建筑组件3D结构的沉浸式方法。&lt;h4&gt;方法&lt;/h4&gt;扩展语言嵌入3D高斯飞溅技术，提出开放词汇场景理解新框架：从重建辐射场渲染多种外观图像，提取多外观CLIP特征和语言特征不确定性图，提出瞬时不确定性感知自动编码器、多外观语言场3DGS表示和后集成策略，并引入PT-OVS基准数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法优于现有方法，能提供准确的开放词汇分割，支持开放词汇查询的交互式漫游、建筑风格模式识别和3D场景编辑等应用。&lt;h4&gt;结论&lt;/h4&gt;该方法成功实现了从不受约束的照片集合中理解和分割建筑组件的3D结构，为建筑风格和结构知识的沉浸式理解提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;利用大规模互联网照片集合进行3D重建的最新进展，使全球地标和历史遗址的沉浸式虚拟探索成为可能。然而，对建筑风格和结构知识的沉浸式理解很少受到关注，这主要局限于浏览静态文本-图像对。因此，我们能否从3D野外重建技术中获得灵感，使用不受约束的照片集合创建一种理解建筑组件3D结构的沉浸式方法？为此，我们扩展了语言嵌入3D高斯飞溅技术，提出了一个用于从不受约束的照片集合进行开放词汇场景理解的新框架。具体而言，我们首先从重建的辐射场中从同一视角渲染多种外观图像，然后提取多外观CLIP特征和两种类型的语言特征不确定性图—瞬时和外观不确定性—源自多外观特征以指导后续优化过程。接下来，我们提出了一种瞬时不确定性感知自动编码器、多外观语言场3DGS表示和后集成策略，以有效地压缩、学习和融合来自多种外观的语言特征。最后，为了定量评估我们的方法，我们引入了PT-OVS，这是一个新的基准数据集，用于评估不受约束照片集合上的开放词汇分割性能。实验结果表明，我们的方法优于现有方法，提供准确的开放词汇分割，并支持如开放词汇查询的交互式漫游、建筑风格模式识别和3D场景编辑等应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从不受约束的网络照片集合中实现开放词汇的建筑场景理解问题。这个问题重要是因为当前3D重建技术主要关注地标虚拟探索，而对建筑风格和结构知识的沉浸式理解研究较少，现有方法局限于静态文本-图像对浏览，缺乏沉浸式体验。通过互联网照片实现低成本的建筑组件细粒度3D理解，可以帮助用户直观地理解建筑结构、风格和历史背景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D场景重建与开放词汇理解之间的技术差距，识别出语言特征在不同外观和遮挡条件下不一致的主要挑战。他们借鉴了3D高斯泼溅(3DGS)技术用于场景表示，语言嵌入3DGS用于开放词汇理解，以及不受约束照片集合的重建方法(如WE-GS)。基于这些工作，作者设计了多外观渲染增强语言特征、不确定性映射量化语义噪声、瞬时不确定性感知自动编码器压缩特征、多外观语言嵌入3DGS表示以及后特征集成策略等创新组件来解决这一挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多外观渲染和特征提取来增强不受约束照片集合的语言特征，利用不确定性映射识别和排除由遮挡和外观变化引起的噪声特征，通过压缩和集成多外观语言特征构建鲁棒的3D语言场。整体流程包括：1)多外观像素级语言特征提取，使用重建辐射场渲染多外观图像并提取CLIP特征；2)训练瞬时不确定性感知自动编码器压缩语言特征；3)使用多外观语言嵌入3DGS(MALE-GS)学习多外观语言特征；4)通过后特征集成和背景过滤实现开放词汇查询和分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多外观CLIP特征增强策略，通过渲染多外观图像解决语言特征不一致问题；2)设计外观和瞬时两种不确定性映射量化语义不确定性；3)提出瞬时不确定性感知自动编码器压缩特征；4)开发多外观语言嵌入3DGS表示；5)设计加权融合的后特征集成策略。相比之前工作，本文将开放词汇3D理解从受控照片集合扩展到不受约束网络照片集合，解决了CLIP特征的不可加性问题，引入多外观渲染和不确定性映射指导优化，并创建了新的PT-OVS评估基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过多外观语言特征增强和不确定性感知优化，首次实现了从不受约束网络照片集合中重建支持开放词汇查询的3D语言场，为建筑风格和结构知识的沉浸式理解提供了新方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in leveraging large-scale Internet photo collections for 3Dreconstruction have enabled immersive virtual exploration of landmarks andhistoric sites worldwide. However, little attention has been given to theimmersive understanding of architectural styles and structural knowledge, whichremains largely confined to browsing static text-image pairs. Therefore, can wedraw inspiration from 3D in-the-wild reconstruction techniques and useunconstrained photo collections to create an immersive approach forunderstanding the 3D structure of architectural components? To this end, weextend language embedded 3D Gaussian splatting (3DGS) and propose a novelframework for open-vocabulary scene understanding from unconstrained photocollections. Specifically, we first render multiple appearance images from thesame viewpoint as the unconstrained image with the reconstructed radiancefield, then extract multi-appearance CLIP features and two types of languagefeature uncertainty maps-transient and appearance uncertainty-derived from themulti-appearance features to guide the subsequent optimization process. Next,we propose a transient uncertainty-aware autoencoder, a multi-appearancelanguage field 3DGS representation, and a post-ensemble strategy to effectivelycompress, learn, and fuse language features from multiple appearances. Finally,to quantitatively evaluate our method, we introduce PT-OVS, a new benchmarkdataset for assessing open-vocabulary segmentation performance on unconstrainedphoto collections. Experimental results show that our method outperformsexisting methods, delivering accurate open-vocabulary segmentation and enablingapplications such as interactive roaming with open-vocabulary queries,architectural style pattern recognition, and 3D scene editing.</description>
      <author>example@mail.com (Yuze Wang, Yue Qi)</author>
      <guid isPermaLink="false">2507.19830v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing</title>
      <link>http://arxiv.org/abs/2507.19691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Co-Win是一种新的鸟瞰图(BEV)感知框架，用于解决复杂城市环境中的自动驾驶感知和理解挑战。&lt;h4&gt;背景&lt;/h4&gt;在复杂城市环境中实现准确感知和场景理解是确保安全高效自主导航的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理环境理解中固有多模态性的感知框架，以增强自动驾驶系统的决策和规划能力。&lt;h4&gt;方法&lt;/h4&gt;提出Co-Win框架，整合点云编码与高效的并行窗口特征提取；采用分层架构，包括专用编码器、基于窗口的主干和基于查询的解码器头；使用基于变分的方法和基于掩码的实例分割，而非简单的回归任务；通过渐进式特征提取阶段处理点云数据。&lt;h4&gt;主要发现&lt;/h4&gt;Co-Win能够有效捕获多样化的空间特征和物体关系；预测的掩码既保持数据一致性又具有上下文相关性；产生可解释和多样化的实例预测。&lt;h4&gt;结论&lt;/h4&gt;Co-Win框架通过细粒度的场景分解和理解，增强了自动驾驶系统中的下游决策和规划能力。&lt;h4&gt;翻译&lt;/h4&gt;在复杂城市环境中实现准确感知和场景理解是确保安全高效自主导航的关键挑战。在本文中，我们提出了Co-Win，一种新的鸟瞰图(BEV)感知框架，它整合了点云编码与高效的并行窗口特征提取，以解决环境理解中固有的多模态性问题。我们的方法采用分层架构，包括专用编码器、基于窗口的主干和基于查询的解码器头，以有效捕获多样化的空间特征和物体关系。与将感知视为简单回归任务的先前方法不同，我们的框架结合了基于变分的方法和基于掩码的实例分割，实现了细粒度的场景分解和理解。Co-Win架构通过渐进式特征提取阶段处理点云数据，确保预测的掩码既保持数据一致性又具有上下文相关性。此外，我们的方法产生可解释和多样化的实例预测，增强了自动驾驶系统中的下游决策和规划能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决复杂城市环境中LiDAR点云的准确感知和场景理解问题，这对自动驾驶系统至关重要，因为精确的环境感知是确保安全高效导航的基础。城市环境中物体形状多样、相互遮挡，传统方法难以提供细粒度的场景分解，而LiDAR点云的无结构性和多模态特性增加了处理难度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者系统分析了现有BEV感知方法的局限性，借鉴了多种技术路线：参考了BEVFusion等BEV表示方法，吸收了VoxelNet和PointPillars等点云处理技术，借鉴了Mask2Former等掩码式分割方法，以及Mask R-CNN等联合检测分割框架。基于这些借鉴，作者设计了一个分层架构，结合点云编码、并行特征提取和掩码式解码，以解决点云处理中的效率和精度问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过协同窗口处理实现LiDAR点云中的联合物体检测和实例分割，使用物体的足迹掩码而非传统边界框来检测物体，从而一次性完成物体检测和足迹补全。整体流程包括：1)AFN编码器处理原始点云生成BEV表示；2)SPCN网络通过子窗口并行处理和线性注意力机制提取多尺度特征；3)基于掩码的解码器将特征转换为实例级物体掩码；4)最终输出包含类别和精确形状的检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AFN编码器采用多视角融合和几何多轴机制保留点云几何信息；2)SPCN网络通过子窗口分区和线性注意力将计算复杂度从O(N²)降至O(N)；3)基于掩码的解码器直接生成精确物体形状而非边界框。相比传统方法，Co-Win不将感知视为简单回归任务，而是通过变分方法实现细粒度场景分解；不同于边界框方法，它能提供精确物体形状；不同于分离式处理，它实现了联合检测和分割任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Co-Win提出了一种创新的协同窗口处理框架，通过结合高效的点云编码、基于窗口的并行特征提取和基于掩码的解码，实现了LiDAR点云中精确的联合物体检测和实例分割，显著提升了自动驾驶系统在复杂城市环境中的感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate perception and scene understanding in complex urban environments isa critical challenge for ensuring safe and efficient autonomous navigation. Inthis paper, we present Co-Win, a novel bird's eye view (BEV) perceptionframework that integrates point cloud encoding with efficient parallelwindow-based feature extraction to address the multi-modality inherent inenvironmental understanding. Our method employs a hierarchical architecturecomprising a specialized encoder, a window-based backbone, and a query-baseddecoder head to effectively capture diverse spatial features and objectrelationships. Unlike prior approaches that treat perception as a simpleregression task, our framework incorporates a variational approach withmask-based instance segmentation, enabling fine-grained scene decomposition andunderstanding. The Co-Win architecture processes point cloud data throughprogressive feature extraction stages, ensuring that predicted masks are bothdata-consistent and contextually relevant. Furthermore, our method producesinterpretable and diverse instance predictions, enabling enhanced downstreamdecision-making and planning in autonomous driving systems.</description>
      <author>example@mail.com (Haichuan Li, Tomi Westerlund)</author>
      <guid isPermaLink="false">2507.19691v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions</title>
      <link>http://arxiv.org/abs/2507.19188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VisHall3D，一种用于单目语义场景完成的两阶段框架，解决了现有方法中的特征纠缠和几何不一致问题。&lt;h4&gt;背景&lt;/h4&gt;现有单目语义场景完成方法存在特征纠缠和几何不一致的问题。&lt;h4&gt;目的&lt;/h4&gt;解决特征纠缠和几何不一致问题，提高场景完成的准确性。&lt;h4&gt;方法&lt;/h4&gt;VisHall3D将场景完成任务分解为两个阶段：第一阶段使用VisFrontierNet模块重建可见区域；第二阶段使用OcclusionMAE网络推断不可见区域。&lt;h4&gt;主要发现&lt;/h4&gt;通过将场景完成解耦为两个不同阶段，VisHall3D有效缓解了特征纠缠和几何不一致问题，显著提高了重建质量。&lt;h4&gt;结论&lt;/h4&gt;在SemanticKITTI和SSCBench-KITTI-360两个基准测试上验证了VisHall3D的有效性，达到了最先进的性能，超越了之前的方法，为自动驾驶等应用中的场景理解提供了更准确可靠的途径。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了VisHall3D，一种新颖的单目语义场景完成两阶段框架，旨在解决现有方法中普遍存在的特征纠缠和几何不一致问题。VisHall3D将场景完成任务分解为两个阶段：重建可见区域(视觉)和推断不可见区域(幻觉)。在第一阶段，引入了可见性感知投影模块VisFrontierNet，以准确追踪视觉前沿同时保留精细细节。在第二阶段，采用幻觉网络OcclusionMAE，利用噪声注入机制为不可见区域生成合理的几何结构。通过将场景完成解耦为这两个不同阶段，VisHall3D有效缓解了特征纠缠和几何不一致问题，显著提高了重建质量。通过在SemanticKITTI和SSCBench-KITTI-360两个具有挑战性的基准测试上进行大量实验，验证了VisHall3D的有效性。VisHall3D达到了最先进的性能，以显著优势超越了之前的方法，为自动驾驶和其他应用中的更准确可靠场景理解铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目语义场景完成(Monocular SSC)中的特征纠缠和几何不一致性问题。特征纠缠指可见和不可见区域的特征混合在一起，几何不一致性指重建的可见和不可见区域之间存在错位或冲突结构。这个问题在自动驾驶等领域非常重要，因为准确的3D场景理解对自主导航和环境感知至关重要，而单目方法只需RGB图像就能重建完整场景，具有成本效益高的优势。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有方法将场景完成视为单一阶段任务，忽视了可见和不可见区域之间的固有区别。他们认识到需要明确分离可见区域重建和不可见区域推断的任务，以减轻特征纠缠和几何不一致性。作者借鉴了多阶段细化策略在计算机视觉中的应用，特别是在目标检测领域；使用了Masked Autoencoder框架设计OcclusionMAE模块；应用了3D变形注意力机制；并利用了深度预测和体素投影等已有技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将单目语义场景完成分解为两个独立阶段：重建可见区域和推断不可见区域，通过明确区分可见和不可见区域来减轻特征纠缠和几何不一致性。整体流程分为两阶段：第一阶段是VisFrontierNet，通过识别视觉边界、使用无符号距离函数编码几何特征、将图像特征注入3D空间和使用3D变形注意力进行特征细化来重建可见区域；第二阶段是OcclusionMAE，通过添加噪声、注入噪声水平和使用3D U-Net来生成完整的占用网格，完成不可见区域的推断。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) VisHall3D两阶段框架，明确分离视觉和幻觉过程；2) VisFrontierNet感知可见性的投影模块，准确追踪视觉边界；3) OcclusionMAE幻觉网络，使用噪声注入机制生成不可见区域的合理几何形状。相比之前的工作，VisHall3D将场景完成分解为两个明确分离的阶段，而非单一任务；使用无符号距离函数而非硬分配提供更丰富的几何信息；引入噪声注入机制提高鲁棒性；并在两个基准数据集上实现了最先进的性能，显著优于之前的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VisHall3D通过将单目语义场景完成分解为可见区域重建和不可见区域推断两个阶段，有效解决了特征纠缠和几何不一致性问题，显著提高了3D场景重建的质量和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces VisHall3D, a novel two-stage framework for monocularsemantic scene completion that aims to address the issues of featureentanglement and geometric inconsistency prevalent in existing methods.VisHall3D decomposes the scene completion task into two stages: reconstructingthe visible regions (vision) and inferring the invisible regions(hallucination). In the first stage, VisFrontierNet, a visibility-awareprojection module, is introduced to accurately trace the visual frontier whilepreserving fine-grained details. In the second stage, OcclusionMAE, ahallucination network, is employed to generate plausible geometries for theinvisible regions using a noise injection mechanism. By decoupling scenecompletion into these two distinct stages, VisHall3D effectively mitigatesfeature entanglement and geometric inconsistency, leading to significantlyimproved reconstruction quality.  The effectiveness of VisHall3D is validated through extensive experiments ontwo challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3Dachieves state-of-the-art performance, outperforming previous methods by asignificant margin and paves the way for more accurate and reliable sceneunderstanding in autonomous driving and other applications.</description>
      <author>example@mail.com (Haoang Lu, Yuanqi Su, Xiaoning Zhang, Longjun Gao, Yu Xue, Le Wang)</author>
      <guid isPermaLink="false">2507.19188v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning</title>
      <link>http://arxiv.org/abs/2507.18531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IntentVCNet的新方法，用于解决大型视觉语言模型(LVLMs)在视频描述生成中无法进行细粒度空间控制的问题。通过统一LVLMs中的时间和空间理解知识，从提示和模型两个角度弥合时空差距，显著提升了LVLMs对视频序列中空间细节的建模能力，使其能够准确生成受控的意图导向描述。&lt;h4&gt;背景&lt;/h4&gt;当前大型视觉语言模型(LVLMs)虽然在空间和时间理解方面表现出色，但无法在时间序列中对空间进行细粒度控制以响应指令。这种显著的时空差距使得在视频中实现细粒度的意图导向控制变得复杂。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法来弥合LVLMs在视频描述中的时空差距，实现细粒度的意图导向控制，使模型能够根据用户意图生成针对性的视频描述。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为IntentVCNet的新方法，包含两个主要策略：(1)提示组合策略，使LLM能够建模表征用户意图的提示与视频序列之间的隐含关系；(2)参数高效的box适配器，增强全局视觉上下文中的对象语义信息，使视觉标记具有关于用户意图的先验信息。&lt;h4&gt;主要发现&lt;/h4&gt;两种策略的结合可以进一步增强LVLM对视频序列中空间细节的建模能力，促进LVLMs准确生成受控的意图导向描述。&lt;h4&gt;结论&lt;/h4&gt;IntentVCNet成功解决了LVLMs在视频描述中的时空差距问题，在多个开源LVLMs上取得了最先进的结果，在IntentVC挑战赛中获得亚军，表明该方法在意图导向的视频描述生成任务上具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;面向意图的受控视频描述旨在根据定制的用户意图为视频中的特定目标生成有针对性的描述。当前大型视觉语言模型(LVLMs)已获得强大的指令遵循和视觉理解能力。尽管LVLMs分别在空间和时间理解方面表现出色，但它们无法在时间序列中直接响应指令进行细粒度的空间控制。这种显著的时空差距使得在视频中实现细粒度的意图导向控制变得复杂。为此，我们提出了一种新颖的IntentVCNet，它统一了LVLMs中固有的时间和空间理解知识，从提示和模型两个角度弥合时空差距。具体而言，我们首先提出了一种提示组合策略，旨在使LLM能够建模表征用户意图的提示与视频序列之间的隐含关系。然后，我们提出了一种参数高效的box适配器，增强全局视觉上下文中的对象语义信息，使视觉标记具有关于用户意图的先验信息。最终实验证明，两种策略的结合可以进一步增强LVLM对视频序列中空间细节的建模能力，促进LVLMs准确生成受控的意图导向描述。我们的方法在多个开源LVLMs上取得了最先进的结果，并在IntentVC挑战赛中获得亚军。我们的代码可在https://github.com/thqiu0419/IntentVCNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intent-oriented controlled video captioning aims to generate targeteddescriptions for specific targets in a video based on customized user intent.Current Large Visual Language Models (LVLMs) have gained strong instructionfollowing and visual comprehension capabilities. Although the LVLMsdemonstrated proficiency in spatial and temporal understanding respectively, itwas not able to perform fine-grained spatial control in time sequences indirect response to instructions. This substantial spatio-temporal gapcomplicates efforts to achieve fine-grained intention-oriented control invideo. Towards this end, we propose a novel IntentVCNet that unifies thetemporal and spatial understanding knowledge inherent in LVLMs to bridge thespatio-temporal gap from both prompting and model perspectives. Specifically,we first propose a prompt combination strategy designed to enable LLM to modelthe implicit relationship between prompts that characterize user intent andvideo sequences. We then propose a parameter efficient box adapter thataugments the object semantic information in the global visual context so thatthe visual token has a priori information about the user intent. The finalexperiment proves that the combination of the two strategies can furtherenhance the LVLM's ability to model spatial details in video sequences, andfacilitate the LVLMs to accurately generate controlled intent-orientedcaptions. Our proposed method achieved state-of-the-art results in several opensource LVLMs and was the runner-up in the IntentVC challenge. Our code isavailable on https://github.com/thqiu0419/IntentVCNet.</description>
      <author>example@mail.com (Tianheng Qiu, Jingchun Gao, Jingyu Li, Huiyi Leong, Xuan Huang, Xi Wang, Xiaocheng Zhang, Kele Xu, Lan Zhang)</author>
      <guid isPermaLink="false">2507.18531v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting</title>
      <link>http://arxiv.org/abs/2507.18678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 (Highlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种可扩展的流程，能够将单视图图像转换为全面的、尺度和外观真实的3D表示，包括点云、相机姿态、深度图和伪RGBD。通过自动生成真实的、具有尺度感知的3D数据，显著降低了数据收集成本，并促进了空间智能的发展。&lt;h4&gt;背景&lt;/h4&gt;空间智能是AI的新兴前沿领域，但受限于大规模3D数据集的稀缺。与丰富的2D图像不同，获取3D数据通常需要专门的传感器和繁琐的标注。&lt;h4&gt;目的&lt;/h4&gt;提出一个可扩展的流程，将单视图图像转换为全面的、尺度和外观真实的3D表示，连接大量图像存储库与日益增长的空间场景理解需求。&lt;h4&gt;方法&lt;/h4&gt;通过集成深度估计、相机校准和尺度校准的技术，自动从图像生成真实的、具有尺度感知的3D数据，包括点云、相机姿态、深度图和伪RGBD。&lt;h4&gt;主要发现&lt;/h4&gt;发布了两个生成的空间数据集（COCO-3D和Objects365-v2-3D），并通过大量实验证明生成的数据可以受益于各种3D任务，从基础感知到基于MLLM的推理。&lt;h4&gt;结论&lt;/h4&gt;研究结果验证了该流程是开发能够感知、理解和与物理环境交互的AI系统的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空间智能正在成为人工智能变革的前沿，但仍受限于大规模三维数据集的稀缺。与丰富的二维图像不同，获取三维数据通常需要专门的传感器和繁琐的标注。在这项工作中，我们提出了一个可扩展的流程，通过集成的深度估计、相机校准和尺度校准，将单视图图像转换为全面的、尺度和外观真实的三维表示——包括点云、相机姿态、深度图和伪RGBD。我们的方法连接了大量图像存储库与日益增长的空间场景理解需求。通过从图像自动生成真实的、具有尺度感知的三维数据，我们显著降低了数据收集成本，并为推进空间智能开辟了新途径。我们发布了两个生成的空间数据集，即COCO-3D和Objects365-v2-3D，并通过大量实验证明，我们生成的数据可以受益于各种三维任务，从基础感知到基于MLLM的推理。这些结果验证了我们的流程是开发能够感知、理解和与物理环境交互的AI系统的有效解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决空间智能领域中大规模3D数据稀缺的问题。这个问题非常重要，因为空间智能是AI的新前沿，能推动自动驾驶、机器人和AR/VR等技术的发展，但与丰富的2D图像数据不同，3D数据获取需要专门传感器和高昂的标注成本，限制了空间智能的发展，就像早期计算机视觉缺乏ImageNet规模数据集一样。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有3D数据获取方法的局限性：模拟方法存在模拟到现实的差距，AI生成的3D资产通常只限于单个物体，传感器捕获的数据成本高且规模小。他们注意到大规模2D图像数据集(如COCO)的潜力尚未被充分利用，因此设计了'2D-to-3D数据提升'方法。该方法借鉴了MoGe进行相对深度估计、Metric3D v2进行度量深度估计、WildCamera预测相机内参、PerspectiveFields推断相机外参等现有工作，并将它们整合到一个完整流程中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将大规模2D图像数据集转换为高质量的3D表示，解决空间智能中数据稀缺问题。方法结合相对深度估计(保留几何细节)和度量深度估计(提供真实尺度)，确保生成的3D数据既精细又符合真实世界尺度。整体流程包括：1)相对深度估计；2)度量深度估计；3)尺度校准；4)相机参数预测；5)3D点云生成；6)3D标注转换；7)无效点过滤，最终生成具有丰富标注的3D场景数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'2D-to-3D数据提升'管道；2)结合相对和度量深度估计；3)系统整合相机参数估计；4)创建COCO-3D和Objects365-v2-3D两个大规模数据集(约200万场景，300多类别)。相比之前工作，不同之处在于：与模拟方法相比保留了真实纹理；与AI生成的3D资产相比能生成场景级数据；与传感器捕获数据相比成本低且可扩展；与之前2D到3D工作相比捕捉了更精细的几何细节和真实尺度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的'2D-to-3D数据提升'方法，将大规模2D图像数据集转换为高质量、多样化的3D表示，为开发能够感知、理解和与物理世界交互的AI系统提供了坚实基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial intelligence is emerging as a transformative frontier in AI, yet itremains constrained by the scarcity of large-scale 3D datasets. Unlike theabundant 2D imagery, acquiring 3D data typically requires specialized sensorsand laborious annotation. In this work, we present a scalable pipeline thatconverts single-view images into comprehensive, scale- and appearance-realistic3D representations - including point clouds, camera poses, depth maps, andpseudo-RGBD - via integrated depth estimation, camera calibration, and scalecalibration. Our method bridges the gap between the vast repository of imageryand the increasing demand for spatial scene understanding. By automaticallygenerating authentic, scale-aware 3D data from images, we significantly reducedata collection costs and open new avenues for advancing spatial intelligence.We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,and demonstrate through extensive experiments that our generated data canbenefit various 3D tasks, ranging from fundamental perception to MLLM-basedreasoning. These results validate our pipeline as an effective solution fordeveloping AI systems capable of perceiving, understanding, and interactingwith physical environments.</description>
      <author>example@mail.com (Xingyu Miao, Haoran Duan, Quanhao Qian, Jiuniu Wang, Yang Long, Ling Shao, Deli Zhao, Ran Xu, Gongjie Zhang)</author>
      <guid isPermaLink="false">2507.18678v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>OpenNav: Open-World Navigation with Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2507.18033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于多模态大型语言模型的机器人导航框架，能够将复杂的自然语言指令转化为具体的轨迹点，在开放集指令和对象环境下完成导航任务。&lt;h4&gt;背景&lt;/h4&gt;预训练的大型语言模型虽展示了强大的常识推理能力，但在开放世界中弥合语言描述与机器人实际动作之间的差距仍面临挑战，特别是超越有限的预定义运动原语方面。&lt;h4&gt;目的&lt;/h4&gt;使机器人能够解释和分解复杂的语言指令，生成轨迹序列来完成多样化的导航任务，处理开放集指令和开放集对象。&lt;h4&gt;方法&lt;/h4&gt;利用多模态大型语言模型的跨模态理解和代码生成能力，使其与视觉语言感知模型交互生成组合的2D鸟瞰图价值地图，整合语义知识与空间信息强化机器人的空间理解。&lt;h4&gt;主要发现&lt;/h4&gt;通过大规模自动驾驶车辆数据集验证了零样本视觉语言导航框架在室外导航任务中的有效性，系统能执行多样化的自由形式自然语言指令，对物体检测错误和语言歧义具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的框架在室内外真实场景中表现出良好的鲁棒性和适用性，能够有效处理开放环境中的导航任务。&lt;h4&gt;翻译&lt;/h4&gt;预训练的大型语言模型（LLMs）已展现出强大的常识推理能力，使其在机器人导航和规划任务中具有良好前景。然而，尽管最近取得进展，在开放世界中弥合语言描述与机器人实际动作之间的差距，超越仅调用有限的预定义运动原语，仍然是一个开放的挑战。在本工作中，我们旨在使机器人能够解释和分解复杂的语言指令，最终生成一系列轨迹点来完成多样化的导航任务，给定开放集指令和开放集对象。我们观察到多模态大型语言模型（MLLMs）在处理自由形式的语言指令时表现出强大的跨模态理解能力，展示了强大的场景理解能力。更重要的是，利用它们的代码生成能力，MLLMs可以与视觉语言感知模型交互，生成组合的2D鸟瞰图价值地图，有效整合MLLMs的语义知识与地图的空间信息，以加强机器人的空间理解。为进一步验证我们的方法，我们有效利用大规模自动驾驶车辆数据集（AVDs）在室外导航任务中验证了提出的零样本视觉语言导航框架，展示了它执行多样化自由形式自然语言导航指令的能力，同时保持对物体检测错误和语言歧义的鲁棒性。此外，我们在室内和室外场景中的Husky机器人上验证了我们的系统，展示了其真实世界的鲁棒性和适用性。补充视频可在 https://trailab.github.io/OpenNav-website/ 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained large language models (LLMs) have demonstrated strongcommon-sense reasoning abilities, making them promising for robotic navigationand planning tasks. However, despite recent progress, bridging the gap betweenlanguage descriptions and actual robot actions in the open-world, beyond merelyinvoking limited predefined motion primitives, remains an open challenge. Inthis work, we aim to enable robots to interpret and decompose complex languageinstructions, ultimately synthesizing a sequence of trajectory points tocomplete diverse navigation tasks given open-set instructions and open-setobjects. We observe that multi-modal large language models (MLLMs) exhibitstrong cross-modal understanding when processing free-form languageinstructions, demonstrating robust scene comprehension. More importantly,leveraging their code-generation capability, MLLMs can interact withvision-language perception models to generate compositional 2D bird-eye-viewvalue maps, effectively integrating semantic knowledge from MLLMs with spatialinformation from maps to reinforce the robot's spatial understanding. Tofurther validate our approach, we effectively leverage large-scale autonomousvehicle datasets (AVDs) to validate our proposed zero-shot vision-languagenavigation framework in outdoor navigation tasks, demonstrating its capabilityto execute a diverse range of free-form natural language navigationinstructions while maintaining robustness against object detection errors andlinguistic ambiguities. Furthermore, we validate our system on a Husky robot inboth indoor and outdoor scenes, demonstrating its real-world robustness andapplicability. Supplementary videos are available athttps://trailab.github.io/OpenNav-website/</description>
      <author>example@mail.com (Mingfeng Yuan, Letian Wang, Steven L. Waslander)</author>
      <guid isPermaLink="false">2507.18033v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Towards Holistic Surgical Scene Graph</title>
      <link>http://arxiv.org/abs/2507.15541v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的图表示方法及其数据集，用于更好地理解手术场景中的关键元素，包括工具-动作-目标组合和操作工具的手的身份。&lt;h4&gt;背景&lt;/h4&gt;手术场景理解对计算机辅助干预系统至关重要，需要理解手术工具、解剖结构及其相互作用。虽然基于图的方法已被用于表示手术场景，但工具-动作-目标的多样组合和操作工具的手的身份等重要方面仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;将工具-动作-目标组合和手身份等关键方面整合到手术场景图的表示中，以提高手术场景理解的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Endoscapes-SG201数据集，包含工具-动作-目标组合和手身份的标注；开发SSG-Com图学习方法，用于学习和表示这些关键元素；在安全关键视图评估和动作三元组识别等下游任务上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;集成工具-动作-目标组合和手身份等基本场景图组件对手术场景理解至关重要，这些组件对提高手术场景理解的准确性有显著贡献。&lt;h4&gt;结论&lt;/h4&gt;通过整合关键场景图组件，可以更有效地表示和理解复杂的手术场景，为计算机辅助干预系统提供更准确的信息支持。代码和数据集已在GitHub平台公开共享。&lt;h4&gt;翻译&lt;/h4&gt;手术场景理解对于计算机辅助干预系统至关重要，需要对涉及手术工具、解剖结构及其相互作用的手术场景进行视觉理解。为有效表示手术场景中的复杂信息，已探索基于图的方法来结构化建模手术实体及其关系。先前的手术场景图研究已证明了使用图表示手术场景的可行性。然而，某些手术场景方面——如工具-动作-目标的多样组合和操作工具的手的身份——尽管很重要，但在基于图的表示中仍未得到充分探索。为将这些方面整合到图表示中，我们提出了Endoscapes-SG201数据集，包含工具-动作-目标组合和手身份的标注。我们还引入了SSG-Com，一种基于图的方法，旨在学习和表示这些关键元素。通过在安全关键视图评估和动作三元组识别等下游任务上的实验，我们证明了整合这些基本场景图组件的重要性，突显了它们对手术场景理解的显著贡献。代码和数据集可在https://github.com/ailab-kyunghee/SSG-Com获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术场景理解不全面的问题，特别是现有图表示方法未能充分捕捉工具-动作-目标组合和操作工具的手的身份信息。这个问题很重要，因为全面的手术场景理解是计算机辅助手术系统的核心，对于手术工作流分析、自动化安全评估和手术报告生成等应用至关重要，而缺少这些关键信息会导致系统无法完全理解手术场景的复杂交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于图的方法的局限性，认识到手术场景中工具-动作-目标组合和手身份信息的重要性。他们借鉴了LG-CVS作为基准模型，该模型主要基于对象间的空间关系构建图。作者扩展了这个框架，引入了手术动作边和手身份分类，同时使用Faster R-CNN作为对象检测器，图卷积网络处理图结构，并采用两阶段训练方法。这些设计都是在现有工作基础上进行的创新扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全面的手术场景图，不仅包含手术工具和解剖结构及其空间关系，还明确包含动作关系和操作手信息。整体流程包括：1)节点生成：使用对象检测识别手术工具和解剖结构作为图节点；2)图构建：建立节点间关系，引入手术动作边编码工具与解剖结构间的交互；3)手术动作边分类：学习工具如何与解剖结构交互；4)手身份分类：预测操作每个工具的手的身份；5)图学习：使用图卷积网络更新图；6)下游任务：针对特定任务设计解码器。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Endoscapes-SG201数据集，包含细化的工具分类、工具-动作-目标组合和手身份标注；2)提出SSG-Com方法，整合空间关系、动作关系和手身份信息；3)通过下游任务验证方法有效性。相比之前工作，不同之处在于：之前的图表示主要关注空间关系，未明确反映医务人员角色；未能充分表示工具-动作-目标组合的复杂交互；新方法引入手术动作边明确编码有意义交互，并包含手身份信息，使模型能理解使用哪只手操作每个工具。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文通过提出新的综合手术场景图数据集和整合工具-动作-目标组合及手身份信息的图表示方法，显著提升了手术场景理解能力，并在下游任务中证明了其有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical scene understanding is crucial for computer-assisted interventionsystems, requiring visual comprehension of surgical scenes that involvesdiverse elements such as surgical tools, anatomical structures, and theirinteractions. To effectively represent the complex information in surgicalscenes, graph-based approaches have been explored to structurally modelsurgical entities and their relationships. Previous surgical scene graphstudies have demonstrated the feasibility of representing surgical scenes usinggraphs. However, certain aspects of surgical scenes-such as diversecombinations of tool-action-target and the identity of the hand operating thetool-remain underexplored in graph-based representations, despite theirimportance. To incorporate these aspects into graph representations, we proposeEndoscapes-SG201 dataset, which includes annotations for tool-action-targetcombinations and hand identity. We also introduce SSG-Com, a graph-based methoddesigned to learn and represent these critical elements. Through experiments ondownstream tasks such as critical view of safety assessment and action tripletrecognition, we demonstrated the importance of integrating these essentialscene graph components, highlighting their significant contribution to surgicalscene understanding. The code and dataset are available athttps://github.com/ailab-kyunghee/SSG-Com</description>
      <author>example@mail.com (Jongmin Shin, Enki Cho, Ka Young Kim, Jung Yong Kim, Seong Tae Kim, Namkee Oh)</author>
      <guid isPermaLink="false">2507.15541v2</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey</title>
      <link>http://arxiv.org/abs/2507.20783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  45 pages, 2 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于预训练语言模型时代通用文本嵌入(GPTE)的综述文章，全面介绍了GPTE的基本架构、PLMs的作用、高级功能及未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;文本嵌入因其检索、分类、聚类、双文本挖掘和摘要等多种NLP任务中的有效性而受到越来越多的关注。随着预训练语言模型(PLMs)的出现，通用文本嵌入(GPTE)因其能够产生丰富、可转移的表示而获得了显著关注。&lt;h4&gt;目的&lt;/h4&gt;提供PLMs时代GPTE的全面概述，重点关注PLMs在推动其发展中的作用，为新手和有经验的研究人员提供有价值的参考，帮助他们理解GPTE的当前状态和未来潜力。&lt;h4&gt;方法&lt;/h4&gt;通过综述形式，首先分析GPTE的基本架构和PLMs的基本作用，然后探讨PLMs带来的高级功能，最后提出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;GPTE通常利用PLMs生成密集文本表示，并通过对比学习进行优化。PLMs在GPTE中扮演着多重角色，包括嵌入提取、表达能力增强、训练策略、学习目标和数据构建。此外，PLMs还实现了多语言支持、多模态集成、代码理解和场景特定适应等高级功能。&lt;h4&gt;结论&lt;/h4&gt;GPTE在PLMs的推动下取得了显著进展，但仍有许多值得探索的方向，包括排名整合、安全考虑、偏见缓解、结构信息整合和嵌入的认知扩展等。&lt;h4&gt;翻译&lt;/h4&gt;文本嵌入因其广泛自然语言处理(NLP)任务中的有效性而受到越来越多的关注，如检索、分类、聚类、双文本挖掘和摘要。随着预训练语言模型(PLMs)的出现，通用文本嵌入(GPTE)因其能够产生丰富、可转移的表示而获得了显著关注。GPTE的通用架构通常利用PLMs来派生密集文本表示，然后通过大规模成对数据集上的对比学习进行优化。在本综述中，我们提供了PLMs时代GPTE的全面概述，重点关注PLMs在推动其发展中的作用。我们首先检查基本架构并描述PLMs在GPTE中的基本作用，即嵌入提取、表达能力增强、训练策略、学习目标和数据构建。然后，我们描述了PLMs实现的高级功能，如多语言支持、多模态集成、代码理解和场景特定适应。最后，我们强调了超越传统改进目标的潜在未来研究方向，包括排名整合、安全考虑、偏见缓解、结构信息整合和嵌入的认知扩展。本综述旨在为寻求理解GPTE当前状态和未来潜力的新手和有经验的研究人员提供有价值的参考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text embeddings have attracted growing interest due to their effectivenessacross a wide range of natural language processing (NLP) tasks, such asretrieval, classification, clustering, bitext mining, and summarization. Withthe emergence of pretrained language models (PLMs), general-purpose textembeddings (GPTE) have gained significant traction for their ability to producerich, transferable representations. The general architecture of GPTE typicallyleverages PLMs to derive dense text representations, which are then optimizedthrough contrastive learning on large-scale pairwise datasets. In this survey,we provide a comprehensive overview of GPTE in the era of PLMs, focusing on theroles PLMs play in driving its development. We first examine the fundamentalarchitecture and describe the basic roles of PLMs in GPTE, i.e., embeddingextraction, expressivity enhancement, training strategies, learning objectives,and data construction. Then, we describe advanced roles enabled by PLMs, suchas multilingual support, multimodal integration, code understanding, andscenario-specific adaptation. Finally, we highlight potential future researchdirections that move beyond traditional improvement goals, including rankingintegration, safety considerations, bias mitigation, structural informationincorporation, and the cognitive extension of embeddings. This survey aims toserve as a valuable reference for both newcomers and established researchersseeking to understand the current state and future potential of GPTE.</description>
      <author>example@mail.com (Meishan Zhang, Xin Zhang, Xinping Zhao, Shouzheng Huang, Baotian Hu, Min Zhang)</author>
      <guid isPermaLink="false">2507.20783v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Counterfactual Learning for Audio-Visual Segmentation</title>
      <link>http://arxiv.org/abs/2507.20740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出隐式反事实框架(ICF)解决视听分割中的模态表示差异和不平衡问题，通过多粒度隐式文本(MIT)建立模态共享空间，语义反事实(SC)学习正交表示，以及协作分布感知对比学习(CDCL)对齐表示，在三个公共数据集上达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;现有视听分割方法主要关注提高交互效率，忽视模态表示差异和不平衡问题。在复杂场景中，异构表示可能导致错误匹配，视觉内容通常占主导地位，边缘化音频特征在决策中的作用。&lt;h4&gt;目的&lt;/h4&gt;实现无偏见的跨模态理解，解决模态表示差异和不平衡问题，减少模态差距，避免复杂功能设计和文本结构修改带来的偏差。&lt;h4&gt;方法&lt;/h4&gt;1) 提出隐式反事实框架(ICF)；2) 引入多粒度隐式文本(MIT)包括视频级、分割级和帧级作为桥梁；3) 提出语义反事实(SC)学习潜在空间中的正交表示；4) 制定协作分布感知对比学习(CDCL)对齐表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过三个公共数据集上的大量实验验证，所提出的方法达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;隐式反事实框架能有效解决视听分割中的模态表示差异和不平衡问题，实现无偏见的跨模态理解。&lt;h4&gt;翻译&lt;/h4&gt;视听分割(AVS)旨在基于音频提示分割视频中的对象。现有的AVS方法主要设计用于提高交互效率，但对模态表示差异和不平衡问题关注有限。为了克服这一点，我们提出了隐式反事实框架(ICF)来实现无偏见的跨模态理解。由于缺乏语义，异构表示可能导致错误匹配，特别是在视觉内容模糊或受多个音频源干扰的复杂场景中。我们引入了涉及视频级、分割级和帧级的多粒度隐式文本(MIT)作为桥梁建立模态共享空间，减少模态差距并提供先验指导。视觉内容携带更多信息并通常占主导地位，从而在决策过程中边缘化音频特征。为了减轻知识偏好，我们提出了语义反事实(SC)来学习潜在空间中的正交表示，生成多样化的反事实样本，从而避免了由复杂功能设计和文本结构或属性的显式修改带来的偏差。我们进一步制定了协作分布感知对比学习(CDCL)，结合事实-反事实和跨模态对比对齐表示，促进内聚和解耦。在三个公共数据集上的大量实验验证了所提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-visual segmentation (AVS) aims to segment objects in videos based onaudio cues. Existing AVS methods are primarily designed to enhance interactionefficiency but pay limited attention to modality representation discrepanciesand imbalances. To overcome this, we propose the implicit counterfactualframework (ICF) to achieve unbiased cross-modal understanding. Due to the lackof semantics, heterogeneous representations may lead to erroneous matches,especially in complex scenes with ambiguous visual content or interference frommultiple audio sources. We introduce the multi-granularity implicit text (MIT)involving video-, segment- and frame-level as the bridge to establish themodality-shared space, reducing modality gaps and providing prior guidance.Visual content carries more information and typically dominates, therebymarginalizing audio features in the decision-making. To mitigate knowledgepreference, we propose the semantic counterfactual (SC) to learn orthogonalrepresentations in the latent space, generating diverse counterfactual samples,thus avoiding biases introduced by complex functional designs and explicitmodifications of text structures or attributes. We further formulate thecollaborative distribution-aware contrastive learning (CDCL), incorporatingfactual-counterfactual and inter-modality contrasts to align representations,promoting cohesion and decoupling. Extensive experiments on three publicdatasets validate that the proposed method achieves state-of-the-artperformance.</description>
      <author>example@mail.com (Mingfeng Zha, Tianyu Li, Guoqing Wang, Peng Wang, Yangyang Wu, Yang Yang, Heng Tao Shen)</author>
      <guid isPermaLink="false">2507.20740v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Attributed Graph Clustering with Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning</title>
      <link>http://arxiv.org/abs/2507.20505v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code for this study is available at  https://github.com/YF-W/MPCCL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了多尺度加权成对粗化与对比学习（MPCCL）模型，一种用于属性图聚类的新方法，有效解决了现有方法中的长程依赖、特征坍塌和信息损失问题。&lt;h4&gt;背景&lt;/h4&gt;传统方法难以捕捉高阶图特征，对比学习技术受限于特征多样性，传统图粗化方法常丢失细粒度结构细节。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的属性图聚类方法，能够捕捉高阶图特征，增强特征多样性，同时保留细粒度结构信息。&lt;h4&gt;方法&lt;/h4&gt;MPCCL采用创新的多尺度粗化策略，基于全局节点相似性合并关键边；引入一对多对比学习范式，集成节点嵌入与增强图视图和聚类中心；结合图重构损失和KL散度确保跨尺度一致性。&lt;h4&gt;主要发现&lt;/h4&gt;MPCCL在聚类性能上取得显著改进，在ACM数据集上NMI提高15.24%，在Citeseer、Cora和DBLP等小规模数据集上表现出显著的稳健性提升。&lt;h4&gt;结论&lt;/h4&gt;MPCCL通过多尺度粗化和对比学习的创新结合，有效解决了属性图聚类中的关键挑战，提高了聚类性能和模型稳健性。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了多尺度加权成对粗化与对比学习（MPCCL）模型，一种用于属性图聚类的新方法，有效弥合了现有方法中的关键差距，包括长程依赖、特征坍塌和信息损失。传统方法往往因其依赖于低阶属性信息而难以捕捉高阶图特征，而对比学习技术通过过度强调局部邻域结构，在特征多样性方面存在局限性。同样，传统的图粗化方法虽然减少了图规模，但经常丢失细粒度的结构细节。MPCCL通过创新的多尺度粗化策略解决这些挑战，该策略逐步压缩图，同时基于全局节点相似性优先合并关键边，以保留必要的结构信息。它进一步引入了一对多对比学习范式，将节点嵌入与增强的图视图和聚类中心集成，以增强特征多样性，同时缓解多尺度粗化过程中高频节点权重积累导致的特征掩码问题。通过在其自监督学习框架中结合图重构损失和KL散度，MPCCL确保了节点表示的跨尺度一致性。实验评估显示，MPCCL在聚类性能方面取得了显著改进，包括在ACM数据集上NMI提高了15.24%，以及在较小规模的数据集如Citeseer、Cora和DBLP上表现出显著的稳健性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.neucom.2025.130796&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces the Multi-Scale Weight-Based Pairwise Coarsening andContrastive Learning (MPCCL) model, a novel approach for attributed graphclustering that effectively bridges critical gaps in existing methods,including long-range dependency, feature collapse, and information loss.Traditional methods often struggle to capture high-order graph features due totheir reliance on low-order attribute information, while contrastive learningtechniques face limitations in feature diversity by overemphasizing localneighborhood structures. Similarly, conventional graph coarsening methods,though reducing graph scale, frequently lose fine-grained structural details.MPCCL addresses these challenges through an innovative multi-scale coarseningstrategy, which progressively condenses the graph while prioritizing themerging of key edges based on global node similarity to preserve essentialstructural information. It further introduces a one-to-many contrastivelearning paradigm, integrating node embeddings with augmented graph views andcluster centroids to enhance feature diversity, while mitigating featuremasking issues caused by the accumulation of high-frequency node weights duringmulti-scale coarsening. By incorporating a graph reconstruction loss and KLdivergence into its self-supervised learning framework, MPCCL ensurescross-scale consistency of node representations. Experimental evaluationsreveal that MPCCL achieves a significant improvement in clustering performance,including a remarkable 15.24% increase in NMI on the ACM dataset and notablerobust gains on smaller-scale datasets such as Citeseer, Cora and DBLP.</description>
      <author>example@mail.com (Binxiong Li, Yuefei Wang, Binyu Zhao, Heyang Gao, Benhan Yang, Quanzhou Luo, Xue Li, Xu Xiang, Yujie Liu, Huijie Tang)</author>
      <guid isPermaLink="false">2507.20505v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>SAViL-Det: Semantic-Aware Vision-Language Model for Multi-Script Text Detection</title>
      <link>http://arxiv.org/abs/2507.20188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAViL-Det的语义感知视觉-语言模型，通过整合文本提示与视觉特征来增强多脚本文本检测，在多语言和弯曲文本数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;自然场景中的文本检测具有挑战性，特别是对于多样化和任意形状的文本实例，仅依靠视觉线索往往不足，现有方法没有充分利用语义上下文。&lt;h4&gt;目的&lt;/h4&gt;引入SAViL-Det模型，通过有效整合文本提示与视觉特征来增强多脚本文本检测能力。&lt;h4&gt;方法&lt;/h4&gt;SAViL-Det利用预训练的CLIP模型与渐近特征金字塔网络结合进行多尺度视觉特征融合，框架核心是一种新颖的语言-视觉解码器，通过跨模态注意力自适应地传播语义信息，并采用文本到像素的对比学习机制对齐文本和视觉像素特征。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的基准测试上进行的广泛实验证明了所提出方法的有效性，在多语言MLT-2019数据集上达到84.8%的F分数，在弯曲文本CTW1500数据集上达到90.2%的F分数。&lt;h4&gt;结论&lt;/h4&gt;SAViL-Det通过语义感知的视觉-语言模型有效提升了自然场景中多脚本文本检测的性能，达到了当前最先进的水平。&lt;h4&gt;翻译&lt;/h4&gt;自然场景中的文本检测仍然具有挑战性，特别是对于多样化和任意形状的文本实例，仅依靠视觉线索往往不足。现有方法没有充分利用语义上下文。本文引入了SAViL-Det，一种新颖的语义感知视觉-语言模型，通过有效整合文本提示与视觉特征来增强多脚本文本检测。SAViL-Det利用预训练的CLIP模型与渐近特征金字塔网络结合进行多尺度视觉特征融合。所提出框架的核心是一种新颖的语言-视觉解码器，通过跨模态注意力自适应地传播从文本提示到视觉特征的细粒度语义信息。此外，文本到像素的对比学习机制明确地对齐文本和相应的视觉像素特征。在具有挑战性的基准测试上进行的广泛实验证明了所提出方法的有效性，在多语言MLT-2019数据集上达到84.8%的F分数，在弯曲文本CTW1500数据集上达到90.2%的F分数，取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting text in natural scenes remains challenging, particularly fordiverse scripts and arbitrarily shaped instances where visual cues alone areoften insufficient. Existing methods do not fully leverage semantic context.This paper introduces SAViL-Det, a novel semantic-aware vision-language modelthat enhances multi-script text detection by effectively integrating textualprompts with visual features. SAViL-Det utilizes a pre-trained CLIP modelcombined with an Asymptotic Feature Pyramid Network (AFPN) for multi-scalevisual feature fusion. The core of the proposed framework is a novellanguage-vision decoder that adaptively propagates fine-grained semanticinformation from text prompts to visual features via cross-modal attention.Furthermore, a text-to-pixel contrastive learning mechanism explicitly alignstextual and corresponding visual pixel features. Extensive experiments onchallenging benchmarks demonstrate the effectiveness of the proposed approach,achieving state-of-the-art performance with F-scores of 84.8% on the benchmarkmulti-lingual MLT-2019 dataset and 90.2% on the curved-text CTW1500 dataset.</description>
      <author>example@mail.com (Mohammed-En-Nadhir Zighem, Abdenour Hadid)</author>
      <guid isPermaLink="false">2507.20188v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis</title>
      <link>http://arxiv.org/abs/2507.20189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出NeuroCLIP，一种新型深度学习框架，整合EEG和fNIRS数据，为甲基苯丙胺成瘾提供更可靠、客观的生物标志物，并用于评估rTMS治疗效果。&lt;h4&gt;背景&lt;/h4&gt;甲基苯丙胺依赖是重大全球健康挑战，目前评估依赖主观自我报告，存在不确定性。虽然EEG和fNIRS提供客观替代方案，但各自局限性和传统特征提取影响生物标志物可靠性。&lt;h4&gt;目的&lt;/h4&gt;克服现有评估方法局限，开发更可靠、客观的甲基苯丙胺成瘾生物标志物，用于评估治疗效果。&lt;h4&gt;方法&lt;/h4&gt;提出NeuroCLIP深度学习框架，通过渐进式学习策略整合同时记录的EEG和fNIRS数据。&lt;h4&gt;主要发现&lt;/h4&gt;1) NeuroCLIP显著提高甲基苯丙胺依赖者与健康对照组的辨别能力；2) 框架可客观评估rTMS治疗效果，显示治疗后神经模式向健康对照组转变；3) 生物标志物与心理验证的渴求评分强相关，具有可信度。&lt;h4&gt;结论&lt;/h4&gt;通过NeuroCLIP从EEG-fNIRS数据衍生的生物标志物比单模态方法更具鲁棒性和可靠性，为成瘾神经科学研究提供有价值工具，可能改善临床评估。&lt;h4&gt;翻译&lt;/h4&gt;甲基苯丙胺依赖构成了重大的全球健康挑战，然而其评估以及对重复经颅磁刺激(rTMS)等治疗的评估常常依赖于主观的自我报告，这可能引入不确定性。虽然脑电图(EEG)和功能性近红外光谱(fNIRS)等客观神经影像学模式提供了替代方案，但它们各自的局限性以及对传统手工制作的特征提取的依赖可能会损害衍生生物标志物的可靠性。为了克服这些局限性，我们提出了NeuroCLIP，一种新型深度学习框架，通过渐进式学习策略整合同时记录的EEG和fNIRS数据。这种方法为甲基苯丙胺成瘾提供了强大而可靠的可信生物标志物。验证实验表明，与仅使用EEG或仅使用fNIRS的模型相比，NeuroCLIP显著提高了甲基苯丙胺依赖个体和健康对照组之间的辨别能力。此外，所提出的框架促进了对rTMS治疗效果的客观、基于大脑的评估，显示出治疗后神经模式向健康对照组模式转变的可测量变化。关键的是，通过显示其与心理测量验证的渴求评分的强相关性，我们建立了这种多模态数据驱动生物标志物的可信度。这些发现表明，通过NeuroCLIP从EEG-fNIRS数据衍生的生物标志物比单模态方法具有更强的鲁棒性和可靠性，为成瘾神经科学研究提供了有价值的工具，并可能改善临床评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Methamphetamine dependence poses a significant global health challenge, yetits assessment and the evaluation of treatments like repetitive transcranialmagnetic stimulation (rTMS) frequently depend on subjective self-reports, whichmay introduce uncertainties. While objective neuroimaging modalities such aselectroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)offer alternatives, their individual limitations and the reliance onconventional, often hand-crafted, feature extraction can compromise thereliability of derived biomarkers. To overcome these limitations, we proposeNeuroCLIP, a novel deep learning framework integrating simultaneously recordedEEG and fNIRS data through a progressive learning strategy. This approachoffers a robust and trustworthy biomarker for methamphetamine addiction.Validation experiments show that NeuroCLIP significantly improvesdiscriminative capabilities among the methamphetamine-dependent individuals andhealthy controls compared to models using either EEG or only fNIRS alone.Furthermore, the proposed framework facilitates objective, brain-basedevaluation of rTMS treatment efficacy, demonstrating measurable shifts inneural patterns towards healthy control profiles after treatment. Critically,we establish the trustworthiness of the multimodal data-driven biomarker byshowing its strong correlation with psychometrically validated craving scores.These findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIPoffers enhanced robustness and reliability over single-modality approaches,providing a valuable tool for addiction neuroscience research and potentiallyimproving clinical assessments.</description>
      <author>example@mail.com (Chengkai Wang, Di Wu, Yunsheng Liao, Wenyao Zheng, Ziyi Zeng, Xurong Gao, Hemmings Wu, Zhoule Zhu, Jie Yang, Lihua Zhong, Weiwei Cheng, Yun-Hsuan Chen, Mohamad Sawan)</author>
      <guid isPermaLink="false">2507.20189v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Improving Audio Classification by Transitioning from Zero- to Few-Shot</title>
      <link>http://arxiv.org/abs/2507.20036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了少样本音频分类方法，以超越零样本方法的分类准确性。通过将音频嵌入按类别分组并处理以替换噪声文本嵌入，结果显示少样本分类通常优于零样本基线。&lt;h4&gt;背景&lt;/h4&gt;最先进的音频分类通常采用零样本方法，涉及将音频嵌入与描述相应音频类别的文本嵌入进行比较。这些嵌入通常通过对比学习训练的神经网络生成，以对齐音频和文本表示。&lt;h4&gt;目的&lt;/h4&gt;研究少样本方法，以超越零样本方法的分类准确性，特别是在音频类别包含各种声音的情况下。&lt;h4&gt;方法&lt;/h4&gt;音频嵌入按类别分组，经过处理以替换固有的噪声文本嵌入，从而改进分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;少样本分类通常优于零样本基线，表明少样本方法在音频分类任务中可能更有效。&lt;h4&gt;结论&lt;/h4&gt;通过采用少样本方法处理音频嵌入，可以克服零样本方法中确定最佳文本描述的挑战，提高音频分类的准确性。&lt;h4&gt;翻译&lt;/h4&gt;最先进的音频分类通常采用零样本方法，涉及将音频嵌入与描述相应音频类别的文本嵌入进行比较。这些嵌入通常通过对比学习训练的神经网络生成，以对齐音频和文本表示。为音频类别确定最佳文本描述具有挑战性，特别是当类别包含各种声音时。本文旨在研究少样本方法，以超越零样本方法的分类准确性。具体而言，音频嵌入按类别分组并经过处理以替换固有的噪声文本嵌入。结果表明，少样本分类通常优于零样本基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art audio classification often employs a zero-shot approach,which involves comparing audio embeddings with embeddings from text describingthe respective audio class. These embeddings are usually generated by neuralnetworks trained through contrastive learning to align audio and textrepresentations. Identifying the optimal text description for an audio class ischallenging, particularly when the class comprises a wide variety of sounds.This paper examines few-shot methods designed to improve classificationaccuracy beyond the zero-shot approach. Specifically, audio embeddings aregrouped by class and processed to replace the inherently noisy text embeddings.Our results demonstrate that few-shot classification typically outperforms thezero-shot baseline.</description>
      <author>example@mail.com (James Taylor, Wolfgang Mack)</author>
      <guid isPermaLink="false">2507.20036v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Visual Analytics Using Tensor Unified Linear Comparative Analysis</title>
      <link>http://arxiv.org/abs/2507.19988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in IEEE Transactions on Visualization and Computer Graphics  and IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为TULCA的新张量分解方法，实现了对张量结构的灵活比较分析，并开发了相应的可视分析界面，通过计算评估和案例研究验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;比较张量并识别它们的(不)相似结构对于理解复杂数据的底层现象是基础性的。张量分解方法帮助分析师提取张量的基本特征，并辅助张量的可视分析。&lt;h4&gt;目的&lt;/h4&gt;解决现有张量分解方法不支持灵活比较分析的问题，引入一种新的张量分解方法来支持张量间的灵活比较。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为张量统一线性比较分析(TULCA)的新方法，通过扩展其降维对应方法ULCA实现。TULCA整合了判别分析和对比学习方案用于张量分解，实现了张量的灵活比较。同时，开发了将核心张量可视化为一系列二维可视化的方法，并将TULCA功能集成到可视分析界面中。&lt;h4&gt;主要发现&lt;/h4&gt;通过计算评估和两个案例研究（包括对超级计算机日志数据的分析）证明了TULCA及其可视分析界面的有效性。&lt;h4&gt;结论&lt;/h4&gt;TULCA方法及其可视分析界面为张量的比较分析提供了有效工具，有助于分析师理解和解释复杂数据中的张量结构。&lt;h4&gt;翻译&lt;/h4&gt;比较张量并识别它们的(不)相似结构对于理解复杂数据的底层现象是基础性的。张量分解方法帮助分析师提取张量的基本特征并辅助张量的可视分析。与仅为分析矩阵（即二阶张量）设计的降维方法不同，现有的张量分解方法不支持灵活的比较分析。为解决这一分析限制，我们引入了一种新的张量分解方法，名为张量统一线性比较分析(TULCA)，通过扩展其降维对应方法ULCA实现张量分析。TULCA整合了判别分析和对比学习方案用于张量分解，实现了张量的灵活比较。我们还介绍了一种将TULCA提取的核心张量可视化为一系列二维可视化的有效方法。我们将TULCA的功能集成到一个可视分析界面中，支持分析师解释和完善TULCA的结果。我们通过计算评估和两个案例研究（包括对从超级计算机收集的日志数据的分析）证明了TULCA和可视分析界面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comparing tensors and identifying their (dis)similar structures isfundamental in understanding the underlying phenomena for complex data. Tensordecomposition methods help analysts extract tensors' essential characteristicsand aid in visual analytics for tensors. In contrast to dimensionalityreduction (DR) methods designed only for analyzing a matrix (i.e., second-ordertensor), existing tensor decomposition methods do not support flexiblecomparative analysis. To address this analysis limitation, we introduce a newtensor decomposition method, named tensor unified linear comparative analysis(TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCAintegrates discriminant analysis and contrastive learning schemes for tensordecomposition, enabling flexible comparison of tensors. We also introduce aneffective method to visualize a core tensor extracted from TULCA into a set of2D visualizations. We integrate TULCA's functionalities into a visual analyticsinterface to support analysts in interpreting and refining the TULCA results.We demonstrate the efficacy of TULCA and the visual analytics interface withcomputational evaluations and two case studies, including an analysis of logdata collected from a supercomputer.</description>
      <author>example@mail.com (Naoki Okami, Kazuki Miyake, Naohisa Sakamoto, Jorji Nonaka, Takanori Fujiwara)</author>
      <guid isPermaLink="false">2507.19988v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation</title>
      <link>http://arxiv.org/abs/2507.19882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了名为DiCap的基于扩散的反事实提示学习框架，通过理论推导和对比学习，实现了能够捕获因果特征的鲁棒提示，在多个任务上表现优异，特别是在未见类别上具有优势。&lt;h4&gt;背景&lt;/h4&gt;提示学习因其高效性受到关注，但现有方法受限于理论基础不足，难以实现因果不变的提示，无法捕获有效跨类别泛化的鲁棒特征。&lt;h4&gt;目的&lt;/h4&gt;解决现有提示学习方法的理论基础不足问题，实现能够捕获因果特征的鲁棒提示，提高模型在未见类别上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出DiCap模型，一个基于扩散的反事实提示学习框架，利用扩散过程从因果模型的边际和条件分布中迭代采样梯度，引导生成满足最小充分性标准的反事实，并采用对比学习框架实现与数据因果特征精确对齐的提示的精细提取。&lt;h4&gt;主要发现&lt;/h4&gt;基于严谨理论推导的方法保证了反事实结果的可识别性并对估计误差施加严格界限；在图像分类、图像文本检索和视觉问答等任务上表现优异，在未见类别上具有特别强的优势。&lt;h4&gt;结论&lt;/h4&gt;DiCap模型通过理论基础的扩散反事实提示学习框架有效解决了现有方法的局限性，实现了能够捕获因果特征的鲁棒提示，显著提高了模型在多个任务上的性能，特别是在未见类别上的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;提示学习因其比传统模型训练和微调更高效而受到关注。然而，现有方法受限于理论基础不足，难以实现因果不变的提示，最终无法捕获能够有效跨类别泛化的鲁棒特征。为解决这些挑战，我们提出了DiCap模型，这是一个有理论基础的基于扩散的反事实提示学习框架，它利用扩散过程从因果模型的边际和条件分布中迭代采样梯度，引导生成满足最小充分性标准的反事实。基于严谨的理论推导，这种方法保证了反事实结果的可识别性，同时对估计误差施加严格界限。我们进一步采用对比学习框架，利用生成的反事实，从而实现与数据因果特征精确对齐的提示的精细提取。大量实验结果表明，我们的方法在图像分类、图像文本检索和视觉问答等任务上表现优异，在未见类别中具有特别强的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prompt learning has garnered attention for its efficiency over traditionalmodel training and fine-tuning. However, existing methods, constrained byinadequate theoretical foundations, encounter difficulties in achievingcausally invariant prompts, ultimately falling short of capturing robustfeatures that generalize effectively across categories. To address thesechallenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoreticallygrounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual$\textbf{p}$rompt learning framework, which leverages a diffusion process toiteratively sample gradients from the marginal and conditional distributions ofthe causal model, guiding the generation of counterfactuals that satisfy theminimal sufficiency criterion. Grounded in rigorous theoretical derivations,this approach guarantees the identifiability of counterfactual outcomes whileimposing strict bounds on estimation errors. We further employ a contrastivelearning framework that leverages the generated counterfactuals, therebyenabling the refined extraction of prompts that are precisely aligned with thecausal features of the data. Extensive experimental results demonstrate thatour method performs excellently across tasks such as image classification,image-text retrieval, and visual question answering, with particularly strongadvantages in unseen categories.</description>
      <author>example@mail.com (Xinshu Li, Ruoyu Wang, Erdun Gao, Mingming Gong, Lina Yao)</author>
      <guid isPermaLink="false">2507.19882v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Towards Domain Specification of Embedding Models in Medicine</title>
      <link>http://arxiv.org/abs/2507.19407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MEDTE的医疗文本嵌入模型和全面的评估基准套件，解决了现有医疗文本嵌入模型的两个关键缺陷：训练数据狭窄和方法论过时，以及评估不足的问题。&lt;h4&gt;背景&lt;/h4&gt;医疗文本嵌入模型是多种医疗应用的基础，包括临床决策支持、生物医学信息检索和医疗问答等。然而，这些模型存在两个关键缺点：首先，大多数模型在狭窄的医疗和生物数据子集上训练，且方法论不够最新，难以捕捉实践中遇到的术语和语义多样性；其次，现有评估往往不充分，即使广泛使用的基准测试也无法在真实世界医疗任务的全谱范围内泛化。&lt;h4&gt;目的&lt;/h4&gt;解决现有医疗文本嵌入模型的两个关键缺陷：训练数据狭窄和方法论过时，以及现有评估不足的问题。&lt;h4&gt;方法&lt;/h4&gt;利用MEDTE模型，一个通过跨多个数据源的自监督对比学习在各种医疗语料库上进行大量微调的GTE模型，提供强大的医疗文本嵌入。同时，提出了一个包含51个任务的全面基准套件，涵盖分类、聚类、配对分类和检索，这些任务基于大规模文本嵌入基准但针对医疗文本的细微差别进行了定制。&lt;h4&gt;主要发现&lt;/h4&gt;这种结合方法不仅建立了强大的评估框架，而且产生的嵌入在不同任务中始终优于最先进的替代方案。&lt;h4&gt;结论&lt;/h4&gt;通过MEDTE模型和全面的评估基准套件，解决了医疗文本嵌入模型的两个关键缺陷，提供了更强大和更适合医疗领域的文本嵌入解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医疗文本嵌入模型是广泛医疗应用的基础，范围从临床决策支持和生物医学信息检索到医疗问答，但它们仍然受到两个关键缺点的阻碍。首先，大多数模型在狭窄的医疗和生物数据子集上训练，同时方法论不够最新，使它们不适合捕捉实践中遇到的术语和语义多样性。其次，现有评估往往不充分：即使广泛使用的基准测试也无法在真实世界医疗任务的全谱范围内泛化。为了解决这些差距，我们利用MEDTE，一个通过跨多个数据源的自监督对比学习在各种医疗语料库上进行大量微调的GTE模型，提供强大的医疗文本嵌入。除了这个模型，我们提出了一个全面的基准套件，包含51个任务，涵盖分类、聚类、配对分类和检索，这些任务基于大规模文本嵌入基准但针对医疗文本的细微差别进行了定制。我们的结果表明，这种结合方法不仅建立了强大的评估框架，而且产生的嵌入在不同任务中始终优于最先进的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical text embedding models are foundational to a wide array of healthcareapplications, ranging from clinical decision support and biomedical informationretrieval to medical question answering, yet they remain hampered by twocritical shortcomings. First, most models are trained on a narrow slice ofmedical and biological data, beside not being up to date in terms ofmethodology, making them ill suited to capture the diversity of terminology andsemantics encountered in practice. Second, existing evaluations are ofteninadequate: even widely used benchmarks fail to generalize across the fullspectrum of real world medical tasks.  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tunedon diverse medical corpora through self-supervised contrastive learning acrossmultiple data sources, to deliver robust medical text embeddings.  Alongside this model, we propose a comprehensive benchmark suite of 51 tasksspanning classification, clustering, pair classification, and retrieval modeledon the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances ofmedical text. Our results demonstrate that this combined approach not onlyestablishes a robust evaluation framework but also yields embeddings thatconsistently outperform state of the art alternatives in different tasks.</description>
      <author>example@mail.com (Mohammad Khodadad, Ali Shiraee, Mahdi Astaraki, Hamidreza Mahyar)</author>
      <guid isPermaLink="false">2507.19407v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>A Markov Categorical Framework for Language Modeling</title>
      <link>http://arxiv.org/abs/2507.19247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://github.com/asiresearch/lm-theory&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文使用马尔可夫范畴框架分析自回归语言模型和NLL目标函数，揭示了NLL训练如何隐式地进行谱对比学习，使模型学习到几何结构化的表示空间。&lt;h4&gt;背景&lt;/h4&gt;自回归语言模型通过分解序列概率并最小化负对数似然(NLL)目标进行训练，尽管在实践中非常强大，但对为什么这一简单目标能产生如此多样表示的深层理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;引入一个使用马尔可夫范畴(MCs)的统一分析框架，解构AR生成过程和NLL目标，以揭示现代语言模型有效性的深层结构原理。&lt;h4&gt;方法&lt;/h4&gt;将单步生成映射建模为Stoch范畴中马尔可夫核的组合，并结合统计散度来解构信息流和学习到的几何结构。&lt;h4&gt;主要发现&lt;/h4&gt;1) 为现代推测解码方法(如EAGLE)提供信息理论依据，量化其利用的隐藏状态信息盈余；2) NLL最小化迫使模型学习数据的内在条件随机性；3) NLL训练本质上是一种谱对比学习，使表示空间与预测相似算子的特征谱对齐。&lt;h4&gt;结论&lt;/h4&gt;通过组合和信息几何的视角，揭示了现代语言模型有效性的深层结构原理，证明NLL训练隐式地学习几何结构化的表示空间。&lt;h4&gt;翻译&lt;/h4&gt;自回归语言模型分解序列概率并通过最小化负对数似然(NLL)目标进行训练。虽然经验上非常强大，但对为什么这一简单目标能产生如此多样表示的深层理论理解仍然有限。本文引入了一个使用马尔可夫范畴(MCs)的统一分析框架，解构AR生成过程和NLL目标。我们将单步生成映射建模为Stoch范畴中马尔可夫核的组合。这种组合视图结合统计散度后，允许我们解构信息流和学习到的几何结构。我们的框架有三个主要贡献。首先，我们为现代推测解码方法(如EAGLE)的成功提供了正式的信息理论依据，量化了这些方法利用的隐藏状态中的信息盈余。其次，我们形式化地说明了NLL最小化如何迫使模型不仅学习下一个token，还学习数据的内在条件随机性，我们使用范畴熵分析这一过程。第三，也是最核心的，我们证明NLL训练本质上是一种谱对比学习。通过分析模型预测头的信息几何，我们表明NLL隐式地迫使学习到的表示空间与预测相似算子的特征谱对齐，从而在没有显式对比对的情况下学习到几何结构化的空间。这种组合和信息几何的视角揭示了现代语言模型有效性的深层结构原理。项目页面：https://github.com/asiresearch/lm-theory&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auto-regressive language models factorize sequence probabilities and aretrained by minimizing the negative log-likelihood (NLL) objective. Whileempirically powerful, a deep theoretical understanding of why this simpleobjective yields such versatile representations remains elusive. This workintroduces a unifying analytical framework using Markov Categories (MCs) todeconstruct the AR generation process and the NLL objective. We model thesingle-step generation map as a composition of Markov kernels in the categoryStoch. This compositional view, when enriched with statistical divergences,allows us to dissect information flow and learned geometry. Our framework makesthree main contributions. First, we provide a formal, information-theoreticrationale for the success of modern speculative decoding methods like EAGLE,quantifying the information surplus in hidden states that these methodsexploit. Second, we formalize how NLL minimization forces the model to learnnot just the next token, but the data's intrinsic conditional stochasticity, aprocess we analyze using categorical entropy. Third, and most centrally, weprove that NLL training acts as an implicit form of spectral contrastivelearning. By analyzing the information geometry of the model's prediction head,we show that NLL implicitly forces the learned representation space to alignwith the eigenspectrum of a predictive similarity operator, thereby learning ageometrically structured space without explicit contrastive pairs. Thiscompositional and information-geometric perspective reveals the deep structuralprinciples underlying the effectiveness of modern LMs. Project Page:https://github.com/asiresearch/lm-theory</description>
      <author>example@mail.com (Yifan Zhang)</author>
      <guid isPermaLink="false">2507.19247v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network</title>
      <link>http://arxiv.org/abs/2507.19095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code for this study is available at  https://github.com/YF-W/GCL-GCN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GCL-GCN的深度图聚类模型，通过创新的Graphormer模块和对比学习模块，有效解决了带属性图聚类中的挑战，显著提高了聚类性能。&lt;h4&gt;背景&lt;/h4&gt;带属性图聚类在现代数据分析中具有重要意义。然而，由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍然具有挑战性。现有模型在处理稀疏和异构图数据时，在捕获局部依赖和复杂结构方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的深度图聚类模型GCL-GCN，旨在解决现有模型在处理稀疏和异构图数据时捕获局部依赖和复杂结构的局限性。&lt;h4&gt;方法&lt;/h4&gt;GCL-GCN引入了创新的Graphormer模块，结合中心编码和空间关系，有效捕获节点间的全局和局部信息；同时提出对比学习模块，通过在原始特征矩阵上进行对比学习来增强特征表示的判别能力，确保后续任务具有更可识别的初始表示。&lt;h4&gt;主要发现&lt;/h4&gt;在六个数据集上的实验结果表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法。在Cora数据集上，与主要对比方法MBN相比，ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。&lt;h4&gt;结论&lt;/h4&gt;GCL-GCN模型通过结合Graphormer模块和对比学习模块，有效解决了带属性图聚类中的挑战，特别是在处理稀疏和异构图数据时表现优异，显著提高了聚类性能。&lt;h4&gt;翻译&lt;/h4&gt;带属性图聚类在现代数据分析中具有重要意义。然而，由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍然具有挑战性。为了解决这个问题，我们提出了一种新颖的深度图聚类模型GCL-GCN，专门设计用于解决现有模型在处理稀疏和异构图数据时捕获局部依赖和复杂结构的局限性。GCL-GCN引入了一个创新的Graphormer模块，该模块结合了中心编码和空间关系，有效捕获节点间的全局和局部信息，从而提高节点表示的质量。此外，我们还提出了一种新颖的对比学习模块，显著增强了特征表示的判别能力。在预训练阶段，该模块通过在原始特征矩阵上进行对比学习来增加特征区分度，确保后续图卷积和聚类任务具有更可识别的初始表示。在六个数据集上的大量实验结果表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法。特别是在Cora数据集上，与主要对比方法MBN相比，ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attributed graph clustering holds significant importance in modern dataanalysis. However, due to the complexity of graph data and the heterogeneity ofnode attributes, leveraging graph information for clustering remainschallenging. To address this, we propose a novel deep graph clustering model,GCL-GCN, specifically designed to address the limitations of existing models incapturing local dependencies and complex structures when dealing with sparseand heterogeneous graph data. GCL-GCN introduces an innovative Graphormermodule that combines centrality encoding and spatial relationships, effectivelycapturing both global and local information between nodes, thereby enhancingthe quality of node representations. Additionally, we propose a novelcontrastive learning module that significantly enhances the discriminativepower of feature representations. In the pre-training phase, this moduleincreases feature distinction through contrastive learning on the originalfeature matrix, ensuring more identifiable initial representations forsubsequent graph convolution and clustering tasks. Extensive experimentalresults on six datasets demonstrate that GCL-GCN outperforms 14 advancedmethods in terms of clustering quality and robustness. Specifically, on theCora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,respectively, compared to the primary comparison method MBN.</description>
      <author>example@mail.com (Binxiong Li, Xu Xiang, Xue Li, Binyu Zhao, Yujie Liu, Huijie Tang, Benhan Yang, Zhixuan Chen)</author>
      <guid isPermaLink="false">2507.19095v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition</title>
      <link>http://arxiv.org/abs/2507.18929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACMMM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多粒度层次融合变压器(MGHFT)模型，用于解决贴纸情感理解中的挑战，通过多模态大型语言模型和层次融合策略显著提升了贴纸情感识别的准确性和细粒度。&lt;h4&gt;背景&lt;/h4&gt;预训练的视觉模型在视觉特征提取方面表现出色，但贴纸情感理解仍然具有挑战性，因为它依赖于多视角信息，如背景知识和风格线索。&lt;h4&gt;目的&lt;/h4&gt;解决贴纸情感理解中的挑战，提出一种新的多粒度层次融合变压器(MGHFT)模型，提高贴纸情感识别的准确率和细粒度。&lt;h4&gt;方法&lt;/h4&gt;1) 使用多模态大型语言模型通过多视角描述提供丰富的文本上下文来解释贴纸；2) 设计层次融合策略将文本上下文融合到视觉理解中；3) 基于金字塔视觉变压器构建，在多个阶段提取全局和局部贴纸特征；4) 通过对比学习和注意力机制，在视觉主干的不同阶段注入文本特征；5) 引入文本引导的融合注意力机制有效整合整体多模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公开的贴纸情感数据集上进行了大量实验，MGHFT显著优于现有的贴纸情感识别方法，实现了更高的准确率和更细粒度的情感识别。与最佳预训练视觉模型相比，MGHFT在F1上提高了5.4%，在准确率上提高了4.0%。&lt;h4&gt;结论&lt;/h4&gt;MGHFT模型通过多粒度层次融合策略和多模态大型语言模型的应用，在贴纸情感识别任务上表现出色，代码已公开发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管具有文本预训练的视觉模型在视觉特征提取方面展示了强大的能力，但贴纸情感理解仍然具有挑战性，因为它依赖于多视角信息，如背景知识和风格线索。为了解决这个问题，我们提出了一种新颖的多粒度层次融合变压器(MGHFT)，并基于多模态大型语言模型构建了多视角贴纸解释器。具体来说，受人类从多视角解释贴纸情感能力的启发，我们首先使用多模态大型语言模型通过多视角描述提供丰富的文本上下文来解释贴纸。然后，我们设计了一种层次融合策略，将文本上下文融合到视觉理解中，该策略基于金字塔视觉变压器，在多个阶段提取全局和局部贴纸特征。通过对比学习和注意力机制，文本特征被注入到视觉主干的不同阶段，增强了全局和局部粒度视觉语义与文本指导的融合。最后，我们引入了文本引导的融合注意力机制，以有效整合整体多模态特征，增强语义理解。在两个公开的贴纸情感数据集上的大量实验表明，MGHFT显著优于现有的贴纸情感识别方法，实现了更高的准确率和更细粒度的情感识别。与最佳预训练视觉模型相比，我们的MGHFT在F1上获得了明显的提升5.4%，在准确率上提升了4.0%。代码已在https://github.com/cccccj-03/MGHFT_ACMMM2025发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although pre-trained visual models with text have demonstrated strongcapabilities in visual feature extraction, sticker emotion understandingremains challenging due to its reliance on multi-view information, such asbackground knowledge and stylistic cues. To address this, we propose a novelmulti-granularity hierarchical fusion transformer (MGHFT), with a multi-viewsticker interpreter based on Multimodal Large Language Models. Specifically,inspired by the human ability to interpret sticker emotions from multipleviews, we first use Multimodal Large Language Models to interpret stickers byproviding rich textual context via multi-view descriptions. Then, we design ahierarchical fusion strategy to fuse the textual context into visualunderstanding, which builds upon a pyramid visual transformer to extract bothglobal and local sticker features at multiple stages. Through contrastivelearning and attention mechanisms, textual features are injected at differentstages of the visual backbone, enhancing the fusion of global- andlocal-granularity visual semantics with textual guidance. Finally, we introducea text-guided fusion attention mechanism to effectively integrate the overallmultimodal features, enhancing semantic understanding. Extensive experiments on2 public sticker emotion datasets demonstrate that MGHFT significantlyoutperforms existing sticker emotion recognition approaches, achieving higheraccuracy and more fine-grained emotion recognition. Compared to the bestpre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%on F1 and 4.0% on accuracy. The code is released athttps://github.com/cccccj-03/MGHFT_ACMMM2025.</description>
      <author>example@mail.com (Jian Chen, Yuxuan Hu, Haifeng Lu, Wei Wang, Min Yang, Chengming Li, Xiping Hu)</author>
      <guid isPermaLink="false">2507.18929v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?</title>
      <link>http://arxiv.org/abs/2507.18881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过注入3D几何先验到视觉平面图定位(FLoc)算法中的方法，解决了现有方法无法处理频繁视觉变化和视图遮挡导致的定位错误问题。&lt;h4&gt;背景&lt;/h4&gt;建筑物的平面图易于获取、随时间保持一致且对视觉变化具有鲁棒性，因此平面图自定位引起了研究兴趣。然而，平面图作为建筑物结构的最简表示，视觉感知与平面图之间的模态和几何差异给这一任务带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法处理由各种形状3D物体引起的频繁视觉变化和视图遮挡所导致的定位错误问题。&lt;h4&gt;方法&lt;/h4&gt;从更高维度看待2D平面图定位问题，通过以下两种3D几何先验建模：1)使用多视图约束建模几何感知视图不变性；2)建模视图-场景对齐的几何先验。两种先验均通过自监督对比学习实现，无需额外几何或语义标注。&lt;h4&gt;主要发现&lt;/h4&gt;这些3D先验在大量真实场景中弥合了模态差距，提高了定位成功率，且未增加计算负担。比较研究表明，该方法显著优于最先进方法，大幅提升了FLoc准确性。&lt;h4&gt;结论&lt;/h4&gt;通过引入3D几何先验，有效解决了现有方法在处理视觉变化和遮挡时的局限性，实现了更准确的平面图定位。&lt;h4&gt;翻译&lt;/h4&gt;由于建筑物的平面图易于获取，随时间保持一致，并且对视觉外观变化具有内在鲁棒性，平面图内的自定位引起了研究人员的兴趣。然而，由于平面图是对建筑物结构的最简表示，视觉感知与平面图之间的模态和几何差异给这一任务带来了挑战。虽然现有方法巧妙地利用2D几何特征和姿态滤波器实现了有希望的性能，但它们无法解决由各种形状的3D物体引起的频繁视觉变化和视图遮挡导致的位置错误。为了解决这些问题，本文通过将3D几何先验注入视觉FLoc算法，从更高维度看待2D平面图定位(FLoc)问题。对于3D几何先验建模，我们首先使用多视图约束建模几何感知视图不变性，即利用成像几何原理为看到相同点的多个图像提供匹配约束。然后，我们进一步建模视图-场景对齐的几何先验，通过将场景的表面重建与序列的RGB帧相关联来增强跨模态几何-颜色对应关系。这两种3D先验都通过自监督对比学习进行建模，因此不需要额外的几何或语义标注。这些在大量真实场景中总结的3D先验弥合了模态差距，同时提高了定位成功率，而没有增加FLoc算法的计算负担。充分的比较研究表明，我们的方法显著优于最先进的方法，并大幅提高了FLoc的准确性。所有数据和代码将在匿名评审后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉平面图定位(FLoc)任务中的挑战，即如何让设备仅使用2D平面图作为地图，通过视觉信息(RGB图像)来确定自身位置。这个问题很重要，因为平面图是建筑物中容易获取、随时间保持一致且对视觉外观变化具有鲁棒性的信息源，对于机器人导航和AR/VR应用非常有价值。然而，由于各种形状的3D物体导致的视觉变化和视图遮挡问题，现有方法难以准确进行定位。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从'更高维度'的视角看待2D FLoc问题，尝试通过注入3D几何先验来增强视觉FLoc算法。他们借鉴了现有的对比学习技术和F3Loc框架(一个经典的前端-后端FLoc框架)，但没有直接使用现有方法，而是设计了两种新的3D几何先验建模技术：Geometry-Constrained View Invariance(GCVI)和View-Scene Aligned Geometric(VSAG) prior。作者利用多视图几何原理和表面重建信息来增强视觉特征表示，使其更好地适应平面图定位任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入3D几何先验来弥合视觉图像和平面图之间的模态差距，提高定位的鲁棒性和准确性。整体实现流程包括：1)预训练阶段，使用RGB-D数据集进行对比学习，建模GCVI和VSAG先验；2)微调阶段，将预训练好的视觉编码器转移到下游FLoc任务进行微调，整合3D几何先验到观察模型中；3)定位过程，对于单帧或多帧视觉信息，通过找到与预测最相似的2D射线来确定在平面图中的位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将3D几何先验引入单目视觉FLoc任务；2)提出两种新的3D几何先验建模技术(GCVI和VSAG)；3)使用硬几何约束(像素/点级对齐)进行对比学习；4)通过自监督对比学习实现，无需额外标注；5)不增加计算负担提高定位成功率。相比之前的工作，不同之处在于：利用3D空间中的成像几何约束关联视觉特征与3D空间；在高精度任务中施加更严格的几何约束；不需要昂贵的标签和注释。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入两种新颖的3D几何先验建模技术(GCVI和VSAG)，显著提高了视觉平面图定位的鲁棒性和准确性，同时无需额外标注且不增加计算负担。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since a building's floorplans are easily accessible, consistent over time,and inherently robust to changes in visual appearance, self-localization withinthe floorplan has attracted researchers' interest. However, since floorplansare minimalist representations of a building's structure, modal and geometricdifferences between visual perceptions and floorplans pose challenges to thistask. While existing methods cleverly utilize 2D geometric features and posefilters to achieve promising performance, they fail to address the localizationerrors caused by frequent visual changes and view occlusions due to variouslyshaped 3D objects. To tackle these issues, this paper views the 2D FloorplanLocalization (FLoc) problem from a higher dimension by injecting 3D geometricpriors into the visual FLoc algorithm. For the 3D geometric prior modeling, wefirst model geometrically aware view invariance using multi-view constraints,i.e., leveraging imaging geometric principles to provide matching constraintsbetween multiple images that see the same points. Then, we further model theview-scene aligned geometric priors, enhancing the cross-modal geometry-colorcorrespondences by associating the scene's surface reconstruction with the RGBframes of the sequence. Both 3D priors are modeled through self-supervisedcontrastive learning, thus no additional geometric or semantic annotations arerequired. These 3D priors summarized in extensive realistic scenes bridge themodal gap while improving localization success without increasing thecomputational burden on the FLoc algorithm. Sufficient comparative studiesdemonstrate that our method significantly outperforms state-of-the-art methodsand substantially boosts the FLoc accuracy. All data and code will be releasedafter the anonymous review.</description>
      <author>example@mail.com (Bolei Chen, Jiaxu Kang, Haonan Yang, Ping Zhong, Jianxin Wang)</author>
      <guid isPermaLink="false">2507.18881v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>CLEAR: Unlearning Spurious Style-Content Associations with Contrastive LEarning with Anti-contrastive Regularization</title>
      <link>http://arxiv.org/abs/2507.18794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages main text, 24 pages in total&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CLEAR（具有反对比正则化的对比学习）框架，用于学习不受表面特征影响的表示，确保在测试时这些特征的变化不会损害下游预测性能。该方法通过在训练期间分离本质特征与表面特征，提高了模型在不同人口统计群体上的公平性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在医疗保健等应用中，模型学习到的特征可能受到种族、性别等生理变异性的影响，导致预测在不同人口统计群体上表现不一致，影响公平性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种直观且易于实现的框架，能够在训练期间有效分离本质（与任务相关）特征和表面（与任务无关）特征，从而在测试时表面特征发生变化的情况下获得更好的性能。&lt;h4&gt;方法&lt;/h4&gt;假设数据表示可分离为内容特征（与任务相关信息）和风格特征（表面属性）。提出Pair-Switching (PS)反对比惩罚，最小化风格属性和内容标签间的互信息。在变分自编码器（VAE）的潜在空间中实现CLEAR，并在多个图像数据集上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;CLEAR-VAE能够在任何样本对之间交换和插值内容与风格；在存在先前未见过的内容和风格组合的情况下，显著改进下游分类性能。&lt;h4&gt;结论&lt;/h4&gt;CLEAR框架有效解决了表面特征干扰问题，提高了模型公平性和泛化能力，在各种图像数据集上均表现出色，代码将公开提供。&lt;h4&gt;翻译&lt;/h4&gt;学习不受表面特征影响的表示对于确保这些特征在测试时的变化不会损害下游预测性能很重要。例如，在医疗保健应用中，我们可能希望学习包含病理信息但不受种族、性别和其他生理变异性影响的特征，从而确保预测对所有人口统计群体都是公平且可推广的。在这里，我们提出了具有反对比正则化的对比学习（CLEAR），这是一个直观且易于实现的框架，能够在训练期间有效分离本质（即与任务相关）特征和表面（与任务无关）特征，从而在测试时表面特征发生变化时获得更好的性能。我们首先假设数据表示可以在语义上分离为与任务相关的内容特征，这些特征包含与下游任务相关的信息，以及与任务无关的风格特征，这些特征包含与这些任务无关的表面属性，但由于与训练数据中存在但不推广的内容相关联，可能会降低性能。然后我们证明了我们的反对比惩罚（我们称之为Pair-Switching (PS)）最小化了风格属性和内容标签之间的互信息。最后，我们在变分自编码器（VAE）的潜在空间中实现了CLEAR，然后在几个图像数据集上对 resulting CLEAR-VAE 进行定量和定性评估。我们的结果表明，CLEAR-VAE允许我们：(a) 在任何样本对之间交换和插值内容和风格，以及 (b) 在存在先前未见过的内容和风格组合的情况下改进下游分类性能。我们的代码将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning representations unaffected by superficial characteristics isimportant to ensure that shifts in these characteristics at test time do notcompromise downstream prediction performance. For instance, in healthcareapplications, we might like to learn features that contain information aboutpathology yet are unaffected by race, sex, and other sources of physiologicvariability, thereby ensuring predictions are equitable and generalizableacross all demographics. Here we propose Contrastive LEarning withAnti-contrastive Regularization (CLEAR), an intuitive and easy-to-implementframework that effectively separates essential (i.e., task-relevant)characteristics from superficial (i.e., task-irrelevant) characteristics duringtraining, leading to better performance when superficial characteristics shiftat test time. We begin by supposing that data representations can besemantically separated into task-relevant content features, which containinformation relevant to downstream tasks, and task-irrelevant style features,which encompass superficial attributes that are irrelevant to these tasks, yetmay degrade performance due to associations with content present in trainingdata that do not generalize. We then prove that our anti-contrastive penalty,which we call Pair-Switching (PS), minimizes the Mutual Information between thestyle attributes and content labels. Finally, we instantiate CLEAR in thelatent space of a Variational Auto-Encoder (VAE), then perform experiments toquantitatively and qualitatively evaluate the resulting CLEAR-VAE over severalimage datasets. Our results show that CLEAR-VAE allows us to: (a) swap andinterpolate content and style between any pair of samples, and (b) improvedownstream classification performance in the presence of previously unseencombinations of content and style. Our code will be made publicly available.</description>
      <author>example@mail.com (Minghui Sun, Benjamin A. Goldstein, Matthew M. Engelhard)</author>
      <guid isPermaLink="false">2507.18794v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling</title>
      <link>http://arxiv.org/abs/2507.18176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个两阶段框架，使用无监督域适应解决3D激光雷达语义分割中的域偏移问题，结合对比预训练和多模型伪标签策略，无需目标域标注即可提高分割准确率。&lt;h4&gt;背景&lt;/h4&gt;3D激光雷达语义分割中由于域偏移（如传感器类型、地理位置）导致的性能下降问题对于自主系统至关重要，但目标数据的手动标注成本过高。&lt;h4&gt;目的&lt;/h4&gt;解决3D激光雷达语义分割中的域偏移问题，无需目标域标注即可提高分割准确率。&lt;h4&gt;方法&lt;/h4&gt;提出一个两阶段框架，第一阶段使用无监督对比学习预训练骨干网络学习鲁棒特征，第二阶段引入多模型伪标签策略，通过硬投票聚合不同架构模型的预测生成高质量伪标签，并用这些伪标签微调网络。&lt;h4&gt;主要发现&lt;/h4&gt;从SemanticKITTI到未标记目标数据集（SemanticPOSS、SemanticSlamantic）的实验表明，与直接传输和单模型UDA方法相比，该方法显著提高了分割准确率。&lt;h4&gt;结论&lt;/h4&gt;结合对比预训练和精细化的集合伪标签可以有效弥合复杂域差距，而无需目标域标注。&lt;h4&gt;翻译&lt;/h4&gt;解决由于域偏移（如传感器类型、地理位置）导致的3D激光雷达语义分割性能下降对自主系统至关重要，但目标数据的手动标注成本过高。本研究使用无监督域适应解决这一挑战，并提出了一个新颖的两阶段框架。首先，使用无监督对比学习在分割级别预训练骨干网络，使其能够在没有标签的情况下学习鲁棒的、域不变的特征。随后，引入多模型伪标签策略，利用多样化的最先进架构集合（包括投影、体素、混合和基于圆柱体的方法）。通过硬投票聚合这些模型的预测，为未标记的目标域生成高质量、精细化的伪标签，减轻单模型偏差。然后使用这些鲁棒的伪标签对对比预训练的网络进行微调。实验将SemanticKITTI适应到未标记的目标数据集（SemanticPOSS、SemanticSlamantic），与直接传输和单模型UDA方法相比，显示出分割准确率的显著提高。这些结果突显了结合对比预训练和精细化的集合伪标签来弥合复杂域差距的有效性，而无需目标域标注。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.24407/KXP:1928486487&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Addressing performance degradation in 3D LiDAR semantic segmentation due todomain shifts (e.g., sensor type, geographical location) is crucial forautonomous systems, yet manual annotation of target data is prohibitive. Thisstudy addresses the challenge using Unsupervised Domain Adaptation (UDA) andintroduces a novel two-stage framework to tackle it. Initially, unsupervisedcontrastive learning at the segment level is used to pre-train a backbonenetwork, enabling it to learn robust, domain-invariant features without labels.Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizingan ensemble of diverse state-of-the-art architectures (including projection,voxel, hybrid, and cylinder-based methods). Predictions from these models areaggregated via hard voting to generate high-quality, refined pseudo-labels forthe unlabeled target domain, mitigating single-model biases. The contrastivelypre-trained network is then fine-tuned using these robust pseudo-labels.Experiments adapting from SemanticKITTI to unlabeled target datasets(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements insegmentation accuracy compared to direct transfer and single-model UDAapproaches. These results highlight the effectiveness of combining contrastivepre-training with refined ensemble pseudo-labeling for bridging complex domaingaps without requiring target domain annotations.</description>
      <author>example@mail.com (Abhishek Kaushik, Norbert Haala, Uwe Soergel)</author>
      <guid isPermaLink="false">2507.18176v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift</title>
      <link>http://arxiv.org/abs/2507.18653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025, 2COOOL Workshop. Total 14 pages, 5 tables, and  4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决车道检测模型在跨数据集分布偏移下灾难性遗忘问题的方法，通过创建分支结构实现参数高效的适应，并在推理时使用监督对比学习动态路由输入到相应分支，实现了接近最优的F1分数同时使用更少的参数。&lt;h4&gt;背景&lt;/h4&gt;车道检测模型通常在封闭世界环境中评估，训练和测试使用相同数据集。即使在同一领域内，跨数据集分布偏移也会在微调过程中导致严重的灾难性遗忘问题。&lt;h4&gt;目的&lt;/h4&gt;解决车道检测模型在跨数据集分布偏移下的灾难性遗忘问题，实现参数高效的适应，同时保持接近最优的性能。&lt;h4&gt;方法&lt;/h4&gt;首先在源分布上训练基础模型，然后通过创建单独分支的方式使其适应每个新的目标分布，仅微调选定的组件同时保持原始源分支固定。基于组件级分析，为目标分布确定有效的微调策略。在推理时，使用监督对比学习模型识别输入分布并将其动态路由到相应分支。&lt;h4&gt;主要发现&lt;/h4&gt;跨数据集分布偏移会导致严重的灾难性遗忘问题；通过创建分支结构和选择性微调可以实现参数高效的适应；使用监督对比学习进行动态路由是有效的。&lt;h4&gt;结论&lt;/h4&gt;该框架在使用显著少于为每个分布训练单独模型的参数的情况下，实现了接近最优的F1分数。&lt;h4&gt;翻译&lt;/h4&gt;车道检测模型通常在封闭世界环境中进行评估，训练和测试发生在相同数据集上。我们观察到，即使在同一领域内，跨数据集分布偏移也会在微调过程中导致严重的灾难性遗忘。为解决这一问题，我们首先在源分布上训练基础模型，然后通过创建单独分支的方式将其适应到每个新的目标分布，仅微调选定的组件同时保持原始源分支固定。基于组件级分析，我们确定了为目标分布实现参数高效适应的有效微调策略。在推理时，我们提出使用监督对比学习模型来识别输入分布并动态将其路由到相应分支。我们的框架在使用显著少于为每个分布训练单独模型的参数的情况下，实现了接近最优的F1分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lane detection models are often evaluated in a closed-world setting, wheretraining and testing occur on the same dataset. We observe that, even withinthe same domain, cross-dataset distribution shifts can cause severecatastrophic forgetting during fine-tuning. To address this, we first train abase model on a source distribution and then adapt it to each new targetdistribution by creating separate branches, fine-tuning only selectedcomponents while keeping the original source branch fixed. Based on acomponent-wise analysis, we identify effective fine-tuning strategies fortarget distributions that enable parameter-efficient adaptation. At inferencetime, we propose using a supervised contrastive learning model to identify theinput distribution and dynamically route it to the corresponding branch. Ourframework achieves near-optimal F1-scores while using significantly fewerparameters than training separate models for each distribution.</description>
      <author>example@mail.com (Mohammed Abdul Hafeez Khan, Parth Ganeriwala, Sarah M. Lehman, Siddhartha Bhattacharyya, Amy Alvarez, Natasha Neogi)</author>
      <guid isPermaLink="false">2507.18653v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation</title>
      <link>http://arxiv.org/abs/2507.20589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于桁架结构中可导航表面检测的方法，以提高攀爬机器人的自主导航能力。研究比较了分析算法和深度学习方法，发现PointTransformerV3模型在分割准确度上表现最佳，达到了约97%的平均交并比(mIoU)。&lt;h4&gt;背景&lt;/h4&gt;网状结构是桥梁、塔台和机场等主要基础设施的骨架，但其检查和维护成本高昂且危险，通常需要人工干预。先前的研究主要集中在通过图像进行故障检测或机器人平台设计方面，而对这些结构内机器人的自主导航探索较少。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在填补这一空白，提出用于桁架结构中可导航表面检测的方法，增强攀爬机器人的自主性。&lt;h4&gt;方法&lt;/h4&gt;论文介绍了几种从金属桁架3D点云中对可导航表面与背景进行二元分割的方法，分为两类：1)分析算法：通过分析点云中平面块的特征分解进行结构分割的自定义算法；2)深度学习模型：训练并评估了PointNet、PointNet++、MinkUNet34C和PointTransformerV3等先进深度学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;分析算法具有更简单的参数调整和与深度学习模型相当的性能；深度学习模型虽然计算密集度更高，但在分割准确度上表现出色；PointTransformerV3实现了约97%的平均交并比(mIoU)。&lt;h4&gt;结论&lt;/h4&gt;研究表明分析方法和深度学习方法在改善复杂桁架环境中的自主导航方面都有潜力。研究结果突出了计算效率与分割性能之间的权衡，为未来研究和基础设施自主检查与维护的实际应用提供了有价值的指导。&lt;h4&gt;翻译&lt;/h4&gt;网状结构构成了桥梁、塔台和机场等主要基础设施的骨架，但它们的检查和维护成本高昂且危险，通常需要人工干预。虽然先前的研究已通过图像或机器人平台设计专注于故障检测，但这些结构内机器人的自主导航探索较少。本研究通过提出用于桁架结构中可导航表面检测的方法来解决这一差距，增强了攀爬机器人的自主性。论文介绍了几种从金属桁架3D点云中对可导航表面与背景进行二元分割的方法。这些方法分为两类：分析算法和深度学习模型。分析方法的特点是自定义算法，通过分析点云中平面块的特征分解来分割结构。同时，训练并评估了先进的深度学习模型PointNet、PointNet++、MinkUNet34C和PointTransformerV3来完成相同任务。比较分析显示，分析算法提供了更简单的参数调整和与深度学习模型相当的性能，而深度学习模型虽然计算密集度更高，但在分割准确度上表现出色。值得注意的是，PointTransformerV3实现了约97%的平均交并比(mIoU)。研究表明分析方法和深度学习方法在改善复杂桁架环境中的自主导航方面都有潜力。结果突出了计算效率与分割性能之间的权衡，为未来研究和基础设施自主检查与维护的实际应用提供了有价值的指导。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D LiDAR数据中分割网状结构（如桥梁、电线杆等基础设施的桁架结构），特别是识别可导航表面，以便攀爬机器人能在这些复杂环境中自主导航。这个问题很重要，因为传统检查和维护这些结构既昂贵又危险，工人面临坠落、高压等风险；同时，现有研究多集中在故障检测或机器人设计上，忽视了机器人在结构中的自主导航能力，而准确的空间信息分割对自主攀爬机器人成功执行检查、维护和导航任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前研究在网状结构自主导航方面的不足，评估了RGB-D相机和LiDAR传感器的优缺点并选择后者。设计方法分为两种范式：分析算法和深度学习方法。作者借鉴了大量现有工作，包括点云处理技术（如RANSAC平面检测、区域生长、PCA）、现有深度学习模型架构（PointNet、PointNet++等）以及数据集生成方法。同时，作者也有创新设计，如开发了专门的Gazebo插件自动生成标记数据集，设计了针对网状结构的两步分析算法，并对多种深度学习模型进行了系统比较。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想：分析算法通过几何特征分析识别可导航表面，利用点云中的平面特征和结构特征进行分割；深度学习方法则利用神经网络自动学习点云特征实现语义分割。整体流程：分析算法采用两步法，先通过RANSAC检测地面平面进行粗略分割，再对地面点进行区域生长聚类和特征值评估进行精细分割；深度学习方法则是使用Gazebo生成训练和测试数据集，训练多种深度学习模型，通过F1分数和mIoU等指标评估性能，并比较不同输入特征的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 全面比较分析算法和深度学习方法在网状结构分割上的性能；2) 提出专门的两步分析算法；3) 开发Gazebo插件自动生成标记数据集；4) 专注于二值分割解决攀爬机器人自主导航问题。相比之前工作：1) 研究重点从故障检测转向自主导航能力；2) 同时评估多种分析算法和深度学习模型；3) 实现完全自动化的数据生成和标记；4) 系统分析不同输入特征对分割性能的影响；5) 明确针对攀爬机器人在网状结构中的导航需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过系统比较分析算法和深度学习方法，提出了一种高效的网状结构分割框架，显著提升了攀爬机器人在复杂基础设施结构中的自主导航能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.32604/cmes.2025.064510&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reticular structures form the backbone of major infrastructure like bridges,pylons, and airports, but their inspection and maintenance are costly andhazardous, often requiring human intervention. While prior research has focusedon fault detection via images or robotic platform design, the autonomousnavigation of robots within these structures is less explored. This studyaddresses that gap by proposing methods to detect navigable surfaces in trussstructures, enhancing the autonomy of climbing robots. The paper introducesseveral approaches for binary segmentation of navigable surfaces versusbackground from 3D point clouds of metallic trusses. These methods fall intotwo categories: analytical algorithms and deep learning models. The analyticalapproach features a custom algorithm that segments structures by analyzing theeigendecomposition of planar patches in the point cloud. In parallel, advanceddeep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3are trained and evaluated for the same task. Comparative analysis shows thatthe analytical algorithm offers easier parameter tuning and performancecomparable to deep learning models, which, while more computationallyintensive, excel in segmentation accuracy. Notably, PointTransformerV3 achievesa Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates thepromise of both analytical and deep learning methods for improving autonomousnavigation in complex truss environments. The results highlight the trade-offsbetween computational efficiency and segmentation performance, providingvaluable guidance for future research and practical applications in autonomousinfrastructure inspection and maintenance.</description>
      <author>example@mail.com (Francisco J. Soler Mora, Adrián Peidró Vidal, Marc Fabregat-Jaén, Luis Payá Castelló, Óscar Reinoso García)</author>
      <guid isPermaLink="false">2507.20589v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments</title>
      <link>http://arxiv.org/abs/2507.20538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Uni-Mapper，一个针对多模态LiDAR系统的动态感知3D点云地图合并框架，用于解决不同传感器模态和动态环境下的地图统一问题。&lt;h4&gt;背景&lt;/h4&gt;不同地图的统一对于机器人在多会话和协作多机器人场景中的可扩展操作至关重要，但实现对传感器模态和动态环境具有鲁棒性的统一地图仍具挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同LiDAR类型和动态环境变化的地图合并方法，实现可靠的位置识别和精确的地图对齐。&lt;h4&gt;方法&lt;/h4&gt;Uni-Mapper框架包含动态物体移除、动态感知闭环和多模态LiDAR地图合并模块；构建体素级自由空间哈希图识别动态物体；集成LiDAR全局描述符编码静态特征；采用集中式锚节点策略进行位姿图优化以减轻漂移误差。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在跨传感器模态的环路检测、动态环境中的鲁棒映射和精确多地图对齐方面表现出色，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;Uni-Mapper有效解决了多模态LiDAR系统在动态环境中的地图统一问题，为机器人在多会话和协作场景中的可扩展操作提供了支持。&lt;h4&gt;翻译&lt;/h4&gt;不同地图的统一对于实现机器人在多会话和协作多机器人场景中的可扩展操作至关重要。然而，实现一个对传感器模态和动态环境具有鲁棒性的统一地图仍然是一个具有挑战性的问题。LiDAR类型和动态元素的变化导致点云分布和场景一致性的差异，阻碍了可靠描述符生成和闭环检测，这对于精确的地图对齐至关重要。为了解决这些挑战，本文提出了Uni-Mapper，一个针对多模态LiDAR系统的动态感知3D点云地图合并框架。它包括动态物体移除、动态感知闭环和多模态LiDAR地图合并模块。构建了一个从粗到细的体素级自由空间哈希图，通过时间占用不一致性来识别和拒绝动态物体。移除模块与LiDAR全局描述符集成，该描述符编码保存的静态局部特征，以确保在动态环境中的稳健位置识别。在最后阶段，对会话内和地图间闭环进行多次位姿图优化。我们采用集中式锚节点策略来减轻地图合并过程中的会话内漂移误差。在最后阶段，执行基于集中式锚节点的位姿图优化，以解决全局一致地图合并的内部和地图间闭环问题。我们在具有动态物体和异构LiDAR的各种真实世界数据集上评估了该框架，显示出在跨传感器模态的环路检测、动态环境中的鲁棒映射和精确多地图对齐方面优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态激光雷达在复杂动态环境中的地图统一问题。不同类型激光雷达采集的点云数据分布不同，同时环境中的移动物体会在地图中留下'鬼影'轨迹，这两个因素使得地图融合变得困难。这个问题在现实应用中很重要，因为统一的地图对机器人导航、多机器人协作和长期SLAM系统至关重要，能够支持更大规模的机器人操作和更复杂的环境应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有动态物体移除和地点识别方法的局限性，然后注意到多模态激光雷达融合是研究空白。他们借鉴了多种现有方法：动态物体移除部分结合了基于占用网格、基于差异和学习方法的优点；地点识别部分基于STD方法改进为DynaSTD；地图融合部分借鉴了pose graph优化技术但引入了锚节点策略来减少漂移。作者的创新在于将这些技术整合到一个统一框架中，同时处理动态物体移除和地图融合问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用统一的体素表示同时处理动态物体移除和场景描述，通过粗到细的体素网格策略高效估计自由空间，将动态移除与地点识别结合提高鲁棒性，并使用锚节点策略减少地图合并中的漂移误差。整体流程分为三模块：1) 在线动态物体移除（预处理、动态候选选择、自由空间估计、动态点云分割）；2) 动态感知回环闭合（场景描述、回环检测、会内姿态图优化）；3) 多模态激光雷达地图合并（多地图对齐、锚节点策略、两阶段配准）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的多模态激光雷达地图合并框架，可同时处理动态移除和地图融合；2) 两级体素表示实现高效在线静态映射；3) DynaSTD描述符提高跨模态鲁棒性；4) 锚节点策略和两阶段配准实现精确地图对齐。相比之前工作，Uni-Mapper是首个同时处理在线动态移除和多模态地图合并的框架，解决了现有方法在动态环境和跨模态激光雷达融合中的局限性，实现了更鲁棒的地图统一。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Uni-Mapper是一个创新的多模态激光雷达地图合并框架，通过动态物体移除和动态感知地点识别技术，实现了在复杂动态环境中鲁棒的地图统一，支持多机器人协作和长期SLAM应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The unification of disparate maps is crucial for enabling scalable robotoperation across multiple sessions and collaborative multi-robot scenarios.However, achieving a unified map robust to sensor modalities and dynamicenvironments remains a challenging problem. Variations in LiDAR types anddynamic elements lead to differences in point cloud distribution and sceneconsistency, hindering reliable descriptor generation and loop closuredetection essential for accurate map alignment. To address these challenges,this paper presents Uni-Mapper, a dynamic-aware 3D point cloud map mergingframework for multi-modal LiDAR systems. It comprises dynamic object removal,dynamic-aware loop closure, and multi-modal LiDAR map merging modules. Avoxel-wise free space hash map is built in a coarse-to-fine manner to identifyand reject dynamic objects via temporal occupancy inconsistencies. The removalmodule is integrated with a LiDAR global descriptor, which encodes preservedstatic local features to ensure robust place recognition in dynamicenvironments. In the final stage, multiple pose graph optimizations areconducted for both intra-session and inter-map loop closures. We adopt acentralized anchor-node strategy to mitigate intra-session drift errors duringmap merging. In the final stage, centralized anchor-node-based pose graphoptimization is performed to address intra- and inter-map loop closures forglobally consistent map merging. Our framework is evaluated on diversereal-world datasets with dynamic objects and heterogeneous LiDARs, showingsuperior performance in loop detection across sensor modalities, robust mappingin dynamic environments, and accurate multi-map alignment over existingmethods. Project Page: https://sparolab.github.io/research/uni_mapper.</description>
      <author>example@mail.com (Gilhwan Kang, Hogyun Kim, Byunghee Choi, Seokhwan Jeong, Young-Sik Shin, Younggun Cho)</author>
      <guid isPermaLink="false">2507.20538v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Automated 3D-GS Registration and Fusion via Skeleton Alignment and Gaussian-Adaptive Features</title>
      <link>http://arxiv.org/abs/2507.20480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种自动化的3D高斯溅射子图配准和融合方法，通过几何骨架提取和多因子高斯融合策略，解决了现有方法中依赖人工干预和硬阈值过滤导致的质量下降问题，实验表明该方法显著提高了配准精度和融合质量。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射在实时渲染和训练效率方面显示出巨大潜力，但现有方法主要关注单图重建，多个3D-GS子图的配准和融合仍探索不足。现有方法通常依赖人工干预选择参考子图作为模板，并使用点云匹配进行配准，同时硬阈值过滤往往导致融合后渲染质量下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的自动3D-GS子图对齐和融合方法，消除对人工干预的需求，同时提高配准精度和融合质量。&lt;h4&gt;方法&lt;/h4&gt;首先，提取多个场景的几何骨架并利用椭球感知卷积捕获3D-GS属性，促进鲁棒场景配准；其次，引入多因子高斯融合策略，减轻刚性阈值导致的场景元素丢失。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet-GSReg和Coord数据集上的实验证明了方法的有效性。对于配准，在复杂场景上实现了RRE降低41.9%，确保更精确的姿态估计；对于融合，PSNR提高了10.11 dB，突显了优越的结构保持能力。&lt;h4&gt;结论&lt;/h4&gt;该方法能够增强场景对齐和重建保真度，确保机器人感知和自主导航中更一致、准确的3D场景表示。&lt;h4&gt;翻译&lt;/h4&gt;近年来，基于3D高斯溅射的场景表示在实时渲染和训练效率方面显示出巨大潜力。然而，大多数现有方法主要关注单图重建，而多个3D-GS子图的配准和融合仍探索不足。现有方法通常依赖人工干预选择参考子图作为模板，并使用点云匹配进行配准。此外，3D-GS原语的硬阈值过滤往往导致融合后渲染质量下降。在本文中，我们提出了一种新颖的自动3D-GS子图对齐和融合方法，消除对人工干预的需求，同时提高配准精度和融合质量。首先，我们提取多个场景的几何骨架，并利用椭球感知卷积捕获3D-GS属性，促进鲁棒场景配准。其次，我们引入多因子高斯融合策略，减轻刚性阈值导致的场景元素丢失。在ScanNet-GSReg和我们的Coord数据集上的实验证明了所提出方法在配准和融合方面的有效性。对于配准，它在复杂场景上实现了RRE降低41.9%，确保更精确的姿态估计。对于融合，它将PSNR提高了10.11 dB，突显了优越的结构保持能力。这些结果确认了其增强场景对齐和重建保真度的能力，确保机器人感知和自主导航中更一致、准确的3D场景表示。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯溅射(3D-GS)场景中多个子图的自动配准和融合问题。这个问题在现实中非常重要，因为它关系到无人系统（如机器人、自动驾驶）如何从不同视角（地面和空中）获取的数据构建完整、一致的3D场景模型。当前方法需要人工干预且融合质量差，限制了3D重建在复杂环境中的应用。解决这一问题能显著提升机器人感知和自主导航的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：需要手动选择参考子图、依赖点云匹配而忽视3D-GS特定属性、硬阈值过滤降低渲染质量。基于这些分析，作者设计了自动化框架，包括骨架提取配准策略、高斯自适应卷积方法、多因素高斯融合策略。作者借鉴了DBSCAN聚类用于骨架初始化和KPConv用于特征提取，但针对3D-GS的各向异性特性进行了改进。同时参考了NeRF和3D-GS领域的相关工作，如UC-GS、DRAGON等，但解决了它们依赖人工干预或强场景先验的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过骨架对齐实现鲁棒配准，并通过多因素融合策略保留场景结构和细节。整体流程包括：1)使用DBSCAN聚类提取初始骨架；2)通过联合能量函数优化骨架，平衡拟合与平滑度；3)利用马氏距离和椭球感知卷积提取高斯自适应特征；4)将场景分为重叠与非重叠区域，在重叠区基于骨架一致性、细节保真度和空间分布计算融合分数，选择保留的高斯基元；5)整合所有高斯生成统一的高保真3D场景模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)骨架初始化和高斯自适应特征提取框架，整合各向异性邻域选择和椭球感知卷积；2)多因素高斯融合策略，平衡结构一致性和细节保真度；3)完全自动化流程，消除人工干预。相比之前工作，不同之处在于：1)配准中利用3D-GS特定属性而非简单点云匹配；2)融合中使用基于分数的选择而非硬阈值，避免结构丢失；3)无需手动选择参考子图，提高了实用性和鲁棒性。实验显示该方法在复杂场景中RRE降低41.9%，PSNR提高10.11dB。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于骨架对齐和高斯自适应特征的自动化3D-GS配准与融合方法，显著提高了多视角3D场景重建的精度和保真度，无需人工干预。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, 3D Gaussian Splatting (3D-GS)-based scene representationdemonstrates significant potential in real-time rendering and trainingefficiency. However, most existing methods primarily focus on single-mapreconstruction, while the registration and fusion of multiple 3D-GS sub-mapsremain underexplored. Existing methods typically rely on manual intervention toselect a reference sub-map as a template and use point cloud matching forregistration. Moreover, hard-threshold filtering of 3D-GS primitives oftendegrades rendering quality after fusion. In this paper, we present a novelapproach for automated 3D-GS sub-map alignment and fusion, eliminating the needfor manual intervention while enhancing registration accuracy and fusionquality. First, we extract geometric skeletons across multiple scenes andleverage ellipsoid-aware convolution to capture 3D-GS attributes, facilitatingrobust scene registration. Second, we introduce a multi-factor Gaussian fusionstrategy to mitigate the scene element loss caused by rigid thresholding.Experiments on the ScanNet-GSReg and our Coord datasets demonstrate theeffectiveness of the proposed method in registration and fusion. Forregistration, it achieves a 41.9\% reduction in RRE on complex scenes, ensuringmore precise pose estimation. For fusion, it improves PSNR by 10.11 dB,highlighting superior structural preservation. These results confirm itsability to enhance scene alignment and reconstruction fidelity, ensuring moreconsistent and accurate 3D scene representation for robotic perception andautonomous navigation.</description>
      <author>example@mail.com (Shiyang Liu, Dianyi Yang, Yu Gao, Bohan Ren, Yi Yang, Mengyin Fu)</author>
      <guid isPermaLink="false">2507.20480v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>PIVOTS: Aligning unseen Structures using Preoperative to Intraoperative Volume-To-Surface Registration for Liver Navigation</title>
      <link>http://arxiv.org/abs/2507.20337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PIVOTS的非刚性配准方法，用于增强现实引导的腹腔镜肝脏手术，通过将术前信息融合到术中视野中提高手术导航能力。&lt;h4&gt;背景&lt;/h4&gt;非刚性配准对于腹腔镜肝脏手术至关重要，可将术前肿瘤位置和血管结构等信息融合到有限的术中视野中。然而，准确预测术中肝脏变形面临诸多挑战，包括气腹、呼吸和器械交互引起的大变形、术中数据噪声以及视野受限等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理大变形、噪声数据和视野限制等挑战的非刚性配准方法，准确预测术中肝脏变形。&lt;h4&gt;方法&lt;/h4&gt;提出PIVOTS（术前到术中体积到表面配准）神经网络，直接以点云为输入进行变形预测。该方法包含几何特征提取编码器（支持多分辨率特征提取）和由新型变形感知交叉注意力模块组成的解码器（实现术前和术中信息交互及精确的多级位移预测）。&lt;h4&gt;主要发现&lt;/h4&gt;与基线方法相比，PIVOTS显示出优越的配准性能，对大量噪声、大变形和各种程度的术中可见性表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;PIVOTS有效解决了非刚性配准在腹腔镜肝脏手术中的挑战，研究人员已发布训练和测试集作为评估基准，并呼吁对肝脏配准方法进行基于体积到表面数据的公平比较。代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;非刚性配准对于增强现实引导的腹腔镜肝脏手术至关重要，它通过将术前信息（如肿瘤位置和血管结构）融合到有限的术中视野中，从而提高手术导航能力。前提是准确预测术中肝脏变形，但由于气腹、呼吸和器械交互引起的大变形、术中数据噪声以及由于遮挡和摄像机运动受限导致的视野有限等因素，这仍然极具挑战性。为应对这些挑战，我们引入了PIVOTS，一种直接以点云为输入进行变形预测的术前到术中体积到表面配准神经网络。几何特征提取编码器允许多分辨率特征提取，而由新型变形感知交叉注意力模块组成的解码器，使术前和术中信息交互以及准确的多级位移预测成为可能。我们在通过生物力学模拟管道合成的合成数据上训练神经网络，并在合成和真实数据集上验证其性能。结果表明，与基线方法相比，我们的方法显示出优越的配准性能，对大量噪声、大变形和各种程度的术中可见性表现出强大的鲁棒性。我们发布了训练和测试集作为评估基准，并呼吁对肝脏配准方法进行基于体积到表面数据的公平比较。代码和数据集可在https://github.com/pengliu-nct/PIVOTS获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决腹腔镜肝脏手术中术前肝脏体积信息（如MRI/CT扫描）与术中有限表面视图的精确配准问题。这个问题在现实中至关重要，因为它关系到手术导航的准确性，直接影响手术安全。准确的配准可以帮助外科医生在微创手术中识别关键血管结构和肿瘤位置，避免损伤重要组织，减少出血等并发症，从而提高手术成功率和患者预后。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到腹腔镜肝脏手术中非刚性配准的重要性，并识别出当前方法面临的挑战：大变形、噪声和有限视野。他们借鉴了点云处理领域的工作，如PointNet、PointNet++、PointTransformer和KPConv等架构，以及注意力机制在点云分析中的应用。作者还参考了生物力学模拟方法，用于生成合成训练数据。在此基础上，他们设计了专门的体积到表面配准网络，引入变形感知交叉注意力模块，使术前和术中信息能够有效交互，并通过多级位移预测提高配准精度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用神经网络直接从术前肝脏体积点云和术中肝脏表面点云预测变形场，实现精确的非刚性配准。整体流程包括：1) 数据预处理：重新采样点云并提取几何特征；2) 几何特征提取：使用DGCNN和FPS在不同分辨率下提取特征；3) 变形感知Transformer：通过相对点注意力和交叉注意力模块处理特征交互；4) 变形场预测：在多个分辨率级别预测位移场；5) 训练数据生成：使用生物力学模拟创建多样化的合成数据，解决数据稀缺问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出专门的PIVOTS神经网络，直接处理体积到表面点云配准；2) 设计变形感知交叉注意力模块，促进术前和术中信息交互；3) 使用多级位移预测提高配准精度；4) 创建高质量的合成训练数据集和健康人类肝脏呼吸运动(HHLBM)数据集。相比之前的工作，PIVOTS专注于体积到表面配准而非表面到表面配准，直接处理点云而非体素表示，更好地处理了术前体积和术中表面之间的信息交互，并通过合成数据生成解决了真实数据稀缺的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了PIVOTS，一种基于Transformer的体积到表面配准神经网络，通过变形感知交叉注意力和多级位移预测，实现了腹腔镜肝脏手术中术前和术中肝脏的高精度配准，并展示了其对噪声、大变形和部分可见性的强大鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-rigid registration is essential for Augmented Reality guided laparoscopicliver surgery by fusing preoperative information, such as tumor location andvascular structures, into the limited intraoperative view, thereby enhancingsurgical navigation. A prerequisite is the accurate prediction ofintraoperative liver deformation which remains highly challenging due tofactors such as large deformation caused by pneumoperitoneum, respiration andtool interaction as well as noisy intraoperative data, and limited field ofview due to occlusion and constrained camera movement. To address thesechallenges, we introduce PIVOTS, a Preoperative to IntraoperativeVOlume-To-Surface registration neural network that directly takes point cloudsas input for deformation prediction. The geometric feature extraction encoderallows multi-resolution feature extraction, and the decoder, comprising noveldeformation aware cross attention modules, enables pre- and intraoperativeinformation interaction and accurate multi-level displacement prediction. Wetrain the neural network on synthetic data simulated from a biomechanicalsimulation pipeline and validate its performance on both synthetic and realdatasets. Results demonstrate superior registration performance of our methodcompared to baseline methods, exhibiting strong robustness against high amountsof noise, large deformation, and various levels of intraoperative visibility.We publish the training and test sets as evaluation benchmarks and call for afair comparison of liver registration methods with volume-to-surface data. Codeand datasets are available here https://github.com/pengliu-nct/PIVOTS.</description>
      <author>example@mail.com (Peng Liu, Bianca Güttner, Yutong Su, Chenyang Li, Jinjing Xu, Mingyang Liu, Zhe Min, Andrey Zhylka, Jasper Smit, Karin Olthof, Matteo Fusaglia, Rudi Apolle, Matthias Miederer, Laura Frohneberger, Carina Riediger, Jügen Weitz, Fiona Kolbinger, Stefanie Speidel, Micha Pfeiffer)</author>
      <guid isPermaLink="false">2507.20337v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Tactile-Guided Robotic Ultrasound: Mapping Preplanned Scan Paths for Intercostal Imaging</title>
      <link>http://arxiv.org/abs/2507.20282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IROS2025, video link: https://youtu.be/SBwpFVzEhAg&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用触觉线索表征皮下肋骨结构作为超声分割替代信号的方法，用于无分割的骨表面点云提取，并通过机器人系统实现肋间成像的准确扫描路径映射。&lt;h4&gt;背景&lt;/h4&gt;医学超声成像因便携性、实时性和无辐射特性在临床广泛应用，但机器人超声系统在肋间成像中的应用受限，原因是缺乏有效的扫描路径生成方法。&lt;h4&gt;目的&lt;/h4&gt;克服肋间成像中缺乏有效扫描路径生成方法的挑战，探索触觉线索在表征皮下肋骨结构方面的潜力，用于无分割的骨表面点云提取。&lt;h4&gt;方法&lt;/h4&gt;利用机器人跟踪数据生成稀疏触觉点云模拟人体触诊，对稀疏触觉骨位置点云进行插值形成密集表示，并与基于图像的密集骨表面点云配准实现准确扫描路径映射，同时引入自动倾斜角度调整方法确保完全覆盖感兴趣区域。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同phantom上的实验表明，扫描路径映射实现了平均最近邻距离误差3.41mm和豪斯多夫距离误差3.65mm，与CT真实值相比，重建的骨下物体误差为0.69mm和2.2mm。&lt;h4&gt;结论&lt;/h4&gt;触觉线索方法可有效解决肋间成像中缺乏有效扫描路径生成方法的问题，实现准确的扫描路径映射和骨下结构可视化。&lt;h4&gt;翻译&lt;/h4&gt;医学超声成像因其便携性、实时性和无辐射特性在临床检查中得到广泛应用。为解决操作者间的变异性问题，机器人超声系统受到越来越多的关注。然而，由于缺乏有效的扫描路径生成方法，其在具有挑战性的肋间成像中的应用仍然有限。为克服这一挑战，我们探索了使用触觉线索表征皮下肋骨结构作为超声分割替代信号的潜力，用于无分割的骨表面点云提取。与2D超声图像相比，1D触觉相关信号具有更高的处理效率，且不易受声学噪声和伪影影响。利用机器人跟踪数据，沿肋骨进行几次扫描生成稀疏触觉点云，模拟人体触诊。为了将扫描轨迹稳健地映射到肋间空间，首先对稀疏触觉骨位置点云进行插值形成更密集的表示，然后将此精炼点云与基于图像的密集骨表面点云进行配准，实现针对个别患者的准确扫描路径映射。此外，为确保完全覆盖感兴趣区域，我们引入了自动倾斜角度调整方法以可视化骨下结构。为验证提出的方法，我们在四个不同的phantom上进行了全面实验。最终的扫描路径映射实现了平均最近邻距离误差3.41mm和豪斯多夫距离误差3.65mm，而重建的骨下物体与CT真实值相比误差为0.69mm和2.2mm。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人超声系统在肋间隙成像中缺乏有效扫描路径生成方法的问题。这个问题很重要，因为肋间隙成像对于肝脏、心脏等重要器官的检查至关重要，而肋骨的高声阻抗会产生声影，遮挡底层组织，影响成像质量。解决这一问题可以提高超声检查的准确性和可靠性，减少操作者差异，使超声检查更加标准化和自动化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类触诊中获得灵感，注意到手指在胸部滑动时感受到的力变化可用于识别肋骨特征。他们借鉴了多项现有工作：之前使用触觉信号进行肿瘤定位、超声图像变形校正和弹性成像的研究；基于骨架图的注册方法；使用实时分割掩码进行血管结构路径优化的方法；以及基于RGB图像或多视角点云进行扫描路径规划的方法。作者的创新在于将触觉信号作为主要信息源用于骨骼表面提取，而非仅作为超声成像的辅助。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用触觉信号作为替代信号，用于无需超声分割的骨骼表面点云提取，并通过点云配准将预规划扫描路径映射到患者个体化设置中。整体流程包括：1)硬件设置(机器人臂、力传感器、触觉压头和超声探头)；2)数据准备(时间校准、模板点云创建、触觉数据采集和注释)；3)扫描路径规划(使用RGB-D相机感知工作空间并映射扫描路径)；4)扫描点分类(使用轻量级网络对触觉信号进行分类)；5)触觉点云生成和配准(通过插值生成密集点云并使用CPD算法配准)；6)扫描路径生成和调整(自动调整倾斜角确保完全覆盖感兴趣区域)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次使用触觉线索作为替代信号进行骨骼表面点云提取；2)通过稀疏触觉扫描策略模拟人类触诊并提高效率；3)实现触觉引导的精确扫描路径映射；4)引入自动倾斜角调整方法确保完全覆盖骨骼下方结构。相比之前工作，本文使用1D触觉信号而非2D超声图像或视觉信息作为主要信息源，处理效率更高，对声学噪声和伪影更不敏感，减少了对骨骼表面分割和分类的依赖，提高了临床实用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的触觉引导机器人超声系统，通过利用触觉信号提取骨骼表面点云并进行配准，实现了在受限肋间隙空间中精确映射预规划扫描路径的方法，显著提高了机器人辅助超声成像的精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical ultrasound (US) imaging is widely used in clinical examinations dueto its portability, real-time capability, and radiation-free nature. To addressinter- and intra-operator variability, robotic ultrasound systems have gainedincreasing attention. However, their application in challenging intercostalimaging remains limited due to the lack of an effective scan path generationmethod within the constrained acoustic window. To overcome this challenge, weexplore the potential of tactile cues for characterizing subcutaneous ribstructures as an alternative signal for ultrasound segmentation-free bonesurface point cloud extraction. Compared to 2D US images, 1D tactile-relatedsignals offer higher processing efficiency and are less susceptible to acousticnoise and artifacts. By leveraging robotic tracking data, a sparse tactilepoint cloud is generated through a few scans along the rib, mimicking humanpalpation. To robustly map the scanning trajectory into the intercostal space,the sparse tactile bone location point cloud is first interpolated to form adenser representation. This refined point cloud is then registered to animage-based dense bone surface point cloud, enabling accurate scan path mappingfor individual patients. Additionally, to ensure full coverage of the object ofinterest, we introduce an automated tilt angle adjustment method to visualizestructures beneath the bone. To validate the proposed method, we conductedcomprehensive experiments on four distinct phantoms. The final scanningwaypoint mapping achieved Mean Nearest Neighbor Distance (MNND) and Hausdorffdistance (HD) errors of 3.41 mm and 3.65 mm, respectively, while thereconstructed object beneath the bone had errors of 0.69 mm and 2.2 mm comparedto the CT ground truth.</description>
      <author>example@mail.com (Yifan Zhang, Dianye Huang, Nassir Navab, Zhongliang Jiang)</author>
      <guid isPermaLink="false">2507.20282v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks</title>
      <link>http://arxiv.org/abs/2507.20170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PUMPS的自编码器架构，用于处理时间点云（TPCs）格式的3D角色动画数据，解决了运动数据在不同骨架间转移的难题，实现了运动预测、过渡生成和关键帧插值等多种任务的高效处理。&lt;h4&gt;背景&lt;/h4&gt;运动骨架通过转换骨骼层次结构驱动3D角色动画，但比例或结构的差异导致运动数据难以在不同骨架间转移，这对数据驱动的运动合成构成了挑战。时间点云（TPCs）虽提供了一种无结构、跨兼容的运动表示方法，但主要用于兼容性而非直接用于运动任务学习。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理TPC数据的自编码器架构，实现直接运动任务学习，解决TPC格式数据合成中的时间一致性和点可识别性挑战。&lt;h4&gt;方法&lt;/h4&gt;PUMPS架构将逐帧点云简化为可采样的特征向量，解码器使用潜在高斯噪声向量作为采样标识符提取时间点。研究引入基于线性分配的点配对方法优化重建过程，避免了昂贵的逐点注意力机制。利用这些潜在特征预训练运动合成模型，支持多种运动任务。&lt;h4&gt;主要发现&lt;/h4&gt;PUMPS即使在无原生数据集监督的情况下，其预训练任务性能也能匹配最先进方法。当针对运动去噪或估计微调时，PUMPS优于许多特定方法，同时保持通用架构优势。&lt;h4&gt;结论&lt;/h4&gt;PUMPS为TPC格式的3D角色动画数据处理提供了有效解决方案，能够处理多种运动任务，并在各种应用中表现出色，无需针对特定任务修改架构。&lt;h4&gt;翻译&lt;/h4&gt;运动骨架通过转换骨骼层次结构来驱动3D角色动画，但比例或结构的差异使得运动数据难以在不同骨架之间转移，这对数据驱动的运动合成构成了挑战。时间点云（TPCs）提供了一种无结构、跨兼容的运动表示方法。虽然TPCs可以与骨架相互转换，但它们主要用于兼容性，而不是直接用于运动任务学习。这样做需要TPC格式的数据合成能力，这涉及到其独特的时间一致性和点可识别性方面的未探索挑战。因此，我们提出了PUMPS，这是一种用于TPC数据的原始自编码器架构。PUMPS独立地将逐帧点云简化为可采样的特征向量，然后从这些特征向量中，解码器使用潜在的高斯噪声向量作为采样标识符来提取不同的时间点。我们引入了基于线性分配的点配对方法来优化TPC重建过程，并在架构中避免了使用昂贵的逐点注意力机制。使用这些潜在特征，我们预训练了一个能够执行运动预测、过渡生成和关键帧插值的运动合成模型。对于这些预训练任务，PUMPS即使在没有原生数据集监督的情况下也能表现优异，匹配了最先进的性能。当针对运动去噪或估计进行微调时，PUMPS优于许多相应的方法，同时不偏离其通用架构。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D运动骨架多样性导致运动数据难以跨骨架迁移的问题，这对数据驱动的运动合成构成挑战。这个问题在现实中很重要，因为它限制了机器学习方法在不同角色动画流程中的泛化能力，导致需要为每个新骨架单独训练运动合成模型，既低效又冗余。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到时间点云(TPCs)作为几何运动表示可以掩盖骨架差异，但发现现有方法仍需为每个骨架单独训练运动合成。他们设计了一个两阶段预训练管道：第一阶段使用编码器-解码器学习TPC表示，第二阶段预训练运动合成模型。他们借鉴了PointTransformer作为编码器基础，使用旋转位置编码(RoPE)和匈牙利算法进行点匹配，并参考了掩码数据建模等自监督学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用骨架无关的点云表示和基于高斯噪声向量的采样机制，实现高效的运动合成。整体流程分为三部分：1)点云帧编码器将TPCs转换为潜在特征向量；2)TPC重建解码器使用高斯噪声向量作为点标识符，从潜在特征重建时间点云；3)潜在运动合成器通过掩码数据建模(关键帧插值、运动过渡和短期预测)预训练运动任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在TPC介质中进行运动合成预训练，实现骨架无关的通用运动学习；2)提出基于噪声向量的架构模拟点云采样，避免昂贵的时空注意力机制；3)在多种任务上达到最先进性能。相比之前工作，PUMPS无需针对特定骨架训练，通过噪声向量维持点身份，能同时从多个大型数据集学习，而传统方法需要为每个骨架单独训练且依赖注意力机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PUMPS提出了一种基于骨架无关点云的通用运动预训练方法，通过创新的噪声向量采样机制实现了高效的运动合成，并在多种任务上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion skeletons drive 3D character animation by transforming bonehierarchies, but differences in proportions or structure make motion data hardto transfer across skeletons, posing challenges for data-driven motionsynthesis. Temporal Point Clouds (TPCs) offer an unstructured, cross-compatiblemotion representation. Though reversible with skeletons, TPCs mainly serve forcompatibility, not for direct motion task learning. Doing so would require datasynthesis capabilities for the TPC format, which presents unexplored challengesregarding its unique temporal consistency and point identifiability. Therefore,we propose PUMPS, the primordial autoencoder architecture for TPC data. PUMPSindependently reduces frame-wise point clouds into sampleable feature vectors,from which a decoder extracts distinct temporal points using latent Gaussiannoise vectors as sampling identifiers. We introduce linear assignment-basedpoint pairing to optimise the TPC reconstruction process, and negate the use ofexpensive point-wise attention mechanisms in the architecture. Using theselatent features, we pre-train a motion synthesis model capable of performingmotion prediction, transition generation, and keyframe interpolation. For thesepre-training tasks, PUMPS performs remarkably well even without native datasetsupervision, matching state-of-the-art performance. When fine-tuned for motiondenoising or estimation, PUMPS outperforms many respective methods withoutdeviating from its generalist architecture.</description>
      <author>example@mail.com (Clinton Ansun Mo, Kun Hu, Chengjiang Long, Dong Yuan, Wan-Chi Siu, Zhiyong Wang)</author>
      <guid isPermaLink="false">2507.20170v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding</title>
      <link>http://arxiv.org/abs/2507.20110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  **14 pages, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NeuroVoxel-LM框架，结合神经辐射场、动态分辨率体素化和轻量级元嵌入，解决了现有3D语言模型处理稀疏大规模点云的挑战。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)和多模态大语言模型(MLLMs)的最新突破显著推动了3D场景感知向语言驱动认知方向发展。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D语言模型难以处理稀疏、大规模点云的问题，提高特征提取速度和表示准确性。&lt;h4&gt;方法&lt;/h4&gt;提出NeuroVoxel-LM框架，包含两种关键技术：1)动态分辨率多尺度体素化(DR-MSV)，根据几何和结构复杂度自适应调整体素粒度；2)基于令牌的自适应池化轻量级元嵌入(TAP-LME)，通过基于注意力的加权和残差融合增强语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DR-MSV显著提高了点云特征提取的效率和准确性，TAP-LME在从NeRF权重捕获细粒度语义方面优于传统最大池化。&lt;h4&gt;结论&lt;/h4&gt;NeuroVoxel-LM框架通过DR-MSV和TAP-LME技术有效解决了现有3D语言模型处理稀疏大规模点云的挑战，提高了3D场景感知能力。&lt;h4&gt;翻译&lt;/h4&gt;近期视觉语言模型(VLMs)和多模态大语言模型(MLLMs)的突破显著推动了3D场景感知向语言驱动认知方向发展。然而，由于特征提取速度慢和表示准确性有限，现有的3D语言模型难以处理稀疏的大规模点云。为应对这些挑战，我们提出了NeuroVoxel-LM这一新框架，它结合了神经辐射场(NeRF)、动态分辨率体素化和轻量级元嵌入。具体而言，我们引入了一种动态分辨率多尺度体素化(DR-MSV)技术，它根据几何和结构复杂度自适应调整体素粒度，在降低计算成本的同时保持重建保真度。此外，我们提出了基于令牌的自适应池化轻量级元嵌入(TAP-LME)机制，它通过基于注意力的加权和残差融合增强语义表示。实验结果表明，DR-MSV显著提高了点云特征提取的效率和准确性，而TAP-LME在从NeRF权重捕获细粒度语义方面优于传统的最大池化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决处理稀疏大规模点云时，现有3D大型语言模型特征提取速度慢和特征表示准确性低的问题。这个问题在现实中很重要，因为随着3D场景感知向语言驱动认知方向发展，实时理解和快速响应3D场景的能力在虚拟现实、自动驾驶和机器人感知等应用中变得越来越关键，而现有方法难以在计算效率和几何精度之间取得平衡。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有NeRF方法在处理大规模点云数据时有固有约束，包括对输入数据质量的依赖和传统特征提取方法的局限性。作者借鉴了神经辐射场(NeRF)作为3D表示的新范式、多模态大型语言模型(MLLMs)在跨模态通信方面的突破，以及一些如SparseNeRF和结合Transformer注意力的变体方法。针对问题，作者设计了两个主要组件：动态分辨率多尺度体素化(DR-MSV)基于几何和结构复杂度指标自适应调整体素粒度；Token级自适应池化轻量级元嵌入(TAP-LME)通过注意力加权和残差融合提高全局表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1)动态分辨率多尺度体素化(DR-MSV)根据场景的结构和几何复杂度自适应调整体素分辨率，在保持几何准确性的同时降低计算成本；2)Token级自适应池化轻量级元嵌入(TAP-LME)通过注意力机制为每个token分配可学习权重，提高对NeRF权重细粒度语义理解。整体流程是：输入3D点云数据→初始体素化→使用六个关键指标评估体素复杂度→将体素分为复杂和非复杂区域→迭代合并非复杂区域形成体素金字塔→使用TAP-LME处理token序列→通过注意力机制和残差融合生成最终全局特征表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DR-MSV技术根据多维几何和结构指标自适应调整体素分辨率，使用数据驱动阈值确定复杂度，通过迭代合并形成多级体素金字塔；2)TAP-LME机制引入token级注意力加权策略，结合最大池化和注意力池化进行残差融合，使用可训练融合系数自适应分配资源。相比之前工作，不同之处在于：DR-MSV相比传统固定分辨率体素化能自适应调整分辨率并提高计算效率；TAP-LME相比仅使用最大池化的方法能更好地捕捉token重要性并增强局部细节和整体结构的建模能力；整体上更专注于解决稀疏大规模点云场景的处理问题，实现了更好的效率和准确性平衡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NeuroVoxel-LM通过动态分辨率多尺度体素化和Token级自适应池化轻量级元嵌入技术，有效解决了3D语言模型在处理稀疏大规模点云时的特征提取速度慢和准确性低问题，实现了更高效准确的语言驱动3D场景感知。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent breakthroughs in Visual Language Models (VLMs) and Multimodal LargeLanguage Models (MLLMs) have significantly advanced 3D scene perception towardslanguage-driven cognition. However, existing 3D language models struggle withsparse, large-scale point clouds due to slow feature extraction and limitedrepresentation accuracy. To address these challenges, we propose NeuroVoxel-LM,a novel framework that integrates Neural Radiance Fields (NeRF) with dynamicresolution voxelization and lightweight meta-embedding. Specifically, weintroduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique thatadaptively adjusts voxel granularity based on geometric and structuralcomplexity, reducing computational cost while preserving reconstructionfidelity. In addition, we propose the Token-level Adaptive Pooling forLightweight Meta-Embedding (TAP-LME) mechanism, which enhances semanticrepresentation through attention-based weighting and residual fusion.Experimental results demonstrate that DR-MSV significantly improves point cloudfeature extraction efficiency and accuracy, while TAP-LME outperformsconventional max-pooling in capturing fine-grained semantics from NeRF weights.</description>
      <author>example@mail.com (Shiyu Liu, Lianlei Shan)</author>
      <guid isPermaLink="false">2507.20110v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Geometric Operator Learning with Optimal Transport</title>
      <link>http://arxiv.org/abs/2507.20065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出将最优传输(OT)集成到复杂几何上的偏微分方程(PDE)的算子学习中，通过将离散网格推广到网格密度函数，实现实例相关的变形，提高方法的灵活性和有效性。对于3D表面模拟，该方法将表面几何嵌入到二维参数化潜在空间，实现计算效率的提升。实验表明，该方法在多个数据集上实现了更高的精度和更低的计算开销。&lt;h4&gt;背景&lt;/h4&gt;经典几何学习方法通常将域表示为网格、图或点云，但这些方法在处理复杂几何上的偏微分方程时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于最优传输的算子学习方法，用于处理复杂几何上的偏微分方程，提高计算效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;将离散网格推广到网格密度函数，将几何嵌入表述为最优传输问题，将这些函数映射到参考空间中的均匀密度；对于3D表面模拟，开发基于OT的神经算子，将表面几何嵌入到二维参数化潜在空间中，直接在此二维表示上进行计算。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基于OT的方法采用实例相关的变形，比依赖插值或共享变形的先前方法提供更大的灵活性和有效性；2) 在ShapeNet-Car和DrivAerNet-Car数据集上对雷诺平均纳维-斯托克斯方程的实验表明，该方法实现了更高的精度，并减少了时间和内存计算开销；3) 在FlowBench数据集上，模型显示出显著提高的准确性，证明了对于具有高度可变几何形状的数据集，使用实例相关变形的好处。&lt;h4&gt;结论&lt;/h4&gt;将最优传输集成到算子学习中，通过实例相关的变形和二维参数化潜在空间的表示，能够有效处理复杂几何上的偏微分方程，显著提高计算效率和准确性，特别是在处理高度可变几何形状的数据集时表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们提出将最优传输(OT)集成到复杂几何上的偏微分方程(PDE)的算子学习中。经典的几何学习方法通常将域表示为网格、图或点云。我们的方法将离散网格推广到网格密度函数，将几何嵌入表述为将这些函数映射到参考空间中均匀密度的OT问题。与依赖插值或共享变形的先前方法相比，我们基于OT的方法采用实例相关的变形，提供了更大的灵活性和有效性。对于专注于表面的3D模拟，我们基于OT的神经算子将表面几何嵌入到二维参数化潜在空间中。通过直接在此表面流形的二维表示上进行计算，它比体积模拟实现了显著的计算效率提升。在ShapeNet-Car和DrivAerNet-Car数据集上对雷诺平均纳维-斯托克斯方程(RANS)的实验表明，我们的方法比现有的机器学习模型实现了更高的精度，并减少了时间和内存使用方面的计算开销。此外，我们的模型在FlowBench数据集上显示出显著提高的准确性，强调了对于具有高度可变几何形状的数据集采用实例相关变形的好处。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在复杂几何形状上高效求解偏微分方程(PDEs)的问题。这个问题在现实中非常重要，因为传统数值方法在处理复杂几何（如汽车外形）时计算成本极高，一个汽车形状的模拟在CPU上需要超过300小时，在GPU上也需要10小时。而现有的机器学习方法虽然提高了效率，但大多受限于特定分辨率，缺乏灵活性。神经算子作为解决PDEs的分辨率无关方法，在处理复杂几何时仍面临挑战，特别是在汽车空气动力学等需要精确模拟表面特性的应用中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：Geo-FNO使用共享变形图不能很好地处理案例相关的几何特征，且计算成本高；GINO结合了图神经算子和傅里叶神经算子，但面临图嵌入的局部性和3D潜在空间的高计算成本。作者提出将几何嵌入重新表述为每个实例的最优传输问题，借鉴了最优传输的数学框架（特别是Sinkhorn算法）和神经算子作为解决PDEs的分辨率无关方法。通过最优传输将离散网格推广到网格密度函数，将几何嵌入建模为将密度函数映射到参考空间中的均匀密度的过程，从而实现了比之前方法更大的灵活性和有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将几何形状表示为密度函数，使用最优传输将这些密度函数映射到参考空间中的均匀密度，在简化的潜在空间中进行计算，然后再转换回原始几何形状。对于3D表面模拟，通过最优传输将表面几何嵌入到2D参数化潜在空间，实现计算效率的提升。整体流程包括：1)接收表面采样网格和对应的PDE解值；2)生成2D参数化潜在网格；3)使用最优传输（Sinkhorn算法或PPMM）计算从潜在网格到物理网格的映射；4)构建索引映射进行编码；5)在潜在空间中部署FNO执行计算；6)使用反向索引映射将结果解码回物理表面；7)输出原始几何形状上的预测解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)最优传输框架，将网格嵌入重新表述为最优传输问题，统一并推广了之前的方法；2)子流形方法，通过维度降低实现计算效率提升；3)实例相关变形，为每个形状单独进行几何嵌入；4)显著的计算效率提升，比现有方法快2-8倍，比传统方法快约7000倍。与之前工作的不同：与Geo-FNO相比，不使用共享变形网络且避免了昂贵的矩阵向量乘法；与GINO相比，不依赖于图嵌入的局部性，计算在2D空间而非3D空间进行；与其他基于transformer或隐式神经表示的方法相比，保留了几何性质且编码器可逆。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于最优传输的几何神经算子方法，通过将复杂几何形状映射到简化空间并在其中高效计算，显著提高了在复杂几何上求解偏微分方程的准确性和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose integrating optimal transport (OT) into operator learning forpartial differential equations (PDEs) on complex geometries. Classicalgeometric learning methods typically represent domains as meshes, graphs, orpoint clouds. Our approach generalizes discretized meshes to mesh densityfunctions, formulating geometry embedding as an OT problem that maps thesefunctions to a uniform density in a reference space. Compared to previousmethods relying on interpolation or shared deformation, our OT-based methodemploys instance-dependent deformation, offering enhanced flexibility andeffectiveness. For 3D simulations focused on surfaces, our OT-based neuraloperator embeds the surface geometry into a 2D parameterized latent space. Byperforming computations directly on this 2D representation of the surfacemanifold, it achieves significant computational efficiency gains compared tovolumetric simulation. Experiments with Reynolds-averaged Navier-Stokesequations (RANS) on the ShapeNet-Car and DrivAerNet-Car datasets show that ourmethod achieves better accuracy and also reduces computational expenses interms of both time and memory usage compared to existing machine learningmodels. Additionally, our model demonstrates significantly improved accuracy onthe FlowBench dataset, underscoring the benefits of employinginstance-dependent deformation for datasets with highly variable geometries.</description>
      <author>example@mail.com (Xinyi Li, Zongyi Li, Nikola Kovachki, Anima Anandkumar)</author>
      <guid isPermaLink="false">2507.20065v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning</title>
      <link>http://arxiv.org/abs/2507.19950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的零样本方法来优化点云配准算法，利用深度图像和预训练扩散模型提取的特征增强点云对应关系的准确性，无需专用训练数据集。&lt;h4&gt;背景&lt;/h4&gt;最近研究表明大规模预训练扩散模型可用于在图像中建立语义对应关系，基于扩散技术的进步为点云配准提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;提出一种零样本方法改进点云配准算法的准确性，消除对专用训练数据集的需求。&lt;h4&gt;方法&lt;/h4&gt;将点云从多个视角投影到深度图中，从预训练扩散网络提取深度扩散特征，将特征与现有几何特征相结合，利用优化后的对应关系提高配准精度。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法显著提高了点云配准准确性，在多种数据集上展现出强大的泛化能力，无需专用训练数据集即可实现高性能。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅提高了现有点云配准技术的性能，还展示了跨数据集的鲁棒泛化能力，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;最近利用大规模预训练扩散模型的研究表明，使用扩散特征可以在图像中建立语义对应关系。受基于扩散技术的进步启发，我们提出了一种新颖的零样本方法来优化点云配准算法。我们的方法利用从深度图像推导出的对应关系来增强点特征表示，消除了对专用训练数据集的需求。具体而言，我们首先将点云从多个视角投影到深度图中，并从预训练的扩散网络中提取隐式知识作为深度扩散特征。然后将这些特征与从现有方法获得的几何特征相结合，以建立点云之间更准确的对应关系。通过利用这些优化后的对应关系，我们的方法显著提高了配准精度。大量实验证明，我们的方法不仅提高了现有点云配准技术的性能，还在各种数据集上展现出强大的泛化能力。代码可在 https://github.com/zhengcy-lambo/RARE.git 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准的精度问题。点云配准是计算机视觉中的基础任务，在自动驾驶、机器人技术和场景重建等领域有广泛应用。然而，现有方法要么依赖手工设计特征性能有限，要么需要大量成对数据训练限制实用性，而基于扩散模型的方法在处理大视差深度图时难以生成准确对应关系。因此提高配准精度对推动相关领域技术发展具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到扩散模型在图像处理中成功应用的启发，注意到现有方法如FreeReg在处理大视差深度图时面临挑战。他们观察到通过初始变换将点云对齐后，生成的深度图视差变小，扩散特征在小视差深度图上表现出更高区分度。因此设计了一种零样本方法，利用预训练扩散模型提取深度图特征，与现有几何特征结合改进配准。该方法借鉴了FreeReg等工作的思想，但通过初始变换解决了其在大视差场景下的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练扩散模型提取深度图的隐含知识，与现有几何特征融合以提高点云配准精度，无需专门训练数据集。整体流程包括：1)使用现有方法获取初始变换；2)将点云投影到深度图；3)用ControlNet提取深度扩散特征；4)将扩散特征与3D几何特征融合建立对应关系；5)通过内点聚合模块计算更精确的刚体变换。关键是通过初始变换减少深度图视差，提高扩散特征区分度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)零样本学习方法，无需专门训练数据集；2)深度扩散特征与几何特征融合；3)利用初始变换减少视差提高特征区分度；4)内点聚合模块整合多源特征对应关系。相比之前工作，RARE解决了FreeReg在大视差场景下的局限性；区别于需要大量成对数据训练的监督学习方法；不同于传统基于几何一致性的异常值去除方法；与其他零样本方法不同，RARE专注于改进现有配准方法而非从头设计完整流程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RARE提出了一种零样本方法，通过融合预训练扩散模型提取的深度特征与几何特征，显著提高了点云配准精度，同时无需专门的训练数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research leveraging large-scale pretrained diffusion models hasdemonstrated the potential of using diffusion features to establish semanticcorrespondences in images. Inspired by advancements in diffusion-basedtechniques, we propose a novel zero-shot method for refining point cloudregistration algorithms. Our approach leverages correspondences derived fromdepth images to enhance point feature representations, eliminating the need fora dedicated training dataset. Specifically, we first project the point cloudinto depth maps from multiple perspectives and extract implicit knowledge froma pretrained diffusion network as depth diffusion features. These features arethen integrated with geometric features obtained from existing methods toestablish more accurate correspondences between point clouds. By leveragingthese refined correspondences, our approach achieves significantly improvedregistration accuracy. Extensive experiments demonstrate that our method notonly enhances the performance of existing point cloud registration techniquesbut also exhibits robust generalization capabilities across diverse datasets.Codes are available at https://github.com/zhengcy-lambo/RARE.git.</description>
      <author>example@mail.com (Chengyu Zheng, Jin Huang, Honghua Chen, Mingqiang Wei)</author>
      <guid isPermaLink="false">2507.19950v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>TrackAny3D: Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking</title>
      <link>http://arxiv.org/abs/2507.19908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TrackAny3D是一种创新的类别无关3D单目标跟踪框架，通过迁移大规模预训练模型解决了现有特定类别方法泛化能力有限的问题，在基准测试上取得了最新技术水平。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR单目标跟踪依赖于稀疏和不规则的点云，面临不同物体类别在尺度、运动模式和结构复杂性方面的几何变化挑战。当前特定类别方法虽然精度高但不实用，需要为每个类别单独建模，且泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有类别特定方法的局限性，提出首个用于类别无关3D单目标跟踪的大规模预训练模型迁移框架。&lt;h4&gt;方法&lt;/h4&gt;1) 集成参数高效适配器，在保持几何先验的同时弥合预训练和跟踪任务间的差距；2) 引入几何专家混合架构，根据不同几何特征自适应激活专门子网络；3) 设计时间上下文优化策略，包含可学习时间标记和动态掩码加权模块，用于传播历史信息并减少时间漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在三个常用基准测试上，TrackAny3D建立了类别无关3D单目标跟踪的最新技术水平，展示了强大的泛化能力和竞争力。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了统一模型在3D目标跟踪领域的重要性，并希望进一步推动大规模预训练模型在该领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;基于3D LiDAR的单目标跟踪依赖于稀疏和不规则的点云，面临着来自不同类别物体在尺度、运动模式和结构复杂性方面的几何变化带来的挑战。当前特定类别的方法虽然精度良好但在实际应用中并不实用，需要为每个类别单独建模且泛化能力有限。为解决这些问题，我们提出了TrackAny3D，这是首个用于类别无关3D单目标跟踪的大规模预训练模型迁移框架。我们首先集成参数高效的适配器来弥合预训练和跟踪任务之间的差距，同时保持几何先验。然后，我们引入了几何专家混合架构，根据不同的几何特征自适应激活专门的子网络。此外，我们还设计了一种时间上下文优化策略，结合了可学习的时间标记和动态掩码加权模块，用于传播历史信息并减少时间漂移。在三个常用的基准测试上的实验表明，TrackAny3D在类别无关的3D单目标跟踪方面建立了新的技术水平，展示了强大的泛化能力和竞争力。我们希望这项工作能让社区认识到统一模型的重要性，并进一步扩大大规模预训练模型在该领域的应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云跟踪中的类别特定性问题。现有方法需要为每个物体类别（如汽车、行人等）单独训练模型，导致计算资源消耗大、无法泛化到新类别，且不同类别物体的几何差异（尺度、运动模式、结构复杂性）使统一模型性能下降。这个问题在自动驾驶和机器人领域尤为重要，因为这些应用需要处理各种未知类别的物体，类别特定方法无法满足开放世界需求，且维护多个模型成本高。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到3D点云跟踪的几何变化挑战和类别特定方法的局限性。受2D视觉和NLP中预训练模型成功的启发，认为预训练3D模型可提供几何先验。他们识别出三个主要挑战：分布不匹配、持久几何差距和缺乏时间建模。针对这些问题，设计了三个核心组件：参数高效适配器（借鉴PEFT技术）、混合几何专家（受MoE架构启发）和时间上下文优化策略。作者还利用RECON作为预训练模型，这是一个结合生成掩码建模和对比建模的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模预训练3D模型的几何先验知识，构建能处理各类物体的统一跟踪框架，无需为每个类别单独训练。流程包括：1)输入处理（接收搜索和模板区域点云，添加掩码和时间令牌）；2)特征提取与匹配（将令牌输入带有适配器和专家架构的预训练编码器）；3)参数高效适配器（双路径设计控制特征适应）；4)混合几何专家（根据几何特征激活专门专家）；5)时间上下文优化（传播时间令牌和动态掩码加权）；6)输出处理（预测边界框和保留时间上下文）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个将大规模预训练3D模型迁移到类别统一3D跟踪的框架；2)参数高效双路径适配器，保持几何先验；3)混合几何专家架构，根据几何特征自适应激活；4)时间上下文优化策略，处理时间变化和状态漂移。不同之处：相比类别特定方法，无需多模型且能泛化到新类别；相比MoCUT等方法，无需手动调整超参数；相比仅使用预训练初始化的方法，充分利用PEFT技术；在技术实现上引入动态控制特征适应和基于几何而非类别的专家选择。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrackAny3D首次成功将大规模预训练3D模型迁移到类别统一的3D点云跟踪任务中，通过参数高效适配器和混合几何专家架构解决了不同物体类别间的几何差异问题，实现了单一模型对各类物体的有效跟踪，显著提升了系统的泛化能力和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D LiDAR-based single object tracking (SOT) relies on sparse and irregularpoint clouds, posing challenges from geometric variations in scale, motionpatterns, and structural complexity across object categories. Currentcategory-specific approaches achieve good accuracy but are impractical forreal-world use, requiring separate models for each category and showing limitedgeneralization. To tackle these issues, we propose TrackAny3D, the firstframework to transfer large-scale pretrained 3D models for category-agnostic 3DSOT. We first integrate parameter-efficient adapters to bridge the gap betweenpretraining and tracking tasks while preserving geometric priors. Then, weintroduce a Mixture-of-Geometry-Experts (MoGE) architecture that adaptivelyactivates specialized subnetworks based on distinct geometric characteristics.Additionally, we design a temporal context optimization strategy thatincorporates learnable temporal tokens and a dynamic mask weighting module topropagate historical information and mitigate temporal drift. Experiments onthree commonly-used benchmarks show that TrackAny3D establishes newstate-of-the-art performance on category-agnostic 3D SOT, demonstrating stronggeneralization and competitiveness. We hope this work will enlighten thecommunity on the importance of unified models and further expand the use oflarge-scale pretrained models in this field.</description>
      <author>example@mail.com (Mengmeng Wang, Haonan Wang, Yulong Li, Xiangjie Kong, Jiaxin Du, Guojiang Shen, Feng Xia)</author>
      <guid isPermaLink="false">2507.19908v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>PlaneHEC: Efficient Hand-Eye Calibration for Multi-view Robotic Arm via Any Point Cloud Plane Detection</title>
      <link>http://arxiv.org/abs/2507.19851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 IEEE International Conference on Robotics &amp;  Automation (ICRA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PlaneHEC是一种基于平面约束的手眼标定方法，仅使用深度相机，无需复杂模型，能够利用任意平面表面实现最优且快速的标定。&lt;h4&gt;背景&lt;/h4&gt;手眼标定在视觉引导机器人系统中很重要，用于确定相机坐标系和机器人末端执行器之间的变换矩阵。现有的多视图机器人系统方法通常依赖精确的几何模型或人工辅助，泛化能力差，且可能非常复杂和低效。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要复杂模型且仅使用深度相机就能完成的手眼标定方法，获得最优且最快的标定结果。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为PlaneHEC的通用手眼标定方法，基于平面约束引入手眼标定方程，使其具有强可解释性和泛化性。PlaneHEC使用综合解决方案，从闭式解开始，通过迭代优化改进，大大提高了准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实环境中全面评估了PlaneHEC的性能，并将其与其他基于点云的标定方法进行了比较，证明了其优越性。&lt;h4&gt;结论&lt;/h4&gt;通过计算模型的创新设计，实现了通用且快速的标定，为多智能体系统和具身智能的发展做出了重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;手眼标定是视觉引导机器人系统中的重要任务，对于确定相机坐标系和机器人末端执行器之间的变换矩阵至关重要。对于多视图机器人系统，现有方法通常依赖精确的几何模型或人工辅助，泛化能力差，而且可能非常复杂和低效。因此，在本研究中，我们提出了PlaneHEC，一种通用的手眼标定方法，不需要复杂模型，仅使用深度相机就能完成，利用墙壁和桌子等任意平面表面实现最优且最快的标定结果。PlaneHEC基于平面约束引入手眼标定方程，使其具有强可解释性和泛化性。PlaneHEC还使用综合解决方案，从闭式解开始，并通过迭代优化加以改进，大大提高了准确性。我们在模拟和真实环境中全面评估了PlaneHEC的性能，并将结果与其他基于点云的标定方法进行了比较，证明了其优越性。我们的方法通过计算模型的创新设计实现了通用且快速的标定，为多智能体系统和具身智能的发展做出了重要贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-eye calibration is an important task in vision-guided robotic systemsand is crucial for determining the transformation matrix between the cameracoordinate system and the robot end-effector. Existing methods, for multi-viewrobotic systems, usually rely on accurate geometric models or manualassistance, generalize poorly, and can be very complicated and inefficient.Therefore, in this study, we propose PlaneHEC, a generalized hand-eyecalibration method that does not require complex models and can be accomplishedusing only depth cameras, which achieves the optimal and fastest calibrationresults using arbitrary planar surfaces like walls and tables. PlaneHECintroduces hand-eye calibration equations based on planar constraints, whichmakes it strongly interpretable and generalizable. PlaneHEC also uses acomprehensive solution that starts with a closed-form solution and improves itwithiterative optimization, which greatly improves accuracy. We comprehensivelyevaluated the performance of PlaneHEC in both simulated and real-worldenvironments and compared the results with other point-cloud-based calibrationmethods, proving its superiority. Our approach achieves universal and fastcalibration with an innovative design of computational models, providing astrong contribution to the development of multi-agent systems and embodiedintelligence.</description>
      <author>example@mail.com (Ye Wang, Haodong Jing, Yang Liao, Yongqiang Ma, Nanning Zheng)</author>
      <guid isPermaLink="false">2507.19851v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>HydraMamba: Multi-Head State Space Model for Global Point Cloud Learning</title>
      <link>http://arxiv.org/abs/2507.19778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MM '25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HydraMamba是一种基于状态空间模型的点云网络，通过创新的序列化策略和局部学习机制解决了点云学习中的长程依赖建模问题，并在多种任务上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;注意力机制在点云学习中是主导算子，但其二次复杂度限制了点间交互和长程依赖建模。选择性状态空间模型（S6）具有线性复杂度和出色的长程建模能力，已被应用于点云学习。&lt;h4&gt;目的&lt;/h4&gt;探索一种基于状态空间模型的点云网络HydraMamba，解决点云序列化不完善和局部性学习不足的问题。&lt;h4&gt;方法&lt;/h4&gt;设计了洗牌序列化策略使无序点集适应S6的因果特性；提出ConvBiS6层协同捕获局部几何形状和全局上下文依赖；通过扩展多头设计到S6提出MHS6增强建模能力。&lt;h4&gt;主要发现&lt;/h4&gt;HydraMamba在物体级和场景级各种任务上取得了最先进的结果，代码已公开在GitHub上。&lt;h4&gt;结论&lt;/h4&gt;HydraMamba成功解决了点云序列化和局部性学习的挑战，通过创新的设计提升了点云学习的性能。&lt;h4&gt;翻译&lt;/h4&gt;注意力机制已成为点云学习中的主导算子，但其二次复杂度导致点间交互受限，阻碍了物体间的长程依赖建模。由于具有出色的长程建模能力和线性复杂度，选择性状态空间模型（S6）作为Mamba的核心，已被用于点云学习中整个点云的长程依赖交互。尽管取得了一些显著进展，相关工作仍存在点云序列化不完善和局部性学习不足的问题。为此，我们探索了一种基于状态空间模型的点云网络，称为HydraMamba，以解决上述挑战。具体而言，我们设计了一种洗牌序列化策略，使无序点集更好地适应S6的因果特性。同时，为了克服现有技术在局部性学习方面的不足，我们提出了ConvBiS6层，能够协同捕获局部几何形状和全局上下文依赖。此外，我们通过将多头设计扩展到S6，提出了MHS6，进一步增强了其建模能力。HydraMamba在物体级和场景级的各种任务上取得了最先进的结果。代码可在https://github.com/Point-Cloud-Learning/HydraMamba获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云学习中注意力机制的二次方复杂度限制长距离依赖建模的问题，以及现有状态空间模型(S6)在点云应用中存在的点云序列化不完善和缺乏局部学习能力的问题。这个问题在现实中非常重要，因为点云是3D视觉的重要数据表示，广泛应用于自动驾驶、机器人、AR/VR等领域；长距离依赖建模对理解物体关系至关重要；计算效率对处理大规模点云数据(如场景级数据)非常重要；而局部学习对捕获点云中的细节(如线条、边缘)同样重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了注意力机制在点云学习中的局限性，以及S6模型在点云应用中存在的序列化和局部学习问题。他们借鉴了PointMamba的Hilbert曲线序列化思想，但发现其存在冗余和效率问题；参考了NIMBA和PCM的轴排序方法，但认识到可能导致空间相近的点在序列中不相邻；同时受到多注意力头机制的启发，将其扩展到S6模型。基于这些观察，作者设计了洗牌序列化策略解决序列化问题，提出了ConvBiS6层增强局部学习能力，并创造了MHS6扩展多头机制到S6。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用S6的线性复杂度和长距离建模能力，结合改进的序列化策略和局部-全局结合的建模方法，实现对点云的高效理解。整体流程采用编码器-解码器架构：点云首先通过嵌入层转换，然后经过下采样和洗牌序列化，再通过多个HydraMamba块进行特征提取，最后通过上采样和任务头输出结果。每个HydraMamba块包含LayerNorm、ConvBiS6层、LayerNorm、MLP和残差连接，其中ConvBiS6层结合了双向S6和1D网格卷积，既能捕获全局上下文依赖，又能提取局部几何特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 洗牌序列化策略：随机分配六种Hilbert曲线变体给不同层，使网络能从多角度推断空间关系；2) ConvBiS6层：在双向S6上添加1D网格卷积分支，同时捕获局部几何形状和全局上下文依赖；3) MHS6：将多头设计扩展到S6，使其能灵活学习不同类型的时空动态。相比之前的工作，HydraMamba解决了PointMamba的冗余问题，优于NIMBA和PCM的轴排序方法，首次将多头设计引入S6，并首次在场景级大规模点云(超过100K点)上验证了S6-based网络的能力，实现了在物体级和场景级任务上的最先进结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HydraMamba通过创新的洗牌序列化策略、ConvBiS6层和多头部S6设计，首次实现了在点云学习中同时具备高效长距离依赖建模和局部几何特征提取能力的状态空间模型，并在物体级和场景级任务上取得了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The attention mechanism has become a dominant operator in point cloudlearning, but its quadratic complexity leads to limited inter-pointinteractions, hindering long-range dependency modeling between objects. Due toexcellent long-range modeling capability with linear complexity, the selectivestate space model (S6), as the core of Mamba, has been exploited in point cloudlearning for long-range dependency interactions over the entire point cloud.Despite some significant progress, related works still suffer from imperfectpoint cloud serialization and lack of locality learning. To this end, weexplore a state space model-based point cloud network termed HydraMamba toaddress the above challenges. Specifically, we design a shuffle serializationstrategy, making unordered point sets better adapted to the causal nature ofS6. Meanwhile, to overcome the deficiency of existing techniques in localitylearning, we propose a ConvBiS6 layer, which is capable of capturing localgeometries and global context dependencies synergistically. Besides, we proposeMHS6 by extending the multi-head design to S6, further enhancing its modelingcapability. HydraMamba achieves state-of-the-art results on various tasks atboth object-level and scene-level. The code is available athttps://github.com/Point-Cloud-Learning/HydraMamba.</description>
      <author>example@mail.com (Kanglin Qu, Pan Gao, Qun Dai, Yuanhao Sun)</author>
      <guid isPermaLink="false">2507.19778v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2507.19370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了BEV-LLM，一个用于自动驾驶场景3D描述的轻量级模型，通过结合激光雷达点云和多视图图像，实现了高透明度和可解释性，并发布了两个新数据集以改进场景描述评估。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶技术有潜力改变交通，但其广泛应用取决于开发可解释和透明的决策系统。场景描述通过生成驾驶环境的自然语言描述，在增强透明度、安全性和人机交互方面发挥着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级的自动驾驶场景3D描述模型，提高决策系统的透明度和可解释性，同时提供新的评估数据集以改进场景描述能力在不同驾驶场景中的评估。&lt;h4&gt;方法&lt;/h4&gt;提出BEV-LLM模型，利用BEVFusion技术结合3D激光雷达点云和多视图图像，并采用新颖的绝对位置编码来生成特定视图的场景描述。同时开发了两个新数据集：nuView和GroundView。&lt;h4&gt;主要发现&lt;/h4&gt;尽管使用小型1B参数基础模型，BEV-LLM在nuCaption数据集上取得了具有竞争力的性能，BLEU分数比最先进的方法提高了5%。新发布的数据集能够更好地评估不同驾驶场景中的场景描述能力。&lt;h4&gt;结论&lt;/h4&gt;BEV-LLM模型展示了轻量级模型在自动驾驶场景描述中的潜力，新数据集填补了当前基准测试中的空白，有助于提高自动驾驶系统的透明度和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶技术有潜力改变交通，但其广泛应用取决于开发可解释和透明的决策系统。场景描述通过生成驾驶环境的自然语言描述，在增强透明度、安全性和人机交互方面发挥着关键作用。我们介绍了BEV-LLM，一个用于自动驾驶场景3D描述的轻量级模型。BEV-LLM利用BEVFusion结合3D激光雷达点云和多视图图像，并采用新颖的绝对位置编码来生成特定视图的场景描述。尽管使用小型1B参数基础模型，BEV-LLM在nuCaption数据集上取得了具有竞争力的性能，BLEU分数比最先进的方法提高了5%。此外，我们还发布了两个新数据集 - nuView（关注环境条件和视角）和GroundView（关注对象基础） - 以更好地评估不同驾驶场景中的场景描述能力，并解决当前基准测试中的差距，同时提供了初步基准测试结果，证明了它们的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶场景中生成准确、详细的3D场景描述问题。这个问题在现实中非常重要，因为自动驾驶技术的广泛采用依赖于可解释和透明的决策系统。场景描述能够提高透明度、安全性和人机交互，使用户能够查询系统关于其环境和决策的信息，从而增强对自动驾驶系统的信任和理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶对可解释决策系统的需求，然后认识到现有方法如LiDAR-LLM的局限性（仅使用单一模态和缺乏绝对位置信息）。他们借鉴了BEVFusion的多模态融合技术，改进了Q-Former架构以处理BEV特征，并引入了绝对位置编码的概念。方法设计上，他们结合了360度多视图图像和LiDAR数据，创建了统一的BEV表示，并设计了基于正弦-余弦的位置嵌入来处理不同视角的信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多模态BEV地图结合绝对位置编码来生成详细的自动驾驶场景描述。整体流程包括：1) 使用BEVFusion处理六个环绕视图图像和LiDAR点云，生成统一的BEV特征图；2) 创建视图分类图并将BEV地图划分为六个视角扇区；3) 使用Positional Q-Former处理BEV特征并编码位置信息；4) 将特征投影到LLaMA-3的嵌入空间并与文本嵌入连接；5) 使用微调后的LLM生成场景描述。训练过程中，BEVFusion和LLM保持冻结，只训练Positional Q-Former、MLP和LoRA适配器。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多模态BEV融合，结合LiDAR和相机图像创建统一表示；2) 绝对位置编码，使用固定的几何关系而非可学习嵌入；3) 轻量级高性能模型，仅用1B参数实现超越最先进方法5%的BLEU分数；4) 提出两个新数据集nuView和GroundView。相比之前工作如LiDAR-LLM，BEV-LLM使用多模态输入而非单一模态，采用绝对位置编码而非可学习嵌入，实现了更高的参数效率和更好的环境描述能力（如天气条件等）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了BEV-LLM，一个通过融合多模态BEV地图和绝对位置编码来实现轻量级高性能自动驾驶场景描述的模型，同时引入了两个新数据集以促进该领域的研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving technology has the potential to transform transportation,but its wide adoption depends on the development of interpretable andtransparent decision-making systems. Scene captioning, which generates naturallanguage descriptions of the driving environment, plays a crucial role inenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,a lightweight model for 3D captioning of autonomous driving scenes. BEV-LLMleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,incorporating a novel absolute positional encoding for view-specific scenedescriptions. Despite using a small 1B parameter base model, BEV-LLM achievescompetitive performance on the nuCaption dataset, surpassing state-of-the-artby up to 5\% in BLEU scores. Additionally, we release two new datasets - nuView(focused on environmental conditions and viewpoints) and GroundView (focused onobject grounding) - to better assess scene captioning across diverse drivingscenarios and address gaps in current benchmarks, along with initialbenchmarking results demonstrating their effectiveness.</description>
      <author>example@mail.com (Felix Brandstaetter, Erik Schuetz, Katharina Winter, Fabian Flohr)</author>
      <guid isPermaLink="false">2507.19370v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection</title>
      <link>http://arxiv.org/abs/2507.19253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的统一多模态异常检测框架，解决了工业异常检测中仅使用2D信息识别3D深度异常不足的问题，以及多模态场景中不同模态信息差异导致的3D信息表示不充分问题，同时解决了工业数据中异常样本稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;工业异常检测在2D对象上已经取得进展，但仅使用2D信息识别3D深度异常是不够的。现有的方法在多模态场景中难以充分表示3D信息，且工业数据中异常样本稀缺。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的多模态异常检测框架，有效融合2D和3D信息进行异常检测，并解决异常样本稀缺的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种统一的多模态异常检测框架，包括：1) 从3D点云提取深度信息，用2D RGB表示外观，分离深度和外观；2) 多尺度高斯异常生成器和统一纹理异常生成器生成更丰富的异常；3) 所有模块共享RGB和深度数据参数，无需复杂融合即可利用两种模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在MVTec-3D AD和Eyecandies数据集上超越了最先进的方法，证明了其在多模态异常检测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架成功连接了2D和3D异常检测，通过统一的异常生成和多模态特征利用，有效解决了工业异常检测中的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;工业2D对象的异常检测已获得显著关注并在异常检测方法上取得进展。然而，仅使用2D信息识别3D深度异常是不够的。尽管将深度信息显式融合到RGB图像中或使用点云骨干网络提取深度特征，但这两种方法都因不同模态信息之间的差异而难以在多模态场景中充分表示3D信息。此外，由于工业数据中异常样本稀缺，特别是在多模态场景中，有必要进行异常生成来模拟真实世界的异常样本。因此，我们提出了一种新颖的统一多模态异常检测框架来解决这些问题。我们的贡献包括3个关键方面：(1) 我们简单地从3D点云数据中提取可见深度信息，并使用2D RGB图像表示外观，从而分离深度和外观以支持统一的异常生成。(2) 受益于灵活的输入表示，所提出的多尺度高斯异常生成器和统一纹理异常生成器可以在RGB和深度中生成更丰富的异常。(3) 所有模块共享RGB和深度数据的参数，有效连接2D和3D异常检测。后续模块可以直接利用两种模态的特征，无需复杂的融合。实验表明，我们的方法在MVTec-3D AD和Eyecandies数据集上优于最先进的方法。代码可在以下网址获取：https://github.com/Xantastic/BridgeNet&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决工业异常检测中2D和3D信息融合的挑战。仅使用2D信息无法充分识别3D深度异常，而现有的多模态融合方法难以充分表示3D信息。此外，工业数据中异常样本稀缺，特别是在多模态场景中，需要异常生成来模拟真实异常。这个问题在现实中非常重要，因为工业异常检测是质量控制的基础，准确检测深度异常对产品质量保证至关重要，而多模态信息融合可以提高异常检测的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：仅2D信息无法充分检测3D深度异常，直接融合深度信息的方法难以充分表示3D信息，现有3D异常生成方法与真实异常差异大。作者借鉴了现有工作：参考了2D异常检测方法（基于嵌入、重建和知识蒸馏的方法），借鉴了GLASS方法中随机选择异常区域组合的思想，参考了2D纹理异常生成方法（使用Perlin噪声和DTD数据集）。设计方法时考虑了将点云转换为深度图像，设计多尺度高斯异常生成器和统一纹理异常生成器，实现RGB和深度图像的参数共享框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 统一参数共享框架，为RGB和深度图像共享所有模型参数，实现双模态特征的隐式对齐；2) 多尺度高斯异常生成器(MGAG)，在不同语义级别添加多尺度高斯噪声；3) 统一纹理异常生成器(UTAG)，将2D纹理异常生成方法扩展到3D深度图像。整体流程：首先使用共享参数的特征提取器提取RGB和深度图像特征，然后通过融合适配器将特征转换到目标域；接着MGAG在不同模型级别添加多尺度高斯噪声，UTAG应用纹理异常；最后双模态判别器接收特征并输出预测掩码，计算损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 统一参数共享框架，实现双模态特征的隐式对齐；2) 多尺度高斯异常生成器(MGAG)，在多个级别添加不同尺度噪声；3) 统一纹理异常生成器(UTAG)，通过纹理桥接2D和3D异常。相比之前工作的不同：与直接拼接3D信息的方法不同，BridgeNet将点云转换为深度图像后提取特征；与单尺度Perlin噪声生成方法不同，使用多尺度高斯噪声；与使用独立自编码器的方法不同，共享所有模块参数；与需要额外模态对齐的方法不同，通过参数共享实现隐式对齐。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BridgeNet通过统一参数共享框架和多模态异常生成方法，有效桥接了2D和3D工业异常检测，显著提升了检测性能，实现了在多模态场景下的最新技术水平。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial anomaly detection for 2D objects has gained significant attentionand achieved progress in anomaly detection (AD) methods. However, identifying3D depth anomalies using only 2D information is insufficient. Despiteexplicitly fusing depth information into RGB images or using point cloudbackbone networks to extract depth features, both approaches struggle toadequately represent 3D information in multimodal scenarios due to thedisparities among different modal information. Additionally, due to thescarcity of abnormal samples in industrial data, especially in multimodalscenarios, it is necessary to perform anomaly generation to simulate real-worldabnormal samples. Therefore, we propose a novel unified multimodal anomalydetection framework to address these issues. Our contributions consist of 3 keyaspects. (1) We extract visible depth information from 3D point cloud datasimply and use 2D RGB images to represent appearance, which disentangles depthand appearance to support unified anomaly generation. (2) Benefiting from theflexible input representation, the proposed Multi-Scale Gaussian AnomalyGenerator and Unified Texture Anomaly Generator can generate richer anomaliesin RGB and depth. (3) All modules share parameters for both RGB and depth data,effectively bridging 2D and 3D anomaly detection. Subsequent modules candirectly leverage features from both modalities without complex fusion.Experiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D ADand Eyecandies datasets. Code available at:https://github.com/Xantastic/BridgeNet</description>
      <author>example@mail.com (An Xiang, Zixuan Huang, Xitong Gao, Kejiang Ye, Cheng-zhong Xu)</author>
      <guid isPermaLink="false">2507.19253v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet</title>
      <link>http://arxiv.org/abs/2507.19209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CounterNet的基于热图的网络，用于提高自动驾驶车辆点云数据中的对象计数准确性，从而改善数据查询结果。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆产生大量点云数据，但只有特定子集与特定任务相关。有效查询这些数据对实现有针对性的分析至关重要，而准确的计数是查询执行的关键组件。&lt;h4&gt;目的&lt;/h4&gt;解决3D点云数据中对象计数不准确的问题，提高点云数据查询的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出CounterNet，一种基于热图的网络，通过查找对象中心而非精确定位来提高计数准确性；使用重叠区域的特征图分区策略处理不同大小的对象；引入每帧动态模型选择策略以适应不同帧特征。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界自动驾驶数据集上评估，CounterNet将各类对象的计数准确性提高了5%至20%，使所有支持的查询类型（检索、计数和聚合）的结果更加可靠。&lt;h4&gt;结论&lt;/h4&gt;CounterNet有效解决了3D点云数据中对象计数不准确的问题，显著提高了点云数据查询的可靠性，对自动驾驶车辆的数据分析具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆产生大量点云数据，但只有特定子集与特定任务相关，如碰撞检测、交通分析或拥堵监控。有效查询这些数据对于实现有针对性的分析至关重要。在这项工作中，我们通过定义三种核心查询类型（检索、计数和聚合）来形式化点云查询，每种类型对应不同的分析场景。所有这些查询都严重依赖准确的计数来产生有意义的结果，使得精确的对象计数成为查询执行的关键组成部分。先前的工作主要关注2D视频数据的索引技术，假设检测模型能提供准确的计数信息。然而，当应用于3D点云数据时，最先进的检测模型往往无法生成可靠的对象计数，导致查询结果中出现重大错误。为解决这一局限，我们提出了CounterNet，一种基于热图的网络，专为大规模点云数据中的准确对象计数而设计。CounterNet不专注于准确的对象定位，而是通过查找对象中心来检测对象存在，以提高计数准确性。我们使用重叠区域的特征图分区策略进一步增强其性能，使其能够更好地处理复杂交通场景中的小对象和大对象。为适应不同帧的特征，我们引入了每帧动态模型选择策略，为每个输入选择最有效的配置。在三个真实世界自动驾驶数据集上的评估显示，CounterNet将各类对象的计数准确性提高了5%至20%，使所有支持的查询类型的结果更加可靠。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶车辆点云数据的有效查询问题。自动驾驶车辆每小时可产生约5TB的点云数据，但只有一部分数据对特定任务（如碰撞检测、交通分析）是相关的。当前3D检测模型在点云数据上往往无法提供可靠的对象计数，导致查询结果出现重大错误。这个问题很重要，因为它关系到如何从海量数据中高效提取有用信息，支持自动驾驶系统的决策和优化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有工作的局限性：2D视频查询方法假设检测模型能提供准确计数，但当应用于3D点云数据时，这种假设不成立。作者借鉴了视频数据查询（如BlazeIt、Seiden）和点云数据查询（如MAST）的索引思想，但转向了更基础的问题——提高对象计数准确性。他们设计了CounterNet，这是一种基于热力图的网络，专注于检测对象中心而非精确定位，并引入分区策略和动态模型选择来提高计数精度，从而解决根本问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用热力图检测对象中心而非传统边界框，通过检测对象存在来提高计数准确性。整体流程包括：1) 使用骨干网络提取点云特征并投影为2D鸟瞰图；2) 为每个对象类别生成热力图；3) 使用焦点损失监督热力图生成；4) 通过阈值化的局部最大值检测对象中心；5) 使用L1损失监督计数准确性。优化方面包括特征图分区处理高密度场景、重叠分区防止对象被分割、以及动态模型选择适应不同帧特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) CounterNet架构，基于热力图检测对象中心而非精确定位；2) 特征图分区策略，将特征图划分为更小区域处理高密度场景；3) 重叠分区设计，防止对象被分割到多个分区；4) 动态模型选择，根据帧特征自动选择最适合的模型配置。与之前工作的不同在于：现有工作主要关注索引技术以提高查询效率，而本文专注于提高对象计数准确性这一更基础的问题；现有方法假设检测模型能提供准确计数，而本文认识到3D检测模型的局限性并直接解决计数问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CounterNet通过基于热力图的对象中心检测和创新的分区策略，显著提高了自动驾驶点云数据中对象计数的准确性，从而提升了三种核心查询类型（RETRIEVAL、COUNT和AGGREGATION）的可靠性和效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles generate massive volumes of point cloud data, yet only asubset is relevant for specific tasks such as collision detection, trafficanalysis, or congestion monitoring. Effectively querying this data is essentialto enable targeted analytics. In this work, we formalize point cloud queryingby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, eachaligned with distinct analytical scenarios. All these queries rely heavily onaccurate object counts to produce meaningful results, making precise objectcounting a critical component of query execution. Prior work has focused onindexing techniques for 2D video data, assuming detection models provideaccurate counting information. However, when applied to 3D point cloud data,state-of-the-art detection models often fail to generate reliable objectcounts, leading to substantial errors in query results. To address thislimitation, we propose CounterNet, a heatmap-based network designed foraccurate object counting in large-scale point cloud data. Rather than focusingon accurate object localization, CounterNet detects object presence by findingobject centers to improve counting accuracy. We further enhance its performancewith a feature map partitioning strategy using overlapping regions, enablingbetter handling of both small and large objects in complex traffic scenes. Toadapt to varying frame characteristics, we introduce a per-frame dynamic modelselection strategy that selects the most effective configuration for eachinput. Evaluations on three real-world autonomous vehicle datasets show thatCounterNet improves counting accuracy by 5% to 20% across object categories,resulting in more reliable query outcomes across all supported query types.</description>
      <author>example@mail.com (Xiaoyu Zhang, Zhifeng Bao, Hai Dong, Ziwei Wang, Jiajun Liu)</author>
      <guid isPermaLink="false">2507.19209v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Preserving Topological and Geometric Embeddings for Point Cloud Recovery</title>
      <link>http://arxiv.org/abs/2507.19121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TopGeoFormer的端到端架构，用于点云恢复，有效解决了现有方法难以同时利用拓扑和几何属性的问题。&lt;h4&gt;背景&lt;/h4&gt;点云恢复涉及采样和恢复的顺序过程，但现有方法难以有效利用拓扑和几何属性。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在采样和恢复阶段同时保持拓扑和几何关键特征的端到端架构。&lt;h4&gt;方法&lt;/h4&gt;1) 重新审视传统特征提取技术，通过连续映射邻居点间的相对关系生成拓扑嵌入；2) 提出交织注意力机制(InterTwining Attention)融合拓扑和几何嵌入；3) 引入完整的几何损失和拓扑约束损失优化欧几里得和拓扑空间中的嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;几何损失通过粗到细生成与目标间的不一致匹配重建更好的几何细节；约束损失限制嵌入方差，以更好地近似拓扑空间。&lt;h4&gt;结论&lt;/h4&gt;定量和定性结果表明，TopGeoFormer方法显著优于现有的采样和恢复方法。&lt;h4&gt;翻译&lt;/h4&gt;点云恢复涉及采样和恢复的顺序过程，然而现有方法难以有效利用拓扑和几何属性。为解决这一问题，我们提出了一种名为TopGeoFormer的端到端架构，它在采样和恢复阶段都保持了这些关键特征。首先，我们重新审视传统特征提取技术，使用连续映射邻居点之间的相对关系来产生拓扑嵌入，并在两个阶段都集成它以保留原始空间结构。其次，我们提出交织注意力机制来完全融合拓扑和几何嵌入，在两个阶段中查询具有局部感知能力的形状，形成可学习的形状上下文，并借助点级、点-形状级和形状内特征。第三，我们引入完整的几何损失和拓扑约束损失来优化欧几里得和拓扑空间中的嵌入。几何损失使用从粗到细生成与目标之间不一致的匹配来重建更好的几何细节，约束损失限制嵌入方差以更好地近似拓扑空间。在实验中，我们使用传统和基于学习的采样/上采样算法全面分析了各种情况。定量和定性结果表明，我们的方法显著优于现有的采样和恢复方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决点云恢复问题，这是一个涉及点云下采样和后续恢复的过程。这个问题在现实中很重要，因为3D扫描技术产生的密集点云数据量很大，需要高效整合来自远程设备的数据；在研究中很重要，因为现有方法难以有效利用点云的拓扑和几何属性，导致恢复过程中细节丢失，而点云恢复在远程机器人导航、增强现实交互和医疗手术交互等多种应用中是基础方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先重新审视传统特征提取技术，利用点之间相对关系的连续映射生成拓扑嵌入；注意到现有方法(如PointSE)使用kNN操作表述拓扑特征但计算昂贵且效果差；发现现有损失函数一致应用于点云和地面真值，导致局部几何细节不足。作者借鉴了PointNet++的拓扑信息提取、转置卷积解码器的自注意力层设计以及manifold learning理论来保持嵌入一致性，最终设计了TopGeoFormer架构，包含下保持和上保持两个阶段。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1)重新审视特征提取技术，利用点间关系生成拓扑嵌入并在采样和恢复阶段整合；2)提出交织注意力(ITA)融合拓扑和几何嵌入；3)设计特定损失函数优化嵌入。整体流程：下保持阶段提取拓扑和几何嵌入，创建点-形状融合表示，应用ITA预测位移生成可微采样点；上保持阶段动态更新点特征，使用上保持注意力逐步恢复点云；优化过程使用完整几何损失和拓扑约束损失在欧几里得和拓扑空间中优化嵌入。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)重新审视特征提取技术生成拓扑嵌入；2)提出参数更少(0.08M)性能更好的交织注意力(ITA)；3)设计完整几何损失和拓扑约束损失；4)端到端TopGeoFormer架构。不同点：相比PointSE，更有效融合拓扑几何信息，使用特定损失优化；相比上采样方法，更关注恢复细节而非均匀点；相比点云补全方法，不需要向解码器传输辅助数据，更适合远程传输场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TopGeoFormer通过保留拓扑和几何嵌入，提出了一种端到端点云恢复方法，在采样和恢复任务中都显著优于现有方法，能够更好地保留原始点云的局部细节和整体形状。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering point clouds involves the sequential process of sampling andrestoration, yet existing methods struggle to effectively leverage bothtopological and geometric attributes. To address this, we propose an end-to-endarchitecture named \textbf{TopGeoFormer}, which maintains these criticalfeatures throughout the sampling and restoration phases. First, we revisittraditional feature extraction techniques to yield topological embedding usinga continuous mapping of relative relationships between neighboring points, andintegrate it in both phases for preserving the structure of the original space.Second, we propose the \textbf{InterTwining Attention} to fully mergetopological and geometric embeddings, which queries shape with local awarenessin both phases to form a learnable shape context facilitated with point-wise,point-shape-wise, and intra-shape features. Third, we introduce a full geometryloss and a topological constraint loss to optimize the embeddings in bothEuclidean and topological spaces. The geometry loss uses inconsistent matchingbetween coarse-to-fine generations and targets for reconstructing bettergeometric details, and the constraint loss limits embedding variances forbetter approximation of the topological space. In experiments, wecomprehensively analyze the circumstances using the conventional andlearning-based sampling/upsampling algorithms. The quantitative and qualitativeresults demonstrate that our method significantly outperforms existing samplingand recovery methods.</description>
      <author>example@mail.com (Kaiyue Zhou, Zelong Tan, Hongxiao Wang, Ya-li Li, Shengjin Wang)</author>
      <guid isPermaLink="false">2507.19121v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis</title>
      <link>http://arxiv.org/abs/2507.18997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025 as a Poster&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种统一的点级提示方法(UPP)，将点云去噪和补全重新构架为提示机制，实现参数高效的鲁棒分析。通过校正提示器、补全提示器和形状感知单元三个组件，有效处理噪声和不完整点云数据，保留关键几何特征，提升下游任务性能。&lt;h4&gt;背景&lt;/h4&gt;预训练的点云分析模型在各种下游任务中表现出色，但受低质量点云（噪声和不完整性）影响显著。这是现实场景中的常见问题，由物体遮挡和3D传感器数据收集不理想导致。现有去噪和补全方法与下游任务隔离，且去噪与补全任务目标冲突，限制了集成范式保留关键几何特征的能力。&lt;h4&gt;目的&lt;/h4&gt;解决点云分析中因噪声和不完整性导致的模型性能下降问题，开发一种统一的方法，将去噪和补全任务整合为提示机制，实现参数高效的鲁棒分析，适用于各种现实世界的下游任务。&lt;h4&gt;方法&lt;/h4&gt;提出包含三个核心组件的统一方法：1) 校正提示器：通过预测的校正向量提示适应噪声点，过滤噪声同时保留关键几何特征；2) 补全提示器：基于校正后点云生成辅助点提示，增强鲁棒性和适应性；3) 形状感知单元模块：统一和捕获过滤后的几何特征，支持下游点云分析任务。&lt;h4&gt;主要发现&lt;/h4&gt;在四个数据集上的大量实验表明，该方法在处理噪声和不完整点云数据时，与现有最先进方法相比具有明显优势和鲁棒性，能有效保留关键几何特征并提升下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;统一的点级提示方法有效解决了点云分析中的噪声和不完整性挑战，通过将去噪和补全任务整合为提示机制，实现了参数高效的鲁棒分析，适用于各种现实世界的下游应用场景。代码已公开发布。&lt;h4&gt;翻译&lt;/h4&gt;预训练的点云分析模型在各种下游任务中显示出有前景的进展，但它们的有效性通常受到低质量点云（即噪声和不完整性）的影响，这是现实场景中的常见问题，由随机的物体遮挡和3D传感器收集的不满意数据导致。为此，现有方法专注于开发专门的去噪和补全模型来增强点云质量。然而，由于点云增强与下游任务之间的隔离，这些方法在各种现实领域中无法有效工作。此外，去噪和补全任务之间的冲突目标进一步限制了集成范式保留关键几何特征的能力。为了解决上述挑战，我们提出了一种统一的点级提示方法，将点云去噪和补全重新构架为提示机制，以实现参数高效的鲁棒分析。我们首先引入了一个校正提示器，通过预测的校正向量提示来适应噪声点，有效过滤噪声同时保留准确分析所需的复杂几何特征。随后，我们进一步融入了一个补全提示器，基于校正后的点云生成辅助点提示，增强其鲁棒性和适应性。最后，利用一个形状感知单元模块来有效地统一和捕获过滤后的几何特征，用于下游点云分析。在四个数据集上的大量实验表明，与现有的最先进方法相比，我们的方法在处理噪声和不完整点云数据时具有优越性和鲁棒性。我们的代码已发布在https://github.com/zhoujiahuan1991/ICCV2025-UPP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决预训练点云分析模型在处理低质量点云（噪声和不完整性）时性能下降的问题。这个问题在现实中非常重要，因为实际场景中收集的点云数据常受物体遮挡、反射表面和传感器分辨率限制等因素影响，含有大量噪声和不完整数据，这严重抑制了预训练模型在实际应用中的性能和可靠性。此外，现有方法虽然开发专门的去噪和补全模型，但由于点云增强与下游任务之间的隔离，这些方法在各种现实领域无法有效工作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有点云增强方法与下游任务之间存在隔离，导致性能下降；去噪和补全任务存在冲突目标，难以集成；大多数参数高效微调方法忽略了输入点云中噪声和缺陷的明确抑制。作者借鉴了预训练点云模型（如Point-MAE）、参数高效微调理念、点云去噪和补全研究成果以及Transformer架构和自注意力机制的设计思想，在此基础上设计了UPP框架，将去噪和补全任务重新定义为点级提示机制，实现参数高效的鲁棒分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云去噪和补全任务重新定义为点级提示机制，通过统一的框架同时处理点云增强和下游分析任务，避免传统方法中任务之间的隔离。整体流程：1)输入噪声和不完整点云；2)通过校正提示器处理，预测校正向量提示过滤噪声点，生成校正后的点云；3)通过补全提示器处理，预测缺失部分并生成辅助点提示，结合校正点和补全点；4)通过形状感知单元处理，利用自注意力和基于空间距离的注意力机制增强特征；5)将处理后的特征提供给下游任务头进行分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的点级提示框架(UPP)，将去噪和补全重新定义为提示机制；2)校正提示器，适应不同噪声水平点云；3)补全提示器，生成辅助点提示；4)形状感知单元，统一捕获几何特征。相比之前工作：1)传统方法使用独立模型形成多步集成范式，UPP采用统一端到端框架；2)传统PEFT方法忽略输入点云中噪声抑制，UPP通过点级提示直接处理；3)传统注意力基于特征相似性易受噪声干扰，UPP的形状感知注意力基于空间距离提高鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UPP提出了一种统一的点级提示框架，将点云去噪和补全重新定义为提示机制，通过参数高效的方式实现了对噪声和不完整点云数据的鲁棒分析，显著提升了预训练点云模型在实际应用中的性能和可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained point cloud analysis models have shown promising advancements invarious downstream tasks, yet their effectiveness is typically suffering fromlow-quality point cloud (i.e., noise and incompleteness), which is a commonissue in real scenarios due to casual object occlusions and unsatisfactory datacollected by 3D sensors. To this end, existing methods focus on enhancing pointcloud quality by developing dedicated denoising and completion models. However,due to the isolation between the point cloud enhancement and downstream tasks,these methods fail to work in various real-world domains. In addition, theconflicting objectives between denoising and completing tasks further limit theensemble paradigm to preserve critical geometric features. To tackle the abovechallenges, we propose a unified point-level prompting method that reformulatespoint cloud denoising and completion as a prompting mechanism, enabling robustanalysis in a parameter-efficient manner. We start by introducing aRectification Prompter to adapt to noisy points through the predictedrectification vector prompts, effectively filtering noise while preservingintricate geometric features essential for accurate analysis. Sequentially, wefurther incorporate a Completion Prompter to generate auxiliary point promptsbased on the rectified point clouds, facilitating their robustness andadaptability. Finally, a Shape-Aware Unit module is exploited to efficientlyunify and capture the filtered geometric features for the downstream pointcloud analysis.Extensive experiments on four datasets demonstrate thesuperiority and robustness of our method when handling noisy and incompletepoint cloud data against existing state-of-the-art methods. Our code isreleased at https://github.com/zhoujiahuan1991/ICCV2025-UPP.</description>
      <author>example@mail.com (Zixiang Ai, Zhenyu Cui, Yuxin Peng, Jiahuan Zhou)</author>
      <guid isPermaLink="false">2507.18997v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>PDT: Point Distribution Transformation with Diffusion Models</title>
      <link>http://arxiv.org/abs/2507.18939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://shanemankiw.github.io/PDT/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PDT的新型点分布变换框架，利用扩散模型将无结构的点云转换为语义上有意义的结构化点分布，为3D几何处理任务提供了强大工具。&lt;h4&gt;背景&lt;/h4&gt;点表示法在几何数据结构中一直发挥着重要作用。大多数点云学习和处理方法通常利用无序和无约束的特性来表示3D形状的底层几何结构。&lt;h4&gt;目的&lt;/h4&gt;解决如何从无结构的点云分布中提取有意义的结构信息，并将其转换为语义上有意义的点分布这一探索不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出PDT框架，给定一组输入点，学习将点集从原始几何分布转换为语义上有意义的目标分布。利用具有新颖架构和学习策略的扩散模型，通过去噪过程有效关联源分布和目标分布。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，该方法能够成功将输入点云转换为各种形式的结构化输出，包括表面对齐的关键点、内部稀疏关节和连续特征线。&lt;h4&gt;结论&lt;/h4&gt;PDT框架能够捕捉几何和语义特征，为需要结构化点分布的各种3D几何处理任务提供了强大工具。&lt;h4&gt;翻译&lt;/h4&gt;点表示法在几何数据结构中一直发挥着重要作用。大多数点云学习和处理方法通常利用无序和无约束的特性来表示3D形状的底层几何结构。然而，如何从无结构的点云分布中提取有意义的结构信息，并将它们转换为语义上有意义的点分布，仍然是一个探索不足的问题。我们提出PDT，一种基于扩散模型的点分布变换新型框架。给定一组输入点，PDT学习将点集从其原始几何分布转换为语义上有意义的目标分布。我们的方法利用具有新颖架构和学习策略的扩散模型，通过去噪过程有效关联源分布和目标分布。通过大量实验，我们表明该方法成功地将输入点云转换为各种形式的结构化输出——从表面对齐的关键点、内部稀疏关节到连续特征线。结果展示了我们的框架捕捉几何和语义特征的能力，为需要结构化点分布的各种3D几何处理任务提供了强大的工具。代码将在以下链接提供：https://github.com/shanemankiw/PDT。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从无结构的点云分布中提取有意义的信息，并将它们转换为语义上有意义的点分布的问题。这个问题在3D几何处理、计算机图形学和计算机视觉领域非常重要，因为点云作为几何数据结构的基本组成部分，其无结构和不规则特性使得有效感知高级语义信息和生成结构化表示具有挑战性。解决这个问题可以应用于多个实际场景，如艺术家启发的网格生成、角色绑定和服装分析等。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了扩散模型（特别是去噪扩散概率模型DDPM）的思想，注意到其在3D生成任务中的成功，但认为其在点分布转换方面的潜力尚未被探索。他们设计了点对点引导的扩散框架，结合了Point Voxel CNN（PVCNN）特征提取器和Diffusion Transformer（DiT）架构，并特别设计了噪声调度和采样梯度调整策略以提高精度。这种设计思路体现了对现有扩散模型技术的创造性应用和针对性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用扩散模型学习从任意表面点分布到语义上有意义的目标点分布的转换，通过建立源分布和目标分布之间的概率映射，并在扩散过程中保持每个点与参考点之间的显式对应关系。整体流程包括：1)使用PVCNN从参考点云提取特征；2)应用位置编码；3)组合特征形成DiT输入；4)加入时间步嵌入；5)通过DiT层处理学习分布转换；6)通过去噪过程将噪声转换为有意义的点分布；7)在推理阶段应用采样梯度调整提高精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)点分布转换框架，使用扩散模型进行点云转换；2)点对点引导的扩散，建立源点和目标点间的显式对应；3)自适应噪声调度，在去噪后期降低噪声水平；4)采样梯度调整策略，提高转换可控性；5)多任务应用能力。相比之前工作，PDT不同于传统的确定性前馈网络方法，采用生成式方法处理点分布转换；不同于现有的点云生成方法，专注于点分布间的转换而非从噪声生成点云；也不同于专注于单个分布建模的方法，学习不同点分布间的映射关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PDT提出了一种基于扩散模型的点分布转换框架，能够将无结构的点云转换为语义上有意义的结构化点分布，并在网格重划分、角色绑定和服装特征线提取等任务中展现出优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3721238.3730717&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point-based representations have consistently played a vital role ingeometric data structures. Most point cloud learning and processing methodstypically leverage the unordered and unconstrained nature to represent theunderlying geometry of 3D shapes. However, how to extract meaningful structuralinformation from unstructured point cloud distributions and transform them intosemantically meaningful point distributions remains an under-explored problem.We present PDT, a novel framework for point distribution transformation withdiffusion models. Given a set of input points, PDT learns to transform thepoint set from its original geometric distribution into a target distributionthat is semantically meaningful. Our method utilizes diffusion models withnovel architecture and learning strategy, which effectively correlates thesource and the target distribution through a denoising process. Throughextensive experiments, we show that our method successfully transforms inputpoint clouds into various forms of structured outputs - ranging fromsurface-aligned keypoints, and inner sparse joints to continuous feature lines.The results showcase our framework's ability to capture both geometric andsemantic features, offering a powerful tool for various 3D geometry processingtasks where structured point distributions are desired. Code will be availableat this link: https://github.com/shanemankiw/PDT.</description>
      <author>example@mail.com (Jionghao Wang, Cheng Lin, Yuan Liu, Rui Xu, Zhiyang Dou, Xiao-Xiao Long, Hao-Xiang Guo, Taku Komura, Wenping Wang, Xin Li)</author>
      <guid isPermaLink="false">2507.18939v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform</title>
      <link>http://arxiv.org/abs/2507.18870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究点云对抗攻击对评估和改进3D深度学习模型鲁棒性至关重要。现有攻击方法在理想白盒环境下开发，但可迁移性有限且对防御机制鲁棒性不足。本文提出MAT-Adv框架，通过扰动中轴变换表示提高攻击的可迁移性和不可防御性。&lt;h4&gt;背景&lt;/h4&gt;研究点云对抗攻击对于评估和改进3D深度学习模型鲁棒性至关重要。然而，大多数现有攻击方法是在理想白盒环境下开发的，通常对未见过模型的迁移能力有限，且对常见防御机制的鲁棒性不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的对抗攻击框架MAT-Adv，通过显式扰动中轴变换(MAT)表示来提高攻击的可迁移性和不可防御性，在生成的点云中引入内在的对抗特性。&lt;h4&gt;方法&lt;/h4&gt;提出MAT-Adv框架，使用自编码器将输入点云投影到捕获点云内在几何结构的紧凑MAT表示，通过扰动这些内在表示引入结构级别的对抗特性，并在MAT扰动的优化中融入dropout策略以缓解过拟合和防止扰动崩溃。&lt;h4&gt;主要发现&lt;/h4&gt;MAT-Adv在可迁移性和不可防御性方面显著优于现有最先进方法；通过结构级别的对抗特性，攻击在不同模型和防御策略中保持有效；dropout策略有效缓解了过拟合和扰动崩溃问题。&lt;h4&gt;结论&lt;/h4&gt;MAT-Adv是一种有效的对抗攻击框架，通过扰动点云的中轴变换表示生成具有内在对抗特性的点云，在评估和改进3D深度学习模型鲁棒性方面具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;研究点云对抗攻击对于评估和改进3D深度学习模型的鲁棒性至关重要。然而，大多数现有攻击方法是在理想白盒环境下开发的，通常对未见过模型的迁移能力有限，且对常见防御机制的鲁棒性不足。在本文中，我们提出了MAT-Adv，一种新的对抗攻击框架，通过显式扰动中轴变换(MAT)表示来提高可迁移性和不可防御性，从而在生成的点云中引入内在的对抗特性。具体而言，我们使用自编码器将输入点云投影到捕获点云内在几何结构的紧凑MAT表示。通过扰动这些内在表示，MAT-Adv引入了结构级别的对抗特性，这些特性在不同模型和防御策略中保持有效。为了缓解过拟合并防止扰动崩溃，我们在MAT扰动的优化中融入了dropout策略，进一步提高了可迁移性和不可防御性。大量实验证明，MAT-Adv在可迁移性和不可防御性方面显著优于现有最先进的方法。代码将在论文接受后公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有点云对抗攻击方法的两个局限性：一是对未见模型的转移性有限，二是对常见防御机制缺乏鲁棒性。这个问题在现实中非常重要，因为自动驾驶、服务机器人等安全关键应用中，模型可靠性至关重要。研究点云对抗攻击有助于评估3D深度学习模型的脆弱性，评估安全风险，并最终促进未来鲁棒性的提升。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到点云本质上是嵌入3D欧几里得空间的2流形表面的采样，受到MAT(中轴变换)这一经典形状分析工具的启发，它能捕捉几何对象的内在结构特性。作者借鉴了AdvPC和Mani-Adv等现有工作，这些方法试图通过结构考虑促进内在对抗性。作者提出了一种更合理和可解释的方法，通过扰动内在和明确定义的几何结构来增强内在对抗性，从而提高对抗样本的转移性和不可防御性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扰动点云的中轴变换(MAT)表示来生成对抗样本。MAT表示捕捉了点云的内在几何结构，扰动这种表示可以增强对抗样本的内在对抗性，使生成的对抗样本能够在不同模型和防御机制下保持有效。整体实现流程包括：1)使用自编码器将输入点云编码为MAT表示(包括中轴球体的中心和半径)；2)对MAT表示应用对抗性扰动；3)使用解码器将扰动后的MAT表示重构为对抗点云；4)引入dropout策略来减少对特定模型的过拟合，防止扰动崩溃。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了一种新的对抗攻击框架MAT-Adv，通过显式扰动中轴变换(MAT)表示来增强转移性和不可防御性；2)设计了一种深度自编码器来提取和重构MAT表示；3)引入了dropout策略来优化MAT扰动，提高转移性和不可防御性；4)首次将MAT应用于点云对抗攻击领域。相比之前的工作，MAT-Adv不直接扰动点坐标，而是扰动内在几何表示；与AdvPC不同，它不使用去噪自编码器作为约束；与Mani-Adv不同，它在更明确和可解释的MAT表示中操作，而非学习的2D流形空间。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于中轴变换(MAT)的点云对抗攻击方法MAT-Adv，通过扰动点云的内在几何结构表示，显著提高了对抗样本的转移性和对常见防御机制的抵抗能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Studying adversarial attacks on point clouds is essential for evaluating andimproving the robustness of 3D deep learning models. However, most existingattack methods are developed under ideal white-box settings and often sufferfrom limited transferability to unseen models and insufficient robustnessagainst common defense mechanisms. In this paper, we propose MAT-Adv, a noveladversarial attack framework that enhances both transferability andundefendability by explicitly perturbing the medial axis transform (MAT)representations, in order to induce inherent adversarialness in the resultingpoint clouds. Specifically, we employ an autoencoder to project input pointclouds into compact MAT representations that capture the intrinsic geometricstructure of point clouds. By perturbing these intrinsic representations,MAT-Adv introduces structural-level adversarial characteristics that remaineffective across diverse models and defense strategies. To mitigate overfittingand prevent perturbation collapse, we incorporate a dropout strategy into theoptimization of MAT perturbations, further improving transferability andundefendability. Extensive experiments demonstrate that MAT-Adv significantlyoutperforms existing state-of-the-art methods in both transferability andundefendability. Codes will be made public upon paper acceptance.</description>
      <author>example@mail.com (Keke Tang, Yuze Gao, Weilong Peng, Xiaofei Wang, Meie Fang, Peican Zhu)</author>
      <guid isPermaLink="false">2507.18870v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2507.18575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了HybridTM，这是首个结合Transformer和Mamba用于3D语义分割的混合架构，解决了Transformer二次方复杂度限制和Mamba特征表示困难的问题。&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的方法在3D语义分割中表现出色，但二次方复杂度限制了其在处理大规模点云时建模长程依赖关系的能力。而基于Mamba的方法虽然具有线性复杂度，但在提取3D特征时存在特征表示的困难。&lt;h4&gt;目的&lt;/h4&gt;有效结合Transformer和Mamba这两种方法的互补优势，解决3D语义分割中的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出HybridTM混合架构，以及内部层混合策略，在更细粒度上结合注意力和Mamba，同时捕获长程依赖关系和细粒度局部特征。&lt;h4&gt;主要发现&lt;/h4&gt;HybridTM在各种室内和室外数据集上表现出良好的有效性和泛化能力，在ScanNet、ScanNet200和nuScenes基准测试上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;HybridTM成功结合了Transformer和Mamba的优势，解决了各自方法的局限性，在3D语义分割任务上取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于Transformer的方法通过其强大的注意力机制在3D语义分割中展现出卓越的能力，但二次方复杂度限制了它们在大规模点云中建模长程依赖关系的能力。虽然最近的基于Mamba的方法具有线性复杂度，能够高效处理，但在提取3D特征时存在特征表示的困难。然而，如何有效结合这些互补优势仍然是该领域的一个开放挑战。在本文中，我们提出了HybridTM，这是第一个结合Transformer和Mamba用于3D语义分割的混合架构。此外，我们还提出了内部层混合策略，在更细的粒度上结合注意力和Mamba，使能够同时捕获长程依赖关系和细粒度的局部特征。大量实验证明，我们的HybridTM在不同的室内和室外数据集上具有有效性和泛化能力。此外，我们的HybridTM在ScanNet、ScanNet200和nuScenes基准测试上取得了最先进的性能。代码将在https://github.com/deepinact/HybridTM上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D语义分割中Transformer和Mamba两种方法的局限性：Transformer虽能捕捉细粒度特征但计算复杂度高，难以处理大规模点云；Mamba虽计算效率高但特征表示能力弱。这个问题在自动驾驶、机器人导航和3D场景理解等现实应用中至关重要，因为准确的3D语义分割是这些系统理解周围环境的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了Transformer和Mamba在3D语义分割中的优缺点，发现它们具有互补性。他们注意到现有混合架构在NLP和2D视觉中有效，但直接应用于3D点云效果不佳，因为点云是稀疏和不规则的。作者借鉴了Point Transformer V3和Serialized Point Mamba等现有工作，但针对3D点云特性设计了'Inner Layer Hybrid Strategy'，在更细粒度上结合两种方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出HybridTM，第一个结合Transformer和Mamba的混合架构，以及'Inner Layer Hybrid Strategy'，同时捕捉长程依赖和细粒度特征。整体流程：1)将输入点云体素化；2)采用类似UNet的架构；3)编码器和解码器使用混合层；4)每个混合层先使用注意力层提取细粒度特征，再使用Mamba层提取全局特征，最后通过FFN融合特征；5)使用xCPE模块进行位置编码；6)通过分类头输出语义分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)HybridTM，首个结合Transformer和Mamba的3D语义分割混合架构；2)'Inner Layer Hybrid Strategy'，在细粒度上结合两种方法。不同之处：1)现有混合架构多用于NLP和2D视觉，本文专门针对3D点云；2)相比'外混合策略'，本文的'内混合策略'在更细粒度上结合注意力层和Mamba层；3)通过先使用注意力层提取高质量特征，再输入Mamba层，解决了Mamba特征表示能力弱的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出HybridTM混合架构和'Inner Layer Hybrid Strategy'，有效解决了3D语义分割中长程依赖建模和细粒度特征提取的权衡问题，在多个室内外数据集上实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformer-based methods have demonstrated remarkable capabilities in 3Dsemantic segmentation through their powerful attention mechanisms, but thequadratic complexity limits their modeling of long-range dependencies inlarge-scale point clouds. While recent Mamba-based approaches offer efficientprocessing with linear complexity, they struggle with feature representationwhen extracting 3D features. However, effectively combining these complementarystrengths remains an open challenge in this field. In this paper, we proposeHybridTM, the first hybrid architecture that integrates Transformer and Mambafor 3D semantic segmentation. In addition, we propose the Inner Layer HybridStrategy, which combines attention and Mamba at a finer granularity, enablingsimultaneous capture of long-range dependencies and fine-grained localfeatures. Extensive experiments demonstrate the effectiveness andgeneralization of our HybridTM on diverse indoor and outdoor datasets.Furthermore, our HybridTM achieves state-of-the-art performance on ScanNet,ScanNet200, and nuScenes benchmarks. The code will be made available athttps://github.com/deepinact/HybridTM.</description>
      <author>example@mail.com (Xinyu Wang, Jinghua Hou, Zhe Liu, Yingying Zhu)</author>
      <guid isPermaLink="false">2507.18575v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping</title>
      <link>http://arxiv.org/abs/2507.18541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的无位姿3D高斯飞溅重建框架，通过结合预训练的多视图立体先验和概率Procrustes映射策略，解决了现有方法在处理大量户外图像时的内存限制和精度下降问题。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅已成为3D表示的核心技术，其有效性依赖于精确的相机位姿和准确的点云初始化，这些通常来自预训练的多视图立体模型。然而，在从数百张户外图像进行无位姿重建时，现有MVS模型面临内存限制，且随着图像数量增加精度下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有MVS模型在无位姿重建任务中的内存限制和准确性下降问题，实现从无位姿图像序列中准确重建3D场景。&lt;h4&gt;方法&lt;/h4&gt;将输入图像划分为子集，将子图映射到全局空间，使用3DGS联合优化几何和位姿；将点云映射表述为概率Procrustes问题并解决闭合形式对齐，采用概率耦合和软垃圾箱机制拒绝不确定对应关系；提出3DGS和相机位姿的联合优化框架，从置信感知锚点构建高斯，集成可微分渲染与解析雅可比矩阵联合优化场景和位姿。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在数百张图像内全局对齐点云和位姿，仅需几分钟；在Waymo和KITTI数据集上的实验表明，该方法从无位姿图像序列实现了准确的重建，设定了新的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;所提出的无位姿3DGS重建框架解决了现有方法的局限性，为无位姿3D重建设定了新的最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅已成为3D表示的核心技术。其有效性很大程度上依赖于精确的相机位姿和准确的点云初始化，这些通常来自预训练的多视图立体模型。然而，在从数百张户外图像进行无位姿重建任务中，现有MVS模型可能面临内存限制，随着输入图像数量增加会失去准确性。为解决这一局限，我们提出了一种新颖的无位姿3DGS重建框架，将预训练MVS先验与概率Procrustes映射策略相结合。该方法将输入图像划分为子集，将子图映射到全局空间，并使用3DGS联合优化几何和位姿。技术上，我们将数千万个点云的映射表述为概率Procrustes问题，并解决闭合形式对齐。通过采用概率耦合和软垃圾箱机制来拒绝不确定对应关系，我们的方法能够在数百张图像内全局对齐点云和位姿，仅需几分钟。此外，我们提出了3DGS和相机位姿的联合优化框架。它从置信感知的锚点构建高斯，并将3DGS可微分渲染与解析雅可比矩阵集成，以联合优化场景和位姿，实现准确重建和位姿估计。Waymo和KITTI数据集上的实验表明，我们的方法能够从无位姿图像序列实现准确重建，为无位姿3DGS重建设定了新的最先进水平。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从大量未标定相机位置的户外图像中进行无位姿的3D高斯溅射(3DGS)重建问题。这个问题在现实和研究中很重要，因为传统3DGS方法严重依赖精确的相机位姿和准确的点云初始化，而现有的MVS模型在处理大量户外图像时面临内存限制和准确度下降的问题，这限制了3DGS在真实世界场景中的应用，特别是在户外大规模场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用'分而治之'策略，将大量图像分解为更小的子集分别处理后再合并。他们借鉴了现有的多视图立体视觉(MVS)模型(如DUSt3R、MASt3R)作为预训练基础，将子图映射重新表述为概率Procrustes问题，结合Kabsch-Umeyama算法和概率耦合机制。同时，他们利用3DGS的可微渲染特性，提出了联合优化框架，通过解析雅可比矩阵提高优化效率。这些方法融合了传统几何算法和现代深度学习技术的优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用'分而治之'策略，结合概率Procrustes映射和联合优化框架。整体流程包括：1)将输入图像划分为多个重叠子集；2)用预训练MVS模型处理每个子集获取局部点云和位姿；3)通过概率Procrustes映射将局部子图对齐到全局坐标系；4)从全局点云中提取高置信度锚点初始化3D高斯；5)通过3DGS可微渲染和解析雅可比矩阵联合优化场景和位姿，实现全局一致的重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)概率Procrustes映射方法，将数千万点云映射重新表述为概率Procrustes问题，结合闭式解和概率耦合机制；2)3DGS和位姿联合优化框架，从置信感知锚点构建高斯并共同优化；3)整体分而治之框架，高效处理户外大规模场景。相比之前工作，不同之处在于：不需要精确预计算位姿，能处理数百张图像而非仅稀疏视图，计算效率高于基于SfM的方法，解决了现有MVS方法的内存瓶颈和准确度下降问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合预训练MVS模型与概率Procrustes映射的无位姿3D高斯溅射重建框架，通过分而治之策略和联合优化方法，实现了从数百张户外图像进行高效、准确的全局3D场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a core technique for 3Drepresentation. Its effectiveness largely depends on precise camera poses andaccurate point cloud initialization, which are often derived from pretrainedMulti-View Stereo (MVS) models. However, in unposed reconstruction task fromhundreds of outdoor images, existing MVS models may struggle with memory limitsand lose accuracy as the number of input images grows. To address thislimitation, we propose a novel unposed 3DGS reconstruction framework thatintegrates pretrained MVS priors with the probabilistic Procrustes mappingstrategy. The method partitions input images into subsets, maps submaps into aglobal space, and jointly optimizes geometry and poses with 3DGS. Technically,we formulate the mapping of tens of millions of point clouds as a probabilisticProcrustes problem and solve a closed-form alignment. By employingprobabilistic coupling along with a soft dustbin mechanism to reject uncertaincorrespondences, our method globally aligns point clouds and poses withinminutes across hundreds of images. Moreover, we propose a joint optimizationframework for 3DGS and camera poses. It constructs Gaussians fromconfidence-aware anchor points and integrates 3DGS differentiable renderingwith an analytical Jacobian to jointly refine scene and poses, enablingaccurate reconstruction and pose estimation. Experiments on Waymo and KITTIdatasets show that our method achieves accurate reconstruction from unposedimage sequences, setting a new state of the art for unposed 3DGSreconstruction.</description>
      <author>example@mail.com (Chong Cheng, Zijian Wang, Sicheng Yu, Yu Hu, Nanjie Yao, Hao Wang)</author>
      <guid isPermaLink="false">2507.18541v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image</title>
      <link>http://arxiv.org/abs/2507.18371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了MVG4D，一种新颖的框架，可以通过单张静态图像生成动态4D内容。该方法结合了多视图合成与4D高斯散射技术，有效提升了时间一致性、几何保真度和视觉真实感，解决了先前4D高斯散射方法中存在的运动不连续和背景退化问题。&lt;h4&gt;背景&lt;/h4&gt;生成式建模的进步显著增强了数字内容创作，从2D图像扩展到复杂的3D和4D场景。尽管取得了实质性进展，但生成高保真和时间一致的动态4D内容仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出MVG4D框架，通过结合多视图合成与4D高斯散射技术，从单张静态图像生成动态4D内容，解决时间一致性、几何保真度和视觉真实感方面的问题。&lt;h4&gt;方法&lt;/h4&gt;MVG4D核心采用图像矩阵模块，合成时间连贯和空间多样的多视图图像，为下游3D和4D重建提供丰富的监督信号。这些多视图图像用于优化3D高斯点云，并通过轻量级变形网络进一步扩展到时间域。&lt;h4&gt;主要发现&lt;/h4&gt;在Objaverse数据集上的大量实验表明，MVG4D在CLIP-I、PSNR、FVD和时间效率方面优于最先进的基线方法。值得注意的是，它减少了闪烁伪影，并增强了跨视图和时间的结构细节，实现了更沉浸式的AR/VR体验。&lt;h4&gt;结论&lt;/h4&gt;MVG4D为从最少输入高效可控地生成4D内容开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模的进步显著增强了数字内容创作，从2D图像扩展到复杂的3D和4D场景。尽管取得了实质性进展，但生成高保真和时间一致的动态4D内容仍然是一个挑战。在本文中，我们提出了MVG4D，一种新颖的框架，通过结合多视图合成与4D高斯散射技术，从单张静态图像生成动态4D内容。其核心是，MVG4D采用图像矩阵模块，合成时间连贯和空间多样的多视图图像，为下游3D和4D重建提供丰富的监督信号。这些多视图图像用于优化3D高斯点云，并通过轻量级变形网络进一步扩展到时间域。我们的方法有效提升了时间一致性、几何保真度和视觉真实感，解决了先前基于4D高斯散射方法中影响的关键挑战，如运动不连续和背景退化。在Objaverse数据集上的大量实验表明，MVG4D在CLIP-I、PSNR、FVD和时间效率方面优于最先进的基线方法。值得注意的是，它减少了闪烁伪影，并增强了跨视图和时间的结构细节，实现了更沉浸式的AR/VR体验。MVG4D为从最少输入高效可控地生成4D内容开辟了新方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单张静态图像生成高质量且时间上连续的动态4D内容的挑战。这个问题在现实中很重要，因为随着生成式建模技术发展，对高质量动态4D内容的需求日益增长，特别是在增强现实(AR)和虚拟现实(VR)领域，这些技术需要更真实、更沉浸的体验。当前方法在保持时间一致性、几何保真度和视觉真实感方面仍有不足，限制了4D内容在实际应用中的广泛使用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前4D高斯飞溅技术在从单张图像生成4D内容时面临的挑战，包括背景点缺失、相机定位不准确以及运动过程中的表面撕裂问题。他们借鉴了现有的多视图图像生成技术(如Zero123系列)来生成多角度图像，同时采用了3D高斯飞溅技术进行高效3D表示和4D高斯飞溅技术进行动态场景渲染。在此基础上，作者创新性地设计了图像矩阵模块来生成时间连贯且空间多样的多视图图像，为后续优化提供丰富监督信号，并构建了一个集成框架将多视图生成、3D重建和4D优化顺序结合，有效解决了现有方法的关键问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合多视图合成与4D高斯飞溅技术，从单张静态图像生成动态4D内容。整体流程分为三个阶段：1)图像矩阵模块：从输入图像生成包含动态信息的视频，然后为每个视频帧生成多视角图像，形成时空连贯的图像矩阵；2)3D高斯飞溅构建：利用多视图融合技术整合不同视角信息，初始化并优化3D高斯点云；3)4D内容合成：将静态3D高斯点云通过时空结构编码器和轻量级变形网络扩展到时间域，生成动态4D内容，并通过损失函数优化确保与原始图像矩阵的一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)图像矩阵模块：生成多样且时间连贯的多视图图像，为下游重建提供丰富监督；2)紧凑的3D场景表示：直接从图像矩阵初始化和优化3D高斯点云，减少计算开销；3)集成框架：顺序结合多视图生成、3D重建和4D优化，实现高效高质量生成；4)改进的4D高斯飞溅优化：解决运动不连续和背景退化问题。相比之前工作，MVG4D仅需单张图像作为输入(而非视频)，生成速度更快(仅需8分46秒)，在CLIP-I、PSNR、FVD等指标上表现更优，细节恢复和边界处理更佳，能有效解决现有4D GS方法中的背景点缺失和相机定位不准确问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MVG4D提出了一种创新的图像矩阵模块与4D高斯飞溅相结合的框架，能够从单张静态图像高效生成高质量、时间连贯的动态4D内容，显著提升了AR/VR等应用场景的视觉体验。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in generative modeling have significantly enhanced digital contentcreation, extending from 2D images to complex 3D and 4D scenes. Despitesubstantial progress, producing high-fidelity and temporally consistent dynamic4D content remains a challenge. In this paper, we propose MVG4D, a novelframework that generates dynamic 4D content from a single still image bycombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,MVG4D employs an image matrix module that synthesizes temporally coherent andspatially diverse multi-view images, providing rich supervisory signals fordownstream 3D and 4D reconstruction. These multi-view images are used tooptimize a 3D Gaussian point cloud, which is further extended into the temporaldomain via a lightweight deformation network. Our method effectively enhancestemporal consistency, geometric fidelity, and visual realism, addressing keychallenges in motion discontinuity and background degradation that affect prior4D GS-based methods. Extensive experiments on the Objaverse dataset demonstratethat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, andtime efficiency. Notably, it reduces flickering artifacts and sharpensstructural details across views and time, enabling more immersive AR/VRexperiences. MVG4D sets a new direction for efficient and controllable 4Dgeneration from minimal inputs.</description>
      <author>example@mail.com (Xiaotian Chen, DongFu Yin, Fei Richard Yu, Xuanchen Li, Xinhao Zhang)</author>
      <guid isPermaLink="false">2507.18371v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments</title>
      <link>http://arxiv.org/abs/2507.18317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为AF-RLIO的自适应融合方法，结合4D毫米波雷达、LiDAR、IMU和GPS，利用多传感器互补优势在复杂环境中实现稳健的机器人导航&lt;h4&gt;背景&lt;/h4&gt;在复杂和动态环境中保持精确的位姿估计和导航对机器人至关重要，但烟雾、隧道和恶劣天气等环境挑战会显著降低单传感器系统性能&lt;h4&gt;目的&lt;/h4&gt;解决复杂环境中的机器人导航挑战，通过多传感器融合提高导航的稳定性和安全性&lt;h4&gt;方法&lt;/h4&gt;AF-RLIO包含三个关键模块：预处理模块(雷达辅助LiDAR移除动态点)、动态感知多模态里程计(点云扫描匹配与IMU耦合)和因子图优化模块(平衡里程计和GPS数据权重)&lt;h4&gt;主要发现&lt;/h4&gt;该方法在数据集评估和实际测试中证明，在烟雾和隧道等挑战性条件下比现有方法更有效&lt;h4&gt;结论&lt;/h4&gt;AF-RLIO通过多传感器融合有效解决了复杂环境中的机器人导航挑战，提高了在恶劣条件下的导航性能&lt;h4&gt;翻译&lt;/h4&gt;在机器人导航中，在复杂和动态环境中保持精确的位姿估计和导航至关重要。然而，烟雾、隧道和恶劣天气等环境挑战会显著降低LiDAR或GPS等单传感器系统的性能，影响自主机器人的整体稳定性和安全。为了解决这些挑战，我们提出了AF-RLIO：一种自适应融合方法，结合4D毫米波雷达、LiDAR、惯性测量单元(IMU)和GPS，利用这些传感器的互补优势，在复杂环境中实现稳健的里程计估计。我们的方法包含三个关键模块。首先，预处理模块利用雷达数据辅助LiDAR移除动态点，并确定LiDAR的环境条件何时退化。其次，动态感知多模态里程计选择适当的点云数据进行扫描到地图匹配，并使用迭代误差状态卡尔曼滤波器将其与IMU紧密耦合。最后，因子图优化模块平衡里程计和GPS数据之间的权重，构建位姿图进行优化。该方法已在数据集上进行了评估，并在实际机器人环境中进行了测试，证明了其在烟雾和隧道等挑战性条件下比现有方法更有效、更具优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In robotic navigation, maintaining precise pose estimation and navigation incomplex and dynamic environments is crucial. However, environmental challengessuch as smoke, tunnels, and adverse weather can significantly degrade theperformance of single-sensor systems like LiDAR or GPS, compromising theoverall stability and safety of autonomous robots. To address these challenges,we propose AF-RLIO: an adaptive fusion approach that integrates 4Dmillimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS toleverage the complementary strengths of these sensors for robust odometryestimation in complex environments. Our method consists of three key modules.Firstly, the pre-processing module utilizes radar data to assist LiDAR inremoving dynamic points and determining when environmental conditions aredegraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selectsappropriate point cloud data for scan-to-map matching and tightly couples itwith the IMU using the Iterative Error State Kalman Filter. Lastly, the factorgraph optimization module balances weights between odometry and GPS data,constructing a pose graph for optimization. The proposed approach has beenevaluated on datasets and tested in real-world robotic environments,demonstrating its effectiveness and advantages over existing methods inchallenging conditions such as smoke and tunnels.</description>
      <author>example@mail.com (Chenglong Qian, Yang Xu, Xiufang Shi, Jiming Chen, Liang Li)</author>
      <guid isPermaLink="false">2507.18317v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>3D Test-time Adaptation via Graph Spectral Driven Point Shift</title>
      <link>http://arxiv.org/abs/2507.18225v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为图谱域测试时适应(GSDTTA)的新方法，用于3D点云分类，通过将适应过程转移到图谱域，利用图傅里叶变换和频谱优化，实现了比现有方法更高效的域适应。&lt;h4&gt;背景&lt;/h4&gt;测试时适应(TTA)方法能够通过在线推理时动态调整预训练模型来处理域偏移问题，但将这些方法应用于3D点云受到其不规则和无序结构的阻碍。当前3D TTA方法通常依赖计算昂贵的空间域优化，且可能需要额外的训练数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的3D点云分类方法，更有效地处理域偏移问题，同时提高计算效率并减少对额外训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出图谱域测试时适应(GSDTTA)方法，将目标域点云表示为异常值感知图并通过图傅里叶变换(GFT)转换为图谱域，仅优化最低10%的频率分量以捕获大部分能量，然后应用逆GFT重建适应的点云，并通过特征映射引导的自训练策略迭代优化频谱调整和模型参数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果和消融研究表明GSDTTA是有效的，在3D点云分类任务中优于现有的TTA方法。&lt;h4&gt;结论&lt;/h4&gt;GSDTTA通过将适应过程转移到图谱域，解决了3D点云测试时适应的计算效率问题，为处理3D点云的域偏移提供了一种创新且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;虽然测试时适应(TTA)方法通过在线推理时动态调整预训练模型到目标域数据来有效处理域偏移，但它们在3D点云上的应用受到其不规则和无序结构的阻碍。当前的3D TTA方法通常依赖计算昂贵的空间域优化，并且可能需要额外的训练数据。相比之下，我们提出了图谱域测试时适应(GSDTTA)，这是一种用于3D点云分类的新方法，它将适应转移到图谱域，通过更少的参数捕获全局结构特性来实现更高效的适应。目标域中的点云被表示为异常值感知图，并通过图傅里叶变换(GFT)转换为图谱域。为提高效率，适应仅通过优化最低10%的频率分量来完成，这些分量捕获了点云的大部分能量。然后应用逆GFT(IGFT)通过图谱驱动的点移位重建适应的点云。这一过程通过特征映射引导的自训练策略得到增强，该策略迭代优化频谱调整和模型参数。基准数据集上的实验结果和消融研究证明了GSDTTA的有效性，在3D点云分类中优于现有的TTA方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云在测试时面临分布偏移的问题，即预训练模型在源域数据上表现良好，但在目标域（带有噪声、遮挡等干扰的真实数据）上性能显著下降。这个问题在现实中非常重要，因为点云数据广泛应用于自动驾驶、虚拟现实和考古等领域，而现实场景中的点云往往不可避免地存在各种噪声和干扰。论文中提到，一个在干净数据集上训练的强大模型在测试真实世界噪声数据时性能可能下降超过35%，这阻碍了这些模型的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于两个关键观察设计该方法：1) 图谱域的点云描述符能捕获底层结构和几何特性，提供高级别全局信息；2) 在图谱域调整低频组件比空间域需要少约90%的参数，减少优化复杂性。作者借鉴了现有工作如图谱分析技术用于点云处理、2D视觉领域的测试时适应方法、图神经网络在点云中的应用以及自训练策略，但创新性地将这些技术组合应用于3D点云的图谱域测试时适应，而非传统的空间域适应。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云从空间域转换到图谱域，在图谱域进行高效的测试时适应，再转换回空间域。这一方法利用了图谱域的两个关键特性：低频分量包含点云95%的能量，可用10%的频率分量控制全局信息；图拉普拉斯特征映射提供域无关的鲁棒特征。整体流程包括：1) 构建异常感知图；2) 应用图傅里叶变换转换到图谱域；3) 仅调整最低10%的低频分量；4) 通过逆图傅里叶变换转换回空间域；5) 使用特征映射引导的自训练策略生成伪标签；6) 迭代优化谱调整和模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次将3D点云测试时适应从空间域转移到图谱域；2) 设计异常感知图构建方法提高鲁棒性；3) 提出特征映射引导的自训练策略结合深度和谱特征；4) 开发迭代优化框架同时优化输入和模型。相比之前工作，本文方法不需要访问源训练数据（如CloudFixer和3DD-TTA），计算效率更高（仅需调整10%低频分量，比空间域优化少90%参数），且更适合在线或数据有限的TTA场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于图谱域的3D点云测试时适应方法，通过在低频谱空间进行高效优化和特征映射引导的自训练策略，显著提高了点云分类模型在分布偏移场景下的性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While test-time adaptation (TTA) methods effectively address domain shifts bydynamically adapting pre-trained models to target domain data during onlineinference, their application to 3D point clouds is hindered by their irregularand unordered structure. Current 3D TTA methods often rely on computationallyexpensive spatial-domain optimizations and may require additional trainingdata. In contrast, we propose Graph Spectral Domain Test-Time Adaptation(GSDTTA), a novel approach for 3D point cloud classification that shiftsadaptation to the graph spectral domain, enabling more efficient adaptation bycapturing global structural properties with fewer parameters. Point clouds intarget domain are represented as outlier-aware graphs and transformed intograph spectral domain by Graph Fourier Transform (GFT). For efficiency,adaptation is performed by optimizing only the lowest 10% of frequencycomponents, which capture the majority of the point cloud's energy. An inverseGFT (IGFT) is then applied to reconstruct the adapted point cloud with thegraph spectral-driven point shift. This process is enhanced by aneigenmap-guided self-training strategy that iteratively refines both thespectral adjustments and the model parameters. Experimental results andablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,outperforming existing TTA methods for 3D point cloud classification.</description>
      <author>example@mail.com (Xin Wei, Qin Yang, Yijie Fang, Mingrui Zhu, Nannan Wang)</author>
      <guid isPermaLink="false">2507.18225v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Comparing and Scaling fMRI Features for Brain-Behavior Prediction</title>
      <link>http://arxiv.org/abs/2507.20601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究比较了从静息态功能MRI中提取的九种特征子类型用于行为预测的效果，发现功能连接(FC)是最有效的特征，特别是在预测认知、年龄和性别方面，而图信号处理(GSP)和基于可变性的特征也显示出潜力。&lt;h4&gt;背景&lt;/h4&gt;从神经影像学模式（如磁共振成像MRI）预测行为变量，可以为精神和神经障碍开发神经影像学生物标志物。提取合适的特征是实现这一目标的关键处理步骤。&lt;h4&gt;目的&lt;/h4&gt;比较从静息态功能MRI记录中提取的九种特征子类型用于行为预测的效果，研究这些特征在不同样本量和扫描时间组合下的扩展特性。&lt;h4&gt;方法&lt;/h4&gt;研究来自人类连接组计划青年成人数据集的979名受试者，预测心理健康、认知、处理速度、物质使用以及年龄和性别的综合评分。比较了从区域功能活动测量到功能连接(FC)以及通过图信号处理(GSP)衍生的指标等九种特征子类型。&lt;h4&gt;主要发现&lt;/h4&gt;FC是预测认知、年龄和性别的最佳特征；图功率谱密度是预测认知和年龄的第二佳特征；对于性别预测，基于可变性的特征也显示出潜力；在预测性别时，低通图滤波耦合FC略优于简单FC变体；没有显著预测其他目标；扩展结果表明性能较好的特征具有更高的性能储备；获取预测研究数据时平衡样本量和扫描时间很重要。&lt;h4&gt;结论&lt;/h4&gt;结果确认FC作为行为预测的稳健特征，但也显示了GSP和基于可变性度量的潜力。讨论了未来预测研究在获取策略和样本构成方面的含义。&lt;h4&gt;翻译&lt;/h4&gt;从神经影像学模式（如磁共振成像MRI）预测行为变量，有可能为精神和神经障碍开发神经影像学生物标志物。实现这一目标的一个关键处理步骤是提取合适的特征。这些特征在预测目标兴趣方面的能力可能不同，以及这种预测如何随样本量和扫描时间而扩展。在这里，我们比较了从静息态功能MRI记录中提取的九种特征子类型用于行为预测，范围从区域功能活动测量到功能连接(FC)以及使用图信号处理(GSP)衍生的指标，这是一种提取结构感知功能特征的原则性方法。我们研究了人类连接组计划青年成人数据集中的979名受试者，预测心理健康、认知、处理速度和物质使用以及年龄和性别的综合评分。针对不同样本量和扫描时间的组合，研究了这些特征的扩展特性。FC被证明是预测认知、年龄和性别的最佳特征。图功率谱密度是预测认知和年龄的第二佳特征，而对于性别，基于可变性的特征也显示出潜力。在预测性别时，低通图滤波耦合FC略优于简单FC变体。没有显著预测其他目标。扩展结果表明性能较好的特征具有更高的性能储备。它们还表明，在获取预测研究数据时平衡样本量和扫描时间很重要。结果确认FC作为行为预测的稳健特征，但也显示了GSP和基于可变性度量的潜力。我们讨论了未来预测研究在获取策略和样本构成方面的含义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting behavioral variables from neuroimaging modalities such as magneticresonance imaging (MRI) has the potential to allow the development ofneuroimaging biomarkers of mental and neurological disorders. A crucialprocessing step to this aim is the extraction of suitable features. These candiffer in how well they predict the target of interest, and how this predictionscales with sample size and scan time. Here, we compare nine feature subtypesextracted from resting-state functional MRI recordings for behavior prediction,ranging from regional measures of functional activity to functionalconnectivity (FC) and metrics derived with graph signal processing (GSP), aprincipled approach for the extraction of structure-informed functionalfeatures. We study 979 subjects from the Human Connectome Project Young Adultdataset, predicting summary scores for mental health, cognition, processingspeed, and substance use, as well as age and sex. The scaling properties of thefeatures are investigated for different combinations of sample size and scantime. FC comes out as the best feature for predicting cognition, age, and sex.Graph power spectral density is the second best for predicting cognition andage, while for sex, variability-based features show potential as well. Whenpredicting sex, the low-pass graph filtered coupled FC slightly outperforms thesimple FC variant. None of the other targets were predicted significantly. Thescaling results point to higher performance reserves for the better-performingfeatures. They also indicate that it is important to balance sample size andscan time when acquiring data for prediction studies. The results confirm FC asa robust feature for behavior prediction, but also show the potential of GSPand variability-based measures. We discuss the implications for futureprediction studies in terms of strategies for acquisition and samplecomposition.</description>
      <author>example@mail.com (Mikkel Schöttner Sieler, Thomas A. W. Bolton, Jagruti Patel, Patric Hagmann)</author>
      <guid isPermaLink="false">2507.20601v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Information Security Based on LLM Approaches: A Review</title>
      <link>http://arxiv.org/abs/2507.18215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了大型语言模型(LLMs)在信息安全领域的应用，指出其能够应对传统保护手段难以处理的复杂多变威胁，并在恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化等方面展现出潜力。&lt;h4&gt;背景&lt;/h4&gt;信息安全面临日益严峻的挑战，传统保护手段难以应对复杂多变的威胁环境。&lt;h4&gt;目的&lt;/h4&gt;关注大语言模型在信息安全中的关键作用，系统回顾其应用进展，并探索提升安全保护性能的潜力。&lt;h4&gt;方法&lt;/h4&gt;基于神经网络和Transformer架构分析大语言模型的技术基础及其在自然语言处理任务中的优势。&lt;h4&gt;主要发现&lt;/h4&gt;引入大型语言建模有助于提高安全系统的检测准确性并降低误报率；大语言模型在恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化等方面有应用价值。&lt;h4&gt;结论&lt;/h4&gt;当前大语言模型在信息安全领域的应用已取得一定成果，但在模型透明度、可解释性和场景适应性等方面仍面临挑战，需要进一步探索模型结构优化和泛化能力提升，以实现更智能、更精准的信息安全保护系统。&lt;h4&gt;翻译&lt;/h4&gt;信息安全正面临日益严峻的挑战，传统保护手段难以应对复杂多变的威胁。近年来，作为新兴智能技术，大型语言模型(LLMs)在信息安全领域展现出广阔的应用前景。本文关注大语言模型在信息安全中的关键作用，系统回顾了其在恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化方面的应用进展，并探讨了其在提升安全保护性能方面的潜力。基于神经网络和Transformer架构，本文分析了大型语言模型的技术基础及其在自然语言处理任务中的优势。研究表明，引入大型语言建模有助于提高安全系统的检测准确性并降低误报率。最后，本文总结了当前应用成果，并指出模型在透明度、可解释性和场景适应性等方面仍面临挑战。有必要进一步探索模型结构优化和泛化能力提升，以实现更智能、更精准的信息安全保护系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Information security is facing increasingly severe challenges, andtraditional protection means are difficult to cope with complex and changingthreats. In recent years, as an emerging intelligent technology, large languagemodels (LLMs) have shown a broad application prospect in the field ofinformation security. In this paper, we focus on the key role of LLM ininformation security, systematically review its application progress inmalicious behavior prediction, network threat analysis, system vulnerabilitydetection, malicious code identification, and cryptographic algorithmoptimization, and explore its potential in enhancing security protectionperformance. Based on neural networks and Transformer architecture, this paperanalyzes the technical basis of large language models and their advantages innatural language processing tasks. It is shown that the introduction of largelanguage modeling helps to improve the detection accuracy and reduce the falsealarm rate of security systems. Finally, this paper summarizes the currentapplication results and points out that it still faces challenges in modeltransparency, interpretability, and scene adaptability, among other issues. Itis necessary to explore further the optimization of the model structure and theimprovement of the generalization ability to realize a more intelligent andaccurate information security protection system.</description>
      <author>example@mail.com (Chang Gong, Zhongwen Li, Xiaoqi Li)</author>
      <guid isPermaLink="false">2507.18215v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>DanceGraph: A Complementary Architecture for Synchronous Dancing Online</title>
      <link>http://arxiv.org/abs/2507.18052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  36th International Conference on Computer Animation and Social Agents&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DanceGraph是一种解决网络延迟问题的同步在线跳舞架构，具有实时带宽高效的特点，并提供了舞蹈动作参数化风格化的交互方法。&lt;h4&gt;背景&lt;/h4&gt;在线跳舞中存在网络身体姿势共享的延迟问题，这影响了同步性和用户体验。&lt;h4&gt;目的&lt;/h4&gt;开发一种架构来克服网络延迟，实现同步在线跳舞，并允许对舞蹈动作进行风格化处理。&lt;h4&gt;方法&lt;/h4&gt;开发了实时带宽高效的架构来最小化延迟，并减少了与音乐节奏同步所需的运动预测时间。此外，还使用了参数化风格化和在线舞蹈校正的方法。&lt;h4&gt;主要发现&lt;/h4&gt;能够有效减少网络延迟对同步在线跳舞的影响，并提供了一种交互式方法用于舞蹈动作的风格化。&lt;h4&gt;结论&lt;/h4&gt;DanceGraph架构成功解决了网络延迟问题，实现了同步在线跳舞，并提供了舞蹈动作风格化的可能性。&lt;h4&gt;翻译&lt;/h4&gt;DanceGraph是一种用于同步在线跳舞的架构，克服了网络身体姿势共享的延迟问题。我们通过开发一种实时带宽高效的架构来分解这一挑战，以最小化延迟并减少与音乐节奏同步所需的运动预测时间。此外，我们展示了一种交互式方法，使用在线舞蹈校正对节奏性舞蹈的舞蹈动作进行参数化风格化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; DanceGraph is an architecture for synchronized online dancing overcoming thelatency of networked body pose sharing. We break down this challenge bydeveloping a real-time bandwidth-efficient architecture to minimize lag andreduce the timeframe of required motion prediction for synchronization with themusic's rhythm. In addition, we show an interactive method for theparameterized stylization of dance motions for rhythmic dance using onlinedance correctives.</description>
      <author>example@mail.com (David Sinclair, Ademyemi Ademola, Babis Koniaris, Kenny Mitchell)</author>
      <guid isPermaLink="false">2507.18052v1</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>All in One: Visual-Description-Guided Unified Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2507.05211v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VDG-Uni3DSeg的新框架，通过整合预训练的视觉-语言模型和大型语言模型来增强3D点云分割性能，解决了现有方法在捕获语义和上下文信息方面的不足，实现了在语义、实例和全景分割方面的最先进结果。&lt;h4&gt;背景&lt;/h4&gt;3D点云统一分割对场景理解至关重要，但受到其稀疏结构、有限标注以及在复杂环境中区分细粒度物体类别的挑战阻碍。现有方法由于监督有限和缺乏多样化的多模态线索，往往难以捕获丰富的语义和上下文信息，导致类别和实例区分不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的框架VDG-Uni3DSeg，通过整合预训练的视觉-语言模型和大型语言模型来增强3D分割性能，解决细粒度类别和实例分离的挑战。&lt;h4&gt;方法&lt;/h4&gt;利用大型语言模型生成的文本描述和来自互联网的参考图像引入丰富的多模态线索；设计语义-视觉对比损失将点特征与多模态查询对齐；设计空间增强模块有效建模场景级关系；在闭集范式下操作，利用离线生成的多模态知识。&lt;h4&gt;主要发现&lt;/h4&gt;VDG-Uni3DSeg在语义分割、实例分割和全景分割方面取得了最先进的结果，为3D理解提供了一种可扩展且实用的解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过整合多模态知识和精心设计的模块，VDG-Uni3DSeg有效地解决了3D点云统一分割中的挑战，提高了细粒度类别和实例的分离能力，代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;三维点云的统一分割对场景理解至关重要，但其稀疏结构、有限标注以及在复杂环境中区分细粒度物体类别的挑战阻碍了这一进程。现有方法由于监督有限和缺乏多样化的多模态线索，往往难以捕获丰富的语义和上下文信息，导致类别和实例区分不佳。为应对这些挑战，我们提出了VDG-Uni3DSeg，一种新颖的框架，它整合了预训练的视觉-语言模型（如CLIP）和大型语言模型来增强三维分割。通过利用大型语言模型生成的文本描述和来自互联网的参考图像，我们的方法引入了丰富的多模态线索，促进了细粒度的类别和实例分离。我们进一步设计了语义-视觉对比损失，将点特征与多模态查询对齐，以及空间增强模块，以有效建模场景级关系。在利用离线生成的多模态知识的闭集范式下运行，VDG-Uni3DSeg在语义、实例和全景分割方面取得了最先进的结果，为三维理解提供了一种可扩展且实用的解决方案。我们的代码可在https://github.com/Hanzy1996/VDG-Uni3DSeg获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云统一分割(包括语义分割、实例分割和全景分割)的问题。点云数据具有稀疏结构、标注有限以及在复杂环境中难以区分细粒度物体类别等挑战。这个问题在现实中非常重要，因为3D点云分割是自动驾驶、机器人和AR/VR应用的基础组件，准确的3D场景理解对这些领域至关重要。相比2D图像分割，3D点云分割进展较慢，主要是因为点云数据稀疏、非结构化，且人工标注成本高，现有方法难以捕获足够的语义和上下文信息来区分复杂环境中的细粒度物体类别。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有3D分割方法在捕获细粒度语义细节方面的局限性，特别是缺乏丰富的多模态信息。他们想到利用预训练的大型语言模型(LLMs)和视觉-语言模型(如CLIP)来整合外部知识，增强3D分割能力。具体设计包括：1)使用LLM生成详细的物体类别描述；2)收集互联网参考图像提供多样化视觉表示；3)设计语义-视觉对比损失对齐点特征与多模态查询；4)开发空间增强模块捕获场景空间关系。该方法借鉴了现有工作如OneFormer3D的Transformer架构，CLIP的嵌入能力，以及3D分割中的超点表示等方法，但在多模态整合和应用方式上进行了创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多模态参考信息(文本描述和图像)来增强3D点云分割性能，通过离线生成类描述和参考图像建立语义参考，提高效率和鲁棒性。整体流程：1)接收3D点云输入并提取点特征；2)应用空间增强模块(SEM)使用稀疏注意力捕获空间关系；3)使用LLM生成每个类别的详细描述，并从互联网收集参考图像；4)通过CLIP将描述和图像嵌入为查询表示；5)使用多模态统一掩码解码器融合点特征和查询生成分割掩码；6)结合描述和图像预测生成鲁棒的语义分割结果；7)使用包含语义-视觉对比损失的组合损失函数训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)VDG-Uni3DSeg框架，整合预训练的视觉-语言模型和大型语言模型；2)利用LLM生成的文本描述和互联网参考图像提供多模态线索；3)语义-视觉对比损失(SVC Loss)对齐点特征与多模态查询；4)空间增强模块(SEM)高效建模空间依赖关系；5)采用闭集范式和离线多模态知识整合提高效率。相比之前工作，不同之处在于：不依赖实时配对的2D-3D对齐；与开放词汇表方法不同，使用预定义类知识；首次整合LLM描述和互联网图像增强3D分割；相比需要配对多视图图像的方法，使用少量未配对通用图像和文本描述；实现了更好的类区分和实例分割效果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了VDG-Uni3DSeg，一种通过整合大型语言模型生成的文本描述和互联网参考图像作为多模态指导，显著提升3D点云在语义、实例和全景分割任务性能的创新框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified segmentation of 3D point clouds is crucial for scene understanding,but is hindered by its sparse structure, limited annotations, and the challengeof distinguishing fine-grained object classes in complex environments. Existingmethods often struggle to capture rich semantic and contextual information dueto limited supervision and a lack of diverse multimodal cues, leading tosuboptimal differentiation of classes and instances. To address thesechallenges, we propose VDG-Uni3DSeg, a novel framework that integratespre-trained vision-language models (e.g., CLIP) and large language models(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textualdescriptions and reference images from the internet, our method incorporatesrich multimodal cues, facilitating fine-grained class and instance separation.We further design a Semantic-Visual Contrastive Loss to align point featureswith multimodal queries and a Spatial Enhanced Module to model scene-widerelationships efficiently. Operating within a closed-set paradigm that utilizesmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-artresults in semantic, instance, and panoptic segmentation, offering a scalableand practical solution for 3D understanding. Our code is available athttps://github.com/Hanzy1996/VDG-Uni3DSeg.</description>
      <author>example@mail.com (Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer)</author>
      <guid isPermaLink="false">2507.05211v2</guid>
      <pubDate>Tue, 29 Jul 2025 16:39:27 +0800</pubDate>
    </item>
    <item>
      <title>Input Conditioned Layer Dropping in Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2507.07954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE MLSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对边缘和物联网环境中的动态架构，用于优化基础语音模型，以适应计算资源随时间变化的需求。&lt;h4&gt;背景&lt;/h4&gt;在边缘和物联网环境中，计算资源可能随时间变化，因此需要动态架构和可适应的减少策略来优化语音模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的层丢弃（layer dropping）方法，该方法在推理过程中跳过部分层以减少计算负载，并将静态模型转换为动态模型。&lt;h4&gt;方法&lt;/h4&gt;提出输入驱动的层丢弃方法，该方法使用网络输入特征和轻量级层选择网络来确定最优的处理层组合。&lt;h4&gt;主要发现&lt;/h4&gt;在四个语音和音频公共基准测试上进行了广泛的实验，使用两种不同的预训练基础模型，证明了该方法的有效性，其性能优于随机丢弃，并与早期退出方法相当甚至更好。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在处理资源受限的环境中的语音模型优化方面是有效的，并且可以显著提高模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Curating foundation speech models for edge and IoT settings, wherecomputational resources vary over time, requires dynamic architecturesfeaturing adaptable reduction strategies. One emerging approach is layerdropping ($\mathcal{LD}$) which skips fraction of the layers of a backbonenetwork during inference to reduce the computational load. This allowstransforming static models into dynamic ones. However, existing approachesexhibit limitations either in the mode of selecting layers or by significantlymodifying the neural architecture. To this end, we propose input-driven$\mathcal{LD}$ that employs the network's input features and a lightweightlayer selecting network to determine the optimum combination of processinglayers. Extensive experimentation on 4 speech and audio public benchmarks,using two different pre-trained foundation models, demonstrates theeffectiveness of our approach, thoroughly outperforming random dropping andproducing on-par (or better) results to early exit.</description>
      <author>example@mail.com (Abdul Hannan, Daniele Falavigna, Alessio Brutti)</author>
      <guid isPermaLink="false">2507.07954v1</guid>
      <pubDate>Mon, 28 Jul 2025 14:09:56 +0800</pubDate>
    </item>
  <item>
      <title>A Unifying Scheme for Extractive Content Selection Tasks</title>
      <link>http://arxiv.org/abs/2507.16922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为指令引导的内容选择（IGCS）的统一框架，用于处理自然语言处理中的内容选择任务，并介绍了IGCSbench基准测试和大型通用合成数据集。&lt;h4&gt;背景&lt;/h4&gt;内容选择任务在自然语言处理中普遍存在，但传统上这些任务被独立研究，拥有各自的建模方法、数据集和评估指标。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一个统一的框架来处理内容选择任务，并通过引入新的基准测试和数据集来促进这一框架的应用。&lt;h4&gt;方法&lt;/h4&gt;提出IGCS框架，其中任务定义和特定实例请求被封装为语言模型的指令；创建IGCSbench基准测试和大型通用合成数据集；通过转移学习来评估这些数据集对性能的影响；解决基于LLM的内容选择建模中的通用推理时间问题。&lt;h4&gt;主要发现&lt;/h4&gt;IGCS框架能够提高内容选择任务的性能，无论是否有针对特定任务的专门训练；使用这些数据集进行转移学习可以提升模型性能；针对内容选择建模提出了一个通用的评估指标。&lt;h4&gt;结论&lt;/h4&gt;IGCS框架和提出的方法及资源对未来的内容选择模型具有实用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要：广泛的自然语言处理任务涉及从给定文本中选择相关文本片段。尽管这些内容选择任务具有共同的目标，但传统上它们被独立研究，各自拥有自己的建模方法、数据集和评估指标。在本工作中，我们提出了指令引导的内容选择（IGCS）作为这种设置的有益统一框架，其中任务定义和任何实例特定请求都被封装为语言模型的指令。为了推广这一框架，我们引入了IGCSbench，这是第一个涵盖各种内容选择任务的统一基准。此外，我们创建了一个可以用于各种内容选择任务的大型通用合成数据集，并表明使用这些数据集进行迁移学习通常可以提高性能，无论是否有针对目标任务的专门训练。最后，我们解决了在基于LLM的内容选择建模中出现的通用推理时间问题，评估了一个通用的评估指标，并总体上提出了我们资源和方法对未来内容选择模型的有用性。模型和数据集可在https://github.com/shmuelamar/igcs获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A broad range of NLP tasks involve selecting relevant text spans from givensource texts. Despite this shared objective, such \textit{content selection}tasks have traditionally been studied in isolation, each with its own modelingapproaches, datasets, and evaluation metrics. In this work, we propose\textit{instruction-guided content selection (IGCS)} as a beneficial unifiedframework for such settings, where the task definition and anyinstance-specific request are encapsulated as instructions to a language model.To promote this framework, we introduce \igcsbench{}, the first unifiedbenchmark covering diverse content selection tasks. Further, we create a largegeneric synthetic dataset that can be leveraged for diverse content selectiontasks, and show that transfer learning with these datasets often boostsperformance, whether dedicated training for the targeted task is available ornot. Finally, we address generic inference time issues that arise in LLM-basedmodeling of content selection, assess a generic evaluation metric, and overallpropose the utility of our resources and methods for future content selectionmodels. Models and datasets available at https://github.com/shmuelamar/igcs.</description>
      <author>example@mail.com (Shmuel Amar, Ori Shapira, Aviv Slobodkin, Ido Dagan)</author>
      <guid isPermaLink="false">2507.16922v1</guid>
      <pubDate>Mon, 28 Jul 2025 14:09:56 +0800</pubDate>
    </item>
    <item>
      <title>On the Interaction of Compressibility and Adversarial Robustness</title>
      <link>http://arxiv.org/abs/2507.17725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了神经网络的压缩性和鲁棒性之间的关系，分析了不同形式的压缩如何影响对抗鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现代神经网络需要同时满足多个优良特性，如对训练数据的准确拟合、对未见输入的泛化能力、参数和计算效率以及对抗干扰的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;建立一种原理性的框架来分析不同形式的压缩（如神经元层面的稀疏性和频谱压缩性）如何影响对抗鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过分析压缩如何诱导表示空间中少数高度敏感的方向，这些方向可以被对手利用来构建有效的干扰。&lt;h4&gt;主要发现&lt;/h4&gt;压缩可以导致表示空间中的敏感方向增加，从而降低鲁棒性。这些风险与压缩实现的方式无关。&lt;h4&gt;结论&lt;/h4&gt;压缩性和鲁棒性之间存在根本性的紧张关系，研究结果为设计既高效又安全的模型提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;Modern neural networks are expected to simultaneously satisfy a host of desirable properties: accurate fitting to training data, generalization to unseen inputs, parameter and computational efficiency, and robustness to adversarial perturbations. While compressibility and robustness have each been studied extensively, a unified understanding of their interaction still remains elusive. In this work, we develop a principled framework to analyze how different forms of compressibility - such as neuron-level sparsity and spectral compressibility - affect adversarial robustness. We show that these forms of compression can induce a small number of highly sensitive directions in the representation space, which adversaries can exploit to construct effective perturbations. Our analysis yields a simple yet instructive robustness bound, revealing how neuron and spectral compressibility impact L_∞ and L_2 robustness via their effects on the learned representations. Crucially, the vulnerabilities we identify arise irrespective of how compression is achieved - whether via regularization, architectural bias, or implicit learning dynamics. Through empirical evaluations across synthetic and realistic tasks, we confirm our theoretical predictions, and further demonstrate that these vulnerabilities persist under adversarial training and transfer learning, and contribute to the emergence of universal adversarial perturbations. Our findings show a fundamental tension between structured compressibility and robustness, and suggest new pathways for designing models that are both efficient and secure.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern neural networks are expected to simultaneously satisfy a host ofdesirable properties: accurate fitting to training data, generalization tounseen inputs, parameter and computational efficiency, and robustness toadversarial perturbations. While compressibility and robustness have each beenstudied extensively, a unified understanding of their interaction still remainselusive. In this work, we develop a principled framework to analyze howdifferent forms of compressibility - such as neuron-level sparsity and spectralcompressibility - affect adversarial robustness. We show that these forms ofcompression can induce a small number of highly sensitive directions in therepresentation space, which adversaries can exploit to construct effectiveperturbations. Our analysis yields a simple yet instructive robustness bound,revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$robustness via their effects on the learned representations. Crucially, thevulnerabilities we identify arise irrespective of how compression is achieved -whether via regularization, architectural bias, or implicit learning dynamics.Through empirical evaluations across synthetic and realistic tasks, we confirmour theoretical predictions, and further demonstrate that these vulnerabilitiespersist under adversarial training and transfer learning, and contribute to theemergence of universal adversarial perturbations. Our findings show afundamental tension between structured compressibility and robustness, andsuggest new pathways for designing models that are both efficient and secure.</description>
      <author>example@mail.com (Melih Barsbey, Antônio H. Ribeiro, Umut Şimşekli, Tolga Birdal)</author>
      <guid isPermaLink="false">2507.17725v1</guid>
      <pubDate>Fri, 25 Jul 2025 14:19:57 +0800</pubDate>
    </item>
  <item>
      <title>STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed Dynamic Point Clouds</title>
      <link>http://arxiv.org/abs/2507.17522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为STQE的网络，用于提升压缩动态点云的质量，通过利用空间和时间相关性来改善G-PCC压缩动态点云的视觉质量。&lt;h4&gt;背景&lt;/h4&gt;目前很少有研究关注压缩动态点云的质量提升，特别是空间时间相关性在点云帧之间的有效利用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来提升压缩动态点云的视觉质量。&lt;h4&gt;方法&lt;/h4&gt;STQE网络包括：基于重着色的运动补偿模块、通道感知的时间注意力模块、高斯引导的邻域特征聚合模块，以及基于皮尔逊相关系数的联合损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;STQE在最新的G-PCC测试模型上实现了0.855 dB、0.682 dB和0.828 dB的delta PSNR提升，以及Luma、Cb和Cr分量的BD-rate分别降低了25.2%、31.6%和32.5%。&lt;h4&gt;结论&lt;/h4&gt;STQE网络能够有效提升压缩动态点云的质量，并在多个方面取得了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种名为STQE的网络，用于提高压缩动态点云的质量。该网络利用空间和时间相关性来改善G-PCC压缩动态点云的视觉效果。通过实验，STQE在最新的G-PCC测试模型上实现了显著的性能提升，包括delta PSNR的提升和BD-rate的降低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Very few studies have addressed quality enhancement for compressed dynamicpoint clouds. In particular, the effective exploitation of spatial-temporalcorrelations between point cloud frames remains largely unexplored. Addressingthis gap, we propose a spatial-temporal attribute quality enhancement (STQE)network that exploits both spatial and temporal correlations to improve thevisual quality of G-PCC compressed dynamic point clouds. Our contributionsinclude a recoloring-based motion compensation module that remaps referenceattribute information to the current frame geometry to achieve preciseinter-frame geometric alignment, a channel-aware temporal attention module thatdynamically highlights relevant regions across bidirectional reference frames,a Gaussian-guided neighborhood feature aggregation module that efficientlycaptures spatial dependencies between geometry and color attributes, and ajoint loss function based on the Pearson correlation coefficient, designed toalleviate over-smoothing effects typical of point-wise mean squared erroroptimization. When applied to the latest G-PCC test model, STQE achievedimprovements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, withBj{\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5%for the Luma, Cb, and Cr components, respectively.</description>
      <author>example@mail.com (Tian Guo, Hui Yuan, Xiaolong Mao, Shiqi Jiang, Raouf Hamzaoui, Sam Kwong)</author>
      <guid isPermaLink="false">2507.17522v1</guid>
      <pubDate>Fri, 25 Jul 2025 14:19:57 +0800</pubDate>
    </item>
    <item>
      <title>Audio-Vision Contrastive Learning for Phonological Class Recognition</title>
      <link>http://arxiv.org/abs/2507.17682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  conference to TSD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合实时磁共振成像和语音信号的多模态深度学习框架，用于对三种关键发音维度进行分类，并取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;准确的发音-语音特征分类对理解人类语音产生和发展鲁棒的语音技术至关重要，尤其在临床环境中，有助于提高疾病诊断准确性和个性化康复。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，结合实时磁共振成像和语音信号，对发音方式、发音位置和浊音进行分类，并评估系统在不同配置下的性能。&lt;h4&gt;方法&lt;/h4&gt;采用15个语音类别进行分类，评估了四种音频/视觉配置：单模态实时磁共振成像、单模态音频信号、多模态中融合和基于对比学习的音频-视觉融合。&lt;h4&gt;主要发现&lt;/h4&gt;在USC-TIMIT数据集上的实验结果表明，基于对比学习的方法实现了最先进的性能，平均F1分数为0.81，比单模态基线提高了0.23。&lt;h4&gt;结论&lt;/h4&gt;对比表示学习对于多模态发音分析是有效的，相关代码和处理后的数据集将公开提供以支持未来的研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：准确分类发音-语音特征在理解人类语音产生和发展鲁棒的语音技术中起着至关重要的作用，尤其是在临床环境中，有针对性的音素分析和治疗可以提高疾病诊断的准确性并实现个性化的康复。在本研究中，我们提出了一种多模态深度学习框架，该框架结合了实时磁共振成像（rtMRI）和语音信号，用于对三种关键的发音维度进行分类：发音方式、发音位置和浊音。我们对由上述发音维度派生的15个语音类别进行了分类，并使用四种音频/视觉配置评估了系统：单模态rtMRI、单模态音频信号、多模态中融合和基于对比学习的音频-视觉融合。在USC-TIMIT数据集上的实验结果表明，我们的基于对比学习的方法实现了最先进的性能，平均F1分数为0.81，比单模态基线提高了0.23。这些结果证实了对比表示学习对于多模态发音分析的有效性。我们的代码和处理的语料库将公开提供在https://github.com/DaE-plz/AC_Contrastive_Phonology上，以支持未来的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate classification of articulatory-phonological features plays a vitalrole in understanding human speech production and developing robust speechtechnologies, particularly in clinical contexts where targeted phonemicanalysis and therapy can improve disease diagnosis accuracy and personalizedrehabilitation. In this work, we propose a multimodal deep learning frameworkthat combines real-time magnetic resonance imaging (rtMRI) and speech signalsto classify three key articulatory dimensions: manner of articulation, place ofarticulation, and voicing. We perform classification on 15 phonological classesderived from the aforementioned articulatory dimensions and evaluate the systemwith four audio/vision configurations: unimodal rtMRI, unimodal audio signals,multimodal middle fusion, and contrastive learning-based audio-vision fusion.Experimental results on the USC-TIMIT dataset show that our contrastivelearning-based approach achieves state-of-the-art performance, with an averageF1-score of 0.81, representing an absolute increase of 0.23 over the unimodalbaseline. The results confirm the effectiveness of contrastive representationlearning for multimodal articulatory analysis. Our code and processed datasetwill be made publicly available athttps://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.</description>
      <author>example@mail.com (Daiqi Liu, Tomás Arias-Vergara, Jana Hutter, Andreas Maier, Paula Andrea Pérez-Toro)</author>
      <guid isPermaLink="false">2507.17682v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
  <item>
      <title>From Atoms to Dynamics: Learning the Committor Without Collective Variables</title>
      <link>http://arxiv.org/abs/2507.17700v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages (including supplementary information with 13 pages), 15  figures (5 figures in the main text and 10 figures in the supplementary  information)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于几何向量感知器的图神经网络架构，可以直接从原子坐标预测承诺函数，无需手工设计集体变量（CVs）。该方法在原子级别具有可解释性，能够准确指出复杂转变中的关键原子，而不依赖先验假设。&lt;h4&gt;背景&lt;/h4&gt;传统方法需要手工设计集体变量来预测承诺函数，这限制了模型的解释性和适用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，直接从原子坐标预测承诺函数，提高模型的解释性和准确性。&lt;h4&gt;方法&lt;/h4&gt;构建基于几何向量感知器的图神经网络架构，从原子坐标预测承诺函数，无需手工设计集体变量。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多种分子系统中准确推断承诺函数，并突出了每个重原子在转变机制中的重要性。同时，它还提供了底层过程的速率常数的精确估计。&lt;h4&gt;结论&lt;/h4&gt;该方法为理解和建模复杂动力学开辟了新的途径，通过实现无CV学习和自动识别复杂分子过程的物理上有意义的反应坐标。&lt;h4&gt;翻译&lt;/h4&gt;This Brief Communication introduces a graph-neural-network architecture built on geometric vector perceptrons to predict the committor function directly from atomic coordinates, bypassing the need for hand-crafted collective variables (CVs). The method offers atom-level interpretability, pinpointing the key atomic players in complex transitions without relying on prior assumptions. Applied across diverse molecular systems, the method accurately infers the committor function and highlights the importance of each heavy atom in the transition mechanism. It also yields precise estimates of the rate constants for the underlying processes. The proposed approach opens new avenues for understanding and modeling complex dynamics, by enabling CV-free learning and automated identification of physically meaningful reaction coordinates of complex molecular processes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This Brief Communication introduces a graph-neural-network architecture builton geometric vector perceptrons to predict the committor function directly fromatomic coordinates, bypassing the need for hand-crafted collective variables(CVs). The method offers atom-level interpretability, pinpointing the keyatomic players in complex transitions without relying on prior assumptions.Applied across diverse molecular systems, the method accurately infers thecommittor function and highlights the importance of each heavy atom in thetransition mechanism. It also yields precise estimates of the rate constantsfor the underlying processes. The proposed approach opens new avenues forunderstanding and modeling complex dynamics, by enabling CV-free learning andautomated identification of physically meaningful reaction coordinates ofcomplex molecular processes.</description>
      <author>example@mail.com (Sergio Contreras Arredondo, Chenyu Tang, Radu A. Talmazan, Alberto Megías, Cheng Giuseppe Chen, Christophe Chipot)</author>
      <guid isPermaLink="false">2507.17700v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Perspective-Invariant 3D Object Detection</title>
      <link>http://arxiv.org/abs/2507.17665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025; 46 pages, 18 figures, 22 tables; Project Page at  https://pi3det.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Pi3DET，一个包含多平台LiDAR数据和3D边界框标注的基准数据集，以及一个用于跨平台适应性的新框架。&lt;h4&gt;背景&lt;/h4&gt;LiDAR-based 3D object detection在学术界和工业界受到关注，但现有数据和方法的关注点主要在车载平台上。&lt;h4&gt;目的&lt;/h4&gt;填补这一空白，促进非车载平台和跨平台3D检测的研究。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于Pi3DET的跨平台适应性框架，通过几何和特征级别的鲁棒对齐实现视角不变3D检测，并建立了一个基准来评估3D检测器在跨平台场景中的鲁棒性和稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了该方法在具有挑战性的跨平台任务中的有效性，比现有适应性方法有显著提升。&lt;h4&gt;结论&lt;/h4&gt;该工作为在多样化和复杂环境中构建可泛化和统一的3D感知系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人的兴起，基于LiDAR的3D目标检测在学术界和工业界受到了广泛关注。然而，现有的数据集和方法主要关注车载平台，而对其他自主平台的研究不足。为了填补这一空白，我们引入了Pi3DET，这是第一个包含来自多个平台（车辆、四足和无人机）的LiDAR数据和3D边界框标注的基准数据集，从而促进了非车载平台以及跨平台3D检测的研究。基于Pi3DET，我们提出了一种新的跨平台适应性框架，该框架将来自研究良好的车载平台的知识迁移到其他平台。该框架通过几何和特征级别的鲁棒对齐实现了视角不变的3D检测。此外，我们建立了一个基准来评估当前3D检测器在跨平台场景中的鲁棒性和稳健性，为开发适应性3D感知系统提供了宝贵的见解。广泛的实验验证了我们的方法在具有挑战性的跨平台任务中的有效性，与现有的适应性方法相比，取得了显著的提升。我们希望这项工作为在多样化和复杂环境中构建可泛化和统一的3D感知系统铺平道路。我们的Pi3DET数据集、跨平台基准套件和标注工具包已经公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of robotics, LiDAR-based 3D object detection has garneredsignificant attention in both academia and industry. However, existing datasetsand methods predominantly focus on vehicle-mounted platforms, leaving otherautonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,the first benchmark featuring LiDAR data and 3D bounding box annotationscollected from multiple platforms: vehicle, quadruped, and drone, therebyfacilitating research in 3D object detection for non-vehicle platforms as wellas cross-platform 3D detection. Based on Pi3DET, we propose a novelcross-platform adaptation framework that transfers knowledge from thewell-studied vehicle platform to other platforms. This framework achievesperspective-invariant 3D detection through robust alignment at both geometricand feature levels. Additionally, we establish a benchmark to evaluate theresilience and robustness of current 3D detectors in cross-platform scenarios,providing valuable insights for developing adaptive 3D perception systems.Extensive experiments validate the effectiveness of our approach on challengingcross-platform tasks, demonstrating substantial gains over existing adaptationmethods. We hope this work paves the way for generalizable and unified 3Dperception systems across diverse and complex environments. Our Pi3DET dataset,cross-platform benchmark suite, and annotation toolkit have been made publiclyavailable.</description>
      <author>example@mail.com (Ao Liang, Lingdong Kong, Dongyue Lu, Youquan Liu, Jian Fang, Huaici Zhao, Wei Tsang Ooi)</author>
      <guid isPermaLink="false">2507.17665v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Confidence Calibration in Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2507.17383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 19 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在视觉-语言-动作（VLA）基础模型中，如何通过系统性地校准置信度来提高机器人行为的可靠性。&lt;h4&gt;背景&lt;/h4&gt;高水平的任务成功和机器人能够可靠地量化成功概率是可信机器人行为的关键。&lt;h4&gt;目的&lt;/h4&gt;对VLA基础模型中的置信度校准进行系统研究，以提升机器人的可靠性和可信度。&lt;h4&gt;方法&lt;/h4&gt;通过广泛的基准测试来理解任务成功与校准误差之间的关系，引入基于贝叶斯启发式的prompt ensembles算法，并分析置信度随任务时间的变化，以及提出针对动作维度的差异校准方法。&lt;h4&gt;主要发现&lt;/h4&gt;任务性能和校准之间不存在冲突；置信度在任务进行一段时间后最为可靠；不同动作维度存在差异校准。&lt;h4&gt;结论&lt;/h4&gt;本研究旨在开发工具和概念理解，以通过可靠的置信度量化使VLA既高性能又可信。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种系统性的置信度校准方法，以提高视觉-语言-动作（VLA）基础模型的可靠性。通过基准测试发现，任务成功与校准误差之间不存在矛盾，置信度在任务进行一段时间后最为可靠，并针对不同动作维度提出了差异校准方法。研究旨在开发工具和概念理解，以实现VLA的高性能和高可信度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trustworthy robot behavior requires not only high levels of task success butalso that the robot can reliably quantify how likely it is to succeed. To thisend, we present the first systematic study of confidence calibration invision-language-action (VLA) foundation models, which map visual observationsand natural-language instructions to low-level robot motor commands. We beginwith extensive benchmarking to understand the critical relationship betweentask success and calibration error across multiple datasets and VLA variants,finding that task performance and calibration are not in tension. Next, weintroduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithmthat averages confidence across paraphrased instructions and consistentlyimproves calibration. We further analyze calibration over the task timehorizon, showing that confidence is often most reliable after making someprogress, suggesting natural points for risk-aware intervention. Finally, wereveal differential miscalibration across action dimensions and proposeaction-wise Platt scaling, a method to recalibrate each action dimensionindependently to produce better confidence estimates. Our aim in this study isto begin to develop the tools and conceptual understanding necessary to renderVLAs both highly performant and highly trustworthy via reliable uncertaintyquantification.</description>
      <author>example@mail.com (Thomas P Zollo, Richard Zemel)</author>
      <guid isPermaLink="false">2507.17383v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2507.17482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LTLZinc，一个用于生成覆盖多种不同问题的数据集的基准框架，可以用于评估神经符号和持续学习方法在时间和约束驱动维度上的表现。&lt;h4&gt;背景&lt;/h4&gt;神经符号人工智能旨在结合神经网络架构和符号方法，以人类可解释的形式表示知识。持续学习关注的是随着时间扩展其知识、提高技能同时避免忘记先前学习概念的人工智能代理。&lt;h4&gt;目的&lt;/h4&gt;提出LTLZinc框架，用于生成可以评估神经符号和持续学习方法在时间和约束驱动维度上的表现的数据集。&lt;h4&gt;方法&lt;/h4&gt;LTLZinc框架从MiniZinc约束的线性时态逻辑规格和任意图像分类数据集中生成表达式的时态推理和持续学习任务。细粒度注释允许在相同生成的数据集上进行多种神经和神经符号训练设置。&lt;h4&gt;主要发现&lt;/h4&gt;在LTLZinc生成的六个神经符号序列分类和四个类持续学习任务上的实验表明，时态学习和推理具有挑战性，并突出了当前最先进方法的局限性。&lt;h4&gt;结论&lt;/h4&gt;LTLZinc生成器和十个可用的任务被发布给神经符号和持续学习社区，希望促进统一时态学习和推理框架的研究。&lt;h4&gt;翻译&lt;/h4&gt;神经符号人工智能旨在结合神经网络架构与符号方法，以人类可解释的形式表示知识。持续学习关注的是智能体随着时间的推移扩展其知识，提高其技能，同时避免忘记先前学到的概念。大多数现有的神经符号人工智能方法仅应用于静态场景，而需要沿着时间维度进行推理的具有挑战性的设置则很少被探索。在这项工作中，我们引入了LTLZinc，这是一个基准框架，可以用于生成覆盖各种不同问题的数据集，神经符号和持续学习方法可以在此框架中沿着时间和约束驱动的维度进行评估。我们的框架从MiniZinc约束的线性时态逻辑规格和任意图像分类数据集中生成表达式的时态推理和持续学习任务。细粒度注释允许在相同生成的数据集上进行多种神经和神经符号训练设置。在LTLZinc生成的六个神经符号序列分类和四个类持续学习任务上的实验表明，时态学习和推理具有挑战性，并突出了当前最先进方法的局限性。我们发布了LTLZinc生成器和十个可用的任务，希望促进向统一时态学习和推理框架的研究发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuro-symbolic artificial intelligence aims to combine neural architectureswith symbolic approaches that can represent knowledge in a human-interpretableformalism. Continual learning concerns with agents that expand their knowledgeover time, improving their skills while avoiding to forget previously learnedconcepts. Most of the existing approaches for neuro-symbolic artificialintelligence are applied to static scenarios only, and the challenging settingwhere reasoning along the temporal dimension is necessary has been seldomexplored. In this work we introduce LTLZinc, a benchmarking framework that canbe used to generate datasets covering a variety of different problems, againstwhich neuro-symbolic and continual learning methods can be evaluated along thetemporal and constraint-driven dimensions. Our framework generates expressivetemporal reasoning and continual learning tasks from a linear temporal logicspecification over MiniZinc constraints, and arbitrary image classificationdatasets. Fine-grained annotations allow multiple neural and neuro-symbolictraining settings on the same generated datasets. Experiments on sixneuro-symbolic sequence classification and four class-continual learning tasksgenerated by LTLZinc, demonstrate the challenging nature of temporal learningand reasoning, and highlight limitations of current state-of-the-art methods.We release the LTLZinc generator and ten ready-to-use tasks to theneuro-symbolic and continual learning communities, in the hope of fosteringresearch towards unified temporal learning and reasoning frameworks.</description>
      <author>example@mail.com (Luca Salvatore Lorello, Nikolaos Manginas, Marco Lippi, Stefano Melacci)</author>
      <guid isPermaLink="false">2507.17482v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception</title>
      <link>http://arxiv.org/abs/2507.17445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IndoorBEV的新型基于掩码的鸟瞰图（BEV）方法，用于解决复杂室内3D点云中多样物体检测的挑战。&lt;h4&gt;背景&lt;/h4&gt;在复杂室内环境中，检测多样物体对机器人感知提出了重大挑战，特别是当物体形状多样、存在杂乱环境和静态与动态元素共存时，传统的边界框方法往往失效。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，提出了一种新的室内BEV方法，用于室内移动机器人的物体检测。&lt;h4&gt;方法&lt;/h4&gt;IndoorBEV方法将3D场景投影到一个2D的BEV网格中，处理自然遮挡并提供一致的俯视图，帮助区分静态障碍物和动态元素。该方法使用轴紧凑编码器和基于窗口的主干网络从BEV图中提取丰富的空间特征。然后，一个基于查询的解码器头使用学习到的物体查询在BEV空间中同时预测物体类别和实例掩码。&lt;h4&gt;主要发现&lt;/h4&gt;这种基于掩码的公式能够有效地捕捉到静态和动态物体的足迹，无论它们的形状如何，为边界框回归提供了一种鲁棒的替代方案。&lt;h4&gt;结论&lt;/h4&gt;通过在一个包含多种静态物体和动态元素（如机器人和杂项物品）的定制室内数据集上进行的实验，证明了IndoorBEV在鲁棒室内场景理解方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在复杂室内3D点云中检测多样化物体对机器人感知构成了重大挑战，尤其是在物体形状多样、杂乱以及静态和动态元素共存的情况下，传统的边界框方法效果不佳。为了解决这些限制，我们提出了室内BEV，这是一种新颖的基于掩码的鸟瞰图（BEV）方法，用于室内移动机器人。在BEV方法中，将3D场景投影到一个2D的BEV网格中，这可以自然地处理遮挡并提供一致的俯视图，有助于区分静态障碍物和动态元素。得到的2D BEV结果可以直接用于下游的机器人任务，如导航、运动预测和规划。我们的架构利用轴紧凑编码器和基于窗口的主干网络从BEV图中提取丰富的空间特征。然后，一个基于查询的解码器头使用学习到的物体查询在BEV空间中同时预测物体类别和实例掩码。这种基于掩码的公式有效地捕捉了静态和动态物体的足迹，无论它们的形状如何，为边界框回归提供了一种鲁棒的替代方案。我们在一个包含多种静态物体和动态元素（如机器人和杂项物品）的定制室内数据集上展示了IndoorBEV的有效性，展示了其在鲁棒室内场景理解方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting diverse objects within complex indoor 3D point clouds presentssignificant challenges for robotic perception, particularly with varied objectshapes, clutter, and the co-existence of static and dynamic elements wheretraditional bounding box methods falter. To address these limitations, wepropose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoormobile robots.  In a BEV method, a 3D scene is projected into a 2D BEV grid which handlesnaturally occlusions and provides a consistent top-down view aiding todistinguish static obstacles from dynamic agents. The obtained 2D BEV resultsis directly usable to downstream robotic tasks like navigation, motionprediction, and planning. Our architecture utilizes an axis compact encoder anda window-based backbone to extract rich spatial features from this BEV map. Aquery-based decoder head then employs learned object queries to concurrentlypredict object classes and instance masks in the BEV space. This mask-centricformulation effectively captures the footprint of both static and dynamicobjects regardless of their shape, offering a robust alternative to boundingbox regression. We demonstrate the effectiveness of IndoorBEV on a customindoor dataset featuring diverse object classes including static objects  and dynamic elements like robots and miscellaneous items, showcasing itspotential for robust indoor scene understanding.</description>
      <author>example@mail.com (Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund)</author>
      <guid isPermaLink="false">2507.17445v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with Sequential Mixture of Experts for Multi-View Mammography</title>
      <link>http://arxiv.org/abs/2507.17662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mammo-Mamba的新型框架，旨在解决现有乳腺癌CAD系统中多视图乳腺摄影的准确和高效解释问题。&lt;h4&gt;背景&lt;/h4&gt;尽管计算机辅助诊断（CAD）系统在乳腺癌诊断中取得了进展，但乳腺癌仍然是女性癌症相关死亡的主要原因。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一个高效的乳腺癌CAD模型，以实现多视图乳腺摄影的准确解读，从而提高早期检测的能力。&lt;h4&gt;方法&lt;/h4&gt;Mammo-Mamba结合了选择性状态空间模型（SSMs）、基于Transformer的注意力机制和专家驱动的特征优化，并通过自定义的SecMamba模块引入了顺序混合专家（SeqMoE）机制。&lt;h4&gt;主要发现&lt;/h4&gt;SecMamba模块通过内容自适应特征优化增强了MambaVision主干网络在高清乳腺影像中的表征学习能力，并在CBIS-DDSM数据集上实现了优异的分类性能。&lt;h4&gt;结论&lt;/h4&gt;Mammo-Mamba在保持计算效率的同时，实现了优于传统Transformer模型的关键性能指标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管近年来计算机辅助诊断（CAD）系统在乳腺癌诊断中取得了进展，但乳腺癌仍然是女性癌症相关死亡的主要原因。准确和高效的多视图乳腺摄影解释对于早期检测至关重要，这推动了人工智能（AI）驱动的CAD模型的热潮。虽然最先进的多视图乳腺摄影分类模型大多基于Transformer架构，但其计算复杂度随着图像块数量的增加而呈二次方增长，这突出了对更有效替代方案的需求。为了应对这一挑战，我们提出了一种名为Mammo-Mamba的新型框架，该框架将选择性状态空间模型（SSMs）、基于Transformer的注意力机制和专家驱动的特征优化整合到一个统一的架构中。Mammo-Mamba通过其定制的SecMamba块引入了顺序混合专家（SeqMoE）机制，从而扩展了MambaVision骨干网络。SecMamba是一个修改后的MambaVision块，通过启用内容自适应特征优化来增强高分辨率乳腺影像中的表征学习。这些模块集成到MambaVision的更深阶段，使模型能够通过动态专家门控逐步调整特征重点，有效缓解了传统Transformer模型的限制。在CBIS-DDSM基准数据集上评估Mammo-Mamba在所有关键指标上都实现了优越的分类性能，同时保持了计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer (BC) remains one of the leading causes of cancer-relatedmortality among women, despite recent advances in Computer-Aided Diagnosis(CAD) systems. Accurate and efficient interpretation of multi-view mammogramsis essential for early detection, driving a surge of interest in ArtificialIntelligence (AI)-powered CAD models. While state-of-the-art multi-viewmammogram classification models are largely based on Transformer architectures,their computational complexity scales quadratically with the number of imagepatches, highlighting the need for more efficient alternatives. To address thischallenge, we propose Mammo-Mamba, a novel framework that integrates SelectiveState-Space Models (SSMs), transformer-based attention, and expert-drivenfeature refinement into a unified architecture. Mammo-Mamba extends theMambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)mechanism through its customized SecMamba block. The SecMamba is a modifiedMambaVision block that enhances representation learning in high-resolutionmammographic images by enabling content-adaptive feature refinement. Theseblocks are integrated into the deeper stages of MambaVision, allowing the modelto progressively adjust feature emphasis through dynamic expert gating,effectively mitigating the limitations of traditional Transformer models.Evaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superiorclassification performance across all key metrics while maintainingcomputational efficiency.</description>
      <author>example@mail.com (Farnoush Bayatmakou, Reza Taleei, Nicole Simone, Arash Mohammadi)</author>
      <guid isPermaLink="false">2507.17662v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding</title>
      <link>http://arxiv.org/abs/2507.17533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要介绍了多模态预训练方法在3D表示学习中的有效性，提出了MMPT多模态多任务预训练框架，以增强点云理解能力。&lt;h4&gt;背景&lt;/h4&gt;现有多模态预训练框架主要依赖单一预训练任务收集多模态数据，限制了模型获取其他相关任务提供的信息，可能影响其在下游任务中的表现。&lt;h4&gt;目的&lt;/h4&gt;提出MMPT框架，旨在通过多任务预训练提高点云理解能力。&lt;h4&gt;方法&lt;/h4&gt;MMPT框架包括三个预训练任务：(i) 令牌级重建(TLR)，用于恢复掩码点令牌；(ii) 点级重建(PLR)，直接预测掩码点位置；(iii) 多模态对比学习(MCL)，结合跨模态特征对应关系，以自监督方式从3D点云和2D图像模态中构建丰富的学习信号。&lt;h4&gt;主要发现&lt;/h4&gt;MMPT框架无需3D标注即可运行，且训练的编码器可以有效地迁移到各种下游任务。&lt;h4&gt;结论&lt;/h4&gt;MMPT框架在广泛使用的基准测试中，与最先进的方法相比，在判别和生成应用中表现出良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in multi-modal pre-training methods have shown promising effectiveness in learning 3D representations by aligning multi-modal features between 3D shapes and their corresponding 2D counterparts. However, existing multi-modal pre-training frameworks primarily rely on a single pre-training task to gather multi-modal data in 3D applications. This limitation prevents the models from obtaining the abundant information provided by other relevant tasks, which can hinder their performance in downstream tasks, particularly in complex and diverse domains. In order to tackle this issue, we propose MMPT, a Multi-modal Multi-task Pre-training framework designed to enhance point cloud understanding. Specifically, three pre-training tasks are devised: (i) Token-level reconstruction (TLR) aims to recover masked point tokens, endowing the model with representative learning abilities. (ii) Point-level reconstruction (PLR) is integrated to predict the masked point positions directly, and the reconstructed point cloud can be considered as a transformed point cloud used in the subsequent task. (iii) Multi-modal contrastive learning (MCL) combines feature correspondences within and across modalities, thus assembling a rich learning signal from both 3D point cloud and 2D image modalities in a self-supervised manner. Moreover, this framework operates without requiring any 3D annotations, making it scalable for use with large datasets. The trained encoder can be effectively transferred to various downstream tasks. To demonstrate its effectiveness, we evaluated its performance compared to state-of-the-art methods in various discriminant and generative applications under widely-used benchmarks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multi-modal pre-training methods have shown promisingeffectiveness in learning 3D representations by aligning multi-modal featuresbetween 3D shapes and their corresponding 2D counterparts. However, existingmulti-modal pre-training frameworks primarily rely on a single pre-trainingtask to gather multi-modal data in 3D applications. This limitation preventsthe models from obtaining the abundant information provided by other relevanttasks, which can hinder their performance in downstream tasks, particularly incomplex and diverse domains. In order to tackle this issue, we propose MMPT, aMulti-modal Multi-task Pre-training framework designed to enhance point cloudunderstanding. Specifically, three pre-training tasks are devised: (i)Token-level reconstruction (TLR) aims to recover masked point tokens, endowingthe model with representative learning abilities. (ii) Point-levelreconstruction (PLR) is integrated to predict the masked point positionsdirectly, and the reconstructed point cloud can be considered as a transformedpoint cloud used in the subsequent task. (iii) Multi-modal contrastive learning(MCL) combines feature correspondences within and across modalities, thusassembling a rich learning signal from both 3D point cloud and 2D imagemodalities in a self-supervised manner. Moreover, this framework operateswithout requiring any 3D annotations, making it scalable for use with largedatasets. The trained encoder can be effectively transferred to variousdownstream tasks. To demonstrate its effectiveness, we evaluated itsperformance compared to state-of-the-art methods in various discriminant andgenerative applications under widely-used benchmarks.</description>
      <author>example@mail.com (Liwen Liu, Weidong Yang, Lipeng Ma, Ben Fei)</author>
      <guid isPermaLink="false">2507.17533v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Clustering-based hard negative sampling for supervised contrastive speaker verification</title>
      <link>http://arxiv.org/abs/2507.17540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to INTERSPEECH 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CHNS的基于聚类的硬负样本采样方法，用于监督对比学习下的说话人表征学习，实验结果表明其性能优于多种基准方法。&lt;h4&gt;背景&lt;/h4&gt;在说话人验证中，对比学习方法逐渐替代传统的基于分类的方法，而硬负样本的有效使用可以提高对比方法的性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的硬负样本采样方法CHNS，以提高监督对比学习在说话人验证任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;CHNS通过聚类相似说话人的嵌入，并调整批处理组成以在对比损失计算中获得最佳的硬负样本与易负样本的比例。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CHNS在VoxCeleb数据集上使用两种轻量级模型架构，与带有或不带有基于损失的硬负样本采样的基线监督对比方法，以及最先进的基于分类的方法相比，在EER和minDCF指标上分别提高了18%。&lt;h4&gt;结论&lt;/h4&gt;CHNS方法能够有效提高说话人验证任务的性能，是一个具有潜力的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In speaker verification, contrastive learning is gaining popularity as analternative to the traditionally used classification-based approaches.Contrastive methods can benefit from an effective use of hard negative pairs,which are different-class samples particularly challenging for a verificationmodel due to their similarity. In this paper, we propose CHNS - aclustering-based hard negative sampling method, dedicated for supervisedcontrastive speaker representation learning. Our approach clusters embeddingsof similar speakers, and adjusts batch composition to obtain an optimal ratioof hard and easy negatives during contrastive loss calculation. Experimentalevaluation shows that CHNS outperforms a baseline supervised contrastiveapproach with and without loss-based hard negative sampling, as well as astate-of-the-art classification-based approach to speaker verification by asmuch as 18 % relative EER and minDCF on the VoxCeleb dataset using twolightweight model architectures.</description>
      <author>example@mail.com (Piotr Masztalski, Michał Romaniuk, Jakub Żak, Mateusz Matuszewski, Konrad Kowalczyk)</author>
      <guid isPermaLink="false">2507.17540v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Towards Effective Open-set Graph Class-incremental Learning</title>
      <link>http://arxiv.org/abs/2507.17687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 33rd ACM International Conference on Multimedia (MM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的开放集图类增量学习（OGCIL）框架，旨在解决传统GCIL方法在开放集场景下的灾难性遗忘和开放集识别不足的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的GCIL方法主要基于闭集假设，限制了其在未知类别自然出现而训练数据中不存在的实际场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决开放集场景下的灾难性遗忘和开放集识别不足的问题，以使图神经网络（GNNs）能够适应不断发展的图分析任务。&lt;h4&gt;方法&lt;/h4&gt;OGCIL框架通过伪样本嵌入生成来缓解灾难性遗忘，并利用基于混合的策略生成异常值样本以检测未知类别。此外，还提出了一个新型的原型超球面分类损失函数，以实现鲁棒的开放集识别。&lt;h4&gt;主要发现&lt;/h4&gt;OGCIL框架在五个基准数据集上的实验表明，它在开放集识别方面优于现有的GCIL和开放集GNN方法。&lt;h4&gt;结论&lt;/h4&gt;OGCIL框架能够有效地解决开放集图类增量学习中的挑战，为GNNs在动态图分析任务中的应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Graph class-incremental learning (GCIL) allows graph neural networks (GNNs) to adapt to evolving graph analytical tasks by incrementally learning new class knowledge while retaining knowledge of old classes. Existing GCIL methods primarily focus on a closed-set assumption, where all test samples are presumed to belong to previously known classes. Such an assumption restricts their applicability in real-world scenarios, where unknown classes naturally emerged during inference, and are absent during training. In this paper, we explore a more challenging open-set graph class-incremental learning scenario with two intertwined challenges: catastrophic forgetting of old classes, which impairs the detection of unknown classes, and inadequate open-set recognition, which destabilizes the retention of learned knowledge. To address the above problems, a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding generation to effectively mitigate catastrophic forgetting and enable robust detection of unknown classes. To be specific, a prototypical conditional variational autoencoder is designed to synthesize node embeddings for old classes, enabling knowledge replay without storing raw graph data. To handle unknown classes, we employ a mixing-based strategy to generate out-of-distribution (OOD) samples from pseudo in-distribution and current node embeddings. A novel prototypical hypersphere classification loss is further proposed, which anchors in-distribution embeddings to their respective class prototypes, while repelling OOD embeddings away. Instead of assigning all unknown samples into one cluster, our proposed objective function explicitly models them as outliers through prototype-aware rejection regions, ensuring a robust open-set recognition. Extensive experiments on five benchmarks demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)to adapt to evolving graph analytical tasks by incrementally learning new classknowledge while retaining knowledge of old classes. Existing GCIL methodsprimarily focus on a closed-set assumption, where all test samples are presumedto belong to previously known classes. Such an assumption restricts theirapplicability in real-world scenarios, where unknown classes naturally emergeduring inference, and are absent during training. In this paper, we explore amore challenging open-set graph class-incremental learning scenario with twointertwined challenges: catastrophic forgetting of old classes, which impairsthe detection of unknown classes, and inadequate open-set recognition, whichdestabilizes the retention of learned knowledge. To address the above problems,a novel OGCIL framework is proposed, which utilizes pseudo-sample embeddinggeneration to effectively mitigate catastrophic forgetting and enable robustdetection of unknown classes. To be specific, a prototypical conditionalvariational autoencoder is designed to synthesize node embeddings for oldclasses, enabling knowledge replay without storing raw graph data. To handleunknown classes, we employ a mixing-based strategy to generateout-of-distribution (OOD) samples from pseudo in-distribution and current nodeembeddings. A novel prototypical hypersphere classification loss is furtherproposed, which anchors in-distribution embeddings to their respective classprototypes, while repelling OOD embeddings away. Instead of assigning allunknown samples into one cluster, our proposed objective function explicitlymodels them as outliers through prototype-aware rejection regions, ensuring arobust open-set recognition. Extensive experiments on five benchmarksdemonstrate the effectiveness of OGCIL over existing GCIL and open-set GNNmethods.</description>
      <author>example@mail.com (Jiazhen Chen, Zheng Ma, Sichao Fu, Mingbin Feng, Tony S. Wirjanto, Weihua Ou)</author>
      <guid isPermaLink="false">2507.17687v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>From Scan to Action: Leveraging Realistic Scans for Embodied Scene Understanding</title>
      <link>http://arxiv.org/abs/2507.17585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the OpenSUN3D Workshop, CVPR 2025. This workshop paper is  not included in the official CVPR proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种有效利用现实世界3D场景扫描及其注释的方法，并通过两个下游应用展示了其有效性。&lt;h4&gt;背景&lt;/h4&gt;现实世界3D场景扫描具有真实感，可以提高下游应用的现实世界泛化能力，但数据量、多样化的注释格式和工具兼容性问题限制了其使用。&lt;h4&gt;目的&lt;/h4&gt;提出一种统一注释集成方法，以有效地利用这些扫描和其注释。&lt;h4&gt;方法&lt;/h4&gt;使用USD进行统一注释集成，并针对特定应用提出USD的特定版本。识别利用整体现实世界扫描数据集的挑战，并提出缓解策略。&lt;h4&gt;主要发现&lt;/h4&gt;通过两个下游应用验证了方法的有效性：基于LLM的场景编辑，实现了LLM对数据的理解和适应（成功率为80%），以及机器人模拟，在策略学习中的成功率为87%。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效利用现实世界3D场景扫描，并通过实际应用验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Real-world 3D scene-level scans offer realism and can enable better real-world generalizability for downstream applications. However, challenges such as data volume, diverse annotation formats, and tool compatibility limit their use. This paper demonstrates a methodology to effectively leverage these scans and their annotations. We propose a unified annotation integration using USD, with application-specific USD flavors. We identify challenges in utilizing holistic real-world scan datasets and present mitigation strategies. The efficacy of our approach is demonstrated through two downstream applications: LLM-based scene editing, enabling effective LLM understanding and adaptation of the data (80% success), and robotic simulation, achieving an 87% success rate in policy learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world 3D scene-level scans offer realism and can enable betterreal-world generalizability for downstream applications. However, challengessuch as data volume, diverse annotation formats, and tool compatibility limittheir use. This paper demonstrates a methodology to effectively leverage thesescans and their annotations. We propose a unified annotation integration usingUSD, with application-specific USD flavors. We identify challenges in utilizingholistic real-world scan datasets and present mitigation strategies. Theefficacy of our approach is demonstrated through two downstream applications:LLM-based scene editing, enabling effective LLM understanding and adaptation ofthe data (80% success), and robotic simulation, achieving an 87% success ratein policy learning.</description>
      <author>example@mail.com (Anna-Maria Halacheva, Jan-Nico Zaech, Sombit Dey, Luc Van Gool, Danda Pani Paudel)</author>
      <guid isPermaLink="false">2507.17585v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Approach to Predicting Magnetization in Quasi-One-Dimensional Ising Systems</title>
      <link>http://arxiv.org/abs/2507.17509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图的深度学习框架，用于预测准一维伊辛自旋系统的磁性。&lt;h4&gt;背景&lt;/h4&gt;利用图神经网络（GNN）处理晶格几何，并在此基础上使用全连接层。&lt;h4&gt;目的&lt;/h4&gt;通过蒙特卡洛模拟数据训练模型，以准确重现磁化曲线的关键特征。&lt;h4&gt;方法&lt;/h4&gt;模型训练于蒙特卡洛模拟数据，并能够捕捉局部模式和全局对称性。&lt;h4&gt;主要发现&lt;/h4&gt;GNN可以直接从结构连通性推断出磁性行为，且能够高效预测磁化而不需要额外的蒙特卡洛模拟。&lt;h4&gt;结论&lt;/h4&gt;该方法为磁化预测提供了一种高效且无需额外蒙特卡洛模拟的方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于图的深度学习框架，用于预测准一维伊辛自旋系统的磁性。晶格几何被编码为图，并通过图神经网络（GNN）处理，随后接入全连接层。该模型在蒙特卡洛模拟数据上训练，并精确地再现了磁化曲线的关键特征，包括平台期、临界转变点和几何挫折效应。它捕捉了局部模式和全局对称性，表明GNN可以直接从结构连通性推断出磁性行为。所提出的方法使得在无需额外蒙特卡洛模拟的情况下高效预测磁化成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a graph-based deep learning framework for predicting the magneticproperties of quasi-one-dimensional Ising spin systems. The lattice geometry isencoded as a graph and processed by a graph neural network (GNN) followed byfully connected layers. The model is trained on Monte Carlo simulation data andaccurately reproduces key features of the magnetization curve, includingplateaus, critical transition points, and the effects of geometric frustration.It captures both local motifs and global symmetries, demonstrating that GNNscan infer magnetic behavior directly from structural connectivity. The proposedapproach enables efficient prediction of magnetization without the need foradditional Monte Carlo simulations.</description>
      <author>example@mail.com (V. Slavin, O. Kryvchikov, D. Laptev)</author>
      <guid isPermaLink="false">2507.17509v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding</title>
      <link>http://arxiv.org/abs/2507.17353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RoadBench，一个用于全面理解道路损坏的多模态基准数据集，以及基于CLIP的RoadCLIP视觉语言模型，旨在提高道路损坏检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的仅基于视觉的道路损坏数据集和模型缺乏对文本信息的丰富上下文理解，这限制了道路损坏检测的准确性。&lt;h4&gt;目的&lt;/h4&gt;提出RoadBench和RoadCLIP，以解决现有方法的局限性，提高道路损坏检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;RoadBench结合了高分辨率道路损坏图像和详细的文本描述，RoadCLIP通过集成特定领域的增强来改进CLIP模型，包括疾病感知的位置编码和注入道路条件先验的机制。此外，使用GPT驱动的数据生成管道来扩展RoadBench中的图像到文本对。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，RoadCLIP在道路损坏识别任务上达到了最先进的性能，比现有的仅视觉模型提高了19.2%。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了结合视觉和文本信息进行道路状况分析的优势，为该领域设定了新的基准，并为通过多模态学习更有效地监测基础设施铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;Accurate road damage detection is crucial for timely infrastructure maintenance and public safety, but existing vision-only datasets and models lack the rich contextual understanding that textual information can provide. To address this limitation, we introduce RoadBench, the first multimodal benchmark for comprehensive road damage understanding. This dataset pairs high resolution images of road damages with detailed textual descriptions, providing a richer context for model training. We also present RoadCLIP, a novel vision language model that builds upon CLIP by integrating domain specific enhancements. It includes a disease aware positional encoding that captures spatial patterns of road defects and a mechanism for injecting road-condition priors to refine the model's understanding of road damages. We further employ a GPT driven data generation pipeline to expand the image to text pairs in RoadBench, greatly increasing data diversity without exhaustive manual annotation. Experiments demonstrate that RoadCLIP achieves state of the art performance on road damage recognition tasks, significantly outperforming existing vision-only models by 19.2%. These results highlight the advantages of integrating visual and textual information for enhanced road condition analysis, setting new benchmarks for the field and paving the way for more effective infrastructure monitoring through multimodal learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate road damage detection is crucial for timely infrastructuremaintenance and public safety, but existing vision-only datasets and modelslack the rich contextual understanding that textual information can provide. Toaddress this limitation, we introduce RoadBench, the first multimodal benchmarkfor comprehensive road damage understanding. This dataset pairs high resolutionimages of road damages with detailed textual descriptions, providing a richercontext for model training. We also present RoadCLIP, a novel vision languagemodel that builds upon CLIP by integrating domain specific enhancements. Itincludes a disease aware positional encoding that captures spatial patterns ofroad defects and a mechanism for injecting road-condition priors to refine themodel's understanding of road damages. We further employ a GPT driven datageneration pipeline to expand the image to text pairs in RoadBench, greatlyincreasing data diversity without exhaustive manual annotation. Experimentsdemonstrate that RoadCLIP achieves state of the art performance on road damagerecognition tasks, significantly outperforming existing vision-only models by19.2%. These results highlight the advantages of integrating visual and textualinformation for enhanced road condition analysis, setting new benchmarks forthe field and paving the way for more effective infrastructure monitoringthrough multimodal learning.</description>
      <author>example@mail.com (Xi Xiao, Yunbei Zhang, Janet Wang, Lin Zhao, Yuxiang Wei, Hengjia Li, Yanshu Li, Xiao Wang, Swalpa Kumar Roy, Hao Xu, Tianyang Wang)</author>
      <guid isPermaLink="false">2507.17353v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Learning in Video and 3D Object Detection: A Survey</title>
      <link>http://arxiv.org/abs/2507.17079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review in ACM Computing Surveys&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文调查了针对视频和3D目标检测的Few-shot learning (FSL)的最新进展，旨在减少昂贵的标注数据需求。&lt;h4&gt;背景&lt;/h4&gt;FSL允许目标检测模型在仅有少量标注示例的情况下识别新的类别，这对于视频和3D对象检测尤为重要，因为相较于静态图像，对视频帧中对象进行标注更加耗时。&lt;h4&gt;目的&lt;/h4&gt;本文的目的是分析FSL在视频和3D对象检测中的最近进展，并探讨其在减少标注需求和提高实际应用中的潜力。&lt;h4&gt;方法&lt;/h4&gt;对于视频，通过跨帧传递信息，如管状候选框和时间匹配网络等技术，可以从少量示例中检测新类别，有效地利用时空结构。对于3D检测，将FSL与专用的点云网络和针对类别不平衡定制的损失函数相结合，以应对稀疏性和纹理缺乏等挑战。&lt;h4&gt;主要发现&lt;/h4&gt;FSL在视频和3D检测中都显示出潜力，能够在减少标注需求的同时，通过有效利用特征、时间和数据模态的信息，实现现实世界的应用。核心问题包括平衡泛化能力和过拟合、集成原型匹配以及处理数据模态特性。&lt;h4&gt;结论&lt;/h4&gt;FSL有望通过有效地利用特征、时间和数据模态的信息来减少标注需求，并实现视频、3D和其他现实世界应用的部署。本文通过全面调查最近的进展，展示了FSL在减少监督需求方面的潜力，并使其在视频、3D和其他实际应用中得以部署。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了Few-shot learning（FSL）在视频和3D目标检测方面的最新进展，旨在减少昂贵的标注数据需求。FSL能够使目标检测模型在仅提供少量标注示例的情况下识别新类别，这对于视频和3D对象检测尤为重要，因为标注视频帧中的对象比标注静态图像更耗时。本文旨在分析FSL在视频和3D对象检测中的最近进展，并探讨其在减少标注需求和提高实际应用潜力方面的作用。对于视频，通过跨帧传递信息，如管状候选框和时间匹配网络等技术，可以从少量示例中检测新类别，有效地利用时空结构。对于3D检测，将FSL与专用的点云网络和针对类别不平衡定制的损失函数相结合，以应对稀疏性和纹理缺乏等挑战。FSL在视频和3D检测中都显示出潜力，能够在减少标注需求的同时，通过有效利用特征、时间和数据模态的信息，实现现实世界的应用。在两个领域中的核心问题包括平衡泛化能力和过拟合、集成原型匹配以及处理数据模态特性。总的来说，FSL有望通过有效地利用特征、时间和数据模态的信息来减少标注需求，并实现视频、3D和其他现实世界应用的部署。本文通过全面调查最近的进展，展示了FSL在减少监督需求方面的潜力，并使其在视频、3D和其他实际应用中得以部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot learning (FSL) enables object detection models to recognize novelclasses given only a few annotated examples, thereby reducing expensive manualdata labeling. This survey examines recent FSL advances for video and 3D objectdetection. For video, FSL is especially valuable since annotating objectsacross frames is more laborious than for static images. By propagatinginformation across frames, techniques like tube proposals and temporal matchingnetworks can detect new classes from a couple examples, efficiently leveragingspatiotemporal structure. FSL for 3D detection from LiDAR or depth data faceschallenges like sparsity and lack of texture. Solutions integrate FSL withspecialized point cloud networks and losses tailored for class imbalance.Few-shot 3D detection enables practical autonomous driving deployment byminimizing costly 3D annotation needs. Core issues in both domains includebalancing generalization and overfitting, integrating prototype matching, andhandling data modality properties. In summary, FSL shows promise for reducingannotation requirements and enabling real-world video, 3D, and otherapplications by efficiently leveraging information across feature, temporal,and data modalities. By comprehensively surveying recent advancements, thispaper illuminates FSL's potential to minimize supervision needs and enabledeployment across video, 3D, and other real-world applications.</description>
      <author>example@mail.com (Md Meftahul Ferdaus, Kendall N. Niles, Joe Tom, Mahdi Abdelguerfi, Elias Ioup)</author>
      <guid isPermaLink="false">2507.17079v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning</title>
      <link>http://arxiv.org/abs/2507.17402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV'25. 13 pages, 6 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HLFormer的基于双曲空间学习的部分相关视频检索（PRVR）方法，用于解决只描述视频部分内容的文本查询与未剪辑视频匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;现有方法在欧几里得空间中存在几何扭曲，有时会错误地表示视频的内在层次结构，并忽略了某些层次语义，最终导致次优的时间建模。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出了一种新的方法，即利用双曲空间学习来补偿欧几里得空间在层次建模方面的不足。&lt;h4&gt;方法&lt;/h4&gt;HLFormer集成了洛伦兹注意力块和欧几里得注意力块，以混合空间编码视频嵌入，并使用平均引导自适应交互模块动态融合特征。此外，还引入了部分顺序保持损失，通过洛伦兹锥约束强制执行“文本 &lt; 视频”的层次结构。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，HLFormer在跨模态匹配方面进一步增强了部分相关性，并优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;HLFormer是一个有效的PRVR框架，能够提高视频检索的准确性。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a novel method named HLFormer, a hyperbolic space learning-based Partially Relevant Video Retrieval (PRVR) framework, to address the problem of matching untrimmed videos with text queries describing only partial content.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partially Relevant Video Retrieval (PRVR) addresses the critical challenge ofmatching untrimmed videos with text queries describing only partial content.Existing methods suffer from geometric distortion in Euclidean space thatsometimes misrepresents the intrinsic hierarchical structure of videos andoverlooks certain hierarchical semantics, ultimately leading to suboptimaltemporal modeling. To address this issue, we propose the first hyperbolicmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic spacelearning to compensate for the suboptimal hierarchical modeling capabilities ofEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Blockand Euclidean Attention Block to encode video embeddings in hybrid spaces,using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.Additionally, we introduce a Partial Order Preservation Loss to enforce "text &lt;video" hierarchy through Lorentzian cone constraints. This approach furtherenhances cross-modal matching by reinforcing partial relevance between videocontent and text queries. Extensive experiments show that HLFormer outperformsstate-of-the-art methods. Code is released athttps://github.com/lijun2005/ICCV25-HLFormer.</description>
      <author>example@mail.com (Li Jun, Wang Jinpeng, Tan Chaolei, Lian Niu, Chen Long, Zhang Min, Wang Yaowei, Xia Shu-Tao, Chen Bin)</author>
      <guid isPermaLink="false">2507.17402v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>How Should We Meta-Learn Reinforcement Learning Algorithms?</title>
      <link>http://arxiv.org/abs/2507.17668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted paper at Reinforcement Learning Conference (RLC) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过实证比较不同元学习算法，研究了元学习在强化学习（RL）中的应用，并提出了优化未来RL算法的指导原则。&lt;h4&gt;背景&lt;/h4&gt;元学习算法越来越受到欢迎，因为它可以从数据中学习算法，而不是依赖于人工设计，这有助于提高机器学习系统的性能。元学习在强化学习领域显示出特别的前景，尽管从监督或无监督学习中改编的算法对于RL来说可能不是最优的。&lt;h4&gt;目的&lt;/h4&gt;对比不同元学习算法在强化学习中的应用效果，并研究元学习算法的可解释性、样本成本和训练时间等因素。&lt;h4&gt;方法&lt;/h4&gt;本文进行了一系列实证比较，涉及多种针对RL不同部分的元学习算法，并分析了元训练和元测试性能，以及其他相关因素。&lt;h4&gt;主要发现&lt;/h4&gt;通过比较发现，不同的元学习算法在性能和效率方面存在差异，同时考虑可解释性、样本成本和训练时间等因素对算法性能有重要影响。&lt;h4&gt;结论&lt;/h4&gt;根据研究结果，提出了优化元学习新RL算法的指导原则，以确保未来学习的算法尽可能高效。&lt;h4&gt;翻译&lt;/h4&gt;Meta-learning algorithms from data, instead of relying on manual design, are growing in popularity as a paradigm for improving the performance of machine learning systems. Meta-learning shows particular promise for reinforcement learning (RL), where algorithms are often adapted from supervised or unsupervised learning despite their suboptimality for RL. However, until now there has been a severe lack of comparison between different meta-learning algorithms, such as using evolution to optimise over black-box functions or LLMs to propose code. In this paper, we carry out this empirical comparison of the different approaches when applied to a range of meta-learned algorithms which target different parts of the RL pipeline. In addition to meta-train and meta-test performance, we also investigate factors including the interpretability, sample cost and train time for each meta-learning algorithm. Based on these findings, we propose several guidelines for meta-learning new RL algorithms which will help ensure that future learned algorithms are as performant as possible.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The process of meta-learning algorithms from data, instead of relying onmanual design, is growing in popularity as a paradigm for improving theperformance of machine learning systems. Meta-learning shows particular promisefor reinforcement learning (RL), where algorithms are often adapted fromsupervised or unsupervised learning despite their suboptimality for RL.However, until now there has been a severe lack of comparison between differentmeta-learning algorithms, such as using evolution to optimise over black-boxfunctions or LLMs to propose code. In this paper, we carry out this empiricalcomparison of the different approaches when applied to a range of meta-learnedalgorithms which target different parts of the RL pipeline. In addition tometa-train and meta-test performance, we also investigate factors including theinterpretability, sample cost and train time for each meta-learning algorithm.Based on these findings, we propose several guidelines for meta-learning new RLalgorithms which will help ensure that future learned algorithms are asperformant as possible.</description>
      <author>example@mail.com (Alexander David Goldie, Zilin Wang, Jakob Nicolaus Foerster, Shimon Whiteson)</author>
      <guid isPermaLink="false">2507.17668v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Citation Recommendation using Deep Canonical Correlation Analysis</title>
      <link>http://arxiv.org/abs/2507.17603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 6 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的引用推荐算法，通过应用深度相关分析（DCCA）来改进传统的线性相关分析（CCA）方法，从而提高了推荐准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的引用推荐方法通过多视角表示学习来整合学术文献中的各种模态，但需要融合技术来捕捉互补信息并保留每种模态的独特特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的引用推荐算法，通过应用DCCA来提高推荐准确性。&lt;h4&gt;方法&lt;/h4&gt;算法基于DCCA，这是一种神经网络扩展，能够捕捉科学文章分布式文本和基于图表示之间的复杂、非线性关系。&lt;h4&gt;主要发现&lt;/h4&gt;在DBLP引用网络数据集上的实验表明，该方法优于基于CCA的现有方法，在平均平均精度@10、精度@10和召回率@10方面分别提高了超过11%、5%和7%。&lt;h4&gt;结论&lt;/h4&gt;DCCA的非线性变换比CCA的线性投影产生了更具表达力的潜在表示，从而提高了推荐的相关性和排名质量。&lt;h4&gt;翻译&lt;/h4&gt;近期在引用推荐领域的研究进展通过利用多视角表示学习来提高准确性。然而，有效地结合多个数据视图需要融合技术来捕捉互补信息并保留每种模态的独特特征。本文提出了一种新的引用推荐算法，通过应用深度相关分析（DCCA）来改进传统的线性相关分析（CCA）方法。在大型DBLP（数字文献与图书馆项目）引用网络数据集上的实验表明，该方法在平均平均精度@10、精度@10和召回率@10方面均优于现有方法，实现了超过11%、5%和7%的相对改进。这些改进反映了更相关的引用推荐和增强的排名质量，表明DCCA的非线性变换比CCA的线性投影产生了更具表达力的潜在表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in citation recommendation have improved accuracy byleveraging multi-view representation learning to integrate the variousmodalities present in scholarly documents. However, effectively combiningmultiple data views requires fusion techniques that can capture complementaryinformation while preserving the unique characteristics of each modality. Wepropose a novel citation recommendation algorithm that improves upon linearCanonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), aneural network extension capable of capturing complex, non-linear relationshipsbetween distributed textual and graph-based representations of scientificarticles. Experiments on the large-scale DBLP (Digital Bibliography &amp; LibraryProject) citation network dataset demonstrate that our approach outperformsstate-of-the-art CCA-based methods, achieving relative improvements of over 11%in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. Thesegains reflect more relevant citation recommendations and enhanced rankingquality, suggesting that DCCA's non-linear transformations yield moreexpressive latent representations than CCA's linear projections.</description>
      <author>example@mail.com (Conor McNamara, Effirul Ramlan)</author>
      <guid isPermaLink="false">2507.17603v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>When and Where Localization Fails: An Analysis of the Iterative Closest Point in Evolving Environment</title>
      <link>http://arxiv.org/abs/2507.17531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures, proceedings in European Conference on Mobile  Robots (ECMR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了动态户外环境中3D激光雷达辅助的鲁棒重定位问题，提出了一个高分辨率、短期多时相数据集，并使用ICP算法评估了扫描与地图对齐的准确性。&lt;h4&gt;背景&lt;/h4&gt;动态户外环境中的短期环境变化对自主系统来说是一个关键挑战，尽管长期定位已经得到广泛研究，但短期环境变化的研究还相对较少。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一研究空白，本文旨在提供一种评估短期定位鲁棒性的结构化数据集，以及分析扫描与地图对齐的框架。&lt;h4&gt;方法&lt;/h4&gt;收集了2025年2月至4月期间每周从自然和半城市环境中收集的高分辨率、短期多时相数据集，包括高密度点云图、360度全景图像和轨迹数据。使用点云图生成的投影激光扫描，并考虑传感器精确的遮挡建模，通过两种ICP变体（点对点和点对平面）评估对齐精度。&lt;h4&gt;主要发现&lt;/h4&gt;结果表明，点对平面ICP在特征稀疏或植被密集的区域提供了更稳定和准确的对齐。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了局部几何和环境可变性对定位成功的影响，为设计更鲁棒的机器人系统提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;Robust relocalization in dynamic outdoor environments remains a key challenge for autonomous systems relying on 3D lidar. While long-term localization has been widely studied, short-term environmental changes, occurring over days or weeks, remain underexplored despite their practical significance. To address this gap, we present a high-resolution, short-term multi-temporal dataset collected weekly from February to April 2025 across natural and semi-urban settings. Each session includes high-density point cloud maps, 360 deg panoramic images, and trajectory data. Projected lidar scans, derived from the point cloud maps and modeled with sensor-accurate occlusions, are used to evaluate alignment accuracy against the ground truth using two Iterative Closest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show that Point-to-Plane offers significantly more stable and accurate registration, particularly in areas with sparse features or dense vegetation. This study provides a structured dataset for evaluating short-term localization robustness, a reproducible framework for analyzing scan-to-map alignment under noise, and a comparative evaluation of ICP performance in evolving outdoor environments. Our analysis underscores how local geometry and environmental variability affect localization success, offering insights for designing more resilient robotic systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust relocalization in dynamic outdoor environments remains a key challengefor autonomous systems relying on 3D lidar. While long-term localization hasbeen widely studied, short-term environmental changes, occurring over days orweeks, remain underexplored despite their practical significance. To addressthis gap, we present a highresolution, short-term multi-temporal datasetcollected weekly from February to April 2025 across natural and semi-urbansettings. Each session includes high-density point cloud maps, 360 degpanoramic images, and trajectory data. Projected lidar scans, derived from thepoint cloud maps and modeled with sensor-accurate occlusions, are used toevaluate alignment accuracy against the ground truth using two IterativeClosest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results showthat Point-to-Plane offers significantly more stable and accurate registration,particularly in areas with sparse features or dense vegetation. This studyprovides a structured dataset for evaluating short-term localizationrobustness, a reproducible framework for analyzing scan-to-map alignment undernoise, and a comparative evaluation of ICP performance in evolving outdoorenvironments. Our analysis underscores how local geometry and environmentalvariability affect localization success, offering insights for designing moreresilient robotic systems.</description>
      <author>example@mail.com (Abdel-Raouf Dannaoui, Johann Laconte, Christophe Debain, Francois Pomerleau, Paul Checchin)</author>
      <guid isPermaLink="false">2507.17531v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery</title>
      <link>http://arxiv.org/abs/2507.17209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HypoChainer的协作可视化框架，旨在通过整合人类专业知识、LLM驱动的推理和知识图谱来增强假设生成和验证，以应对现代科学发现中的挑战。&lt;h4&gt;背景&lt;/h4&gt;现代科学发现面临整合大量异质知识以实现生物医学和药物开发突破的挑战。传统假设驱动研究受限于人类认知限制、生物系统复杂性和实验的高成本。&lt;h4&gt;目的&lt;/h4&gt;提出HypoChainer框架，以解决传统研究方法的局限性，提高假设生成和验证的效率和可靠性。&lt;h4&gt;方法&lt;/h4&gt;HypoChainer框架包括三个阶段：探索和情境化、假设链形成、验证优先级。使用RAGs和降维技术导航大规模GNN预测，并通过交互式解释辅助；专家迭代检查KG关系和语义相关实体，利用LLM和KG建议细化假设；基于KG支持的证据过滤假设，识别高优先级实验候选，并通过可视化分析加强推理中的薄弱环节。&lt;h4&gt;主要发现&lt;/h4&gt;通过案例研究和专家访谈，证明了HypoChainer在两个领域的有效性，突出了其在支持可解释、可扩展和基于知识的科学发现方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;HypoChainer框架能够有效支持科学发现，通过结合人类专业知识、LLM和知识图谱，提高假设生成和验证的效率和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern scientific discovery faces growing challenges in integrating vast andheterogeneous knowledge critical to breakthroughs in biomedicine and drugdevelopment. Traditional hypothesis-driven research, though effective, isconstrained by human cognitive limits, the complexity of biological systems,and the high cost of trial-and-error experimentation. Deep learning models,especially graph neural networks (GNNs), have accelerated predictiongeneration, but the sheer volume of outputs makes manual selection forvalidation unscalable. Large language models (LLMs) offer promise in filteringand hypothesis generation, yet suffer from hallucinations and lack grounding instructured knowledge, limiting their reliability. To address these issues, wepropose HypoChainer, a collaborative visualization framework that integrateshuman expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhancehypothesis generation and validation. HypoChainer operates in three stages:First, exploration and contextualization -- experts use retrieval-augmentedLLMs (RAGs) and dimensionality reduction to navigate large-scale GNNpredictions, assisted by interactive explanations. Second, hypothesis chainformation -- experts iteratively examine KG relationships around predictionsand semantically linked entities, refining hypotheses with LLM and KGsuggestions. Third, validation prioritization -- refined hypotheses arefiltered based on KG-supported evidence to identify high-priority candidatesfor experimentation, with visual analytics further strengthening weak links inreasoning. We demonstrate HypoChainer's effectiveness through case studies intwo domains and expert interviews, highlighting its potential to supportinterpretable, scalable, and knowledge-grounded scientific discovery.</description>
      <author>example@mail.com (Haoran Jiang, Shaohan Shi, Yunjie Yao, Chang Jiang, Quan Li)</author>
      <guid isPermaLink="false">2507.17209v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits</title>
      <link>http://arxiv.org/abs/2507.17327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为CartoonAlive的创新方法，用于从单个输入肖像图像生成高质量的Live2D数字人类。&lt;h4&gt;背景&lt;/h4&gt;随着大型基础模型、AIGC、云渲染和实时动作捕捉技术的快速发展，数字人类能够实现同步的面部表情和身体动作，进行智能对话，并快速创建个性化头像。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出一种高效且富有表现力的解决方案，用于创建交互式2D卡通角色。&lt;h4&gt;方法&lt;/h4&gt;CartoonAlive利用3D面建模中常用的形状基概念来构建适合Live2D的面部混合形状，并基于从输入图像中检测到的面部关键点来推断相应的混合形状权重。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法可以在不到半分钟内快速生成一个高度表达和视觉准确的Live2D模型，该模型与输入肖像非常相似。&lt;h4&gt;结论&lt;/h4&gt;CartoonAlive为创建交互式2D卡通角色提供了一种实用且可扩展的解决方案，为数字内容创作和虚拟角色动画开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了CartoonAlive，一种从单个输入肖像图像生成高质量Live2D数字人类的新方法。该方法结合了3D面建模的形状基概念，通过快速生成与输入肖像高度相似的Live2D模型，为创建交互式2D卡通角色提供了实用且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of large foundation models, AIGC, cloud rendering,and real-time motion capture technologies, digital humans are now capable ofachieving synchronized facial expressions and body movements, engaging inintelligent dialogues driven by natural language, and enabling the fastcreation of personalized avatars. While current mainstream approaches todigital humans primarily focus on 3D models and 2D video-based representations,interactive 2D cartoon-style digital humans have received relatively lessattention. Compared to 3D digital humans that require complex modeling and highrendering costs, and 2D video-based solutions that lack flexibility andreal-time interactivity, 2D cartoon-style Live2D models offer a more efficientand expressive alternative. By simulating 3D-like motion through layeredsegmentation without the need for traditional 3D modeling, Live2D enablesdynamic and real-time manipulation. In this technical report, we presentCartoonAlive, an innovative method for generating high-quality Live2D digitalhumans from a single input portrait image. CartoonAlive leverages the shapebasis concept commonly used in 3D face modeling to construct facial blendshapessuitable for Live2D. It then infers the corresponding blendshape weights basedon facial keypoints detected from the input image. This approach allows for therapid generation of a highly expressive and visually accurate Live2D model thatclosely resembles the input portrait, within less than half a minute. Our workprovides a practical and scalable solution for creating interactive 2D cartooncharacters, opening new possibilities in digital content creation and virtualcharacter animation. The project homepage ishttps://human3daigc.github.io/CartoonAlive_webpage/.</description>
      <author>example@mail.com (Chao He, Jianqiang Ren, Jianjing Xiang, Xiejie Shen)</author>
      <guid isPermaLink="false">2507.17327v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>IONext: Unlocking the Next Era of Inertial Odometry</title>
      <link>http://arxiv.org/abs/2507.17089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于CNN的惯性里程计模块，通过结合大核卷积和Transformer启发式设计，提高了全局运动感知能力，并通过实验证明了其在多个数据集上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型在惯性里程计中应用广泛，但其在局部运动变化敏感度和归纳偏置方面的不足限制了定位精度和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的CNN模块，旨在提高惯性里程计的定位精度和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;设计了一种名为DADM的模块，能够自适应地捕捉全局和局部运动特征；引入了STGU单元，用于在时间域中提取代表性的运动特征；构建了基于DADM和STGU的CNN惯性里程计模型IONext。&lt;h4&gt;主要发现&lt;/h4&gt;IONext在六个公开数据集上优于现有的基于Transformer和CNN的方法，例如在RNIN数据集上平均ATE降低了10%，平均RTE降低了12%。&lt;h4&gt;结论&lt;/h4&gt;DADM和STGU模块有效地提高了惯性里程计的性能，IONext模型在多个数据集上实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;摘要：研究人员越来越多地采用基于Transformer的模型进行惯性里程计。虽然Transformer在建模长距离依赖关系方面表现出色，但它们对局部、精细的运动变化的敏感性有限，以及缺乏固有的归纳偏置，通常阻碍了定位精度和泛化能力。最近的研究表明，将大核卷积和Transformer启发的架构设计纳入CNN中可以有效地扩展感受野，从而提高全局运动感知。受这些见解的启发，我们提出了一种新的基于CNN的模块，称为双翼自适应动态混合器（DADM），该模块能够自适应地从动态输入中捕捉全局运动模式和局部、精细的运动特征。该模块根据输入动态生成选择权重，从而实现高效的多尺度特征聚合。为了进一步提高时间建模，我们引入了时空门控单元（STGU），该单元在时间域中选择性地提取代表性的和与任务相关的运动特征。该单元解决了现有CNN方法中观察到的时间建模的局限性。基于DADM和STGU，我们提出了一种新的基于CNN的惯性里程计主干网络，称为下一代惯性里程计（IONext）。在六个公开数据集上的大量实验表明，IONext在性能上始终优于最先进的（SOTA）基于Transformer和CNN的方法。例如，在RNIN数据集上，与代表性模型iMOT相比，IONext将平均ATE降低了10%，平均RTE降低了12%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Researchers have increasingly adopted Transformer-based models for inertialodometry. While Transformers excel at modeling long-range dependencies, theirlimited sensitivity to local, fine-grained motion variations and lack ofinherent inductive biases often hinder localization accuracy andgeneralization. Recent studies have shown that incorporating large-kernelconvolutions and Transformer-inspired architectural designs into CNN caneffectively expand the receptive field, thereby improving global motionperception. Motivated by these insights, we propose a novel CNN-based modulecalled the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively capturesboth global motion patterns and local, fine-grained motion features fromdynamic inputs. This module dynamically generates selective weights based onthe input, enabling efficient multi-scale feature aggregation. To furtherimprove temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),which selectively extracts representative and task-relevant motion features inthe temporal domain. This unit addresses the limitations of temporal modelingobserved in existing CNN approaches. Built upon DADM and STGU, we present a newCNN-based inertial odometry backbone, named Next Era of Inertial Odometry(IONext). Extensive experiments on six public datasets demonstrate that IONextconsistently outperforms state-of-the-art (SOTA) Transformer- and CNN-basedmethods. For instance, on the RNIN dataset, IONext reduces the average ATE by10% and the average RTE by 12% compared to the representative model iMOT.</description>
      <author>example@mail.com (Shanshan Zhang, Siyue Wang, Tianshui Wen, Qi Zhang, Ziheng Zhou, Lingxiang Zheng, Yu Yang)</author>
      <guid isPermaLink="false">2507.17089v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>The Early Bird Identifies the Worm: You Can't Beat a Head Start in Long-Term Body Re-ID (ECHO-BID)</title>
      <link>http://arxiv.org/abs/2507.17640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ECHO-BID的长时程重识别模型，用于在非约束环境中进行人员识别。该模型在长时程重识别任务上取得了最先进的成果，特别是在处理服装变化数据时表现突出。&lt;h4&gt;背景&lt;/h4&gt;在非约束环境中进行人员识别面临距离、视角、成像条件和服装变化等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出ECHO-BID模型，旨在解决非约束环境中的人员识别问题。&lt;h4&gt;方法&lt;/h4&gt;ECHO-BID基于预训练的EVA-02大型骨干网络，通过迁移学习与9个其他模型进行了比较。模型在约束、非约束和遮挡场景的基准数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;ECHO-BID在长时程重识别任务上取得了最先进的成果，特别是在处理服装变化数据时。模型大小和预训练期间的掩码图像建模是ECHO-BID性能强大的原因。选择正确的预训练骨干架构和迁移学习协议可以显著提高长时程重识别的性能。&lt;h4&gt;结论&lt;/h4&gt;ECHO-BID模型在非约束环境中的人员识别任务中表现优异，特别是在处理服装变化和遮挡场景时，为长时程重识别领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在非约束视图中进行人员识别由于距离、视角、成像条件和服装的变化而面临重大挑战。我们引入了Eva Clothes-Change from Hidden Objects - Body Identification（ECHO-BID），这是一类基于对象预训练的EVA-02大型骨干网络构建的长时程重识别模型。我们将ECHO-BID与9个其他模型进行了比较，这些模型在骨干架构、模型大小、对象分类预训练的规模和迁移学习协议方面存在系统性的差异。这些模型在约束、非约束和遮挡设置下的基准数据集上进行了评估。ECHO-BID在最具挑战性的服装变化数据上实现了迁移学习，在长时程重识别上取得了最先进的成果——显著优于其他方法。ECHO-BID在遮挡视图场景中也以很大的优势超过了其他方法。模型大小增加和预训练期间的掩码图像建模是ECHO-BID在长时程重识别上表现强大的基础。值得注意的是，一个较小但更具挑战性的迁移学习数据集比一个更大但更具挑战性的数据集在数据集之间推广得更好。然而，包含额外微调步骤的较大数据集在最具挑战性的数据上表现最佳。选择正确的预训练骨干架构和迁移学习协议可以在长时程重识别性能上带来实质性的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person identification in unconstrained viewing environments presentssignificant challenges due to variations in distance, viewpoint, imagingconditions, and clothing. We introduce $\textbf{E}$va $\textbf{C}$lothes-Changefrom $\textbf{H}$idden $\textbf{O}$bjects - $\textbf{B}$ody$\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models builton object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 othermodels that vary systematically in backbone architecture, model size, scale ofobject classification pretraining, and transfer learning protocol. Models wereevaluated on benchmark datasets across constrained, unconstrained, and occludedsettings. ECHO-BID, with transfer learning on the most challengingclothes-change data, achieved state-of-the-art results on long-term re-id --substantially outperforming other methods. ECHO-BID also surpassed othermethods by a wide margin in occluded viewing scenarios. A combination ofincreased model size and Masked Image Modeling during pretraining underlieECHO-BID's strong performance on long-term re-id. Notably, a smaller, but morechallenging transfer learning dataset, generalized better across datasets thana larger, less challenging one. However, the larger dataset with an additionalfine-tuning step proved best on the most difficult data. Selecting the correctpretrained backbone architecture and transfer learning protocols can drivesubstantial gains in long-term re-id performance.</description>
      <author>example@mail.com (Thomas M. Metz, Matthew Q. Hill, Alice J. O'Toole)</author>
      <guid isPermaLink="false">2507.17640v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2507.17050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVAM Workshop at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoNarrator，一种新型的无需训练的流水线设计，用于生成密集的视频字幕，这些字幕提供了视频内容的结构化快照。这些字幕包含详细的叙述和精确的时间戳，捕捉视频每个片段的细微差别。&lt;h4&gt;背景&lt;/h4&gt;尽管在视频理解的多模态大型语言模型（MLLMs）方面取得了进展，但这些模型在时间对齐的叙述方面往往存在困难，并且容易在陌生场景中产生幻觉。&lt;h4&gt;目的&lt;/h4&gt;VideoNarrator通过利用灵活的流水线来解决这些挑战，其中现成的MLLMs和视觉语言模型（VLMs）可以作为字幕生成器、上下文提供者或字幕验证器。&lt;h4&gt;方法&lt;/h4&gt;实验结果表明，这些组件的协同作用显著提高了视频叙述的质量和准确性，有效减少了幻觉并改善了时间对齐。&lt;h4&gt;主要发现&lt;/h4&gt;这种结构化方法不仅增强了视频理解，还促进了视频摘要和视频问答等下游任务，并且可以潜在地扩展到广告和营销应用。&lt;h4&gt;结论&lt;/h4&gt;VideoNarrator通过其创新的流水线设计，为视频字幕生成提供了一种高效且准确的方法，有助于提升视频内容的理解和应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce VideoNarrator, a novel training-free pipelinedesigned to generate dense video captions that offer a structured snapshot ofvideo content. These captions offer detailed narrations with precisetimestamps, capturing the nuances present in each segment of the video. Despiteadvancements in multimodal large language models (MLLMs) for videocomprehension, these models often struggle with temporally aligned narrationsand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarratoraddresses these challenges by leveraging a flexible pipeline whereoff-the-shelf MLLMs and visual-language models (VLMs) can function as captiongenerators, context providers, or caption verifiers. Our experimental resultsdemonstrate that the synergistic interaction of these components significantlyenhances the quality and accuracy of video narrations, effectively reducinghallucinations and improving temporal alignment. This structured approach notonly enhances video understanding but also facilitates downstream tasks such asvideo summarization and video question answering, and can be potentiallyextended for advertising and marketing applications.</description>
      <author>example@mail.com (Tz-Ying Wu, Tahani Trigui, Sharath Nittur Sridhar, Anand Bodas, Subarna Tripathi)</author>
      <guid isPermaLink="false">2507.17050v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Exposure Correction</title>
      <link>http://arxiv.org/abs/2507.17252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种创新的非监督曝光校正（UEC）方法，解决了当前曝光校正方法的三个挑战：劳动密集型配对数据标注、泛化能力有限以及在低级计算机视觉任务中的性能下降。&lt;h4&gt;背景&lt;/h4&gt;当前曝光校正方法存在三个挑战：劳动密集型的配对数据标注、泛化能力有限以及在低级计算机视觉任务中的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要手动标注、提高泛化能力并增强低级下游任务性能的非监督曝光校正方法。&lt;h4&gt;方法&lt;/h4&gt;模型使用来自模拟图像信号处理（ISP）管道的免费配对数据进行训练。此外，还开发了一个大规模的辐射度校正数据集，特别强调曝光变化，以促进无监督学习。还开发了一个转换函数，它保留了图像细节，并优于最先进的监督方法，同时仅使用其0.01%的参数。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在边缘检测等下游任务中进一步研究了曝光校正的更广泛影响，表明其能够减轻曝光不良对低级特征的不利影响。&lt;h4&gt;结论&lt;/h4&gt;本文提出的UEC方法有效解决了现有曝光校正方法的挑战，并提供了公开的源代码和数据集。&lt;h4&gt;翻译&lt;/h4&gt;Current exposure correction methods have three challenges, labor-intensive paired data annotation, limited generalizability, and performance degradation in low-level computer vision tasks. In this work, we introduce an innovative Unsupervised Exposure Correction (UEC) method that eliminates the need for manual annotations, offers improved generalizability, and enhances performance in low-level downstream tasks. Our model is trained using freely available paired data from an emulated Image Signal Processing (ISP) pipeline. This approach does not need expensive manual annotations, thereby minimizing individual style biases from the annotation and consequently improving its generalizability. Furthermore, we present a large-scale Radiometry Correction Dataset, specifically designed to emphasize exposure variations, to facilitate unsupervised learning. In addition, we develop a transformation function that preserves image details and outperforms state-of-the-art supervised methods [12], while utilizing only 0.01% of their parameters. Our work further investigates the broader impact of exposure correction on downstream tasks, including edge detection, demonstrating its effectiveness in mitigating the adverse effects of poor exposure on low-level features. The source code and dataset are publicly available at https://github.com/BeyondHeaven/uec_code.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current exposure correction methods have three challenges, labor-intensivepaired data annotation, limited generalizability, and performance degradationin low-level computer vision tasks. In this work, we introduce an innovativeUnsupervised Exposure Correction (UEC) method that eliminates the need formanual annotations, offers improved generalizability, and enhances performancein low-level downstream tasks. Our model is trained using freely availablepaired data from an emulated Image Signal Processing (ISP) pipeline. Thisapproach does not need expensive manual annotations, thereby minimizingindividual style biases from the annotation and consequently improving itsgeneralizability. Furthermore, we present a large-scale Radiometry CorrectionDataset, specifically designed to emphasize exposure variations, to facilitateunsupervised learning. In addition, we develop a transformation function thatpreserves image details and outperforms state-of-the-art supervised methods[12], while utilizing only 0.01% of their parameters. Our work furtherinvestigates the broader impact of exposure correction on downstream tasks,including edge detection, demonstrating its effectiveness in mitigating theadverse effects of poor exposure on low-level features. The source code anddataset are publicly available at https://github.com/BeyondHeaven/uec_code.</description>
      <author>example@mail.com (Ruodai Cui, Li Niu, Guosheng Hu)</author>
      <guid isPermaLink="false">2507.17252v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>DFDNet: Dynamic Frequency-Guided De-Flare Network</title>
      <link>http://arxiv.org/abs/2507.17489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的动态频率引导去噪网络（DFDNet），用于去除夜间摄影中强光源产生的光晕，提高图像质量和下游任务性能。&lt;h4&gt;背景&lt;/h4&gt;夜间摄影中强光源常导致图像出现光晕，严重影响视觉效果和后续处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法来去除大范围光晕伪影和修复光源附近的结构损伤。&lt;h4&gt;方法&lt;/h4&gt;DFDNet主要由全局动态频率域引导模块（GDFG）和局部细节引导模块（LDGM）组成。GDFG模块通过动态优化全局频率域特征，使网络能够感知光晕伪影的频率特性，从而有效分离光晕信息和内容信息。LDGM模块通过对比学习策略，使光源的局部特征与参考图像对齐，减少光晕去除过程中的局部细节损伤，并提高图像的精细修复。&lt;h4&gt;主要发现&lt;/h4&gt;DFDNet在频率域中比在空间域中更能显著地识别出光晕伪影与参考图像的差异。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，DFDNet在性能上优于现有的最先进方法。&lt;h4&gt;翻译&lt;/h4&gt;This paper presents a novel dynamic frequency-guided deflare network (DFDNet) for removing the flares produced by strong light sources in nighttime photography, improving image quality and the performance of downstream tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Strong light sources in nighttime photography frequently produce flares inimages, significantly degrading visual quality and impacting the performance ofdownstream tasks. While some progress has been made, existing methods continueto struggle with removing large-scale flare artifacts and repairing structuraldamage in regions near the light source. We observe that these challengingflare artifacts exhibit more significant discrepancies from the referenceimages in the frequency domain compared to the spatial domain. Therefore, thispaper presents a novel dynamic frequency-guided deflare network (DFDNet) thatdecouples content information from flare artifacts in the frequency domain,effectively removing large-scale flare artifacts. Specifically, DFDNet consistsmainly of a global dynamic frequency-domain guidance (GDFG) module and a localdetail guidance module (LDGM). The GDFG module guides the network to perceivethe frequency characteristics of flare artifacts by dynamically optimizingglobal frequency domain features, effectively separating flare information fromcontent information. Additionally, we design an LDGM via a contrastive learningstrategy that aligns the local features of the light source with the referenceimage, reduces local detail damage from flare removal, and improvesfine-grained image restoration. The experimental results demonstrate that theproposed method outperforms existing state-of-the-art methods in terms ofperformance. The code is available at\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.</description>
      <author>example@mail.com (Minglong Xue, Aoxiang Ning, Shivakumara Palaiahnakote, Mingliang Zhou)</author>
      <guid isPermaLink="false">2507.17489v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Model Compression Engine for Wearable Devices Skin Cancer Diagnosis</title>
      <link>http://arxiv.org/abs/2507.17125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于AI的诊断工具，用于皮肤癌的早期检测，特别是在资源有限的地区，该工具通过优化模型在嵌入式系统上的性能和能耗来解决这个问题。&lt;h4&gt;背景&lt;/h4&gt;皮肤癌是一种常见且可预防的癌症，但早期检测仍然是一个挑战，特别是在那些难以获得专业医疗保健的资源有限地区。&lt;h4&gt;目的&lt;/h4&gt;开发一个AI驱动的诊断工具，针对嵌入式系统进行优化，以解决皮肤癌早期检测的难题。&lt;h4&gt;方法&lt;/h4&gt;使用MobileNetV2架构和迁移学习进行模型训练，对皮肤病变进行二分类（皮肤癌和其他）。利用TensorRT框架对模型进行压缩和优化，以便在NVIDIA Jetson Orin Nano上部署。通过多个基准进行综合评估，包括模型大小、推理速度、吞吐量和功耗。&lt;h4&gt;主要发现&lt;/h4&gt;优化后的模型在性能上保持稳定，F1分数达到87.18%，精确度为93.18%，召回率为81.91%。模型压缩后，大小减少了0.41，推理速度和吞吐量有所提高，INT8精度下的能耗降低了0.93。&lt;h4&gt;结论&lt;/h4&gt;该研究验证了在资源受限的边缘设备上部署高性能、节能的诊断工具的可行性。此外，研究中的方法在其他医疗诊断和需要可访问、高效AI解决方案的领域有更广泛的应用。这表明优化后的AI系统有可能革新医疗诊断，从而弥合先进技术与未受服务地区之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;摘要：皮肤癌是最常见且可预防的癌症类型之一，但其早期检测仍然是一个挑战，特别是在资源有限的环境中，获取专业医疗保健的机会很少。本研究提出了一种针对嵌入式系统优化的AI驱动诊断工具，以解决这一差距。使用MobileNetV2架构和迁移学习，该模型被调整为对皮肤病变进行二分类，即“皮肤癌”和“其他”。TensorRT框架被用于压缩和优化模型，以便在NVIDIA Jetson Orin Nano上部署，在性能和能耗之间取得平衡。在多个基准上进行了综合评估，包括模型大小、推理速度、吞吐量和功耗。优化后的模型保持了其性能，实现了87.18%的F1分数，精确度为93.18%，召回率为81.91%。压缩后的结果表明，模型大小减少了最多0.41，推理速度和吞吐量有所提高，INT8精度下的能耗降低了最多0.93。这些发现验证了在资源受限的边缘设备上部署高性能、节能的诊断工具的可行性。除了皮肤癌检测之外，本研究中应用的方法在其他医疗诊断和需要可访问、高效AI解决方案的领域有更广泛的应用。这项研究强调了优化AI系统革新医疗诊断的潜力，从而弥合了先进技术与未受服务地区之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skin cancer is one of the most prevalent and preventable types of cancer, yetits early detection remains a challenge, particularly in resource-limitedsettings where access to specialized healthcare is scarce. This study proposesan AI-driven diagnostic tool optimized for embedded systems to address thisgap. Using transfer learning with the MobileNetV2 architecture, the model wasadapted for binary classification of skin lesions into "Skin Cancer" and"Other." The TensorRT framework was employed to compress and optimize the modelfor deployment on the NVIDIA Jetson Orin Nano, balancing performance withenergy efficiency. Comprehensive evaluations were conducted across multiplebenchmarks, including model size, inference speed, throughput, and powerconsumption. The optimized models maintained their performance, achieving anF1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.Post-compression results showed reductions in model size of up to 0.41, alongwith improvements in inference speed and throughput, and a decrease in energyconsumption of up to 0.93 in INT8 precision. These findings validate thefeasibility of deploying high-performing, energy-efficient diagnostic tools onresource-constrained edge devices. Beyond skin cancer detection, themethodologies applied in this research have broader applications in othermedical diagnostics and domains requiring accessible, efficient AI solutions.This study underscores the potential of optimized AI systems to revolutionizehealthcare diagnostics, thereby bridging the divide between advanced technologyand underserved regions.</description>
      <author>example@mail.com (Jacob M. Delgado-López, Andrea P. Seda-Hernandez, Juan D. Guadalupe-Rosado, Luis E. Fernandez Ramirez, Miguel Giboyeaux-Camilo, Wilfredo E. Lugo-Beauchamp)</author>
      <guid isPermaLink="false">2507.17125v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2507.17479v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为SRMambaV2的新方法，用于提高自动驾驶场景中稀疏LiDAR点云的插值准确性，同时保持几何重建的整体质量。&lt;h4&gt;背景&lt;/h4&gt;由于LiDAR点云数据的稀疏性和复杂的三维结构，在自动驾驶场景中对点云进行上采样是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出SRMambaV2方法，以解决稀疏区域点云上采样中的准确性问题。&lt;h4&gt;方法&lt;/h4&gt;SRMambaV2通过以下方式实现：1. 设计了一种仿生2D选择性扫描自注意力机制（2DSSA）来模拟远距离稀疏区域的特征分布。2. 引入双分支网络架构以增强稀疏特征的表达。3. 引入渐进自适应损失（PAL）函数以在插值过程中进一步细化细节的重建。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SRMambaV2在定性和定量评估中均表现出优异的性能。&lt;h4&gt;结论&lt;/h4&gt;SRMambaV2在自动驾驶场景中的稀疏点云上采样任务中表现出有效性和实际价值。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a novel method named SRMambaV2 to enhance the upsampling accuracy of sparse LiDAR point clouds in autonomous driving scenarios while preserving the overall geometric reconstruction quality. The method includes designing a biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the feature distribution in distant sparse areas, introducing a dual-branch network architecture to enhance the representation of sparse features, and using a progressive adaptive loss (PAL) function to refine the reconstruction of fine-grained details during the upsampling process. Experimental results demonstrate the superior performance of SRMambaV2 in both qualitative and quantitative evaluations, highlighting its effectiveness and practical value in automotive sparse point cloud upsampling tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Upsampling LiDAR point clouds in autonomous driving scenarios remains asignificant challenge due to the inherent sparsity and complex 3D structures ofthe data. Recent studies have attempted to address this problem by convertingthe complex 3D spatial scenes into 2D image super-resolution tasks. However,due to the sparse and blurry feature representation of range images, accuratelyreconstructing detailed and complex spatial topologies remains a majordifficulty. To tackle this, we propose a novel sparse point cloud upsamplingmethod named SRMambaV2, which enhances the upsampling accuracy in long-rangesparse regions while preserving the overall geometric reconstruction quality.Specifically, inspired by human driver visual perception, we design abiomimetic 2D selective scanning self-attention (2DSSA) mechanism to model thefeature distribution in distant sparse areas. Meanwhile, we introduce adual-branch network architecture to enhance the representation of sparsefeatures. In addition, we introduce a progressive adaptive loss (PAL) functionto further refine the reconstruction of fine-grained details during theupsampling process. Experimental results demonstrate that SRMambaV2 achievessuperior performance in both qualitative and quantitative evaluations,highlighting its effectiveness and practical value in automotive sparse pointcloud upsampling tasks.</description>
      <author>example@mail.com (Chuang Chen, Xiaolin Qin, Jing Hu, Wenyi Ge)</author>
      <guid isPermaLink="false">2507.17479v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement</title>
      <link>http://arxiv.org/abs/2507.17112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DGCDR的跨领域推荐方法，通过解耦表示学习来缓解数据稀疏性问题，并通过增强编码器-解码器框架和锚点监督策略来提高推荐系统的鲁棒性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;跨领域推荐旨在通过跨域知识迁移来缓解数据稀疏问题，而解耦表示学习可以有效地分离用户偏好中的域共享和域特定特征，提高模型的鲁棒性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出DGCDR方法，旨在解决基于解耦表示的跨领域推荐方法中存在的两个关键挑战：特征预分离策略引入的噪声和缺乏任务特定指导的解耦目标。&lt;h4&gt;方法&lt;/h4&gt;DGCDR方法首先使用图神经网络（GNN）提取高阶协作信号，然后编码器动态地将特征解耦为域共享和域特定空间，同时解码器引入基于锚点的监督，利用层次特征关系增强域内一致性和跨域对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的实验表明，DGCDR在关键指标上实现了最先进的性能，改进幅度高达11.59%。定性分析进一步验证了其优越的解耦质量和迁移能力。&lt;h4&gt;结论&lt;/h4&gt;DGCDR方法通过解决现有方法的挑战，显著提高了跨领域推荐系统的性能，并且其源代码和数据集已公开，便于进一步比较。&lt;h4&gt;翻译&lt;/h4&gt;Cross-domain recommendation (CDR) aims to alleviate the data sparsity by transferring knowledge across domains. Disentangled representation learning provides an effective solution to model complex user preferences by separating intra-domain features (domain-shared and domain-specific features), thereby enhancing robustness and interpretability. However, disentanglement-based CDR methods employing generative modeling or GNNs with contrastive objectives face two key challenges: (i) pre-separation strategies decouple features before extracting collaborative signals, disrupting intra-domain interactions and introducing noise; (ii) unsupervised disentanglement objectives lack explicit task-specific guidance, resulting in limited consistency and suboptimal alignment. To address these challenges, we propose DGCDR, a GNN-enhanced encoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to extract high-order collaborative signals, providing enriched representations as a robust foundation for disentanglement. The encoder then dynamically disentangles features into domain-shared and -specific spaces, preserving collaborative information during the separation process. To handle challenge (ii), the decoder introduces an anchor-based supervision that leverages hierarchical feature relationships to enhance intra-domain consistency and cross-domain alignment. Extensive experiments on real-world datasets demonstrate that DGCDR achieves state-of-the-art performance, with improvements of up to 11.59% across key metrics. Qualitative analyses further validate its superior disentanglement quality and transferability. Our source code and datasets are available on GitHub for further comparison.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3705328.3748044&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain recommendation (CDR) aims to alleviate the data sparsity bytransferring knowledge across domains. Disentangled representation learningprovides an effective solution to model complex user preferences by separatingintra-domain features (domain-shared and domain-specific features), therebyenhancing robustness and interpretability. However, disentanglement-based CDRmethods employing generative modeling or GNNs with contrastive objectives facetwo key challenges: (i) pre-separation strategies decouple features beforeextracting collaborative signals, disrupting intra-domain interactions andintroducing noise; (ii) unsupervised disentanglement objectives lack explicittask-specific guidance, resulting in limited consistency and suboptimalalignment. To address these challenges, we propose DGCDR, a GNN-enhancedencoder-decoder framework. To handle challenge (i), DGCDR first applies GNN toextract high-order collaborative signals, providing enriched representations asa robust foundation for disentanglement. The encoder then dynamicallydisentangles features into domain-shared and -specific spaces, preservingcollaborative information during the separation process. To handle challenge(ii), the decoder introduces an anchor-based supervision that leverageshierarchical feature relationships to enhance intra-domain consistency andcross-domain alignment. Extensive experiments on real-world datasetsdemonstrate that DGCDR achieves state-of-the-art performance, with improvementsof up to 11.59% across key metrics. Qualitative analyses further validate itssuperior disentanglement quality and transferability. Our source code anddatasets are available on GitHub for further comparison.</description>
      <author>example@mail.com (Yuhan Wang, Qing Xie, Zhifeng Bao, Mengzi Tang, Lin Li, Yongjian Liu)</author>
      <guid isPermaLink="false">2507.17112v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation</title>
      <link>http://arxiv.org/abs/2507.17121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 1 Figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种鲁棒的深度学习框架，用于对糖尿病视网膜病变（DR）进行二分类和五分类，利用迁移学习和数据增强来应对类别不平衡和训练数据有限的问题。&lt;h4&gt;背景&lt;/h4&gt;糖尿病视网膜病变是全球导致失明的主要原因，通过自动化视网膜图像分析进行早期诊断可以显著降低失明的风险。&lt;h4&gt;目的&lt;/h4&gt;开发一种准确且高效的深度学习模型，用于DR的早期诊断和分类。&lt;h4&gt;方法&lt;/h4&gt;研究者在APTOS 2019数据集上评估了多种预训练卷积神经网络架构，包括ResNet和EfficientNet的变体，并利用迁移学习和数据增强技术来提高模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;在二分类任务中，提出的模型达到了98.9%的准确率，精度为98.6%，召回率为99.3%，F1分数为98.9%，AUC为99.4%。在更具挑战性的五分类严重程度分类任务中，模型获得了84.6%的准确率和94.1%的AUC，优于几种现有方法。 EfficientNet-B0和ResNet34在两个任务中提供了准确性和计算效率之间的最佳平衡。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，结合类别平衡增强和迁移学习对于高性能DR诊断是有效的。提出的框架为DR筛查提供了一种可扩展且准确的解决方案，具有在现实世界临床环境中部署的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and early diagnosis through automated retinal image analysis can significantly reduce the risk of blindness. This paper presents a robust deep learning framework for both binary and five-class DR classification, leveraging transfer learning and extensive data augmentation to address the challenges of class imbalance and limited training data. We evaluate a range of pretrained convolutional neural network architectures, including variants of ResNet and EfficientNet, on the APTOS 2019 dataset. For binary classification, our proposed model achieves a state-of-the-art accuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of 98.9%, and an AUC of 99.4%. In the more challenging five-class severity classification task, our model obtains a competitive accuracy of 84.6% and an AUC of 94.1%, outperforming several existing approaches. Our findings also demonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between accuracy and computational efficiency across both tasks. These results underscore the effectiveness of combining class-balanced augmentation with transfer learning for high-performance DR diagnosis. The proposed framework provides a scalable and accurate solution for DR screening, with potential for deployment in real-world clinical environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, andearly diagnosis through automated retinal image analysis can significantlyreduce the risk of blindness. This paper presents a robust deep learningframework for both binary and five-class DR classification, leveraging transferlearning and extensive data augmentation to address the challenges of classimbalance and limited training data. We evaluate a range of pretrainedconvolutional neural network architectures, including variants of ResNet andEfficientNet, on the APTOS 2019 dataset.  For binary classification, our proposed model achieves a state-of-the-artaccuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of98.9%, and an AUC of 99.4%. In the more challenging five-class severityclassification task, our model obtains a competitive accuracy of 84.6% and anAUC of 94.1%, outperforming several existing approaches. Our findings alsodemonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs betweenaccuracy and computational efficiency across both tasks.  These results underscore the effectiveness of combining class-balancedaugmentation with transfer learning for high-performance DR diagnosis. Theproposed framework provides a scalable and accurate solution for DR screening,with potential for deployment in real-world clinical environments.</description>
      <author>example@mail.com (Faisal Ahmed, Mohammad Alfrad Nobel Bhuiyan)</author>
      <guid isPermaLink="false">2507.17121v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task</title>
      <link>http://arxiv.org/abs/2507.17326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了Whisper ASR模型在卒中后语言障碍评估中的应用效果，发现经过微调后，模型在语音转录和语言功能预测方面表现良好，但仍存在跨领域泛化方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;卒中后的语言障碍评估是一个复杂且耗时的工作，目前缺乏有效的自动评估方法。&lt;h4&gt;目的&lt;/h4&gt;研究Whisper ASR模型在卒中后患者图片命名任务中的转录和分析能力，以评估其对语言功能的预测能力。&lt;h4&gt;方法&lt;/h4&gt;研究者使用Whisper模型对卒中患者和健康人的语音进行转录，并评估其转录准确性和对语言功能的预测能力。&lt;h4&gt;主要发现&lt;/h4&gt;Whisper模型在单词语音转录方面表现不佳，但经过微调后转录准确性显著提高，同时模型能够准确预测语音质量。然而，在未见过的数据集上测试发现模型的泛化能力有限。&lt;h4&gt;结论&lt;/h4&gt;Whisper模型在经过适当微调后，在自动语音和语言评估及康复方面具有潜力，但仍需针对特定临床人群进行模型调整，以提高其泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;This study evaluates the application of the Whisper ASR model in the assessment of language impairment following stroke. The results show that, after fine-tuning, the model performs well in speech transcription and prediction of language function, although it still faces challenges in cross-domain generalization. Fine-tuning the model can significantly improve transcription accuracy and enable accurate prediction of speech quality, but further adaptation to specific clinical populations is needed to enhance its generalization ability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detailed assessment of language impairment following stroke remains acognitively complex and clinician-intensive task, limiting timely and scalablediagnosis. Automatic Speech Recognition (ASR) foundation models offer apromising pathway to augment human evaluation through intelligent systems, buttheir effectiveness in the context of speech and language impairment remainsuncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASRfoundation model, can be applied to transcribe and analyze speech from patientswith stroke during a commonly used picture-naming task. We assess both verbatimtranscription accuracy and the model's ability to support downstream predictionof language function, which has major implications for outcomes after stroke.Our results show that the baseline Whisper model performs poorly on single-wordspeech utterances. Nevertheless, fine-tuning Whisper significantly improvestranscription accuracy (reducing Word Error Rate by 87.72% in healthy speechand 71.22% in speech from patients). Further, learned representations from themodel enable accurate prediction of speech quality (average F1 Macro of 0.74for healthy, 0.75 for patients). However, evaluations on an unseen (TORGO)dataset reveal limited generalizability, highlighting the inability of Whisperto perform zero-shot transcription of single-word utterances on out-of-domainclinical speech and emphasizing the need to adapt models to specific clinicalpopulations. While challenges remain in cross-domain generalization, thesefindings highlight the potential of foundation models, when appropriatelyfine-tuned, to advance automated speech and language assessment andrehabilitation for stroke-related impairments.</description>
      <author>example@mail.com (Milena Davudova, Ziyuan Cai, Valentina Giunchiglia, Dragos C. Gruia, Giulia Sanguedolce, Adam Hampshire, Fatemeh Geranmayeh)</author>
      <guid isPermaLink="false">2507.17326v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning</title>
      <link>http://arxiv.org/abs/2507.17454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为C3RL的新型表示学习框架，用于多变量时间序列预测，旨在解决现有方法中混合策略的局限性和不足。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列预测因其实际应用的重要性而受到越来越多的关注。现有方法通常采用通道混合（CM）或通道独立（CI）策略，但每种策略都有其局限性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的不足，本文提出C3RL框架，旨在同时模型CM和CI策略，以实现更好的表示学习和预测性能。&lt;h4&gt;方法&lt;/h4&gt;C3RL框架受到计算机视觉中对比学习的启发，将两种策略的输入视为转置视图，并构建了一个Siamese网络架构。一种策略作为主干，另一种策略进行补充。通过联合优化对比和预测损失，并采用自适应加权，C3RL平衡了表示和预测性能。&lt;h4&gt;主要发现&lt;/h4&gt;在七个模型上的大量实验表明，C3RL将基于CI策略的模型的最佳性能率提升至81.4%，而基于CM策略的模型提升至76.3%，显示出强大的泛化能力和有效性。&lt;h4&gt;结论&lt;/h4&gt;C3RL框架能够有效提升多变量时间序列预测的性能，其代码将在论文被接受后提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series forecasting has drawn increasing attention due toits practical importance. Existing approaches typically adopt eitherchannel-mixing (CM) or channel-independence (CI) strategies. CM strategy cancapture inter-variable dependencies but fails to discern variable-specifictemporal patterns. CI strategy improves this aspect but fails to fully exploitcross-variable dependencies like CM. Hybrid strategies based on feature fusionoffer limited generalization and interpretability. To address these issues, wepropose C3RL, a novel representation learning framework that jointly modelsboth CM and CI strategies. Motivated by contrastive learning in computervision, C3RL treats the inputs of the two strategies as transposed views andbuilds a siamese network architecture: one strategy serves as the backbone,while the other complements it. By jointly optimizing contrastive andprediction losses with adaptive weighting, C3RL balances representation andforecasting performance. Extensive experiments on seven models show that C3RLboosts the best-case performance rate to 81.4\% for models based on CI strategyand to 76.3\% for models based on CM strategy, demonstrating stronggeneralization and effectiveness. The code will be available once the paper isaccepted.</description>
      <author>example@mail.com (Shusen Ma, Yun-Bo Zhao, Yu Kang)</author>
      <guid isPermaLink="false">2507.17454v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Controllable Hybrid Captioner for Improved Long-form Video Understanding</title>
      <link>http://arxiv.org/abs/2507.17047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于文本的视频内容摘要方法，通过视频字幕生成和视觉语言模型来丰富记忆，提高视频理解系统的性能。&lt;h4&gt;背景&lt;/h4&gt;视频数据密度高、维度大，基于文本的摘要可以更紧凑地表示内容，且易于大型语言模型处理。&lt;h4&gt;目的&lt;/h4&gt;解决视频内容理解问题，通过文本摘要和视觉语言模型来丰富记忆，提高视频理解系统的性能。&lt;h4&gt;方法&lt;/h4&gt;使用视频字幕生成器对视频进行分段处理，并利用视觉语言模型来增加静态场景描述，结合LaViLa视频字幕生成器和大型语言模型来回答视频相关问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过将静态场景描述融入字幕生成流程，提高了字幕日志的详细度和完整性，扩展了可回答问题的范围。同时，通过微调LaViLa视频字幕生成器，实现了动作和场景字幕的生成，提高了字幕生成效率。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法有效提高了视频理解系统的性能，并通过控制混合字幕生成器实现了不同类型字幕的交替生成。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a text-based video content summarization method, enriching the memory with the help of video captioning and Vision Language Models (VLMs) to improve the performance of video understanding systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video data, especially long-form video, is extremely dense andhigh-dimensional. Text-based summaries of video content offer a way torepresent query-relevant content in a much more compact manner than raw video.In addition, textual representations are easily ingested by state-of-the-artlarge language models (LLMs), which enable reasoning over video content toanswer complex natural language queries. To solve this issue, we rely on theprogressive construction of a text-based memory by a video captioner operatingon shorter chunks of the video, where spatio-temporal modeling iscomputationally feasible. We explore ways to improve the quality of theactivity log comprised solely of short video captions. Because the videocaptions tend to be focused on human actions, and questions may pertain toother information in the scene, we seek to enrich the memory with static scenedescriptions using Vision Language Models (VLMs). Our video understandingsystem relies on the LaViLa video captioner in combination with a LLM to answerquestions about videos. We first explored different ways of partitioning thevideo into meaningful segments such that the textual descriptions moreaccurately reflect the structure of the video content. Furthermore, weincorporated static scene descriptions into the captioning pipeline using LLaVAVLM, resulting in a more detailed and complete caption log and expanding thespace of questions that are answerable from the textual memory. Finally, wehave successfully fine-tuned the LaViLa video captioner to produce both actionand scene captions, significantly improving the efficiency of the captioningpipeline compared to using separate captioning models for the two tasks. Ourmodel, controllable hybrid captioner, can alternate between different types ofcaptions according to special input tokens that signals scene changes detectedin the video.</description>
      <author>example@mail.com (Kuleen Sasse, Efsun Sarioglu Kayi, Arun Reddy)</author>
      <guid isPermaLink="false">2507.17047v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>PyG 2.0: Scalable Learning on Real World Graphs</title>
      <link>http://arxiv.org/abs/2507.16991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PyG自发布以来发展迅速，已成为图神经网络的主要框架。本文介绍了PyG 2.0及其后续版本，这是一个全面的更新，引入了显著的扩展性和实际应用能力改进。&lt;h4&gt;背景&lt;/h4&gt;PyG自发布以来，在图神经网络领域取得了显著进展，并逐渐成为该领域的领先框架。&lt;h4&gt;目的&lt;/h4&gt;本文旨在详细描述PyG 2.0的增强架构，包括对异构和时序图的支持、可扩展的特征/图存储以及各种优化，以帮助研究人员和从业者高效地处理大规模图学习问题。&lt;h4&gt;方法&lt;/h4&gt;通过引入新的架构和优化，PyG 2.0及其后续版本在扩展性和实际应用能力方面进行了全面更新。&lt;h4&gt;主要发现&lt;/h4&gt;PyG近年在多个应用领域支持图学习，包括关系深度学习和大型语言模型等重要领域。&lt;h4&gt;结论&lt;/h4&gt;PyG 2.0及其后续版本通过增强的架构和优化，为处理大规模图学习问题提供了高效的方法，并在多个应用领域得到了广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：PyG（PyTorch Geometric）自首次发布以来已经发生了显著的变化，确立了其在图神经网络领域的主导地位。在这篇论文中，我们介绍了PyG 2.0（及其后续的小版本），这是一个全面的更新，引入了在可扩展性和实际应用能力方面的重大改进。我们详细介绍了框架的增强架构，包括对异构和时序图的支持、可扩展的特征/图存储以及各种优化，使研究人员和从业者能够高效地处理大规模图学习问题。在过去的几年中，PyG已经在各种应用领域支持图学习，我们将对此进行总结，同时深入探讨关系深度学习和大型语言模型等重要领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PyG (PyTorch Geometric) has evolved significantly since its initial release,establishing itself as a leading framework for Graph Neural Networks. In thispaper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensiveupdate that introduces substantial improvements in scalability and real-worldapplication capabilities. We detail the framework's enhanced architecture,including support for heterogeneous and temporal graphs, scalable feature/graphstores, and various optimizations, enabling researchers and practitioners totackle large-scale graph learning problems efficiently. Over the recent years,PyG has been supporting graph learning in a large variety of application areas,which we will summarize, while providing a deep dive into the important areasof relational deep learning and large language modeling.</description>
      <author>example@mail.com (Matthias Fey, Jinu Sunil, Akihiro Nitta, Rishi Puri, Manan Shah, Blaž Stojanovič, Ramona Bendias, Alexandria Barghi, Vid Kocijan, Zecheng Zhang, Xinwei He, Jan Eric Lenssen, Jure Leskovec)</author>
      <guid isPermaLink="false">2507.16991v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Prompt Programming Tasks and Questions</title>
      <link>http://arxiv.org/abs/2507.17264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了提示编程的过程和工具，通过构建一个任务和问题的分类体系，分析了提示编程中存在的问题和机遇。&lt;h4&gt;背景&lt;/h4&gt;提示编程是一种将提示嵌入到软件中的方法，使大型语言模型能够实现新的AI功能，但当前提示编程工具的支持不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决提示编程中存在的问题，本文旨在通过开发任务和问题的分类体系，分析提示编程中的需求，并探讨改进机会。&lt;h4&gt;方法&lt;/h4&gt;本文通过访谈、观察和调查，收集了16位提示编程者、8位开发者的行为数据，并与48款研究和商业工具进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;发现提示编程没有得到充分的工具支持，大部分任务需要手动完成，且51个问题中的16个，包括多数重要问题尚未得到解答。&lt;h4&gt;结论&lt;/h4&gt;本文提出了提示编程工具的重要改进机会，为后续研究和开发提供了指导方向。&lt;h4&gt;翻译&lt;/h4&gt;This paper studies the process and tools of prompting programming, by developing a classification system for tasks and questions, analyzes the problems and opportunities in prompting programming.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prompting foundation models (FMs) like large language models (LLMs) haveenabled new AI-powered software features (e.g., text summarization) thatpreviously were only possible by fine-tuning FMs. Now, developers are embeddingprompts in software, known as prompt programs. The process of promptprogramming requires the developer to make many changes to their prompt. Yet,the questions developers ask to update their prompt is unknown, despite theanswers to these questions affecting how developers plan their changes. Withthe growing number of research and commercial prompt programming tools, it isunclear whether prompt programmers' needs are being adequately addressed. Weaddress these challenges by developing a taxonomy of 25 tasks promptprogrammers do and 51 questions they ask, measuring the importance of each taskand question. We interview 16 prompt programmers, observe 8 developers makeprompt changes, and survey 50 developers. We then compare the taxonomy with 48research and commercial tools. We find that prompt programming is notwell-supported: all tasks are done manually, and 16 of the 51 questions --including a majority of the most important ones -- remain unanswered. Based onthis, we outline important opportunities for prompt programming tools.</description>
      <author>example@mail.com (Jenny T. Liang, Chenyang Yang, Agnia Sergeyuk, Travis D. Breaux, Brad A. Myers)</author>
      <guid isPermaLink="false">2507.17264v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>PointLAMA: Latent Attention meets Mamba for Efficient Point Cloud Pretraining</title>
      <link>http://arxiv.org/abs/2507.17296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PointLAMA是一种点云预训练框架，旨在解决Mamba在捕捉3D数据中细粒度几何结构方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;Mamba作为点云建模的骨干模型，虽然具有高效的全球序列建模能力，但缺乏局部归纳偏置，限制了其捕捉3D数据中细粒度几何结构的能力。&lt;h4&gt;目的&lt;/h4&gt;提出PointLAMA框架，以增强点云模型的局部结构捕捉能力。&lt;h4&gt;方法&lt;/h4&gt;PointLAMA结合了任务感知的点云序列化、混合编码器（集成潜在注意力和Mamba块）以及基于Mamba骨干的条件扩散机制。具体包括：使用Hilbert/Trans-Hilbert空间填充曲线和轴排序来结构化对齐点标记，以适应分类和分割任务；轻量级的潜在注意力块包含点-wise多头潜在注意力（PMLA）模块，以增强局部上下文建模；在预训练期间引入条件扩散机制，以去噪扰动的特征序列。&lt;h4&gt;主要发现&lt;/h4&gt;PointLAMA在多个基准数据集上实现了有竞争力的性能，同时参数数量和浮点运算次数（FLOPs）最小。&lt;h4&gt;结论&lt;/h4&gt;PointLAMA验证了其在高效点云预训练方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Mamba最近作为点云建模的骨干模型受到广泛关注，它利用状态空间架构实现了具有线性复杂度的有效全局序列建模。然而，其缺乏局部归纳偏置限制了其在3D数据中捕捉细粒度几何结构的能力。为了解决这一局限性，我们提出了PointLAMA，一种结合任务感知的点云序列化、混合编码器（集成潜在注意力和Mamba块）以及基于Mamba骨干的条件扩散机制的点云预训练框架。具体来说，任务感知的点云序列化采用Hilbert/Trans-Hilbert空间填充曲线和轴排序来结构化对齐点标记，分别用于分类和分割任务。我们轻量级的潜在注意力块包含点-wise多头潜在注意力（PMLA）模块，该模块专门设计为与Mamba架构相匹配，通过利用PMLA和Mamba共享的潜在空间特征。这增强了局部上下文建模，同时保持了整体效率。为了进一步增强表示学习，我们在预训练期间引入了条件扩散机制，该机制在不依赖于显式点-wise重建的情况下去噪扰动的特征序列。实验结果表明，PointLAMA在多个基准数据集上实现了有竞争力的性能，同时参数数量和浮点运算次数（FLOPs）最小，验证了其在高效点云预训练方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mamba has recently gained widespread attention as a backbone model for pointcloud modeling, leveraging a state-space architecture that enables efficientglobal sequence modeling with linear complexity. However, its lack of localinductive bias limits its capacity to capture fine-grained geometric structuresin 3D data. To address this limitation, we propose \textbf{PointLAMA}, a pointcloud pretraining framework that combines task-aware point cloud serialization,a hybrid encoder with integrated Latent Attention and Mamba blocks, and aconditional diffusion mechanism built upon the Mamba backbone. Specifically,the task-aware point cloud serialization employs Hilbert/Trans-Hilbertspace-filling curves and axis-wise sorting to structurally align point tokensfor classification and segmentation tasks, respectively. Our lightweight LatentAttention block features a Point-wise Multi-head Latent Attention (PMLA)module, which is specifically designed to align with the Mamba architecture byleveraging the shared latent space characteristics of PMLA and Mamba. Thisenables enhanced local context modeling while preserving overall efficiency. Tofurther enhance representation learning, we incorporate a conditional diffusionmechanism during pretraining, which denoises perturbed feature sequenceswithout relying on explicit point-wise reconstruction. Experimental resultsdemonstrate that PointLAMA achieves competitive performance on multiplebenchmark datasets with minimal parameter count and FLOPs, validating itseffectiveness for efficient point cloud pretraining.</description>
      <author>example@mail.com (Xuanyu Lin, Xiaona Zeng, Xianwei Zheng, Xutao Li)</author>
      <guid isPermaLink="false">2507.17296v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning</title>
      <link>http://arxiv.org/abs/2507.17368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ViRN的持续学习框架，用于解决长尾数据分布下的持续学习挑战，该框架通过结合变分推断和分布三角定位来实现鲁棒的长尾学习。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的AI系统中，持续学习（CL）是一个关键挑战，模型需要在适应新类别的同时保持对旧类别的知识，尽管存在严重的类别不平衡。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文旨在提出一种能够平衡稳定性和可塑性的新方法，以应对极端样本稀缺的情况。&lt;h4&gt;方法&lt;/h4&gt;ViRN框架首先通过变分自动编码器来建模类条件分布，以减轻对头部类别的偏差。其次，通过基于Wasserstein距离的邻域检索和几何融合来重建尾部类别的分布，从而实现尾部类别表示的高效对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在六个长尾分类基准测试中，包括语音（例如，罕见声学事件、口音）和图像任务，ViRN在平均准确率上比最先进的方法提高了10.24%。&lt;h4&gt;结论&lt;/h4&gt;ViRN框架在长尾学习方面表现出色，为解决现实世界AI系统中的持续学习问题提供了一种有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：持续学习（CL）在长尾数据分布下仍然是一个关键的挑战，对于现实世界的AI系统来说，模型必须在适应新类别的同时保留对旧类别的知识，尽管存在严重的类别不平衡。现有的方法在平衡稳定性和可塑性方面存在困难，经常在极端样本稀缺的情况下崩溃。为了解决这个问题，我们提出了ViRN，这是一种新的CL框架，它将变分推断（VI）与分布三角定位相结合，以实现鲁棒的长尾学习。首先，我们通过变分自动编码器来建模类条件分布，以减轻对头部类别的偏差。其次，我们通过基于Wasserstein距离的邻域检索和几何融合来重建尾部类别的分布，从而实现尾部类别表示的高效对齐。在包括语音（例如，罕见声学事件、口音）和图像任务在内的六个长尾分类基准测试中评估，ViRN比最先进的方法实现了10.24%的平均准确率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning (CL) with long-tailed data distributions remains acritical challenge for real-world AI systems, where models must sequentiallyadapt to new classes while retaining knowledge of old ones, despite severeclass imbalance. Existing methods struggle to balance stability and plasticity,often collapsing under extreme sample scarcity. To address this, we proposeViRN, a novel CL framework that integrates variational inference (VI) withdistributional trilateration for robust long-tailed learning. First, we modelclass-conditional distributions via a Variational Autoencoder to mitigate biastoward head classes. Second, we reconstruct tail-class distributions viaWasserstein distance-based neighborhood retrieval and geometric fusion,enabling sample-efficient alignment of tail-class representations. Evaluated onsix long-tailed classification benchmarks, including speech (e.g., rareacoustic events, accents) and image tasks, ViRN achieves a 10.24% averageaccuracy gain over state-of-the-art methods.</description>
      <author>example@mail.com (Hao Dai, Chong Tang, Jagmohan Chauhan)</author>
      <guid isPermaLink="false">2507.17368v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training</title>
      <link>http://arxiv.org/abs/2507.17239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to MedAGI 2025 (Oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了半监督视觉-语言预训练任务，旨在充分利用配对和未配对图像数据，提出了一种名为MaskedCLIP的框架，用于学习更具泛化能力的图像特征。&lt;h4&gt;背景&lt;/h4&gt;目前最先进的医学图像分析方法依赖于配对图像-文本数据或未配对图像数据来学习具有泛化能力的图像特征，但仅使用一种数据类型限制了模型学习更丰富和全面的图像特征的能力。&lt;h4&gt;目的&lt;/h4&gt;提出半监督视觉-语言预训练方法，旨在充分利用配对和未配对图像数据，提高基础模型学习图像特征的全面性。&lt;h4&gt;方法&lt;/h4&gt;提出MaskedCLIP框架，该框架结合了掩码图像建模和对比语言-图像预训练，通过桥接Transformer连接掩码特征空间和CLIP特征空间，并使用掩码知识蒸馏损失来提高特征提取的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验证明，该方法在视网膜图像分析任务中表现出有效性和数据效率。&lt;h4&gt;结论&lt;/h4&gt;MaskedCLIP框架能够有效利用配对和未配对图像数据，学习更具泛化能力的图像特征，从而提高下游任务性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have recently gained tremendous popularity in medical imageanalysis. State-of-the-art methods leverage either paired image-text data viavision-language pre-training or unpaired image data via self-supervisedpre-training to learn foundation models with generalizable image features toboost downstream task performance. However, learning foundation modelsexclusively on either paired or unpaired image data limits their ability tolearn richer and more comprehensive image features. In this paper, weinvestigate a novel task termed semi-supervised vision-language pre-training,aiming to fully harness the potential of both paired and unpaired image datafor foundation model learning. To this end, we propose MaskedCLIP, asynergistic masked image modeling and contrastive language-image pre-trainingframework for semi-supervised vision-language pre-training. The key challengein combining paired and unpaired image data for learning a foundation modellies in the incompatible feature spaces derived from these two types of data.To address this issue, we propose to connect the masked feature space with theCLIP feature space with a bridge transformer. In this way, the more semanticspecific CLIP features can benefit from the more general masked features forsemantic feature extraction. We further propose a masked knowledge distillationloss to distill semantic knowledge of original image features in CLIP featurespace back to the predicted masked image features in masked feature space. Withthis mutually interactive design, our framework effectively leverages bothpaired and unpaired image data to learn more generalizable image features fordownstream tasks. Extensive experiments on retinal image analysis demonstratethe effectiveness and data efficiency of our method.</description>
      <author>example@mail.com (Lei Zhu, Jun Zhou, Rick Siow Mong Goh, Yong Liu)</author>
      <guid isPermaLink="false">2507.17239v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Deformable Cluster Manipulation via Whole-Arm Policy Learning</title>
      <link>http://arxiv.org/abs/2507.17085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架，用于学习无需模型的策略，通过结合三维点云和本体感觉指标，实现对变形物体的全身体操感知操控。该框架通过分布状态表示和核均值嵌入，提高了训练效率和实时推理能力。此外，还提出了一个与上下文无关的遮挡启发式算法，用于去除目标区域的变形物体。在输电线清理场景中，该框架能够生成利用多臂链的创意策略。最后，实现了从仿真到现实的无样本策略迁移。&lt;h4&gt;背景&lt;/h4&gt;操控变形物体具有广泛的应用前景，但需要丰富的接触式全臂交互，面临着现实模型合成能力有限、感知高不确定性、缺乏有效空间抽象等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，用于解决操控变形物体的问题，提高训练效率，实现实时推理，并能够去除目标区域的遮挡。&lt;h4&gt;方法&lt;/h4&gt;该框架结合了三维点云和本体感觉指标，使用分布状态表示和核均值嵌入进行学习，并提出了一种遮挡启发式算法。&lt;h4&gt;主要发现&lt;/h4&gt;在输电线清理场景中，该框架能够生成创意策略，并实现从仿真到现实的无样本策略迁移。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效地操控变形物体，具有实际应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;We propose a novel framework for learning model-free policies integrating two modalities: 3D point clouds and proprioceptive touch indicators, emphasizing manipulation with full body contact awareness, going beyond traditional end-effector modes. Our reinforcement learning framework leverages a distributional state representation, aided by kernel mean embeddings, to achieve improved training efficiency and real-time inference. Furthermore, we propose a novel context-agnostic occlusion heuristic to clear deformables from a target region for exposure tasks. We deploy the framework in a power line clearance scenario and observe that the agent generates creative strategies leveraging multiple arm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy transfer, allowing the arm to clear real branches with unknown occlusion patterns, unseen topology, and uncertain dynamics.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manipulating clusters of deformable objects presents a substantial challengewith widespread applicability, but requires contact-rich whole-arminteractions. A potential solution must address the limited capacity forrealistic model synthesis, high uncertainty in perception, and the lack ofefficient spatial abstractions, among others. We propose a novel framework forlearning model-free policies integrating two modalities: 3D point clouds andproprioceptive touch indicators, emphasising manipulation with full bodycontact awareness, going beyond traditional end-effector modes. Ourreinforcement learning framework leverages a distributional staterepresentation, aided by kernel mean embeddings, to achieve improved trainingefficiency and real-time inference. Furthermore, we propose a novelcontext-agnostic occlusion heuristic to clear deformables from a target regionfor exposure tasks. We deploy the framework in a power line clearance scenarioand observe that the agent generates creative strategies leveraging multiplearm links for de-occlusion. Finally, we perform zero-shot sim-to-real policytransfer, allowing the arm to clear real branches with unknown occlusionpatterns, unseen topology, and uncertain dynamics.</description>
      <author>example@mail.com (Jayadeep Jacob, Wenzheng Zhang, Houston Warren, Paulo Borges, Tirthankar Bandyopadhyay, Fabio Ramos)</author>
      <guid isPermaLink="false">2507.17085v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models</title>
      <link>http://arxiv.org/abs/2507.17220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PIG-Nav，一种基于预训练模型进行视觉机器人导航的新方法，该方法在多个环境中的导航性能得到提升，并减少了需要微调的数据量。&lt;h4&gt;背景&lt;/h4&gt;近期研究探讨了预训练模型在视觉机器人导航中的应用，旨在实现不同环境下的通用导航和正迁移，并提高在未知设置中的零样本性能。&lt;h4&gt;目的&lt;/h4&gt;进一步研究视觉导航模型的预训练策略，并在两个关键领域做出贡献：模型设计和数据集处理。&lt;h4&gt;方法&lt;/h4&gt;模型设计方面，通过以下两种策略提升预训练导航模型的性能：(1) 整合早期融合网络结构，利用预训练的Vision Transformer图像编码器结合视觉观察和目标图像；(2) 引入适当的辅助任务，增强全局导航表示学习。数据集处理方面，提出了一种新的数据预处理流程，用于高效标注大规模游戏视频数据集。&lt;h4&gt;主要发现&lt;/h4&gt;PIG-Nav在两个复杂模拟环境和真实世界中，相较于现有视觉导航基础模型，在零样本和微调设置下分别提高了22.6%和37.5%的性能。&lt;h4&gt;结论&lt;/h4&gt;PIG-Nav在减少需要微调数据量的同时，保持了有竞争力的性能，具有在现实世界中应用的最小标记监督潜力。&lt;h4&gt;翻译&lt;/h4&gt;Recent studies have explored pretrained (foundation) models for vision-based robotic navigation, aiming to achieve generalizable navigation and positive transfer across diverse environments while enhancing zero-shot performance in unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal Navigation), a new approach that further investigates pretraining strategies for vision-based navigation models and contributes in two key areas. Model-wise, we identify two critical design choices that consistently improve the performance of pretrained navigation models: (1) integrating an early-fusion network structure to combine visual observations and goal images via appropriately pretrained Vision Transformer (ViT) image encoder, and (2) introducing suitable auxiliary tasks to enhance global navigation representation learning, thus further improving navigation performance. Dataset-wise, we propose a novel data preprocessing pipeline for efficiently labeling large-scale game video datasets for navigation model training. We demonstrate that augmenting existing open navigation datasets with diverse gameplay videos improves model performance. Our model achieves an average improvement of 22.6% in zero-shot settings and a 37.5% improvement in fine-tuning settings over existing visual navigation foundation models in two complex simulated environments and one real-world environment. These results advance the state-of-the-art in pretrained image-goal navigation models. Notably, our model maintains competitive performance while requiring significantly less fine-tuning data, highlighting its potential for real-world deployment with minimal labeled supervision.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have explored pretrained (foundation) models for vision-basedrobotic navigation, aiming to achieve generalizable navigation and positivetransfer across diverse environments while enhancing zero-shot performance inunseen settings. In this work, we introduce PIG-Nav (Pretrained Image-GoalNavigation), a new approach that further investigates pretraining strategiesfor vision-based navigation models and contributes in two key areas.Model-wise, we identify two critical design choices that consistently improvethe performance of pretrained navigation models: (1) integrating anearly-fusion network structure to combine visual observations and goal imagesvia appropriately pretrained Vision Transformer (ViT) image encoder, and (2)introducing suitable auxiliary tasks to enhance global navigationrepresentation learning, thus further improving navigation performance.Dataset-wise, we propose a novel data preprocessing pipeline for efficientlylabeling large-scale game video datasets for navigation model training. Wedemonstrate that augmenting existing open navigation datasets with diversegameplay videos improves model performance. Our model achieves an averageimprovement of 22.6% in zero-shot settings and a 37.5% improvement infine-tuning settings over existing visual navigation foundation models in twocomplex simulated environments and one real-world environment. These resultsadvance the state-of-the-art in pretrained image-goal navigation models.Notably, our model maintains competitive performance while requiringsignificantly less fine-tuning data, highlighting its potential for real-worlddeployment with minimal labeled supervision.</description>
      <author>example@mail.com (Jiansong Wan, Chengming Zhou, Jinkua Liu, Xiangge Huang, Xiaoyu Chen, Xiaohan Yi, Qisen Yang, Baiting Zhu, Xin-Qiang Cai, Lixing Liu, Rushuai Yang, Chuheng Zhang, Sherif Abdelfattah, Hayong Shin, Pushi Zhang, Li Zhao, Jiang Bian)</author>
      <guid isPermaLink="false">2507.17220v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Principled Multimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2507.17343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 9 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Principled Multimodal Representation Learning (PMRL)的新框架，旨在通过同时无锚依赖地对齐多种模态来提高多模态理解。&lt;h4&gt;背景&lt;/h4&gt;传统的多模态表示学习方法依赖于成对对比学习，并依赖于预定义的锚模态，限制了跨所有模态的对齐。&lt;h4&gt;目的&lt;/h4&gt;解决固定锚点限制和优化奇异值乘积引起的不稳定性等挑战。&lt;h4&gt;方法&lt;/h4&gt;PMRL框架基于理论洞察，即完全对齐对应于秩-1的Gram矩阵，通过优化表示矩阵的主奇异值来实现模态沿共享的主方向对齐。此外，使用基于softmax的损失函数，将奇异值视为logits，优先考虑最大的奇异值，并通过对主特征向量的实例对比正则化来保持实例间分离性，防止表示坍缩。&lt;h4&gt;主要发现&lt;/h4&gt;在多种任务上的广泛实验表明，PMRL相比基线方法具有优越性。&lt;h4&gt;结论&lt;/h4&gt;PMRL是一个稳定的框架，能够同时无锚依赖地对齐多种模态，提高了多模态理解的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态表示学习旨在通过整合不同的数据模态来创建一个统一的表示空间，以改善多模态理解。传统方法通常依赖于成对对比学习，这依赖于预定义的锚模态，限制了所有模态之间的对齐。最近的研究探索了同时对齐多种模态，但仍存在一些挑战，例如固定锚点带来的限制和优化奇异值乘积引起的不稳定性。为了解决这些挑战，本文提出了一种名为原理性多模态表示学习（PMRL）的新框架，以一种更稳定的方式在不依赖于锚点的情况下实现多种模态的同时对齐。具体而言，基于完全对齐对应于秩-1的Gram矩阵的理论洞察，PMRL通过优化表示矩阵的主奇异值来沿共享的主方向对齐模态。此外，基于softmax的损失函数将奇异值视为logits，以优先考虑最大的奇异值。另外，对主特征向量的实例对比正则化保持了实例间的分离性，并防止了表示的坍缩。在多种任务上的广泛实验表明，PMRL相比基线方法具有优越性。源代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning seeks to create a unified representationspace by integrating diverse data modalities to improve multimodalunderstanding. Traditional methods often depend on pairwise contrastivelearning, which relies on a predefined anchor modality, restricting alignmentacross all modalities. Recent advances have investigated the simultaneousalignment of multiple modalities, yet several challenges remain, such aslimitations imposed by fixed anchor points and instability arising fromoptimizing the product of singular values. To address the challenges, in thispaper, we propose Principled Multimodal Representation Learning (PMRL), a novelframework that achieves simultaneous alignment of multiple modalities withoutanchor dependency in a more stable manner. Specifically, grounded in thetheoretical insight that full alignment corresponds to a rank-1 Gram matrix,PMRL optimizes the dominant singular value of the representation matrix toalign modalities along a shared leading direction. We propose a softmax-basedloss function that treats singular values as logits to prioritize the largestsingular value. Besides, instance-wise contrastive regularization on theleading eigenvectors maintains inter-instance separability and preventsrepresentation collapse. Extensive experiments across diverse tasks demonstratePMRL's superiority compared to baseline methods. The source code will bepublicly available.</description>
      <author>example@mail.com (Xiaohao Liu, Xiaobo Xia, See-Kiong Ng, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2507.17343v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations</title>
      <link>http://arxiv.org/abs/2507.17165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 41st IEEE International Conference on Software  Maintenance and Evolution 2025 (ICSME'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文评估了六种大型语言模型（LLMs）在生成GitHub Actions配置方面的能力，并分析了LLMs在CI配置生成中的局限性。&lt;h4&gt;背景&lt;/h4&gt;虽然CI服务如GitHub Actions越来越受欢迎，但开发者需要编写YAML配置，这既繁琐又容易出错。同时，LLMs在软件工程任务自动化中的应用逐渐增加，但其在生成CI配置方面的能力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;评估六种LLMs从自然语言描述中生成GitHub Actions配置的能力。&lt;h4&gt;方法&lt;/h4&gt;评估了三种通用基础模型（GPT-4o、Llama和Gemma）和三种代码预训练模型（GPT-4.1、CodeLlama和CodeGemma）。同时，引入了首个此类标签数据集，该数据集由GitHub Actions文档构建，将描述与相应的最佳实践YAML配置配对。&lt;h4&gt;主要发现&lt;/h4&gt;零样本提示达到与真实值的69%相似度，仅有3%完全匹配。代码预训练模型在基于YAML的CI任务中略逊于通用模型，揭示了LLMs在CI配置生成方面的局限性。分析GPT-4o的输出揭示了步骤缺失或重命名、描述误解和添加不必要的步骤等问题，这可能会影响结构和上下文正确性，表明生成质量和可执行CI配置所需的精度之间存在差距。&lt;h4&gt;结论&lt;/h4&gt;研究为改进LLMs与配置语言的匹配度以及指导CI自动化和工具支持的未来努力提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuous Integration (CI) services, such as GitHub Actions, requiredevelopers to write YAML-based configurations, which can be tedious anderror-prone. Despite the increasing use of Large Language Models (LLMs) toautomate software engineering tasks, their ability to generate CIconfigurations remains underexplored. This paper presents a preliminary studyevaluating six LLMs for generating GitHub Actions configurations from naturallanguage descriptions. We assess three general-purpose foundation models(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, CodeLlama, and CodeGemma). We also introduce the first labeled dataset of its kind,constructed from GitHub Actions documentation, pairing descriptions withcorresponding best-practice YAML configurations. Zero-shot prompting achievesup to 69% similarity with the ground truth, with only 3% perfect matches.Code-pretrained models slightly underperform compared to general-purpose onesin YAML-based CI tasks, revealing LLM limitations for CI configurationgeneration. Analyzing GPT-4o outputs reveals issues like missing or renamedsteps, misinterpreted descriptions, and unnecessary additions that may affectstructural and contextual correctness, indicating a gap between generationquality and the precision required for executable CI configurations. Ourresearch offers insights for improving LLM alignment with configurationlanguages and guiding future efforts on CI automation and tooling support.</description>
      <author>example@mail.com (Taher A. Ghaleb, Dulina Rathnayake)</author>
      <guid isPermaLink="false">2507.17165v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations</title>
      <link>http://arxiv.org/abs/2507.17323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EndoFinder的在线息肉检索框架，用于解释性和可扩展的大肠癌诊断。&lt;h4&gt;背景&lt;/h4&gt;大肠癌是癌症相关死亡的主要原因，及时检测和诊断息肉至关重要。深度学习模型在光学辅助诊断方面有所改进，但通常需要大量标记数据集，并产生难以解释的“黑盒”输出。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够进行解释性和可扩展的大肠癌诊断的在线息肉检索框架。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了一个息肉感知图像编码器，结合对比学习和重建任务，由息肉分割掩码引导。2. 将每个息肉视为一个三维“场景”，并引入了一个场景表示转换器，将息肉的多个视图融合成一个单一的潜在表示。3. 通过哈希层对这种表示进行离散化，EndoFinder能够从历史息肉病例数据库中实现实时检索。&lt;h4&gt;主要发现&lt;/h4&gt;EndoFinder在准确率上优于现有方法，同时为临床决策提供了基于检索的透明见解。&lt;h4&gt;结论&lt;/h4&gt;EndoFinder通过贡献一个新颖的数据集和一个可扩展、可解释的框架，解决了息肉诊断中的关键挑战，并为更高效的AI驱动结肠镜检查工作流程提供了有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：结直肠癌（CRC）仍然是癌症相关死亡的主要原因，这强调了及时检测和诊断息肉的重要性。虽然深度学习模型已经改进了光学辅助诊断，但它们通常需要大量的标记数据集，并产生难以解释的“黑盒”输出。在本文中，我们提出了一种名为EndoFinder的在线息肉检索框架，该框架利用多视图场景表示进行可解释和可扩展的大肠癌诊断。首先，我们开发了一个息肉感知图像编码器，通过结合对比学习和重建任务，由息肉分割掩码引导。这种自监督方法捕获了鲁棒特征，而不依赖于大规模标注数据。接下来，我们将每个息肉视为一个三维“场景”，并引入了一个场景表示转换器，将息肉的多个视图融合成一个单一的潜在表示。通过哈希层对这种表示进行离散化，EndoFinder能够从历史息肉病例数据库中实现实时检索，其中诊断信息作为新查询的可解释参考。我们在公共和新建的息肉数据集上对EndoFinder进行了再识别和病理分类的评估。结果表明，EndoFinder在准确率上优于现有方法，同时为临床决策提供了基于检索的透明见解。通过贡献一个新颖的数据集和一个可扩展、可解释的框架，我们的工作解决了息肉诊断中的关键挑战，并为更高效的AI驱动结肠镜检查工作流程提供了有希望的方向。源代码可在https://github.com/ku262/EndoFinder-Scene上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,underscoring the importance of timely polyp detection and diagnosis. While deeplearning models have improved optical-assisted diagnostics, they often demandextensive labeled datasets and yield "black-box" outputs with limitedinterpretability. In this paper, we propose EndoFinder, an online polypretrieval framework that leverages multi-view scene representations forexplainable and scalable CRC diagnosis. First, we develop a Polyp-aware ImageEncoder by combining contrastive learning and a reconstruction task, guided bypolyp segmentation masks. This self-supervised approach captures robustfeatures without relying on large-scale annotated data. Next, we treat eachpolyp as a three-dimensional "scene" and introduce a Scene RepresentationTransformer, which fuses multiple views of the polyp into a single latentrepresentation. By discretizing this representation through a hashing layer,EndoFinder enables real-time retrieval from a compiled database of historicalpolyp cases, where diagnostic information serves as interpretable referencesfor new queries. We evaluate EndoFinder on both public and newly collectedpolyp datasets for re-identification and pathology classification. Results showthat EndoFinder outperforms existing methods in accuracy while providingtransparent, retrieval-based insights for clinical decision-making. Bycontributing a novel dataset and a scalable, explainable framework, our workaddresses key challenges in polyp diagnosis and offers a promising directionfor more efficient AI-driven colonoscopy workflows. The source code isavailable at https://github.com/ku262/EndoFinder-Scene.</description>
      <author>example@mail.com (Ruijie Yang, Yan Zhu, Peiyao Fu, Yizhe Zhang, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang)</author>
      <guid isPermaLink="false">2507.17323v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension</title>
      <link>http://arxiv.org/abs/2507.16877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架ReMeREC，用于在图像中根据自然语言描述定位多个实体，并建模它们之间的关系。&lt;h4&gt;背景&lt;/h4&gt;现有方法在处理单实体定位时效果较好，但在处理多实体场景中的复杂实体关系时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在处理多实体场景中复杂关系和缺乏高质量标注数据集的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 构建了关系感知的多实体REC数据集ReMeX，包含详细的关系和文本标注。2. 提出ReMeREC框架，联合利用视觉和文本线索定位多个实体。3. 引入文本自适应多实体感知器TMP，动态推断实体的数量和范围。4. 提出实体间关系推理器EIR，增强关系推理和全局场景理解。5. 构建了小规模辅助数据集EntityText，用于提高对细粒度提示的语言理解。&lt;h4&gt;主要发现&lt;/h4&gt;ReMeREC在多实体接地和关系预测方面实现了最先进的性能，显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;ReMeREC框架能够有效提高多实体场景中的REC性能，对未来的研究具有指导意义。&lt;h4&gt;翻译&lt;/h4&gt;Referring Expression Comprehension (REC) aims to localize specified entities or regions in an image based on natural language descriptions. While existing methods handle single-entity localization, they often ignore complex inter-entity relationships in multi-entity scenes, limiting their accuracy and reliability. Additionally, the lack of high-quality datasets with fine-grained, paired image-text-relation annotations hinders further progress. To address this challenge, we first construct a relation-aware, multi-entity REC dataset called ReMeX, which includes detailed relationship and textual annotations. We then propose ReMeREC, a novel framework that jointly leverages visual and textual cues to localize multiple entities while modeling their inter-relations. To address the semantic ambiguity caused by implicit entity boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron (TMP), which dynamically infers both the quantity and span of entities from fine-grained textual cues, producing distinctive representations. Additionally, our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and global scene understanding. To further improve language comprehension for fine-grained prompts, we also construct a small-scale auxiliary dataset, EntityText, generated using large language models. Experiments on four benchmark datasets show that ReMeREC achieves state-of-the-art performance in multi-entity grounding and relation prediction, outperforming existing approaches by a large margin.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring Expression Comprehension (REC) aims to localize specified entitiesor regions in an image based on natural language descriptions. While existingmethods handle single-entity localization, they often ignore complexinter-entity relationships in multi-entity scenes, limiting their accuracy andreliability. Additionally, the lack of high-quality datasets with fine-grained,paired image-text-relation annotations hinders further progress. To addressthis challenge, we first construct a relation-aware, multi-entity REC datasetcalled ReMeX, which includes detailed relationship and textual annotations. Wethen propose ReMeREC, a novel framework that jointly leverages visual andtextual cues to localize multiple entities while modeling theirinter-relations. To address the semantic ambiguity caused by implicit entityboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron(TMP), which dynamically infers both the quantity and span of entities fromfine-grained textual cues, producing distinctive representations. Additionally,our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning andglobal scene understanding. To further improve language comprehension forfine-grained prompts, we also construct a small-scale auxiliary dataset,EntityText, generated using large language models. Experiments on fourbenchmark datasets show that ReMeREC achieves state-of-the-art performance inmulti-entity grounding and relation prediction, outperforming existingapproaches by a large margin.</description>
      <author>example@mail.com (Yizhi Hu, Zezhao Tian, Xingqun Qi, Chen Su, Bingkun Yang, Junhui Yin, Muyi Sun, Man Zhang, Zhenan Sun)</author>
      <guid isPermaLink="false">2507.16877v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study</title>
      <link>http://arxiv.org/abs/2507.17118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了人工智能（AI）在自动驾驶系统（ADS）和机器人等安全关键领域的应用，分析了最近自主系统趋向于端到端（E2E）的单一架构，如大型语言模型（LLMs）和视觉语言模型（VLMs）。文章回顾了不同的架构解决方案，评估了常见的安全分析方法如故障模式和影响分析（FMEA）和故障树分析（FTA）的有效性，并展示了如何改进这些技术以适应基础模型复杂性的特点。此外，介绍了HySAFE-AI，这是一个混合框架，用于评估AI系统的安全性，并提出了未来工作方向和AI安全标准的建议。&lt;h4&gt;背景&lt;/h4&gt;AI已成为自动驾驶系统和机器人等安全关键领域的核心部分，这些系统的架构趋向于采用端到端（E2E）的单一架构，如大型语言模型（LLMs）和视觉语言模型（VLMs）。&lt;h4&gt;目的&lt;/h4&gt;评估AI系统的安全性，并提出改进现有安全分析技术的方案。&lt;h4&gt;方法&lt;/h4&gt;回顾不同的架构解决方案，评估故障模式和影响分析（FMEA）和故障树分析（FTA）的有效性，并介绍HySAFE-AI混合框架。&lt;h4&gt;主要发现&lt;/h4&gt;这些技术可以改进以适应基础模型的复杂性，尤其是它们如何形成和使用潜在表示。&lt;h4&gt;结论&lt;/h4&gt;HySAFE-AI是一个混合框架，可以用于评估AI系统的安全性，并提出了未来AI安全标准的建议。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了人工智能在自动驾驶系统（ADS）和机器人等安全关键领域的应用，讨论了最近自主系统趋向于端到端（E2E）的单一架构，如大型语言模型（LLMs）和视觉语言模型（VLMs）。在本文中，我们回顾了不同的架构解决方案，评估了常见的安全分析方法，如故障模式和影响分析（FMEA）和故障树分析（FTA）的有效性。我们展示了如何改进这些技术以适应基础模型的复杂特性，尤其是它们如何形成和使用潜在表示。我们介绍了HySAFE-AI，一个用于评估AI系统安全性的混合框架。最后，我们提出了未来工作的暗示和建议，以指导未来AI安全标准的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI has become integral to safety-critical areas like autonomous drivingsystems (ADS) and robotics. The architecture of recent autonomous systems aretrending toward end-to-end (E2E) monolithic architectures such as largelanguage models (LLMs) and vision language models (VLMs). In this paper, wereview different architectural solutions and then evaluate the efficacy ofcommon safety analyses such as failure modes and effect analysis (FMEA) andfault tree analysis (FTA). We show how these techniques can be improved for theintricate nature of the foundational models, particularly in how they form andutilize latent representations. We introduce HySAFE-AI, Hybrid SafetyArchitectural Analysis Framework for AI Systems, a hybrid framework that adaptstraditional methods to evaluate the safety of AI systems. Lastly, we offerhints of future work and suggestions to guide the evolution of future AI safetystandards.</description>
      <author>example@mail.com (Mandar Pitale, Jelena Frtunikj, Abhinaw Priyadershi, Vasu Singh, Maria Spence)</author>
      <guid isPermaLink="false">2507.17118v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Spikes</title>
      <link>http://arxiv.org/abs/2507.17224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为HuiduRep的鲁棒自监督表示学习框架，用于从细胞外神经元的尖峰波形中提取有判别性和通用性的特征，并展示了其在尖峰排序和细胞外记录处理中的潜力。&lt;h4&gt;背景&lt;/h4&gt;细胞外记录是神经科学中解码大脑活动的基础，尖峰排序是关键步骤，但在低信噪比、电极漂移和跨会话变化的情况下具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出HuiduRep框架，以实现从细胞外尖峰波形中提取鲁棒特征，并开发一个无监督的尖峰排序流程。&lt;h4&gt;方法&lt;/h4&gt;结合对比学习和降噪自编码器，HuiduRep学习对噪声和漂移鲁棒的潜在表示，并在此基础上开发了一个尖峰排序流程。&lt;h4&gt;主要发现&lt;/h4&gt;HuiduRep在混合和真实世界数据集上表现出了强大的鲁棒性，其排序流程的性能与KiloSort4和MountainSort5等最先进的工具相当或更优。&lt;h4&gt;结论&lt;/h4&gt;自监督尖峰表示学习在处理细胞外记录方面具有成为鲁棒和通用工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extracellular recordings are brief voltage fluctuations recorded nearneurons, widely used in neuroscience as the basis for decoding brain activityat single-neuron resolution. Spike sorting, which assigns each spike to itssource neuron, is a critical step in brain sensing pipelines. However, itremains challenging under low signal-to-noise ratio (SNR), electrode drift, andcross-session variability. In this paper, we propose HuiduRep, a robustself-supervised representation learning framework that extracts discriminativeand generalizable features from extracellular spike waveforms. By combiningcontrastive learning with a denoising autoencoder, HuiduRep learns latentrepresentations that are robust to noise and drift. Built on HuiduRep, wedevelop a spike sorting pipeline that clusters spike representations withoutsupervision. Experiments on hybrid and real-world datasets demonstrate thatHuiduRep achieves strong robustness and the pipeline matches or outperformsstate-of-the-art tools such as KiloSort4 and MountainSort5. These findingsdemonstrate the potential of self-supervised spike representation learning as afoundational tool for robust and generalizable processing of extracellularrecordings.</description>
      <author>example@mail.com (Feng Cao, Zishuo Feng)</author>
      <guid isPermaLink="false">2507.17224v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation</title>
      <link>http://arxiv.org/abs/2507.17066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Agentic &amp; GenAI Evaluation KDD2025, poster presentation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用大型预训练模型生成合成表格数据在低数据环境下的隐私风险，并通过基准测试和实验评估了不同模型的性能。&lt;h4&gt;背景&lt;/h4&gt;合成表格数据对于机器学习工作流程至关重要，尤其是在数据集小或不平衡以及需要隐私保护的数据共享时。然而，目前最先进的生成模型需要大量数据。&lt;h4&gt;目的&lt;/h4&gt;评估使用大型预训练模型生成合成表格数据的隐私风险，并提出改进方法。&lt;h4&gt;方法&lt;/h4&gt;对三个基础模型（GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2）与四个基线在35个来自健康、金融和政策领域的真实世界表格上进行了基准测试，评估了统计保真度、下游效用和成员推理泄露。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型具有最高的隐私风险，其中LLaMA 3.3 70B在1% FPR下的真阳性率比最安全的基线高出54个百分点。通过实验发现，三种零成本的提示调整（小批量大小、低温度、使用摘要统计）可以降低最坏情况下的AUC值14点，并减少罕见类别泄露高达39点，同时保持超过90%的保真度。&lt;h4&gt;结论&lt;/h4&gt;本文提供了一个实际指南，用于使用基础模型进行更安全的低数据合成，并提出了降低隐私风险的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic tabular data is essential for machine learning workflows,especially for expanding small or imbalanced datasets and enablingprivacy-preserving data sharing. However, state-of-the-art generative models(GANs, VAEs, diffusion models) rely on large datasets with thousands ofexamples. In low-data settings, often the primary motivation for syntheticdata, these models can overfit, leak sensitive records, and require frequentretraining. Recent work uses large pre-trained transformers to generate rowsvia in-context learning (ICL), which needs only a few seed examples and noparameter updates, avoiding retraining. But ICL repeats seed rows verbatim,introducing a new privacy risk that has only been studied in text. The severityof this risk in tabular synthesis-where a single row may identify aperson-remains unclear. We address this gap with the first benchmark of threefoundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against fourbaselines on 35 real-world tables from health, finance, and policy. We evaluatestatistical fidelity, downstream utility, and membership inference leakage.Results show foundation models consistently have the highest privacy risk.LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highlyvulnerable. We plot the privacy-utility frontier and show that CTGAN andGPT-4o-mini offer better tradeoffs. A factorial study finds that threezero-cost prompt tweaks-small batch size, low temperature, and using summarystatistics-can reduce worst-case AUC by 14 points and rare-class leakage by upto 39 points while maintaining over 90% fidelity. Our benchmark offers apractical guide for safer low-data synthesis with foundation models.</description>
      <author>example@mail.com (Jessup Byun, Xiaofeng Lin, Joshua Ward, Guang Cheng)</author>
      <guid isPermaLink="false">2507.17066v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>StreamME: Simplify 3D Gaussian Avatar within Live Stream</title>
      <link>http://arxiv.org/abs/2507.17029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 15 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamME是一种专注于快速3D角色重建的方法，能够从实时视频流中同步记录和重建头部角色形象，无需预存数据，并能将重建的外观无缝集成到下游应用中。&lt;h4&gt;背景&lt;/h4&gt;传统方法依赖于预缓存数据和复杂的机器学习模型，导致重建速度慢且效率低下。&lt;h4&gt;目的&lt;/h4&gt;提高3D角色重建的速度和效率，实现实时重建和隐私保护。&lt;h4&gt;方法&lt;/h4&gt;StreamME基于3D高斯分层（3DGS）技术，通过仅依赖几何信息来消除可变形3DGS中对MLP的依赖，并引入基于主点的简化策略，优化点云分布以提高渲染效率。&lt;h4&gt;主要发现&lt;/h4&gt;StreamME实现了快速重建，通过优化点云分布和简化训练策略，显著提高了适应面部表情的速度，同时保护了面部隐私并减少了VR系统或在线会议中的通信带宽。&lt;h4&gt;结论&lt;/h4&gt;StreamME可以直接应用于动画、卡通化、重光照等下游应用，提供了快速、高效、隐私保护的3D角色重建解决方案。&lt;h4&gt;翻译&lt;/h4&gt;We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live videostreams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose StreamME, a method focuses on fast 3D avatar reconstruction. TheStreamME synchronously records and reconstructs a head avatar from live videostreams without any pre-cached data, enabling seamless integration of thereconstructed appearance into downstream applications. This exceptionally fasttraining strategy, which we refer to as on-the-fly training, is central to ourapproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminatingthe reliance on MLPs in deformable 3DGS and relying solely on geometry, whichsignificantly improves the adaptation speed to facial expression. To furtherensure high efficiency in on-the-fly training, we introduced a simplificationstrategy based on primary points, which distributes the point clouds moresparsely across the facial surface, optimizing points number while maintainingrendering quality. Leveraging the on-the-fly training capabilities, our methodprotects the facial privacy and reduces communication bandwidth in VR system oronline conference. Additionally, it can be directly applied to downstreamapplication such as animation, toonify, and relighting. Please refer to ourproject page for more details: https://songluchuan.github.io/StreamME/.</description>
      <author>example@mail.com (Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu)</author>
      <guid isPermaLink="false">2507.17029v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning</title>
      <link>http://arxiv.org/abs/2507.16802v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Agentar-Fin-R1系列金融大语言模型，这些模型基于Qwen3基础模型，旨在提升金融应用中的推理能力、可靠性和领域专业化。&lt;h4&gt;背景&lt;/h4&gt;现有的金融大语言模型在需要复杂推理、严格可靠性和适应特定领域要求的情况下存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提升金融大语言模型在金融应用中的推理能力、可靠性和领域专业化。&lt;h4&gt;方法&lt;/h4&gt;采用高质量、系统的金融任务标签系统与多层次的可靠性保证框架，包括高质量的可信知识工程、多代理可信数据合成和严格的数据验证治理。通过标签引导的自动化难度感知优化、两阶段训练流程和动态属性系统，提高了训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;Agentar-Fin-R1在主流金融基准测试（如Fineva、FinEval和FinanceIQ）以及通用推理数据集（如MATH-500和GPQA-diamond）上表现出色。创新性地提出了Finova评估基准，专注于代理级别的金融推理和合规性验证。&lt;h4&gt;结论&lt;/h4&gt;Agentar-Fin-R1不仅在金融任务上达到最先进的性能，还展现出卓越的通用推理能力，验证了其在高风险金融应用中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, two-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) exhibit considerable promise in financialapplications; however, prevailing models frequently demonstrate limitationswhen confronted with scenarios that necessitate sophisticated reasoningcapabilities, stringent trustworthiness criteria, and efficient adaptation todomain-specific requirements. We introduce the Agentar-Fin-R1 series offinancial large language models (8B and 32B parameters), specificallyengineered based on the Qwen3 foundation model to enhance reasoningcapabilities, reliability, and domain specialization for financialapplications. Our optimization approach integrates a high-quality, systematicfinancial task label system with a comprehensive multi-layered trustworthinessassurance framework. This framework encompasses high-quality trustworthyknowledge engineering, multi-agent trustworthy data synthesis, and rigorousdata validation governance. Through label-guided automated difficulty-awareoptimization, tow-stage training pipeline, and dynamic attribution systems, weachieve substantial improvements in training efficiency. Our models undergocomprehensive evaluation on mainstream financial benchmarks including Fineva,FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500and GPQA-diamond. To thoroughly assess real-world deployment capabilities, weinnovatively propose the Finova evaluation benchmark, which focuses onagent-level financial reasoning and compliance verification. Experimentalresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-artperformance on financial tasks but also exhibits exceptional general reasoningcapabilities, validating its effectiveness as a trustworthy solution forhigh-stakes financial applications. The Finova bench is available athttps://github.com/antgroup/Finova.</description>
      <author>example@mail.com (Yanjun Zheng, Xiyang Du, Longfei Liao, Xiaoke Zhao, Zhaowen Zhou, Bo Zhang, Jiawei Liu, Xiang Qi, Zhe Li, Zhiqiang Zhang, Wei Wang, Peng Zhang)</author>
      <guid isPermaLink="false">2507.16802v2</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation</title>
      <link>http://arxiv.org/abs/2507.17001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的模型自适应方法，该方法在推理过程中战略性地利用偏差来补充不变性表示，以应对分布外域的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有方法大多依赖于不变性表示学习来消除偏差特征的影响，但作者提出质疑：偏差是否应该总是被消除，以及何时应该保留偏差并如何利用它。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，本文首先进行了理论分析，探讨了在什么条件下可以识别和有效利用偏差特征，然后提出了一种新的框架。&lt;h4&gt;方法&lt;/h4&gt;该框架包括两个关键组件：(1) 使用不变性作为指导来从偏差中提取预测因素；(2) 利用识别出的偏差来估计环境条件，然后使用它来探索适当的偏差感知预测器以缓解环境差距。&lt;h4&gt;主要发现&lt;/h4&gt;通过在合成数据集和标准领域泛化基准上的实验验证了该方法的有效性，结果表明该方法优于现有方法，证明了其鲁棒性和适应性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法通过有效地利用偏差特征，在处理分布外域的数据时展现出优异的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing methods for adapting models to out-of-distribution (OOD)domains rely on invariant representation learning to eliminate the influence ofbiased features. However, should bias always be eliminated -- and if not, whenshould it be retained, and how can it be leveraged? To address these questions,we first present a theoretical analysis that explores the conditions underwhich biased features can be identified and effectively utilized. Building onthis theoretical foundation, we introduce a novel framework that strategicallyleverages bias to complement invariant representations during inference. Theframework comprises two key components that leverage bias in both direct andindirect ways: (1) using invariance as guidance to extract predictiveingredients from bias, and (2) exploiting identified bias to estimate theenvironmental condition and then use it to explore appropriate bias-awarepredictors to alleviate environment gaps. We validate our approach throughexperiments on both synthetic datasets and standard domain generalizationbenchmarks. Results consistently demonstrate that our method outperformsexisting approaches, underscoring its robustness and adaptability.</description>
      <author>example@mail.com (Yan Li, Guangyi Chen, Yunlong Deng, Zijian Li, Zeyu Tang, Anpeng Wu, Kun Zhang)</author>
      <guid isPermaLink="false">2507.17001v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Confidence Optimization for Probabilistic Encoding</title>
      <link>http://arxiv.org/abs/2507.16881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种置信度优化概率编码（CPE）方法，用于提高神经网络中概率编码的距离可靠性，增强表示学习，并显著提升自然语言分类任务中的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;概率编码通过引入高斯噪声到神经网络中，使网络从确定性状态平滑过渡到不确定性状态，从而增强泛化能力。然而，高斯噪声的随机性会扭曲分类任务中的基于点的距离测量。&lt;h4&gt;目的&lt;/h4&gt;提出CPE方法以减轻高斯噪声对距离测量的扭曲，提高距离可靠性，并增强表示学习。&lt;h4&gt;方法&lt;/h4&gt;CPE方法通过以下两个关键策略进行概率编码的优化：1. 引入一个置信度感知机制来调整距离计算，确保在概率编码分类任务中的一致性和可靠性；2. 用简单的L2正则化项替换传统的基于KL散度的方差正则化，以直接约束方差。&lt;h4&gt;主要发现&lt;/h4&gt;该方法对模型无依赖性，在BERT和RoBERTa模型上的自然语言分类任务中，CPE方法显著提高了性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CPE方法能够有效提升神经网络在自然语言处理任务中的性能和泛化能力，为概率编码提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Probabilistic encoding introduces Gaussian noise into neural networks, enabling a smooth transition from deterministic to uncertain states and enhancing generalization ability. However, the randomness of Gaussian noise distorts point-based distance measurements in classification tasks. To mitigate this issue, we propose a confidence optimization probabilistic encoding (CPE) method that improves distance reliability and enhances representation learning. Specifically, we refine probabilistic encoding with two key strategies: First, we introduce a confidence-aware mechanism to adjust distance calculations, ensuring consistency and reliability in probabilistic encoding classification tasks. Second, we replace the conventional KL divergence-based variance regularization, which relies on unreliable prior assumptions, with a simpler L2 regularization term to directly constrain variance. The method we proposed is model-agnostic, and extensive experiments on natural language classification tasks demonstrate that our method significantly improves performance and generalization on both the BERT and the RoBERTa model.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic encoding introduces Gaussian noise into neural networks,enabling a smooth transition from deterministic to uncertain states andenhancing generalization ability. However, the randomness of Gaussian noisedistorts point-based distance measurements in classification tasks. To mitigatethis issue, we propose a confidence optimization probabilistic encoding (CPE)method that improves distance reliability and enhances representation learning.Specifically, we refine probabilistic encoding with two key strategies: First,we introduce a confidence-aware mechanism to adjust distance calculations,ensuring consistency and reliability in probabilistic encoding classificationtasks. Second, we replace the conventional KL divergence-based varianceregularization, which relies on unreliable prior assumptions, with a simpler L2regularization term to directly constrain variance. The method we proposed ismodel-agnostic, and extensive experiments on natural language classificationtasks demonstrate that our method significantly improves performance andgeneralization on both the BERT and the RoBERTa model.</description>
      <author>example@mail.com (Pengjiu Xia, Yidian Huang, Wenchao Wei, Yuwen Tan)</author>
      <guid isPermaLink="false">2507.16881v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage</title>
      <link>http://arxiv.org/abs/2507.16872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CompLeak的隐私风险评估框架，用于评估深度学习模型压缩过程中产生的隐私风险。&lt;h4&gt;背景&lt;/h4&gt;模型压缩对于减少内存存储和加速深度学习模型（包括大型语言模型）的推理至关重要。然而，压缩过程中引入的隐私风险常被忽视。&lt;h4&gt;目的&lt;/h4&gt;通过成员推理攻击（MIA）的视角，提出CompLeak，旨在评估三种常用压缩配置（剪枝、量化和权重聚类）的隐私风险。&lt;h4&gt;方法&lt;/h4&gt;CompLeak有三个变体，根据压缩模型和原始模型的数量提供不同的评估方式。CompLeakNR通过攻击单个压缩模型来识别不同压缩模型对成员和非成员的影响。CompLeakSR利用压缩模型作为原始模型的信息，通过结合模型的元信息来揭示更多隐私。CompLeakMR利用多个压缩版本的隐私泄露信息来显著标识总体隐私泄露。&lt;h4&gt;主要发现&lt;/h4&gt;实验在七个不同的模型架构和六个图像及文本基准数据集上进行了，揭示了压缩模型对隐私泄露的影响。&lt;h4&gt;结论&lt;/h4&gt;CompLeak框架为评估深度学习模型压缩过程中的隐私风险提供了一个有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：模型压缩对于减少深度学习（DL）模型（包括最近的基础模型如大型语言模型（LLM））的内存存储和加速推理至关重要。用户可以根据其资源和预算访问不同的压缩模型版本。然而，现有的压缩操作主要关注优化资源效率和模型性能之间的权衡，而压缩过程中引入的隐私风险却被忽视且理解不足。在这项工作中，通过成员推理攻击（MIA）的视角，我们提出了CompLeak，这是第一个评估三个广泛使用的压缩配置（剪枝、量化和权重聚类）的隐私风险框架，这些配置由Google的TensorFlow-Lite（TF-Lite）和Facebook的PyTorch Mobile的商业模型压缩框架支持。CompLeak有三个变体，根据可用的压缩模型数量和原始模型数量。CompLeakNR首先采用现有的MIA方法攻击单个压缩模型，并发现不同的压缩模型对成员和非成员的影响不同。当原始模型和单个压缩模型可用时，CompLeakSR利用压缩模型作为原始模型的参考，并通过结合来自两个模型的元信息（例如，置信向量）来揭示更多隐私。当多个压缩模型（有/无访问原始模型）可用时，CompLeakMR创新性地利用多个压缩版本的隐私泄露信息，显著标识总体隐私泄露。我们在七个不同的模型架构（从ResNet到BERT和GPT-2的基础模型）和六个图像和文本基准数据集上进行了广泛的实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model compression is crucial for minimizing memory storage and acceleratinginference in deep learning (DL) models, including recent foundation models likelarge language models (LLMs). Users can access different compressed modelversions according to their resources and budget. However, while existingcompression operations primarily focus on optimizing the trade-off betweenresource efficiency and model performance, the privacy risks introduced bycompression remain overlooked and insufficiently understood.  In this work, through the lens of membership inference attack (MIA), wepropose CompLeak, the first privacy risk evaluation framework examining threewidely used compression configurations that are pruning, quantization, andweight clustering supported by the commercial model compression framework ofGoogle's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak hasthree variants, given available access to the number of compressed models andoriginal model. CompLeakNR starts by adopting existing MIA methods to attack asingle compressed model, and identifies that different compressed modelsinfluence members and non-members differently. When the original model and onecompressed model are available, CompLeakSR leverages the compressed model as areference to the original model and uncovers more privacy by combining metainformation (e.g., confidence vector) from both models. When multiplecompressed models are available with/without accessing the original model,CompLeakMR innovatively exploits privacy leakage info from multiple compressedversions to substantially signify the overall privacy leakage. We conductextensive experiments on seven diverse model architectures (from ResNet tofoundation models of BERT and GPT-2), and six image and textual benchmarkdatasets.</description>
      <author>example@mail.com (Na Li, Yansong Gao, Hongsheng Hu, Boyu Kuang, Anmin Fu)</author>
      <guid isPermaLink="false">2507.16872v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2507.16854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为CLAMP的多模态基于方面的情感分析框架，用于识别图像-文本数据中的方面术语及其细粒度情感极性。&lt;h4&gt;背景&lt;/h4&gt;现有方法在跨模态对齐噪声和细粒度表示的连贯性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个端到端的对比学习框架CLAMP，以解决现有方法的限制。&lt;h4&gt;方法&lt;/h4&gt;CLAMP框架包括三个创新模块：渐进式注意力融合网络、多任务对比学习和自适应多损失聚合。渐进式注意力融合网络通过分层、多阶段的跨模态交互增强了文本特征与图像区域之间的细粒度对齐，有效地抑制了无关视觉噪声。多任务对比学习结合了全局模态对比和局部粒度对齐以增强跨模态表示的连贯性。自适应多损失聚合使用基于动态不确定性的加权机制来校准损失贡献，根据每个任务的不确定性进行调整，从而减轻梯度干扰。&lt;h4&gt;主要发现&lt;/h4&gt;在标准公共基准上的评估表明，CLAMP在一致性上优于大多数现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;CLAMP框架有效地解决了多模态基于方面的情感分析中的挑战，并显著提高了情感分析的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspectterms within paired image-text data and determine their fine grained sentimentpolarities, representing a fundamental task for improving the effectiveness ofapplications such as product review systems and public opinion monitoring.Existing methods face challenges such as cross modal alignment noise andinsufficient consistency in fine-grained representations. While global modalityalignment methods often overlook the connection between aspect terms and theircorresponding local visual regions, bridging the representation gap betweentext and images remains a challenge. To address these limitations, this paperintroduces an end to end Contrastive Learning framework with AdaptiveMulti-loss and Progressive Attention Fusion(CLAMP). The framework is composedof three novel modules: Progressive Attention Fusion network, Multi-taskContrastive Learning, and Adaptive Multi-loss Aggregation. The ProgressiveAttention Fusion network enhances fine-grained alignment between textualfeatures and image regions via hierarchical, multi-stage cross modalinteractions, effectively suppressing irrelevant visual noise. Secondly,multi-task contrastive learning combines global modal contrast and localgranularity alignment to enhance cross modal representation consistency.Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weightingmechanism to calibrate loss contributions according to each task's uncertainty,thereby mitigating gradient interference. Evaluation on standard publicbenchmarks demonstrates that CLAMP consistently outperforms the vast majorityof existing state of the art methods.</description>
      <author>example@mail.com (Xiaoqiang He)</author>
      <guid isPermaLink="false">2507.16854v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>Controllable Video Generation: A Survey</title>
      <link>http://arxiv.org/abs/2507.16869v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  project page:  https://github.com/mayuelala/Awesome-Controllable-Video-Generation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了可控视频生成领域，包括理论基础和最新进展。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能生成内容（AIGC）的快速发展，视频生成已成为其最具活力和影响力的子领域之一。&lt;h4&gt;目的&lt;/h4&gt;为了更精确地反映用户意图，研究可控视频生成方法的需求日益增长。&lt;h4&gt;方法&lt;/h4&gt;通过整合非文本条件，如相机运动、深度图和人体姿态，扩展预训练的视频生成模型，实现更可控的视频合成。&lt;h4&gt;主要发现&lt;/h4&gt;现有的基础模型多针对文本到视频生成，而文本提示通常不足以表达复杂的、多模态的和精细的用户需求。&lt;h4&gt;结论&lt;/h4&gt;本文对可控视频生成进行了系统回顾，并基于所利用的控制信号类型对现有方法进行了分类。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着人工智能生成内容（AIGC）的快速发展，视频生成已成为其最具活力和影响力的子领域之一。特别是视频生成基础模型的进步，导致了对可控视频生成方法的需求增长，这些方法能够更精确地反映用户意图。大多数现有基础模型是为文本到视频生成而设计的，单独的文本提示通常不足以表达复杂的、多模态的和精细的用户需求。这种限制使得用户难以使用当前模型精确控制地生成视频。为了解决这个问题，最近的研究探索了将额外的非文本条件，如相机运动、深度图和人体姿态的整合，以扩展预训练的视频生成模型并实现更可控的视频合成。这些方法旨在提高AIGC驱动的视频生成系统的灵活性和实用性。在本调查中，我们对可控视频生成提供了系统的回顾，涵盖了该领域的理论基础和最新进展。我们首先介绍了关键概念和常用的开源视频生成模型。然后，我们关注视频扩散模型中的控制机制，分析了如何将不同类型的条件纳入去噪过程以引导生成。最后，我们根据所利用的控制信号类型对现有方法进行了分类，包括单条件生成、多条件生成和通用可控生成。有关可控视频生成文献的完整列表，请访问我们的整理仓库https://github.com/mayuelala/Awesome-Controllable-Video-Generation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of AI-generated content (AIGC), video generationhas emerged as one of its most dynamic and impactful subfields. In particular,the advancement of video generation foundation models has led to growing demandfor controllable video generation methods that can more accurately reflect userintent. Most existing foundation models are designed for text-to-videogeneration, where text prompts alone are often insufficient to express complex,multi-modal, and fine-grained user requirements. This limitation makes itchallenging for users to generate videos with precise control using currentmodels. To address this issue, recent research has explored the integration ofadditional non-textual conditions, such as camera motion, depth maps, and humanpose, to extend pretrained video generation models and enable more controllablevideo synthesis. These approaches aim to enhance the flexibility and practicalapplicability of AIGC-driven video generation systems. In this survey, weprovide a systematic review of controllable video generation, covering boththeoretical foundations and recent advances in the field. We begin byintroducing the key concepts and commonly used open-source video generationmodels. We then focus on control mechanisms in video diffusion models,analyzing how different types of conditions can be incorporated into thedenoising process to guide generation. Finally, we categorize existing methodsbased on the types of control signals they leverage, including single-conditiongeneration, multi-condition generation, and universal controllable generation.For a complete list of the literature on controllable video generationreviewed, please visit our curated repository athttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.</description>
      <author>example@mail.com (Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Wei Liu, Dan Xu, Linfeng Zhang, Qifeng Chen)</author>
      <guid isPermaLink="false">2507.16869v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion</title>
      <link>http://arxiv.org/abs/2506.15610v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://lanlan96.github.io/BoxFusion/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的无重建在线3D目标检测框架，旨在实现内存高效和实时检测。&lt;h4&gt;背景&lt;/h4&gt;Open-vocabulary 3D目标检测在自动驾驶和具身AI等领域有重要应用，但现有方法依赖密集点云重建，导致计算和内存开销大，难以实时部署。&lt;h4&gt;目的&lt;/h4&gt;提出一种内存高效和实时3D检测的方法。&lt;h4&gt;方法&lt;/h4&gt;使用Cubify Anything作为预训练的视觉基础模型进行单视图3D目标检测，结合CLIP捕获检测物体的开放词汇语义。通过关联模块进行多视图对应，优化模块融合多视图中预测的同一实例的3D边界框。关联模块使用3D非极大值抑制(NMS)和框对应匹配模块，优化模块使用基于粒子滤波的IoU引导高效随机优化技术，以最小化计算复杂度并确保多视图一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNetV2和CA-1M数据集上的实验表明，该方法在在线方法中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在多种场景中表现出良好的泛化能力，即使在超过1000平方米的环境中也能实现实时感知。&lt;h4&gt;翻译&lt;/h4&gt;The abstract is summarized as follows: Open-vocabulary 3D object detection has gained significant interest due to its critical applications in autonomous driving and embodied AI. Existing detection methods, whether offline or online, typically rely on dense point cloud reconstruction, which imposes substantial computational overhead and memory constraints, hindering real-time deployment in downstream tasks. To address this, we propose a novel reconstruction-free online framework tailored for memory-efficient and real-time 3D detection. Specifically, given streaming posed RGB-D video input, we leverage Cubify Anything as a pre-trained visual foundation model (VFM) for single-view 3D object detection by bounding boxes, coupled with CLIP to capture open-vocabulary semantics of detected objects. To fuse all detected bounding boxes across different views into a unified one, we employ an association module for correspondences of multi-views and an optimization module to fuse the 3D bounding boxes of the same instance predicted in multi-views. The association module utilizes 3D Non-Maximum Suppression (NMS) and a box correspondence matching module, while the optimization module uses an IoU-guided efficient random optimization technique based on particle filtering to enforce multi-view consistency of the 3D bounding boxes while minimizing computational complexity. Extensive experiments on ScanNetV2 and CA-1M datasets demonstrate that our method achieves state-of-the-art performance among online methods. Benefiting from this novel reconstruction-free paradigm for 3D object detection, our method exhibits great generalization abilities in various scenarios, enabling real-time perception even in environments exceeding 1000 square meters.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-06-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D object detection has gained significant interest due toits critical applications in autonomous driving and embodied AI. Existingdetection methods, whether offline or online, typically rely on dense pointcloud reconstruction, which imposes substantial computational overhead andmemory constraints, hindering real-time deployment in downstream tasks. Toaddress this, we propose a novel reconstruction-free online framework tailoredfor memory-efficient and real-time 3D detection. Specifically, given streamingposed RGB-D video input, we leverage Cubify Anything as a pre-trained visualfoundation model (VFM) for single-view 3D object detection by bounding boxes,coupled with CLIP to capture open-vocabulary semantics of detected objects. Tofuse all detected bounding boxes across different views into a unified one, weemploy an association module for correspondences of multi-views and anoptimization module to fuse the 3D bounding boxes of the same instancepredicted in multi-views. The association module utilizes 3D Non-MaximumSuppression (NMS) and a box correspondence matching module, while theoptimization module uses an IoU-guided efficient random optimization techniquebased on particle filtering to enforce multi-view consistency of the 3Dbounding boxes while minimizing computational complexity. Extensive experimentson ScanNetV2 and CA-1M datasets demonstrate that our method achievesstate-of-the-art performance among online methods. Benefiting from this novelreconstruction-free paradigm for 3D object detection, our method exhibits greatgeneralization abilities in various scenarios, enabling real-time perceptioneven in environments exceeding 1000 square meters.</description>
      <author>example@mail.com (Yuqing Lan, Chenyang Zhu, Zhirui Gao, Jiazhao Zhang, Yihan Cao, Renjiao Yi, Yijie Wang, Kai Xu)</author>
      <guid isPermaLink="false">2506.15610v2</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    <item>
      <title>CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples</title>
      <link>http://arxiv.org/abs/2507.16840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为CASPER的新颖对比学习方法，用于检测区块链交易中的智能庞氏骗局。该方法能够使用少量标注数据有效检测庞氏骗局。&lt;h4&gt;背景&lt;/h4&gt;数字货币交易的发展促进了区块链技术的整合，但也引发了创新和智能庞氏骗局的涌现。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效且成本效益高的方法来检测智能庞氏骗局。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于对比学习的框架CASPER，通过使用无标注数据集学习智能合同源代码的有效表示来提高庞氏骗局检测。&lt;h4&gt;主要发现&lt;/h4&gt;在XBlock数据集上，与基线模型相比，CASPER在使用100%标注数据时F1分数提高了2.3%。即使在只有25%标注数据的情况下，CASPER的F1分数也比基线高近20%。&lt;h4&gt;结论&lt;/h4&gt;CASPER在检测智能庞氏骗局方面具有潜力，为未来的可扩展欺诈检测解决方案铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;The rapid evolution of digital currency trading, fueled by the integration of blockchain technology, has led to both innovation and the emergence of smart Ponzi schemes. A smart Ponzi scheme is a fraudulent investment operation in smart contract that uses funds from new investors to pay returns to earlier investors. Traditional Ponzi scheme detection methods based on deep learning typically rely on fully supervised models, which require large amounts of labeled data. However, such data is often scarce, hindering effective model training. To address this challenge, we propose a novel contrastive learning framework, CASPER (Contrastive Approach for Smart Ponzi detectER with more negative samples), designed to enhance smart Ponzi scheme detection in blockchain transactions. By leveraging contrastive learning techniques, CASPER can learn more effective representations of smart contract source code using unlabeled datasets, significantly reducing both operational costs and system complexity. We evaluate CASPER on the XBlock dataset, where it outperforms the baseline by 2.3% in F1 score when trained with 100% labeled data. More impressively, with only 25% labeled data, CASPER achieves an F1 score nearly 20% higher than the baseline under identical experimental conditions. These results highlight CASPER's potential for effective and cost-efficient detection of smart Ponzi schemes, paving the way for scalable fraud detection solutions in the future.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid evolution of digital currency trading, fueled by the integration ofblockchain technology, has led to both innovation and the emergence of smartPonzi schemes. A smart Ponzi scheme is a fraudulent investment operation insmart contract that uses funds from new investors to pay returns to earlierinvestors. Traditional Ponzi scheme detection methods based on deep learningtypically rely on fully supervised models, which require large amounts oflabeled data. However, such data is often scarce, hindering effective modeltraining. To address this challenge, we propose a novel contrastive learningframework, CASPER (Contrastive Approach for Smart Ponzi detectER with morenegative samples), designed to enhance smart Ponzi scheme detection inblockchain transactions. By leveraging contrastive learning techniques, CASPERcan learn more effective representations of smart contract source code usingunlabeled datasets, significantly reducing both operational costs and systemcomplexity. We evaluate CASPER on the XBlock dataset, where it outperforms thebaseline by 2.3% in F1 score when trained with 100% labeled data. Moreimpressively, with only 25% labeled data, CASPER achieves an F1 score nearly20% higher than the baseline under identical experimental conditions. Theseresults highlight CASPER's potential for effective and cost-efficient detectionof smart Ponzi schemes, paving the way for scalable fraud detection solutionsin the future.</description>
      <author>example@mail.com (Weijia Yang, Tian Lan, Leyuan Liu, Wei Chen, Tianqing Zhu, Sheng Wen, Xiaosong Zhang)</author>
      <guid isPermaLink="false">2507.16840v1</guid>
      <pubDate>Thu, 24 Jul 2025 14:36:42 +0800</pubDate>
    </item>
    </channel>
</rss>