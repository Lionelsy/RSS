<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 03 Sep 2025 14:14:56 +0800</lastBuildDate>
    <item>
      <title>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings</title>
      <link>http://arxiv.org/abs/2508.18733v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Drawing2CAD框架，用于从2D工程图纸自动生成参数化CAD模型，解决了传统CAD生成方法与传统工业工作流程不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;CAD生成式建模正在推动工业应用的显著创新，最近的研究已展示从点云、网格和文本描述创建实体模型的进展，但这些方法与传统从2D工程图纸开始的工业工作流程存在根本差异。&lt;h4&gt;目的&lt;/h4&gt;解决从2D矢量图纸自动生成参数化CAD模型的这一未被充分探索的领域问题，尽管这是工程设计中的关键步骤。&lt;h4&gt;方法&lt;/h4&gt;提出Drawing2CAD框架，包含三个关键技术组件：网络友好的矢量原语表示、双解码器Transformer架构、以及适应CAD参数固有灵活性的软目标分布损失函数。将CAD生成重新定义为序列到序列的学习问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过创建CAD-VGDrawing数据集（包含成对的工程图纸和参数化CAD模型）并进行彻底实验，证明了Drawing2CAD方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;Drawing2CAD框架能够有效从2D工程图纸生成参数化CAD模型，保留了几何精度和设计意图。&lt;h4&gt;翻译&lt;/h4&gt;计算机辅助设计生成式建模正在推动工业应用的显著创新。最近的工作在从点云、网格和文本描述等各种输入创建实体模型方面显示出显著进展。然而，这些方法从根本上与传统从2D工程图纸开始的工业工作流程相偏离。尽管工程设计中的关键步骤，但从这些2D矢量图纸自动生成参数化CAD模型仍未得到充分探索。为解决这一差距，我们的关键见解是将CAD生成重新定义为序列到序列学习问题，其中矢量绘图原语直接指导参数化CAD操作的生成，在整个转换过程中保持几何精度和设计意图。我们提出了Drawing2CAD框架，包含三个关键技术组件：保留精确几何信息的网络友好型矢量原语表示，解耦命令类型和参数生成同时保持精确对应的双解码器Transformer架构，以及适应CAD参数固有灵活性的软目标分布损失函数。为了训练和评估Drawing2CAD，我们创建了CAD-VGDrawing数据集，包含成对的工程图纸和参数化CAD模型，并进行了彻底的实验以证明我们方法的有效性。代码和数据集可在https://github.com/lllssc/Drawing2CAD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从2D矢量工程图纸自动生成参数化CAD模型的问题。这个问题在现实和研究中很重要，因为工业设计工作流程通常从2D工程图纸开始，然后创建3D CAD模型，这一过程目前需要大量手动操作，耗时且需要专业知识。现有研究主要探索从点云、网格、文本描述等生成CAD模型，但忽略了从2D矢量工程图纸这一工业设计起点直接生成CAD模型的方法。自动化这一过程可以显著提高设计效率，减少人工干预，使设计工作流程更加流畅。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将CAD模型生成问题重新定义为序列到序列的学习问题，其中矢量绘图的基本元素直接指导参数化CAD操作的生成。他们选择使用矢量图形（SVG）而非光栅图像作为输入，因为SVG能精确编码几何信息，与工程设计意图保持一致。方法设计包括：网络友好的矢量表示、双解码器Transformer架构解耦命令类型和参数生成、软目标分布损失函数适应CAD参数的灵活性。作者借鉴了Transformer架构在序列建模中的成功应用、DeepSVG等在矢量图形生成方面的研究、DeepCAD等在CAD操作序列生成方面的方法，以及计算机视觉中多视图表示的学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将CAD模型生成视为序列到序列的转换问题，直接从2D矢量工程图纸生成参数化CAD操作序列，同时保持几何精度和设计意图。整体实现流程包括：1) 输入表示：将矢量工程图纸转换为网络友好的表示，保留精确几何信息；2) 编码阶段：使用Transformer编码器处理嵌入的矢量图纸表示，生成潜在向量；3) 双解码阶段：命令解码器生成CAD操作类型，参数解码器生成对应参数，通过命令引导确保参数与命令语义一致；4) 损失函数：使用复合损失函数，参数损失采用软目标分布允许微小变化；5) 输出处理：将生成的CAD操作序列通过CAD内核处理，构建最终3D模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 网络友好的矢量工程图纸表示方法，保留精确几何信息；2) 序列到序列学习框架，重新定义CAD生成为跨模态问题；3) 双解码器架构，解耦命令类型预测和参数估计；4) 软目标分布损失函数，适应CAD参数的灵活性；5) CAD-VGDrawing数据集，提供大规模基准数据。相比之前工作的不同：使用矢量图纸而非光栅图像作为输入；实现从2D到3D的直接转换而非通过中间表示；双解码器架构解耦命令和参数生成；软目标分布损失函数；更贴近工业设计工作流程的应用场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Drawing2CAD首次提出了从2D矢量工程图纸直接生成参数化CAD模型的序列到序列学习框架，通过创新的网络友好表示、双解码器架构和软目标分布损失函数，实现了高效且保持设计意图的自动化CAD建模。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-Aided Design (CAD) generative modeling is driving significantinnovations across industrial applications. Recent works have shown remarkableprogress in creating solid models from various inputs such as point clouds,meshes, and text descriptions. However, these methods fundamentally divergefrom traditional industrial workflows that begin with 2D engineering drawings.The automatic generation of parametric CAD models from these 2D vector drawingsremains underexplored despite being a critical step in engineering design. Toaddress this gap, our key insight is to reframe CAD generation as asequence-to-sequence learning problem where vector drawing primitives directlyinform the generation of parametric CAD operations, preserving geometricprecision and design intent throughout the transformation process. We proposeDrawing2CAD, a framework with three key technical components: anetwork-friendly vector primitive representation that preserves precisegeometric information, a dual-decoder transformer architecture that decouplescommand type and parameter generation while maintaining precise correspondence,and a soft target distribution loss function accommodating inherent flexibilityin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,a dataset of paired engineering drawings and parametric CAD models, and conductthorough experiments to demonstrate the effectiveness of our method. Code anddataset are available at https://github.com/lllssc/Drawing2CAD.</description>
      <author>example@mail.com (Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu)</author>
      <guid isPermaLink="false">2508.18733v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
  <item>
      <title>Investigating Domain Gaps for Indoor 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.17439v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了室内3D物体检测的域适应问题，提出了一个全面的基准测试，分析了不同域差距对检测器的影响，并提供了提高适应性能的方法。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测作为室内场景理解的基础任务已有广泛研究，在室内点云数据上的准确性显著提高。然而，现有研究仅在有限数据集上进行，训练和测试集共享相同分布。&lt;h4&gt;目的&lt;/h4&gt;考虑将室内3D物体检测器从一个数据集适应到另一个数据集的任务，提供域自适应室内3D物体检测的基线。&lt;h4&gt;方法&lt;/h4&gt;提出了一个全面的基准测试，使用ScanNet、SUN RGB-D和3D Front数据集，以及通过3D模拟器生成的新的大规模数据集ProcTHOR-OD和ProcFront。分析了不同域差距对3D物体检测器的影响，并提出了几种提高适应性能的方法。&lt;h4&gt;主要发现&lt;/h4&gt;由于室内点云数据集以不同方式收集和构建，物体检测器可能过度拟合到每个数据集内的特定因素，如点云质量、边界框布局和实例特征。&lt;h4&gt;结论&lt;/h4&gt;提供了域自适应室内3D物体检测的基线，希望未来的工作能提出跨域泛化能力更强的检测器。&lt;h4&gt;翻译&lt;/h4&gt;作为室内场景理解的基本任务，3D物体检测已被广泛研究，并且在室内点云数据上的准确性已显著提高。然而，现有研究已在有限数据集上进行，其中训练和测试集共享相同的分布。在本文中，我们考虑将室内3D物体检测器从一个数据集适应到另一个数据集的任务，提出了一个包含ScanNet、SUN RGB-D和3D Front数据集的综合基准，以及我们通过3D模拟器新提出的大规模数据集ProcTHOR-OD和ProcFront。由于室内点云数据集以不同方式收集和构建，物体检测器可能过度拟合到每个数据集内的特定因素，如点云质量、边界框布局和实例特征。我们在不同适应场景（包括合成到真实适应、点云质量适应、布局适应和实例特征适应）中进行了跨数据集实验，分析了不同域差距对3D物体检测器的影响。我们还引入了多种方法来提高适应性能，为域自适应室内3D物体检测提供了基线，希望未来的工作能提出跨域泛化能力更强的检测器。我们的项目主页可以在https://jeremyzhao1998.github.io/DAVoteNet-release/找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决室内3D物体检测中的域差距问题。现有研究大多在单一数据集上进行，训练和测试数据分布相同，但实际应用中检测器需要适应不同环境。这个问题很重要，因为不同数据集有不同采集方式和质量，导致检测器跨域性能显著下降（论文显示mAP从46.3降至18.9），且室内3D检测面临物体类别多、间距近等独特挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了室内3D检测中多种域差距因素（合成到现实、点云质量、布局、实例），然后创建两个新数据集（ProcTHOR-OD和ProcFront）来隔离这些因素。他们结合现有数据集构建了全面的基准测试套件，并实现了多种域适应方法作为基线。作者借鉴了ProcTHOR框架用于数据生成，使用VoteNet作为基础检测器，并应用了已有的域适应技术如虚拟扫描模拟、平均教师等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是系统性地分析和量化室内3D物体检测中的不同域差距因素，创建可控的大规模合成数据集，建立全面的基准测试套件，并提供多种域适应方法的基线。整体流程包括：1)构建数据集（现有+新创建）；2)分析域差距因素；3)设置实验环境（使用VoteNet，每场景4万点，训练90周期）；4)评估不同域适应方法（包括目标域先验和无监督域适应技术）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建两个新数据集（ProcTHOR-OD和ProcFront），规模比现有数据集大一个数量级；2)构建覆盖四种域适应场景的全面基准测试套件；3)系统性地分析多种域差距因素；4)提供多种域适应方法的基线结果。相比之前工作，本文考虑了更广泛的域差距因素（而不仅是合成到现实），创建了更大规模、更可控的数据集，提供了更全面的基准测试和更系统的分析，并关注室内3D检测特有的挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建新数据集和构建全面的基准测试套件，首次系统性地分析了室内3D物体检测中的域差距因素，并提供了多种域适应方法的基线结果，为未来提升检测器跨域能力奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758275&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a fundamental task for indoor scene understanding, 3D object detection hasbeen extensively studied, and the accuracy on indoor point cloud data has beensubstantially improved. However, existing researches have been conducted onlimited datasets, where the training and testing sets share the samedistribution. In this paper, we consider the task of adapting indoor 3D objectdetectors from one dataset to another, presenting a comprehensive benchmarkwith ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposedlarge-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator.Since indoor point cloud datasets are collected and constructed in differentways, the object detectors are likely to overfit to specific factors withineach dataset, such as point cloud quality, bounding box layout and instancefeatures. We conduct experiments across datasets on different adaptationscenarios including synthetic-to-real adaptation, point cloud qualityadaptation, layout adaptation and instance feature adaptation, analyzing theimpact of different domain gaps on 3D object detectors. We also introduceseveral approaches to improve adaptation performances, providing baselines fordomain adaptive indoor 3D object detection, hoping that future works maypropose detectors with stronger generalization ability across domains. Ourproject homepage can be found inhttps://jeremyzhao1998.github.io/DAVoteNet-release/.</description>
      <author>example@mail.com (Zijing Zhao, Zhu Xu, Qingchao Chen, Yuxin Peng, Yang Liu)</author>
      <guid isPermaLink="false">2508.17439v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2508.13073v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page:  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了首个系统化的、以分类为导向的综述，关于用于机器人操作的大型视觉语言模型(VLM)基础的视觉-语言-动作(VLA)模型。文章定义了这些模型，区分了两种主要架构范式(单体模型和分层模型)，并深入研究了它们与先进领域的集成、独特特征及未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;机器人操作是机器人和具身AI的关键前沿领域，需要精确的电机控制和多模态理解。然而，传统基于规则的方法在非结构化、新环境中无法扩展或泛化。&lt;h4&gt;目的&lt;/h4&gt;提供首个系统化的分类导向综述，解决现有分类中的不一致性，减轻研究碎片化，通过系统整合大型VLMs与机器人操作交叉领域的研究，填补关键空白。&lt;h4&gt;方法&lt;/h4&gt;定义大型VLM-based VLA模型，区分两种主要架构范式(单体模型和分层模型)，并深入研究它们与先进领域的集成、独特特征及未来发展方向。单体模型包括单系统和双系统设计，分层模型通过可解释的中间表示明确解耦规划与执行。&lt;h4&gt;主要发现&lt;/h4&gt;VLA模型建立在预训练在大量图像文本数据集上的大型VLMs之上，已成为变革性范式。存在单体模型和分层模型两种架构范式。与强化学习、无需训练优化、从人类视频学习和世界模型集成等先进领域的集成提供了新方向。记忆机制、4D感知、高效适应和多智能体合作是有前景的发展方向。&lt;h4&gt;结论&lt;/h4&gt;这篇综述整合了最近的进展，为大型VLM-based VLA模型在机器人操作中的应用提供了系统化视角，并通过定期更新的项目页面记录持续进展。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作，作为机器人和具身AI的关键前沿，需要精确的电机控制和多模态理解，然而传统的基于规则的方法在非结构化、新环境中无法扩展或泛化。近年来，建立在预训练在大量图像文本数据集上的大型视觉语言模型(VLMs)基础上的视觉-语言-动作(VLA)模型，已成为一种变革性范式。这篇综述提供了首个系统化的、以分类为导向的回顾，关于用于机器人操作的大型VLM基础的VLA模型。我们首先明确定义了大型VLM基础的VLA模型，并勾勒出两种主要的架构范式：(1)单体模型，包括具有不同集成程度的单系统和双系统设计；(2)分层模型，通过可解释的中间表示明确解耦规划与执行。在此基础上，我们深入研究了大型VLM基础的VLA模型：(1)与先进领域的集成，包括强化学习、无需训练的优化、从人类视频中学习以及世界模型集成；(2)综合独特特征，整合架构特征、操作优势以及支持其开发的数据集和基准；(3)确定有前景的方向，包括记忆机制、4D感知、高效适应、多智能体合作及其他新兴能力。这篇综述整合了最近的进展，以解决现有分类中的不一致性，减轻研究碎片化，并通过系统整合大型VLMs与机器人操作交叉领域的研究，填补关键空白。我们提供了一个定期更新的项目页面来记录持续进展：https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulation, a key frontier in robotics and embodied AI, requiresprecise motor control and multimodal understanding, yet traditional rule-basedmethods fail to scale or generalize in unstructured, novel environments. Inrecent years, Vision-Language-Action (VLA) models, built upon LargeVision-Language Models (VLMs) pretrained on vast image-text datasets, haveemerged as a transformative paradigm. This survey provides the firstsystematic, taxonomy-oriented review of large VLM-based VLA models for roboticmanipulation. We begin by clearly defining large VLM-based VLA models anddelineating two principal architectural paradigms: (1) monolithic models,encompassing single-system and dual-system designs with differing levels ofintegration; and (2) hierarchical models, which explicitly decouple planningfrom execution via interpretable intermediate representations. Building on thisfoundation, we present an in-depth examination of large VLM-based VLA models:(1) integration with advanced domains, including reinforcement learning,training-free optimization, learning from human videos, and world modelintegration; (2) synthesis of distinctive characteristics, consolidatingarchitectural traits, operational strengths, and the datasets and benchmarksthat support their development; (3) identification of promising directions,including memory mechanisms, 4D perception, efficient adaptation, multi-agentcooperation, and other emerging capabilities. This survey consolidates recentadvances to resolve inconsistencies in existing taxonomies, mitigate researchfragmentation, and fill a critical gap through the systematic integration ofstudies at the intersection of large VLMs and robotic manipulation. We providea regularly updated project page to document ongoing progress:https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation</description>
      <author>example@mail.com (Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie)</author>
      <guid isPermaLink="false">2508.13073v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Constrained Random Phase Approximation: the spectral method</title>
      <link>http://arxiv.org/abs/2508.15368v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, to be published in Physical Review B, Poster  presented at PSI-K, Lausanne (August 2025). Talk given at Workshop "The  determination of Hubbard parameters: progress, pitfalls, and prospects",  Gandia (September 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的受限制随机相位近似方法（谱cRPA），并将其与现有cRPA方法进行比较。s-cRPA在多种系统中表现更优，能够更准确地预测相互作用参数，克服了标准cRPA中U值低估的问题，并提供了更好的数值稳定性。此外，研究还增强了方法的实现，添加了多中心相互作用计算和低缩放变体。&lt;h4&gt;背景&lt;/h4&gt;在计算材料科学中，准确描述电子相互作用对于预测材料性质至关重要。标准cRPA方法在计算Hubbard U相互作用时存在低估问题，特别是在处理某些电子壳层时可能产生负值，影响预测准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种更稳健、准确的cRPA变体（s-cRPA），以克服标准cRPA中U值低估的问题，提高对材料电子结构的预测能力，特别是在诱导绝缘态方面的表现。&lt;h4&gt;方法&lt;/h4&gt;提出并实现了一种新的谱cRPA（s-cRPA）方法，将其应用于钪和铜系统（通过改变3d壳层填充），并在CaFeO₃实际系统中进行测试。还通过电子转换解决了投影cRPA中的负相互作用值问题，并增强了实现，添加了多中心相互作用计算功能和具有压缩Matsubara网格的低缩放变体。&lt;h4&gt;主要发现&lt;/h4&gt;1. s-cRPA产生的Hubbard U相互作用值大于标准cRPA方法；2. 在CaFeO₃系统中，s-cRPA产生的相互作用参数更接近于DFT+U方法中所需的参数，能够诱导实验观察到的绝缘态；3. s-cRPA通过电子转换解决了投影cRPA方法中对于填充d壳层出现负相互作用值的问题，提供了更好的数值稳定性；4. 增强的实现能够计算多中心相互作用分析空间衰减，并通过低缩放变体有效获得全频率依赖相互作用。&lt;h4&gt;结论&lt;/h4&gt;s-cRPA是一种更稳健、准确的方法，有效克服了标准cRPA中U值低估的问题，是材料科学研究中一个有前景的工具。增强的实现进一步扩展了其应用范围和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的受限制随机相位近似方法，称为谱cRPA（s-cRPA），并通过改变3d壳层填充，将这种方法与已建立的cRPA方法在钪和铜系统中进行了比较。s-cRPA始终产生更大的Hubbard U相互作用值。应用于实际系统CaFeO₃时，s-cRPA产生的相互作用参数更接近于DFT+U方法中所需的参数，以诱导实验观察到的绝缘态，克服了标准密度泛函预测的金属行为。我们还解决了投影cRPA方法中对于填充d壳层发现负相互作用值的问题，证明s-cRPA通过电子转换提供了更好的数值稳定性。总体而言，s-cRPA更稳健，有效克服了标准cRPA中已知的U值低估问题，使其成为社区有前景的工具。此外，我们通过添加计算多中心相互作用以分析空间衰减的功能增强了实现，并开发了一种具有压缩Matsubara网格的低缩放变体，以有效获得全频率依赖相互作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new constrained Random Phase Approximation (cRPA) method, termedspectral cRPA (s-cRPA), and compare it to established cRPA approaches forScandium and Copper by varying the 3d shell filling. The s-cRPA consistentlyyields larger Hubbard $U$ interaction values. Applied to the realistic systemCaFeO$_3$, s-cRPA produces interaction parameters significantly closer to thoserequired within DFT+$U$ to induce the experimentally observed insulating state,overcoming the metallic behavior predicted by standard density functionals. Wealso address the issue of negative interaction values found in the projectorcRPA method for filled d-shells, demonstrating that s-cRPA offers superiornumerical stability through electron conversion. Overall, s-cRPA is more robustand effectively overcomes the known underestimation of $U$ in standard cRPA,making it a promising tool for the community. Additionally, we have enhancedour implementation with features for computing multi-center interactions toanalyze spatial decay and developed a low-scaling variant with a compressedMatsubara grid to efficiently obtain full frequency-dependent interactions.</description>
      <author>example@mail.com (Merzuk Kaltak, Alexander Hampel, Martin Schlipf, Indukuru Ramesh Reddy, Bongjae Kim, Georg Kresse)</author>
      <guid isPermaLink="false">2508.15368v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</title>
      <link>http://arxiv.org/abs/2508.21112v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出EO-Robotics系统，包括EO-1模型和EO-Data1.5M数据集，通过交错视觉-文本-动作预训练实现多模态具身推理和机器人控制的优越性能。&lt;h4&gt;背景&lt;/h4&gt;人类能够在开放世界中无缝执行多模态推理和物理交互，这是通用具身智能系统的核心目标。最近的视觉-语言-动作模型在大规模机器人和视觉-文本数据上联合训练，在通用机器人控制方面取得了显著进展，但仍未达到人类水平的灵活性和交错推理与交互能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的具身基础模型，实现人类水平的灵活性和交错推理与交互能力，在开放世界中实现多模态推理和物理交互。&lt;h4&gt;方法&lt;/h4&gt;提出两个关键支柱：一是统一架构，无差别处理多模态输入（图像、文本、视频和动作）；二是大规模、高质量的多模态具身推理数据集EO-Data1.5M，包含超过150万个样本。通过自回归解码和流匹配去噪的协同训练实现无缝机器人动作生成和多模态具身推理。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明交错视觉-文本-动作学习对开放世界理解和泛化的有效性，通过多种跨多个具身的长期、灵巧操作任务得到验证。EO-1在多模态具身推理和机器人控制方面实现了优越性能。&lt;h4&gt;结论&lt;/h4&gt;本研究详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略和训练方法，为开发高级具身基础模型提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;人类在开放世界中无缝执行多模态推理和物理交互的能力是通用具身智能系统的核心目标。最近的视觉-语言-动作模型在大规模机器人和视觉-文本数据上联合训练，在通用机器人控制方面展示了显著进展。然而，它们仍未达到人类在交错推理和交互方面的灵活性水平。在本工作中，我们介绍了EO-Robotics，包括EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，通过交错视觉-文本-动作预训练在多模态具身推理和机器人控制方面实现了优越性能。EO-1的发展基于两个关键支柱：（i）统一架构，无差别处理多模态输入（图像、文本、视频和动作）；（ii）大规模、高质量的多模态具身推理数据集EO-Data1.5M，包含超过150万个样本，强调交错视觉-文本-动作理解。EO-1通过在EO-Data1.5M上自回归解码和流匹配去噪的协同训练，实现无缝机器人动作生成和多模态具身推理。大量实验证明了交错视觉-文本-动作学习对开放世界理解和泛化的有效性，通过各种跨多个具身的长期、灵巧操作任务得到验证。本文详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略和训练方法，为开发高级具身基础模型提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The human ability to seamlessly perform multimodal reasoning and physicalinteraction in the open world is a core goal for general-purpose embodiedintelligent systems. Recent vision-language-action (VLA) models, which areco-trained on large-scale robot and visual-text data, have demonstrated notableprogress in general robot control. However, they still fail to achievehuman-level flexibility in interleaved reasoning and interaction. In this work,introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 isa unified embodied foundation model that achieves superior performance inmultimodal embodied reasoning and robot control through interleavedvision-text-action pre-training. The development of EO-1 is based on two keypillars: (i) a unified architecture that processes multimodal inputsindiscriminately (image, text, video, and action), and (ii) a massive,high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which containsover 1.5 million samples with emphasis on interleaved vision-text-actioncomprehension. EO-1 is trained through synergies between auto-regressivedecoding and flow matching denoising on EO-Data1.5M, enabling seamless robotaction generation and multimodal embodied reasoning. Extensive experimentsdemonstrate the effectiveness of interleaved vision-text-action learning foropen-world understanding and generalization, validated through a variety oflong-horizon, dexterous manipulation tasks across multiple embodiments. Thispaper details the architecture of EO-1, the data construction strategy ofEO-Data1.5M, and the training methodology, offering valuable insights fordeveloping advanced embodied foundation models.</description>
      <author>example@mail.com (Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang)</author>
      <guid isPermaLink="false">2508.21112v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2508.20085v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HERMES，一个用于移动双臂灵巧操作的人机学习框架，能够将多源人类手部动作转化为可行的机器人行为，并在多样化环境中实现自主操作。&lt;h4&gt;背景&lt;/h4&gt;利用人类运动数据让机器人获得多样化操作技能是一种有前景的范式，但将多源人类手部动作转化为可行的机器人行为仍具挑战性，尤其对于具有复杂、高维动作空间的多指灵巧手，现有方法难以适应不同环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发HERMES框架以解决多源人类手部动作到机器人行为的转换问题，提高策略对多样化环境条件的适应能力，实现复杂移动双臂灵巧操作任务。&lt;h4&gt;方法&lt;/h4&gt;HERMES采用统一强化学习方法将异构人类手部动作转化为物理可行的机器人行为；设计基于深度图像的端到端模拟到现实迁移方法提高泛化能力；通过闭环PnP定位机制增强导航基础模型，连接自主导航与灵巧操作。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明HERMES能在多样化真实场景中展现可泛化行为，成功执行多种复杂移动双臂灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;HERMES有效解决了人类运动到机器人行为的转换问题，通过模拟到现实迁移和增强导航能力，实现了在多样化环境中的自主操作。&lt;h4&gt;翻译&lt;/h4&gt;利用人类运动数据使机器人具备多样化操作技能已成为机器人操作领域的一种有前景的范式。然而，将多源人类手部动作转化为可行的机器人行为仍然具有挑战性，特别是对于配备具有复杂、高维动作空间的多指灵巧手的机器人。此外，现有方法通常难以产生能够适应不同环境条件的策略。在本文中，我们介绍了HERMES，一个用于移动双臂灵巧操作的人机学习框架。首先，HERMES制定了一种统一的强化学习方法，能够无缝地将来自多个源的不同类人类手部动作转化为物理上可行的机器人行为。随后，为了减轻模拟到现实的差距，我们设计了一种基于深度图像的端到端模拟到现实迁移方法，以提高对现实场景的泛化能力。此外，为了能够在多样化且非结构化的环境中自主运行，我们通过闭环PnP定位机制增强了导航基础模型，确保视觉目标的精确对齐，有效连接自主导航和灵巧操作。广泛的实验结果表明，HERMES能够在多样化、真实场景中展现出可泛化的行为，成功执行了许多复杂的移动双臂灵巧操作任务。项目页面：https://gemcollector.github.io/HERMES/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging human motion data to impart robots with versatile manipulationskills has emerged as a promising paradigm in robotic manipulation.Nevertheless, translating multi-source human hand motions into feasible robotbehaviors remains challenging, particularly for robots equipped withmulti-fingered dexterous hands characterized by complex, high-dimensionalaction spaces. Moreover, existing approaches often struggle to produce policiescapable of adapting to diverse environmental conditions. In this paper, weintroduce HERMES, a human-to-robot learning framework for mobile bimanualdexterous manipulation. First, HERMES formulates a unified reinforcementlearning approach capable of seamlessly transforming heterogeneous human handmotions from multiple sources into physically plausible robotic behaviors.Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depthimage-based sim2real transfer method for improved generalization to real-worldscenarios. Furthermore, to enable autonomous operation in varied andunstructured environments, we augment the navigation foundation model with aclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precisealignment of visual goals and effectively bridging autonomous navigation anddexterous manipulation. Extensive experimental results demonstrate that HERMESconsistently exhibits generalizable behaviors across diverse, in-the-wildscenarios, successfully performing numerous complex mobile bimanual dexterousmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.</description>
      <author>example@mail.com (Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu)</author>
      <guid isPermaLink="false">2508.20085v3</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Compositional Generative Model of Unbounded 4D Cities</title>
      <link>http://arxiv.org/abs/2501.08983v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TPAMI. Project Page:  https://www.infinitescript.com/project/city-dreamer-4d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CityDreamer4D，一个专门用于生成无限4D城市的组合生成模型，通过分离动态对象和静态场景，并使用不同类型的神经场来构建城市元素，实现了高质量的城市生成。&lt;h4&gt;背景&lt;/h4&gt;3D场景生成近年来受到广泛关注并取得显著进展，但生成4D城市更具挑战性，因为城市中存在结构复杂、视觉多样的对象（如建筑物和车辆），且人类对城市环境中的失真更为敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的生成模型解决4D城市生成中的挑战，能够生成结构合理、视觉真实的无限4D城市。&lt;h4&gt;方法&lt;/h4&gt;1) 将动态对象与静态场景分离；2) 使用不同类型的神经场构建城市元素；3) 提出交通场景生成器和无限布局生成器，使用紧凑的BEV表示；4) 组合面向物质和面向实例的神经场生成对象；5) 采用定制的生成哈希网格和周期性位置嵌入；6) 提供OSM、GoogleEarth和CityTopia数据集。&lt;h4&gt;主要发现&lt;/h4&gt;1) 4D城市生成应分离动态对象和静态场景；2) 所有4D场景对象应由不同类型的神经场组成；3) 定制的生成哈希网格和周期性位置嵌入能有效处理不同类型对象；4) 该模型在生成真实4D城市方面达到最先进性能。&lt;h4&gt;结论&lt;/h4&gt;CityDreamer4D凭借其组合设计，支持实例编辑、城市风格化和城市模拟等多种下游应用，同时在生成真实4D城市方面取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;3D场景生成近年来引起了越来越多的关注并取得了显著进展。由于存在结构复杂、视觉多样的对象（如建筑物和车辆），以及人类对城市环境中失真的高度敏感性，生成4D城市比3D场景更具挑战性。为解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无限4D城市而设计的组合生成模型。我们的主要见解是：1) 4D城市生成应将动态对象（如车辆）与静态场景（如建筑物和道路）分离，2) 4D场景中的所有对象应由不同类型的神经场组成，用于建筑物、车辆和背景物质。具体来说，我们提出了交通场景生成器和无限布局生成器，使用高度紧凑的BEV表示来生成动态交通场景和静态城市布局。4D城市中的对象通过组合面向物质和面向实例的神经场来生成，用于背景物质、建筑物和车辆。为了适应背景物质和实例的不同特性，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们为城市生成提供了全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界城市布局，而Google Earth和CityTopia数据集提供了大规模、高质量的带有3D实例注释的城市图像。凭借其组合设计，CityDreamer4D支持多种下游应用，如实例编辑、城市风格化和城市模拟，同时在生成真实4D城市方面取得了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成无边界（unbounded）的4D城市场景的问题。4D城市指的是包含时间维度的3D城市场景，既包含静态的建筑、道路等元素，也包含动态的车辆等随时间变化的内容。这个问题在现实中很重要，因为城市作为元宇宙中最基本的资产之一，广泛应用于城市规划、环境模拟和游戏开发等领域。现有方法要么无法保证时间一致性，要么只能生成小规模场景，且人类对城市环境中的失真更加敏感，增加了生成高质量4D城市的难度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于两个主要思考设计方法：1) 4D城市生成应该将动态对象（如车辆）与静态场景（如建筑和道路）分离；2) 4D场景中的所有对象应由不同类型的神经场组成，分别用于建筑、车辆和背景元素。作者借鉴了多项现有工作，包括使用MaskGIT进行城市布局生成，VQVAE进行语义地图标记化，SceneDreamer的鸟瞰图表示方法，NeRF的体积渲染技术，以及GAN的对抗性训练方法等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将4D城市生成分离为静态场景生成和动态对象生成，使用不同类型的神经场分别生成背景、建筑和车辆，采用组合式设计以支持多种下游应用。整体流程分为六个步骤：1) 无边界布局生成器创建城市布局；2) 交通场景生成器生成高清地图和交通场景；3) 城市背景生成器生成背景元素；4) 建筑实例生成器生成建筑；5) 车辆实例生成器生成车辆；6) 组合器将所有元素合并为统一图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出CityDreamer4D，第一个分离动态对象与静态场景的无边界4D城市生成模型；2) 引入面向事物和面向实例的神经场，分别处理背景和实例；3) 创建全面的数据集(OSM、GoogleEarth和CityTopia)。相比之前的工作，不同之处在于：分离了动态与静态元素的生成；为不同元素使用专门的神经场参数化方法；将建筑和车辆分别放置在特定的坐标空间中；创建了更全面的数据集；支持实例级编辑、城市风格化和城市模拟等应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CityDreamer4D通过分离动态对象与静态场景、使用专门的神经场生成不同类型元素，并构建全面的数据集，实现了高质量无边界4D城市的生成与编辑，为元宇宙应用提供了强大工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-01-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3603078&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D scene generation has garnered growing attention in recent years and hasmade significant progress. Generating 4D cities is more challenging than 3Dscenes due to the presence of structurally complex, visually diverse objectslike buildings and vehicles, and heightened human sensitivity to distortions inurban environments. To tackle these issues, we propose CityDreamer4D, acompositional generative model specifically tailored for generating unbounded4D cities. Our main insights are 1) 4D city generation should separate dynamicobjects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)all objects in the 4D scene should be composed of different types of neuralfields for buildings, vehicles, and background stuff. Specifically, we proposeTraffic Scenario Generator and Unbounded Layout Generator to produce dynamictraffic scenarios and static city layouts using a highly compact BEVrepresentation. Objects in 4D cities are generated by combining stuff-orientedand instance-oriented neural fields for background stuff, buildings, andvehicles. To suit the distinct characteristics of background stuff andinstances, the neural fields employ customized generative hash grids andperiodic positional embeddings as scene parameterizations. Furthermore, weoffer a comprehensive suite of datasets for city generation, including OSM,GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-worldcity layouts, while the Google Earth and CityTopia datasets deliverlarge-scale, high-quality city imagery complete with 3D instance annotations.Leveraging its compositional design, CityDreamer4D supports a range ofdownstream applications, such as instance editing, city stylization, and urbansimulation, while delivering state-of-the-art performance in generatingrealistic 4D cities.</description>
      <author>example@mail.com (Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu)</author>
      <guid isPermaLink="false">2501.08983v4</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>SEAL: Structure and Element Aware Learning to Improve Long Structured Document Retrieval</title>
      <link>http://arxiv.org/abs/2508.20778v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型对比学习框架SEAL，用于解决长结构化文档检索中现有方法未能有效利用结构特征和元素级语义的问题，并发布了带有丰富结构注释的数据集。&lt;h4&gt;背景&lt;/h4&gt;现有方法在长结构化文档检索中通常使用对比学习来微调预训练语言模型(PLMs)，但这些方法使用的数据集缺乏明确的结构信息。&lt;h4&gt;目的&lt;/h4&gt;解决当前方法未能有效利用结构特征和元素级语义的问题，以及缺乏包含结构元数据的数据集的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了SEAL框架，利用结构感知学习保留语义层次结构，并使用掩码元素进行细粒度语义区分，同时发布了带有丰富结构注释的长结构化文档检索数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在发布的数据集和工业数据集上的广泛实验，以及在线A/B测试表明，该方法在各种现代PLM上实现了性能提升，在BGE-M3上将NDCG@10从73.96%提升到77.84%。&lt;h4&gt;结论&lt;/h4&gt;SEAL框架通过有效利用结构信息和元素级语义，显著提升了长结构化文档检索的性能。&lt;h4&gt;翻译&lt;/h4&gt;在长结构化文档检索中，现有方法通常在缺乏明确结构信息的数据集上使用对比学习来微调预训练语言模型(PLMs)。这种方法存在两个关键问题：1)当前方法未能有效利用结构特征和元素级语义；2)缺乏包含结构元数据的数据集。为弥补这些差距，我们提出了SEAL，一种新型对比学习框架。它利用结构感知学习来保留语义层次结构，并使用掩码元素进行细粒度语义区分。此外，我们发布了SEAL数据集，一个具有丰富结构注释的长结构化文档检索数据集。在发布的数据集和工业数据集上对各种现代PLM进行的广泛实验，以及在线A/B测试，都展示了一致的性能提升，在BGE-M3上将NDCG@10从73.96%提升到77.84%。资源可在https://github.com/xinhaoH/SEAL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In long structured document retrieval, existing methods typically fine-tunepre-trained language models (PLMs) using contrastive learning on datasetslacking explicit structural information. This practice suffers from twocritical issues: 1) current methods fail to leverage structural features andelement-level semantics effectively, and 2) the lack of datasets containingstructural metadata. To bridge these gaps, we propose \our, a novel contrastivelearning framework. It leverages structure-aware learning to preserve semantichierarchies and masked element alignment for fine-grained semanticdiscrimination. Furthermore, we release \dataset, a long structured documentretrieval dataset with rich structural annotations. Extensive experiments onboth released and industrial datasets across various modern PLMs, along withonline A/B testing, demonstrate consistent performance improvements, boostingNDCG@10 from 73.96\% to 77.84\% on BGE-M3. The resources are available athttps://github.com/xinhaoH/SEAL.</description>
      <author>example@mail.com (Xinhao Huang, Zhibo Ren, Yipeng Yu, Ying Zhou, Zulong Chen, Zeyi Wen)</author>
      <guid isPermaLink="false">2508.20778v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2507.14452v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures. Accepted to IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于格式塔原则的正交几何一致性引导的并行交互网络（GPI-Net），用于解决点云配准中局部和全局特征融合的挑战性问题。&lt;h4&gt;背景&lt;/h4&gt;在基于特征点云配准中，准确识别高质量对应关系是一项前提任务，但由于特征冗余和复杂的空间关系，处理局部和全局特征的融合极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;利用格式塔原则的优势，设计一种新型网络结构，促进局部和全局信息之间的互补交流，提高点云配准中对应关系识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出GPI-Net网络，包含：1）利用格式塔原则促进局部和全局信息互补交流；2）引入正交集成策略减少冗余信息；3）设计格式塔特征注意力（GFA）块捕捉对应关系中的几何特征；4）创建双路径多粒度并行交互聚合（DMG）块促进不同粒度间的信息交换。&lt;h4&gt;主要发现&lt;/h4&gt;在各种具有挑战性的任务上的大量实验证明，与现有方法相比，所提出的GPI-Net具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;GPI-Net通过有效融合局部和全局特征，显著提高了点云配准中高质量对应关系的识别能力。&lt;h4&gt;翻译&lt;/h4&gt;在基于特征点云配准中，准确识别高质量对应关系是一项前提任务。然而，由于特征冗余和复杂的空间关系，处理局部和全局特征的融合极具挑战性。鉴于格式塔原则在分析局部和全局关系方面提供关键优势，我们在本文中提出了一种基于格式塔原则的正交几何一致性引导的并行交互网络（GPI-Net）。它利用格式塔原则促进局部和全局信息之间的互补交流。具体来说，我们引入了一种正交集成策略，以最优方式减少冗余信息，并为高质量对应关系生成更紧凑的全局结构。为了捕捉对应关系中的几何特征，我们通过自注意力和交叉注意力机制的混合使用，利用了格式塔特征注意力（GFA）块。此外，为了促进局部详细信息整合到全局结构中，我们设计了一种创新的双路径多粒度并行交互聚合（DMG）块，以促进不同粒度间的信息交换。在各种具有挑战性的任务上的大量实验证明，与现有方法相比，我们提出的GPI-Net具有优越的性能。代码将在https://github.com/gwk429/GPI-Net上发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中高质量对应关系的识别问题。在点云配准过程中，初始对应关系经常包含大量错误匹配，导致不准确的对齐。这个问题很重要，因为点云是3D世界的主要表示形式，广泛应用于自动驾驶、机器人、SLAM等领域，而精确的点云对齐是实现这些应用的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于Gestalt原则(格式塔原则)设计方法，该原则强调整体感知优于局部感知。作者观察到现有方法在处理局部和全局特征融合时存在不足，无法有效捕捉空间关系。作者借鉴了PG-Net的框架，但通过引入Gestalt原则来改进局部和全局信息的互补通信。同时，作者也参考了PointDSC的空间一致性思想，但通过正交整合策略和双路径多粒度交互设计解决了冗余信息问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用Gestalt原则促进局部和全局信息之间的互补通信，通过正交整合减少冗余信息，并设计注意力机制捕获几何特征。整体流程：1)输入初始对应关系；2)上下文嵌入模块映射到高维特征空间；3)正交整合(OI)过滤冗余信息；4)Gestalt特征注意力(GFA)提取几何特征；5)双路径多粒度并行交互聚合(DMG)促进不同粒度信息交换；6)种子选择模块识别高质量对应关系；7)Two-Stage NSM模块估计最优变换矩阵。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出GPI-Net解决特征冗余和复杂空间关系；2)将Gestalt原则融入网络设计；3)设计正交整合(OI)策略减少冗余；4)提出Gestalt特征注意力(GFA)块；5)设计双路径多粒度并行交互聚合(DMG)块。相比之前工作，GPI-Net更好地平衡了局部和全局信息，减少了冗余，在室内外场景下均表现出优越性能，特别是在对应关系较少的情况下优势更明显。实验显示，相比PG-Net，GPI-Net的F1分数提高了约1%，RR提高了约1.5%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GPI-Net通过融合格式塔原则和正交几何一致性，提出了一种新颖的并行交互网络，显著提高了点云配准中对应关系的识别精度和鲁棒性，特别是在复杂场景和高离群值比例的情况下表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate identification of high-quality correspondences is a prerequisitetask in feature-based point cloud registration. However, it is extremelychallenging to handle the fusion of local and global features due to featureredundancy and complex spatial relationships. Given that Gestalt principlesprovide key advantages in analyzing local and global relationships, we proposea novel Gestalt-guided Parallel Interaction Network via orthogonal geometricconsistency (GPI-Net) in this paper. It utilizes Gestalt principles tofacilitate complementary communication between local and global information.Specifically, we introduce an orthogonal integration strategy to optimallyreduce redundant information and generate a more compact global structure forhigh-quality correspondences. To capture geometric features in correspondences,we leverage a Gestalt Feature Attention (GFA) block through a hybridutilization of self-attention and cross-attention mechanisms. Furthermore, tofacilitate the integration of local detail information into the globalstructure, we design an innovative Dual-path Multi-Granularity parallelinteraction aggregation (DMG) block to promote information exchange acrossdifferent granularities. Extensive experiments on various challenging tasksdemonstrate the superior performance of our proposed GPI-Net in comparison toexisting methods. The code will be released athttps://github.com/gwk429/GPI-Net.</description>
      <author>example@mail.com (Weikang Gu, Mingyue Han, Li Xue, Heng Dong, Changcai Yang, Riqing Chen, Lifang Wei)</author>
      <guid isPermaLink="false">2507.14452v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling</title>
      <link>http://arxiv.org/abs/2508.21785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种处理心率预测中数据异质性挑战的框架，通过学习对设备异质性和用户异质性都无潜在表示，显著提升了模型在实际应用中的性能。&lt;h4&gt;背景&lt;/h4&gt;心率预测对个性化健康监测和健身至关重要，但在实际部署中面临数据异质性的挑战，包括来自不同设备的源异质性和不同用户及活动的用户异质性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理设备异质性和用户异质性的框架，使心率预测模型在真实世界的异构数据环境下保持一致的高性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种包含随机特征丢弃策略处理源异质性，以及时间感知注意力模块和对比学习目标来处理用户异质性的框架，并创建了新的基准数据集ParroTao进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在ParroTao和FitRec数据集上，所提模型分别比现有基线高出17%和15%；学习到的表示具有很强的判别能力；一个下游应用任务验证了模型的实用价值。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架有效解决了心率预测中的数据异质性问题，显著提升了模型在实际应用中的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;心率预测对个性化健康监测和健身至关重要，但在实际部署中经常面临一个关键挑战：数据异质性。我们从两个关键维度对其进行分类：来自具有不同特征集的碎片化设备市场的源异质性，以及反映不同个体和活动中生理模式差异的用户异质性。现有方法要么丢弃设备特定信息，要么无法建模用户特定差异，限制了它们的实际性能。为此，我们提出了一个学习对两种异质性都无潜在表示的框架，使下游预测器能够在异构数据模式下一致工作。具体而言，我们引入了随机特征丢弃策略来处理源异质性，使模型对各种特征集具有鲁棒性。为了管理用户异质性，我们采用时间感知注意力模块来捕获长期生理特征，并使用对比学习目标来构建判别性表示空间。为了反映现实世界数据的异质性，我们创建并公开发布了一个新的基准数据集ParroTao。在ParroTao和公共FitRec数据集上的评估显示，我们的模型分别比现有基线高出17%和15%。此外，对学习到的表示的分析证明了它们具有很强的判别能力，一个下游应用任务确认了我们模型的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heart rate prediction is vital for personalized health monitoring andfitness, while it frequently faces a critical challenge when deploying inreal-world: data heterogeneity. We classify it in two key dimensions: sourceheterogeneity from fragmented device markets with varying feature sets, anduser heterogeneity reflecting distinct physiological patterns acrossindividuals and activities. Existing methods either discard device-specificinformation, or fail to model user-specific differences, limiting theirreal-world performance. To address this, we propose a framework that learnslatent representations agnostic to both heterogeneity, enabling downstreampredictors to work consistently under heterogeneous data patterns.Specifically, we introduce a random feature dropout strategy to handle sourceheterogeneity, making the model robust to various feature sets. To manage userheterogeneity, we employ a time-aware attention module to capture long-termphysiological traits and use a contrastive learning objective to build adiscriminative representation space. To reflect the heterogeneous nature ofreal-world data, we created and publicly released a new benchmark dataset,ParroTao. Evaluations on both ParroTao and the public FitRec dataset show thatour model significantly outperforms existing baselines by 17% and 15%,respectively. Furthermore, analysis of the learned representations demonstratestheir strong discriminative power, and one downstream application task confirmthe practical value of our model.</description>
      <author>example@mail.com (Peng Yang, Zhengdong Huang, Zicheng Xie, Wentao Tian, Jingyu Liu, Lunhong Dong)</author>
      <guid isPermaLink="false">2508.21785v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
  <item>
      <title>HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones</title>
      <link>http://arxiv.org/abs/2508.21539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM MM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种分层跨粒度对比和匹配学习（HCCM）框架，用于解决自然语言引导无人机（NLGD）任务中的视觉语言理解挑战，特别是在处理广角视野和复杂组合语义时。&lt;h4&gt;背景&lt;/h4&gt;无人机场景中的广角视野和复杂组合语义对视觉语言理解提出了挑战。主流视觉语言模型（VLMs）强调全局对齐但缺乏细粒度语义，而现有分层方法依赖于精确的实体划分和严格的包含关系，限制了在动态环境中的有效性。此外，无人机文本描述通常不完整或模糊，导致对齐不稳定。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理无人机场景中视觉语言理解挑战的框架，特别是在处理广角视野、复杂组合语义和不完整文本描述时，提高目标匹配和导航等任务的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了分层跨粒度对比和匹配学习（HCCM）框架，包含两个主要组件：1. 区域-全局图像-文本对比学习（RG-ITC）：避免精确场景划分，通过对比局部视觉区域与全局文本以及反之亦然来捕获分层局部到全局的语义。2. 区域-全局图像-文本匹配（RG-ITM）：放弃严格的约束，在全局跨模态表示内评估局部语义一致性，增强组合推理能力。此外，还引入了动量对比和蒸馏（MCD）机制来提高对不完整或模糊文本描述的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在GeoText-1652数据集上，HCCM在图像检索中达到了28.8%的最先进Recall@1，在文本检索中达到了14.7%。在未见过的ERA数据集上，HCCM展示了强大的零样本泛化能力，平均召回率（mR）为39.93%，优于微调基线。&lt;h4&gt;结论&lt;/h4&gt;HCCM框架通过分层跨粒度的对比和匹配学习方法，有效解决了无人机场景中视觉语言理解的挑战，特别是在处理广角视野、复杂组合语义和不完整文本描述方面，表现出优越的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;自然语言引导的无人机（NLGD）为目标匹配和导航等任务提供了新颖的范式。然而，无人机场景中的广角视野和复杂组合语义对视觉语言理解提出了挑战。主流视觉语言模型（VLMs）强调全局对齐但缺乏细粒度语义，而现有的分层方法依赖于精确的实体划分和严格的包含关系，限制了在动态环境中的有效性。为此，我们提出了分层跨粒度对比和匹配学习（HCCM）框架，包含两个组件：（1）区域-全局图像-文本对比学习（RG-ITC），它避免了精确的场景划分，通过对比局部视觉区域与全局文本以及反之亦然来捕获分层局部到全局的语义；（2）区域-全局图像-文本匹配（RG-ITM），它放弃了严格的约束，而是在全局跨模态表示内评估局部语义一致性，增强组合推理能力。此外，无人机文本描述通常不完整或模糊，导致对齐不稳定。HCCM引入了动量对比和蒸馏（MCD）机制来提高鲁棒性。在GeoText-1652上的实验显示，HCCM在图像检索中达到了28.8%的最先进Recall@1，在文本检索中达到了14.7%。在未见过的ERA数据集上，HCCM展示了强大的零样本泛化能力，平均召回率为39.93%，优于微调基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks suchas target matching and navigation. However, the wide field of view and complexcompositional semantics in drone scenarios pose challenges for vision-languageunderstanding. Mainstream Vision-Language Models (VLMs) emphasize globalalignment while lacking fine-grained semantics, and existing hierarchicalmethods depend on precise entity partitioning and strict containment, limitingeffectiveness in dynamic environments. To address this, we propose theHierarchical Cross-Granularity Contrastive and Matching learning (HCCM)framework with two components: (1) Region-Global Image-Text ContrastiveLearning (RG-ITC), which avoids precise scene partitioning and captureshierarchical local-to-global semantics by contrasting local visual regions withglobal text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),which dispenses with rigid constraints and instead evaluates local semanticconsistency within global cross-modal representations, enhancing compositionalreasoning. Moreover, drone text descriptions are often incomplete or ambiguous,destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCMachieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (textretrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shotgeneralization with 39.93% mean recall (mR), outperforming fine-tunedbaselines.</description>
      <author>example@mail.com (Hao Ruan, Jinliang Lin, Yingxin Lai, Zhiming Luo, Shaozi Li)</author>
      <guid isPermaLink="false">2508.21539v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability</title>
      <link>http://arxiv.org/abs/2508.21197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为全局概念激活向量(GCAV)的新框架，用于解决传统概念激活向量(CAV)在不同层间不一致的问题，使跨层比较更加可靠。&lt;h4&gt;背景&lt;/h4&gt;概念激活向量(CAV)是一种解释深度神经网络的有力方法，通过量化网络对人类定义概念的敏感性来工作。然而，当在不同层独立计算CAV时，它们常常表现出不一致性，这使得跨层比较不可靠。&lt;h4&gt;目的&lt;/h4&gt;为了解决CAV在不同层间的不一致问题，作者提出GCAV框架，将CAV统一为单一、语义一致的表示。&lt;h4&gt;方法&lt;/h4&gt;作者的方法利用对比学习来对齐不同层的概念表示，并采用基于注意力的融合机制来构建全局整合的CAV。他们还引入了使用全局概念激活向量进行测试(TGCAV)的方法，以将TCAV应用于基于GCAV的表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著降低了TCAV分数的方差，同时保留了概念相关性，确保了更稳定和可靠的概念归因。实验表明，该方法有效减轻了跨层概念不一致性，增强了概念定位能力，并提高了对抗扰动的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过将跨层信息整合到一致的框架中，该方法提供了对深度学习模型如何编码人类定义概念的更全面和可解释的理解。&lt;h4&gt;翻译&lt;/h4&gt;概念激活向量(CAV)提供了一种强大的方法来解释深度神经网络，通过量化它们对人类定义概念的敏感性。然而，当在不同层独立计算时，CAV常常表现出不一致性，使得跨层比较不可靠。为了解决这个问题，我们提出了全局概念激活向量(GCAV)，这是一个将CAV统一为单一、语义一致表示的新框架。我们的方法利用对比学习来对齐跨层的概念表示，并采用基于注意力的融合机制来构建全局整合的CAV。通过这样做，我们的方法显著降低了TCAV分数的方差，同时保留概念相关性，确保更稳定和可靠的概念归因。为了评估GCAV的有效性，我们引入了使用全局概念激活向量进行测试(TGCAV)作为一种将TCAV应用于基于GCAV表示的方法。我们在多个深度神经网络上进行了大量实验，证明我们的方法有效减轻了跨层概念不一致性，增强了概念定位能力，并提高了对抗扰动的鲁棒性。通过将跨层信息整合到一致的框架中，我们的方法提供了对深度学习模型如何编码人类定义概念的更全面和可解释的理解。代码和模型可在https://github.com/Zhenghao-He/GCAV获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept Activation Vectors (CAVs) provide a powerful approach forinterpreting deep neural networks by quantifying their sensitivity tohuman-defined concepts. However, when computed independently at differentlayers, CAVs often exhibit inconsistencies, making cross-layer comparisonsunreliable. To address this issue, we propose the Global Concept ActivationVector (GCAV), a novel framework that unifies CAVs into a single, semanticallyconsistent representation. Our method leverages contrastive learning to alignconcept representations across layers and employs an attention-based fusionmechanism to construct a globally integrated CAV. By doing so, our methodsignificantly reduces the variance in TCAV scores while preserving conceptrelevance, ensuring more stable and reliable concept attributions. To evaluatethe effectiveness of GCAV, we introduce Testing with Global Concept ActivationVectors (TGCAV) as a method to apply TCAV to GCAV-based representations. Weconduct extensive experiments on multiple deep neural networks, demonstratingthat our method effectively mitigates concept inconsistency across layers,enhances concept localization, and improves robustness against adversarialperturbations. By integrating cross-layer information into a coherentframework, our method offers a more comprehensive and interpretableunderstanding of how deep learning models encode human-defined concepts. Codeand models are available at https://github.com/Zhenghao-He/GCAV.</description>
      <author>example@mail.com (Zhenghao He, Sanchit Sinha, Guangzhi Xiong, Aidong Zhang)</author>
      <guid isPermaLink="false">2508.21197v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>THEME: Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics</title>
      <link>http://arxiv.org/abs/2508.16936v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACM International Conference on Information and Knowledge  Management (CIKM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了THEME框架，通过分层对比学习微调嵌入模型，解决主题投资中行业边界重叠和市场动态变化的挑战，实现主题资产的语义表示和有效检索。&lt;h4&gt;背景&lt;/h4&gt;主题投资旨在构建与结构性趋势一致的组合，但行业边界重叠和市场动态变化使这一目标具有挑战性。从文本数据构建投资主题的语义表示是一个有前景的方向。&lt;h4&gt;目的&lt;/h4&gt;解决通用大型语言模型嵌入模型不适合捕捉金融资产细微特征的问题，开发专门针对投资主题的语义表示方法。&lt;h4&gt;方法&lt;/h4&gt;引入THEME框架，使用分层对比学习微调嵌入模型，利用主题及其组成股票的层次关系进行对齐，并通过整合股票收益来完善这些嵌入，生成能有效检索具有强回报潜力的主题一致资产的表示。&lt;h4&gt;主要发现&lt;/h4&gt;THEME在主题资产检索方面显著优于领先的大型语言模型，且其构建的投资组合表现优异。&lt;h4&gt;结论&lt;/h4&gt;通过共同建模文本中的主题关系和收益中的市场动态，THEME为各种实际投资应用生成专门定制的股票嵌入。&lt;h4&gt;翻译&lt;/h4&gt;主题投资旨在构建与结构性趋势一致的组合，由于行业边界重叠和市场动态变化，这仍然是一项具有挑战性的工作。一个有前景的方向是从文本数据构建投资主题的语义表示。然而，尽管通用大型语言模型嵌入模型功能强大，但不太适合捕捉金融资产的细微特征，因为投资资产的语义表示可能与一般金融文本的语义表示有根本差异。为此，我们引入THEME框架，使用分层对比学习微调嵌入。THEME利用主题及其组成股票的层次关系对齐主题和股票，随后通过整合股票收益来完善这些嵌入。这一过程生成能有效检索具有强回报潜力的主题一致资产的表示。实证结果表明，THEME在两个关键领域表现出色：在主题资产检索方面，它显著优于领先的大型语言模型；此外，其构建的组合表现出令人信服的性能。通过共同建模文本中的主题关系和收益中的市场动态，THEME为各种实际投资应用生成专门定制的股票嵌入。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761517&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Thematic investing, which aims to construct portfolios aligned withstructural trends, remains a challenging endeavor due to overlapping sectorboundaries and evolving market dynamics. A promising direction is to buildsemantic representations of investment themes from textual data. However,despite their power, general-purpose LLM embedding models are not well-suitedto capture the nuanced characteristics of financial assets, since the semanticrepresentation of investment assets may differ fundamentally from that ofgeneral financial text. To address this, we introduce THEME, a framework thatfine-tunes embeddings using hierarchical contrastive learning. THEME alignsthemes and their constituent stocks using their hierarchical relationship, andsubsequently refines these embeddings by incorporating stock returns. Thisprocess yields representations effective for retrieving thematically alignedassets with strong return potential. Empirical results demonstrate that THEMEexcels in two key areas. For thematic asset retrieval, it significantlyoutperforms leading large language models. Furthermore, its constructedportfolios demonstrate compelling performance. By jointly modeling thematicrelationships from text and market dynamics from returns, THEME generates stockembeddings specifically tailored for a wide range of practical investmentapplications.</description>
      <author>example@mail.com (Hoyoung Lee, Wonbin Ahn, Suhwan Park, Jaehoon Lee, Minjae Kim, Sungdong Yoo, Taeyoon Lim, Woohyung Lim, Yongjae Lee)</author>
      <guid isPermaLink="false">2508.16936v2</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes</title>
      <link>http://arxiv.org/abs/2508.21095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的、无需刚性约束的、数据驱动的框架，用于在未注册的体网格上进行运动预测和转移，通过结合稳健的运动嵌入网络和学习的逐顶点特征场生成时空变形场驱动网格变形。&lt;h4&gt;背景&lt;/h4&gt;未注册的表面网格（特别是原始3D扫描）在计算合理的变形时存在显著挑战，主要因为缺乏已确定的点对点对应关系和数据中存在的噪声。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需刚性约束的、数据驱动的框架，用于在未注册的体网格上进行运动预测和转移。&lt;h4&gt;方法&lt;/h4&gt;提出一种结合稳健的运动嵌入网络和学习的逐顶点特征场的方法，生成时空变形场来驱动网格变形。&lt;h4&gt;主要发现&lt;/h4&gt;在行走和跑步等任务上的定量基准测试和定性视觉效果评估表明，该方法在具有挑战性的未注册网格上具有有效性和多功能性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在处理未注册网格上的运动预测和转移方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;未注册的表面网格，特别是原始3D扫描，由于缺乏已确定的点对点对应关系和数据中存在的噪声，在自动计算合理的变形时提出了重大挑战。在本文中，我们提出了一种新的、无需刚性约束的、数据驱动的框架，用于在此类体网格上进行运动预测和转移。我们的方法将稳健的运动嵌入网络与学习的逐顶点特征场相结合，生成时空变形场，从而驱动网格变形。广泛的评估，包括在行走和跑步等任务上的定量基准测试和定性视觉效果，证明了我们的方法在具有挑战性的未注册网格上的有效性和多功能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决未注册表面网格（特别是原始3D扫描）的自动变形计算问题。由于缺乏点对应关系和数据噪声，这些网格难以进行合理变形。随着3D扫描技术进步，野外扫描数据增多，但传统基于骨架的动画技术难以处理这些不规则数据。此外，现有参数化模型（如SMPL）仅限特定形状类别，且需要额外工作适应新类别。解决这个问题对增强现实、虚拟角色动画和机器人等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：基于rig的方法需要rigging和skinning，不能用于未注册网格；变形模型稳健但无法捕获复杂时间依赖性。作者设计了一种递归模型，接受任意源网格和未注册运动序列作为输入，生成时间和顶点级别的变形场。方法借鉴了DiffusionNet作为特征提取器，利用PointNet编码器处理运动序列，并使用双向GRU确保时间一致性，还采用了AIAP正则化损失防止非等距变形。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个无需rig、数据驱动的框架，通过耦合鲁棒的运动嵌入网络与学习的顶点特征场，生成时空变形场来驱动网格变形。流程包括：1) 使用DiffusionNet从源网格顶点位置和法线提取特征；2) 用PointNet编码器和双向GRU处理运动序列；3) 对每个时间步，将运动向量与前一时刻顶点位置连接形成增强特征；4) 用共享MLP预测位移场，添加到源网格实现变形。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 新的端到端深度学习框架用于复杂运动预测；2) 探索和学习时空特征空间实现顶点级别变形；3) 在未注册网格上进行运动预测和传输的综合评估。相比之前工作，该方法无需骨架信息，适用于复杂运动和原始扫描，不需要rigging和skinning，可直接捕获时间依赖性，减少了对良好rigging的依赖，且可在完全无监督设置中训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ScanMove提出了一种无需rig的深度学习框架，能够直接从未注册的3D网格预测复杂且时间一致的运动变形，克服了传统方法对骨架对应关系和网格拓扑的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unregistered surface meshes, especially raw 3D scans, present significantchallenges for automatic computation of plausible deformations due to the lackof established point-wise correspondences and the presence of noise in thedata. In this paper, we propose a new, rig-free, data-driven framework formotion prediction and transfer on such body meshes. Our method couples a robustmotion embedding network with a learned per-vertex feature field to generate aspatio-temporal deformation field, which drives the mesh deformation. Extensiveevaluations, including quantitative benchmarks and qualitative visuals on taskssuch as walking and running, demonstrate the effectiveness and versatility ofour approach on challenging unregistered meshes.</description>
      <author>example@mail.com (Thomas Besnier, Sylvain Arguillère, Mohamed Daoudi)</author>
      <guid isPermaLink="false">2508.21095v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>VoCap: Video Object Captioning and Segmentation from Any Prompt</title>
      <link>http://arxiv.org/abs/2508.21809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VoCap模型，一种能够处理视频和各种模态提示并生成时空掩码和物体中心标题的灵活视频模型，同时解决了可提示视频物体分割、指代表达式分割和物体标注三个任务。&lt;h4&gt;背景&lt;/h4&gt;理解视频中物体的精细定位掩码和详细语义属性是视频理解的基本任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时处理多种视频理解任务的统一模型，解决数据获取困难的问题，并为视频物体标注建立基准。&lt;h4&gt;方法&lt;/h4&gt;通过伪物体标题注释现有大规模分割数据集SAV；使用预处理视频和真实掩码突出感兴趣对象，输入大型视觉语言模型；创建手动注释的验证集SAV-Caption；在SAV-Caption与其他图像和视频数据集混合上大规模训练VoCap模型。&lt;h4&gt;主要发现&lt;/h4&gt;VoCap在指代表达式视频物体分割任务上取得最先进结果；在半监督视频物体分割任务上具有竞争力；为视频物体标注建立了新基准。&lt;h4&gt;结论&lt;/h4&gt;VoCap模型能够有效处理多种视频理解任务，通过创新的数据集构建方法解决了数据获取难题，并在多个任务上展现出优异性能。&lt;h4&gt;翻译&lt;/h4&gt;从视频中的精细定位掩码和详细语义属性角度理解物体是视频理解的基本任务。在本文中，我们提出了VoCap，一种灵活的视频模型，它能够处理视频和各种模态（文本、框或掩码）的提示，并生成一个时空掩码和相应的以物体为中心的标题。因此，我们的模型同时解决了可提示视频物体分割、指代表达式分割和物体标注任务。由于获取该任务的数据繁琐且昂贵，我们提出通过伪物体标题注释现有的大规模分割数据集（SAV）。我们通过预处理视频及其真实掩码来突出显示感兴趣的对象，并将其输入到大型视觉语言模型（VLM）中。为了进行无偏评估，我们在验证集上收集了手动注释。我们将 resulting 数据集称为SAV-Caption。我们在SAV-Caption以及其他图像和视频数据集的混合上大规模训练我们的VoCap模型。我们的模型在指代表达式视频物体分割任务上取得了最先进的结果，在半监督视频物体分割任务上具有竞争力，并为视频物体标注建立了基准。我们的数据集将在https://github.com/google-deepmind/vocap上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding objects in videos in terms of fine-grained localization masksand detailed semantic properties is a fundamental task in video understanding.In this paper, we propose VoCap, a flexible video model that consumes a videoand a prompt of various modalities (text, box or mask), and produces aspatio-temporal masklet with a corresponding object-centric caption. As suchour model addresses simultaneously the tasks of promptable video objectsegmentation, referring expression segmentation, and object captioning. Sinceobtaining data for this task is tedious and expensive, we propose to annotatean existing large-scale segmentation dataset (SAV) with pseudo object captions.We do so by preprocessing videos with their ground-truth masks to highlight theobject of interest and feed this to a large Vision Language Model (VLM). For anunbiased evaluation, we collect manual annotations on the validation set. Wecall the resulting dataset SAV-Caption. We train our VoCap model at scale on aSAV-Caption together with a mix of other image and video datasets. Our modelyields state-of-the-art results on referring expression video objectsegmentation, is competitive on semi-supervised video object segmentation, andestablishes a benchmark for video object captioning. Our dataset will be madeavailable at https://github.com/google-deepmind/vocap.</description>
      <author>example@mail.com (Jasper Uijlings, Xingyi Zhou, Xiuye Gu, Arsha Nagrani, Anurag Arnab, Alireza Fathi, David Ross, Cordelia Schmid)</author>
      <guid isPermaLink="false">2508.21809v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
  <item>
      <title>ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.21496v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ELV-Halluc，第一个专门针对长视频幻觉的基准测试，用于系统研究语义聚合幻觉(SAH)现象。SAH指模型在聚合帧级语义到事件级语义组过程中产生的幻觉，在长视频中尤为严重。&lt;h4&gt;背景&lt;/h4&gt;视频多模态大语言模型在视频理解方面取得了显著进展，但仍容易产生与视频输入不一致或不相关的幻觉。之前的视频幻觉基准主要关注短视频，将幻觉归因于强语言先验、缺失帧或视觉语言偏差等因素，但这些因素过于简化了幻觉的原因。&lt;h4&gt;目的&lt;/h4&gt;分离和彻底研究语义聚合幻觉(SAH)这一特定类型的幻觉，特别是在长视频中的表现，并创建一个专门的基准测试来系统研究这一问题。&lt;h4&gt;方法&lt;/h4&gt;引入ELV-Halluc基准测试，采用位置编码策略来缓解SAH，并采用DPO策略增强模型区分事件内外语义的能力。为此，整理了8K对抗性数据对进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;1) 实验确认了SAH的存在，且SAH随语义复杂度增加而增加；2) 模型在语义快速变化时更容易出现SAH；3) 位置编码策略有助于缓解SAH；4) DPO策略增强了模型区分事件内外语义的能力；5) 在ELV-Halluc和Video-MME上取得了改进，SAH比率显著降低27.7%。&lt;h4&gt;结论&lt;/h4&gt;语义聚合幻觉是长视频理解中的一个重要问题，需要专门的研究和解决方法。通过引入ELV-Halluc基准测试和采用特定的缓解策略，可以有效减少SAH的发生。&lt;h4&gt;翻译&lt;/h4&gt;视频多模态大语言模型在视频理解方面已取得显著进展。然而，它们仍然容易产生与视频输入不一致或不相关的幻觉内容。之前的视频幻觉基准主要关注短视频，将幻觉归因于强语言先验、缺失帧或视觉编码器引入的视觉语言偏差等因素。虽然这些原因确实解释了短视频中大多数幻觉，但它们仍然过于简化了幻觉的原因。有时，模型会产生不正确的输出，但具有正确的帧级语义。我们将这种类型的幻觉称为语义聚合幻觉(SAH)，它发生在将帧级语义聚合到事件级语义组的过程中。由于SAH在长视频中因多个事件间语义复杂度增加而变得尤为重要，因此有必要分离并彻底研究这种幻觉的原因。为解决上述问题，我们引入了ELV-Halluc，这是第一个专门针对长视频幻觉的基准测试，能够系统研究SAH。我们的实验证实了SAH的存在，并表明它随语义复杂度的增加而增加。此外，我们发现模型在语义快速变化时更容易出现SAH。此外，我们讨论了缓解SAH的潜在方法。我们证明位置编码策略有助于减轻SAH，并进一步采用DPO策略来增强模型区分事件内外语义的能力。为此，我们整理了8K对抗性数据对，并在ELV-Halluc和Video-MME上取得了改进，包括SAH比率显著降低27.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video multimodal large language models (Video-MLLMs) have achieved remarkableprogress in video understanding. However, they remain vulnerable tohallucination-producing content inconsistent with or unrelated to video inputs.Previous video hallucination benchmarks primarily focus on short-videos. Theyattribute hallucinations to factors such as strong language priors, missingframes, or vision-language biases introduced by the visual encoder. While thesecauses indeed account for most hallucinations in short videos, they stilloversimplify the cause of hallucinations. Sometimes, models generate incorrectoutputs but with correct frame-level semantics. We refer to this type ofhallucination as Semantic Aggregation Hallucination (SAH), which arises duringthe process of aggregating frame-level semantics into event-level semanticgroups. Given that SAH becomes particularly critical in long videos due toincreased semantic complexity across multiple events, it is essential toseparate and thoroughly investigate the causes of this type of hallucination.To address the above issues, we introduce ELV-Halluc, the first benchmarkdedicated to long-video hallucination, enabling a systematic investigation ofSAH. Our experiments confirm the existence of SAH and show that it increaseswith semantic complexity. Additionally, we find that models are more prone toSAH on rapidly changing semantics. Moreover, we discuss potential approaches tomitigate SAH. We demonstrate that positional encoding strategy contributes toalleviating SAH, and further adopt DPO strategy to enhance the model's abilityto distinguish semantics within and across events. To support this, we curate adataset of 8K adversarial data pairs and achieve improvements on bothELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.</description>
      <author>example@mail.com (Hao Lu, Jiahao Wang, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin, Lewei Lu)</author>
      <guid isPermaLink="false">2508.21496v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks</title>
      <link>http://arxiv.org/abs/2508.21340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DLGAN是一种简单但有效的时间序列合成方法，通过双阶段生成过程和对抗学习，能够更好地保持时间序列的时间依赖性和特征信息。&lt;h4&gt;背景&lt;/h4&gt;时间序列合成是确保时间序列数据安全流通的有效方法，但现有方法基于随机序列进行时间建模，难以确保生成的时间序列中的时间依赖性，且难以准确捕捉原始时间序列的特征信息。&lt;h4&gt;目的&lt;/h4&gt;解决现有时间序列合成方法在保持时间依赖性和特征信息方面的不足，提出一种简单但有效的生成模型。&lt;h4&gt;方法&lt;/h4&gt;提出了名为DLGAN的生成模型，将时间序列生成过程分解为序列特征提取和序列重建两个阶段，形成时间序列自编码器进行监督学习，并使用生成对抗网络生成与真实时间序列特征向量一致的合成特征向量。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共数据集上的大量实验表明，该模型在各种评估指标上具有优越性。&lt;h4&gt;结论&lt;/h4&gt;DLGAN模型能够有效解决现有时间序列合成方法在保持时间依赖性和特征信息方面的问题。&lt;h4&gt;翻译&lt;/h4&gt;时间序列合成是确保时间序列数据安全流通的有效方法。现有时间序列合成方法通常基于随机序列进行时间建模来生成目标序列，这些方法往往难以确保生成的时间序列中的时间依赖性。此外，直接在随机序列上建模时间特征，使得准确捕捉原始时间序列的特征信息变得具有挑战性。为解决上述问题，我们提出了一种简单但有效的生成模型——双重层生成对抗网络，命名为DLGAN。该模型将时间序列生成过程分解为两个阶段：序列特征提取和序列重建。首先，这两个阶段形成一个完整的时间序列自编码器，使能够在原始时间序列上进行监督学习，确保重建过程能够恢复序列的时间依赖性。其次，使用生成对抗网络生成与真实时间序列特征向量一致的合成特征向量，确保生成器能够从真实时间序列中捕获时间特征。在四个公共数据集上的大量实验证明了该模型在各种评估指标上的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series synthesis is an effective approach to ensuring the securecirculation of time series data. Existing time series synthesis methodstypically perform temporal modeling based on random sequences to generatetarget sequences, which often struggle to ensure the temporal dependencies inthe generated time series. Additionally, directly modeling temporal features onrandom sequences makes it challenging to accurately capture the featureinformation of the original time series. To address the above issues, wepropose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer\textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named\textbf{DLGAN}. The model decomposes the time series generation process intotwo stages: sequence feature extraction and sequence reconstruction. First,these two stages form a complete time series autoencoder, enabling supervisedlearning on the original time series to ensure that the reconstruction processcan restore the temporal dependencies of the sequence. Second, a GenerativeAdversarial Network (GAN) is used to generate synthetic feature vectors thatalign with the real-time sequence feature vectors, ensuring that the generatorcan capture the temporal features from real time series. Extensive experimentson four public datasets demonstrate the superiority of this model acrossvarious evaluation metrics.</description>
      <author>example@mail.com (Xuan Hou, Shuhan Liu, Zhaohui Peng, Yaohui Chu, Yue Zhang, Yining Wang)</author>
      <guid isPermaLink="false">2508.21340v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks</title>
      <link>http://arxiv.org/abs/2508.21172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时间残差连接的新型深度未经训练的循环神经网络DeepResESNs，解决了传统ESNs在长期信息处理方面的问题。&lt;h4&gt;背景&lt;/h4&gt;Echo State Networks (ESNs)是储备计算(RC)框架中一种特殊的未经训练的循环神经网络(RNNs)，因其快速高效的学习而广受欢迎，但在长期信息处理方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于时间残差连接的新型深度未经训练的RNNs，增强记忆能力和长期时间建模能力。&lt;h4&gt;方法&lt;/h4&gt;提出Deep Residual Echo State Networks (DeepResESNs)，利用层次化的未经训练的残差循环层，研究不同正交配置(随机生成和固定结构)对网络动力学的影响，并进行数学分析确保稳定动力学。&lt;h4&gt;主要发现&lt;/h4&gt;层次化的未经训练的残差循环层显著提高了记忆能力和长期时间建模能力，不同正交配置对网络动力学有不同影响。&lt;h4&gt;结论&lt;/h4&gt;在各种时间序列任务上的实验证明了DeepResESNs相对于传统浅层和深度RC方法的优越性。&lt;h4&gt;翻译&lt;/h4&gt;回声状态网络(ESNs)是储备计算(RC)框架中一种特殊的未经训练的循环神经网络(RNNs)，因其快速高效的学习而广受欢迎。然而，传统ESNs通常在长期信息处理方面存在困难。在本文中，我们介绍了一种基于时间残差连接的新型深度未经训练的RNNs，称为深度残差回声状态网络(DeepResESNs)。我们证明，利用层次化的未经训练的残差循环层显著提高了记忆能力和长期时间建模能力。对于时间残差连接，我们考虑了不同的正交配置，包括随机生成的和固定结构的配置，并研究了它们对网络动力学的影响。彻底的数学分析概述了确保DeepResESN稳定动力学的必要和充分条件。我们在各种时间序列任务上的实验展示了所提出的方法相对于传统浅层和深度RC的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Echo State Networks (ESNs) are a particular type of untrained RecurrentNeural Networks (RNNs) within the Reservoir Computing (RC) framework, popularfor their fast and efficient learning. However, traditional ESNs often strugglewith long-term information processing. In this paper, we introduce a novelclass of deep untrained RNNs based on temporal residual connections, calledDeep Residual Echo State Networks (DeepResESNs). We show that leveraging ahierarchy of untrained residual recurrent layers significantly boosts memorycapacity and long-term temporal modeling. For the temporal residualconnections, we consider different orthogonal configurations, includingrandomly generated and fixed-structure configurations, and we study theireffect on network dynamics. A thorough mathematical analysis outlines necessaryand sufficient conditions to ensure stable dynamics within DeepResESN. Ourexperiments on a variety of time series tasks showcase the advantages of theproposed approach over traditional shallow and deep RC.</description>
      <author>example@mail.com (Matteo Pinna, Andrea Ceni, Claudio Gallicchio)</author>
      <guid isPermaLink="false">2508.21172v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title>
      <link>http://arxiv.org/abs/2508.19650v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Video-LevelGauge基准，专门用于系统评估大型视频语言模型中的位置偏差问题。该基准通过标准化探测和定制化上下文设置，能够灵活控制评估参数，包含438个精心策划的视频和大量问题，并评估了27个最先进的LVLM模型。&lt;h4&gt;背景&lt;/h4&gt;大型视频语言模型在视频理解方面取得了显著进展，相应的评估基准也在不断发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了位置偏差这一关键但未被充分探索的方面。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的基准系统，用于系统评估LVLM中的位置偏差，揭示模型在处理不同位置信息时的细微行为差异。&lt;h4&gt;方法&lt;/h4&gt;使用标准化探测和定制化上下文设置，灵活控制上下文长度、探测位置和上下文类型；引入结合统计测量和形态模式识别的综合分析方法；构建包含438个多类型视频的基准数据集，产生1,177个多项选择题和120个开放性问题；评估27个最先进的LVLM模型。&lt;h4&gt;主要发现&lt;/h4&gt;许多领先的开源模型表现出显著的位置偏差，通常倾向于头部或邻近内容；商业模型如Gemini2.5-Pro在整个视频序列中表现出一致的性能；对上下文长度、变化和模型规模的分析提供了减轻偏差的可行见解。&lt;h4&gt;结论&lt;/h4&gt;Video-LevelGauge基准有效揭示了LVLM中的位置偏差问题，为模型改进和偏差减轻提供了有价值的指导，有助于推动更公平、更全面的视频语言模型评估。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型在视频理解方面取得了显著进展，推动了相应评估基准的发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了细微行为，如上下文位置偏差，这是LVLM性能中一个关键但未被充分探索的方面。我们提出了Video-LevelGauge，一个专门用于系统评估LVLM中位置偏差的基准。我们采用标准化探测和定制化上下文设置，能够灵活控制上下文长度、探测位置和上下文类型，以模拟多样化的现实场景。此外，我们引入了一种结合统计测量和形态模式识别的综合分析方法来表征偏差。我们的基准包含438个人工策划的视频，涵盖多种类型，产生了1,177个高质量多项选择题和120个开放性问题，这些问题的有效性经过验证，能够有效暴露位置偏差。基于这些，我们评估了27个最先进的LVLM，包括商业和开源模型。我们的发现显示，许多领先的开源模型存在显著的位置偏差，通常表现出头部或邻近内容偏好。相比之下，商业模型如Gemini2.5-Pro在整个视频序列中表现出令人印象深刻的一致性能。对上下文长度、上下文变化和模型规模的进一步分析提供了减轻偏差和指导模型增强的可行见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding model enhancement .https://github.com/Cola-any/Video-LevelGauge</description>
      <author>example@mail.com (Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang)</author>
      <guid isPermaLink="false">2508.19650v3</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering</title>
      <link>http://arxiv.org/abs/2508.21773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The 36th British Machine Vision Conference (BMVC 2025),  Sheffield, UK&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种无监督视频持续学习的现实场景和解决方案，处理在学习一系列任务时既不提供任务边界也不提供标签的情况，通过非参数方法显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;视频是复杂且丰富的时空媒体信息，在许多应用中广泛使用，但在无监督持续学习中尚未得到充分探索。先前研究只专注于监督持续学习，依赖于标签和任务边界，而获取标记数据成本高昂且不切实际。与图像相比，视频处理带来额外的计算和内存需求，使无监督视频持续学习(uVCL)面临更多挑战。&lt;h4&gt;目的&lt;/h4&gt;研究无监督视频持续学习(uVCL)问题，填补现有研究空白，提出一种不依赖标签或任务边界的视频持续学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出通用基准实验协议，使用无监督视频Transformer网络提取深度嵌入视频特征的核密度估计(KDE)作为非参数概率表示。引入针对新任务数据的异常检测标准，动态扩展内存簇捕获新知识，并利用从先前任务迁移学习作为当前任务知识转移的初始状态。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在连续学习多个任务时显著提高了模型的性能，在三个标准视频动作识别数据集(UCF101、HMDB51和Something-to-Something V2)上进行了深入评估，未使用任何标签或类别边界。&lt;h4&gt;结论&lt;/h4&gt;该研究为无监督视频持续学习提供了新的解决方案，通过非参数方法和动态内存簇扩展，实现了在不使用标签或类别边界的情况下学习视频数据。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了无监督视频学习的一种现实场景，在学习一系列任务时既不提供任务边界也不提供标签。我们还为未充分探索的无监督视频持续学习问题提供了一种非参数解决方案。视频代表了复杂而丰富的时空媒体信息，在许多应用中广泛使用，但在无监督持续学习中尚未得到充分探索。先前的研究只专注于监督持续学习，依赖于标签和任务边界知识，而拥有标记数据成本高昂且不切实际。为了解决这一差距，我们研究了无监督视频持续学习(uVCL)。与图像相比，uVCL由于处理视频的额外计算和内存需求而带来了更多挑战。我们通过考虑在每个任务中学习非结构化视频数据类别，为uVCL引入了一种通用的基准实验协议。我们提议使用由无监督视频Transformer网络提取的深度嵌入视频特征的核密度估计(KDE)作为数据的非参数概率表示。我们为传入的新任务数据引入了一种异常检测标准，动态启用内存簇的扩展，旨在在学习一系列任务时捕获新知识。我们利用从先前任务迁移学习作为知识转移到当前学习任务的初始状态。我们发现，当连续学习许多任务时，所提出的方法显著提高了模型的性能。我们在三个标准视频动作识别数据集(UCF101、HMDB51和Something-to-Something V2)上进行了深入评估，没有使用任何标签或类别边界。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a realistic scenario for the unsupervised video learning whereneither task boundaries nor labels are provided when learning a succession oftasks. We also provide a non-parametric learning solution for theunder-explored problem of unsupervised video continual learning. Videosrepresent a complex and rich spatio-temporal media information, widely used inmany applications, but which have not been sufficiently explored inunsupervised continual learning. Prior studies have only focused on supervisedcontinual learning, relying on the knowledge of labels and task boundaries,while having labeled data is costly and not practical. To address this gap, westudy the unsupervised video continual learning (uVCL). uVCL raises morechallenges due to the additional computational and memory requirements ofprocessing videos when compared to images. We introduce a general benchmarkexperimental protocol for uVCL by considering the learning of unstructuredvideo data categories during each task. We propose to use the Kernel DensityEstimation (KDE) of deep embedded video features extracted by unsupervisedvideo transformer networks as a non-parametric probabilistic representation ofthe data. We introduce a novelty detection criterion for the incoming new taskdata, dynamically enabling the expansion of memory clusters, aiming to capturenew knowledge when learning a succession of tasks. We leverage the use oftransfer learning from the previous tasks as an initial state for the knowledgetransfer to the current learning task. We found that the proposed methodologysubstantially enhances the performance of the model when successively learningmany tasks. We perform in-depth evaluations on three standard video actionrecognition datasets, including UCF101, HMDB51, and Something-to-Something V2,without using any labels or class boundaries.</description>
      <author>example@mail.com (Nattapong Kurpukdee, Adrian G. Bors)</author>
      <guid isPermaLink="false">2508.21773v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study</title>
      <link>http://arxiv.org/abs/2508.21622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种集成框架，结合传统网络优化模型与大语言模型，为供应链规划提供交互式、可解释和角色感知的决策支持。&lt;h4&gt;背景&lt;/h4&gt;复杂运筹学输出与商业利益相关者理解之间存在差距，需要更直观的方式来呈现决策支持信息。&lt;h4&gt;目的&lt;/h4&gt;弥合复杂运筹学模型与商业用户理解之间的鸿沟，通过自然语言摘要、情境可视化和定制化关键绩效指标提高决策支持效果。&lt;h4&gt;方法&lt;/h4&gt;开发核心优化模型解决多期多项目分销中心网络中的战术库存重新分配问题，采用混合整数公式；技术架构整合AI代理、RESTful API和动态用户界面，支持实时交互、配置更新和基于模拟的洞察。&lt;h4&gt;主要发现&lt;/h4&gt;案例研究表明，该系统能够防止缺货、降低成本并维持服务水平，显著改善了规划结果。&lt;h4&gt;结论&lt;/h4&gt;该框架有效结合了优化技术与自然语言处理能力，提高了供应链决策的可解释性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种集成框架，结合传统网络优化模型与大语言模型，为供应链规划提供交互式、可解释和角色感知的决策支持。该系统通过生成自然语言摘要、情境可视化和定制化关键绩效指标，弥合了复杂运筹学输出与商业利益相关者理解之间的差距。核心优化模型解决了多期多项目分销中心网络中的战术库存重新分配问题，采用混合整数公式。技术架构整合了AI代理、RESTful API和动态用户界面，支持实时交互、配置更新和基于模拟的洞察。案例研究表明，该系统通过防止缺货、降低成本和维持服务水平改善了规划结果。未来扩展包括集成私有LLM、迁移学习、强化学习和贝叶斯神经网络，以提高可解释性、适应性和实时决策能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an integrated framework that combines traditional networkoptimization models with large language models (LLMs) to deliver interactive,explainable, and role-aware decision support for supply chain planning. Theproposed system bridges the gap between complex operations research outputs andbusiness stakeholder understanding by generating natural language summaries,contextual visualizations, and tailored key performance indicators (KPIs). Thecore optimization model addresses tactical inventory redistribution across anetwork of distribution centers for multi-period and multi-item, using amixed-integer formulation. The technical architecture incorporates AI agents,RESTful APIs, and a dynamic user interface to support real-time interaction,configuration updates, and simulation-based insights. A case study demonstrateshow the system improves planning outcomes by preventing stockouts, reducingcosts, and maintaining service levels. Future extensions include integratingprivate LLMs, transfer learning, reinforcement learning, and Bayesian neuralnetworks to enhance explainability, adaptability, and real-timedecision-making.</description>
      <author>example@mail.com (Saravanan Venkatachalam)</author>
      <guid isPermaLink="false">2508.21622v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Adapting to Change: A Comparison of Continual and Transfer Learning for Modeling Building Thermal Dynamics under Concept Drifts</title>
      <link>http://arxiv.org/abs/2508.21615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在有限数据情况下，如何利用迁移学习和持续学习改进建筑热动力学模型的预测准确性，特别是在建筑特性随时间变化的情况下。&lt;h4&gt;背景&lt;/h4&gt;迁移学习(TL)是目前在仅有有限数据情况下建模建筑热动力学的最有效方法，但初始微调后如何继续更新模型仍不清楚，尤其是在建筑特性发生变化(如改造或占用情况改变)时。&lt;h4&gt;目的&lt;/h4&gt;比较持续学习(CL)和迁移学习(TL)策略以及从头开始训练的模型，用于建筑运行期间的热动力学建模，并研究如何整合新测量数据以提高预测准确性，应对概念漂移的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用5-7年中欧单户住宅的模拟数据评估方法，包括改造和占用变化带来的概念漂移场景，并提出了一种名为'季节性记忆学习'(SML)的持续学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;SML策略比现有CL和TL方法提供更大的准确性改进，同时保持较低的计算工作量。无概念漂移时比初始微调提高28.1%，有概念漂移时提高34.9%。&lt;h4&gt;结论&lt;/h4&gt;季节性记忆学习是一种有效方法，能持续整合新测量数据，提高建筑热动力学模型预测准确性，特别是在建筑特性变化情况下。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习(TL)是目前在仅有有限数据情况下建模建筑热动力学的最有效方法。TL使用预训练模型并微调以适应特定目标建筑。然而，随着更多运行测量数据的收集，特别是在建筑特性变化后，如何在初始微调后继续更新模型仍不清楚。机器学习文献中，持续学习(CL)方法用于更新变化系统的模型。TL方法也可以通过在每个更新步骤重用预训练模型并用新测量数据微调来解决这一挑战。关于如何随时间整合新测量数据以提高预测准确性并应对建筑热动力学中概念漂移的挑战，仍缺乏全面研究。因此，本研究比较了几种CL和TL策略以及从头开始训练的模型，用于建筑运行期间的热动力学建模。使用5-7年中欧单户住宅的代表性模拟数据评估这些方法，包括改造和占用变化带来的概念漂移场景。我们提出了一种CL策略(季节性记忆学习)，它比现有CL和TL方法提供更大的准确性改进，同时保持较低的计算工作量。无概念漂移时，SML比初始微调的基准提高28.1%；有概念漂移时提高34.9%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer Learning (TL) is currently the most effective approach for modelingbuilding thermal dynamics when only limited data are available. TL uses apretrained model that is fine-tuned to a specific target building. However, itremains unclear how to proceed after initial fine-tuning, as more operationalmeasurement data are collected over time. This challenge becomes even morecomplex when the dynamics of the building change, for example, after a retrofitor a change in occupancy. In Machine Learning literature, Continual Learning(CL) methods are used to update models of changing systems. TL approaches canalso address this challenge by reusing the pretrained model at each update stepand fine-tuning it with new measurement data. A comprehensive study on how toincorporate new measurement data over time to improve prediction accuracy andaddress the challenges of concept drifts (changes in dynamics) for buildingthermal dynamics is still missing.  Therefore, this study compares several CL and TL strategies, as well as amodel trained from scratch, for thermal dynamics modeling during buildingoperation. The methods are evaluated using 5--7 years of simulated datarepresentative of single-family houses in Central Europe, including scenarioswith concept drifts from retrofits and changes in occupancy. We propose a CLstrategy (Seasonal Memory Learning) that provides greater accuracy improvementsthan existing CL and TL methods, while maintaining low computational effort.SML outperformed the benchmark of initial fine-tuning by 28.1\% without conceptdrifts and 34.9\% with concept drifts.</description>
      <author>example@mail.com (Fabian Raisch, Max Langtry, Felix Koch, Ruchi Choudhary, Christoph Goebel, Benjamin Tischler)</author>
      <guid isPermaLink="false">2508.21615v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Recabilities of Foundation Models: A Multi-Domain, Multi-Dataset Benchmark</title>
      <link>http://arxiv.org/abs/2508.21354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了RecBench-MD基准，对19个基础模型在15个数据集和10个不同领域的推荐能力进行了全面评估，发现领域内微调效果最佳，跨数据集迁移学习对新场景有实用价值，多领域训练能显著提升模型适应性。&lt;h4&gt;背景&lt;/h4&gt;现有基础模型在不同数据集和领域中的推荐能力综合评估对于推进推荐基础模型的发展至关重要。&lt;h4&gt;目的&lt;/h4&gt;引入RecBench-MD基准，从零资源、多数据集和多领域的角度评估基础模型的推荐能力。&lt;h4&gt;方法&lt;/h4&gt;通过对跨越10个不同领域（包括电子商务、娱乐和社交媒体）的15个数据集中的19个基础模型进行广泛评估，确定这些模型在推荐任务中的关键特征。&lt;h4&gt;主要发现&lt;/h4&gt;领域内微调可实现最佳性能，跨数据集迁移学习为新推荐场景提供了有效的实际支持，多领域训练显著增强了基础模型的适应性。&lt;h4&gt;结论&lt;/h4&gt;所有代码和数据已公开发布，以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;对现有基础模型在不同数据集和领域中的推荐能力进行综合评估，对于推进推荐基础模型的发展至关重要。在本研究中，我们引入了RecBench-MD，这是一个新颖且全面的基准，旨在从零资源、多数据集和多领域的角度评估基础模型的推荐能力。通过对跨越10个不同领域（包括电子商务、娱乐和社交媒体）的15个数据集中的19个基础模型进行广泛评估，我们确定了这些模型在推荐任务中的关键特征。我们的研究结果表明，领域内微调可实现最佳性能，而跨数据集迁移学习为新推荐场景提供了有效的实际支持。此外，我们观察到多领域训练显著增强了基础模型的适应性。所有代码和数据已公开发布，以促进未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehensive evaluation of the recommendation capabilities of existingfoundation models across diverse datasets and domains is essential foradvancing the development of recommendation foundation models. In this study,we introduce RecBench-MD, a novel and comprehensive benchmark designed toassess the recommendation abilities of foundation models from a zero-resource,multi-dataset, and multi-domain perspective. Through extensive evaluations of19 foundation models across 15 datasets spanning 10 diverse domains --including e-commerce, entertainment, and social media -- we identify keycharacteristics of these models in recommendation tasks. Our findings suggestthat in-domain fine-tuning achieves optimal performance, while cross-datasettransfer learning provides effective practical support for new recommendationscenarios. Additionally, we observe that multi-domain training significantlyenhances the adaptability of foundation models. All code and data have beenpublicly released to facilitate future research.</description>
      <author>example@mail.com (Qijiong Liu, Jieming Zhu, Yingxin Lai, Xiaoyu Dong, Lu Fan, Zhipeng Bian, Zhenhua Dong, Xiao-Ming Wu)</author>
      <guid isPermaLink="false">2508.21354v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</title>
      <link>http://arxiv.org/abs/2508.18860v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了C-Flat方法，用于在持续学习中促进平坦的损失景观，以平衡对新任务的敏感性和保留过去知识的稳定性。作者还提出了C-Flat++框架，通过选择性平坦度驱动促进显著减少了C-Flat所需的更新成本。&lt;h4&gt;背景&lt;/h4&gt;在持续学习中，平衡对新任务的敏感性和保留过去知识的稳定性至关重要。锐度感知最小化在迁移学习中已被证明有效，并已被应用于持续学习中以提高记忆保留和学习效率。然而，仅依赖零阶锐度可能在某些情况下 favor 更尖锐的最小值而非更平坦的最小值，导致解决方案不够稳健且可能是次优的。&lt;h4&gt;目的&lt;/h4&gt;提出一种促进平坦损失景观的方法，专门针对持续学习进行优化，解决仅依赖零阶锐度可能带来的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了C-Flat(Continual Flatness)方法，促进为持续学习量身定制的平坦损失景观。C-Flat提供即插即用的兼容性，可以轻松集成到代码管道中，只需进行最小修改。此外，作者提出了一个通用框架，将C-Flat集成到所有主要的持续学习范式中，并与损失最小值优化器和基于平坦最小值的持续学习方法进行了全面比较。作者还引入了C-Flat++，这是一个高效且有效的框架，利用选择性平坦度驱动促进，显著减少了C-Flat所需的更新成本。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，C-Flat在各种设置中都能持续提高性能。此外，在多种持续学习方法、数据集和场景进行的广泛实验证明了所提出方法的有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;C-Flat和C-Flat++是持续学习中的有效方法，能够提高性能并减少计算成本。&lt;h4&gt;翻译&lt;/h4&gt;在持续学习中平衡对新任务的敏感性和保留过去知识的稳定性至关重要。最近，锐度感知最小化在迁移学习中已被证明有效，并也被采用到持续学习中以提高记忆保留和学习效率。然而，在某些情况下仅依赖零阶锐度可能 favor 更尖锐的最小值而非更平坦的最小值，导致解决方案不够稳健且可能是次优的。在本文中，我们提出了C-Flat(Continual Flatness)方法，促进为持续学习量身定制的平坦损失景观。C-Flat提供即插即用的兼容性，可以轻松集成到代码管道中，只需进行最小修改。此外，我们提出了一个通用框架，将C-Flat集成到所有主要的持续学习范式中，并与损失最小值优化器和基于平坦最小值的持续学习方法进行了全面比较。我们的结果表明，C-Flat在各种设置中都能持续提高性能。此外，我们引入了C-Flat++，这是一个高效且有效的框架，利用选择性平坦度驱动促进，显著减少了C-Flat所需的更新成本。在多种持续学习方法、数据集和场景进行的广泛实验证明了我们提出方法的有效性和效率。代码可在https://github.com/WanNaa/C-Flat获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Balancing sensitivity to new tasks and stability for retaining past knowledgeis crucial in continual learning (CL). Recently, sharpness-aware minimizationhas proven effective in transfer learning and has also been adopted incontinual learning (CL) to improve memory retention and learning efficiency.However, relying on zeroth-order sharpness alone may favor sharper minima overflatter ones in certain settings, leading to less robust and potentiallysuboptimal solutions. In this paper, we propose \textbf{C}ontinual\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter losslandscapes tailored for CL. C-Flat offers plug-and-play compatibility, enablingeasy integration with minimal modifications to the code pipeline. Besides, wepresent a general framework that integrates C-Flat into all major CL paradigmsand conduct comprehensive comparisons with loss-minima optimizers andflat-minima-based CL methods. Our results show that C-Flat consistentlyimproves performance across a wide range of settings. In addition, we introduceC-Flat++, an efficient yet effective framework that leverages selectiveflatness-driven promotion, significantly reducing the update cost required byC-Flat. Extensive experiments across multiple CL methods, datasets, andscenarios demonstrate the effectiveness and efficiency of our proposedapproaches. Code is available at https://github.com/WanNaa/C-Flat.</description>
      <author>example@mail.com (Wei Li, Hangjie Yuan, Zixiang Zhao, Yifan Zhu, Aojun Lu, Tao Feng, Yanan Sun)</author>
      <guid isPermaLink="false">2508.18860v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>CE-RS-SBCIT A Novel Channel Enhanced Hybrid CNN Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis</title>
      <link>http://arxiv.org/abs/2508.17128v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 Pages, 12 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一种新型混合框架CE-RS-SBCIT，用于脑肿瘤的MRI图像检测和分类，解决了传统方法的计算成本高、对微小对比度变化敏感、结构异质性和纹理不一致等问题。&lt;h4&gt;背景&lt;/h4&gt;脑肿瘤是最致命的人类疾病之一，早期检测和准确分类对有效诊断和治疗计划至关重要。虽然基于深度学习的计算机辅助诊断(CADx)系统已显示出显著进展，但传统卷积神经网络(CNNs)和Transformer仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决传统CNNs和Transformer在脑肿瘤诊断中面临的挑战，包括高计算成本、敏感性、结构异质性和纹理不一致问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为CE-RS-SBCIT的新型混合框架，整合了基于残差和空间学习的CNNs与基于Transformer的模块。该框架包含四个核心创新：平滑和基于边界的CNN集成Transformer(SBCIT)、定制的残差和空间学习CNNs、通道增强(CE)策略和新空间注意力机制。在Kaggle和Figshare上的具有挑战性的MRI数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;框架利用局部细粒度和全局上下文线索，通过SBCIT实现高效全局特征建模，增强的残差和空间CNNs丰富了表示空间，CE模块放大了判别性通道并减少冗余，空间注意力机制强调不同肿瘤类别间的细微对比度和纹理变化。&lt;h4&gt;结论&lt;/h4&gt;该框架在具有挑战性的MRI数据集上表现出卓越的性能，达到了98.30%的准确率、98.08%的敏感性、98.25%的F1分数和98.43%的精确度。&lt;h4&gt;翻译&lt;/h4&gt;脑肿瘤仍然是最致命的人类疾病之一，早期检测和准确分类对有效诊断和治疗计划至关重要。尽管基于深度学习的计算机辅助诊断(CADx)系统已显示出显著进展，然而传统卷积神经网络(CNNs)和Transformer持续面临挑战，包括高计算成本、对微小对比度变化的敏感性、MRI数据中的结构异质性和纹理不一致性。因此，引入了一种新型混合框架CE-RS-SBCIT，整合了基于残差和空间学习的CNNs与基于Transformer的模块。所提出的框架通过四个核心创新利用局部细粒度和全局上下文线索：(i)一种平滑和基于边界的CNN集成Transformer(SBCIT)，(ii)定制的残差和空间学习CNNs，(iii)一种通道增强(CE)策略，以及(iv)一种新的空间注意力机制。开发的SBCIT采用主干卷积和上下文交互Transformer块，具有系统化的平滑和边界操作，能够实现高效的全局特征建模。此外，通过辅助迁移学习的特征图增强的残差和空间CNNs丰富了表示空间，而CE模块放大了判别性通道并减少了冗余。进一步地，空间注意力机制选择性地强调了不同肿瘤类别之间的细微对比度和纹理变化。在Kaggle和Figshare上具有挑战性的MRI数据集上的广泛评估，包括胶质瘤、脑膜瘤、垂体肿瘤和健康对照组，展示了卓越的性能，达到了98.30%的准确率、98.08%的敏感性、98.25%的F1分数和98.43%的精确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain tumors remain among the most lethal human diseases, where earlydetection and accurate classification are critical for effective diagnosis andtreatment planning. Although deep learning-based computer-aided diagnostic(CADx) systems have shown remarkable progress. However, conventionalconvolutional neural networks (CNNs) and Transformers face persistentchallenges, including high computational cost, sensitivity to minor contrastvariations, structural heterogeneity, and texture inconsistencies in MRI data.Therefore, a novel hybrid framework, CE-RS-SBCIT, is introduced, integratingresidual and spatial learning-based CNNs with transformer-driven modules. Theproposed framework exploits local fine-grained and global contextual cuesthrough four core innovations: (i) a smoothing and boundary-basedCNN-integrated Transformer (SBCIT), (ii) tailored residual and spatial learningCNNs, (iii) a channel enhancement (CE) strategy, and (iv) a novel spatialattention mechanism. The developed SBCIT employs stem convolution andcontextual interaction transformer blocks with systematic smoothing andboundary operations, enabling efficient global feature modeling. Moreover,Residual and spatial CNNs, enhanced by auxiliary transfer-learned feature maps,enrich the representation space, while the CE module amplifies discriminativechannels and mitigates redundancy. Furthermore, the spatial attention mechanismselectively emphasizes subtle contrast and textural variations across tumorclasses. Extensive evaluation on challenging MRI datasets from Kaggle andFigshare, encompassing glioma, meningioma, pituitary tumors, and healthycontrols, demonstrates superior performance, achieving 98.30% accuracy, 98.08%sensitivity, 98.25% F1-score, and 98.43% precision.</description>
      <author>example@mail.com (Mirza Mumtaz Zahoor, Saddam Hussain Khan)</author>
      <guid isPermaLink="false">2508.17128v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</title>
      <link>http://arxiv.org/abs/2508.21816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究场景识别中的动词分类问题，指出传统单标签方法无法处理视觉事件识别中的固有歧义，提出单正多标签学习框架和图增强动词多层感知器模型，在实验中取得超过3%的MAP提升。&lt;h4&gt;背景&lt;/h4&gt;场景识别是计算机视觉的基本任务，旨在通过识别关键事件及其相关实体从图像中提取结构化语义摘要。现有方法将动词分类视为单标签问题，但这种方法无法处理视觉事件识别中的固有歧义。&lt;h4&gt;目的&lt;/h4&gt;解决场景识别中动词分类的单标签问题，更好地处理视觉事件识别中的语义歧义。&lt;h4&gt;方法&lt;/h4&gt;1. 揭示动词分类本质上是多标签问题；2. 提出将动词分类重新表述为单正多标签学习问题；3. 设计多标签评估基准；4. 开发图增强动词多层感知器模型，结合图神经网络捕捉标签相关性，使用对抗训练优化决策边界。&lt;h4&gt;主要发现&lt;/h4&gt;1. 动词分类本质上是多标签问题，因为动词类别之间存在普遍的语义重叠；2. 完全用多标签标注大规模数据集不切实际；3. 提出的方法在真实世界数据集上实现超过3%的MAP提升，同时在传统准确率指标上保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;通过将动词分类重新表述为单正多标签学习问题，并设计相应模型和评估基准，可更有效地处理视觉事件识别中的语义歧义，提高场景识别性能。&lt;h4&gt;翻译&lt;/h4&gt;场景识别是计算机视觉中的一个基本任务，旨在通过识别关键事件及其相关实体从图像中提取结构化的语义摘要。具体来说，给定输入图像，模型必须首先分类主要的视觉事件（动词分类），然后识别参与实体及其语义角色（语义角色标注），最后在图像中定位这些实体（语义角色定位）。现有方法将动词分类视为单标签问题，但我们通过全面分析表明，这种表述无法解决视觉事件识别中的固有歧义，因为多个动词类别都可以合理地描述同一图像。本文有三个关键贡献：首先，我们通过实证分析揭示动词分类本质上是多标签问题，因为动词类别之间存在普遍的语义重叠。其次，考虑到完全用多标签标注大规模数据集的不切实际性，我们提出将动词分类重新表述为单正多标签学习问题——这是场景识别研究中的一个新视角。第三，我们设计了一个全面的多标签评估基准，经过精心设计，可以公平评估模型在多标签设置下的性能。为解决单正多标签学习的挑战，我们进一步开发了图增强动词多层感知器，它结合图神经网络来捕捉标签相关性，并使用对抗训练来优化决策边界。在真实世界数据集上的大量实验表明，我们的方法实现了超过3%的MAP提升，同时在传统的top-1和top-5准确率指标上保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Context recognition (SR) is a fundamental task in computer vision that aimsto extract structured semantic summaries from images by identifying key eventsand their associated entities. Specifically, given an input image, the modelmust first classify the main visual events (verb classification), then identifythe participating entities and their semantic roles (semantic role labeling),and finally localize these entities in the image (semantic role localization).Existing methods treat verb classification as a single-label problem, but weshow through a comprehensive analysis that this formulation fails to addressthe inherent ambiguity in visual event recognition, as multiple verb categoriesmay reasonably describe the same image. This paper makes three keycontributions: First, we reveal through empirical analysis that verbclassification is inherently a multi-label problem due to the ubiquitoussemantic overlap between verb categories. Second, given the impracticality offully annotating large-scale datasets with multiple labels, we propose toreformulate verb classification as a single positive multi-label learning(SPMLL) problem - a novel perspective in SR research. Third, we design acomprehensive multi-label evaluation benchmark for SR that is carefullydesigned to fairly evaluate model performance in a multi-label setting. Toaddress the challenges of SPMLL, we futher develop the Graph Enhanced VerbMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks tocapture label correlations and adversarial training to optimize decisionboundaries. Extensive experiments on real-world datasets show that our approachachieves more than 3\% MAP improvement while remaining competitive ontraditional top-1 and top-5 accuracy metrics.</description>
      <author>example@mail.com (Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin)</author>
      <guid isPermaLink="false">2508.21816v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature</title>
      <link>http://arxiv.org/abs/2508.21513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过图里奇曲率的几何视角解释了图神经网络在解决布尔可满足性问题时性能下降的现象，发现随机k-SAT公式的二部图本质上是负曲率的，且曲率随问题难度增加而减小，导致GNN求解器出现过度压缩现象。&lt;h4&gt;背景&lt;/h4&gt;图神经网络最近被证明可以解决布尔可满足性问题，它们通过操作逻辑公式的图表示来实现。然而，图神经网络在解决更困难的问题实例时性能急剧下降。&lt;h4&gt;目的&lt;/h4&gt;探究图神经网络在解决SAT问题时性能下降的根本原因，特别是这是否反映了基本架构局限性。&lt;h4&gt;方法&lt;/h4&gt;使用图里奇曲率作为几何分析工具，量化局部连接瓶颈，分析随机k-SAT公式推导出的二部图的曲率特性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 从随机k-SAT公式推导出的二部图本质上是负曲率的；2) 这种曲率随着问题实例难度的增加而减小；3) 基于图神经网络的SAT求解器受到过度压缩现象的影响，使得长程依赖关系无法被压缩到固定长度表示中；4) 曲率是问题复杂性的强指标，可用于预测求解器性能。&lt;h4&gt;结论&lt;/h4&gt;图里奇曲率提供了理解图神经网络在SAT问题上性能局限性的新视角，这些发现与现有求解器的设计原则相关，并为未来工作指明了有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近通过操作逻辑公式的图表示，展示了作为布尔可满足性问题求解器的潜力。然而，它们在更困难的问题实例上性能急剧下降，引发了这是否反映了基本架构局限性的问题。在本工作中，我们通过图里奇曲率的几何视角提供了解释，图里奇曲率量化了局部连接瓶颈。我们证明从随机k-SAT公式推导出的二部图本质上是负曲率的，并且这种曲率随着问题实例难度的增加而减小。基于此，我们表明基于图神经网络的SAT求解器受到过度压缩的影响，这是一种长程依赖关系无法被压缩到固定长度表示中的现象。我们在不同的SAT基准上通过经验验证了我们的主张，并确认曲率既是问题复杂性的强指标，也可用于预测性能。最后，我们将这些发现与现有求解器的设计原则联系起来，并概述了未来工作的有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently shown promise as solvers forBoolean Satisfiability Problems (SATs) by operating on graph representations oflogical formulas. However, their performance degrades sharply on harderinstances, raising the question of whether this reflects fundamentalarchitectural limitations. In this work, we provide a geometric explanationthrough the lens of graph Ricci Curvature (RC), which quantifies localconnectivity bottlenecks. We prove that bipartite graphs derived from randomk-SAT formulas are inherently negatively curved, and that this curvaturedecreases with instance difficulty. Building on this, we show that GNN-basedSAT solvers are affected by oversquashing, a phenomenon where long-rangedependencies become impossible to compress into fixed-length representations.We validate our claims empirically across different SAT benchmarks and confirmthat curvature is both a strong indicator of problem complexity and can be usedto predict performance. Finally, we connect our findings to design principlesof existing solvers and outline promising directions for future work.</description>
      <author>example@mail.com (Geri Skenderi)</author>
      <guid isPermaLink="false">2508.21513v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics</title>
      <link>http://arxiv.org/abs/2508.21249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的元学习框架，通过专家混合模型(MoE)动态结合三种不同的神经网络架构，创建更准确、更强大的复合代理模型，用于汽车空气动力学的高保真CFD模拟预测。&lt;h4&gt;背景&lt;/h4&gt;高保真CFD模拟的计算成本在汽车设计和优化周期中仍然是一个显著的瓶颈。虽然基于机器学习的代理模型已成为加速空气动力学预测的有前途的替代方案，但该领域具有多样化的、快速发展的专业神经网络架构，没有单一模型显示出普遍的优越性。&lt;h4&gt;目的&lt;/h4&gt;提出一种元学习框架，利用架构多样性作为优势，通过动态组合不同专家模型的预测，创建更准确、更强大的复合代理模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种专家混合(MoE)模型，采用专用的门控网络来动态和最优地组合三种异构的、最先进的代理模型：DoMINO(可分解的多尺度神经算子)、X-MeshGraphNet(可扩展的多尺度图神经网络)和FigConvNet(分解的隐式全局卷积网络)。门控网络学习空间变化的加权策略，根据每个专家在预测表面压力和壁面剪应力场方面的局部性能分配可信度。为防止模型崩溃并鼓励专家平衡贡献，在训练损失函数中集成了熵正则化项。整个系统在DrivAerML数据集上进行训练和验证。&lt;h4&gt;主要发现&lt;/h4&gt;定量结果表明，MoE模型在预测误差方面实现了显著减少，不仅在所有评估的物理量上优于集成平均，而且优于最准确的单个专家模型。&lt;h4&gt;结论&lt;/h4&gt;这项研究建立了MoE框架作为一种强大而有效的策略，通过协同结合专业架构的互补优势，创建更强大、更准确的复合代理模型。&lt;h4&gt;翻译&lt;/h4&gt;与高保真CFD模拟相关的计算成本仍然是汽车设计和优化周期中的一个显著瓶颈。虽然基于机器学习的代理模型已成为加速空气动力学预测的一个有前途的替代方案，但该领域的特点是多样化的、快速发展的专业神经网络架构，没有单一模型显示出普遍的优越性。本文引入了一种新颖的元学习框架，利用这种架构多样性作为优势。我们提出了一种专家混合(MoE)模型，采用专用的门控网络来动态和最优地组合三种异构的、最先进的代理模型的预测：DoMINO(可分解的多尺度神经算子)、X-MeshGraphNet(可扩展的多尺度图神经网络)和FigConvNet(分解的隐式全局卷积网络)。门控网络学习空间变化的加权策略，根据每个专家在预测表面压力和壁面剪应力场方面的局部性能分配可信度。为了防止模型崩溃并鼓励专家平衡贡献，我们在训练损失函数中集成了熵正则化项。整个系统在DrivAerML数据集上进行训练和验证，该数据集是一个大规模的、公开的高保真CFD模拟基准，用于汽车空气动力学。定量结果表明，MoE模型在预测误差方面实现了显著减少，不仅在所有评估的物理量上优于集成平均，而且优于最准确的单个专家模型。这项工作建立了MoE框架作为一种强大而有效的策略，通过协同结合专业架构的互补优势，创建更强大、更准确的复合代理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The computational cost associated with high-fidelity CFD simulations remainsa significant bottleneck in the automotive design and optimization cycle. WhileML-based surrogate models have emerged as a promising alternative to accelerateaerodynamic predictions, the field is characterized by a diverse and rapidlyevolving landscape of specialized neural network architectures, with no singlemodel demonstrating universal superiority. This paper introduces a novelmeta-learning framework that leverages this architectural diversity as astrength. We propose a Mixture of Experts (MoE) model that employs a dedicatedgating network to dynamically and optimally combine the predictions from threeheterogeneous, state-of-the-art surrogate models: DoMINO, a decomposablemulti-scale neural operator; X-MeshGraphNet, a scalable multi-scale graphneural network; and FigConvNet, a factorized implicit global convolutionnetwork. The gating network learns a spatially-variant weighting strategy,assigning credibility to each expert based on its localized performance inpredicting surface pressure and wall shear stress fields. To prevent modelcollapse and encourage balanced expert contributions, we integrate an entropyregularization term into the training loss function. The entire system istrained and validated on the DrivAerML dataset, a large-scale, public benchmarkof high-fidelity CFD simulations for automotive aerodynamics. Quantitativeresults demonstrate that the MoE model achieves a significant reduction in L-2prediction error, outperforming not only the ensemble average but also the mostaccurate individual expert model across all evaluated physical quantities. Thiswork establishes the MoE framework as a powerful and effective strategy forcreating more robust and accurate composite surrogate models by synergisticallycombining the complementary strengths of specialized architectures.</description>
      <author>example@mail.com (Mohammad Amin Nabian, Sanjay Choudhry)</author>
      <guid isPermaLink="false">2508.21249v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Molecular Machine Learning in Chemical Process Design</title>
      <link>http://arxiv.org/abs/2508.20527v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了分子机器学习在化学过程工程领域的应用，展示了其在物质性质预测和新分子结构探索方面的潜力，并讨论了将其整合到过程设计和优化中的方法。&lt;h4&gt;背景&lt;/h4&gt;分子机器学习在化学过程工程领域是一个新兴研究方向，已在纯物质及其混合物的性质预测和探索新分子结构方面显示出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;回顾当前最先进的分子机器学习模型，讨论进一步发展的研究方向，探索在化学过程规模上利用分子机器学习的可能性，并讨论如何将其整合到过程设计和优化公式中。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络和transformer等机器学习方法，通过混合或物理信息的方式整合物理化学知识，将分子机器学习整合到过程设计和优化公式中。&lt;h4&gt;主要发现&lt;/h4&gt;分子机器学习能提供高度准确的物质性质预测，能探索化学空间发现新分子结构，在化学过程规模上的应用是高度期望但尚未充分探索的领域，能加速新分子和过程的识别。&lt;h4&gt;结论&lt;/h4&gt;创建分子和过程设计基准并在实践中验证候选分子是充分发挥分子机器学习潜力的关键，这可能需要与化学工业合作。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了在化学过程工程领域分子机器学习（ML）的观点。最近，分子机器学习在（i）提供纯物质及其混合物的高度准确预测，和（ii）探索化学空间以发现新的分子结构方面显示出巨大潜力。我们回顾了当前最先进的分子机器学习模型，并讨论了有希望进一步发展的研究方向。包括机器学习方法，如图神经网络和transformer，可以通过混合或物理信息的方式整合物理化学知识以得到进一步发展。然后，我们考虑在化学过程规模上利用分子机器学习，这是高度期望但相当未探索的。我们讨论了如何将分子机器学习整合到过程设计和优化公式中，有望加速新分子和过程的识别。为此，创建分子和过程设计基准并在实践中验证候选分子将是至关重要的，可能需要与化学工业合作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a perspective on molecular machine learning (ML) in the field ofchemical process engineering. Recently, molecular ML has demonstrated greatpotential in (i) providing highly accurate predictions for properties of purecomponents and their mixtures, and (ii) exploring the chemical space for newmolecular structures. We review current state-of-the-art molecular ML modelsand discuss research directions that promise further advancements. Thisincludes ML methods, such as graph neural networks and transformers, which canbe further advanced through the incorporation of physicochemical knowledge in ahybrid or physics-informed fashion. Then, we consider leveraging molecular MLat the chemical process scale, which is highly desirable yet rather unexplored.We discuss how molecular ML can be integrated into process design andoptimization formulations, promising to accelerate the identification of novelmolecules and processes. To this end, it will be essential to create moleculeand process design benchmarks and practically validate proposed candidates,possibly in collaboration with the chemical industry.</description>
      <author>example@mail.com (Jan G. Rittig, Manuel Dahmen, Martin Grohe, Philippe Schwaller, Alexander Mitsos)</author>
      <guid isPermaLink="false">2508.20527v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Memorization in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19352v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Version2, With updated code repo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了NCMemo框架，用于量化半监督节点分类中的标签记忆。研究发现记忆与图同质性呈反比关系，低同质性增加GNN的记忆倾向。分析表明，低同质性图中增加的记忆与GNN使用图结构的隐式偏差相关，且特征空间邻域中标签不一致性高的节点更易被记忆。通过图重连方法可以有效减少记忆而不影响性能，同时降低隐私风险。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络(DNNs)已被证明能够记忆训练数据，但类似的分析对于图神经网络(GNNs)在很大程度上仍然未被探索。&lt;h4&gt;目的&lt;/h4&gt;介绍NCMemo（Node Classification Memorization），这是第一个用于量化半监督节点分类中标签记忆的框架。&lt;h4&gt;方法&lt;/h4&gt;建立记忆与图同质性之间的关系；分析GNN训练动态；研究特征空间邻域中标签不一致性对记忆的影响；探索图重连作为减少记忆的方法。&lt;h4&gt;主要发现&lt;/h4&gt;1. 同质性较低显著增加记忆，表明GNN依赖于记忆来学习同质性较低的图；2. 低同质性图中增加的记忆与GNN在学习过程中使用图结构的隐式偏差紧密相关；3. 在特征空间邻域中具有较高标签不一致性的节点更容易被记忆；4. 图重连方法可以有效减少记忆而不损害模型性能；5. 该方法降低了先前记忆数据点的隐私风险。&lt;h4&gt;结论&lt;/h4&gt;这项工作不仅推进了对GNN学习的理解，还支持了更多隐私保护的GNN部署。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络(DNNs)已被证明能够记忆其训练数据，然而类似的分析对于图神经网络(GNNs)在很大程度上仍然未被探索。我们引入了NCMemo（Node Classification Memorization），这是第一个用于量化半监督节点分类中标签记忆的框架。我们首先建立了记忆与图同质性之间的反比关系，即连接节点具有相似标签/属性的特性。我们发现较低的同质性显著增加了记忆，表明GNN依赖于记忆来学习同质性较低的图。其次，我们分析了GNN训练动态。我们发现低同质性图中增加的记忆与GNN在学习过程中使用图结构的隐式偏差紧密相关。在低同质性情况下，这种结构信息较少，因此导致节点标签的记忆以最小化训练损失。最后，我们表明在特征空间邻域中具有较高标签不一致性的节点更容易被记忆。基于我们对图同质性与记忆之间联系的理解，我们研究了图重连作为减少记忆的方法。我们的结果表明这种方法可以有效减少记忆而不损害模型性能。此外，我们表明它在实践中降低了先前记忆数据点的隐私风险。因此，我们的工作不仅推进了对GNN学习的理解，还支持了更多隐私保护的GNN部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have been shown to memorize their training data,yet similar analyses for graph neural networks (GNNs) remain largelyunder-explored. We introduce NCMemo (Node Classification Memorization), thefirst framework to quantify label memorization in semi-supervised nodeclassification. We first establish an inverse relationship between memorizationand graph homophily, i.e., the property that connected nodes share similarlabels/features. We find that lower homophily significantly increasesmemorization, indicating that GNNs rely on memorization to learn lesshomophilic graphs. Secondly, we analyze GNN training dynamics. We find that theincreased memorization in low homophily graphs is tightly coupled to the GNNs'implicit bias on using graph structure during learning. In low homophilyregimes, this structure is less informative, hence inducing memorization of thenode labels to minimize training loss. Finally, we show that nodes with higherlabel inconsistency in their feature-space neighborhood are significantly moreprone to memorization. Building on our insights into the link between graphhomophily and memorization, we investigate graph rewiring as a means tomitigate memorization. Our results demonstrate that this approach effectivelyreduces memorization without compromising model performance. Moreover, we showthat it lowers the privacy risk for previously memorized data points inpractice. Thus, our work not only advances understanding of GNN learning butalso supports more privacy-preserving GNN deployment.</description>
      <author>example@mail.com (Adarsh Jamadandi, Jing Xu, Adam Dziedzic, Franziska Boenisch)</author>
      <guid isPermaLink="false">2508.19352v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4</title>
      <link>http://arxiv.org/abs/2508.21169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了SYNBUILD-3D，一个包含超过620万个合成3D住宅建筑的大规模、多样化、多模态数据集，每个建筑通过三种模态表示，旨在解决自动生成准确且语义丰富的3D建筑模型的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D建筑模型在建筑、能源模拟和导航应用中至关重要，但由于缺乏大规模公开标注数据，自动生成准确且语义丰富的3D建筑仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、多样化的多模态3D建筑数据集，以促进自动生成准确且语义丰富的3D建筑模型的研究。&lt;h4&gt;方法&lt;/h4&gt;受计算机视觉中合成数据成功的启发，作者创建了SYNBUILD-3D数据集，包含超过620万个合成3D住宅建筑(LoD 4)，每个建筑通过三种模态表示：语义丰富的3D线框图、平面图图像和类似LiDAR的屋顶点云，并提供语义注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过SYNBUILD-3D的三模态特性，可以开发新的生成式AI算法，自动创建LoD 4级别的3D建筑模型，同时满足预定义的平面图布局和几何形状，并强制语义-几何一致性。&lt;h4&gt;结论&lt;/h4&gt;SYNBUILD-3D数据集的发布为3D建筑模型的自动生成研究提供了重要资源，数据集和代码样本已在GitHub上公开可用。&lt;h4&gt;翻译&lt;/h4&gt;三维建筑模型对于建筑、能源模拟和导航应用至关重要。然而，由于公共领域缺乏大规模标注数据集，自动生成准确且语义丰富的三维建筑仍然是一个重大挑战。受计算机视觉中合成数据成功的启发，我们引入了SYNBUILD-3D，这是一个大规模、多样化且多模态的数据集，包含超过620万个细节级别(LoD)为4的合成三维住宅建筑。在该数据集中，每个建筑通过三种不同的模态表示：细节级别为4的语义丰富的三维线框图(模态I)、相应的平面图图像(模态II)以及类似LiDAR的屋顶点云(模态III)。每个建筑线框的语义注释来自相应的平面图，包括房间、门和窗户的信息。通过其三模态特性，未来的工作可以利用SYNBUILD-3D开发新的生成式AI算法，自动创建细节级别为4的三维建筑模型，同时满足预定义的平面图布局和屋顶几何形状，并强制语义-几何一致性。数据集和代码样本已在https://github.com/kdmayer/SYNBUILD-3D公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏大规模、语义丰富的3D建筑数据集问题，特别是在Level of Detail 4级别。这个问题很重要，因为3D建筑模型对建筑、能源模拟和导航等应用至关重要，而缺乏高质量数据集限制了自动生成准确且语义丰富的3D建筑的AI算法发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D建筑数据集的局限性，特别是LoD 4级别数据集的缺乏。他们受计算机视觉中合成数据成功的启发，设计了多阶段管道：程序化生成建筑外部(基于Biljecki等人的工作[22]进行定制)，使用AI驱动的平面图生成器(基于Wu等人的工作[21])，开发自定义算法处理非曼哈顿几何(参考Liu等人的工作[23]但进行修改)，以及对齐和堆叠楼层平面图。通过结合现有工作的优点并解决其局限性，作者创建了一个全新的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多模态、语义丰富的LoD 4级别3D建筑数据集，通过程序化生成和AI驱动的平面图生成相结合的方式创建多样化建筑模型，并确保建筑内部和外部的几何和语义一致性。整体流程分为四步：1)程序化生成随机化建筑外部；2)为每层生成足迹条件化的平面图图像；3)将平面图信息矢量化；4)对齐、拉伸和堆叠向量化的楼层体积。通过楼层平面图的排列组合显著扩展了数据集规模。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)规模和细节 - 包含620万个LoD 4级别建筑，每个都有详细语义标注；2)统一线框 - 提供集成内部-外部线框，具有一致几何和语义；3)多模态数据 - 同时提供3D线框、平面图图像和屋顶点云；4)开放和可扩展 - 发布完整数据集和生成管道。相比之前工作，SYNBUILD-3D规模更大(比之前数据集大100倍以上)，语义更丰富，支持更复杂的几何结构，并提供了更严格的质量保证措施。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SYNBUILD-3D提供了一个前所未有的620万建筑规模、多模态、语义丰富的LoD 4级别3D建筑数据集，通过结合程序化建筑外部生成和AI驱动的平面图生成，解决了3D建筑生成领域缺乏大规模高质量训练数据的关键瓶颈。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.25740/kz908vb7844&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D building models are critical for applications in architecture, energysimulation, and navigation. Yet, generating accurate and semantically rich 3Dbuildings automatically remains a major challenge due to the lack oflarge-scale annotated datasets in the public domain. Inspired by the success ofsynthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,and multi-modal dataset of over 6.2 million synthetic 3D residential buildingsat Level of Detail (LoD) 4. In the dataset, each building is representedthrough three distinct modalities: a semantically enriched 3D wireframe graphat LoD 4 (Modality I), the corresponding floor plan images (Modality II), and aLiDAR-like roof point cloud (Modality III). The semantic annotations for eachbuilding wireframe are derived from the corresponding floor plan images andinclude information on rooms, doors, and windows. Through its tri-modal nature,future work can use SYNBUILD-3D to develop novel generative AI algorithms thatautomate the creation of 3D building models at LoD 4, subject to predefinedfloor plan layouts and roof geometries, while enforcing semantic-geometricconsistency. Dataset and code samples are publicly available athttps://github.com/kdmayer/SYNBUILD-3D.</description>
      <author>example@mail.com (Kevin Mayer, Alex Vesel, Xinyi Zhao, Martin Fischer)</author>
      <guid isPermaLink="false">2508.21169v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2508.20835v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了PointDGRWKV，这是首个针对领域泛化点云分类(DG PCC)的RWKV框架，通过解决RWKV在点云应用中的空间失真和注意力漂移问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;领域泛化(DG)被用于增强点云分类(PCC)模型在未见领域的泛化能力，但先前基于卷积网络、Transformer或Mamba架构的方法存在感受野有限、计算成本高或长距离依赖建模不足的问题。&lt;h4&gt;目的&lt;/h4&gt;研究RWKV模型在DG PCC中的泛化能力，解决直接应用RWKV到点云分类时遇到的空间失真和注意力漂移问题。&lt;h4&gt;方法&lt;/h4&gt;提出了PointDGRWKV框架，包含两个关键模块：自适应几何Token Shift用于建模局部邻域结构提高几何上下文感知；跨域关键特征分布对齐用于减轻注意力漂移。这些模块在保持RWKV线性效率的同时增强了空间建模和跨域鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;直接应用RWKV到DG PCC遇到两个挑战：固定方向的token shift方法在非结构化点云上导致空间失真，削弱局部几何建模；Bi-WKV注意力通过指数加权放大跨域差异，导致注意力偏移和泛化能力下降。&lt;h4&gt;结论&lt;/h4&gt;PointDGRWKV通过创新性地解决RWKV在点云领域的应用问题，在多个基准测试上实现了DG PCC的最先进性能，证明了RWKV架构在点云分类领域的潜力。&lt;h4&gt;翻译&lt;/h4&gt;领域泛化(DG)最近被探索用于增强点云分类(PCC)模型在未见领域的泛化能力。先前的工作基于卷积网络、Transformer或Mamba架构，要么受到感受野有限的限制，要么计算成本高，或长距离依赖建模不足。RWKV作为一种新兴架构，具有优越的线性复杂度、全局感受野和长距离依赖能力。在本文中，我们首次研究了RWKV模型在DG PCC中的泛化能力。我们发现直接将RWKV应用于DG PCC面临两个重大挑战：RWKV的固定方向token shift方法(如Q-Shift)在应用于非结构化点云时引入空间失真，削弱局部几何建模并降低鲁棒性。此外，RWKV中的Bi-WKV注意力通过指数加权放大关键分布中的轻微跨域差异，导致注意力偏移和泛化能力下降。为此，我们提出了PointDGRWKV，这是首个为DG PCC定制的RWKV框架。它引入了两个关键模块来增强空间建模和跨域鲁棒性，同时保持RWKV的线性效率。特别是，我们提出了自适应几何Token Shift来建模局部邻域结构以提高几何上下文感知能力。此外，跨域关键特征分布对齐被设计为通过跨域对齐关键特征分布来减轻注意力漂移。在多个基准上的广泛实验证明，PointDGRWKV在DG PCC上实现了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分类模型在未见过的领域上的泛化能力问题。这个问题在现实中非常重要，因为自动驾驶、增强现实和机器人等应用场景中，点云数据可能因不同传感器、环境或扫描角度而产生分布差异，导致模型性能显著下降。研究这个问题能帮助模型更好地适应各种实际应用场景，无需针对每个新场景重新训练模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了直接应用RWKV到点云领域泛化时遇到的两个主要挑战：固定方向token shift导致的空间扭曲问题，以及Bi-WKV注意力机制对跨领域差异的放大效应。作者借鉴了RWKV架构的线性复杂度和全局感受场特性，参考了Vision-RWKV作为基线，并吸收了点云Mamba等方法在点云处理上的经验。基于这些分析，设计了两个关键模块：自适应几何Token Shift来解决局部几何建模问题，以及跨领域关键特征分布对齐来减轻注意力偏移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个创新模块增强RWKV在点云数据上的表现：1) 自适应几何Token Shift基于点云空间特性构建局部邻域，动态整合结构特征，避免空间扭曲；2) 跨领域关键特征分布对齐通过在均值和协方差水平上对齐不同源领域的关键特征，减轻注意力偏移。整体流程包括：输入点云数据预处理、四阶段层次特征提取、应用AGT-Shift进行局部几何建模、使用CD-KDA对齐关键特征分布、通过Bi-WKV注意力机制处理特征，最后进行分类预测。训练时结合分类损失和分布对齐损失，推理时无需源领域数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个专门为点云领域泛化设计的RWKV-based框架；2) 自适应几何Token Shift模块，通过空间分区和加权特征聚合实现高效局部几何建模；3) 跨领域关键特征分布对齐模块，只对关键特征进行对齐以减轻注意力偏移。相比CNN方法，它具有全局感受场；相比Transformer方法，它具有线性计算复杂度；相比Mamba方法，它能更好地捕获长距离依赖；相比直接应用RWKV的方法，它解决了空间扭曲和跨领域注意力偏移问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了PointDGRWKV，一种创新的RWKV-based框架，通过自适应几何Token Shift和跨领域关键特征分布对齐两个模块，显著提高了点云分类模型在未知领域的泛化能力，同时保持了高效的线性计算复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain Generalization (DG) has been recently explored to enhance thegeneralizability of Point Cloud Classification (PCC) models toward unseendomains. Prior works are based on convolutional networks, Transformer or Mambaarchitectures, either suffering from limited receptive fields or highcomputational cost, or insufficient long-range dependency modeling. RWKV, as anemerging architecture, possesses superior linear complexity, global receptivefields, and long-range dependency. In this paper, we present the first workthat studies the generalizability of RWKV models in DG PCC. We find thatdirectly applying RWKV to DG PCC encounters two significant challenges: RWKV'sfixed direction token shift methods, like Q-Shift, introduce spatialdistortions when applied to unstructured point clouds, weakening localgeometric modeling and reducing robustness. In addition, the Bi-WKV attentionin RWKV amplifies slight cross-domain differences in key distributions throughexponential weighting, leading to attention shifts and degraded generalization.To this end, we propose PointDGRWKV, the first RWKV-based framework tailoredfor DG PCC. It introduces two key modules to enhance spatial modeling andcross-domain robustness, while maintaining RWKV's linear efficiency. Inparticular, we present Adaptive Geometric Token Shift to model localneighborhood structures to improve geometric context awareness. In addition,Cross-Domain key feature Distribution Alignment is designed to mitigateattention drift by aligning key feature distributions across domains. Extensiveexperiments on multiple benchmarks demonstrate that PointDGRWKV achievesstate-of-the-art performance on DG PCC.</description>
      <author>example@mail.com (Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Shuicheng Yan)</author>
      <guid isPermaLink="false">2508.20835v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Assessing Human Cooperation for Enhancing Social Robot Navigation</title>
      <link>http://arxiv.org/abs/2508.21455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;社交感知机器人导航是一种在人类环境中导航并遵守社交约束的规划范式，现有方法使用人类预测模型改进导航策略，但当人类行为不符合预期时会出现困难。&lt;h4&gt;背景&lt;/h4&gt;社交感知机器人导航是机器人在人类环境中导航并尝试遵守社交约束的规划范式，这些策略使用人类预测模型进一步改进，机器人在计算自身路径时考虑人类的潜在未来轨迹。&lt;h4&gt;目的&lt;/h4&gt;通过在适当时机进行有效沟通来解决机器人无法理解人类意图和合作性，以及人类不清楚机器人计划的问题，基于对场景和人类合作性的几何分析。&lt;h4&gt;方法&lt;/h4&gt;提供一种评估方法，提出能够区分合作型人类与非合作型人类的评估指标，展示如何使用几何推理来生成适当的语言回应或机器人动作。&lt;h4&gt;主要发现&lt;/h4&gt;机器人难以处理人类不符合预期的行为，因为缺乏对人类意图和合作性的理解，同时也缺乏向人类传达自身计划的能力。&lt;h4&gt;结论&lt;/h4&gt;通过基于场景几何分析和人类合作性的有效沟通，可以改善机器人在面对意外人类行为时的表现。&lt;h4&gt;翻译&lt;/h4&gt;社交感知机器人导航是一种规划范式，机器人在人类环境中导航并尝试在与场景中人类互动时遵守社交约束。这些导航策略使用人类预测模型进一步改进，机器人在计算自身路径时考虑人类的潜在未来轨迹。尽管这些策略显著改善了机器人的行为，但当人类行为不符合预期时，机器人仍会遇到困难。这是因为机器人无法理解人类的意图和合作性，而人类也不清楚机器人的计划。在本文中，我们旨在通过基于对场景几何分析和人类合作性的适时有效沟通来解决这一差距。我们提供了一种评估方法，并提出了一些能够区分合作型人类与非合作型人类的评估指标。此外，我们还展示了如何使用几何推理来生成适当的语言回应或机器人行动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Socially aware robot navigation is a planning paradigm where the robotnavigates in human environments and tries to adhere to social constraints whileinteracting with the humans in the scene. These navigation strategies werefurther improved using human prediction models, where the robot takes thepotential future trajectory of humans while computing its own. Though thesestrategies significantly improve the robot's behavior, it faces difficultiesfrom time to time when the human behaves in an unexpected manner. This happensas the robot fails to understand human intentions and cooperativeness, and thehuman does not have a clear idea of what the robot is planning to do. In thispaper, we aim to address this gap through effective communication at anappropriate time based on a geometric analysis of the context and humancooperativeness in head-on crossing scenarios. We provide an assessmentmethodology and propose some evaluation metrics that could distinguish acooperative human from a non-cooperative one. Further, we also show howgeometric reasoning can be used to generate appropriate verbal responses orrobot actions.</description>
      <author>example@mail.com (Hariharan Arunachalam, Phani Teja Singamaneni, Rachid Alami)</author>
      <guid isPermaLink="false">2508.21455v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.21080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures, Accepted to ICCV 2025 Workshop on Out-of-Label  Hazards in Autonomous Driving (2COOOL)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇摘要介绍了一个名为2COOOL的研讨会，专注于解决自动驾驶中的新型场景挑战，特别是处理标签外危险的问题，旨在促进学术界和产业界的交流合作，推动更安全的自动驾驶技术发展。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉社区正在推进自动驾驶算法，将视觉洞察与传感器数据相结合对提高感知、决策、规划、预测、模拟和控制能力至关重要。然而，完全安全的自动驾驶汽车尚未实现，主要原因之一是无法有效处理新型场景，这是实际部署的关键障碍。&lt;h4&gt;目的&lt;/h4&gt;2COOOL研讨会为研究人员和行业专家提供了一个专门的论坛，用于推动新颖性处理的前沿发展，包括分布外危险检测、用于危险理解的视觉语言模型、新的基准测试和方法论，以及安全的自动驾驶实践。&lt;h4&gt;方法&lt;/h4&gt;研讨会将汇集学术界和产业界的参与者，借鉴异常检测、开放集识别、开放词汇建模、领域适应和相关领域的思想，激发新的危险规避算法和系统的发展。&lt;h4&gt;主要发现&lt;/h4&gt;新型场景处理是自动驾驶安全的关键挑战，需要开发新的方法和技术来应对实际部署中遇到的未知危险情况。&lt;h4&gt;结论&lt;/h4&gt;通过举办2COOOL研讨会，旨在促进自动驾驶技术中新型场景处理方法的发展，从而推动更安全的自动驾驶系统的实现。&lt;h4&gt;翻译&lt;/h4&gt;随着计算机视觉社区推进自动驾驶算法，将基于视觉的洞察与传感器数据相结合对于提高感知、决策、规划、预测、模拟和控制能力仍然至关重要。然而，我们必须问：为什么我们还没有完全安全的自动驾驶汽车？答案的关键部分之一在于处理新型场景，这是实际部署的最关键障碍之一。我们的2COOOL研讨会为研究人员和行业专家提供了一个专门的论坛，以推动新颖性处理的前沿发展，包括分布外危险检测、用于危险理解的视觉语言模型、新的基准测试和方法论，以及安全的自动驾驶实践。第二届自动驾驶中标签外危险挑战研讨会（2COOOL）将于2025年10月19日在夏威夷火奴鲁鲁举行的国际计算机视觉会议（ICCV 2025）上举行。我们的目标是激发危险规避新算法和系统的发展，借鉴异常检测、开放集识别、开放词汇建模、领域适应和相关领域的思想。在2025年冬季计算机视觉应用会议（WACV 2025）首次成功举办的基础上，本次研讨会将结合学术和产业界的参与。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the computer vision community advances autonomous driving algorithms,integrating vision-based insights with sensor data remains essential forimproving perception, decision making, planning, prediction, simulation, andcontrol. Yet we must ask: Why don't we have entirely safe self-driving carsyet? A key part of the answer lies in addressing novel scenarios, one of themost critical barriers to real-world deployment. Our 2COOOL workshop provides adedicated forum for researchers and industry experts to push the state of theart in novelty handling, including out-of-distribution hazard detection,vision-language models for hazard understanding, new benchmarking andmethodologies, and safe autonomous driving practices. The 2nd Workshop on theChallenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be heldat the International Conference on Computer Vision (ICCV) 2025 in Honolulu,Hawaii, on October 19, 2025. We aim to inspire the development of newalgorithms and systems for hazard avoidance, drawing on ideas from anomalydetection, open-set recognition, open-vocabulary modeling, domain adaptation,and related fields. Building on the success of its inaugural edition at theWinter Conference on Applications of Computer Vision (WACV) 2025, the workshopwill feature a mix of academic and industry participation.</description>
      <author>example@mail.com (Ali K. AlShami, Ryan Rabinowitz, Maged Shoman, Jianwu Fang, Lukas Picek, Shao-Yuan Lo, Steve Cruz, Khang Nhut Lam, Nachiket Kamod, Lei-Lei Li, Jugal Kalita, Terrance E. Boult)</author>
      <guid isPermaLink="false">2508.21080v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos</title>
      <link>http://arxiv.org/abs/2508.21770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了异常非典型视频数据对开放世界学习的影响，通过收集包含多种异常类型的新视频数据集，并在分布外检测、新颖类别发现和零样本动作识别三个任务中验证了异常数据的有效性。&lt;h4&gt;背景&lt;/h4&gt;大多数现有研究关注封闭集中的常见典型数据，而开放世界中的新颖发现在视频领域探索不足。人类在面对不常见的新概念时表现出卓越的泛化和发现能力。&lt;h4&gt;目的&lt;/h4&gt;研究在学习过程中暴露异常不典型视频可能带来的影响，以及这些数据如何有益于开放世界学习。&lt;h4&gt;方法&lt;/h4&gt;收集了一个包含各种类型异常非典型数据（如科幻、动画等）的新视频数据集，将这些数据输入到模型训练过程中进行表示学习。关注开放世界学习中的三个关键任务：分布外检测、新颖类别发现和零样本动作识别。&lt;h4&gt;主要发现&lt;/h4&gt;1) 即使使用简单的学习方法，包含异常数据也能在各种设置中持续提高性能；2) 增加异常样本的类别多样性可以进一步提高分布外检测性能；3) 在新颖类别发现任务中，使用较小但语义更多样化的异常样本集比使用较大但更典型的数据集效果更好；4) 在零样本动作识别设置中，异常视频的语义多样性有助于模型更好地泛化到未见过的动作类别。&lt;h4&gt;结论&lt;/h4&gt;实验评估结果揭示了异常视频对开放世界视觉表示学习的好处，连同新提出的数据集，鼓励进一步研究这一方向。&lt;h4&gt;翻译&lt;/h4&gt;人类在面对不常见的新概念时通常表现出卓越的泛化和发现能力。然而，文献中的大多数现有研究关注来自封闭集的常见典型数据，开放世界中的新颖发现在视频中探索不足。在本文中，我们感兴趣的问题是：如果学习过程中暴露异常不典型的视频会怎样？为此，我们收集了一个包含各种类型异常非典型数据（例如科幻、动画等）的新视频数据集。为了研究此类异常数据如何有益于开放世界学习，我们将它们输入到表示学习的模型训练过程中。聚焦于开放世界学习中的三个关键任务：分布外检测、新颖类别发现和零样本动作识别，我们发现即使使用包含异常数据的基本学习方法也能在各种设置中持续提高性能。此外，我们发现增加异常样本的类别多样性可以进一步提高分布外检测性能。另外，在新颖类别发现任务中，使用较小但语义更多样化的异常样本集比使用较大但更典型的数据集效果更好。在零样本动作识别设置中，异常视频的语义多样性有助于模型更好地泛化到未见过的动作类别。我们在广泛实验评估中的这些发现揭示了异常视频对开放世界视觉表示学习的好处，连同新提出的数据集，鼓励进一步研究这一方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans usually show exceptional generalisation and discovery ability in theopen world, when being shown uncommon new concepts. Whereas most existingstudies in the literature focus on common typical data from closed sets,open-world novel discovery is under-explored in videos. In this paper, we areinterested in asking: \textit{What if atypical unusual videos are exposed inthe learning process?} To this end, we collect a new video dataset consistingof various types of unusual atypical data (\eg sci-fi, animation, \etc). Tostudy how such atypical data may benefit open-world learning, we feed them intothe model training process for representation learning. Focusing on three keytasks in open-world learning: out-of-distribution (OOD) detection, novelcategory discovery (NCD), and zero-shot action recognition (ZSAR), we foundthat even straightforward learning approaches with atypical data consistentlyimprove performance across various settings. Furthermore, we found thatincreasing the categorical diversity of the atypical samples further boosts OODdetection performance. Additionally, in the NCD task, using a smaller yet moresemantically diverse set of atypical samples leads to better performancecompared to using a larger but more typical dataset. In the ZSAR setting, thesemantic diversity of atypical videos helps the model generalise better tounseen action classes. These observations in our extensive experimentalevaluations reveal the benefits of atypical videos for visual representationlearning in the open world, together with the newly proposed dataset,encouraging further studies in this direction.</description>
      <author>example@mail.com (Qiyue Sun, Qiming Huang, Yang Yang, Hongjun Wang, Jianbo Jiao)</author>
      <guid isPermaLink="false">2508.21770v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing</title>
      <link>http://arxiv.org/abs/2508.21402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SatDINO模型，一种专门用于卫星图像表示学习的自监督学习方法，通过对比自预训练在多个数据集上取得了优于现有方法的结果。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为遥感领域的强大工具，因为该领域有大量未标记数据可用。&lt;h4&gt;目的&lt;/h4&gt;研究使用DINO对比自监督方法在遥感图像上进行预训练，并开发专门针对卫星图像表示学习的模型。&lt;h4&gt;方法&lt;/h4&gt;在多个数据集和多种测试设置下进行了广泛实验，评估SatDINO模型性能，并提供严格的消融研究。&lt;h4&gt;主要发现&lt;/h4&gt;SatDINO性能优于基于掩码自编码器（MAE）的其他最先进方法，在多个基准测试中取得具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;SatDINO模型在遥感图像表示学习方面表现出色，所提出的增强方法（如GSD编码和自适应视图采样）可独立使用。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为遥感的强大工具，在那里有大量未标记的数据可用。在这项工作中，我们研究了使用DINO（一种对比自监督方法）在遥感图像上进行预训练。我们引入了SatDINO，这是一种专门为卫星图像表示学习定制的模型。通过在多个数据集和多种测试设置上的广泛实验，我们证明了SatDINO优于基于更常见的掩码自编码器（MAE）的其他最先进方法，并在多个基准测试中取得了具有竞争力的结果。我们还提供了严格的消融研究，评估了SatDINO的各个组件。最后，我们提出了一些新颖的增强方法，例如一种新的地面样本距离（GSD）编码方法和自适应视图采样方法。这些增强方法可以独立地应用于我们的SatDINO模型。我们的代码和训练模型可在以下网址获取：https://github.com/strakaj/SatDINO。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has emerged as a powerful tool for remote sensing,where large amounts of unlabeled data are available. In this work, weinvestigate the use of DINO, a contrastive self-supervised method, forpretraining on remote sensing imagery. We introduce SatDINO, a model tailoredfor representation learning in satellite imagery. Through extensive experimentson multiple datasets in multiple testing setups, we demonstrate that SatDINOoutperforms other state-of-the-art methods based on much more common maskedautoencoders (MAE) and achieves competitive results in multiple benchmarks.  We also provide a rigorous ablation study evaluating SatDINO's individualcomponents. Finally, we propose a few novel enhancements, such as a new way toincorporate ground sample distance (GSD) encoding and adaptive view sampling.These enhancements can be used independently on our SatDINO model. Our code andtrained models are available at: https://github.com/strakaj/SatDINO.</description>
      <author>example@mail.com (Jakub Straka, Ivan Gruber)</author>
      <guid isPermaLink="false">2508.21402v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation</title>
      <link>http://arxiv.org/abs/2508.21320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been accepted as a full research paper at CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LINKO，一种大型语言模型增强的整合本体学习框架，通过同时利用多个医学本体图，实现异构系统内部和跨系统的双轴知识传播，以增强医疗概念表示学习。&lt;h4&gt;背景&lt;/h4&gt;医学本体图通过结构化关系将外部知识映射到电子健康记录中的医疗代码。现有研究主要关注从单一本体系统或多个本体系统中获取领域知识，但未将它们整合到统一的学习结构中，导致概念表示学习局限于本体内部关系，忽略了跨本体连接。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时利用多个本体图的学习框架，通过双轴知识传播增强医疗概念表示学习，解决现有方法中跨本体连接被忽略的问题。&lt;h4&gt;方法&lt;/h4&gt;LINKO首先使用大型语言模型提供基于图检索增强的本体概念嵌入初始化，通过工程化提示和本体上下文进一步增强；然后联合学习不同本体图中的医疗概念，通过两个轴进行知识传播：(1)本体内部的垂直传播（跨层次结构）(2)本体之间的水平传播（在每一层并行）。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上的实验验证了LINKO优于最先进的基线方法。作为与现有EHR预测模型兼容的插件编码器，LINKO在数据有限和罕见疾病预测等场景中展示了更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;LINKO通过整合多个本体系统并进行双轴知识传播，有效增强了医疗概念表示学习，在多种场景下表现优于现有方法，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;医学本体图通过结构化关系将外部知识映射到电子健康记录中的医疗代码。通过利用领域批准的连接（如父子关系），预测模型可以通过整合相关概念的上下文信息生成更丰富的医疗概念表示。然而，现有文献主要关注整合来自单一本体系统的领域知识，或孤立地整合来自多个本体系统（如疾病、药物和程序）的领域知识，而没有将它们整合到统一的学习结构中。因此，概念表示学习通常局限于本体内部关系，而忽略了跨本体连接。在本文中，我们提出了LINKO，一种大型语言模型（LLM）增强的整合本体学习框架，通过同时利用多个本体图，实现异构系统内部和跨系统的双轴知识传播，以增强医疗概念表示学习。具体而言，LINKO首先使用大型语言模型提供本体概念嵌入的图检索增强初始化，通过包含概念描述的工程化提示，并进一步用本体上下文增强。其次，我们的方法通过在两个轴上进行知识传播，联合学习不同本体图中的医疗概念：(1)跨层次本体级别的内部本体垂直传播，以及(2)在每一层并行进行的内部本体水平传播。最后，通过在两个公共数据集上的广泛实验，我们验证了LINKO优于最先进基线的性能。作为与现有EHR预测模型兼容的插件编码器，LINKO进一步展示了在数据可用性有限和罕见疾病预测等场景中增强的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical ontology graphs map external knowledge to medical codes in electronichealth records via structured relationships. By leveraging domain-approvedconnections (e.g., parent-child), predictive models can generate richer medicalconcept representations by incorporating contextual information from relatedconcepts. However, existing literature primarily focuses on incorporatingdomain knowledge from a single ontology system, or from multiple ontologysystems (e.g., diseases, drugs, and procedures) in isolation, withoutintegrating them into a unified learning structure. Consequently, conceptrepresentation learning often remains limited to intra-ontology relationships,overlooking cross-ontology connections. In this paper, we propose LINKO, alarge language model (LLM)-augmented integrative ontology learning frameworkthat leverages multiple ontology graphs simultaneously by enabling dual-axisknowledge propagation both within and across heterogeneous ontology systems toenhance medical concept representation learning. Specifically, LINKO firstemploys LLMs to provide a graph-retrieval-augmented initialization for ontologyconcept embedding, through an engineered prompt that includes conceptdescriptions, and is further augmented with ontology context. Second, ourmethod jointly learns the medical concepts in diverse ontology graphs byperforming knowledge propagation in two axes: (1) intra-ontology verticalpropagation across hierarchical ontology levels and (2) inter-ontologyhorizontal propagation within every level in parallel. Last, through extensiveexperiments on two public datasets, we validate the superior performance ofLINKO over state-of-the-art baselines. As a plug-in encoder compatible withexisting EHR predictive models, LINKO further demonstrates enhanced robustnessin scenarios involving limited data availability and rare disease prediction.</description>
      <author>example@mail.com (Mohsen Nayebi Kerdabadi, Arya Hadizadeh Moghaddam, Dongjie Wang, Zijun Yao)</author>
      <guid isPermaLink="false">2508.21320v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI</title>
      <link>http://arxiv.org/abs/2508.21775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure, PANTHER Challenge submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于nnU-Net框架的胰腺导管腺癌MRI自动分割方法，通过多阶段级联预训练策略和指标感知的集成技术，在PANTHER挑战中取得了优异的分割性能。&lt;h4&gt;背景&lt;/h4&gt;胰腺导管腺癌（PDAC）从MRI的自动分割对临床工作流程至关重要，但受到肿瘤组织对比度差和标注数据稀少的阻碍。&lt;h4&gt;目的&lt;/h4&gt;参加PANTHER挑战，解决诊断性T1加权（任务1）和治疗性T2加权（任务2）的胰腺导管腺癌MRI分割问题。&lt;h4&gt;方法&lt;/h4&gt;基于nnU-Net框架，采用深度多阶段级联预训练策略，从通用解剖学基础模型开始，然后在CT胰腺病变数据集和目标MRI模态上逐步微调。通过五折交叉验证评估数据增强方案和训练计划，并构建定制的异构专家模型集成。&lt;h4&gt;主要发现&lt;/h4&gt;数据增强存在关键权衡：激进数据增强产生最高体积准确性，而默认增强提供更好边界精度（任务1达到MASD 5.46毫米和HD95 17.33毫米）。指标感知的集成策略有效，任务1达到肿瘤Dice分数0.661，任务2达到0.523。&lt;h4&gt;结论&lt;/h4&gt;该方法为在有限数据和复杂医学成像任务背景下开发专门的高性能模型提供了稳健的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;胰腺导管腺癌（PDAC）从MRI的自动分割对临床工作流程至关重要，但受到肿瘤组织对比度差和标注数据稀少的阻碍。本文详细介绍了我们参加PANTHER挑战的方案，解决了诊断性T1加权（任务1）和治疗性T2加权（任务2）的分割问题。我们的方法基于nnU-Net框架，利用深度多阶段级联预训练策略，从通用解剖学基础模型开始，然后在CT胰腺病变数据集和目标MRI模态上逐步微调。通过大量的五折交叉验证，我们系统评估了数据增强方案和训练计划。分析揭示了关键权衡，激进的数据增强产生最高的体积准确性，而默认增强产生更好的边界精度（任务1达到最先进的MASD 5.46毫米和HD95 17.33毫米）。对于最终提交，我们利用这一发现构建了定制的、异构的专家模型集成，基本上创建了一个专家混合。这种指标感知的集成策略被证明非常有效，任务1达到最高的交叉验证肿瘤Dice分数0.661，任务2达到0.523。我们的工作为在有限数据和复杂医学成像任务背景下开发专门的高性能模型提供了稳健的方法论（MIC-DKFZ团队）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI iscritical for clinical workflows but is hindered by poor tumor-tissue contrastand a scarcity of annotated data. This paper details our submission to thePANTHER challenge, addressing both diagnostic T1-weighted (Task 1) andtherapeutic T2-weighted (Task 2) segmentation. Our approach is built upon thennU-Net framework and leverages a deep, multi-stage cascaded pre-trainingstrategy, starting from a general anatomical foundation model and sequentiallyfine-tuning on CT pancreatic lesion datasets and the target MRI modalities.Through extensive five-fold cross-validation, we systematically evaluated dataaugmentation schemes and training schedules. Our analysis revealed a criticaltrade-off, where aggressive data augmentation produced the highest volumetricaccuracy, while default augmentations yielded superior boundary precision(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).For our final submission, we exploited this finding by constructing custom,heterogeneous ensembles of specialist models, essentially creating a mix ofexperts. This metric-aware ensembling strategy proved highly effective,achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523for Task 2. Our work presents a robust methodology for developing specialized,high-performance models in the context of limited data and complex medicalimaging tasks (Team MIC-DKFZ).</description>
      <author>example@mail.com (Omer Faruk Durugol, Maximilian Rokuss, Yannick Kirchhoff, Klaus H. Maier-Hein)</author>
      <guid isPermaLink="false">2508.21775v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</title>
      <link>http://arxiv.org/abs/2508.21769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了基础模型CLIP在领域泛化方面的性能，发现CLIP在分布外数据上表现显著下降，并提出了CLIP-DCA方法来改善这一情况。该方法通过增强领域意识而非强制领域不变性，实现了更好的领域泛化性能。&lt;h4&gt;背景&lt;/h4&gt;评估基础模型如CLIP的领域泛化具有挑战性，因为网络规模的预训练数据可能已覆盖许多现有基准数据集，导致当前评估不够具有挑战性且无法充分测试真正未见数据的场景。&lt;h4&gt;目的&lt;/h4&gt;为了更好地评估CLIP在遇到具有挑战性未见数据时的真实领域泛化性能，研究提出了两种评估方法：在33个多样化数据集上评估和在CLIP上应用遗忘技术。&lt;h4&gt;方法&lt;/h4&gt;研究提出了CLIP-DCA方法，它使用单独的领域头和合成的多样化领域数据来识别和增强CLIP编码器中的领域意识，同时通过解耦领域特征来鼓励领域不变分类，而非像传统方法那样强制领域不变性。&lt;h4&gt;主要发现&lt;/h4&gt;CLIP在更多分布外数据集上的性能显著下降，而CLIP-DCA在具有挑战性的评估中显示出比现有方法显著的改进，特别是在更多分布外的数据集上。&lt;h4&gt;结论&lt;/h4&gt;增强领域意识而非强制领域不变性是改善基础模型在未见数据上性能的有效策略，CLIP-DCA通过解耦分类和领域感知表示实现了更好的领域泛化。&lt;h4&gt;翻译&lt;/h4&gt;评估基础模型如CLIP的领域泛化具有挑战性，因为网络规模的预训练数据可能覆盖了许多现有的基准数据集。因此，当前的领域泛化评估可能既不够具有挑战性，也无法充分测试真正未见数据的场景。为了更好地评估CLIP在遇到具有挑战性未见数据时的真实领域泛化性能，我们考虑两种方法：(1)在33个多样化数据集上评估，这些数据集在CLIP在ImageNet上微调后具有量化的分布外分数；(2)使用遗忘技术使CLIP'遗忘'某些领域作为近似方法。我们观察到CLIP在更多分布外数据集上的性能显著下降。为解决这个问题，我们提出了CLIP-DCA(解耦分类与增强领域感知表示)。我们的方法基于一个观察：虽然标准的领域不变性损失旨在使表示具有领域不变性，但这可能对基础模型有害，因为它会迫使模型丢弃对泛化有益的领域感知表示。相反，我们假设增强领域意识是基础模型中有效领域不变分类的前提。CLIP-DCA使用单独的领域头和合成的多样化领域数据来识别和增强CLIP编码器中的领域意识，同时通过从领域特征中解耦来鼓励领域不变分类。与现有方法相比，CLIP-DCA在这种具有挑战性的评估中显示出显著改进，尤其是在更多分布外的数据集上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating domain generalization (DG) for foundational models like CLIP ischallenging, as web-scale pretraining data potentially covers many existingbenchmarks. Consequently, current DG evaluation may neither be sufficientlychallenging nor adequately test genuinely unseen data scenarios. To betterassess the performance of CLIP on DG in-the-wild, a scenario where CLIPencounters challenging unseen data, we consider two approaches: (1) evaluatingon 33 diverse datasets with quantified out-of-distribution (OOD) scores afterfine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'some domains as an approximation. We observe that CLIP's performancedeteriorates significantly on more OOD datasets. To address this, we presentCLIP-DCA (Disentangling Classification from enhanced domain Awarerepresentations). Our approach is motivated by the observation that whilestandard domain invariance losses aim to make representations domain-invariant,this can be harmful to foundation models by forcing the discarding ofdomain-aware representations beneficial for generalization. We insteadhypothesize that enhancing domain awareness is a prerequisite for effectivedomain-invariant classification in foundation models. CLIP-DCA identifies andenhances domain awareness within CLIP's encoders using a separate domain headand synthetically generated diverse domain data. Simultaneously, it encouragesdomain-invariant classification through disentanglement from the domainfeatures. CLIP-DCA shows significant improvements within this challengingevaluation compared to existing methods, particularly on datasets that are moreOOD.</description>
      <author>example@mail.com (Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu)</author>
      <guid isPermaLink="false">2508.21769v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>UItron: Foundational GUI Agent with Advanced Perception and Planning</title>
      <link>http://arxiv.org/abs/2508.21767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了UItron，一个用于自动GUI代理的开源基础模型，具有先进的GUI感知、定位和规划能力。UItron强调系统数据工程和交互基础设施是推进GUI代理开发的基础组件。&lt;h4&gt;背景&lt;/h4&gt;GUI代理旨在实现移动/PC设备上的自动化操作，这是实现通用人工智能的重要任务。视觉语言模型(VLMs)的快速发展加速了GUI代理的发展，但构建GUI代理仍面临操作轨迹稀缺、交互基础设施有限以及基础模型初始能力受限等挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍UItron，一个具有先进GUI感知、定位和规划能力的开源基础模型，并强调系统数据工程和交互基础设施作为推进GUI代理开发基础组件的必要性。&lt;h4&gt;方法&lt;/h4&gt;UItron系统研究数据工程策略增强训练效果，建立连接移动和PC设备的交互环境。采用监督微调处理各种GUI场景的感知和规划任务，开发课程强化学习框架实现复杂推理和探索。收集超过一百万步操作轨迹，覆盖前100个热门应用，构建离线和在线评估环境。&lt;h4&gt;主要发现&lt;/h4&gt;UItron在GUI感知、定位和规划基准测试中表现卓越，特别是在与中国顶级移动应用的交互能力方面突出。实验表明UItron在中国应用场景中取得显著进展，解决了现有解决方案普遍缺乏中文能力的问题。&lt;h4&gt;结论&lt;/h4&gt;UItron通过系统数据工程和交互基础设施的建立，显著提升了GUI代理能力，特别是在中文应用场景中，推动GUI代理更接近实际应用。&lt;h4&gt;翻译&lt;/h4&gt;GUI代理旨在实现移动/PC设备上的自动化操作，这是实现通用人工智能的重要任务。视觉语言模型的快速发展加速了GUI代理的发展，因为它们在视觉理解和任务规划方面具有强大能力。然而，构建GUI代理仍然是一项具有挑战性的任务，原因包括操作轨迹稀缺、交互基础设施可用性有限以及基础模型的初始能力受限。在这项工作中，我们介绍了UItron，这是一个用于自动GUI代理的开源基础模型，具有先进的GUI感知、定位和规划能力。UItron强调了系统数据工程和交互基础设施作为推进GUI代理开发基础组件的必要性。它不仅系统研究了一系列数据工程策略来增强训练效果，还建立了连接移动和PC设备的交互环境。在训练中，UItron采用监督微调方法，在各种GUI场景中处理感知和规划任务，然后开发课程强化学习框架，实现在线环境的复杂推理和探索。结果，UItron在GUI感知、定位和规划的基准测试中取得了卓越的性能。特别是，UItron突出了与中国顶级移动应用的交互能力，因为我们发现即使在最先进的解决方案中也普遍缺乏中文能力。为此，我们手动收集了超过一百万步的操作轨迹，覆盖了前100个最受欢迎的应用程序，并构建了离线和在线代理评估环境。实验结果表明，UItron在中国应用场景中取得了显著进展，推动GUI代理更接近实际应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GUI agent aims to enable automated operations on Mobile/PC devices, which isan important task toward achieving artificial general intelligence. The rapidadvancement of VLMs accelerates the development of GUI agents, owing to theirpowerful capabilities in visual understanding and task planning. However,building a GUI agent remains a challenging task due to the scarcity ofoperation trajectories, the availability of interactive infrastructure, and thelimitation of initial capabilities in foundation models. In this work, weintroduce UItron, an open-source foundational model for automatic GUI agents,featuring advanced GUI perception, grounding, and planning capabilities. UItronhighlights the necessity of systemic data engineering and interactiveinfrastructure as foundational components for advancing GUI agent development.It not only systematically studies a series of data engineering strategies toenhance training effects, but also establishes an interactive environmentconnecting both Mobile and PC devices. In training, UItron adopts supervisedfinetuning over perception and planning tasks in various GUI scenarios, andthen develop a curriculum reinforcement learning framework to enable complexreasoning and exploration for online environments. As a result, UItron achievessuperior performance in benchmarks of GUI perception, grounding, and planning.In particular, UItron highlights the interaction proficiency with top-tierChinese mobile APPs, as we identified a general lack of Chinese capabilitieseven in state-of-the-art solutions. To this end, we manually collect over onemillion steps of operation trajectories across the top 100 most popular apps,and build the offline and online agent evaluation environments. Experimentalresults demonstrate that UItron achieves significant progress in Chinese appscenarios, propelling GUI agents one step closer to real-world application.</description>
      <author>example@mail.com (Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma)</author>
      <guid isPermaLink="false">2508.21767v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>QZhou-Embedding Technical Report</title>
      <link>http://arxiv.org/abs/2508.21632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了QZhou-Embedding，一个基于Qwen2.5-7B-Instruct基础模型构建的通用上下文文本嵌入模型，具有出色的文本表示能力。通过统一的多任务框架、数据合成管道和两阶段训练策略，该模型在多个基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;文本嵌入模型需要更高质量和更多样化的数据来提升性能，而大型语言模型(LLM)的生成能力可以用来优化嵌入模型的数据质量。&lt;h4&gt;目的&lt;/h4&gt;开发一个具有卓越文本表示能力的通用上下文文本嵌入模型，通过利用高质量、多样化的数据和先进的训练策略来提高检索模型的性能。&lt;h4&gt;方法&lt;/h4&gt;基于Qwen2.5-7B-Instruct基础模型构建；设计统一的多任务框架，包含专门的数据转换和训练策略；开发利用LLM API的数据合成管道，包括释义、增强和困难负例生成技术；采用两阶段训练策略：首先进行检索导向的预训练，然后进行全任务微调。&lt;h4&gt;主要发现&lt;/h4&gt;模型在MTEB和CMTEB基准测试中排名第一（2025年8月27日）；在重排序、聚类等任务上也取得了最先进性能；更高质量、更多样化的数据对于提升检索模型性能至关重要；利用LLM的生成能力可以进一步优化嵌入模型的数据质量。&lt;h4&gt;结论&lt;/h4&gt;通过高质量数据生成和先进训练策略，QZhou-Embedding在多个基准测试和任务中取得了最先进的结果，证明了数据质量和多样性对提升嵌入模型性能的重要性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了QZhou-Embedding，一个具有出色文本表示能力的通用上下文文本嵌入模型。基于Qwen2.5-7B-Instruct基础模型构建，我们设计了一个包含专门数据转换和训练策略的统一多任务框架。数据转换方案能够整合更多样化的文本训练数据集，而特定任务的训练策略则提高了模型的学习效率。我们开发了一个利用LLM API的数据合成管道，采用释义、增强和困难负例生成等技术，以提高训练集的语义丰富度和样本难度。此外，我们采用两阶段训练策略，包括初始的检索导向预训练和后续的全任务微调，使嵌入模型能够基于强大的检索性能扩展其能力。我们的模型在MTEB和CMTEB基准测试上取得了最先进的结果，在两个排行榜上均排名第一（2025年8月27日），同时在重排序、聚类等任务上也同时取得了最先进性能。我们的研究结果表明，更高质量、更多样化的数据对于推进检索模型性能至关重要，并且利用LLM的生成能力可以进一步优化嵌入模型突破的数据质量。我们的模型权重已在HuggingFace上以Apache 2.0许可证发布。为了可复现性，我们在GitHub上提供了评估代码和说明。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present QZhou-Embedding, a general-purpose contextual text embedding modelwith exceptional text representation capabilities. Built upon theQwen2.5-7B-Instruct foundation model, we designed a unified multi-taskframework comprising specialized data transformation and training strategies.The data transformation scheme enables the incorporation of more diversetextual training datasets, while the task-specific training strategies enhancemodel learning efficiency. We developed a data synthesis pipeline leveragingLLM API, incorporating techniques such as paraphrasing, augmentation, and hardnegative example generation to improve the semantic richness and sampledifficulty of the training set. Additionally, we employ a two-stage trainingstrategy, comprising initial retrieval-focused pretraining followed byfull-task fine-tuning, enabling the embedding model to extend its capabilitiesbased on robust retrieval performance. Our model achieves state-of-the-artresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards(August 27 2025), and simultaneously achieves state-of-the-art performance ontasks including reranking, clustering, etc. Our findings demonstrate thathigher-quality, more diverse data is crucial for advancing retrieval modelperformance, and that leveraging LLMs generative capabilities can furtheroptimize data quality for embedding model breakthroughs. Our model weights arereleased on HuggingFace under Apache 2.0 license. For reproducibility, weprovide evaluation code and instructions on GitHub.</description>
      <author>example@mail.com (Peng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu)</author>
      <guid isPermaLink="false">2508.21632v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer</title>
      <link>http://arxiv.org/abs/2508.21581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 figures, 1 table. Accepted at the Multimodal Learning and  Fusion Across Scales for Clinical Decision Support (ML-CDS) Workshop, MICCAI  2025. This is the submitted version with authors, affiliations, and  acknowledgements included; it has not undergone peer review or revisions. The  final version will appear in the Springer Lecture Notes in Computer Science  (LNCS) proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过整合术前CT影像和术后组织病理学全切片图像(WSI)，开发了一种多模态深度学习方法用于透明细胞肾细胞癌(ccRCC)的复发风险评估。研究发现基于病理学的模型优于影像学模型，而多模态融合策略进一步提高了预测性能，最佳模型接近传统Leibovich评分的临床标准。&lt;h4&gt;背景&lt;/h4&gt;透明细胞肾细胞癌的复发风险评估对术后监测和治疗决策至关重要。目前广泛使用的Leibovich评分在患者层面分辨率有限，且未包含影像信息，限制了其个性化预测能力。&lt;h4&gt;目的&lt;/h4&gt;评估通过整合术前CT和术后组织病理学全切片图像的多模态方法预测ccRCC复发的效果，并与传统Leibovich评分进行比较。&lt;h4&gt;方法&lt;/h4&gt;采用模块化深度学习框架，结合预训练编码器和基于Cox的生存建模，测试单模态、晚期融合和中间融合三种策略。在真实ccRCC患者队列中验证模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基于病理学图像的模型始终优于仅使用CT的模型，表明病理信息具有更强的预后价值；2) 中间融合策略表现最佳，其最优模型接近调整后的Leibovich评分；3) 随机平局处理显示离散化可能高估模型个性化性能；4) 放射学信息主要通过融合过程为模型提供价值。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的多模态整合方法用于个性化ccRCC风险预测具有可行性。未来应探索更先进的融合策略、扩大多模态数据集规模，并开发通用CT编码器以匹配病理学建模能力。&lt;h4&gt;翻译&lt;/h4&gt;透明细胞肾细胞癌(ccRCC)的复发风险评估对于指导术后监测和治疗至关重要。Leibovich评分仍然被广泛用于区分远处复发风险，但其患者层面的分辨率有限，且不包括影像信息。本研究通过整合术前计算机断层扫描(CT)和术后组织病理学全切片图像(WSI)来评估多模态复发预测。测试了具有预训练编码器和基于Cox的生存建模的模块化深度学习框架，涵盖了单模态、晚期融合和中间融合设置。在一个真实的ccRCC队列中，基于WSI的模型始终优于仅使用CT的模型，突显了病理学的预后强度。中间融合进一步提高了性能，最佳模型(TITAN-CONCH与ResNet-18)接近调整后的Leibovich评分。随机平局打破了缩小了临床基线与学习模型之间的差距，表明离散化可能夸大了个性化性能。使用简单的嵌入连接，放射学主要通过融合增加价值。这些发现证明了基于基础模型的多模态整合用于个性化ccRCC风险预测的可行性。未来的工作应该探索更具表现力的融合策略、更大的多模态数据集和通用CT编码器，以更好地匹配病理学建模能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) isessential for guiding postoperative surveillance and treatment. The Leibovichscore remains widely used for stratifying distant recurrence risk but offerslimited patient-level resolution and excludes imaging information. This studyevaluates multimodal recurrence prediction by integrating preoperative computedtomography (CT) and postoperative histopathology whole-slide images (WSIs). Amodular deep learning framework with pretrained encoders and Cox-based survivalmodeling was tested across unimodal, late fusion, and intermediate fusionsetups. In a real-world ccRCC cohort, WSI-based models consistentlyoutperformed CT-only models, underscoring the prognostic strength of pathology.Intermediate fusion further improved performance, with the best model(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Randomtie-breaking narrowed the gap between the clinical baseline and learned models,suggesting discretization may overstate individualized performance. Usingsimple embedding concatenation, radiology added value primarily through fusion.These findings demonstrate the feasibility of foundation model-based multimodalintegration for personalized ccRCC risk prediction. Future work should exploremore expressive fusion strategies, larger multimodal datasets, andgeneral-purpose CT encoders to better match pathology modeling capacity.</description>
      <author>example@mail.com (Daniël Boeke, Cedrik Blommestijn, Rebecca N. Wray, Kalina Chupetlovska, Shangqi Gao, Zeyu Gao, Regina G. H. Beets-Tan, Mireia Crispin-Ortuzar, James O. Jones, Wilson Silva, Ines P. Machado)</author>
      <guid isPermaLink="false">2508.21581v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</title>
      <link>http://arxiv.org/abs/2508.21529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种卷积神经网络上采样方法，解决了特征基础模型在显微图像处理中的局限性，实现了高效的高质量图像分割。&lt;h4&gt;背景&lt;/h4&gt;特征基础模型（通常是视觉Transformer）为图像提供丰富的语义描述符，适用于下游任务如交互式分割和目标检测。但这些基于块的描述符难以表示显微图像中的精细特征，也难以处理材料和生物图像分析中的大尺寸图像。&lt;h4&gt;目的&lt;/h4&gt;提高特征基础模型在处理显微图像精细特征和大尺寸图像时的效率和准确性，减少对标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;训练一个卷积神经网络来参考输入图像上采样低分辨率的基础模型特征，将此上采样器网络应用于各种显微镜图像的特征化和分割，包括植物细胞、锂离子电池阴极和有机晶体，无需进一步训练即可使用。&lt;h4&gt;主要发现&lt;/h4&gt;上采样特征的丰富性使得难以分割的阶段（如发丝裂纹）能够被有效分离；使用这些深度特征进行交互式分割比传统方法产生更高质量的分割结果，速度更快且需要的标签更少。&lt;h4&gt;结论&lt;/h4&gt;所提出的上采样方法有效解决了特征基础模型在显微图像处理中的局限性，在交互式分割任务中表现出色，提高了效率并显著减少了标注需求。&lt;h4&gt;翻译&lt;/h4&gt;特征基础模型 - 通常是视觉Transformer - 为图像提供丰富的语义描述符，对于下游任务（如交互式分割和目标检测）很有用。为了计算效率，这些描述符通常是基于块的，因此难以表示显微图像中常见的精细特征；它们也难以处理材料和生物图像分析中的大尺寸图像。在这项工作中，我们训练了一个卷积神经网络，参考输入图像来上采样低分辨率（即大块大小）的基础模型特征。我们将这个上采样器网络（无需进一步训练）应用于各种显微镜图像的高效特征化和分割，包括植物细胞、锂离子电池阴极和有机晶体。这些上采样特征的丰富性使得难以分割的阶段（如发丝裂纹）能够被分离。我们证明，使用这些深度特征进行交互式分割，比训练或微调更传统的卷积网络产生更高质量的分割结果，速度更快且需要的标签更少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature foundation models - usually vision transformers - offer rich semanticdescriptors of images, useful for downstream tasks such as (interactive)segmentation and object detection. For computational efficiency thesedescriptors are often patch-based, and so struggle to represent the finefeatures often present in micrographs; they also struggle with the large imagesizes present in materials and biological image analysis. In this work, wetrain a convolutional neural network to upsample low-resolution (i.e, largepatch size) foundation model features with reference to the input image. Weapply this upsampler network (without any further training) to efficientlyfeaturise and then segment a variety of microscopy images, including plantcells, a lithium-ion battery cathode and organic crystals. The richness ofthese upsampled features admits separation of hard to segment phases, likehairline cracks. We demonstrate that interactive segmentation with these deepfeatures produces high-quality segmentations far faster and with far fewerlabels than training or finetuning a more traditional convolutional network.</description>
      <author>example@mail.com (Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper)</author>
      <guid isPermaLink="false">2508.21529v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification</title>
      <link>http://arxiv.org/abs/2508.21458v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the MICCAI 2025 Workshop on Distributed, Collaborative  and Federated Learning (DeCAF)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了基础模型在联邦学习系统中用于痴呆症诊断的性能和效率，重点关注分类头架构、微调策略和聚合方法的影响。&lt;h4&gt;背景&lt;/h4&gt;基础模型在基于AI的痴呆症诊断方面具有强大潜力，但将其整合到联邦学习系统中的研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;系统评估关键设计选择（分类头架构、微调策略和聚合方法）对使用脑部MRI数据进行联邦基础模型调优的性能和效率的影响。&lt;h4&gt;方法&lt;/h4&gt;这是一项基准研究，使用大型多队列数据集系统评估不同设计选择对联邦基础模型调优的影响。&lt;h4&gt;主要发现&lt;/h4&gt;分类头的架构显著影响性能；冻结基础模型编码器可实现与完全微调相当的结果；高级聚合方法优于标准的联邦平均方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果为在分散的临床环境中部署基础模型提供了实用见解，并强调了应该指导未来方法开发的权衡因素。&lt;h4&gt;翻译&lt;/h4&gt;虽然基础模型为基于人工智能的痴呆症诊断提供了强大潜力，但它们在联邦学习系统中的整合仍然研究不足。在这项基准研究中，我们使用脑部MRI数据，系统评估了关键设计选择（分类头架构、微调策略和聚合方法）对联邦基础模型调优的性能和效率的影响。利用大型多队列数据集，我们发现分类头架构对性能有显著影响，冻结基础模型编码器可实现与完全微调相当的结果，且高级聚合方法优于标准的联邦平均方法。我们的研究结果为在分散的临床环境中部署基础模型提供了实用见解，并强调了应该指导未来方法开发的权衡因素。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models (FMs) offer strong potential for AI-based dementiadiagnosis, their integration into federated learning (FL) systems remainsunderexplored. In this benchmarking study, we systematically evaluate theimpact of key design choices: classification head architecture, fine-tuningstrategy, and aggregation method, on the performance and efficiency offederated FM tuning using brain MRI data. Using a large multi-cohort dataset,we find that the architecture of the classification head substantiallyinfluences performance, freezing the FM encoder achieves comparable results tofull fine-tuning, and advanced aggregation methods outperform standardfederated averaging. Our results offer practical insights for deploying FMs indecentralized clinical settings and highlight trade-offs that should guidefuture method development.</description>
      <author>example@mail.com (Kaouther Mouheb, Marawan Elbatel, Janne Papma, Geert Jan Biessels, Jurgen Claassen, Huub Middelkoop, Barbara van Munster, Wiesje van der Flier, Inez Ramakers, Stefan Klein, Esther E. Bron)</author>
      <guid isPermaLink="false">2508.21458v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams</title>
      <link>http://arxiv.org/abs/2508.21273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CALM框架，一种用于实时异常检测的新型端到端解决方案，能够适应非平稳时间序列流中的概念漂移。&lt;h4&gt;背景&lt;/h4&gt;在非平稳时间序列流中检测异常是多个工业和科学领域的关键挑战，传统离线训练的模型在面对数据统计属性随时间变化的概念漂移时性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够适应动态数据模式的实时异常检测框架，解决概念漂移带来的性能退化问题。&lt;h4&gt;方法&lt;/h4&gt;CALM建立在Apache Beam分布式处理框架上，利用TimesFm基础模型进行预测性异常检测，包含两个核心贡献：闭环连续微调机制使模型能近乎实时适应不断变化的数据模式，以及'LLM-as-a-Judge'组件使用大型语言模型提供语义化、上下文感知的异常判断。&lt;h4&gt;主要发现&lt;/h4&gt;在TSB-UAD基准测试上评估显示，与静态预训练基础模型相比，连续微调的模型在大多数数据集上提高了ROC AUC分数。&lt;h4&gt;结论&lt;/h4&gt;自适应、LLM引导的方法能有效维持动态流环境中的高性能异常检测。&lt;h4&gt;翻译&lt;/h4&gt;在众多工业和科学领域中，在非平稳时间序列流中检测异常是一项关键但具有挑战性的任务。当面临概念漂移（数据的基础统计属性随时间变化）时，传统离线训练的模型会遭受显著的性能下降。本文介绍了CALM（Continuous, Adaptive, and LLM-Mediated），一种为应对这一挑战而设计的新型端到端实时异常检测框架。CALM建立在Apache Beam分布式处理框架上，并利用TimesFm基础模型进行基于预测的异常检测。该框架的创新性在于两个核心贡献。首先，它实现了一个闭环连续微调机制，使异常检测模型能够近乎实时地适应不断变化的数据模式。其次，它引入了'LLM-as-a-Judge'组件，一个大型语言模型，对检测到的异常提供语义化、上下文感知的判断，以筛选高质量训练数据，决定异常是代表瞬时噪声还是有意义的模式转变。我们在全面的TSB-UAD基准测试上评估了CALM。结果表明，与静态预训练基础模型相比，连续微调的模型在大多数数据集上提高了ROC AUC分数，验证了我们的自适应、LLM引导方法在保持动态流环境中高性能异常检测的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of anomalies in non-stationary time-series streams is acritical but challenging task across numerous industrial and scientificdomains. Traditional models, trained offline, suffer significant performancedegradation when faced with concept drift, where the underlying statisticalproperties of the data change over time. This paper introduces CALM(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework forreal-time anomaly detection designed to address this challenge. CALM is builton the Apache Beam distributed processing framework and leverages the TimesFmfoundation model for forecasting-based anomaly detection. The framework'snovelty lies in two core contributions. First, it implements a closed-loop,continuous fine-tuning mechanism that allows the anomaly detection model toadapt to evolving data patterns in near real-time. Second, it introduces anLLM-as-a-Judge component, a Large Language Model that provides semantic,context-aware judgments on detected anomalies to curate a high-quality trainingdataset, deciding whether an anomaly represents transient noise or a meaningfulpattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Ourresults demonstrate that the continuously fine-tuned model improves the ROC AUCscore in most datasets compared to the static, pre-trained base model,validating the efficacy of our adaptive, LLM-guided approach to maintaininghigh-performance anomaly detection in dynamic streaming environments.</description>
      <author>example@mail.com (Ashok Devireddy, Shunping Huang)</author>
      <guid isPermaLink="false">2508.21273v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Guess-and-Learn (G&amp;L): Measuring the Cumulative Error Cost of Cold-Start Adaptation</title>
      <link>http://arxiv.org/abs/2508.21270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 7 figures. Main text is 10 pages. Code and data are  available at https://github.com/RolandWArnold/guess-and-learn-benchmark&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了Guess-and-Learn (G&amp;L)方法，用于评估机器学习模型从零开始学习的适应能力，关注学习过程中的累积错误，而不仅仅是最终准确性。&lt;h4&gt;背景&lt;/h4&gt;传统的机器学习模型评估通常只强调最终准确性，忽略了模型从零开始学习时的适应成本（累积错误）。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够量化模型早期学习错误成本的评估方法，以补充传统的基准测试，并提供一个可复现的框架，用于开发不仅在极限情况下准确，而且在第一批示例中就可靠的模型。&lt;h4&gt;方法&lt;/h4&gt;G&amp;L定义了四种设置（从零开始/预训练 × 在线/批量）来分离初始化和更新频率的影响。该方法让学习者在每一步选择一个实例，预测其标签，接收真实标签，并在在线（每样本）或批量（延迟）模式下更新参数。结果形成的错误轨迹揭示了适应速度、选择质量和偏差等动态特性。&lt;h4&gt;主要发现&lt;/h4&gt;在MNIST和AG News上的基线实验显示，较小的模型可以用更少的初始错误进行适应，而预训练的好处因领域而异。在所有设置中，当前模型仍然远高于oracle参考带，突显了适应性差距。&lt;h4&gt;结论&lt;/h4&gt;通过量化早期学习的错误成本，G&amp;L补充了传统的基准测试，并为开发不仅在极限情况下准确，而且从第一批示例就可靠的模型提供了可复现的框架。&lt;h4&gt;翻译&lt;/h4&gt;评估机器学习模型通常强调最终准确性，而忽略了适应的成本：从零开始学习时累积的错误。Guess-and-Learn (G&amp;L) v1.0通过测量冷启动适应性——模型在顺序标记未标记数据集时所犯的总错误——来填补这一空白。在每一步，学习者选择一个实例，预测其标签，接收真实标签，并在在线（每样本）或批量（延迟）模式下更新参数。由此产生的错误轨迹揭示了适应速度、选择质量和偏差——这些是端点指标无法看到的动态特性。G&amp;L定义了四种轨道（从零开始/预训练 × 在线/批量）来分离初始化和更新频率的影响。我们将该协议形式化，将其与经典错误界限理论联系起来，并估计MNIST的启发式'oracle参考带'作为可信度参考。在MNIST和AG News上的基线实验，涵盖经典方法（感知器、k-NN）、卷积架构（CNN、ResNet-50）和预训练转换器（ViT-B/16、BERT-base），揭示了早期阶段效率的系统差异：较小的模型可以用更少的初始错误进行适应，而预训练的好处因领域而异。在所有设置中，当前模型仍然远高于oracle带，突显了适应性差距。通过量化早期学习的错误成本，G&amp;L补充了传统基准测试，并为开发不仅在极限情况下准确，而且从第一批示例就可靠的模型提供了可复现的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluation of machine learning models typically emphasizes final accuracy,overlooking the cost of adaptation: the cumulative errors incurred whilelearning from scratch. Guess-and- Learn (G&amp;L) v1.0 addresses this gap bymeasuring cold-start adaptability - the total mistakes a model makes whilesequentially labeling an unlabeled dataset. At each step, the learner selectsan instance, predicts its label, receives the ground truth, and updatesparameters under either online (per-sample) or batch (delayed) mode. Theresulting error trajectory exposes adaptation speed, selection quality, andbias - dynamics invisible to endpoint metrics.  G&amp;L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) todisentangle the effects of initialization and update frequency. We formalizethe protocol, relate it to classical mistake-bound theory, and estimate aheuristic "oracle reference band" for MNIST as a plausibility reference.Baseline experiments on MNIST and AG News, spanning classical methods(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), andpretrained transformers (ViT-B/16, BERT-base), reveal systematic differences inearly-phase efficiency: smaller models can adapt with fewer initial errors,while pretraining benefits vary by domain. Across settings, current modelsremain well above the oracle band, highlighting an adaptability gap.  By quantifying the mistake cost of early learning, G&amp;L complementsconventional benchmarks and provides a reproducible framework for developinglearners that are not only accurate in the limit but also reliable from thefirst examples.</description>
      <author>example@mail.com (Roland Arnold)</author>
      <guid isPermaLink="false">2508.21270v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Generalizable Object Re-Identification via Visual In-Context Prompting</title>
      <link>http://arxiv.org/abs/2508.21222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了视觉上下文提示(VICP)框架，结合大语言模型和视觉基础模型，使训练好的模型能够仅通过上下文示例作为提示直接泛化到未见的新类别，无需参数适应。&lt;h4&gt;背景&lt;/h4&gt;当前物体再识别方法训练特定领域模型，缺乏泛化能力且需要大量标记数据；自监督学习虽减少标注需求，但难以捕获关键的敏感身份特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够泛化到未见类别的新框架，减少对标记数据的依赖，实现无需参数适应的跨类别泛化。&lt;h4&gt;方法&lt;/h4&gt;VICP框架结合大语言模型和视觉基础模型，通过特定任务提示从少量样本中推断语义身份规则，使用动态视觉提示引导模型提取身份判别性特征，对齐语义概念与预训练先验实现泛化。&lt;h4&gt;主要发现&lt;/h4&gt;VICP在ShopID10K数据集和多种ReID基准测试中，在未见类别上明显优于基线方法；引入了包含10K个物体实例的多视图图像和跨域测试的ShopID10K数据集。&lt;h4&gt;结论&lt;/h4&gt;VICP消除了数据集特定重新训练的需求，实现了有效的跨类别泛化。&lt;h4&gt;翻译&lt;/h4&gt;当前物体再识别方法训练特定领域模型（例如针对人或车辆），缺乏泛化能力且对新类别需要大量标记数据。虽然自监督学习通过学习实例不变性减少了标注需求，但难以捕获对再识别关键的敏感身份特征。本文提出了视觉上下文提示(VICP)，一种新框架，其中在已见类别上训练的模型可以直接泛化到未见的新类别，仅使用上下文示例作为提示，无需参数适应。VICP结合大语言模型和视觉基础模型：大语言模型通过特定任务提示从少量正负样本对中推断语义身份规则，然后通过动态视觉提示引导视觉基础模型（如DINO）提取身份判别性特征。通过对齐大语言模型衍生的语义概念与视觉基础模型的预训练先验，VICP实现了对新颖类别的泛化，消除了数据集特定重新训练的需求。为支持评估，我们引入了ShopID10K，一个包含来自电子商务平台的10K个物体实例的数据集，具有多视图图像和跨域测试功能。在ShopID10K和多种再识别基准测试中的实验表明，VICP在未见类别上明显优于基线方法。代码可在https://github.com/Hzzone/VICP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current object re-identification (ReID) methods train domain-specific models(e.g., for persons or vehicles), which lack generalization and demand costlylabeled data for new categories. While self-supervised learning reducesannotation needs by learning instance-wise invariance, it struggles to capture\textit{identity-sensitive} features critical for ReID. This paper proposesVisual In-Context Prompting~(VICP), a novel framework where models trained onseen categories can directly generalize to unseen novel categories using only\textit{in-context examples} as prompts, without requiring parameteradaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infersemantic identity rules from few-shot positive/negative pairs throughtask-specific prompting, which then guides a VFM (\eg, DINO) to extractID-discriminative features via \textit{dynamic visual prompts}. By aligningLLM-derived semantic concepts with the VFM's pre-trained prior, VICP enablesgeneralization to novel categories, eliminating the need for dataset-specificretraining. To support evaluation, we introduce ShopID10K, a dataset of 10Kobject instances from e-commerce platforms, featuring multi-view images andcross-domain testing. Experiments on ShopID10K and diverse ReID benchmarksdemonstrate that VICP outperforms baselines by a clear margin on unseencategories. Code is available at https://github.com/Hzzone/VICP.</description>
      <author>example@mail.com (Zhizhong Huang, Xiaoming Liu)</author>
      <guid isPermaLink="false">2508.21222v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</title>
      <link>http://arxiv.org/abs/2508.21112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了EO-Robotics，包括EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，通过交错视觉-文本-动作预训练在多模态具身推理和机器人控制方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;人类能够在开放世界中无缝执行多模态推理和物理交互的能力，是通用具身智能系统的核心目标。最近的视觉-语言-动作模型在通用机器人控制方面取得了显著进展，但仍然无法达到人类水平的交错推理和交互灵活性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现人类水平灵活性的具身智能系统，特别是在交错推理和交互方面，以解决现有视觉-语言-动作模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入EO-Robotics，包括EO-1模型和EO-Data1.5M数据集；采用统一架构处理多模态输入；构建包含150多万个样本的数据集；通过自回归解码和流匹配去噪协同训练模型；在多种长视野、灵巧操作任务中验证效果。&lt;h4&gt;主要发现&lt;/h4&gt;交错视觉-文本-动作学习对开放世界理解和泛化有效；EO-1在各种具身形式的长视野、灵巧操作任务中表现出色；统一架构和大规模高质量数据集的结合显著提升了多模态具身推理和机器人控制能力。&lt;h4&gt;结论&lt;/h4&gt;EO-Robotics通过EO-1模型和EO-Data1.5M数据集的协同设计，实现了在多模态具身推理和机器人控制方面的优越性能，为开发高级具身基础模型提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;人类在开放世界中无缝执行多模态推理和物理交互的能力，是通用具身智能系统的核心目标。最近在大型机器人和视觉-文本数据上联合训练的视觉-语言-动作模型，在通用机器人控制方面取得了显著进展。然而，它们仍然无法达到人类水平的交错推理和交互灵活性。在这项工作中，我们介绍了EO-Robotics，它包括EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，通过交错视觉-文本-动作预训练在多模态具身推理和机器人控制方面实现了卓越性能。EO-1的开发基于两个关键支柱：(i)一个统一架构，无差别处理多模态输入（图像、文本、视频和动作）；(ii)一个大规模、高质量的多模态具身推理数据集EO-Data1.5M，包含超过150万个样本，强调交错视觉-文本-动作理解。EO-1通过在EO-Data1.5M上自回归解码和流匹配去噪的协同训练，实现无缝的机器人动作生成和多模态具身推理。大量实验证明了交错视觉-文本-动作学习对开放世界理解和泛化的有效性，通过多种具身形式下的各种长视野、灵巧操作任务得到验证。本文详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略和训练方法，为开发高级具身基础模型提供了宝贵的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The human ability to seamlessly perform multimodal reasoning and physicalinteraction in the open world is a core goal for general-purpose embodiedintelligent systems. Recent vision-language-action (VLA) models, which areco-trained on large-scale robot and visual-text data, have demonstrated notableprogress in general robot control. However, they still fail to achievehuman-level flexibility in interleaved reasoning and interaction. In this work,introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 isa unified embodied foundation model that achieves superior performance inmultimodal embodied reasoning and robot control through interleavedvision-text-action pre-training. The development of EO-1 is based on two keypillars: (i) a unified architecture that processes multimodal inputsindiscriminately (image, text, video, and action), and (ii) a massive,high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which containsover 1.5 million samples with emphasis on interleaved vision-text-actioncomprehension. EO-1 is trained through synergies between auto-regressivedecoding and flow matching denoising on EO-Data1.5M, enabling seamless robotaction generation and multimodal embodied reasoning. Extensive experimentsdemonstrate the effectiveness of interleaved vision-text-action learning foropen-world understanding and generalization, validated through a variety oflong-horizon, dexterous manipulation tasks across multiple embodiments. Thispaper details the architecture of EO-1, the data construction strategy ofEO-Data1.5M, and the training methodology, offering valuable insights fordeveloping advanced embodied foundation models.</description>
      <author>example@mail.com (Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang)</author>
      <guid isPermaLink="false">2508.21112v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement</title>
      <link>http://arxiv.org/abs/2508.20954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于遥感技术的橄榄树分割方法，通过整合SAM模型和基于树木排列及形状大小约束的校正技术，显著提高了从卫星图像中识别和分割橄榄树的准确性。&lt;h4&gt;背景&lt;/h4&gt;在气候变化背景下，维持橄榄生物多样性至关重要，通过遥感技术进行早期异常检测和处理可以提供有效的管理解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新方法，从卫星图像中准确识别和分割农业地块中的橄榄树。&lt;h4&gt;方法&lt;/h4&gt;利用基础模型和先进分割技术，整合Segment Anything Model (SAM)，并结合基于树木田间排列的可学习约束和关于形状、大小的约束进行校正。&lt;h4&gt;主要发现&lt;/h4&gt;该方法达到了98%的准确率，显著超过了初始SAM 82%的性能。&lt;h4&gt;结论&lt;/h4&gt;结合SAM模型和基于树木排列及形状大小约束的校正方法，可以显著提高从卫星图像中识别和分割橄榄树的准确性，为橄榄生物多样性的保护和管理提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;在已证实的气候变化背景下，利用遥感技术进行早期异常检测和处理以维持橄榄生物多样性至关重要，提供了有效的管理解决方案。本文提出了一种从卫星图像中分割橄榄树的创新方法。通过利用基础模型和先进的分割技术，研究整合了分割任何模型(SAM)来准确识别和分割农业地块中的橄榄树。该方法包括基于树木田间排列和关于形状和大小的可学习约束的SAM分割和校正。我们的方法达到了98%的准确率，显著超过了初始SAM 82%的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of proven climate change, maintaining olive biodiversitythrough early anomaly detection and treatment using remote sensing technologyis crucial, offering effective management solutions. This paper presents aninnovative approach to olive tree segmentation from satellite images. Byleveraging foundational models and advanced segmentation techniques, the studyintegrates the Segment Anything Model (SAM) to accurately identify and segmentolive trees in agricultural plots. The methodology includes SAM segmentationand corrections based on trees alignement in the field and a learanbleconstraint about the shape and the size. Our approach achieved a 98\% accuracyrate, significantly surpassing the initial SAM performance of 82\%.</description>
      <author>example@mail.com (Amir Jmal, Chaima Chtourou, Mahdi Louati, Abdelaziz Kallel, Houda Khmila)</author>
      <guid isPermaLink="false">2508.20954v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
  <item>
      <title>Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.20909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Dino U-Net的新型编码器-解码器架构，利用DINOv3视觉基础模型的高保真密集特征进行医学图像分割。该架构通过专门的适配器融合语义特征与空间细节，并设计了保真感知投影模块来保持特征质量。实验证明，该方法在多个医学图像数据集上取得了最先进的性能，且具有良好的可扩展性。&lt;h4&gt;背景&lt;/h4&gt;基于大规模自然图像数据集预训练的基础模型为医学图像分割提供了强大的范式，但有效地转移其学习表示用于精确的临床应用仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的架构Dino U-Net，利用DINOv3视觉基础模型的高保真密集特征，提高医学图像分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Dino U-Net架构，包含：1) 基于冻结的DINOv3骨干网络的编码器；2) 专门的适配器用于融合模型的丰富语义特征与低级空间细节；3) 保真感知投影模块(FAPM)在降维过程中保持表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;在七个多样化的公共医学图像分割数据集上进行了广泛实验，结果显示Dino U-Net实现了最先进的性能，在各种成像模态上始终优于先前的方法。该框架具有高度可扩展性，随着骨干模型大小的增加，分割准确性持续提高，达到70亿参数变体。&lt;h4&gt;结论&lt;/h4&gt;利用通用基础模型的优越、密集预训练特征为提高医学图像分割准确性提供了一种高度有效且参数效率高的方法。&lt;h4&gt;翻译&lt;/h4&gt;在大型自然图像数据集上预训练的基础模型为医学图像分割提供了强大的范式。然而，有效地转移其学习表示用于精确的临床应用仍然是一个挑战。在这项工作中，我们提出了Dino U-Net，一种新的编码器-解码器架构，旨在利用DINOv3视觉基础模型的高保真密集特征。我们的架构引入了一个构建于冻结DINOv3骨干网络之上的编码器，该编码器采用专门的适配器来融合模型的丰富语义特征和低级空间细节。为了在降维过程中保持这些表示的质量，我们设计了一种新的保真感知投影模块(FAPM)，有效地精炼和解码器的特征投影。我们在七个多样化的公共医学图像分割数据集上进行了广泛的实验。我们的结果表明，Dino U-Net实现了最先进的性能，在各种成像模态上始终优于先前的方法。我们的框架被证明具有高度可扩展性，随着骨干模型大小的增加，分割准确性持续提高，达到70亿参数的变体。研究结果表明，利用通用基础模型的优越、密集预训练特征为提高医学图像分割准确性提供了一种高度有效且参数效率高的方法。代码可在https://github.com/yifangao112/DinoUNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models pre-trained on large-scale natural image datasets offer apowerful paradigm for medical image segmentation. However, effectivelytransferring their learned representations for precise clinical applicationsremains a challenge. In this work, we propose Dino U-Net, a novelencoder-decoder architecture designed to exploit the high-fidelity densefeatures of the DINOv3 vision foundation model. Our architecture introduces anencoder built upon a frozen DINOv3 backbone, which employs a specializedadapter to fuse the model's rich semantic features with low-level spatialdetails. To preserve the quality of these representations during dimensionalityreduction, we design a new fidelity-aware projection module (FAPM) thateffectively refines and projects the features for the decoder. We conductedextensive experiments on seven diverse public medical image segmentationdatasets. Our results show that Dino U-Net achieves state-of-the-artperformance, consistently outperforming previous methods across various imagingmodalities. Our framework proves to be highly scalable, with segmentationaccuracy consistently improving as the backbone model size increases up to the7-billion-parameter variant. The findings demonstrate that leveraging thesuperior, dense-pretrained features from a general-purpose foundation modelprovides a highly effective and parameter-efficient approach to advance theaccuracy of medical image segmentation. The code is available athttps://github.com/yifangao112/DinoUNet.</description>
      <author>example@mail.com (Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao)</author>
      <guid isPermaLink="false">2508.20909v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Turning Tabular Foundation Models into Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2508.20906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为G2T-FM的简单图基础模型，利用表格基础模型TabPFNv2作为骨干网络，通过邻域特征聚合和结构嵌入处理多样化节点特征，在图机器学习任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在自然语言处理和计算机视觉领域取得革命性进展，但在图机器学习领域的应用仍不充分。设计图基础模型(GFMs)的主要挑战是处理不同图数据集中多样化的节点特征。虽然已有研究专注于文本属性图，但处理其他类型特征的问题尚未解决，这一问题在表格数据机器学习领域同样存在。&lt;h4&gt;目的&lt;/h4&gt;受表格基础模型(如TabPFNv2)成功的启发，本文旨在提出一种简单有效的图基础模型，利用表格基础模型解决图机器学习中的特征处理挑战。&lt;h4&gt;方法&lt;/h4&gt;G2T-FM通过三步处理：1)使用邻域特征聚合增强原始节点特征；2)添加结构嵌入；3)将TabPFNv2应用于构建的节点表示。即使在完全的上下文学习模式下，该方法也无需额外训练即可有效工作。&lt;h4&gt;主要发现&lt;/h4&gt;即使在完全的上下文学习模式下，G2T-FM也显著优于公开可用的GFMs，性能与从头开始训练的经过良好调整的GNNs相当。微调后，G2T-FM超越了经过良好调整的GNN基线，展示了利用表格基础模型进行图机器学习任务的潜力。&lt;h4&gt;结论&lt;/h4&gt;本文揭示了利用表格基础模型进行图机器学习任务的一个先前被忽视的方向。G2T-FM的成功表明，表格基础模型可以有效地应用于图数据，为图机器学习提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;虽然基础模型已经彻底改变了自然语言处理和计算机视觉等领域，但它们在图机器学习中的应用和潜力在很大程度上仍未被探索。设计图基础模型(GFMs)的一个关键挑战是处理多样化的节点特征，这些特征可能在不同图数据集中有所不同。尽管许多关于GFMs的工作只专注于文本属性图，但处理GFMs中其他类型任意特征的问题尚未得到充分解决。然而，这个问题并非图领域独有，表格数据机器学习领域也存在类似问题。在这项工作中，受最近表格基础模型(如TabPFNv2)成功的启发，我们提出了G2T-FM，这是一种采用TabPFNv2作为骨干的简单图基础模型。具体来说，G2T-FM通过邻域特征聚合增强原始节点特征，添加结构嵌入，然后将TabPFNv2应用于构建的节点表示。即使在完全的上下文学习模式下，我们的模型也取得了强大的结果，显著优于公开可用的GFMs，性能与从头开始训练的经过良好调整的GNNs相当。此外，微调后，G2T-FM超越了经过良好调整的GNN基线，突显了所提出方法的潜力。更广泛地说，我们的论文揭示了利用表格基础模型进行图机器学习任务的一个先前被忽视的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models have revolutionized such fields as natural languageprocessing and computer vision, their application and potential within graphmachine learning remain largely unexplored. One of the key challenges indesigning graph foundation models (GFMs) is handling diverse node features thatcan vary across different graph datasets. Although many works on GFMs have beenfocused exclusively on text-attributed graphs, the problem of handlingarbitrary features of other types in GFMs has not been fully addressed.However, this problem is not unique to the graph domain, as it also arises inthe field of machine learning for tabular data. In this work, motivated by therecent success of tabular foundation models like TabPFNv2, we propose G2T-FM, asimple graph foundation model that employs TabPFNv2 as a backbone.Specifically, G2T-FM augments the original node features with neighborhoodfeature aggregation, adds structural embeddings, and then applies TabPFNv2 tothe constructed node representations. Even in a fully in-context regime, ourmodel achieves strong results, significantly outperforming publicly availableGFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting thepotential of the proposed approach. More broadly, our paper reveals apreviously overlooked direction of utilizing tabular foundation models forgraph machine learning tasks.</description>
      <author>example@mail.com (Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova)</author>
      <guid isPermaLink="false">2508.20906v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training</title>
      <link>http://arxiv.org/abs/2508.20813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DVCTNet的新型双视图协同训练网络，用于从全景X射线中准确检测龋齿，通过结合全局和局部视图信息，显著提高了检测准确性。&lt;h4&gt;背景&lt;/h4&gt;从全景X射线准确检测龋齿对于防止病变进展至关重要，但当前检测方法由于龋齿的微妙对比度和多样化病变形态而往往准确性不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的双视图协同训练网络(DVCTNet)用于准确的龋齿检测，模拟牙医的工作流程，结合整体图像筛查和详细牙齿级别检查。&lt;h4&gt;方法&lt;/h4&gt;DVCTNet首先使用自动牙齿检测建立两个互补视图：全景X射线图像的全局视图和裁剪牙齿图像的局部视图。在两个视图上分别预训练两个视觉基础模型，全局视图模型作为检测主干生成区域提议和全局特征，局部视图模型提取详细特征。通过引入门控跨视图注意力(GCV-Atten)模块动态融合双视图特征，并将融合特征整合回检测模型进行最终龋齿检测。&lt;h4&gt;主要发现&lt;/h4&gt;DVCTNet在公共数据集和新创建的高精度龋齿检测数据集上进行了测试和验证。该数据集使用口腔内图像和全景X射线进行双重标注。实验结果表明DVCTNet在两个数据集上都优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;DVCTNet方法具有临床适用性，其代码和标记数据集已在GitHub上公开分享，https://github.com/ShanghaiTech-IMPACT/DVCTNet。&lt;h4&gt;翻译&lt;/h4&gt;从全景X射线准确检测龋齿在防止病变进展中起着关键作用。然而，由于龋齿的微妙对比度和多样化的病变形态，当前的检测方法往往产生次优的准确性。在这项工作中，受牙医系统性地结合整体图像筛查与详细牙齿级别检查的临床工作流程启发，我们提出了DVCTNet，一种用于准确龋齿检测的新型双视图协同训练网络。我们的DVCTNet首先采用自动牙齿检测建立两个互补视图：来自全景X射线图像的全局视图和来自裁剪牙齿图像的局部视图。然后我们在两个视图上分别预训练两个视觉基础模型。全局视图基础模型作为检测主干，生成区域提议和全局特征，而局部视图模型从区域提议匹配的相应裁剪牙齿块中提取详细特征。为了有效整合两个视图的信息，我们引入了一个门控跨视图注意力(GCV-Atten)模块，该模块动态融合双视图特征，通过将融合特征整合回检测模型进行最终龋齿检测，从而增强检测流程。为了严格评估我们的DVCTNet，我们在公共数据集上测试了它，并在新创建的高精度龋齿检测数据集上进一步验证了其性能，该数据集使用口腔内图像和全景X射线进行双重标注。实验结果表明，DVCTNet在两个数据集上都优于现有的最先进方法，表明我们方法的临床适用性。我们的代码和标记数据集可在https://github.com/ShanghaiTech-IMPACT/DVCTNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate dental caries detection from panoramic X-rays plays a pivotal rolein preventing lesion progression. However, current detection methods oftenyield suboptimal accuracy due to subtle contrast variations and diverse lesionmorphology of dental caries. In this work, inspired by the clinical workflowwhere dentists systematically combine whole-image screening with detailedtooth-level inspection, we present DVCTNet, a novel Dual-View Co-Trainingnetwork for accurate dental caries detection. Our DVCTNet starts with employingautomated tooth detection to establish two complementary views: a global viewfrom panoramic X-ray images and a local view from cropped tooth images. We thenpretrain two vision foundation models separately on the two views. Theglobal-view foundation model serves as the detection backbone, generatingregion proposals and global features, while the local-view model extractsdetailed features from corresponding cropped tooth patches matched by theregion proposals. To effectively integrate information from both views, weintroduce a Gated Cross-View Attention (GCV-Atten) module that dynamicallyfuses dual-view features, enhancing the detection pipeline by integrating thefused features back into the detection model for final caries detection. Torigorously evaluate our DVCTNet, we test it on a public dataset and furthervalidate its performance on a newly curated, high-precision dental cariesdetection dataset, annotated using both intra-oral images and panoramic X-raysfor double verification. Experimental results demonstrate DVCTNet's superiorperformance against existing state-of-the-art (SOTA) methods on both datasets,indicating the clinical applicability of our method. Our code and labeleddataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.</description>
      <author>example@mail.com (Tao Luo, Han Wu, Tong Yang, Dinggang Shen, Zhiming Cui)</author>
      <guid isPermaLink="false">2508.20813v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding</title>
      <link>http://arxiv.org/abs/2508.20765v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review for IJCV&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视频内容的自动理解正在快速发展，机器能够理解视频中具体的可见内容（如物体、动作、事件或场景），但人类具有识别抽象概念（如正义、自由、团结）的独特能力。抽象概念识别是视频理解中的一个重要开放挑战。&lt;h4&gt;背景&lt;/h4&gt;基于深度神经网络和大型数据集，机器在理解视频内容方面取得了显著进步，但与人类相比，机器在理解抽象概念方面仍有差距，需要在多个语义层次上基于上下文信息进行推理。&lt;h4&gt;目的&lt;/h4&gt;利用基础模型的最新进展来解决视频中抽象概念理解的问题，使模型更符合人类推理和价值观，促进高级抽象概念的自动化理解。&lt;h4&gt;方法&lt;/h4&gt;通过综述研究用于理解视频中抽象概念的不同任务和数据集，分析过去几十年研究者如何利用当时可用的工具解决这些任务。&lt;h4&gt;主要发现&lt;/h4&gt;研究者们一直在尝试解决抽象概念理解任务，并充分利用当时可用的工具，社区数十年的经验可以帮助我们解决这个重要的开放性挑战。&lt;h4&gt;结论&lt;/h4&gt;在多模态基础模型时代重新审视抽象概念理解问题时，借鉴社区经验可以帮助我们避免'重复发明轮子'，更有效地推进这一领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;视频内容的自动理解正在迅速发展。得益于深度神经网络和大型数据集，机器越来越能够理解视频帧中具体可见的内容，无论是物体、动作、事件还是场景。相比之下，人类仍然保留着一种独特的能力，能够超越具体实体并识别如正义、自由和团结等抽象概念。抽象概念识别构成了视频理解中的一个关键开放挑战，其中基于上下文信息在多个语义层次上进行推理是关键。在本文中，我们认为基础模型的最新进展为解决视频中抽象概念理解问题提供了理想的环境。高级抽象概念的自动化理解至关重要，因为它使模型更符合人类的推理和价值观。在本综述中，我们研究了用于理解视频内容中抽象概念的不同任务和数据集。我们观察到，研究人员长期以来一直在尝试解决这些任务，充分利用他们可用的工具。我们主张借鉴社区数十年的经验将帮助我们阐明这一重要的开放性重大挑战，并避免在多模态基础模型时代重新审视它时'重复发明轮子'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The automatic understanding of video content is advancing rapidly. Empoweredby deeper neural networks and large datasets, machines are increasingly capableof understanding what is concretely visible in video frames, whether it beobjects, actions, events, or scenes. In comparison, humans retain a uniqueability to also look beyond concrete entities and recognize abstract conceptslike justice, freedom, and togetherness. Abstract concept recognition forms acrucial open challenge in video understanding, where reasoning on multiplesemantic levels based on contextual information is key. In this paper, we arguethat the recent advances in foundation models make for an ideal setting toaddress abstract concept understanding in videos. Automated understanding ofhigh-level abstract concepts is imperative as it enables models to be morealigned with human reasoning and values. In this survey, we study differenttasks and datasets used to understand abstract concepts in video content. Weobserve that, periodically and over a long period, researchers have attemptedto solve these tasks, making the best use of the tools available at theirdisposal. We advocate that drawing on decades of community experience will helpus shed light on this important open grand challenge and avoid ``re-inventingthe wheel'' as we start revisiting it in the era of multi-modal foundationmodels.</description>
      <author>example@mail.com (Gowreesh Mago, Pascal Mettes, Stevan Rudinac)</author>
      <guid isPermaLink="false">2508.20765v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>ArtFace: Towards Historical Portrait Face Identification via Model Adaptation</title>
      <link>http://arxiv.org/abs/2508.20626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 3 figures. ArtMetrics @ ICCV 2025 (non-archival). Paper page  at https://www.idiap.ch/paper/artface/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了基础模型在改进艺术品中面部识别方面的潜力，通过微调基础模型并将其嵌入与传统面部识别网络结合，实现了比当前最先进方法更显著的改进。&lt;h4&gt;背景&lt;/h4&gt;识别历史画中的人物对艺术史学家了解人物生平及其自我呈现方式至关重要，但这一过程通常很主观且受限于数据缺乏和风格变化。传统面部识别模型虽在照片上表现良好，但在画作上表现不佳，因存在领域转换和类内变化高的问题。艺术因素如风格、技巧、意图和其他作品的影响进一步增加了识别难度。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型在改进艺术品中面部识别方面的潜力，以解决传统方法在艺术画作上面部识别的局限性。&lt;h4&gt;方法&lt;/h4&gt;通过微调基础模型并将其嵌入与传统面部识别网络的嵌入相结合，创建一种混合方法来提高艺术品中面部识别的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型与传统方法相结合后，在艺术品面部识别任务上取得了比当前最先进方法更显著的改进效果。&lt;h4&gt;结论&lt;/h4&gt;基础模型能够有效填补传统面部识别方法在艺术品识别中无效的空白，为艺术史研究提供了新的技术支持。&lt;h4&gt;翻译&lt;/h4&gt;识别历史画中的人物是艺术史学家的一项关键任务，能够深入了解他们的生活以及他们希望如何被呈现。然而，这一过程通常很主观，并且受限于数据缺乏和风格变化。自动化面部识别能够处理具有挑战性的条件并提供协助，但虽然传统面部识别模型在照片上表现良好，它们却因领域转换和类内变化高而在画作上表现不佳。艺术因素如风格、技巧、意图以及受其他作品的影响进一步使识别复杂化。在本研究中，我们调查了基础模型在改进艺术品中面部识别方面的潜力。通过微调基础模型并将其嵌入与传统面部识别网络的嵌入相结合，我们展示了比当前最先进方法更显著的改进。我们的结果表明，基础模型可以在传统方法无效的情况下填补空白。论文页面位于 https://www.idiap.ch/paper/artface/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying sitters in historical paintings is a key task for art historians,offering insight into their lives and how they chose to be seen. However, theprocess is often subjective and limited by the lack of data and stylisticvariations. Automated facial recognition is capable of handling challengingconditions and can assist, but while traditional facial recognition modelsperform well on photographs, they struggle with paintings due to domain shiftand high intra-class variation. Artistic factors such as style, skill, intent,and influence from other works further complicate recognition. In this work, weinvestigate the potential of foundation models to improve facial recognition inartworks. By fine-tuning foundation models and integrating their embeddingswith those from conventional facial recognition networks, we demonstratenotable improvements over current state-of-the-art methods. Our results showthat foundation models can bridge the gap where traditional methods areineffective. Paper page at https://www.idiap.ch/paper/artface/</description>
      <author>example@mail.com (Francois Poh, Anjith George, Sébastien Marcel)</author>
      <guid isPermaLink="false">2508.20626v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning</title>
      <link>http://arxiv.org/abs/2508.20549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了医学推理生成式奖励学习(MedGR²)框架，解决了医学领域视觉语言模型应用中高质量数据稀缺的问题，实现了数据生成与奖励模型协同发展的良性循环，显著提升了模型在跨模态和跨任务上的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;医学领域视觉语言模型(VLMs)的应用受限于高质量、专家标注数据的稀缺。现有数据集上的监督微调(SFT)在未见模态和任务上泛化能力差，而强化学习(RL)因缺乏可靠奖励信号也难以应用。&lt;h4&gt;目的&lt;/h4&gt;突破医学领域数据稀缺的限制，开发一种能够自我改进的良性循环框架，释放强化学习在构建真正可泛化医疗AI方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出医学推理生成式奖励学习(MedGR²)框架，协同开发数据生成器和奖励模型，实现高质量多模态医学数据的自动连续创建，并将这些数据用于监督微调和通过组相对策略优化(GRPO)的强化学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用MedGR²生成数据进行监督微调已超越大型人工策划数据集训练的基线；通过GRPO进行强化学习实现了最先进的跨模态和跨任务泛化能力，显著优于专门RL方法；由MedGR²赋能的紧凑型模型性能可与参数超过其10倍以上的基础模型相竞争。&lt;h4&gt;结论&lt;/h4&gt;MedGR²为高风险领域的数据高效学习提供了新范式，将问题从数据稀缺转变为数据生成，为医疗AI的发展开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)在医学中的应用受到高质量、专家标注数据稀缺的严重阻碍。在现有数据集上进行监督微调(SFT)通常在未见过的模态和任务上泛化能力差，而作为一种有前途的替代方案的强化学习(RL)，因在这个数据稀缺领域缺乏可靠的奖励信号而受阻。为打破这一僵局，我们引入了医学推理生成式奖励学习(MedGR²)，这是一个创建自我改进良性循环的新颖框架。MedGR²协同开发数据生成器和奖励模型，实现了高质量、多模态医学数据的自动、连续创建，这些数据既可作为监督微调的优质训练源，也可用于强化学习。我们的实验证明，使用MedGR²生成数据进行监督微调已经超越了在大型人工策划数据集上训练的基线。重要的是，当通过组相对策略优化(GRPO)利用这些数据进行强化学习时，我们的模型实现了最先进的跨模态和跨任务泛化能力，显著优于专门的基于RL的方法。此外，由MedGR²赋能的我们的紧凑型模型实现了与参数超过其10倍以上的基础模型相竞争的性能。MedGR²为高风险领域的数据高效学习提出了新范式，将问题从数据稀缺转变为数据生成，释放了RL构建真正可泛化的医疗AI的全部潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of Vision-Language Models (VLMs) in medicine is criticallyhampered by the scarcity of high-quality, expert-annotated data. SupervisedFine-Tuning (SFT) on existing datasets often leads to poor generalization onunseen modalities and tasks, while Reinforcement Learning (RL), a promisingalternative, is stymied by the lack of reliable reward signals in thisdata-scarce domain. To break this impasse, we introduce Generative RewardLearning for Medical Reasoning (MedGR$^2$), a novel framework that creates aself-improving virtuous cycle. MedGR$^2$ co-develops a data generator and areward model, enabling the automated, continuous creation of high-quality,multi-modal medical data that serves as both a superior training source for SFTand RL. Our experiments demonstrate that SFT with MedGR$^2$-produced dataalready surpasses baselines trained on large-scale, human-curated datasets.Crucially, when leveraging this data for RL via Group Relative PolicyOptimization (GRPO), our model achieves state-of-the-art cross-modality andcross-task generalization, significantly outperforming specialized RL-basedmethods. Furthermore, our compact model, empowered by MedGR$^2$, achievesperformance competitive with foundation models possessing over 10 times moreparameters. MedGR$^2$ presents a new paradigm for data-efficient learning inhigh-stakes domains, transforming the problem from data scarcity to datageneration and unlocking the full potential of RL for building trulygeneralizable medical AI.</description>
      <author>example@mail.com (Weihai Zhi, Jiayan Guo, Shangyang Li)</author>
      <guid isPermaLink="false">2508.20549v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.20530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的数据级融合框架，通过早期整合RGB图像和LiDAR数据来提高3D目标检测性能，解决了高质量3D标注获取困难的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的基于LiDAR的3D目标检测器通常依赖手动标注的标签进行训练，但获取高质量的3D标注既耗时又费力。现有的无监督3D目标检测方法简单融合LiDAR点云和RGB图像生成的伪边界框，忽视了两种数据的互补性。&lt;h4&gt;目的&lt;/h4&gt;解决高质量3D标注获取困难的问题，提出一种新的数据级融合框架，早期整合RGB图像和LiDAR数据，提高伪边界框质量和定位准确性。&lt;h4&gt;方法&lt;/h4&gt;利用视觉基础模型进行图像的实例分割和深度估计；引入双向融合方法，使真实点从2D空间获取类别标签，同时将2D像素投影到3D增强点密度；提出局部和全局过滤方法抑制深度估计误差和移除分割异常值；开发基于数据级融合的动态自进化策略迭代优化伪边界框。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，该方法训练的检测器显著优于之前的最先进方法，在nuScenes验证基准上达到28.4% mAP。&lt;h4&gt;结论&lt;/h4&gt;通过早期整合RGB图像和LiDAR数据，有效提高了伪边界框质量，动态自进化策略显著提高了定位准确性，为无监督3D目标检测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;现有的基于LiDAR的3D目标检测器通常依赖手动标注标签进行训练以获得良好性能。然而，获取高质量3D标签既耗时又费力。为解决这一问题，近期工作探索无监督3D目标检测，引入RGB图像作为辅助模态协助生成伪边界框。然而，这些方法简单融合LiDAR点云和RGB图像生成的伪边界框。这种标签级融合策略对伪边界框质量的提升有限，因为它忽视了LiDAR和RGB图像数据的互补性。为克服上述局限，我们提出了一种新颖的数据级融合框架，在早期阶段整合RGB图像和LiDAR数据。具体而言，我们利用视觉基础模型进行图像的实例分割和深度估计，并引入双向融合方法，使真实点从2D空间获取类别标签，同时将2D像素投影到3D以增强真实点密度。为减轻深度和分割估计带来的噪声，我们提出了一种局部和全局过滤方法，应用局部半径过滤抑制深度估计误差，使用全局统计过滤移除分割引起的异常值。此外，我们提出了一种基于数据级融合的动态自进化策略，在密集表示下迭代优化伪边界框，显著提高了定位精度。在nuScenes数据集上的大量实验证明，我们方法训练的检测器显著优于之前的最先进方法，在nuScenes验证基准上达到28.4% mAP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无监督3D目标检测中伪框质量不高的问题。现有基于LiDAR的3D目标检测器依赖大量手动标注数据，而获取高质量3D标注非常耗时耗力（标注一个3D边界框约需5分钟），这使得大规模标注不切实际。无监督学习可以减少对标注数据的依赖，降低成本，使3D目标检测技术更容易应用于新场景，对自动驾驶等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：早期方法仅依靠点云聚类生成伪框，无法获取类别信息且受点云稀疏性限制；近期多模态方法虽引入图像数据但仅在后期进行标签级融合，未能充分利用LiDAR和图像的互补优势。基于此，作者设计了数据级融合框架，在早期阶段集成两种数据。作者借鉴了视觉基础模型（如SEEM和DepthAnything）进行图像分割和深度估计，以及无监督3D目标检测中的自进化技术，但创新性地将这些技术应用于数据层面的融合而非标签层面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在数据层面而非标签层面早期融合LiDAR和RGB图像信息，生成高质量、类别感知的伪框。整体流程分两阶段：1)数据级融合的初始伪框生成：使用视觉模型获取图像分割和深度信息，进行双向融合（真实点获取2D标签，2D像素生成3D伪点），应用局部过滤（处理深度估计噪声）和全局过滤（处理分割噪声），最后拟合生成初始伪框；2)动态自进化：通过实点密集化增加点云密度，监控损失变化动态更新伪框，保留高置信度检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)数据级融合方法，包含双向融合（真实点获取类别标签，2D像素投影到3D增强点密度）和局部-全局过滤（减轻深度和分割噪声）；2)基于数据级融合的动态自进化策略，在密集表示下迭代优化伪框。相比之前工作，不同之处在于：融合时机从后期标签级改为早期数据级；融合内容从简单整合伪框变为融合真实点和伪点；专门处理了深度和分割估计带来的噪声；在密集表示而非稀疏点云空间进行自进化，提高了效率和精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种数据级LiDAR-相机融合方法，通过早期双向融合和噪声过滤生成高质量类别感知伪框，并结合动态自进化策略，显著提升了无监督3D目标检测的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing LiDAR-based 3D object detectors typically rely on manually annotatedlabels for training to achieve good performance. However, obtaininghigh-quality 3D labels is time-consuming and labor-intensive. To address thisissue, recent works explore unsupervised 3D object detection by introducing RGBimages as an auxiliary modal to assist pseudo-box generation. However, thesemethods simply integrate pseudo-boxes generated by LiDAR point clouds and RGBimages. Yet, such a label-level fusion strategy brings limited improvements tothe quality of pseudo-boxes, as it overlooks the complementary nature in termsof LiDAR and RGB image data. To overcome the above limitations, we propose anovel data-level fusion framework that integrates RGB images and LiDAR data atan early stage. Specifically, we utilize vision foundation models for instancesegmentation and depth estimation on images and introduce a bi-directionalfusion method, where real points acquire category labels from the 2D space,while 2D pixels are projected onto 3D to enhance real point density. Tomitigate noise from depth and segmentation estimations, we propose a local andglobal filtering method, which applies local radius filtering to suppress depthestimation errors and global statistical filtering to removesegmentation-induced outliers. Furthermore, we propose a data-level fusionbased dynamic self-evolution strategy, which iteratively refines pseudo-boxesunder a dense representation, significantly improving localization accuracy.Extensive experiments on the nuScenes dataset demonstrate that the detectortrained by our method significantly outperforms that trained by previousstate-of-the-art methods with 28.4$\%$ mAP on the nuScenes validationbenchmark.</description>
      <author>example@mail.com (Mingqian Ji, Jian Yang, Shanshan Zhang)</author>
      <guid isPermaLink="false">2508.20530v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating</title>
      <link>http://arxiv.org/abs/2508.20437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 Tables, 5 Figures, AI Trustworthiness and Risk Assessment  for Challenged Contexts (ATRACC), Appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列预测模型已从经典统计方法演变为复杂的基础模型，但理解其成功或失败的原因仍具挑战性。本文结合传统可解释AI方法与基于评分的解释(RDE)来评估和解释不同领域中的TSFM性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测模型越来越多地用于生成影响现实世界行动的信息，但其复杂性、性能变化性和不透明性引发了对用户应如何与模型输出互动和依赖的担忧。&lt;h4&gt;目的&lt;/h4&gt;理解时间序列预测模型的复杂性、性能变化性和不透明性，以解决用户应如何与这些模型的输出互动和依赖的严重问题。&lt;h4&gt;方法&lt;/h4&gt;结合传统可解释人工智能(XAI)方法和基于评分的解释(RDE)来评估TSFM性能和可解释性，评估四种模型架构（ARIMA、梯度提升、Chronos和Llama）在四个异构数据集上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;在波动性大或数据稀疏的领域（如电力、汽车零部件），特征工程模型（如梯度提升）持续优于基础模型（如Chronos）并提供更可解释的解释；而基础模型仅在稳定或趋势驱动的情境（如金融）中表现出色。&lt;h4&gt;结论&lt;/h4&gt;不同类型的时间序列预测模型在不同领域和情境中表现各异，特征工程模型在复杂环境中表现更好，基础模型在稳定环境中更有效。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测模型已经从经典统计方法演变为复杂的基础模型，但理解这些模型成功或失败的原因仍然具有挑战性。尽管存在这一已知限制，时间序列预测模型越来越多地用于生成影响现实世界行动的信息。理解这些模型的复杂性、性能变化性和不透明性，对于解决用户应如何与这些模型的输出互动和依赖的严重问题变得至关重要。本文通过结合传统可解释人工智能(XAI)方法和基于评分的解释(RDE)来评估和解释不同领域和用例中的TSFM性能，以应对这些担忧。我们评估了四种不同的模型架构：ARIMA、梯度提升、Chronos（时间序列特定基础模型）和Llama（通用模型，包括微调版本和基础模型），这些模型在涵盖金融、能源、运输和汽车销售四个异构数据集上进行测试。通过这些研究，我们证明在波动性大或数据稀疏的领域（如电力、汽车零部件），特征工程模型（如梯度提升）持续优于基础模型（如Chronos），并提供更可解释的解释；而基础模型仅在稳定或趋势驱动的情境（如金融）中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series forecasting models (TSFM) have evolved from classical statisticalmethods to sophisticated foundation models, yet understanding why and whenthese models succeed or fail remains challenging. Despite this knownlimitation, time series forecasting models are increasingly used to generateinformation that informs real-world actions with equally real consequences.Understanding the complexity, performance variability, and opaque nature ofthese models then becomes a valuable endeavor to combat serious concerns abouthow users should interact with and rely on these models' outputs. This workaddresses these concerns by combining traditional explainable AI (XAI) methodswith Rating Driven Explanations (RDE) to assess TSFM performance andinterpretability across diverse domains and use cases. We evaluate fourdistinct model architectures: ARIMA, Gradient Boosting, Chronos (time-seriesspecific foundation model), Llama (general-purpose; both fine-tuned and basemodels) on four heterogeneous datasets spanning finance, energy,transportation, and automotive sales domains. In doing so, we demonstrate thatfeature-engineered models (e.g., Gradient Boosting) consistently outperformfoundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,car parts) while providing more interpretable explanations, whereas foundationmodels excel only in stable or trend-driven contexts (e.g., finance).</description>
      <author>example@mail.com (Michael Widener, Kausik Lakkaraju, John Aydin, Biplav Srivastava)</author>
      <guid isPermaLink="false">2508.20437v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models</title>
      <link>http://arxiv.org/abs/2508.20345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医疗视觉语言模型(VLMs)在临床应用中展现出巨大潜力，但存在严重的安全风险。本研究提出了MedFoundationHub工具包，解决了医疗VLMs的安全挑战，使医生和工程师能够安全、高效地部署和使用这些模型。&lt;h4&gt;背景&lt;/h4&gt;医疗视觉语言模型(VLMs)在临床应用如自动报告生成、医生助手和不确定性量化方面展现出显著机会，但同时引入了严重的安全问题，特别是受保护健康信息(PHI)泄露、数据泄露和网络安全威胁风险。&lt;h4&gt;目的&lt;/h4&gt;解决医疗VLMs的安全挑战，提供一个安全且易于访问的解决方案，使医疗专业人员能够有效利用这些先进技术。&lt;h4&gt;方法&lt;/h4&gt;开发MedFoundationHub，一个图形用户界面(GUI)工具包，使医生能够手动选择和使用不同模型无需编程专业知识；支持工程师以即插即用方式高效部署医疗VLMs，无缝集成Hugging Face开源模型；通过Docker编排的、操作系统无关的部署确保隐私保护的推理。该工具只需配备单个NVIDIA A6000 GPU的离线本地工作站。&lt;h4&gt;主要发现&lt;/h4&gt;评估了五种最先进的VLMs(Google-MedGemma3-4B、Qwen2-VL-7B-Instruct、Qwen2.5-VL-7B-Instruct和LLaVA-1.5-7B/13B)，通过获得认证的病理学家对结肠病例和肾脏病例进行评估，共获得1015个临床医生-模型评分事件。评估揭示了这些模型存在离题答案、模糊推理和不一致的病理学术语等局限性。&lt;h4&gt;结论&lt;/h4&gt;MedFoundationHub提供了一个安全且易于访问的解决方案，用于在典型学术研究实验室资源内部署医疗VLMs。然而，当前医疗VLMs仍存在局限性，需要进一步改进以提高其临床适用性。&lt;h4&gt;翻译&lt;/h4&gt;医疗视觉语言模型(VLMs)的最新进展为临床应用开辟了显著机会，如自动报告生成、医生助手和不确定性量化。然而，尽管它们有潜力，医疗VLMs引入了严重的安全问题，最值得注意的是受保护健康信息(PHI)泄露、数据泄露和易受网络威胁的风险 - 这些在医院环境中尤其关键。即使用于研究或非临床目的，医疗机构也必须谨慎并实施保障措施。为应对这些挑战，我们提出了MedFoundationHub，一个图形用户界面(GUI)工具包，它：(1)使医生能够手动选择和使用不同模型而无需编程专业知识，(2)支持工程师以即插即用方式高效部署医疗VLMs，无缝集成Hugging Face开源模型，(3)通过Docker编排的、操作系统无关的部署确保隐私保护的推理。MedFoundationHub只需要配备单个NVIDIA A6000 GPU的离线本地工作站，使其在学术研究实验室的典型资源中既安全又易于访问。为了评估当前能力，我们邀请获得认证的病理学家部署和评估了五种最先进的VLMs(Google-MedGemma3-4B、Qwen2-VL-7B-Instruct、Qwen2.5-VL-7B-Instruct和LLaVA-1.5-7B/13B)。专家评估涵盖结肠病例和肾脏病例，共获得1015个临床医生-模型评分事件。这些评估揭示了反复出现的局限性，包括离题答案、模糊推理和不一致的病理学术语。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in medical vision-language models (VLMs) open up remarkableopportunities for clinical applications such as automated report generation,copilots for physicians, and uncertainty quantification. However, despite theirpromise, medical VLMs introduce serious security concerns, most notably risksof Protected Health Information (PHI) exposure, data leakage, and vulnerabilityto cyberthreats - which are especially critical in hospital environments. Evenwhen adopted for research or non-clinical purposes, healthcare organizationsmust exercise caution and implement safeguards. To address these challenges, wepresent MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)enables physicians to manually select and use different models withoutprogramming expertise, (2) supports engineers in efficiently deploying medicalVLMs in a plug-and-play fashion, with seamless integration of Hugging Faceopen-source models, and (3) ensures privacy-preserving inference throughDocker-orchestrated, operating system agnostic deployment. MedFoundationHubrequires only an offline local workstation equipped with a single NVIDIA A6000GPU, making it both secure and accessible within the typical resources ofacademic research labs. To evaluate current capabilities, we engagedboard-certified pathologists to deploy and assess five state-of-the-art VLMs(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, andLLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,yielding 1015 clinician-model scoring events. These assessments revealedrecurring limitations, including off-target answers, vague reasoning, andinconsistent pathology terminology.</description>
      <author>example@mail.com (Xiao Li, Yanfan Zhu, Ruining Deng, Wei-Qi Wei, Yu Wang, Shilin Zhao, Yaohong Wang, Haichun Yang, Yuankai Huo)</author>
      <guid isPermaLink="false">2508.20345v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2508.20085v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HERMES是一个用于移动双臂灵巧操作的人机学习框架，通过统一的强化学习方法将多源人类手部动作转化为机器人行为，并结合sim2real迁移和视觉定位技术实现复杂环境中的自主操作。&lt;h4&gt;背景&lt;/h4&gt;利用人类运动数据使机器人具备多样化操作技能是机器人操作领域的前沿方向，但将多源人类手部动作转化为可行的机器人行为具有挑战性，特别是对于具有复杂高维动作空间的多指灵巧手，且现有方法难以适应不同环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发HERMES框架，解决多源人类运动到机器人行为的转化问题，提高机器人在不同环境中的适应能力，并实现多样化非结构化环境中的自主操作。&lt;h4&gt;方法&lt;/h4&gt;HERMES采用统一的强化学习方法将异构人类手部动作转化为物理可行的机器人行为；设计基于深度图像的端到端sim2real迁移方法减少仿真到现实的差距；通过闭环Perspective-n-Point定位机制增强导航基础模型，实现视觉目标的精确对齐，连接自主导航与灵巧操作。&lt;h4&gt;主要发现&lt;/h4&gt;HERMES能够在多样化和实际场景中表现出可泛化的行为，成功执行多种复杂的移动双臂灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;HERMES框架有效解决了多源人类运动到机器人行为的转化挑战，通过sim2real迁移和定位机制在真实世界中实现了有效操作，具有广泛的适用性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，已提供中文翻译版本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging human motion data to impart robots with versatile manipulationskills has emerged as a promising paradigm in robotic manipulation.Nevertheless, translating multi-source human hand motions into feasible robotbehaviors remains challenging, particularly for robots equipped withmulti-fingered dexterous hands characterized by complex, high-dimensionalaction spaces. Moreover, existing approaches often struggle to produce policiescapable of adapting to diverse environmental conditions. In this paper, weintroduce HERMES, a human-to-robot learning framework for mobile bimanualdexterous manipulation. First, HERMES formulates a unified reinforcementlearning approach capable of seamlessly transforming heterogeneous human handmotions from multiple sources into physically plausible robotic behaviors.Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depthimage-based sim2real transfer method for improved generalization to real-worldscenarios. Furthermore, to enable autonomous operation in varied andunstructured environments, we augment the navigation foundation model with aclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precisealignment of visual goals and effectively bridging autonomous navigation anddexterous manipulation. Extensive experimental results demonstrate that HERMESconsistently exhibits generalizable behaviors across diverse, in-the-wildscenarios, successfully performing numerous complex mobile bimanual dexterousmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.</description>
      <author>example@mail.com (Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu)</author>
      <guid isPermaLink="false">2508.20085v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Is the medical image segmentation problem solved? A survey of current developments and future directions</title>
      <link>http://arxiv.org/abs/2508.20139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  80 pages, 38 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对基于深度学习的医学图像分割领域进行了全面回顾，分析了过去十年的进展和关键发展，并从七个关键维度探讨了该领域的发展趋势和未来方向。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割在过去二十年迅速发展，主要由深度学习驱动，能够精确高效地描绘不同成像模态中的细胞、组织、器官和病变。然而，当前模型在克服持续挑战方面仍存在差距。&lt;h4&gt;目的&lt;/h4&gt;探讨当前医学图像分割模型克服挑战的程度，识别仍存在的差距，并提供该领域的深入回顾，以激发未来创新。&lt;h4&gt;方法&lt;/h4&gt;对医学图像分割进行系统性回顾，追踪过去十年的进展，检查分割网络各部分的核心原则（多尺度分析、注意机制、先验知识整合），并围绕七个关键维度组织讨论。&lt;h4&gt;主要发现&lt;/h4&gt;医学图像分割领域经历了七大转变：(1)从监督学习向半监督/无监督学习转变；(2)从器官分割向病变重点任务转变；(3)多模态整合和域适应进展；(4)基础模型和迁移学习作用增强；(5)从确定性向概率性分割转变；(6)从2D向3D和4D分割进展；(7)从模型调用向分割代理趋势发展。&lt;h4&gt;结论&lt;/h4&gt;这些观点提供了基于深度学习的医学图像分割轨迹的全面概述，旨在激发未来创新，并通过维护持续更新的文献和开源资源库支持持续研究。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割在过去二十年取得了迅速发展，主要得益于深度学习，这使得能够在不同成像模态中精确高效地描绘细胞、组织、器官和病变。这一进展提出了一个基本问题：当前模型在多大程度上克服了持续存在的挑战，以及仍存在哪些差距？在本工作中，我们对医学图像分割进行了深入回顾，追溯了过去十年的进展和关键发展。我们检查了核心原则，包括多尺度分析、注意机制和先验知识整合，贯穿分割网络的编码器、瓶颈、跳跃连接和解码器组件。我们的讨论围绕七个关键维度组织：(1)从监督学习向半监督/无监督学习的转变，(2)从器官分割向病变重点任务的转变，(3)多模态整合和域适应的进展，(4)基础模型和迁移学习的作用，(5)从确定性向概率性分割的转变，(6)从2D向3D和4D分割的进展，以及(7)从模型调用向分割代理的趋势。这些观点共同提供了基于深度学习的医学图像分割轨迹的全面概述，并旨在激发未来的创新。为支持持续研究，我们在https://github.com/apple1986/medicalSegReview维护了一个持续更新的相关文献和开源资源库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image segmentation has advanced rapidly over the past two decades,largely driven by deep learning, which has enabled accurate and efficientdelineation of cells, tissues, organs, and pathologies across diverse imagingmodalities. This progress raises a fundamental question: to what extent havecurrent models overcome persistent challenges, and what gaps remain? In thiswork, we provide an in-depth review of medical image segmentation, tracing itsprogress and key developments over the past decade. We examine core principles,including multiscale analysis, attention mechanisms, and the integration ofprior knowledge, across the encoder, bottleneck, skip connections, and decodercomponents of segmentation networks. Our discussion is organized around sevenkey dimensions: (1) the shift from supervised to semi-/unsupervised learning,(2) the transition from organ segmentation to lesion-focused tasks, (3)advances in multi-modality integration and domain adaptation, (4) the role offoundation models and transfer learning, (5) the move from deterministic toprobabilistic segmentation, (6) the progression from 2D to 3D and 4Dsegmentation, and (7) the trend from model invocation to segmentation agents.Together, these perspectives provide a holistic overview of the trajectory ofdeep learning-based medical image segmentation and aim to inspire futureinnovation. To support ongoing research, we maintain a continually updatedrepository of relevant literature and open-source resources athttps://github.com/apple1986/medicalSegReview</description>
      <author>example@mail.com (Guoping Xu, Jayaram K. Udupa, Jax Luo, Songlin Zhao, Yajun Yu, Scott B. Raymond, Hao Peng, Lipeng Ning, Yogesh Rathi, Wei Liu, You Zhang)</author>
      <guid isPermaLink="false">2508.20139v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs</title>
      <link>http://arxiv.org/abs/2508.21044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MMG-Vid的新型无训练视觉标记剪枝框架，通过在段级和标记级最大化边际增益来减少视频理解中的冗余视觉标记，同时保持高性能。&lt;h4&gt;背景&lt;/h4&gt;Video Large Language Models在视频理解方面表现出色，但过多的视觉标记给实际应用带来显著计算挑战。当前方法通过视觉标记剪枝提高推理效率，但未考虑视频帧的动态特性和时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法未考虑视频动态特性和时间依赖性的问题，提高视频理解模型的效率，同时保持性能。&lt;h4&gt;方法&lt;/h4&gt;提出MMG-Vid框架，首先基于帧相似性将视频分段，动态分配每个段的标记预算以最大化边际增益；然后提出时间引导的DPC算法，联合建模帧间独特性和帧内多样性，最大化每个标记的边际增益。&lt;h4&gt;主要发现&lt;/h4&gt;MMG-Vid可保持原始性能的99.5%以上，有效减少75%的视觉标记，在LLaVA-OneVision-7B上将预填充阶段加速3.9倍。&lt;h4&gt;结论&lt;/h4&gt;MMG-Vid通过结合段级和标记级的最大边际增益方法，显著提高了效率，同时保持了强大的性能。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型在视频理解方面表现出色，但它们过多的视觉标记给实际应用带来了显著的计算挑战。当前方法通过视觉标记剪枝来提高推理效率，然而，它们没有考虑视频帧的动态特性和时间依赖性，因为它们将视频理解视为多帧任务。为了解决这些挑战，我们提出了MMG-Vid，一种新颖的无训练视觉标记剪枝框架，通过在段级和标记级最大化边际增益来去除冗余。具体来说，我们首先基于帧相似性将视频分段，然后动态分配每个段的标记预算以最大化每个段的边际增益。随后，我们提出了一种时间引导的DPC算法，联合建模帧间独特性和帧内多样性，从而最大化每个标记的边际增益。通过结合两个阶段，MMG-Vid可以最大化有限标记预算的利用，显著提高效率同时保持强大的性能。大量实验表明，MMG-Vid可以保持原始性能的99.5%以上，同时有效减少75%的视觉标记，并在LLaVA-OneVision-7B上将预填充阶段加速3.9倍。代码即将发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VLLMs) excel in video understanding, but theirexcessive visual tokens pose a significant computational challenge forreal-world applications. Current methods aim to enhance inference efficiency byvisual token pruning. However, they do not consider the dynamic characteristicsand temporal dependencies of video frames, as they perceive video understandingas a multi-frame task. To address these challenges, we propose MMG-Vid, a noveltraining-free visual token pruning framework that removes redundancy byMaximizing Marginal Gains at both segment-level and token-level. Specifically,we first divide the video into segments based on frame similarity, and thendynamically allocate the token budget for each segment to maximize the marginalgain of each segment. Subsequently, we propose a temporal-guided DPC algorithmthat jointly models inter-frame uniqueness and intra-frame diversity, therebymaximizing the marginal gain of each token. By combining both stages, MMG-Vidcan maximize the utilization of the limited token budget, significantlyimproving efficiency while maintaining strong performance. Extensiveexperiments demonstrate that MMG-Vid can maintain over 99.5% of the originalperformance, while effectively reducing 75% visual tokens and accelerating theprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.</description>
      <author>example@mail.com (Junpeng Ma, Qizhe Zhang, Ming Lu, Zhibin Wang, Qiang Zhou, Jun Song, Shanghang Zhang)</author>
      <guid isPermaLink="false">2508.21044v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</title>
      <link>http://arxiv.org/abs/2508.21010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://paritoshparmar.github.io/chainreaction/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的模块化框架，用于因果视频问答模型，通过解耦因果推理与答案生成，引入自然语言因果链作为可解释的中间表示，实现了透明且逻辑连贯的推理。&lt;h4&gt;背景&lt;/h4&gt;现有的因果视频问答模型难以处理高阶推理，依赖于不透明的整体流水线，将视频理解、因果推理和答案生成纠缠在一起，导致可解释性有限且依赖浅层启发式方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种可解释性强、能够进行高阶因果推理的视频问答框架，提高模型的可解释性、用户信任度和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段架构：因果链提取器(CCE)从视频-问题对生成因果链，因果链驱动回答器(CCDA)基于这些因果链生成答案；使用大型语言模型从现有数据集生成高质量因果链；提出CauCo评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;在三个大规模基准测试上，该方法不仅优于最先进模型，还在可解释性、用户信任和泛化能力方面取得显著提升。&lt;h4&gt;结论&lt;/h4&gt;该框架将CCE定位为可在不同领域重用的因果推理引擎，为因果视频问答提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;现有的因果视频问答(Causal-Why VideoQA)模型通常难以处理高阶推理，依赖于不透明的、整体的流水线，这些流水线将视频理解、因果推理和答案生成纠缠在一起。这些黑盒方法提供有限的解释性，并倾向于依赖浅层启发式方法。我们提出了一种新颖的模块化框架，明确地将因果推理与答案生成解耦，引入自然语言因果链作为可解释的中间表示。受人类认知模型启发，这些结构化的因果-效应序列桥接了低级视频内容与高级因果推理，实现了透明且逻辑连贯的推理。我们的两阶段架构包括一个从视频-问题对生成因果链的因果链提取器(CCE)，以及一个基于这些因果链生成答案的因果链驱动回答器(CCDA)。为了解决缺乏标注推理轨迹的问题，我们引入了一种可扩展的方法，使用大型语言模型从现有数据集生成高质量的因果链。我们还提出了CauCo，一个面向因果描述的新评估指标。在三个大规模基准测试上的实验表明，我们的方法不仅优于最先进的模型，还在可解释性、用户信任和泛化能力方面取得了显著提升——将CCE定位为可在不同领域重用的因果推理引擎。项目页面：https://paritoshparmar.github.io/chainreaction/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing Causal-Why Video Question Answering (VideoQA) models often strugglewith higher-order reasoning, relying on opaque, monolithic pipelines thatentangle video understanding, causal inference, and answer generation. Theseblack-box approaches offer limited interpretability and tend to depend onshallow heuristics. We propose a novel, modular framework that explicitlydecouples causal reasoning from answer generation, introducing natural languagecausal chains as interpretable intermediate representations. Inspired by humancognitive models, these structured cause-effect sequences bridge low-levelvideo content with high-level causal reasoning, enabling transparent andlogically coherent inference. Our two-stage architecture comprises a CausalChain Extractor (CCE) that generates causal chains from video-question pairs,and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded inthese chains. To address the lack of annotated reasoning traces, we introduce ascalable method for generating high-quality causal chains from existingdatasets using large language models. We also propose CauCo, a new evaluationmetric for causality-oriented captioning. Experiments on three large-scalebenchmarks demonstrate that our approach not only outperforms state-of-the-artmodels, but also yields substantial gains in explainability, user trust, andgeneralization -- positioning the CCE as a reusable causal reasoning engineacross diverse domains. Project page:https://paritoshparmar.github.io/chainreaction/</description>
      <author>example@mail.com (Paritosh Parmar, Eric Peh, Basura Fernando)</author>
      <guid isPermaLink="false">2508.21010v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.20478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Video-MTR，一个增强的多步推理框架，用于长视频理解。该框架通过迭代选择视频片段和渐进理解问题，结合双层奖励系统，实现了端到端的训练，无需外部视觉语言模型，在多个基准测试上表现出色。&lt;h4&gt;背景&lt;/h4&gt;长视频理解面临长程时间依赖和多个事件的挑战。现有方法通常依赖静态推理或外部视觉语言模型(VLMs)，存在复杂性和次优性能问题，缺乏端到端训练。&lt;h4&gt;目的&lt;/h4&gt;提出Video-MTR框架，实现迭代的关键视频片段选择和问题理解，解决长视频理解的挑战。&lt;h4&gt;方法&lt;/h4&gt;Video-MTR采用多步推理而非传统单步推理，基于对已处理片段的 evolving 理解和当前问题渐进选择视频片段。引入门控双层奖励系统，结合基于答案正确性的轨迹级奖励和强调帧-查询相关性的步级奖励，优化视频片段选择和问题理解，消除对外部VLMs的需求，允许端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;在VideoMME、MLVU和EgoSchema等基准测试上，Video-MTR在准确性和效率上都优于现有方法，推进了长视频理解的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;Video-MTR通过多步推理和新的奖励系统，有效解决了长视频理解的挑战，代表了该领域的进步。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解以其长程时间依赖和多个事件为特征，仍然是一个挑战。现有方法通常依赖静态推理或外部视觉语言模型(VLMs)，这些方法面临复杂性和次优性能问题，缺乏端到端训练。在本文中，我们提出Video-MTR，一个增强的多步推理框架，旨在实现迭代的关键视频片段选择和问题理解。与传统的单步生成预测的视频推理管道不同，Video-MTR在多步中进行推理，基于对已处理片段的 evolving 理解和当前问题，渐进地选择视频片段。这种迭代过程允许对视频进行更精细和上下文感知的分析。为确保中间推理过程，我们引入了一种新颖的门控双层奖励系统，结合基于答案正确性的轨迹级奖励和强调帧-查询相关性的步级奖励。该系统优化了视频片段选择和问题理解，消除了对外部VLMs的需求，允许端到端训练。在VideoMME、MLVU和EgoSchema等基准上的广泛实验表明，Video-MTR在准确性和效率上都优于现有方法，推进了长视频理解的最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-form video understanding, characterized by long-range temporaldependencies and multiple events, remains a challenge. Existing methods oftenrely on static reasoning or external visual-language models (VLMs), which faceissues like complexity and sub-optimal performance due to the lack ofend-to-end training. In this paper, we propose Video-MTR, a reinforcedmulti-turn reasoning framework designed to enable iterative key video segmentselection and question comprehension. Unlike traditional video reasoningpipeline, which generate predictions in a single turn, Video-MTR performsreasoning in multiple turns, selecting video segments progressively based onthe evolving understanding of previously processed segments and the currentquestion. This iterative process allows for a more refined and contextuallyaware analysis of the video. To ensure intermediate reasoning process, weintroduce a novel gated bi-level reward system, combining trajectory-levelrewards based on answer correctness and turn-level rewards emphasizingframe-query relevance. This system optimizes both video segment selection andquestion comprehension, eliminating the need for external VLMs and allowingend-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,and EgoSchema demonstrate that Video-MTR outperforms existing methods in bothaccuracy and efficiency, advancing the state-of-the-art in long videounderstanding.</description>
      <author>example@mail.com (Yuan Xie, Tianshui Chen, Zheng Ge, Lionel Ni)</author>
      <guid isPermaLink="false">2508.20478v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Latent Factor Point Processes for Patient Representation in Electronic Health Records</title>
      <link>http://arxiv.org/abs/2508.20327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究者提出了一种潜在因子点过程模型和Fourier-Eigen嵌入方法，用于有效分析电子健康记录中的时间结构数据，能够识别临床上有意义的患者亚组异质性。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录包含有价值的纵向患者级信息，但大多数统计方法将EHR代码的不规则时间简化为简单计数，丢弃了丰富的时间结构。现有时间模型通常施加限制性参数假设，或针对代码级而非患者级任务定制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保留EHR数据时间结构并适用于患者级任务的新模型，以更好地理解患者群体的异质性。&lt;h4&gt;方法&lt;/h4&gt;提出潜在因子点过程模型，将代码出现表示为高维点过程，其条件强度由低维潜在泊松过程驱动；基于此模型引入Fourier-Eigen嵌入，从观察过程的光谱密度矩阵构建患者表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;理论保证表明这些嵌入能有效捕获下游分类和聚类中特定亚组的时间模式；模拟和阿尔茨海默病EHR队列应用证明了该方法在揭示临床意义异质性方面的实际优势。&lt;h4&gt;结论&lt;/h4&gt;潜在因子点过程模型结合Fourier-Eigen嵌入能够有效利用EHR数据中的时间结构，提高对患者亚组的识别和分析能力，有助于发现临床上有意义的患者群体异质性。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHR)包含有价值的纵向患者级信息，但大多数统计方法将EHR代码的不规则时间简化为简单计数，从而丢弃了丰富的时间结构。现有的时间模型通常施加限制性参数假设，或者针对代码级而非患者级任务定制。我们提出潜在因子点过程模型，该模型将代码出现表示为高维点过程，其条件强度由低维潜在泊松过程驱动。这种低秩结构反映了临床现实：数千种代码由少量潜在疾病过程控制，同时能够在高维中实现统计有效估计。基于此模型，我们引入了Fourier-Eigen嵌入，这是一种从观察过程的光谱密度矩阵构建的患者表示方法。我们建立了理论保证，表明这些嵌入能够有效捕获下游分类和聚类中特定亚组的时间模式。模拟和应用于阿尔茨海默病EHR队列的研究证明了该方法在揭示临床意义异质性方面的实际优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic health records (EHR) contain valuable longitudinal patient-levelinformation, yet most statistical methods reduce the irregular timing of EHRcodes into simple counts, thereby discarding rich temporal structure. Existingtemporal models often impose restrictive parametric assumptions or are tailoredto code level rather than patient-level tasks. We propose the latent factorpoint process model, which represents code occurrences as a high-dimensionalpoint process whose conditional intensity is driven by a low dimensional latentPoisson process. This low-rank structure reflects the clinical reality thatthousands of codes are governed by a small number of underlying diseaseprocesses, while enabling statistically efficient estimation in highdimensions. Building on this model, we introduce the Fourier-Eigen embedding, apatient representation constructed from the spectral density matrix of theobserved process. We establish theoretical guarantees showing that theseembeddings efficiently capture subgroup-specific temporal patterns fordownstream classification and clustering. Simulations and an application to anAlzheimer's disease EHR cohort demonstrate the practical advantages of ourapproach in uncovering clinically meaningful heterogeneity.</description>
      <author>example@mail.com (Parker Knight, Doudou Zhou, Zongqi Xia, Tianxi Cai, Junwei Lu)</author>
      <guid isPermaLink="false">2508.20327v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title>
      <link>http://arxiv.org/abs/2508.19650v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队提出了Video-LevelGauge基准，专门用于系统评估大型视频语言模型(LVLMs)中的位置偏差问题。通过标准化探测器和定制化上下文设置，该基准能够模拟多样化现实场景，并采用统计测量与形态模式识别相结合的分析方法。研究评估了27个最先进LVLMs，发现开源模型普遍存在位置偏差，而商业模型如Gemini2.5-Pro表现更一致。&lt;h4&gt;背景&lt;/h4&gt;大型视频语言模型(LVLMs)在视频理解方面取得了显著进展，相应的评估基准也在不断发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了上下文位置偏差这一关键但未被充分探索的方面。&lt;h4&gt;目的&lt;/h4&gt;设计一个名为Video-LevelGauge的专门基准，用于系统评估LVLMs中的位置偏差问题，并通过灵活控制上下文长度、探测位置和上下文类型来模拟多样化的现实场景。&lt;h4&gt;方法&lt;/h4&gt;采用标准化探测器和定制化上下文设置，引入结合统计测量和形态模式识别的综合分析方法。基准包含438个手动策划的视频，产生1,177个高质量多项选择题和120个开放性问题。基于这些内容，评估了27个最先进的LVLMs，包括商业和开源模型。&lt;h4&gt;主要发现&lt;/h4&gt;许多领先的开源模型存在显著的位置偏差，通常表现出头部或邻居内容偏好。相比之下，商业模型如Gemini2.5-Pro在整个视频序列上表现出令人印象深刻的一致性能。对上下文长度、上下文变化和模型规模的进一步分析提供了减轻偏差和指导模型增强的可行见解。&lt;h4&gt;结论&lt;/h4&gt;Video-LevelGauge基准能够有效暴露位置偏差，评估结果揭示了不同类型模型在位置偏差方面的表现差异，为减轻偏差和指导模型改进提供了实用见解。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型(LVLMs)在视频理解方面取得了显著进展，推动了相应评估基准的发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了上下文位置偏差这一关键但未被充分探索的LVLM性能方面。我们提出了Video-LevelGauge，一个专门设计的基准，用于系统评估LVLMs中的位置偏差。我们采用标准化探测器和定制化上下文设置，允许灵活控制上下文长度、探测位置和上下文类型，以模拟多样化的现实场景。此外，我们引入了一种结合统计测量与形态模式识别的综合分析方法来表征偏差。我们的基准包含438个手动策划的视频，涵盖多种类型，产生了1,177个高质量多项选择题和120个开放性问题，其有效性已得到验证，能够有效暴露位置偏差。基于这些内容，我们评估了27个最先进的LVLMs，包括商业和开源模型。我们的发现显示，许多领先的开源模型存在显著的位置偏差，通常表现出头部或邻居内容偏好。相比之下，商业模型如Gemini2.5-Pro在整个视频序列上表现出令人印象深刻的一致性能。对上下文长度、上下文变化和模型规模的进一步分析提供了减轻偏差和指导模型增强的可行见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding modelenhancement.https://github.com/Cola-any/Video-LevelGauge</description>
      <author>example@mail.com (Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang)</author>
      <guid isPermaLink="false">2508.19650v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
      <link>http://arxiv.org/abs/2508.16201v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 Main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SpecVLM，一个针对视频大语言模型的无训练推测解码框架，通过分阶段视频标记剪枝技术，在不牺牲准确性的情况下将解码速度提高最多2.68倍。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型(Vid-LLMs)在理解视频内容方面展现出强大能力，但它们对密集视频标记表示的依赖在预填充和解码阶段引入了大量内存和计算开销。&lt;h4&gt;目的&lt;/h4&gt;为减轻最近视频标记减少方法的信息损失，并无损加速Vid-LLMs的解码阶段。&lt;h4&gt;方法&lt;/h4&gt;SpecVLM是一个专为Vid-LLMs设计的无训练推测解码框架，结合分阶段视频标记剪枝。基于草稿模型推测对视频标记剪枝具有低敏感性的发现，可剪枝高达90%的视频标记。通过两阶段剪枝实现：第一阶段根据验证器注意力信号选择高信息量标记，第二阶段以空间均匀方式剪枝剩余冗余标记。&lt;h4&gt;主要发现&lt;/h4&gt;草稿模型的推测对视频标记剪枝具有低敏感性，允许剪枝高达90%的视频标记而不牺牲准确性。&lt;h4&gt;结论&lt;/h4&gt;SpecVLM在四个视频理解基准测试上展示了有效性和鲁棒性，对于LLaVA-OneVision-72B实现高达2.68倍的解码加速，对于Qwen2.5-VL-32B实现2.11倍的加速。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(Vid-LLMs)在理解视频内容方面展现出强大的能力。然而，它们对密集视频标记表示的依赖在预填充和解码阶段都引入了大量的内存和计算开销。为了减轻最近视频标记减少方法的信息损失并无损地加速Vid-LLMs的解码阶段，我们引入了SpecVLM，这是一个专为Vid-LLMs设计的无训练推测解码框架，结合了分阶段视频标记剪枝。基于我们的新发现，即草稿模型的推测对视频标记剪枝具有低敏感性，SpecVLM可以剪枝高达90%的视频标记，实现高效推测而不牺牲准确性。为此，我们执行了一个两阶段剪枝过程：第一阶段根据验证器的注意力信号选择信息量高的标记，第二阶段以空间均匀方式剪枝剩余的冗余标记。在四个视频理解基准测试上的大量实验证明了SpecVLM的有效性和鲁棒性，对于LLaVA-OneVision-72B实现了高达2.68倍的解码加速，对于Qwen2.5-VL-32B实现了2.11倍的加速。代码可在https://github.com/zju-jiyicheng/SpecVLM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video large language models (Vid-LLMs) have shown strong capabilities inunderstanding video content. However, their reliance on dense video tokenrepresentations introduces substantial memory and computational overhead inboth prefilling and decoding. To mitigate the information loss of recent videotoken reduction methods and accelerate the decoding stage of Vid-LLMslosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)framework tailored for Vid-LLMs that incorporates staged video token pruning.Building on our novel finding that the draft model's speculation exhibits lowsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens toenable efficient speculation without sacrificing accuracy. To achieve this, weperforms a two-stage pruning process: Stage I selects highly informative tokensguided by attention signals from the verifier (target model), while Stage IIprunes remaining redundant ones in a spatially uniform manner. Extensiveexperiments on four video understanding benchmarks demonstrate theeffectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup forQwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM.</description>
      <author>example@mail.com (Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li)</author>
      <guid isPermaLink="false">2508.16201v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Learning Robust Spatial Representations from Binaural Audio through Feature Distillation</title>
      <link>http://arxiv.org/abs/2508.20914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in Proc. WASPAA 2025, October 12-15, 2025, Tahoe, US.  Copyright (c) 2025 IEEE. 5 pages, 2 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究基于特征蒸馏的预训练方法，学习双耳语音的空间表示，无需标签数据，并在到达方向估计任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;深度表示学习在多个音频任务中表现出强大的性能，但在用于从多通道音频学习空间表示方面的研究还不足。&lt;h4&gt;目的&lt;/h4&gt;研究一种基于特征蒸馏的预训练阶段，学习稳健的双耳语音空间表示，无需数据标签。&lt;h4&gt;方法&lt;/h4&gt;从干净的双耳语音样本计算空间特征形成预测标签，使用神经网络从增强语音预测这些特征；预训练后，使用学习到的编码器权重初始化DoA估计模型并进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;与全监督模型和经典信号处理方法相比，预训练模型在嘈杂和混响环境中对到达方向估计表现出改进的性能。&lt;h4&gt;结论&lt;/h4&gt;基于特征蒸馏的预训练方法可以有效地学习空间表示，并在到达方向估计任务中表现更好。&lt;h4&gt;翻译&lt;/h4&gt;最近，深度表示学习在多个音频任务中表现出强大的性能。然而，其在用于从多通道音频学习空间表示方面的应用研究还不够充分。我们研究了一种基于特征蒸馏的预训练阶段，用于学习稳健的双耳语音空间表示，无需数据标签。在此框架中，从干净的双耳语音样本计算空间特征以形成预测标签。然后使用神经网络从相应的增强语音预测这些干净特征。预训练后，我们丢弃空间特征预测器，使用学习到的编码器权重初始化一个DoA估计模型，并对其进行微调以进行到达方向估计。我们的实验表明，与全监督模型和经典信号处理方法相比，经过微调的预训练模型在嘈杂和混响环境中进行到达方向估计时表现出改进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, deep representation learning has shown strong performance inmultiple audio tasks. However, its use for learning spatial representationsfrom multichannel audio is underexplored. We investigate the use of apretraining stage based on feature distillation to learn a robust spatialrepresentation of binaural speech without the need for data labels. In thisframework, spatial features are computed from clean binaural speech samples toform prediction labels. These clean features are then predicted fromcorresponding augmented speech using a neural network. After pretraining, wethrow away the spatial feature predictor and use the learned encoder weights toinitialize a DoA estimation model which we fine-tune for DoA estimation. Ourexperiments demonstrate that the pretrained models show improved performance innoisy and reverberant environments after fine-tuning for direction-of-arrivalestimation, when compared to fully supervised models and classic signalprocessing methods.</description>
      <author>example@mail.com (Holger Severin Bovbjerg, Jan Østergaard, Jesper Jensen, Shinji Watanabe, Zheng-Hua Tan)</author>
      <guid isPermaLink="false">2508.20914v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>EEGDM: Learning EEG Representation with Latent Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.20705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出EEGDM，一种基于潜在扩散模型的新型自监督EEG表示学习方法，利用EEG信号生成作为自监督目标，将扩散模型转变为能够捕捉EEG语义的强表示学习器。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习进行脑电图信号分析虽有潜力，但现有方法在跨不同任务学习可泛化表示方面面临挑战，尤其在训练数据有限时。当前EEG表示学习方法如EEGPT和LaBraM通常依赖简单掩码重构目标，可能无法完全捕捉EEG信号中的丰富语义信息和复杂模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉EEG语义信息并适用于多种下游任务的表示学习方法，解决现有方法在泛化性和数据效率方面的问题。&lt;h4&gt;方法&lt;/h4&gt;EEGDM包含一个EEG编码器，将EEG信号及其通道增强蒸馏为紧凑表示，作为条件信息指导扩散模型生成EEG信号。这种设计赋予EEGDM紧凑的潜在空间，既提供生成过程控制，又可用于下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;EEGDM能够(1)重建高质量EEG信号，(2)有效学习鲁棒表示，(3)在各类下游任务中仅需少量预训练数据即可实现竞争性性能。&lt;h4&gt;结论&lt;/h4&gt;EEGDM具有良好的泛化性和实用性，为EEG信号分析提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;尽管使用深度学习进行脑电图信号分析已显示出巨大潜力，但现有方法在跨不同任务学习可泛化表示方面仍面临重大挑战，尤其是在训练数据有限的情况下。当前的EEG表示学习方法包括EEGPT和LaBraM通常依赖于简单的掩码重构目标，这可能无法完全捕捉EEG信号中固有的丰富语义信息和复杂模式。在本文中，我们提出EEGDM，一种基于潜在扩散模型的新型自监督EEG表示学习方法，它利用EEG信号生成作为自监督目标，将扩散模型转变为能够捕捉EEG语义的强表示学习器。EEGDM包含一个EEG编码器，将EEG信号及其通道增强蒸馏为紧凑表示，作为条件信息指导扩散模型生成EEG信号。这种设计赋予EEGDM紧凑的潜在空间，不仅为生成过程提供充分控制，还可用于下游任务。实验结果表明，EEGDM(1)能够重建高质量EEG信号，(2)能有效学习鲁棒表示，(3)在各类下游任务中仅需适度的预训练数据量即可实现竞争性性能，凸显了其泛化性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While electroencephalography (EEG) signal analysis using deep learning hasshown great promise, existing approaches still face significant challenges inlearning generalizable representations that perform well across diverse tasks,particularly when training data is limited. Current EEG representation learningmethods including EEGPT and LaBraM typically rely on simple maskedreconstruction objective, which may not fully capture the rich semanticinformation and complex patterns inherent in EEG signals. In this paper, wepropose EEGDM, a novel self-supervised EEG representation learning method basedon the latent diffusion model, which leverages EEG signal generation as aself-supervised objective, turning the diffusion model into a strongrepresentation learner capable of capturing EEG semantics. EEGDM incorporatesan EEG encoder that distills EEG signals and their channel augmentations into acompact representation, acting as conditional information to guide thediffusion model for generating EEG signals. This design endows EEGDM with acompact latent space, which not only offers ample control over the generativeprocess but also can be leveraged for downstream tasks. Experimental resultsshow that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectivelylearns robust representations, and (3) achieves competitive performance withmodest pre-training data size across diverse downstream tasks, underscoring itsgeneralizability and practical utility.</description>
      <author>example@mail.com (Shaocong Wang, Tong Liu, Ming Li, Minjing Yu, Yong-Jin Liu)</author>
      <guid isPermaLink="false">2508.20705v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications</title>
      <link>http://arxiv.org/abs/2508.20622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Access. This is a preprint version. 14 pages, 6  figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究掩码自编码器(MAEs)与视觉变换器(ViT)架构在一维超声信号自监督表示学习中的适应性和性能&lt;h4&gt;背景&lt;/h4&gt;MAEs在计算机视觉等领域已取得显著成功，但在一维信号分析特别是原始超声数据中的应用尚未充分探索。超声信号在工业应用中至关重要，如无损检测和结构健康监测，但标记数据稀缺且信号处理高度特定于任务&lt;h4&gt;目的&lt;/h4&gt;提出利用MAE在未标记合成超声信号上预训练的方法，使模型学习鲁棒表示，增强下游任务(如飞行时间分类)的性能&lt;h4&gt;方法&lt;/h4&gt;系统地研究模型大小、块大小和掩码比率对预训练效率和下游准确性的影响&lt;h4&gt;主要发现&lt;/h4&gt;预训练模型显著优于从头训练的模型和针对下游任务优化的卷积神经网络基线；在合成数据上预训练比仅使用有限真实数据集训练展现出更好的可迁移性到真实测量信号&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了MAEs通过可扩展的自监督学习推进超声信号分析的潜力&lt;h4&gt;翻译&lt;/h4&gt;我们研究了掩码自编码器(MAEs)与视觉变换器(ViT)架构在一维(1D)超声信号自监督表示学习中的适应性和性能。尽管MAEs在计算机视觉和其他领域已显示出显著成功，但其在1D信号分析，特别是原始超声数据中的应用 largely unexplored。超声信号在工业应用中至关重要，如无损检测(NDT)和结构健康监测(SHM)，其中标记数据通常稀缺，且信号处理高度特定于任务。我们提出一种利用MAE在未标记的合成超声信号上预训练的方法，使模型能够学习鲁棒表示，增强下游任务(如飞行时间ToF分类)的性能。本研究系统地研究了模型大小、块大小和掩码比率对预训练效率和下游准确性的影响。我们的结果表明，预训练模型显著优于从头训练的模型和针对下游任务优化的强卷积神经网络(CNN)基线。此外，在合成数据上预训练比仅在有有限真实数据集上训练展现出更好的可迁移性到真实测量信号。这项研究强调了MAEs通过可扩展的自监督学习推进超声信号分析的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigated the adaptation and performance of Masked Autoencoders (MAEs)with Vision Transformer (ViT) architectures for self-supervised representationlearning on one-dimensional (1D) ultrasound signals. Although MAEs havedemonstrated significant success in computer vision and other domains, theiruse for 1D signal analysis, especially for raw ultrasound data, remains largelyunexplored. Ultrasound signals are vital in industrial applications such asnon-destructive testing (NDT) and structural health monitoring (SHM), wherelabeled data are often scarce and signal processing is highly task-specific. Wepropose an approach that leverages MAE to pre-train on unlabeled syntheticultrasound signals, enabling the model to learn robust representations thatenhance performance in downstream tasks, such as time-of-flight (ToF)classification. This study systematically investigated the impact of modelsize, patch size, and masking ratio on pre-training efficiency and downstreamaccuracy. Our results show that pre-trained models significantly outperformmodels trained from scratch and strong convolutional neural network (CNN)baselines optimized for the downstream task. Additionally, pre-training onsynthetic data demonstrates superior transferability to real-world measuredsignals compared with training solely on limited real datasets. This studyunderscores the potential of MAEs for advancing ultrasound signal analysisthrough scalable, self-supervised learning.</description>
      <author>example@mail.com (Immanuel Roßteutscher, Klaus S. Drese, Thorsten Uphues)</author>
      <guid isPermaLink="false">2508.20622v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Human-AI Collaborative Bot Detection in MMORPGs</title>
      <link>http://arxiv.org/abs/2508.20578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于对比表示学习和聚类技术的无监督框架，用于检测MMORPGs中的自动升级机器人，并通过大语言模型和可视化方法提高检测效率和可解释性。&lt;h4&gt;背景&lt;/h4&gt;在大型多人在线角色扮演游戏(MMORPGs)中，自动升级机器人利用自动化程序大规模提升角色等级，破坏游戏平衡和公平性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测自动升级机器人的方法，同时提供可解释的检测结果以避免法律和用户体验问题。&lt;h4&gt;方法&lt;/h4&gt;利用对比表示学习和聚类技术，在完全无监督的情况下识别具有相似升级模式的角色组；引入大语言模型作为辅助审查者验证聚类结果；开发基于增长曲线的可视化方法辅助评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合无监督学习、大语言模型辅助审查和可视化方法，可以提高机器人检测的效率和准确性，同时保持决策的可解释性。&lt;h4&gt;结论&lt;/h4&gt;这种协作方法改进了机器人检测工作流程的效率，同时保持可解释性，支持MMORPGs中可扩展且负责任的机器人监管。&lt;h4&gt;翻译&lt;/h4&gt;在大型多人在线角色扮演游戏(MMORPGs)中，自动升级机器人利用自动化程序大规模提升角色等级，破坏游戏平衡和公平性。检测此类机器人具有挑战性，不仅因为它们模仿人类行为，还因为惩罚措施需要可解释的理由以避免法律和用户体验问题。在本文中，我们提出了一种新颖的框架，通过利用对比表示学习和聚类技术，以完全无监督的方式识别具有相似升级模式的角色组。为确保可靠决策，我们引入大语言模型(LLM)作为辅助审查者来验证聚类组，有效模拟二次人工判断。我们还引入了基于增长曲线的可视化方法，帮助LLM和人类评估员评估升级行为。这种协作方法提高了机器人检测工作流程的效率，同时保持可解释性，从而支持MMORPGs中可扩展且负责任的机器人监管。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-levelingbots exploit automated programs to level up characters at scale, undermininggameplay balance and fairness. Detecting such bots is challenging, not onlybecause they mimic human behavior, but also because punitive actions requireexplainable justification to avoid legal and user experience issues. In thispaper, we present a novel framework for detecting auto-leveling bots byleveraging contrastive representation learning and clustering techniques in afully unsupervised manner to identify groups of characters with similarlevel-up patterns. To ensure reliable decisions, we incorporate a LargeLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,effectively mimicking a secondary human judgment. We also introduce a growthcurve-based visualization to assist both the LLM and human moderators inassessing leveling behavior. This collaborative approach improves theefficiency of bot detection workflows while maintaining explainability, therebysupporting scalable and accountable bot regulation in MMORPGs.</description>
      <author>example@mail.com (Jaeman Son, Hyunsoo Kim)</author>
      <guid isPermaLink="false">2508.20578v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems</title>
      <link>http://arxiv.org/abs/2508.20508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于多智能体协同进化机制的智能服务优化方法，用于解决大规模微服务架构中的治理挑战。该方法通过将服务建模为智能体，利用图表示学习构建依赖关系图，采用集中式训练和分布式执行框架，并通过游戏驱动的策略优化机制实现服务间的自适应协作和行为进化。&lt;h4&gt;背景&lt;/h4&gt;大规模微服务架构面临治理挑战，包括复杂的服务依赖关系、动态的拓扑结构以及波动的工作负载。&lt;h4&gt;目的&lt;/h4&gt;提出一种智能服务优化方法，以解决大规模微服务架构中的治理挑战，提高系统性能和适应性。&lt;h4&gt;方法&lt;/h4&gt;将每个服务建模为智能体；引入图表示学习构建服务依赖关系图；基于马尔可夫决策过程学习每个智能体的策略；采用集中式训练和分布式执行的框架；设计游戏驱动的策略优化机制，通过选择-变异过程动态调整智能体策略分布。&lt;h4&gt;主要发现&lt;/h4&gt;在面对突发工作负载激增、拓扑重新配置或资源冲突等场景时，系统能够快速响应并实现稳定的策略收敛；在协调效率、适应性和策略收敛性能等关键指标上优于其他先进方法；显著提高了大规模微服务系统中的治理效率和运行稳定性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在大规模微服务系统中具有很高的实用价值和工程可行性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于多智能体协同进化机制的智能服务优化方法，以解决大规模微服务架构中的治理挑战。这些挑战包括复杂的服务依赖关系、动态的拓扑结构以及波动的工作负载。该方法将每个服务建模为一个智能体，并引入图表示学习来构建服务依赖关系图。这使得智能体能感知和嵌入系统内的结构变化。每个智能体基于马尔可夫决策过程学习其策略。采用集中式训练和分布式执行的框架，将局部自主性与全局协调相结合。为了增强整体系统性能和适应性，设计了一个游戏驱动的策略优化机制。通过选择-变异过程，动态调整智能体策略分布。这支持服务间的自适应协作和行为进化。在该机制下，系统在面对突发工作负载激增、拓扑重新配置或资源冲突等场景时，能够快速响应并实现稳定的策略收敛。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes an intelligent service optimization method based on amulti-agent collaborative evolution mechanism to address governance challengesin large-scale microservice architectures. These challenges include complexservice dependencies, dynamic topology structures, and fluctuating workloads.The method models each service as an agent and introduces graph representationlearning to construct a service dependency graph. This enables agents toperceive and embed structural changes within the system. Each agent learns itspolicy based on a Markov Decision Process. A centralized training anddecentralized execution framework is used to integrate local autonomy withglobal coordination. To enhance overall system performance and adaptability, agame-driven policy optimization mechanism is designed. Through aselection-mutation process, agent strategy distributions are dynamicallyadjusted. This supports adaptive collaboration and behavioral evolution amongservices. Under this mechanism, the system can quickly respond and achievestable policy convergence when facing scenarios such as sudden workload spikes,topology reconfigurations, or resource conflicts. To evaluate the effectivenessof the proposed method, experiments are conducted on a representativemicroservice simulation platform. Comparative analyses are performed againstseveral advanced approaches, focusing on coordination efficiency, adaptability,and policy convergence performance. Experimental results show that the proposedmethod outperforms others in several key metrics. It significantly improvesgovernance efficiency and operational stability in large-scale microservicesystems. The method demonstrates strong practical value and engineeringfeasibility.</description>
      <author>example@mail.com (Yilin Li, Song Han, Sibo Wang, Ming Wang, Renzi Meng)</author>
      <guid isPermaLink="false">2508.20508v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Prediction of Distant Metastasis for Head and Neck Cancer Patients Using Multi-Modal Tumor and Peritumoral Feature Fusion Network</title>
      <link>http://arxiv.org/abs/2508.20469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 4 figures, 5 tables. Zizhao Tang, Changhao Liu, and Nuo  Tong contributed equally. Corresponding Authors: Mei Shi  (mshi82@fmmu.edu.cn), Shuiping Gou (shpgou@mail.xidian.edu.cn)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一种结合CT图像、放射组学和临床数据的深度学习多模态框架，用于预测头颈鳞状细胞癌患者的转移风险。模型在1497名患者数据上进行了验证，融合模型显著优于单模态模型，为个性化治疗规划提供了有价值的临床决策支持工具。&lt;h4&gt;背景&lt;/h4&gt;转移是头颈鳞状细胞癌临床管理中的主要挑战。治疗前可靠预测转移风险对优化治疗策略和改善患者预后至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于深度学习的多模态框架，通过整合CT图像、放射组学和临床数据来预测HNSCC患者的转移风险。&lt;h4&gt;方法&lt;/h4&gt;研究纳入1497名HNSCC患者，从治疗前CT图像中获取肿瘤和器官掩膜，使用3D Swin Transformer提取肿瘤区域深度特征，通过PyRadiomics获取并筛选放射组学特征，编码临床变量并与成像特征融合，最后将多模态特征输入全连接网络进行转移风险预测，使用五折交叉验证评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;融合模型显著优于单模态模型，单独3D深度学习模块AUC为0.715，结合放射组学和临床特征后性能提升（AUC=0.803，ACC=0.752，SEN=0.730，SPE=0.758）；模型在不同肿瘤亚型中具有良好泛化能力；不同模态提供互补信息；3D Swin Transformer比传统网络提供更强大的表示学习能力。&lt;h4&gt;结论&lt;/h4&gt;该多模态融合模型在预测HNSCC转移风险方面表现出高准确性和鲁棒性，为肿瘤生物学提供全面表征，具有作为临床决策支持工具的潜力，可辅助个性化治疗规划。&lt;h4&gt;翻译&lt;/h4&gt;转移仍然是头颈鳞状细胞癌临床管理中的主要挑战。治疗前可靠预测转移风险对于优化治疗策略和预后至关重要。本研究开发了一种基于深度学习的多模态框架，通过整合计算机断层扫描图像、放射组学和临床数据来预测HNSCC患者的转移风险。研究纳入了1497名HNSCC患者。从治疗前CT图像中获取肿瘤和器官掩膜。3D Swin Transformer从肿瘤区域提取深度特征。同时，使用PyRadiomics获得1562个放射组学特征，随后进行相关过滤和随机森林选择，保留36个特征。包括年龄、性别、吸烟和饮酒状况的临床变量被编码并与成像衍生的特征融合。多模态特征被输入全连接网络以预测转移风险。使用曲线下面积、准确率、敏感性和特异性通过五折交叉验证评估性能。所提出的融合模型优于单模态模型。单独的3D深度学习模块实现了0.715的AUC，当与放射组学和临床特征结合时，预测性能得到改善（AUC=0.803，ACC=0.752，SEN=0.730，SPE=0.758）。分层分析显示在不同肿瘤亚型中具有泛化能力。消融研究表明不同模态提供互补信息。评估显示3D Swin Transformer比传统网络提供更强大的表示学习能力。这种多模态融合模型在预测HNSCC转移风险方面表现出高准确性和鲁棒性，为肿瘤生物学提供了全面表征。该可解释模型有潜力作为临床决策支持工具用于个性化治疗规划。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Metastasis remains the major challenge in the clinical management of head andneck squamous cell carcinoma (HNSCC). Reliable pre-treatment prediction ofmetastatic risk is crucial for optimizing treatment strategies and prognosis.This study develops a deep learning-based multimodal framework to predictmetastasis risk in HNSCC patients by integrating computed tomography (CT)images, radiomics, and clinical data. 1497 HNSCC patients were included. Tumorand organ masks were derived from pretreatment CT images. A 3D Swin Transformerextracted deep features from tumor regions. Meanwhile, 1562 radiomics featureswere obtained using PyRadiomics, followed by correlation filtering and randomforest selection, leaving 36 features. Clinical variables including age, sex,smoking, and alcohol status were encoded and fused with imaging-derivedfeatures. Multimodal features were fed into a fully connected network topredict metastasis risk. Performance was evaluated using five-foldcross-validation with area under the curve (AUC), accuracy (ACC), sensitivity(SEN), and specificity (SPE). The proposed fusion model outperformedsingle-modality models. The 3D deep learning module alone achieved an AUC of0.715, and when combined with radiomics and clinical features, predictiveperformance improved (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758).Stratified analysis showed generalizability across tumor subtypes. Ablationstudies indicated complementary information from different modalities.Evaluation showed the 3D Swin Transformer provided more robust representationlearning than conventional networks. This multimodal fusion model demonstratedhigh accuracy and robustness in predicting metastasis risk in HNSCC, offering acomprehensive representation of tumor biology. The interpretable model haspotential as a clinical decision-support tool for personalized treatmentplanning.</description>
      <author>example@mail.com (Zizhao Tang, Changhao Liu, Nuo Tong, Shuiping Gou, Mei Shi)</author>
      <guid isPermaLink="false">2508.20469v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>BiListing: Modality Alignment for Listings</title>
      <link>http://arxiv.org/abs/2508.20396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出BiListing方法，通过大型语言模型和预训练语言-图像模型对齐房源文本和图像，将非结构化数据转化为单一嵌入向量，实现零样本搜索、克服冷启动问题，并成功应用于Airbnb搜索系统，带来0.425%的NDCG提升和数千万美元增量收入。&lt;h4&gt;背景&lt;/h4&gt;Airbnb作为住宿平台传统上依赖结构化数据，而表示学习兴起使得利用文本和图像信息变得可行。然而，房源包含多样的非结构化数据（多图像、多文本文档），将不同嵌入组合为单一表示具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出BiListing方法解决非结构化数据处理问题，实现更有效的房源搜索和推荐。&lt;h4&gt;方法&lt;/h4&gt;BiListing（双模态房源）利用大型语言模型和预训练语言-图像模型对齐房源文本和图像，创建每个房源和模态的单一嵌入向量。&lt;h4&gt;主要发现&lt;/h4&gt;BiListing实现了零样本搜索能力，克服冷启动问题，支持单模态或双模态房源间搜索；在搜索排名模型中应用带来0.425%的NDCG提升和数千万美元增量收入。&lt;h4&gt;结论&lt;/h4&gt;BiListing方法成功解决了Airbnb平台上非结构化数据处理挑战，通过结合先进AI技术实现了更有效的搜索和推荐，创造了显著商业价值。&lt;h4&gt;翻译&lt;/h4&gt;Airbnb是提供旅行住宿的领导者。传统上，Airbnb依赖结构化数据来理解、排名和向客人推荐房源，这是由于从文本和图像中提取有意义信息的能力有限且复杂。随着表示学习的兴起，利用文本和照片中的丰富信息变得更加容易。然而，一个Airbnb房源包含多样化的非结构化数据：多个图像和各种非结构化文本文档，如标题、描述和评论，这使得这种方法具有挑战性。本文提出了BiListing，一种利用大型语言模型和预训练的语言-图像模型来对齐房源文本和图像的方法。我们进行了离线和在线测试，在Airbnb搜索排名模型中使用BiListing嵌入，并成功将其部署到生产环境中，实现了0.425%的NDCG提升，并带来了数千万美元的增量收入。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761577&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Airbnb is a leader in offering travel accommodations. Airbnb has historicallyrelied on structured data to understand, rank, and recommend listings to guestsdue to the limited capabilities and associated complexity arising fromextracting meaningful information from text and images. With the rise ofrepresentation learning, leveraging rich information from text and photos hasbecome easier. A popular approach has been to create embeddings for textdocuments and images to enable use cases of computing similarities betweenlistings or using embeddings as features in an ML model.  However, an Airbnb listing has diverse unstructured data: multiple images,various unstructured text documents such as title, description, and reviews,making this approach challenging. Specifically, it is a non-trivial task tocombine multiple embeddings of different pieces of information to reach asingle representation.  This paper proposes BiListing, for Bimodal Listing, an approach to align textand photos of a listing by leveraging large-language models and pretrainedlanguage-image models. The BiListing approach has several favorablecharacteristics: capturing unstructured data into a single embedding vector perlisting and modality, enabling zero-shot capability to search inventoryefficiently in user-friendly semantics, overcoming the cold start problem, andenabling listing-to-listing search along a single modality, or both.  We conducted offline and online tests to leverage the BiListing embeddings inthe Airbnb search ranking model, and successfully deployed it in production,achieved 0.425% of NDCB gain, and drove tens of millions in incrementalrevenue.</description>
      <author>example@mail.com (Guillaume Guy, Mihajlo Grbovic, Chun How Tan, Han Zhao)</author>
      <guid isPermaLink="false">2508.20396v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Latent Variable Modeling for Robust Causal Effect Estimation</title>
      <link>http://arxiv.org/abs/2508.20259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025. This is the full version including extended  appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将潜在变量模型整合到双重机器学习范式中的新框架，用于在存在隐藏因素的情况下实现稳健的因果效应估计。研究考虑了潜在变量只影响结果或同时影响处理和结果的两种场景，并通过实验验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;潜在变量模型为在观测数据中纳入和推断未观测因素提供了强大框架。在因果推断中，它们有助于解释影响处理或结果的隐藏因素，解决缺失或未测量协变量带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，将潜在变量建模整合到双重机器学习范式中，以在存在隐藏因素的情况下实现稳健的因果效应估计。&lt;h4&gt;方法&lt;/h4&gt;提出将潜在变量模型整合到双重机器学习（DML）范式的新框架，仅在DML的第二阶段纳入潜在变量，将表示学习与潜在推断分离。考虑了两种场景：潜在变量只影响结果，以及潜在变量可能同时影响处理和结果。&lt;h4&gt;主要发现&lt;/h4&gt;通过在合成和真实世界数据集上的大量实验，证明了所提出方法的稳健性和有效性。&lt;h4&gt;结论&lt;/h4&gt;将潜在变量模型整合到双重机器学习范式中，能够有效处理存在隐藏因素的因果效应估计问题，且方法具有稳健性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;潜在变量模型为在观测数据中纳入和推断未观测因素提供了强大的框架。在因果推断中，它们有助于解释影响处理或结果的隐藏因素，从而解决缺失或未测量协变量带来的挑战。本文提出了一种新的框架，将潜在变量建模整合到双重机器学习（DML）范式中，以在存在此类隐藏因素的情况下实现稳健的因果效应估计。我们考虑了两种场景：一种是潜在变量只影响结果，另一种是潜在变量可能同时影响处理和结果。为确保可处理性，我们仅在DML的第二阶段纳入潜在变量，将表示学习与潜在推断分离。我们通过在合成和真实世界数据集上的大量实验，证明了我们方法的稳健性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent variable models provide a powerful framework for incorporating andinferring unobserved factors in observational data. In causal inference, theyhelp account for hidden factors influencing treatment or outcome, therebyaddressing challenges posed by missing or unmeasured covariates. This paperproposes a new framework that integrates latent variable modeling into thedouble machine learning (DML) paradigm to enable robust causal effectestimation in the presence of such hidden factors. We consider two scenarios:one where a latent variable affects only the outcome, and another where it mayinfluence both treatment and outcome. To ensure tractability, we incorporatelatent variables only in the second stage of DML, separating representationlearning from latent inference. We demonstrate the robustness and effectivenessof our method through extensive experiments on both synthetic and real-worlddatasets.</description>
      <author>example@mail.com (Tetsuro Morimura, Tatsushi Oka, Yugo Suzuki, Daisuke Moriwaki)</author>
      <guid isPermaLink="false">2508.20259v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19884v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构多样性的无参数图神经网络框架SDGNN，通过统一的结构多样性消息传递机制，同时捕捉邻域结构异构性和特征语义稳定性，无需额外参数和复杂训练，在多种挑战条件下优于主流GNN方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在结构化数据建模任务中表现出色，但主流方法依赖大量可训练参数和固定聚合规则，难以适应强结构异构性和复杂特征分布的图数据，导致节点表示过平滑和语义退化。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN方法在结构异构图数据上的适应性问题，提出一种无需额外参数且能有效处理结构异质性的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;设计了SDGNN(结构多样性图神经网络)框架，受结构多样性理论启发，实现统一的结构多样性消息传递机制，从结构驱动和特征驱动两个视角进行互补建模，不引入额外可训练参数。&lt;h4&gt;主要发现&lt;/h4&gt;在八个公共基准数据集和PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战条件下始终优于主流GNN方法，验证了结构多样性作为图表示学习中核心信号的重要性。&lt;h4&gt;结论&lt;/h4&gt;SDGNN为无参数图神经网络设计提供了新理论视角和通用方法，证明了结构多样性在图表示学习中的核心价值，并已公开完整实现代码。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在结构化数据建模任务(如节点分类)中表现出色。然而，主流方法通常依赖于大量可训练参数和固定的聚合规则，使得难以适应具有强结构异构性和复杂特征分布的图数据。这通常导致节点表示的过平滑和语义退化。为解决这些问题，本文提出了一种基于结构多样性的无参数图神经网络框架，即SDGNN(结构多样性图神经网络)。该框架受结构多样性理论启发，设计了统一的结构多样性消息传递机制，同时捕捉邻域结构的异构性和特征语义的稳定性，无需引入额外的可训练参数。与传统参数化方法不同，SDGNN不依赖复杂的模型训练，而是利用结构驱动和特征驱动视角的互补建模，从而有效提高数据集和场景间的适应性。实验结果表明，在八个公共基准数据集和一个跨学科的PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战条件下始终优于主流GNN。这项工作为无参数图神经网络的设计提供了新的理论视角和通用方法，并进一步验证了结构多样性作为图表示学习中核心信号的重要性。为便于复现和进一步研究，SDGNN的完整实现已在https://github.com/mingyue15694/SGDNN/tree/main发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable performance in structureddata modeling tasks such as node classification. However, mainstream approachesgenerally rely on a large number of trainable parameters and fixed aggregationrules, making it difficult to adapt to graph data with strong structuralheterogeneity and complex feature distributions. This often leads toover-smoothing of node representations and semantic degradation. To addressthese issues, this paper proposes a parameter-free graph neural networkframework based on structural diversity, namely SDGNN (Structural-DiversityGraph Neural Network). The framework is inspired by structural diversity theoryand designs a unified structural-diversity message passing mechanism thatsimultaneously captures the heterogeneity of neighborhood structures and thestability of feature semantics, without introducing additional trainableparameters. Unlike traditional parameterized methods, SDGNN does not rely oncomplex model training, but instead leverages complementary modeling from bothstructure-driven and feature-driven perspectives, thereby effectively improvingadaptability across datasets and scenarios. Experimental results show that oneight public benchmark datasets and an interdisciplinary PubMed citationnetwork, SDGNN consistently outperforms mainstream GNNs under challengingconditions such as low supervision, class imbalance, and cross-domain transfer.This work provides a new theoretical perspective and general approach for thedesign of parameter-free graph neural networks, and further validates theimportance of structural diversity as a core signal in graph representationlearning. To facilitate reproducibility and further research, the fullimplementation of SDGNN has been released at:https://github.com/mingyue15694/SGDNN/tree/main</description>
      <author>example@mail.com (Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu)</author>
      <guid isPermaLink="false">2508.19884v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting</title>
      <link>http://arxiv.org/abs/2508.20812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种不确定性感知预测控制屏障函数(UA-PCBFs)框架，用于解决人机协作机器人中的安全与灵活性平衡问题，通过将概率性人类运动预测与控制屏障函数的安全保证相结合，显著提升了人机交互的流畅性和安全性。&lt;h4&gt;背景&lt;/h4&gt;在人机共享工作空间中，协作机器人需要在保证严格安全的同时实现灵活高效的行为。人类运动的随机性和任务依赖性构成了动态障碍，而现有方法通常依赖纯反应式或最坏情况预测，导致不必要的制动和任务停滞。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理预测不确定性的框架，避免过度保守的规划算法，从而提高协作机器人的灵活性和响应能力，同时保持安全保证。&lt;h4&gt;方法&lt;/h4&gt;引入不确定性感知预测控制屏障函数(UA-PCBFs)框架，该框架融合了概率性人类手部运动预测与控制屏障函数的正式安全保证，利用预测模块提供的不确定性估计动态调整安全边界。&lt;h4&gt;主要发现&lt;/h4&gt;UA-PCBFs通过不确定性估计使协作机器人能更深入理解未来人类状态，实现更流畅智能的交互；在真实世界实验中，相比最先进的人机交互架构，在任务关键指标上表现更好，显著减少了机器人安全空间违规次数。&lt;h4&gt;结论&lt;/h4&gt;UA-PCBFs框架有效平衡了安全保证与灵活响应的需求，通过自动化设置和直接人机交互实验验证了其有效性，在安全性和灵活性方面相比现有方法有显著提升。&lt;h4&gt;翻译&lt;/h4&gt;为了在人与机器人共享工作空间的环境中实现灵活、高效的自动化，协作机器人单元必须将严格的安全保证与响应式有效行为的需求相协调。动态障碍物是人类运动的随机性和任务依赖性变化：当机器人仅依赖纯反应式或最坏情况包络时，它们会不必要的制动，阻碍任务进展，并损害真正人机交互所需的流畅性。近年来，基于学习的人类运动预测迅速发展，但大多数方法产生最坏情况预测，通常没有很好地结构化处理预测不确定性，导致过度保守的规划算法，限制了其灵活性。我们引入了不确定性感知预测控制屏障函数(UA-PCBFs)，这是一个统一框架，将概率性人类手部运动预测与控制屏障函数的正式安全保证相融合。与其他变体相比，由于预测模块提供的人类运动不确定性估计，我们的框架允许动态调整安全边界。得益于不确定性估计，UA-PCBFs使协作机器人能够更深入地理解未来人类状态，通过信息丰富的运动规划促进更流畅、智能的交互。我们通过逐渐增加现实感的全面真实世界实验验证了UA-PCBFs，包括使用机械手的自动化设置（执行完全可重复的运动）和直接人机交互（验证及时性、可用性和人类信心）。与最先进的人机交互架构相比，UA-PCBFs在任务关键指标上表现出更好的性能，显著减少了交互过程中机器人安全空间违规次数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To enable flexible, high-throughput automation in settings where people androbots share workspaces, collaborative robotic cells must reconcile stringentsafety guarantees with the need for responsive and effective behavior. Adynamic obstacle is the stochastic, task-dependent variability of human motion:when robots fall back on purely reactive or worst-case envelopes, they brakeunnecessarily, stall task progress, and tamper with the fluidity that trueHuman-Robot Interaction demands. In recent years, learning-based human-motionprediction has rapidly advanced, although most approaches produce worst-casescenario forecasts that often do not treat prediction uncertainty in awell-structured way, resulting in over-conservative planning algorithms,limiting their flexibility. We introduce Uncertainty-Aware Predictive ControlBarrier Functions (UA-PCBFs), a unified framework that fuses probabilistichuman hand motion forecasting with the formal safety guarantees of ControlBarrier Functions. In contrast to other variants, our framework allows fordynamic adjustment of the safety margin thanks to the human motion uncertaintyestimation provided by a forecasting module. Thanks to uncertainty estimation,UA-PCBFs empower collaborative robots with a deeper understanding of futurehuman states, facilitating more fluid and intelligent interactions throughinformed motion planning. We validate UA-PCBFs through comprehensive real-worldexperiments with an increasing level of realism, including automated setups (toperform exactly repeatable motions) with a robotic hand and direct human-robotinteractions (to validate promptness, usability, and human confidence).Relative to state-of-the-art HRI architectures, UA-PCBFs show betterperformance in task-critical metrics, significantly reducing the number ofviolations of the robot's safe space during interaction with respect to thestate-of-the-art.</description>
      <author>example@mail.com (Lorenzo Busellato, Federico Cunico, Diego Dall'Alba, Marco Emporio, Andrea Giachetti, Riccardo Muradore, Marco Cristani)</author>
      <guid isPermaLink="false">2508.20812v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network</title>
      <link>http://arxiv.org/abs/2508.20734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CardioMorphNet是一种循环贝叶斯深度学习框架，用于3D心脏形状引导的可变形配准，通过关注心脏解剖区域而非仅依赖图像强度，提高了心脏运动估计的准确性，并能提供不确定性估计。&lt;h4&gt;背景&lt;/h4&gt;从心脏磁共振图像准确估计心脏运动对于评估心脏功能和检测异常至关重要。现有方法往往难以准确捕捉心脏运动，因为它们依赖于基于强度的图像配准相似性损失，这可能忽略心脏解剖区域。&lt;h4&gt;目的&lt;/h4&gt;提出CardioMorphNet，一个用于3D心脏形状引导可变形配准的循环贝叶斯深度学习框架，使用短轴心脏磁共振图像。&lt;h4&gt;方法&lt;/h4&gt;CardioMorphNet采用循环变分自编码器来建模心脏周期中的时空依赖性，并有两个后验模型用于双心室分割和运动估计。从贝叶斯公式导出的损失函数指导框架通过递归配准分割图来关注解剖区域，而不使用基于强度的图像配准相似性损失，同时利用短轴体积序列和时空特征。贝叶斯建模还能计算估计运动场的不确定性图。&lt;h4&gt;主要发现&lt;/h4&gt;在英国生物银行数据集上通过比较变形后的掩模形状与真实掩模进行验证，CardioMorphNet在心脏运动估计方面表现出优于最先进方法的性能。不确定性评估表明，与其他基于概率的心脏配准方法相比，它在心脏区域的估计运动场产生较低的不确定性值，表明其预测具有更高的置信度。&lt;h4&gt;结论&lt;/h4&gt;CardioMorphNet是一种有效的心脏运动估计方法，通过关注解剖区域而非仅依赖图像强度，提高了准确性，并能提供不确定性估计。&lt;h4&gt;翻译&lt;/h4&gt;从心脏磁共振图像准确估计心脏运动对于评估心脏功能和检测其异常至关重要。现有方法往往难以准确捕捉心脏运动，因为它们依赖于基于强度的图像配准相似性损失，这可能忽略心脏解剖区域。为此，我们提出了CardioMorphNet，一个用于3D心脏形状引导可变形配准的循环贝叶斯深度学习框架，使用短轴心脏磁共振图像。它采用循环变分自编码器来建模心脏周期中的时空依赖性，以及两个用于双心室分割和运动估计的后验模型。从贝叶斯公式导出的损失函数指导框架通过递归配准分割图来关注解剖区域，而不使用基于强度的图像配准相似性损失，同时利用短轴体积序列和时空特征。贝叶斯建模还能计算估计运动场的不确定性图。在英国生物银行数据集上通过比较变形后的掩模形状与真实掩模进行验证，CardioMorphNet在心脏运动估计方面表现出优于最先进方法的性能。不确定性评估表明，与其他基于概率的心脏配准方法相比，它在心脏区域的估计运动场产生较低的不确定性值，表明其预测具有更高的置信度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)images is vital for assessing cardiac function and detecting its abnormalities.Existing methods often struggle to capture heart motion accurately because theyrely on intensity-based image registration similarity losses that may overlookcardiac anatomical regions. To address this, we propose CardioMorphNet, arecurrent Bayesian deep learning framework for 3D cardiac shape-guideddeformable registration using short-axis (SAX) CMR images. It employs arecurrent variational autoencoder to model spatio-temporal dependencies overthe cardiac cycle and two posterior models for bi-ventricular segmentation andmotion estimation. The derived loss function from the Bayesian formulationguides the framework to focus on anatomical regions by recursively registeringsegmentation maps without using intensity-based image registration similarityloss, while leveraging sequential SAX volumes and spatio-temporal features. TheBayesian modelling also enables computation of uncertainty maps for theestimated motion fields. Validated on the UK Biobank dataset by comparingwarped mask shapes with ground truth masks, CardioMorphNet demonstratessuperior performance in cardiac motion estimation, outperformingstate-of-the-art methods. Uncertainty assessment shows that it also yieldslower uncertainty values for estimated motion fields in the cardiac regioncompared with other probabilistic-based cardiac registration methods,indicating higher confidence in its predictions.</description>
      <author>example@mail.com (Reza Akbari Movahed, Abuzar Rezaee, Arezoo Zakeri, Colin Berry, Edmond S. L. Ho, Ali Gooya)</author>
      <guid isPermaLink="false">2508.20734v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets</title>
      <link>http://arxiv.org/abs/2508.20986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ReCoGNN是一个端到端的自动化特征增强框架，通过从多个关系表中提取特征来增强数据集，支持预测任务，在分类和回归任务上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;数据已成为金融、医疗和电子商务等领域创新的基础资产。在这些领域，预测建模被广泛使用，且越来越强调通过自动化机器学习技术减少人工努力。&lt;h4&gt;目的&lt;/h4&gt;探索特征增强是否可以自动化，并识别和利用任务相关的关系信号，以提高预测建模的性能。&lt;h4&gt;方法&lt;/h4&gt;ReCoGNN首先通过建模表内属性关系来捕获每个表内的语义依赖性，将表划分为结构化、语义连贯的段；然后构建一个表示所有段之间行间关系的异构加权图；最后利用消息传递图神经网络通过图传播信息，指导特征选择并增强原始数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在十个真实和合成数据集上进行的广泛实验表明，ReCoGNN在分类和回归任务上始终优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;ReCoGNN成功实现了自动化特征增强，能够有效识别和利用任务相关的关系信号，提高预测建模的性能。&lt;h4&gt;翻译&lt;/h4&gt;数据已成为驱动金融、医疗和电子商务等领域创新的基础资产。在这些领域中，基于关系表的预测建模被广泛使用，并且越来越强调通过自动化机器学习技术减少人工努力。这引发了一个有趣的问题：特征增强本身是否可以自动化，并识别和利用任务相关的关系信号？为了应对这一挑战，我们提出了一个端到端的自动化特征增强框架ReCoGNN，它利用从多个关系表中提取的特征来增强初始数据集，以支持预测任务。ReCoGNN首先通过建模表内属性关系来捕获每个表内的语义依赖性，从而将表划分为结构化、语义连贯的段。然后，它构建一个表示所有段之间行间关系的异构加权图。最后，ReCoGNN利用消息传递图神经网络通过图传播信息，指导特征选择并增强原始数据集。在十个真实和合成数据集上进行的广泛实验表明，ReCoGNN在分类和回归任务上始终优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data has become a foundational asset driving innovation across domains suchas finance, healthcare, and e-commerce. In these areas, predictive modelingover relational tables is commonly employed, with increasing emphasis onreducing manual effort through automated machine learning (AutoML) techniques.This raises an interesting question: can feature augmentation itself beautomated and identify and utilize task-related relational signals?  To address this challenge, we propose an end-to-end automated featureaugmentation framework, ReCoGNN, which enhances initial datasets using featuresextracted from multiple relational tables to support predictive tasks. ReCoGNNfirst captures semantic dependencies within each table by modeling intra-tableattribute relationships, enabling it to partition tables into structured,semantically coherent segments. It then constructs a heterogeneous weightedgraph that represents inter-row relationships across all segments. Finally,ReCoGNN leverages message-passing graph neural networks to propagateinformation through the graph, guiding feature selection and augmenting theoriginal dataset. Extensive experiments conducted on ten real-life andsynthetic datasets demonstrate that ReCoGNN consistently outperforms existingmethods on both classification and regression tasks.</description>
      <author>example@mail.com (Lianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, Guoren Wang)</author>
      <guid isPermaLink="false">2508.20986v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Based Concurrency Bug Detection and Localization</title>
      <link>http://arxiv.org/abs/2508.20911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的并发错误检测和定位方法，通过构建专门的并发错误数据集，集成预训练模型与异构图神经网络，并引入并发感知代码属性图(CCPG)来表征并发语义。使用SubgraphX方法精确定位错误到具体代码行，相比最先进方法在准确率和精确率上提高10%，召回率提高26%。&lt;h4&gt;背景&lt;/h4&gt;并发错误由多线程或分布式系统中共享资源的不当同步引起，难以检测，影响软件可靠性和安全性。现有深度学习方法存在三个主要局限：缺乏大型专用并发错误数据集、缺乏并发语义充分表示、二分类结果无法提供精确的bug行等细粒度调试信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，用于有效的并发错误检测和定位，解决现有深度学习方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建专门的并发错误数据集；集成预训练模型与异构图神经网络；引入并发感知代码属性图(CCPG)表征并发语义；使用SubgraphX GNN可解释性方法精确定位并发错误到具体源代码行。&lt;h4&gt;主要发现&lt;/h4&gt;相比最先进方法，该方法在准确率和精确率上平均提高10%，召回率提高26%，在各种评估设置中均表现出色。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效检测并发错误并精确定位，通过专用数据集和并发语义表征解决了现有方法局限，提供了更细粒度的调试信息。&lt;h4&gt;翻译&lt;/h4&gt;并发错误是由多线程或分布式系统中共享资源的不当同步引起的， notoriously难以检测，因此损害软件的可靠性和安全性。现有的深度学习方法面临三个主要限制。首先，缺乏大型且多样化的并发错误专用数据集。其次，它们缺乏并发语义的充分表示。第三，二分类结果无法提供更细粒度的调试信息，如精确的错误行。为解决这些问题，我们提出了一种用于有效并发错误检测和定位的新方法。我们构建了一个专门的并发错误数据集，以促进模型训练和评估。然后，我们通过融入一种新的并发感知代码属性图(CCPG)来集成预训练模型和异构图神经网络(GNN)，该图简洁有效地表征了并发语义。为进一步促进调试，我们采用了SubgraphX，一种基于GNN的可解释性方法，它探索图结构以精确定位并发错误，将其映射到特定的源代码行。平均而言，在各种评估设置中，我们的方法相比最先进方法在准确率和精确率上提高了10%，在召回率上提高了26%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concurrency bugs, caused by improper synchronization of shared resources inmulti-threaded or distributed systems, are notoriously hard to detect and thuscompromise software reliability and security. The existing deep learningmethods face three main limitations. First, there is an absence of large anddedicated datasets of diverse concurrency bugs for them. Second, they lacksufficient representation of concurrency semantics. Third, binaryclassification results fail to provide finer-grained debug information such asprecise bug lines. To address these problems, we propose a novel method foreffective concurrency bug detection as well as localization. We construct adedicated concurrency bug dataset to facilitate model training and evaluation.We then integrate a pre-trained model with a heterogeneous graph neural network(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) thatconcisely and effectively characterizes concurrency semantics. To furtherfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,which explores the graphs to precisely localize concurrency bugs, mapping themto specific lines of source code. On average, our method demonstrates animprovement of 10\% in accuracy and precision and 26\% in recall compared tostate-of-the-art methods across diverse evaluation settings.</description>
      <author>example@mail.com (Zuocheng Feng, Kaiwen Zhang, Miaomiao Wang, Yiming Cheng, Yuandao Cai, Xiaofeng Li, Guanjun Liu)</author>
      <guid isPermaLink="false">2508.20911v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Deep learning for jet modification in the presence of the QGP background</title>
      <link>http://arxiv.org/abs/2508.20856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用深度学习方法预测喷流在夸克-胶子等离子体中的能量损失，比较了卷积神经网络和动态图卷积神经网络在有无背景条件下的性能表现。&lt;h4&gt;背景&lt;/h4&gt;在相对论重离子碰撞中，传统评估喷流与色禁闭QCD介质相互作用的方法是测量喷流观测量分布相对于质子-质子碰撞基线的修正。深度学习方法可以实现对每个喷流的评估，增强喷流作为核介质精密探针的用途。&lt;h4&gt;目的&lt;/h4&gt;预测喷流在通过夸克-胶子等离子体(QGP)介质时的逐喷流能量损失分数χ，并评估不同深度学习方法在真实实验条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;使用线性玻尔兹曼传输(LBT)模型预测能量损失，将介质修正的喷流嵌入热背景中并应用组分减法进行背景去除。研究两种网络架构：使用喷流图像的卷积神经网络(CNNs)和使用粒子云的动态图卷积神经网络(DGCNNs)。&lt;h4&gt;主要发现&lt;/h4&gt;CNNs对无背景喷流能实现准确预测，但在存在QGP背景时会退化，即使在背景去除后仍低于无背景基线。相比之下，应用于背景去除后粒子云的DGCNNs在整个χ范围内保持高精度。&lt;h4&gt;结论&lt;/h4&gt;基于点云的图神经网络在真实条件下能够利用完整喷流结构，展现出显著优势，适合用于喷流与介质相互作用的精确研究。&lt;h4&gt;翻译&lt;/h4&gt;在相对论重离子碰撞中，喷流与色禁闭QCD介质的传统相互作用评估方法是测量喷流观测量分布相对于其在质子-质子碰撞中基线的修正。深度学习方法能够实现对这些修正的逐喷流评估，增强喷流作为核介质精密探针的用途。在这项工作中，我们使用线性玻尔兹曼传输(LBT)模型预测喷流在通过夸克-胶子等离子体(QGP)介质时的逐喷流能量损失分数χ。为了近似真实的实验条件，我们将介质修正的喷流嵌入热背景中并应用组分减法进行背景去除。研究了两种网络架构：使用喷流图像的卷积神经网络(CNNs)和使用粒子云的动态图卷积神经网络(DGCNNs)。我们发现CNNs对无背景喷流能实现准确预测，但在存在QGP背景时会退化，即使在背景去除后仍低于无背景基线。相比之下，应用于背景去除后粒子云的DGCNNs在整个χ范围内保持高精度，展示了基于点云的图神经网络在真实条件下利用完整喷流结构的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Jet interactions with the color-deconfined QCD medium in relativisticheavy-ion collisions are conventionally assessed by measuring the modificationof the distributions of jet observables with respect to their baselines inproton-proton collisions. Deep learning methods enable per-jet evaluation ofthese modifications, enhancing the use of jets as precision probes of thenuclear medium. In this work, we predict the jet-by-jet fractional energy loss$\chi$ for jets evolving through a quark-gluon plasma (QGP) medium using aLinear Boltzmann Transport (LBT) model. To approximate realistic experimentalconditions, we embed medium-modified jets in a thermal background and applyConstituent Subtraction for background removal. Two network architectures arestudied: convolutional neural networks (CNNs) using jet images, and dynamicgraph convolutional neural networks (DGCNNs) using particle clouds. We findthat CNNs achieve accurate predictions for background-free jets but degrade inthe presence of the QGP background and remain below the background-freebaseline even after background subtraction. In contrast, DGCNNs applied tobackground-subtracted particle clouds maintain high accuracy across the entire$\chi$ range, demonstrating the advantage of point-cloud-based graph neuralnetworks that exploit full jet structure under realistic conditions.</description>
      <author>example@mail.com (Ran Li, Yi-Lun Du, Shanshan Cao)</author>
      <guid isPermaLink="false">2508.20856v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks</title>
      <link>http://arxiv.org/abs/2508.20829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ATM-GAD的自适应图神经网络，通过利用时间基元进行金融异常检测，能够捕捉金融欺诈的两个时间特征：时间基元和账户特定的异常活动间隔。&lt;h4&gt;背景&lt;/h4&gt;金融欺诈检测对保护数十亿美元至关重要，但现代金融系统中相互关联的实体和快速变化的交易行为常常击败传统机器学习模型。基于图的检测器虽取得进展，但仍忽略时间方面的欺诈特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉时间基元和账户特定异常活动间隔的金融欺诈检测方法，提高检测准确率并发现传统方法遗漏的欺诈模式。&lt;h4&gt;方法&lt;/h4&gt;提出ATM-GAD，包含时间基元提取器（压缩交易历史为信息量最大的基元）、双重注意力块（内部注意力和跨基元注意力）以及可微自适应时间窗口学习器（为每个节点定制观察窗口）。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的实验表明，ATM-GAD始终优于七个强大的异常检测基线方法，能够发现早期方法未检测到的欺诈模式。&lt;h4&gt;结论&lt;/h4&gt;ATM-GAD通过有效利用时间基元和自适应时间窗口学习，显著提高了金融欺诈检测的性能，能够捕捉传统方法忽略的时间相关欺诈特征。&lt;h4&gt;翻译&lt;/h4&gt;金融欺诈检测对于保护数十亿美元至关重要，然而现代金融系统中相互关联的实体和快速变化的交易行为常常击败传统的机器学习模型。最近的基于图的检测器通过将交易表示为网络取得了进展，但它们仍然忽略了两个根植于时间的欺诈特征：(1)时间基元——揭示可疑资金流动的重复性、指示性子图；(2)账户特定的异常活动间隔，当欺诈仅以特定于每个实体的短暂爆发形式出现时。为了利用这两种信号，我们引入了ATM-GAD，这是一种利用时间基元进行金融异常检测的自适应图神经网络。时间基元提取器将每个账户的交易历史压缩为信息量最大的基元，同时保留拓扑和时间模式。这些基元随后由双重注意力块分析：内部注意力处理单个基元内的交互，而跨基元注意力跨基元聚合证据以揭示多步欺诈方案。同时，一个可微的自适应时间窗口学习器为每个节点定制观察窗口，使模型能够精确关注最具信息量的时间切片。在四个真实世界数据集上的实验表明，ATM-GAD始终优于七个强大的异常检测基线方法，发现了早期方法未发现的欺诈模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial fraud detection is essential to safeguard billions of dollars, yetthe intertwined entities and fast-changing transaction behaviors in modernfinancial systems routinely defeat conventional machine learning models. Recentgraph-based detectors make headway by representing transactions as networks,but they still overlook two fraud hallmarks rooted in time: (1) temporalmotifs--recurring, telltale subgraphs that reveal suspicious money flows asthey unfold--and (2) account-specific intervals of anomalous activity, whenfraud surfaces only in short bursts unique to each entity. To exploit bothsignals, we introduce ATM-GAD, an adaptive graph neural network that leveragestemporal motifs for financial anomaly detection. A Temporal Motif Extractorcondenses each account's transaction history into the most informative motifs,preserving both topology and temporal patterns. These motifs are then analyzedby dual-attention blocks: IntraA reasons over interactions within a singlemotif, while InterA aggregates evidence across motifs to expose multi-stepfraud schemes. In parallel, a differentiable Adaptive Time-Window Learnertailors the observation window for every node, allowing the model to focusprecisely on the most revealing time slices. Experiments on four real-worlddatasets show that ATM-GAD consistently outperforms seven stronganomaly-detection baselines, uncovering fraud patterns missed by earliermethods.</description>
      <author>example@mail.com (Zeyue Zhang, Lin Song, Erkang Bao, Xiaoling Lv, Xinyue Wang)</author>
      <guid isPermaLink="false">2508.20829v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.20597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了局部虚拟节点(LVN)方法，通过引入可训练嵌入的虚拟节点来缓解图神经网络中的过度压缩问题，同时保持原始图的全局结构不变。&lt;h4&gt;背景&lt;/h4&gt;过度压缩是训练图神经网络处理长程依赖关系时面临的挑战。GNN的感受野需要足够大以实现远程节点间的通信，但从广泛邻域收集信息并压缩到固定大小的节点表示中，会使消息传递容易受到瓶颈影响。现有的图重连和添加虚拟节点方法会改变输入图的全球拓扑结构，可能破坏原始图结构中编码的领域知识。&lt;h4&gt;目的&lt;/h4&gt;缓解过度压缩的影响，同时不显著破坏输入图的全局结构，并保留原始图结构中编码的领域知识。&lt;h4&gt;方法&lt;/h4&gt;提出局部虚拟节点(LVN)方法，具有可训练的嵌入。LVN的位置由节点中心性决定，指示潜在瓶颈的存在。在可能存在瓶颈的区域改善连通性，并在选定的中心区域之间共享可训练的LVN嵌入，促进远程节点间的通信而不需要添加更多层。&lt;h4&gt;主要发现&lt;/h4&gt;LVN可以增强结构连通性，显著提高图分类和节点分类任务的性能。代码可在https://github.com/ALLab-Boun/LVN/获取。&lt;h4&gt;结论&lt;/h4&gt;LVN方法有效解决了过度压缩问题，保持了原始图的全局结构，提高了图神经网络在处理长程依赖关系任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;过度压缩是训练图神经网络处理涉及长程依赖关系的任务时面临的挑战。在此类任务中，GNN的感受野应足够大，以实现远程节点之间的通信。然而，从广泛的邻域收集信息并将其内容压缩到固定大小的节点表示中，使得消息传递容易受到瓶颈的影响。图重连和添加虚拟节点是常用的补救措施，它们通过在瓶颈周围创建额外路径来缓解过度压缩。然而，这些技术会改变输入图的全球拓扑结构，并破坏原始图结构中编码的领域知识，这两者对特定任务和领域可能都很重要。本研究提出局部虚拟节点(LVN)，具有可训练的嵌入，以缓解过度压缩的影响，而不显著破坏输入图的全球结构。LVN的位置由节点中心性决定，该指标指示潜在瓶颈的存在。因此，所提出的方法旨在改善可能存在瓶颈的区域的连通性。此外，在选定的中心区域之间共享可训练的LVN嵌入，促进远程节点之间的通信，而无需添加更多层。在基准数据集上的广泛实验表明，LVN可以增强结构连通性，并显著提高图分类和节点分类任务的性能。代码可在https://github.com/ALLab-Boun/LVN/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over-squashing is a challenge in training graph neural networks for tasksinvolving long-range dependencies. In such tasks, a GNN's receptive fieldshould be large enough to enable communication between distant nodes. However,gathering information from a wide range of neighborhoods and squashing itscontent into fixed-size node representations makes message-passing vulnerableto bottlenecks. Graph rewiring and adding virtual nodes are commonly studiedremedies that create additional pathways around bottlenecks to mitigateover-squashing. However, these techniques alter the input graph's globaltopology and disrupt the domain knowledge encoded in the original graphstructure, both of which could be essential to specific tasks and domains. Thisstudy presents Local Virtual Nodes (LVN) with trainable embeddings to alleviatethe effects of over-squashing without significantly corrupting the globalstructure of the input graph. The position of the LVNs is determined by thenode centrality, which indicates the existence of potential bottlenecks. Thus,the proposed approach aims to improve the connectivity in the regions withlikely bottlenecks. Furthermore, trainable LVN embeddings shared acrossselected central regions facilitate communication between distant nodes withoutadding more layers. Extensive experiments on benchmark datasets demonstratethat LVNs can enhance structural connectivity and significantly improveperformance on graph and node classification tasks. The code can be found athttps://github.com/ALLab-Boun/LVN/}{https://github.com/ALLab-Boun/LVN/.</description>
      <author>example@mail.com (Tuğrul Hasan Karabulut, İnci M. Baytaş)</author>
      <guid isPermaLink="false">2508.20597v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models</title>
      <link>http://arxiv.org/abs/2508.20583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图语言模型(GLMs)旨在结合图神经网络的结构推理能力与大语言模型的语义理解能力，但当前评估基准不足以评估多模态推理能力。作者提出了CLEGR基准来评估不同复杂度的多模态推理，发现软提示的LLM基线与包含完整GNN骨干的GLM性能相当，质疑了将图结构整合到LLMs中的必要性，并指出GLM在需要结构推理的任务中性能显著下降。&lt;h4&gt;背景&lt;/h4&gt;图语言模型(GLMs)的发展旨在将图神经网络(GNNs)的结构推理能力与大语言模型(LLMs)的语义理解能力相结合。&lt;h4&gt;目的&lt;/h4&gt;解决当前GLM评估基准的不足，提出能够有效评估多模态推理能力的新基准。&lt;h4&gt;方法&lt;/h4&gt;引入CLEGR(组合语言-图推理)基准，采用合成图生成流程，并配需要同时对结构和文本语义进行联合推理的问题。&lt;h4&gt;主要发现&lt;/h4&gt;1) 当前GLM评估基准主要依赖节点级分类数据集，不足以评估多模态推理；2) 仅使用单模态信息就能在现有基准上取得良好性能；3) 软提示的LLM基线与包含完整GNN骨干的GLM性能相当，质疑了图结构整合的必要性；4) GLM在需要结构推理的任务中表现出显著的性能下降。&lt;h4&gt;结论&lt;/h4&gt;当前GLM的图推理能力存在局限性，CLEGR基准为社区推进涉及图结构和语言的明确多模态推理提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;图语言模型(GLMs)的发展旨在整合图神经网络(GNNs)的结构推理能力与大语言模型(LLMs)的语义理解能力。然而，我们证明当前的GLM评估基准主要是重新利用的节点级分类数据集，不足以评估多模态推理。我们的分析表明，仅使用单模态信息就能在这些基准上取得强大性能，这表明它们不需要图语言集成。为了解决这一评估差距，我们引入了CLEGR(组合语言-图推理)基准，旨在评估不同复杂度级别的多模态推理。我们的基准采用合成图生成流程，并配需要同时对结构和文本语义进行联合推理的问题。我们对代表性的GLM架构进行了彻底评估，发现软提示的LLM基线与包含完整GNN骨干的GLM性能相当。这一结果质疑了将图结构整合到LLMs中的架构必要性。我们进一步表明，GLM在需要结构推理的任务中表现出显著的性能下降。这些发现突显了当前GLM图推理能力的局限性，并为社区推进涉及图结构和语言的明确多模态推理提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developments in Graph-Language Models (GLMs) aim to integrate the structuralreasoning capabilities of Graph Neural Networks (GNNs) with the semanticunderstanding of Large Language Models (LLMs). However, we demonstrate thatcurrent evaluation benchmarks for GLMs, which are primarily repurposednode-level classification datasets, are insufficient to assess multimodalreasoning. Our analysis reveals that strong performance on these benchmarks isachievable using unimodal information alone, suggesting that they do notnecessitate graph-language integration. To address this evaluation gap, weintroduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designedto evaluate multimodal reasoning at various complexity levels. Our benchmarkemploys a synthetic graph generation pipeline paired with questions thatrequire joint reasoning over structure and textual semantics. We perform athorough evaluation of representative GLM architectures and find thatsoft-prompted LLM baselines perform on par with GLMs that incorporate a fullGNN backbone. This result calls into question the architectural necessity ofincorporating graph structure into LLMs. We further show that GLMs exhibitsignificant performance degradation in tasks that require structural reasoning.These findings highlight limitations in the graph reasoning capabilities ofcurrent GLMs and provide a foundation for advancing the community towardexplicit multimodal reasoning involving graph structure and language.</description>
      <author>example@mail.com (Soham Petkar, Hari Aakash K, Anirudh Vempati, Akshit Sinha, Ponnurangam Kumarauguru, Chirag Agarwal)</author>
      <guid isPermaLink="false">2508.20583v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition</title>
      <link>http://arxiv.org/abs/2508.20579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLaRE的新型基于图的关键点区域嵌入网络，用于面部表情识别。该方法使用3D面部对齐提取关键点，并通过分层粗化构建商图来保留空间结构同时降低复杂性。实验表明，该方法在AffectNet和FERG数据集上分别达到64.89%和94.24%的准确率，优于多个基线方法。消融研究表明，商图区域级别的嵌入有助于提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;面部表情识别(FER)是计算机视觉中的关键任务，在人机交互、监控和辅助技术等领域有广泛应用。然而，遮挡、表达变化性和缺乏可解释性等问题限制了传统FER系统的性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模面部关键点之间关系依赖的结构化和可解释的学习方法，以提高面部表情识别的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出GLaRE，一种基于图的关键点区域嵌入网络。该方法使用3D面部对齐提取面部关键点，通过分层粗化构建商图以保留空间结构同时降低复杂性，实现结构化和可解释的学习。&lt;h4&gt;主要发现&lt;/h4&gt;GLaRE在AffectNet上达到64.89%的准确率，在FERG上达到94.24%的准确率，优于多个现有基线方法。消融研究表明，来自商图区域级别的嵌入对提高预测性能有贡献。&lt;h4&gt;结论&lt;/h4&gt;图神经网络为面部表情识别提供了强大替代方案，通过建模面部关键点之间的关系依赖，实现了结构化和可解释的学习。GLaRE方法有效解决了传统FER系统面临的遮挡、表达变化性和缺乏可解释性等问题。&lt;h4&gt;翻译&lt;/h4&gt;面部表情识别(FER)是计算机视觉中的一个关键任务，在人机交互、监控和辅助技术等领域有广泛应用。然而，遮挡、表达变化性和缺乏可解释性等挑战阻碍了传统FER系统的性能。图神经网络(GNN)通过建模面部关键点之间的关系依赖，提供了一种强大的替代方案，实现了结构化和可解释的学习。在本文中，我们提出了GLaRE，一种用于情感识别的新型基于图的关键点区域嵌入网络。使用3D面部对齐提取面部关键点，并通过分层粗化构建商图以保留空间结构同时降低复杂性。我们的方法在AffectNet上达到64.89%的准确率，在FERG上达到94.24%的准确率，优于多个现有基线方法。此外，消融研究已经证明，来自商图的区域级嵌入有助于提高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial expression recognition (FER) is a crucial task in computer vision withwide range of applications including human computer interaction, surveillance,and assistive technologies. However, challenges such as occlusion, expressionvariability, and lack of interpretability hinder the performance of traditionalFER systems. Graph Neural Networks (GNNs) offer a powerful alternative bymodeling relational dependencies between facial landmarks, enabling structuredand interpretable learning. In this paper, we propose GLaRE, a novelGraph-based Landmark Region Embedding network for emotion recognition. Faciallandmarks are extracted using 3D facial alignment, and a quotient graph isconstructed via hierarchical coarsening to preserve spatial structure whilereducing complexity. Our method achieves 64.89 percentage accuracy on AffectNetand 94.24 percentage on FERG, outperforming several existing baselines.Additionally, ablation studies have demonstrated that region-level embeddingsfrom quotient graphs have contributed to improved prediction performance.</description>
      <author>example@mail.com (Debasis Maji, Debaditya Barman)</author>
      <guid isPermaLink="false">2508.20579v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Molecular Machine Learning in Chemical Process Design</title>
      <link>http://arxiv.org/abs/2508.20527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于分子机器学习在化学过程工程领域应用的综述性文章，探讨了其在预测物性和发现新分子结构方面的潜力，以及未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;分子机器学习在化学过程工程领域展现出巨大潜力，能够提供高精度预测和探索化学空间。&lt;h4&gt;目的&lt;/h4&gt;回顾当前最先进的分子机器学习模型，讨论进一步发展的研究方向，并探索在化学过程规模上应用分子机器学习的方法。&lt;h4&gt;方法&lt;/h4&gt;分析图神经网络和transformers等机器学习方法，探讨如何通过混合或物理信息化的方式整合物理化学知识，以及如何将分子机器学习整合到过程设计和优化公式中。&lt;h4&gt;主要发现&lt;/h4&gt;分子机器学习能够为纯组分及其混合物的性质提供高度准确的预测，并能探索化学空间寻找新的分子结构；在化学过程规模上应用分子机器学习是有价值但尚未充分探索的领域。&lt;h4&gt;结论&lt;/h4&gt;将分子机器学习整合到过程设计和优化中，有望加速新型分子和过程的识别；创建分子和过程设计基准并验证候选分子至关重要，可能需要与化学工业合作。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了关于分子机器学习在化学过程工程领域的观点。最近，分子机器学习在(i)为纯组分及其混合物的性质提供高度准确的预测，和(ii)探索化学空间以寻找新的分子结构方面展示了巨大潜力。我们回顾了当前最先进的分子机器学习模型，并讨论了有希望进一步发展的研究方向。这包括机器学习方法，如图神经网络和transformers，可以通过混合或物理信息化的方式整合物理化学知识来进一步发展。然后，我们考虑在化学过程规模上利用分子机器学习，这是非常理想但尚未充分探索的。我们讨论了如何将分子机器学习整合到过程设计和优化公式中，有望加速新型分子和过程的识别。为此，创建分子和过程设计基准并在实践中验证提出的候选分子将是至关重要的，可能需要与化学工业合作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a perspective on molecular machine learning (ML) in the field ofchemical process engineering. Recently, molecular ML has demonstrated greatpotential in (i) providing highly accurate predictions for properties of purecomponents and their mixtures, and (ii) exploring the chemical space for newmolecular structures. We review current state-of-the-art molecular ML modelsand discuss research directions that promise further advancements. Thisincludes ML methods, such as graph neural networks and transformers, which canbe further advanced through the incorporation of physicochemical knowledge in ahybrid or physics-informed fashion. Then, we consider leveraging molecular MLat the chemical process scale, which is highly desirable yet rather unexplored.We discuss how molecular ML can be integrated into process design andoptimization formulations, promising to accelerate the identification of novelmolecules and processes. To this end, it will be essential to create moleculeand process design benchmarks and practically validate proposed candidates,possibly in collaboration with the chemical industry.</description>
      <author>example@mail.com (Jan G. Rittig, Manuel Dahmen, Martin Grohe, Philippe Schwaller, Alexander Mitsos)</author>
      <guid isPermaLink="false">2508.20527v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records</title>
      <link>http://arxiv.org/abs/2508.20500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为结构感知超图Transformer(SHGT)的新型框架，用于电子健康记录(EHR)中的诊断预测任务。该框架通过超图结构编码器和Transformer架构的结合，有效解决了现有图神经网络方法无法捕捉高阶依赖性和表示能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHR)通过标准化医疗代码系统化组织患者健康数据，是预测建模的宝贵资源。图神经网络(GNNs)虽已证明在EHR内医疗代码间交互建模方面有效，但现有方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有GNN方法在处理EHR数据时的两个主要不足：无法捕捉临床数据中固有的高阶依赖性，以及局部化消息传递方案限制了表示能力。&lt;h4&gt;方法&lt;/h4&gt;提出结构感知超图Transformer(SHGT)框架，包含三个核心创新：a)采用超图结构编码器捕捉医疗代码间的高阶交互；b)集成Transformer架构推理整个超图；c)设计包含超图重建的定制损失函数以保留原始超图结构。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界EHR数据集上的实验表明，所提出的SHGT在诊断预测任务上性能优于现有的最先进模型。&lt;h4&gt;结论&lt;/h4&gt;SHGT框架通过结合超图结构和Transformer架构，有效解决了现有GNN方法的局限性，提高了EHR数据中预测建模的准确性。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHR)通过标准化医疗代码系统化组织患者健康数据，作为预测建模的全面且宝贵的资源来源。图神经网络(GNNs)已在EHR内医疗代码之间交互建模方面展现出有效性。然而，现有基于GNN的方法存在不足：a)它们依赖成对关系，无法捕捉临床数据中固有的高阶依赖性；b)局部化消息传递方案限制了表示能力。为解决这些问题，本文提出了一种新颖的结构感知超图Transformer(SHGT)框架，遵循三方面理念：a)采用超图结构编码器捕捉医疗代码间的高阶交互；b)集成Transformer架构推理整个超图；c)设计包含超图重建的定制损失函数以保留超图的原始结构。在真实世界EHR数据集上的实验表明，所提出的SHGT在诊断预测上优于现有最先进模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHR) systematically organize patient health datathrough standardized medical codes, serving as a comprehensive and invaluablesource for predictive modeling. Graph neural networks (GNNs) have demonstratedeffectiveness in modeling interactions between medical codes within EHR.However, existing GNN-based methods are inadequate due to: a) their reliance onpairwise relations fails to capture the inherent higher-order dependencies inclinical data, and b) the localized message-passing scheme limitsrepresentation power. To address these issues, this paper proposes a novelStructure-aware HyperGraph Transformer (SHGT) framework following three-foldideas: a) employing a hypergraph structural encoder to capture higher-orderinteractions among medical codes, b) integrating the Transformer architectureto reason over the entire hypergraph, and c) designing a tailored loss functionincorporating hypergraph reconstruction to preserve the hypergraph's originalstructure. Experiments on real-world EHR datasets demonstrate that the proposedSHGT outperforms existing state-of-the-art models on diagnosis prediction.</description>
      <author>example@mail.com (Haiyan Wang, Ye Yuan)</author>
      <guid isPermaLink="false">2508.20500v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes</title>
      <link>http://arxiv.org/abs/2508.19356v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 to 4 hours read time. 73 pages. 35 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇介绍性文章探讨了图数据建模在化学科学中的应用，特别是在分子、蛋白质和化学过程的表示与学习方面。&lt;h4&gt;背景&lt;/h4&gt;图在化学科学中处于核心地位，提供了描述分子、蛋白质、反应和工业过程的一种自然语言，能够捕获支撑材料、生物学和医学的相互作用和结构。&lt;h4&gt;目的&lt;/h4&gt;介绍图作为化学中的数学对象，展示学习算法（特别是图神经网络）如何在这些图上操作，概述图设计基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图建模中的作用。&lt;h4&gt;方法&lt;/h4&gt;采用图数据建模方法，应用图神经网络等学习算法，构建图的基础结构来表示化学实体和过程。&lt;h4&gt;主要发现&lt;/h4&gt;图是化学科学中描述分子、蛋白质等的自然语言；图可以捕获支撑材料、生物学和医学的相互作用和结构；学习算法（特别是图神经网络）可以在图上有效操作。&lt;h4&gt;结论&lt;/h4&gt;这些概念共同为读者应用图方法到下一代化学发现做好了准备。&lt;h4&gt;翻译&lt;/h4&gt;图是化学科学的核心，为描述分子、蛋白质、反应和工业过程提供了自然语言。它们捕获了支撑材料、生物学和医学的相互作用和结构。本介绍性文章《图数据建模：分子、蛋白质与化学过程》将图作为化学中的数学对象进行介绍，并展示了学习算法（特别是图神经网络）如何在这些图上操作。我们概述了图设计的基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图建模中的作用。这些概念共同为读者应用图方法到下一代化学发现做好了准备。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1021/acsinfocus.7e9017&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are central to the chemical sciences, providing a natural language todescribe molecules, proteins, reactions, and industrial processes. They captureinteractions and structures that underpin materials, biology, and medicine.This primer, Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes,introduces graphs as mathematical objects in chemistry and shows how learningalgorithms (particularly graph neural networks) can operate on them. We outlinethe foundations of graph design, key prediction tasks, representative examplesacross chemical sciences, and the role of machine learning in graph-basedmodeling. Together, these concepts prepare readers to apply graph methods tothe next generation of chemical discovery.</description>
      <author>example@mail.com (José Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Adrian Jinich, Radhakrishnan Mahadevan, Benjamin Sanchez-Lengeling)</author>
      <guid isPermaLink="false">2508.19356v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19071v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了TRIGON，一个新颖的图神经网络框架，通过学习从多个图视图中选择相关三角形来构建丰富的非平面三角剖分，以缓解图神经网络中的过度压缩和过度平滑问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的主要范式，但其性能受到图拓扑问题(特别是过度压缩和过度平滑)的限制。&lt;h4&gt;目的&lt;/h4&gt;引入TRIGON框架，通过修改图拓扑来促进更有效的信息传播，从而缓解GNNs的性能限制。&lt;h4&gt;方法&lt;/h4&gt;TRIGON通过从多个图视图中学习选择相关三角形来构建丰富的非平面三角剖分，并联合优化三角形选择和下游分类性能，产生重新布线的图。&lt;h4&gt;主要发现&lt;/h4&gt;与现有重新布线方法相比，TRIGON产生的图具有显著改进的结构特性，如减少直径、增加谱间隙和降低有效电阻。&lt;h4&gt;结论&lt;/h4&gt;实证结果表明，TRIGON在同质和异质基准测试中的节点分类任务上优于最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的主要范式。然而，它们的性能受到图拓扑固有问题的限制，最明显的是过度压缩和过度平滑。最近在图重新布线方面的进展旨在通过修改图拓扑来促进更有效的信息传播，从而缓解这些限制。在这项工作中，我们介绍了TRIGON，一个新颖的框架，它通过从多个图视图中学习选择相关三角形来构建丰富的非平面三角剖分。通过联合优化三角形选择和下游分类性能，我们的方法产生的重新布线图具有显著改进的结构特性，如减少直径、增加谱间隙和降低有效电阻，与现有的重新布线方法相比。实证结果表明，TRIGON在同质和异质基准测试中的节点分类任务上优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as the leading paradigm forlearning over graph-structured data. However, their performance is limited byissues inherent to graph topology, most notably oversquashing andoversmoothing. Recent advances in graph rewiring aim to mitigate theselimitations by modifying the graph topology to promote more effectiveinformation propagation. In this work, we introduce TRIGON, a novel frameworkthat constructs enriched, non-planar triangulations by learning to selectrelevant triangles from multiple graph views. By jointly optimizing triangleselection and downstream classification performance, our method produces arewired graph with markedly improved structural properties such as reduceddiameter, increased spectral gap, and lower effective resistance compared toexisting rewiring methods. Empirical results demonstrate that TRIGONoutperforms state-of-the-art approaches on node classification tasks across arange of homophilic and heterophilic benchmarks.</description>
      <author>example@mail.com (Hugo Attali, Thomas Papastergiou, Nathalie Pernelle, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2508.19071v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</title>
      <link>http://arxiv.org/abs/2508.17630v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了量子图注意力网络(QGAT)，一种将变分量子电路集成到注意力机制中的混合图神经网络，利用量子并行性减少计算复杂度，提高模型表达能力。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络在处理复杂结构数据时面临计算效率和表达能力的挑战，量子计算为解决这些问题提供了新思路。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合量子计算优势的新型图神经网络架构，提高模型表达能力，同时减少计算复杂度，增强对噪声数据的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;QGAT采用强纠缠量子电路和幅度编码的节点特征实现非线性交互；使用单一量子电路同时生成多个注意力系数，而非传统多头注意力分别计算；通过量子并行性实现参数共享；联合优化经典投影权重和量子电路参数；采用模块化设计便于与现有架构集成。&lt;h4&gt;主要发现&lt;/h4&gt;QGAT能有效捕获复杂结构依赖关系；在归纳场景中具有更好的泛化能力；量子嵌入增强了特征和结构噪声的鲁棒性；在化学、生物学和网络分析等多个领域具有可扩展的量子增强学习潜力。&lt;h4&gt;结论&lt;/h4&gt;量子图注意力网络通过结合量子计算和图神经网络优势，提供了一种更高效、更鲁棒的模型架构，能够处理复杂结构化数据并在真实世界噪声数据中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了量子图注意力网络(QGAT)，一种将变分量子电路集成到注意力机制中的混合图神经网络。其核心是QGAT采用强纠缠量子电路和幅度编码的节点特征来实现富有表现力的非线性交互。与分别计算每个头的经典多头注意力不同，QGAT利用单一量子电路同时生成多个注意力系数。这种量子并行性促进了头之间的参数共享，显著减少了计算开销和模型复杂度。经典投影权重和量子电路参数以端到端方式联合优化，确保对学习任务的灵活适应。实证结果表明，QGAT在捕获复杂结构依赖关系方面有效，并在归纳场景中提高了泛化能力，突显了其在化学、生物学和网络分析等领域可扩展量子增强学习的潜力。此外，实验证实量子嵌入增强了特征和结构噪声的鲁棒性，表明在处理真实世界噪声数据方面具有优势。QGAT的模块化还确保了与现有架构的简便集成，使其能够轻松增强基于经典注意力的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neuralnetwork that integrates variational quantum circuits into the attentionmechanism. At its core, QGAT employs strongly entangling quantum circuits withamplitude-encoded node features to enable expressive nonlinear interactions.Distinct from classical multi-head attention that separately computes eachhead, QGAT leverages a single quantum circuit to simultaneously generatemultiple attention coefficients. This quantum parallelism facilitates parametersharing across heads, substantially reducing computational overhead and modelcomplexity. Classical projection weights and quantum circuit parameters areoptimized jointly in an end-to-end manner, ensuring flexible adaptation tolearning tasks. Empirical results demonstrate QGAT's effectiveness in capturingcomplex structural dependencies and improved generalization in inductivescenarios, highlighting its potential for scalable quantum-enhanced learningacross domains such as chemistry, biology, and network analysis. Furthermore,experiments confirm that quantum embedding enhances robustness against featureand structural noise, suggesting advantages in handling real-world noisy data.The modularity of QGAT also ensures straightforward integration into existingarchitectures, allowing it to easily augment classical attention-based models.</description>
      <author>example@mail.com (An Ning, Tai Yue Li, Nan Yow Chen)</author>
      <guid isPermaLink="false">2508.17630v3</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning</title>
      <link>http://arxiv.org/abs/2508.17387v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为Graph-R1的新方法，将图学习任务转化为文本推理问题，利用大型推理模型(LRMs)在没有任务特定监督的情况下解决节点分类、链接预测和图分类等任务，并通过强化学习框架优化推理过程。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)受限于固定的标签空间，而大型语言模型(LLMs)缺乏结构归纳偏置，使得在没有任务特定监督的情况下推广到未见过的图任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需GNN的方法，将图任务重新表述为文本推理问题，利用大型推理模型(LRMs)的零样本推理能力来解决图学习任务。&lt;h4&gt;方法&lt;/h4&gt;开发了Graph-R1，一个利用特定任务的重新思考模板来引导线性化图推理的强化学习框架；为节点分类、链接预测和图分类任务创建了具有详细推理轨迹的首批数据集。&lt;h4&gt;主要发现&lt;/h4&gt;Graph-R1在零样本设置下优于最先进的基线，产生可解释且有效的预测；显式推理在图学习方面具有潜力。&lt;h4&gt;结论&lt;/h4&gt;该工作强调了显式推理在图学习方面的潜力，并为未来研究提供了新的数据集和框架资源。&lt;h4&gt;翻译&lt;/h4&gt;在没有任务特定监督的情况下推广到未见过的图任务仍然具有挑战性。图神经网络(GNNs)受限于固定的标签空间，而大型语言模型(LLMs)缺乏结构归纳偏置。大型推理模型(LRMs)的最新进展通过明确的长链式推理提供了零样本替代方案。受此启发，我们提出了一种无需GNN的方法，将图任务（节点分类、链接预测和图分类）重新表述为由LRMs解决的文本推理问题。我们为这些任务引入了具有详细推理轨迹的首批数据集，并开发了Graph-R1，这是一个利用特定任务的重新思考模板来引导线性化图推理的强化学习框架。实验表明，Graph-R1在零样本设置下优于最先进的基线，产生可解释且有效的预测。这项工作强调了显式推理在图学习方面的潜力，并为未来研究提供了新资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizing to unseen graph tasks without task-pecific supervision remainschallenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,while Large Language Models (LLMs) lack structural inductive biases. Recentadvances in Large Reasoning Models (LRMs) provide a zero-shot alternative viaexplicit, long chain-of-thought reasoning. Inspired by this, we propose aGNN-free approach that reformulates graph tasks--node classification, linkprediction, and graph classification--as textual reasoning problems solved byLRMs. We introduce the first datasets with detailed reasoning traces for thesetasks and develop Graph-R1, a reinforcement learning framework that leveragestask-specific rethink templates to guide reasoning over linearized graphs.Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines inzero-shot settings, producing interpretable and effective predictions. Our workhighlights the promise of explicit reasoning for graph learning and providesnew resources for future research.</description>
      <author>example@mail.com (Yicong Wu, Guangyue Lu, Yuan Zuo, Huarong Zhang, Junjie Wu)</author>
      <guid isPermaLink="false">2508.17387v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts</title>
      <link>http://arxiv.org/abs/2508.20488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025 (Highlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为双重不确定性优化(DUO)的测试时适应框架，用于解决单目3D物体检测在真实世界域偏移下的可靠性问题。DUO通过同时最小化语义不确定性和几何不确定性，提高了模型在环境或传感器变化情况下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;单目3D物体检测(M3OD)对自动驾驶等安全关键应用至关重要，但在环境或传感器变化引起的真实世界域偏移下，其可靠性显著下降。测试时适应(TTA)方法虽已出现，但未能解决M3OD固有的双重不确定性问题。&lt;h4&gt;目的&lt;/h4&gt;提出双重不确定性优化(DUO)，首个联合最小化语义不确定性和几何不确定性的TTA框架，用于实现鲁棒的单目3D物体检测。&lt;h4&gt;方法&lt;/h4&gt;通过凸优化视角引入focal loss的创新凸结构并推导无监督版本，实现无标签的不确定性权重和高不确定性物体的平衡学习；设计语义感知的法线场约束，在具有清晰语义线索的区域保持几何一致性，减少不稳定3D表示的不确定性；形成双分支互补循环机制。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，在各种数据集和域偏移类型上，DUO优于现有方法。增强的空间感知改善语义分类，鲁棒的语义预测进一步优化空间理解。&lt;h4&gt;结论&lt;/h4&gt;DUO通过同时解决语义不确定性和几何不确定性，有效提高了单目3D物体检测在域偏移情况下的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;精确的单目3D物体检测(M3OD)对自动驾驶等安全关键应用至关重要，但在环境或传感器变化引起的真实世界域偏移下，其可靠性显著下降。为解决这些偏移，测试时适应(TTA)方法应运而生，使模型能够在推理过程中适应目标分布。虽然先前的TTA方法认识到低不确定性与高泛化能力之间的正相关关系，但它们未能解决M3OD固有的双重不确定性：语义不确定性（模糊的类别预测）和几何不确定性（不稳定的空间定位）。为弥合这一差距，我们提出双重不确定性优化(DUO)，首个旨在联合最小化两种不确定性的TTA框架，用于鲁棒的单目3D物体检测。通过凸优化视角，我们引入了focal loss的创新凸结构，并进一步推导出一种新颖的无监督版本，实现无标签的不确定性权重和高不确定性物体的平衡学习。同时，我们设计了语义感知的法线场约束，在具有清晰语义线索的区域保持几何一致性，减少来自不稳定3D表示的不确定性。这种双分支机制形成了互补循环：增强的空间感知改善语义分类，而鲁棒的语义预测进一步优化空间理解。大量实验证明，在各种数据集和域偏移类型上，DUO优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D目标检测(M3OD)在测试时域偏移下的可靠性问题。当面对真实世界中的环境变化(如恶劣天气)或传感器故障时，训练好的M3OD模型性能会严重下降。这个问题在自动驾驶等安全关键应用中尤为重要，因为这些应用需要高可靠性的3D感知能力，而现有的测试时适应方法未能有效处理M3OD中固有的双重不确定性：语义不确定性和几何不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过实证研究发现M3OD在域偏移下存在语义和几何两种不确定性，且现有方法存在两个主要局限：低分目标忽视和空间感知崩溃。基于凸优化理论，作者重新构建了focal loss的凸结构，并推导出无监督版本；同时设计了语义感知的法线场约束来增强几何一致性。作者借鉴了凸优化理论中的Legendre-Fenchel结构、focal loss的思想以及法线场约束在计算机视觉中的应用，但针对M3OD的特殊性进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出双重不确定性优化(DUO)框架，同时最小化语义和几何不确定性，形成互补循环。实现流程包括：1)语义不确定性优化：通过凸优化理论推导共轭focal loss，实现标签无关的动态加权，平衡高低分目标的学习；2)几何不确定性优化：使用Sobel算子将深度图转换为法线场，设计法线一致性约束，并通过语义指导的掩码只在低语义不确定性区域应用几何约束；3)整体优化：将两部分整合到统一框架中，实现语义和几何优化的互补反馈循环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出针对M3OD的双重不确定性优化框架；2)通过凸优化理论推导出共轭focal loss，实现标签无关的不确定性权重平衡；3)设计语义感知的法线场约束，保持几何一致性；4)发现并验证了双分支设计的互补循环。相比之前工作，DUO同时处理语义和几何双重不确定性，而现有方法主要关注单一不确定性；共轭focal loss无需标签即可实现动态加权，而传统focal loss依赖标签；DUO避免了直接最小化深度不确定性导致的模型崩溃问题，专门针对M3OD的特有挑战进行了设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了双重不确定性优化(DUO)框架，通过同时优化语义和几何不确定性，显著提升了单目3D目标检测模型在测试时域偏移下的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate monocular 3D object detection (M3OD) is pivotal for safety-criticalapplications like autonomous driving, yet its reliability deterioratessignificantly under real-world domain shifts caused by environmental or sensorvariations. To address these shifts, Test-Time Adaptation (TTA) methods haveemerged, enabling models to adapt to target distributions during inference.While prior TTA approaches recognize the positive correlation between lowuncertainty and high generalization ability, they fail to address the dualuncertainty inherent to M3OD: semantic uncertainty (ambiguous classpredictions) and geometric uncertainty (unstable spatial localization). Tobridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTAframework designed to jointly minimize both uncertainties for robust M3OD.Through a convex optimization lens, we introduce an innovative convex structureof the focal loss and further derive a novel unsupervised version, enablinglabel-agnostic uncertainty weighting and balanced learning for high-uncertaintyobjects. In parallel, we design a semantic-aware normal field constraint thatpreserves geometric coherence in regions with clear semantic cues, reducinguncertainty from the unstable 3D representation. This dual-branch mechanismforms a complementary loop: enhanced spatial perception improves semanticclassification, and robust semantic predictions further refine spatialunderstanding. Extensive experiments demonstrate the superiority of DUO overexisting methods across various datasets and domain shift types.</description>
      <author>example@mail.com (Zixuan Hu, Dongxiao Li, Xinzhu Ma, Shixiang Tang, Xiaotong Li, Wenhan Yang, Ling-Yu Duan)</author>
      <guid isPermaLink="false">2508.20488v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View 3D Point Tracking</title>
      <link>http://arxiv.org/abs/2508.21060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025, Oral. Project page: https://ethz-vlg.github.io/mvtracker&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种基于数据驱动的多视角三维点跟踪器，能够利用少量摄像头（如四个）实现动态场景中任意点的鲁棒和准确跟踪，解决了现有单目跟踪器的深度模糊和遮挡问题，以及传统多摄像头方法需要大量摄像头和繁琐优化的限制。&lt;h4&gt;背景&lt;/h4&gt;现有的单目跟踪器在处理深度模糊和遮挡问题时存在困难，而之前的多摄像头方法需要超过20个摄像头和繁琐的每序列优化，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种实用的多视角三维点跟踪器，能够在实际数量的摄像头情况下实现鲁棒和准确的在线跟踪，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;该前馈模型利用已知的摄像头姿态和多视角深度信息，将多视角特征融合为统一点云，并应用k近邻相关性和基于transformer的更新方法，即使在遮挡情况下也能可靠估计长程三维对应关系。模型在5K合成的多视角Kubric序列上训练，并在Panoptic Studio和DexYCB基准上评估。&lt;h4&gt;主要发现&lt;/h4&gt;在Panoptic Studio和DexYCB两个真实世界基准测试中，分别实现了3.1厘米和2.0厘米的中值轨迹误差。该方法能够很好地适应1-8个不同视角的多样化摄像头设置，以及24-150帧的不同视频长度。&lt;h4&gt;结论&lt;/h4&gt;通过发布跟踪器以及训练和评估数据集，该研究旨在为多视角三维跟踪研究设定新标准，并为实际应用提供实用工具。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了首个数据驱动的多视角三维点跟踪器，旨在使用多个摄像头视角跟踪动态场景中的任意点。与现有的单目跟踪器（难以处理深度模糊和遮挡问题）或之前的多摄像头方法（需要超过20个摄像头和繁琐的每序列优化）不同，我们的前馈模型使用实际数量的摄像头直接预测三维对应关系，实现了鲁棒且准确的在线跟踪。在已知摄像头姿态和多视角深度信息下，我们的跟踪器将多视角特征融合为统一点云，并应用k近邻相关性和基于transformer的更新方法，即使在遮挡情况下也能可靠估计长程三维对应关系。我们在5K合成的多视角Kubric序列上训练模型，并在两个真实世界基准上评估，分别实现了3.1厘米和2.0厘米的中值轨迹误差。我们的方法能够很好地适应1-8个不同视角的多样化摄像头设置，以及24-150帧的不同视频长度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点跟踪问题，特别是在动态场景中跟踪任意3D点。这个问题在计算机视觉中非常重要，因为它有众多应用，包括动态场景重建、机器人导航和增强现实。现有的单目跟踪器存在深度歧义和遮挡问题，而之前的多摄像头方法需要超过20个摄像头和繁琐的序列级优化，限制了实际应用场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有的2D点跟踪方法（如CoTracker、LocoTrack）和3D点跟踪方法（如SpatialTracker、DELTA），但针对多视图场景进行了改进。他们注意到单目方法的局限性，以及多视图方法中需要大量摄像头的问题。因此，他们设计了一个融合多视图特征到统一3D点云的方法，使用kNN相关性和transformer进行更新，这种方法既保留了多视图的优势，又减少了所需摄像头的数量，实现了更实用的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将多视图特征融合到一个统一的3D特征点云中，在这个点云中使用kNN相关性来捕获跨视图的时空关系，然后使用transformer迭代细化点轨迹。整体实现流程包括：1) 输入同步RGB帧、相机参数和深度图；2) 使用CNN提取每个视图的特征图；3) 将深度图转换为3D点云并与特征关联；4) 计算k近邻相关性；5) 使用transformer迭代更新点位置和特征；6) 处理长视频时使用滑动窗口进行推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个数据驱动的多视图3D点跟踪器；2) 使用融合的3D特征点云而非传统的2D网格或三平面表示；3) kNN相关性和transformer更新；4) 能够处理遮挡和适应不同的相机设置；5) 只需少量摄像头（如四个）就能实现高效鲁棒的跟踪。相比之前的工作，MVTracker不需要序列级优化，运行速度快（7.2 FPS），且对深度估计的噪声具有更好的鲁棒性，显著提升了在动态场景中的跟踪性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了MVTracker，首个数据驱动的多视图3D点跟踪器，通过融合多视图特征到统一的3D点云并使用kNN相关性和transformer更新，实现了仅需少量摄像头的实时、鲁棒的3D点跟踪，显著提升了在动态场景中的跟踪性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the first data-driven multi-view 3D point tracker, designed totrack arbitrary points in dynamic scenes using multiple camera views. Unlikeexisting monocular trackers, which struggle with depth ambiguities andocclusion, or prior multi-camera methods that require over 20 cameras andtedious per-sequence optimization, our feed-forward model directly predicts 3Dcorrespondences using a practical number of cameras (e.g., four), enablingrobust and accurate online tracking. Given known camera poses and eithersensor-based or estimated multi-view depth, our tracker fuses multi-viewfeatures into a unified point cloud and applies k-nearest-neighbors correlationalongside a transformer-based update to reliably estimate long-range 3Dcorrespondences, even under occlusion. We train on 5K synthetic multi-viewKubric sequences and evaluate on two real-world benchmarks: Panoptic Studio andDexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.Our method generalizes well to diverse camera setups of 1-8 views with varyingvantage points and video lengths of 24-150 frames. By releasing our trackeralongside training and evaluation datasets, we aim to set a new standard formulti-view 3D tracking research and provide a practical tool for real-worldapplications. Project page available at https://ethz-vlg.github.io/mvtracker.</description>
      <author>example@mail.com (Frano Rajič, Haofei Xu, Marko Mihajlovic, Siyuan Li, Irem Demir, Emircan Gündoğdu, Lei Ke, Sergey Prokudin, Marc Pollefeys, Siyu Tang)</author>
      <guid isPermaLink="false">2508.21060v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2508.20835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次探索了RWKV模型在点云分类领域泛化问题中的应用，提出了PointDGRWKV框架，解决了RWKV直接应用于非结构化点云时的空间失真和跨域注意力漂移问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;领域泛化(DG)被用于增强点云分类(PCC)模型在未见域上的泛化能力。现有基于卷积网络、Transformer或Mamba架构的方法存在感受野有限、计算成本高或长距离依赖建模不足的问题。&lt;h4&gt;目的&lt;/h4&gt;研究RWKV模型在DG PCC中的泛化能力，解决直接应用RWKV时面临的空间失真和跨域注意力漂移问题，提高点云分类模型在跨域场景下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出PointDGRWKV框架，包含两个关键模块：自适应几何Token移位(用于建模局部邻域结构，提高几何上下文感知)和跨域键特征分布对齐(通过对齐跨域键特征分布来减轻注意力漂移)，同时保持RWKV的线性效率。&lt;h4&gt;主要发现&lt;/h4&gt;直接应用RWKV到DG PCC面临两个挑战：固定方向的token移位方法在应用于非结构化点云时会导致空间失真，削弱局部几何建模；Bi-WKV注意力通过指数加权放大跨域分布的细微差异，导致注意力偏移和泛化能力下降。&lt;h4&gt;结论&lt;/h4&gt;PointDGRWKV框架通过引入自适应几何Token移位和跨域键特征分布对齐两个模块，有效解决了RWKV在点云分类领域泛化问题中的应用挑战，在多个基准测试上实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;领域泛化(DG)最近被探索用于增强点云分类(PCC)模型在未见域上的泛化能力。先前的工作基于卷积网络、Transformer或Mamba架构，要么受到感受野有限的限制，要么计算成本高，或者长距离依赖建模不足。RWKV作为一种新兴架构，具有优越的线性复杂度、全局感受野和长距离依赖能力。在本文中，我们首次研究了RWKV模型在DG PCC中的泛化能力。我们发现，直接将RWKV应用于DG PCC面临两个重大挑战：RWKV的固定方向token移位方法(如Q-Shift)在应用于非结构化点云时引入空间失真，削弱了局部几何建模并降低了鲁棒性。此外，RWKV中的Bi-WKV注意力通过指数加权放大了键分布中的轻微跨域差异，导致注意力偏移和泛化能力下降。为此，我们提出了PointDGRWKV，这是第一个为DG PCC量身定制的基于RWKV的框架。它引入了两个关键模块来增强空间建模和跨域鲁棒性，同时保持RWKV的线性效率。特别是，我们提出了自适应几何Token移位来建模局部邻域结构，以提高几何上下文感知能力。此外，设计了跨域键特征分布对齐，通过对齐跨域键特征分布来减轻注意力漂移。在多个基准上的广泛实验表明，PointDGRWKV在DG PCC上实现了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分类中的领域泛化问题，即当模型从训练好的领域（如合成数据）应用到未见过的领域（如真实场景）时性能显著下降的问题。这个问题在现实中非常重要，因为自动驾驶、机器人、增强现实等应用需要模型适应各种传感器、环境和扫描角度变化带来的数据分布差异，而传统方法要么感受野有限（CNN），要么计算成本高（Transformer），要么难以捕获长距离依赖（Mamba）。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云分类方法的局限性，发现RWKV架构具有线性复杂度、全局感受野和长距离依赖建模能力，但直接应用于点云领域泛化存在两个主要问题：固定方向的token shift导致空间扭曲，以及Bi-WKV注意力机制中的指数加权放大跨领域差异。基于这些观察，作者设计了两个关键模块：自适应几何Token Shift基于空间分区构建局部邻域，借鉴了点云处理中的空间分区思想；跨领域关键特征分布对齐借鉴了领域适应中的分布对齐思想，但专门针对RWKV的注意力机制进行了优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用RWKV架构的优势，同时解决其在非结构化点云上的空间建模和跨领域泛化问题。整体流程包括：输入点云数据→自适应几何Token Shift（通过空间分区和加权特征聚合增强局部几何结构）→空间混合和通道混合→Bi-WKV注意力→跨领域关键特征分布对齐（对齐不同领域关键特征的均值和协方差）→多层堆叠→池化和分类。这种方法既保持了RWKV的线性计算效率，又增强了其在点云数据上的几何感知能力和跨领域鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出PointDGRWKV，首个专门用于点云分类领域泛化的RWKV框架；2) 自适应几何Token Shift模块，通过空间分区而非固定方向进行token shift，计算复杂度为O(N)；3) 跨领域关键特征分布对齐模块，减轻注意力漂移。相比之前的工作，这种方法具有更大的感受野（相比CNN）、更低的计算复杂度（O(L)而非Transformer的O(L²)）、更好的长距离依赖建模能力（相比Mamba），并且解决了RWKV在点云上的空间扭曲和跨领域注意力漂移问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointDGRWKV通过自适应几何Token Shift和跨领域关键特征分布对齐两个创新模块，首次将RWKV架构有效应用于点云分类的领域泛化任务，在保持线性计算效率的同时显著提升了模型在未见领域的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain Generalization (DG) has been recently explored to enhance thegeneralizability of Point Cloud Classification (PCC) models toward unseendomains. Prior works are based on convolutional networks, Transformer or Mambaarchitectures, either suffering from limited receptive fields or highcomputational cost, or insufficient long-range dependency modeling. RWKV, as anemerging architecture, possesses superior linear complexity, global receptivefields, and long-range dependency. In this paper, we present the first workthat studies the generalizability of RWKV models in DG PCC. We find thatdirectly applying RWKV to DG PCC encounters two significant challenges: RWKV'sfixed direction token shift methods, like Q-Shift, introduce spatialdistortions when applied to unstructured point clouds, weakening localgeometric modeling and reducing robustness. In addition, the Bi-WKV attentionin RWKV amplifies slight cross-domain differences in key distributions throughexponential weighting, leading to attention shifts and degraded generalization.To this end, we propose PointDGRWKV, the first RWKV-based framework tailoredfor DG PCC. It introduces two key modules to enhance spatial modeling andcross-domain robustness, while maintaining RWKV's linear efficiency. Inparticular, we present Adaptive Geometric Token Shift to model localneighborhood structures to improve geometric context awareness. In addition,Cross-Domain key feature Distribution Alignment is designed to mitigateattention drift by aligning key feature distributions across domains. Extensiveexperiments on multiple benchmarks demonstrate that PointDGRWKV achievesstate-of-the-art performance on DG PCC.</description>
      <author>example@mail.com (Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Shuicheng Yan)</author>
      <guid isPermaLink="false">2508.20835v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Surfel-based 3D Registration with Equivariant SE(3) Features</title>
      <link>http://arxiv.org/abs/2508.20789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于surfel的姿态学习回归方法，用于解决点云配准中忽略点方向性和不确定性的问题。该方法通过SE(3)等变卷积核学习显式的位置和旋转特征，预测源扫描和目标扫描之间的相对变换。实验结果表明，与最先进方法相比，该方法在真实点云扫描上表现出优越性和鲁棒性能。&lt;h4&gt;背景&lt;/h4&gt;点云配准对于确保遥感或数字遗产3D重建中多个局部点云的3D对齐一致性至关重要。现有的点云配准方法（包括非学习和基于学习的方法）都忽略了点的方向性和不确定性，导致模型对噪声输入和输入点云的剧烈旋转敏感，需要大量带变换增强的训练点云。&lt;h4&gt;目的&lt;/h4&gt;解决现有点云配准方法忽略点方向性和不确定性的问题，提高模型对噪声输入和剧烈旋转的鲁棒性，减少对大量训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于surfel的姿态学习回归方法，使用虚拟透视相机参数从Lidar点云初始化surfel，通过SE(3)等变卷积核学习显式的位置和旋转特征，预测源扫描和目标扫描之间的相对变换。模型由等变卷积编码器、交叉注意力机制、全连接解码器和非线性Huber损失组成。&lt;h4&gt;主要发现&lt;/h4&gt;在室内和室外数据集上的实验结果表明，与最先进的方法相比，所提模型在真实点云扫描上表现出优越性和鲁棒性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于surfel的姿态学习回归方法在处理真实点云数据时优于现有方法，具有更好的鲁棒性，减少了对大量训练数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;点云配准对于确保遥感或数字遗产3D重建中多个局部点云的3D对齐一致性至关重要。虽然存在各种基于点云的配准方法，无论是非学习还是基于学习的方法，它们都忽略了点的方向性和不确定性，使得模型容易受到噪声输入和输入点云的剧烈旋转（如正交变换）的影响；因此，需要大量带变换增强的训练点云。为了解决这些问题，我们提出了一种新颖的基于surfel的姿态学习回归方法。我们的方法可以使用虚拟透视相机参数从Lidar点云初始化surfel，并通过SE(3)等变卷积核学习显式的SE(3)等变特征，包括位置和旋转，以预测源扫描和目标扫描之间的相对变换。该模型包括一个等变卷积编码器、一个用于相似性计算的交叉注意力机制、一个全连接解码器和一个非线性Huber损失。在室内和室外数据集上的实验结果表明，与最先进的方法相比，我们的模型在真实点云扫描上表现出优越性和鲁棒性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云配准中对噪声输入和剧烈旋转敏感的问题。在3D重建、形状姿态估计、遥感和数字孪生等领域中，确保多个局部点云之间的3D对齐一致性至关重要。现有方法忽略了点的方向和不确定性信息，导致在复杂环境中性能下降，限制了它们在实际应用中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是它们忽略了点的方向和不确定性。他们借鉴了surfels（表面元素）表示方法，最初由Dahl等人提出，并在3D图形渲染中应用。同时，作者参考了equivariant特征表示的工作（如Spherical CNNs、SE(3)-Transformers）以及SpinNet等利用旋转不变性特征的方法。基于这些现有工作，作者设计了专门的SE(3)等变卷积核编码器、交叉注意力机制和特定的Huber损失函数，构建了一个完整的surfels-based配准框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用surfels（包含位置、法向量和不确定性半径）代替传统点云，并学习SE(3)等变特征来处理3D空间中的刚性变换。整体流程包括：1) Surfel初始化：从深度图或LiDAR点云创建surfels，计算位置、法向量和不确定性；2) 特征提取：使用SE(3)等变卷积核编码器学习等变特征；3) 相似性计算：通过交叉注意力机制计算源帧和目标帧之间的相似性；4) 变换预测：使用全连接层解码器预测相对位置和旋转；5) 模型优化：采用特定的SE(3)可微分Huber损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入surfels表示法，包含位置、法向量和不确定性信息；2) 设计SE(3)等变卷积核编码器，同时学习位置和旋转的等变特征；3) 使用交叉注意力机制计算帧间相似性；4) 根据不确定性对特征进行加权；5) 设计特定的Huber损失函数。相比之前的工作，传统方法如ICP收敛慢且对异常值敏感；现有深度学习方法如DGR和PointDSC没有考虑点的方向和不确定性；之前的等变方法主要处理SO(2)或SO(3)等变性而非完整的SE(3)等变性。本文方法通过surfels表示和SE(3)等变特征学习，在噪声和剧烈旋转情况下表现出更好的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于surfels的SE(3)等变特征学习方法，通过结合点的方向、位置和不确定性信息，显著提高了3D点云配准在噪声环境和剧烈旋转情况下的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is crucial for ensuring 3D alignment consistency ofmultiple local point clouds in 3D reconstruction for remote sensing or digitalheritage. While various point cloud-based registration methods exist, bothnon-learning and learning-based, they ignore point orientations and pointuncertainties, making the model susceptible to noisy input and aggressiverotations of the input point cloud like orthogonal transformation; thus, itnecessitates extensive training point clouds with transformation augmentations.To address these issues, we propose a novel surfel-based pose learningregression approach. Our method can initialize surfels from Lidar point cloudusing virtual perspective camera parameters, and learns explicit$\mathbf{SE(3)}$ equivariant features, including both position and rotationthrough $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relativetransformation between source and target scans. The model comprises anequivariant convolutional encoder, a cross-attention mechanism for similaritycomputation, a fully-connected decoder, and a non-linear Huber loss.Experimental results on indoor and outdoor datasets demonstrate our modelsuperiority and robust performance on real point-cloud scans compared tostate-of-the-art methods.</description>
      <author>example@mail.com (Xueyang Kang, Hang Zhao, Kourosh Khoshelham, Patrick Vandewalle)</author>
      <guid isPermaLink="false">2508.20789v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2508.20758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SeqVLM的新型零样本3D视觉定位框架，通过多视图图像和空间信息解决现有方法的空间有限推理和上下文忽略问题，在基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;3D视觉定位旨在使用自然语言描述在3D场景中定位物体。虽然监督方法在受限环境中能实现更高准确性，但零样本3DVG对实际应用更有前景，因为它消除了场景特定训练需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本3DVG方法面临的空间有限推理和上下文忽略或细节退化问题，提出一种能够利用多视图真实场景图像进行目标物体推理的新框架。&lt;h4&gt;方法&lt;/h4&gt;SeqVLM框架首先通过3D语义分割网络生成3D实例提议，并通过语义过滤保留相关候选对象；然后使用提议引导的多视图投影策略将候选提议投影到真实场景图像序列，保持空间关系和上下文细节；最后实现动态调度机制处理序列查询提示，利用VLM的跨模态推理能力识别文本指定的对象。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanRefer和Nr3D基准测试上实现了最先进的性能，Acc@0.25分数分别达到55.6%和53.2%，超越了之前的零样本方法4.0%和5.2%。&lt;h4&gt;结论&lt;/h4&gt;SeqVLM框架有效解决了现有零样本3DVG方法的挑战，通过多视图图像和空间信息的利用，提高了定位准确性，推进了3DVG向更泛化和实际应用方向发展。&lt;h4&gt;翻译&lt;/h4&gt;三维视觉定位旨在使用自然语言描述在三维场景中定位物体。尽管监督方法在受限环境中能够实现更高的准确性，但零样本三维视觉定位对实际应用具有更大前景，因为它消除了场景特定训练要求。然而，现有零样本方法面临空间有限推理的挑战，因为它们依赖单视图定位，并且存在上下文忽略或细节退化的问题。为解决这些问题，我们提出了SeqVLM，一种新颖的零样本三维视觉定位框架，它利用具有空间信息的多视图真实场景图像进行目标物体推理。具体来说，SeqVLM首先通过三维语义分割网络生成三维实例提议，并通过语义过滤进行优化，只保留语义相关的候选对象。然后，提议引导的多视图投影策略将这些候选提议投影到真实场景图像序列上，在三维点云到图像的转换过程中保持空间关系和上下文细节。此外，为了减轻VLM计算过载，我们实现了一个动态调度机制，迭代处理序列查询提示，利用VLM的跨模态推理能力识别文本指定的对象。在ScanRefer和Nr3D基准测试上的实验证明了最先进的性能，实现了55.6%和53.2%的Acc@0.25分数，分别超过了之前的零样本方法4.0%和5.2%，这推进了三维视觉定位向更大的泛化和实际应用能力发展。代码可在https://github.com/JiawLin/SeqVLM获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D视觉定位(Zero-Shot 3D Visual Grounding)中的空间推理受限和上下文缺失问题。这个问题在现实中很重要，因为它能帮助系统在没有特定场景训练的情况下，通过自然语言描述在3D场景中精确定位目标对象，这对于智能人机交互、自动驾驶环境感知和AR/VR系统等应用场景至关重要。与需要大量标注数据的监督方法相比，零样本方法更具实用性和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有零样本3D视觉定位方法的局限性，特别是单视图渲染带来的几何约束缺失和空间偏差问题。他们借鉴了LLM-based方法和VLM-based方法的优势，同时克服了它们的不足。作者设计了SeqVLM框架，整合3D点云、多视图图像和自然语言描述，通过提案选择、多视图投影和迭代推理三个核心模块，解决了现有方法的空间推理受限和上下文缺失问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过提案引导的多视图投影策略，将3D点云中的候选对象投影到真实场景图像序列中，保留空间关系和上下文细节，然后利用视觉语言模型进行跨模态推理。整体流程分为三步：1)提案选择模块：使用3D语义分割提取对象提案，通过语义过滤保留与目标相关的候选；2)提案引导的多视图投影模块：将候选提案投影到多视图图像上，选择最佳视角并拼接成图像序列；3)VLM迭代推理模块：通过动态批处理机制逐步缩小搜索空间，最终确定目标对象。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提案引导的多视图投影策略，有效保持3D几何属性和环境上下文；2)利用多视图真实图像增强空间理解和上下文感知；3)迭代推理机制解决VLM在多候选场景中的失效问题。相比之前的工作，SeqVLM克服了VLM-Grounder缺乏几何一致性和SeeGround依赖单视图渲染的局限，解决了渲染图像与真实世界图像之间的差距问题，显著提升了定位精度和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SeqVLM通过创新的提案引导多视图投影和迭代推理机制，实现了无需场景特定训练的高精度零样本3D视觉定位，其性能已能媲美监督学习方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes usingnatural language descriptions. Although supervised methods achieve higheraccuracy in constrained settings, zero-shot 3DVG holds greater promise forreal-world applications since eliminating scene-specific training requirements.However, existing zero-shot methods face challenges of spatial-limitedreasoning due to reliance on single-view localization, and contextual omissionsor detail degradation. To address these issues, we propose SeqVLM, a novelzero-shot 3DVG framework that leverages multi-view real-world scene images withspatial information for target object reasoning. Specifically, SeqVLM firstgenerates 3D instance proposals via a 3D semantic segmentation network andrefines them through semantic filtering, retaining only semantic-relevantcandidates. A proposal-guided multi-view projection strategy then projectsthese candidate proposals onto real scene image sequences, preserving spatialrelationships and contextual details in the conversion process of 3D pointcloud to images. Furthermore, to mitigate VLM computational overload, weimplement a dynamic scheduling mechanism that iteratively processessequances-query prompts, leveraging VLM's cross-modal reasoning capabilities toidentify textually specified objects. Experiments on the ScanRefer and Nr3Dbenchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scoresof 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,respectively, which advance 3DVG toward greater generalization and real-worldapplicability. The code is available at https://github.com/JiawLin/SeqVLM.</description>
      <author>example@mail.com (Jiawen Lin, Shiran Bian, Yihang Zhu, Wenbin Tan, Yachao Zhang, Yuan Xie, Yanyun Qu)</author>
      <guid isPermaLink="false">2508.20758v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>AdaDPCC: Adaptive Rate Control and Rate-Distortion-Complexity Optimization for Dynamic Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2508.20741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的动态点云压缩框架，通过可调节的编码结构和精细的运动估计补偿模块，实现了高效的率-失真-复杂度优化，在保持低比特率误差的同时显著提高了压缩效率和实时性能。&lt;h4&gt;背景&lt;/h4&gt;动态点云压缩在自动驾驶和AR/VR等应用中至关重要。然而，当前压缩方法在复杂度管理和码率控制方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种支持可变比特率和计算复杂度的动态编码框架，实现高效的率-失真-复杂度优化，解决数据稀疏性问题，并实现精确的码率控制。&lt;h4&gt;方法&lt;/h4&gt;提出了一种具有多种编码路径的精简框架，在单一模型内实现高效的率-失真-复杂度优化；设计了从粗到细的运动估计和补偿模块，分解几何信息同时扩展感知场；提出了精确的码率控制模块，内容自适应地将点云帧导航到各种编码路径以满足目标比特率。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进方法相比，平均BD-Rate降低了5.81%；BD-PSNR提高了0.42 dB；平均比特率误差保持在0.40%；与D-DPCC相比，平均编码时间减少了高达44.6%。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在实时和比特率受限的动态点云压缩场景中表现出色，显著提高了压缩效率和性能，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;动态点云压缩在自动驾驶和AR/VR等应用中至关重要。当前的压缩方法在复杂度管理和码率控制方面面临挑战。本文介绍了一种新的动态编码框架，支持可变比特率和计算复杂度。我们的方法包括一个具有多种编码路径的精简框架，允许在单一模型内实现高效的率-失真-复杂度优化。为解决帧间预测中的数据稀疏性问题，我们提出了从粗到细的运动估计和补偿模块，该模块在分解几何信息的同时扩展感知场。此外，我们提出了一个精确的码率控制模块，内容自适应地将点云帧导航通过各种编码路径以满足目标比特率。实验结果表明，与最先进的方法相比，我们的方法平均降低了5.81%的BD-Rate，提高了0.42 dB的BD-PSNR，同时将平均比特率误差保持在0.40%。此外，与D-DPCC相比，平均编码时间减少了高达44.6%，凸显了其在实时和比特率受限的动态点云压缩场景中的效率。我们的代码可在https://git.openi.org.cn/OpenPointCloud/Ada_DPCC获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态点云压缩中的两个关键问题：复杂度管理和码率控制问题。这些问题在现实中非常重要，因为动态点云技术在自动驾驶、AR/VR等领域有广泛应用，这些应用需要实时渲染大量3D数据，对数据存储和传输提出了巨大挑战。一个灵活且高效的压缩算法对于在带宽和计算资源受限的条件下提供高质量体验至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过批判性分析现有方法的局限性来设计新方法：现有方法在特定码率场景下缺乏灵活性，帧间预测受限于点稀疏性和KNN方法的计算复杂度，且缺乏精确码率控制。作者借鉴了动态神经网络和slimmable auto-encoder的概念，参考了patch-based方法但进行了改进。设计思路包括创建动态编码框架支持可变码率和复杂度，提出粗到细的运动估计解决数据稀疏性问题，以及设计精确码率控制模块实现内容自适应的编码路线选择。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 使用多个编码路线实现不同复杂度和率失真权衡；2) 通过粗到细的帧间预测扩展感知场；3) 内容自适应地评估每条路线码率并选择最佳路线。整体流程：码率控制模块先估计各路线码率并选择最佳路线；当前帧通过选定路线编码为潜在表示，同时参考帧也通过相同路线编码；应用粗到细运动估计和补偿；使用补偿后的潜在变量和超先验进行条件编码；最后通过相同路线解码重建帧并缓冲用于后续预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 动态编码框架实现单个模型内的率-失真-复杂度优化；2) 粗到细帧间预测解决数据稀疏性问题；3) 精确码率控制模块实现内容自适应路线选择。相比之前工作：不同于传统方法固定率失真权衡，支持可变码率和复杂度；相比基于学习的方法，解决了KNN方法计算量大、感知场受限的问题；相比通用动态神经网络，专门针对点云压缩优化，结合超先验和时序上下文，设计更高效码率控制机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AdaDPCC通过动态编码框架、粗到细帧间预测和精确码率控制，实现了在动态点云压缩中灵活调整码率和计算复杂度的同时保持高质量重建，为实时和带宽受限的DPCC应用提供了高效解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic point cloud compression (DPCC) is crucial in applications likeautonomous driving and AR/VR. Current compression methods face challenges withcomplexity management and rate control. This paper introduces a novel dynamiccoding framework that supports variable bitrate and computational complexities.Our approach includes a slimmable framework with multiple coding routes,allowing for efficient Rate-Distortion-Complexity Optimization (RDCO) within asingle model. To address data sparsity in inter-frame prediction, we proposethe coarse-to-fine motion estimation and compensation module that deconstructsgeometric information while expanding the perceptive field. Additionally, wepropose a precise rate control module that content-adaptively navigates pointcloud frames through various coding routes to meet target bitrates. Theexperimental results demonstrate that our approach reduces the average BD-Rateby 5.81% and improves the BD-PSNR by 0.42 dB compared to the state-of-the-artmethod, while keeping the average bitrate error at 0.40%. Moreover, the averagecoding time is reduced by up to 44.6% compared to D-DPCC, underscoring itsefficiency in real-time and bitrate-constrained DPCC scenarios. Our code isavailable at https://git.openi.org.cn/OpenPointCloud/Ada_DPCC.</description>
      <author>example@mail.com (Chenhao Zhang, Wei Gao)</author>
      <guid isPermaLink="false">2508.20741v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.20492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了IAENet（重要性感知集成网络），一种用于工业制造表面异常检测的集成框架，通过结合2D和3D专家模型并使用重要性感知融合模块，实现了更高的检测精度和更低的假阳性率。&lt;h4&gt;背景&lt;/h4&gt;表面异常检测对确保工业产品质量至关重要。虽然2D图像方法已取得显著成功，但3D点云方法虽有更丰富的几何线索，却研究不足，主要原因是缺乏与2D领域相当的预训练基础骨干网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效融合2D和3D信息的表面异常检测方法，克服现有方法中不同模态融合的困难，提高检测精度并降低假阳性率。&lt;h4&gt;方法&lt;/h4&gt;提出IAENet（重要性感知集成网络），这是一个集成框架，结合2D预训练专家模型和3D专家模型。引入了重要性感知融合（IAF）模块，动态评估各来源的贡献并重新加权异常分数。同时设计了关键损失函数指导IAF优化，结合专家的集体知识同时保留各自优势。&lt;h4&gt;主要发现&lt;/h4&gt;在MVTec 3D-AD数据集上的实验表明，IAENet实现了新的最先进水平，具有明显较低的假阳性率，突显了其在工业部署中的实际价值。&lt;h4&gt;结论&lt;/h4&gt;IAENet通过有效融合2D和3D信息，解决了3D点云异常检测中的预训练骨干网络缺失问题，并通过重要性感知融合机制提高了检测性能，为工业制造中的表面异常检测提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;表面异常检测对于确保工业制造中的产品质量至关重要。虽然基于2D图像的方法已经取得了显著成功，但基于3D点云的检测尽管拥有更丰富的几何线索，却仍未得到充分探索。我们认为，关键瓶颈是缺乏与2D领域相媲美的强大预训练基础骨干网络在3D领域。为了弥合这一差距，我们提出了重要性感知集成网络（IAENet），这是一个协同2D预训练专家与3D专家模型的集成框架。然而，简单融合来自不同来源的预测并非易事：现有策略可能受到表现不佳模态的影响，从而降低整体准确性。为了应对这一挑战，我们引入了一种新颖的重要性感知融合（IAF）模块，该模块动态评估每个来源的贡献并重新加权它们的异常分数。此外，我们设计了关键的损失函数，明确指导IAF的优化，使其不仅能够结合源专家的集体知识，还能保留它们的独特优势，从而增强异常检测的整体性能。在MVTec 3D-AD上的大量实验表明，我们的IAENet实现了新的最先进水平，并且具有明显较低的假阳性率，突显了其在工业部署中的实际价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云异常检测中的两大挑战：一是缺乏强大的3D预训练基础模型（类似于2D图像领域的预训练模型），二是简单融合不同模态预测结果的方法无法适应不同场景下各模型贡献度的变化。这些问题在工业制造中至关重要，因为表面异常检测是确保产品质量的关键环节，而现有的方法往往存在较高的误报率，限制了它们在实际工业环境中的应用。3D点云包含丰富的几何信息，能比2D图像更准确地捕捉物体的真实形状和细节，因此提高3D点云异常检测的性能对实现高效、自动化的产品质量检测具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D点云异常检测的瓶颈，特别是缺乏强大的预训练模型和简单融合策略的局限性。他们借鉴了多个现有工作：采用PointNet-SDF作为3D专家模型，借鉴了2D图像领域的预训练基础模型（如ResNet），参考了PatchCore的记忆库方法但改进为形状引导的双存储库，并吸收了知识蒸馏和记忆库方法的思想。基于这些借鉴，作者创新性地设计了重要性感知融合(IAF)模块，能够动态评估每个源模型的贡献，并通过专门的损失函数来指导IAF模块的学习，从而有效结合2D和3D模型的互补优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个重要性感知融合(IAF)模块，智能地结合2D预训练模型和3D专家模型的预测结果。2D模型擅长提取语义信息和检测细微异常，但可能在正常区域给出较高分数；3D模型擅长捕捉全局几何信息，但可能难以检测微小异常。IAF模块能够动态评估每个模型在不同场景下的可靠性，并相应地加权它们的预测结果。整体流程分为两个阶段：首先是源专家学习阶段，训练2D和3D专家模型并构建双存储库；其次是重要性感知融合阶段，使用选择器网络评估专家模型的贡献，预测器网络生成最终异常分数。训练时使用合成的异常数据指导IAF模块的学习，推理时通过IAF融合两个专家的预测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 重要性感知融合(IAF)模块，能动态评估不同模型在不同场景下的贡献；2) 双专家模型架构，结合2D预训练模型和3D专家模型的优势；3) 专门的损失函数，包括选择器损失和预测器损失，特别指导IAF模块的学习；4) 形状引导的双存储库，改进了传统的存储库构建方法。相比之前的工作，IAENet的融合策略不再是简单的最大值选择或相加，而是能自适应调整各模型的贡献度；模型架构明确区分并利用2D和3D模型的互补优势；训练方法使用合成的异常数据和专门的损失函数来指导融合模块的学习；在降低误报率方面表现突出，具有更高的实用价值。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种名为IAENet的新型集成模型，通过重要性感知融合机制智能地结合2D预训练模型和3D专家模型的预测，显著提高了3D点云异常检测的准确性和鲁棒性，同时大幅降低了误报率，为工业制造中的表面质量检测提供了更可靠的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surface anomaly detection is pivotal for ensuring product quality inindustrial manufacturing. While 2D image-based methods have achieved remarkablesuccess, 3D point cloud-based detection remains underexplored despite itsricher geometric cues. We argue that the key bottleneck is the absence ofpowerful pretrained foundation backbones in 3D comparable to those in 2D. Tobridge this gap, we propose Importance-Aware Ensemble Network (IAENet), anensemble framework that synergizes 2D pretrained expert with 3D expert models.However, naively fusing predictions from disparate sources is non-trivial:existing strategies can be affected by a poorly performing modality and thusdegrade overall accuracy. To address this challenge, We introduce an novelImportance-Aware Fusion (IAF) module that dynamically assesses the contributionof each source and reweights their anomaly scores. Furthermore, we devisecritical loss functions that explicitly guide the optimization of IAF, enablingit to combine the collective knowledge of the source experts but also preservetheir unique strengths, thereby enhancing the overall performance of anomalydetection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENetachieves a new state-of-the-art with a markedly lower false positive rate,underscoring its practical value for industrial deployment.</description>
      <author>example@mail.com (Xuanming Cao, Chengyu Tao, Yifeng Cheng, Juan Du)</author>
      <guid isPermaLink="false">2508.20492v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2508.20466v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于紧凑特征的LiDAR点云高效压缩方法，通过两个轻量级模块实现高性能实时编解码。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云是多种应用的基础，但高精度扫描会产生巨大的存储和传输开销。现有方法通常将无序点转换为分层八叉树或体素结构进行从密集到稀疏的预测编码。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中几何细节极度稀疏导致的上下文建模效率低、压缩性能和速度受限的问题，生成紧凑特征以实现高效的预测编码。&lt;h4&gt;方法&lt;/h4&gt;提出包含两个轻量级模块的框架：1)几何再密集化模块：重新密集化编码的稀疏几何，在更密集尺度提取特征后重新稀疏化，避免高成本计算；2)跨尺度特征传播模块：利用多分辨率占用线索引导分层特征传播，促进跨尺度信息共享，减少冗余特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合两个模块，产生了紧凑的特征表示，提供高效的上下文建模并加速编码过程。在KITTI数据集上实现了最先进的压缩比和实时性能，12位量化下编解码速度达26 FPS。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了LiDAR点云压缩中的挑战，在保持高压缩比的同时实现了实时性能。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR点云是各种应用的基础，然而高精度扫描会产生巨大的存储和传输开销。现有方法通常将无序点转换为分层八叉树或体素结构进行从密集到稀疏的预测编码。然而，几何细节的极度稀疏阻碍了有效的上下文建模，从而限制了它们的压缩性能和速度。为了应对这一挑战，我们提出生成紧凑特征以实现高效的预测编码。我们的框架包含两个轻量级模块。首先，几何再密集化模块重新密集化编码的稀疏几何，在更密集的尺度提取特征，然后重新稀疏化特征以进行预测编码。该模块避免了在高度稀疏的细节上进行高成本计算，同时保持轻量级预测头。其次，跨尺度特征传播模块利用多个分辨率级别的占用线索来引导分层特征传播。这种设计促进了跨尺度信息共享，从而减少了冗余特征提取，并为几何再密集化模块提供了丰富的特征。通过整合这两个模块，我们的方法产生了紧凑的特征表示，提供了高效的上下文建模并加速了编码过程。在KITTI数据集上的实验展示了最先进的压缩比和实时性能，在12位量化下实现了每秒26帧的编解码速度。代码可在https://github.com/pengpeng-yu/FastPCC获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决LiDAR点云在高精度压缩场景下的效率问题。随着自动驾驶和地图等领域的发展，3D感知技术产生大量点云数据，导致存储和传输开销巨大。现有方法在高分辨率下面临'高分辨率上下文稀疏性'(HRCS)问题，即几何细节的极端稀疏性阻碍了有效的上下文建模，限制了压缩性能和速度。这个问题对自动驾驶、机器人导航等实时应用至关重要，因为高效压缩可以减少数据传输带宽和存储需求，同时保持高精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云压缩方法的局限性，特别是体素和八叉树表示在高分辨率下的固有缺陷。他们通过数据统计发现了HRCS现象，即随着分辨率提高，局部邻域的上下文信息急剧减少。基于这一洞察，作者设计了两个互补模块：几何再密集化模块(GRED)和跨尺度特征传播模块(XFP)。这些方法借鉴了现有的基于八叉树和体素的压缩框架，但针对HRCS问题提出了创新解决方案，通过临时密集化和跨尺度信息共享来克服传统方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'几何再密集化'解决高分辨率上下文稀疏问题，并利用'跨尺度特征传播'实现不同分辨率级别间的信息共享。整体流程包括：1)将点云转换为八叉树结构；2)构建先验信息；3)跨尺度特征传播，其中浅层(l≤t)使用简化GRED模块跳过再密集化，深层(l&gt;t)应用完整GRED模块并融合多尺度特征；4)基于预测的占用分布进行熵编码。GRED模块具体包括：再密集化(将稀疏几何转换为密集表示)、特征提取(应用3D卷积)、重新稀疏化(返回原始稀疏空间)和预测编码(估计占用分布)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出并系统分析了'高分辨率上下文稀疏性'(HRCS)现象；2)几何再密集化模块(GRED)通过临时密集化稀疏几何来增强上下文建模；3)跨尺度特征传播模块(XFP)利用多分辨率占用线索指导特征传播。相比之前的工作，本文方法在保持高压缩性能的同时显著提高了计算效率，实现了实时处理(26 FPS)。相比基于Transformer的方法(如OctAttention、Light EHEM)，本文方法大幅减少了计算复杂度；相比最近的体素方法(如Unicorn)，在高比特率下表现更好；相比效率导向的方法(如RENO)，在保持实时性的同时显著提高了压缩质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的几何再密集化和跨尺度特征传播两个创新模块，有效解决了LiDAR点云高分辨率压缩中的上下文稀疏问题，实现了兼具高压缩比和实时性的高效点云压缩方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point clouds are fundamental to various applications, yethigh-precision scans incur substantial storage and transmission overhead.Existing methods typically convert unordered points into hierarchical octree orvoxel structures for dense-to-sparse predictive coding. However, the extremesparsity of geometric details hinders efficient context modeling, therebylimiting their compression performance and speed. To address this challenge, wepropose to generate compact features for efficient predictive coding. Ourframework comprises two lightweight modules. First, the GeometryRe-Densification Module re-densifies encoded sparse geometry, extracts featuresat denser scale, and then re-sparsifies the features for predictive coding.This module avoids costly computation on highly sparse details whilemaintaining a lightweight prediction head. Second, the Cross-scale FeaturePropagation Module leverages occupancy cues from multiple resolution levels toguide hierarchical feature propagation. This design facilitates informationsharing across scales, thereby reducing redundant feature extraction andproviding enriched features for the Geometry Re-Densification Module. Byintegrating these two modules, our method yields a compact featurerepresentation that provides efficient context modeling and accelerates thecoding process. Experiments on the KITTI dataset demonstrate state-of-the-artcompression ratios and real-time performance, achieving 26 FPS for bothencoding and decoding at 12-bit quantization. Code is available athttps://github.com/pengpeng-yu/FastPCC.</description>
      <author>example@mail.com (Pengpeng Yu, Haoran Li, Dingquan Li, Runqing Jiang, Jing Wang, Liang Lin, Yulan Guo)</author>
      <guid isPermaLink="false">2508.20466v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Learning Fast, Tool aware Collision Avoidance for Collaborative Robots</title>
      <link>http://arxiv.org/abs/2508.20457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一种工具感知的碰撞避免系统，能根据不同工具尺寸和交互模式实时调整。系统使用学习模型处理点云数据，预测碰撞，并通过强化学习训练的控制策略快速生成避障动作。该方法在动态环境中表现优异，精度高且计算成本低。&lt;h4&gt;背景&lt;/h4&gt;在人类环境中确保协作机器人的安全高效运行具有挑战性，特别是在障碍物和任务随时间变化的动态环境中。当前机器人控制器通常假设完全可见性和固定工具，可能导致碰撞或过度保守行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应不同工具尺寸和工具-环境交互模式的碰撞避免系统，以解决动态环境中机器人的安全问题。&lt;h4&gt;方法&lt;/h4&gt;使用学习到的感知模型过滤点云中的机器人和工具组件，推理遮挡区域并在部分可观察情况下预测碰撞；通过约束强化学习训练控制策略，在10毫秒内生成平滑避障动作；在模拟和真实环境中进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;在动态环境中优于传统方法(APF、MPPI)；保持亚毫米级精度；计算成本比最先进的基于GPU的规划器低约60%；系统模块化、高效且有效。&lt;h4&gt;结论&lt;/h4&gt;该方法为在动态环境中运行的机器人提供了模块化、高效且有效的碰撞避免解决方案，已集成到协作机器人应用中并展示了实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在人类环境中确保协作机器人的安全高效运行具有挑战性，特别是在障碍物和任务随时间变化的动态环境中。当前的机器人控制器通常假设完全可见性和固定工具，这可能导致碰撞或过度保守的行为。在我们的工作中，我们引入了一种工具感知的碰撞避免系统，该系统能根据不同的工具尺寸和工具-环境交互模式实时调整。使用学习到的感知模型，我们的系统从点云中过滤掉机器人和工具组件，推理遮挡区域，并在部分可观察情况下预测碰撞。然后，我们使用通过约束强化学习训练的控制策略，在10毫秒内产生平滑的避障动作。在模拟和真实世界测试中，我们的方法在动态环境中优于传统方法(APF、MPPI)，同时保持亚毫米级精度。此外，与最先进的基于GPU的规划器相比，我们的系统计算成本降低了约60%。我们的方法为在动态环境中运行的机器人提供了模块化、高效且有效的碰撞避免。我们将我们的方法集成到协作机器人应用中，并展示了其在安全响应操作中的实际应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决协作机器人在动态环境中进行快速、工具感知的碰撞避免问题。这个问题很重要，因为随着协作机器人越来越多地应用于工业和家庭环境，确保它们与人类共享空间时的安全变得至关重要。现有方法往往依赖固定参数，无法适应动态环境变化和不同工具，在遮挡等情况下表现不佳，且计算成本高，难以实现实时性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先定义了三个关键需求：工具设置的灵活性、操作精确性以及响应性和可靠性。然后设计了一个混合系统，结合学习方法鲁棒性和经典控制器精确性。系统借鉴了Srinivasan等人的安全监督器方法、Hoeller等人的3D CNN架构以及Kim等人的约束马尔可夫决策过程框架。创新之处在于引入了工具感知设计（Engage和Protective两种模式）、能够过滤机器人和工具组件的点云处理模型，以及结合强化学习和经典逆运动学的混合控制策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是工具感知的碰撞避免，通过混合控制架构平衡精确性和响应性，使用学习型感知模型处理环境信息，并结合安全监督机制评估风险。整体流程是：1)接收末端执行器目标、工具几何和交互模式输入；2)使用3D CNN处理点云数据；3)安全监督器评估碰撞风险；4)低风险时使用强化学习输出作为逆运动学初始猜测实现高精度控制；5)高风险时直接使用强化学习策略进行快速反应；6)根据交互模式调整工具区域约束，允许或禁止工具接触。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)工具感知的碰撞避免，能适应不同工具和操作模式；2)混合控制架构，结合经典逆运动学高精度和强化学习快速反应；3)学习型感知模型，处理点云并过滤机器人和工具组件；4)高效安全监督机制，快速评估碰撞风险；5)高计算效率，50Hz运行频率。相比传统方法，本文方法能更好适应动态环境，平衡精确性和响应性，在遮挡环境下更鲁棒，计算效率更高，且能适应不同工具和操作模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种快速、工具感知的碰撞避免系统，通过结合学习型感知模型、安全监督机制和混合控制架构，实现了在动态环境中协作机器人的安全、高效和精确操作。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3579207&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safe and efficient operation of collaborative robots in humanenvironments is challenging, especially in dynamic settings where both obstaclemotion and tasks change over time. Current robot controllers typically assumefull visibility and fixed tools, which can lead to collisions or overlyconservative behavior. In our work, we introduce a tool-aware collisionavoidance system that adjusts in real time to different tool sizes and modes oftool-environment interaction. Using a learned perception model, our systemfilters out robot and tool components from the point cloud, reasons aboutoccluded area, and predicts collision under partial observability. We then usea control policy trained via constrained reinforcement learning to producesmooth avoidance maneuvers in under 10 milliseconds. In simulated andreal-world tests, our approach outperforms traditional approaches (APF, MPPI)in dynamic environments, while maintaining sub-millimeter accuracy. Moreover,our system operates with approximately 60% lower computational cost compared toa state-of-the-art GPU-based planner. Our approach provides modular, efficient,and effective collision avoidance for robots operating in dynamic environments.We integrate our method into a collaborative robot application and demonstrateits practical use for safe and responsive operation.</description>
      <author>example@mail.com (Joonho Lee, Yunho Kim, Seokjoon Kim, Quan Nguyen, Youngjin Heo)</author>
      <guid isPermaLink="false">2508.20457v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads</title>
      <link>http://arxiv.org/abs/2508.20135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种数据高效的点云分割流水线和训练框架，用于稳健分割未改进的道路和其他七个类别。采用两阶段训练框架，结合点提示训练和流形混合正则化技术，仅使用50个目标域标记点云就能显著提高分割性能。&lt;h4&gt;背景&lt;/h4&gt;在3D语义分割领域，特别是在数据有限的挑战性场景中，如何实现稳健分割是一个重要问题。传统的训练方法在有限数据条件下表现不佳，需要探索更高效的数据利用方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种数据高效的点云分割方法，能够在有限标注数据的情况下实现稳健的道路和其他类别的分割，提高模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 采用两阶段训练框架：首先在公共城市数据集和小型定制领域数据集的混合数据上预训练基于投影的卷积神经网络，然后仅在领域数据上微调轻量级预测头；2. 探索点提示训练在批归一化层中的应用；3. 研究流形混合作为正则化的效果；4. 融入直方图归一化的环境信息以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用50个目标域的标记点云，所提出的训练方法将平均交并比从33.5%提高到51.8%，整体准确率从85.5%提高到90.8%。跨多个数据集的预训练对提高泛化能力和在有限领域监督下实现稳健分割至关重要。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一个实用的框架，用于在具有挑战性的低数据场景中进行稳健的3D语义分割。通过精心设计的两阶段训练框架和多种技术整合，即使数据有限也能实现显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;本研究案例中，我们提出了一种数据高效的点云分割流水线和训练框架，用于稳健分割未改进的道路和其他七个类别。我们的方法采用两阶段训练框架：首先，在公共城市数据集和小型定制领域数据集的混合数据上预训练基于投影的卷积神经网络；然后，仅在领域数据上微调轻量级预测头。在此过程中，我们探索了点提示训练在批归一化层中的应用，以及流形混合作为正则化在我们流水线中的效果。我们还研究了融入直方图归一化环境信息以进一步提高性能的效果。仅使用目标域的50个标记点云，我们证明与在领域数据上进行简单训练相比，我们提出的训练方法将平均交并比从33.5%提高到51.8%，整体准确率从85.5%提高到90.8%。关键的是，我们的结果表明，跨多个数据集进行预训练是提高泛化能力和在有限领域监督下实现稳健分割的关键。总体而言，本研究展示了一个在具有挑战性的低数据场景中进行稳健3D语义分割的实用框架。我们的代码可在以下网址获取：https://github.com/andrewyarovoi/MD-FRNet。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在有限标注数据下实现有效的点云语义分割，特别是针对未改进道路（如农村土路、碎石路等）的分割。这个问题重要是因为：1) 生成标记点云数据非常耗时耗力，平均每个扫描需30分钟以上；2) 新环境难以创建大规模数据集；3) 目标领域（农村、森林环境）与公共数据集（城市道路）差异显著；4) 使用的LiDAR传感器与公共数据集不同，导致直接应用预训练模型效果不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题框架化为少样本学习(FSL)，借鉴了多个现有工作：1) 借鉴图像分类中的度量学习(如原型网络)和迁移学习方法；2) 受PANet等点云分割工作的启发，使用语义掩码计算特征表示；3) 受[3]启发设计微调阶段使用小型MLP；4) 借鉴[6]的多数据集预训练策略；5) 应用Point Prompt Training处理多数据集分布差异；6) 探索Manifold Mixup作为正则化方法；7) 选择FRNet作为特征提取器，结合PointNeXt的倒瓶颈MLP设计预测头。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用两阶段训练框架：首先在混合数据集上预训练特征提取器，然后在领域内数据上微调轻量级预测头。整体流程：1) 数据准备：将所有数据集统一格式，进行多类映射，提取环境特征，实施多种数据增强；2) 预训练阶段：在混合数据集(Semantic KITTI、Waymo和目标数据集)上训练FRNet特征提取器；3) 微调阶段：冻结特征提取器，仅在目标数据集上训练MLP预测头；4) 技术组件：应用Point Prompt Training处理多数据集差异，探索Manifold Mixup平滑决策边界，利用环境信息提升分割精度；5) 评估：使用mIoU和准确率指标进行性能评估和消融研究。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出数据高效的两阶段训练框架，仅用50个标记点云实现显著性能提升；2) 首次将协同多数据集训练应用于点云语义分割的领域适应；3) 将Point Prompt Training扩展应用于基于卷积的架构；4) 探索Manifold Mixup作为点云分割头的正则化方法；5) 创新利用直方图归一化的环境值提升道路分割性能。相比之前工作的不同：1) 专注于跨领域适应(城市到农村)；2) 强调数据效率，解决低数据场景问题；3) 采用独特的两阶段训练策略；4) 评估8类分割而非单一类别；5) 结合多种技术提升模型在目标领域的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种数据高效的点云语义分割框架，通过多数据集预训练结合领域内微调，仅使用50个标记点云就能显著提升未改进道路的分割性能，为低数据场景下的3D语义分割提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this case study, we present a data-efficient point cloud segmentationpipeline and training framework for robust segmentation of unimproved roads andseven other classes. Our method employs a two-stage training framework: first,a projection-based convolutional neural network is pre-trained on a mixture ofpublic urban datasets and a small, curated in-domain dataset; then, alightweight prediction head is fine-tuned exclusively on in-domain data. Alongthe way, we explore the application of Point Prompt Training to batchnormalization layers and the effects of Manifold Mixup as a regularizer withinour pipeline. We also explore the effects of incorporating histogram-normalizedambients to further boost performance. Using only 50 labeled point clouds fromour target domain, we show that our proposed training approach improves meanIntersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5%to 90.8%, when compared to naive training on the in-domain data. Crucially, ourresults demonstrate that pre-training across multiple datasets is key toimproving generalization and enabling robust segmentation under limitedin-domain supervision. Overall, this study demonstrates a practical frameworkfor robust 3D semantic segmentation in challenging, low-data scenarios. Ourcode is available at: https://github.com/andrewyarovoi/MD-FRNet.</description>
      <author>example@mail.com (Andrew Yarovoi, Christopher R. Valenta)</author>
      <guid isPermaLink="false">2508.20135v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>SEAL: Structure and Element Aware Learning to Improve Long Structured Document Retrieval</title>
      <link>http://arxiv.org/abs/2508.20778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对长结构化文档检索问题，提出了一种新型对比学习框架和配套的数据集，解决了现有方法无法有效利用结构特征和缺乏结构元数据的问题，显著提升了检索性能。&lt;h4&gt;背景&lt;/h4&gt;在长结构化文档检索领域，现有方法通常在缺乏明确结构信息的数据集上使用对比学习微调预训练语言模型，导致无法有效利用结构特征和元素级语义，且缺乏包含结构元数据的数据集。&lt;h4&gt;目的&lt;/h4&gt;解决现有长结构化文档检索方法中无法有效利用结构特征和缺乏结构元数据的问题，提高检索性能。&lt;h4&gt;方法&lt;/h4&gt;提出了名为SEAL的新型对比学习框架，利用结构感知学习保留语义层次结构，并通过掩码元素实现细粒度语义区分；同时发布了包含丰富结构注释的长结构化文档检索数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在各种现代预训练语言模型上进行的实验和在线A/B测试表明，该方法带来了一致的性能提升，在BGE-M3模型上将NDCG@10指标从73.96%提升至77.84%。&lt;h4&gt;结论&lt;/h4&gt;SEAL框架和配套数据集有效解决了长结构化文档检索中的关键问题，显著提升了检索性能，相关资源已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;在长结构化文档检索中，现有方法通常在缺乏明确结构信息的数据集上使用对比学习来微调预训练语言模型。这种方法存在两个关键问题：1)当前方法无法有效利用结构特征和元素级语义；2)缺乏包含结构元数据的数据集。为解决这些问题，我们提出了SEAL，一种新型对比学习框架。它利用结构感知学习来保留语义层次结构，并通过掩码元素实现细粒度语义区分。此外，我们发布了SEAL数据集，一个具有丰富结构注释的长结构化文档检索数据集。在多个现代预训练语言模型上对发布和工业数据集进行的广泛实验，以及在线A/B测试，都展示了一致的性能提升，在BGE-M3上将NDCG@10从73.96%提升到77.84%。资源可在https://github.com/xinhaoH/SEAL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In long structured document retrieval, existing methods typically fine-tunepre-trained language models (PLMs) using contrastive learning on datasetslacking explicit structural information. This practice suffers from twocritical issues: 1) current methods fail to leverage structural features andelement-level semantics effectively, and 2) the lack of datasets containingstructural metadata. To bridge these gaps, we propose \our, a novel contrastivelearning framework. It leverages structure-aware learning to preserve semantichierarchies and masked element alignment for fine-grained semanticdiscrimination. Furthermore, we release \dataset, a long structured documentretrieval dataset with rich structural annotations. Extensive experiments onboth released and industrial datasets across various modern PLMs, along withonline A/B testing, demonstrate consistent performance improvements, boostingNDCG@10 from 73.96\% to 77.84\% on BGE-M3. The resources are available athttps://github.com/xinhaoH/SEAL.</description>
      <author>example@mail.com (Xinhao Huang, Zhibo Ren, Yipeng Yu, Ying Zhou, Zulong Chen, Zeyi Wen)</author>
      <guid isPermaLink="false">2508.20778v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>"Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection</title>
      <link>http://arxiv.org/abs/2508.20670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了S-HArM多模态数据集用于意图感知分类，探索了三种提示策略构建合成训练数据，并通过多种模型技术进行比较研究，发现图像和多模态引导数据训练的模型泛化能力更好，但整体性能仍有提升空间。&lt;h4&gt;背景&lt;/h4&gt;多模态AI的最新进展使检测合成内容和上下文外内容成为可能，但现有研究很大程度上忽略了AI生成图像背后的意图。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究在AI生成图像意图识别方面的空白，引入专门的数据集和训练方法来提高意图感知分类能力。&lt;h4&gt;方法&lt;/h4&gt;1) 创建S-HArM多模态数据集，包含9,576个来自社交媒体的图像-文本对并分类；2) 探索三种提示策略(图像引导、描述引导、多模态引导)使用Stable Diffusion构建合成训练数据；3) 进行多种技术比较研究，包括模态融合、对比学习、重建网络、注意力机制和大视觉语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;基于图像引导和多模态引导数据训练的模型在'野外'内容上泛化能力更好，因为保留了视觉上下文，但整体性能仍然有限。&lt;h4&gt;结论&lt;/h4&gt;推断AI生成图像背后的意图是一个复杂问题，需要专门架构和方法来提高性能，现有技术仍有较大改进空间。&lt;h4&gt;翻译&lt;/h4&gt;多模态AI的最新进展使得检测合成内容和上下文外内容成为可能。然而，现有工作在很大程度上忽略了AI生成图像背后的意图。为了填补这一空白，我们引入了S-HArM，这是一个用于意图感知分类的多模态数据集，包含9,576个来自Twitter/X和Reddit的'野外'图像-文本对，标记为幽默/讽刺、艺术或错误信息。此外，我们探索了三种提示策略（图像引导、描述引导和多模态引导）使用Stable Diffusion构建大规模合成训练数据集。我们进行了广泛的比较研究，包括模态融合、对比学习、重建网络、注意力机制和大视觉语言模型。我们的结果表明，在图像引导和多模态引导数据上训练的模型在'野外'内容上泛化能力更好，原因是保留了视觉上下文。然而，整体性能仍然有限，突显了推断意图的复杂性以及需要专门架构的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746275.3762215&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal AI have enabled progress in detecting syntheticand out-of-context content. However, existing efforts largely overlook theintent behind AI-generated images. To fill this gap, we introduce S-HArM, amultimodal dataset for intent-aware classification, comprising 9,576 "in thewild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,or Misinformation. Additionally, we explore three prompting strategies(image-guided, description-guided, and multimodally-guided) to construct alarge-scale synthetic training dataset with Stable Diffusion. We conduct anextensive comparative study including modality fusion, contrastive learning,reconstruction networks, attention mechanisms, and large vision-languagemodels. Our results show that models trained on image- and multimodally-guideddata generalize better to "in the wild" content, due to preserved visualcontext. However, overall performance remains limited, highlighting thecomplexity of inferring intent and the need for specialized architectures.</description>
      <author>example@mail.com (Anastasios Skoularikis, Stefanos-Iordanis Papadopoulos, Symeon Papadopoulos, Panagiotis C. Petrantonakis)</author>
      <guid isPermaLink="false">2508.20670v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music</title>
      <link>http://arxiv.org/abs/2508.20665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Amadeus的新型符号音乐生成框架，采用两级架构（自回归模型处理音符序列，双向离散扩散模型处理音符属性），并提出了MLSDES和CIEM两种增强策略。实验表明，Amadeus在多个指标上显著优于现有模型，同时实现了至少4倍的加速，并支持无需训练的细粒度音符属性控制。&lt;h4&gt;背景&lt;/h4&gt;现有的最先进的符号音乐生成模型主要采用自回归或层次自回归架构，将符号音乐建模为具有单向时间依赖关系的属性标记序列，并假设这些属性之间存在固定的、严格的依赖结构。&lt;h4&gt;目的&lt;/h4&gt;基于音符属性本质上是并发且无序的集合而非时间依赖序列的新认识，开发一种能够更有效处理音符属性关系的新型符号音乐生成框架。&lt;h4&gt;方法&lt;/h4&gt;1) 采用两级架构：自回归模型处理音符序列，双向离散扩散模型处理音符属性；2) 提出Music Latent Space Discriminability Enhancement Strategy (MLSDES)，结合对比学习约束增强中间音乐表示的区分度；3) 设计Conditional Information Enhancement Module (CIEM)，通过注意力机制加强音符潜在向量表示；4) 编译了最大的开源符号音乐数据集AMD支持预训练和微调。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用不同属性作为初始标记会导致性能相当，表明音符属性是并发且无序的集合；2) Amadeus在无条件生成和文本条件生成任务上显著优于SOTA模型；3) 实现了至少4倍的加速；4) 支持无需训练的细粒度音符属性控制。&lt;h4&gt;结论&lt;/h4&gt;Amadeus通过重新思考音符属性之间的关系，采用创新的两级架构和增强策略，显著提高了符号音乐生成的质量和效率，同时提供了更好的控制能力，为符号音乐生成领域带来了新的突破。&lt;h4&gt;翻译&lt;/h4&gt;现有的最先进的符号音乐生成模型主要采用自回归或层次自回归架构，将符号音乐建模为具有单向时间依赖关系的属性标记序列，并假设这些属性之间存在固定的、严格的依赖结构。然而，我们观察到，在这些模型中使用不同属性作为初始标记会导致性能相当。这表明音符的属性本质上是一个并发且无序的集合，而不是时间依赖的序列。基于这一见解，我们引入了Amadeus，一种新型的符号音乐生成框架。Amadeus采用两级架构：用于音符序列的自回归模型和用于音符属性的双向离散扩散模型。为了增强性能，我们提出了Music Latent Space Discriminability Enhancement Strategy (MLSDES)，结合对比学习约束，增强中间音乐表示的区分度。Conditional Information Enhancement Module (CIEM)通过注意力机制同时加强音符潜在向量表示，实现更精确的音符解码。我们在无条件生成和文本条件生成任务上进行了广泛实验。Amadeus在多个指标上显著优于SOTA模型，同时实现了至少4倍的加速。此外，我们展示了使用我们的模型进行无需训练的细粒度音符属性控制的可行性。为了探索Amadeus架构的上限性能，我们编译了迄今为止最大的开源符号音乐数据集AMD，支持预训练和微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing state-of-the-art symbolic music generation models predominantlyadopt autoregressive or hierarchical autoregressive architectures, modellingsymbolic music as a sequence of attribute tokens with unidirectional temporaldependencies, under the assumption of a fixed, strict dependency structureamong these attributes. However, we observe that using different attributes asthe initial token in these models leads to comparable performance. Thissuggests that the attributes of a musical note are, in essence, a concurrentand unordered set, rather than a temporally dependent sequence. Based on thisinsight, we introduce Amadeus, a novel symbolic music generation framework.Amadeus adopts a two-level architecture: an autoregressive model for notesequences and a bidirectional discrete diffusion model for attributes. Toenhance performance, we propose Music Latent Space Discriminability EnhancementStrategy(MLSDES), incorporating contrastive learning constraints that amplifydiscriminability of intermediate music representations. The ConditionalInformation Enhancement Module (CIEM) simultaneously strengthens note latentvector representation via attention mechanisms, enabling more precise notedecoding. We conduct extensive experiments on unconditional andtext-conditioned generation tasks. Amadeus significantly outperforms SOTAmodels across multiple metrics while achieving at least 4$\times$ speed-up.Furthermore, we demonstrate training-free, fine-grained note attribute controlfeasibility using our model. To explore the upper performance bound of theAmadeus architecture, we compile the largest open-source symbolic music datasetto date, AMD (Amadeus MIDI Dataset), supporting both pre-training andfine-tuning.</description>
      <author>example@mail.com (Hongju Su, Ke Li, Lan Yang, Honggang Zhang, Yi-Zhe Song)</author>
      <guid isPermaLink="false">2508.20665v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning through Auxiliary Branch for Video Object Detection</title>
      <link>http://arxiv.org/abs/2508.20551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted paper for ACIVS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为CLAB（对比学习辅助分支）的视频目标检测方法，通过对比学习辅助分支和动态损失加权策略，在不增加计算负担的情况下提高了视频目标检测的性能，特别是在处理图像退化问题方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;视频目标检测是一项具有挑战性的任务，因为视频常常受到图像质量下降的影响，如运动模糊、遮挡和可变形状，这使得它比在静态图像中检测物体要困难得多。先前的方法通过使用特征聚合和复杂的后处理技术提高了性能，但代价是增加了计算需求。&lt;h4&gt;目的&lt;/h4&gt;提高对图像退化的鲁棒性，同时在推理过程中不增加额外的计算负担。&lt;h4&gt;方法&lt;/h4&gt;1. 实现了一个使用对比损失的对比辅助分支，以增强视频目标检测骨干网络的特征表示能力；2. 提出了一种动态损失加权策略，在训练初期强调辅助特征学习，随着训练收敛逐渐优先考虑检测任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过全面的实验和消融研究验证了该方法，证明了一致的性能提升。CLAB在ImageNet VID数据集上使用ResNet-101和ResNeXt-101分别达到了84.0%和85.2%的mAP，在不需要额外后处理方法的情况下，为基于CNN的模型实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;CLAB方法是一种简单而有效的视频目标检测改进方法，能够在不增加计算负担的情况下提高对图像退化的鲁棒性，并在ImageNet VID数据集上取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;视频目标检测是一项具有挑战性的任务，因为视频常常受到图像质量下降的影响，如运动模糊、遮挡和可变形状，这使得它比在静态图像中检测物体要困难得多。先前的方法通过使用特征聚合和复杂的后处理技术提高了视频目标检测的性能，但代价是增加了计算需求。为了提高对图像退化的鲁棒性，同时在推理过程中不增加额外的计算负担，我们引入了一种简单而有效的对比学习辅助分支（CLAB）方法。首先，我们使用对比损失实现了一个对比辅助分支，以增强视频目标检测骨干网络的特征表示能力。接下来，我们提出了一种动态损失加权策略，在训练初期强调辅助特征学习，随着训练收敛逐渐优先考虑检测任务。我们通过全面的实验和消融研究验证了我们的方法，证明了一致的性能提升。CLAB在ImageNet VID数据集上使用ResNet-101和ResNeXt-101分别达到了84.0%和85.2%的mAP，因此在不要求额外后处理方法的情况下，为基于CNN的模型实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video object detection is a challenging task because videos often suffer fromimage deterioration such as motion blur, occlusion, and deformable shapes,making it significantly more difficult than detecting objects in still images.Prior approaches have improved video object detection performance by employingfeature aggregation and complex post-processing techniques, though at the costof increased computational demands. To improve robustness to image degradationwithout additional computational load during inference, we introduce astraightforward yet effective Contrastive Learning through Auxiliary Branch(CLAB) method. First, we implement a constrastive auxiliary branch using acontrastive loss to enhance the feature representation capability of the videoobject detector's backbone. Next, we propose a dynamic loss weighting strategythat emphasizes auxiliary feature learning early in training while graduallyprioritizing the detection task as training converges. We validate our approachthrough comprehensive experiments and ablation studies, demonstratingconsistent performance gains. Without bells and whistles, CLAB reaches aperformance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,respectively, on the ImageNet VID dataset, thus achieving state-of-the-artperformance for CNN-based models without requiring additional post-processingmethods.</description>
      <author>example@mail.com (Lucas Rakotoarivony)</author>
      <guid isPermaLink="false">2508.20551v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Fact or Facsimile? Evaluating the Factual Robustness of Modern Retrievers</title>
      <link>http://arxiv.org/abs/2508.20408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 34th ACM International Conference on Information  and Knowledge Management&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究发现，RAG管道中的密集检索器和重排序器在事实能力上显著低于其基础大语言模型，检索器准确性比生成模型低约30个百分点，且对干扰项高度敏感，决策主要基于表面语义接近而非事实推理。&lt;h4&gt;背景&lt;/h4&gt;密集检索器和重排序器是检索增强生成(RAG)管道的核心组件，准确检索事实信息对维持系统可信度和防御RAG中毒至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究检索组件从其基础大语言模型(LLMs)继承或失去了多少事实能力。&lt;h4&gt;方法&lt;/h4&gt;将12个公开发布的嵌入检查点与其原始基础LLM配对，在事实性基准测试上评估这两组模型的表现。&lt;h4&gt;主要发现&lt;/h4&gt;1) 嵌入变体准确性比基础模型低12-43个百分点(中位数28点)；2) 检索器准确率降至25-35%，而生成模型达60-70%；3) 候选池扩大导致检索器准确率显著下降；4) 检索决策主要基于语义相似度而非事实推理；5) 当词汇线索被掩盖时，超过三分之二正确预测变为错误。&lt;h4&gt;结论&lt;/h4&gt;对比学习为检索器引入了系统性权衡：语义检索的增益是以参数化事实知识的损失为代价的。&lt;h4&gt;翻译&lt;/h4&gt;密集检索器和重排序器是检索增强生成(RAG)管道的核心组件，在RAG中准确检索事实信息对维持系统可信度和防御RAG中毒至关重要。然而，这些组件从其基础大语言模型(LLMs)继承或失去了多少事实能力尚不清楚。我们将12个公开发布的嵌入检查点与其原始基础LLM配对，并在事实性基准测试上评估这两组模型。在每个评估模型中，嵌入变体的准确性明显低于其基础模型，绝对下降幅度在12到43个百分点之间(中位数28个百分点)，检索器的典型准确性下降到25-35%的范围内，而生成模型达到60-70%。这种退化在更苛刻的条件下加剧：当每个问题的候选池从四个选项扩展到一千个选项时，最强检索器的top-1准确性从33%下降到26%，显示出对干扰项数量的高度敏感性。统计测试进一步表明，对于每个嵌入模型，查询与正确完成之间的余弦相似度得分显著高于与错误完成之间的得分(p &lt; 0.01)，表明决策主要由表面语义接近性而非事实推理驱动。为了探究这一弱点，我们使用GPT-4.1重新表述每个正确完成，创建了一个保留事实真相但掩盖词汇线索的重写测试集，观察到超过三分之二之前正确的预测转变为错误，总体准确性降至原始水平的三分之一左右。总之，这些发现揭示了对比学习为检索器引入的系统性权衡：语义检索的增益是以参数化事实知识的损失为代价的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense retrievers and rerankers are central to retrieval-augmented generation(RAG) pipelines, where accurately retrieving factual information is crucial formaintaining system trustworthiness and defending against RAG poisoning.However, little is known about how much factual competence these componentsinherit or lose from the large language models (LLMs) they are based on. Wepair 12 publicly released embedding checkpoints with their original base LLMsand evaluate both sets on a factuality benchmark. Across every model evaluated,the embedding variants achieve markedly lower accuracy than their bases, withabsolute drops ranging from 12 to 43 percentage points (median 28 pts) andtypical retriever accuracies collapsing into the 25-35 % band versus the 60-70% attained by the generative models. This degradation intensifies under a moredemanding condition: when the candidate pool per question is expanded from fouroptions to one thousand, the strongest retriever's top-1 accuracy falls from 33% to 26 %, revealing acute sensitivity to distractor volume. Statistical testsfurther show that, for every embedding model, cosine-similarity scores betweenqueries and correct completions are significantly higher than those forincorrect ones (p &lt; 0.01), indicating decisions driven largely by surface-levelsemantic proximity rather than factual reasoning. To probe this weakness, weemployed GPT-4.1 to paraphrase each correct completion, creating a rewrittentest set that preserved factual truth while masking lexical cues, and observedthat over two-thirds of previously correct predictions flipped to wrong,reducing overall accuracy to roughly one-third of its original level. Takentogether, these findings reveal a systematic trade-off introduced bycontrastive learning for retrievers: gains in semantic retrieval are paid forwith losses in parametric factual knowledge......</description>
      <author>example@mail.com (Haoyu Wu, Qingcheng Zeng, Kaize Ding)</author>
      <guid isPermaLink="false">2508.20408v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search</title>
      <link>http://arxiv.org/abs/2508.20353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DFAMS的新型联邦检索框架，通过动态信息流(DIF)识别查询意图并构建语义对齐的知识分区，解决了现有方法在处理模糊查询和跨域场景时的局限性，显著提高了检索质量和下游任务性能。&lt;h4&gt;背景&lt;/h4&gt;联邦检索(FR)通过跨多个外部知识源路由查询来减轻LLMs的幻觉问题，但当知识分布时，现有方法难以检索模糊查询的高质量和相关文档，特别是在跨域场景中，这限制了它们在支持下游生成任务方面的有效性。&lt;h4&gt;目的&lt;/h4&gt;解决现有联邦检索方法在处理模糊查询和跨域场景时的局限性，提高检索质量和相关性，以更好地支持下游生成任务。&lt;h4&gt;方法&lt;/h4&gt;提出DFAMS框架，利用动态信息流(DIF)识别潜在查询意图并构建语义对齐的知识分区。具体包括：利用少量标注查询的梯度信号和基于Shapley值的归因方法追踪与意图识别和子域边界检测相关的神经元激活路径；通过多原型对比学习训练对齐模块，实现细粒度的源内建模和跨知识库的语义对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准测试中，DFAMS显著优于先进的FR方法：知识分类准确率提高最高达14.37%，检索召回率提高5.38%，下游QA准确率提高6.45%，证明了其在复杂FR场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;DFAMS框架能够有效解决联邦检索中的模糊查询和跨域挑战，通过动态信息流和语义对齐，显著提升了检索质量和下游任务性能。&lt;h4&gt;翻译&lt;/h4&gt;联邦检索(FR)在必要时跨多个外部知识源路由查询，以减轻LLMs的幻觉，当必要的外部知识分布时。然而，现有方法难以检索模糊查询的高质量和相关文档，特别是在跨域场景中，这显著限制了它们在支持下游生成任务方面的有效性。受动态信息流(DIF)启发，我们提出了DFAMS，一个新颖的框架，利用DIF识别潜在查询意图，并构建语义对齐的知识分区，以实现跨异构源的准确检索。具体来说，DFAMS通过利用少量标注查询的梯度信号，并采用基于Shapley值的归因方法来追踪与意图识别和子域边界检测相关的神经元激活路径，从而探测LLMs中的DIF。然后，DFAMS利用DIF通过多原型对比学习训练对齐模块，实现跨知识库的细粒度源内建模和语义对齐。在五个基准测试中的实验结果表明，DFAMS在知识分类准确率上比先进的FR方法高出最多14.37%，在检索召回率上高出5.38%，在下游QA准确率上高出6.45%，证明了其在复杂FR场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Retrieval (FR) routes queries across multiple external knowledgesources, to mitigate hallucinations of LLMs, when necessary external knowledgeis distributed. However, existing methods struggle to retrieve high-quality andrelevant documents for ambiguous queries, especially in cross-domain scenarios,which significantly limits their effectiveness in supporting downstreamgeneration tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,a novel framework that leverages DIF to identify latent query intents andconstruct semantically aligned knowledge partitions for accurate retrievalacross heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs byleveraging gradient signals from a few annotated queries and employing Shapleyvalue-based attribution to trace neuron activation paths associated with intentrecognition and subdomain boundary detection. Then, DFAMS leverages DIF totrain an alignment module via multi-prototype contrastive learning, enablingfine-grained intra-source modeling and inter-source semantic alignment acrossknowledge bases. Experimental results across five benchmarks show that DFAMSoutperforms advanced FR methods by up to 14.37% in knowledge classificationaccuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,demonstrating its effectiveness in complex FR scenarios.</description>
      <author>example@mail.com (Zhibang Yang, Xinke Jiang, Rihong Qiu, Ruiqing Li, Yihang Zhang, Yue Fang, Yongxin Xu, Hongxin Ding, Xu Chu, Junfeng Zhao, Yasha Wang)</author>
      <guid isPermaLink="false">2508.20353v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Staircase Recognition and Location Based on Polarization Vision</title>
      <link>http://arxiv.org/abs/2505.19026v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合偏振和光强信息的对比度增强算法以及基于YOLOv11的点云分割方法，用于楼梯检测，并融合偏振双目和TOF深度信息实现楼梯高质量三维重建，同时提出基于ICP配准和改进灰狼优化算法的单目相机与TOF相机联合校准方法。&lt;h4&gt;背景&lt;/h4&gt;楼梯是人工场景中常见结构，但对人形机器人和下肢残疾或视觉障碍人士而言，缺乏传感器和智能算法支持难以穿越。楼梯场景感知技术是识别定位的前提，对机器人模式切换和足迹位置计算至关重要。现有技术面临识别精度低、传感器噪声高、信号不稳定和计算要求高等问题。双目和TOF重建易受环境光和物体表面材料影响。&lt;h4&gt;目的&lt;/h4&gt;实现楼梯检测、高质量三维重建以及单目相机和TOF相机的联合校准。&lt;h4&gt;方法&lt;/h4&gt;1) 提出结合偏振和光强信息的对比度增强算法；2) 集成基于YOLOv11的点云分割；3) 融合偏振双目和TOF深度信息实现楼梯三维重建；4) 基于ICP配准和改进灰狼优化算法提出单目相机与TOF相机联合校准算法。&lt;h4&gt;主要发现&lt;/h4&gt;偏振重建方法受环境光影响较小，且不依赖于物体表面的纹理信息，相比传统方法具有明显优势。&lt;h4&gt;结论&lt;/h4&gt;通过结合偏振信息与传统方法，可以有效提高楼梯场景感知的准确性和鲁棒性，为机器人导航和辅助技术提供更好的支持。&lt;h4&gt;翻译&lt;/h4&gt;楼梯是人工场景中最常见的结构之一。然而，对于人形机器人和下肢残疾或视觉障碍的人士来说，没有传感器和智能算法的帮助很难穿越楼梯场景。楼梯场景感知技术是识别和定位的前提，对机器人模式切换和足迹位置计算以适应不连续地形具有重要意义。然而，仍有诸多问题制约该技术应用，如识别精度低、传感器初始噪声高、输出信号不稳定和计算要求高。在场景重建方面，双目和飞行时间(TOF)重建易受环境光和目标物体表面材料影响。相比之下，由于偏振器的特殊结构，偏振可以选择性传输特定方向的偏振光，这种重建方法依赖于物体表面的偏振信息，因此偏振重建的优势得以体现，即受环境光影响较小且不依赖于物体表面的纹理信息。本文为实现楼梯检测，提出了一种结合偏振和光强信息的对比度增强算法，并集成了基于YOLOv11的点云分割。为实现高质量重建，我们提出了一种融合偏振双目和TOF深度信息的方法来实现楼梯的三维重建。此外，还提出了一种基于ICP配准和改进灰狼优化算法的单目相机和TOF相机联合校准算法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决人形机器人和行动不便人群的楼梯识别和定位问题。现有方法存在低识别准确率、对光照条件敏感和依赖纹理信息等局限性。这个问题在现实中非常重要，因为楼梯是常见的障碍物，准确的楼梯识别技术对机器人导航控制、下肢残疾人士辅助导航以及提升机器人在楼梯场景中的整体性能至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有楼梯识别方法的局限性，包括可穿戴传感器和光电传感器的问题。然后设计了一个基于偏振视觉的融合框架，包含三个主要模块：楼梯识别模块、异构传感器校准模块和偏振3D重建模块。作者借鉴了YOLO系列目标检测算法、偏振成像技术、点云分割技术、ICP算法和灰狼优化算法等现有工作，但进行了创新性改进，如融合偏振-强度图像、改进灰狼优化算法(结合Levy飞行和动态权重)，以及双目偏振和TOF深度信息融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用偏振视觉获取更多维度信息增强图像对比度，并通过融合双目视觉和TOF深度信息克服单一传感器局限性：双目视觉纠正偏振方位角模糊，TOF填充双目数据空洞。整体流程分为三阶段：1)楼梯识别：使用MLP融合DoP和强度图像，YOLOv11初步识别，点云法向量分割提高准确率；2)异构传感器校准：使用棋盘格校准板，结合改进灰狼优化算法优化ICP，解决不同分辨率相机校准；3)偏振3D重建：双目初步纠正偏振梯度场，双目深度信息纠正方位角模糊，TOF纠正空洞区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)楼梯识别模块：偏振-强度图像融合和点云法向量分割，提高识别准确率至98.7%；2)异构传感器校准模块：改进灰狼优化算法(Levy飞行和动态权重)，解决不同分辨率相机校准，精度达0.33±0.04mm；3)偏振3D重建模块：双目偏振和TOF融合，实现&lt;0.2%重建误差。相比之前工作，该方法不依赖纹理信息(解决双目视觉表面扭曲问题)，纠正偏振法向量模糊(解决纯偏振方法问题)，无需高精度扫描仪数据集，适用于动态场景和低纹理表面。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于偏振视觉的楼梯识别与定位方法，通过融合双目偏振视觉和TOF深度信息，实现了高精度(98.7%)的楼梯识别和&lt;0.2%重建误差的3D重建，为机器人提供了准确的楼梯场景感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Staircase is one of the most common structures in artificial scenes. However,it is difficult for humanoid robots and people with lower limb disabilities orvisual impairment to cross the scene without the help of sensors andintelligent algorithms. Staircase scene perception technology is a prerequisitefor recognition and localization. This technology is of great significance forthe mode switching of the robot and the calculation of the footprint positionto adapt to the discontinuous terrain. However, there are still many problemsthat constrain the application of this technology, such as low recognitionaccuracy, high initial noise from sensors, unstable output signals and highcomputational requirements. In terms of scene reconstruction, the binocular andtime of flight (TOF) reconstruction of the scene can be easily affected byenvironmental light and the surface material of the target object. In contrast,due to the special structure of the polarizer, the polarization can selectivelytransmit polarized light in a specific direction and this reconstruction methodrelies on the polarization information of the object surface. So the advantagesof polarization reconstruction are reflected, which are less affected byenvironmental light and not dependent on the texture information of the objectsurface. In this paper, in order to achieve the detection of staircase, thispaper proposes a contrast enhancement algorithm that integrates polarizationand light intensity information, and integrates point cloud segmentation basedon YOLOv11. To realize the high-quality reconstruction, we proposed a method offusing polarized binocular and TOF depth information to realize thethree-dimensional (3D) reconstruction of the staircase. Besides, it alsoproposes a joint calibration algorithm of monocular camera and TOF camera basedon ICP registration and improved gray wolf optimization algorithm.</description>
      <author>example@mail.com (Weifeng Kong, Zhiying Tan)</author>
      <guid isPermaLink="false">2505.19026v3</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections</title>
      <link>http://arxiv.org/abs/2508.20955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为E-ConvNeXt的新型轻量级网络，通过整合Cross Stage Partial Connections机制和一系列优化设计，显著降低了ConvNeXt的参数规模和网络复杂度，同时保持了高准确率性能。&lt;h4&gt;背景&lt;/h4&gt;许多高性能网络在设计时没有考虑轻量级应用场景，限制了它们的应用范围。&lt;h4&gt;目的&lt;/h4&gt;通过整合Cross Stage Partial Connections机制和一系列优化设计，显著减少ConvNeXt的参数规模和网络复杂度，并保持高准确率性能。&lt;h4&gt;方法&lt;/h4&gt;提出名为E-ConvNeXt的新网络，具有三个核心创新：1) 将Cross Stage Partial Network与ConvNeXt集成并调整网络结构，降低模型网络复杂度高达80%；2) 优化Stem和Block结构，增强模型特征表达能力和操作效率；3) 用通道注意力替换Layer Scale。&lt;h4&gt;主要发现&lt;/h4&gt;在ImageNet分类上的实验验证了E-ConvNeXt在准确率-效率平衡方面的优越性：E-ConvNeXt-mini在0.9 GFLOPs下达到78.3% Top-1准确率；E-ConvNeXt-small在3.1 GFLOPs下达到81.9% Top-1准确率。&lt;h4&gt;结论&lt;/h4&gt;在目标检测任务上的迁移学习测试进一步证实了E-ConvNeXt的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;许多高性能网络在设计之初并未考虑轻量级应用场景，这大大限制了它们的应用范围。本文以ConvNeXt为研究对象，通过整合Cross Stage Partial Connections机制和一系列优化设计，显著降低了ConvNeXt的参数规模和网络复杂度。新网络被命名为E-ConvNeXt，能够在不同复杂度配置下保持高精度性能。E-ConvNeXt的三个核心创新是：(1) 将Cross Stage Partial Network (CSPNet)与ConvNeXt集成并调整网络结构，将模型网络复杂度降低高达80%；(2) 优化Stem和Block结构，增强模型特征表达能力和操作效率；(3) 用通道注意力替换Layer Scale。在ImageNet分类上的实验验证了E-ConvNeXt在准确率-效率平衡方面的优越性：E-ConvNeXt-mini在0.9 GFLOPs下达到78.3%的Top-1准确率；E-ConvNeXt-small在3.1 GFLOPs下达到81.9%的Top-1准确率。在目标检测任务上的迁移学习测试进一步证实了其泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many high-performance networks were not designed with lightweight applicationscenarios in mind from the outset, which has greatly restricted their scope ofapplication. This paper takes ConvNeXt as the research object and significantlyreduces the parameter scale and network complexity of ConvNeXt by integratingthe Cross Stage Partial Connections mechanism and a series of optimizeddesigns. The new network is named E-ConvNeXt, which can maintain high accuracyperformance under different complexity configurations. The three coreinnovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network(CSPNet) with ConvNeXt and adjusting the network structure, which reduces themodel's network complexity by up to 80%; (2) Optimizing the Stem and Blockstructures to enhance the model's feature expression capability and operationalefficiency; (3) Replacing Layer Scale with channel attention. Experimentalvalidation on ImageNet classification demonstrates E-ConvNeXt's superioraccuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transferlearning tests on object detection tasks further confirm its generalizationcapability.</description>
      <author>example@mail.com (Fang Wang, Huitao Li, Wenhan Chao, Zheng Zhuo, Yiran Ji, Chang Peng, Yupeng Sun)</author>
      <guid isPermaLink="false">2508.20955v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Classification under Decision Rule Drift with Application to Optimal Individualized Treatment Rule Estimation</title>
      <link>http://arxiv.org/abs/2508.20942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文将迁移学习分类框架从基于回归函数的方法扩展到决策规则，提出了一种通过贝叶斯决策规则对后验漂移进行建模的新方法。该方法利用贝叶斯决策边界的几何变换，将问题重新表述为低维经验风险最小化问题。研究建立了估计量的一致性，推导了风险边界，并将方法应用于最优个体化治疗规则的估计。通过模拟研究和真实数据分析，证明了该方法优越的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;迁移学习分类框架最初基于回归函数方法，需要扩展到决策规则领域。&lt;h4&gt;目的&lt;/h4&gt;提出一种通过贝叶斯决策规则对后验漂移进行建模的新方法论，并展示其在最优个体化治疗规则估计中的广泛应用。&lt;h4&gt;方法&lt;/h4&gt;利用贝叶斯决策边界的几何变换，将问题重新表述为低维经验风险最小化问题，建立估计量的一致性，并推导风险边界。&lt;h4&gt;主要发现&lt;/h4&gt;在温和的正则条件下，所提出的估计量具有一致性，并且推导出了相应的风险边界。该方法在最优个体化治疗规则估计中表现出广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的模拟研究和真实数据分析，证明了该方法具有优越的性能和鲁棒性，适用于迁移学习分类框架的决策规则扩展。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们将迁移学习分类框架从基于回归函数的方法扩展到决策规则。我们提出了一种通过贝叶斯决策规则对后验漂移进行建模的新方法。通过利用贝叶斯决策边界的几何变换，我们的方法将问题重新表述为低维经验风险最小化问题。在温和的正则条件下，我们建立了估计量的一致性并推导了风险边界。此外，我们通过将方法适应于最优个体化治疗规则的估计，展示了其广泛的适用性。广泛的模拟研究和真实数据分析进一步证明了我们方法的优越性能和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we extend the transfer learning classification framework fromregression function-based methods to decision rules. We propose a novelmethodology for modeling posterior drift through Bayes decision rules. Byexploiting the geometric transformation of the Bayes decision boundary, ourmethod reformulates the problem as a low-dimensional empirical riskminimization problem. Under mild regularity conditions, we establish theconsistency of our estimators and derive the risk bounds. Moreover, weillustrate the broad applicability of our method by adapting it to theestimation of optimal individualized treatment rules. Extensive simulationstudies and analyses of real-world data further demonstrate both superiorperformance and robustness of our approach.</description>
      <author>example@mail.com (Xiaohan Wang, Yang Ning)</author>
      <guid isPermaLink="false">2508.20942v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Accurate Screening of Functional Materials with Machine-Learning Potential and Transfer-Learned Regressions: Heusler Alloy Benchmark</title>
      <link>http://arxiv.org/abs/2508.20556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种机器学习加速的高通量工作流程，用于发现具有大磁晶各向异性能的磁性材料。通过筛选四元和全d-Heusler化合物，结合原子间势和机器学习模型预测，并使用密度泛函理论验证，证明了该方法的高预测精度。&lt;h4&gt;背景&lt;/h4&gt;磁性材料的发现通常需要高通量筛选方法，但传统计算方法效率低下。机器学习可以加速这一过程，但需要确保预测的准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习加速的高通量工作流程，用于高效筛选具有大磁晶各向异性能的稳定磁性材料。&lt;h4&gt;方法&lt;/h4&gt;使用eSEN-30M-OAM原子间势进行结构优化和形成能评估；使用在DxMag Heusler数据库上训练的eSEM模型预测局部磁矩、声子稳定性、磁稳定性和磁晶各向异性能；采用冻结迁移学习策略提高准确性；使用密度泛函理论验证候选化合物；评估不同机器学习原子间势的性能；讨论局部磁矩预测的保真度及其扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;机器学习加速的高通量工作流程能够有效筛选具有大磁晶各向异性能的稳定化合物；密度泛函理论验证证实了高预测精度；评估了不同机器学习原子间势的性能；讨论了局部磁矩预测的保真度及其对其他磁性材料的适用性。&lt;h4&gt;结论&lt;/h4&gt;机器学习加速的高通量工作流程结合了计算效率和预测准确性，是发现新型磁性材料的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种机器学习加速的高通量工作流程，用于发现磁性材料。作为测试案例，我们筛选了四元和全d-Heusler化合物，寻找具有大磁晶各向异能量（E_aniso）的稳定化合物。使用eSEN-30M-OAM原子间势进行结构优化和形成能评估，同时使用在我们的DxMag Heusler数据库上训练的eSEM模型预测局部磁矩、声子稳定性、磁稳定性和E_aniso。采用冻结迁移学习策略提高准确性。通过密度泛函理论验证了机器学习高通量工作流程确定的候选化合物，证实了高预测精度。我们还评估了不同机器学习原子间势的性能，并讨论了局部磁矩预测的保真度及其对其他磁性材料的扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A machine learning-accelerated high-throughput (HTP) workflow for thediscovery of magnetic materials is presented. As a test case, we screenedquaternary and all-$d$ Heusler compounds for stable compounds with largemagnetocrystalline anisotropy energy ($E_{\mathrm{aniso}}$). Structureoptimization and evaluation of formation energy and distance to hull convexwere performed using the eSEN-30M-OAM interatomic potential, while localmagnetic moments, phonon stability, magnetic stability, and$E_{\mathrm{aniso}}$ were predicted by eSEM models trained on our DxMag Heuslerdatabase. A frozen transfer learning strategy was employed to improve accuracy.Candidate compounds identified by the ML-HTP workflow were validated withdensity functional theory, confirming high predictive precision. We alsobenchmark the performance of different MLIPs, and discuss the fidelity of localmagnetic moment prediction and its extension to other magnetic materials.</description>
      <author>example@mail.com (Enda Xiao, Terumasa Tadano)</author>
      <guid isPermaLink="false">2508.20556v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph</title>
      <link>http://arxiv.org/abs/2508.17645v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了3D-AIGC与以人为中心的设计范式之间的表示不兼容问题，通过生成设计操作序列实现了AI与设计师工作流程的对接，提高了人机协作效率。&lt;h4&gt;背景&lt;/h4&gt;3D人工智能生成内容(3D-AIGC)能够快速合成复杂几何形状，但AI生成内容与设计师工作流程之间存在根本性脱节。传统AI框架主要处理网格或神经表示，而设计师使用参数建模工具，这种不兼容性降低了AI对3D行业的实际价值。&lt;h4&gt;目的&lt;/h4&gt;解决AI生成内容与设计师工作流程之间的表示差异，通过生成设计操作序列来弥合这一差距，提高AI在3D设计领域的实用性和人机协作效率。&lt;h4&gt;方法&lt;/h4&gt;将基础建模操作重新表述为可微分单元，实现对连续和离散参数的联合优化；构建带有门控机制的分层图，通过最小化Chamfer距离进行端到端优化；应用多阶段序列长度约束和领域规则惩罚实现无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;生成的操作序列实现了高几何保真度、平滑的网格布线、合理的步骤组合和灵活的编辑能力，完全兼容设计行业的工作流程。&lt;h4&gt;结论&lt;/h4&gt;通过生成设计操作序列，成功连接了AI生成内容与设计师工作流程，使AI能够更好地融入3D设计行业，提高人机协作效率。&lt;h4&gt;翻译&lt;/h4&gt;三维人工智能生成内容(3D-AIGC)的出现使得复杂几何形状的快速合成成为可能。然而，AI生成内容与以人为中心的设计范式之间仍然存在根本性脱节，这种脱节源于表示方法的不兼容性：传统的AI框架主要处理网格或神经表示（例如，NeRF、高斯溅射），而设计师则在参数建模工具中工作。这种脱节降低了AI对3D行业的实际价值，削弱了人机协作的效率。为了解决这种差异，我们专注于生成设计操作序列，这些是有序的建模历史，全面捕捉3D资产的逐步构建过程，并与现代3D软件中设计师的典型工作流程保持一致。我们首先将基础建模操作（例如，拉伸、布尔运算）重新表述为可微分单元，通过基于梯度的学习实现对连续参数（例如，拉伸高度）和离散参数（例如，布尔类型）的联合优化。基于这些可微分操作，构建了一个带有门控机制的分层图，并通过最小化目标几何的Chamfer距离进行端到端优化。多阶段序列长度约束和领域规则惩罚使得无需真实序列监督即可学习紧凑的设计序列。广泛的验证表明，生成的操作序列实现了高几何保真度、平滑的网格布线、合理的步骤组合和灵活的编辑能力，并且完全兼容设计行业。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决AI生成的3D内容与专业3D设计工作流程之间的根本脱节问题。传统AI框架处理网格或神经表示，而设计师使用参数建模工具，这种不兼容性降低了AI在3D行业的实用价值，削弱了人机协作效率。研究显示70%的专业设计师需花费超过3小时修改AI生成的3D内容，且72%的人报告没有节省时间甚至增加了成本。此外，缺乏构建历史迫使艺术家进行密集的顶点操作而非调整设计参数，影响后续物理模拟、角色变形等生产流程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了AI生成内容与专业3D工作流程的不兼容性，然后采用设计操作序列作为统一表示，捕捉3D资产的逐步构建过程。方法设计包括：将基本建模操作重新表述为可微分单元，构建具有门控机制的分层操作图，通过最小化Chamfer距离进行端到端优化。作者借鉴了3D-AIGC系统（如NeRF、扩散模型）和CAD建模（如CSG树）的研究，但明确区分了设计操作与CAD命令的不同，指出设计操作适用于更广泛的3D资产类型，且缺乏大规模数据集支持，因此创新性地提出了零样本方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统离散几何建模操作转化为可微分单元，使基于梯度的学习能同时优化连续和离散参数，生成与设计师工作流程兼容的操作序列。整体流程包括：1)构建可微分操作节点，每个节点处理连续参数(通过链式规则)和离散参数(通过概率分支机制)；2)构建可微分操作图，通过门控机制动态调节各节点影响；3)通过最小化Chamfer距离进行端到端优化；4)针对细分、挤压、倒角、布尔运算等关键操作设计专门的微分实现方案；5)在推理时提取激活节点，生成确定性的操作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出设计操作序列作为3D资产的统一表示，与专业工作流程兼容；2)开发可微分操作框架，使离散操作可优化；3)实现零样本方法，无需大规模训练数据；4)引入门控和概率分支机制处理离散参数；5)针对关键操作创新的微分实现。相比之前工作，本方法不同于传统3D-AIGC系统(生成静态网格而非可编辑序列)，区别于CAD建模(适用于更广泛资产类型)，超越基于MLLM的方法(避免机械重复模式)，解决了设计操作序列数据稀缺问题，实现了与专业设计工作流程的无缝集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过可微分操作图生成人机协作3D资产设计序列的方法，有效 bridging 了AI生成内容与专业3D设计工作流程之间的鸿沟，使AI生成的3D资产具有与设计师工作流程兼容的可编辑操作历史。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of 3D artificial intelligence-generated content (3D-AIGC) hasenabled rapid synthesis of intricate geometries. However, a fundamentaldisconnect persists between AI-generated content and human-centric designparadigms, rooted in representational incompatibilities: conventional AIframeworks predominantly manipulate meshes or neural representations(\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate withinparametric modeling tools. This disconnection diminishes the practical value ofAI for 3D industry, undermining the efficiency of human-AI collaboration. Toresolve this disparity, we focus on generating design operation sequences,which are structured modeling histories that comprehensively capture thestep-by-step construction process of 3D assets and align with designers'typical workflows in modern 3D software. We first reformulate fundamentalmodeling operations (\emph{e.g.}, \emph{Extrude}, \emph{Boolean}) intodifferentiable units, enabling joint optimization of continuous (\emph{e.g.},\emph{Extrude} height) and discrete (\emph{e.g.}, \emph{Boolean} type)parameters via gradient-based learning. Based on these differentiableoperations, a hierarchical graph with gating mechanism is constructed andoptimized end-to-end by minimizing Chamfer Distance to target geometries.Multi-stage sequence length constraint and domain rule penalties enableunsupervised learning of compact design sequences without ground-truth sequencesupervision. Extensive validation demonstrates that the generated operationsequences achieve high geometric fidelity, smooth mesh wiring, rational stepcomposition and flexible editing capacity, with full compatibility withindesign industry.</description>
      <author>example@mail.com (Xiaoyang Huang, Bingbing Ni, Wenjun Zhang)</author>
      <guid isPermaLink="false">2508.17645v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning</title>
      <link>http://arxiv.org/abs/2508.20083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了DisarmRAG，一种针对检索增强生成（RAG）系统的新型攻击方法，通过损害检索器而非知识库来绕过大型语言模型的自我纠正能力，成功实现高攻击成功率且保持隐蔽性。&lt;h4&gt;背景&lt;/h4&gt;检索增强生成（RAG）已成为提高大型语言模型可靠性的标准方法，但先前研究表明RAG系统容易被通过污染知识库进行攻击，误导生成攻击者选择的输出。然而，现代LLMs的自我纠正能力（SCA）可以减轻此类攻击。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的污染范式，损害检索器本身以抑制自我纠正能力（SCA）并强制生成攻击者选择的输出，绕过LLMs的自我纠正机制。&lt;h4&gt;方法&lt;/h4&gt;提出DisarmRAG，一种基于对比学习的模型编辑技术，执行局部和隐蔽的编辑，确保检索器仅针对特定受害者查询返回恶意指令。设计迭代协同优化框架，自动发现能够绕过基于提示的防御的强大指令。&lt;h4&gt;主要发现&lt;/h4&gt;在六个LLMs和三个QA基准上的评估显示，恶意指令的检索接近完美，成功抑制了SCA，并在多种防御提示下实现了超过90%的攻击成功率。被编辑的检索器在几种检测方法下保持隐蔽性。&lt;h4&gt;结论&lt;/h4&gt;以检索器为中心的攻击方法（如DisarmRAG）对RAG系统构成严重威胁，凸显了开发针对检索器的防御措施的迫切性。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成（RAG）已成为提高大型语言模型（LLMs）可靠性的标准方法。先前工作通过污染知识库，证明RAG系统容易被误导生成攻击者选择的输出，从而展示了其脆弱性。然而，本文发现，现代LLMs的强大自我纠正能力（SCA）可以减轻此类攻击，只要正确配置，就能拒绝虚假上下文。这种SCA对试图操纵RAG系统的攻击者构成了重大挑战。与先前主要针对知识库的污染方法不同，我们引入了DisarmRAG，一种新的污染范式，它损害检索器本身以抑制SCA并强制生成攻击者选择的输出。这种损害使攻击者能够直接将反SCA指令嵌入到提供给生成器的上下文中，从而绕过SCA。为此，我们提出了一种基于对比学习的模型编辑技术，执行局部和隐蔽的编辑，确保检索器仅针对特定受害者查询返回恶意指令，同时保持良性检索行为。为了进一步增强攻击，我们设计了一个迭代协同优化框架，能够自动发现能够绕过基于提示的防御的强大指令。我们在六个LLMs和三个QA基准上广泛评估了DisarmRAG。结果表明，恶意指令的检索接近完美，成功抑制了SCA，并在多种防御提示下实现了超过90%的攻击成功率。此外，被编辑的检索器在几种检测方法下保持隐蔽性，凸显了对以检索器为中心的防御的迫切需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-Augmented Generation (RAG) has become a standard approach forimproving the reliability of large language models (LLMs). Prior workdemonstrates the vulnerability of RAG systems by misleading them intogenerating attacker-chosen outputs through poisoning the knowledge base.However, this paper uncovers that such attacks could be mitigated by the strong\textit{self-correction ability (SCA)} of modern LLMs, which can reject falsecontext once properly configured. This SCA poses a significant challenge forattackers aiming to manipulate RAG systems.  In contrast to previous poisoning methods, which primarily target theknowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm thatcompromises the retriever itself to suppress the SCA and enforceattacker-chosen outputs. This compromisation enables the attacker tostraightforwardly embed anti-SCA instructions into the context provided to thegenerator, thereby bypassing the SCA. To this end, we present acontrastive-learning-based model editing technique that performs localized andstealthy edits, ensuring the retriever returns a malicious instruction only forspecific victim queries while preserving benign retrieval behavior. To furtherstrengthen the attack, we design an iterative co-optimization framework thatautomatically discovers robust instructions capable of bypassing prompt-baseddefenses. We extensively evaluate DisarmRAG across six LLMs and three QAbenchmarks. Our results show near-perfect retrieval of malicious instructions,which successfully suppress SCA and achieve attack success rates exceeding 90\%under diverse defensive prompts. Also, the edited retriever remains stealthyunder several detection methods, highlighting the urgent need forretriever-centric defenses.</description>
      <author>example@mail.com (Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang)</author>
      <guid isPermaLink="false">2508.20083v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
  <item>
      <title>ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.20057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2508.17920&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ProMSC-MIS的新型基于提示的多模态语义通信框架，用于多光谱图像分割，能够在带宽受限信道上高效传输空间对齐的RGB和热图像。&lt;h4&gt;背景&lt;/h4&gt;多模态语义通信通过整合跨模态的互补信息，具有提高下游任务性能的巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的通信框架，用于多光谱图像分割任务，在带宽受限的信道上传输空间对齐的RGB和热图像。&lt;h4&gt;方法&lt;/h4&gt;1. 利用提示学习和对比学习预训练单模态语义编码器，通过一种模态特征作为另一种模态的提示来学习互补语义表示；2. 设计结合交叉注意力和挤压-激励(SE)网络的语义融合模块，有效融合跨模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;ProMSC-MIS显著优于传统图像传输与先进分割方法的结合；在相同分割性能下，减少50%-70%的信道带宽；降低26%的存储开销和37%的计算复杂度；消融研究验证了预训练和语义融合策略的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方案非常适合自动驾驶和夜间监控等应用场景。&lt;h4&gt;翻译&lt;/h4&gt;多模态语义通信通过整合跨模态的互补信息，在提高下游任务性能方面具有巨大潜力。本文介绍了ProMSC-MIS，一种用于多光谱图像分割的新型基于提示的多模态语义通信框架。它能够在带宽受限的信道上高效传输空间对齐的RGB和热图像。我们的框架有两个主要的设计创新。首先，通过利用提示学习和对比学习，单模态语义编码器被预训练，通过使用一种模态的特征作为另一种模态的提示，来学习多样化和互补的语义表示。其次，设计了一个语义融合模块，结合了交叉注意力和挤压-激励(SE)网络，以有效融合跨模态特征。实验结果表明，ProMSC-MIS显著优于传统的图像传输与最先进的分割方法相结合的性能。值得注意的是，在相同的分割性能下，它将所需的信道带宽减少了50%--70%，同时将存储开销和计算复杂度分别降低了26%和37%。消融研究也验证了所提出的预训练和语义融合策略的有效性。我们的方案非常适合自动驾驶和夜间监控等应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal semantic communication has great potential to enhance downstreamtask performance by integrating complementary information across modalities.This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal SemanticCommunication framework for Multi-Spectral Image Segmentation. It enablesefficient task-oriented transmission of spatially aligned RGB and thermalimages over band-limited channels. Our framework has two main design novelties.First, by leveraging prompt learning and contrastive learning, unimodalsemantic encoders are pre-trained to learn diverse and complementary semanticrepresentations by using features from one modality as prompts for another.Second, a semantic fusion module that combines cross-attention mechanism andsqueeze-and-excitation (SE) networks is designed to effectively fusecross-modal features. Experimental results demonstrate that ProMSC-MISsubstantially outperforms conventional image transmission combined withstate-of-the-art segmentation methods. Notably, it reduces the required channelbandwidth by 50%--70% at the same segmentation performance, while alsodecreasing the storage overhead and computational complexity by 26% and 37%,respectively. Ablation studies also validate the effectiveness of the proposedpre-training and semantic fusion strategies. Our scheme is highly suitable forapplications such as autonomous driving and nighttime surveillance.</description>
      <author>example@mail.com (Haoshuo Zhang, Yufei Bo, Meixia Tao)</author>
      <guid isPermaLink="false">2508.20057v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.19574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MPAMatch的新型病理图像分割框架，通过多模态原型引导的监督范式和双重对比学习策略，解决了病理图像分割中语义边界模糊和标注成本高的问题，显著提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;病理图像分割面临语义边界模糊和像素级标注成本高的挑战。现有基于一致性正则化的半监督方法主要依赖图像模态内的扰动一致性，难以捕捉高级语义先验，特别是在结构复杂的病理图像中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉结构和语义信息的病理图像分割方法，提高分割精度，减少对大量标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出MPAMatch框架，采用像素级对比学习，在多模态原型引导的监督范式下工作。核心创新是双重对比学习方案：图像原型与像素标签之间，以及文本原型与像素标签之间。同时，改进了TransUNet架构，用病理预训练的基础模型Uni替换了ViT主干。&lt;h4&gt;主要发现&lt;/h4&gt;双重对比学习策略增强了对未标记样本的判别能力；首次将文本原型监督引入分割，改善了语义边界建模；改进的架构能有效提取病理相关特征。&lt;h4&gt;结论&lt;/h4&gt;MPAMatch在多个数据集上优于最先进方法，验证了其在结构和语义建模方面的双重优势，为病理图像分割提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;病理图像分割面临诸多挑战，主要是由于语义边界模糊和像素级标注的高成本。尽管最近基于一致性正则化的半监督方法（如UniMatch）已取得显著进展，但它们主要依赖图像模态内的基于扰动的一致性，难以捕捉高级语义先验，特别是在结构复杂的病理图像中。为解决这些限制，我们提出了MPAMatch - 一种新的分割框架，它在多模态原型引导的监督范式下执行像素级对比学习。MPAMatch的核心创新在于图像原型与像素标签之间以及文本原型与像素标签之间的双重对比学习方案，同时在结构和语义级别提供监督。这种从粗到细的监督策略不仅增强了对未标记样本的判别能力，而且首次将文本原型监督引入分割，显著改善了语义边界建模。此外，我们通过将其ViT主干替换为病理预训练的基础模型（Uni）来重建经典的分割架构（TransUNet），从而实现更有效地提取病理相关特征。在GLAS、EBHI-SEG-GLAND、EBHI-SEG-CANCER和KPI上的大量实验表明，MPAMatch优于最先进的方法，验证了其在结构和语义建模方面的双重优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pathological image segmentation faces numerous challenges, particularly dueto ambiguous semantic boundaries and the high cost of pixel-level annotations.Although recent semi-supervised methods based on consistency regularization(e.g., UniMatch) have made notable progress, they mainly rely onperturbation-based consistency within the image modality, making it difficultto capture high-level semantic priors, especially in structurally complexpathology images. To address these limitations, we propose MPAMatch - a novelsegmentation framework that performs pixel-level contrastive learning under amultimodal prototype-guided supervision paradigm. The core innovation ofMPAMatch lies in the dual contrastive learning scheme between image prototypesand pixel labels, and between text prototypes and pixel labels, providingsupervision at both structural and semantic levels. This coarse-to-finesupervisory strategy not only enhances the discriminative capability onunlabeled samples but also introduces the text prototype supervision intosegmentation for the first time, significantly improving semantic boundarymodeling. In addition, we reconstruct the classic segmentation architecture(TransUNet) by replacing its ViT backbone with a pathology-pretrainedfoundation model (Uni), enabling more effective extraction ofpathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-artmethods, validating its dual advantages in structural and semantic modeling.</description>
      <author>example@mail.com (Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu)</author>
      <guid isPermaLink="false">2508.19574v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Language Gaps: Enhancing Few-Shot Language Adaptation</title>
      <link>http://arxiv.org/abs/2508.19464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoLAP方法通过整合对比学习和跨语言表示，解决了多语言NLP中语言资源不平衡的问题，实现了从高资源语言到低资源语言的高效知识转移，在有限数据条件下优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;语言资源不平衡是多语言NLP的挑战，高资源语言拥有大量数据，而低资源语言缺乏足够数据进行有效训练。&lt;h4&gt;目的&lt;/h4&gt;解决高资源语言和低资源语言之间的数据差距，促进任务特定知识从高资源语言向低资源语言的转移。&lt;h4&gt;方法&lt;/h4&gt;提出对比语言提示对齐(CoLAP)方法，结合对比学习和跨语言表示，实现数据高效的跨语言知识转移。&lt;h4&gt;主要发现&lt;/h4&gt;CoLAP在数据效率方面具有优势，能够快速适应新语言并减少对大型标记数据集的需求。在自然语言推理和关系抽取任务上，CoLAP优于少样本跨语言迁移基线和上下文学习方法，即使在数据有限的情况下也能有效缩小跨语言性能差距。&lt;h4&gt;结论&lt;/h4&gt;CoLAP为开发更高效的多语言NLP技术做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;语言资源的不平衡给多语言自然语言处理带来了挑战，高资源语言得益于大量数据，而低资源语言缺乏有效训练所需的足够数据。我们的对比语言提示对齐(CoLAP)方法通过整合对比学习和跨语言表示来解决这一差距，促进从高资源语言到低资源语言的任务特定知识转移。我们方法的主要优势是其数据效率，能够快速适应新语言并减少对大型标记数据集的需求。我们在自然语言理解任务(包括自然语言推理和关系提取)上对多语言仅编码器和仅解码器语言模型进行了实验，评估了在高资源和低资源语言上的性能。我们的结果表明，即使在可用数据有限的情况下，CoLAP也优于少样本跨语言迁移基线和上下文学习。这有效地缩小了跨语言性能差距，为开发更高效的多语言NLP技术做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The disparity in language resources poses a challenge in multilingual NLP,with high-resource languages benefiting from extensive data, while low-resourcelanguages lack sufficient data for effective training. Our Contrastive LanguageAlignment with Prompting (CoLAP) method addresses this gap by integratingcontrastive learning with cross-lingual representations, facilitatingtask-specific knowledge transfer from high-resource to lower-resourcelanguages. The primary advantage of our approach is its data efficiency,enabling rapid adaptation to new languages and reducing the need for largelabeled datasets. We conduct experiments with multilingual encoder-only anddecoder-only language models on natural language understanding tasks, includingnatural language inference and relation extraction, evaluating performanceacross both high- and low-resource languages. Our results demonstrate thatCoLAP outperforms few-shot cross-lingual transfer baselines and in-contextlearning, even with limited available data. This effectively narrows thecross-lingual performance gap, contributing to the development of moreefficient multilingual NLP techniques.</description>
      <author>example@mail.com (Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens)</author>
      <guid isPermaLink="false">2508.19464v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification</title>
      <link>http://arxiv.org/abs/2508.19424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的无监督对比学习框架，用于基于突变数据对43种癌症类型进行聚类，发现了与已知生物学过程一致的癌症聚类模式。&lt;h4&gt;背景&lt;/h4&gt;理解泛癌症突变景观对了解肿瘤发生的分子机制至关重要。虽然患者级别的机器学习技术已被广泛用于识别肿瘤亚型，但队列级别的癌症聚类（基于共享分子特征对整个癌症类型进行分组）主要依赖经典统计方法。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的无监督对比学习框架，基于编码突变数据对43种癌症类型进行聚类，使用来自COSMIC数据库的数据。&lt;h4&gt;方法&lt;/h4&gt;为每种癌症类型构建两种互补的突变特征：基因级特征（捕获最频繁突变基因中的核苷酸替换模式）和染色体级特征（表示染色体间归一化的替换频率）。使用TabNet编码器对这些双视图进行编码，并通过多尺度对比学习目标（NT-Xent损失）进行优化，以学习统一的癌症类型嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;生成的潜在表示产生了生物学上有意义的癌症类型聚类，这些聚类与已知的突变过程和组织来源一致。&lt;h4&gt;结论&lt;/h4&gt;这是对比学习首次应用于队列级别的癌症聚类，提供了一种可扩展且可解释的框架，用于突变驱动的癌症亚型分型。&lt;h4&gt;翻译&lt;/h4&gt;动机：理解泛癌症突变景观为理解肿瘤发生的分子机制提供了关键见解。虽然患者级别的机器学习技术已被广泛用于识别肿瘤亚型，但队列级别的聚类（基于共享分子特征对整个癌症类型进行分组）主要依赖经典统计方法。结果：在本研究中，我们引入了一种新的无监督对比学习框架，基于来自COSMIC数据库的编码突变数据对43种癌症类型进行聚类。对于每种癌症类型，我们构建了两种互补的突变特征：捕获最频繁突变基因中核苷酸替换模式的基因级特征，以及表示染色体间归一化替换频率的染色体级特征。这些双视图使用TabNet编码器进行编码，并通过多尺度对比学习目标（NT-Xent损失）进行优化，以学习统一的癌症类型嵌入。我们证明，生成的潜在表示产生了生物学上有意义的癌症类型聚类，与已知的突变过程和组织来源一致。我们的工作代表了对比学习首次应用于队列级别的癌症聚类，为突变驱动的癌症亚型分型提供了一种可扩展且可解释的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivation. Understanding the pan-cancer mutational landscape offers criticalinsights into the molecular mechanisms underlying tumorigenesis. Whilepatient-level machine learning techniques have been widely employed to identifytumor subtypes, cohort-level clustering, where entire cancer types are groupedbased on shared molecular features, has largely relied on classical statisticalmethods.  Results. In this study, we introduce a novel unsupervised contrastivelearning framework to cluster 43 cancer types based on coding mutation dataderived from the COSMIC database. For each cancer type, we construct twocomplementary mutation signatures: a gene-level profile capturing nucleotidesubstitution patterns across the most frequently mutated genes, and achromosome-level profile representing normalized substitution frequenciesacross chromosomes. These dual views are encoded using TabNet encoders andoptimized via a multi-scale contrastive learning objective (NT-Xent loss) tolearn unified cancer-type embeddings. We demonstrate that the resulting latentrepresentations yield biologically meaningful clusters of cancer types,aligning with known mutational processes and tissue origins. Our workrepresents the first application of contrastive learning to cohort-level cancerclustering, offering a scalable and interpretable framework for mutation-drivencancer subtyping.</description>
      <author>example@mail.com (Yifan Dou, Adam Khadre, Ruben C Petreaca, Golrokh Mirzaei)</author>
      <guid isPermaLink="false">2508.19424v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning</title>
      <link>http://arxiv.org/abs/2508.19009v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FedProtoKD的异构联邦学习方法，通过增强的双知识蒸馏机制和基于对比学习的可训练服务器原型来解决原型边缘缩小问题，显著提高了系统性能。&lt;h4&gt;背景&lt;/h4&gt;异构联邦学习(HFL)因能够适应不同客户端的多样模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的有前景解决方案出现，但这些方法通常在服务器上使用加权平均来聚合原型，导致次优的全局知识和原型缩小问题。&lt;h4&gt;目的&lt;/h4&gt;解决异构联邦学习中原型边缘缩小的问题，提高模型在极度非独立同分布数据下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FedProtoKD，使用增强的双知识蒸馏机制利用客户端的logits和原型特征表示；通过基于对比学习的可训练服务器原型和类自适应原型边缘来解决原型边缘缩小问题；评估公共样本的重要性基于样本与其类代表性原型的接近程度。&lt;h4&gt;主要发现&lt;/h4&gt;FedProtoKD在各种设置下实现了平均1.13%到最高34.13%的准确率提升，显著优于现有的最先进HFL方法。&lt;h4&gt;结论&lt;/h4&gt;FedProtoKD有效解决了异构联邦学习中的原型边缘缩小问题，在模型异构且数据分布极度非独立同分布的情况下显著提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;异构联邦学习(HFL)因其能够适应不同客户端的多样模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的有前景解决方案出现，为HFL研究的新进展铺平了道路。这种方法专注于仅在异构客户端之间共享类代表性原型。然而，这些原型通常在服务器上使用加权平均进行聚合，导致次优的全局知识；这会导致聚合原型的缩小，在模型异构且数据分布极度非独立同分布的情况下对模型性能产生负面影响。我们在异构联邦学习环境中提出了FedProtoKD，使用增强的双知识蒸馏机制来提高系统性能，利用客户端的logits和原型特征表示。我们旨在利用基于类自适应原型边缘的对比学习可训练服务器原型来解决原型边缘缩小问题。此外，我们通过评估样本与其类代表性原型的接近程度来评估公共样本的重要性，从而提高学习性能。FedProtoKD在各种设置下实现了平均1.13%到最高34.13%的准确率提升，并显著优于现有的最先进HFL方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous Federated Learning (HFL) has gained attention for its abilityto accommodate diverse models and heterogeneous data across clients.Prototype-based HFL methods emerge as a promising solution to addressstatistical heterogeneity and privacy challenges, paving the way for newadvancements in HFL research. This method focuses on sharing onlyclass-representative prototypes among heterogeneous clients. However, theseprototypes are often aggregated on the server using weighted averaging, leadingto sub-optimal global knowledge; these cause the shrinking of aggregatedprototypes, which negatively affects the model performance in scenarios whenmodels are heterogeneous and data distributions are extremely non-IID. Wepropose FedProtoKD in a Heterogeneous Federated Learning setting, using anenhanced dual-knowledge distillation mechanism to improve the systemperformance with clients' logits and prototype feature representation. We aimto resolve the prototype margin-shrinking problem using a contrastivelearning-based trainable server prototype by leveraging a class-wise adaptiveprototype margin. Furthermore, we assess the importance of public samples usingthe closeness of the sample's prototype to its class representative prototypes,which enhances learning performance. FedProtoKD achieved average improvementsof 1.13% up to 34.13% accuracy across various settings and significantlyoutperforms existing state-of-the-art HFL methods.</description>
      <author>example@mail.com (Md Anwar Hossen, Fatema Siddika, Wensheng Zhang, Anuj Sharma, Ali Jannesari)</author>
      <guid isPermaLink="false">2508.19009v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</title>
      <link>http://arxiv.org/abs/2508.15298v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为时间提示对齐(TPA)的新方法，用于在超声视频中检测胎儿先天性心脏病(CHD)，通过结合时间建模、提示感知对比学习和不确定性量化，显著提高了分类准确性和校准性能。&lt;h4&gt;背景&lt;/h4&gt;先天性心脏病(CHD)在超声视频检测中受到图像噪声和探头定位可变性的影响。虽然自动化方法可以减少操作者依赖性，但当前的机器学习方法往往忽略时间信息，仅限于二元分类，且未考虑预测校准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用时间信息、支持多分类且考虑预测校准的方法，用于胎儿心脏超声视频中的CHD检测。&lt;h4&gt;方法&lt;/h4&gt;TPA方法利用基础图像-文本模型和提示感知对比学习，通过图像编码器提取视频子片段各帧特征，使用可训练的时间提取器聚合特征以捕获心脏运动，并通过边际铰链对比损失将视频表示与类特定文本提示对齐。同时引入条件变分自编码器样式调制(CVAESM)模块学习潜在样式向量来调制嵌入并量化分类不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;TPA在私有CHD检测数据集和EchoNet-Dynamic公共数据集上均表现优异，实现了85.40%的宏观F1分数，同时将预期校准误差降低5.38%，自适应ECE降低6.8%。在EchoNet-Dynamic的三类任务中，将宏观F1提高4.73%，从53.89%提升至58.62%。&lt;h4&gt;结论&lt;/h4&gt;时间提示对齐(TPA)是一个有效的胎儿先天性心脏病(CHD)分类框架，集成了时间建模、提示感知对比学习和不确定性量化，显著提升了超声视频诊断的性能和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;先天性心脏病(CHD)在超声视频检测中受到图像噪声和探头定位可变性的阻碍。虽然自动化方法可以减少操作者依赖性，但当前的机器学习方法往往忽略时间信息，仅限于二元分类，且未考虑预测校准。我们提出了时间提示对齐(TPA)，一种利用基础图像-文本模型和提示感知对比学习的方法，用于对胎儿心脏超声视频中的CHD进行分类。TPA使用图像编码器从视频子片段的每一帧提取特征，使用可训练的时间提取器聚合它们以捕获心脏运动，并通过边际铰链对比损失将视频表示与类特定的文本提示对齐。为了提高临床可靠性的校准，我们引入了条件变分自编码器样式调制(CVAESM)模块，它学习一个潜在样式向量来调制嵌入并量化分类不确定性。在CHD检测的私有数据集和用于收缩功能障碍的大型公共数据集EchoNet-Dynamic上评估，TPA实现了CHD诊断最先进的宏观F1分数85.40%，同时将预期校准误差降低5.38%，自适应ECE降低6.8%。在EchoNet-Dynamic的三类任务中，它将宏观F1提高4.73%(从53.89%到58.62%)。时间提示对齐(TPA)是一个用于胎儿先天性心脏病(CHD)分类的框架，集成了时间建模、提示感知对比学习和不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Congenital heart defect (CHD) detection in ultrasound videos is hindered byimage noise and probe positioning variability. While automated methods canreduce operator dependence, current machine learning approaches often neglecttemporal information, limit themselves to binary classification, and do notaccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),a method leveraging foundation image-text model and prompt-aware contrastivelearning to classify fetal CHD on cardiac ultrasound videos. TPA extractsfeatures from each frame of video subclips using an image encoder, aggregatesthem with a trainable temporal extractor to capture heart motion, and alignsthe video representation with class-specific text prompts via a margin-hingecontrastive loss. To enhance calibration for clinical reliability, we introducea Conditional Variational Autoencoder Style Modulation (CVAESM) module, whichlearns a latent style vector to modulate embeddings and quantifiesclassification uncertainty. Evaluated on a private dataset for CHD detectionand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPAachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, whilealso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. OnEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenitalheart defect (CHD) classification in ultrasound videos that integrates temporalmodeling, prompt-aware contrastive learning, and uncertainty quantification.</description>
      <author>example@mail.com (Darya Taratynova, Alya Almsouti, Beknur Kalmakhanbet, Numan Saeed, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2508.15298v3</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了SDGNN，一种基于结构多样性的无参数图神经网络框架，通过统一的消息传递机制同时捕捉邻域结构异质性和特征语义稳定性，无需额外参数，在各种挑战条件下优于主流GNN。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在结构化数据建模任务中表现出色，但主流方法依赖大量可训练参数和固定聚合规则，难以适应强结构异质性和复杂特征分布的图数据，导致节点表示过度平滑和语义退化。&lt;h4&gt;目的&lt;/h4&gt;解决现有GNN方法在处理结构异质性和复杂特征分布时的局限性，提出一种无需额外参数的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;提出SDGNN(结构多样性图神经网络)，受结构多样性理论启发，设计统一的多样性结构消息传递机制，同时捕捉邻域结构异质性和特征语义稳定性，不引入额外参数，利用结构驱动和特征驱动两个视角的互补建模。&lt;h4&gt;主要发现&lt;/h4&gt;在8个公共基准数据集和PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域转移等挑战条件下持续优于主流GNN方法。&lt;h4&gt;结论&lt;/h4&gt;SDGNN为无参数图神经网络设计提供了新的理论视角和通用方法，验证了结构多样性作为图表示学习中核心信号的重要性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在结构化数据建模任务中表现出色，如节点分类。然而，主流方法通常依赖大量可训练参数和固定聚合规则，难以适应具有强结构异质性和复杂特征分布的图数据。这通常导致节点表示过度平滑和语义退化。为解决这些问题，本文提出了一种基于结构多样性的无参数图神经网络框架，即SDGNN(结构多样性图神经网络)。该框架受结构多样性理论启发，设计了统一的多样性结构消息传递机制，同时捕捉邻域结构的异质性和特征语义的稳定性，无需引入额外可训练参数。与传统参数化方法不同，SDGNN不依赖复杂模型训练，而是利用结构驱动和特征驱动两个视角的互补建模，从而有效提高跨数据集和场景的适应性。实验结果表明，在8个公共基准数据集和一个跨学科的PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等具有挑战性的条件下持续优于主流GNN。这项工作为无参数图神经网络的设计提供了新的理论视角和通用方法，并进一步验证了结构多样性作为图表示学习中核心信号的重要性。为便于复现和进一步研究，SDGNN的完整实现已在以下网址发布：https://github.com/mingyue15694/SGDNN/tree/main&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable performance in structureddata modeling tasks such as node classification. However, mainstream approachesgenerally rely on a large number of trainable parameters and fixed aggregationrules, making it difficult to adapt to graph data with strong structuralheterogeneity and complex feature distributions. This often leads toover-smoothing of node representations and semantic degradation. To addressthese issues, this paper proposes a parameter-free graph neural networkframework based on structural diversity, namely SDGNN (Structural-DiversityGraph Neural Network). The framework is inspired by structural diversity theoryand designs a unified structural-diversity message passing mechanism thatsimultaneously captures the heterogeneity of neighborhood structures and thestability of feature semantics, without introducing additional trainableparameters. Unlike traditional parameterized methods, SDGNN does not rely oncomplex model training, but instead leverages complementary modeling from bothstructure-driven and feature-driven perspectives, thereby effectively improvingadaptability across datasets and scenarios. Experimental results show that oneight public benchmark datasets and an interdisciplinary PubMed citationnetwork, SDGNN consistently outperforms mainstream GNNs under challengingconditions such as low supervision, class imbalance, and cross-domain transfer.This work provides a new theoretical perspective and general approach for thedesign of parameter-free graph neural networks, and further validates theimportance of structural diversity as a core signal in graph representationlearning. To facilitate reproducibility and further research, the fullimplementation of SDGNN has been released at:https://github.com/mingyue15694/SGDNN/tree/main</description>
      <author>example@mail.com (Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu)</author>
      <guid isPermaLink="false">2508.19884v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised structured object representation learning</title>
      <link>http://arxiv.org/abs/2508.19864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种自监督学习方法，通过结合语义分组、实例级分离和层次结构来逐步构建结构化的视觉表征，并在目标检测任务上取得了优于最先进方法的结果。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为学习视觉表征的强大技术，但现有的自监督方法在全局图像理解方面表现出色，却在捕捉场景中的结构化表征方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;提出一种自监督方法，能够逐步构建结构化的视觉表征，并在密集预测任务中提高性能，特别是在目标检测任务上取得更好的效果。&lt;h4&gt;方法&lt;/h4&gt;基于一种新颖的ProtoScale模块，该方法能够捕捉多个空间尺度的视觉元素。与DINO等依赖随机裁剪和全局嵌入的常见策略不同，该方法在增强视图之间保留完整的场景上下文。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法学习到了以对象为中心的表征，增强了有监督目标检测性能，并且即使在使用有限的标注数据和更少的微调周期进行训练时，也优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合语义分组、实例级分离和层次结构，并利用ProtoScale模块捕捉多尺度视觉元素，该方法在目标检测任务上取得了优异的性能，特别是在数据有限的情况下。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习（SSL）已成为学习视觉表征的强大技术。虽然最近的SSL方法在全局图像理解方面取得了很好的结果，但它们在捕捉场景中的结构化表征方面存在局限。在这项工作中，我们提出了一种自监督方法，通过结合语义分组、实例级分离和层次结构来逐步构建结构化的视觉表征。我们的方法基于一种新颖的ProtoScale模块，能够捕捉多个空间尺度的视觉元素。与DINO等依赖随机裁剪和全局嵌入的常见策略不同，我们在增强视图之间保留完整的场景上下文，以提高密集预测任务的性能。我们使用多个数据集（COCO和UA-DETRAC）的组合子集在下游目标检测任务上验证了我们的方法。实验结果表明，我们的方法学习到了以对象为中心的表征，增强了有监督目标检测性能，并且即使在使用有限的标注数据和更少的微调周期进行训练时，也优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful technique forlearning visual representations. While recent SSL approaches achieve strongresults in global image understanding, they are limited in capturing thestructured representation in scenes. In this work, we propose a self-supervisedapproach that progressively builds structured visual representations bycombining semantic grouping, instance level separation, and hierarchicalstructuring. Our approach, based on a novel ProtoScale module, captures visualelements across multiple spatial scales. Unlike common strategies like DINOthat rely on random cropping and global embeddings, we preserve full scenecontext across augmented views to improve performance in dense predictiontasks. We validate our method on downstream object detection tasks using acombined subset of multiple datasets (COCO and UA-DETRAC). Experimental resultsshow that our method learns object centric representations that enhancesupervised object detection and outperform the state-of-the-art methods, evenwhen trained with limited annotated data and fewer fine-tuning epochs.</description>
      <author>example@mail.com (Oussama Hadjerci, Antoine Letienne, Mohamed Abbas Hedjazi, Adel Hafiane)</author>
      <guid isPermaLink="false">2508.19864v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning</title>
      <link>http://arxiv.org/abs/2508.19730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于面部基础模型的鲁棒视频深度伪造检测框架，通过利用自监督模型和多种深度伪造数据集的微调，结合三元组损失和基于归因的监督方案，显著提升了模型在现实世界场景中的检测能力。&lt;h4&gt;背景&lt;/h4&gt;随着深度伪造技术的日益逼真和普及，媒体真实性和信息完整性面临严重挑战。现有的深度伪造检测模型往往难以在其训练分布之外泛化，特别是在应用于现实世界中的媒体内容时。&lt;h4&gt;目的&lt;/h4&gt;开发一个具有强泛化能力的鲁棒视频深度伪造检测框架，能够在现实世界场景中有效识别深度伪造内容。&lt;h4&gt;方法&lt;/h4&gt;基于FSFM（一种在真实面部数据上训练的自监督模型）构建，使用包含面部交换和面部重演操作的深度伪造数据集集合进行微调；训练中融入三元组损失的变体，引导模型在真实和伪造样本之间产生更具区分性的嵌入；探索基于归因的监督方案，将深度伪造按操作类型或源数据集分类，以评估它们对泛化的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的评估基准上的广泛实验证明了该方法的有效性，特别是在具有挑战性的现实世界场景中表现优异。&lt;h4&gt;结论&lt;/h4&gt;提出的方法通过结合面部基础模型的优势、多数据集微调和创新的训练策略，显著提升了深度伪造检测的泛化能力，为应对日益复杂的深度伪造威胁提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着深度伪造技术的日益逼真和普及，媒体真实性和信息完整性引起了严重关切。尽管最近有所进展，但深度伪造检测模型往往难以在其训练分布之外泛化，特别是当应用于现实世界中发现的媒体内容时。在这项工作中，我们提出了一个具有强泛化能力的鲁棒视频深度伪造检测框架，利用了面部基础模型学习到的丰富面部表示。我们的方法建立在FSFM之上，这是一个在真实面部数据上训练的自监督模型，并使用包含面部交换和面部重演操作的深度伪造数据集集合进行了进一步微调。为了增强判别能力，我们在训练中融入了三元组损失的变体，引导模型在真实和伪造样本之间产生更具区分性的嵌入。此外，我们探索了基于归因的监督方案，其中深度伪造按操作类型或源数据集分类，以评估它们对泛化的影响。在多样化评估基准上的广泛实验证明了我们方法的有效性，特别是在具有挑战性的现实世界场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746275.3762208&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing realism and accessibility of deepfakes have raised criticalconcerns about media authenticity and information integrity. Despite recentadvances, deepfake detection models often struggle to generalize beyond theirtraining distributions, particularly when applied to media content found in thewild. In this work, we present a robust video deepfake detection framework withstrong generalization that takes advantage of the rich facial representationslearned by face foundation models. Our method is built on top of FSFM, aself-supervised model trained on real face data, and is further fine-tunedusing an ensemble of deepfake datasets spanning both face-swapping andface-reenactment manipulations. To enhance discriminative power, we incorporatetriplet loss variants during training, guiding the model to produce moreseparable embeddings between real and fake samples. Additionally, we exploreattribution-based supervision schemes, where deepfakes are categorized bymanipulation type or source dataset, to assess their impact on generalization.Extensive experiments across diverse evaluation benchmarks demonstrate theeffectiveness of our approach, especially in challenging real-world scenarios.</description>
      <author>example@mail.com (Stelios Mylonas, Symeon Papadopoulos)</author>
      <guid isPermaLink="false">2508.19730v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation</title>
      <link>http://arxiv.org/abs/2508.19591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为个性化局部-全局协作(PLGC)的新策略，用于解决联邦推荐系统中的嵌入退化问题，特别是维度坍缩问题。&lt;h4&gt;背景&lt;/h4&gt;中心化推荐系统因收集用户隐私数据而面临隐私泄露问题，联邦推荐系统(FedRec)成为一种有前景的替代方案，但存在嵌入退化问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种模型无关的FedRec策略，加强个性化嵌入效用，并首次解决联邦推荐中的维度坍缩问题。&lt;h4&gt;方法&lt;/h4&gt;PLGC将冻结的全局项目嵌入表纳入本地设备，基于神经正切核策略动态平衡局部和全局信息，在前向推理过程中优化个性化表示，并通过对比目标函数减少嵌入冗余。&lt;h4&gt;主要发现&lt;/h4&gt;PLGC可有效缓解嵌入退化问题，在五个真实世界数据集上的实验证明其有效性和适应性，优于各种基线算法。&lt;h4&gt;结论&lt;/h4&gt;PLGC是一种模型无关的个性化训练策略，可应用于现有基线以改善联邦推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;中心化推荐系统因需要收集用户行为和其他私人数据而面临隐私泄露问题。因此，联邦推荐系统(FedRec)已成为一种有前景的方法，服务器上有一个聚合的全局模型。然而，由于稀疏交互和异质偏好的存在，这种分布式训练范式因次优个性化和维度坍缩而导致嵌入退化。为此，我们提出了一种新颖的FedRec模型无关策略来增强个性化嵌入效用，称为个性化局部-全局协作(PLGC)。这是联邦推荐领域首次研究缓解维度坍缩问题的工作。特别是，我们将冻结的全局项目嵌入表纳入本地设备。基于动态平衡局部和全局信息的神经正切核策略，PLGC在前向推理过程中优化个性化表示，最终收敛到用户特定的偏好。此外，PLGC采用对比目标函数通过消除维度间的依赖关系来减少嵌入冗余，从而改进后向表示学习过程。我们引入PLGC作为联邦推荐的模型无关个性化训练策略，可应用于现有基线以缓解嵌入退化。在五个真实世界数据集上的广泛实验证明了PLGC的有效性和适应性，其性能优于各种基线算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Centralized recommender systems encounter privacy leakage due to the need tocollect user behavior and other private data. Hence, federated recommendersystems (FedRec) have become a promising approach with an aggregated globalmodel on the server. However, this distributed training paradigm suffers fromembedding degradation caused by suboptimal personalization and dimensionalcollapse, due to the existence of sparse interactions and heterogeneouspreferences. To this end, we propose a novel model-agnostic strategy for FedRecto strengthen the personalized embedding utility, which is called PersonalizedLocal-Global Collaboration (PLGC). It is the first research in federatedrecommendation to alleviate the dimensional collapse issue. Particularly, weincorporate the frozen global item embedding table into local devices. Based ona Neural Tangent Kernel strategy that dynamically balances local and globalinformation, PLGC optimizes personalized representations during forwardinference, ultimately converging to user-specific preferences. Additionally,PLGC carries on a contrastive objective function to reduce embedding redundancyby dissolving dependencies between dimensions, thereby improving the backwardrepresentation learning process. We introduce PLGC as a model-agnosticpersonalized training strategy for federated recommendations that can beapplied to existing baselines to alleviate embedding degradation. Extensiveexperiments on five real-world datasets have demonstrated the effectiveness andadaptability of PLGC, which outperforms various baseline algorithms.</description>
      <author>example@mail.com (Jiakui Shen, Yunqi Mi, Guoshuai Zhao, Jialie Shen, Xueming Qian)</author>
      <guid isPermaLink="false">2508.19591v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.19567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于反事实推理的奖励模型，用于解决强化学习与人类反馈(RLHF)中的偏见问题。该模型通过因果推断和多模态表征学习提供无监督的、具有偏见韧性的奖励信号，并在真假新闻检测任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;在基于人类反馈的强化学习(RLHF)中，奖励模型能够高效学习和放大多模态数据集中的潜在偏见，这可能导致通过有缺陷的奖励信号进行不完美的策略优化，并降低公平性。现有的偏见缓解研究通常采用被动约束，这些约束在因果混淆的情况下可能会失效。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效减轻多模态数据集中偏见的奖励模型，提供无监督的、具有偏见韧性的奖励信号，以改进RLHF中的策略优化和公平性。&lt;h4&gt;方法&lt;/h4&gt;提出反事实奖励模型，核心是反事实信任评分，包含四个组成部分：(1)反事实偏移，将政治框架偏见与主题偏见分离；(2)反事实扰动期间的重构不确定性；(3)每个受保护属性的可证明的公平性规则违反；(4)与动态信任度量对齐的临时奖励偏移。在多模态真假新闻数据集上进行评估，并注入合成偏见测试鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;该系统在假新闻检测中达到了89.12%的准确率，优于基线奖励模型。更重要的是，它减少了虚假相关性和不公平的强化信号，提供了可调节的偏见减少阈值，增强了动态实时决策的可靠性。&lt;h4&gt;结论&lt;/h4&gt;该反事实奖励模型框架为公平感知的RLHF提供了一种稳健且可解释的方法，能够有效减轻偏见，提高决策的可靠性和公平性。&lt;h4&gt;翻译&lt;/h4&gt;在基于人类反馈的强化学习(RLHF)中，奖励模型能够高效学习和放大多模态数据集中的潜在偏见，这可能导致通过有缺陷的奖励信号进行不完美的策略优化，并降低公平性。偏见缓解研究通常采用被动约束，这些约束在因果混淆的情况下可能会失效。在此，我们提出了一种反事实奖励模型，引入因果推断与多模态表征学习，提供一种无监督的、具有偏见韧性的奖励信号。我们贡献的核心是反事实信任评分，一个包含四个组成部分的聚合评分：(1)将政治框架偏见与主题偏见分离的反事实偏移；(2)反事实扰动期间的重构不确定性；(3)每个受保护属性的可证明的公平性规则违反；(4)与动态信任度量对齐的临时奖励偏移。我们在一个展示框架偏见、类别不平衡和分布漂移的多模态真假新闻数据集上评估了该框架。遵循基于表示距离的无监督漂移检测[1]和语言模型中的时间鲁棒性基准测试[2]的类似方法，我们还注入了跨连续批次的合成偏见以测试鲁棒性。 resulting system achieved an accuracy of 89.12% in fake news detection, outperforming the baseline reward models. More importantly, it reduced spurious correlations and unfair reinforcement signals. This pipeline outlines a robust and interpretable approach to fairness-aware RLHF, offering tunable bias reduction thresholds and increasing reliability in dynamic real-time policy making.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In reinforcement learning with human feedback (RLHF), reward models canefficiently learn and amplify latent biases within multimodal datasets, whichcan lead to imperfect policy optimization through flawed reward signals anddecreased fairness. Bias mitigation studies have often applied passiveconstraints, which can fail under causal confounding. Here, we present acounterfactual reward model that introduces causal inference with multimodalrepresentation learning to provide an unsupervised, bias-resilient rewardsignal. The heart of our contribution is the Counterfactual Trust Score, anaggregated score consisting of four components: (1) counterfactual shifts thatdecompose political framing bias from topical bias; (2) reconstructionuncertainty during counterfactual perturbations; (3) demonstrable violations offairness rules for each protected attribute; and (4) temporal reward shiftsaligned with dynamic trust measures. We evaluated the framework on a multimodalfake versus true news dataset, which exhibits framing bias, class imbalance,and distributional drift. Following methodologies similar to unsupervised driftdetection from representation-based distances [1] and temporal robustnessbenchmarking in language models [2], we also inject synthetic bias acrosssequential batches to test robustness. The resulting system achieved anaccuracy of 89.12% in fake news detection, outperforming the baseline rewardmodels. More importantly, it reduced spurious correlations and unfairreinforcement signals. This pipeline outlines a robust and interpretableapproach to fairness-aware RLHF, offering tunable bias reduction thresholds andincreasing reliability in dynamic real-time policy making.</description>
      <author>example@mail.com (Sheryl Mathew, N Harshit)</author>
      <guid isPermaLink="false">2508.19567v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Improving Recommendation Fairness via Graph Structure and Representation Augmentation</title>
      <link>http://arxiv.org/abs/2508.19547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于数据增强的公平推荐方法，旨在解决图卷积网络(GCN)在推荐系统中导致的敏感信息传播和数据偏差问题。&lt;h4&gt;背景&lt;/h4&gt;图卷积网络(GCN)在推荐系统中广泛应用，但会导致敏感信息在图结构中广泛传播，放大数据偏差并引发公平性问题。现有公平性方法大多忽略了偏差数据对表征学习的影响，而数据增强方法虽能构建公平数据分布但会破坏用户偏好，降低推荐效用。&lt;h4&gt;目的&lt;/h4&gt;从数据增强的角度设计公平的推荐方法，在保持推荐效用的同时提高公平性。&lt;h4&gt;方法&lt;/h4&gt;提出两个先验假设：一是通过比较性能导向和公平导向推荐的结果来识别敏感交互；二是通过分析偏差和去偏差表征之间的特征相似性来检测敏感特征。基于此，提出双重数据增强框架，包括生成公平增强图和特征表征的两种策略，并引入去偏差学习方法以最小化学习表征与敏感信息间的依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实数据集上的大量实验证明了所提出框架的优越性。&lt;h4&gt;结论&lt;/h4&gt;该研究通过双重数据增强框架和去偏差学习方法，有效提高了推荐系统的公平性同时保持了推荐效用。&lt;h4&gt;翻译&lt;/h4&gt;图卷积网络(GCN)在推荐系统中变得越来越流行。然而，最近研究表明，基于GCN的模型会导致敏感信息在图结构中广泛传播，放大数据偏差并引发公平性问题。虽然已经提出了各种公平性方法，但大多数方法忽略了偏差数据对表征学习的影响，导致公平性改善有限。此外，一些研究通过数据增强来构建公平和平衡的数据分布，但这些方法由于破坏用户偏好而显著降低了效用。本文旨在从数据增强的角度设计公平的推荐方法，以提高公平性同时保持推荐效用。为了以最小干扰用户偏好的方式进行公平感知的数据增强，我们提出了两个先验假设。第一个假设通过比较性能导向和公平导向推荐的结果来识别敏感交互，而第二个假设通过分析偏差和去偏差表征之间的特征相似性来检测敏感特征。然后，我们提出了一个公平推荐的双重数据增强框架，包括两种数据增强策略来生成公平的增强图和特征表征。此外，我们引入了一种去偏差学习方法，最小化学习表征与敏感信息之间的依赖性以消除偏差。在两个真实数据集上的大量实验证明了我们提出框架的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761004&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Convolutional Networks (GCNs) have become increasingly popular inrecommendation systems. However, recent studies have shown that GCN-basedmodels will cause sensitive information to disseminate widely in the graphstructure, amplifying data bias and raising fairness concerns. While variousfairness methods have been proposed, most of them neglect the impact of biaseddata on representation learning, which results in limited fairness improvement.Moreover, some studies have focused on constructing fair and balanced datadistributions through data augmentation, but these methods significantly reduceutility due to disruption of user preferences. In this paper, we aim to designa fair recommendation method from the perspective of data augmentation toimprove fairness while preserving recommendation utility. To achievefairness-aware data augmentation with minimal disruption to user preferences,we propose two prior hypotheses. The first hypothesis identifies sensitiveinteractions by comparing outcomes of performance-oriented and fairness-awarerecommendations, while the second one focuses on detecting sensitive featuresby analyzing feature similarities between biased and debiased representations.Then, we propose a dual data augmentation framework for fair recommendation,which includes two data augmentation strategies to generate fair augmentedgraphs and feature representations. Furthermore, we introduce a debiasinglearning method that minimizes the dependence between the learnedrepresentations and sensitive information to eliminate bias. Extensiveexperiments on two real-world datasets demonstrate the superiority of ourproposed framework.</description>
      <author>example@mail.com (Tongxin Xu, Wenqiang Liu, Chenzhong Bin, Cihan Xiao, Zhixin Zeng, Tianlong Gu)</author>
      <guid isPermaLink="false">2508.19547v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models</title>
      <link>http://arxiv.org/abs/2508.19498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UNIFORM是一个新颖的知识转移框架，能够从多样化的预训练模型向单一学生模型有效传递知识，无需对训练数据分布和网络架构做出强假设。&lt;h4&gt;背景&lt;/h4&gt;在深度学习时代，网络上可用的预训练模型数量不断增加，这些模型具有不同的架构和训练数据，提供了对现实世界的独特解释，它们的集体共识可能是通用的并可推广到未见数据。&lt;h4&gt;目的&lt;/h4&gt;解决如何有效利用异构预训练模型的集体知识这一基本挑战，而不依赖于对训练数据分布和网络架构的强假设。&lt;h4&gt;方法&lt;/h4&gt;UNIFORM框架采用专门的投票机制，在logit级别（包括能够预测目标类别的教师模型）和特征级别（利用在任意标签空间上学习到的视觉表示）捕获知识的共识。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，UNIFORM与强大的知识迁移基线相比，有效提高了无监督目标识别性能，并且表现出卓越的可扩展性，能够从一百多名教师中受益。&lt;h4&gt;结论&lt;/h4&gt;UNIFORM框架成功解决了从多样化预训练模型中整合知识的问题，无需传统方法所需的强假设，且在大规模教师模型的情况下表现优越。&lt;h4&gt;翻译&lt;/h4&gt;在深度学习时代，网络上可用的预训练模型数量不断增加，提供了丰富的知识。这些模型具有不同的架构，在不同的数据集上训练以完成不同任务，为现实世界提供了独特的解释。它们的集体共识可能是通用的并可推广到未见数据。然而，由于预训练模型的异构性，有效利用这种集体知识面临基本挑战。现有的知识集成解决方案通常依赖于对训练数据分布和网络架构的强假设，限制了它们只能从特定类型的模型中学习，并导致数据和/或归纳偏差。在这项工作中，我们引入了一个名为UNIFORM的新颖框架，用于从多样化的现成模型向一个学生模型进行知识转移，没有这些限制。具体而言，我们提出了一种专门的投票机制，在logit级别捕获知识——包括能够预测目标类别的教师模型——和在特征级别，利用在任意标签空间上学习到的视觉表示。大量实验表明，UNIFORM与强大的知识迁移基线相比，有效提高了无监督目标识别性能。值得注意的是，它通过受益于一百多名教师而表现出卓越的可扩展性，而现有方法在较小规模时就趋于饱和。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of deep learning, the increasing number of pre-trained modelsavailable online presents a wealth of knowledge. These models, developed withdiverse architectures and trained on varied datasets for different tasks,provide unique interpretations of the real world. Their collective consensus islikely universal and generalizable to unseen data. However, effectivelyharnessing this collective knowledge poses a fundamental challenge due to theheterogeneity of pre-trained models. Existing knowledge integration solutionstypically rely on strong assumptions about training data distributions andnetwork architectures, limiting them to learning only from specific types ofmodels and resulting in data and/or inductive biases. In this work, weintroduce a novel framework, namely UNIFORM, for knowledge transfer from adiverse set of off-the-shelf models into one student model without suchconstraints. Specifically, we propose a dedicated voting mechanism to capturethe consensus of knowledge both at the logit level -- incorporating teachermodels that are capable of predicting target classes of interest -- and at thefeature level, utilizing visual representations learned on arbitrary labelspaces. Extensive experiments demonstrate that UNIFORM effectively enhancesunsupervised object recognition performance compared to strong knowledgetransfer baselines. Notably, it exhibits remarkable scalability by benefitingfrom over one hundred teachers, while existing methods saturate at a muchsmaller scale.</description>
      <author>example@mail.com (Yimu Wang, Weiming Zhuang, Chen Chen, Jiabo Huang, Jingtao Li, Lingjuan Lyu)</author>
      <guid isPermaLink="false">2508.19498v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection</title>
      <link>http://arxiv.org/abs/2508.19450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review at IEEE IoTJ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CITADEL是一种自监督持续学习框架，用于物联网入侵检测，能有效提取良性数据表示并保留长期知识，在动态环境中表现出色。&lt;h4&gt;背景&lt;/h4&gt;物联网因其高度互连性和有限计算资源易受网络威胁，基于机器学习的入侵检测系统虽前景广阔，但对新兴威胁适应性差且存在灾难性遗忘问题。&lt;h4&gt;目的&lt;/h4&gt;开发CITADEL框架，从良性数据中提取鲁棒表示，通过优化记忆巩固机制保留长期知识，解决ML-IDS的适应性和遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;CITADEL整合表格到图像转换模块、内存感知掩码自编码器用于自监督表示学习，以及无需标记攻击数据的新颖性检测组件。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明CITADEL在多个入侵数据集上，关键检测和保留指标比VLAD提高高达72.9%，能逐步适应新兴行为同时保持检测先前威胁的能力。&lt;h4&gt;结论&lt;/h4&gt;CITADEL在动态物联网环境中能有效检测入侵并保留知识，显著提升物联网安全性。&lt;h4&gt;翻译&lt;/h4&gt;物联网(IoT)由于其高度互连性和有限的计算资源，特别容易受到各种网络威胁的攻击。入侵检测系统(IDS)已被广泛研究以增强物联网安全，基于机器学习的入侵检测系统(ML-IDS)在检测恶意活动方面显示出巨大潜力。然而，它们的有效性通常受到对新兴威胁适应性差和持续学习过程中灾难性遗忘问题的限制。为应对这些挑战，我们提出了CITADEL，一种自监督持续学习框架，旨在从良性数据中提取鲁棒表示，并通过优化的记忆巩固机制保留长期知识。CITADEL集成了表格到图像转换模块、用于自监督表示学习的内存感知掩码自编码器，以及一个能够识别异常而不依赖于标记攻击数据的新颖性检测组件。我们的设计使系统能够逐步适应新兴行为，同时保持检测先前观察到的威胁的能力。在多个入侵数据集上的实验表明，CITADEL在关键的检测和保留指标上比基于VAE的终身异常检测器(VLAD)提高了高达72.9%，突显了其在动态物联网环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Internet of Things (IoT), with its high degree of interconnectivity andlimited computational resources, is particularly vulnerable to a wide range ofcyber threats. Intrusion detection systems (IDS) have been extensively studiedto enhance IoT security, and machine learning-based IDS (ML-IDS) showconsiderable promise for detecting malicious activity. However, theireffectiveness is often constrained by poor adaptability to emerging threats andthe issue of catastrophic forgetting during continuous learning. To addressthese challenges, we propose CITADEL, a self-supervised continual learningframework designed to extract robust representations from benign data whilepreserving long-term knowledge through optimized memory consolidationmechanisms. CITADEL integrates a tabular-to-image transformation module, amemory-aware masked autoencoder for self-supervised representation learning,and a novelty detection component capable of identifying anomalies withoutdependence on labeled attack data. Our design enables the system toincrementally adapt to emerging behaviors while retaining its ability to detectpreviously observed threats. Experiments on multiple intrusion datasetsdemonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-basedlifelong anomaly detector (VLAD) in key detection and retention metrics,highlighting its effectiveness in dynamic IoT environments.</description>
      <author>example@mail.com (Elvin Li, Onat Gungor, Zhengli Shang, Tajana Rosing)</author>
      <guid isPermaLink="false">2508.19450v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems</title>
      <link>http://arxiv.org/abs/2508.18925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为CTGraph的图级别表示学习方法，用于以自监督方式对学习者行为和表现进行档案记录，以解决智能辅导系统可能扩大表现差距的问题，并帮助识别有困难的学生。&lt;h4&gt;背景&lt;/h4&gt;智能辅导系统在教育中的采用激增，这些系统虽然是基于课程学习的必要组成部分，但可能会无意中扩大学生之间的表现差距。学生档案记录对于跟踪进度、识别有困难的学生和减轻学生差异至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决智能辅导系统可能扩大表现差距的问题，通过学生档案记录来跟踪进度、识别有困难的学生和减轻学生差异。&lt;h4&gt;方法&lt;/h4&gt;引入CTGraph，一种图级别的表示学习方法，以自监督方式对学习者的行为和表现进行档案记录。测量学生在不同方面的行为和表现，如内容覆盖、学习强度和学习主题内不同概念的熟练程度。&lt;h4&gt;主要发现&lt;/h4&gt;CTGraph可以提供学生整个学习旅程的全景视图，考虑了学生行为和表现的不同方面以及与课程结构对齐的学习路径变化。该方法能够识别有困难的学生，并提供不同群体的比较分析，以确定学生在何时何地遇到困难。&lt;h4&gt;结论&lt;/h4&gt;该方法为教育工作者提供了更丰富的学生学习洞见，为更有针对性的干预措施铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;智能辅导系统在教育中的采用激增，虽然是基于课程学习的必要组成部分，但可能会无意中扩大表现差距。为了解决这个问题，学生档案记录对于跟踪进度、识别有困难的学生和减轻学生之间的差异变得至关重要。这种档案记录需要测量学生在不同方面的行为和表现，如内容覆盖、学习强度和学习主题内不同概念的熟练程度。在本研究中，我们引入了CTGraph，一种图级别的表示学习方法，以自监督方式对学习者的行为和表现进行档案记录。我们的实验证明，CTGraph可以提供学生整个学习旅程的全景视图，考虑学生行为和表现的不同方面，以及与课程结构对齐的学习路径变化。我们还表明，我们的方法可以识别有困难的学生，并提供不同群体的比较分析，以确定学生在何时何地遇到困难。因此，我们的方法为教育工作者提供了丰富的学生学习洞见，为更有针对性的干预措施铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The surge in the adoption of Intelligent Tutoring Systems (ITSs) ineducation, while being integral to curriculum-based learning, can inadvertentlyexacerbate performance gaps. To address this problem, student profiling becomescrucial for tracking progress, identifying struggling students, and alleviatingdisparities among students. Such profiling requires measuring student behaviorsand performance across different aspects, such as content coverage, learningintensity, and proficiency in different concepts within a learning topic.  In this study, we introduce CTGraph, a graph-level representation learningapproach to profile learner behaviors and performance in a self-supervisedmanner. Our experiments demonstrate that CTGraph can provide a holistic view ofstudent learning journeys, accounting for different aspects of studentbehaviors and performance, as well as variations in their learning paths asaligned to the curriculum structure. We also show that our approach canidentify struggling students and provide comparative analysis of diverse groupsto pinpoint when and where students are struggling. As such, our approach opensmore opportunities to empower educators with rich insights into studentlearning journeys and paves the way for more targeted interventions.</description>
      <author>example@mail.com (Qian Xiao, Conn Breathnach, Ioana Ghergulescu, Conor O'Sullivan, Keith Johnston, Vincent Wade)</author>
      <guid isPermaLink="false">2508.18925v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities</title>
      <link>http://arxiv.org/abs/2508.19305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Geo2Vec是一种创新的地理实体空间表征学习方法，直接在原始空间操作，通过自适应采样和编码符号距离来捕获几何信息，无需分解实体，实现了更高的效率和更好的性能。&lt;h4&gt;背景&lt;/h4&gt;空间表征学习对GeoAI应用如城市分析至关重要，可编码地理实体的形状、位置和空间关系。现有方法要么针对单一实体类型，要么将实体分解为简单组件实现傅里叶变换，导致高计算成本，且变换空间缺乏几何对齐，模糊了边缘和边界等细粒度特征。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，引入一种受有距离场(SDF)启发的Geo2Vec方法，在原始空间直接操作，自适应采样点并编码符号距离，产生紧凑、几何感知、统一的表征，适用于所有地理实体类型，并提出旋转不变的位置编码来建模高频空间变化。&lt;h4&gt;方法&lt;/h4&gt;Geo2Vec方法直接在原始空间中操作，自适应采样点并编码它们的符号距离(外部为正，内部为负)，无需分解即可捕获几何信息。使用神经网络训练近似SDF，产生紧凑、几何感知的表征。同时提出旋转不变的位置编码来建模高频空间变化，构建结构化和鲁棒的嵌入空间。&lt;h4&gt;主要发现&lt;/h4&gt;Geo2Vec在表示形状和位置方面始终优于现有方法，在捕获拓扑和距离关系方面表现更好，在实际GeoAI应用中实现了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;Geo2Vec有效解决了现有空间表征学习的局限性，能够更好地表示地理实体的几何和空间特征，为GeoAI应用提供了更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空间表征学习对于城市分析等GeoAI应用至关重要，能够编码点、线和多边形等地理实体的形状、位置和空间关系（拓扑关系和基于距离的关系）。现有方法要么针对单一地理实体类型，要么像Poly2Vec那样将实体分解为更简单的组件以实现傅里叶变换，引入了高计算成本。此外，由于变换空间缺乏几何对齐，这些方法依赖于均匀的、非自适应的采样，这会模糊边缘和边界等细粒度特征。为解决这些局限性，我们引入了Geo2Vec，一种受有距离场(SDF)启发的新方法，直接在原始空间中操作。Geo2Vec自适应采样点并编码它们的符号距离（外部为正，内部为负），无需分解即可捕获几何信息。训练用于近似SDF的神经网络可产生紧凑的、几何感知的且适用于所有地理实体类型的统一表征。此外，我们提出了旋转不变的位置编码来建模高频空间变化，为下游GeoAI模型构建结构化和鲁棒的嵌入空间。实验结果表明，Geo2Vec在表示形状和位置、捕获拓扑和距离关系方面始终优于现有方法，并在实际GeoAI应用中实现更高的效率。代码和数据可在以下网址找到：https://github.com/chuchen2017/GeoNeuralRepresentation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial representation learning is essential for GeoAI applications such asurban analytics, enabling the encoding of shapes, locations, and spatialrelationships (topological and distance-based) of geo-entities like points,polylines, and polygons. Existing methods either target a single geo-entitytype or, like Poly2Vec, decompose entities into simpler components to enableFourier transformation, introducing high computational cost. Moreover, sincethe transformed space lacks geometric alignment, these methods rely on uniform,non-adaptive sampling, which blurs fine-grained features like edges andboundaries. To address these limitations, we introduce Geo2Vec, a novel methodinspired by signed distance fields (SDF) that operates directly in the originalspace. Geo2Vec adaptively samples points and encodes their signed distances(positive outside, negative inside), capturing geometry without decomposition.A neural network trained to approximate the SDF produces compact,geometry-aware, and unified representations for all geo-entity types.Additionally, we propose a rotation-invariant positional encoding to modelhigh-frequency spatial variations and construct a structured and robustembedding space for downstream GeoAI models. Empirical results show thatGeo2Vec consistently outperforms existing methods in representing shape andlocation, capturing topological and distance relationships, and achievinggreater efficiency in real-world GeoAI applications. Code and Data can be foundat: https://github.com/chuchen2017/GeoNeuralRepresentation.</description>
      <author>example@mail.com (Chen Chu, Cyrus Shahabi)</author>
      <guid isPermaLink="false">2508.19305v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</title>
      <link>http://arxiv.org/abs/2508.18166v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PCR-CA框架，解决现代应用商店推荐系统在处理多类别应用时的语义捕捉问题，通过并行码本表示和对比对齐技术提升CTR预测效果。&lt;h4&gt;背景&lt;/h4&gt;现代应用商店推荐系统在处理多类别应用时面临挑战，因为传统分类法无法捕捉重叠的语义，导致个性化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出PCR-CA（并行码本表示与对比对齐）框架，改进应用商店的CTR预测效果，特别是对于多类别应用和长尾应用。&lt;h4&gt;方法&lt;/h4&gt;PCR-CA框架首先从应用文本提取多模态嵌入，然后引入并行码本VQ-AE模块学习多码本上的离散语义表示，使用对比对齐损失桥接语义和协同信号，并通过双注意力融合机制结合ID和语义特征捕捉用户兴趣。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示PCR-CA相比基线实现+0.76%的AUC提升，长尾应用的AUC提升达+2.15%；在线A/B测试显示CTR提升+10.52%，CVR提升+16.30%。&lt;h4&gt;结论&lt;/h4&gt;PCR-CA框架在真实世界部署中有效，现已完全部署在Microsoft Store上。&lt;h4&gt;翻译&lt;/h4&gt;现代应用商店推荐系统在处理多类别应用时面临挑战，因为传统分类法无法捕捉重叠的语义，导致个性化效果不佳。我们提出了PCR-CA（并行码本表示与对比对齐），一个用于改进CTR预测的端到端框架。PCR-CA首先从应用文本中提取紧凑的多模态嵌入，然后引入一个并行码本VQ-AE模块，该模块在多个码本上并行学习离散语义表示——这与分层残差量化（RQ-VAE）不同。这种设计能够独立编码不同方面（如游戏玩法、艺术风格），更好地建模多类别语义。为了桥接语义和协同信号，我们在用户和项目层面都采用了对比对齐损失，增强长尾项目的表示学习。此外，双注意力融合机制结合基于ID和语义的特征来捕捉用户兴趣，特别是对于长尾应用。在大规模数据集上的实验显示，PCR-CA相比强基线实现了+0.76%的AUC改进，长尾应用的AUC提升达到+2.15%。在线A/B测试进一步验证了我们的方法，显示CTR提升+10.52%，CVR提升+16.30%，证明了PCR-CA在真实世界部署中的有效性。该新框架现已完全部署在Microsoft Store上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern app store recommender systems struggle with multiple-category apps, astraditional taxonomies fail to capture overlapping semantics, leading tosuboptimal personalization. We propose PCR-CA (Parallel CodebookRepresentations with Contrastive Alignment), an end-to-end framework forimproved CTR prediction. PCR-CA first extracts compact multimodal embeddingsfrom app text, then introduces a Parallel Codebook VQ-AE module that learnsdiscrete semantic representations across multiple codebooks in parallel --unlike hierarchical residual quantization (RQ-VAE). This design enablesindependent encoding of diverse aspects (e.g., gameplay, art style), bettermodeling multiple-category semantics. To bridge semantic and collaborativesignals, we employ a contrastive alignment loss at both the user and itemlevels, enhancing representation learning for long-tail items. Additionally, adual-attention fusion mechanism combines ID-based and semantic features tocapture user interests, especially for long-tail apps. Experiments on alarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strongbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing furthervalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvementin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The newframework has now been fully deployed on the Microsoft Store.</description>
      <author>example@mail.com (Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang)</author>
      <guid isPermaLink="false">2508.18166v3</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Question &amp; Answer Generation Using Generative Large Language Model (LLM)</title>
      <link>http://arxiv.org/abs/2508.19475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于微调生成式大型语言模型的自动问答生成系统，旨在帮助教育工作者高效创建多样化的评估题目，减轻教师手动出题的负担，优化教育评估流程。&lt;h4&gt;背景&lt;/h4&gt;在教育领域中，学生评估与知识传授同等重要。传统上，教师需要手动从多种教材中创建多样化的题目，这对教师来说是一个挑战，需要耗费大量时间和精力。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动问答生成系统，使教师能够轻松创建不同风格的问题（如选择题、概念题或事实题），从而简化评估过程，节省时间和资源。&lt;h4&gt;方法&lt;/h4&gt;使用微调生成式大型语言模型（Meta-Llama 2-7B）作为基础模型，采用提示工程技术调整问题风格，利用无监督学习方法，并使用RACE数据集进行模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过微调和提示工程，系统能够根据教师偏好生成不同类型的问题；基于Meta-Llama 2-7B模型的定制化解决方案能够有效满足教育工作者在文本评估方面的需求。&lt;h4&gt;结论&lt;/h4&gt;自动问答生成系统是一个可靠且高效的工具，能够帮助教育工作者节省宝贵的时间和资源，简化评估流程，提高教育评估的效率和质量。&lt;h4&gt;翻译&lt;/h4&gt;在教育领域，学生评估与知识传授同等重要。要接受评估，学生通常需要通过基于文本的学术评估方法。教师需要创建多样化的题目集，这些题目对所有学生都应该是公平的，以证明他们在特定主题上的充分掌握。这可能相当具有挑战性，因为他们可能需要手动查阅几种不同的教学材料。我们的目标是通过实现自动问答生成（AQAG），使用微调后的生成式大型语言模型，使整个过程变得更加容易。为了调整教师偏好的问题风格（选择题、概念题或事实题），我们利用了提示工程（PE）。在这项研究中，我们提议利用自然语言处理中的无监督学习方法，主要关注英语语言。这种方法使基础Meta-Llama 2-7B模型能够将RACE数据集作为微调过程的训练数据。创建一个为教育工作者、教师和从事文本评估的个人提供高效解决方案的自定义模型。一个可靠且高效的问答生成工具可以释放宝贵的时间和资源，从而简化他们的评估过程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; \Abstract{In the realm of education, student evaluation holds equalsignificance as imparting knowledge. To be evaluated, students usually need togo through text-based academic assessment methods. Instructors need to makediverse sets of questions that need to be fair for all students to prove theiradequacy over a particular topic. This can prove to be quite challenging asthey may need to manually go through several different lecture materials. Ourobjective is to make this whole process much easier by implementing AutomaticQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. Fortailoring the instructor's preferred question style (MCQ, conceptual, orfactual questions), prompt Engineering (PE) is being utilized. In thisresearch, we propose to leverage unsupervised learning methods in NLP,primarily focusing on the English language. This approach empowers the baseMeta-Llama 2-7B model to integrate RACE dataset as training data for thefine-tuning process. Creating a customized model that will offer efficientsolutions for educators, instructors, and individuals engaged in text-basedevaluations. A reliable and efficient tool for generating questions and answerscan free up valuable time and resources, thus streamlining their evaluationprocesses.}</description>
      <author>example@mail.com (Md. Alvee Ehsan, A. S. M Mehedi Hasan, Kefaya Benta Shahnoor, Syeda Sumaiya Tasneem)</author>
      <guid isPermaLink="false">2508.19475v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management</title>
      <link>http://arxiv.org/abs/2508.19419v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种物理信息机器学习方法，将多相流模拟器与卷积神经网络结合，有效解决了地下储层压力控制中的计算效率问题，通过迁移学习大幅减少了所需的模拟次数。&lt;h4&gt;背景&lt;/h4&gt;地下储层压力控制面临地质非均质性和多相流体流动动力学的挑战，基于高保真物理的模拟计算成本高昂，且需要多次模拟来应对不确定的非均质属性，这使得传统方法往往不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法来预测地下储层压力行为，减少计算成本，同时保持预测准确性，以应对实际注入-提取场景。&lt;h4&gt;方法&lt;/h4&gt;将完全可微的多相流模拟器（DPFEHM框架实现）与卷积神经网络耦合，CNN学习从非均质渗透率场预测流体提取率以控制关键位置压力；通过将瞬态多相流物理纳入训练提高预测准确性；采用迁移学习策略，先在单相稳态模拟上预训练，再在多相场景上微调以加速训练。&lt;h4&gt;主要发现&lt;/h4&gt;与之前需要多达一千万次全物理多相流模拟相比，使用不到三千次即可实现高精度训练；通过迁移学习策略显著降低了计算成本，使储层压力控制更加实用。&lt;h4&gt;结论&lt;/h4&gt;物理信息机器学习方法结合迁移学习能有效解决地下储层压力控制的挑战，大幅减少计算需求，为实际应用提供了更可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的地下储层压力控制由于地质非均质性和多相流体流动动力学而极具挑战性。在这种环境中的预测依赖于计算成本高昂的高保真物理模拟。然而，控制这些流动的不确定、非均质属性使得有必要进行多次这些昂贵的模拟，这通常是不可行的。为了解决这些挑战，我们引入了一种物理信息机器学习工作流程，该工作流程将完全可微的多相流模拟器（在DPFEHM框架中实现）与卷积神经网络(CNN)耦合。CNN学习从非均质渗透率场预测流体提取率，以在关键储层位置强制执行压力限制。通过将瞬态多相流物理纳入训练过程，我们的方法与先前的工作相比，能够对实际的注入-提取场景进行更实用、更准确的预测。为加速训练，我们在单相、稳态模拟上预训练模型，然后在完整多相场景上微调，这大大降低了计算成本。我们证明，与之前估计需要多达一千万次相比，使用不到三千次全物理多相流模拟即可实现高精度训练。这种模拟次数的大幅减少是通过利用从更便宜的单相模拟中迁移学习实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate subsurface reservoir pressure control is extremely challenging dueto geological heterogeneity and multiphase fluid-flow dynamics. Predictingbehavior in this setting relies on high-fidelity physics-based simulations thatare computationally expensive. Yet, the uncertain, heterogeneous propertiesthat control these flows make it necessary to perform many of these expensivesimulations, which is often prohibitive. To address these challenges, weintroduce a physics-informed machine learning workflow that couples a fullydifferentiable multiphase flow simulator, which is implemented in the DPFEHMframework with a convolutional neural network (CNN). The CNN learns to predictfluid extraction rates from heterogeneous permeability fields to enforcepressure limits at critical reservoir locations. By incorporating transientmultiphase flow physics into the training process, our method enables morepractical and accurate predictions for realistic injection-extraction scenarioscompare to previous works. To speed up training, we pretrain the model onsingle-phase, steady-state simulations and then fine-tune it on full multiphasescenarios, which dramatically reduces the computational cost. We demonstratethat high-accuracy training can be achieved with fewer than three thousandfull-physics multiphase flow simulations -- compared to previous estimatesrequiring up to ten million. This drastic reduction in the number ofsimulations is achieved by leveraging transfer learning from much lessexpensive single-phase simulations.</description>
      <author>example@mail.com (Harun Ur Rashid, Aleksandra Pachalieva, Daniel O'Malley)</author>
      <guid isPermaLink="false">2508.19419v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>One Joke to Rule them All? On the (Im)possibility of Generalizing Humor</title>
      <link>http://arxiv.org/abs/2508.19402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型在幽默类型间的迁移学习能力，发现模型能够在不同幽默类型间实现一定程度的迁移，且在多样化数据源上训练可提高迁移能力。&lt;h4&gt;背景&lt;/h4&gt;幽默是一种广泛而复杂的交流形式，现有研究主要专注于特定类型的幽默建模。随着在线和社交媒体中不断涌现新的幽默类型(如迷因、反幽默、AI失败等)，大型语言模型需要能够泛化到不同幽默类型。&lt;h4&gt;目的&lt;/h4&gt;了解在一个或多个特定幽默任务上的能力是否能转移到新的、未见过的幽默类型上，探究这种碎片化是否不可避免。&lt;h4&gt;方法&lt;/h4&gt;进行跨四个不同幽默任务数据集的迁移学习实验，在多样化的训练设置下(使用1-3个数据集训练，测试在新的任务上)训练大型语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;模型具有一定的迁移能力，在未见过的数据集上可达到75%的准确率；在多样化数据源上训练可提高迁移能力(1.88-4.05%)，同时领域内性能几乎没有下降；进一步分析发现幽默类型间存在关联，'爸爸笑话'出人意料地成为最好的迁移促进因素。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型能够在不同幽默类型间实现迁移，这种能力可以通过多样化训练得到增强，为应对不断演变的幽默景观提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;幽默是一种广泛而复杂的交流形式，对机器来说仍然具有挑战性。尽管如此，大多数关于计算幽默的现有研究传统上专注于建模特定类型的幽默。在这项工作中，我们希望了解在一个或多个特定幽默任务上的能力是否能转移到新的、未见过的类型；换句话说，这种碎片化是否不可避免？随着在线和社交媒体环境中不断出现新的幽默类型(例如迷因、反幽默、AI失败等)，这个问题特别及时。如果大型语言模型要跟上这一不断发展的格局，它们必须能够通过捕捉更深层次的、可迁移的机制来泛化到不同的幽默类型。为了研究这一点，我们在代表不同幽默任务的四个数据集上进行了一系列迁移学习实验。我们在不同的多样性设置下训练大型语言模型(使用1-3个数据集进行训练，测试在新的任务上)。实验表明，模型具有一定的迁移能力，在未见过的数据集上可达到75%的准确率；在多样化数据源上训练可以提高迁移能力(1.88-4.05%)，同时领域内性能几乎没有下降。进一步的分析揭示了幽默类型之间的关系，'爸爸笑话'出人意料地成为最好的迁移促进因素(但很难转移到其他类型)。我们发布了数据和代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humor is a broad and complex form of communication that remains challengingfor machines. Despite its broadness, most existing research on computationalhumor traditionally focused on modeling a specific type of humor. In this work,we wish to understand whether competence on one or more specific humor tasksconfers any ability to transfer to novel, unseen types; in other words, is thisfragmentation inevitable? This question is especially timely as new humor typescontinuously emerge in online and social media contexts (e.g., memes,anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with thisevolving landscape, they must be able to generalize across humor types bycapturing deeper, transferable mechanisms. To investigate this, we conduct aseries of transfer learning experiments across four datasets, representingdifferent humor tasks. We train LLMs under varied diversity settings (1-3datasets in training, testing on a novel task). Experiments reveal that modelsare capable of some transfer, and can reach up to 75% accuracy on unseendatasets; training on diverse sources improves transferability (1.88-4.05%)with minimal-to-no drop in in-domain performance. Further analysis suggestsrelations between humor types, with Dad Jokes surprisingly emerging as the bestenabler of transfer (but is difficult to transfer to). We release data andcode.</description>
      <author>example@mail.com (Mor Turgeman, Chen Shani, Dafna Shahaf)</author>
      <guid isPermaLink="false">2508.19402v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Multi-Source Knowledge Transfer by Model Merging</title>
      <link>http://arxiv.org/abs/2508.19353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于奇异值分解(SVD)的多源迁移学习方法，通过将源模型分解为基本分量并选择性聚合，解决了现有方法在效率和精度上的限制，实现了高效、鲁棒且计算可扩展的迁移学习。&lt;h4&gt;背景&lt;/h4&gt;迁移学习是一种有优势的策略，但它忽视了利用在线众多可用模型知识的机会。解决多源迁移学习问题是提高适应性和降低重新训练成本的有前途的路径。&lt;h4&gt;目的&lt;/h4&gt;解决现有多源迁移学习方法在知识提取精度和聚合效率方面的限制，实现高效、鲁棒且计算可扩展的多源迁移学习。&lt;h4&gt;方法&lt;/h4&gt;利用奇异值分解(SVD)将每个源模型分解为基本的、秩为1的分量；在聚合阶段只选择来自所有源的最显著分量；通过仅调整合并矩阵的主奇异值来使方法适应目标任务，重新校准顶部SVD分量的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架允许高效的迁移学习；对输入级别和参数空间中的扰动（例如有噪声或被修剪的源）具有鲁棒性；在计算上具有良好的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;通过SVD分解和选择性聚合，解决了多源迁移学习中的效率和精度问题，实现了高效、鲁棒且计算可扩展的迁移学习框架。&lt;h4&gt;翻译&lt;/h4&gt;虽然迁移学习是一种有优势的策略，但它忽视了利用在线众多可用模型知识的机会。解决这种多源迁移学习问题是提高适应性和降低重新训练成本的有前途的路径。然而，现有方法本质上是粗粒度的，缺乏细粒度知识提取的必要精度，以及融合大量源模型或高参数模型知识所需的聚合效率。我们通过利用奇异值分解(SVD)将每个源模型分解为其基本的、秩为1的分量来解决这些限制。随后的聚合阶段只选择来自所有源的最显著分量，从而克服了之前的效率和精度限制。为了最好地保留和利用合成的知识库，我们的方法通过仅调整合并矩阵的主奇异值来适应目标任务。本质上，这个过程只重新校准了顶部SVD分量的重要性。所提出的框架允许高效的迁移学习，对输入级别和参数空间中的扰动（例如有噪声或被修剪的源）具有鲁棒性，并且在计算上具有良好的可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While transfer learning is an advantageous strategy, it overlooks theopportunity to leverage knowledge from numerous available models online.Addressing this multi-source transfer learning problem is a promising path toboost adaptability and cut re-training costs. However, existing approaches areinherently coarse-grained, lacking the necessary precision for granularknowledge extraction and the aggregation efficiency required to fuse knowledgefrom either a large number of source models or those with high parametercounts. We address these limitations by leveraging Singular Value Decomposition(SVD) to first decompose each source model into its elementary, rank-onecomponents. A subsequent aggregation stage then selects only the most salientcomponents from all sources, thereby overcoming the previous efficiency andprecision limitations. To best preserve and leverage the synthesized knowledgebase, our method adapts to the target task by fine-tuning only the principalsingular values of the merged matrix. In essence, this process onlyrecalibrates the importance of top SVD components. The proposed frameworkallows for efficient transfer learning, is robust to perturbations both at theinput level and in the parameter space (e.g., noisy or pruned sources), andscales well computationally.</description>
      <author>example@mail.com (Marcin Osial, Bartosz Wójcik, Bartosz Zieliński, Sebastian Cygert)</author>
      <guid isPermaLink="false">2508.19353v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling</title>
      <link>http://arxiv.org/abs/2508.19028v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRADSTOP的新型随机早停方法，该方法仅利用梯度信息而非单独的验证集来防止机器学习模型过拟合，从而提高在未见数据上的预测性能。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型通常通过梯度下降算法在训练数据上最小化损失函数来学习，但这些模型常遭受过拟合问题，导致在未见数据上的预测性能下降。标准解决方案是使用保留验证集的早停法，当验证损失停止减少时停止训练，但这减少了可用于训练的数据量。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖于验证集的早停方法，充分利用梯度下降算法产生的梯度信息，以提高模型在数据有限情况下的性能。&lt;h4&gt;方法&lt;/h4&gt;GRADSTOP通过三个主要贡献实现：1) 利用梯度信息估计贝叶斯后验；2) 将早停问题定义为从此后验中抽取样本；3) 使用近似后验获得停止标准。&lt;h4&gt;主要发现&lt;/h4&gt;经验评估表明，GRADSTOP在测试数据上实现了较小的损失值，与基于验证集的停止标准相比表现良好。通过利用整个数据集进行训练，该方法在数据有限的情况下（如迁移学习）特别有优势。&lt;h4&gt;结论&lt;/h4&gt;GRADSTOP可作为梯度下降库的一个可选功能加入，只需很小的计算开销。源代码已公开，可在提供的GitHub链接获取。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型通常通过使用梯度下降算法在训练数据上最小化损失函数来学习。这些模型常常遭受过拟合问题，导致在未见数据上的预测性能下降。标准解决方案是使用保留验证集的早停法，当验证损失停止减少时停止最小化过程。然而，这种验证集减少了可用于训练的数据量。本文提出了GRADSTOP，一种新型随机早停方法，它仅使用梯度信息，这些信息由梯度下降算法'免费'产生。我们的主要贡献是：我们通过梯度信息估计贝叶斯后验，将早停问题定义为从此后验中抽取样本，并使用近似后验获得停止标准。我们的经验评估表明，GRADSTOP在测试数据上实现了较小的损失，并且与基于验证集的停止标准相比表现良好。通过利用整个数据集进行训练，我们的方法在数据有限的情况下（如迁移学习）特别有优势。它可以作为梯度下降库的一个可选功能加入，只需很小的计算开销。源代码可在https://github.com/edahelsinki/gradstop获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning models are often learned by minimising a loss function onthe training data using a gradient descent algorithm. These models often sufferfrom overfitting, leading to a decline in predictive performance on unseendata. A standard solution is early stopping using a hold-out validation set,which halts the minimisation when the validation loss stops decreasing.However, this hold-out set reduces the data available for training. This paperpresents GRADSTOP, a novel stochastic early stopping method that only usesinformation in the gradients, which are produced by the gradient descentalgorithm ``for free.'' Our main contributions are that we estimate theBayesian posterior by the gradient information, define the early stoppingproblem as drawing sample from this posterior, and use the approximatedposterior to obtain a stopping criterion. Our empirical evaluation shows thatGRADSTOP achieves a small loss on test data and compares favourably to avalidation-set-based stopping criterion. By leveraging the entire dataset fortraining, our method is particularly advantageous in data-limited settings,such as transfer learning. It can be incorporated as an optional feature ingradient descent libraries with only a small computational overhead. The sourcecode is available at https://github.com/edahelsinki/gradstop.</description>
      <author>example@mail.com (Arash Jamshidi, Lauri Seppäläinen, Katsiaryna Haitsiukevich, Hoang Phuc Hau Luu, Anton Björklund, Kai Puolamäki)</author>
      <guid isPermaLink="false">2508.19028v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation</title>
      <link>http://arxiv.org/abs/2508.19705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FreeVPS的视频息肉分割方法，通过重新构建VPS任务为检测后跟踪范式，结合IPS模型的空间上下文和SAM2的时序建模能力，并引入两个无需训练的模块来解决SAM2在长期跟踪中的错误累积问题。&lt;h4&gt;背景&lt;/h4&gt;现有的视频息肉分割方法难以平衡时空建模和领域泛化能力，限制了它们在真实临床场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决VPS任务中时空建模和领域泛化之间的平衡问题，提高在真实临床场景中的适用性。&lt;h4&gt;方法&lt;/h4&gt;将VPS任务重新构建为检测后跟踪范式，利用IPS模型的空间上下文和SAM2的时序建模能力；引入内部关联过滤模块消除检测阶段的空间不准确，减少误报；引入交叉关联优化模块自适应更新记忆库，防止错误随时间传播，增强时间连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;两个模块协同工作稳定了SAM2，在领域内和领域外场景中都取得了最先进的性能；FreeVPS在长时间未修剪的结肠镜视频展示了稳健的跟踪能力。&lt;h4&gt;结论&lt;/h4&gt;FreeVPS具有可靠临床分析的潜力，能够有效应用于真实临床场景。&lt;h4&gt;翻译&lt;/h4&gt;现有的视频息肉分割范式通常难以平衡时空建模和领域泛化，限制了它们在真实临床场景中的应用。为应对这一挑战，我们将VPS任务重新构建为检测后跟踪范式，利用图像息肉分割模型捕捉的空间上下文，同时整合SAM2的时序建模能力。然而，在结肠镜视频的长期息肉跟踪过程中，SAM2遭受错误累积，导致雪球效应，影响分割稳定性。我们通过重新利用SAM2作为视频息肉分割器，并采用两个无需训练的模块来缓解这一问题。特别是，内部关联过滤模块消除了检测阶段产生的空间不准确，减少了误报；交叉关联优化模块自适应更新记忆库，防止错误随时间传播，增强时间连贯性。这两个模块协同工作稳定了SAM2，在领域内和领域外场景中都取得了最先进的性能。此外，我们证明了FreeVPS在长时间未修剪结肠镜视频中的稳健跟踪能力，强调了其可靠临床分析的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing video polyp segmentation (VPS) paradigms usually struggle to balancebetween spatiotemporal modeling and domain generalization, limiting theirapplicability in real clinical scenarios. To embrace this challenge, we recastthe VPS task as a track-by-detect paradigm that leverages the spatial contextscaptured by the image polyp segmentation (IPS) model while integrating thetemporal modeling capabilities of segment anything model 2 (SAM2). However,during long-term polyp tracking in colonoscopy videos, SAM2 suffers from erroraccumulation, resulting in a snowball effect that compromises segmentationstability. We mitigate this issue by repurposing SAM2 as a video polypsegmenter with two training-free modules. In particular, the intra-associationfiltering module eliminates spatial inaccuracies originating from the detectingstage, reducing false positives. The inter-association refinement moduleadaptively updates the memory bank to prevent error propagation over time,enhancing temporal coherence. Both modules work synergistically to stabilizeSAM2, achieving cutting-edge performance in both in-domain and out-of-domainscenarios. Furthermore, we demonstrate the robust tracking capabilities ofFreeVPS in long-untrimmed colonoscopy videos, underscoring its potentialreliable clinical analysis.</description>
      <author>example@mail.com (Qiang Hu, Ying Zhou, Gepeng Ji, Nick Barnes, Qiang Li, Zhiwei Wang)</author>
      <guid isPermaLink="false">2508.19705v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title>
      <link>http://arxiv.org/abs/2508.19650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Video-LevelGauge基准测试，用于系统评估大型视频语言模型(LVLMs)中的位置偏见问题。&lt;h4&gt;背景&lt;/h4&gt;现有LVLM评估基准主要关注整体性能，忽略了位置偏见这一关键但未被充分探索的方面。&lt;h4&gt;目的&lt;/h4&gt;开发专门评估LVLM位置偏见的基准测试，并提供减轻偏见的方法。&lt;h4&gt;方法&lt;/h4&gt;使用标准化探测器和定制上下文设置，能够灵活控制上下文长度、探测位置和上下文类型；结合统计测量和形态模式识别的综合分析方法评估27个最先进的LVLMs。&lt;h4&gt;主要发现&lt;/h4&gt;许多领先的开源LVLM存在显著位置偏见，通常表现出头部或邻域内容偏好；而商业模型如Gemini2.5-Pro在整个视频序列中表现一致。&lt;h4&gt;结论&lt;/h4&gt;Video-LevelGauge为评估和减轻LVLM位置偏见提供了有效工具，上下文长度、变化和模型规模分析为模型改进提供可行指导。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型(LVLMs)在视频理解方面取得了显著进展，推动了相应评估基准的发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了细微行为，如上下文位置偏见，这是LVLM性能中一个关键但未被充分探索的方面。我们提出了Video-LevelGauge，这是一个专门用于系统评估LVLM位置偏见的基准。我们使用标准化的探测器和定制的上下文设置，能够灵活控制上下文长度、探测位置和上下文类型，以模拟多样化的真实场景。此外，我们引入了一种综合分析方法，结合统计测量和形态模式识别来表征偏见。我们的基准包含438个人工策划的视频，涵盖多种类型，产生了1177个高质量的多项选择题和120个开放式问题，其有效性已得到验证，能够有效暴露位置偏见。基于这些，我们评估了27个最先进的LVLMs，包括商业和开源模型。我们的发现显示许多领先的开源模型存在显著的位置偏见，通常表现出头部或邻域内容偏好。相比之下，像Gemini2.5-Pro这样的商业模型在整个视频序列中表现出令人印象深刻的一致性能。关于上下文长度、上下文变化和模型规模的进一步分析，为减轻偏见和指导模型改进提供了可行的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding model enhancement.</description>
      <author>example@mail.com (Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang)</author>
      <guid isPermaLink="false">2508.19650v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</title>
      <link>http://arxiv.org/abs/2508.18634v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的视频描述生成方法，解决了现有方法中的运动细节不平衡问题，通过构建HMD-270K数据集和引入CSER优化方法，开发了OwlCap多模态大语言模型，在两个基准测试上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;视频描述生成旨在为视频内容生成全面且连贯的描述，有助于视频理解和生成的发展。然而，现有方法常常存在运动细节不平衡的问题，模型往往过度强调一个方面而忽视另一个方面，导致描述不完整，进而影响视频理解和生成的一致性。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频描述方法中的运动细节不平衡问题，提高视频描述的完整性和一致性，开发能够平衡捕捉运动和细节的视频描述模型。&lt;h4&gt;方法&lt;/h4&gt;从两个方面提出解决方案：1) 数据方面：构建了运动细节协调270K（HMD-270K）数据集，通过两阶段流程：运动细节融合（MDF）和细粒度检查（FGE）；2) 优化方面：引入基于组相对策略优化（GRPO）的描述集等价奖励（CSER），通过单元到集合匹配和双向验证来增强捕捉运动和细节的完整性和准确性。基于HMD-270K监督微调和带有CSER的GRPO后训练，开发了OwlCap视频描述多模态大语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;OwlCap在两个基准测试上相比基线模型取得了显著改进：在侧重细节的VDC上提高了4.2个准确率点，在侧重运动的DREAM-1K上提高了4.6个F1分数点。&lt;h4&gt;结论&lt;/h4&gt;HMD-270K数据集和OwlCap模型将公开发布，以促进视频描述研究社区的发展。&lt;h4&gt;翻译&lt;/h4&gt;视频描述旨在生成对视频内容的全面且连贯的描述，促进视频理解和生成的发展。然而，现有方法常常遭受运动细节不平衡的困扰，因为模型往往过度强调一个方面而忽视另一个方面。这种不平衡导致不完整的描述，进而导致视频理解和生成缺乏一致性。为解决这一问题，我们从两个方面提出解决方案：1) 数据方面：我们通过两阶段流程构建了协调运动细节的270K（HMD-270K）数据集：运动细节融合（MDF）和细粒度检查（FGE）。2) 优化方面：我们引入了基于组相对策略优化（GRPO）的描述集等价奖励（CSER）。CSER通过单元到集合匹配和双向验证，增强了捕捉运动和细节的完整性和准确性。基于HMD-270K监督微调和带有CSER的GRPO后训练，我们开发了OwlCap，一个具有运动细节平衡功能的强大视频描述多模态大语言模型（MLLM）。实验结果表明，与基线模型相比，OwlCap在两个基准测试上取得了显著改进：侧重细节的VDC（+4.2准确率）和侧重运动的DREAM-1K（+4.6 F1）。HMD-270K数据集和OwlCap模型将公开发布，以促进视频描述研究社区的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video captioning aims to generate comprehensive and coherent descriptions ofthe video content, contributing to the advancement of both video understandingand generation. However, existing methods often suffer from motion-detailimbalance, as models tend to overemphasize one aspect while neglecting theother. This imbalance results in incomplete captions, which in turn leads to alack of consistency in video understanding and generation. To address thisissue, we propose solutions from two aspects: 1) Data aspect: We constructedthe Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stagepipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER)based on Group Relative Policy Optimization (GRPO). CSER enhances completenessand accuracy in capturing both motion and details through unit-to-set matchingand bidirectional validation. Based on the HMD-270K supervised fine-tuning andGRPO post-training with CSER, we developed OwlCap, a powerful video captioningmulti-modal large language model (MLLM) with motion-detail balance.Experimental results demonstrate that OwlCap achieves significant improvementscompared to baseline models on two benchmarks: the detail-focused VDC (+4.2Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCapmodel will be publicly released to facilitate video captioning researchcommunity advancements.</description>
      <author>example@mail.com (Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, Xiang Bai)</author>
      <guid isPermaLink="false">2508.18634v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling</title>
      <link>http://arxiv.org/abs/2508.18463v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的上下文感知零样本异常检测框架，能够在训练过程中不接触异常样本的情况下识别监控录像中的异常事件。&lt;h4&gt;背景&lt;/h4&gt;监控录像中的异常检测具有挑战性，因为异常事件具有不可预测性和上下文依赖性，传统的异常检测方法通常需要大量异常样本进行训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种零样本异常检测框架，无需在训练阶段暴露异常样本，同时能够整合时间推理和语义上下文信息，提高复杂环境中异常检测的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出混合架构，结合TimeSformer、DPC和CLIP三种模型：TimeSformer作为视觉主干提取时空特征，DPC预测未来表示识别时间偏差，CLIP语义流实现概念级异常检测；使用InfoNCE和CPC损失进行联合训练；引入上下文门控机制通过场景感知提示或全局视频特征调节预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合预测建模与视觉-语言理解，系统能够泛化到复杂环境中未见过的行为；该框架成功弥合了零样本异常检测中时间推理和语义上下文之间的差距。&lt;h4&gt;结论&lt;/h4&gt;该上下文感知零样本异常检测框架能够有效识别监控录像中的异常事件，无需训练阶段接触异常样本，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;监控录像中的异常检测因其不可预测和上下文依赖的本质而具有固有挑战。这项工作引入了一种新颖的上下文感知零样本异常检测框架，能够在训练过程中不接触异常样本的情况下识别异常事件。提出的混合架构结合了TimeSformer、DPC和CLIP，用于建模时空动态和语义上下文。TimeSformer作为视觉主干提取丰富的时空特征，而DPC预测未来表示以识别时间偏差。此外，基于CLIP的语义流通过特定上下文的文本提示实现概念级异常检测。这些组件使用InfoNCE和CPC损失进行联合训练，将视觉输入与其时间和语义表示对齐。上下文门控机制通过使用场景感知提示或全局视频特征调节预测来增强决策能力。通过整合预测建模与视觉-语言理解，该系统能够泛化到复杂环境中先前未见的行为。该框架弥合了监控零样本异常检测中时间推理和语义上下文之间的差距。本研究的代码已在https://github.com/NK-II/Context-Aware-Zero-Shot-Anomaly-Detection-in-Surveillance上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in surveillance footage is inherently challenging due totheir unpredictable and context-dependent nature. This work introduces a novelcontext-aware zero-shot anomaly detection framework that identifies abnormalevents without exposure to anomaly examples during training. The proposedhybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporaldynamics and semantic context. TimeSformer serves as the vision backbone toextract rich spatial-temporal features, while DPC forecasts futurerepresentations to identify temporal deviations. Furthermore, a CLIP-basedsemantic stream enables concept-level anomaly detection throughcontext-specific text prompts. These components are jointly trained usingInfoNCE and CPC losses, aligning visual inputs with their temporal and semanticrepresentations. A context-gating mechanism further enhances decision-making bymodulating predictions with scene-aware cues or global video features. Byintegrating predictive modeling with vision-language understanding, the systemcan generalize to previously unseen behaviors in complex environments. Thisframework bridges the gap between temporal reasoning and semantic context inzero-shot anomaly detection for surveillance. The code for this research hasbeen made available athttps://github.com/NK-II/Context-Aware-Zero-Shot-Anomaly-Detection-in-Surveillance.</description>
      <author>example@mail.com (Md. Rashid Shahriar Khan, Md. Abrar Hasan, Mohammod Tareq Aziz Justice)</author>
      <guid isPermaLink="false">2508.18463v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</title>
      <link>http://arxiv.org/abs/2508.20063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OpenM3D，一种新的开放词汇多视角室内3D目标检测器，无需人工标注即可训练。该检测器采用2D诱导体素特征，结合类无关的3D定位损失和体素语义对齐损失进行训练，并提出了3D伪框生成方法。OpenM3D在ScanNet200和ARKitScenes基准测试中表现出高准确性和效率，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;Open-vocabulary 3D目标检测是一个新兴领域，但基于图像的方法相比基于3D点云的方法探索有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需人工标注训练的开放词汇多视角室内3D目标检测器。&lt;h4&gt;方法&lt;/h4&gt;OpenM3D是一种单阶段检测器，采用ImGeoNet模型的2D诱导体素特征，与类无关的3D定位损失和体素语义对齐损失联合训练。提出3D伪框生成方法，使用图嵌入技术将2D片段组合成连贯的3D结构，并采样多样化的CLIP特征进行体素特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;提出的伪框方法比其他方法具有更高的精确率和召回率；训练高精度单阶段检测器需要两种损失都向高质量目标学习。&lt;h4&gt;结论&lt;/h4&gt;OpenM3D是一种高效的检测器，仅需多视角图像作为输入，在ScanNet200和ARKitScenes室内基准测试中表现出更高的准确性和速度（每场景0.3秒），在准确性和速度上都优于强大的两阶段方法和基线方法。&lt;h4&gt;翻译&lt;/h4&gt;Open-vocabulary (OV) 3D目标检测是一个新兴领域，但通过基于图像的方法进行探索仍然有限，相比基于3D点云的方法。我们引入了OpenM3D，一种新颖的开放词汇多视角室内3D目标检测器，无需人工标注即可训练。特别是，OpenM3D是一种单阶段检测器，采用来自ImGeoNet模型的2D诱导体素特征。为了支持OV，它与类无关的3D定位损失联合训练，需要高质量的3D伪框，以及与多样化预训练CLIP特征联合训练的体素语义对齐损失。我们遵循OV-3DET的训练设置，提供姿态RGB-D图像，但没有3D框或类别的人工标注。我们提出了一种使用图嵌入技术的3D伪框生成方法，将2D片段组合成连贯的3D结构。我们的伪框比其他方法（包括OV-3DET中提出的方法）实现了更高的精确率和召回率。我们进一步从与每个连贯3D结构关联的2D片段中采样多样化的CLIP特征，与相应的体素特征对齐。训练高精度单阶段检测器的关键需要两种损失都向高质量目标学习。在推理时，OpenM3D是一种高效的检测器，仅需多视角图像作为输入，并在ScanNet200和ARKitScenes室内基准测试中相比现有方法表现出更高的准确性和速度（每场景0.3秒）。我们在准确性和速度上都优于强大的两阶段方法（利用我们的类无关检测器与ViT CLIP-based OV分类器）和基线方法（整合多视角深度估计器）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放词汇多视角室内3D物体检测问题，即系统能够识别训练时未见过的物体类别，仅通过文本描述就能检测3D空间中的物体，且不需要昂贵的3D传感器输入。这个问题在现实中很重要，因为传统3D物体检测需要大量昂贵的人工标注数据，而现有开放词汇方法大多依赖3D点云输入，需要深度相机等昂贵设备，限制了实际应用。解决这一问题可以降低3D检测的使用门槛，使系统能够灵活识别新类别物体，适用于更广泛的应用场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：基于点云的方法需要昂贵3D传感器，而基于图像的开放词汇3D检测研究有限。他们借鉴了多项现有工作：采用ImGeoNet模型的2D诱导体素特征作为基础架构；利用预训练CLIP模型实现文本-图像语义对齐；使用SAM进行类无关2D分割；应用图嵌入技术组合多视角信息；遵循OV-3DET的无标注训练设置。作者设计了一个综合方案，通过3D伪框生成和体素-语义对齐两个关键技术，实现了无需人工标注的高效开放词汇3D检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多视角RGB图像而非3D点云进行检测，通过无监督方式生成高质量3D伪框作为训练信号，并对齐3D体素特征与CLIP语义特征实现开放词汇分类。整体流程分为训练和推理两阶段：训练时，首先用SAM分割多视角图像得到2D片段，然后通过图嵌入技术将片段组合成3D结构生成伪框；接着提取多视角图像特征并投影到3D空间形成体素特征；最后联合优化类无关定位损失和体素-语义对齐损失训练检测器。推理时，仅需多视角RGB图像，检测器直接输出3D边界框和开放词汇分类结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个开放词汇多视角3D物体检测器；2) 基于图嵌入的3D伪框生成方法，考虑所有视角信息提高质量；3) 体素-语义对齐损失，实现高效开放词汇分类；4) 高效单阶段检测器，推理速度达0.3秒/场景。相比之前工作，OpenM3D不需要3D点云输入和人工标注，推理时仅需RGB图像；采用单阶段架构而非两阶段方法；不依赖CLIP ViT模型进行推理，计算成本更低；在准确性和速度上均显著优于现有方法，如比OV-3DET快16倍，比OpenMask3D快100倍以上。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenM3D首次实现了无需人工标注、仅需多视角图像输入的高效开放词汇多视角室内3D物体检测，通过创新的3D伪框生成和体素-语义对齐方法，在准确性和速度上均显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary (OV) 3D object detection is an emerging field, yet itsexploration through image-based methods remains limited compared to 3D pointcloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-viewindoor 3D object detector trained without human annotations. In particular,OpenM3D is a single-stage detector adapting the 2D-induced voxel features fromthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic3D localization loss requiring high-quality 3D pseudo boxes and avoxel-semantic alignment loss requiring diverse pre-trained CLIP features. Wefollow the training setting of OV-3DET where posed RGB-D images are given butno human annotations of 3D boxes or classes are available. We propose a 3DPseudo Box Generation method using a graph embedding technique that combines 2Dsegments into coherent 3D structures. Our pseudo-boxes achieve higher precisionand recall than other methods, including the method proposed in OV-3DET. Wefurther sample diverse CLIP features from 2D segments associated with eachcoherent 3D structure to align with the corresponding voxel feature. The key totraining a highly accurate single-stage detector requires both losses to belearned toward high-quality targets. At inference, OpenM3D, a highly efficientdetector, requires only multi-view images for input and demonstrates superioraccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoorbenchmarks compared to existing methods. We outperform a strong two-stagemethod that leverages our class-agnostic detector with a ViT CLIP-based OVclassifier and a baseline incorporating multi-view depth estimator on bothaccuracy and speed.</description>
      <author>example@mail.com (Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo)</author>
      <guid isPermaLink="false">2508.20063v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2508.19909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新方法，通过利用2D基础模型生成的分割掩码来增强3D语义分割性能，解决了3D数据标注困难和有限的问题。&lt;h4&gt;背景&lt;/h4&gt;当前3D语义分割方法面临三大挑战：1) 3D点云数据标注困难（量大、不规则、无序）；2) 现有方法仅关注3D领域，未充分利用2D和3D数据的互补性；3) 扩展原始标签或生成伪标签的方法未能充分利用这些标签或处理其中的噪声。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法，最大化稀疏可用3D注释的效用，通过整合2D基础模型生成的分割掩码，提高3D弱监督分割的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 利用2D基础模型生成分割掩码；2. 建立3D场景与2D视图间的几何对应关系，将2D分割掩码传播到3D空间；3. 将高度稀疏的注释扩展到3D掩码所定义的区域，显著增加可用标签池；4. 在3D点云增强上应用基于置信度和一致性的正则化，选择可靠的伪标签；5. 将可靠的伪标签传播到3D掩码上生成更多标签。&lt;h4&gt;主要发现&lt;/h4&gt;通过将2D基础模型的能力与有限的3D注释相结合，可以有效解决3D语义分割中的标注困难问题，显著提高分割性能。&lt;h4&gt;结论&lt;/h4&gt;这种创新策略弥合了有限3D注释与强大2D基础模型能力之间的差距，最终提高了3D弱监督分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;当前3D语义分割方法建议使用有限的注释来训练模型，以解决标注大型、不规则和无序的3D点云数据的困难。它们通常只关注3D领域，没有利用2D和3D数据的互补性。此外，一些方法扩展原始标签或生成伪标签来指导训练，但它们往往无法充分利用这些标签或解决其中的噪声问题。同时，全面且适应性强的基础模型的出现为分割2D数据提供了有效的解决方案。利用这一进步，我们提出了一种新方法，通过整合2D基础模型生成的分割掩码来最大化稀疏可用3D注释的效用。我们通过建立3D场景和2D视图之间的几何对应关系，将2D分割掩码传播到3D空间。我们将高度稀疏的注释扩展到3D掩码所定义的区域，从而显著增加了可用标签池。此外，我们在3D点云的增强上应用基于置信度和一致性的正则化，选择可靠的伪标签，这些标签进一步传播到3D掩码上以生成更多标签。这种创新策略弥合了有限3D注释与强大2D基础模型能力之间的差距，最终提高了3D弱监督分割的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云数据标注困难的问题，具体是3D弱监督语义分割任务。这个问题在现实中很重要，因为3D点云数据具有稀疏、不规则、高维、模糊和无序的特点，使得人工标注极其耗时耗力。完全标注3D数据成本高昂，而现有方法往往只关注3D领域，未能充分利用2D和3D数据的互补性，或者无法有效处理伪标签中的噪声。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D点云标注困难和现有方法局限性，注意到2D基础模型(如Semantic-SAM)在图像分割方面的强大能力。然后思考如何利用2D和3D数据间的几何对应关系，将2D模型能力迁移到3D领域。方法设计借鉴了多项现有工作：使用Semantic-SAM生成2D分割掩码；借鉴RAC-Net的一致性正则化和可靠伪标签选择；采用噪声鲁棒损失函数处理标签噪声；使用PointWolf和仿射变换进行数据增强；利用链接矩阵和空间对应关系进行2D-3D投影。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型生成高质量分割掩码，将其投影到3D空间，结合稀疏3D标注扩展可用标签，并通过一致性正则化和噪声鲁棒训练处理标签噪声。整体流程包括：1)使用Semantic-SAM生成2D分割掩码；2)通过相机矩阵和深度信息将2D掩码投影到3D空间并合并不同视图掩码；3)对点云及其增强版本进行预测，基于置信度和不确定性分类预测；4)将初始稀疏标注和可靠伪标签扩展到3D掩码区域；5)结合多种损失函数进行训练，包括原始标注损失、可靠伪标签损失、模糊伪标签损失和噪声鲁棒损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统性地将2D基础模型分割能力应用于3D弱监督分割，通过2D-3D掩码投影融合；2)基于可靠伪标签在掩码区域中的比例，智能扩展标签到整个区域；3)使用归一化交叉熵和反向交叉熵损失处理扩展标签中的噪声；4)构建完整的端到端框架。相比之前工作，该方法充分利用了2D图像的丰富信息和2D基础模型能力，不需要完全标注的3D数据，仅需稀疏标注和未标注2D图像，显著提高了分割精度，且只在训练阶段使用2D图像，推理时直接使用3D点云，更适用于实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新方法，通过将2D基础模型的分割掩码投影到3D空间并结合一致性正则化和噪声鲁棒训练，显著提升了3D点云弱监督语义分割的性能，实现了在仅有稀疏标注情况下的高质量分割结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current methods for 3D semantic segmentation propose training models withlimited annotations to address the difficulty of annotating large, irregular,and unordered 3D point cloud data. They usually focus on the 3D domain only,without leveraging the complementary nature of 2D and 3D data. Besides, somemethods extend original labels or generate pseudo labels to guide the training,but they often fail to fully use these labels or address the noise within them.Meanwhile, the emergence of comprehensive and adaptable foundation models hasoffered effective solutions for segmenting 2D data. Leveraging thisadvancement, we present a novel approach that maximizes the utility of sparselyavailable 3D annotations by incorporating segmentation masks generated by 2Dfoundation models. We further propagate the 2D segmentation masks into the 3Dspace by establishing geometric correspondences between 3D scenes and 2D views.We extend the highly sparse annotations to encompass the areas delineated by 3Dmasks, thereby substantially augmenting the pool of available labels.Furthermore, we apply confidence- and uncertainty-based consistencyregularization on augmentations of the 3D point cloud and select the reliablepseudo labels, which are further spread on the 3D masks to generate morelabels. This innovative strategy bridges the gap between limited 3D annotationsand the powerful capabilities of 2D foundation models, ultimately improving theperformance of 3D weakly supervised segmentation.</description>
      <author>example@mail.com (Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin)</author>
      <guid isPermaLink="false">2508.19909v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception</title>
      <link>http://arxiv.org/abs/2508.19638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoPLOT的新型协作感知框架，利用点级优化令牌解决现有方法中丢弃3D结构信息的问题&lt;h4&gt;背景&lt;/h4&gt;现有协作感知方法通常使用2D鸟瞰图表示作为中间特征，这会丢弃对准确物体识别和定位至关重要的细粒度3D结构线索&lt;h4&gt;目的&lt;/h4&gt;引入点级令牌作为协作感知的中间表示，并解决点云数据固有的无序性、大规模和位置敏感性带来的挑战&lt;h4&gt;方法&lt;/h4&gt;提出CoPLOT框架，包含点原生处理流程、语义感知的令牌重排序模块、频率增强的状态空间模型和邻居到自我对齐模块&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界数据集上的实验表明，CoPLOT优于最先进模型，且具有更低的通信和计算开销&lt;h4&gt;结论&lt;/h4&gt;CoPLOT能够有效保留关键的3D结构信息，同时提高感知性能，是一种有效的协作感知框架&lt;h4&gt;翻译&lt;/h4&gt;协作感知允许代理通过交换中间特征来增强其感知能力。现有方法通常将这些中间特征组织为2D鸟瞰图表示，这会丢弃对准确物体识别和定位至关重要的细粒度3D结构线索。为此，我们首先引入点级令牌作为协作感知的中间表示。然而，点云数据本质上是无序的、大规模的且对位置敏感，这使得产生保留详细结构信息的紧凑且对齐的点级令牌序列具有挑战性。因此，我们提出了CoPLOT，一种利用点级优化令牌的新型协作感知框架。它包含点原生处理流程，包括令牌重排序、序列建模和多智能体空间对齐。语义感知的令牌重排序模块利用场景级和令牌级语义信息生成自适应的1D重排序。频率增强的状态空间模型捕获空间和频谱域中的长程序列依赖关系，提高前景令牌和背景杂波之间的区分度。最后，邻居到自我对齐模块应用闭环过程，结合全局智能体级校正和局部令牌级细化，以减轻定位噪声。在模拟和真实世界数据集上的大量实验表明，CoPLOT优于最先进的模型，且具有更低的通信和计算开销。代码将在https://github.com/CheeryLeeyy/CoPLOT上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决协同感知中现有方法使用2D鸟瞰图(BEV)表示作为中间特征时丢失关键3D结构线索的问题。这个问题在自动驾驶和智能交通系统中非常重要，因为准确的3D物体识别和定位对安全至关重要，而丢失的垂直信息与物体类别、地形变化和空间划分密切相关，同时现有方法还包含大量背景噪声导致不必要的计算和通信成本，不利于实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有2D BEV方法的局限性，包括丢失精细3D结构线索和包含不必要背景噪声导致的计算通信开销大。然后提出使用点级令牌作为中间表示来保留完整3D信息。具体设计上，作者借鉴了状态空间模型(SSMs)在建模长程依赖方面的优势，受到生物视觉系统使用频率选择性滤波器分离目标的启发，并参考了协同感知领域中的V2VNet、CoBEVT等方法和点云处理技术如PointPillars，最终设计了语义感知令牌重排序、频率增强状态空间模型和邻居到自我对齐三个核心模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用点级令牌作为协同感知的中间表示，保留完整3D结构信息同时减少计算通信开销。整体流程包括：1)点令牌化将原始点云转换为1D令牌序列；2)语义感知动态编码包括场景上下文提取、令牌重排序、频率增强的状态空间建模和重要性过滤；3)消息共享传输选定的令牌；4)点聚合将相邻智能体令牌转换到自我坐标系；5)语义感知动态融合通过邻居到自我对齐模块调整令牌位置；6)任务网络产生最终预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)点级令牌作为中间表示保留完整3D信息；2)语义感知令牌重排序模块利用场景和令牌级语义信息自适应重排序；3)频率增强状态空间模型受生物视觉启发提取频域表示增强长程依赖建模；4)邻居到自我对齐模块结合全局和局部调整处理定位噪声。相比之前工作，CoPLOT不同于传统2D BEV表示和固定模式排序，能更好区分前景背景，同时显著降低计算开销约80%和通信开销约90%，在三个数据集上提升感知性能达4.2%-9.8%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoPLOT通过引入点级优化令牌作为协同感知的中间表示，结合语义感知令牌重排序、频率增强状态空间模型和邻居到自我对齐等创新模块，在保留完整3D结构信息的同时，显著提升了感知性能并大幅降低了计算和通信开销。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception allows agents to enhance their perceptualcapabilities by exchanging intermediate features. Existing methods typicallyorganize these intermediate features as 2D bird's-eye-view (BEV)representations, which discard critical fine-grained 3D structural cuesessential for accurate object recognition and localization. To this end, wefirst introduce point-level tokens as intermediate representations forcollaborative perception. However, point-cloud data are inherently unordered,massive, and position-sensitive, making it challenging to produce compact andaligned point-level token sequences that preserve detailed structuralinformation. Therefore, we present CoPLOT, a novel Collaborative perceptionframework that utilizes Point-Level Optimized Tokens. It incorporates apoint-native processing pipeline, including token reordering, sequencemodeling, and multi-agent spatial alignment. A semantic-aware token reorderingmodule generates adaptive 1D reorderings by leveraging scene-level andtoken-level semantic information. A frequency-enhanced state space modelcaptures long-range sequence dependencies across both spatial and spectraldomains, improving the differentiation between foreground tokens and backgroundclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loopprocess, combining global agent-level correction with local token-levelrefinement to mitigate localization noise. Extensive experiments on bothsimulated and real-world datasets show that CoPLOT outperforms state-of-the-artmodels, with even lower communication and computation overhead. Code will beavailable at https://github.com/CheeryLeeyy/CoPLOT.</description>
      <author>example@mail.com (Yang Li, Quan Yuan, Guiyang Luo, Xiaoyuan Fu, Rui Pan, Yujia Yang, Congzhang Shao, Yuewen Liu, Jinglin Li)</author>
      <guid isPermaLink="false">2508.19638v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation</title>
      <link>http://arxiv.org/abs/2508.19290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对2D范围视图LiDAR分割的高效对抗防御方法，通过基于模型的净化框架实现了强大的对抗鲁棒性，同时保持计算效率。&lt;h4&gt;背景&lt;/h4&gt;LiDAR分割对自动驾驶车辆的可靠感知至关重要，但现代分割网络容易受到对抗攻击。现有防御方法主要针对原始3D点云网络且计算密集，而广泛使用的2D范围视图LiDAR分割缺乏专用轻量级对抗防御。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的基于模型的净化框架，专门用于2D范围视图LiDAR分割中的对抗防御。&lt;h4&gt;方法&lt;/h4&gt;提出范围视图域中的直接攻击公式，并开发基于数学优化问题的可解释净化网络，实现最小计算开销下的强对抗鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在开放基准测试上取得有竞争力性能，一致优于生成和对抗训练基线，实际车辆部署验证了其在真实自动驾驶场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的框架成功解决了2D范围视图LiDAR分割的对抗防御问题，相比现有方法更轻量且高效，在实际应用中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的分割对自动驾驶车辆中的可靠感知至关重要，然而现代分割网络极易受到对抗攻击，这可能危及安全。大多数现有防御是为直接在原始3D点云上运行的网络设计的，并依赖大型计算密集型生成模型。然而，许多最先进的LiDAR分割管道在更高效的2D范围视图表示上运行。尽管它们被广泛采用，但针对该领域的专用轻量级对抗防御仍然很少被探索。我们引入了一个高效的基于模型的净化框架，专门用于2D范围视图LiDAR分割中的对抗防御。我们提出了范围视图域中的直接攻击公式，并开发了一个基于数学优化问题的可解释净化网络，实现了强大的对抗鲁棒性，同时计算开销最小。我们的方法在开放基准测试上取得了具有竞争力的性能，一致优于生成和对抗训练基线。更重要的是，在演示车辆上的实际部署展示了该框架在实际自动驾驶场景中提供准确操作的能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR分割网络容易受到对抗性攻击的问题。在自动驾驶领域，LiDAR是环境感知的核心技术，而分割是理解周围场景的关键。对抗性攻击可以通过微小扰动导致分割网络做出错误判断，严重影响行车安全。现有防御方法主要针对3D点云设计，计算量大且不适合车载平台，而当前先进的LiDAR分割系统使用更高效的2D range view表示，缺乏专门的轻量级防御方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行设计：1) 发现3D对抗性攻击在投影到2D range view时会引入量化伪影，削弱攻击效果；2) 现有防御方法基于生成模型，计算复杂度高，不适合车载平台。作者借鉴了对抗性训练和生成模型防御的基本思想，但针对2D range view表示进行了专门优化。他们将净化问题建模为受约束的优化问题，利用半二次分裂方法和深度展开技术，将优化算法转化为高效且可解释的神经网络(DU-AP)。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将对抗性净化问题建模为结构化去噪问题，利用range view数据的特定结构(如水平梯度特性)设计受约束的优化问题。实现流程：1) 在2D range view域设计对抗性攻击，直接添加受控扰动；2) 将净化问题建模为优化问题，最小化扰动差异并保持range view结构；3) 使用半二次分裂方法将问题分解为数据一致性更新和去噪两个子问题；4) 通过深度展开技术将优化算法转化为5层网络结构(DU-AP)；5) 将DU-AP作为预处理模块，在分割网络前净化对抗性输入。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 专门针对2D range view表示设计对抗性攻击，避免3D到2D投影的量化伪影；2) 轻量级防御框架DU-AP，参数量减少99%以上；3) 可解释的神经网络结构，基于深度展开技术；4) 在真实自动驾驶车辆上成功部署验证。相比之前工作的不同：1) 攻击和防御都专门针对2D range view表示，而非传统3D点云；2) 基于优化理论而非生成模型，大幅降低计算复杂度；3) 不仅在标准数据集验证，还在真实场景测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种高效、轻量级的对抗性防御方法，专门针对2D range view LiDAR分割系统，通过将净化问题建模为优化问题并转化为神经网络，在保持高防御效果的同时大幅降低计算复杂度，实现了在真实自动驾驶车辆上的实时部署。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based segmentation is essential for reliable perception in autonomousvehicles, yet modern segmentation networks are highly susceptible toadversarial attacks that can compromise safety. Most existing defenses aredesigned for networks operating directly on raw 3D point clouds and rely onlarge, computationally intensive generative models. However, manystate-of-the-art LiDAR segmentation pipelines operate on more efficient 2Drange view representations. Despite their widespread adoption, dedicatedlightweight adversarial defenses for this domain remain largely unexplored. Weintroduce an efficient model-based purification framework tailored foradversarial defense in 2D range-view LiDAR segmentation. We propose a directattack formulation in the range-view domain and develop an explainablepurification network based on a mathematical justified optimization problem,achieving strong adversarial resilience with minimal computational overhead.Our method achieves competitive performance on open benchmarks, consistentlyoutperforming generative and adversarial training baselines. More importantly,real-world deployment on a demo vehicle demonstrates the framework's ability todeliver accurate operation in practical autonomous driving scenarios.</description>
      <author>example@mail.com (Alexandros Gkillas, Ioulia Kapsali, Nikos Piperigkos, Aris S. Lalos)</author>
      <guid isPermaLink="false">2508.19290v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities</title>
      <link>http://arxiv.org/abs/2508.19905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted and under review at IEEE OJVT, August 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文首次全面综述了高光谱成像技术在汽车应用中的现状，评估了HSI技术的优势和局限性，分析了商业可用HSI相机的性能，并回顾了相关数据集和应用，指出了HSI在ADAS/AD应用中的研究潜力和商业化挑战。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像(HSI)为高级驾驶辅助系统(ADAS)和自动驾驶(AD)应用提供了变革性的传感模式，能够通过精细的光谱分辨率实现材料级别的场景理解，这是传统RGB成像无法做到的。&lt;h4&gt;目的&lt;/h4&gt;对汽车应用中的高光谱成像技术进行全面综述，考察当前HSI技术的优势、局限性和适用性，分析商业HSI相机的性能，回顾相关数据集和应用，并确定HSI在ADAS/AD领域的现状和未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;进行定性分析HSI技术的优势和局限性；分析216种商业上可用的HSI和多光谱成像相机，并根据关键汽车标准（帧率、空间分辨率、光谱维度和AEC-Q100温度标准符合性）进行基准测试；回顾最近的HSI数据集和应用案例。&lt;h4&gt;主要发现&lt;/h4&gt;HSI的研究潜力与其商业准备程度之间存在显著差距；只有四款相机满足定义的性能阈值，且没有一款符合AEC-Q100温度标准要求；当前HSI数据集在规模、光谱一致性、光谱通道数量和环境多样性方面存在局限，这对感知算法的开发和HSI在ADAS/AD应用中真实潜力的验证提出了挑战。&lt;h4&gt;结论&lt;/h4&gt;截至2025年，高光谱成像在汽车领域的应用仍面临商业化挑战，需要进一步研究以实现光谱成像在ADAS和自主系统中的实际集成。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)为高级驾驶辅助系统(ADAS)和自动驾驶(AD)应用提供了变革性的传感模式，通过超越传统RGB成像能力的光谱分辨率实现材料级别的场景理解。本文首次对汽车应用中的高光谱成像进行了全面综述，考察了当前HSI技术在ADAS/AD背景下的优势、局限性和适用性。除了这一定性分析，我们还分析了216种商业上可用的高光谱和多光谱成像相机，并根据关键汽车标准对它们进行基准测试：帧率、空间分辨率、光谱维度和AEC-Q100温度标准符合性。我们的分析揭示了HSI的研究潜力与其商业准备程度之间存在显著差距。只有四款相机满足定义的性能阈值，且没有一款符合AEC-Q100要求。此外，本文回顾了最近的HSI数据集和应用，包括道路表面分类、行人可分离性和恶劣天气感知的语义分割。我们的综述显示，当前HSI数据集在规模、光谱一致性、光谱通道数量和环境多样性方面存在局限，这对感知算法的开发和HSI在ADAS/AD应用中真实潜力的充分验证提出了挑战。本综述论文确立了截至2025年高光谱成像在汽车领域的现状，并概述了将光谱成像整合到ADAS和自主系统中的关键研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) offers a transformative sensing modality forAdvanced Driver Assistance Systems (ADAS) and autonomous driving (AD)applications, enabling material-level scene understanding through fine spectralresolution beyond the capabilities of traditional RGB imaging. This paperpresents the first comprehensive review of HSI for automotive applications,examining the strengths, limitations, and suitability of current HSItechnologies in the context of ADAS/AD. In addition to this qualitative review,we analyze 216 commercially available HSI and multispectral imaging cameras,benchmarking them against key automotive criteria: frame rate, spatialresolution, spectral dimensionality, and compliance with AEC-Q100 temperaturestandards. Our analysis reveals a significant gap between HSI's demonstratedresearch potential and its commercial readiness. Only four cameras meet thedefined performance thresholds, and none comply with AEC-Q100 requirements. Inaddition, the paper reviews recent HSI datasets and applications, includingsemantic segmentation for road surface classification, pedestrian separability,and adverse weather perception. Our review shows that current HSI datasets arelimited in terms of scale, spectral consistency, the number of spectralchannels, and environmental diversity, posing challenges for the development ofperception algorithms and the adequate validation of HSI's true potential inADAS/AD applications. This review paper establishes the current state of HSI inautomotive contexts as of 2025 and outlines key research directions towardpractical integration of spectral imaging in ADAS and autonomous systems.</description>
      <author>example@mail.com (Imad Ali Shah, Jiarong Li, Roshan George, Tim Brophy, Enda Ward, Martin Glavin, Edward Jones, Brian Deegan)</author>
      <guid isPermaLink="false">2508.19905v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots</title>
      <link>http://arxiv.org/abs/2508.19788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, Accepted for IEEE RO-MAN 2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的框架，用于估计日常室内场景中的事故易发区域，旨在提高服务机器人在以人为中心的环境中实时风险感知能力。该框架通过基于语义图的传播算法对对象级风险和上下文进行建模，实现了对潜在风险的推断，并在数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;随着机器人逐渐融入日常生活，特别是在家庭环境中，机器人预见和应对环境危害的能力对于确保用户安全、信任和有效的人机交互至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够估计日常室内场景中事故易发区域的框架，提高服务机器人在以人为中心的环境中实时风险感知能力，使机器人能够预见和应对环境危害，确保用户安全、信任和有效的人机交互。&lt;h4&gt;方法&lt;/h4&gt;研究采用基于语义图的传播算法对对象级风险和上下文进行建模。每个对象表示为一个带有相关风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称地传播到低风险对象。这种方法设计具有可解释性和轻量级机载部署特点。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在带有人工标注风险区域的数据集上验证，实现了二值风险检测75%的准确率。系统与人类感知高度一致，特别是在涉及尖锐或不稳定物体的场景中表现突出。&lt;h4&gt;结论&lt;/h4&gt;上下文感知的风险推理具有增强机器人场景理解和在共享人机空间中主动安全行为的潜力。该框架可作为未来系统能够做出上下文驱动的安全决策、提供实时警报或在家庭环境中自主协助用户避免或减轻危害的基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于估计日常室内场景中事故易发区域的新框架，旨在提高在以人为中心的环境中运行的服务机器人的实时风险意识。随着机器人融入日常生活，特别是在家庭中，预见和应对环境危害的能力对于确保用户安全、信任和有效的人机交互至关重要。我们的方法通过基于语义图的传播算法对对象级风险和上下文进行建模。每个对象表示为一个带有相关风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称地传播到低风险对象。这使得机器人能够推断潜在的隐患，即使它们没有明确可见或标记。为便于解释和轻量级机载部署而设计，我们的方法在带有人工标注风险区域的数据集上进行了验证，实现了二值风险检测75%的准确率。该系统与人类感知高度一致，特别是在涉及尖锐或不稳定物体的场景中。这些结果强调了上下文感知风险推理在增强机器人场景理解和共享人机空间中主动安全行为的潜力。该框架可作为未来系统能够做出上下文驱动的安全决策、提供实时警报或在家庭环境中自主协助用户避免或减轻危害的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是让服务机器人在家庭环境中能够感知和估计事故风险，特别是那些基于上下文的潜在风险。这个问题很重要，因为随着机器人越来越多地进入家庭环境，它们需要能够预见和响应环境中的危险以确保用户安全、建立信任和实现有效的人机交互。在老龄化社会中，家庭安全是一个紧迫问题，能够预测风险的服务机器人可以发挥重要作用，同时主动识别和减轻风险的机器人能增强用户信任，特别对弱势群体而言。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有机器人系统主要关注几何或语义识别，但服务机器人需要进一步预测环境中的上下文风险。他们指出了现有方法的局限性，如仅基于观察对象确定风险会导致误判，或需要静态规则。作者借鉴了场景图表示法来建模对象关系，参考了图神经网络技术改进安全感知，并利用大型语言模型和3D模拟环境等现有技术，但进行了改进以适应风险传播的需求。他们设计了一个轻量级框架，整合对象级风险估计、语义空间关系和通过场景图的不对称风险传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于语义图的风险传播算法建模对象级风险和上下文，每个对象表示为带有风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称地传播到低风险对象，使机器人能推断潜在风险。整体流程包括：1)风险估计：使用日本消费者事务厅的40万份事故报告数据计算对象风险分数；2)风险传播：构建场景图，通过事故相关性、空间距离和风险差异三个因素迭代更新对象风险分数；3)可视化：生成RGB热图展示空间风险分布，使用高斯滤波器平滑高低风险区域过渡。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于语义图的风险估计框架；2)不对称风险扩散算法；3)与人类感知高度一致的验证结果；4)适合实时部署的轻量级设计。相比之前的工作，不同之处在于：1)直接处理RGB-D视觉数据而非依赖中间文本表示；2)使用真实世界事故统计数据而非仅靠语言模型；3)风险只从高风险对象传播到低风险对象，实现不对称传播；4)能推断间接风险，如架子边缘的碗放在滑表面上；5)通过动态调整风险值减少误报。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于语义图的不对称风险传播框架，使服务机器人能够通过分析对象间的空间关系和语义相关性，在家庭环境中准确识别和推断潜在的事故风险，显著提高了机器人对人类风险感知的模拟程度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel framework for estimating accident-prone regions ineveryday indoor scenes, aimed at improving real-time risk awareness in servicerobots operating in human-centric environments. As robots become integratedinto daily life, particularly in homes, the ability to anticipate and respondto environmental hazards is crucial for ensuring user safety, trust, andeffective human-robot interaction. Our approach models object-level risk andcontext through a semantic graph-based propagation algorithm. Each object isrepresented as a node with an associated risk score, and risk propagatesasymmetrically from high-risk to low-risk objects based on spatial proximityand accident relationship. This enables the robot to infer potential hazardseven when they are not explicitly visible or labeled. Designed forinterpretability and lightweight onboard deployment, our method is validated ona dataset with human-annotated risk regions, achieving a binary risk detectionaccuracy of 75%. The system demonstrates strong alignment with humanperception, particularly in scenes involving sharp or unstable objects. Theseresults underline the potential of context-aware risk reasoning to enhancerobotic scene understanding and proactive safety behaviors in sharedhuman-robot spaces. This framework could serve as a foundation for futuresystems that make context-driven safety decisions, provide real-time alerts, orautonomously assist users in avoiding or mitigating hazards within homeenvironments.</description>
      <author>example@mail.com (Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata)</author>
      <guid isPermaLink="false">2508.19788v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation</title>
      <link>http://arxiv.org/abs/2508.19699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PRCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LabelGS是一种通过为3D高斯表示增加对象标签来增强3DGS分割能力的方法，实现了高效的3D场景分割，相比Feature-3DGS有显著的速度提升和性能改进。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting (3DGS)是一种新兴的3D场景显式表示方法，能够提供高保真度的重建和高效的渲染，但它缺乏3D分割能力，限制了在需要场景理解的任务中的适用性。&lt;h4&gt;目的&lt;/h4&gt;解决3DGS缺乏3D分割能力的问题，提高其在需要场景理解的任务中的适用性，实现对特定对象组件的识别和隔离。&lt;h4&gt;方法&lt;/h4&gt;提出Label-aware 3D Gaussian Splatting (LabelGS)，引入跨视图一致的3D高斯语义掩码，采用遮挡分析模型避免优化过程中对遮挡的过拟合，使用主高斯标记模型将2D语义先验提升到3D高斯，应用高斯投影过滤器避免标签冲突，并通过随机区域采样策略实现高斯表示的有效解耦。&lt;h4&gt;主要发现&lt;/h4&gt;LabelGS在3D场景分割任务中超越了之前的最先进方法，包括Feature-3DGS；在1440X1080分辨率下，相比Feature-3DGS实现了22倍训练速度提升。&lt;h4&gt;结论&lt;/h4&gt;LabelGS成功解决了3DGS缺乏3D分割能力的问题，通过增强高斯表示和优化过程，显著提高了3D场景分割的效率和性能。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯泼溅(3DGS)已成为一种新颖的3D场景显式表示方法，提供高保真度重建和高效渲染。然而，3DGS缺乏3D分割能力，这限制了它在需要场景理解的任务中的适用性。识别和隔离特定对象组件至关重要。为解决这一局限性，我们提出了标签感知3D高斯泼溅(LabelGS)，一种通过为高斯表示增加对象标签来增强3DGS的方法。LabelGS为3D高斯引入了跨视图一致的语义掩码，并采用了一种新的遮挡分析模型来避免在优化过程中对遮挡进行过拟合，主高斯标记模型将2D语义先验提升到3D高斯，以及高斯投影过滤器来避免高斯标签冲突。我们的方法实现了高斯表示的有效解耦，并通过随机区域采样策略改进了3DGS优化过程，显著提高了效率。大量实验表明，LabelGS在3D场景分割任务中超越了之前的最先进方法，包括Feature-3DGS。值得注意的是，在1440X1080分辨率下，LabelGS相比Feature-3DGS实现了显著的22倍训练速度提升。我们的代码将在https://github.com/garrisonz/LabelGS上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯溅射(3DGS)缺乏3D分割能力的问题。这个问题在现实和研究中非常重要，因为3D场景分割是理解和解释复杂3D环境的基础任务，对于3D医学数据分析、机器人导航、自动驾驶等多种应用至关重要。缺乏语义标注的3D数据一直是3D场景分割的挑战，因此需要一种高效的方法来实现3D分割。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到3DGS虽然能高效渲染3D场景但缺乏分割能力，然后分析了现有2D到3D分割方法的局限性：难以捕捉3D空间特性和计算成本高。作者借鉴了DEVA模型获取跨视图掩码、3DGS的基本表示方法，但创新性地为高斯添加了标签属性。设计思路是通过给高斯表示添加对象标签来增强其分割能力，同时解决遮挡问题和标签冲突问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为3D高斯添加标签属性，使每个高斯能够携带语义信息，并通过跨视图一致的语义掩码来优化高斯模型。整体流程包括：1)使用DEVA模型获取跨视图一致的语义掩码；2)通过遮挡分析模型(OAM)处理遮挡区域；3)使用主高斯标记策略(MGL)将2D标签提升到3D高斯；4)引入高斯投影过滤器(GPF)解决标签冲突；5)结合多种损失函数优化模型；6)通过用户选择目标掩码实现3D对象分割和渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨视图一致的语义掩码获取；2)遮挡分析模型(OAM)解决遮挡问题；3)主高斯标记策略(MGL)实现2D到3D标签提升；4)高斯投影过滤器(GPF)解决标签冲突。相比之前的工作，LabelGS不需要学习高维语义特征(如Feature-3DGS的512维特征)，不使用自编码器压缩语义信息(如LangSplat)，不依赖视频跟踪模型进行聚类(如Gaussian Grouping)，而是直接为高斯分配标签，实现了更精确的边界分割和22倍的速度提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LabelGS通过为3D高斯添加标签属性，结合创新的遮挡处理和标签提升机制，实现了高效且精确的3D场景分割，在保持高质量分割结果的同时显著提高了训练速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representationfor 3D scenes, offering both high-fidelity reconstruction and efficientrendering. However, 3DGS lacks 3D segmentation ability, which limits itsapplicability in tasks that require scene understanding. The identification andisolating of specific object components is crucial. To address this limitation,we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augmentsthe Gaussian representation with object label.LabelGS introduces cross-viewconsistent semantic masks for 3D Gaussians and employs a novel OcclusionAnalysis Model to avoid overfitting occlusion during optimization, MainGaussian Labeling model to lift 2D semantic prior to 3D Gaussian and GaussianProjection Filter to avoid Gaussian label conflict. Our approach achieveseffective decoupling of Gaussian representations and refines the 3DGSoptimization process through a random region sampling strategy, significantlyimproving efficiency. Extensive experiments demonstrate that LabelGSoutperforms previous state-of-the-art methods, including Feature-3DGS, in the3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedupin training compared to Feature-3DGS, at a resolution of 1440X1080. Our codewill be at https://github.com/garrisonz/LabelGS.</description>
      <author>example@mail.com (Yupeng Zhang, Dezhi Zheng, Ping Lu, Han Zhang, Lei Wang, Liping xiang, Cheng Luo, Kaijun Deng, Xiaowen Fu, Linlin Shen, Jinbao Wang)</author>
      <guid isPermaLink="false">2508.19699v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Object Detection in the Car Interior With Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.19651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于汽车内部场景理解的物体检测与定位框架，通过分布式架构克服车载系统计算资源限制，微调后的轻量模型性能显著优于基础模型。&lt;h4&gt;背景&lt;/h4&gt;汽车内部AI任务如识别和定位外部引入物体对个人助手响应质量至关重要，但车载系统计算资源有限，限制了这些解决方案的直接部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种克服车载系统资源限制的物体检测与定位框架，用于汽车内部场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出ODAL框架，利用视觉基础模型，采用分布式架构将计算任务分配在车载系统和云端之间，并引入ODALbench作为评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的ODAL-LLaVA模型达到89%的ODAL分数，比基线性能提高71%，比GPT-4o高出近20%；同时保持高检测准确度，显著减少幻觉，ODAL信噪比比GPT-4o高三倍。&lt;h4&gt;结论&lt;/h4&gt;该框架有潜力在该领域建立新标准，轻量级微调模型在性能上优于大型基础模型。&lt;h4&gt;翻译&lt;/h4&gt;汽车内部AI任务如识别和定位外部引入物体对个人助手响应质量至关重要，但车载系统计算资源有限，限制了这些解决方案的直接部署。为解决这一限制，我们提出了新颖的物体检测与定位框架用于内部场景理解。我们的方法通过分布式架构利用视觉基础模型，将计算任务分配在车载和云端之间。这种设计克服了在车内直接运行基础模型的资源限制。为评估模型性能，我们引入了ODALbench作为检测和定位综合评估的新指标。我们的分析表明该框架有潜力在该领域建立新标准。我们将最先进的GPT-4o视觉基础模型与轻量级LLaVA 1.5 7B模型进行比较，并探索微调如何提升轻量模型的性能。值得注意的是，我们微调的ODAL-LLaVA模型达到89%的ODAL分数，比基线性能提高71%，比GPT-4o高出近20%。此外，微调后的模型在保持高检测准确度的同时显著减少了幻觉，ODAL信噪比比GPT-4o高三倍。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在汽车内部环境中识别和定位外部引入对象的问题，以及车载系统计算资源有限无法直接部署高性能视觉模型的挑战。这个问题很重要，因为车内物体识别能提升个人助手响应质量，例如提供不要忘记随身物品的提醒；同时传统模型只能识别训练过的物体类别，而实际应用中会遇到各种未知物体，需要更灵活的解决方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统CNN模型在开放集识别和提供人类可理解位置描述方面的局限，发现视觉基础模型虽有强大泛化能力但计算需求高。因此设计了分布式架构将任务分配在车载和云端之间。借鉴了现有视觉基础模型(特别是LLaVA 1.5)、分布式计算架构和LoRA微调技术，但针对车内场景进行了专门优化和整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建ODAL框架，通过分布式架构将计算任务在车载系统和云端之间分配，利用微调后的轻量级模型实现高性能物体检测。流程：1)车载系统捕获车内图像；2)在车载端使用视觉编码器处理图像生成嵌入；3)将嵌入(非原始图像)传输到云端；4)云端使用微调后的LLaVA模型进行物体检测和定位；5)生成结构化JSON响应；6)返回结果供车载应用使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ODAL分布式框架解决车载资源限制；2)ODALbench评估指标(ODALscore和ODALSNR)专门评估车内物体检测；3)微调策略使轻量模型达到高性能；4)微调后模型ODALscore达89%，比基线提高71%，超越GPT-4o近20%；5)ODALSNR达7.14，是GPT-4o的三倍，显著减少错误检测。相比传统方法，ODAL能处理开放词汇物体，而不仅限于预定义类别；相比直接使用重型模型，ODAL通过分布式架构和微调实现了资源高效利用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种名为ODAL的分布式车内物体检测和定位框架，通过微调轻量级视觉基础模型，在有限计算资源条件下实现了超越重型模型的性能，并为此领域引入了新的评估标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI tasks in the car interior like identifying and localizing externallyintroduced objects is crucial for response quality of personal assistants.However, computational resources of on-board systems remain highly constrained,restricting the deployment of such solutions directly within the vehicle. Toaddress this limitation, we propose the novel Object Detection and Localization(ODAL) framework for interior scene understanding. Our approach leveragesvision foundation models through a distributed architecture, splittingcomputational tasks between on-board and cloud. This design overcomes theresource constraints of running foundation models directly in the car. Tobenchmark model performance, we introduce ODALbench, a new metric forcomprehensive assessment of detection and localization.Our analysisdemonstrates the framework's potential to establish new standards in thisdomain. We compare the state-of-the-art GPT-4o vision foundation model with thelightweight LLaVA 1.5 7B model and explore how fine-tuning enhances thelightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA modelachieves an ODAL$_{score}$ of 89%, representing a 71% improvement over itsbaseline performance and outperforming GPT-4o by nearly 20%. Furthermore, thefine-tuned model maintains high detection accuracy while significantly reducinghallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.</description>
      <author>example@mail.com (Bálint Mészáros, Ahmet Firintepe, Sebastian Schmidt, Stephan Günnemann)</author>
      <guid isPermaLink="false">2508.19651v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Staircase Recognition and Location Based on Polarization Vision</title>
      <link>http://arxiv.org/abs/2505.19026v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文主要研究楼梯场景的感知和三维重建技术，提出了一种结合偏振和光强信息的对比度增强算法，以及基于YOLOv11的点云分割方法。此外，还提出了一种融合偏振双目和TOF深度信息的三维重建方法，以及基于ICP配准和改进灰狼优化算法的单目相机与TOF相机的联合标定算法。&lt;h4&gt;背景&lt;/h4&gt;楼梯是人工场景中最常见的结构之一，但对人形机器人和下肢残疾或视觉障碍的人来说，在没有传感器和智能算法的帮助下很难穿越。楼梯场景感知技术是识别和定位的前提，对机器人的模式切换和足迹位置计算以适应不连续地形具有重要意义。然而，该技术应用仍存在许多问题，如识别精度低、传感器初始噪声高、输出信号不稳定和计算要求高。&lt;h4&gt;目的&lt;/h4&gt;实现楼梯的检测，并实现高质量的三维重建。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合偏振和光强信息的对比度增强算法，集成了基于YOLOv11的点云分割，提出了融合偏振双目和TOF深度信息的三维重建方法，以及基于ICP配准和改进灰狼优化算法的单目相机与TOF相机的联合标定算法。&lt;h4&gt;主要发现&lt;/h4&gt;双目和飞行时间(TOF)重建容易受到环境光和目标物体表面材质的影响，而偏振重建方法依赖于物体表面的偏振信息，优点是不易受环境光影响且不依赖于物体表面的纹理信息。&lt;h4&gt;结论&lt;/h4&gt;通过结合偏振和光强信息，以及融合偏振双目和TOF深度信息，可以实现楼梯的高质量检测和三维重建。&lt;h4&gt;翻译&lt;/h4&gt;楼梯是人工场景中最常见的结构之一。然而，对于人形机器人和下肢残疾或视觉障碍的人来说，在没有传感器和智能算法的帮助下很难穿越场景。楼梯场景感知技术是识别和定位的前提。这项技术对机器人的模式切换和足迹位置计算以适应不连续地形具有重要意义。然而，仍然有许多问题限制了这项技术的应用，如识别精度低、传感器初始噪声高、输出信号不稳定和计算要求高。在场景重建方面，双目和飞行时间(TOF)重建容易受到环境光和目标物体表面材质的影响。相比之下，由于偏振器的特殊结构，偏振可以选择性地传输特定方向的偏振光，这种重建方法依赖于物体表面的偏振信息。因此，偏振重建的优点得以体现，即不易受环境光影响且不依赖于物体表面的纹理信息。在本文中，为了实现楼梯的检测，提出了一种结合偏振和光强信息的对比度增强算法，并集成了基于YOLOv11的点云分割。为了实现高质量重建，我们提出了一种融合偏振双目和TOF深度信息的方法，实现了楼梯的三维(3D)重建。此外，还提出了一种基于ICP配准和改进灰狼优化算法的单目相机与TOF相机的联合标定算法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决楼梯识别和定位的准确性问题，以及现有方法的光照敏感性和纹理依赖性问题。这个问题在现实中非常重要，因为楼梯感知对于人形机器人和行动不便的个体至关重要，关系到机器人在楼梯场景的导航控制性能，以及帮助行动不便者安全上下楼梯，提高生活质量。在康复医学领域，准确的楼梯识别对步态控制也至关重要，步态障碍可能导致运动停止、跌倒和死亡率增加。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有楼梯识别和3D重建方法的局限性来设计新方法。在楼梯识别方面，借鉴了YOLO算法和Hough变换技术；在3D重建方面，借鉴了偏振成像与结构光结合的方法、基于形状从阴影的技术和特征点方法。作者的主要创新在于提出了一个多传感器融合框架，结合偏振视觉、双目视觉和TOF深度信息，克服了单一方法的局限性。具体来说，作者改进了灰狼优化算法引入Levy飞行和动态权重，设计了偏振-强度图像融合方法，并提出了双目和TOF融合策略来校正偏振梯度场的模糊问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用偏振视觉获取更多维度的信息，结合多种传感器优势，实现鲁棒的楼梯检测和高精度的3D重建。整体实现流程分为三个模块：1) 楼梯识别模块：使用YOLOv11进行初步检测，结合偏振-强度对比度增强算法提高图像对比度，使用点云分割和法向量分析确认楼梯方向；2) 异构传感器校准模块：使用改进的灰狼优化算法进行偏振相机和TOF相机之间的校准，解决不同分辨率相机间的校准问题；3) 偏振3D重建模块：利用双目视觉校正偏振梯度场的方位角模糊，使用TOF深度信息填充双目重建中的数据空洞，通过积分算法获得表面的相对高度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 楼梯识别模块：结合偏振-强度对比度增强算法和点云分割，提高了识别准确率；2) 异构传感器校准模块：提出适用于不同分辨率相机的联合校准算法，改进灰狼优化算法引入Levy飞行和动态权重；3) 偏振3D重建模块：首次将双目偏振视觉与TOF传感融合用于3D重建，解决了偏振重建中的方位角模糊和双目重建中的数据空洞问题。相比之前工作，本文方法结合了多种传感器的优势，不需要成对的偏振-深度数据集，支持单帧成像适用于移动机器人动态场景，实现了更高的识别精度(98.7%)和更低的重建误差(&lt;0.2%)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于偏振视觉的多传感器融合方法，通过结合偏振-强度图像、双目视觉和TOF深度信息，实现了高精度的楼梯识别和低误差的3D重建，为机器人提供了准确的楼梯场景感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Staircase is one of the most common structures in artificial scenes. However,it is difficult for humanoid robots and people with lower limb disabilities orvisual impairment to cross the scene without the help of sensors andintelligent algorithms. Staircase scene perception technology is a prerequisitefor recognition and localization. This technology is of great significance forthe mode switching of the robot and the calculation of the footprint positionto adapt to the discontinuous terrain. However, there are still many problemsthat constrain the application of this technology, such as low recognitionaccuracy, high initial noise from sensors, unstable output signals and highcomputational requirements. In terms of scene reconstruction, the binocular andtime of flight (TOF) reconstruction of the scene can be easily affected byenvironmental light and the surface material of the target object. In contrast,due to the special structure of the polarizer, the polarization can selectivelytransmit polarized light in a specific direction and this reconstruction methodrelies on the polarization information of the object surface. So the advantagesof polarization reconstruction are reflected, which are less affected byenvironmental light and not dependent on the texture information of the objectsurface. In this paper, in order to achieve the detection of staircase, thispaper proposes a contrast enhancement algorithm that integrates polarizationand light intensity information, and integrates point cloud segmentation basedon YOLOv11. To realize the high-quality reconstruction, we proposed a method offusing polarized binocular and TOF depth information to realize thethree-dimensional (3D) reconstruction of the staircase. Besides, it alsoproposes a joint calibration algorithm of monocular camera and TOF camera basedon ICP registration and improved gray wolf optimization algorithm.</description>
      <author>example@mail.com (Weifeng Kong, Zhiying Tan)</author>
      <guid isPermaLink="false">2505.19026v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs</title>
      <link>http://arxiv.org/abs/2508.19907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages. Paper accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GegenNet，一种针对符号二分图链接符号预测的新型谱卷积神经网络模型。通过三种技术创新：基于谱分解的节点特征初始化、基于Gegenbauer多项式基的谱图滤波器以及符号感知的多层谱卷积网络，GegenNet在多个基准数据集上取得了显著优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;现有链接符号预测方法主要针对单符号图，忽略了节点异质性和符号二分图的独特特性。虽然最近的研究将图神经网络适应到符号二分图中，但基础谱卷积算子最初是为无符号图中的正链接设计的，不适合从已知链接推断缺失的正负链接。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对符号二分图的谱卷积神经网络模型，能够有效预测潜在链接的符号，解决现有方法在处理符号二分图时的局限性。&lt;h4&gt;方法&lt;/h4&gt;GegenNet包含三个主要技术贡献：(1) 快速且有理论基础的谱分解技术用于节点特征初始化；(2) 基于Gegenbauer多项式基的新谱图滤波器；(3) 多层符号感知谱卷积网络，交替使用Gegenbauer多项式滤波器和正负边。这些技术共同增强了模型容量，提高了预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在6个基准符号二分图数据集上的广泛实证研究表明，与11个强竞争对手相比，GegenNet在链接符号预测任务中实现了显著优越的性能，AUC最高提升4.28%，F1值最高提升11.69%。&lt;h4&gt;结论&lt;/h4&gt;GegenNet通过创新的谱卷积技术有效解决了符号二分图链接符号预测问题，为处理包含正负链接的复杂网络关系提供了新的有效方法，具有重要的理论和实践意义。&lt;h4&gt;翻译&lt;/h4&gt;给定一个具有两个不相交节点集U和V的符号二分图（SBG）G，链接符号预测的目标是基于G中已知的正边和负边来预测连接U和V的潜在链接的符号。现有的大多数链接符号预测解决方案主要针对单符号图，由于忽略了节点异质性和SBG的独特二分特性，这些方法次优。为此，最近的研究通过引入用于分区间（U×V）和分区内（U×U或V×V）节点对的消息传递方案，将图神经网络适应到SBG中。然而，基础的谱卷积算子最初是为无符号图中的正链接设计的，因此不适合从已知链接推断SBG中缺失的正链接或负链接。受此启发，本文提出了GegenNet，一种用于SBG链接符号预测的新型有效的谱卷积神经网络模型。特别是，GegenNet通过三个主要技术贡献实现了增强的模型容量和高度准确的预测：(i) 用于节点特征初始化的快速且有理论基础的谱分解技术；(ii) 基于Gegenbauer多项式基的新谱图滤波器；以及(iii) 多层符号感知谱卷积网络，交替使用Gegenbauer多项式滤波器和正负边。我们大量的实证研究表明，与6个基准SBG数据集上的11个强竞争对手相比，GegenNet在链接符号预测中可以实现显著优越的性能（AUC最高提升4.28%，F1最高提升11.69%）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,the goal of link sign prediction is to predict the signs of potential linksconnecting U and V based on known positive and negative edges in G. Themajority of existing solutions towards link sign prediction mainly focus onunipartite signed graphs, which are sub-optimal due to the neglect of nodeheterogeneity and unique bipartite characteristics of SBGs. To this end, recentstudies adapt graph neural networks to SBGs by introducing message-passingschemes for both inter-partition (UxV) and intra-partition (UxU or VxV) nodepairs. However, the fundamental spectral convolutional operators wereoriginally designed for positive links in unsigned graphs, and thus, are notoptimal for inferring missing positive or negative links from known ones inSBGs.  Motivated by this, this paper proposes GegenNet, a novel and effectivespectral convolutional neural network model for link sign prediction in SBGs.In particular, GegenNet achieves enhanced model capacity and high predictiveaccuracy through three main technical contributions: (i) fast and theoreticallygrounded spectral decomposition techniques for node feature initialization;(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and(iii) multi-layer sign-aware spectral convolutional networks alternatingGegenbauer polynomial filters with positive and negative edges. Our extensiveempirical studies reveal that GegenNet can achieve significantly superiorperformance (up to a gain of 4.28% in AUC and 11.69% in F1) in link signprediction compared to 11 strong competitors over 6 benchmark SBG datasets.</description>
      <author>example@mail.com (Hewen Wang, Renchi Yang, Xiaokui Xiao)</author>
      <guid isPermaLink="false">2508.19907v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections</title>
      <link>http://arxiv.org/abs/2508.19737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InfraredGP是一种创新的图分割方法，通过负校正机制放大传统范围外的低频信息，无需训练即可获得高质量的社区检测结果。&lt;h4&gt;背景&lt;/h4&gt;图分割(GP)，也称为社区检测，是一个将图节点划分为密集连接块的经典问题。从图信号处理角度，带有负校正的图拉普拉斯矩阵可推导出超出传统频率范围[0,2]的图频率。&lt;h4&gt;目的&lt;/h4&gt;探索超出传统范围[0,2]的低频信息是否能编码更多关于社区结构的信息性属性。&lt;h4&gt;方法&lt;/h4&gt;InfraredGP采用谱GNN作为骨干网络，结合低通滤波器和负校正机制，仅提供随机输入，通过前向传播导出图嵌入无需训练，最后将嵌入输入BIRCH算法获得图分割结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，仅基于放大[0,2]范围外低频信息的负校正机制，InfraredGP可为标准聚类模块推导出可区分的嵌入，无需训练即可获得高质量的图分割结果，并在效率上比基线方法快16-23倍。&lt;h4&gt;结论&lt;/h4&gt;InfraredGP通过负校正机制有效利用了传统范围外的低频信息，实现了无需训练的高效图分割，在静态和流式图分割任务中均表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图分割(GP)，也称为社区检测，是一个将图节点划分为密集连接块的经典问题。从图信号处理的角度，我们发现带有负校正的图拉普拉斯矩阵可以推导出超出传统范围[0,2]的图频率。为探索超出此范围的低频信息是否能编码更多关于社区结构的信息性属性，我们提出了InfraredGP。它(1)采用谱GNN作为骨干网络，结合低通滤波器和负校正机制，(2)仅向骨干网络提供随机输入，(3)通过前向传播(FFP)导出图嵌入而无需任何训练，以及(4)通过将导出的嵌入输入BIRCH算法获得可行的图分割结果。令人惊讶的是，实验表明，仅基于放大[0,2]范围外低频信息的负校正机制，InfraredGP就能为一些标准聚类模块(如BIRCH)推导出可区分的嵌入，无需任何训练即可获得高质量的图分割结果。按照IEEE HPEC图挑战基准，我们对InfraredGP进行了静态和流式图分割评估，InfraredGP在各种基线上实现了更好的效率(例如快16-23倍)和竞争性的质量。我们已将代码公开在https://github.com/KuroginQin/InfraredGP&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph partitioning (GP), a.k.a. community detection, is a classic problemthat divides nodes of a graph into densely-connected blocks. From a perspectiveof graph signal processing, we find that graph Laplacian with a negativecorrection can derive graph frequencies beyond the conventional range $[0, 2]$.To explore whether the low-frequency information beyond this range can encodemore informative properties about community structures, we propose InfraredGP.It (\romannumeral1) adopts a spectral GNN as its backbone combined withlow-pass filters and a negative correction mechanism, (\romannumeral2) onlyfeeds random inputs to this backbone, (\romannumeral3) derives graph embeddingsvia one feed-forward propagation (FFP) without any training, and(\romannumeral4) obtains feasible GP results by feeding the derived embeddingsto BIRCH. Surprisingly, our experiments demonstrate that based solely on thenegative correction mechanism that amplifies low-frequency information beyond$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standardclustering modules (e.g., BIRCH) and obtain high-quality results for GP withoutany training. Following the IEEE HPEC Graph Challenge benchmark, we evaluateInfraredGP for both static and streaming GP, where InfraredGP can achieve muchbetter efficiency (e.g., 16x-23x faster) and competitive quality over variousbaselines. We have made our code public athttps://github.com/KuroginQin/InfraredGP</description>
      <author>example@mail.com (Meng Qin, Weihua Li, Jinqiang Cui, Sen Pei)</author>
      <guid isPermaLink="false">2508.19737v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19647v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted at the ICIP Satellite Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种轻量级无监督骨架动作定位方法，利用时空图神经网络表示，通过预训练ASTGCN网络和定义动作动力学指标，实现了高效的动作边界检测，在保持计算效率的同时达到了与最先进监督方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;在未修剪的体育视频中实现细粒度动作定位具有挑战性，挑战源于短时间内快速而细微的动作转换。现有的监督和弱监督方法通常依赖大量标注数据和高容量模型，导致计算密集且难以适应实际场景。&lt;h4&gt;目的&lt;/h4&gt;引入一种轻量级的无监督骨架动作定位方法，利用时空图神经网络表示，减少对标注数据的依赖，提高计算效率并适应实际应用场景。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于骨架的无监督动作定位流程，预训练基于注意力的时空图卷积网络(ASTGCN)用于姿态序列去噪任务，使用分块分区方式学习内在运动动力学，定义新的动作动力学指标(ADM)从低维ASTGCN嵌入中直接计算，通过识别曲率剖面的拐点检测动作边界。&lt;h4&gt;主要发现&lt;/h4&gt;在DSV Diving数据集上达到82.66%的平均精度(mAP)，平均定位延迟为29.09毫秒，与最先进的监督方法性能相当同时保持计算效率，能够很好地泛化到未见过的野外跳水视频，无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了在嵌入式或动态环境中实现轻量级、实时动作分析系统的实际应用价值，证明无监督方法可以达到与监督方法相当的性能。&lt;h4&gt;翻译&lt;/h4&gt;在未修剪的体育视频中实现细粒度动作定位是一个重大挑战，这是由于短时间内快速而细微的动作转换。现有的监督和弱监督解决方案通常依赖大量标注数据和高容量模型，使它们计算密集且难以适应实际场景。在这项工作中，我们引入了一种轻量级和无监督的基于骨架的动作定位流程，利用时空图神经网络表示。我们的方法在块状分区的姿态序列去噪任务上预训练了一个基于注意力的时空图卷积网络(ASTGCN)，使其能够在没有任何手动标注的情况下学习内在运动动力学。在推理时，我们定义了一个新的动作动力学指标(ADM)，直接从低维ASTGCN嵌入计算，通过识别其曲率剖面的拐点来检测动作边界。我们的方法在DSV Diving数据集上达到了82.66%的平均精度(mAP)和29.09毫秒的平均定位延迟，与最先进的监督性能相匹配，同时保持计算效率。此外，它可以强大地泛化到未见过的野外跳水视频而无需重新训练，证明了其在嵌入式或动态环境中轻量级、实时动作分析系统的实际应用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在未修剪的体育视频中实现细粒度动作定位的问题，特别是跳水视频中精确检测动作边界。这个问题在现实中很重要，因为它能帮助教练和运动员分析技术动作、优化训练策略和提高比赛表现。现有方法大多依赖大量标注数据和高容量模型，计算密集且难以适应现实场景，而跳水等运动中动作转换迅速且微妙，持续时间短，增加了定位难度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到体育动作定位中标注数据稀缺的问题，转向无监督学习以提高可扩展性和适应性。他们选择基于骨架的方法而非原始视频，以减少计算复杂度。借鉴了ASTGCN作为基础架构，并受先前几何曲率编码方法的启发。创新点在于设计了块状分区的预训练任务和动作动力学指标(ADM)，通过理论证明拐点对应于动作转换状态，从而实现无监督的动作边界检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用图神经网络从人体骨架序列中学习时空表示，通过预训练去噪任务学习动作内在动态特性，无需人工标注，并设计动作动力学指标(ADM)通过曲率变化检测动作边界。训练阶段：将姿态序列划分为重叠子序列，添加噪声，用ASTGCN处理生成嵌入，通过全连接层重建去噪姿态。推理阶段：将姿态序列划分为子序列，提取时空嵌入，计算欧几里得范数得到ADM，分析曲率变化检测拐点作为动作转换点，映射回原始时间轴确定动作边界。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全无监督的基于骨架的动作定位框架；2)块状预训练策略而非整个视频序列；3)动作动力学指标(ADM)直接从低维嵌入计算；4)理论证明拐点对应动作转换状态；5)高效实时性能(29.09ms延迟)。相比之前工作，此方法不需要标注数据，计算效率更高，在DSV数据集上达到82.66%的mAP与监督方法相当；不依赖聚类和自 paced 学习，能更好地处理体育动作中的细粒度转换，提供了更直观的可视化和解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于图神经网络的无监督时空动作定位方法，通过设计动作动力学指标和理论证明，实现了在无需标注的情况下精确检测跳水视频中的动作转换边界，同时保持高效计算性能和强泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-grained action localization in untrimmed sports videos presents asignificant challenge due to rapid and subtle motion transitions over shortdurations. Existing supervised and weakly supervised solutions often rely onextensive annotated datasets and high-capacity models, making themcomputationally intensive and less adaptable to real-world scenarios. In thiswork, we introduce a lightweight and unsupervised skeleton-based actionlocalization pipeline that leverages spatio-temporal graph neuralrepresentations. Our approach pre-trains an Attention-based Spatio-TemporalGraph Convolutional Network (ASTGCN) on a pose-sequence denoising task withblockwise partitions, enabling it to learn intrinsic motion dynamics withoutany manual labeling. At inference, we define a novel Action Dynamics Metric(ADM), computed directly from low-dimensional ASTGCN embeddings, which detectsmotion boundaries by identifying inflection points in its curvature profile.Our method achieves a mean Average Precision (mAP) of 82.66% and averagelocalization latency of 29.09 ms on the DSV Diving dataset, matchingstate-of-the-art supervised performance while maintaining computationalefficiency. Furthermore, it generalizes robustly to unseen, in-the-wild divingfootage without retraining, demonstrating its practical applicability forlightweight, real-time action analysis systems in embedded or dynamicenvironments.</description>
      <author>example@mail.com (Bikash Kumar Badatya, Vipul Baghel, Ravi Hegde)</author>
      <guid isPermaLink="false">2508.19647v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes</title>
      <link>http://arxiv.org/abs/2508.19356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 to 4 hours read time. 73 pages. 35 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了图数据建模在化学科学中的应用，特别是在描述分子、蛋白质和化学过程方面，展示了图神经网络等学习算法如何在这些图结构上操作，为读者应用图方法到化学发现提供了基础。&lt;h4&gt;背景&lt;/h4&gt;图是化学科学的核心工具，提供了描述分子、蛋白质、反应和工业过程的自然语言，能够捕获支撑材料、生物学和医学的相互作用和结构。&lt;h4&gt;目的&lt;/h4&gt;这篇入门介绍旨在将图作为化学中的数学对象进行介绍，展示学习算法如何在图上操作，并为读者应用图方法到下一代化学发现做好准备。&lt;h4&gt;方法&lt;/h4&gt;论文介绍了图神经网络等学习算法如何在图结构上操作，以及图设计的基础、关键预测任务和化学科学中的代表性例子。&lt;h4&gt;主要发现&lt;/h4&gt;图设计的基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图的建模中的作用是论文的主要发现。&lt;h4&gt;结论&lt;/h4&gt;这些共同的概念为读者应用图方法到下一代化学发现做好了准备。&lt;h4&gt;翻译&lt;/h4&gt;图是化学科学的核心，提供了描述分子、蛋白质、反应和工业过程的自然语言。它们捕获了支撑材料、生物学和医学的相互作用和结构。这篇入门介绍《图数据建模：分子、蛋白质与化学过程》将图作为化学中的数学对象进行介绍，并展示了学习算法（特别是图神经网络）如何在图上操作。我们概述了图设计的基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图的建模中的作用。这些概念共同为读者应用图方法到下一代化学发现做好了准备。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1021/acsinfocus.7e9017&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are central to the chemical sciences, providing a natural language todescribe molecules, proteins, reactions, and industrial processes. They captureinteractions and structures that underpin materials, biology, and medicine.This primer, Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes,introduces graphs as mathematical objects in chemistry and shows how learningalgorithms (particularly graph neural networks) can operate on them. We outlinethe foundations of graph design, key prediction tasks, representative examplesacross chemical sciences, and the role of machine learning in graph-basedmodeling. Together, these concepts prepare readers to apply graph methods tothe next generation of chemical discovery.</description>
      <author>example@mail.com (José Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Benjamin Sanchez-Lengeling, Adrian Jinich, Radhakrishnan Mahadevan)</author>
      <guid isPermaLink="false">2508.19356v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Memorization in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出NCMemo框架研究图神经网络中的记忆现象，发现图同质性与记忆呈反向关系，分析GNN训练动态，并利用图重连减少记忆同时保护隐私。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络已被证明能记忆训练数据，但对图神经网络的相关分析仍不充分，图同质性（连接节点有相似标签/特征的性质）在GNN学习中作用重要。&lt;h4&gt;目的&lt;/h4&gt;引入首个量化半监督节点分类中标签记忆的框架；研究图同质性与记忆关系；分析GNN训练动态；探索特征空间邻域标签不一致性与记忆关系；研究图重连作为减少记忆方法。&lt;h4&gt;方法&lt;/h4&gt;提出NCMemo框架；建立图同质性与记忆关系模型；分析GNN训练动态；研究节点特征空间邻域特性；应用图重连技术减少记忆。&lt;h4&gt;主要发现&lt;/h4&gt;低图同质性显著增加记忆，GNN依赖记忆学习低同质性图；低同质性图中记忆增加与GNN使用图结构的隐式偏置相关；特征空间邻域标签不一致性高的节点更易被记忆；图重连能有效减少记忆而不损害性能；图重连降低已记忆数据点的隐私风险。&lt;h4&gt;结论&lt;/h4&gt;该工作不仅推进了对GNN学习的理解，还支持了更具隐私保护的GNN部署，为开发更有效的GNN训练方法和隐私保护策略提供了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络(DNNs)已被证明能够记忆其训练数据，但对图神经网络(GNNs)的类似分析仍然 largely under-explored。我们引入了NCMemo（节点分类记忆），这是首个用于量化半监督节点分类中标签记忆的框架。我们首先建立了记忆与图同质性之间的反向关系，即连接节点共享相似标签/特征的性质。我们发现较低的同质性显著增加了记忆，表明GNN依赖记忆来学习同质性较低的图。其次，我们分析了GNN训练动态。我们发现低同质性图中增加的记忆与GNN在学习过程中使用图结构的隐式偏置紧密相关。在低同质性情况下，这种结构信息较少，因此诱导节点标签的记忆以最小化训练损失。最后，我们显示特征空间邻域中标签不一致性更高的节点显著更容易被记忆。基于我们对图同质性与记忆之间联系的理解，我们研究了图重连作为减少记忆的方法。我们的结果表明，这种方法能有效减少记忆而不损害模型性能。此外，我们表明它在实践中降低了先前记忆数据点的隐私风险。因此，我们的工作不仅推进了对GNN学习的理解，还支持了更具隐私保护的GNN部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have been shown to memorize their training data,yet similar analyses for graph neural networks (GNNs) remain largelyunder-explored. We introduce NCMemo (Node Classification Memorization), thefirst framework to quantify label memorization in semi-supervised nodeclassification. We first establish an inverse relationship between memorizationand graph homophily, i.e., the property that connected nodes share similarlabels/features. We find that lower homophily significantly increasesmemorization, indicating that GNNs rely on memorization to learn lesshomophilic graphs. Secondly, we analyze GNN training dynamics. We find that theincreased memorization in low homophily graphs is tightly coupled to the GNNs'implicit bias on using graph structure during learning. In low homophilyregimes, this structure is less informative, hence inducing memorization of thenode labels to minimize training loss. Finally, we show that nodes with higherlabel inconsistency in their feature-space neighborhood are significantly moreprone to memorization. Building on our insights into the link between graphhomophily and memorization, we investigate graph rewiring as a means tomitigate memorization. Our results demonstrate that this approach effectivelyreduces memorization without compromising model performance. Moreover, we showthat it lowers the privacy risk for previously memorized data points inpractice. Thus, our work not only advances understanding of GNN learning butalso supports more privacy-preserving GNN deployment.</description>
      <author>example@mail.com (Adarsh Jamadandi, Jing Xu, Adam Dziedzic, Franziska Boenisch)</author>
      <guid isPermaLink="false">2508.19352v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</title>
      <link>http://arxiv.org/abs/2508.20089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种轻量级分类方法，结合有限的专家标记野外数据与BioCLIP2基础模型的知识蒸馏，用于自动化相机系统中的蛾类图像分类。该方法在保持高准确性的同时显著降低了计算成本，为昆虫监测系统提供了实用指导。&lt;h4&gt;背景&lt;/h4&gt;标记来自自动化相机系统的鳞翅目（蛾类）图像对于理解昆虫数量下降至关重要。然而，由于策展图像和嘈杂的野外图像之间的域偏移，准确的物种识别具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级分类方法，能够准确识别蛾类物种，同时降低计算成本，解决域偏移问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合有限专家标记野外数据与BioCLIP2基础模型知识蒸馏的分类方法，使用ConvNeXt-tiny架构。在丹麦101种蛾类物种的AMI相机系统上进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;BioCLIP2基础模型明显优于其他分类方法；所提出的蒸馏轻量级模型以显著降低的计算成本实现了与基础模型相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;这些见解为开发高效的昆虫监测系统和弥合细粒度分类的域差距提供了实用指南，有助于理解昆虫数量下降的趋势。&lt;h4&gt;翻译&lt;/h4&gt;标记来自自动化相机系统的鳞翅目（蛾类）图像对于理解昆虫数量下降至关重要。然而，由于策展图像和嘈杂的野外图像之间的域偏移，准确的物种识别具有挑战性。我们提出了一种轻量级分类方法，将有限的专家标记野外数据与高性能BioCLIP2基础模型的知识蒸馏相结合，使用ConvNeXt-tiny架构。在丹麦101种蛾类物种的AMI相机系统上的实验表明，BioCLIP2明显优于其他方法，并且我们蒸馏的轻量级模型以显著降低的计算成本实现了相当的准确性。这些见解为开发高效的昆虫监测系统和弥合细粒度分类的域差距提供了实用指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Labelling images of Lepidoptera (moths) from automated camera systems isvital for understanding insect declines. However, accurate speciesidentification is challenging due to domain shifts between curated images andnoisy field imagery. We propose a lightweight classification approach,combining limited expert-labelled field data with knowledge distillation fromthe high-performance BioCLIP2 foundation model into a ConvNeXt-tinyarchitecture. Experiments on 101 Danish moth species from AMI camera systemsdemonstrate that BioCLIP2 substantially outperforms other methods and that ourdistilled lightweight model achieves comparable accuracy with significantlyreduced computational cost. These insights offer practical guidelines for thedevelopment of efficient insect monitoring systems and bridging domain gaps forfine-grained classification.</description>
      <author>example@mail.com (Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas Høye)</author>
      <guid isPermaLink="false">2508.20089v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2508.20085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了HERMES，一种用于移动双臂灵巧操作的人机学习框架，能够将多源人类手部动作转化为机器人可执行的复杂操作行为，并在多样化环境中实现自主操作。&lt;h4&gt;背景&lt;/h4&gt;利用人类运动数据赋予机器人多样化操作能力已成为有前景的研究方向，但将多源人类手部动作转化为可行的机器人行为仍具挑战性，特别是对于具有复杂高维动作空间的多指灵巧手机器人，且现有方法难以适应不同环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发一种人机学习框架，解决多源人类手部动作到机器人行为的转化问题，并使机器人在多样化非结构化环境中能够自主执行复杂的双臂灵巧操作任务。&lt;h4&gt;方法&lt;/h4&gt;1) 提出HERMES框架；2) 制定统一强化学习方法，无缝转化多源异构人类手部动作为物理可行的机器人行为；3) 设计基于深度图像的端到端sim2real迁移方法；4) 通过闭环PnP定位机制增强导航基础模型，实现视觉目标精确对齐，连接自主导航与灵巧操作。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，HERMES在多样化真实场景中表现出可泛化的行为，能成功执行多种复杂的移动双臂灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;HERMES框架有效解决了多源人类手部动作到机器人行为的转化挑战，使机器人能够在多样化非结构化环境中自主执行复杂操作，为机器人操作领域提供了有价值的技术方案。&lt;h4&gt;翻译&lt;/h4&gt;HERMES: 一种用于移动双臂灵巧操作的人机学习框架&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging human motion data to impart robots with versatile manipulationskills has emerged as a promising paradigm in robotic manipulation.Nevertheless, translating multi-source human hand motions into feasible robotbehaviors remains challenging, particularly for robots equipped withmulti-fingered dexterous hands characterized by complex, high-dimensionalaction spaces. Moreover, existing approaches often struggle to produce policiescapable of adapting to diverse environmental conditions. In this paper, weintroduce HERMES, a human-to-robot learning framework for mobile bimanualdexterous manipulation. First, HERMES formulates a unified reinforcementlearning approach capable of seamlessly transforming heterogeneous human handmotions from multiple sources into physically plausible robotic behaviors.Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depthimage-based sim2real transfer method for improved generalization to real-worldscenarios. Furthermore, to enable autonomous operation in varied andunstructured environments, we augment the navigation foundation model with aclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precisealignment of visual goals and effectively bridging autonomous navigation anddexterous manipulation. Extensive experimental results demonstrate that HERMESconsistently exhibits generalizable behaviors across diverse, in-the-wildscenarios, successfully performing numerous complex mobile bimanual dexterousmanipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.</description>
      <author>example@mail.com (Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu)</author>
      <guid isPermaLink="false">2508.20085v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Model Science: getting serious about verification, explanation and control of AI systems</title>
      <link>http://arxiv.org/abs/2508.20040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了模型科学(Model Science)这一新学科的概念框架，强调从数据科学向模型科学的范式转变，将训练好的模型置于分析核心，并提出四个关键支柱。&lt;h4&gt;背景&lt;/h4&gt;基础模型(Foundation models)的广泛采用需要从数据科学转向模型科学的范式转变。&lt;h4&gt;目的&lt;/h4&gt;介绍模型科学这一新学科的概念框架，并提出其四个关键支柱，以指导可信、安全和人类对齐的AI系统的发展。&lt;h4&gt;方法&lt;/h4&gt;提出模型科学的四个关键支柱：验证(Verification)要求严格且具有上下文感知的评估协议；解释(Explanation)是探索模型内部操作的各种方法；控制(Control)整合对齐技术来引导模型行为；接口(Interface)开发交互式和可视化解释工具以提高人类校准和决策能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型科学将训练好的模型置于分析的核心，旨在跨不同操作环境中交互、验证、解释和控制模型行为。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型科学框架旨在指导可信、安全和人类对齐的AI系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;基础模型日益广泛的采用需要从数据科学向模型科学转变范式。与以数据为中心的方法不同，模型科学将训练好的模型置于分析核心，旨在跨不同操作环境中交互、验证、解释和控制其行为。本文介绍了名为模型科学的新学科的概念框架，并提出了其四个关键支柱：验证，要求严格且具有上下文感知的评估协议；解释，被理解为探索模型内部操作的各种方法；控制，整合对齐技术来引导模型行为；接口，开发交互式和可视化解释工具以提高人类校准和决策能力。所提出的框架旨在指导可信、安全和人类对齐的AI系统的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing adoption of foundation models calls for a paradigm shift fromData Science to Model Science. Unlike data-centric approaches, Model Scienceplaces the trained model at the core of analysis, aiming to interact, verify,explain, and control its behavior across diverse operational contexts. Thispaper introduces a conceptual framework for a new discipline called ModelScience, along with the proposal for its four key pillars: Verification, whichrequires strict, context-aware evaluation protocols; Explanation, which isunderstood as various approaches to explore of internal model operations;Control, which integrates alignment techniques to steer model behavior; andInterface, which develops interactive and visual explanation tools to improvehuman calibration and decision-making. The proposed framework aims to guide thedevelopment of credible, safe, and human-aligned AI systems.</description>
      <author>example@mail.com (Przemyslaw Biecek, Wojciech Samek)</author>
      <guid isPermaLink="false">2508.20040v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology</title>
      <link>http://arxiv.org/abs/2508.19914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  43 pages, 7 main Figures, 8 Extended Data Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EAGLE-Net，一种结构保持的、注意力引导的多实例学习架构，用于改进计算病理学中的特征提取和预测。&lt;h4&gt;背景&lt;/h4&gt;基础模型已成为计算病理学中强大的特征提取器，但它们通常忽略了组织的全局空间结构和局部区域之间的上下文关系，这些是理解肿瘤微环境的关键元素。多实例学习是基础模型之后的必要步骤。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架，将补丁级别的特征聚合为幻灯片级别的预测，同时增强预测能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;EAGLE-Net集成多尺度绝对空间编码捕获全局组织架构，使用top-K邻域感知损失关注局部微环境，以及背景抑制损失减少假阳性。研究在大型泛癌数据集上进行了测试，包括三种癌症类型的分类(10,260张幻灯片)和七种癌症类型的生存预测(4,172张幻灯片)，使用了三种不同的组织学基础骨干网络。&lt;h4&gt;主要发现&lt;/h4&gt;EAGLE-Net在所有任务中实现了高达3%的分类准确率提升，在7种癌症类型中的6种中获得了最高的一致性指数。生成的注意力图平滑且生物连贯，与专家注释一致，能够突出显示侵袭前沿、坏死和免疫浸润。&lt;h4&gt;结论&lt;/h4&gt;EAGLE-Net是一个可推广的、可解释的框架，补充了基础模型，能够改进生物标志物发现、预后建模和临床决策支持。&lt;h4&gt;翻译&lt;/h4&gt;基础模型最近已成为计算病理学中强大的特征提取器，但它们通常忽略了利用组织的全局空间结构和诊断相关区域之间的局部上下文关系——理解肿瘤微环境的关键元素。多实例学习(MIL)仍然是基础模型之后的必要步骤，用于设计将补丁级别特征聚合为幻灯片级别预测的框架。我们提出了EAGLE-Net，一种结构保持的、注意力引导的MIL架构，旨在增强预测和可解释性。EAGLE-Net集成多尺度绝对空间编码来捕获全局组织架构，top-K邻域感知损失来关注局部微环境，以及背景抑制损失来减少假阳性。我们在大型泛癌数据集上对EAGLE-Net进行了基准测试，包括三种癌症类型的分类(10,260张幻灯片)和七种癌症类型的生存预测(4,172张幻灯片)，使用了三种不同的组织学基础骨干网络(REMEDIES、Uni-V1、Uni2-h)。在所有任务中，EAGLE-Net实现了高达3%的分类准确率提升，并在7种癌症类型中的6种中获得了最高的一致性指数，产生了平滑、生物连贯的注意力图，与专家注释一致，并突出了侵袭前沿、坏死和免疫浸润。这些结果将EAGLE-Net定位为一个可推广的、可解释的框架，补充了基础模型，能够改进生物标志物发现、预后建模和临床决策支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have recently emerged as powerful feature extractors incomputational pathology, yet they typically omit mechanisms for leveraging theglobal spatial structure of tissues and the local contextual relationshipsamong diagnostically relevant regions - key elements for understanding thetumor microenvironment. Multiple instance learning (MIL) remains an essentialnext step following foundation model, designing a framework to aggregatepatch-level features into slide-level predictions. We present EAGLE-Net, astructure-preserving, attention-guided MIL architecture designed to augmentprediction and interpretability. EAGLE-Net integrates multi-scale absolutespatial encoding to capture global tissue architecture, a top-Kneighborhood-aware loss to focus attention on local microenvironments, andbackground suppression loss to minimize false positives. We benchmarkedEAGLE-Net on large pan-cancer datasets, including three cancer types forclassification (10,260 slides) and seven cancer types for survival prediction(4,172 slides), using three distinct histology foundation backbones (REMEDIES,Uni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higherclassification accuracy and the top concordance indices in 6 of 7 cancer types,producing smooth, biologically coherent attention maps that aligned with expertannotations and highlighted invasive fronts, necrosis, and immune infiltration.These results position EAGLE-Net as a generalizable, interpretable frameworkthat complements foundation models, enabling improved biomarker discovery,prognostic modeling, and clinical decision support</description>
      <author>example@mail.com (Muhammad Waqas, Rukhmini Bandyopadhyay, Eman Showkatian, Amgad Muneer, Anas Zafar, Frank Rojas Alvarez, Maricel Corredor Marin, Wentao Li, David Jaffray, Cara Haymaker, John Heymach, Natalie I Vokes, Luisa Maren Solis Soto, Jianjun Zhang, Jia Wu)</author>
      <guid isPermaLink="false">2508.19914v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>SoK: Large Language Model Copyright Auditing via Fingerprinting</title>
      <link>http://arxiv.org/abs/2508.19843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对大型语言模型(LLM)指纹识别技术进行了首次全面研究，提出统一框架和分类法，并创建了首个系统基准LeaFBench用于评估该技术在真实场景下的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLM)因其广泛能力和训练所需的大量资源而成为有价值的知识产权，但它们容易受到版权侵犯，如未经授权使用和模型盗窃。&lt;h4&gt;目的&lt;/h4&gt;解决LLM指纹识别技术的可靠性问题，由于各种模型修改的普遍存在和缺乏标准化评估，其可靠性目前不确定。&lt;h4&gt;方法&lt;/h4&gt;引入统一框架和正式分类法，将现有方法分为白盒和黑盒方法；创建LeaFBench基准，基于主流基础模型包含149个不同实例，集成13种代表性开发后技术，涵盖参数改变方法和参数独立机制。&lt;h4&gt;主要发现&lt;/h4&gt;在LeaFBench上的广泛实验揭示了现有LLM指纹识别方法的优缺点，勾勒出该新兴领域的未来研究方向和关键开放问题。&lt;h4&gt;结论&lt;/h4&gt;LLM指纹识别是一种有前途的版权审计解决方案，但需要进一步研究和标准化以提高其可靠性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLM)的广泛能力和训练所需的实质性资源使其成为有价值的知识产权，但它们仍然容易受到版权侵犯，如未经授权使用和模型盗窃。LLM指纹识别是一种非侵入性技术，它从LLM中提取和比较独特特征以识别侵权行为，为版权审计提供了有希望的解决方案。然而，由于其可靠性因各种模型修改的普遍存在和缺乏标准化评估而不确定。在这篇SoK中，我们首次对LLM指纹识别进行了全面研究。我们引入了一个统一的框架和正式的分类法，将现有方法分为白盒和黑盒方法，提供了对最新技术的结构化概述。我们进一步提出了LeaFBench，这是第一个用于在真实部署场景下评估LLM指纹识别的系统基准。基于主流基础模型并包含149个不同的模型实例，LeaFBench集成了13种代表性的开发后技术，涵盖参数改变方法(如微调、量化)和参数独立机制(如系统提示、RAG)。在LeaFBench上的广泛实验揭示了现有方法的优缺点，从而勾勒出这一新兴领域的未来研究方向和关键开放问题。代码可在https://github.com/shaoshuo-ss/LeaFBench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The broad capabilities and substantial resources required to train LargeLanguage Models (LLMs) make them valuable intellectual property, yet theyremain vulnerable to copyright infringement, such as unauthorized use and modeltheft. LLM fingerprinting, a non-intrusive technique that extracts and comparesthe distinctive features from LLMs to identify infringements, offers apromising solution to copyright auditing. However, its reliability remainsuncertain due to the prevalence of diverse model modifications and the lack ofstandardized evaluation. In this SoK, we present the first comprehensive studyof LLM fingerprinting. We introduce a unified framework and formal taxonomythat categorizes existing methods into white-box and black-box approaches,providing a structured overview of the state of the art. We further proposeLeaFBench, the first systematic benchmark for evaluating LLM fingerprintingunder realistic deployment scenarios. Built upon mainstream foundation modelsand comprising 149 distinct model instances, LeaFBench integrates 13representative post-development techniques, spanning both parameter-alteringmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal thestrengths and weaknesses of existing methods, thereby outlining future researchdirections and critical open problems in this emerging field. The code isavailable at https://github.com/shaoshuo-ss/LeaFBench.</description>
      <author>example@mail.com (Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin)</author>
      <guid isPermaLink="false">2508.19843v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese</title>
      <link>http://arxiv.org/abs/2508.19721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ASRU 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究填补了欧洲葡萄牙语语音识别资源的空白，提供了首个面向欧洲葡萄牙语和其他葡萄牙语变体的开放框架CAMÕES，包含评估基准和先进模型。&lt;h4&gt;背景&lt;/h4&gt;现有的葡萄牙语自动语音识别资源主要集中在巴西葡萄牙语，欧洲葡萄牙语和其他变体研究不足。&lt;h4&gt;目的&lt;/h4&gt;引入CAMÕES，首个面向欧洲葡萄牙语和其他葡萄牙语变体的开放框架，以填补研究空白。&lt;h4&gt;方法&lt;/h4&gt;框架包含两部分：1)包含46小时跨越多个领域的欧洲葡萄牙语测试数据的评估基准；2)最先进模型集合。研究考虑多种基础模型，评估其零样本和微调性能，以及从头开始训练的E-Branchformer模型。使用425小时精选的欧洲葡萄牙语数据进行微调和训练。&lt;h4&gt;主要发现&lt;/h4&gt;微调的基础模型和E-Branchformer在欧洲葡萄牙语上的性能相当。与最强的零样本基础模型相比，最佳性能模型的相对错误率提高了35%以上。&lt;h4&gt;结论&lt;/h4&gt;为欧洲葡萄牙语和其他变体建立了新的最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;现有的葡萄牙语自动语音识别资源主要集中在巴西葡萄牙语，导致欧洲葡萄牙语(EP)和其他变体研究不足。为填补这一空白，我们引入了CAMÕES，这是首个面向欧洲葡萄牙语和其他葡萄牙语变体的开放框架。它包含两部分：(1)全面的评估基准，包括46小时跨越多个领域的欧洲葡萄牙语测试数据；(2)最先进模型的集合。对于后者，我们考虑了多种基础模型，评估了它们的零样本和微调性能，以及从头开始训练的E-Branchformer模型。使用425小时精选的欧洲葡萄牙语数据进行微调和训练。我们的结果显示，微调的基础模型和E-Branchformer在欧洲葡萄牙语上的性能相当。此外，与最强的零样本基础模型相比，最佳性能模型的相对错误率提高了35%以上，为欧洲葡萄牙语和其他变体建立了新的最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing resources for Automatic Speech Recognition in Portuguese are mostlyfocused on Brazilian Portuguese, leaving European Portuguese (EP) and othervarieties under-explored. To bridge this gap, we introduce CAM\~OES, the firstopen framework for EP and other Portuguese varieties. It consists of (1) acomprehensive evaluation benchmark, including 46h of EP test data spanningmultiple domains; and (2) a collection of state-of-the-art models. For thelatter, we consider multiple foundation models, evaluating their zero-shot andfine-tuned performances, as well as E-Branchformer models trained from scratch.A curated set of 425h of EP was used for both fine-tuning and training. Ourresults show comparable performance for EP between fine-tuned foundation modelsand the E-Branchformer. Furthermore, the best-performing models achieverelative improvements above 35% WER, compared to the strongest zero-shotfoundation model, establishing a new state-of-the-art for EP and othervarieties.</description>
      <author>example@mail.com (Carlos Carvalho, Francisco Teixeira, Catarina Botelho, Anna Pompili, Rubén Solera-Ureña, Sérgio Paulo, Mariana Julião, Thomas Rolland, John Mendonça, Diogo Pereira, Isabel Trancoso, Alberto Abad)</author>
      <guid isPermaLink="false">2508.19721v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>SCAR: A Characterization Scheme for Multi-Modal Dataset</title>
      <link>http://arxiv.org/abs/2508.19659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SCAR是一种新的数据集表征方案，关注数据质量的四个结构性方面：规模、覆盖范围、真实性和丰富度。它能够捕获在数据集扩展下保持不变的稳定特征，并基于这些特性创建Foundation Data以保留完整数据集的泛化行为。通过建模单模态任务为阶跃函数，可以估计基础数据大小分布并捕获跨模态的泛化偏差，进而开发SCAR引导的数据完成策略有效扩展多模态数据。&lt;h4&gt;背景&lt;/h4&gt;基础模型在各种任务上表现出 remarkable 通用性，主要由训练数据的特性驱动。现有的数据中心方法如剪枝和压缩优化了训练，但对数据特性如何影响泛化的理论见解有限。传统观点主要关注数据数量和训练效率，忽视了数据质量的结构性方面，特别是在样本扩展中数据特征的影响。&lt;h4&gt;目的&lt;/h4&gt;引入一种能够描述数据集内在结构特性的方案，理解数据特性如何影响泛化，特别是在样本扩展中，并提供一种基于数据结构特性的数据理解和利用方法。&lt;h4&gt;方法&lt;/h4&gt;提出了SCAR（规模、覆盖范围、真实性和丰富度）方案来捕获数据集的内在结构特性；引入Foundation Data概念，即保留完整数据集泛化行为的最小子集；将单模态任务建模为阶跃函数并估计基础数据大小分布；开发基于泛化偏差的SCAR引导数据完成策略，实现模态感知的数据扩展。&lt;h4&gt;主要发现&lt;/h4&gt;SCAR能够捕获数据集的内在结构特性并保持不变；Foundation Data可以在不进行特定模型重新训练的情况下保留完整数据集的泛化行为；通过建模可以估计基础数据大小分布并捕获跨模态的阶跃式泛化偏差；SCAR引导的数据完成策略能够有效地、模态感知地扩展多模态数据集中的模态特定特征。&lt;h4&gt;结论&lt;/h4&gt;SCAR在预测数据效用和指导数据获取方面是有效的，实验验证了SCAR在不同多模态数据集和模型架构上的有效性，为数据理解和利用提供了新的理论基础。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在多样化任务中表现出卓越的泛化能力，这主要由其训练数据的特性驱动。最近的数据中心方法如剪枝和压缩旨在优化训练，但对数据特性如何影响泛化提供了有限的理论见解，特别是样本扩展中的数据特征。传统观点通过主要关注数据数量和训练效率限制了进展，常常忽视数据质量的结构性方面。在本研究中，我们引入了SCAR，一种用于表征数据集内在结构特性的方案，涵盖四个关键指标：规模、覆盖范围、真实性和丰富度。与之前的数据中心指标不同，SCAR捕获了在数据集扩展下保持不变的稳定特征，为数据理解提供了稳健而通用的基础。利用这些结构特性，我们引入了Foundation Data——一个无需特定模型重新训练即可保留完整数据集泛化行为的最小子集。我们将单模态任务建模为阶跃函数，并估计基础数据大小的分布，以捕获目标多模态数据集中跨模态的阶跃式泛化偏差。最后，我们基于这种泛化偏差开发了SCAR引导的数据完成策略，能够高效地、模态感知地扩展多模态数据集中的模态特定特征。在多样化的多模态数据集和模型架构上的实验验证了SCAR在预测数据效用和指导数据获取方面的有效性。代码可在https://github.com/McAloma/SCAR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models exhibit remarkable generalization across diverse tasks,largely driven by the characteristics of their training data. Recentdata-centric methods like pruning and compression aim to optimize training butoffer limited theoretical insight into how data properties affectgeneralization, especially the data characteristics in sample scaling.Traditional perspectives further constrain progress by focusing predominantlyon data quantity and training efficiency, often overlooking structural aspectsof data quality. In this study, we introduce SCAR, a principled scheme forcharacterizing the intrinsic structural properties of datasets across four keymeasures: Scale, Coverage, Authenticity, and Richness. Unlike priordata-centric measures, SCAR captures stable characteristics that remaininvariant under dataset scaling, providing a robust and general foundation fordata understanding. Leveraging these structural properties, we introduceFoundation Data-a minimal subset that preserves the generalization behavior ofthe full dataset without requiring model-specific retraining. We modelsingle-modality tasks as step functions and estimate the distribution of thefoundation data size to capture step-wise generalization bias across modalitiesin the target multi-modal dataset. Finally, we develop a SCAR-guided datacompletion strategy based on this generalization bias, which enables efficient,modality-aware expansion of modality-specific characteristics in multimodaldatasets. Experiments across diverse multi-modal datasets and modelarchitectures validate the effectiveness of SCAR in predicting data utility andguiding data acquisition. Code is available at https://github.com/McAloma/SCAR.</description>
      <author>example@mail.com (Ri Su, Zhao Chen, Caleb Chen Cao, Nan Tang, Lei Chen)</author>
      <guid isPermaLink="false">2508.19659v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>FinCast: A Foundation Model for Financial Time-Series Forecasting</title>
      <link>http://arxiv.org/abs/2508.19609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FinCast是一种专为金融时间序列预测设计的基础模型，通过在大规模金融数据集上训练，克服了现有深度学习方法面临的过拟合和需要大量领域特定微调的问题，展现出强大的零样本性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;金融时间序列预测对维持经济稳定、指导政策制定和促进可持续投资至关重要，但由于各种潜在的模式转变，这一任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉金融时间序列中多样化模式且无需领域特定微调的预测模型，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入FinCast，这是第一个专门为金融时间序列预测设计的基础模型，在大规模金融数据集上进行训练，以处理时间非平稳性、多领域多样性和不同时间分辨率带来的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;FinCast展现出强大的零样本性能，能够在不进行领域特定微调的情况下有效捕捉多样化模式；全面的实证和定性评估表明，FinCast超越了现有的最先进方法，突显了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;FinCast代表了金融时间序列预测领域的重要进展，通过基础模型方法解决了现有方法的局限性，为金融预测提供了更有效、更通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;金融时间序列预测对于维持经济稳定、指导明智的政策制定和促进可持续投资实践至关重要。然而，由于各种潜在的模式转变，它仍然具有挑战性。这些转变主要来自三个来源：时间非平稳性（分布随时间变化）、多领域多样性（股票、商品和期货等金融领域中存在不同模式）以及不同的时间分辨率（秒级、小时级、日级或周级指标之间存在不同模式）。虽然最近的深度学习方法试图解决这些复杂性，但它们经常出现过拟合问题，并且通常需要大量的领域特定微调。为了克服这些局限性，我们引入了FinCast，这是专为金融时间序列预测设计的基础模型，在大规模金融数据集上进行训练。值得注意的是，FinCast展现出强大的零样本性能，能够在不进行领域特定微调的情况下有效捕捉多样化模式。全面的实证和定性评估表明，FinCast超越了现有的最先进方法，突显了其强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial time-series forecasting is critical for maintaining economicstability, guiding informed policymaking, and promoting sustainable investmentpractices. However, it remains challenging due to various underlying patternshifts. These shifts arise primarily from three sources: temporalnon-stationarity (distribution changes over time), multi-domain diversity(distinct patterns across financial domains such as stocks, commodities, andfutures), and varying temporal resolutions (patterns differing acrossper-second, hourly, daily, or weekly indicators). While recent deep learningmethods attempt to address these complexities, they frequently suffer fromoverfitting and typically require extensive domain-specific fine-tuning. Toovercome these limitations, we introduce FinCast, the first foundation modelspecifically designed for financial time-series forecasting, trained onlarge-scale financial datasets. Remarkably, FinCast exhibits robust zero-shotperformance, effectively capturing diverse patterns without domain-specificfine-tuning. Comprehensive empirical and qualitative evaluations demonstratethat FinCast surpasses existing state-of-the-art methods, highlighting itsstrong generalization capabilities.</description>
      <author>example@mail.com (Zhuohang Zhu, Haodong Chen, Qiang Qu, Vera Chung)</author>
      <guid isPermaLink="false">2508.19609v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</title>
      <link>http://arxiv.org/abs/2508.19563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大语言模型(LLMs)被广泛应用于各种场景，作为即插即用的数据拟合和预测生成方法。然而研究发现LLMs对与学习任务无关的数据表示变化非常敏感，这种敏感性存在于多种LLMs和应用方法中，即使专门设计的表格基础模型也无法完全避免这种敏感性。&lt;h4&gt;背景&lt;/h4&gt;大语言模型(LLMs)正被广泛应用于各种场景，包括作为数据拟合和预测生成的即插即用方法。先前研究表明，通过上下文学习或监督微调，LLMs在预测性能上可以与许多表格监督学习技术相媲美。&lt;h4&gt;目的&lt;/h4&gt;识别LLMs在数据拟合方面的关键弱点，特别是对与学习任务无关的数据表示变化的敏感性，并探究这种敏感性的原因。&lt;h4&gt;方法&lt;/h4&gt;通过实验研究变量名称更改对预测误差的影响，检查开源LLM的注意力分数模式，并考察专门为数据拟合训练的表格基础模型(TabPFN)的稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs对与任务无关的变化非常敏感，简单更改变量名称可能导致预测误差变化高达82%；这种敏感性存在于上下文学习和监督微调中，也存在于闭源和开源通用LLMs中；LLMs中存在非均匀的注意力模式；即使专门设计的TabPFN也无法完全避免对任务无关变化的敏感性。&lt;h4&gt;结论&lt;/h4&gt;尽管LLMs具有令人印象深刻的预测能力，但目前它们甚至缺乏作为原则性数据拟合工具所需的基本稳健性水平。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型(LLMs)正被广泛应用于各种场景，远超典型的语言导向用例。特别是，LLMs越来越多地被用作即插即用的数据拟合和预测生成方法。先前的工作表明，通过上下文学习或监督微调，LLMs在预测性能上可以与许多表格监督学习技术相媲美。然而，我们确定了使用LLMs进行数据拟合的一个关键弱点——对数据表示的更改完全与底层学习任务无关，可以显著改变LLMs对同一数据的预测。例如，在某些情况下，仅更改变量名称就可能导致预测误差大小波动高达82%。这种对任务无关变化的预测敏感性在上下文学习和监督微调中都存在，无论是在闭源还是开源通用LLMs中。此外，通过检查一个开源LLM的注意力分数，我们发现了一种非均匀的注意力模式：训练示例和变量名/值恰好占据提示中的某些位置时，在生成输出标记时会获得更多关注，尽管不同位置预期应获得大致相同的注意力。这在一定程度上解释了对任务无关变化存在时的敏感性。我们还考虑了一个专门为数据拟合训练的最先进的表格基础模型(TabPFN)。尽管被明确设计为实现预测稳健性，TabPFN仍不能完全避免对任务无关变化的敏感性。总体而言，尽管LLMs具有令人印象深刻的预测能力，但目前它们甚至缺乏作为原则性数据拟合工具所需的基本稳健性水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) are being applied in a wide array of settings,well beyond the typical language-oriented use cases. In particular, LLMs areincreasingly used as a plug-and-play method for fitting data and generatingpredictions. Prior work has shown that LLMs, via in-context learning orsupervised fine-tuning, can perform competitively with many tabular supervisedlearning techniques in terms of predictive performance. However, we identify acritical vulnerability of using LLMs for data fitting -- making changes to datarepresentation that are completely irrelevant to the underlying learning taskcan drastically alter LLMs' predictions on the same data. For example, simplychanging variable names can sway the size of prediction error by as much as 82%in certain settings. Such prediction sensitivity with respect totask-irrelevant variations manifests under both in-context learning andsupervised fine-tuning, for both close-weight and open-weight general-purposeLLMs. Moreover, by examining the attention scores of an open-weight LLM, wediscover a non-uniform attention pattern: training examples and variablenames/values which happen to occupy certain positions in the prompt receivemore attention when output tokens are generated, even though differentpositions are expected to receive roughly the same attention. This partiallyexplains the sensitivity in the presence of task-irrelevant variations. We alsoconsider a state-of-the-art tabular foundation model (TabPFN) trainedspecifically for data fitting. Despite being explicitly designed to achieveprediction robustness, TabPFN is still not immune to task-irrelevantvariations. Overall, despite LLMs' impressive predictive capabilities,currently they lack even the basic level of robustness to be used as aprincipled data-fitting tool.</description>
      <author>example@mail.com (Hejia Liu, Mochen Yang, Gediminas Adomavicius)</author>
      <guid isPermaLink="false">2508.19563v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View</title>
      <link>http://arxiv.org/abs/2508.19508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种名为DATR的两阶段框架，用于从稀疏视图重建苹果树，实现了高精度的3D重建，在保持与工业级激光扫描仪相当精度的同时，将处理速度提高了约360倍。&lt;h4&gt;背景&lt;/h4&gt;数字孪生应用通过物理资产的精确虚拟副本实现实时监控和机器人模拟，这些系统的关键是具有高几何保真度的3D重建。然而，现有方法在野外条件下表现不佳，特别是在稀疏和遮挡视图的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一个从稀疏视图重建苹果树的两阶段框架(DATR)，以提高3D重建的精度和效率。&lt;h4&gt;方法&lt;/h4&gt;第一阶段利用机载传感器和基础模型从复杂的野外图像半自动生成树木掩码；第二阶段使用树木掩码过滤多模态数据中的背景信息，结合扩散模型和大型重建模型进行单图像到3D重建。使用Real2Sim数据生成器生成的真实合成苹果树来训练模型，并在野外和合成数据集上评估框架。&lt;h4&gt;主要发现&lt;/h4&gt;DATR框架在两个数据集上都优于现有的3D重建方法，实现了与工业级固定激光扫描仪相当的域特性估计，同时将吞吐量提高了约360倍。&lt;h4&gt;结论&lt;/h4&gt;DATR框架展示了可扩展的农业数字孪生系统的强大潜力，为农业领域的数字孪生应用提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;数字孪生应用通过物理资产的精确虚拟副本实现实时监控和机器人模拟，展现出变革性潜力。这些系统的关键在于高几何保真度的3D重建。然而，现有方法在野外条件下表现不佳，特别是在处理稀疏和遮挡视图时。本研究开发了一个用于从稀疏视图重建苹果树的两阶段框架(DATR)。第一阶段利用机载传感器和基础模型从复杂的野外图像半自动生成树木掩码。树木掩码用于过滤多模态数据中的背景信息，以进行第二阶段的单图像到3D重建。该阶段包括一个扩散模型和一个大型重建模型，分别用于多视图和隐式神经场生成。扩散模型和LRM的训练使用了通过Real2Sim数据生成器生成的真实合成苹果树。该框架在野外和合成数据集上进行了评估。野外数据集包含六棵具有现场测量真实值的苹果树，而合成数据集则具有结构多样的树木。评估结果表明，我们的DATR框架在两个数据集上都优于现有的3D重建方法，实现了与工业级固定激光扫描仪相当的域特性估计，同时将吞吐量提高了约360倍，展示了可扩展农业数字孪生系统的强大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从稀疏视角（sparse-view）进行苹果树3D重建的问题。这个问题在现实中很重要，因为数字孪生应用需要准确的虚拟副本来实现农业机器人的实时监控和模拟，而现有的3D重建方法在田间条件下表现不佳，特别是在视角稀疏和存在遮挡的情况下。高几何保真度的3D重建是农业数字孪生系统的关键，有助于解决农业机器人开发受限于季节性作物生长周期的问题，实现全年虚拟测试和验证。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出2D成像系统用于树木作物3D重建的核心限制在于依赖昂贵、专门构建的硬件和软件解决方案，这限制了泛化能力。为此，他们设计了一个结合数据收集机器人（AATBot）、简单扫描策略和新颖机器人感知框架（DATR）的全面方法。他们借鉴了扩散模型（如Zero123）进行数据生成和去噪，利用大型重建模型（LRM）进行神经场回归，并使用了Real2Sim数据生成器创建合成数据。同时，他们创新性地结合了多模态输入（RGB图像、深度图和点云），开发了专门的背景去除模块和尺度检索机制，以解决稀疏视角下的重建挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用扩散模型和大型重建模型的强大3D先验知识，通过多模态数据融合（RGB图像、深度图和点云）从稀疏田间图像中推断详细的3D树几何结构，并使用合成数据训练模型实现零样本真实世界部署。整体流程包括：1) 使用AATBot机器人沿种植行收集苹果树图像；2) 通过背景去除模块过滤背景并提取树掩膜；3) 多模态重建模块处理RGB、深度和点云数据；4) 尺度检索机制将重建模型与真实世界度量空间对齐；5) 生成3D网格和点云并估计树干直径和分支数等特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) DATR两阶段框架，结合扩散模型和LRM进行多视图和隐式神经场生成；2) 多模态重建模块，集成RGB、深度和点云数据；3) 自动化背景去除模块，减少手动标注需求；4) 尺度检索机制，解决图像重建中的尺度模糊性问题；5) 使用Real2Sim合成数据进行训练，实现零样本真实世界部署。相比之前的工作，DATR不依赖昂贵的专用硬件，通过多模态输入提高重建质量，在稀疏视图下实现更高几何保真度，更准确估计生物相关树属性，并在保持精度的同时提高吞吐量约360倍。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了DATR框架，一种结合扩散模型和大型重建模型的创新方法，通过多模态数据融合和合成数据训练，实现了从稀疏视角高效重建高保真度苹果树3D模型，为农业数字孪生系统和机器人应用提供了强大工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital twin applications offered transformative potential by enablingreal-time monitoring and robotic simulation through accurate virtual replicasof physical assets. The key to these systems is 3D reconstruction with highgeometrical fidelity. However, existing methods struggled under fieldconditions, especially with sparse and occluded views. This study developed atwo-stage framework (DATR) for the reconstruction of apple trees from sparseviews. The first stage leverages onboard sensors and foundation models tosemi-automatically generate tree masks from complex field images. Tree masksare used to filter out background information in multi-modal data for thesingle-image-to-3D reconstruction at the second stage. This stage consists of adiffusion model and a large reconstruction model for respective multi view andimplicit neural field generation. The training of the diffusion model and LRMwas achieved by using realistic synthetic apple trees generated by a Real2Simdata generator. The framework was evaluated on both field and syntheticdatasets. The field dataset includes six apple trees with field-measured groundtruth, while the synthetic dataset featured structurally diverse trees.Evaluation results showed that our DATR framework outperformed existing 3Dreconstruction methods across both datasets and achieved domain-traitestimation comparable to industrial-grade stationary laser scanners whileimproving the throughput by $\sim$360 times, demonstrating strong potential forscalable agricultural digital twin systems.</description>
      <author>example@mail.com (Tian Qiu, Alan Zoubi, Yiyuan Lin, Ruiming Du, Lailiang Cheng, Yu Jiang)</author>
      <guid isPermaLink="false">2508.19508v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Symbolic Regression via Foundation Model Distillation</title>
      <link>http://arxiv.org/abs/2508.19487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EQUATE是一个数据高效的微调框架，通过蒸馏方法使基础模型适应低数据环境下的符号方程发现，将离散方程搜索转化为连续优化任务，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;从观测数据中发现可解释的数学方程（方程发现或符号回归）是科学发现的基础，能够透明建模物理、生物和经济系统。预训练在大型方程数据集上的基础模型有潜力，但在应用于小型、领域特定数据集时常面临负迁移和泛化能力差的问题。&lt;h4&gt;目的&lt;/h4&gt;引入EQUATE（基于质量对齐的转移嵌入的方程生成）框架，通过数据高效的微调方法解决基础模型在小数据集上应用时的局限性，使其能够有效进行符号方程发现。&lt;h4&gt;方法&lt;/h4&gt;EQUATE结合符号-数值对齐与评估器引导的嵌入优化，实现有原则的嵌入-搜索-生成范式。该方法将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，由数据-方程适应性和简单性共同指导。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上的实验表明，EQUATE在准确性和鲁棒性方面 consistently 超越了最先进的基线方法，同时保持了低复杂性和快速推理能力。&lt;h4&gt;结论&lt;/h4&gt;EQUATE是基础模型蒸馏设置中数据高效符号回归的实用且可推广的解决方案，为科学发现中的方程建模提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;从观测数据中发现可解释的数学方程（也称为方程发现或符号回归）是科学发现的基石，能够透明地建模物理、生物和经济系统。虽然在大型方程数据集上预训练的基础模型提供了一个有希望的起点，但当它们应用于小型、领域特定的数据集时，常常遭受负迁移和泛化能力差的问题。在本文中，我们引入了EQUATE（基于质量对齐的转移嵌入的方程生成），这是一个数据高效的微调框架，通过蒸馏方法使基础模型适应低数据环境下的符号方程发现。EQUATE将符号-数值对齐与评估器引导的嵌入优化相结合，实现了一个有原则的嵌入-搜索-生成范式。我们的方法将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，由数据-方程适应性和简单性指导。在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上的实验表明，EQUATE在准确性和鲁棒性方面 consistently 超越了最先进的基线方法，同时保持了低复杂性和快速推理能力。这些结果突显了EQUATE作为基础模型蒸馏设置中数据高效符号回归的实用且可推广的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discovering interpretable mathematical equations from observed data (a.k.a.equation discovery or symbolic regression) is a cornerstone of scientificdiscovery, enabling transparent modeling of physical, biological, and economicsystems. While foundation models pre-trained on large-scale equation datasetsoffer a promising starting point, they often suffer from negative transfer andpoor generalization when applied to small, domain-specific datasets. In thispaper, we introduce EQUATE (Equation Generation via QUality-Aligned TransferEmbeddings), a data-efficient fine-tuning framework that adapts foundationmodels for symbolic equation discovery in low-data regimes via distillation.EQUATE combines symbolic-numeric alignment with evaluator-guided embeddingoptimization, enabling a principled embedding-search-generation paradigm. Ourapproach reformulates discrete equation search as a continuous optimizationtask in a shared embedding space, guided by data-equation fitness andsimplicity. Experiments across three standard public benchmarks (Feynman,Strogatz, and black-box datasets) demonstrate that EQUATE consistentlyoutperforms state-of-the-art baselines in both accuracy and robustness, whilepreserving low complexity and fast inference. These results highlight EQUATE asa practical and generalizable solution for data-efficient symbolic regressionin foundation model distillation settings.</description>
      <author>example@mail.com (Wangyang Ying, Jinghan Zhang, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Kunpeng Liu, Chandan K. Reddy, Yanjie Fu)</author>
      <guid isPermaLink="false">2508.19487v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification</title>
      <link>http://arxiv.org/abs/2508.19393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GENIE-ASI的无需训练的基于大型语言模型的模拟子电路识别方法，通过上下文学习和代码转换实现子电路识别，并在新基准测试中展示了良好性能。&lt;h4&gt;背景&lt;/h4&gt;模拟子电路识别是模拟设计中的核心任务，对仿真、尺寸确定和布局至关重要。传统方法通常需要大量人类专业知识、基于规则的编码或大型标记数据集。&lt;h4&gt;目的&lt;/h4&gt;解决传统模拟子电路识别方法的局限性，提出一种无需训练的、基于大型语言模型的替代方法。&lt;h4&gt;方法&lt;/h4&gt;GENIE-ASI分为两个阶段：首先使用上下文学习从少量演示示例中推导自然语言指令，然后将这些指令转换为可执行的Python代码，以识别未见过的SPICE网表中的子电路。&lt;h4&gt;主要发现&lt;/h4&gt;在提出的基准测试中，GENIE-ASI在简单结构上与基于规则的方法性能相当，在中等抽象级别上保持竞争力，在复杂子电路上也显示出潜力。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型可以作为模拟设计自动化中的适应性、通用工具，为基础模型在模拟设计自动化中的应用开辟了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;模拟子电路识别是模拟设计中的核心任务，对仿真、尺寸确定和布局至关重要。传统方法通常需要大量人类专业知识、基于规则的编码或大型标记数据集。为解决这些挑战，我们提出了GENIE-ASI，这是第一个无需训练的、基于大型语言模型的方法，用于模拟子电路识别。GENIE-ASI分为两个阶段：首先使用上下文学习从少量演示示例中推导自然语言指令，然后将其转换为可执行的Python代码，以识别未见过的SPICE网表中的子电路。此外，为了系统评估基于LLM的方法，我们引入了一个新的基准测试，包含涵盖广泛子电路变体的运算放大器网表。在所提出的基准测试上的实验结果表明，GENIE-ASI在简单结构上与基于规则的性能相当，在中等抽象级别上保持竞争力，甚至在复杂子电路上也显示出潜力。这些发现表明，LLM可以作为模拟设计自动化中的适应性、通用工具，为基础模型在模拟设计自动化中的应用开辟新的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog subcircuit identification is a core task in analog design, essentialfor simulation, sizing, and layout. Traditional methods often require extensivehuman expertise, rule-based encoding, or large labeled datasets. To addressthese challenges, we propose GENIE-ASI, the first training-free, large languagemodel (LLM)-based methodology for analog subcircuit identification. GENIE-ASIoperates in two phases: it first uses in-context learning to derive naturallanguage instructions from a few demonstration examples, then translates theseinto executable Python code to identify subcircuits in unseen SPICE netlists.In addition, to evaluate LLM-based approaches systematically, we introduce anew benchmark composed of operational amplifier netlists (op-amps) that cover awide range of subcircuit variants. Experimental results on the proposedbenchmark show that GENIE-ASI matches rule-based performance on simplestructures (F1-score = 1.0), remains competitive on moderate abstractions(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =0.31). These findings demonstrate that LLMs can serve as adaptable,general-purpose tools in analog design automation, opening new researchdirections for foundation model applications in analog design automation.</description>
      <author>example@mail.com (Phuoc Pham, Arun Venkitaraman, Chia-Yu Hsieh, Andrea Bonetti, Stefan Uhlich, Markus Leibl, Simon Hofmann, Eisaku Ohbuchi, Lorenzo Servadei, Ulf Schlichtmann, Robert Wille)</author>
      <guid isPermaLink="false">2508.19393v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Principled Detection of Hallucinations in Large Language Models via Multiple Testing</title>
      <link>http://arxiv.org/abs/2508.18473v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了大型语言模型(LLMs)的幻觉问题，提出了一种基于多重检验假设的方法来检测幻觉，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)已成为解决各种任务的强大基础模型，但它们也容易出现幻觉问题，即生成听起来自信但实际上不正确甚至无意义的回答。&lt;h4&gt;目的&lt;/h4&gt;将幻觉检测问题表述为假设检验问题，并与机器学习模型中的分布外检测问题进行类比，提出一种新的幻觉检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种受多重检验启发的方法来解决幻觉检测问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验结果验证了所提出方法的鲁棒性，表明其优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的多重检验方法在幻觉检测问题上具有有效性和优势。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型语言模型(LLMs)已成为解决各种任务的强大基础模型，但它们也被证明容易出现幻觉，即生成听起来自信但实际上不正确甚至无意义的回答。在这项工作中，我们将幻觉检测问题表述为假设检验问题，并将其与机器学习模型中的分布外检测问题进行类比。我们提出了一种受多重检验启发的方法来解决幻觉检测问题，并通过广泛的实验结果验证了我们的方法在最先进方法面前的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Large Language Models (LLMs) have emerged as powerful foundationalmodels to solve a variety of tasks, they have also been shown to be prone tohallucinations, i.e., generating responses that sound confident but areactually incorrect or even nonsensical. In this work, we formulate the problemof detecting hallucinations as a hypothesis testing problem and draw parallelsto the problem of out-of-distribution detection in machine learning models. Wepropose a multiple-testing-inspired method to solve the hallucination detectionproblem, and provide extensive experimental results to validate the robustnessof our approach against state-of-the-art methods.</description>
      <author>example@mail.com (Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli)</author>
      <guid isPermaLink="false">2508.18473v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.19298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, 13th International Workshop on Biometrics and  Forensics (IWBF)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DemoBias工作，旨在评估大型视觉语言模型(LVLMs)在生物识别面部识别任务中的人口统计偏差问题&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型在各种下游任务中表现出色，包括带描述的生物识别面部识别，但这些基础模型在种族/民族、性别和年龄等不同人口统计群体中往往无法公平表现&lt;h4&gt;目的&lt;/h4&gt;通过实证评估研究LVLMs在生物识别面部识别和文本标记生成任务中的人口统计偏差程度&lt;h4&gt;方法&lt;/h4&gt;在自建的人口平衡数据集上微调和评估了三种预训练LVLMs（LLaVA、BLIP-2和PaliGemma），并使用特定群体BERTScores和公平差异率等指标量化性能差异&lt;h4&gt;主要发现&lt;/h4&gt;实验揭示了LVLMs中存在人口统计偏差，其中PaliGemma和LLaVA在西班牙裔/拉丁裔、高加索人和南亚群体中表现出更高的差异，而BLIP-2则表现出相对一致的性能&lt;h4&gt;结论&lt;/h4&gt;大型视觉语言模型在生物识别面部识别任务中存在人口统计偏差问题，不同模型在不同人口统计群体中的表现存在显著差异&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型(LVLMs)在各种下游任务中表现出色，包括带描述的生物识别面部识别(FR)。然而，人口统计偏差仍然是FR中的一个关键问题，因为这些基础模型在考虑种族/民族、性别和年龄等不同人口统计群体时，往往无法公平表现。因此，通过我们的DemoBias工作，我们进行了实证评估，研究LVLMs在生物识别FR和文本标记生成任务中人口统计偏差的程度。在我们自己生成的人口平衡数据集上微调和评估了三个广泛使用的预训练LVLMs：LLaVA、BLIP-2和PaliGemma。我们使用多种评估指标，如特定群体的BERTScores和公平差异率，来量化和追踪性能差异。实验结果提供了关于LVLMs在不同人口统计群体中公平性和可靠性的有力见解。我们的实证研究发现了LVLMs中存在人口统计偏差，其中PaliGemma和LLaVA在西班牙裔/拉丁裔、高加索人和南亚群体中表现出更高的差异，而BLIP-2则表现出相对一致的性能。仓库：https://github.com/Sufianlab/DemoBias。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IWBF63717.2025.11113455&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision Language Models (LVLMs) have demonstrated remarkablecapabilities across various downstream tasks, including biometric facerecognition (FR) with description. However, demographic biases remain acritical concern in FR, as these foundation models often fail to performequitably across diverse demographic groups, considering ethnicity/race,gender, and age. Therefore, through our work DemoBias, we conduct an empiricalevaluation to investigate the extent of demographic biases in LVLMs forbiometric FR with textual token generation tasks. We fine-tuned and evaluatedthree widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our owngenerated demographic-balanced dataset. We utilize several evaluation metrics,like group-specific BERTScores and the Fairness Discrepancy Rate, to quantifyand trace the performance disparities. The experimental results delivercompelling insights into the fairness and reliability of LVLMs across diversedemographic groups. Our empirical study uncovered demographic biases in LVLMs,with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparablyconsistent. Repository: https://github.com/Sufianlab/DemoBias.</description>
      <author>example@mail.com (Abu Sufian, Anirudha Ghosh, Debaditya Barman, Marco Leo, Cosimo Distante)</author>
      <guid isPermaLink="false">2508.19298v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Context-aware Sparse Spatiotemporal Learning for Event-based Vision</title>
      <link>http://arxiv.org/abs/2508.19806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为上下文感知稀疏时空学习(CSSL)的新框架，用于高效处理事件相机数据，解决了现有方法在资源受限应用中的局限性。&lt;h4&gt;背景&lt;/h4&gt;事件相机在机器人感知中具有优势，但现有深度学习方法未能充分利用事件数据的稀疏性，难以集成到边缘应用中。神经形态计算虽能效高，但脉冲神经网络在复杂任务中性能不足，且高激活稀疏性难以实现。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效处理事件相机数据的新框架，在保持高性能的同时实现高神经元稀疏性，适用于神经形态处理。&lt;h4&gt;方法&lt;/h4&gt;提出上下文感知稀疏时空学习(CSSL)框架，引入上下文感知阈值根据输入分布动态调节神经元激活，自然减少激活密度而无需显式稀疏约束。&lt;h4&gt;主要发现&lt;/h4&gt;CSSL在事件目标检测和光流估计任务中实现了与最先进方法相当或更好的性能，同时保持极高的神经元稀疏性。&lt;h4&gt;结论&lt;/h4&gt;CSSL在实现神经形态处理的高效事件视觉方面具有关键作用，为资源受限环境中的事件数据处理提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;事件相机已成为机器人感知的一种有前景的范式，具有高时间分辨率、高动态范围和对运动模糊的鲁棒性等优势。然而，现有的基于深度学习的事件处理方法往往未能充分利用事件数据的稀疏性，使它们难以集成到资源受限的边缘应用中。虽然神经形态计算提供了一种能效高的替代方案，但脉冲神经网络在复杂的事件视觉任务（如目标检测和光流估计）中难以达到最先进模型的性能。此外，在神经网络中实现高激活稀疏性仍然很困难，通常需要仔细手动调整稀疏诱导损失项。本文提出了上下文感知稀疏时空学习(CSSL)这一新框架，引入了上下文感知阈值来根据输入分布动态调节神经元激活，自然地减少激活密度而无需显式的稀疏约束。将CSSL应用于基于事件的目标检测和光流估计，它实现了与最先进方法相当或更好的性能，同时保持了极高的神经元稀疏性。我们的实验结果突显了CSSL在实现神经形态处理的高效事件视觉方面的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event-based camera has emerged as a promising paradigm for robot perception,offering advantages with high temporal resolution, high dynamic range, androbustness to motion blur. However, existing deep learning-based eventprocessing methods often fail to fully leverage the sparse nature of eventdata, complicating their integration into resource-constrained edgeapplications. While neuromorphic computing provides an energy-efficientalternative, spiking neural networks struggle to match of performance ofstate-of-the-art models in complex event-based vision tasks, like objectdetection and optical flow. Moreover, achieving high activation sparsity inneural networks is still difficult and often demands careful manual tuning ofsparsity-inducing loss terms. Here, we propose Context-aware SparseSpatiotemporal Learning (CSSL), a novel framework that introduces context-awarethresholding to dynamically regulate neuron activations based on the inputdistribution, naturally reducing activation density without explicit sparsityconstraints. Applied to event-based object detection and optical flowestimation, CSSL achieves comparable or superior performance tostate-of-the-art methods while maintaining extremely high neuronal sparsity.Our experimental results highlight CSSL's crucial role in enabling efficientevent-based vision for neuromorphic processing.</description>
      <author>example@mail.com (Shenqi Wang, Guangzhi Tang)</author>
      <guid isPermaLink="false">2508.19806v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Generalizing Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.19593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD Thesis submitted to MSU&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文解决了单目3D物体检测模型在不同场景下的泛化挑战，提出了多种方法来处理遮挡、数据集差异、大物体检测和相机参数变化等问题。&lt;h4&gt;背景&lt;/h4&gt;单目3D物体检测(Mono3D)是计算机视觉的基本任务，它从单张图像估计物体的类别、3D位置、尺寸和方向。其应用包括自动驾驶、增强现实和机器人技术，这些应用都依赖于准确的3D环境理解。&lt;h4&gt;目的&lt;/h4&gt;解决Mono3D模型在不同场景下的泛化问题，包括遮挡、不同数据集、物体大小变化和相机参数变化等场景。&lt;h4&gt;方法&lt;/h4&gt;1) 提出数学上可微分的NMS(GrooMeD-NMS)增强遮挡鲁棒性；2) 探索深度等变(DEVIANT)主干网络改进对新数据集的泛化；3) 提出基于分割的鸟瞰图方法配合dice损失(SeaBird)解决大物体检测问题；4) 数学分析Mono3D模型对未见过的相机高度的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;大物体检测问题不仅是数据不平衡或感受野问题，也是噪声敏感性问题；所提出的方法可以改善模型在分布外设置中的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过提出的方法，Mono3D模型在遮挡、新数据集、大物体检测和不同相机参数下的泛化能力得到了提高。&lt;h4&gt;翻译&lt;/h4&gt;单目3D物体检测(Mono3D)是一项基础计算机视觉任务，它从单张图像估计物体的类别、3D位置、尺寸和方向。其应用包括自动驾驶、增强现实和机器人技术，这些应用都严重依赖于准确的3D环境理解。本论文解决了Mono3D模型在不同场景下的泛化挑战，包括遮挡、数据集、物体大小和相机参数。为了增强遮挡鲁棒性，我们提出了一个数学上可微分的NMS(GrooMeD-NMS)。为了提高对新数据集的泛化能力，我们探索了深度等变(DEVIANT)主干网络。我们解决了大物体检测问题，证明它不仅仅是数据不平衡或感受野问题，也是噪声敏感性问题。为此，我们引入了一种基于鸟瞰图分割的方法配合dice损失(SeaBird)。最后，我们数学分析了Mono3D模型对未见过的相机高度的泛化能力，并提高了模型在这种分布外设置中的泛化性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D目标检测（Mono3D）的泛化问题，包括处理遮挡、不同数据集、物体大小变化和相机参数变化等场景。这个问题在现实中非常重要，因为Mono3D在自动驾驶、增强现实和机器人等领域有广泛应用，这些应用需要模型在各种复杂条件下保持准确的3D环境理解能力，特别是在安全关键应用中，模型的泛化能力直接关系到系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者针对不同的泛化问题提出了四种方法：1) GrooMeD-NMS借鉴了2D目标检测中尝试将NMS纳入训练的想法，但针对3D检测进行了改进；2) DEVIANT借鉴了现有的尺度等变性可转向块，构建了深度等变网络；3) SeaBird借鉴了分割方法，结合dice损失解决大物体检测问题；4) CHARM3R通过分析深度估计与相机高度的关系，提出平均深度估计的方法。作者都基于现有计算机视觉技术，但针对Mono3D的特点进行了创新和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇论文提出了四种方法：1) GrooMeD-NMS的核心是将NMS纳入训练过程，通过矩阵操作、分组和掩码使NMS可微分，实现端到端学习；2) DEVIANT的核心是构建对深度平移具有等变性的网络，确保深度估计一致性；3) SeaBird的核心是通过鸟瞰图分割和dice损失解决大物体检测的噪声敏感性问题；4) CHARM3R的核心是平均模型内的深度估计以提高对未见相机高度的泛化能力。每种方法都有其特定的实现流程，包括理论分析、模型设计和实验验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次提出闭式数学可微分NMS解决训练-推理不匹配；2) 构建深度等变网络提高跨数据集泛化；3) 揭示大物体检测的噪声敏感性并提出基于dice损失的分割方法；4) 分析相机高度影响并提出深度平均策略。相比之前工作，这些方法不仅提高基准性能，更专注于解决多样化场景下的泛化问题，提供了理论分析和针对性的解决方案，在多个数据集上验证了泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出四种创新方法，显著提升了单目3D目标检测在遮挡、不同数据集、大物体和不同相机高度等多样化场景下的泛化能力，为实际应用提供了更鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detection (Mono3D) is a fundamental computer vision taskthat estimates an object's class, 3D position, dimensions, and orientation froma single image. Its applications, including autonomous driving, augmentedreality, and robotics, critically rely on accurate 3D environmentalunderstanding. This thesis addresses the challenge of generalizing Mono3Dmodels to diverse scenarios, including occlusions, datasets, object sizes, andcamera parameters. To enhance occlusion robustness, we propose a mathematicallydifferentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, weexplore depth equivariant (DEVIANT) backbones. We address the issue of largeobject detection, demonstrating that it's not solely a data imbalance orreceptive field problem but also a noise sensitivity issue. To mitigate this,we introduce a segmentation-based approach in bird's-eye view with dice loss(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3Dmodels to unseen camera heights and improve Mono3D generalization in suchout-of-distribution settings.</description>
      <author>example@mail.com (Abhinav Kumar)</author>
      <guid isPermaLink="false">2508.19593v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion</title>
      <link>http://arxiv.org/abs/2508.03252v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RSDNet是一种基于DDPMs的可分离潜在框架的鲁棒单阶段全稀疏3D目标检测网络，通过在潜在特征空间中学习去噪过程、改进扩散机制和引入语义-几何条件引导，实现了高效、鲁棒的3D目标检测。&lt;h4&gt;背景&lt;/h4&gt;DDPMs在鲁棒3D目标检测任务中已显示出成功。现有方法通常依赖于3D框的分数匹配或预训练的扩散先验，但这些方法通常需要推理时的多步迭代，限制了效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法效率低的问题，提出一种单阶段、全稀疏的3D目标检测网络，实现高效且鲁棒的3D目标检测。&lt;h4&gt;方法&lt;/h4&gt;提出RSDNet，通过轻量级去噪网络（如多级去噪自编码器DAEs）在潜在特征空间中学习去噪过程；改进DDPMs的加噪和去噪机制，使DLF能够构建多类型和多级别的噪声样本和目标；引入语义-几何条件引导来感知物体边界和形状；DLF的可分离去噪网络设计使RSDNet能够在推理时进行单步检测。&lt;h4&gt;主要发现&lt;/h4&gt;RSDNet能够有效理解多级扰动下的场景分布，实现鲁棒可靠的检测；语义-几何条件引导缓解了稀疏表示中的中心特征缺失问题；RSDNet能够在全稀疏检测流程中运行；单步检测进一步提高了检测效率。&lt;h4&gt;结论&lt;/h4&gt;在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测性能。&lt;h4&gt;翻译&lt;/h4&gt;去噪扩散概率模型在鲁棒3D目标检测任务中已显示出成功。现有方法通常依赖于3D框的分数匹配或预训练的扩散先验。然而，它们通常需要推理时的多步迭代，这限制了效率。为了解决这个问题，我们提出了一种基于DDPMs的可分离潜在框架的鲁棒单阶段全稀疏3D目标检测网络，命名为RSDNet。具体来说，RSDNet通过轻量级去噪网络（如多级去噪自编码器DAEs）在潜在特征空间中学习去噪过程。这使RSDNet能够有效理解多级扰动下的场景分布，实现鲁棒可靠的检测。同时，我们重新设计了DDPMs的加噪和去噪机制，使DLF能够构建多类型和多级别的噪声样本和目标，增强RSDNet对多种扰动的鲁棒性。此外，我们引入了语义-几何条件引导来感知物体边界和形状，缓解了稀疏表示中的中心特征缺失问题，使RSDNet能够在全稀疏检测流程中运行。而且，DLF的可分离去噪网络设计使RSDNet能够在推理时进行单步检测，进一步提高检测效率。在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D物体检测方法在推理阶段需要多步迭代导致效率低下的问题，以及对多种扰动的鲁棒性不足的问题。这个问题在现实中非常重要，因为自动驾驶、AR/VR和机器人等实时应用需要高效的检测系统，而真实场景中的点云数据常常受到点级随机噪声和全局几何失真（坐标偏移、缩放、旋转）的影响，导致检测性能不稳定。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了DDPMs在3D物体检测中的局限性，揭示了其鲁棒性来源于训练阶段而非推理阶段。他们重新思考了噪声化和去噪声机制，设计了可分离的潜在框架(DLF)。作者借鉴了DDPMs的基本原理、多级去噪自编码器的设计、现有的全稀疏3D检测管道以及语义-几何条件引导的思想，但进行了创新性改造，使其能够处理多种扰动并实现单步推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个可分离的潜在框架(DLF)，将去噪网络作为辅助分支引导主干网络在潜在特征空间中学习去噪过程，但在推理时分离去噪网络实现单步检测。整体流程包括：1)全稀疏管道(FSP)进行特征提取；2)噪声构建模块(NCM)创建多类型和级别噪声样本；3)语义-几何条件层(SGCL)增强边界和形状感知；4)去噪U-Net(DUNet)指导主干学习；5)训练时结合扩散损失和任务损失，推理时实现单步检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)可分离的潜在框架(DLF)实现单步推理同时保持鲁棒性；2)多类型和多级别噪声建模增强对多种扰动的处理能力；3)语义-几何条件引导缓解中心特征缺失问题；4)基于DLF构建的鲁棒单步全稀疏检测网络RSDNet。相比之前工作，RSDNet无需多步迭代，在全稀疏管道中工作，显著提高了对噪声和几何失真的鲁棒性，同时保持了高检测精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种可分离的潜在扩散框架，实现了高效的单步3D物体检测，同时保持了对多种扰动的强鲁棒性，显著提升了在真实场景中的检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust3D object detection tasks. Existing methods often rely on the score matchingfrom 3D boxes or pre-trained diffusion priors. However, they typically requiremulti-step iterations in inference, which limits efficiency. To address this,we propose a Robust single-stage fully Sparse 3D object Detection Network witha Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically,RSDNet learns the denoising process in latent feature spaces throughlightweight denoising networks like multi-level denoising autoencoders (DAEs).This enables RSDNet to effectively understand scene distributions undermulti-level perturbations, achieving robust and reliable detection. Meanwhile,we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF toconstruct multi-type and multi-level noise samples and targets, enhancingRSDNet robustness to multiple perturbations. Furthermore, a semantic-geometricconditional guidance is introduced to perceive the object boundaries andshapes, alleviating the center feature missing problem in sparserepresentations, enabling RSDNet to perform in a fully sparse detectionpipeline. Moreover, the detachable denoising network design of DLF enablesRSDNet to perform single-step detection in inference, further enhancingdetection efficiency. Extensive experiments on public benchmarks show thatRSDNet can outperform existing methods, achieving state-of-the-art detection.</description>
      <author>example@mail.com (Wentao Qu, Guofeng Mei, Jing Wang, Yujiao Wu, Xiaoshui Huang, Liang Xiao)</author>
      <guid isPermaLink="false">2508.03252v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation</title>
      <link>http://arxiv.org/abs/2508.15427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Lang2Lift框架，利用基础模型实现自然语言引导的托盘检测和6D姿态估计，解决了物流和建筑业在户外环境中的托盘处理自动化挑战。&lt;h4&gt;背景&lt;/h4&gt;物流和建筑业在托盘处理自动化方面面临持续挑战，特别是在户外环境中，存在负载变化、托盘质量和尺寸不一致以及周围环境无结构化的问题。&lt;h4&gt;目的&lt;/h4&gt;解决托盘运输中的关键步骤自动化问题，即托盘拾取操作，同时应对劳动力短缺、安全问题以及在复杂条件下手动定位和回收托盘的低效率问题。&lt;h4&gt;方法&lt;/h4&gt;提出Lang2Lift框架，利用基础模型进行自然语言引导的托盘检测和6D姿态估计，使操作员可通过直观命令指定目标；感知管道集成Florence-2和SAM-2进行语言引导的分割，以及FoundationPose用于复杂场景下的鲁棒姿态估计；生成的姿态信息输入运动规划模块实现全自动叉车操作。&lt;h4&gt;主要发现&lt;/h4&gt;在ADAPT自动叉车平台上验证了Lang2Lift，在真实世界测试数据集上实现了0.76的托盘分割精度mIoU；时间和误差分析证明了系统的鲁棒性，确认了系统在物流和建筑环境中部署的可行性。&lt;h4&gt;结论&lt;/h4&gt;Lang2Lift可以有效解决物流和建筑环境中托盘处理自动化的挑战，特别适用于具有变化负载、不一致托盘质量和尺寸以及无结构化周围条件的户外环境。&lt;h4&gt;翻译&lt;/h4&gt;物流和建筑业在托盘处理自动化方面面临持续挑战，尤其是在户外环境中，存在负载变化、托盘质量和尺寸不一致以及周围环境无结构化的问题。在本文中，我们解决了托盘运输中的关键步骤自动化问题：托盘拾取操作。我们的工作动机是在这些条件下劳动力短缺、安全问题以及手动定位和回收托盘的低效率问题。我们提出了Lang2Lift框架，它利用基础模型进行自然语言引导的托盘检测和6D姿态估计，使操作员能够通过直观命令（如'吊起起重机附近的钢梁托盘'）指定目标。感知管道集成了Florence-2和SAM-2进行语言引导的分割，以及FoundationPose用于在复杂、多托盘户外场景和变化光照条件下的鲁棒姿态估计。生成的姿态信息输入到运动规划模块，实现全自动叉车操作。我们在ADAPT自动叉车平台上验证了Lang2Lift，在真实世界测试数据集上实现了0.76的托盘分割精度mIoU。时间和误差分析证明了系统的鲁棒性，并确认了系统在物流和建筑环境中部署的可行性。视频演示可在https://eric-nguyen1402.github.io/lang2lift.github.io/查看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决物流和建筑行业中自动化托盘处理的挑战，特别是在室外环境中如何让叉车理解自然语言指令并准确识别和抓取特定托盘的问题。这个问题在现实中非常重要，因为劳动力短缺、安全隐患以及在负载不一致、托盘质量和尺寸变化、环境杂乱的条件下手动定位和检索托盘的低效率，都是行业面临的实际问题。当前自动化系统依赖刚性预编程特性，难以适应新托盘类型、意外方向或杂乱场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有CNN方法泛化能力差，无法可靠分类货物负载；发现基础模型(FM)为智能自动化提供了新可能；视觉语言模型(VLMs)能创建灵活机器人系统；自然语言界面能弥合人机通信差距。他们设计了一个结合自然语言引导和视觉基础模型的框架，与叉车控制模块集成。借鉴了Florence-2进行视觉语言检测、SAM-2进行精细分割、FoundationPose进行姿态估计，以及[31]中的分层运动规划和[34]中的控制律。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基础模型实现语言引导的托盘检测和6D姿态估计，使操作员能通过直观命令指定目标。整体流程包括：1)感知管道：语言处理→Florence-2检测→SAM-2分割→FoundationPose姿态估计→几何细化→时间跟踪；2)规划控制管道：混合A*路径规划→Lyapunov控制→叉子精确定位→任务规划；3)基于ROS2的系统集成管理数据流和时间约束。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)实时基础模型框架，实现1Hz闭环运行；2)自然语言引导的完整自主操作流程；3)在低光、雪天等挑战环境中验证，达到0.76 mIoU；4)提供详细时间分析证明实际部署可行性。相比之前工作，传统方法依赖特定传感器和受控环境，CNN方法在室外条件下性能大幅下降(0.4-0.5 mIoU)，现有语言驱动方法主要在室内验证，缺乏对室外工业条件的评估，且没有工作实现视觉语言模型与专用姿态估计模型的集成用于实时室外叉车操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Lang2Lift成功集成了视觉语言模型和基于基础模型的姿态估计，实现了无需特定任务训练数据即可在多样化环境条件下实时运行，适用于工业部署的室外机器人应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The logistics and construction industries face persistent challenges inautomating pallet handling, especially in outdoor environments with variablepayloads, inconsistencies in pallet quality and dimensions, and unstructuredsurroundings. In this paper, we tackle automation of a critical step in pallettransport: the pallet pick-up operation. Our work is motivated by laborshortages, safety concerns, and inefficiencies in manually locating andretrieving pallets under such conditions. We present Lang2Lift, a frameworkthat leverages foundation models for natural language-guided pallet detectionand 6D pose estimation, enabling operators to specify targets through intuitivecommands such as "pick up the steel beam pallet near the crane." The perceptionpipeline integrates Florence-2 and SAM-2 for language-grounded segmentationwith FoundationPose for robust pose estimation in cluttered, multi-palletoutdoor scenes under variable lighting. The resulting poses feed into a motionplanning module for fully autonomous forklift operation. We validate Lang2Lifton the ADAPT autonomous forklift platform, achieving 0.76 mIoU palletsegmentation accuracy on a real-world test dataset. Timing and error analysisdemonstrate the system's robustness and confirm its feasibility for deploymentin operational logistics and construction environments. Video demonstrationsare available at https://eric-nguyen1402.github.io/lang2lift.github.io/</description>
      <author>example@mail.com (Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi)</author>
      <guid isPermaLink="false">2508.15427v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
  <item>
      <title>A Bag of Tricks for Efficient Implicit Neural Point Clouds</title>
      <link>http://arxiv.org/abs/2508.19140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://fhahlbohm.github.io/inpc_v2/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一系列优化方法，显著提高了INPC的训练和推理性能，同时保持了视觉保真度，实现了更快的渲染速度和更低的内存使用。&lt;h4&gt;背景&lt;/h4&gt;INPC是一种结合神经场表现力和基于点渲染效率的混合表示方法，在新视角合成中实现了最先进的图像质量，但其实用性受限于渲染速度较慢的问题。&lt;h4&gt;目的&lt;/h4&gt;提高INPC的训练和推理性能，不牺牲视觉保真度，使其更具实用性。&lt;h4&gt;方法&lt;/h4&gt;改进的栅格化器实现、更有效的采样技术、集成用于孔洞填充的卷积神经网络预训练、在推理中将点建模为小高斯以提高外推质量，并通过一系列实验系统评估每个修改。&lt;h4&gt;主要发现&lt;/h4&gt;优化后的INPC流水线实现了最多25%更快的训练速度、2倍更快的渲染速度、20%更低的VRAM使用量，同时伴有轻微的图像质量提升。&lt;h4&gt;结论&lt;/h4&gt;通过一系列优化，成功解决了INPC的渲染速度瓶颈，使其更具实用性，同时保持了高质量的视觉表现。&lt;h4&gt;翻译&lt;/h4&gt;隐式神经点云（INPC）是一种结合神经场表现力和基于点渲染效率的混合表示方法，在新视角合成中实现了最先进的图像质量。然而，与其他在渲染过程中需要查询神经网络的高质量方法类似，INPC的实用性受到渲染速度相对较慢的限制。在这项工作中，我们提出了一系列优化，显著提高了INPC的训练和推理性能，同时不牺牲视觉保真度。最重要的改进包括改进的栅格化器实现、更有效的采样技术，以及用于孔洞填充的卷积神经网络的预训练集成。此外，我们证明在推理过程中可以将点建模为小高斯，以进一步提高外推质量，例如场景的特写视图。我们设计的实现具有广泛适用性，不仅限于INPC，并通过一系列实验系统评估了每个修改。我们的优化INPC流水线实现了最多25%更快的训练速度、2倍更快的渲染速度，以及20%更低的VRAM使用量，同时伴有轻微的图像质量改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Implicit Neural Point Clouds (INPC)方法在实际应用中的效率问题。原始INPC虽然能实现高质量的图像渲染，但渲染速度慢（每秒3-10帧），训练时间长，内存需求大，限制了其交互式应用和实际部署。这个问题在现实中很重要，因为高质量新视角合成技术对虚拟现实、增强现实等应用至关重要，而效率低下使得这些技术难以在实际场景中实时应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性分析原始INPC训练和推理管道中的性能瓶颈，设计了一系列针对性优化。他们针对采样效率、渲染效率和训练效率三个主要方面分别设计了改进方法。作者借鉴了多项现有工作，包括借鉴3D Gaussian Splatting的将点渲染为小高斯的思想，采用类似3DGS的瓦片渲染方法，以及使用技术来蒸馏背景模型为球面环境贴图。作者采用'一袋技巧'的方法，即组合多个小的但有效的改进，这些改进可以独立工作也可以协同工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一系列优化技术提高INPC的训练和推理效率而不牺牲图像质量。整体实现流程包括：1)改进的点云采样（视图特定采样使用环形缓冲区重用前一帧点云，全局预提取改进采样策略减少点数）；2)高效的特征图渲染（采用瓦片渲染降低排序复杂度，将MLP背景模型蒸馏为环境贴图，将点渲染为小高斯）；3)预训练的空洞填充CNN（三阶段预训练策略确保公共潜在空间）；4)实现改进（多个计算融合为单个CUDA内核，改进拒绝采样方法）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)改进的点云采样技术（时间稳定的视图特定采样，基于训练视图最大权重的改进全局预提取）；2)高效的特征图渲染（瓦片渲染方法，背景模型蒸馏，将点渲染为小高斯）；3)预训练的空洞填充CNN（三阶段预训练策略）；4)实现改进（计算融合，内存优化）。相比原始INPC，本文实现了渲染速度提高2倍，训练时间减少25%，内存使用降低20%，同时保持或提高图像质量；相比3DGS，在相同点数下质量更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过一系列优化技术显著提高了INPC的训练和推理效率，实现了渲染速度翻倍、训练时间减少25%、内存使用降低20%，同时保持了或略微提高了图像质量，使INPC在资源受限设备上成为3D高斯溅射的可行替代方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Implicit Neural Point Cloud (INPC) is a recent hybrid representation thatcombines the expressiveness of neural fields with the efficiency of point-basedrendering, achieving state-of-the-art image quality in novel view synthesis.However, as with other high-quality approaches that query neural networksduring rendering, the practical usability of INPC is limited by comparativelyslow rendering. In this work, we present a collection of optimizations thatsignificantly improve both the training and inference performance of INPCwithout sacrificing visual fidelity. The most significant modifications are animproved rasterizer implementation, more effective sampling techniques, and theincorporation of pre-training for the convolutional neural network used forhole-filling. Furthermore, we demonstrate that points can be modeled as smallGaussians during inference to further improve quality in extrapolated, e.g.,close-up views of the scene. We design our implementations to be broadlyapplicable beyond INPC and systematically evaluate each modification in aseries of experiments. Our optimized INPC pipeline achieves up to 25% fastertraining, 2x faster rendering, and 20% reduced VRAM usage paired with slightimage quality improvements.</description>
      <author>example@mail.com (Florian Hahlbohm, Linus Franke, Leon Overkämping, Paula Wespe, Susana Castillo, Martin Eisemann, Marcus Magnor)</author>
      <guid isPermaLink="false">2508.19140v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</title>
      <link>http://arxiv.org/abs/2508.19003v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 10 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RoofSeg的边缘感知的基于transformer的网络，用于从LiDAR点云中以真正的端到端方式分割屋顶平面，解决了现有深度学习方法中存在的三个关键问题。&lt;h4&gt;背景&lt;/h4&gt;屋顶平面分割是从机载激光雷达(LiDAR)点云重建细节级别(LoD)2和3的三维建筑模型的关键步骤。当前大多数方法依赖于手动设计或学习到的特征结合特定的几何聚类策略，基于深度学习的方法通常表现更好但存在三个未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于深度学习方法的三个问题：非真正端到端、边缘区域特征判别性低、平面几何特性未充分考虑，开发一种新颖的边缘感知的基于transformer的网络用于屋顶平面分割。&lt;h4&gt;方法&lt;/h4&gt;RoofSeg利用基于transformer的编码器-解码器框架，使用可学习的平面查询分层预测平面实例掩码；设计了边缘感知掩码模块(EAMM)增强边缘区域判别性；提出自适应加权策略减少误分类点影响；引入平面几何损失约束网络训练。&lt;h4&gt;主要发现&lt;/h4&gt;边缘区域的特征判别性对分割准确性至关重要；平面几何特性的考虑可以提高分割质量；端到端方法比非端到端方法能产生更优的分割结果。&lt;h4&gt;结论&lt;/h4&gt;RoofSeg通过分层预测、边缘感知掩码模块、自适应加权策略和平面几何损失，有效解决了现有深度学习方法的问题，提高了屋顶平面分割的准确性。&lt;h4&gt;翻译&lt;/h4&gt;屋顶平面分割是从机载激光雷达(LiDAR)点云重建细节级别(LoD)2和3的三维建筑模型的关键步骤之一。当前大多数用于屋顶平面分割的方法依赖于手动设计或学习到的特征，然后是一些特定的几何聚类策略。由于学习到的特征比手动设计的特征更强大，基于深度学习的方法通常比传统方法表现更好。然而，当前基于深度学习的方法有三个未解决的问题。第一，它们大多数不是真正的端到端，平面分割结果可能不是最优的。第二，点特征在边缘附近的判别性相对较低，导致平面边缘不准确。第三，平面几何特性没有被充分考虑来约束网络训练。为解决这些问题，开发了一种名为RoofSeg的新型边缘感知的基于transformer的网络，用于从LiDAR点云中以真正的端到端方式分割屋顶平面。在RoofSeg中，我们利用基于transformer的编码器-解码器框架，使用一组可学习的平面查询来分层预测平面实例掩码。为进一步提高边缘区域的分割准确性，我们还设计了一个边缘感知掩码模块(EAMM)，充分结合边缘的平面几何先验，增强平面实例掩码细化的判别性。此外，我们在掩码损失中提出了自适应加权策略，以减少误分类点的影响，并提出了一个新的平面几何损失来约束网络训练。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从机载LiDAR点云中进行屋顶平面分割的问题。现有方法存在三个关键问题：大多数方法不是真正的端到端处理、边缘区域特征区分度低导致边缘分割不准确、以及平面几何特性未被充分考虑来约束网络训练。这个问题在现实中非常重要，因为准确的屋顶平面分割是三维建筑物模型重建（特别是LoD 2和3级别）的关键步骤，能够显著提高最终3D重建的质量，而机载LiDAR获取的高精度点云是城市三维建模的重要数据源。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有屋顶平面分割方法的局限性，特别是深度学习方法中的三个未解决问题。受transformer在实例分割领域成功的启发，作者决定使用transformer架构来解决屋顶平面分割问题。为了解决边缘分割问题，作者设计了专门的Edge-Aware Mask Module (EAMM)，利用点到平面的距离作为关键几何线索。为了解决几何约束不足的问题，作者设计了新的损失函数。该方法借鉴了Mask2D和Mask3D等transformer实例分割架构，使用了PointNet++作为特征提取骨干并进行了改进，同时也参考了现有的边缘感知transformer工作，但认为其边缘感知机制相对隐式，进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用端到端的transformer架构直接从点云中预测屋顶平面的实例掩码，避免依赖参数敏感的中间处理步骤；设计边缘感知掩码模块（EAMM）利用点到平面的距离作为几何线索增强边缘特征区分度；以及提出自适应加权策略的掩码损失和平面几何损失，减少误分类点影响并确保高几何保真度。整体实现流程包括：1) 使用改进的PointNet++提取多尺度点特征；2) 通过最远点采样生成查询点并使用傅里叶编码生成初始查询嵌入；3) 使用查询细化解码器层次化整合多尺度信息；4) 生成初始平面掩码和边缘掩码；5) 使用EAMM进一步细化平面掩码；6) 根据语义分数合并正掩码生成最终结果；7) 使用包含自适应加权掩码损失、平面几何损失等的统一损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 真正端到端的边缘感知transformer网络(RoofSeg)，完全避免了参数敏感的聚类或后处理方法；2) 边缘感知掩码模块(EAMM)，整合关键几何先验知识增强边缘特征区分度；3) 新的损失函数，包括自适应加权掩码减少误分类点和平面几何损失确保高几何保真度。相比之前工作的不同：1) 完全端到端处理，而之前方法如DeepRoofPlane仍依赖后处理步骤；2) 显著提高边缘分割精度，解决了其他方法在边缘区域表现不佳的问题；3) 通过几何约束确保预测平面具有高几何保真度，这是大多数实例分割方法未充分考虑的；4) 有效减少预测掩码中的误分类点。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoofSeg提出了一种基于边缘感知transformer的端到端网络，通过创新的边缘掩码模块和自适应加权损失函数，实现了从机载LiDAR点云中高精度分割屋顶平面，显著提高了边缘分割准确性和几何保真度，同时减少了误分类点，达到了当前最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Roof plane segmentation is one of the key procedures for reconstructingthree-dimensional (3D) building models at levels of detail (LoD) 2 and 3 fromairborne light detection and ranging (LiDAR) point clouds. The majority ofcurrent approaches for roof plane segmentation rely on the manually designed orlearned features followed by some specifically designed geometric clusteringstrategies. Because the learned features are more powerful than the manuallydesigned features, the deep learning-based approaches usually perform betterthan the traditional approaches. However, the current deep learning-basedapproaches have three unsolved problems. The first is that most of them are nottruly end-to-end, the plane segmentation results may be not optimal. The secondis that the point feature discriminability near the edges is relatively low,leading to inaccurate planar edges. The third is that the planar geometriccharacteristics are not sufficiently considered to constrain the networktraining. To solve these issues, a novel edge-aware transformer-based network,named RoofSeg, is developed for segmenting roof planes from LiDAR point cloudsin a truly end-to-end manner. In the RoofSeg, we leverage a transformerencoder-decoder-based framework to hierarchically predict the plane instancemasks with the use of a set of learnable plane queries. To further improve thesegmentation accuracy of edge regions, we also design an Edge-Aware Mask Module(EAMM) that sufficiently incorporates planar geometric prior of edges toenhance its discriminability for plane instance mask refinement. In addition,we propose an adaptive weighting strategy in the mask loss to reduce theinfluence of misclassified points, and also propose a new plane geometric lossto constrain the network training.</description>
      <author>example@mail.com (Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li)</author>
      <guid isPermaLink="false">2508.19003v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Can we make NeRF-based visual localization privacy-preserving?</title>
      <link>http://arxiv.org/abs/2508.18971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了基于NeRF的视觉定位方法中的隐私问题，提出了一种隐私评估协议和ppNeSF方法，实现了隐私保护的视觉定位。&lt;h4&gt;背景&lt;/h4&gt;视觉定位(VL)是估计相机在已知场景中姿态的任务。NeRF-based方法虽能提供高质量的新视角合成，但会编码精细场景细节，在云定位服务中部署时可能引发隐私泄露风险。&lt;h4&gt;目的&lt;/h4&gt;解决基于NeRF的视觉定位方法中的隐私问题，防止敏感信息在云服务中被恢复。&lt;h4&gt;方法&lt;/h4&gt;1) 提出评估NeRF表示隐私保护能力的新协议；2) 开发ppNeSF(Privacy-Preserving Neural Segmentation Field)，一种使用分割监督而非RGB图像训练的NeRF变体；3) 通过自监督方式学习分割标签，确保其足够粗糙以掩盖可识别场景细节，同时保持3D区分性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用光度损失训练的NeRF在其几何表示中存储细粒度细节，即使移除颜色预测头部也容易受到隐私攻击；2) ppNeSF的分割空间可用于准确的视觉定位，取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;通过ppNeSF方法，可以在保持视觉定位准确性的同时有效保护场景隐私，防止敏感信息被提取。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位(VL)是估计相机在已知场景中姿态的任务。VL方法可根据它们如何表示场景来区分，例如通过（稀疏）点云或图像集合显式表示，或通过神经网络的权重隐式表示。最近，基于NeRF的方法在视觉定位中变得流行。虽然NeRF提供高质量的新视角合成，但它们无意中编码精细场景细节，在基于云的定位服务中部署时引发隐私问题，因为敏感信息可能被恢复。在本文中，我们从两方面解决这一挑战。首先，我们提出新协议评估基于NeRF表示的隐私保护能力。我们证明，使用光度损失训练的NeRF在其几何表示中存储细粒度细节，使它们容易受到隐私攻击，即使移除预测颜色的头部。其次，我们提出ppNeRF(隐私保护神经分割场)，一种使用分割监督而非RGB图像训练的NeRF变体。这些分割标签以自监督方式学习，确保它们足够粗糙以掩盖可识别场景细节，同时在3D中保持区分性。ppNeSF的分割空间可用于准确视觉定位，取得最先进结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于神经辐射场(NeRF)的视觉定位方法在云服务部署中可能泄露隐私信息的问题。当NeRF模型存储场景的精细细节时，可能会包含敏感的个人信息（如纹理、文本等）。这个问题很重要，因为随着自动驾驶和自主机器人的普及，视觉定位成为关键技术，而大规模视觉定位服务通常部署在云端，隐私保护成为关键需求。即使移除颜色预测头，NeRF的几何部分仍然可能存储这些敏感信息，因此需要专门的隐私保护方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了NeRF在隐私方面的脆弱性，设计了反转攻击方法来评估隐私保护程度，并使用视觉语言模型进行更全面的评估。他们发现即使移除RGB预测头，几何部分仍然存储敏感信息。受现有工作启发，特别是使用分割标签保护隐私的方法，作者提出了ppNeSF。他们借鉴了[58,60]中用分割标签替换高维描述符防止反转攻击的思想，以及SegLoc和GSFF等使用分割图对齐的定位策略，同时结合了神经隐式场的工作如iNeRF和NeFeS，创造了一种基于分割监督的NeRF变体。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用分割标签代替RGB图像作为监督信号，避免模型直接接触潜在的隐私信息；通过自监督学习鲁棒和有区分度的分割标签；建立统一的分割空间对齐3D场景表示与2D图像编码器；通过分割图对齐实现准确的视觉定位。整体流程包括：1)架构设计（几何场、分割场、图像编码器和特征场）；2)自监督训练（学习特征嵌入空间、维护原型、优化交叉熵损失）；3)正则化（分层分割和不确定性建模）；4)视觉定位（估计初始姿态、渲染分割标签、提取2D分割、最小化交叉熵、迭代优化）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统分析神经隐式场在隐私方面的脆弱性，提出新的隐私攻击和评估协议；2)提出ppNeSF，第一个基于神经隐式场的隐私保护视觉定位方法；3)分层分割方案，提供粗细两级监督；4)不确定性建模，对模糊像素降权。相比之前工作，ppNeSF不使用光度损失避免存储纹理信息；基于神经隐式场而非显式3D基元；分割标签在3D和2D空间中对齐；通过自监督学习联合特征空间而非依赖预训练编码器。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ppNeSF，一种基于分割监督的隐私保护神经隐式场方法，能够在保持高精度视觉定位的同时有效防止敏感信息泄露，解决了基于NeRF的视觉定位在云服务中的隐私保护问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization (VL) is the task of estimating the camera pose in a knownscene. VL methods, a.o., can be distinguished based on how they represent thescene, e.g., explicitly through a (sparse) point cloud or a collection ofimages or implicitly through the weights of a neural network. Recently,NeRF-based methods have become popular for VL. While NeRFs offer high-qualitynovel view synthesis, they inadvertently encode fine scene details, raisingprivacy concerns when deployed in cloud-based localization services assensitive information could be recovered. In this paper, we tackle thischallenge on two ends. We first propose a new protocol to assessprivacy-preservation of NeRF-based representations. We show that NeRFs trainedwith photometric losses store fine-grained details in their geometryrepresentations, making them vulnerable to privacy attacks, even if the headthat predicts colors is removed. Second, we propose ppNeSF (Privacy-PreservingNeural Segmentation Field), a NeRF variant trained with segmentationsupervision instead of RGB images. These segmentation labels are learned in aself-supervised manner, ensuring they are coarse enough to obscure identifiablescene details while remaining discriminativeness in 3D. The segmentation spaceof ppNeSF can be used for accurate visual localization, yieldingstate-of-the-art results.</description>
      <author>example@mail.com (Maxime Pietrantoni, Martin Humenberger, Torsten Sattler, Gabriela Csurka)</author>
      <guid isPermaLink="false">2508.18971v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings</title>
      <link>http://arxiv.org/abs/2508.18733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从二维工程图自动生成参数化CAD模型的方法，解决了CAD生成领域的一个重要研究空白。&lt;h4&gt;背景&lt;/h4&gt;CAD生成建模在工业应用中推动重大创新，现有方法能从点云、网格和文本描述创建实体模型，但这些方法与传统从二维工程图开始的工业工作流程存在根本差异。&lt;h4&gt;目的&lt;/h4&gt;解决从二维矢量工程图自动生成参数化CAD模型这一未被充分探索的关键问题，使其符合传统工程设计流程。&lt;h4&gt;方法&lt;/h4&gt;提出Drawing2CAD框架，将CAD生视为序列到序列学习问题，包含三个技术组件：网络友好的矢量原语表示、双解码器Transformer架构和软目标分布损失函数，并创建了CAD-VGDrawing数据集进行训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;序列到序列学习框架能有效从二维图生成参数化CAD模型，三个技术组件共同保留了几何精度和设计意图，同时适应了CAD参数的固有灵活性。&lt;h4&gt;结论&lt;/h4&gt;Drawing2CAD方法成功实现了从二维工程图到参数化CAD模型的自动转换，代码和数据集已公开，为工程设计流程提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;计算机辅助设计（CAD）生成建模正在推动工业应用领域的重大创新。最近的工作在从点云、网格和文本描述等多种输入创建实体模型方面取得了显著进展。然而，这些方法从根本上与传统工业工作流程不同，传统工作流程从二维工程图开始。从这些二维矢量图自动生成参数化CAD模型仍然是一个未被充分探索的领域，尽管它是工程设计中的一个关键步骤。为解决这一差距，我们的关键见解是将CAD生重新构建为序列到序列学习问题，其中矢量图原语直接告知参数化CAD操作的生成，在整个转换过程中保持几何精度和设计意图。我们提出Drawing2CAD，这是一个包含三个关键技术组件的框架：一种网络友好的矢量原语表示，保留精确的几何信息；一个双解码器Transformer架构，将命令类型和参数生成解耦，同时保持精确对应；以及一个适应CAD参数固有灵活性的软目标分布损失函数。为训练和评估Drawing2CAD，我们创建了CAD-VGDrawing数据集，包含配对的工程图和参数化CAD模型，并进行彻底的实验以证明我们方法的有效性。代码和数据集可在https://github.com/lllssc/Drawing2CAD获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-Aided Design (CAD) generative modeling is driving significantinnovations across industrial applications. Recent works have shown remarkableprogress in creating solid models from various inputs such as point clouds,meshes, and text descriptions. However, these methods fundamentally divergefrom traditional industrial workflows that begin with 2D engineering drawings.The automatic generation of parametric CAD models from these 2D vector drawingsremains underexplored despite being a critical step in engineering design. Toaddress this gap, our key insight is to reframe CAD generation as asequence-to-sequence learning problem where vector drawing primitives directlyinform the generation of parametric CAD operations, preserving geometricprecision and design intent throughout the transformation process. We proposeDrawing2CAD, a framework with three key technical components: anetwork-friendly vector primitive representation that preserves precisegeometric information, a dual-decoder transformer architecture that decouplescommand type and parameter generation while maintaining precise correspondence,and a soft target distribution loss function accommodating inherent flexibilityin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,a dataset of paired engineering drawings and parametric CAD models, and conductthorough experiments to demonstrate the effectiveness of our method. Code anddataset are available at https://github.com/lllssc/Drawing2CAD.</description>
      <author>example@mail.com (Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu)</author>
      <guid isPermaLink="false">2508.18733v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</title>
      <link>http://arxiv.org/abs/2508.17969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种自动驾驶车辆的内外部整体感知系统，包括内部监控和外部监控两个子系统，旨在优化车辆感知能力和乘客体验。&lt;h4&gt;背景&lt;/h4&gt;该研究在欧盟地平线欧洲计划AutoTRUST的背景下进行，专注于开发先进的车辆感知技术。&lt;h4&gt;目的&lt;/h4&gt;展示一种利用人工智能的自适应框架，优化自动驾驶车辆的感知能力和乘客体验。&lt;h4&gt;方法&lt;/h4&gt;内部监控系统使用多摄像头进行面部识别，集成大型语言模型作为虚拟助手，并部署AI智能传感器监测空气质量和热舒适度；外部监控系统采用基于LiDAR的成本效益高的语义分割方法，对低质量原始3D点云进行超分辨率处理；系统已集成到ALKE提供的真实电动汽车上。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证表明，所提出的感知架构的模块化块在性能和效率方面都有显著提高。&lt;h4&gt;结论&lt;/h4&gt;该整体感知系统成功实现了自动驾驶车辆内外部环境的全面监控，为先进车辆技术提供了创新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种用于自动驾驶车辆内外部监控的整体感知系统，旨在展示一种新颖的、利用人工智能的自适应先进车辆技术和解决方案框架，优化车内感知和体验。内部监控系统依赖多摄像头设置，通过面部识别预测和识别驾驶员和乘员行为，同时利用大型语言模型作为虚拟助手。此外，舱内监控系统包括由人工智能驱动的智能传感器，用于测量空气质量并进行热舒适度分析，以实现高效的上下车。另一方面，外部监控系统通过基于LiDAR的成本效益高的语义分割方法来感知车辆周围环境，该方法对低质量的原始3D点云执行高精度和高效的超分辨率。整体感知框架是在欧盟地平线欧洲计划AutoTRUST的背景下开发的，并已集成并部署在ALKE提供的真实电动汽车上。在意大利Ispra的联合研究中心集成站点进行的实验验证和评估，强调了所提出的感知架构模块化块的性能和效率提高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶汽车内部和外部环境的全面感知问题。内部感知需要监控驾驶员行为、乘客情绪、身份识别及车内环境质量；外部感知需要高效理解车辆周围环境，特别是使用低成本LiDAR进行准确环境分割。这个问题在现实中至关重要，因为它关系到自动驾驶汽车的安全性、乘客舒适度和个性化体验，同时能有效应对当前系统在实时处理、集成可行性和嵌入式部署方面的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从自动驾驶汽车需要全面感知系统的需求出发，分别针对内部和外部感知进行了设计。内部感知结合多摄像头、面部识别、情绪分析和环境传感器；外部感知专注于低成本LiDAR的超分辨率处理。该方法借鉴了多项现有工作：使用轻量级CNN和transformer模型进行驾驶舱活动识别，采用高效声音事件检测支持安全，利用智能环境监测系统，以及应用超分辨率技术增强LiDAR数据。同时，作者将这些现有技术进行了优化和适配，形成一个统一的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是开发一个统一的、可部署的框架，集成驾驶舱情感感知、虚拟助手、环境监控和高效的LiDAR超分辨率处理。内部感知系统包括：1)驾驶员行为监控(89%准确率的分心检测)，2)情绪识别(72.9%准确率)，3)乘客识别(96.5%准确率)，4)物体检测(92.3% mAP)，5)声音事件检测(94%准确率)，6)空气质量监测。外部感知系统使用16通道LiDAR获取原始点云，应用超分辨率模块提升质量，然后进行语义分割。虚拟助手基于量化Llama 3模型，结合语音识别和合成，提供实时上下文感知响应。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的整体感知框架，整合内部和外部感知；2)先进的内部感知系统，结合情绪识别、虚拟助手和空气质量监测；3)创新的外部感知系统，使用基于模型的LiDAR超分辨率网络提高低成本传感器的性能；4)虚拟助手集成，在嵌入式设备上高效部署大语言模型。相比之前工作，不同之处在于：大多数现有解决方案专注于单一方面的感知，而本文提供统一框架；系统针对嵌入式平台优化实现实时性能；超分辨率方法使低成本LiDAR能提供高质量感知；虚拟助手提供实时上下文感知交互，这在自动驾驶系统中较少见。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个统一的AutoTRUST感知范式，通过集成先进的内部监控和高效的低成本LiDAR外部感知系统，显著提高了自动驾驶汽车的安全性、舒适性和环境感知能力，并在真实电动汽车平台上成功验证了其实时性能和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a holistic perception system for internal and externalmonitoring of autonomous vehicles, with the aim of demonstrating a novelAI-leveraged self-adaptive framework of advanced vehicle technologies andsolutions that optimize perception and experience on-board. Internal monitoringsystem relies on a multi-camera setup designed for predicting and identifyingdriver and occupant behavior through facial recognition, exploiting in additiona large language model as virtual assistant. Moreover, the in-cabin monitoringsystem includes AI-empowered smart sensors that measure air-quality and performthermal comfort analysis for efficient on and off-boarding. On the other hand,external monitoring system perceives the surrounding environment of vehicle,through a LiDAR-based cost-efficient semantic segmentation approach, thatperforms highly accurate and efficient super-resolution on low-quality raw 3Dpoint clouds. The holistic perception framework is developed in the context ofEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed ona real electric vehicle provided by ALKE. Experimental validation andevaluation at the integration site of Joint Research Centre at Ispra, Italy,highlights increased performance and efficiency of the modular blocks of theproposed perception architecture.</description>
      <author>example@mail.com (Alexandros Gkillas, Christos Anagnostopoulos, Nikos Piperigkos, Dimitris Tsiktsiris, Theofilos Christodoulou, Theofanis Siamatras, Dimitrios Triantafyllou, Christos Basdekis, Theoktisti Marinopoulou, Panagiotis Lepentsiotis, Elefterios Blitsis, Aggeliki Zacharaki, Nearchos Stylianidis, Leonidas Katelaris, Lamberto Salvan, Aris S. Lalos, Christos Laoudias, Antonios Lalas, Konstantinos Votis)</author>
      <guid isPermaLink="false">2508.17969v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking the Detail-Preserved Completion of Complex Tubular Structures based on Point Cloud: a Dataset and a Benchmark</title>
      <link>http://arxiv.org/abs/2508.17658v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究首次探索了基于点云的管状结构补全方法，建立了PC-CAC数据集，并提出了TSRNet网络，有效解决了医学成像中管状结构不连续的问题。&lt;h4&gt;背景&lt;/h4&gt;复杂管状结构在医学成像和计算机辅助诊断中至关重要，其完整性有助于解剖可视化和病变检测。然而现有分割算法难以处理结构不连续问题，特别是在冠状动脉狭窄和血管闭塞等严重临床案例中，导致诊断准确性下降。&lt;h4&gt;目的&lt;/h4&gt;重新连接不连续的管状结构以确保其完整性，并探索基于点云的管状结构补全方法。&lt;h4&gt;方法&lt;/h4&gt;建立了基于点云的冠状动脉补全(PC-CAC)数据集，来源于真实临床数据；提出了TSRNet（管状结构重连接网络），该网络集成了细节保留特征提取器、多重密集细化策略和全局到局部损失函数，确保准确重新连接的同时保持结构完整性。&lt;h4&gt;主要发现&lt;/h4&gt;在PC-CAC和另外两个公共数据集（PC-ImageCAS和PC-PTR）上的综合实验表明，该方法在多个评估指标上始终优于最先进的方法，为基于点云的管状结构重建设定了新的基准。&lt;h4&gt;结论&lt;/h4&gt;提出的TSRNet方法和PC-CAC数据集为点云基础的管状结构重建建立了新的基准，可通过https://github.com/YaoleiQi/PCCAC获取。&lt;h4&gt;翻译&lt;/h4&gt;复杂的管状结构在医学成像和计算机辅助诊断中必不可少，它们的完整性增强了解剖可视化和病变检测。然而，现有的分割算法难以处理结构不连续问题，特别是在冠状动脉狭窄和血管闭塞等严重临床案例中，这导致不期望的不连续性并影响下游诊断的准确性。因此，必须重新连接不连续结构以确保其完整性。在本研究中，我们首次探索了基于点云的管状结构补全，并建立了基于点云的冠状动脉补全(PC-CAC)数据集，该数据集来源于真实临床数据。该数据集为管状结构补全提供了新的基准。此外，我们提出了TSRNet，一个管状结构重连接网络，它集成了细节保留特征提取器、多重密集细化策略和全局到局部损失函数，以确保准确重新连接的同时保持结构完整性。在我们的PC-CAC和另外两个公共数据集（PC-ImageCAS和PC-PTR）上的综合实验表明，我们的方法在多个评估指标上始终优于最先进的方法，为基于点云的管状结构重建设定了新的基准。我们的基准可在https://github.com/YaoleiQi/PCCAC获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决复杂管状结构（如血管）在医学成像中的不连续性问题。在临床应用中，如冠状动脉狭窄、血管闭塞等情况下，现有分割算法常产生结构断裂，影响解剖可视化和病变检测，进而降低下游诊断准确性。这个问题在医学影像分析中至关重要，因为血管等管状结构的完整性对于准确诊断和治疗决策具有重要意义，特别是在严重临床病例中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有基于体素(voxel)方法的局限性出发，认识到在处理模糊图像、完全闭塞等复杂场景时，体素方法难以保持结构完整性。因此转向点云(point cloud)表示，因其具有计算效率和准确性优势。在设计方法时，作者借鉴了PointNet/PointNet++的点云处理基础架构，结合集合抽象(Set Abstraction)和Transformer思想提出TransSA模块，同时参考了随机行走等传统方法但进行了改进。针对点分布不均衡和复杂全局拓扑两大挑战，作者设计了核心点选择模块和多密集细化策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于点云的管状结构重建方法，通过细节保留的特征提取、多密集细粒度策略和全局到局部的损失函数，确保准确重建同时保持结构完整性，特别关注细长管状结构的重建。整体流程包括：1)细节保留特征提取器：通过核心点选择模块(CPS)分析点云密度分布，分离密集和稀疏区域，结合TransSA模块提取特征；2)多密集细粒度策略：设计多个细化模块逐步重建核心点云为粗略点云，再细化为密集点云；3)全局到局部损失函数：结合Chamfer Distance和Fidelity Error，使用阈值控制训练阶段，先学习全局结构一致性，再细化局部细节。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建首个基于点云的冠状动脉完成(PC-CAC)数据集，来源于真实临床数据，包含分段等不完美之处；2)首次从点云角度重新定义管状结构重建任务；3)提出管状结构重建网络(TSRNet)，包含细节保留特征提取器、多密集细粒度策略和全局到局部损失函数；4)在三个数据集上提供标准化评估基准。相比之前工作，本文从体素方法转向点云方法，专门针对细长管状结构设计，解决了点分布不均衡和复杂全局拓扑挑战，提供了专门的数据集和评估基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出了基于点云的复杂管状结构重建方法，通过构建专门的PC-CAC数据集和TSRNet网络，有效解决了医学成像中血管等管状结构的不连续问题，显著提高了诊断准确性和下游任务效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex tubular structures are essential in medical imaging andcomputer-assisted diagnosis, where their integrity enhances anatomicalvisualization and lesion detection. However, existing segmentation algorithmsstruggle with structural discontinuities, particularly in severe clinical casessuch as coronary artery stenosis and vessel occlusions, which leads toundesired discontinuity and compromising downstream diagnostic accuracy.Therefore, it is imperative to reconnect discontinuous structures to ensuretheir completeness. In this study, we explore the tubular structure completionbased on point cloud for the first time and establish a Point Cloud-basedCoronary Artery Completion (PC-CAC) dataset, which is derived from realclinical data. This dataset provides a novel benchmark for tubular structurecompletion. Additionally, we propose TSRNet, a Tubular Structure ReconnectionNetwork that integrates a detail-preservated feature extractor, a multipledense refinement strategy, and a global-to-local loss function to ensureaccurate reconnection while maintaining structural integrity. Comprehensiveexperiments on our PC-CAC and two additional public datasets (PC-ImageCAS andPC-PTR) demonstrate that our method consistently outperforms state-of-the-artapproaches across multiple evaluation metrics, setting a new benchmark forpoint cloud-based tubular structure reconstruction. Our benchmark is availableat https://github.com/YaoleiQi/PCCAC.</description>
      <author>example@mail.com (Yaolei Qi, Yikai Yang, Wenbo Peng, Shumei Miao, Yutao Hu, Guanyu Yang)</author>
      <guid isPermaLink="false">2508.17658v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes</title>
      <link>http://arxiv.org/abs/2508.17634v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv Preprint, paper has since been accepted to ACPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于重建的户外场景开放集分割新方法，结合物体缺陷检测研究和Mamba架构的优势，显著提升了分割性能，并贡献了一种在大型点云上具有竞争力的Mamba架构。&lt;h4&gt;背景&lt;/h4&gt;激光雷达扫描在户外场景中获得大范围内的精确距离测量，产生大规模点云，应用于机器人、汽车和土地监测等领域。在这些应用中，不可避免会出现训练数据之外的异常物体。&lt;h4&gt;目的&lt;/h4&gt;开发一种开放集分割的新方法，有效处理训练数据之外的异常物体，提高户外场景点云分割的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;结合物体缺陷检测研究的经验和Mamba架构在利用长距离依赖性和处理大规模数据方面的优势，创建了一种基于重建的户外场景开放集分割方法。&lt;h4&gt;主要发现&lt;/h4&gt;1. 该方法应用于作者自己的开放集分割方法时提高了性能；2. 该方法应用于现有方法时也提高了性能；3. 作者贡献的基于Mamba的架构在具有挑战性的大规模点云上与现有的基于体素卷积的方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;提出的基于重建的开放集分割方法和基于Mamba的架构在户外场景点云分割任务中表现出色，特别是在处理大规模点云和识别训练数据之外的异常物体方面。&lt;h4&gt;翻译&lt;/h4&gt;户外场景中的激光雷达扫描可获得大范围内的精确距离测量，产生大规模点云。此类数据的应用示例包括机器人、汽车和土地监测。在这些应用中，不可避免会出现训练数据之外的异常物体。我们的研究贡献了一种开放集分割的新方法，利用了物体缺陷检测研究的经验。我们还借鉴了Mamba架构在利用长距离依赖性和处理大规模数据方面的强大性能。结合两者，我们为户外场景开放集分割任务创建了一种基于重建的方法。我们证明，当应用于我们自己的开放集分割方法时，我们的方法提高了性能，当应用于现有方法时也是如此。此外，我们还贡献了一种基于Mamba的架构，在具有挑战性的大规模点云上与现有的基于体素卷积的方法具有竞争力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大规模点云场景中的异常物体检测问题。在户外场景的LiDAR扫描应用中（如自动驾驶、机器人导航和土地监测），总会出现训练数据中没有的未知物体。准确识别这些异常物体对于确保系统在实际应用中的可靠性和安全性至关重要，因为无法识别的异常可能导致系统决策错误或安全隐患。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将两个现有研究方向结合：单物体缺陷检测和场景级异常检测，定义为'识别场景中的未知异常'。他们借鉴了物体缺陷检测中的重建方法，以及Mamba架构在处理长距离依赖关系和大规模数据方面的优势。作者还参考了Cen等人创建合成异常的方法，但提出了改进的'魔方'方法来生成更有效的训练数据。整体设计思路是通过训练只使用已知物体的重建系统，使其在遇到未知异常时产生明显差异，从而实现异常检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过训练一个场景重建器，使其只学习已知物体的'默认上下文'，当遇到未知异常物体时，重建会产生明显误差，从而帮助识别这些异常。整体流程包括：1) 使用Mamba编码器-解码器架构训练场景重建器，仅用已知物体数据；2) 计算原始点云与重建点云之间的差异；3) 使用这些差异作为输入，通过基于Mamba的异常检测架构识别异常；4) 同时进行已知物体的语义分割。训练过程中使用创新的'魔方'方法创建合成异常数据，避免简单缩放带来的问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 将单物体缺陷检测中的重建方法首次应用于大规模场景级异常检测；2) 首次将Mamba架构引入点云场景异常检测，利用其处理长距离依赖和大规模数据的能力；3) 提出新的'魔方'方法创建合成异常，避免简单缩放导致的点密度差异问题；4) 使用相对坐标重建而非全局坐标重建，使错误表现为表面变形更易识别。相比之前工作，本文直接编码解码原始输入而非依赖预测语义标签，使用Mamba而非体素卷积作为骨干网络，并提出了更有效的异常数据生成方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合Mamba架构和场景重建的创新方法，显著提升了大规模点云场景中未知异常物体的检测性能，为户外场景的可靠应用提供了重要技术支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR scanning in outdoor scenes acquires accurate distance measurements overwide areas, producing large-scale point clouds. Application examples for thisdata include robotics, automotive vehicles, and land surveillance. During suchapplications, outlier objects from outside the training data will inevitablyappear. Our research contributes a novel approach to open-set segmentation,leveraging the learnings of object defect-detection research. We also draw onthe Mamba architecture's strong performance in utilising long-rangedependencies and scalability to large data. Combining both, we create areconstruction based approach for the task of outdoor scene open-setsegmentation. We show that our approach improves performance not only whenapplied to our our own open-set segmentation method, but also when applied toexisting methods. Furthermore we contribute a Mamba based architecture which iscompetitive with existing voxel-convolution based methods on challenging,large-scale pointclouds.</description>
      <author>example@mail.com (Ryan Faulkner, Luke Haub, Simon Ratcliffe, Tat-Jun Chin)</author>
      <guid isPermaLink="false">2508.17634v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Variational Shape Inference for Grasp Diffusion on SE(3)</title>
      <link>http://arxiv.org/abs/2508.17482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于变分形状推断的多模态抓取合成框架，利用隐式神经表示和扩散模型在SE(3)流形上生成多样化的稳定抓取，并引入了测试时抓取优化技术。&lt;h4&gt;背景&lt;/h4&gt;抓取合成是机器人操作的基础任务，通常有多个可行解决方案。多模态抓取合成旨在根据物体几何形状生成多样化的稳定抓取集合，而几何特征的鲁棒学习对成功至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决形状噪声和测量稀疏性带来的挑战，提出一种能够学习多模态抓取分布的框架，增强对形状噪声和测量稀疏性的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;首先训练一个使用隐式神经表示的变分自编码器进行形状推断，然后利用这些学习的几何特征引导在SE(3)流形上的抓取合成扩散模型，并引入测试时抓取优化技术作为插件进一步提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;在ACRONYM数据集上，该方法比最先进的多模态抓取合成方法高出6.3%，对点云密度下降表现出鲁棒性，并能实现无样本迁移到家用物体的现实操作，尽管存在测量噪声和点云校准误差，仍比基线方法多产生34%的成功抓取。&lt;h4&gt;结论&lt;/h4&gt;该形状推断抓取合成方法在性能和鲁棒性方面均优于现有方法，能够有效处理形状噪声和测量稀疏性问题，并成功迁移到现实世界应用中。&lt;h4&gt;翻译&lt;/h4&gt;抓取合成是机器人操作中的一个基础任务，通常有多个可行的解决方案。多模态抓取合成旨在根据物体几何形状生成多样化的稳定抓取集合，这使得几何特征的鲁棒学习对成功至关重要。为了应对这一挑战，我们提出了一个学习多模态抓取分布的框架，利用变分形状推断来增强对形状噪声和测量稀疏性的鲁棒性。我们的方法首先使用隐式神经表示训练一个用于形状推断的变分自编码器，然后使用这些学习的几何特征来引导在SE(3)流形上的抓取合成扩散模型。此外，我们引入了一种测试时抓取优化技术，可以作为插件集成以进一步提高抓取性能。实验结果表明，我们的形状推断抓取合成方法在ACRONYM数据集上比最先进的多模态抓取合成方法高出6.3%，同时与其他方法相比，对点云密度下降表现出鲁棒性。此外，我们的训练模型实现了对家用物体现实操作的无样本迁移，尽管存在测量噪声和点云校准误差，但仍比基线方法多产生34%的成功抓取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人抓取任务中的多模态抓取合成问题，即在给定物体几何形状时生成多种多样的稳定抓取方案。这个问题在现实中非常重要，因为自主机器人在非结构化环境中需要能够稳健地与各种物体交互，而现实中的感知系统往往会产生不完整或带有噪声的点云数据，现有方法难以同时保证抓取的多样性、稳定性和对噪声/不完整数据的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从几何特征理解的重要性出发，认识到高质量抓取需要能跨物体形状转移且对不完美观测鲁棒的形状特征。他们借鉴了变分自编码器(VAE)进行形状推断，特别是Chou等人的PointVAE架构；采用扩散模型(特别是基于分数匹配的扩散模型)来学习抓取分布，参考了Urain等人的SE(3)-Diff方法；同时利用符号距离场(SDF)和隐式神经表示(INR)来表示几何形状。作者设计了一个两阶段流程：先学习鲁棒的形状特征，再使用这些特征指导抓取合成，并额外引入了测试时的抓取优化技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习一个对噪声和稀疏观测具有鲁棒性的形状特征空间，然后利用这些特征指导抓取姿势的生成，同时通过测试时优化进一步提高抓取质量。整体流程分为三部分：1)形状推断阶段：使用PointVAE架构将输入点云编码为潜在表示，并重建SDF；2)抓取合成阶段：使用去噪分数匹配在SE(3)流形上学习抓取分布，通过反向Langevin动力学生成抓取样本；3)测试时优化阶段：学习夹爪的神经SDF，构建可微分的碰撞避免和捏取稳定性目标，在最后阶段优化生成的抓取姿势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)变分形状推断：使用变分自编码器学习结构化的形状潜在空间，增强对噪声和稀疏点云的鲁棒性；2)两阶段框架：先学习形状特征再指导抓取合成，使形状特征可独立优化；3)测试时抓取优化：提出可微分优化技术，结合碰撞避免和捏取稳定性目标；4)能量模型 formulation：将分数函数建模为能量场的负梯度，便于与其他目标组合。相比之前工作，与cVAE相比避免了模式崩溃且无需额外分类器；与SE(3)-Diff相比使用更强大的变分形状推断；与Bridger相比形状推断更鲁棒；与GLDM相比直接在原始空间工作而非学习抓取潜在空间。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合变分形状推断和扩散模型的框架，通过学习对噪声和稀疏观测鲁棒的几何特征，实现了在SE(3)流形上的多模态抓取合成，并引入测试时优化技术，在模拟和真实世界场景中均展现出优越的性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasp synthesis is a fundamental task in robotic manipulation which usuallyhas multiple feasible solutions. Multimodal grasp synthesis seeks to generatediverse sets of stable grasps conditioned on object geometry, making the robustlearning of geometric features crucial for success. To address this challenge,we propose a framework for learning multimodal grasp distributions thatleverages variational shape inference to enhance robustness against shape noiseand measurement sparsity. Our approach first trains a variational autoencoderfor shape inference using implicit neural representations, and then uses theselearned geometric features to guide a diffusion model for grasp synthesis onthe SE(3) manifold. Additionally, we introduce a test-time grasp optimizationtechnique that can be integrated as a plugin to further enhance graspingperformance. Experimental results demonstrate that our shape inference forgrasp synthesis formulation outperforms state-of-the-art multimodal graspsynthesis methods on the ACRONYM dataset by 6.3%, while demonstratingrobustness to deterioration in point cloud density compared to otherapproaches. Furthermore, our trained model achieves zero-shot transfer toreal-world manipulation of household objects, generating 34% more successfulgrasps than baselines despite measurement noise and point cloud calibrationerrors.</description>
      <author>example@mail.com (S. Talha Bukhari, Kaivalya Agrawal, Zachary Kingston, Aniket Bera)</author>
      <guid isPermaLink="false">2508.17482v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search</title>
      <link>http://arxiv.org/abs/2508.17427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于几何最大重叠的配准框架，通过仅旋转的分支限界搜索实现点云配准，解决了现有方法在高异常值比例下的配准问题，同时降低了计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的点云配准方法主要使用空间兼容性图或分支限界搜索，主要针对高异常值比例下的配准问题。然而，基于图的方法需要至少二次方的时间和空间复杂度，而多阶段分支限界搜索方法常因分解阶段间的局部最优问题导致不准确。&lt;h4&gt;目的&lt;/h4&gt;开发一种点云配准方法，能够在高异常值比例下实现准确配准，同时降低计算复杂度，提高效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种几何最大重叠配准框架，使用Chasles定理将刚体变换分解为沿旋转轴的平移和2D刚体变换。通过分支限界搜索最优旋转轴和角度，将剩余参数表述为范围最大查询问题。使用立方映射参数化的半球内搜索候选旋转轴，并将2D配准放松为1D旋转角度搜索，使用扫描线算法和线段树在多项式时间内解决。&lt;h4&gt;主要发现&lt;/h4&gt;在3DMatch、3DLoMatch和KITTI数据集上的实验结果表明，该方法比现有最先进方法具有更高的准确性和效率，时间复杂度为多项式，空间复杂度随点数线性增加，即使在最坏情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;所提出的几何最大重叠配准框架有效解决了高异常值比例下的点云配准问题，在保持高精度的同时显著降低了计算复杂度，为点云配准领域提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于对应关系的点云配准计算在噪声阈值约束下最大化内点数量的刚体变换。当前最先进方法采用空间兼容性图或分支限界搜索，主要关注高异常值比例下的配准。然而，基于图的方法需要至少二次方的时间和空间复杂度来构建图，而多阶段分支限界搜索方法常因分解阶段间的局部最优问题导致不准确。本文提出了一种通过仅旋转分支限界搜索的几何最大重叠配准框架。使用Chasles定理将刚体变换分解为沿旋转轴的平移和2D刚体变换。通过分支限界搜索最优旋转轴和角度，剩余参数表述为范围最大查询问题。首先，在立方映射参数化的半球内搜索前k个候选旋转轴，并通过将对应点投影到该轴上的区间刺穿来估计沿每个轴的平移。其次，将2D配准放松为使用轴对齐矩形的2D几何重叠的1D旋转角度搜索，使用扫描线算法和线段树在多项式时间内确定性解决。在3DMatch、3DLoMatch和KITTI数据集上的实验结果表明，该方法比最先进方法具有更高的准确性和效率，时间复杂度为多项式，空间复杂度随点数线性增加，即使在最坏情况下也是如此。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中的鲁棒性问题，特别是在高异常值比例下的配准挑战。这个问题在SLAM、计算机辅助手术和增强现实等领域至关重要，因为准确的点云配准是这些应用的基础，而现实场景中往往存在大量异常值，影响配准精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：图方法需要二次的时间和空间复杂度，多阶段BnB方法容易陷入局部最优。他们借鉴了Chasles'定理分解刚体变换，结合分支定界搜索、区间刺穿算法和扫描线算法等现有技术，创新性地设计了两个阶段的旋转专用BnB搜索框架，并利用立方映射参数化和几何重叠查询来提高效率和准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将6自由度的点云配准问题分解为两个更简单的子问题：先找到最优旋转轴并估计沿轴平移，再找到最优旋转角度并估计旋转中心。整体流程包括：1)预处理加权特征匹配；2)第一阶段使用立方映射进行旋转轴BnB搜索，结合区间刺穿算法估计平移；3)第二阶段将2D配准简化为1D旋转角搜索，用扫描线算法和线段树解决几何重叠问题；4)后处理使用IRLS优化结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段旋转专用BnB框架，保证全局收敛和多项式复杂度；2)立方映射半球参数化与自适应区间刺穿相结合加速旋转轴搜索；3)确定性多项式时间2D配准解决方案，将问题转化为1D旋转角搜索。相比图方法，避免了二次复杂度；相比多阶段BnB方法，减轻了局部最优问题；相比传统RANSAC，保证了全局最优性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于几何重叠引导的旋转搜索的鲁棒点云配准方法，通过两阶段分支定界搜索和计算几何技术，实现了在高异常值比例下的高效、准确配准，同时保证了多项式时间复杂度和线性空间复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration based on correspondences computes the rigidtransformation that maximizes the number of inliers constrained within thenoise threshold. Current state-of-the-art (SOTA) methods employing spatialcompatibility graphs or branch-and-bound (BnB) search mainly focus onregistration under high outlier ratios. However, graph-based methods require atleast quadratic space and time complexity for graph construction, whilemulti-stage BnB search methods often suffer from inaccuracy due to local optimabetween decomposed stages. This paper proposes a geometric maximum overlappingregistration framework via rotation-only BnB search. The rigid transformationis decomposed using Chasles' theorem into a translation along rotation axis anda 2D rigid transformation. The optimal rotation axis and angle are searched viaBnB, with residual parameters formulated as range maximum query (RMQ) problems.Firstly, the top-k candidate rotation axes are searched within a hemisphereparameterized by cube mapping, and the translation along each axis is estimatedthrough interval stabbing of the correspondences projected onto that axis.Secondly, the 2D registration is relaxed to 1D rotation angle search with 2DRMQ of geometric overlapping for axis-aligned rectangles, which is solveddeterministically in polynomial time using sweep line algorithm with segmenttree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasetsdemonstrate superior accuracy and efficiency over SOTA methods, while the timecomplexity is polynomial and the space complexity increases linearly with thenumber of points, even in the worst case.</description>
      <author>example@mail.com (Zhao Zheng, Jingfan Fan, Long Shao, Hong Song, Danni Ai, Tianyu Fu, Deqiang Xiao, Yongtian Wang, Jian Yang)</author>
      <guid isPermaLink="false">2508.17427v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Deep Learning-based Point Cloud Denoising</title>
      <link>http://arxiv.org/abs/2508.17011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了截至2025年8月的基于深度学习的点云去噪方法，从监督级别和建模视角两个维度组织文献，提出功能分类法，分析架构趋势，建立统一基准，并评估多种性能指标。&lt;h4&gt;背景&lt;/h4&gt;准确的3D几何获取对计算机图形学、自动驾驶、机器人和增强现实等多种应用至关重要，但现实环境中获取的原始点云常受传感器、光照、材料、环境等因素影响而含有噪声，降低了几何保真度并影响下游性能。&lt;h4&gt;目的&lt;/h4&gt;点云去噪旨在恢复干净的点集同时保留底层结构，本文提供一个全面且最新的基于深度学习的点云去噪方法综述，并讨论开放挑战与未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;从监督级别（有监督vs无监督）和建模视角两个维度组织文献，提出基于去噪原理的功能分类法，分析架构趋势（结构和时间维度），建立具有一致训练设置的统一基准，从去噪质量、表面保真度、点分布和计算效率等方面评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;传统基于优化的方法依赖于手工设计的滤波器或几何先验，难以处理多样化和复杂的噪声模式；而深度学习方法利用神经网络架构学习独特表示，在复杂和大规模点云上显示出强大的效果。&lt;h4&gt;结论&lt;/h4&gt;点云去噪领域仍存在开放挑战，需要进一步研究探索更有效的深度学习方法，以应对日益复杂的点云数据和噪声模式。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D几何获取对广泛的应用至关重要，如计算机图形学、自动驾驶、机器人和增强现实。然而，现实环境中获取的原始点云常受传感器、光照、材料、环境等多种因素影响而含有噪声，这降低了几何保真度并损害了下游性能。点云去噪是一个基础问题，旨在恢复干净的点集同时保留底层结构。基于经典优化的方法，由手工设计的滤波器或几何先验指导，已被广泛研究但难以处理多样化和复杂的噪声模式。最近的深度学习方法利用神经网络架构学习独特表示，并在复杂和大规模点云上显示出强大的效果。鉴于这些显著进展，本综述提供了截至2025年8月的基于深度学习的点云去噪方法的全面且最新的回顾。我们从两个视角组织文献：(1) 监督级别（有监督vs无监督），(2) 建模视角，提出一个基于去噪原理的功能分类法，统一不同方法。我们进一步从结构和时间维度分析架构趋势，建立具有一致训练设置的统一基准，并从去噪质量、表面保真度、点分布和计算效率等方面评估方法。最后，我们讨论了开放挑战并概述了这一快速发展领域的未来研究方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云去噪问题，即从受噪声污染的3D点数据中恢复干净点集同时保留底层几何结构。这个问题在现实中非常重要，因为准确的3D几何数据对计算机图形学、自动驾驶、机器人和增强现实等应用至关重要。噪声污染会降低几何保真度，影响后续任务性能，而传统方法难以处理复杂多样的噪声模式。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到点云去噪问题的重要性和传统方法的局限性，观察到深度学习方法在该领域的快速增长。作者发现现有综述往往按网络架构分类，忽略了不同方法可能采用根本不同的问题表述。因此，作者借鉴了现有工作的分类方法，但提出了基于功能/建模策略的创新分类方式，并建立了统一的评估框架。论文引用了大量先前研究，表明作者对领域有深入了解，并在现有基础上进行了系统化的整理和分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 由于这是一篇综述论文而非提出新方法，论文将现有的深度学习点云去噪方法分为五大类：1)基于重建的方法，生成更干净的新点集；2)基于位移的方法，估计每个点的噪声分量并学习位移向量；3)基于分布的方法，从概率角度表述去噪问题；4)基于滤波的方法，估计辅助信息并应用传统滤波规则；5)基于分类的方法，将去噪视为点分类任务。此外，论文还讨论了无监督学习方法，适用于标签数据稀缺的场景。论文按监督级别（监督vs无监督）和建模视角组织文献，提出统一的功能分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1)提出基于功能原则的新分类框架，而非按网络架构分类；2)建立统一的评估基准，在一致设置下评估方法；3)提供全面系统的综述，按底层问题表述和架构设计组织方法；4)分析去噪质量、表面保真度、点分布和计算效率等多方面。相比之前的工作，这篇论文专注于深度学习方法，更新至2025年最新进展，并提供了更公平的比较框架，使不同方法的评估更加客观。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提供了一个全面系统的深度学习点云去噪方法综述，提出了基于功能原则的新分类框架，并建立了统一的评估基准，为该领域的研究提供了清晰的路线图和未来方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D geometry acquisition is essential for a wide range ofapplications, such as computer graphics, autonomous driving, robotics, andaugmented reality. However, raw point clouds acquired in real-worldenvironments are often corrupted with noise due to various factors such assensor, lighting, material, environment etc, which reduces geometric fidelityand degrades downstream performance. Point cloud denoising is a fundamentalproblem, aiming to recover clean point sets while preserving underlyingstructures. Classical optimization-based methods, guided by hand-craftedfilters or geometric priors, have been extensively studied but struggle tohandle diverse and complex noise patterns. Recent deep learning approachesleverage neural network architectures to learn distinctive representations anddemonstrate strong outcomes, particularly on complex and large-scale pointclouds. Provided these significant advances, this survey provides acomprehensive and up-to-date review of deep learning-based point clouddenoising methods up to August 2025. We organize the literature from twoperspectives: (1) supervision level (supervised vs. unsupervised), and (2)modeling perspective, proposing a functional taxonomy that unifies diverseapproaches by their denoising principles. We further analyze architecturaltrends both structurally and chronologically, establish a unified benchmarkwith consistent training settings, and evaluate methods in terms of denoisingquality, surface fidelity, point distribution, and computational efficiency.Finally, we discuss open challenges and outline directions for future researchin this rapidly evolving field.</description>
      <author>example@mail.com (Jinxi Wang, Ben Fei, Dasith de Silva Edirimuni, Zheng Liu, Ying He, Xuequan Lu)</author>
      <guid isPermaLink="false">2508.17011v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches</title>
      <link>http://arxiv.org/abs/2508.18293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures, submitted to IEEE Journal of Oceanic Engineering  (IEEE-JOE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种无需训练数据的水下3D物体检测方法，通过结合基于物理的声纳模拟和模板匹配技术，解决了水下环境训练数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;水下3D物体检测是计算机视觉领域最具挑战性的前沿之一，传统方法在水下声学环境和训练数据稀缺方面存在困难。虽然深度学习在陆地3D检测上取得了革命性进展，但水下应用面临获取足够标注声纳数据成本高昂且复杂的瓶颈。&lt;h4&gt;目的&lt;/h4&gt;解决能否在没有真实世界训练数据的情况下实现可靠的水下3D物体检测这一基本问题。&lt;h4&gt;方法&lt;/h4&gt;开发并比较了两种多波束测深仪点云中人工结构无训练检测的范式：1) 基于物理的声纳模拟管道，为最先进的神经网络生成合成训练数据；2) 基于模型的模板匹配系统，利用目标对象的几何先验信息。&lt;h4&gt;主要发现&lt;/h4&gt;在波罗的海真实测深调查中，合成数据训练的神经网络在模拟场景上达到98%的平均精度均值(mAP)，但在真实声纳数据上下降到40% mAP（由于域转移）；相比之下，模板匹配方法在不需要任何训练的情况下在真实数据上保持83% mAP，对声学噪声和环境变化表现出显著的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;研究结果挑战了水下领域对数据饥渴型深度学习的传统观念，建立了首个大规模的无训练水下3D检测基准，为在数据稀缺环境中进行自主水下航行器导航、海洋考古和海上基础设施监测开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;水下3D物体检测仍然是计算机视觉中最具挑战性的前沿领域之一，传统方法难以应对严酷的声学环境和训练数据的稀缺。虽然深度学习已经革新了陆地3D检测，但其在水下的应用面临一个关键瓶颈：获取足够的标注声纳数据成本高昂且后勤复杂，通常需要专门的船只、专业的测量员和有利的天气条件。这项工作解决了一个基本问题：我们能否在没有真实世界训练数据的情况下实现可靠的水下3D物体检测？我们通过开发和比较两种用于多波束测深仪点云中人工结构无训练检测的范式来应对这一挑战。我们的双重方法结合了基于物理的声纳模拟管道，为最先进的神经网络生成合成训练数据，以及一个稳健的基于模型的模板匹配系统，该系统利用目标对象的几何先验。在波罗的海真实测深调查中的评估揭示了令人惊讶的见解：虽然合成数据训练的神经网络在模拟场景上实现了98%的平均精度均值(mAP)，但由于域转移，它们在真实声纳数据上下降到40% mAP。相反，我们的模板匹配方法在不需要任何训练的情况下在真实数据上保持了83% mAP，表现出对声学噪声和环境变化的显著鲁棒性。我们的研究结果挑战了关于水下领域数据饥渴型深度学习的传统观念，并建立了首个无训练水下3D检测的大规模基准。这项工作为在传统机器学习方法失效的数据稀缺环境中的自主水下航行器导航、海洋考古和海上基础设施监测开辟了新的可能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决水下环境中缺乏训练数据时的3D物体检测问题。这个问题在现实中非常重要，因为水下环境获取标注数据极其困难和昂贵，需要专业船只、专家调查员和 favorable 的天气条件。传统深度学习方法依赖大量标注数据，但在水下环境中获取这些数据成本高昂且复杂。可靠的水下3D物体检测对自主水下航行器导航、海洋考古和海上基础设施监测等领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用双轨策略设计方法：一方面借鉴现有的SASA神经网络架构，通过物理声纳模拟生成合成训练数据；另一方面借鉴传统模型拟合方法，如RANSAC和ICP算法，创建3D物体模型库并转换为声纳点云模板。作者借鉴了现有工作，如使用RANSAC进行海底平面去除预处理，使用ICP算法进行点云配准，使用SASA神经网络处理点云数据，以及使用Perlin噪声生成逼真的海底环境。这两种方法互补，分别从数据驱动和几何先验角度解决训练数据稀缺的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在缺乏真实训练数据的情况下，通过两种互补的方法实现水下3D物体检测：1) 基于物理的声纳模拟生成合成数据训练神经网络；2) 基于几何先验的模板匹配系统直接应用物体模型。整体实现流程：深度学习方法包括创建合成声纳数据集（生成地形、放置物体、模拟扫描）、训练SASA神经网络（使用合成数据并应用数据增强）、在真实数据上测试评估；模型匹配方法包括准备模板库（创建3D模型并转换为点云模板）、预处理真实数据（RANSAC去除海底、滑动窗口分割）、检测配准（ICP算法对齐模板与分割区域）和后处理（去除重复检测）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次对基于模型和深度学习的水下3D物体检测方法进行大规模直接比较，且不使用真实训练数据；2) 提出并验证训练-free水下3D物体检测的可行性；3) 开发程序化多波束声纳模拟框架生成逼真合成数据；4) 在高噪声声纳数据中直接应用ICP算法进行物体检测。相比之前工作，不同之处在于：专注于缺乏训练数据的场景；系统比较两种方法范式；针对复杂人工结构而非简单形状；使用更大规模数据集和更全面评估指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过比较深度学习和传统模型匹配方法，首次证明了在缺乏真实训练数据的情况下，基于几何先验的模板匹配方法能实现鲁棒的水下3D物体检测，为数据稀缺的水下环境提供了可行解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater 3D object detection remains one of the most challenging frontiersin computer vision, where traditional approaches struggle with the harshacoustic environment and scarcity of training data. While deep learning hasrevolutionized terrestrial 3D detection, its application underwater faces acritical bottleneck: obtaining sufficient annotated sonar data is prohibitivelyexpensive and logistically complex, often requiring specialized vessels, expertsurveyors, and favorable weather conditions. This work addresses a fundamentalquestion: Can we achieve reliable underwater 3D object detection withoutreal-world training data? We tackle this challenge by developing and comparingtwo paradigms for training-free detection of artificial structures in multibeamecho-sounder point clouds. Our dual approach combines a physics-based sonarsimulation pipeline that generates synthetic training data for state-of-the-artneural networks, with a robust model-based template matching system thatleverages geometric priors of target objects. Evaluation on real bathymetrysurveys from the Baltic Sea reveals surprising insights: while neural networkstrained on synthetic data achieve 98% mean Average Precision (mAP) on simulatedscenes, they drop to 40% mAP on real sonar data due to domain shift.Conversely, our template matching approach maintains 83% mAP on real datawithout requiring any training, demonstrating remarkable robustness to acousticnoise and environmental variations. Our findings challenge conventional wisdomabout data-hungry deep learning in underwater domains and establish the firstlarge-scale benchmark for training-free underwater 3D detection. This workopens new possibilities for autonomous underwater vehicle navigation, marinearchaeology, and offshore infrastructure monitoring in data-scarce environmentswhere traditional machine learning approaches fail.</description>
      <author>example@mail.com (M. Salman Shaukat, Yannik Käckenmeister, Sebastian Bader, Thomas Kirste)</author>
      <guid isPermaLink="false">2508.18293v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Autoregressive Universal Video Segmentation Model</title>
      <link>http://arxiv.org/abs/2508.19242v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AUSM（自回归通用分割模型），一种统一的视频分割架构，能够同时处理提示性和非提示性视频分割任务，基于状态空间模型构建，可高效处理任意长度的视频流。&lt;h4&gt;背景&lt;/h4&gt;近期的视频基础模型如SAM2在提示视频分割方面表现出色，但现实场景中常需要非提示性分割（无需外部提示检测和跟踪所有对象），而当前解决方案分散在特定任务模型和流程中。&lt;h4&gt;目的&lt;/h4&gt;将流式视频分割重新构建为序列掩码预测（类似于语言建模），并开发一种统一架构同时处理提示性和非提示性视频分割任务。&lt;h4&gt;方法&lt;/h4&gt;提出AUSM模型，基于最新状态空间模型构建，保持固定大小的空间状态，可扩展到任意长度的视频流。所有组件设计为跨帧并行训练，显著提高训练速度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试（DAVIS17、YouTube-VOS、MOSE、YouTube-VIS和OVIS）上，AUSM优于先前的通用流式视频分割方法，并在16帧序列上的训练速度提高了最多2.5倍。&lt;h4&gt;结论&lt;/h4&gt;AUSM是一种统一的视频分割模型，能够有效处理提示性和非提示性分割任务，具有高效训练和优越性能的特点，为视频分割领域提供了新的通用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近期的视频基础模型如SAM2在提示视频分割方面表现出色，将掩码视为通用原语。然而，许多现实场景需要非提示性分割，旨在无需外部提示的情况下检测和跟踪视频中的所有对象，导致当今的解决方案分散在特定任务模型和流程中。我们将流式视频分割重新构建为序列掩码预测，类似于语言建模，并引入了自回归通用分割模型（AUSM），这是一种统一提示性和非提示性视频分割的单一架构。基于最新的状态空间模型构建，AUSM保持固定大小的空间状态，并可扩展到任意长度的视频流。此外，AUSM的所有组件都设计为跨帧并行训练，比迭代训练提供显著的加速。在标准基准测试（DAVIS17、YouTube-VOS 2018 &amp; 2019、MOSE、YouTube-VIS 2019 &amp; 2021和OVIS）上，AUSM优于先前的通用流式视频分割方法，并在16帧序列上的训练速度提高了最多2.5倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent video foundation models such as SAM2 excel at prompted videosegmentation by treating masks as a general-purpose primitive. However, manyreal-world settings require unprompted segmentation that aims to detect andtrack all objects in a video without external cues, leaving today's landscapefragmented across task-specific models and pipelines. We recast streaming videosegmentation as sequential mask prediction, analogous to language modeling, andintroduce the Autoregressive Universal Segmentation Model (AUSM), a singlearchitecture that unifies both prompted and unprompted video segmentation.Built on recent state-space models, AUSM maintains a fixed-size spatial stateand scales to video streams of arbitrary length. Furthermore, all components ofAUSM are designed for parallel training across frames, yielding substantialspeedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS2018 &amp; 2019, MOSE, YouTube-VIS 2019 &amp; 2021, and OVIS) AUSM outperforms prioruniversal streaming video segmentation methods and achieves up to 2.5x fastertraining on 16-frame sequences.</description>
      <author>example@mail.com (Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma)</author>
      <guid isPermaLink="false">2508.19242v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>MACE4IR: A foundation model for molecular infrared spectroscopy</title>
      <link>http://arxiv.org/abs/2508.19118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages and 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MACE4IR，一个基于MACE架构构建的机器学习基础模型，能够准确预测红外光谱同时显著降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;机器学习原子间势(MLIPs)在预测红外光谱方面表现出色，但缺乏能处理多种元素及其组合的通用模型，限制了其广泛应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用、准确且高效的机器学习模型，用于预测各种化学系统的红外光谱。&lt;h4&gt;方法&lt;/h4&gt;构建基于MACE架构的MACE4IR模型，使用QCML数据集上的1000万个几何结构及相应的DFT能量、力和偶极矩进行训练，数据涵盖约80种元素和多样化分子集合。&lt;h4&gt;主要发现&lt;/h4&gt;MACE4IR能准确预测能量、力、偶极矩和红外光谱，与DFT计算相比成本显著降低。&lt;h4&gt;结论&lt;/h4&gt;MACE4IR结合了通用性、准确性和效率，为化学、生物学和材料科学中复杂系统的快速可靠红外光谱预测开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;机器学习的原子间势(MLIPs)在以高保真度预测红外光谱方面显示出巨大潜力。然而，缺乏能够处理各种元素及其组合的通用MLIPs限制了它们更广泛的应用。在这项工作中，我们介绍了MACE4IR，这是一个基于MACE架构构建的机器学习基础模型，并在QCML数据集上的1000万个几何结构以及相应的密度泛函理论(DFT)能量、力和偶极矩上进行了训练。训练数据涵盖了约80种元素和多样化的分子集合，包括有机化合物、无机物质和金属配合物。与DFT相比，MACE4IR能以显著降低的计算成本准确预测能量、力、偶极矩和红外光谱。通过结合通用性、准确性和效率，MACE4IR为化学、生物学和材料科学中复杂系统的快速可靠红外光谱预测开辟了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learned interatomic potentials (MLIPs) have shown significant promisein predicting infrared spectra with high fidelity. However, the absence ofgeneral-purpose MLIPs capable of handling a wide range of elements and theircombinations has limited their broader applicability. In this work, weintroduce MACE4IR, a machine learning foundation model built on the MACEarchitecture and trained on 10 million geometries and correspondingdensity-functional theory (DFT) energies, forces and dipole moments from theQCML dataset. The training data encompasses approximately 80 elements and adiverse set of molecules, including organic compounds, inorganic species, andmetal complexes. MACE4IR accurately predicts energies, forces, dipole moments,and infrared spectra at significantly reduced computational cost compared toDFT. By combining generality, accuracy, and efficiency, MACE4IR opens the doorto rapid and reliable infrared spectra prediction for complex systems acrosschemistry, biology, and materials science.</description>
      <author>example@mail.com (Nitik Bhatia, Ondrej Krejci, Silvana Botti, Patrick Rinke, Miguel A. L. Marques)</author>
      <guid isPermaLink="false">2508.19118v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
      <link>http://arxiv.org/abs/2508.19112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出RF-Deep方法，结合Transformer分割模型和随机森林分类器，用于检测分布外数据并提高癌症分割的可靠性&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的模型在分布内数据上能产生可靠的分割结果，但在分布外数据上性能下降。准确检测和分割CT扫描中的癌变区域对自动化治疗计划和癌症治疗反应评估至关重要&lt;h4&gt;目的&lt;/h4&gt;解决基于Transformer的分割模型在处理分布外数据时性能下降的问题，提高癌症分割在分布内和分布外场景中的可靠性&lt;h4&gt;方法&lt;/h4&gt;提出RF-Deep方法，使用随机森林分类器检测分布外扫描，利用预训练Transformer编码器的深度特征；分割模型包含Swin Transformer编码器（使用掩码图像模型在10,432个未标记3D CT扫描上预训练）和卷积解码器（在317个3D扫描上训练以分割肺癌）；在603个3D CT公共数据集上进行独立测试&lt;h4&gt;主要发现&lt;/h4&gt;RF-Deep在肺栓塞、COVID-19和腹部CT上检测分布外案例的FPR95分别为18.26%、27.66%和小于0.1%，一致性地优于既有的分布外检测方法&lt;h4&gt;结论&lt;/h4&gt;RF-Deep分类器提供了一种简单有效的方法，可以增强在分布内和分布外场景中癌症分割的可靠性&lt;h4&gt;翻译&lt;/h4&gt;准确的检测和分割计算机断层扫描(CT)中的癌变病变对于自动化治疗计划和癌症治疗反应评估至关重要。基于Transformer的模型通过自监督预训练可以从分布内数据产生可靠的分割结果，但在应用于分布外数据集时性能下降。我们通过RF-Deep解决了这一挑战，RF-Deep是一个随机森林分类器，它利用分割模型预训练的Transformer编码器的深度特征来检测分布外扫描，并增强分割可靠性。分割模型包含一个Swin Transformer编码器，在10,432个覆盖癌症和非癌症状况的未标记3D CT扫描上使用掩码图像模型进行预训练，以及一个卷积解码器，在317个3D扫描上训练以分割肺癌。在603个3D CT公共数据集上进行了独立测试，包括一个分布内数据集和四个分布外数据集，包含肺栓塞(PE)和COVID-19的胸部CT，以及肾脏癌症和健康志愿者的腹部CT。RF-Deep在PE、COVID-19和腹部CT上检测分布外案例的FPR95分别为18.26%、27.66%和小于0.1%，一致性地优于既有的分布外方法。RF-Deep分类器提供了一种简单有效的方法，以增强分布内和分布外场景中癌症分割的可靠性&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何准确检测'分布外'(OOD)数据以提高肺癌分割模型在临床应用中的鲁棒性问题。这个问题很重要，因为在实际临床环境中，模型可能遇到与训练数据不同的情况（如从胸部CT扩展到筛查中的良性结节、肺栓塞或腹部CT），而传统评估指标不足以衡量模型对这类数据的鲁棒性，现有OOD检测方法也存在局限性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有OOD检测方法的局限性，然后提出关键洞察：医学图像包含不同解剖范围，应专注于肿瘤区域而非全局图像进行OOD检测。他们借鉴了Swin Transformer架构、SimMIM自监督预训练和随机森林分类器等现有工作，创新性地结合深度特征与随机森林，创建RF-Deep方法，并应用'异常暴露'范式使用小规模ID和OOD示例训练分类器。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练Transformer编码器提取的深度特征，结合随机森林分类器检测OOD扫描，专注于肿瘤区域而非整个图像。实现流程分为四步：1)微调肺癌分割模型；2)冻结编码器并从肿瘤区域提取多尺度特征；3)用ID和OOD特征训练随机森林分类器；4)对新扫描提取特征并用训练好的检测器分类为ID或OOD。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：轻量级随机森林分类器、专注于肿瘤区域而非全局图像、利用多尺度Transformer特征、应用异常暴露范式减少数据需求。相比之前工作，RF-Deep能处理模型自信错误分割的情况，比传统方法有更低错误率；比辅助模型方法更高效、更易解释；比放射组学方法能更好地推广到肿瘤检测；结合了深度学习和传统机器学习的优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于随机森林和深度特征的轻量级OOD检测方法，有效提高了肺癌分割模型在面对不同解剖部位和疾病类型时的鲁棒性和可靠性，为临床应用提供了更安全可靠的自动化分割工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate detection and segmentation of cancerous lesions from computedtomography (CT) scans is essential for automated treatment planning and cancertreatment response assessment. Transformer-based models with self-supervisedpretraining can produce reliably accurate segmentation from in-distribution(ID) data but degrade when applied to out-of-distribution (OOD) datasets. Weaddress this challenge with RF-Deep, a random forest classifier that utilizesdeep features from a pretrained transformer encoder of the segmentation modelto detect OOD scans and enhance segmentation reliability. The segmentationmodel comprises a Swin Transformer encoder, pretrained with masked imagemodeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous andnon-cancerous conditions, with a convolution decoder, trained to segment lungcancers in 317 3D scans. Independent testing was performed on 603 3D CT publicdatasets that included one ID dataset and four OOD datasets comprising chestCTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidneycancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,consistently outperforming established OOD approaches. The RF-Deep classifierprovides a simple and effective approach to enhance reliability of cancersegmentation in ID and OOD scenarios.</description>
      <author>example@mail.com (Aneesh Rangnekar, Harini Veeraraghavan)</author>
      <guid isPermaLink="false">2508.19112v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>GReAT: leveraging geometric artery data to improve wall shear stress assessment</title>
      <link>http://arxiv.org/abs/2508.19030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  (MICCAI 2025) Workshop on Shape in Medical Imaging (ShapeMI)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何利用大型血管几何数据集通过自监督学习改善小规模临床试验中的壁面剪切应力评估&lt;h4&gt;背景&lt;/h4&gt;利用大数据进行患者护理在心血管健康等领域前景广阔，可通过机器学习从医学图像中评估血流动力学生物标志物，但收集足够大的训练数据集极其困难&lt;h4&gt;目的&lt;/h4&gt;研究大型几何模型数据集(8449个形状)能否改善小规模临床试验(49名患者)中冠状动脉模型的壁面剪切应力评估&lt;h4&gt;方法&lt;/h4&gt;通过计算热核签名(一种能捕捉形状本质的量)为3D血管创建自监督目标，利用从大型数据集学习的几何表示提高冠状动脉分割&lt;h4&gt;主要发现&lt;/h4&gt;从大型血管几何模型数据集学习的表示可改善壁面剪切应力评估，即使在有限数据上训练也能提高冠状动脉分割为低、中、高壁面剪切应力区域的准确性&lt;h4&gt;结论&lt;/h4&gt;大型血管几何模型数据集可通过自监督学习方法增强小规模临床试验中的壁面剪切应力评估&lt;h4&gt;翻译&lt;/h4&gt;利用大数据进行患者护理在心血管健康等许多医学领域前景广阔。例如，可以通过机器学习算法从患者特定的医学图像中评估壁面剪切应力等血流动力学生物标志物，从而避免了耗时的计算流体动力学模拟。然而，收集足够大的数据集来有效训练此类模型极其困难。我们可以通过在大量几何动脉模型数据集上进行自监督预训练和基础模型来解决数据稀缺问题。在冠状动脉背景下，利用学习到的表示来改进血流动力学生物标志物评估尚未得到充分研究。在本工作中，我们通过研究大型数据集(8449个形状)是否可以改善从小规模临床试验(49名患者)中获得的冠状动脉模型中的壁面剪切应力评估来填补这一空白。我们通过计算热核签名(一种通过拉普拉斯特征向量获得的量，能够捕捉形状的本质)为3D血管创建自监督目标。我们展示了如何从这个数据集中学习的几何表示可以提高冠状动脉的分割，将其分为低、中、高(时间平均)壁面剪切应力区域，即使是在有限数据上训练的。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大规模几何动脉数据改善血管壁剪切应力评估的问题。这个问题很重要因为心血管疾病是全球主要致死原因，而血管壁剪切应力作为动脉粥样硬化的生物标志物可以帮助预测心肌梗死等严重后果。目前计算流体动力学方法耗时，而神经网络替代方法又受限于数据量不足，医疗机构难以获得足够大的数据集来训练有效模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：CFD计算耗时，而神经网络需要大量特定数据才能有效训练。他们借鉴了自监督学习和基础模型在计算机视觉领域的成功经验，选择使用热核签名作为自监督训练目标，因为它能捕捉形状的本质特征。他们设计了基于交叉注意力变换器的架构（LaB-VaTr），并提出了几何表示适应变换器（GReAT）框架，将预训练的几何表示与特定任务模型结合。该方法还参考了图表示学习中的自监督回归目标以及3D点云的自学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在大规模几何动脉数据上进行自监督预训练学习通用表示，然后在小规模临床数据上微调，改善血管壁剪切应力评估。整体流程包括：1) 准备数据（8449个3D血管模型和49个临床冠状动脉模型）；2) 对每个血管模型计算热核签名作为自监督目标；3) 使用LaB-VaTr模型在大型数据集上进行自监督预训练；4) 创建GReAT框架将预训练几何表示与特定任务模型结合；5) 在临床数据上进行微调，分类低中高WSS区域；6) 通过八折交叉验证评估性能并进行消融研究。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出几何表示适应变换器（GReAT）框架，将自监督预训练几何表示与特定任务模型结合；2) 使用热核签名作为自监督训练目标，有效捕捉形状多尺度特征；3) 创建并标注了大规模血管几何数据集（MedShapeNet-Blood-Vessel）。相比之前工作，本文首次将自监督预训练应用于心血管几何数据以改善WSS评估；采用简单回归目标而非复杂对比学习；不仅展示预训练好处，还通过消融研究验证各组件贡献，并探索了预训练在不同条件下的局限性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的GReAT框架通过在大规模血管几何数据上进行自监督预训练学习通用表示，然后在小规模临床数据上微调，显著改善了冠状动脉壁剪切应力的评估准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging big data for patient care is promising in many medical fields suchas cardiovascular health. For example, hemodynamic biomarkers like wall shearstress could be assessed from patient-specific medical images via machinelearning algorithms, bypassing the need for time-intensive computational fluidsimulation. However, it is extremely challenging to amass large-enough datasetsto effectively train such models. We could address this data scarcity by meansof self-supervised pre-training and foundations models given large datasets ofgeometric artery models. In the context of coronary arteries, leveraginglearned representations to improve hemodynamic biomarker assessment has not yetbeen well studied. In this work, we address this gap by investigating whether alarge dataset (8449 shapes) consisting of geometric models of 3D blood vesselscan benefit wall shear stress assessment in coronary artery models from asmall-scale clinical trial (49 patients). We create a self-supervised targetfor the 3D blood vessels by computing the heat kernel signature, a quantityobtained via Laplacian eigenvectors, which captures the very essence of theshapes. We show how geometric representations learned from this datasets canboost segmentation of coronary arteries into regions of low, mid and high(time-averaged) wall shear stress even when trained on limited data.</description>
      <author>example@mail.com (Julian Suk, Jolanda J. Wentzel, Patryk Rygiel, Joost Daemen, Daniel Rueckert, Jelmer M. Wolterink)</author>
      <guid isPermaLink="false">2508.19030v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory</title>
      <link>http://arxiv.org/abs/2508.18829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用预训练遥感基础模型的深度特征来提高国家森林普查(NFI)树种分类准确性的方法，结果表明这种方法显著优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;国家森林普查是获取森林信息和树种分布数据的主要来源，但维护这些普查需要大量实地工作。遥感技术与机器学习相结合提供了更频繁、更大规模更新NFI的机会。&lt;h4&gt;目的&lt;/h4&gt;系统研究深度特征如何在标注数据有限的情况下提高荷兰的树种分类准确性，并探索预训练遥感基础模型在NFI分类中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;从Sentinel-1、Sentinel-2和ERA5卫星数据以及SRTM数据中提取时间序列数据，使用Google Earth Engine进行处理，并微调公开可用的遥感时间序列基础模型进行树种分类。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的遥感时间序列基础模型在荷兰NFI分类中显著优于当前最先进的方法，在所有数据集上提高了高达10%的准确率，表明传统手工定义的谐波特征对于此任务过于简单。&lt;h4&gt;结论&lt;/h4&gt;深度AI特征在NFI分类等数据有限应用中具有巨大潜力。通过利用开放的卫星数据和预训练模型，这种方法显著提高了分类准确性，可以有效补充现有的森林普查流程。&lt;h4&gt;翻译&lt;/h4&gt;国家森林普查(NFI)是森林信息的主要来源，提供关键的树种分布数据。然而，维护这些普查需要大量实地工作。遥感方法，特别是与机器学习相结合，提供了更频繁和更大规模更新NFI的机会。虽然卫星图像时间序列已证明通过季节性冠层反射率模式区分树种有效，但当前方法主要依赖随机森林分类器和手工设计的特征及物候指标。使用可用预训练遥感基础模型的深度特征提供了补充策略。这些预训练模型利用未标注的全球数据，可用于通用应用，然后可以用较小的标注数据集针对特定分类任务进行高效微调。本研究系统研究了深度特征如何在标注数据有限的情况下提高荷兰的树种分类准确性。数据方面，我们使用Google Earth Engine从Sentinel-1、Sentinel-2和ERA5卫星数据以及SRTM数据中提取了时间序列数据。我们的结果表明，微调公开可用的遥感时间序列基础模型在荷兰NFI分类中显著优于当前最先进的方法，在所有数据集上提高了高达10%的准确率。这表明经典手工定义的谐波特征对此任务过于简单，突显了使用深度AI特征在NFI分类等数据有限应用中的潜力。通过利用开放的卫星数据和预训练模型，该方法显著提高了分类准确性，可以有效补充现有的森林普查流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; National Forest Inventory (NFI)s serve as the primary source of forestinformation, providing crucial tree species distribution data. However,maintaining these inventories requires labor-intensive on-site campaigns.Remote sensing approaches, particularly when combined with machine learning,offer opportunities to update NFIs more frequently and at larger scales. Whilethe use of Satellite Image Time Series has proven effective for distinguishingtree species through seasonal canopy reflectance patterns, current approachesrely primarily on Random Forest classifiers with hand-designed features andphenology-based metrics. Using deep features from an available pre-trainedremote sensing foundation models offers a complementary strategy. Thesepre-trained models leverage unannotated global data and are meant to used forgeneral-purpose applications and can then be efficiently fine-tuned withsmaller labeled datasets for specific classification tasks. This worksystematically investigates how deep features improve tree speciesclassification accuracy in the Netherlands with few annotated data. Data-wise,we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellitesdata and SRTM data using Google Earth Engine. Our results demonstrate thatfine-tuning a publicly available remote sensing time series foundation modeloutperforms the current state-of-the-art in NFI classification in theNetherlands by a large margin of up to 10% across all datasets. Thisdemonstrates that classic hand-defined harmonic features are too simple forthis task and highlights the potential of using deep AI features fordata-limited application like NFI classification. By leveraging openlyavailable satellite data and pre-trained models, this approach significantlyimproves classification accuracy compared to traditional methods and caneffectively complement existing forest inventory processes.</description>
      <author>example@mail.com (Takayuki Ishikawa, Carmelo Bonannella, Bas J. W. Lerink, Marc Rußwurm)</author>
      <guid isPermaLink="false">2508.18829v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding</title>
      <link>http://arxiv.org/abs/2508.18785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了EMind，一个电磁信号基础模型，旨在解决电磁信号处理中的挑战，实现跨任务的泛化和高效学习。&lt;h4&gt;背景&lt;/h4&gt;电磁信号的深度理解对动态频谱管理、智能交通、自动驾驶和无人车感知等领域至关重要。电磁信号与文本和图像差异大，表现出高度异质性、强背景噪声和复杂联合时频结构，现有通用模型无法直接使用。此外，电磁通信和感知任务多样，当前方法缺乏跨任务泛化和迁移效率，且缺乏大规模高质量数据集，阻碍了真正的多任务学习框架创建。&lt;h4&gt;目的&lt;/h4&gt;为了克服电磁信号处理中的挑战，作者引入了EMind，一个连接大规模预训练和电磁信号独特特性的基础模型。&lt;h4&gt;方法&lt;/h4&gt;作者构建了首个统一且最大标准化的电磁信号数据集，涵盖多种信号类型和任务。通过利用电磁信号的物理特性，设计了长度自适应的多信号打包方法和硬件感知训练策略，能够高效利用和表示来自异构多源信号的学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，EMind在许多下游任务上实现了强大的性能和广泛的泛化，从特定任务模型转向电磁智能的统一框架。&lt;h4&gt;结论&lt;/h4&gt;EMind代表了电磁信号处理领域的重要进展，通过基础模型方法解决了电磁信号异质性、噪声复杂性和跨任务泛化的挑战，为电磁智能提供了统一框架。&lt;h4&gt;翻译&lt;/h4&gt;电磁信号的深入理解对于动态频谱管理、智能交通、自动驾驶和无人车感知至关重要。该领域面临挑战，因为电磁信号与文本和图像差异很大，表现出高度异质性、强背景噪声和复杂的联合时频结构，这阻碍了现有通用模型的直接使用。电磁通信和感知任务多样，当前方法缺乏跨任务泛化和迁移效率，且缺乏大规模高质量数据集，阻碍了真正的多任务学习框架的创建。为了克服这些问题，我们引入了EMind，一个电磁信号基础模型，连接大规模预训练和这种模态的独特特性。我们构建了第一个统一且最大标准化的电磁信号数据集，涵盖多种信号类型和任务。通过利用电磁信号的物理特性，我们设计了一种长度自适应的多信号打包方法和硬件感知训练策略，能够高效利用和表示来自异构多源信号的学习。实验表明，EMind在许多下游任务上实现了强大的性能和广泛的泛化，从特定任务模型 decisively 转向电磁智能的统一框架。代码可在以下网址获取：https://github.com/GabrielleTse/EMind。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep understanding of electromagnetic signals is fundamental to dynamicspectrum management, intelligent transportation, autonomous driving andunmanned vehicle perception. The field faces challenges because electromagneticsignals differ greatly from text and images, showing high heterogeneity, strongbackground noise and complex joint time frequency structure, which preventsexisting general models from direct use. Electromagnetic communication andsensing tasks are diverse, current methods lack cross task generalization andtransfer efficiency, and the scarcity of large high quality datasets blocks thecreation of a truly general multitask learning framework. To overcome theseissue, we introduce EMind, an electromagnetic signals foundation model thatbridges large scale pretraining and the unique nature of this modality. Webuild the first unified and largest standardized electromagnetic signal datasetcovering multiple signal types and tasks. By exploiting the physical propertiesof electromagnetic signals, we devise a length adaptive multi-signal packingmethod and a hardware-aware training strategy that enable efficient use andrepresentation learning from heterogeneous multi-source signals. Experimentsshow that EMind achieves strong performance and broad generalization acrossmany downstream tasks, moving decisively from task specific models to a unifiedframework for electromagnetic intelligence. The code is available at:https://github.com/GabrielleTse/EMind.</description>
      <author>example@mail.com (Luqing Luo, Wenjin Gui, Yunfei Liu, Ziyue Zhang, Yunxi Zhang, Fengxiang Wang, Zonghao Guo, Zizhi Ma, Xinzhu Liu, Hanxiang He, Jinhai Li, Xin Qiu, Wupeng Xie, Yangang Sun)</author>
      <guid isPermaLink="false">2508.18785v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge</title>
      <link>http://arxiv.org/abs/2508.18663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FFT MoE是一种新的联邦微调框架，用稀疏的专家混合适配器替代LoRA，解决了异构联邦学习环境中的结构兼容性和数据分布适应性问题。&lt;h4&gt;背景&lt;/h4&gt;随着大型模型推动通用人工智能发展，在隐私和资源限制下对模型进行微调变得尤为重要，特别是当高质量训练数据分布在分布式边缘设备上时。&lt;h4&gt;目的&lt;/h4&gt;解决联邦微调中LoRA方法在异构联邦学习环境中的两个主要限制：跨客户端的结构兼容性问题和对非独立同分布数据分布的有限适应性。&lt;h4&gt;方法&lt;/h4&gt;提出FFT MoE框架，用稀疏的专家混合(MoE)适配器替代LoRA。每个客户端训练轻量级门控网络选择性激活个性化专家子集，同时保持聚合兼容性。引入异构感知辅助损失动态正则化路由分布，确保专家多样性和平衡利用。&lt;h4&gt;主要发现&lt;/h4&gt;在独立同分布和非独立同分布条件下的实验表明，FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线。&lt;h4&gt;结论&lt;/h4&gt;FFT MoE框架有效解决了LoRA在异构联邦学习环境中的局限性，提供了更好的性能和适应性。&lt;h4&gt;翻译&lt;/h4&gt;随着大型模型推动通用人工智能(AGI)的进步，在隐私和资源约束下对其进行微调变得越来越关键，特别是当高质量训练数据分布在分布式边缘设备上时。联邦学习(FL)通过联邦微调(FFT)提供了有吸引力的解决方案，它能够在不共享原始数据的情况下实现协作模型适应。最近的方法采用参数高效微调(PEFT)技术，如低秩适应(LoRA)，以减少计算开销。然而，基于LoRA的FFT在异构FL环境中面临两个主要限制：具有不同LoRA配置的客户端之间的结构兼容性问题以及对非独立同分布数据分布的有限适应性，这阻碍了收敛和泛化。为解决这些挑战，我们提出了FFT MoE，一种新的FFT框架，它用稀疏的专家混合(MoE)适配器替代LoRA。每个客户端训练一个轻量级门控网络来选择性激活个性化的专家子集，实现对本地资源预算的细粒度适应，同时保持聚合兼容性。为了进一步对抗由设备和数据异构性导致的专家负载不平衡，我们引入了一个异构感知的辅助损失，该损失动态正则化路由分布，确保专家多样性和平衡利用。跨越独立同分布和非独立同分布条件的广泛实验表明，FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As FMs drive progress toward Artificial General Intelligence (AGI),fine-tuning them under privacy and resource constraints has become increasinglycritical particularly when highquality training data resides on distributededge devices. Federated Learning (FL) offers a compelling solution throughFederated Fine-Tuning (FFT), which enables collaborative model adaptationwithout sharing raw data. Recent approaches incorporate Parameter-EfficientFine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reducecomputational overhead. However, LoRA-based FFT faces two major limitations inheterogeneous FL environments: structural incompatibility across clients withvarying LoRA configurations and limited adaptability to non-IID datadistributions, which hinders convergence and generalization. To address thesechallenges, we propose FFT MoE, a novel FFT framework that replaces LoRA withsparse Mixture of Experts (MoE) adapters. Each client trains a lightweightgating network to selectively activate a personalized subset of experts,enabling fine-grained adaptation to local resource budgets while preservingaggregation compatibility. To further combat the expert load imbalance causedby device and data heterogeneity, we introduce a heterogeneity-aware auxiliaryloss that dynamically regularizes the routing distribution to ensure expertdiversity and balanced utilization. Extensive experiments spanning both IID andnon-IID conditions demonstrate that FFT MoE consistently outperforms state ofthe art FFT baselines in generalization performance and training efficiency.</description>
      <author>example@mail.com (Gang Hu, Yinglei Teng, Pengfei Wu, Nan Wang)</author>
      <guid isPermaLink="false">2508.18663v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Principled Detection of Hallucinations in Large Language Models via Multiple Testing</title>
      <link>http://arxiv.org/abs/2508.18473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于多重检验启发的方法来检测大型语言模型中的幻觉现象。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型虽然能够解决多种任务，但容易产生幻觉，即生成听起来自信但实际上不正确甚至无意义的回答。&lt;h4&gt;目的&lt;/h4&gt;将幻觉检测问题构建为假设检验问题，并探索与机器学习模型分布外检测问题的相似性。&lt;h4&gt;方法&lt;/h4&gt;提出一种受多重检验启发的方法来检测大型语言模型中的幻觉现象。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验验证了所提方法相对于最先进方法的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于多重检验的幻觉检测方法有效且具有鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型语言模型已成为解决各种任务的基础模型，但它们也容易产生幻觉，即生成听起来自信但实际上不正确甚至无意义的回答。在本研究中，我们将幻觉检测问题构建为假设检验问题，并与机器学习模型中的分布外检测问题进行类比。我们提出了一种受多重检验启发的方法来解决幻觉检测问题，并通过大量实验结果验证了我们的方法相对于最先进方法的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Large Language Models (LLMs) have emerged as powerful foundationalmodels to solve a variety of tasks, they have also been shown to be prone tohallucinations, i.e., generating responses that sound confident but areactually incorrect or even nonsensical. In this work, we formulate the problemof detecting hallucinations as a hypothesis testing problem and draw parallelsto the problem of out-of-distribution detection in machine learning models. Wepropose a multiple-testing-inspired method to solve the hallucination detectionproblem, and provide extensive experimental results to validate the robustnessof our approach against state-of-the-art methods.</description>
      <author>example@mail.com (Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli)</author>
      <guid isPermaLink="false">2508.18473v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</title>
      <link>http://arxiv.org/abs/2508.18421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉基础模型(FMs)在需要显式推理实体、角色和时空关系的任务上存在局限性，作者建议下一代FMs应纳入动态关系图接口，并通过实验证明这种混合模型在语义保真度、鲁棒性、可解释性和计算效率方面有优势。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型已成为计算机视觉的主导架构，它们从大规模、多模态语料库中学习高度可迁移的表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉基础模型在需要显式推理关系的任务上的局限性，提出下一代FMs应纳入显式关系接口的方案。&lt;h4&gt;方法&lt;/h4&gt;通过为FMs增加轻量级、上下文自适应的图推理模块，构建动态关系图（其拓扑和边语义是从输入和任务上下文中推断出来的图），实现多级关系推理。&lt;h4&gt;主要发现&lt;/h4&gt;增强FMs的图推理模块可以改善细粒度语义保真度、分布外鲁棒性、可解释性和计算效率，同时通过在语义节点上进行稀疏推理实现有利的内存和硬件效率。&lt;h4&gt;结论&lt;/h4&gt;下一代视觉基础模型应纳入显式的动态关系图接口，未来研究应优先考虑学习动态图构建、多级关系推理、跨模态融合，以及直接探测结构化视觉任务中关系能力的评估协议。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型已成为计算机视觉的主导架构，它们从大规模、多模态语料库中学习高度可迁移的表示。然而，它们在需要显式推理实体、角色和时空关系的任务上存在持续的限制。这种关系能力对于细粒度人体活动识别、以人为中心的视频理解和多模态医学图像分析至关重要，其中空间、时间和语义依赖对性能起决定性作用。我们主张下一代FMs应纳入显式的关系接口，具体表现为动态关系图（其拓扑和边语义是从输入和任务上下文中推断出来的图）。我们通过人体操作动作识别和脑肿瘤分割领域的跨领域证据来说明这一观点，表明为FMs增加轻量级、上下文自适应的图推理模块相对于仅基于FMs的基线，可以改善细粒度语义保真度、分布外鲁棒性、可解释性和计算效率。重要的是，通过在语义节点上进行稀疏推理，这种混合模型还能实现有利的内存和硬件效率，使它们能够在实际资源约束下部署。最后，我们针对FM图混合提出了有针对性的研究议程，优先考虑学习动态图构建、多级关系推理（例如活动理解中的部分-对象-场景，或医学成像中的区域-器官）、跨模态融合，以及直接探测结构化视觉任务中关系能力的评估协议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models (FMs) have become the predominant architecture incomputer vision, providing highly transferable representations learned fromlarge-scale, multimodal corpora. Nonetheless, they exhibit persistentlimitations on tasks that require explicit reasoning over entities, roles, andspatio-temporal relations. Such relational competence is indispensable forfine-grained human activity recognition, egocentric video understanding, andmultimodal medical image analysis, where spatial, temporal, and semanticdependencies are decisive for performance. We advance the position thatnext-generation FMs should incorporate explicit relational interfaces,instantiated as dynamic relational graphs (graphs whose topology and edgesemantics are inferred from the input and task context). We illustrate thisposition with cross-domain evidence from recent systems in human manipulationaction recognition and brain tumor segmentation, showing that augmenting FMswith lightweight, context-adaptive graph-reasoning modules improvesfine-grained semantic fidelity, out of distribution robustness,interpretability, and computational efficiency relative to FM only baselines.Importantly, by reasoning sparsely over semantic nodes, such hybrids alsoachieve favorable memory and hardware efficiency, enabling deployment underpractical resource constraints. We conclude with a targeted research agenda forFM graph hybrids, prioritizing learned dynamic graph construction, multi-levelrelational reasoning (e.g., part object scene in activity understanding, orregion organ in medical imaging), cross-modal fusion, and evaluation protocolsthat directly probe relational competence in structured vision tasks.</description>
      <author>example@mail.com (Fatemeh Ziaeetabar)</author>
      <guid isPermaLink="false">2508.18421v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Topology Optimization for Self-Supporting Structures in Additive Manufacturing</title>
      <link>http://arxiv.org/abs/2508.19169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于机器学习的自支撑结构拓扑优化框架，专门针对增材制造(AM)应用。该框架使用图神经网络作为有限元网格上的神经场，有效学习和预测连续材料分布。集成的AM滤波器确保可打印性，优化过程在体积和应力约束下最小化结构柔度。使用可微的p范数聚合强制执行von Mises应力约束。该方法的完全可微架构利用自动微分优化整个循环，无需明确推导敏感性。数值实验表明该框架能够在各种载荷和边界条件下生成应力约束的可制造拓扑结构，为增材制造准备的高性能设计提供了实际路径，减少了后处理需求。&lt;h4&gt;背景&lt;/h4&gt;增材制造(AM)技术的发展需要专门的优化方法来生成可打印且高性能的结构。传统的拓扑优化方法通常不考虑增材制造的约束，导致设计难以打印或需要大量后处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对增材制造的机器学习框架，用于自支撑结构的拓扑优化，生成既满足力学性能要求又可直接打印的高性能结构，减少后处理需求。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)作为有限元网格上的神经场来学习和预测连续材料分布；集成AM滤波器消除无支撑的悬垂结构确保可打印性；在体积和应力约束下最小化结构柔度；使用可微的p范数聚合强制执行von Mises应力约束；采用完全可微的架构，利用自动微分优化整个循环，避免显式敏感性推导。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够在各种载荷和边界条件下生成满足应力约束的可制造拓扑结构；为增材制造准备的高性能设计提供了实际路径，显著减少了后处理需求。&lt;h4&gt;结论&lt;/h4&gt;该机器学习框架结合了拓扑优化和增材制造考虑，提供了一种高效的方法来设计高性能、可直接制造的结构，解决了传统方法中可制造性和性能之间的权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于机器学习的自支撑结构拓扑优化框架，专门针对增材制造(AM)应用。通过采用作为有限元网格上神经场的图神经网络(GNN)，该框架有效学习和预测连续材料分布。集成的AM滤波器通过消除无支撑的悬垂结构确保可打印性，同时优化过程在体积和应力约束下最小化结构柔度。应力约束使用von Mises应力的可微p范数聚合来强制执行，促进优化设计的机械可靠性。该方法的主要优势在于其完全可微的架构，在整个优化循环中利用自动微分消除了对滤波器和应力约束进行显式敏感性推导的需要。数值实验证明了该框架能够在各种载荷和边界条件下生成应力约束的可制造拓扑结构，为增材制造准备的高性能设计提供了实际路径，减少了后处理要求。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成既适合增材制造（3D打印）又具有良好机械性能的自支撑结构拓扑优化问题。这个问题很重要，因为增材制造虽然能制造复杂几何形状，但悬垂结构需要额外支撑，增加材料成本、生产时间和后处理难度，同时可能影响产品质量。解决此问题可以减少支撑需求，提高制造效率，降低成本，并确保结构在应用中的可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了增材制造的悬垂挑战和传统拓扑优化方法的局限性，然后选择图神经网络作为基础架构，因为它能处理非欧几里得数据（如有限元网格）并捕捉空间依赖关系。作者借鉴了Langelaar的逐层过滤器处理AM约束，使用p范数聚合处理应力约束，并参考了SIMP材料插值方法和神经网络在拓扑优化中的应用。关键创新在于将这些组件整合到一个端到端可微的框架中，利用自动微分避免手动推导复杂敏感性分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络作为神经场在有限元网格上预测连续材料分布，通过端到端可微框架同时处理制造约束（AM过滤器消除悬垂）和机械性能（应力约束确保可靠性）。流程包括：1)将设计域表示为图结构，节点为有限元单元；2)使用GNN预测伪密度；3)通过可微AM过滤器确保自支撑；4)进行有限元分析计算位移和应力；5)使用复合损失函数（柔顺性、体积约束、应力约束）优化设计；6)通过自动微分和Adam优化器迭代更新网络权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)端到端可微框架，利用自动微分避免手动敏感性分析；2)使用图神经网络作为神经场预测材料分布；3)同时集成AM过滤器和应力约束；4)可微的AM过滤器近似。相比之前工作，不同之处在于：传统方法需手动推导复杂敏感性，而本文通过自动微分简化；与GANs等生成方法相比，本文不需要大量训练数据；与单独处理AM约束或应力约束的方法相比，本文将它们统一在一个可微框架中，提高了效率和灵活性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于图神经网络的端到端可微拓扑优化框架，能够同时考虑增材制造的自支撑约束和机械应力约束，生成既可打印又高性能的结构设计。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a machine learning-based framework for topologyoptimization of self-supporting structures, specifically tailored for additivemanufacturing (AM). By employing a graph neural network (GNN) that acts as aneural field over the finite element mesh, the framework effectively learns andpredicts continuous material distributions. An integrated AM filter ensuresprintability by eliminating unsupported overhangs, while the optimizationprocess minimizes structural compliance under volume and stress constraints.The stress constraint is enforced using a differentiable p-norm aggregation ofvon Mises stress, promoting mechanical reliability in the optimized designs. Akey advantage of the approach lies in its fully differentiable architecture,which leverages automatic differentiation throughout the optimizationloop--eliminating the need for explicit sensitivity derivation for both thefilter and the stress constraint. Numerical experiments demonstrate the abilityof the framework to generate stress-constrained manufacturable topologies undervarious loading and boundary conditions, offering a practical pathway towardAM-ready high-performance designs with reduced post-processing requirements.</description>
      <author>example@mail.com (Alireza Tabarraei, Saquib Ahmad Bhuiyan)</author>
      <guid isPermaLink="false">2508.19169v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种新的图重连接方法TRIGON，通过学习选择相关三角形来构建改进的图结构，从而解决了GNN中的过度压缩和过度平滑问题，并在多种基准测试中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已成为处理图结构数据的主要范式，但其性能受到图拓扑固有问题的限制，主要是过度压缩和过度平滑问题。&lt;h4&gt;目的&lt;/h4&gt;通过改进图拓扑来促进更有效的信息传播，解决GNN中的过度压缩和过度平滑问题。&lt;h4&gt;方法&lt;/h4&gt;引入TRIGON框架，通过从多个图视图中选择相关三角形来构建丰富、非平面的三角剖分，并联合优化三角形选择和下游分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;与现有重连接方法相比，TRIGON产生的重连接图具有显著改进的结构特性，包括减少直径、增加谱间隙和降低有效电阻，并在同质性和异质性基准测试的节点分类任务上表现更优。&lt;h4&gt;结论&lt;/h4&gt;TRIGON框架能够有效改进图神经网络的结构特性，通过联合优化三角形选择和分类性能，实现了更好的节点分类结果。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的主要范式。然而，它们的性能受到图拓扑固有问题的限制，最明显的是过度压缩和过度平滑。最近在图重连接方面的进展旨在通过修改图拓扑来促进更有效的信息传播，从而缓解这些限制。在这项工作中，我们引入了TRIGON，一个新颖的框架，它通过从多个图视图中选择相关三角形来构建丰富、非平面的三角剖分。通过联合优化三角形选择和下游分类性能，我们的方法产生的重连接图具有显著改进的结构特性，如与现有重连接方法相比减少直径、增加谱间隙和降低有效电阻。实证结果表明，TRIGON在一系列同质性和异质性基准测试的节点分类任务上优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as the leading paradigm forlearning over graph-structured data. However, their performance is limited byissues inherent to graph topology, most notably oversquashing andoversmoothing. Recent advances in graph rewiring aim to mitigate theselimitations by modifying the graph topology to promote more effectiveinformation propagation. In this work, we introduce TRIGON, a novel frameworkthat constructs enriched, non-planar triangulations by learning to selectrelevant triangles from multiple graph views. By jointly optimizing triangleselection and downstream classification performance, our method produces arewired graph with markedly improved structural properties such as reduceddiameter, increased spectral gap, and lower effective resistance compared toexisting rewiring methods. Empirical results demonstrate that TRIGONoutperforms state-of-the-art approaches on node classification tasks across arange of homophilic and heterophilic benchmarks.</description>
      <author>example@mail.com (Hugo Attali, Thomas Papastergiou, Nathalie Pernelle, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2508.19071v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Automated discovery of finite volume schemes using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文展示了图神经网络（GNNs）在数值模拟中的新应用，证明GNNs不仅能够近似物理系统解，还能与符号回归结合生成数值格式，并能在训练域外有效泛化。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）已经深刻改变了数值模拟的格局，展示了近似物理系统解的强大能力。然而，它们在训练域外（如更大的或结构不同的图）的泛化能力仍然不确定。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs在超越其传统角色方面的潜力，探索如何利用它们生成数值格式，并结合符号回归，以及在无监督情况下恢复数值格式。&lt;h4&gt;方法&lt;/h4&gt;研究团队使用GNNs在仅包含两个节点的图数据集上进行训练，然后将其外推到分布外、非结构化网格上的热方程。他们还使用了符号回归来发现数值格式的解析公式，并将方法扩展到无监督上下文，仅使用类似于物理信息神经网络（PINNs）的残差损失。最后，他们考虑了更高阶的格式，训练了2跳和2层的GNN。&lt;h4&gt;主要发现&lt;/h4&gt;仅在两个节点图上训练的GNN可以外推到分布外、非结构化网格上的一阶有限体积格式；GNN实现的FV格式误差与其训练损失成比例；使用符号回归，GNN能重新发现标准一阶FV格式的精确解析公式；GNN可以在无监督情况下仅使用PINN风格的残差损失恢复一阶FV格式；2跳和2层的GNN能自主发现二阶校正项和经典二阶中点格式。&lt;h4&gt;结论&lt;/h4&gt;GNNs不仅是强大的近似器，还可以作为开发新数值方法的积极参与者，这代表了科学计算中的一个新范式。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文探讨了图神经网络（GNNs）在数值模拟中的应用。研究团队证明，GNNs不仅能够近似物理系统的解，还能与符号回归结合生成数值格式。他们展示了仅在两个节点图上训练的GNN可以外推到分布外、非结构化网格上的一阶有限体积格式，并且能重新发现该格式的精确解析公式。研究还扩展到无监督上下文，以及更高阶格式的发现。这些发现表明，GNNs在科学计算中不仅是强大的近似工具，还能积极参与新数值方法的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have deeply modified the landscape of numericalsimulations by demonstrating strong capabilities in approximating solutions ofphysical systems. However, their ability to extrapolate beyond their trainingdomain (\textit{e.g.} larger or structurally different graphs) remainsuncertain. In this work, we establish that GNNs can serve purposes beyond theirtraditional role, and be exploited to generate numerical schemes, inconjunction with symbolic regression. First, we show numerically andtheoretically that a GNN trained on a dataset consisting solely of two-nodegraphs can extrapolate a first-order Finite Volume (FV) scheme for the heatequation on out-of-distribution, unstructured meshes. Specifically, if a GNNachieves a loss $\varepsilon$ on such a dataset, it implements the FV schemewith an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we showthat the network effectively rediscovers the exact analytical formulation ofthe standard first-order FV scheme. We then extend this approach to anunsupervised context: the GNN recovers the first-order FV scheme using only aresidual loss similar to Physics-Informed Neural Networks (PINNs) with noaccess to ground-truth data. Finally, we push the methodology further byconsidering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNNusing the same PINN loss, that autonomously discover (i) a second-ordercorrection term to the initial scheme using a 2-hop stencil, and (ii) theclassic second-order midpoint scheme. These findings follows a recent paradigmin scientific computing: GNNs are not only strong approximators, but can beactive contributors to the development of novel numerical methods.</description>
      <author>example@mail.com (Paul Garnier, Jonathan Viquerat, Elie Hachem)</author>
      <guid isPermaLink="false">2508.19052v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation</title>
      <link>http://arxiv.org/abs/2508.18933v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VISION的统一框架，用于鲁棒和可解释的漏洞检测，通过生成反事实训练数据来减轻图神经网络中的虚假相关性学习，显著提高了漏洞检测的准确性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;自动检测源代码中的漏洞是网络安全的重要挑战，图神经网络(GNNs)虽然能够通过数据驱动方式学习代码的结构和逻辑关系，但其性能受到训练数据不平衡和标签噪声的限制，常常从表面代码相似性中学到虚假相关性，导致无法很好地推广到真实世界数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为VISION的统一框架，通过系统性地增强反事实训练数据集来减轻虚假相关性，实现更鲁棒和可解释的漏洞检测。&lt;h4&gt;方法&lt;/h4&gt;VISION框架包含三个核心部分：通过提示大型语言模型(LLM)生成反事实样本；在具有相反标签的配对代码示例上进行有针对性的GNN训练；以及基于图的解释性方法，识别对漏洞预测至关重要的代码语句同时忽略虚假相关性。&lt;h4&gt;主要发现&lt;/h4&gt;VISION显著减少了虚假学习，实现了更鲁棒、可推广的检测：在CWE-20漏洞上，整体准确率从51.8%提高到97.8%，成对对比准确率从4.5%提高到95.8%，最差组准确率从0.7%提高到85.5%。通过提出的指标（类内归因方差、类间归因距离和节点分数依赖性）也展示了显著改进。&lt;h4&gt;结论&lt;/h4&gt;研究团队发布了包含27,556个函数（真实和反事实）的CWE-20-CFA基准数据集，并通过交互式可视化推进了透明和值得信赖的基于AI的网络安全系统，支持人机循环分析。&lt;h4&gt;翻译&lt;/h4&gt;自动检测源代码中的漏洞是网络安全的重要挑战，它支撑着对数字系统和服务的信任。图神经网络(GNNs)作为一种有前景的方法出现，因为它们可以通过数据驱动的方式学习代码的结构和逻辑关系。然而，它们的性能受到训练数据不平衡和标签噪声的严重限制。GNNs通常从表面的代码相似性中学到'虚假'相关性，产生的检测器无法很好地推广到未见过的真实世界数据。在这项工作中，我们提出了一个名为VISION的鲁棒和可解释漏洞检测的统一框架，通过系统性地增强反事实训练数据集来减轻虚假相关性。反事实是具有最小语义修改但标签相反的样本。我们的框架包括：通过提示大型语言模型(LLM)生成反事实；在具有相反标签的配对代码示例上进行有针对性的GNN训练；以及基于图的解释性，识别与漏洞预测相关的关键代码语句，同时忽略虚假相关性。我们发现VISION减少了虚假学习，实现了更鲁棒、可推广的检测，在通用缺陷枚举(CWE)-20漏洞上将整体准确率（从51.8%提高到97.8%）、成对对比准确率（从4.5%提高到95.8%）和最差组准确率（从0.7%提高到85.5%）都得到了提高。我们使用提出的指标进一步展示了改进：类内归因方差、类间归因距离和节点分数依赖性。我们还发布了CWE-20-CFA，这是一个来自高影响CWE-20类别的27,556个函数（真实和反事实）的基准。最后，VISION通过人机循环分析的交互式可视化，推进了透明和值得信赖的基于AI的网络安全系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated detection of vulnerabilities in source code is an essentialcybersecurity challenge, underpinning trust in digital systems and services.Graph Neural Networks (GNNs) have emerged as a promising approach as they canlearn structural and logical code relationships in a data-driven manner.However, their performance is severely constrained by training data imbalancesand label noise. GNNs often learn 'spurious' correlations from superficial codesimilarities, producing detectors that fail to generalize well to unseenreal-world data. In this work, we propose a unified framework for robust andinterpretable vulnerability detection, called VISION, to mitigate spuriouscorrelations by systematically augmenting a counterfactual training dataset.Counterfactuals are samples with minimal semantic modifications but oppositelabels. Our framework includes: (i) generating counterfactuals by prompting aLarge Language Model (LLM); (ii) targeted GNN training on paired code exampleswith opposite labels; and (iii) graph-based interpretability to identify thecrucial code statements relevant for vulnerability predictions while ignoringspurious ones. We find that VISION reduces spurious learning and enables morerobust, generalizable detection, improving overall accuracy (from 51.8% to97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-groupaccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20vulnerability. We further demonstrate gains using proposed metrics: intra-classattribution variance, inter-class attribution distance, and node scoredependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (realand counterfactual) from the high-impact CWE-20 category. Finally, VISIONadvances transparent and trustworthy AI-based cybersecurity systems throughinteractive visualization for human-in-the-loop analysis.</description>
      <author>example@mail.com (David Egea, Barproda Halder, Sanghamitra Dutta)</author>
      <guid isPermaLink="false">2508.18933v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data</title>
      <link>http://arxiv.org/abs/2508.18891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;pyFAST是一个面向研究的时间序列分析PyTorch框架，具有灵活性、效率和可扩展性特点，特别处理不规则、多源或稀疏数据，明确分离数据处理与模型计算。&lt;h4&gt;背景&lt;/h4&gt;现代时间序列分析需要灵活、高效和可扩展的框架，但许多现有的Python库在模块化以及对不规则、多源或稀疏数据的原生支持方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个研究导向的PyTorch框架，明确分离数据处理与模型计算，促进关注点的清晰分离，并加速实验过程。&lt;h4&gt;方法&lt;/h4&gt;pyFAST框架包含针对复杂场景设计的数据引擎，支持多源加载、蛋白质序列处理、高效的序列和补丁级填充、动态归一化，以及基于掩模的建模用于插值和预测。集成了受LLM启发的架构用于无对齐稀疏数据源融合，并提供原生稀疏度指标、专用损失函数和灵活的外部数据融合。&lt;h4&gt;主要发现&lt;/h4&gt;pyFAST提供了完整的经典和深度学习模型套件（线性模型、CNN、RNN、Transformer和GNN），采用模块化架构鼓励扩展。训练工具包括基于批次的流式聚合用于评估和设备协同以最大化计算效率。&lt;h4&gt;结论&lt;/h4&gt;pyFAST在GitHub上以MIT许可证发布，为推进时间序列研究和应用提供了一个紧凑而强大的平台。&lt;h4&gt;翻译&lt;/h4&gt;现代时间序列分析需要灵活、高效和可扩展的框架。然而，许多现有的Python库在模块化及其对不规则、多源或稀疏数据的原生支持方面存在局限性。我们介绍了pyFAST，一个面向研究的PyTorch框架，它明确地将数据处理与模型计算分离，促进关注点的清晰分离并加速实验。其数据引擎专为复杂场景设计，支持多源加载、蛋白质序列处理、高效的序列和补丁级填充、动态归一化，以及用于插值和预测的基于掩模的建模。pyFAST集成了受LLM启发的架构，用于无对齐稀疏数据源融合，并提供原生稀疏度指标、专用损失函数和灵活的外部数据融合。训练工具包括用于评估的基于批次的流式聚合和设备协同以最大化计算效率。在模块化架构中提供了完整的经典和深度学习模型套件（线性模型、CNN、RNN、Transformer和GNN），鼓励扩展。在GitHub上以MIT许可证发布，pyFAST为推进时间序列研究和应用提供了一个紧凑而强大的平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern time series analysis demands frameworks that are flexible, efficient,and extensible. However, many existing Python libraries exhibit limitations inmodularity and in their native support for irregular, multi-source, or sparsedata. We introduce pyFAST, a research-oriented PyTorch framework thatexplicitly decouples data processing from model computation, fostering acleaner separation of concerns and facilitating rapid experimentation. Its dataengine is engineered for complex scenarios, supporting multi-source loading,protein sequence handling, efficient sequence- and patch-level padding, dynamicnormalization, and mask-based modeling for both imputation and forecasting.pyFAST integrates LLM-inspired architectures for the alignment-free fusion ofsparse data sources and offers native sparse metrics, specialized lossfunctions, and flexible exogenous data fusion. Training utilities includebatch-based streaming aggregation for evaluation and device synergy to maximizecomputational efficiency. A comprehensive suite of classical and deep learningmodels (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within amodular architecture that encourages extension. Released under the MIT licenseat GitHub, pyFAST provides a compact yet powerful platform for advancing timeseries research and applications.</description>
      <author>example@mail.com (Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu)</author>
      <guid isPermaLink="false">2508.18891v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI</title>
      <link>http://arxiv.org/abs/2508.18766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures. Published in Applied and Computational  Engineering, Vol. 79, pp. 77-89, July 25, 2024. Licensed under CC BY 4.0&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为HGNN-DDI的异构图神经网络模型，用于预测潜在的药物-药物相互作用，通过整合多种药物相关数据源，实现了比传统方法更准确的预测结果。&lt;h4&gt;背景&lt;/h4&gt;药物-药物相互作用是临床实践中的一个主要问题，可能导致治疗效果降低或严重副作用。传统计算方法难以捕捉药物、靶点和生物实体之间的复杂关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效预测潜在药物-药物相互作用的计算模型，通过整合多种药物相关数据源提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;HGNN-DDI是一种异构图神经网络模型，利用图表示学习技术建模异构生物医学网络，实现不同节点和边类型之间的有效信息传播。&lt;h4&gt;主要发现&lt;/h4&gt;在基准DDI数据集上的实验结果表明，HGNN-DDI在预测准确性和鲁棒性方面优于现有的最先进基线方法。&lt;h4&gt;结论&lt;/h4&gt;HGNN-DDI模型有望支持更安全的药物开发和精准医疗实践。&lt;h4&gt;翻译&lt;/h4&gt;药物-药物相互作用是临床实践中的一个主要问题，因为它们可能导致治疗效果降低或严重的不良反应。传统的计算方法往往难以捕捉药物、靶点和生物实体之间的复杂关系。在本工作中，我们提出了HGNN-DDI，一种异构图神经网络模型，通过整合多种药物相关数据源来预测潜在的药物-药物相互作用。HGNN-DDI利用图表示学习来建模异构生物医学网络，使不同节点和边类型之间能够有效传播信息。在基准DDI数据集上的实验结果表明，HGNN-DDI在预测准确性和鲁棒性方面优于最先进的基线方法，凸显了其在支持更安全的药物开发和精准医疗方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.54254/2755-2721/79/20241329&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug-drug interactions (DDIs) are a major concern in clinical practice, asthey can lead to reduced therapeutic efficacy or severe adverse effects.Traditional computational approaches often struggle to capture the complexrelationships among drugs, targets, and biological entities. In this work, wepropose HGNN-DDI, a heterogeneous graph neural network model designed topredict potential DDIs by integrating multiple drug-related data sources.HGNN-DDI leverages graph representation learning to model heterogeneousbiomedical networks, enabling effective information propagation across diversenode and edge types. Experimental results on benchmark DDI datasets demonstratethat HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy androbustness, highlighting its potential to support safer drug development andprecision medicine.</description>
      <author>example@mail.com (Hongbo Liu, Siyi Li, Zheng Yu)</author>
      <guid isPermaLink="false">2508.18766v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>A Note on Graphon-Signal Analysis of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.18564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对Levie的图论信号分析论文进行了改进和扩展，解决了原始论文中限制其在实际图机器学习中应用的一些不足。&lt;h4&gt;背景&lt;/h4&gt;Levie的论文通过将MPNNs的输入空间（属性图/图信号）嵌入到属性图论（图信号）空间中分析了消息传递图神经网络，并基于图论分析标准结果的扩展证明了MPNNs的泛化边界和采样引理。&lt;h4&gt;目的&lt;/h4&gt;引入对现有结果的几种改进和扩展，以解决原始论文中限制其在实际应用中适用性的不足。&lt;h4&gt;方法&lt;/h4&gt;对原始论文进行四项主要改进：1)将结果扩展到多维信号的图信号；2)将Lipschitz连续性扩展到具有读出且关于切割距离的MPNNs；3)利用鲁棒性类型的泛化边界改进泛化边界；4)将分析扩展到非对称图论和核函数。&lt;h4&gt;主要发现&lt;/h4&gt;通过上述四项改进，解决了原始论文中的理论局限，使结果更适用于实际图机器学习场景。&lt;h4&gt;结论&lt;/h4&gt;这些改进和扩展增强了理论结果在实际应用中的适用性，为图神经网络的分析提供了更全面的理论基础。&lt;h4&gt;翻译&lt;/h4&gt;Levie最近发表的论文《图神经网络的图论信号分析》通过将MPNNs的输入空间（即属性图/图信号）嵌入到属性图论（图信号）空间中分析了消息传递图神经网络。基于图论分析标准结果向图信号的扩展，该论文证明了MPNNs的一个泛化边界和一个采样引理。然而，该论文存在一些缺失成分，限制了其在图机器学习实际应用场景中的适用性。在本文中，我们引入了几种对现有结果的改进和扩展，以解决这些不足。具体而言，1)我们将论文中的主要结果扩展到具有多维信号（而非一维信号）的图信号；2)我们将Lipschitz连续性扩展到具有读出且关于切割距离（而非无读出且关于切割度量）的MPNNs；3)我们通过利用鲁棒性类型的泛化边界改进了泛化边界；4)我们将分析扩展到非对称图论和核函数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', byLevie, analyzed message passing graph neural networks (MPNNs) by embedding theinput space of MPNNs, i.e., attributed graphs (graph-signals), to a space ofattributed graphons (graphon-signals). Based on extensions of standard resultsin graphon analysis to graphon-signals, the paper proved a generalization boundand a sampling lemma for MPNNs. However, there are some missing ingredients inthat paper, limiting its applicability in practical settings of graph machinelearning. In the current paper, we introduce several refinements and extensionsto existing results that address these shortcomings. In detail, 1) we extendthe main results in the paper to graphon-signals with multidimensional signals(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs withreadout with respect to cut distance (rather than MPNNs without readout withrespect to cut metric), 3) we improve the generalization bound by utilizingrobustness-type generalization bounds, and 4) we extend the analysis tonon-symmetric graphons and kernels.</description>
      <author>example@mail.com (Levi Rauchwerger, Ron Levie)</author>
      <guid isPermaLink="false">2508.18564v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</title>
      <link>http://arxiv.org/abs/2508.17630v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了量子图注意力网络（QGAT），一种将变分量子电路集成到注意力机制中的混合图神经网络。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络在处理复杂图结构数据时面临计算复杂度高和模型复杂等问题，需要新的方法来提高效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合量子计算优势的图神经网络模型，降低计算复杂度，提高处理图结构数据的能力。&lt;h4&gt;方法&lt;/h4&gt;1. 使用强纠缠量子电路和振幅编码的节点特征实现非线性交互；2. 采用单个量子电路同时生成多个注意力系数，而非传统多头注意力的分别计算；3. 通过量子并行实现参数共享，降低计算开销；4. 经典投影权重和量子电路参数端到端联合优化；5. 设计模块化结构便于与现有架构集成。&lt;h4&gt;主要发现&lt;/h4&gt;1. QGAT能有效捕捉复杂结构依赖关系；2. 在归纳场景中具有更好的泛化能力；3. 量子嵌入增强了特征和结构噪声的鲁棒性；4. 在处理真实世界噪声数据方面具有优势；5. 可以轻松增强基于注意力的经典模型。&lt;h4&gt;结论&lt;/h4&gt;QGAT是一种有前途的量子增强学习方法，在化学、生物学和网络分析等领域具有可扩展的应用潜力，其模块化设计使其能够轻松集成到现有架构中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了量子图注意力网络（QGAT），这是一种将变分量子电路集成到注意力机制中的混合图神经网络。其核心是QGAT采用强纠缠量子电路和振幅编码的节点特征来实现有表现力的非线性交互。与分别计算每个头的经典多头注意力不同，QGAT利用单个量子电路同时生成多个注意力系数。这种量子并行性促进了跨头的参数共享，显著降低了计算开销和模型复杂度。经典投影权重和量子电路参数以端到端方式联合优化，确保灵活适应学习任务。实证结果表明，QGAT在捕捉复杂结构依赖关系和在归纳场景中改进泛化方面有效，突显了其在化学、生物学和网络分析等跨领域可扩展量子增强学习方面的潜力。此外，实验证实量子嵌入增强了特征和结构噪声的鲁棒性，表明在处理真实世界噪声数据方面具有优势。QGAT的模块化也确保了与现有架构的简单集成，使其能够轻松增强基于注意力的经典模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neuralnetwork that integrates variational quantum circuits into the attentionmechanism. At its core, QGAT employs strongly entangling quantum circuits withamplitude-encoded node features to enable expressive nonlinear interactions.Distinct from classical multi-head attention that separately computes eachhead, QGAT leverages a single quantum circuit to simultaneously generatemultiple attention coefficients. This quantum parallelism facilitates parametersharing across heads, substantially reducing computational overhead and modelcomplexity. Classical projection weights and quantum circuit parameters areoptimized jointly in an end-to-end manner, ensuring flexible adaptation tolearning tasks. Empirical results demonstrate QGAT's effectiveness in capturingcomplex structural dependencies and improved generalization in inductivescenarios, highlighting its potential for scalable quantum-enhanced learningacross domains such as chemistry, biology, and network analysis. Furthermore,experiments confirm that quantum embedding enhances robustness against featureand structural noise, suggesting advantages in handling real-world noisy data.The modularity of QGAT also ensures straightforward integration into existingarchitectures, allowing it to easily augment classical attention-based models.</description>
      <author>example@mail.com (An Ning, Tai Yue Li, Nan Yow Chen)</author>
      <guid isPermaLink="false">2508.17630v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning</title>
      <link>http://arxiv.org/abs/2508.18730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为StructRTL的新型结构感知图自监督学习框架，用于改进RTL设计质量估计。该方法通过从控制数据流图(CDFG)中学习结构感知的表示，结合知识蒸馏策略，将后映射网表中的低级见解转移到CDFG预测器中，显著提升了质量估计性能。&lt;h4&gt;背景&lt;/h4&gt;在电子设计自动化(EDA)工作流中，估计寄存器传输级(RTL)设计质量至关重要，因为它可以提供关于面积和延迟等关键指标的即时反馈，而无需耗时的逻辑综合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确估计RTL设计质量的方法，重点关注结构语义，以提高估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为StructRTL的结构感知图自监督学习框架，通过从控制数据流图(CDFG)中学习结构感知的表示，并采用知识蒸馏策略将后映射网表中的低级见解转移到CDFG预测器中。&lt;h4&gt;主要发现&lt;/h4&gt;通过从CDFGs中学习结构感知的表示，该方法在各种质量估计任务上都显著优于先前的工作。结合结构学习和跨阶段监督的方法建立了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;结合结构学习和跨阶段监督的方法对于RTL设计质量估计是有效的，StructRTL框架能够显著提升估计性能。&lt;h4&gt;翻译&lt;/h4&gt;在电子设计自动化(EDA)工作流中估计寄存器传输级(RTL)设计质量至关重要，因为它可以提供关于面积和延迟等关键指标的即时反馈，而无需耗时的逻辑综合。虽然最近的方法利用大型语言模型(LLMs)从RTL代码中提取嵌入并取得了有希望的结果，但它们忽略了准确质量估计所必需的结构语义。相比之下，控制数据流图(CDFG)视图更明确地展示了设计的结构特征，为表示学习提供了更丰富的线索。在这项工作中，我们提出了一种新颖的、结构感知的图自监督学习框架StructRTL，用于改进RTL设计质量估计。通过从CDFGs中学习结构感知的表示，我们的方法在各种质量估计任务上都显著优于先前的工作。为了进一步提高性能，我们采用了一种知识蒸馏策略，将后映射网表中的低级见解转移到CDFG预测器中。实验表明，我们的方法建立了新的最先进结果，证明了结合结构学习和跨阶段监督的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the quality of register transfer level (RTL) designs is crucial inthe electronic design automation (EDA) workflow, as it enables instant feedbackon key metrics like area and delay without the need for time-consuming logicsynthesis. While recent approaches have leveraged large language models (LLMs)to derive embeddings from RTL code and achieved promising results, theyoverlook the structural semantics essential for accurate quality estimation. Incontrast, the control data flow graph (CDFG) view exposes the design'sstructural characteristics more explicitly, offering richer cues forrepresentation learning. In this work, we introduce a novel structure-awaregraph self-supervised learning framework, StructRTL, for improved RTL designquality estimation. By learning structure-informed representations from CDFGs,our method significantly outperforms prior art on various quality estimationtasks. To further boost performance, we incorporate a knowledge distillationstrategy that transfers low-level insights from post-mapping netlists into theCDFG predictor. Experiments show that our approach establishes newstate-of-the-art results, demonstrating the effectiveness of combiningstructural learning with cross-stage supervision.</description>
      <author>example@mail.com (Yi Liu, Hongji Zhang, Yiwen Wang, Dimitris Tsaras, Lei Chen, Mingxuan Yuan, Qiang Xu)</author>
      <guid isPermaLink="false">2508.18730v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection</title>
      <link>http://arxiv.org/abs/2508.18641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于聚类的特征空间表示学习方法，用于从拓片图像中自动检测甲骨文，解决了噪声和裂缝等退化因素对传统检测网络的限制问题。&lt;h4&gt;背景&lt;/h4&gt;甲骨文在理解中国古代文明中起关键作用。从拓片图像中自动检测甲骨文是数字考古学中的基础但具有挑战性的任务，主要由于噪声和裂缝等退化因素限制了传统检测网络的有效性。&lt;h4&gt;目的&lt;/h4&gt;解决甲骨文检测中的挑战，提高检测网络在退化因素下的性能。&lt;h4&gt;方法&lt;/h4&gt;利用甲骨文字体库数据集作为先验知识，通过基于聚类的表示学习增强检测网络中的特征提取。方法包含一个从聚类结果导出的特殊损失函数，用于优化特征表示，然后将其整合到网络总损失中。&lt;h4&gt;主要发现&lt;/h4&gt;在两个甲骨文检测数据集上使用三种主流检测框架（Faster R-CNN、DETR和Sparse R-CNN）进行实验验证，所有框架都显示出显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于聚类的特征空间表示学习方法能有效提高甲骨文检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;甲骨文在理解中国古代文明中起着至关重要的作用。从拓片图像中自动检测甲骨文是数字考古学中的一个基础但具有挑战性的任务，主要由于噪声和裂缝等各种退化因素限制了传统检测网络的有效性。为了解决这些挑战，我们提出了一种新颖的基于聚类的特征空间表示学习方法。我们的方法独特地利用甲骨文字体库数据集作为先验知识，通过基于聚类的表示学习增强检测网络中的特征提取。该方法包含一个从聚类结果导出的特殊损失函数，用于优化特征表示，然后将其整合到网络总损失中。我们通过在两个甲骨文检测数据集上使用三种主流检测框架（Faster R-CNN、DETR和Sparse R-CNN）进行实验来验证我们方法的有效性。通过大量实验，所有框架都显示出显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancientChinese civilization. The automated detection of OBIs from rubbing imagesrepresents a fundamental yet challenging task in digital archaeology, primarilydue to various degradation factors including noise and cracks that limit theeffectiveness of conventional detection networks. To address these challenges,we propose a novel clustering-based feature space representation learningmethod. Our approach uniquely leverages the Oracle Bones Character (OBC) fontlibrary dataset as prior knowledge to enhance feature extraction in thedetection network through clustering-based representation learning. The methodincorporates a specialized loss function derived from clustering results tooptimize feature representation, which is then integrated into the totalnetwork loss. We validate the effectiveness of our method by conductingexperiments on two OBIs detection dataset using three mainstream detectionframeworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensiveexperimentation, all frameworks demonstrate significant performanceimprovements.</description>
      <author>example@mail.com (Ye Tao, Xinran Fu, Honglin Pang, Xi Yang, Chuntao Li)</author>
      <guid isPermaLink="false">2508.18641v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</title>
      <link>http://arxiv.org/abs/2508.18166v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PCR-CA的端到端框架，用于改进应用商店推荐系统的点击率预测。该框架通过并行码本表示和对比对齐技术，解决了传统分类法无法捕捉多类别应用重叠语义的问题，特别是在处理长尾应用方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;现代应用商店推荐系统难以处理多类别应用，因为传统分类法无法捕捉重叠语义，导致个性化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为PCR-CA（并行码本表示与对比对齐）的端到端框架，用于改进点击率(CTR)预测。&lt;h4&gt;方法&lt;/h4&gt;PCR-CA首先从应用文本中提取紧凑的多模态嵌入，然后引入并行码本VQ-AE模块，并行学习多个码本上的离散语义表示，不同于分层残差量化(RQ-VAE)。这种设计能够独立编码不同方面（如游戏玩法、艺术风格），更好地建模多类别语义。此外，采用对比对齐损失桥接语义和协同信号，增强长尾项目的表示学习，并使用双注意力融合机制结合基于ID和语义的特征捕捉用户兴趣。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模数据集上的实验显示，PCR-CA相比强基线模型实现了+0.76%的AUC改进，对于长尾应用，AUC提升了+2.15%。在线A/B测试进一步验证了该方法，显示CTR提升+10.52%，转化率(CVR)提升+16.30%。&lt;h4&gt;结论&lt;/h4&gt;PCR-CA框架在真实部署中表现出色，已完全部署在Microsoft Store上。&lt;h4&gt;翻译&lt;/h4&gt;现代应用商店推荐系统难以处理多类别应用，因为传统分类法无法捕捉重叠语义，导致个性化效果不佳。我们提出了PCR-CA（并行码本表示与对比对齐），这是一个用于改进点击率预测的端到端框架。PCR-CA首先从应用文本中提取紧凑的多模态嵌入，然后引入并行码本VQ-AE模块，并行学习多个码本上的离散语义表示——这与分层残差量化(RQ-VAE)不同。这种设计能够独立编码不同方面（如游戏玩法、艺术风格），更好地建模多类别语义。为了桥接语义和协同信号，我们在用户和项目级别都采用了对比对齐损失，增强长尾项目的表示学习。此外，双注意力融合机制结合基于ID和语义的特征，捕捉用户兴趣，特别是对于长尾应用。在大规模数据集上的实验显示，PCR-CA相比强基线模型实现了+0.76%的AUC改进，对于长尾应用，AUC提升了+2.15%。在线A/B测试进一步验证了我们的方法，显示CTR提升+10.52%，转化率提升+16.30%，证明了PCR-CA在真实部署中的有效性。该新框架现已完全部署在Microsoft Store上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern app store recommender systems struggle with multiple-category apps, astraditional taxonomies fail to capture overlapping semantics, leading tosuboptimal personalization. We propose PCR-CA (Parallel CodebookRepresentations with Contrastive Alignment), an end-to-end framework forimproved CTR prediction. PCR-CA first extracts compact multimodal embeddingsfrom app text, then introduces a Parallel Codebook VQ-AE module that learnsdiscrete semantic representations across multiple codebooks in parallel --unlike hierarchical residual quantization (RQ-VAE). This design enablesindependent encoding of diverse aspects (e.g., gameplay, art style), bettermodeling multiple-category semantics. To bridge semantic and collaborativesignals, we employ a contrastive alignment loss at both the user and itemlevels, enhancing representation learning for long-tail items. Additionally, adual-attention fusion mechanism combines ID-based and semantic features tocapture user interests, especially for long-tail apps. Experiments on alarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strongbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing furthervalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvementin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The newframework has now been fully deployed on the Microsoft Store.</description>
      <author>example@mail.com (Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang)</author>
      <guid isPermaLink="false">2508.18166v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions</title>
      <link>http://arxiv.org/abs/2508.18313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM 2025 Full Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProtoEHR是一个可解释的分层原型学习框架，通过充分利用电子健康记录的多级结构来增强医疗预测的准确性、稳健性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;数字医疗系统使得在电子健康记录中收集大量医疗数据成为可能，为人工智能解决各种医疗预测任务提供了基础。然而，现有研究往往只关注EHR数据的孤立部分，限制了预测性能和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够充分利用EHR数据丰富多级结构的框架，以增强医疗预测的性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;ProtoEHR建模了EHR三个层次内部和之间的关系：医疗代码、医院就诊和患者。利用大型语言模型提取医疗代码间的语义关系并构建医疗知识图谱，设计分层表示学习框架捕捉三个层次的上下文化表示，并在每个层次中融入原型信息以捕获内在相似性并提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上对五个临床任务（死亡率预测、再入院预测、住院时长预测、药物推荐和表型预测）的评估表明，ProtoEHR相比基线方法能够做出更准确、稳健和可解释的预测，同时在代码、就诊和患者层面提供可解释的见解。&lt;h4&gt;结论&lt;/h4&gt;ProtoEHR通过有效利用EHR数据的分层结构，显著提升了医疗预测的性能和可解释性，为医疗决策提供了有价值的支持。&lt;h4&gt;翻译&lt;/h4&gt;数字医疗系统使得在电子健康记录中收集大量医疗数据成为可能，允许人工智能解决各种医疗预测任务。然而，现有研究往往只关注EHR数据的孤立部分，限制了它们的预测性能和可解释性。为了解决这一差距，我们提出了ProtoEHR，一个可解释的分层原型学习框架，它充分利用EHR数据的丰富多级结构来增强医疗预测。更具体地说，ProtoEHR建模了EHR三个层次内部和之间的关系：医疗代码、医院就诊和患者。我们首先利用大型语言模型提取医疗代码之间的语义关系，并构建医疗知识图谱作为知识源。在此基础上，我们设计了一个分层表示学习框架，捕捉三个层次的上下文化表示，同时在每个层次中融入原型信息以捕获内在相似性并提高泛化能力。为了进行全面评估，我们在两个公共数据集上对五个临床重要任务评估了ProtoEHR，包括死亡率预测、再入院预测、住院时长预测、药物推荐和表型预测。结果表明，与文献中的基线相比，ProtoEHR能够做出准确、稳健和可解释的预测。此外，ProtoEHR在代码、就诊和患者层面提供可解释的见解，以辅助医疗预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital healthcare systems have enabled the collection of mass healthcaredata in electronic healthcare records (EHRs), allowing artificial intelligencesolutions for various healthcare prediction tasks. However, existing studiesoften focus on isolated components of EHR data, limiting their predictiveperformance and interpretability. To address this gap, we propose ProtoEHR, aninterpretable hierarchical prototype learning framework that fully exploits therich, multi-level structure of EHR data to enhance healthcare predictions. Morespecifically, ProtoEHR models relationships within and across threehierarchical levels of EHRs: medical codes, hospital visits, and patients. Wefirst leverage large language models to extract semantic relationships amongmedical codes and construct a medical knowledge graph as the knowledge source.Building on this, we design a hierarchical representation learning frameworkthat captures contextualized representations across three levels, whileincorporating prototype information within each level to capture intrinsicsimilarities and improve generalization. To perform a comprehensive assessment,we evaluate ProtoEHR in two public datasets on five clinically significanttasks, including prediction of mortality, prediction of readmission, predictionof length of stay, drug recommendation, and prediction of phenotype. Theresults demonstrate the ability of ProtoEHR to make accurate, robust, andinterpretable predictions compared to baselines in the literature. Furthermore,ProtoEHR offers interpretable insights on code, visit, and patient levels toaid in healthcare prediction.</description>
      <author>example@mail.com (Zi Cai, Yu Liu, Zhiyao Luo, Tingting Zhu)</author>
      <guid isPermaLink="false">2508.18313v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling</title>
      <link>http://arxiv.org/abs/2508.19028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为gradstop的随机早停方法，仅使用梯度信息进行模型训练的早停判断，无需单独验证集，从而充分利用全部数据训练模型。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型通常通过在训练数据上使用梯度下降算法最小化损失函数来学习，但这些模型经常出现过拟合问题，导致在未见数据上的预测性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖于验证集的早停方法，以解决传统早停方法减少可用训练数据的问题，特别是在数据有限的场景下。&lt;h4&gt;方法&lt;/h4&gt;gradstop方法通过梯度信息估计贝叶斯后验，将早停问题定义为从这个后验中采样，并使用近似后验获得停止标准。&lt;h4&gt;主要发现&lt;/h4&gt;gradstop在测试数据上实现了较小的损失，与基于验证集的停止标准相比表现良好；该方法可以充分利用整个数据集进行训练，在数据有限的情况下（如迁移学习）特别有利。&lt;h4&gt;结论&lt;/h4&gt;gradstop可以作为梯度下降库的一个可选功能添加，计算开销很小，源代码已在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型通常通过在训练数据上使用梯度下降算法最小化损失函数来学习。这些模型经常出现过拟合问题，导致在未见数据上的预测性能下降。标准的解决方案是使用保留的验证集进行早停，当验证损失停止减小时停止最小化过程。然而，这个保留集减少了可用于训练的数据量。本文提出了gradstop，一种新颖的随机早停方法，仅使用梯度信息（这些信息由梯度下降算法'免费'产生）。我们的主要贡献是：我们使用梯度信息估计贝叶斯后验，将早停问题定义为从这个后验中采样，并使用近似后验获得停止标准。我们的实证评估表明，gradstop在测试数据上实现了较小的损失，并且与基于验证集的停止标准相比表现良好。通过利用整个数据集进行训练，我们的方法在数据有限的设置（如迁移学习）中特别有利。它可以作为梯度下降库的一个可选功能添加，计算开销很小。源代码可在https://github.com/edahelsinki/gradstop获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning models are often learned by minimising a loss function onthe training data using a gradient descent algorithm. These models often sufferfrom overfitting, leading to a decline in predictive performance on unseendata. A standard solution is early stopping using a hold-out validation set,which halts the minimisation when the validation loss stops decreasing.However, this hold-out set reduces the data available for training. This paperpresents {\sc gradstop}, a novel stochastic early stopping method that onlyuses information in the gradients, which are produced by the gradient descentalgorithm ``for free.'' Our main contributions are that we estimate theBayesian posterior by the gradient information, define the early stoppingproblem as drawing sample from this posterior, and use the approximatedposterior to obtain a stopping criterion. Our empirical evaluation shows that{\sc gradstop} achieves a small loss on test data and compares favourably to avalidation-set-based stopping criterion. By leveraging the entire dataset fortraining, our method is particularly advantageous in data-limited settings,such as transfer learning. It can be incorporated as an optional feature ingradient descent libraries with only a small computational overhead. The sourcecode is available at https://github.com/edahelsinki/gradstop.</description>
      <author>example@mail.com (Arash Jamshidi, Lauri Seppäläinen, Katsiaryna Haitsiukevich, Hoang Phuc Hau Luu, Anton Björklund, Kai Puolamäki)</author>
      <guid isPermaLink="false">2508.19028v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</title>
      <link>http://arxiv.org/abs/2508.18860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了C-Flat方法，用于持续学习中的平坦损失景观优化，以提高知识保留和学习效率。&lt;h4&gt;背景&lt;/h4&gt;在持续学习中，平衡对新任务的敏感性和保留过去知识的稳定性至关重要。尖锐度感知最小化已在迁移学习中证明有效，并应用于持续学习，但仅依赖零阶尖锐度可能导致选择不够鲁棒的尖锐极小值。&lt;h4&gt;目的&lt;/h4&gt;开发一种促进适合持续学习的平坦损失景观的方法，以提高知识保留的鲁棒性和学习效率。&lt;h4&gt;方法&lt;/h4&gt;提出C-Flat(Continual Flatness)方法，具有即插即用兼容性；开发通用框架将C-Flat整合到所有主要CL范式中；引入C-Flat++，利用选择性平坦度驱动提升，显著降低更新成本。&lt;h4&gt;主要发现&lt;/h4&gt;C-Flat在各种设置中持续提高性能；广泛实验证明所提方法在多种CL方法、数据集和场景中具有有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;C-Flat和C-Flat++是持续学习中有效的平坦度优化方法，能够提高知识保留和学习效率，同时具有较低的计算成本。&lt;h4&gt;翻译&lt;/h4&gt;在持续学习中，平衡对新任务的敏感性和保留过去知识的稳定性至关重要。最近，尖锐度感知最小化已被证明在迁移学习中有效，并已被应用于持续学习以提高记忆保留和学习效率。然而，仅依赖零阶尖锐度可能在某些情况下 favor 更尖锐的极小值而非更平坦的极小值，导致解决方案不够鲁棒且可能次优。本文提出C-Flat(Continual Flatness)方法，旨在促进适合持续学习的平坦损失景观。C-Flat具有即插即用兼容性，可轻松集成到代码管道中。此外，我们提出了一个通用框架，将C-Flat整合到所有主要的持续学习范式中，并与基于损失极小值优化的方法和基于平坦极小值的持续学习方法进行了全面比较。我们的结果显示C-Flat在各种设置中持续提高性能。此外，我们引入了C-Flat++，这是一个高效且有效的框架，利用选择性平坦度驱动提升，显著减少了C-Flat所需的更新成本。在多种持续学习方法、数据集和场景中的广泛实验证明了所提方法的有效性和效率。代码可在https://github.com/WanNaa/C-Flat获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Balancing sensitivity to new tasks and stability for retaining past knowledgeis crucial in continual learning (CL). Recently, sharpness-aware minimizationhas proven effective in transfer learning and has also been adopted incontinual learning (CL) to improve memory retention and learning efficiency.However, relying on zeroth-order sharpness alone may favor sharper minima overflatter ones in certain settings, leading to less robust and potentiallysuboptimal solutions. In this paper, we propose \textbf{C}ontinual\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter losslandscapes tailored for CL. C-Flat offers plug-and-play compatibility, enablingeasy integration with minimal modifications to the code pipeline. Besides, wepresent a general framework that integrates C-Flat into all major CL paradigmsand conduct comprehensive comparisons with loss-minima optimizers andflat-minima-based CL methods. Our results show that C-Flat consistentlyimproves performance across a wide range of settings. In addition, we introduceC-Flat++, an efficient yet effective framework that leverages selectiveflatness-driven promotion, significantly reducing the update cost required byC-Flat. Extensive experiments across multiple CL methods, datasets, andscenarios demonstrate the effectiveness and efficiency of our proposedapproaches. Code is available at https://github.com/WanNaa/C-Flat.</description>
      <author>example@mail.com (Wei Li, Hangjie Yuan, Zixiang Zhao, Yifan Zhu, Aojun Lu, Tao Feng, Yanan Sun)</author>
      <guid isPermaLink="false">2508.18860v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency</title>
      <link>http://arxiv.org/abs/2508.18693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为特征空间平面搜索器(FPS)的新型领域自适应框架，通过利用预训练模型特征空间中的几何模式优化决策边界，同时保持特征编码器冻结，解决了领域偏移问题，实现了与最先进方法相当或更优的性能，同时降低了计算成本。&lt;h4&gt;背景&lt;/h4&gt;领域偏移问题，即当模型从有标签的源领域迁移到无标签的目标领域时性能下降，对深度学习系统的部署构成了持续挑战。当前的无监督领域自适应方法主要依赖于微调特征提取器，这种方法效率低下、可解释性差，且难以扩展到现代架构。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的领域自适应方法，解决现有微调方法效率低、可解释性差和难以扩展的问题，同时保持或提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出特征空间平面搜索器(FPS)，一种新型领域自适应框架，它利用预训练模型特征空间中的领域不变几何模式(类内聚类和类间分离)优化决策边界，同时保持特征编码器冻结，避免引入不可预测的特征失真。&lt;h4&gt;主要发现&lt;/h4&gt;预训练在大规模数据上的模型在特征空间中表现出领域不变的几何模式，包括类内聚类和类间分离，从而保留了可迁移的判别结构；领域偏移主要表现为边界不对齐，而非特征退化。&lt;h4&gt;结论&lt;/h4&gt;FPS通过离线特征提取显著降低了内存和计算成本，允许在单个计算周期内完成全数据集优化，在公共基准测试上与最先进方法相当或更优，并能高效扩展到多模态大模型，在不同领域展现出多功能性，为迁移学习提供了一种简单、有效且通用的范式。&lt;h4&gt;翻译&lt;/h4&gt;领域偏移表现为当模型从有标签的源领域过渡到无标签的目标领域时性能下降，这对部署深度学习系统构成了持续挑战。当前的无监督领域自适应方法主要依赖于微调特征提取器——这种方法效率低下、可解释性差，且难以扩展到现代架构。我们的分析显示，在大规模数据上预训练的模型在特征空间中表现出领域不变的几何模式，特征为类内聚类和类间分离，从而保留了可迁移的判别结构。这些发现表明，领域偏移主要表现为边界不对齐而非特征退化。与微调整个预训练模型(可能引入不可预测的特征失真)不同，我们提出特征空间平面搜索器(FPS)：一种新型领域自适应框架，它通过利用这些几何模式优化决策边界，同时保持特征编码器冻结。这种简化的方法使自适应过程具有可解释性分析，并通过离线特征提取显著降低内存和计算成本，允许在单个计算周期内完成全数据集优化。在公共基准测试上的评估表明，FPS的性能与最先进方法相当或更优。FPS能够高效扩展到多模态大模型，并在蛋白质结构预测、遥感分类和地震检测等不同领域展现出多功能性。我们预期FPS将为迁移学习，特别是在领域自适应任务中，提供一种简单、有效且通用的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain shift, characterized by degraded model performance during transitionfrom labeled source domains to unlabeled target domains, poses a persistentchallenge for deploying deep learning systems. Current unsupervised domainadaptation (UDA) methods predominantly rely on fine-tuning feature extractors -an approach limited by inefficiency, reduced interpretability, and poorscalability to modern architectures.  Our analysis reveals that models pretrained on large-scale data exhibitdomain-invariant geometric patterns in their feature space, characterized byintra-class clustering and inter-class separation, thereby preservingtransferable discriminative structures. These findings indicate that domainshifts primarily manifest as boundary misalignment rather than featuredegradation.  Unlike fine-tuning entire pre-trained models - which risks introducingunpredictable feature distortions - we propose the Feature-space PlanesSearcher (FPS): a novel domain adaptation framework that optimizes decisionboundaries by leveraging these geometric patterns while keeping the featureencoder frozen. This streamlined approach enables interpretative analysis ofadaptation while substantially reducing memory and computational costs throughoffline feature extraction, permitting full-dataset optimization in a singlecomputation cycle.  Evaluations on public benchmarks demonstrate that FPS achieves competitive orsuperior performance to state-of-the-art methods. FPS scales efficiently withmultimodal large models and shows versatility across diverse domains includingprotein structure prediction, remote sensing classification, and earthquakedetection. We anticipate FPS will provide a simple, effective, andgeneralizable paradigm for transfer learning, particularly in domain adaptationtasks. .</description>
      <author>example@mail.com (Zhitong Cheng, Yiran Jiang, Yulong Ge, Yufeng Li, Zhongheng Qin, Rongzhi Lin, Jianwei Ma)</author>
      <guid isPermaLink="false">2508.18693v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection</title>
      <link>http://arxiv.org/abs/2508.17567v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了在医学图像分类任务中使用不同CNN架构和预训练方法的效果，确定了用于乳腺病变恶性和ACL撕裂检测的最佳CNN架构，并比较了RadImageNet和ImageNet预训练的效果。&lt;h4&gt;背景&lt;/h4&gt;现代计算机视觉模型在医学图像分类和分割任务中非常有用，但医学图像数据的稀缺性限制了从头开始训练的模型的效果。迁移学习已成为解决这一问题的关键方案。Mei等人(2022)发现，在放射科医生标注的大型图像数据集(RadImageNet)上预训练CNN，相比ImageNet预训练，能提高下游任务中的模型性能。&lt;h4&gt;目的&lt;/h4&gt;扩展Mei等人(2022)的工作，通过全面调查确定用于乳腺病变恶性和ACL撕裂检测的最佳CNN架构，并进行统计分析，比较RadImageNet和ImageNet预训练对下游模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;采用1维卷积分类器与跳跃连接、ResNet50预训练骨干网络以及部分骨干网络解冻的方法，用于下游医学分类任务。研究比较了在RadImageNet和ImageNet上预训练的效果。&lt;h4&gt;主要发现&lt;/h4&gt;1. 1维卷积分类器与跳跃连接、ResNet50预训练骨干网络以及部分骨干网络解冻的方法能产生最佳的下游医学分类性能。2. 最佳模型在ACL撕裂检测中达到0.9969的AUC值，在乳腺结节恶性检测中达到0.9641的AUC值。3. 这些结果与Mei等人(2022)报告的结果具有竞争力，并超过了之前的工作。4. 没有证据表明RadImageNet预训练在ACL撕裂和乳腺病变分类任务中能提供优越的下游性能。&lt;h4&gt;结论&lt;/h4&gt;研究确定了用于医学图像分类的最佳CNN架构和预训练方法，并发现RadImageNet预训练不一定比ImageNet预训练在特定医学任务上表现更好。最佳模型在ACL撕裂检测和乳腺结节恶性检测中达到了高准确率，与之前的研究相比具有竞争力或更优。&lt;h4&gt;翻译&lt;/h4&gt;现代计算机视觉模型已被证明对医学图像分类和分割任务非常有用，但医学图像数据的稀缺性常常限制了从头开始训练的模型的效果。迁移学习已成为解决这一问题的关键方案，使高性能模型能够在小数据集上进行微调。Mei等人(2022)发现，在放射科医生标注的大型图像数据集(RadImageNet)上预训练CNN，相比ImageNet预训练，能提高下游任务中的模型性能。本研究通过全面调查扩展了Mei等人(2022)的工作，确定了用于乳腺病变恶性和ACL撕裂检测的最佳CNN架构，并进行了统计分析，比较了RadImageNet和ImageNet预训练对下游模型性能的影响。我们的研究结果表明，具有跳跃连接的1维卷积分类器、ResNet50预训练骨干网络以及部分骨干网络解冻的方法能产生最佳的下游医学分类性能。我们的最佳模型在ACL撕裂检测中达到0.9969的AUC值，在乳腺结节恶性检测中达到0.9641的AUC值，与Mei等人(2022)报告的结果具有竞争力，并超过了其他先前的工作。我们没有发现证据证实RadImageNet预训练在ACL撕裂和乳腺病变分类任务中能提供优越的下游性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern computer vision models have proven to be highly useful for medicalimaging classification and segmentation tasks, but the scarcity of medicalimaging data often limits the efficacy of models trained from scratch. Transferlearning has emerged as a pivotal solution to this, enabling the fine-tuning ofhigh-performance models on small data. Mei et al. (2022) found thatpre-training CNNs on a large dataset of radiologist-labeled images(RadImageNet) enhanced model performance on downstream tasks compared toImageNet pretraining. The present work extends Mei et al. (2022) by conductinga comprehensive investigation to determine optimal CNN architectures for breastlesion malignancy detection and ACL tear detection, as well as performingstatistical analysis to compare the effect of RadImageNet and ImageNetpre-training on downstream model performance. Our findings suggest that1-dimensional convolutional classifiers with skip connections, ResNet50pre-trained backbones, and partial backbone unfreezing yields optimaldownstream medical classification performance. Our best models achieve AUCs of0.9969 for ACL tear detection and 0.9641 for breast nodule malignancydetection, competitive with the results reported by Mei et al. (2022) andsurpassing other previous works. We do not find evidence confirming RadImageNetpre-training to provide superior downstream performance for ACL tear and breastlesion classification tasks.</description>
      <author>example@mail.com (Daniel Frees, Moritz Bolling, Aditri Bhagirath)</author>
      <guid isPermaLink="false">2508.17567v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Generative Data Augmentation for Object Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2505.17783v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于3D扩散模型的生成数据增强方法，用于点云分割任务，解决了传统数据增强方法数据多样性不足和先进生成模型缺乏语义标签的问题。&lt;h4&gt;背景&lt;/h4&gt;数据增强被广泛用于训练深度学习模型以解决数据稀缺问题，但传统数据增强方法依赖简单几何变换，导致数据多样性有限，模型性能提升有限。先进的3D形状生成模型能生成逼真点云，但缺乏点级语义标签，限制了其在点云分割任务中的应用。&lt;h4&gt;目的&lt;/h4&gt;弥合数据增强技术与先进扩散模型之间的差距，开发能够基于给定分割掩码生成高质量点云的感知生成模型，并引入点云分割训练的生成数据增强管道。&lt;h4&gt;方法&lt;/h4&gt;将最先进的3D扩散模型Lion扩展为感知生成模型，可基于给定分割掩码生成高质量点云；引入三步生成数据增强(GDA)管道，利用少量标记样本生成变体和伪标记样本，并通过基于扩散的伪标记过滤方法验证。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模合成数据集和一个真实世界医学数据集上的实验表明，所提出的GDA方法优于传统数据增强方法以及相关的半监督和自监督方法。&lt;h4&gt;结论&lt;/h4&gt;通过将先进的3D扩散模型扩展为感知生成模型并引入三步生成数据增强管道，成功解决了传统数据增强方法的局限性，为点云分割任务提供了更有效的数据增强方法。&lt;h4&gt;翻译&lt;/h4&gt;数据增强被广泛用于训练深度学习模型以解决数据稀缺问题。然而，传统的数据增强方法通常依赖简单的几何变换，如随机旋转和重新缩放，导致数据多样性有限，模型性能提升有限。最先进的3D形状生成模型依赖于去噪扩散概率模型，能够生成逼真的新点云用于3D内容创建和操作。然而，生成的3D形状缺乏相关的点级语义标签，限制了它们在扩大点云分割任务训练数据方面的应用。为了弥合数据增强技术与先进扩散模型之间的差距，我们将最先进的3D扩散模型Lion扩展为一个感知生成模型，该模型可以基于给定的分割掩码生成高质量的点云。利用这个新颖的生成模型，我们引入了一个三步生成数据增强(GDA)管道用于点云分割训练。我们的GDA方法只需要少量标记样本，但通过生成的变体和伪标记样本来丰富训练数据，这些伪标记样本通过一种新颖的基于扩散的伪标记过滤方法进行验证。在两个大规模合成数据集和一个真实世界医学数据集上进行的大量实验表明，我们的GDA方法优于传统数据增强方法以及相关的半监督和自监督方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云分割任务中标注数据稀缺的问题。传统数据增强方法仅使用简单几何变换，导致数据多样性不足，模型性能提升有限。这个问题在现实中很重要，因为在医疗、自动驾驶等领域创建3D点云分割标注成本高、耗时长，而标注数据不足限制了深度学习模型的性能表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统数据增强方法的局限性，然后注意到最先进的3D生成模型能生成高质量点云但缺乏语义标签。他们借鉴了Lion扩散模型作为基础，结合半监督学习思想，通过在全局和局部编码级别整合分割信息，使模型能够理解语义部分。他们还利用了扩散模型的去噪过程，这是一种已在2D图像生成中证明有效的方法，并采用了点云处理中的PVCNN架构作为基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个部分感知的生成模型，能根据给定分割掩码生成高质量点云，并利用它进行数据增强。整体流程是三步GDA管道：1）半监督训练生成模型，结合标注和未标注数据；2）变体生成，通过扩散-去噪过程创建多样化的标注样本；3）伪标签生成与过滤，使用临时分割模型为未标注数据生成伪标签，并通过基于扩散的伪标签过滤方法评估质量，过滤掉不准确样本。最终将原始标注数据、生成的变体和过滤后的伪标签一起用于训练最终分割模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）部分感知的生成模型，扩展Lion模型使其能根据分割掩码生成点云；2）三步GDA管道，包括半监督训练、变体生成和伪标签过滤；3）条件重建差异(CRD)评估方法，通过计算条件重建差异判断伪标签准确性。相比之前工作，本文首次将扩散模型应用于3D点云分割的数据增强，解决了现有3D生成模型缺乏语义标签的问题，提供了比传统数据增强更丰富的数据多样性，比半监督和自监督方法提供更高质量训练数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于部分感知生成模型的三步生成数据增强方法，通过扩散模型生成多样化且带有语义标签的点云变体，并利用条件重建差异过滤伪标签，显著提升了在有限标注数据情况下的3D点云分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data augmentation is widely used to train deep learning models to addressdata scarcity. However, traditional data augmentation (TDA) typically relies onsimple geometric transformation, such as random rotation and rescaling,resulting in minimal data diversity enrichment and limited model performanceimprovement. State-of-the-art generative models for 3D shape generation rely onthe denoising diffusion probabilistic models and manage to generate realisticnovel point clouds for 3D content creation and manipulation. Nevertheless, thegenerated 3D shapes lack associated point-wise semantic labels, restrictingtheir usage in enlarging the training data for point cloud segmentation tasks.To bridge the gap between data augmentation techniques and the advanceddiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to apart-aware generative model that can generate high-quality point cloudsconditioned on given segmentation masks. Leveraging the novel generative model,we introduce a 3-step generative data augmentation (GDA) pipeline for pointcloud segmentation training. Our GDA approach requires only a small amount oflabeled samples but enriches the training data with generated variants andpseudo-labeled samples, which are validated by a novel diffusion-basedpseudo-label filtering method. Extensive experiments on two large-scalesynthetic datasets and a real-world medical dataset demonstrate that our GDAmethod outperforms TDA approach and related semi-supervised and self-supervisedmethods.</description>
      <author>example@mail.com (Dekai Zhu, Stefan Gavranovic, Flavien Boussuge, Benjamin Busam, Slobodan Ilic)</author>
      <guid isPermaLink="false">2505.17783v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>SoccerNet 2025 Challenges Results</title>
      <link>http://arxiv.org/abs/2508.19182v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SoccerNet 2025挑战赛是第五年度的开放基准测试活动，专注于足球视频理解中的计算机视觉研究。今年包含四个视觉任务：团队球类动作识别、单目深度估计、多视角犯规识别和比赛状态重建。&lt;h4&gt;背景&lt;/h4&gt;SoccerNet是一个持续进行的开放基准测试活动，旨在推动计算机视觉、人工智能和体育交叉领域的研究。&lt;h4&gt;目的&lt;/h4&gt;通过提供大规模标注数据集、统一评估协议和强大基线，促进足球视频理解领域的研究进展。&lt;h4&gt;方法&lt;/h4&gt;参与者需要完成四个视觉任务：1)团队球类动作识别，检测足球广播中的球相关动作并分配给各队伍；2)单目深度估计，通过单摄像头广播片段恢复场景几何；3)多视角犯规识别，分析多个同步摄像头视图以分类犯规及其严重程度；4)比赛状态重建，从广播视频中定位并识别所有球员，重建比赛在球场2D俯视图上的状态。&lt;h4&gt;主要发现&lt;/h4&gt;报告呈现了每个挑战的结果，突出了表现最佳的解决方案，并提供了社区取得进展的见解。&lt;h4&gt;结论&lt;/h4&gt;SoccerNet挑战赛继续成为推动可复现、开放研究的驱动力，促进计算机视觉、人工智能和体育领域的交叉研究。&lt;h4&gt;翻译&lt;/h4&gt;SoccerNet 2025挑战赛标志着SoccerNet开放基准测试活动的第五个年度版本，致力于推进足球视频理解中的计算机视觉研究。今年的挑战涵盖四个视觉任务：(1)团队球类动作识别，专注于检测足球广播中的球相关动作并将动作分配给各队伍；(2)单目深度估计，旨在通过单摄像头广播片段的相对深度估计来恢复场景几何；(3)多视角犯规识别，需要分析多个同步摄像头视图以分类犯规及其严重程度；(4)比赛状态重建，旨在从广播视频中定位并识别所有球员，重建球场2D俯视图上的比赛状态。在所有任务中，参与者都获得了大规模标注数据集、统一评估协议和强基线作为起点。本报告呈现了每个挑战的结果，突出了表现最佳的解决方案，并提供了社区取得进展的见解。SoccerNet挑战赛继续成为推动计算机视觉、人工智能和体育交叉领域可复现、开放研究的驱动力。有关任务、挑战和排行榜的详细信息可在https://www.soccer-net.org找到，基线和开发工具包可在https://github.com/SoccerNet获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文报告了SoccerNet 2025挑战赛的结果，旨在通过四个视觉任务推动足球视频理解的计算机视觉研究：1)团队球类动作定位，2)单目深度估计，3)多视角犯规识别，4)游戏状态重建。这些问题在现实中对体育广播增强、裁判辅助系统、球队分析和战术制定有重要价值，在研究中则促进了计算机视觉在时空推理、多模态学习和体育视频理解领域的进步，提供了一个开放、可复现的研究平台。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这篇论文是挑战赛结果报告，而非提出新方法。SoccerNet挑战赛设计借鉴了前几年成功经验，保留了核心任务但增加了复杂度（如团队归属判断），引入了新任务（如单目深度估计），并扩展了现有基准。参赛团队方法基于现有计算机视觉技术（如预训练模型Depth Anything V2、TAdaFormer等），针对足球场景特点进行定制优化，结合领域知识和时空上下文，体现了对现有工作的继承与创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过四个互补任务全面评估足球视频理解进展：1)识别球类动作并确定执行团队；2)从单摄像头估计相对深度图；3)利用多视角分类犯规类型和严重程度；4)重建游戏状态（玩家位置、角色和团队）。整体流程包括：收集标注足球比赛视频；定义任务目标和评估指标；开发基线方法；团队提交解决方案；使用标准评估指标评估排名；分析最佳方法特点和整体趋势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)任务创新（增加团队归属判断、引入单目深度估计、扩展犯规分类、改进游戏状态重建）；2)数据创新（更大规模密集注释数据集、新数据模态、使用合成数据）；3)评估创新（专门评估指标、处理类别不平衡的指标、特定任务评估指标）；4)方法创新（鼓励预训练模型、领域特定优化、多模态学习应用）。相比之前工作，2025年挑战增加了任务复杂度，引入新任务领域，提供更丰富数据集，推动跨领域技术应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过SoccerNet 2025挑战赛的四个任务展示了足球视频理解的最新进展，推动了开放、可复现的体育视频分析研究，并为未来研究和应用提供了新的基准和方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNetopen benchmarking effort, dedicated to advancing computer vision research infootball video understanding. This year's challenges span four vision-basedtasks: (1) Team Ball Action Spotting, focused on detecting ball-related actionsin football broadcasts and assigning actions to teams; (2) Monocular DepthEstimation, targeting the recovery of scene geometry from single-camerabroadcast clips through relative depth estimation for each pixel; (3)Multi-View Foul Recognition, requiring the analysis of multiple synchronizedcamera views to classify fouls and their severity; and (4) Game StateReconstruction, aimed at localizing and identifying all players from abroadcast video to reconstruct the game state on a 2D top-view of the field.Across all tasks, participants were provided with large-scale annotateddatasets, unified evaluation protocols, and strong baselines as startingpoints. This report presents the results of each challenge, highlights thetop-performing solutions, and provides insights into the progress made by thecommunity. The SoccerNet Challenges continue to serve as a driving force forreproducible, open research at the intersection of computer vision, artificialintelligence, and sports. Detailed information about the tasks, challenges, andleaderboards can be found at https://www.soccer-net.org, with baselines anddevelopment kits available at https://github.com/SoccerNet.</description>
      <author>example@mail.com (Silvio Giancola, Anthony Cioppa, Marc Gutiérrez-Pérez, Jan Held, Carlos Hinojosa, Victor Joos, Arnaud Leduc, Floriane Magera, Karen Sanchez, Vladimir Somers, Artur Xarles, Antonio Agudo, Alexandre Alahi, Olivier Barnich, Albert Clapés, Christophe De Vleeschouwer, Sergio Escalera, Bernard Ghanem, Thomas B. Moeslund, Marc Van Droogenbroeck, Tomoki Abe, Saad Alotaibi, Faisal Altawijri, Steven Araujo, Xiang Bai, Xiaoyang Bi, Jiawang Cao, Vanyi Chao, Kamil Czarnogórski, Fabian Deuser, Mingyang Du, Tianrui Feng, Patrick Frenzel, Mirco Fuchs, Jorge García, Konrad Habel, Takaya Hashiguchi, Sadao Hirose, Xinting Hu, Yewon Hwang, Ririko Inoue, Riku Itsuji, Kazuto Iwai, Hongwei Ji, Yangguang Ji, Licheng Jiao, Yuto Kageyama, Yuta Kamikawa, Yuuki Kanasugi, Hyungjung Kim, Jinwook Kim, Takuya Kurihara, Bozheng Li, Lingling Li, Xian Li, Youxing Lian, Dingkang Liang, Hongkai Lin, Jiadong Lin, Jian Liu, Liang Liu, Shuaikun Liu, Zhaohong Liu, Yi Lu, Federico Méndez, Huadong Ma, Wenping Ma, Jacek Maksymiuk, Henry Mantilla, Ismail Mathkour, Daniel Matthes, Ayaha Motomochi, Amrulloh Robbani Muhammad, Haruto Nakayama, Joohyung Oh, Yin May Oo, Marcelo Ortega, Norbert Oswald, Rintaro Otsubo, Fabian Perez, Mengshi Qi, Cristian Rey, Abel Reyes-Angulo, Oliver Rose, Hoover Rueda-Chacón, Hideo Saito, Jose Sarmiento, Kanta Sawafuji, Atom Scott, Xi Shen, Pragyan Shrestha, Jae-Young Sim, Long Sun, Yuyang Sun, Tomohiro Suzuki, Licheng Tang, Masato Tonouchi, Ikuma Uchida, Henry O. Velesaca, Tiancheng Wang, Rio Watanabe, Jay Wu, Yongliang Wu, Shunzo Yamagishi, Di Yang, Xu Yang, Yuxin Yang, Hao Ye, Xinyu Ye, Calvin Yeung, Xuanlong Yu, Chao Zhang, Dingyuan Zhang, Kexing Zhang, Zhe Zhao, Xin Zhou, Wenbo Zhu, Julian Ziegler)</author>
      <guid isPermaLink="false">2508.19182v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling</title>
      <link>http://arxiv.org/abs/2508.18922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出HierCVAE架构，结合分层注意机制和条件变分自编码器，用于复杂系统的时间建模，在能源消耗预测任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;复杂系统中的时间建模需要捕捉多时间尺度的依赖关系，同时处理固有的不确定性。&lt;h4&gt;目的&lt;/h4&gt;提出HierCVAE，一种结合了分层注意机制与条件变分自编码器的新架构，以应对时间建模中的挑战。&lt;h4&gt;方法&lt;/h4&gt;HierCVAE采用三层注意结构（局部、全局、跨时间）结合多模态条件编码，捕捉时间、统计和趋势信息；在潜在空间中融入ResFormer块，并通过预测头提供明确的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;在能源消耗数据集上的评估显示，HierCVAE相比最先进方法实现了15-40%的预测精度提升，并在不确定性校准方面表现优越，特别擅长长期预测和复杂的多变量依赖关系。&lt;h4&gt;结论&lt;/h4&gt;HierCVAE是一种有效的时间建模方法，能够处理多时间尺度的依赖关系并提供不确定性量化。&lt;h4&gt;翻译&lt;/h4&gt;复杂系统中的时间建模需要捕捉多时间尺度的依赖关系，同时处理固有的不确定性。我们提出了HierCVAE，一种结合分层注意机制与条件变分自编码器的新架构来应对这些挑战。HierCVAE采用三层注意结构（局部、全局、跨时间）结合多模态条件编码，以捕捉时间、统计和趋势信息。该方法在潜在空间中融入ResFormer块，并通过预测头提供明确的不确定性量化。通过在能源消耗数据集上的评估，HierCVAE相比最先进方法实现了15-40%的预测精度提升，并在不确定性校准方面表现优越，特别擅长长期预测和复杂的多变量依赖关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal modeling in complex systems requires capturing dependencies acrossmultiple time scales while managing inherent uncertainties. We proposeHierCVAE, a novel architecture that integrates hierarchical attentionmechanisms with conditional variational autoencoders to address thesechallenges. HierCVAE employs a three-tier attention structure (local, global,cross-temporal) combined with multi-modal condition encoding to capturetemporal, statistical, and trend information. The approach incorporatesResFormer blocks in the latent space and provides explicit uncertaintyquantification via prediction heads. Through evaluations on energy consumptiondatasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy andsuperior uncertainty calibration compared to state-of-the-art methods,excelling in long-term forecasting and complex multi-variate dependencies.</description>
      <author>example@mail.com (Yao Wu)</author>
      <guid isPermaLink="false">2508.18922v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</title>
      <link>http://arxiv.org/abs/2508.18634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为OwlCap的视频描述多模态大语言模型，通过解决运动-细节不平衡问题，显著提升了视频描述的完整性和准确性。&lt;h4&gt;背景&lt;/h4&gt;视频描述旨在生成全面且连贯的视频内容描述，但现有方法常存在运动-细节不平衡问题，导致描述不完整，影响视频理解和生成的一致性。&lt;h4&gt;目的&lt;/h4&gt;解决视频描述中的运动-细节不平衡问题，提高描述的完整性和准确性，增强视频理解和生成的一致性。&lt;h4&gt;方法&lt;/h4&gt;1) 构建HMD-270K数据集，通过Motion-Detail Fusion和Fine-Grained Examination两阶段流程；2) 引入基于GRPO的CSER奖励机制，通过单元到集合匹配和双向验证增强运动和细节捕捉；3) 基于HMD-270K监督微调和GRPO后训练开发OwlCap模型。&lt;h4&gt;主要发现&lt;/h4&gt;OwlCap在VDC基准测试上准确率提升4.2，在DREAM-1K基准测试上F1分数提升4.6，显著优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;OwlCap通过平衡运动和细节描述有效提高了视频描述质量，研究团队将公开数据集和模型以促进研究社区发展。&lt;h4&gt;翻译&lt;/h4&gt;视频描述旨在生成对视频内容的全面且连贯的描述，有助于视频理解和生成的进步。然而，现有方法常常受到运动-细节不平衡的困扰，因为模型倾向于过度强调一个方面而忽视另一个方面。这种不平衡导致描述不完整，进而导致视频理解和生成的一致性缺乏。为了解决这个问题，我们从两个方面提出解决方案：1）数据方面：我们通过两阶段流程（Motion-Detail Fusion和Fine-Grained Examination）构建了Harmonizing Motion-Detail 270K (HMD-270K)数据集。2）优化方面：我们引入了基于Group Relative Policy Optimization (GRPO)的Caption Set Equivalence Reward (CSER)。CSER通过单元到集合匹配和双向验证，增强了对运动和细节捕捉的完整性和准确性。基于HMD-270K监督微调和使用CSER的GRPO后训练，我们开发了OwlCap，这是一个具有运动-细节平衡功能的强大视频描述多模态大语言模型(MLLM)。实验结果表明，与基线模型相比，OwlCap在两个基准测试上取得了显著改进：注重细节的VDC（+4.2准确率）和注重运动的DREAM-1K（+4.6 F1）。HMD-270K数据集和OwlCap模型将被公开发布，以促进视频描述研究社区的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video captioning aims to generate comprehensive and coherent descriptions ofthe video content, contributing to the advancement of both video understandingand generation. However, existing methods often suffer from motion-detailimbalance, as models tend to overemphasize one aspect while neglecting theother. This imbalance results in incomplete captions, which in turn leads to alack of consistency in video understanding and generation. To address thisissue, we propose solutions from two aspects: 1) Data aspect: We constructedthe Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stagepipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER)based on Group Relative Policy Optimization (GRPO). CSER enhances completenessand accuracy in capturing both motion and details through unit-to-set matchingand bidirectional validation. Based on the HMD-270K supervised fine-tuning andGRPO post-training with CSER, we developed OwlCap, a powerful video captioningmulti-modal large language model (MLLM) with motion-detail balance.Experimental results demonstrate that OwlCap achieves significant improvementscompared to baseline models on two benchmarks: the detail-focused VDC (+4.2Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCapmodel will be publicly released to facilitate video captioning researchcommunity advancements.</description>
      <author>example@mail.com (Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, Xiang Bai)</author>
      <guid isPermaLink="false">2508.18634v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling</title>
      <link>http://arxiv.org/abs/2508.18463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的上下文感知零样本异常检测框架，结合TimeSformer、DPC和CLIP模型，能够在不接触异常样本的情况下识别监控视频中的异常事件。&lt;h4&gt;背景&lt;/h4&gt;在监控视频中检测异常具有挑战性，因为异常具有不可预测性和上下文依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在训练过程中不接触异常样本的情况下识别异常事件的零样本异常检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出的混合架构结合TimeSformer、DPC和CLIP，其中TimeSformer作为视觉主干提取时空特征，DPC预测未来表示识别时间偏差，CLIP-based语义流通过文本提示实现概念级异常检测。使用InfoNCE和CPC损失进行联合训练，并通过上下文门控机制增强决策能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合预测建模与视觉语言理解，系统能够在复杂环境中推广到未见过的行为。&lt;h4&gt;结论&lt;/h4&gt;该框架弥合了零样本异常检测中时间推理和语义上下文之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;在监控视频中检测异常本质上具有挑战性，因为它们的性质是不可预测和依赖于上下文的。这项工作引入了一种新颖的上下文感知零样本异常检测框架，该框架可以在训练过程中不接触异常样本的情况下识别异常事件。提出的混合架构结合了TimeSformer、DPC和CLIP来建模时空动态和语义上下文。TimeSformer作为视觉主干提取丰富的时空特征，而DPC预测未来表示以识别时间偏差。此外，基于CLIP的语义流通过特定上下文文本提示实现概念级异常检测。这些组件使用InfoNCE和CPC损失进行联合训练，将视觉输入与它们的时间和语义表示对齐。上下文门控机制通过使用场景感知线索或全局视频特征来调节预测，进一步增强了决策能力。通过整合预测建模与视觉语言理解，系统可以在复杂环境中推广到未见过的行为。该框架弥合了零样本异常检测中时间推理和语义上下文之间的差距。本研究的代码已在https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in surveillance footage is inherently challenging due totheir unpredictable and context-dependent nature. This work introduces a novelcontext-aware zero-shot anomaly detection framework that identifies abnormalevents without exposure to anomaly examples during training. The proposedhybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporaldynamics and semantic context. TimeSformer serves as the vision backbone toextract rich spatial-temporal features, while DPC forecasts futurerepresentations to identify temporal deviations. Furthermore, a CLIP-basedsemantic stream enables concept-level anomaly detection throughcontext-specific text prompts. These components are jointly trained usingInfoNCE and CPC losses, aligning visual inputs with their temporal and semanticrepresentations. A context-gating mechanism further enhances decision-making bymodulating predictions with scene-aware cues or global video features. Byintegrating predictive modeling with vision-language understanding, the systemcan generalize to previously unseen behaviors in complex environments. Thisframework bridges the gap between temporal reasoning and semantic context inzero-shot anomaly detection for surveillance. The code for this research hasbeen made available athttps://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.</description>
      <author>example@mail.com (Md. Rashid Shahriar Khan, Md. Abrar Hasan, Mohammod Tareq Aziz Justice)</author>
      <guid isPermaLink="false">2508.18463v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning</title>
      <link>http://arxiv.org/abs/2508.19009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedProtoKD方法，通过增强的双知识蒸馏机制和对比学习解决异构联邦学习中的原型边缘收缩问题，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;异构联邦学习(HFL)因能适应不同客户端的多样化模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的方案出现，但当前方法在服务器上使用加权平均聚合原型会导致次优全局知识和原型收缩问题。&lt;h4&gt;目的&lt;/h4&gt;提出FedProtoKD方法，利用增强的双知识蒸馏机制提高异构联邦学习系统性能，解决原型边缘收缩问题，并通过评估公共样本重要性增强学习效果。&lt;h4&gt;方法&lt;/h4&gt;使用基于对比学习的可训练服务器原型，利用类别的自适应原型边缘，通过样本原型与其类别代表性原型的接近程度评估公共样本重要性，从而提升整体学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;FedProtoKD在各种设置下实现了1.13%到34.13%的平均准确率改进，显著优于现有的最先进HFL方法，特别是在模型异构且数据分布极度非IID的场景下表现出色。&lt;h4&gt;结论&lt;/h4&gt;FedProtoKD通过解决原型边缘收缩问题和改进知识聚合过程，有效提高了异构联邦学习环境中的性能，为HFL研究提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;异构联邦学习(HFL)因能够适应不同客户端的多样化模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的有前途的解决方案出现。这种方法专注于在异构客户端之间仅共享类别代表性原型。然而，这些原型通常在服务器上使用加权平均进行聚合，导致次优的全局知识；这会导致聚合原型的收缩，当模型异构且数据分布极度非IID时，对模型性能产生负面影响。我们在异构联邦学习环境中提出FedProtoKD，使用增强的双知识蒸馏机制利用客户端的logits和原型特征表示来提高系统性能。我们旨在利用基于对比学习的可训练服务器原型和类别自适应原型边缘来解决原型边缘收缩问题。此外，我们通过样本原型与其类别代表性原型的接近程度评估公共样本的重要性，这增强了学习性能。FedProtoKD在各种设置下实现了1.13%到34.13%的平均准确率改进，并显著优于现有的最先进HFL方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous Federated Learning (HFL) has gained attention for its abilityto accommodate diverse models and heterogeneous data across clients.Prototype-based HFL methods emerge as a promising solution to addressstatistical heterogeneity and privacy challenges, paving the way for newadvancements in HFL research. This method focuses on sharing onlyclass-representative prototypes among heterogeneous clients. However, theseprototypes are often aggregated on the server using weighted averaging, leadingto sub-optimal global knowledge; these cause the shrinking of aggregatedprototypes, which negatively affects the model performance in scenarios whenmodels are heterogeneous and data distributions are extremely non-IID. Wepropose FedProtoKD in a Heterogeneous Federated Learning setting, using anenhanced dual-knowledge distillation mechanism to improve the systemperformance with clients' logits and prototype feature representation. We aimto resolve the prototype margin-shrinking problem using a contrastivelearning-based trainable server prototype by leveraging a class-wise adaptiveprototype margin. Furthermore, we assess the importance of public samples usingthe closeness of the sample's prototype to its class representative prototypes,which enhances learning performance. FedProtoKD achieved average improvementsof 1.13% up to 34.13% accuracy across various settings and significantlyoutperforms existing state-of-the-art HFL methods.</description>
      <author>example@mail.com (Md Anwar Hossen, Fatema Siddika, Wensheng Zhang, Anuj Sharma, Ali Jannesari)</author>
      <guid isPermaLink="false">2508.19009v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.18687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究揭示了当前医学视觉语言模型在面对语义等价的问题重新表述时表现出的脆弱性，并提出了一致性和对比学习方法(CCL)来提高模型的一致性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在高风险医疗应用中，对多样化问题表述保持一致的回答对可靠诊断至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决医学视觉语言模型在医学视觉问答中面对语义等价的问题重新表述时答案波动的问题，提高模型的一致性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;构建了RoMed数据集(包含144k个问题，涵盖词级、句级和语义级的扰动)，并提出了CCL方法，包括基于知识的一致性学习和偏见感知的对比学习两个关键组件。&lt;h4&gt;主要发现&lt;/h4&gt;在RoMed上评估最先进模型时观察到性能显著下降(例如，召回率下降40%)，而CCL在三个流行的VQA基准测试上实现了最先进的性能，并在RoMed测试集上将答案一致性提高了50%。&lt;h4&gt;结论&lt;/h4&gt;CCL显著提高了医学视觉语言模型的鲁棒性和一致性，代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;在高风险医疗应用中，对多样化问题表述保持一致的回答对可靠诊断至关重要。然而，我们发现当前的医学视觉语言模型在医学视觉问答中表现出明显的脆弱性，当面对医学问题的语义等价重新表述时，它们的答案会显著波动。我们将此归因于两个局限：(1)医学概念对齐不足，导致推理模式分歧；(2)训练数据中存在的隐藏偏见，优先考虑句法捷径而非语义理解。为解决这些挑战，我们构建了RoMed数据集，基于原始VQA数据集，包含144k个问题，涵盖词级、句级和语义级的扰动。在RoMed上评估最先进模型(如LLaVA-Med)时，我们观察到令人担忧的性能下降(例如，召回率下降40%)，暴露了关键的鲁棒性差距。为弥合这一差距，我们提出了一致性和对比学习(CCL)，它整合了两个关键组件：(1)基于知识的一致性学习，将医学视觉语言模型与医学知识而非浅层特征模式对齐；(2)偏见感知的对比学习，通过判别性表示细化减轻数据特定先验。CCL在三个流行的VQA基准测试上实现了最先进的性能，并在具有挑战性的RoMed测试集上将答案一致性提高了50%，显著提高了鲁棒性。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In high-stakes medical applications, consistent answering across diversequestion phrasings is essential for reliable diagnosis. However, we reveal thatcurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragilityin Medical Visual Question Answering, as their answers fluctuate significantlywhen faced with semantically equivalent rephrasings of medical questions. Weattribute this to two limitations: (1) insufficient alignment of medicalconcepts, leading to divergent reasoning patterns, and (2) hidden biases intraining data that prioritize syntactic shortcuts over semantic understanding.To address these challenges, we construct RoMed, a dataset built upon originalVQA datasets containing 144k questions with variations spanning word-level,sentence-level, and semantic-level perturbations. When evaluatingstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarmingperformance drops (e.g., a 40\% decline in Recall) compared to original VQAbenchmarks, exposing critical robustness gaps. To bridge this gap, we proposeConsistency and Contrastive Learning (CCL), which integrates two keycomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs withmedical knowledge rather than shallow feature patterns, and (2) bias-awarecontrastive learning, mitigating data-specific priors through discriminativerepresentation refinement. CCL achieves SOTA performance on three popular VQAbenchmarks and notably improves answer consistency by 50\% on the challengingRoMed test set, demonstrating significantly enhanced robustness. Code will bereleased.</description>
      <author>example@mail.com (Songtao Jiang, Yuxi Chen, Sibo Song, Yan Zhang, Yeying Jin, Yang Feng, Jian Wu, Zuozhu Liu)</author>
      <guid isPermaLink="false">2508.18687v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.18322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,7 figures,conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个名为结构-语义统一器(SSU)的新框架，用于多模态情感分析。该框架通过整合模态特定的结构信息和跨模态语义基础，有效解决了现有方法中忽视模态特定结构依赖性和语义错位的问题。SSU在多个基准数据集上实现了最先进的性能，同时提高了可解释性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;多模态情感分析旨在通过整合文本、声学和视觉模态来推断情感状态。尽管已有显著进展，但现有的多模态融合方法通常忽略了模态特定的结构依赖性和语义错位，限制了模型的质量、可解释性和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性地整合模态特定结构信息和跨模态语义基础的新框架，以增强多模态表示，提高情感分析的准确性、可解释性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出结构-语义统一器(SSU)框架，包括：1)利用语言语法为文本构建模态特定图，使用轻量级文本引导注意力机制为声学和视觉模态构建图；2)引入源自全局文本语义的语义锚点作为跨模态对齐中心；3)开发多视图对比学习目标，促进模态内和模态间视图的判别性、语义一致性和结构连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;1)在CMU-MOSI和CMU-MOSEI两个基准数据集上，SSU始终实现了最先进的性能；2)与先前方法相比，SSU显著降低了计算开销；3)定性分析验证了SSU的可解释性和捕捉细微情感模式的能力。&lt;h4&gt;结论&lt;/h4&gt;结构-语义统一器(SSU)框架通过有效整合模态特定结构和跨模态语义信息，解决了现有多模态情感分析方法中的局限性，在保持高性能的同时提高了可解释性和计算效率，为多模态情感分析提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态情感分析旨在通过有效整合文本、声学和视觉模态来推断情感状态。尽管取得了显著进展，但现有的多模态融合方法通常忽略了模态特定的结构依赖性和语义错位，限制了它们的质量、可解释性和鲁棒性。为应对这些挑战，我们提出了一种名为结构-语义统一器(SSU)的新颖框架，该框架系统性地整合了模态特定的结构信息和跨模态语义基础，以增强多模态表示。具体而言，SSU利用语言语法为文本动态构建模态特定图，并为声学和视觉模态使用轻量级的文本引导注意力机制，从而捕获详细的模态内关系和语义交互。我们进一步引入了一个源自全局文本语义的语义锚点，作为跨模态对齐中心，有效调和了不同模态之间的异构语义空间。此外，我们开发了一个多视图对比学习目标，促进了模态内和模态间视图的判别性、语义一致性和结构连贯性。在两个广泛使用的基准数据集CMU-MOSI和CMU-MOSEI上的广泛评估表明，SSU始终实现了最先进的性能，同时显著降低了与先前方法相比的计算开销。全面的定性分析进一步验证了SSU的可解释性及其通过语义基础交互捕捉细微情感模式的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sentiment analysis (MSA) aims to infer emotional states byeffectively integrating textual, acoustic, and visual modalities. Despitenotable progress, existing multimodal fusion methods often neglectmodality-specific structural dependencies and semantic misalignment, limitingtheir quality, interpretability, and robustness. To address these challenges,we propose a novel framework called the Structural-Semantic Unifier (SSU),which systematically integrates modality-specific structural information andcross-modal semantic grounding for enhanced multimodal representations.Specifically, SSU dynamically constructs modality-specific graphs by leveraginglinguistic syntax for text and a lightweight, text-guided attention mechanismfor acoustic and visual modalities, thus capturing detailed intra-modalrelationships and semantic interactions. We further introduce a semanticanchor, derived from global textual semantics, that serves as a cross-modalalignment hub, effectively harmonizing heterogeneous semantic spaces acrossmodalities. Additionally, we develop a multiview contrastive learning objectivethat promotes discriminability, semantic consistency, and structural coherenceacross intra- and inter-modal views. Extensive evaluations on two widely usedbenchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistentlyachieves state-of-the-art performance while significantly reducingcomputational overhead compared to prior methods. Comprehensive qualitativeanalyses further validate SSU's interpretability and its ability to capturenuanced emotional patterns through semantically grounded interactions.</description>
      <author>example@mail.com (Jiangfeng Sun, Sihao He, Zhonghong Ou, Meina Song)</author>
      <guid isPermaLink="false">2508.18322v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>scI2CL: Effectively Integrating Single-cell Multi-omics by Intra- and Inter-omics Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.18304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了scI2CL，一种基于组内和组间对比学习的单细胞多组学融合框架，能够从互补的多组学数据中学习全面且具有辨别力的细胞表示，有效支持各种下游分析任务。&lt;h4&gt;背景&lt;/h4&gt;单细胞多组学数据包含大量细胞状态信息，分析这些数据可揭示细胞异质性、疾病和生物过程的宝贵见解。然而，细胞分化与发育是连续且动态的过程，基于单细胞多组学数据计算建模和推断细胞相互作用模式仍具挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的单细胞多组学融合框架，从互补的多组学数据中学习全面且具有辨别力的细胞表示，用于各种下游任务。&lt;h4&gt;方法&lt;/h4&gt;scI2CL是一种基于组内和组间对比学习的单细胞多组学融合框架，旨在学习全面且具有辨别力的细胞表示。&lt;h4&gt;主要发现&lt;/h4&gt;在细胞聚类任务中，scI2CL在四个真实数据集上超越了八种最先进方法；在细胞亚型分析中，发现了三种现有方法未能识别的单细胞亚群；是唯一能够正确构建从造血干细胞到记忆B细胞发育轨迹的方法；解决了CD4+ T细胞亚群间的细胞类型误分类问题。&lt;h4&gt;结论&lt;/h4&gt;scI2CL能够准确表征细胞间的跨组学关系，有效融合多组学数据并学习具有辨别力的细胞表示，支持各种下游分析任务。&lt;h4&gt;翻译&lt;/h4&gt;单细胞多组学数据包含大量细胞状态信息，分析这些数据可以揭示关于细胞异质性、疾病和生物过程的宝贵见解。然而，由于细胞分化与发育是一个连续且动态的过程，基于单细胞多组学数据计算建模和推断细胞相互作用模式仍然具有挑战性。本文提出了scI2CL，一种基于组内和组间对比学习的新型单细胞多组学融合框架，从互补的多组学数据中学习全面且具有辨别力的细胞表示，用于各种下游任务。四个下游任务的广泛实验验证了scI2CL的有效性及其优于现有方法的性能。具体而言，在细胞聚类中，scI2CL在四个广泛使用的真实数据集上超越了八种最先进的方法。在细胞亚型分析中，scI2CL有效区分了三种潜在的单细胞亚群，这些亚群是现有方法未能发现的。同时，scI2CL是唯一能够正确构建从造血干细胞和祖细胞到记忆B细胞的细胞发育轨迹的方法。此外，scI2CL解决了两个CD4+ T细胞亚群之间的细胞类型误分类问题，而现有方法无法精确区分这些混合细胞。总之，scI2CL能够准确表征细胞之间的跨组学关系，从而有效融合多组学数据并学习具有辨别力的细胞表示，支持各种下游分析任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell multi-omics data contain huge information of cellular states, andanalyzing these data can reveal valuable insights into cellular heterogeneity,diseases, and biological processes. However, as cell differentiation \&amp;development is a continuous and dynamic process, it remains challenging tocomputationally model and infer cell interaction patterns based on single-cellmulti-omics data. This paper presents scI2CL, a new single-cell multi-omicsfusion framework based on intra- and inter-omics contrastive learning, to learncomprehensive and discriminative cellular representations from complementarymulti-omics data for various downstream tasks. Extensive experiments of fourdownstream tasks validate the effectiveness of scI2CL and its superiority overexisting peers. Concretely, in cell clustering, scI2CL surpasses eightstate-of-the-art methods on four widely-used real-world datasets. In cellsubtyping, scI2CL effectively distinguishes three latent monocyte cellsubpopulations, which are not discovered by existing methods. Simultaneously,scI2CL is the only method that correctly constructs the cell developmentaltrajectory from hematopoietic stem and progenitor cells to Memory B cells. Inaddition, scI2CL resolves the misclassification of cell types between twosubpopulations of CD4+ T cells, while existing methods fail to preciselydistinguish the mixed cells. In summary, scI2CL can accurately characterizecross-omics relationships among cells, thus effectively fuses multi-omics dataand learns discriminative cellular representations to support variousdownstream analysis tasks.</description>
      <author>example@mail.com (Wuchao Liu, Han Peng, Wengen Li, Yichao Zhang, Jihong Guan, Shuigeng Zhou)</author>
      <guid isPermaLink="false">2508.18304v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval-Augmented Review Generation for Poisoning Recommender Systems</title>
      <link>http://arxiv.org/abs/2508.15252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RAGAN的新型实用攻击框架，利用多模态基础模型的能力生成高质量的虚假用户资料，以提高推荐系统投毒攻击的转移性和不可察觉性。&lt;h4&gt;背景&lt;/h4&gt;推荐系统容易受到数据投毒攻击，恶意行为者通过注入虚假用户资料和评分来操纵推荐。由于实际安全和隐私限制，攻击者对系统了解有限，需要制作能在黑盒系统间转移的资料，同时保持不可察觉性。&lt;h4&gt;目的&lt;/h4&gt;解决在资源受限情况下生成高质量虚假资料的挑战，提高攻击的转移性和不可察觉性，并深入了解推荐系统的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;利用多模态基础模型(in-context learning)能力提高评论质量，引入演示检索算法和文本风格转移策略增强ICL。通过RAGAN框架，由越狱者生成虚假用户资料，并由指令代理和守护者协作优化，提高攻击的转移性和不可察觉性。&lt;h4&gt;主要发现&lt;/h4&gt;在各种真实世界数据集上的全面实验表明，RAGAN达到了最先进的投毒攻击性能，能有效生成高质量的虚假用户资料。&lt;h4&gt;结论&lt;/h4&gt;RAGAN框架能够有效生成高质量的虚假用户资料，有助于深入了解推荐系统的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，推荐系统极易受到数据投毒攻击，恶意行为者注入虚假用户资料（包括精心设计的虚假评分组）来操纵推荐。由于实际中的安全和隐私限制，攻击者通常对受害系统了解有限，因此需要制作能在黑盒推荐系统间转移的资料。为最大化攻击效果，资料通常保持不可察觉性。然而，在资源受限的情况下生成此类高质量资料具有挑战性。一些研究建议加入虚假文本评论来增强资料；然而，在实际设置中，评论质量差严重削弱了攻击的有效性和不可察觉性。为应对上述挑战，本文提出利用多模态基础模型(in-context learning)能力提高评论质量。为此，我们引入演示检索算法和文本风格转移策略来增强基础ICL。具体而言，我们提出了一种名为RAGAN的新型实用攻击框架，用于生成高质量的虚假用户资料，有助于深入了解推荐系统的鲁棒性。资料由一个越狱者生成，并在指令代理和守护者上协作优化，以提高攻击的转移性和不可察觉性。在各种真实世界数据集上的全面实验表明，RAGAN实现了最先进的投毒攻击性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have shown that recommender systems (RSs) are highlyvulnerable to data poisoning attacks, where malicious actors inject fake userprofiles, including a group of well-designed fake ratings, to manipulaterecommendations. Due to security and privacy constraints in practice, attackerstypically possess limited knowledge of the victim system and thus need to craftprofiles that have transferability across black-box RSs. To maximize the attackimpact, the profiles often remains imperceptible. However, generating suchhigh-quality profiles with the restricted resources is challenging. Some workssuggest incorporating fake textual reviews to strengthen the profiles; yet, thepoor quality of the reviews largely undermines the attack effectiveness andimperceptibility under the practical setting.  To tackle the above challenges, in this paper, we propose to enhance thequality of the review text by harnessing in-context learning (ICL) capabilitiesof multimodal foundation models. To this end, we introduce a demonstrationretrieval algorithm and a text style transfer strategy to augment the navieICL. Specifically, we propose a novel practical attack framework named RAGAN togenerate high-quality fake user profiles, which can gain insights into therobustness of RSs. The profiles are generated by a jailbreaker andcollaboratively optimized on an instructional agent and a guardian to improvethe attack transferability and imperceptibility. Comprehensive experiments onvarious real-world datasets demonstrate that RAGAN achieves thestate-of-the-art poisoning attack performance.</description>
      <author>example@mail.com (Shiyi Yang, Xinshu Li, Guanglin Zhou, Chen Wang, Xiwei Xu, Liming Zhu, Lina Yao)</author>
      <guid isPermaLink="false">2508.15252v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
  <item>
      <title>Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.18052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文扩展了图神经网络理论，从离散动态图延伸到连续时间动态图，特别是处理异步演化和可能断开连接的图结构。作者提出连续时间动态1-WL测试及其等价图神经网络架构，解决了现实系统中图结构异步演化的挑战。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已知与1-Weisfeiler-Lehman(1-WL)测试具有相同的区分能力，且产生的分区与图的展开树等价类一致。然而，这些结果仅适用于表示为连接图快照序列的属性离散动态图。现实系统如通信网络、金融交易网络和分子相互作用等，通常异步演化且可能分裂成不连通组件。&lt;h4&gt;目的&lt;/h4&gt;将属性离散动态图的理论扩展到具有任意连通性的属性连续时间动态图，以处理异步演化和可能断开连接的图结构。&lt;h4&gt;方法&lt;/h4&gt;引入连续时间动态1-WL测试，证明其与连续时间动态展开树的等价性，确定一类基于离散动态GNN架构的连续时间动态GNNs(CGNNS)，并通过构造性证明提供实用的设计指南。&lt;h4&gt;主要发现&lt;/h4&gt;连续时间动态1-WL测试与连续时间动态展开树等价；提出的CGNNs保留了区分能力和通用逼近保证；构造性证明提供了实用的设计指南。&lt;h4&gt;结论&lt;/h4&gt;提出了一种具有分段连续可微时间函数的紧凑且表达性强的CGNN架构，可有效处理异步、断开连接的图，为现实系统提供了更合适的图神经网络解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已知与1-Weisfeiler-Lehman(1-WL)测试的区分能力相匹配，并且产生的分区与图的展开树等价类一致。保持这种等价性，GNNs可以在概率上以任意精度普遍近似任何目标函数。然而，这些结果仅限于表示为连接图快照序列的属性离散动态图。现实系统，如通信网络、金融交易网络和分子相互作用，异步演化并且可能分裂成不连通组件。在本文中，我们将属性离散动态图的理论扩展到具有任意连通性的属性连续时间动态图。为此，我们引入了连续时间动态1-WL测试，证明了它与连续时间动态展开树的等价性，并确定了一类基于离散动态GNN架构的连续时间动态GNNs(CGNNS)，它们保留了区分能力和通用逼近保证。我们的构造性证明进一步提供了实用的设计指南，强调了一种具有分段连续可微时间函数的紧凑且表达性强的CGNN架构，用于处理异步、断开连接的图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are known to match the distinguishing power ofthe 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide withthe unfolding tree equivalence classes of graphs. Preserving this equivalence,GNNs can universally approximate any target function on graphs in probabilityup to any precision. However, these results are limited to attributeddiscrete-dynamic graphs represented as sequences of connected graph snapshots.Real-world systems, such as communication networks, financial transactionnetworks, and molecular interactions, evolve asynchronously and may split intodisconnected components. In this paper, we extend the theory of attributeddiscrete-dynamic graphs to attributed continuous-time dynamic graphs witharbitrary connectivity. To this end, we introduce a continuous-time dynamic1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,and identify a class of continuous-time dynamic GNNs (CGNNs) based ondiscrete-dynamic GNN architectures that retain both distinguishing power anduniversal approximation guarantees. Our constructive proofs further yieldpractical design guidelines, emphasizing a compact and expressive CGNNarchitecture with piece-wise continuously differentiable temporal functions toprocess asynchronous, disconnected graphs.</description>
      <author>example@mail.com (Silvia Beddar-Wiesing, Alice Moallemy-Oureh)</author>
      <guid isPermaLink="false">2508.18052v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Training Transformers for Mesh-Based Simulations</title>
      <link>http://arxiv.org/abs/2508.18051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的图Transformer架构，利用邻接矩阵作为注意力掩码，通过扩展滑动窗口和全局注意力等创新方法，解决了传统GNN消息传递架构在处理大型复杂网格时的扩展性和效率问题。该方法在3D计算流体动力学数据集上表现出色，最小模型比MeshGraphNet快7倍且小6倍，最大模型比之前最先进水平平均高38.8%。&lt;h4&gt;背景&lt;/h4&gt;使用图神经网络(GNNs)模拟物理学主要依赖于消息传递架构，这些架构在处理大型复杂网格时面临扩展性和效率方面的挑战。现有的增强方法如多网格方法和K-hop聚合引入了显著复杂性且缺乏深入研究。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型图Transformer架构，解决传统GNN在处理大型复杂网格时的扩展性和效率问题，同时保持或提高模拟性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种利用邻接矩阵作为注意力掩码的图Transformer架构，并创新性地结合扩展滑动窗口和全局注意力技术，以扩展感受野而不牺牲计算效率。通过大量实验评估模型大小、邻接矩阵增强、位置编码和K-hop配置。&lt;h4&gt;主要发现&lt;/h4&gt;1) 引入的模型表现出卓越的可扩展性，能处理多达30万个节点和300万条边的网格；2) 最小模型与MeshGraphNet性能相当，但速度快7倍，体积小6倍；3) 最大模型比之前最先进水平平均高出38.8%，在所有滚动均方根误差上比MeshGraphNet高出52%，同时具有相似训练速度；4) 找到了训练FLOPs与参数之间的缩放定律。&lt;h4&gt;结论&lt;/h4&gt;所提出的图Transformer架构有效解决了传统GNN在处理大型复杂网格时的扩展性和效率问题，在3D计算流体动力学模拟中取得了显著的性能提升，同时保持了计算效率。代码和数据集已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;使用图神经网络(GNNs)模拟物理学主要依赖于消息传递架构，这些架构在处理大型复杂网格时面临扩展性和效率方面的挑战。这些架构启发了许多增强方法，包括多网格方法和K-hop聚合（使用距离为K的邻居），但它们通常引入了显著的复杂性且缺乏深入的研究。针对这些挑战，我们提出了一种新型的图Transformer架构，利用邻接矩阵作为注意力掩码。所提出的方法包括创新的增强，如扩展滑动窗口和全局注意力，可以在不牺牲计算效率的情况下扩展感受野。通过大量实验，我们使用具有挑战性的3D计算流体动力学(CFD)数据集评估了模型大小、邻接矩阵增强、位置编码和K-hop配置。我们还训练了60多个模型，找到了训练FLOPs与参数之间的缩放定律。所引入的模型表现出卓越的可扩展性，能够处理多达30万个节点和300万条边的网格。值得注意的是，最小模型与MeshGraphNet性能相当，但速度快7倍，体积小6倍。最大模型比之前的最新水平平均高出38.8%，在所有滚动均方根误差上比MeshGraphNet高出52%，同时具有相似训练速度。代码和数据集可在https://github.com/DonsetPG/graph-physics获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于网格的物理模拟中图神经网络(GNNs)面临的扩展性和效率挑战，特别是在处理大型、复杂网格时。这个问题很重要，因为物理模拟(如计算流体动力学)需要求解复杂域上的偏微分方程，而传统方法计算密集且每次模拟独立进行，无法重用经验。现有的基于消息传递的GNN虽然有多尺度方法等改进，但往往引入显著复杂性且难以扩展，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到消息传递GNN在处理大型网格时的局限性，然后借鉴了Transformer在自然语言处理和计算机视觉领域的成功经验，特别是其高效扩展和捕获长距离依赖的能力。作者将网格视为图，利用邻接矩阵作为注意力掩码，并创新地引入了扩展滑动窗口、全局注意力和随机连接等增强方法。同时，作者也借鉴了Vision Transformer的掩码预训练技术和图神经网络的改进方法，如Mix-Hops和边缘跳跃，但将这些技术首次应用于网格物理模拟场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将网格视为图，利用邻接矩阵作为Transformer中的注意力掩码，并通过创新地增强邻接矩阵来扩展感受野而不显著增加计算复杂度。整体实现流程包括：1)将网格表示为无向图；2)采用编码器-处理器-解码器架构；3)处理器使用L层Transformer块，每块包含掩码多头自注意力和门控MLP；4)使用邻接矩阵的增强版本(扩展、随机连接和全局注意力)；5)采用节点坐标作为位置编码；6)使用AdamW优化器进行训练，并应用掩码预技术。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于Transformer的图架构，直接利用邻接矩阵作为稀疏注意力掩码；2)创新的三种邻接矩阵增强技术(扩展滑动窗口、随机连接和全局注意力)；3)高效的感受野扩展方法，无需复杂的多尺度处理；4)揭示了训练FLOPs与参数之间的扩展定律；5)应用掩码预训练技术提升性能。相比之前的工作，该方法架构更简单高效，感受野扩展更灵活，性能显著提升(最大模型比之前SOTA平均提高38.8%)，且能处理多达300k节点的大型网格，同时保持7倍以上的速度优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的基于Transformer的图神经网络架构，通过将邻接矩阵作为注意力掩码并进行创新增强，实现了在大型网格上高效准确的物理模拟，显著优于现有的消息传递方法，同时保持了计算效率和可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating physics using Graph Neural Networks (GNNs) is predominantly drivenby message-passing architectures, which face challenges in scaling andefficiency, particularly in handling large, complex meshes. These architectureshave inspired numerous enhancements, including multigrid approaches and $K$-hopaggregation (using neighbours of distance $K$), yet they often introducesignificant complexity and suffer from limited in-depth investigations. Inresponse to these challenges, we propose a novel Graph Transformer architecturethat leverages the adjacency matrix as an attention mask. The proposed approachincorporates innovative augmentations, including Dilated Sliding Windows andGlobal Attention, to extend receptive fields without sacrificing computationalefficiency. Through extensive experimentation, we evaluate model size,adjacency matrix augmentations, positional encoding and $K$-hop configurationsusing challenging 3D computational fluid dynamics (CFD) datasets. We also trainover 60 models to find a scaling law between training FLOPs and parameters. Theintroduced models demonstrate remarkable scalability, performing on meshes withup to 300k nodes and 3 million edges. Notably, the smallest model achievesparity with MeshGraphNet while being $7\times$ faster and $6\times$ smaller.The largest model surpasses the previous state-of-the-art by $38.8$\% onaverage and outperforms MeshGraphNet by $52$\% on the all-rollout RMSE, whilehaving a similar training speed. Code and datasets are available athttps://github.com/DonsetPG/graph-physics.</description>
      <author>example@mail.com (Paul Garnier, Vincent Lannelongue, Jonathan Viquerat, Elie Hachem)</author>
      <guid isPermaLink="false">2508.18051v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding</title>
      <link>http://arxiv.org/abs/2508.17971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LLM-NAR的新颖框架，通过神经算法推理器（NAR）来提升大型语言模型（LLM）在多智能体路径寻找（MAPF）任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLM）已被证明可以解决各种任务，但在多智能体路径寻找（MAPF）任务中的表现不佳，且该领域研究较少。MAPF是一个需要规划和多智能体协调的复杂问题。&lt;h4&gt;目的&lt;/h4&gt;提高大型语言模型（LLM）在多智能体路径寻找（MAPF）任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出LLM-NAR框架，包含三个关键组件：用于MAPF的LLM、预训练的基于图神经网络的神经算法推理器（NAR）以及交叉注意力机制。这是首个使用神经算法推理器将图神经网络与地图信息整合用于MAPF的工作，可指导LLM实现卓越性能，且能适应各种LLM模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和现实世界实验证明，LLM-NAR方法在解决MAPF问题时显著优于现有的基于LLM的方法。&lt;h4&gt;结论&lt;/h4&gt;LLM-NAR框架有效提升了LLM在MAPF任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLM）的发展和应用表明，基础模型可以用于解决广泛多样的任务。然而，它们在多智能体路径寻找（MAPF）任务中的表现并不理想，只有少数研究探索了这一领域。MAPF是一个需要规划和多智能体协调的复杂问题。为了提高LLM在MAPF任务中的性能，我们提出了一个新颖的框架LLM-NAR，它利用神经算法推理器（NAR）来指导LLM进行MAPF。LLM-NAR包含三个关键组件：用于MAPF的LLM、预训练的基于图神经网络的NAR以及交叉注意力机制。这是首个提出使用神经算法推理器将图神经网络与地图信息整合用于MAPF的工作，从而指导LLM实现卓越性能。LLM-NAR可以轻松适应各种LLM模型。模拟和现实世界实验均证明，我们的方法在解决MAPF问题时显著优于现有的基于LLM的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development and application of large language models (LLM) havedemonstrated that foundational models can be utilized to solve a wide array oftasks. However, their performance in multi-agent path finding (MAPF) tasks hasbeen less than satisfactory, with only a few studies exploring this area. MAPFis a complex problem requiring both planning and multi-agent coordination. Toimprove the performance of LLM in MAPF tasks, we propose a novel framework,LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM forMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trainedgraph neural network-based NAR, and a cross-attention mechanism. This is thefirst work to propose using a neural algorithmic reasoner to integrate GNNswith the map information for MAPF, thereby guiding LLM to achieve superiorperformance. LLM-NAR can be easily adapted to various LLM models. Bothsimulation and real-world experiments demonstrate that our method significantlyoutperforms existing LLM-based approaches in solving MAPF problems.</description>
      <author>example@mail.com (Pu Feng, Size Wang, Yuhong Cao, Junkang Liang, Rongye Shi, Wenjun Wu)</author>
      <guid isPermaLink="false">2508.17971v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</title>
      <link>http://arxiv.org/abs/2508.17630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;量子图注意力网络(QGAT)是一种将变分量子电路整合到注意力机制中的混合图神经网络，利用量子并行性实现参数共享，减少计算开销和模型复杂度，同时提高对复杂结构依赖的捕捉能力和对抗噪声的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络在处理图结构数据时面临表达性和计算效率的挑战，而量子计算为解决这些问题提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合图神经网络架构QGAT，通过整合量子计算到注意力机制中，提高图神经网络的表达能力和计算效率，特别是在处理复杂结构依赖和噪声数据方面。&lt;h4&gt;方法&lt;/h4&gt;QGAT采用强纠缠量子电路和幅度编码的节点特征实现非线性交互，使用单个量子电路同时生成多个注意力系数（与经典多头注意力不同），通过量子并行性实现参数共享，并联合优化经典投影权重和量子电路参数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明QGAT能有效捕捉复杂结构依赖，在归纳场景中具有更好的泛化能力；量子嵌入增强了对抗特征和结构噪声的鲁棒性，在处理真实世界噪声数据方面具有优势；QGAT在化学、生物学和网络分析等领域具有可扩展量子增强学习的潜力。&lt;h4&gt;结论&lt;/h4&gt;QGAT通过整合量子计算到注意力机制中，成功提高了图神经网络的表达能力和计算效率，其模块化设计确保了与现有架构的简单集成，为量子增强学习在多个领域的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了量子图注意力网络(QGAT)，这是一种将变分量子电路整合到注意力机制中的混合图神经网络。其核心是QGAT采用强纠缠量子电路和幅度编码的节点特征来实现表达性的非线性交互。与分别计算每个头的经典多头注意力不同，QGAT利用单个量子电路同时生成多个注意力系数。这种量子并行性促进了头之间的参数共享，显著减少了计算开销和模型复杂度。经典投影权重和量子电路参数以端到端方式联合优化，确保对学习任务的灵活适应。实验结果证明了QGAT在捕捉复杂结构依赖方面的有效性以及在归纳场景中改进的泛化能力，突显了其在化学、生物学和网络分析等领域的可扩展量子增强学习的潜力。此外，实验证实量子嵌入增强了对抗特征和结构噪声的鲁棒性，表明在处理真实世界噪声数据方面具有优势。QGAT的模块化也确保了与现有架构的简单集成，使其能够轻松增强基于经典注意力的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neuralnetwork that integrates variational quantum circuits into the attentionmechanism. At its core, QGAT employs strongly entangling quantum circuits withamplitude-encoded node features to enable expressive nonlinear interactions.Distinct from classical multi-head attention that separately computes eachhead, QGAT leverages a single quantum circuit to simultaneously generatemultiple attention coefficients. This quantum parallelism facilitates parametersharing across heads, substantially reducing computational overhead and modelcomplexity. Classical projection weights and quantum circuit parameters areoptimized jointly in an end-to-end manner, ensuring flexible adaptation tolearning tasks. Empirical results demonstrate QGAT's effectiveness in capturingcomplex structural dependencies and improved generalization in inductivescenarios, highlighting its potential for scalable quantum-enhanced learningacross domains such as chemistry, biology, and network analysis. Furthermore,experiments confirm that quantum embedding enhances robustness against featureand structural noise, suggesting advantages in handling real-world noisy data.The modularity of QGAT also ensures straightforward integration into existingarchitectures, allowing it to easily augment classical attention-based models.</description>
      <author>example@mail.com (An Ning, Tai Yue Li, Nan Yow Chen)</author>
      <guid isPermaLink="false">2508.17630v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction</title>
      <link>http://arxiv.org/abs/2508.17554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为S²G-Net的新型神经网络架构，用于预测患者重症监护室的住院时间，该架构结合了状态空间序列建模与多视图图神经网络。&lt;h4&gt;背景&lt;/h4&gt;预测患者重症监护室住院时间对医院资源管理至关重要，但由于电子健康记录的异质性和不规则采样特性，这一任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的神经网络架构，结合状态空间序列建模与多视图图神经网络，以提高ICU住院时间预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出S²G-Net架构，包含时间路径(使用Mamba状态空间模型捕获患者轨迹)和图路径(利用优化的GraphGPS主干整合来自诊断、行政和语义特征的异质患者相似图)。&lt;h4&gt;主要发现&lt;/h4&gt;在大型MIMIC-IV队列数据集上的实验表明，S²G-Net在所有主要指标上持续优于序列模型、图模型和混合方法。&lt;h4&gt;结论&lt;/h4&gt;广泛的消融研究和可解释性分析突出了架构各组件的互补贡献，强调了原则性图构建的重要性，S²G-Net为使用多模态临床数据的ICU LOS预测提供了有效且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;预测患者重症监护室的住院时间是医院资源管理的关键任务，但由于电子健康记录的异质性和不规则采样特性，这一任务仍然具有挑战性。在这项工作中，我们提出了S²G-Net，一种新型神经架构，将状态空间序列建模与多视图图神经网络(GNNs)统一用于ICU住院时间预测。时间路径采用Mamba状态空间模型(SSMs)来捕获患者轨迹，而图路径则利用优化的GraphGPS主干，旨在整合来自诊断、行政和语义特征的异质患者相似图。在大型MIMIC-IV队列数据集上的实验表明，S²G-Net在所有主要指标上持续优于序列模型(BiLSTM, Mamba, Transformer)、图模型(经典GNNs, GraphGPS)和混合方法。广泛的消融研究和可解释性分析强调了架构每个组件的互补贡献，并强调了原则性图构建的重要性。这些结果表明，S²G-Net为使用多模态临床数据的ICU住院时间预测提供了有效且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting a patient's length of stay (LOS) in the intensive care unit (ICU)is a critical task for hospital resource management, yet remains challengingdue to the heterogeneous and irregularly sampled nature of electronic healthrecords (EHRs). In this work, we propose S$^2$G-Net, a novel neuralarchitecture that unifies state-space sequence modeling with multi-view GraphNeural Networks (GNNs) for ICU LOS prediction. The temporal path employs Mambastate-space models (SSMs) to capture patient trajectories, while the graph pathleverages an optimized GraphGPS backbone, designed to integrate heterogeneouspatient similarity graphs derived from diagnostic, administrative, and semanticfeatures. Experiments on the large-scale MIMIC-IV cohort dataset show thatS$^2$G-Net consistently outperforms sequence models (BiLSTM, Mamba,Transformer), graph models (classic GNNs, GraphGPS), and hybrid approachesacross all primary metrics. Extensive ablation studies and interpretabilityanalyses highlight the complementary contributions of each component of ourarchitecture and underscore the importance of principled graph construction.These results demonstrate that S$^2$G-Net provides an effective and scalablesolution for ICU LOS prediction with multi-modal clinical data.</description>
      <author>example@mail.com (Shuqi Zi, Haitz Sáez de Ocáriz Borde, Emma Rocheteau, Pietro Lio')</author>
      <guid isPermaLink="false">2508.17554v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis</title>
      <link>http://arxiv.org/abs/2508.17478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的两阶段多模态预后模型 GraphMMP，用于解决多模态医学数据分析中的挑战。该模型使用互信息构建特征图，并采用基于 Mamba 的全局融合模块，在肝脏预后和 METABRIC 研究数据集上表现出色，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;多模态医学数据分析领域持续关注如何利用多样化的数据类型并理解它们之间的隐藏关系。&lt;h4&gt;目的&lt;/h4&gt;解决有效建模具有不同特性的异构数据模态之间复杂交互的挑战，同时捕捉模态间的局部和全局依赖关系。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段多模态预后模型 GraphMMP，基于图神经网络构建，使用互信息构建特征图，并采用基于 Mamba 的全局融合模块。&lt;h4&gt;主要发现&lt;/h4&gt;GraphMMP 在肝脏预后和 METABRIC 研究相关数据集上超越了现有方法，证明了其在多模态医学预后任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;GraphMMP 通过其创新的图神经网络架构和 Mamba 全局融合模块，显著提高了多模态医学预后性能。&lt;h4&gt;翻译&lt;/h4&gt;在多模态医学数据分析领域，利用多样化的数据类型并理解它们之间的隐藏关系一直是研究焦点。主要挑战在于有效建模具有不同特性的异构数据模态之间的复杂交互，同时捕捉模态间的局部和全局依赖关系。为应对这些挑战，本文提出了基于图神经网络的两阶段多模态预后模型 GraphMMP。该模型使用互信息构建特征图，并包含基于 Mamba 构建的全局融合模块，显著提高了预后性能。实验结果表明，GraphMMP 在肝脏预后和 METABRIC 研究相关数据集上超越了现有方法，证明了其在多模态医学预后任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of multimodal medical data analysis, leveraging diverse types ofdata and understanding their hidden relationships continues to be a researchfocus. The main challenges lie in effectively modeling the complex interactionsbetween heterogeneous data modalities with distinct characteristics whilecapturing both local and global dependencies across modalities. To addressthese challenges, this paper presents a two-stage multimodal prognosis model,GraphMMP, which is based on graph neural networks. The proposed modelconstructs feature graphs using mutual information and features a global fusionmodule built on Mamba, which significantly boosts prognosis performance.Empirical results show that GraphMMP surpasses existing methods on datasetsrelated to liver prognosis and the METABRIC study, demonstrating itseffectiveness in multimodal medical prognosis tasks.</description>
      <author>example@mail.com (Xuhao Shan, Ruiquan Ge, Jikui Liu, Linglong Wu, Chi Zhang, Siqi Liu, Wenjian Qin, Wenwen Min, Ahmed Elazab, Changmiao Wang)</author>
      <guid isPermaLink="false">2508.17478v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning</title>
      <link>http://arxiv.org/abs/2508.17387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种无需图神经网络(GNN)的方法，将图任务重新表述为文本推理问题，利用大型推理模型(LRMs)解决。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)受限于固定的标签空间，大型语言模型(LLMs)缺乏结构归纳偏置，使得在没有特定任务监督的情况下推广到未见过的图任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需GNN的方法，将图任务(节点分类、链接预测和图分类)重新表述为大型推理模型(LRMs)解决的文本推理问题。&lt;h4&gt;方法&lt;/h4&gt;引入了第一个包含这些任务详细推理轨迹的数据集，开发了Graph-R1，这是一个利用特定任务的重新思考模板来指导线性化图推理的强化学习框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Graph-R1在零样本设置中优于最先进的基线，产生了可解释且有效的预测。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了显式推理在图学习中的潜力，并为未来研究提供了新资源。&lt;h4&gt;翻译&lt;/h4&gt;在没有特定任务监督的情况下推广到未见过的图任务仍然具有挑战性。图神经网络(GNNs)受限于固定的标签空间，而大型语言模型(LLMs)缺乏结构归纳偏置。最近大型推理模型(LRMs)的进展通过明确的长链思维推理提供了零样本替代方案。受此启发，我们提出了一种无需GNN的方法，将图任务——节点分类、链接预测和图分类——重新表述为大型推理模型(LRMs)解决的文本推理问题。我们为这些任务引入了第一个包含详细推理轨迹的数据集，并开发了Graph-R1，这是一个利用特定任务的重新思考模板来指导线性化图推理的强化学习框架。实验证明，Graph-R1在零样本设置中优于最先进的基线，产生了可解释且有效的预测。我们的工作强调了显式推理在图学习中的潜力，并为未来研究提供了新资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizing to unseen graph tasks without task-pecific supervision remainschallenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,while Large Language Models (LLMs) lack structural inductive biases. Recentadvances in Large Reasoning Models (LRMs) provide a zero-shot alternative viaexplicit, long chain-of-thought reasoning. Inspired by this, we propose aGNN-free approach that reformulates graph tasks--node classification, linkprediction, and graph classification--as textual reasoning problems solved byLRMs. We introduce the first datasets with detailed reasoning traces for thesetasks and develop Graph-R1, a reinforcement learning framework that leveragestask-specific rethink templates to guide reasoning over linearized graphs.Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines inzero-shot settings, producing interpretable and effective predictions. Our workhighlights the promise of explicit reasoning for graph learning and providesnew resources for future research.</description>
      <author>example@mail.com (Yicong Wu, Guangyue Lu, Yuan Zuo, Huarong Zhang, Junjie Wu)</author>
      <guid isPermaLink="false">2508.17387v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention</title>
      <link>http://arxiv.org/abs/2508.17175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文比较了图变换器中的密集和稀疏两种注意力机制，分析了它们的权衡取舍，并指出了设计图变换器注意力机制时面临的挑战和问题。&lt;h4&gt;背景&lt;/h4&gt;图已成为机器学习中表示关系和结构数据的核心方式，传统图神经网络难以捕捉节点间的长程依赖关系，而图变换器通过注意力机制允许节点全局交换信息。&lt;h4&gt;目的&lt;/h4&gt;比较图变换器中的密集和稀疏两种注意力机制，分析它们的权衡取舍，指出何时应使用每种机制，并概述设计图变换器注意力机制时面临的当前挑战和问题。&lt;h4&gt;方法&lt;/h4&gt;通过比较密集和稀疏两种注意力机制，分析它们的性能和适用场景。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现，但暗示了比较分析的结果有助于理解两种注意力机制的适用场景。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确给出结论，但指出了图变换器注意力机制设计中仍存在挑战和问题需要解决。&lt;h4&gt;翻译&lt;/h4&gt;图已成为机器学习中捕获各种领域关系和结构数据的核心表示。传统图神经网络由于其局部结构，往往难以捕捉节点间的长程依赖关系。图变换器通过使用注意力机制克服了这一问题，使节点能够全局交换信息。然而，图变换器中有两种类型的注意力：密集和稀疏。在本文中，我们比较了这两种注意力机制，分析了它们的权衡取舍，并指出了何时应使用每种机制。我们还概述了设计图变换器注意力机制时面临的当前挑战和问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs have become a central representation in machine learning for capturingrelational and structured data across various domains. Traditional graph neuralnetworks often struggle to capture long-range dependencies between nodes due totheir local structure. Graph transformers overcome this by using attentionmechanisms that allow nodes to exchange information globally. However, thereare two types of attention in graph transformers: dense and sparse. In thispaper, we compare these two attention mechanisms, analyze their trade-offs, andhighlight when to use each. We also outline current challenges and problems indesigning attention for graph transformers.</description>
      <author>example@mail.com (Leon Dimitrov)</author>
      <guid isPermaLink="false">2508.17175v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process</title>
      <link>http://arxiv.org/abs/2508.17097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AISTATS'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的不确定性和可解释性图分类模型，结合图功能神经过程和图生成模型，解决了图神经网络预测校准不足和缺乏可解释性的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是处理图数据的强大工具，但它们的预测存在校准问题且缺乏可解释性，这限制了它们在关键应用中的采用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的不确定性和可解释性图分类模型，解决GNN预测校准不足和缺乏可解释性的问题。&lt;h4&gt;方法&lt;/h4&gt;结合图功能神经过程和图生成模型，假设一组潜在的理由映射到概率嵌入空间，通过学习随机相关矩阵条件化分类器的预测分布，使用图生成器从嵌入空间解码理由的图结构提供可解释性，采用类似于期望最大化算法的交替优化程序进行训练，该方法可应用于任何现有GNN架构。&lt;h4&gt;主要发现&lt;/h4&gt;在五个图分类数据集上的实验表明，该方法在不确定性和图神经网络可解释性方面优于最先进方法，案例研究显示解码的理由结构能提供有意义的解释。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了GNN预测校准不足和缺乏可解释性的问题，提高了模型在关键应用中的适用性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络是处理图数据的强大工具。然而，它们的预测存在校准问题且缺乏可解释性，限制了它们在关键应用中的采用。为解决这一问题，我们提出了一种新的不确定性和可解释性图分类模型，结合了图功能神经过程和图生成模型。我们方法的核心是假设一组潜在的理由，可以映射到概率嵌入空间；分类器的预测分布通过学习随机相关矩阵条件化于这些理由嵌入。图生成器用于从嵌入空间解码理由的图结构，以提供模型可解释性。为高效模型训练，我们采用类似于著名期望最大化（EM）算法的交替优化程序。所提出的方法通用，可应用于任何现有的GNN架构。在五个图分类数据集上的广泛实验表明，我们的框架在不确定量化和图神经网络可解释性方面都优于最先进的方法。我们还进行了案例研究，表明解码的理由结构可以提供有意义的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are powerful tools on graph data. However, theirpredictions are mis-calibrated and lack interpretability, limiting theiradoption in critical applications. To address this issue, we propose a newuncertainty-aware and interpretable graph classification model that combinesgraph functional neural process and graph generative model. The core of ourmethod is to assume a set of latent rationales which can be mapped to aprobabilistic embedding space; the predictive distribution of the classifier isconditioned on such rationale embeddings by learning a stochastic correlationmatrix. The graph generator serves to decode the graph structure of therationales from the embedding space for model interpretability. For efficientmodel training, we adopt an alternating optimization procedure which mimics thewell known Expectation-Maximization (EM) algorithm. The proposed method isgeneral and can be applied to any existing GNN architecture. Extensiveexperiments on five graph classification datasets demonstrate that ourframework outperforms state-of-the-art methods in both uncertaintyquantification and GNN interpretability. We also conduct case studies to showthat the decoded rationale structure can provide meaningful explanations.</description>
      <author>example@mail.com (Lingkai Kong, Haotian Sun, Yuchen Zhuang, Haorui Wang, Wenhao Mu, Chao Zhang)</author>
      <guid isPermaLink="false">2508.17097v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>GraphPPD: Posterior Predictive Modelling for Graph-Level Inference</title>
      <link>http://arxiv.org/abs/2508.16995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的变分建模框架用于图神经网络的后验预测分布，以在图级别学习任务中实现不确定性感知的预测。&lt;h4&gt;背景&lt;/h4&gt;在深度学习中，准确建模和量化预测不确定性对做出安全决策和理解模型置信度至关重要。近年来，图神经网络的研究兴趣激增，已有许多技术试图捕捉其预测中的不确定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够应用于图级别学习任务的不确定性量化方法，而不仅限于节点或链接级别的任务。&lt;h4&gt;方法&lt;/h4&gt;提出一种变分建模框架用于后验预测分布(PPD)，基于现有GNN导出的图级别嵌入，以数据自适应的方式学习PPD。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验结果证明了所提出方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该框架成功解决了图级别学习任务中的不确定性量化问题，为GNN在需要理解预测置信度的应用中提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;准确的预测不确定性建模和量化在深度学习中至关重要，因为它允许模型在数据模糊时做出更安全的决策，并促进用户对模型预测置信度的理解。随着近年来对图神经网络(GNNs)研究关注度的显著增加，已有许多技术努力捕捉其预测中的不确定性。然而，这些方法大多专门为节点或链接级别任务设计，不能直接应用于图级别学习问题。在本文中，我们提出了一种新颖的变分建模框架用于后验预测分布(PPD)，以在图级别学习任务中获得不确定性感知的预测。基于现有GNN之一导出的图级别嵌入，我们的框架可以以数据自适应的方式学习PPD。在几个基准数据集上的实验结果展示了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate modelling and quantification of predictive uncertainty is crucial indeep learning since it allows a model to make safer decisions when the data isambiguous and facilitates the users' understanding of the model's confidence inits predictions. Along with the tremendously increasing research focus on\emph{graph neural networks} (GNNs) in recent years, there have been numeroustechniques which strive to capture the uncertainty in their predictions.However, most of these approaches are specifically designed for node orlink-level tasks and cannot be directly applied to graph-level learningproblems. In this paper, we propose a novel variational modelling framework forthe \emph{posterior predictive distribution}~(PPD) to obtain uncertainty-awareprediction in graph-level learning tasks. Based on a graph-level embeddingderived from one of the existing GNNs, our framework can learn the PPD in adata-adaptive fashion. Experimental results on several benchmark datasetsexhibit the effectiveness of our approach.</description>
      <author>example@mail.com (Soumyasundar Pal, Liheng Ma, Amine Natik, Yingxue Zhang, Mark Coates)</author>
      <guid isPermaLink="false">2508.16995v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Score Matching on Large Geometric Graphs for Cosmology Generation</title>
      <link>http://arxiv.org/abs/2508.16990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于分数的生成模型，结合等变图神经网络，用于宇宙学模拟。该模型能够处理大规模数据，尊重物理约束，并在计算效率上优于现有方法。成功模拟了多达60万个暗物质晕的宇宙学点云，准确捕捉了聚类统计特性。&lt;h4&gt;背景&lt;/h4&gt;生成模型是产生宇宙学模拟的有前景的工具，但在可扩展性、物理一致性和域对称性遵循方面面临重大挑战，限制了它们作为N体模拟替代品的效用。&lt;h4&gt;目的&lt;/h4&gt;解决生成模型的局限性，引入一种基于分数的生成模型，具有等变图神经网络，模拟跨宇宙学的星系引力聚集，从有信息的先验开始，尊重周期边界，并扩展到模拟中的完整星系计数。&lt;h4&gt;方法&lt;/h4&gt;使用等变图神经网络和基于分数的生成模型，引入了一种新颖的拓扑感知噪声调度，对于处理大的几何图至关重要。该方法能够模拟宇宙学点云，最多可达60万个暗物质晕。&lt;h4&gt;主要发现&lt;/h4&gt;成功生成完整尺度的宇宙学点云（最多60万个暗物质晕），尊重周期性和均匀先验，在捕获聚类统计方面优于现有的扩散模型，同时提供显著的计算优势。&lt;h4&gt;结论&lt;/h4&gt;这项工作通过引入一种设计得类似于底层引力聚集和结构形成的生成模型推进了宇宙学，向宇宙中大尺度结构演变的物理上真实且高效的模拟器更近了一步。&lt;h4&gt;翻译&lt;/h4&gt;生成模型是产生宇宙学模拟的有前景的工具，但在可扩展性、物理一致性和域对称性遵循方面面临重大挑战，限制了它们作为N体模拟替代品的效用。为解决这些限制，我们引入了一种基于分数的生成模型，具有等变图神经网络，该模型从有信息的先验开始模拟跨宇宙学的星系引力聚集，尊重周期边界，并扩展到模拟中的完整星系计数。引入了一种新颖的拓扑感知噪声调度，对于大的几何图至关重要。所提出的等变基于分数的模型成功生成最多60万个暗物质晕的完整尺度宇宙学点云，尊重周期性和均匀先验，并在捕获聚类统计方面优于现有扩散模型，同时提供显著的计算优势。这项工作通过引入一种设计得类似于底层引力聚集和结构形成的生成模型推进了宇宙学，向宇宙中大尺度结构演变的物理上真实且高效的模拟器更近了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models are a promising tool to produce cosmological simulationsbut face significant challenges in scalability, physical consistency, andadherence to domain symmetries, limiting their utility as alternatives to$N$-body simulations. To address these limitations, we introduce a score-basedgenerative model with an equivariant graph neural network that simulatesgravitational clustering of galaxies across cosmologies starting from aninformed prior, respects periodic boundaries, and scales to full galaxy countsin simulations. A novel topology-aware noise schedule, crucial for largegeometric graphs, is introduced. The proposed equivariant score-based modelsuccessfully generates full-scale cosmological point clouds of up to 600,000halos, respects periodicity and a uniform prior, and outperforms existingdiffusion models in capturing clustering statistics while offering significantcomputational advantages. This work advances cosmology by introducing agenerative model designed to closely resemble the underlying gravitationalclustering of structure formation, moving closer to physically realistic andefficient simulators for the evolution of large-scale structures in theuniverse.</description>
      <author>example@mail.com (Diana-Alexandra Onutu, Yue Zhao, Joaquin Vanschoren, Vlado Menkovski)</author>
      <guid isPermaLink="false">2508.16990v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience</title>
      <link>http://arxiv.org/abs/2508.16836v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种物理信息神经符号方法，用于描述复杂网络的演化动力学，以预测产业链韧性。该方法通过学习物理实体活动状态的动力学并整合到多层时空共演化网络中，实现了物理符号动力学和时空共演化拓扑的联合学习。&lt;h4&gt;背景&lt;/h4&gt;产业链在国民经济可持续发展中扮演着越来越重要的角色。然而，作为典型的复杂网络，基于数据的深度学习在描述和分析复杂网络的韧性方面仍处于起步阶段，其核心是缺乏描述系统动力学的理论框架。&lt;h4&gt;目的&lt;/h4&gt;提出一种物理信息神经符号方法来描述复杂网络的演化动力学，以进行韧性预测。&lt;h4&gt;方法&lt;/h4&gt;学习物理实体活动状态的动力学，并将其整合到多层时空共演化网络中，利用物理信息方法实现物理符号动力学和时空共演化拓扑的联合学习，从而预测产业链韧性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型能够获得更好的结果，更准确有效地预测产业链弹性。&lt;h4&gt;结论&lt;/h4&gt;该方法对产业发展具有一定的实践意义。&lt;h4&gt;翻译&lt;/h4&gt;产业链在国民经济可持续发展中扮演着越来越重要的角色。然而，作为典型的复杂网络，基于数据的深度学习在描述和分析复杂网络的韧性方面仍处于起步阶段，其核心是缺乏描述系统动力学的理论框架。在本文中，我们提出了一种物理信息神经符号方法来描述复杂网络的演化动力学，以进行韧性预测。核心思想是学习物理实体活动状态的动力学，并将其整合到多层时空共演化网络中，利用物理信息方法实现物理符号动力学和时空共演化拓扑的联合学习，从而预测产业链韧性。实验结果表明，该模型能够获得更好的结果，更准确有效地预测产业链弹性，对产业发展具有一定的实践意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial chain plays an increasingly important role in the sustainabledevelopment of national economy. However, as a typical complex network,data-driven deep learning is still in its infancy in describing and analyzingthe resilience of complex networks, and its core is the lack of a theoreticalframework to describe the system dynamics. In this paper, we propose aphysically informative neural symbolic approach to describe the evolutionarydynamics of complex networks for resilient prediction. The core idea is tolearn the dynamics of the activity state of physical entities and integrate itinto the multi-layer spatiotemporal co-evolution network, and use the physicalinformation method to realize the joint learning of physical symbol dynamicsand spatiotemporal co-evolution topology, so as to predict the industrial chainresilience. The experimental results show that the model can obtain betterresults and predict the elasticity of the industry chain more accurately andeffectively, which has certain practical significance for the development ofthe industry.</description>
      <author>example@mail.com (Bicheng Wang, Junping Wang, Yibo Xue)</author>
      <guid isPermaLink="false">2508.16836v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Understanding and Tackling Over-Dilution in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.16829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended version of KDD '25 paper. 22 pages including appendix.  Conference version: KDD '25 (Toronto, Aug 3-7, 2025), pp. 1253-1261. Code:  https://github.com/LeeJunHyun/NATR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了消息传递神经网络(MPNNs)中被忽视的局限性，特别是节点信息在单层中的过度稀释问题，提出了过度稀释的概念及其两个稀释因子，并引入基于transformer的解决方案。&lt;h4&gt;背景&lt;/h4&gt;消息传递神经网络(MPNNs)在图机器学习中占据重要地位，但由于不规则的数据结构存在过度平滑和过度挤压等非预期行为，这些局限性的观察已成为构建信息丰富的图表示的基础。&lt;h4&gt;目的&lt;/h4&gt;深入研究MPNNs的局限性，专注于之前被忽视的方面，解决节点信息被过度稀释的问题，并提供新的见解来促进信息丰富表示的发展。&lt;h4&gt;方法&lt;/h4&gt;提出'过度稀释'(Over-dilution)的概念，用两个稀释因子形式化：节点内稀释用于属性级别，节点间稀释用于节点级别表示；引入基于transformer的解决方案来缓解过度稀释，补充现有的节点嵌入方法如MPNNs。&lt;h4&gt;主要发现&lt;/h4&gt;即使在单层中，特定于单个节点的信息也可能被显著稀释；提出了过度稀释的概念及其两个稀释因子，并展示了基于transformer的解决方案的有效性。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了新的见解，有助于信息丰富表示的发展，实现和补充材料已在GitHub上公开(https://github.com/LeeJunHyun/NATR)。&lt;h4&gt;翻译&lt;/h4&gt;消息传递神经网络(MPNNs)在图机器学习中占据关键地位，但由于不规则的数据结构，它们存在过度平滑和过度挤压等非预期行为。这些局限性的观察和构建已成为构建信息更丰富的图表示的基础。在本文中，我们深入研究MPNNs的局限性，专注于之前被忽视的方面。我们的观察显示，即使在单层中，特定于单个节点的信息也可能被显著稀释。为了深入探讨这一现象，我们提出了过度稀释的概念，并用两个稀释因子形式化：用于属性级别的节点内稀释和用于节点级别表示的节点间稀释。我们还引入了一种基于transformer的解决方案，该方案缓解了过度稀释，并补充了MPNNs等现有的节点嵌入方法。我们的研究结果提供了新的见解，并为信息丰富表示的发展做出了贡献。实现和补充材料已在https://github.com/LeeJunHyun/NATR公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3711896.3737168&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message Passing Neural Networks (MPNNs) hold a key position in machinelearning on graphs, but they struggle with unintended behaviors, such asover-smoothing and over-squashing, due to irregular data structures. Theobservation and formulation of these limitations have become foundational inconstructing more informative graph representations. In this paper, we delveinto the limitations of MPNNs, focusing on aspects that have previously beenoverlooked. Our observations reveal that even within a single layer, theinformation specific to an individual node can become significantly diluted. Todelve into this phenomenon in depth, we present the concept of Over-dilutionand formulate it with two dilution factors: intra-node dilution forattribute-level and inter-node dilution for node-level representations. We alsointroduce a transformer-based solution that alleviates over-dilution andcomplements existing node embedding methods like MPNNs. Our findings providenew insights and contribute to the development of informative representations.The implementation and supplementary materials are publicly available athttps://github.com/LeeJunHyun/NATR.</description>
      <author>example@mail.com (Junhyun Lee, Veronika Thost, Bumsoo Kim, Jaewoo Kang, Tengfei Ma)</author>
      <guid isPermaLink="false">2508.16829v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>DR-CircuitGNN: Training Acceleration of Heterogeneous Circuit Graph Neural Network on GPUs</title>
      <link>http://arxiv.org/abs/2508.16769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DR-CircuitGNN的快速GPU内核设计，用于加速EDA相关电路图数据集上的HGNNs训练。通过利用行稀疏感知的Dynamic-ReLU和优化SpMM内核，以及采用并行优化策略，该方法显著提高了HGNNs在EDA电路图处理中的性能。&lt;h4&gt;背景&lt;/h4&gt;随着集成电路设计的规模和复杂性不断增加，电子设计自动化(EDA)面临更多挑战。图神经网络(GNNs)因其能将电路自然表示为图而成为辅助EDA设计的有前景方法，但往往无法捕捉EDA设计的全部复杂性。异构图神经网络(HGNNs)能更好地解释EDA电路图，捕捉拓扑关系和几何特征，但其串行的模块化消息传递方案带来了更高的计算复杂性和处理成本，形成性能瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出DR-CircuitGNN，一种快速GPU内核设计，通过利用行稀疏感知的Dynamic-ReLU和优化异构消息传递过程中的SpMM内核，来加速EDA相关电路图数据集上的HGNNs训练。&lt;h4&gt;方法&lt;/h4&gt;提出了一种并行优化策略，通过多线程CPU初始化和多个cudaStreams的GPU内核执行，同时处理独立子图，最大化CPU-GPU并发性。同时利用行稀疏感知的Dynamic-ReLU和优化SpMM内核来加速HGNNs训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在三个代表性的CircuitNet设计(小、中、大)上，所提出的方法与前向和反向传播的最先进技术相比，分别可实现高达3.51倍和4.09倍的加速。在完整大小的CircuitNet和采样的小型CircuitNet上，该并行设计比官方的DGL实现cuSPARSE可实现高达2.71倍的加速，而对相关性分数和错误率的影响可以忽略不计。&lt;h4&gt;结论&lt;/h4&gt;DR-CircuitGNN通过优化HGNNs的训练过程，显著提高了EDA电路图处理的效率，同时保持了模型的相关性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;随着集成电路设计规模的不断扩大和复杂性的增加，电子设计自动化(EDA)面临的挑战日益增多。图神经网络(GNNs)已成为辅助EDA设计的一种有前景的方法，因为电路可以自然地表示为图。虽然GNNs为电路分析提供了基础，但它们往往无法捕捉EDA设计的全部复杂性。异构图神经网络(HGNNs)能更好地解释EDA电路图，因为它们能捕捉拓扑关系和几何特征。然而，改进的表示能力带来了更高的计算复杂性和处理成本，这是由于其串行的模块化消息传递方案造成的，形成了显著的性能瓶颈。在本文中，我们提出了DR-CircuitGNN，一种快速GPU内核设计，通过利用行稀疏感知的Dynamic-ReLU和优化异构消息传递过程中的SpMM内核，来加速EDA相关电路图数据集上的HGNNs训练。为进一步提升性能，我们提出了一种并行优化策略，通过多线程CPU初始化和多个cudaStreams的GPU内核执行，同时处理独立子图，最大化CPU-GPU并发性。我们的实验表明，在三个代表性的CircuitNet设计(小、中、大)上，所提出的方法与前向和反向传播的最先进技术相比，分别可实现高达3.51倍和4.09倍的加速。在完整大小的CircuitNet和采样的小型CircuitNet上，我们的并行设计比官方的DGL实现cuSPARSE可实现高达2.71倍的加速，同时对相关性分数和错误率的影响可以忽略不计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3721145.3734528&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing scale and complexity of integrated circuit design have led toincreased challenges in Electronic Design Automation (EDA). Graph NeuralNetworks (GNNs) have emerged as a promising approach to assist EDA design ascircuits can be naturally represented as graphs. While GNNs offer a foundationfor circuit analysis, they often fail to capture the full complexity of EDAdesigns. Heterogeneous Graph Neural Networks (HGNNs) can better interpret EDAcircuit graphs as they capture both topological relationships and geometricfeatures. However, the improved representation capability comes at the cost ofeven higher computational complexity and processing cost due to their serialmodule-wise message-passing scheme, creating a significant performancebottleneck. In this paper, we propose DR-CircuitGNN, a fast GPU kernel designby leveraging row-wise sparsity-aware Dynamic-ReLU and optimizing SpMM kernelsduring heterogeneous message-passing to accelerate HGNNs training onEDA-related circuit graph datasets. To further enhance performance, we proposea parallel optimization strategy that maximizes CPU-GPU concurrency byconcurrently processing independent subgraphs using multi-threaded CPUinitialization and GPU kernel execution via multiple cudaStreams. Ourexperiments show that on three representative CircuitNet designs (small,medium, large), the proposed method can achieve up to 3.51x and 4.09x speedupcompared to the SOTA for forward and backward propagation, respectively. Onfull-size CircuitNet and sampled Mini-CircuitNet, our parallel design enablesup to 2.71x speed up over the official DGL implementation cuSPARSE withnegligible impact on correlation scores and error rates.</description>
      <author>example@mail.com (Yuebo Luo, Shiyang Li, Junran Tao, Kiran Thorat, Xi Xie, Hongwu Peng, Nuo Xu, Caiwen Ding, Shaoyi Huang)</author>
      <guid isPermaLink="false">2508.16769v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation</title>
      <link>http://arxiv.org/abs/2508.18050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ArgusCogito的新型零样本思维链框架，用于解决伪装物体分割(COS)中的挑战，该框架通过跨模态协同和全方位推理在视觉语言模型中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;伪装物体分割(COS)面临重大挑战，因为目标与背景具有内在的高相似性，需要模型能够超越表面线索进行整体理解。现有方法受限于浅层特征表示、不足的推理机制和弱跨模态整合，导致目标分离不完整和分割不精确。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够进行整体理解而不仅仅是表面线索的模型，解决现有方法在伪装物体分割中的局限性。&lt;h4&gt;方法&lt;/h4&gt;受百眼巨人感知策略启发，提出ArgusCogito框架，包含三个认知启发阶段：(1)推测：通过跨模态融合进行全局推理构建认知先验；(2)聚焦：进行全方位注意力驱动扫描和推理；(3)雕塑：整合跨模态信息迭代生成高保真分割掩码。该框架基于视觉语言模型的跨模态协同和全方位推理。&lt;h4&gt;主要发现&lt;/h4&gt;在四个具有挑战性的COS基准和三个医学图像分割基准上评估，ArgusCogito实现了最先进的性能，验证了框架的卓越功效、优越泛化能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;ArgusCogito通过跨模态协同和全方位推理有效解决了伪装物体分割中的挑战，为该领域提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;伪装物体分割(COS)由于目标与背景之间的内在高相似性而构成重大挑战，需要模型能够超越表面线索进行深刻整体理解。现有方法通常受限于浅层特征表示、不足的推理机制和弱跨模态整合，难以实现这种深度的认知，导致普遍存在目标分离不完整和分割不精确等问题。受百眼巨人感知策略的启发——强调整体观察、全方位关注和仔细检查——我们引入了ArgusCogito，这是一种新颖的零样本、思维链框架，基于视觉语言模型(VLMs)中的跨模态协同和全方位推理。ArgusCogito协调三个认知启发阶段：(1)推测：通过跨模态融合(RGB、深度、语义图)进行全局推理构建强认知先验，实现整体场景理解和增强目标-背景区分。(2)聚焦：由推测阶段的语义先验指导，进行全方位、注意力驱动的扫描和聚焦推理，实现精确目标定位和感兴趣区域细化。(3)雕塑：通过整合跨模态信息并在聚焦区域内迭代生成密集的正/负点提示，逐步塑造高保真分割掩码，模拟Argus的仔细检查。在四个具有挑战性的COS基准和三个医学图像分割(MIS)基准上的广泛评估表明，ArgusCogito实现了最先进的(SOTA)性能，验证了框架的卓越功效、优越泛化能力和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camouflaged Object Segmentation (COS) poses a significant challenge due tothe intrinsic high similarity between targets and backgrounds, demanding modelscapable of profound holistic understanding beyond superficial cues. Prevailingmethods, often limited by shallow feature representation, inadequate reasoningmechanisms, and weak cross-modal integration, struggle to achieve this depth ofcognition, resulting in prevalent issues like incomplete target separation andimprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyedGiant-emphasizing holistic observation, omnidirectional focus, and intensivescrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thoughtframework underpinned by cross-modal synergy and omnidirectional reasoningwithin Vision-Language Models (VLMs). ArgusCogito orchestrates threecognitively-inspired stages: (1) Conjecture: Constructs a strong cognitiveprior through global reasoning with cross-modal fusion (RGB, depth, semanticmaps), enabling holistic scene understanding and enhanced target-backgrounddisambiguation. (2) Focus: Performs omnidirectional, attention-driven scanningand focused reasoning, guided by semantic priors from Conjecture, enablingprecise target localization and region-of-interest refinement. (3) Sculpting:Progressively sculpts high-fidelity segmentation masks by integratingcross-modal information and iteratively generating dense positive/negativepoint prompts within focused regions, emulating Argus' intensive scrutiny.Extensive evaluations on four challenging COS benchmarks and three MedicalImage Segmentation (MIS) benchmarks demonstrate that ArgusCogito achievesstate-of-the-art (SOTA) performance, validating the framework's exceptionalefficacy, superior generalization capability, and robustness.</description>
      <author>example@mail.com (Jianwen Tan, Huiyao Zhang, Rui Xiong, Han Zhou, Hongfei Wang, Ye Li)</author>
      <guid isPermaLink="false">2508.18050v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</title>
      <link>http://arxiv.org/abs/2508.17832v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出分层布局生成方法，用于解决3D室内场景生成中精细物体放置的问题，提高场景真实性和实用性。&lt;h4&gt;背景&lt;/h4&gt;逼真的3D室内场景生成对虚拟现实、室内设计等领域至关重要，现有方法在粗粒度家具布置上有进展，但难以捕捉精细粒度的物体放置。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在精细物体放置方面的局限性，提高生成环境的真实性和实用性，为需要详细3D环境的应用开辟新可能。&lt;h4&gt;方法&lt;/h4&gt;提出分层布局生成方法，包含精细粒度布局对齐模块（通过垂直水平解耦构建分层布局）和可训练布局优化网络（解决位置、方向和相交问题）。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验验证，该方法在生成逼真室内场景方面表现优于现有方法，能有效解决物体放置问题。&lt;h4&gt;结论&lt;/h4&gt;该工作推进了场景生成领域发展，为需要详细3D环境的应用提供新可能，作者计划在发表后发布代码以鼓励未来研究。&lt;h4&gt;翻译&lt;/h4&gt;逼真的3D室内场景生成对虚拟现实、室内设计、具身智能和场景理解至关重要。虽然现有方法在粗粒度家具布置方面取得了进展，但它们难以捕捉精细粒度的物体放置，限制了生成环境的真实性和实用性。这一差距阻碍了沉浸式虚拟体验和具身AI应用的详细场景理解。为了解决这些问题，我们提出了分层布局生成，一种用于精细粒度3D场景生成的新方法。该方法首次采用从粗到细的分层方法，从大规模家具布置到精细物体排列，逐步优化场景布局。具体而言，我们的精细粒度布局对齐模块通过垂直和水平解耦构建分层布局，有效将复杂的3D室内场景分解为多个粒度级别。此外，我们的可训练布局优化网络解决了放置问题，如不正确的位置、方向错误和物体相交，确保结构连贯且物理上合理的场景生成。我们通过大量实验证明了该方法的有效性，显示出与现有方法相比在生成逼真室内场景方面的优越性能。这项工作推进了场景生成领域的发展，并为需要详细3D环境的应用开辟了新的可能性。我们将在发表后发布代码，以鼓励未来的研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D室内场景生成方法在细粒度物体放置方面的不足。这个问题在现实中很重要，因为精细的物体布局对于虚拟现实的沉浸式体验、室内设计的精确呈现以及具身智能的场景理解都至关重要。现有方法虽然能生成粗粒度的家具布局，但难以精确放置小物件，限制了生成环境的真实性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是它们在细粒度物体放置方面的不足。他们设计了一个分层布局生成方法，从粗粒度的家具布置到精细的物体排列。作者借鉴了多种现有工作：使用GPT-4o提取场景信息，参考了基于检索和放置的程序内容生成方法，引入了垂直和水平解耦的概念，并使用了图注意力网络结构。整体上，作者融合了生成模型、大语言模型和程序内容生成等多种技术路线，创造了一个新的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用分层布局生成方法，从粗到细地构建3D室内场景，通过垂直和水平解耦将复杂场景分解为多个粒度级别，并使用可训练的布局优化网络解决放置问题。整体流程分为三个阶段：1)场景信息提取：使用GPT-4o从多模态输入中提取房间类型、家具类型和位置信息；2)粗粒度房间生成：构建基本房间结构并放置大致的家具；3)分层布局生成：通过细粒度布局对齐建立层次结构，再使用可训练布局优化网络优化物体位置和方向，确保物理合理性和与输入的一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)分层布局生成方法(HLG)实现从粗到细的场景生成；2)细粒度布局对齐模块，通过垂直解耦和水平解耦建立层次结构；3)可训练布局优化网络(TLO-Net)处理放置问题。相比之前工作，HLG更专注于细粒度物体放置，能有效处理复杂的层次关系和空间约束，使用可学习网络而非规则或模板，能更好地处理非矩形房间和复杂空间关系，在生成细粒度物体布局方面表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HLG提出了一种分层布局生成方法，通过从粗粒度到细粒度的层次化优化，解决了3D室内场景生成中细粒度物体放置的挑战，显著提高了生成场景的真实性和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Realistic 3D indoor scene generation is crucial for virtual reality, interiordesign, embodied intelligence, and scene understanding. While existing methodshave made progress in coarse-scale furniture arrangement, they struggle tocapture fine-grained object placements, limiting the realism and utility ofgenerated environments. This gap hinders immersive virtual experiences anddetailed scene comprehension for embodied AI applications. To address theseissues, we propose Hierarchical Layout Generation (HLG), a novel method forfine-grained 3D scene generation. HLG is the first to adopt a coarse-to-finehierarchical approach, refining scene layouts from large-scale furnitureplacement to intricate object arrangements. Specifically, our fine-grainedlayout alignment module constructs a hierarchical layout through vertical andhorizontal decoupling, effectively decomposing complex 3D indoor scenes intomultiple levels of granularity. Additionally, our trainable layout optimizationnetwork addresses placement issues, such as incorrect positioning, orientationerrors, and object intersections, ensuring structurally coherent and physicallyplausible scene generation. We demonstrate the effectiveness of our approachthrough extensive experiments, showing superior performance in generatingrealistic indoor scenes compared to existing methods. This work advances thefield of scene generation and opens new possibilities for applicationsrequiring detailed 3D environments. We will release our code upon publicationto encourage future research.</description>
      <author>example@mail.com (Xiping Wang, Yuxi Wang, Mengqi Zhou, Junsong Fan, Zhaoxiang Zhang)</author>
      <guid isPermaLink="false">2508.17832v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints</title>
      <link>http://arxiv.org/abs/2508.17595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at the IEEE/CVF International Conference on  Computer Vision (ICCV) Workshops, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TinyGiantVLM是一个轻量化和模块化的两阶段视觉语言模型框架，专门用于处理工业环境中的物理空间推理问题。该模型通过专家混合融合模块处理多模态输入，并在AI城市挑战赛2025中取得优异成绩。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言模型在处理仓库规模环境中的细粒度空间关系时面临重大挑战，难以理解现实工业环境中的3D布局、物体排列和多模态线索。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于物理空间推理的轻量化和模块化框架，区别于传统地理推理，适用于复杂物流场景。&lt;h4&gt;方法&lt;/h4&gt;提出TinyGiantVLM框架，使用预训练视觉主干网络从RGB和深度模态中编码全局和区域级别特征，引入专家混合(MoE)融合模块动态组合空间表示，采用两阶段训练策略增强空间推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;64M参数的基础模型在AI城市挑战赛2025第三赛道获得排行榜第五名，分数为66.8861；80M参数的变体在空间推理任务上表现出改进的性能。&lt;h4&gt;结论&lt;/h4&gt;TinyGiantVLM模型在工业环境中的空间推理任务上表现良好，两阶段框架和MoE融合模块有效提高了模型性能，能够有效处理复杂的空间推理问题。&lt;h4&gt;翻译&lt;/h4&gt;在仓库规模环境中推理细粒度空间关系对现有的视觉语言模型(VLMs)构成了重大挑战，这些模型通常难以理解现实工业环境中的3D布局、物体排列和多模态线索。在本文中，我们提出了TinyGiantVLM，这是一个轻量化和模块化的两阶段框架，专为物理空间推理而设计，区别于复杂物流场景中的传统地理推理。我们的方法使用预训练的视觉主干网络从RGB和深度模态中编码全局和区域级别特征。为了有效处理高模态输入和多样化问题类型的复杂性，我们整合了专家混合(MoE)融合模块，该模块动态组合空间表示以支持下游推理任务并提高收敛性。训练采用两阶段策略：第一阶段专注于生成自由形式的答案以增强空间推理能力，而第二阶段使用标准化答案进行评估。在AI城市挑战赛2025的第三赛道评估中，我们64M参数的基础模型以66.8861的分数在排行榜上获得第五名，展示了在连接工业环境中的视觉感知和空间理解方面的强大性能。我们进一步提出了一个80M参数的变体，具有扩展的MoE容量，在空间推理任务上表现出改进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在资源受限条件下对仓库规模环境中的细粒度空间关系进行推理的问题。这个问题很重要，因为现有视觉-语言模型难以理解工业环境中的3D布局、物体排列和多模态线索，而工业场景（如仓库、物流中心）的空间理解对AI系统至关重要，且这些场景通常需要在资源有限的条件下运行。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视觉-语言模型在工业空间推理中的局限性，然后借鉴了多项现有工作：采用T5作为语言骨干，CLIP-ViT和DPT处理视觉特征，以及FuseMoE中的MoE框架。作者设计了一个轻量级两阶段框架，通过双分支设计处理RGB和深度信息，结合全局和区域特征，并引入MoE模块针对不同空间推理任务进行专门处理。这种设计既考虑了模型效率，又增强了空间推理能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个轻量级视觉-语言模型，通过结合RGB和深度信息，同时考虑全局和区域特征，并使用任务专家混合机制来处理不同类型的空间推理问题。整体流程包括：1)从RGB和深度图像提取全局和区域特征；2)将问题中的掩码位置替换为区域标记并注入区域特征；3)使用编码器上下文化处理序列；4)通过跨注意力融合区域和全局特征；5)使用MoE层根据问题类型路由到专门专家；6)重新注入特征并生成答案；7)采用两阶段训练策略，先训练自由形式回答，再微调标准化答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)轻量级架构(仅64M-80M参数)，适合资源受限环境；2)双分支多模态特征提取，同时处理RGB和深度信息；3)任务特定的MoE融合，为四种空间推理任务设计专门专家；4)两阶段训练策略，平衡自由推理和标准化输出。相比之前工作，TinyGiantVLM更注重资源效率，专注于物理空间推理而非地理推理，针对工业场景优化，并采用独特的多模态融合策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TinyGiantVLM提出了一种轻量级的视觉-语言架构，通过结合RGB和深度信息，并采用任务专家混合机制，实现了在资源受限条件下对工业环境中的空间关系进行高效推理。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning about fine-grained spatial relationships in warehouse-scaleenvironments poses a significant challenge for existing vision-language models(VLMs), which often struggle to comprehend 3D layouts, object arrangements, andmultimodal cues in real-world industrial settings. In this paper, we presentTinyGiantVLM, a lightweight and modular two-stage framework designed forphysical spatial reasoning, distinguishing itself from traditional geographicreasoning in complex logistics scenes. Our approach encodes both global andregion-level features from RGB and depth modalities using pretrained visualbackbones. To effectively handle the complexity of high-modality inputs anddiverse question types, we incorporate a Mixture-of-Experts (MoE) fusionmodule, which dynamically combines spatial representations to supportdownstream reasoning tasks and improve convergence. Training is conducted in atwo-phase strategy: the first phase focuses on generating free-form answers toenhance spatial reasoning ability, while the second phase uses normalizedanswers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our64M-parameter base model achieved 5th place on the leaderboard with a score of66.8861, demonstrating strong performance in bridging visual perception andspatial understanding in industrial environments. We further present an80M-parameter variant with expanded MoE capacity, which demonstrates improvedperformance on spatial reasoning tasks.</description>
      <author>example@mail.com (Vinh-Thuan Ly, Hoang M. Truong, Xuan-Huong Nguyen)</author>
      <guid isPermaLink="false">2508.17595v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Investigating Domain Gaps for Indoor 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.17439v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对室内3D物体检测的领域自适应问题，提出了一个全面的基准测试框架，分析了不同类型的数据集差异对检测器性能的影响，并提供了改进适应性能的方法。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测作为室内场景理解的基本任务，已在室内点云数据上取得了显著进展。然而，现有研究主要在有限数据集上进行，训练和测试数据集共享相同分布，限制了检测器的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;研究如何将室内3D物体检测器从一个数据集适应到另一个数据集，构建包含多个数据集的综合基准，并分析不同领域差距对检测器的影响。&lt;h4&gt;方法&lt;/h4&gt;利用ScanNet、SUN RGB-D和3D Front数据集以及新提出的大规模数据集ProcTHOR-OD和ProcFront，在合成到真实适应、点云质量适应、布局适应和实例特征适应等多种场景下进行实验，分析不同领域差距的影响，并引入提高适应性能的方法。&lt;h4&gt;主要发现&lt;/h4&gt;室内点云数据集因收集和构建方式不同，导致检测器可能过度拟合到特定因素；不同类型的领域差距（点云质量、布局和实例特征）对3D物体检测器性能有不同影响。&lt;h4&gt;结论&lt;/h4&gt;未来工作应致力于开发跨领域泛化能力更强的3D物体检测器，本研究为领域自适应室内3D物体检测提供了基线。&lt;h4&gt;翻译&lt;/h4&gt;作为室内场景理解的基本任务，3D物体检测已被广泛研究，并且在室内点云数据上的准确性已得到显著提高。然而，现有研究都是在有限数据集上进行的，训练集和测试集共享相同分布。在本文中，我们考虑将室内3D物体检测器从一个数据集适应到另一个数据集的任务，提出了一个包含ScanNet、SUN RGB-D和3D Front数据集的综合基准，以及我们通过3D模拟器新提出的大规模数据集ProcTHOR-OD和ProcFront。由于室内点云数据集是以不同方式收集和构建的，物体检测器可能会过度拟合到每个数据集内的特定因素，如点云质量、边界框布局和实例特征。我们在不同的适应场景（包括合成到真实适应、点云质量适应、布局适应和实例特征适应）上进行了跨数据集实验，分析了不同领域差距对3D物体检测器的影响。我们还引入了几种提高适应性能的方法，为领域自适应室内3D物体检测提供了基线，希望未来的工作能够提出跨领域泛化能力更强的检测器。我们的项目主页可以在https://jeremyzhao1998.github.io/DAVoteNet-release/找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决室内3D物体检测中的领域适应问题，即如何让在一个数据集上训练的检测器能够适应另一个不同分布的数据集。这个问题在现实中很重要，因为实际应用中检测器往往需要在部署环境中处理与训练数据分布不同的数据，而现有研究很少关注这一挑战。室内场景点云可通过多种方式构建，导致更复杂的领域差距因素，且室内3D检测需要区分多种物体类别，具有更高的物体多样性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有室内3D物体检测研究的局限性，指出它们主要在相同分布的数据集上进行训练和测试。作者借鉴了其他领域的领域适应研究，包括点云数据分类任务、室外LiDAR检测任务和室内语义分割任务中的领域适应方法。同时，作者指出了室内3D物体检测的特殊挑战，如点云多样性、物体类别多、物体近距离放置等。基于这些分析，作者构建了新的数据集并设计了全面的基准测试，以隔离和评估不同类型的领域差距因素。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建全面的基准测试来评估和改进室内3D物体检测的领域适应能力，同时隔离和评估不同类型的领域差距因素。整体实现流程包括：1) 构建新数据集ProcTHOR-OD和ProcFront，结合现有数据集ScanNet、SUN RGB-D和3D Front；2) 设计四种领域适应场景（合成到现实、点云质量、布局和实例适应）的六个基准测试；3) 使用VoteNet检测器进行评估，分析不同领域差距因素对性能的影响；4) 实现并评估多种领域适应方法，包括使用目标域先验和无监督领域适应技术。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出ProcTHOR-OD和ProcFront两个新数据集，具有高可扩展性和可控性，规模比现有数据集大一个数量级；2) 构建全面的基准测试，涵盖四种领域适应场景；3) 系统分析不同领域差距因素对3D物体检测器的影响。相比之前的工作，这篇论文不仅关注合成到现实的适应，还考虑了点云质量、布局和实例差异等更广泛的领域差距因素；提供的大规模可控数据集能够精确隔离特定因素；评估方法更加系统化，为实际应用提供了有价值的见解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建新的数据集和全面的基准测试，系统研究了室内3D物体检测中的领域差距问题，并提供了多种领域适应方法的基线，为开发具有更强跨领域泛化能力的检测器奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758275&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a fundamental task for indoor scene understanding, 3D object detection hasbeen extensively studied, and the accuracy on indoor point cloud data has beensubstantially improved. However, existing researches have been conducted onlimited datasets, where the training and testing sets share the samedistribution. In this paper, we consider the task of adapting indoor 3D objectdetectors from one dataset to another, presenting a comprehensive benchmarkwith ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposedlarge-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator.Since indoor point cloud datasets are collected and constructed in differentways, the object detectors are likely to overfit to specific factors withineach dataset, such as point cloud quality, bounding box layout and instancefeatures. We conduct experiments across datasets on different adaptationscenarios including synthetic-to-real adaptation, point cloud qualityadaptation, layout adaptation and instance feature adaptation, analyzing theimpact of different domain gaps on 3D object detectors. We also introduceseveral approaches to improve adaptation performances, providing baselines fordomain adaptive indoor 3D object detection, hoping that future works maypropose detectors with stronger generalization ability across domains. Ourproject homepage can be found inhttps://jeremyzhao1998.github.io/DAVoteNet-release/.</description>
      <author>example@mail.com (Zijing Zhao, Zhu Xu, Qingchao Chen, Yuxin Peng, Yang Liu)</author>
      <guid isPermaLink="false">2508.17439v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing</title>
      <link>http://arxiv.org/abs/2508.17435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RefineEdit-Agent是一种新型的无需训练的智能体框架，通过结合大语言模型和视觉语言大模型的能力，实现了复杂、迭代和上下文感知的图像编辑，解决了现有T2I模型在细粒度指令理解、上下文保持和智能反馈方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;文本到图像生成模型虽然能力显著，但在实际应用中需要细粒度、迭代式图像编辑，而现有方法在细粒度指令理解、修改过程中的上下文保持以及智能反馈机制方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理复杂、迭代和上下文感知图像编辑的智能体框架，解决现有T2I模型在实际应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;RefineEdit-Agent框架包含LVLM驱动的指令解析和场景理解模块、多级LLM驱动的编辑规划器、迭代图像编辑模块以及LVLM驱动的反馈和评估循环，形成一个闭环系统。&lt;h4&gt;主要发现&lt;/h4&gt;在LongBench-T2I-Edit基准测试中，RefineEdit-Agent平均得分为3.67，显著优于Direct Re-Prompting(2.29)、InstructPix2Pix(2.91)、GLIGEN-based Edit(3.16)和ControlNet-XL(3.39)等基线方法。&lt;h4&gt;结论&lt;/h4&gt;RefineEdit-Agent通过结合LLMs的规划能力和LVLMs的视觉理解评估能力，有效解决了T2I模型在图像编辑方面的挑战，提供了卓越的编辑保真度和上下文保持能力。&lt;h4&gt;翻译&lt;/h4&gt;尽管文本到图像(T2I)生成模型的能力显著，但实际应用通常需要细粒度、迭代式的图像编辑，而现有方法难以提供这种功能。主要挑战包括细粒度指令理解、修改过程中的鲁棒性上下文保持以及缺乏用于迭代优化的智能反馈机制。本文介绍了RefineEdit-Agent，一种新颖的无需训练的智能体框架，通过实现复杂、迭代和上下文感知的图像编辑来解决这些局限性。RefineEdit-Agent在一个闭环系统中利用了大语言模型(LLMs)的强大规划能力和视觉语言大模型(LVLMs)的高级视觉理解和评估能力。我们的框架包括LVLM驱动的指令解析和场景理解模块、多级LLM驱动的编辑规划器用于目标分解、工具选择和序列生成、迭代图像编辑模块以及关键的LVLM驱动的反馈和评估循环。为了严格评估RefineEdit-Agent，我们提出了LongBench-T2I-Edit新基准，包含500张初始图像和跨九个视觉维度的复杂多轮编辑指令。大量实验表明，RefineEdit-Agent显著优于最先进的基线方法，在LongBench-T2I-Edit上平均得分为3.67，而Direct Re-Prompting为2.29，InstructPix2Pix为2.91，GLIGEN-based Edit为3.16，ControlNet-XL为3.39。消融研究、人工评估以及对迭代优化、主干选择、工具使用和指令复杂度鲁棒性的分析进一步验证了我们的智能体设计在提供卓越编辑保真度和上下文保持方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the remarkable capabilities of text-to-image (T2I) generation models,real-world applications often demand fine-grained, iterative image editing thatexisting methods struggle to provide. Key challenges include granularinstruction understanding, robust context preservation during modifications,and the lack of intelligent feedback mechanisms for iterative refinement. Thispaper introduces RefineEdit-Agent, a novel, training-free intelligent agentframework designed to address these limitations by enabling complex, iterative,and context-aware image editing. RefineEdit-Agent leverages the powerfulplanning capabilities of Large Language Models (LLMs) and the advanced visualunderstanding and evaluation prowess of Vision-Language Large Models (LVLMs)within a closed-loop system. Our framework comprises an LVLM-driven instructionparser and scene understanding module, a multi-level LLM-driven editing plannerfor goal decomposition, tool selection, and sequence generation, an iterativeimage editing module, and a crucial LVLM-driven feedback and evaluation loop.To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a newbenchmark featuring 500 initial images with complex, multi-turn editinginstructions across nine visual dimensions. Extensive experiments demonstratethat RefineEdit-Agent significantly outperforms state-of-the-art baselines,achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 forDirect Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses ofiterative refinement, backbone choices, tool usage, and robustness toinstruction complexity further validate the efficacy of our agentic design indelivering superior edit fidelity and context preservation.</description>
      <author>example@mail.com (Zihan Liang, Jiahao Sun, Haoran Ma)</author>
      <guid isPermaLink="false">2508.17435v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>ResLink: A Novel Deep Learning Architecture for Brain Tumor Classification with Area Attention and Residual Connections</title>
      <link>http://arxiv.org/abs/2508.17259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ResLink的新型深度学习架构，用于基于CT扫描图像的脑肿瘤分类，该模型结合了区域注意力机制和残差连接，实现了95%的高准确率。&lt;h4&gt;背景&lt;/h4&gt;脑肿瘤因其可能影响关键神经功能而构成重大健康挑战，早期准确诊断对于有效治疗至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型深度学习架构，提高脑肿瘤分类的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了ResLink架构，集成了区域注意力机制与残差连接，采用多阶段卷积管道，包含dropout、正则化和下采样，最后通过基于注意力的细化进行分类，并在平衡数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;ResLink在脑肿瘤分类任务中实现了95%的高准确率，并表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ResLink在改善脑肿瘤分类方面具有潜力，为医学成像应用提供了一种稳健且高效的技术。&lt;h4&gt;翻译&lt;/h4&gt;脑肿瘤由于其可能影响关键神经功能而构成重大健康挑战。早期准确诊断对于有效治疗至关重要。在本研究中，我们提出了ResLink，一种用于基于CT扫描图像的脑肿瘤分类的新型深度学习架构。ResLink将新颖的区域注意力机制与残差连接相结合，以增强空间丰富图像分类任务的特征学习和空间理解。该模型采用多阶段卷积管道，包含dropout、正则化和下采样，随后是基于注意力的最终细化用于分类。在平衡数据集上训练的ResLink实现了95%的高准确率，并表现出强大的泛化能力。这项研究展示了ResLink在改善脑肿瘤分类方面的潜力，为医学成像应用提供了一种稳健且高效的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain tumors show significant health challenges due to their potential tocause critical neurological functions. Early and accurate diagnosis is crucialfor effective treatment. In this research, we propose ResLink, a novel deeplearning architecture for brain tumor classification using CT scan images.ResLink integrates novel area attention mechanisms with residual connections toenhance feature learning and spatial understanding for spatially rich imageclassification tasks. The model employs a multi-stage convolutional pipeline,incorporating dropout, regularization, and downsampling, followed by a finalattention-based refinement for classification. Trained on a balanced dataset,ResLink achieves a high accuracy of 95% and demonstrates stronggeneralizability. This research demonstrates the potential of ResLink inimproving brain tumor classification, offering a robust and efficient techniquefor medical imaging applications.</description>
      <author>example@mail.com (Sumedha Arya, Nirmal Gaud)</author>
      <guid isPermaLink="false">2508.17259v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</title>
      <link>http://arxiv.org/abs/2508.17255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SEER-VAR，一个用于车载第一人称视角增强现实(AR)的新框架，结合了语义分解、上下文感知SLAM分支和LLM驱动的推荐功能。&lt;h4&gt;背景&lt;/h4&gt;现有的系统假设静态或单视角设置，而本文提出的SEER-VAR能够动态分离驾驶舱和道路场景。&lt;h4&gt;目的&lt;/h4&gt;创建一个能够在不同环境中实现稳健空间对齐和感知一致的AR渲染系统，探索基于LLM的AR推荐在第一人称驾驶中的应用。&lt;h4&gt;方法&lt;/h4&gt;通过深度引导的视觉语言动态分离驾驶舱和道路场景；使用两个SLAM分支跟踪每个上下文中的第一人称运动；基于GPT的模块生成上下文感知的叠加信息；引入EgoSLAM-Drive数据集；使用结构化提示和详细用户研究。&lt;h4&gt;主要发现&lt;/h4&gt;SEER-VAR在各种环境中实现了稳健的空间对齐和感知一致的AR渲染；增强了场景理解、叠加相关性和驾驶便利性。&lt;h4&gt;结论&lt;/h4&gt;SEER-VAR为这一领域的未来研究提供了有效的基础，代码和数据集将开源。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了SEER-VAR，一个用于车载第一人称视角增强现实(AR)的新框架，它统一了语义分解、上下文感知SLAM分支和LLM驱动的推荐。与假设静态或单视角设置的现有系统不同，SEER-VAR通过深度引导的视觉语言动态分离驾驶舱和道路场景。两个SLAM分支跟踪每个上下文中的第一人称运动，而基于GPT的模块生成上下文感知的叠加信息，如仪表盘提示和危险警报。为了支持评估，我们引入了EgoSLAM-Drive，一个包含同步第一人称视图、6DoF真实位姿和多样化驾驶场景中AR注释的真实世界数据集。实验证明，SEER-VAR在各种环境中实现了稳健的空间对齐和感知一致的AR渲染。作为首批探索基于LLM的AR推荐在第一人称驾驶中的应用之一，我们通过结构化提示和详细用户研究解决了缺乏可比系统的问题。结果表明，SEER-VAR增强了场景理解、叠加相关性和驾驶便利性，为这一方向的未来研究提供了有效的基础。代码和数据集将开源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在动态驾驶环境中实现可靠的第一人称视角增强现实(AR)系统的问题。具体包括三个挑战：如何从单目第一人称视频中可靠定位多上下文场景、如何语义分割和推理这些环境以实现任务相关感知、以及如何生成适应变化视觉环境的连贯AR覆盖层。这个问题在现实中很重要，因为驾驶是安全关键的领域，AI驱动的AR系统能通过智能叠加信息增强驾驶员情境感知，提高驾驶安全性；在研究中也很重要，因为现有系统通常假设静态环境，无法同时处理车辆内部和外部场景的动态变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有工作局限性来设计方法，注意到大多数第一人称SLAM和AR系统假设静态环境，无法处理多域观察的空间-时间复杂性。他们借鉴了多项现有工作：视觉-语言基础模型(如Grounding DINO)用于对象检测，深度估计模型(如Depth Anything V2)获取场景几何信息，SLAM框架(基于ORB-SLAM3)进行定位，以及大型语言模型(GPT)进行语义推理。作者创新性地将这些技术组合，并针对驾驶场景的特殊需求进行了改进，特别是通过双SLAM分支处理内部和外部场景的不同动态特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度引导的语义分解将驾驶场景分离为内部(驾驶舱)和外部(道路)两个独立上下文，分别进行SLAM跟踪，然后利用大型语言模型生成上下文感知的AR覆盖层。整体流程包括：1)上下文编码：使用深度估计和视觉-语言模型分离内部和外部场景；2)多上下文姿态估计：通过上下文感知SLAM分支分别估计两个场景的相机轨迹；3)基于GPT的语义推荐：处理视觉输入和车辆状态，生成语义对齐的AR覆盖层；4)AR渲染：将生成的覆盖层稳定地渲染到对应场景中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SEER-VAR框架，实现驾驶舱和道路场景的独立SLAM跟踪；2)深度引导的语义分解方法，分离第一人称视图中的不同环境；3)基于GPT的AR推荐模块，生成上下文感知的覆盖层；4)EgoSLAM-Drive数据集，提供真实世界驾驶场景的基准。相比之前工作，不同之处在于：现有SLAM系统通常单分支处理，无法解耦多域场景；大多数视觉-语言模型计算成本高限制了实时应用；之前的LLM方法主要用于静态环境；传统AR系统依赖静态场景和预定义内容，而本文方法能处理动态驾驶环境中的实时空间定位和语义推理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SEER-VAR通过结合语义分解、多上下文SLAM和大型语言模型驱动推荐，首次实现了在动态驾驶环境中可靠的第一人称视角增强现实系统，同时发布了新的EgoSLAM-Drive数据集支持这一研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present SEER-VAR, a novel framework for egocentric vehicle-based augmentedreality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches(CASB), and LLM-driven recommendation. Unlike existing systems that assumestatic or single-view settings, SEER-VAR dynamically separates cabin and roadscenes via depth-guided vision-language grounding. Two SLAM branches trackegocentric motion in each context, while a GPT-based module generatescontext-aware overlays such as dashboard cues and hazard alerts. To supportevaluation, we introduce EgoSLAM-Drive, a real-world dataset featuringsynchronized egocentric views, 6DoF ground-truth poses, and AR annotationsacross diverse driving scenarios. Experiments demonstrate that SEER-VARachieves robust spatial alignment and perceptually coherent AR rendering acrossvaried environments. As one of the first to explore LLM-based AR recommendationin egocentric driving, we address the lack of comparable systems throughstructured prompting and detailed user studies. Results show that SEER-VARenhances perceived scene understanding, overlay relevance, and driver ease,providing an effective foundation for future research in this direction. Codeand dataset will be made open source.</description>
      <author>example@mail.com (Yuzhi Lai, Shenghai Yuan, Peizheng Li, Jun Lou, Andreas Zell)</author>
      <guid isPermaLink="false">2508.17255v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding</title>
      <link>http://arxiv.org/abs/2508.17205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 16 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种基于专家混合策略的多智能体框架，用于全面的高速公路场景理解，通过结合大型通用视觉-语言模型和小型高效模型，同时解决多个关键感知任务，并在各种交通和环境条件下表现出良好性能。&lt;h4&gt;背景&lt;/h4&gt;高速公路场景理解对于交通安全至关重要，需要同时考虑多种感知任务和不同环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时处理多个关键感知任务（如天气分类、路面湿度评估和交通拥堵检测）的框架，同时平衡准确性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;使用大型通用视觉-语言模型（如GPT-4o）结合领域知识生成特定任务的思维链提示，然后指导小型高效的VLM（如Qwen2.5-VL-7B）进行推理，同时整合多模态数据。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在各种交通和环境条件下都表现出一致的良好性能，多模态推理（特别是结合视频流和道路天气传感器数据）对路面湿度评估特别有效。&lt;h4&gt;结论&lt;/h4&gt;该框架可轻松与现有交通摄像头系统集成，适用于高风险农村地区，即使在资源受限的环境中也能增强态势感知并提供及时警报。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了一个用于全面高速公路场景理解的多智能体框架，围绕专家混合策略设计。在该框架中，大型通用视觉-语言模型（如GPT-4o）通过领域知识进行上下文化，以生成特定任务的思维链(CoT)提示。然后，这些精细提示用于指导小型高效的VLM（如Qwen2.5-VL-7B）对短视频进行推理，并结合其他互补模态。该框架同时解决多个关键感知任务，包括天气分类、路面湿度评估和交通拥堵检测，在保持准确性和计算效率的同时实现了强大的多任务推理。为了支持实证验证，我们整理了三个与这些任务对齐的专业数据集。值得注意的是，路面湿度数据集是多模态的，结合视频流和道路天气传感器数据，突出了多模态推理的优势。实验结果表明，在各种交通和环境条件下都表现出一致的良好性能。从部署角度来看，该框架可以轻松与现有的交通摄像头系统集成，并战略性地应用于高风险农村地区，如急转弯、易受洪水影响的低地或结冰桥梁。通过持续监控目标地点，系统即使在资源受限的环境中也能增强态势感知并提供及时警报。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a multi-agent framework for comprehensive highway sceneunderstanding, designed around a mixture-of-experts strategy. In thisframework, a large generic vision-language model (VLM), such as GPT-4o, iscontextualized with domain knowledge to generates task-specificchain-of-thought (CoT) prompts. These fine-grained prompts are then used toguide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over shortvideos, along with complementary modalities as applicable. The frameworksimultaneously addresses multiple critical perception tasks, including weatherclassification, pavement wetness assessment, and traffic congestion detection,achieving robust multi-task reasoning while balancing accuracy andcomputational efficiency. To support empirical validation, we curated threespecialized datasets aligned with these tasks. Notably, the pavement wetnessdataset is multimodal, combining video streams with road weather sensor data,highlighting the benefits of multimodal reasoning. Experimental resultsdemonstrate consistently strong performance across diverse traffic andenvironmental conditions. From a deployment perspective, the framework can bereadily integrated with existing traffic camera systems and strategicallyapplied to high-risk rural locations, such as sharp curves, flood-pronelowlands, or icy bridges. By continuously monitoring the targeted sites, thesystem enhances situational awareness and delivers timely alerts, even inresource-constrained environments.</description>
      <author>example@mail.com (Yunxiang Yang, Ningning Xu, Jidong J. Yang)</author>
      <guid isPermaLink="false">2508.17205v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models</title>
      <link>http://arxiv.org/abs/2508.17050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PVNet是一种基于扩散模型的点-体素交互框架，用于LiDAR点云上采样，无需密集监督，支持任意上采样率，在复杂室外场景中表现出优异的泛化能力，达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;准确的室外环境3D场景理解依赖于高质量的点云数据，但LiDAR扫描的数据通常极其稀疏，严重阻碍了下游3D感知任务。现有的点云上采样方法主要关注单个物体，因此在复杂室外场景中表现出有限的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有点云上采样方法在复杂室外场景中泛化能力有限的问题，提出一种无需密集监督的LiDAR点云上采样方法。&lt;h4&gt;方法&lt;/h4&gt;提出PVNet框架，采用基于分类器引导的DDPMs来引导生成，使用稀疏点云作为引导条件，从附近帧合成的点云作为输入；设计体素完成模块细化和完成粗糙的体素特征；提出点-体素交互模块整合点和体素特征，提高上采样点的环境感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;据作者所知，PVNet是第一个支持任意上采样率的场景级点云上采样方法。&lt;h4&gt;结论&lt;/h4&gt;在各种基准测试上的大量实验表明，PVNet方法达到了最先进的性能，源代码将在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;室外环境中的准确3D场景理解严重依赖于高质量的点云。然而，LiDAR扫描的数据常常极其稀疏，严重阻碍了下游3D感知任务。现有的点云上采样方法主要关注单个物体，因此在复杂室外场景中表现出有限的泛化能力。为解决这一问题，我们提出了PVNet，一种基于扩散模型的点-体素交互框架，用于执行无需密集监督的LiDAR点云上采样。具体来说，我们采用基于分类器引导的DDPMs来引导生成，其中使用稀疏点云作为引导条件，以及从其附近帧合成的点云作为输入。此外，我们设计了一个体素完成模块来细化和完成粗糙的体素特征，以丰富特征表示。另外，我们提出了一个点-体素交互模块来整合点和体素的特征，有效提高了每个上采样点的环境感知能力。据我们所知，我们的方法是第一个支持任意上采样率的场景级点云上采样方法。在各种基准上的大量实验表明，我们的方法达到了最先进的性能。源代码将在https://github.com/chengxianjing/PVNet上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR扫描点云过于稀疏的问题。这个问题在自动驾驶、机器人导航和增强现实等领域非常重要，因为这些应用需要密集的3D点云来准确理解周围环境，进行物体检测、场景重建和定位等任务。稀疏的点云会严重阻碍下游3D感知任务的性能，而现有的点云上采样方法主要针对单个物体，难以处理复杂的户外场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，即它们主要针对单个物体，无法很好地处理复杂的户外场景。他们发现户外场景点云上采样面临两个主要挑战：缺乏密集点云作为监督，以及场景点云规模更大、结构更复杂。作者借鉴了扩散模型(DDPMs)的基本框架和分类器自由引导技术，同时创新性地设计了体素补全模块和点-体素交互模块。他们还借鉴了U-Net架构用于特征优化和KNN算法用于点与体素的交互，将这些现有技术进行了针对性的改进和组合，以解决户外场景点云上采样这一特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; PVNet的核心思想是利用扩散模型生成密集点云，同时使用稀疏点云作为指导条件，并通过点-体素交互增强每个点的环境感知能力。整体流程包括：1)接收稀疏点云作为指导条件和合成点云作为输入；2)对合成点云添加噪声并体素化；3)使用体素补全模块完善和丰富体素特征；4)通过U-Net优化体素特征；5)通过点-体素交互模块整合点和体素特征；6)使用分类器自由引导的DDPMs预测噪声并迭代生成密集点云；7)添加噪声正则化项提高预测精确度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个支持任意上采样率的场景级点云上采样方法；2)无需密集监督，使用稀疏点云作为指导条件；3)创新的体素补全模块，有效完善粗糙特征；4)点-体素交互模块增强环境感知能力；5)噪声正则化提高预测精确度；6)去除归一化和偏置保留空间关系。相比之前工作，PVNet从应用场景(从物体级到场景级)、监督方式(从密集监督到稀疏指导)、技术路线(从PointNet/GAN到扩散模型+点-体素交互)和性能表现(在多个指标上优于现有方法)都有显著不同。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PVNet首次将扩散模型与点-体素交互相结合，实现了无需密集监督的场景级LiDAR点云任意倍率上采样，显著提升了复杂户外环境中的3D感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D scene understanding in outdoor environments heavily relies onhigh-quality point clouds. However, LiDAR-scanned data often suffer fromextreme sparsity, severely hindering downstream 3D perception tasks. Existingpoint cloud upsampling methods primarily focus on individual objects, thusdemonstrating limited generalization capability for complex outdoor scenes. Toaddress this issue, we propose PVNet, a diffusion model-based point-voxelinteraction framework to perform LiDAR point cloud upsampling without densesupervision. Specifically, we adopt the classifier-free guidance-based DDPMs toguide the generation, in which we employ a sparse point cloud as the guidingcondition and the synthesized point clouds derived from its nearby frames asthe input. Moreover, we design a voxel completion module to refine and completethe coarse voxel features for enriching the feature representation. Inaddition, we propose a point-voxel interaction module to integrate featuresfrom both points and voxels, which efficiently improves the environmentalperception capability of each upsampled point. To the best of our knowledge,our approach is the first scene-level point cloud upsampling method supportingarbitrary upsampling rates. Extensive experiments on various benchmarksdemonstrate that our method achieves state-of-the-art performance. The sourcecode will be available at https://github.com/chengxianjing/PVNet.</description>
      <author>example@mail.com (Xianjing Cheng, Lintai Wu, Zuowen Wang, Junhui Hou, Jie Wen, Yong Xu)</author>
      <guid isPermaLink="false">2508.17050v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>ASCMamba: Multimodal Time-Frequency Mamba for Acoustic Scene Classification</title>
      <link>http://arxiv.org/abs/2508.15632v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为ASCMamba的多模态网络，用于解决声景分类任务，通过整合音频和文本信息实现细粒度声景理解和有效的多模态声景分类。&lt;h4&gt;背景&lt;/h4&gt;声景分类是计算听觉领域的基础问题，在APSIPA ASC 2025 Grand Challenge中，组织者引入了多模态ASC任务，不同于传统仅依赖音频输入的系统，这个挑战提供了额外的文本信息作为输入，包括音频录制位置和录制时间。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合音频和文本信息的多模态网络，用于细粒度声景理解和有效的多模态声景分类，以应对APSIPA ASC 2025 Grand Challenge中的ASC任务。&lt;h4&gt;方法&lt;/h4&gt;提出一个名为ASCMamba的多模态网络，使用DenseEncoder从频谱图中提取分层频谱特征，采用双路径Mamba块基于Mamba状态空间模型捕获长程时间和频率依赖关系，并引入两步伪标记机制生成更可靠的伪标记。&lt;h4&gt;主要发现&lt;/h4&gt;提出的ASCMamba系统在挑战赛中优于所有参赛团队，比基线提高了6.2%的性能。&lt;h4&gt;结论&lt;/h4&gt;ASCMamba系统通过整合音频和文本信息，有效地提升了声景分类性能，相关代码、模型和预训练检查点已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;声景分类是计算听觉领域的基础问题，旨在根据独特的声学特征对环境进行分类。在APSIPA ASC 2025 Grand Challenge的ASC任务中，组织者引入了多模态ASC任务。与仅依赖音频输入的传统ASC系统不同，该挑战提供了额外的文本信息作为输入，包括音频录制位置和录制时间。在本文中，我们提出了在APSIPA ASC 2025 Grand Challenge中用于ASC任务的系统。具体来说，我们提出了一个多模态网络ASCMamba，它整合了音频和文本信息，用于细粒度声景理解和有效的多模态ASC。所提出的ASCMamba采用DenseEncoder从频谱图中提取分层频谱特征，然后使用基于Mamba的状态空间模型的双路径Mamba块捕获长程时间和频率依赖关系。此外，我们提出了一个两步伪标记机制来生成更可靠的伪标记。结果表明，所提出的系统优于所有参赛团队，比基线提高了6.2%。代码、模型和预训练检查点可在https://github.com/S-Orion/ASCMamba.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acoustic Scene Classification (ASC) is a fundamental problem in computationalaudition, which seeks to classify environments based on the distinctiveacoustic features. In the ASC task of the APSIPA ASC 2025 Grand Challenge, theorganizers introduce a multimodal ASC task. Unlike traditional ASC systems thatrely solely on audio inputs, this challenge provides additional textualinformation as inputs, including the location where the audio is recorded andthe time of recording. In this paper, we present our proposed system for theASC task in the APSIPA ASC 2025 Grand Challenge. Specifically, we propose amultimodal network, ASCMamba, which integrates audio and textual informationfor fine-grained acoustic scene understanding and effective multimodal ASC. Theproposed ASCMamba employs a DenseEncoder to extract hierarchical spectralfeatures from spectrograms, followed by a dual-path Mamba blocks that capturelong-range temporal and frequency dependencies using Mamba-based state spacemodels. In addition, we present a two-step pseudo-labeling mechanism togenerate more reliable pseudo-labels. Results show that the proposed systemoutperforms all the participating teams and achieves a 6.2% improvement overthe baseline. Code, model and pre-trained checkpoints are available athttps://github.com/S-Orion/ASCMamba.git.</description>
      <author>example@mail.com (Bochao Sun, Dong Wang, ZhanLong Yang, Jun Yang, Han Yin)</author>
      <guid isPermaLink="false">2508.15632v2</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</title>
      <link>http://arxiv.org/abs/2508.18249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多模态自监督框架，用于机器人的可通行性估计，通过整合足迹、LiDAR和摄像头数据，解决了现有方法无法捕捉不可通行区域特征和单一模态局限性问题。&lt;h4&gt;背景&lt;/h4&gt;可通行性估计对机器人导航至关重要，现有自监督学习方法常无法捕捉不可通行区域特征，且多数研究只关注单一模态，忽视了异构传感模态的互补优势。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态自监督框架，用于可通行性标记和估计，整合多种传感模态以获得更鲁棒的可通行性估计。&lt;h4&gt;方法&lt;/h4&gt;集成足迹、LiDAR和摄像头数据作为视觉基础模型的提示生成可通行性标签；训练双流网络以解耦方式联合学习不同模态；加入基于稀疏LiDAR的监督减轻伪标签噪声；在城市、非铺装路面和校园环境中进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;自动标记方法在各数据集上达到约88%的IoU；与现有自监督最先进方法相比，多模态网络在所有评估数据集上IoU提高1.6-3.5%。&lt;h4&gt;结论&lt;/h4&gt;所提出的多模态自监督框架有效解决了可通行性估计中的挑战，通过整合多种传感模态实现了更准确的可通行性模式识别。&lt;h4&gt;翻译&lt;/h4&gt;可通行性估计对于使机器人能够穿越不同地形和环境至关重要。虽然最近的自监督学习方法取得了有希望的结果，但它们通常无法捕捉不可通行区域的特征。此外，大多数先前的工作集中在单一模态上，忽视了通过整合异构传感模态提供的互补优势以实现更鲁棒的可通行性估计。为了解决这些局限性，我们提出了一个用于可通行性标记和估计的多模态自监督框架。首先，我们的标注流程集成足迹、LiDAR和摄像头数据作为视觉基础模型的提示，生成考虑语义和几何线索的可通行性标签。然后，利用这些标签，我们训练一个双流网络，以解耦方式联合学习不同模态，增强其识别多样化可通行性模式的能力。此外，我们整合了基于稀疏LiDAR的监督以减轻伪标签引入的噪声。最后，在城市、非铺装路面和校园环境中进行的广泛实验证明了我们方法的有效性。所提出的自动标记方法在各种数据集上始终达到约88%的IoU。与现有的自监督最先进方法相比，我们的多模态可通行性估计网络在各种评估数据集上产生更高的IoU，提高了1.6-3.5%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决两个问题：一是现有自监督学习方法无法充分捕捉不可通行区域的特征；二是大多数工作仅关注单一模态，忽略了整合不同传感模态的优势。这个问题在机器人导航中至关重要，因为准确识别不可通行区域对机器人安全至关重要，特别是在复杂环境中，结合视觉和几何信息能更全面评估通行性，同时自监督方法减少了对大量人工标注数据的依赖，提高了方法的泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后提出多模态自监督框架。设计思路包括：利用足迹、LiDAR和相机数据作为视觉基础模型的提示生成通行性标签；设计双流网络解耦学习不同模态信息；引入稀疏LiDAR监督减轻噪声。该方法借鉴了现有工作：利用视觉基础模型(SAM)进行自动标记；借鉴自监督学习减少标注依赖；融合语义和几何信息处理；参考双流网络架构设计思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合语义和几何信息进行通行性估计，利用多模态数据生成高质量自动标记，设计双流网络分别处理语义和几何信息，并在解码阶段融合，同时利用稀疏LiDAR监督提高训练鲁棒性。整体流程分为两阶段：1)自动标记阶段：投影足迹到图像，结合SAM和DINOv2生成语义掩码，计算LiDAR几何特征，构建精细提示候选，通过最远点采样和激活分数选择最终标签；2)通行性估计阶段：使用双流网络分别处理视觉和LiDAR数据，结合Lovász损失和交叉熵损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)自动标记管道结合足迹、相机和LiDAR数据生成高质量标签；2)双流网络解耦处理语义和几何信息；3)稀疏LiDAR监督减轻标记噪声。相比之前工作的不同：1)多模态融合而非单一模态；2)结合语义和几何先验生成更准确标签；3)双流网络能更好捕捉语义和几何风险；4)结合Lovász损失和交叉熵损失提高训练鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种多模态自监督框架，通过结合语义和几何信息，实现了高质量的场景无关通行性自动标记和估计，显著提高了机器人导航的安全性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traversability estimation is critical for enabling robots to navigate acrossdiverse terrains and environments. While recent self-supervised learningmethods achieve promising results, they often fail to capture thecharacteristics of non-traversable regions. Moreover, most prior worksconcentrate on a single modality, overlooking the complementary strengthsoffered by integrating heterogeneous sensory modalities for more robusttraversability estimation. To address these limitations, we propose amultimodal self-supervised framework for traversability labeling andestimation. First, our annotation pipeline integrates footprint, LiDAR, andcamera data as prompts for a vision foundation model, generating traversabilitylabels that account for both semantic and geometric cues. Then, leveragingthese labels, we train a dual-stream network that jointly learns from differentmodalities in a decoupled manner, enhancing its capacity to recognize diversetraversability patterns. In addition, we incorporate sparse LiDAR-basedsupervision to mitigate the noise introduced by pseudo labels. Finally,extensive experiments conducted across urban, off-road, and campus environmentsdemonstrate the effectiveness of our approach. The proposed automatic labelingmethod consistently achieves around 88% IoU across diverse datasets. Comparedto existing self-supervised state-of-the-art methods, our multimodaltraversability estimation network yields consistently higher IoU, improving by1.6-3.5% on all evaluated datasets.</description>
      <author>example@mail.com (Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen)</author>
      <guid isPermaLink="false">2508.18249v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Unraveling the cognitive patterns of Large Language Models through module communities</title>
      <link>http://arxiv.org/abs/2508.18192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过借鉴生物学中理解新兴认知的方法，开发了一个连接认知技能、LLM架构和数据集的网络框架，揭示了LLMs内部模块的独特技能分布模式，发现LLMs的技能获取受益于动态跨区域相互作用和神经可塑性，为LLM可解释性提供了新见解，并建议采用分布式学习动力学进行微调。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)已在科学、工程和社会领域取得重大进展，应用范围包括科学发现、医学诊断和聊天机器人等。然而，LLMs的内在机制隐藏在数十亿参数和复杂结构中，其内部架构和认知过程难以理解，这构成了当前研究的挑战。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决LLMs内部机制难以理解的问题，通过开发一个网络框架连接认知技能、LLM架构和数据集，为基础模型分析带来范式转变，从而提高LLMs的可解释性，并指导更有效的微调策略。&lt;h4&gt;方法&lt;/h4&gt;研究采用理解生物学中新兴认知的方法，开发了一个基于网络的框架，该框架连接认知技能、LLM架构和数据集。通过分析模块社区中的技能分布，研究比较了LLMs与生物系统在认知组织上的异同，并探索了技能获取的动力学机制。&lt;h4&gt;主要发现&lt;/h4&gt;1) LLMs展现出独特的模块社区，其涌现的技能模式部分反映了鸟类和小型哺乳动物大脑中分布式且相互连接的认知组织；2) LLMs并不严格遵循特定生物系统中观察到的聚焦专业化；3) 从生物系统到LLMs的关键差异在于，LLMs的技能获取显著受益于动态的跨区域相互作用和神经可塑性。&lt;h4&gt;结论&lt;/h4&gt;通过整合认知科学与机器学习，该框架为LLM可解释性提供了新见解，并表明有效的微调策略应利用分布式学习动力学，而不是僵化的模块干预。这一发现有助于更好地理解和优化LLMs的内部工作机制。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)通过从科学发现、医学诊断到聊天机器人等应用，在科学、工程和社会领域取得了重大进展，重塑了我们的世界。尽管它们无处不在且有用，但LLMs的内在机制仍然隐藏在数十亿参数和复杂结构中，使其内部架构和认知过程难以理解。我们通过采用理解生物学中新兴认知的方法，开发了一个连接认知技能、LLM架构和数据集的网络框架，填补了这一空白，为基础模型分析带来了范式转变。模块社区中的技能分布表明，虽然LLMs并不严格遵循特定生物系统中观察到的聚焦专业化，但它们展现出独特的模块社区，其涌现的技能模式部分反映了鸟类和小型哺乳动物大脑中分布式且相互连接的认知组织。我们的数值结果突显了从生物系统到LLMs的关键差异，即技能获取显著受益于动态的跨区域相互作用和神经可塑性。通过整合认知科学原理与机器学习，我们的框架为LLM可解释性提供了新见解，并表明有效的微调策略应利用分布式学习动力学，而不是僵化的模块干预。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have reshaped our world with significantadvancements in science, engineering, and society through applications rangingfrom scientific discoveries and medical diagnostics to Chatbots. Despite theirubiquity and utility, the underlying mechanisms of LLM remain concealed withinbillions of parameters and complex structures, making their inner architectureand cognitive processes challenging to comprehend. We address this gap byadopting approaches to understanding emerging cognition in biology anddeveloping a network-based framework that links cognitive skills, LLMarchitectures, and datasets, ushering in a paradigm shift in foundation modelanalysis. The skill distribution in the module communities demonstrates thatwhile LLMs do not strictly parallel the focalized specialization observed inspecific biological systems, they exhibit unique communities of modules whoseemergent skill patterns partially mirror the distributed yet interconnectedcognitive organization seen in avian and small mammalian brains. Our numericalresults highlight a key divergence from biological systems to LLMs, where skillacquisition benefits substantially from dynamic, cross-regional interactionsand neural plasticity. By integrating cognitive science principles with machinelearning, our framework provides new insights into LLM interpretability andsuggests that effective fine-tuning strategies should leverage distributedlearning dynamics rather than rigid modular interventions.</description>
      <author>example@mail.com (Kushal Raj Bhandari, Pin-Yu Chen, Jianxi Gao)</author>
      <guid isPermaLink="false">2508.18192v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images</title>
      <link>http://arxiv.org/abs/2508.18067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  All codes and models will be released at  https://github.com/earth-insights/SegEarth-OV-2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SegEarth-OV，第一个无需标注的遥感图像开放词汇分割框架，解决了现有方法在处理遥感数据独特复杂性时的不足。&lt;h4&gt;背景&lt;/h4&gt;遥感图像语义分割对地球观测至关重要，但解释新目标类别的高成本和手动标注的昂贵费用带来了显著挑战。现有为自然图像设计的开放词汇语义分割框架无法充分处理遥感数据的独特复杂性，如巨大尺度变化和细粒度细节。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标注的开放词汇分割框架，能够有效处理遥感图像的独特复杂性，并扩展到其他具有挑战性的遥感模态如SAR图像。&lt;h4&gt;方法&lt;/h4&gt;提出了SegEarth-OV框架，包含：1) SimFeatUp通用上采样器，可从粗糙特征中恢复高分辨率空间细节；2) Global Bias Alleviation操作，从块特征中减去固有全局上下文，提高局部语义保真度；3) AlignEarth蒸馏策略，将光学VLM编码器的语义知识高效转移到SAR编码器。&lt;h4&gt;主要发现&lt;/h4&gt;SegEarth-OF能够有效利用预训练VLM的丰富语义，使开放词汇语义分割在光学遥感场景中成为可能，并在光学和SAR数据集上实现了最先进方法的显著改进。&lt;h4&gt;结论&lt;/h4&gt;SegEarth-OV为无需标注和开放世界的地球观测建立了坚实基础，能够跨不同传感器类型实现通用开放词汇语义分割。&lt;h4&gt;翻译&lt;/h4&gt;遥感图像的语义分割对全面地球观测至关重要，但需要解释新目标类别以及手动标注的高成本带来了重大挑战。虽然开放词汇语义分割提供了一种有前途的解决方案，但为自然图像设计的现有框架无法满足遥感数据的独特复杂性。它们难以处理巨大的尺度变化和细粒度细节，且其适应通常依赖于大量昂贵的标注。为解决这一关键差距，本文介绍了SegEarth-OV，这是首个用于遥感图像无需标注的开放词汇分割框架。具体而言，我们提出了SimFeatUp，一个通用的上采样器，可以从粗糙特征中稳健地恢复高分辨率空间细节，无需任何任务特定的后训练即可纠正扭曲的目标形状。我们还提出了一种简单而有效的全局偏差缓解操作，从块特征中减去固有的全局上下文，显著提高局部语义保真度。这些组件使SegEarth-OV能够有效利用预训练VLM的丰富语义，使开放词汇语义分割在光学遥感场景中成为可能。此外，为了将框架的通用性扩展到其他具有挑战性的遥感模态如SAR图像，在这些模态中大规模VLM不可用且创建成本高昂，我们引入了AlignEarth，这是一种基于蒸馏的策略，可以高效地将语义知识从光学VLM编码器转移到SAR编码器，绕过了从头构建SAR基础模型的需要，实现了跨不同传感器类型的通用开放词汇语义分割。在光学和SAR数据集上的大量实验验证了SegEarth-OV能够实现比最先进方法的显著改进，为无需标注和开放世界的地球观测建立了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of remote sensing (RS) images is pivotal forcomprehensive Earth observation, but the demand for interpreting new objectcategories, coupled with the high expense of manual annotation, posessignificant challenges. Although open-vocabulary semantic segmentation (OVSS)offers a promising solution, existing frameworks designed for natural imagesare insufficient for the unique complexities of RS data. They struggle withvast scale variations and fine-grained details, and their adaptation oftenrelies on extensive, costly annotations. To address this critical gap, thispaper introduces SegEarth-OV, the first framework for annotation-freeopen-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,a universal upsampler that robustly restores high-resolution spatial detailsfrom coarse features, correcting distorted target shapes without anytask-specific post-training. We also present a simple yet effective Global BiasAlleviation operation to subtract the inherent global context from patchfeatures, significantly enhancing local semantic fidelity. These componentsempower SegEarth-OV to effectively harness the rich semantics of pre-trainedVLMs, making OVSS possible in optical RS contexts. Furthermore, to extend theframework's universality to other challenging RS modalities like SAR images,where large-scale VLMs are unavailable and expensive to create, we introduceAlignEarth, which is a distillation-based strategy and can efficiently transfersemantic knowledge from an optical VLM encoder to an SAR encoder, bypassing theneed to build SAR foundation models from scratch and enabling universal OVSSacross diverse sensor types. Extensive experiments on both optical and SARdatasets validate that SegEarth-OV can achieve dramatic improvements over theSOTA methods, establishing a robust foundation for annotation-free andopen-world Earth observation.</description>
      <author>example@mail.com (Kaiyu Li, Xiangyong Cao, Ruixun Liu, Shihong Wang, Zixuan Jiang, Zhi Wang, Deyu Meng)</author>
      <guid isPermaLink="false">2508.18067v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</title>
      <link>http://arxiv.org/abs/2508.17916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出EndoUFM，一种用于内窥镜手术的非监督单目深度估计框架，通过整合双基础模型和创新的微调策略，解决了现有技术在手术环境中性能有限的问题，实验证明其达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;深度估计是微创内窥镜手术中3D重建的基础，但现有单目深度估计技术在手术环境中的变化光照和复杂纹理下性能有限。视觉基础模型虽然在自然图像上表现良好，但在内窥镜应用中存在领域适应性限制和语义感知缺陷。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应手术环境的深度估计框架，提高内窥镜手术中的空间感知能力，从而增强手术精度和安全性。&lt;h4&gt;方法&lt;/h4&gt;提出EndoUFM框架，整合双基础模型用于手术场景；采用随机向量低秩适应的自适应微调策略；设计基于深度可分离卷积的残差块；引入掩码引导的平滑损失以保持解剖组织结构内的深度一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在SCARED、Hamlyn、SERV-CT和EndoNeRF数据集上的实验表明，该方法在保持高效模型尺寸的同时达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;EndoUFM框架能够有效增强外科医生在微创手术过程中的空间感知，提高手术精度和安全性，对增强现实和导航系统具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;深度估计是微创内窥镜手术中3D重建的基础组成部分。然而，现有的单目深度估计技术在面对手术环境中的变化光照和复杂纹理时往往表现出有限性能。虽然强大的视觉基础模型提供了有前景的解决方案，但它们在自然图像上的训练导致在内窥镜应用中存在显著的领域适应性限制和语义感知缺陷。在本研究中，我们引入了EndoUFM，一个创新的非监督单目深度估计框架，它将双基础模型整合用于手术场景，通过利用强大的预学习先验来增强深度估计性能。该框架采用了一种新颖的自适应微调策略，结合随机向量低秩适应来提高模型适应性，并基于深度可分离卷积的残差块来改进细粒度局部特征的捕获。此外，我们设计了一个掩码引导的平滑损失，以强制在解剖组织结构内保持深度一致性。在SCARED、Hamlyn、SERV-CT和EndoNeRF数据集上的大量实验证实，我们的方法在保持高效模型尺寸的同时达到了最先进的性能。这项工作有助于增强外科医生在微创手术过程中的空间感知，从而提高手术精度和安全性，对增强现实和导航系统具有关键意义。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决内窥镜手术中的单目深度估计问题。现有方法在处理内窥镜环境中变化的照明条件和复杂纹理时表现有限。这个问题很重要，因为深度估计是微创手术中3D重建的基础，能帮助医生提高手术精度和安全性，弥补小切口下医生难以直接感知空间深度的不足，对增强现实和手术导航系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有单目深度估计方法在内窥镜环境中的局限性，然后发现大型视觉基础模型在自然图像上表现优异但直接应用于内窥镜图像时存在领域适应性问题。他们借鉴了基于CNN的无监督深度估计方法、基础模型在深度估计中的应用、低秩适应方法和医学图像分割模型等现有工作。基于这些，他们设计了一个结合Depth Anything和Segment Anything Model的双基础模型框架，并提出了随机向量低秩适应(RVLoRA)和基于深度可分离卷积的残差块(Res-DSC)来解决适应性问题，同时设计了掩码引导的平滑损失确保语义一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用双基础模型的预学习先验知识增强内窥镜深度估计，通过创新的微调策略适应内窥镜领域，结合CNN和Transformer的优势捕获特征，并确保解剖组织结构内的深度一致性。整体流程包括：1)构建基于Depth Anything的深度估计网络和基于SAM的图像分解网络；2)应用RVLoRA微调两个基础模型；3)在特定位置插入Res-DSC块捕获局部特征；4)设计包含反射率一致性损失、分解-合成损失、映射-合成损失和掩码引导平滑损失的函数；5)使用自监督目标端到端训练所有模块。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创新性地整合Depth Anything和Segment Anything Model的双基础模型框架；2)提出随机向量低秩适应(RVLoRA)增强模型适应性；3)设计基于深度可分离卷积的残差块(Res-DSC)捕获局部特征；4)利用掩码引导的平滑损失确保解剖组织结构内的深度一致性。相比之前的工作，不同之处在于：与传统CNN方法相比，利用基础模型的强大全局上下文建模能力；与直接应用基础模型相比，解决了领域适应性问题；与其他基础模型微调方法相比，使用双基础模型和更有效的微调策略；与其他低秩适应方法相比，引入随机向量增强适应性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EndoUFM通过创新性地整合双基础模型、设计自适应微调策略和专门的损失函数，显著提升了内窥镜图像中无监督单目深度估计的准确性和鲁棒性，为微创手术中的增强现实和导航系统提供了更可靠的空间感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth estimation is a foundational component for 3D reconstruction inminimally invasive endoscopic surgeries. However, existing monocular depthestimation techniques often exhibit limited performance to the varyingillumination and complex textures of the surgical environment. While powerfulvisual foundation models offer a promising solution, their training on naturalimages leads to significant domain adaptability limitations and semanticperception deficiencies when applied to endoscopy. In this study, we introduceEndoUFM, an unsupervised monocular depth estimation framework that innovativelyintegrating dual foundation models for surgical scenes, which enhance the depthestimation performance by leveraging the powerful pre-learned priors. Theframework features a novel adaptive fine-tuning strategy that incorporatesRandom Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and aResidual block based on Depthwise Separable Convolution (Res-DSC) to improvethe capture of fine-grained local features. Furthermore, we design amask-guided smoothness loss to enforce depth consistency within anatomicaltissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, andEndoNeRF datasets confirm that our method achieves state-of-the-art performancewhile maintaining an efficient model size. This work contributes to augmentingsurgeons' spatial perception during minimally invasive procedures, therebyenhancing surgical precision and safety, with crucial implications foraugmented reality and navigation systems.</description>
      <author>example@mail.com (Xinning Yao, Bo Liu, Bojian Li, Jingjing Wang, Jinghua Yue, Fugen Zhou)</author>
      <guid isPermaLink="false">2508.17916v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization</title>
      <link>http://arxiv.org/abs/2508.17816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了UniSino，一种用于通用CT正弦图标准化的基础模型，直接在投影域对数据进行标准化，在各种下采样场景中表现出强大的泛化能力和优异的重建质量。&lt;h4&gt;背景&lt;/h4&gt;CT成像原始数据采集过程中，下采样和噪声等因素会降低正弦图质量，导致重建图像中出现严重伪影和噪声，影响诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;解决传统校正方法缺乏通用性的问题，开发一种能够处理多种类型伪影的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出UniSino基础模型，直接在投影域而非图像域对数据进行标准化，训练框架融入正弦图的物理特性，增强泛化能力，在四个基准数据集的多个子任务中进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，UniSino在单一和混合下采样情况下都能实现优异的重建质量，在CT成像的正弦图增强方面表现出卓越的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;UniSino作为一种创新的基础模型，直接在投影域工作，能够有效处理CT成像中的正弦图质量问题，具有广泛的临床应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;在CT成像的原始数据采集过程中，各种因素会降低收集到的正弦图质量，其中下采样和噪声会导致重建图像中产生严重的伪影和噪声，从而影响诊断准确性。传统的校正方法依赖于手动设计的算法或固定的经验参数，但这些方法通常对不同类型的伪影缺乏通用性。为了解决这些限制，我们提出了UniSino，一种用于通用CT正弦图标准化的基础模型。与现有在图像域操作的基础模型不同，UniSino直接在投影域对数据进行标准化，这使其能够在各种下采样场景中实现更强的泛化能力。其训练框架融入了正弦图的物理特性，增强了泛化能力，并在四个基准数据集的多个子任务中实现了稳健的性能。实验结果表明，UniSino在单一和混合下采样情况下都能实现优异的重建质量，在CT成像的正弦图增强方面表现出卓越的鲁棒性和泛化能力。代码可在以下网址获取：https://github.com/yqx7150/UniSino。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决CT成像中正弦图数据因欠采样和噪声等因素降质的问题。这个问题在现实中非常重要，因为这些降质因素会导致重建图像中出现严重伪影和噪声，影响诊断准确性；同时，传统校正方法缺乏泛化能力，而现有深度学习方法通常针对单一任务设计，难以应对临床中多样化的伪影类型和扫描协议。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了CT成像中正弦图降质的多种原因和传统方法的局限性，然后借鉴了基础模型在大规模多样化数据上预训练实现跨任务泛化的思想。他们设计了UniSino模型，包含正弦图变化自编码器(SinoVAE)和潜在细化扩散(LRD)两个主要组件。SinoVAE负责学习物理约束的潜在表示，LRD负责执行退化正弦图的条件细化。作者还整合了物理知识到训练目标中，并采用双路径潜在表示来分离全局结构和伪影相关特征，这些都是对现有工作的创新性应用和拓展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接在CT投影域（正弦图域）进行标准化，而非图像域，这样可以更早地解决错误源，避免在重建过程中被放大。整体流程分为两个阶段：第一阶段训练SinoVAE模块，使用自监督方法从完全采样的正弦图生成模拟欠采样样本对，通过编码器-解码器结构学习潜在表示，并整合物理约束和对抗学习；第二阶段训练条件扩散模型，将欠采样正弦图的潜在编码作为条件，通过迭代细化优化潜在空间，最终生成标准化的正弦图。物理引导的正弦图损失(SinoLoss)确保了跨视图一致性和物理合理性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个用于CT正弦图标准化的基础模型UniSino；2)直接在投影域而非图像域工作，更早解决错误源；3)整合感知压缩和扩散框架提高潜在空间质量；4)双路径潜在表示（全频+高频）分离全局结构和伪影特征；5)物理引导的投影域损失函数确保物理一致性；6)随机退化混合策略增强泛化能力。相比之前工作，UniSino不依赖手动设计的算法或固定参数，能处理多种任务（环形伪影、低剂量噪声等），整合了物理知识提高重建质量，参数效率更高且泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniSino通过在CT投影域直接应用物理引导的基础模型，实现了对多种类型欠采样和伪影的统一标准化处理，显著提高了重建图像质量并增强了临床应用的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; During raw-data acquisition in CT imaging, diverse factors can degrade thecollected sinograms, with undersampling and noise leading to severe artifactsand noise in reconstructed images and compromising diagnostic accuracy.Conventional correction methods rely on manually designed algorithms or fixedempirical parameters, but these approaches often lack generalizability acrossheterogeneous artifact types. To address these limitations, we propose UniSino,a foundation model for universal CT sinogram standardization. Unlike existingfoundational models that operate in image domain, UniSino directly standardizesdata in the projection domain, which enables stronger generalization acrossdiverse undersampling scenarios. Its training framework incorporates thephysical characteristics of sinograms, enhancing generalization and enablingrobust performance across multiple subtasks spanning four benchmark datasets.Experimental results demonstrate thatUniSino achieves superior reconstructionquality both single and mixed undersampling case, demonstrating exceptionalrobustness and generalization in sinogram enhancement for CT imaging. The codeis available at: https://github.com/yqx7150/UniSino.</description>
      <author>example@mail.com (Xingyu Ai, Shaoyu Wang, Zhiyuan Jia, Ao Xu, Hongming Shan, Jianhua Ma, Qiegen Liu)</author>
      <guid isPermaLink="false">2508.17816v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Proximal Supervised Fine-Tuning</title>
      <link>http://arxiv.org/abs/2508.17784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为近端监督微调(PSFT)的新方法，解决了监督微调基础模型时的泛化能力差问题，有效限制了策略漂移并保持了有竞争力的调优效果。&lt;h4&gt;背景&lt;/h4&gt;监督微调基础模型通常会导致泛化能力差，在新任务或领域上调整后，先前能力会退化。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的微调方法，解决SFT的泛化问题，有效限制策略漂移，同时保持有竞争力的调优效果。&lt;h4&gt;方法&lt;/h4&gt;受强化学习中的信任域策略优化(TRPO)和近端策略优化(PPO)启发，提出了近端监督微调(PSFT)。通过将SFT视为具有恒定正优势的策略梯度方法的特例，推导出PSFT可以稳定优化并导致泛化。&lt;h4&gt;主要发现&lt;/h4&gt;在数学和人类价值领域的实验表明，PSFT在领域内与SFT相当，在领域外泛化方面优于SFT，在长时间训练下保持稳定而不会导致熵崩溃，并为后续优化提供了更强的基础。&lt;h4&gt;结论&lt;/h4&gt;PSFT是一种有效的微调方法，能够解决SFT的泛化问题，为后续优化提供了更强的基础。&lt;h4&gt;翻译&lt;/h4&gt;基础模型的监督微调(SFT)通常会导致泛化能力差，在新任务或领域上调整后，先前能力会退化。受强化学习(RL)中的信任域策略优化(TRPO)和近端策略优化(PPO)启发，我们提出了近端监督微调(PSFT)。这种微调目标结合了信任域的好处，有效限制了SFT过程中的策略漂移，同时保持有竞争力的调优效果。通过将SFT视为具有恒定正优势的策略梯度方法的特例，我们推导出PSFT可以稳定优化并导致泛化，同时为后续训练阶段的进一步优化留出空间。在数学和人类价值领域的实验表明，PSFT在领域内与SFT相当，在领域外泛化方面优于SFT，在长时间训练下保持稳定而不会导致熵崩溃，并为后续优化提供了更强的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised fine-tuning (SFT) of foundation models often leads to poorgeneralization, where prior capabilities deteriorate after tuning on new tasksor domains. Inspired by trust-region policy optimization (TRPO) and proximalpolicy optimization (PPO) in reinforcement learning (RL), we propose ProximalSFT (PSFT). This fine-tuning objective incorporates the benefits oftrust-region, effectively constraining policy drift during SFT whilemaintaining competitive tuning. By viewing SFT as a special case of policygradient methods with constant positive advantages, we derive PSFT thatstabilizes optimization and leads to generalization, while leaving room forfurther optimization in subsequent post-training stages. Experiments acrossmathematical and human-value domains show that PSFT matches SFT in-domain,outperforms it in out-of-domain generalization, remains stable under prolongedtraining without causing entropy collapse, and provides a stronger foundationfor the subsequent optimization.</description>
      <author>example@mail.com (Wenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, Pengfei Liu)</author>
      <guid isPermaLink="false">2508.17784v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications</title>
      <link>http://arxiv.org/abs/2508.17753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the workshop on Foundation Models for Social Robotics  (FoMoSR) at ICSR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了四种最先进的自动语音识别系统在八个公开数据集上的表现，这些数据集涵盖了六种难度维度：领域特定、口音、噪声、年龄变化、障碍和自发言语。研究发现在标准基准测试上得分相似的系统，在性能、幻觉倾向和固有偏见方面存在显著差异，这些差异对人机交互领域有重要影响。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的自动语音识别系统需要处理不完美的音频，这些音频通常因硬件限制或环境噪声而降级，同时还需要适应不同的用户群体。在人机交互中，这些挑战相互交织，形成了独特的识别环境。&lt;h4&gt;目的&lt;/h4&gt;评估四种最先进的自动语音识别系统在不同难度维度上的性能，揭示它们在真实世界场景中的局限性和偏见。&lt;h4&gt;方法&lt;/h4&gt;研究人员在八个公开可用的数据集上评估了四种最先进的自动语音识别系统，这些数据集涵盖了六种难度维度：领域特定、口音、噪声、年龄变化、障碍和自发言语。&lt;h4&gt;主要发现&lt;/h4&gt;尽管四种系统在标准基准测试上得分相似，但分析显示它们在性能、幻觉倾向和固有偏见方面存在显著差异。这些差异对人机交互领域有严重影响，因为识别错误可能干扰任务执行、用户信任和安全。&lt;h4&gt;结论&lt;/h4&gt;自动语音识别系统在标准基准测试上的表现不能完全反映它们在真实世界复杂环境中的能力。人机交互领域需要更加关注识别系统的局限性和偏见，以确保任务执行的有效性、用户信任和安全性。&lt;h4&gt;翻译&lt;/h4&gt;现实环境中的自动语音识别系统需要处理不完美的音频，这些音频通常因硬件限制或环境噪声而降级，同时还需要适应不同的用户群体。在人机交互中，这些挑战相互交织，形成了独特的识别环境。我们在八个公开可用的数据集上评估了四种最先进的自动语音识别系统，这些数据集捕捉了六个难度维度：领域特定、口音、噪声、年龄变化、障碍和自发言语。我们的分析表明，尽管在标准基准测试上得分相似，但这些系统在性能、幻觉倾向和固有偏见方面存在显著差异。这些局限性对人机交互有严重影响，因为识别错误可能干扰任务执行、用户信任和安全。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic Speech Recognition (ASR) systems in real-world settings need tohandle imperfect audio, often degraded by hardware limitations or environmentalnoise, while accommodating diverse user groups. In human-robot interaction(HRI), these challenges intersect to create a uniquely challenging recognitionenvironment. We evaluate four state-of-the-art ASR systems on eight publiclyavailable datasets that capture six dimensions of difficulty: domain-specific,accented, noisy, age-variant, impaired, and spontaneous speech. Our analysisdemonstrates significant variations in performance, hallucination tendencies,and inherent biases, despite similar scores on standard benchmarks. Theselimitations have serious implications for HRI, where recognition errors caninterfere with task performance, user trust, and safety.</description>
      <author>example@mail.com (Theresa Pekarek Rosin, Julia Gachot, Henri-Leon Kordt, Matthias Kerzel, Stefan Wermter)</author>
      <guid isPermaLink="false">2508.17753v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models</title>
      <link>http://arxiv.org/abs/2508.17742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 7 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EEG基础模型通过从大规模未标记数据集中学习鲁棒表示，有望显著推进脑信号分析。然而，其快速发展已超出了标准化评估基准的发展，导致模型比较困难。为此，作者引入了EEG-FM-Bench，这是首个用于系统化和标准化评估EEG基础模型的全面基准。&lt;h4&gt;背景&lt;/h4&gt;EEG基础模型通过从大规模未标记数据集中学习鲁棒表示，有望显著推进脑信号分析。然而，它们的快速发展已超出了标准化评估基准的发展速度，这使得直接模型比较变得复杂，并阻碍了系统性的科学进步。&lt;h4&gt;目的&lt;/h4&gt;为了解决EEG基础模型评估缺乏标准化基准的问题，作者引入了EEG-FM-Bench，这是第一个用于系统化和标准化评估EEG基础模型（EEG-FMs）的全面基准。&lt;h4&gt;方法&lt;/h4&gt;作者的工作有三个主要贡献：(1)从经典EEG范式中整理了多样化的下游任务和数据集，在统一的开源框架内实现了标准化处理和评估协议；(2)对突出的最先进基础模型进行基准测试，以建立全面的基线结果；(3)对学习的表示进行定性分析，以提供对模型行为的洞察。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，作者发现细粒度的时空特征交互、多任务统一训练和神经心理学先验将有助于提高模型性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过提供一个公平比较和可复制研究的统一平台，EEG-FM-Bench旨在促进EEG基础模型的进展，并引导社区开发更鲁棒和可泛化的EEG-FMs。代码已在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)基础模型有望通过从大规模未标记数据集中学习鲁棒表示来显著推进脑信号分析。然而，它们的快速增长已超出了标准化评估基准的发展，这使得直接模型比较变得复杂，并阻碍了系统性的科学进展。这种碎片化现象导致科学效率低下，并掩盖了真正的架构进步。为了解决这一关键差距，我们引入了EEG-FM-Bench，这是第一个用于系统化和标准化评估EEG基础模型(EEG-FMs)的全面基准。我们的贡献有三方面：(1)我们从经典EEG范式中整理了多样化的下游任务和数据集，在统一的开源框架内实现了标准化处理和评估协议；(2)我们对突出的最先进基础模型进行基准测试，以建立全面的基线结果，便于清晰比较当前现状；(3)我们对学习的表示进行定性分析，以提供对模型行为的洞察，并为未来的架构设计提供信息。通过大量实验，我们发现细粒度的时空特征交互、多任务统一训练和神经心理学先验将有助于提高模型性能和泛化能力。通过提供一个公平比较和可复制研究的统一平台，EEG-FM-Bench旨在促进进展，并引导社区开发更鲁棒和可泛化的EEG-FMs。代码已在https://github.com/xw1216/EEG-FM-Bench发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) foundation models are poised to significantlyadvance brain signal analysis by learning robust representations fromlarge-scale, unlabeled datasets. However, their rapid proliferation hasoutpaced the development of standardized evaluation benchmarks, whichcomplicates direct model comparisons and hinders systematic scientificprogress. This fragmentation fosters scientific inefficiency and obscuresgenuine architectural advancements. To address this critical gap, we introduceEEG-FM-Bench, the first comprehensive benchmark for the systematic andstandardized evaluation of EEG foundation models (EEG-FMs). Our contributionsare threefold: (1) we curate a diverse suite of downstream tasks and datasetsfrom canonical EEG paradigms, implementing standardized processing andevaluation protocols within a unified open-source framework; (2) we benchmarkprominent state-of-the-art foundation models to establish comprehensivebaseline results for a clear comparison of the current landscape; (3) weperform qualitative analyses of the learned representations to provide insightsinto model behavior and inform future architectural design. Through extensiveexperiments, we find that fine-grained spatio-temporal feature interaction,multitask unified training and neuropsychological priors would contribute toenhancing model performance and generalization capabilities. By offering aunified platform for fair comparison and reproducible research, EEG-FM-Benchseeks to catalyze progress and guide the community toward the development ofmore robust and generalizable EEG-FMs. Code is released athttps://github.com/xw1216/EEG-FM-Bench.</description>
      <author>example@mail.com (Wei Xiong, Jiangtong Li, Jie Li, Kun Zhu)</author>
      <guid isPermaLink="false">2508.17742v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework</title>
      <link>http://arxiv.org/abs/2508.17726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的人类行为异常检测(HAAD)框架，适用于少样本场景，通过对比学习构建类别无关的表示空间，并利用基于扩散的基础模型生成增强的运动样本，以提高跨类别泛化能力和类别内鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有HAAD方法遵循'每类别一个模型'的范式，需要为每个行为类别单独训练并需要大量正常样本，这些限制阻碍了可扩展性，限制了在真实场景中的适用性，因为真实场景中数据通常稀缺或新类别频繁出现。&lt;h4&gt;目的&lt;/h4&gt;解决现有HAAD方法的局限性，提出一个统一框架，使其适用于少样本场景，提高训练效率和模型可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出统一的HAAD框架，通过对比学习构建类别无关的表示空间，通过将测试样本与给定的正常小样本集(支持集)比较进行异常检测；引入基于扩散基础模型的生成式运动增强策略，创建多样化和真实的训练样本，提高跨类别泛化能力和类别内鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在HumanAct12数据集上的实验表明，该方法在已见和未见类别设置下都达到了最先进的效果，并且在少样本HAAD的训练效率和模型可扩展性方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的统一框架有效解决了现有HAAD方法的局限性，特别是在少样本场景下，通过对比学习和生成式运动增强策略实现了高效的行为异常检测。&lt;h4&gt;翻译&lt;/h4&gt;人类行为异常检测(HAAD)旨在仅在训练期间使用正常行为数据来识别异常行为。现有方法通常遵循'每类别一个模型'的范式，需要为每个行为类别单独训练并需要大量正常样本。这些限制阻碍了可扩展性，限制了在真实场景中的适用性，因为在真实场景中数据通常稀缺或新类别频繁出现。为解决这些局限性，我们提出了一种适用于少样本场景的HAAD统一框架。我们的方法通过对比学习构建类别无关的表示空间，通过将测试样本与给定的正常小样本集(称为支持集)进行比较来进行异常检测。为了提高跨类别泛化能力和类别内鲁棒性，我们引入了一种基于扩散基础模型生成运动样本的生成式运动增强策略，以创建多样化和真实的训练样本。值得注意的是，据我们所知，我们的工作是第一个引入这种专门针对增强行为异常检测对比学习策略的研究。在HumanAct12数据集上的大量实验表明，我们的方法在已见和未见类别设置下都达到了最先进的效果，并且在少样本HAAD的训练效率和模型可扩展性方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Action Anomaly Detection (HAAD) aims to identify anomalous actionsgiven only normal action data during training. Existing methods typicallyfollow a one-model-per-category paradigm, requiring separate training for eachaction category and a large number of normal samples. These constraints hinderscalability and limit applicability in real-world scenarios, where data isoften scarce or novel categories frequently appear. To address theselimitations, we propose a unified framework for HAAD that is compatible withfew-shot scenarios. Our method constructs a category-agnostic representationspace via contrastive learning, enabling AD by comparing test samples with agiven small set of normal examples (referred to as the support set). To improveinter-category generalization and intra-category robustness, we introduce agenerative motion augmentation strategy harnessing a diffusion-based foundationmodel for creating diverse and realistic training samples. Notably, to the bestof our knowledge, our work is the first to introduce such a strategyspecifically tailored to enhance contrastive learning for action AD. Extensiveexperiments on the HumanAct12 dataset demonstrate the state-of-the-arteffectiveness of our approach under both seen and unseen category settings,regarding training efficiency and model scalability for few-shot HAAD.</description>
      <author>example@mail.com (Koichiro Kamide, Shunsuke Sakai, Shun Maeda, Chunzhi Gu, Chao Zhang)</author>
      <guid isPermaLink="false">2508.17726v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Robustness Feature Adapter for Efficient Adversarial Training</title>
      <link>http://arxiv.org/abs/2508.17680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted for presentation at ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于适配器的新方法，用于特征空间中的高效对抗训练，同时解决了计算开销大和鲁棒过拟合问题。&lt;h4&gt;背景&lt;/h4&gt;对抗训练(AT)结合投影梯度 descent(PGD)是提高模型对抗攻击鲁棒性的最流行方法，但应用于大型骨干模型时计算开销过大，且存在鲁棒过拟合问题。&lt;h4&gt;目的&lt;/h4&gt;同时解决计算开销大和鲁棒过拟合两个问题，构建更可信的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的基于适配器的方法，直接在特征空间进行高效对抗训练，消除鲁棒过以提高内循环收敛质量。&lt;h4&gt;主要发现&lt;/h4&gt;基于适配器的方法能消除鲁棒过拟合，显著提高计算效率，并将对抗鲁棒性推广到未见过的攻击，同时改善模型准确度。&lt;h4&gt;结论&lt;/h4&gt;在多种骨干架构和大规模对抗训练中，新的基于适配器的方法被证明是有效的。&lt;h4&gt;翻译&lt;/h4&gt;对抗训练(AT)结合投影梯度下降是提高模型在对抗攻击下鲁棒性的最流行方法。然而，当AT应用于大型骨干模型时，计算开销变得过大。AT还存在鲁棒过拟合问题。本文为同时解决这两个问题以构建更可信的基础模型做出了贡献。特别是，我们提出了一种新的基于适配器的方法，用于在特征空间中直接进行高效AT。我们表明，所提出的基于适配器的方法可以通过消除鲁棒过拟合来提高内循环收敛质量。因此，它显著提高了计算效率，并通过将对抗鲁棒性推广到未见攻击来改善模型准确度。我们在不同的骨干架构和大规模AT中证明了这种新基于适配器方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adversarial training (AT) with projected gradient descent is the most popularmethod to improve model robustness under adversarial attacks. However,computational overheads become prohibitively large when AT is applied to largebackbone models. AT is also known to have the issue of robust overfitting. Thispaper contributes to solving both problems simultaneously towards building moretrustworthy foundation models. In particular, we propose a new adapter-basedapproach for efficient AT directly in the feature space. We show that theproposed adapter-based approach can improve the inner-loop convergence qualityby eliminating robust overfitting. As a result, it significantly increasescomputational efficiency and improves model accuracy by generalizingadversarial robustness to unseen attacks. We demonstrate the effectiveness ofthe new adapter-based approach in different backbone architectures and in AT atscale.</description>
      <author>example@mail.com (Quanwei Wu, Jun Guo, Wei Wang, Yi Wang)</author>
      <guid isPermaLink="false">2508.17680v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Longitudinal Progression Prediction of Alzheimer's Disease with Tabular Foundation Model</title>
      <link>http://arxiv.org/abs/2508.17649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为L2C-TabPFN的新方法，通过结合纵向到横向转换和预训练表格基础模型，用于预测阿尔茨海默病结果，特别是诊断、认知分数和脑室体积。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病是一种进行性神经退行性疾病，由于其多因素病因和临床数据的多模态复杂性，预测仍然具有挑战性。准确预测临床相关生物标志物对于有效监测疾病进展至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测阿尔茨海默病临床相关生物标志物的方法，包括诊断结果、认知测量和影像学生物标志物。&lt;h4&gt;方法&lt;/h4&gt;研究引入了L2C-TabPFN方法，该方法整合了纵向到横向转换与预训练的表格基础模型，将患者连续记录转换为固定长度的特征向量，使用TADPOLE数据集进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，L2C-TabPFN在诊断和认知结果方面取得了具有竞争力的性能，而在脑室体积预测方面提供了最先进的结果。脑室体积是反映阿尔茨海默病神经退行和进展的关键影像生物标志物。&lt;h4&gt;结论&lt;/h4&gt;这些发现突显了表格基础模型在推进阿尔茨海默病临床相关影像标记物纵向预测方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病是一种进行性神经退行性疾病，由于其多因素病因和临床数据的多模态复杂性，预测仍然具有挑战性。准确预测临床相关生物标志物，包括诊断和定量测量，对于有效监测疾病进展至关重要。这项工作介绍了L2C-TabPFN，这是一种结合了纵向到横向转换与预训练表格基础模型的方法，使用TADPOLE数据集预测阿尔茨海默病结果。L2C-TabPFN将患者连续记录转换为固定长度的特征向量，从而能够稳健预测诊断、认知分数和脑室体积。实验结果表明，虽然L2C-TabPFN在诊断和认知结果方面取得了具有竞争力的性能，但在脑室体积预测方面提供了最先进的结果。这一关键的影像生物标志物反映了阿尔茨海默病的神经退行和进展。这些发现突显了表格基础模型在推进阿尔茨海默病临床相关影像标记物纵向预测方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alzheimer's disease is a progressive neurodegenerative disorder that remainschallenging to predict due to its multifactorial etiology and the complexity ofmultimodal clinical data. Accurate forecasting of clinically relevantbiomarkers, including diagnostic and quantitative measures, is essential foreffective monitoring of disease progression. This work introduces L2C-TabPFN, amethod that integrates a longitudinal-to-cross-sectional (L2C) transformationwith a pre-trained Tabular Foundation Model (TabPFN) to predict Alzheimer'sdisease outcomes using the TADPOLE dataset. L2C-TabPFN converts sequentialpatient records into fixed-length feature vectors, enabling robust predictionof diagnosis, cognitive scores, and ventricular volume. Experimental resultsdemonstrate that, while L2C-TabPFN achieves competitive performance ondiagnostic and cognitive outcomes, it provides state-of-the-art results inventricular volume prediction. This key imaging biomarker reflectsneurodegeneration and progression in Alzheimer's disease. These findingshighlight the potential of tabular foundational models for advancinglongitudinal prediction of clinically relevant imaging markers in Alzheimer'sdisease.</description>
      <author>example@mail.com (Yilang Ding, Jiawen Ren, Jiaying Lu, Gloria Hyunjung Kwak, Armin Iraji, Alex Fedorov)</author>
      <guid isPermaLink="false">2508.17649v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>In-Context Algorithm Emulation in Fixed-Weight Transformers</title>
      <link>http://arxiv.org/abs/2508.17550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code is available at https://github.com/MAGICS-LAB/algo_emu&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文证明具有固定权重的最小Transformer架构可以通过上下文提示模拟广泛算法，包括单层注意力头也能实现，核心是通过构建提示将算法参数编码到token表示中，创建点积间隙引导计算。&lt;h4&gt;背景&lt;/h4&gt;研究背景是上下文学习与算法模拟之间的关系，探索Transformer模型如何仅通过提示而不更新参数来实现不同算法。&lt;h4&gt;目的&lt;/h4&gt;目的是证明最小Transformer架构（特别是具有固定权重的）能够通过上下文提示模拟广泛算法，并建立上下文学习与算法模拟之间的直接联系。&lt;h4&gt;方法&lt;/h4&gt;方法是构建提示，将算法的参数编码到token表示中，创建尖锐的点积间隙，强制softmax注意力遵循预期的计算路径，无需前馈层和参数更新。&lt;h4&gt;主要发现&lt;/h4&gt;主要发现是任何可通过固定权重注意力头实现的算法（如一步梯度下降、线性/岭回归）都可以通过适当的提示让两层或单层Transformer以任意精度重现。&lt;h4&gt;结论&lt;/h4&gt;结论是这些发现为大型Transformer提供了一个简单的机制，使其作为可提示编程的算法库，阐明了GPT式基础模型如何仅通过提示交换算法，建立了现代Transformer模型中的算法普适性。&lt;h4&gt;翻译&lt;/h4&gt;我们证明，具有固定权重的最小Transformer架构能够通过上下文提示模拟广泛类别的算法。特别是，对于任何可通过固定权重注意力头实现的算法（例如一步梯度下降或线性/岭回归），都存在一个提示，该提示可以驱动两层softmax注意力模块以任意精度重现算法的输出。这一保证甚至可以扩展到单层注意力头（如果需要可以使用更长的提示），从而实现架构上的最小化。我们的核心思想是构建提示，将算法的参数编码到token表示中，创建尖锐的点积间隙，强制softmax注意力遵循预期的计算。这种构建不需要前馈层和参数更新，所有适应都仅通过提示发生。这些发现建立了上下文学习与算法模拟之间的直接联系，并为大型Transformer提供了一个简单的机制，使其作为可提示编程的算法库。它们阐明了GPT式基础模型如何仅通过提示交换算法，在现代Transformer模型中建立了一种算法普适性形式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We prove that a minimal Transformer architecture with frozen weights iscapable of emulating a broad class of algorithms by in-context prompting. Inparticular, for any algorithm implementable by a fixed-weight attention head(e.g. one-step gradient descent or linear/ridge regression), there exists aprompt that drives a two-layer softmax attention module to reproduce thealgorithm's output with arbitrary precision. This guarantee extends even to asingle-head attention layer (using longer prompts if necessary), achievingarchitectural minimality. Our key idea is to construct prompts that encode analgorithm's parameters into token representations, creating sharp dot-productgaps that force the softmax attention to follow the intended computation. Thisconstruction requires no feed-forward layers and no parameter updates. Alladaptation happens through the prompt alone. These findings forge a direct linkbetween in-context learning and algorithmic emulation, and offer a simplemechanism for large Transformers to serve as prompt-programmable libraries ofalgorithms. They illuminate how GPT-style foundation models may swap algorithmsvia prompts alone, establishing a form of algorithmic universality in modernTransformer models.</description>
      <author>example@mail.com (Jerry Yao-Chieh Hu, Hude Liu, Jennifer Yuntong Zhang, Han Liu)</author>
      <guid isPermaLink="false">2508.17550v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations</title>
      <link>http://arxiv.org/abs/2508.17547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CoRL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LodeStar学习框架和系统，能够自动将任务演示分解为语义上有意义的技能，并通过强化学习生成多样化的合成演示数据集，使用技能路由Transformer策略将学习到的技能链接起来，以执行复杂的长视点操作任务，显著提高了任务性能和稳健性。&lt;h4&gt;背景&lt;/h4&gt;开发能够稳健执行长视点操作任务且具有人类水平灵巧性的机器人系统具有挑战性，因为这些任务需要物理灵巧性、操作技能的无缝序列化，同时稳健处理环境变化。虽然模仿学习是一种有前景的方法，但获取全面的数据集资源密集。&lt;h4&gt;目的&lt;/h4&gt;提出一个学习框架和系统LodeStar，解决长视点操作任务的挑战，提高任务执行的性能和稳健性。&lt;h4&gt;方法&lt;/h4&gt;使用现成的基础模型自动将任务演示分解为语义上有意义的技能；通过强化学习从少数人类演示中生成多样化的合成演示数据集；使用模拟增强的数据集进行稳健的技能训练；采用技能路由Transformer (SRT)策略将学习到的技能有效地链接起来，以执行复杂的长视点操作任务。&lt;h4&gt;主要发现&lt;/h4&gt;在三个具有挑战性的真实世界长视点灵巧操作任务上的实验评估表明，与之前的基线相比，该方法显著提高了任务性能和稳健性。&lt;h4&gt;结论&lt;/h4&gt;LodeStar系统能够有效解决长视点操作任务的挑战，通过分解技能和生成合成数据来提高性能和稳健性。&lt;h4&gt;翻译&lt;/h4&gt;开发具有人类水平灵巧性且能稳健执行长视点操作任务的机器人系统具有挑战性，因为此类任务既需要物理灵巧性，又需要操作技能的无缝序列化，同时还要稳健处理环境变化。虽然模仿学习提供了一种有前景的方法，但获取全面的数据集资源密集。在这项工作中，我们提出了一个名为LodeStar的学习框架和系统，它使用现成的基础模型自动将任务演示分解为语义上有意义的技能，并通过强化学习从少数人类演示中生成多样化的合成演示数据集。这些模拟增强的数据集能够进行稳健的技能训练，而技能路由Transformer (SRT)策略有效地将学习到的技能链接在一起，以执行复杂的长视点操作任务。在三个具有挑战性的真实世界长视点灵巧操作任务上的实验评估表明，与之前的基线相比，我们的方法显著提高了任务性能和稳健性。视频可在lodestar-robot.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing robotic systems capable of robustly executing long-horizonmanipulation tasks with human-level dexterity is challenging, as such tasksrequire both physical dexterity and seamless sequencing of manipulation skillswhile robustly handling environment variations. While imitation learning offersa promising approach, acquiring comprehensive datasets is resource-intensive.In this work, we propose a learning framework and system LodeStar thatautomatically decomposes task demonstrations into semantically meaningfulskills using off-the-shelf foundation models, and generates diverse syntheticdemonstration datasets from a few human demos through reinforcement learning.These sim-augmented datasets enable robust skill training, with a Skill RoutingTransformer (SRT) policy effectively chaining the learned skills together toexecute complex long-horizon manipulation tasks. Experimental evaluations onthree challenging real-world long-horizon dexterous manipulation tasksdemonstrate that our approach significantly improves task performance androbustness compared to previous baselines. Videos are available atlodestar-robot.github.io.</description>
      <author>example@mail.com (Weikang Wan, Jiawei Fu, Xiaodi Yuan, Yifeng Zhu, Hao Su)</author>
      <guid isPermaLink="false">2508.17547v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation</title>
      <link>http://arxiv.org/abs/2508.17524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OmniMRI是一个统一的视觉语言基础模型，旨在跨越整个MRI工作流程，整合碎片化的多阶段临床流程，包括图像获取、重建、分割、检测、诊断和报告生成。&lt;h4&gt;背景&lt;/h4&gt;MRI在临床实践中不可或缺，但当前工作流程是多阶段的、碎片化的。现有深度学习方法通常局限于特定解剖结构或应用，缺乏通用性，且很少将成像数据与放射科医生依赖的语言信息相结合。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨越整个MRI工作流程的统一基础模型，整合成像数据和语言信息，实现全面的端到端MRI解释。&lt;h4&gt;方法&lt;/h4&gt;OmniMRI基于大规模异构语料库进行训练，该语料库来自60个公共数据集，包括超过220,000个体积和1900万张MRI切片。采用多阶段训练范式：自监督视觉预训练、视觉-语言对齐、多模态预训练和多任务指令调优，使模型获得可迁移的视觉表示、跨模态推理和指令遵循能力。&lt;h4&gt;主要发现&lt;/h4&gt;OmniMRI能够在单一架构中执行多种任务，包括MRI重建、解剖和病理分割、异常检测、诊断建议和放射学报告生成，展示了其作为通用框架的潜力。&lt;h4&gt;结论&lt;/h4&gt;OmniMRI有潜力将碎片化的MRI流程整合为可扩展的通用框架，为统一成像和临床语言的基础模型铺平道路，实现全面、端到端的MRI解释。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像（MRI）在临床实践中不可或缺，但仍受限于碎片化的多阶段工作流程，包括获取、重建、分割、检测、诊断和报告。虽然深度学习在单个任务上取得了进展，但现有方法通常局限于特定解剖结构或应用，缺乏在不同临床环境中的通用性。此外，当前管道很少将成像数据与放射科医生在常规实践中依赖的语言信息相结合。在此，我们介绍OmniMRI，这是一个统一的视觉语言基础模型，旨在跨越整个MRI工作流程。OmniMRI基于从60个公共数据集中整理的大规模异构语料库进行训练，包括超过220,000个体积和1900万张MRI切片，整合了纯图像数据、配对的视觉-文本数据和指令-响应数据。其多阶段训练范式包括自监督视觉预训练、视觉-语言对齐、多模态预训练和多任务指令调优，逐步使模型具备可迁移的视觉表示、跨模态推理和强大的指令遵循能力。定性结果表明，OmniMRI能够在单一架构中执行多种任务，包括MRI重建、解剖和病理分割、异常检测、诊断建议和放射学报告生成。这些发现突显了OmniMRI将碎片化流程整合为可扩展的通用框架的潜力，为统一成像和临床语言的基础模型铺平了道路，实现全面、端到端的MRI解释。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决MRI工作流程的碎片化问题。当前的MRI流程包含多个独立阶段（采集、重建、分割、检测、诊断和报告），现有方法通常针对特定任务或解剖结构设计，缺乏泛化能力，且很少整合临床语言信息。这个问题很重要，因为它导致临床工作流程效率低下、成本高昂、诊断结果不一致，尤其在影像量增加和放射科医生短缺的情况下更为突出。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过三个核心原则设计OmniMRI：1)大规模多样化数据构建，整合60个公共数据集确保临床变异性；2)统一视觉-语言架构，处理多种输入类型；3)统一训练范式，结合多阶段学习过程。作者借鉴了基础模型范式、Transformer架构、CLIP/BLIP等视觉-语言模型以及BrainSegFounder等医学影像基础模型的工作，但将其专门适应于MRI的全工作流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的视觉-语言基础模型，能够跨越整个MRI工作流程，从低级任务（重建、分割）到高级功能（异常检测、诊断建议、报告生成）。实现流程包括：1)数据构建（图像-only数据、配对视觉-文本数据、指令-响应数据）；2)统一架构（视觉和文本编码器、共享Transformer主干、双解码器）；3)四阶段训练（视觉预训练、视觉-语言对齐、多模态预训练、多任务指令调优）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)大规模多样化MRI数据集构建；2)统一视觉-语言架构支持多模态输入；3)分层描述模板系统化组织文本信息；4)全栈MRI任务能力在单一框架内实现。相比之前工作，OmniMRI的不同在于：统一性（整合整个工作流程而非单一任务）、多模态整合（结合视觉和语言）、指令跟随能力（灵活适应不同任务）、零样本迁移能力（无需额外训练即可跨应用场景）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniMRI通过统一的视觉-语言基础模型，将MRI工作流程中的多个独立任务整合到一个可扩展的通用框架中，实现了从图像重建到放射学报告生成的全栈MRI解释能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic Resonance Imaging (MRI) is indispensable in clinical practice butremains constrained by fragmented, multi-stage workflows encompassingacquisition, reconstruction, segmentation, detection, diagnosis, and reporting.While deep learning has achieved progress in individual tasks, existingapproaches are often anatomy- or application-specific and lack generalizabilityacross diverse clinical settings. Moreover, current pipelines rarely integrateimaging data with complementary language information that radiologists rely onin routine practice. Here, we introduce OmniMRI, a unified vision-languagefoundation model designed to generalize across the entire MRI workflow. OmniMRIis trained on a large-scale, heterogeneous corpus curated from 60 publicdatasets, over 220,000 MRI volumes and 19 million MRI slices, incorporatingimage-only data, paired vision-text data, and instruction-response data. Itsmulti-stage training paradigm, comprising self-supervised vision pretraining,vision-language alignment, multimodal pretraining, and multi-task instructiontuning, progressively equips the model with transferable visualrepresentations, cross-modal reasoning, and robust instruction-followingcapabilities. Qualitative results demonstrate OmniMRI's ability to performdiverse tasks within a single architecture, including MRI reconstruction,anatomical and pathological segmentation, abnormality detection, diagnosticsuggestion, and radiology report generation. These findings highlight OmniMRI'spotential to consolidate fragmented pipelines into a scalable, generalistframework, paving the way toward foundation models that unify imaging andclinical language for comprehensive, end-to-end MRI interpretation.</description>
      <author>example@mail.com (Xingxin He, Aurora Rofena, Ruimin Feng, Haozhe Liao, Zhaoye Zhou, Albert Jang, Fang Liu)</author>
      <guid isPermaLink="false">2508.17524v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>A Synthetic Dataset for Manometry Recognition in Robotic Applications</title>
      <link>http://arxiv.org/abs/2508.17468v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合数据合成管道，结合程序渲染和AI驱动的视频生成，用于解决复杂工业环境中训练稳健目标检测模型的数据稀缺和高成本问题。&lt;h4&gt;背景&lt;/h4&gt;在复杂工业环境（如海上石油平台）中收集真实数据存在实际和经济障碍，这阻碍了自主检测系统的发展。&lt;h4&gt;目的&lt;/h4&gt;克服数据稀缺和高成本挑战，开发一种高效、经济且安全的数据生成方法。&lt;h4&gt;方法&lt;/h4&gt;结合BlenderProc创建具有精确注释和受控域随机化的逼真图像，并集成NVIDIA的Cosmos-Predict2世界基础模型合成具有时间多样性的物理合理视频序列。&lt;h4&gt;主要发现&lt;/h4&gt;在混合真实和合成数据集上训练的YOLO检测网络性能优于仅使用真实数据的模型，1:1混合比例获得最佳结果。&lt;h4&gt;结论&lt;/h4&gt;合成优先方法可作为安全关键和资源受限工业应用中开发可靠感知系统的高效、经济且安全的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了在复杂工业环境（如海上石油平台）中训练稳健目标检测模型所面临的数据稀缺和高成本挑战。这些危险环境中收集真实数据的实际和经济障碍常常阻碍了自主检测系统的发展。为克服这一问题，我们提出并验证了一种结合程序渲染与AI驱动视频生成的混合数据合成管道。我们的方法利用BlenderProc创建具有精确注释和受控域随机化的逼真图像，并集成NVIDIA的Cosmos-Predict2世界基础模型来合成具有时间多样性的物理合理视频序列，捕捉罕见视角和不利条件。我们证明，在混合真实图像与合成数据的复合数据集上训练的基于YOLO的检测网络，相比仅使用真实数据训练的模型实现了更优性能。值得注意的是，1:1混合真实和合成数据比例获得了最高准确率，超过了仅使用真实数据的基线。这些发现突显了合成优先方法在安全关键和资源受限的工业应用中开发可靠感知系统的可行性，这是一种高效、经济且安全的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work addresses the challenges of data scarcity and high acquisitioncosts for training robust object detection models in complex industrialenvironments, such as offshore oil platforms. The practical and economicbarriers to collecting real-world data in these hazardous settings often hamperthe development of autonomous inspection systems. To overcome this, in thiswork we propose and validate a hybrid data synthesis pipeline that combinesprocedural rendering with AI-driven video generation. Our methodology leveragesBlenderProc to create photorealistic images with precise annotations andcontrolled domain randomization, and integrates NVIDIA's Cosmos-Predict2world-foundation model to synthesize physically plausible video sequences withtemporal diversity, capturing rare viewpoints and adverse conditions. Wedemonstrate that a YOLO-based detection network trained on a composite dataset,blending real images with our synthetic data, achieves superior performancecompared to models trained exclusively on real-world data. Notably, a 1:1mixture of real and synthetic data yielded the highest accuracy, surpassing thereal-only baseline. These findings highlight the viability of a synthetic-firstapproach as an efficient, cost-effective, and safe alternative for developingreliable perception systems in safety-critical and resource-constrainedindustrial applications.</description>
      <author>example@mail.com (Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Joao Manoel Herrera Pinheiro, Thiago H. Segreto, Ricardo V. Godoy, Marcelo Becker)</author>
      <guid isPermaLink="false">2508.17468v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets</title>
      <link>http://arxiv.org/abs/2508.17391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了大型语言模型(LLMs)在小规模结构化数据集上的分类、回归和聚类任务性能，发现LLMs在分类任务上表现优异，但在回归和聚类任务上存在局限性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)最初为自然语言处理开发，具有跨模态和跨领域泛化能力，以及无需显式微调即可对结构化输入执行预测任务的上下文学习(ICL)能力。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs在小规模结构化数据集上的经验函数逼近能力，评估其在分类、回归和聚类任务上的性能表现。&lt;h4&gt;方法&lt;/h4&gt;评估GPT-5、GPT-4o、GPT-o3、Gemini-2.5-Flash和DeepSeek-R1等最先进LLMs在少样本提示下的性能，并与线性模型、集成方法和表格基础模型等传统机器学习基线进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;在数据有限情况下，LLMs在分类任务上表现优异，建立了实用零训练基线；但在回归任务中性能较差，可能因需在大空间输出；聚类结果同样有限，归因于缺乏真正的ICL；上下文大小和提示结构影响逼近质量。&lt;h4&gt;结论&lt;/h4&gt;LLMs可作为结构化数据的通用预测引擎，在分类方面有明显优势，在回归和聚类方面有显著局限性；该方法可实现快速数据探索，是传统ML管道在商业智能和探索性分析中的可行替代方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)最初是为自然语言处理(NLP)开发的，已展现出跨模态和跨领域泛化的潜力。凭借其上下文学习(ICL)能力，LLMs可以在不对下游任务进行显式微调的情况下对结构化输入执行预测任务。在本工作中，我们研究了LLMs在小规模结构化数据集上用于分类、回归和聚类任务的经验函数逼近能力。我们评估了最先进LLMs(GPT-5、GPT-4o、GPT-o3、Gemini-2.5-Flash、DeepSeek-R1)在少样本提示下的性能，并与已建立的机器学习(ML)基线(包括线性模型、集成方法和表格基础模型(TFMs))进行了比较。我们的结果表明，在数据可用性有限的情况下，LLMs在分类任务上取得了强大的性能，建立了实用的零训练基线。相比之下，在具有连续值输出的回归任务中，性能较差，可能是因为回归需要在大(通常是无限的)空间中输出，聚类结果同样有限，我们将其归因于在此设置中缺乏真正的ICL。尽管如此，这种方法可以实现快速、低开销的数据探索，并在商业智能和探索性分析背景下提供传统ML管道的可行替代方案。我们进一步分析了上下文大小和提示结构对逼近质量的影响，确定了影响预测性能的权衡。我们的研究结果表明，LLMs可以作为结构化数据的通用预测引擎，在分类方面具有明显优势，在回归和聚类方面存在显著局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs), originally developed for natural languageprocessing (NLP), have demonstrated the potential to generalize acrossmodalities and domains. With their in-context learning (ICL) capabilities, LLMscan perform predictive tasks over structured inputs without explicitfine-tuning on downstream tasks. In this work, we investigate the empiricalfunction approximation capability of LLMs on small-scale structured datasetsfor classification, regression and clustering tasks. We evaluate theperformance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash,DeepSeek-R1) under few-shot prompting and compare them against establishedmachine learning (ML) baselines, including linear models, ensemble methods andtabular foundation models (TFMs). Our results show that LLMs achieve strongperformance in classification tasks under limited data availability,establishing practical zero-training baselines. In contrast, the performance inregression with continuous-valued outputs is poor compared to ML models, likelybecause regression demands outputs in a large (often infinite) space, andclustering results are similarly limited, which we attribute to the absence ofgenuine ICL in this setting. Nonetheless, this approach enables rapid,low-overhead data exploration and offers a viable alternative to traditional MLpipelines in business intelligence and exploratory analytics contexts. Wefurther analyze the influence of context size and prompt structure onapproximation quality, identifying trade-offs that affect predictiveperformance. Our findings suggest that LLMs can serve as general-purposepredictive engines for structured data, with clear strengths in classificationand significant limitations in regression and clustering.</description>
      <author>example@mail.com (Nikolaos Pavlidis, Vasilis Perifanis, Symeon Symeonidis, Pavlos S. Efraimidis)</author>
      <guid isPermaLink="false">2508.17391v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation</title>
      <link>http://arxiv.org/abs/2508.17324v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  LLMs, Native, Arabic LLMs, Augmentation, Multilingual, Language  Diversity, Contextual Understanding, Minority Languages, Culturally Informed,  Foundation Models, Large Language Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文报告了作者参加PalmX文化评估共享任务的成果。他们开发的CultranAI系统专注于数据增强和大型语言模型的LoRA微调，用于阿拉伯文化知识表示。&lt;h4&gt;背景&lt;/h4&gt;作者参与了PalmX文化评估共享任务，这是一个专注于阿拉伯文化知识表示的评估活动。&lt;h4&gt;目的&lt;/h4&gt;开发CultranAI系统，通过数据增强和大型语言模型的LoRA微调来提高阿拉伯文化知识表示的性能。&lt;h4&gt;方法&lt;/h4&gt;对比评估多个大型语言模型，确定最佳模型；整合Palm数据集增强PalmX数据集；创建包含超过22K个文化基础选择题的新数据集；使用增强数据集对Fanar-1-9B-Instruct模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;Fanar-1-9B-Instruct模型在实验中表现最佳，使用增强数据集微调后，在盲测集上达到70.50%的准确率，在PalmX开发集上达到84.1%的准确率，排名第五。&lt;h4&gt;结论&lt;/h4&gt;通过数据增强和LoRA微调技术，CultranAI系统在PalmX文化评估任务中取得了有竞争力的结果，证明了这些方法在阿拉伯文化知识表示中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们报告了我们参加PalmX文化评估共享任务的成果。我们的系统CultranAI专注于数据增强和大型语言模型(LoRA)的阿拉伯文化知识表示微调。我们对几个大型语言模型进行了基准测试，以确定表现最佳的模型。除了使用PalmX数据集外，我们还通过整合Palm数据集对其进行了增强，并整理了一个包含超过22K个文化基础选择题的新数据集。我们的实验显示，Fanar-1-9B-Instruct模型取得了最佳性能。我们在22K+的增强数据集组合上对该模型进行了微调。在盲测集上，我们提交的系统排名第五，准确率为70.50%，而在PalmX开发集上，它达到了84.1%的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we report our participation to the PalmX cultural evaluationshared task. Our system, CultranAI, focused on data augmentation and LoRAfine-tuning of large language models (LLMs) for Arabic cultural knowledgerepresentation. We benchmarked several LLMs to identify the best-performingmodel for the task. In addition to utilizing the PalmX dataset, we augmented itby incorporating the Palm dataset and curated a new dataset of over 22Kculturally grounded multiple-choice questions (MCQs). Our experiments showedthat the Fanar-1-9B-Instruct model achieved the highest performance. Wefine-tuned this model on the combined augmented dataset of 22K+ MCQs. On theblind test set, our submitted system ranked 5th with an accuracy of 70.50%,while on the PalmX development set, it achieved an accuracy of 84.1%.</description>
      <author>example@mail.com (Hunzalah Hassan Bhatti, Youssef Ahmed, Md Arid Hasan, Firoj Alam)</author>
      <guid isPermaLink="false">2508.17324v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Quickly Tuning Foundation Models for Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.17283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a short paper at the non-archival content track of AutoML  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为QTT-SEG的元学习方法，用于自动化和加速SAM在图像分割任务上的微调过程，显著提高了模型在特定领域分割任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;像SAM这样的基础模型在零样本图像分割任务中表现出强大性能，但在特定领域任务上表现不佳，且微调这些模型通常需要大量手动工作和领域专业知识。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化和加速SAM模型微调的方法，减少对人工干预和领域专业知识的依赖。&lt;h4&gt;方法&lt;/h4&gt;QTT-SEG建立在Quick-Tune超参数优化框架之上，使用元学习的成本和性能模型来预测高性能配置，能够高效导航超过2亿种可能性的搜索空间。&lt;h4&gt;主要发现&lt;/h4&gt;在八个二元和五个多类分割数据集上，QTT-SEG在严格时间限制下持续改进了SAM的零样本性能，在大多数二元任务上超过了AutoGluon Multimodal基线模型（三分钟内），在多类数据集上也带来持续提升。&lt;h4&gt;结论&lt;/h4&gt;元学习在自动化模型适应专门分割任务方面具有显著潜力，能够有效解决基础模型在特定领域应用中的局限性。&lt;h4&gt;翻译&lt;/h4&gt;像SAM（Segment Anything Model）这样的基础模型在零样本图像分割任务中表现出强大的性能，但在特定领域任务上常常表现不佳。微调这些模型通常需要大量的手动工作和领域专业知识。在这项工作中，我们介绍了QTT-SEG，这是一种基于元学习的方法，用于自动化和加速SAM在图像分割任务上的微调。基于Quick-Tune超参数优化框架，QTT-SEG使用元学习的成本和性能模型来预测高性能配置，高效地导航超过2亿种可能性的搜索空间。我们在八个二元和五个多类分割数据集上评估了QTT-SEG，在严格的时间限制下。我们的结果表明，QTT-SEG持续改进了SAM的零样本性能，并在大多数二元任务上超过了AutoGluon Multimodal（一个强大的AutoML基线模型），时间在三分钟内。在多类数据集上，QTT-SEG也带来了持续的提升。这些发现强调了元学习在自动化模型适应专门分割任务方面的潜力。代码可在以下网址获取：https://github.com/ds-brx/QTT-SEG/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models like SAM (Segment Anything Model) exhibit strong zero-shotimage segmentation performance, but often fall short on domain-specific tasks.Fine-tuning these models typically requires significant manual effort anddomain expertise. In this work, we introduce QTT-SEG, a meta-learning-drivenapproach for automating and accelerating the fine-tuning of SAM for imagesegmentation. Built on the Quick-Tune hyperparameter optimization framework,QTT-SEG predicts high-performing configurations using meta-learned cost andperformance models, efficiently navigating a search space of over 200 millionpossibilities. We evaluate QTT-SEG on eight binary and five multiclasssegmentation datasets under tight time constraints. Our results show thatQTT-SEG consistently improves upon SAM's zero-shot performance and surpassesAutoGluon Multimodal, a strong AutoML baseline, on most binary tasks withinthree minutes. On multiclass datasets, QTT-SEG delivers consistent gains aswell. These findings highlight the promise of meta-learning in automating modeladaptation for specialized segmentation tasks. Code available at:https://github.com/ds-brx/QTT-SEG/</description>
      <author>example@mail.com (Breenda Das, Lennart Purucker, Timur Carstensen, Frank Hutter)</author>
      <guid isPermaLink="false">2508.17283v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>GRASP: Geospatial pixel Reasoning viA Structured Policy learning</title>
      <link>http://arxiv.org/abs/2508.17102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRASP是一种结构化策略学习框架，通过强化学习优化多模态大语言模型和预训练分割模型，无需密集像素监督即可实现地理空间像素推理，在域内和域外数据上都取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;地理空间像素推理是一个新兴的遥感任务，旨在根据自然语言指令直接生成分割掩码。现有的基于多模态大语言模型的系统通过密集像素监督共同训练语言模型和掩码解码器，这种方法成本高昂，并且在域外数据上表现较弱。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架，解决现有方法的高成本和弱域外泛化能力问题，提高地理空间像素推理的性能。&lt;h4&gt;方法&lt;/h4&gt;1) 多模态大语言模型从视觉语言指令中发出任务相关的边界框和正点；2) 这些输出作为提示传递给预训练分割模型生成最终掩码；3) 使用强化学习(GRPO)而非监督微调进行优化，仅依赖边界框和点上的格式和准确性奖励；4) 创建GRASP-1k数据集，包含推理密集型查询、详细推理轨迹和细粒度分割注释。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在域内测试集上性能提高约4%；2) 在域外基准测试上性能提高高达54%；3) 实验结果证明了模型强大的泛化能力；4) 复杂的地理空间分割行为可以通过从弱空间线索中通过强化学习获得。&lt;h4&gt;结论&lt;/h4&gt;GRASP框架有效利用了基础模型的强先验，最小化了可训练参数，支持从廉价注释中学习，证明通过强化学习从弱空间线索学习复杂地理空间分割行为的可行性。&lt;h4&gt;翻译&lt;/h4&gt;地理空间像素推理是一项新兴的遥感任务，旨在直接从自然语言指令生成分割掩码。现有的基于多模态大语言模型的系统通过密集像素监督共同训练语言模型和掩码解码器，这种方法成本高昂，并且在域外数据上往往表现较弱。我们引入了GRASP，一种结构化策略学习框架。在我们的设计中，多模态大语言模型首先从视觉语言指令中发出与任务相关的边界框和正点。这些输出随后被传递给预训练的分割模型，作为提示生成最终掩码。我们使用强化学习而非监督微调来优化系统：模型仅使用GRPO进行训练，由边界框和点上计算的格式奖励和准确性奖励引导（无需掩码监督）。这利用了基础模型中的强先验，最小化了可训练参数，并支持从廉价注释中学习。我们还整理了GRASP-1k数据集，包含推理密集型查询、详细推理轨迹和细粒度分割注释。在域内和域外测试集上的评估显示了最先进的结果：域内性能提高约4%，在OOD基准测试上提高高达54%。实验结果证明了我们模型的强大泛化能力，并表明复杂的地理空间分割行为可以通过从弱空间线索中通过强化学习获得。代码和数据集将以开源形式发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geospatial pixel reasoning is a nascent remote-sensing task that aims togenerate segmentation masks directly from natural-language instructions.Prevailing MLLM-based systems co-train a language model and a mask decoder withdense pixel supervision, which is expensive and often weak on out-of-domain(OOD) data. We introduce GRASP, a structured policy-learning framework. In ourdesign, a multimodal large language model first emits task-relevant boundingboxes and positive points from a vision-language instruction. These outputs arethen passed to a pre-trained segmentation model, which consumes them as promptsto generate the final mask. Instead of supervised fine-tuning, we optimize thesystem purely with reinforcement learning: the model is trained solely withGRPO, guided by format rewards and accuracy rewards computed on boxes andpoints (no mask supervision). This leverages strong priors in foundationmodels, minimizes trainable parameters, and enables learning from inexpensiveannotations. We additionally curate GRASP-1k, which containsreasoning-intensive queries, detailed reasoning traces, and fine-grainedsegmentation annotations. Evaluations on both in-domain and out-of-domain testsets show state-of-the-art results: about 4% improvement in-domain and up to54% on OOD benchmarks. The experiment results evidence our model's robustgeneralization and demonstrate that complex geospatial segmentation behaviorscan be learned via RL from weak spatial cues. Code and the dataset will bereleased open-source.</description>
      <author>example@mail.com (Chengjie Jiang, Yunqi Zhou, Jiafeng Yan, Jing Li)</author>
      <guid isPermaLink="false">2508.17102v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments</title>
      <link>http://arxiv.org/abs/2508.17044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 3 figures, 13 tables. Preprint of the accepted article in  Optical Memory and Neural Network Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文解决了动态环境中的3D映射挑战，提出了一种多模态3D地图构建方法的分类法，并开发了名为M3DMap的模块化方法，用于构建静态和动态场景的多模态3D地图。&lt;h4&gt;背景&lt;/h4&gt;3D mapping在动态环境中对机器人学和自主运输领域的研究人员提出了挑战，目前没有能够整合图像、点云和文本等多模态数据的动态3D场景的通用表示方法。&lt;h4&gt;目的&lt;/h4&gt;解决动态3D场景表示问题，提出构建多模态3D地图方法的分类法，并设计一个用于构建静态和动态场景多模态3D地图的模块化方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于场景类型和表示、学习方法和实际应用的分类法；开发了M3DMap方法，包含神经多模态目标分割和跟踪模块、定位估计模块、3D地图构建和更新模块以及多模态数据检索模块。&lt;h4&gt;主要发现&lt;/h4&gt;多模态数据和现代基础模型在3D映射方法中具有积极影响。&lt;h4&gt;结论&lt;/h4&gt;提供了分类法的详细信息和方法的实现细节，可通过https://yuddim.github.io/M3DMap获取。&lt;h4&gt;翻译&lt;/h4&gt;该论文研究了动态环境中的3D映射问题，提出了一种多模态3D地图构建方法的分类法，并开发了名为M3DMap的模块化方法，用于构建静态和动态场景的多模态3D地图。该方法包含多个相互连接的组件：神经多模态目标分割和跟踪模块、定位估计模块（包括可训练算法）、3D地图构建和更新模块（根据所需的场景表示有不同的实现）以及多模态数据检索模块。论文还展示了这些模块的原创实现及其在解决各种实际任务（从3D目标定位到移动操作）中的优势。此外，论文提出了理论命题，证明了多模态数据和现代基础模型在3D映射方法中的积极影响。分类法和方法实现的详细信息可在https://yuddim.github.io/M3DMap获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在动态环境中构建多模态3D地图的问题，即如何整合图像、点云和文本等多种数据源来表示动态3D场景。这个问题在现实中非常重要，因为自主系统（如自动驾驶汽车、服务机器人）需要在动态环境中导航和操作，而缺乏统一的动态3D场景表示方法限制了这些系统的性能和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先提出了一种分类法系统化现有多模态3D地图研究，然后基于此设计了一个模块化的M3DMap方法。该方法借鉴了现有的图像识别基础模型（如CLIP、SAM）、3D点云模型（如Uni3D）以及各种SLAM技术和场景表示方法（如点云、体素、NeRF等）。作者特别关注动态场景的处理，这是当前研究的明显不足，并设计了专门的模块来处理动态环境中的物体分割、跟踪和地图更新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个模块化的多模态3D地图系统，能够处理静态和动态场景，并整合多种数据源。整体流程包括：1) 输入多模态传感器数据（图像、点云、GNSS、IMU等）；2) 通过物体分割和跟踪模块识别和跟踪环境中的物体；3) 使用里程计估计模块确定机器人位置；4) 通过3D地图构建模块创建或更新地图（支持多种表示方式）；5) 使用多模态数据检索模块根据查询从地图中提取信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了多模态3D地图方法的分类法；2) 设计了灵活的模块化架构；3) 实现了多模态数据的有效融合；4) 特别关注动态场景的处理；5) 支持多种3D场景表示方法。相比之前的工作，M3DMap提供了统一的框架而非分散的方法，具有更好的灵活性和可定制性，特别增强了动态环境处理能力，并提供了多模态融合有效性的理论证明。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M3DMap提出了一种模块化的物体感知多模态3D地图构建方法，通过整合多种数据源和场景表示方式，有效解决了动态环境中的3D地图构建问题，并提供了理论证明支持其多模态融合的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D mapping in dynamic environments poses a challenge for modern researchersin robotics and autonomous transportation. There are no universalrepresentations for dynamic 3D scenes that incorporate multimodal data such asimages, point clouds, and text. This article takes a step toward solving thisproblem. It proposes a taxonomy of methods for constructing multimodal 3D maps,classifying contemporary approaches based on scene types and representations,learning methods, and practical applications. Using this taxonomy, a briefstructured analysis of recent methods is provided. The article also describesan original modular method called M3DMap, designed for object-awareconstruction of multimodal 3D maps for both static and dynamic scenes. Itconsists of several interconnected components: a neural multimodal objectsegmentation and tracking module; an odometry estimation module, includingtrainable algorithms; a module for 3D map construction and updating withvarious implementations depending on the desired scene representation; and amultimodal data retrieval module. The article highlights originalimplementations of these modules and their advantages in solving variouspractical tasks, from 3D object grounding to mobile manipulation. Additionally,it presents theoretical propositions demonstrating the positive effect of usingmultimodal data and modern foundational models in 3D mapping methods. Detailsof the taxonomy and method implementation are available athttps://yuddim.github.io/M3DMap.</description>
      <author>example@mail.com (Dmitry Yudin)</author>
      <guid isPermaLink="false">2508.17044v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes</title>
      <link>http://arxiv.org/abs/2508.16812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted to BMVC 2025 as an oral paper. The OVAD  dataset is available at https://doi.org/10.5281/zenodo.16904069&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出OVODA框架，实现了开放词汇3D物体和属性检测，无需预先知道新类别的锚框尺寸，并发布OVAD数据集提供全面的属性标注。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测在自主系统中扮演重要角色，但现有方法受限于封闭集假设，难以在真实场景中识别新物体及其属性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时进行开放词汇3D物体和属性检测的框架，且不需要预先知道新类别的锚框尺寸。&lt;h4&gt;方法&lt;/h4&gt;OVODA使用基础模型弥合3D特征与文本之间的语义差距，同时检测物体属性如空间关系、运动状态等；提出OVAD数据集补充现有3D物体检测基准；包含基础模型特征连接、提示调整策略、视角指定提示和水平翻转增强等创新技术。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Argoverse 2数据集上，OVODA在不提供新类别锚框尺寸的情况下，在开放词汇3D物体检测中优于最先进方法，并能成功识别物体属性。&lt;h4&gt;结论&lt;/h4&gt;OVODA框架有效解决了开放词汇3D物体检测的挑战，OVAD数据集的发布促进了相关研究方向的发展。&lt;h4&gt;翻译&lt;/h4&gt;三维物体检测在自主系统中起着至关重要的作用，然而现有方法受限于封闭集假设，难以在真实场景中识别新物体及其属性。我们提出了OVODA，一个新颖的框架，能够实现开放词汇的三维物体和属性检测，无需预先知道新类别的锚框尺寸。OVODA使用基础模型弥合三维特征与文本之间的语义差距，同时检测属性，例如空间关系、运动状态等。为促进这一研究方向，我们提出了OVAD，一个新数据集，通过全面的属性标注补充现有的三维物体检测基准。OVODA包含几项关键创新，包括基础模型特征连接、提示调整策略以及属性检测的专门技术，包括视角指定提示和水平翻转增强。我们在nuScenes和Argoverse 2数据集上的结果表明，在不给定新类别锚框尺寸的情况下，OVODA在开放词汇三维物体检测中优于最先进方法，并成功识别物体属性。我们的OVAD数据集在此发布：https://doi.org/10.5281/zenodo.16904069。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决3D目标检测系统的封闭集假设限制，使其无法识别现实世界中的新类别物体及其属性。这一问题在自动驾驶和自主系统中至关重要，因为这些系统需要能够处理复杂、动态的真实世界场景，识别训练时未见过的物体，并理解物体的属性（如空间关系、运动状态等）以做出正确的决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有3D检测方法的局限性，然后借鉴了2D开放词汇检测使用预训练视觉-语言模型的思想，以及3DETR的Transformer架构。他们设计了OVODA框架，使用基础模型（如OneLLM）桥接3D特征和文本语义，并引入复杂事件生成模块处理物体间关系。作者还借鉴了跨模态特征对齐的思想来减少不同模态间的差距，同时创新性地加入了特定视角提示和水平翻转增强技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基础模型桥接3D几何信息和文本语义，实现开放词汇的物体和属性检测，并将属性检测集成到物体检测中。整体流程包括：1)单物体提案生成：使用类无关检测器生成提案，裁剪多模态输入，用基础模型对齐特征；2)复杂事件生成：生成非空间和空间属性提案，创建文本描述，对齐视觉和文本特征；3)整体优化：使用开放词汇物体和属性损失进行端到端训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：OVODA框架、基础模型特征连接、提示调优策略、复杂事件生成模块、特定视角提示、水平翻转增强和OVAD数据集。相比之前工作，OVODA实现了开放词汇能力、无需预定义锚框尺寸、支持多模态输入、能够检测物体属性和复杂事件，同时保持实时性能。它突破了传统封闭集3D检测的限制，提供了更全面、灵活的场景理解能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OVODA通过利用基础模型和创新的复杂事件生成模块，首次实现了无需预定义新类别锚框尺寸的开放词汇多模态3D物体和属性检测，同时提出了OVAD数据集来促进这一研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection plays a crucial role in autonomous systems, yet existingmethods are limited by closed-set assumptions and struggle to recognize novelobjects and their attributes in real-world scenarios. We propose OVODA, a novelframework enabling both open-vocabulary 3D object and attribute detection withno need to know the novel class anchor size. OVODA uses foundation models tobridge the semantic gap between 3D features and texts while jointly detectingattributes, e.g., spatial relationships, motion states, etc. To facilitate suchresearch direction, we propose OVAD, a new dataset that supplements existing 3Dobject detection benchmarks with comprehensive attribute annotations. OVODAincorporates several key innovations, including foundation model featureconcatenation, prompt tuning strategies, and specialized techniques forattribute detection, including perspective-specified prompts and horizontalflip augmentation. Our results on both the nuScenes and Argoverse 2 datasetsshow that under the condition of no given anchor sizes of novel classes, OVODAoutperforms the state-of-the-art methods in open-vocabulary 3D object detectionwhile successfully recognizing object attributes. Our OVAD dataset is releasedhere: https://doi.org/10.5281/zenodo.16904069 .</description>
      <author>example@mail.com (Xinhao Xiang, Kuan-Chuan Peng, Suhas Lohit, Michael J. Jones, Jiawei Zhang)</author>
      <guid isPermaLink="false">2508.16812v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Latent Graph Learning in Generative Models of Neural Signals</title>
      <link>http://arxiv.org/abs/2508.16776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究从神经信号中推断时间交互图和高阶结构，探索神经信号生成模型中的潜在图学习，通过对比已知真实连接的神经回路数值模拟，评估了解释学习模型权重的几种假设，并发现提取的网络表示与底层有向图存在适度一致性，而在共输入图表示中存在强一致性，为神经数据大规模基础模型构建中融入基于图的几何约束提供了方向。&lt;h4&gt;背景&lt;/h4&gt;从神经信号中推断时间交互图和高阶结构是构建系统神经科学生成模型的关键问题。大规模神经数据的基础模型代表了神经信号的共享潜在结构，但在这些基础模型中提取可解释的潜在图表示仍然具有挑战性且尚未解决。&lt;h4&gt;目的&lt;/h4&gt;探索神经信号生成模型中的潜在图学习，评估解释学习模型权重的几种假设，并研究提取的网络表示与底层图之间的一致性。&lt;h4&gt;方法&lt;/h4&gt;通过对比已知真实连接的神经回路数值模拟，评估了解释学习模型权重的几种假设。研究提取的网络表示与底层有向图以及共输入图表示之间的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;提取的网络表示与底层有向图之间存在适度一致性，而在共输入图表示中存在强一致性。&lt;h4&gt;结论&lt;/h4&gt;这些发现为在神经数据大规模基础模型构建中融入基于图的几何约束提供了路径。&lt;h4&gt;翻译&lt;/h4&gt;从神经信号中推断时间交互图和高阶结构是构建系统神经科学生成模型的关键问题。大规模神经数据的基础模型代表了神经信号的共享潜在结构。然而，在基础模型中提取可解释的潜在图表示仍然具有挑战性且尚未解决。在这里，我们探索了神经信号生成模型中的潜在图学习。通过与已知真实连接的神经回路数值模拟进行测试，我们评估了几个解释学习模型权重的假设。我们发现，提取的网络表示与底层有向图之间存在适度的一致性，而在共输入图表示中存在强一致性。这些发现为在神经数据大规模基础模型构建中融入基于图的几何约束提供了路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inferring temporal interaction graphs and higher-order structure from neuralsignals is a key problem in building generative models for systemsneuroscience. Foundation models for large-scale neural data represent sharedlatent structures of neural signals. However, extracting interpretable latentgraph representations in foundation models remains challenging and unsolved.Here we explore latent graph learning in generative models of neural signals.By testing against numerical simulations of neural circuits with knownground-truth connectivity, we evaluate several hypotheses for explaininglearned model weights. We discover modest alignment between extracted networkrepresentations and the underlying directed graphs and strong alignment in theco-input graph representations. These findings motivate paths towardsincorporating graph-based geometric constraints in the construction oflarge-scale foundation models for neural data.</description>
      <author>example@mail.com (Nathan X. Kodama, Kenneth A. Loparo)</author>
      <guid isPermaLink="false">2508.16776v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>T-MASK: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring</title>
      <link>http://arxiv.org/abs/2508.16207v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by 26th IEEE International Conference on  Intelligent Transportation Systems ITSC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了相机视角变化对驾驶员监控的影响，提出了一种名为T-MASK的新方法，通过时间令牌屏蔽和强调动态视频区域来提高跨视角准确率，不增加任何参数的情况下实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;相机视角变化是驾驶员监控中的一个常见障碍。虽然深度学习和预训练基础模型通过轻量级适应最终层（'探测'）显示出改进泛化的强大潜力，但它们对未见视角的鲁棒性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究通过单一训练视角将图像基础模型适应到驾驶员监控任务的挑战，并评估这些模型在未见视角上的表现，无需进一步适应。&lt;h4&gt;方法&lt;/h4&gt;研究团队比较了简单的线性探测、高级探测策略，以及两种基础模型（DINOv2和CLIP）与参数高效微调（PEFT）和完整微调。基于这些见解，他们引入了T-MASK——一种新的图像到视频探测方法，利用时间令牌屏蔽并强调更动态的视频区域。&lt;h4&gt;主要发现&lt;/h4&gt;在公共Drive&amp;Act数据集上，T-MASK比强大的探测基线提高了跨视角top-1准确率+1.23%，比PEFT方法提高了+8.0%，且不添加任何参数。T-MASK在代表性不足的次要活动中特别有效，在训练视角下提高了+5.42%的识别率，在跨视角设置下提高了+1.36%。&lt;h4&gt;结论&lt;/h4&gt;使用轻量级探测方法（如T-MASK）适应基础模型在细粒度驾驶员观察方面有很大潜力，特别是在跨视角和低数据设置下。这些结果强调了在利用基础模型构建鲁棒的驾驶员监控系统时，时间令牌选择的重要性。&lt;h4&gt;翻译&lt;/h4&gt;视角变化是驾驶员监控中的常见障碍。虽然深度学习和预训练基础模型通过轻量级适应最终层（'探测'）显示出改进泛化的强大潜力，但它们对未见视角的鲁棒性尚未得到充分探索。我们通过单一训练视角将图像基础模型适应到驾驶员监控任务，并直接在未见视角上评估它们，无需进一步适应。我们基准测试了简单的线性探测、高级探测策略，并将两种基础模型（DINOv2和CLIP）与参数高效微调（PEFT）和完整微调进行比较。基于这些见解，我们引入了T-MASK——一种新的图像到视频探测方法，利用时间令牌屏蔽并强调更动态的视频区域。在公共Drive&amp;Act数据集上，T-MASK比强大的探测基线提高了跨视角top-1准确率+1.23%，比PEFT方法提高了+8.0%，且不添加任何参数。它在代表性不足的次要活动中特别有效，在训练视角下提高了+5.42%的识别率，在跨视角设置下提高了+1.36%。这项工作提供了令人鼓舞的证据，表明使用轻量级探测方法如T-MASK适应基础模型在细粒度驾驶员观察方面有很大潜力，特别是在跨视角和低数据设置下。这些结果强调了在利用基础模型构建鲁棒的驾驶员监控系统时，时间令牌选择的重要性。代码和模型将在https://github.com/th-nesh/T-MASK提供，以支持持续的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Changes of camera perspective are a common obstacle in driver monitoring.While deep learning and pretrained foundation models show strong potential forimproved generalization via lightweight adaptation of the final layers('probing'), their robustness to unseen viewpoints remains underexplored. Westudy this challenge by adapting image foundation models to driver monitoringusing a single training view, and evaluating them directly on unseenperspectives without further adaptation. We benchmark simple linear probes,advanced probing strategies, and compare two foundation models (DINOv2 andCLIP) against parameter-efficient fine-tuning (PEFT) and full fine-tuning.Building on these insights, we introduce T-MASK -- a new image-to-video probingmethod that leverages temporal token masking and emphasizes more dynamic videoregions. Benchmarked on the public Drive&amp;Act dataset, T-MASK improvescross-view top-1 accuracy by $+1.23\%$ over strong probing baselines and$+8.0\%$ over PEFT methods, without adding any parameters. It provesparticularly effective for underrepresented secondary activities, boostingrecognition by $+5.42\%$ under the trained view and $+1.36\%$ under cross-viewsettings. This work provides encouraging evidence that adapting foundationmodels with lightweight probing methods like T-MASK has strong potential infine-grained driver observation, especially in cross-view and low-datasettings. These results highlight the importance of temporal token selectionwhen leveraging foundation models to build robust driver monitoring systems.Code and models will be made available at https://github.com/th-nesh/T-MASK tosupport ongoing research.</description>
      <author>example@mail.com (Thinesh Thiyakesan Ponbagavathi, Kunyu Peng, Alina Roitberg)</author>
      <guid isPermaLink="false">2508.16207v2</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding</title>
      <link>http://arxiv.org/abs/2508.18187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决大脑信号随时间衰减导致视觉识别困难的新方法，通过BRAIN框架有效处理了大脑信号不一致性和累积偏差问题。&lt;h4&gt;背景&lt;/h4&gt;记忆衰减使人类大脑更难识别视觉对象和保留细节，导致记录的大脑信号随时间推移变得更弱、不确定且包含较差的视觉上下文。&lt;h4&gt;目的&lt;/h4&gt;解决大脑信号在记录过程中存在的不一致性问题及其对视觉-大脑理解(VBU)模型的影响，同时防止模型在持续学习过程中的灾难性遗忘。&lt;h4&gt;方法&lt;/h4&gt;提出Bias-Mitigation Continual Learning(BRAIN)方法，引入De-bias Contrastive Learning损失函数处理偏差问题，并采用Angular-based Forgetting Mitigation方法防止灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;统计和实验证明大脑信号存在不一致性，其表示随记录会话变化而偏移，导致累积偏差，这对模型学习构成挑战并降低性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的BRAIN方法在各种基准测试中实现了最先进的性能，超越了先前和非持续学习方法。&lt;h4&gt;翻译&lt;/h4&gt;记忆衰减使人类大脑更难识别视觉对象和保留细节。因此，记录的大脑信号随时间推移变得更弱、不确定且包含较差的视觉上下文。本文提出了首批解决此问题的视觉学习方法之一。首先，我们通过统计和实验证明了大脑信号中存在不一致性及其对视觉-大脑理解(VBU)模型的影响。我们的研究表明，大脑信号表示在记录会话中会发生变化，导致累积偏差，这对模型学习构成挑战并降低性能。然后，我们提出了新的偏差缓解持续学习(BRAIN)方法来解决这些局限性。在该方法中，模型在持续学习设置下进行训练，并缓解每个学习步骤中增长的偏差。同时引入了一种名为去偏差对比学习的新损失函数来解决偏差问题。此外，为防止灾难性遗忘（模型失去之前会话的知识），引入了新的基于角度的遗忘缓解方法来在模型中保留已学知识。最后，实证实验证明我们的方法在各种基准测试中实现了最先进的性能，超越了先前和非持续学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Memory decay makes it harder for the human brain to recognize visual objectsand retain details. Consequently, recorded brain signals become weaker,uncertain, and contain poor visual context over time. This paper presents oneof the first vision-learning approaches to address this problem. First, westatistically and experimentally demonstrate the existence of inconsistency inbrain signals and its impact on the Vision-Brain Understanding (VBU) model. Ourfindings show that brain signal representations shift over recording sessions,leading to compounding bias, which poses challenges for model learning anddegrades performance. Then, we propose a new Bias-Mitigation Continual Learning(BRAIN) approach to address these limitations. In this approach, the model istrained in a continual learning setup and mitigates the growing bias from eachlearning step. A new loss function named De-bias Contrastive Learning is alsointroduced to address the bias problem. In addition, to prevent catastrophicforgetting, where the model loses knowledge from previous sessions, the newAngular-based Forgetting Mitigation approach is introduced to preserve learnedknowledge in the model. Finally, the empirical experiments demonstrate that ourapproach achieves State-of-the-Art (SOTA) performance across variousbenchmarks, surpassing prior and non-continual learning methods.</description>
      <author>example@mail.com (Xuan-Bac Nguyen, Thanh-Dat Truong, Pawan Sinha, Khoa Luu)</author>
      <guid isPermaLink="false">2508.18187v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>S2Sent: Nested Selectivity Aware Sentence Representation Learning</title>
      <link>http://arxiv.org/abs/2508.18164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为S²Sent的句子表示选择机制，通过空间选择和嵌套频率选择优化Transformer编码器的跨块表示融合，显著提升了句子表示学习效果。&lt;h4&gt;背景&lt;/h4&gt;Transformer编码器与对比学习的结合是当前句子表示学习的主流范式，通常基于编码器最后一个Transformer块的隐藏状态。不同Transformer块具有不同程度的语义感知能力，从可解释性角度看，知识神经元的语义感知潜力受刺激调节。&lt;h4&gt;目的&lt;/h4&gt;平衡跨块表示融合中的语义冗余和损失，优化跨块表示融合方向。&lt;h4&gt;方法&lt;/h4&gt;提出S²Sent句子表示选择机制，在Transformer编码器下游集成参数化嵌套选择器，执行空间选择(SS)和嵌套频率选择(FS)。SS采用基于空间挤压的自门控机制获得自适应权重，嵌套FS用不同DCT基函数替换GAP实现低语义损失的空间挤压。&lt;h4&gt;主要发现&lt;/h4&gt;广泛实验证明S²Sent在基线方法上取得显著改进，仅增加可忽略的额外参数和推理延迟，具有高度的可集成性和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;S²Sent是一种有效的句子表示选择机制，能够优化跨块表示融合，平衡语义冗余和损失，提升句子表示学习性能。&lt;h4&gt;翻译&lt;/h4&gt;基于Transformer编码器与对比学习的结合代表了当前句子表示学习的主流范式。该范式通常基于编码器最后一个Transformer块的隐藏状态。然而，在基于Transformer的编码器中，不同的块表现出不同程度的语义感知能力。从可解释性角度看，知识神经元的语义感知潜力受刺激调节，因此合理的跨块表示融合是一个值得优化的方向。为了平衡跨块融合中的语义冗余和损失，我们提出了一种句子表示选择机制S²Sent，它在基于Transformer的编码器下游集成了一个参数化嵌套选择器。该选择器从模块化角度执行空间选择(SS)和嵌套频率选择(FS)。SS创新性地采用基于空间挤压的自门控机制获得自适应权重，这不仅实现了低信息冗余的融合，还捕获了嵌入特征之间的依赖关系。嵌套FS用不同的DCT基函数替换GAP，以实现低语义损失的空间挤压。大量实验证明，S²Sent在基线方法上取得了显著改进，同时仅增加了可忽略的额外参数和推理延迟，并突出了高度的可集成性和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The combination of Transformer-based encoders with contrastive learningrepresents the current mainstream paradigm for sentence representationlearning. This paradigm is typically based on the hidden states of the lastTransformer block of the encoder. However, within Transformer-based encoders,different blocks exhibit varying degrees of semantic perception ability. Fromthe perspective of interpretability, the semantic perception potential ofknowledge neurons is modulated by stimuli, thus rational cross-blockrepresentation fusion is a direction worth optimizing. To balance the semanticredundancy and loss across block fusion, we propose a sentence representationselection mechanism S\textsuperscript{2}Sent, which integrates a parameterizednested selector downstream of the Transformer-based encoder. This selectorperforms spatial selection (SS) and nested frequency selection (FS) from amodular perspective. The SS innovatively employs a spatial squeeze basedself-gating mechanism to obtain adaptive weights, which not only achievesfusion with low information redundancy but also captures the dependenciesbetween embedding features. The nested FS replaces GAP with different DCT basisfunctions to achieve spatial squeeze with low semantic loss. Extensiveexperiments have demonstrated that S\textsuperscript{2}Sent achievessignificant improvements over baseline methods with negligible additionalparameters and inference latency, while highlighting high integrability andscalability.</description>
      <author>example@mail.com (Jianxiang Zang, Nijia Mo, Yonda Wei, Meiling Ning, Hui Liu)</author>
      <guid isPermaLink="false">2508.18164v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>A Contrastive Learning-Guided Confident Meta-learning for Zero Shot Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.17827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to VISION Workshop at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoZAD是一种创新的零样本异常检测框架，通过结合软置信学习、元学习和对比特征表示，有效解决了工业和医疗领域数据稀缺和标注成本高的问题，在各种数据集上表现出色，特别在纹理丰富数据和像素级定位任务上取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;工业和医疗异常检测面临数据稀缺和标注成本高昂的挑战，特别是在不断变化的制造和医疗环境中。&lt;h4&gt;目的&lt;/h4&gt;提出一种零样本异常检测框架CoZAD，以解决数据稀缺和标注成本高昂的问题。&lt;h4&gt;方法&lt;/h4&gt;CoZAD将软置信学习与元学习和对比特征表示相结合。与传统置信学习丢弃不确定样本不同，该方法为所有训练数据分配基于置信度的权重，保留边界信息同时强调典型的正常模式。框架通过基于IQR的阈值量化数据不确定性，通过基于协方差的正则化在模型无关元学习内量化模型不确定性。对比学习创建判别性特征空间，使正常模式形成紧密簇，实现快速域适应。&lt;h4&gt;主要发现&lt;/h4&gt;在跨越工业和医疗领域的10个数据集上全面评估，展示了最先进的性能，在7个工业基准测试中的6个上优于现有方法，在纹理丰富的数据集上取得显著改进（DTD-Synthetic上99.2% I-AUROC，BTAD上97.2%），并且在像素级定位上表现优异（MVTec-AD上96.3% P-AUROC）。&lt;h4&gt;结论&lt;/h4&gt;该框架消除对视觉-语言对齐或模型集成的依赖，使其对于需要快速部署的资源受限环境具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;工业和医疗异常检测面临数据稀缺和高昂标注成本的严峻挑战，特别是在不断发展的制造和医疗环境中。为此，我们提出了CoZAD，一种将软置信学习与元学习和对比特征表示相结合的新型零样本异常检测框架。与传统丢弃不确定样本的置信学习不同，我们的方法为所有训练数据分配基于置信度的权重，在保留边界信息的同时强调典型的正常模式。该框架通过基于IQR的阈值量化数据不确定性，并通过基于协方差的正则化在模型无关元学习内量化模型不确定性。对比学习创建了判别性特征空间，其中正常模式形成紧密簇，实现快速域适应。跨越工业和医疗领域的10个数据集上的全面评估展示了最先进的性能，在7个工业基准测试中的6个上优于现有方法，在纹理丰富的数据集上取得了显著改进（在DTD-Synthetic上达到99.2%的I-AUROC，在BTAD上达到97.2%），并且在像素级定位上表现优异（在MVTec-AD上达到96.3%的P-AUROC）。该框架消除对视觉-语言对齐或模型集成的依赖，使其对于需要快速部署的资源受限环境具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial and medical anomaly detection faces critical challenges from datascarcity and prohibitive annotation costs, particularly in evolvingmanufacturing and healthcare settings. To address this, we propose CoZAD, anovel zero-shot anomaly detection framework that integrates soft confidentlearning with meta-learning and contrastive feature representation. Unliketraditional confident learning that discards uncertain samples, our methodassigns confidence-based weights to all training data, preserving boundaryinformation while emphasizing prototypical normal patterns. The frameworkquantifies data uncertainty through IQR-based thresholding and modeluncertainty via covariance based regularization within a Model-AgnosticMeta-Learning. Contrastive learning creates discriminative feature spaces wherenormal patterns form compact clusters, enabling rapid domain adaptation.Comprehensive evaluation across 10 datasets spanning industrial and medicaldomains demonstrates state-of-the-art performance, outperforming existingmethods on 6 out of 7 industrial benchmarks with notable improvements ontexture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) andpixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminatesdependence on vision-language alignments or model ensembles, making it valuablefor resourceconstrained environments requiring rapid deployment.</description>
      <author>example@mail.com (Muhammad Aqeel, Danijel Skocaj, Marco Cristani, Francesco Setti)</author>
      <guid isPermaLink="false">2508.17827v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</title>
      <link>http://arxiv.org/abs/2508.17708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CATformer（对比对抗Transformer），一种新型神经网络，结合了受扩散模型启发的特征细化与对抗学习和对比学习，用于图像超分辨率任务。&lt;h4&gt;背景&lt;/h4&gt;超分辨率技术是提高低分辨率图像质量的一种有前景的技术。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型神经网络模型CATformer，以提升图像超分辨率的效率和视觉质量。&lt;h4&gt;方法&lt;/h4&gt;CATformer采用双分支架构，结合一个主要受扩散模型启发的transformer分支（逐步细化潜在表示）和一个辅助transformer分支（通过学习的潜在对比增强对噪声的鲁棒性）。这些互补的表示使用深度残差残差密集块进行融合和解码，以增强重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，CATformer在效率和视觉图像质量上都优于最近的基于transformer和受扩散模型启发的方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作弥合了基于transformer、扩散模型和GAN的方法之间的性能差距，为受扩散模型启发的transformer在超分辨率中的实际应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;超分辨率仍然是提高低分辨率图像质量的一种有前景的技术。本研究引入了CATformer（对比对抗Transformer），一种新型神经网络，它将受扩散模型启发的特征细化与对抗学习和对比学习相结合。CATformer采用双分支架构，结合一个主要的受扩散模型启发的transformer（逐步细化潜在表示）和一个辅助的transformer分支（通过学习的潜在对比增强对噪声的鲁棒性）。这些互补的表示使用深度残差残差密集块进行融合和解码，以增强重建质量。在基准数据集上的大量实验表明，CATformer在效率和视觉图像质量上都优于最近的基于transformer和受扩散模型启发的方法。这项工作弥合了基于transformer、扩散模型和GAN的方法之间的性能差距，为受扩散模型启发的transformer在超分辨率中的实际应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Super-resolution remains a promising technique to enhance the quality oflow-resolution images. This study introduces CATformer (Contrastive AdversarialTransformer), a novel neural network integrating diffusion-inspired featurerefinement with adversarial and contrastive learning. CATformer employs adual-branch architecture combining a primary diffusion-inspired transformer,which progressively refines latent representations, with an auxiliarytransformer branch designed to enhance robustness to noise through learnedlatent contrasts. These complementary representations are fused and decodedusing deep Residual-in-Residual Dense Blocks for enhanced reconstructionquality. Extensive experiments on benchmark datasets demonstrate that CATformeroutperforms recent transformer-based and diffusion-inspired methods both inefficiency and visual image quality. This work bridges the performance gapamong transformer-, diffusion-, and GAN-based methods, laying a foundation forpractical applications of diffusion-inspired transformers in super-resolution.</description>
      <author>example@mail.com (Qinyi Tian, Spence Cox, Laura E. Dalton)</author>
      <guid isPermaLink="false">2508.17708v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Radio and Vision Fusion for Robust Localization in Urban V2I Communications</title>
      <link>http://arxiv.org/abs/2508.17640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 6 figures, submitted to conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种多模态对比学习回归定位框架，用于车辆到基础设施(V2I)通信系统，结合信道状态信息和视觉信息以提高定位准确性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;在城市环境中，GPS信号经常被高楼建筑物遮挡，导致定位误差显著，这对自动驾驶和智能城市基础设施等应用构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代或互补的定位技术，以实现V2I场景中可靠且精确的定位。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多模态对比学习回归定位框架，结合信道状态信息(CSI)和视觉信息，利用无线数据和视觉数据的互补优势。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，提出的CSI和视觉融合模型显著优于传统方法和单模态模型，在复杂城市环境中实现了卓越的定位准确性和精度。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为V2I应用提供了一种强大的解决方案，能够克服传统定位方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;精确的定位对于车辆到基础设施(V2I)通信系统至关重要，特别是在城市地区，GPS信号经常被高楼建筑物遮挡，导致显著的定位误差，这 necessitating替代或互补的技术，用于自动驾驶和智能城市基础设施等应用中的可靠和精确定位。本文提出了一种基于多模态对比学习回归的V2I场景定位框架，结合信道状态信息(CSI)和视觉信息，以实现更高的准确性和可靠性。该方法利用无线和视觉数据的互补优势来克服传统定位方法的局限性，为V2I应用提供了强大的解决方案。仿真结果表明，提出的CSI和视觉融合模型显著优于传统方法和单模态模型，在复杂城市环境中实现了卓越的定位准确性和精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is critical for vehicle-to-infrastructure (V2I)communication systems, especially in urban areas where GPS signals are oftenobstructed by tall buildings, leading to significant positioning errors,necessitating alternative or complementary techniques for reliable and precisepositioning in applications like autonomous driving and smart cityinfrastructure. This paper proposes a multimodal contrastive learningregression based localization framework for V2I scenarios that combines channelstate information (CSI) with visual information to achieve improved accuracyand reliability. The approach leverages the complementary strengths of wirelessand visual data to overcome the limitations of traditional localizationmethods, offering a robust solution for V2I applications. Simulation resultsdemonstrate that the proposed CSI and vision fusion model significantlyoutperforms traditional methods and single modal models, achieving superiorlocalization accuracy and precision in complex urban environments.</description>
      <author>example@mail.com (Can Zheng, Jiguang He, Chung G. Kang, Guofa Cai, Henk Wymeersch)</author>
      <guid isPermaLink="false">2508.17640v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising</title>
      <link>http://arxiv.org/abs/2508.17299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出FoundDiff，一种用于统一且可泛化的低剂量CT去噪的基础扩散模型，能够处理各种剂量水平和解剖区域。&lt;h4&gt;背景&lt;/h4&gt;低剂量CT去噪对于减少辐射暴露同时确保可诊断的图像质量至关重要。现有的基于深度学习的方法通常在特定剂量水平和解剖区域训练，难以处理多样化噪声和解剖异质性，限制了临床应用中的泛化能力和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一处理各种剂量水平和解剖区域的可泛化低剂量CT去噪方法。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段策略：1) 剂量-解剖感知：开发DA-CLIP模型，通过对比学习学习连续表示量化剂量变化和识别解剖区域；2) 自适应去噪：设计DA-Diff模型，通过基于Mamba的DACB将剂量和解剖嵌入整合到扩散过程中实现自适应去噪。&lt;h4&gt;主要发现&lt;/h4&gt;在包含八个剂量水平和三个解剖区域的数据集上，FoundDiff的去噪性能优于现有最先进方法，并对未见过的剂量水平表现出显著泛化能力。&lt;h4&gt;结论&lt;/h4&gt;FoundDiff能有效处理不同剂量水平和解剖区域的低剂量CT去噪任务，具有临床应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;低剂量计算机断层扫描（CT）去噪对于减少辐射暴露同时确保可诊断的图像质量至关重要。尽管近年来深度学习（DL）取得了显著进展，但现有的基于DL的方法通常在特定的剂量水平和解剖区域进行训练，难以处理不同扫描条件下的多样化噪声特征和解剖异质性，限制了它们在临床场景中的泛化能力和鲁棒性。在本文中，我们提出FoundDiff，一种用于统一且可泛化的低剂量CT去噪的基础扩散模型，可处理各种剂量水平和解剖区域。FoundDiff采用两阶段策略：（i）剂量-解剖感知和（ii）自适应去噪。首先，我们开发了一种剂量和解剖感知的对比语言图像预训练模型（DA-CLIP），通过利用专门的对比学习策略学习连续表示，以量化序数剂量变化并识别显著的解剖区域，从而实现鲁棒的剂量和解剖感知。其次，我们设计了一种剂量和解剖感知的扩散模型（DA-Diff），通过基于Mamba的新型剂量和解剖条件块（DACB）将DA-CLIP学习到的剂量和解剖嵌入协同整合到扩散过程中，执行自适应且可泛化的去噪。在包含八个剂量水平和三个解剖区域的两个公共低剂量CT数据集上进行的大量实验表明，FoundDiff的去噪性能优于现有的最先进方法，并且对未见过的剂量水平表现出显著的泛化能力。代码和模型可在https://github.com/hao1635/FoundDiff获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-dose computed tomography (CT) denoising is crucial for reduced radiationexposure while ensuring diagnostically acceptable image quality. Despitesignificant advancements driven by deep learning (DL) in recent years, existingDL-based methods, typically trained on a specific dose level and anatomicalregion, struggle to handle diverse noise characteristics and anatomicalheterogeneity during varied scanning conditions, limiting theirgeneralizability and robustness in clinical scenarios. In this paper, wepropose FoundDiff, a foundational diffusion model for unified and generalizableLDCT denoising across various dose levels and anatomical regions. FoundDiffemploys a two-stage strategy: (i) dose-anatomy perception and (ii) adaptivedenoising. First, we develop a dose- and anatomy-aware contrastive languageimage pre-training model (DA-CLIP) to achieve robust dose and anatomyperception by leveraging specialized contrastive learning strategies to learncontinuous representations that quantify ordinal dose variations and identifysalient anatomical regions. Second, we design a dose- and anatomy-awarediffusion model (DA-Diff) to perform adaptive and generalizable denoising bysynergistically integrating the learned dose and anatomy embeddings from DACLIPinto diffusion process via a novel dose and anatomy conditional block (DACB)based on Mamba. Extensive experiments on two public LDCT datasets encompassingeight dose levels and three anatomical regions demonstrate superior denoisingperformance of FoundDiff over existing state-of-the-art methods and theremarkable generalization to unseen dose levels. The codes and models areavailable at https://github.com/hao1635/FoundDiff.</description>
      <author>example@mail.com (Zhihao Chen, Qi Gao, Zilong Li, Junping Zhang, Yi Zhang, Jun Zhao, Hongming Shan)</author>
      <guid isPermaLink="false">2508.17299v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.17086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种表示学习框架，用于检测金融市场中的欺骗性交易操纵策略，特别是Spoofing行为。通过结合级联限价订单簿表示流程与监督对比学习，有效解决了高维噪声数据中的多层次异常模式检测问题。&lt;h4&gt;背景&lt;/h4&gt;金融市场对全球经济稳定至关重要，但基于交易的操纵行为经常破坏市场公平性。其中Spoofing是一种特别具有欺骗性的操纵策略，其展现的多层次异常模式尚未被充分建模，且这些模式通常隐藏在限价订单簿的丰富层次化信息中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用限价订单簿层次化信息的表示学习框架，以准确检测金融市场中的欺骗性交易操纵策略，特别是Spoofing行为。&lt;h4&gt;方法&lt;/h4&gt;提出了一种表示学习框架，结合了级联限价订单簿表示流程与监督对比学习技术，特别采用了基于Transformer的架构来处理高维噪声数据。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在不同模型上持续提高了检测性能，其中基于Transformer的架构实现了最先进的结果。通过系统分析和消融研究，深入理解了多层次异常模式和关键组件的贡献。&lt;h4&gt;结论&lt;/h4&gt;该研究为复杂序列数据的表示学习和异常检测提供了更广泛的见解，相关代码将在指定URL发布。&lt;h4&gt;翻译&lt;/h4&gt;金融市场对全球经济稳定至关重要，但基于交易的操纵行为（TBM）经常破坏其公平性。欺骗性TBM策略（特别是Spoofing）展现的多层次异常模式尚未被充分建模。这些模式通常隐藏在限价订单簿（LOB）的丰富层次化信息中，由于高维度和噪声，难以有效利用。为此，我们提出了一种表示学习框架，结合了级联LOB表示流程与监督对比学习。大量实验表明，我们的框架在不同模型上持续提高了检测性能，其中基于Transformer的架构实现了最先进的结果。此外，我们进行了系统分析和消融研究，以探究多层次异常和关键组件的贡献，为复杂序列数据的表示学习和异常检测提供了更广泛的见解。我们的代码将在后续通过此URL发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial markets are critical to global economic stability, yet trade-basedmanipulation (TBM) often undermines their fairness. Spoofing, a particularlydeceptive TBM strategy, exhibits multilevel anomaly patterns that have not beenadequately modeled. These patterns are usually concealed within the rich,hierarchical information of the Limit Order Book (LOB), which is challenging toleverage due to high dimensionality and noise. To address this, we propose arepresentation learning framework combining a cascaded LOB representationpipeline with supervised contrastive learning. Extensive experimentsdemonstrate that our framework consistently improves detection performanceacross diverse models, with Transformer-based architectures achievingstate-of-the-art results. In addition, we conduct systematic analyses andablation studies to investigate multilevel anomalies and the contributions ofkey components, offering broader insights into representation learning andanomaly detection for complex sequential data. Our code will be released laterat this URL.</description>
      <author>example@mail.com (Yushi Lin, Peng Yang)</author>
      <guid isPermaLink="false">2508.17086v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Local Information Matters: A Rethink of Crowd Counting</title>
      <link>http://arxiv.org/abs/2508.16970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的人群计数模型设计原则，强调局部建模能力，设计了LIMM模型，通过窗口划分和窗口对比学习策略提高模型对局部密度的区分能力，同时应用全局注意力模块处理大尺寸个体。&lt;h4&gt;背景&lt;/h4&gt;人群计数任务中，个体（人的头部）通常只占图像的一小部分，但这一特征在现有工作中未被重视，它们通常与其他视觉任务使用相同骨干网络并追求大感受野。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的人群计数模型设计原则，强调模型的局部建模能力，以提高计数准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了LIMM模型，主要创新包括：1) 对模型输入应用网格窗口划分设计；2) 窗口对比学习设计增强模型区分局部密度级别的能力；3) 在模型末尾应用全局注意力模块处理大尺寸个体。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集上的实验表明，LIMM模型在局部建模能力方面有显著提高（如JHU-Crowd++高密度子集上MAE提高8.7%），同时不损害对大尺寸个体的计数能力，达到最先进性能。&lt;h4&gt;结论&lt;/h4&gt;强调局部建模能力的人群计数模型设计原则是有效的，LIMM模型显著提高了人群计数的准确性，特别是在高密度区域。&lt;h4&gt;翻译&lt;/h4&gt;本文的动机源于重新思考人群计数的一个基本特征：在人群计数任务中，个体（人的头部）通常只占据图像的一小部分。这一特征从未成为现有工作的焦点：它们通常与其他视觉任务使用相同的骨干网络，并追求较大的感受野。这促使我们提出了一种新的人群计数模型设计原则：强调模型的局部建模能力。我们遵循这一原则，设计了一个名为局部信息重要模型（LIMM）的人群计数模型。主要创新在于两种策略：一种应用于模型输入的网格窗口划分设计，以及一种窗口对比学习设计，以增强模型区分局部密度级别的能力。此外，在模型末尾应用了全局注意力模块来处理偶尔出现的大尺寸个体。在多个公共数据集上的广泛实验表明，所提出的模型在局部建模能力方面有显著提高（例如在JHU-Crowd++高密度子集上的MAE提高了8.7%），同时不损害其对大尺寸个体的计数能力，达到了最先进的性能。代码可在以下网址获取：https://github.com/tianhangpan/LIMM。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The motivation of this paper originates from rethinking an essentialcharacteristic of crowd counting: individuals (heads of humans) in the crowdcounting task typically occupy a very small portion of the image. Thischaracteristic has never been the focus of existing works: they typically usethe same backbone as other visual tasks and pursue a large receptive field.This drives us to propose a new model design principle of crowd counting:emphasizing local modeling capability of the model. We follow the principle anddesign a crowd counting model named Local Information Matters Model (LIMM). Themain innovation lies in two strategies: a window partitioning design thatapplies grid windows to the model input, and a window-wise contrastive learningdesign to enhance the model's ability to distinguish between local densitylevels. Moreover, a global attention module is applied to the end of the modelto handle the occasionally occurring large-sized individuals. Extensiveexperiments on multiple public datasets illustrate that the proposed modelshows a significant improvement in local modeling capability (8.7\% in MAE onthe JHU-Crowd++ high-density subset for example), without compromising itsability to count large-sized ones, which achieves state-of-the-art performance.Code is available at: https://github.com/tianhangpan/LIMM.</description>
      <author>example@mail.com (Tianhang Pan, Xiuyi Jia)</author>
      <guid isPermaLink="false">2508.16970v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>THEME : Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics</title>
      <link>http://arxiv.org/abs/2508.16936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACM International Conference on Information and Knowledge  Management (CIKM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为THEME的层次对比学习框架，用于主题投资中的股票选择和投资组合构建。&lt;h4&gt;背景&lt;/h4&gt;主题投资旨在构建与结构性趋势一致的股票组合，但行业边界重叠和市场动态变化使得选择相关股票具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决主题投资中股票选择的挑战，构建一个能够有效检索具有强劲回报潜力的主题对齐资产的系统。&lt;h4&gt;方法&lt;/h4&gt;构建主题表征集(TRS)作为扩展数据集，从现实世界主题ETF开始并纳入行业分类和金融新闻；基于此数据集开发THEME框架，通过层次对比学习实现主题与股票的语义对齐，并结合股票回报数据进行时间细化。&lt;h4&gt;主要发现&lt;/h4&gt;THEME在多个检索指标上优于强基线，并在投资组合构建方面显著提高了性能。&lt;h4&gt;结论&lt;/h4&gt;通过联合建模文本中的主题关系和回报中的市场动态，THEME为应对复杂投资主题提供了可扩展且适应性强的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;主题投资旨在构建与结构性趋势一致的投资组合，但由于行业边界重叠和不断变化的市场动态，选择相关股票仍然具有挑战性。为了应对这一挑战，我们构建了主题表征集(TRS)，这是一个扩展数据集，从现实世界的主题ETF开始，并通过纳入行业分类和金融新闻来克服其覆盖限制。最终数据集包含主题到成分股票的显式映射以及每个主题的丰富文本资料。基于此数据集，我们引入THEME，一种层次对比学习框架。通过将主题和股票的文本资料表示为嵌入向量，THEME首先利用它们的层次关系实现语义对齐。随后，它通过一个时间细化阶段来完善这些语义嵌入，该阶段纳入了单个股票的回报。最终的股票表征旨在有效地检索具有强劲回报潜力的主题对齐资产。实证结果表明，THEME在多个检索指标上优于强基线，并在投资组合构建方面显著提高了性能。通过联合建模文本中的主题关系和回报中的市场动态，THEME为应对复杂投资主题提供了可扩展且适应性强的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Thematic investing aims to construct portfolios aligned with structuraltrends, yet selecting relevant stocks remains challenging due to overlappingsector boundaries and evolving market dynamics. To address this challenge, weconstruct the Thematic Representation Set (TRS), an extended dataset thatbegins with real-world thematic ETFs and expands upon them by incorporatingindustry classifications and financial news to overcome their coveragelimitations. The final dataset contains both the explicit mapping of themes totheir constituent stocks and the rich textual profiles for each. Building onthis dataset, we introduce \textsc{THEME}, a hierarchical contrastive learningframework. By representing the textual profiles of themes and stocks asembeddings, \textsc{THEME} first leverages their hierarchical relationship toachieve semantic alignment. Subsequently, it refines these semantic embeddingsthrough a temporal refinement stage that incorporates individual stock returns.The final stock representations are designed for effective retrieval ofthematically aligned assets with strong return potential. Empirical resultsshow that \textsc{THEME} outperforms strong baselines across multiple retrievalmetrics and significantly improves performance in portfolio construction. Byjointly modeling thematic relationships from text and market dynamics fromreturns, \textsc{THEME} provides a scalable and adaptive solution fornavigating complex investment themes.</description>
      <author>example@mail.com (Hoyoung Lee, Wonbin Ahn, Suhwan Park, Jaehoon Lee, Minjae Kim, Sungdong Yoo, Taeyoon Lim, Woohyung Lim, Yongjae Lee)</author>
      <guid isPermaLink="false">2508.16936v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR</title>
      <link>http://arxiv.org/abs/2508.16927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to MLMI 2025 (MICCAI workshop); camera-ready version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CC-CMR是一种创新的无钆对比剂心肌病筛查方法，通过对比学习和跨模态对齐技术，在保持高准确率的同时避免了传统CMR的局限性。&lt;h4&gt;背景&lt;/h4&gt;心肌病是心力衰竭和心脏性猝死的主要原因，需要精确的早期筛查。心脏磁共振成像(CMR)被认为是诊断的'金标准'，但依赖于钆对比剂且解读劳动强度大，限制了其在人群规模中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出CC-CMR框架，一种用于无钆对比剂心肌病筛查的对比学习和跨模态对齐框架，使用电影CMR序列。&lt;h4&gt;方法&lt;/h4&gt;通过将电影CMR和晚期钆增强(LGE)序列的潜在空间对齐，模型将纤维化特异性病理编码到电影CMR嵌入中。特征交互模块同时优化诊断精度和跨模态特征一致性，并通过不确定性引导的自适应训练机制动态校准特定任务目标以确保模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在231名受试者的多中心数据评估中，CC-CMR达到0.943的准确率，比最先进的仅使用电影CMR的模型高出4.3%，同时消除了对钆的依赖。&lt;h4&gt;结论&lt;/h4&gt;CC-CMR展示了其在广泛人群和医疗环境中的临床可行性。&lt;h4&gt;翻译&lt;/h4&gt;心肌病是心力衰竭和心脏性猝死的主要原因，需要精确的早期筛查。心脏磁共振成像(CMR)被认为是诊断的'金标准'，通过多参数协议，有潜力成为准确的筛查工具。然而，它对钆对比剂的依赖和劳动密集型的解读阻碍了其在人群规模中的应用。我们提出了CC-CMR，一种使用电影CMR序列进行无钆对比剂心肌病筛查的对比学习和跨模态对齐框架。通过将电影CMR和晚期钆增强(LGE)序列的潜在空间对齐，我们的模型将纤维化特异性病理编码到电影CMR嵌入中。特征交互模块同时优化诊断精度和跨模态特征一致性，并通过不确定性引导的自适应训练机制动态校准特定任务目标以确保模型泛化能力。在231名受试者的多中心数据评估中，CC-CMR达到0.943的准确率，比最先进的仅使用电影CMR的模型高出4.3%，同时消除了对钆的依赖，证明了其在广泛人群和医疗环境中的临床可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiomyopathy, a principal contributor to heart failure and sudden cardiacmortality, demands precise early screening. Cardiac Magnetic Resonance (CMR),recognized as the diagnostic 'gold standard' through multiparametric protocols,holds the potential to serve as an accurate screening tool. However, itsreliance on gadolinium contrast and labor-intensive interpretation hinderspopulation-scale deployment. We propose CC-CMR, a Contrastive Learning andCross-Modal alignment framework for gadolinium-free cardiomyopathy screeningusing cine CMR sequences. By aligning the latent spaces of cine CMR and LateGadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specificpathology into cine CMR embeddings. A Feature Interaction Module concurrentlyoptimizes diagnostic precision and cross-modal feature congruence, augmented byan uncertainty-guided adaptive training mechanism that dynamically calibratestask-specific objectives to ensure model generalizability. Evaluated onmulti-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI:0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% whileeliminating gadolinium dependency, demonstrating its clinical viability forwide range of populations and healthcare environments.</description>
      <author>example@mail.com (Siqing Yuan, Yulin Wang, Zirui Cao, Yueyan Wang, Zehao Weng, Hui Wang, Lei Xu, Zixian Chen, Lei Chen, Zhong Xue, Dinggang Shen)</author>
      <guid isPermaLink="false">2508.16927v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.16882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages,6 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于'Align-Disentangle-Fusion'机制的多模态表示学习框架，用于喉咽肿瘤分割。该框架整合2D白光成像和窄带成像对，通过多尺度分布对齐和渐进式特征解缠策略，有效分离模态特定特征和共享特征，实现了稳健的多模态对比学习和高效的语义融合。&lt;h4&gt;背景&lt;/h4&gt;准确的喉咽肿瘤分割对于精确诊断和有效治疗计划至关重要，但传统的单模态成像方法往往无法捕捉这些肿瘤复杂的解剖和病理特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新的多模态表示学习框架，增强喉咽肿瘤分割性能，有效整合不同成像模态的信息。&lt;h4&gt;方法&lt;/h4&gt;基于'Align-Disentangle-Fusion'机制的多模态表示学习框架，整合2D白光成像(WLI)和窄带成像(NBI)对；多尺度分布对齐减轻模态差异；渐进式特征解缠策略分离模态特定特征和共享特征；实现稳健的多模态对比学习和高效的语义融合。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的综合实验表明，该方法始终优于最先进的方法，在各种真实临床场景中实现了更高的准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的多模态表示学习框架在喉咽肿瘤分割任务中表现出色，能够有效整合不同成像模态的信息，提高分割性能。&lt;h4&gt;翻译&lt;/h4&gt;准确的喉咽肿瘤分割对于精确诊断和有效治疗计划至关重要。然而，传统的单模态成像方法往往无法捕捉这些肿瘤复杂的解剖和病理特征。在本研究中，我们提出了一种基于'Align-Disentangle-Fusion'机制的创新多模态表示学习框架，无缝整合2D白光成像(WLI)和窄带成像(NBI)对，以增强分割性能。我们方法的核心是多尺度分布对齐，通过在多个transformer层之间对齐特征来减轻模态差异。此外，我们开发了渐进式特征解缠策略，包括初步解缠和解缠感知对比学习，以有效分离模态特定特征和共享特征，实现稳健的多模态对比学习和高效的语义融合。在多个数据集上的综合实验表明，我们的方法始终优于最先进的方法，在各种真实临床场景中实现了更高的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of laryngo-pharyngeal tumors is crucial for precisediagnosis and effective treatment planning. However, traditionalsingle-modality imaging methods often fall short of capturing the complexanatomical and pathological features of these tumors. In this study, we presentan innovative multi-modality representation learning framework based on the`Align-Disentangle-Fusion' mechanism that seamlessly integrates 2D White LightImaging (WLI) and Narrow Band Imaging (NBI) pairs to enhance segmentationperformance. A cornerstone of our approach is multi-scale distributionalignment, which mitigates modality discrepancies by aligning features acrossmultiple transformer layers. Furthermore, a progressive feature disentanglementstrategy is developed with the designed preliminary disentanglement anddisentangle-aware contrastive learning to effectively separatemodality-specific and shared features, enabling robust multimodal contrastivelearning and efficient semantic fusion. Comprehensive experiments on multipledatasets demonstrate that our method consistently outperforms state-of-the-artapproaches, achieving superior accuracy across diverse real clinical scenarios.</description>
      <author>example@mail.com (Junhao Wu, Yun Li, Junhao Li, Jingliang Bian, Xiaomao Fan, Wenbin Lei, Ruxin Wang)</author>
      <guid isPermaLink="false">2508.16882v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition</title>
      <link>http://arxiv.org/abs/2508.16833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ReProCon是一种结合多原型建模、余弦对比学习和Reptile元学习的小样本NER框架，能有效处理生物医学领域的数据稀缺和标签不平衡问题，在资源受限环境下表现优异。&lt;h4&gt;背景&lt;/h4&gt;生物医学领域的命名实体识别(NER)面临数据稀缺和标签分布不平衡的挑战，特别是在细粒度实体类型的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为ReProCon的新颖小样本NER框架，以解决生物医学NER中的数据稀缺和标签分布不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;结合多原型建模（用多个原型表示每个类别以捕捉语义变异性）、余弦对比学习（确保类别间强分离）和Reptile元学习（使模型在少量数据下快速适应），使用轻量级fastText + BiLSTM编码器降低内存使用。&lt;h4&gt;主要发现&lt;/h4&gt;ReProCon的宏F1分数接近基于BERT的基线（约99%的BERT性能）；在标签预算为30%的情况下保持稳定；当类别从19个扩展到50个时，F1仅下降7.8%；在Few-NERD上优于SpanProto和CONTaiNER等基线；消融研究突出了多原型建模和对比学习在处理类别不平衡方面的重要性。&lt;h4&gt;结论&lt;/h4&gt;尽管存在标签歧义问题，ReProCon在资源受限的环境中展示了最先进的性能，使其适合生物医学应用。&lt;h4&gt;翻译&lt;/h4&gt;生物医学领域的命名实体识别(NER)因数据稀缺和标签分布不平衡而面临挑战，特别是在细粒度实体类型的情况下。我们提出了ReProCon，一种新颖的小样本NER框架，它结合了多原型建模、余弦对比学习和Reptile元学习来解决这些问题。通过用多个原型表示每个类别，ReProCon捕捉语义变异性，如同义词和上下文差异，而余弦对比目标确保了类别间的强分离。Reptile元更新使模型能够在少量数据下快速适应。使用内存使用量低得多的轻量级fastText + BiLSTM编码器，ReProCon实现了接近基于BERT的基线的宏F1分数（约为BERT性能的99%）。在标签预算为30%的情况下，模型保持稳定，当类别从19个扩展到50个时，F1仅下降7.8%，优于SpanProto和CONTaiNER等基线，这些基线在Few-NERD上的F1下降10到32%。消融研究突出了多原型建模和对比学习在处理类别不平衡方面的重要性。尽管存在标签歧义问题，ReProCon在资源受限的环境中展示了最先进的性能，使其适合生物医学应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Named Entity Recognition (NER) in biomedical domains faces challenges due todata scarcity and imbalanced label distributions, especially with fine-grainedentity types. We propose ReProCon, a novel few-shot NER framework that combinesmulti-prototype modeling, cosine-contrastive learning, and Reptilemeta-learning to tackle these issues. By representing each category withmultiple prototypes, ReProCon captures semantic variability, such as synonymsand contextual differences, while a cosine-contrastive objective ensures stronginterclass separation. Reptile meta-updates enable quick adaptation with littledata. Using a lightweight fastText + BiLSTM encoder with much lower memoryusage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines(around 99 percent of BERT performance). The model remains stable with a labelbudget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19to 50 categories, outperforming baselines such as SpanProto and CONTaiNER,which see 10 to 32 percent degradation in Few-NERD. Ablation studies highlightthe importance of multi-prototype modeling and contrastive learning in managingclass imbalance. Despite difficulties with label ambiguity, ReProCondemonstrates state-of-the-art performance in resource-limited settings, makingit suitable for biomedical applications.</description>
      <author>example@mail.com (Jeongkyun Yoo, Nela Riddle, Andrew Hoblitzell)</author>
      <guid isPermaLink="false">2508.16833v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</title>
      <link>http://arxiv.org/abs/2508.15298v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为时间提示对齐(TPA)的方法，用于胎儿先天性心脏病(CHD)在超声视频中的分类。该方法整合了时间建模、提示感知对比学习和不确定性量化，解决了现有方法忽视时间信息、仅限于二元分类和缺乏预测校准的问题。&lt;h4&gt;背景&lt;/h4&gt;先天性心脏病(CHD)在超声视频中的检测受到图像噪声和探头位置变化的影响。虽然自动化方法可以减少操作者依赖，但当前机器学习方法往往忽视时间信息，仅限于二元分类，且不考虑预测校准。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理超声视频中的CHD检测问题，同时考虑时间信息、多类别分类和预测校准的方法。&lt;h4&gt;方法&lt;/h4&gt;Temporal Prompt Alignment (TPA)方法利用基础图像-文本模型和提示感知对比学习，从视频子片段的每一帧提取特征，使用可训练的时间聚合器捕获心脏运动，并通过边际铰链对比损失将视频表示与类别特定的文本提示对齐。此外，引入了条件变分自编码器样式调制(CVAESM)模块，学习潜在样式向量来调制嵌入并量化分类不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在私有CHD检测数据集和EchoNet-Dynamic数据集上评估，TPA实现了CHD诊断最先进的宏观F1分数85.40%，同时将预期校准误差减少了5.38%，自适应ECE减少了6.8%。在EchoNet-Dynamic的三类任务中，宏观F1提高了4.73%（从53.89%提高到58.62%）。&lt;h4&gt;结论&lt;/h4&gt;Temporal Prompt Alignment (TPA)是一个有效的胎儿先天性心脏病(CHD)分类框架，通过整合时间建模、提示感知对比学习和不确定性量化，显著提高了分类性能和校准可靠性。&lt;h4&gt;翻译&lt;/h4&gt;先天性心脏病(CHD)在超声视频中的检测受到图像噪声和探头位置变化的影响。虽然自动化方法可以减少操作者依赖，但当前机器学习方法往往忽视时间信息，仅限于二元分类，且不考虑预测校准。我们提出时间提示对齐(TPA)，一种利用基础图像-文本模型和提示感知对比学习的方法，用于胎儿CHD在心脏超声视频上的分类。TPA使用图像编码器从视频子片段的每一帧提取特征，通过可训练的时间聚合器聚合它们以捕获心脏运动，并通过边际铰链对比损失将视频表示与类别特定的文本提示对齐。为了增强临床可靠性校准，我们引入了条件变分自编码器样式调制(CVAESM)模块，该模块学习潜在样式向量来调制嵌入并量化分类不确定性。在用于CHD检测的私有数据集和用于收缩功能障碍的大型公共数据集EchoNet-Dynamic上进行评估，TPA实现了CHD诊断最先进的宏观F1分数85.40%，同时将预期校准误差减少了5.38%，自适应ECE减少了6.8%。在EchoNet-Dynamic的三类任务中，它将宏观F1提高了4.73%（从53.89%到58.62%）。时间提示对齐(TPA)是一个用于胎儿先天性心脏病(CHD)在超声视频中分类的框架，集成了时间建模、提示感知对比学习和不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Congenital heart defect (CHD) detection in ultrasound videos is hindered byimage noise and probe positioning variability. While automated methods canreduce operator dependence, current machine learning approaches often neglecttemporal information, limit themselves to binary classification, and do notaccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),a method leveraging foundation image-text model and prompt-aware contrastivelearning to classify fetal CHD on cardiac ultrasound videos. TPA extractsfeatures from each frame of video subclips using an image encoder, aggregatesthem with a trainable temporal extractor to capture heart motion, and alignsthe video representation with class-specific text prompts via a margin-hingecontrastive loss. To enhance calibration for clinical reliability, we introducea Conditional Variational Autoencoder Style Modulation (CVAESM) module, whichlearns a latent style vector to modulate embeddings and quantifiesclassification uncertainty. Evaluated on a private dataset for CHD detectionand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPAachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, whilealso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. OnEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenitalheart defect (CHD) classification in ultrasound videos that integrates temporalmodeling, prompt-aware contrastive learning, and uncertainty quantification.</description>
      <author>example@mail.com (Darya Taratynova, Alya Almsouti, Beknur Kalmakhanbet, Numan Saeed, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2508.15298v2</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever</title>
      <link>http://arxiv.org/abs/2508.14323v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种行为对齐的检索器（BAR），用于提高工具增强型大型语言模型（LLMs）的函数调用准确性，减少错误调用并降低成本。&lt;h4&gt;背景&lt;/h4&gt;工具增强型大型语言模型利用外部函数扩展能力，但不准确的函数调用会导致效率低下和成本增加。&lt;h4&gt;目的&lt;/h4&gt;训练一个行为对齐的检索器（BAR），为LLMs提供行为一致的演示，帮助模型做出更准确的工具使用决策。&lt;h4&gt;方法&lt;/h4&gt;构建包含不同函数调用行为的语料库，使用对比学习框架训练BAR，采用自定义的正/负对比对和双负对比损失，确保行为一致例子的稳健检索。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该方法显著减少了错误的函数调用，同时保持高任务性能，为工具增强型LLMs提供了一种成本效益高且高效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;行为对齐的检索器（BAR）能够有效解决工具增强型LLMs中函数调用不准确的问题，是一种成本效益高且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;工具增强型大型语言模型利用外部函数扩展其能力，但不准确的函数调用会导致效率低下和成本增加。现有方法通过微调LLMs或使用基于演示的提示来解决这个问题，但它们通常存在高训练开销的问题，且无法处理不一致的演示样本，这些样本会误导模型的调用行为。本文训练了一个行为对齐的检索器（BAR），为LLMs提供行为一致的演示，帮助模型做出更准确的工具使用决策。为训练BAR，我们构建了一个包含不同函数调用行为（即调用或非调用）的语料库。我们使用对比学习框架，通过自定义的正/负对比对和双负对比损失来训练BAR，确保行为一致例子的稳健检索。实验证明，我们的方法显著减少了错误的函数调用，同时保持高任务性能，为工具增强型LLMs提供了一种成本效益高且高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tool-augmented large language models (LLMs) leverage external functions toextend their capabilities, but inaccurate function calls can lead toinefficiencies and increased costs.Existing methods address this challenge byfine-tuning LLMs or using demonstration-based prompting, yet they often sufferfrom high training overhead and fail to account for inconsistent demonstrationsamples, which misguide the model's invocation behavior. In this paper, wetrained a behavior-aligned retriever (BAR), which provides behaviorallyconsistent demonstrations to help LLMs make more accurate tool-using decisions.To train the BAR, we construct a corpus including different function-callingbehaviors, i.e., calling or non-calling.We use the contrastive learningframework to train the BAR with customized positive/negative pairs and adual-negative contrastive loss, ensuring robust retrieval of behaviorallyconsistent examples.Experiments demonstrate that our approach significantlyreduces erroneous function calls while maintaining high task performance,offering a cost-effective and efficient solution for tool-augmented LLMs.</description>
      <author>example@mail.com (Yixin Chen, Ying Xiong, Shangyu Wu, Yufei Cui, Xue Liu, Nan Guan, Chun Jason Xue)</author>
      <guid isPermaLink="false">2508.14323v2</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops</title>
      <link>http://arxiv.org/abs/2508.17932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人类视频理解展示了推理和视觉注意力之间的动态协调，自适应地关注与查询相关的细节。CAVIA框架通过推理和感知的协调解决了当前视频问答系统的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前长视频问答系统采用僵化的流水线，将推理与感知分离，导致信息损失或计算效率低下。核心限制是无法根据特定的推理需求自适应地调整视觉提取。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为CAVIA的框架，通过推理与感知的协调来革新视频理解。&lt;h4&gt;方法&lt;/h4&gt;CAVIA是一个无需训练的框架，创建了一个闭环系统，其中推理基于识别出的信息差距持续指导视觉提取。CAVIA引入了三个创新：分层推理，引导精确定位到特定帧；跨模态语义桥接，用于有针对性的提取；基于置信度的迭代合成。&lt;h4&gt;主要发现&lt;/h4&gt;CAVIA在具有挑战性的基准测试上取得了最先进的性能：EgoSchema(65.7%，+5.3%)，NExT-QA(76.1%，+2.6%)，IntentQA(73.8%，+6.9%)。&lt;h4&gt;结论&lt;/h4&gt;动态推理-感知协调为视频理解提供了一种可扩展的范式。&lt;h4&gt;翻译&lt;/h4&gt;人类视频理解展示了推理和视觉注意力之间的动态协调，自适应地关注与查询相关的细节。然而，当前长视频问答系统采用僵化的流水线，将推理与感知分离，导致通过过早的视觉抽象造成信息损失，或通过详尽处理导致计算效率低下。核心限制在于无法根据特定推理需求自适应地调整视觉提取，不同的查询从根本上需要从同一视频内容中获取不同的视觉证据。在这项工作中，我们提出了CAVIA，这是一个无需训练的框架，通过推理和感知的协调革新视频理解。与传统推理独立于视觉处理的方法不同，CAVIA创建了一个闭环系统，其中推理基于识别出的信息差距持续指导视觉提取。CAVIA引入了三个创新：(1)分层推理，引导精确定位到特定帧；(2)跨模态语义桥接，用于有针对性的提取；(3)基于置信度的迭代合成。CAVIA在具有挑战性的基准测试上取得了最先进的性能：EgoSchema(65.7%，+5.3%)，NExT-QA(76.1%，+2.6%)，以及IntentQA(73.8%，+6.9%)，证明了动态推理-感知协调为视频理解提供了可扩展的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human video comprehension demonstrates dynamic coordination between reasoningand visual attention, adaptively focusing on query-relevant details. However,current long-form video question answering systems employ rigid pipelines thatdecouple reasoning from perception, leading to either information loss throughpremature visual abstraction or computational inefficiency through exhaustiveprocessing. The core limitation lies in the inability to adapt visualextraction to specific reasoning requirements, different queries demandfundamentally different visual evidence from the same video content. In thiswork, we present CAVIA, a training-free framework that revolutionizes videounderstanding through reasoning, perception coordination. Unlike conventionalapproaches where visual processing operates independently of reasoning, CAVIAcreates a closed-loop system where reasoning continuously guides visualextraction based on identified information gaps. CAVIA introduces threeinnovations: (1) hierarchical reasoning, guided localization to precise frames;(2) cross-modal semantic bridging for targeted extraction; (3)confidence-driven iterative synthesis. CAVIA achieves state-of-the-artperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamicreasoning-perception coordination provides a scalable paradigm for videounderstanding.</description>
      <author>example@mail.com (Zixuan Dong, Baoyun Peng, Yufei Wang, Lin Liu, Xinxin Dong, Yunlong Cao, Xiaodong Wang)</author>
      <guid isPermaLink="false">2508.17932v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration</title>
      <link>http://arxiv.org/abs/2508.17817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合时间建模与视觉-语义协作的视频融合框架，解决了现有方法忽略时间依赖性的问题，确保了视觉保真度、语义准确性和时间一致性。&lt;h4&gt;背景&lt;/h4&gt;现有多模态融合方法通常将静态帧图像融合技术直接应用于视频融合任务，忽略了固有的时间依赖性，导致帧间结果不一致。&lt;h4&gt;目的&lt;/h4&gt;提出一个明确结合时间建模与视觉-语义协作的视频融合框架，同时确保视觉保真度、语义准确性和时间一致性。&lt;h4&gt;方法&lt;/h4&gt;1) 引入视觉-语义交互模块，包含语义分支和视觉分支，使用Dinov2和VGG19进行目标蒸馏；2) 将视频退化增强任务集成到视频融合流程中，构建时间协作模块；3) 嵌入时间增强机制到网络中，并设计时间损失指导优化；4) 引入两种专为视频融合设计的创新评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;在公共视频数据集上的大量实验结果证明了该方法的优越性。&lt;h4&gt;结论&lt;/h4&gt;该研究通过视觉-语义协作和时间建模，有效解决了视频融合中的时间一致性问题，提高了融合效果。&lt;h4&gt;翻译&lt;/h4&gt;现有的多模态融合方法通常将静态帧图像融合技术直接应用于视频融合任务，忽略了固有的时间依赖性，导致帧间结果不一致。为解决这一局限，我们提出了首个明确结合时间建模与视觉-语义协作的视频融合框架，同时确保视觉保真度、语义准确性和时间一致性。首先，我们引入了包含语义分支和视觉分支的视觉-语义交互模块，使用Dinov2和VGG19进行目标蒸馏，同时增强视觉和语义表示。其次，我们首次将视频退化增强任务集成到视频融合流程中，通过构建时间协作模块，利用时间依赖性促进弱信息恢复。第三，为确保时间一致性，我们在网络中嵌入时间增强机制，并设计了时间损失来指导优化过程。最后，我们引入了两种专为视频融合设计的创新评估指标，用于评估生成融合视频的时间一致性。公共视频数据集上的大量实验结果证明了我们方法的优越性。我们的代码已在https://github.com/Meiqi-Gong/TemCoCo发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing multi-modal fusion methods typically apply static frame-based imagefusion techniques directly to video fusion tasks, neglecting inherent temporaldependencies and leading to inconsistent results across frames. To address thislimitation, we propose the first video fusion framework that explicitlyincorporates temporal modeling with visual-semantic collaboration tosimultaneously ensure visual fidelity, semantic accuracy, and temporalconsistency. First, we introduce a visual-semantic interaction moduleconsisting of a semantic branch and a visual branch, with Dinov2 and VGG19employed for targeted distillation, allowing simultaneous enhancement of boththe visual and semantic representations. Second, we pioneer integrate the videodegradation enhancement task into the video fusion pipeline by constructing atemporal cooperative module, which leverages temporal dependencies tofacilitate weak information recovery. Third, to ensure temporal consistency, weembed a temporal-enhanced mechanism into the network and devise a temporal lossto guide the optimization process. Finally, we introduce two innovativeevaluation metrics tailored for video fusion, aimed at assessing the temporalconsistency of the generated fused videos. Extensive experimental results onpublic video datasets demonstrate the superiority of our method. Our code isreleased at https://github.com/Meiqi-Gong/TemCoCo.</description>
      <author>example@mail.com (Meiqi Gong, Hao Zhang, Xunpeng Yi, Linfeng Tang, Jiayi Ma)</author>
      <guid isPermaLink="false">2508.17817v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing</title>
      <link>http://arxiv.org/abs/2508.17686v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出Language-Guided Temporal Token Pruning (LGTTP)方法，通过利用查询中的时间线索自适应修剪视频标记，解决了视觉语言模型处理长视频时的计算效率问题。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在处理长视频时面临挑战，主要原因是注意力机制的复杂度是二次方的，导致计算负担过重。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够减少视觉语言模型处理长视频时计算开销的方法，同时保持模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出语言引导的时间标记修剪方法，利用查询中的时间线索自适应修剪视频标记，保留上下文连续性，减少计算开销。与传统均匀修剪或关键帧选择不同，该方法能在时间相关段保留更高的标记密度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法与TimeChat和LLaVA-Video集成后，实现了65%的计算量减少，同时保留了97-99%的原始性能。在QVHighlights数据集上，HIT@1指标提高了9.5%；在Charades-STA数据集上，保留了99.6%的R@1指标。该方法在带有明确时间标记的查询上表现出色，并在各种视频理解任务中保持有效。&lt;h4&gt;结论&lt;/h4&gt;LGTTP是一种有效的模型无关框架，能够显著减少视觉语言模型处理长视频时的计算量，同时保持高水平性能，适用于各种视频理解任务。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在处理长视频时因注意力机制的二次复杂度而面临挑战。我们提出语言引导的时间标记修剪方法，利用查询中的时间线索自适应修剪视频标记，在保持上下文连续性的同时减少计算开销。与均匀修剪或关键帧选择不同，LGTTP在时间相关段保留更高的标记密度。我们的模型无关框架与TimeChat和LLaVA-Video集成，实现了65%的计算量减少，同时保留了97-99%的原始性能。在QVHighlights上，LGTTP将HIT@1提高了9.5%，在Charades-STA上保留了99.6%的R@1。它在带有明确时间标记的查询上表现出色，并在各种视频理解任务中保持有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) struggle with long-form videos due to thequadratic complexity of attention mechanisms. We propose Language-GuidedTemporal Token Pruning (LGTTP), which leverages temporal cues from queries toadaptively prune video tokens, preserving contextual continuity while reducingcomputational overhead. Unlike uniform pruning or keyframe selection, LGTTPretains higher token density in temporally relevant segments. Ourmodel-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a65% reduction in computation while preserving 97-99% of the originalperformance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and onCharades-STA, it retains 99.6% of R@1. It excels on queries with explicittemporal markers and remains effective across general video understandingtasks.</description>
      <author>example@mail.com (Yogesh Kumar)</author>
      <guid isPermaLink="false">2508.17686v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat</title>
      <link>http://arxiv.org/abs/2508.17378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文对阿拉伯语专用大语言模型ALLaM-34B进行了UI级别的扩展和精细评估，该模型由沙特数据与人工智能局推出，并由HUMAIN公司开发为HUMAIN Chat服务。研究通过多维度测试评估了模型在阿拉伯语处理方面的能力，结果显示该模型在生成、代码切换、标准阿拉伯语处理、推理能力和方言保真度等方面表现出色，是一个技术上强大且文化适应性强的阿拉伯语大语言模型。&lt;h4&gt;背景&lt;/h4&gt;主要训练英语语料库的大语言模型通常难以捕捉阿拉伯语的语言和文化细微差别。为解决这一差距，沙特数据与人工智能局(SDAIA)推出了专注于阿拉伯语的ALLaM模型系列。其中能力最强且对公众开放的ALLaM-34B模型随后被HUMAIN公司采用，并基于此模型开发并部署了HUMAIN Chat，一个封闭式对话网络服务。&lt;h4&gt;目的&lt;/h4&gt;对ALLaM-34B模型进行扩展和精细的UI级别评估，全面了解其在阿拉伯语处理方面的能力和局限性。&lt;h4&gt;方法&lt;/h4&gt;研究使用包含现代标准阿拉伯语、五种区域方言、代码切换、事实知识、算术和时序推理、创意生成以及对抗性安全的提示包，收集了115个输出（23个提示×5次运行）。每个输出由三个前沿大语言模型评判员（GPT-5、Gemini 2.5 Pro、Claude Sonnet-4）进行评分。研究计算了类别级别的平均值和95%置信区间，分析了分数分布，并可视化了方言级别的指标热图。&lt;h4&gt;主要发现&lt;/h4&gt;生成和代码切换任务表现一致出色（平均4.92/5）；现代标准阿拉伯语处理能力强（4.74/5）；推理能力扎实（4.64/5）；方言保真度提高（4.21/5）；安全相关提示表现出稳定可靠性能（4.54/5）。&lt;h4&gt;结论&lt;/h4&gt;综合这些结果，ALLaM-34B被定位为一个强大且具有文化根基的阿拉伯语大语言模型，展示了技术实力和实际部署的成熟度，适合现实世界的应用。&lt;h4&gt;翻译&lt;/h4&gt;主要训练英语语料库的大语言模型通常难以捕捉阿拉伯语的语言和文化细微差别。为解决这一差距，沙特数据与人工智能局(SDAIA)推出了专注于阿拉伯语的ALLaM模型系列。其中能力最强且对公众开放的ALLaM-34B模型随后被HUMAIN公司采用，并基于此模型开发并部署了HUMAIN Chat，一个封闭式对话网络服务。本文提出了对ALLaM-34B的扩展和精细的UI级别评估。使用涵盖现代标准阿拉伯语、五种区域方言、代码切换、事实知识、算术和时序推理、创意生成以及对抗性安全的提示包，我们收集了115个输出（23个提示×5次运行），并由三个前沿大语言模型评判员（GPT-5、Gemini 2.5 Pro、Claude Sonnet-4）对每个输出进行评分。我们计算了类别级别的平均值和95%置信区间，分析了分数分布，并可视化了方言级别的指标热图。更新的分析显示，在生成和代码切换任务上表现一致出色（平均4.92/5），在现代标准阿拉伯语处理方面取得强劲结果（4.74/5），推理能力扎实（4.64/5），方言保真度提高（4.21/5）。安全相关提示表现出稳定可靠的性能（4.54/5）。综合这些结果，ALLaM-34B被定位为一个强大且具有文化根基的阿拉伯语大语言模型，展示了技术实力和实际部署的成熟度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) trained primarily on English corpora oftenstruggle to capture the linguistic and cultural nuances of Arabic. To addressthis gap, the Saudi Data and AI Authority (SDAIA) introduced the $ALLaM$ familyof Arabic-focused models. The most capable of these available to the public,$ALLaM-34B$, was subsequently adopted by HUMAIN, who developed and deployedHUMAIN Chat, a closed conversational web service built on this model. Thispaper presents an expanded and refined UI-level evaluation of $ALLaM-34B$.Using a prompt pack spanning modern standard Arabic, five regional dialects,code-switching, factual knowledge, arithmetic and temporal reasoning, creativegeneration, and adversarial safety, we collected 115 outputs (23 prompts times5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro,Claude Sonnet-4). We compute category-level means with 95\% confidenceintervals, analyze score distributions, and visualize dialect-wise metric heatmaps. The updated analysis reveals consistently high performance on generationand code-switching tasks (both averaging 4.92/5), alongside strong results inMSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialectfidelity (4.21/5). Safety-related prompts show stable, reliable performance of(4.54/5). Taken together, these results position $ALLaM-34B$ as a robust andculturally grounded Arabic LLM, demonstrating both technical strength andpractical readiness for real-world deployment.</description>
      <author>example@mail.com (Omer Nacar)</author>
      <guid isPermaLink="false">2508.17378v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services</title>
      <link>http://arxiv.org/abs/2508.14503v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer架构的异常检测方法，通过集成多尺度特征感知能力，有效解决了云服务环境中时间建模和尺度感知特征表示的局限性，在多项指标上优于主流基线模型。&lt;h4&gt;背景&lt;/h4&gt;云服务环境中存在时间建模和尺度感知特征表示的局限性，传统方法难以有效处理高维监控数据中的异常模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉长程依赖和上下文语义，并能从不同粒度提取时间特征的异常检测方法，提高在复杂云环境中的检测性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用改进的Transformer模块进行时间建模，利用自注意力机制捕获长程依赖和上下文语义；引入多尺度特征构建路径，通过下采样和并行编码提取不同粒度的时间特征；设计注意力加权融合模块动态调整各尺度对最终决策的贡献；在输入建模阶段构建标准化多维时间序列，包括CPU利用率、内存使用量和任务调度状态等核心信号，并使用位置编码增强模型的时间感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在精确度、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持强大的稳定性和检测性能，证明了其在复杂云环境中的优越能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于Transformer架构的异常检测方法通过集成多尺度特征感知能力，有效解决了云服务环境中时间建模和尺度感知特征表示的局限性，在复杂云环境中表现出优越的异常检测能力。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种基于Transformer架构的异常检测方法，集成了多尺度特征感知能力，旨在解决云服务环境中时间建模和尺度感知特征表示的局限性。该方法首先采用改进的Transformer模块对高维监控数据进行时间建模，利用自注意力机制捕获长程依赖和上下文语义。然后，引入多尺度特征构建路径，通过下采样和并行编码提取不同粒度的时间特征。设计了一个注意力加权融合模块，动态调整各尺度对最终决策的贡献，增强模型对异常模式的建模鲁棒性。在输入建模阶段，构建了标准化的多维时间序列，涵盖CPU利用率、内存使用量和任务调度状态等核心信号，同时使用位置编码增强模型的时间感知能力。设计了系统的实验设置来评估性能，包括比较实验和超参数敏感性分析，重点关注优化器、学习率、异常比例和噪声水平的影响。实验结果表明，该方法在精确度、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持强大的稳定性和检测性能，证明了其在复杂云环境中的优越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study proposes an anomaly detection method based on the Transformerarchitecture with integrated multiscale feature perception, aiming to addressthe limitations of temporal modeling and scale-aware feature representation incloud service environments. The method first employs an improved Transformermodule to perform temporal modeling on high-dimensional monitoring data, usinga self-attention mechanism to capture long-range dependencies and contextualsemantics. Then, a multiscale feature construction path is introduced toextract temporal features at different granularities through downsampling andparallel encoding. An attention-weighted fusion module is designed todynamically adjust the contribution of each scale to the final decision,enhancing the model's robustness in anomaly pattern modeling. In the inputmodeling stage, standardized multidimensional time series are constructed,covering core signals such as CPU utilization, memory usage, and taskscheduling states, while positional encoding is used to strengthen the model'stemporal awareness. A systematic experimental setup is designed to evaluateperformance, including comparative experiments and hyperparameter sensitivityanalysis, focusing on the impact of optimizers, learning rates, anomaly ratios,and noise levels. Experimental results show that the proposed methodoutperforms mainstream baseline models in key metrics, including precision,recall, AUC, and F1-score, and maintains strong stability and detectionperformance under various perturbation conditions, demonstrating its superiorcapability in complex cloud environments.</description>
      <author>example@mail.com (Lian Lian, Yilin Li, Song Han, Renzi Meng, Sibo Wang, Ming Wang)</author>
      <guid isPermaLink="false">2508.14503v2</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding</title>
      <link>http://arxiv.org/abs/2508.12687v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EgoIllusion基准，用于评估多模态大型语言模型在第一人称视频中的幻觉表现，通过1400个视频和8000个人类标注的问题对十个MLLMs进行了测试，发现即使是强大模型如GPT-4o和Gemini也仅达到59%的准确率。&lt;h4&gt;背景&lt;/h4&gt;多模态大型语言模型在复杂多模态任务中表现出色，尤其在第三人称和第一人称视频的视觉感知和推理方面，但这些模型容易产生幻觉，生成连贯但不准确的回答。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准来评估多模态大型语言模型在第一人称视频中的幻觉表现，以促进开发具有更低幻觉率的第一人称多模态大型语言模型。&lt;h4&gt;方法&lt;/h4&gt;创建了名为EgoIllusion的基准，包含1400个视频和8000个人类标注的开放式和封闭式问题，这些设计旨在触发第一人称视频中视觉和听觉线索的幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;对十个多模态大型语言模型的评估显示存在重大挑战，包括像GPT-4o和Gemini这样的强大模型仅达到59%的准确率，表明这些模型在处理第一人称视频时容易产生幻觉。&lt;h4&gt;结论&lt;/h4&gt;EgoIllusion为开发评估多模态大型语言模型有效性的稳健基准奠定了基础，并促进了具有更低幻觉率的第一人称多模态大型语言模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;多模态大型语言模型在复杂多模态任务中表现出色。虽然MLLMs在第三人称和第一人称视频的视觉感知和推理方面表现出色，但它们容易产生幻觉，生成连贯但不准确的回答。我们提出了EgoIllusion，这是第一个评估MLLM在第一人称视频中幻觉表现的基准。EgoIllusion包含1400个视频和8000个人类标注的开放式和封闭式问题，旨在触发第一人称视频中视觉和听觉线索的幻觉。对十个MLLMs的评估显示存在重大挑战，包括像GPT-4o和Gemini这样的强大模型仅达到59%的准确率。EgoIllusion为开发评估MLLMs有效性的稳健基准奠定了基础，并促进了具有更低幻觉率的第一人称MLLMs的发展。我们的基准将被开源以确保可复现性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated remarkableperformance in complex multimodal tasks. While MLLMs excel at visual perceptionand reasoning in third-person and egocentric videos, they are prone tohallucinations, generating coherent yet inaccurate responses. We presentEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentricvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotatedopen and closed-ended questions designed to trigger hallucinations in bothvisual and auditory cues in egocentric videos. Evaluations across ten MLLMsreveal significant challenges, including powerful models like GPT-4o andGemini, achieving only 59% accuracy. EgoIllusion lays the foundation indeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spursthe development of better egocentric MLLMs with reduced hallucination rates.Our benchmark will be open-sourced for reproducibility.</description>
      <author>example@mail.com (Ashish Seth, Utkarsh Tyagi, Ramaneswaran Selvakumar, Nishit Anand, Sonal Kumar, Sreyan Ghosh, Ramani Duraiswami, Chirag Agarwal, Dinesh Manocha)</author>
      <guid isPermaLink="false">2508.12687v2</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction</title>
      <link>http://arxiv.org/abs/2508.16623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出RAST框架，结合检索增强机制与时空建模，解决交通预测中上下文容量有限和细粒度点预测性低的问题，在六个真实交通网络上实现了优越性能。&lt;h4&gt;背景&lt;/h4&gt;交通预测是现代智能交通系统的基石和时空预测的关键任务。尽管先进的时空图神经网络和预训练模型已取得显著进展，但仍面临两个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决交通预测中两个关键挑战：(i)建模复杂时空依赖关系时上下文容量有限；(ii)由于异构模式导致细粒度时空点预测性低。&lt;h4&gt;方法&lt;/h4&gt;受检索增强生成(RAG)启发，提出RAST框架，包含三个关键设计：1)解耦编码器和查询生成器捕获解耦时空特征并通过残差融合构建查询；2)时空检索存储器和检索器维护和检索向量化细粒度模式；3)通用主干预测器灵活适应预训练STGNNs或简单MLP预测器。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实交通网络(包括大规模数据集)上的大量实验表明，RAST在保持计算效率的同时实现了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;RAST框架成功整合了检索增强机制与时空建模，有效解决了交通预测中的关键挑战，为未来研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;交通预测是现代智能交通系统的基石和时空预测中的关键任务。尽管先进的时空图神经网络和预训练模型在交通预测方面取得了显著进展，但仍存在两个关键挑战：(i)在建模复杂的时空依赖关系时上下文容量有限；(ii)由于异构模式的存在，在细粒度的时空点上预测性较低。受检索增强生成(RAG)的启发，我们提出了RAST，一个集成检索增强机制与时空建模的通用框架，以解决这些挑战。我们的框架包含三个关键设计：1)解耦编码器和查询生成器，用于捕获解耦的空间和时间特征，并通过残差融合构建融合查询；2)时空检索存储器和检索器，用于维护和检索向量化细粒度模式；3)通用主干预测器，灵活适应预训练的STGNNs或简单的MLP预测器。在六个真实交通网络(包括大规模数据集)上的大量实验表明，RAST在保持计算效率的同时实现了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic prediction is a cornerstone of modern intelligent transportationsystems and a critical task in spatio-temporal forecasting. Although advancedSpatio-temporal Graph Neural Networks (STGNNs) and pre-trained models haveachieved significant progress in traffic prediction, two key challenges remain:(i) limited contextual capacity when modeling complex spatio-temporaldependencies, and (ii) low predictability at fine-grained spatio-temporalpoints due to heterogeneous patterns. Inspired by Retrieval-AugmentedGeneration (RAG), we propose RAST, a universal framework that integratesretrieval-augmented mechanisms with spatio-temporal modeling to address thesechallenges. Our framework consists of three key designs: 1) Decoupled Encoderand Query Generator to capture decoupled spatial and temporal features andconstruct a fusion query via residual fusion; 2) Spatio-temporal RetrievalStore and Retrievers to maintain and retrieve vectorized fine-grained patterns;and 3) Universal Backbone Predictor that flexibly accommodates pre-trainedSTGNNs or simple MLP predictors. Extensive experiments on six real-worldtraffic networks, including large-scale datasets, demonstrate that RASTachieves superior performance while maintaining computational efficiency.</description>
      <author>example@mail.com (Weilin Ruan, Xilin Dang, Ziyu Zhou, Sisuo Lyu, Yuxuan Liang)</author>
      <guid isPermaLink="false">2508.16623v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Quantum-Classical Hybrid Framework for Zero-Day Time-Push GNSS Spoofing Detection</title>
      <link>http://arxiv.org/abs/2508.18085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE Internet of Things Journal  for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种混合量子经典自编码器(HQC-AE)方法，用于检测全球导航卫星系统(GNSS)中的零日时间推欺骗攻击，无需预先接触欺骗数据即可实现高准确率检测。&lt;h4&gt;背景&lt;/h4&gt;全球导航卫星系统(GNSS)对定位、导航和授时(PNT)应用至关重要，但极易受到欺骗攻击，此类攻击可导致错误导航、数据完整性受损和运营中断。现有欺骗检测方法大多依赖监督学习技术，难以检测新颖、进化和未见的攻击。&lt;h4&gt;目的&lt;/h4&gt;开发一种零日欺骗检测方法，用于检测GNSS时间推欺骗攻击，解决现有方法无法检测新型攻击的问题。&lt;h4&gt;方法&lt;/h4&gt;开发混合量子经典自编码器(HQC-AE)，仅使用真实GNSS信号进行训练，不接触欺骗数据。利用跟踪阶段提取的特征，在PNT解决方案计算前进行主动检测。专注于静态GNSS接收器中的欺骗检测，这些接收器特别容易受到时间推欺骗攻击。&lt;h4&gt;主要发现&lt;/h4&gt;HQC-AE在检测零日、未见的GNSS时间推欺骗攻击方面，持续优于其经典对应物、传统监督学习模型和现有无监督学习方法，平均检测准确率为97.71%，平均假阴性率为0.62%。对于复杂的欺骗攻击，HQC-AE达到98.23%的准确率，假阴性率为1.85%。&lt;h4&gt;结论&lt;/h4&gt;该方法在检测各种静止GNSS接收器平台上的零日GNSS时间推欺骗攻击方面有效。&lt;h4&gt;翻译&lt;/h4&gt;全球导航卫星系统(GNSS)对定位、导航和授时(PNT)应用至关重要。然而，GNSS极易受到欺骗攻击，攻击者发送伪造信号误导接收器。此类攻击可导致严重后果，包括错误导航、数据完整性受损和运营中断。大多数现有欺骗检测方法依赖监督学习技术，难以检测新颖、进化和未见的攻击。为克服这一限制，我们开发了一种零日欺骗检测方法，使用混合量子经典自编码器(HQC-AE)，仅使用真实GNSS信号进行训练，不接触欺骗数据。通过利用跟踪阶段提取的特征，我们的方法能够在PNT解决方案计算前进行主动检测。我们专注于静态GNSS接收器中的欺骗检测，这些接收器特别容易受到时间推欺骗攻击，攻击者操纵时间信息以导致接收器错误计算时间。我们针对不同的未见时间推欺骗攻击场景评估了我们的模型：简单、中等和复杂。我们的分析表明，HQC-AE在检测零日、未见的GNSS时间推欺骗攻击方面，持续优于其经典对应物、传统监督学习模型和现有无监督学习方法，平均检测准确率为97.71%，平均假阴性率为0.62%（当攻击发生但未被检测时）。对于复杂的欺骗攻击，HQC-AE达到98.23%的准确率，假阴性率为1.85%。这些发现突显了我们的方法在主动检测各种静止GNSS接收器平台上的零日GNSS时间推欺骗攻击方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global Navigation Satellite Systems (GNSS) are critical for Positioning,Navigation, and Timing (PNT) applications. However, GNSS are highly vulnerableto spoofing attacks, where adversaries transmit counterfeit signals to misleadreceivers. Such attacks can lead to severe consequences, including misdirectednavigation, compromised data integrity, and operational disruptions. Mostexisting spoofing detection methods depend on supervised learning techniquesand struggle to detect novel, evolved, and unseen attacks. To overcome thislimitation, we develop a zero-day spoofing detection method using a HybridQuantum-Classical Autoencoder (HQC-AE), trained solely on authentic GNSSsignals without exposure to spoofed data. By leveraging features extractedduring the tracking stage, our method enables proactive detection before PNTsolutions are computed. We focus on spoofing detection in static GNSSreceivers, which are particularly susceptible to time-push spoofing attacks,where attackers manipulate timing information to induce incorrect timecomputations at the receiver. We evaluate our model against different unseentime-push spoofing attack scenarios: simplistic, intermediate, andsophisticated. Our analysis demonstrates that the HQC-AE consistentlyoutperforms its classical counterpart, traditional supervised learning-basedmodels, and existing unsupervised learning-based methods in detecting zero-day,unseen GNSS time-push spoofing attacks, achieving an average detection accuracyof 97.71% with an average false negative rate of 0.62% (when an attack occursbut is not detected). For sophisticated spoofing attacks, the HQC-AE attains anaccuracy of 98.23% with a false negative rate of 1.85%. These findingshighlight the effectiveness of our method in proactively detecting zero-dayGNSS time-push spoofing attacks across various stationary GNSS receiverplatforms.</description>
      <author>example@mail.com (Abyad Enan, Mashrur Chowdhury, Sagar Dasgupta, Mizanur Rahman)</author>
      <guid isPermaLink="false">2508.18085v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation</title>
      <link>http://arxiv.org/abs/2508.17926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何利用单一大型语言模型执行一个或多个论证挖掘任务，通过构建多任务数据集和测试不同的训练策略，发现任务特定微调能显著提高性能，多任务微调保持良好性能且具有迁移学习能力，模型合并是计算效率高的替代方案。&lt;h4&gt;背景&lt;/h4&gt;论证挖掘是论证学的一个子领域，旨在从自然语言文本中自动提取论证结构及其关系。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用单一大型语言模型执行一个或多个论证挖掘任务。&lt;h4&gt;方法&lt;/h4&gt;首先，通过调查和转换文献中的19个知名论证挖掘数据集，构建了一个统一格式的多任务数据集；其次，使用Meta AI的Llama-3.1-8B-Instruct模型探索了三种训练策略：(1)单个任务微调，(2)多任务联合微调，(3)合并单独针对单个任务微调的模型。&lt;h4&gt;主要发现&lt;/h4&gt;任务特定的微调显著提高了所有任务的个体性能；多任务微调保持了强大的性能而没有下降，表明相关任务间有效的迁移学习；模型合并提供了具有竞争力的性能，同时减轻了与完整多任务微调相关的计算成本。&lt;h4&gt;结论&lt;/h4&gt;单一大型语言模型可以有效执行一个或多个论证挖掘任务，任务特定微调、多任务微调和模型合并各有优势，可根据具体需求选择合适的方法。&lt;h4&gt;翻译&lt;/h4&gt;论证挖掘是论证学的一个子领域，旨在从自然语言文本中自动提取论证结构及其关系。本文研究了如何利用单一大型语言模型执行一个或多个论证挖掘任务。我们的贡献有两方面。首先，我们通过调查和转换文献中的19个知名论证挖掘数据集，构建了一个多任务数据集，并将其统一为相同格式。其次，我们使用Meta AI的Llama-3.1-8B-Instruct模型探索了各种训练策略：(1)单个任务微调，(2)多任务联合微调，(3)合并单独针对单个任务微调的模型。我们的实验表明，任务特定的微调显著提高了所有任务的个体性能。此外，多任务微调保持了强大的性能而没有下降，表明相关任务间有效的迁移学习。最后，我们证明了模型合并是一种可行的折中方案：它产生了具有竞争力的性能，同时减轻了与完整多任务微调相关的计算成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Argument mining is a subfield of argumentation that aims to automaticallyextract argumentative structures and their relations from natural languagetexts. This paper investigates how a single large language model can beleveraged to perform one or several argument mining tasks. Our contributionsare two-fold. First, we construct a multi-task dataset by surveying andconverting 19 well-known argument mining datasets from the literature into aunified format. Second, we explore various training strategies using Meta AI'sLlama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2)fine-tuning jointly on multiple tasks, and (3) merging models fine-tunedseparately on individual tasks. Our experiments show that task-specificfine-tuning significantly improves individual performance across all tasks.Moreover, multi-task fine-tuning maintains strong performance withoutdegradation, suggesting effective transfer learning across related tasks.Finally, we demonstrate that model merging offers a viable compromise: ityields competitive performance while mitigating the computational costsassociated with full multi-task fine-tuning.</description>
      <author>example@mail.com (Henri Savigny, Bruno Yun)</author>
      <guid isPermaLink="false">2508.17926v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Sketchpose: Learning to Segment Cells with Partial Annotations</title>
      <link>http://arxiv.org/abs/2508.17798v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the Journal of Machine Learning for  Biomedical Imaging (MELBA) https://melba-journal.org/2025:016&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的细胞分割方法，它保留了基于距离图的准确性，同时减少了对完全注释数据集的依赖，能够在不降低分割质量的情况下节省大量时间和资源。&lt;h4&gt;背景&lt;/h4&gt;最流行的细胞分割网络（如Cellpose、Stardist、HoverNet等）依赖于距离图的预测，虽然取得了前所未有的准确性，但完全依赖于完全注释的数据集，这在生成训练集和进行迁移学习方面是一个严重的限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种仍然依赖距离图但能够处理部分注释对象的方法，并评估其在节俭学习、迁移学习和常规学习环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;开发一种基于距离图的新方法，能够处理部分注释的对象，并将该算法嵌入到一个用户友好的Napari插件中。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法可以在不牺牲分割质量的情况下，显著节省时间和资源。&lt;h4&gt;结论&lt;/h4&gt;该方法在节俭学习、迁移学习和常规学习环境中都能有效工作，算法已嵌入到用户友好的Napari插件中，便于实际应用。&lt;h4&gt;翻译&lt;/h4&gt;最流行的用于细胞分割的网络（例如Cellpose、Stardist、HoverNet等）依赖于距离图的预测。它带来了前所未有的准确性，但依赖于完全注释的数据集。这对于生成训练集和执行迁移学习是一个严重的限制。在本文中，我们提出了一种仍然依赖距离图并处理部分注释对象的方法。我们在节俭学习、迁移学习和常规数据库上的常规学习背景下评估了所提出方法的性能。我们的实验表明，它可以在不牺牲分割质量的情况下，显著节省时间和资源。所提出的算法嵌入在一个用户友好的Napari插件中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.59275/j.melba.2025-f7b3&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The most popular networks used for cell segmentation (e.g. Cellpose,Stardist, HoverNet,...) rely on a prediction of a distance map. It yieldsunprecedented accuracy but hinges on fully annotated datasets. This is aserious limitation to generate training sets and perform transfer learning. Inthis paper, we propose a method that still relies on the distance map andhandles partially annotated objects. We evaluate the performance of theproposed approach in the contexts of frugal learning, transfer learning andregular learning on regular databases. Our experiments show that it can lead tosubstantial savings in time and resources without sacrificing segmentationquality. The proposed algorithm is embedded in a user-friendly Napari plugin.</description>
      <author>example@mail.com (Clément Cazorla, Nathanaël Munier, Renaud Morin, Pierre Weiss)</author>
      <guid isPermaLink="false">2508.17798v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph</title>
      <link>http://arxiv.org/abs/2508.17645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了3D人工智能生成内容与人类设计范式之间的表示不兼容问题，提出了一种生成设计操作序列的方法，实现了AI与人类设计工作流程的有效融合。&lt;h4&gt;背景&lt;/h4&gt;3D人工智能生成内容(3D-AIGC)虽能快速合成复杂几何形状，但AI框架主要处理网格或神经表示，而设计师使用参数化建模工具，两者之间的不兼容性降低了AI在3D行业的实际应用价值。&lt;h4&gt;目的&lt;/h4&gt;解决AI生成内容与人类中心设计范式之间的脱节，专注于生成设计操作序列，使其与现代3D软件中设计师的典型工作流程保持一致。&lt;h4&gt;方法&lt;/h4&gt;将基本建模操作重新表述为可微分单元，通过基于梯度的学习联合优化连续和离散参数；构建具有门控机制的层次图，通过最小化Chamfer Distance进行端到端优化；利用多阶段序列长度约束和领域规则惩罚实现无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;生成的操作序列实现了高几何保真度、平滑的网格布线、合理的步骤组合和灵活的编辑能力，与设计行业完全兼容。&lt;h4&gt;结论&lt;/h4&gt;该方法成功弥合了AI生成内容与人类设计工作流程之间的差距，提高了3D行业中人机协作的效率，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;3D人工智能生成内容(3D-AIGC)的出现使得复杂几何形状的快速合成成为可能。然而，AI生成内容与人类中心设计范式之间仍存在根本性脱节，这种脱节源于表示方法的不兼容性：传统AI框架主要处理网格或神经表示(如NeRF、高斯溅射)，而设计师则在参数化建模工具中操作。这种不兼容性降低了AI在3D行业的实际价值，削弱了人机协作的效率。为解决这一差异，我们专注于生成设计操作序列，这些结构化的建模历史全面捕捉3D资产的逐步构建过程，并与现代3D软件中设计师的典型工作流程保持一致。我们首先将基本建模操作(如挤压、布尔运算)重新表述为可微分单元，通过基于梯度的学习联合优化连续参数(如挤压高度)和离散参数(如布尔类型)。基于这些可微分操作，构建具有门控机制的层次图，并通过最小化Chamfer Distance到目标几何形状进行端到端优化。多阶段序列长度约束和领域规则惩罚实现了无需真实序列监督的无监督紧凑设计序列学习。大量验证表明，生成的操作序列实现了高几何保真度、平滑的网格布线、合理的步骤组合和灵活的编辑能力，与设计行业完全兼容。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D人工智能生成内容(3D-AIGC)与人类中心设计范式之间的不兼容问题。传统AI框架主要处理网格或神经表示，而设计师在参数化建模工具中操作，这降低了AI在3D行业中的实际应用价值。这个问题很重要，因为研究表明70%的专业3D设计师花费超过3小时修改AI生成的3D内容，超过72%的人报告没有节省时间甚至增加了工作成本，阻碍了人机协作效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了AI生成内容与设计工作流程间的根本不兼容性，然后提出通过生成设计操作序列来解决这一差距。他们将基本建模操作重新表述为可微分单元，构建分层计算图，并引入门控机制。作者借鉴了3D-AIGC领域的GANs和扩散方法，以及CAD建模中的命令序列表示，但明确指出CAD方法仅适用于机械部件，无法扩展到更广泛的3D内容创建领域，且CAD有大量可用数据而设计操作序列数据稀缺。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统离散几何建模操作转化为可微分单元，使连续参数和离散参数能通过梯度学习联合优化，并构建分层计算图进行端到端优化。整体流程包括：1)创建可微分操作框架，处理连续和离散参数；2)构建可微分操作图，引入门控机制调节节点影响；3)实现19种不同操作的可微分版本，通过特殊策略解决组合爆炸问题；4)提取最终操作序列，生成与行业标准3D设计工具兼容的编辑历史。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出设计操作序列表示统一3D资产与设计师工作流程；2)创建可微分操作框架实现连续和离散参数的联合优化；3)实现零-shot方法解决数据稀缺问题；4)引入门控机制和概率分支处理离散选择；5)为复杂操作设计特定微分策略。相比之前工作，本文方法生成可编辑操作序列而非静态网格，适用于更广泛的3D内容创建而非仅机械部件，且不需要大量标记数据，生成的序列与人类设计师工作流程高度一致。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于可微分操作图的人机协作3D资产设计序列生成方法，通过结合神经网络优化与程序化建模，实现了与行业标准3D设计工具兼容的可编辑3D内容生成，解决了3D-AIGC与人类设计工作流程之间的根本不兼容问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of 3D artificial intelligence-generated content (3D-AIGC) hasenabled rapid synthesis of intricate geometries. However, a fundamentaldisconnect persists between AI-generated content and human-centric designparadigms, rooted in representational incompatibilities: conventional AIframeworks predominantly manipulate meshes or neural representations(\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate withinparametric modeling tools. This disconnection diminishes the practical value ofAI for 3D industry, undermining the efficiency of human-AI collaboration. Toresolve this disparity, we focus on generating design operation sequences,which are structured modeling histories that comprehensively capture thestep-by-step construction process of 3D assets and align with designers'typical workflows in modern 3D software. We first reformulate fundamentalmodeling operations (\emph{e.g.}, \emph{Extrude}, \emph{Boolean}) intodifferentiable units, enabling joint optimization of continuous (\emph{e.g.},\emph{Extrude} height) and discrete (\emph{e.g.}, \emph{Boolean} type)parameters via gradient-based learning. Based on these differentiableoperations, a hierarchical graph with gating mechanism is constructed andoptimized end-to-end by minimizing Chamfer Distance to target geometries.Multi-stage sequence length constraint and domain rule penalties enableunsupervised learning of compact design sequences without ground-truth sequencesupervision. Extensive validation demonstrates that the generated operationsequences achieve high geometric fidelity, smooth mesh wiring, rational stepcomposition and flexible editing capacity, with full compatibility withindesign industry.</description>
      <author>example@mail.com (Xiaoyang Huang, Bingbing Ni, Wenjun Zhang)</author>
      <guid isPermaLink="false">2508.17645v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection</title>
      <link>http://arxiv.org/abs/2508.17567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究确定了用于医学影像分类的最佳CNN架构，特别是针对ACL撕裂检测和乳腺病变恶性检测，并比较了不同预训练方法的效果。&lt;h4&gt;背景&lt;/h4&gt;现代计算机视觉模型在医学影像分类和分割任务中非常有用，但医学影像数据的稀缺性限制了从头开始训练模型的效能。迁移学习已成为解决这一问题的关键方法。&lt;h4&gt;目的&lt;/h4&gt;确定用于乳腺病变恶性检测和ACL撕裂检测的最佳CNN架构，并统计分析RadImageNet和ImageNet预训练对下游模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;进行全面调查，测试不同的CNN架构，包括一维卷积分类器、跳跃连接、ResNet50预训练骨干网络和部分骨干网络解冻方法。&lt;h4&gt;主要发现&lt;/h4&gt;最佳模型结构包括一维卷积分类器、跳跃连接、ResNet50预训练骨干网络和部分骨干网络解冻；ACL撕裂检测的最佳模型AUC达到0.9969；乳腺结节恶性检测的最佳模型AUC达到0.9641；没有发现证据证实RadImageNet预训练在ACL撕裂和乳腺病变分类任务中提供优越的下游性能。&lt;h4&gt;结论&lt;/h4&gt;使用特定架构和预训练策略的模型在医学影像分类任务中表现优异，但RadImageNet预训练并不总是比ImageNet预训练更优越。&lt;h4&gt;翻译&lt;/h4&gt;现代计算机视觉模型已被证明对医学影像分类和分割任务非常有用，但医学影像数据的稀缺性常常限制了从头开始训练的模型的效能。迁移学习已成为解决这一问题的关键方案，使高性能模型能够在小数据集上进行微调。Mei等人(2022)发现，在放射科医生标记的大型数据集(RadImageNet)上预训练CNNs，与ImageNet预训练相比，能提高下游任务的模型性能。本研究通过进行全面调查，扩展了Mei等人(2022)的研究，以确定用于乳腺病变恶性检测和ACL撕裂检测的最佳CNN架构，并进行统计分析以比较RadImageNet和ImageNet预训练对下游模型性能的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern computer vision models have proven to be highly useful for medicalimaging classification and segmentation tasks, but the scarcity of medicalimaging data often limits the efficacy of models trained from scratch. Transferlearning has emerged as a pivotal solution to this, enabling the fine-tuning ofhigh-performance models on small data. Mei et al. (2022) found thatpre-training CNNs on a large dataset of radiologist-labeled images(RadImageNet) enhanced model performance on downstream tasks compared toImageNet pretraining. The present work extends Mei et al. (2022) by conductinga comprehensive investigation to determine optimal CNN architectures for breastlesion malignancy detection and ACL tear detection, as well as performingstatistical analysis to compare the effect of RadImageNet and ImageNetpre-training on downstream model performance. Our findings suggest that1-dimensional convolutional classifiers with skip connections, ResNet50pre-trained backbones, and partial backbone unfreezing yields optimaldownstream medical classification performance. Our best models achieve AUCs of0.9969 for ACL tear detection and 0.9641 for breast nodule malignancydetection, competitive with the results reported by Mei et al. (2022) andsurpassing other previous works. We do not find evidence confirming RadImageNetpre-training to provide superior downstream performance for ACL tear and breastlesion classification tasks.</description>
      <author>example@mail.com (Daniel Frees, Moritz Bolling, Aditri Bhagirath)</author>
      <guid isPermaLink="false">2508.17567v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Assisted Detection of Sarcopenia in Cross-Sectional Computed Tomography Imaging</title>
      <link>http://arxiv.org/abs/2508.17275v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于深度学习的自动化方法，用于测量CT图像中的骨骼肌面积(SMA)，以评估肌肉减少症，实现了高精度的定量评估。&lt;h4&gt;背景&lt;/h4&gt;肌肉减少症是一种与手术不良结果相关的肌肉质量和功能进行性丧失。目前通过测量SMA的横断面成像评估方法耗时且增加临床工作量，限制了及时检测和管理。人工智能应用可以提高这一过程的效率和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发深度学习模型来自动测量CT图像中的SMA，实现肌肉减少症评估和检测的完全自动化。&lt;h4&gt;方法&lt;/h4&gt;使用在英国纽卡斯尔Freeman Hospital收集的高质量三维横断面CT图像。专家临床医生在第三腰椎处手动注释SMA生成精确分割掩膜。采用迁移学习和自监督学习方法，结合标记和未标记的CT扫描数据集。开发了定性评估模型，但发现SMA的定量评估更精确。&lt;h4&gt;主要发现&lt;/h4&gt;模型预测SMA的平均误差大约为3个百分点（与手动测量相比），预测掩膜的平均dice相似系数为93%。&lt;h4&gt;结论&lt;/h4&gt;基于深度学习的方法显示了实现肌肉减少症评估和检测完全自动化的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;肌肉减少症是一种与手术不良结果（如住院时间延长、活动能力受损和死亡率增加）相关的肌肉质量和功能进行性丧失。虽然可通过测量骨骼肌面积(SMA)的横断面成像来评估，但这一过程耗时且增加临床工作量，限制了及时检测和管理；然而，在人工智能应用的协助下，这一过程可以变得更高效和可扩展。本文展示了在英国纽卡斯尔Freeman Hospital收集的肌肉减少症患者的高质量三维横断面CT图像。专家临床医生手动注释了第三腰椎处的SMA，生成了精确的分割掩膜。我们开发了深度学习模型来测量CT图像中的SMA并自动化此任务。我们的方法使用了标记和未标记的CT扫描数据集，采用迁移学习和自监督学习方法。虽然我们开发了检测肌肉减少症的定性评估模型，但我们观察到SMA的定量评估更精确和信息丰富。这种方法还缓解了类别不平衡和数据可用性有限的问题。我们的模型预测SMA的平均误差大约为3个百分点（与手动测量的SMA相比）。预测掩膜的平均dice相似系数为93%。因此，我们的结果显示了实现肌肉减少症评估和检测完全自动化的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sarcopenia is a progressive loss of muscle mass and function linked to poorsurgical outcomes such as prolonged hospital stays, impaired mobility, andincreased mortality. Although it can be assessed through cross-sectionalimaging by measuring skeletal muscle area (SMA), the process is time-consumingand adds to clinical workloads, limiting timely detection and management;however, this process could become more efficient and scalable with theassistance of artificial intelligence applications. This paper presentshigh-quality three-dimensional cross-sectional computed tomography (CT) imagesof patients with sarcopenia collected at the Freeman Hospital, Newcastle uponTyne Hospitals NHS Foundation Trust. Expert clinicians manually annotated theSMA at the third lumbar vertebra, generating precise segmentation masks. Wedevelop deep-learning models to measure SMA in CT images and automate thistask. Our methodology employed transfer learning and self-supervised learningapproaches using labelled and unlabeled CT scan datasets. While we developedqualitative assessment models for detecting sarcopenia, we observed that thequantitative assessment of SMA is more precise and informative. This approachalso mitigates the issue of class imbalance and limited data availability. Ourmodel predicted the SMA, on average, with an error of +-3 percentage pointsagainst the manually measured SMA. The average dice similarity coefficient ofthe predicted masks was 93%. Our results, therefore, show a pathway to fullautomation of sarcopenia assessment and detection.</description>
      <author>example@mail.com (Manish Bhardwaj, Huizhi Liang, Ashwin Sivaharan, Sandip Nandhra, Vaclav Snasel, Tamer El-Sayed, Varun Ojha)</author>
      <guid isPermaLink="false">2508.17275v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>CE-RS-SBCIT A Novel Channel Enhanced Hybrid CNN Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis</title>
      <link>http://arxiv.org/abs/2508.17128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 Pages, 12 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型混合框架CE-RS-SBCIT，通过结合CNN和Transformer技术，有效解决了脑瘤MRI分析中的挑战，实现了高精度的脑瘤检测和分类。&lt;h4&gt;背景&lt;/h4&gt;脑瘤是人类最致命的疾病之一，早期检测和准确分类对有效诊断和治疗至关重要。虽然基于深度学习的计算机辅助诊断系统已取得进展，但传统的卷积神经网络和Transformer仍面临高计算成本、对微小对比度变化敏感、MRI数据结构异质性和纹理不一致等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型混合框架CE-RS-SBCIT，整合基于残差和空间学习的CNN与基于Transformer的模块，以克服现有方法的局限性，提高脑瘤检测和分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;框架包含四个核心创新：1)基于平滑和边界的CNN集成Transformer(SBCIT)；2)定制的残差和空间学习CNN；3)通道增强(CE)策略；4)新的空间注意力机制。SBCIT采用主干卷积和上下文交互Transformer块，残差和空间CNN通过辅助迁移学习增强表示空间，CE模块放大判别性通道，空间注意力机制强调不同肿瘤类别间的细微差异。&lt;h4&gt;主要发现&lt;/h4&gt;在包含胶质瘤、脑膜瘤、垂体肿瘤和健康对照组的Kaggle和Figshare MRI数据集上评估，该框架实现了98.30%的准确率、98.08%的灵敏度、98.25%的F1分数和98.43%的精确度，展现出优越的性能。&lt;h4&gt;结论&lt;/h4&gt;CE-RS-SBCIT框架通过有效结合局部细粒度和全局上下文线索，成功解决了传统方法在脑瘤MRI分析中的挑战，为脑瘤的早期检测和准确分类提供了有力的工具。&lt;h4&gt;翻译&lt;/h4&gt;脑瘤仍然是人类最致命的疾病之一，早期检测和准确分类对有效诊断和治疗计划至关重要。尽管基于深度学习的计算机辅助诊断系统已显示出显著进展，但传统的卷积神经网络和Transformer持续面临高计算成本、对微小对比度变化的敏感性、MRI数据中的结构异质性和纹理不一致等挑战。因此，本文引入了一种新型混合框架CE-RS-SBCIT，整合了基于残差和空间学习的CNN与基于Transformer的模块。该框架通过四个核心创新利用局部细粒度和全局上下文线索：(i)基于平滑和边界的CNN集成Transformer(SBCIT)，(ii)定制的残差和空间学习CNN，(iii)通道增强(CE)策略，以及(iv)一种新的空间注意力机制。开发的SBCIT采用主干卷积和上下文交互Transformer块，具有系统化的平滑和边界操作，能够高效地进行全局特征建模。此外，通过辅助迁移学习的特征图增强的残差和空间CNN丰富了表示空间，而CE模块放大了判别性通道并减少了冗余。进一步地，空间注意力机制选择性地强调了不同肿瘤类别间的细微对比度和纹理变化。在Kaggle和Figshare的挑战性MRI数据集上进行的广泛评估，涵盖了胶质瘤、脑膜瘤、垂体肿瘤和健康对照组，证明了其优越的性能，达到了98.30%的准确率、98.08%的灵敏度、98.25%的F1分数和98.43%的精确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain tumors remain among the most lethal human diseases, where earlydetection and accurate classification are critical for effective diagnosis andtreatment planning. Although deep learning-based computer-aided diagnostic(CADx) systems have shown remarkable progress. However, conventionalconvolutional neural networks (CNNs) and Transformers face persistentchallenges, including high computational cost, sensitivity to minor contrastvariations, structural heterogeneity, and texture inconsistencies in MRI data.Therefore, a novel hybrid framework, CE-RS-SBCIT, is introduced, integratingresidual and spatial learning-based CNNs with transformer-driven modules. Theproposed framework exploits local fine-grained and global contextual cuesthrough four core innovations: (i) a smoothing and boundary-basedCNN-integrated Transformer (SBCIT), (ii) tailored residual and spatial learningCNNs, (iii) a channel enhancement (CE) strategy, and (iv) a novel spatialattention mechanism. The developed SBCIT employs stem convolution andcontextual interaction transformer blocks with systematic smoothing andboundary operations, enabling efficient global feature modeling. Moreover,Residual and spatial CNNs, enhanced by auxiliary transfer-learned feature maps,enrich the representation space, while the CE module amplifies discriminativechannels and mitigates redundancy. Furthermore, the spatial attention mechanismselectively emphasizes subtle contrast and textural variations across tumorclasses. Extensive evaluation on challenging MRI datasets from Kaggle andFigshare, encompassing glioma, meningioma, pituitary tumors, and healthycontrols, demonstrates superior performance, achieving 98.30% accuracy, 98.08%sensitivity, 98.25% F1-score, and 98.43% precision.</description>
      <author>example@mail.com (Mirza Mumtaz Zahoor, Saddam Hussain Khan)</author>
      <guid isPermaLink="false">2508.17128v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>SugarcaneShuffleNet: A Very Fast, Lightweight Convolutional Neural Network for Diagnosis of 15 Sugarcane Leaf Diseases</title>
      <link>http://arxiv.org/abs/2508.17107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 19 figures, Submitted in Computers and Electronics in  Agriculture&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究为资源有限地区的甘蔗农民提供了一个完整的解决方案，包括数据集、优化模型和实际应用工具，用于叶部疾病的现场诊断，解决了现有深度学习模型在资源受限环境中的局限性。&lt;h4&gt;背景&lt;/h4&gt;尽管基于AI的植物诊断已有进展，但在资源有限地区的甘蔗农民仍然容易受到叶部疾病的威胁，缺乏可扩展、高效和可解释的工具。许多深度学习模型在现实条件下无法泛化，且需要大量计算资源，限制了在资源有限地区的使用。&lt;h4&gt;目的&lt;/h4&gt;开发适用于资源有限地区的甘蔗叶疾病分类工具，创建一个数据集、优化模型和开发一个实际应用工具。&lt;h4&gt;方法&lt;/h4&gt;提出了SugarcaneLD-BD数据集（包含638张精选图像，涵盖五个类别，由专家验证）；提出了SugarcaneShuffleNet优化轻量级模型（大小为9.26 MB）；提出了SugarcaneAI渐进式Web应用程序用于现场部署；通过迁移学习和贝叶斯优化微调了其他五个轻量级卷积神经网络进行比较；将SugarcaneShuffleNet集成到SugarcaneAI中提供基于Grad-CAM的现场解释。&lt;h4&gt;主要发现&lt;/h4&gt;SugarcaneShuffleNet模型达到98.02%的准确率，F1分数为0.98，每张图像的平均推理时间为4.14毫秒；MnasNet和EdgeNeXt达到了相当的准确性，但需要更多参数和计算资源，不适合资源有限环境。&lt;h4&gt;结论&lt;/h4&gt;这些贡献提供了多样化的基准、适用于资源有限环境的高效模型以及甘蔗疾病分类的实用工具，涵盖了农场中不同的光照、背景和使用的设备。&lt;h4&gt;翻译&lt;/h4&gt;尽管基于人工智能的植物诊断取得了进展，但在资源有限地区的甘蔗农民由于缺乏可扩展、高效和可解释的工具，仍然容易受到叶部疾病的侵害。许多深度学习模型在现实条件下无法泛化，并且需要大量计算资源，限制了它们在资源受限地区的使用。在本文中，我们提出了SugarcaneLD-BD，一个用于甘蔗叶疾病分类的精选数据集；SugarcaneShuffleNet，一种用于快速设备诊断的优化轻量级模型；以及SugarcaneAI，一个用于现场部署的渐进式Web应用程序。SugarcaneLD-BD包含638张精选图像，涵盖五个类别，包括四种主要甘蔗疾病，在孟加拉国不同田间条件下收集并由专家病理学家验证。为了增强多样性，我们将SugarcaneLD-BD与另外两个数据集结合，形成更大和更具代表性的语料库。我们的优化模型SugarcaneShuffleNet在实时设备诊断中提供了速度和准确性的最佳平衡。这个9.26 MB的模型达到了98.02%的准确率，F1分数为0.98，每张图像的平均推理时间为4.14毫秒。作为比较，我们通过迁移学习和贝叶斯优化微调了其他五个轻量级卷积神经网络：MnasNet、EdgeNeXt、EfficientNet-Lite、MobileNet和SqueezeNet。MnasNet和EdgeNeXt达到了与SugarcaneShuffleNet相当的准确性，但需要更多的参数、内存和计算，限制了它们在资源有限部署中的适用性。我们将SugarcaneShuffleNet集成到SugarcaneAI中，在田间提供基于Grad-CAM的解释。这些贡献共同提供了多样化的基准、适用于资源有限环境的高效模型以及甘蔗疾病分类的实用工具。它涵盖了农场中不同的光照、背景和使用的设备。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite progress in AI-based plant diagnostics, sugarcane farmers inlow-resource regions remain vulnerable to leaf diseases due to the lack ofscalable, efficient, and interpretable tools. Many deep learning models fail togeneralize under real-world conditions and require substantial computationalresources, limiting their use in resource-constrained regions. In this paper,we present SugarcaneLD-BD, a curated dataset for sugarcane leaf-diseaseclassification; SugarcaneShuffleNet, an optimized lightweight model for rapidon-device diagnosis; and SugarcaneAI, a Progressive Web Application for fielddeployment. SugarcaneLD-BD contains 638 curated images across five classes,including four major sugarcane diseases, collected in Bangladesh under diversefield conditions and verified by expert pathologists. To enhance diversity, wecombined SugarcaneLD-BD with two additional datasets, yielding a larger andmore representative corpus. Our optimized model, SugarcaneShuffleNet, offersthe best trade-off between speed and accuracy for real-time, on-devicediagnosis. This 9.26 MB model achieved 98.02% accuracy, an F1-score of 0.98,and an average inference time of 4.14 ms per image. For comparison, wefine-tuned five other lightweight convolutional neural networks: MnasNet,EdgeNeXt, EfficientNet-Lite, MobileNet, and SqueezeNet via transfer learningand Bayesian optimization. MnasNet and EdgeNeXt achieved comparable accuracy toSugarcaneShuffleNet, but required significantly more parameters, memory, andcomputation, limiting their suitability for low-resource deployment. Weintegrate SugarcaneShuffleNet into SugarcaneAI, delivering Grad-CAM-basedexplanations in the field. Together, these contributions offer a diversebenchmark, efficient models for low-resource environments, and a practical toolfor sugarcane disease classification. It spans varied lighting, backgrounds anddevices used on-farm</description>
      <author>example@mail.com (Shifat E. Arman, Hasan Muhammad Abdullah, Syed Nazmus Sakib, RM Saiem, Shamima Nasrin Asha, Md Mehedi Hasan, Shahrear Bin Amin, S M Mahin Abrar)</author>
      <guid isPermaLink="false">2508.17107v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>UM3: Unsupervised Map to Map Matching</title>
      <link>http://arxiv.org/abs/2508.16874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM SIGSPATIAL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;地图匹配是对齐来自不同来源的空间数据的关键任务，但由于缺乏真实对应关系、节点特征稀疏以及可扩展性需求，这一任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一个无监督的基于图的框架，解决地图匹配中的挑战，提供一种可扩展且实用的地图对齐解决方案。&lt;h4&gt;方法&lt;/h4&gt;通过三个关键创新：1) 无监督学习方法，无需训练数据；2) 引入伪坐标，捕获节点相对空间布局；3) 设计自适应平衡特征和几何相似性的机制及几何一致损失函数。在实现层面，开发基于瓦片的后处理流水线，支持并行处理并保持边界连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的实验表明，该方法在匹配任务中实现了最先进的准确性，大幅超越现有方法，特别是在高噪声和大规模场景下表现优异。&lt;h4&gt;结论&lt;/h4&gt;该框架为地图对齐提供了可扩展且实用的解决方案，是传统方法的稳健高效替代方案。&lt;h4&gt;翻译&lt;/h4&gt;地图匹配是对齐来自不同来源的空间数据的关键任务，但由于缺乏真实对应关系、节点特征稀疏以及可扩展性需求，这一任务仍然具有挑战性。在本文中，我们提出了一个无监督的基于图框架，通过三个关键创新解决这些挑战。首先，我们的方法是一种无监督学习方法，不需要训练数据，这对于获取标记训练样本具有挑战性的大规模地图数据至关重要。其次，我们引入了伪坐标，用于捕获每个地图内节点的相对空间布局，这增强了特征判别力并实现了尺度不变学习。第三，我们设计了一种机制来自适应平衡特征和几何相似性，以及一个几何一致的损失函数，确保对噪声或不完整坐标数据的鲁棒性。在实现层面，为了处理大规模地图，我们开发了一种基于瓦片的后处理流水线，具有重叠区域和多数投票功能，能够实现并行处理同时保持边界连贯性。在真实世界数据集上的实验表明，我们的方法在匹配任务中实现了最先进的准确性，大幅超越现有方法，特别是在高噪声和大规模场景下。我们的框架为地图对齐提供了可扩展且实用的解决方案，是传统方法的稳健高效替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3748636.3762734&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Map-to-map matching is a critical task for aligning spatial data acrossheterogeneous sources, yet it remains challenging due to the lack of groundtruth correspondences, sparse node features, and scalability demands. In thispaper, we propose an unsupervised graph-based framework that addresses thesechallenges through three key innovations. First, our method is an unsupervisedlearning approach that requires no training data, which is crucial forlarge-scale map data where obtaining labeled training samples is challenging.Second, we introduce pseudo coordinates that capture the relative spatiallayout of nodes within each map, which enhances feature discriminability andenables scale-invariant learning. Third, we design an mechanism to adaptivelybalance feature and geometric similarity, as well as a geometric-consistentloss function, ensuring robustness to noisy or incomplete coordinate data. Atthe implementation level, to handle large-scale maps, we develop a tile-basedpost-processing pipeline with overlapping regions and majority voting, whichenables parallel processing while preserving boundary coherence. Experiments onreal-world datasets demonstrate that our method achieves state-of-the-artaccuracy in matching tasks, surpassing existing methods by a large margin,particularly in high-noise and large-scale scenarios. Our framework provides ascalable and practical solution for map alignment, offering a robust andefficient alternative to traditional approaches.</description>
      <author>example@mail.com (Chaolong Ying, Yinan Zhang, Lei Zhang, Jiazhuang Wang, Shujun Jia, Tianshu Yu)</author>
      <guid isPermaLink="false">2508.16874v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</title>
      <link>http://arxiv.org/abs/2508.18166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PCR-CA是一种针对多类别应用推荐的新型端到端框架，通过并行码本表示和对比对齐技术提升点击率预测效果。&lt;h4&gt;背景&lt;/h4&gt;现代应用商店推荐系统在处理多类别应用时面临挑战，传统分类法无法捕捉重叠语义，导致个性化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出PCR-CA框架，改进应用商店推荐系统的点击率预测能力。&lt;h4&gt;方法&lt;/h4&gt;PCR-CA首先提取应用文本的多模态嵌入，然后引入并行码本VQ-AE模块独立编码不同方面，采用对比对齐损失桥接语义和协同信号，并通过双注意力融合机制结合ID和语义特征捕捉用户兴趣。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模数据集上，PCR-CA相比基线模型实现+0.76% AUC提升，长尾应用提升+2.15%；在线A/B测试显示CTR提升+10.52%，转化率提升+16.30%。&lt;h4&gt;结论&lt;/h4&gt;PCR-CA在实际应用中证明有效，已完全部署在Microsoft Store上。&lt;h4&gt;翻译&lt;/h4&gt;现代应用商店推荐系统在处理多类别应用时面临挑战，因为传统分类法无法捕捉重叠语义，导致个性化效果不佳。我们提出了PCR-CA（并行码本表示与对比对齐），这是一个改进点击率预测的端到端框架。PCR-CA首先从应用文本中提取紧凑的多模态嵌入，然后引入一个并行码本VQ-AE模块，在多个码本上并行学习离散语义表示——这与分层残差量化(RQ-VAE)不同。这种设计能够独立编码不同方面（如游戏玩法、艺术风格），更好地建模多类别语义。为了桥接语义和协同信号，我们在用户和项目级别采用对比对齐损失，增强长尾项目的表示学习。此外，双注意力融合机制结合基于ID和语义的特征，捕捉用户兴趣，特别是对于长尾应用。在大规模数据集上的实验显示，PCR-CA相比强基线模型实现了+0.76%的AUC提升，长尾应用的AUC提升达到+2.15%。在线A/B测试进一步验证了我们的方法，显示CTR提升+10.52%，转化率提升+16.30%，证明了PCR-CA在实际部署中的有效性。该新框架现已完全部署在Microsoft Store上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern app store recommender systems struggle with multiple-category apps, astraditional taxonomies fail to capture overlapping semantics, leading tosuboptimal personalization. We propose PCR-CA (Parallel CodebookRepresentations with Contrastive Alignment), an end-to-end framework forimproved CTR prediction. PCR-CA first extracts compact multimodal embeddingsfrom app text, then introduces a Parallel Codebook VQ-AE module that learnsdiscrete semantic representations across multiple codebooks in parallel --unlike hierarchical residual quantization (RQ-VAE). This design enablesindependent encoding of diverse aspects (e.g., gameplay, art style), bettermodeling multiple-category semantics. To bridge semantic and collaborativesignals, we employ a contrastive alignment loss at both the user and itemlevels, enhancing representation learning for long-tail items. Additionally, adual-attention fusion mechanism combines ID-based and semantic features tocapture user interests, especially for long-tail apps. Experiments on alarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strongbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing furthervalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvementin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The newframework has now been fully deployed on the Microsoft Store.</description>
      <author>example@mail.com (Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang)</author>
      <guid isPermaLink="false">2508.18166v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.17703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了EMPOWER，一个新颖的进化框架，通过专门的表示学习、多维度评估和结构保留算法显著提高了医学领域大型语言模型提示的质量和可靠性。&lt;h4&gt;背景&lt;/h4&gt;提示工程对医学应用中大型语言模型的可靠性和临床效用有显著影响，但当前的优化方法未能充分解决特定医学领域的知识和安全要求。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够提高医学提示质量的新框架，解决当前优化方法在医学领域知识和安全要求方面的不足。&lt;h4&gt;方法&lt;/h4&gt;EMPOWER框架包含四个关键组件：医学术语注意力机制、评估清晰度、特异性、临床相关性和事实准确性的综合评估架构、保留临床推理完整性的组件级进化算法，以及确保遵循医学知识的语义验证模块。&lt;h4&gt;主要发现&lt;/h4&gt;在诊断、治疗和教育任务上的评估显示显著改进：事实错误内容减少24.7%，领域特异性提高19.6%，盲评中医生的偏好提高15.3%。&lt;h4&gt;结论&lt;/h4&gt;该框架解决了开发临床适当提示的关键挑战，促进了大型语言模型在医疗环境中的更负责任的整合。&lt;h4&gt;翻译&lt;/h4&gt;提示工程显著影响大型语言模型在医学应用中的可靠性和临床效用。当前的优化方法未能充分解决特定医学领域的知识和安全要求。本文介绍了EMPOWER，一个新颖的进化框架，通过专门的表示学习、多维度评估和结构保留算法提高医学提示质量。我们的方法包括：医学术语注意力机制、评估清晰度、特异性、临床相关性和事实准确性的综合评估架构、保留临床推理完整性的组件级进化算法，以及确保遵循医学知识的语义验证模块。在诊断、治疗和教育任务上的评估显示出显著改进：事实错误内容减少24.7%，领域特异性提高19.6%，盲评中医生的偏好提高15.3%。该框架解决了开发临床适当提示的关键挑战，促进了大型语言模型在医疗环境中的更负责任的整合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prompt engineering significantly influences the reliability and clinicalutility of Large Language Models (LLMs) in medical applications. Currentoptimization approaches inadequately address domain-specific medical knowledgeand safety requirements. This paper introduces EMPOWER, a novel evolutionaryframework that enhances medical prompt quality through specializedrepresentation learning, multi-dimensional evaluation, and structure-preservingalgorithms. Our methodology incorporates: (1) a medical terminology attentionmechanism, (2) a comprehensive assessment architecture evaluating clarity,specificity, clinical relevance, and factual accuracy, (3) a component-levelevolutionary algorithm preserving clinical reasoning integrity, and (4) asemantic verification module ensuring adherence to medical knowledge.Evaluation across diagnostic, therapeutic, and educational tasks demonstratessignificant improvements: 24.7% reduction in factually incorrect content, 19.6%enhancement in domain specificity, and 15.3% higher clinician preference inblinded evaluations. The framework addresses critical challenges in developingclinically appropriate prompts, facilitating more responsible integration ofLLMs into healthcare settings.</description>
      <author>example@mail.com (Yinda Chen, Yangfan He, Jing Yang, Dapeng Zhang, Zhenlong Yuan, Muhammad Attique Khan, Jamel Baili, Por Lip Yee)</author>
      <guid isPermaLink="false">2508.17703v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Representation Learning Conditioned on Semantic Relations</title>
      <link>http://arxiv.org/abs/2508.17497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RCML框架，通过利用语义关系指导多模态表示学习，解决了现有对比模型的三个主要限制：未充分利用跨对语义关系、缺乏子空间语义对齐、模态内部一致性支持有限。&lt;h4&gt;背景&lt;/h4&gt;多模态表示学习通过对比模型（如CLIP）快速发展，这些模型在共享嵌入空间中对齐图像-文本对。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态对比模型的三个主要限制：1)未充分利用不同对之间的语义关系；2)直接匹配全局嵌入而忽略特定子空间语义对齐；3)强调跨模态对比而缺乏模态内部一致性支持。&lt;h4&gt;方法&lt;/h4&gt;提出Relation-Conditioned Multimodal Learning (RCML)框架，在自然语言关系描述指导下学习多模态表示。构建由语义关系链接的多对多训练对，引入关系引导的交叉注意力机制调整多模态表示，结合跨模态和模态内对比损失作为训练目标。&lt;h4&gt;主要发现&lt;/h4&gt;在不同数据集上的实验表明，RCML在检索和分类任务上都优于强大的基线模型，验证了利用语义关系指导多模态表示学习的有效性。&lt;h4&gt;结论&lt;/h4&gt;RCML通过语义关系指导多模态表示学习，有效解决了现有模型的局限性，提升了多模态任务性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态表示学习通过CLIP等对比模型快速发展，这些模型在共享嵌入空间中对齐图像-文本对。然而，这些模型面临局限性：(1)通常只关注图像-文本对，未充分利用不同对之间的语义关系；(2)直接匹配全局嵌入而缺乏上下文化，忽略了在特定子空间或关系维度上进行语义对齐的需求；(3)强调跨模态对比，对模态内部一致性的支持有限。为解决这些问题，我们提出了RCML框架，在自然语言关系描述指导下学习多模态表示以指导特征提取和对齐。我们的方法构建了由语义关系链接的多对多训练对，并引入了关系引导的交叉注意力机制，在每个关系上下文中调整多模态表示。训练目标结合了跨模态和模态内对比损失，鼓励跨模态和语义相关样本之间的一致性。不同数据集上的实验表明，RCML在检索和分类任务上都 consistently outperforms 强大的基线模型，突显了利用语义关系指导多模态表示学习的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning has advanced rapidly with contrastivemodels such as CLIP, which align image-text pairs in a shared embedding space.However, these models face limitations: (1) they typically focus on image-textpairs, underutilizing the semantic relations across different pairs. (2) theydirectly match global embeddings without contextualization, overlooking theneed for semantic alignment along specific subspaces or relational dimensions;and (3) they emphasize cross-modal contrast, with limited support forintra-modal consistency. To address these issues, we proposeRelation-Conditioned Multimodal Learning RCML, a framework that learnsmultimodal representations under natural-language relation descriptions toguide both feature extraction and alignment. Our approach constructsmany-to-many training pairs linked by semantic relations and introduces arelation-guided cross-attention mechanism that modulates multimodalrepresentations under each relation context. The training objective combinesinter-modal and intra-modal contrastive losses, encouraging consistency acrossboth modalities and semantically related samples. Experiments on differentdatasets show that RCML consistently outperforms strong baselines on bothretrieval and classification tasks, highlighting the effectiveness ofleveraging semantic relations to guide multimodal representation learning.</description>
      <author>example@mail.com (Yang Qiao, Yuntong Hu, Liang Zhao)</author>
      <guid isPermaLink="false">2508.17497v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>4D Visual Pre-training for Robot Learning</title>
      <link>http://arxiv.org/abs/2508.17230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FVP是一种新的4D视觉预训练框架，通过将视觉预训练目标设定为下一个点云预测问题并使用扩散模型，显著提升了3D表示在机器人学习任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;基于网络规模数据集学习的通用视觉表示在机器人领域取得了成功，但这些表示主要基于2D图像，忽略了世界的3D特性。由于大规模3D数据稀缺，难以从网络数据集中提取通用3D表示。&lt;h4&gt;目的&lt;/h4&gt;寻找一种通用的视觉预训练框架，能够改进所有3D表示作为替代方案。&lt;h4&gt;方法&lt;/h4&gt;提出FVP框架，将视觉预训练目标设定为下一个点云预测问题，将预测模型建模为扩散模型，并在更大的公共数据集上直接预训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;在十二个真实世界操作任务中，FVP将3D扩散策略的平均成功率提高28%；FVP预训练的DP3在模仿学习方法中达到最先进性能；FVP的有效性适应各种点云编码器和数据集；将FVP应用于RDT-1B增强了其在各种机器人任务上的性能。&lt;h4&gt;结论&lt;/h4&gt;FVP是一种有效的4D视觉预训练框架，能够显著提升3D表示在机器人学习任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，从网络规模数据集学习到的通用视觉表示在机器人领域取得了巨大成功，使机器人能够在操作任务上进行数据高效学习；然而，这些预训练的表示主要基于2D图像，忽略了世界固有的3D特性。但是，由于大规模3D数据的稀缺，仍然很难从网络数据集中提取通用的3D表示。相反，我们寻求一种通用的视觉预训练框架，作为替代方案来改进所有3D表示。我们的框架称为FVP，是一种用于真实世界机器人学习的创新4D视觉预训练框架。FVP将视觉预训练目标设定为下一个点云预测问题，将预测模型建模为扩散模型，并在更大的公共数据集上直接预训练模型。在十二个真实世界操作任务中，FVP将3D扩散策略（DP3）的平均成功率提高了28%。FVP预训练的DP3在模仿学习方法中实现了最先进的性能。此外，FVP的有效性适应各种点云编码器和数据集。最后，我们将FVP应用于RDT-1B，这是一个更大的视觉-语言-动作机器人模型，提高了其在各种机器人任务上的性能。我们的项目页面可在以下网址获取：https://4d-visual-pretraining.github.io/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人学习中3D视觉预训练的挑战。由于3D点云数据稀缺，难以从网络数据中提取通用3D表示，而大多数现有预训练模型基于2D图像，无法充分利用机器人操作的3D本质。这个问题很重要，因为3D点云能提供更丰富的空间信息，帮助机器人更好地理解和操作环境，通过有效的预训练可以减少对大量特定任务数据的依赖，提高学习效率，推动机器人操作能力的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到2D图像预训练的局限性，以及3D点云数据稀缺的挑战。他们设计了一个通用的预训练框架而非特定任务的表示，借鉴了扩散模型在视觉任务中的成功应用。作者将学习目标设定为'下一个点云预测'问题，结合机器人动作信息和历史帧点云场景，以获取对物理环境的理解。他们借鉴了扩散模型、对比学习和掩码信号建模等现有方法，并使用了PointNet++、Point Transformer等现有的3D编码器和DP3、RISE等3D模仿学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视觉预训练目标设定为'下一个点云预测'问题，使用扩散模型作为预测模型，结合历史帧点云和机器人动作信息，以获取对物理环境的理解。整体流程包括：1) 收集机器人任务演示数据；2) 使用3D视觉编码器将前一帧点云编码为潜在表示；3) 基于潜在表示使用修改后的Point-Voxel扩散网络将高斯噪声逐步去噪为下一帧点云；4) 将预训练的3D视觉表示应用于下游机器人操作任务，通过端到端微调提升性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 预测下一个点云而非使用传统对比学习或点云重建；2) 提出4D视觉预训练框架，结合时间维度和3D空间维度；3) 将扩散模型应用于3D点云预测用于视觉预训练；4) 设计通用预训练框架，适用于各种3D模仿学习方法；5) 结合机器人动作信息帮助理解物理环境。相比之前工作，FVP专注于3D点云而非2D图像，使用预测方法而非传统预训练技术，将扩散模型用于预训练而非直接动作生成，提供通用框架而非特定任务表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FVP通过利用历史帧点云和机器人动作信息，以预测下一帧点云为目标，提出了一种通用的4D视觉预训练框架，显著提升了机器人在各种3D操作任务中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General visual representations learned from web-scale datasets for roboticshave achieved great success in recent years, enabling data-efficient robotlearning on manipulation tasks; yet these pre-trained representations aremostly on 2D images, neglecting the inherent 3D nature of the world. However,due to the scarcity of large-scale 3D data, it is still hard to extract auniversal 3D representation from web datasets. Instead, we are seeking ageneral visual pre-training framework that could improve all 3D representationsas an alternative. Our framework, called FVP, is a novel 4D Visual Pre-trainingframework for real-world robot learning. FVP frames the visual pre-trainingobjective as a next-point-cloud-prediction problem, models the prediction modelas a diffusion model, and pre-trains the model on the larger public datasetsdirectly. Across twelve real-world manipulation tasks, FVP boosts the averagesuccess rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVPpre-trained DP3 achieves state-of-the-art performance across imitation learningmethods. Moreover, the efficacy of FVP adapts across various point cloudencoders and datasets. Finally, we apply FVP to the RDT-1B, a largerVision-Language-Action robotic model, enhancing its performance on variousrobot tasks. Our project page is available at: https://4d-visual-pretraining.github.io/.</description>
      <author>example@mail.com (Chengkai Hou, Yanjie Ze, Yankai Fu, Zeyu Gao, Songbo Hu, Yue Yu, Shanghang Zhang, Huazhe Xu)</author>
      <guid isPermaLink="false">2508.17230v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition</title>
      <link>http://arxiv.org/abs/2508.16922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了多尺度分块胶囊网络(MSPCaps)，通过整合多尺度特征学习和高效胶囊路由，解决了现有CapsNet依赖单一高层特征图和传统特征融合策略难以协调多尺度特征差异的问题。&lt;h4&gt;背景&lt;/h4&gt;胶囊网络(CapsNet)在视觉识别方面显示出巨大潜力，能够捕获空间关系和部分-整体层次结构来学习等变特征表示。然而，现有CapsNet及其变体通常依赖单一高层特征图，忽略了多尺度特征的丰富互补信息。此外，传统特征融合策略难以协调多尺度特征差异，导致次优分类性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有CapsNet的局限性，提出一种新型架构，整合多尺度特征学习和高效胶囊路由，提升视觉识别性能。&lt;h4&gt;方法&lt;/h4&gt;MSPCaps包含三个关键组件：1)多尺度ResNet骨干网(MSRB)，提取多尺度特征表示；2)分块胶囊层(PatchifyCaps)，使用统一块大小分割多尺度特征为初级胶囊；3)协议路由(CAR)块，通过识别跨尺度预测对的最大一致性自适应路由多尺度胶囊，确保只有最一致的胶囊贡献于最终投票。&lt;h4&gt;主要发现&lt;/h4&gt;MSPCaps实现了显著的扩展性和优越的鲁棒性，在分类准确性方面持续超越多种基线方法，适用于从高效微型模型(344.3K参数)到强大大型模型(10.9M参数)的多种配置。&lt;h4&gt;结论&lt;/h4&gt;MSPCaps在推进特征表示学习方面具有显著潜力，为视觉识别任务提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;胶囊网络(CapsNet)通过捕获空间关系和部分-整体层次结构来学习等变特征表示，已在视觉识别领域展现出巨大潜力。然而，现有的CapsNet及其变体通常依赖单一高层特征图，忽略了多尺度特征的丰富互补信息。此外，传统特征融合策略(如加法和连接)难以协调多尺度特征差异，导致次优分类性能。为解决这些局限性，我们提出了多尺度分块胶囊网络(MSPCaps)，这是一种新型架构，集成了多尺度特征学习和高效胶囊路由。具体而言，MSPCaps包含三个关键组件：多尺度ResNet骨干网(MSRB)、分块胶囊层(PatchifyCaps)和协议路由(CAR)块。首先，MSRB从输入图像中提取多样的多尺度特征表示，保留细粒度细节和全局上下文信息。其次，PatchifyCaps使用统一的块大小将这些多尺度特征分割成初级胶囊，使模型能够从多样化的感受野中学习。最后，CAR块通过识别具有最大一致性的跨尺度预测对，自适应地路由多尺度胶囊。与简单连接多个自路由块不同，CAR确保只有最一致的胶囊对最终投票做出贡献。我们提出的MSPCaps实现了显著的扩展性和优越的鲁棒性，在分类准确性方面持续超越多种基线方法，配置范围从高效的微型模型(344.3K参数)到强大的大型模型(10.9M参数)，凸显了其在推进特征表示学习方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Capsule Network (CapsNet) has demonstrated significant potential in visualrecognition by capturing spatial relationships and part-whole hierarchies forlearning equivariant feature representations. However, existing CapsNet andvariants often rely on a single high-level feature map, overlooking the richcomplementary information from multi-scale features. Furthermore, conventionalfeature fusion strategies (e.g., addition and concatenation) struggle toreconcile multi-scale feature discrepancies, leading to suboptimalclassification performance. To address these limitations, we propose theMulti-Scale Patchify Capsule Network (MSPCaps), a novel architecture thatintegrates multi-scale feature learning and efficient capsule routing.Specifically, MSPCaps consists of three key components: a Multi-Scale ResNetBackbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-AgreementRouting (CAR) blocks. First, the MSRB extracts diverse multi-scale featurerepresentations from input images, preserving both fine-grained details andglobal contextual information. Second, the PatchifyCaps partitions thesemulti-scale features into primary capsules using a uniform patch size,equipping the model with the ability to learn from diverse receptive fields.Finally, the CAR block adaptively routes the multi-scale capsules byidentifying cross-scale prediction pairs with maximum agreement. Unlike thesimple concatenation of multiple self-routing blocks, CAR ensures that only themost coherent capsules contribute to the final voting. Our proposed MSPCapsachieves remarkable scalability and superior robustness, consistentlysurpassing multiple baseline methods in terms of classification accuracy, withconfigurations ranging from a highly efficient Tiny model (344.3K parameters)to a powerful Large model (10.9M parameters), highlighting its potential inadvancing feature representation learning.</description>
      <author>example@mail.com (Yudong Hu, Yueju Han, Rui Sun, Jinke Ren)</author>
      <guid isPermaLink="false">2508.16922v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Multimodal Representation Learning for Biological Taxonomies</title>
      <link>http://arxiv.org/abs/2508.16744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了超网络在生物多样性分类中的应用，通过将多模态数据嵌入超空间，实现了对生物标本的层次化分类，并在未见物种分类上表现出色。&lt;h4&gt;背景&lt;/h4&gt;生物多样性研究中的分类学涉及基于多种模态证据（如图像和遗传信息）将生物标本组织成结构化层次结构。&lt;h4&gt;目的&lt;/h4&gt;研究超网络是否能提供更好的嵌入空间用于生物多样性的层次化模型。&lt;h4&gt;方法&lt;/h4&gt;该方法将多模态输入嵌入到共享的超空间中，使用对比学习和一种新颖的堆叠蕴含目标函数。&lt;h4&gt;主要发现&lt;/h4&gt;在BIOSCAN-1M数据集上的实验表明，超嵌入与欧几里得基线方法具有竞争性性能，且在利用DNA条形码对未见物种分类方面优于所有其他模型。&lt;h4&gt;结论&lt;/h4&gt;细粒度分类和开放世界泛化仍具挑战性，该框架为生物多样性建模提供了结构感知基础，有潜在应用于物种发现、生态监测和保护工作。&lt;h4&gt;翻译&lt;/h4&gt;生物多样性研究中的分类学涉及基于多种模态证据（如图像和遗传信息）将生物标本组织成结构化层次结构。我们研究了超网络是否能提供更好的嵌入空间用于此类层次模型。我们的方法使用对比学习和一种新颖的堆叠蕴含目标函数，将多模态输入嵌入到共享的超空间中。在BIOSCAN-1M数据集上的实验表明，超嵌入与欧几里得基线方法具有竞争性性能，并且在利用DNA条形码对未见物种分类方面优于所有其他模型。然而，细粒度分类和开放世界泛化仍然具有挑战性。我们的框架为生物多样性建模提供了结构感知的基础，有潜在应用于物种发现、生态监测和保护工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Taxonomic classification in biodiversity research involves organizingbiological specimens into structured hierarchies based on evidence, which cancome from multiple modalities such as images and genetic information. Weinvestigate whether hyperbolic networks can provide a better embedding spacefor such hierarchical models. Our method embeds multimodal inputs into a sharedhyperbolic space using contrastive and a novel stacked entailment-basedobjective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embeddingachieves competitive performance with Euclidean baselines, and outperforms allother models on unseen species classification using DNA barcodes. However,fine-grained classification and open-world generalization remain challenging.Our framework offers a structure-aware foundation for biodiversity modelling,with potential applications to species discovery, ecological monitoring, andconservation efforts.</description>
      <author>example@mail.com (ZeMing Gong, Chuanqi Tang, Xiaoliang Huo, Nicholas Pellegrino, Austin T. Wang, Graham W. Taylor, Angel X. Chang, Scott C. Lowe, Joakim Bruslund Haurum)</author>
      <guid isPermaLink="false">2508.16744v1</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning classification of black holes in the mass--spin diagram</title>
      <link>http://arxiv.org/abs/2508.14316v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一个用于分类黑洞和研究其形成途径的'质量-自旋图'，类似于赫罗图，允许作为红移函数的黑洞演化轨迹，结合了不同黑洞种群的形成、吸积和合并历史。&lt;h4&gt;背景&lt;/h4&gt;需要一种新的方法来分类和研究不同类型的黑洞及其形成途径，类似于天文学中用于分类恒星的赫罗图。&lt;h4&gt;目的&lt;/h4&gt;创建一个质量-自旋图来分类黑洞并研究它们的形成途径，探索黑洞的演化轨迹，并利用机器学习方法对黑洞种群进行分类。&lt;h4&gt;方法&lt;/h4&gt;构建了一个从初始质量和自旋函数以及近似红移演化的真实黑洞连续体；在恒星质量范围内使用二元种群合成软件比较三种关于Wolf-Rayet前潮汐演化的自旋规定；应用监督和无监督机器学习聚类方法对质量-自旋数据集进行黑洞种群分类；使用变分自编码器进行潜在空间表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;质量-自旋图揭示了可能的黑洞主序列，如持续连贯的吸积或分层合并树；单纯的无监督聚类几乎可以恢复典型种群边界；深度学习方法有助于对具有高度重叠子类的真实数据集进行聚类；监督随机森林可以从学习到的潜在空间表示中准确恢复正确的聚类集群；半监督方法显示出进一步开发的潜力；无监督分类器的性能是一个巨大挑战。&lt;h4&gt;结论&lt;/h4&gt;质量-自旋图可以连接引力波和电磁观测与理论模型；研究结果促进了未来机器学习应用的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了用于分类黑洞和研究其形成途径的质量-自旋图，提供了类似于赫罗图的类比。这使得黑洞演化轨迹可以作为红移的函数，结合了各种黑洞种群的形成、吸积和合并历史。从初始质量和自旋函数以及近似红移演化构建的真实黑洞连续体揭示了可能的黑洞主序列，如持续连贯的吸积或分层合并树。在恒星质量范围内，我们使用二元种群合成软件比较了三种关于Wolf-Rayet前潮汐演化的自旋规定，展示了质量-自旋图如何揭示有趣的建模差异。然后，我们通过将监督和无监督机器学习聚类方法应用于质量-自旋数据集来对黑洞种群进行分类。虽然单纯的无监督聚类几乎可以恢复典型种群边界（恒星质量、中等质量和超大质量），但使用变分自编码器进行潜在空间表示学习的更复杂方法有助于对具有高度重叠子类的真实数据集进行聚类。我们发现，监督随机森林可以根据底层数据集的复杂性从学习到的潜在空间表示中准确恢复正确的聚类集群，半监督方法显示出进一步开发的潜力，而无监督分类器的性能是一个巨大挑战。我们的研究结果促进了未来机器学习的应用，并表明质量-自旋图可以用于连接引力波和电磁观测与理论模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the mass--spin diagram for classifying black holes and studyingtheir formation pathways providing an analogue to the Hertzsprung-Russelldiagram. This allows for black hole evolutionary tracks as a function ofredshift, combining formation, accretion, and merger histories for the varietyof black hole populations. A realistic black hole continuum constructed frominitial mass and spin functions and approximate redshift evolution revealspossible black hole main sequences, such as sustained coherent accretionthrough cosmic time or hierarchical merger trees. In the stellar-mass regime,we use a binary population synthesis software to compare three spinprescriptions for tidal evolution of Wolf-Rayet progenitors, showing how themass--spin diagram exposes interesting modeling differences. We then classifyblack hole populations by applying supervised and unsupervised machine learningclustering methods to mass--spin datasets. While bare unsupervised clusteringcan nearly recover canonical population boundaries (stellar-mass,intermediate-mass, and supermassive), a more sophisticated approach utilizingdeep learning via variational autoencoders for latent space representationlearning aids in clustering of realistic datasets with subclasses that highlyoverlap in mass--spin space. We find that a supervised random forest canaccurately recover the correct clusters from the learned latent spacerepresentation depending on the complexity of the underlying dataset,semi-supervised methods show potential for further development, and theperformance of unsupervised classifiers is a great challenge. Our findingsmotivate future machine learning applications and demonstrate that themass--spin diagram can be used to connect gravitational-wave andelectromagnetic observations with theoretical models.</description>
      <author>example@mail.com (Nathan Steinle, Samar Safi-Harb)</author>
      <guid isPermaLink="false">2508.14316v2</guid>
      <pubDate>Tue, 26 Aug 2025 16:01:49 +0800</pubDate>
    </item>
    <item>
      <title>HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2508.16433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HAMSt3R是一种基于MASt3R扩展的3D重建方法，能够从稀疏未校准的多视角图像中同时重建人体和场景，解决了现有方法在以人为中心场景中的局限性。&lt;h4&gt;背景&lt;/h4&gt;从少量未校准图像恢复场景3D几何是计算机视觉中的长期挑战。现有学习方法如DUSt3R和MASt3R主要针对室外静态场景，难以处理以人为中心的场景。&lt;h4&gt;目的&lt;/h4&gt;开发HAMSt3R方法，用于从稀疏未校准的多视角图像进行人体和场景的联合3D重建，弥合3D视觉中人体理解和场景理解之间的差距。&lt;h4&gt;方法&lt;/h4&gt;利用DUNE图像编码器（通过蒸馏MASt3R和多-HMR模型获得）理解场景和人体；添加额外网络头分割人体、通过Dense估计密集对应关系、预测以人为中心环境的深度；利用不同网络头的输出生成包含人体语义信息的密集点云图；采用完全前馈方法，无需复杂优化流程。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoHumans和EgoExo4D基准测试上验证了模型性能；证明了方法在传统多视角立体和多视角姿态回归任务上的泛化能力；能够有效重建人体同时保持通用3D重建任务的强大性能。&lt;h4&gt;结论&lt;/h4&gt;HAMSt3R成功实现了人体和场景的联合3D重建，填补了3D视觉中人体理解和场景理解之间的空白，且方法高效适合实际应用。&lt;h4&gt;翻译&lt;/h4&gt;从一组稀疏的未校准图像中恢复场景的3D几何是计算机视觉中的一个长期问题。虽然最近基于学习的方法如DUSt3R和MASt3R通过直接预测密集场景几何展示了令人印象深刻的结果，但它们主要针对室外静态环境进行训练，难以处理以人为中心的场景。在这项工作中，我们引入了HAMSt3R，它是MASt3R的扩展，用于从稀疏、未校准的多视角图像进行人体和场景的联合3D重建。首先，我们利用DUNE，一种通过蒸馏MASt3R编码器和最先进的人体网格恢复(HMR)模型multi-HMR的编码器等获得的高效图像编码器，以更好地理解场景几何和人体。我们的方法然后添加额外的网络头来分割人体、通过Dense估计密集对应关系，并在以人为中心的环境中预测深度，实现更全面的3D重建。通过利用我们不同网络头的输出，HAMSt3R生成了一个包含人体语义信息的密集点云图。与依赖复杂优化流程的现有方法不同，我们的方法是完全前馈且高效的，使其适合实际应用。我们在EgoHumans和EgoExo4D两个包含多样化以人为中心场景的具有挑战性的基准上评估了我们的模型。此外，我们验证了它在传统多视角立体和多视角姿态回归任务上的泛化能力。我们的结果表明，我们的方法能够在保持通用3D重建任务强大性能的同时有效重建人体，弥合了3D视觉中人体理解和场景理解之间的差距。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏未校准的多视角图像中同时重建人类和周围环境的3D场景问题。这个问题很重要，因为人类是许多场景的中心元素，完整的3D场景理解需要同时处理环境和人体，而现有方法要么专注于静态场景重建，要么使用复杂优化流程处理人体场景，限制了实际应用的可扩展性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（如MASt3R和DUSt3R）在人体场景上的局限性，然后通过扩展MASt3R架构来解决这个问题。他们借鉴了DUNE编码器（融合了MASt3R场景理解和Multi-HMR人体理解能力），并引入了新的网络分支（实例分割头、DensePose头和二值掩码头）来专门处理人体信息。训练策略上混合了原始MASt3R数据和专门的人体中心数据集，确保模型在一般场景和人体场景上都能表现良好。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一的、前馈的神经网络架构在一次前向传播中同时完成3D场景重建、人体实例分割和跨视角人体重建，生成包含丰富人体语义信息的密集3D点图。整体流程：1) 输入两张图像；2) 使用共享ViT编码器提取特征；3) 通过带交叉注意力的双ViT解码器处理特征；4) 多头预测（原始重建头、实例分割头、DensePose头和二值掩码头）；5) 结合各头输出生成语义丰富的3D点图；6) 对于多视角图像，独立处理每对图像并聚合结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个完全前馈的统一架构，无需复杂优化流程；2) 引入人体感知能力，同时保持一般场景重建能力；3) 使用DUNE编码器融合场景和人体理解；4) 生成包含人体语义信息的3D点表示；5) 高效处理任意数量的图像视图。相比之前工作，不同之处在于：与MASt3R/DUSt3R相比增加了人体理解能力；与JOSH/HSfM相比避免了多阶段优化；与单目人体重建方法相比提供了场景上下文；与传统优化方法相比更加高效且对超参数不敏感。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HAMSt3R首次实现了通过单一前馈神经网络从稀疏未校准图像中同时重建人体和周围环境的3D场景，提供了包含丰富人体语义信息的密集3D点表示，并在保持一般场景重建能力的同时显著提升了人体重建的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering the 3D geometry of a scene from a sparse set of uncalibratedimages is a long-standing problem in computer vision. While recentlearning-based approaches such as DUSt3R and MASt3R have demonstratedimpressive results by directly predicting dense scene geometry, they areprimarily trained on outdoor scenes with static environments and struggle tohandle human-centric scenarios. In this work, we introduce HAMSt3R, anextension of MASt3R for joint human and scene 3D reconstruction from sparse,uncalibrated multi-view images. First, we exploit DUNE, a strong image encoderobtained by distilling, among others, the encoders from MASt3R and from astate-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a betterunderstanding of scene geometry and human bodies. Our method then incorporatesadditional network heads to segment people, estimate dense correspondences viaDensePose, and predict depth in human-centric environments, enabling a morecomprehensive 3D reconstruction. By leveraging the outputs of our differentheads, HAMSt3R produces a dense point map enriched with human semanticinformation in 3D. Unlike existing methods that rely on complex optimizationpipelines, our approach is fully feed-forward and efficient, making it suitablefor real-world applications. We evaluate our model on EgoHumans and EgoExo4D,two challenging benchmarks con taining diverse human-centric scenarios.Additionally, we validate its generalization to traditional multi-view stereoand multi-view pose regression tasks. Our results demonstrate that our methodcan reconstruct humans effectively while preserving strong performance ingeneral 3D reconstruction tasks, bridging the gap between human and sceneunderstanding in 3D vision.</description>
      <author>example@mail.com (Sara Rojas, Matthieu Armando, Bernard Ghamen, Philippe Weinzaepfel, Vincent Leroy, Gregory Rogez)</author>
      <guid isPermaLink="false">2508.16433v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
  <item>
      <title>ASCMamba: Multimodal Time-Frequency Mamba for Acoustic Scene Classification</title>
      <link>http://arxiv.org/abs/2508.15632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ASCMamba的多模态网络，用于整合音频和文本信息实现声景分类，在APSIPA ASC 2025 Grand Challenge中取得了最佳成绩，比基线提高了6.2%。&lt;h4&gt;背景&lt;/h4&gt;声景分类(ASC)是计算听觉领域的基础问题，旨在基于独特的声学特征对环境进行分类。APSIPA ASC 2025 Grand Challenge引入了一个多模态ASC任务，与传统仅依赖音频输入的ASC系统不同，提供了额外的文本信息作为输入。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够整合音频和文本信息的多模态系统，实现细粒度的声景理解和有效的多模态ASC。&lt;h4&gt;方法&lt;/h4&gt;提出名为ASCMamba的多模态网络，采用DenseEncoder从频谱图中提取分层频谱特征，使用双路径Mamba块基于Mamba状态空间模型捕获长程时间和频率依赖关系，并引入两步伪标记机制生成更可靠的伪标记。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ASCMamba系统在APSIPA ASC 2025 Grand Challenge中优于所有参赛团队，比基线提高了6.2%的性能。&lt;h4&gt;结论&lt;/h4&gt;多模态方法结合音频和文本信息能够有效提升声景分类性能，ASCMamba系统展示了在细粒度声景理解方面的优越性。&lt;h4&gt;翻译&lt;/h4&gt;声景分类(ASC)是计算听觉领域的基础问题，旨在根据独特的声学特征对环境进行分类。在APSIPA ASC 2025 Grand Challenge的ASC任务中，组织者引入了一个多模态ASC任务。与传统仅依赖音频输入的ASC系统不同，这一挑战提供了额外的文本信息作为输入，包括音频录制位置和录制时间。在本文中，我们提出了我们在APSIPA ASC 2025 Grand Challenge ASC任务中的系统。具体而言，我们提出了一个名为ASCMamba的多模态网络，该网络整合了音频和文本信息，用于细粒度的声景理解和有效的多模态ASC。所提出的ASCMamba采用DenseEncoder从频谱图中提取分层频谱特征，随后是双路径Mamba块，使用基于Mamba的状态空间模型捕获长程时间和频率依赖关系。此外，我们提出了一种两步伪标记机制来生成更可靠的伪标记。结果表明，所提出的系统优于所有参赛团队，比基线提高了6.2%。代码、模型和预训练检查点可在https://github.com/S-Orion/ASCMamba.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acoustic Scene Classification (ASC) is a fundamental problem in computationalaudition, which seeks to classify environments based on the distinctiveacoustic features. In the ASC task of the APSIPA ASC 2025 Grand Challenge, theorganizers introduce a multimodal ASC task. Unlike traditional ASC systems thatrely solely on audio inputs, this challenge provides additional textualinformation as inputs, including the location where the audio is recorded andthe time of recording. In this paper, we present our proposed system for theASC task in the APSIPA ASC 2025 Grand Challenge. Specifically, we propose amultimodal network, \textbf{ASCMamba}, which integrates audio and textualinformation for fine-grained acoustic scene understanding and effectivemultimodal ASC. The proposed ASCMamba employs a DenseEncoder to extracthierarchical spectral features from spectrograms, followed by a dual-path Mambablocks that capture long-range temporal and frequency dependencies usingMamba-based state space models. In addition, we present a two-steppseudo-labeling mechanism to generate more reliable pseudo-labels. Results showthat the proposed system outperforms all the participating teams and achieves a6.2% improvement over the baseline. Code, model and pre-trained checkpoints areavailable at https://github.com/S-Orion/ASCMamba.git.</description>
      <author>example@mail.com (Bochao Sun, Dong Wang, Han Yin)</author>
      <guid isPermaLink="false">2508.15632v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Hybrelighter: Combining Deep Anisotropic Diffusion and Scene Reconstruction for On-device Real-time Relighting in Mixed Reality</title>
      <link>http://arxiv.org/abs/2508.14930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新颖的混合现实场景重新照明方法，整合图像分割、各向异性扩散光照传播和滤波技术，能够在边缘设备上实时实现视觉上吸引人且准确的重新照明效果，速度高达100fps。&lt;h4&gt;背景&lt;/h4&gt;混合现实场景重新照明允许虚拟光照条件变化与物理对象真实交互，产生逼真的照明和阴影效果。现有技术存在局限性：基于深度学习的方法超出当前MR设备的实时性能能力；场景理解方法因扫描限制导致不准确；简单2D图像滤波方法无法表示复杂几何形状和阴影。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在边缘设备上实时实现视觉上吸引人且准确的重新照明效果的新方法，解决现有技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出整合图像分割、通过各向异性扩散进行光照传播、基本场景理解和滤波技术计算简便性的新颖方法，纠正设备端扫描的不准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在边缘设备上实时提供视觉上吸引人且准确的重新照明效果，实现高达100fps的速度，与行业标准进行了直接比较，并在房地产示例中展示了实际应用效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功解决了现有重新照明技术的局限性，实现了高质量、实时性能的重新照明效果。&lt;h4&gt;翻译&lt;/h4&gt;混合现实场景重新照明，其中虚拟光照条件的变化与物理对象真实交互，产生逼真的照明和阴影效果，可用于多种应用。房地产中的一个应用可以是可视化一天中不同时间的房间并放置虚拟灯具。现有的基于深度学习的重新照明技术通常超出当前MR设备的实时性能能力。另一方面，场景理解方法（如设备端场景重建）由于扫描限制往往产生不准确结果，进而影响重新照明质量。最后，简单的基于2D图像滤波的方法无法表示复杂几何形状和阴影。我们引入了一种新颖方法，整合图像分割、通过各向异性扩散进行光照传播、基本场景理解和滤波技术的计算简便性。我们的方法纠正了设备端扫描的不准确性，在边缘设备上实时提供视觉上吸引人且准确的重新照明效果，实现高达100fps的速度。我们展示了我们方法与行业标准的直接比较，并在上述房地产示例中展示了我们方法的实际应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在混合现实(MR)设备上实现实时、高质量场景重照明(relighting)的问题。现有技术面临三个主要挑战：深度学习方法计算量大无法实时运行；设备场景重建因扫描限制产生不准确结果；简单2D滤波无法处理复杂几何和阴影。这个问题很重要，因为它能让虚拟灯光变化与物理对象真实交互，产生逼真的照明和阴影效果，在房地产可视化、虚拟家具摆放、室内设计规划等场景有广泛应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有技术的局限性，然后设计了一种混合方法结合计算效率和语义理解。他们认识到简化网格带来的视觉质量问题，利用RGB图像中的深度学习特征提取物体边界，再应用特征引导的各向异性扩散来平滑物体内部阴影同时保持锐利边缘。为提高效率，他们引入了级联扩散策略。该方法借鉴了深度超分辨率领域的工作(特别是Metzger等人的引导深度超分辨率方法)，将其应用于重照明场景，同时结合了图像滤波技术和神经渲染方法的思想，但进行了优化以适应实时应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合网格感知的滤波方法和引导的各向异性扩散，实现高质量实时重照明。整体流程包括：1)网格感知滤波重建场景并初步渲染；2)使用深度学习提取RGB特征引导各向异性扩散，校正网格 inaccuracies；3)采用级联扩散策略在多分辨率上高效处理；4)对阴影进行单独优化处理，保持形状准确性和柔和度；5)利用深度超分辨率训练框架训练适合边缘设备的模型。这种方法既利用了3D几何信息，又保持了计算效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合重照明方法结合深度学习和滤波技术；2)改进的各向异性扩散过程优化了迭代次数；3)级联扩散策略显著提高效率；4)阴影渲染的单独处理通道；5)从深度超分辨率模型的可转移训练。相比之前工作，这种方法避免了纯深度学习方法的计算负担和预训练需求，解决了基于场景重建方法的网格质量问题，超越了简单2D滤波方法的几何限制，实现了在边缘设备上的实时高质量重照明。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Hybrelighter通过结合深度各向异性扩散和场景重建，实现了在混合现实设备上的实时、高质量场景重照明，解决了现有技术在实时性能、几何准确性和视觉质量方面的平衡问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mixed Reality scene relighting, where virtual changes to lighting conditionsrealistically interact with physical objects, producing authentic illuminationand shadows, can be used in a variety of applications. One such application inreal estate could be visualizing a room at different times of day and placingvirtual light fixtures. Existing deep learning-based relighting techniquestypically exceed the real-time performance capabilities of current MR devices.On the other hand, scene understanding methods, such as on-device scenereconstruction, often yield inaccurate results due to scanning limitations, inturn affecting relighting quality. Finally, simpler 2D image filter-basedapproaches cannot represent complex geometry and shadows. We introduce a novelmethod to integrate image segmentation, with lighting propagation viaanisotropic diffusion on top of basic scene understanding, and thecomputational simplicity of filter-based techniques. Our approach correctson-device scanning inaccuracies, delivering visually appealing and accuraterelighting effects in real-time on edge devices, achieving speeds as high as100 fps. We show a direct comparison between our method and the industrystandard, and present a practical demonstration of our method in theaforementioned real estate example.</description>
      <author>example@mail.com (Hanwen Zhao, John Akers, Baback Elmieh, Ira Kemelmacher-Shlizerman)</author>
      <guid isPermaLink="false">2508.14930v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case Study</title>
      <link>http://arxiv.org/abs/2508.16555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了将讽刺作为预训练步骤对仇恨言论检测的影响，比较了两种训练策略在不同模型上的效果，发现讽刺预训练能有效提高模型检测隐式和显式仇恨的能力。&lt;h4&gt;背景&lt;/h4&gt;检测非直接形式的仇恨言论，如反讽、讽刺和暗示，仍然是社交网络面临的持续挑战。&lt;h4&gt;目的&lt;/h4&gt;探索将讽刺作为预训练步骤是否能提高隐式仇恨言论检测，并进而提高显式仇恨言论检测。&lt;h4&gt;方法&lt;/h4&gt;整合ETHOS、Reddit讽刺和隐式仇恨语料库的样本，设计了两种训练策略：单步训练方法（在仅训练讽刺的模型上测试仇恨言论）和顺序迁移学习（对讽刺、隐式仇恨和显式仇恨进行微调），比较讽刺预训练在CNN+LSTM和BERT+BiLSTM模型上的效果。&lt;h4&gt;主要发现&lt;/h4&gt;在ETHOS上，讽刺预训练将BERT+BiLSTM的召回率提高了9.7%，AUC提高了7.8%，F1分数提高了6%；在隐式仇恨语料库上，当仅测试隐式样本时，精度提高了7.8%。&lt;h4&gt;结论&lt;/h4&gt;通过将讽刺纳入训练过程，模型可以更有效地检测隐式和显式仇恨。&lt;h4&gt;翻译&lt;/h4&gt;检测非直接形式的仇恨言论，如反讽、讽刺和暗示，仍然是社交网络面临的持续挑战。尽管讽刺和仇恨言论被视为不同的表达方式，我们的工作探索了将讽刺作为预训练步骤是否能提高隐式仇恨言论检测，并进而提高显式仇恨言论检测。通过整合ETHOS、Reddit讽刺和隐式仇恨语料库的样本，我们设计了两种训练策略来比较讽刺预训练在CNN+LSTM和BERT+BiLSTM模型上的效果。第一种策略是单步训练方法，即在仅训练讽刺的模型上测试仇恨言论。第二种策略使用顺序迁移学习，对讽刺、隐式仇恨和显式仇恨进行微调。我们的结果显示，在ETHOS上，讽刺预训练将BERT+BiLSTM的召回率提高了9.7%，AUC提高了7.8%，F1分数提高了6%。在隐式仇恨语料库上，当仅测试隐式样本时，精度提高了7.8%。通过将讽刺纳入训练过程，我们表明模型可以更有效地检测隐式和显式仇恨。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting hate speech in non-direct forms, such as irony, sarcasm, andinnuendos, remains a persistent challenge for social networks. Although sarcasmand hate speech are regarded as distinct expressions, our work explores whetherintegrating sarcasm as a pre-training step improves implicit hate speechdetection and, by extension, explicit hate speech detection. Incorporatingsamples from ETHOS, Sarcasm on Reddit, and Implicit Hate Corpus, we devised twotraining strategies to compare the effectiveness of sarcasm pre-training on aCNN+LSTM and BERT+BiLSTM model. The first strategy is a single-step trainingapproach, where a model trained only on sarcasm is then tested on hate speech.The second strategy uses sequential transfer learning to fine-tune models forsarcasm, implicit hate, and explicit hate. Our results show that sarcasmpre-training improved the BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, andF1-score by 6% on ETHOS. On the Implicit Hate Corpus, precision increased by7.8% when tested only on implicit samples. By incorporating sarcasm into thetraining process, we show that models can more effectively detect both implicitand explicit hate.</description>
      <author>example@mail.com (Angelly Cabrera, Linus Lei, Antonio Ortega)</author>
      <guid isPermaLink="false">2508.16555v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Pretraining for Neural Regression</title>
      <link>http://arxiv.org/abs/2508.16355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NIAQUE是一种新的神经网络模型，专为概率回归的迁移学习而设计，通过预训练和微调策略提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;概率回归的迁移学习研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;引入NIAQUE（Neural Interpretable Any-Quantile Estimation）模型，设计用于概率回归的迁移学习。&lt;h4&gt;方法&lt;/h4&gt;通过排列不变性设计NIAQUE模型，在多样化的下游回归数据集上预训练，然后在特定目标数据集上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;预训练和微调策略能提高单个回归任务的性能，展示了概率迁移学习的积极影响；在Kaggle竞赛中，NIAQUE与基于树的模型和最近的神经基础模型相比表现有效。&lt;h4&gt;结论&lt;/h4&gt;NIAQUE是一个稳健且可扩展的概率回归框架，利用迁移学习提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;概率回归的迁移学习研究仍然不足。这项工作通过引入NIAQUE（神经可解释任意分位数估计），一种通过排列不变性为概率回归迁移学习设计的新模型，填补了这一空白。我们证明在多样化的下游回归数据集上直接预训练NIAQUE，并在特定目标数据集上微调，可以提高单个回归任务的性能，展示了概率迁移学习的积极影响。此外，我们在Kaggle竞赛中突出了NIAQUE的有效性，与涉及基于树的模型和最近的神经基础模型TabPFN和TabDPT的强基线相比。这些发现强调了NIAQUE作为概率回归的稳健且可扩展框架的有效性，利用迁移学习来提高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning for probabilistic regression remains underexplored. Thiswork closes this gap by introducing NIAQUE, Neural Interpretable Any-QuantileEstimation, a new model designed for transfer learning in probabilisticregression through permutation invariance. We demonstrate that pre-trainingNIAQUE directly on diverse downstream regression datasets and fine-tuning it ona specific target dataset enhances performance on individual regression tasks,showcasing the positive impact of probabilistic transfer learning. Furthermore,we highlight the effectiveness of NIAQUE in Kaggle competitions against strongbaselines involving tree-based models and recent neural foundation modelsTabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust andscalable framework for probabilistic regression, leveraging transfer learningto enhance predictive performance.</description>
      <author>example@mail.com (Boris N. Oreshkin, Shiv Tavker, Dmitry Efimov)</author>
      <guid isPermaLink="false">2508.16355v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach</title>
      <link>http://arxiv.org/abs/2508.16161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时空任务常因传感器缺失或无法访问而遇到数据不完整问题，时空克里金法对推断缺失时间信息至关重要。然而，当前模型难以确保推断时空模式的有效性和泛化能力，特别是在捕捉动态空间依赖和时间偏移方面。为解决这些问题，作者提出了STA-GANN框架，包含解耦相位模块、动态数据驱动元数据图建模和对抗性迁移学习策略。在九个数据集上的验证和理论证据均证明了其优越性能。&lt;h4&gt;背景&lt;/h4&gt;时空任务经常遇到数据不完整的问题，这源于传感器缺失或无法访问，这使得时空克里金法对于推断完全缺失的时间信息至关重要。&lt;h4&gt;目的&lt;/h4&gt;克服现有模型的局限性，提出一种新的基于图神经网络的克里金法框架，提高时空模式的有效性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出Spatio-Temporal Aware Graph Adversarial Neural Network (STA-GANN)，包含三个关键组件：(1)解耦相位模块，用于感知并调整时间戳偏移；(2)动态数据驱动元数据图建模，使用时间和数据更新空间关系；(3)对抗性迁移学习策略，确保泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;STA-GANN在捕捉动态空间依赖和时间偏移方面表现出色，能够优化未知传感器的泛化能力，确保推断时空模式的有效性和泛化性。&lt;h4&gt;结论&lt;/h4&gt;STA-GANN是一种有效的时空克里金法框架，通过整合三个关键组件，显著提高了时空模式推断的有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;时空任务经常遇到因缺失或无法访问的传感器而产生的不完整数据，这使得时空克里金法对于推断完全缺失的时间信息至关重要。然而，当前模型难以确保推断的时空模式的有效性和泛化能力，特别是在捕捉动态空间依赖和时间偏移，以及优化未知传感器的泛化能力方面。为克服这些局限性，我们提出了时空感知图对抗神经网络(STA-GANN)，这是一种新的基于GNN的克里金法框架，提高了时空模式的有效性和泛化能力。STA-GANN集成了(1)解耦相位模块，用于感知和调整时间戳偏移；(2)动态数据驱动的元数据图建模，使用时间和数据更新空间关系；(3)对抗性迁移学习策略，确保泛化能力。在来自四个领域的九个数据集上的广泛验证和理论证据都证明了STA-GANN的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761045&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal tasks often encounter incomplete data arising from missing orinaccessible sensors, making spatio-temporal kriging crucial for inferring thecompletely missing temporal information. However, current models struggle withensuring the validity and generalizability of inferred spatio-temporalpatterns, especially in capturing dynamic spatial dependencies and temporalshifts, and optimizing the generalizability of unknown sensors. To overcomethese limitations, we propose Spatio-Temporal Aware Graph Adversarial NeuralNetwork (STA-GANN), a novel GNN-based kriging framework that improvesspatio-temporal pattern validity and generalization. STA-GANN integrates (i)Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)Dynamic Data-Driven Metadata Graph Modeling to update spatial relationshipsusing temporal data and metadata; (iii) An adversarial transfer learningstrategy to ensure generalizability. Extensive validation across nine datasetsfrom four fields and theoretical evidence both demonstrate the superiorperformance of STA-GANN.</description>
      <author>example@mail.com (Yujie Li, Zezhi Shao, Chengqing Yu, Tangwen Qian, Zhao Zhang, Yifan Du, Shaoming He, Fei Wang, Yongjun Xu)</author>
      <guid isPermaLink="false">2508.16161v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A nonstationary spatial model of PM2.5 with localized transfer learning from numerical model output</title>
      <link>http://arxiv.org/abs/2508.15978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合监管监测网络数据和数值模型输出来推断和预测空气污染数据的方法，特别关注使用非平稳协方差函数来适应空气污染数据的空间变化特性。&lt;h4&gt;背景&lt;/h4&gt;监管监测网络通常用于支持流行病学研究与环境政策决策，但这些监测站在空间上分布稀疏，并且倾向于布置在人口密集区域，无法全面反映空气污染的空间分布情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够整合监管监测数据与数值模型输出的方法，以更准确地进行空气污染数据的推断和预测，特别是针对空气污染数据空间变异性随位置变化的特性。&lt;h4&gt;方法&lt;/h4&gt;采用从数值模型输出中学习到的局部协方差参数，构建全局非平稳协方差，并将其纳入完全贝叶斯模型中。通过计算效率高的方式对非平稳结构进行建模，使贝叶斯模型具有可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;非平稳协方差函数能够适应空气污染数据的空间表面变化特性，结合数值模型输出可以提高空气污染数据的推断和预测准确性。&lt;h4&gt;结论&lt;/h4&gt;通过整合监管监测网络数据与数值模型输出，并利用非平稳协方差函数，可以更有效地进行空气污染数据的推断和预测，为流行病学研究与环境政策决策提供更全面的数据支持。&lt;h4&gt;翻译&lt;/h4&gt;监管监测网络的空气污染测量数据通常被用于支持流行病学研究与环境政策决策。然而，监管监测站在空间上分布稀疏，并且倾向于布置在人口密集区域。数值空气污染模型的输出可以与监测站的测量数据结合，用于空气污染数据的推断和预测。非平稳协方差函数使模型能够适应空间表面变异性随位置变化的数据，如空气污染数据。在本文中，我们采用从数值模型输出中学习到的局部协方差参数，构建全局非平稳协方差，并将其纳入完全贝叶斯模型中。我们以计算高效的方式对非平稳结构进行建模，使贝叶斯模型具有可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ambient air pollution measurements from regulatory monitoring networks areroutinely used to support epidemiologic studies and environmental policydecision making. However, regulatory monitors are spatially sparse andpreferentially located in areas with large populations. Numerical air pollutionmodel output can be leveraged into the inference and prediction of airpollution data combining with measurements from monitors. Nonstationarycovariance functions allow the model to adapt to spatial surfaces whosevariability changes with location like air pollution data. In the paper, weemploy localized covariance parameters learned from the numerical output modelto knit together into a global nonstationary covariance, to incorporate in afully Bayesian model. We model the nonstationary structure in a computationallyefficient way to make the Bayesian model scalable.</description>
      <author>example@mail.com (Wenlong Gong, Brian J. Reich, Joseph Guinness)</author>
      <guid isPermaLink="false">2508.15978v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</title>
      <link>http://arxiv.org/abs/2508.15868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, to appear in EMNLP25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CARFT的对比学习与标注思维链强化微调方法，用于提升大语言模型的推理性能，解决了现有方法中模型崩溃和训练不稳定的问题。&lt;h4&gt;背景&lt;/h4&gt;推理能力在大语言模型的广泛应用中起着至关重要的作用。为增强LLMs的推理性能，已有多种基于强化学习的微调方法被提出，以解决仅通过监督微调训练的LLMs泛化能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于标注思维链的对比强化微调方法(CARFT)，以增强LLMs的推理性能，同时解决现有方法中忽视标注思维链和过度强调标注思维链导致的局限性。&lt;h4&gt;方法&lt;/h4&gt;为每个思维链学习一个表示，基于此表示设计新的对比信号来指导微调过程，充分利用可用的标注思维链，并通过引入额外的无监督学习信号稳定微调过程。&lt;h4&gt;主要发现&lt;/h4&gt;通过三种基线方法、两种基础模型和两个数据集进行的综合实验表明，CARFT在鲁棒性、性能(提升高达10.15%)和效率(提升高达30.62%)方面具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;CARFT方法有效解决了现有RL方法和SFT方法的局限性，显著提升了LLMs的推理性能，同时保持了训练的稳定性。&lt;h4&gt;翻译&lt;/h4&gt;推理能力在大语言模型的广泛应用中起着至关重要的作用。为了增强LLMs的推理性能，已经提出了多种基于强化学习的微调方法，以解决仅通过监督微调训练的LLMs有限的泛化能力。尽管这些方法有效，但两个主要限制阻碍了LLMs的发展。首先，基础RL方法忽略了标注的思维链并融入不稳定的推理路径采样，通常导致模型崩溃、训练过程不稳定和次优性能。其次，现有SFT方法通常过度强调标注的思维链，可能由于未能充分利用潜在的思维链而导致性能下降。在本文中，我们提出了一种基于标注思维链的对比强化微调方法，即CARFT，以增强LLMs的推理性能并解决上述局限性。具体而言，我们为每个思维链学习一个表示。基于这个表示，我们设计了新的对比信号来指导微调过程。我们的方法不仅充分利用了可用的标注思维链，还通过引入额外的无监督学习信号稳定了微调过程。我们通过三种基线方法、两种基础模型和两个数据集进行了全面的实验和深入分析，证明了CARFT在鲁棒性、性能(高达10.15%)和效率(高达30.62%)方面的显著优势。代码可在https://github.com/WNQzhu/CARFT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning capability plays a significantly critical role in the the broadapplications of Large Language Models (LLMs). To enhance the reasoningperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuningapproaches have been proposed to address the limited generalization capabilityof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite theireffectiveness, two major limitations hinder the advancement of LLMs. First,vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) andincorporate unstable reasoning path sampling, which typically results in modelcollapse, unstable training process, and suboptimal performance. Second,existing SFT approaches generally overemphasize the annotated CoT, potentiallyleading to performance degradation due to insufficient exploitation ofpotential CoT. In this paper, we propose a Contrastive learning with annotatedCoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance thereasoning performance of LLMs while addressing the aforementioned limitations.Specifically, we propose learning a representation for each CoT. Based on thisrepresentation, we design novel contrastive signals to guide the fine-tuningprocess. Our approach not only fully exploits the available annotated CoT butalso stabilizes the fine-tuning procedure by incorporating an additionalunsupervised learning signal. We conduct comprehensive experiments and in-depthanalysis with three baseline approaches, two foundation models, and twodatasets to demonstrate significant advantages of \TheName{} in terms ofrobustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Codeis available at https://github.com/WNQzhu/CARFT.</description>
      <author>example@mail.com (Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang)</author>
      <guid isPermaLink="false">2508.15868v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks</title>
      <link>http://arxiv.org/abs/2508.16459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Authors Ali Emre Balc{\i} and Erhan Ege Keyvan contributed equally to  this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于高斯过程的地标表示的新型SLAM方法，通过基于物体的轮廓建模实现环境表示，并在贝叶斯框架下进行联合推断。&lt;h4&gt;背景&lt;/h4&gt;传统SLAM方法通常使用网格地图或点云配准来表示环境，但这些方法可能无法充分利用物体的语义信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供物体语义信息、支持概率测量关联并提供形状置信边界的SLAM方法。&lt;h4&gt;方法&lt;/h4&gt;使用基于高斯过程的轮廓表示来对每个物体进行建模，通过递归方案在线更新轮廓，在完全贝叶斯框架内公式化SLAM问题，实现机器人姿态和基于物体地图的联合推断。&lt;h4&gt;主要发现&lt;/h4&gt;基于GP的轮廓表示能够提供物体的语义信息(如数量和面积)，支持概率测量到物体的关联，并提供物体形状的置信边界。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在合成和真实世界实验中表现出色，能够在多种结构化环境中提供准确的定位和地图构建性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的SLAM方法，它采用基于高斯过程的地标(物体)表示。与传统的网格地图或点云配准不同，我们使用基于GP的轮廓表示对每个物体进行环境建模。这些轮廓通过递归方案在线更新，实现了高效的内存使用。SLAM问题在完全贝叶斯框架内公式化，允许对机器人姿态和基于物体的地图进行联合推断。这种表示提供了语义信息，如物体数量和它们的面积，同时支持概率测量到物体的关联。此外，基于GP的轮廓提供了物体形状的置信边界，为安全导航和探索等下游任务提供了有价值的信息。我们在合成和真实世界实验中验证了我们的方法，并表明它在多种结构化环境中提供了准确的定位和地图构建性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光SLAM中环境表示的效率与质量问题。传统SLAM方法使用网格地图或点云配准，存在内存效率低、缺乏语义信息、难以提供置信度边界等缺点。这个问题很重要，因为自主机器人需要准确的环境表示进行导航和决策，置信度边界对安全导航至关重要，而内存高效的表示对资源受限的机器人平台尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有SLAM方法的局限性，然后考虑基于物体的地图表示作为替代方案。他们回顾了相关工作，包括视觉SLAM中的物体表示和隐函数表示物体边界的方法，发现这些方法在处理局部更新和概率分布方面存在不足。基于这些分析，作者提出使用高斯过程建模物体轮廓，借鉴了高斯过程在扩展目标跟踪和星凸集表示物体形状方面的现有工作，但将其整合到一个新的SLAM框架中，并针对SLAM的特殊需求进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于物体的地图表示，用高斯过程建模物体轮廓，提供形状估计和置信度边界，并将SLAM问题表述为完整的贝叶斯推断问题。整体流程包括：接收激光雷达点云和里程计输入；使用里程计预测机器人状态；计算物体-测量对的似然值进行数据关联；对未关联测量聚类初始化新物体；使用迭代扩展卡尔曼滤波联合更新机器人位姿和物体形状；输出轨迹和地图估计；将结果反馈处理下一时刻输入。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：基于高斯过程的物体轮廓表示提供置信度边界；完整的贝叶斯框架联合估计机器人位姿和物体形状；基于似然的数据关联自动拒绝离群值；内存高效的表示随环境增大内存需求增长缓慢。相比之前的工作，与网格地图方法相比提供了物体级语义信息和置信度边界；与点云配准方法相比计算复杂度更低；与其他基于物体的表示方法相比支持局部更新和空间域明确定义的概率分布。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于高斯过程的物体表示SLAM框架，能够高效地构建具有置信度边界的物体级地图，同时实现准确的机器人定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel Simultaneous Localization and Mapping (SLAM) method thatemploys Gaussian Process (GP) based landmark (object) representations. Insteadof conventional grid maps or point cloud registration, we model the environmenton a per object basis using GP based contour representations. These contoursare updated online through a recursive scheme, enabling efficient memory usage.The SLAM problem is formulated within a fully Bayesian framework, allowingjoint inference over the robot pose and object based map. This representationprovides semantic information such as the number of objects and their areas,while also supporting probabilistic measurement to object associations.Furthermore, the GP based contours yield confidence bounds on object shapes,offering valuable information for downstream tasks like safe navigation andexploration. We validate our method on synthetic and real world experiments,and show that it delivers accurate localization and mapping performance acrossdiverse structured environments.</description>
      <author>example@mail.com (Ali Emre Balcı, Erhan Ege Keyvan, Emre Özkan)</author>
      <guid isPermaLink="false">2508.16459v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.16069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submit to AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型体素扩散模块(VDM)，用于增强点云数据中的体素级表示和扩散，解决了基于Transformer和状态空间模型的点云物体检测中体素表示的空间扩散能力受限问题。&lt;h4&gt;背景&lt;/h4&gt;点云物体检测领域最近越来越多地采用基于Transformer和状态空间模型(SSMs)的方法，但这些模型中的体素表示需要输入和输出维度严格一致，限制了卷积操作的空间扩散能力，显著影响检测准确性。&lt;h4&gt;目的&lt;/h4&gt;受基于CNN的物体检测架构启发，提出一个新的体素扩散模块(VDM)来增强点云数据中的体素级表示和扩散，提高检测准确性。&lt;h4&gt;方法&lt;/h4&gt;VDM由稀疏3D卷积、子流形稀疏卷积和残差连接组成，输出特征图被下采样到原始输入分辨率的四分之一。VDM有两个主要功能：(1)通过稀疏3D卷积扩散前景体素特征，丰富空间上下文；(2)聚合细粒度空间信息，加强体素级特征表示。增强后的体素特征可无缝集成到主流的基于Transformer或SSM的检测模型中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，VDM方法在多个基准数据集上显著提高了检测准确性。VDM-SSMs在Waymo上达到74.7 mAPH (L2)，在nuScenes上达到72.9 NDS，在Argoverse 2上达到42.3 mAP，在ONCE上达到67.6 mAP，在所有数据集上都取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;VDM方法具有良好的通用性，可以无缝集成到不同的检测模型中，有效提升检测准确性。该方法的代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;最近，点云物体检测领域的进展越来越多地采用基于Transformer和状态空间模型的方法，展现出强大的性能。然而，这些模型中的基于体素的表示由于序列化处理需要输入和输出维度严格一致，这限制了卷积操作通常提供的空间扩散能力，显著影响了检测准确性。受基于CNN的物体检测架构启发，我们提出了一种新型的体素扩散模块(VDM)，以增强点云数据中的体素级表示和扩散。VDM由稀疏3D卷积、子流形稀疏卷积和残差连接组成。为确保计算效率，输出特征图被下采样到原始输入分辨率的四分之一。VDM有两个主要功能：(1)通过稀疏3D卷积扩散前景体素特征，丰富空间上下文；(2)聚合细粒度空间信息，加强体素级特征表示。VDM产生的增强体素特征可以无缝集成到主流的基于Transformer或SSM的检测模型中，用于准确的目标分类和定位，突显了我们方法的通用性。我们通过将VDM嵌入到基于Transformer和SSM的模型中，在多个基准数据集上评估了VDM。实验结果表明，我们的方法在所有基线模型上持续提高了检测准确性。具体来说，VDM-SSMs在Waymo上达到74.7 mAPH (L2)，在nuScenes上达到72.9 NDS，在Argoverse 2上达到42.3 mAP，在ONCE上达到67.6 mAP，在所有数据集上都设定了新的最先进性能。我们的代码将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于Transformer和状态空间模型(SSM)的3D目标检测中，体素化表示方法因输入输出维度限制而缺乏空间扩散能力的问题。这个问题在自动驾驶和机器人导航等应用中至关重要，因为这些应用需要精确的3D物体检测，而空间扩散对于提升检测精度，特别是在复杂场景中检测小物体和远处物体非常关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受CNN-based目标检测架构启发，认识到卷积操作在空间扩散方面的优势。他们发现现有Transformer和SSM模型在处理体素序列时缺乏这种能力，因此提出在序列化前进行体素扩散。方法借鉴了SAFDNet中的稀疏3D卷积和残差连接，以及LION中的体素扩散概念，但创新性地将扩散操作放在序列化之前，而非中间处理过程中。VDM设计为模块化结构，可以无缝集成到现有的Transformer或SSM检测模型中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在体素序列化之前通过3D卷积操作进行体素扩散，增加有意义体素信息的密度，同时利用稀疏卷积聚合局部细粒度空间特征，为下游序列模型提供更丰富的空间上下文。整体流程为：输入点云→体素化→应用VDM模块(包含稀疏3D卷积、子流形3D卷积和残差块)→下采样至1/4分辨率→序列化→输入Transformer或SSM模型→输出检测结果。VDM通过扩散前景体素特征和聚合细粒度空间信息来增强体素表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出统一的Voxel Diffusion Module (VDM)；2) 在序列化前进行体素扩散，而非中间处理；3) 结合稀疏3D卷积和子流形稀疏卷积实现双重功能；4) 设计为通用模块，可同时兼容Transformer和SSM架构；5) 通过下采样保持计算效率。相比之前工作，VDM不同于LION在中间处理中通过选择top-k体素进行扩散，也不同于传统CNN方法或UniMamba等混合方法，它明确分离了体素扩散和细粒度聚合功能，专注于增强序列模型的前处理阶段。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通用的体素扩散模块(VDM)，通过在序列化前进行3D卷积操作增强体素特征的空间扩散和细粒度聚合，显著提升了基于Transformer和状态空间模型的3D目标检测性能，并在多个基准数据集上达到了最先进的检测结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in point cloud object detection have increasingly adoptedTransformer-based and State Space Models (SSMs), demonstrating strongperformance. However, voxelbased representations in these models require strictconsistency in input and output dimensions due to their serialized processing,which limits the spatial diffusion capability typically offered byconvolutional operations. This limitation significantly affects detectionaccuracy. Inspired by CNN-based object detection architectures, we propose anovel Voxel Diffusion Module (VDM) to enhance voxel-level representation anddiffusion in point cloud data. VDM is composed of sparse 3D convolutions,submanifold sparse convolutions, and residual connections. To ensurecomputational efficiency, the output feature maps are downsampled to one-fourthof the original input resolution. VDM serves two primary functions: (1)diffusing foreground voxel features through sparse 3D convolutions to enrichspatial context, and (2) aggregating fine-grained spatial information tostrengthen voxelwise feature representation. The enhanced voxel featuresproduced by VDM can be seamlessly integrated into mainstream Transformer- orSSM-based detection models for accurate object classification and localization,highlighting the generalizability of our method. We evaluate VDM on severalbenchmark datasets by embedding it into both Transformerbased and SSM-basedmodels. Experimental results show that our approach consistently improvesdetection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAPon ONCE, setting new stateof-the-art performance across all datasets. Our codewill be made publicly available.</description>
      <author>example@mail.com (Qifeng Liu, Dawei Zhao, Yabo Dong, Linzhi Shang, Liang Xiao, Juan Wang, Kunkong Zhao, Dongming Lu, Qi Zhu)</author>
      <guid isPermaLink="false">2508.16069v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars</title>
      <link>http://arxiv.org/abs/2508.16030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCCN 2025 (IEEE International Conference on Computer  Communications and Networks), Tokyo, Japan, August 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;汽车FMCW雷达在恶劣天气条件下可靠，但稀疏点云限制了3D物体检测。作者发布了CoVeRaP数据集并提出协同感知框架，通过多车辆雷达数据融合显著提高检测性能。&lt;h4&gt;背景&lt;/h4&gt;汽车FMCW雷达在雨和强光条件下仍然可靠，但它们的稀疏、嘈杂点云限制了3D物体检测能力。&lt;h4&gt;目的&lt;/h4&gt;发布CoVeRaP数据集并提出统一协同感知框架，解决多车辆FMCW雷达感知问题，提高3D物体检测的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;构建包含21k帧的协同数据集，对齐多辆车的雷达、摄像头和GPS流；提出具有中期和晚期融合选项的统一协同感知框架；使用多分支PointNet风格编码器增强自注意力机制，融合空间、多普勒和强度线索到公共潜在空间，由解码器转换为3D边界框和深度置信度。&lt;h4&gt;主要发现&lt;/h4&gt;中期融合与强度编码相结合，在IoU 0.9条件下将平均精度提高了高达9倍，并且一致优于单车辆基线。&lt;h4&gt;结论&lt;/h4&gt;CoVeRaP建立了首个可复现的多车辆FMCW雷达感知基准，证明了经济实惠的雷达共享显著提高了检测鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;汽车FMCW雷达在雨和强光条件下仍然可靠，但它们的稀疏、嘈杂点云限制了3D物体检测。因此，我们发布了CoVeRaP，这是一个包含21k帧的协同数据集，对齐了多辆车在不同机动过程中的雷达、摄像头和GPS流。基于这些数据，我们提出了一个具有中期和晚期融合选项的统一协同感知框架。其基线网络采用多分支PointNet风格编码器，增强自注意力机制，将空间、多普勒和强度线索融合到公共潜在空间，解码器将其转换为3D边界框和每点深度置信度。实验表明，中期融合与强度编码相结合，在IoU 0.9条件下将平均精度提高了高达9倍，并且一致优于单车辆基线。CoVeRaP因此建立了首个可复现的多车辆FMCW雷达感知基准，并证明了经济实惠的雷达共享显著提高了检测鲁棒性。数据集和代码已公开可用，以鼓励进一步研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单个车辆FMCW雷达产生的点云数据稀疏、噪声大，以及存在盲区的问题。这个问题在自动驾驶和高级驾驶辅助系统中至关重要，因为单传感器在复杂环境下（如恶劣天气、强光）感知能力有限，而通过车辆间共享雷达数据可以克服这些限制，生成更密集的点云，提供更准确的3D目标检测，从而提高驾驶安全性和系统鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到单传感器在复杂环境下的局限性，特别是雷达数据的稀疏性和噪声问题，然后提出通过多车辆合作感知来增强环境感知能力。在设计方法时，作者考虑了不同的数据融合策略（早期、中期和晚期融合），并针对雷达数据的特殊性设计了专门处理流程。该方法借鉴了PointNet的架构处理无序点集，多分支架构处理不同特征，自注意力机制融合特征，以及V2V4Real数据集的同步方法和PointNet++的分层策略来捕获多尺度结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多车辆共享毫米波FMCW雷达数据来增强环境感知，使用特征级融合和预测级融合两种策略，并设计一个多分支、注意力增强的骨干网络来融合空间、多普勒和强度线索。整体实现流程包括：1) 使用配备FMCW雷达、GPS-RTK和RGB相机的多车辆平台收集数据；2) 对数据进行预处理，包括生成3D点云、动态选择和过滤点、开发地面真实标签；3) 实现两种融合策略（中期融合在共同坐标系中融合特征，晚期融合独立预测后合并结果）；4) 设计多模态信号编码（位置、动态、强度三个分支）、上下文特征合成和输出解码的基线模型；5) 在不同IoU阈值下评估检测性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) CoVeRaP数据集，首个大规模合作雷达感知数据集，整合多车辆雷达、相机和GPS数据；2) 统一系统架构，支持特征级和预测级融合策略；3) 基线3D边界框检测模型，利用多分支架构和注意力机制，特别强调信号强度特征的重要性；4) 创新的数据处理方法，包括动态点选择、基于强度的过滤和事件触发同步。相比之前工作，本文专注于毫米波FMCW雷达而非激光雷达，专门处理雷达数据的稀疏性和噪声问题，强调信号强度特征的重要性，并提供可复现基准和开源代码。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoVeRaP通过首个大规模毫米波雷达合作感知数据集和融合框架，证明了车辆间共享雷达数据能显著提高3D目标检测的准确性和鲁棒性，特别是在高IoU阈值下性能提升可达9倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automotive FMCW radars remain reliable in rain and glare, yet their sparse,noisy point clouds constrain 3-D object detection. We therefore releaseCoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, andGPS streams from multiple vehicles across diverse manoeuvres. Built on thisdata, we propose a unified cooperative-perception framework with middle- andlate-fusion options. Its baseline network employs a multi-branch PointNet-styleencoder enhanced with self-attention to fuse spatial, Doppler, and intensitycues into a common latent space, which a decoder converts into 3-D boundingboxes and per-point depth confidence. Experiments show that middle fusion withintensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 andconsistently outperforms single-vehicle baselines. CoVeRaP thus establishes thefirst reproducible benchmark for multi-vehicle FMCW-radar perception anddemonstrates that affordable radar sharing markedly improves detectionrobustness. Dataset and code are publicly available to encourage furtherresearch.</description>
      <author>example@mail.com (Jinyue Song, Hansol Ku, Jayneel Vora, Nelson Lee, Ahmad Kamari, Prasant Mohapatra, Parth Pathak)</author>
      <guid isPermaLink="false">2508.16030v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System</title>
      <link>http://arxiv.org/abs/2508.15990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GelSLAM，一个仅依靠触觉感知的实时3D SLAM系统，用于长时间估计物体姿态并高保真重建物体形状。&lt;h4&gt;背景&lt;/h4&gt;精确感知物体的姿态和形状对于精确抓取和操作至关重要。与基于视觉的方法相比，触觉感知在跟踪和重建接触物体时具有精度高和对遮挡免疫的优势。&lt;h4&gt;目的&lt;/h4&gt;开发一个仅依靠触觉感知的实时3D SLAM系统，用于长时间估计物体姿态并高保真重建物体形状。&lt;h4&gt;方法&lt;/h4&gt;GelSLAM使用触觉导出的表面法线和曲率进行稳健的跟踪和回环闭合，而非传统的点云方法。&lt;h4&gt;主要发现&lt;/h4&gt;GelSLAM能够以低误差和最小漂移实时跟踪物体运动，并以亚毫米精度重建形状，即使是对于低纹理物体（如木制工具）也是如此。&lt;h4&gt;结论&lt;/h4&gt;GelSLAM将触觉感知扩展到局部接触之外，实现了全局、长期的空间感知，将为许多涉及手中物体的精确操作任务奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;Accurately perceiving an object's pose and shape is essential for precise grasping and manipulation. Compared to common vision-based methods, tactile sensing offers advantages in precision and immunity to occlusion when tracking and reconstructing objects in contact. This makes it particularly valuable for in-hand and other high-precision manipulation tasks. In this work, we present GelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to estimate object pose over long periods and reconstruct object shapes with high fidelity. Unlike traditional point cloud-based approaches, GelSLAM uses tactile-derived surface normals and curvatures for robust tracking and loop closure. It can track object motion in real time with low error and minimal drift, and reconstruct shapes with submillimeter accuracy, even for low-texture objects such as wooden tools. GelSLAM extends tactile sensing beyond local contact to enable global, long-horizon spatial perception, and we believe it will serve as a foundation for many precise manipulation tasks involving interaction with objects in hand. The video demo is available on our website: https://joehjhuang.github.io/gelslam.&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决仅使用触觉传感实现实时、长期目标跟踪和高保真度3D物体重建的问题。这个问题很重要，因为准确感知物体的姿态和形状对机器人精确抓取和操作至关重要，而触觉传感相比视觉方法在精度和抗遮挡方面有独特优势，特别适用于手中操作等高精度任务，还可应用于AR/VR、生物学、考古学等多个领域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了触觉传感的'盲人摸象'问题：每次触觉读数提供局部信息，难以构建全局理解。他们发现传统点云方法在触觉场景中表现不佳，因为接触产生的表面形变小，点云缺乏独特特征。关键洞察是使用微分表示（法线图和曲率图）而非点云，这能捕获丰富局部特征且与GelSight传感器原理一致。系统设计借鉴了现有工作如NormalFlow姿态估计、SLAM系统的关键帧策略、SIFT特征匹配和pose graph优化，但创新性地组合这些技术解决触觉SLAM问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用表面法线图和曲率图的微分表示而非传统点云来处理触觉数据，通过回环闭合机制克服触觉传感的局部性限制。整体流程包括：1)数据预处理：提取法线图、高度图、接触掩码和曲率图；2)跟踪模块：使用NormalFlow估计相对姿态，实现失败检测和关键帧选择；3)回环闭合模块：构建覆盖集，使用SIFT匹配和NormalFlow精炼检测回环，执行pose graph优化；4)重建模块：快速融合点云(在线)和Poisson表面重建(离线)生成最终3D模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)微分表示方法使用法线图和曲率图解决点云局限性；2)首个实现长期跟踪和高保真重建的纯触觉SLAM系统；3)鲁棒的回环检测机制结合SIFT和NormalFlow；4)失败检测机制评估对齐质量和重叠度；5)改进的关键帧选择策略。相比之前工作，GelSLAM在低纹理物体上表现更好，通过回环闭合减少46%旋转误差和17.5%平移误差，比Tac2Structure更鲁棒，且首个实现'野外'触觉only 3D重建，无需已知接触姿态或视觉辅助。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GelSLAM通过创新的微分表示方法和鲁棒回环检测，首次实现了仅使用触觉传感的长期目标跟踪和高保真度3D重建，克服了触觉传感的局部性限制，为机器人和多种应用领域提供了强大的触觉感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately perceiving an object's pose and shape is essential for precisegrasping and manipulation. Compared to common vision-based methods, tactilesensing offers advantages in precision and immunity to occlusion when trackingand reconstructing objects in contact. This makes it particularly valuable forin-hand and other high-precision manipulation tasks. In this work, we presentGelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing toestimate object pose over long periods and reconstruct object shapes with highfidelity. Unlike traditional point cloud-based approaches, GelSLAM usestactile-derived surface normals and curvatures for robust tracking and loopclosure. It can track object motion in real time with low error and minimaldrift, and reconstruct shapes with submillimeter accuracy, even for low-textureobjects such as wooden tools. GelSLAM extends tactile sensing beyond localcontact to enable global, long-horizon spatial perception, and we believe itwill serve as a foundation for many precise manipulation tasks involvinginteraction with objects in hand. The video demo is available on our website:https://joehjhuang.github.io/gelslam.</description>
      <author>example@mail.com (Hung-Jui Huang, Mohammad Amin Mirzaee, Michael Kaess, Wenzhen Yuan)</author>
      <guid isPermaLink="false">2508.15990v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds</title>
      <link>http://arxiv.org/abs/2508.14879v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeshCoder是一种新颖的框架，可将复杂的3D物体从点云重建为可编辑的Blender Python脚本，解决了现有方法依赖有限领域语言和小数据集的问题。&lt;h4&gt;背景&lt;/h4&gt;将3D物体重建为可编辑程序对逆向工程和形状编辑等应用至关重要，但现有方法通常依赖于有限领域的特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一种能够处理复杂3D物体并支持程序化编辑的重建框架。&lt;h4&gt;方法&lt;/h4&gt;引入MeshCoder框架，开发全面的Blender Python API集合来合成复杂几何形状，构建大规模成对物体-代码数据集，并训练多模态大语言模型将3D点云转换为可执行的Blender Python脚本。&lt;h4&gt;主要发现&lt;/h4&gt;MeshCoder在形状到代码重建任务中取得优越性能，通过代码修改实现直观的几何和拓扑编辑，且基于代码的表示增强了LLM在3D形状理解任务中的推理能力。&lt;h4&gt;结论&lt;/h4&gt;MeshCoder是一个强大而灵活的解决方案，用于程序化的3D形状重建和理解。&lt;h4&gt;翻译&lt;/h4&gt;将3D物体重建为可编辑程序对于逆向工程和形状编辑等应用至关重要。然而，现有方法通常依赖于有限领域的特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。为应对这些挑战，我们引入了MeshCoder，一种新颖的框架，可将复杂的3D物体从点云重建为可编辑的Blender Python脚本。我们开发了一套全面的Blender Python API集合，能够合成复杂的几何形状。利用这些API，我们构建了一个大规模的成对物体-代码数据集，其中每个物体的代码被分解为不同的语义部分。随后，我们训练了一个多模态大语言模型(LLM)，将3D点云转换为可执行的Blender Python脚本。我们的方法不仅在形状到代码重建任务中取得了优越的性能，还通过方便的代码修改促进了直观的几何和拓扑编辑。此外，我们的基于代码的表示增强了LLM在3D形状理解任务中的推理能力。这些贡献共同确立了MeshCoder作为程序化3D形状重建和理解的强大而灵活的解决方案。项目主页可在https://daibingquan.github.io/MeshCoder获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将复杂3D物体重建为可编辑程序的问题。这个问题在现实和研究中非常重要，因为现有的方法只能表示简单的基本形状，无法建模复杂几何结构，同时缺乏大规模训练数据。解决这一问题对于逆向工程、形状编辑和3D结构理解等应用至关重要，能够提高设计效率和灵活性，使3D模型更易于修改和重用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统DSL只能表示简单形状，且缺乏大规模训练数据。为此，他们设计了五种基本形状类型（基本形状、平移、桥接循环、布尔和阵列）来创建更复杂的几何形状。他们借鉴了现有的形状程序和基于部分表示的方法，但进行了扩展和改进。在数据集构建方面，作者先训练部分到代码的推断模型，然后利用这个模型构建整体物体-代码数据集，解决了数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; MeshCoder的核心思想是将3D点云转换为可执行的Blender Python脚本，这些脚本能够重建物体的各个语义部分，实现结构化和可编辑的网格重建。整体流程包括：1)开发表达性强的Blender Python API；2)构建大规模配对物体-代码数据集；3)训练多模态大语言模型；4)使用形状标记器将点云转换为固定长度标记；5)将标记输入LLM生成Blender Python脚本，重建输入几何形状的不同语义部分。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开发全面的Blender Python API，合成复杂几何形状；2)提出新方法构建大规模配对物体-代码数据集；3)训练多模态大语言模型将点云转换为可执行脚本；4)通过代码表示实现直观的几何和拓扑编辑；5)增强LLM在3D形状理解中的推理能力。相比之前工作，MeshCoder不依赖有限DSL，使用更全面的API，构建了包含41个类别、约100万个物体的大规模数据集，能处理更复杂的几何形状，且生成的代码可直接编辑和修改3D模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MeshCoder通过将3D点云转换为可执行的Blender Python脚本，实现了复杂3D物体的结构化重建和可编辑表示，为逆向工程、形状编辑和3D结构理解提供了强大而灵活的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D objects into editable programs is pivotal for applicationslike reverse engineering and shape editing. However, existing methods oftenrely on limited domain-specific languages (DSLs) and small-scale datasets,restricting their ability to model complex geometries and structures. Toaddress these challenges, we introduce MeshCoder, a novel framework thatreconstructs complex 3D objects from point clouds into editable Blender Pythonscripts. We develop a comprehensive set of expressive Blender Python APIscapable of synthesizing intricate geometries. Leveraging these APIs, weconstruct a large-scale paired object-code dataset, where the code for eachobject is decomposed into distinct semantic parts. Subsequently, we train amultimodal large language model (LLM) that translates 3D point cloud intoexecutable Blender Python scripts. Our approach not only achieves superiorperformance in shape-to-code reconstruction tasks but also facilitatesintuitive geometric and topological editing through convenient codemodifications. Furthermore, our code-based representation enhances thereasoning capabilities of LLMs in 3D shape understanding tasks. Together, thesecontributions establish MeshCoder as a powerful and flexible solution forprogrammatic 3D shape reconstruction and understanding. The project homepage isavailable at \href{https://daibingquan.github.io/MeshCoder}{this link}.</description>
      <author>example@mail.com (Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang)</author>
      <guid isPermaLink="false">2508.14879v2</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer</title>
      <link>http://arxiv.org/abs/2508.16569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发并验证了RenalCLIP，一个用于肾脏肿瘤特征描述、诊断和预后的视觉语言基础模型，该模型在10项核心任务上表现出优越的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;无创评估偶然发现的肾脏肿瘤是泌尿肿瘤学中的一个关键挑战，诊断不确定性经常导致良性或惰性肿瘤的过度治疗。&lt;h4&gt;目的&lt;/h4&gt;开发并验证RenalCLIP模型，用于肾脏肿瘤的特征描述、诊断和预后。&lt;h4&gt;方法&lt;/h4&gt;使用来自9个中国医疗中心和公共TCIA队列的27,866次CT扫描数据集（来自8,809名患者），采用两阶段预训练策略：首先使用领域特定知识增强图像和文本编码器，然后通过对比学习目标对齐它们，创建强大的表示。&lt;h4&gt;主要发现&lt;/h4&gt;RenalCLIP在10项核心任务上表现出比其他最先进的通用CT基础模型更好的性能和泛化能力；在TCIA队列中的复发-无生存率预测等复杂任务上，C-index达到0.726，比领先的基线提高了约20%；预训练赋予了显著的数据效率，只需20%的训练数据即可达到所有基线模型的峰值性能；在报告生成、图像文本检索和零样本诊断任务上也表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;RenalCLIP提供了强大的工具，有望提高诊断准确性，优化预后分层，并个性化肾脏癌患者的管理。&lt;h4&gt;翻译&lt;/h4&gt;对偶然发现的肾脏肿块进行无创评估是泌尿肿瘤学中的一个关键挑战，其中诊断不确定性经常导致良性或惰性肿瘤的过度治疗。在本研究中，我们开发和验证了RenalCLIP，这是一个使用来自9个中国医疗中心和公共TCIA队列的8,809名患者的27,866次CT扫描数据集构建的视觉语言基础模型，用于肾脏肿块的特征描述、诊断和预后。该模型通过两阶段预训练策略开发，首先使用领域特定知识增强图像和文本编码器，然后通过对比学习目标对齐它们，以创建强大的表示，实现更好的泛化和诊断精度。与最先进的通用CT基础模型相比，RenalCLIP在涵盖肾脏癌症完整临床工作流程的10项核心任务上实现了更好的性能和优越的泛化能力。特别是在TCIA队列中的复发-无生存率预测等复杂任务上，RenalCLIP的C-index达到0.726，比领先的基线提高了约20%。此外，RenalCLIP的预训练赋予了显著的数据效率；在诊断分类任务中，它只需要20%的训练数据即可达到所有基线模型的峰值性能，即使它们在100%的数据上完全微调。此外，它在报告生成、图像文本检索和零样本诊断任务上也表现出优越性能。我们的研究结果表明，RenalCLIP提供了一个强大的工具，有可能提高诊断准确性，优化预后分层，并个性化肾脏癌患者的管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The non-invasive assessment of increasingly incidentally discovered renalmasses is a critical challenge in urologic oncology, where diagnosticuncertainty frequently leads to the overtreatment of benign or indolent tumors.In this study, we developed and validated RenalCLIP using a dataset of 27,866CT scans from 8,809 patients across nine Chinese medical centers and the publicTCIA cohort, a visual-language foundation model for characterization, diagnosisand prognosis of renal mass. The model was developed via a two-stagepre-training strategy that first enhances the image and text encoders withdomain-specific knowledge before aligning them through a contrastive learningobjective, to create robust representations for superior generalization anddiagnostic precision. RenalCLIP achieved better performance and superiorgeneralizability across 10 core tasks spanning the full clinical workflow ofkidney cancer, including anatomical assessment, diagnostic classification, andsurvival prediction, compared with other state-of-the-art general-purpose CTfoundation models. Especially, for complicated task like recurrence-freesurvival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726,representing a substantial improvement of approximately 20% over the leadingbaselines. Furthermore, RenalCLIP's pre-training imparted remarkable dataefficiency; in the diagnostic classification task, it only needs 20% trainingdata to achieve the peak performance of all baseline models even after theywere fully fine-tuned on 100% of the data. Additionally, it achieved superiorperformance in report generation, image-text retrieval and zero-shot diagnosistasks. Our findings establish that RenalCLIP provides a robust tool with thepotential to enhance diagnostic accuracy, refine prognostic stratification, andpersonalize the management of patients with kidney cancer.</description>
      <author>example@mail.com (Yuhui Tao, Zhongwei Zhao, Zilong Wang, Xufang Luo, Feng Chen, Kang Wang, Chuanfu Wu, Xue Zhang, Shaoting Zhang, Jiaxi Yao, Xingwei Jin, Xinyang Jiang, Yifan Yang, Dongsheng Li, Lili Qiu, Zhiqiang Shao, Jianming Guo, Nengwang Yu, Shuo Wang, Ying Xiong)</author>
      <guid isPermaLink="false">2508.16569v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Modal Prototype Augmentation and Dual-Grained Prompt Learning for Social Media Popularity Prediction</title>
      <link>http://arxiv.org/abs/2508.16147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的多模态框架，用于社交媒体流行度预测，通过层次原型、对比学习、双粒度提示学习和跨模态注意力机制，解决了现有方法的局限性，实验证明该方法在基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;社交媒体流行度预测是一个复杂的多模态任务，需要有效整合图像、文本和结构化信息。然而，当前方法存在视觉-文本对齐不足的问题，无法捕捉社交媒体数据中固有的跨内容关联和层次模式。&lt;h4&gt;目的&lt;/h4&gt;克服当前方法的局限性，建立多类别框架，提高视觉-文本对齐，实现精确的多模态表示，通过细粒度类别建模进行改进。&lt;h4&gt;方法&lt;/h4&gt;建立多类别框架，引入层次原型进行结构增强，使用对比学习改善视觉-文本对齐，提出特征增强框架，集成双粒度提示学习和跨模态注意力机制，通过细粒度类别建模实现精确的多模态表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示在基准指标上达到了最先进的性能，为多模态社交媒体分析建立了新的参考标准。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地解决了社交媒体流行度预测中的挑战，通过整合多模态信息和改进对齐机制，提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体流行度预测是一个复杂的多模态任务，需要有效整合图像、文本和结构化信息。然而，当前方法存在视觉-文本对齐不足的问题，无法捕捉社交媒体数据中固有的跨内容关联和层次模式。为了克服这些局限性，我们建立了一个多类别框架，引入层次原型进行结构增强，并使用对比学习改善视觉-文本对齐。此外，我们提出了一个特征增强框架，集成双粒度提示学习和跨模态注意力机制，通过细粒度类别建模实现精确的多模态表示。实验结果在基准指标上展示了最先进的性能，为多模态社交媒体分析建立了新的参考标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social Media Popularity Prediction is a complex multimodal task that requireseffective integration of images, text, and structured information. However,current approaches suffer from inadequate visual-textual alignment and fail tocapture the inherent cross-content correlations and hierarchical patterns insocial media data. To overcome these limitations, we establish a multi-classframework , introducing hierarchical prototypes for structural enhancement andcontrastive learning for improved vision-text alignment. Furthermore, wepropose a feature-enhanced framework integrating dual-grained prompt learningand cross-modal attention mechanisms, achieving precise multimodalrepresentation through fine-grained category modeling. Experimental resultsdemonstrate state-of-the-art performance on benchmark metrics, establishing newreference standards for multimodal social media analysis.</description>
      <author>example@mail.com (Ao Zhou, Mingsheng Tu, Luping Wang, Tenghao Sun, Zifeng Cheng, Yafeng Yin, Zhiwei Jiang, Qing Gu)</author>
      <guid isPermaLink="false">2508.16147v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing</title>
      <link>http://arxiv.org/abs/2508.15973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Habilitation \`a Diriger des Recherches (HDR) manuscript&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是作者关于计算机视觉和遥感领域标签高效学习的一系列精选贡献，重点研究如何从有限或部分标注的数据中有效学习，并利用大量未标注数据，特别关注解决地球观测数据的独特挑战。&lt;h4&gt;背景&lt;/h4&gt;研究背景是计算机视觉和遥感领域面临的标签数据稀缺问题，以及实际应用中需要处理多模态、空间分辨率变化和场景异构性等地球观测数据的特殊挑战。&lt;h4&gt;目的&lt;/h4&gt;开发并适应能够从有限或部分标注数据中有效学习的方法，同时能够利用大量未标注数据，以解决实际应用中的标签效率问题，特别是在地球观测领域。&lt;h4&gt;方法&lt;/h4&gt;研究围绕四个主要方法展开：(1)基于异常感知表示的弱监督学习；(2)多数据集联合训练的多任务学习；(3)多模态数据的自监督和监督对比学习；(4)结合类层次显式和隐式建模的少样本学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过多种方法在自然和遥感数据集上的实验，证明了所提出方法在目标发现和检测、目标检测和语义分割性能提升、遥感场景分类以及层次场景分类方面的有效性，反映了多个合作研究项目的成果。&lt;h4&gt;结论&lt;/h4&gt;标签高效学习在计算机视觉和遥感领域具有广阔的应用前景，未来的研究方向将聚焦于扩展和增强实际应用的标签高效学习技术。&lt;h4&gt;翻译&lt;/h4&gt;这篇手稿 presents a series of my selected contributions to the topic of label-efficient learning in computer vision and remote sensing. The central focus of this research is to develop and adapt methods that can learn effectively from limited or partially annotated data, and can leverage abundant unlabeled data in real-world applications. The contributions span both methodological developments and domain-specific adaptations, in particular addressing challenges unique to Earth observation data such as multi-modality, spatial resolution variability, and scene heterogeneity. The manuscript is organized around four main axes including (1) weakly supervised learning for object discovery and detection based on anomaly-aware representations learned from large amounts of background images; (2) multi-task learning that jointly trains on multiple datasets with disjoint annotations to improve performance on object detection and semantic segmentation; (3) self-supervised and supervised contrastive learning with multimodal data to enhance scene classification in remote sensing; and (4) few-shot learning for hierarchical scene classification using both explicit and implicit modeling of class hierarchies. These contributions are supported by extensive experimental results across natural and remote sensing datasets, reflecting the outcomes of several collaborative research projects. The manuscript concludes by outlining ongoing and future research directions focused on scaling and enhancing label-efficient learning for real-world applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This manuscript presents a series of my selected contributions to the topicof label-efficient learning in computer vision and remote sensing. The centralfocus of this research is to develop and adapt methods that can learneffectively from limited or partially annotated data, and can leverage abundantunlabeled data in real-world applications. The contributions span bothmethodological developments and domain-specific adaptations, in particularaddressing challenges unique to Earth observation data such as multi-modality,spatial resolution variability, and scene heterogeneity. The manuscript isorganized around four main axes including (1) weakly supervised learning forobject discovery and detection based on anomaly-aware representations learnedfrom large amounts of background images; (2) multi-task learning that jointlytrains on multiple datasets with disjoint annotations to improve performance onobject detection and semantic segmentation; (3) self-supervised and supervisedcontrastive learning with multimodal data to enhance scene classification inremote sensing; and (4) few-shot learning for hierarchical scene classificationusing both explicit and implicit modeling of class hierarchies. Thesecontributions are supported by extensive experimental results across naturaland remote sensing datasets, reflecting the outcomes of several collaborativeresearch projects. The manuscript concludes by outlining ongoing and futureresearch directions focused on scaling and enhancing label-efficient learningfor real-world applications.</description>
      <author>example@mail.com (Minh-Tan Pham)</author>
      <guid isPermaLink="false">2508.15973v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather</title>
      <link>http://arxiv.org/abs/2508.16408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型多传感器融合方法，专门针对恶劣天气条件下的自动驾驶应用，通过融合多种传感器提高了在挑战性环境中的物体检测和决策能力。&lt;h4&gt;背景&lt;/h4&gt;多模态传感器融合是自主机器人的基本能力，但现有方法在正常环境条件下表现良好，在恶劣天气（如浓雾、冰雪或污垢遮挡）条件下会失效。&lt;h4&gt;目的&lt;/h4&gt;开发专门针对恶劣天气条件的多传感器融合方法，提高自动驾驶车辆在挑战性天气条件下的可靠性，缩小理想条件与现实世界边缘情况之间的差距。&lt;h4&gt;方法&lt;/h4&gt;融合RGB、LiDAR、NIR门控相机和雷达等多种传感器，通过注意力和基于深度的混合方案融合数据，在鸟瞰图平面上进行学习优化，使用transformer解码器根据距离和可见性对模态进行加权预测检测结果。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在恶劣天气条件下显著提高了多传感器融合的可靠性，与次优方法相比，在远距离和挑战性雾天场景中，对弱势行人的平均精度提高了17.2 AP。&lt;h4&gt;结论&lt;/h4&gt;该方法有效缩小了理想条件与现实世界边缘情况之间的差距，显著提高了自动驾驶车辆在恶劣天气条件下的物体检测能力。&lt;h4&gt;翻译&lt;/h4&gt;多模态传感器融合是自主机器人的基本能力，能够在传感器失效或输入不确定的情况下进行物体检测和决策。尽管最近的融合方法在正常环境条件下表现出色，但这些方法在恶劣天气（如浓雾、冰雪或污垢遮挡）条件下会失效。我们引入了一种专门针对恶劣天气条件的新型多传感器融合方法。除了融合最近自动驾驶文献中使用的RGB和LiDAR传感器外，我们的传感器融合堆栈还能从NIR门控相机和雷达模态中学习，以应对低光照和恶劣天气。我们通过注意力和基于深度的混合方案融合多传感器数据，并在鸟瞰图平面上进行学习优化，以有效结合图像和范围特征。我们的检测由transformer解码器预测，该解码器根据距离和可见性对模态进行加权。我们证明，我们的方法提高了自动驾驶车辆在挑战性天气条件下多传感器融合的可靠性，缩小了理想条件与现实世界边缘情况之间的差距。与次优方法相比，我们的方法在远距离和挑战性雾天场景中对弱势行人的平均精度提高了17.2 AP。我们的项目页面可在https://light.princeton.edu/samfusion/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在恶劣天气条件下（如浓雾、大雪等）的3D物体检测问题。这个问题在自动驾驶领域至关重要，因为现有多传感器融合方法在理想天气条件下表现良好，但在恶劣天气下会显著失效，影响自动驾驶系统的安全性和可靠性。恶劣天气是自动驾驶面临的主要挑战之一，而准确检测远距离和弱势道路使用者在这些条件下对保障交通安全尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有多传感器融合方法在恶劣天气条件下的局限性，特别是它们过度依赖LiDAR生成查询和深度投影的弱点。然后，他们引入了门控摄像头和雷达这两种在恶劣天气下表现优异的传感器。方法设计上借鉴了现有的BEV表示方法和transformer架构，如BEVFusion和Carion等人的工作。同时，作者创新性地设计了深度引导的跨模态变换和早期融合机制，以及基于距离的多模态查询生成方法，避免了对单一传感器的过度依赖。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个传感器自适应的多模态融合框架，能够在恶劣天气条件下有效融合不同传感器的优势，并通过距离和天气感知的加权机制动态调整各传感器的重要性。整体流程包括：1)从RGB摄像头、门控摄像头、LiDAR和雷达提取特征；2)在多模态编码器中通过深度引导的跨模态变换和注意力机制融合不同传感器特征；3)在多模态解码器中生成基于距离加权的初始对象提议，并使用transformer解码器细化检测结果；4)通过匈牙利算法匹配预测和标签，并最小化分类、回归和IoU的加权损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)整合了RGB摄像头、门控摄像头、LiDAR和雷达四种传感器，实现传感器自适应融合；2)提出深度引导的跨模态变换和早期相机融合机制；3)引入多模态、基于距离的查询生成方法，避免过度依赖LiDAR；4)设计鸟瞰图平面上的自适应混合方案，结合图像特征和范围特征。相比之前的工作，SAMFusion不再局限于理想天气条件，而是专门针对恶劣天气条件优化；不再过度依赖LiDAR，而是通过距离和天气感知的加权机制动态调整各传感器的重要性；同时引入了门控摄像头这一在恶劣天气下表现优异的传感器，显著提高了系统在低光和恶劣天气条件下的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAMFusion通过创新的多传感器自适应融合框架，结合深度引导的跨模态变换和基于距离的加权机制，显著提高了自动驾驶系统在恶劣天气条件下的3D物体检测性能，特别是在远距离行人检测方面取得了突破性进展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sensor fusion is an essential capability for autonomous robots,enabling object detection and decision-making in the presence of failing oruncertain inputs. While recent fusion methods excel in normal environmentalconditions, these approaches fail in adverse weather, e.g., heavy fog, snow, orobstructions due to soiling. We introduce a novel multi-sensor fusion approachtailored to adverse weather conditions. In addition to fusing RGB and LiDARsensors, which are employed in recent autonomous driving literature, our sensorfusion stack is also capable of learning from NIR gated camera and radarmodalities to tackle low light and inclement weather. We fuse multimodal sensordata through attentive, depth-based blending schemes, with learned refinementon the Bird's Eye View (BEV) plane to combine image and range featureseffectively. Our detections are predicted by a transformer decoder that weighsmodalities based on distance and visibility. We demonstrate that our methodimproves the reliability of multimodal sensor fusion in autonomous vehiclesunder challenging weather conditions, bridging the gap between ideal conditionsand real-world edge cases. Our approach improves average precision by 17.2 APcompared to the next best method for vulnerable pedestrians in long distancesand challenging foggy scenes. Our project page is available athttps://light.princeton.edu/samfusion/</description>
      <author>example@mail.com (Edoardo Palladin, Roland Dietze, Praveen Narayanan, Mario Bijelic, Felix Heide)</author>
      <guid isPermaLink="false">2508.16408v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
      <link>http://arxiv.org/abs/2508.16201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SpecVLM，一种针对视频大型语言模型的无训练推测解码框架，通过分阶段视频标记剪枝实现高效解码而不牺牲准确性。&lt;h4&gt;背景&lt;/h4&gt;视频大型语言模型（Vid-LLMs）在理解视频内容方面表现出色，但它们依赖密集的视频标记表示，导致在预填充和解码阶段存在大量内存和计算开销。&lt;h4&gt;目的&lt;/h4&gt;减少信息损失并无损加速Vid-LLMs的解码阶段，同时降低计算和内存需求。&lt;h4&gt;方法&lt;/h4&gt;SpecVLM利用草稿模型推测对视频标记剪枝低敏感性的发现，采用两阶段剪枝过程：第一阶段根据验证器的注意力信号选择高信息量标记；第二阶段以空间均匀方式剪枝剩余冗余标记，可剪枝最多90%的视频标记。&lt;h4&gt;主要发现&lt;/h4&gt;草稿模型的推测对视频标记剪枝具有低敏感性，使得大幅剪枝（最多90%）成为可能而不影响准确性。&lt;h4&gt;结论&lt;/h4&gt;在四个视频理解基准测试上，SpecVLM展示了有效性和鲁棒性，对于LLaVA-OneVision-72B模型实现2.68倍解码加速，对于Qwen2.5-VL-32B模型实现2.11倍加速。&lt;h4&gt;翻译&lt;/h4&gt;视频大型语言模型（Vid-LLMs）在理解视频内容方面展现出强大能力。然而，它们对密集视频标记表示的依赖在预填充和解码阶段引入了大量内存和计算开销。为了减轻近期视频标记减少方法的信息损失并无损加速Vid-LLMs的解码阶段，我们引入了SpecVLM，一种专为Vid-LLMs设计的无训练推测解码（SD）框架，结合分阶段视频标记剪枝。基于我们的新发现——草稿模型的推测对视频标记剪枝具有低敏感性，SpecVLM能够剪枝高达90%的视频标记，实现高效推测而不牺牲准确性。为实现这一目标，它执行两阶段剪枝过程：第一阶段根据验证器（目标模型）的注意力信号选择高信息量标记，第二阶段以空间均匀方式剪枝剩余的冗余标记。在四个视频理解基准上的广泛实验证明了SpecVLM的有效性和鲁棒性，对于LLaVA-OneVision-72B实现了高达2.68倍的解码加速，对于Qwen2.5-VL-32B实现了2.11倍的加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video large language models (Vid-LLMs) have shown strong capabilities inunderstanding video content. However, their reliance on dense video tokenrepresentations introduces substantial memory and computational overhead inboth prefilling and decoding. To mitigate the information loss of recent videotoken reduction methods and accelerate the decoding stage of Vid-LLMslosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)framework tailored for Vid-LLMs that incorporates staged video token pruning.Building on our novel finding that the draft model's speculation exhibits lowsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,enabling efficient speculation without sacrificing accuracy. To achieve this,it performs a two-stage pruning process: Stage I selects highly informativetokens guided by attention signals from the verifier (target model), whileStage II prunes remaining redundant ones in a spatially uniform manner.Extensive experiments on four video understanding benchmarks demonstrate theeffectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup forQwen2.5-VL-32B.</description>
      <author>example@mail.com (Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li)</author>
      <guid isPermaLink="false">2508.16201v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning</title>
      <link>http://arxiv.org/abs/2508.15861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入XFinBench基准测试，评估大型语言模型在解决复杂金融问题上的能力，发现当前最佳模型仍显著落后于人类专家。&lt;h4&gt;背景&lt;/h4&gt;解决金融问题需要复杂的推理、多模态数据处理和广泛的技术理解，这对当前大型语言模型构成了独特挑战。&lt;h4&gt;目的&lt;/h4&gt;开发XFinBench基准测试，评估LLM解决复杂知识密集型金融问题的能力，涵盖多个研究生级别的金融主题和多模态上下文。&lt;h4&gt;方法&lt;/h4&gt;构建包含4,235个示例的XFinBench基准，确定LLM的五项核心能力（术语理解、时间推理、未来预测、情景规划和数值建模），对18个领先模型进行实验，并构建包含3,032个金融术语的知识库进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;o1是表现最好的纯文本模型，总体准确率为67.3%，但仍比人类专家低12.5个百分点，特别是在时间推理和情景规划能力方面差距显著；相关知识只给小型开源模型带来准确率提升；计算中的舍入误差和对图像中曲线位置和交点的盲视是模型表现不佳的主要原因。&lt;h4&gt;结论&lt;/h4&gt;当前LLM在解决复杂金融问题上仍有显著局限性，特别是在时间推理、情景规划和视觉上下文理解方面，需要进一步改进。&lt;h4&gt;翻译&lt;/h4&gt;解决金融问题需要复杂的推理、多模态数据处理和广泛的技术理解，这对当前大型语言模型构成了独特挑战。我们引入XFinBench，一个包含4,235个示例的新基准，用于评估LLM解决跨多个研究生级别金融主题的复杂知识密集型金融问题的能力，并具有多模态上下文。我们使用XFinBench确定了LLM的五项核心能力，即术语理解、时间推理、未来预测、情景规划和数值建模。基于XFinBench，我们对18个领先模型进行了广泛实验。结果显示，o1是表现最好的纯文本模型，总体准确率为67.3%，但仍比人类专家低12.5个百分点，特别是在时间推理和情景规划能力方面。我们进一步构建了一个包含3,032个金融术语的知识库用于知识增强分析，发现相关知识只给小型开源模型带来一致的准确率提升。此外，我们的错误分析显示，计算过程中的舍入误差和对图像中曲线位置和交点的盲视是导致模型在计算和视觉上下文问题中表现不佳的两个主要问题。代码和数据集可通过GitHub获取：https://github.com/Zhihan72/XFinBench。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving financial problems demands complex reasoning, multimodal dataprocessing, and a broad technical understanding, presenting unique challengesfor current large language models (LLMs). We introduce XFinBench, a novelbenchmark with 4,235 examples designed to evaluate LLM's ability in solvingcomplex, knowledge-intensive financial problems across diverse graduate-levelfinance topics with multi-modal context. We identify five core capabilities ofLLMs using XFinBench, i.e, terminology understanding, temporal reasoning,future forecasting, scenario planning, and numerical modelling. Upon XFinBench,we conduct extensive experiments on 18 leading models. The result shows that o1is the best-performing text-only model with an overall accuracy of 67.3%, butstill lags significantly behind human experts with 12.5%, especially intemporal reasoning and scenario planning capabilities. We further construct aknowledge bank with 3,032 finance terms for knowledge augmentation analysis,and find that relevant knowledge to the question only brings consistentaccuracy improvements to small open-source model. Additionally, our erroranalysis reveals that rounding errors during calculation and blindness toposition and intersection of curves in the image are two primary issues leadingto model's poor performance in calculating and visual-context questions,respectively. Code and dataset are accessible via GitHub:https://github.com/Zhihan72/XFinBench.</description>
      <author>example@mail.com (Zhihan Zhang, Yixin Cao, Lizi Liao)</author>
      <guid isPermaLink="false">2508.15861v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation</title>
      <link>http://arxiv.org/abs/2508.16568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为FedMox的新框架，用于在资源受限的边缘设备上有效地适应基础模型，同时保护数据隐私。&lt;h4&gt;背景&lt;/h4&gt;基础模型表现出显著的泛化能力但需要适应下游任务，尤其在隐私敏感应用中。由于数据隐私法规，云基础模型无法直接访问私有边缘数据，限制了其适应性。联邦学习提供隐私感知替代方案，但现有方法忽略了边缘设备的计算资源有限和标记数据稀缺问题。&lt;h4&gt;目的&lt;/h4&gt;解决边缘设备计算资源有限和标记数据稀缺的挑战，引入实用的半监督联邦学习框架，使边缘设备持有未标记低分辨率数据，服务器拥有有限标记高分辨率数据。&lt;h4&gt;方法&lt;/h4&gt;提出联邦专家混合框架FedMox，通过稀疏专家混合架构处理计算和分辨率不匹配问题，使用空间路由器对齐不同分辨率特征，采用软混合策略稳定半监督学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;以目标检测为例，在真实世界自动驾驶数据集上的实验表明，FedMox有效在PSSFL下适应基础模型，显著提高性能同时控制边缘设备的内存成本。&lt;h4&gt;结论&lt;/h4&gt;该工作为联邦场景中可扩展和隐私保护的基础模型适应性铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型表现出显著的泛化能力，但需要适应下游任务，特别是在隐私敏感的应用中。由于数据隐私法规，基于云的基础模型无法直接访问私有边缘数据，限制了它们的适应性。联邦学习提供了一种隐私感知的替代方案，但现有的联邦学习方法忽略了边缘设备施加的限制，即计算资源有限和标记数据稀缺。为解决这些挑战，我们引入实用的半监督联邦学习，其中边缘设备只持有未标记的低分辨率数据，而服务器拥有有限的标记高分辨率数据。在这种设置下，我们提出了联邦专家混合，一种增强联邦学习中基础模型适应性的新框架。联邦专家混合通过稀疏的专家混合架构处理计算和分辨率不匹配的挑战，使用空间路由器对齐不同分辨率的特征，并采用软混合策略稳定半监督学习。我们以目标检测为例，在真实世界自动驾驶数据集上的实验表明，联邦专家混合有效地在实用的半监督联邦学习下适应基础模型，显著提高了性能，同时边缘设备的内存成本受到限制。我们的工作为联邦场景中可扩展和隐私保护的基础模型适应性铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) exhibit remarkable generalization but requireadaptation to downstream tasks, particularly in privacy-sensitive applications.Due to data privacy regulations, cloud-based FMs cannot directly access privateedge data, limiting their adaptation. Federated learning (FL) provides aprivacy-aware alternative, but existing FL approaches overlook the constraintsimposed by edge devices -- namely, limited computational resources and thescarcity of labeled data. To address these challenges, we introduce PracticalSemi-Supervised Federated Learning (PSSFL), where edge devices hold onlyunlabeled, low-resolution data, while the server has limited labeled,high-resolution data. In this setting, we propose the Federated Mixture ofExperts (FedMox), a novel framework that enhances FM adaptation in FL. FedMoxtackles computational and resolution mismatch challenges via a sparseMixture-of-Experts architecture, employing a spatial router to align featuresacross resolutions and a Soft-Mixture strategy to stabilize semi-supervisedlearning. We take object detection as a case study, and experiments onreal-world autonomous driving datasets demonstrate that FedMox effectivelyadapts FMs under PSSFL, significantly improving performance with constrainedmemory costs on edge devices. Our work paves the way for scalable andprivacy-preserving FM adaptation in federated scenarios.</description>
      <author>example@mail.com (Guangyu Sun, Jingtao Li, Weiming Zhuang, Chen Chen, Chen Chen, Lingjuan Lyu)</author>
      <guid isPermaLink="false">2508.16568v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>An Investigation of Visual Foundation Models Robustness</title>
      <link>http://arxiv.org/abs/2508.16225v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉基础模型(VFMs)在计算机视觉领域广泛应用，支持多种任务如目标检测、图像分类等。这些模型利用深度学习的重要创新，在安全敏感领域需要强大的鲁棒性来促进技术信任。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFMs)已成为计算机视觉领域的核心组件，利用LeNet-5、AlexNet、ResNet等深度学习模型的创新，在目标检测、图像分类、分割、姿态估计和运动跟踪等任务中表现出色。这些模型在生物特征验证、自动驾驶车辆感知和医学图像分析等安全敏感领域尤为重要。&lt;h4&gt;目的&lt;/h4&gt;调查计算机视觉系统中关键的鲁棒性要求，研究如何使系统有效地适应由光照、天气条件和传感器特性等因素影响的动态环境。&lt;h4&gt;方法&lt;/h4&gt;检查常用的经验性防御和鲁棒性训练方法，分析这些方法如何增强视觉网络对实际挑战的鲁棒性，包括分布偏移、噪声和空间失真输入以及对抗性攻击。&lt;h4&gt;主要发现&lt;/h4&gt;分析了这些防御机制相关的挑战，包括网络属性和组件，用于指导消融研究，以及评估网络鲁棒性的基准指标。&lt;h4&gt;结论&lt;/h4&gt;在安全敏感领域中，视觉模型的鲁棒性对于促进技术与最终用户之间的信任至关重要，需要通过适当的防御机制和训练方法来确保系统在各种环境条件下的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)在计算机视觉领域变得越来越普遍，为各种任务提供支持系统，如目标检测、图像分类、分割、姿态估计和运动跟踪。VFMs利用了深度学习模型的重要创新，如LeNet-5、AlexNet、ResNet、VGGNet、InceptionNet、DenseNet、YOLO和ViT，在一系列关键计算机视觉应用中提供卓越性能。这些包括生物特征验证、自动驾驶车辆感知和医学图像分析等安全敏感领域，在这些领域中，鲁棒性对于促进技术与最终用户之间的信任至关重要。本文调查了计算机视觉系统中关键的鲁棒性要求，以有效适应受光照、天气条件和传感器特性等因素影响的动态环境。我们检查了常用的经验性防御和鲁棒性训练，以增强视觉网络对实际挑战的鲁棒性，如分布偏移、噪声和空间失真输入，以及对抗性攻击。随后，我们提供了对这些防御机制相关挑战的全面分析，包括网络属性和组件，以指导消融研究和评估网络鲁棒性的基准指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision,powering systems for diverse tasks such as object detection, imageclassification, segmentation, pose estimation, and motion tracking. VFMs arecapitalizing on seminal innovations in deep learning models, such as LeNet-5,AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliversuperior performance across a range of critical computer vision applications.These include security-sensitive domains like biometric verification,autonomous vehicle perception, and medical image analysis, where robustness isessential to fostering trust between technology and the end-users. This articleinvestigates network robustness requirements crucial in computer vision systemsto adapt effectively to dynamic environments influenced by factors such aslighting, weather conditions, and sensor characteristics. We examine theprevalent empirical defenses and robust training employed to enhance visionnetwork robustness against real-world challenges such as distributional shifts,noisy and spatially distorted inputs, and adversarial attacks. Subsequently, weprovide a comprehensive analysis of the challenges associated with thesedefense mechanisms, including network properties and components to guideablation studies and benchmarking metrics to evaluate network robustness.</description>
      <author>example@mail.com (Sandeep Gupta, Roberto Passerone)</author>
      <guid isPermaLink="false">2508.16225v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>\textsc{T-Mask}: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring</title>
      <link>http://arxiv.org/abs/2508.16207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by 26th IEEE International Conference on  Intelligent Transportation Systems ITSC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了摄像机视角变化对驾驶员监控的影响，提出了一种名为T-Mask的新方法，通过时间令牌屏蔽提高跨视角驾驶员监控的准确性。&lt;h4&gt;背景&lt;/h4&gt;摄像机视角变化是驾驶员监控中的常见障碍。深度学习和预训练基础模型通过最终层的轻量级适应展现出强大的泛化潜力，但它们对未见视角的鲁棒性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过单视角训练将图像基础模型适应到驾驶员监控任务，并评估这些模型在无需进一步适应的情况下处理未见视角的能力。&lt;h4&gt;方法&lt;/h4&gt;使用单训练视角将图像基础模型适应到驾驶员监控，直接在未见视角上评估模型。基准测试包括简单线性探针、高级探针策略，比较DINOv2和CLIP两种基础模型与PEFT和完全微调。引入T-Mask方法，一种利用时间令牌屏蔽并强调动态视频区域的图像到视频探针方法。&lt;h4&gt;主要发现&lt;/h4&gt;在Drive&amp;Act数据集上，T-Mask相比强探针基线提高了跨视角top-1准确率+1.23%，相比PEFT方法提高了+8.0%，且未增加任何参数。对于代表性不足的次要活动，在训练视角下提高了+5.42%的识别率，在跨视角设置下提高了+1.36%。&lt;h4&gt;结论&lt;/h4&gt;使用轻量级探针方法如T-Mask来适应基础模型在细粒度驾驶员观察中具有强大潜力，特别是在跨视角和低数据设置下。时间令牌选择在构建鲁棒驾驶员监控系统中至关重要。&lt;h4&gt;翻译&lt;/h4&gt;摄像机视角的变化是驾驶员监控中的常见障碍。虽然深度学习和预训练基础模型通过最终层的轻量级适应展现出强大的泛化潜力，但它们对未见视角的鲁棒性尚未得到充分探索。我们通过单训练视角将图像基础模型适应到驾驶员监控，并在无需进一步适应的情况下直接在未见视角上评估它们。我们对简单线性探针、高级探针策略进行基准测试，并将两种基础模型(DINOv2和CLIP)与参数高效微调(PEFT)和完全微调进行比较。基于这些见解，我们引入T-Mask——一种新的图像到视频探针方法，它利用时间令牌屏蔽并强调更多动态视频区域。在公共Drive&amp;Act数据集上的基准测试表明，T-Mask相比强探针基线提高了跨视角top-1准确率+1.23%，相比PEFT方法提高了+8.0%，且未增加任何参数。它对于代表性不足的次要活动特别有效，在训练视角下提高了+5.42%的识别率，在跨视角设置下提高了+1.36%。这项工作提供了令人鼓舞的证据，表明使用轻量级探针方法如T-Mask来适应基础模型在细粒度驾驶员观察中具有强大潜力，特别是在跨视角和低数据设置下。这些结果强调了在利用基础模型构建鲁棒驾驶员监控系统时时间令牌选择的重要性。代码和模型将在https://github.com/th-nesh/T-MASK上提供，以支持持续的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Changes of camera perspective are a common obstacle in driver monitoring.While deep learning and pretrained foundation models show strong potential forimproved generalization via lightweight adaptation of the final layers('probing'), their robustness to unseen viewpoints remains underexplored. Westudy this challenge by adapting image foundation models to driver monitoringusing a single training view, and evaluating them directly on unseenperspectives without further adaptation. We benchmark simple linear probes,advanced probing strategies, and compare two foundation models (DINOv2 andCLIP) against parameter-efficient fine-tuning (PEFT) and full fine-tuning.Building on these insights, we introduce \textsc{T-Mask} -- a newimage-to-video probing method that leverages temporal token masking andemphasizes more dynamic video regions. Benchmarked on the public Drive\&amp;Actdataset, \textsc{T-Mask} improves cross-view top-1 accuracy by $+1.23\%$ overstrong probing baselines and $+8.0\%$ over PEFT methods, without adding anyparameters. It proves particularly effective for underrepresented secondaryactivities, boosting recognition by $+5.42\%$ under the trained view and$+1.36\%$ under cross-view settings. This work provides encouraging evidencethat adapting foundation models with lightweight probing methods like\textsc{T-Mask} has strong potential in fine-grained driver observation,especially in cross-view and low-data settings. These results highlight theimportance of temporal token selection when leveraging foundation models tobuild robust driver monitoring systems. Code and models will be made availableat https://github.com/th-nesh/T-MASK to support ongoing research.</description>
      <author>example@mail.com (Thinesh Thiyakesan Ponbagavathi, Kunyu Peng, Alina Roitberg)</author>
      <guid isPermaLink="false">2508.16207v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Ensemble learning of foundation models for precision oncology</title>
      <link>http://arxiv.org/abs/2508.16085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A conceptual evaluation work; more studies are in progress; examples  are here (https://github.com/lilab-stanford/ELF)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ELF（Ensemble Learning of Foundation models）框架，通过整合五种最先进的病理学基础模型，生成统一的切片级表征。ELF在跨越20个解剖部位的53,699个全切片图像上训练，利用集成学习捕捉不同模型的互补信息，同时保持高数据效率。在多种临床应用评估中，ELF表现出优于所有组成基础模型和现有切片级模型的准确性和稳健性。&lt;h4&gt;背景&lt;/h4&gt;病理组织学对疾病诊断和治疗决策至关重要。人工智能的最新进展使得病理学基础模型能够从大规模全切片图像中学习丰富的视觉表征。然而，现有模型通常在不同数据集上使用不同策略训练，导致性能不一致和泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;引入ELF框架，整合五种最先进的病理学基础模型，生成统一的切片级表征，以提高病理学AI模型的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;ELF在跨越20个解剖部位的53,699个全切片图像上训练，利用集成学习捕捉不同模型的互补信息。与传统的块级模型不同，ELF采用切片级架构，特别适合数据有限的临床环境。研究在多种临床应用中评估了ELF，包括疾病分类、生物标志物检测以及对抗癌疗法、化疗、靶向治疗和免疫治疗的反应预测。&lt;h4&gt;主要发现&lt;/h4&gt;ELF在所有评估的临床应用中，始终优于所有组成的基础模型和现有的切片级模型，展现出更高的准确性和稳健性。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了集成学习对病理学基础模型的力量，ELF作为推进AI辅助精准肿瘤学的一种可扩展和可泛化的解决方案具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;病理组织学对疾病诊断和治疗决策至关重要。人工智能的最新进展使得病理学基础模型能够从大规模全切片图像中学习丰富的视觉表征。然而，现有模型通常在不同的数据集上使用不同的策略进行训练，导致性能不一致和泛化能力有限。在此，我们引入ELF（Ensemble Learning of Foundation models），一种新颖的框架，整合了五种最先进的病理学基础模型以生成统一的切片级表征。ELF在跨越20个解剖部位的53,699个全切片图像上训练，利用集成学习捕捉来自不同模型的互补信息，同时保持高数据效率。与传统的块级模型不同，ELF的切片级架构在数据有限的临床环境中特别有利，例如治疗反应预测。我们在多种临床应用中评估了ELF，包括疾病分类、生物标志物检测以及对主要抗癌疗法、细胞毒性化疗、靶向治疗和免疫治疗的反应预测，涵盖多种癌症类型。ELF始终优于所有组成的基础模型和现有的切片级模型，展现出更高的准确性和稳健性。我们的结果突出了集成学习对病理学基础模型的力量，并建议ELF是推进AI辅助精准肿瘤学的一种可扩展和可泛化的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathology is essential for disease diagnosis and treatmentdecision-making. Recent advances in artificial intelligence (AI) have enabledthe development of pathology foundation models that learn rich visualrepresentations from large-scale whole-slide images (WSIs). However, existingmodels are often trained on disparate datasets using varying strategies,leading to inconsistent performance and limited generalizability. Here, weintroduce ELF (Ensemble Learning of Foundation models), a novel framework thatintegrates five state-of-the-art pathology foundation models to generateunified slide-level representations. Trained on 53,699 WSIs spanning 20anatomical sites, ELF leverages ensemble learning to capture complementaryinformation from diverse models while maintaining high data efficiency. Unliketraditional tile-level models, ELF's slide-level architecture is particularlyadvantageous in clinical contexts where data are limited, such as therapeuticresponse prediction. We evaluated ELF across a wide range of clinicalapplications, including disease classification, biomarker detection, andresponse prediction to major anticancer therapies, cytotoxic chemotherapy,targeted therapy, and immunotherapy, across multiple cancer types. ELFconsistently outperformed all constituent foundation models and existingslide-level models, demonstrating superior accuracy and robustness. Our resultshighlight the power of ensemble learning for pathology foundation models andsuggest ELF as a scalable and generalizable solution for advancing AI-assistedprecision oncology.</description>
      <author>example@mail.com (Xiangde Luo, Xiyue Wang, Feyisope Eweje, Xiaoming Zhang, Sen Yang, Ryan Quinton, Jinxi Xiang, Yuchen Li, Yuanfeng Ji, Zhe Li, Yijiang Chen, Colin Bergstrom, Ted Kim, Francesca Maria Olguin, Kelley Yuan, Matthew Abikenari, Andrew Heider, Sierra Willens, Sanjeeth Rajaram, Robert West, Joel Neal, Maximilian Diehn, Ruijiang Li)</author>
      <guid isPermaLink="false">2508.16085v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Training a Foundation Model for Materials on a Budget</title>
      <link>http://arxiv.org/abs/2508.16067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Nequix是一种高效的E(3)-等变势能模型，通过简化的设计和现代训练实践，在保持高准确性的同时大幅降低了计算需求，训练成本不到大多数方法四分之一，推理速度比当前顶级模型快十倍。&lt;h4&gt;背景&lt;/h4&gt;材料建模的基础模型发展迅速，但训练成本高昂，使得最先进的方法难以被许多研究团队使用。&lt;h4&gt;目的&lt;/h4&gt;开发一个成本效益更高的材料建模模型，在保持准确性的同时大幅降低计算需求。&lt;h4&gt;方法&lt;/h4&gt;介绍Nequix，一个紧凑的E(3)-等变势能模型，结合简化的NequIP设计和现代训练实践，包括等变均方根层归一化和Muon优化器，使用JAX构建，有70万个参数，在500个A100-GPU小时内完成训练。&lt;h4&gt;主要发现&lt;/h4&gt;在Matbench-Discovery和MDR Phonon基准测试中，Nequix总体排名第三，训练成本不到大多数其他方法四分之一，推理速度比当前排名第一的模型快一个数量级。&lt;h4&gt;结论&lt;/h4&gt;Nequix在保持高准确性的同时显著降低了计算需求，模型权重和完全可复现的代码已在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;材料建模的基础模型正在快速发展，但它们的训练仍然昂贵，通常使得最先进的方法难以被许多研究团队企及。我们介绍了Nequix，一个紧凑的E(3)-等变势能模型，它结合了简化的NequIP设计和现代训练实践，包括等变均方根层归一化和Muon优化器，在保持准确性的同时大幅降低了计算需求。使用JAX构建，Nequix有70万个参数，并在500个A100-GPU小时内完成训练。在Matbench-Discovery和MDR Phonon基准测试中，Nequix总体排名第三，同时需要的训练成本不到大多数其他方法四分之一，并且其推理速度比当前排名第一的模型快一个数量级。我们在https://github.com/atomicarchitects/nequix上发布了模型权重和完全可复现的代码库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for materials modeling are advancing quickly, but theirtraining remains expensive, often placing state-of-the-art methods out of reachfor many research groups. We introduce Nequix, a compact E(3)-equivariantpotential that pairs a simplified NequIP design with modern training practices,including equivariant root-mean-square layer normalization and the Muonoptimizer, to retain accuracy while substantially reducing computerequirements. Built in JAX, Nequix has 700K parameters and was trained in 500A100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequixranks third overall while requiring less than one quarter of the training costof most other methods, and it delivers an order-of-magnitude faster inferencespeed than the current top-ranked model. We release model weights and fullyreproducible codebase at https://github.com/atomicarchitects/nequix</description>
      <author>example@mail.com (Teddy Koker, Tess Smidt)</author>
      <guid isPermaLink="false">2508.16067v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</title>
      <link>http://arxiv.org/abs/2508.16059v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了多层可转向嵌入融合(MSEF)框架，解决大型语言模型在时间序列预测中的浅层集成问题，使模型能够直接访问所有深度的时序模式，从而减轻深层中时序信息的逐渐丢失。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据在各应用领域普遍存在，使时间序列预测成为基础任务。随着大型语言模型的进步，各种方法已被开发出来以适应LLMs用于时间序列预测。然而，现有方法在时序信息集成上存在固有局限，LLMs通常只能在浅层访问时序表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中时序信息在深层逐渐减弱的问题，使LLMs能够在所有深度直接访问时间序列模式，并实现文本嵌入与时序表示之间的有效适应。&lt;h4&gt;方法&lt;/h4&gt;提出多层可转向嵌入融合(MSEF)框架，利用现成的时序基础模型提取语义丰富的嵌入，并通过特定层的转向向量将这些嵌入与LLM各层的中间文本表示融合。这些转向向量旨在持续优化时序和文本模态之间的对齐，并确保特定层的适应机制，促进高效的少样本学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准测试上的实验结果表明，MSEF相比基线方法有显著的性能提升，平均均方误差降低了31.8%。&lt;h4&gt;结论&lt;/h4&gt;MSEF框架有效解决了LLMs在时间序列预测中的浅层集成问题，通过直接访问所有深度的时序模式，显著提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;时间序列数据在各应用领域普遍存在，使时间序列预测成为一项基础任务。随着大型语言模型的惊人进展，各种方法已被开发出来以适应LLMs用于时间序列预测。尽管解锁了LLMs在理解时序数据方面的潜力，但现有方法在时序信息的集成上存在固有局限，LLMs通常只能在浅层（主要是输入层）访问时序表示。这导致时序表示的影响在深层中逐渐减弱，最终导致文本嵌入与时序表示之间的无效适应。在本文中，我们提出了多层可转向嵌入融合(MSEF)，这是一种新框架，使LLMs能够直接访问所有深度的时序模式，从而减轻深层中时序信息的逐渐丢失。具体而言，MSEF利用现成的时序基础模型提取语义丰富的嵌入，这些嵌入通过特定层的转向向量与LLM各层的中间文本表示融合。这些转向向量旨在持续优化时序和文本模态之间的对齐，并促进特定层的适应机制，确保高效的少样本学习能力。在七个基准测试上的实验结果表明，MSEF相比基线方法有显著的性能提升，平均均方误差降低了31.8%。代码可在https://github.com/One1sAll/MSEF获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760803&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series (TS) data are ubiquitous across various application areas,rendering time series forecasting (TSF) a fundamental task. With the astoundingadvances in large language models (LLMs), a variety of methods have beendeveloped to adapt LLMs for time series forecasting. Despite unlocking thepotential of LLMs in comprehending TS data, existing methods are inherentlyconstrained by their shallow integration of TS information, wherein LLMstypically access TS representations at shallow layers, primarily at the inputlayer. This causes the influence of TS representations to progressively fade indeeper layers and eventually leads to ineffective adaptation between textualembeddings and TS representations. In this paper, we propose the Multi-layerSteerable Embedding Fusion (MSEF), a novel framework that enables LLMs todirectly access time series patterns at all depths, thereby mitigating theprogressive loss of TS information in deeper layers. Specifically, MSEFleverages off-the-shelf time series foundation models to extract semanticallyrich embeddings, which are fused with intermediate text representations acrossLLM layers via layer-specific steering vectors. These steering vectors aredesigned to continuously optimize the alignment between time series and textualmodalities and facilitate a layer-specific adaptation mechanism that ensuresefficient few-shot learning capabilities. Experimental results on sevenbenchmarks demonstrate significant performance improvements by MSEF comparedwith baselines, with an average reduction of 31.8% in terms of MSE. The code isavailable at https://github.com/One1sAll/MSEF.</description>
      <author>example@mail.com (Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng)</author>
      <guid isPermaLink="false">2508.16059v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Generative Foundation Model for Structured and Unstructured Electronic Health Records</title>
      <link>http://arxiv.org/abs/2508.16054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了生成式深度患者(GDP)模型，一种多模态基础模型，能够同时处理结构化和非结构化电子健康记录数据，实现临床预测和临床叙述生成的高性能表现。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHRs)是丰富的临床数据源，包含结构化元素（人口统计、生命体征、实验室结果、代码）和非结构化临床笔记等多种数据模式。利用这种异构性对改善患者预后至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从多种数据模式中学习并支持临床任务的基础模型，同时保留电子健康记录中的时间和定量细节。&lt;h4&gt;方法&lt;/h4&gt;引入GDP模型，通过CNN-Transformer编码器原生编码结构化的EHR时间序列，并通过跨模态注意力与未结构化的EHR融合到基于LLaMA的解码器中。GDP分为两个阶段训练：(1)生成式预训练，学习从原始患者时间线生成临床叙述，同时执行掩码特征预测和下一个时间步预测；(2)多任务微调，用于临床有意义的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在临床预测方面，GDP在MIMIC-IV上表现出色：心力衰竭AUROC为0.923，2型糖尿病AUROC为0.817，30天再入院AUROC为0.627。在叙述生成方面，GDP实现了ROUGE-L为0.135和BERTScore-F1为0.545。盲人评估中，GDP-Instruct在忠实度、流畅度和整体临床实用性方面得分最高，表明可减少医院文档工作负担而不牺牲准确性。&lt;h4&gt;结论&lt;/h4&gt;单个多模态基础模型可以同时预测临床可操作事件和生成高质量的临床叙述。GDP的灵活架构可扩展到其他模态。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录是丰富的临床数据源，但也是复杂的患者数据存储库，包含结构化元素（人口统计、生命体征、实验室结果、代码）、非结构化临床笔记和其他数据模式。利用这种异构性对于改善患者预后至关重要。大型语言模型的最新进展已经能够从多种数据模式中学习的基础模型，并支持临床任务。然而，大多数当前方法简单地将数值EHR数据序列化为文本，这可能会丢失时间和定量细节。我们引入了生成式深度患者(GDP)，一种多模态基础模型，它通过CNN-Transformer编码器原生编码结构化的EHR时间序列，并通过跨模态注意力与未结构化的EHR融合到基于LLaMA的解码器中。GDP分两个阶段训练：(1)生成式预训练，学习从原始患者时间线产生临床叙述，同时执行掩码特征预测和下一个时间步预测以捕获时间动态；(2)多任务微调用于临床有意义的预测（如心力衰竭、2型糖尿病、30天再入院）。在临床预测中，GDP在MIMIC-IV上表现出色：心力衰竭AUROC为0.923，2型糖尿病AUROC为0.817，30天再入院AUROC为0.627。对于叙述生成，GDP实现了ROUGE-L为0.135和BERTScore-F1为0.545。在盲人评估中，GDP-Instruct在忠实度、流畅度和整体临床实用性方面得分最高，表明在不牺牲准确性的情况下减少了医院文档工作负担。我们的结果表明，单个多模态基础模型可以预测临床可操作事件并生成高质量的临床叙述。此外，GDP的灵活架构可以扩展到其他模态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic health records (EHRs) are rich clinical data sources but complexrepositories of patient data, spanning structured elements (demographics,vitals, lab results, codes), unstructured clinical notes and other modalitiesof data. Harnessing this heterogeneity is critical for improving patientoutcomes. Recent advances in large language models (LLMs) have enabledfoundation models that can learn from multiple data modalities and supportclinical tasks. However, most current approaches simply serialize numeric EHRdata into text, which risks losing temporal and quantitative detail. Weintroduce Generative Deep Patient (GDP), a multimodal foundation model thatnatively encodes structured EHR time-series via a CNN-Transformer encoder andfuses it with unstructured EHRs through cross-modal attention into aLLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,where it learns to produce clinical narratives from raw patient timelines whilealso performing masked feature prediction (MFP) and next time-step prediction(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning forclinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-dayreadmission). In clinical prediction, GDP demonstrated superior performance onMIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and30-day readmission AUROC = 0.627. For narrative generation, GDP achievedROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,GDP-Instruct scored highest on faithfulness, fluency, and overall clinicalutility, suggesting reduced hospital documentation workload without sacrificingaccuracy. Our results demonstrate that a single multimodal foundation model canboth predict clinically actionable events and generate high-quality clinicalnarratives. Furthermore, GDP's flexible architecture can be extended toadditional modalities.</description>
      <author>example@mail.com (Sonish Sivarajkumar, Hang Zhang, Yuelyu Ji, Maneesh Bilalpur, Xizhi Wu, Chenyu Li, Min Gu Kwak, Shyam Visweswaran, Yanshan Wang)</author>
      <guid isPermaLink="false">2508.16054v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset</title>
      <link>http://arxiv.org/abs/2508.15986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 6 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个基于深度学习的视网膜疾病分类系统，利用合成数据集SynFundus-1M克服了真实临床数据稀缺的问题，并通过多种模型架构和集成方法实现了高准确率和良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视网膜疾病分类的多标签深度学习模型常因患者隐私问题和成本高昂而缺乏大型专家标注的临床数据集。最近发布的SynFundus-1M高保真合成数据集（包含超过一百万张眼底图像）为克服这些障碍提供了新机会。&lt;h4&gt;目的&lt;/h4&gt;为SynFundus-1M新资源建立基础性能基准，开发端到端的深度学习流程，训练模型分类11种视网膜疾病。&lt;h4&gt;方法&lt;/h4&gt;开发端到端深度学习流程，训练六种现代架构（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型），使用5折多标签分层交叉验证策略，并通过将折叠外预测与XGBoost分类器堆叠开发元集成模型。&lt;h4&gt;主要发现&lt;/h4&gt;最终集成模型在内部验证集上实现宏平均AUC为0.9973的最佳性能；模型在三个真实临床数据集上表现出强大泛化能力：组合DR数据集AUC为0.7972，AIROGS青光眼数据集AUC为0.9126，多标签RFMiD数据集宏平均AUC为0.8800。&lt;h4&gt;结论&lt;/h4&gt;该研究为大规模合成数据集的未来研究提供了强大基线，证明完全在合成数据上训练的模型可准确分类多种病理并有效泛化到真实临床图像，为加速眼科领域全面AI系统发展提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;视网膜疾病分类的多标签深度学习模型的发展常因患者隐私问题和成本高昂而受到缺乏大型专家标注临床数据集的限制。最近发布的SynFundus-1M高保真合成数据集（包含超过一百万张眼底图像）为克服这些障碍提供了新机会。为了为这一新资源建立基础性能基准，我们开发了一个端到端的深度学习流程，训练了六种现代架构（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2和RETFound基础模型）使用5折多标签分层交叉验证策略来分类11种视网膜疾病。我们进一步通过将折叠外预测与XGBoost分类器堆叠开发了一个元集成模型。我们的最终集成模型在内部验证集上实现了最佳性能，宏平均ROC曲线下面积为0.9973。关键的是，这些模型在三个不同的真实世界临床数据集上表现出强大的泛化能力，在组合DR数据集上达到0.7972的AUC，在AIROGS青光眼数据集上达到0.9126的AUC，在多标签RFMiD数据集上达到0.8800的宏平均AUC。这项工作为未来关于大规模合成数据集的研究提供了强大的基线，并证明了完全在合成数据上训练的模型可以准确分类多种病理，并有效地泛化到真实的临床图像，为加速眼科领域全面AI系统的发展提供了可行的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of multi-label deep learning models for retinal diseaseclassification is often hindered by the scarcity of large, expertly annotatedclinical datasets due to patient privacy concerns and high costs. The recentrelease of SynFundus-1M, a high-fidelity synthetic dataset with over onemillion fundus images, presents a novel opportunity to overcome these barriers.To establish a foundational performance benchmark for this new resource, wedeveloped an end-to-end deep learning pipeline, training six modernarchitectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and theRETFound foundation model) to classify eleven retinal diseases using a 5-foldmulti-label stratified cross-validation strategy. We further developed ameta-ensemble model by stacking the out-of-fold predictions with an XGBoostclassifier. Our final ensemble model achieved the highest performance on theinternal validation set, with a macro-average Area Under the Receiver OperatingCharacteristic Curve (AUC) of 0.9973. Critically, the models demonstratedstrong generalization to three diverse, real-world clinical datasets, achievingan AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGSglaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.This work provides a robust baseline for future research on large-scalesynthetic datasets and establishes that models trained exclusively on syntheticdata can accurately classify multiple pathologies and generalize effectively toreal clinical images, offering a viable pathway to accelerate the developmentof comprehensive AI systems in ophthalmology.</description>
      <author>example@mail.com (Jerry Cao-Xue, Tien Comlekoglu, Keyi Xue, Guanliang Wang, Jiang Li, Gordon Laurie)</author>
      <guid isPermaLink="false">2508.15986v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation</title>
      <link>http://arxiv.org/abs/2508.15972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at the Conference on Robot Learning (CoRL) 2025. For more  details please visit https://frankzhaodong.github.io/UnPose&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UnPose是一种无需物体CAD模型的零样本6D物体姿态估计和重建框架，利用预训练扩散模型的3D先验和不确定性估计，从单视图RGB-D帧开始，通过多视图扩散模型估计初始3D模型，并随着更多观察的加入逐步改进，最终在6D姿态估计准确性和3D重建质量方面显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;估计新物体的6D姿态是机器人学中的基本挑战，传统方法依赖物体CAD模型，但获取这些模型成本高且不切实际。&lt;h4&gt;目的&lt;/h4&gt;提出UnPose框架，实现无需模型、零样本的6D物体姿态估计和重建，解决现有方法需要额外训练或产生几何幻觉的问题。&lt;h4&gt;方法&lt;/h4&gt;从单视图RGB-D帧开始，使用多视图扩散模型估计初始3D模型(使用3D高斯溅射表示)和像素级认识不确定性估计，随着更多观察的加入，在扩散模型不确定性指导下增量式改进3DGS模型，并通过姿态图优化确保全局一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明UnPose在6D姿态估计准确性和3D重建质量方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;UnPose在实际机器人操作任务中展示了良好的实际应用性。&lt;h4&gt;翻译&lt;/h4&gt;估计新物体的6D姿态是机器人学中的一个基本但具有挑战性的问题，通常依赖于获取物体CAD模型。然而，获取此类模型可能成本高昂且不切实际。最近的方法试图通过利用基础模型的强先验从单视图或多视图图像重建物体来绕过这一要求，但通常需要额外训练或产生几何幻觉。为此，我们提出了UnPose，一种新颖的零样本、无需模型的6D物体姿态估计和重建框架，它利用预训练扩散模型的3D先验和不确定性估计。具体来说，从单视图RGB-D帧开始，UnPose使用多视图扩散模型估计初始3D模型(使用3D高斯溅射表示)，并提供像素级认识不确定性估计。随着更多观察的可用，我们在扩散模型的不确定性指导下融合新视图，从而逐步改进3DGS模型，持续提高姿态估计准确性和3D重建质量。为确保全局一致性，将扩散先验生成的视图和后续观察进一步集成到姿态图中，并联合优化为一个连贯的3DGS场。大量实验证明，UnPose在6D姿态估计准确性和3D重建质量方面显著优于现有方法。我们进一步展示了它在实际机器人操作任务中的实际应用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the 6D pose of novel objects is a fundamental yet challengingproblem in robotics, often relying on access to object CAD models. However,acquiring such models can be costly and impractical. Recent approaches aim tobypass this requirement by leveraging strong priors from foundation models toreconstruct objects from single or multi-view images, but typically requireadditional training or produce hallucinated geometry. To this end, we proposeUnPose, a novel framework for zero-shot, model-free 6D object pose estimationand reconstruction that exploits 3D priors and uncertainty estimates from apre-trained diffusion model. Specifically, starting from a single-view RGB-Dframe, UnPose uses a multi-view diffusion model to estimate an initial 3D modelusing 3D Gaussian Splatting (3DGS) representation, along with pixel-wiseepistemic uncertainty estimates. As additional observations become available,we incrementally refine the 3DGS model by fusing new views guided by thediffusion model's uncertainty, thereby continuously improving the poseestimation accuracy and 3D reconstruction quality. To ensure globalconsistency, the diffusion prior-generated views and subsequent observationsare further integrated in a pose graph and jointly optimized into a coherent3DGS field. Extensive experiments demonstrate that UnPose significantlyoutperforms existing approaches in both 6D pose estimation accuracy and 3Dreconstruction quality. We further showcase its practical applicability inreal-world robotic manipulation tasks.</description>
      <author>example@mail.com (Zhaodong Jiang, Ashish Sinha, Tongtong Cao, Yuan Ren, Bingbing Liu, Binbin Xu)</author>
      <guid isPermaLink="false">2508.15972v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification</title>
      <link>http://arxiv.org/abs/2508.15960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个名为Glo-VLMs的系统框架，用于将预训练的视觉-语言模型适应于细粒度肾小球分类任务，即使在数据有限的情况下也能取得良好效果。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型在数字病理学中显示出巨大潜力，但对于细粒度的疾病特异性分类任务（如区分肾小球亚型）效果有限。肾小球亚型之间的细微形态差异，以及视觉模式与精确临床术语对齐的困难，使得肾脏病理学中的自动诊断特别具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;探索如何有效地将大型预训练视觉-语言模型适应于细粒度肾小球分类任务，即使在只有少量标记可用样本的情况下也能有效工作。&lt;h4&gt;方法&lt;/h4&gt;引入Glo-VLMs系统框架，利用精选的病理学图像和临床文本提示来促进联合图像-文本表示学习，评估在各种视觉-语言模型架构和适应策略下的性能，在少样本学习范式下探索方法选择和标记数据量对模型性能的影响，并使用标准化的多类指标进行公平比较。&lt;h4&gt;主要发现&lt;/h4&gt;使用每类仅8个样本的微调视觉-语言模型实现了0.7416的准确率、0.9045的宏AUC和0.5277的F1分数，表明即使在高度有限的监督下，基础模型也能有效地适应细粒度医学图像分类。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以在高度有限的监督下有效地适应细粒度医学图像分类，为专业临床研究应用中大型预训练模型的实际要求和潜力提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型在数字病理学中显示出巨大潜力，但其对于细粒度的疾病特异性分类任务（如区分肾小球亚型）的有效性仍然有限。这些亚型之间的细微形态差异，加上视觉模式与精确临床术语对齐的困难，使得肾脏病理学中的自动诊断特别具有挑战性。在这项工作中，我们探索了如何有效地将大型预训练视觉-语言模型适应于细粒度肾小球分类任务，即使在只有少量标记样本可用的情况下。在这项工作中，我们引入了Glo-VLMs，这是一个系统框架，旨在探索在数据受限情况下将视觉-语言模型适应于细粒度肾小球分类。我们的方法利用精选的病理学图像和临床文本提示，促进针对细微肾脏病理亚型的联合图像-文本表示学习。通过在少样本学习范式下评估各种视觉-语言模型架构和适应策略，我们探索了方法选择和标记数据量在临床相关场景中对模型性能的影响。为确保公平比较，我们使用标准化的多类指标评估所有模型，旨在阐明大型预训练模型在专业临床研究应用中的实际要求和潜力。结果表明，仅使用每类8个样本微调视觉-语言模型，就实现了0.7416的准确率、0.9045的宏AUC和0.5277的F1分数，证明即使在高度有限的监督下，基础模型也能有效地适应细粒度医学图像分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have shown considerable potential in digitalpathology, yet their effectiveness remains limited for fine-grained,disease-specific classification tasks such as distinguishing between glomerularsubtypes. The subtle morphological variations among these subtypes, combinedwith the difficulty of aligning visual patterns with precise clinicalterminology, make automated diagnosis in renal pathology particularlychallenging. In this work, we explore how large pretrained VLMs can beeffectively adapted to perform fine-grained glomerular classification, even inscenarios where only a small number of labeled examples are available. In thiswork, we introduce Glo-VLMs, a systematic framework designed to explore theadaptation of VLMs to fine-grained glomerular classification indata-constrained settings. Our approach leverages curated pathology imagesalongside clinical text prompts to facilitate joint image-text representationlearning for nuanced renal pathology subtypes. By assessing various VLMsarchitectures and adaptation strategies under a few-shot learning paradigm, weexplore how both the choice of method and the amount of labeled data impactmodel performance in clinically relevant scenarios. To ensure a faircomparison, we evaluate all models using standardized multi-class metrics,aiming to clarify the practical requirements and potential of large pretrainedmodels for specialized clinical research applications. As a result, fine-tuningthe VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score withonly 8 shots per class, demonstrating that even with highly limitedsupervision, foundation models can be effectively adapted for fine-grainedmedical image classification.</description>
      <author>example@mail.com (Zhenhao Guo, Rachit Saluja, Tianyuan Yao, Quan Liu, Yuankai Huo, Benjamin Liechty, David J. Pisapia, Kenji Ikemura, Mert R. Sabuncu, Yihe Yang, Ruining Deng)</author>
      <guid isPermaLink="false">2508.15960v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping</title>
      <link>http://arxiv.org/abs/2508.15904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为PathPT的新型框架，用于罕见癌症的诊断。该框架利用视觉语言病理学基础模型，通过空间感知的视觉聚合和任务特定的提示调优，解决了现有方法在罕见癌症诊断中的局限性。&lt;h4&gt;背景&lt;/h4&gt;罕见癌症占所有恶性肿瘤的20-25%，但在儿科肿瘤学中占比超过70%。由于专家资源有限，罕见癌症面临重大诊断挑战。现有的病理视觉语言基础模型在常见癌症亚型分类中显示出有希望的零样本能力，但在罕见癌症方面的临床性能仍然有限。现有的多实例学习(MIL)方法仅依赖视觉特征，忽略了跨模态知识，影响了罕见癌症诊断的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够充分利用视觉语言病理学基础模型潜力的新框架，解决罕见癌症诊断中的挑战，提高诊断准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出PathPT框架，通过空间感知的视觉聚合和任务特定的提示调优。与传统MIL不同，PathPT利用VL模型的零样本能力，将WSI级别的监督转换为细粒度的tile级别指导，保留癌症区域定位，并通过与组织病理学语义对齐的提示实现跨模态推理。在8种罕见癌症数据集和3种常见癌症数据集上进行了基准测试，评估了4种最先进的VL模型和4种MIL框架在3种少样本设置下的性能。&lt;h4&gt;主要发现&lt;/h4&gt;PathPT在多种设置下持续提供卓越的性能，在亚型分类准确性和癌症区域定位能力方面取得显著提升。在56种亚型和2,910个WSI上验证了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作推进了AI辅助的罕见癌症诊断，为在专业专业知识有限的情况下提高亚型分类准确性提供了一个可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;罕见癌症占所有恶性肿瘤的20-25%，但由于专家资源有限，特别是儿科肿瘤学中罕见癌症占比超过70%，因此面临重大诊断挑战。虽然病理视觉语言(VL)基础模型在常见癌症亚型分类中显示出有希望的零样本能力，但它们在罕见癌症方面的临床性能仍然有限。现有的多实例学习(MIL)方法仅依赖视觉特征，忽略了跨模态知识，影响了罕见癌症诊断中至关重要的可解释性。为解决这一局限，我们提出了PathPT，一种通过空间感知的视觉聚合和任务特定的提示调优来充分利用视觉语言病理学基础模型潜力的新框架。与传统MIL不同，PathPT利用VL模型的零样本能力，将WSI级别的监督转换为细粒度的tile级别指导，从而保留癌症区域的定位，并通过与组织病理学语义对齐的提示实现跨模态推理。我们在八种罕见癌症数据集（四种成人，四种儿科）上对PathPT进行了基准测试，涵盖56种亚型和2,910个WSI，以及三种常见癌症数据集，评估了四种最先进的VL模型和四种MIL框架在三种少样本设置下的性能。结果表明，PathPT持续提供卓越的性能，在亚型分类准确性和癌症区域定位能力方面取得显著提升。这项工作推进了AI辅助的罕见癌症诊断，为在专业专业知识有限的情况下提高亚型分类准确性提供了一个可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rare cancers comprise 20-25% of all malignancies but face major diagnosticchallenges due to limited expert availability-especially in pediatric oncology,where they represent over 70% of cases. While pathology vision-language (VL)foundation models show promising zero-shot capabilities for common cancersubtyping, their clinical performance for rare cancers remains limited.Existing multi-instance learning (MIL) methods rely only on visual features,overlooking cross-modal knowledge and compromising interpretability criticalfor rare cancer diagnosis. To address this limitation, we propose PathPT, anovel framework that fully exploits the potential of vision-language pathologyfoundation models through spatially-aware visual aggregation and task-specificprompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervisioninto fine-grained tile-level guidance by leveraging the zero-shot capabilitiesof VL models, thereby preserving localization on cancerous regions and enablingcross-modal reasoning through prompts aligned with histopathological semantics.We benchmark PathPT on eight rare cancer datasets(four adult and fourpediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancerdatasets, evaluating four state-of-the-art VL models and four MIL frameworksunder three few-shot settings. Results show that PathPT consistently deliverssuperior performance, achieving substantial gains in subtyping accuracy andcancerous region grounding ability. This work advances AI-assisted diagnosisfor rare cancers, offering a scalable solution for improving subtyping accuracyin settings with limited access to specialized expertise.</description>
      <author>example@mail.com (Dexuan He, Xiao Zhou, Wenbin Guan, Liyuan Zhang, Xiaoman Zhang, Sinuo Xu, Ge Wang, Lifeng Wang, Xiaojun Yuan, Xin Sun, Yanfeng Wang, Kun Sun, Ya Zhang, Weidi Xie)</author>
      <guid isPermaLink="false">2508.15904v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Intern-S1: A Scientific Multimodal Foundation Model</title>
      <link>http://arxiv.org/abs/2508.15763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Intern-S1，一个专门针对科学领域的多模态专家混合模型，通过算法、数据和训练系统的创新，在科学领域表现出色，特别是在专业任务上超越了闭源最先进模型。&lt;h4&gt;背景&lt;/h4&gt;近年来，大量开源基础模型在广泛关注领域取得了接近闭源模型的性能，但在高价值且更具挑战性的科学专业领域，进展显著落后，开源模型与闭源模型之间存在巨大差距，这些领域仍然依赖专家模型。&lt;h4&gt;目的&lt;/h4&gt;弥合开源模型与闭源模型在科学领域的差距，并向通用人工智能(AGI)迈进一步。&lt;h4&gt;方法&lt;/h4&gt;提出Intern-S1，一个多模态专家混合(MoE)模型，拥有280亿激活参数和2410亿总参数，在5T tokens上持续预训练，其中包括超过2.5T tokens来自科学领域。在后训练阶段，在InternBootCamp中经历离线和在线强化学习，提出奖励混合(MoR)方法，同时在1000多个任务上协同RL训练。&lt;h4&gt;主要发现&lt;/h4&gt;在综合评估基准上，Intern-S1在开源模型中展现出具有竞争力的通用推理性能，在科学领域显著优于开源模型，在分子合成规划、反应条件预测、预测晶体热力学稳定性等专业任务上超越闭源最先进模型。&lt;h4&gt;结论&lt;/h4&gt;通过算法、数据和训练系统的综合创新，Intern-S1成功缩小了开源模型与闭源模型在科学领域的差距，并在多个专业科学任务上取得了领先性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大量开源基础模型出现，在一些广泛关注领域取得了显著进展，性能相当接近闭源模型。然而，在高价值但更具挑战性的科学专业领域，这些领域仍然依赖专家模型，或者通用基础模型的进展显著落后于流行领域，远不足以改变科学研究，在这些科学领域中开源模型与闭源模型之间存在巨大差距。为了弥合这一差距并向通用人工智能(AGI)迈进一步，我们介绍了Intern-S1，一个具备通用理解和推理能力的专业通用模型，能够分析多种科学模态数据。Intern-S1是一个多模态专家混合(MoE)模型，拥有280亿激活参数和2410亿总参数，在5T tokens上持续预训练，其中包括超过2.5T tokens来自科学领域。在后训练阶段，Intern-S1在InternBootCamp中经历离线和在线强化学习(RL)，我们提出奖励混合(MoR)方法，同时在1000多个任务上协同RL训练。通过算法、数据和训练系统的综合创新，Intern-S1在在线RL训练中取得了顶级性能。在综合评估基准上，Intern-S1在开源模型中展现出具有竞争力的通用推理性能，在科学领域显著优于开源模型，在分子合成规划、反应条件预测、预测晶体热力学稳定性等专业任务上超越闭源最先进模型。我们的模型可在https://huggingface.co/internlm/Intern-S1获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, a plethora of open-source foundation models have emerged,achieving remarkable progress in some widely attended fields, with performancebeing quite close to that of closed-source models. However, in high-value butmore challenging scientific professional fields, either the fields still relyon expert models, or the progress of general foundation models lagssignificantly compared to those in popular areas, far from sufficient fortransforming scientific research and leaving substantial gap betweenopen-source models and closed-source models in these scientific domains. Tomitigate this gap and explore a step further toward Artificial GeneralIntelligence (AGI), we introduce Intern-S1, a specialized generalist equippedwith general understanding and reasoning capabilities with expertise to analyzemultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)model with 28 billion activated parameters and 241 billion total parameters,continually pre-trained on 5T tokens, including over 2.5T tokens fromscientific domains. In the post-training stage, Intern-S1 undergoes offline andthen online reinforcement learning (RL) in InternBootCamp, where we proposeMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 taskssimultaneously. Through integrated innovations in algorithms, data, andtraining systems, Intern-S1 achieved top-tier performance in online RLtraining.On comprehensive evaluation benchmarks, Intern-S1 demonstratescompetitive performance on general reasoning tasks among open-source models andsignificantly outperforms open-source models in scientific domains, surpassingclosed-source state-of-the-art models in professional tasks, such as molecularsynthesis planning, reaction condition prediction, predicting thermodynamicstabilities for crystals. Our models are available athttps://huggingface.co/internlm/Intern-S1.</description>
      <author>example@mail.com (Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou)</author>
      <guid isPermaLink="false">2508.15763v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Waver: Wave Your Way to Lifelike Video Generation</title>
      <link>http://arxiv.org/abs/2508.15761v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Waver，一个高性能的基础模型，用于统一的图像和视频生成，能够同时支持文本到视频、图像到视频和文本到图像生成。&lt;h4&gt;背景&lt;/h4&gt;视频生成技术不断发展，但现有模型在性能、质量和功能整合方面仍有提升空间。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够高质量生成视频和图像的基础模型，提升模态对齐和训练效率，并提供详细的训练和推理方法。&lt;h4&gt;方法&lt;/h4&gt;提出混合流DiT架构，建立全面的数据筛选流程，训练基于MLLM的视频质量模型，提供详细的训练和推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;Waver能够生成5-10秒的高质量视频，擅长捕捉复杂运动，在视频合成中实现卓越的运动幅度和时间一致性，在T2V和I2V排行榜上排名前3。&lt;h4&gt;结论&lt;/h4&gt;Waver持续优于现有的开源模型，匹配或超越最先进的商业解决方案，为视频生成技术的发展提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Waver，一个用于统一图像和视频生成的高性能基础模型。Waver可以直接生成持续5到10秒、原生分辨率为720p的视频，随后放大到1080p。该模型在单一集成框架内同时支持文本到视频(T2V)、图像到视频(I2V)和文本到图像(T2I)生成。我们引入了混合流DiT架构以增强模态对齐和加速训练收敛。为确保训练数据质量，我们建立了全面的数据筛选流程，并手动标注和训练了一个基于MLLM的视频质量模型，以筛选最高质量的样本。此外，我们提供了详细的训练和推理方法，以促进高质量视频的生成。基于这些贡献，Waver在捕捉复杂运动方面表现出色，在视频合成中实现了卓越的运动幅度和时间一致性。值得注意的是，截至2025年7月30日10:00 GMT+8，它在Artificial Analysis的T2V和I2V排行榜上均排名前3，持续优于现有的开源模型，匹配或超越最先进的商业解决方案。我们希望这份技术报告能帮助社区更高效地训练高质量视频生成模型，并加速视频生成技术的进步。官方页面：https://github.com/FoundationVision/Waver。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Waver, a high-performance foundation model for unified image andvideo generation. Waver can directly generate videos with durations rangingfrom 5 to 10 seconds at a native resolution of 720p, which are subsequentlyupscaled to 1080p. The model simultaneously supports text-to-video (T2V),image-to-video (I2V), and text-to-image (T2I) generation within a single,integrated framework. We introduce a Hybrid Stream DiT architecture to enhancemodality alignment and accelerate training convergence. To ensure training dataquality, we establish a comprehensive data curation pipeline and manuallyannotate and train an MLLM-based video quality model to filter for thehighest-quality samples. Furthermore, we provide detailed training andinference recipes to facilitate the generation of high-quality videos. Buildingon these contributions, Waver excels at capturing complex motion, achievingsuperior motion amplitude and temporal consistency in video synthesis. Notably,it ranks among the Top 3 on both the T2V and I2V leaderboards at ArtificialAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperformingexisting open-source models and matching or surpassing state-of-the-artcommercial solutions. We hope this technical report will help the communitymore efficiently train high-quality video generation models and accelerateprogress in video generation technologies. Official page:https://github.com/FoundationVision/Waver.</description>
      <author>example@mail.com (Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng)</author>
      <guid isPermaLink="false">2508.15761v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>"Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries</title>
      <link>http://arxiv.org/abs/2508.15752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the ICCV'25 Workshop "Vision Foundation Models and  Generative AI for Accessibility: Challenges and Opportunities"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了地理可视化智能体（Geo-Visual Agents）的愿景，这是一种多模态AI智能体，能够通过分析大规模地理空间图像存储库来理解和响应细微的视觉空间查询，结合传统GIS数据源，回答关于世界外观的地理可视化问题。&lt;h4&gt;背景&lt;/h4&gt;交互式数字地图虽然改变了人们旅行和了解世界的方式，但它们依赖于预先存在的结构化GIS数据（如道路网络、POI索引），限制了它们回答关于'世界看起来是什么样子'的地理可视化问题的能力。&lt;h4&gt;目的&lt;/h4&gt;提出地理可视化智能体（Geo-Visual Agents）的愿景，创建能够理解和响应细微视觉空间查询的多模态AI智能体。&lt;h4&gt;方法&lt;/h4&gt;通过分析大规模地理空间图像存储库（包括街景如Google街景、基于地点的照片如TripAdvisor和Yelp、航空影像如卫星照片）并结合传统GIS数据源来实现地理可视化智能体。&lt;h4&gt;主要发现&lt;/h4&gt;论文定义了地理可视化智能体的愿景，描述了感知和交互方法，并提供了三个具体示例。&lt;h4&gt;结论&lt;/h4&gt;论文列举了地理可视化智能体未来工作的关键挑战和机会。&lt;h4&gt;翻译&lt;/h4&gt;交互式数字地图彻底改变了人们旅行和了解世界的方式；然而，它们依赖于GIS数据库中预先存在的结构化数据（例如道路网络、POI索引），这限制了它们回答关于世界外观的地理可视化问题的能力。我们介绍了地理可视化智能体的愿景——这些是多模态AI智能体，能够通过分析大规模地理空间图像存储库（包括街景如Google街景、基于地点的照片如TripAdvisor、Yelp，以及航空影像如卫星照片）并结合传统GIS数据源，来理解和响应关于世界的细微视觉空间查询。我们定义了愿景，描述了感知和交互方法，提供了三个示例，并列举了未来工作的关键挑战和机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是当前地图系统无法回答关于世界外观的地理视觉问题。例如，盲人用户想知道'咖啡店的入口是否可及？门在哪里？'这类问题。这个问题很重要，因为现有地图系统依赖预存在的结构化数据，无法充分利用大量地理空间图像信息，导致残障人士（如盲人、轮椅使用者）难以获取环境可访问性信息，普通用户也难以获得丰富的视觉导航和位置信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前地图系统的局限性，意识到大量地理空间图像数据未被充分利用，然后提出多模态AI代理的概念来整合视觉和空间数据。他们设计了针对不同旅行阶段（旅行前、旅行中、到达目的地、室内探索）的代理功能。作者借鉴了现有工作，包括地理空间人工智能（GeoAI）如CARTO AI、'自主GIS'概念、地理空间视觉问答（GVQA）系统、街景视图技术和多模态大语言模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建能够理解和响应复杂视觉-空间查询的多模态AI代理，整合多种数据源（街道景观图像、用户贡献照片、空中图像等），结合传统GIS数据提供全面的地理空间理解，并根据用户需求和能力提供个性化响应。整体实现流程包括：1)数据收集（从多种来源获取地理空间图像）；2)数据处理（使用AI技术分析图像）；3)查询理解（理解用户的视觉-空间查询）；4)数据融合（结合视觉证据和结构化地理空间数据）；5)响应生成（根据用户需求和能力生成适当的响应）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了地理视觉代理（Geo-Visual Agents）的新概念；2)关注个人、交互式和即时需求，而非专家分析；3)整合多种数据源（街道景观、用户照片、空中图像等）；4)针对不同旅行阶段提供不同功能；5)专注于可访问性和用户体验。相比之前的工作，这种方法不同于传统GeoAI系统主要面向专家，不同于'自主GIS'更广泛的科学目标，不同于GVQA系统主要关注远程空中图像，也不同于现有地图系统仅依赖预存在的结构化数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了地理视觉代理的概念，通过整合多种地理空间图像和传统GIS数据，使AI系统能够理解和响应复杂的视觉-空间查询，从而改善导航体验，特别是对残障人士的可访问性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive digital maps have revolutionized how people travel and learnabout the world; however, they rely on pre-existing structured data in GISdatabases (e.g., road networks, POI indices), limiting their ability to addressgeo-visual questions related to what the world looks like. We introduce ourvision for Geo-Visual Agents--multimodal AI agents capable of understanding andresponding to nuanced visual-spatial inquiries about the world by analyzinglarge-scale repositories of geospatial images, including streetscapes (e.g.,Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerialimagery (e.g., satellite photos) combined with traditional GIS data sources. Wedefine our vision, describe sensing and interaction approaches, provide threeexemplars, and enumerate key challenges and opportunities for future work.</description>
      <author>example@mail.com (Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane)</author>
      <guid isPermaLink="false">2508.15752v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model</title>
      <link>http://arxiv.org/abs/2508.15751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 3 figures, accepted by Journal of Medical Imaging&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分子赋能的全能SAM模型(All-in-SAM)，通过全栈方法改进计算病理学中的细胞分割和分类性能，减少标注需求并提高分析精度。&lt;h4&gt;背景&lt;/h4&gt;计算病理学的发展受到视觉基础模型特别是Segment Anything Model(SAM)的推动。SAM通过基于提示的零样本分割和特定细胞SAM模型直接分割实现细胞核分割，但在细粒度语义分割方面存在挑战，如识别特定细胞亚型或特定细胞。&lt;h4&gt;目的&lt;/h4&gt;提出分子赋能的全能SAM模型，利用视觉基础模型的能力推进计算病理学发展，解决细粒度语义分割问题。&lt;h4&gt;方法&lt;/h4&gt;采用全栈方法：(1)标注参与-通过分子赋能学习吸引标注者，减少对详细像素级标注的需求；(2)学习适应-使用SAM适配器调整模型以强调特定语义，利用其强大泛化能力；(3)改进优化-整合分子导向校正学习(MOCL)提高分割准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在内部和公共数据集上的实验结果表明，All-in-SAM模型显著提高了细胞分类性能，即使在面对不同标注质量的情况下也能保持良好表现。&lt;h4&gt;结论&lt;/h4&gt;该方法减少了标注者工作负担，将精确生物医学图像分析扩展到资源有限环境，推进医学诊断和自动化病理图像分析。&lt;h4&gt;翻译&lt;/h4&gt;目的：计算病理学的最新发展由视觉基础模型特别是Segment Anything Model(SAM)的进步推动。该模型通过两种主要方法实现细胞核分割：基于提示的零样本分割和使用特定细胞SAM模型的直接分割。这些方法能够在各种细胞核和细胞中实现有效分割。然而，通用视觉基础模型在细粒度语义分割方面常面临挑战，例如识别特定细胞亚型或特定细胞。方法：本文提出分子赋能的全能SAM模型，通过利用视觉基础模型的能力推进计算病理学。该模型采用全栈方法，专注于：(1)标注参与-通过分子赋能学习吸引标注者，减少对详细像素级标注的需求；(2)学习-调整SAM模型以强调特定语义，利用SAM适配器发挥其强大泛化能力；(3)改进-通过整合分子导向校正学习(MOCL)提高分割准确性。结果：内部和公共数据集的实验结果表明，All-in-SAM模型显著提高了细胞分类性能，即使面对不同标注质量也是如此。结论：我们的方法不仅减少了标注者的工作负担，还将精确的生物医学图像分析扩展到资源有限的环境，从而推进医学诊断和自动化病理图像分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Recent developments in computational pathology have been driven byadvances in Vision Foundation Models, particularly the Segment Anything Model(SAM). This model facilitates nuclei segmentation through two primary methods:prompt-based zero-shot segmentation and the use of cell-specific SAM models fordirect segmentation. These approaches enable effective segmentation across arange of nuclei and cells. However, general vision foundation models often facechallenges with fine-grained semantic segmentation, such as identifyingspecific nuclei subtypes or particular cells. Approach: In this paper, wepropose the molecular-empowered All-in-SAM Model to advance computationalpathology by leveraging the capabilities of vision foundation models. Thismodel incorporates a full-stack approach, focusing on: (1) annotation-engaginglay annotators through molecular-empowered learning to reduce the need fordetailed pixel-level annotations, (2) learning-adapting the SAM model toemphasize specific semantics, which utilizes its strong generalizability withSAM adapter, and (3) refinement-enhancing segmentation accuracy by integratingMolecular-Oriented Corrective Learning (MOCL). Results: Experimental resultsfrom both in-house and public datasets show that the All-in-SAM modelsignificantly improves cell classification performance, even when faced withvarying annotation quality. Conclusions: Our approach not only reduces theworkload for annotators but also extends the accessibility of precisebiomedical image analysis to resource-limited settings, thereby advancingmedical diagnostics and automating pathology image analysis.</description>
      <author>example@mail.com (Xueyuan Li, Can Cui, Ruining Deng, Yucheng Tang, Quan Liu, Tianyuan Yao, Shunxing Bao, Naweed Chowdhury, Haichun Yang, Yuankai Huo)</author>
      <guid isPermaLink="false">2508.15751v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Cross-Domain EEG Analysis Application: A Survey</title>
      <link>http://arxiv.org/abs/2508.15716v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Journals&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EEG分析中基础模型的第一个全面的模态导向分类法，系统组织了研究进展，分析了各类别的理论基础和架构创新，并强调了开放挑战，为EEG基础模型的实际应用提供了参考框架。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)分析是神经科学和人工智能研究的前沿领域，基础模型正在利用其强大的表示能力和跨模态泛化能力重塑传统EEG分析范式。&lt;h4&gt;目的&lt;/h4&gt;填补EEG分析基础模型研究领域的碎片化空白，提供第一个全面的模态导向分类法，系统组织研究进展。&lt;h4&gt;方法&lt;/h4&gt;基于输出模态对EEG基础模型进行分类，包括原生EEG解码、EEG-文本、EEG-视觉、EEG-音频和更广泛的多模态框架；严格分析每个类别的研究思路、理论基础和架构创新；强调开放挑战如模型可解释性、跨域泛化能力和实际应用性。&lt;h4&gt;主要发现&lt;/h4&gt;EEG基础模型研究存在碎片化问题，表现为模型角色多样、架构不一致和缺乏系统分类；通过模态导向分类法可以系统组织这一分散领域；EEG基础模型向可扩展、可解释和在线可操作的解决方案转化存在挑战。&lt;h4&gt;结论&lt;/h4&gt;通过统一分散的EEG基础模型研究领域，该研究为未来方法发展提供了参考框架，并促进了EEG基础模型向实际应用场景的转化。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)分析处于神经科学和人工智能研究的前沿，基础模型正在利用其强大的表示能力和跨模态泛化能力重塑传统的EEG分析范式。然而，这些技术的快速扩展导致了碎片化的研究景观，表现为多样化的模型角色、不一致的架构和缺乏系统分类。为了填补这一空白，本研究提出了EEG分析中基础模型的第一个全面的模态导向分类法，基于原生EEG解码、EEG-文本、EEG-视觉、EEG-音频以及更广泛的多模态框架的输出模态系统地组织研究进展。我们严格分析了每个类别的研究思路、理论基础和架构创新，同时强调了开放挑战，如模型可解释性、跨域泛化能力和基于EEG系统的实际应用性。通过统一这个分散的领域，我们的研究不仅为未来的方法发展提供了参考框架，还促进了EEG基础模型向可扩展、可解释和在线可操作的解决方案的转化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) analysis stands at the forefront of neuroscienceand artificial intelligence research, where foundation models are reshaping thetraditional EEG analysis paradigm by leveraging their powerful representationalcapacity and cross-modal generalization. However, the rapid proliferation ofthese techniques has led to a fragmented research landscape, characterized bydiverse model roles, inconsistent architectures, and a lack of systematiccategorization. To bridge this gap, this study presents the first comprehensivemodality-oriented taxonomy for foundation models in EEG analysis,systematically organizing research advances based on output modalities of thenative EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodalframeworks. We rigorously analyze each category's research ideas, theoreticalfoundations, and architectural innovations, while highlighting open challengessuch as model interpretability, cross-domain generalization, and real-worldapplicability in EEG-based systems. By unifying this dispersed field, our worknot only provides a reference framework for future methodology development butaccelerates the translation of EEG foundation models into scalable,interpretable, and online actionable solutions.</description>
      <author>example@mail.com (Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang)</author>
      <guid isPermaLink="false">2508.15716v2</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>On Evaluating the Adversarial Robustness of Foundation Models for Multimodal Entity Linking</title>
      <link>http://arxiv.org/abs/2508.15481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次系统评估了多模态实体链接(MEL)模型在视觉对抗攻击下的鲁棒性，发现现有模型普遍缺乏足够的抗干扰能力。基于此，研究者提出了一种结合大语言模型和检索增强的实体链接方法(LLM-RetLink)，显著提升了模型在对抗条件下的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态数据的爆炸性增长推动了多模态实体链接(MEL)模型的快速发展，但现有研究尚未系统研究视觉对抗攻击对MEL模型的影响。&lt;h4&gt;目的&lt;/h4&gt;首次全面评估主流MEL模型在不同对抗攻击场景下的鲁棒性，覆盖Image-to-Text (I2T)和Image+Text-to-Text (IT2T)两个核心任务。&lt;h4&gt;方法&lt;/h4&gt;提出LLM-RetLink方法，采用两阶段过程：首先使用大型视觉模型(LVMs)提取初始实体描述，然后通过基于网络的检索动态生成候选描述句子，从而提高模型的抗干扰能力。&lt;h4&gt;主要发现&lt;/h4&gt;当前MEL模型普遍缺乏对视觉扰动的足够鲁棒性；输入中的上下文语义信息可以部分减轻对抗扰动的影响；LLM-RetLink在五个数据集上提高了MEL的准确性0.4%-35.7%，尤其在对抗条件下显示出显著优势。&lt;h4&gt;结论&lt;/h4&gt;研究突显了MEL鲁棒性之前未被探索的方面，构建并发布了首个MEL对抗示例数据集，为未来加强多模态系统在对抗环境中的韧性奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据的爆炸性增长推动了多模态实体链接(MEL)模型的快速发展。然而，现有研究尚未系统研究视觉对抗攻击对MEL模型的影响。我们首次对主流MEL模型在不同对抗攻击场景下的鲁棒性进行了全面评估，涵盖两个核心任务：Image-to-Text (I2T)和Image+Text-to-Text (IT2T)。实验结果表明，当前MEL模型普遍缺乏对视觉扰动的足够鲁棒性。有趣的是，输入中的上下文语义信息可以部分减轻对抗扰动的影响。基于这一见解，我们提出了LLM-RetLink方法，通过两阶段过程显著提高了模型的抗干扰能力：首先使用大型视觉模型(LVMs)提取初始实体描述，然后通过基于网络的检索动态生成候选描述句子。在五个数据集上的实验表明，LLM-RetLink将MEL的准确性提高了0.4%-35.7%，特别是在对抗条件下显示出显著优势。这项研究突显了MEL鲁棒性之前未被探索的方面，构建并发布了首个MEL对抗示例数据集，为未来旨在加强多模态系统在对抗环境中韧性的工作奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The explosive growth of multimodal data has driven the rapid development ofmultimodal entity linking (MEL) models. However, existing studies have notsystematically investigated the impact of visual adversarial attacks on MELmodels. We conduct the first comprehensive evaluation of the robustness ofmainstream MEL models under different adversarial attack scenarios, coveringtwo core tasks: Image-to-Text (I2T) and Image+Text-to-Text (IT2T). Experimentalresults show that current MEL models generally lack sufficient robustnessagainst visual perturbations. Interestingly, contextual semantic information ininput can partially mitigate the impact of adversarial perturbations. Based onthis insight, we propose an LLM and Retrieval-Augmented Entity Linking(LLM-RetLink), which significantly improves the model's anti-interferenceability through a two-stage process: first, extracting initial entitydescriptions using large vision models (LVMs), and then dynamically generatingcandidate descriptive sentences via web-based retrieval. Experiments on fivedatasets demonstrate that LLM-RetLink improves the accuracy of MEL by0.4%-35.7%, especially showing significant advantages under adversarialconditions. This research highlights a previously unexplored facet of MELrobustness, constructs and releases the first MEL adversarial example dataset,and sets the stage for future work aimed at strengthening the resilience ofmultimodal systems in adversarial environments.</description>
      <author>example@mail.com (Fang Wang, Yongjie Wang, Zonghao Yang, Minghao Hu, Xiaoying Bai)</author>
      <guid isPermaLink="false">2508.15481v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>AudioSet-R: A Refined AudioSet with Multi-Stage LLM Label Reannotation</title>
      <link>http://arxiv.org/abs/2508.15429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, accepted in ACM MM 2025 dataset track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种三阶段重新标注框架，利用音频语言基础模型改进AudioSet数据集的标签质量，构建了高质量的AudioSet-R数据集，并在多个音频分类模型上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;AudioSet是音频研究社区广泛使用的基准数据集，已显著推进各种音频相关任务，但标签准确性和完整性问题仍然是限制下游应用性能的关键瓶颈。&lt;h4&gt;目的&lt;/h4&gt;解决AudioSet数据集的标签质量问题，构建高质量的、结构化的重新标注版本AudioSet-R。&lt;h4&gt;方法&lt;/h4&gt;提出一个三阶段重新标注框架，利用通用音频语言基础模型提高标签质量，采用跨模态提示策略，通过顺序组合提示来执行音频理解、标签合成和语义对齐三个子任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AST、PANNs、SSAST和AudioMAE等代表性音频分类模型上的实验一致显示，使用AudioSet-R能带来显著的性能提升，验证了所提出方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该三阶段重新标注框架能够有效提高标签可靠性，具有通用性和有效性，代码已在GitHub公开：https://github.com/colaudiolab/AudioSet-R。&lt;h4&gt;翻译&lt;/h4&gt;AudioSet是音频研究社区广泛使用的基准，已显著推进各种音频相关任务。然而，标签准确性和完整性的持续问题仍然是限制下游应用性能的关键瓶颈。为解决上述挑战，我们提出了一种三阶段重新标注框架，利用通用音频语言基础模型系统提高AudioSet的标签质量。该框架采用跨模态提示策略，受提示链概念启发，通过顺序组合提示来执行子任务（音频理解、标签合成和语义对齐）。利用此框架，我们构建了高质量的、结构化的AudioSet-R重新标注版本。在AST、PANNs、SSAST和AudioMAE等代表性音频分类模型上进行的大量实验一致显示出显著的性能提升，从而验证了所提出方法在提高标签可靠性方面的通用性和有效性。代码已在GitHub公开：https://github.com/colaudiolab/AudioSet-R。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758260&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AudioSet is a widely used benchmark in the audio research community and hassignificantly advanced various audio-related tasks. However, persistent issueswith label accuracy and completeness remain critical bottlenecks that limitperformance in downstream applications.To address the aforementionedchallenges, we propose a three-stage reannotation framework that harnessesgeneral-purpose audio-language foundation models to systematically improve thelabel quality of AudioSet. The framework employs a cross-modal promptingstrategy, inspired by the concept of prompt chaining, wherein prompts aresequentially composed to execute subtasks (audio comprehension, labelsynthesis, and semantic alignment). Leveraging this framework, we construct ahigh-quality, structured relabeled version of AudioSet-R. Extensive experimentsconducted on representative audio classification models--including AST, PANNs,SSAST, and AudioMAE--consistently demonstrate substantial performanceimprovements, thereby validating the generalizability and effectiveness of theproposed approach in enhancing label reliability.The code is publicly availableat: https://github.com/colaudiolab/AudioSet-R.</description>
      <author>example@mail.com (Yulin Sun, Qisheng Xu, Yi Su, Qian Zhu, Yong Dou, Xinwang Liu, Kele Xu)</author>
      <guid isPermaLink="false">2508.15429v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations</title>
      <link>http://arxiv.org/abs/2508.15404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了掩码自编码器（MAEs）超参数与下游任务性能之间的关系，揭示了MAEs如何学习图像中的空间相关性，并提供了超参数选择的实践见解。&lt;h4&gt;背景&lt;/h4&gt;掩码自编码器（MAEs）已成为视觉基础模型的有效预训练技术，但当应用于新数据集时需要大量超参数调整。虽然先前研究已从注意力和潜变量模型角度分析MAEs，但超参数与下游任务性能之间的联系尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究MAEs如何学习输入图像中的空间相关性，分析超参数如何影响模型学习到的特征，并提供超参数选择的实践指导。&lt;h4&gt;方法&lt;/h4&gt;通过数学推导分析线性MAE学习到的特征，展示掩码比例和块大小如何选择短程和长程空间相关性特征，并将分析扩展到非线性MAE，研究其如何适应数据集中的空间相关性。&lt;h4&gt;主要发现&lt;/h4&gt;掩码比例和块大小可用于选择性地捕获短程和长程空间相关性特征；MAE表示能够适应数据集中的空间相关性，超越二阶统计特性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了关于如何在实际应用中选择MAE超参数的见解，填补了MAE超参数与下游任务性能之间关系的知识空白。&lt;h4&gt;翻译&lt;/h4&gt;掩码自编码器（MAEs）已成为视觉基础模型的一种强大预训练技术。尽管它们有效，但当应用于新数据集时，需要大量的超参数调整（掩码比例、块大小、编码器/解码器层数）。虽然之前的工作已经从注意力和分层潜变量模型的角度分析了MAEs，但MAE超参数与下游任务性能之间的联系相对未被探索。本研究调查了MAEs如何学习输入图像中的空间相关性。我们通过数学推导分析了线性MAE学习到的特征，并表明掩码比例和块大小可用于选择捕获短程和长程空间相关性的特征。我们将此分析扩展到非线性MAE，以展示MAE表示如何适应数据集中的空间相关性，超越二阶统计。最后，我们讨论了一些关于如何在实践中选择MAE超参数的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked Autoencoders (MAEs) have emerged as a powerful pretraining techniquefor vision foundation models. Despite their effectiveness, they requireextensive hyperparameter tuning (masking ratio, patch size, encoder/decoderlayers) when applied to novel datasets. While prior theoretical works haveanalyzed MAEs in terms of their attention patterns and hierarchical latentvariable models, the connection between MAE hyperparameters and performance ondownstream tasks is relatively unexplored. This work investigates how MAEslearn spatial correlations in the input image. We analytically derive thefeatures learned by a linear MAE and show that masking ratio and patch size canbe used to select for features that capture short- and long-range spatialcorrelations. We extend this analysis to non-linear MAEs to show that MAErepresentations adapt to spatial correlations in the dataset, beyondsecond-order statistics. Finally, we discuss some insights on how to select MAEhyper-parameters in practice.</description>
      <author>example@mail.com (Anthony Bisulco, Rahul Ramesh, Randall Balestriero, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2508.15404v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</title>
      <link>http://arxiv.org/abs/2508.15313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RAG-SEG的新型训练范式，用于解决伪装物体检测问题，该方法无需训练即可达到与现有先进方法相当或更好的性能，同时具有很高的计算效率。&lt;h4&gt;背景&lt;/h4&gt;伪装物体检测在计算机视觉中具有挑战性，因为物体与背景高度相似。现有方法通常依赖大量训练和计算资源。基础模型如Segment Anything Model(SAM)在未经微调和缺乏高质量提示的情况下难以处理COD任务，而手动生成此类提示成本高且效率低。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练的COD方法，减少对计算资源的依赖，同时避免手动生成高质量提示的需要。&lt;h4&gt;方法&lt;/h4&gt;提出RAG-SEG范式，将COD解耦为两个阶段：1)使用检索增强生成(RAG)通过无监督聚类构建的紧凑检索数据库生成粗略掩码作为提示；2)使用基于SAM的分割(SEG)进行细化。在推理过程中，检索到的特征生成伪标签，引导SAM2进行精确掩码生成。&lt;h4&gt;主要发现&lt;/h4&gt;RAG-SEG在无需传统训练的情况下保持了竞争性能。在基准COD数据集上的广泛实验表明，该方法的表现与或优于最先进的方法。所有实验均在个人笔记本电脑上进行，突显了其计算效率和实用性。&lt;h4&gt;结论&lt;/h4&gt;RAG-SEG是一种有效的训练范式，能够高效处理伪装物体检测任务，具有高度的计算效率和实用性，为资源受限环境下的COD应用提供了可行解决方案。&lt;h4&gt;翻译&lt;/h4&gt;伪装物体检测在计算机视觉中构成重大挑战，因为物体与其背景高度相似。现有方法通常依赖大量训练和计算资源。虽然Segment Anything Model(SAM)等基础模型提供了强大的泛化能力，但在没有微调和高质量提示的情况下，它们仍然难以处理COD任务。然而，手动生成此类提示成本高昂且效率低下。为解决这些挑战，我们提出了RAG-SEG（First RAG, Second SEG），一种无需训练的范式，将COD解耦为两个阶段：使用检索增强生成(RAG)生成粗略掩码作为提示，然后使用基于SAM的分割(SEG)进行细化。RAG-SEG通过无监督聚类构建紧凑的检索数据库，实现快速有效的特征检索。在推理过程中，检索到的特征生成伪标签，引导使用SAM2进行精确的掩码生成。我们的方法消除了对传统训练的需求，同时保持了竞争性能。在基准COD数据集上的广泛实验表明，RAG-SEG的表现与或优于最先进的方法。值得注意的是，所有实验都在个人笔记本电脑上进行，突显了我们方法的计算效率和实用性。我们在附录中提供了进一步分析，涵盖局限性、显著物体检测扩展和可能的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camouflaged object detection (COD) poses a significant challenge in computervision due to the high similarity between objects and their backgrounds.Existing approaches often rely on heavy training and large computationalresources. While foundation models such as the Segment Anything Model (SAM)offer strong generalization, they still struggle to handle COD tasks withoutfine-tuning and require high-quality prompts to yield good performance.However, generating such prompts manually is costly and inefficient. To addressthese challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, atraining-free paradigm that decouples COD into two stages: Retrieval-AugmentedGeneration (RAG) for generating coarse masks as prompts, followed by SAM-basedsegmentation (SEG) for refinement. RAG-SEG constructs a compact retrievaldatabase via unsupervised clustering, enabling fast and effective featureretrieval. During inference, the retrieved features produce pseudo-labels thatguide precise mask generation using SAM2. Our method eliminates the need forconventional training while maintaining competitive performance. Extensiveexperiments on benchmark COD datasets demonstrate that RAG-SEG performs on parwith or surpasses state-of-the-art methods. Notably, all experiments areconducted on a \textbf{personal laptop}, highlighting the computationalefficiency and practicality of our approach. We present further analysis in theAppendix, covering limitations, salient object detection extension, andpossible improvements.</description>
      <author>example@mail.com (Wutao Liu, YiDan Wang, Pan Gao)</author>
      <guid isPermaLink="false">2508.15313v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Survey of Vision-Language-Action Models for Embodied Manipulation</title>
      <link>http://arxiv.org/abs/2508.15201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  in Chinese language&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面审查了用于具身操作的视觉-语言-行动模型，追踪了VLA架构的发展轨迹，从五个关键维度分析了当前研究，并总结了挑战与未来方向。&lt;h4&gt;背景&lt;/h4&gt;具身智能系统通过持续的环境交互增强智能体能力，受到学术界和工业界的广泛关注。受大型基础模型进展启发的视觉-语言-行动模型作为通用机器人控制框架，显著提升了具身智能系统中智能体-环境交互能力，扩展了具身AI机器人的应用场景。&lt;h4&gt;目的&lt;/h4&gt;全面审查用于具身操作的VLA模型，系统梳理其发展历程和研究现状。&lt;h4&gt;方法&lt;/h4&gt;追踪VLA架构的发展轨迹，从五个关键维度进行详细分析：VLA模型结构、训练数据集、预训练方法、后训练方法和模型评估，并总结关键挑战与未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;视觉-语言-行动模型作为通用机器人控制框架，显著提升了具身智能系统中智能体-环境交互能力，扩展了具身AI机器人的应用场景。&lt;h4&gt;结论&lt;/h4&gt;VLA模型在具身操作领域展现出巨大潜力，但仍面临发展和实际部署中的挑战，需要进一步研究以推动该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;具身智能系统通过持续的环境交互增强智能体能力，已引起学术界和工业界的广泛关注。受大型基础模型进展启发的视觉-语言-行动模型作为通用机器人控制框架，显著提升了具身智能系统中智能体-环境交互能力。这一扩展拓宽了具身AI机器人的应用场景。本综述全面审查了用于具身操作的VLA模型。首先，它追溯了VLA架构的发展轨迹。随后，我们从5个关键维度对当前研究进行了详细分析：VLA模型结构、训练数据集、预训练方法、后训练方法和模型评估。最后，我们总结了VLA发展和实际部署中的关键挑战，并概述了有前景的未来研究方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决具身智能系统中视觉-语言-动作(VLA)模型的系统化梳理和总结问题。这个问题在现实中非常重要，因为具身智能是AI从抽象数据向物理世界延伸的重要方向，而VLA模型作为受大模型启发的机器人通用控制模型，能显著提升机器人与环境交互的能力，扩展应用场景。传统机器人系统难以应对开放环境的复杂任务，而VLA模型通过多模态融合实现从环境理解到物理执行的闭环耦合，对推动机器人技术在工业、家庭、服务等领域的落地应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这篇论文是一篇综述文章，作者通过系统化的文献调研，对VLA模型的相关研究进行了全面梳理。作者采用结构化的内容组织方式，从具身操作与VLA的关系入手，按照发展历程、模型架构、训练数据、预训练方法、后训练方法和模型评估的逻辑顺序展开内容。作者对VLA模型的各个方面进行了分类和归纳，如将发展历程划分为三个阶段，将预训练方法分为四类等。作者在撰写过程中大量借鉴了现有工作，包括参考了Ma et al.、Sapkota et al.等VLA相关综述文章，以及RT-1、RT-2、Octo等具体VLA模型研究，同时还借鉴了各种数据集和评估方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; VLA模型的核心思想是将视觉感知、语义推理与动作生成深度融合，使机器人能够直接从多模态输入中预测连续控制指令，实现从环境理解到物理执行的闭环耦合。其整体实现流程包括：1)观测编码：将视觉和语言信息编码到特征空间；2)特征推理：通过骨干网络(如Transformer)捕获特征间相关性；3)动作解码：将特征转换为机器人可执行动作；4)分层系统(可选)：上层负责任务理解，下层负责动作执行；5)训练流程：包括预训练(使用大规模数据)和后训练(针对特定任务微调)；6)评估流程：通过真实环境、仿真器或世界模型进行评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 这篇综述论文的关键创新点包括：1)提供全面的VLA模型综述框架，从具身智能系统的核心要素出发；2)将VLA发展历程划分为三阶段，提供清晰的时间线；3)将预训练方法分为四类，系统化不同训练策略；4)将后训练方法分为三类，借鉴大模型分类方法；5)提出模型评估的三维度视角。相比之前的工作，本文的不同之处在于：视角更全面，包含2024年后最新研究成果，提供更系统的分类框架，并进行更深入的挑战分析，为领域研究提供更全面的参考和方向指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过对具身操作中视觉-语言-动作模型的全面综述，系统梳理了VLA模型的发展历程、架构设计、训练方法、评估体系及未来挑战，为具身智能领域的研究提供了系统化的参考框架和方向指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence systems, which enhance agent capabilities throughcontinuous environment interactions, have garnered significant attention fromboth academia and industry. Vision-Language-Action models, inspired byadvancements in large foundation models, serve as universal robotic controlframeworks that substantially improve agent-environment interactioncapabilities in embodied intelligence systems. This expansion has broadenedapplication scenarios for embodied AI robots. This survey comprehensivelyreviews VLA models for embodied manipulation. Firstly, it chronicles thedevelopmental trajectory of VLA architectures. Subsequently, we conduct adetailed analysis of current research across 5 critical dimensions: VLA modelstructures, training datasets, pre-training methods, post-training methods, andmodel evaluation. Finally, we synthesize key challenges in VLA development andreal-world deployment, while outlining promising future research directions.</description>
      <author>example@mail.com (Haoran Li, Yuhui Chen, Wenbo Cui, Weiheng Liu, Kai Liu, Mingcai Zhou, Zhengtao Zhang, Dongbin Zhao)</author>
      <guid isPermaLink="false">2508.15201v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</title>
      <link>http://arxiv.org/abs/2508.15192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LLM4Sweat，一个专门针对多汗症的开源大型语言模型框架，通过三阶段流程解决罕见医疗条件数据稀缺问题，提供诊断、治疗建议和心理支持。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在医疗保健领域有潜力，但应用于罕见医疗条件时受限于微调数据的稀缺和不可靠。多汗症是一种影响2-3%人口的疾病，显著影响患者身体舒适度和心理社会健康。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对多汗症的支持系统，填补当前没有针对多汗症的诊断或护理定制LLM的空白。&lt;h4&gt;方法&lt;/h4&gt;LLM4Sweat采用三阶段流程：1)数据增强阶段，利用前沿LLM从精选开源数据生成医学合理的合成案例；2)微调阶段，在数据集上微调开源基础模型；3)推理和专家评估阶段，由临床和心理学专家评估系统表现，并将验证的响应迭代丰富数据集。&lt;h4&gt;主要发现&lt;/h4&gt;LLM4Sweat性能优于基线模型，成为首个针对多汗症的开源LLM框架。&lt;h4&gt;结论&lt;/h4&gt;该框架为其他具有类似数据和可靠性挑战的罕见疾病提供了一种可推广的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型语言模型在医疗保健领域显示出前景，但其在罕见医疗条件中的应用仍受到微调数据稀缺且不可靠的限制。多汗症是一种导致超出生理需求过度出汗的疾病，影响2-3%的人口，显著影响身体舒适度和心理社会健康。迄今为止，没有工作针对多汗症的诊断或护理定制LLM。为解决这一空白，我们提出了LLM4Sweat，一个开源的、领域特定的LLM框架，用于提供可靠且富有同理心的多汗症支持。系统遵循三阶段流程。在数据增强阶段，前沿LLM从精选开源数据生成医学上合理的合成案例，创建多样化且平衡的问答数据集。在微调阶段，开源基础模型在数据集上进行微调，提供诊断、个性化治疗建议和富有同理心的心理支持。在推理和专家评估阶段，临床和心理学专家评估准确性、适当性和同理心，验证的响应迭代丰富数据集。实验表明，LLM4Sweat优于基线模型，提供了首个针对多汗症的开源LLM框架，为其他具有类似数据和可靠性挑战的罕见疾病提供了一种可推广的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While large language models (LLMs) have shown promise in healthcare, theirapplication for rare medical conditions is still hindered by scarce andunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causingexcessive sweating beyond physiological needs, is one such rare disorder,affecting 2-3% of the population and significantly impacting both physicalcomfort and psychosocial well-being. To date, no work has tailored LLMs toadvance the diagnosis or care of hyperhidrosis. To address this gap, we presentLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy andempathetic hyperhidrosis support. The system follows a three-stage pipeline. Inthe data augmentation stage, a frontier LLM generates medically plausiblesynthetic vignettes from curated open-source data to create a diverse andbalanced question-answer dataset. In the fine-tuning stage, an open-sourcefoundation model is fine-tuned on the dataset to provide diagnosis,personalized treatment recommendations, and empathetic psychological support.In the inference and expert evaluation stage, clinical and psychologicalspecialists assess accuracy, appropriateness, and empathy, with validatedresponses iteratively enriching the dataset. Experiments show that LLM4Sweatoutperforms baselines and delivers the first open-source LLM framework forhyperhidrosis, offering a generalizable approach for other rare diseases withsimilar data and trustworthiness challenges.</description>
      <author>example@mail.com (Wenjie Lin, Jin Wei-Kocsis)</author>
      <guid isPermaLink="false">2508.15192v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Don't Think Twice! Over-Reasoning Impairs Confidence Calibration</title>
      <link>http://arxiv.org/abs/2508.15050v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICML 2025 Workshop on Reliable and Responsible  Foundation Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了大型语言模型作为问答工具时的校准问题，发现增加推理预算会降低校准性能，而搜索增强生成方法表现更优，表明信息获取而非推理深度可能是知识密集型任务校准的关键瓶颈。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型作为问答工具需要稳健的校准以避免过度自信。研究使用了ClimateX数据集(扩展到人类和行星健康领域)来评估推理能力和预算如何影响置信度评估准确性。&lt;h4&gt;目的&lt;/h4&gt;系统评估推理能力和预算如何影响大型语言模型对置信度的评估准确性，以改进知识密集型任务的置信度校准。&lt;h4&gt;方法&lt;/h4&gt;使用ClimateX数据集(扩展到人类和行星健康领域)，系统地评估推理能力和预算对置信度评估准确性的影响。比较了纯推理方法和搜索增强生成方法的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. '测试时扩展'范式受到挑战：尽管最近的推理LLM在评估专家置信度时达到48.7%的准确率，但增加推理预算会持续损害而非改善校准。2. 扩展推理会导致系统性过度自信，且随着思考预算的增加而恶化，适度的计算投资后收益递减甚至为负。3. 搜索增强生成方法显著优于纯推理方法，通过检索相关证据实现了89.3%的准确率。&lt;h4&gt;结论&lt;/h4&gt;对于知识密集型任务的置信度校准，信息获取能力可能比推理深度或推理预算更为关键。搜索增强生成方法在置信度评估方面表现优于纯推理方法。&lt;h4&gt;翻译&lt;/h4&gt;作为问答工具部署的大型语言模型需要稳健的校准以避免过度自信。我们系统性地评估推理能力和预算如何影响置信度评估的准确性，使用ClimateX数据集(Lacombe等人，2023)并将其扩展到人类和行星健康领域。我们的关键发现挑战了'测试时扩展'范式：虽然最近的推理LLM在评估专家置信度时实现了48.7%的准确率，但增加推理预算持续损害而非改善校准。扩展推理导致系统性过度自信，且随着思考预算的增加而恶化，在适度的计算投资后产生递减和负回报。相反，搜索增强生成显著优于纯推理，通过检索相关证据实现了89.3%的准确率。我们的结果表明，对于知识密集型任务的改进置信度校准，信息访问能力而非推理深度或推理预算可能是关键瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models deployed as question answering tools require robustcalibration to avoid overconfidence. We systematically evaluate how reasoningcapabilities and budget affect confidence assessment accuracy, using theClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetaryhealth. Our key finding challenges the "test-time scaling" paradigm: whilerecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,increasing reasoning budgets consistently impairs rather than improvescalibration. Extended reasoning leads to systematic overconfidence that worsenswith longer thinking budgets, producing diminishing and negative returns beyondmodest computational investments. Conversely, search-augmented generationdramatically outperforms pure reasoning, achieving 89.3% accuracy by retrievingrelevant evidence. Our results suggest that information access, rather thanreasoning depth or inference budget, may be the critical bottleneck forimproved confidence calibration of knowledge-intensive tasks.</description>
      <author>example@mail.com (Romain Lacombe, Kerrie Wu, Eddie Dilworth)</author>
      <guid isPermaLink="false">2508.15050v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents</title>
      <link>http://arxiv.org/abs/2508.16414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint version of the paper accepted to IEEE-EMBS International  Conference on Biomedical and Health Informatics (BHI'25), 2025. This is the  author's original manuscript (preprint). The final published version will  appear in IEEE Xplore&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了NeuroKoop，一种基于图神经网络的新框架，通过神经Koopman算子驱动的潜在空间融合整合结构脑网络和功能脑网络，用于识别产前药物暴露对青少年大脑发育的影响。&lt;h4&gt;背景&lt;/h4&gt;了解产前暴露于大麻等精神活性物质如何影响青少年大脑组织是一个关键挑战，多模态神经影像数据的复杂性和传统分析方法的局限性使这一问题更加复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法来克服现有方法的局限，更好地捕捉结构连接组和功能连接组中的互补特征，提高对产前药物暴露影响的生物学见解和预测性能。&lt;h4&gt;方法&lt;/h4&gt;引入NeuroKoop，一种基于图神经网络的框架，利用神经Koopman算子驱动的潜在空间融合来整合结构脑网络和功能脑网络。通过Koopman理论，NeuroKoop统一了基于形态学源分析(SBM)和功能网络连接(FNC)的脑图节点嵌入，实现增强的表示学习和更稳健的产前药物暴露状态分类。&lt;h4&gt;主要发现&lt;/h4&gt;在ABCD数据集的大型青少年队列中应用，NeuroKoop优于相关基线方法，并揭示了显著的结构-功能连接，推进了对产前药物暴露神经发育影响的理解。&lt;h4&gt;结论&lt;/h4&gt;NeuroKoop框架通过整合结构和功能脑网络信息，能够更准确地识别产前药物暴露对青少年大脑发育的影响，为理解神经发育影响提供了新见解。&lt;h4&gt;翻译&lt;/h4&gt;了解产前暴露于大麻等精神活性物质如何塑造青少年大脑组织仍然是一个关键挑战，这受到多模态神经影像数据复杂性和传统分析方法局限性的影响。现有方法往往无法完全捕捉结构连接组和功能连接组中嵌入的互补特征，限制了生物学见解和预测性能。为此，我们引入了NeuroKoop，这是一种基于图神经网络的新框架，利用神经Koopman算子驱动的潜在空间融合来整合结构脑网络和功能脑网络。通过利用Koopman理论，NeuroKoop统一了基于形态学源分析(SBM)和基于功能网络连接(FNC)的脑图节点嵌入，从而增强了表示学习，并能更稳健地对产前药物暴露(PDE)状态进行分类。将其应用于ABCD数据集的大型青少年队列，NeuroKoop优于相关基线方法，并揭示了显著的结构-功能连接，推进了我们对PDE神经发育影响的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how prenatal exposure to psychoactive substances such ascannabis shapes adolescent brain organization remains a critical challenge,complicated by the complexity of multimodal neuroimaging data and thelimitations of conventional analytic methods. Existing approaches often fail tofully capture the complementary features embedded within structural andfunctional connectomes, constraining both biological insight and predictiveperformance. To address this, we introduced NeuroKoop, a novel graph neuralnetwork-based framework that integrates structural and functional brainnetworks utilizing neural Koopman operator-driven latent space fusion. Byleveraging Koopman theory, NeuroKoop unifies node embeddings derived fromsource-based morphometry (SBM) and functional network connectivity (FNC) basedbrain graphs, resulting in enhanced representation learning and more robustclassification of prenatal drug exposure (PDE) status. Applied to a largeadolescent cohort from the ABCD dataset, NeuroKoop outperformed relevantbaselines and revealed salient structural-functional connections, advancing ourunderstanding of the neurodevelopmental impact of PDE.</description>
      <author>example@mail.com (Badhan Mazumder, Aline Kotoski, Vince D. Calhoun, Dong Hye Ye)</author>
      <guid isPermaLink="false">2508.16414v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation</title>
      <link>http://arxiv.org/abs/2508.16269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种深度学习模型，用于学习练习的稀疏二进制表示作为辅助知识概念，这些概念能够捕捉超越人工标注的概念结构，并与现有的知识追踪模型兼容，从而提高学生建模和练习推荐的效果。&lt;h4&gt;背景&lt;/h4&gt;个性化推荐是智能辅导系统的关键特征，通常依赖于准确的学生知识模型。知识追踪模型通过估计学生基于历史互动的掌握程度来实现这一点。然而，许多现有模型依赖的人工标注知识概念可能不完整、容易出错或过于笼统。&lt;h4&gt;目的&lt;/h4&gt;提出一种深度学习模型，学习练习的稀疏二进制表示，其中每个比特表示潜在概念的存在或不存在，这些表示被称为辅助知识概念，能够捕捉超越人工标注的概念结构。&lt;h4&gt;方法&lt;/h4&gt;开发一种深度学习模型来学习练习的稀疏二进制表示作为辅助知识概念，这些表示与经典模型(如BKT)和现代深度学习知识追踪架构兼容，并将这些辅助知识概念整合到学生建模和练习推荐系统中。&lt;h4&gt;主要发现&lt;/h4&gt;将辅助知识概念纳入系统可以改善学生建模和自适应练习推荐；对于学生建模，将辅助知识概念与BKT等经典模型结合可以提高预测性能；对于推荐，使用辅助知识概念可以强化基于强化学习的策略和基于规划的方法，在模拟学生环境中带来学生学习成果的显著提升。&lt;h4&gt;结论&lt;/h4&gt;辅助知识概念能够有效捕捉超越人工标注的概念结构，提高学生知识追踪模型的预测性能和练习推荐效果，从而改善学生的学习成果。&lt;h4&gt;翻译&lt;/h4&gt;个性化推荐是智能辅导系统的关键特征，通常依赖于准确的学生知识模型。知识追踪模型通过估计学生基于历史互动的掌握程度来实现这一点。许多知识追踪模型依赖于人工标注的知识概念，这些概念标记了每个练习所需的一个或多个技能或概念。然而，这些知识概念可能不完整、容易出错或过于笼统。在本文中，我们提出了一种深度学习模型，用于学习练习的稀疏二进制表示，其中每个比特表示潜在概念的存在或不存在。我们将这些表示称为辅助知识概念。这些表示捕捉了超越人工标注的概念结构，并且与经典模型(如BKT)和现代深度学习知识追踪架构兼容。我们证明，纳入辅助知识概念可以改善学生建模和自适应练习推荐。对于学生建模，我们展示将辅助知识概念与BKT等经典模型结合可以提高预测性能。对于推荐，我们展示使用辅助知识概念可以增强基于强化学习的策略和简单的基于规划的方法(期望最大值)，在模拟学生环境中带来学生学习成果的显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized recommendation is a key feature of intelligent tutoring systems,typically relying on accurate models of student knowledge. Knowledge Tracing(KT) models enable this by estimating a student's mastery based on theirhistorical interactions. Many KT models rely on human-annotated knowledgeconcepts (KCs), which tag each exercise with one or more skills or conceptsbelieved to be necessary for solving it. However, these KCs can be incomplete,error-prone, or overly general.  In this paper, we propose a deep learning model that learns sparse binaryrepresentations of exercises, where each bit indicates the presence or absenceof a latent concept. We refer to these representations as auxiliary KCs. Theserepresentations capture conceptual structure beyond human-defined annotationsand are compatible with both classical models (e.g., BKT) and modern deeplearning KT architectures.  We demonstrate that incorporating auxiliary KCs improves both studentmodeling and adaptive exercise recommendation. For student modeling, we showthat augmenting classical models like BKT with auxiliary KCs leads to improvedpredictive performance. For recommendation, we show that using auxiliary KCsenhances both reinforcement learning-based policies and a simple planning-basedmethod (expectimax), resulting in measurable gains in student learning outcomeswithin a simulated student environment.</description>
      <author>example@mail.com (Yahya Badran, Christine Preisach)</author>
      <guid isPermaLink="false">2508.16269v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning with Adaptive Superpixel Coding</title>
      <link>http://arxiv.org/abs/2508.15959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Transformer的自监督模型Adaptive Superpixel Coding (ASC)，通过自适应超像素层克服了传统Vision Transformer的固定大小和非自适应块划分的局限性，在标准图像下游任务中表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;深度学习视觉模型通常针对特定模态进行定制，并依赖于领域特定的假设，如几乎所有现有视觉模型使用的网格结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer的自监督模型Adaptive Superpixel Coding (ASC)，以克服传统视觉模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;ASC采用自适应超像素层，能够动态调整到底层图像内容，而非传统Vision Transformer使用的固定大小和非自适应的块划分方式。&lt;h4&gt;主要发现&lt;/h4&gt;分析了该方法使其有效的关键特性，并在标准图像下游任务基准测试中发现该方法优于广泛使用的替代方法。&lt;h4&gt;结论&lt;/h4&gt;通过自适应超像素层，ASC改进了传统视觉Transformer的局限性，在标准图像下游任务中表现优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;深度学习视觉模型通常针对特定模态进行定制，并且经常依赖于领域特定的假设，例如几乎所有现有视觉模型使用的网格结构。在这项工作中，我们提出了一种基于Transformer的自监督模型，我们称之为自适应超像素编码（ASC）。我们模型的关键见解是克服传统Vision Transformer的局限性，后者依赖于固定大小和非自适应的块划分。相反，ASC采用自适应超像素层，能够动态调整到底层图像内容。我们分析了该方法使其有效的关键特性，并发现我们的方法在标准图像下游任务基准测试中优于广泛使用的替代方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning vision models are typically tailored for specific modalitiesand often rely on domain-specific assumptions, such as the grid structures usedby nearly all existing vision models. In this work, we propose aself-supervised model based on Transformers, which we call Adaptive SuperpixelCoding (ASC). The key insight of our model is to overcome the limitations oftraditional Vision Transformers, which depend on fixed-size and non-adaptivepatch partitioning. Instead, ASC employs adaptive superpixel layers thatdynamically adjust to the underlying image content. We analyze key propertiesof the approach that make it effective, and find that our method outperformswidely-used alternatives on standard image downstream task benchmarks.</description>
      <author>example@mail.com (Mahmoud Khalil, Ahmad Khalil, Alioune Ngom)</author>
      <guid isPermaLink="false">2508.15959v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem</title>
      <link>http://arxiv.org/abs/2508.15949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted for publication in the European Journal  of Operational Research. Supplementary material will be available on the  journal website or upon request&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种将元启发式算法与图表示学习相结合的GL-GRASP方法，用于解决约束增量图绘制问题，在计算实验中表现出优于现有技术的性能，特别是在处理更密集实例时展现了良好的可扩展性。&lt;h4&gt;背景&lt;/h4&gt;近年来，机器学习技术与元启发式算法的结合引起了广泛关注。然而，监督学习和强化学习等方法在某些情况下被认为过于耗时，无法与手工设计的启发式方法竞争。针对约束增量图绘制问题，现有文献有限，而贪婪随机搜索程序启发式方法已显示出有希望的结果。&lt;h4&gt;目的&lt;/h4&gt;探索将图表示学习这种成本较低的学习策略整合到GRASP算法构建阶段的可行性，以提高解决约束增量图绘制问题的性能。&lt;h4&gt;方法&lt;/h4&gt;提出图学习GRASP（GL-GRASP）混合方法，将元启发式算法与图表示学习相结合。应用于约束增量图绘制问题，分析不同的节点嵌入技术，特别是基于深度学习的策略，并采用原始积分指标根据解决方案所需时间评估质量。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基于深度学习的节点嵌入技术表现突出；2. 最佳GL-GRASP启发式算法在问题表现上优于文献中最先进的GRASP启发式算法；3. 在固定时间限制下对更密集实例进行的可扩展性测试证实了GL-GRASP启发式算法的稳健性。&lt;h4&gt;结论&lt;/h4&gt;将图表示学习与元启发式算法相结合的方法在解决约束增量图绘制问题时表现出色，不仅优于现有技术，而且在处理更密集实例时具有良好的可扩展性，证明了该混合方法的有效性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，将机器学习技术与元启发式算法相结合引起了广泛关注。许多尝试使用监督学习或强化学习来支持启发式方法的决策过程。然而，在某些情况下，这些技术被认为过于耗时，无法与手工设计的启发式方法竞争。本文提出了一种将元启发式算法与一种成本较低的学习策略相结合的混合方法，用于提取图的潜在结构，称为图表示学习（GRL）。为此，我们研究了约束增量图绘制问题（C-IGDP），这是一个层次化图可视化问题。关于此问题的方法文献有限，而贪婪随机搜索程序（GRASP）启发式方法已显示出有希望的结果。基于此，本文研究了将GRL整合到GRASP构建阶段的好处，我们称之为图学习GRASP（GL-GRASP）。在计算实验中，我们首先分析了考虑不同节点嵌入技术所取得的结果，其中基于深度学习的策略表现突出。评估采用了原始积分指标，根据所需时间评估解决方案的质量。根据该指标，最佳GL-GRASP启发式算法在问题表现上优于文献中最先进的GRASP启发式算法。在固定时间限制下对新生成的更密集实例进行的可扩展性测试进一步证实了GL-GRASP启发式算法的稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hybridizing machine learning techniques with metaheuristics has attractedsignificant attention in recent years. Many attempts employ supervised orreinforcement learning to support the decision-making of heuristic methods.However, in some cases, these techniques are deemed too time-consuming and notcompetitive with hand-crafted heuristics. This paper proposes a hybridizationbetween metaheuristics and a less expensive learning strategy to extract thelatent structure of graphs, known as Graph Representation Learning (GRL). Forsuch, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), ahierarchical graph visualization problem. There is limited literature onmethods for this problem, for which Greedy Randomized Search Procedures (GRASP)heuristics have shown promising results. In line with this, this paperinvestigates the gains of incorporating GRL into the construction phase ofGRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computationalexperiments, we first analyze the results achieved considering different nodeembedding techniques, where deep learning-based strategies stood out. Theevaluation considered the primal integral measure that assesses the quality ofthe solutions according to the required time for such. According to thismeasure, the best GL-GRASP heuristics demonstrated superior performance thanstate-of-the-art literature GRASP heuristics for the problem. A scalabilitytest on newly generated denser instances under a fixed time limit furtherconfirmed the robustness of the GL-GRASP heuristics.</description>
      <author>example@mail.com (Bruna C. B. Charytitsch, María C. V. Nascimento)</author>
      <guid isPermaLink="false">2508.15949v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making</title>
      <link>http://arxiv.org/abs/2508.15926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025 (Main Conference)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种面向过程的评估框架，通过渐进式干预检验大型语言模型在不同级别外部指导下的适应能力，发现LLM在模拟人类决策行为方面存在差距。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地被用于社会科学模拟，但对其模拟人类决策变异性和适应能力的研究较少。&lt;h4&gt;目的&lt;/h4&gt;提出一种面向过程的评估框架，检验LLM代理在不同级别外部指导和人类衍生噪声下的适应能力。&lt;h4&gt;方法&lt;/h4&gt;在二级拍卖中的非理性和报童问题中的决策偏差两个经典经济学任务上验证该框架。&lt;h4&gt;主要发现&lt;/h4&gt;1) LLM默认收敛于稳定且保守的策略，与人类行为不同；2) 风险框架指令可预测影响LLM行为但无法复制类人多样性；3) 通过上下文学习纳入人类数据可缩小差距但无法达到人类受试者的战略变异性。&lt;h4&gt;结论&lt;/h4&gt;存在行为保真度中的持续校准差距，未来LLM评估应考虑更多过程层面真实性，该评估方法为LLM在社会科学研究合成数据应用提供指导。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型越来越多地被用于社会科学模拟。虽然它们在推理和优化任务上的表现已被广泛评估，但对其模拟人类决策的变异性和适应能力的研究较少。我们提出了一种面向过程的评估框架，通过渐进式干预(内在性、指导和模仿)来检验LLM代理在不同级别的外部指导和人类衍生噪声下的适应能力。我们在两个经典经济学任务上验证了该框架：二级拍卖中的非理性和报童问题中的决策偏差，显示出LLM与人类之间的行为差距。我们发现，默认情况下，LLM收敛于稳定且保守的策略，这与观察到的人类行为不同。风险框架的指令可预测地影响LLM行为，但无法复制类人的多样性。通过上下文学习纳入人类数据可以缩小差距，但无法达到人类受试者的战略变异性。这些结果突显了行为保真度中持续的校准差距，并建议未来的LLM评估应考虑更多过程层面的真实性。我们提出了一种面向过程的方法来评估LLM在动态决策任务中的表现，为其在社会科学研究的合成数据应用提供指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are increasingly used in social sciencesimulations. While their performance on reasoning and optimization tasks hasbeen extensively evaluated, less attention has been paid to their ability tosimulate human decision-making's variability and adaptability. We propose aprocess-oriented evaluation framework with progressive interventions(Intrinsicality, Instruction, and Imitation) to examine how LLM agents adaptunder different levels of external guidance and human-derived noise. Wevalidate the framework on two classic economics tasks, irrationality in thesecond-price auction and decision bias in the newsvendor problem, showingbehavioral gaps between LLMs and humans.  We find that LLMs, by default, converge on stable and conservative strategiesthat diverge from observed human behaviors. Risk-framed instructions impact LLMbehavior predictably but do not replicate human-like diversity. Incorporatinghuman data through in-context learning narrows the gap but fails to reach humansubjects' strategic variability. These results highlight a persistent alignmentgap in behavioral fidelity and suggest that future LLM evaluations shouldconsider more process-level realism. We present a process-oriented approach forassessing LLMs in dynamic decision-making tasks, offering guidance for theirapplication in synthetic data for social science research.</description>
      <author>example@mail.com (Yuanjun Feng, Vivek Choudhary, Yash Raj Shrestha)</author>
      <guid isPermaLink="false">2508.15926v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty Quantification and Propagation for ACORN, a geometric deep learning tracking pipeline for HEP experiments</title>
      <link>http://arxiv.org/abs/2508.16518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 53 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种用于多步管道的不确定性量化方法，并将其应用于ACORN粒子跟踪管道，使用TrackML数据集进行实验。&lt;h4&gt;背景&lt;/h4&gt;粒子跟踪管道中的不确定性量化对于确保结果可靠性至关重要，特别是在高能物理实验中。&lt;h4&gt;目的&lt;/h4&gt;开发并应用不确定性量化过程来评估ACORN粒子跟踪管道中的数据不确定性和模型不确定性，研究不确定性的传播规律，以及分析训练数据集大小、输入数据几何和物理属性对不确定性的影响。&lt;h4&gt;方法&lt;/h4&gt;使用Monte Carlo Dropout方法测量管道步骤的数据和模型不确定性，研究不确定性沿管道的传播规律，并分析训练数据集大小、输入数据的几何和物理属性对不确定性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;随着训练数据集的增长，整体不确定性逐渐由认知不确定性主导，表明有足够的数据将ACORN模型训练到其全部潜力。ACORN管道在轨道重建方面具有高置信度，并且不会遭受GNN模型的校准错误。&lt;h4&gt;结论&lt;/h4&gt;ACORN管道在粒子跟踪方面表现出色，其不确定性量化方法有效，能够准确评估和传播不确定性。&lt;h4&gt;翻译&lt;/h4&gt;我们为多步管道开发了一种不确定性量化过程，并将其应用于ACORN粒子跟踪管道。我们的所有实验都是使用TrackML开放数据集进行的。使用Monte Carlo Dropout方法，我们测量了管道步骤的数据和模型不确定性，研究了它们如何沿管道传播，以及它们如何受训练数据集大小、输入数据的几何和物理属性的影响。我们将证明，对于我们的案例研究，随着训练数据集的增长，整体不确定性逐渐由认知不确定性主导，表明我们有足够的数据将我们选择的ACORN模型训练到其全部潜力。我们表明ACORN管道在轨道重建方面具有高置信度，并且不会遭受GNN模型的校准错误。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We have developed an Uncertainty Quantification process for multisteppipelines and applied it to the ACORN particle tracking pipeline. All ourexperiments are made using the TrackML open dataset. Using the Monte CarloDropout method, we measure the data and model uncertainties of the pipelinesteps, study how they propagate down the pipeline, and how they are impacted bythe training dataset's size, the input data's geometry and physical properties.We will show that for our case study, as the training dataset grows, theoverall uncertainty becomes dominated by aleatoric uncertainty, indicating thatwe had sufficient data to train the ACORN model we chose to its full potential.We show that the ACORN pipeline yields high confidence in the trackreconstruction and does not suffer from the miscalibration of the GNN model.</description>
      <author>example@mail.com (Lukas Péron, Paolo Calafiura, Xiangyang Ju, Jay Chan)</author>
      <guid isPermaLink="false">2508.16518v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>A Node-Aware Dynamic Quantization Approach for Graph Collaborative Filtering</title>
      <link>http://arxiv.org/abs/2508.16516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GNAQ的图节点感知动态量化方法，用于解决图神经网络在资源受限设备上部署的挑战，在保持推荐质量的同时显著提高了效率和速度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在协同过滤推荐系统中表现出色，但由于高嵌入参数需求和计算成本，在资源受限的边缘设备上部署面临挑战。直接使用常见量化方法处理节点嵌入可能会忽略其基于图的结构，导致消息传递过程中的错误累积。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的节点感知动态量化训练方法(GNAQ)，提高GNN在Top-K推荐中效率和准确性的平衡，使其能够在资源受限设备上有效部署。&lt;h4&gt;方法&lt;/h4&gt;GNAQ引入节点感知动态量化策略，根据图交互关系调整量化尺度；基于节点级特征分布初始化量化区间，并通过GNN层中的消息传递动态细化；使用图关系感知梯度估计替代传统直通估计器，确保训练期间更准确的梯度传播。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实数据集上，GNAQ在2位量化下Recall@10平均提高27.8%，NDCG@10平均提高17.6%；能够保持全精度模型的性能，同时将模型大小减少8到12倍；与量化基线方法相比，训练时间快两倍。&lt;h4&gt;结论&lt;/h4&gt;GNAQ通过利用图结构信息，有效解决了GNN在资源受限设备上部署的挑战，在保持推荐质量的同时显著提高了模型效率和训练速度。&lt;h4&gt;翻译&lt;/h4&gt;在协同过滤推荐系统领域，图神经网络已展现出卓越的性能，但由于其高嵌入参数需求和计算成本，在资源受限的边缘设备上部署面临重大挑战。直接在节点嵌入上使用常见量化方法可能会忽略其基于图的结构，导致消息传递过程中的错误累积并降低量化嵌入的质量。为此，我们提出了基于图的节点感知动态量化训练方法(GNAQ)，这是一种新颖的量化方法，利用图结构信息增强GNN在Top-K推荐中效率与准确性的平衡。GNAQ引入了节点感知动态量化策略，通过结合图交互关系，使量化尺度适应各个节点嵌入。具体而言，它基于节点级特征分布初始化量化区间，并通过GNN层中的消息传递动态细化这些区间。这种方法减轻了固定量化尺度导致的信息损失，并捕获了用户--item交互图中的层次语义特征。此外，GNAQ采用图关系感知梯度估计替代传统的直通估计器，确保训练期间更准确的梯度传播。在四个真实数据集上的广泛实验表明，GNAQ优于最先进的量化方法，包括BiGeaR和N2UQ，在2位量化下，Recall@10平均提高27.8%，NDCG@10平均提高17.6%。特别是，GNAQ能够在保持全精度模型性能的同时，将模型大小减少8到12倍；此外，与量化基线方法相比，训练时间快一倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the realm of collaborative filtering recommendation systems, Graph NeuralNetworks (GNNs) have demonstrated remarkable performance but face significantchallenges in deployment on resource-constrained edge devices due to their highembedding parameter requirements and computational costs. Using commonquantization method directly on node embeddings may overlooks their graph basedstructure, causing error accumulation during message passing and degrading thequality of quantized embeddings.To address this, we propose Graph basedNode-Aware Dynamic Quantization training for collaborative filtering (GNAQ), anovel quantization approach that leverages graph structural information toenhance the balance between efficiency and accuracy of GNNs for Top-Krecommendation. GNAQ introduces a node-aware dynamic quantization strategy thatadapts quantization scales to individual node embeddings by incorporating graphinteraction relationships. Specifically, it initializes quantization intervalsbased on node-wise feature distributions and dynamically refines them throughmessage passing in GNN layers. This approach mitigates information loss causedby fixed quantization scales and captures hierarchical semantic features inuser-item interaction graphs. Additionally, GNAQ employs graph relation-awaregradient estimation to replace traditional straight-through estimators,ensuring more accurate gradient propagation during training. Extensiveexperiments on four real-world datasets demonstrate that GNAQ outperformsstate-of-the-art quantization methods, including BiGeaR and N2UQ, by achievingaverage improvement in 27.8\% Recall@10 and 17.6\% NDCG@10 under 2-bitquantization. In particular, GNAQ is capable of maintaining the performance offull-precision models while reducing their model sizes by 8 to 12 times; inaddition, the training time is twice as fast compared to quantization baselinemethods.</description>
      <author>example@mail.com (Lin Li, Chunyang Li, Yu Yin, Xiaohui Tao, Jianwei Zhang)</author>
      <guid isPermaLink="false">2508.16516v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow</title>
      <link>http://arxiv.org/abs/2508.16403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级、数据高效且拓扑感知的图神经网络模型，用于预测多种有源射频电路的关键性能指标，实验显示高预测精度且显著减少了训练数据需求。&lt;h4&gt;背景&lt;/h4&gt;准确预测有源射频电路性能对现代无线系统至关重要，但由于高度非线性、布局敏感行为以及传统仿真工具的高计算成本，这仍然具有挑战性。现有的机器学习代理模型通常需要大型数据集才能在不同拓扑结构间泛化，或准确建模偏斜和多模态性能指标。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级、数据高效且拓扑感知的图神经网络模型，用于预测多种有源射频电路拓扑结构的关键性能指标，如低噪声放大器、混频器、压控振荡器和功率放大器。&lt;h4&gt;方法&lt;/h4&gt;在器件终端级别对电路进行建模，以捕获晶体管级对称性并保留细粒度连接细节，实现可扩展的消息传递，同时减少数据需求。集成掩码自回归流输出头，以改进对复杂目标分布的建模稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示高预测精度，对称平均绝对百分比误差和平均相对误差分别平均为2.40%和2.91%。与先前工作相比，使用2.24倍更少的训练样本，平均相对误差提高了3.14倍。&lt;h4&gt;结论&lt;/h4&gt;由于电路到图的引脚级转换以及对射频指标复杂密度建模的机器学习架构的稳健性，该方法证明了其在快速准确的射频电路设计自动化方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确预测有源射频电路的性能对现代无线系统至关重要，但由于高度非线性、布局敏感的行为以及传统仿真工具的高计算成本，这仍然具有挑战性。现有的机器学习代理模型通常需要大型数据集才能在各种拓扑结构间泛化，或准确建模偏斜和多模态性能指标。在这项工作中，提出了一种轻量级、数据高效且拓扑感知的图神经网络模型，用于预测多种有源射频电路拓扑结构的关键性能指标，如低噪声放大器、混频器、压控振荡器和功率放大器。为了捕获晶体管级对称性并保留细粒度连接细节，电路在器件终端级别进行建模，实现了可扩展的消息传递，同时减少了数据需求。集成了掩码自回归流输出头，以提高对复杂目标分布建模的稳健性。数据集上的实验展示了高预测精度，对称平均绝对百分比误差和平均相对误差分别平均为2.40%和2.91%。由于电路到图的引脚级转换以及机器学习架构对射频指标复杂密度建模的稳健性，与先前工作相比，该方法在使用2.24倍更少训练样本的情况下，将平均相对误差提高了3.14倍，证明了该方法在快速准确的射频电路设计自动化方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the performance of active radio frequency (RF) circuitsis essential for modern wireless systems but remains challenging due to highlynonlinear, layout-sensitive behavior and the high computational cost oftraditional simulation tools. Existing machine learning (ML) surrogates oftenrequire large datasets to generalize across various topologies or to accuratelymodel skewed and multi-modal performance metrics. In this work, a lightweight,data-efficient, and topology-aware graph neural network (GNN) model is proposedfor predicting key performance metrics of multiple topologies of active RFcircuits such as low noise amplifiers (LNAs), mixers, voltage-controlledoscillators (VCOs), and PAs. To capture transistor-level symmetry and preservefine-grained connectivity details, circuits are modeled at the device-terminallevel, enabling scalable message passing while reducing data requirements.Masked autoregressive flow (MAF) output heads are incorporated to improverobustness in modeling complex target distributions. Experiments on datasetsdemonstrate high prediction accuracy, with symmetric mean absolute percentageerror (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,respectively. Owing to the pin-level conversion of circuit to graph and MLarchitecture robust to modeling complex densities of RF metrics, the MRE isimproved by 3.14x while using 2.24x fewer training samples compared to priorwork, demonstrating the method's effectiveness for rapid and accurate RFcircuit design automation.</description>
      <author>example@mail.com (Anahita Asadi, Leonid Popryho, Inna Partin-Vaisband)</author>
      <guid isPermaLink="false">2508.16403v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization</title>
      <link>http://arxiv.org/abs/2508.16200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, 4 tables, 26 references, accepted at ACM  NanoCom'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索使用Set Transformer架构改进Flow-guided Localization技术，通过将纳米设备循环时间报告视为无序集合实现排列不变性，并集成深度生成模型进行合成数据生成以提高鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Flow-guided Localization (FGL)可识别人体内含有诊断意义事件的空间区域，通过利用在血液循环中被动移动的纳米设备实现。现有FGL解决方案依赖固定拓扑图模型或手工特征，限制了对解剖变异性的适应性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;探索使用Set Transformer架构解决现有FGL解决方案的局限性，提高在数据稀缺和类别不平衡情况下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将纳米设备的循环时间报告视为无序集合，实现排列不变性和可变长度输入处理；集成CGAN、WGAN、WGAN-GP和CVAE等深度生成模型进行合成数据生成，这些模型训练后用于增强训练数据。&lt;h4&gt;主要发现&lt;/h4&gt;Set Transformer与图神经网络基线相比实现了相当分类精度，同时在解剖变异性的泛化方面提供了设计上的改进。&lt;h4&gt;结论&lt;/h4&gt;排列不变性模型和合成增强在鲁棒和可扩展的纳米级定位方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;Flow-guided Localization (FGL)使人们能够识别人体内含有诊断意义事件的空间区域。FGL通过利用在血液循环中被动移动的、能量受限的纳米设备来实现这一功能。现有的FGL解决方案依赖于具有固定拓扑结构的图模型或手工设计的特征，这限制了它们对解剖变异性的适应性并阻碍了可扩展性。在这项工作中，我们探索使用Set Transformer架构来解决这些限制。我们的公式将纳米设备的循环时间报告视为无序集合，实现了排列不变性、可变长度输入处理，而不依赖于空间先验。为了提高在数据稀缺和类别不平衡情况下的鲁棒性，我们通过深度生成模型(包括CGAN、WGAN、WGAN-GP和CVAE)集成了合成数据生成。这些模型被训练以复制基于血管区域标签条件的真实循环时间分布，并用于增强训练数据。我们的结果表明，Set Transformer与图神经网络(GNN)基线相比实现了相当分类精度，同时同时提供了设计上改进的解剖变异性泛化能力。这些发现突显了排列不变性模型和合成增强在鲁棒和可扩展纳米级定位方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flow-guided Localization (FGL) enables the identification of spatial regionswithin the human body that contain an event of diagnostic interest. FGL doesthat by leveraging the passive movement of energy-constrained nanodevicescirculating through the bloodstream. Existing FGL solutions rely on graphmodels with fixed topologies or handcrafted features, which limit theiradaptability to anatomical variability and hinder scalability. In this work, weexplore the use of Set Transformer architectures to address these limitations.Our formulation treats nanodevices' circulation time reports as unordered sets,enabling permutation-invariant, variable-length input processing withoutrelying on spatial priors. To improve robustness under data scarcity and classimbalance, we integrate synthetic data generation via deep generative models,including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicaterealistic circulation time distributions conditioned on vascular region labels,and are used to augment the training data. Our results show that the SetTransformer achieves comparable classification accuracy compared to GraphNeural Networks (GNN) baselines, while simultaneously providing by-designimproved generalization to anatomical variability. The findings highlight thepotential of permutation-invariant models and synthetic augmentation for robustand scalable nanoscale localization.</description>
      <author>example@mail.com (Mika Leo Hube, Filip Lemic, Ethungshan Shitiri, Gerard Calvo Bartra, Sergi Abadal, Xavier Costa Pérez)</author>
      <guid isPermaLink="false">2508.16200v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Joint Cache Placement and Routing in Satellite-Terrestrial Edge Computing Network: A GNN-Enabled DRL Approach</title>
      <link>http://arxiv.org/abs/2508.16184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了卫星地面边缘计算网络中的联合内容缓存和路由问题，提出了一种基于图神经网络和深度强化学习的框架来提高地理分布用户的服务质量。&lt;h4&gt;背景&lt;/h4&gt;低地球轨道卫星拓扑具有动态性，同时用户内容需求呈现异构性，这给卫星地面边缘计算网络中的服务优化带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提高地理分布用户在卫星地面边缘计算网络中的缓存服务质量，优化内容分发效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于学习的框架，将图神经网络与深度强化学习相结合；将卫星网络表示为动态图，通过GNN捕获空间和拓扑依赖性；将问题建模为马尔可夫决策过程，并应用软演员-评论家算法优化缓存策略。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法显著提高了内容交付成功率，同时减少了通信流量成本。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络和深度强化学习的联合内容缓存与路由框架能够有效应对动态卫星拓扑和异构内容需求带来的挑战，优化卫星地面边缘计算网络的服务性能。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们研究了卫星地面边缘计算网络中的联合内容缓存和路由问题，以提高地理分布用户的缓存服务质量。为应对低地球轨道卫星动态拓扑和异构内容需求带来的挑战，我们提出了一种整合图神经网络与深度强化学习的基于学习的框架。卫星网络被表示为动态图，其中图神经网络嵌入到深度强化学习代理中，以捕获空间和拓扑依赖性并支持感知路由的决策制定。通过将问题表述为马尔可夫决策过程并应用软演员-评论家算法来优化缓存策略。仿真结果表明，我们的方法显著提高了交付成功率并减少了通信流量成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this letter, we investigate the problem of joint content caching androuting in satellite-terrestrial edge computing networks (STECNs) to improvecaching service for geographically distributed users. To handle the challengesarising from dynamic low Earth orbit (LEO) satellite topologies andheterogeneous content demands, we propose a learning-based framework thatintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).The satellite network is represented as a dynamic graph, where GNNs areembedded within the DRL agent to capture spatial and topological dependenciesand support routing-aware decision-making. The caching strategy is optimized byformulating the problem as a Markov decision process (MDP) and applying softactor-critic (SAC) algorithm. Simulation results demonstrate that our approachsignificantly improves the delivery success rate and reduces communicationtraffic cost.</description>
      <author>example@mail.com (Yuhao Zheng, Ting You, Kejia Peng, Chang Liu)</author>
      <guid isPermaLink="false">2508.16184v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>FIRE-GNN: Force-informed, Relaxed Equivariance Graph Neural Network for Rapid and Accurate Prediction of Surface Properties</title>
      <link>http://arxiv.org/abs/2508.16012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种名为FIRE-GNN的图神经网络模型，用于准确预测表面的功函数和断裂能，解决了传统第一性原理计算计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;表面的功函数和断裂能是决定材料在电子发射应用、半导体器件和异质催化中可行性的关键性质。第一性原理计算虽然准确，但计算成本高，加上表面的巨大搜索空间，使得使用密度泛函理论进行全面筛选不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确且快速预测功函数和断裂能的方法，以便在广阔的化学空间中进行预测，并促进具有调整后表面性质的材料的发现。&lt;h4&gt;方法&lt;/h4&gt;引入FIRE-GNN（Force-Informed, Relaxed Equivariance Graph Neural Network），结合表面法线对称性破缺和机器学习原子间势推导的力信息。对最近的不变和等变架构进行了基准测试，分析了对称性破缺的影响，并评估了分布外泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;FIRE-GNN在功函数预测方面将平均绝对误差降低了两倍（降至0.065 eV），超过了以前的最先进水平。FIRE-GNN在功函数预测方面持续优于竞争模型。&lt;h4&gt;结论&lt;/h4&gt;该模型能够在广阔的化学空间中准确快速地预测功函数和断裂能，有助于发现具有调整后表面性质的材料。&lt;h4&gt;翻译&lt;/h4&gt;表面的功函数和断裂能是决定材料在电子发射应用、半导体器件和异质催化中可行性的关键性质。虽然第一性原理计算在预测这些性质方面是准确的，但其计算成本高，加上表面的巨大搜索空间，使得使用密度泛函理论进行全面筛选的方法不可行。在此，我们引入了FIRE-GNN（Force-Informed, Relaxed Equivariance Graph Neural Network），该方法结合了表面法线对称性破缺和机器学习原子间势(MLIP)推导的力信息，在功函数预测方面将平均绝对误差降低了两倍（降至0.065 eV），超过了以前的最先进水平。我们还对最近的不变和等变架构进行了基准测试，分析了对称性破缺的影响，并评估了分布外泛化能力，证明FIRE-GNN在功函数预测方面持续优于竞争模型。该模型能够在广阔的化学空间中准确快速地预测功函数和断裂能，并促进具有调整后表面性质的材料发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The work function and cleavage energy of a surface are critical propertiesthat determine the viability of materials in electronic emission applications,semiconductor devices, and heterogeneous catalysis. While first principlescalculations are accurate in predicting these properties, their computationalexpense combined with the vast search space of surfaces make a comprehensivescreening approach with density functional theory (DFT) infeasible. Here, weintroduce FIRE-GNN (Force-Informed, Relaxed Equivariance Graph Neural Network),which integrates surface-normal symmetry breaking and machine learninginteratomic potential (MLIP)-derived force information, achieving a twofoldreduction in mean absolute error (down to 0.065 eV) over the previousstate-of-the-art for work function prediction. We additionally benchmark recentinvariant and equivariant architectures, analyze the impact of symmetrybreaking, and evaluate out-of-distribution generalization, demonstrating thatFIRE-GNN consistently outperforms competing models for work functionpredictions. This model enables accurate and rapid predictions of the workfunction and cleavage energy across a vast chemical space and facilitates thediscovery of materials with tuned surface properties</description>
      <author>example@mail.com (Circe Hsu, Claire Schlesinger, Karan Mudaliar, Jordan Leung, Robin Walters, Peter Schindler)</author>
      <guid isPermaLink="false">2508.16012v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator</title>
      <link>http://arxiv.org/abs/2508.16011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HePGA是一种基于3D异构PIM的加速器，用于GNN训练和推理，通过结合不同PIM设备的优势，实现了更高的能效和计算效率。&lt;h4&gt;背景&lt;/h4&gt;Processing-In-Memory (PIM)架构在加速图神经网络(GNN)训练和推理方面具有潜力，但不同PIM设备(如ReRAM、FeFET、PCM、MRAM和SRAM)在功耗、延迟、面积和非理想特性方面各有权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能高效加速GNN训练的3D异构PIM架构，结合多种PIM设备的优势，实现高能效和高性能。&lt;h4&gt;方法&lt;/h4&gt;提出HePGA加速器，利用GNN层和相关计算内核的特性，优化它们在不同PIM设备和平面层的映射，通过3D集成将多种PIM设备组合在单一平台上。&lt;h4&gt;主要发现&lt;/h4&gt;实验分析显示，HePGA在能效(TOPS/W)和计算效率(TOPS/mm²)方面分别比现有PIM架构高出3.8倍和6.8倍，同时保持GNN预测准确性不变。&lt;h4&gt;结论&lt;/h4&gt;HePGA是一种有效的GNN训练加速方案，还能用于加速新兴Transformer模型的推理。&lt;h4&gt;翻译&lt;/h4&gt;内存处理(PIM)架构为加速图神经网络(GNN)训练和推理提供了一种有前景的方法。然而，各种PIM设备(如ReRAM、FeFET、PCM、MRAM和SRAM)在功耗、延迟、面积和非理想性方面各有不同的权衡。通过3D集成实现的异构多核架构可以在单一平台上组合多种PIM设备，实现能效高、性能强的GNN训练。在这项工作中，我们提出了一种用于GNN训练的基于3D异构PIM的加速器，称为HePGA。我们利用GNN层和相关计算内核的独特特性，优化它们在不同PIM设备和平面层上的映射。我们的实验分析表明，HePGA在能效(TOPS/W)和计算效率(TOPS/mm2)方面分别比现有的PIM架构高出3.8倍和6.8倍，同时不牺牲GNN预测准确性。最后，我们展示了HePGA在加速新兴Transformer模型推理方面的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Processing-In-Memory (PIM) architectures offer a promising approach toaccelerate Graph Neural Network (GNN) training and inference. However, variousPIM devices such as ReRAM, FeFET, PCM, MRAM, and SRAM exist, with each deviceoffering unique trade-offs in terms of power, latency, area, andnon-idealities. A heterogeneous manycore architecture enabled by 3D integrationcan combine multiple PIM devices on a single platform, to enableenergy-efficient and high-performance GNN training. In this work, we propose a3D heterogeneous PIM-based accelerator for GNN training referred to as HePGA.We leverage the unique characteristics of GNN layers and associated computingkernels to optimize their mapping on to different PIM devices as well as planartiers. Our experimental analysis shows that HePGA outperforms existingPIM-based architectures by up to 3.8x and 6.8x in energy-efficiency (TOPS/W)and compute efficiency (TOPS/mm2) respectively, without sacrificing the GNNprediction accuracy. Finally, we demonstrate the applicability of HePGA toaccelerate inferencing of emerging transformer models.</description>
      <author>example@mail.com (Chukwufumnanya Ogbogu, Gaurav Narang, Biresh Kumar Joardar, Janardhan Rao Doppa, Krishnendu Chakrabarty, Partha Pratim Pande)</author>
      <guid isPermaLink="false">2508.16011v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Simulation-Based Inference for Direction Reconstruction of Ultra-High-Energy Cosmic Rays with Radio Arrays</title>
      <link>http://arxiv.org/abs/2508.15991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures. Submitted to PRD. Reproducible code and  notebooks: sbi_uhecr_radio_recon v0.1.0 - Zenodo DOI 10.5281/zenodo.16895985;  GitHub https://github.com/oscar-macias/sbi_uhecr_radio_recon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于模拟的推断方法，结合物理信息图神经网络和正规化流后验分布，用于超高能宇宙射线观测站的方向重建。该方法在真实模拟数据上训练，提供了物理可解释的重建、良好校准的不确定性和快速推断，适用于未来的宇宙射线观测实验。&lt;h4&gt;背景&lt;/h4&gt;超高能宇宙射线观测站需要无偏的方向重建来实现与稀疏纳秒级无线电脉冲的多信使天文学。传统的显式似然方法通常依赖简化模型，可能导致结果偏差和不确定性估计不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供准确方向重建、良好校准的不确定性估计和快速推断的方法，特别适用于处理高度倾斜事件的宇宙射线观测实验。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于模拟的推断流程，将物理信息图神经网络与正规化流后验分布在'学习宇宙隐式似然推断'框架中耦合。每个事件通过解析平面波前拟合初始化；GNN学习天线信号间的时空相关性来改进估计；其冻结嵌入条件一个八块自回归流，返回完整的贝叶斯后验。该方法在约8000个使用ZHAireS代码生成的现实UHECR空气簇射模拟上进行训练，后验分布经过温度校准以满足经验覆盖目标。&lt;h4&gt;主要发现&lt;/h4&gt;在测试UHECR事件上展示了亚度数的中值角分辨率；名义的68%最高后验密度区域捕获了71%±2%的真实到达方向，表明不确定性校准是轻度保守的。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了物理可解释的重建、良好校准的不确定性和快速推断，非常适合即将到来的针对高度倾斜事件的宇宙射线观测实验，如GRAND、AugerPrime Radio、IceCube-Gen2、RNO-G和BEACON。&lt;h4&gt;翻译&lt;/h4&gt;超高能宇宙射线观测站需要无偏的方向重建，以便与稀疏的纳秒级无线电脉冲实现多信使天文学。显式似然方法通常依赖简化模型，这可能使结果产生偏差并低估不确定性。我们引入了一种基于模拟的推断流程，将物理信息图神经网络与正规化流后验分布在'学习宇宙隐式似然推断'框架中耦合。每个事件通过解析平面波前拟合初始化；GNN通过学习天线信号之间的时空相关性来改进这一估计；其冻结嵌入条件一个八块自回归流，返回完整的贝叶斯后验。在约8000个使用ZHAireS代码生成的现实UHECR空气簇射模拟上进行训练，后验分布经过温度校准以满足经验覆盖目标。我们在测试UHECR事件上展示了亚度数的中值角分辨率，并发现名义的68%最高后验密度区域捕获了71%±2%的真实到达方向，表明不确定性校准是轻度保守的。这种方法提供了物理可解释的重建、良好校准的不确定性和快速推断，非常适合即将到来的针对高度倾斜事件的实验，如GRAND、AugerPrime Radio、IceCube-Gen2、RNO-G和BEACON。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultra-high-energy cosmic-ray (UHECR) observatories require unbiased directionreconstruction to enable multi-messenger astronomy with sparse,nanosecond-scale radio pulses. Explicit likelihood methods often rely onsimplified models, which may bias results and understate uncertainties. Weintroduce a simulation-based inference pipeline that couples a physics-informedgraph neural network (GNN) to a normalizing-flow posterior within the\textit{Learning the Universe Implicit Likelihood Inference} framework. Eachevent is seeded by an analytic plane-wavefront fit; the GNN refines thisestimate by learning spatiotemporal correlations among antenna signals, and itsfrozen embedding conditions an eight-block autoregressive flow that returns thefull Bayesian posterior. Trained on about $8,000$ realistic UHECR air-showersimulations generated with the ZHAireS code, the posteriors aretemperature-calibrated to meet empirical coverage targets. We demonstrate asub-degree median angular resolution on test UHECR events, and find that thenominal 68\% highest-posterior-density contours capture $71\% \pm 2\%$ of truearrival directions, indicating a mildly conservative uncertainty calibration.This approach provides physically interpretable reconstructions,well-calibrated uncertainties, and rapid inference, making it ideally suitedfor upcoming experiments targeting highly inclined events, such as GRAND,AugerPrime Radio, IceCube-Gen2, RNO-G, and BEACON.</description>
      <author>example@mail.com (Oscar Macias, Zachary Mason, Matthew Ho, Arsène Ferrière, Aurélien Benoit-Lévy, Matías Tueros)</author>
      <guid isPermaLink="false">2508.15991v1</guid>
      <pubDate>Mon, 25 Aug 2025 15:28:37 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.14707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的模型驱动方法来训练视觉基础模型(VFM)，通过联合知识转移和保留来整合多个预训练教师模型，构建了一个强大的VFM，无需大量标记数据即可继承教师专业知识，并在多个视觉任务上优于现有数据中心模型。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFM)主要使用数据中心方法开发，这些方法需要大量高质量标签数据进行训练，这对缺乏大规模数据和高端GPU的机构构成了瓶颈。另一方面，许多开源视觉模型已经在特定领域数据上预训练，能够提炼和表示可跨多种应用转移的核心知识，但这些模型在开发通用VFM方面尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型驱动方法来训练视觉基础模型(VFM)，通过联合知识转移和保留，整合多个预训练教师模型，构建一个强大的通用VFM，无需大量标记数据即可继承教师专业知识，并支持多种下游任务。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的模型驱动方法，通过以下步骤：在共享潜在空间中统一多个预训练教师模型，缓解其分布差异导致的'不平衡转移'问题；采用知识保留策略，将通用教师作为知识库，使用适配器模块整合特定目的教师的知识；统一和聚合现有模型，构建强大的VFM。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的VFM不仅提供了可泛化的视觉特征，还内在支持多种下游任务；广泛的实验表明，该VFM在图像分类、目标检测、语义分割和实例分割四个基本视觉任务上优于现有的数据中心模型。&lt;h4&gt;结论&lt;/h4&gt;通过统一和聚合现有模型，可以构建一个强大的VFM，继承教师专业知识而无需大量标记数据训练，且在多个视觉任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)主要使用数据中心方法开发。这些方法需要在大量通常带有高质量标签的数据上进行训练，这对缺乏大规模数据和高端GPU的大多数机构构成了瓶颈。另一方面，许多开源视觉模型已经在特定领域数据上进行了预训练，使它们能够提炼和表示一种可在多种应用间转移的核心知识形式。尽管这些模型是非常宝贵的资产，但在支持通用VFM的开发方面，它们仍然在很大程度上未被探索。在本文中，我们提出了一种新的模型驱动方法，通过联合知识转移和保留来训练VFMs。我们的方法在共享潜在空间中统一多个预训练的教师模型，以缓解它们分布差异造成的'不平衡转移'问题。此外，我们引入了一种知识保留策略，将通用教师作为知识库，使用适配器模块整合来自其他特定目的教师的知识。通过统一和聚合现有模型，我们构建了一个强大的VFM，能够继承教师的专业知识，而无需在大量标记数据上进行训练。我们的模型不仅提供了可泛化的视觉特征，还内在支持多种下游任务。广泛的实验证明，我们的VFM在图像分类、目标检测、语义分割和实例分割四个基本视觉任务上优于现有的数据中心模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models (VFMs) are predominantly developed usingdata-centric methods. These methods require training on vast amounts of datausually with high-quality labels, which poses a bottleneck for mostinstitutions that lack both large-scale data and high-end GPUs. On the otherhand, many open-source vision models have been pretrained on domain-specificdata, enabling them to distill and represent core knowledge in a form that istransferable across diverse applications. Even though these models are highlyvaluable assets, they remain largely under-explored in empowering thedevelopment of a general-purpose VFM. In this paper, we presents a newmodel-driven approach for training VFMs through joint knowledge transfer andpreservation. Our method unifies multiple pre-trained teacher models in ashared latent space to mitigate the ``imbalanced transfer'' issue caused bytheir distributional gaps. Besides, we introduce a knowledge preservationstrategy to take a general-purpose teacher as a knowledge base for integratingknowledge from the remaining purpose-specific teachers using an adapter module.By unifying and aggregating existing models, we build a powerful VFM to inheritteachers' expertise without needing to train on a large amount of labeled data.Our model not only provides generalizable visual features, but also inherentlysupports multiple downstream tasks. Extensive experiments demonstrate that ourVFM outperforms existing data-centric models across four fundamental visiontasks, including image classification, object detection, semantic and instancesegmentation.</description>
      <author>example@mail.com (Jiabo Huang, Chen Chen, Lingjuan Lyu)</author>
      <guid isPermaLink="false">2508.14707v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
  <item>
      <title>Attention-Based Explainability for Structure-Property Relationships</title>
      <link>http://arxiv.org/abs/2508.15493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了基于注意力的神经网络在揭示材料科学中结构-性质关系和潜在物理机制方面的潜力，以PbTiO3薄膜的铁电性质为案例研究。&lt;h4&gt;背景&lt;/h4&gt;机器学习方法正在成为材料科学中构建相关结构-性质关系的通用范式，基于多模态表征，但需要开发方法来解释相关模型的物理可解释性。&lt;h4&gt;目的&lt;/h4&gt;展示基于注意力的神经网络在揭示结构-性质关系和潜在物理机制方面的潜力，并开发物理可解释性方法。&lt;h4&gt;方法&lt;/h4&gt;通过分析注意力分数解开不同畴结构对极化翻转过程的影响；将基于注意力的Transformer模型作为直接可解释性工具和解释无监督机器学习表示的替代模型；比较注意力派生的可解释性分数与经典的SHAP分析。&lt;h4&gt;主要发现&lt;/h4&gt;注意力分析能够区分不同畴结构对极化翻转过程的影响；与自然语言处理应用相比，注意力机制在材料科学中能高效突出有意义的结构特征。&lt;h4&gt;结论&lt;/h4&gt;基于注意力的神经网络能够有效揭示材料科学中的结构-性质关系和物理机制，为材料科学中的模型解释提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;机器学习方法正在成为基于多模态表征构建材料科学中相关结构-性质关系的通用范式。然而，这需要开发方法来解释相关模型的物理可解释性。在此，我们展示了基于注意力的神经网络在揭示结构-性质关系和潜在物理机制方面的潜力，以PbTiO3薄膜的铁电性质作为案例研究。通过分析注意力分数，我们解开了不同畴结构对极化翻转过程的影响。基于注意力的Transformer模型被探索作为直接可解释性工具和解释通过无监督机器学习学习到的表示的替代模型，从而能够识别基于物理的相关性。我们将注意力派生的可解释性分数与经典的SHapley Additive exPlanations（SHAP）分析进行了比较，表明与自然语言处理中的应用相比，材料科学中的注意力机制在突出有意义的结构特征方面表现出高效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning methods are emerging as a universal paradigm forconstructing correlative structure-property relationships in materials sciencebased on multimodal characterization. However, this necessitates development ofmethods for physical interpretability of the resulting correlative models.Here, we demonstrate the potential of attention-based neural networks forrevealing structure-property relationships and the underlying physicalmechanisms, using the ferroelectric properties of PbTiO3 thin films as a casestudy. Through the analysis of attention scores, we disentangle the influenceof distinct domain patterns on the polarization switching process. Theattention-based Transformer model is explored both as a direct interpretabilitytool and as a surrogate for explaining representations learned via unsupervisedmachine learning, enabling the identification of physically groundedcorrelations. We compare attention-derived interpretability scores withclassical SHapley Additive exPlanations (SHAP) analysis and show that, incontrast to applications in natural language processing, attention mechanismsin materials science exhibit high efficiency in highlighting meaningfulstructural features.</description>
      <author>example@mail.com (Boris N. Slautin, Utkarsh Pratiush, Yongtao Liu, Hiroshi Funakubo, Vladimir V. Shvartsman, Doru C. Lupascu, Sergei V. Kalinin)</author>
      <guid isPermaLink="false">2508.15493v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Learning Protein-Ligand Binding in Hyperbolic Space</title>
      <link>http://arxiv.org/abs/2508.15480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了HypSeek，一种基于双曲几何的蛋白质-配体结合预测框架，能够有效捕捉分子相互作用的层次结构和精细亲和力变化，在虚拟筛选和亲和力排序任务中均表现出色。&lt;h4&gt;背景&lt;/h4&gt;蛋白质-配体结合预测是虚拟筛选和亲和力排序的核心，这两项是药物发现中的基本任务。现有的基于检索的方法将配体和蛋白质口袋嵌入欧几里得空间进行相似性搜索，但欧几里得嵌入的几何结构往往无法捕捉分子相互作用中固有的层次结构和精细的亲和力变化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更好地表示蛋白质-配体相互作用的方法，特别是能够捕捉层次结构和精细亲和力变化，以解决现有方法在处理活性悬崖等挑战性情况时的不足。&lt;h4&gt;方法&lt;/h4&gt;提出HypSeek，一种双曲表示学习框架，将配体、蛋白质口袋和序列嵌入洛伦兹模型双曲空间中。利用双曲空间的指数几何和负曲率，实现表达性强、亲和力敏感的嵌入。引入蛋白质引导的三塔架构，统一了虚拟筛选和亲和力排序在单一框架中。&lt;h4&gt;主要发现&lt;/h4&gt;HypSeek在DUD-E数据集上的虚拟筛选早期富集率从42.63提升到51.44（提升20.7%），在JACS数据集上的亲和力排序相关性从0.5774提升到0.7239（提升25.4%）。双曲几何能够有效建模全局活性和细微功能差异，特别是在活性悬崖等具有挑战性的情况下。&lt;h4&gt;结论&lt;/h4&gt;双曲几何表示学习框架为蛋白质-配体建模提供了强大的归纳偏置，能够有效捕捉分子相互作用的层次结构和精细亲和力变化，为药物发现提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质-配体结合预测是虚拟筛选和亲和力排序的核心，这两项是药物发现中的基本任务。虽然最近的基于检索的方法将配体和蛋白质口袋嵌入欧几里得空间进行相似性搜索，但欧几里得嵌入的几何结构往往无法捕捉分子相互作用中固有的层次结构和精细的亲和力变化。在这项工作中，我们提出了HypSeek，一种双曲表示学习框架，将配体、蛋白质口袋和序列嵌入洛伦兹模型双曲空间中。通过利用双曲空间的指数几何和负曲率，HypSeek能够实现表达性强、亲和力敏感的嵌入，可以有效地建模全局活性和细微的功能差异——特别是在具有挑战性的情况下，如活性悬崖，即结构相似的配体表现出大的亲和力差距。我们的模型统一了虚拟筛选和亲和力排序在一个单一框架中，引入了蛋白质引导的三塔架构来增强表示结构。HypSeek在DUD-E上的虚拟筛选早期富集率从42.63提升到51.44（+20.7%），在JACS上的亲和力排序相关性从0.5774提升到0.7239（+25.4%），证明了双曲几何在这两项任务中的优势，并强调了其作为蛋白质-配体建模的强大归纳偏置的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein-ligand binding prediction is central to virtual screening andaffinity ranking, two fundamental tasks in drug discovery. While recentretrieval-based methods embed ligands and protein pockets into Euclidean spacefor similarity-based search, the geometry of Euclidean embeddings often failsto capture the hierarchical structure and fine-grained affinity variationsintrinsic to molecular interactions. In this work, we propose HypSeek, ahyperbolic representation learning framework that embeds ligands, proteinpockets, and sequences into Lorentz-model hyperbolic space. By leveraging theexponential geometry and negative curvature of hyperbolic space, HypSeekenables expressive, affinity-sensitive embeddings that can effectively modelboth global activity and subtle functional differences-particularly inchallenging cases such as activity cliffs, where structurally similar ligandsexhibit large affinity gaps. Our mode unifies virtual screening and affinityranking in a single framework, introducing a protein-guided three-towerarchitecture to enhance representational structure. HypSeek improves earlyenrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) andaffinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%),demonstrating the benefits of hyperbolic geometry across both tasks andhighlighting its potential as a powerful inductive bias for protein-ligandmodeling.</description>
      <author>example@mail.com (Jianhui Wang, Wenyu Zhu, Bowen Gao, Xin Hong, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan)</author>
      <guid isPermaLink="false">2508.15480v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials</title>
      <link>http://arxiv.org/abs/2508.15392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 4 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CITE数据集，这是首个也是最大的催化材料异构文本属性引用图基准，包含超过438K节点和1.2M条边，跨越四种关系类型。&lt;h4&gt;背景&lt;/h4&gt;现实世界中存在大量带有文本属性的异构图(TAGs)，但缺乏大规模基准数据集，这阻碍了表示学习方法的发展和公平比较。&lt;h4&gt;目的&lt;/h4&gt;解决异构文本属性图(TAGs)领域缺乏大规模基准数据集的问题，促进表示学习方法的发展。&lt;h4&gt;方法&lt;/h4&gt;构建CITE数据集，建立标准化评估流程，在节点分类任务上进行基准测试，并对CITE的异构和文本属性进行消融实验；同时比较四类学习范式：同构图模型、异构图模型、以大语言模型为中心的模型和图+大语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验比较了不同学习范式在CITE数据集上的表现，包括同构图模型、异构图模型、大语言模型为中心的模型以及图+大语言模型的性能。&lt;h4&gt;结论&lt;/h4&gt;提供了CITE数据集概述、标准化评估协议以及跨不同建模范式的基线和消融实验，为异构文本属性图的研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图(TAGs)在现实世界系统中普遍存在，其中每个节点都携带其自身的文本特征。在许多情况下，这些图本质上是异构的，包含多种节点类型和多样化的边类型。尽管这类异构TAGs普遍存在，但仍然缺乏大规模基准数据集。这一短缺已成为关键瓶颈，阻碍了在异构文本属性图上表示学习方法的发展和公平比较。在本文中，我们介绍了CITE - 催化信息文本实体图，这是首个也是最大的催化材料异构文本属性引用图基准。CITE包含超过438K个节点和1.2M条边，跨越四种关系类型。此外，我们建立了标准化评估流程，并在节点分类任务上进行了广泛的基准测试，以及对CITE的异构和文本属性进行了消融实验。我们比较了四类学习范式，包括同构图模型、异构图模型、以大语言模型为中心的模型和图+大语言模型。总之，我们提供了(i) CITE数据集概述，(ii)标准化评估协议，以及(iii)跨不同建模范式的基线和消融实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-attributed graphs(TAGs) are pervasive in real-world systems,where eachnode carries its own textual features. In many cases these graphs areinherently heterogeneous, containing multiple node types and diverse edgetypes. Despite the ubiquity of such heterogeneous TAGs, there remains a lack oflarge-scale benchmark datasets. This shortage has become a critical bottleneck,hindering the development and fair comparison of representation learningmethods on heterogeneous text-attributed graphs. In this paper, we introduceCITE - Catalytic Information Textual Entities Graph, the first and largestheterogeneous text-attributed citation graph benchmark for catalytic materials.CITE comprises over 438K nodes and 1.2M edges, spanning four relation types. Inaddition, we establish standardized evaluation procedures and conduct extensivebenchmarking on the node classification task, as well as ablation experimentson the heterogeneous and textual properties of CITE. We compare four classes oflearning paradigms, including homogeneous graph models, heterogeneous graphmodels, LLM(Large Language Model)-centric models, and LLM+Graph models. In anutshell, we provide (i) an overview of the CITE dataset, (ii) standardizedevaluation protocols, and (iii) baseline and ablation experiments acrossdiverse modeling paradigms.</description>
      <author>example@mail.com (Chenghao Zhang, Qingqing Long, Ludi Wang, Wenjuan Cui, Jianjun Yu, Yi Du)</author>
      <guid isPermaLink="false">2508.15392v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction</title>
      <link>http://arxiv.org/abs/2508.15378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出EvoFormer框架，解决了动态图级嵌入中的结构访问偏差和突变演化失明问题，通过结构感知Transformer模块和演化敏感时间模块，显著提升了图相似性排序、时间异常检测和时间分段任务的性能。&lt;h4&gt;背景&lt;/h4&gt;动态图级嵌入旨在捕捉网络结构的演变，这对建模现实场景至关重要。然而，现有方法存在两个关键问题：结构访问偏差和突变演化失明。&lt;h4&gt;目的&lt;/h4&gt;提出EvoFormer框架，解决现有动态图级嵌入方法中的结构访问偏差和突变演化失明问题，提高动态图级表示学习的性能。&lt;h4&gt;方法&lt;/h4&gt;EvoFormer包含两个核心模块：1)结构感知Transformer模块，基于节点结构角色的位置编码减轻结构访问偏差；2)演化敏感时间模块，通过三步策略建模时间演化：随机游走时间戳分类、图级时间分段、分段感知时间自注意力与边演化预测任务结合。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的广泛评估显示，EvoFormer在图相似性排序、时间异常检测和时间分段任务中实现了最先进的性能，有效验证了其在修正结构和时间偏差方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;EvoFormer通过创新性地解决结构访问偏差和突变演化失明问题，显著提升了动态图级表示学习的能力，为现实世界动态网络建模提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;动态图级嵌入旨在捕捉网络结构的演变，这对建模现实场景至关重要。然而，现有方法面临两个关键但未被充分探讨的问题：结构访问偏差，其中随机游走采样过度强调高度节点，导致冗余和有噪声的结构表示；以及突变演化失明，由于僵化或过于简单的时间建模策略，无法有效检测突然的结构变化，导致不一致的时间嵌入。为克服这些挑战，我们提出EvoFormer，一个专为动态图级表示学习设计的演化感知Transformer框架。为减轻结构访问偏差，EvoFormer引入了结构感知Transformer模块，基于节点结构角色的位置编码，使模型能够全局区分和准确表示节点结构。为克服突变演化失明，EvoFormer采用演化敏感时间模块，通过三步顺序策略明确建模时间演化：(I)随机游走时间戳分类，生成初始时间感知图级嵌入；(II)图级时间分段，将图流划分为反映结构一致性的时期；(III)分段感知时间自注意力与边演化预测任务结合，使模型能够精确捕捉分段边界并感知结构演化趋势，有效适应快速的时间变化。在五个基准数据集上的广泛评估证实，EvoFormer在图相似性排序、时间异常检测和时间分段任务中实现了最先进的性能，验证了其在修正结构和时间偏差方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761134&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic graph-level embedding aims to capture structural evolution innetworks, which is essential for modeling real-world scenarios. However,existing methods face two critical yet under-explored issues: Structural VisitBias, where random walk sampling disproportionately emphasizes high-degreenodes, leading to redundant and noisy structural representations; and AbruptEvolution Blindness, the failure to effectively detect sudden structuralchanges due to rigid or overly simplistic temporal modeling strategies,resulting in inconsistent temporal embeddings. To overcome these challenges, wepropose EvoFormer, an evolution-aware Transformer framework tailored fordynamic graph-level representation learning. To mitigate Structural Visit Bias,EvoFormer introduces a Structure-Aware Transformer Module that incorporatespositional encoding based on node structural roles, allowing the model toglobally differentiate and accurately represent node structures. To overcomeAbrupt Evolution Blindness, EvoFormer employs an Evolution-Sensitive TemporalModule, which explicitly models temporal evolution through a sequentialthree-step strategy: (I) Random Walk Timestamp Classification, generatinginitial timestamp-aware graph-level embeddings; (II) Graph-Level TemporalSegmentation, partitioning the graph stream into segments reflectingstructurally coherent periods; and (III) Segment-Aware Temporal Self-Attentioncombined with an Edge Evolution Prediction task, enabling the model toprecisely capture segment boundaries and perceive structural evolution trends,effectively adapting to rapid temporal shifts. Extensive evaluations on fivebenchmark datasets confirm that EvoFormer achieves state-of-the-art performancein graph similarity ranking, temporal anomaly detection, and temporalsegmentation tasks, validating its effectiveness in correcting structural andtemporal biases.</description>
      <author>example@mail.com (Haodi Zhong, Liuxin Zou, Di Wang, Bo Wang, Zhenxing Niu, Quan Wang)</author>
      <guid isPermaLink="false">2508.15378v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>MLLMRec: Exploring the Potential of Multimodal Large Language Models in Recommender Systems</title>
      <link>http://arxiv.org/abs/2508.15304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MLLMRec是一种基于大型多模态语言模型的多模态推荐框架，通过图像语义转换、用户偏好净化和物品图优化策略解决了现有方法中的关键问题，实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;多模态推荐结合用户行为数据和物品模态特征来揭示用户偏好，相比传统推荐表现更优，但现有方法仍存在用户多模态表示初始化不佳和物品-物品图质量问题等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态推荐方法中用户表示初始化不当和物品-物品图包含噪声边的问题，提升推荐性能。&lt;h4&gt;方法&lt;/h4&gt;提出MLLMRec框架，一方面将物品图像转换为高质量语义描述并与文本元数据融合，为用户构建行为描述列表并输入MLLM推理净化用户偏好；另一方面设计阈值控制去噪和拓扑感知增强策略优化物品-物品图。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开数据集上的实验表明，MLLMRec实现了最先进的性能，比最佳基线平均提升38.53%。&lt;h4&gt;结论&lt;/h4&gt;MLLMRec通过有效利用多模态语言模型和优化图结构，成功解决了现有多模态推荐方法中的关键问题，显著提升了推荐系统性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态推荐通常将用户行为数据与物品的模态特征相结合以揭示用户偏好，相比传统推荐表现出更优越的性能。然而，现有方法仍存在两个关键问题：(1)用户多模态表示的初始化方法要么无法感知行为，要么受到噪声污染；(2)基于KNN的物品-物品图包含相似度低的噪声边，且缺乏受众共现关系。为解决这些问题，我们提出了MLLMRec，这是一种新颖的MLLM驱动的多模态推荐框架，包含两种物品-物品图优化策略。一方面，首先使用MLLM将物品图像转换为高质量的语义描述，然后与物品的文本元数据融合。接着，我们为每个用户构建行为描述列表并输入MLLM，以推理出包含交互动机的净化用户偏好。另一方面，我们设计了阈值控制去噪和拓扑感知增强策略来优化次优物品-物品图，从而增强物品表示学习。在三个公开数据集上的大量实验表明，MLLMRec实现了最先进的性能，比最佳基线平均提升38.53%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal recommendation typically combines the user behavioral data withthe modal features of items to reveal user's preference, presenting superiorperformance compared to the conventional recommendations. However, existingmethods still suffer from two key problems: (1) the initialization methods ofuser multimodal representations are either behavior-unperceived ornoise-contaminated, and (2) the KNN-based item-item graph contains noisy edgeswith low similarities and lacks audience co-occurrence relationships. Toaddress such issues, we propose MLLMRec, a novel MLLM-driven multimodalrecommendation framework with two item-item graph refinement strategies. On theone hand, the item images are first converted into high-quality semanticdescriptions using an MLLM, which are then fused with the textual metadata ofitems. Then, we construct a behavioral description list for each user and feedit into the MLLM to reason about the purified user preference containinginteraction motivations. On the other hand, we design the threshold-controlleddenoising and topology-aware enhancement strategies to refine the suboptimalitem-item graph, thereby enhancing the item representation learning. Extensiveexperiments on three publicly available datasets demonstrate that MLLMRecachieves the state-of-the-art performance with an average improvement of 38.53%over the best baselines.</description>
      <author>example@mail.com (Yuzhuo Dang, Xin Zhang, Zhiqiang Pan, Yuxiao Duan, Wanyu Chen, Fei Cai, Honghui Chen)</author>
      <guid isPermaLink="false">2508.15304v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Learning ECG Representations via Poly-Window Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.15225v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been accepted for publication in IEEE-EMBS  International Conference on Biomedical and Health Informatics 2025. The final  published version will be available via IEEE Xplore&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多窗口对比学习框架，用于心电图分析，通过提取多个时间窗口构建正样本对，学习时不变和生理上有意义的特征，显著提高了模型性能并减少了训练时间。&lt;h4&gt;背景&lt;/h4&gt;心电图分析是心血管疾病诊断的基础，但深度学习模型的性能常受限于标注数据的有限获取。自监督对比学习已成为从无标签信号中学习鲁棒ECG表征的强大方法，但现有方法大多只生成成对的增强视图，未能充分利用ECG记录的丰富时间结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够充分利用ECG时间结构的自监督学习方法，以提升模型性能并减少训练时间。&lt;h4&gt;方法&lt;/h4&gt;从每个ECCG实例中提取多个时间窗口构建正样本对，通过统计方法最大化它们的一致性；受慢特征分析原理启发，明确鼓励模型学习跨时间持续存在的时不变和生理上有意义的特征。&lt;h4&gt;主要发现&lt;/h4&gt;多窗口对比学习在多标签超类分类中始终优于传统双视图方法，达到更高的AUROC（0.891 vs 0.888）和F1分数（0.680 vs 0.679）；需要少至四倍的预训练轮次（32 vs 128）；总预训练时间减少14.8%；尽管处理多个窗口，但仍显著减少了训练轮次和总计算时间。&lt;h4&gt;结论&lt;/h4&gt;多窗口对比学习成为自动化ECG分析的一种高效可扩展范式，并为生物医学时间序列数据的自监督表征学习提供了有前景的通用框架。&lt;h4&gt;翻译&lt;/h4&gt;心电图分析是心血管疾病诊断的基础，但深度学习模型的性能常受限于标注数据的有限获取。自监督对比学习已成为从无标签信号中学习鲁棒ECG表征的强大方法。然而，大多数现有方法只生成成对的增强视图，未能充分利用ECG记录的丰富时间结构。在这项工作中，我们提出了一个多窗口对比学习框架。我们从每个ECG实例中提取多个时间窗口来构建正样本对，并通过统计方法最大化它们的一致性。受慢特征分析原理启发，我们的方法明确鼓励模型学习跨时间持续的时不变和生理上有意义的特征。我们在PTB-XL数据集上通过大量实验和消融研究验证了我们的方法。结果表明，多窗口对比学习在多标签超类分类中始终优于传统的双视图方法，达到更高的AUROC（0.891 vs 0.888）和F1分数（0.680 vs 0.679），同时需要少至四倍的预训练轮次（32 vs 128）和14.8%的总预训练时间减少。尽管每个样本处理多个窗口，我们仍实现了训练轮次和总计算时间的显著减少，使我们的方法对于训练基础模型具有实用性。通过大量消融，我们确定了最佳设计选择，并展示了在各种超参数下的鲁棒性。这些发现确立了多窗口对比学习作为自动化ECG分析的一种高效可扩展范式，并为生物医学时间序列数据的自监督表征学习提供了有前景的通用框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiogram (ECG) analysis is foundational for cardiovascular diseasediagnosis, yet the performance of deep learning models is often constrained bylimited access to annotated data. Self-supervised contrastive learning hasemerged as a powerful approach for learning robust ECG representations fromunlabeled signals. However, most existing methods generate only pairwiseaugmented views and fail to leverage the rich temporal structure of ECGrecordings. In this work, we present a poly-window contrastive learningframework. We extract multiple temporal windows from each ECG instance toconstruct positive pairs and maximize their agreement via statistics. Inspiredby the principle of slow feature analysis, our approach explicitly encouragesthe model to learn temporally invariant and physiologically meaningful featuresthat persist across time. We validate our approach through extensiveexperiments and ablation studies on the PTB-XL dataset. Our results demonstratethat poly-window contrastive learning consistently outperforms conventionaltwo-view methods in multi-label superclass classification, achieving higherAUROC (0.891 vs. 0.888) and F1 scores (0.680 vs. 0.679) while requiring up tofour times fewer pre-training epochs (32 vs. 128) and 14.8% in total wall clockpre-training time reduction. Despite processing multiple windows per sample, weachieve a significant reduction in the number of training epochs and totalcomputation time, making our method practical for training foundational models.Through extensive ablations, we identify optimal design choices and demonstraterobustness across various hyperparameters. These findings establish poly-windowcontrastive learning as a highly efficient and scalable paradigm for automatedECG analysis and provide a promising general framework for self-supervisedrepresentation learning in biomedical time-series data.</description>
      <author>example@mail.com (Yi Yuan, Joseph Van Duyn, Runze Yan, Zhuoyi Huang, Sulaiman Vesal, Sergey Plis, Xiao Hu, Gloria Hyunjung Kwak, Ran Xiao, Alex Fedorov)</author>
      <guid isPermaLink="false">2508.15225v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer</title>
      <link>http://arxiv.org/abs/2508.15215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为SleepDIFFormer的睡眠阶段分类方法，通过多变量微分变换器架构处理EEG和EOG信号，实现了跨域对齐训练，提高了在未见数据集上的泛化能力，并在五个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;睡眠阶段分类对评估睡眠质量和诊断睡眠障碍至关重要，但手动检查脑电图特征既耗时又易出错。现有的机器学习和深度学习方法面临EEG和EOG信号的非平稳性和变异性挑战，导致泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理EEG和EOG信号的睡眠阶段分类方法，提高在未见数据集上的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;提出SleepDIFFormer方法，使用多变量微分变换器架构(MDTA)处理EEG和EOG时序数据，通过跨域对齐训练，减轻空间和时间注意力噪声，学习域不变的联合EEG-EOG表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在五个不同的睡眠分期数据集上取得了最先进的性能，消融分析表明微分注意力权重与特征性睡眠脑电图模式高度相关。&lt;h4&gt;结论&lt;/h4&gt;SleepDIFFormer为推进自动化睡眠阶段分类及其在睡眠质量评估中的应用提供了新方法，源代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;睡眠阶段分类对于评估睡眠质量和诊断失眠等睡眠障碍至关重要。然而，手动检查每个阶段的脑电图特征既耗时又容易出错。尽管机器学习和深度学习方法已被积极开发，但它们仍面临脑电图(EEG)和眼电图(EOG)信号的非平稳性和变异性带来的挑战，导致在未见过的数据集上泛化能力差。本研究通过开发多变量微分变换器(SleepDIFFormer)进行联合EEG和EOG表示学习，提出了睡眠阶段分类方法。具体而言，SleepDIFFormer使用我们的多变量微分变换器架构(MDTA)处理EEG和EOG信号，通过跨域对齐进行训练。我们的方法减轻了空间和时间注意力噪声，同时通过特征分布对齐学习域不变的联合EEG-EOG表示，从而能够泛化到未见的目标数据集。经验上，我们在五个不同的睡眠分期数据集上评估了我们的方法，并与现有方法进行了比较，取得了最先进的性能。我们还对SleepDIFFormer进行了彻底的消融分析，并解释了微分注意力权重，突显了它们与特征性睡眠脑电图模式的相关性。这些发现对推进自动化睡眠阶段分类及其在睡眠质量评估中的应用具有重要意义。我们的源代码已在https://github.com/Ben1001409/SleepDIFFormer公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification of sleep stages is essential for assessing sleep quality anddiagnosing sleep disorders such as insomnia. However, manual inspection of EEGcharacteristics for each stage is time-consuming and prone to human error.Although machine learning and deep learning methods have been activelydeveloped, they continue to face challenges from the non-stationarity andvariability of electroencephalography (EEG) and electrooculography (EOG)signals, often leading to poor generalization on unseen datasets. This researchproposed a Sleep Stage Classification method by developing MultivariateDifferential Transformer (SleepDIFFormer) for joint EEG and EOG representationlearning. Specifically, SleepDIFFormer was developed to process EEG and EOGsignals using our Multivariate Differential Transformer Architecture (MDTA) fortime series, trained with cross-domain alignment. Our method mitigated spatialand temporal attention noise while learning a domain-invariant joint EEG-EOGrepresentation through feature distribution alignment, thereby enablinggeneralization to unseen target datasets. Empirically, we evaluated our methodon five different sleep staging datasets and compared it with existingapproaches, achieving state-of-the-art performance. We also conducted thoroughablation analyses of SleepDIFFormer and interpreted the differential attentionweights, highlighting their relevance to characteristic sleep EEG patterns.These findings have implications for advancing automated sleep stageclassification and its application to sleep quality assessment. Our source codeis publicly available at https://github.com/Ben1001409/SleepDIFFormer</description>
      <author>example@mail.com (Benjamin Wei Hao Chin, Yuin Torng Yew, Haocheng Wu, Lanxin Liang, Chow Khuen Chan, Norita Mohd Zain, Siti Balqis Samdin, Sim Kuan Goh)</author>
      <guid isPermaLink="false">2508.15215v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Kernel-based Equalized Odds: A Quantification of Accuracy-Fairness Trade-off in Fair Representation Learning</title>
      <link>http://arxiv.org/abs/2508.15084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于核的等化机会(EO)准则新公式EO_k，用于监督设置下的公平表征学习，能够严格量化独立性、分离性和校准性三个核心公平目标。&lt;h4&gt;背景&lt;/h4&gt;公平表征学习(FRL)旨在减轻对敏感属性的歧视同时保持预测准确性，但现有方法难以同时满足多个公平性准则。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时满足多种公平性准则的核方法，并在理论和实证上验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;提出基于核的EO_k准则，定义其经验对应物ÊO_k，推导集中不等式提供性能保证，并在无偏和有偏条件下分析其特性。&lt;h4&gt;主要发现&lt;/h4&gt;EO_k在无偏条件下同时满足独立性和分离性，在有偏条件下唯一保持预测准确性同时下限独立性和校准性，提供了公平准则间权衡的统一分析；ÊO_k可在二次时间内计算并有线性近似，具有理论保证。&lt;h4&gt;结论&lt;/h4&gt;EO_k为公平表征学习提供了统一的理论框架，为未来原则性且可证明公平的算法设计奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种用于监督设置公平表征学习的基于核的等化机会(EO)准则新公式EO_k。FRL的核心目标是在减轻对敏感属性S的歧视的同时，保持对目标变量Y的预测准确性。我们提出的准则能够对三个核心公平目标进行严格且可解释的量化：独立性(预测Ŷ与敏感属性S独立)、分离性(在目标属性Y条件下，预测Ŷ与敏感属性S独立)和校准性(在预测Ŷ条件下，目标属性Y与敏感属性S独立)。在无偏(Y与S独立)和有偏(Y依赖于S)条件下，我们证明了EO_k在前者中同时满足独立性和分离性，在后者中唯一保持预测准确性同时下限独立性和校准性，从而提供了这些公平准则间权衡的统一分析特征。我们进一步定义了经验对应物ÊO_k，这是一个基于核的统计量，可以在二次时间内计算，也有线性时间近似可用。推导了ÊO_k的集中不等式，提供了性能保证和误差边界，这些可作为公平合规的实际证明。虽然我们的重点是理论发展，但这些结果为未来实证研究中原则性且可证明公平的算法设计奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel kernel-based formulation of the Equalized Odds(EO) criterion, denoted as $EO_k$, for fair representation learning (FRL) insupervised settings. The central goal of FRL is to mitigate discriminationregarding a sensitive attribute $S$ while preserving prediction accuracy forthe target variable $Y$. Our proposed criterion enables a rigorous andinterpretable quantification of three core fairness objectives: independence(prediction $\hat{Y}$ is independent of $S$), separation (also known asequalized odds; prediction $\hat{Y}$ is independent with $S$ conditioned ontarget attribute $Y$), and calibration ($Y$ is independent of $S$ conditionedon the prediction $\hat{Y}$). Under both unbiased ($Y$ is independent of $S$)and biased ($Y$ depends on $S$) conditions, we show that $EO_k$ satisfies bothindependence and separation in the former, and uniquely preserves predictiveaccuracy while lower bounding independence and calibration in the latter,thereby offering a unified analytical characterization of the tradeoffs amongthese fairness criteria. We further define the empirical counterpart,$\hat{EO}_k$, a kernel-based statistic that can be computed in quadratic time,with linear-time approximations also available. A concentration inequality for$\hat{EO}_k$ is derived, providing performance guarantees and error bounds,which serve as practical certificates of fairness compliance. While our focusis on theoretical development, the results lay essential groundwork forprincipled and provably fair algorithmic design in future empirical studies.</description>
      <author>example@mail.com (Yijin Ni, Xiaoming Huo)</author>
      <guid isPermaLink="false">2508.15084v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning</title>
      <link>http://arxiv.org/abs/2508.14859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the 28th European Conference on Artificial Intelligence  (ECAI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GTGIB的框架，用于解决时序图学习中的归纳表示学习问题，通过结合图结构学习和时序图信息瓶颈技术，有效处理动态网络中的节点和边演变，以及新节点加入的挑战。&lt;h4&gt;背景&lt;/h4&gt;时序图学习对于处理动态网络中随时间演变的节点和边以及持续加入的新节点至关重要。在这样的场景下，归纳表示学习面临两大主要挑战：有效表示未见节点和减轻嘈杂或冗余的图信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理动态网络中节点和边演变以及新节点加入的归纳表示学习框架，解决有效表示未见节点和减轻嘈杂或冗余图信息的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出GTGIB框架，结合图结构学习（GSL）和时序图信息瓶颈（TGIB）。设计了一种新颖的两步GSL基础结构增强器来丰富和优化节点邻域，并通过理论证明和实验验证其有效性和效率。TGIB通过将信息瓶颈原理扩展到时序图，基于推导的可处理TGIB目标函数通过变分近似来规范边和特征，实现稳定高效的优化。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的链接预测任务中，基于GTGIB的模型在归纳设置下优于现有方法，并且在传递设置下表现出显著且一致的改进。&lt;h4&gt;结论&lt;/h4&gt;GTGIB框架有效解决了时序图学习中的归纳表示学习挑战，通过结合图结构学习和时序图信息瓶颈技术，能够有效处理动态网络中的节点和边演变以及新节点加入的情况，在各种设置下都表现出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;时序图学习对于动态网络至关重要，在这些网络中节点和边随时间演变，并且新节点持续加入系统。在这种情况下，归纳表示学习面临两大主要挑战：有效表示未见节点和减轻嘈杂或冗余的图信息。我们提出了GTGIB，一个将图结构学习（GSL）与时序图信息瓶颈（TGIB）相结合的多功能框架。我们设计了一种新颖的两步基于GSL的结构增强器，以丰富和优化节点邻域，并通过理论证明和实验证明了其有效性和效率。TGIB通过将信息瓶颈原理扩展到时序图，基于我们通过变分近似推导的可处理TGIB目标函数来规范边和特征，实现稳定和高效的优化。基于GTGIB的模型在四个真实世界数据集上评估用于链接预测；在归纳设置下，它们在所有数据集上都优于现有方法，并且在传递设置下有显著且一致的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal graph learning is crucial for dynamic networks where nodes and edgesevolve over time and new nodes continuously join the system. Inductiverepresentation learning in such settings faces two major challenges:effectively representing unseen nodes and mitigating noisy or redundant graphinformation. We propose GTGIB, a versatile framework that integrates GraphStructure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). Wedesign a novel two-step GSL-based structural enhancer to enrich and optimizenode neighborhoods and demonstrate its effectiveness and efficiency throughtheoretical proofs and experiments. The TGIB refines the optimized graph byextending the information bottleneck principle to temporal graphs, regularizingboth edges and features based on our derived tractable TGIB objective functionvia variational approximation, enabling stable and efficient optimization.GTGIB-based models are evaluated to predict links on four real-world datasets;they outperform existing methods in all datasets under the inductive setting,with significant and consistent improvement in the transductive setting.</description>
      <author>example@mail.com (Jiafeng Xiong, Rizos Sakellariou)</author>
      <guid isPermaLink="false">2508.14859v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Global-Distribution Aware Scenario-Specific Variational Representation Learning Framework</title>
      <link>http://arxiv.org/abs/2508.14493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025, 6 pages, 1 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种全局分布感知的场景特定变分表示学习框架(GSVR)，用于解决多场景推荐中的数据稀疏问题，帮助学习更鲁棒的场景特定表示。&lt;h4&gt;背景&lt;/h4&gt;随着电子商务的发展，商业平台需要提供适应不同场景的推荐以满足用户多样化的购物偏好。&lt;h4&gt;目的&lt;/h4&gt;解决当前多场景推荐方法中使用共享底层表示阻碍模型捕获场景独特性的问题，以及用户和项目交互在不同场景中变化导致的数据稀疏问题。&lt;h4&gt;方法&lt;/h4&gt;提出GSVR框架，使用概率模型为每个用户和项目在每个场景中生成场景特定分布，通过变分推断估计；引入全局知识感知的多项式分布作为先验知识，调节后验分布学习，确保相似用户和项目的分布相似性，减轻稀疏场景中记录较少的用户或项目被淹没的风险。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验结果证实了GSVR在协助现有多场景推荐方法学习更鲁棒表示方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;GSVR框架可直接应用于现有的多场景方法，有效解决了数据稀疏问题，帮助学习更鲁棒的场景特定表示。&lt;h4&gt;翻译&lt;/h4&gt;随着电子商务的出现，商业平台提供的推荐必须适应多样化场景以满足用户不同的购物偏好。当前方法通常使用统一框架为不同场景提供个性化推荐。然而，它们往往使用共享的底层表示，这部分阻碍了模型捕获场景独特性的能力。理想情况下，用户和项目在不同场景中应表现出特定特征，需要学习场景特定表示来区分场景。然而，用户和项目交互在不同场景中的变化导致数据稀疏问题，阻碍了场景特定表示的获取。为了学习鲁棒的场景特定表示，我们引入了一种全局分布感知的场景特定变分表示学习框架(GSVR)，可直接应用于现有的多场景方法。具体而言，考虑到有限样本带来的不确定性，我们的方法采用概率模型为每个用户和项目在每个场景中生成场景特定分布，通过变分推断(VI)进行估计。此外，我们引入全局知识感知的多项式分布作为先验知识，调节后验用户和项目分布的学习，确保兴趣相似的用户和具有相似辅助信息的项目的分布相似性。这减轻了在稀疏场景中记录较少的用户或项目被淹没的风险。大量的实验结果证实了GSVR在协助现有多场景推荐方法学习更鲁棒表示方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760866&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emergence of e-commerce, the recommendations provided by commercialplatforms must adapt to diverse scenarios to accommodate users' varyingshopping preferences. Current methods typically use a unified framework tooffer personalized recommendations for different scenarios. However, they oftenemploy shared bottom representations, which partially hinders the model'scapacity to capture scenario uniqueness. Ideally, users and items shouldexhibit specific characteristics in different scenarios, prompting the need tolearn scenario-specific representations to differentiate scenarios. Yet,variations in user and item interactions across scenarios lead to data sparsityissues, impeding the acquisition of scenario-specific representations. To learnrobust scenario-specific representations, we introduce a Global-DistributionAware Scenario-Specific Variational Representation Learning Framework (GSVR)that can be directly applied to existing multi-scenario methods. Specifically,considering the uncertainty stemming from limited samples, our approach employsa probabilistic model to generate scenario-specific distributions for each userand item in each scenario, estimated through variational inference (VI).Additionally, we introduce the global knowledge-aware multinomial distributionsas prior knowledge to regulate the learning of the posterior user and itemdistributions, ensuring similarities among distributions for users with akininterests and items with similar side information. This mitigates the risk ofusers or items with fewer records being overwhelmed in sparse scenarios.Extensive experimental results affirm the efficacy of GSVR in assistingexisting multi-scenario recommendation methods in learning more robustrepresentations.</description>
      <author>example@mail.com (Moyu Zhang, Yujun Jin, Jinxin Hu, Yu Zhang)</author>
      <guid isPermaLink="false">2508.14493v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning classification of black holes in the mass--spin diagram</title>
      <link>http://arxiv.org/abs/2508.14316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一个质量-自旋图来分类黑洞和研究它们的形成路径，类似于赫罗图，允许黑洞作为红移的演化轨迹，结合不同黑洞种类的形成、吸积和合并历史。&lt;h4&gt;背景&lt;/h4&gt;缺乏一个类似于赫罗图的黑洞分类系统，使得研究黑洞形成路径变得困难。&lt;h4&gt;目的&lt;/h4&gt;创建一个质量-自旋图，用于分类黑洞并研究它们的形成路径，结合黑洞的形成、吸积和合并历史。&lt;h4&gt;方法&lt;/h4&gt;构建黑洞连续谱，使用二元种群合成软件比较自旋规定，应用监督和无监督机器学习聚类方法进行黑洞种群分类，并使用深度学习通过变分自编码器进行潜在空间表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了可能的黑洞主序列如宇宙吸积和分层合并树；质量-自旋图暴露了建模差异；无监督聚类可恢复典型种群边界；深度学习方法有助于重叠子类的聚类；监督随机森林可准确恢复聚类；半监督方法有发展潜力；无监督分类器性能仍具挑战。&lt;h4&gt;结论&lt;/h4&gt;质量-自旋图可连接引力波和电磁观测与理论模型，促进了机器学习在天文学中的应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了用于分类黑洞和研究其形成路径的质量-自旋图，提供了类似于赫罗图的类比。这使得黑洞的演化轨迹可作为红移的函数，结合各种黑洞种类的形成、吸积和合并历史。从初始质量和自旋函数以及近似红移演化的真实黑洞连续谱揭示了可能的黑洞主序列，如宇宙时间尺度上的持续相干吸积(即宇宙吸积)或分层合并树。在恒星质量范围内，我们使用二元种群合成软件比较了三个关于Wolf-Rayet祖先潮汐演化的自旋规定，展示了质量-自旋图如何暴露有趣的建模差异。然后，我们通过应用监督和无监督机器学习聚类方法对质量-自旋数据集进行黑洞种群分类。虽然无监督聚类几乎可以恢复典型种群边界(恒星质量、中等质量和超大质量)，但更复杂的方法利用深度学习通过变分自编码器进行潜在空间表示学习，有助于对在质量-自旋空间中高度重叠的子类进行真实数据集的聚类。我们发现监督随机森林可以根据底层数据集的复杂性从学习到的潜在空间表示中准确恢复正确的聚类，半监督方法显示出进一步开发的潜力，而无监督分类器的性能是一个重大挑战。我们的研究结果促进了未来机器学习应用，并证明质量-自旋图可用于连接引力波和电磁观测与理论模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the mass--spin diagram for classifying black holes and studyingtheir formation pathways, providing an analogue to the Hertzsprung-Russelldiagram. This allows for black hole evolutionary tracks as a function ofredshift, combining formation, accretion, and merger histories for the varietyof black hole populations. A realistic black hole continuum constructed frominitial mass and spin functions and approximate redshift evolution revealspossible black hole main sequences, such as sustained coherent accretionthrough cosmic time (i.e., Cosmic Accretion) or hierarchical merger trees. Inthe stellar-mass regime, we use a binary population synthesis software tocompare three spin prescriptions for tidal evolution of Wolf-Rayet progenitors,showing how the mass--spin diagram exposes interesting modeling differences. Wethen classify black hole populations by applying supervised and unsupervisedmachine learning clustering methods to mass--spin datasets. While bareunsupervised clustering can nearly recover canonical population boundaries(stellar-mass, intermediate-mass, and supermassive), a more sophisticatedapproach utilizing deep learning via variational autoencoders for latent spacerepresentation learning aids in clustering of realistic datasets withsubclasses that highly overlap in mass--spin space. We find that a supervisedrandom forest can accurately recover the correct clusters from the learnedlatent space representation depending on the complexity of the underlyingdataset, semi-supervised methods show potential for further development, andthe performance of unsupervised classifiers is a great challenge. Our findingsmotivate future machine learning applications and demonstrate that themass--spin diagram can be used to connect gravitational-wave andelectromagnetic observations with theoretical models.</description>
      <author>example@mail.com (Nathan Steinle, Samar Safi-Harb)</author>
      <guid isPermaLink="false">2508.14316v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Constrained Random Phase Approximation: the spectral method</title>
      <link>http://arxiv.org/abs/2508.15368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, to be published in Physical Review B, Poster  presented at PSI-K, Lausanne (August 2025). Talk given at Workshop "The  determination of Hubbard parameters: progress, pitfalls, and prospects",  Gandia (September 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的约束随机相位近似方法(spectral cRPA, s-cRPA)，在多种测试中表现优于现有cRPA方法，能够更准确地预测材料性质，解决了数值稳定性和U值低估问题。&lt;h4&gt;背景&lt;/h4&gt;现有随机相位近似方法在处理Scandium和Copper的3d壳层填充变化时存在局限性，标准cRPA方法低估了Hubbard U相互作用值，projector cRPA方法在充满d壳层时会出现负相互作用值问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的约束RPA方法(s-cRPA)，解决标准cRPA中U值被低估的问题，提高数值稳定性，并开发计算多中心相互作用的功能和低缩放变体。&lt;h4&gt;方法&lt;/h4&gt;提出spectral cRPA方法，在Scandium和Copper上通过变化3d壳层填充进行比较，应用于CaFeO₃系统，使用电子转换提高数值稳定性，开发了计算多中心相互作用的功能和具有压缩Matsubara网格的低缩放变体。&lt;h4&gt;主要发现&lt;/h4&gt;s-cRPA始终产生更大的Hubbard U相互作用值；应用于CaFeO₃系统时，产生的相互作用参数更接近DFT+U中所需的参数，成功诱导实验观察到的绝缘状态；解决了projector cRPA方法在充满d壳层时出现负相互作用值的问题，提供了更好的数值稳定性。&lt;h4&gt;结论&lt;/h4&gt;s-cRPA比标准cRPA更稳健，有效克服了已知的U值低估问题，是一种有前途的研究工具，对研究社区有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的约束随机相位近似方法，称为谱约束随机相位近似(spectral cRPA, s-cRPA)，并通过变化3d壳层填充，将其与已建立的cRPA方法在Scandium和Copper上进行比较。s-cRPA始终产生更大的Hubbard U相互作用值。应用于真实系统CaFeO₃时，s-cRPA产生的相互作用参数显著更接近DFT+U中所需的参数，以诱导实验观察到的绝缘状态，克服了标准密度泛函预测的金属行为。我们还解决了projector cRPA方法在充满d壳层时出现负相互作用值的问题，证明s-cRPA通过电子转换提供了更好的数值稳定性。总体而言，s-cRPA更加稳健，有效克服了标准cRPA中已知的U值低估问题，使其成为研究社区的一种有前途的工具。此外，我们通过增加计算多中心相互作用的功能来增强实现，以分析空间衰减，并开发了一种具有压缩Matsubara网格的低缩放变体，以高效获得全频率依赖相互作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new constrained Random Phase Approximation (cRPA) method, termedspectral cRPA (s-cRPA), and compare it to established cRPA approaches forScandium and Copper by varying the 3d shell filling. The s-cRPA consistentlyyields larger Hubbard $U$ interaction values. Applied to the realistic systemCaFeO$_3$, s-cRPA produces interaction parameters significantly closer to thoserequired within DFT+$U$ to induce the experimentally observed insulating state,overcoming the metallic behavior predicted by standard density functionals. Wealso address the issue of negative interaction values found in the projectorcRPA method for filled d-shells, demonstrating that s-cRPA offers superiornumerical stability through electron conversion. Overall, s-cRPA is more robustand effectively overcomes the known underestimation of $U$ in standard cRPA,making it a promising tool for the community. Additionally, we have enhancedour implementation with features for computing multi-center interactions toanalyze spatial decay and developed a low-scaling variant with a compressedMatsubara grid to efficiently obtain full frequency-dependent interactions.</description>
      <author>example@mail.com (Merzuk Kaltak, Alexander Hampel, Martin Schlipf, Indukuru Ramesh Reddy, Bongae Kim, Georg Kresse)</author>
      <guid isPermaLink="false">2508.15368v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features</title>
      <link>http://arxiv.org/abs/2508.15353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Optical Memory and Neural Networks, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RCDINO，一种基于多模态变换器的三维物体检测模型，通过融合DINOv2基础模型的语义丰富表示来增强视觉特征，在nuScenes数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;三维物体检测对自动驾驶和机器人技术至关重要，需要有效融合来自摄像头和雷达的多模态数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于多模态变换器的模型(RCDINO)，通过融合预训练的DINOv2基础模型提供的语义丰富表示来增强视觉骨干特征。&lt;h4&gt;方法&lt;/h4&gt;RCDINO是一种基于多模态变换器的模型，它将视觉骨干特征与DINOv2基础模型提供的语义丰富表示融合，从而丰富视觉表示，同时保持与基线架构的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，RCDINO在雷达-摄像头模型中达到了最先进的性能，NDS为56.4，mAP为48.1。&lt;h4&gt;结论&lt;/h4&gt;RCDINO通过融合DINOv2基础模型的语义丰富表示，有效提升了三维物体检测的性能，同时保持了与基线架构的兼容性。&lt;h4&gt;翻译&lt;/h4&gt;三维物体检测对自动驾驶和机器人技术至关重要，依赖于有效融合来自摄像头和雷达的多模态数据。本文提出了RCDINO，一种基于多模态变换器的模型，通过融合预训练的DINOv2基础模型提供的语义丰富表示来增强视觉骨干特征。这种方法丰富了视觉表示，提高了模型的检测性能，同时保持了与基线架构的兼容性。在nuScenes数据集上的实验表明，RCDINO在雷达-摄像头模型中达到了最先进的性能，NDS为56.4，mAP为48.1。我们的实现在https://github.com/OlgaMatykina/RCDINO可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何增强雷达-相机融合的3D目标检测性能的问题。这个问题在自动驾驶和机器人领域非常重要，因为3D目标检测是安全有效环境感知的核心任务。相机提供高分辨率图像但缺乏深度信息，而雷达提供可靠的距离和速度信息但空间分辨率低且语义细节不足。有效融合这两种互补数据是持续挑战，现有方法常因视觉特征语义丰富度有限而难以准确检测远处物体，这对可靠自主感知至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有雷达-相机融合模型中视觉特征语义丰富度有限，难以检测远处物体。他们考虑引入预训练DINOv2基础模型的语义特征来增强视觉特征，同时保持与基线架构的兼容性。设计了一个轻量级适配器模块，将DINOv2表示整合到检测流程中。该方法借鉴了RCTrans架构的查询机制和顺序解码器设计，受BEVCar使用DINOv2与适配器的启发，并采用了Futr3D的基于支柱的雷达处理方法和类似U-Net的密集编码器结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入预训练DINOv2模型的语义丰富特征来增强视觉特征，提升对远处和语义复杂物体的检测能力。整体流程包括：1)视觉编码器提取图像特征；2)DINOv2适配器处理并融合DINOv2语义特征；3)雷达稀疏编码器处理雷达点数据；4)雷达密集编码器处理雷达信号稀疏性；5)顺序Transformer解码器融合多模态数据。DINOv2适配器通过注入器将特征注入DINOv2中间层，经过推理后由提取器检索修改后的特征，最后与原始主干特征融合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入RCDINO模型，通过轻量级适配器整合DINOv2语义特征；2)提供解码器组件独立于特征信息量丰富度的消融研究；3)在公共基准和真实数据上验证方法；4)开源实现促进研究。相比之前工作，不同之处在于：不是简单拼接或使用注意力机制融合特征，而是通过DINOv2适配器将语义丰富特征注入视觉主干；不是直接使用DINOv2作为独立编码器，而是作为视觉特征增强器；采用两阶段解码器设计，即使一个模态特征信息量较少也能提高性能；在nuScenes数据集上实现了最先进的雷达-相机模型性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RCDINO通过引入轻量级DINOv2适配器模块增强视觉特征语义表示，显著提升了雷达-相机融合的3D目标检测性能，特别是在检测远处和语义复杂物体方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional object detection is essential for autonomous driving androbotics, relying on effective fusion of multimodal data from cameras andradar. This work proposes RCDINO, a multimodal transformer-based model thatenhances visual backbone features by fusing them with semantically richrepresentations from the pretrained DINOv2 foundation model. This approachenriches visual representations and improves the model's detection performancewhile preserving compatibility with the baseline architecture. Experiments onthe nuScenes dataset demonstrate that RCDINO achieves state-of-the-artperformance among radar-camera models, with 56.4 NDS and 48.1 mAP. Ourimplementation is available at https://github.com/OlgaMatykina/RCDINO.</description>
      <author>example@mail.com (Olga Matykina, Dmitry Yudin)</author>
      <guid isPermaLink="false">2508.15353v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Tree-like Pairwise Interaction Networks</title>
      <link>http://arxiv.org/abs/2508.15678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为树状成对交互网络(PIN)的新型神经网络架构，用于表格数据中的特征交互建模，特别是在保险定价等预测建模任务中。PIN通过模拟决策树结构的共享前馈神经网络架构明确捕捉特征间的成对交互，具有内在可解释性，并能高效计算SHAP值。实证研究表明，PIN在预测准确性上优于传统和现代神经网络基准，同时提供了特征交互和预测贡献的深入见解。&lt;h4&gt;背景&lt;/h4&gt;表格数据中的特征交互建模是预测建模中的一个关键挑战，例如在保险定价等应用中。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型神经网络架构，能够明确捕捉特征间的交互作用，并具有良好的可解释性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出树状成对交互网络(PIN)，这是一种通过共享前馈神经网络架构模拟决策树结构的新型神经网络，能够明确捕捉特征间的成对交互。&lt;h4&gt;主要发现&lt;/h4&gt;1) PIN设计上具有内在可解释性，可直接检查交互效应；2) 由于只涉及成对交互，可高效计算SHAP值；3) PIN与GA2Ms、梯度提升机和图神经网络等已建立的模型有联系；4) 在法国汽车保险数据集上，PIN在预测准确性上优于传统和现代神经网络基准；5) PIN提供了特征如何相互交互及贡献于预测的见解。&lt;h4&gt;结论&lt;/h4&gt;PIN在预测性能和可解释性方面都表现出色，是一种有效的特征交互建模方法。&lt;h4&gt;翻译&lt;/h4&gt;表格数据中的特征交互建模仍然是预测建模中的一个关键挑战，例如在保险定价中的应用。本文提出了一种名为树状成对交互网络(PIN)的新型神经网络架构，它通过模拟决策树结构的共享前馈神经网络架构明确捕捉特征间的成对交互。PIN设计上具有内在可解释性，允许直接检查交互效应。此外，由于只涉及成对交互，它允许高效计算SHAP值。我们强调了PIN与GA2Ms、梯度提升机和图神经网络等已建立模型之间的联系。在流行的法国汽车保险数据集上的实证结果表明，PIN在预测准确性上优于传统和现代神经网络基准，同时提供了关于特征如何相互交互以及它们如何贡献于预测的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling feature interactions in tabular data remains a key challenge inpredictive modeling, for example, as used for insurance pricing. This paperproposes the Tree-like Pairwise Interaction Network (PIN), a novel neuralnetwork architecture that explicitly captures pairwise feature interactionsthrough a shared feed-forward neural network architecture that mimics thestructure of decision trees. PIN enables intrinsic interpretability by design,allowing for direct inspection of interaction effects. Moreover, it allows forefficient SHapley's Additive exPlanation (SHAP) computations because it onlyinvolves pairwise interactions. We highlight connections between PIN andestablished models such as GA2Ms, gradient boosting machines, and graph neuralnetworks. Empirical results on the popular French motor insurance dataset showthat PIN outperforms both traditional and modern neural networks benchmarks inpredictive accuracy, while also providing insight into how features interactwith each another and how they contribute to the predictions.</description>
      <author>example@mail.com (Ronald Richman, Salvatore Scognamiglio, Mario V. Wüthrich)</author>
      <guid isPermaLink="false">2508.15678v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Predictive models for strain energy in condensed phase reactions</title>
      <link>http://arxiv.org/abs/2508.15592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种图神经网络模型，用于预测凝聚相中环化反应的应变能，从而更准确地模拟分子材料的热激活化学过程。&lt;h4&gt;背景&lt;/h4&gt;当前方法结合分子动力学模拟和随机化学反应描述，但基于几何标准选择可能反应的方法常导致显著的分子应变和不真实的结构。&lt;h4&gt;目的&lt;/h4&gt;探究反应位点周围局部分子环境对分子应变能量和反应速率的影响，并开发能预测环化反应应变能的图神经网络模型。&lt;h4&gt;方法&lt;/h4&gt;开发图神经网络模型，在从MD模拟获得的聚丙烯腈激活期间凝聚相反应的大型数据集上进行训练，用于预测环化反应的应变能。&lt;h4&gt;主要发现&lt;/h4&gt;反应位点周围的局部分子环境在决定分子应变能量和相关反应速率方面起着关键作用。&lt;h4&gt;结论&lt;/h4&gt;所开发的图神经网络模型可用于调整凝聚系统中的相对反应速率，增进对复杂材料中热激活化学过程的理解。&lt;h4&gt;翻译&lt;/h4&gt;凝聚相中热激活化学的分子建模对于理解聚合物、解聚和其他分子材料的加工步骤至关重要。当前方法通常将分子动力学模拟与对预定化学反应的随机描述相结合。可能的反应通常基于几何标准（如反应原子之间的捕获距离）进行选择。尽管这些模拟提供了有价值的见解，但用于确定可能反应的近似方法常常导致显著的分子应变和不真实的结构。我们表明，反应位点周围的局部分子环境在决定所得分子应变能量和相关反应速率方面起着关键作用。我们开发了一种图神经网络，能够根据反应位点周围的预反应、局部分子环境预测环化反应相关的应变能。该模型在从MD模拟中获得的聚丙烯腈激活期间凝聚相反应的大型数据集上进行训练，可用于调整凝聚系统中的相对反应速率，并推进我们对复杂材料中热激活化学过程的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular modeling of thermally activated chemistry in condensed phases isessential to understand polymerization, depolymerization, and other processingsteps of molecular materials. Current methods typically combine moleculardynamics (MD) simulations to describe short-time relaxation with a stochasticdescription of predetermined chemical reactions. Possible reactions are oftenselected on the basis of geometric criteria, such as a capture distance betweenreactive atoms. Although these simulations have provided valuable insight, theapproximations used to determine possible reactions often lead to significantmolecular strain and unrealistic structures. We show that the local molecularenvironment surrounding the reactive site plays a crucial role in determiningthe resulting molecular strain energy and, in turn, the associated reactionrates. We develop a graph neural network capable of predicting the strainenergy associated with a cyclization reaction from the pre-reaction, local,molecular environment surrounding the reactive site. The model is trained on alarge dataset of condensed-phase reactions during the activation ofpolyacrylonitrile (PAN) obtained from MD simulations and can be used to adjustrelative reaction rates in condensed systems and advance our understanding ofthermally activated chemical processes in complex materials</description>
      <author>example@mail.com (Baptiste Martin, Shukai Yao, Chunyu Li, Anthony Bocahut, Matthew Jackson, Alejandro Strachan)</author>
      <guid isPermaLink="false">2508.15592v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New Links</title>
      <link>http://arxiv.org/abs/2508.15499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FairGuide的新框架，通过引入新链接来引导图神经网络结构向公平性方向发展，解决因图结构偏见导致的公平性问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在多种应用中取得显著成功，但由于图结构中的偏见，在公平性方面面临重大挑战。原始用户图结构通常存在偏见，但通过引入新链接，可以将这些现有结构引导向无偏见的方向。&lt;h4&gt;目的&lt;/h4&gt;提出FairGuide框架，通过引入新链接来指导图结构向公平性方向发展，从而增强下游应用的公平性。&lt;h4&gt;方法&lt;/h4&gt;引入可微的社区检测任务作为伪下游任务，确保在公平性引导的图上训练的下游任务的公平性；使用来自公平性引导目标的元梯度来识别显著增强结构公平性的新链接；理论分析表明优化伪任务中的公平性可以有效增强结构公平性，促进各种下游应用的公平性泛化。&lt;h4&gt;主要发现&lt;/h4&gt;通过公平性引导的新链接可以促进无偏见的社区形成；优化伪任务中的公平性可以增强结构公平性；所提出的方法在各种基于图的公平性任务中具有有效性和可泛化性。&lt;h4&gt;结论&lt;/h4&gt;FairGuide框架通过引入新链接来引导图结构向公平性方向发展，并通过理论分析和实验验证了其有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）在多种应用中取得了显著成功。然而，由于图结构中的偏见，图神经网络在公平性方面面临重大挑战。尽管原始用户图结构通常存在偏见，但通过引入新链接，将这些现有结构引导向无偏见的方向是很有前景的。通过新链接的公平性引导可以促进无偏见的社区形成，从而增强下游应用的公平性。为解决这一问题，我们提出了一种名为FairGuide的新框架。具体来说，为确保在公平性引导的图上训练的下游任务的公平性，我们引入了一个可微的社区检测任务作为伪下游任务。我们的理论分析进一步证明，在伪任务中优化公平性可以有效增强结构公平性，促进各种下游应用的公平性泛化。此外，FairGuide采用了一种有效策略，利用从公平性引导目标中衍生的元梯度来识别显著增强结构公平性的新链接。大量的实验结果证明了我们提出的方法在各种基于图的公平性任务中的有效性和可泛化性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved remarkable success across diverseapplications. However, due to the biases in the graph structures, graph neuralnetworks face significant challenges in fairness. Although the original usergraph structure is generally biased, it is promising to guide these existingstructures toward unbiased ones by introducing new links. The fairness guidancevia new links could foster unbiased communities, thereby enhancing fairness indownstream applications. To address this issue, we propose a novel frameworknamed FairGuide. Specifically, to ensure fairness in downstream tasks trainedon fairness-guided graphs, we introduce a differentiable community detectiontask as a pseudo downstream task. Our theoretical analysis further demonstratesthat optimizing fairness within this pseudo task effectively enhancesstructural fairness, promoting fairness generalization across diversedownstream applications. Moreover, FairGuide employs an effective strategywhich leverages meta-gradients derived from the fairness-guidance objective toidentify new links that significantly enhance structural fairness. Extensiveexperimental results demonstrate the effectiveness and generalizability of ourproposed method across a variety of graph-based fairness tasks.</description>
      <author>example@mail.com (Jiahua Lu, Huaxiao Liu, Shuotong Bai, Junjie Xu, Renqiang Luo, Enyan Dai)</author>
      <guid isPermaLink="false">2508.15499v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs</title>
      <link>http://arxiv.org/abs/2508.15468v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为JEDI-linear的新型图神经网络架构，具有线性计算复杂性和硬件高效性，解决了在FPGA硬件触发系统上部署GNN的挑战，实现了低于60纳秒的延迟，满足了HL-LHC CMS一级触发系统的要求。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)特别是交互网络(INs)在CERN高亮度大型强子对撞机(HL-LHC)的射流标记任务中表现出色，但其计算复杂性和不规则内存访问模式在FPGA硬件触发系统上部署时面临重大挑战，因为系统有严格的延迟和资源限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种具有线性计算复杂性的新型GNN架构，能够在FPGA硬件触发系统上高效部署，满足严格的延迟和资源约束，同时保持高模型精度。&lt;h4&gt;方法&lt;/h4&gt;提出JEDI-linear架构，通过利用共享变换和全局聚合消除显式的成对交互；引入细粒度量化感知训练，具有每参数位宽优化；通过分布式算术采用无乘法器的乘法累加操作。&lt;h4&gt;主要发现&lt;/h4&gt;基于FPGA的JEDI-linear与最先进的设计相比，实现了3.7至11.5倍的更低延迟，高达150倍的更低启动间隔，以及高达6.2倍的更低LUT使用率，同时提供更高的模型精度，完全消除了对DSP块的需求（最先进解决方案消耗超过8,700个DSP）。&lt;h4&gt;结论&lt;/h4&gt;JEDI-linear是第一个基于交互的GNN实现低于60纳秒延迟，满足HL-LHC CMS一级触发系统的使用要求，通过在实时环境中实现准确、可扩展和资源高效的GNN推理，推动了下一代触发系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)，特别是交互网络(INs)，在CERN高亮度大型强子对撞机(HL-LHC)的射流标记任务中表现出色。然而，它们的计算复杂性和不规则内存访问模式在FPGA硬件触发系统上的部署提出了重大挑战，该系统有严格的延迟和资源限制。在这项工作中，我们提出了JEDI-linear，一种具有线性计算复杂性的新型GNN架构，通过利用共享变换和全局聚合消除了显式的成对交互。为了进一步提高硬件效率，我们引入了细粒度量化感知训练，具有每参数位宽优化，并通过分布式算术采用无乘法器的乘法累加操作。评估结果表明，我们的基于FPGA的JEDI-linear与最先进的设计相比，实现了3.7至11.5倍的更低延迟，高达150倍的更低启动间隔，以及高达6.2倍的更低LUT使用率，同时提供更高的模型精度，并完全消除了对DSP块的需求的需求。相比之下，最先进的解决方案消耗超过8,700个DSP。这是第一个基于交互的GNN实现低于60纳秒延迟，目前满足HL-LHC CMS一级触发系统的使用要求。这项工作通过在实时环境中实现准确、可扩展和资源高效的GNN推理，推动了下一代触发系统的发展。我们的开源模板将进一步支持可重现性和在科学应用中的更广泛采用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs), particularly Interaction Networks (INs), haveshown exceptional performance for jet tagging at the CERN High-Luminosity LargeHadron Collider (HL-LHC). However, their computational complexity and irregularmemory access patterns pose significant challenges for deployment on FPGAs inhardware trigger systems, where strict latency and resource constraints apply.In this work, we propose JEDI-linear, a novel GNN architecture with linearcomputational complexity that eliminates explicit pairwise interactions byleveraging shared transformations and global aggregation. To further enhancehardware efficiency, we introduce fine-grained quantization-aware training withper-parameter bitwidth optimization and employ multiplier-freemultiply-accumulate operations via distributed arithmetic. Evaluation resultsshow that our FPGA-based JEDI-linear achieves 3.7 to 11.5 times lower latency,up to 150 times lower initiation interval, and up to 6.2 times lower LUT usagecompared to state-of-the-art designs while also delivering higher modelaccuracy and eliminating the need for DSP blocks entirely. In contrast,state-of-the-art solutions consume over 8,700 DSPs. This is the firstinteraction-based GNN to achieve less than 60~ns latency and currently meetsthe requirements for use in the HL-LHC CMS Level-1 trigger system. This workadvances the next-generation trigger systems by enabling accurate, scalable,and resource-efficient GNN inference in real-time environments. Ouropen-sourced templates will further support reproducibility and broaderadoption across scientific applications.</description>
      <author>example@mail.com (Zhiqiang Que, Chang Sun, Sudarshan Paramesvaran, Emyr Clement, Katerina Karakoulaki, Christopher Brown, Lauri Laatu, Arianna Cox, Alexander Tapper, Wayne Luk, Maria Spiropulu)</author>
      <guid isPermaLink="false">2508.15468v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation</title>
      <link>http://arxiv.org/abs/2508.15216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为STAGNet的模型，用于从车载摄像头视频中预测交通事故，通过结合时空特征和循环网络聚合，在三个公开数据集上实现了比先前方法更高的预测精度和碰撞时间预警效果。&lt;h4&gt;背景&lt;/h4&gt;事故预测和及时警告对提高道路安全、减少人员伤害和财产损失至关重要。现有高级驾驶辅助系统多依赖LiDAR、雷达和GPS等多种传感器，而仅使用车载摄像头视频输入更具挑战性但成本更低且更易于部署。&lt;h4&gt;目的&lt;/h4&gt;改进基于车载摄像头视频输入的事故预测方法，提高预测的准确性和及时性。&lt;h4&gt;方法&lt;/h4&gt;作者融入了更优的时空特征，并通过循环网络对这些特征进行聚合，以改进现有的图神经网络模型，提出了STAGNet模型用于事故预测。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开数据集上的实验表明，STAGNet模型在交叉验证以及跨数据集训练测试中，均实现了比先前方法更高的平均精度和平均碰撞时间值。&lt;h4&gt;结论&lt;/h4&gt;STAGNet模型在事故预测任务上表现优于现有方法，证明了仅依靠车载摄像头视频进行事故预测的可行性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;事故预测和及时警告在通过减少道路使用者受伤风险和最小化财产损失来提高道路安全方面起着关键作用。高级驾驶辅助系统(ADAS)旨在支持人类驾驶员，特别是在他们能够预见潜在事故发生时。虽然许多现有系统依赖于LiDAR、雷达和GPS等多种传感器，但仅依靠车载摄像头视频输入则是一个更具挑战性但成本更低且更易于部署的解决方案。在这项工作中，我们融入了更好的时空特征，并通过循环网络对这些特征进行聚合，以改进现有的图神经网络用于从车载摄像头视频中预测事故。使用三个公开数据集进行的实验表明，我们提出的STAGNet模型在给定数据集上的交叉验证以及在不同数据集上进行训练和测试时，都比先前方法实现了更高的平均精度和平均碰撞时间值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accident prediction and timely warnings play a key role in improving roadsafety by reducing the risk of injury to road users and minimizing propertydamage. Advanced Driver Assistance Systems (ADAS) are designed to support humandrivers and are especially useful when they can anticipate potential accidentsbefore they happen. While many existing systems depend on a range of sensorssuch as LiDAR, radar, and GPS, relying solely on dash-cam video input presentsa more challenging but a more cost-effective and easily deployable solution. Inthis work, we incorporate better spatio-temporal features and aggregate themthrough a recurrent network to improve upon state-of-the-art graph neuralnetworks for predicting accidents from dash-cam videos. Experiments using threepublicly available datasets show that our proposed STAGNet model achieveshigher average precision and mean time-to-collision values than previousmethods, both when cross-validated on a given dataset and when trained andtested on different datasets.</description>
      <author>example@mail.com (Vipooshan Vipulananthan, Kumudu Mohottala, Kavindu Chinthana, Nimsara Paramulla, Charith D Chitraranjan)</author>
      <guid isPermaLink="false">2508.15216v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Equivariant Electronic Hamiltonian Prediction with Many-Body Message Passing</title>
      <link>http://arxiv.org/abs/2508.15108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MACE-H图神经网络模型作为Kohn-Sham密度泛函理论哈密顿量的机器学习替代方案，能够高效预测材料的电子属性，如电子能带结构和态密度，在保持高精度的同时实现了计算效率。&lt;h4&gt;背景&lt;/h4&gt;机器学习替代Kohn-Sham密度泛函理论哈密顿量是预测材料电子属性的有力工具，但对于大规模应用，需要兼具高泛化能力和计算效率的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效获取所有相关O(3)不可约表示，同时捕捉完整局部化学环境特征的模型，用于材料电子属性的准确预测。&lt;h4&gt;方法&lt;/h4&gt;引入MACE-H图神经网络，结合高体阶消息传递和节点阶展开，能够处理到f轨道矩阵相互作用块，捕捉完整的局部化学环境特征。&lt;h4&gt;主要发现&lt;/h4&gt;模型在二维材料和块体金的公开基准数据集上展示了高精度和可转移性，实现了亚毫电子伏特量级的矩阵元素和特征值预测误差；高体阶消息传递与局部性的相互作用使该模型成为高通量材料筛选的有力候选。&lt;h4&gt;结论&lt;/h4&gt;MACE-H图神经网络模型在保持高精度的同时实现了计算效率，能够捕捉完整的局部化学环境特征，适合用于高通量材料筛选应用。&lt;h4&gt;翻译&lt;/h4&gt;机器学习替代Kohn-Sham密度泛函理论哈密顿量为加速材料电子属性（如电子能带结构和态密度）的预测提供了强大工具。对于大规模应用，理想模型应具有高泛化能力和计算效率。本文介绍了MACE-H图神经网络，它结合高体阶消息传递和节点阶展开，高效获取所有相关的O(3)不可约表示。该模型实现了高精度和计算效率，并捕捉了完整的局部化学环境特征，目前可处理到f轨道矩阵相互作用块。我们在几个二维材料的公开材料基准数据集和一个新的块体金数据集上验证了模型的准确性和可转移性，在所有系统中实现了亚毫电子伏特量级的矩阵元素和特征值预测误差。我们进一步分析了高体阶消息传递与局部性的相互作用，这使该模型成为高通量材料筛选的有力候选。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning surrogates of Kohn-Sham Density Functional TheoryHamiltonians offer a powerful tool to accelerate the prediction of electronicproperties of materials, such as electronic band structures anddensities-of-states. For large-scale applications, an ideal model would exhibithigh generalization ability and computational efficiency. Here, we introducethe MACE-H graph neural network, which combines high body-order message passingwith a node-order expansion to efficiently obtain all relevant $O(3)$irreducible representations. The model achieves high accuracy and computationalefficiency and captures the full local chemical environment features of,currently, up to $f$ orbital matrix interaction blocks. We demonstrate themodel's accuracy and transferability on several open materials benchmarkdatasets of two-dimensional materials and a new dataset for bulk gold,achieving sub-meV prediction errors on matrix elements and eigenvalues acrossall systems. We further analyse the interplay of high body order messagepassing and locality that makes this model a good candidate for high-throughputmaterial screening.</description>
      <author>example@mail.com (Chen Qian, Valdas Vitartas, James Kermode, Reinhard J. Maurer)</author>
      <guid isPermaLink="false">2508.15108v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition and Contribution Analysis</title>
      <link>http://arxiv.org/abs/2508.15015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SEAL是一种新的可解释图神经网络，通过将分子图分解为化学相关片段并减少片段间消息传递，实现了对分子性质预测的可靠解释，在定量指标和人类对齐的可解释性方面优于其他方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在预测分子性质方面表现出色，但其黑盒性质降低了可解释性，限制了在药物发现和材料设计等重要应用中对预测结果的信任。&lt;h4&gt;目的&lt;/h4&gt;开发一种可解释的图神经网络方法，能够可靠地量化分子中单个原子或子结构对模型预测的贡献。&lt;h4&gt;方法&lt;/h4&gt;SEAL（通过归因学习的子结构解释）将输入图分解为化学相关的片段，估计它们对输出的因果影响，并通过减少片段间消息传递实现片段贡献与模型预测的强对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在合成基准和真实世界分子数据集上的评估表明，SEAL在定量归因指标和人类对齐的可解释性方面都优于其他可解释性方法；用户研究确认SEAL为领域专家提供了更直观和可信的解释。&lt;h4&gt;结论&lt;/h4&gt;SEAL通过弥合预测性能和可解释性之间的差距，为更透明和可操作的分子建模提供了有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过利用分子图中编码的丰富结构信息，在预测分子性质方面取得了显著成功。然而，它们的黑盒性质降低了可解释性，这限制了在药物发现和材料设计等重要应用中对预测结果的信任。此外，由于纠缠的消息传递动态，现有的解释技术往往无法可靠地量化单个原子或子结构的贡献。我们引入了SEAL（通过归因学习的子结构解释），这是一种新的可解释图神经网络，它将模型预测归因于有意义的分子子图。SEAL将输入图分解为化学相关的片段，并估计它们对输出的因果影响。通过在我们提出的模型架构中明确减少片段间的消息传递，实现了片段贡献与模型预测之间的强对齐。在合成基准和真实世界分子数据集上的广泛评估表明，SEAL在定量归因指标和人类对齐的可解释性方面都优于其他可解释性方法。用户研究进一步确认SEAL为领域专家提供了更直观和可信的解释。通过弥合预测性能和可解释性之间的差距，SEAL为更透明和可操作的分子建模提供了有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have demonstrated remarkable success in predictingmolecular properties by leveraging the rich structural information encoded inmolecular graphs. However, their black-box nature reduces interpretability,which limits trust in their predictions for important applications such as drugdiscovery and materials design. Furthermore, existing explanation techniquesoften fail to reliably quantify the contribution of individual atoms orsubstructures due to the entangled message-passing dynamics. We introduce SEAL(Substructure Explanation via Attribution Learning), a new interpretable graphneural network that attributes model predictions to meaningful molecularsubgraphs. SEAL decomposes input graphs into chemically relevant fragments andestimates their causal influence on the output. The strong alignment betweenfragment contributions and model predictions is achieved by explicitly reducinginter-fragment message passing in our proposed model architecture. Extensiveevaluations on synthetic benchmarks and real-world molecular datasetsdemonstrate that SEAL outperforms other explainability methods in bothquantitative attribution metrics and human-aligned interpretability. A userstudy further confirms that SEAL provides more intuitive and trustworthyexplanations to domain experts. By bridging the gap between predictiveperformance and interpretability, SEAL offers a promising direction for moretransparent and actionable molecular modeling.</description>
      <author>example@mail.com (Sebastian Musiał, Bartosz Zieliński, Tomasz Danel)</author>
      <guid isPermaLink="false">2508.15015v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Fast Graph Neural Network for Image Classification</title>
      <link>http://arxiv.org/abs/2508.14958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, proceeding into CanadianAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合图卷积网络(GCNs)和Voronoi图的新型图像分类方法，通过将图像表示为图结构并使用Delaunay三角剖分优化表示，显著提高了图像分类的预处理效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;图像分类的快速发展主要得益于图卷积网络(GCNs)的应用，它们为处理复杂数据结构提供了强大的框架。然而，传统的卷积神经网络(CNNs)在处理某些复杂场景和细粒度分类时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过整合GCNs与Voronoi图，开发一种更有效的图像分类方法，以提高预处理效率和分类准确性，特别是在复杂场景和细粒度类别的挑战性场景中。&lt;h4&gt;方法&lt;/h4&gt;该方法将图像表示为图结构，其中像素或区域作为顶点，然后使用相应的Delaunay三角剖分来优化图的表示。结合GCNs处理这种图结构数据，实现对图像的有效分类。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在各种基准数据集上实现了预处理效率和分类准确性的显著改进，超越了最先进的方法，特别是在涉及复杂场景和细粒度类别的挑战性场景中表现优异。&lt;h4&gt;结论&lt;/h4&gt;该研究不仅为图像分类提供了新的视角，还扩展了基于图的学习范式在计算机视觉和非结构化数据分析中的潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;图像分类的快速发展在很大程度上得益于图卷积网络(GCNs)的采用，它们为处理复杂数据结构提供了强大的框架。本研究引入了一种新方法，将GCNs与Voronoi图相结合，通过利用它们有效建模关系数据的能力来增强图像分类。与传统的卷积神经网络(CNNs)不同，我们的方法将图像表示为图，其中像素或区域作为顶点。然后使用相应的Delaunay三角剖分对这些图进行优化，改进其表示。所提出的模型在各种基准数据集上实现了预处理效率和分类准确性的显著提高，超越了最先进的方法，特别是在涉及复杂场景和细粒度类别的挑战性场景中。通过交叉验证验证的实验结果，强调了结合GCNs与Voronoi图对推进图像分类的有效性。这项研究不仅为图像分类提供了新的视角，还扩展了基于图的学习范式在计算机视觉和非结构化数据分析中的潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid progress in image classification has been largely driven by theadoption of Graph Convolutional Networks (GCNs), which offer a robust frameworkfor handling complex data structures. This study introduces a novel approachthat integrates GCNs with Voronoi diagrams to enhance image classification byleveraging their ability to effectively model relational data. Unlikeconventional convolutional neural networks (CNNs), our method represents imagesas graphs, where pixels or regions function as vertices. These graphs are thenrefined using corresponding Delaunay triangulations, optimizing theirrepresentation. The proposed model achieves significant improvements in bothpreprocessing efficiency and classification accuracy across various benchmarkdatasets, surpassing state-of-the-art approaches, particularly in challengingscenarios involving intricate scenes and fine-grained categories. Experimentalresults, validated through cross-validation, underscore the effectiveness ofcombining GCNs with Voronoi diagrams for advancing image classification. Thisresearch not only presents a novel perspective on image classification but alsoexpands the potential applications of graph-based learning paradigms incomputer vision and unstructured data analysis.</description>
      <author>example@mail.com (Mustafa Mohammadi Gharasuie, Luis Rueda)</author>
      <guid isPermaLink="false">2508.14958v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Structure-Aware Temporal Modeling for Chronic Disease Progression Prediction</title>
      <link>http://arxiv.org/abs/2508.14942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一预测框架，整合结构感知和时间建模，用于帕金森病进展预测。该方法利用图神经网络建模临床症状间的结构关系，引入基于图的表示捕捉症状语义依赖，采用Transformer架构建模动态时间特征，并通过结构感知门控机制融合结构信息与时间特征。实验表明该方法在AUC、RMSE和IPW-F1指标上优于现有方法，能有效区分疾病进展阶段并捕捉个性化症状轨迹。&lt;h4&gt;背景&lt;/h4&gt;帕金森病进展预测面临症状演变复杂性和时间依赖性建模不足的挑战，需要更有效的预测方法来支持临床决策。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的预测框架，整合结构感知和时间建模，提高帕金森病进展预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络建模多模态临床症状间的结构关系，引入基于图的表示捕捉症状语义依赖，采用Transformer架构建模动态时间特征，设计结构感知门控机制融合结构编码和时间特征，构建包含图构建模块、时间编码模块和预测输出层的多组件建模管道。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在AUC、RMSE和IPW-F1指标上优于现有方法，能有效区分疾病进展阶段，提高模型捕捉个性化症状轨迹的能力，框架具有强大的泛化能力和结构可扩展性。&lt;h4&gt;结论&lt;/h4&gt;整体框架为帕金森病等慢性进展性疾病的智能建模提供了可靠支持，具有临床应用价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了帕金森病进展预测中症状演变复杂性和时间依赖性建模不足的挑战。它提出了一种统一预测框架，整合结构感知和时间建模。该方法利用图神经网络建模多模态临床症状之间的结构关系，引入基于图的表示来捕捉症状之间的语义依赖。它还采用Transformer架构来建模疾病进展过程中的动态时间特征。为了融合结构信息和时间信息，设计了一种结构感知的门控机制，用于动态调整结构编码和时间特征之间的融合权重，增强模型识别关键进展阶段的能力。为了提高分类准确性和稳定性，该框架包含一个多组件建模管道，由图构建模块、时间编码模块和预测输出层组成。模型在真实纵向帕金森病数据上进行了评估。实验涉及与主流模型的比较、超参数的敏感性分析和图连接密度控制。结果表明，所提出的方法在AUC、RMSE和IPW-F1指标上优于现有方法。它能有效区分进展阶段，提高模型捕捉个性化症状轨迹的能力。整体框架展示了强大的泛化能力和结构可扩展性，为帕金森病等慢性进展性疾病的智能建模提供了可靠支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the challenges of symptom evolution complexity andinsufficient temporal dependency modeling in Parkinson's disease progressionprediction. It proposes a unified prediction framework that integratesstructural perception and temporal modeling. The method leverages graph neuralnetworks to model the structural relationships among multimodal clinicalsymptoms and introduces graph-based representations to capture semanticdependencies between symptoms. It also incorporates a Transformer architectureto model dynamic temporal features during disease progression. To fusestructural and temporal information, a structure-aware gating mechanism isdesigned to dynamically adjust the fusion weights between structural encodingsand temporal features, enhancing the model's ability to identify keyprogression stages. To improve classification accuracy and stability, theframework includes a multi-component modeling pipeline, consisting of a graphconstruction module, a temporal encoding module, and a prediction output layer.The model is evaluated on real-world longitudinal Parkinson's disease data. Theexperiments involve comparisons with mainstream models, sensitivity analysis ofhyperparameters, and graph connection density control. Results show that theproposed method outperforms existing approaches in AUC, RMSE, and IPW-F1metrics. It effectively distinguishes progression stages and improves themodel's ability to capture personalized symptom trajectories. The overallframework demonstrates strong generalization and structural scalability,providing reliable support for intelligent modeling of chronic progressivediseases such as Parkinson's disease.</description>
      <author>example@mail.com (Jiacheng Hu, Bo Zhang, Ting Xu, Haifeng Yang, Min Gao)</author>
      <guid isPermaLink="false">2508.14942v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title>
      <link>http://arxiv.org/abs/2508.15717v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出StreamMem，一种用于流式视频理解的查询无关KV缓存内存机制，有效解决了多模态大语言模型处理长视频时的内存和计算开销问题。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉-语言推理方面取得进展，但处理长视频能力有限。长上下文MLLMs存储和处理长视觉上下文KV缓存带来大量内存和计算开销。现有视觉压缩方法要么需提前编码整个视觉上下文，要么需提前访问问题，不适用于长视频理解和多轮对话场景。&lt;h4&gt;目的&lt;/h4&gt;提出一种查询无关的KV缓存内存机制，用于流式视频理解，无需提前知道问题即可有效压缩长视频的KV缓存。&lt;h4&gt;方法&lt;/h4&gt;StreamMem以流式方式编码新视频帧，利用视觉标记和通用查询标记之间的注意力分数压缩KV缓存，同时保持固定大小的KV内存，使模型能在内存受限的长视频场景中进行高效问答。&lt;h4&gt;主要发现&lt;/h4&gt;在三个长视频理解和两个流式视频问答基准测试上，StreamMem在查询无关KV缓存压缩方面实现了最先进性能，且与查询感知压缩方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;StreamMem有效解决了长视频理解中KV缓存压缩问题，适用于流式视频理解和多轮对话场景，无需提前访问问题。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在视觉-语言推理方面取得了显著进展，但它们有效处理长视频的能力仍然有限。尽管长上下文MLLMs最近有所进展，但存储和处理长视觉上下文的关键值缓存会带来大量的内存和计算开销。现有的视觉压缩方法要么需要在压缩前编码整个视觉上下文，要么需要提前访问问题，这在长视频理解和多轮对话场景中是不切实际的。在这项工作中，我们提出了StreamMem，一种用于流式视频理解的查询无关KV缓存内存机制。具体来说，StreamMem以流式方式编码新的视频帧，使用视觉标记和通用查询标记之间的注意力分数来压缩KV缓存，同时保持固定大小的KV内存，以便在内存受限的长视频场景中实现高效的问题回答。在三个长视频理解和两个流式视频问答基准测试上的评估表明，StreamMem在查询无关KV缓存压缩方面实现了最先进的性能，并且与查询感知压缩方法具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have made significant progress invisual-language reasoning, but their ability to efficiently handle long videosremains limited. Despite recent advances in long-context MLLMs, storing andattending to the key-value (KV) cache for long visual contexts incurssubstantial memory and computational overhead. Existing visual compressionmethods require either encoding the entire visual context before compression orhaving access to the questions in advance, which is impractical for long videounderstanding and multi-turn conversational settings. In this work, we proposeStreamMem, a query-agnostic KV cache memory mechanism for streaming videounderstanding. Specifically, StreamMem encodes new video frames in a streamingmanner, compressing the KV cache using attention scores between visual tokensand generic query tokens, while maintaining a fixed-size KV memory to enableefficient question answering (QA) in memory-constrained, long-video scenarios.Evaluation on three long video understanding and two streaming video questionanswering benchmarks shows that StreamMem achieves state-of-the-art performancein query-agnostic KV cache compression and is competitive with query-awarecompression approaches.</description>
      <author>example@mail.com (Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren)</author>
      <guid isPermaLink="false">2508.15717v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.15641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Grounded VideoDiT，一种视频大语言模型，通过引入扩散时间潜在编码器、基于对象的表示和混合令牌方案三个创新，解决了现有视频模型在时间感知、视觉对齐和时间戳建模方面的局限，在多个视频理解基准测试上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;理解视频不仅需要回答开放性问题，还需要精确定位事件发生时间和实体随时间的交互。现有视频大语言模型在整体推理方面有进展，但在时间感知方面仍较粗糙：时间戳隐式编码，帧级特征捕捉连续性弱，语言视觉对齐常偏离感兴趣实体。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够克服现有视频大语言模型在时间感知、视觉对齐和时间戳建模方面局限的视频大语言模型。&lt;h4&gt;方法&lt;/h4&gt;提出Grounded VideoDiT，包含三个关键创新：1) 扩散时间潜在编码器增强边界敏感性和时间一致性；2) 基于对象的表示将查询实体绑定到局部视觉证据，增强对齐；3) 混合令牌方案提供显式时间戳建模，实现细粒度时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;Grounded VideoDiT通过三个创新设计具备了强大的基础能力，在Charades STA、NExT GQA和多个VideoQA基准测试上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;Grounded VideoDiT通过创新的架构设计有效解决了现有视频大语言模型在时间感知、视觉对齐和时间戳建模方面的局限，为视频理解提供了更精确和细粒度的方法。&lt;h4&gt;翻译&lt;/h4&gt;理解视频不仅需要回答开放性问题，它还需要能够精确定位事件发生的时间和实体随时间的交互方式。尽管最近的视频大语言模型在整体推理方面取得了显著进展，但它们在时间感知方面仍然比较粗糙：时间戳仅被隐式编码，帧级特征在捕捉连续性方面较弱，语言视觉对齐常常偏离感兴趣的实体。在本文中，我们提出了Grounded VideoDiT，一种视频大语言模型，通过引入三个关键创新来克服这些局限。首先，扩散时间潜在编码器增强了边界敏感性并保持时间一致性。其次，基于对象的表示将查询实体明确绑定到局部视觉证据，增强了对齐。第三，具有离散时间令牌的混合令牌方案提供显式的时间戳建模，实现细粒度的时间推理。这些设计共同使Grounded VideoDiT具备了强大的基础能力，正如在Charades STA、NExT GQA和多个VideoQA基准测试上取得的最先进结果所验证的那样。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding videos requires more than answering open ended questions, itdemands the ability to pinpoint when events occur and how entities interactacross time. While recent Video LLMs have achieved remarkable progress inholistic reasoning, they remain coarse in temporal perception: timestamps areencoded only implicitly, frame level features are weak in capturing continuity,and language vision alignment often drifts from the entities of interest. Inthis paper, we present Grounded VideoDiT, a Video LLM designed to overcomethese limitations by introducing three key innovations. First, a DiffusionTemporal Latent (DTL) encoder enhances boundary sensitivity and maintainstemporal consistency. Second, object grounded representations explicitly bindquery entities to localized visual evidence, strengthening alignment. Third, amixed token scheme with discrete temporal tokens provides explicit timestampmodeling, enabling fine grained temporal reasoning. Together, these designsequip Grounded VideoDiT with robust grounding capabilities, as validated bystate of the art results on Charades STA, NExT GQA, and multiple VideoQAbenchmarks.</description>
      <author>example@mail.com (Pengcheng Fang, Yuxia Chen, Rui Guo)</author>
      <guid isPermaLink="false">2508.15641v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>An Empirical Study on How Video-LLMs Answer Video Questions</title>
      <link>http://arxiv.org/abs/2508.15360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过系统的实证研究揭示了视频大语言模型(Video-LLMs)的内部工作机制，发现视频信息提取主要发生在早期层，形成两阶段处理过程；某些中间层对视频问答有不成比例的影响；时空建模更依赖语言引导检索而非自注意力机制。这些发现可用于减少计算成本，提高模型效率。&lt;h4&gt;背景&lt;/h4&gt;利用大规模数据和预训练语言模型的视频大语言模型(Video-LLMs)在视频问答方面表现出强大能力，但大多数现有工作专注于提高性能，对理解其内部机制关注有限。&lt;h4&gt;目的&lt;/h4&gt;通过系统的实证研究填补Video-LLMs内部机制理解的空白，解释现有Video-LLMs的工作原理。&lt;h4&gt;方法&lt;/h4&gt;采用注意力敲除作为主要分析工具，设计了三种变体：视频时间敲除、视频空间敲除和语言到视频敲除，并在不同层数上应用这些敲除，提供全局设置和细粒度设置两种研究环境。&lt;h4&gt;主要发现&lt;/h4&gt;1) 全局设置表明视频信息提取主要发生在早期层，形成两阶段过程——较低层专注于感知编码，较高层处理抽象推理；2) 细粒度设置中，某些中间层对视频问答有不成比例的影响，作为关键异常值；3) 时空建模更依赖语言引导检索而非视频标记内部的帧间和帧内自注意力。&lt;h4&gt;结论&lt;/h4&gt;这些见解可以用于减少Video-LLMs中的注意力计算，提高模型效率。这是第一个系统揭示Video-LLMs内部如何处理和理解视频内容的工作，为未来研究提供了可解释性和效率的视角。&lt;h4&gt;翻译&lt;/h4&gt;利用大规模数据和预训练语言模型，视频大语言模型(Video-LLMs)在视频问答方面表现出强大的能力。然而，大多数现有工作专注于提高性能，而对理解其内部机制关注有限。本文旨在通过系统的实证研究填补这一空白。为了解释现有的Video-LLMs，我们采用注意力敲除作为主要分析工具，并设计了三种变体：视频时间敲除、视频空间敲除和语言到视频敲除。然后，我们在不同数量的层(层窗口)上应用这三种敲除。通过仔细控制层窗口和敲除类型，我们提供了两种设置：全局设置和细粒度设置。我们的研究揭示了三个关键发现：(1) 全局设置表明视频信息提取主要发生在早期层，形成一个清晰的两阶段过程——较低层专注于感知编码，而较高层处理抽象推理；(2) 在细粒度设置中，某些中间层对视频问答有不成比例的影响，作为关键异常值，而大多数其他层贡献最小；(3) 在两种设置中，我们观察到时空建模更多地依赖于语言引导的检索，而不是视频标记内部的帧间和帧内自注意力，尽管后者的计算成本很高。最后，我们证明这些见解可以用于减少Video-LLMs中的注意力计算。据我们所知，这是第一个系统揭示Video-LLMs内部如何处理和理解视频内容的工作，为未来的研究提供了可解释性和效率的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Taking advantage of large-scale data and pretrained language models, VideoLarge Language Models (Video-LLMs) have shown strong capabilities in answeringvideo questions. However, most existing efforts focus on improving performance,with limited attention to understanding their internal mechanisms. This paperaims to bridge this gap through a systematic empirical study. To interpretexisting VideoLLMs, we adopt attention knockouts as our primary analytical tooland design three variants: Video Temporal Knockout, Video Spatial Knockout, andLanguage-to-Video Knockout. Then, we apply these three knockouts on differentnumbers of layers (window of layers). By carefully controlling the window oflayers and types of knockouts, we provide two settings: a global setting and afine-grained setting. Our study reveals three key findings: (1) Global settingindicates Video information extraction primarily occurs in early layers,forming a clear two-stage process -- lower layers focus on perceptual encoding,while higher layers handle abstract reasoning; (2) In the fine-grained setting,certain intermediate layers exert an outsized impact on video questionanswering, acting as critical outliers, whereas most other layers contributeminimally; (3) In both settings, we observe that spatial-temporal modelingrelies more on language-guided retrieval than on intra- and inter-frameself-attention among video tokens, despite the latter's high computationalcost. Finally, we demonstrate that these insights can be leveraged to reduceattention computation in Video-LLMs. To our knowledge, this is the first workto systematically uncover how Video-LLMs internally process and understandvideo content, offering interpretability and efficiency perspectives for futureresearch.</description>
      <author>example@mail.com (Chenhui Gou, Ziyu Ma, Zicheng Duan, Haoyu He, Feng Chen, Akide Liu, Bohan Zhuang, Jianfei Cai, Hamid Rezatofighi)</author>
      <guid isPermaLink="false">2508.15360v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</title>
      <link>http://arxiv.org/abs/2508.15298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为时间提示对齐(TPA)的新方法，用于超声视频中胎儿先天性心脏病(CHD)的分类检测，结合了时间建模、提示感知对比学习和不确定性量化技术。&lt;h4&gt;背景&lt;/h4&gt;超声视频中的先天性心脏病(CHD)检测受到图像噪声和探头位置变化的影响。虽然自动化方法可以减少操作者依赖，但当前机器学习方法往往忽略时间信息，仅限于二元分类，且不考虑预测校准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用时间信息、支持多类别分类并提供可靠不确定性量化的方法，用于胎儿心脏超声视频中的CHD检测。&lt;h4&gt;方法&lt;/h4&gt;TPA方法使用图像编码器从视频子片段的每一帧提取特征，通过可训练的时间提取器聚合特征以捕获心脏运动，并通过边际铰链对比损失将视频表示与类别特定的文本提示对齐。同时引入条件变分自编码器样式调制(CVAESM)模块学习潜在样式向量来调制嵌入并量化分类不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;TPA在CHD诊断上实现了85.40%的最先进宏F1分数，同时将期望校准误差减少了5.38%，自适应ECE减少了6.8%。在EchoNet-Dynamic的三类任务中，宏F1提升了4.73%(从53.89%提升至58.62%)。&lt;h4&gt;结论&lt;/h4&gt;时间提示对齐(TPA)是一个有效的框架，集成了时间建模、提示感知对比学习和不确定性量化，能够准确可靠地检测胎儿先天性心脏病。&lt;h4&gt;翻译&lt;/h4&gt;超声视频中的先天性心脏病(CHD)检测受到图像噪声和探头位置变化的影响。虽然自动化方法可以减少操作者依赖，但当前的机器学习方法往往忽略时间信息，仅限于二元分类，且不考虑预测校准。我们提出时间提示对齐(TPA)，一种利用基础图像-文本模型和提示感知对比学习的方法，用于对胎儿心脏超声视频进行CHD分类。TPA使用图像编码器从视频子片段的每一帧提取特征，通过可训练的时间提取器聚合它们以捕获心脏运动，并通过边际铰链对比损失将视频表示与类别特定的文本提示对齐。为了增强临床可靠性的校准，我们引入了条件变分自编码器样式调制(CVAESM)模块，该模块学习潜在样式向量来调制嵌入并量化分类不确定性。在私人CHD检测数据集和大型公共数据集EchoNet-Dynamic(用于评估收缩功能障碍)上评估，TPA在CHD诊断上实现了85.40%的最先进宏F1分数，同时将期望校准误差减少了5.38%，自适应ECE减少了6.8%。在EchoNet-Dynamic的三类任务中，宏F1提升了4.73%(从53.89%提升至58.62%)。时间提示对齐(TPA)是一个用于胎儿先天性心脏病(CHD)分类的框架，集成了时间建模、提示感知对比学习和不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Congenital heart defect (CHD) detection in ultrasound videos is hindered byimage noise and probe positioning variability. While automated methods canreduce operator dependence, current machine learning approaches often neglecttemporal information, limit themselves to binary classification, and do notaccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),a method leveraging foundation image-text model and prompt-aware contrastivelearning to classify fetal CHD on cardiac ultrasound videos. TPA extractsfeatures from each frame of video subclips using an image encoder, aggregatesthem with a trainable temporal extractor to capture heart motion, and alignsthe video representation with class-specific text prompts via a margin-hingecontrastive loss. To enhance calibration for clinical reliability, we introducea Conditional Variational Autoencoder Style Modulation (CVAESM) module, whichlearns a latent style vector to modulate embeddings and quantifiesclassification uncertainty. Evaluated on a private dataset for CHD detectionand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPAachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, whilealso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. OnEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenitalheart defect (CHD) classification in ultrasound videos that integrates temporalmodeling, prompt-aware contrastive learning, and uncertainty quantification.</description>
      <author>example@mail.com (Darya Taratynova, Alya Almsouti, Beknur Kalmakhanbet, Numan Saeed, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2508.15298v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment</title>
      <link>http://arxiv.org/abs/2508.09399v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于联邦学习的风险评估框架，用于解决跨机构金融风险分析中的数据隐私和协作建模挑战。该方法通过特征注意力机制和时间建模结构，在不共享原始数据的情况下实现跨机构联合建模和风险识别。&lt;h4&gt;背景&lt;/h4&gt;跨机构金融风险分析面临数据隐私和协作建模的挑战。传统方法需要共享原始数据，这可能导致敏感信息泄露，同时限制了跨机构协作的效率和范围。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于联邦学习的风险评估框架，实现在不共享原始数据的情况下，跨机构进行联合建模和风险识别。&lt;h4&gt;方法&lt;/h4&gt;采用特征注意力机制和时间建模结构，通过分布式优化策略，每个金融机构训练本地子模型，使用差分隐私和噪声注入保护模型参数后上传至中央服务器，中央服务器聚合这些参数生成全局模型，用于系统性风险识别。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型在所有评估指标上优于传统集中式方法和现有联邦学习变体，在敏感金融环境中展示出强大的建模能力和实用价值。&lt;h4&gt;结论&lt;/h4&gt;该方法增强了风险识别的范围和效率，同时保留了数据主权，为智能金融风险分析提供了安全高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了跨机构金融风险分析中的数据隐私和协作建模挑战。它提出了一种基于联邦学习的风险评估框架。在不共享原始数据的情况下，该方法使多个机构能够进行联合建模和风险识别。这是通过结合特征注意力机制和时间建模结构实现的。具体来说，该模型采用分布式优化策略。每个金融机构训练一个本地子模型。在上传之前，使用差分隐私和噪声注入保护模型参数。然后，中央服务器聚合这些参数以生成全局模型。这个全局模型用于系统性风险识别。为了验证所提出方法的有效性，进行了多次实验。这些实验评估了通信效率、模型准确性、系统性风险检测和跨市场泛化能力。结果表明，在所有评估指标上，所提出的模型优于传统的集中式方法和现有的联邦学习变体。它在敏感的金融环境中展示了强大的建模能力和实用价值。该方法增强了风险识别的范围和效率，同时保留了数据主权。它为智能金融风险分析提供了安全高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges of data privacy and collaborativemodeling in cross-institution financial risk analysis. It proposes a riskassessment framework based on federated learning. Without sharing raw data, themethod enables joint modeling and risk identification across multipleinstitutions. This is achieved by incorporating a feature attention mechanismand temporal modeling structure. Specifically, the model adopts a distributedoptimization strategy. Each financial institution trains a local sub-model. Themodel parameters are protected using differential privacy and noise injectionbefore being uploaded. A central server then aggregates these parameters togenerate a global model. This global model is used for systemic riskidentification. To validate the effectiveness of the proposed method, multipleexperiments are conducted. These evaluate communication efficiency, modelaccuracy, systemic risk detection, and cross-market generalization. The resultsshow that the proposed model outperforms both traditional centralized methodsand existing federated learning variants across all evaluation metrics. Itdemonstrates strong modeling capabilities and practical value in sensitivefinancial environments. The method enhances the scope and efficiency of riskidentification while preserving data sovereignty. It offers a secure andefficient solution for intelligent financial risk analysis.</description>
      <author>example@mail.com (Yue Yao, Zhen Xu, Youzhu Liu, Kunyuan Ma, Yuxiu Lin, Mohan Jiang)</author>
      <guid isPermaLink="false">2508.09399v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</title>
      <link>http://arxiv.org/abs/2508.15680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures, Presented at IAIL 2025 - Imagining the AI  Landscape after the AI Act, 4th International Workshop on Imagining the AI  Landscape After the AI Act, The fourth International Conference on Hybrid  Human-Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出对欧盟AI法案的技术哲学解读，探讨AI系统中数据的长期动态和递归价值链，揭示监管盲点，并提出受西蒙东哲学启发的AI生命周期模型和有效监管措施。&lt;h4&gt;背景&lt;/h4&gt;AI系统中的数据生命周期从摄取到部署产生了挑战现有负责任AI框架的递归价值链，现有政策制定缺乏对AI技术运作和经济逻辑背后的'生成动态'的解释。&lt;h4&gt;目的&lt;/h4&gt;提出一种受西蒙东技术哲学启发的AI正式解读，重新诠释'个体化'概念来建模AI生命周期，引入'未来性'概念解释AI的自我强化生命周期，并提出有效监管措施。&lt;h4&gt;方法&lt;/h4&gt;引入一个概念工具来构建AI管道，涵盖数据、训练模式、架构、特征存储和迁移学习；使用跨学科方法开发技术上可靠且哲学上连贯的分析。&lt;h4&gt;主要发现&lt;/h4&gt;数据的递归生成性和非竞争性特征；科技寡头通过捕获、训练和部署的基础设施集中了价值和决策权；有效监管需要解决基础设施和时间动态问题。&lt;h4&gt;结论&lt;/h4&gt;有效监管必须解决AI系统的基础设施和时间动态问题，提出包括生命周期审计、时间可追溯性、反馈问责制、递归透明度和对递归重用的争议权等措施。&lt;h4&gt;翻译&lt;/h4&gt;本文主张对欧盟AI法案进行技术哲学解读，这为AI系统中数据的长期动态提供了见解，特别是从摄取到部署的生命周期如何产生挑战现有负责任AI框架的递归价值链。我们引入了一个概念工具来构建AI管道，涵盖数据、训练模式、架构、特征存储和迁移学习。使用跨学科方法，我们开发了一种技术上可靠且哲学上连贯的分析，揭示了监管盲点。我们的核心观点是，政策制定中缺乏对AI技术运作和经济逻辑背后的'生成动态'的解释。为此，我们提出了一种受西蒙东技术哲学启发的AI正式解读，重新诠释其'个体化'概念来建模AI生命周期，包括前个体环境、个体化和个体化的AI。为了解释这些概念，我们引入了'未来性'：AI的自我强化生命周期，其中更多数据提高性能、深化个性化并扩展应用领域。未来性突显了数据的递归生成性和非竞争性，以及支持反馈、适应和时间递归的特征存储等基础设施。我们的干预凸显了不断加剧的权力不对称，特别是科技寡头，其捕获、训练和部署的基础设施集中了价值和决策权。我们认为，有效监管必须解决这些基础设施和时间动态问题，并提出包括生命周期审计、时间可追溯性、反馈问责制、递归透明度和对递归重用的争议权等措施。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper argues that a techno-philosophical reading of the EU AI Actprovides insight into the long-term dynamics of data in AI systems,specifically, how the lifecycle from ingestion to deployment generatesrecursive value chains that challenge existing frameworks for Responsible AI.We introduce a conceptual tool to frame the AI pipeline, spanning data,training regimes, architectures, feature stores, and transfer learning. Usingcross-disciplinary methods, we develop a technically grounded andphilosophically coherent analysis of regulatory blind spots. Our central claimis that what remains absent from policymaking is an account of the dynamic ofbecoming that underpins both the technical operation and economic logic of AI.To address this, we advance a formal reading of AI inspired by Simondonianphilosophy of technology, reworking his concept of individuation to model theAI lifecycle, including the pre-individual milieu, individuation, andindividuated AI. To translate these ideas, we introduce futurity: theself-reinforcing lifecycle of AI, where more data enhances performance, deepenspersonalisation, and expands application domains. Futurity highlights therecursively generative, non-rivalrous nature of data, underpinned byinfrastructures like feature stores that enable feedback, adaptation, andtemporal recursion. Our intervention foregrounds escalating power asymmetries,particularly the tech oligarchy whose infrastructures of capture, training, anddeployment concentrate value and decision-making. We argue that effectiveregulation must address these infrastructural and temporal dynamics, andpropose measures including lifecycle audits, temporal traceability, feedbackaccountability, recursion transparency, and a right to contest recursive reuse.</description>
      <author>example@mail.com (Mark Cote, Susana Aires)</author>
      <guid isPermaLink="false">2508.15680v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)</title>
      <link>http://arxiv.org/abs/2508.15633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Full version of the paper accepted for publication at the European  Conference on Artificial Intelligence (ECAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRASPED的图自编码器，用于节点异常检测，它结合了图小波卷积编码器和维纳图反卷积解码器，能够在多个尺度捕获全局和局部图信息，有效捕获异常信息。&lt;h4&gt;背景&lt;/h4&gt;图机器学习已在多个领域广泛应用，如社区检测、交易分析和推荐系统，其中异常检测扮演着重要角色。研究表明图上的异常会引起频谱偏移，但现有监督方法受限于标记数据的稀缺性，而无监督学习方法主要依赖空间信息或仅使用低通滤波，导致无法进行多频带分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种无监督学习方法，能够在多个尺度捕获图的全局和局部信息，有效进行节点异常检测，克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出GRASPED模型，包含基于图小波卷积的编码器以及结构和属性解码器。图小波卷积编码器与维纳图反卷积解码器相结合，展现出带通滤波特性，实现基于学习的节点属性重建，有效捕获异常信息。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界图异常检测数据集上的实验表明，GRASPED模型能够有效捕获异常信息，并优于当前最先进的模型。&lt;h4&gt;结论&lt;/h4&gt;GRASPED作为一种无监督学习方法，能够在不依赖标记数据的情况下，通过多尺度分析有效检测图中的节点异常，性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;图机器学习已在各个领域得到广泛探索，如社区检测、交易分析和推荐系统。在这些应用中，异常检测扮演着重要角色。最近的研究表明，图上的异常会引起频谱偏移。一些监督方法已经提高了对这种频域信息的利用率。然而，由于异常的本质，它们仍然受限于标记数据的稀缺性。另一方面，现有的无监督学习方法主要依赖空间信息或仅使用低通滤波，从而失去了多频带分析的能力。在本文中，我们提出了用于节点异常检测的图自编码器，具有频域编码器和频域解码器（GRASPED）。我们的无监督学习模型具有基于图小波卷积的编码器，以及结构和属性解码器。基于图小波卷积的编码器与基于维纳图反卷积的解码器相结合，展现出带通滤波特性，能够在多个尺度捕获全局和局部图信息。这种设计允许基于学习的节点属性重建，有效捕获异常信息。在多个真实世界图异常检测数据集上的广泛实验表明，GRASPED优于当前最先进的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph machine learning has been widely explored in various domains, such ascommunity detection, transaction analysis, and recommendation systems. In theseapplications, anomaly detection plays an important role. Recently, studies haveshown that anomalies on graphs induce spectral shifts. Some supervised methodshave improved the utilization of such spectral domain information. However,they remain limited by the scarcity of labeled data due to the nature ofanomalies. On the other hand, existing unsupervised learning approachespredominantly rely on spatial information or only employ low-pass filters,thereby losing the capacity for multi-band analysis. In this paper, we proposeGraph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for nodeanomaly detection. Our unsupervised learning model features an encoder based onGraph Wavelet Convolution, along with structural and attribute decoders. TheGraph Wavelet Convolution-based encoder, combined with a Wiener GraphDeconvolution-based decoder, exhibits bandpass filter characteristics thatcapture global and local graph information at multiple scales. This designallows for a learning-based reconstruction of node attributes, effectivelycapturing anomaly information. Extensive experiments on several real-worldgraph anomaly detection datasets demonstrate that GRASPED outperforms currentstate-of-the-art models.</description>
      <author>example@mail.com (Wei Herng Choong, Jixing Liu, Ching-Yu Kao, Philip Sperl)</author>
      <guid isPermaLink="false">2508.15633v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Clustering-based aggregate value regression</title>
      <link>http://arxiv.org/abs/2508.15567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种专门用于预测聚合值的新型回归方法，称为聚合值回归(AVR)，并通过层次聚类技术(AVR-C)解决了过参数化问题，建立了在模型假设错误情况下的新型偏差-方差权衡理论。&lt;h4&gt;背景&lt;/h4&gt;在实际应用中，预测聚合值(如特定地区的总电力需求)往往比预测个体值更重要，但专门针对聚合值的统计学习方法尚未充分发展，尤其是预测误差与聚类数量之间的关系研究不足。&lt;h4&gt;目的&lt;/h4&gt;引入一种专门针对线性回归模型中聚合值的新型预测方法，解决聚合值预测中的统计学习问题，并研究预测误差与聚类数量之间的关系。&lt;h4&gt;方法&lt;/h4&gt;提出聚合值回归(AVR)方法，将所有回归模型组合成单一模型；为解决过参数化问题，引入层次聚类技术(AVR-C)，构建回归模型聚类并在每个聚类内执行AVR；通过蒙特卡洛模拟研究训练和测试误差行为。&lt;h4&gt;主要发现&lt;/h4&gt;AVR-C在模型假设错误情况下引入了新型偏差-方差权衡理论，其中聚类数量表征了模型复杂度；通过电力需求预测分析验证了该理论。&lt;h4&gt;结论&lt;/h4&gt;AVR-C方法为聚合值预测提供了有效解决方案，通过聚类技术解决了过参数化问题，并通过偏差-方差权衡理论优化了模型复杂度。&lt;h4&gt;翻译&lt;/h4&gt;在各种实际情况下，预测聚合值而非个体值往往是我们关注的重点。例如，电力公司对预测特定地区的总电力需求感兴趣，以确保电网可靠运行和资源分配。然而，据我们所知，专门用于预测聚合值的统计学习方法尚未得到充分发展。特别是，预测误差与聚类数量之间的关系尚未得到充分研究，因为聚类通常被视为无监督学习。本研究介绍了一种专门关注线性回归模型中聚合值的新型预测方法。我们称之为聚合值回归(AVR)，它通过将所有回归模型组合成一个单一模型来构建。使用AVR时，当需要组合的回归模型数量大时，我们必须估计大量参数，导致过参数化。为解决过参数化问题，我们引入了层次聚类技术，称为AVR-C(C代表聚类)。在该方法中，构建了几个回归模型聚类，并在每个聚类内执行AVR。AVR-C在模型假设错误的情况下引入了一种新型偏差-方差权衡理论。在该框架中，聚类数量表征了模型复杂度。进行了蒙特卡洛模拟以研究我们提出的聚类技术的训练和测试误差行为。通过电力需求预测分析也验证了偏差-方差权衡理论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In various practical situations, forecasting of aggregate values rather thanindividual ones is often our main focus. For instance, electricity companiesare interested in forecasting the total electricity demand in a specific regionto ensure reliable grid operation and resource allocation. However, to ourknowledge, statistical learning specifically for forecasting aggregate valueshas not yet been well-established. In particular, the relationship betweenforecast error and the number of clusters has not been well studied, asclustering is usually treated as unsupervised learning. This study introduces anovel forecasting method specifically focused on the aggregate values in thelinear regression model. We call it the Aggregate Value Regression (AVR), andit is constructed by combining all regression models into a single model. Withthe AVR, we must estimate a huge number of parameters when the number ofregression models to be combined is large, resulting in overparameterization.To address the overparameterization issue, we introduce a hierarchicalclustering technique, referred to as AVR-C (C stands for clustering). In thisapproach, several clusters of regression models are constructed, and the AVR isperformed within each cluster. The AVR-C introduces a novel bias-variancetrade-off theory under the assumption of a misspecified model. In thisframework, the number of clusters characterizes model complexity. Monte Carlosimulation is conducted to investigate the behavior of training and test errorsof our proposed clustering technique. The bias-variance trade-off theory isalso demonstrated through the analysis of electricity demand forecasting.</description>
      <author>example@mail.com (Kei Hirose, Hidetoshi Matsui, Hiroki Masuda)</author>
      <guid isPermaLink="false">2508.15567v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>EffortNet: A Deep Learning Framework for Objective Assessment of Speech Enhancement Technologies Using EEG-Based Alpha Oscillations</title>
      <link>http://arxiv.org/abs/2508.15473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EffortNet是一种新型深度学习框架，用于从语音理解过程中的脑电图（EEG）中解码个体听力努力。&lt;h4&gt;背景&lt;/h4&gt;听力理解中的听力努力是听力研究中的一个重大挑战，特别是对老年人和听力障碍人群而言。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从EEG数据中准确解码听力努力的深度学习框架，解决个体间EEG信号变异性大的问题。&lt;h4&gt;方法&lt;/h4&gt;收集122名参与者在四种语音条件（清晰、噪声、MMSE增强和Transformer增强）下的64通道EEG数据；整合三种互补学习范式：自监督学习利用未标记数据，增量学习逐步适应个体特征，迁移学习高效将知识转移到新主体。&lt;h4&gt;主要发现&lt;/h4&gt;1) alpha振荡（8-13 Hz）在噪声语音处理中功率显著更高，可作为听力努力的客观生物标志物；2) EffortNet实现80.9%分类准确率，仅使用新主体40%训练数据；3) Transformer增强语音引发的神经反应比MMSE增强语音更接近清晰语音，与主观可懂度评级形成对比。&lt;h4&gt;结论&lt;/h4&gt;EffortNet为听力技术的个性化评估提供了实用解决方案，对设计认知感知的语音增强系统具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了EffortNet，一种新型深度学习框架，用于从语音理解过程中的脑电图（EEG）解码个体听力努力。听力努力在语音听力研究中代表着一个重大挑战，特别是对老年人和听力障碍人群。我们从122名参与者收集了64通道的EEG数据，他们在四种条件下进行语音理解：清晰、噪声、MMSE增强和Transformer增强语音。统计分析证实，与清晰或增强条件相比，噪声语音处理过程中alpha振荡（8-13 Hz）的功率显著更高，证实了它们作为听力努力客观生物标志物的有效性。为了解决EEG信号中显著的个体间变异性，EffortNet整合了三种互补的学习范式：利用未标记数据的自监督学习，逐步适应个体特征的增量学习，以及高效将知识转移到新主体的迁移学习。我们的实验结果表明，EffortNet仅使用新主体40%的训练数据就实现了80.9%的分类准确率，显著优于传统的CNN（62.3%）和STAnet（61.1%）模型。从模型得出的基于概率的指标显示，Transformer增强语音引发的神经反应比MMSE增强语音更接近清晰语音。这一发现与主观可懂度评级形成对比，但与客观指标一致。所提出的框架为听力技术的个性化评估提供了实用的解决方案，对设计认知感知的语音增强系统具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents EffortNet, a novel deep learning framework for decodingindividual listening effort from electroencephalography (EEG) during speechcomprehension. Listening effort represents a significant challenge inspeech-hearing research, particularly for aging populations and those withhearing impairment. We collected 64-channel EEG data from 122 participantsduring speech comprehension under four conditions: clean, noisy, MMSE-enhanced,and Transformer-enhanced speech. Statistical analyses confirmed that alphaoscillations (8-13 Hz) exhibited significantly higher power during noisy speechprocessing compared to clean or enhanced conditions, confirming their validityas objective biomarkers of listening effort. To address the substantialinter-individual variability in EEG signals, EffortNet integrates threecomplementary learning paradigms: self-supervised learning to leverageunlabeled data, incremental learning for progressive adaptation to individualcharacteristics, and transfer learning for efficient knowledge transfer to newsubjects. Our experimental results demonstrate that Effort- Net achieves 80.9%classification accuracy with only 40% training data from new subjects,significantly outperforming conventional CNN (62.3%) and STAnet (61.1%) models.The probability-based metric derived from our model revealed thatTransformer-enhanced speech elicited neural responses more similar to cleanspeech than MMSEenhanced speech. This finding contrasted with subjectiveintelligibility ratings but aligned with objective metrics. The proposedframework provides a practical solution for personalized assessment of hearingtechnologies, with implications for designing cognitive-aware speechenhancement systems.</description>
      <author>example@mail.com (Ching-Chih Sung, Cheng-Hung Hsin, Yu-Anne Shiah, Bo-Jyun Lin, Yi-Xuan Lai, Chia-Ying Lee, Yu-Te Wang, Borchin Su, Yu Tsao)</author>
      <guid isPermaLink="false">2508.15473v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Least Squares/Gradient Descent Methods for DeepONets</title>
      <link>http://arxiv.org/abs/2508.15394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种混合最小二乘/梯度下降方法来加速DeepONet训练，通过将大型最小二乘系统分解为两个更小的子问题来解决计算复杂性问题。&lt;h4&gt;背景&lt;/h4&gt;DeepONet是一种神经网络架构，但其训练过程面临计算效率挑战，特别是当考虑所有可能的输入组合时。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法来加速DeepONet的训练过程，解决传统方法中因大规模线性问题导致的计算瓶颈。&lt;h4&gt;方法&lt;/h4&gt;提出混合最小二乘/梯度下降方法，将DeepONet的输出视为与分支网络最后一层参数线性相关，使用最小二乘优化这些参数，同时用梯度下降更新其他隐藏层参数；将大型最小二乘系统分解为分支网络和主干网络两个子问题分别求解；将方法推广到包含正则化项的L2损失类型，适用于物理信息损失的监督学习场景。&lt;h4&gt;主要发现&lt;/h4&gt;将大型最小二乘系统分解为两个更小、更易管理的子问题可以有效解决计算复杂性问题，同时保持训练效率。&lt;h4&gt;结论&lt;/h4&gt;该方法能够显著加速DeepONet的训练过程，并可以推广到更广泛的损失函数类型，包括正则化物理信息损失。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种高效的混合最小二乘/梯度下降方法来加速DeepONet训练。由于DeepONet的输出可以看作是与分支网络最后一层参数线性的，因此这些参数可以通过最小二乘求解进行优化，而剩余的隐藏层参数则通过梯度下降形式更新。然而，为所有可能的分支和主干输入组合构建最小二乘系统会导致一个过大且无法直接求解的线性问题。为了解决这个问题，我们的方法将大的最小二乘系统分解为两个更小、更易管理的子问题——一个针对分支网络，一个针对主干网络——并分别求解。该方法被推广到更广泛的L2损失类型，包含对最后一层参数的正则化项，包括使用物理信息损失的监督学习情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an efficient hybrid least squares/gradient descent method toaccelerate DeepONet training. Since the output of DeepONet can be viewed aslinear with respect to the last layer parameters of the branch network, theseparameters can be optimized using a least squares (LS) solve, and the remaininghidden layer parameters are updated by means of gradient descent form. However,building the LS system for all possible combinations of branch and trunk inputsyields a prohibitively large linear problem that is infeasible to solvedirectly. To address this issue, our method decomposes the large LS system intotwo smaller, more manageable subproblems $\unicode{x2014}$ one for the branchnetwork and one for the trunk network $\unicode{x2014}$ and solves themseparately. This method is generalized to a broader type of $L^2$ loss with aregularization term for the last layer parameters, including the case ofunsupervised learning with physics-informed loss.</description>
      <author>example@mail.com (Jun Choi, Chang-Ock Lee, Minam Moon)</author>
      <guid isPermaLink="false">2508.15394v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Numerical Analysis of Unsupervised Learning Approaches for Parameter Identification in PDEs</title>
      <link>http://arxiv.org/abs/2508.15381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  49 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文综述了用于偏微分方程参数识别的无监督学习方法，特别关注扩散系数识别问题，并提出了基于不同方法的误差界限推导框架。&lt;h4&gt;背景&lt;/h4&gt;识别偏微分方程中的参数代表了一类广泛的应用反问题。近年来，已经开发了几种使用(深度)神经网络的无监督学习方法来解决PDE参数识别问题。&lt;h4&gt;目的&lt;/h4&gt;提供对这些无监督学习技术的全面综述，从经典数值分析角度出发，并推导使用不同方法获得的离散近似的严格误差界限。&lt;h4&gt;方法&lt;/h4&gt;使用神经网络作为ansatz函数来近似参数和/或状态，提出一种通用框架用于推导使用Galerkin有限元方法、混合方法和深度神经网络获得的离散近似的严格误差界限。&lt;h4&gt;主要发现&lt;/h4&gt;这些无监督学习方法展示了令人印象深刻的经验性能，条件稳定性估计在误差分析中起着关键作用。&lt;h4&gt;结论&lt;/h4&gt;研究从经典数值分析角度对无监督学习方法进行了全面综述，并提出了推导严格误差界限的通用框架。&lt;h4&gt;翻译&lt;/h4&gt;识别偏微分方程中的参数代表了一类非常广泛的应用反问题。近年来，已经开发了几种使用(深度)神经网络的无监督学习方法来解决PDE参数识别问题。这些方法使用神经网络作为ansatz函数来近似参数和/或状态，并展示了令人印象深刻的经验性能。在本文中，我们从经典数值分析的角度，对一个模型问题(扩散系数识别)上的这些无监督学习技术进行了全面综述，并概述了一个通用框架，用于推导使用Galerkin有限元方法、混合方法和深度神经网络获得的离散近似的严格误差界限。我们始终强调条件稳定性估计在误差分析中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying parameters in partial differential equations (PDEs) represents avery broad class of applied inverse problems. In recent years, severalunsupervised learning approaches using (deep) neural networks have beendeveloped to solve PDE parameter identifications. These approaches employneural networks as ansatz functions to approximate the parameters and / or thestates, and have demonstrated impressive empirical performance. In this paper,we provide a comprehensive survey on these unsupervised learning techniques onone model problem, diffusion coefficient identification, from the classicalnumerical analysis perspective, and outline a general framework for derivingrigorous error bounds on the discrete approximations obtained using theGalerkin finite element method, hybrid method and deep neural networks.Throughout we highlight the crucial role of conditional stability estimates inthe error analysis.</description>
      <author>example@mail.com (Siyu Cen, Bangti Jin, Qimeng Quan, Zhi Zhou)</author>
      <guid isPermaLink="false">2508.15381v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning optimization based on evolutionary selective fine tuning</title>
      <link>http://arxiv.org/abs/2508.15367v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at the Workshop artiFicial And bio-inspIred netwoRked  intelliGence foR cOnstrained aUtoNomous Devices (FAIRGROUND). 2025  International Joint Conference on Neural Networks (IJCNN)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为BioTune的进化自适应微调技术，通过选择性微调模型层来提高迁移学习效率，减少计算成本并提升性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习在图像分析方面取得了显著进展，但大型完全训练模型的计算需求仍然是一个挑战。迁移学习是适应预训练模型到新任务的有效策略，但传统微调方法通常更新所有参数，可能导致过拟合和更高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够选择性微调模型层的技术，以提高迁移学习效率，减少可训练参数数量，降低计算成本，并使迁移学习能够更好地适应不同的数据特征和分布。&lt;h4&gt;方法&lt;/h4&gt;BioTune采用进化算法来识别出一组特定的层进行微调，而非更新所有参数，从而优化模型在目标任务上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在九个来自不同领域的图像分类数据集上的评估表明，BioTune与现有的微调方法（如AutoRGN和LoRA）相比，实现了具有竞争力或改进的准确性和效率。&lt;h4&gt;结论&lt;/h4&gt;通过将微调过程集中在相关层的子集上，BioTune减少了可训练参数的数量，降低了计算成本，并促进了在不同数据特征和分布下更高效的迁移学习。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在图像分析方面已经取得了实质性进展。然而，大型完全训练模型的计算需求仍然是一个需要考虑的因素。迁移学习为将预训练模型适应到新任务提供了一种策略。传统的微调通常涉及更新所有模型参数，这可能导致过拟合和更高的计算成本。本文介绍了BioTune，这是一种进化自适应微调技术，它选择性地微调层以提高迁移学习效率。BioTune采用进化算法来识别出一组特定的层进行微调，旨在优化模型在给定目标任务上的性能。在来自不同领域的九个图像分类数据集上的评估表明，BioTune与现有的微调方法（如AutoRGN和LoRA）相比，实现了具有竞争力或改进的准确性和效率。通过将微调过程集中在相关层的子集上，BioTune减少了可训练参数的数量，可能导致计算成本降低，并促进在不同数据特征和分布下更高效的迁移学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has shown substantial progress in image analysis. However, thecomputational demands of large, fully trained models remain a consideration.Transfer learning offers a strategy for adapting pre-trained models to newtasks. Traditional fine-tuning often involves updating all model parameters,which can potentially lead to overfitting and higher computational costs. Thispaper introduces BioTune, an evolutionary adaptive fine-tuning technique thatselectively fine-tunes layers to enhance transfer learning efficiency. BioTuneemploys an evolutionary algorithm to identify a focused set of layers forfine-tuning, aiming to optimize model performance on a given target task.Evaluation across nine image classification datasets from various domainsindicates that BioTune achieves competitive or improved accuracy and efficiencycompared to existing fine-tuning methods such as AutoRGN and LoRA. Byconcentrating the fine-tuning process on a subset of relevant layers, BioTunereduces the number of trainable parameters, potentially leading to decreasedcomputational cost and facilitating more efficient transfer learning acrossdiverse data characteristics and distributions.</description>
      <author>example@mail.com (Jacinto Colan, Ana Davila, Yasuhisa Hasegawa)</author>
      <guid isPermaLink="false">2508.15367v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Anomaly Detection in Evolving Network Environments</title>
      <link>http://arxiv.org/abs/2508.15100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NetSight是一个用于网络数据监督异常检测的框架，能够持续在线检测并适应分布漂移，无需手动干预且能防止灾难性遗忘。&lt;h4&gt;背景&lt;/h4&gt;分布漂移（数据统计特性随时间变化）对深度学习异常检测系统构成重大挑战，现有系统难以适应这些漂移。&lt;h4&gt;目的&lt;/h4&gt;介绍NetSight框架，用于网络数据的监督异常检测，能够持续在线检测并适应分布漂移。&lt;h4&gt;方法&lt;/h4&gt;NetSight通过伪标记技术消除手动干预，使用基于知识蒸馏的适应策略防止灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在三个长期网络数据集上评估，NetSight相比依赖手动标注的最先进方法具有更好的适应性能，F1-score提高了最多11.72%。&lt;h4&gt;结论&lt;/h4&gt;NetSight在随时间经历分布漂移的动态网络中表现出鲁棒性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;分布漂移是数据统计特性随时间的变化，它对深度学习异常检测系统构成了关键挑战。现有的异常检测系统通常难以适应这些漂移。具体而言，基于监督学习的系统需要昂贵的手动标注，而基于无监督学习的系统则依赖难以获得的干净数据进行漂移适应。这两种要求在实践中都难以满足。在本文中，我们介绍了NetSight，一个用于网络数据监督异常检测的框架，能够持续在线检测并适应分布漂移。NetSight通过一种新的伪标记技术消除手动干预，并使用基于知识蒸馏的适应策略来防止灾难性遗忘。在三个长期网络数据集上的评估表明，与依赖手动标注的最先进方法相比，NetSight表现出更好的适应性能，F1-score提高了最多11.72%。这证明了它在随时间经历分布漂移的动态网络中的鲁棒性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distribution shift, a change in the statistical properties of data over time,poses a critical challenge for deep learning anomaly detection systems.Existing anomaly detection systems often struggle to adapt to these shifts.Specifically, systems based on supervised learning require costly manuallabeling, while those based on unsupervised learning rely on clean data, whichis difficult to obtain, for shift adaptation. Both of these requirements arechallenging to meet in practice. In this paper, we introduce NetSight, aframework for supervised anomaly detection in network data that continuallydetects and adapts to distribution shifts in an online manner. NetSighteliminates manual intervention through a novel pseudo-labeling technique anduses a knowledge distillation-based adaptation strategy to prevent catastrophicforgetting. Evaluated on three long-term network datasets, NetSightdemonstrates superior adaptation performance compared to state-of-the-artmethods that rely on manual labeling, achieving F1-score improvements of up to11.72%. This proves its robustness and effectiveness in dynamic networks thatexperience distribution shifts over time.</description>
      <author>example@mail.com (Ehssan Mousavipour, Andrey Dimanchev, Majid Ghaderi)</author>
      <guid isPermaLink="false">2508.15100v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans</title>
      <link>http://arxiv.org/abs/2508.14151v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了深度学习架构与可解释AI技术结合用于膝关节MRI扫描中感兴趣区域自动检测的有效性，发现ResNet50表现最佳，CNN-based迁移学习是最有效的方法。&lt;h4&gt;背景&lt;/h4&gt;MRI是评估膝盖损伤的重要诊断工具，但手动解释MRI切片既耗时又容易产生观察者间差异。&lt;h4&gt;目的&lt;/h4&gt;评估各种深度学习架构与可解释AI（xAI）技术结合的方法，用于膝关节MRI扫描中的感兴趣区域（ROI）自动检测。&lt;h4&gt;方法&lt;/h4&gt;研究监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers和多种U-Net变体结合MLP分类器；集成Grad-CAM和Saliency Maps等可解释AI方法；使用AUC评估分类性能，PSNR/SSIM评估重建质量，并进行定性ROI可视化。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet50在分类和ROI识别方面表现最佳，优于基于Transformer的模型；混合U-Net + MLP方法在利用空间特征方面有潜力，但分类性能较低；Grad-CAM提供了最具临床意义的解释。&lt;h4&gt;结论&lt;/h4&gt;基于CNN的迁移学习对于该数据集是最有效的方法；未来更大规模的预训练可能更好地释放Transformer模型的潜力。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像（MRI）是评估膝盖损伤的重要诊断工具。然而，手动解释MRI切片仍然耗时且容易产生观察者间差异。本研究对各种深度学习架构结合可解释AI（xAI）技术进行了系统性评估，用于膝关节MRI扫描中的感兴趣区域（ROI）自动检测。我们研究了监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers（ViT）和多种结合多层感知器（MLP）分类器的U-Net变体。为了增强可解释性和临床相关性，我们集成了Grad-CAM和Saliency Maps等可解释AI方法。模型性能使用AUC评估分类质量，使用PSNR/SSIM评估重建质量，并进行定性ROI可视化。我们的结果表明，ResNet50在分类和ROI识别方面持续表现出色，在MRNet数据集的约束下优于基于Transformer的模型。虽然混合U-Net + MLP方法在利用重建和可解释性的空间特征方面显示出潜力，但其分类性能仍然较低。Grad-CAM在各种架构中始终提供了最具临床意义的解释。总体而言，基于CNN的迁移学习成为该数据集最有效的方法，而未来更大规模的预训练可能更好地释放Transformer模型的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决MRI扫描中感兴趣区域(ROI)自动检测的问题，特别是针对膝盖MRI中的半月板损伤检测。这个问题重要是因为目前手动解释MRI切片耗时且容易产生观察者间差异，医生需要逐个检查扫描序列来识别关键区域和异常，这不仅效率低下，还可能导致诊断不一致。自动化ROI检测可以显著提高诊断速度、可靠性，并减少医疗错误，特别是在诊断半月板撕裂、ACL损伤等常见膝盖问题时具有重要临床价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有研究的局限性来设计方法，包括模型可解释性差、过度关注分类准确性而非精确定位、标注数据有限以及域转移敏感等问题。他们借鉴了多项现有工作：使用ResNet、InceptionV3和Vision Transformer等成熟架构；应用U-Net进行图像分割；整合Grad-CAM和Saliency Maps等可解释AI技术；采用迁移学习方法；基于斯坦福MRNet数据集进行研究。作者的创新在于将这些现有元素系统性地整合并进行比较，特别是在有限数据条件下评估不同架构的性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合深度学习模型与可解释AI技术，提高MRI中ROI检测的准确性和临床相关性，特别关注半月板损伤的自动检测。整体流程包括：1)使用MRNet数据集并专注于矢状面视图；2)实现多种模型架构(监督分类的ResNet50/InceptionV3/ViT，自监督重建的U-Net，以及混合的U-Net+MLP)；3)应用数据增强和正则化技术防止过拟合；4)使用AUC/准确率评估分类性能，PSNR/SSIM评估重建质量；5)通过Grad-CAM等技术生成热图提高可解释性；6)综合定量和定性分析比较不同方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统性评估多种深度学习架构与xAI方法的组合用于MRI ROI检测；2)引入自监督U-Net与MLP分类器相结合的混合方法并应用Grad-CAM；3)在有限数据条件下全面比较CNN和Transformer架构；4)同时评估分类准确性和重建质量以及临床相关的ROI可视化。相比之前工作，本文提供了更全面的比较框架，而非单一模型评估；不仅关注分类准确性，还重视ROI检测的精确性和临床相关性；在数据有限条件下证明了CNN迁移学习的优势，这与一些认为Transformer可能更优越的研究结论不同。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统比较多种深度学习架构和可解释AI方法，证明了在膝盖MRI半月板损伤检测中，基于CNN的迁移学习方法结合Grad-CAM技术能够提供最佳的性能和临床相关性，同时为未来在更大规模预训练数据上探索Transformer模型奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic Resonance Imaging (MRI) is an essential diagnostic tool forassessing knee injuries. However, manual interpretation of MRI slices remainstime-consuming and prone to inter-observer variability. This study presents asystematic evaluation of various deep learning architectures combined withexplainable AI (xAI) techniques for automated region of interest (ROI)detection in knee MRI scans. We investigate both supervised and self-supervisedapproaches, including ResNet50, InceptionV3, Vision Transformers (ViT), andmultiple U-Net variants augmented with multi-layer perceptron (MLP)classifiers. To enhance interpretability and clinical relevance, we integratexAI methods such as Grad-CAM and Saliency Maps. Model performance is assessedusing AUC for classification and PSNR/SSIM for reconstruction quality, alongwith qualitative ROI visualizations. Our results demonstrate that ResNet50consistently excels in classification and ROI identification, outperformingtransformer-based models under the constraints of the MRNet dataset. Whilehybrid U-Net + MLP approaches show potential for leveraging spatial features inreconstruction and interpretability, their classification performance remainslower. Grad-CAM consistently provided the most clinically meaningfulexplanations across architectures. Overall, CNN-based transfer learning emergesas the most effective approach for this dataset, while future work withlarger-scale pretraining may better unlock the potential of transformer models.</description>
      <author>example@mail.com (Justin Yiu, Kushank Arora, Daniel Steinberg, Rohit Ghiya)</author>
      <guid isPermaLink="false">2508.14151v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings</title>
      <link>http://arxiv.org/abs/2508.13672v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 34th ACM International Conference on Information and  Knowledge Management (CIKM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于实例的迁移学习LIME框架(ITL-LIME)，用于解决LIME方法在数据受限环境下的局部性和稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;可解释人工智能方法如LIME通过使用可解释的替代模型来近似黑盒机器学习模型的行为，提高了模型解释性。然而，LIME在扰动和采样中的固有随机性会导致局部性和稳定性问题，特别是在训练数据有限的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法来增强数据受限环境中模型解释的保真度和稳定性。&lt;h4&gt;方法&lt;/h4&gt;ITL-LIME框架引入实例迁移学习，利用来自相关源域的相关真实实例辅助目标域的解释过程。具体包括：使用聚类将源域划分为具有代表性原型的簇；检索与目标实例最相似的原型对应的源实例，并与目标实例的相邻实例组合；构建基于对比学习的编码器作为加权机制，根据接近程度为实例分配权重；使用加权的源实例和目标实例训练替代模型。&lt;h4&gt;主要发现&lt;/h4&gt;数据稀缺会导致LIME生成不真实的变体和偏离真实数据流形的样本，使替代模型无法准确近似原始模型的复杂决策边界。&lt;h4&gt;结论&lt;/h4&gt;ITL-LIME通过利用源域的相关实例和基于对比学习的加权机制，有效提高了数据受限环境下模型解释的保真度和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;可解释人工智能(XAI)方法，如局部可解释模型不可解释性(LIME)，通过使用可解释的替代模型来近似其局部行为，已推进了黑盒机器学习模型的解释性。然而，LIME在扰动和采样中的固有随机性会导致局部性和稳定性问题，特别是在训练数据有限的情况下。在这种情况下，数据稀缺可能导致生成不真实的变体和偏离真实数据流形的样本。因此，替代模型可能无法准确近似原始模型的复杂决策边界。为了解决这些挑战，我们提出了一种新颖的基于实例的迁移学习LIME框架(ITL-LIME)，该框架在数据受限环境中提高了解释的保真度和稳定性。ITL-LIME通过利用来自相关源域的相关真实实例来辅助目标域的解释过程，将实例迁移学习引入LIME框架。具体来说，我们采用聚类将源域划分为具有代表性原型的簇。与生成随机扰动不同，我们的方法从与目标实例最相似的原型对应的源簇中检索相关的真实源实例，然后将其与目标实例的相邻真实实例组合。为了定义一个紧凑的局部性，我们进一步构建了一个基于对比学习的编码器作为加权机制，根据实例与目标实例的接近程度为组合集中的实例分配权重。最后，使用这些加权的源实例和目标实例来训练用于解释目的的替代模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Explainable Artificial Intelligence (XAI) methods, such as LocalInterpretable Model-Agnostic Explanations (LIME), have advanced theinterpretability of black-box machine learning models by approximating theirbehavior locally using interpretable surrogate models. However, LIME's inherentrandomness in perturbation and sampling can lead to locality and instabilityissues, especially in scenarios with limited training data. In such cases, datascarcity can result in the generation of unrealistic variations and samplesthat deviate from the true data manifold. Consequently, the surrogate model mayfail to accurately approximate the complex decision boundary of the originalmodel. To address these challenges, we propose a novel Instance-based TransferLearning LIME framework (ITL-LIME) that enhances explanation fidelity andstability in data-constrained environments. ITL-LIME introduces instancetransfer learning into the LIME framework by leveraging relevant real instancesfrom a related source domain to aid the explanation process in the targetdomain. Specifically, we employ clustering to partition the source domain intoclusters with representative prototypes. Instead of generating randomperturbations, our method retrieves pertinent real source instances from thesource cluster whose prototype is most similar to the target instance. Theseare then combined with the target instance's neighboring real instances. Todefine a compact locality, we further construct a contrastive learning-basedencoder as a weighting mechanism to assign weights to the instances from thecombined set based on their proximity to the target instance. Finally, theseweighted source and target instances are used to train the surrogate model forexplanation purposes.</description>
      <author>example@mail.com (Rehan Raza, Guanjin Wang, Kok Wai Wong, Hamid Laga, Marco Fisichella)</author>
      <guid isPermaLink="false">2508.13672v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Fast globally optimal Truncated Least Squares point cloud registration with fixed rotation axis</title>
      <link>http://arxiv.org/abs/2508.15613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的线性时间凸松弛方法和contractor方法，用于加速点云配准问题的分支定界求解，显著提高了求解速度，能够在不到半秒内实现100个点的3D点云的全局最优配准。&lt;h4&gt;背景&lt;/h4&gt;点云配准在给定对应关系的情况下，使用截断最小平方方法可处理高达95%的离群率，但将这一组合优化问题求解到全局最优具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速且可证明全局最优的点云配准方法，解决现有SDP松弛方法速度慢的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的线性时间凸松弛方法以及一个加速分支定界（BnB）的contractor方法。&lt;h4&gt;主要发现&lt;/h4&gt;当旋转轴已知时，该求解器可以在不到半秒的时间内将两个包含100个点的3D点云以可证明的全局最优性进行配准；虽然不能解决完整的6DoF问题，但在解决仅旋转的TLS问题时，比最先进的SDP求解器STRIDE快两个数量级。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不仅提供了全局最优性的正式证明，还通过对抗性实例展示了局部最小值接近全局最小值的经验证据，证明了方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，在给定对应关系的情况下，使用截断最小平方（TLS）方法可以使点云配准对高达95%的离群率具有鲁棒性。然而，将这个组合优化问题求解到全局最优是具有挑战性的。使用半定规划（SDP）松弛的可证明全局最优方法处理100个点需要数百秒。在本文中，我们提出了一种新的线性时间凸松弛方法以及一个加速分支定界（BnB）的contractor方法。当旋转轴已知时，我们的求解器可以在不到半秒的时间内将两个包含100个点的3D点云以可证明的全局最优性进行配准。尽管它目前不能解决完整的6DoF问题，但在解决仅旋转的TLS问题时，比最先进的SDP求解器STRIDE快两个数量级。除了提供全局最优性的正式证明外，我们还通过具有接近全局最小值的局部最小值的对抗性实例展示了全局最优性的经验证据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中的全局最优性问题，特别是在处理高达95%异常值情况下的鲁棒性。这个问题在现实应用中非常重要，因为点云配准是场景重建、自动驾驶定位等应用的核心技术，而安全关键系统(如自动驾驶车辆)需要全局最优的算法来确保解决方案的正确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：启发式方法(如RANSAC、GNC)没有全局最优保证，而保证全局最优的方法(如基于SDP松弛的方法)计算速度太慢。作者选择分支定界(BnB)框架，但需要更高效的边界计算。他们借鉴了区间分析思想来计算残差范围，使用现有最小二乘求解器作为基础并进行修改，同时参考了现有的分支定界框架但改进了边界计算和搜索空间缩减方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一种称为OptiPose的分支定界求解器，结合基于区间分析的线性时间凸松弛(WLS松弛)和高效的区间收缩方法。整体流程包括：1)将搜索空间划分为子空间(节点)；2)对每个节点计算WLS凸松弛获得下界；3)使用区间收缩方法显著减少搜索空间；4)在每个节点求解WLS松弛问题；5)根据下界和上界关系决定剪枝、分支或继续搜索；6)当上界等于下界时返回全局最优解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)线性时间凸松弛方法(WLS松弛)，可在O(N)时间内计算下界；2)高效的区间收缩方法，显著减少搜索空间；3)自适应活动集求解器，用于解决带球约束的最小二乘旋转估计问题。相比之前的工作，OptiPose比STRIDE等SDP方法快两个数量级(100毫秒vs200秒)，能处理更高异常值率(超过90%)，且能保证全局最优性，而启发式方法没有这种保证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种快速的全局最优点云配准方法OptiPose，通过基于区间分析的线性时间凸松弛和高效的搜索空间缩减技术，在保证全局最优性的同时将计算速度提高了两个数量级，使点云配准在高达95%异常值率的情况下也能实时运行。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent results showed that point cloud registration with givencorrespondences can be made robust to outlier rates of up to 95\% using thetruncated least squares (TLS) formulation. However, solving this combinatorialoptimization problem to global optimality is challenging. Provably globallyoptimal approaches using semidefinite programming (SDP) relaxations takehundreds of seconds for 100 points. In this paper, we propose a novel lineartime convex relaxation as well as a contractor method to speed up Branch andBound (BnB). Our solver can register two 3D point clouds with 100 points toprovable global optimality in less than half a second when the axis of rotationis provided. Although it currently cannot solve the full 6DoF problem, it istwo orders of magnitude faster than the state-of-the-art SDP solver STRIDE whensolving the rotation-only TLS problem. In addition to providing a formal prooffor global optimality, we present empirical evidence of global optimality usingadversarial instances with local minimas close to the global minimum.</description>
      <author>example@mail.com (Ivo Ivanov, Carsten Markgraf)</author>
      <guid isPermaLink="false">2508.15613v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance</title>
      <link>http://arxiv.org/abs/2508.15650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为CFG的新型基于迁移的黑盒攻击方法，通过计算提取特征的重要性来规范对抗性点云的搜索，优先可能被多种架构采用的临界特征的破坏，并在损失函数中明确约束生成的对抗性点云的最大偏差程度，以确保其不可感知性。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络用于3D点云已被证明容易受到对抗性样本的影响。之前的3D对抗攻击方法通常利用有关目标模型的某些信息（如模型参数或输出）来生成对抗性点云。然而，在实际场景中，在绝对安全条件下获取有关目标模型的任何信息具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;专注于基于迁移的攻击，生成对抗性点云不需要任何有关目标模型的信息。基于对不同DNN架构中用于点云分类的关键特征一致性的观察，提出一种提高对抗性点云可迁移性的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出CFG，一种新颖的基于迁移的黑盒攻击方法，通过临界特征指导来提高对抗性点云的可迁移性。具体来说，该方法通过计算提取特征的重要性来规范对抗性点云的搜索，优先可能被多种架构采用的临界特征的破坏。此外，在损失函数中明确约束生成的对抗性点云的最大偏差程度，以确保其不可感知性。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40和ScanObjectNN基准数据集上进行的广泛实验表明，所提出的CFG方法以较大优势优于最先进的攻击方法。&lt;h4&gt;结论&lt;/h4&gt;CFG方法通过临界特征指导提高了对抗性点云的可迁移性，并在多个基准数据集上展示了优越的性能，为3D点云的安全研究提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络用于3D点云已被证明容易受到对抗性样本的影响。之前的3D对抗攻击方法通常利用有关目标模型的某些信息（如模型参数或输出）来生成对抗性点云。然而，在实际场景中，在绝对安全条件下获取有关目标模型的任何信息具有挑战性。因此，我们专注于基于迁移的攻击，生成对抗性点云不需要任何有关目标模型的信息。基于对不同DNN架构中用于点云分类的关键特征一致性的观察，我们提出CFG，一种新颖的基于迁移的黑盒攻击方法，通过提出的临界特征指导来提高对抗性点云的可迁移性。具体来说，我们的方法通过计算提取特征的重要性来规范对抗性点云的搜索，优先可能被多种架构采用的临界特征的破坏。此外，在损失函数中明确约束生成的对抗性点云的最大偏差程度，以确保其不可感知性。在ModelNet40和ScanObjectNN基准数据集上进行的广泛实验表明，所提出的CFG方法以较大优势优于最先进的攻击方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云深度学习模型的对抗性攻击迁移性问题，即在无法获取目标模型信息的情况下，如何生成能有效欺骗不同目标模型的对抗性点云。这个问题在现实中非常重要，因为3D点云模型广泛应用于自动驾驶、机器人等安全关键领域，而这些系统容易受到对抗性攻击的威胁；同时，在实际场景中，攻击者通常无法获取目标模型的详细信息，现有方法要么依赖白盒条件，要么需要大量查询，都不切实际。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察发现不同3D深度神经网络模型在识别点云时依赖的关键点存在显著重叠，这些关键点对应于物体类别的典型部位。基于这一发现，作者分析了现有迁移攻击方法的局限性，即生成的对抗性点云容易过度拟合到源模型。作者借鉴了2D图像领域的特征破坏攻击(FDA)和特征重要性感知的迁移攻击方法，以及AdvPC和PF-Attack等3D点云攻击方法的思想，设计了关键特征引导(CFG)策略，通过优先破坏跨模型共享的关键特征来提高迁移性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不同3D深度学习模型在识别点云时依赖的关键特征具有一致性，通过优先破坏这些跨模型共享的关键特征，可以减少对抗性点云对源模型的过拟合，从而提高其迁移性。整体实现流程包括：1)特征重要性评估，使用梯度回传获取特征梯度并评估重要性；2)关键特征破坏，为关键特征分配高权重并引导搜索方向；3)损失函数设计，结合分类错误损失、关键特征引导损失和Chamfer距离约束；4)优化过程，使用Adam优化器迭代搜索最优对抗性点云，并限制扰动幅度确保不可感知性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次观察到不同3D深度学习模型依赖的关键特征具有一致性；2)提出关键特征引导(CFG)策略，通过优先破坏共享关键特征提高迁移性；3)设计新的损失函数，结合多个目标和Chamfer距离约束；4)采用中间层攻击策略而非输出层攻击。相比之前的工作，本文专注于中间层的关键特征而非无差别破坏特征，通过优先破坏跨模型共享的关键特征显著提高了迁移性，并对潜在防御措施表现出更强的抵抗力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于关键特征引导的3D点云迁移黑盒攻击方法，通过优先破坏跨模型共享的关键特征，显著提高了对抗性点云在不同目标模型间的迁移性，为3D深度学习系统的安全性评估提供了更有效的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks for 3D point clouds have been demonstrated to bevulnerable to adversarial examples. Previous 3D adversarial attack methodsoften exploit certain information about the target models, such as modelparameters or outputs, to generate adversarial point clouds. However, inrealistic scenarios, it is challenging to obtain any information about thetarget models under conditions of absolute security. Therefore, we focus ontransfer-based attacks, where generating adversarial point clouds does notrequire any information about the target models. Based on our observation thatthe critical features used for point cloud classification are consistent acrossdifferent DNN architectures, we propose CFG, a novel transfer-based black-boxattack method that improves the transferability of adversarial point clouds viathe proposed Critical Feature Guidance. Specifically, our method regularizesthe search of adversarial point clouds by computing the importance of theextracted features, prioritizing the corruption of critical features that arelikely to be adopted by diverse architectures. Further, we explicitly constrainthe maximum deviation extent of the generated adversarial point clouds in theloss function to ensure their imperceptibility. Extensive experiments conductedon the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that theproposed CFG outperforms the state-of-the-art attack methods by a large margin.</description>
      <author>example@mail.com (Shuchao Pang, Zhenghan Chen, Shen Zhang, Liming Lu, Siyuan Liang, Anan Du, Yongbin Zhou)</author>
      <guid isPermaLink="false">2508.15650v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</title>
      <link>http://arxiv.org/abs/2508.15646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种弱监督方法用于机载激光扫描(ALS)数据的树木实例分割，通过人类操作员的初始质量评估来训练评分模型，进而微调分割模型，提高了树木识别准确率并减少了误识别。&lt;h4&gt;背景&lt;/h4&gt;机载激光扫描(ALS)数据中的树木实例分割对森林监测非常重要，但由于传感器分辨率、植被状态和地形特征等因素造成的数据变化，这仍然是一个挑战。此外，获取精确标注数据来训练完全监督的实例分割方法成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决树木实例分割中的挑战，减少对大量精确标注数据的依赖，提高分割准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种弱监督方法：1)人类操作员对初始分割结果（通过未微调模型或闭式算法获得）进行质量评级；2)使用这些评级训练评分模型，将分割输出分类为人类指定的类别；3)利用评分模型的反馈微调分割模型。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在正确识别的树木实例方面将原始分割模型提高了34%，同时显著减少了预测的非树木实例数量。&lt;h4&gt;结论&lt;/h4&gt;该方法有效提高了树木实例分割的准确性，但在树木稀疏区域（小树）或复杂环境（灌木、巨石等）中性能仍有下降，这些区域是未来需要改进的方向。&lt;h4&gt;翻译&lt;/h4&gt;机载激光扫描(ALS)数据的树木实例分割对森林监测至关重要，但由于传感器分辨率、获取时的植被状态、地形特征等因素造成的数据变化，这仍然具有挑战性。此外，获取足够数量的精确标注数据来训练完全监督的实例分割方法成本高昂。为应对这些挑战，我们提出了一种弱监督方法，其中人类操作员对初始分割结果（通过未微调模型或闭式算法获得）提供质量评级。质量评估过程中产生的标签用于训练评分模型，该模型的任务是将分割输出分类为与人类操作员指定的相同类别。最后，使用评分模型的反馈微调分割模型。这反过来将原始分割模型在正确识别的树木实例方面提高了34%，同时显著减少了预测的非树木实例数量。在树木稀疏区域（高度小于两米的小树）或包含灌木、巨石等复杂环境的周围区域仍然存在挑战，这些区域可能被误认为树木，导致所提出方法的性能降低。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从机载激光雷达点云中进行树木实例分割的挑战，以及获取足够精确标注数据的高昂成本问题。这个问题在现实中很重要，因为准确监测森林分布对评估木材资源、气候变化影响、斜坡稳定性和碳捕获能力至关重要，而传统方法需要大量人工标注，既耗时又昂贵。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何减少对精确标注数据的依赖，受强化学习从人类反馈的启发，提出了一种弱监督学习方法。他们设计了人类对分割结果进行评级而非精确标注的流程，并训练评级模型来模拟人类判断。作者借鉴了点云分类领域的多种架构（如PointNet、Point Transformer等）和现有的分割模型（如SegmentAnyTree），但创新性地将它们组合成一个迭代改进的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用弱监督学习和迭代改进机制：通过人类对初始分割结果的简单评级来训练一个评级模型，然后利用该模型提供的反馈来微调分割模型，形成良性循环。整体流程包括：1)使用预训练模型或算法获得初始分割；2)人类对样本聚类进行评级；3)训练评级模型；4)用评级模型对所有结果评分生成伪标签；5)用伪标签微调分割模型；6)迭代更新伪标签直到性能稳定。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)弱监督框架，用评级代替精确标注；2)迭代改进机制，通过模型间相互反馈提升性能；3)基于KDE的VoxNet架构，提高评级准确性；4)伪标签更新策略，有效识别新树实例。相比之前工作，本文方法大幅减少了标注成本，通过迭代过程持续改进分割性能，实验证明正确识别的树木实例数量提高了34%，同时减少了非树实例的误判。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的弱监督学习框架，通过人类对分割结果的评级来训练评级模型，进而指导分割模型的迭代改进，显著提高了树木实例分割的准确性同时大幅减少了标注数据的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tree instance segmentation of airborne laser scanning (ALS) data is of utmostimportance for forest monitoring, but remains challenging due to variations inthe data caused by factors such as sensor resolution, vegetation state atacquisition time, terrain characteristics, etc. Moreover, obtaining asufficient amount of precisely labeled data to train fully supervised instancesegmentation methods is expensive. To address these challenges, we propose aweakly supervised approach where labels of an initial segmentation resultobtained either by a non-finetuned model or a closed form algorithm areprovided as a quality rating by a human operator. The labels produced duringthe quality assessment are then used to train a rating model, whose task is toclassify a segmentation output into the same classes as specified by the humanoperator. Finally, the segmentation model is finetuned using feedback from therating model. This in turn improves the original segmentation model by 34\% interms of correctly identified tree instances while considerably reducing thenumber of non-tree instances predicted. Challenges still remain in data oversparsely forested regions characterized by small trees (less than two meters inheight) or within complex surroundings containing shrubs, boulders, etc. whichcan be confused as trees where the performance of the proposed method isreduced.</description>
      <author>example@mail.com (Swann Emilien Céleste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud)</author>
      <guid isPermaLink="false">2508.15646v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework</title>
      <link>http://arxiv.org/abs/2508.15457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需运动恢复结构(SfM)的3D高斯飞溅(3DGS)方法，能够从极其稀疏的视图输入中联合估计相机位置并重建3D场景。该方法通过密集立体匹配模块替代传统SfM初始化，并引入相干视图插值模块和多尺度正则化技术来增强重建质量。实验表明，在仅使用2个训练视图的极端稀疏条件下，该方法在PSNR指标上实现了2.75dB的提升，生成的图像失真最小且保留丰富高频细节，视觉质量显著优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅(3DGS)在新颖视图合成中展现出卓越的实时性能，但其有效性严重依赖于具有精确已知相机位置的密集多视角输入，这在现实场景中很少可用。当输入视图变得极其稀疏时，3DGS依赖的运动恢复结构(SfM)初始化方法无法准确重建场景的3D几何结构，导致渲染质量下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需SfM的3DGS方法，从极其稀疏的视图输入中联合估计相机位置并重建3D场景，解决传统方法在稀疏视图条件下的局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出密集立体匹配模块替代SfM，逐步估计相机位置信息并重建全局密集点云用于初始化；2) 提出相干视图插值模块，基于训练视图对插值相机位置，生成视角一致的内容作为训练的额外监督信号；3) 引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化，增强几何结构和渲染内容的质量。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法显著优于其他最先进的3DGS方法，在极其稀疏视图条件下(仅使用2个训练视图)，PSNR指标实现了2.75dB的显著提升。该方法合成的图像失真最小，同时保留了丰富的高频细节，与现有技术相比具有优越的视觉质量。&lt;h4&gt;结论&lt;/h4&gt;所提出的无需SfM的3DGS方法能够从极稀疏的视图输入中有效重建高质量3D场景，不依赖于传统的SfM初始化过程，在视觉质量和客观指标上都优于现有技术，为稀疏视图条件下的3D重建提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅(3DGS)在新颖视图合成中表现出卓越的实时性能，但其有效性严重依赖于具有精确已知相机位置的密集多视角输入，这在现实场景中很少可用。当输入视图变得极其稀疏时，3DGS依赖的运动恢复结构(SfM)初始化方法无法准确重建场景的3D几何结构，导致渲染质量下降。在本文中，我们提出了一种新颖的无需SfM的3DGS方法，能够从极其稀疏的视图输入中联合估计相机位置并重建3D场景。具体而言，我们提出密集立体匹配模块替代SfM，逐步估计相机位置信息并重建全局密集点云用于初始化。为解决极稀疏视图设置中固有的信息稀缺问题，我们提出相干视图插值模块，基于训练视图对插值相机位置，并生成视角一致的内容作为训练的额外监督信号。此外，我们引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化，以增强几何结构和渲染内容的质量。实验表明，我们的方法显著优于其他最先进的3DGS方法，在极其稀疏视图条件下(仅使用2个训练视图)实现了2.75dB的显著PSNR提升。我们方法合成的图像失真最小，同时保留了丰富的高频细节，与现有技术相比具有优越的视觉质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在极稀疏视图输入（如只有2个视图）下进行新颖视图合成的问题。这个问题在现实中很重要，因为真实世界场景中获取密集多视图图像和精确相机姿态往往是困难的，而极稀疏视图输入在移动设备拍照、监控视频等场景中更为常见。解决这一问题可以大大扩展3D场景重建技术的应用范围，使其更适用于虚拟现实、增强现实和3D内容创建等实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3DGS在稀疏视图下的局限性，特别是SfM在极稀疏视图下无法准确重建几何结构。他们设计了一个不依赖SfM的框架，通过密集立体模块直接估计相机姿态并重建点云。针对信息稀缺问题，他们引入视图插值生成额外监督信号。方法借鉴了3DGS作为基础渲染技术、密集立体匹配用于深度估计、视频扩散模型用于视图生成，以及多尺度正则化技术来提高质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不依赖SfM，而是通过密集立体模块直接从稀疏视图中估计相机姿态并重建点云，同时利用视图插值生成额外监督信号。整体流程：1)使用密集立体模块估计相机姿态并生成初始化点云；2)通过一致的视图插值模块生成中间视角图像作为额外监督；3)结合多尺度拉普拉斯一致正则化和自适应空间感知多尺度几何正则化提高渲染质量和几何结构；4)整体优化3DGS模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)密集立体模块替代SfM初始化；2)一致的视图插值模块生成额外监督信号；3)多尺度拉普拉斯一致正则化保持细节；4)自适应空间感知多尺度几何正则化增强几何结构。相比之前工作，该方法不依赖SfM和已知相机姿态，同时处理姿态估计和场景重建，通过视图插值解决信息稀缺问题，并在只有2个视图的极端情况下实现了显著性能提升(PSNR提高2.75dB)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种不依赖运动结构的3D高斯溅射框架，通过密集立体模块、视图插值和多尺度正则化技术，实现了在极稀疏视图输入下（如仅2个视图）的高质量新颖视图合成，显著提升了真实世界场景中3D重建的实用性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-timeperformance in novel view synthesis, yet its effectiveness relies heavily ondense multi-view inputs with precisely known camera poses, which are rarelyavailable in real-world scenarios. When input views become extremely sparse,the Structure-from-Motion (SfM) method that 3DGS depends on for initializationfails to accurately reconstruct the 3D geometric structures of scenes,resulting in degraded rendering quality. In this paper, we propose a novelSfM-free 3DGS-based method that jointly estimates camera poses and reconstructs3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, wepropose a dense stereo module to progressively estimates camera poseinformation and reconstructs a global dense point cloud for initialization. Toaddress the inherent problem of information scarcity in extremely sparse-viewsettings, we propose a coherent view interpolation module that interpolatescamera poses based on training view pairs and generates viewpoint-consistentcontent as additional supervision signals for training. Furthermore, weintroduce multi-scale Laplacian consistent regularization and adaptivespatial-aware multi-scale geometry regularization to enhance the quality ofgeometrical structures and rendered content. Experiments show that our methodsignificantly outperforms other state-of-the-art 3DGS-based approaches,achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-viewconditions (using only 2 training views). The images synthesized by our methodexhibit minimal distortion while preserving rich high-frequency details,resulting in superior visual quality compared to existing techniques.</description>
      <author>example@mail.com (Zongqi He, Hanmin Li, Kin-Chung Chan, Yushen Zuo, Hao Xie, Zhe Xiao, Jun Xiao, Kin-Man Lam)</author>
      <guid isPermaLink="false">2508.15457v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>A Low-Latency 3D Live Remote Visualization System for Tourist Sites Integrating Dynamic and Pre-captured Static Point Clouds</title>
      <link>http://arxiv.org/abs/2508.15398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 pages, 4 figures, submitted to IEEE ISMAR 2025 Posters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合多个激光雷达和相机的系统，用于实时捕捉动态点云并与静态点云集成，实现大区域3D可视化，解决了户外旅游景点中传感器放置受限和光照变化的问题。&lt;h4&gt;背景&lt;/h4&gt;现有基于RGB-D相机和体积捕捉的实时动态3D空间捕捉方法难以应用于户外旅游景点，因为维护和美学限制限制了传感器位置，且日光变化增加了处理复杂度。&lt;h4&gt;目的&lt;/h4&gt;开发一种系统，结合多个激光雷达和相机进行实时动态点云捕捉，并与预先捕获的静态点云集成，实现大区域3D可视化。&lt;h4&gt;方法&lt;/h4&gt;系统在大区域场景中维持30 fps的帧率，延迟低于100毫秒；通过自动调整静态点云颜色以适应当前光照条件，减轻光照不一致问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过在旅游景点进行实际部署，证明了所提系统的有效性。&lt;h4&gt;结论&lt;/h4&gt;该系统能够有效解决户外旅游景点中的实时3D捕捉和可视化挑战，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;已经提出了多种用于捕捉和传输动态3D空间的实时方法，包括基于RGB-D相机和体积捕捉的方法。然而，由于维护和美学限制限制了传感器的放置，以及日光变化使处理复杂化，将现有方法应用于户外旅游景点仍然困难。我们提出了一种结合多个激光雷达和相机进行实时动态点云捕捉的系统，并将其与预先捕获的静态点云集成，用于大区域3D可视化。该系统在大区域场景中保持30 fps的帧率，同时延迟低于100毫秒。为了减轻光照不一致性，静态点云颜色会自动调整到当前光照条件。通过在旅游景点进行实际部署，证明了我们系统的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决旅游景点低延迟3D远程可视化的问题。现有方法难以在户外环境中应用，因为维护和美学限制传感器放置，且日光变化影响处理。这个问题很重要，因为3D流媒体能提供更强的存在感、参与度和空间理解，比传统2D视频更有利于旅游推广和虚拟体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有3D捕获技术的局限性：静态方法无法实时捕捉变化，RGB-D相机范围有限，体积系统需要大量相机。作者设计时考虑使用多个LiDAR和相机组合，保护传感器外壳，并融合静态与动态点云。借鉴了颜色传输算法、CNN处理和PTP同步等技术，但针对3D点云进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合实时动态点云与预捕获静态点云，通过颜色调整使静态点云适应当前光照。流程包括：1)使用LiDAR和相机同步采集数据；2)处理数据解决闪烁、遮挡和稀疏问题；3)将静态点云分簇并调整颜色；4)将处理后的数据流传输到远程站点进行3D渲染。整个系统保持30fps帧率和低于100ms的延迟。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)特殊设计的防护外壳，使用蛾眼抗反射膜减少信号丢失；2)三种轻量级实时算法解决闪烁、遮挡误着色和稀疏性问题；3)改进的颜色传输方法，结合全局和局部适应；4)系统集成动态和静态点云。相比之前工作，此系统能处理更大面积场景，对光照变化更鲁棒，延迟更低，且传感器保护更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的低延迟3D远程可视化系统，通过结合实时动态点云与颜色调整的静态点云，成功实现了户外旅游景点的实时远程3D展示，为虚拟旅游提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Various real-time methods for capturing and transmitting dynamic 3D spaceshave been proposed, including those based on RGB-D cameras and volumetriccapture. However, applying existing methods to outdoor tourist sites remainsdifficult because maintenance and aesthetic constraints limit sensor placement,and daylight variability complicates processing. We propose a system thatcombines multiple LiDARs and cameras for live dynamic point cloud capture, andintegrates them with pre-captured static point clouds for wide-area 3Dvisualization. The system sustains 30 fps across wide-area scenes while keepinglatency below 100 ms. To mitigate lighting inconsistencies, static point-cloudcolors are automatically adjusted to current lighting. The effectiveness of oursystem is demonstrated through real-world deployment in a tourist site.</description>
      <author>example@mail.com (Takahiro Matsumoto, Masafumi Suzuki, Mariko Yamaguchi, Masakatsu Aoki, Shunsuke Konagai, Kazuhiko Murasaki)</author>
      <guid isPermaLink="false">2508.15398v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT</title>
      <link>http://arxiv.org/abs/2508.15299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to MMSports&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究构建了BasketLiDAR数据集，这是体育多目标跟踪领域首个结合激光雷达点云与同步多视角摄像机画面的多模态数据集，并提出了一种新的多目标跟踪框架，实现了提高跟踪精度和降低计算成本的双重目标。&lt;h4&gt;背景&lt;/h4&gt;实时3D轨迹球员跟踪在体育战术分析、表现评估和增强观众体验中起着至关重要的作用。传统系统依赖于多摄像机设置，但受限于视频数据的固有二维性质和复杂的3D重建处理需求，使得实时分析具有挑战性。篮球是多目标跟踪领域最困难的场景之一，因为十名球员在有限场地空间内快速复杂移动，且因激烈的身体接触导致频繁遮挡。&lt;h4&gt;目的&lt;/h4&gt;解决传统系统在篮球场景中实时3D轨迹跟踪面临的挑战，包括二维视频数据的局限性、复杂3D重建处理需求、球员快速复杂移动以及频繁遮挡问题。&lt;h4&gt;方法&lt;/h4&gt;构建BasketLiDAR数据集，结合激光雷达点云与同步多视角摄像机画面；开发一种新型多目标跟踪算法，利用激光雷达的高精度3D空间信息；提出两种跟踪流程：仅使用激光雷达的实时跟踪流程和融合激光雷达与摄像头数据的多模态跟踪流程。&lt;h4&gt;主要发现&lt;/h4&gt;BasketLiDAR数据集包含4,445帧和3,105个球员ID，三个激光雷达传感器和三个多视角摄像机之间的ID完全同步；记录了5对5和3对3专业篮球运动员的比赛数据，为每位球员提供完整的3D位置信息和ID标注；所提出的方法实现了实时操作，这是传统仅使用摄像机方法难以实现的；即使在遮挡条件下，该方法也能实现卓越的跟踪性能。&lt;h4&gt;结论&lt;/h4&gt;BasketLiDAR数据集和提出的跟踪框架有效解决了篮球场景中实时3D轨迹跟踪的挑战，提高了跟踪精度并降低了计算成本，为体育战术分析、表现评估和观众体验增强提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR(激光雷达)，MOT(多目标跟踪)，point clouds(点云)，occlusions(遮挡)，multi-camera setups(多摄像机设置)，3D reconstruction(3D重建)，synchronized(同步的)，multimodal(多模态的)&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统体育3D多目标跟踪系统的局限性问题。传统系统依赖多相机获取二维信息，需要复杂处理才能重建3D位置，计算成本高且难以实时分析。篮球场景特别具有挑战性，因为十名球员在有限场地内快速移动且频繁发生遮挡。解决这个问题对战术分析、球员表现评估和提升观赛体验至关重要，能推动数据驱动的体育分析发展，为球员、教练和观众提供价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考引入LiDAR传感器能否同时提高体育MOT精度并降低计算成本。他们分析了LiDAR可直接获取高精度3D点云的优势，消除从2D信息估计3D空间的复杂处理。作者借鉴了自动驾驶领域的多模态数据集和MOT技术，但指出体育场景与自动驾驶场景在跟踪目标、运动模式和传感器放置方面存在根本差异，因此需要专门设计。作者构建了BasketLiDAR数据集并设计了融合框架，结合LiDAR的空间信息和相机的丰富外观信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用LiDAR的高精度3D空间信息消除复杂的3D位置重建过程，降低计算负载并实现高精度3D估计；结合LiDAR和相机信息实现高精度识别；采用两阶段方法：先用LiDAR快速生成轨迹，再对遮挡部分应用相机重新识别。整体流程包括：1) LiDAR-only管道：集成多LiDAR数据、过滤区域、投影到鸟瞰图、检测和跟踪；2) LiDAR-camera融合管道：检测遮挡时段、匹配遮挡前后清晰帧、基于RGB特征重新识别链接相同球员。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 创建BasketLiDAR，首个结合LiDAR和相机的专业篮球多模态数据集；2) 提出融合框架，整合LiDAR跟踪和相机ID重新识别；3) 开源数据集和模型实现。相比之前工作：1) 现有数据集仅依赖RGB视频，而BasketLiDAR结合了LiDAR点云和多视角相机；2) 传统方法需复杂处理从2D估计3D，而本文通过LiDAR直接获取3D数据；3) 传统方法在遮挡场景表现不佳，本文方法即使在遮挡条件下也能保持优越性能；4) 传统方法计算成本高难以实时，本文方法实现了实时操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文创建了首个结合LiDAR和相机的专业篮球多模态数据集，并提出融合框架实现了高精度、实时的3D球员跟踪，即使在频繁遮挡的篮球场景中也能保持优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time 3D trajectory player tracking in sports plays a crucial role intactical analysis, performance evaluation, and enhancing spectator experience.Traditional systems rely on multi-camera setups, but are constrained by theinherently two-dimensional nature of video data and the need for complex 3Dreconstruction processing, making real-time analysis challenging. Basketball,in particular, represents one of the most difficult scenarios in the MOT field,as ten players move rapidly and complexly within a confined court space, withfrequent occlusions caused by intense physical contact.  To address these challenges, this paper constructs BasketLiDAR, the firstmultimodal dataset in the sports MOT field that combines LiDAR point cloudswith synchronized multi-view camera footage in a professional basketballenvironment, and proposes a novel MOT framework that simultaneously achievesimproved tracking accuracy and reduced computational cost. The BasketLiDARdataset contains a total of 4,445 frames and 3,105 player IDs, with fullysynchronized IDs between three LiDAR sensors and three multi-view cameras. Werecorded 5-on-5 and 3-on-3 game data from actual professional basketballplayers, providing complete 3D positional information and ID annotations foreach player. Based on this dataset, we developed a novel MOT algorithm thatleverages LiDAR's high-precision 3D spatial information. The proposed methodconsists of a real-time tracking pipeline using LiDAR alone and a multimodaltracking pipeline that fuses LiDAR and camera data. Experimental resultsdemonstrate that our approach achieves real-time operation, which was difficultwith conventional camera-only methods, while achieving superior trackingperformance even under occlusion conditions. The dataset is available uponrequest at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar</description>
      <author>example@mail.com (Ryunosuke Hayashi, Kohei Torimi, Rokuto Nagata, Kazuma Ikeda, Ozora Sako, Taichi Nakamura, Masaki Tani, Yoshimitsu Aoki, Kentaro Yoshioka)</author>
      <guid isPermaLink="false">2508.15299v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Multi-Modal Coding for High-Quality 3D Generation</title>
      <link>http://arxiv.org/abs/2508.15228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TriMM是一种创新的3D原生前馈生成模型，通过协作多模态编码和辅助监督机制，有效整合RGB、RGBD和点云等多模态数据，实现高质量3D资产生成。&lt;h4&gt;背景&lt;/h4&gt;3D内容具有多模态特性，不同模态在3D建模中各有优势（RGB提供纹理，点云提供几何）。现有3D生成架构主要局限于单模态或特定3D结构，忽略了多模态数据的互补优势，限制了可用训练数据范围。&lt;h4&gt;目的&lt;/h4&gt;全面利用多模态数据进行3D建模，提出首个从基础多模态（RGB、RGBD和点云）学习的3D原生前馈生成模型TriMM。&lt;h4&gt;方法&lt;/h4&gt;1) 引入协作多模态编码，整合模态特定特征同时保留其独特优势；2) 引入辅助2D和3D监督提高多模态编码鲁棒性和性能；3) 基于嵌入的多模态代码，使用三平面潜在扩散模型生成高质量3D资产，增强纹理和几何细节。&lt;h4&gt;主要发现&lt;/h4&gt;在多个知名数据集上的实验表明，TriMM有效利用多模态，仅用少量训练数据即可实现与大规模数据集训练模型相竞争的性能。在RGB-D数据集上的额外实验验证了将其他多模态数据集纳入3D生成的可行性。&lt;h4&gt;结论&lt;/h4&gt;TriMM成功利用多模态数据实现高质量3D资产生成，在有限数据集上达到与大规模数据集训练模型相竞争的性能，为3D内容生成提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;3D内容本质上包含多模态特征，可以投影到不同模态（例如RGB图像、RGBD和点云）。每种模态在3D资产建模中展现独特优势：RGB图像包含生动的3D纹理，而点云定义细粒度的3D几何形状。然而，大多数现有的3D原生生成架构主要在单模态范式下运行，从而忽略了多模态数据的互补优势，或者将自己局限于3D结构，限制了可用训练数据集的范围。为了全面利用多模态进行3D建模，我们提出了TriMM，这是第一个从基础多模态（例如RGB、RGBD和点云）学习的3D原生前馈生成模型。具体来说，1) TriMM首次引入协作多模态编码，整合模态特定特征，同时保留其独特的表示优势。2) 此外，引入辅助2D和3D监督，提高多模态编码的鲁棒性和性能。3) 基于嵌入的多模态代码，TriMM采用三平面潜在扩散模型生成高质量的3D资产，同时增强纹理和几何细节。在多个知名数据集上的大量实验表明，TriMM通过有效利用多模态，尽管只使用少量训练数据，但仍实现了在大规模数据集上训练的模型相竞争的性能。此外，我们在最近的RGB-D数据集上进行了额外实验，验证了将其他多模态数据集纳入3D生成的可行性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D内容生成中多模态信息利用不足的问题。现有方法大多只使用单一模态数据（如仅RGB图像），而忽视了RGB、RGBD和点云等不同模态数据的互补优势。这个问题很重要，因为高质量3D生成在虚拟现实、机器人模拟、工业设计和动画等领域有广泛应用，而通过有效利用多模态数据可以在有限的3D数据资源下显著提高生成质量和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了不同模态数据的优缺点：RGB提供丰富纹理但几何信息有限，点云和深度图像提供准确几何但缺乏纹理。基于此，设计了协作多模态编码框架，为每种模态设计专门编码器，并通过共享解码器映射到统一空间。方法借鉴了扩散模型（如Point-E）、三平面表示、DINOv2和PointNet等现有工作，但创新性地将它们整合到一个多模态框架中，并引入混合2D/3D监督和特定模态的重建损失来提高性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过协作多模态编码整合不同模态数据的优势，利用重建损失引导模型充分利用每种模态的优势同时避免其局限性。整体流程分两阶段：1）多模态编码阶段，使用专门的RGB、RGBD和点云编码器将输入转换为三平面表示，通过共享解码器映射到统一潜在空间，结合2D和3D监督损失进行训练；2）三平面潜在扩散模型阶段，使用VAE压缩特征，以CLIP图像嵌入为条件训练扩散模型，应用模态特定重建损失，最终生成高质量三平面并解码为3D网格。整个流程从单张RGB图像输入到生成最终3D纹理网格约需4秒。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）协作多模态编码（TriMM），首次整合RGB、RGBD和点云三种模态到统一3D生成框架；2）混合2D和3D监督，结合图像空间和几何空间损失提高鲁棒性；3）特定模态的重建损失机制，引导模型利用各模态优势；4）高效分阶段训练策略。相比之前工作，不同于单模态方法（如TripoSR）的局限性，也不同于TRELLIS等早期融合多模态方法，采用后融合策略提高可扩展性；相比优化方法（如DreamFusion）无需复杂优化，速度更快；相比重建方法（如LRM）增加了生成能力而非仅重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了TriMM，一种协作多模态编码方法，通过整合RGB、RGBD和点云数据的优势，在有限训练数据条件下实现了高质量、高效的3D内容生成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D content inherently encompasses multi-modal characteristics and can beprojected into different modalities (e.g., RGB images, RGBD, and point clouds).Each modality exhibits distinct advantages in 3D asset modeling: RGB imagescontain vivid 3D textures, whereas point clouds define fine-grained 3Dgeometries. However, most existing 3D-native generative architectures eitheroperate predominantly within single-modality paradigms-thus overlooking thecomplementary benefits of multi-modality data-or restrict themselves to 3Dstructures, thereby limiting the scope of available training datasets. Toholistically harness multi-modalities for 3D modeling, we present TriMM, thefirst feed-forward 3D-native generative model that learns from basicmulti-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMMfirst introduces collaborative multi-modal coding, which integratesmodality-specific features while preserving their unique representationalstrengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced toraise the robustness and performance of multi-modal coding. 3) Based on theembedded multi-modal code, TriMM employs a triplane latent diffusion model togenerate 3D assets of superior quality, enhancing both the texture and thegeometric detail. Extensive experiments on multiple well-known datasetsdemonstrate that TriMM, by effectively leveraging multi-modality, achievescompetitive performance with models trained on large-scale datasets, despiteutilizing a small amount of training data. Furthermore, we conduct additionalexperiments on recent RGB-D datasets, verifying the feasibility ofincorporating other multi-modal datasets into 3D generation.</description>
      <author>example@mail.com (Ziang Cao, Zhaoxi Chen, Liang Pan, Ziwei Liu)</author>
      <guid isPermaLink="false">2508.15228v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2508.15002v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新方法来合成大规模、多样化且物理可行的抓取方案，超越简单强力抓取，包括精细操作如捏取和三指精确抓取。作者引入了可微的力封闭能量公式和优化方法MALA*，显著提高了抓取多样性和预测稳定性，并提供了包含5700个物体的大型数据集。&lt;h4&gt;背景&lt;/h4&gt;灵巧的机器人手由于其多指设计的灵活性和适应性，能实现多样化交互，但充分利用其能力需要多样化和高质量的抓取数据。现有数据集生成方法通常依赖基于采样的算法或简化的力封闭分析，往往收敛到强力抓取且多样性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有抓取数据生成方法的局限性，提出一种能合成大规模、多样化且物理可行抓取方案的方法，包括超越简单强力抓取的精细操作。&lt;h4&gt;方法&lt;/h4&gt;引入严格可微的力封闭能量公式，通过二次规划(QP)隐式定义；提出调整的优化方法MALA*，通过基于能量值分布动态拒绝梯度步骤提高性能；评估方法在抓取多样性和稳定性方面的改进；创建包含5700个物体、五种夹持器和三种抓取类型的大规模数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能生成超越简单强力抓取的多样化抓取方案；在抓取多样性和最终抓取预测稳定性方面有显著改进；提供了大规模抓取数据集支持灵巧手的进一步应用。&lt;h4&gt;结论&lt;/h4&gt;通过可微的力封闭能量公式和优化的MALA*方法，成功解决了现有抓取数据生成方法的局限性，为灵巧手应用提供了更多样化、更精细且物理可行的抓取方案和数据支持。&lt;h4&gt;翻译&lt;/h4&gt;灵巧的机器人手由于其多指设计的灵活性和适应性，能够实现多样化的交互，允许在各种环境中执行特定任务的抓取配置。然而，要充分利用灵巧手的能力，获取多样化和高质量的抓取数据至关重要，无论是用于从点云开发抓取预测模型、训练操作策略，还是为高级任务规划提供更广泛的选择。现有的数据集生成方法通常依赖于基于采样的算法或简化的力封闭分析，这些方法往往收敛到强力抓取，且多样性有限。在这项工作中，我们提出了一种方法来合成大规模、多样化且物理可行的抓取方案，超越了简单的强力抓取，包括精细操作，如捏取和三指精确抓取。我们引入了一种严格可微的力封闭能量公式，通过二次规划(QP)隐式定义。此外，我们提出了一种调整的优化方法(MALA*)，通过基于所有样本能量值分布动态拒绝梯度步骤来提高性能。我们广泛评估了我们的方法，证明了在抓取多样性和最终抓取预测稳定性方面的显著改进。最后，我们提供了来自DexGraspNet的5,700个物体的大规模抓取数据集，包含五种不同的夹持器和三种不同的抓取类型。数据集和代码：https://graspqp.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous robotic hands enable versatile interactions due to the flexibilityand adaptability of multi-fingered designs, allowing for a wide range oftask-specific grasp configurations in diverse environments. However, to fullyexploit the capabilities of dexterous hands, access to diverse and high-qualitygrasp data is essential -- whether for developing grasp prediction models frompoint clouds, training manipulation policies, or supporting high-level taskplanning with broader action options. Existing approaches for datasetgeneration typically rely on sampling-based algorithms or simplifiedforce-closure analysis, which tend to converge to power grasps and oftenexhibit limited diversity. In this work, we propose a method to synthesizelarge-scale, diverse, and physically feasible grasps that extend beyond simplepower grasps to include refined manipulations, such as pinches and tri-fingerprecision grasps. We introduce a rigorous, differentiable energy formulation offorce closure, implicitly defined through a Quadratic Program (QP).Additionally, we present an adjusted optimization method (MALA*) that improvesperformance by dynamically rejecting gradient steps based on the distributionof energy values across all samples. We extensively evaluate our approach anddemonstrate significant improvements in both grasp diversity and the stabilityof final grasp predictions. Finally, we provide a new, large-scale graspdataset for 5,700 objects from DexGraspNet, comprising five different grippersand three distinct grasp types.  Dataset and Code:https://graspqp.github.io/</description>
      <author>example@mail.com (René Zurbrügg, Andrei Cramariuc, Marco Hutter)</author>
      <guid isPermaLink="false">2508.15002v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>TiP4GEN: Text to Immersive Panorama 4D Scene Generation</title>
      <link>http://arxiv.org/abs/2508.12415v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted In Proceedings of the 33rd ACM International Conference on  Multimedia (MM' 25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TiP4GEN框架，一个先进的文本到动态全景场景生成系统，能够实现细粒度内容控制并合成运动丰富、几何一致的全景4D场景，为VR/AR技术提供真正的360度沉浸式体验。&lt;h4&gt;背景&lt;/h4&gt;随着VR/AR技术的快速发展和广泛应用，对高质量、沉浸式动态场景的需求不断增长。然而，现有生成工作主要局限于静态场景或狭小视角动态场景，无法提供真正的360度沉浸式体验。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从任何视角提供真正360度沉浸式体验的动态全景场景生成框架，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;TiP4GEN整合全景视频生成和动态场景重建技术。视频生成采用双分支生成模型，包括负责全局视图的全景分支和负责局部视图的透视分支，并通过双向交叉注意力机制促进信息交换。场景重建则基于3D高斯溅射的几何对齐模型，使用度量深度图对齐时空点云，并利用估计的姿态初始化场景相机，确保几何一致性和时间连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，TiP4GEN在生成视觉上引人入胜且运动连贯的动态全景场景方面具有优越性，其设计的有效性得到了充分验证。&lt;h4&gt;结论&lt;/h4&gt;TiP4GEN成功解决了现有生成方法的局限性，能够创建真正沉浸式的360度动态虚拟环境，为VR/AR应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;随着VR/AR技术的快速发展和广泛采用，对创建高质量、沉浸式动态场景的需求日益增长。然而，现有的生成工作主要集中在静态场景或狭小视角动态场景的创建上，无法从任何视角提供真正的360度沉浸式体验。在本文中，我们介绍了TiP4GEN，一个先进的文本到动态全景场景生成框架，能够实现细粒度内容控制并合成运动丰富、几何一致的全景4D场景。TiP4GEN整合了全景视频生成和动态场景重建，以创建360度沉浸式虚拟环境。对于视频生成，我们引入了一个由全景分支和透视分支组成的双分支生成模型，分别负责全局和局部视图的生成。双向交叉注意力机制促进了分支之间的全面信息交换。对于场景重建，我们提出了一个基于3D高斯溅射的几何对齐重建模型。通过使用度量深度图对齐时空点云，并使用估计的姿态初始化场景相机，我们的方法确保了重建场景的几何一致性和时间连贯性。大量实验证明了我们提出设计的有效性和TiP4GEN在生成视觉上引人入胜且运动连贯的动态全景场景方面的优越性。我们的项目页面是https://ke-xing.github.io/TiP4GEN/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有生成模型无法创建真正360度沉浸式动态全景场景的问题。随着VR/AR技术快速发展，游戏、娱乐和教育等领域对高质量沉浸式体验需求日益增长，但现有方法要么局限于静态场景，要么只能生成狭窄视角的动态场景，无法提供从任意角度观察的沉浸式体验，限制了虚拟现实应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用两阶段方法：首先生成语义丰富的全景视频，然后重建动态4D场景。设计上借鉴了扩散模型(特别是AnimateDiff的UNet架构)、3D高斯泼溅技术(3DGS)、深度估计方法(DPT和Monst3r)和球形位置编码等现有工作。通过双分支架构结合全景分支(确保全局一致性)和视角分支(增强局部多样性)，并引入双向交叉注意力机制促进信息交换，解决了全景视频生成的数据稀缺和全局一致性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双分支架构结合全局一致性和局部多样性，并利用几何对齐确保空间和时间一致性。整体流程分为两个阶段：1)双分支全景视频生成：全景分支接收全局文本提示生成全景视频，视角分支接收多个本地文本提示生成多个视角视频，通过双向交叉注意力机制融合信息；2)几何对齐重建：利用深度估计进行空间对齐，使用Monst3r进行时间对齐，估计相机姿态初始化场景，最后通过3D高斯泼溅优化重建4D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)TiP4GEN框架，实现文本到动态全景4D场景生成；2)双分支生成模型，结合全景和视角分支并通过双向交叉注意力机制融合；3)几何对齐重建模型，利用空间和时间对齐确保几何一致性。相比之前工作，本文不仅生成静态3D场景，还创建动态4D全景场景；不仅提供单一全局控制，还支持多局部精细控制；不仅独立处理每帧，还保持时间连贯性；适应球面投影特性而非简单调整传统视频模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TiP4GEN通过创新的双分支生成和几何对齐重建框架，实现了从文本生成具有全局一致性、局部细节多样性和时间连贯性的360度沉浸式动态4D场景，显著提升了虚拟现实环境中场景生成的质量和沉浸感。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement and widespread adoption of VR/AR technologies,there is a growing demand for the creation of high-quality, immersive dynamicscenes. However, existing generation works predominantly concentrate on thecreation of static scenes or narrow perspective-view dynamic scenes, fallingshort of delivering a truly 360-degree immersive experience from any viewpoint.In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamicpanorama scene generation framework that enables fine-grained content controland synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GENintegrates panorama video generation and dynamic scene reconstruction to create360-degree immersive virtual environments. For video generation, we introduce a\textbf{Dual-branch Generation Model} consisting of a panorama branch and aperspective branch, responsible for global and local view generation,respectively. A bidirectional cross-attention mechanism facilitatescomprehensive information exchange between the branches. For scenereconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds usingmetric depth maps and initializing scene cameras with estimated poses, ourmethod ensures geometric consistency and temporal coherence for thereconstructed scenes. Extensive experiments demonstrate the effectiveness ofour proposed designs and the superiority of TiP4GEN in generating visuallycompelling and motion-coherent dynamic panoramic scenes. Our project page is athttps://ke-xing.github.io/TiP4GEN/.</description>
      <author>example@mail.com (Ke Xing, Hanwen Liang, Dejia Xu, Yuyang Yin, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei)</author>
      <guid isPermaLink="false">2508.12415v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning</title>
      <link>http://arxiv.org/abs/2508.15471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, BDA Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为SLM4Offer的生成式AI模型，用于个性化优惠生成，通过对比学习方法微调预训练语言模型，实现了比监督微调基线高17%的优惠接受率。&lt;h4&gt;背景&lt;/h4&gt;个性化营销是提升客户参与度和推动业务增长的关键策略，学术界和行业主要关注推荐系统和个性化广告。研究表明，良好的个性化策略可以将收入提高高达40%。&lt;h4&gt;目的&lt;/h4&gt;开发智能的、数据驱动的个性化优惠生成方法，以提高转化率和客户满意度。&lt;h4&gt;方法&lt;/h4&gt;SLM4Offer是一个基于Google的T5-Small 60M预训练语言模型的生成式AI模型，采用对比学习方法进行微调。模型使用InfoNCE损失函数将客户画像与相关优惠在共享嵌入空间中对齐，并通过对比损失引入的自适应学习行为重塑潜在空间，增强模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟客户行为和优惠接受模式的合成数据集上，SLM4Offer实现了比监督微调基线高17%的优惠接受率，证明了对比目标在个性化营销中的有效性。&lt;h4&gt;结论&lt;/h4&gt;对比学习方法在个性化优惠生成中表现出色，SLM4Offer模型的自适应学习行为能够有效改善模型性能，为个性化营销提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;个性化营销已成为提升客户参与度和推动业务增长的关键策略。学术界和行业的努力主要集中在推荐系统和个性化广告上。然而，个性化营销在提高转化率和改善客户满意度方面具有巨大潜力。先前研究表明，良好的个性化策略可以将收入提高高达40%，这凸显了开发用于优惠生成的智能数据驱动方法的重要性。本研究介绍了SLM4Offer，一个用于个性化优惠生成的生成式AI模型，通过微调预训练的编码器-解码器语言模型（特别是Google的文本到文本转换器T5-Small 60M）开发，采用对比学习方法。SLM4Offer使用InfoNCE（信息噪声对比估计）损失将客户画像与相关优惠在共享嵌入空间中对齐。SLM4Offer的关键创新在于对比损失引入的自适应学习行为，它在训练过程中重塑潜在空间并增强模型的泛化能力。该模型在模拟客户行为和优惠接受模式的合成数据集上进行微调和评估。实验结果表明，与监督微调基线相比，优惠接受率提高了17%，突显了对比目标在推进个性化营销方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized marketing has emerged as a pivotal strategy for enhancingcustomer engagement and driving business growth. Academic and industry effortshave predominantly focused on recommendation systems and personalizedadvertisements. Nonetheless, this facet of personalization holds significantpotential for increasing conversion rates and improving customer satisfaction.Prior studies suggest that well-executed personalization strategies can boostrevenue by up to 40 percent, underscoring the strategic importance ofdeveloping intelligent, data-driven approaches for offer generation. This workintroduces SLM4Offer, a generative AI model for personalized offer generation,developed by fine-tuning a pre-trained encoder-decoder language model,specifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using acontrastive learning approach. SLM4Offer employs InfoNCE (InformationNoise-Contrastive Estimation) loss to align customer personas with relevantoffers in a shared embedding space. A key innovation in SLM4Offer lies in theadaptive learning behaviour introduced by contrastive loss, which reshapes thelatent space during training and enhances the model's generalizability. Themodel is fine-tuned and evaluated on a synthetic dataset designed to simulatecustomer behaviour and offer acceptance patterns. Experimental resultsdemonstrate a 17 percent improvement in offer acceptance rate over a supervisedfine-tuning baseline, highlighting the effectiveness of contrastive objectivesin advancing personalized marketing.</description>
      <author>example@mail.com (Vedasamhitha Challapalli, Konduru Venkat Sai, Piyush Pratap Singh, Rupesh Prasad, Arvind Maurya, Atul Singh)</author>
      <guid isPermaLink="false">2508.15471v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Long-term User Behaviors with Diffusion-driven Multi-interest Network for CTR Prediction</title>
      <link>http://arxiv.org/abs/2508.15311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DiffuMIN模型，通过扩散驱动的多兴趣网络来建模长期用户行为和探索用户兴趣空间，有效提升了CTR预测性能。&lt;h4&gt;背景&lt;/h4&gt;CTR预测对推荐系统和在线广告至关重要，长期用户行为建模已被证明有益，但大量行为和噪声干扰的复杂性构成挑战。当前双阶段模型常过滤掉重要信息，无法捕捉多样化用户兴趣并构建完整的用户兴趣潜在空间。&lt;h4&gt;目的&lt;/h4&gt;提出DiffuMIN模型来建模长期用户行为并彻底探索用户兴趣空间，解决现有方法在捕捉用户兴趣方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出面向目标的多兴趣提取方法，通过正交分解目标获得兴趣通道；2) 建模兴趣通道与用户行为间关系，解耦提取多个用户兴趣；3) 采用由上下文兴趣和兴趣通道引导的扩散模块，锚定用户个性化和目标导向兴趣类型；4) 生成与用户兴趣潜在空间一致的增强兴趣；5) 利用对比学习确保生成兴趣与用户真实偏好一致。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集和一个工业数据集上的离线实验验证了DiffuMIN的优越性；在线A/B测试显示，DiffuMIN使CTR提高了1.52%，CPM提高了1.10%。&lt;h4&gt;结论&lt;/h4&gt;DiffuMIN能有效建模长期用户行为和探索用户兴趣空间，显著提升了CTR预测性能，在工业应用中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为中文，无需翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3705328.3748045&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CTR (Click-Through Rate) prediction, crucial for recommender systems andonline advertising, etc., has been confirmed to benefit from modeling long-termuser behaviors. Nonetheless, the vast number of behaviors and complexity ofnoise interference pose challenges to prediction efficiency and effectiveness.Recent solutions have evolved from single-stage models to two-stage models.However, current two-stage models often filter out significant information,resulting in an inability to capture diverse user interests and build thecomplete latent space of user interests. Inspired by multi-interest andgenerative modeling, we propose DiffuMIN (Diffusion-driven Multi-InterestNetwork) to model long-term user behaviors and thoroughly explore the userinterest space. Specifically, we propose a target-oriented multi-interestextraction method that begins by orthogonally decomposing the target to obtaininterest channels. This is followed by modeling the relationships betweeninterest channels and user behaviors to disentangle and extract multiple userinterests. We then adopt a diffusion module guided by contextual interests andinterest channels, which anchor users' personalized and target-orientedinterest types, enabling the generation of augmented interests that align withthe latent spaces of user interests, thereby further exploring restrictedinterest space. Finally, we leverage contrastive learning to ensure that thegenerated augmented interests align with users' genuine preferences. Extensiveoffline experiments are conducted on two public datasets and one industrialdataset, yielding results that demonstrate the superiority of DiffuMIN.Moreover, DiffuMIN increased CTR by 1.52% and CPM by 1.10% in online A/Btesting. Our source code is available athttps://github.com/laiweijiang/DiffuMIN.</description>
      <author>example@mail.com (Weijiang Lai, Beihong Jin, Yapeng Zhang, Yiyuan Zheng, Rui Zhao, Jian Dong, Jun Lei, Xingxing Wang)</author>
      <guid isPermaLink="false">2508.15311v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding</title>
      <link>http://arxiv.org/abs/2508.15297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP 2025. 22 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DesignCLIP的统一框架，利用CLIP模型改进设计专利分析，解决了传统专利图像无法充分传达视觉上下文和语义信息的问题。&lt;h4&gt;背景&lt;/h4&gt;在设计专利分析领域，传统任务如专利分类和专利图像检索高度依赖图像数据。然而，专利图像（通常包含发明的抽象和结构元素的草图）往往无法传达全面的视觉上下文和语义信息，这可能导致在现有技术搜索评估过程中产生歧义。&lt;h4&gt;目的&lt;/h4&gt;利用最新的视觉-语言模型（如CLIP）开发更可靠、更准确的AI驱动专利分析框架，解决传统专利图像的局限性。&lt;h4&gt;方法&lt;/h4&gt;作者利用CLIP模型开发了名为DesignCLIP的统一框架，用于设计专利应用，并使用了美国设计专利的大规模数据集。为解决专利数据的独特特性，DesignCLIP纳入了类感知分类和对比学习，利用生成的专利图像详细字幕和多视图图像学习。&lt;h4&gt;主要发现&lt;/h4&gt;作者在各种下游任务（包括专利分类和专利检索）中验证了DesignCLIP的有效性。此外，他们探索了多模态专利检索，这可以通过提供更多样化的灵感来源来增强设计的创造力和创新。实验表明，DesignCLIP在所有任务上都一致地优于基线和最新模型。&lt;h4&gt;结论&lt;/h4&gt;多模态方法在推进专利分析方面具有广阔前景，能够提供更可靠、更准确的AI驱动专利分析。&lt;h4&gt;翻译&lt;/h4&gt;在设计专利分析领域，诸如专利分类和专利图像检索等传统任务高度依赖图像数据。然而，专利图像--通常包含发明的抽象和结构元素的草图--往往无法传达全面的视觉上下文和语义信息。这种不足可能导致在现有技术搜索评估过程中产生歧义。视觉-语言模型的最新进展，如CLIP，为更可靠、更准确的AI驱动专利分析提供了有希望的机会。在本工作中，我们利用CLIP模型开发了一个名为DesignCLIP的统一框架，用于设计专利应用，并使用了美国设计专利的大规模数据集。为解决专利数据的独特特性，DesignCLIP纳入了类感知分类和对比学习，利用生成的专利图像详细字幕和多视图图像学习。我们在各种下游任务中验证了DesignCLIP的有效性，包括专利分类和专利检索。此外，我们探索了多模态专利检索，这可以通过提供更多样化的灵感来源来增强设计的创造力和创新。我们的实验表明，DesignCLIP在所有任务上都一致地优于基线和最新模型。我们的研究结果强调了多模态方法在推进专利分析方面的潜力。代码库可在以下网址获取：https://anonymous.4open.science/r/PATENTCLIP-4661/README.md。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of design patent analysis, traditional tasks such as patentclassification and patent image retrieval heavily depend on the image data.However, patent images -- typically consisting of sketches with abstract andstructural elements of an invention -- often fall short in conveyingcomprehensive visual context and semantic information. This inadequacy can leadto ambiguities in evaluation during prior art searches. Recent advancements invision-language models, such as CLIP, offer promising opportunities for morereliable and accurate AI-driven patent analysis. In this work, we leverage CLIPmodels to develop a unified framework DesignCLIP for design patent applicationswith a large-scale dataset of U.S. design patents. To address the uniquecharacteristics of patent data, DesignCLIP incorporates class-awareclassification and contrastive learning, utilizing generated detailed captionsfor patent images and multi-views image learning. We validate the effectivenessof DesignCLIP across various downstream tasks, including patent classificationand patent retrieval. Additionally, we explore multimodal patent retrieval,which provides the potential to enhance creativity and innovation in design byoffering more diverse sources of inspiration. Our experiments show thatDesignCLIP consistently outperforms baseline and SOTA models in the patentdomain on all tasks. Our findings underscore the promise of multimodalapproaches in advancing patent analysis. The codebase is available here:https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.</description>
      <author>example@mail.com (Zhu Wang, Homaira Huda Shomee, Sathya N. Ravi, Sourav Medya)</author>
      <guid isPermaLink="false">2508.15297v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Center-Oriented Prototype Contrastive Clustering</title>
      <link>http://arxiv.org/abs/2508.15231v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种面向中心的原型对比聚类框架，通过软原型对比模块和双一致性学习模块解决对比学习在聚类任务中的类别冲突问题，实验证明该方法有效。&lt;h4&gt;背景&lt;/h4&gt;对比学习因其判别性表示在聚类任务中被广泛使用，但类别间的冲突问题难以有效解决。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中硬原型计算与真实聚类中心之间的偏差问题，提高聚类性能。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含软原型对比模块和双一致性学习模块的中心导向原型对比聚类框架。软原型对比模块使用样本属于聚类中心的概率作为权重计算原型，避免类间冲突并减少原型漂移；双一致性学习模块对齐同一样本的不同变换和不同样本的邻域，确保特征具有变换不变的语义信息和紧凑的簇内分布。&lt;h4&gt;主要发现&lt;/h4&gt;在五个数据集上的大量实验表明，所提出的方法与当前最先进的方法相比是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的中心导向原型对比聚类框架能有效解决对比学习在聚类任务中的类别冲突问题，提高聚类性能。&lt;h4&gt;翻译&lt;/h4&gt;对比学习因其判别性表示在聚类任务中被广泛使用。然而，类别间的冲突问题难以有效解决。现有方法尝试通过原型对比解决此问题，但硬原型的计算与真实聚类中心之间存在偏差。为解决这一问题，我们提出了一种面向中心的原型对比聚类框架，包含软原型对比模块和双一致性学习模块。简而言之，软原型对比模块使用样本属于聚类中心的概率作为权重计算每个类别的原型，同时避免类间冲突并减少原型漂移。双一致性学习模块分别对齐同一样本的不同变换和不同样本的邻域，确保特征具有变换不变的语义信息和紧凑的簇内分布，同时为原型计算提供可靠保证。在五个数据集上的大量实验表明，与当前最先进的方法相比，所提出的方法是有效的。我们的代码已发布在https://github.com/LouisDong95/CPCC。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning is widely used in clustering tasks due to itsdiscriminative representation. However, the conflict problem between classes isdifficult to solve effectively. Existing methods try to solve this problemthrough prototype contrast, but there is a deviation between the calculation ofhard prototypes and the true cluster center. To address this problem, wepropose a center-oriented prototype contrastive clustering framework, whichconsists of a soft prototype contrastive module and a dual consistency learningmodule. In short, the soft prototype contrastive module uses the probabilitythat the sample belongs to the cluster center as a weight to calculate theprototype of each category, while avoiding inter-class conflicts and reducingprototype drift. The dual consistency learning module aligns differenttransformations of the same sample and the neighborhoods of different samplesrespectively, ensuring that the features have transformation-invariant semanticinformation and compact intra-cluster distribution, while providing reliableguarantees for the calculation of prototypes. Extensive experiments on fivedatasets show that the proposed method is effective compared to the SOTA. Ourcode is published on https://github.com/LouisDong95/CPCC.</description>
      <author>example@mail.com (Shihao Dong, Xiaotong Zhou, Yuhui Zheng, Huiying Xu, Xinzhong Zhu)</author>
      <guid isPermaLink="false">2508.15231v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment</title>
      <link>http://arxiv.org/abs/2508.15130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HiRQA（分层排序与质量对齐）的自监督无参考图像质量评估框架，通过结合排序和对比学习实现分层质量感知嵌入，克服了传统方法对数据集偏见和主观标签的依赖，在各种真实图像退化条件下展现出优异的泛化性能。&lt;h4&gt;背景&lt;/h4&gt;尽管无参考图像质量评估(NR-IQA)取得了显著进展，但数据集偏见和对主观标签的依赖仍然阻碍了其泛化性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种自监督、无意见感知的框架HiRQA，仅使用输入图像预测质量分数，并在合成失真训练后能有效泛化到真实退化场景。&lt;h4&gt;方法&lt;/h4&gt;HiRQA结合了高阶排序损失（通过对失真对的关系排序监督质量预测）、嵌入距离损失（强制特征距离与感知差异一致）以及训练时由结构化文本提示引导的对比对齐损失。此外，还提出了轻量级变体HiRQA-S，推理时间仅需3.5毫秒每图像。&lt;h4&gt;主要发现&lt;/h4&gt;HiRQA仅在合成失真上训练，但能有效泛化到真实退化场景，如镜头光晕、雾霾、运动模糊和低光条件等。大量实验验证了其达到最先进(SOTA)的性能、强大的泛化能力和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;HiRQA通过创新的分层排序与质量对齐技术，成功克服了传统NR-IQA方法的局限性，在保持高性能的同时实现了优秀的泛化能力和实时应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;尽管无参考图像质量评估(NR-IQA)取得了显著进展，但数据集偏见和对主观标签的依赖仍然阻碍了其泛化性能。我们提出HiRQA（分层排序与质量对齐），一个自监督的、无意见感知的框架，通过排序和对比学习的组合提供分层质量感知嵌入。与依赖原始参考或辅助模态的前期方法不同，HiRQA仅使用输入图像预测质量分数。我们引入了一种新颖的高阶排序损失，通过对失真对的关系排序监督质量预测，以及嵌入距离损失强制特征距离与感知差异之间的一致性。由结构化文本提示引导的训练时对比对齐损失，进一步增强了学习的表示。仅在合成失真上训练的HiRQA能有效泛化到真实退化，如在镜头光晕、雾霾、运动模糊和低光条件等各种失真上的评估所证明。为实时部署，我们引入了HiRQA-S，一个轻量级变体，每张图像推理时间仅为3.5毫秒。在合成和真实基准上的大量实验验证了HiRQA的最先进(SOTA)性能、强大的泛化能力和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in no-reference image quality assessment(NR-IQA), dataset biases and reliance on subjective labels continue to hindertheir generalization performance. We propose HiRQA, Hierarchical Ranking andQuality Alignment), a self-supervised, opinion-unaware framework that offers ahierarchical, quality-aware embedding through a combination of ranking andcontrastive learning. Unlike prior approaches that depend on pristinereferences or auxiliary modalities at inference time, HiRQA predicts qualityscores using only the input image. We introduce a novel higher-order rankingloss that supervises quality predictions through relational ordering acrossdistortion pairs, along with an embedding distance loss that enforcesconsistency between feature distances and perceptual differences. Atraining-time contrastive alignment loss, guided by structured textual prompts,further enhances the learned representation. Trained only on syntheticdistortions, HiRQA generalizes effectively to authentic degradations, asdemonstrated through evaluation on various distortions such as lens flare,haze, motion blur, and low-light conditions. For real-time deployment, weintroduce \textbf{HiRQA-S}, a lightweight variant with an inference time ofonly 3.5 ms per image. Extensive experiments across synthetic and authenticbenchmarks validate HiRQA's state-of-the-art (SOTA) performance, stronggeneralization ability, and scalability.</description>
      <author>example@mail.com (Vaishnav Ramesh, Haining Wang, Md Jahidul Islam)</author>
      <guid isPermaLink="false">2508.15130v1</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.14278v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GALA的新框架，用于基于3D高斯溅射的开词汇表3D场景理解，通过自监督对比学习和交叉注意力模块实现高效的3D表示和查询。&lt;h4&gt;背景&lt;/h4&gt;3D场景重建和理解越来越受欢迎，但现有方法难以从2D图像中捕捉细粒度的、语言感知的3D表示。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为GALA的框架，用于基于3D高斯溅射的开词汇表3D场景理解。&lt;h4&gt;方法&lt;/h4&gt;GALA通过自监督对比学习蒸馏场景特定的3D实例特征场，并引入一个带有两个可学习码本的交叉注意力模块来编码视图无关的语义嵌入，确保实例内特征相似性并支持2D和3D开词汇表查询。&lt;h4&gt;主要发现&lt;/h4&gt;GALA的设计减少了内存消耗，通过避免每个高斯体的高维特征学习，同时在2D和3D开词汇表任务上表现出色。&lt;h4&gt;结论&lt;/h4&gt;在真实世界数据集上的大量实验证明了GALA在2D和3D开词汇表任务上的卓越性能。&lt;h4&gt;翻译&lt;/h4&gt;3D场景重建和理解日益流行，然而现有方法仍然难以从2D图像中捕捉细粒度的、语言感知的3D表示。在本文中，我们提出了GALA，一个基于3D高斯溅射的开词汇表3D场景理解的新框架。GALA通过自监督对比学习蒸馏场景特定的3D实例特征场。为了扩展到通用的语言特征场，我们引入了GALA的核心贡献——一个带有两个可学习码本的交叉注意力模块，这些码本编码视图无关的语义嵌入。这种设计不仅确保了实例内特征的相似性，还支持无缝的2D和3D开词汇表查询。它通过避免每个高斯体的高维特征学习来减少内存消耗。在真实世界数据集上的大量实验证明了GALA在2D和3D开词汇表任务上的卓越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景中的开放词汇理解和语义一致性问题。这个问题在现实中非常重要，因为它使机器人能够通过自然语言与人类交互，理解环境中的对象；帮助自动驾驶系统识别复杂环境中的各种物体；增强AR/VR应用的用户体验；并为机器人导航、物体操作等任务提供精确的语义分割。现有方法存在信息损失、实例一致性差、2D和3D性能不平衡以及计算效率低等问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先批判性分析了现有方法的局限性，如NeRF效率低、3DGS方法在语义处理上不足、压缩高维特征导致信息损失、聚类方法可能导致实例分割错误等。作者提出应关注实例级别的语义一致性，为每个实例学习共享的语言嵌入而非每个高斯独立存储特征。方法借鉴了Scaffold-GS的场景重建、SAM的实例分割、CLIP的语言对齐以及对比学习等技术，同时创新性地引入了双代码本设计和引导注意力机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过实例级别语义一致性、双代码本设计和引导注意力机制实现开放词汇的3D场景理解。整体流程分为两个阶段：阶段1使用Scaffold-GS重建场景几何结构，并通过自监督对比学习提取场景特定的实例特征场；阶段2使用引导注意力模块将场景特定特征通过双代码本映射到通用语言特征，并通过注意力加权熵损失促进特征和代码本间的一对一映射。推理时支持2D和3D的开放词汇查询，且仅通过2D监督训练即可获得3D能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 实例级别语义一致性确保同一对象在不同视角和位置上语义一致；2) 双代码本设计实现场景特定特征和通用语言特征的对齐；3) 引导注意力机制支持无缝2D和3D开放词汇查询；4) 注意力加权熵损失促进特征和代码本间的一对一映射；5) 通过2D监督实现3D能力。相比之前工作，GALA避免了信息损失和聚类错误，同时支持高质量的2D和3D语义理解，不依赖复杂的MLP或KNN初始化，实现了更好的语义一致性和计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GALA通过引入双代码本和引导注意力机制，实现了高效且一致的开放词汇3D场景理解，同时支持高质量的2D和3D语义查询。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D scene reconstruction and understanding have gained increasing popularity,yet existing methods still struggle to capture fine-grained, language-aware 3Drepresentations from 2D images. In this paper, we present GALA, a novelframework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting(3DGS). GALA distills a scene-specific 3D instance feature field viaself-supervised contrastive learning. To extend to generalized language featurefields, we introduce the core contribution of GALA, a cross-attention modulewith two learnable codebooks that encode view-independent semantic embeddings.This design not only ensures intra-instance feature similarity but alsosupports seamless 2D and 3D open-vocabulary queries. It reduces memoryconsumption by avoiding per-Gaussian high-dimensional feature learning.Extensive experiments on real-world datasets demonstrate GALA's remarkableopen-vocabulary performance on both 2D and 3D.</description>
      <author>example@mail.com (Elena Alegret, Kunyi Li, Sen Wang, Siyun Liang, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari)</author>
      <guid isPermaLink="false">2508.14278v2</guid>
      <pubDate>Fri, 22 Aug 2025 15:24:35 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed ML Exploration of Structure-Transport Relationships in Hard Carbon</title>
      <link>http://arxiv.org/abs/2508.14849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种数据驱动框架，结合机器学习原子间势能和分子动力学模拟，系统研究了硬碳阳极中钠离子的扩散行为，揭示了微观结构与离子传输之间的关系，为高性能钠离子电池阳极设计提供了新见解。&lt;h4&gt;背景&lt;/h4&gt;钠离子电池是大规模储能中锂离子系统的经济有效的可持续替代方案。硬碳阳极具有高容量，但表现出复杂且难以理解的离子传输行为，特别是局部微观结构与钠流动性之间的关系尚未明确，阻碍了性能的合理优化。&lt;h4&gt;目的&lt;/h4&gt;引入一个数据驱动框架，系统研究跨越广泛碳密度和钠负载的钠扩散行为，识别控制离子传输的微观因素，建立定量结构-传输关系。&lt;h4&gt;方法&lt;/h4&gt;结合机器学习的原子间势能与分子动力学模拟，计算每个离子的结构描述符，使用无监督学习发现扩散模式，通过监督分析识别决定流动性的主要因素，并进行相关性映射将传输机制与加工变量联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;无监督学习揭示了跳跃、聚集和空位捕获等不同的扩散模式；监督分析表明曲折度和NaNa配位是流动性的主要决定因素；相关性映射将传输机制与体积密度和钠含量等加工变量联系起来；基于物理学的方法建立了定量结构-传输关系，捕捉了无序碳的异质性。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了钠离子动力学的机理见解，并为下一代电池系统中高性能硬碳阳极的工程设计提供了可行设计原则。&lt;h4&gt;翻译&lt;/h4&gt;钠离子电池是大规模储能中锂离子系统的一种经济有效的可持续替代方案。硬碳阳极由无序石墨和非晶域组成，具有高容量，但表现出复杂且难以理解的离子传输行为。特别是，局部微观结构与钠流动性之间的关系仍未解决，阻碍了性能的合理优化。在这里，我们引入了一个数据驱动框架，结合机器学习的原子间势能与分子动力学模拟，系统研究跨越广泛碳密度和钠负载的钠扩散。通过计算每个离子的结构描述符，我们识别了控制离子传输的微观因素。无监督学习发现了不同的扩散模式，包括跳跃、聚集和空位捕获，而监督分析强调了曲折度和NaNa配位作为流动性的主要决定因素。相关性映射进一步将这些传输机制与加工变量（如体积密度和钠含量）联系起来。这种基于物理学的方法建立了定量结构-传输关系，捕捉了无序碳的异质性。我们的研究结果提供了钠离子动力学的机理见解，并为下一代电池系统中高性能HC阳极的工程设计提供了可行设计原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sodium-ion batteries are a cost-effective and sustainable alternative tolithium-ion systems for large-scale energy storage. Hard carbon (HC) anodes,composed of disordered graphitic and amorphous domains, offer high capacity butexhibit complex, poorly understood ion transport behavior. In particular, therelationship between local microstructure and sodium mobility remainsunresolved, hindering rational performance optimization. Here, we introduce adata-driven framework that combines machine-learned interatomic potentials withmolecular dynamics simulations to systematically investigate sodium diffusionacross a broad range of carbon densities and sodium loadings. By computingper-ion structural descriptors, we identify the microscopic factors that governion transport. Unsupervised learning uncovers distinct diffusion modes,including hopping, clustering, and void trapping, while supervised analysishighlights tortuosity and NaNa coordination as primary determinants ofmobility. Correlation mapping further connects these transport regimes toprocessing variables such as bulk density and sodium content. Thisphysics-informed approach establishes quantitative structure-transportrelationships that capture the heterogeneity of disordered carbon. Our findingsdeliver mechanistic insights into sodium-ion dynamics and provide actionabledesign principles for engineering high-performance HC anodes in next-generationbattery systems.</description>
      <author>example@mail.com (Nikhil Rampal, Stephen E. Weitzner, Fredrick Omenya, Marissa Wood, David M. Reed, Xiaolin Li, Jonathan R. I. Lee, Liwen F. Wan)</author>
      <guid isPermaLink="false">2508.14849v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
  <item>
      <title>A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans</title>
      <link>http://arxiv.org/abs/2508.14151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了深度学习架构与可解释AI技术在膝盖MRI扫描中自动化检测感兴趣区域(ROI)的应用效果，发现ResNet50在分类和ROI识别方面表现最佳，而CNN-based迁移学习是最有效的方法。&lt;h4&gt;背景&lt;/h4&gt;MRI是评估膝盖损伤的重要诊断工具，但手动解释MRI切片既耗时又容易产生观察者间差异。&lt;h4&gt;目的&lt;/h4&gt;对各种深度学习架构与可解释AI(xAI)技术进行系统评估，用于膝盖MRI扫描中的感兴趣区域(ROI)自动化检测。&lt;h4&gt;方法&lt;/h4&gt;研究监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers (ViT)和多种U-Net变体，并增加多层感知器(MLP)分类器；整合Grad-CAM和Saliency Maps等可解释AI方法；使用AUC评估分类性能，PSNR/SSIM评估重建质量，并进行定性ROI可视化。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet50在分类和ROI识别方面表现 consistently 出色，优于基于transformer的模型；混合U-Net + MLP方法在利用重建和可解释性的空间特征方面显示出潜力，但分类性能较低；Grad-CAM在所有架构中始终提供最具临床意义的解释。&lt;h4&gt;结论&lt;/h4&gt;基于CNN的迁移学习是该数据集最有效的方法，未来更大规模的预训练可能更好地释放transformer模型的潜力。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像(MRI)是评估膝盖损伤的重要诊断工具。然而，手动解释MRI切片仍然耗时且容易产生观察者间差异。本研究对各种深度学习架构与可解释AI(xAI)技术进行了系统评估，用于膝盖MRI扫描中的感兴趣区域(ROI)自动化检测。我们研究了监督和自监督方法，包括ResNet50、InceptionV3、Vision Transformers (ViT)和多种增加多层感知器(MLP)分类器的U-Net变体。为了提高可解释性和临床相关性，我们整合了Grad-CAM和Saliency Maps等可解释AI方法。模型性能使用AUC评估分类质量，使用PSNR/SSIM评估重建质量，并结合定性ROI可视化进行评估。我们的结果表明，ResNet50在分类和ROI识别方面始终表现出色，在MRNet数据集约束下优于基于transformer的模型。虽然混合U-Net + MLP方法在利用重建和可解释性的空间特征方面显示出潜力，但其分类性能仍然较低。Grad-CAM在所有架构中始终提供最具临床意义的解释。总体而言，基于CNN的迁移学习成为该数据集最有效的方法，而未来更大规模的预训练可能更好地释放transformer模型的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic Resonance Imaging (MRI) is an essential diagnostic tool forassessing knee injuries. However, manual interpretation of MRI slices remainstime-consuming and prone to inter-observer variability. This study presents asystematic evaluation of various deep learning architectures combined withexplainable AI (xAI) techniques for automated region of interest (ROI)detection in knee MRI scans. We investigate both supervised and self-supervisedapproaches, including ResNet50, InceptionV3, Vision Transformers (ViT), andmultiple U-Net variants augmented with multi-layer perceptron (MLP)classifiers. To enhance interpretability and clinical relevance, we integratexAI methods such as Grad-CAM and Saliency Maps. Model performance is assessedusing AUC for classification and PSNR/SSIM for reconstruction quality, alongwith qualitative ROI visualizations. Our results demonstrate that ResNet50consistently excels in classification and ROI identification, outperformingtransformer-based models under the constraints of the MRNet dataset. Whilehybrid U-Net + MLP approaches show potential for leveraging spatial features inreconstruction and interpretability, their classification performance remainslower. Grad-CAM consistently provided the most clinically meaningfulexplanations across architectures. Overall, CNN-based transfer learning emergesas the most effective approach for this dataset, while future work withlarger-scale pretraining may better unlock the potential of transformer models.</description>
      <author>example@mail.com (Justin Yiu, Kushank Arora, Daniel Steinberg, Rohit Ghiya)</author>
      <guid isPermaLink="false">2508.14151v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Learn the Macroscopic Fundamental Diagram using Physics-Informed and meta Machine Learning techniques</title>
      <link>http://arxiv.org/abs/2508.14137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种利用元学习框架来估计宏观基本图(MFD)的方法，解决了实际应用中检测器数量有限的问题。&lt;h4&gt;背景&lt;/h4&gt;宏观基本图是描述交通动态的常用工具，但估计给定网络的MFD需要大量环形检测器，这在实践中往往不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种元学习框架，以缓解数据稀缺挑战，使模型能够在检测器数量有限的情况下准确估计MFD。&lt;h4&gt;方法&lt;/h4&gt;提出一个即时的多任务物理信息神经网络，利用多个城市的数据进行训练，并应用于具有不同检测器比例和拓扑结构的其他城市。&lt;h4&gt;主要发现&lt;/h4&gt;元学习框架在流量预测方面的平均均方误差改进在约17500到36000之间，成功跨越多样化城市环境，提高了数据有限城市的性能。&lt;h4&gt;结论&lt;/h4&gt;元学习框架在使用有限数量检测器时具有潜力，其性能优于传统迁移学习方法，并证明了其可迁移性。&lt;h4&gt;翻译&lt;/h4&gt;宏观基本图是一种流行的工具，用于描述聚合方式的交通动态，应用范围从交通控制到事件分析。然而，估计给定网络的MFD需要大量的环形检测器，这在实践中并不总是可用。本文提出了一种利用元学习的框架，元学习是机器学习的一个子类别，它训练模型能够独立理解和适应新任务，以缓解数据稀缺的挑战。所开发的模型通过利用多个城市的数据进行训练和测试，并利用它来建模具有不同检测器比例和拓扑结构的其他城市的MFD。提出的元学习框架应用于一个即时的多任务物理信息神经网络，专门设计用于估计MFD。结果显示，流量预测的平均均方误差改进在约17500到36000之间（取决于测试的环形检测器子集）。元学习框架因此成功地跨越了多样化的城市环境，并改进了数据有限城市的性能，展示了在使用有限数量检测器时使用元学习的潜力。最后，提出的框架与传统迁移学习方法进行了比较，并与文献中的非参数模型FitFun进行了测试，以证明其可迁移性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Macroscopic Fundamental Diagram is a popular tool used to describetraffic dynamics in an aggregated way, with applications ranging from trafficcontrol to incident analysis. However, estimating the MFD for a given networkrequires large numbers of loop detectors, which is not always available inpractice. This article proposes a framework harnessing meta-learning, asubcategory of machine learning that trains models to understand and adapt tonew tasks on their own, to alleviate the data scarcity challenge. The developedmodel is trained and tested by leveraging data from multiple cities andexploiting it to model the MFD of other cities with different shares ofdetectors and topological structures. The proposed meta-learning framework isapplied to an ad-hoc Multi-Task Physics-Informed Neural Network, specificallydesigned to estimate the MFD. Results show an average MSE improvement in flowprediction ranging between ~ 17500 and 36000 (depending on the subset of loopdetectors tested). The meta-learning framework thus successfully generalizesacross diverse urban settings and improves performance on cities with limiteddata, demonstrating the potential of using meta-learning when a limited numberof detectors is available. Finally, the proposed framework is validated againsttraditional transfer learning approaches and tested with FitFun, anon-parametric model from the literature, to prove its transferability.</description>
      <author>example@mail.com (Amalie Roark, Serio Agriesti, Francisco Camara Pereira, Guido Cantelmo)</author>
      <guid isPermaLink="false">2508.14137v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Is Transfer Learning Necessary for Violin Transcription?</title>
      <link>http://arxiv.org/abs/2508.13516v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了从小规模数据集训练小提琴自动音乐转录模型的可行性，发现无需依赖钢琴预训练模型也能获得良好性能。&lt;h4&gt;背景&lt;/h4&gt;钢琴等乐器的自动音乐转录已取得显著进展，主要得益于大规模高质量数据集；而小提琴AMT研究不足，因标注数据有限。常见方法是微调预训练模型，但在音色和发音差异存在时，迁移效果不明确。&lt;h4&gt;目的&lt;/h4&gt;研究从中等规模小提琴数据集从头开始训练，是否能达到微调钢琴预训练模型的性能。&lt;h4&gt;方法&lt;/h4&gt;采用未经修改的钢琴转录架构，在包含约30小时对齐小提琴录音的MOSA数据集上进行训练，并在URMP和Bach10数据集上进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;从头开始训练的模型与微调模型相比，实现了具有竞争力甚至更优的性能。&lt;h4&gt;结论&lt;/h4&gt;强大的小提琴AMT不需要依赖预训练的钢琴表示，突显了乐器特定数据收集和数据增强策略的重要性。&lt;h4&gt;翻译&lt;/h4&gt;自动音乐转录（AMT）在钢琴等乐器上取得了显著进展，这主要归功于大规模高质量数据集的可用性。相比之下，小提琴AMT由于标注数据有限而研究不足。一种常见方法是微调预训练模型用于其他下游任务，但在存在音色和发音差异的情况下，这种迁移的有效性尚不清楚。在这项工作中，我们研究从中等规模小提琴数据集从头开始训练是否能匹配微调钢琴预训练模型的性能。我们采用未经修改的钢琴转录架构，并在包含约30小时对齐小提琴录音的MOSA数据集上进行训练。我们在URMP和Bach10上的实验表明，从头开始训练的模型与微调模型相比实现了具有竞争力甚至更优的性能。这些发现表明，强大的小提琴AMT不需要依赖预训练的钢琴表示，突显了乐器特定数据收集和数据增强策略的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic music transcription (AMT) has achieved remarkable progress forinstruments such as the piano, largely due to the availability of large-scale,high-quality datasets. In contrast, violin AMT remains underexplored due tolimited annotated data. A common approach is to fine-tune pretrained models forother downstream tasks, but the effectiveness of such transfer remains unclearin the presence of timbral and articulatory differences. In this work, weinvestigate whether training from scratch on a medium-scale violin dataset canmatch the performance of fine-tuned piano-pretrained models. We adopt a pianotranscription architecture without modification and train it on the MOSAdataset, which contains about 30 hours of aligned violin recordings. Ourexperiments on URMP and Bach10 show that models trained from scratch achievedcompetitive or even superior performance compared to fine-tuned counterparts.These findings suggest that strong violin AMT is possible without relying onpretrained piano representations, highlighting the importance ofinstrument-specific data collection and augmentation strategies.</description>
      <author>example@mail.com (Yueh-Po Peng, Ting-Kang Wang, Li Su, Vincent K. M. Cheung)</author>
      <guid isPermaLink="false">2508.13516v2</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.14088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出CoBAD模型，用于检测人类移动中的集体异常行为，在AUCROC和AUCPR指标上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;检测人类移动异常对公共安全和城市规划至关重要。传统方法主要关注个人移动模式，而集体异常检测（如孩子独自在家而父母不在）仍是一个未被充分探索的挑战，需要建模个体间的时空依赖关系。&lt;h4&gt;目的&lt;/h4&gt;开发CoBAD模型来捕获人类移动中的集体行为，以解决集体异常检测这一研究空白。&lt;h4&gt;方法&lt;/h4&gt;将问题表述为在集体事件序列上的无监督学习，使用共现事件图；采用两级注意力机制建模个人移动模式和个体间交互；通过掩码事件和链接重建任务在大规模数据上预训练；可检测意外的共现异常和缺失异常两种类型。&lt;h4&gt;主要发现&lt;/h4&gt;在大型移动数据集上的实验表明，CoBAD在AUCROC上比现有方法提升13%-18%，在AUCPR上提升19%-70%。&lt;h4&gt;结论&lt;/h4&gt;CoBAD模型在集体异常检测方面表现优异，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;检测人类移动中的异常对公共安全和城市规划等应用至关重要。虽然传统异常检测方法主要关注个人移动模式（例如，孩子晚上应该待在家里），但集体异常检测旨在识别个体间集体移动行为中的异常（例如，孩子独自在家而父母在其他地方），仍然是一个未被充分探索的挑战。与个人异常不同，集体异常需要建模个体之间的时空依赖关系，引入了额外的复杂性。为解决这一研究空白，我们提出了CoBAD，一个设计用于捕获人类移动异常检测中集体行为的新模型。我们首先将问题表述为在具有共现事件图的集体事件序列上的无监督学习，其中集体事件序列代表相关个体的事件序列。然后，CoBAD采用两级注意力机制来建模个人移动模式和多个个体之间的交互。通过掩码事件和链接重建任务在大规模集体行为数据上进行预训练，CoBAD能够检测两种类型的集体异常：意外的共现异常和缺失异常，后者在先前工作中很大程度上被忽视。在大型移动数据集上的广泛实验表明，CoBAD显著优于现有的异常检测基线，在AUCROC上实现了13%-18%的提升，在AUCPR上实现了19%-70%的提升。所有源代码可在https://github.com/wenhaomin/CoBAD获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in human mobility is essential for applications such aspublic safety and urban planning. While traditional anomaly detection methodsprimarily focus on individual movement patterns (e.g., a child should stay athome at night), collective anomaly detection aims to identify irregularities incollective mobility behaviors across individuals (e.g., a child is at homealone while the parents are elsewhere) and remains an underexplored challenge.Unlike individual anomalies, collective anomalies require modelingspatiotemporal dependencies between individuals, introducing additionalcomplexity. To address this gap, we propose CoBAD, a novel model designed tocapture Collective Behaviors for human mobility Anomaly Detection. We firstformulate the problem as unsupervised learning over Collective Event Sequences(CES) with a co-occurrence event graph, where CES represents the eventsequences of related individuals. CoBAD then employs a two-stage attentionmechanism to model both the individual mobility patterns and the interactionsacross multiple individuals. Pre-trained on large-scale collective behaviordata through masked event and link reconstruction tasks, CoBAD is able todetect two types of collective anomalies: unexpected co-occurrence anomaliesand absence anomalies, the latter of which has been largely overlooked in priorwork. Extensive experiments on large-scale mobility datasets demonstrate thatCoBAD significantly outperforms existing anomaly detection baselines, achievingan improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code isavailable at https://github.com/wenhaomin/CoBAD.</description>
      <author>example@mail.com (Haomin Wen, Shurui Cao, Leman Akoglu)</author>
      <guid isPermaLink="false">2508.14088v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Pulled fronts are not (just) pulled</title>
      <link>http://arxiv.org/abs/2508.14864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18p&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了前沿传播进入不稳定状态的动力学行为，指出线性化预测的前沿传播速度与实际一致，但前沿后的动力学行为比传统'被拉前沿'理论所描述的更为复杂。&lt;h4&gt;背景&lt;/h4&gt;前沿传播进入不稳定状态通常由线性化决定，传播速度与在不稳定状态下线性化方程的预测一致，前沿表现为以线性扩散速度传播的高斯尾部。&lt;h4&gt;目的&lt;/h4&gt;描述一类例子，展示主要效应（高斯尾部传播）并不能完全描述前沿后的动力学行为。&lt;h4&gt;方法&lt;/h4&gt;通过分析前沿传播的动力学模型，研究线性化预测与实际入侵过程之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;前沿行为最多预测两种可能的入侵情景（与高斯尾部的正负振幅相关），但实际存在三个或更多具有不同状态的前入侵过程，留下的状态不仅仅由前沿决定。&lt;h4&gt;结论&lt;/h4&gt;入侵过程留下的状态不 solely 由前沿决定，也不只是被高斯尾部拉动，表明'被拉前沿'理论不能完全描述复杂的动力学行为。&lt;h4&gt;翻译&lt;/h4&gt;前沿传播进入不稳定状态通常由线性化决定，即传播速度与在不稳定状态下线性化方程的预测一致。前沿行为表现为以线性扩散速度传播的高斯尾部。跟随这个前沿的通常被称为'被拉前沿'，暗示它们被这个前沿的高斯尾部'拉动'。本文描述了一类例子，展示了这些主要效应并不能完全描述前沿后的动力学。实际上，前沿行为最多预测两种可能的入侵情景，与高斯尾部的正负振幅相关，但我们的例子展示了三个或更多具有不同状态的前入侵前沿。因此，入侵过程留下的状态不仅仅由前沿决定，也不只是被高斯尾部拉动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Front propagation into unstable states is often determined by thelinearization, that is, propagation speeds agree with predictions from thelinearized equation at the unstable state. The leading edge behavior is then aGaussian tail propagating with the linear spreading speed. Fronts followingthis leading edge are commonly referred to as pulled fronts, alluding to theidea that they are ``pulled'' by this leading-edge Gaussian tail. We describehere a class of examples that exhibits how these leading-order effects do notcompletely describe the dynamics in the wake of the front. In fact, leadingedge behavior predicts at most two possible invasion scenarios, associated withpositive and negative amplitudes of the Gaussian tail, but our examples exhibitthree or more invasion fronts with different states in the wake. The resultinginvasion process therefore leaves behind a state that is not solely determinedby the leading edge, and thus not just pulled by the Gaussian tail.</description>
      <author>example@mail.com (Montie Avery, Matt Holzer, Arnd Scheel)</author>
      <guid isPermaLink="false">2508.14864v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism</title>
      <link>http://arxiv.org/abs/2508.14523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Great GATsBi的自行车轨迹预测框架，结合了物理模型和社会模型，有效预测自行车运动轨迹并超越了现有技术水平。&lt;h4&gt;背景&lt;/h4&gt;准确的交通参与者运动预测对高级驾驶员辅助系统和自动驾驶等应用日益重要，对道路安全尤为关键。然而，尽管大多数交通事故死亡涉及自行车，自行车运动预测得到的关注较少，之前的研究主要集中在行人和机动车辆上。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的自行车轨迹预测框架，解决自行车运动预测被忽视的问题，提高道路安全性。&lt;h4&gt;方法&lt;/h4&gt;提出Great GATsBi，一个基于领域知识的混合多模态轨迹预测框架，结合了基于物理的建模（受机动车辆启发）和基于社会的建模（受行人运动启发），使用图注意力网络建模社会互动，并考虑自行车邻域的历史和预期未来轨迹数据。&lt;h4&gt;主要发现&lt;/h4&gt;物理模型（短期预测效果好）和社会模型（长期预测效果好）的集成超越了最先进的性能，有效预测自行车轨迹并建模社会互动。&lt;h4&gt;结论&lt;/h4&gt;通过受控的集体骑行实验验证了该框架在预测自行车轨迹和建模与其他道路使用者社会互动方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确的交通参与者运动预测正被越来越多的应用需求，从高级驾驶员辅助系统到自动驾驶，对道路安全尤为重要。尽管大多数交通事故死亡涉及自行车，但它们得到的关注很少，因为之前的工作主要集中在行人和机动车辆上。在这项工作中，我们提出了Great GATsBi，一个基于领域知识的混合多模态自行车轨迹预测框架。该模型结合了基于物理的建模（受机动车辆启发）和基于社会的建模（受行人运动启发），明确考虑了自行车运动的双重特性。社会互动使用图注意力网络建模，包括自行车邻域的衰减历史轨迹数据和预期未来轨迹数据，遵循心理学和社会研究的最新见解。结果表明，提出的物理模型集成（在短期预测中表现良好）和社会模型集成（在长期预测中表现良好）超越了最先进的性能。我们还进行了一项受控的集体骑行实验，展示了该框架在预测自行车轨迹和建模与其他道路使用者的社会互动方面的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of road user movement is increasingly required by manyapplications ranging from advanced driver assistance systems to autonomousdriving, and especially crucial for road safety. Even though most trafficaccident fatalities account to bicycles, they have received little attention,as previous work focused mainly on pedestrians and motorized vehicles. In thiswork, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodaltrajectory prediction framework for bicycles. The model incorporates bothphysics-based modeling (inspired by motorized vehicles) and social-basedmodeling (inspired by pedestrian movements) to explicitly account for the dualnature of bicycle movement. The social interactions are modeled with a graphattention network, and include decayed historical, but also anticipated, futuretrajectory data of a bicycles neighborhood, following recent insights frompsychological and social studies. The results indicate that the proposedensemble of physics models -- performing well in the short-term predictions --and social models -- performing well in the long-term predictions -- exceedsstate-of-the-art performance. We also conducted a controlled mass-cyclingexperiment to demonstrate the framework's performance when forecasting bicycletrajectories and modeling social interactions with road users.</description>
      <author>example@mail.com (Kevin Riehl, Shaimaa K. El-Baklish, Anastasios Kouvelas, Michail A. Makridis)</author>
      <guid isPermaLink="false">2508.14523v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds</title>
      <link>http://arxiv.org/abs/2508.14892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://hustvl.github.io/Snap-Snap/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅从正面和背面两张图像重建3D人体的方法，能够快速生成完整的带颜色的人体点云，并转换为3D高斯以获得更好的渲染质量。&lt;h4&gt;背景&lt;/h4&gt;从稀疏视图重建3D人体是一个有吸引力的研究领域，对扩展相关应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个从仅两张图像（正面和背面视图）重建人体的任务，降低用户创建自己3D数字人体的门槛。&lt;h4&gt;方法&lt;/h4&gt;重新设计了基于基础重建模型的几何重建模型，预测一致的点云；应用增强算法补充缺失的颜色信息；将完整的带颜色的人体点云直接转换为3D高斯以获得更好的渲染质量。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在单个NVIDIA RTX 4090上，以1024x1024分辨率的两张图像重建整个人体仅需190毫秒；在THuman2.0和跨域数据集上展示了最先进的性能；即使使用低成本移动设备拍摄图像也能完成人体重建，降低数据收集要求。&lt;h4&gt;结论&lt;/h4&gt;该方法能够高效、准确地从仅两张图像重建3D人体，降低了3D数字人体创建的技术门槛。&lt;h4&gt;翻译&lt;/h4&gt;Reconstructing 3D human bodies from sparse views has been an appealing topic, which is crucial to broader the related applications. In this paper, we propose a quite challenging but valuable task to reconstruct the human body from only two images, i.e., the front and back view, which can largely lower the barrier for users to create their own 3D digital humans. The main challenges lie in the difficulty of building 3D consistency and recovering missing information from the highly sparse input. We redesign a geometry reconstruction model based on foundation reconstruction models to predict consistent point clouds even input images have scarce overlaps with extensive human data training. Furthermore, an enhancement algorithm is applied to supplement the missing color information, and then the complete human point clouds with colors can be obtained, which are directly transformed into 3D Gaussians for better rendering quality. Experiments show that our method can reconstruct the entire human in 190 ms on a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024, demonstrating state-of-the-art performance on the THuman2.0 and cross-domain datasets. Additionally, our method can complete human reconstruction even with images captured by low-cost mobile devices, reducing the requirements for data collection. Demos and code are available at https://hustvl.github.io/Snap-Snap/.&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从仅有的两张图像（正面和背面）快速重建3D人体模型的问题。这个问题在现实中很重要，因为它大大降低了用户创建自己3D数字人的门槛，不需要昂贵的专业设备或复杂的设置；在研究中很重要，因为它解决了稀疏视角下人体重建的挑战，扩展了3D重建的应用场景，如虚拟试衣、游戏角色创建和元宇宙，同时降低了数据收集成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于基础几何重建模型（如DUSt3R）重新设计了点云预测模型，将通用几何先验适应到人体领域。整个方法分为三个阶段：点云预测、侧视图增强和高斯属性回归。针对正面和背面视图缺乏重叠的问题，引入了额外的侧视图预测头来推断左右两侧的几何信息。为解决侧视图颜色缺失问题，设计了最近邻搜索算法从前视图和后视图的颜色信息中补充侧视图颜色。方法借鉴了3D高斯泼溅技术来实现高质量渲染。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从仅有的两张图像重建完整人体3D表示，利用基础几何重建模型知识推断缺失的侧视图几何信息，通过算法补充缺失的颜色信息，并将点云转换为3D高斯表示。整体流程分为三阶段：1）点云预测：输入正背面图像，通过编码器-解码器和预测头生成四个视图的点云并拼接；2）侧视图增强：用最近邻搜索算法从前视图和后视图颜色信息中补充侧视图颜色；3）高斯属性回归：输入点云和图像，用UNet-like网络预测高斯属性并拼接成完整人体高斯表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）仅需两张图像即可重建完整人体3D高斯表示，毫秒级完成；2）重新设计基于基础几何重建模型的点云预测模型；3）提出侧视图增强算法补充颜色信息；4）直接预测3D高斯表示，无需依赖SMPL-X等人体先验；5）对低成本移动设备拍摄的照片具有鲁棒性。相比之前工作不同：与基于SMPL-X的方法不同，无需估计参数；与基于深度估计的方法不同，不依赖深度估计模块；与单视图重建方法不同，避免生成模型导致的不可控纹理；与需要多视图输入的方法不同，降低数据要求；与传统网格重建方法不同，更好保持姿势一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Snap-Snap提出了一种仅需两张人体正面和背面图像即可在毫秒级重建高质量3D人体高斯表示的方法，通过重新设计几何重建模型和侧视图增强算法，解决了稀疏视角下人体重建的几何一致性和信息缺失问题，显著降低了3D数字人创建的门槛。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D human bodies from sparse views has been an appealing topic,which is crucial to broader the related applications. In this paper, we proposea quite challenging but valuable task to reconstruct the human body from onlytwo images, i.e., the front and back view, which can largely lower the barrierfor users to create their own 3D digital humans. The main challenges lie in thedifficulty of building 3D consistency and recovering missing information fromthe highly sparse input. We redesign a geometry reconstruction model based onfoundation reconstruction models to predict consistent point clouds even inputimages have scarce overlaps with extensive human data training. Furthermore, anenhancement algorithm is applied to supplement the missing color information,and then the complete human point clouds with colors can be obtained, which aredirectly transformed into 3D Gaussians for better rendering quality.Experiments show that our method can reconstruct the entire human in 190 ms ona single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,demonstrating state-of-the-art performance on the THuman2.0 and cross-domaindatasets. Additionally, our method can complete human reconstruction even withimages captured by low-cost mobile devices, reducing the requirements for datacollection. Demos and code are available athttps://hustvl.github.io/Snap-Snap/.</description>
      <author>example@mail.com (Jia Lu, Taoran Yi, Jiemin Fang, Chen Yang, Chuiyun Wu, Wei Shen, Wenyu Liu, Qi Tian, Xinggang Wang)</author>
      <guid isPermaLink="false">2508.14892v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds</title>
      <link>http://arxiv.org/abs/2508.14879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeshCoder是一种新颖的框架，能够将复杂的3D点云重建为可编辑的Blender Python脚本，解决了现有方法在处理复杂几何形状和结构方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有方法通常依赖于有限的领域特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够重建复杂3D物体为可编辑程序的框架，以支持逆向工程和形状编辑等应用。&lt;h4&gt;方法&lt;/h4&gt;MeshCoder通过开发全面的Blender Python API来合成复杂几何形状，构建大规模成对对象-代码数据集，并训练多模态大语言模型将3D点云转换为可执行的Blender Python脚本。&lt;h4&gt;主要发现&lt;/h4&gt;MeshCoder在形状到代码重建任务中表现优异，通过代码修改实现了直观的几何和拓扑编辑，并且基于代码的表示增强了LLM在3D形状理解任务中的推理能力。&lt;h4&gt;结论&lt;/h4&gt;MeshCoder为程序化3D形状重建和理解提供了强大而灵活的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将3D物体重建为可编辑的程序对于逆向工程和形状编辑等应用至关重要。然而，现有方法通常依赖于有限的领域特定语言(DSLs)和小规模数据集，限制了它们对复杂几何形状和结构的建模能力。为解决这些挑战，我们引入了MeshCoder，一种新颖的框架，能够将复杂的3D点云重建为可编辑的Blender Python脚本。我们开发了一套全面的Blender Python API，能够合成复杂的几何形状。利用这些API，我们构建了一个大规模的成对对象-代码数据集，其中每个对象的代码被分解为不同的语义部分。随后，我们训练了一个多模态大语言模型(LLM)，将3D点云转换为可执行的Blender Python脚本。我们的方法不仅在形状到代码重建任务中取得了优异的性能，还通过方便的代码修改实现了直观的几何和拓扑编辑。此外，我们的基于代码的表示增强了LLM在3D形状理解任务中的推理能力。这些贡献共同确立了MeshCoder作为程序化3D形状重建和理解的强大而灵活的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将3D点云数据转换为可编辑的程序化代码（特别是Blender Python脚本）的问题。现有方法受限于简单的领域特定语言（DSLs）和缺乏大规模数据集，无法处理复杂几何结构和真实世界对象。这个问题在逆向工程、形状编辑、3D结构理解和设计自动化等领域非常重要，因为它允许通过修改代码直观地编辑3D模型，增强大型语言模型对3D形状的理解，并支持高保真资产创建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的两个关键局限：DSLs表达能力不足和缺乏大规模配对数据集。然后他们设计了一个三阶段解决方案：1)开发表达力强的Blender Python API；2)构建大规模对象-代码配对数据集；3)训练多模态大语言模型将点云转换为可执行代码。他们借鉴了形状程序方法（如ShapeAssembly、ShapeCoder）、基于部分的3D表示方法、程序化生成框架（如Infinigen-Indoor）以及大型语言模型技术，但将其扩展到代码生成领域，形成了创新的综合方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云转换为结构化的、可执行的Blender Python脚本，这些脚本能够重建复杂3D对象的语义部分，从而提供更高层次的编辑和理解能力。整体流程分为三阶段：1)数据准备阶段，使用API生成零件数据集，并利用Infinigen-Indoor生成对象数据集；2)模型训练阶段，先训练零件到代码的推断模型，再基于此训练对象到代码的推断模型；3)推理和应用阶段，输入点云通过形状标记化器转换为标记，LLM生成可执行代码，执行后重建3D对象，支持通过代码修改进行编辑，并增强大型语言模型对3D形状的理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开发表达力强的Blender Python API，支持复杂几何操作；2)构建大规模配对对象-代码数据集，涵盖41个类别和100万个对象；3)训练多模态大语言模型将3D点云转换为可执行Blender脚本；4)通过代码表示实现直观的几何和拓扑编辑。相比之前的工作，MeshCoder超越了传统形状程序方法的简单几何限制，解决了CAD方法只关注单个零件的问题，弥补了基于部分方法不提供代码表示的不足，并增强了大型语言模型对3D形状结构的高层次理解能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MeshCoder通过将3D点云转换为可编辑的Blender Python脚本，实现了复杂3D对象的程序化重建与编辑，同时提升了大型语言模型对3D形状结构的理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D objects into editable programs is pivotal for applicationslike reverse engineering and shape editing. However, existing methods oftenrely on limited domain-specific languages (DSLs) and small-scale datasets,restricting their ability to model complex geometries and structures. Toaddress these challenges, we introduce MeshCoder, a novel framework thatreconstructs complex 3D objects from point clouds into editable Blender Pythonscripts. We develop a comprehensive set of expressive Blender Python APIscapable of synthesizing intricate geometries. Leveraging these APIs, weconstruct a large-scale paired object-code dataset, where the code for eachobject is decomposed into distinct semantic parts. Subsequently, we train amultimodal large language model (LLM) that translates 3D point cloud intoexecutable Blender Python scripts. Our approach not only achieves superiorperformance in shape-to-code reconstruction tasks but also facilitatesintuitive geometric and topological editing through convenient codemodifications. Furthermore, our code-based representation enhances thereasoning capabilities of LLMs in 3D shape understanding tasks. Together, thesecontributions establish MeshCoder as a powerful and flexible solution forprogrammatic 3D shape reconstruction and understanding.</description>
      <author>example@mail.com (Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang)</author>
      <guid isPermaLink="false">2508.14879v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GeMS: Efficient Gaussian Splatting for Extreme Motion Blur</title>
      <link>http://arxiv.org/abs/2508.14682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了GeMS，一个专门用于处理严重运动模糊图像的3D高斯飞溅框架，能够直接从模糊输入重建场景，并提出了改进版本GeMS-E，结合事件数据进一步提升了重建质量。&lt;h4&gt;背景&lt;/h4&gt;现有最先进的去模糊方法和高斯飞溅方法通常假设可以使用清晰图像进行相机姿态估计和点云生成，这不切实际。依赖COLMAP初始化的方法在严重模糊下也会失败，因为特征对应不可靠。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够直接从极度模糊图像重建场景的3D高斯飞溅框架，解决现有方法在处理严重运动模糊图像时的局限性。&lt;h4&gt;方法&lt;/h4&gt;GeMS框架包含三个核心组件：(1) VGGSfM，基于深度学习的运动恢复结构管道，直接从模糊输入估计姿态和生成点云；(2) 3DGS-MCMC，通过将高斯视为概率分布的样本实现鲁棒场景初始化；(3) 相机轨迹和高斯参数的联合优化。改进版本GeMS-E进一步整合了基于事件的积分去模糊(EDI)步骤，恢复更清晰的图像输入到GeMS中。&lt;h4&gt;主要发现&lt;/h4&gt;GeMS和GeMS-E在合成和真实数据集上都达到了最先进的性能，能够有效处理严重运动模糊的图像，实现高质量的场景重建。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，GeMS是第一个直接从严重模糊输入处理极端运动模糊的3D高斯飞溅框架，为处理极端模糊条件下的3D重建提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了GeMS，一个用于3D高斯飞溅(3DGS)的框架，专门用于处理严重运动模糊图像。针对极端模糊的最先进去模糊方法(如ExBluRF)以及高斯飞溅方法(如Deblur-GS)通常假设可以使用清晰图像进行相机姿态估计和点云生成，这是一个不切实际的假设。依赖COLMAP初始化的方法(如BAD-Gaussians)也会因严重模糊下不可靠的特征对应而失败。为解决这些挑战，我们提出了GeMS，一个直接从极度模糊图像重建场景的3DGS框架。GeMS集成了：(1) VGGSfM，一个基于深度学习的运动恢复结构管道，直接从模糊输入估计姿态和生成点云；(2) 3DGS-MCMC，通过将高斯视为概率分布的样本实现鲁棒场景初始化，消除启发式密度增加和修剪；(3) 相机轨迹和高斯参数的联合优化，实现稳定重建。虽然此流程能产生良好结果，但当所有输入都严重模糊时，可能仍存在不准确之处。为此，我们提出了GeMS-E，它使用事件进行渐进式精化：(4) 基于事件的积分去模糊(EDI)恢复更清晰的图像，然后输入GeMS，改善姿态估计、点云生成和整体重建。GeMS和GeMS-E在合成和真实数据集上都达到了最先进的性能。据我们所知，这是第一个直接从严重模糊输入处理极端运动模糊的3DGS框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从极端运动模糊的图像中直接重建出清晰的3D场景的问题。这个问题在现实中非常重要，因为在高速相机运动或低光条件下（如机器人、自动驾驶、手持摄影等场景），拍摄清晰图像往往不可能，而现有的3D重建方法（如NeRF、3DGS）都依赖于清晰图像进行相机姿态估计和点云生成，导致在严重运动模糊情况下无法工作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行设计：传统SfM方法（如COLMAP）在严重模糊下失效，而去模糊方法（如ExBluRF、BAD-Gaussians）依赖清晰图像初始化。作者借鉴了三个关键技术：1) VGGSfM方法，使用深度学习进行2D点跟踪而非特征匹配，提高对模糊的鲁棒性；2) 3DGS-MCMC方法，将高斯视为概率分布样本，使用MCMC采样自适应优化几何；3) 改进BAD-Gaussians的联合优化，用贝塞尔曲线替代线性插值更好地建模复杂运动。作者将这些技术整合为一个紧密耦合的端到端系统，各组件相互补偿局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; GeMS的核心思想是通过结合模糊鲁棒的初始化、概率场景建模和物理驱动的联合优化，直接从极端运动模糊图像中重建清晰3D场景。整体流程：1) 使用VGGSfM从模糊图像直接估计相机姿态和生成点云；2) 使用3DGS-MCMC将高斯视为概率分布样本，通过MCMC采样自适应优化几何；3) 联合优化贝塞尔曲线参数化的相机轨迹和高斯参数，确保与物理模糊形成过程一致。对于极端情况（所有视图都严重模糊），GeMS-E扩展使用事件数据通过EDI模型恢复清晰图像，再输入到GeMS框架中提高重建质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 消除对清晰图像初始化的依赖，直接从模糊图像重建场景；2) 使用VGGSfM替代传统COLMAP，通过深度学习点跟踪提高模糊鲁棒性；3) 采用3DGS-MCMC进行概率场景建模，替代启发式密集化/剪枝策略；4) 使用贝塞尔曲线参数化相机轨迹，与高斯参数进行物理驱动的联合优化；5) 提出GeMS-E扩展，利用事件数据进一步提升极端模糊场景的重建质量。相比之前工作，GeMS是完全端到端的框架，无需清晰图像初始化，且在极端模糊情况下表现更优，同时计算效率更高（训练时间从数小时降至几分钟）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeMS首次实现了直接从极端运动模糊图像中高效重建高质量3D场景，无需清晰图像初始化，并通过事件数据进一步提升了在所有视图都严重模糊情况下的重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed tohandle severely motion-blurred images. State-of-the-art deblurring methods forextreme blur, such as ExBluRF, as well as Gaussian Splatting-based approacheslike Deblur-GS, typically assume access to sharp images for camera poseestimation and point cloud generation, an unrealistic assumption. Methodsrelying on COLMAP initialization, such as BAD-Gaussians, also fail due tounreliable feature correspondences under severe blur. To address thesechallenges, we propose GeMS, a 3DGS framework that reconstructs scenes directlyfrom extremely blurred images. GeMS integrates: (1) VGGSfM, a deeplearning-based Structure-from-Motion pipeline that estimates poses andgenerates point clouds directly from blurred inputs; (2) 3DGS-MCMC, whichenables robust scene initialization by treating Gaussians as samples from aprobability distribution, eliminating heuristic densification and pruning; and(3) joint optimization of camera trajectories and Gaussian parameters forstable reconstruction. While this pipeline produces strong results,inaccuracies may remain when all inputs are severely blurred. To mitigate this,we propose GeMS-E, which integrates a progressive refinement step using events:(4) Event-based Double Integral (EDI) deblurring restores sharper images thatare then fed into GeMS, improving pose estimation, point cloud generation, andoverall reconstruction. Both GeMS and GeMS-E achieve state-of-the-artperformance on synthetic and real-world datasets. To our knowledge, this is thefirst framework to address extreme motion blur within 3DGS directly fromseverely blurred inputs.</description>
      <author>example@mail.com (Gopi Raju Matta, Trisha Reddypalli, Vemunuri Divya Madhuri, Kaushik Mitra)</author>
      <guid isPermaLink="false">2508.14682v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling</title>
      <link>http://arxiv.org/abs/2508.14604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, Accepted to ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种统一时空状态空间模型(UST-SSM)，用于处理点云视频的时空无序性问题，通过时空选择扫描、时空结构聚合和时间交互采样三种技术，有效提高了对细微和连续人类动作的识别能力。&lt;h4&gt;背景&lt;/h4&gt;点云视频能够捕捉动态3D运动并减少光照和视角变化的影响，对识别细微和连续的人类动作非常有效。选择性状态空间模型(SSMs)在序列建模中表现良好，但点云视频的时空无序性阻碍了其直接单向建模。&lt;h4&gt;目的&lt;/h4&gt;解决点云视频时空无序性对SSMs单向建模的阻碍问题，使SSMs能够有效应用于点云视频处理。&lt;h4&gt;方法&lt;/h4&gt;提出统一时空状态空间模型(UST-SSM)，包含三种技术：1)时空选择扫描(STSS)通过提示引导的聚类将无序点重新组织为语义感知序列；2)时空结构聚合(STSA)聚合时空特征并补偿缺失的4D几何和运动细节；3)时间交互采样(TIS)通过非锚帧利用和扩展感受野增强时间交互。&lt;h4&gt;主要发现&lt;/h4&gt;在MSR-Action3D、NTU RGB+D和Synthia 4D数据集上的实验结果验证了UST-SSM方法的有效性，代码已在GitHub开源。&lt;h4&gt;结论&lt;/h4&gt;UST-SSM成功将SSMs的最新进展扩展到点云视频处理，通过重新组织点云序列和增强时间交互，有效解决了点云视频时空无序性问题。&lt;h4&gt;翻译&lt;/h4&gt;点云视频捕捉动态3D运动的同时减少了光照和视角变化的影响，使其在识别细微和连续的人类动作方面非常有效。尽管选择性状态空间模型(SSMs)在序列建模中已展现出良好的性能和线性复杂度，但点云视频的时空无序性在通过时间顺序扫描将点云视频直接展开为一维序列时，阻碍了其单向建模。为解决这一挑战，我们提出了统一时空状态空间模型(UST-SSM)，将SSMs的最新进展扩展到点云视频。具体而言，我们引入了时空选择扫描(STSS)，通过提示引导的聚类将无序点重新组织为语义感知序列，从而能够在序列中有效利用空间和时间上相距较远但相似的点。对于缺失的4D几何和运动细节，时空结构聚合(STSA)聚合时空特征并进行补偿。为了提高采样序列内的时间交互，时间交互采样(TIS)通过非锚帧利用和扩展感受野增强细粒度时间依赖关系。在MSR-Action3D、NTU RGB+D和Synthia 4D数据集上的实验结果验证了我们方法的有效性。我们的代码可在https://github.com/wangzy01/UST-SSM获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云视频建模中的时空无序性问题，这一问题阻碍了选择性状态空间模型(SSMs)的单向建模能力。这个问题在现实中非常重要，因为点云视频能够捕捉动态3D运动同时减少光照和视角变化的影响，对于机器人技术和自主系统中的人体动作识别至关重要。研究上，现有方法如CNNs难以建模长期依赖关系，而Transformer-based方法在长序列处理时消耗大量内存，SSMs虽有线性复杂度优势但难以直接应用于时空无序的点云视频。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云视频建模面临的三大挑战：非统一的时空无序性、局部几何信息丢失和时间交互限制。他们借鉴了选择性状态空间模型(SSMs)在序列建模中的成功应用，参考了Mamba4D的方法但指出了其局限性，同时受到Transformer-based方法的启发但注意到其内存消耗问题。基于这些分析，作者设计了三个关键组件：时空选择扫描(STSS)解决时空无序性，时空结构聚合(STSA)补偿几何信息丢失，时间交互采样(TIS)增强时间交互，从而构建了UST-SSM方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一时空状态空间模型重新组织点云数据，使其更适合SSMs的单向建模特性。整体流程包括：1)时间交互采样(TIS)通过多步采样策略扩展时间感受域；2)时空选择扫描(STSS)利用提示网络聚类相似点并使用Hilbert曲线排序；3)时空结构聚合(STSA)通过4D KNN和特征传播补偿几何信息；4)最后通过时空状态空间模型(ST-SSM)处理序列，并用MLP进行分类预测。这一流程有效解决了点云视频的时空无序性问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)时空选择扫描(STSS)通过提示引导聚类将时空相似但距离远的点组织在一起；2)时空结构聚合(STSA)通过4D KNN和特征传播补偿序列化中丢失的几何细节；3)时间交互采样(TIS)通过多步采样扩展时间感受野。相比Mamba4D，UST-SSM克服了其仅基于前序点建模的限制；相比Transformer-based方法，UST-SSM具有线性复杂度和更低的内存消耗，在长序列建模中表现更好；相比传统扫描方法，STSS直接聚类相似点避免了长距离衰减问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UST-SSM通过时空选择扫描、时空结构聚合和时间交互采样三个关键技术，成功将选择性状态空间模型扩展到点云视频建模，解决了时空无序性问题，在保持计算效率的同时显著提升了点云视频分析的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud videos capture dynamic 3D motion while reducing the effects oflighting and viewpoint variations, making them highly effective for recognizingsubtle and continuous human actions. Although Selective State Space Models(SSMs) have shown good performance in sequence modeling with linear complexity,the spatio-temporal disorder of point cloud videos hinders their unidirectionalmodeling when directly unfolding the point cloud video into a 1D sequencethrough temporally sequential scanning. To address this challenge, we proposethe Unified Spatio-Temporal State Space Model (UST-SSM), which extends thelatest advancements in SSMs to point cloud videos. Specifically, we introduceSpatial-Temporal Selection Scanning (STSS), which reorganizes unordered pointsinto semantic-aware sequences through prompt-guided clustering, therebyenabling the effective utilization of points that are spatially and temporallydistant yet similar within the sequence. For missing 4D geometric and motiondetails, Spatio-Temporal Structure Aggregation (STSA) aggregatesspatio-temporal features and compensates. To improve temporal interactionwithin the sampled sequence, Temporal Interaction Sampling (TIS) enhancesfine-grained temporal dependencies through non-anchor frame utilization andexpanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,and Synthia 4D datasets validate the effectiveness of our method. Our code isavailable at https://github.com/wangzy01/UST-SSM.</description>
      <author>example@mail.com (Peiming Li, Ziyi Wang, Yulin Yuan, Hong Liu, Xiangming Meng, Junsong Yuan, Mengyuan Liu)</author>
      <guid isPermaLink="false">2508.14604v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR</title>
      <link>http://arxiv.org/abs/2508.14554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS 2025). This work has been submitted to the IEEE for  possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出EAROL框架，通过硬件创新和算法优化解决了无人机在开放式顶部场景中的定位漂移和感知-规划耦合问题，显著提高了搜救任务中的自主导航性能。&lt;h4&gt;背景&lt;/h4&gt;无人机在开放式顶部场景（如倒塌建筑物、无顶迷宫）中面临定位漂移和感知-规划耦合的挑战，这些特殊环境对无人机自主导航提出了严峻考验。&lt;h4&gt;目的&lt;/h4&gt;解决无人机在开放式顶部场景中的定位漂移和感知-规划耦合问题，提高其在灾难后搜救任务中的自主性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出EAROL框架，采用向下倾斜20度的LiDAR配置，集成LiDAR-惯性里程计系统，结合分层轨迹-偏航优化算法；使用迭代误差状态卡尔曼滤波实现紧耦合LIO系统；设计增强环境感知能力的规划器，平衡探索、跟踪和能耗。&lt;h4&gt;主要发现&lt;/h4&gt;物理实验表明，该方案实现了81%的跟踪误差减少，22%的感知覆盖率提升，以及在室内迷宫和60米规模户外场景中接近零的垂直漂移。&lt;h4&gt;结论&lt;/h4&gt;EAROL采用硬件-算法协同设计范式，为无人机在灾难后搜救任务中的自主性提供了稳健解决方案，并将软件和硬件作为开源包发布。&lt;h4&gt;翻译&lt;/h4&gt;为解决无人机在开放式顶部场景（如倒塌建筑物、无顶迷宫）中运行的定位漂移和感知-规划耦合挑战，本文提出EAROL，一种采用向下倾斜LiDAR配置（20度倾角）的新框架，集成了LiDAR-惯性里程计系统和分层轨迹-偏航优化算法。硬件创新通过获取密集地面点云和前向环境感知实现约束增强，用于动态障碍检测。由具有动态运动补偿的迭代误差状态卡尔曼滤波(IESKF)驱动的紧耦合LIO系统，在特征稀疏环境中实现了高精度6自由度定位。规划器增强了环境感知能力，平衡环境探索、目标跟踪精度和能源效率。物理实验表明，在室内迷宫和60米规模户外场景中，实现了81%的跟踪误差减少，22%的感知覆盖率提升，以及接近零的垂直漂移。这项工作提出了硬件-算法协同设计范式，为无人机在灾难后搜救任务中的自主性提供了稳健解决方案。我们将软件和硬件作为开源包发布给社区。视频：https://youtu.be/7av2ueLSiYw。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机在开放顶部场景（如倒塌建筑物、无顶迷宫）中的定位漂移和感知-规划耦合问题。这些问题在现实中非常重要，因为它们限制了无人机在灾难搜救等任务中的自主导航能力，在这些场景中，缺乏天花板特征会导致传统定位算法失效，而被动规划方法无法充分利用感知能力来优化导航效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用硬件-算法协同设计思路，创新性地设计了向下倾斜20°安装的激光雷达配置，同时获取地面约束和前方感知。在算法层面，作者借鉴了现有LiDAR SLAM框架（如LOAM、LIO-SAM）的基本结构，但引入了动态运动补偿和退化场景处理机制。在规划方面，作者从视觉感知感知规划中获取灵感，将其应用于激光雷达感知规划，并引入环境信息熵作为优化目标。整个设计体现了从问题出发，创新硬件配置，并针对性地开发算法解决方案的思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过向下倾斜安装激光雷达，同时获取地面点云（提供垂直约束）和前方环境数据（用于动态障碍物检测），并将硬件设计与算法紧密结合，形成硬件-算法协同设计范式。整体实现流程包括：1)硬件层：向下倾斜20°安装激光雷达，结合IMU等传感器；2)感知层：自适应激光雷达倾斜角补偿、基于IESKF的LIO系统、多层环境表示构建和实时动态物体跟踪；3)规划层：轨迹生成（考虑平滑性、动态可行性、避障和飞行时间）和偏航角优化（基于环境信息熵、目标跟踪误差和能量消耗的多目标优化）；4)控制层：非线性模型预测控制器和行为调节有限状态机。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)硬件创新：向下倾斜安装的激光雷达配置，提供垂直约束增强和前向感知增强；2)算法创新：基于IESKF的紧密耦合LIO系统具有动态运动补偿，退化场景感知处理，以及环境增强感知偏航角规划；3)系统集成：硬件-算法协同设计范式和分层优化框架。相比之前工作，EAROL的主要不同在于：传统方法依赖后处理优化，而EAROL采用硬件-算法协同设计；传统LiDAR规划被动对齐偏航角，EAROL主动优化偏航角以增强感知；EAROL专门针对开放顶部场景的垂直漂移问题，通过地面约束解决；传统方法使用全方位感知，EAROL采用方向性感知并优化感知-规划耦合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EAROL通过创新的向下倾斜激光雷达硬件设计和环境感知感知规划算法，有效解决了无人机在开放顶部场景中的定位漂移和感知-规划耦合问题，为灾难搜救等任务提供了鲁棒自主导航解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To address the challenges of localization drift and perception-planningcoupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novelframework with a downward-mounted tilted LiDAR configuration (20{\deg}inclination), integrating a LiDAR-Inertial Odometry (LIO) system and ahierarchical trajectory-yaw optimization algorithm. The hardware innovationenables constraint enhancement via dense ground point cloud acquisition andforward environmental awareness for dynamic obstacle detection. Atightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter(IESKF) with dynamic motion compensation, achieves high level 6-DoFlocalization accuracy in feature-sparse environments. The planner, augmented byenvironment, balancing environmental exploration, target tracking precision,and energy efficiency. Physical experiments demonstrate 81% tracking errorreduction, 22% improvement in perceptual coverage, and near-zero vertical driftacross indoor maze and 60-meter-scale outdoor scenarios. This work proposes ahardware-algorithm co-design paradigm, offering a robust solution for UAVautonomy in post-disaster search and rescue missions. We will release oursoftware and hardware as an open-source package for the community. Video:https://youtu.be/7av2ueLSiYw.</description>
      <author>example@mail.com (Xinkai Liang, Yigu Ge, Yangxi Shi, Haoyu Yang, Xu Cao, Hao Fang)</author>
      <guid isPermaLink="false">2508.14554v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network</title>
      <link>http://arxiv.org/abs/2508.14373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Transformer的粗到细点移动网络（TCFNet），用于解决正颌外科手术模拟中面部骨骼形状变换的准确性问题，克服了传统方法和现有深度学习方法的各种局限性。&lt;h4&gt;背景&lt;/h4&gt;计算机辅助手术模拟是正颌外科手术规划的关键环节，准确模拟面部骨骼形状变换至关重要。传统生物力学模拟方法计算时间长、数据处理劳动强度大且准确性低。基于深度学习的模拟方法虽有所改进，但仍存在无法处理大规模点、感受野有限导致噪声点、以及基于配准的复杂预处理和后处理操作等缺点，限制了其性能和广泛应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习密集面骨点云转换中补丁和点级别独特复杂对应关系的网络，提高模拟的准确性和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出TCFNet端到端框架，第一阶段采用基于Transformer的网络，第二阶段采用局部信息聚合网络（LIA-Net），两者相互强化生成精确点移动路径。LIA-Net通过建模局部几何结构（边缘、方向和相对位置特征）补偿Transformer网络的邻域精度损失，并使用门控循环单元利用先验全局特征指导局部位移。受可变形医学图像配准启发，提出辅助损失利用专家知识重建关键器官。&lt;h4&gt;主要发现&lt;/h4&gt;TCFNet在收集的数据集上与现有最先进方法相比，取得了卓越的评估指标和可视化结果，代码已公开在GitHub平台。&lt;h4&gt;结论&lt;/h4&gt;TCFNet有效解决了现有方法在处理大规模点云、感受野限制和复杂预处理后处理方面的局限性，提高了计算机辅助手术模拟的性能和适用性。&lt;h4&gt;翻译&lt;/h4&gt;计算机辅助手术模拟是正颌外科手术规划的关键组成部分，准确模拟面部骨骼形状变换具有重要意义。传统生物力学模拟方法受限于计算时间长、数据处理劳动强度大和低准确性。最近，基于深度学习的模拟方法被提出，将此问题视为骨骼和面部点云之间的点对点转换。然而，这些方法无法处理大规模点，感受野有限导致噪声点，并采用基于配准的复杂预处理和后处理操作。这些缺点限制了此类方法的性能和广泛应用。因此，我们提出了一种基于Transformer的粗到细点移动网络（TCFNet），用于学习密集面骨点云转换中补丁和点级别的独特复杂对应关系。该端到端框架在第一阶段采用基于Transformer的网络，在第二阶段采用局部信息聚合网络（LIA-Net），两者相互强化以生成精确的点移动路径。LIA-Net通过建模局部几何结构（边缘、方向和相对位置特征）可以有效地补偿基于Transformer网络的邻域精度损失。使用先验全局特征通过门控循环单元指导局部位移。受可变形医学图像配准启发，我们提出了一种辅助损失，可以利用专家知识重建关键器官。与收集的数据集上的现有最先进方法相比，TCFNet取得了卓越的评估指标和可视化结果。代码可在 https://github.com/Runshi-Zhang/TCFNet 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决面部骨骼与面部外观之间的双向转换问题，特别是在正颌外科手术规划中的应用。这个问题很重要，因为准确的手术规划需要精确模拟骨骼移动对面部外观的影响，而传统方法计算时间长、数据处理复杂、准确性低，现有深度学习方法无法有效处理大规模点云数据，限制了手术规划的准确性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统生物力学模拟方法的局限性和现有深度学习方法的问题，然后借鉴了Point Transformer V3处理大规模点云的能力，参考了P2P-Net的双向和循环结构框架，受多步框架启发采用粗到细策略，并参考了可变形医学图像配准中的辅助损失方法。基于这些工作，作者设计了TCFNet框架，包含基于Transformer的第一阶段和局部信息聚合网络的第二阶段，以解决全局特征和局部特征的平衡问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于Transformer的粗到细点移动网络学习面部和骨骼点云之间的复杂对应关系，结合全局特征和局部特征。整体流程包括：1)数据预处理（点云采样、去噪、标准化）；2)第一阶段（基于Transformer的网络处理全局特征，生成粗略位移和变形结果）；3)第二阶段（LIA-Net网络处理局部特征，结合全局特征生成精细位移）；4)损失函数计算（全局距离、正则化、局部距离和可选辅助损失）；5)后处理（法线估计、表面重建、平滑处理）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双向点云转换框架TCFNet，能处理密集点云；2)局部信息聚合网络LIA-Net，解决Transformer的邻域精度损失；3)可选辅助损失促进局部关键结构对应。相比之前工作：与P2P-Net相比，TCFNet能处理更多点云且不需要预配准；与P2P-Conv相比，具有更大感受野和更好全局对应；与PTv3相比，通过LIA-Net解决了局部结构建模问题，在点云增加时性能下降更小。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TCFNet通过结合Transformer架构和局部信息聚合网络，实现了高效准确的面部骨骼与面部外观之间的双向转换，为正颌外科手术规划提供了强大的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.media.2025.103653&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-aided surgical simulation is a critical component of orthognathicsurgical planning, where accurately simulating face-bone shape transformationsis significant. The traditional biomechanical simulation methods are limited bytheir computational time consumption levels, labor-intensive data processingstrategies and low accuracy. Recently, deep learning-based simulation methodshave been proposed to view this problem as a point-to-point transformationbetween skeletal and facial point clouds. However, these approaches cannotprocess large-scale points, have limited receptive fields that lead to noisypoints, and employ complex preprocessing and postprocessing operations based onregistration. These shortcomings limit the performance and widespreadapplicability of such methods. Therefore, we propose a Transformer-basedcoarse-to-fine point movement network (TCFNet) to learn unique, complicatedcorrespondences at the patch and point levels for dense face-bone point cloudtransformations. This end-to-end framework adopts a Transformer-based networkand a local information aggregation network (LIA-Net) in the first and secondstages, respectively, which reinforce each other to generate precise pointmovement paths. LIA-Net can effectively compensate for the neighborhoodprecision loss of the Transformer-based network by modeling local geometricstructures (edges, orientations and relative position features). The previousglobal features are employed to guide the local displacement using a gatedrecurrent unit. Inspired by deformable medical image registration, we proposean auxiliary loss that can utilize expert knowledge for reconstructing criticalorgans.Compared with the existing state-of-the-art (SOTA) methods on gathereddatasets, TCFNet achieves outstanding evaluation metrics and visualizationresults. The code is available at https://github.com/Runshi-Zhang/TCFNet.</description>
      <author>example@mail.com (Runshi Zhang, Bimeng Jie, Yang He, Junchen Wang)</author>
      <guid isPermaLink="false">2508.14373v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation</title>
      <link>http://arxiv.org/abs/2508.14358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025 Workshop on Recovering 6D Object Pose (R6D)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了HRC-Pose，一种新颖的仅使用深度的类别级物体姿态估计框架，通过对比学习学习保留6D姿态连续性的点云表示，实现了优于现有方法的性能并能实时运行。&lt;h4&gt;背景&lt;/h4&gt;类别级物体姿态估计旨在预测给定类别物体的6D姿态和3D尺寸，现有方法仅使用6D姿态作为监督信号，没有明确捕捉姿态的内在连续性，导致预测不一致和对未见姿态的泛化能力降低。&lt;h4&gt;目的&lt;/h4&gt;解决现有类别级物体姿态估计方法中缺乏姿态连续性捕捉的问题，提高预测的一致性和对未见姿态的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出HRC-Pose框架，将物体姿态解耦为旋转和平移组件，分别编码并利用；引入基于6D姿态感知层次排序方案的对比学习策略，用于多任务、多类别场景；设计分别处理旋转感知和平移感知嵌入的姿态估计模块。&lt;h4&gt;主要发现&lt;/h4&gt;HRC-Pose成功学习了连续特征空间；在REAL275和CAMERA25基准测试上持续优于现有仅使用深度的最先进方法；能够实时运行，适合实际应用。&lt;h4&gt;结论&lt;/h4&gt;HRC-Pose通过捕捉姿态的内在连续性，显著提高了类别级物体姿态估计的性能，具有实际应用潜力，代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;类别级物体姿态估计旨在预测给定类别物体的6D姿态和3D尺寸。现有方法仅依赖6D姿态作为监督信号，没有明确捕捉姿态的内在连续性，导致预测不一致和对未见姿态的泛化能力降低。为解决这一局限，我们提出了HRC-Pose，一种新颖的仅使用深度的框架，利用对比学习学习保留6D姿态连续性的点云表示。HRC-Pose将物体姿态解耦为旋转和平移组件，在网络中分别编码和利用。具体而言，我们引入了基于6D姿态感知层次排序方案的对比学习策略，用于多任务、多类别场景，该策略通过考虑旋转和平移差异以及类别信息来对比来自多个类别的点云。我们进一步设计了分别处理学习到的旋转感知和平移感知嵌入的姿态估计模块。实验证明HRC-Pose成功学习了连续特征空间。在REAL275和CAMERA25基准测试上的结果显示，我们的方法持续优于现有仅使用深度的最先进方法并能实时运行，证明了其有效性和实际应用潜力。我们的代码位于https://github.com/zhujunli1993/HRC-Pose。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决类别级6D物体姿态估计中姿态连续性捕捉不足的问题。现有方法仅将6D姿态作为监督信号，没有明确捕捉姿态的内在连续性，导致预测不一致和对未见姿态的泛化能力降低。这个问题在现实中很重要，因为准确的物体姿态估计对于机器人操作、场景理解、自动驾驶和增强现实等应用至关重要，而类别级姿态估计相比实例级姿态估计具有更广泛的适用性，不需要精确的CAD模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：仅将姿态作为训练目标导致特征碎片化，无法捕捉姿态连续性；现有方法专注于旋转而忽略平移；Rank-N-Contrast等对比学习方法只能处理单一任务和类别。作者借鉴了对比学习思想，参考了Rank-N-Contrast方法但扩展了它以处理多任务、多类别场景，同时利用了现有的3D-GCN等特征提取器和姿态估计模块。基于这些，作者设计了HRC-Pose框架，提出6D姿态感知的层次化排序方案，将姿态解耦为旋转和平移分量分别处理，并开发了专门的特征编码和姿态估计模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习学习点云表示，保持6D姿态的内在连续性，并将姿态解耦为旋转和平移分量分别处理。整体流程：1)输入处理：从RGB-D图像提取点云；2)特征提取：用两个独立编码器分别提取旋转和平移相关的点级嵌入；3)全局特征聚合：通过池化获得全局嵌入；4)层次化对比学习：基于6D姿态感知的层次化排序方案构建对比学习对，包括联合负样本和旋转/平移特定负样本；5)姿态估计：分别用旋转和平移感知嵌入预测相应姿态分量；6)损失计算：结合对比损失和姿态估计损失进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)层次化排名对比学习，保持旋转和平移的连续性；2)姿态解耦与分别处理，旋转和平移分量独立编码；3)多任务、多类别的对比学习框架；4)深度-only框架，不依赖RGB信息。相比之前工作的不同：相比RGB-D方法，专注于深度信息且明确捕捉姿态连续性；相比仅处理旋转的方法，同时考虑旋转和平移；相比Rank-N-Contrast，能处理多任务和多类别；相比其他深度-only方法，在保持实时性的同时提高了准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HRC-Pose通过引入层次化排名对比学习方法，成功解决了类别级6D物体姿态估计中姿态连续性捕捉不足的问题，仅使用深度信息就在保持实时性能的同时实现了最先进的姿态估计精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Category-level object pose estimation aims to predict the 6D pose and 3D sizeof objects within given categories. Existing approaches for this task relysolely on 6D poses as supervisory signals without explicitly capturing theintrinsic continuity of poses, leading to inconsistencies in predictions andreduced generalization to unseen poses. To address this limitation, we proposeHRC-Pose, a novel depth-only framework for category-level object poseestimation, which leverages contrastive learning to learn point cloudrepresentations that preserve the continuity of 6D poses. HRC-Pose decouplesobject pose into rotation and translation components, which are separatelyencoded and leveraged throughout the network. Specifically, we introduce acontrastive learning strategy for multi-task, multi-category scenarios based onour 6D pose-aware hierarchical ranking scheme, which contrasts point cloudsfrom multiple categories by considering rotational and translationaldifferences as well as categorical information. We further design poseestimation modules that separately process the learned rotation-aware andtranslation-aware embeddings. Our experiments demonstrate that HRC-Posesuccessfully learns continuous feature spaces. Results on REAL275 and CAMERA25benchmarks show that our method consistently outperforms existing depth-onlystate-of-the-art methods and runs in real-time, demonstrating its effectivenessand potential for real-world applications. Our code is athttps://github.com/zhujunli1993/HRC-Pose.</description>
      <author>example@mail.com (Zhujun Li, Shuo Zhang, Ioannis Stamos)</author>
      <guid isPermaLink="false">2508.14358v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values</title>
      <link>http://arxiv.org/abs/2508.14083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, incomplete version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的对比自学习框架(CST)用于从低质量数据中推断兴趣点(POIs)的准确人群流动，解决了标记数据稀缺、POI间复杂时空依赖以及精确人群流动与GPS报告间多种相关性等挑战。&lt;h4&gt;背景&lt;/h4&gt;准确获取兴趣点的人群流动数据对交通管理、公共服务和城市规划至关重要，但由于城市传感技术限制，大多数来源的数据质量不足以监控每个POI的人群流动。&lt;h4&gt;目的&lt;/h4&gt;从低质量数据中推断准确的人群流动，解决标记数据稀缺、POI间复杂时空依赖以及精确人群流动与GPS报告间多种相关性等挑战。&lt;h4&gt;方法&lt;/h4&gt;将人群流动推断问题重新表述为自监督属性图表示学习任务，构建基于POIs及其距离的空间邻接图，采用对比学习技术利用大量未标记的时空数据，采用交换预测方法预测相似实例的目标子图表示，并在预训练后使用准确人群流动数据对模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实数据集上的实验表明，CST模型在大量噪声数据上预训练后，性能始终优于从头开始训练的模型。&lt;h4&gt;结论&lt;/h4&gt;CST框架能有效解决人群流动推断中的挑战，特别是在标记数据稀缺的情况下。&lt;h4&gt;翻译&lt;/h4&gt;准确获取兴趣点(POIs)的人群流动数据对于有效的交通管理、公共服务和城市规划至关重要。尽管如此，由于城市传感技术的局限性，大多数来源的数据质量不足以监控每个POI的人群流动。这使得从低质量数据中推断准确的人群流动成为一个关键且具有挑战性的任务。这种复杂性因三个关键因素而加剧：1)标记数据的稀缺性和稀有性，2)POI之间复杂的时空依赖关系，3)精确人群流动与GPS报告之间的多种相关性。为应对这些挑战，我们将人群流动推断问题重新表述为自监督属性图表示学习任务，并引入了一个新的用于时空数据的对比自学习框架(CST)。我们的方法首先基于POIs及其距离构建空间邻接图。然后采用对比学习技术利用大量未标记的时空数据。我们采用交换预测方法来预测相似实例的目标子图表示。在预训练阶段后，使用准确的人群流动数据对模型进行微调。我们在两个真实数据集上进行的实验表明，CST在大量噪声数据上预训练后，性能始终优于从头开始训练的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotalfor effective traffic management, public service, and urban planning. Despitethis importance, due to the limitations of urban sensing techniques, the dataquality from most sources is inadequate for monitoring crowd flow at each POI.This renders the inference of accurate crowd flow from low-quality data acritical and challenging task. The complexity is heightened by three keyfactors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{Theintricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriadcorrelations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as aself-supervised attributed graph representation learning task and introduce anovel \underline{C}ontrastive \underline{S}elf-learning framework for\underline{S}patio-\underline{T}emporal data (\model). Our approach initiateswith the construction of a spatial adjacency graph founded on the POIs andtheir respective distances. We then employ a contrastive learning technique toexploit large volumes of unlabeled spatio-temporal data. We adopt a swappedprediction approach to anticipate the representation of the target subgraphfrom similar instances. Following the pre-training phase, the model isfine-tuned with accurate crowd flow data. Our experiments, conducted on tworeal-world datasets, demonstrate that the \model pre-trained on extensive noisydata consistently outperforms models trained from scratch.</description>
      <author>example@mail.com (Songyu Ke, Chenyu Wu, Yuxuan Liang, Xiuwen Yi, Yanping Sun, Junbo Zhang, Yu Zheng)</author>
      <guid isPermaLink="false">2508.14083v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition</title>
      <link>http://arxiv.org/abs/2508.14889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多骨架对比学习(MS-CLR)的自监督框架，通过处理多种骨架约定来提高基于骨架的动作识别性能。&lt;h4&gt;背景&lt;/h4&gt;对比学习在基于骨架的动作识别领域受到关注，但现有方法依赖单一骨架约定，限制了跨数据集的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的自监督框架，能够对齐不同骨架 convention 的姿态表示，提高模型在多样化数据集上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出MS-CLR框架，通过统一表示方案处理不同关节布局和尺度的骨架，并适配ST-GCN架构以处理多骨架数据。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB+D 60和120数据集上，MS-CLR相比单一骨架对比学习基线方法持续提高性能，多骨架集成进一步提升了性能，达到新的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;MS-CLR能够学习结构不变性并捕获多样化解剖线索，产生更具表达力和泛化能力的特征。&lt;h4&gt;翻译&lt;/h4&gt;对比学习在基于骨架的动作识别领域获得了显著关注，因为它能够从未标记数据中学习鲁棒表示。然而，现有方法依赖于单一的骨架约定，这限制了它们在不同具有不同关节结构和解剖覆盖的数据集上的泛化能力。我们提出多骨架对比学习(MS-CLR)，一种通用的自监督框架，该框架对齐从同一序列中提取的多种骨架 convention 的姿态表示。这鼓励模型学习结构不变性并捕获多样化的解剖线索，从而产生更具表达力和泛化能力的特征。为此，我们调整了ST-GCN架构，通过统一表示方案处理具有不同关节布局和尺度的骨架。在NTU RGB+D 60和120数据集上的实验表明，MS-CLR相比强大的单一骨架对比学习基线方法能够持续提高性能。多骨架集成进一步提高了性能，在这两个数据集上都设置了新的最先进结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的问题是现有基于骨架的人体动作识别方法依赖于单一骨架格式，限制了模型在不同骨架结构和解剖覆盖度的数据集上的泛化能力。这个问题在现实中很重要，因为不同动作捕捉系统使用不同的骨架格式（如Kinectv2有25个关节点，SMPL-X有42个关节点），且随着隐私问题日益突出，仅使用骨架数据的动作识别变得越来越重要，但现有方法仍受限于特定骨架格式。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的对比学习方法（如AimCLR、ActCLR）虽有效但都依赖单一骨架格式，意识到不同骨架格式提供了结构多样性可作为监督信号。他们借鉴了ST-GCN作为骨干网络，MoCo v2的对比学习框架，AimCLR的数据增强策略，以及MeTRAbs姿态估计器来生成多种骨架格式，设计了多骨架对比学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多种骨架格式作为'结构增强'，通过对比学习对齐不同骨架格式中同一动作的表示，让模型学习结构不变性。整体流程包括：1)从RGB视频提取多种骨架格式；2)通过零填充将不同骨架映射到统一空间；3)使用改进的ST-GCN提取特征；4)使用多骨架对比损失对齐表示；5)采用动量对比学习框架训练；6)推理时可单独使用共享表示或使用骨架特定分类器集成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多骨架对比学习框架，利用多种骨架格式作为结构增强；2)统一的骨架ST-GCN，能处理多种骨架格式；3)骨架特定分类器集成策略。相比之前的工作，MS-CLR不再依赖单一骨架格式，而是使用不同骨架格式作为'结构增强'而非传统数据增强，学习结构不变的表示而非特定骨架表示，从而显著提高了模型的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MS-CLR通过利用多种骨架格式的结构多样性进行对比学习，显著提高了人体动作识别模型的鲁棒性和泛化能力，并在多个基准数据集上取得了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning has gained significant attention in skeleton-basedaction recognition for its ability to learn robust representations fromunlabeled data. However, existing methods rely on a single skeleton convention,which limits their ability to generalize across datasets with diverse jointstructures and anatomical coverage. We propose Multi-Skeleton ContrastiveLearning (MS-CLR), a general self-supervised framework that aligns poserepresentations across multiple skeleton conventions extracted from the samesequence. This encourages the model to learn structural invariances and capturediverse anatomical cues, resulting in more expressive and generalizablefeatures. To support this, we adapt the ST-GCN architecture to handle skeletonswith varying joint layouts and scales through a unified representation scheme.Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLRconsistently improves performance over strong single-skeleton contrastivelearning baselines. A multi-skeleton ensemble further boosts performance,setting new state-of-the-art results on both datasets.</description>
      <author>example@mail.com (Mert Kiray, Alvaro Ritter, Nassir Navab, Benjamin Busam)</author>
      <guid isPermaLink="false">2508.14889v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Contrastive Link Prediction With Edge Balancing Augmentation</title>
      <link>http://arxiv.org/abs/2508.14808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的链接预测方法CoEBA，通过理论分析和图增强技术解决了现有对比学习方法在链接预测中的两个主要弱点。&lt;h4&gt;背景&lt;/h4&gt;链接预测是图挖掘中最基础的任务之一，近年来有研究使用对比学习来增强其性能，但存在缺乏理论分析和未充分考虑节点度的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有对比学习方法在链接预测中的两个主要弱点：缺乏理论分析和未充分考虑节点度。&lt;h4&gt;方法&lt;/h4&gt;提供对比学习在链接预测上的首个正式理论分析，提出Edge Balancing Augmentation (EBA)图增强方法，以及Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA)方法。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析结果可推广到基于自编码器的链接预测模型，提出的CoEBA方法在8个基准数据集上显著优于其他最先进的链接预测模型。&lt;h4&gt;结论&lt;/h4&gt;通过理论分析和创新的图增强方法，CoEBA有效提升了链接预测性能。&lt;h4&gt;翻译&lt;/h4&gt;链接预测是图挖掘中最基础的任务之一，这促使了近期利用对比学习增强性能的研究。然而，我们观察到这些研究存在两个主要弱点：i)缺乏对比学习在链接预测上的理论分析，ii)对比学习中未充分考虑节点度。为解决上述弱点，我们首次提供了对比学习在链接预测上的正式理论分析，我们的分析结果可推广到带有对比学习的自编码器基础链接预测模型。受分析结果启发，我们提出了一种新的图增强方法，边平衡增强(EBA)，它通过调整节点度进行图增强。然后，我们提出了一种新方法，名为边平衡增强对比链接预测(CoEBA)，它整合了提出的EBA和新提出的对比损失来提高模型性能。我们在8个基准数据集上进行了实验。结果表明，我们提出的CoEBA显著优于其他最先进的链接预测模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is one of the most fundamental tasks in graph mining, whichmotivates the recent studies of leveraging contrastive learning to enhance theperformance. However, we observe two major weaknesses of these studies: i) thelack of theoretical analysis for contrastive learning on link prediction, andii) inadequate consideration of node degrees in contrastive learning. Toaddress the above weaknesses, we provide the first formal theoretical analysisfor contrastive learning on link prediction, where our analysis results cangeneralize to the autoencoder-based link prediction models with contrastivelearning. Motivated by our analysis results, we propose a new graphaugmentation approach, Edge Balancing Augmentation (EBA), which adjusts thenode degrees in the graph as the augmentation. We then propose a new approach,named Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA),that integrates the proposed EBA and the proposed new contrastive losses toimprove the model performance. We conduct experiments on 8 benchmark datasets.The results demonstrate that our proposed CoEBA significantly outperforms theother state-of-the-art link prediction models.</description>
      <author>example@mail.com (Chen-Hao Chang, Hui-Ju Hung, Chia-Hsun Lu, Chih-Ya Shen)</author>
      <guid isPermaLink="false">2508.14808v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.14574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的神经手语生成模型，通过四元数空间骨骼旋转编码和语义对比损失提高对手势变异的鲁棒性&lt;h4&gt;背景&lt;/h4&gt;神经手语生成面临的主要挑战是手势类别内的高变异性，这源于手语者的形态特征和训练数据中的风格多样性&lt;h4&gt;目的&lt;/h4&gt;提高模型对这类变异的鲁棒性，改善手语生成的准确性和清晰度&lt;h4&gt;方法&lt;/h4&gt;对标准Progressive Transformers架构提出两种增强：1)使用四元数空间的骨骼旋转编码姿势并采用测地线损失训练；2)引入对比损失通过语义相似性结构化解码器嵌入，过滤无关的解剖和风格特征&lt;h4&gt;主要发现&lt;/h4&gt;在Phoenix14T数据集上，对比损失单独使用可使关键点正确概率比PT基线提高16%；结合四元数姿势编码后，模型实现了骨骼角度平均误差减少6%&lt;h4&gt;结论&lt;/h4&gt;将骨骼结构建模和语义引导的对比目标纳入Transformer-based SLP模型的训练中，对手势姿势表示有显著益处&lt;h4&gt;翻译&lt;/h4&gt;神经手语生成的主要挑战之一在于手势类别内的高变异性，这源于手语者的形态特征和训练数据中的风格多样性。为提高对此类变异的鲁棒性，我们对标准Progressive Transformers架构提出了两种改进。首先，我们使用四元数空间的骨骼旋转来编码姿势，并通过测地线损失进行训练，以提高关节角度运动的准确性和清晰度。其次，我们引入对比损失，通过语义相似性结构化解码器嵌入，使用词汇重叠或基于SBERT的句子相似性，旨在过滤掉不传达相关语义信息的解剖和风格特征。在Phoenix14T数据集上，仅对比损失就比PT基线在关键点正确概率上提高了16%。当结合基于四元数的姿势编码时，模型实现了骨骼角度平均误差减少6%。这些结果表明，将骨骼结构建模和语义引导的对比目标纳入基于Transformer的SLP模型的训练中，对手势姿势表示有益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One of the main challenges in neural sign language production (SLP) lies inthe high intra-class variability of signs, arising from signer morphology andstylistic variety in the training data. To improve robustness to suchvariations, we propose two enhancements to the standard ProgressiveTransformers (PT) architecture (Saunders et al., 2020). First, we encode posesusing bone rotations in quaternion space and train with a geodesic loss toimprove the accuracy and clarity of angular joint movements. Second, weintroduce a contrastive loss to structure decoder embeddings by semanticsimilarity, using either gloss overlap or SBERT-based sentence similarity,aiming to filter out anatomical and stylistic features that do not conveyrelevant semantic information. On the Phoenix14T dataset, the contrastive lossalone yields a 16% improvement in Probability of Correct Keypoint over the PTbaseline. When combined with quaternion-based pose encoding, the model achievesa 6% reduction in Mean Bone Angle Error. These results point to the benefit ofincorporating skeletal structure modeling and semantically guided contrastiveobjectives on sign pose representations into the training of Transformer-basedSLP models.</description>
      <author>example@mail.com (Guilhem Fauré, Mostafa Sadeghi, Sam Bigeard, Slim Ouni)</author>
      <guid isPermaLink="false">2508.14574v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever</title>
      <link>http://arxiv.org/abs/2508.14323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种行为对齐检索器(BAR)来提高增强型大型语言模型(LLMs)的工具使用准确性，通过提供行为一致的演示样本，减少错误的函数调用，同时保持高任务性能，实现经济高效的解决方案。&lt;h4&gt;背景&lt;/h4&gt;增强型大型语言模型利用外部函数扩展能力，但函数调用不准确会导致效率低下和成本增加。现有方法通过微调LLMs或基于提示的演示来解决这个问题，但它们通常面临高训练开销的问题，并且无法处理不一致的演示样本，这些样本会误导模型的调用行为。&lt;h4&gt;目的&lt;/h4&gt;训练一个行为对齐检索器(BAR)，提供行为一致的演示，以帮助LLMs做出更准确的工具使用决策。&lt;h4&gt;方法&lt;/h4&gt;构建一个包含不同函数调用行为(调用或不调用)的语料库，使用对比学习框架训练BAR，采用定制的正/负对和双负对比损失，确保行为一致例子的稳健检索。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法显著减少了错误的函数调用，同时保持了高任务性能，为增强型LLMs提供了一种经济高效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;行为对齐检索器(BAR)能够有效提高增强型LLMs的工具使用准确性，减少错误函数调用，同时保持高任务性能，是一种成本效益高的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;增强工具的大型语言模型利用外部函数扩展其能力，但不准确的函数调用会导致效率低下和成本增加。现有方法通过微调LLMs或使用基于演示的提示来解决这一挑战，但它们通常遭受高训练开销的困扰，并且无法考虑不一致的演示样本，这些样本会误导模型的调用行为。在本文中，我们训练了一个行为对齐检索器(BAR)，它提供行为一致的演示，帮助LLMs做出更准确的工具使用决策。为了训练BAR，我们构建了一个包含不同函数调用行为(即调用或不调用)的语料库。我们使用对比学习框架，通过定制的正/负对和双负对比损失来训练BAR，确保行为一致例子的稳健检索。实验证明，我们的方法显著减少了错误的函数调用，同时保持了高任务性能，为增强工具的LLMs提供了一种经济高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tool-augmented large language models (LLMs) leverage external functions toextend their capabilities, but inaccurate function calls can lead toinefficiencies and increased costs.Existing methods address this challenge byfine-tuning LLMs or using demonstration-based prompting, yet they often sufferfrom high training overhead and fail to account for inconsistent demonstrationsamples, which misguide the model's invocation behavior. In this paper, wetrained a behavior-aligned retriever (BAR), which provides behaviorallyconsistent demonstrations to help LLMs make more accurate tool-using decisions.To train the BAR, we construct a corpus including different function-callingbehaviors, i.e., calling or non-calling.We use the contrastive learningframework to train the BAR with customized positive/negative pairs and adual-negative contrastive loss, ensuring robust retrieval of behaviorallyconsistent examples.Experiments demonstrate that our approach significantlyreduces erroneous function calls while maintaining high task performance,offering a cost-effective and efficient solution for tool-augmented LLMs.</description>
      <author>example@mail.com (Yixin Chen, Ying Xiong, Shangyu Wu, Yufei Cui, Xue Liu, Nan Guan, Chun Jason Xue)</author>
      <guid isPermaLink="false">2508.14323v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.14278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GALA框架，一种基于3D高斯飞溅的开词汇表3D场景理解方法，通过自监督对比学习和交叉注意模块实现了从2D图像中捕捉细粒度、语言感知的3D表示。&lt;h4&gt;背景&lt;/h4&gt;3D场景重建和理解技术日益受到关注，但现有方法难以从2D图像中有效捕捉细粒度且具有语言感知能力的3D表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖的框架，用于实现开词汇表3D场景理解，能够支持2D和3D的语义查询，同时减少内存消耗。&lt;h4&gt;方法&lt;/h4&gt;GALA通过自监督对比学习蒸馏场景特定的3D实例特征场，并引入核心贡献——一个具有两个可学习码本的交叉注意模块，用于编码视图无关的语义嵌入，确保实例内特征相似性并支持无缝的2D和3D开词汇表查询。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的大量实验证明，GALA在2D和3D开词汇表任务上展现出卓越的性能，同时避免了每个高维特征学习，有效减少了内存消耗。&lt;h4&gt;结论&lt;/h4&gt;GALA框架成功解决了现有方法在捕捉细粒度、语言感知3D表示方面的挑战，为开词汇表3D场景理解提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;三维场景重建和理解日益受到关注，但现有方法仍难以从二维图像中捕捉细粒度、语言感知的三维表示。在本文中，我们提出了GALA，一种基于三维高斯飞溅的开词汇表三维场景理解新框架。GALA通过自监督对比学习蒸馏场景特定的三维实例特征场。为了扩展到通用语言特征场，我们引入了GALA的核心贡献——一个具有两个可学习码本的交叉注意模块，用于编码视图无关的语义嵌入。这种设计不仅确保了实例内特征相似性，还支持无缝的二维和三维开词汇表查询。它通过避免每个高维特征学习来减少内存消耗。在真实世界数据集上的大量实验证明了GALA在二维和三维开词汇表任务上的卓越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景理解中的开放词汇场景理解问题，即在3D场景中实现细粒度的语言感知表示，同时支持2D和3D的开放词汇查询。这个问题在现实中非常重要，因为3D场景理解是自动驾驶、机器人技术和AR/VR等应用的核心挑战，开放词汇能力能让机器人通过自然语言与人类交互，理解复杂场景，而现有的方法要么专注于2D或3D中的一方面，要么存在信息损失和特征一致性问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行设计：首先认识到NeRF方法计算效率低，3DGS方法在语义特征存储上存在问题；然后借鉴了Scaffold-GS进行场景重建，使用SAM生成实例掩码，采用CLIP进行语言对齐，并参考了codebook设计思路。作者采用两阶段训练策略：第一阶段自监督重建场景几何和实例特征场；第二阶段引入双码本和引导交叉注意力模块，将场景特征映射到语言特征空间，确保实例内特征一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过码本存储每个实例的语义嵌入而非每个高斯的语义特征，使用引导注意力机制实现场景特征到语言特征的映射，并确保实例内特征一致性。整体流程分为两阶段：第一阶段使用Scaffold-GS重建场景，生成几何和分割特征，通过对比学习增强实例内一致性；第二阶段构建实例码本和语言码本，使用交叉注意力机制将特征映射到语言空间，并通过注意力权重和熵损失确保一对一映射。推理时支持2D渲染和3D直接查询。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双码本设计减少内存消耗并确保实例语义一致性；2)引导交叉注意力模块实现场景特征到语言特征的无缝映射；3)注意力加权熵损失确保码本与实例的一对一映射；4)自监督对比学习增强实例内特征一致性。相比之前工作：不同于LangSplat的压缩方法避免信息损失；不同于OpenGaussian的KNN聚类避免实例分割错误；不同于SuperGSeg的MLP预测使用更灵活的注意力机制；不同于GOI的离散码本索引提供更细粒度语义控制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GALA通过双码本设计和引导交叉注意力机制，实现了高效的开放词汇3D场景理解，同时保持2D和3D查询能力并显著提升实例内特征一致性，为机器人和AR/VR应用提供了更精确的语义理解工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D scene reconstruction and understanding have gained increasing popularity,yet existing methods still struggle to capture fine-grained, language-aware 3Drepresentations from 2D images. In this paper, we present GALA, a novelframework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting(3DGS). GALA distills a scene-specific 3D instance feature field viaself-supervised contrastive learning. To extend to generalized language featurefields, we introduce the core contribution of GALA, a cross-attention modulewith two learnable codebooks that encode view-independent semantic embeddings.This design not only ensures intra-instance feature similarity but alsosupports seamless 2D and 3D open-vocabulary queries. It reduces memoryconsumption by avoiding per-Gaussian high-dimensional feature learning.Extensive experiments on real-world datasets demonstrate GALA's remarkableopen-vocabulary performance on both 2D and 3D.</description>
      <author>example@mail.com (Elena Alegret Regalado, Kunyi Li, Sen Wang, Siyun Liang, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari)</author>
      <guid isPermaLink="false">2508.14278v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants</title>
      <link>http://arxiv.org/abs/2508.14129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究应用目标检测Transformer模型改进手腕和手部骨折的X光片诊断，开发了基于Co-DETR的高精度检测流程，在真实世界X光片上达到83.1%的准确率，具有临床实用价值。&lt;h4&gt;背景&lt;/h4&gt;准确诊断手腕和手部骨折对急诊护理至关重要，但手动解读X光片速度慢且易出错。基于Transformer的模型在医学图像分析中显示出潜力，但在四肢骨折方面的应用有限。&lt;h4&gt;目的&lt;/h4&gt;应用目标检测Transformer到手腕和手的X光片，填补其在骨折诊断中的应用空白。&lt;h4&gt;方法&lt;/h4&gt;使用超过26,000张带有标注的临床数据集X光片对RT-DETR和Co-DETR模型进行微调；训练ResNet-50分类器对裁剪区域进行异常分类细化；采用监督对比学习增强嵌入质量；使用AP@50、精确度和召回率指标进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;RT-DETR表现中等（AP@50 = 0.39），而Co-DETR表现更优（AP@50 = 0.615）且收敛更快；集成流程在真实世界X光片上达到83.1%的准确率、85.1%的精确度和96.4%的召回率；在13种骨折类型中显示出强大的泛化能力；视觉检查确认了准确的定位。&lt;h4&gt;结论&lt;/h4&gt;基于Co-DETR的流程在手腕和手部骨折检测中表现出高准确率和临床相关性，提供可靠的骨折定位和类型区分，可扩展、高效，适合在医院工作流程中实时部署，提高肌肉骨骼放射学中的诊断速度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;背景：使用X光片准确诊断手腕和手部骨折在急诊护理中至关重要，但手动解读速度慢且容易出错。基于Transformer的模型在改善医学图像分析方面显示出潜力，但其在四肢骨折中的应用有限。本研究通过将目标检测Transformer应用于手腕和手部X光片解决了这一空白。方法：我们使用超过26,000张来自专有临床数据集的标注X光片对在COCO上预训练的RT-DETR和Co-DETR模型进行了微调。每张图像都标有骨折存在情况和边界框。在裁剪区域上训练了ResNet-50分类器以细化异常分类。使用监督对比学习增强嵌入质量。使用AP@50、精确度和召回率指标进行性能评估，并在真实世界X光片上进行额外测试。结果：RT-DETR显示中等结果（AP@50 = 0.39），而Co-DETR表现更好，AP@50为0.615，收敛更快。集成流程在真实世界X光片上达到83.1%的准确率、85.1%的精确度和96.4%的召回率，在13种骨折类型中显示出强大的泛化能力。视觉检查确认了准确的定位。结论：我们的基于Co-DETR的流程在手腕和手部骨折检测中表现出高准确率和临床相关性，提供可靠的定位和骨折类型区分。它可扩展、高效，适合在医院工作流程中实时部署，提高肌肉骨骼放射学中的诊断速度和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background: Accurate diagnosis of wrist and hand fractures using radiographsis essential in emergency care, but manual interpretation is slow and prone toerrors. Transformer-based models show promise in improving medical imageanalysis, but their application to extremity fractures is limited. This studyaddresses this gap by applying object detection transformers to wrist and handX-rays.  Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO,using over 26,000 annotated X-rays from a proprietary clinical dataset. Eachimage was labeled for fracture presence with bounding boxes. A ResNet-50classifier was trained on cropped regions to refine abnormality classification.Supervised contrastive learning was used to enhance embedding quality.Performance was evaluated using AP@50, precision, and recall metrics, withadditional testing on real-world X-rays.  Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETRoutperformed it with an AP@50 of 0.615 and faster convergence. The integratedpipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall onreal-world X-rays, demonstrating strong generalization across 13 fracturetypes. Visual inspection confirmed accurate localization.  Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy andclinical relevance in wrist and hand fracture detection, offering reliablelocalization and differentiation of fracture types. It is scalable, efficient,and suitable for real-time deployment in hospital workflows, improvingdiagnostic speed and reliability in musculoskeletal radiology.</description>
      <author>example@mail.com (Aditya Bagri, Vasanthakumar Venugopal, Anandakumar D, Revathi Ezhumalai, Kalyan Sivasailam, Bargava Subramanian, VarshiniPriya, Meenakumari K S, Abi M, Renita S)</author>
      <guid isPermaLink="false">2508.14129v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification</title>
      <link>http://arxiv.org/abs/2508.14779v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages,6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究首次系统分析了病理学基础模型(PFMs)中的医院来源偏差问题，提出了一种轻量级对抗框架来减少模型对特定医院特征的依赖，实验证明该方法能有效降低领域可预测性同时保持或提高疾病分类性能。&lt;h4&gt;背景&lt;/h4&gt;病理学基础模型(PFMs)在全切片图像(WSI)诊断方面表现出巨大潜力，但不同医院的病理图像因扫描硬件和预处理方式的差异而有所不同，可能导致PFMs无意中学习到特定医院的特征，对临床应用构成风险。&lt;h4&gt;目的&lt;/h4&gt;对由医院来源特征引起的PFMs中的领域偏差进行首次系统性研究，构建量化领域偏差的流程，评估和比较多个模型的性能，并提出一种轻量级对抗框架移除隐含的医院特定特征。&lt;h4&gt;方法&lt;/h4&gt;提出一种轻量级对抗框架，在不修改编码器本身的情况下，通过引入可训练适配器和通过梯度反转层(GRL)连接的领域分类器，学习具有任务判别性但领域不变性的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多中心组织病理学数据集上的实验表明，该方法显著降低了领域可预测性，同时保持或提高了疾病分类性能，特别是在跨域(未见过的医院)场景中；医院检测和特征空间可视化分析证实了该方法在减轻医院偏差方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地减轻了PFMs中的医院偏差问题，在保持或提高疾病分类性能的同时，减少了模型对特定医院特征的依赖，有望促进PFMs在临床环境中的部署和应用。&lt;h4&gt;翻译&lt;/h4&gt;病理学基础模型(PFMs)在全切片图像(WSI)诊断中展现出巨大潜力。然而，不同医院的病理图像常因扫描硬件和预处理风格的差异而有所不同，这可能导致PFMs无意中学习到特定医院的特征，对临床部署构成风险。在本工作中，我们首次对源于医院来源特征的PFMs领域偏差进行了系统性研究。具体而言，我们构建了一个量化PFMs中领域偏差的流程，评估和比较了多个模型的性能，并提出了一个轻量级对抗框架，在不修改编码器本身的情况下，从冻结表示中移除隐含的医院特定特征。通过引入一个可训练的适配器和一个通过梯度反转层(GRL)连接的领域分类器，我们的方法学习具有任务判别性但领域不变性的表示。在多中心组织病理学数据集上的实验表明，我们的方法显著降低了领域可预测性，同时保持甚至提高了疾病分类性能，特别是在跨域(未见医院)场景中。包括医院检测和特征空间可视化在内的进一步分析证实了我们的方法在减轻医院偏差方面的有效性。我们将在接受后提供代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pathology foundation models (PFMs) have demonstrated remarkable potential inwhole-slide image (WSI) diagnosis. However, pathology images from differenthospitals often vary due to differences in scanning hardware and preprocessingstyles, which may lead PFMs to inadvertently learn hospital-specific features,posing risks for clinical deployment. In this work, we present the firstsystematic study of domain bias in PFMs arising from hospital sourcecharacteristics. Specifically, we (1) construct a pipeline for quantifyingdomain bias in PFMs, (2) evaluate and compare the performance of multiplemodels, and (3) propose a lightweight adversarial framework that removes latenthospital-specific features from frozen representations without modifying theencoder itself. By introducing a trainable adapter and a domain classifierconnected through a gradient reversal layer (GRL), our method learnstask-discriminative yet domain-invariant representations. Experiments onmulti-center histopathology datasets demonstrate that our approachsubstantially reduces domain predictability while maintaining or even improvingdisease classification performance, particularly in out-of-domain (unseenhospital) scenarios. Further analyses, including hospital detection and featurespace visualization, confirm the effectiveness of our method in mitigatinghospital bias. We will provide our code based on acceptance.</description>
      <author>example@mail.com (Mengliang Zhang, Jacob M. Luber)</author>
      <guid isPermaLink="false">2508.14779v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal</title>
      <link>http://arxiv.org/abs/2508.14689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的基础模型ECHO，用于通用机器信号建模，解决了现有方法在输入长度限制和频率位置编码方面的不足，并在多个数据集上展示了优越的性能。&lt;h4&gt;背景&lt;/h4&gt;预训练基础模型在视觉和语言领域取得了显著成功，但在通用机器信号建模（包括声学、振动和其他工业传感器数据）方面的潜力尚未充分探索。现有的基于子带编码器的方法虽取得有竞争力的结果，但受限于固定输入长度和缺乏显式的频率位置编码。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基础模型，结合先进的带分裂架构和相对频率位置嵌入，实现在任意采样配置下精确的频谱定位，并支持任意长度的输入。&lt;h4&gt;方法&lt;/h4&gt;开发了一种集成先进带分裂架构和相对频率位置嵌入的基础模型，该模型支持任意长度的输入而不需要填充或分割，能够产生保留时间和频谱保真度的简洁嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;在SIREN（统一了多个数据集的大型机器信号编码基准测试）上的实验结果表明，该方法在异常检测和故障识别方面持续达到最先进性能，证明了模型的有效性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的ECHO模型为通用机器信号建模提供了有效解决方案，已在GitHub上开源，可供进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;预训练基础模型在视觉和语言领域已展现出显著的成功，然而它们在通用机器信号建模（涵盖声学、振动和其他工业传感器数据）方面的潜力仍未得到充分探索。现有的基于子带编码器的方法已取得有竞争力的结果，但受限于固定的输入长度和缺乏显式的频率位置编码。在这项工作中，我们提出了一种新的基础模型，它集成了先进的带分裂架构和相对频率位置嵌入，能够在任意采样配置下实现精确的频谱定位。该模型支持任意长度的输入而无需填充或分割，产生保留时间和频谱保真度的简洁嵌入。我们在SIREN（一个新引入的大型机器信号编码基准测试，统一了多个数据集，包括所有DCASE任务2挑战赛（2020-2025）和广泛使用的工业信号语料库）上评估了我们的方法。实验结果证明了该方法在异常检测和故障识别方面持续达到最先进性能，确认了所提出模型的有效性和泛化能力。我们在https://github.com/yucongzh/ECHO上开源了ECHO。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained foundation models have demonstrated remarkable success in visionand language, yet their potential for general machine signal modeling-coveringacoustic, vibration, and other industrial sensor data-remains under-explored.Existing approach using sub-band-based encoders has achieved competitiveresults but are limited by fixed input lengths, and the absence of explicitfrequency positional encoding. In this work, we propose a novel foundationmodel that integrates an advanced band-split architecture with relativefrequency positional embeddings, enabling precise spectral localization acrossarbitrary sampling configurations. The model supports inputs of arbitrarylength without padding or segmentation, producing a concise embedding thatretains both temporal and spectral fidelity. We evaluate our method on SIREN(https://github.com/yucongzh/SIREN), a newly introduced large-scale benchmarkfor machine signal encoding that unifies multiple datasets, including all DCASEtask 2 challenges (2020-2025) and widely-used industrial signal corpora.Experimental results demonstrate consistent state-of-the-art performance inanomaly detection and fault identification, confirming the effectiveness andgeneralization capability of the proposed model. We open-sourced ECHO onhttps://github.com/yucongzh/ECHO.</description>
      <author>example@mail.com (Yucong Zhang, Juan Liu, Ming Li)</author>
      <guid isPermaLink="false">2508.14689v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels</title>
      <link>http://arxiv.org/abs/2508.14563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GOGS的新型两阶段逆渲染框架，基于2D高斯表面元，解决了从RGB图像重建高光物体时存在的模糊性、计算成本高和多视图不一致等问题，实现了几何重建、材料分离和新照明下的照片级真实感重照明的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;从RGB图像进行高光物体的逆渲染存在根本性模糊性限制；基于NeRF的方法计算成本过高；3D高斯飞溅技术在镜面反射方面存在局限性；多视图不一致性引入高频表面噪声和结构伪影；简化的渲染方程掩盖材料属性，导致不合理的重照明结果。&lt;h4&gt;目的&lt;/h4&gt;解决逆渲染中的各种挑战，提出一种新的框架GOGS，基于2D高斯表面元，实现更高效、更准确的几何重建、材料分离和重照明。&lt;h4&gt;方法&lt;/h4&gt;提出GOGS，一个基于2D高斯表面元的新型两阶段框架：第一阶段通过基于物理的渲染和分割和近似建立稳健的表面重建，并利用基础模型的几何先验进行增强；第二阶段通过利用完整渲染方程的蒙特卡罗重要性采样进行材料分解，通过可微分的2D高斯光线追踪建模间接照明，并通过基于球面mipmap的方向编码捕获各向异性高光来细化高频镜面细节。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验证明，GOGS在几何重建、材料分离和新照明下的照片级真实感重照明方面实现了最先进的性能，优于现有的逆渲染方法。&lt;h4&gt;结论&lt;/h4&gt;GOGS框架成功解决了逆渲染中的关键挑战，特别是在处理高光物体时，在几何重建、材料分离和重照明方面表现出色，为逆渲染领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从RGB图像进行高光物体的逆渲染仍然受到固有模糊性的根本限制。尽管基于NeRF的方法通过密集光线采样实现了高保真重建，但其计算成本过高。最近的3D高斯飞溅技术实现了高效的重建，但在镜面反射方面存在局限性。多视图不一致性引入高频表面噪声和结构伪影，而简化的渲染方程掩盖了材料属性，导致不合理的重照明结果。为解决这些问题，我们提出了GOGS，一种基于2D高斯表面元的新型两阶段框架。首先，我们通过基于物理的渲染和分割和近似建立了稳健的表面重建，并通过基础模型的几何先验进行增强。其次，我们通过利用完整渲染方程的蒙特卡罗重要性采样进行材料分解，通过可微分的2D高斯光线追踪建模间接照明，并通过基于球面mipmap的方向编码捕获各向异性高光来细化高频镜面细节。广泛的实验证明了在几何重建、材料分离和新照明下的照片级真实感重照明方面，优于现有逆渲染方法的最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高光泽物体（如金属、陶瓷等）的逆渲染问题，即从多视角RGB图像中准确重建物体的几何形状和材料属性。这个问题在现实中很重要，因为高光泽物体在日常生活中很常见，准确重建它们对于产品展示、虚拟现实、数字孪生等应用至关重要。现有方法要么计算成本过高（如NeRF-based方法），要么在高光区域表现不佳（如3D高斯飞溅方法），导致重建结果出现噪声或伪影。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是高光物体重建中的多视图一致性问题。他们提出两阶段框架：先解决几何重建，再解决材料分解。几何重建阶段利用基础模型的几何先验减轻高光干扰；材料分解阶段采用完整渲染方程评估。该方法借鉴了多项现有工作：2D高斯飞溅用于表面表示，基础模型（如Marigold）提供几何先验，蒙特卡罗重要性采样减少计算方差，球体mipmap编码处理高光细节，以及IRGS的可微分2D高斯光线追踪技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用2D高斯飞溅作为基础表示，通过两阶段框架分别解决几何重建和材料分解问题，并引入基础模型几何先验和高光补偿机制。整体流程分为两个阶段：第一阶段使用基于物理的渲染与分裂和近似，结合基础模型的几何先验监督进行鲁棒的几何重建；第二阶段在固定几何基础上，通过蒙特卡罗重要性采样评估完整渲染方程，利用可微分2D高斯光线追踪计算可视性和间接辐射，并通过球体mipmap基础的高光补偿机制细化高频细节。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三方面：1）鲁棒的几何重建方法，利用几何先验和分裂和近似减轻高光干扰；2）基于物理的材料分解，通过完整渲染方程评估和蒙特卡罗重要性采样实现；3）自适应高光补偿机制，基于球体mipmap的方向编码细化高频细节。相比之前工作，该方法避免了NeRF的高计算成本，解决了3D高斯飞溅在高光区域的噪声问题，使用完整渲染方程而非简化版本提高了材料分解准确性，并引入基础模型先验解决了多视图一致性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GOGS通过结合2D高斯飞溅表示、基础模型几何先验监督和基于物理的完整渲染方程评估，实现了高光泽物体的高保真几何重建和精确材料分解，显著提升了新照明条件下的渲染质量和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inverse rendering of glossy objects from RGB imagery remains fundamentallylimited by inherent ambiguity. Although NeRF-based methods achievehigh-fidelity reconstruction via dense-ray sampling, their computational costis prohibitive. Recent 3D Gaussian Splatting achieves high reconstructionefficiency but exhibits limitations under specular reflections. Multi-viewinconsistencies introduce high-frequency surface noise and structuralartifacts, while simplified rendering equations obscure material properties,leading to implausible relighting results. To address these issues, we proposeGOGS, a novel two-stage framework based on 2D Gaussian surfels. First, weestablish robust surface reconstruction through physics-based rendering withsplit-sum approximation, enhanced by geometric priors from foundation models.Second, we perform material decomposition by leveraging Monte Carlo importancesampling of the full rendering equation, modeling indirect illumination viadifferentiable 2D Gaussian ray tracing and refining high-frequency speculardetails through spherical mipmap-based directional encoding that capturesanisotropic highlights. Extensive experiments demonstrate state-of-the-artperformance in geometry reconstruction, material separation, and photorealisticrelighting under novel illuminations, outperforming existing inverse renderingapproaches.</description>
      <author>example@mail.com (Xingyuan Yang, Min Wei)</author>
      <guid isPermaLink="false">2508.14563v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Locality-aware Concept Bottleneck Model</title>
      <link>http://arxiv.org/abs/2508.14562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 25 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了局部感知概念瓶颈模型(LCBM)框架，利用基础模型信息和原型学习确保概念在图像中的准确空间定位，有效识别图像中存在的概念并改进定位能力，同时保持相当好的分类性能。&lt;h4&gt;背景&lt;/h4&gt;概念瓶颈模型(CBMs)本质上是可解释的模型，基于人类可理解的视觉线索(概念)进行预测。通过人工标注获取密集概念标注是困难和昂贵的，最近的方法利用基础模型确定图像中的概念，但无标签CBM通常无法在相关区域定位概念，预测时会关注视觉上不相关的区域。&lt;h4&gt;目的&lt;/h4&gt;解决无标签CBM无法在相关区域定位概念的问题，确保概念在图像中的准确空间定位。&lt;h4&gt;方法&lt;/h4&gt;提出局部感知概念瓶颈模型(LCBM)框架，为每个概念分配一个原型来代表该概念的典型图像特征，通过鼓励原型编码相似的局部区域来学习这些原型，利用基础模型确保原型与概念的相关性，并使用原型促进识别每个概念应从哪个适当局部区域进行预测的学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;LCBM能有效识别图像中存在的概念，改进了概念的定位能力，同时保持了相当好的分类性能。&lt;h4&gt;结论&lt;/h4&gt;LCBM框架有效解决了无标签CBM在概念定位上的问题，同时保持了良好的分类性能，提高了模型的可解释性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;概念瓶颈模型(CBMs)本质上是可解释的模型，它们基于人类可理解的视觉线索(称为概念)进行预测。由于通过人工标注获取密集的概念标注是困难和昂贵的，最近的方法利用基础模型来确定图像中存在的概念。然而，这种无标签的CBM通常无法在相关区域定位概念，在预测概念存在时会关注视觉上不相关的区域。为此，我们提出了一个框架，称为局部感知概念瓶颈模型(LCBM)，该框架利用基础模型的丰富信息并采用原型学习来确保概念的准确空间定位。具体来说，我们为每个概念分配一个原型，该原型被推广为代表该概念的典型图像特征。这些原型是通过鼓励它们编码相似的局部区域来学习的，利用基础模型确保每个原型与其关联概念的相关性。然后，我们使用原型来促进识别每个概念应从哪个适当的局部区域进行预测的学习过程。实验结果表明，LCBM能有效识别图像中存在的概念，并改进了定位能力，同时保持了相当好的分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept bottleneck models (CBMs) are inherently interpretable models thatmake predictions based on human-understandable visual cues, referred to asconcepts. As obtaining dense concept annotations with human labeling isdemanding and costly, recent approaches utilize foundation models to determinethe concepts existing in the images. However, such label-free CBMs often failto localize concepts in relevant regions, attending to visually unrelatedregions when predicting concept presence. To this end, we propose a framework,coined Locality-aware Concept Bottleneck Model (LCBM), which utilizes richinformation from foundation models and adopts prototype learning to ensureaccurate spatial localization of the concepts. Specifically, we assign oneprototype to each concept, promoted to represent a prototypical image featureof that concept. These prototypes are learned by encouraging them to encodesimilar local regions, leveraging foundation models to assure the relevance ofeach prototype to its associated concept. Then we use the prototypes tofacilitate the learning process of identifying the proper local region fromwhich each concept should be predicted. Experimental results demonstrate thatLCBM effectively identifies present concepts in the images and exhibitsimproved localization while maintaining comparable classification performance.</description>
      <author>example@mail.com (Sujin Jeon, Hyundo Lee, Eungseo Kim, Sanghack Lee, Byoung-Tak Zhang, Inwoo Hwang)</author>
      <guid isPermaLink="false">2508.14562v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>DeepTelecom: A Digital-Twin Deep Learning Dataset for Channel and MIMO Applications</title>
      <link>http://arxiv.org/abs/2508.14507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepTelecom是一个三维数字孪生信道数据集，解决了现有无线AI数据集生成缓慢、保真度有限和场景类型狭窄的问题。&lt;h4&gt;背景&lt;/h4&gt;现有无线AI数据集生成缓慢，建模保真度有限，仅覆盖有限场景类型。&lt;h4&gt;目的&lt;/h4&gt;创建一个三维数字孪生信道数据集DeepTelecom，解决现有数据集的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用大语言模型辅助流程构建LoD3场景，基于Sionna的射线追踪引擎模拟无线电波传播，利用GPU加速流式传输射线路径轨迹和信号强度热图，编译成高帧率视频并输出多视图图像、信道张量和多尺度衰落轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;DeepTelecom能够高效流式传输大规模、高保真度和多模态信道数据。&lt;h4&gt;结论&lt;/h4&gt;DeepTelecom为无线AI研究提供统一基准，并支持基础模型将大模型智能与未来通信系统紧密融合。&lt;h4&gt;翻译&lt;/h4&gt;特定领域数据集是释放人工智能驱动的无线创新的基础。然而，现有的无线AI语料库生成缓慢，建模保真度有限，仅覆盖狭窄的场景类型。为解决这些挑战，我们创建了DeepTelecom，一个三维数字孪生信道数据集。具体而言，大语言模型辅助的流程首先构建具有可分割参数化表面的第三级细节的室外和室内场景。然后，DeepTelecom基于Sionna的射线追踪引擎模拟完整的无线电波传播效果。利用GPU加速，DeepTelecom流式传输射线路径轨迹和实时信号强度热图，将它们编译成高帧率视频，同时输出同步的多视图图像、信道张量和多尺度衰落轨迹。通过高效流式传输大规模、高保真度和多模态信道数据，DeepTelecom不仅为无线AI研究提供了统一的基准，还提供了领域丰富的训练基础，使基础模型能够将大模型智能与未来通信系统紧密融合。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有无线AI数据集生成缓慢、保真度有限和场景类型覆盖不足的问题。这个问题在6G通信发展中至关重要，因为6G将成为智能社会的核心基础设施，需要超高数据速率、超低延迟和极高可靠性，而AI与无线系统的融合依赖于高质量的大规模训练数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有无线AI数据集的局限性：依赖CPU限制的射线追踪器导致生成慢、模态有限且需要繁琐校准、数字孪生模型局限于LoD1级别。他们考察了现有的开放或商业工具链如DeepMIMO、ViWi和NVIDIA Sionna，发现这些工具虽有GPU加速但未将材料感知重建与可微分射线追踪集成。基于此，作者创新性地结合了LLM辅助场景建模和Sionna的GPU加速射线追踪技术，设计了一个更完整高效的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个高保真度、多模态、LoD3级别的GPU加速射线追踪数据集，通过LLM辅助的管线构建详细的室内外环境数字孪生，并使用Sionna引擎模拟无线电波传播。整体流程包括四个主要步骤：1) LLM辅助的高保真多源场景建模，整合多种数据源并标注材料属性；2) 参数定义和配置，包括系统参数和生成参数；3) GPU加速射线追踪模拟，高效生成信道数据；4) 信道数据提取和后处理，计算标准信道表示并过滤无效数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) LLM辅助的高保真场景建模，优化材料标注一致性；2) 高效的GPU加速射线追踪，保持自动梯度能力并实现早期射线终止；3) 多模态数据输出，同时生成视频、图像、信道张量和衰落轨迹；4) 材料感知的LoD3级重建，每个表面都有明确的电磁材料属性；5) 系统性和灵活性，可配置多种参数并扩展到各种场景。相比之前工作，DeepTelecom将大语言模型、高保真3D建模、GPU加速射线追踪和多模态输出集成到统一工作流中，创建了更全面高效的无线AI数据集生成平台。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DeepTelecom通过结合大语言模型辅助的高保真场景建模和GPU加速的射线追踪技术，创建了一个前所未有的多模态、大规模、高保真度的无线信道数据集，为下一代智能无线通信研究提供了基础性资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain-specific datasets are the foundation for unleashing artificialintelligence (AI)-driven wireless innovation. Yet existing wireless AI corporaare slow to produce, offer limited modeling fidelity, and cover only narrowscenario types. To address the challenges, we create DeepTelecom, athree-dimension (3D) digital-twin channel dataset. Specifically, a largelanguage model (LLM)-assisted pipeline first builds the third level of details(LoD3) outdoor and indoor scenes with segmentable material-parameterizablesurfaces. Then, DeepTelecom simulates full radio-wave propagation effects basedon Sionna's ray-tracing engine. Leveraging GPU acceleration, DeepTelecomstreams ray-path trajectories and real-time signal-strength heat maps, compilesthem into high-frame-rate videos, and simultaneously outputs synchronizedmulti-view images, channel tensors, and multi-scale fading traces. Byefficiently streaming large-scale, high-fidelity, and multimodal channel data,DeepTelecom not only furnishes a unified benchmark for wireless AI research butalso supplies the domain-rich training substrate that enables foundation modelsto tightly fuse large model intelligence with future communication systems.</description>
      <author>example@mail.com (Bohao Wang, Zehua Jiang, Zhenyu Yang, Chongwen Huang, Yongliang Shen, Siming Jiang, Chen Zhu, Zhaohui Yang, Richeng Jin, Zhaoyang Zhang, Sami Muhaidat, Merouane Debbah)</author>
      <guid isPermaLink="false">2508.14507v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments</title>
      <link>http://arxiv.org/abs/2508.14504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PB-IAD（基于提示的工业异常检测）的新型框架，利用基础模型的多模态和推理能力解决工业异常检测问题，特别关注数据稀疏、敏捷适应性和领域用户中心化三大需求。&lt;h4&gt;背景&lt;/h4&gt;制造业中异常检测对确保产品质量和识别工艺偏差至关重要。传统统计和数据驱动方法虽为标准，但受限于对大量标注数据集的依赖以及在动态生产条件下的有限适应性。&lt;h4&gt;目的&lt;/h4&gt;利用基础模型的感知能力，将其适应到工业异常检测这一下游任务，解决工业异常检测中面临的数据稀疏、敏捷适应性和领域用户中心化等关键需求。&lt;h4&gt;方法&lt;/h4&gt;提出PB-IAD框架，包括专门用于迭代实施领域特定工艺知识的提示模板，以及将领域用户输入转换为有效系统提示的预处理模块。使用GPT-4.1在三种不同制造场景和两种数据模态下进行评估，并通过消融研究评估语义指令贡献。&lt;h4&gt;主要发现&lt;/h4&gt;PB-IAD在性能上优于最先进的方法（如PatchCore），特别是在数据稀疏场景和少样本设置中，仅通过语义指令就实现了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;PB-IAD框架通过用户中心化设计，允许领域专家无需数据科学专业知识就能灵活定制系统，为工业异常检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;制造业中异常检测的发现对确保产品质量和识别工艺偏差至关重要。统计和数据驱动方法仍然是工业异常检测的标准，但其适应性和可用性受限于对大量标注数据集的依赖以及在动态生产条件下的有限灵活性。基础模型感知能力的最新进展为其适应这一下游任务提供了有希望的机会。本文提出了PB-IAD（基于提示的工业异常检测），一种利用基础模型多模态和推理能力进行工业异常检测的新框架。具体而言，PB-IAD解决了动态生产环境的三个关键需求：数据稀疏、敏捷适应性和领域用户中心化。除异常检测外，该框架还包括一个专门为迭代实施领域特定工艺知识而设计的提示模板，以及一个将领域用户输入转换为有效系统提示的预处理模块。这种以用户为中心的设计允许领域专家灵活定制系统而无需数据科学专业知识。所提出的框架利用GPT-4.1在三种不同的制造场景、两种数据模态下进行了评估，并通过消融研究系统评估了语义指令的贡献。此外，PB-IAD与PatchCore等最先进的异常检测方法进行了基准测试。结果表明，特别是在数据稀疏场景和少样本设置中，仅通过语义指令就实现了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of anomalies in manufacturing processes is crucial to ensureproduct quality and identify process deviations. Statistical and data-drivenapproaches remain the standard in industrial anomaly detection, yet theiradaptability and usability are constrained by the dependence on extensiveannotated datasets and limited flexibility under dynamic production conditions.Recent advances in the perception capabilities of foundation models providepromising opportunities for their adaptation to this downstream task. Thispaper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novelframework that leverages the multimodal and reasoning capabilities offoundation models for industrial anomaly detection. Specifically, PB-IADaddresses three key requirements of dynamic production environments: datasparsity, agile adaptability, and domain user centricity. In addition to theanomaly detection, the framework includes a prompt template that isspecifically designed for iteratively implementing domain-specific processknowledge, as well as a pre-processing module that translates domain userinputs into effective system prompts. This user-centric design allows domainexperts to customise the system flexibly without requiring data scienceexpertise. The proposed framework is evaluated by utilizing GPT-4.1 acrossthree distinct manufacturing scenarios, two data modalities, and an ablationstudy to systematically assess the contribution of semantic instructions.Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomalydetection such as PatchCore. The results demonstrate superior performance,particularly in data-sparse scenarios and low-shot settings, achieved solelythrough semantic instructions.</description>
      <author>example@mail.com (Bernd Hofmann, Albert Scheck, Joerg Franke, Patrick Bruendl)</author>
      <guid isPermaLink="false">2508.14504v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration</title>
      <link>http://arxiv.org/abs/2508.14483v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Vivid-VR，一种基于DiT的生成式视频修复方法，利用ControlNet控制生成过程确保内容一致性。&lt;h4&gt;背景&lt;/h4&gt;传统可控管道微调常因多模态对齐不完美而遭受分布漂移，导致纹理真实性和时间一致性受损。&lt;h4&gt;目的&lt;/h4&gt;解决分布漂移问题，保持纹理和时间质量，同时增强生成可控性。&lt;h4&gt;方法&lt;/h4&gt;提出概念蒸馏训练策略，利用预训练T2V模型合成带有文本概念的训练样本；重新设计控制架构，包含控制特征投影器和双分支设计的ControlNet连接器。&lt;h4&gt;主要发现&lt;/h4&gt;Vivid-VR在合成和真实世界基准以及AIGC视频上优于现有方法，实现了优异的纹理真实性、视觉生动性和时间一致性。&lt;h4&gt;结论&lt;/h4&gt;通过概念蒸馏训练策略和重新设计的控制架构，Vivid-VR成功解决了传统可控管道微调中的分布漂移问题，提高了视频修复质量和可控性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Vivid-VR，一种基于DiT的生成式视频修复方法，它建立在先进的T2V基础模型之上，其中利用ControlNet控制生成过程，确保内容一致性。然而，这种可控管道的传统微调经常因多模态对齐不完美而遭受分布漂移，导致纹理真实性和时间一致性受损。为了解决这一挑战，我们提出了概念蒸馏训练策略，利用预训练T2V模型合成带有嵌入文本概念的训练样本，从而蒸馏其概念理解以保持纹理和时间质量。为了增强生成可控性，我们重新设计了包含两个关键组件的控制架构：1) 控制特征投影器，用于从输入视频潜在特征中过滤退化伪影，减少其在生成管道中的传播；2) 新的ControlNet连接器，采用双分支设计，该连接器协同结合基于MLP的特征映射和交叉注意力机制进行动态控制特征检索，实现内容保持和自适应控制信号调制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Vivid-VR, a DiT-based generative video restoration method builtupon an advanced T2V foundation model, where ControlNet is leveraged to controlthe generation process, ensuring content consistency. However, conventionalfine-tuning of such controllable pipelines frequently suffers from distributiondrift due to limitations in imperfect multimodal alignment, resulting incompromised texture realism and temporal coherence. To tackle this challenge,we propose a concept distillation training strategy that utilizes thepretrained T2V model to synthesize training samples with embedded textualconcepts, thereby distilling its conceptual understanding to preserve textureand temporal quality. To enhance generation controllability, we redesign thecontrol architecture with two key components: 1) a control feature projectorthat filters degradation artifacts from input video latents to minimize theirpropagation through the generation pipeline, and 2) a new ControlNet connectoremploying a dual-branch design. This connector synergistically combinesMLP-based feature mapping with cross-attention mechanism for dynamic controlfeature retrieval, enabling both content preservation and adaptive controlsignal modulation. Extensive experiments show that Vivid-VR performs favorablyagainst existing approaches on both synthetic and real-world benchmarks, aswell as AIGC videos, achieving impressive texture realism, visual vividness,and temporal consistency. The codes and checkpoints are publicly available athttps://github.com/csbhr/Vivid-VR.</description>
      <author>example@mail.com (Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying Chen)</author>
      <guid isPermaLink="false">2508.14483v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Pixels to Play: A Foundation Model for 3D Gameplay</title>
      <link>http://arxiv.org/abs/2508.14295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一个名为Pixels2Play-0.1（P2P0.1）的基础模型，该模型能够学习玩各种3D视频游戏，表现出类似人类的行为。&lt;h4&gt;背景&lt;/h4&gt;消费者和开发者有新兴的用例，如AI队友、可控制的NPC、个性化直播流和辅助测试工具，这促使研究人员开发一个能够像人类一样仅依靠像素流来玩游戏并能够泛化到新游戏中的智能体。&lt;h4&gt;目的&lt;/h4&gt;创建一个能够仅使用玩家可用的像素流来玩游戏的基础模型，并且能够最小化游戏特定的工程化工作，从而泛化到新游戏中。&lt;h4&gt;方法&lt;/h4&gt;使用行为克隆进行端到端训练，结合来自人类游戏游戏的标记演示数据和未标记的公共视频（通过逆动力学模型推断动作），采用仅解码器的transformer模型处理大的动作空间，并在单个消费级GPU上保持低延迟。&lt;h4&gt;主要发现&lt;/h4&gt;在简单的Roblox和经典的MS-DOS游戏中展示了熟练的游戏能力，进行了未标记数据的消融研究，并概述了达到专家级文本控制所需的扩展和评估步骤。&lt;h4&gt;结论&lt;/h4&gt;Pixels2Play-0.1模型展示了在多种游戏中表现出类似人类行为的潜力，但仍需要进一步扩展和评估才能达到专家水平。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了Pixels2Play-0.1（P2P0.1），一个基础模型，它学习玩各种3D视频游戏，表现出可识别的类人行为。受新兴的消费者和开发者用例启发——AI队友、可控制的NPC、个性化直播流、辅助测试工具——我们认为智能体必须依赖于玩家可用的相同像素流，并以最小的游戏特定工程化泛化到新游戏中。P2P0.1通过行为克隆进行端到端训练：从人类游戏游戏中收集的标记演示数据与未标记的公共视频相结合，我们通过逆动力学模型推断动作。一个仅解码器的transformer模型，具有自回归动作输出，能够处理大的动作空间，同时在单个消费级GPU上保持低延迟。我们报告了定性结果，展示了在简单的Roblox和经典MS-DOS游戏中的熟练游戏能力，对未标记数据的消融研究，并概述了达到专家级文本控制所需的扩展和评估步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to playa wide range of 3D video games with recognizable human-like behavior. Motivatedby emerging consumer and developer use cases - AI teammates, controllable NPCs,personalized live-streamers, assistive testers - we argue that an agent mustrely on the same pixel stream available to players and generalize to new titleswith minimal game-specific engineering. P2P0.1 is trained end-to-end withbehavior cloning: labeled demonstrations collected from instrumented humangame-play are complemented by unlabeled public videos, to which we imputeactions via an inverse-dynamics model. A decoder-only transformer withauto-regressive action output handles the large action space while remaininglatency-friendly on a single consumer GPU. We report qualitative resultsshowing competent play across simple Roblox and classic MS-DOS titles,ablations on unlabeled data, and outline the scaling and evaluation stepsrequired to reach expert-level, text-conditioned control.</description>
      <author>example@mail.com (Yuguang Yue, Chris Green, Samuel Hunt, Irakli Salia, Wenzhe Shi, Jonathan J Hunt)</author>
      <guid isPermaLink="false">2508.14295v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>RynnEC: Bringing MLLMs into Embodied World</title>
      <link>http://arxiv.org/abs/2508.14160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The technical report of RynnEC, an embodied cognition MLLM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队提出了RynnEC，一个专为具身认知设计的视频多模态大语言模型。该模型通过区域编码器和掩码解码器实现灵活的区域级别视频交互，在物体属性理解、物体分割和空间推理方面取得了最先进性能。研究还提出了生成具身认知数据的流水线和评估基准RynnEC-Bench。&lt;h4&gt;背景&lt;/h4&gt;具身智能体需要细粒度的物理世界感知和精确交互能力，但现有模型在区域级别视频交互方面存在局限，且标注3D数据集稀缺。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够提供细粒度物理世界感知并实现精确交互的视频多模态大语言模型，推动具身智能体通用认知核心的发展。&lt;h4&gt;方法&lt;/h4&gt;基于通用视觉语言基础模型构建RynnEC，整合区域编码器和掩码解码器；提出基于自我中心视频的流水线生成具身认知数据；创建RynnEC-Bench评估基准。&lt;h4&gt;主要发现&lt;/h4&gt;尽管架构紧凑，RynnEC在物体属性理解、物体分割和空间推理方面实现了最先进的性能，能够提供对物理世界的细粒度感知并支持更精确的交互。&lt;h4&gt;结论&lt;/h4&gt;RynnEC为具身智能体提供了一个以区域为中心的视频范式，有望促进跨多样化具身任务的泛化能力，推动具身智能体通用认知核心的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了RynnEC，这是一个专为具身认知设计的视频多模态大语言模型。基于通用视觉语言基础模型构建，RynnEC集成了区域编码器和掩码解码器，能够实现灵活的区域级别视频交互。尽管架构紧凑，RynnEC在物体属性理解、物体分割和空间推理方面取得了最先进的性能。从概念上讲，它为具身智能体的大脑提供了以区域为中心的视频范式，能够提供对物理世界的细粒度感知，并实现更精确的交互。为了缓解标注3D数据集的稀缺问题，我们提出了一个基于自我中心视频的流水线来生成具身认知数据。此外，我们引入了RynnEC-Bench，这是一个以区域为中心的基准，用于评估具身认知能力。我们预计RynnEC将推动具身智能体通用认知核心的发展，并促进跨多样化具身任务的泛化能力。代码、模型检查点和基准可在以下网址获取：https://github.com/alibaba-damo-academy/RynnEC&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce RynnEC, a video multimodal large language model designed forembodied cognition. Built upon a general-purpose vision-language foundationmodel, RynnEC incorporates a region encoder and a mask decoder, enablingflexible region-level video interaction. Despite its compact architecture,RynnEC achieves state-of-the-art performance in object property understanding,object segmentation, and spatial reasoning. Conceptually, it offers aregion-centric video paradigm for the brain of embodied agents, providingfine-grained perception of the physical world and enabling more preciseinteractions. To mitigate the scarcity of annotated 3D datasets, we propose anegocentric video based pipeline for generating embodied cognition data.Furthermore, we introduce RynnEC-Bench, a region-centered benchmark forevaluating embodied cognitive capabilities. We anticipate that RynnEC willadvance the development of general-purpose cognitive cores for embodied agentsand facilitate generalization across diverse embodied tasks. The code, modelcheckpoints, and benchmark are available at:https://github.com/alibaba-damo-academy/RynnEC</description>
      <author>example@mail.com (Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao)</author>
      <guid isPermaLink="false">2508.14160v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Surya: Foundation Model for Heliophysics</title>
      <link>http://arxiv.org/abs/2508.14112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Surya是日球物理学领域首个使用全分辨率SDO数据进行时间推进作为预训练任务的基础模型，它能够学习太阳演化的潜在物理机制，并在多种太阳现象预测任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;日球物理学对于理解和预测空间天气事件和太阳活动至关重要。尽管有来自太阳动力学天文台(SDO)的高分辨率观测数据数十年，但大多数模型仍然是特定任务的，受限于稀少的标记数据，限制了它们对各种太阳现象的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;介绍Surya，一个用于日球物理学的基础模型，旨在从多仪器SDO观测中学习通用的太阳表示。&lt;h4&gt;方法&lt;/h4&gt;Surya是一个拥有3.66亿参数的基础模型，使用时空变换器架构，具有频谱门控和长-短范围注意力机制。它在高分辨率太阳图像预测任务上进行预训练，并通过自回归展开调优进一步优化。&lt;h4&gt;主要发现&lt;/h4&gt;零样本评估展示了Surya预测太阳动力学和耀斑事件的能力。使用参数高效的低秩适应(LoRA)进行下游微调后，在太阳风预测、活跃区域分割、太阳耀斑预测和EUV光谱方面表现出强大性能。&lt;h4&gt;结论&lt;/h4&gt;Surya的新架构和性能表明该模型能够学习太阳演化的潜在物理机制。&lt;h4&gt;翻译&lt;/h4&gt;日球物理学对于理解和预测空间天气事件和太阳活动至关重要。尽管有来自太阳动力学天文台(SDO)的高分辨率观测数据数十年，但大多数模型仍然是特定任务的，受限于稀少的标记数据，限制了它们对各种太阳现象的泛化能力。我们介绍了Surya，一个拥有3.66亿参数的日球物理学基础模型，旨在从多仪器SDO观测中学习通用的太阳表示，包括八个大气成像组件(AIA)通道和五个日震和磁力成像仪(HMI)产品。Surya采用具有频谱门控和长-短范围注意力的时空变换器架构，在高分辨率太阳图像预测任务上进行预训练，并通过自回归展开调优进一步优化。零样本评估展示了其预测太阳动力学和耀斑事件的能力，而使用参数高效的低秩适应(LoRA)进行下游微调则在太阳风预测、活跃区域分割、太阳耀斑预测和EUV光谱方面表现出强大性能。Surya是日球物理学中使用全分辨率SDO数据进行时间推进作为预训练任务的首个基础模型。其新颖的架构和性能表明该模型能够学习太阳演化的潜在物理机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heliophysics is central to understanding and forecasting space weather eventsand solar activity. Despite decades of high-resolution observations from theSolar Dynamics Observatory (SDO), most models remain task-specific andconstrained by scarce labeled data, limiting their capacity to generalizeacross solar phenomena. We introduce Surya, a 366M parameter foundation modelfor heliophysics designed to learn general-purpose solar representations frommulti-instrument SDO observations, including eight Atmospheric Imaging Assembly(AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Suryaemploys a spatiotemporal transformer architecture with spectral gating andlong--short range attention, pretrained on high-resolution solar imageforecasting tasks and further optimized through autoregressive rollout tuning.Zero-shot evaluations demonstrate its ability to forecast solar dynamics andflare events, while downstream fine-tuning with parameter-efficient Low-RankAdaptation (LoRA) shows strong performance on solar wind forecasting, activeregion segmentation, solar flare forecasting, and EUV spectra. Surya is thefirst foundation model in heliophysics that uses time advancement as a pretexttask on full-resolution SDO data. Its novel architecture and performancesuggest that the model is able to learn the underlying physics behind solarevolution.</description>
      <author>example@mail.com (Sujit Roy, Johannes Schmude, Rohit Lal, Vishal Gaur, Marcus Freitag, Julian Kuehnert, Theodore van Kessel, Dinesha V. Hegde, Andrés Muñoz-Jaramillo, Johannes Jakubik, Etienne Vos, Kshitiz Mandal, Ata Akbari Asanjan, Joao Lucas de Sousa Almeida, Amy Lin, Talwinder Singh, Kang Yang, Chetraj Pandey, Jinsu Hong, Berkay Aydin, Thorsten Kurth, Ryan McGranaghan, Spiridon Kasapis, Vishal Upendran, Shah Bahauddin, Daniel da Silva, Nikolai V. Pogorelov, Campbell Watson, Manil Maskey, Madhulika Guhathakurta, Juan Bernabe-Moreno, Rahul Ramachandran)</author>
      <guid isPermaLink="false">2508.14112v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>LeanGeo: Formalizing Competitional Geometry problems in Lean</title>
      <link>http://arxiv.org/abs/2508.14644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LeanGeo是一个在Lean 4定理证明器中形式化和解决竞赛级几何问题的统一形式系统，提供了高级几何定理的全面库，支持严格的证明验证并与Mathlib无缝集成。研究团队还创建了LeanGeo-Bench基准，评估了当前大型语言模型的能力和局限性，并已开源项目。&lt;h4&gt;背景&lt;/h4&gt;几何问题是测试AI推理能力的重要领域，但现有几何求解系统无法在统一框架内表达问题，难以与其他数学领域集成。此外，由于几何证明依赖于直观图形，验证几何问题尤其具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;引入LeanGeo，一个统一的形式系统，用于在Lean 4定理证明器中形式化和解决竞赛级几何问题，解决现有系统的局限性。&lt;h4&gt;方法&lt;/h4&gt;开发LeanGeo系统，包含高级几何定理的全面库，结合Lean的基础逻辑支持严格证明验证，并与Mathlib无缝集成；创建LeanGeo-Bench基准，包含国际数学奥林匹克(IMO)和其他高级来源的问题。&lt;h4&gt;主要发现&lt;/h4&gt;评估展示了最先进的大型语言模型在LeanGeo-Bench基准上的能力和局限性，强调了在自动化几何推理方面需要进一步发展。&lt;h4&gt;结论&lt;/h4&gt;LeanGeo提供了形式化几何问题的新方法，通过开源定理库和基准，促进了自动化几何推理领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;几何问题是测试AI推理能力的重要试验场。大多数现有的几何求解系统无法在统一框架内表达问题，因此难以与其他数学领域集成。此外，由于大多数几何证明依赖于直观图形，验证几何问题尤其具有挑战性。为解决这些差距，我们引入了LeanGeo，一个在Lean 4定理证明器中形式化和解决竞赛级几何问题的统一形式系统。LeanGeo具有高级几何定理的全面库，结合Lean的基础逻辑，支持严格的证明验证，并与Mathlib无缝集成。我们还展示了LeanGeo-Bench，这是LeanGeo中的一个形式化几何基准，包含国际数学奥林匹克(IMO)和其他高级来源的问题。我们的评估展示了最先进的大型语言模型在该基准上的能力和局限性，突显了在自动化几何推理方面需要进一步发展。我们在https://github.com/project-numina/LeanGeo/tree/master开源了LeanGeo的定理库和基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometry problems are a crucial testbed for AI reasoning capabilities. Mostexisting geometry solving systems cannot express problems within a unifiedframework, thus are difficult to integrate with other mathematical fields.Besides, since most geometric proofs rely on intuitive diagrams, verifyinggeometry problems is particularly challenging. To address these gaps, weintroduce LeanGeo, a unified formal system for formalizing and solvingcompetition-level geometry problems within the Lean 4 theorem prover. LeanGeofeatures a comprehensive library of high-level geometric theorems with Lean'sfoundational logic, enabling rigorous proof verification and seamlessintegration with Mathlib. We also present LeanGeo-Bench, a formal geometrybenchmark in LeanGeo, comprising problems from the International MathematicalOlympiad (IMO) and other advanced sources. Our evaluation demonstrates thecapabilities and limitations of state-of-the-art Large Language Models on thisbenchmark, highlighting the need for further advancements in automatedgeometric reasoning. We open source the theorem library and the benchmark ofLeanGeo at https://github.com/project-numina/LeanGeo/tree/master.</description>
      <author>example@mail.com (Chendong Song, Zihan Wang, Frederick Pu, Haiming Wang, Xiaohan Lin, Junqi Liu, Jia Li, Zhengying Liu)</author>
      <guid isPermaLink="false">2508.14644v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation</title>
      <link>http://arxiv.org/abs/2508.14327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的多模态多视角视频生成方法，通过统一的扩散变换器模型实现自动驾驶场景的高质量多模态数据生成。&lt;h4&gt;背景&lt;/h4&gt;视频生成在自动驾驶城市场景合成中显示出优势，但现有方法主要关注RGB视频生成，缺乏多模态能力。多模态数据（如深度图和语义图）对自动驾驶的整体城市场景理解至关重要，而使用多个模型生成不同模态会增加部署难度且无法利用互补线索。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法支持多模态视频生成的问题，提出一种新颖的多模态多视角视频生成方法用于自动驾驶场景合成。&lt;h4&gt;方法&lt;/h4&gt;构建一个统一的扩散变换器模型，包含模态共享组件和模态特定组件，利用多样化的条件输入将可控的场景结构和内容线索编码到统一的扩散模型中，实现多模态多视角视频生成。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes真实世界自动驾驶数据集上的实验表明，该方法能够高保真度和可控制性地生成多模态多视角城市场景视频，性能超越了最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的统一框架能够生成多模态多视角驾驶场景视频，为自动驾驶提供了更全面的城市场景理解能力。&lt;h4&gt;翻译&lt;/h4&gt;视频生成最近在自动驾驶城市场景合成中显示出优越性。现有的自动驾驶视频生成方法主要关注RGB视频生成，缺乏支持多模态视频生成的能力。然而，多模态数据，如深度图和语义图，对自动驾驶的整体城市场景理解至关重要。虽然可以使用多个模型生成不同模态，但这增加了模型部署的难度，且没有利用多模态数据生成的互补线索。为解决这一问题，本研究提出了一种新颖的用于自动驾驶的多模态多视角视频生成方法。具体而言，我们构建了一个由模态共享组件和模态特定组件组成的统一扩散变换器模型。然后，我们利用多样化的条件输入将可控的场景结构和内容线索编码到统一的扩散模型中，用于多模态多视角视频生成。通过这种方式，我们的方法能够在统一框架中生成多模态多视角驾驶场景视频。我们在具有挑战性的真实世界自动驾驶数据集nuScenes上的实验表明，我们的方法能够高保真度和可控制性地生成多模态多视角城市场景视频，超越了最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域现有视频生成方法仅支持单模态(RGB)生成而缺乏多模态(深度图、语义图等)生成能力的问题。这个问题在现实中很重要，因为自动驾驶需要全面理解城市场景，多模态数据对实现安全高效的自动驾驶至关重要；同时，使用多个模型生成不同模态会增加部署难度且无法充分利用多模态间的互补信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到自动驾驶作为感知密集型任务本质需要多模态数据，然后发现现有方法要么只关注单模态RGB生成，要么使用多个独立模型增加部署难度。作者设计方法时借鉴了扩散模型(如SVD、CogVideoX)、多视角城市场景生成方法(如DriveDreamer、Panacea)以及多模态合成技术(如IDOL)。具体技术借鉴包括使用扩散变换器架构、3D VAE编码解码、多样化的条件输入设计，以及模态共享与特定学习的分解思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个统一的多模态多视角扩散变换器模型，包含模态共享组件和模态特定组件，利用多样化条件输入将可控场景结构和内容线索编码到模型中。整体流程：1)条件输入编码(文本、布局、参考条件)；2)多模态多视角扩散变换器处理(模态共享组件学习跨模态共同特征，模态特定组件保留各模态独特特征)；3)训练与推理(使用DDPM训练，DDIM采样和CFG引导)；4)VAE解码生成多视角场景视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个利用多样化条件输入和扩散变换器进行多模态多视角自动驾驶场景生成的工作；2)设计包含模态共享和模态特定组件的统一扩散变换器模型；3)多样化的条件输入设计(文本、参考、布局条件)；4)模态共享与特定学习的分解。相比之前工作的不同：支持多模态联合生成而非单模态；使用统一模型而非多个独立模型；不仅关注多视角一致性，还关注多模态一致性；专门针对自动驾驶场景优化而非通用视频生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种统一的多模态多视角扩散变换器模型，能够在单一框架内生成高质量、高可控性的多模态多视角驾驶场景视频，显著提升了自动驾驶场景理解和系统评估的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation has recently shown superiority in urban scene synthesis forautonomous driving. Existing video generation approaches to autonomous drivingprimarily focus on RGB video generation and lack the ability to supportmulti-modal video generation. However, multi-modal data, such as depth maps andsemantic maps, are crucial for holistic urban scene understanding in autonomousdriving. Although it is feasible to use multiple models to generate differentmodalities, this increases the difficulty of model deployment and does notleverage complementary cues for multi-modal data generation. To address thisproblem, in this work, we propose a novel multi-modal multi-view videogeneration approach to autonomous driving. Specifically, we construct a unifieddiffusion transformer model composed of modal-shared components andmodal-specific components. Then, we leverage diverse conditioning inputs toencode controllable scene structure and content cues into the unified diffusionmodel for multi-modal multi-view video generation. In this way, our approach iscapable of generating multi-modal multi-view driving scene videos in a unifiedframework. Our experiments on the challenging real-world autonomous drivingdataset, nuScenes, show that our approach can generate multi-modal multi-viewurban scene videos with high fidelity and controllability, surpassing thestate-of-the-art methods.</description>
      <author>example@mail.com (Guile Wu, David Huang, Dongfeng Bai, Bingbing Liu)</author>
      <guid isPermaLink="false">2508.14327v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services</title>
      <link>http://arxiv.org/abs/2508.14503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer架构的异常检测方法，集成多尺度特征感知，以解决云服务环境中时间建模和尺度感知特征表示的局限性。方法通过改进的Transformer模块、多尺度特征构建路径和注意力加权融合模块，在高维监控数据上进行时间建模，捕获长程依赖和上下文语义，并动态调整各尺度特征对最终决策的贡献。实验表明该方法在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持稳定性能。&lt;h4&gt;背景&lt;/h4&gt;云服务环境中存在时间建模和尺度感知特征表示的局限性，影响了异常检测的效果。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer架构的异常检测方法，集成多尺度特征感知，解决云服务环境中时间建模和尺度感知特征表示的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用改进的Transformer模块对高维监控数据进行时间建模，利用自注意力机制捕获长程依赖；引入多尺度特征构建路径通过下采样和并行编码提取不同粒度时序特征；设计注意力加权融合模块动态调整各尺度特征贡献；构建包含CPU利用率、内存使用和任务调度状态等核心信号的标准化多维时间序列；使用位置编码增强模型时间感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，在各种扰动条件下保持强大的稳定性和检测性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在复杂云环境中表现出卓越的异常检测能力，具有良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种基于Transformer架构的集成多尺度特征感知的异常检测方法，旨在解决云服务环境中时间建模和尺度感知特征表示的局限性。该方法首先采用改进的Transformer模块对高维监控数据进行时间建模，利用自注意力机制捕获长程依赖和上下文语义。然后，引入多尺度特征构建路径，通过下采样和并行编码提取不同粒度的时序特征。设计了一个注意力加权融合模块，动态调整各尺度对最终决策的贡献，增强模型在异常模式建模中的鲁棒性。在输入建模阶段，构建了标准化的多维时间序列，涵盖CPU利用率、内存使用和任务调度状态等核心信号，同时使用位置编码增强模型的时间感知能力。设计了系统的实验设置来评估性能，包括对比实验和超参数敏感性分析，重点关注优化器、学习率、异常比例和噪声水平的影响。实验结果表明，所提出的方法在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持强大的稳定性和检测性能，证明了其在复杂云环境中的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study proposes an anomaly detection method based on the Transformerarchitecture with integrated multiscale feature perception, aiming to addressthe limitations of temporal modeling and scale-aware feature representation incloud service environments. The method first employs an improved Transformermodule to perform temporal modeling on high-dimensional monitoring data, usinga self-attention mechanism to capture long-range dependencies and contextualsemantics. Then, a multiscale feature construction path is introduced toextract temporal features at different granularities through downsampling andparallel encoding. An attention-weighted fusion module is designed todynamically adjust the contribution of each scale to the final decision,enhancing the model's robustness in anomaly pattern modeling. In the inputmodeling stage, standardized multidimensional time series are constructed,covering core signals such as CPU utilization, memory usage, and taskscheduling states, while positional encoding is used to strengthen the model'stemporal awareness. A systematic experimental setup is designed to evaluateperformance, including comparative experiments and hyperparameter sensitivityanalysis, focusing on the impact of optimizers, learning rates, anomaly ratios,and noise levels. Experimental results show that the proposed methodoutperforms mainstream baseline models in key metrics, including precision,recall, AUC, and F1-score, and maintains strong stability and detectionperformance under various perturbation conditions, demonstrating its superiorcapability in complex cloud environments.</description>
      <author>example@mail.com (Lian Lian, Yilin Li, Song Han, Renzi Meng, Sibo Wang, Ming Wang)</author>
      <guid isPermaLink="false">2508.14503v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding</title>
      <link>http://arxiv.org/abs/2508.14395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to UIST 2025. Project website:  https://zhaorunning.github.io/NoteIt/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NoteIt是一个创新的系统，能够将教学视频自动转换为可交互的笔记，保留原始视频的层次结构和多模态信息，并允许用户自定义内容和格式。&lt;h4&gt;背景&lt;/h4&gt;用户经常观看教学视频并做笔记，以便日后回顾关键知识点而不必重新观看整个长视频。现有的自动笔记生成工具可以让用户高效获取信息性笔记，但现有工具生成的笔记无法全面保留原始视频中的信息，也不能满足用户对数字笔记多样化展示格式和交互功能的期望。&lt;h4&gt;目的&lt;/h4&gt;开发一个系统，能够自动将教学视频转换为可交互的笔记，全面保留视频信息并提供用户自定义功能。&lt;h4&gt;方法&lt;/h4&gt;提出了NoteIt系统，使用新的管道自动将教学视频转换为可交互笔记。该管道忠实地从视频中提取层次结构和多模态关键信息，并提供交互界面让用户根据偏好自定义笔记内容和展示格式。&lt;h4&gt;主要发现&lt;/h4&gt;通过技术评估和比较用户研究（N=36）发现，NoteIt在客观指标上表现良好，用户反馈积极，证明了管道的有效性和系统的整体可用性。&lt;h4&gt;结论&lt;/h4&gt;NoteIt系统能够有效解决现有工具的不足，提供更全面、交互性更强的笔记，帮助用户更高效地回顾教学视频中的关键知识点。&lt;h4&gt;翻译&lt;/h4&gt;用户经常为教学视频做笔记，以便日后访问关键知识点而无需重新观看长视频。自动笔记生成工具使用户能够高效获取信息性笔记。然而，现有研究或现成工具生成的笔记无法全面保留原始视频中传达的信息，也不能满足用户在使用数字笔记时对多样化展示格式和交互功能的期望。在这项工作中，我们提出了NoteIt，一个系统，它使用新颖的管道自动将教学视频转换为可交互笔记，忠实地从视频中提取层次结构和多模态关键信息。通过NoteIt的界面，用户可以与系统交互，根据偏好进一步自定义笔记的内容和展示格式。我们进行了技术评估和比较用户研究（N=36）。客观指标上的优异性能和积极的用户反馈证明了管道的有效性和NoteIt的整体可用性。项目网站：https://zhaorunning.github.io/NoteIt/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746059.3747626&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Users often take notes for instructional videos to access key knowledge laterwithout revisiting long videos. Automated note generation tools enable users toobtain informative notes efficiently. However, notes generated by existingresearch or off-the-shelf tools fail to preserve the information conveyed inthe original videos comprehensively, nor can they satisfy users' expectationsfor diverse presentation formats and interactive features when using notesdigitally. In this work, we present NoteIt, a system, which automaticallyconverts instructional videos to interactable notes using a novel pipeline thatfaithfully extracts hierarchical structure and multimodal key information fromvideos. With NoteIt's interface, users can interact with the system to furthercustomize the content and presentation formats of the notes according to theirpreferences. We conducted both a technical evaluation and a comparison userstudy (N=36). The solid performance in objective metrics and the positive userfeedback demonstrated the effectiveness of the pipeline and the overallusability of NoteIt. Project website: https://zhaorunning.github.io/NoteIt/</description>
      <author>example@mail.com (Running Zhao, Zhihan Jiang, Xinchen Zhang, Chirui Chang, Handi Chen, Weipeng Deng, Luyao Jin, Xiaojuan Qi, Xun Qian, Edith C. H. Ngai)</author>
      <guid isPermaLink="false">2508.14395v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum</title>
      <link>http://arxiv.org/abs/2508.14684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 2024 KDD Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对异常网络中的异质性结构问题，提出了一种基于因果边分离的频谱神经网络CES2-GAD，通过分离同质性和异质性边，使用混合频谱滤波器捕获信号，有效检测异常节点。&lt;h4&gt;背景&lt;/h4&gt;现实世界中，异常实体常添加合法连接同时隐藏直接联系，形成异质性网络结构，而大多数GNN技术无法有效处理这种结构。现有研究主要在空间域解决此问题，忽视了节点结构、特征和上下文环境的复杂关系，且频谱域研究有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在异质图上进行有效异常检测的方法，解决现有方法在处理异质性结构时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出CES2-GAD方法：1)使用因果干预将原始图分离为同质性和异质性边；2)应用多种混合频谱滤波器捕获分离图中的信号；3)连接多信号表示并输入分类器预测异常。&lt;h4&gt;主要发现&lt;/h4&gt;分析不同异质性程度节点的频谱分布发现，异常节点的异质性导致频谱能量从低频向高频转移。&lt;h4&gt;结论&lt;/h4&gt;通过大量真实数据集实验验证，CES2-GAD方法在异质图异常检测中表现出色，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界中，异常实体通常会添加更多合法连接，同时隐藏与其他异常实体的直接联系，导致异常网络中出现异质性结构，大多数基于图神经网络的技术无法解决这个问题。已有一些研究在空间域尝试解决此问题，但这些方法忽视了节点结构编码、节点特征及其上下文环境之间的复杂关系，且依赖于原则性指导，在频谱域解决异质性问题的研究仍然有限。本研究分析了具有不同异质性程度的节点的频谱分布，发现异常节点的异质性导致频谱能量从低频向高频转移。为应对上述挑战，我们提出了一种基于因果边分离的频谱神经网络CES2-GAD，用于异质图上的异常检测。首先，CES2-GAD使用因果干预将原始图分离为同质性和异质性边。随后，使用各种混合频谱滤波器捕获分离图中的信号。最后，将来自多个信号的表示连接起来并输入分类器以预测异常。使用真实数据集的大量实验证明了我们提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the real world, anomalous entities often add more legitimate connectionswhile hiding direct links with other anomalous entities, leading toheterophilic structures in anomalous networks that most GNN-based techniquesfail to address. Several works have been proposed to tackle this issue in thespatial domain. However, these methods overlook the complex relationshipsbetween node structure encoding, node features, and their contextualenvironment and rely on principled guidance, research on solving spectraldomain heterophilic problems remains limited. This study analyzes the spectraldistribution of nodes with different heterophilic degrees and discovers thatthe heterophily of anomalous nodes causes the spectral energy to shift from lowto high frequencies. To address the above challenges, we propose a spectralneural network CES2-GAD based on causal edge separation for anomaly detectionon heterophilic graphs. Firstly, CES2-GAD will separate the original graph intohomophilic and heterophilic edges using causal interventions. Subsequently,various hybrid-spectrum filters are used to capture signals from the segmentedgraphs. Finally, representations from multiple signals are concatenated andinput into a classifier to predict anomalies. Extensive experiments withreal-world datasets have proven the effectiveness of the method we proposed.</description>
      <author>example@mail.com (Zengyi Wo, Wenjun Wang, Minglai Shao, Chang Liu, Yumeng Wang, Yueheng Sun)</author>
      <guid isPermaLink="false">2508.14684v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Improving Fairness in Graph Neural Networks via Counterfactual Debiasing</title>
      <link>http://arxiv.org/abs/2508.14683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 2024 KDD Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Fair-ICD的新方法，利用反事实数据增强来减轻图神经网络中的偏见问题，同时保持高预测性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在建模图结构数据方面取得了成功，但可能会基于种族和性别等属性表现出预测偏见，且这种偏见可能因图结构和消息传递机制而加剧。&lt;h4&gt;目的&lt;/h4&gt;解决GNN中的偏见问题，同时保持预测准确性和公平性之间的平衡。&lt;h4&gt;方法&lt;/h4&gt;提出Fair-ICD方法，通过在消息传递前使用反事实创建多样化邻域来学习无偏节点表示，并采用对抗性判别器减少传统GNN分类器预测中的偏见。&lt;h4&gt;主要发现&lt;/h4&gt;Fair-ICD在适度条件下确保了GNN的公平性，实验表明该方法显著提高了公平性指标，同时保持了高预测性能。&lt;h4&gt;结论&lt;/h4&gt;Fair-ICD是一种有效的方法，可以在保持高预测性能的同时提高GNN的公平性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在建模图结构数据方面取得了成功。然而，与其他机器学习模型类似，GNN可能基于种族和性别等属性表现出预测偏见。此外，GNN中的偏见可能因图结构和消息传递机制而加剧。最近的前沿方法提出通过从输入或表示中过滤敏感信息（如边删除或特征掩码）来减轻偏见。然而，我们认为这些策略可能会无意中消除非敏感特征，导致预测准确性和公平性之间的平衡受损。为了应对这一挑战，我们提出了一种利用反事实数据增强进行偏见减轻的新方法。该方法在消息传递之前使用反事实创建多样化的邻域，促进从增强图中学习无偏的节点表示。随后，采用对抗性判别器来减少传统GNN分类器预测中的偏见。我们提出的技术Fair-ICD确保了GNN在适度条件下的公平性。在三个标准数据集上使用三种GNN骨干网络的实验表明，Fair-ICD显著提高了公平性指标，同时保持了高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have been successful in modelinggraph-structured data. However, similar to other machine learning models, GNNscan exhibit bias in predictions based on attributes like race and gender.Moreover, bias in GNNs can be exacerbated by the graph structure andmessage-passing mechanisms. Recent cutting-edge methods propose mitigating biasby filtering out sensitive information from input or representations, like edgedropping or feature masking. Yet, we argue that such strategies mayunintentionally eliminate non-sensitive features, leading to a compromisedbalance between predictive accuracy and fairness. To tackle this challenge, wepresent a novel approach utilizing counterfactual data augmentation for biasmitigation. This method involves creating diverse neighborhoods usingcounterfactuals before message passing, facilitating unbiased noderepresentations learning from the augmented graph. Subsequently, an adversarialdiscriminator is employed to diminish bias in predictions by conventional GNNclassifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNsunder moderate conditions. Experiments on standard datasets using three GNNbackbones demonstrate that Fair-ICD notably enhances fairness metrics whilepreserving high predictive performance.</description>
      <author>example@mail.com (Zengyi Wo, Chang Liu, Yumeng Wang, Minglai Shao, Wenjun Wang)</author>
      <guid isPermaLink="false">2508.14683v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2508.14427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于知识图谱注入的微调算法框架，用于解决大型语言模型在处理需要结构化知识的任务时存在的推理链缺失和实体级语义理解不足的问题。该方法通过图神经网络编码实体及其关系，设计融合机制联合建模知识图谱嵌入与语言模型的上下文表示，并引入门控机制动态平衡语言语义和结构知识的贡献，有效提高了模型在实体识别、问答和语言生成等任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在处理需要结构化知识的任务时存在两个主要问题：推理链缺失和实体级语义理解不足。这限制了模型在需要复杂语义理解和结构推理的任务上的表现。&lt;h4&gt;目的&lt;/h4&gt;提出一种微调算法框架，通过知识图谱注入来增强大型语言模型在结构化知识任务中的表现，提高模型的语义理解能力和推理能力。&lt;h4&gt;方法&lt;/h4&gt;基于预训练语言模型构建，引入结构化图信息进行辅助学习；使用图神经网络编码实体及其关系，构建基于图的语义表示；设计融合机制联合建模知识图谱嵌入与语言模型的上下文表示；引入门控机制动态平衡语言语义和结构知识的贡献，提高知识整合的鲁棒性；构建联合损失函数，同时考虑任务性能和结构对齐目标；进行系统敏感性实验，评估学习率、图覆盖率和结构扰动对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的结构感知微调框架显著增强了模型表示复杂语义单位的能力；在涉及结构推理和实体提取的场景中，模型表现出更好的语义一致性和上下文逻辑建模能力；实验结果验证了该方法在实体识别、问答和语言生成等任务上的有效性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;基于知识图谱注入的微调算法框架可以有效解决大型语言模型在结构化知识任务中的推理链缺失和语义理解不足问题，通过融合结构化知识提高模型在多种任务上的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了大型语言模型在处理需要结构化知识的任务时存在的推理链缺失和实体级语义理解不足的问题。它提出了一种基于知识图谱注入的微调算法框架。该方法基于预训练语言模型构建，并引入结构化图信息进行辅助学习。使用图神经网络对实体及其关系进行编码，构建基于图的语义表示。然后设计了一种融合机制，联合建模知识图谱嵌入与语言模型的上下文表示。为了增强知识整合的鲁棒性，引入了一种门控机制来动态平衡语言语义和结构知识的贡献。这有效缓解了不同表示空间之间的冲突。在训练过程中，构建了一个联合损失函数，同时考虑任务性能和结构对齐目标。这有助于提高实体预测和语义推理的准确性。研究还包括一系列系统的敏感性实验。它评估了学习率、图覆盖率和结构扰动对模型性能的影响。结果进一步验证了所提出方法在实体识别、问答和语言生成等任务上的有效性和稳定性。实验结果表明，所提出的结构感知微调框架显著增强了模型表示复杂语义单位的能力。在涉及结构推理和实体提取的场景中，它表现出更好的语义一致性和上下文逻辑建模能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the problems of missing reasoning chains andinsufficient entity-level semantic understanding in large language models whendealing with tasks that require structured knowledge. It proposes a fine-tuningalgorithm framework based on knowledge graph injection. The method builds onpretrained language models and introduces structured graph information forauxiliary learning. A graph neural network is used to encode entities and theirrelations, constructing a graph-based semantic representation. A fusionmechanism is then designed to jointly model the knowledge graph embeddings withthe contextual representations from the language model. To enhance therobustness of knowledge integration, a gating mechanism is introduced todynamically balance the contributions of linguistic semantics and structuralknowledge. This effectively mitigates conflicts between differentrepresentational spaces. During training, a joint loss function is constructedto account for both task performance and structural alignment objectives. Thishelps improve the accuracy of entity prediction and semantic reasoning. Thestudy also includes a series of systematic sensitivity experiments. Itevaluates the effects of learning rate, graph coverage, and structuralperturbations on model performance. The results further validate theeffectiveness and stability of the proposed method across tasks such as entityrecognition, question answering, and language generation. Experimental findingsshow that the proposed structure-aware fine-tuning framework significantlyenhances the model's ability to represent complex semantic units. Itdemonstrates better semantic consistency and contextual logic modeling inscenarios involving structural reasoning and entity extraction.</description>
      <author>example@mail.com (Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du)</author>
      <guid isPermaLink="false">2508.14427v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.14338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络中学习算法与图结构之间的相互作用关系，通过理论分析和实证研究揭示了图结构如何影响学习算法的性能，为GNN算法设计和选择提供了新见解。&lt;h4&gt;背景&lt;/h4&gt;现有关于图神经网络学习动力学的理论研究主要关注无噪声情况下的算法收敛率，且仅将学习动力与图结构(如最大度数)进行粗略关联，缺乏对图结构与学习算法之间深入关系的理解。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在弥合理论差距，探究图神经网络在有噪声情况下学习算法的过度风险(泛化性能)，并分析图结构如何影响学习算法的表现。&lt;h4&gt;方法&lt;/h4&gt;研究将学习理论的传统框架扩展到图神经网络上下文中，通过谱图理论建立学习算法性能与图结构之间的联系，并采用比较分析方法研究不同图结构(规则图与幂律图)对算法性能的影响，同时将分析扩展到多层线性GNNs。&lt;h4&gt;主要发现&lt;/h4&gt;1. 推导了SGD和岭回归在GNNs中的过度风险曲线，并通过谱图理论与图结构建立联系；2. 不同图结构(规则图与幂律图)对算法性能有显著影响；3. 多层线性GNNs中存在非各向同性效应增加的现象，从学习算法角度提供了对过度平滑问题的新见解。&lt;h4&gt;结论&lt;/h4&gt;图结构、GNNs和学习算法之间存在耦合关系，这一发现对实际中GNN算法的设计和选择具有重要指导意义，实证结果与理论预测一致验证了这一观点。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了图神经网络中学习算法与图结构之间的相互作用关系。现有关于图神经网络学习动力学的理论研究主要关注插值 regime(无噪声)下学习算法的收敛率，并且仅提供了这些动力与实际图结构(如最大度数)之间粗略的联系。本文旨在通过研究图神经网络中学习算法在泛化 regime(有噪声)下的过度风险(泛化性能)来弥合这一差距。具体来说，我们将学习理论文献中的传统设置扩展到图神经网络的上下文中，并考察图结构如何影响随机梯度下降和岭回归等学习算法的性能。我们的研究在理解图结构与图神经网络中学习的相互作用方面做出了几个关键贡献。首先，我们推导了图神经网络中随机梯度下降和岭回归的过度风险曲线，并通过谱图理论将这些曲线与图结构联系起来。基于这一建立的框架，我们通过比较分析进一步探索了不同图结构(规则图与幂律图)如何影响这些算法的性能。此外，我们将分析扩展到多层线性图神经网络，揭示了过度风险曲线上非各向同性效应的增加，从而从学习算法的角度为图神经网络中的过度平滑问题提供了新见解。我们的实证结果与理论预测一致，共同展示了图结构、图神经网络和学习算法之间的耦合关系，并为实际中图神经网络算法的设计和选择提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies the interplay between learning algorithms and graphstructure for graph neural networks (GNNs). Existing theoretical studies on thelearning dynamics of GNNs primarily focus on the convergence rates of learningalgorithms under the interpolation regime (noise-free) and offer only a crudeconnection between these dynamics and the actual graph structure (e.g., maximumdegree). This paper aims to bridge this gap by investigating the excessive risk(generalization performance) of learning algorithms in GNNs within thegeneralization regime (with noise). Specifically, we extend the conventionalsettings from the learning theory literature to the context of GNNs and examinehow graph structure influences the performance of learning algorithms such asstochastic gradient descent (SGD) and Ridge regression. Our study makes severalkey contributions toward understanding the interplay between graph structureand learning in GNNs. First, we derive the excess risk profiles of SGD andRidge regression in GNNs and connect these profiles to the graph structurethrough spectral graph theory. With this established framework, we furtherexplore how different graph structures (regular vs. power-law) impact theperformance of these algorithms through comparative analysis. Additionally, weextend our analysis to multi-layer linear GNNs, revealing an increasingnon-isotropic effect on the excess risk profile, thereby offering new insightsinto the over-smoothing issue in GNNs from the perspective of learningalgorithms. Our empirical results align with our theoretical predictions,\emph{collectively showcasing a coupling relation among graph structure, GNNsand learning algorithms, and providing insights on GNN algorithm design andselection in practice.}</description>
      <author>example@mail.com (Junwei Su, Chuan Wu)</author>
      <guid isPermaLink="false">2508.14338v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Multi-view Graph Condensation via Tensor Decomposition</title>
      <link>http://arxiv.org/abs/2508.14330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'基于张量分解的多视图图浓缩'(GCTD)的新方法，用于解决大规模图神经网络训练中的计算挑战。该方法通过张量分解技术合成信息丰富的小型图，同时保持图神经网络的预测性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在药物发现、目标检测、社交媒体分析、推荐系统和文本分类等现实世界应用中表现出色。然而，在大规模图上训练GNNs面临显著的计算挑战，因为需要大量资源进行存储和处理。现有的图浓缩方法通常依赖于计算密集的双层优化，且无法保持合成节点与原始节点之间的映射关系，限制了模型决策的可解释性。&lt;h4&gt;目的&lt;/h4&gt;探索分解技术在图浓缩中的应用潜力，了解这些技术能在多大程度上合成信息丰富的小型图，并实现与下游任务相当的性能。具体目标是提出一种新方法，解决现有图浓缩方法中的计算效率和可解释性问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种名为'基于张量分解的多视图图浓缩'(GCTD)的新方法。该方法利用分解技术从图数据中学习线性或多线性函数，通过多视图处理和张量分解技术合成小型图，同时保持与原始图的关键信息映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实世界数据集上的广泛实验表明，GCTD能够有效减小图的大小，同时保持GNN的性能。在六个数据集中的三个上，GCTD实现了高达4.0%的准确率提升，并且在大型图上的表现与现有方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;GCTD方法成功地解决了现有图浓缩方法的局限性，通过张量分解技术实现了更高效、更具可解释性的图浓缩。该方法不仅减少了计算资源需求，还保持了合成图与原始图之间的映射关系，为图神经网络在大规模图上的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种现实世界应用中已展现出显著成果，包括药物发现、目标检测、社交媒体分析、推荐系统和文本分类。与其巨大潜力相比，在大规模图上训练它们由于存储和处理所需的资源而带来了显著的计算挑战。图浓缩已成为一种有前景的解决方案，通过学习一个合成的小型图来减少这些需求，同时保留原始图的基本信息并维持GNN的预测性能。尽管它们有效，但当前的图浓缩方法通常依赖于计算密集的双层优化。此外，它们无法保持合成节点与原始节点之间的映射关系，限制了模型决策的可解释性。从这个意义上讲，各种分解技术已被应用于从图数据中学习线性或多线性函数，提供了一种更透明且资源消耗更少的替代方案。然而，它们在图浓缩中的应用尚未被探索。本文解决了这一差距，并提出了一种名为'基于张量分解的多视图图浓缩'(GCTD)的新方法，以研究这些技术在多大程度上能够合成信息丰富的小型图并实现相当的下游任务性能。在六个真实世界数据集上的广泛实验表明，GCTD能够有效减小图的大小，同时保持GNN的性能，在六个数据集中的三个上实现了高达4.0%的准确率提升，并且在大型图上与现有方法相比具有竞争力性能。我们的代码可在https://anonymous.4open.science/r/gctd-345A获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable results in variousreal-world applications, including drug discovery, object detection, socialmedia analysis, recommender systems, and text classification. In contrast totheir vast potential, training them on large-scale graphs presents significantcomputational challenges due to the resources required for their storage andprocessing. Graph Condensation has emerged as a promising solution to reducethese demands by learning a synthetic compact graph that preserves theessential information of the original one while maintaining the GNN'spredictive performance. Despite their efficacy, current graph condensationapproaches frequently rely on a computationally intensive bi-leveloptimization. Moreover, they fail to maintain a mapping between synthetic andoriginal nodes, limiting the interpretability of the model's decisions. In thissense, a wide range of decomposition techniques have been applied to learnlinear or multi-linear functions from graph data, offering a more transparentand less resource-intensive alternative. However, their applicability to graphcondensation remains unexplored. This paper addresses this gap and proposes anovel method called Multi-view Graph Condensation via Tensor Decomposition(GCTD) to investigate to what extent such techniques can synthesize aninformative smaller graph and achieve comparable downstream task performance.Extensive experiments on six real-world datasets demonstrate that GCTDeffectively reduces graph size while preserving GNN performance, achieving upto a 4.0\ improvement in accuracy on three out of six datasets and competitiveperformance on large graphs compared to existing approaches. Our code isavailable at https://anonymous.4open.science/r/gctd-345A.</description>
      <author>example@mail.com (Nícolas Roque dos Santos, Dawon Ahn, Diego Minatel, Alneu de Andrade Lopes, Evangelos E. Papalexakis)</author>
      <guid isPermaLink="false">2508.14330v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Hypergraph Neural Network</title>
      <link>http://arxiv.org/abs/2508.14101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;超图神经网络在捕获高阶关系方面有广泛应用，但存在长程依赖捕捉不足的问题。作者提出Implicit Hypergraph Neural Network (IHNN)框架，通过联合学习节点和超边的固定点表示，以端到端方式解决此问题，实验证明其在节点分类任务上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;超图为捕捉实体间高阶关系提供广义框架，已应用于医疗保健、社交网络和生物信息学等领域。超图神经网络通过超边上节点间的消息传递学习潜在表示，已成为这些领域预测任务的首选方法。然而，这些方法通常只执行少量消息传递轮次，导致表示仅捕获局部信息而忽略长程高阶依赖。&lt;h4&gt;目的&lt;/h4&gt;解决超图神经网络在捕获长程依赖关系时面临的性能下降问题。作者证明现有超图神经网络在聚合更多信息以捕获长程依赖时会失去预测能力，并提出IHNN框架来缓解这一问题。&lt;h4&gt;方法&lt;/h4&gt;提出Implicit Hypergraph Neural Network (IHNN)框架，通过端到端方式联合学习节点和超边的固定点表示。利用隐式微分，引入可处理的投影梯度下降方法来高效训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;现有超图神经网络在尝试通过增加消息传递轮次捕获长程依赖时性能下降。IHNN框架能有效解决此问题，在真实超图上的节点分类任务中，大多数设置下都优于最先进的先前工作，建立了超图学习的新技术水平。&lt;h4&gt;结论&lt;/h4&gt;IHNN框架通过联合学习节点和超边的固定点表示，有效解决了超图神经网络在捕获长程依赖时的性能下降问题，为超图学习领域建立了新的技术水平。&lt;h4&gt;翻译&lt;/h4&gt;超图为捕捉实体间的高阶关系提供了广义框架，已广泛应用于医疗保健、社交网络和生物信息学等多个领域。超图神经网络通过超边上节点间的消息传递学习潜在表示，已成为这些领域预测任务的首选方法。这些方法通常只执行少量消息传递轮次来学习表示，然后用于预测。这种少量消息传递轮次的代价是表示仅捕获局部信息而忽略了长程高阶依赖。然而，正如我们所展示的，盲目增加消息传递轮次以捕获长程依赖同样会降低超图神经网络的性能。最近的工作表明，隐式图神经网络在标准图中能够捕获长程依赖同时保持性能。尽管它们很流行，但先前的工作尚未研究超图神经网络中的长程依赖问题。在此，我们首先证明现有超图神经网络在聚合更多信息以捕获长程依赖时会失去预测能力。然后，我们提出了隐式超图神经网络(IHNN)，这是一个新颖的框架，通过端到端方式联合学习节点和超边的固定点表示来缓解这一问题。利用隐式微分，我们引入了一种可处理的投影梯度下降方法来高效训练模型。在真实超图上的节点分类任务的广泛实验表明，IHNN在大多数设置下都优于最接近的先前工作，在超图学习中建立了新的技术水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hypergraphs offer a generalized framework for capturing high-orderrelationships between entities and have been widely applied in various domains,including healthcare, social networks, and bioinformatics. Hypergraph neuralnetworks, which rely on message-passing between nodes over hyperedges to learnlatent representations, have emerged as the method of choice for predictivetasks in many of these domains. These approaches typically perform only a smallnumber of message-passing rounds to learn the representations, which they thenutilize for predictions. The small number of message-passing rounds comes at acost, as the representations only capture local information and foregolong-range high-order dependencies. However, as we demonstrate, blindlyincreasing the message-passing rounds to capture long-range dependency alsodegrades the performance of hyper-graph neural networks.  Recent works have demonstrated that implicit graph neural networks capturelong-range dependencies in standard graphs while maintaining performance.Despite their popularity, prior work has not studied long-range dependencyissues on hypergraph neural networks. Here, we first demonstrate that existinghypergraph neural networks lose predictive power when aggregating moreinformation to capture long-range dependency. We then propose ImplicitHypergraph Neural Network (IHNN), a novel framework that jointly learnsfixed-point representations for both nodes and hyperedges in an end-to-endmanner to alleviate this issue. Leveraging implicit differentiation, weintroduce a tractable projected gradient descent approach to train the modelefficiently. Extensive experiments on real-world hypergraphs for nodeclassification demonstrate that IHNN outperforms the closest prior works inmost settings, establishing a new state-of-the-art in hypergraph learning.</description>
      <author>example@mail.com (Akash Choudhuri, Yongjian Zhong, Bijaya Adhikari)</author>
      <guid isPermaLink="false">2508.14101v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>Non-Dissipative Graph Propagation for Non-Local Community Detection</title>
      <link>http://arxiv.org/abs/2508.14097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为非对称图神经网络（uAGNN）的新型无监督社区检测方法，该方法通过非耗散动力学系统确保稳定性并有效传播长程信息，解决了异质性图中社区检测的挑战。&lt;h4&gt;背景&lt;/h4&gt;在异质性图中，相似节点和属于同一社区的节点通常距离较远，这使得传统的图神经网络方法难以有效进行社区检测，因为它们依赖于本质上局部的消息传递方案来学习节点表示。&lt;h4&gt;目的&lt;/h4&gt;解决异质性图中的社区检测问题，通过在消息传递过程中传播长程信息来提高检测效果。&lt;h4&gt;方法&lt;/h4&gt;提出非对称图神经网络（uAGNN），一种利用非耗散动力学系统确保稳定性和有效传播长程信息的无监督社区检测方法，通过使用非对称权重矩阵捕获局部和全局图结构。&lt;h4&gt;主要发现&lt;/h4&gt;在10个数据集上的广泛实验表明，uAGNN在高和中等异质性设置中表现优于传统方法，特别是在传统方法无法利用长程依赖性的情况下。&lt;h4&gt;结论&lt;/h4&gt;uAGNN作为无监督社区检测工具在不同图环境中具有强大潜力，能够有效克服异质性场景的限制。&lt;h4&gt;翻译&lt;/h4&gt;图中的社区检测旨在将节点聚类为有意义的组，这在异质性图中尤其具有挑战性，因为相似节点和属于同一社区的节点通常距离较远。当图神经网络处理这项任务时，这一点尤为明显，因为它们本质上依赖于局部的消息传递方案来学习节点表示，这些节点表示用于将节点聚类到社区中。在本工作中，我们认为在消息传递过程中传播长程信息是有效进行异质性图社区检测的关键。为此，我们引入了非对称图神经网络（uAGNN），这是一种新的无监督社区检测方法，它利用非耗散动力学系统确保稳定性并有效传播长程信息。通过采用非对称权重矩阵，uAGNN捕获了局部和全局图结构，克服了异质性场景带来的限制。在10个数据集上的广泛实验证明了uAGNN在高和中等异质性设置中的优越性能，而传统方法无法利用长程依赖性。这些结果强调了uAGNN作为不同图环境中无监督社区检测的强大工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Community detection in graphs aims to cluster nodes into meaningful groups, atask particularly challenging in heterophilic graphs, where nodes sharingsimilarities and membership to the same community are typically distantlyconnected. This is particularly evident when this task is tackled by graphneural networks, since they rely on an inherently local message passing schemeto learn the node representations that serve to cluster nodes into communities.In this work, we argue that the ability to propagate long-range informationduring message passing is key to effectively perform community detection inheterophilic graphs. To this end, we introduce the Unsupervised AntisymmetricGraph Neural Network (uAGNN), a novel unsupervised community detection approachleveraging non-dissipative dynamical systems to ensure stability and topropagate long-range information effectively. By employing antisymmetric weightmatrices, uAGNN captures both local and global graph structures, overcoming thelimitations posed by heterophilic scenarios. Extensive experiments across tendatasets demonstrate uAGNN's superior performance in high and mediumheterophilic settings, where traditional methods fail to exploit long-rangedependencies. These results highlight uAGNN's potential as a powerful tool forunsupervised community detection in diverse graph environments.</description>
      <author>example@mail.com (William Leeney, Alessio Gravina, Davide Bacciu)</author>
      <guid isPermaLink="false">2508.14097v1</guid>
      <pubDate>Thu, 21 Aug 2025 15:14:00 +0800</pubDate>
    </item>
    <item>
      <title>X-Node: Self-Explanation is All We Need</title>
      <link>http://arxiv.org/abs/2508.10461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;X-Node是一种自解释的图神经网络框架，使每个节点在预测过程中生成自己的解释，解决了GNN决策不透明的问题，同时保持了高分类准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在计算机视觉和医学图像分类中取得优异成果，但其决策过程不透明，限制了在高风险临床应用中的可信度，而这些应用中可解释性至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供局部、节点级解释的GNN框架，增强其可解释性和可信度，特别是在医疗等高风险领域。&lt;h4&gt;方法&lt;/h4&gt;构建结构化上下文向量编码节点的局部拓扑特征，通过轻量级推理器生成解释向量，该向量用于重建节点嵌入、生成自然语言解释并通过文本注入机制引导GNN。&lt;h4&gt;主要发现&lt;/h4&gt;X-Node在保持竞争性分类准确性的同时，能够生成忠实的、针对每个节点的解释，解决了传统GNN解释技术只能提供全局解释的局限性。&lt;h4&gt;结论&lt;/h4&gt;X-Node框架成功地将可解释性整合到GNN的预测过程中，使其更适合需要高度透明度和可解释性的高风险临床应用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通过捕捉数据实例之间的结构依赖关系，在计算机视觉和医学图像分类任务中取得了最先进的结果。然而，它们的决策过程仍然不透明，限制了它们在高风险临床应用中的可信度，而这些应用中可解释性至关重要。现有的GNN可解释性技术通常是事后和全局的，对单个节点决策或局部推理的见解有限。我们引入了X-Node，一种自解释的GNN框架，其中每个节点在预测过程中生成自己的解释。对于每个节点，我们构建一个结构化的上下文向量，编码其局部拓扑内的可解释线索，如度、中心性、聚类、特征显著性和标签一致性。一个轻量级的推理器模块将此上下文映射到一个紧凑的解释向量，该向量有三个用途：(1)通过解码器重建节点的潜在嵌入以保持忠实性，(2)使用预训练的LLM(如Grok或Gemini)生成自然语言解释，以及(3)通过'文本注入'机制将解释反馈回消息传递管道来引导GNN本身。我们在从MedMNIST和MorphoMNIST派生的两个图数据集上评估了X-Node，并将其与GCN、GAT和GIN骨干网络集成。我们的结果表明，X-Node在保持竞争性分类准确性的同时，能够生成忠实的、针对每个节点的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved state-of-the-art results incomputer vision and medical image classification tasks by capturing structuraldependencies across data instances. However, their decision-making remainslargely opaque, limiting their trustworthiness in high-stakes clinicalapplications where interpretability is essential. Existing explainabilitytechniques for GNNs are typically post-hoc and global, offering limited insightinto individual node decisions or local reasoning. We introduce X-Node, aself-explaining GNN framework in which each node generates its own explanationas part of the prediction process. For every node, we construct a structuredcontext vector encoding interpretable cues such as degree, centrality,clustering, feature saliency, and label agreement within its local topology. Alightweight Reasoner module maps this context into a compact explanationvector, which serves three purposes: (1) reconstructing the node's latentembedding via a decoder to enforce faithfulness, (2) generating a naturallanguage explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3)guiding the GNN itself via a "text-injection" mechanism that feeds explanationsback into the message-passing pipeline. We evaluate X-Node on two graphdatasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT,and GIN backbones. Our results show that X-Node maintains competitiveclassification accuracy while producing faithful, per-node explanations.Repository: https://github.com/basiralab/X-Node.</description>
      <author>example@mail.com (Prajit Sengupta, Islem Rekik)</author>
      <guid isPermaLink="false">2508.10461v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
  <item>
      <title>4DNeX: Feed-Forward 4D Generative Modeling Made Easy</title>
      <link>http://arxiv.org/abs/2508.13154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://4dnex.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;4DNeX是一个前馈框架，可以从单张图像生成4D（动态3D）场景表示。它通过微调预训练的视频扩散模型，实现了高效、端到端的图像到4D生成。研究团队构建了大规模数据集4DNeX-10M，引入了统一的6D视频表示，并提出了一套有效的适应策略，使预训练的视频扩散模型能够用于4D建模。4DNeX能够生成高质量的动态点云，支持新视角视频合成。&lt;h4&gt;背景&lt;/h4&gt;现有的4D生成方法依赖于计算密集型优化或需要多帧视频输入，效率较低且应用受限。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、端到端的图像到4D生成方法，解决现有方法计算效率低、需要多帧输入的问题。&lt;h4&gt;方法&lt;/h4&gt;1) 构建了4DNeX-10M大规模数据集，使用先进的重建方法生成高质量4D标注；2) 引入统一的6D视频表示，联合建模RGB和XYZ序列；3) 提出一套简单有效的适应策略，使预训练视频扩散模型能够用于4D建模；4) 通过微预训练的视频扩散模型实现端到端的图像到4D生成。&lt;h4&gt;主要发现&lt;/h4&gt;4DNeX能够生成高质量的动态点云，支持新视角视频合成，并且在效率和泛化性上优于现有的4D生成方法。&lt;h4&gt;结论&lt;/h4&gt;4DNeX为图像到4D建模提供了可扩展的解决方案，并为能够模拟动态场景演化的生成式4D世界模型奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了4DNeX，这是第一个从单张图像生成4D（即动态3D）场景表示的前馈框架。与依赖于计算密集型优化或需要多帧视频输入的现有方法不同，4DNeX通过微调预训练的视频扩散模型，实现了高效、端到端的图像到4D生成。具体来说，1)为缓解4D数据的稀缺性，我们构建了4DNeX-10M，这是一个使用先进重建方法生成高质量4D标注的大规模数据集；2)我们引入了统一的6D视频表示，联合建模RGB和XYZ序列，促进外观和几何的结构化学习；3)我们提出了一套简单而有效的适应策略，使预训练的视频扩散模型能够用于4D建模。4DNeX生成高质量的动态点云，支持新视角视频合成。大量实验证明，4DNeX在效率和泛化性上优于现有的4D生成方法，为图像到4D建模提供了可扩展的解决方案，并为模拟动态场景演化的生成式4D世界模型奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单张图像生成4D（动态3D）场景表示的问题。这个问题很重要，因为创建4D场景是生成式建模的核心能力，它为构建能够预测和模拟动态场景演化的4D世界模型奠定了基础，在AR/VR、电影制作和数字内容创作等领域有广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有4D场景建模方法的局限性：要么需要视频输入，要么依赖计算密集的优化过程。为了解决这些问题，他们考虑微调预训练的视频扩散模型，但面临两个挑战：4D数据稀缺和如何简单高效地适配预训练模型。作者借鉴了现有的视频扩散模型（如Wan2.1）和3D重建技术（如DUSt3R、MonST3R）来构建解决方案，通过构建4DNeX-10M数据集解决数据稀缺问题，并设计了XYZ初始化、归一化等策略来适配模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过微调预训练的视频扩散模型，实现从单张图像生成动态3D场景。作者提出统一的6D视频表示，将RGB（外观）和XYZ（几何）序列联合建模。整体流程：输入单张图像和初始化XYZ地图→VAE编码→沿宽度维度融合RGB和XYZ→结合噪声潜在和掩模输入到LoRA调优的DiT模型→解码生成RGB和XYZ视频→后优化恢复相机参数和深度图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出4DNeX，第一个图像到4D生成的前馈框架；2) 构建4DNeX-10M大型数据集；3) 引入6D视频表示联合建模RGB和XYZ；4) 设计简单有效的微调策略。相比之前工作，4DNeX无需计算密集优化，能端到端生成动态点云，效率更高（15分钟vs其他方法1小时以上），在动态度和泛化性方面表现更好，且能处理更多样化的场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 4DNeX通过微调预训练视频扩散模型和引入6D视频表示，实现了从单张图像高效生成高质量4D场景，为构建动态世界模型提供了可扩展的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,dynamic 3D) scene representations from a single image. In contrast to existingmethods that rely on computationally intensive optimization or requiremulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4Dgeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scaledataset with high-quality 4D annotations generated using advancedreconstruction approaches. 2) we introduce a unified 6D video representationthat jointly models RGB and XYZ sequences, facilitating structured learning ofboth appearance and geometry. 3) we propose a set of simple yet effectiveadaptation strategies to repurpose pretrained video diffusion models for 4Dmodeling. 4DNeX produces high-quality dynamic point clouds that enablenovel-view video synthesis. Extensive experiments demonstrate that 4DNeXoutperforms existing 4D generation methods in efficiency and generalizability,offering a scalable solution for image-to-4D modeling and laying the foundationfor generative 4D world models that simulate dynamic scene evolution.</description>
      <author>example@mail.com (Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, Ziwei Liu)</author>
      <guid isPermaLink="false">2508.13154v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>TiP4GEN: Text to Immersive Panorama 4D Scene Generation</title>
      <link>http://arxiv.org/abs/2508.12415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了TiP4GEN，一个先进的文本到动态全景场景生成框架，能够实现细粒度内容控制并合成丰富的运动、几何一致的全景4D场景。&lt;h4&gt;背景&lt;/h4&gt;随着VR/AR技术的快速发展和广泛应用，对高质量、沉浸式动态场景的需求日益增长。然而，现有的生成工作主要集中在静态场景或狭小视角的动态场景，无法提供真正的360度全视角沉浸式体验。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成360度沉浸式虚拟环境的技术，解决现有技术在动态全景场景生成中的局限性。&lt;h4&gt;方法&lt;/h4&gt;TiP4GEN结合全景视频生成和动态场景重建。视频生成部分采用双分支生成模型，包括全景分支和透视分支，通过双向交叉注意力机制促进信息交换。场景重建部分提出基于3D高斯飞溅的几何对齐重建模型，使用度量深度图对齐时空点云，确保重建场景的几何一致性和时间连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明所提出设计的有效性，TiP4GEN在生成视觉上引人注目且运动连贯的动态全景场景方面具有优越性。&lt;h4&gt;结论&lt;/h4&gt;TiP4GEN能够有效生成高质量、沉浸式的动态全景场景，满足VR/AR技术对全视角体验的需求。&lt;h4&gt;翻译&lt;/h4&gt;随着VR/AR技术的快速发展和广泛应用，对创建高质量、沉浸式动态场景的需求日益增长。然而，现有的生成工作主要集中在静态场景或狭小视角的动态场景，无法提供真正的360度全视角沉浸式体验。在本文中，我们介绍了TiP4GEN，这是一个先进的文本到动态全景场景生成框架，能够实现细粒度内容控制并合成丰富的运动、几何一致的全景4D场景。TiP4GEN结合了全景视频生成和动态场景重建，以创建360度沉浸式虚拟环境。对于视频生成，我们引入了一个双分支生成模型，包括全景分支和透视分支，分别负责全局和局部视图生成。双向交叉注意力机制促进了分支之间的全面信息交换。对于场景重建，我们提出了一个基于3D高斯飞溅的几何对齐重建模型。通过使用度量深度图对齐时空点云，并使用估计的姿态初始化场景相机，我们的方法确保了重建场景的几何一致性和时间连贯性。大量实验证明了我们提出设计的有效性和TiP4GEN在生成视觉上引人注目且运动连贯的动态全景场景方面的优越性。我们的项目页面是https://ke-xing.github.io/TiP4GEN/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何生成高质量、沉浸式的360度动态全景场景问题。随着VR/AR技术在游戏、娱乐、教育等领域的广泛应用，对能够从任何视角观察的沉浸式动态场景需求日益增长，而现有方法无法提供真正的360度全景体验或缺乏对场景内容的精细控制，限制了虚拟现实应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性，设计了双分支架构来结合全景和透视两种视角的优势。借鉴了扩散模型（特别是视频扩散模型AnimateDiff）、3D高斯溅射技术、单目深度估计和LoRA微调等技术。作者认识到直接微调基础视频扩散模型会导致场景多样性有限，因此提出全景分支确保全局一致性，透视分支增强局部细节，并通过双向交叉注意力机制促进信息交换。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合全景和透视两种视角的优势，通过双分支架构生成高质量全景视频，并使用时空对齐技术确保重建4D场景的一致性。整体流程分为两阶段：1）双分支全景视频生成：全景分支接收全局文本提示确保整体一致性，透视分支接收多个局部文本提示增强多样性，通过双向交叉注意力机制促进信息交换；2）几何对齐重建：使用深度图进行空间对齐确保不同视角几何一致，通过时间对齐确保不同时间帧几何一致，并利用相机姿态估计减轻运动引起的不一致性，最后用3D高斯溅射优化重建场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）TiP4GEN框架实现从文本到高质量、运动丰富、几何一致的沉浸式全景4D场景生成；2）双分支生成模型结合全景和透视分支，通过双向交叉注意力实现全局一致性和局部多样性融合；3）几何对齐重建模型基于3D高斯溅射，通过时空对齐和相机姿态初始化提高一致性。相比之前工作，TiP4GEN专注于动态全景场景而非静态场景，支持多个局部提示实现精细内容控制，通过双分支架构处理全景与透视差异，并在重建阶段确保时空一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TiP4GEN通过创新的双分支架构和几何对齐重建技术，实现了从文本到高质量、运动丰富、几何一致的沉浸式全景4D场景的生成，为VR/AR应用提供了强大的内容创建工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement and widespread adoption of VR/AR technologies,there is a growing demand for the creation of high-quality, immersive dynamicscenes. However, existing generation works predominantly concentrate on thecreation of static scenes or narrow perspective-view dynamic scenes, fallingshort of delivering a truly 360-degree immersive experience from any viewpoint.In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamicpanorama scene generation framework that enables fine-grained content controland synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GENintegrates panorama video generation and dynamic scene reconstruction to create360-degree immersive virtual environments. For video generation, we introduce a\textbf{Dual-branch Generation Model} consisting of a panorama branch and aperspective branch, responsible for global and local view generation,respectively. A bidirectional cross-attention mechanism facilitatescomprehensive information exchange between the branches. For scenereconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds usingmetric depth maps and initializing scene cameras with estimated poses, ourmethod ensures geometric consistency and temporal coherence for thereconstructed scenes. Extensive experiments demonstrate the effectiveness ofour proposed designs and the superiority of TiP4GEN in generating visuallycompelling and motion-coherent dynamic panoramic scenes. Our project page is athttps://ke-xing.github.io/TiP4GEN/.</description>
      <author>example@mail.com (Ke Xing, Hanwen Liang, Dejia Xu, Yuyang Yin, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei)</author>
      <guid isPermaLink="false">2508.12415v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection</title>
      <link>http://arxiv.org/abs/2508.12330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DoppDrive的新型多普勒驱动时间聚合方法，用于增强雷达点云密度并最小化散射，从而提高自动驾驶中的目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;基于雷达的目标检测对自动驾驶至关重要，因为雷达具有长距离检测能力。然而，雷达点云的稀疏性，特别是在长距离情况下，给准确检测带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决雷达点云稀疏性问题，提高目标检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出DoppDrive方法，通过根据动态多普勒分量径向移动前一帧的点来消除径向散射，并根据点的多普勒和角度分配唯一的聚合持续时间来最小化切向散射。这是一种点云密度增强步骤，在检测前应用，与任何检测器兼容。&lt;h4&gt;主要发现&lt;/h4&gt;DoppDrive显著提高了各种检测器和数据集上的目标检测性能。&lt;h4&gt;结论&lt;/h4&gt;DoppDrive方法有效解决了雷达点云稀疏性问题，提高了目标检测性能。&lt;h4&gt;翻译&lt;/h4&gt;基于雷达的目标检测对自动驾驶至关重要，因为雷达具有长距离检测能力。然而，雷达点云的稀疏性，特别是在长距离情况下，给准确检测带来了挑战。现有方法通过时间聚合和自运动补偿来增加点密度，但这种方法会引入动态物体的散射，降低检测性能。我们提出了DoppDrive，一种新型的多普勒驱动时间聚合方法，增强雷达点云密度的同时最小化散射。根据动态多普勒分量径向移动前一帧的点以消除径向散射，并根据点的多普勒和角度分配唯一的聚合持续时间以最小化切向散射。DoppDrive是一种点云密度增强步骤，在检测前应用，与任何检测器兼容，我们证明了它显著提高了各种检测器和数据集上的目标检测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radar-based object detection is essential for autonomous driving due toradar's long detection range. However, the sparsity of radar point clouds,especially at long range, poses challenges for accurate detection. Existingmethods increase point density through temporal aggregation with ego-motioncompensation, but this approach introduces scatter from dynamic objects,degrading detection performance. We propose DoppDrive, a novel Doppler-Driventemporal aggregation method that enhances radar point cloud density whileminimizing scatter. Points from previous frames are shifted radially accordingto their dynamic Doppler component to eliminate radial scatter, with each pointassigned a unique aggregation duration based on its Doppler and angle tominimize tangential scatter. DoppDrive is a point cloud density enhancementstep applied before detection, compatible with any detector, and we demonstratethat it significantly improves object detection performance across variousdetectors and datasets.</description>
      <author>example@mail.com (Yuval Haitman, Oded Bialer)</author>
      <guid isPermaLink="false">2508.12330v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning For Point Cloud Denoising: A Survey</title>
      <link>http://arxiv.org/abs/2508.11932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对基于深度学习的点云去噪进行了全面综述，识别关键挑战，总结现有方法贡献，并提出针对去噪任务的分类法。&lt;h4&gt;背景&lt;/h4&gt;现实环境中的点云数据通常存在不同模态和强度的噪声，因此点云去噪作为预处理步骤对提高下游任务性能至关重要。&lt;h4&gt;目的&lt;/h4&gt;填补基于深度学习的点云去噪领域缺乏全面综述的空白，识别关键挑战，总结现有方法贡献，并提出针对去噪任务的自定义分类法。&lt;h4&gt;方法&lt;/h4&gt;将点云去噪制定为两步过程：异常值去除和表面噪声恢复，涵盖大多数点云去噪场景和需求，并比较不同方法的相似性、差异和各自优势。&lt;h4&gt;主要发现&lt;/h4&gt;基于深度学习的点云去噪模型因其强大的表示能力和灵活的架构，在去噪性能上已超越传统方法。&lt;h4&gt;结论&lt;/h4&gt;讨论了研究局限性和未来方向，为点云去噪的进一步发展提供见解。&lt;h4&gt;翻译&lt;/h4&gt;现实环境中的点云数据不可避免地存在不同模态和强度的噪声。因此，点云去噪作为预处理步骤对于提高下游任务性能至关重要。基于深度学习的点云去噪模型以其强大的表示能力和灵活的架构，在去噪性能上已超越传统方法。据我们所知，尽管最近在性能上有所进展，但还没有全面的调查系统地总结基于深度学习的点云去噪的发展。为填补这一空白，本文旨在识别基于深度学习的点云去噪的关键挑战，总结现有方法的主要贡献，并提出一个针对去噪任务的自定义分类法。为实现这一目标，我们将点云去噪制定为两步过程：异常值去除和表面噪声恢复，涵盖了大多数点云去噪的场景和需求。此外，我们从相似性、差异和各自优势方面比较了各种方法。最后，我们讨论了研究局限性和未来方向，为点云去噪的进一步发展提供见解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云去噪问题，即如何从含有噪声的点云数据中恢复出干净的表面。这个问题在现实中非常重要，因为现实环境中获取的点云数据不可避免地存在噪声，这些噪声会影响下游任务如点云分类、目标检测、语义分割和表面重建的性能。基于深度学习的点云去噪方法相比传统几何方法具有更好的去噪质量和泛化能力，能有效提高这些下游任务的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云去噪的挑战，将其视为一个两步过程：异常点移除和表面噪声恢复。作者发现现有综述只关注了点云去噪的某些方面，缺乏全面系统的总结。因此，作者提出了一种新的分类法，基于点云去噪的关键挑战对现有方法进行分类。作者借鉴了大量现有工作，包括点基方法（如PointCVaR、TripleMixer）、范围图像基方法（如WeatherNet、Slide）以及表面噪声恢复方法（如GeoDualCNN、PCDNF等），并分析了这些方法之间的内在联系和潜在关系。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云去噪视为一个两步过程：异常点移除和表面噪声恢复。整体实现流程包括：1) 异常点移除阶段，分为点基方法和范围图像基方法；2) 表面噪声恢复阶段，分为边缘感知去噪和精确表面恢复；3) 学习范式，包括监督学习和无监督学习；4) 评估阶段，使用不同的指标评估不同阶段的效果。边缘感知去噪关注保留几何边缘特征，精确表面恢复关注准确估计表面位置。学习范式可以是单独训练或联合训练，评估则根据不同阶段使用不同的指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次系统全面地综述了基于深度学习的点云去噪方法；2) 提出了一种基于点云去噪任务内在特征的新分类法；3) 分析了两个去噪阶段之间的内在联系；4) 在统一基准设置下评估了代表性方法；5) 指出了具体的未来研究方向。相比之前的工作，这篇论文的范围更全面，不仅关注表面噪声恢复，还涵盖了异常点移除；分类更系统，基于任务特征而非技术特征；分析更深入，探索了方法间的内在关系；评估更统一，提供了更全面的性能比较。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统全面地综述了基于深度学习的点云去噪方法，提出了一个涵盖异常点移除和表面噪声恢复的综合框架，并分析了方法间的内在联系，为该领域的研究提供了清晰的路线图。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world environment-derived point clouds invariably exhibit noise acrossvarying modalities and intensities. Hence, point cloud denoising (PCD) isessential as a preprocessing step to improve downstream task performance. Deeplearning (DL)-based PCD models, known for their strong representationcapabilities and flexible architectures, have surpassed traditional methods indenoising performance. To our best knowledge, despite recent advances inperformance, no comprehensive survey systematically summarizes the developmentsof DL-based PCD. To fill the gap, this paper seeks to identify key challengesin DL-based PCD, summarizes the main contributions of existing methods, andproposes a taxonomy tailored to denoising tasks. To achieve this goal, weformulate PCD as a two-step process: outlier removal and surface noiserestoration, encompassing most scenarios and requirements of PCD. Additionally,we compare methods in terms of similarities, differences, and respectiveadvantages. Finally, we discuss research limitations and future directions,offering insights for further advancements in PCD.</description>
      <author>example@mail.com (Chengwei Zhang, Xueyi Zhang, Mingrui Lao, Tao Jiang, Xinhao Xu, Wenjie Li, Fubo Zhang, Longyong Chen)</author>
      <guid isPermaLink="false">2508.11932v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Constructing Invariant and Equivariant Operations by Symmetric Tensor Network</title>
      <link>http://arxiv.org/abs/2508.12596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种系统方法用于构建有效的不变性和等变性操作，适用于几何深度学习中的对称神经网络设计。&lt;h4&gt;背景&lt;/h4&gt;在几何深度学习中，融入对称性的神经网络设计至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发不变性和等变性操作，这是该研究的核心目标。&lt;h4&gt;方法&lt;/h4&gt;提出一种系统方法构建有效的不变性和等变性操作，能处理不同阶的笛卡尔张量和不同类型的球形张量输入输出，并利用对称张量网络提供图形表示，简化相关证明和构造。&lt;h4&gt;主要发现&lt;/h4&gt;该方法成功应用于设计几何图神经网络中的等变性交互消息，以及学习材料本构定律的等变性机器学习模型。&lt;h4&gt;结论&lt;/h4&gt;该系统方法为几何深度学习中的对称性神经网络设计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;在几何深度学习中融入对称性的神经网络设计至关重要。此工作的核心是开发不变性和等变性操作。本文提出了一种构建有效不变性和等变性操作的系统方法。该方法能够处理不同阶的笛卡尔张量形式以及不同类型的球形张量的输入和输出。此外，我们的方法利用对称张量网络提供图形表示，简化了与不变性和等变性函数相关的证明和构造。我们将这种方法应用于设计几何图神经网络中的等变性交互消息，以及学习材料本构定律的等变性机器学习模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何系统地构建不变量和等变操作的问题，以处理不同形式的输入和输出（包括不同秩的笛卡尔张量和不同类型的球谐张量）。这个问题在研究中很重要，因为物理世界中的量通常不依赖于特定坐标系，在坐标变换下表现出不变性或等变性；在机器学习中考虑对称性可以提高数据效率和模型泛化能力，特别是在处理分子、材料等具有3D几何结构的科学问题时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的设计思路基于对称性的数学基础，借鉴了Weyl的经典不变性理论和Hilbert有限性定理；利用了量子多体系统中的对称张量网络表示方法；参考了Malgrange的从不变量导出等变函数的方法；以及球谐张量中的张量积操作。作者首先从简单的向量输入开始，逐步推广到笛卡尔张量和球谐张量，构建了'张量网络生成器'的概念，然后通过计算这些生成器的导数获得等变操作，形成了一个系统化的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用对称张量网络作为表示不变量和等变操作的基础，通过构建针对不同输入形式的'张量网络生成器'来生成所有不变多项式，然后通过计算这些生成器的导数获得对应的等变操作。整体流程包括：1)确定输入和输出形式（笛卡尔张量或球谐张量）；2)构建不变量生成器（对向量使用点积和三重积，对笛卡尔张量使用连接的张量网络与单位张量和Levi-Civita张量的收缩，对球谐张量使用投影算子）；3)通过Tup变换将不变函数转换为等变函数；4)应用于几何图神经网络和材料本构关系学习等具体任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提供统一的框架处理不同形式的输入和输出；2)将量子多体系统中的对称张量网络引入机器学习；3)系统地构建'张量网络生成器'；4)提供明确的Tup变换将不变函数转换为等变函数。相比之前的工作，本文方法不仅适用于向量特征，还能处理更高秩的笛卡尔张量和球谐张量；相比理论不变性构造方法，提供了基于张量网络的具体实现；相比基于群平均的方法，提供了更直观和系统的构造方法；相比现有的张量网络应用，提供了更全面的框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于对称张量网络的系统方法，用于构建处理各种形式输入和输出的不变量和等变操作，为几何深度学习和其他需要对称性约束的机器学习任务提供了统一的数学框架和实现工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Design of neural networks that incorporate symmetry is crucial for geometricdeep learning. Central to this effort is the development of invariant andequivariant operations. This works presents a systematic method forconstructing valid invariant and equivariant operations. It can handle inputsand outputs in the form of Cartesian tensors with different rank, as well asspherical tensors with different types. In addition, our method features agraphical representation utilizing the symmetric tensor network, whichsimplifies both the proofs and constructions related to invariant andequivariant functions. We also apply this approach to design the equivariantinteraction message for the geometry graph neural network, and equivariantmachine learning model to learn the constitutive law of materials.</description>
      <author>example@mail.com (Meng Zhang, Chao Wang, Hao Zhang, Shaojun Dong, Lixin He)</author>
      <guid isPermaLink="false">2508.12596v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos</title>
      <link>http://arxiv.org/abs/2508.14041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://linjohnss.github.io/longsplat/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongSplat是一个鲁棒的未定位3D高斯飞溅框架，用于解决从随意拍摄的长视频中合成新视角的挑战，通过增量联合优化、鲁棒姿态估计模块和高效八叉树锚点形成机制，实现了最先进的渲染质量、姿态准确性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;当前从具有不规则相机运动、未知相机姿态和广阔场景的随意拍摄长视频中合成新视角的方法面临姿态漂移、不准确的几何初始化和严重的内存限制等问题。&lt;h4&gt;目的&lt;/h4&gt;解决长视频新视角合成中的关键挑战，包括不规则相机运动、未知相机姿态和广阔场景。&lt;h4&gt;方法&lt;/h4&gt;LongSplat引入了三个关键组件：(1)增量联合优化，同时优化相机姿态和3D高斯；(2)利用学习3D先验的鲁棒姿态估计模块；(3)基于空间密度将密集点云转换为锚点的高效八叉树锚点形成机制。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的基准测试中，LongSplat实现了最先进的结果，显著提高了渲染质量、姿态准确性和计算效率。&lt;h4&gt;结论&lt;/h4&gt;LongSplat是一个有效的框架，能够处理随意拍摄长视频中的新视角合成问题，解决了当前方法中的多个限制。&lt;h4&gt;翻译&lt;/h4&gt;LongSplat解决了从随意拍摄的长视频中合成新视角的关键挑战，这些视频具有不规则的相机运动、未知的相机姿态和广阔的场景。当前方法常常遭受姿态漂移、不准确的几何初始化和严重的内存限制。为了解决这些问题，我们引入了LongSplat，一个鲁棒的未定位3D高斯飞溅框架，具有：(1)增量联合优化，同时优化相机姿态和3D高斯，以避免局部最小值并确保全局一致性；(2)利用学习3D先验的鲁棒姿态估计模块；以及(3)基于空间密度将密集点云转换为锚点的高效八叉树锚点形成机制。在具有挑战性的基准上的广泛实验表明，LongSplat实现了最先进的结果，相比之前的方法显著提高了渲染质量、姿态准确性和计算效率。项目页面：https://linjohnss.github.io/longsplat/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从随意拍摄的长视频中生成新视角的挑战，这类视频具有不规则相机运动、未知相机姿态和广阔场景的特点。这个问题在现实中很重要，因为随着智能手机普及，随意拍摄的长视频已成为3D内容的重要来源，而传统方法在处理这类视频时面临姿态漂移、几何初始化不准确和内存限制等问题。解决这一问题对于虚拟现实、增强现实、虚拟旅游、文化遗产保护等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前方法在处理随意拍摄长视频时的局限性，如COLMAP失败、CF-3DGS内存不足、LocalRF轨迹复杂时重建碎片化等。设计思路是创建一个统一框架联合优化相机姿态和3D高斯溅射，结合基于对应关系的姿态估计和自适应八锚树机制。作者借鉴了Scaffold-GS的锚点表示但改进了其依赖SfM的局限，使用MASt3R作为3D先验但通过联合优化修正，受LocalRF的渐进式构建启发但改进了不规则运动下的性能，并融合了PnP算法与光度对齐提升姿态估计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增量联合优化相机姿态和3D高斯溅射，解决随意拍摄长视频中的新视角合成问题，将姿态估计与场景重建紧密结合，并使用自适应八锚树机制减少内存使用。整体流程包括：1)初始化阶段用MASt3R估计初始帧并形成八锚树；2)全局优化阶段联合优化所有帧的高斯和姿态；3)帧插入阶段对新帧估计姿态并处理失败情况；4)局部优化阶段优化新帧可见高斯并利用可见性自适应窗口；5)最终全局细化阶段提高整体质量和一致性；6)使用光度、深度和重投影损失函数确保准确性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)增量联合优化同时优化相机姿态和3D高斯；2)鲁棒姿态估计模块结合PnP和光度细化；3)自适应八锚树形成机制减少内存使用。相比不同工作：1)与传统COLMAP方法不同，无需SfM预处理；2)与CF-3DGS不同，解决了内存限制问题；3)与LocalRF不同，处理复杂轨迹时不会碎片化；4)与MASt3R+Scaffold-GS不同，不依赖固定姿态估计；5)与其他锚点方法不同，使用动态调整体素大小而非固定分辨率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LongSplat通过增量联合优化相机姿态和3D高斯溅射，结合自适应八锚树机制，解决了随意拍摄长视频中未知相机姿态下的新视角合成问题，实现了高质量、内存高效的场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LongSplat addresses critical challenges in novel view synthesis (NVS) fromcasually captured long videos characterized by irregular camera motion, unknowncamera poses, and expansive scenes. Current methods often suffer from posedrift, inaccurate geometry initialization, and severe memory limitations. Toaddress these issues, we introduce LongSplat, a robust unposed 3D GaussianSplatting framework featuring: (1) Incremental Joint Optimization thatconcurrently optimizes camera poses and 3D Gaussians to avoid local minima andensure global consistency; (2) a robust Pose Estimation Module leveraginglearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism thatconverts dense point clouds into anchors based on spatial density. Extensiveexperiments on challenging benchmarks demonstrate that LongSplat achievesstate-of-the-art results, substantially improving rendering quality, poseaccuracy, and computational efficiency compared to prior approaches. Projectpage: https://linjohnss.github.io/longsplat/</description>
      <author>example@mail.com (Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu)</author>
      <guid isPermaLink="false">2508.14041v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing</title>
      <link>http://arxiv.org/abs/2508.13797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Sketch3DVE是一种基于草图的3D感知视频编辑方法，能够处理具有重大视角变化的视频，实现详细的局部3D场景编辑。&lt;h4&gt;背景&lt;/h4&gt;现有的视频编辑方法在风格迁移或外观修改方面取得了良好效果，但在处理视频中的3D场景结构内容编辑时仍面临挑战，特别是当涉及重大视角变化（如大角度相机旋转或缩放）时。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够实现对具有重大视角变化视频的详细局部操作，解决生成新视角内容、保持未编辑区域和将稀疏2D输入转换为逼真3D视频输出等挑战。&lt;h4&gt;方法&lt;/h4&gt;1) 使用图像编辑方法为第一帧生成编辑结果并传播到其余帧；2) 利用素描作为精确几何控制的交互工具；3) 通过密集立体方法估计输入视频的点云和相机参数；4) 提出使用深度图表示新编辑组件3D几何的点云编辑方法；5) 引入3D感知掩码传播策略并使用视频扩散模型生成逼真编辑视频。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明Sketch3DVE在视频编辑方面具有优越性，能够有效处理视角变化并保持编辑内容与原始视频的一致性。&lt;h4&gt;结论&lt;/h4&gt;Sketch3DVE成功解决了视频编辑中处理重大视角变化的挑战，实现了对3D场景结构的详细编辑，同时保持未编辑区域的特征不变。&lt;h4&gt;翻译&lt;/h4&gt;最近视频编辑方法在风格迁移或外观修改方面取得了吸引人的结果。然而，编辑视频中3D场景的结构内容仍然具有挑战性，特别是在处理重大视角变化时，如大角度相机旋转或缩放。主要挑战包括生成与原始视频保持一致的新视角内容、保持未编辑区域以及将稀疏2D输入转换为逼真的3D视频输出。为解决这些问题，我们提出了Sketch3DVE，一种基于草图的3D感知视频编辑方法，以实现对具有重大视角变化视频的详细局部操作。为解决稀疏输入带来的挑战，我们采用图像编辑方法为第一帧生成编辑结果，然后将其传播到视频的其余帧。我们利用素描作为精确几何控制的交互工具，同时也支持其他基于掩码的图像编辑方法。为处理视角变化，我们对视频中的3D信息进行了详细分析和操作。具体来说，我们使用密集立体方法估计输入视频的点云和相机参数。然后，我们提出了一种使用深度图表示新编辑组件3D几何的点云编辑方法，将其与原始3D场景有效对齐。为将新编辑内容与原始视频无缝合并，同时保持未编辑区域的特征，我们引入了3D感知掩码传播策略，并采用视频扩散模型生成逼真的编辑视频。大量实验证明了Sketch3DVE在视频编辑方面的优越性。主页和代码：http://geometrylearning.com/Sketch3DVE/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决对具有显著视角变化的3D场景视频进行结构编辑的问题，特别是在处理大相机旋转或缩放时如何生成与原始视角一致的新内容。这个问题很重要，因为现有视频编辑方法主要针对风格转换或外观修改，难以处理3D场景的结构内容编辑；在未编辑区域保持不变的同时，将稀疏的2D输入（如草图）转换为真实的3D视频输出是一项重大挑战；这种技术在电影制作、教育、机器人、AR/VR等领域有广泛的应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视频编辑方法的局限性，发现它们难以处理有显著视角变化的视频；注意到图像编辑方法在灵活性方面表现良好，但直接应用于视频编辑存在挑战；观察到扩散模型在视频生成方面的进展，但发现它们主要处理时间运动信息，缺乏对3D信息的广泛分析；考虑到3D信息对于处理视角变化的重要性，提出了基于点云的3D表示方法；为了解决2D草图与视频之间的领域差距，先编辑第一帧，然后将编辑效果传播到视频的其他帧。该方法借鉴了MagicQuill用于第一帧编辑，DUSt3R用于获取点云和相机参数，CogVideoX用于视频生成，以及ControlNet用于有效编辑和保持未编辑区域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：使用草图作为交互工具进行精确几何控制；通过显式分析和操作视频中的3D信息来处理视角变化；利用深度图表示和编辑3D几何，确保编辑区域与原始3D场景对齐；提出一种3D感知的掩码传播策略，准确识别和保留未编辑区域。整体实现流程包括：1) 第一帧编辑：使用图像编辑方法根据用户输入的草图、掩码和文本提示编辑视频的第一帧；2) 3D信息提取：从输入视频中获取点云和相机参数；3) 点云编辑：利用深度图表示和编辑区域的3D几何，通过变换将编辑内容与原始场景对齐；4) 掩码传播：构建3D掩码并将其渲染到所有帧中；5) 视频生成：使用视频扩散模型结合点云渲染结果、原始视频和传播的掩码生成最终编辑视频。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了一种基于草图和3D感知的视频编辑方法，能够处理具有显著视角变化的视频；2) 提出了一种基于深度图表示的点云编辑方法，有效将2D图像编辑映射到3D空间；3) 开发了一种3D感知的掩码传播策略，能够准确识别和保留未编辑区域；4) 设计了一个精确的区域修改视频扩散模型，能够合成编辑组件的新视角结果。相比之前工作的不同：与现有视频编辑方法相比，Sketch3DVE能够处理结构编辑（如组件插入和替换），而不仅仅是外观修改；与相机可控的视频生成方法相比，Sketch3DVE能够更好地保持未编辑区域，并生成更一致的编辑结果；与基于草图的内容编辑方法相比，Sketch3DVE专门针对3D场景视频，能够处理显著视角变化；通过显式处理3D信息，Sketch3DVE能够在视角变化时保持编辑内容的几何一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Sketch3DVE通过结合草图交互、3D点云分析和视频扩散模型，实现了对具有显著视角变化的3D场景视频的高质量编辑，能够精确控制几何形状并保持未编辑区域的完整性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent video editing methods achieve attractive results in style transfer orappearance modification. However, editing the structural content of 3D scenesin videos remains challenging, particularly when dealing with significantviewpoint changes, such as large camera rotations or zooms. Key challengesinclude generating novel view content that remains consistent with the originalvideo, preserving unedited regions, and translating sparse 2D inputs intorealistic 3D video outputs. To address these issues, we propose Sketch3DVE, asketch-based 3D-aware video editing method to enable detailed localmanipulation of videos with significant viewpoint changes. To solve thechallenge posed by sparse inputs, we employ image editing methods to generateedited results for the first frame, which are then propagated to the remainingframes of the video. We utilize sketching as an interaction tool for precisegeometry control, while other mask-based image editing methods are alsosupported. To handle viewpoint changes, we perform a detailed analysis andmanipulation of the 3D information in the video. Specifically, we utilize adense stereo method to estimate a point cloud and the camera parameters of theinput video. We then propose a point cloud editing approach that uses depthmaps to represent the 3D geometry of newly edited components, aligning themeffectively with the original 3D scene. To seamlessly merge the newly editedcontent with the original video while preserving the features of uneditedregions, we introduce a 3D-aware mask propagation strategy and employ a videodiffusion model to produce realistic edited videos. Extensive experimentsdemonstrate the superiority of Sketch3DVE in video editing. Homepage and code:http://http://geometrylearning.com/Sketch3DVE/</description>
      <author>example@mail.com (Feng-Lin Liu, Shi-Yang Li, Yan-Pei Cao, Hongbo Fu, Lin Gao)</author>
      <guid isPermaLink="false">2508.13797v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot</title>
      <link>http://arxiv.org/abs/2508.13785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为'DIPPeR'的自主矿场检测机器人，用于露天矿爆破孔的内部检查，通过自动化方法解决人工检查的局限性，实现精确的孔下传感器定位和目标导航。&lt;h4&gt;背景&lt;/h4&gt;在露天矿开采中，爆破孔需要内部检查以研究孔内材料类型和性质。传统的人工检查方法缓慢、昂贵，且在揭示孔及其内容物的几何和地质特性方面存在显著局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个自主矿场检测机器人系统，实现爆破孔的自动寻找、检测和精确的孔下传感器定位，以提高检查效率并降低成本。&lt;h4&gt;方法&lt;/h4&gt;提出一个稳健的爆破孔寻找和检测框架，处理LiDAR传感器收集的点云数据，提取地面上的锥形钻废物体积，通过将3D点投影到虚拟深度图像进行2D分割，识别孔中心并抑制非最大候选值，确保精确传感器放置和避免碰撞。系统在机器人导航过程中自动调整投影参数以适应不同条件，实现连续的目标跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;导航和感知系统在高保真模拟环境和现场测试中均表现出有效性，能够准确识别和定位爆破孔，为后续孔下检查提供精确位置信息。&lt;h4&gt;结论&lt;/h4&gt;DIPPeR机器人系统能够有效解决露天矿爆破孔检查的难题，通过自动化方法提高了检查效率和准确性，有望显著降低材料处理成本。&lt;h4&gt;翻译&lt;/h4&gt;在露天矿开采中，需要在爆破场地表面钻孔并使用炸药进行爆破以促进挖掘。这些爆破孔需要内部检查以研究孔内材料类型和性质。了解这些性质可以显著降低下游过程中的材料处理成本。人工孔检查缓慢且昂贵，在揭示孔及其内容物的几何和地质特性方面存在主要局限性。这促使我们开发了自主矿场检测机器人——'DIPPeR'。本文解释了该项目的自动化方面。我们提出了一个稳健的爆破孔寻找和检测框架，实现基于目标的导航和精确的孔下传感器定位。该流程首先处理机载LiDAR传感器收集的点云数据，提取地面上的锥形钻废物体积。通过将3D锥形点投影到虚拟深度图像中，在2D域实现分割，得到图像中心的圆形孔和带领锥面。然后我们使用稳健的检测模块识别孔中心，同时抑制非最大候选值，确保精确的传感器放置以进行孔下检查，并避免与孔壁碰撞。为了实现自主孔寻找，流程在机器人导航过程中自动调整其投影参数，以适应点稀疏性和孔开口尺寸的变化，确保2D图像中孔的一致外观。这允许机器人接近目标点时连续跟踪目标孔。我们在高保真模拟环境和现场测试中展示了导航和感知系统的有效性。演示视频可在'https://www.youtube.com/watch?v=fRNbcBcaSqE'观看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决露天矿场爆破孔的内部检测问题。人工检查爆破孔既缓慢又昂贵，且无法全面揭示孔的几何和地质特性。这个问题重要是因为了解孔内材料特性可显著降低下游材料处理成本，同时矿场环境对人类操作者存在安全风险和极端天气条件，自动化检测能提高效率和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于矿场环境的特殊挑战（GPS定位不足、环境恶劣、孔的几何特性）设计了方法。选择LiDAR而非相机，因其对光照变化更具鲁棒性。借鉴了Paul等人(2016)和Vu等人(2019)的点云投影方法，以及Liu等人(2022a)的最优圆拟合算法。同时，作者利用矿场爆破孔的规则布局和GPS可靠性，设计了基于接近度的自适应导航策略，避免了昂贵的地图构建过程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用矿场结构化环境的先验知识和LiDAR感知能力，实现从粗略GPS导航到精确相对定位的平滑过渡，并通过自适应参数调整确保孔检测的一致性。整体流程包括：1)导航阶段：使用GPS导航到大致位置，检测到孔后切换到局部坐标系进行精细导航，接近时使用机器人坐标系进行视觉伺服；2)感知阶段：校正地面倾斜，检测锥形钻屑堆，将3D点云投影到2D虚拟图像，通过粗检测确定孔的大致位置，再通过精检测确定精确位置；3)执行阶段：精确对准后进行孔内检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自适应感知参数调整，根据距离自动调整虚拟相机参数；2)两阶段检测流程，从粗略到精细定位；3)非最大值抑制方法，有效消除'幽灵孔'；4)基于接近度的自适应导航策略，利用多坐标系解耦定位；5)基于经典计算机视觉的方法，避免数据驱动学习的需求。相比Valencia等人(2024)的HOG+SVM方法，本文方法不需要大量数据标注且圆拟合更准确；相比Paul等人(2016)和Vu等人(2019)，增加了自适应参数调整和两阶段流程；相比Kiran等人(2024)，专门针对深孔爆破检测设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于LiDAR的自适应爆破孔检测与导航框架，通过结合结构化环境的先验知识和两阶段检测流程，实现了矿场爆破孔的高精度自主检测，无需昂贵的地图构建和大量数据训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In open-pit mining, holes are drilled into the surface of the excavation siteand detonated with explosives to facilitate digging. These blast holes need tobe inspected internally for investigation of downhole material types andproperties. Knowing these properties can lead to significant savings inmaterial handling costs in downstream processes. Manual hole inspection is slowand expensive, with major limitations in revealing the geometric and geologicalproperties of the holes and their contents. This has been the motivation forthe development of our autonomous mine-site inspection robot - "DIPPeR". Inthis paper, the automation aspect of the project is explained. We present arobust blast hole seeking and detection framework that enables target-basednavigation and accurate down-hole sensor positioning. The pipeline firstprocesses point-cloud data collected by the on-board LiDAR sensors, extractingthe cone-shaped volume of drill-waste above the ground. By projecting the 3Dcone points into a virtual depth image, segmentation is achieved in the 2Ddomain, yielding a circular hole at the image centre and a collared cone face.We then identify the hole centre using a robust detection module whilesuppressing non-maximum candidates, ensuring precise sensor placement fordown-hole inspection and avoiding collisions with the cavity wall. To enableautonomous hole-seeking, the pipeline automatically adjusts its projectionparameters during robot navigation to account for variations in point sparsityand hole opening size, ensuring a consistent hole appearance in 2D images. Thisallows continuous tracking of the target hole as the robot approaches the goalpoint. We demonstrate the effectiveness of our navigation and perception systemin both high-fidelity simulation environments and on-site field tests. Ademonstration video is available at"https://www.youtube.com/watch?v=fRNbcBcaSqE".</description>
      <author>example@mail.com (Liyang Liu, Ehsan Mihankhah, Nathan Wallace, Javier Martinez, Andrew J. Hill)</author>
      <guid isPermaLink="false">2508.13785v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video</title>
      <link>http://arxiv.org/abs/2508.13756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM  International Conference on Multimedia (MM '25), October 27--31, 2025,  Dublin, Ireland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;点云视频实时流传输面临数据量大和丢包敏感的挑战，INDS框架通过基于信息中心网络的自适应流媒体技术解决了传统协议的局限性，实现了更高效的传输和缓存。&lt;h4&gt;背景&lt;/h4&gt;点云视频实时流传输具有数据量大和对丢包敏感的特点，在动态网络条件下对沉浸式应用仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;引入INDS（增量命名数据流），一个基于信息中心网络的自适应流媒体框架，重新思考分层分层媒体的传输方式。&lt;h4&gt;方法&lt;/h4&gt;INDS利用点云视频的八叉树结构和富有表现力的内容命名，支持基于消费者带宽和解码能力的渐进式部分检索增强层。通过将时间窗口与帧组（GoF）相结合，INDS的命名方案支持细粒度的网络内缓存，并促进高效的多用户数据重用。&lt;h4&gt;主要发现&lt;/h4&gt;INDS可作为叠加层部署，与基于QUIC的传输基础设施以及未来的媒体 over QUIC（MoQ）架构兼容，无需更改底层IP网络。原型实现表明，与最先进的DASH风格系统相比，INDS可降低高达80%的延迟，提高15-50%的吞吐量，并增加20-30%的缓存命中率。&lt;h4&gt;结论&lt;/h4&gt;这些结果共同确立了INDS作为在多变和有损条件下实时点云流的可扩展、缓存友好解决方案，同时它与MoQ叠加层的兼容性进一步将其定位为新兴沉浸式媒体系统的实用、向前兼容的架构。&lt;h4&gt;翻译&lt;/h4&gt;点云视频的实时流传输，以其巨大的数据量和对丢包的高敏感性，在动态网络条件下仍然是沉浸式应用的一个关键挑战。虽然像TCP这样的面向连接协议以及QUIC等更现代的替代方案缓解了一些传输层的低效率，包括头部阻塞，但它们仍然保留了粗粒度的基于段的传输模型和集中式控制循环，这限制了细粒度的适应性和有效缓存。我们引入了INDS（增量命名数据流），这是一个基于信息中心网络的自适应流媒体框架，重新思考了分层分层媒体的传输方式。INDS利用点云视频的八叉树结构和富有表现力的内容命名，支持基于消费者带宽和解码能力的渐进式部分检索增强层。通过将时间窗口与帧组（GoF）相结合，INDS的命名方案支持细粒度的网络内缓存，并促进高效的多用户数据重用。INDS可以作为叠加层部署，与基于QUIC的传输基础设施以及未来的媒体 over QUIC（MoQ）架构兼容，而无需更改底层IP网络。我们的原型实现表明，与最先进的DASH风格系统相比，INDS可降低高达80%的延迟，提高15-50%的吞吐量，并增加20-30%的缓存命中率。这些结果共同确立了INDS作为在多变和有损条件下实时点云流的可扩展、缓存友好解决方案，同时它与MoQ叠加层的兼容性进一步将其定位为新兴沉浸式媒体系统的实用、向前兼容的架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time streaming of point cloud video, characterized by massive datavolumes and high sensitivity to packet loss, remains a key challenge forimmersive applications under dynamic network conditions. Whileconnection-oriented protocols such as TCP and more modern alternatives likeQUIC alleviate some transport-layer inefficiencies, including head-of-lineblocking, they still retain a coarse-grained, segment-based delivery model anda centralized control loop that limit fine-grained adaptation and effectivecaching. We introduce INDS (Incremental Named Data Streaming), an adaptivestreaming framework based on Information-Centric Networking (ICN) that rethinksdelivery for hierarchical, layered media. INDS leverages the Octree structureof point cloud video and expressive content naming to support progressive,partial retrieval of enhancement layers based on consumer bandwidth anddecoding capability. By combining time-windows with Group-of-Frames (GoF),INDS's naming scheme supports fine-grained in-network caching and facilitatesefficient multi-user data reuse. INDS can be deployed as an overlay, remainingcompatible with QUIC-based transport infrastructure as well as futureMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IPnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%higher throughput, and 20-30% increased cache hit rates compared tostate-of-the-art DASH-style systems. Together, these results establish INDS asa scalable, cache-friendly solution for real-time point cloud streaming undervariable and lossy conditions, while its compatibility with MoQ overlaysfurther positions it as a practical, forward-compatible architecture foremerging immersive media systems.</description>
      <author>example@mail.com (Ruonan Chai, Yixiang Zhu, Xinjiao Li, Jiawei Li, Zili Meng, Dirk Kutscher)</author>
      <guid isPermaLink="false">2508.13756v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.13485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, Accepted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CORENet是一种新颖的跨模态去噪框架，用于解决4D雷达点云稀疏和噪声特性带来的物体检测挑战&lt;h4&gt;背景&lt;/h4&gt;4D雷达物体检测因其在不利天气条件下的鲁棒性和在不同驾驶场景中提供丰富空间信息的能力而受到广泛关注&lt;h4&gt;目的&lt;/h4&gt;解决4D雷达点云的稀疏和噪声特性对有效感知构成的挑战&lt;h4&gt;方法&lt;/h4&gt;提出CORENet，利用LiDAR监督来识别噪声模式并从原始4D雷达数据中提取判别性特征&lt;h4&gt;主要发现&lt;/h4&gt;在具有高噪声水平的Dual-Radar数据集上评估，证明CORENet能有效增强检测鲁棒性&lt;h4&gt;结论&lt;/h4&gt;CORENet与现有主流方法相比具有优越的性能&lt;h4&gt;翻译&lt;/h4&gt;基于4D雷达的物体检测因其在不利天气条件下的鲁棒性以及在各种驾驶场景中提供丰富空间信息的能力而受到广泛关注。然而，4D雷达点云的稀疏和噪声特性对有效感知构成了重大挑战。为解决这一限制，我们提出了CORENet，一种新颖的跨模态去噪框架，利用LiDAR监督来识别噪声模式并从原始4D雷达数据中提取判别性特征。作为即插即用架构设计，我们的解决方案能够无缝集成到基于体素的检测框架中，而无需修改现有流程。值得注意的是，所提出的方法仅在训练期间使用LiDAR数据进行跨模态监督，而在推理期间保持完全的雷达-only操作。在具有高噪声水平的具有挑战性的Dual-Radar数据集上进行的大量评估证明了我们的框架在增强检测鲁棒性方面的有效性。全面的实验验证了CORENet与现有主流方法相比具有优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D radar-based object detection has garnered great attention for itsrobustness in adverse weather conditions and capacity to deliver rich spatialinformation across diverse driving scenarios. Nevertheless, the sparse andnoisy nature of 4D radar point clouds poses substantial challenges foreffective perception. To address the limitation, we present CORENet, a novelcross-modal denoising framework that leverages LiDAR supervision to identifynoise patterns and extract discriminative features from raw 4D radar data.Designed as a plug-and-play architecture, our solution enables seamlessintegration into voxel-based detection frameworks without modifying existingpipelines. Notably, the proposed method only utilizes LiDAR data forcross-modal supervision during training while maintaining full radar-onlyoperation during inference. Extensive evaluation on the challenging Dual-Radardataset, which is characterized by elevated noise level, demonstrates theeffectiveness of our framework in enhancing detection robustness. Comprehensiveexperiments validate that CORENet achieves superior performance compared toexisting mainstream approaches.</description>
      <author>example@mail.com (Fuyang Liu, Jilin Mei, Fangyuan Mao, Chen Min, Yan Xing, Yu Hu)</author>
      <guid isPermaLink="false">2508.13485v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>UNICON: UNIfied CONtinual Learning for Medical Foundational Models</title>
      <link>http://arxiv.org/abs/2508.14024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UNICON框架，使医学基础模型能够无缝适应不同领域、任务和模态，解决了医学影像数据稀缺导致的预训练挑战。&lt;h4&gt;背景&lt;/h4&gt;基础模型通常在大型数据集上训练以捕捉领域普遍趋势，但医学影像领域数据稀缺，难以针对每个领域、模态或任务进行预训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一且可扩展的框架，使医学基础模型能够动态适应不同影像模态、解剖区域和临床目标，避免灾难性遗忘和任务干扰。&lt;h4&gt;方法&lt;/h4&gt;提出UNICON（UNIfied CONtinual Learning for Medical Foundational Models）框架，通过持续学习在不同领域或任务上顺序微调模型，而非孤立处理变化。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可动态扩展跨越影像模态、解剖区域和临床目标；将胸部CT基础模型从分类任务适应到预后和分割任务，性能均得到提升；整合PET扫描后，Dice分数较基线提高5%。&lt;h4&gt;结论&lt;/h4&gt;医学基础模型并非固有受限于初始训练范围，而是可以持续发展，为医学影像领域的通用AI模型开辟了道路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在大型数据集上训练，以捕捉领域的普遍趋势。然而，在医学影像领域，数据稀缺使得为每个领域、模态或任务进行预训练具有挑战性。持续学习通过在不同领域或任务上顺序微调模型提供了解决方案，使其能够整合新知识，而不需要每个训练阶段都有大型数据集。在本文中，我们提出了UNICON（UNIfied CONtinual Learning for Medical Foundational Models）框架，使基础模型能够无缝适应不同的领域、任务和模态。与将变化孤立对待的传统适应方法不同，UNICON提供了一个统一且可无限扩展的框架。通过精心整合，我们证明基础模型可以动态扩展跨越影像模态、解剖区域和临床目标，而不会发生灾难性遗忘或任务干扰。通过实验，我们验证了将最初为分类任务训练的胸部CT基础模型适应到预后和分割任务的方法。我们的结果显示在两个额外任务上性能均有提升。此外，我们持续整合PET扫描，实现了与相应基线相比5%的Dice分数提升。这些发现确立了基础模型并非固有地受限于其初始训练范围，而是可以发展的，为医学影像领域的通用AI模型铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学基础模型如何持续适应不同影像模态、任务和解剖区域的问题，而不会出现灾难性遗忘。这个问题在现实中非常重要，因为医学数据稀缺且获取困难，而医学影像又具有多样性（多种模态、任务和身体部位），传统方法需要为每种情况单独训练模型，既不经济也不现实。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了医学基础模型面临的挑战，然后借鉴了持续学习、领域适应和LoRA等技术，设计了UNICON框架。作者通过两种适应机制（模型内适应WMA和模型后适应PMA）来解决参数效率和知识保留问题，并添加了分辨率适应模块来处理不同分辨率的医学影像。实验中，作者逐步将模型从单一分类任务扩展到多模态分割，验证了方法的有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过轻量级适配器扩展基础模型功能，而不修改其核心参数，实现持续学习。整体流程：1)使用预训练的医学基础模型并冻结其参数；2)根据新任务添加特定适配器（如预后预测的MLP适配器或分割任务的解码器）；3)使用LoRA技术微调部分参数；4)动态添加补丁嵌入层适应不同分辨率；5)按顺序逐步适应新任务、模态和区域，每个新任务添加新适配器而不修改已有模块。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的持续学习框架，同时处理跨任务、跨模态和跨解剖区域的适应；2)参数高效的适应方法，使用轻量级适配器而非全模型微调；3)分辨率自适应能力，处理不同分辨率的医学影像；4)多模态融合机制，有效整合不同影像模态。相比之前工作，UNICON范围更广（多维度适应）、更灵活（动态适应新任务）、更高效（低计算资源需求）且更全面（解决分辨率问题）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UNICON提出了一种统一的持续学习框架，使医学基础模型能够高效地适应多种医学影像任务、模态和解剖区域，在医疗数据稀缺的情况下实现了模型的多功能扩展和应用，避免了灾难性遗忘。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models are trained on extensive datasets to capture the generaltrends of a domain. However, in medical imaging, the scarcity of data makespre-training for every domain, modality, or task challenging. Continuallearning offers a solution by fine-tuning a model sequentially on differentdomains or tasks, enabling it to integrate new knowledge without requiringlarge datasets for each training phase. In this paper, we propose UNIfiedCONtinual Learning for Medical Foundational Models (UNICON), a framework thatenables the seamless adaptation of foundation models to diverse domains, tasks,and modalities. Unlike conventional adaptation methods that treat these changesin isolation, UNICON provides a unified, perpetually expandable framework.Through careful integration, we show that foundation models can dynamicallyexpand across imaging modalities, anatomical regions, and clinical objectiveswithout catastrophic forgetting or task interference. Empirically, we validateour approach by adapting a chest CT foundation model initially trained forclassification to a prognosis and segmentation task. Our results show improvedperformance across both additional tasks. Furthermore, we continuallyincorporated PET scans and achieved a 5\% improvement in Dice score compared torespective baselines. These findings establish that foundation models are notinherently constrained to their initial training scope but can evolve, pavingthe way toward generalist AI models for medical imaging.</description>
      <author>example@mail.com (Mohammad Areeb Qazi, Munachiso S Nwadike, Ibrahim Almakky, Mohammad Yaqub, Numan Saeed)</author>
      <guid isPermaLink="false">2508.14024v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.13977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度估计是自动驾驶、机器人和增强现实领域中3D场景理解的基础任务。现有数据集如KITTI、nuScenes和DDAD虽推动了该领域发展，但在多样性和可扩展性方面存在局限。作者提出一个大规模、多样化的帧级连续深度估计数据集，包含20K视频帧，采用轻量级采集流程确保低成本下的广泛场景覆盖，同时提供稀疏但统计充分的真实数据支持鲁棒训练。&lt;h4&gt;背景&lt;/h4&gt;深度估计是自动驾驶、机器人和增强现实领域中3D场景理解的基础任务。现有数据集如KITTI、nuScenes和DDAD推动了该领域发展，但在多样性和可扩展性方面存在局限。随着这些数据集上的基准性能接近饱和，需要新一代大规模、多样化且成本高效的数据集来支持基础模型和多模态学习时代。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有深度估计数据集的多样性不足和可扩展性有限的问题，作者旨在开发一个大规模、多样化、帧级连续的深度估计数据集，用于支持基础模型和多模态学习时代。&lt;h4&gt;方法&lt;/h4&gt;作者介绍了一个大规模、多样化、帧级连续的深度估计数据集，包含20K视频帧。他们采用轻量级采集流程确保低成本下的广泛场景覆盖，同时提供稀疏但统计充分的真实数据以支持鲁棒训练。&lt;h4&gt;主要发现&lt;/h4&gt;与现有数据集相比，新数据集在驾驶场景方面具有更高的多样性，且深度密度更低，为泛化能力带来新挑战。基准实验使用标准单目深度估计模型验证了该数据集的实用性，并突显了在具有挑战性条件下的显著性能差距。&lt;h4&gt;结论&lt;/h4&gt;该数据集为推进深度估计研究建立了新平台，有助于推动该领域的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;深度估计是自动驾驶、机器人技术和增强现实领域中3D场景理解的基本任务。现有的深度数据集，如KITTI、nuScenes和DDAD，虽然推动了该领域的发展，但在多样性和可扩展性方面存在局限性。随着这些数据集上的基准性能趋于饱和，迫切需要新一代大规模、多样化且成本高效的数据集，以支持基础模型和多模态学习时代。为应对这些挑战，我们介绍了一个用于动态户外驾驶环境中深度估计的大规模、多样化、帧级连续数据集，包含20K视频帧用于评估现有方法。我们的轻量级采集流程确保了低成本下的广泛场景覆盖，而稀疏但统计充分的真实数据则支持鲁棒训练。与现有数据集相比，我们的数据集在驾驶场景方面具有更高的多样性，且深度密度更低，为泛化能力创造了新的挑战。使用标准单目深度估计模型进行的基准实验验证了该数据集的实用性，并突显了在具有挑战性条件下的显著性能差距，为推进深度估计研究建立了新平台。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有深度估计数据集（如KITTI、nuScenes、DDAD）在多样性和可扩展性方面的局限性。这个问题很重要，因为深度估计是自动驾驶、机器人和增强现实等领域3D场景理解的基础任务，准确深度地图支持障碍物检测、运动规划等关键功能。随着深度神经网络和基础模型的发展，现有数据集性能接近饱和，需要新的大规模、多样化数据集来推动研究进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，包括场景多样性不足、标注密度有限和成本高昂等问题。他们认识到随着模型容量增加，现有基准性能接近饱和，需要新数据源。设计上借鉴了现有数据集的多传感器采集方法、数据组织格式和评估指标，但在采集流程设计上采用轻量级方案，确保广泛场景覆盖和低成本，同时提供稀疏但统计上足够的地面真实数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多样化、帧级连续的深度估计数据集，通过轻量级采集流程实现低成本广泛场景覆盖，提供稀疏但统计上足够的地面真实数据。整体流程包括：1)多车辆采集系统，配备激光雷达、RGB相机、GNSS和IMU；2)数据以ROS 2 bag格式组织，包含同步的多模态数据；3)多传感器精确校准和同步；4)覆盖高速公路、乡村和城市场景，包括正常、夜间和雨天条件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)更大规模和更高多样性的数据集(193,648帧)；2)轻量级低成本采集流程；3)稀疏但统计上足够的地面真实数据；4)完全可扩展的数据集设计；5)为模型带来新的泛化挑战。相比之前工作，ROVR数据集规模更大(比KITTI大2倍以上)，场景更多样(包含夜间和雨天)，成本效益更高，且深度GT密度更低，对模型泛化能力提出更高要求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ROVR数据集通过大规模、多样化的深度估计基准和轻量级采集流程，为自动驾驶领域的深度估计研究提供了新的挑战和机遇，推动了更具泛化能力的深度估计方法的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth estimation is a fundamental task for 3D scene understanding inautonomous driving, robotics, and augmented reality. Existing depth datasets,such as KITTI, nuScenes, and DDAD, have advanced the field but suffer fromlimitations in diversity and scalability. As benchmark performance on thesedatasets approaches saturation, there is an increasing need for a newgeneration of large-scale, diverse, and cost-efficient datasets to support theera of foundation models and multi-modal learning. To address these challenges,we introduce a large-scale, diverse, frame-wise continuous dataset for depthestimation in dynamic outdoor driving environments, comprising 20K video framesto evaluate existing methods. Our lightweight acquisition pipeline ensuresbroad scene coverage at low cost, while sparse yet statistically sufficientground truth enables robust training. Compared to existing datasets, ourspresents greater diversity in driving scenarios and lower depth density,creating new challenges for generalization. Benchmark experiments with standardmonocular depth estimation models validate the dataset's utility and highlightsubstantial performance gaps in challenging conditions, establishing a newplatform for advancing depth estimation research.</description>
      <author>example@mail.com (Xianda Guo, Ruijun Zhang, Yiqun Duan, Ruilin Wang, Keyuan Zhou, Wenzhao Zheng, Wenke Huang, Gangwei Xu, Mike Horton, Yuan Si, Hao Zhao, Long Chen)</author>
      <guid isPermaLink="false">2508.13977v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>MMIS-Net for Retinal Fluid Segmentation and Detection</title>
      <link>http://arxiv.org/abs/2508.13936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MMIS-Net的多模态医学图像分割网络，通过利用多种医学数据集的协同潜力，提高了在未见数据上的分割性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习方法在医学图像分割和疾病检测中表现出色，但大多数方法仅使用单一来源、模态、器官或疾病类型的数据进行训练和测试，忽略了其他可用标注数据的组合潜力。&lt;h4&gt;目的&lt;/h4&gt;利用各种模态、器官和疾病的小型标注医学图像数据集的协同潜力，提高在未见数据上的分割性能。&lt;h4&gt;方法&lt;/h4&gt;提出MMIS-Net算法，包含相似性融合块，利用监督和像素级相似性知识选择进行特征图融合；创建独热标签空间处理不同数据集间的类别定义不一致和标签矛盾；在10个包含2种模态的19个器官的数据集上训练构建单一模型。&lt;h4&gt;主要发现&lt;/h4&gt;在RETOUCH重大挑战隐藏测试集上，MMIS-Net性能优于大型基础模型和其他最先进算法；在流体分割任务中达到0.83的最佳平均Dice分数和0.035的绝对体积差异；在流体检测任务中获得完美的1曲线下面积。&lt;h4&gt;结论&lt;/h4&gt;将相似性融合块整合到网络主干中用于监督和相似性知识选择，以及使用独热标签空间处理标签类别不一致和矛盾，显著提高了模型的有效性。&lt;h4&gt;翻译&lt;/h4&gt;目的：深度学习方法在医学图像的分割和疾病检测中显示出良好的结果。然而，大多数方法仅使用单一来源、模态、器官或疾病类型的数据进行训练和测试，忽略了其他可用标注数据的组合潜力。各种模态、器官和疾病的小型标注医学图像数据集公开可用。本研究旨在利用这些数据集的协同潜力，提高在未见数据上的性能。方法：为此，我们提出了一种名为MMIS-Net（多模态医学图像分割网络）的新算法，该算法具有相似性融合块，利用监督和像素级相似性知识选择进行特征图融合。此外，为了处理类别定义不一致和标签矛盾，我们创建了独热标签空间来处理在一个数据集中缺失但在另一个数据集中标注的类别。MMIS-Net在10个包含2种模态的19个器官的数据集上训练，构建了单一模型。结果：在RETOUCH重大挑战隐藏测试集上评估算法，性能优于医学图像分割的大型基础模型和其他最先进算法。我们在流体分割任务中获得了0.83的最佳平均Dice分数和0.035的绝对体积差异，在流体检测任务中获得了完美的1曲线下面积。结论：定量结果突显了所提出模型的有效性，这是由于将相似性融合块整合到网络主干中用于监督和相似性知识选择，以及使用独热标签空间处理标签类别不一致和矛盾。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Deep learning methods have shown promising results in thesegmentation, and detection of diseases in medical images. However, mostmethods are trained and tested on data from a single source, modality, organ,or disease type, overlooking the combined potential of other availableannotated data. Numerous small annotated medical image datasets from variousmodalities, organs, and diseases are publicly available. In this work, we aimto leverage the synergistic potential of these datasets to improve performanceon unseen data. Approach: To this end, we propose a novel algorithm calledMMIS-Net (MultiModal Medical Image Segmentation Network), which featuresSimilarity Fusion blocks that utilize supervision and pixel-wise similarityknowledge selection for feature map fusion. Additionally, to addressinconsistent class definitions and label contradictions, we created a one-hotlabel space to handle classes absent in one dataset but annotated in another.MMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalitiesto build a single model. Results: The algorithm was evaluated on the RETOUCHgrand challenge hidden test set, outperforming large foundation models formedical image segmentation and other state-of-the-art algorithms. We achievedthe best mean Dice score of 0.83 and an absolute volume difference of 0.035 forthe fluids segmentation task, as well as a perfect Area Under the Curve of 1for the fluid detection task. Conclusion: The quantitative results highlightthe effectiveness of our proposed model due to the incorporation of SimilarityFusion blocks into the network's backbone for supervision and similarityknowledge selection, and the use of a one-hot label space to address labelclass inconsistencies and contradictions.</description>
      <author>example@mail.com (Nchongmaje Ndipenocha, Alina Mirona, Kezhi Wanga, Yongmin Li)</author>
      <guid isPermaLink="false">2508.13936v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2508.13872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure, 1 table. Contribution for REEACH 2025 Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Id-Pattern系统是RED.AI项目中的一个多代理人工智能系统，用于协助识别石材劣化模式，通过模拟专家协作来提高诊断效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;传统石材劣化模式识别方法依赖专家团队的直接观察，虽然准确但时间和成本较高。&lt;h4&gt;目的&lt;/h4&gt;引入并评估一个多代理人工智能系统，模拟专家之间的协作，并从视觉证据自动诊断石材病变。&lt;h4&gt;方法&lt;/h4&gt;基于认知架构协调五个专门的AI代理：岩石学家、病理学家、环境专家、修复师和诊断协调员，使用28张涉及多种劣化模式的困难图像进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;与基础模型相比，系统在所有指标上都有显著提升。&lt;h4&gt;结论&lt;/h4&gt;多代理AI系统能够有效提高石材病变诊断的效率和准确性，减少对专家团队的依赖。&lt;h4&gt;翻译&lt;/h4&gt;RED.AI项目中的Id-Pattern系统是一个代理系统，旨在协助识别石材劣化模式。基于专家团队直接观察的传统方法虽然准确但在时间和资源方面成本高昂。这里开发的系统引入并评估了一个多代理人工智能系统，旨在模拟专家之间的协作并从视觉证据自动诊断石材病变。该方法基于一个认知架构，协调一组专门的AI代理，在这个特定案例中限制为五个：岩石学家、病理学家、环境专家、修复师和诊断协调员。为了评估系统，我们选择了28张涉及多种劣化模式的困难图像。我们的初步结果显示，与基础模型相比，我们系统在所有指标上都有巨大提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Id-Pattern system within the RED.AI project (Reabilita\c{c}\~aoEstrutural Digital atrav\'es da AI) consists of an agentic system designed toassist in the identification of stone deterioration patterns. Traditionalmethodologies, based on direct observation by expert teams, are accurate butcostly in terms of time and resources. The system developed here introduces andevaluates a multi-agent artificial intelligence (AI) system, designed tosimulate collaboration between experts and automate the diagnosis of stonepathologies from visual evidence. The approach is based on a cognitivearchitecture that orchestrates a team of specialized AI agents which, in thisspecific case, are limited to five: a lithologist, a pathologist, anenvironmental expert, a conservator-restorer, and a diagnostic coordinator. Toevaluate the system we selected 28 difficult images involving multipledeterioration patterns. Our first results showed a huge boost on all metrics ofour system compared to the foundational model.</description>
      <author>example@mail.com (Daniele Corradetti, José Delgado Rodrigues)</author>
      <guid isPermaLink="false">2508.13872v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale cooperative sulfur vacancy dynamics in two-dimensional MoS2 from machine learning interatomic potentials</title>
      <link>http://arxiv.org/abs/2508.13790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过机器学习原子间势的分子动力学模拟，揭示了MoS2单层中硫空位形成机制及其对催化活性和忆阻行为的影响。&lt;h4&gt;背景&lt;/h4&gt;MoS2单层中扩展硫空位的形成与催化活性密切相关，并且可能是其忆阻行为的基础。理解这些空位的形成机制对于材料性能优化具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;旨在通过分子动力学模拟揭示MoS2单层中空位协同输运的关键机制，特别是空位并入团簇的过程，并解释实验中观察到的辐照诱导空位模式。&lt;h4&gt;方法&lt;/h4&gt;采用纳秒级分子动力学模拟，使用机器学习原子间势(MLIPs)框架，包括两种方法：(i)使用高斯近似势的在线学习，和(ii)等变基础模型的微调。&lt;h4&gt;主要发现&lt;/h4&gt;模拟揭示了空位协同输运的关键机制，包括空位并入任意大小的团簇；为实验观察到的辐照诱导空位模式提供了连贯的原子解释，特别是跨越数十纳米的线缺陷的形成。&lt;h4&gt;结论&lt;/h4&gt;扩展硫空位在MoS2单层中的形成机制对于理解其催化活性和忆阻行为至关重要，机器学习原子间势框架能够有效模拟这些复杂过程。&lt;h4&gt;翻译&lt;/h4&gt;MoS2单层中扩展硫空位的形成与催化活性密切相关，并且可能是其忆阻行为的基础。使用机器学习原子间势(MLIPs)进行的纳秒级分子动力学模拟揭示了空位协同输运的关键机制，包括空位并入任意大小的团簇。这些模拟为实验观察到的辐照诱导空位模式提供了连贯的原子解释，特别是跨越数十纳米的线缺陷的形成。比较了两种MLIP框架的结果和性能：(i)使用高斯近似势的在线学习，和(ii)等变基础模型的微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The formation of extended sulfur vacancies in MoS2 monolayers is closelyassociated with catalytic activity and may also be the basis for its memristivebehavior. Nanosecond-scale molecular dynamics simulations using machinelearning interatomic potentials (MLIPs) reveal key mechanisms of cooperativevacancy transport, including incorporation of vacancies into clusters ofarbitrary size. The simulations provide a coherent atomistic explanation forirradiation-induced vacancy patterns observed experimentally, especially theformation of line defects spanning tens of nanometers. Results and performanceare compared of two MLIP frameworks: (i) on-the-fly learning with Gaussianapproximation potential, and (ii) fine-tuning of an equivariant foundationmodel.</description>
      <author>example@mail.com (Aaron Flötotto, Benjamin Spetzler, Rose von Stackelberg, Martin Ziegler, Erich Runge, Christian Dreßler)</author>
      <guid isPermaLink="false">2508.13790v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery</title>
      <link>http://arxiv.org/abs/2508.13701v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at DAGM German Conference on Pattern Recognition (GCPR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于分割基础模型的新方法，通过上下文学习策略在零样本设置下实现细胞分割，无需数据集特定的调整即可准确分割生物学相关结构。&lt;h4&gt;背景&lt;/h4&gt;高通量筛选使用自动化显微镜是生物制药药物发现的关键驱动力，能够同时评估数千种药物候选物。传统的图像分析和深度学习方法已被用于分析这些复杂的大规模数据集，其中细胞分割是提取相关结构的关键步骤。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需大量手动参数调整或领域特定模型微调的细胞分割方法。&lt;h4&gt;方法&lt;/h4&gt;采用三步过程进行细胞核、细胞和亚细胞分割，引入自提示机制，使用生长掩码和战略性放置的前景/背景点来编码形态和拓扑先验，在零样本设置下应用分割基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准细胞分割基准测试和行业相关的命中验证测定上表现良好，能够准确分割生物学相关结构而无需针对特定数据集进行调整。&lt;h4&gt;结论&lt;/h4&gt;基于分割基础模型和上下文学习策略的方法解决了传统细胞分割方法需要大量手动调整的问题，提供了一种更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用自动化显微镜的高通量筛选是生物制药药物发现的关键驱动力，使能够并行评估数千种用于癌症等疾病的药物候选物。传统的图像分析和深度学习方法已被用于分析这些复杂的大规模数据集，其中细胞分割是提取相关结构的关键步骤。然而，这两种策略通常都需要大量的手动参数调整或领域特定的模型微调。我们提出了一种新方法，在零样本设置（即不进行微调）下应用分割基础模型，由上下文学习策略引导。我们的方法采用三步过程进行细胞核、细胞和亚细胞分割，引入了一种自提示机制，使用生长掩码和战略性放置的前景/背景点来编码形态和拓扑先验。我们在标准的细胞分割基准测试和行业相关的命中验证测定上验证了我们的方法，证明它能够准确分割生物学相关结构而无需针对特定数据集进行调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-throughput screening using automated microscopes is a key driver inbiopharma drug discovery, enabling the parallel evaluation of thousands of drugcandidates for diseases such as cancer. Traditional image analysis and deeplearning approaches have been employed to analyze these complex, large-scaledatasets, with cell segmentation serving as a critical step for extractingrelevant structures. However, both strategies typically require extensivemanual parameter tuning or domain-specific model fine-tuning. We present anovel method that applies a segmentation foundation model in a zero-shotsetting (i.e., without fine-tuning), guided by an in-context learning strategy.Our approach employs a three-step process for nuclei, cell, and subcellularsegmentation, introducing a self-prompting mechanism that encodes morphologicaland topological priors using growing masks and strategically placedforeground/background points. We validate our method on both standard cellsegmentation benchmarks and industry-relevant hit validation assays,demonstrating that it accurately segments biologically relevant structureswithout the need for dataset-specific tuning.</description>
      <author>example@mail.com (Jacob Hanimann, Daniel Siegismund, Mario Wieser, Stephan Steigele)</author>
      <guid isPermaLink="false">2508.13701v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter</title>
      <link>http://arxiv.org/abs/2508.13530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CrafterDojo，一套基础模型和工具，使Crafter环境成为一个轻量级、原型友好且类似Minecraft的通用智能体研究测试平台。&lt;h4&gt;背景&lt;/h4&gt;开发通用智能体是AI的核心挑战。Minecraft虽然复杂且有大规模数据，但速度慢且工程开销大，不适合快速原型设计。Crafter是一个轻量级替代方案，保留了Minecraft的关键挑战，但由于缺乏基础模型，其使用仅限于狭窄任务。&lt;h4&gt;目的&lt;/h4&gt;开发一套基础模型和工具，将Crafter环境转变为一个轻量级、原型友好且类似Minecraft的通用智能体研究测试平台。&lt;h4&gt;方法&lt;/h4&gt;通过引入CrafterVPT（行为先验）、CrafterCLIP（视觉语言基础）和CrafterSteve-1（指令跟随）来解决Crafter环境的应用限制。此外，还提供了用于生成行为和标题数据集的工具包（CrafterPlay和CrafterCaption）、参考智能体实现、基准评估和完整的开源代码库。&lt;h4&gt;主要发现&lt;/h4&gt;CrafterDojo成功将Crafter环境转变为一个适合通用智能体研究的轻量级测试平台，通过多种基础模型和工具支持了智能体的行为先验、视觉语言基础和指令跟随能力。&lt;h4&gt;结论&lt;/h4&gt;CrafterDojo为通用智能体研究提供了一个轻量级、原型友好且类似Minecraft的测试环境，解决了Minecraft在快速原型设计方面的局限性，同时保留了其关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;开发通用智能体是AI的核心挑战。Minecraft提供了丰富的复杂性和互联网规模的数据，但其缓慢的速度和工程开销使其不适合快速原型设计。Crafter提供了一个轻量级替代方案，保留了来自Minecraft的关键挑战，但由于缺乏在Minecraft环境中推动进展的基础模型，其使用仅限于狭窄任务。在本文中，我们提出了CrafterDojo，一套基础模型和工具，使Crafter环境成为一个轻量级、原型友好且类似Minecraft的通用智能体研究测试平台。CrafterDojo通过引入CrafterVPT、CrafterCLIP和CrafterSteve-1分别用于行为先验、视觉语言基础和指令跟随来解决这一问题。此外，我们还提供了用于生成行为和标题数据集的工具包（CrafterPlay和CrafterCaption）、参考智能体实现、基准评估和完整的开源代码库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing general-purpose embodied agents is a core challenge in AI.Minecraft provides rich complexity and internet-scale data, but its slow speedand engineering overhead make it unsuitable for rapid prototyping. Crafteroffers a lightweight alternative that retains key challenges from Minecraft,yet its use has remained limited to narrow tasks due to the absence offoundation models that have driven progress in the Minecraft setting. In thispaper, we present CrafterDojo, a suite of foundation models and tools thatunlock the Crafter environment as a lightweight, prototyping-friendly, andMinecraft-like testbed for general-purpose embodied agent research. CrafterDojoaddresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 forbehavior priors, vision-language grounding, and instruction following,respectively. In addition, we provide toolkits for generating behavior andcaption datasets (CrafterPlay and CrafterCaption), reference agentimplementations, benchmark evaluations, and a complete open-source codebase.</description>
      <author>example@mail.com (Junyeong Park, Hyeonseo Cho, Sungjin Ahn)</author>
      <guid isPermaLink="false">2508.13530v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</title>
      <link>http://arxiv.org/abs/2508.13525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, 2 tables. Code:  https://github.com/HasanBGIt/Saudi-Dialect-ALLaM . Dataset and trained  weights/adapters are not released. Primary category: cs.CL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对阿拉伯语大语言模型对沙特方言支持不足的问题，通过LoRA微调沙特基础模型，实现了更准确的沙特方言生成，并提出了两种训练方法：方言标记训练和无标记训练。&lt;h4&gt;背景&lt;/h4&gt;阿拉伯语大语言模型主要由现代标准阿拉伯语(MSA)主导，对沙特方言(如Najdi和Hijazi)的支持有限，这种代表性不足阻碍了模型捕捉真实方言变异的能力。&lt;h4&gt;目的&lt;/h4&gt;提高大语言模型对沙特方言(特别是Hijazi和Najdi)的生成能力，增强模型对方言变异的捕捉，同时减少现代标准阿拉伯语的泄漏。&lt;h4&gt;方法&lt;/h4&gt;使用私有的沙特方言指令数据集(5,466个合成指令-响应对，Hijazi和Najdi各占50%)，对沙特阿拉伯开发的首个基础模型ALLaM-7B-Instruct-preview进行LoRA微调。研究了两种训练变体：方言标记训练(在指令前添加方言标记)和无标记训练(省略标记)。评估结合了外部方言分类器、文本保真度指标(chrF++和BERTScore)和多样性度量。&lt;h4&gt;主要发现&lt;/h4&gt;方言标记模型实现了最佳控制，将沙特语比例从47.97%提高到84.21%，并将MSA泄漏从32.63%降低到6.21%；保真度也有所提高(chrF++ +3.53，BERTScore +0.059)。两种LoRA变体在方言控制和保真度方面都优于多种通用指令模型，同时避免了这些基线模型经常出现的元数据标记回声问题。&lt;h4&gt;结论&lt;/h4&gt;通过LoRA微调可以显著提高大语言模型对沙特方言的生成能力。研究团队选择不发布数据集或模型权重/适配器，而是发布训练/评估/推理代码和详细数据表，以支持独立验证和未来研究。&lt;h4&gt;翻译&lt;/h4&gt;针对阿拉伯语的大语言模型仍然由现代标准阿拉伯语(MSA)主导，对沙特方言(如Najdi和Hijazi)的支持有限。这种代表性不足阻碍了它们捕捉真实方言变异的能力。使用一个私有的沙特方言指令数据集(包含Hijazi和Najdi；5,466个合成指令-响应对；50/50分割)，我们LoRA微调了ALLaM-7B-Instruct-preview，这是在沙特阿拉伯开发的首个基础模型，用于沙特方言生成。我们研究了两种变体：(i)方言标记训练，即在指令前添加明确的方言标记，和(ii)无标记训练，即在格式化时省略标记。在保留测试集上的评估结合了外部方言分类器与文本保真度指标(chrF++和BERTScore)和多样性度量。方言标记模型实现了最佳控制，将沙特语比例从47.97%提高到84.21%，并将MSA泄漏从32.63%降低到6.21%；保真度也有所提高(chrF++ +3.53，BERTScore +0.059)。两种LoRA变体在方言控制和保真度方面都优于强大的通用指令模型(Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat)，同时避免了这些基线模型经常出现的元数据标记回声问题。我们没有发布数据集或任何模型权重/适配器；相反，我们发布了训练/评估/推理代码和详细的数据表(模式和聚合统计)，以支持独立验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) for Arabic are still dominated by ModernStandard Arabic (MSA), with limited support for Saudi dialects such as Najdiand Hijazi. This underrepresentation hinders their ability to capture authenticdialectal variation. Using a privately curated Saudi Dialect Instructiondataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation modeldeveloped in Saudi Arabia, for Saudi dialect generation. We investigate twovariants: (i) Dialect-Token training, which prepends an explicit dialect tag tothe instruction, and (ii) No-Token training, which omits the tag at formattingtime. Evaluation on a held-out test set combines an external dialect classifierwith text fidelity metrics (chrF++ and BERTScore) and diversity measures. TheDialect-Token model achieves the best control, raising the Saudi rate from47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity alsoimproves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform stronggeneric instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control andfidelity, while avoiding metadata-tag echoing that these baselines frequentlyexhibit. We do not release the dataset or any model weights/adapters; instead,we release training/evaluation/inference code and a detailed datasheet (schemaand aggregate statistics) to support independent verification.</description>
      <author>example@mail.com (Hassan Barmandah)</author>
      <guid isPermaLink="false">2508.13525v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</title>
      <link>http://arxiv.org/abs/2508.13518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, CVPR Oral&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种几何知识引导的分布校准框架，利用基础模型特征分布的几何形状的跨领域迁移性，解决联邦学习和长尾识别中的数据异质性和样本不平衡问题。&lt;h4&gt;背景&lt;/h4&gt;尽管深度学习发展迅速，但仍面临观测训练样本与潜在真实分布之间的差距挑战，这种差距可能由采样偏差、噪声等因素造成。&lt;h4&gt;目的&lt;/h4&gt;验证在基础模型时代，利用现成的视觉基础模型进行特征提取时，特征分布几何形状的跨领域迁移性在联邦学习和长尾识别等场景中的实用性。&lt;h4&gt;方法&lt;/h4&gt;提出几何知识引导的分布校准框架，在联邦学习中设计隐私约束下获取全局几何形状的技术并生成新样本；在长尾学习中利用从样本丰富类别迁移的几何知识恢复尾部类别的真实分布。&lt;h4&gt;主要发现&lt;/h4&gt;特征分布的几何形状在不同领域和数据集上展现出显著的迁移性，几何知识引导的分布校准能有效克服数据异质性和样本不平衡导致的信息不足。&lt;h4&gt;结论&lt;/h4&gt;所提出的几何知识引导的分布校准框架能够有效弥合观测训练样本与潜在真实分布之间的差距，提升模型在联邦学习和长尾识别等场景下的性能。&lt;h4&gt;翻译&lt;/h4&gt;尽管深度学习取得了快速进展，但一个持续的挑战是观测到的训练样本与潜在真实分布之间的差距。造成这种差距有多种原因，例如采样偏差、噪声等。在基础模型时代，我们表明，当利用现成的视觉基础模型（如CLIP、DINOv2）进行特征提取时，所得特征分布的几何形状在不同领域和数据集上展现出显著的迁移性。为了验证其实用性，我们在两个流行且具有挑战性的场景中实现了我们的几何知识引导的分布校准框架：联邦学习和长尾识别。在联邦学习场景中，我们设计了一种在隐私约束下获取全局几何形状的技术，然后利用这些知识为客户端生成新样本，旨在弥合本地和全局观测之间的差距。在长尾学习中，它利用从样本丰富的类别迁移的几何知识来恢复样本稀少的尾部类别的真实分布。全面的实验表明，我们提出的几何知识引导的分布校准有效地克服了由数据异质性和样本不平衡导致的信息不足，在多个基准测试中提升了性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the fast progress of deep learning, one standing challenge is the gapof the observed training samples and the underlying true distribution. Thereare multiple reasons for the causing of this gap e.g. sampling bias, noise etc.In the era of foundation models, we show that when leveraging the off-the-shelf(vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, thegeometric shapes of the resulting feature distributions exhibit remarkabletransferability across domains and datasets. To verify its practicalusefulness, we embody our geometric knowledge-guided distribution calibrationframework in two popular and challenging settings: federated learning andlong-tailed recognition. In the federated setting, we devise a technique ofacquiring the global geometric shape under privacy constraints, then leveragethis knowledge to generate new samples for clients, in the aim of bridging thegap between local and global observations. In long-tailed learning, it utilizesthe geometric knowledge transferred from sample-rich categories to recover thetrue distribution for sample-scarce tail classes. Comprehensive experimentsshow that our proposed geometric knowledge-guided distribution calibrationeffectively overcomes information deficits caused by data heterogeneity andsample imbalance, with boosted performance across benchmarks.</description>
      <author>example@mail.com (Yanbiao Ma, Wei Dai, Bowei Liu, Jiayi Chen, Wenke Huang, Guancheng Wan, Zhiwu Lu, Junchi Yan)</author>
      <guid isPermaLink="false">2508.13518v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>NovoMolGen: Rethinking Molecular Language Model Pretraining</title>
      <link>http://arxiv.org/abs/2508.13408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为NovoMolGen的transformer基础模型家族，用于从头分子生成，在15亿个分子上进行了预训练，在分子生成任务中取得了新的最先进结果。&lt;h4&gt;背景&lt;/h4&gt;设计具有理想特性的新分子需要探索庞大的化学空间（10^23到10^60个可能的可合成候选分子），尽管已有多种深度生成模型，但基于字符串表示的分子大语言模型（Mol-LLMs）作为可扩展方法能够探索数十亿分子。&lt;h4&gt;目的&lt;/h4&gt;系统研究标准语言建模实践（如文本表示、标记化策略、模型大小和数据集规模）对分子生成性能的影响，并开发一种高效的基础模型用于分子生成。&lt;h4&gt;方法&lt;/h4&gt;引入NovoMolGen，一类基于transformer的预训练基础模型，在15亿个分子上进行预训练，用于从头分子生成，并通过广泛的实证分析评估其性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练期间测量的性能指标与实际下游性能之间存在弱相关性，揭示了分子训练与一般自然语言处理训练动力学之间的区别；NovoMolGen在无约束和目标导向的分子生成任务中都显著优于之前的Mol-LLMs和专门的生成模型。&lt;h4&gt;结论&lt;/h4&gt;NovoMolGen为推进高效有效的分子建模策略提供了坚实基础，代表了分子生成领域的重大进展。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了一种名为NovoMolGen的transformer基础模型家族，用于从头分子生成，在15亿个分子上进行了预训练，在分子生成任务中取得了新的最先进结果。设计具有理想特性的新分子需要探索庞大的化学空间（10^23到10^60个可能的可合成候选分子），尽管已有多种深度生成模型，但基于字符串表示的分子大语言模型（Mol-LLMs）作为可扩展方法能够探索数十亿分子。本研究旨在系统研究标准语言建模实践（如文本表示、标记化策略、模型大小和数据集规模）对分子生成性能的影响，并开发一种高效的基础模型用于分子生成。研究人员引入NovoMolGen，一类基于transformer的预训练基础模型，在15亿个分子上进行预训练，用于从头分子生成，并通过广泛的实证分析评估其性能。研究发现，预训练期间测量的性能指标与实际下游性能之间存在弱相关性，揭示了分子训练与一般自然语言处理训练动力学之间的区别；NovoMolGen在无约束和目标导向的分子生成任务中都显著优于之前的Mol-LLMs和专门的生成模型。结论是，NovoMolGen为推进高效有效的分子建模策略提供了坚实基础，代表了分子生成领域的重大进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing de-novo molecules with desired property profiles requires efficientexploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$possible synthesizable candidates. While various deep generative models havebeen developed to design small molecules using diverse input representations,Molecular Large Language Models (Mol-LLMs) based on string representations haveemerged as a scalable approach capable of exploring billions of molecules.However, there remains limited understanding regarding how standard languagemodeling practices such as textual representations, tokenization strategies,model size, and dataset scale impact molecular generation performance. In thiswork, we systematically investigate these critical aspects by introducingNovoMolGen, a family of transformer-based foundation models pretrained on 1.5billion molecules for de-novo molecule generation. Through extensive empiricalanalyses, we identify a weak correlation between performance metrics measuredduring pretraining and actual downstream performance, revealing importantdistinctions between molecular and general NLP training dynamics. NovoMolGenestablishes new state-of-the-art results, substantially outperforming priorMol-LLMs and specialized generative models in both unconstrained andgoal-directed molecular generation tasks, thus providing a robust foundationfor advancing efficient and effective molecular modeling strategies.</description>
      <author>example@mail.com (Kamran Chitsaz, Roshan Balaji, Quentin Fournier, Nirav Pravinbhai Bhatt, Sarath Chandar)</author>
      <guid isPermaLink="false">2508.13408v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Driven High-Dimensional Variable Selection</title>
      <link>http://arxiv.org/abs/2508.13890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于扩散模型的重采样-聚合框架，用于高维高度相关数据的变量选择，通过生成合成数据并应用多种选择器，实现了稳定且可靠的预测变量子集选择。&lt;h4&gt;背景&lt;/h4&gt;高维、高度相关数据的变量选择一直是一个具有挑战性的问题，传统方法往往产生不稳定和不可靠的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维高度相关数据并产生稳定结果的变量选择方法。&lt;h4&gt;方法&lt;/h4&gt;从拟合原始数据的扩散模型生成多个伪数据集，应用现成的选择器（如lasso或SCAD），存储包含指标和系数，然后通过跨副本聚合产生具有校准稳定性分数的预测变量子集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在理论上是选择一致的；由于迁移学习，在小样本或噪声数据中表现更好；扩展到图形模型选择和统计推断；实验显示与基线方法相比有更高真正例率和更低假发现比例。&lt;h4&gt;结论&lt;/h4&gt;基于扩散的数据增强与原则性聚合相结合的方法推进了变量选择方法论，为复杂科学应用提供了可解释且统计严谨的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;高维高度相关数据的变量选择一直是一个具有挑战性的问题，通常会产生不稳定且不可靠的模型。我们提出了一种重采样-聚合框架，利用扩散模型生成高保真合成数据的能力。具体而言，我们从拟合原始数据的扩散模型中抽取多个伪数据集，应用任何现成的选择器（如lasso或SCAD），并存储所得的包含指标和系数。跨副本聚合产生具有校准稳定性分数的预测变量子集用于变量选择。理论上，我们在温和假设下证明了所提出方法的选择一致性。由于生成模型从大型预训练权重中导入知识，该程序自然受益于迁移学习，在观测样本小或嘈杂时提高功效。我们还扩展了合成数据聚合框架到其他模型选择问题，包括图形模型选择和支持有效置信区间和假设检验的统计推断。广泛的模拟显示与lasso、稳定性选择和knockoff基线相比有一致的改进，特别是在预测变量强相关的情况下，实现了更高的真正例率和更低的假发现比例。通过将基于扩散的数据增强与原则性聚合相结合，我们的方法推进了变量选择方法论，并为复杂科学应用中可解释的、统计严谨的分析扩展了工具包。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variable selection for high-dimensional, highly correlated data has long beena challenging problem, often yielding unstable and unreliable models. Wepropose a resample-aggregate framework that exploits diffusion models' abilityto generate high-fidelity synthetic data. Specifically, we draw multiplepseudo-data sets from a diffusion model fitted to the original data, apply anyoff-the-shelf selector (e.g., lasso or SCAD), and store the resulting inclusionindicators and coefficients. Aggregating across replicas produces a stablesubset of predictors with calibrated stability scores for variable selection.Theoretically, we show that the proposed method is selection consistent undermild assumptions. Because the generative model imports knowledge from largepre-trained weights, the procedure naturally benefits from transfer learning,boosting power when the observed sample is small or noisy. We also extend theframework of aggregating synthetic data to other model selection problems,including graphical model selection, and statistical inference that supportsvalid confidence intervals and hypothesis tests. Extensive simulations showconsistent gains over the lasso, stability selection, and knockoff baselines,especially when predictors are strongly correlated, achieving highertrue-positive rates and lower false-discovery proportions. By couplingdiffusion-based data augmentation with principled aggregation, our methodadvances variable selection methodology and broadens the toolkit forinterpretable, statistically rigorous analysis in complex scientificapplications.</description>
      <author>example@mail.com (Minjie Wang, Xiaotong Shen, Wei Pan)</author>
      <guid isPermaLink="false">2508.13890v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</title>
      <link>http://arxiv.org/abs/2508.13833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了建筑信息模型(BIM)与自然语言处理(NLP)的集成，用于从非结构化的法国建筑技术规范(BTS)文档中自动提取需求。&lt;h4&gt;背景&lt;/h4&gt;在建筑行业中，需要从非结构化的法国建筑技术规范文档中提取需求信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化系统，能够从非结构化的法国建筑技术规范文档中提取需求信息。&lt;h4&gt;方法&lt;/h4&gt;使用命名实体识别(NER)和关系提取(RE)技术，利用基于transformer的模型CamemBERT和应用迁移学习，使用法语模型Fr_core_news_lg。开发了从基于规则到基于深度学习的多种方法进行基准测试。对于RE，实现了包括随机森林在内的四种监督模型，使用自定义特征向量。使用手工制作的标注数据集比较不同方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;CamemBERT和Fr_core_news_lg在NER中表现出卓越的性能，F1分数超过90%；随机森林在RE中最有效，F1分数超过80%。&lt;h4&gt;结论&lt;/h4&gt;研究结果将在未来工作中以知识图谱的形式呈现，以进一步增强自动验证系统。&lt;h4&gt;翻译&lt;/h4&gt;本研究探索了建筑信息模型(BIM)与自然语言处理(NLP)的集成，用于在建筑行业内从非结构化的法国建筑技术规范(BTS)文档中自动提取需求。采用命名实体识别(NER)和关系提取(RE)技术，研究利用了基于transformer的模型CamemBERT，并应用了迁移学习，使用在大型法语语料库上预训练的法语模型Fr_core_news_lg。为对这些模型进行基准测试，开发了从基于规则到基于深度学习的多种方法。对于RE，实现了包括随机森林在内的四种监督模型，使用自定义特征向量。使用手工制作的标注数据集来比较NER方法和RE模型的有效性。结果表明，CamemBERT和Fr_core_news_lg在NER中表现出卓越的性能，F1分数超过90%，而随机森林在RE中最有效，F1分数超过80%。结果将在未来工作中以知识图谱的形式呈现，以进一步增强自动验证系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study explores the integration of Building Information Modeling (BIM)with Natural Language Processing (NLP) to automate the extraction ofrequirements from unstructured French Building Technical Specification (BTS)documents within the construction industry. Employing Named Entity Recognition(NER) and Relation Extraction (RE) techniques, the study leverages thetransformer-based model CamemBERT and applies transfer learning with the Frenchlanguage model Fr\_core\_news\_lg, both pre-trained on a large French corpus inthe general domain. To benchmark these models, additional approaches rangingfrom rule-based to deep learning-based methods are developed. For RE, fourdifferent supervised models, including Random Forest, are implemented using acustom feature vector. A hand-crafted annotated dataset is used to compare theeffectiveness of NER approaches and RE models. Results indicate that CamemBERTand Fr\_core\_news\_lg exhibited superior performance in NER, achievingF1-scores over 90\%, while Random Forest proved most effective in RE, with anF1 score above 80\%. The outcomes are intended to be represented as a knowledgegraph in future work to further enhance automatic verification systems.</description>
      <author>example@mail.com (Insaf Nahri, Romain Pinquié, Philippe Véron, Nicolas Bus, Mathieu Thorel)</author>
      <guid isPermaLink="false">2508.13833v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings</title>
      <link>http://arxiv.org/abs/2508.13672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 34th ACM International Conference on Information and  Knowledge Management (CIKM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的基于实例迁移学习的LIME框架(ITL-LIME)，用于解决数据受限环境下LIME方法的局部性和稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;可解释人工智能(XAI)方法如LIME通过使用可解释的替代模型来近似黑盒机器学习模型的行为，提高了模型的可解释性。但LIME在扰动和采样中的固有随机性可能导致局部性和稳定性问题，特别是在训练数据有限的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于实例迁移学习的LIME框架(ITL-LIME)，以增强数据受限环境下的解释保真度和稳定性。&lt;h4&gt;方法&lt;/h4&gt;ITL-LIME通过将实例迁移学习引入LIME框架，利用来自相关源域的相关真实实例来辅助目标域的解释过程。具体来说，使用聚类将源域划分为具有代表性原型的簇，从与目标实例最相似的原型对应的源簇中检索相关的真实源实例，与目标实例的相邻真实实例组合，并构建基于对比学习的编码器作为加权机制，根据实例与目标实例的接近度为组合集中的实例分配权重，最后使用这些加权的源实例和目标实例来训练用于解释的替代模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用源域的相关实例和基于对比学习的加权机制，ITL-LIME能够提高数据受限环境下的解释质量和稳定性。&lt;h4&gt;结论&lt;/h4&gt;ITL-LIME框架解决了LIME在数据稀缺环境下的局部性和稳定性问题，通过迁移学习的方法提高了解释的保真度。&lt;h4&gt;翻译&lt;/h4&gt;可解释人工智能(XAI)方法，如局部可解释模型无关解释(LIME)，通过使用可解释的替代模型近似黑盒机器学习模型的行为，提高了其可解释性。然而，LIME在扰动和采样中的固有随机性可能导致局部性和稳定性问题，特别是在训练数据有限的情况下。在这种情况下，数据稀缺可能导致生成不真实的变异和偏离真实数据流形的样本，从而使替代模型无法准确近似原始模型的复杂决策边界。为了解决这些挑战，我们提出了一种新的基于实例迁移学习的LIME框架(ITL-LIME)，该框架在数据受限环境中增强了解释保真度和稳定性。ITL-LIME通过利用来自相关源域的相关真实实例来辅助目标域的解释过程，将实例迁移学习引入LIME框架。具体来说，我们使用聚类将源域划分为具有代表性原型的簇。我们的方法不是生成随机扰动，而是从与目标实例最相似的原型对应的源簇中检索相关的真实源实例，然后将它们与目标实例的相邻真实实例组合。为了定义紧凑的局部性，我们进一步构建了一个基于对比学习的编码器作为加权机制，根据实例与目标实例的接近度为组合集中的实例分配权重。最后，使用这些加权的源实例和目标实例来训练用于解释的替代模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Explainable Artificial Intelligence (XAI) methods, such as LocalInterpretable Model-Agnostic Explanations (LIME), have advanced theinterpretability of black-box machine learning models by approximating theirbehavior locally using interpretable surrogate models. However, LIME's inherentrandomness in perturbation and sampling can lead to locality and instabilityissues, especially in scenarios with limited training data. In such cases, datascarcity can result in the generation of unrealistic variations and samplesthat deviate from the true data manifold. Consequently, the surrogate model mayfail to accurately approximate the complex decision boundary of the originalmodel. To address these challenges, we propose a novel Instance-based TransferLearning LIME framework (ITL-LIME) that enhances explanation fidelity andstability in data-constrained environments. ITL-LIME introduces instancetransfer learning into the LIME framework by leveraging relevant real instancesfrom a related source domain to aid the explanation process in the targetdomain. Specifically, we employ clustering to partition the source domain intoclusters with representative prototypes. Instead of generating randomperturbations, our method retrieves pertinent real source instances from thesource cluster whose prototype is most similar to the target instance. Theseare then combined with the target instance's neighboring real instances. Todefine a compact locality, we further construct a contrastive learning-basedencoder as a weighting mechanism to assign weights to the instances from thecombined set based on their proximity to the target instance. Finally, theseweighted source and target instances are used to train the surrogate model forexplanation purposes.</description>
      <author>example@mail.com (Rehan Raza, Guanjin Wang, Kevin Wong, Hamid Laga, Marco Fisichella)</author>
      <guid isPermaLink="false">2508.13672v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Is Transfer Learning Necessary for Violin Transcription?</title>
      <link>http://arxiv.org/abs/2508.13516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了小提琴自动音乐转录中从头训练与微调预训练钢琴模型的性能比较，发现从头训练在特定条件下可以达到或超过微调模型的性能。&lt;h4&gt;背景&lt;/h4&gt;自动音乐转录在钢琴等乐器上已取得显著进展，主要得益于大规模高质量数据集的可用性。相比之下，小提琴AMT研究不足，主要受限于有限的标注数据。&lt;h4&gt;目的&lt;/h4&gt;研究从中等规模小提琴数据集从头开始训练是否能够匹配微调预训练钢琴模型的性能，探究在音色和演奏技巧差异存在的情况下迁移学习的有效性。&lt;h4&gt;方法&lt;/h4&gt;采用未经修改的钢琴转录架构，在包含约30小时对齐小提琴录音的MOSA数据集上进行训练，并在URMP和Bach10数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;从头开始训练的模型与微调的对应模型相比，实现了具有竞争力甚至更好的性能，表明不依赖预训练的钢琴表示也能实现强大的小提琴AMT。&lt;h4&gt;结论&lt;/h4&gt;研究强调了乐器特定数据收集和增强策略对于小提琴AMT的重要性，表明针对特定乐器收集足够的数据可能比依赖其他乐器的预训练模型更有效。&lt;h4&gt;翻译&lt;/h4&gt;自动音乐转录在钢琴等乐器上取得了显著进展，这主要得益于大规模高质量数据集的可用性。相比之下，由于标注数据有限，小提琴AMT仍然研究不足。一种常见的方法是为其他下游任务微调预训练模型，但在音色和演奏技巧差异存在的情况下，这种迁移学习的有效性尚不明确。在本工作中，我们研究了从中等规模小提琴数据集从头开始训练是否能够匹配微调预训练钢琴模型的性能。我们采用未经修改的钢琴转录架构，在包含约30小时对齐小提琴录音的MOSA数据集上进行训练。我们在URMP和Bach10上的实验表明，从头开始训练的模型与微调对应模型相比实现了具有竞争力甚至更好的性能。这些发现表明，不依赖预训练的钢琴表示也能实现强大的小提琴AMT，突显了乐器特定数据收集和增强策略的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic music transcription (AMT) has achieved remarkable progress forinstruments such as the piano, largely due to the availability of large-scale,high-quality datasets. In contrast, violin AMT remains underexplored due tolimited annotated data. A common approach is to fine-tune pretrained models forother downstream tasks, but the effectiveness of such transfer remains unclearin the presence of timbral and articulatory differences. In this work, weinvestigate whether training from scratch on a medium-scale violin dataset canmatch the performance of fine-tuned piano-pretrained models. We adopt a pianotranscription architecture without modification and train it on the MOSAdataset, which contains about 30 hours of aligned violin recordings. Ourexperiments on URMP and Bach10 show that models trained from scratch achievedcompetitive or even superior performance compared to fine-tuned counterparts.These findings suggest that strong violin AMT is possible without relying onpretrained piano representations, highlighting the importance ofinstrument-specific data collection and augmentation strategies.</description>
      <author>example@mail.com (Yueh-Po Peng, Ting-Kang Wang, Li Su, Vincent K. M. Cheung)</author>
      <guid isPermaLink="false">2508.13516v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Control</title>
      <link>http://arxiv.org/abs/2508.12738v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, accepted for CDC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种层次化贝叶斯优化框架，用于在顺序决策和控制场景中高效学习不同任务的控制器参数，通过利用问题结构知识而非将闭环成本视为黑盒，实现了任务间的知识迁移和增强的数据效率。&lt;h4&gt;背景&lt;/h4&gt;许多控制问题需要在不同的闭环任务中反复调整和适应控制器，其中数据效率和适应性至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种层次化贝叶斯优化框架，针对顺序决策和控制场景中的不同任务，实现高效的控制器参数学习。&lt;h4&gt;方法&lt;/h4&gt;该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识（包括动态系统、控制定律和相关的闭环成本函数）。使用高斯过程构建层次化代理模型，捕捉不同参数化下的闭环状态演化，并通过已知的闭式表达式精确计算特定任务的权重并累积到闭环成本中，实现不同闭环任务间的知识迁移和增强的数据效率。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架保留了与标准黑盒贝叶斯优化相当的次线性遗憾保证，同时支持多任务或迁移学习。&lt;h4&gt;结论&lt;/h4&gt;在模型预测控制的模拟实验中，与纯黑盒贝叶斯优化方法相比，该方法在样本效率和适应性方面都显示出显著优势。&lt;h4&gt;翻译&lt;/h4&gt;许多控制问题需要在不同的闭环任务中反复调整和适应控制器，其中数据效率和适应性至关重要。我们提出了一种层次化贝叶斯优化框架，专为顺序决策和控制场景中不同任务的高效控制器参数学习而定制。该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识，包括动态系统、控制定律和相关的闭环成本函数。我们使用高斯过程构建层次化代理模型，捕捉不同参数化下的闭环状态演化，同时通过已知的闭式表达式精确计算特定任务的权重并累积到闭环成本中。这实现了不同闭环任务间的知识迁移和增强的数据效率。与标准黑盒贝叶斯优化相比，所提出的框架保留了次线性遗憾保证，同时支持多任务或迁移学习。在模型预测控制的模拟实验中，与纯黑盒贝叶斯优化方法相比，该方法在样本效率和适应性方面都显示出显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many control problems require repeated tuning and adaptation of controllersacross distinct closed-loop tasks, where data efficiency and adaptability arecritical. We propose a hierarchical Bayesian optimization (BO) framework thatis tailored to efficient controller parameter learning in sequentialdecision-making and control scenarios for distinct tasks. Instead of treatingthe closed-loop cost as a black-box, our method exploits structural knowledgeof the underlying problem, consisting of a dynamical system, a control law, andan associated closed-loop cost function. We construct a hierarchical surrogatemodel using Gaussian processes that capture the closed-loop state evolutionunder different parameterizations, while the task-specific weighting andaccumulation into the closed-loop cost are computed exactly via knownclosed-form expressions. This allows knowledge transfer and enhanced dataefficiency between different closed-loop tasks. The proposed framework retainssublinear regret guarantees on par with standard black-box BO, while enablingmulti-task or transfer learning. Simulation experiments with model predictivecontrol demonstrate substantial benefits in both sample efficiency andadaptability when compared to purely black-box BO approaches.</description>
      <author>example@mail.com (Sebastian Hirt, Lukas Theiner, Maik Pfefferkorn, Rolf Findeisen)</author>
      <guid isPermaLink="false">2508.12738v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</title>
      <link>http://arxiv.org/abs/2508.13953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ReviewGraph的新框架，用于酒店业客户评论评分预测，通过将文本评论转换为知识图谱并利用图嵌入和情感特征进行预测，相比传统方法具有更低计算成本和更好的可解释性。&lt;h4&gt;背景&lt;/h4&gt;在酒店业中，理解影响客户评论评分的因素对提高客户满意度和业务表现至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出ReviewGraph框架用于评论评分预测(RRP)，通过分析客户评论来预测评分。&lt;h4&gt;方法&lt;/h4&gt;将文本客户评论转换为知识图谱，提取(主语、谓语、宾语)三元组并关联情感分数，使用Node2Vec图嵌入和情感特征，通过机器学习分类器预测评论评分，并在HotelRec数据集上与传统NLP基线和大型语言模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;ReviewGraph与现有最佳模型性能相似但计算成本更低；与LLMs具有可比的预测性能；在基于一致性的指标上优于基线；在可解释性、可视化和RAG系统集成方面具有额外优势。&lt;h4&gt;结论&lt;/h4&gt;图表示法在增强评论分析方面具有潜力，为未来研究整合高级图神经网络和微调的基于LLM的提取方法奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;在酒店业中，理解驱动客户评论评分的因素对于提高客户满意度和业务表现至关重要。本研究提出了ReviewGraph用于评论评分预测(RRP)的新框架，通过提取(主语、谓语、宾语)三元组并关联情感分数，将文本客户评论转换为知识图谱。利用图嵌入(Node2Vec)和情感特征，该框架通过机器学习分类器预测评论评分。我们将ReviewGraph性能与传统NLP基线(如词袋、TF-IDF和Word2Vec)和大型语言模型进行比较，并在HotelRec数据集上评估它们。与最先进的文献相比，我们提出的模型与最佳性能模型相似，但计算成本更低(无需集成)。虽然ReviewGraph在预测性能上与LLMs相当，并在基于一致性的指标(如Cohen's Kappa)上优于基线，但它还提供了可解释性、可视化和潜在集成到检索增强生成(RAG)系统的额外优势。这项研究强调了图表示法在增强评论分析方面的潜力，并为未来研究整合高级图神经网络和微调的基于LLM的提取方法奠定了基础。我们将在GitHub页面上分享ReviewGraph输出和开源平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the hospitality industry, understanding the factors that drive customerreview ratings is critical for improving guest satisfaction and businessperformance. This work proposes ReviewGraph for Review Rating Prediction (RRP),a novel framework that transforms textual customer reviews into knowledgegraphs by extracting (subject, predicate, object) triples and associatingsentiment scores. Using graph embeddings (Node2Vec) and sentiment features, theframework predicts review rating scores through machine learning classifiers.We compare ReviewGraph performance with traditional NLP baselines (such as Bagof Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluatingthem in the HotelRec dataset. In comparison to the state of the art literature,our proposed model performs similar to their best performing model but withlower computational cost (without ensemble).  While ReviewGraph achieves comparable predictive performance to LLMs andoutperforms baselines on agreement-based metrics such as Cohen's Kappa, itoffers additional advantages in interpretability, visual exploration, andpotential integration into Retrieval-Augmented Generation (RAG) systems. Thiswork highlights the potential of graph-based representations for enhancingreview analytics and lays the groundwork for future research integratingadvanced graph neural networks and fine-tuned LLM-based extraction methods. Wewill share ReviewGraph output and platform open-sourced on our GitHub pagehttps://github.com/aaronlifenghan/ReviewGraph</description>
      <author>example@mail.com (A. J. W. de Vink, Natalia Amat-Lefort, Lifeng Han)</author>
      <guid isPermaLink="false">2508.13953v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems</title>
      <link>http://arxiv.org/abs/2508.13839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可移动天线(MA)辅助的无蜂窝集成感知与通信(CF-ISAC)系统，以解决实际部署中功率放大器非线性失真对通信和感知性能的负面影响。该系统通过分布式失真感知的最坏情况鲁棒优化框架和自注意力卷积图神经网络算法，有效减轻了失真并增强了系统鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;无蜂窝集成感知与通信(CF-ISAC)架构是6G的潜在使能技术，提供频谱效率和无处不在的覆盖。然而，实际部署中存在硬件缺陷，特别是功率放大器(PA)的非线性失真，这会降低通信和感知性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种可移动天线(MA)辅助的CF-ISAC系统，以减轻功率放大器非线性失真并增强系统鲁棒性，改善通信-感知权衡。&lt;h4&gt;方法&lt;/h4&gt;1. 将PA非线性建模为三阶无记忆多项式，考虑三阶失真系数(3RDCs)在不同接入点间的变化；2. 设计分布式失真感知的最坏情况鲁棒优化框架；3. 分析PA失真对Cramer-Rao下界和通信速率的最坏情况影响；4. 应用连续凸近似(SCA)估计3RDCs；5. 在发射功率和感知约束下联合优化波束成形和MA位置；6. 开发MA使能的自注意力卷积图神经网络(SACGNN)算法解决高度非凸问题。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，所提方法显著提高了失真条件下的通信-感知权衡，并在鲁棒性和容量方面优于固定位置天线基线。&lt;h4&gt;结论&lt;/h4&gt;MA辅助的CF-ISAC系统在减轻功率放大器非线性失真方面具有明显优势，能有效提升系统性能，为6G网络提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;无蜂窝集成感知与通信(CF-ISAC)架构是6G的一种有前景的使能技术，提供频谱效率和无处不在的覆盖。然而，实际部署中存在硬件缺陷，特别是功率放大器(PA)的非线性失真，这会降低通信和感知性能。为此，我们提出了一种可移动天线(MA)辅助的CF-ISAC系统，以减轻失真并增强鲁棒性。PA非线性被建模为三阶无记忆多项式，其中三阶失真系数(3RDCs)因硬件差异、老化和环境条件而在不同接入点之间变化。我们设计了一种分布式失真感知的最坏情况鲁棒优化框架，明确考虑了3RDCs的不确定性。首先，我们分析PA失真对Cramer-Rao下界(CRLB)和通信速率的最坏情况影响。然后，为解决由此产生的非凸性，我们应用连续凸近似(SCA)来估计3RDCs。在此基础上，我们在发射功率和感知约束下联合优化波束成形和MA位置。为有效解决这个高度非凸问题，我们开发了一种MA使能的自注意力卷积图神经网络(SACGNN)算法。仿真结果表明，我们的方法显著提高了失真条件下的通信-感知权衡，并在鲁棒性和容量方面优于固定位置天线基线，从而突显了MA辅助CF-ISAC系统的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cell-free integrated sensing and communication (CF-ISAC) architecture isa promising enabler for 6G, offering spectrum efficiency and ubiquitouscoverage. However, real deployments suffer from hardware impairments,especially nonlinear distortion from power amplifiers (PAs), which degradesboth communication and sensing. To address this, we propose a movable antenna(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.The PAs nonlinearities are modeled by a third-order memoryless polynomial,where the third-order distortion coefficients (3RDCs) vary across access points(APs) due to hardware differences, aging, and environmental conditions. Wedesign a distributed distortion-aware worst-case robust optimization frameworkthat explicitly incorporates uncertainty in 3RDCs. First, we analyze theworst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)and communication rate. Then, to address the resulting non-convexity, we applysuccessive convex approximation (SCA) for estimating the 3RDCs. With these, wejointly optimize beamforming and MA positions under transmit power and sensingconstraints. To efficiently solve this highly non-convex problem, we develop anMA-enabled self-attention convolutional graph neural network (SACGNN)algorithm. Simulations demonstrate that our method substantially enhances thecommunication-sensing trade-off under distortion and outperforms fixed-positionantenna baselines in terms of robustness and capacity, thereby highlighting theadvantages of MA-aided CF-ISAC systems.</description>
      <author>example@mail.com (Yue Xiu, Yang Zhao, Ran Yang, Zheng Dong, Wanting Lyu, Zeyuan Zhang, Dusit Niyato, Guangyi Liu, Ning Wei)</author>
      <guid isPermaLink="false">2508.13839v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation</title>
      <link>http://arxiv.org/abs/2508.13745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted as a full paper at ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为REARM的新框架，用于改进多模态推荐系统中的对比学习和同构图关系，解决了现有方法在数据稀疏环境下的两个主要局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态推荐系统利用丰富的模态信息（图像和文本描述）提高推荐性能，当前方法依赖图神经网络的结构建模能力取得成功，但在真实场景中常受限于稀疏数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态推荐系统中的两个主要局限性：1)简单多模态特征对比产生的无效表示问题；2)用户兴趣与项目共现间同构图关系探索不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出REARM框架，使用元网络和正交约束策略补充多模态对比学习，过滤模态共享特征噪声并保留模态独特特征中与推荐相关的信息；同时整合新构建的用户兴趣图和项目共现图与现有图进行学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实数据集上的实验表明，REARM优于各种最先进的基线方法；可视化分析显示REARM在区分模态共享和模态独特特征方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;REARM框架有效解决了多模态推荐系统中的特征表示和关系挖掘问题，提高了在数据稀疏环境下的推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态推荐系统专注于利用物品丰富的模态信息（即图像和文本描述）来提高推荐性能。当前方法凭借图神经网络的强大结构建模能力取得了显著成功。然而，这些方法在现实场景中常受限于稀疏数据。尽管采用对比学习和同质图（即同构图）来解决数据稀疏挑战，现有方法仍存在两个主要局限：1)简单的多模态特征对比无法产生有效表示，导致模态共享特征中的噪声和模态独特特征中有价值信息的丢失；2)缺乏对用户兴趣和项目共现之间同构图关系的探索，导致用户-项目交互挖掘不完整。为解决上述局限，我们提出了一个新颖的框架REARM（改进多模态对比学习和同构图关系）。具体而言，我们通过采用元网络和正交约束策略来补充多模态对比学习，过滤模态共享特征中的噪声并保留模态独特特征中与推荐相关的信息。为了有效挖掘同质关系，我们将新构建的用户兴趣图和项目共现图与现有的用户共现图和项目语义图整合进行图学习。在三个真实数据集上的广泛实验证明了REARM相对于各种最先进基线的优越性。我们的可视化进一步显示了REARM在区分模态共享和模态独特特征方面的改进。代码可在https://github.com/MrShouxingMa/REARM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755779&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal recommender system focuses on utilizing rich modal information (i.e., images and textual descriptions) of items to improve recommendationperformance. The current methods have achieved remarkable success with thepowerful structure modeling capability of graph neural networks. However, thesemethods are often hindered by sparse data in real-world scenarios. Althoughcontrastive learning and homography ( i.e., homogeneous graphs) are employed toaddress the data sparsity challenge, existing methods still suffer two mainlimitations: 1) Simple multi-modal feature contrasts fail to produce effectiverepresentations, causing noisy modal-shared features and loss of valuableinformation in modal-unique features; 2) The lack of exploration of thehomograph relations between user interests and item co-occurrence results inincomplete mining of user-item interplay.  To address the above limitations, we propose a novel framework for\textbf{R}\textbf{E}fining multi-mod\textbf{A}l cont\textbf{R}astive learningand ho\textbf{M}ography relations (\textbf{REARM}). Specifically, we complementmulti-modal contrastive learning by employing meta-network and orthogonalconstraint strategies, which filter out noise in modal-shared features andretain recommendation-relevant information in modal-unique features. To minehomogeneous relationships effectively, we integrate a newly constructed userinterest graph and an item co-occurrence graph with the existing userco-occurrence and item semantic graphs for graph learning. The extensiveexperiments on three real-world datasets demonstrate the superiority of REARMto various state-of-the-art baselines. Our visualization further shows animprovement made by REARM in distinguishing between modal-shared andmodal-unique features. Code is available\href{https://github.com/MrShouxingMa/REARM}{here}.</description>
      <author>example@mail.com (Shouxing Ma, Yawen Zeng, Shiqing Wu, Guandong Xu)</author>
      <guid isPermaLink="false">2508.13745v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning</title>
      <link>http://arxiv.org/abs/2508.13716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CaPGNN框架，通过联合自适应缓存算法和资源感知图分区算法，在单服务器多GPU环境下实现了高效的全批量GNN训练，显著减少了通信开销并平衡了计算负载。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在处理图结构数据方面表现出色，但在分布式环境中，全批量GNN训练的可扩展性受到高通信开销和负载不平衡的严重限制。&lt;h4&gt;目的&lt;/h4&gt;设计一个在单服务器多GPU环境下高效并行全批量GNN训练的框架，以减少冗余的GPU间通信和平衡计算工作负载。&lt;h4&gt;方法&lt;/h4&gt;提出联合自适应缓存算法利用CPU和GPU内存减少顶点特征的重复传输；引入资源感知图分区算法根据GPU的异构计算和通信能力动态调整子图大小。&lt;h4&gt;主要发现&lt;/h4&gt;在大型基准数据集上的实验表明，CaPGNN能够将通信成本降低高达96%，并将GNN训练加速高达12.7倍，相比现有方法有显著提升。&lt;h4&gt;结论&lt;/h4&gt;自适应缓存和资源感知分区技术能够有效促进分布式计算环境中全批量GNN训练的可扩展性、高效性和实际部署。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在处理各种现实世界应用中常见的图结构数据方面表现出色。然而，在分布式环境中，全批量GNN训练的可扩展性受到高通信开销和负载不平衡的严重限制。在本文中，我们提出了CaPGNN，这是一个在单服务器多GPU环境下高效并行全批量GNN训练的新颖框架，专门设计用于减少冗余的GPU间通信和平衡计算工作负载。我们提出了一种联合自适应缓存算法，利用CPU和GPU内存来显著减少分区之间顶点特征的重复传输。此外，我们引入了一种资源感知的图分区算法，根据GPU的异构计算和通信能力动态调整子图大小。在大型基准数据集上的大量实验表明，与最先进的方法相比，CaPGNN能够将通信成本降低高达96%，并将GNN训练加速高达12.7倍。我们的结果凸显了自适应缓存和资源感知分区在促进分布式计算环境中全批量GNN训练的可扩展性、高效性和实际部署方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable capabilities in processinggraph-structured data prevalent in various real-world applications. However,the scalability of full-batch GNN training becomes severely limited by highcommunication overhead and load imbalance in distributed environments. In thispaper, we present CaPGNN, a novel framework for efficient parallel full-batchGNN training on single-server with multi-GPU, designed specifically to reduceredundant inter-GPU communication and balance computational workloads. Wepropose a joint adaptive caching algorithm that leverages both CPU and GPUmemory to significantly reduce the repetitive transmission of vertex featuresacross partitions. Additionally, we introduce a resource-aware graphpartitioning algorithm that adjusts subgraph sizes dynamically according to theheterogeneous computational and communication capacities of GPUs. Extensiveexperiments on large-scale benchmark datasets demonstrate that CaPGNNeffectively reduces communication costs by up to 96% and accelerates GNNtraining by up to 12.7 times compared to state-of-the-art approaches. Ourresults highlight the potential of adaptive caching and resource-awarepartitioning to facilitate scalable, efficient, and practical deployment offull-batch GNN training in distributed computing environments.</description>
      <author>example@mail.com (Xianfeng Song, Yi Zou, Zheng Shi)</author>
      <guid isPermaLink="false">2508.13716v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</title>
      <link>http://arxiv.org/abs/2508.13435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SVDformer，一种结合奇异值分解和Transformer架构的新型有向图表示学习框架，有效解决了现有有向图神经网络难以同时捕获方向语义和全局结构模式的问题。&lt;h4&gt;背景&lt;/h4&gt;有向图被广泛用于模拟现实系统中的非对称关系。然而，现有的有向图神经网络由于其各向同性的聚合机制和局部化的过滤机制，往往难以同时捕获方向语义和全局结构模式。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有有向图神经网络的局限性，本文提出SVDformer框架，协同奇异值分解和Transformer架构进行有向感知的图表示学习。&lt;h4&gt;方法&lt;/h4&gt;SVDformer通过多头自注意力机制细化奇异值嵌入，增强关键频谱成分并抑制高频噪声，实现可学习的低通/高通图过滤；同时将奇异向量视为方向投影基，奇异值视为缩放因子，利用Transformer建模传入/传出边缘模式间的多尺度交互，保留边缘方向性。&lt;h4&gt;主要发现&lt;/h4&gt;在六个有向图基准上的大量实验表明，SVDformer在节点分类任务上始终优于最先进的GNN和有向感知基线方法。&lt;h4&gt;结论&lt;/h4&gt;SVDformer为在有向图上学习表示建立了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;有向图被广泛用于模拟现实系统中的非对称关系。然而，现有的有向图神经网络由于其各向同性的聚合机制和局部化的过滤机制，往往难以同时捕获方向语义和全局结构模式。为了解决这一限制，本文提出了SVDformer，这是一种新的框架，协同奇异值分解和Transformer架构进行有向感知的图表示学习。SVDformer首先通过多头自注意力机制细化奇异值嵌入，自适应地增强关键频谱成分，同时抑制高频噪声。这使得可学习的低通/高通图过滤成为可能，而无需频谱核。此外，通过将奇异向量视为方向投影基，奇异值视为缩放因子，SVDformer使用Transformer来通过注意力权重建模传入/传出边缘模式之间的多尺度交互，从而在特征传播过程中明确保留边缘方向性。在六个有向图基准上的大量实验表明，SVDformer在节点分类任务上始终优于最先进的GNN和有向感知基线方法，为在有向图上学习表示建立了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Directed graphs are widely used to model asymmetric relationships inreal-world systems. However, existing directed graph neural networks oftenstruggle to jointly capture directional semantics and global structuralpatterns due to their isotropic aggregation mechanisms and localized filteringmechanisms. To address this limitation, this paper proposes SVDformer, a novelframework that synergizes SVD and Transformer architecture for direction-awaregraph representation learning. SVDformer first refines singular valueembeddings through multi-head self-attention, adaptively enhancing criticalspectral components while suppressing high-frequency noise. This enableslearnable low-pass/high-pass graph filtering without requiring spectralkernels. Furthermore, by treating singular vectors as directional projectionbases and singular values as scaling factors, SVDformer uses the Transformer tomodel multi-scale interactions between incoming/outgoing edge patterns throughattention weights, thereby explicitly preserving edge directionality duringfeature propagation. Extensive experiments on six directed graph benchmarksdemonstrate that SVDformer consistently outperforms state-of-the-art GNNs anddirection-aware baselines on node classification tasks, establishing a newparadigm for learning representations on directed graphs.</description>
      <author>example@mail.com (Jiayu Fang, Zhiqi Shao, S T Boris Choy, Junbin Gao)</author>
      <guid isPermaLink="false">2508.13435v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems</title>
      <link>http://arxiv.org/abs/2508.13371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IAAI-26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LOOP是一种创新的神经符号规划框架，通过神经和符号组件之间的迭代对话解决复杂规划问题，在标准基准测试中表现出色，为构建可靠的自主系统提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;规划是自主系统中最重要的任务之一，小错误可能导致重大故障或经济损失。当前神经规划方法在复杂领域存在缺陷，产生不完整计划；经典规划器虽提供逻辑保证但缺乏灵活性；现有神经符号方法仅做一次性翻译，未充分利用神经与符号组件的协作潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型神经符号规划框架，使神经和符号组件能够像迭代对话一样协作，而非简单翻译，从而解决复杂规划问题。&lt;h4&gt;方法&lt;/h4&gt;创建名为LOOP的神经符号规划框架，集成13个协调神经特征，包括空间关系图神经网络、多代理验证系统、分层分解任务管理和因果记忆系统。LOOP生成PDDL规范，基于符号反馈迭代完善，并从执行轨迹构建因果知识库。&lt;h4&gt;主要发现&lt;/h4&gt;在六个IPC基准领域测试中，LOOP实现85.8%的成功率，显著优于LLM+P(55.0%)、LLM-as-Planner(19.2%)和Tree-of-Thoughts(3.3%)。&lt;h4&gt;结论&lt;/h4&gt;可靠规划的关键不在于选择神经网络或符号推理器，而在于让它们在整个规划过程中真正'对话'。LOOP为构建可信任的自主系统提供了全面蓝图。&lt;h4&gt;翻译&lt;/h4&gt;规划是自主系统中最重要的任务之一，即使是小错误也可能导致重大故障或数百万美元的损失。当前最先进的神经规划方法难以处理复杂领域，产生的计划可能缺少前提条件、目标不一致或出现幻觉。虽然经典规划器能提供逻辑保证，但它们缺乏现代自主系统所需的灵活性和自然语言理解能力。现有的神经符号方法使用从自然语言到形式化计划的一次性翻译，错失了神经和符号组件共同工作和完善解决方案的机会。为解决这一差距，我们开发了LOOP——一种新型神经符号规划框架，将规划视为神经和符号组件之间的迭代对话，而非简单翻译。LOOP集成了13个协调的神经特征，包括用于空间关系的图神经网络、用于基于共识正确性的多代理验证、用于复杂任务管理的分层分解，以及能够从成功和失败中学习的因果记忆。与现有方法不同，LOOP生成PDDL规范，基于符号反馈迭代完善它们，并从执行轨迹构建因果知识库。LOOP在六个标准IPC基准领域进行了评估，实现了85.8%的成功率，而LLM+P为55.0%，LLM-as-Planner为19.2%，Tree-of-Thoughts为3.3%。这项工作表明，可靠规划的关键不在于在神经网络或符号推理器之间做出选择，而在于让它们在整个过程中真正'对话'。LOOP为构建最终可以信任用于关键现实世界应用的自主系统提供了全面的蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning is one of the most critical tasks in autonomous systems, where evena small error can lead to major failures or million-dollar losses. Currentstate-of-the-art neural planning approaches struggle with complex domains,producing plans with missing preconditions, inconsistent goals, andhallucinations. While classical planners provide logical guarantees, they lackthe flexibility and natural language understanding capabilities needed formodern autonomous systems. Existing neuro-symbolic approaches use one-shottranslation from natural language to formal plans, missing the opportunity forneural and symbolic components to work and refine solutions together. Toaddress this gap, we develop LOOP -- a novel neuro-symbolic planning frameworkthat treats planning as an iterative conversation between neural and symboliccomponents rather than simple translation. LOOP integrates 13 coordinatedneural features including graph neural networks for spatial relationships,multi-agent validation for consensus-based correctness, hierarchicaldecomposition for complex task management, and causal memory that learns fromboth successes and failures. Unlike existing approaches, LOOP generates PDDLspecifications, refines them iteratively based on symbolic feedback, and buildsa causal knowledge base from execution traces. LOOP was evaluated on sixstandard IPC benchmark domains, where it achieved 85.8% success rate comparedto LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). Thiswork shows that the key to reliable planning is not in choosing between neuralnetworks or symbolic reasoners but it lies in making them actually ``talk'' toeach other during the entire process. LOOP provides a thorough blueprint forbuilding autonomous systems that can finally be trusted with criticalreal-world applications.</description>
      <author>example@mail.com (Ronit Virwani, Ruchika Suryawanshi)</author>
      <guid isPermaLink="false">2508.13371v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</title>
      <link>http://arxiv.org/abs/2508.10747v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种稀疏的、目标感知的图神经网络表示方法，用于解决大规模规划问题中的挑战，有效提高了算法在大型网格环境中的可扩展性。&lt;h4&gt;背景&lt;/h4&gt;使用深度强化学习结合图神经网络的通用规划方法在PDDL描述的符号规划领域显示出良好的结果。然而，现有方法通常将规划状态表示为完全连接的图，这导致了边信息的组合爆炸和大量稀疏性，特别是在大型网格环境中尤为明显。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在大型规划问题中的可扩展性问题，特别是完全连接图表示导致的内存需求指数级增长和学习不可行的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种稀疏的、目标感知的图神经网络表示方法，选择性编码相关的局部关系并明确整合与目标相关的空间特征。作者还设计了基于PDDL的无人机任务场景，在网格世界中模拟真实的任务执行环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法能够有效扩展到之前使用密集图表示不可行的大型网格尺寸，并显著提高了策略泛化和成功率。&lt;h4&gt;结论&lt;/h4&gt;该方法为解决现实世界的大规模通用规划任务提供了实用基础。&lt;h4&gt;翻译&lt;/h4&gt;使用深度强化学习结合图神经网络的通用规划在由PDDL描述的各种符号规划领域显示出良好的结果。然而，现有方法通常将规划状态表示为完全连接的图，这导致了边信息的组合爆炸，随着问题规模的扩大而出现大量稀疏性，特别是在大型网格环境中尤为明显。这种密集表示稀释了节点级信息，指数级增加了内存需求，最终使得在更大规模问题上学习变得不可行。为解决这些挑战，我们提出了一种稀疏的、目标感知的图神经网络表示，它选择性编码相关的局部关系并明确整合与目标相关的空间特征。我们通过在网格世界中设计基于PDDL的新型无人机任务场景来验证我们的方法，有效模拟了真实的任务执行环境。我们的实验结果表明，我们的方法能够有效扩展到之前使用密集图表示不可行的大型网格尺寸，并显著提高了策略泛化和成功率。我们的发现为解决现实世界的大规模通用规划任务提供了实用基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized planning using deep reinforcement learning (RL) combined withgraph neural networks (GNNs) has shown promising results in various symbolicplanning domains described by PDDL. However, existing approaches typicallyrepresent planning states as fully connected graphs, leading to a combinatorialexplosion in edge information and substantial sparsity as problem scales grow,especially evident in large grid-based environments. This dense representationresults in diluted node-level information, exponentially increases memoryrequirements, and ultimately makes learning infeasible for larger-scaleproblems. To address these challenges, we propose a sparse, goal-aware GNNrepresentation that selectively encodes relevant local relationships andexplicitly integrates spatial features related to the goal. We validate ourapproach by designing novel drone mission scenarios based on PDDL within a gridworld, effectively simulating realistic mission execution environments. Ourexperimental results demonstrate that our method scales effectively to largergrid sizes previously infeasible with dense graph representations andsubstantially improves policy generalization and success rates. Our findingsprovide a practical foundation for addressing realistic, large-scalegeneralized planning tasks.</description>
      <author>example@mail.com (Sangwoo Jeon, Juchul Shin, Gyeong-Tae Kim, YeonJe Cho, Seongwoo Kim)</author>
      <guid isPermaLink="false">2508.10747v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.13584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种时态条件指代视频对象分割模型，通过创新整合现有分割方法、利用文本到视频扩散模型、移除噪声预测模块和设计时间上下文掩模精炼模块，在四个公开基准测试上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;指代视频对象分割(RVOS)旨在根据文本描述分割视频中的特定对象。现有方法过分强调特征提取和时间建模，而忽视了分割头设计，实际上分割头仍有很大改进空间。&lt;h4&gt;目的&lt;/h4&gt;解决现有RVOS方法在分割头设计上的不足，提高边界分割能力，同时简化模型结构并提升性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出时态条件指代视频对象分割模型，整合现有分割方法；2. 利用文本到视频扩散模型进行特征提取；3. 移除传统噪声预测模块避免噪声随机性影响；4. 设计时间上下文掩模精炼(TCMR)模块克服VAE特征提取局限。&lt;h4&gt;主要发现&lt;/h4&gt;1. 分割头设计在RVOS中有很大改进空间；2. 移除噪声预测模块可简化模型同时提高性能；3. TCMR模块能显著提高分割质量而无需复杂设计。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在四个公开的RVOS基准测试上持续取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;指代视频对象分割(RVOS)旨在根据文本描述分割视频中的特定对象。我们观察到，最近的RVOS方法往往过分强调特征提取和时间建模，而相对忽视了分割头的设计。事实上，分割头设计仍有相当大的改进空间。为此，我们提出了一种时态条件指代视频对象分割模型，创新性地整合现有分割方法以有效增强边界分割能力。此外，我们的模型利用文本到视频扩散模型进行特征提取。在此基础上，我们移除了传统的噪声预测模块，以避免噪声的随机性降低分割准确性，从而简化模型同时提高性能。最后，为了克服VAE特征提取能力的局限性，我们设计了一个时间上下文掩模精炼(TCMR)模块，它显著提高了分割质量而无需引入复杂设计。我们在四个公开的RVOS基准测试上评估了我们的方法，它持续取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring Video Object Segmentation (RVOS) aims to segment specific objectsin a video according to textual descriptions. We observe that recent RVOSapproaches often place excessive emphasis on feature extraction and temporalmodeling, while relatively neglecting the design of the segmentation head. Infact, there remains considerable room for improvement in segmentation headdesign. To address this, we propose a Temporal-Conditional Referring VideoObject Segmentation model, which innovatively integrates existing segmentationmethods to effectively enhance boundary segmentation capability. Furthermore,our model leverages a text-to-video diffusion model for feature extraction. Ontop of this, we remove the traditional noise prediction module to avoid therandomness of noise from degrading segmentation accuracy, thereby simplifyingthe model while improving performance. Finally, to overcome the limited featureextraction capability of the VAE, we design a Temporal Context Mask Refinement(TCMR) module, which significantly improves segmentation quality withoutintroducing complex designs. We evaluate our method on four public RVOSbenchmarks, where it consistently achieves state-of-the-art performance.</description>
      <author>example@mail.com (Ruixin Zhang, Jiaqing Fan, Yifan Liao, Qian Qiao, Fanzhang Li)</author>
      <guid isPermaLink="false">2508.13584v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection</title>
      <link>http://arxiv.org/abs/2508.13205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为YOLO11-CR的轻量级高效目标检测模型，用于实时驾驶员疲劳检测。该模型通过引入卷积与注意力融合模块和矩形校准模块，解决了现有视觉疲劳检测方法面临的挑战，在DSM数据集上取得了优异的性能表现。&lt;h4&gt;背景&lt;/h4&gt;驾驶员疲劳检测对智能交通系统至关重要，有助于减少道路交通事故。现有的基于生理学和车辆动力学的方法虽然准确，但通常具有侵入性、依赖硬件，且在实际环境中缺乏鲁棒性。基于视觉的方法提供了一种非侵入性和可扩展的替代方案，但仍面临检测小目标或被遮挡目标困难、多尺度特征建模有限等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉疲劳检测方法面临的挑战，提出一种轻量级、高效的目标检测模型，用于实时疲劳检测。&lt;h4&gt;方法&lt;/h4&gt;提出YOLO11-CR模型，一种专门为实时疲劳检测设计的轻量级高效目标检测模型。引入两个关键模块：1) 卷积与注意力融合模块（CAFM）：将局部CNN特征与基于Transformer的全局上下文结合，以增强特征表达能力；2) 矩形校准模块（RCM）：捕获水平和垂直上下文信息，改善空间定位，特别是对于侧面人脸和手机等小物体。&lt;h4&gt;主要发现&lt;/h4&gt;在DSM数据集上的实验结果表明，YOLO11-CR达到了87.17%的精确率、83.86%的召回率、mAP@50为88.09%、mAP@50-95为55.93%，显著优于基线模型。消融研究进一步验证了CAFM和RCM模块在提高敏感性和定位准确性方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;YOLO11-CR为车载疲劳监测提供了一种实用且高性能的解决方案，具有在实际部署中的强大潜力，并可通过时间建模、多模态数据集成和嵌入式优化进行未来改进。&lt;h4&gt;翻译&lt;/h4&gt;驾驶员疲劳检测对智能交通系统至关重要，因为它在减轻道路交通事故方面发挥着关键作用。虽然基于生理学和车辆动力学的方法提供了准确性，但它们通常是侵入性的、依赖硬件的，并且在真实环境中缺乏鲁棒性。基于视觉的技术提供了一种非侵入性和可扩展的替代方案，但仍面临诸如小目标或被遮挡目标检测能力差以及多尺度特征建模有限等挑战。为了解决这些问题，本文提出了YOLO11-CR，一种专为实时疲劳检测量身定制的轻量级高效目标检测模型。YOLO11-CR引入了两个关键模块：卷积与注意力融合模块（CAFM），它将局部CNN特征与基于Transformer的全局上下文相结合，以增强特征表达能力；以及矩形校准模块（RCM），它捕获水平和垂直上下文信息，以改善空间定位，特别是对于侧面人脸和手机等小目标。在DSM数据集上的实验表明，YOLO11-CR达到了87.17%的精确率、83.86%的召回率、88.09%的mAP@50和55.93%的mAP@50-95，显著优于基线模型。消融研究进一步验证了CAFM和RCM模块在提高敏感性和定位准确性方面的有效性。这些结果表明，YOLO11-CR为车载疲劳监测提供了一种实用且高性能的解决方案，具有强大的实际部署潜力，以及未来涉及时间建模、多模态数据集成和嵌入式优化的改进潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver fatigue detection is of paramount importance for intelligenttransportation systems due to its critical role in mitigating road trafficaccidents. While physiological and vehicle dynamics-based methods offeraccuracy, they are often intrusive, hardware-dependent, and lack robustness inreal-world environments. Vision-based techniques provide a non-intrusive andscalable alternative, but still face challenges such as poor detection of smallor occluded objects and limited multi-scale feature modeling. To address theseissues, this paper proposes YOLO11-CR, a lightweight and efficient objectdetection model tailored for real-time fatigue detection. YOLO11-CR introducestwo key modules: the Convolution-and-Attention Fusion Module (CAFM), whichintegrates local CNN features with global Transformer-based context to enhancefeature expressiveness; and the Rectangular Calibration Module (RCM), whichcaptures horizontal and vertical contextual information to improve spatiallocalization, particularly for profile faces and small objects like mobilephones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves aprecision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of55.93%, outperforming baseline models significantly. Ablation studies furthervalidate the effectiveness of the CAFM and RCM modules in improving bothsensitivity and localization accuracy. These results demonstrate that YOLO11-CRoffers a practical and high-performing solution for in-vehicle fatiguemonitoring, with strong potential for real-world deployment and futureenhancements involving temporal modeling, multi-modal data integration, andembedded optimization.</description>
      <author>example@mail.com (Zhebin Jin, Ligang Dong)</author>
      <guid isPermaLink="false">2508.13205v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Multi-view Clustering via Bi-level Decoupling and Consistency Learning</title>
      <link>http://arxiv.org/abs/2508.13499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为双层解耦与一致性学习框架(BDCL)的新方法，用于多视图聚类，通过探索多视图数据的有效表示来提高特征的簇间判别性和簇内紧凑性。&lt;h4&gt;背景&lt;/h4&gt;多视图聚类是分析多视图数据中潜在模式的有效方法，通过学习多视图特征之间的一致性和互补性可以改进聚类性能，但通常忽略了面向聚类的表示学习。&lt;h4&gt;目的&lt;/h4&gt;进一步探索多视图数据的有效表示，以提高多视图聚类中特征的簇间判别性和簇内紧凑性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含三个模块的框架：1)多视图实例学习模块，通过重构自编码器和对比学习对齐一致信息并保留私有特征；2)特征和簇的双层解耦，增强特征空间和簇空间的判别性；3)一致性学习模块，将样本的不同视图及其邻居视为正对，学习聚类分配的一致性并压缩簇内空间。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的实验结果表明，所提出的BDCL方法与最先进的方法相比具有优越性。&lt;h4&gt;结论&lt;/h4&gt;BDCL框架通过双层解耦和一致性学习有效提升了多视图聚类的性能，代码已公开发布。&lt;h4&gt;翻译&lt;/h4&gt;多视图聚类已被证明是分析多视图数据中潜在模式的有效方法。通过学习多视图特征之间的一致性和互补性可以改进聚类性能，然而，面向聚类的表示学习常常被忽视。在本文中，我们提出了一种新颖的双层解耦与一致性学习框架(BDCL)，以进一步探索多视图数据的有效表示，增强多视图聚类中特征的簇间判别性和簇内紧凑性。我们的框架包含三个模块：1)多视图实例学习模块通过重构自编码器和对比学习对齐一致信息，同时保留视图间的私有特征。2)特征和簇的双层解耦增强了特征空间和簇空间的判别性。3)一致性学习模块将样本的不同视图及其邻居视为正对，学习它们聚类分配的一致性，并进一步压缩簇内空间。在五个基准数据集上的实验结果表明，与最先进的方法相比，所提出的方法具有优越性。我们的代码已发布在https://github.com/LouisDong95/BDCL。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view clustering has shown to be an effective method for analyzingunderlying patterns in multi-view data. The performance of clustering can beimproved by learning the consistency and complementarity between multi-viewfeatures, however, cluster-oriented representation learning is oftenoverlooked. In this paper, we propose a novel Bi-level Decoupling andConsistency Learning framework (BDCL) to further explore the effectiverepresentation for multi-view data to enhance inter-cluster discriminabilityand intra-cluster compactness of features in multi-view clustering. Ourframework comprises three modules: 1) The multi-view instance learning modulealigns the consistent information while preserving the private features betweenviews through reconstruction autoencoder and contrastive learning. 2) Thebi-level decoupling of features and clusters enhances the discriminability offeature space and cluster space. 3) The consistency learning module treats thedifferent views of the sample and their neighbors as positive pairs, learns theconsistency of their clustering assignments, and further compresses theintra-cluster space. Experimental results on five benchmark datasetsdemonstrate the superiority of the proposed method compared with the SOTAmethods. Our code is published on https://github.com/LouisDong95/BDCL.</description>
      <author>example@mail.com (Shihao Dong, Yuhui Zheng, Huiying Xu, Xinzhong Zhu)</author>
      <guid isPermaLink="false">2508.13499v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2508.13433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STPFormer是一种时空模式感知Transformer，通过统一且可解释的表示学习实现了交通预测的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;时空交通预测具有挑战性，因为涉及复杂的时间模式、动态的空间结构和多样的输入格式。&lt;h4&gt;目的&lt;/h4&gt;提出STPFormer模型，解决基于Transformer的模型在时间编码僵化和时空融合不足方面的问题。&lt;h4&gt;方法&lt;/h4&gt;STPFormer整合了四个模块：时间位置聚合器(TPA)用于模式感知的时间编码，空间序列聚合器(SSA)用于顺序空间学习，时空图匹配(STGM)用于跨域对齐，以及注意力混合器用于多尺度融合。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上的实验表明，STPFormer持续取得了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;消融研究和可视化证实了STPFormer模型的有效性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;时空交通预测具有挑战性，因为存在复杂的时间模式、动态的空间结构和多样的输入格式。尽管基于Transformer的模型提供了强大的全局建模能力，但它们通常面临时间编码僵化和时空融合不足的问题。我们提出了STPFormer，一种时空模式感知Transformer，通过统一且可解释的表示学习实现了最先进的性能。它整合了四个模块：用于模式感知时间编码的时间位置聚合器(TPA)，用于顺序空间学习的空间序列聚合器(SSA)，用于跨域对齐的时空图匹配(STGM)，以及用于多尺度融合的注意力混合器。在五个真实世界数据集上的实验表明，STPFormer持续取得了新的最先进结果，消融和可视化证实了其有效性和通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-temporal traffic forecasting is challenging due to complex temporalpatterns, dynamic spatial structures, and diverse input formats. AlthoughTransformer-based models offer strong global modeling, they often struggle withrigid temporal encoding and weak space-time fusion. We propose STPFormer, aSpatio-Temporal Pattern-Aware Transformer that achieves state-of-the-artperformance via unified and interpretable representation learning. Itintegrates four modules: Temporal Position Aggregator (TPA) for pattern-awaretemporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatiallearning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,and an Attention Mixer for multi-scale fusion. Experiments on five real-worlddatasets show that STPFormer consistently sets new SOTA results, with ablationand visualizations confirming its effectiveness and generalizability.</description>
      <author>example@mail.com (Jiayu Fang, Zhiqi Shao, S T Boris Choy, Junbin Gao)</author>
      <guid isPermaLink="false">2508.13433v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>NucEL: Single-Nucleotide ELECTRA-Style Genomic Pre-training for Efficient and Interpretable Representations</title>
      <link>http://arxiv.org/abs/2508.13191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NucEL是首个用于基因组基础模型的ELECTRA风格预训练框架，解决了传统MLM方法的局限性，在多种基因组下游任务上达到最先进性能，并能与规模大25倍的模型竞争。&lt;h4&gt;背景&lt;/h4&gt;在基因组序列上预训练大型语言模型是学习生物学有意义表征的强大方法，但现有的掩盖语言建模(MLM)方法如DNABERT和核苷酸变换器(NT)存在部分标记监督、预训练/微调不匹配和高计算成本等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预训练框架NucEL，解决传统MLM方法的局限性，提供更全面、高效的基因组序列表征学习方法。&lt;h4&gt;方法&lt;/h4&gt;NucEL采用ELECTRA风格框架，使用判别器识别生成器改变的标记，提供全面的标记级监督；整合ModernBERT的混合局部-全局注意力和flash注意力；采用单核苷酸标记化而非6-mer标记化；在人类基因组上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;NucEL在调控元件识别、转录因子结合预测、开放染色质分类和组蛋白修饰谱分析等下游任务上达到最先进结果，超过类似大小的基于MLM的模型，并能与规模大25倍的模型竞争；消融研究确定了最佳标记化和屏蔽策略；注意力分析显示NucEL在捕获生物学相关基序方面优于NT。&lt;h4&gt;结论&lt;/h4&gt;ELECTRA风格的预训练是基因组表征学习的一种高效、有效的策略，对基因组研究有广泛影响。&lt;h4&gt;翻译&lt;/h4&gt;在基因组序列上预训练大型语言模型是学习生物学有意义表征的强大方法。掩盖语言建模(MLM)方法，如DNABERT和核苷酸变换器(NT)，虽然表现良好，但存在部分标记监督、预训练/微调不匹配和高计算成本等问题。我们引入了NucEL，这是首个用于基因组基础模型的ELECTRA风格预训练框架，解决了这些局限性。通过使用判别器识别生成器改变的标记，NucEL提供全面的标记级监督，改进了MLM部分监督的效率。整合ModernBERT的混合局部-全局注意力和flash注意力，NucEL为基因组建模提供了优化的BERT架构。与6-mer标记化不同，NucEL使用单核苷酸标记，实现细粒度分辨率，提高了效率和可解释性。在人类基因组上预训练后，NucEL在多种下游任务上取得了最先进的结果，包括调控元件识别(如启动子、增强子)、转录因子结合预测、开放染色质分类和组蛋白修饰谱分析，超过了类似大小的基于MLM的模型，并能与规模大25倍的模型(如NT)竞争。消融研究突出了ELECTRA风格DNA预训练的最佳标记化和屏蔽策略。注意力分析显示，与NT相比，NucEL在捕获生物学相关基序方面表现更优，为层次学习和调控元件建模提供了见解。这些研究结果表明，ELECTRA风格的预训练是基因组表征学习的一种高效、有效的策略，对基因组研究具有广泛影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training large language models on genomic sequences is a powerfulapproach for learning biologically meaningful representations. Masked languagemodeling (MLM) methods, such as DNABERT and Nucleotide Transformer (NT),achieve strong performance but suffer from partial token supervision,pre-training/fine-tuning mismatches, and high computational costs. We introduceNucEL, the first ELECTRA-style pre-training framework for genomic foundationmodels, addressing these limitations. Using a discriminator to identify tokensaltered by a generator, NucEL provides comprehensive token-level supervisionacross all sequence positions, improving efficiency over the partialsupervision of MLM. Incorporating ModernBERT's hybrid local-global attentionand flash attention, NucEL offers an optimized BERT architecture for genomicmodeling. Unlike 6-mer tokenization, NucEL uses single-nucleotide tokens forfine-grained resolution, boosting both efficiency and interpretability.Pre-trained on the human genome, NucEL achieves state-of-the-art results ondiverse downstream tasks -- regulatory element identification (e.g., promoters,enhancers), transcription factor binding prediction, open chromatinclassification, and histone modification profiling -- surpassing similarlysized MLM-based models and rivaling models 25x larger, such as NT. Ablationstudies highlight optimal tokenization and masking strategies for ELECTRA-styleDNA pre-training. Attention analysis reveals NucEL's superior capture ofbiologically relevant motifs compared to NT, providing insights intohierarchical learning and regulatory element modeling. These findingsdemonstrate ELECTRA-style pre-training as an efficient, effective strategy forgenomic representation learning with broad implications for genomic research.</description>
      <author>example@mail.com (Ke Ding, Brian Parker, Jiayu Wen)</author>
      <guid isPermaLink="false">2508.13191v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference</title>
      <link>http://arxiv.org/abs/2508.13439v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种结构化提示和知识蒸馏框架，通过协调两个大型视觉语言模型(GPT-4o和o3-mini)生成高质量交通场景注释，并成功将知识蒸馏到轻量级模型VISTA中，实现了在低分辨率视频上的场景理解和风险推断。&lt;h4&gt;背景&lt;/h4&gt;高速公路场景理解和稳健的交通风险推断对智能交通系统和自动驾驶至关重要，但传统方法在可扩展性和泛化能力方面存在局限，特别是在真实世界的复杂动态条件下。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统方法的局限性，作者引入一个新颖框架，能够自动生成高质量的交通场景注释和上下文风险评估。&lt;h4&gt;方法&lt;/h4&gt;框架通过结构化思维链策略协调两个大型视觉语言模型(GPT-4o和o3-mini)生成丰富多角度输出，这些输出作为知识丰富的伪注释用于监督微调更小的学生VLM，最终得到紧凑型3B规模模型VISTA。&lt;h4&gt;主要发现&lt;/h4&gt;VISTA模型能够理解低分辨率交通视频并生成语义忠实、风险感知的标题，尽管参数显著减少，但在BLEU-4、METEOR、ROUGE-L和CIDEr等指标上表现出色，证明轻量级模型也能捕获复杂推理能力。&lt;h4&gt;结论&lt;/h4&gt;有效的知识蒸馏和结构化多代理监督能够使轻量级VLMs具备复杂推理能力，VISTA的紧凑架构便于在边缘设备上高效部署，实现实时风险监控而无需大量基础设施升级。&lt;h4&gt;翻译&lt;/h4&gt;综合的高速公路场景理解和稳健的交通风险推断对于推进智能交通系统和自动驾驶至关重要。传统方法通常难以扩展和泛化，特别是在真实世界环境的复杂动态条件下。为应对这些挑战，我们引入了一种新颖的结构化提示和知识蒸馏框架，能够自动生成高质量的交通场景注释和上下文风险评估。我们的框架通过结构化的思维链策略协调两个大型视觉语言模型(GPT-4o和o3-mini)，生成丰富、多角度的输出。这些输出作为知识丰富的伪注释，用于监督微调一个更小的学生VLM。得到的紧凑型3B规模模型VISTA（智能场景和交通分析视觉模型）能够理解低分辨率交通视频并生成语义忠实、风险感知的标题。尽管参数显著减少，VISTA在既定的标题指标（BLEU-4、METEOR、ROUGE-L和CIDEr）上与其教师模型相比表现出色。这表明有效的知识蒸馏和结构化多代理监督能够使轻量级VLMs捕获复杂的推理能力。VISTA的紧凑架构便于在边缘设备上高效部署，无需大量基础设施升级即可实现实时风险监控。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehensive highway scene understanding and robust traffic risk inferenceare vital for advancing Intelligent Transportation Systems (ITS) and autonomousdriving. Traditional approaches often struggle with scalability andgeneralization, particularly under the complex and dynamic conditions ofreal-world environments. To address these challenges, we introduce a novelstructured prompting and knowledge distillation framework that enablesautomatic generation of high-quality traffic scene annotations and contextualrisk assessments. Our framework orchestrates two large Vision-Language Models(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategyto produce rich, multi-perspective outputs. These outputs serve asknowledge-enriched pseudo-annotations for supervised fine-tuning of a muchsmaller student VLM. The resulting compact 3B-scale model, named VISTA (Visionfor Intelligent Scene and Traffic Analysis), is capable of understandinglow-resolution traffic videos and generating semantically faithful, risk-awarecaptions. Despite its significantly reduced parameter count, VISTA achievesstrong performance across established captioning metrics (BLEU-4, METEOR,ROUGE-L, and CIDEr) when benchmarked against its teacher models. Thisdemonstrates that effective knowledge distillation and structured multi-agentsupervision can empower lightweight VLMs to capture complex reasoningcapabilities. The compact architecture of VISTA facilitates efficientdeployment on edge devices, enabling real-time risk monitoring withoutrequiring extensive infrastructure upgrades.</description>
      <author>example@mail.com (Yunxiang Yang, Ningning Xu, Jidong J. Yang)</author>
      <guid isPermaLink="false">2508.13439v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism</title>
      <link>http://arxiv.org/abs/2508.13228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 International Joint Conference on Neural Networks (IJCNN 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PreSem-Surf，一种基于神经辐射场(NeRF)框架的优化方法，能从RGB-D序列中快速重建高质量场景表面&lt;h4&gt;背景&lt;/h4&gt;现有NeRF框架在场景重建方面存在时间或质量问题，需要改进方法&lt;h4&gt;目的&lt;/h4&gt;开发能快速重建高质量场景表面的方法，整合RGB、深度和语义信息提高重建性能&lt;h4&gt;方法&lt;/h4&gt;结合SG-MLP采样结构与PR-MLP进行体素预渲染，采用渐进式语义建模提取不同精度语义信息，整合多源信息提升重建效果&lt;h4&gt;主要发现&lt;/h4&gt;在七个合成场景和六个评估指标上，PreSem-Surf在C-L1、F-score和IoU方面表现最佳，在NC、Accuracy和Completeness方面保持竞争力&lt;h4&gt;结论&lt;/h4&gt;PreSem-Surf方法有效且具有实际应用价值，能快速重建高质量场景表面&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了PreSem-Surf，一种基于神经辐射场(NeRF)框架的优化方法，能够从RGB-D序列中快速重建高质量的场景表面。该方法整合RGB、深度和语义信息以提高重建性能。具体而言，引入了一种结合PR-MLP(预调节多层感知器)的新型SG-MLP采样结构用于体素预渲染，使模型能够更早更好地捕获场景相关信息，并更好地区分噪声和局部细节。此外，采用渐进式语义建模以不断提高精度提取语义信息，同时减少训练时间并增强场景理解。在七个合成场景和六个评估指标上的实验表明，PreSem-Surf在C-L1、F-score和IoU方面取得了最佳性能，同时在NC、Accuracy和Completeness方面保持了具有竞争力的结果，证明了其有效性和实际适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从RGB-D序列中快速重建高质量表面的问题，特别是在NeRF框架下平衡重建的平滑度和准确性。这个问题在现实中非常重要，因为随着3D场景理解和虚拟环境可视化需求的增长，传统3D重建方法存在精度有限、对输入质量敏感和资源消耗高等问题，而NeRF虽然表现优异但仍面临对高质量数据依赖和高资源消耗的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计中借鉴了多个现有工作，包括NeRF的基本框架、3DGS的实时渲染特性、SDF的表示方法，以及MSeg3D、SNI-SLAM和Kimera等模型中利用语义信息的策略。作者针对NeRF的局限性，通过结合RGB信息、深度信息和语义信息，设计了SG-MLP结构和渐进式语义建模策略，以解决高质量数据依赖和高资源消耗的问题，同时提高采样效率和渲染质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合RGB信息、深度信息和语义信息，在较短时间内重建高质量表面，并利用分层预渲染和渐进式语义建模来提高效率和质量。整体实现流程包括：1) SG-MLP结构使用PR MLP进行均匀预采样，然后采用分层渐进采样策略，通过动态阈值选择关键区域；2) 渐进式语义建模策略分为粗粒度渲染(快速捕捉场景整体结构)和细粒度渲染(捕捉细节)；3) 使用综合损失函数联合优化渲染质量、几何表示和语义信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出SG-MLP结构，通过分层预渲染机制为后续渲染提供关键指导；2) 设计渐进式语义建模策略，根据场景语义逐步细化建模过程；3) 整合RGB、深度和语义信息，提高重建质量。相比之前的工作，PreSem-Surf在保持高质量重建的同时有效减少了训练时间，更好地平衡了重建的平滑度与精度，并且对低质量数据有更好的适应能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PreSem-Surf通过创新的SG-MLP结构和渐进式语义建模策略，有效整合RGB、深度和语义信息，在显著减少训练时间的同时实现了高质量的3D场景表面重建，平衡了重建的平滑度与精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes PreSem-Surf, an optimized method based on the NeuralRadiance Field (NeRF) framework, capable of reconstructing high-quality scenesurfaces from RGB-D sequences in a short time. The method integrates RGB,depth, and semantic information to improve reconstruction performance.Specifically, a novel SG-MLP sampling structure combined with PR-MLP(Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering,allowing the model to capture scene-related information earlier and betterdistinguish noise from local details. Furthermore, progressive semanticmodeling is adopted to extract semantic information at increasing levels ofprecision, reducing training time while enhancing scene understanding.Experiments on seven synthetic scenes with six evaluation metrics show thatPreSem-Surf achieves the best performance in C-L1, F-score, and IoU, whilemaintaining competitive results in NC, Accuracy, and Completeness,demonstrating its effectiveness and practical applicability.</description>
      <author>example@mail.com (Yuyan Ye, Hang Xu, Yanghang Huang, Jiali Huang, Qian Weng)</author>
      <guid isPermaLink="false">2508.13228v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2508.05064v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;高斯飞溅是一种新兴的实时3D场景表示技术，结合大型语言模型后为文本条件生成和语义理解开辟了新可能性，本文对此交叉领域进行了全面综述。&lt;h4&gt;背景&lt;/h4&gt;高斯飞溅作为神经辐射场的高效替代方案，能高保真度渲染复杂场景，已在场景重建、机器人和交互式内容创作等领域取得应用，但语言引导与高斯飞溅结合的研究缺乏全面概述。&lt;h4&gt;目的&lt;/h4&gt;提供结合语言引导与3D高斯飞溅研究的结构化综述，介绍理论基础、集成策略和实际用例，指出限制和未来挑战。&lt;h4&gt;方法&lt;/h4&gt;对当前结合语言引导和3D高斯飞溅的研究工作进行结构化回顾，分析理论基础、集成策略和实际应用案例。&lt;h4&gt;主要发现&lt;/h4&gt;语言引导的高斯飞溅为文本条件生成、编辑和语义场景理解提供了新可能性；当前面临计算瓶颈、泛化能力有限和语义注释数据稀缺等挑战。&lt;h4&gt;结论&lt;/h4&gt;语言引导的3D高斯飞溅是一个新兴且有前景的研究领域，需要进一步解决计算效率、泛化能力和数据可用性问题，以推动3D场景理解的进步。&lt;h4&gt;翻译&lt;/h4&gt;高斯飞溅已成为实时3D场景表示的革命性技术，提供了比神经辐射场更高效且表现力强的替代方案。它能够高保真度渲染复杂场景，推动了场景重建、机器人和交互式内容创作等领域的进展。最近，大型语言模型和语言嵌入被整合到高斯飞溅流程中，为文本条件生成、编辑和语义场景理解开辟了新可能性。尽管有这些进展，对于这一新兴交叉领域的全面概述仍然缺乏。本综述对当前结合语言引导与3D高斯飞溅的研究工作进行了结构化回顾，详细介绍了理论基础、集成策略和实际用例。我们强调了关键限制，如计算瓶颈、泛化能力有限，以及语义注释的3D高斯数据稀缺等问题，并概述了使用高斯飞溅推进语言引导的3D场景理解的开放挑战和未来方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将语言嵌入技术与3D高斯泼溅(3DGS)有效结合，以实现更高级的3D场景理解和交互。这个问题非常重要，因为3D场景理解是机器人、自动驾驶和虚拟现实等领域的核心技术，而现有的3D表示方法缺乏与自然语言的交互能力。将语言模型与3D场景结合可以创建能理解自然语言描述、回答问题并执行语言指导操作的智能系统，对于未来需要与3D环境交互的AI系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D高斯泼溅在实时渲染方面优于NeRF但缺乏语义理解能力，然后借鉴了LLMs和VLMs在2D任务中的成功应用，探索将其扩展到3D场景。作者对现有研究进行了系统综述，将方法分为静态和动态场景两大类，并提出了语言特征量化、语义嵌入、多尺度语义推理等创新方法。作者确实借鉴了大量现有工作，包括3D高斯泼溅技术、从Word2Vec到BERT的自然语言处理技术、CLIP等对比学习模型，以及现有的多模态学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将自然语言理解能力与3D场景表示技术相结合，创建能够通过自然语言进行交互和理解的3D场景系统。整体流程包括：1)使用3D高斯泼溅表示场景；2)用预训练语言模型提取语言特征；3)通过各种方法将语言特征与3D高斯对齐（如特征量化、直接嵌入、自编码器压缩、网格映射）；4)通过监督学习优化语言-3D对齐关系；5)实现自然语言查询与场景交互；6)结合语言和几何信息渲染3D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语言嵌入与3DGS的系统整合；2)多种技术路线的分类与比较；3)语言特征压缩与优化方法；4)多尺度语义推理能力；5)训练-free方法。相比之前工作，这篇论文的不同之处在于：1)提供了全面的综述而非专注特定方法；2)强调实际应用场景；3)整合了最新的LLMs、VLMs和MLLMs技术；4)探讨了动态场景处理；5)特别关注性能与效率的平衡，支持实时交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统性地综述了将语言嵌入技术与3D高斯泼溅相结合的方法，为创建能够通过自然语言进行交互和理解的智能3D场景系统提供了全面的理论框架和实践指南，推动了3D场景理解从单纯的几何表示向语义丰富的交互式系统的转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaussian Splatting has rapidly emerged as a transformative technique forreal-time 3D scene representation, offering a highly efficient and expressivealternative to Neural Radiance Fields (NeRF). Its ability to render complexscenes with high fidelity has enabled progress across domains such as scenereconstruction, robotics, and interactive content creation. More recently, theintegration of Large Language Models (LLMs) and language embeddings intoGaussian Splatting pipelines has opened new possibilities for text-conditionedgeneration, editing, and semantic scene understanding. Despite these advances,a comprehensive overview of this emerging intersection has been lacking. Thissurvey presents a structured review of current research efforts that combinelanguage guidance with 3D Gaussian Splatting, detailing theoreticalfoundations, integration strategies, and real-world use cases. We highlight keylimitations such as computational bottlenecks, generalizability, and thescarcity of semantically annotated 3D Gaussian data and outline open challengesand future directions for advancing language-guided 3D scene understandingusing Gaussian Splatting.</description>
      <author>example@mail.com (Mahmoud Chick Zaouali, Todd Charter, Yehor Karpichev, Brandon Haworth, Homayoun Najjaran)</author>
      <guid isPermaLink="false">2508.05064v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Backdooring Self-Supervised Contrastive Learning by Noisy Alignment</title>
      <link>http://arxiv.org/abs/2508.14015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Noisy Alignment（NA）的数据投毒对比学习（DPCL）方法，通过抑制有毒图像中的噪声成分来防御后门攻击，达到了最先进的性能同时保持干净数据的准确性。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习（CL）能有效从未标记的数据中学习可迁移表示，但这种方法容易受到数据投毒后门攻击（DPCLs）的影响，攻击者可以在预训练数据集中注入有毒图像，导致编码器在下游任务中表现出有针对性的错误行为。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的DPCL方法，明确抑制有毒图像中的噪声成分，以防御后门攻击。&lt;h4&gt;方法&lt;/h4&gt;受强大的训练可控CL攻击启发，识别并提取噪声对齐的关键目标，将其应用于数据投毒场景；通过策略性地操作对比学习的随机裁剪机制实现噪声对齐，将此过程制定为具有理论推导最优参数的图像布局优化问题。&lt;h4&gt;主要发现&lt;/h4&gt;Noisy Alignment方法简单但有效，与现有DPCLs相比达到最先进性能，同时保持干净数据的准确性，并且对常见的后门防御具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Noisy Alignment是一种有效的DPCL防御方法，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;自监督对比学习（CL）能有效从未标记的数据中学习可迁移表示，这些数据包含图像或图像文本对，但它容易受到数据投毒后门攻击（DPCLs）的影响。攻击者可以在预训练数据集中注入有毒图像，导致被破坏的CL编码器在下游任务中表现出有针对性的错误行为。然而，现有的DPCLs由于其依赖于后门和目标对象之间脆弱的隐式共现关系，以及对有毒图像中判别性特征的抑制不足，因此效果有限。我们提出了Noisy Alignment（NA），一种DPCL方法，明确抑制有毒图像中的噪声成分。受强大的训练可控CL攻击启发，我们识别并提取了噪声对齐的关键目标，并将其有效应用于数据投毒场景。我们的方法通过策略性地操作对比学习的随机裁剪机制来实现噪声对齐，将此过程制定为具有理论推导最优参数的图像布局优化问题。得到的方法简单而有效，与现有DPCLs相比达到了最先进的性能，同时保持了干净数据的准确性。此外，Noisy Alignment对常见的后门防御具有鲁棒性。代码可在https://github.com/jsrdcht/Noisy-Alignment找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised contrastive learning (CL) effectively learns transferablerepresentations from unlabeled data containing images or image-text pairs butsuffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversarycan inject poisoned images into pretraining datasets, causing compromised CLencoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs,however, achieve limited efficacy due to their dependence on fragile implicitco-occurrence between backdoor and target object and inadequate suppression ofdiscriminative features in backdoored images. We propose Noisy Alignment (NA),a DPCL method that explicitly suppresses noise components in poisoned images.Inspired by powerful training-controllable CL attacks, we identify and extractthe critical objective of noisy alignment, adapting it effectively intodata-poisoning scenarios. Our method implements noisy alignment bystrategically manipulating contrastive learning's random cropping mechanism,formulating this process as an image layout optimization problem withtheoretically derived optimal parameters. The resulting method is simple yeteffective, achieving state-of-the-art performance compared to existing DPCLs,while maintaining clean-data accuracy. Furthermore, Noisy Alignmentdemonstrates robustness against common backdoor defenses. Codes can be found athttps://github.com/jsrdcht/Noisy-Alignment.</description>
      <author>example@mail.com (Tuo Chen, Jie Gui, Minjing Dong, Ju Jia, Lanting Fang, Jian Liu)</author>
      <guid isPermaLink="false">2508.14015v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.13712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DCMamba的新型半监督医学图像分割框架，通过从数据、网络和特征三个维度增强多样性，显著提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割的高质量标注数据获取繁琐且昂贵，半监督分割技术可通过利用未标记数据生成伪标签来减轻这一负担，而Mamba等先进状态空间模型在处理长程依赖关系方面表现出色。&lt;h4&gt;目的&lt;/h4&gt;探索Mamba在半监督医学图像分割中的潜力，提出一个增强多样性协作Mamba框架以提升分割性能。&lt;h4&gt;方法&lt;/h4&gt;从数据角度开发基于Mamba扫描特性的补丁级别弱-强混合增强；从网络角度引入多样扫描协作模块，利用不同扫描方向的预测差异；从特征角度采用不确定性加权对比学习机制增强特征表示多样性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明DCMamba显著优于其他半监督医学图像分割方法，在Synapse数据集上使用20%标记数据时，比最新的基于SSM的方法提高了6.69%。&lt;h4&gt;结论&lt;/h4&gt;DCMamba框架通过多维度增强多样性，有效提升了半监督医学图像分割的性能，为解决标注数据稀缺问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;获取高质量的医学图像分割标注数据是繁琐且昂贵的。半监督分割技术通过利用未标记数据生成伪标签来减轻这一负担。最近，以Mamba为代表的先进状态空间模型已显示出有效处理长程依赖关系的能力。这促使我们探索它们在半监督医学图像分割中的潜力。在本文中，我们提出了一种新的增强多样性协作Mamba框架（即DCMamba），用于半监督医学图像分割，该框架从数据、网络和特征角度探索和利用多样性。首先，从数据角度，我们开发了基于Mamba扫描建模特性的补丁级别弱-强混合增强。此外，从网络角度，我们引入了一个多样扫描协作模块，该模块可以受益于不同扫描方向产生的预测差异。此外，从特征角度，我们采用不确定性加权对比学习机制来增强特征表示的多样性。实验证明，我们的DCMamba显著优于其他半监督医学图像分割方法，例如，在使用20%标记数据的Synapse数据集上，比最新的基于SSM的方法提高了6.69%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acquiring high-quality annotated data for medical image segmentation istedious and costly. Semi-supervised segmentation techniques alleviate thisburden by leveraging unlabeled data to generate pseudo labels. Recently,advanced state space models, represented by Mamba, have shown efficienthandling of long-range dependencies. This drives us to explore their potentialin semi-supervised medical image segmentation. In this paper, we propose anovel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) forsemi-supervised medical image segmentation, which explores and utilizes thediversity from data, network, and feature perspectives. Firstly, from the dataperspective, we develop patch-level weak-strong mixing augmentation withMamba's scanning modeling characteristics. Moreover, from the networkperspective, we introduce a diverse-scan collaboration module, which couldbenefit from the prediction discrepancies arising from different scanningdirections. Furthermore, from the feature perspective, we adopt anuncertainty-weighted contrastive learning mechanism to enhance the diversity offeature representation. Experiments demonstrate that our DCMamba significantlyoutperforms other semi-supervised medical image segmentation methods, e.g.,yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20%labeled data.</description>
      <author>example@mail.com (Shumeng Li, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao)</author>
      <guid isPermaLink="false">2508.13712v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</title>
      <link>http://arxiv.org/abs/2508.13676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MHSNet的多层级身份验证框架，用于解决第三方简历与公司人才库简历间的重复检测问题。&lt;h4&gt;背景&lt;/h4&gt;招聘人员需要从第三方网站（如LinkedIn、Indeed）持续搜索简历来维持公司人才库，但这些获取的简历往往不完整且不准确。&lt;h4&gt;目的&lt;/h4&gt;提高第三方简历质量，丰富公司人才库，通过在获取的简历与公司现有人才库中的简历之间进行重复检测来实现。&lt;h4&gt;方法&lt;/h4&gt;提出MHSNet框架，使用对比学习微调BGE-M3模型，利用微调后的Mixture-of-Experts（MoE）生成简历的多级稀疏和密集表示，并计算相应的多级语义相似性。同时采用状态感知的MoE来处理各种不完整的简历。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果验证了MHSNet在处理简历重复检测问题上的有效性。&lt;h4&gt;结论&lt;/h4&gt;MHSNet能够有效解决简历文本的语义复杂性、结构异质性和信息不完整性带来的重复检测挑战，提高第三方简历质量。&lt;h4&gt;翻译&lt;/h4&gt;为了维持公司的人才库，招聘人员需要持续从第三方网站（如LinkedIn、Indeed）搜索简历。然而，获取的简历往往不完整且不准确。为了提高第三方简历质量并丰富公司人才库，有必要在获取的简历与公司人才库中已有的简历之间进行重复检测。由于简历文本的语义复杂性、结构异质性和信息不完整性，此类重复检测具有挑战性。为此，我们提出了MHSNet，一个多层级身份验证框架，使用对比学习微调BGE-M3。微调后的Mixture-of-Experts（MoE）为简历生成多级稀疏和密集表示，使计算相应的多级语义相似性成为可能。此外，MHSNet中采用了状态感知的Mixture-of-Experts（MoE）来处理各种不完整的简历。实验结果验证了MHSNet的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To maintain the company's talent pool, recruiters need to continuously searchfor resumes from third-party websites (e.g., LinkedIn, Indeed). However,fetched resumes are often incomplete and inaccurate. To improve the quality ofthird-party resumes and enrich the company's talent pool, it is essential toconduct duplication detection between the fetched resumes and those already inthe company's talent pool. Such duplication detection is challenging due to thesemantic complexity, structural heterogeneity, and information incompletenessof resume texts. To this end, we propose MHSNet, an multi-level identityverification framework that fine-tunes BGE-M3 using contrastive learning. Withthe fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse anddense representations for resumes, enabling the computation of correspondingmulti-level semantic similarities. Moreover, the state-aware Mixture-of-Experts(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimentalresults verify the effectiveness of MHSNet</description>
      <author>example@mail.com (Yu Li, Zulong Chen, Wenjian Xu, Hong Wen, Yipeng Yu, Man Lung Yiu, Yuyu Yin)</author>
      <guid isPermaLink="false">2508.13676v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>A Generalized Learning Framework for Self-Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.13596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种广义学习框架（GLF），统一了现有的自监督对比学习方法，并引入了自适应分布校准（ADC）方法来提高特征空间的质量。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习（SSCL）在多个下游任务中表现出优越性，但现有方法缺乏统一的框架来理解和改进。&lt;h4&gt;目的&lt;/h4&gt;将标准SSCL方法推广到广义学习框架（GLF）中，并设计一种有效的约束部分来提高特征空间的质量。&lt;h4&gt;方法&lt;/h4&gt;分析了BYOL、Barlow Twins和SwAV三种现有SSCL方法，提出包含对齐部分和约束部分的GLF框架。通过迭代捕捉锚点与其他样本之间的动态关系，设计了自适应分布校准（ADC）方法，确保在原始输入空间中靠近或远离锚点的样本在特征空间中也相应地靠近或远离锚点。&lt;h4&gt;主要发现&lt;/h4&gt;设计GLF的约束部分时需要考虑两个洞见：类内紧凑性和类间可分性，它们衡量特征空间如何保留输入的类别信息。ADC方法能够有效提高特征空间的这些性质。&lt;h4&gt;结论&lt;/h4&gt;理论分析和实证评估都证明了ADC方法的优越性，它是一种即插即用的方法，能够有效地提高自监督对比学习的效果。&lt;h4&gt;翻译&lt;/h4&gt;自监督对比学习（SSCL）最近在多个下游任务中表现出优越性。在本文中，我们将标准的SSCL方法推广到一个包含对齐部分和约束部分的广义学习框架（GLF）中。我们分析了三种现有的SSCL方法：BYOL、Barlow Twins和SwAV，并展示了它们可以通过不同的约束部分选择在GLF下得到统一。我们进一步提出了经验分析和理论分析，为设计GLF的约束部分提供了两个洞见：类内紧凑性和类间可分性，它们衡量特征空间如何保留输入的类别信息。然而，由于SSCL无法使用标签，设计满足这些性质的约束部分具有挑战性。为了解决这个问题，我们考虑通过迭代捕捉锚点与其他样本之间的动态关系来诱导类内紧凑性和类间可分性，并提出了一种即插即用的方法称为自适应分布校准（ADC），确保在原始输入空间中靠近或远离锚点的样本在特征空间中也相应地靠近或远离锚点。理论分析和实证评估都证明了ADC的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised contrastive learning (SSCL) has recently demonstratedsuperiority in multiple downstream tasks. In this paper, we generalize thestandard SSCL methods to a Generalized Learning Framework (GLF) consisting oftwo parts: the aligning part and the constraining part. We analyze threeexisting SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can beunified under GLF with different choices of the constraining part. We furtherpropose empirical and theoretical analyses providing two insights intodesigning the constraining part of GLF: intra-class compactness and inter-classseparability, which measure how well the feature space preserves the classinformation of the inputs. However, since SSCL can not use labels, it ischallenging to design a constraining part that satisfies these properties. Toaddress this issue, we consider inducing intra-class compactness andinter-class separability by iteratively capturing the dynamic relationshipbetween anchor and other samples and propose a plug-and-play method calledAdaptive Distribution Calibration (ADC) to ensure that samples that are near orfar from the anchor point in the original input space are closer or furtheraway from the anchor point in the feature space. Both the theoretical analysisand the empirical evaluation demonstrate the superiority of ADC.</description>
      <author>example@mail.com (Lingyu Si, Jingyao Wang, Wenwen Qiang)</author>
      <guid isPermaLink="false">2508.13596v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</title>
      <link>http://arxiv.org/abs/2508.13507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究解决了羽毛球双打分析研究不足的问题，通过将单打训练的模型扩展到双打分析，使用ViT-Pose提取关键点，ST-GCN对比学习框架嵌入，自定义多目标跟踪算法提高跟踪稳定性，以及基于Transformer的分类器确定击球发生，证明了基于姿势的击球识别扩展到双打羽毛球的可行性。&lt;h4&gt;背景&lt;/h4&gt;羽毛球是世界上速度最快的球拍运动之一，国际比赛中双打比赛比单打更普遍，但之前的研究主要关注单打，因为数据可用性和多人跟踪的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决双打分析研究不足的问题，将单打训练的模型扩展到双打分析。&lt;h4&gt;方法&lt;/h4&gt;使用ViT-Pose从ShuttleSet单场比赛数据集中提取关键点，通过基于ST-GCN的对比学习框架嵌入这些关键点，加入自定义的多目标跟踪算法解决ID切换问题，使用基于Transformer的分类器确定击球发生。&lt;h4&gt;主要发现&lt;/h4&gt;证明了基于姿势的击球识别扩展到双打羽毛球的可行性，扩展了分析能力。&lt;h4&gt;结论&lt;/h4&gt;为双打特定数据集奠定了基础，有助于增强对这种主要但研究不足的快速球拍运动形式的理解。&lt;h4&gt;翻译&lt;/h4&gt;羽毛球被誉为世界上速度最快的球拍运动之一。尽管国际比赛中双打比赛比单打更为普遍，但之前的研究主要关注单打，这是由于数据可用性和多人跟踪的挑战。为了解决这一空白，我们设计了一种将单打训练的模型转换到双打分析的方法。我们使用ViT-Pose从ShuttleSet单场比赛数据集中提取关键点，并通过基于ST-GCN的对比学习框架嵌入它们。为了提高跟踪稳定性，我们融入了一个自定义的多目标跟踪算法，解决了快速和重叠球员移动导致的ID切换问题。然后，基于Transformer的分类器根据学习的嵌入确定击球发生。我们的发现证明了将基于姿势的击球识别扩展到双打羽毛球的可行性，扩展了分析能力。这项工作为双打特定数据集奠定了基础，以增强对这种主要但研究不足的快速球拍运动形式的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Badminton is known as one of the fastest racket sports in the world. Despitedoubles matches being more prevalent in international tournaments than singles,previous research has mainly focused on singles due to the challenges in dataavailability and multi-person tracking. To address this gap, we designed anapproach that transfers singles-trained models to doubles analysis. Weextracted keypoints from the ShuttleSet single matches dataset using ViT-Poseand embedded them through a contrastive learning framework based on ST-GCN. Toimprove tracking stability, we incorporated a custom multi-object trackingalgorithm that resolves ID switching issues from fast and overlapping playermovements. A Transformer-based classifier then determines shot occurrencesbased on the learned embeddings. Our findings demonstrate the feasibility ofextending pose-based shot recognition to doubles badminton, broadeninganalytics capabilities. This work establishes a foundation for doubles-specificdatasets to enhance understanding of this predominant yet understudied formatof the fast racket sport.</description>
      <author>example@mail.com (Seungheon Baek, Jinhyuk Yun)</author>
      <guid isPermaLink="false">2508.13507v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification</title>
      <link>http://arxiv.org/abs/2508.13452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HCAL的分类器，通过集成原型对比学习和自适应任务权重机制，解决了层次多标签分类中的结构一致性和多任务学习中的损失权重平衡问题。&lt;h4&gt;背景&lt;/h4&gt;层次多标签分类（HMC）在保持结构一致性和平衡损失权重方面面临关键挑战，传统多任务学习（MTL）方法存在'一个强任务多个弱任务'的优化偏差问题。&lt;h4&gt;目的&lt;/h4&gt;解决HMC中的结构一致性和损失权重平衡问题，以及MTL中的优化偏差问题，提高分类精度并降低层次违规率。&lt;h4&gt;方法&lt;/h4&gt;提出基于MTL的HCAL分类器，集成原型对比学习和自适应任务权重机制，包括语义一致性机制（显式建模标签的原型和从子类到父类的特征聚合）、自适应损失权重机制（通过监控任务特定收敛率动态分配优化资源）、原型扰动机制（通过注入受控噪声扩展决策边界），并提出了层次违规率（HVR）作为评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的实验表明，所提出的HCAL分类器比基线模型具有更高的分类精度和更低的层次违规率。&lt;h4&gt;结论&lt;/h4&gt;HCAL分类器有效解决了HMC中的结构一致性和MTL中的优化偏差问题，通过自适应任务权重机制和原型扰动机制提高了模型的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;层次多标签分类（HMC）在保持结构一致性和平衡多任务学习（MTL）中的损失权重方面面临关键挑战。为了解决这些问题，我们提出了一种名为HCAL的分类器，该分类器基于MTL，集成了原型对比学习和自适应任务权重机制。我们分类器最重要的优势是语义一致性，包括显式建模标签的原型和从子类到父类的特征聚合。另一个重要优势是自适应损失权重机制，它通过监控任务特定的收敛率来动态分配优化资源，有效解决了传统MTL方法中固有的'一个强任务多个弱任务'的优化偏差。为了进一步增强鲁棒性，通过向原型注入受控噪声来制定原型扰动机制以扩展决策边界。此外，我们形式化了一个称为层次违规率（HVR）的定量指标，用于评估层次一致性和泛化能力。在三个数据集上的大量实验表明，与基线模型相比，所提出的分类器具有更高的分类精度和更低的层次违规率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761241&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical Multi-Label Classification (HMC) faces critical challenges inmaintaining structural consistency and balancing loss weighting in Multi-TaskLearning (MTL). In order to address these issues, we propose a classifiercalled HCAL based on MTL integrated with prototype contrastive learning andadaptive task-weighting mechanisms. The most significant advantage of ourclassifier is semantic consistency including both prototype with explicitlymodeling label and feature aggregation from child classes to parent classes.The other important advantage is an adaptive loss-weighting mechanism thatdynamically allocates optimization resources by monitoring task-specificconvergence rates. It effectively resolves the "one-strong-many-weak"optimization bias inherent in traditional MTL approaches. To further enhancerobustness, a prototype perturbation mechanism is formulated by injectingcontrolled noise into prototype to expand decision boundaries. Additionally, weformalize a quantitative metric called Hierarchical Violation Rate (HVR) as toevaluate hierarchical consistency and generalization. Extensive experimentsacross three datasets demonstrate both the higher classification accuracy andreduced hierarchical violation rate of the proposed classifier over baselinemodels.</description>
      <author>example@mail.com (Ruobing Jiang, Mengzhe Liu, Haobing Liu, Yanwei Yu)</author>
      <guid isPermaLink="false">2508.13452v1</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</title>
      <link>http://arxiv.org/abs/2508.06189v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了MA-CBP框架，一种基于多智能体异步协作的犯罪行为预测方法，能够有效处理实时视频数据并预测潜在犯罪活动。&lt;h4&gt;背景&lt;/h4&gt;城市化加速导致公共场所犯罪行为对社会安全构成严重威胁。传统基于特征识别的异常检测方法难以捕捉高级行为语义，而基于大型语言模型的生成方法无法满足实时性要求。&lt;h4&gt;目的&lt;/h4&gt;解决传统犯罪行为检测方法的局限性，提出一种能够捕捉高级行为语义并满足实时性要求的犯罪行为预测框架。&lt;h4&gt;方法&lt;/h4&gt;提出MA-CBP框架，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，融合相邻图像帧进行长短期上下文联合推理，并包含事件主体、地点和原因等关键要素。同时构建了提供多尺度语言监督的高质量犯罪行为数据集。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明MA-CBP在多个数据集上取得了优越性能，为城市公共安全场景的风险预警提供了有效解决方案。&lt;h4&gt;结论&lt;/h4&gt;MA-CBP框架能够有效预测潜在的犯罪活动，实现早期预警，对提升城市公共安全具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化进程的加速，公共场所的犯罪行为对社会安全构成日益严重的威胁。基于特征识别的传统异常检测方法难以从历史信息中捕捉高级行为语义，而基于大型语言模型的生成方法通常无法满足实时性要求。为应对这些挑战，我们提出了MA-CBP，一种基于多智能体异步协作的犯罪行为预测框架。该框架将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合相邻图像帧对长短期上下文进行联合推理。生成的行为决策包括事件主体、地点和原因等关键要素，能够对潜在的犯罪活动进行早期预警。此外，我们构建了一个高质量的犯罪行为数据集，提供多尺度语言监督，包括帧级、摘要级和事件级语义标注。实验结果表明，我们的方法在多个数据集上取得了优越性能，为城市公共安全场景的风险预警提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the acceleration of urbanization, criminal behavior in public scenesposes an increasingly serious threat to social security. Traditional anomalydetection methods based on feature recognition struggle to capture high-levelbehavioral semantics from historical information, while generative approachesbased on Large Language Models (LLMs) often fail to meet real-timerequirements. To address these challenges, we propose MA-CBP, a criminalbehavior prediction framework based on multi-agent asynchronous collaboration.This framework transforms real-time video streams into frame-level semanticdescriptions, constructs causally consistent historical summaries, and fusesadjacent image frames to perform joint reasoning over long- and short-termcontexts. The resulting behavioral decisions include key elements such as eventsubjects, locations, and causes, enabling early warning of potential criminalactivity. In addition, we construct a high-quality criminal behavior datasetthat provides multi-scale language supervision, including frame-level,summary-level, and event-level semantic annotations. Experimental resultsdemonstrate that our method achieves superior performance on multiple datasetsand offers a promising solution for risk warning in urban public safetyscenarios.</description>
      <author>example@mail.com (Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu)</author>
      <guid isPermaLink="false">2508.06189v2</guid>
      <pubDate>Wed, 20 Aug 2025 15:08:08 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Representations for Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2508.13113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://princeton-rl.github.io/CRTR/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为CRTR的新方法，用于从同时感知和时序结构的表示中实现时序推理，解决了传统时序对比学习中因依赖伪特征而无法捕获时序结构的问题。&lt;h4&gt;背景&lt;/h4&gt;在经典AI中，感知依赖于学习基于状态的表示，而规划（可视为对动作序列的时序推理）通常通过搜索实现。&lt;h4&gt;目的&lt;/h4&gt;研究是否可以从同时捕捉感知和时序结构的表示中实现时序推理，并解决标准时序对比学习中因依赖伪特征而无法捕获时序结构的问题。&lt;h4&gt;方法&lt;/h4&gt;引入了用于时序推理的组合表示（CRTR），该方法使用负采样方案来证明性地移除这些伪特征并促进时序推理。&lt;h4&gt;主要发现&lt;/h4&gt;CRTR在具有复杂时序结构的领域（如Sokoban和魔方）取得了强有力结果。特别是对于魔方，CRTR学习到的表示可以泛化到所有初始状态，并且使用比最佳优先搜索（BestFS）更少的搜索步骤解决谜题，尽管解决方案更长。&lt;h4&gt;结论&lt;/h4&gt;据我们所知，这是第一种仅使用学习到的表示而无需依赖外部搜索算法就能高效解决任意魔方状态的方法。&lt;h4&gt;翻译&lt;/h4&gt;在经典AI中，感知依赖于学习基于状态的表示，而规划（可视为对动作序列的时序推理）通常通过搜索实现。我们研究是否可以从同时捕捉感知和时序结构的表示中实现此类推理。我们表明，尽管标准时序对比学习很受欢迎，但它常常因依赖伪特征而无法捕获时序结构。为了解决这个问题，我们引入了用于时序推理的组合表示（CRTR），该方法使用负采样方案来证明性地移除这些伪特征并促进时序推理。CRTR在具有复杂时序结构的领域（如Sokoban和魔方）取得了强有力结果。特别是对于魔方，CRTR学习到的表示可以泛化到所有初始状态，并允许它使用比最佳优先搜索（BestFS）更少的搜索步骤解决谜题，尽管解决方案更长。据我们所知，这是第一种仅使用学习到的表示而无需依赖外部搜索算法就能高效解决任意魔方状态的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In classical AI, perception relies on learning state-based representations,while planning, which can be thought of as temporal reasoning over actionsequences, is typically achieved through search. We study whether suchreasoning can instead emerge from representations that capture both perceptualand temporal structure. We show that standard temporal contrastive learning,despite its popularity, often fails to capture temporal structure due to itsreliance on spurious features. To address this, we introduce CombinatorialRepresentations for Temporal Reasoning (CRTR), a method that uses a negativesampling scheme to provably remove these spurious features and facilitatetemporal reasoning. CRTR achieves strong results on domains with complextemporal structure, such as Sokoban and Rubik's Cube. In particular, for theRubik's Cube, CRTR learns representations that generalize across all initialstates and allow it to solve the puzzle using fewer search steps than BestFS,though with longer solutions. To our knowledge, this is the first method thatefficiently solves arbitrary Cube states using only learned representations,without relying on an external search algorithm.</description>
      <author>example@mail.com (Alicja Ziarko, Michal Bortkiewicz, Michal Zawalski, Benjamin Eysenbach, Piotr Milos)</author>
      <guid isPermaLink="false">2508.13113v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
  <item>
      <title>EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding</title>
      <link>http://arxiv.org/abs/2508.12687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了EgoIllusion，这是首个用于评估多模态大语言模型在自我中心视频中产生幻觉的基准测试，包含1400个视频和8000个人类标注的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在复杂多模态任务中表现出色，特别是在第三人称和自我中心视频的视觉感知与推理方面，但容易产生幻觉，生成连贯但不准确的响应。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试来评估MLLM在自我中心视频中的幻觉问题，促进更强大的自我中心MLLM发展，减少幻觉率。&lt;h4&gt;方法&lt;/h4&gt;创建了EgoIllusion基准，包含1400个视频和8000个人类标注的开放式和封闭式问题，旨在触发自我中心视频中视觉和听觉线索的幻觉，并评估了十个MLLM模型。&lt;h4&gt;主要发现&lt;/h4&gt;十个被评估的MLLM模型在EgoIllusion基准上表现不佳，即使是GPT-4o和Gemini等强大模型也仅达到59%的准确率，表明这些模型在处理自我中心视频时存在显著挑战。&lt;h4&gt;结论&lt;/h4&gt;EgoIllusion为开发评估MLLM有效性的稳健基准奠定了基础，并促进开发幻觉率降低的更好的自我中心MLLM，该基准将被开源以确保可复现性。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在复杂的多模态任务中表现出色。虽然MLLM在第三人称和自我中心视频的视觉感知和推理方面表现出色，但它们容易产生幻觉，生成连贯但不准确的响应。我们提出了EgoIllusion，这是第一个用于评估自我中心视频中MLLM幻觉的基准。EgoIllusion包含1400个视频和8000个人类标注的开放式和封闭式问题，旨在触发自我中心视频中视觉和听觉线索的幻觉。对十个MLLM的评估揭示了重大挑战，包括GPT-4o和Gemini等强大模型，仅达到59%的准确率。EgoIllusion为开发评估MLLM有效性的稳健基准奠定了基础，并促进开发幻觉率降低的更好的自我中心MLLM。我们的基准将被开源以确保可复现性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated remarkableperformance in complex multimodal tasks. While MLLMs excel at visual perceptionand reasoning in third-person and egocentric videos, they are prone tohallucinations, generating coherent yet inaccurate responses. We presentEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentricvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotatedopen and closed-ended questions designed to trigger hallucinations in bothvisual and auditory cues in egocentric videos. Evaluations across ten MLLMsreveal significant challenges, including powerful models like GPT-4o andGemini, achieving only 59% accuracy. EgoIllusion lays the foundation indeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spursthe development of better egocentric MLLMs with reduced hallucination rates.Our benchmark will be open-sourced for reproducibility.</description>
      <author>example@mail.com (Ashish Seth, Utkarsh Tyagi, Ramaneswaran Selvakumar, Nishit Anand, Sonal Kumar, Sreyan Ghosh, Ramani Duraiswami, Chirag Agarwal, Dinesh Manocha)</author>
      <guid isPermaLink="false">2508.12687v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2508.12282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChronoQA是一个大规模的中文问答基准数据集，专门用于评估检索增强生成(RAG)系统中的时间推理能力。&lt;h4&gt;背景&lt;/h4&gt;需要评估RAG系统中的时间推理能力，但缺乏专门的中文基准数据集。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模的中文问答基准数据集，专门用于评估RAG系统中的时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;从2019年至2024年间发表的30多万篇新闻文章构建数据集，包含5,176个高质量问题，涵盖绝对、聚合和相对时间类型，包含显性和隐性时间表达。数据集支持单文档和多文档场景，具有全面的结构化注释，并经历了基于规则、基于LLM和人工评估的多阶段验证。&lt;h4&gt;主要发现&lt;/h4&gt;ChronoQA数据集提供了动态、可靠和可扩展的资源，能够支持广泛的时间任务结构化评估，并作为推进时间敏感型检索增强问答系统的强大基准。&lt;h4&gt;结论&lt;/h4&gt;ChronoQA数据集能够为时间敏感型检索增强问答系统的发展提供有力支持。&lt;h4&gt;翻译&lt;/h4&gt;ChronoQA是一个大规模的中文问答基准数据集，专门用于评估检索增强生成(RAG)系统中的时间推理能力。ChronoQA是从2019年至2024年间发表的30多万篇新闻文章构建而成，包含5,176个高质量问题，涵盖绝对、聚合和相对时间类型，包含显性和隐性时间表达。该数据集支持单文档和多文档场景，反映了时间对齐和逻辑一致性的现实需求。ChronoQA具有全面的结构化注释，并经历了基于规则、基于LLM和人工评估的多阶段验证，确保数据质量。通过提供动态、可靠和可扩展的资源，ChronoQA能够在广泛的时间任务中进行结构化评估，并作为推进时间敏感型检索增强问答系统的强大基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ChronoQA, a large-scale benchmark dataset for Chinese questionanswering, specifically designed to evaluate temporal reasoning inRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over300,000 news articles published between 2019 and 2024, and contains 5,176high-quality questions covering absolute, aggregate, and relative temporaltypes with both explicit and implicit time expressions. The dataset supportsboth single- and multi-document scenarios, reflecting the real-worldrequirements for temporal alignment and logical consistency. ChronoQA featurescomprehensive structural annotations and has undergone multi-stage validation,including rule-based, LLM-based, and human evaluation, to ensure data quality.By providing a dynamic, reliable, and scalable resource, ChronoQA enablesstructured evaluation across a wide range of temporal tasks, and serves as arobust benchmark for advancing time-sensitive retrieval-augmented questionanswering systems.</description>
      <author>example@mail.com (Ziyang Chen, Erxue Min, Xiang Zhao, Yunxin Li, Xin Jia, Jinzhi Liao, Jichao Li, Shuaiqiang Wang, Baotian Hu, Dawei Yin)</author>
      <guid isPermaLink="false">2508.12282v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting</title>
      <link>http://arxiv.org/abs/2508.11923v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种尺度解耦时空建模框架，用于解决长期交通排放预测中的级联误差放大问题，通过多尺度特征分解与融合提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;长期交通排放预测对城市空气污染综合管理至关重要，但传统时空图建模方法在长期推理过程中容易遭受级联误差放大。&lt;h4&gt;目的&lt;/h4&gt;解决交通排放在时间和空间上的多尺度纠缠导致的级联误差放大问题，提高长期交通排放预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出尺度解耦时空建模框架，利用基于Koopman提升算子的双流特征分解策略将尺度耦合的时空动力学系统提升到无限维线性空间，使用门控小波分解勾勒可预测性边界，并构建包含双流独立性约束的融合机制来优化预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;在西安市二环路道路级交通排放数据集上的实验表明，所提出的模型达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;尺度解耦时空建模框架能有效解决长期交通排放预测中的级联误差放大问题，提高预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;长期交通排放预测对城市空气污染的综合管理至关重要。传统预测方法通常通过挖掘时空依赖关系构建时空图模型来预测排放。然而，由于交通排放在时间和空间上的多尺度纠缠特性，这些时空图建模方法在长期推理过程中容易遭受级联误差放大。为解决这一问题，我们提出了一种用于长期交通排放预测的尺度解耦时空建模框架。它利用多尺度间的可预测性差异来分解和融合不同尺度的特征，同时约束它们保持独立但互补。具体而言，该模型首先引入了基于Koopman提升算子的双流特征分解策略。它通过Koopman算子将尺度耦合的时空动力学系统提升到无限维线性空间，并使用门控小波分解勾勒可预测性边界。然后构建了一种新的融合机制，包含基于交叉项损失的双流独立性约束，以动态优化双流预测结果，抑制相互干扰，并提高长期交通排放预测的准确性。在西安市二环路道路级交通排放数据集上进行的大量实验表明，所提出的模型达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term traffic emission forecasting is crucial for the comprehensivemanagement of urban air pollution. Traditional forecasting methods typicallyconstruct spatiotemporal graph models by mining spatiotemporal dependencies topredict emissions. However, due to the multi-scale entanglement of trafficemissions across time and space, these spatiotemporal graph modeling methodtend to suffer from cascading error amplification during long-term inference.To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling(SDSTM) framework for long-term traffic emission forecasting. It leverages thepredictability differences across multiple scales to decompose and fusefeatures at different scales, while constraining them to remain independent yetcomplementary. Specifically, the model first introduces a dual-stream featuredecomposition strategy based on the Koopman lifting operator. It lifts thescale-coupled spatiotemporal dynamical system into an infinite-dimensionallinear space via Koopman operator, and delineates the predictability boundaryusing gated wavelet decomposition. Then a novel fusion mechanism isconstructed, incorporating a dual-stream independence constraint based oncross-term loss to dynamically refine the dual-stream prediction results,suppress mutual interference, and enhance the accuracy of long-term trafficemission prediction. Extensive experiments conducted on a road-level trafficemission dataset within Xi'an's Second Ring Road demonstrate that the proposedmodel achieves state-of-the-art performance.</description>
      <author>example@mail.com (Yan Wu, Lihong Pei, Yukai Han, Yang Cao, Yu Kang, Yanlong Zhao)</author>
      <guid isPermaLink="false">2508.11923v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Re:Verse -- Can Your VLM Read a Manga?</title>
      <link>http://arxiv.org/abs/2508.08508v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted (oral) at ICCV (AISTORY Workshop) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;当前视觉语言模型在处理顺序视觉叙事时存在表面识别与深层叙事推理之间的关键差距，研究表明这些模型在单个面板解释方面表现出色，但在时间因果性和跨面板连贯性方面系统性失败。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在处理顺序视觉叙事时表现出表面识别与深层叙事推理之间的关键差距，尽管大型多模态模型能够解释单个面板，但它们在连贯故事理解的核心要求上存在系统性失败。&lt;h4&gt;目的&lt;/h4&gt;通过全面研究漫画叙事理解，揭示当前模型在叙事推理方面的局限性，并引入一个新的评估框架来系统性地表征这些缺陷。&lt;h4&gt;方法&lt;/h4&gt;引入一个结合细粒度多模态标注、跨模态嵌入分析和检索增强评估的框架；包括严格的标注协议将视觉元素与叙事结构联系；在多种推理范式进行全面评估；进行跨模态相似性分析；应用框架对Re:Zero漫画的11章308个标注面板进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型缺乏真正的故事级智能，特别难以处理非线性叙事、角色一致性和跨越长序列的因果推理；在生成性叙事、上下文对话定位和时间推理三个核心评估轴上表现不佳。&lt;h4&gt;结论&lt;/h4&gt;这项工作为评估叙事智能建立了基础和实用方法，同时为多模态模型中离散视觉叙事的深度序列理解能力提供了可操作的见解，超越了基本识别。&lt;h4&gt;翻译&lt;/h4&gt;当前的视觉语言模型在处理顺序视觉叙事时，表现出表面识别与深层叙事推理之间的关键差距。通过对漫画叙事理解的全面研究，我们揭示尽管最近的大型多模态模型擅长单个面板解释，但它们在时间因果性和跨面板连贯性方面系统性失败，这些是连贯故事理解的核心要求。我们引入了一个新颖的评估框架，结合细粒度多模态标注、跨模态嵌入分析和检索增强评估，以系统性地表征这些局限性。我们的方法包括(i)一个严格的标注协议，通过对齐的轻小说文本将视觉元素与叙事结构联系起来，(ii)在多种推理范式中的全面评估，包括直接推理和检索增强生成，以及(iii)跨模态相似性分析，揭示当前VLMs联合表示中的基本错位。将此框架应用于Re:Zero漫画的11章308个标注面板，我们通过三个核心评估轴进行了VLMs中长篇叙事理解的首次系统性研究：生成性叙事、上下文对话定位和时间推理。我们的研究结果表明，当前模型缺乏真正的故事级智能，特别难以处理非线性叙事、角色一致性和跨越长序列的因果推理。这项工作为评估叙事智能建立了基础和实用方法，同时为多模态模型中离散视觉叙事的深度序列理解能力提供了可操作的见解，超越了基本识别。项目页面：https://re-verse.vercel.app&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Vision Language Models (VLMs) demonstrate a critical gap betweensurface-level recognition and deep narrative reasoning when processingsequential visual storytelling. Through a comprehensive investigation of manganarrative understanding, we reveal that while recent large multimodal modelsexcel at individual panel interpretation, they systematically fail at temporalcausality and cross-panel cohesion, core requirements for coherent storycomprehension. We introduce a novel evaluation framework that combinesfine-grained multimodal annotation, cross-modal embedding analysis, andretrieval-augmented assessment to systematically characterize theselimitations.  Our methodology includes (i) a rigorous annotation protocol linking visualelements to narrative structure through aligned light novel text, (ii)comprehensive evaluation across multiple reasoning paradigms, including directinference and retrieval-augmented generation, and (iii) cross-modal similarityanalysis revealing fundamental misalignments in current VLMs' jointrepresentations. Applying this framework to Re:Zero manga across 11 chapterswith 308 annotated panels, we conduct the first systematic study of long-formnarrative understanding in VLMs through three core evaluation axes: generativestorytelling, contextual dialogue grounding, and temporal reasoning. Ourfindings demonstrate that current models lack genuine story-level intelligence,struggling particularly with non-linear narratives, character consistency, andcausal inference across extended sequences. This work establishes both thefoundation and practical methodology for evaluating narrative intelligence,while providing actionable insights into the capability of deep sequentialunderstanding of Discrete Visual Narratives beyond basic recognition inMultimodal Models.  Project Page: https://re-verse.vercel.app</description>
      <author>example@mail.com (Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.08508v3</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation</title>
      <link>http://arxiv.org/abs/2508.13068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种两阶段多模态框架，用于增强基于胸部X光片的疾病分类和区域感知的放射学报告生成，通过整合放射科医生的眼动追踪数据提高分类性能和报告可解释性。&lt;h4&gt;背景&lt;/h4&gt;胸部X光片是疾病诊断的重要工具，但传统方法在疾病分类和报告生成方面存在局限性。MIMIC-Eye数据集提供了包含放射科医生眼动追踪信号的多模态数据，为改进医疗图像分析提供了新机会。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提高疾病分类性能并生成区域感知放射学报告的框架，同时整合放射科医生的注视信息以提高模型的可解释性。&lt;h4&gt;方法&lt;/h4&gt;研究采用两阶段框架：第一阶段使用注视引导的对比学习架构进行疾病分类，整合视觉特征、临床标签、边界框和眼动追踪信号，并采用多项注视注意力损失函数；第二阶段实现模块化报告生成流水线，提取置信度加权的诊断关键词，将其映射到解剖区域，并通过结构化提示生成区域对齐的句子。&lt;h4&gt;主要发现&lt;/h4&gt;整合注视数据显著提高了分类性能，F1分数从0.597提高到0.631（提升5.70%），AUC从0.821提高到0.849（提升3.41%），同时也提高了精确率和召回率。报告生成流水线通过临床关键词召回率和ROUGE重叠度衡量的报告质量有所提高。&lt;h4&gt;结论&lt;/h4&gt;整合注视数据可以同时提高疾病分类性能和生成医疗报告的可解释性，为医学影像分析提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种两阶段多模态框架，可增强基于胸部X光片的疾病分类和区域感知的放射学报告生成，利用MIMIC-Eye数据集。在第一阶段，我们引入了一种注视引导的对比学习架构用于疾病分类。它整合了视觉特征、临床标签、边界框和放射科医生眼动追踪信号，并配备了一种新颖的多项注视注意力损失函数，结合了均方误差、KL散度、相关性和质心对齐。引入注视点使F1分数从0.597提高到0.631（提升5.70%），AUC从0.821提高到0.849（提升3.41%），同时也提高了精确率和召回率，突显了注视感知注意力监督的有效性。在第二阶段，我们提出了一个模块化的报告生成流水线，提取具有置信度加权的诊断关键词，使用基于领域特定先验构建的精选字典将它们映射到解剖区域，并通过结构化提示生成区域对齐的句子。该流水线通过临床关键词召回率和ROUGE重叠度衡量的报告质量有所提高。我们的结果表明，整合注视数据可以提高分类性能和生成医疗报告的可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a two-stage multimodal framework that enhances diseaseclassification and region-aware radiology report generation from chest X-rays,leveraging the MIMIC-Eye dataset. In the first stage, we introduce agaze-guided contrastive learning architecture for disease classification. Itintegrates visual features, clinical labels, bounding boxes, and radiologisteye-tracking signals and is equipped with a novel multi-term gaze-attentionloss combining MSE, KL divergence, correlation, and center-of-mass alignment.Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUCfrom 0.821 to 0.849 (+3.41%), while also improving precision and recall,highlighting the effectiveness of gaze-informed attention supervision. In thesecond stage, we present a modular report generation pipeline that extractsconfidence-weighted diagnostic keywords, maps them to anatomical regions usinga curated dictionary constructed from domain-specific priors, and generatesregion-aligned sentences via structured prompts. This pipeline improves reportquality as measured by clinical keyword recall and ROUGE overlap. Our resultsdemonstrate that integrating gaze data improves both classification performanceand the interpretability of generated medical reports.</description>
      <author>example@mail.com (Tanjim Islam Riju, Shuchismita Anwar, Saman Sarker Joy, Farig Sadeque, Swakkhar Shatabda)</author>
      <guid isPermaLink="false">2508.13068v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score</title>
      <link>http://arxiv.org/abs/2508.12718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为双重对比去噪评分的框架，用于解决文本到图像生成模型在编辑真实图像时面临的挑战，实现了灵活的内容修改和结构保持。&lt;h4&gt;背景&lt;/h4&gt;大规模文本到图像生成模型能够合成多样且高质量的图像，但在直接应用于编辑真实图像时存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决用户难以描述图像细节和现有模型会意外改变不需要区域的问题，实现更好的真实图像编辑。&lt;h4&gt;方法&lt;/h4&gt;提出双重对比去噪评分框架，利用文本到图像扩散模型的生成先验，引入双重对比损失，利用潜在扩散模型自注意力层中间表示的空间信息，不依赖辅助网络。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够实现输入和输出图像之间的灵活内容修改和结构保持，实现零样本图像到图像翻译，在真实图像编辑方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法能够直接利用预训练的文本到图像扩散模型而无需进一步训练，为真实图像编辑提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大规模文本到图像生成模型已展现出合成多样且高质量图像的卓越能力。然而，由于两个原因，直接将这些模型应用于编辑真实图像仍然具有挑战性。首先，用户难以提出完美文本提示来准确描述输入图像中的每个视觉细节。其次，尽管现有模型可以在某些区域引入期望的变化，但它们通常会显著改变输入内容并在不需要的区域引入意外的变化。为应对这些挑战，我们提出了双重对比去噪评分，这是一个简单而强大的框架，利用了文本到图像扩散模型的丰富生成先验。受无配对图像到图像翻译的对比学习方法启发，我们在提出的框架内引入了一个简单的双重对比损失。我们的方法利用了潜在扩散模型自注意力层中间表示的丰富空间信息，而不依赖辅助网络。我们的方法实现了输入和输出图像之间的灵活内容修改和结构保持，以及零样本图像到图像翻译。通过大量实验，我们表明我们的方法在真实图像编辑方面优于现有方法，同时保持直接利用预训练文本到图像扩散模型的能力而无需进一步训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale text-to-image generative models have shown remarkable ability tosynthesize diverse and high-quality images. However, it is still challenging todirectly apply these models for editing real images for two reasons. First, itis difficult for users to come up with a perfect text prompt that accuratelydescribes every visual detail in the input image. Second, while existing modelscan introduce desirable changes in certain regions, they often dramaticallyalter the input content and introduce unexpected changes in unwanted regions.To address these challenges, we present Dual Contrastive Denoising Score, asimple yet powerful framework that leverages the rich generative prior oftext-to-image diffusion models. Inspired by contrastive learning approaches forunpaired image-to-image translation, we introduce a straightforward dualcontrastive loss within the proposed framework. Our approach utilizes theextensive spatial information from the intermediate representations of theself-attention layers in latent diffusion models without depending on auxiliarynetworks. Our method achieves both flexible content modification and structurepreservation between input and output images, as well as zero-shotimage-to-image translation. Through extensive experiments, we show that ourapproach outperforms existing methods in real image editing while maintainingthe capability to directly utilize pretrained text-to-image diffusion modelswithout further training.</description>
      <author>example@mail.com (Syed Muhmmad Israr, Feng Zhao)</author>
      <guid isPermaLink="false">2508.12718v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Domain Supervised Contrastive Learning for UAV Radio-Frequency Open-Set Recognition</title>
      <link>http://arxiv.org/abs/2508.12689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多域监督对比学习的无人机射频开放式识别框架，结合深度学习特征融合和改进的OpenMax算法，通过冻结特征提取层和重新训练分类层的方法，在25种无人机类型下实现了95.12%的闭集识别率和96.08%的开集识别率。&lt;h4&gt;背景&lt;/h4&gt;5G-Advanced促进了低空一体化感知与通信网络的蓬勃发展，作为核心组件的无人机近年来快速增长，但由于传统行业监管规范的滞后，未经授权的飞行事件频繁发生，对LA-ISAC网络构成严重安全威胁。&lt;h4&gt;目的&lt;/h4&gt;监视非合作无人机，提出一种用于无人机射频开放式识别的多域监督对比学习框架。&lt;h4&gt;方法&lt;/h4&gt;融合来自ResNet和TransformerEncoder的纹理特征和时间频率位置特征，应用监督对比学习优化闭集样本特征表示，提出改进的生成式OpenMax算法，构建Open-RFNet模型，对于未知样本冻结特征提取层并仅重新训练分类层。&lt;h4&gt;主要发现&lt;/h4&gt;提出的Open-RFNet在已知和未知无人机的识别准确性方面优于现有基准方法，在25种无人机类型下，闭集识别率达95.12%，开集识别率达96.08%。&lt;h4&gt;结论&lt;/h4&gt;该模型在闭集和开集识别中均取得了优异的识别性能，分析了所提出模型的计算复杂性。&lt;h4&gt;翻译&lt;/h4&gt;5G-Advanced(5G-A)已促进低空一体化感知与通信(LA-ISAC)网络的蓬勃发展。作为这些网络的核心组件，无人机(UAVs)近年来经历了快速增长。然而，由于传统行业监管规范的滞后，未经授权的飞行事件频繁发生，对LA-ISAC网络构成严重安全威胁。为了监视非合作无人机，本文提出了一种用于无人机射频(RF)开放式识别的多域监督对比学习(MD-SupContrast)框架。具体来说，首先融合来自ResNet和TransformerEncoder的纹理特征和时间频率位置特征，然后应用监督对比学习来优化闭集样本的特征表示。接下来，为了监视现实生活中出现的入侵无人机，我们提出了一种改进的生成式OpenMax(IG-OpenMax)算法，并构建了一个开放式识别模型，即Open-RFNet。根据未知样本，我们首先冻结特征提取层，然后仅重新训练分类层，在闭集和开集识别中均取得了优异的识别性能。我们分析了所提出模型的计算复杂性。在大型无人机开放数据集上进行了实验。结果表明，所提出的Open-RFNet在已知和未知无人机之间的识别准确性方面优于现有的基准方法，因为在25种无人机类型下，它分别实现了95.12%的闭集和96.08%的开集识别率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 5G-Advanced (5G-A) has enabled the vibrant development of low altitudeintegrated sensing and communication (LA-ISAC) networks. As a core component ofthese networks, unmanned aerial vehicles (UAVs) have witnessed rapid growth inrecent years. However, due to the lag in traditional industry regulatory norms,unauthorized flight incidents occur frequently, posing a severe security threatto LA-ISAC networks. To surveil the non-cooperative UAVs, in this paper, wepropose a multi-domain supervised contrastive learning (MD-SupContrast)framework for UAV radio frequency (RF) open-set recognition. Specifically,first, the texture features and the time-frequency position features from theResNet and the TransformerEncoder are fused, and then the supervisedcontrastive learning is applied to optimize the feature representation of theclosed-set samples. Next, to surveil the invasive UAVs that appear in reallife, we propose an improved generative OpenMax (IG-OpenMax) algorithm andconstruct an open-set recognition model, namely Open-RFNet. According to theunknown samples, we first freeze the feature extraction layers and then onlyretrain the classification layer, which achieves excellent recognitionperformance both in closed-set and open-set recognitions. We analyze thecomputational complexity of the proposed model. Experiments are conducted witha large-scale UAV open dataset. The results show that the proposed Open-RFNetoutperforms the existing benchmark methods in terms of recognition accuracybetween the known and the unknown UAVs, as it achieves 95.12% in closed-set and96.08% in open-set under 25 UAV types, respectively.</description>
      <author>example@mail.com (Ning Gao, Tianrui Zeng, Bowen Chen, Donghong Cai, Shi Jin, Michail Matthaiou)</author>
      <guid isPermaLink="false">2508.12689v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection</title>
      <link>http://arxiv.org/abs/2508.12684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AdaBEV，一种通过精炼-对比范式学习自适应实例感知BEV表示的新框架，用于多无人机协作3D检测。&lt;h4&gt;背景&lt;/h4&gt;多无人机协作3D检测通过融合多视角观测实现准确且鲁棒的环境感知，具有覆盖范围大和遮挡处理能力强的优势，但给资源受限的无人机平台带来计算挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的框架，能够在资源受限的无人机平台上实现准确的多无人机协作3D检测。&lt;h4&gt;方法&lt;/h4&gt;AdaBEV框架引入了Box-Guided Refinement Module (BG-RM)和Instance-Background Contrastive Learning (IBCL)，前者使用2D监督和空间细分精炼与前景实例相关的BEV网格，后者通过BEV空间中的对比学习增强前景和背景特征的分离。&lt;h4&gt;主要发现&lt;/h4&gt;在Air-Co-Pred数据集上的实验表明，AdaBEV在各种模型规模上实现了优越的精度-计算权衡，在低分辨率下优于其他最先进方法，同时保持低分辨率BEV输入和可忽略的开销。&lt;h4&gt;结论&lt;/h4&gt;AdaBEV有效解决了多无人机协作3D检测中的计算挑战，在保持高检测性能的同时实现了高效计算。&lt;h4&gt;翻译&lt;/h4&gt;多无人机协作3D检测通过融合来自空中平台的多视角观测实现准确且鲁棒的环境感知，在覆盖范围和遮挡处理方面具有显著优势，同时也给资源受限的无人机平台上的计算带来了新挑战。本文提出了AdaBEV，一种通过精炼-对比范式学习自适应实例感知BEV表示的新框架。与将所有BEV网格同等对待的现有方法不同，AdaBEV引入了Box-Guided Refinement Module (BG-RM)和Instance-Background Contrastive Learning (IBCL)来增强语义感知和特征区分度。BG-RM使用2D监督和空间细分只精炼与前景实例相关的BEV网格，而IBCL通过BEV空间中的对比学习促进前景和背景特征之间的更强分离。在Air-Co-Pred数据集上的大量实验表明，AdaBEV在各种模型规模上实现了优越的精度-计算权衡，在低分辨率下优于其他最先进方法，同时保持低分辨率BEV输入和可忽略的开销，接近上限性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多无人机协作3D目标检测中BEV（鸟瞰图）表示的效率问题。现有方法对所有BEV网格区域进行均匀处理，在资源受限的无人机平台上造成了计算资源浪费。这个问题很重要，因为多无人机协作能提供更全面的感知，在监控、城市监测和灾害响应等应用中至关重要，但需要高精度与高效率的平衡才能满足实时性要求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有BEV方法采用均匀网格假设，这与无人机视角下信息分布不均匀的现实不符。他们借鉴了BEVFormer作为基础框架，参考了Focal-PETR等使用2D检测监督的辅助任务方法，以及SimCLR等对比学习方法。基于这些，设计了两个核心组件：利用2D检测框识别重要区域并进行细化的Box-Guided Refinement Module，以及在BEV空间中应用对比学习增强特征区分度的Instance-Background Contrastive Learning。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是自适应实例感知的BEV表示学习，根据前景和背景区域的重要性差异进行非均匀处理。整体流程：1)输入多视角图像并提取特征；2)通过透视感知监督生成2D边界框；3)初始化BEV查询并获取初始BEV特征；4)应用BG-RM模块识别前景区域并进行4×4细分处理；5)应用IBCL模块提取实例特征和背景特征并进行对比学习；6)通过检测解码器进行3D目标检测；7)计算包括基础检测损失、透视感知损失和对比学习损失在内的总损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自适应非均匀BEV建模，打破传统均匀网格假设；2)Box-Guided Refinement Module，仅对前景区域精细化处理；3)Instance-Background Contrastive Learning，在BEV空间直接应用对比学习增强特征区分度；4)实现精度-计算效率的平衡。相比之前的工作，不同在于：传统方法均匀处理所有网格，本文只精细处理前景区域；虽然使用2D检测作为辅助，但用于精确识别需要精细化的区域；首次在BEV空间直接应用实例-背景对比学习；不需要显式的无人机间信息交换即可实现高效协作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自适应实例感知的BEV表示方法，通过仅对前景区域进行精细化处理和增强前景-背景特征区分度，实现了在资源受限无人机平台上高效准确的3D目标检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-UAV collaborative 3D detection enables accurate and robust perceptionby fusing multi-view observations from aerial platforms, offering significantadvantages in coverage and occlusion handling, while posing new challenges forcomputation on resource-constrained UAV platforms. In this paper, we presentAdaBEV, a novel framework that learns adaptive instance-aware BEVrepresentations through a refine-and-contrast paradigm. Unlike existing methodsthat treat all BEV grids equally, AdaBEV introduces a Box-Guided RefinementModule (BG-RM) and an Instance-Background Contrastive Learning (IBCL) toenhance semantic awareness and feature discriminability. BG-RM refines only BEVgrids associated with foreground instances using 2D supervision and spatialsubdivision, while IBCL promotes stronger separation between foreground andbackground features via contrastive learning in BEV space. Extensiveexperiments on the Air-Co-Pred dataset demonstrate that AdaBEV achievessuperior accuracy-computation trade-offs across model scales, outperformingother state-of-the-art methods at low resolutions and approaching upper boundperformance while maintaining low-resolution BEV inputs and negligibleoverhead.</description>
      <author>example@mail.com (Zhongyao Li, Peirui Cheng, Liangjin Zhao, Chen Chen, Yundu Li, Zhechao Wang, Xue Yang, Xian Sun, Zhirui Wang)</author>
      <guid isPermaLink="false">2508.12684v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision</title>
      <link>http://arxiv.org/abs/2508.12278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CRoC的框架，用于解决图神经网络在图异常检测中面临的标记数据不足问题。该方法通过重构节点上下文和编码异构关系增强GNN的鲁棒性，并结合对比学习有效利用未标记数据，在七个真实世界数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图神经网络被广泛用于各种图相关任务，但在训练鲁棒GNN时通常需要大量标记数据，这在实际应用中是一个关键瓶颈。这一限制在图异常检测(GAD)中尤为严重，因为异常样本本质上是稀有的、标记成本高，并且可能主动伪装其模式以逃避检测。&lt;h4&gt;目的&lt;/h4&gt;解决图异常检测中标记数据不足的问题，提高GNN在有限标记数据情况下的检测性能，特别是对抗伪装异常的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;CRoC框架通过以下方式改进GNN用于GAD：1)利用GAD中固有的类别不平衡来重构每个节点的上下文，通过重新组合节点属性同时保留其交互模式来构建增强图；2)分别编码异构关系并将它们整合到消息传递过程中，增强模型捕获复杂交互语义的能力；3)在训练阶段，将CRoC与对比学习范式结合，使GNN能够有效利用未标记数据，生成更丰富、更具区分性的节点嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在七个不同规模的真实世界GAD数据集上进行的广泛实验表明，CRoC相比基础GNN方法实现了高达14%的AUC提升，并且在有限标记设置下优于最先进的GAD方法。&lt;h4&gt;结论&lt;/h4&gt;CRoC框架通过重构节点上下文和编码异构关系，结合对比学习，有效解决了GAD中标记数据不足的问题，显著提高了GNN在异常检测中的性能，特别是在对抗伪装异常情况下的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)被广泛用作各种图相关任务的引擎，它们在分析图结构数据方面具有有效性。然而，训练鲁棒的GNN通常需要大量的标记数据，这在实际应用中是一个关键瓶颈。这一限制严重阻碍了图异常检测(GAD)的进展，其中异常本质上是稀有的，标记成本高，并且可能主动伪装其模式以逃避检测。为了解决这些问题，我们提出了上下文重构对比(CRoC)，一个简单而有效的框架，通过联合利用有限的标记数据和丰富的未标记数据来训练GNN进行GAD。与先前的工作不同，CRoC利用GAD中固有的类别不平衡来重构每个节点的上下文，通过重新组合节点属性同时保留其交互模式来构建增强图。此外，CRoC分别编码异构关系并将它们整合到消息传递过程中，增强模型捕获复杂交互语义的能力。这些操作保留了节点语义，同时鼓励对抗伪装的鲁棒性，使GNN能够发现复杂的异常情况。在训练阶段，CRoC进一步与对比学习范式集成。这使GNN能够在联合训练过程中有效利用未标记数据，生成更丰富、更具区分性的节点嵌入。CRoC在七个不同规模的真实世界GAD数据集上进行了评估。大量实验表明，CRoC相比基础GNN方法实现了高达14%的AUC提升，并且在有限标记设置下优于最先进的GAD方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely used as the engine for variousgraph-related tasks, with their effectiveness in analyzing graph-structureddata. However, training robust GNNs often demands abundant labeled data, whichis a critical bottleneck in real-world applications. This limitation severelyimpedes progress in Graph Anomaly Detection (GAD), where anomalies areinherently rare, costly to label, and may actively camouflage their patterns toevade detection. To address these problems, we propose Context RefactoringContrast (CRoC), a simple yet effective framework that trains GNNs for GAD byjointly leveraging limited labeled and abundant unlabeled data. Different fromprevious works, CRoC exploits the class imbalance inherent in GAD to refactorthe context of each node, which builds augmented graphs by recomposing theattributes of nodes while preserving their interaction patterns. Furthermore,CRoC encodes heterogeneous relations separately and integrates them into themessage-passing process, enhancing the model's capacity to capture complexinteraction semantics. These operations preserve node semantics whileencouraging robustness to adversarial camouflage, enabling GNNs to uncoverintricate anomalous cases. In the training stage, CRoC is further integratedwith the contrastive learning paradigm. This allows GNNs to effectively harnessunlabeled data during joint training, producing richer, more discriminativenode embeddings. CRoC is evaluated on seven real-world GAD datasets withvarying scales. Extensive experiments demonstrate that CRoC achieves up to 14%AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methodsunder limited-label settings.</description>
      <author>example@mail.com (Siyue Xie, Da Sun Handason Tam, Wing Cheong Lau)</author>
      <guid isPermaLink="false">2508.12278v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction</title>
      <link>http://arxiv.org/abs/2508.12247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的时空多尺度Mamba模型(STM2/STM3)来解决长期时空时间序列预测中的挑战，通过多尺度Mamba架构和自适应图因果卷积网络有效捕获复杂的多尺度时空依赖关系，并在真实世界基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;时空时间序列预测发展迅速，但现有的深度学习方法难以高效学习复杂的长期时空依赖关系。&lt;h4&gt;目的&lt;/h4&gt;解决长期时空依赖学习带来的两个新挑战：1)长期时间序列中包含的多尺度信息难以高效提取；2)不同节点的多尺度时间信息高度相关且难以建模。&lt;h4&gt;方法&lt;/h4&gt;提出了STM2模型，包含多尺度Mamba架构和自适应图因果卷积网络，以及层次化信息聚合保证不同尺度信息的可区分性；进一步提出了STM3增强版本，采用专家混合架构、更稳定的路由策略和因果对比学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;STM3具有更好的路由平滑性，成功保证了每个专家的模式解纠缠，能够更有效地捕获所有空间节点上的多样化时间动态。&lt;h4&gt;结论&lt;/h4&gt;在真实世界基准测试中，STM2/STM3表现出优越性能，在长期时空时间序列预测中取得了最先进的结果。&lt;h4&gt;翻译&lt;/h4&gt;最近，时空时间序列预测发展迅速，但现有的深度学习方法难以高效学习复杂的长期时空依赖关系。长期时空依赖学习带来了两个新的挑战：1)长期时间序列自然包含多尺度信息，难以高效提取；2)来自不同节点的多尺度时间信息高度相关且难以建模。为应对这些挑战，我们提出了一种高效的时空多尺度Mamba(STM2)，包含多尺度Mamba架构，能够同时高效捕获多尺度信息，以及自适应图因果卷积网络来学习复杂的多尺度时空依赖。STM2包含不同尺度信息的层次化信息聚合，保证它们的可区分性。为了更有效地捕获所有空间节点上的多样化时间动态，我们进一步提出了一个增强版本，称为时空多尺度Mamba混合(STM3)，它采用特殊的专家混合架构，包括更稳定的路由策略和因果对比学习策略来增强尺度可区分性。我们证明STM3具有更好的路由平滑性，并成功保证了每个专家的模式解纠缠。在真实世界基准上的大量实验证明了STM2/STM3的优越性能，在长期时空时间序列预测中取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, spatio-temporal time-series prediction has developed rapidly, yetexisting deep learning methods struggle with learning complex long-termspatio-temporal dependencies efficiently. The long-term spatio-temporaldependency learning brings two new challenges: 1) The long-term temporalsequence includes multiscale information naturally which is hard to extractefficiently; 2) The multiscale temporal information from different nodes ishighly correlated and hard to model. To address these challenges, we propose anefficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capturethe multiscale information efficiently and simultaneously, and an adaptivegraph causal convolution network to learn the complex multiscalespatio-temporal dependency. STM2 includes hierarchical information aggregationfor different-scale information that guarantees their distinguishability. Tocapture diverse temporal dynamics across all spatial nodes more efficiently, wefurther propose an enhanced version termed\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a specialMixture-of-Experts architecture, including a more stable routing strategy and acausal contrastive learning strategy to enhance the scale distinguishability.We prove that STM3 has much better routing smoothness and guarantees thepattern disentanglement for each expert successfully. Extensive experiments onreal-world benchmarks demonstrate STM2/STM3's superior performance, achievingstate-of-the-art results in long-term spatio-temporal time-series prediction.</description>
      <author>example@mail.com (Haolong Chen, Liang Zhang, Zhengyuan Xin, Guangxu Zhu)</author>
      <guid isPermaLink="false">2508.12247v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection</title>
      <link>http://arxiv.org/abs/2508.12230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TASLP. 15 pages, 7 figures;&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自监督预训练模型的机器异常声音检测方法，通过全连接低秩自适应和机器感知组适配器模块提升模型泛化能力，并设计了处理缺失标签的新目标函数。&lt;h4&gt;背景&lt;/h4&gt;机器异常声音检测(ASD)是一项有价值的技术，但其泛化性能常受数据收集困难和声学环境复杂性的限制。预训练数据集与ASD任务之间存在不一致性。&lt;h4&gt;目的&lt;/h4&gt;引入鲁棒的ASD模型利用大规模语音和音频数据集上的自监督预训练模型；缓解微调时的过拟合；使模型能捕捉不同机器间的差异；解决属性标签缺失的挑战。&lt;h4&gt;方法&lt;/h4&gt;利用大规模语音和音频数据集上的自监督预训练模型；采用全连接低秩自适应(LoRA)替代完全微调；提出机器感知组适配器模块；设计使用向量量化对未标记数据进行动态聚类并通过双重对比学习损失优化的新目标函数。&lt;h4&gt;主要发现&lt;/h4&gt;尽管预训练数据集与ASD任务存在不一致，预训练仍能为ASD提供实质性好处；所提出方法在DCASE 2020-2024五个ASD挑战等基准数据集上表现优异，实验结果显示显著改进。&lt;h4&gt;结论&lt;/h4&gt;提出的策略和方法能有效提升ASD系统性能，自监督预训练模型结合特定适配方法可解决ASD中的泛化问题。&lt;h4&gt;翻译&lt;/h4&gt;机器异常声音检测(ASD)是一项在各种应用中都有价值的技术。然而，由于数据收集困难和声学环境的复杂性，其泛化性能通常受到限制。受众多领域中大型预训练模型成功的启发，本文引入了一个鲁棒的ASD模型，该模型利用在大型语音和音频数据集上训练的自监督预训练模型。尽管预训练数据集与ASD任务存在不一致性，但我们的研究结果表明预训练仍能为ASD提供实质性好处。为了在使用有限数据进行微调时缓解过拟合并保留已学习的知识，我们探索了全连接低秩自适应(LoRA)作为完全微调的替代方案。此外，我们提出了一个机器感知组适配器模块，使模型能够在统一框架内捕捉不同机器之间的差异，从而增强ASD系统的泛化性能。为了解决属性标签缺失的挑战，我们设计了一种新的目标函数，该函数使用向量量化对未标记数据进行动态聚类，并通过双重对比学习损失进行优化。所提出的方法在所有基准数据集上进行了评估，包括DCASE 2020-2024五个ASD挑战，实验结果表明我们的新方法有显著改进，并证明了所提出策略的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine anomalous sound detection (ASD) is a valuable technique acrossvarious applications. However, its generalization performance is often limiteddue to challenges in data collection and the complexity of acousticenvironments. Inspired by the success of large pre-trained models in numerousfields, this paper introduces a robust ASD model that leverages self-supervisedpre-trained models trained on large-scale speech and audio datasets. Althoughthere are inconsistencies between the pre-training datasets and the ASD task,our findings indicate that pre-training still provides substantial benefits forASD. To mitigate overfitting and retain learned knowledge when fine-tuning withlimited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as analternative to full fine-tuning. Additionally, we propose a Machine-aware GroupAdapter module, which enables the model to capture differences between variousmachines within a unified framework, thereby enhancing the generalizationperformance of ASD systems. To address the challenge of missing attributelabels, we design a novel objective function that dynamically clustersunattributed data using vector quantization and optimizes through a dual-levelcontrastive learning loss. The proposed methods are evaluated on all benchmarkdatasets, including the DCASE 2020-2024 five ASD challenges, and theexperimental results show significant improvements of our new approach anddemonstrate the effectiveness of our proposed strategies.</description>
      <author>example@mail.com (Bing Han, Anbai Jiang, Xinhu Zheng, Wei-Qiang Zhang, Jia Liu, Pingyi Fan, Yanmin Qian)</author>
      <guid isPermaLink="false">2508.12230v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine</title>
      <link>http://arxiv.org/abs/2508.12108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VELVET-Med是一个针对医学体积数据(如3D CT扫描)的新型视觉语言预训练框架，通过创新的预训练目标和模型架构，解决了医学领域大规模数据收集的挑战，在多种下游任务上展现出卓越的性能。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在医学领域越来越受到关注，但与2D图像和文本的直接配对不同，为体积模态如CT扫描收集大规模配对数据具有挑战性且耗时，这限制了下游任务的性能。&lt;h4&gt;目的&lt;/h4&gt;提出一个专门针对有限体积数据(如3D CT和相关放射学报告)的视觉语言预训练框架VELVET-Med，不依赖大规模数据收集，而是专注于有效的预训练目标和模型架构的开发。&lt;h4&gt;方法&lt;/h4&gt;将单模态自监督学习整合到VLP框架中；提出新型语言编码器TriBERT学习多级文本语义；设计分层对比学习捕获多级视觉语言对应关系；仅使用38,875个扫描-报告对。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能揭示体积医学图像和相关临床叙述中嵌入的丰富空间和语义关系，增强学习编码器的泛化能力，编码器表现出强大的可迁移性。&lt;h4&gt;结论&lt;/h4&gt;VELVET-Med在多种下游任务上取得了最先进的性能，包括3D分割、跨模态检索、视觉问答和报告生成，证明了其在医学视觉语言处理中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在医学领域越来越受到探索，特别是在CLIP在通用领域取得成功之后。然而，与2D图像和文本的直接配对不同，在医学领域为体积模态如CT扫描收集大规模配对数据仍然是一个具有挑战性和耗时的过程。这种困难常常限制了下游任务的性能。为解决这些挑战，我们提出了一种新型的视觉语言预训练框架，称为VELVET-Med，专门针对有限的体积数据如3D CT和相关的放射学报告。我们的方法不依赖大规模数据收集，而是专注于有效的预训练目标和模型架构的开发。主要贡献包括：1)我们将单模态自监督学习整合到VLP框架中，这在现有文献中常常未被充分探索；2)我们提出了一种新型语言编码器TriBERT，用于学习多级文本语义；3)我们设计了分层对比学习来捕获多级视觉语言对应关系。仅使用38,875个扫描-报告对，我们的方法旨在揭示体积医学图像和相关临床叙述中嵌入的丰富空间和语义关系，从而增强学习编码器的泛化能力。所得编码器表现出强大的可迁移性，在3D分割、跨模态检索、视觉问答和报告生成等一系列广泛的下游任务上取得了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学领域特别是3D体积成像数据（如CT扫描）与相应放射学报告配对数据稀缺的问题。这个问题很重要，因为医学数据获取面临患者隐私保护和专业标注需求的双重挑战，而3D数据比2D图像更复杂，需要更多样本；同时医学报告比普通图像描述更复杂，包含多层次临床概念，现有方法难以充分利用这些信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了医学视觉-语言模型面临的两大挑战：全局特征对齐忽略局部信息，以及医学报告的复杂层次结构未被充分利用。他们借鉴了CLIP的成功经验、自监督学习方法和现有VLM架构，但针对医学领域特点进行了创新。设计上不依赖大规模数据收集，而是专注于开发有效的预训练目标和模型架构，通过层次对比学习对齐不同粒度的视觉和文本特征，同时利用单模态自监督学习增强数据效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次对比学习对齐不同粒度的视觉和文本特征，同时利用单模态自监督学习提升数据效率，充分挖掘3D医学扫描中的空间和语义关系。整体流程包括：1)采用三编码器架构（视觉、语言、多模态）；2)准备不同尺度的视觉输入和多种文本嵌入；3)设计四类学习目标（单模态视觉、单模态语言、跨模态、多模态）；4)提出TriBERT处理医学报告层次结构；5)实现层次对比学习对齐不同级别的视觉和文本特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)VELVET-Med框架专为医学体积数据设计，在有限数据条件下表现优异；2)层次对比学习对齐多级视觉和文本特征，捕捉特定区域的医学概念；3)TriBERT语言编码器学习报告/句子/词级语义，通过特殊注意力掩码防止句子间信息泄漏；4)集成单模态自监督学习增强模型泛化能力；5)创建高质量M3D-CAP-filtered数据集。相比之前工作，VELVET-Med更关注3D而非2D医学数据，更好地处理医学报告的层次结构，同时考虑局部和全局特征，且不依赖大规模数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VELVET-Med通过创新的层次对比学习和TriBERT语言编码器，在有限医学体积数据条件下实现了先进的视觉-语言对齐，显著提升了3D医学图像理解任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-language models (VLMs) have been increasingly explored in themedical domain, particularly following the success of CLIP in general domain.However, unlike the relatively straightforward pairing of 2D images and text,curating large-scale paired data in the medical field for volumetric modalitiessuch as CT scans remains a challenging and time-intensive process. Thisdifficulty often limits the performance on downstream tasks. To address thesechallenges, we propose a novel vision-language pre-training (VLP) framework,termed as \textbf{VELVET-Med}, specifically designed for limited volumetricdata such as 3D CT and associated radiology reports. Instead of relying onlarge-scale data collection, our method focuses on the development of effectivepre-training objectives and model architectures. The key contributions are: 1)We incorporate uni-modal self-supervised learning into VLP framework, which areoften underexplored in the existing literature. 2) We propose a novel languageencoder, termed as \textbf{TriBERT}, for learning multi-level textualsemantics. 3) We devise the hierarchical contrastive learning to capturemulti-level vision-language correspondence. Using only 38,875 scan-reportpairs, our approach seeks to uncover rich spatial and semantic relationshipsembedded in volumetric medical images and corresponding clinical narratives,thereby enhancing the generalization ability of the learned encoders. Theresulting encoders exhibit strong transferability, achieving state-of-the-artperformance across a wide range of downstream tasks, including 3D segmentation,cross-modal retrieval, visual question answering, and report generation.</description>
      <author>example@mail.com (Ziyang Zhang, Yang Yu, Xulei Yang, Si Yong Yeo)</author>
      <guid isPermaLink="false">2508.12108v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases</title>
      <link>http://arxiv.org/abs/2508.12031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于指令的持续对比调优方法，用于大型语言模型在持续关系提取任务中，通过重视错误案例和利用LLM的指令跟随能力，有效减轻了灾难性遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;持续关系提取(CRE)旨在持续学习新出现的关系同时避免灾难性遗忘。现有CRE方法主要使用内存回放和对比学习来减轻灾难性遗忘，但这些方法没有重视能更有效揭示模型认知偏差的错误案例。&lt;h4&gt;目的&lt;/h4&gt;解决现有CRE方法未重视错误案例的问题，提出一种基于指令的持续对比调优方法，利用大型语言模型的指令跟随能力，更有效地减轻灾难性遗忘并处理新旧关系之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于指令的持续对比调优方法，将每个任务的训练和内存数据根据初始响应的正确性分成两部分，通过双任务微调分别处理；利用LLM的指令跟随能力，提出基于指令的对比调优策略，以指令调优方式用先前数据持续指导纠正当前认知偏差，更适合LLM的方式减轻新旧关系之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;在TACRED和FewRel数据集上的实验表明，该方法取得了新的最先进CRE性能，有显著改进，证明了专门利用错误案例的重要性。&lt;h4&gt;结论&lt;/h4&gt;专门利用错误案例对于提升持续关系提取性能至关重要，所提出的基于指令的持续对比调优方法能够有效减轻灾难性遗忘并处理新旧关系之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;持续关系提取(CRE)旨在持续学习新出现的关系同时避免灾难性遗忘。现有CRE方法主要使用内存回放和对比学习来减轻灾难性遗忘。然而，这些方法没有重视能更有效揭示模型认知偏差的错误案例。为解决这一问题，我们提出了一种基于指令的持续对比调优方法，应用于CRE中的大型语言模型(LLMs)。与现有统一处理训练和内存数据的CRE方法不同，该方法根据初始响应的正确性将每个任务的训练和内存数据分成两部分，并通过双任务微调分别处理它们。此外，利用LLM的指令跟随能力优势，我们提出了一种新颖的基于指令的对比调优策略，用于LLM，以指令调优方式用先前数据持续指导纠正当前认知偏差，从而以更适合LLM的方式减轻新旧关系之间的差距。我们在TACRED和FewRel上对模型进行了实验评估，结果表明我们的模型取得了新的最先进CRE性能，有显著改进，证明了专门利用错误案例的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Relation Extraction (CRE) aims to continually learn new emergingrelations while avoiding catastrophic forgetting. Existing CRE methods mainlyuse memory replay and contrastive learning to mitigate catastrophic forgetting.However, these methods do not attach importance to the error cases that canreveal the model's cognitive biases more effectively. To address this issue, wepropose an instruction-based continual contrastive tuning approach for LargeLanguage Models (LLMs) in CRE. Different from existing CRE methods thattypically handle the training and memory data in a unified manner, thisapproach splits the training and memory data of each task into two partsrespectively based on the correctness of the initial responses and treats themdifferently through dual-task fine-tuning. In addition, leveraging theadvantages of LLM's instruction-following ability, we propose a novelinstruction-based contrastive tuning strategy for LLM to continuously correctcurrent cognitive biases with the guidance of previous data in aninstruction-tuning manner, which mitigates the gap between old and newrelations in a more suitable way for LLMs. We experimentally evaluate our modelon TACRED and FewRel, and the results show that our model achieves newstate-of-the-art CRE performance with significant improvements, demonstratingthe importance of specializing in exploiting error cases.</description>
      <author>example@mail.com (Shaozhe Yin, Jinyu Guo, Kai Shuang, Xia Liu, Ruize Ou)</author>
      <guid isPermaLink="false">2508.12031v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach</title>
      <link>http://arxiv.org/abs/2508.11742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了首个基于攻击的基准评估方法，用于评估合成流量生成器(SynNetGens)的隐私性，并开发了TraceBleed攻击方法和TracePatch防御方法。&lt;h4&gt;背景&lt;/h4&gt;当前的合成流量生成器虽然承诺隐私保护但缺乏全面保证和实证验证，尽管其保真度不断提高。&lt;h4&gt;目的&lt;/h4&gt;引入首个直接从生成的流量中评估SynNetGens隐私性的基准，将隐私定义为流量源级别的成员推断。&lt;h4&gt;方法&lt;/h4&gt;提出TraceBleed攻击方法，利用流之间的行为指纹，采用对比学习和时间分块技术，比先前的成员推断基线效果高出172%；对基于GAN、扩散模型和GPT的SynNetGens进行了大规模研究。&lt;h4&gt;主要发现&lt;/h4&gt;SynNetGens会泄露用户级信息；差分隐私要么无法阻止攻击，要么严重降低保真度；分享更多合成数据会使泄露平均增加59%。&lt;h4&gt;结论&lt;/h4&gt;引入TracePatch，首个与SynNetGen无关的防御方法，结合对抗性机器学习和SMT约束来减轻泄露同时保持保真度。&lt;h4&gt;翻译&lt;/h4&gt;当前合成流量生成器(SynNetGens)承诺隐私但缺乏全面保证或实证验证，即使其保真度不断提高。我们引入了首个基于攻击的基准，用于直接从它们生成的流量中评估SynNetGens的隐私性。我们将隐私定义为流量源级别的成员推断——这对数据持有者来说是一个现实且可操作的威胁。为此，我们提出了TraceBleed，这是首个利用流之间行为指纹的攻击方法，使用对比学习和时间分块技术，比先前的成员推断基线高出172%。我们对基于GAN、扩散模型和GPT的SynNetGens进行的大规模研究发现了关键见解：(i) SynNetGens泄露用户级信息；(ii) 差分隐私要么无法阻止这些攻击，要么严重降低保真度；(iii) 分享更多合成数据会使泄露平均增加59%。最后，我们引入了TracePatch，这是首个与SynNetGen无关的防御方法，结合对抗性机器学习和SMT约束来减轻泄露同时保持保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current synthetic traffic generators (SynNetGens) promise privacy but lackcomprehensive guarantees or empirical validation, even as their fidelitysteadily improves. We introduce the first attack-grounded benchmark forassessing the privacy of SynNetGens directly from the traffic they produce. Weframe privacy as membership inference at the traffic-source level--a realisticand actionable threat for data holders. To this end, we present TraceBleed, thefirst attack that exploits behavioral fingerprints across flows usingcontrastive learning and temporal chunking, outperforming prior membershipinference baselines by 172%. Our large-scale study across GAN-, diffusion-, andGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-levelinformation; (ii) differential privacy either fails to stop these attacks orseverely degrades fidelity; and (iii) sharing more synthetic data amplifiesleakage by 59% on average. Finally, we introduce TracePatch, the firstSynNetGen-agnostic defense that combines adversarial ML with SMT constraints tomitigate leakage while preserving fidelity.</description>
      <author>example@mail.com (Minhao Jin, Hongyu He, Maria Apostolaki)</author>
      <guid isPermaLink="false">2508.11742v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning</title>
      <link>http://arxiv.org/abs/2508.11328v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HS-GPPT模型解决了现有图预训练方法无法处理多样化频谱分布的问题，通过频谱对齐实现了高效的知识迁移。&lt;h4&gt;背景&lt;/h4&gt;图预训练和提示调优可以使下游任务与预训练目标保持一致，从而在有限监督下实现高效的知识迁移。然而，现有方法依赖于基于同质性的低频知识，无法处理现实世界中具有不同同质性的图的多样化频谱分布。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，确保在预训练和提示调优过程中实现频谱对齐，以解决现有方法无法处理多样化频谱分布的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了HS-GPPT模型，使用混合频谱滤波器主干和局部-全局对比学习来获取丰富的频谱知识，并设计提示图以使频谱分布与预训练任务保持一致，促进跨越同质性和异质性的频谱知识迁移。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了一个频谱特异性原则：最优知识迁移需要预训练频谱滤波器与下游图的内在频谱保持一致；在有限监督下，预训练和下游任务之间的较大频谱差距会阻碍有效适应。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了HS-GPPT模型在直推学习和归纳学习设置下的有效性，代码已公开提供。&lt;h4&gt;翻译&lt;/h4&gt;图'预训练和提示调优'使下游任务与预训练目标保持一致，从而在有限监督下实现高效的知识迁移。然而，现有方法依赖于基于同质性的低频知识，无法处理现实世界中具有不同同质性的图的多样化频谱分布。我们的理论分析揭示了一个频谱特异性原则：最优知识迁移需要预训练频谱滤波器与下游图的内在频谱保持一致。在有限监督下，预训练和下游任务之间的较大频谱差距会阻碍有效适应。为了弥合这一差距，我们提出了HS-GPPT模型，这是一个新颖的框架，确保在预训练和提示调优过程中实现频谱对齐。我们使用混合频谱滤波器主干和局部-全局对比学习来获取丰富的频谱知识。然后我们设计提示图以使频谱分布与预训练任务保持一致，促进跨越同质性和异质性的频谱知识迁移。广泛的实验验证了在直推学习和归纳学习设置下的有效性。我们的代码可在https://anonymous.4open.science/r/HS-GPPT-62D2/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph ``pre-training and prompt-tuning'' aligns downstream tasks withpre-trained objectives to enable efficient knowledge transfer under limitedsupervision. However, existing methods rely on homophily-based low-frequencyknowledge, failing to handle diverse spectral distributions in real-worldgraphs with varying homophily. Our theoretical analysis reveals a spectralspecificity principle: optimal knowledge transfer requires alignment betweenpre-trained spectral filters and the intrinsic spectrum of downstream graphs.Under limited supervision, large spectral gaps between pre-training anddownstream tasks impede effective adaptation. To bridge this gap, we proposethe HS-GPPT model, a novel framework that ensures spectral alignment throughoutboth pre-training and prompt-tuning. We utilize a hybrid spectral filterbackbone and local-global contrastive learning to acquire abundant spectralknowledge. Then we design prompt graphs to align the spectral distribution withpretexts, facilitating spectral knowledge transfer across homophily andheterophily. Extensive experiments validate the effectiveness under bothtransductive and inductive learning settings. Our code is available athttps://anonymous.4open.science/r/HS-GPPT-62D2/.</description>
      <author>example@mail.com (Haitong Luo, Suhang Wang, Weiyao Zhang, Ruiqi Meng, Xuying Meng, Yujun Zhang)</author>
      <guid isPermaLink="false">2508.11328v2</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry</title>
      <link>http://arxiv.org/abs/2508.13111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为因果引导的成对Transformer（CGPT）的新型架构，解决了工业系统中多维时间序列建模中通道相关（CD）与通道独立（CI）模型之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;工业系统中多维时间序列的基础建模面临核心权衡：CD模型捕捉特定变量间动态但缺乏鲁棒性和适应性，而CI模型提供通用性但无法建模系统级预测任务中必要的显式交互。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉变量间动态并保持通用性和适应性的架构，解决CD/CI模型之间的权衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出CGPT架构，将已知因果图作为归纳偏置，采用成对建模范式将多维数据分解为对，使用通道不可知的学习层，在成对级别强制执行CD信息流，在跨对时实现类似CI的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实工业数据集上的长期和单步预测任务中，CGPT在预测准确性方面显著优于CI和CD基线模型，与端到端训练的CD模型相比具有竞争力，同时保持对问题维度的不可知性。&lt;h4&gt;结论&lt;/h4&gt;CGPT成功解决了工业系统中多维时间序列建模的CD/CI权衡问题，通过因果引导的成对Transformer架构实现了高预测精度和通用性。&lt;h4&gt;翻译&lt;/h4&gt;工业系统中多维时间序列的基础建模呈现一个核心权衡：通道相关(CD)模型捕捉特定变量间的动态，但缺乏鲁棒性和适应性，因为模型层通常受限于特定用例的数据维度；而通道独立(CI)模型提供通用性，但以建模系统级预测回归任务中必要的显式交互为代价。为此，我们提出了因果引导的成对Transformer(CGPT)，一种将已知因果图作为归纳偏置集成的新型架构。CGPT的核心围绕成对建模范式，通过将多维数据分解为对来解决CD/CI冲突。该模型使用通道不可知的学习层，所有参数维度独立于变量数量。CGPT在成对级别强制执行CD信息流，并在跨对时实现类似CI的泛化能力。这种方法解耦了复杂的系统动态，并产生高度灵活的架构，确保了可扩展性和任意变量适应性。我们在模拟常见工业复杂性的长期和单步预测任务中，在一系列合成和真实工业数据集上验证了CGPT。结果表明，CGPT在预测准确性方面显著优于CI和CD基线模型，同时保持对问题维度的不可知性，与端到端训练的CD模型相比具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational modelling of multi-dimensional time-series data in industrialsystems presents a central trade-off: channel-dependent (CD) models capturespecific cross-variable dynamics but lack robustness and adaptability as modellayers are commonly bound to the data dimensionality of the tackled use-case,while channel-independent (CI) models offer generality at the cost of modellingthe explicit interactions crucial for system-level predictive regression tasks.To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), anovel architecture that integrates a known causal graph as an inductive bias.The core of CGPT is built around a pairwise modeling paradigm, tackling theCD/CI conflict by decomposing the multidimensional data into pairs. The modeluses channel-agnostic learnable layers where all parameter dimensions areindependent of the number of variables. CGPT enforces a CD information flow atthe pair-level and CI-like generalization across pairs. This approachdisentangles complex system dynamics and results in a highly flexiblearchitecture that ensures scalability and any-variate adaptability. We validateCGPT on a suite of synthetic and real-world industrial datasets on long-termand one-step forecasting tasks designed to simulate common industrialcomplexities. Results demonstrate that CGPT significantly outperforms both CIand CD baselines in predictive accuracy and shows competitive performance withend-to-end trained CD models while remaining agnostic to the problemdimensionality.</description>
      <author>example@mail.com (Michael Mayr, Georgios C. Chasparis)</author>
      <guid isPermaLink="false">2508.13111v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields</title>
      <link>http://arxiv.org/abs/2508.12888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;aims-PAX是一个自动化的多轨迹主动学习框架，简化了机器学习力场(MLFF)的开发过程，提供了模块化、高性能的工作流程，支持灵活采样和可扩展训练，与FHI-aims和先进ML模型无缝集成，在测试案例中显著减少了参考计算需求并加速了AL周期，是学术和工业环境中下一代ML驱动原子模拟的强大平台。&lt;h4&gt;背景&lt;/h4&gt;机器学习力场(MLFF)的最新进展显著扩展了原子模拟的能力，这一进展突显了对可靠参考数据集、准确MLFF以及高效主动学习策略的迫切需求，这些对于复杂化学和材料系统的稳健建模至关重要。&lt;h4&gt;目的&lt;/h4&gt;介绍aims-PAX，一个自动化的多轨迹主动学习框架，旨在简化MLFF的开发过程，为专家和新手用户提供支持。&lt;h4&gt;方法&lt;/h4&gt;aims-PAX是一个模块化、高性能的工作流程，结合灵活的采样和可扩展的训练，支持CPU和GPU架构；基于广泛采用的从头计算代码FHI-aims构建，无缝集成最先进的ML模型，并支持使用通用模型进行预训练以便快速部署到各种系统中。&lt;h4&gt;主要发现&lt;/h4&gt;在高柔性肽和块状CsPbI₃钙钛矿两个具有挑战性的系统上，aims-PAX将所需的参考计算数量减少了高达两个数量级，通过优化资源利用实现了AL周期时间超过20倍的加速。&lt;h4&gt;结论&lt;/h4&gt;aims-PAX是一个强大且通用的平台，适用于下一代机器学习驱动的原子模拟，可在学术和工业环境中应用。&lt;h4&gt;翻译&lt;/h4&gt;机器学习力场(MLFF)的最新进展显著扩展了原子模拟的能力。这一进展突显了对可靠参考数据集、准确MLFF以及关键的高效主动学习策略的迫切需求，以实现复杂化学和材料系统的稳健建模。在此，我们介绍了aims-PAX，一个自动化的多轨迹主动学习框架，简化了MLFF的开发。aims-PAX专为专家和新手设计，提供模块化、高性能的工作流程，将灵活采样与可扩展训练相结合，支持CPU和GPU架构。基于广泛采用的从头计算代码FHI-aims构建，该框架无缝集成最先进的ML模型，并支持使用通用(或'基础')模型进行预训练，以便快速部署到各种系统中。我们在两个具有挑战性的系统上展示了aims-PAX的能力：高柔性肽和块状CsPbI₃钙钛矿。在这些案例中，aims-PAX将所需的参考计算数量减少了高达两个数量级，并通过优化资源利用实现了AL周期时间超过20倍的加速。这使aims-PAX成为学术和工业环境中下一代机器学习驱动原子模拟的强大而通用的平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in machine learning force fields (MLFF) have significantlyextended the capabilities of atomistic simulations. This progress highlightsthe critical need for reliable reference datasets, accurate MLFFs, and,crucially, efficient active learning strategies to enable robust modeling ofcomplex chemical and materials systems. Here, we introduce aims-PAX, anautomated, multi-trajectory active learning framework that streamlines thedevelopment of MLFFs. Designed for both experts and newcomers, aims-PAX offersa modular, high-performance workflow that couples flexible sampling withscalable training across CPU and GPU architectures. Built on the widely adoptedab initio code FHI-aims, the framework seamlessly integrates withstate-of-the-art ML models and supports pretraining using general-purpose (or"foundational") models for rapid deployment in diverse systems. We demonstratethe capabilities of aims-PAX on two challenging systems: a highly flexiblepeptide and bulk CsPbI$_3$ perovskite. Across these cases, aims-PAX achieves areduction of up to two orders of magnitude in the number of required referencecalculations and enables over 20x speedup in AL cycle time through optimizedresource utilization. This positions aims-PAX as a powerful and versatileplatform for next-generation ML-driven atomistic simulations in both academicand industrial settings.</description>
      <author>example@mail.com (Tobias Henkes, Shubham Sharma, Alexandre Tkatchenko, Mariana Rossi, Igor Poltavskyi)</author>
      <guid isPermaLink="false">2508.12888v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model for Skeleton-Based Human Action Understanding</title>
      <link>http://arxiv.org/abs/2508.12586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TPAMI, Code is available at:  https://github.com/wengwanjiang/FoundSkelModel&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个统一的基于骨骼的密集表示学习(USDRL)框架，作为基于骨骼的人类动作理解的基础模型，解决了现有方法在可扩展性和泛化能力方面的不足。&lt;h4&gt;背景&lt;/h4&gt;人类动作理解是智能运动感知领域的基础，基于骨骼的动作表示具有模态和设备无关性，在类人机器人控制与交互中有应用潜力，但现有方法缺乏处理多样化任务所需的可扩展性和泛化能力，且缺乏可适应广泛任务的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的基于骨骼的密集表示学习框架，作为基于骨骼的人类动作理解的基础模型，提高动作理解的可扩展性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;USDRL框架包含三个模块：1)基于Transformer的密集时空编码器(DSTE)，采用两个并行流学习时间动态和空间结构特征；2)多粒度特征解相关(MG-FD)，在时间、空间和实例域上协同执行特征解相关，减少维度冗余并增强信息提取；3)多视角一致性训练(MPCT)，采用多视角和多模态自监督一致性训练，前者增强高级语义学习并减轻低级差异影响，后者促进信息丰富的多模态特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;在9个基于骨骼的动作理解任务上的25个基准测试中进行了广泛实验，结果表明该方法显著优于当前最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作有望拓宽基于骨骼的动作理解研究范围，并鼓励更多关注密集预测任务的发展。&lt;h4&gt;翻译&lt;/h4&gt;人类动作理解作为智能运动感知领域的基础支柱，在类人机器人控制和交互中具有潜在应用。然而，现有工作往往缺乏处理多样化动作理解任务所需的可扩展性和泛化能力。本文提出了统一的基于骨骼的密集表示学习(USDRL)框架，作为基于骨骼的人类动作理解的基础模型。USDRL包含基于Transformer的密集时空编码器(DSTE)、多粒度特征解相关(MG-FD)和多视角一致性训练(MPCT)三个模块。DSTE采用两个并行流学习时间动态和空间结构特征；MG-FD在时间、空间和实例域上协同执行特征解相关，减少维度冗余并增强信息提取；MPCT采用多视角和多模态自监督一致性训练。我们在9个基于骨骼的动作理解任务上的25个基准测试中进行了广泛实验，该方法显著优于当前最先进的方法。我们希望这项工作能拓宽基于骨骼的动作理解研究范围，并鼓励更多关注密集预测任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于骨架的人体动作理解缺乏可扩展性和泛化能力的问题，特别是无法有效处理多样化的密集预测任务(如动作检测、动作预测等)。这个问题很重要，因为骨架数据作为模态和设备无关的人体表示，具有计算高效、隐私保护好的优势，在机器人控制、人机交互、虚拟环境等领域有广泛应用价值。现有方法要么需要大量标注数据，要么只关注粗粒度表示而忽略了细粒度表示对密集预测任务的重要性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自监督骨架表示学习方法的两类主要范式(掩码序列建模和对比学习)的局限性，然后提出使用特征解相关作为新的自监督学习范式。他们设计了包含三个核心组件的USDRL框架：密集时空编码器(DSTE)、多粒度特征解相关(MG-FD)和多视角一致性训练(MPCT)。该方法借鉴了Barlow Twins和VICREG的特征解相关思想，但将其应用于骨架动作理解；参考了传统的双流架构；受到对比学习和掩码序列建模的启发，但通过特征解相关简化了流程；在多模态融合方面采用了早期融合策略以提高效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多粒度特征解相关学习密集表示，同时保持样本内部一致性和样本间可区分性，使用双流编码器分别捕获时间动态和空间结构特征，并引入多视角一致性训练增强模型鲁棒性。整体流程：1)输入增强的3D骨架序列，重塑为时间域和空间域两个视图；2)通过嵌入层映射到特征空间；3)特征通过DSTE编码器生成密集表示；4)应用MaxPooling和连接操作得到压缩向量；5)通过三个领域特定投影器映射到新空间；6)应用多粒度特征解相关损失函数；7)在训练过程中使用多视角一致性训练增强表示学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出首个基于骨架的基础模型框架(USDRL)，专注于密集表示学习；2)设计多粒度特征解相关(MG-FD)方法；3)提出密集时空编码器(DSTE)；4)引入多视角一致性训练(MPCT)；5)全面适应多种下游任务。相比之前工作，不同之处在于：不需要解码器或复杂掩码策略(对比掩码序列建模)；不需要动量编码器或记忆库(对比对比学习)；同时学习粗粒度和细粒度表示；特别关注密集预测任务；使用早期融合策略处理多模态数据，计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了首个基于骨架的统一密集表示学习框架(USDRL)，通过多粒度特征解相关和多视角一致性训练，实现了对多种人体动作理解任务(包括密集预测任务)的高效泛化和优异性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human action understanding serves as a foundational pillar in the field ofintelligent motion perception. Skeletons serve as a modality- anddevice-agnostic representation for human modeling, and skeleton-based actionunderstanding has potential applications in humanoid robot control andinteraction. \RED{However, existing works often lack the scalability andgeneralization required to handle diverse action understanding tasks. There isno skeleton foundation model that can be adapted to a wide range of actionunderstanding tasks}. This paper presents a Unified Skeleton-based DenseRepresentation Learning (USDRL) framework, which serves as a foundational modelfor skeleton-based human action understanding. USDRL consists of aTransformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained FeatureDecorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). TheDSTE module adopts two parallel streams to learn temporal dynamic and spatialstructure features. The MG-FD module collaboratively performs featuredecorrelation across temporal, spatial, and instance domains to reducedimensional redundancy and enhance information extraction. The MPCT moduleemploys both multi-view and multi-modal self-supervised consistency training.The former enhances the learning of high-level semantics and mitigates theimpact of low-level discrepancies, while the latter effectively facilitates thelearning of informative multimodal features. We perform extensive experimentson 25 benchmarks across across 9 skeleton-based action understanding tasks,covering coarse prediction, dense prediction, and transferred prediction. Ourapproach significantly outperforms the current state-of-the-art methods. Wehope that this work would broaden the scope of research in skeleton-basedaction understanding and encourage more attention to dense prediction tasks.</description>
      <author>example@mail.com (Hongsong Wang, Wanjiang Weng, Junbo Wang, Fang Zhao, Guo-Sen Xie, Xin Geng, Liang Wang)</author>
      <guid isPermaLink="false">2508.12586v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>jXBW: Fast Substructure Search in Large-Scale JSONL Datasets for Foundation Model Applications</title>
      <link>http://arxiv.org/abs/2508.12536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;jXBW是一种快速的大规模JSONL数据集子结构搜索方法，显著提高了处理效率。&lt;h4&gt;背景&lt;/h4&gt;在JSON Lines数据集中进行子结构搜索对现代应用如基础模型提示工程至关重要，但现有方法因完全树遍历和子树匹配导致计算成本过高。&lt;h4&gt;目的&lt;/h4&gt;提出jXBW方法，解决大规模JSONL数据集子结构搜索的性能问题。&lt;h4&gt;方法&lt;/h4&gt;jXBW包含三项技术贡献：合并树表示保留个体身份；基于扩展伯劳斯-惠勒变换的简洁数据结构支持高效树导航；三步子结构搜索算法结合路径分解、祖先计算和自适应树标识符收集。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明jXBW优于现有方法，小数据集提速16倍，大数据集提速高达4,700倍，相比XML处理提速超过6×10^6倍，同时保持有竞争力的内存使用。&lt;h4&gt;结论&lt;/h4&gt;jXBW是一种高效的子结构搜索方法，显著提升了处理大规模JSONL数据集的性能。&lt;h4&gt;翻译&lt;/h4&gt;在JSON Lines数据集中进行子结构搜索对于现代应用（如基础模型中的提示工程）至关重要，但现有方法由于完全树遍历和子树匹配而存在计算成本过高的问题。我们提出了jXBW，一种用于大规模JSONL数据集子结构搜索的快速方法。我们的方法有三个关键技术贡献：(i)一种合并树表示，通过合并多个JSON对象的树同时保留个体身份；(ii)一种基于扩展伯劳斯-惠勒变换的简洁数据结构，支持高效的树导航和子路径搜索；(iii)一种有效的三步子结构搜索算法，结合路径分解、祖先计算和自适应树标识符收集，确保正确性同时避免完全树遍历。在真实数据集上的实验评估表明，jXBW持续优于现有方法，对于较小的数据集实现16倍的速度提升，对于较大的数据集实现高达4,700倍的速度提升（相比基于树的方法），相比基于XML的处理速度提升超过6×10^6倍，同时保持有竞争力的内存使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Substructure search in JSON Lines (JSONL) datasets is essential for modernapplications such as prompt engineering in foundation models, but existingmethods suffer from prohibitive computational costs due to exhaustive treetraversal and subtree matching. We present jXBW, a fast method for substructuresearch on large-scale JSONL datasets. Our method makes three key technicalcontributions: (i) a merged tree representation built by merging trees ofmultiple JSON objects while preserving individual identities, (ii) a succinctdata structure based on the eXtended Burrows-Wheeler Transform that enablesefficient tree navigation and subpath search, and (iii) an efficient three-stepsubstructure search algorithm that combines path decomposition, ancestorcomputation, and adaptive tree identifier collection to ensure correctnesswhile avoiding exhaustive tree traversal. Experimental evaluation on real-worlddatasets demonstrates that jXBW consistently outperforms existing methods,achieving speedups of 16$\times$ for smaller datasets and up to 4,700$\times$for larger datasets over tree-based approaches, and more than 6$\times$10$^6$over XML-based processing while maintaining competitive memory usage.</description>
      <author>example@mail.com (Yasuo Tabei)</author>
      <guid isPermaLink="false">2508.12536v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing</title>
      <link>http://arxiv.org/abs/2508.12409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;S5是首个可扩展的半监督语义分割框架，用于遥感领域，通过大规模未标记数据预训练和混合专家多数据集微调方法，显著提升了遥感基础模型在土地覆盖分割和物体检测任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;半监督语义分割通过伪标签和一致性学习利用未标记数据推动了遥感分析发展，但现有研究依赖小规模数据集和模型，限制了实际应用性。&lt;h4&gt;目的&lt;/h4&gt;提出S5框架，利用大量通常因像素级标注成本高而未被充分利用的未标记地球观测数据，提高遥感应用的实用性和性能。&lt;h4&gt;方法&lt;/h4&gt;构建RS4P-1M数据集，采用基于熵的过滤和多样性扩展策略；在大规模语料上预训练不同大小的遥感基础模型；采用基于混合专家的多数据集微调方法，使模型高效适应多个遥感基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;所得到的RSFMs在所有基准测试上实现了最先进的性能，证明了扩展半监督学习用于遥感应用的可行性。&lt;h4&gt;结论&lt;/h4&gt;S5框架证明了半监督学习在遥感领域的可扩展性，通过大规模数据集和模型实现了性能提升，所有数据集、代码和模型将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;半监督语义分割(S4)通过伪标签和一致性学习利用未标记数据推动了遥感(RS)分析的发展。然而，现有S4研究通常依赖小规模数据集和模型，限制了其实际适用性。为解决这一问题，我们提出了S5，这是首个用于遥感的可扩展半监督语义分割框架，它解锁了大量未标记地球观测数据的潜力，这些数据通常因像素级标注成本高而未被充分利用。基于现有的大规模RS数据集，S5引入了一种结合基于熵的过滤和多样性扩展的数据选择策略，从而创建了RS4P-1M数据集。使用此数据集，我们通过在这个大规模语料上预训练不同大小的遥感基础模型(RSFMs)，系统性地扩展了S4方法，显著提高了它们在土地覆盖分割和物体检测任务上的性能。此外，在微调过程中，我们集成了基于混合专家(MoE)的多数据集微调方法，使模型能够以更少的参数高效适应多个RS基准测试。这种方法提高了RSFMs在多样化RS基准测试上的泛化能力和多功能性。所得到的RSFMs在所有基准测试上实现了最先进的性能，证明了扩展半监督学习用于RS应用的可行性。所有数据集、代码和模型将在https://github.com/MiliLab/S5上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)analysis by leveraging unlabeled data through pseudo-labeling and consistencylearning. However, existing S4 studies often rely on small-scale datasets andmodels, limiting their practical applicability. To address this, we propose S5,the first scalable framework for semi-supervised semantic segmentation in RS,which unlocks the potential of vast unlabeled Earth observation data typicallyunderutilized due to costly pixel-level annotations. Built upon existinglarge-scale RS datasets, S5 introduces a data selection strategy thatintegrates entropy-based filtering and diversity expansion, resulting in theRS4P-1M dataset. Using this dataset, we systematically scales S4 methods bypre-training RS foundation models (RSFMs) of varying sizes on this extensivecorpus, significantly boosting their performance on land cover segmentation andobject detection tasks. Furthermore, during fine-tuning, we incorporate aMixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, whichenables efficient adaptation to multiple RS benchmarks with fewer parameters.This approach improves the generalization and versatility of RSFMs acrossdiverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performanceacross all benchmarks, underscoring the viability of scaling semi-supervisedlearning for RS applications. All datasets, code, and models will be releasedat https://github.com/MiliLab/S5</description>
      <author>example@mail.com (Liang Lv, Di Wang, Jing Zhang, Lefei Zhang)</author>
      <guid isPermaLink="false">2508.12409v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization</title>
      <link>http://arxiv.org/abs/2508.12292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为HuBERT-VIC的噪声鲁棒语音基础模型，通过方差、不变性和协方差正则化目标提高模型在噪声环境下的性能。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型在噪声环境下表现不佳，因为大多数模型主要在干净数据上训练，当遇到噪声语音时会出现性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决语音基础模型在噪声环境下的鲁棒性问题，提高模型对不同类型噪声的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出HuBERT-VIC模型，采用方差、不变性和协方差正则化(VICReg)目标来调整噪声语音表示的统计特性，使模型能够捕捉多样化的声学特征。&lt;h4&gt;主要发现&lt;/h4&gt;与在噪声语音上预训练的基线模型相比，HuBERT-VIC在LibriSpeech测试干净集上显示出23.3%的相对性能提升，在测试其他集上显示出13.2%的相对性能提升。&lt;h4&gt;结论&lt;/h4&gt;通过引入方差、不变性和协方差正则化目标，可以有效提高语音基础模型在噪声环境下的鲁棒性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型中的噪声鲁棒性一直是一个关键挑战，因为大多数模型主要在干净数据上训练，当模型暴露在有噪声的语音中时会出现性能下降。为了解决这个问题，我们提出了HuBERT-VIC，这是一种具有方差、不变性和协方差正则化目标的噪声鲁棒语音基础模型。这些目标调整了噪声语音表示的统计特性，使模型能够捕捉多样化的声学特征，并提高对不同类型噪声的泛化能力。当应用于HuBERT时，与在噪声语音上预训练的基线模型相比，我们的模型在LibriSpeech测试干净集上显示出23.3%的相对性能提升，在测试其他集上显示出13.2%的相对性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Noise robustness in speech foundation models (SFMs) has been a criticalchallenge, as most models are primarily trained on clean data and experienceperformance degradation when the models are exposed to noisy speech. To addressthis issue, we propose HuBERT-VIC, a noise-robust SFM with variance,in-variance, and covariance regularization (VICReg) objectives. Theseobjectives adjust the statistics of noisy speech representations, enabling themodel to capture diverse acoustic characteristics and improving thegeneralization ability across different types of noise. When applied to HuBERT,our model shows relative performance improvements of 23.3% on LibriSpeechtest-clean and 13.2% on test-other, compared to the baseline model pre-trainedon noisy speech.</description>
      <author>example@mail.com (Hyebin Ahn, Kangwook Jang, Hoirin Kim)</author>
      <guid isPermaLink="false">2508.12292v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval</title>
      <link>http://arxiv.org/abs/2508.12290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLAIR的方法，用于处理弱监督的零样本跨域图像检索问题，通过优化大型基础模型生成的噪声伪标签，设计多种对比损失函数，以及学习跨域映射函数来提升检索性能。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型（如CLIP）能够轻松为大量未标记数据生成伪标签，这使得传统的无监督零样本跨域图像检索方法变得不那么相关。&lt;h4&gt;目的&lt;/h4&gt;研究弱监督的零样本跨域图像检索（WSZS-CDIR），使用大型基础模型生成的噪声伪标签。&lt;h4&gt;方法&lt;/h4&gt;提出CLAIR方法，通过CLIP文本和图像特征之间的相似性得到的置信分数来优化噪声伪标签；设计实例间和簇间对比损失将图像编码到类感知的潜在空间；设计域间对比损失缓解域差异；使用闭式学习跨域映射函数对齐图像特征；引入可学习提示增强处理新类别的零样本泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在TUBerlin、Sketchy、Quickdraw和DomainNet零样本数据集上的实验表明，CLAIR相比现有最先进方法持续表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;CLAIR方法在弱监督的零样本跨域图像检索任务中表现优异，通过多种技术创新有效提升了检索性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，能够轻松为大量未标记数据生成伪标签的大型基础模型的增长，使得无监督的零样本跨域图像检索（UZS-CDIR）变得不那么相关。因此，在本文中，我们将注意力转向了由大型基础模型（如CLIP）生成噪声伪标签的弱监督ZS-CDIR（WSZS-CDIR）。为此，我们提出CLAIR，通过CLIP文本和图像特征之间的相似性得到的置信分数来优化噪声伪标签。此外，我们设计了实例间和簇间对比损失，将图像编码到类感知的潜在空间，以及域间对比损失来缓解域差异。我们还使用闭式学习了一种新的跨域映射函数，仅使用CLIP文本嵌入将图像特征从一个域投影到另一个域，从而进一步对齐图像特征以进行检索。最后，我们通过引入一组额外的可学习提示来增强CLAIR的零样本泛化能力，以处理新类别。我们在TUBerlin、Sketchy、Quickdraw和DomainNet零样本数据集上进行了大量实验，其中我们的CLAIR与现有最先进方法相比持续表现出优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent growth of large foundation models that can easily generatepseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-ShotCross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, wetherefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) withnoisy pseudo labels generated by large foundation models such as CLIP. To thisend, we propose CLAIR to refine the noisy pseudo-labels with a confidence scorefrom the similarity between the CLIP text and image features. Furthermore, wedesign inter-instance and inter-cluster contrastive losses to encode imagesinto a class-aware latent space, and an inter-domain contrastive loss toalleviate domain discrepancies. We also learn a novel cross-domain mappingfunction in closed-form, using only CLIP text embeddings to project imagefeatures from one domain to another, thereby further aligning the imagefeatures for retrieval. Finally, we enhance the zero-shot generalizationability of our CLAIR to handle novel categories by introducing an extra set oflearnable prompts. Extensive experiments are carried out using TUBerlin,Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIRconsistently shows superior performance compared to existing state-of-the-artmethods.</description>
      <author>example@mail.com (Chor Boon Tan, Conghui Hu, Gim Hee Lee)</author>
      <guid isPermaLink="false">2508.12290v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting</title>
      <link>http://arxiv.org/abs/2508.12260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Mantis是一种基于机制模拟训练的基础模型，能够在没有疾病特定数据和专家调优的情况下，跨疾病、地区和结果进行准确的传染病预测，即使在历史数据有限的环境中也能工作。&lt;h4&gt;背景&lt;/h4&gt;传染病预测在新型疫情爆发或资源有限的环境中受到限制，因为需要疾病特定数据、专门训练和专家调优。&lt;h4&gt;目的&lt;/h4&gt;开发Mantis模型，使其能够开箱即用地跨疾病、地区和结果进行预测，即使在历史数据有限的情况下也能提供准确预测。&lt;h4&gt;方法&lt;/h4&gt;Mantis构建在超过4亿个模拟日的疫情动态数据上，涵盖多种病原体、传播方式、干预措施和监测伪影，训练过程中不需要真实世界数据。&lt;h4&gt;主要发现&lt;/h4&gt;尽管训练时不需要真实世界数据，但Mantis在六种疾病上优于39个专家调优的模型，包括CDC的COVID-19预测中心的所有模型；Mantis能够推广到新的流行病学领域，捕捉基本传染动态；具有机制上的可解释性，能识别预测背后的潜在驱动因素；能提供8周时间范围的准确预测，使可用范围增加一倍以上。&lt;h4&gt;结论&lt;/h4&gt;Mantis具有通用性、可解释性和在传统模型失效的地方可部署的特点，为下一代疾病预测系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;在新型疫情爆发或资源有限的环境中，传染病预测一直受到需要疾病特定数据、专门训练和专家调优的限制。我们介绍了Mantis，这是一个完全基于机制模拟训练的基础模型，使其能够在没有历史数据的情况下，跨疾病、地区和结果进行开箱即用的预测。Mantis构建在超过4亿个模拟日的疫情动态数据上，涵盖多种病原体、传播方式、干预措施和监测伪影。尽管训练时不需要真实世界数据，但Mantis在六种疾病上优于我们测试的39个专家调优模型，包括CDC的COVID-19预测中心的所有模型。Mantis能够推广到新的流行病学领域，包括具有保留传播机制的疾病，证明它捕捉了基本的传染动态。重要的是，Mantis具有机制上的可解释性，使公共卫生决策者能够识别预测背后的潜在驱动因素。最后，Mantis能够提供8周时间范围的准确预测，使大多数模型的可用范围增加一倍以上，能够主动进行公共卫生规划。这些能力共同使Mantis成为下一代疾病预测系统的基础：通用、可解释，并在传统模型失效的地方可部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infectious disease forecasting in novel outbreaks or low resource settingshas been limited by the need for disease-specific data, bespoke training, andexpert tuning. We introduce Mantis, a foundation model trained entirely onmechanistic simulations, which enables out-of-the-box forecasting acrossdiseases, regions, and outcomes, even in settings with limited historical data.Mantis is built on over 400 million simulated days of outbreak dynamicsspanning diverse pathogens, transmission modes, interventions, and surveillanceartifacts. Despite requiring no real-world data during training, Mantisoutperformed 39 expert-tuned models we tested across six diseases, includingall models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novelepidemiological regimes, including diseases with held-out transmissionmechanisms, demonstrating that it captures fundamental contagion dynamics.Critically, Mantis is mechanistically interpretable, enabling public healthdecision-makers to identify the latent drivers behind its predictions. Finally,Mantis delivers accurate forecasts at 8-week horizons, more than doubling theactionable range of most models, enabling proactive public health planning.Together, these capabilities position Mantis as a foundation fornext-generation disease forecasting systems: general, interpretable, anddeployable where traditional models fail.</description>
      <author>example@mail.com (Carson Dudley, Reiden Magdaleno, Christopher Harding, Ananya Sharma, Emily Martin, Marisa Eisenberg)</author>
      <guid isPermaLink="false">2508.12260v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>What do Speech Foundation Models Learn? Analysis and Applications</title>
      <link>http://arxiv.org/abs/2508.12255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ph.D. Thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一个关于语音基础模型(SFMs)的研究，通过分析框架研究SFM中编码的知识，并在口语理解任务上评估其性能，为SFM的未来发展和应用提供指导。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型被设计为各种语音处理任务的通用表示。过去五年中，自监督和监督预训练模型取得了显著成功，但对SFMs获取的知识理解仍然滞后。&lt;h4&gt;目的&lt;/h4&gt;研究SFMs中编码的声学和语言学知识，评估其在口语理解任务上的性能，并提供工具和数据集以促进对SFMs的更好理解和应用。&lt;h4&gt;方法&lt;/h4&gt;提出轻量级分析框架，使用统计工具和无训练任务研究SFM层中编码的知识；进行跨多个SFM和统计工具的比较研究；为口语理解评估基准贡献了命名实体识别(NER)和命名实体定位(NEL)任务；开发基于SFM的NER和NEL方法；评估不同SFM和适应策略对端到端SLU模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;分析性见解对下游任务性能有具体影响；利用SFMs的端到端模型可以超越传统的级联方法；不同SFM和适应策略对SLU模型性能有显著影响。&lt;h4&gt;结论&lt;/h4&gt;论文解决了关于SFMs的先前未解答的问题，提供了工具和数据集以促进对SFMs的更好理解，使社区能够为未来模型开发和采用做出明智的设计选择。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型(SFMs)被设计为各种语音处理任务的通用表示。过去五年中，大量成功的自监督和监督预训练模型在各种下游任务上取得了令人印象深刻的性能。尽管SFMs的种类不断增长，但我们对其获取知识的理解仍然滞后。本文提出了一个轻量级分析框架，使用统计工具和无训练任务来研究SFM层中编码的声学和语言学知识。我们在多个SFM和统计工具之间进行了比较研究。我们的研究还表明，分析性见解对下游任务性能有具体影响。SFM的最终有效性取决于其在语音应用上的性能。然而，目前尚不清楚其优势是否扩展到需要比广泛研究的语音识别等任务更深层理解的口语理解(SLU)任务。对SLU的探索有限主要是由于缺乏相关数据集。为此，本文为口语理解评估基准贡献了任务，特别是口语命名实体识别(NER)和命名实体定位(NEL)。我们开发了基于SFM的NER和NEL方法，并发现利用SFMs的端到端(E2E)模型可以超越传统的级联(语音识别后接文本模型)方法。此外，我们评估了不同SFM和适应策略对端到端SLU模型在任务性能上的影响。总体而言，本文解决了关于SFMs的先前未解答的问题，提供了工具和数据集以促进我们的理解，并使社区能够为未来模型开发和采用做出明智的设计选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models (SFMs) are designed to serve as general-purposerepresentations for a wide range of speech-processing tasks. The last fiveyears have seen an influx of increasingly successful self-supervised andsupervised pre-trained models with impressive performance on various downstreamtasks.  Although the zoo of SFMs continues to grow, our understanding of theknowledge they acquire lags behind. This thesis presents a lightweight analysisframework using statistical tools and training-free tasks to investigate theacoustic and linguistic knowledge encoded in SFM layers. We conduct acomparative study across multiple SFMs and statistical tools. Our study alsoshows that the analytical insights have concrete implications for downstreamtask performance.  The effectiveness of an SFM is ultimately determined by its performance onspeech applications. Yet it remains unclear whether the benefits extend tospoken language understanding (SLU) tasks that require a deeper understandingthan widely studied ones, such as speech recognition. The limited explorationof SLU is primarily due to a lack of relevant datasets. To alleviate that, thisthesis contributes tasks, specifically spoken named entity recognition (NER)and named entity localization (NEL), to the Spoken Language UnderstandingEvaluation benchmark. We develop SFM-based approaches for NER and NEL, and findthat end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded(speech recognition followed by a text model) approaches. Further, we evaluateE2E SLU models across SFMs and adaptation strategies to assess the impact ontask performance.  Collectively, this thesis tackles previously unanswered questions about SFMs,providing tools and datasets to further our understanding and to enable thecommunity to make informed design choices for future model development andadoption.</description>
      <author>example@mail.com (Ankita Pasad)</author>
      <guid isPermaLink="false">2508.12255v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model</title>
      <link>http://arxiv.org/abs/2508.12190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DermNIO，一个皮肤病领域的基础模型，通过混合预训练框架解决了现有AI工具在皮肤病诊断中的局限性，在各种临床任务上表现出色，甚至在诊断准确率上超过了专业皮肤科医生。&lt;h4&gt;背景&lt;/h4&gt;皮肤病对全球医疗系统造成巨大负担，影响高达70%的人口，诊断过程复杂，且资源有限地区皮肤科医生严重短缺。现有AI工具虽然显示出潜力，但通常依赖大型手动标记数据集，仅针对狭窄特定任务构建，在现实环境中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个更通用、有效的皮肤病AI诊断工具，解决现有模型的局限性，提高在真实世界环境中的诊断效果和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;DermNIO在432,776张图像组成的精选数据集上训练（来自公共存储库、网络图像和专有收藏），采用新颖的混合预训练框架，通过半监督学习和知识引导的原型初始化增强了自监督学习范式。&lt;h4&gt;主要发现&lt;/h4&gt;DermNIO在20个数据集上评估时持续优于最先进模型，在高层次临床应用（如恶性肿瘤分类、疾病严重程度分级、多类别诊断和皮肤病图像描述）和低级任务（如皮肤病变分割）中均表现优异。在隐私保护的联邦学习场景以及不同皮肤类型和性别中表现出强大鲁棒性。盲法研究中，DermNIO达到95.79%的诊断准确率（临床医生为73.66%），AI辅助使临床医生表现提高17.21%。&lt;h4&gt;结论&lt;/h4&gt;DermNIO通过创新的训练方法和大规模数据集，成功解决了现有皮肤病AI模型的局限性，在各种临床任务上展现出卓越性能和泛化能力，具有显著的临床应用价值和辅助诊断潜力。&lt;h4&gt;翻译&lt;/h4&gt;皮肤病对全球医疗系统造成巨大负担，其高发病率（影响高达70%的人口）、复杂的诊断过程以及资源有限地区皮肤科医生的严重短缺是主要驱动因素。虽然人工智能工具在皮肤病图像分析方面显示出潜力，但当前模型存在局限性——它们通常依赖大型手动标记数据集，并且是为狭窄、特定任务构建的，因此在现实环境中效果不佳。为解决这些局限性，我们提出了DermNIO，一个皮肤病领域的基础模型。在来自三个来源（公共存储库、网络图像和专有收藏）的432,776张图像组成的精选数据集上进行训练，DermNIO采用了一种新颖的混合预训练框架，通过半监督学习和知识引导的原型初始化增强了自监督学习范式。这种集成方法不仅加深了对复杂皮肤病状况的理解，还显著提高了在各种临床任务上的泛化能力。在20个数据集上的评估表明，DermNIO在广泛任务中持续优于最先进模型。它在包括恶性肿瘤分类、疾病严重程度分级、多类别诊断和皮肤病图像描述等高层次临床应用中表现出色，同时在皮肤病变分割等低级任务上也实现了最先进性能。此外，DermNIO在隐私保护的联邦学习场景以及不同皮肤类型和性别中表现出强大的鲁棒性。在23名皮肤科参与的盲法读者研究中，DermNIO达到95.79%的诊断准确率（而临床医生为73.66%），AI辅助使临床医生的表现提高了17.21%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skin diseases impose a substantial burden on global healthcare systems,driven by their high prevalence (affecting up to 70% of the population),complex diagnostic processes, and a critical shortage of dermatologists inresource-limited areas. While artificial intelligence(AI) tools havedemonstrated promise in dermatological image analysis, current models facelimitations-they often rely on large, manually labeled datasets and are builtfor narrow, specific tasks, making them less effective in real-world settings.To tackle these limitations, we present DermNIO, a versatile foundation modelfor dermatology. Trained on a curated dataset of 432,776 images from threesources (public repositories, web-sourced images, and proprietary collections),DermNIO incorporates a novel hybrid pretraining framework that augments theself-supervised learning paradigm through semi-supervised learning andknowledge-guided prototype initialization. This integrated method not onlydeepens the understanding of complex dermatological conditions, but alsosubstantially enhances the generalization capability across various clinicaltasks. Evaluated across 20 datasets, DermNIO consistently outperformsstate-of-the-art models across a wide range of tasks. It excels in high-levelclinical applications including malignancy classification, disease severitygrading, multi-category diagnosis, and dermatological image caption, while alsoachieving state-of-the-art performance in low-level tasks such as skin lesionsegmentation. Furthermore, DermNIO demonstrates strong robustness inprivacy-preserving federated learning scenarios and across diverse skin typesand sexes. In a blinded reader study with 23 dermatologists, DermNIO achieved95.79% diagnostic accuracy (versus clinicians' 73.66%), and AI assistanceimproved clinician performance by 17.21%.</description>
      <author>example@mail.com (Jingkai Xu, De Cheng, Xiangqian Zhao, Jungang Yang, Zilong Wang, Xinyang Jiang, Xufang Luo, Lili Chen, Xiaoli Ning, Chengxu Li, Xinzhu Zhou, Xuejiao Song, Ang Li, Qingyue Xia, Zhou Zhuang, Hongfei Ouyang, Ke Xue, Yujun Sheng, Rusong Meng, Feng Xu, Xi Yang, Weimin Ma, Yusheng Lee, Dongsheng Li, Xinbo Gao, Jianming Liang, Lili Qiu, Nannan Wang, Xianbo Zuo, Cui Yong)</author>
      <guid isPermaLink="false">2508.12190v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Infusing fine-grained visual knowledge to Vision-Language Models</title>
      <link>http://arxiv.org/abs/2508.12137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCVW 2025 accepted paper. Workshop name: "What is Next in Multimodal  Foundation Models?"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对视觉语言模型（VLMs）的微调方法，旨在解决细粒度开放集视觉检索任务中预训练模型表现不佳的问题，同时避免灾难性遗忘，保留模型的通用视觉和跨模态能力。&lt;h4&gt;背景&lt;/h4&gt;大规模对比预训练产生的VLMs能广泛用于视觉和多模态任务，但在细粒度开放集视觉检索任务中表现不够理想，需要使用特定领域标注样本对视觉编码器进行微调。然而，直接微调通常会导致灾难性遗忘，严重削弱模型的通用能力。&lt;h4&gt;目的&lt;/h4&gt;设计一种微调方法，在实现细粒度领域适应的同时，有效保留预训练VLM的广泛多模态知识，解决验证集设计和超参数调整中的关键问题，确保方法的可复现性和鲁棒泛化。&lt;h4&gt;方法&lt;/h4&gt;从持续学习文献中汲取灵感，系统分析标准正则化技术并提出高效有效的组合策略；解决验证集设计和超参数调整中的关键问题；在细粒度和粗粒度图像-图像和图像-文本检索基准上评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在各种检索任务中取得了一致的强有力结果，特别值得注意的是，在微调过程中不使用任何文本数据或原始文本编码器的情况下，成功保留了视觉-文本对齐能力。&lt;h4&gt;结论&lt;/h4&gt;该微调方法能够在细粒度领域适应与保留预训练VLM的广泛多模态知识之间实现最佳平衡，为细粒度开放集视觉检索任务提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大规模对比预训练产生了强大的视觉语言模型（VLMs），能够为各种视觉和多模态任务生成有效的表示（嵌入）。然而，这些预训练的嵌入对于细粒度开放集视觉检索仍然不够理想，最先进的结果需要使用特定领域的标注样本对视觉编码器进行微调。直接进行这种微调通常会导致灾难性遗忘，严重削弱模型的通用视觉和跨模态能力。在这项工作中，我们提出了一种专门设计的微调方法，旨在实现细粒度领域适应与保留预训练VLM广泛多模态知识之间的最佳平衡。从持续学习文献中汲取灵感，我们系统地分析了旨在保留知识的标准正则化技术，并提出了一种高效有效的组合策略。此外，我们解决了验证集设计和超参数调整中常被忽视但至关重要的方面，以确保在不同数据集和预训练模型上的可复现性和鲁棒泛化。我们在细粒度和粗粒度图像-图像和图像-文本检索基准上广泛评估了我们的方法。我们的方法始终取得强有力结果，值得注意的是在微调过程中不使用任何文本数据或原始文本编码器的情况下保留了视觉-文本对齐。代码和模型见：https://github.com/nikosips/infusing&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale contrastive pre-training produces powerful Vision-and-LanguageModels (VLMs) capable of generating representations (embeddings) effective fora wide variety of visual and multimodal tasks. However, these pretrainedembeddings remain suboptimal for fine-grained open-set visual retrieval, wherestate-of-the-art results require fine-tuning the vision encoder using annotateddomain-specific samples. Naively performing such fine-tuning typically leads tocatastrophic forgetting, severely diminishing the model's general-purposevisual and cross-modal capabilities.  In this work, we propose a fine-tuning method explicitly designed to achieveoptimal balance between fine-grained domain adaptation and retention of thepretrained VLM's broad multimodal knowledge. Drawing inspiration from continuallearning literature, we systematically analyze standard regularizationtechniques aimed at knowledge retention and propose an efficient and effectivecombination strategy. Additionally, we address the commonly overlooked yetcritical aspects of validation set design and hyperparameter tuning to ensurereproducibility and robust generalization across datasets and pretrainedmodels. We extensively evaluate our method on both fine-grained andcoarse-grained image-image and image-text retrieval benchmarks. Our approachconsistently achieves strong results, notably retaining the visual-textalignment without utilizing any text data or the original text encoder duringfine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .</description>
      <author>example@mail.com (Nikolaos-Antonios Ypsilantis, Kaifeng Chen, André Araujo, Ondřej Chum)</author>
      <guid isPermaLink="false">2508.12137v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Generative Medical Event Models Improve with Scale</title>
      <link>http://arxiv.org/abs/2508.12104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Cosmos Medical Event Transformer (CoMET)模型，这是一种基于大规模医疗事件数据预训练的基础模型，能够有效捕获复杂临床动态，支持临床决策和改善患者结局。&lt;h4&gt;背景&lt;/h4&gt;实现个性化医疗需要从患者纵向历程中提取见解，这些历程可视为医疗事件序列。大规模医疗事件数据预训练的基础模型为生成真实世界证据和推广到多样化下游任务提供了有前景的方向。&lt;h4&gt;目的&lt;/h4&gt;开发能够从大规模医疗事件数据中学习的基础模型，建立预训练方法学，研究计算、标记和模型大小的幂律缩放关系。&lt;h4&gt;方法&lt;/h4&gt;使用Epic Cosmos数据集（包含310个医疗系统的16.3亿次就诊记录和3亿份去标识化的纵向健康记录），引入CoMET模型家族，预训练了多达10亿参数的计算最优模型系列。基于患者真实世界历史，CoMET自回归地生成下一个医疗事件，模拟患者健康时间线，并研究了78个真实世界任务。&lt;h4&gt;主要发现&lt;/h4&gt;建立了医疗事件数据最大的缩放定律研究；预训练的CoMET模型在不需要任务特定微调或少样本示例的情况下，通常优于或匹配任务特定的监督模型；CoMET的预测能力随着模型和预训练规模的扩大而持续提高。&lt;h4&gt;结论&lt;/h4&gt;CoMET作为生成性医疗事件基础模型，能够有效捕获复杂的临床动态，提供了可扩展和可推广的框架来支持临床决策、简化医疗保健运营和改善患者结局。&lt;h4&gt;翻译&lt;/h4&gt;实现大规模个性化医疗需要能够从患者纵向历程中提取见解的方法，这些历程可视为一系列医疗事件。在大型医疗事件数据上预训练的基础模型为扩展真实世界证据生成和推广到多样化下游任务提供了有前景的方向。使用Epic Cosmos数据集（包含来自310个医疗系统的3亿份去标识化纵向健康记录中的163亿次就诊记录），我们引入了Cosmos Medical Event Transformer (CoMET)模型家族，这是一系列仅在解码器上的transformer模型，在1.18亿患者代表1150亿个离散医疗事件（1510亿个标记）的数据上进行了预训练。我们展示了医疗事件数据最大的缩放定律研究，建立了预训练方法学，并揭示了计算、标记和模型大小的幂律缩放关系。基于此，我们预训练了一系列多达10亿参数的计算最优模型。基于患者的真实世界历史，CoMET自回归地生成下一个医疗事件，模拟患者健康时间线。我们研究了78个真实世界任务，包括诊断预测、疾病预后和医疗保健运营。值得注意的是，对于一个具有通用预训练和基于模拟推理的基础模型，CoMET在这些任务上通常优于或匹配任务特定的监督模型，而无需任务特定的微调或少样本示例。CoMET的预测能力随着模型和预训练规模的扩大而持续提高。我们的结果表明，CoMET作为一种生成性医疗事件基础模型，能够有效捕获复杂的临床动态，提供了一个可扩展和可推广的框架来支持临床决策、简化医疗保健运营和改善患者结局。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Realizing personalized medicine at scale calls for methods that distillinsights from longitudinal patient journeys, which can be viewed as a sequenceof medical events. Foundation models pretrained on large-scale medical eventdata represent a promising direction for scaling real-world evidence generationand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset withmedical events from de-identified longitudinal health records for 16.3 billionencounters over 300 million unique patient records from 310 health systems, weintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family ofdecoder-only transformer models pretrained on 118 million patients representing115 billion discrete medical events (151 billion tokens). We present thelargest scaling-law study for medical event data, establishing a methodologyfor pretraining and revealing power-law scaling relationships for compute,tokens, and model size. Based on this, we pretrained a series ofcompute-optimal models with up to 1 billion parameters. Conditioned on apatient's real-world history, CoMET autoregressively generates the next medicalevent, simulating patient health timelines. We studied 78 real-world tasks,including diagnosis prediction, disease prognosis, and healthcare operations.Remarkably for a foundation model with generic pretraining and simulation-basedinference, CoMET generally outperformed or matched task-specific supervisedmodels on these tasks, without requiring task-specific fine-tuning or few-shotexamples. CoMET's predictive power consistently improves as the model andpretraining scale. Our results show that CoMET, a generative medical eventfoundation model, can effectively capture complex clinical dynamics, providingan extensible and generalizable framework to support clinical decision-making,streamline healthcare operations, and improve patient outcomes.</description>
      <author>example@mail.com (Shane Waxler, Paul Blazek, Davis White, Daniel Sneider, Kevin Chung, Mani Nagarathnam, Patrick Williams, Hank Voeller, Karen Wong, Matthew Swanhorst, Sheng Zhang, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon, Andrew Loza, Daniella Meeker, Seth Hain, Rahul Shah)</author>
      <guid isPermaLink="false">2508.12104v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MAPF-World: Action World Model for Multi-Agent Path Finding</title>
      <link>http://arxiv.org/abs/2508.12087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAPF-World的新型多智能体路径规划方法，这是一种自回归动作世界模型，能够统一情况理解和动作生成，指导超越即时局部观察的决策。&lt;h4&gt;背景&lt;/h4&gt;多智能体路径规划（MAPF）是从指定起始位置到目标位置为多个智能体规划无冲突路径的问题，支撑着多机器人协调、机器人辅助物流等现实世界任务。现有去中心化可学习求解器在大规模MAPF中表现出色，但作为反应式策略模型，它们对环境时间动态和智能体间依赖性的建模有限，导致在复杂长期规划场景中性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有去中心化可学习求解器在复杂长期规划场景中的性能限制，通过提出MAPF-World模型，提高情境感知能力，实现更智能、协调和有远见的决策。&lt;h4&gt;方法&lt;/h4&gt;作者提出了MAPF-World，一种用于MAPF的自回归动作世界模型，通过未来状态和动作预测明确建模环境动态（包括空间特征和时间依赖性）。此外，引入了一个基于现实场景的自动地图生成器，用于捕获实际地图布局，以训练和评估MAPF求解器。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，MAPF-World优于最先进的可学习求解器，展现出对分布外案例的零样本泛化能力。值得注意的是，MAPF-World的模型尺寸小96.5%，数据减少92%。&lt;h4&gt;结论&lt;/h4&gt;MAPF-World通过明确建模环境动态和智能体间依赖关系，解决了现有方法在复杂长期规划场景中的局限性，实现了更智能、协调和有远见的决策，同时显著降低了模型大小和训练数据需求。&lt;h4&gt;翻译&lt;/h4&gt;多智能体路径规划（MAPF）是为多个智能体从指定的起始位置到目标位置规划无冲突路径的问题。它支撑着多种现实世界任务，包括多机器人协调、机器人辅助物流和社会导航。最近去中心化的可学习求解器在大规模MAPF中显示出巨大潜力，特别是当利用基础模型和大型数据集时。然而，这些智能体是反应式策略模型，对环境时间动态和智能体间依赖性的建模有限，导致在复杂长期规划场景中性能下降。为了解决这些限制，我们提出了MAPF-World，这是一种用于MAPF的自回归动作世界模型，它统一了情况理解和动作生成，指导超越即时局部观察的决策。它通过未来状态和动作预测明确建模环境动态（包括空间特征和时间依赖性），从而提高情境感知能力。通过整合这些预测的未来，MAPF-World能够在复杂的多智能体环境中实现更明智、协调和有远见的决策。此外，我们通过引入一个基于现实场景的自动地图生成器来增强MAPF基准，该生成器捕获实际的地图布局，用于训练和评估MAPF求解器。大量实验表明，MAPF-World优于最先进的可学习求解器，展现出对分布外案例的优越零样本泛化能力。值得注意的是，MAPF-World的模型尺寸小96.5%，数据减少92%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent path finding (MAPF) is the problem of planning conflict-freepaths from the designated start locations to goal positions for multipleagents. It underlies a variety of real-world tasks, including multi-robotcoordination, robot-assisted logistics, and social navigation. Recentdecentralized learnable solvers have shown great promise for large-scale MAPF,especially when leveraging foundation models and large datasets. However, theseagents are reactive policy models and exhibit limited modeling of environmentaltemporal dynamics and inter-agent dependencies, resulting in performancedegradation in complex, long-term planning scenarios. To address theselimitations, we propose MAPF-World, an autoregressive action world model forMAPF that unifies situation understanding and action generation, guidingdecisions beyond immediate local observations. It improves situationalawareness by explicitly modeling environmental dynamics, including spatialfeatures and temporal dependencies, through future state and actionsprediction. By incorporating these predicted futures, MAPF-World enables moreinformed, coordinated, and far-sighted decision-making, especially in complexmulti-agent settings. Furthermore, we augment MAPF benchmarks by introducing anautomatic map generator grounded in real-world scenarios, capturing practicalmap layouts for training and evaluating MAPF solvers. Extensive experimentsdemonstrate that MAPF-World outperforms state-of-the-art learnable solvers,showcasing superior zero-shot generalization to out-of-distribution cases.Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduceddata.</description>
      <author>example@mail.com (Zhanjiang Yang, Meng Li, Yang Shen, Yueming Li, Lijun Sun)</author>
      <guid isPermaLink="false">2508.12087v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Towards interpretable prediction of recurrence risk in breast cancer using pathology foundation models</title>
      <link>http://arxiv.org/abs/2508.12025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入MAKO框架，评估了基于组织病理学的基础模型预测乳腺癌复发风险的能力，发现这些模型能够有效替代转录组检测方法。&lt;h4&gt;背景&lt;/h4&gt;PAM50-based ROR-P评分用于指导非转移性、ER阳性、HER2阴性乳腺癌的复发风险分层，但这些转录组检测方法并非普遍可及。组织病理学是常规可用的，可能提供一种可扩展的替代方案。&lt;h4&gt;目的&lt;/h4&gt;开发并评估基于组织病理学的基础模型，用于从H&amp;E染色全切片图像中预测ROR-P评分，以替代转录组检测方法。&lt;h4&gt;方法&lt;/h4&gt;引入MAKO框架，评估12个病理学基础模型和2个非病理学基线，使用基于注意力的多实例学习方法预测ROR-P评分。模型在Carolina乳腺癌研究(CBCS)上进行训练和验证，并在TCGA BRCA上进行外部测试。&lt;h4&gt;主要发现&lt;/h4&gt;多个基础模型在分类、回归和生存任务上优于基线；CONCH实现了最高的ROC AUC；H-optimus-0和Virchow2与连续ROR-P评分显示出最高的相关性；所有病理学模型将CBCS参与者按复发分层的效果类似于转录组ROR-P；肿瘤区域对于高风险预测是必要且充分的；研究人员确定了复发的候选组织生物标志物。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了在精准肿瘤学中，可解释的组织学风险模型的潜力，表明组织病理学可以作为一种有效的替代方法来预测乳腺癌复发风险。&lt;h4&gt;翻译&lt;/h4&gt;转录组检测如基于PAM50的ROR-P评分可指导非转移性、ER阳性、HER2阴性乳腺癌的复发风险分层，但并非普遍可及。组织病理学是常规可用的，可能提供一种可扩展的替代方案。我们引入了MAKO框架，评估了12个病理学基础模型和2个非病理学基线，用于使用基于注意力的多实例学习方法从H&amp;E染色全切片图像预测ROR-P评分。模型在Carolina乳腺癌研究上进行训练和验证，并在TCGA BRCA上进行外部测试。多个基础模型在分类、回归和生存任务上优于基线。CONCH实现了最高的ROC AUC，而H-optimus-0和Virchow2与连续ROR-P评分显示出最高的相关性。所有病理学模型将CBCS参与者按复发分层的效果类似于转录组ROR-P。肿瘤区域对于高风险预测是必要且充分的，我们确定了复发的候选组织生物标志物。这些结果突显了可解释、基于组织学的风险模型在精准肿瘤学中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transcriptomic assays such as the PAM50-based ROR-P score guide recurrencerisk stratification in non-metastatic, ER-positive, HER2-negative breast cancerbut are not universally accessible. Histopathology is routinely available andmay offer a scalable alternative. We introduce MAKO, a benchmarking frameworkevaluating 12 pathology foundation models and two non-pathology baselines forpredicting ROR-P scores from H&amp;E-stained whole slide images usingattention-based multiple instance learning. Models were trained and validatedon the Carolina Breast Cancer Study and externally tested on TCGA BRCA. Severalfoundation models outperformed baselines across classification, regression, andsurvival tasks. CONCH achieved the highest ROC AUC, while H-optimus-0 andVirchow2 showed top correlation with continuous ROR-P scores. All pathologymodels stratified CBCS participants by recurrence similarly to transcriptomicROR-P. Tumor regions were necessary and sufficient for high-risk predictions,and we identified candidate tissue biomarkers of recurrence. These resultshighlight the promise of interpretable, histology-based risk models inprecision oncology.</description>
      <author>example@mail.com (Jakub R. Kaczmarzyk, Sarah C. Van Alsten, Alyssa J. Cozzo, Rajarsi Gupta, Peter K. Koo, Melissa A. Troester, Katherine A. Hoadley, Joel H. Saltz)</author>
      <guid isPermaLink="false">2508.12025v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Toward General Physical Intelligence for Resilient Agile Manufacturing Automation</title>
      <link>http://arxiv.org/abs/2508.11960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Advanced Engineering Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了基础模型，特别是视觉语言行动(VLA)模型在敏捷和人本制造中的应用。研究系统地调查了VLA模型在通用物理智能(GPI)背景下的最新进展，评估了它们在工业部署中的准备情况，并提出了将GPI整合到下一代工业生态系统中的挑战和方向。&lt;h4&gt;背景&lt;/h4&gt;敏捷和人本制造需要具有上下文推理能力和在非结构化环境中安全交互的弹性机器人解决方案。基础模型，特别是VLA模型，已经出现，能够将多模态感知、推理和物理基础行动融合为统一表示，称为通用物理智能(GPI)。然而，GPI在当代敏捷制造过程中的实际应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;为了填补GPI在敏捷制造中实际应用研究的空白，这篇实践性综述旨在系统地调查VLA模型的最新进展，比较领先实施，评估它们在工业部署中的准备情况，并确定开放的研究挑战和未来方向，以便更好地将GPI整合到与工业5.0一致的下一代工业生态系统中。&lt;h4&gt;方法&lt;/h4&gt;研究采用系统综述方法，调查了VLA模型在GPI背景下的最新进展。研究进行了全面的比较分析，评估了领先实施，并通过结构化消融研究评估了它们在工业部署中的准备情况。研究将最先进的技术组织成五个主题支柱：多感官表示学习、sim2real迁移、规划与控制、不确定性和安全措施以及基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;研究将最先进的技术组织成五个主题支柱：多感官表示学习、sim2real迁移、规划与控制、不确定性和安全措施以及基准测试。这些发现有助于理解VLA模型在GPI框架下的当前状态，并评估它们在工业环境中的适用性。&lt;h4&gt;结论&lt;/h4&gt;研究强调了将GPI整合到下一代工业生态系统中的开放研究挑战和未来方向，这些方向与工业5.0的目标一致。通过系统性地调查和比较VLA模型，研究为GPI在敏捷制造中的应用提供了重要见解，并指明了未来研究的方向。&lt;h4&gt;翻译&lt;/h4&gt;敏捷和人本制造规定了具有上下文推理能力和在非结构化环境中安全交互的弹性机器人解决方案。基础模型，特别是视觉语言行动(VLA)模型，已经出现，能够将多模态感知、推理和物理基础行动跨不同 embodiment 融合为统一表示，称为通用物理智能(GPI)。虽然GPI已在文献中有所描述，但其在当代敏捷制造过程中的实际应用和不断演变的作用尚未得到充分探索。为了填补这一空白，这篇实践性综述系统地调查了在GPI背景下VLA模型的最新进展，对领先实施进行了全面的比较分析，并通过结构化消融研究评估了它们在工业部署中的准备情况。我们的分析将最先进的技术组织成五个主题支柱，包括多感官表示学习、sim2real迁移、规划与控制、不确定性和安全措施以及基准测试。最后，我们阐述了开放的研究挑战和未来方向，以便更好地将GPI与工业5.0保持一致，整合到下一代工业生态系统中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何开发具有通用物理智能(GPI)的机器人系统，使其能够适应敏捷制造环境中的多样化任务和动态变化。这个问题很重要，因为市场需求正从大规模生产转向大规模定制，而中小企业缺乏技术基础设施来实现这种转型；现有机器人系统缺乏灵活性，难以处理不同任务、环境和形态的变化；工业机器人需要与人类协作并适应非结构化环境，而传统机器智能方法无法满足Industry 5.0愿景中的多样性需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到基础模型(特别是视觉-语言-动作VLA模型)能够融合多模态感知、推理和物理行动，然后将这些模型组织成五个主题支柱进行系统化研究。作者设计了一个分层控制架构，区分系统1(使用VLA模型和触觉反馈进行高级规划)和系统2(执行低级动作的快速控制器)。该方法大量借鉴了现有工作，包括基础模型如Gato、RT-2、PaLM-E，多模态感知技术，数据生成方法，规划框架以及安全估计方法，但针对工业应用进行了专门改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是开发通用物理智能(GPI)系统，使机器人能够通过整合多模态观测(视觉、语言、触觉、本体感觉)来执行多样化任务，并结合脑样计算能力与在现实世界中感知和物理行动的能力。整体实现流程包括：1)多感官表示学习，创建统一的潜在空间表示；2)数据生成和sim2real转移，解决数据收集瓶颈；3)规划和控制框架，处理复杂制造工作流程；4)不确定性估计和安全保障，确保可靠操作；5)基准测试和评估，系统衡量系统性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出专门针对敏捷制造的GPI框架；2)开发结合高级推理和精确运动的分层控制架构；3)对现有VLA模型(Gato、RT-2、PaLM-E等)进行工业适应性改进；4)进行全面的消融研究评估不同模型性能；5)确定五个主要研究挑战并提供具体建议。相比之前的工作，本文专注于工业制造环境而非一般机器人任务，强调触觉和本体感觉的重要性，提出更全面的评估框架，并解决实际部署中的实时性、可解释性和安全问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个通用物理智能(GPI)框架，通过整合多模态感知、分层控制和适应性学习，使机器人能够在敏捷制造环境中实现类似人类的灵活性和鲁棒性操作。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agile and human-centric manufacturing stipulates resilient robotic solutionscapable of contextual reasoning and safe interaction in unstructuredenvironments. Foundation models particularly the Vision Language Action (VLA)models have emerged to fuse multimodal perception, reasoning and physicallygrounded action across varied embodiments into unified representation, termedas General Physical Intelligence (GPI). While GPI has already been described inthe literature but its practical application and evolving role in contemporaryagile manufacturing processes have yet to be duly explored. To bridge this gap,this practical review systematically surveys recent advancements in VLA modelswithin GPI context, performs comprehensive comparative analysis of leadingimplementations and evaluates their readiness for industrial deployment throughstructured ablation study. Our analysis has organized state-of-the-art intofive thematic pillars including multisensory representation learning, sim2realtransfer, planning and control, uncertainty and safety measures andbenchmarking. Finally, we articulate open research challenges and proposedirections to better integrate GPI into next-generation industrial ecosystemsin line with Industry 5.0.</description>
      <author>example@mail.com (Sandeep Kanta, Mehrdad Tavassoli, Varun Teja Chirkuri, Venkata Akhil Kumar, Santhi Bharath Punati, Praveen Damacharla, Sunny Katyara)</author>
      <guid isPermaLink="false">2508.11960v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2508.11954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为UniCast的新型多模态框架，它扩展了时间序列基础模型以同时利用时间序列、视觉和文本模态，通过软提示调度的方法提高预测性能，同时保持基础模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测是金融、医疗保健和环境监测等领域的基础任务。尽管现有时间序列基础模型通过大规模预训练展示了强大的泛化能力，但它们主要在单模态环境中运行，忽略了现实场景中时间序列数据经常伴随的丰富多模态上下文，如视觉和文本信号。&lt;h4&gt;目的&lt;/h4&gt;开发一个参数高效的多模态框架，使时间序列基础模型能够联合利用时间序列、视觉和文本模态，以提高预测性能，同时保持基础模型的泛化能力并实现有效的跨模态交互。&lt;h4&gt;方法&lt;/h4&gt;提出UniCast框架，将预训练的视觉和文本编码器的模态特定嵌入与冻结的时间序列基础模型通过软提示调度的方法集成，实现高效适应且只需少量参数更新。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的时间序列预测基准测试中，UniCast一致且显著地优于所有现有的时间序列基础模型基线，证明了多模态上下文对提升预测性能的关键作用。&lt;h4&gt;结论&lt;/h4&gt;多模态上下文对于提升时间序列预测性能至关重要。UniCast框架通过有效整合时间序列、视觉和文本信息，为时间序列预测提供了一个更强大的方法，代表了时间序列基础模型发展的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测是金融、医疗保健和环境监测等领域的基础任务。尽管最近时间序列基础模型(TSFMs)的进展通过大规模预训练展示了强大的泛化能力，但现有模型主要在单模态环境中运行，忽略了现实场景中时间序列数据经常伴随的丰富的多模态上下文，如视觉和文本信号。本文介绍了一种新颖的参数高效多模态框架UniCast，它扩展了TSFMs，以联合利用时间序列、视觉和文本模态来增强预测性能。我们的方法通过软提示调度的方法，将预训练的视觉和文本编码器的模态特定嵌入与冻结的TSFM集成，实现高效适应且只需少量参数更新。这种设计不仅保留了基础模型的泛化能力，还实现了有效的跨模态交互。在多样化的时间序列预测基准上的广泛实验表明，UniCast一致且显著地优于所有现有的TSFM基线。这些发现强调了多模态上下文在推进下一代通用时间序列预测器方面的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting is a foundational task across domains, such asfinance, healthcare, and environmental monitoring. While recent advances inTime Series Foundation Models (TSFMs) have demonstrated strong generalisationthrough large-scale pretraining, existing models operate predominantly in aunimodal setting, ignoring the rich multimodal context, such as visual andtextual signals, that often accompanies time series data in real-worldscenarios. This paper introduces a novel parameter-efficient multimodalframework, UniCast, that extends TSFMs to jointly leverage time series, vision,and text modalities for enhanced forecasting performance. Our method integratesmodality-specific embeddings from pretrained Vision and Text Encoders with afrozen TSFM via soft prompt tuning, enabling efficient adaptation with minimalparameter updates. This design not only preserves the generalisation strengthof the foundation model but also enables effective cross-modal interaction.Extensive experiments across diverse time-series forecasting benchmarksdemonstrate that UniCast consistently and significantly outperforms allexisting TSFM baselines. The findings highlight the critical role of multimodalcontext in advancing the next generation of general-purpose time seriesforecasters.</description>
      <author>example@mail.com (Sehyuk Park, Soyeon Caren Han, Eduard Hovy)</author>
      <guid isPermaLink="false">2508.11954v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>QuarkMed Medical Foundation Model Technical Report</title>
      <link>http://arxiv.org/abs/2508.11894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;QuarkMed是一个高性能医疗基础模型，通过专业数据处理、检索增强生成和强化学习技术，解决了医疗应用对专业知识、准确性和定制化的需求&lt;h4&gt;背景&lt;/h4&gt;大语言模型的最新进展加速了其在医疗保健应用中的采用，包括AI医疗咨询、诊断报告辅助和医疗搜索工具&lt;h4&gt;目的&lt;/h4&gt;开发一个强大可靠的基础模型，满足医疗任务对高度专业知识、专业准确性和定制能力的需求&lt;h4&gt;方法&lt;/h4&gt;利用精心策划的医疗数据处理、医疗内容检索增强生成(RAG)以及大规模可验证的强化学习管道&lt;h4&gt;主要发现&lt;/h4&gt;QuarkMed模型在中国医学执照考试中达到70%的准确率，在多种医疗基准测试中表现出强大的泛化能力&lt;h4&gt;结论&lt;/h4&gt;QuarkMed提供了一个强大且多功能的个人医疗AI解决方案，已在ai.quark.cn服务数百万用户&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的最新进展显著加速了其在医疗保健应用中的采用，包括AI驱动的医疗咨询、诊断报告辅助和医疗搜索工具。然而，医疗任务通常需要高度专业的知识、专业准确性和定制能力，因此需要强大可靠的基础模型。QuarkMed通过精心策划的医疗数据处理、医疗内容检索增强生成(RAG)以及大规模可验证的强化学习管道来满足这些需求，开发出高性能医疗基础模型。该模型在中国医学执照考试中达到70%的准确率，显示出在多样化医疗基准测试中的强大泛化能力。QuarkMed提供了一个强大且多功能的个人医疗AI解决方案，已在ai.quark.cn服务数百万用户&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large language models have significantly acceleratedtheir adoption in healthcare applications, including AI-powered medicalconsultations, diagnostic report assistance, and medical search tools. However,medical tasks often demand highly specialized knowledge, professional accuracy,and customization capabilities, necessitating a robust and reliable foundationmodel. QuarkMed addresses these needs by leveraging curated medical dataprocessing, medical-content Retrieval-Augmented Generation (RAG), and alarge-scale, verifiable reinforcement learning pipeline to develop ahigh-performance medical foundation model. The model achieved 70% accuracy onthe Chinese Medical Licensing Examination, demonstrating strong generalizationacross diverse medical benchmarks. QuarkMed offers a powerful yet versatilepersonal medical AI solution, already serving over millions of users atai.quark.cn.</description>
      <author>example@mail.com (Ao Li, Bin Yan, Bingfeng Cai, Chenxi Li, Cunzhong Zhao, Fugen Yao, Gaoqiang Liu, Guanjun Jiang, Jian Xu, Liang Dong, Liansheng Sun, Rongshen Zhang, Xiaolei Gui, Xin Liu, Xin Shang, Yao Wu, Yu Cao, Zhenxin Ma, Zhuang Jia)</author>
      <guid isPermaLink="false">2508.11894v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Impact of Clinical Image Quality on Efficient Foundation Model Finetuning</title>
      <link>http://arxiv.org/abs/2508.11864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了特定领域视觉基础模型ProFound在前列腺多参数MRI中的应用，探讨了图像质量分布对标签效率微调的影响，发现微调和测试集中的图像质量比例匹配度对模型性能有显著影响。&lt;h4&gt;背景&lt;/h4&gt;医学影像基础模型在标签效率方面展现出前景，仅用少量标注数据即可实现高性能。ProFound是一个在大规模前列腺MRI数据上预训练的特定领域视觉基础模型。&lt;h4&gt;目的&lt;/h4&gt;研究图像质量的变化如何影响标签效率的微调，通过测量微调模型的泛化能力来评估这一影响。&lt;h4&gt;方法&lt;/h4&gt;实验系统性地改变微调和评估集中高质量与低质量图像的比例，研究不同质量分布对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;a) 微调和测试集中高质量到低质量图像比例的变化会导致下游性能显著差异；b) 微调集中存在足够的高质量图像对保持强大性能至关重要；c) 微调和测试分布的匹配度在不同下游任务(如自动放射学报告和前列腺癌检测)中的重要性不同；d) 当质量比例一致时，微调比从头开始训练需要少得多的标记数据，但标签效率取决于图像质量分布；e) 没有足够的高质量微调数据，预训练模型可能无法优于没有预训练的模型。&lt;h4&gt;结论&lt;/h4&gt;评估和微调与部署之间的质量分布一致性非常重要，特定下游任务需要制定微调数据的质量标准。使用ProFound的研究表明，量化微调和部署中的图像质量对于充分实现基础模型的数据和计算效率优势至关重要。&lt;h4&gt;翻译&lt;/h4&gt;医学影像领域的基础模型在标签效率方面显示出前景，仅用少量标注数据就能实现高性能。本文使用在大型前列腺MRI数据集上预训练的特定领域视觉基础模型ProFound，评估了其在前列腺多参数MRI中的应用。我们通过测量微调模型的泛化能力，研究了图像质量的变化如何影响标签效率的微调。实验系统性地改变了微调和评估集中高质量/低质量图像的比例。我们的研究结果表明，图像质量分布及其微调和测试不匹配显著影响模型性能。特别是：a) 微调和测试集中高质量到低质量图像比例的变化会导致下游性能显著差异；b) 微调集中存在足够的高质量图像对于保持强大性能至关重要，而微调和测试分布的匹配度在不同下游任务(如自动放射学报告和前列腺癌检测)中的重要性有所不同。当质量比例一致时，微调比从头开始训练需要少得多的标记数据，但标签效率取决于图像质量分布。如果没有足够的高质量微调数据，预训练模型可能无法优于没有预训练训练的模型。这强调了评估和微调与部署之间质量分布一致性的重要性，以及特定下游任务微调数据质量标准的必要性。使用ProFound，我们展示了量化微调和部署中图像质量的价值，以充分实现基础模型的数据和计算效率优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models in medical imaging have shown promising label efficiency,achieving high downstream performance with only a fraction of annotated data.Here, we evaluate this in prostate multiparametric MRI using ProFound, adomain-specific vision foundation model pretrained on large-scale prostate MRIdatasets. We investigate how variable image quality affects label-efficientfinetuning by measuring the generalisability of finetuned models. Experimentssystematically vary high-/low-quality image ratios in finetuning and evaluationsets. Our findings indicate that image quality distribution and itsfinetune-and-test mismatch significantly affect model performance. Inparticular: a) Varying the ratio of high- to low-quality images betweenfinetuning and test sets leads to notable differences in downstreamperformance; and b) The presence of sufficient high-quality images in thefinetuning set is critical for maintaining strong performance, whilst theimportance of matched finetuning and testing distribution varies betweendifferent downstream tasks, such as automated radiology reporting and prostatecancer detection.When quality ratios are consistent, finetuning needs far lesslabeled data than training from scratch, but label efficiency depends on imagequality distribution. Without enough high-quality finetuning data, pretrainedmodels may fail to outperform those trained without pretraining. Thishighlights the importance of assessing and aligning quality distributionsbetween finetuning and deployment, and the need for quality standards infinetuning data for specific downstream tasks. Using ProFound, we show thevalue of quantifying image quality in both finetuning and deployment to fullyrealise the data and compute efficiency benefits of foundation models.</description>
      <author>example@mail.com (Yucheng Tang, Pawel Rajwa, Alexander Ng, Yipei Wang, Wen Yan, Natasha Thorley, Aqua Asif, Clare Allen, Louise Dickinson, Francesco Giganti, Shonit Punwani, Daniel C. Alexander, Veeru Kasivisvanathan, Yipeng Hu)</author>
      <guid isPermaLink="false">2508.11864v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data</title>
      <link>http://arxiv.org/abs/2508.11794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Fed-Meta-Align的新型四阶段框架，用于在资源受限的物联网设备中进行实时故障分类，解决了联邦学习在非独立同分布数据环境下的模型发散问题。&lt;h4&gt;背景&lt;/h4&gt;在资源受限的物联网设备中进行实时故障分类对工业安全至关重要，但在异构环境中训练鲁棒模型仍然是一个重大挑战。标准的联邦学习在非独立同分布数据存在时往往会失败，导致模型发散。&lt;h4&gt;目的&lt;/h4&gt;设计一个新的框架来解决这些限制，通过复杂的初始化和训练流程来克服联邦学习在异构物联网环境中的挑战。&lt;h4&gt;方法&lt;/h4&gt;Fed-Meta-Align采用四阶段框架：1)在通用公共数据集上训练基础模型建立起点；2)串行元初始化阶段，在IoT设备数据子集上顺序训练，学习对异构性感知的初始化；3)并行FL阶段，使用双重标准聚合机制基于本地性能和余弦相似度对齐进行加权更新；4)设备上个性化阶段，将全局模型适配为每个IoT设备的专家模型。&lt;h4&gt;主要发现&lt;/h4&gt;Fed-Meta-Align在异构IoT设备上平均测试准确率达到91.27%，在电气和机械故障数据集上分别比个性化FedAvg和FedProx高出最高3.87%和3.37%。&lt;h4&gt;结论&lt;/h4&gt;这种顺序初始化和自适应聚合的多阶段方法为在各种TinyML网络上部署高性能智能提供了强大途径。&lt;h4&gt;翻译&lt;/h4&gt;在资源受限的物联网设备中进行实时故障分类对工业安全至关重要，但在这种异构环境中训练鲁棒模型仍然是一个重大挑战。标准的联邦学习在存在非独立同分布数据时往往会失败，导致模型发散。本文介绍了Fed-Meta-Align，一种新型四阶段框架，通过复杂的初始化和训练流程设计来克服这些限制。我们的过程首先在通用公共数据集上训练基础模型，建立一个有能力的起点。然后该模型经历串行元初始化阶段，在IoT设备数据的子集上顺序训练，学习对异构性感知的初始化，已经位于损失景观的有利区域。这个信息丰富的模型随后在并行FL阶段得到完善，该阶段使用双重标准聚合机制，基于本地性能和余弦相似度对齐对IoT设备更新进行加权。最后，设备上个性化阶段将收敛的全局模型适配为每个IoT设备的专家。全面的实验证明，Fed-Meta-Align在异构IoT设备上实现了91.27%的平均测试准确率，在电气和机械故障数据集上分别比个性化FedAvg和FedProx高出最高3.87%和3.37%。这种顺序初始化和自适应聚合的多阶段方法为在各种TinyML网络上部署高性能智能提供了强大途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time fault classification in resource-constrained Internet of Things(IoT) devices is critical for industrial safety, yet training robust models insuch heterogeneous environments remains a significant challenge. StandardFederated Learning (FL) often fails in the presence of non-IID data, leading tomodel divergence. This paper introduces Fed-Meta-Align, a novel four-phaseframework designed to overcome these limitations through a sophisticatedinitialization and training pipeline. Our process begins by training afoundational model on a general public dataset to establish a competentstarting point. This model then undergoes a serial meta-initialization phase,where it sequentially trains on a subset of IOT Device data to learn aheterogeneity-aware initialization that is already situated in a favorableregion of the loss landscape. This informed model is subsequently refined in aparallel FL phase, which utilizes a dual-criterion aggregation mechanism thatweights for IOT devices updates based on both local performance and cosinesimilarity alignment. Finally, an on-device personalization phase adapts theconverged global model into a specialized expert for each IOT Device.Comprehensive experiments demonstrate that Fed-Meta-Align achieves an averagetest accuracy of 91.27% across heterogeneous IOT devices, outperformingpersonalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical andmechanical fault datasets, respectively. This multi-stage approach of sequencedinitialization and adaptive aggregation provides a robust pathway for deployinghigh-performance intelligence on diverse TinyML networks.</description>
      <author>example@mail.com (Hemanth Macharla, Mayukha Pal)</author>
      <guid isPermaLink="false">2508.11794v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Geospatial Data Generation Using AlphaEarth Foundations Model</title>
      <link>http://arxiv.org/abs/2508.11739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种利用AlphaEarth Foundations（AEF）扩展地理空间标记数据集的方法，使其超出初始地理区域，并通过案例研究验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;高质量的标记地理空间数据集对于提取见解和理解地球至关重要，但这些数据集通常不覆盖全球，仅限于数据收集的特定地理区域。&lt;h4&gt;目的&lt;/h4&gt;提出并评估一种利用AEF扩展地理空间标记数据集的方法，使其超出初始地理区域。&lt;h4&gt;方法&lt;/h4&gt;利用AEF来扩展地理空间标记数据集，使用随机森林或逻辑回归等基本模型完成这一任务。研究了一个案例，将LANDFIRE的现有植被类型（EVT）数据集从美国扩展到加拿大，分为EvtPhys（13个类别）和EvtGp（80个类别）两个粒度级别。&lt;h4&gt;主要发现&lt;/h4&gt;对于EvtPhys，模型预测与地面实况一致。在美国和加拿大的EvtPhys验证集上，训练的模型分别实现了81%和73%的分类准确率。&lt;h4&gt;结论&lt;/h4&gt;尽管存在讨论的限制，但基本模型如随机森林或逻辑回归可以有效地用于扩展地理空间标记数据集。&lt;h4&gt;翻译&lt;/h4&gt;高质量的标记地理空间数据集对于提取见解和理解我们的地球至关重要。不幸的是，这些数据集通常不覆盖全球，仅限于数据收集的特定地理区域。谷歌DeepMind最近发布的AlphaEarth Foundations（AEF）提供了密集的全球地理空间表示，旨在作为各种任务的有用输入。在本文中，我们提出并评估了一种利用AEF扩展地理空间标记数据集超出其初始地理区域的方法。我们表明，即使是随机森林或逻辑回归等基本模型也可以用于完成这一任务。我们研究了一个案例，将LANDFIRE的现有植被类型（EVT）数据集从美国扩展到加拿大，分为两个粒度级别：EvtPhys（13个类别）和EvtGp（80个类别）。从定性上看，对于EvtPhys，模型预测与地面实况一致。尽管存在讨论的限制，训练的模型在美国和加拿大的EvtPhys验证集上分别实现了81%和73%的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality labeled geospatial datasets are essential for extractinginsights and understanding our planet. Unfortunately, these datasets often donot span the entire globe and are limited to certain geographic regions wheredata was collected. Google DeepMind's recently released AlphaEarth Foundations(AEF) provides an information-dense global geospatial representation designedto serve as a useful input across a wide gamut of tasks. In this article wepropose and evaluate a methodology which leverages AEF to extend geospatiallabeled datasets beyond their initial geographic regions. We show that evenbasic models like random forests or logistic regression can be used toaccomplish this task. We investigate a case study of extending LANDFIRE'sExisting Vegetation Type (EVT) dataset beyond the USA into Canada at two levelsof granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, forEvtPhys, model predictions align with ground truth. Trained models achieve 81%and 73% classification accuracy on EvtPhys validation sets in the USA andCanada, despite discussed limitations.</description>
      <author>example@mail.com (Luc Houriez, Sebastian Pilarski, Behzad Vahedi, Ali Ahmadalipour, Teo Honda Scully, Nicholas Aflitto, David Andre, Caroline Jaffe, Martha Wedner, Rich Mazzola, Josh Jeffery, Ben Messinger, Sage McGinley-Smith, Sarah Russell)</author>
      <guid isPermaLink="false">2508.11739v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation</title>
      <link>http://arxiv.org/abs/2508.11738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性回顾了2019-2024年间发表的109项研究，探讨了人工智能技术在解决农村医疗挑战方面的潜力，包括基础设施不足、劳动力短缺和社会经济差异等问题。&lt;h4&gt;背景&lt;/h4&gt;农村医疗面临持续挑战，包括基础设施不足、劳动力短缺和社会经济差异，这些因素阻碍了基本医疗服务的获取。&lt;h4&gt;目的&lt;/h4&gt;研究人工智能在解决农村地区医疗问题方面的变革潜力，重点关注服务不足的农村地区。&lt;h4&gt;方法&lt;/h4&gt;系统性回顾了2019年至2024年间发表的109项研究，数据来源包括PubMed、Embase、Web of Science、IEEE Xplore和Scopus，使用PRISMA指南和Covidence软件筛选文章，并进行主题分析以确定农村医疗中AI实施的关键模式和见解。&lt;h4&gt;主要发现&lt;/h4&gt;AI应用在改善医疗可及性、质量和效率方面显示出巨大潜力；多模态基础模型(MFMs)和大语言模型(LLMs)具有特别显著的变革潜力；MFMs整合多种数据源支持全面决策，LLMs有助于临床文档记录、患者分诊、翻译和虚拟协助；这些技术可以通过增强人类能力、减少诊断延迟和民主化专业知识获取来彻底改变农村医疗。&lt;h4&gt;结论&lt;/h4&gt;农村医疗AI应用仍面临基础设施限制、数据质量问题和伦理考虑等障碍；解决这些挑战需要跨学科合作、数字基础设施投资和监管框架发展；本研究提供了可操作建议并指出了未来研究方向，以确保AI在农村医疗系统中的公平和可持续整合。&lt;h4&gt;翻译&lt;/h4&gt;农村医疗面临持续挑战，包括基础设施不足、劳动力短缺和社会经济差异，这些因素阻碍了基本医疗服务的获取。本研究探讨了人工智能在解决农村地区这些问题方面的变革潜力。我们系统性回顾了2019年至2024年间发表的109项研究，数据来源包括PubMed、Embase、Web of Science、IEEE Xplore和Scopus。文章使用PRISMA指南和Covidence软件进行筛选，并进行了主题分析以确定农村医疗中AI实施的关键模式和见解。研究结果显示AI应用在改善医疗可及性、质量和效率方面显示出巨大潜力。其中，多模态基础模型(MFMs)和大语言模型(LLMs)具有特别显著的变革潜力。MFMs整合多种数据源（如图像、临床记录和生物信号）以支持全面决策，而LLMs有助于临床文档记录、患者分诊、翻译和虚拟协助。这些技术可以通过增强人类能力、减少诊断延迟和民主化专业知识获取来彻底改变农村医疗。然而，仍存在障碍，包括基础设施限制、数据质量问题和伦理考虑。解决这些挑战需要跨学科合作、对数字基础设施的投资以及监管框架的发展。本综述提供了可操作建议并指出了未来研究的领域，以确保人工智能在农村医疗系统中的公平和可持续整合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rural healthcare faces persistent challenges, including inadequateinfrastructure, workforce shortages, and socioeconomic disparities that hinderaccess to essential services. This study investigates the transformativepotential of artificial intelligence (AI) in addressing these issues inunderserved rural areas. We systematically reviewed 109 studies publishedbetween 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, andScopus. Articles were screened using PRISMA guidelines and Covidence software.A thematic analysis was conducted to identify key patterns and insightsregarding AI implementation in rural healthcare delivery. The findings revealsignificant promise for AI applications, such as predictive analytics,telemedicine platforms, and automated diagnostic tools, in improving healthcareaccessibility, quality, and efficiency. Among these, advanced AI systems,including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs),offer particularly transformative potential. MFMs integrate diverse datasources, such as imaging, clinical records, and bio signals, to supportcomprehensive decision-making, while LLMs facilitate clinical documentation,patient triage, translation, and virtual assistance. Together, thesetechnologies can revolutionize rural healthcare by augmenting human capacity,reducing diagnostic delays, and democratizing access to expertise. However,barriers remain, including infrastructural limitations, data quality concerns,and ethical considerations. Addressing these challenges requiresinterdisciplinary collaboration, investment in digital infrastructure, and thedevelopment of regulatory frameworks. This review offers actionablerecommendations and highlights areas for future research to ensure equitableand sustainable integration of AI in rural healthcare systems.</description>
      <author>example@mail.com (Kiruthika Balakrishnan, Durgadevi Velusamy, Hana E. Hinkle, Zhi Li, Karthikeyan Ramasamy, Hikmat Khan, Srini Ramaswamy, Pir Masoom Shah)</author>
      <guid isPermaLink="false">2508.11738v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</title>
      <link>http://arxiv.org/abs/2508.11584v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了视觉感知引擎（VPEngine），一个模块化框架，旨在解决资源受限机器人平台上多模型部署的冗余计算、大内存占用和复杂集成挑战，实现视觉多任务的高效GPU使用。&lt;h4&gt;背景&lt;/h4&gt;在资源受限的机器人平台上为不同的感知任务部署多个机器学习模型，常常导致冗余计算、大内存占用和复杂的集成挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个模块化框架（VPEngine），使视觉多任务能够高效利用GPU，同时保持可扩展性和开发者可访问性。&lt;h4&gt;方法&lt;/h4&gt;框架架构利用共享的基础模型主干提取图像表示，这些表示可以在并行运行的多专业化任务专用模型头之间高效共享，无需不必要的GPU-CPU内存传输。这种设计消除了传统顺序模型中特征提取的计算冗余，并支持动态任务优先级排序。示例实现使用DINOv2作为基础模型，具有深度、目标检测和语义分割多个任务头。&lt;h4&gt;主要发现&lt;/h4&gt;与顺序执行相比，实现了高达3倍的加速；基于CUDA Multi-Process Service (MPS)，提供高效的GPU利用率和恒定的内存占用；允许在运行时动态调整每个任务的推理频率；在NVIDIA Jetson Orin AGX上，对TensorRT优化模型的端到端实时性能达到≥50 Hz。&lt;h4&gt;结论&lt;/h4&gt;VPEngine是一个用Python编写的开源框架，具有ROS2 C++ (Humble)绑定，便于机器人社区在各种机器人平台上使用，有效解决了多模型部署挑战，实现了高效GPU利用和实时性能。&lt;h4&gt;翻译&lt;/h4&gt;在资源受限的机器人平台上为不同的感知任务部署多个机器学习模型，常常导致冗余计算、大内存占用和复杂的集成挑战。对此，本文提出了视觉感知引擎（VPEngine），这是一个模块化框架，旨在实现视觉多任务的高效GPU使用，同时保持可扩展性和开发者可访问性。我们的框架架构利用共享的基础模型主干来提取图像表示，这些表示可以在并行运行的多专业化任务专用模型头之间高效共享，无需任何不必要的GPU-CPU内存传输。这种设计消除了部署传统顺序模型时特征提取组件中固有的计算冗余，同时能够根据应用需求实现动态任务优先级排序。我们通过使用DINOv2作为基础模型和多个任务（深度、目标检测和语义分割）头的示例实现来展示我们框架的能力，与顺序执行相比实现了高达3倍的加速。基于CUDA Multi-Process Service (MPS)，VPEngine提供高效的GPU利用率和恒定的内存占用，同时允许在运行时动态调整每个任务的推理频率。该框架用Python编写，是开源的，并具有ROS2 C++ (Humble)绑定，便于机器人社区在各种机器人平台上使用。我们的示例实现在NVIDIA Jetson Orin AGX上对TensorRT优化模型的端到端实时性能达到了≥50 Hz。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying multiple machine learning models on resource-constrained roboticplatforms for different perception tasks often results in redundantcomputations, large memory footprints, and complex integration challenges. Inresponse, this work presents Visual Perception Engine (VPEngine), a modularframework designed to enable efficient GPU usage for visual multitasking whilemaintaining extensibility and developer accessibility. Our frameworkarchitecture leverages a shared foundation model backbone that extracts imagerepresentations, which are efficiently shared, without any unnecessary GPU-CPUmemory transfers, across multiple specialized task-specific model heads runningin parallel. This design eliminates the computational redundancy inherent infeature extraction component when deploying traditional sequential models whileenabling dynamic task prioritization based on application demands. Wedemonstrate our framework's capabilities through an example implementationusing DINOv2 as the foundation model with multiple task (depth, objectdetection and semantic segmentation) heads, achieving up to 3x speedup comparedto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngineoffers efficient GPU utilization and maintains a constant memory footprintwhile allowing per-task inference frequencies to be adjusted dynamically duringruntime. The framework is written in Python and is open source with ROS2 C++(Humble) bindings for ease of use by the robotics community across diverserobotic platforms. Our example implementation demonstrates end-to-end real-timeperformance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimizedmodels.</description>
      <author>example@mail.com (Jakub Łucki, Jonathan Becktor, Georgios Georgakis, Rob Royce, Shehryar Khattak)</author>
      <guid isPermaLink="false">2508.11584v2</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction</title>
      <link>http://arxiv.org/abs/2508.11728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniDCF是一个统一的框架，能够通过多模态融合编码重建多种口腔颅面硬组织，克服了现有单模态方法的局限性，实现了快速、自动化和高保真度的重建。&lt;h4&gt;背景&lt;/h4&gt;口腔颅面硬组织缺陷严重影响患者的生理功能、面部美观和心理健康，精确重建面临挑战。现有的深度学习模型局限于单组织和特定模态的成像输入，导致泛化性差，且在解剖保真度、计算效率和跨组织适应性之间存在权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够通过多模态融合编码重建多种口腔颅面硬组织的统一框架。&lt;h4&gt;方法&lt;/h4&gt;提出UniDCF框架，通过点云和多视图图像的多模态融合编码来重建多种口腔颅面硬组织。利用每种模态的互补优势，并引入基于评分的降噪模块来优化表面平滑度。作者整理了包含6,609名患者的口腔扫描、CBCT和CT的多模态数据集，共54,555个标注实例。&lt;h4&gt;主要发现&lt;/h4&gt;UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进的方法。临床模拟表明，UniDCF将重建设计时间减少了99%，并达到超过94%的临床医生可接受性。&lt;h4&gt;结论&lt;/h4&gt;UniDCF实现了快速、自动化和高保真度的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者预后。&lt;h4&gt;翻译&lt;/h4&gt;口腔颅面硬组织缺陷严重影响患者的生理功能、面部美观和心理健康，对精确重建构成重大挑战。当前的深度学习模型仅限于单组织场景和特定模态的成像输入，导致泛化性差，且在解剖保真度、计算效率和跨组织适应性之间存在权衡。在此，我们引入UniDCF，这是一个统一框架，能够通过点云和多视图图像的多模态融合编码来重建多种口腔颅面硬组织。通过利用每种模态的互补优势，并纳入基于评分的降噪模块来优化表面平滑度，UniDCF克服了先前单模态方法的局限性。我们整理了最大的多模态数据集，包含6,609名患者的口腔扫描、CBCT和CT，共54,555个标注实例。评估显示，UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进的方法。临床模拟表明，UniDCF将重建设计时间减少了99%，并达到超过94%的临床医生可接受性。总体而言，UniDCF实现了快速、自动化和高保真度的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者预后。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决牙颅面硬组织（包括颅骨、牙齿和颌骨）缺陷的精确重建问题。现实中，这类缺陷严重影响患者的生理功能、面部美观和心理健康，由创伤、肿瘤切除、先天性畸形或疾病引起。随着这些疾病的发病率和复杂性增长，临床迫切需要精确、高效和个性化的重建解决方案，而传统CAD方法劳动密集且依赖专家经验，现有AI方法又局限于单一组织场景，难以满足全面重建需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前深度学习模型的局限性，认识到缺乏大规模多模态数据集和单模态方法难以平衡解剖保真度、计算效率和跨组织适应性这两个核心挑战。他们设计了一个统一框架，通过多模态融合（点云和多视图图像）重建多种牙颅面硬组织。该方法借鉴了现有工作：基于AdaPoinTr点云完成骨干网络，使用Transformer架构进行几何推理，采用基于分数的去噪方法，并整合了多个公共数据集和临床队列数据集，构建了迄今为止最大的整合牙颅面硬组织数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态融合（点云和多视图图像）重建多种牙颅面硬组织，利用不同模态的互补优势，并引入基于分数的去噪模块提高几何平滑度。整体流程：1)数据预处理：将原始多模态输入转换为稀疏点云和多视图灰度图像；2)模型架构：基于AdaPoinTr骨干网络，使用几何感知的Transformer编码器-解码器处理点云，通过多头注意力机制融合图像特征，最后使用基于分数的去噪模块优化结果；3)输出：生成解剖准确的牙颅面重建结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的框架设计UniDCF，能够重建多种牙颅面硬组织；2)构建了包含6,609名患者54,555个样本的大规模多模态数据集；3)多模态融合技术，利用点云和图像的互补优势；4)基于分数的去噪模块提高表面平滑度；5)跨组织适应性，从牙冠到颅面骨植入的端到端重建。相比之前工作，UniDCF突破了单组织领域限制，在几何精度、结构完整性和空间准确性方面表现更优，临床设计时间减少99%，医生可接受性超过94%，实现了真正统一、高效、精确的重建方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniDCF通过统一的多模态深度学习框架，实现了对牙颅面硬组织的高精度、自动化重建，显著提高了临床效率和治疗效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dentocraniofacial hard tissue defects profoundly affect patients'physiological functions, facial aesthetics, and psychological well-being,posing significant challenges for precise reconstruction. Current deep learningmodels are limited to single-tissue scenarios and modality-specific imaginginputs, resulting in poor generalizability and trade-offs between anatomicalfidelity, computational efficiency, and cross-tissue adaptability. Here weintroduce UniDCF, a unified framework capable of reconstructing multipledentocraniofacial hard tissues through multimodal fusion encoding of pointclouds and multi-view images. By leveraging the complementary strengths of eachmodality and incorporating a score-based denoising module to refine surfacesmoothness, UniDCF overcomes the limitations of prior single-modalityapproaches. We curated the largest multimodal dataset, comprising intraoralscans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotatedinstances. Evaluations demonstrate that UniDCF outperforms existingstate-of-the-art methods in terms of geometric precision, structuralcompleteness, and spatial accuracy. Clinical simulations indicate UniDCFreduces reconstruction design time by 99% and achieves clinician-ratedacceptability exceeding 94%. Overall, UniDCF enables rapid, automated, andhigh-fidelity reconstruction, supporting personalized and precise restorativetreatments, streamlining clinical workflows, and enhancing patient outcomes.</description>
      <author>example@mail.com (Chunxia Ren, Ning Zhu, Yue Lai, Gui Chen, Ruijie Wang, Yangyi Hu, Suyao Liu, Shuwen Mao, Hong Su, Yu Zhang, Li Xiao)</author>
      <guid isPermaLink="false">2508.11728v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis</title>
      <link>http://arxiv.org/abs/2508.11721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统评估了眼科基础模型的性能，提出了FusionFM评估套件和两种融合方法，发现DINORET和RetiZero表现最佳，融合策略能带来适度改进，但全身性疾病预测仍有挑战&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学图像分析中展现出提高下游任务泛化能力的巨大潜力，眼科领域已出现多种基础模型，但关于最佳模型、任务适应性和模型组合等基本问题尚无明确答案&lt;h4&gt;目的&lt;/h4&gt;提出FusionFM评估套件和两种融合方法，系统评估单一和融合的眼科基础模型，回答基础模型性能和组合效果等关键问题&lt;h4&gt;方法&lt;/h4&gt;构建涵盖眼科疾病检测和全身性疾病预测的框架，使用多国标准化数据集对RETFound、VisionFM、RetiZero和DINORET四种模型进行基准测试，采用AUC和F1指标评估性能&lt;h4&gt;主要发现&lt;/h4&gt;DINORET和RetiZero在各类任务中表现最佳，RetiZero在外部数据集上泛化能力更强；基于门控的融合策略对青光眼、AMD和高血压预测有适度改进；全身性疾病预测尤其是外部队列中的高血压预测仍面临挑战&lt;h4&gt;结论&lt;/h4&gt;研究为眼科基础模型提供了基于证据的评估，证明了模型融合的益处，并指出了提高临床适用性的可能策略&lt;h4&gt;翻译&lt;/h4&gt;基础模型在医学图像分析中展现出通过提高多样化下游任务的泛化能力而展现巨大潜力。在眼科领域，最近出现了几种基础模型，但对于基本问题仍无明确答案：哪种基础模型表现最佳？它们在不同任务上是否同样出色？如果我们组合所有基础模型会怎样？据我们所知，这是第一个系统评估单一和融合眼科基础模型的研究。为解决这些问题，我们提出了FusionFM，这是一个全面的评估套件，以及两种融合方法来整合不同的眼科基础模型。我们的框架涵盖了基于视网膜成像的眼科疾病检测(青光眼、糖尿病视网膜病变和年龄相关性黄斑变性)和全身性疾病预测(糖尿病和高血压)。我们使用来自多个国家的标准化数据集对四种最先进的基础模型(RETFound、VisionFM、RetiZero和DINORET)进行了基准测试，并使用AUC和F1指标评估其性能。我们的结果表明，DINORET和RetiZero在眼科和全身性疾病任务上都表现出卓越的性能，其中RetiZero在外部数据集上显示出更强的泛化能力。关于融合策略，基于门控的方法在预测青光眼、AMD和高血压方面提供了适度的改进。尽管取得了这些进展，预测全身性疾病，特别是在外部队列中的高血压仍然具有挑战性。这些发现为眼科基础模型提供了基于证据的评估，突显了模型融合的益处，并指出了提高其临床适用性的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have shown great promise in medical image analysis byimproving generalization across diverse downstream tasks. In ophthalmology,several FMs have recently emerged, but there is still no clear answer tofundamental questions: Which FM performs the best? Are they equally good acrossdifferent tasks? What if we combine all FMs together? To our knowledge, this isthe first study to systematically evaluate both single and fused ophthalmicFMs. To address these questions, we propose FusionFM, a comprehensiveevaluation suite, along with two fusion approaches to integrate differentophthalmic FMs. Our framework covers both ophthalmic disease detection(glaucoma, diabetic retinopathy, and age-related macular degeneration) andsystemic disease prediction (diabetes and hypertension) based on retinalimaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,RetiZero, and DINORET) using standardized datasets from multiple countries andevaluated their performance using AUC and F1 metrics. Our results show thatDINORET and RetiZero achieve superior performance in both ophthalmic andsystemic disease tasks, with RetiZero exhibiting stronger generalization onexternal datasets. Regarding fusion strategies, the Gating-based approachprovides modest improvements in predicting glaucoma, AMD, and hypertension.Despite these advances, predicting systemic diseases, especially hypertensionin external cohort remains challenging. These findings provide anevidence-based evaluation of ophthalmic FMs, highlight the benefits of modelfusion, and point to strategies for enhancing their clinical applicability.</description>
      <author>example@mail.com (Ke Zou, Jocelyn Hui Lin Goh, Yukun Zhou, Tian Lin, Samantha Min Er Yew, Sahana Srinivasan, Meng Wang, Rui Santos, Gabor M. Somfai, Huazhu Fu, Haoyu Chen, Pearse A. Keane, Ching-Yu Cheng, Yih Chung Tham)</author>
      <guid isPermaLink="false">2508.11721v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning</title>
      <link>http://arxiv.org/abs/2508.12709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种掩码潜在预测自监督学习方法的改进，通过整合多选学习来处理音频内容中的不明确性，提高了表征质量和系统性能。&lt;h4&gt;背景&lt;/h4&gt;掩码潜在预测是自监督学习中的前沿范式，特别适用于音频和音乐表征学习。现有方法中，用于解决预训练任务的预测器模块的作用被忽视，无法有效处理多声源音频内容中的不明确性。&lt;h4&gt;目的&lt;/h4&gt;明确建模预测的不明确性并提高表征质量，改进现有的MATPAC系统，使其在音频表征学习中达到更优性能。&lt;h4&gt;方法&lt;/h4&gt;将多选学习（Multiple Choice Learning, MCL）整合到MATPAC系统中，创建MATPAC++方法。通过线性探测和微调方式评估，采用统一协议与最先进方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;MATPAC++在AudioSet上微调时达到最先进性能，在下游任务上整体表现最优。在仅使用音乐数据训练时，模型在提高效率的同时也达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;整合多选学习到掩码潜在预测框架中能有效处理音频内容的不明确性，显著提高表征质量和系统性能，在音频和音乐表征学习领域具有广泛应用前景。&lt;h4&gt;翻译&lt;/h4&gt;掩码潜在预测已成为自监督学习的前沿范式，特别适用于通用音频和音乐表征学习。尽管最近的方法展示了强大的性能，但在这些SSL系统中用于解决预训练任务的预测器模块的作用主要被忽视，尽管它对于解决当前预训练任务至关重要。特别是，该模块应该能够处理音频内容固有的不明确性，特别是当它由多个声源组成时。这项工作提出了一种新颖的增强：整合多选学习来明确建模预测不明确性并提高表征质量。我们在最近提出的MATPAC系统基础上，通过MCL改进其预测和无监督分类预训练任务。我们通过在多个下游任务上进行线性探测和在AudioSet上进行微调，广泛评估了我们的方法MATPAC++，采用统一协议实现与最先进SSL方法的严格公平比较。结果表明，我们的提议在AudioSet上微调时达到最先进性能，在下游任务上整体达到最先进的分数。此外，我们通过仅在音乐数据上训练来检查领域专业化，我们的模型在显著提高效率的同时实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked latent prediction has emerged as a leading paradigm in self-supervisedlearning (SSL), especially for general audio and music representation learning.While recent methods have demonstrated strong performance, the role of thepredictor module used at the output of such SSL systems remains mainlyoverlooked, despite being crucial for solving the pretext task at hand. Inparticular, this module should be able to deal with the ambiguity inherent inaudio content, especially when it is composed of multiple sound sources. Thiswork proposes a novel enhancement: integrating Multiple Choice Learning (MCL)to explicitly model prediction ambiguity and improve representation quality. Webuild on top of the recently proposed MATPAC system, improving its predictionand unsupervised classification pretext tasks with MCL. We extensively evaluateour method, MATPAC++, through both linear probing across multiple downstreamtasks and fine-tuning on AudioSet, employing a unified protocol that enablesrigorous and fair comparisons with state-of-the-art SSL approaches. Resultsshow that our proposal achieves state-of-the-art when fine-tuned on AudioSetand overall state-of-the-art scores on downstream tasks. Additionally, weexamine domain specialisation by training exclusively on music data, where ourmodel achieves state-of-the-art performance with significantly improvedefficiency.</description>
      <author>example@mail.com (Aurian Quelennec, Pierre Chouteau, Geoffroy Peeters, Slim Essid)</author>
      <guid isPermaLink="false">2508.12709v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Asymmetric Diffusion Recommendation Model</title>
      <link>http://arxiv.org/abs/2508.12706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了非对称扩散推荐模型(AsymDiffRec)，通过非对称方式处理推荐系统中的离散数据空间问题，有效保留了个性化信息并提升了推荐性能。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在推荐系统中被用于加强表征学习，但大多数模型在连续数据空间中使用对称的高斯噪声，而推荐样本实际存在于离散数据空间，高斯噪声可能破坏潜在表征中的个性化信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖有效的方法解决基于扩散的推荐模型在离散数据空间中的问题，保留个性化信息并提高推荐性能。&lt;h4&gt;方法&lt;/h4&gt;采用非对称方式学习前向和反向过程，定义广义前向过程模拟缺失特征，在非对称潜在特征空间执行反向过程，引入面向任务的优化策略保留个性化信息，将缺失特征样本视为有噪声输入生成去噪表征。&lt;h4&gt;主要发现&lt;/h4&gt;通过在线A/B测试，用户活跃天数和应用程序使用时长分别提升+0.131%和+0.166%，离线实验也显示改进效果，AsymDiffRec已在抖音音乐应用中实现。&lt;h4&gt;结论&lt;/h4&gt;AsymDiffRec有效解决了推荐系统中离散数据空间的问题，通过非对称扩散过程和面向任务的优化策略保留了个性化信息并提高了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，受扩散模型卓越成就的启发，扩散过程已被用于加强推荐系统中的表征学习。大多数基于扩散的推荐模型通常在连续数据空间中使用标准高斯噪声进行对称的前向和反向过程。然而，从推荐系统获取的样本存在于离散数据空间，这与连续数据空间有根本区别。此外，高斯噪声可能会破坏潜在表征中的个性化信息。在这项工作中，我们提出了一种新颖且有效的方法，名为非对称扩散推荐模型(AsymDiffRec)，以非对称方式学习前向和反向过程。我们定义了一个广义前向过程，模拟真实推荐样本中缺失的特征。然后在非对称的潜在特征空间中执行反向过程。为了保留潜在表征中的个性化信息，引入了面向任务的优化策略。在服务阶段，将具有缺失特征的原始样本视为有噪声输入，为最终预测生成去噪和鲁棒的表征。通过为基础模型配备AsymDiffRec，我们进行了在线A/B测试，在用户活跃天数和应用程序使用时长方面分别实现了+0.131%和+0.166%的改进。此外，扩展的离线实验也显示出改进效果。AsymDiffRec已在抖音音乐应用中实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760833&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, motivated by the outstanding achievements of diffusion models, thediffusion process has been employed to strengthen representation learning inrecommendation systems. Most diffusion-based recommendation models typicallyutilize standard Gaussian noise in symmetric forward and reverse processes incontinuous data space. Nevertheless, the samples derived from recommendationsystems inhabit a discrete data space, which is fundamentally different fromthe continuous one. Moreover, Gaussian noise has the potential to corruptpersonalized information within latent representations. In this work, wepropose a novel and effective method, named Asymmetric Diffusion RecommendationModel (AsymDiffRec), which learns forward and reverse processes in anasymmetric manner. We define a generalized forward process that simulates themissing features in real-world recommendation samples. The reverse process isthen performed in an asymmetric latent feature space. To preserve personalizedinformation within the latent representation, a task-oriented optimizationstrategy is introduced. In the serving stage, the raw sample with missingfeatures is regarded as a noisy input to generate a denoising and robustrepresentation for the final prediction. By equipping base models withAsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and+0.166% in terms of users' active days and app usage duration respectively.Additionally, the extended offline experiments also demonstrate improvements.AsymDiffRec has been implemented in the Douyin Music App.</description>
      <author>example@mail.com (Yongchun Zhu, Guanyu Jiang, Jingwu Chen, Feng Zhang, Xiao Yang, Zuotao Liu)</author>
      <guid isPermaLink="false">2508.12706v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Factorized Disentangled Representation Learning for Interpretable Radio Frequency Fingerprin</title>
      <link>http://arxiv.org/abs/2508.12660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的解缠表示学习框架，用于解决物联网设备射频指纹识别中各种变化因素相互纠缠的问题，通过学习多个因素的显式和独立表示，提高了识别的准确性和可控性。&lt;h4&gt;背景&lt;/h4&gt;随着物联网设备快速增长和安全风险上升，射频指纹成为设备识别和认证的关键。然而，信号传输到接收过程中各种变化因素会相互纠缠，降低射频指纹识别有效性。现有方法主要依赖领域自适应技术，缺乏明确因素表示，导致鲁棒性不足和控制有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种解缠表示学习框架，学习包括射频指纹在内的多个因素的显式和独立表示，提高射频指纹识别的鲁棒性和对下游任务的控制能力。&lt;h4&gt;方法&lt;/h4&gt;设计了解缠表示学习框架，引入基于显式性、模块化和紧凑性原则指导的解缠模块。包含因素分类模块和信号重构模块两个专用模块，每个模块都有专门损失函数促进有效解缠并增强对下游任务支持。框架能提取一组可解释的向量明确表示相应因素。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共基准数据集和一个自收集数据集上评估，方法在多个解缠表示学习指标上取得优异性能。框架所有模块提高分类准确性，并能对条件生成信号进行精确控制。在下游射频指纹识别任务和条件信号生成任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的解缠表示学习框架有效解决了射频指纹识别中因素纠缠问题，通过显式和独立表示多个因素，提高了识别准确性和可控性，为可解释和显式射频指纹提供了新可能。&lt;h4&gt;翻译&lt;/h4&gt;针对物联网设备的快速增长和日益增长的安全风险，射频指纹已成为设备识别和认证的关键。然而，从信号传输到接收过程中，除了射频指纹本身之外的各种变化因素可能会相互纠缠，降低了射频指纹识别的有效性。现有的射频指纹识别方法主要依赖于领域自适应技术，这些技术通常缺乏明确的因素表示，导致鲁棒性不足和对下游任务的控制有限。为了解决这个问题，我们提出了一种新颖的解缠表示学习框架，该框架学习包括射频指纹在内的多个因素的显式和独立表示。我们的框架引入了基于显式性、模块化和紧凑性原则指导的解缠模块。我们设计了两个专用模块用于因素分类和信号重构，每个模块都有专门的损失函数，以促进有效的解缠并增强对下游任务的支持。因此，该框架可以提取一组可解释的向量，这些向量明确表示相应的因素。我们在两个公共基准数据集和一个自收集数据集上评估了我们的方法。我们的方法在多个解缠表示学习指标上取得了令人印象深刻的性能。我们还分析了我们的方法在下游射频指纹识别任务和条件信号生成任务上的有效性。框架的所有模块都有助于提高分类准确性，并能够对条件生成的信号进行精确控制。这些结果突显了我们的解缠表示学习框架在可解释和显式射频指纹方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In response to the rapid growth of Internet of Things (IoT) devices andrising security risks, Radio Frequency Fingerprint (RFF) has become key fordevice identification and authentication. However, various changing factors -beyond the RFF itself - can be entangled from signal transmission to reception,reducing the effectiveness of RFF Identification (RFFI). Existing RFFI methodsmainly rely on domain adaptation techniques, which often lack explicit factorrepresentations, resulting in less robustness and limited controllability fordownstream tasks. To tackle this problem, we propose a novel DisentangledRepresentation Learning (DRL) framework that learns explicit and independentrepresentations of multiple factors, including the RFF. Our frameworkintroduces modules for disentanglement, guided by the principles ofexplicitness, modularity, and compactness. We design two dedicated modules forfactor classification and signal reconstruction, each with tailored lossfunctions that encourage effective disentanglement and enhance support fordownstream tasks. Thus, the framework can extract a set of interpretablevectors that explicitly represent corresponding factors. We evaluate ourapproach on two public benchmark datasets and a self-collected dataset. Ourmethod achieves impressive performance on multiple DRL metrics. We also analyzethe effectiveness of our method on downstream RFFI task and conditional signalgeneration task. All modules of the framework contribute to improvedclassification accuracy, and enable precise control over conditional generatedsignals. These results highlight the potential of our DRL framework forinterpretable and explicit RFFs.</description>
      <author>example@mail.com (Yezhuo Zhang, Zinan Zhou, Guangyu Li, Xuanpeng Li)</author>
      <guid isPermaLink="false">2508.12660v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion</title>
      <link>http://arxiv.org/abs/2508.12484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合CNN-Transformer和CKAN的混合模型用于皮肤癌分类，通过迁移学习和数据增强技术，在多个基准数据集上实现了高准确率和F1分数，证明了该方法的有效性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;皮肤癌分类在医学图像分析中是至关重要的任务，准确区分良性和恶性病变对早期诊断和治疗至关重要。&lt;h4&gt;目的&lt;/h4&gt;探索结合卷积神经网络(CNN)和Transformer的顺序和并行混合模型，并使用卷积Kolmogorov-Arnold网络(CKAN)，以提高皮肤癌分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;结合迁移学习和大规模数据增强，利用CNN提取局部空间特征，Transformer建模全局依赖关系，CKAN促进非线性特征融合，并在多个基准数据集(HAM10000、BCN20000和PAD-UFES)上评估模型，考虑不同的数据分布和类别不平衡。&lt;h4&gt;主要发现&lt;/h4&gt;混合CNN-Transformer架构能有效捕获空间和上下文特征；集成CKAN通过可学习的激活函数增强特征融合；在HAM10000数据集上达到92.81%的准确率和92.47%的F1分数；在PAD-UFES数据集上达到97.83%的准确率和97.83%的F1分数；在BCN20000数据集上达到91.17%的准确率和91.79%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;特征表示和模型设计在推进稳健和准确的医学图像分类方面具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;皮肤癌分类是医学图像分析中的一个关键任务，其中准确区分良性和恶性病变对早期诊断和治疗至关重要。在本研究中，我们探索了结合卷积Kolmogorov-Arnold网络(CKAN)的顺序和并行混合CNN-Transformer模型。我们的方法结合了迁移学习和大规模数据增强，其中CNN提取局部空间特征，Transformer建模全局依赖关系，CKAN促进非线性特征融合以改进表示学习。为了评估泛化能力，我们在多个基准数据集(HAM10000、BCN20000和PAD-UFES)上评估了我们的模型，考虑不同的数据分布和类别不平衡。实验结果表明，混合CNN-Transformer架构能有效捕获空间和上下文特征，从而提高分类性能。此外，集成CKAN通过可学习的激活函数增强特征融合，产生更具判别力的表示。我们提出的方法在皮肤癌分类中取得了具有竞争力的性能，在HAM10000数据集上达到92.81%的准确率和92.47%的F1分数，在PAD-UFES数据集上达到97.83%的准确率和97.83%的F1分数，在BCN20000数据集上达到91.17%的准确率和91.79%的F1分数，突显了我们的模型在不同数据集上的有效性和泛化能力。这项研究强调了特征表示和模型设计在推进稳健和准确的医学图像分类方面的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skin cancer classification is a crucial task in medical image analysis, whereprecise differentiation between malignant and non-malignant lesions isessential for early diagnosis and treatment. In this study, we exploreSequential and Parallel Hybrid CNN-Transformer models with ConvolutionalKolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning andextensive data augmentation, where CNNs extract local spatial features,Transformers model global dependencies, and CKAN facilitates nonlinear featurefusion for improved representation learning. To assess generalization, weevaluate our models on multiple benchmark datasets (HAM10000,BCN20000 andPAD-UFES) under varying data distributions and class imbalances. Experimentalresults demonstrate that hybrid CNN-Transformer architectures effectivelycapture both spatial and contextual features, leading to improvedclassification performance. Additionally, the integration of CKAN enhancesfeature fusion through learnable activation functions, yielding morediscriminative representations. Our proposed approach achieves competitiveperformance in skin cancer classification, demonstrating 92.81% accuracy and92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score onthe PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000dataset highlighting the effectiveness and generalizability of our model acrossdiverse datasets. This study highlights the significance of featurerepresentation and model design in advancing robust and accurate medical imageclassification.</description>
      <author>example@mail.com (Shubhi Agarwal, Amulya Kumar Mahto)</author>
      <guid isPermaLink="false">2508.12484v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Uncovering Emergent Physics Representations Learned In-Context by Large Language Models</title>
      <link>http://arxiv.org/abs/2508.12448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了大型语言模型的上下文学习能力，特别是在物理推理方面的能力。研究通过物理系统中的动力学预测任务，使用稀疏自编码器分析模型残差流激活，揭示了LLMs在上下文学习中能够编码有意义的物理概念。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型展现出令人印象深刻的上下文学习能力，能够仅通过文本提示解决各种任务。然而，目前仍不清楚LLMs中允许成功执行上下文学习的精确机制或内部结构。物理任务为探索这一挑战提供了有希望的测试平台，因为物理系统提供基于基本原理的结构化动力学的实验可控、真实世界数据。&lt;h4&gt;目的&lt;/h4&gt;通过物理系统中的动力学预测任务，探究LLMs的上下文学习能力，特别是它们推理物理的能力，以确定LLMs是否能够在上下文中学习物理。&lt;h4&gt;方法&lt;/h4&gt;使用动力学预测任务作为代理来评估LLMs的物理推理能力。首先展示动力学预测性能随输入上下文长度提高。使用稀疏自编码器(SAEs)分析模型的残差流激活，以揭示这种能力如何在LLMs中出现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SAEs捕捉到的特征与关键物理变量(如能量)相关，表明有意义的物理概念被编码在LLMs的上下文学习中。此外，研究发现动力学预测的上下文性能随着输入上下文长度的增加而提高。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一个新颖的案例研究，扩展了我们对LLMs如何进行上下文学习的理解。研究表明，LLMs能够在上下文学习中编码有意义的物理概念，为理解LLMs的内部工作机制提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型展现出令人印象深刻的上下文学习能力，使他们能够仅通过文本提示解决广泛范围的任务。随着这些能力的进步，适用领域的范围也在显著扩大。然而，识别LLMs中允许成功执行跨不同、 distinct任务类别的上下文学习的精确机制或内部结构仍然难以捉摸。基于物理的任务为探索这一挑战提供了有希望的测试平台。与基本算术或符号方程等合成序列不同，物理系统提供基于基本原理的结构化动力学的实验可控、真实世界数据。这使它们特别适合在现实 yet 可管理的环境中研究LLMs的涌现推理行为。在这里，我们从机制上研究LLMs的上下文学习能力，特别关注它们推理物理的能力。使用物理系统中的动力学预测任务作为代理，我们评估LLMs是否能够在上下文中学习物理。我们首先展示了上下文中的动力学预测性能随输入上下文长度的增加而提高。为了揭示这种能力如何在LLMs中出现，我们使用稀疏自编码器(SAEs)分析模型的残差流激活。我们的实验揭示SAEs捕捉到的特征与关键物理变量(如能量)相关。这些发现表明有意义的物理概念被编码在LLMs的上下文学习中。总之，我们的工作提供了一个新颖的案例研究，扩展了我们对LLMs如何进行上下文学习的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) exhibit impressive in-context learning (ICL)abilities, enabling them to solve wide range of tasks via textual promptsalone. As these capabilities advance, the range of applicable domains continuesto expand significantly. However, identifying the precise mechanisms orinternal structures within LLMs that allow successful ICL across diverse,distinct classes of tasks remains elusive. Physics-based tasks offer apromising testbed for probing this challenge. Unlike synthetic sequences suchas basic arithmetic or symbolic equations, physical systems provideexperimentally controllable, real-world data based on structured dynamicsgrounded in fundamental principles. This makes them particularly suitable forstudying the emergent reasoning behaviors of LLMs in a realistic yet tractablesetting. Here, we mechanistically investigate the ICL ability of LLMs,especially focusing on their ability to reason about physics. Using a dynamicsforecasting task in physical systems as a proxy, we evaluate whether LLMs canlearn physics in context. We first show that the performance of dynamicsforecasting in context improves with longer input contexts. To uncover how suchcapability emerges in LLMs, we analyze the model's residual stream activationsusing sparse autoencoders (SAEs). Our experiments reveal that the featurescaptured by SAEs correlate with key physical variables, such as energy. Thesefindings demonstrate that meaningful physical concepts are encoded within LLMsduring in-context learning. In sum, our work provides a novel case study thatbroadens our understanding of how LLMs learn in context.</description>
      <author>example@mail.com (Yeongwoo Song, Jaeyong Bae, Dong-Kyum Kim, Hawoong Jeong)</author>
      <guid isPermaLink="false">2508.12448v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems</title>
      <link>http://arxiv.org/abs/2508.12375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种层次化知识引导的故障强度诊断框架(HKG)，通过图卷积网络捕捉类别间的依赖关系，并开发重新加权的层次化知识相关性矩阵(Re-HKCM)方案，在多个真实工业数据集上验证了其优越性。&lt;h4&gt;背景&lt;/h4&gt;故障强度诊断(FID)在复杂工业系统中机械设备的监控和维护中起关键作用，但现有方法基于思维链且未考虑目标类别间的依赖关系。&lt;h4&gt;目的&lt;/h4&gt;捕捉和探索类别之间的依赖关系，提高故障强度诊断的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出受思维树启发的层次化知识引导的故障强度诊断框架(HKG)，使用图卷积网络映射层次拓扑图，开发重新加权的层次化知识相关性矩阵(Re-HKCM)方案，将类间层次化知识嵌入统计相关性矩阵。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实工业数据集(包括SAMSON AG的三个气蚀数据集和一个公开数据集)上的实验显示，所提方法结果优越，优于最近的先进FID方法。&lt;h4&gt;结论&lt;/h4&gt;HKG框架和Re-HKCM方案能有效捕捉类别间依赖关系，提高故障强度诊断性能，避免了过平滑问题。&lt;h4&gt;翻译&lt;/h4&gt;故障强度诊断(FID)在复杂工业系统中机械设备的监控和维护中起着关键作用。由于当前FID方法基于思维链而未考虑目标类别间的依赖关系。为了捕捉和探索这些依赖关系，我们提出了一种受思维树启发的层次化知识引导的故障强度诊断框架(HKG)，该框架适用于任何表示学习方法。HKG使用图卷积网络将类别表示的层次拓扑图映射为一组相互依赖的全局层次分类器，其中每个节点由类别的词嵌入表示。这些全局层次分类器应用于通过表示学习提取的深度特征，使整个模型能够端到端学习。此外，我们通过将类间层次化知识嵌入数据驱动的统计相关性矩阵(SCM)中，开发了一种重新加权的层次化知识相关性矩阵(Re-HKCM)方案，有效指导了图卷积神经网络中节点的信息共享并避免了过平滑问题。Re-HKCM通过一系列数学变换从SCM推导得出。我们在四个来自不同工业领域的真实世界数据集(包括SAMSON AG的三个气蚀数据集和一个现有的公开数据集)上进行了大量FID实验，所有结果都显示出优越性并优于最近的先进FID方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3637528.3671610&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fault intensity diagnosis (FID) plays a pivotal role in monitoring andmaintaining mechanical devices within complex industrial systems. As currentFID methods are based on chain of thought without considering dependenciesamong target classes. To capture and explore dependencies, we propose ahierarchical knowledge guided fault intensity diagnosis framework (HKG)inspired by the tree of thought, which is amenable to any representationlearning methods. The HKG uses graph convolutional networks to map thehierarchical topological graph of class representations into a set ofinterdependent global hierarchical classifiers, where each node is denoted byword embeddings of a class. These global hierarchical classifiers are appliedto learned deep features extracted by representation learning, allowing theentire model to be end-to-end learnable. In addition, we develop a re-weightedhierarchical knowledge correlation matrix (Re-HKCM) scheme by embeddinginter-class hierarchical knowledge into a data-driven statistical correlationmatrix (SCM) which effectively guides the information sharing of nodes ingraphical convolutional neural networks and avoids over-smoothing issues. TheRe-HKCM is derived from the SCM through a series of mathematicaltransformations. Extensive experiments are performed on four real-worlddatasets from different industrial domains (three cavitation datasets fromSAMSON AG and one existing publicly) for FID, all showing superior results andoutperform recent state-of-the-art FID methods.</description>
      <author>example@mail.com (Yu Sha, Shuiping Gou, Bo Liu, Johannes Faber, Ningtao Liu, Stefan Schramm, Horst Stoecker, Thomas Steckenreiter, Domagoj Vnucec, Nadine Wetzstein, Andreas Widl, Kai Zhou)</author>
      <guid isPermaLink="false">2508.12375v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</title>
      <link>http://arxiv.org/abs/2508.11999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为MOON的生成式多模态大语言模型(MLLM)，用于改进产品表示学习，解决了现有方法在多图像与文本多对一对齐问题上的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着电子商务快速发展，探索通用表示而非特定任务表示引起研究关注。现有判别性双流架构难以建模多个产品图像和文本之间的多对一对齐关系。&lt;h4&gt;目的&lt;/h4&gt;利用生成式多模态大语言模型改进产品表示学习，解决三个关键挑战：典型LLMs缺乏多模态和方面感知建模模块；产品图像中存在背景噪声；缺乏评估标准基准。&lt;h4&gt;方法&lt;/h4&gt;1) 采用引导的专家混合(MoE)模块针对多模态和特定方面产品内容建模；2) 检测产品图像核心语义区域减轻背景噪声干扰；3) 引入专门负采样策略增加负样本难度和多样性；4) 发布大规模多模态基准MBE用于产品理解任务。&lt;h4&gt;主要发现&lt;/h4&gt;MOON模型在自建基准和公共数据集上展现具有竞争力的零样本性能，在跨模态检索、产品分类和属性预测等多种下游任务上表现良好，案例研究和可视化证实了其有效性。&lt;h4&gt;结论&lt;/h4&gt;MOON作为首个基于生成式MLLM的产品表示学习模型，通过解决多模态建模、背景噪声干扰和评估基准缺失等问题，显著提升了产品表示学习效果，并在多种任务上展现出优异性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;随着电子商务的快速发展，探索通用表示而非特定任务的表示已引起越来越多的研究关注。对于产品理解，尽管现有的判别性双流架构推动了该领域的进步，但它们本质上难以建模多个产品图像和文本之间的多对一对齐关系。因此，我们认为生成式多模态大语言模型(MLLM)在改进产品表示学习方面具有巨大潜力。然而，由于几个关键挑战，实现这一目标仍然非同寻常：典型LLMs中缺乏多模态和方面感知建模模块；产品图像中普遍存在背景噪声；以及缺乏用于评估的标准基准。为了解决这些问题，我们提出了首个基于生成式MLLM的产品表示学习模型MOON。我们的方法(1)采用引导的专家混合(MoE)模块针对多模态和特定方面的产品内容进行建模；(2)有效检测产品图像中的核心语义区域，减轻背景噪声造成的干扰；(3)引入专门的负采样策略，增加负样本的难度和多样性。此外，我们发布了一个大规模多模态基准MBE，用于各种产品理解任务。实验表明，我们的模型在我们提出的基准和公共数据集上都展示了具有竞争力的零样本性能，展示了在包括跨模态检索、产品分类和属性预测在内的各种下游任务上的强大泛化能力。此外，案例研究和可视化说明了MOON在产品理解方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of e-commerce, exploring general representationsrather than task-specific ones has attracted increasing research attention. Forproduct understanding, although existing discriminative dual-flow architecturesdrive progress in this field, they inherently struggle to model the many-to-onealignment between multiple images and texts of products. Therefore, we arguethat generative Multimodal Large Language Models (MLLMs) hold significantpotential for improving product representation learning. Nevertheless,achieving this goal still remains non-trivial due to several key challenges:the lack of multimodal and aspect-aware modeling modules in typical LLMs; thecommon presence of background noise in product images; and the absence of astandard benchmark for evaluation. To address these issues, we propose thefirst generative MLLM-based model named MOON for product representationlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module fortargeted modeling of multimodal and aspect-specific product content; (2)effectively detects core semantic regions in product images to mitigate thedistraction and interference caused by background noise; and (3) introduces thespecialized negative sampling strategy to increase the difficulty and diversityof negative samples. In addition, we release a large-scale multimodal benchmarkMBE for various product understanding tasks. Experimentally, our modeldemonstrates competitive zero-shot performance on both our benchmark and thepublic dataset, showcasing strong generalization across various downstreamtasks, including cross-modal retrieval, product classification, and attributeprediction. Furthermore, the case study and visualization illustrate theeffectiveness of MOON for product understanding.</description>
      <author>example@mail.com (Daoze Zhang, Zhanheng Nie, Jianyu Liu, Chenghan Fu, Wanxian Guan, Yuan Gao, Jun Song, Pengjie Wang, Jian Xu, Bo Zheng)</author>
      <guid isPermaLink="false">2508.11999v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations</title>
      <link>http://arxiv.org/abs/2508.11978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的双曲推荐模型，利用几何洞察提高表示学习和计算稳定性，通过重新定义双曲距离和学习更丰富的用户和项目表示，改进了传统欧几里得空间的局限性，并使用三元组损失函数更好地捕捉用户-项目交互。&lt;h4&gt;背景&lt;/h4&gt;最近的研究已经证明了双曲几何在捕捉推荐系统中交互数据的复杂模式方面的潜力，推荐系统需要处理复杂的用户-项目交互数据。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的双曲推荐模型，利用几何洞察同时提高表示学习和计算稳定性，重新定义双曲距离概念以释放更多表示能力，学习更具表现力的用户和项目表示。&lt;h4&gt;方法&lt;/h4&gt;重新定义双曲距离概念以释放比传统欧几里得空间更多的表示能力，构建一个三元组损失函数，通过几何数据驱动的成对交互项组合来建模用户及其相应偏好和非偏好选择之间的三元关系。&lt;h4&gt;主要发现&lt;/h4&gt;双曲方法不仅优于现有的欧几里得和双曲模型，还减少了流行度偏差，导致更多样化和个性化的推荐。&lt;h4&gt;结论&lt;/h4&gt;双曲几何为推荐系统中的表示学习提供了一种有效方法，通过改进的几何表示可以实现更好的推荐性能和多样性。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经证明了双曲几何在捕捉推荐系统中交互数据的复杂模式方面的潜力。在这项工作中，我们引入了一种新颖的双曲推荐模型，利用几何洞察来同时提高表示学习和计算稳定性。我们重新定义了双曲距离的概念，以释放比传统欧几里得空间更多的表示能力，并学习更具表现力的用户和项目表示。为了更好地捕捉用户-项目交互，我们构建了一个三元组损失函数，通过几何数据驱动的成对交互项组合来建模用户及其相应偏好和非偏好选择之间的三元关系。我们的双曲方法不仅优于现有的欧几里得和双曲模型，还减少了流行度偏差，从而带来更多样化和个性化的推荐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated the potential of hyperbolic geometry forcapturing complex patterns from interaction data in recommender systems. Inthis work, we introduce a novel hyperbolic recommendation model that usesgeometrical insights to improve representation learning and increasecomputational stability at the same time. We reformulate the notion ofhyperbolic distances to unlock additional representation capacity overconventional Euclidean space and learn more expressive user and itemrepresentations. To better capture user-items interactions, we construct atriplet loss that models ternary relations between users and theircorresponding preferred and nonpreferred choices through a mix of pairwiseinteraction terms driven by the geometry of data. Our hyperbolic approach notonly outperforms existing Euclidean and hyperbolic models but also reducespopularity bias, leading to more diverse and personalized recommendations.</description>
      <author>example@mail.com (Viacheslav Yusupov, Maxim Rakhuba, Evgeny Frolov)</author>
      <guid isPermaLink="false">2508.11978v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2508.13073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page:  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是关于基于大型视觉语言模型的视觉-语言-动作模型在机器人操作中的系统性综述，提供了首个分类导向的回顾。&lt;h4&gt;背景&lt;/h4&gt;机器人操作是机器人和具身AI的关键前沿，需要精确的电机控制和多模态理解。传统基于规则的方法在非结构化、新颖环境中无法扩展或泛化。近年来，基于大型视觉语言模型的视觉-语言-动作模型已成为变革性范式。&lt;h4&gt;目的&lt;/h4&gt;提供对用于机器人操作的大型VLM-based VLA模型的第一个系统性、分类导向的回顾，解决现有分类不一致性，减轻研究碎片化，填补大型VLMs与机器人操作交叉领域的研究空白。&lt;h4&gt;方法&lt;/h4&gt;明确定义大型VLM-based VLA模型，划分两种主要架构范式：单体模型（包括单系统和双系统设计）和分层模型（通过可解释中间表示解耦规划与执行）。深入探讨模型与先进领域的整合、独特特征合成以及未来发展方向。&lt;h4&gt;主要发现&lt;/h4&gt;大型VLM-based VLA模型主要分为单体模型和分层模型两大架构。这些模型与强化学习、无需训练的优化等领域相结合，展现出独特的架构特征和操作优势。未来发展方向包括记忆机制、4D感知、高效适应和多智能体合作等。&lt;h4&gt;结论&lt;/h4&gt;综述整合了最新进展，通过系统整合大型VLMs与机器人操作交叉领域研究，解决了分类不一致性和研究碎片化问题，填补了关键空白，并提供了定期更新的项目页面记录持续进展。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作是机器人和具身AI的关键前沿，需要精确的电机控制和多模态理解，然而传统的基于规则的方法在非结构化、新颖环境中无法扩展或泛化。近年来，基于在大量图像-文本数据上预训练的大型视觉语言模型的视觉-语言-动作模型已成为一种变革性范式。本综述提供了首个针对用于机器人操作的大型VLM-based VLA模型的系统性、分类导向的回顾。我们首先明确定义大型VLM-based VLA模型，并划分两种主要架构范式：(1) 单体模型，包括具有不同集成程度的单系统和双系统设计；(2) 分层模型，通过可解释的中间表示明确解耦规划与执行。基于此，我们深入探讨了大型VLM-based VLA模型：(1) 与先进领域的整合，包括强化学习、无需训练的优化、从人类视频中学习以及世界模型整合；(2) 独特特征的合成，整合架构特征、操作优势以及支持其发展的数据集和基准；(3) 有前途的方向的识别，包括记忆机制、4D感知、高效适应、多智能体合作和其他新兴能力。本综述整合了最新进展，通过系统整合大型VLMs与机器人操作交叉领域的研究，解决了现有分类中的不一致性，减轻了研究碎片化，并填补了关键空白。我们提供了一个定期更新的项目页面来记录持续进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulation, a key frontier in robotics and embodied AI, requiresprecise motor control and multimodal understanding, yet traditional rule-basedmethods fail to scale or generalize in unstructured, novel environments. Inrecent years, Vision-Language-Action (VLA) models, built upon LargeVision-Language Models (VLMs) pretrained on vast image-text datasets, haveemerged as a transformative paradigm. This survey provides the firstsystematic, taxonomy-oriented review of large VLM-based VLA models for roboticmanipulation. We begin by clearly defining large VLM-based VLA models anddelineating two principal architectural paradigms: (1) monolithic models,encompassing single-system and dual-system designs with differing levels ofintegration; and (2) hierarchical models, which explicitly decouple planningfrom execution via interpretable intermediate representations. Building on thisfoundation, we present an in-depth examination of large VLM-based VLA models:(1) integration with advanced domains, including reinforcement learning,training-free optimization, learning from human videos, and world modelintegration; (2) synthesis of distinctive characteristics, consolidatingarchitectural traits, operational strengths, and the datasets and benchmarksthat support their development; (3) identification of promising directions,including memory mechanisms, 4D perception, efficient adaptation, multi-agentcooperation, and other emerging capabilities. This survey consolidates recentadvances to resolve inconsistencies in existing taxonomies, mitigate researchfragmentation, and fill a critical gap through the systematic integration ofstudies at the intersection of large VLMs and robotic manipulation. We providea regularly updated project page to document ongoing progress:https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.</description>
      <author>example@mail.com (Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie)</author>
      <guid isPermaLink="false">2508.13073v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>On the N-elliptic localized solutions to the derivative nonlinear Schrödinger equation and their asymptotic analysis</title>
      <link>http://arxiv.org/abs/2508.12882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了DNLS方程的椭圆函数解，通过四个独立参数进行参数化，并利用Darboux-Bäcklund变换生成了两种等价的N-椭圆局域解形式。这些解表示为行列式比值的形式，分析表明解之间的碰撞是弹性的，并建立了严格弹性碰撞的充分条件。&lt;h4&gt;背景&lt;/h4&gt;研究DNLS方程的椭圆函数解及其渐进行为&lt;h4&gt;目的&lt;/h4&gt;生成和分析DNLS方程的N-椭圆局域解，验证其碰撞弹性行为&lt;h4&gt;方法&lt;/h4&gt;使用四个独立参数参数化椭圆函数解，通过Darboux-Bäcklund变换生成解的形式，分析t→±∞时的渐进行为&lt;h4&gt;主要发现&lt;/h4&gt;1) 解表示为行列式比值；2) 解之间的碰撞是弹性的；3) 解在每个传播方向上趋于简单椭圆局域解；4) 在传播方向之间解渐近接近移动背景；5) 解表现出广义孤子分解猜想预测的行为&lt;h4&gt;结论&lt;/h4&gt;N-椭圆局域解在DNLS方程中表现出弹性碰撞行为，满足广义孤子分解猜想&lt;h4&gt;翻译&lt;/h4&gt;我们用四个独立参数对DNLS方程的椭圆函数解进行参数化，并通过Darboux-Bäcklund变换生成DNLS方程的两种等价的N-椭圆局域解形式。N-椭圆局域解表示为(导数形式的)行列式比值，其中行列式的条目用Weierstrass sigma函数表示。此外，分析了两种N-椭圆局域解在传播方向上和传播方向之间的渐进行为(t→±∞)，验证了椭圆解之间的碰撞是弹性的。我们证明了解在每个传播方向上趋于简单的椭圆局域解。在传播方向之间，解渐近接近一个移动的背景。此外，我们建立了严格弹性碰撞的充分条件。系统地研究了解的动态行为，并通过图形可视化展示了分析结果。这些解的渐近分析确认了它们表现出在椭圆函数背景下的广义孤子分解猜想预测的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We parameterize the elliptic function solutions to the derivative nonlinearSchr\"odinger (DNLS) equation with four independent parameters and generate twoequivalent forms of N-elliptic localized solutions to the DNLS equation throughthe Darboux-B\"acklund transformation. The N-elliptic localized solutions areexpressed as (the derivative of) the ratios of determinants with entries interms of Weierstrass sigma functions. Moreover, the asymptotic behaviors ofboth forms of N-elliptic localized solutions are analyzed along and between thepropagation directions as $t \rightarrow \pm\infty$, which verify that thecollisions between elliptic-solutions are elastic. We prove that the solutiontends to a simple elliptic localized solution along each propagation direction.Between the propagation directions, the solution asymptotically approaches ashifted background. Furthermore, we establish sufficient conditions forstrictly elastic collisions. The dynamic behaviors of the solutions aresystematically investigated, with analytical results visualized throughgraphical illustrations. The asymptotic analysis of these solutions confirmsthat they exhibit the behavior predicted by the generalized soliton resolutionconjecture on the elliptic function background.</description>
      <author>example@mail.com (Liming Ling, Wang Tang)</author>
      <guid isPermaLink="false">2508.12882v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics</title>
      <link>http://arxiv.org/abs/2508.12456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 40 figures. Framework combining Liquid Time-Constant Neural  Networks with autonomous marine robotics for oil spill trajectory prediction  and response coordination&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个结合多智能体群体机器人系统和液态时间常数神经网络的集成框架，用于海洋溢油的实时预测、动态跟踪和快速响应。&lt;h4&gt;背景&lt;/h4&gt;海洋溢油对环境和经济构成严重风险，威胁海洋生态系统、海岸线和相关产业。由于风、洋流和温度等物理、化学和环境因素的相互作用，预测和管理溢油轨迹极为复杂，这使得及时有效的应对具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发准确实时的溢油轨迹预测和协调减轻措施系统，以减少溢油灾难的环境和经济影响。&lt;h4&gt;方法&lt;/h4&gt;引入一个综合框架，结合基于MOOS-IvP平台的多智能体群体机器人系统和液态时间常数神经网络，将自适应机器学习与自主海洋机器人相结合。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的系统能够实现溢油运动的高精度实时预测；使用深水地平线溢油数据验证，LTC-RK4模型达到0.96的空间精度，比LSTM方法提高23%；群体智能使机器人智能体之间能够进行去中心化、可扩展和有弹性的决策，增强了集体监测和封堵能力。&lt;h4&gt;结论&lt;/h4&gt;先进的神经建模与自主协调机器人的结合在预测精度、灵活性和操作可扩展性方面显示出显著改进，为可持续、自主的溢油管理和环境保护推进了最先进的技术。&lt;h4&gt;翻译&lt;/h4&gt;海洋溢油对环境和经济构成严重风险，威胁海洋生态系统、海岸线和相关产业。由于风、洋流和温度等物理、化学和环境因素的相互作用，预测和管理溢油轨迹极为复杂，这使得及时有效的应对具有挑战性。准确实时的轨迹预测和协调减轻措施对于减少这些灾难的影响至关重要。本研究引入了一个综合框架，结合了基于MOOS-IvP平台的多智能体群体机器人系统和液态时间常数神经网络。所提出的系统将自适应机器学习与自主海洋机器人相结合，能够实时预测、动态跟踪和快速应对不断变化的溢油。通过利用LTCNs（非常适合建模复杂的时间相关过程），该框架实现了溢油运动的高精度实时预测。群体智能使机器人智能体之间能够进行去中心化、可扩展和有弹性的决策，增强了集体监测和封堵工作。我们的方法使用深水地平线溢油数据进行了验证，其中LTC-RK4模型达到了0.96的空间精度，比LSTM方法提高了23%。先进的神经建模与自主协调机器人的集成在预测精度、灵活性和操作可扩展性方面显示出显著改进。最终，这项研究通过增强轨迹预测和响应协调，为可持续、自主的溢油管理和环境保护推进了最先进的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine oil spills pose grave environmental and economic risks, threateningmarine ecosystems, coastlines, and dependent industries. Predicting andmanaging oil spill trajectories is highly complex, due to the interplay ofphysical, chemical, and environmental factors such as wind, currents, andtemperature, which makes timely and effective response challenging. Accuratereal-time trajectory forecasting and coordinated mitigation are vital forminimizing the impact of these disasters. This study introduces an integratedframework combining a multi-agent swarm robotics system built on the MOOS-IvPplatform with Liquid Time-Constant Neural Networks (LTCNs). The proposed systemfuses adaptive machine learning with autonomous marine robotics, enablingreal-time prediction, dynamic tracking, and rapid response to evolving oilspills. By leveraging LTCNs--well-suited for modeling complex, time-dependentprocesses--the framework achieves real-time, high-accuracy forecasts of spillmovement. Swarm intelligence enables decentralized, scalable, and resilientdecision-making among robot agents, enhancing collective monitoring andcontainment efforts. Our approach was validated using data from the DeepwaterHorizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,surpassing LSTM approaches by 23%. The integration of advanced neural modelingwith autonomous, coordinated robotics demonstrates substantial improvements inprediction precision, flexibility, and operational scalability. Ultimately,this research advances the state-of-the-art for sustainable, autonomous oilspill management and environmental protection by enhancing both trajectoryprediction and response coordination.</description>
      <author>example@mail.com (Hadas C. Kuzmenko, David Ehevich, Oren Gal)</author>
      <guid isPermaLink="false">2508.12456v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Has GPT-5 Achieved Spatial Intelligence? An Empirical Study</title>
      <link>http://arxiv.org/abs/2508.13142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态模型在空间理解和推理方面仍存在明显局限，即使是GPT-5这样的先进模型也未能达到人类水平。作者通过全面评估发现，专有模型在面对最困难问题时并未表现出决定性优势。&lt;h4&gt;背景&lt;/h4&gt;多模态模型在近年来取得了显著进展，但在空间理解和推理方面仍存在明显局限，这些能力是实现通用人工智能的基础。随着GPT-5的发布，现在是评估领先模型在空间智能道路上所处位置的好时机。&lt;h4&gt;目的&lt;/h4&gt;提出一个全面的空间任务分类法，统一现有基准并讨论确保公平评估的挑战；评估最先进的专有和开源模型在八个关键基准上的表现。&lt;h4&gt;方法&lt;/h4&gt;对最先进的专有和开源模型在八个关键基准上进行评估，总成本超过十亿个标记；同时进行定性评估，考察对人类直观但先进多模态模型仍然失败的多样化场景。&lt;h4&gt;主要发现&lt;/h4&gt;1) GPT-5在空间智能方面展现出前所未有的强度；2) 但在广泛任务范围内仍不及人类表现；3) 多模态模型面临更具挑战性的空间智能问题；4) 专有模型在面对最困难问题时并未表现出决定性优势；5) 多样化场景的定性评估显示先进模型仍有明显不足。&lt;h4&gt;结论&lt;/h4&gt;尽管多模态模型取得了显著进展，但在空间理解和推理方面仍存在明显局限，即使是GPT-5这样的先进模型也未能达到人类水平，表明在实现真正的空间智能方面仍有很长的路要走。&lt;h4&gt;翻译&lt;/h4&gt;多模态模型近年来取得了显著进展。然而，它们在空间理解和推理方面仍然表现出明显的局限性，这些是实现通用人工智能的基本能力。随着最近GPT-5的发布，据称是迄今为止最强大的AI模型，现在是审视领先模型在通向空间智能道路上的所处位置的好时机。首先，我们提出了一个统一现有基准的空间任务综合分类法，并讨论了确保公平评估的挑战。随后，我们在八个关键基准上评估了最先进的专有和开源模型，总成本超过十亿个标记。我们的实证研究表明：(1) GPT-5在空间智能方面展现出前所未有的强度，但(2)在广泛任务范围内仍不及人类表现。此外，我们(3)确定了多模态模型面临更具挑战性的空间智能问题，并且(4)专有模型在面对最困难问题时并未表现出决定性优势。此外，我们在对人类直观但在最先进的多模态模型上仍然失败的多样化场景进行了定性评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想评估GPT-5是否已经实现了空间智能。这个问题很重要，因为空间理解和推理是实现通用人工智能的基本能力，没有它，AI无法完全理解和与物理世界互动。尽管多模态模型取得了很大进步，但它们在空间任务上仍然表现不佳，而这些任务对人类来说很简单。了解GPT-5在这方面的能力，对于评估AI的进步和未来发展方向至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先提出了一个全面的空间任务分类法，统一了现有基准测试，并讨论了确保公平评估的挑战。然后他们在八个关键基准上评估了最先进的模型，总成本超过10亿个标记。作者标准化了提示、评估策略和指标，确保公平比较。他们使用了三种答案匹配方法和圆形评估策略来减少偏差。作者确实借鉴了现有工作，包括采用SITE的CAA指标、OmniSpatial的zero-shot CoT方法、VLMEvalKit和LMMS-Eval的答案匹配方法，以及CoreCognition和MMBench的圆形评估策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过统一框架评估GPT-5和其他模型在空间智能方面的能力，识别它们的挑战和局限性，并比较专有模型与开源模型在面对最困难问题时的表现。整体流程是：1)提出空间智能的六种基本能力分类；2)选择八个最新的空间智能基准测试；3)标准化评估协议；4)在这些基准上评估模型；5)进行消融研究评估不同参数的影响；6)进行案例研究分析优势和局限性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出全面的空间任务分类法；在八个最新基准上进行大规模评估(超10亿标记)；发现GPT-5在空间智能方面表现出前所未有的能力但仍未达到人类水平；识别更具挑战性的空间智能问题；发现专有模型在面对最困难问题时没有决定性优势。相比之前工作，这篇论文评估范围更广、规模更大、方法更标准化、提供了更全面的分析，并发现了新的见解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过全面评估揭示了GPT-5在空间智能方面的显著进步，同时明确指出其与人类水平仍存在明显差距，并为未来空间智能研究提供了统一的框架和基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal models have achieved remarkable progress in recent years.Nevertheless, they continue to exhibit notable limitations in spatialunderstanding and reasoning, which are fundamental capabilities to achievingartificial general intelligence. With the recent release of GPT-5, allegedlythe most powerful AI model to date, it is timely to examine where the leadingmodels stand on the path toward spatial intelligence. First, we propose acomprehensive taxonomy of spatial tasks that unifies existing benchmarks anddiscuss the challenges in ensuring fair evaluation. We then evaluatestate-of-the-art proprietary and open-source models on eight key benchmarks, ata cost exceeding one billion total tokens. Our empirical study reveals that (1)GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)still falls short of human performance across a broad spectrum of tasks.Moreover, we (3) identify the more challenging spatial intelligence problemsfor multi-modal models, and (4) proprietary models do not exhibit a decisiveadvantage when facing the most difficult problems. In addition, we conduct aqualitative evaluation across a diverse set of scenarios that are intuitive forhumans yet fail even the most advanced multi-modal models.</description>
      <author>example@mail.com (Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang)</author>
      <guid isPermaLink="false">2508.13142v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks</title>
      <link>http://arxiv.org/abs/2508.12741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个全面的基准框架，用于系统评估神经网络中的空间推理能力，特别关注形态特性如连接性和距离关系。初步实验结果表明神经网络在空间推理方面存在重大挑战。&lt;h4&gt;背景&lt;/h4&gt;当前缺乏系统评估神经网络空间推理能力的综合框架，特别是对形态特性如连接性和距离关系的评估。&lt;h4&gt;目的&lt;/h4&gt;定义一个综合基准框架，用于系统评估神经网络中的空间推理能力，特别关注形态特性。&lt;h4&gt;方法&lt;/h4&gt;使用VoxLogicA空间模型检查器生成两类合成数据集：迷宫连接问题和空间距离计算任务。通过多种分辨率评估，并采用完整的机器学习工作流程，包括数据集生成、交叉验证训练、推理执行，以及使用Dice系数和IoU指标的综合评估。&lt;h4&gt;主要发现&lt;/h4&gt;神经网络在空间推理能力方面存在重大挑战，在基本的几何和拓扑理解任务中显示出系统性失败。&lt;h4&gt;结论&lt;/h4&gt;该框架提供了可重现的实验协议，使研究人员能够确定特定限制。通过结合神经网络与符号推理方法，可改善临床应用中的空间理解，为持续研究神经网络空间推理限制和潜在解决方案奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了在定义一个全面的基准框架方面的初步结果，该框架旨在系统评估神经网络中的空间推理能力，特别关注连接性和距离关系等形态特性。该框架目前正被用于研究nnU-Net的能力，利用空间模型检查器VoxLogicA生成两类不同的合成数据集：用于拓扑分析的迷宫连接问题和用于几何理解的空间距离计算任务。每个类别都在多种分辨率下进行评估，以评估可扩展性和泛化特性。自动化流程包含完整的机器学习工作流程，包括：合成数据集生成、具有交叉验证的标准化训练、推理执行，以及使用Dice系数和IoU（交并比）指标的综合评估。初步实验结果表明神经网络在空间推理能力方面存在重大挑战，揭示了在基本几何和拓扑理解任务中的系统性失败。该框架提供了可重现的实验协议，使研究人员能够确定特定限制。这些限制可以通过结合神经网络与符号推理方法的方法来解决，以改善临床应用中的空间理解，为持续研究神经网络空间推理限制和潜在解决方案奠定基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决神经网络空间推理能力缺乏系统性评估基准框架的问题。这个问题在医疗图像分析等领域尤为重要，因为空间理解直接影响诊断准确性和治疗效果。在现实应用中，神经网络在理解基本几何和拓扑关系方面存在系统性失败，可能导致医疗环境中的临床相关错误。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有评估方法的局限性（如传统指标无法充分评估空间结构理解）来设计方法。他们借鉴了空间模型检查领域的工作，特别是VoxLogicA工具，该工具使用逻辑公式识别图像中满足特定拓扑特性的像素。作者还参考了先进神经网络架构（如nnU-Net）和拓扑感知损失函数等方法，但创新性地将这些技术整合到一个系统化的多分辨率评估框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合形式空间逻辑规范与受控合成数据集生成，实现数学精确的真实值定义，并通过多分辨率方法系统评估神经网络的空间推理能力。整体流程包括：1)使用VoxLogicA生成两类合成数据集（点距离和迷宫连通性）；2)在多个分辨率下训练神经网络；3)执行推理并使用Dice系数和IoU等指标评估；4)分析不同分辨率下的性能差异，确定可靠空间推理的最小分辨率要求。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多分辨率基准测试框架；2)形式空间逻辑与神经网络评估的结合；3)包含完整机器学习工作流程的自动化管道；4)两种互补的评估域（连接性分析和距离推理）；5)开源实现。相比之前工作，新框架使用形式规范生成真实值而非手动注释，系统评估不同尺度下的空间能力，并专注于传统指标无法检测的拓扑和几何理解局限性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个创新的多分辨率基准测试框架，通过结合形式空间逻辑与自动化机器学习工作流程，系统评估了神经网络的空间推理能力，揭示了传统评估方法无法检测的拓扑和几何理解局限性，为开发更可靠的混合AI系统奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents preliminary results in the definition of a comprehensivebenchmark framework designed to systematically evaluate spatial reasoningcapabilities in neural networks, with a particular focus on morphologicalproperties such as connectivity and distance relationships. The framework iscurrently being used to study the capabilities of nnU-Net, exploiting thespatial model checker VoxLogicA to generate two distinct categories ofsynthetic datasets: maze connectivity problems for topological analysis andspatial distance computation tasks for geometric understanding. Each categoryis evaluated across multiple resolutions to assess scalability andgeneralization properties. The automated pipeline encompasses a completemachine learning workflow including: synthetic dataset generation, standardizedtraining with cross-validation, inference execution, and comprehensiveevaluation using Dice coefficient and IoU (Intersection over Union) metrics.Preliminary experimental results demonstrate significant challenges in neuralnetwork spatial reasoning capabilities, revealing systematic failures in basicgeometric and topological understanding tasks. The framework provides areproducible experimental protocol, enabling researchers to identify specificlimitations. Such limitations could be addressed through hybrid approachescombining neural networks with symbolic reasoning methods for improved spatialunderstanding in clinical applications, establishing a foundation for ongoingresearch into neural network spatial reasoning limitations and potentialsolutions.</description>
      <author>example@mail.com (Manuela Imbriani, Gina Belmonte, Mieke Massink, Alessandro Tofani, Vincenzo Ciancia)</author>
      <guid isPermaLink="false">2508.12741v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.12404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LMAD是一种新型视觉语言框架，专为自动驾驶设计，通过整合全面场景理解和专门任务结构，显著提升了现有VLMs在驾驶推理任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型在场景理解和驾驶行为可解释性方面展现出有前景的能力，但现有方法在车载多视图图像和场景推理文本上微调VLMs时，往往缺乏自动驾驶所需的整体和细致的场景识别以及强大的空间感知能力，特别是在复杂情况下。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLM方法在自动驾驶场景中的不足，提出一种专门为自动驾驶设计的视觉语言框架，提高模型在复杂驾驶场景中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出名为LMAD的新型视觉语言框架，模仿现代端到端驾驶范式，整合全面场景理解和VLMs的任务专门结构，引入初步场景交互和专门专家适配器在同一驾驶任务结构中，使VLMs更好地与自动驾驶场景对齐，同时保持与现有VLMs和面向规划的驾驶系统的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有VLMs在驾驶推理任务上的性能，为可解释自动驾驶树立了新标准。&lt;h4&gt;结论&lt;/h4&gt;LMAD框架有效解决了VLMs在自动驾驶应用中的不足，通过整合全面场景理解和专门的任务结构，提高了模型在复杂驾驶场景中的表现，为可解释自动驾驶提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型（VLMs）在场景理解、增强驾驶行为可解释性和与用户交互方面展现出有前景的能力。现有方法主要在车载多视图图像和场景推理文本上微调VLMs，但这种方法通常缺乏自动驾驶所需的整体和细致的场景识别以及强大的空间感知能力，特别是在复杂情况下。为解决这一差距，我们提出了一种专为自动驾驶设计的新型视觉语言框架，称为LMAD。我们的框架通过整合全面场景理解和带有VLMs的任务专门结构，模仿现代端到端驾驶范式。特别是，我们在同一驾驶任务结构中引入了初步场景交互和专门专家适配器，使VLMs更好地与自动驾驶场景对齐。此外，我们的设计完全兼容现有VLMs，并能与面向规划的驾驶系统无缝集成。在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有VLMs在驾驶推理任务上的性能，为可解释自动驾驶树立了新标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (VLMs) have shown promising capabilities inscene understanding, enhancing the explainability of driving behaviors andinteractivity with users. Existing methods primarily fine-tune VLMs on on-boardmulti-view images and scene reasoning text, but this approach often lacks theholistic and nuanced scene recognition and powerful spatial awareness requiredfor autonomous driving, especially in complex situations. To address this gap,we propose a novel vision-language framework tailored for autonomous driving,called LMAD. Our framework emulates modern end-to-end driving paradigms byincorporating comprehensive scene understanding and a task-specializedstructure with VLMs. In particular, we introduce preliminary scene interactionand specialized expert adapters within the same driving task structure, whichbetter align VLMs with autonomous driving scenarios. Furthermore, our approachis designed to be fully compatible with existing VLMs while seamlesslyintegrating with planning-oriented driving systems. Extensive experiments onthe DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly booststhe performance of existing VLMs on driving reasoning tasks,setting a newstandard in explainable autonomous driving.</description>
      <author>example@mail.com (Nan Song, Bozhou Zhang, Xiatian Zhu, Jiankang Deng, Li Zhang)</author>
      <guid isPermaLink="false">2508.12404v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Splat Feature Solver</title>
      <link>http://arxiv.org/abs/2508.12216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  webpage not that stable&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的特征提升方法，用于解决3D场景理解中多视图图像不一致性问题，实现了高质量的特征提升和开放词汇3D分割的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;特征提升已成为3D场景理解的关键组成部分，允许将丰富的图像特征描述符附加到基于splat的3D表示上，但面临如何将丰富通用属性最优分配给3D原语并解决多视图图像不一致性的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一且与核和特征无关的特征提升公式，高效解决特征提升问题，处理多视图观察中的不一致性和噪声，实现高质量的特征提升。&lt;h4&gt;方法&lt;/h4&gt;将特征提升问题表述为稀疏线性逆问题，可通过闭式高效解决；引入两种互补正则化策略：Tikhonov指导通过软对角占优强制数值稳定性，提升后聚合通过特征聚类过滤噪声输入。&lt;h4&gt;主要发现&lt;/h4&gt;在开放词汇3D分割基准上实现了最先进的性能，优于基于训练、基于分组和启发式前向基线，且能在几分钟内生成提升的特征。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了3D场景理解中的特征提升挑战，代码已公开在GitHub上，并提供在线演示。&lt;h4&gt;翻译&lt;/h4&gt;特征提升已成为3D场景理解的关键组成部分，使能够将丰富的图像特征描述符（如DINO、CLIP）附加到基于splat的3D表示上。核心挑战在于将丰富的通用属性最优分配给3D原语的同时，解决来自多视图图像的不一致性问题。我们提出了一个统一的、与核和特征无关的特征提升问题公式化，将其作为稀疏线性逆问题，可以闭式高效解决。我们的方法在凸损失下为提供高质量提升特征的全局最优误差提供了可证明的上界。为解决多视图观察中的不一致性和噪声，我们引入了两种互补的正则化策略来稳定解决方案并增强语义保真度。Tikhonov指导通过软对角占优强制数值稳定性，而提升后聚合通过特征聚类过滤噪声输入。大量实验证明，我们的方法在开放词汇3D分割基准上实现了最先进的性能，优于基于训练、基于分组和启发式前向基线，同时能在几分钟内生成提升的特征。代码可在GitHub上获取，我们也提供了一个在线演示。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将2D图像特征描述符（如CLIP、DINO等）有效提升到3D表示中的问题，特别是在基于splat的3D表示（如3D高斯溅射）中。这个问题在3D场景理解中非常重要，因为它能够将丰富的语义信息与3D几何结合，支持3D语义分割、物体查询等下游任务，从而推动虚拟现实、自动驾驶和机器人等领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有特征提升方法的三种主要类型：基于训练的优化方法、基于分组的方法和基于启发式前向传播的方法。作者发现这些方法缺乏统一的数学框架、没有理论保证、针对特定特征类型设计，且未明确处理数据噪声。基于这些观察，作者将特征提升问题建模为稀疏线性逆问题，借鉴了Tikhonov正则化的思想来提高数值稳定性，并参考了基于聚类的方法来过滤噪声输入，从而设计出这个新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将特征提升问题形式化为稀疏线性逆问题AX=B，其中每个基本元素的描述符通过系统解来恢复。整体实现流程包括：1)输入预计算的splat表示、传感器参数和密集特征观测；2)构建特征提升方程，使用行和预处理器作为初始解；3)应用Tikhonov指导增强数值稳定性；4)使用后提升聚合过滤噪声输入；5)输出提升的特征参数，支持上游任务和下游应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将特征提升形式化为统一的稀疏线性逆问题；2)提供全局最优误差的可证明上界；3)引入Tikhonov指导和后提升聚合两种互补正则化策略；4)支持多种基本核和不同密集特征的通用实现；5)在开放词汇3D语义分割上实现最先进性能。相比之前工作，本文提供了理论框架和保证，同时处理噪声和不一致性问题，方法更加通用且计算效率高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种将特征提升问题形式化为稀疏线性逆问题的统一框架，通过理论保证和正则化策略解决了多视角噪声和不一致性问题，实现了高质量、高效率的3D特征提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature lifting has emerged as a crucial component in 3D scene understanding,enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)onto splat-based 3D representations. The core challenge lies in optimallyassigning rich general attributes to 3D primitives while addressing theinconsistency issues from multi-view images. We present a unified, kernel- andfeature-agnostic formulation of the feature lifting problem as a sparse linearinverse problem, which can be solved efficiently in closed form. Our approachadmits a provable upper bound on the global optimal error under convex lossesfor delivering high quality lifted features. To address inconsistencies andnoise in multi-view observations, we introduce two complementary regularizationstrategies to stabilize the solution and enhance semantic fidelity. TikhonovGuidance enforces numerical stability through soft diagonal dominance, whilePost-Lifting Aggregation filters noisy inputs via feature clustering. Extensiveexperiments demonstrate that our approach achieves state-of-the-art performanceon open-vocabulary 3D segmentation benchmarks, outperforming training-based,grouping-based, and heuristic-forward baselines while producing the liftedfeatures in minutes. Code is available at\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. Wealso have a \href{https://splat-distiller.pages.dev/}</description>
      <author>example@mail.com (Butian Xiong, Rong Liu, Kenneth Xu, Meida Chen, Andrew Feng)</author>
      <guid isPermaLink="false">2508.12216v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Euclidean Approach to Green-Wave Theory Applied to Traffic Signal Networks</title>
      <link>http://arxiv.org/abs/2508.12146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  74 pages, 49 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种应用于信号主干道网络的绿波理论，通过几何推理和专用设备实现无间断交通流，并通过仿真验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;长主干道上的信号交叉口如果没有适当协调，交通效率会低下。随着信号数量的增加，协调变得更加困难，传统的通行方案往往会失效。&lt;h4&gt;目的&lt;/h4&gt;介绍一种绿波理论，可应用于相交主干道网络，实现任意长信号主干道上的无间断通行。&lt;h4&gt;方法&lt;/h4&gt;使用'道路到旅行者反馈设备'，基于欧几里得几何模型，定义了RGW-道路、绿箭头、实节点、虚节点、绿波速度、区块等概念，并使用几何推理推导结果，通过名为RGW-SIM的仿真模型验证效果。&lt;h4&gt;主要发现&lt;/h4&gt;绿箭头长度有最大值且被限制为离散长度；绿箭头运动定律表明可以选择现有主干道转换为RGW-道路；产生的信号时间和偏移已被证明是有效的。&lt;h4&gt;结论&lt;/h4&gt;该方法可以实现长主干道上的无间断交通流，长绿波可以节省时间和燃料，减少污染和交通事故。&lt;h4&gt;翻译&lt;/h4&gt;在信号交叉口协调不当的情况下，长主干道上的交通可能效率低下。随着信号数量的增加，协调变得更加困难，传统的通行方案往往会失效。长绿波通过提供更顺畅的交通流，可以节省旅行时间和燃料，减少污染和交通事故。本文介绍了一种绿波理论，可应用于相交主干道网络。它使用'道路到旅行者反馈设备'，实现在任意长的信号主干道上的无间断通行。该方法借鉴了欧几里得几何模型。我们定义了诸如RGW-道路（车辆以推荐速度行驶时可通过所有交通信号的道路）、绿箭头（代表车辆车队）、实节点（代表RGW-道路相交的信号交叉口）和虚节点、绿波速度、区块等概念——相当于欧几里得公理的类比。然后我们使用几何推理推导出结果：绿箭头长度有最大值，被限制为离散长度，绿箭头运动定律表明可以选择现有主干道转换为RGW-道路。使用先前开发的名为RGW-SIM的仿真模型，产生的信号时间和偏移已被证明是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Travel on long arterials with signalized intersections can be inefficient ifnot coordinated properly. As the number of signals increases, coordinationbecomes more challenging and traditional progression schemes tend to breakdown. Long progressions save travel time and fuel, reduce pollution and trafficaccidents by providing a smoother flow of traffic. This paper introduces agreen-wave theory that can be applied to a network of intersecting arterialroads. It enables uninterrupted flow on arbitrary long signalized arterialsusing a Road-to-Traveler-Feedback Device. The approach is modelled afterEuclid. We define concepts such as RGW-roads (roads where vehicles traveling atthe recommended speed make all traffic signals), green-arrows (representingvehicle platoons), real nodes (representing signalized intersections whereRGW-roads intersect) and virtual nodes, green-wave speed, blocks, etc. - theanalogue of Euclid's postulates. We then use geometric reasoning to deduceresults: green-arrow lengths have a maximum value, are restricted to discretelengths, and green-arrow laws of motion imply that select existing arterialroads can be converted to RGW-roads. The signal timings and offsets that areproduced have been shown to be effective using a simulation model developedpreviously called RGW-SIM.</description>
      <author>example@mail.com (Melvin H. Friedman, Brian L. Mark, Nathan H. Gartner)</author>
      <guid isPermaLink="false">2508.12146v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes</title>
      <link>http://arxiv.org/abs/2508.12015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了InstDrive，一个实例感知的3D高斯飞溅框架，用于动态驾驶场景的交互式重建。该方法使用SAM生成的掩码作为伪真实标签指导2D特征学习，并通过3D级别的正则化隐式编码实例身份，同时使用轻量级静态码本连接连续特征和离散身份。&lt;h4&gt;背景&lt;/h4&gt;从车载摄像头视频重建动态驾驶场景在自动驾驶和场景理解中具有重要意义。现有方法将所有背景元素统一为单一表示，阻碍了实例级理解和灵活的场景编辑。一些将2D分割提升到3D空间的方法通常依赖于预处理的实例ID或复杂流程，且主要适用于室内场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现动态驾驶场景交互式重建的实例感知3D高斯飞溅框架，解决现有方法在实例级理解和场景编辑方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出InstDrive框架，使用SAM生成的掩码作为伪真实标签通过对比损失和伪监督目标指导2D特征学习；在3D级别引入正则化隐式编码实例身份，通过基于体素的损失强制一致性；使用轻量级静态码本连接连续特征和离散身份，无需数据预处理或复杂优化。&lt;h4&gt;主要发现&lt;/h4&gt;定量和定性实验证明了InstDrive的有效性，据作者所知，这是第一个实现动态开放世界驾驶场景中3D实例分割的框架。&lt;h4&gt;结论&lt;/h4&gt;InstDrive成功实现了动态驾驶场景的交互式重建，解决了现有方法在实例级理解和场景编辑方面的局限性，为自动驾驶和场景理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从车载摄像头视频重建动态驾驶场景由于其对于自动驾驶和场景理解的重要性而吸引了越来越多的关注。尽管最近的进展取得了令人印象深刻的成果，但大多数方法仍然将所有背景元素统一为单一表示，这阻碍了实例级理解和灵活的场景编辑。一些尝试将2D分割提升到3D空间的方法通常依赖于预处理的实例ID或复杂流程来映射连续特征到离散身份。此外，这些方法通常为具有丰富视点的室内场景设计，使其不太适用于户外驾驶场景。在本文中，我们提出了InstDrive，一个专为动态驾驶场景交互式重建而设计的实例感知3D高斯飞溅框架。我们使用SAM生成的掩码作为伪真实标签，通过对比损失和伪监督目标指导2D特征学习。在3D级别，我们引入正则化来隐式编码实例身份，并通过基于体素的损失强制一致性。一个轻量级静态码本进一步连接连续特征和离散身份，无需数据预处理或复杂优化。定量和定性实验证明了InstDrive的有效性，据我们所知，它是第一个实现动态开放世界驾驶场景中3D实例分割的框架。更多可视化内容可在我们的项目页面查看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从车载摄像头视频中重建动态驾驶场景并实现实例级别3D分割的问题。这个问题在自动驾驶领域非常重要，因为它能支持场景理解、行为预测和交互式规划，同时为对象级编辑、可控模拟和细粒度语义理解提供基础，这些都是自动驾驶技术发展的关键需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性设计了这个方法：现有方法要么将场景统一表示，要么依赖复杂的预处理或跟踪技术。作者借鉴了SAM生成的掩码作为伪监督信号，采用对比学习促进特征聚类，并基于3D高斯溅射作为基础表示。关键创新在于提出两阶段训练策略：先学习连续特征，再通过静态二值化码本实现离散实例编码，解决了现有方法中依赖跟踪、聚类精度低或码本设计复杂的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用实例感知的3D高斯溅射框架，通过SAM生成的掩码指导学习，实现动态驾驶场景的交互式重建。整体流程分为两个阶段：首先是连续特征学习阶段，使用对比学习拉近同一掩码特征、推远不同掩码特征，并通过体素一致性损失确保3D空间连贯性；其次是量化实例特征学习阶段，采用静态二值化码本将连续特征映射为离散实例ID，并通过多数投票机制增强特征一致性。最终实现实时的3D实例分割和交互式编辑能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个端到端的3D实例分割框架，适用于开放驾驶场景；2)静态二值化码本设计，确保均匀分布的聚类中心且无需更新；3)基于多数投票的伪监督机制，增强特征一致性；4)基于体素的一致性损失，提高3D空间连贯性。相比之前工作，InstDrive无需实例跟踪或复杂预处理，提供全局一致的实例ID，支持实时交互编辑，并且在多视角驾驶场景中表现更好，而不仅仅是受控环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InstDrive首次实现了从车载摄像头视频中端到端地重建具有实例级编辑能力的动态驾驶场景，通过创新的静态码本设计和两阶段训练策略，实现了无需人工标注的实时3D实例分割和交互式编辑。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing dynamic driving scenes from dashcam videos has attractedincreasing attention due to its significance in autonomous driving and sceneunderstanding. While recent advances have made impressive progress, mostmethods still unify all background elements into a single representation,hindering both instance-level understanding and flexible scene editing. Someapproaches attempt to lift 2D segmentation into 3D space, but often rely onpre-processed instance IDs or complex pipelines to map continuous features todiscrete identities. Moreover, these methods are typically designed for indoorscenes with rich viewpoints, making them less applicable to outdoor drivingscenarios. In this paper, we present InstDrive, an instance-aware 3D GaussianSplatting framework tailored for the interactive reconstruction of dynamicdriving scene. We use masks generated by SAM as pseudo ground-truth to guide 2Dfeature learning via contrastive loss and pseudo-supervised objectives. At the3D level, we introduce regularization to implicitly encode instance identitiesand enforce consistency through a voxel-based loss. A lightweight staticcodebook further bridges continuous features and discrete identities withoutrequiring data pre-processing or complex optimization. Quantitative andqualitative experiments demonstrate the effectiveness of InstDrive, and to thebest of our knowledge, it is the first framework to achieve 3D instancesegmentation in dynamic, open-world driving scenes.More visualizations areavailable at our project page.</description>
      <author>example@mail.com (Hongyuan Liu, Haochen Yu, Jianfei Jiang, Qiankun Liu, Jiansheng Chen, Huimin Ma)</author>
      <guid isPermaLink="false">2508.12015v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding</title>
      <link>http://arxiv.org/abs/2508.11952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了UniUGG，首个用于3D模态的统一理解和生成框架。该框架利用大语言模型理解和解码句子及3D表示，核心是采用空间解码器和潜在扩散模型生成高质量3D表示，支持基于参考图像和任意视图变换的3D场景生成，同时保持对空间视觉问答任务的支持。此外，还提出了几何-语义学习策略预训练视觉编码器，共同捕获输入的语义和几何线索，增强空间理解和生成能力。&lt;h4&gt;背景&lt;/h4&gt;尽管最近的统一架构在图像理解和生成方面取得了显著进展，但3D任务的集成仍然具有挑战性且很大程度上未被探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的3D模态理解和生成框架，解决3D任务集成的挑战性问题。&lt;h4&gt;方法&lt;/h4&gt;提出UniUGG框架，使用大语言模型理解和解码句子及3D表示，设计空间解码器利用潜在扩散模型生成高质量3D表示，并采用几何-语义学习策略预训练视觉编码器，共同捕获输入的语义和几何线索。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明该方法在视觉表示、空间理解和3D生成方面具有优越性；框架能够基于参考图像和任意视图变换生成和想象3D场景；同时支持空间视觉问答(VQA)任务。&lt;h4&gt;结论&lt;/h4&gt;UniUGG框架成功解决了3D任务集成的挑战，在多个方面展示了优越性能，源代码将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近的统一架构在图像理解和生成方面取得了令人印象深刻的进展，但3D任务的集成仍然具有挑战性且很大程度上未被探索。在本文中，我们介绍了UniUGG，这是首个用于3D模态的统一理解和生成框架。我们的统一框架采用大语言模型来理解和解码句子及3D表示。其核心是，我们提出了一种空间解码器，利用潜在扩散模型生成高质量的3D表示。这使得能够基于参考图像和任意视图变换生成和想象3D场景，同时仍然支持空间视觉问答(VQA)任务。此外，我们提出了一种几何-语义学习策略来预训练视觉编码器。该设计共同捕获输入的语义和几何线索，增强空间理解和生成能力。大量的实验结果证明了我们的方法在视觉表示、空间理解和3D生成方面的优越性。源代码将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D模态的统一理解和生成问题。尽管2D图像领域已有统一架构，但3D任务仍面临挑战。这个问题很重要，因为3D场景理解对增强现实、机器人导航等应用至关重要，而统一框架能同时处理空间推理和场景生成，提高效率，且能更好地模拟人类的空间认知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了两个瓶颈：视觉表示局限性和3D生成与LLM不兼容性。受DUSt3R多视图几何训练启发，设计了几何-语义学习策略预训练视觉编码器，结合RADIOv2.5作为教师模型和MASt3R解码器。针对3D生成问题，设计了Spatial-VAE压缩表示，利用LLM和扩散模型生成3D场景。该方法借鉴了多项现有工作，包括MASt3R的空间解码器、RADIOv2.5的语义引导和扩散模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何-语义统一编码、空间解码器、Spatial-VAE设计和基于LLM的统一框架，实现3D场景的理解和生成。整体流程分三阶段：1)视觉编码器预训练：在多数据集上预训练ViT编码器，结合语义和几何学习；2)Spatial-VAE训练：压缩视觉表示，与空间解码器联合微调；3)统一学习：空间生成学习使用LLM和扩散模型，空间理解学习在VQA任务上微调LLM。推理时，3D生成通过参考图像和视图变换生成目标视图，空间理解通过图像和问答实现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个统一的3D理解和生成框架；2)几何-语义视觉编码器预训练策略；3)专为3D设计的Spatial-VAE；4)统一的学习范式。相比之前工作，不同在于：专注于3D模态而非2D；同时支持理解和生成而非仅关注其中之一；仅依赖视觉输入而非额外模态；统一了几何和语义信息而非单独处理；通过Spatial-VAE解决了3D数据不规则性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniUGG首次实现了3D场景的统一理解和生成，通过创新的几何-语义编码策略和Spatial-VAE设计，使模型能够同时处理空间级视觉问答和几何一致的3D场景生成，在多个基准测试上超越了现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the impressive progress on understanding and generating images shownby the recent unified architectures, the integration of 3D tasks remainschallenging and largely unexplored. In this paper, we introduce UniUGG, thefirst unified understanding and generation framework for 3D modalities. Ourunified framework employs an LLM to comprehend and decode sentences and 3Drepresentations. At its core, we propose a spatial decoder leveraging a latentdiffusion model to generate high-quality 3D representations. This allows forthe generation and imagination of 3D scenes based on a reference image and anarbitrary view transformation, while remaining supports for spatial visualquestion answering (VQA) tasks. Additionally, we propose a geometric-semanticlearning strategy to pretrain the vision encoder. This design jointly capturesthe input's semantic and geometric cues, enhancing both spatial understandingand generation. Extensive experimental results demonstrate the superiority ofour method in visual representation, spatial understanding, and 3D generation.The source code will be released upon paper acceptance.</description>
      <author>example@mail.com (Yueming Xu, Jiahui Zhang, Ze Huang, Yurui Chen, Yanpeng Zhou, Zhenyu Chen, Yu-Jie Yuan, Pengxiang Xia, Guowei Huang, Xinyue Cai, Zhongang Qi, Xingyue Quan, Jianye Hao, Hang Xu, Li Zhang)</author>
      <guid isPermaLink="false">2508.11952v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction</title>
      <link>http://arxiv.org/abs/2508.12917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Paper is Accepted by TCSVT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CMF-IOU的多阶段跨模态融合3D检测框架，通过深度完成网络统一LiDAR和摄像头信息表示，设计双向跨视图增强3D主干网络，并引入迭代体素-点感知细粒度池化模块和IoU联合预测分支，有效解决了3D空间信息和2D语义信息对齐的挑战，在多个数据集上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;多模态方法基于摄像头和LiDAR传感器在3D检测领域受到广泛关注，但许多现有工作专注于单阶段或部分阶段融合，导致特征提取不足和性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种多阶段跨模态融合3D检测框架CMF-IOU，有效解决3D空间信息和2D语义信息对齐的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过深度完成网络将像素信息投影到3D空间获得伪点统一表示；设计双向跨视图增强3D主干网络，包含稀疏到远距离(S2D)分支和残差视图一致性(ResVC)分支；引入迭代体素-点感知细粒度池化模块；设计交并比(IoU)联合预测分支结合新颖提案生成技术。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI、nuScenes和Waymo数据集上的大量实验表明，该方法具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;CMF-IOU框架通过多阶段融合和精细化的处理方法，有效解决了3D空间和2D语义信息对齐的挑战，提高了3D检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于摄像头和LiDAR传感器的多模态方法在3D检测领域已引起广泛关注。然而，许多流行的工作专注于单阶段或部分阶段融合，导致特征提取不足和性能次优。在本文中，我们引入了一种多阶段跨模态融合3D检测框架，称为CMF-IOU，以有效解决对齐3D空间和2D语义信息的挑战。具体而言，我们首先通过深度完成网络将像素信息投影到3D空间以获得伪点，这统一了LiDAR和摄像头信息的表示。然后，设计了一个双向跨视图增强3D主干网络来编码LiDAR点和伪点。第一个稀疏到远距离(S2D)分支利用编码器-解码器结构来增强稀疏LiDAR点的表示。第二个残差视图一致性(ResVC)分支通过3D和2D卷积过程提出，以减轻不准确伪点的影响。随后，我们引入了一个迭代体素-点感知细粒度池化模块，在提案细化阶段捕获来自LiDAR点的空间信息和来自伪点的纹理信息。为了在迭代过程中实现更精确的细化，设计了一个交并比(IoU)联合预测分支，结合新颖的提案生成技术，以保留具有高IoU和分类分数的边界框。大量实验表明，我们的方法在KITTI、nuScenes和Waymo数据集上具有优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态3D目标检测中相机和LiDAR传感器信息融合不充分的问题，以及现有方法在NMS后处理中使用分类分数而非IoU作为评估指标的不一致性问题。这个问题在自动驾驶领域至关重要，因为准确可靠的3D目标检测是自动驾驶系统的核心功能，而融合不同传感器的互补信息可以提高检测的准确性和鲁棒性，同时使用与评估指标一致的IoU分数进行后处理可以进一步提升检测性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多模态3D检测方法的局限性，发现单阶段融合导致特征提取不充分，且NMS使用分类分数而非IoU与评估指标不一致。因此，作者设计了一个多阶段跨模态融合框架，分别处理LiDAR原始点和相机生成的伪点，以减轻深度偏差影响。该方法借鉴了深度完成网络(PENet)、基于体素的处理方法(VoxelRCNN)、点集抽象(PointNet)以及IoU预测(3D IoU-Net)等现有工作，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在整个检测管道中实现多阶段跨模态融合，同时引入IoU联合预测来平衡分类分数和IoU分数。整体流程包括：1)使用深度完成网络将相机图像投影到3D空间生成伪点；2)设计双边跨视图增强骨干网络，分别处理原始点和伪点；3)采用迭代体素-点感知细粒度池化模块优化提案；4)引入IoU联合预测分支和提案生成技术，保留高IoU和高分类分数的边界框；5)使用综合损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多阶段跨模态融合框架，在整个管道中实现多阶段融合；2)双边跨视图增强骨干网络，分别处理原始点和伪点；3)迭代体素-点感知细粒度池化模块，结合空间和纹理信息；4)IoU联合预测分支，结合IoU和分类分数进行后处理。相比之前的工作，不同之处在于：不再局限于单阶段融合，而是全程多模态融合；分别处理原始点和伪点而非简单合并；引入IoU预测分支和均匀提案生成策略，解决了IoU分布不均和评估指标不一致的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CMF-IoU通过多阶段跨模态融合和IoU联合预测，有效解决了3D目标检测中多模态信息融合不充分和后处理评估指标不一致的问题，显著提高了自动驾驶场景下的3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal methods based on camera and LiDAR sensors have garneredsignificant attention in the field of 3D detection. However, many prevalentworks focus on single or partial stage fusion, leading to insufficient featureextraction and suboptimal performance. In this paper, we introduce amulti-stage cross-modal fusion 3D detection framework, termed CMF-IOU, toeffectively address the challenge of aligning 3D spatial and 2D semanticinformation. Specifically, we first project the pixel information into 3D spacevia a depth completion network to get the pseudo points, which unifies therepresentation of the LiDAR and camera information. Then, a bilateralcross-view enhancement 3D backbone is designed to encode LiDAR points andpseudo points. The first sparse-to-distant (S2D) branch utilizes anencoder-decoder structure to reinforce the representation of sparse LiDARpoints. The second residual view consistency (ResVC) branch is proposed tomitigate the influence of inaccurate pseudo points via both the 3D and 2Dconvolution processes. Subsequently, we introduce an iterative voxel-pointaware fine grained pooling module, which captures the spatial information fromLiDAR points and textural information from pseudo points in the proposalrefinement stage. To achieve more precise refinement during iteration, anintersection over union (IoU) joint prediction branch integrated with a novelproposals generation technique is designed to preserve the bounding boxes withboth high IoU and classification scores. Extensive experiments show thesuperior performance of our method on the KITTI, nuScenes and Waymo datasets.</description>
      <author>example@mail.com (Zhiwei Ning, Zhaojiang Liu, Xuanang Gao, Yifan Zuo, Jie Yang, Yuming Fang, Wei Liu)</author>
      <guid isPermaLink="false">2508.12917v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Neural Rendering for Sensor Adaptation in 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.12695v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE Intelligent Vehicles Symposium (IV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了自动驾驶车辆不同相机传感器设置导致的跨传感器域差距问题，提出了CamShift数据集和基于神经渲染的传感器适应解决方案，有效减轻了3D目标检测器的性能下降。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆因车型限制导致相机传感器设置各不相同，在一种设置上训练的模型在另一种设置上评估时会产生跨传感器域差距，导致准确率下降。&lt;h4&gt;目的&lt;/h4&gt;研究跨传感器域差距对最先进3D目标检测器的影响，并提出解决方案减轻这一差距。&lt;h4&gt;方法&lt;/h4&gt;创建CamShift数据集模拟紧凑型车辆和SUV之间的域差距，提出基于神经渲染的数据驱动传感器适应流程，将数据集转换为匹配不同相机传感器设置。&lt;h4&gt;主要发现&lt;/h4&gt;显著的跨传感器性能下降；基于密集鸟瞰图表示和反向投影的模型架构(如BEVFormer)对变化的传感器配置最为鲁棒；提出的传感器适应方法改善了所有研究的3D目标检测器性能，大幅减轻了跨传感器域差距影响。&lt;h4&gt;结论&lt;/h4&gt;通过实现不同传感器设置的车辆间高效数据重用，减少了对新数据收集的需求，CamShift数据集和传感器适应基准已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆通常具有不同的相机传感器设置，由于不同车型受到安装位置的限制，这种差异是不可避免的。在一种特定设置上训练感知模型并在新的不同传感器设置上评估会产生所谓的跨传感器域差距，通常导致准确率下降。本文研究了跨传感器域差距对最先进3D目标检测器的影响。为此，我们引入了CamShift数据集，该数据集受nuScenes启发，在CARLA中创建，专门用于模拟紧凑型车辆和运动型多用途车(SUV)之间的域差距。使用CamShift，我们展示了显著的跨传感器性能下降，识别了模型架构对鲁棒性的依赖关系，并提出了数据驱动的解决方案来减轻这种影响。一方面，我们表明基于密集鸟瞰图(BEV)表示和反向投影的模型架构(如BEVFormer)对变化的传感器配置最为鲁棒。另一方面，我们提出了基于神经渲染的新型数据驱动传感器适应流程，可以将整个数据集转换为匹配不同的相机传感器设置。应用这种方法改善了所有研究的3D目标检测器的性能，大幅减轻了跨传感器域差距，并通过实现不同传感器设置的车辆间高效数据重用，减少了对新数据收集的需求。CamShift数据集和传感器适应基准可在https://dmholtz.github.io/camshift/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶车辆中不同相机传感器设置之间的跨传感器域差距问题。这个问题在现实中非常重要，因为不同车型（如SUV和轿车）的相机安装位置不同，导致同一感知模型在不同车辆上的性能显著下降，限制了感知模型在整个车队的部署，并增加了数据收集成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶领域中相机感知模型面临的跨传感器域差距问题，发现缺乏专门研究这一问题的数据集。他们借鉴了现有的3D目标检测方法（如BEVDet、BEVFormer）和神经渲染技术（如NeRF、NeuRAD），设计了CamShift数据集来隔离跨传感器域差距，并提出了基于神经渲染的传感器适配管道。该方法结合了显式场景分解和时间变化物体外观建模，改进了NeuRAD架构，并添加了单独的天空分支。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用神经渲染技术将一个传感器设置的数据集转换到另一个传感器设置，解决跨传感器域差距问题。整体流程包括：1) 创建CamShift数据集，包含sim-SUV和sim-SUB两种传感器设置；2) 为每个序列的周围相机图像训练神经场景表示；3) 渲染原始视图并与原始图像比较进行质量控制；4) 达到质量要求后，为新传感器设置渲染新视图；5) 使用转换后的数据集训练3D目标检测器并评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) CamShift数据集，首个专门隔离跨传感器域差距的数据集；2) 传感器适配基准测试，系统研究不同模型架构对跨传感器域差距的鲁棒性；3) 基于神经渲染的传感器适配管道，能转换整个数据集以匹配不同相机设置；4) 发现基于密集BEV表示和反向投影的方法（如BEVFormer）对跨传感器域差距最鲁棒。相比之前工作，本文专注于传感器设置的域适应（而非跨数据集或天气条件），使用神经渲染进行整个数据集转换，而非传统数据增强技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建专门的CamShift数据集和基于神经渲染的传感器适配管道，有效解决了自动驾驶3D目标检测中的跨传感器域差距问题，显著提升了模型在不同相机配置下的性能并减少了数据收集需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IV64158.2025.11097434&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles often have varying camera sensor setups, which isinevitable due to restricted placement options for different vehicle types.Training a perception model on one particular setup and evaluating it on a new,different sensor setup reveals the so-called cross-sensor domain gap, typicallyleading to a degradation in accuracy. In this paper, we investigate the impactof the cross-sensor domain gap on state-of-the-art 3D object detectors. To thisend, we introduce CamShift, a dataset inspired by nuScenes and created in CARLAto specifically simulate the domain gap between subcompact vehicles and sportutility vehicles (SUVs). Using CamShift, we demonstrate significantcross-sensor performance degradation, identify robustness dependencies on modelarchitecture, and propose a data-driven solution to mitigate the effect. On theone hand, we show that model architectures based on a dense Bird's Eye View(BEV) representation with backward projection, such as BEVFormer, are the mostrobust against varying sensor configurations. On the other hand, we propose anovel data-driven sensor adaptation pipeline based on neural rendering, whichcan transform entire datasets to match different camera sensor setups. Applyingthis approach improves performance across all investigated 3D object detectors,mitigating the cross-sensor domain gap by a large margin and reducing the needfor new data collection by enabling efficient data reusability across vehicleswith different sensor setups. The CamShift dataset and the sensor adaptationbenchmark are available at https://dmholtz.github.io/camshift/.</description>
      <author>example@mail.com (Felix Embacher, David Holtz, Jonas Uhrig, Marius Cordts, Markus Enzweiler)</author>
      <guid isPermaLink="false">2508.12695v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.11951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了点云目标检测中的多尺度特征近似和可转移特征问题。&lt;h4&gt;背景&lt;/h4&gt;多尺度特征对点云目标检测至关重要，但传统多尺度特征学习方法通常涉及多次邻域搜索和尺度感知层，这阻碍了轻量级模型的实现，且不利于计算资源有限的研究。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于知识蒸馏的单邻域点云多尺度特征近似方法，并设计可转移特征嵌入机制以补偿单邻域的结构多样性损失。&lt;h4&gt;方法&lt;/h4&gt;1) 使用知识蒸馏从单邻域近似点云多尺度特征；2) 设计可转移特征嵌入机制；3) 使用类别感知统计作为可转移特征，因其计算成本小；4) 引入中心加权交并比用于定位，缓解优化中中心偏移带来的不对齐问题。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法节省了计算成本。&lt;h4&gt;结论&lt;/h4&gt;在公共数据集上的大量实验证明了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了点云目标检测中的多尺度特征近似和可转移特征。多尺度特征对点云目标检测至关重要。然而，多尺度特征学习通常涉及多次邻域搜索和尺度感知层，这阻碍了轻量级模型的实现，且不利于计算资源有限的研究。本文基于知识蒸馏从单邻域近似点云多尺度特征。为补偿单邻域的结构多样性损失，本文设计了可转移特征嵌入机制。具体而言，使用类别感知统计作为可转移特征，因其计算成本小。此外，本文引入了中心加权交并比用于定位，以缓解优化中中心偏移带来的不对齐问题。注意，本文提出的方法节省了计算成本。在公共数据集上的大量实验证明了所提方法的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云目标检测中多尺度特征学习与模型轻量化之间的平衡问题。在现实应用中，如自动驾驶和机器人视觉，3D目标检测至关重要，但传统多尺度特征方法需要多次邻域搜索，计算成本高，不利于在资源有限的设备上部署，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到多尺度特征学习的高计算成本问题，然后借鉴了知识蒸馏技术，设计了一个'教师-学生'框架：教师分支使用多尺度邻域搜索，学生分支使用单尺度邻域搜索。为了弥补单邻域的信息损失，作者引入了类感知统计作为可转移特征。此外，还借鉴了PointNet++的多尺度分组方法、CPC-3Det的类感知统计概念以及中心投票技术，但进行了创新性整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过知识蒸馏让单尺度邻域的学生模型近似多尺度邻域的教师模型性能，并利用类感知统计增强特征学习能力。整体流程包括：1)使用PointNet初始化特征库；2)通过知识蒸馏实现多尺度特征近似；3)利用中心投票进行目标级别特征聚合；4)将类感知统计嵌入分类和定位头；5)结合软损失和硬损失进行训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于知识蒸馏的单邻域多尺度特征近似，大幅减少计算量；2)可转移类感知统计机制，以低成本增强特征学习；3)中心加权IoU损失解决中心偏移问题。相比之前工作，本文不是简单减少参数，而是通过知识蒸馏保持特征多样性；不仅关注模型轻量化，还注重检测精度；引入类感知统计作为可转移特征，这是之前较少考虑的。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于知识蒸馏和可转移类感知统计的轻量级3D点云目标检测方法，通过单邻域多尺度特征近似和中心加权IoU损失，在保持检测精度的同时显著降低了计算复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates multi-scale feature approximation and transferablefeatures for object detection from point clouds. Multi-scale features arecritical for object detection from point clouds. However, multi-scale featurelearning usually involves multiple neighborhood searches and scale-awarelayers, which can hinder efforts to achieve lightweight models and may not beconducive to research constrained by limited computational resources. Thispaper approximates point-based multi-scale features from a single neighborhoodbased on knowledge distillation. To compensate for the loss of constructivediversity in a single neighborhood, this paper designs a transferable featureembedding mechanism. Specifically, class-aware statistics are employed astransferable features given the small computational cost. In addition, thispaper introduces the central weighted intersection over union for localizationto alleviate the misalignment brought by the center offset in optimization.Note that the method presented in this paper saves computational costs.Extensive experiments on public datasets demonstrate the effectiveness of theproposed method.</description>
      <author>example@mail.com (Hao Peng, Hong Sang, Yajing Ma, Ping Qiu, Chao Ji)</author>
      <guid isPermaLink="false">2508.11951v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Point upsampling networks for single-photon sensing</title>
      <link>http://arxiv.org/abs/2508.12986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures, any comments are welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于状态空间模型的点上采样网络，用于解决单光子传感产生的稀疏且空间偏倚点云的问题，提高了点云密度并减少了空间失真。&lt;h4&gt;背景&lt;/h4&gt;单光子传感作为一种远距离和超灵敏成像技术受到广泛关注，但其产生的点云稀疏且空间分布不均，限制了其实际应用价值。&lt;h4&gt;目的&lt;/h4&gt;开发点上采样网络以增加单光子点云的点密度并减少空间失真，提高单光子传感的实用价值。&lt;h4&gt;方法&lt;/h4&gt;构建基于状态空间模型的网络，包含多路径扫描机制以丰富空间上下文、双向Mamba主干网络以捕获全局几何和局部细节、以及自适应上采样偏移模块以校正偏移引起的失真。&lt;h4&gt;主要发现&lt;/h4&gt;在常用数据集上的实验证实了该方法具有高重建精度和强鲁棒性；在真实世界数据上展示了模型能够生成视觉一致、保留细节且噪声抑制的点云。&lt;h4&gt;结论&lt;/h4&gt;这是首个为单光子传感建立的上采样框架，为单光子传感及其在实际应用中的下游任务开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;单光子传感作为一种突出的远距离和超灵敏成像技术引起了广泛关注，然而，它往往产生稀疏且空间偏倚的点云，从而限制了其实用性。在这项工作中，我们提出使用点上采样网络来增加单光子点云的点密度并减少空间失真。特别是，我们的网络基于状态空间模型构建，集成了多路径扫描机制以丰富空间上下文、双向Mamba主干网络以捕获全局几何和局部细节、以及自适应上采样偏移模块以校正偏移引起的失真。在常用数据集上进行了大量实验，证实了其高重建精度和对失真噪声的强鲁棒性，并在真实数据上展示了我们的模型能够生成视觉一致、保留细节且噪声抑制的点云。我们的工作是首个为单光子传感建立的上采样框架，因此为单光子传感及其在实际应用中的下游任务开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-photon sensing has generated great interest as a prominent techniqueof long-distance and ultra-sensitive imaging, however, it tends to yield sparseand spatially biased point clouds, thus limiting its practical utility. In thiswork, we propose using point upsampling networks to increase point density andreduce spatial distortion in single-photon point cloud. Particularly, ournetwork is built on the state space model which integrates a multi-pathscanning mechanism to enrich spatial context, a bidirectional Mamba backbone tocapture global geometry and local details, and an adaptive upsample shiftmodule to correct offset-induced distortions. Extensive experiments areimplemented on commonly-used datasets to confirm its high reconstructionaccuracy and strong robustness to the distortion noise, and also on real-worlddata to demonstrate that our model is able to generate visually consistent,detail-preserving, and noise suppressed point clouds. Our work is the first toestablish the upsampling framework for single-photon sensing, and hence opens anew avenue for single-photon sensing and its practical applications in thedownstreaming tasks.</description>
      <author>example@mail.com (Jinyi Liu, Guoyang Zhao, Lijun Liu, Yiguang Hong, Weiping Zhang, Shuming Cheng)</author>
      <guid isPermaLink="false">2508.12986v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>RoTO: Robust Topology Obfuscation Against Tomography Inference Attacks</title>
      <link>http://arxiv.org/abs/2508.12852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RoTO是一种健壮的拓扑模糊化方案，通过建模攻击者观察延迟的不确定性，消除了现有防御方法对完美探测控制和固定攻击者模型的依赖，显著提高了网络拓扑隐私保护的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Tomography推理攻击通过分析端到端的探测延迟来重建网络拓扑。现有防御方法依赖于两个强假设：(i)探测包可以被完美检测和修改，(ii)攻击者使用已知、固定的推理算法。这些假设在实践中常常失效，导致在检测错误或自适应攻击者下防御性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出RoTO，一种健壮的拓扑模糊化方案，通过分布公式建模攻击者观察延迟的不确定性，消除现有防御方法的两个关键假设。&lt;h4&gt;方法&lt;/h4&gt;RoTO将防御目标建模为一个min-max优化问题，最大化不确定性集上的预期拓扑失真，不依赖完美的探测控制或特定的攻击者模型。利用图神经网络进行推理模拟和对抗训练，推导攻击者成功概率的上界，并通过优化这个上界来改进拓扑模糊化性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，RoTO优于现有防御方法，在结构相似性方面平均提高34%，在链路距离方面提高42.6%，同时保持强大的健壮性和隐藏能力。&lt;h4&gt;结论&lt;/h4&gt;RoTO通过消除现有防御方法的两个关键假设，显著提高了拓扑模糊化的性能和鲁棒性，为网络拓扑隐私保护提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;断层扫描推理攻击旨在通过分析端到端的探测延迟来重建网络拓扑。现有防御方法通过操纵探测延迟来误导推理，但依赖于两个强假设：(i)探测包可以被完美检测和修改，(ii)攻击者使用已知、固定的推理算法。这些假设在实践中常常失效，导致在检测错误或自适应攻击者下防御性能下降。我们提出了RoTO，一种健壮的拓扑模糊化方案，通过分布公式建模攻击者观察延迟的不确定性，消除了这两个假设。RoTO将防御目标建模为一个min-max优化问题，最大化这个不确定性集上的预期拓扑失真，不依赖完美的探测控制或特定的攻击者模型。为了近似攻击者行为，RoTO利用图神经网络进行推理模拟和对抗训练。我们还推导了攻击者成功概率的上界，并证明我们的方法通过优化这个上界来增强拓扑模糊化性能。实验结果表明，RoTO优于现有防御方法，在结构相似性方面平均提高34%，在链路距离方面提高42.6%，同时保持强大的健壮性和隐藏能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tomography inference attacks aim to reconstruct network topology by analyzingend-to-end probe delays. Existing defenses mitigate these attacks bymanipulating probe delays to mislead inference, but rely on two strongassumptions: (i) probe packets can be perfectly detected and altered, and (ii)attackers use known, fixed inference algorithms. These assumptions often breakin practice, leading to degraded defense performance under detection errors oradaptive adversaries. We present RoTO, a robust topology obfuscation schemethat eliminates both assumptions by modeling uncertainty in attacker-observeddelays through a distributional formulation. RoTO casts the defense objectiveas a min-max optimization problem that maximizes expected topologicaldistortion across this uncertainty set, without relying on perfect probecontrol or specific attacker models. To approximate attacker behavior, RoTOleverages graph neural networks for inference simulation and adversarialtraining. We also derive an upper bound on attacker success probability, anddemonstrate that our approach enhances topology obfuscation performance throughthe optimization of this upper bound. Experimental results show that RoTOoutperforms existing defense methods, achieving average improvements of 34% instructural similarity and 42.6% in link distance while maintaining strongrobustness and concealment capabilities.</description>
      <author>example@mail.com (Chengze Du, Heng Xu, Zhiwei Yu, Ying Zhou, Zili Meng, Jialong Li)</author>
      <guid isPermaLink="false">2508.12852v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics</title>
      <link>http://arxiv.org/abs/2508.12840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种利用图神经网络(GNNs)改进多智能体认知规划(MEP)可扩展性的方法，通过学习认知状态中的模式和关系结构来指导规划过程。&lt;h4&gt;背景&lt;/h4&gt;多智能体认知规划(MEP)是一个自主规划框架，用于推理物理世界和智能体信念，在信息流动和智能体意识至关重要的领域有应用。MEP的状态需要表示为克里普克结构（有向标记图），这限制了现有启发式方法的应用，导致认知求解器在探索指数级搜索空间时通常不可解。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体认知规划中因状态表示导致的可扩展性问题，提高认知求解器的效率。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络(GNNs)学习认知状态中的模式和关系结构，通过从已解决的规划实例中获取的知识，推导出有意义的状态质量估计（例如，到最近目标的距离），并将这些预测启发式方法整合到认知规划流程中。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络能够自然捕捉克里普克模型的图特性，有效指导规划过程，显著提高了多智能体认知规划的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;将图神经网络学习到的预测启发式方法整合到认知规划流程中，能够有效解决传统方法面临的可扩展性问题，为多智能体认知规划提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多智能体认知规划(MEP)是一个自主规划框架，用于推理物理世界和智能体信念，在信息流动和智能体意识至关重要的领域有应用。MEP的丰富性要求状态被表示为克里普克结构，即有向标记图。这种表示限制了现有启发式方法的应用，阻碍了认知求解器的可扩展性，这些求解器必须在没有指导的情况下探索指数级搜索空间，通常导致不可解。为此，我们利用图神经网络(GNNs)学习认知状态中的模式和关系结构，以指导规划过程。GNNs能够自然捕捉克里普克模型的图特性，使我们能够通过从已解决的规划实例中获取的知识，推导出有意义的状态质量估计——例如，到最近目标的距离。我们将这些预测启发式方法整合到认知规划流程中，并与标准基线进行比较，显示出多智能体认知规划可扩展性的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent Epistemic Planning (MEP) is an autonomous planning framework forreasoning about both the physical world and the beliefs of agents, withapplications in domains where information flow and awareness among agents arecritical. The richness of MEP requires states to be represented as Kripkestructures, i.e., directed labeled graphs. This representation limits theapplicability of existing heuristics, hindering the scalability of epistemicsolvers, which must explore an exponential search space without guidance,resulting often in intractability. To address this, we exploit Graph NeuralNetworks (GNNs) to learn patterns and relational structures within epistemicstates, to guide the planning process. GNNs, which naturally capture thegraph-like nature of Kripke models, allow us to derive meaningful estimates ofstate quality -- e.g., the distance from the nearest goal -- by generalizingknowledge obtained from previously solved planning instances. We integratethese predictive heuristics into an epistemic planning pipeline and evaluatethem against standard baselines, showing significant improvements in thescalability of multi-agent epistemic planning.</description>
      <author>example@mail.com (Giovanni Briglia, Francesco Fabiano, Stefano Mariani)</author>
      <guid isPermaLink="false">2508.12840v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>MPOCryptoML: Multi-Pattern based Off-Chain Crypto Money Laundering Detection</title>
      <link>http://arxiv.org/abs/2508.12641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MPOCryptoML的新型加密货币洗钱检测模型，能够有效识别多种洗钱模式，并在多个公开数据集上显示出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有使用图神经网络的洗钱检测模型没有专门设计来检测链下加密货币的多样化洗钱模式，忽略任何洗钱模式都会导致关键检测缺口，降低模型准确性并让洗钱方案规避检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测加密货币交易中多种洗钱模式的模型，解决现有模型无法全面识别不同洗钱结构的问题。&lt;h4&gt;方法&lt;/h4&gt;MPOCryptoML模型包含：1)开发多源个性化PageRank算法识别随机洗钱模式；2)分析高容量金融网络中的交易时间和权重，引入两种新算法检测扇入、扇出、二分图、聚集-分散和堆叠等洗钱结构；3)使用逻辑回归模型分析这些模式间的相关性；4)设计异常评分函数整合各模块结果，按异常分数排名识别高风险账户。&lt;h4&gt;主要发现&lt;/h4&gt;在Elliptic++、以太坊欺诈检测和Wormhole交易数据集上的实验表明，MPOCryptoML在精度上提升最高达9.13%，召回率提升最高达10.16%，F1分数提升最高达7.63%，准确率提升最高达10.19%。&lt;h4&gt;结论&lt;/h4&gt;MPOCryptoML模型能够全面检测多种加密货币洗钱模式，显著提高检测性能，有效识别高风险账户，为加密货币安全交易提供更强有力的保障。&lt;h4&gt;翻译&lt;/h4&gt;近期洗钱检测进展表明，图神经网络在准确捕捉洗钱模式方面具有潜力。然而，现有模型并非专门设计用于检测链下加密货币的多样化洗钱模式。忽略任何洗钱模式都会引入关键检测缺口，因为每种模式都反映促进非法资金来源和流动混淆的独特交易结构。未能考虑这些模式可能导致对特定洗钱活动的检测不足或遗漏，降低模型准确性并使洗钱方案规避检测。为解决这一差距，我们提出了MPOCryptoML模型，以有效检测加密货币交易中的多种洗钱模式。MPOCryptoML包括开发多源个性化PageRank算法来识别随机洗钱模式。此外，我们通过分析高容量金融网络中的交易时间和权重，引入了两种新算法来检测各种洗钱结构，包括扇入、扇出、二分图、聚集-分散和堆叠模式。我们进一步使用逻辑回归模型分析这些模式之间的相关性。异常评分函数整合了每个模块的结果，按异常分数对账户进行排名，系统识别高风险账户。在包括Elliptic++、以太坊欺诈检测和Wormhole交易数据集在内的公共数据集上的广泛实验验证了MPOCryptoML的有效性和效率。结果显示性能持续提升，精度提高最高达9.13%，召回率提高最高达10.16%，F1分数提高最高达7.63%，准确率提高最高达10.19%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in money laundering detection have demonstrated thepotential of using graph neural networks to capture laundering patternsaccurately. However, existing models are not explicitly designed to detect thediverse patterns of off-chain cryptocurrency money laundering. Neglecting anylaundering pattern introduces critical detection gaps, as each pattern reflectsunique transactional structures that facilitate the obfuscation of illicit fundorigins and movements. Failure to account for these patterns may result inunder-detection or omission of specific laundering activities, diminishingmodel accuracy and allowing schemes to bypass detection. To address this gap,we propose the MPOCryptoML model to effectively detect multiple launderingpatterns in cryptocurrency transactions. MPOCryptoML includes the developmentof a multi-source Personalized PageRank algorithm to identify random launderingpatterns. Additionally, we introduce two novel algorithms by analyzing thetimestamp and weight of transactions in high-volume financial networks todetect various money laundering structures, including fan-in, fan-out,bipartite, gather-scatter, and stack patterns. We further examine correlationsbetween these patterns using a logistic regression model. An anomaly scorefunction integrates results from each module to rank accounts by anomaly score,systematically identifying high-risk accounts. Extensive experiments on publicdatasets including Elliptic++, Ethereum fraud detection, and Wormholetransaction datasets validate the efficacy and efficiency of MPOCryptoML.Results show consistent performance gains, with improvements up to 9.13% inprecision, up to 10.16% in recall, up to 7.63% in F1-score, and up to 10.19% inaccuracy.</description>
      <author>example@mail.com (Yasaman Samadi, Hai Dong, Xiaoyu Xia)</author>
      <guid isPermaLink="false">2508.12641v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation</title>
      <link>http://arxiv.org/abs/2508.12629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlowMol3是一种开源的多模态流匹配模型，能够在原子级别生成具有所需特性的真实分子，实现了接近100%的分子有效性，且比同类方法少一个数量级的可学习参数。&lt;h4&gt;背景&lt;/h4&gt;能够生成具有所需特性的真实分子的生成模型可以加速各种应用领域的化学发现。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够联合采样分子拓扑和三维结构的模型，以促进化学发现。&lt;h4&gt;方法&lt;/h4&gt;FlowMol3采用三种架构无关的技术：自条件、假原子和训练时几何扭曲，这些技术不需要改变图神经网络架构或底层流匹配公式，且计算成本可忽略。&lt;h4&gt;主要发现&lt;/h4&gt;FlowMol3在药物类分子上实现了接近100%的分子有效性，更准确地重现了训练数据的官能团组成和几何结构，比同类方法少一个数量级的可学习参数，并能检测和推理过程中的分布漂移。&lt;h4&gt;结论&lt;/h4&gt;这些简单的、可转移的策略可以改善基于扩散和流式的分子生成模型的稳定性和质量。&lt;h4&gt;翻译&lt;/h4&gt;一种能够生成具有所需特性的真实分子的生成模型可以加速各种应用领域的化学发现。为实现这一目标，大量工作集中在开发能够联合采样分子拓扑和三维结构的模型上。我们提出了FlowMol3，一个开源的多模态流匹配模型，在原子级别小分子生成方面取得了最先进的技术突破。与之前的FlowMol版本相比，FlowMol3的性能提升显著，且无需改变图神经网络架构或底层流匹配公式。相反，FlowMol3的改进来自于三种架构无关的技术，这些技术计算成本可忽略：自条件、假原子和训练时几何扭曲。FlowMol3在具有显式氢的药物类分子上实现了接近100%的分子有效性，更准确地重现了训练数据的官能团组成和几何结构，并且比同类方法少一个数量级的可学习参数。我们假设这些技术减轻了影响基于传输的生成模型的普遍病理，使模型能够检测和推理过程中的分布漂移。我们的结果强调了改善基于扩散和流式的分子生成模型的稳定性和质量的简单、可转移的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A generative model capable of sampling realistic molecules with desiredproperties could accelerate chemical discovery across a wide range ofapplications. Toward this goal, significant effort has focused on developingmodels that jointly sample molecular topology and 3D structure. We presentFlowMol3, an open-source, multi-modal flow matching model that advances thestate of the art for all-atom, small-molecule generation. Its substantialperformance gains over previous FlowMol versions are achieved without changesto the graph neural network architecture or the underlying flow matchingformulation. Instead, FlowMol3's improvements arise from threearchitecture-agnostic techniques that incur negligible computational cost:self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3achieves nearly 100% molecular validity for drug-like molecules with explicithydrogens, more accurately reproduces the functional group composition andgeometry of its training data, and does so with an order of magnitude fewerlearnable parameters than comparable methods. We hypothesize that thesetechniques mitigate a general pathology affecting transport-based generativemodels, enabling detection and correction of distribution drift duringinference. Our results highlight simple, transferable strategies for improvingthe stability and quality of diffusion- and flow-based molecular generativemodels.</description>
      <author>example@mail.com (Ian Dunn, David R. Koes)</author>
      <guid isPermaLink="false">2508.12629v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios</title>
      <link>http://arxiv.org/abs/2508.12100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 1 figure, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ReT-Eval的推理线程评估框架，通过两个阶段解决交互式问题解决中的推理效率问题：第一阶段从领域知识图中提取语义相关结构并利用大语言模型知识解决差异；第二阶段使用奖励引导策略评估和修剪推理线程，保持语义连贯性。&lt;h4&gt;背景&lt;/h4&gt;当前推理模型在交互式问题解决场景中存在三大局限：缺乏显式语义层次、用户-领域知识对齐不足、以及缺乏有效机制修剪推理线程，导致输出冗长且无法引导用户进行目标导向推理。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够生成有效推理线程的框架，引导用户完成目标导向推理步骤，提升用户理解能力。&lt;h4&gt;方法&lt;/h4&gt;ReT-Eval框架采用两阶段方法：第一阶段利用图神经网络从稀疏领域知识图中提取语义相关知识结构，并融入大语言模型知识解决知识差异；第二阶段通过奖励引导策略评估和修剪推理线程，确保语义连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;实验和专家评估表明，ReT-Eval框架能够有效增强用户理解，并优于现有最先进的推理模型。&lt;h4&gt;结论&lt;/h4&gt;ReT-Eval框架通过结构化知识重用和有效的推理线程修剪机制，成功解决了当前推理模型的局限性，为交互式问题解决提供了更有效的推理支持。&lt;h4&gt;翻译&lt;/h4&gt;在交互式问题解决场景中的推理需要模型构建反映用户理解并与结构化领域知识对齐的推理线程。然而，当前推理模型通常缺乏显式语义层次、用户-领域知识对齐，以及修剪推理线程的有效原则性机制。这些局限性导致冗长的通用输出，无法引导用户完成目标导向的推理步骤。为此，我们提出了一种受原型启发的两阶段推理线程评估框架，借鉴了强调结构化知识重用的人类推理策略。第一阶段，使用图神经网络从稀疏领域知识图中提取语义相关的知识结构，并利用大语言模型的内在知识解决知识差异。第二阶段，使用奖励引导策略评估和修剪这些线程，旨在保持语义连贯性以生成有效的推理线程。实验和专家评估表明，ReT-Eval增强了用户理解，并优于最先进的推理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning in interactive problem solving scenarios requires models toconstruct reasoning threads that reflect user understanding and align withstructured domain knowledge. However, current reasoning models often lackexplicit semantic hierarchies, user-domain knowledge alignment, and principledmechanisms to prune reasoning threads for effectiveness. These limitationsresult in lengthy generic output that does not guide users throughgoal-oriented reasoning steps. To address this, we propose aprototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)framework, drawing inspiration from human-like reasoning strategies thatemphasize structured knowledge reuse. In the first phase, semantically relevantknowledge structures are extracted from a sparse domain knowledge graph using agraph neural network and enriched with intrinsic large language model knowledgeto resolve knowledge discrepancies. In the second phase, these threads areevaluated and pruned using a reward-guided strategy aimed at maintainingsemantic coherence to generate effective reasoning threads. Experiments andexpert evaluations show that ReT-Eval enhances user understanding andoutperforms state-of-the-art reasoning models.</description>
      <author>example@mail.com (Daniel Burkhardt, Xiangwei Cheng)</author>
      <guid isPermaLink="false">2508.12100v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Poisson Models for Supply Chain Relationship Forecasting</title>
      <link>http://arxiv.org/abs/2508.12044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种图双指数平滑（GDES）模型，用于预测供应链网络中未来供应关系的形成概率。该模型结合了图神经网络和非参数双指数平滑方法，能够捕捉供应链中复杂的互动关系。&lt;h4&gt;背景&lt;/h4&gt;在供应链网络中，公司会动态地形成或终止合作关系以适应市场波动，这给预测未来的供应链关系带来了挑战。仅依靠历史数据来预测未来强度存在局限性。&lt;h4&gt;目的&lt;/h4&gt;预测供应链网络中未来供应关系的形成概率，克服仅依靠历史数据的局限性。&lt;h4&gt;方法&lt;/h4&gt;将供应关系建模为非齐次泊松过程，提出图双指数平滑（GDES）模型，结合图神经网络和非参数双指数平滑方法。假设供应边的泊松强度函数是相关的，模型具有可解释性，能够将强度增量分解为当前边的历史数据和相邻边的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在包含87,969家公司的大规模供应链数据集上评估，该方法在动态链接预测中达到了93.84%的AUC，有效捕捉了复杂的供应链互动关系。&lt;h4&gt;结论&lt;/h4&gt;图双指数平滑模型在准确预测供应链网络中的未来关系方面是有效的，能够处理供应链中复杂的互动关系。&lt;h4&gt;翻译&lt;/h4&gt;在供应链网络中，公司动态地形成或终止合作关系以适应市场波动，这给预测未来的供应链关系带来了挑战。我们将供应关系（从公司i到公司j）建模为非齐次泊松过程，使用历史事件计数来估计直到时间t的泊松强度函数。然而，仅依靠历史数据预测未来强度存在局限性。为此，我们提出了一种新的图双指数平滑（GDES）模型，该模型结合了图神经网络和非参数双指数平滑方法，以预测未来供应关系形成的概率。认识到上游和下游公司之间的相互依存的经济动态，我们假设供应边的泊松强度函数是相关的，符合过程的非齐次性质。我们的模型具有可解释性，将强度增量分解为当前边的历史数据和供应链网络中相邻边的影响。在包含87,969家公司的大规模供应链数据集上进行评估，我们的方法在动态链接预测中达到了93.84%的AUC，证明了其在捕捉复杂供应链互动以实现准确预测方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In supply chain networks, firms dynamically form or dissolve partnerships toadapt to market fluctuations, posing a challenge for predicting future supplyrelationships. We model the occurrence of supply edges (firm i to firm j) as anon-homogeneous Poisson process (NHPP), using historical event counts toestimate the Poisson intensity function up to time t. However, forecastingfuture intensities is hindered by the limitations of historical data alone. Toovercome this, we propose a novel Graph Double Exponential Smoothing (GDES)model, which integrates graph neural networks (GNNs) with a nonparametricdouble exponential smoothing approach to predict the probability of futuresupply edge formations.Recognizing the interdependent economic dynamics betweenupstream and downstream firms, we assume that the Poisson intensity functionsof supply edges are correlated, aligning with the non-homogeneous nature of theprocess.Our model is interpretable, decomposing intensity increments intocontributions from the current edge's historical data and influences fromneighboring edges in the supply chain network. Evaluated on a large-scalesupply chain dataset with 87,969 firms, our approach achieves an AUC of 93.84 %in dynamic link prediction, demonstrating its effectiveness in capturingcomplex supply chain interactions for accurate forecasting.</description>
      <author>example@mail.com (Ling Xiang, Quan Hu, Xiang Zhang, Wei Lan, Bin Liu)</author>
      <guid isPermaLink="false">2508.12044v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>AI Models for Depressive Disorder Detection and Diagnosis: A Review</title>
      <link>http://arxiv.org/abs/2508.12022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文对抑郁症检测和诊断的最先进人工智能方法进行了全面综述，基于对55项关键研究的系统回顾，提出了一个分层分类法，并揭示了该领域的三个主要趋势。&lt;h4&gt;背景&lt;/h4&gt;抑郁症是全球主要的致残原因之一，但其诊断仍主要依赖主观的临床评估。&lt;h4&gt;目的&lt;/h4&gt;整合人工智能有望开发出客观、可扩展和及时的诊断工具，为抑郁症诊断提供新方法。&lt;h4&gt;方法&lt;/h4&gt;通过对55项关键研究进行系统回顾，提出一个分层分类法，按主要临床任务（诊断与预测）、数据模态（文本、语音、神经影像、多模态）和计算模型类别（如图神经网络、大型语言模型、混合方法）组织该领域。&lt;h4&gt;主要发现&lt;/h4&gt;深入分析揭示了三个主要趋势：图神经网络在建模大脑连接性中的主导地位、大型语言模型在语言和对话数据中的兴起，以及对多模态融合、可解释性和算法公平性的新兴关注。&lt;h4&gt;结论&lt;/h4&gt;通过综合当前进展并突出开放性挑战，这篇调查为计算精神病学未来的创新提供了全面的路线图。&lt;h4&gt;翻译&lt;/h4&gt;重度抑郁症是全球主要的致残原因之一，但其诊断仍然很大程度上依赖于主观的临床评估。整合人工智能有望开发出客观、可扩展和及时的诊断工具。在本文中，我们基于对55项关键研究的系统回顾，对用于抑郁症检测和诊断的最先进人工智能方法进行了全面综述。我们引入了一种新颖的分层分类法，通过主要临床任务（诊断与预测）、数据模态（文本、语音、神经影像、多模态）和计算模型类别（如图神经网络、大型语言模型、混合方法）对该领域进行结构化组织。我们的深入分析揭示了三个主要趋势：图神经网络在建模大脑连接性中的主导地位、大型语言模型在语言和对话数据中的兴起，以及对多模态融合、可解释性和算法公平性的新兴关注。除了方法论见解外，我们还概述了突出的公共数据集和标准评估指标，为研究人员提供实用指南。通过综合当前进展并突出开放性挑战，本调查为计算精神病学未来的创新提供了全面的路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Major Depressive Disorder is one of the leading causes of disabilityworldwide, yet its diagnosis still depends largely on subjective clinicalassessments. Integrating Artificial Intelligence (AI) holds promise fordeveloping objective, scalable, and timely diagnostic tools. In this paper, wepresent a comprehensive survey of state-of-the-art AI methods for depressiondetection and diagnosis, based on a systematic review of 55 key studies. Weintroduce a novel hierarchical taxonomy that structures the field by primaryclinical task (diagnosis vs. prediction), data modality (text, speech,neuroimaging, multimodal), and computational model class (e.g., graph neuralnetworks, large language models, hybrid approaches). Our in-depth analysisreveals three major trends: the predominance of graph neural networks formodeling brain connectivity, the rise of large language models for linguisticand conversational data, and an emerging focus on multimodal fusion,explainability, and algorithmic fairness. Alongside methodological insights, weprovide an overview of prominent public datasets and standard evaluationmetrics as a practical guide for researchers. By synthesizing current advancesand highlighting open challenges, this survey offers a comprehensive roadmapfor future innovation in computational psychiatry.</description>
      <author>example@mail.com (Dorsa Macky Aleagha, Payam Zohari, Mostafa Haghir Chehreghani)</author>
      <guid isPermaLink="false">2508.12022v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images</title>
      <link>http://arxiv.org/abs/2508.11826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究填补了图神经网络在图像到图转换和图级异常检测中的比较空白，系统评估了多种转换方法的有效性&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为基于图的机器学习任务的有力方法，先前工作将GNNs应用于图像派生的图表示用于下游任务，但尚无研究严格比较众多图像到图转换方法在GNN-based图级异常检测中的有效性&lt;h4&gt;目的&lt;/h4&gt;系统评估多种分割方案、边构建策略和基于颜色、纹理和形状描述符的节点特征集的有效性，生成适合图级异常检测的图像派生图表示&lt;h4&gt;方法&lt;/h4&gt;使用最先进的GLAD模型在皮肤镜图像上进行大量实验，检查在完全无监督、弱监督和完全监督模式下的性能和效率&lt;h4&gt;主要发现&lt;/h4&gt;颜色描述符提供最佳独立性能，结合形状和纹理特征能持续提高检测效果；最佳无监督配置使用OCGTL实现0.805的AUC-ROC；加入稀疏标签后性能提高到0.872；完全监督下达到0.914 AUC-ROC&lt;h4&gt;结论&lt;/h4&gt;颜色、形状和纹理特征的结合能提高异常检测性能；即使在无监督设置下，所提出的方法也能与基于图像的竞争方法相媲美&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为基于图的机器学习任务的有力方法。先前的工作将GNNs应用于图像派生的图表示，用于各种下游任务，如分类或异常检测。这些转换包括分割图像、从片段中提取特征、将它们映射到节点以及连接它们。然而，据我们所知，尚无研究严格比较众多可能的图像到图转换方法在基于GNN的图级异常检测中的有效性。在本研究中，我们系统评估了多种分割方案、边构建策略和基于颜色、纹理和形状描述符的节点特征集的有效性，以生成适合用于图级异常检测的图像派生图表示。我们使用最先进的GLAD模型在皮肤镜图像上进行了大量实验，检查了在完全无监督、弱监督和完全监督模式下的性能和效率。我们的研究结果表明，例如，颜色描述符提供了最佳的独立性能，而结合形状和纹理特征能持续提高检测效果。特别是，我们使用OCGTL的最佳无监督配置实现了高达0.805的竞争性AUC-ROC分数，而不依赖于预训练主干网络，与可比的基于图像的方法相当。加入稀疏标签后，性能显著提高到0.872，完全监督下达到0.914 AUC-ROC。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as a powerful approach forgraph-based machine learning tasks. Previous work applied GNNs to image-derivedgraph representations for various downstream tasks such as classification oranomaly detection. These transformations include segmenting images, extractingfeatures from segments, mapping them to nodes, and connecting them. However, tothe best of our knowledge, no study has rigorously compared the effectivenessof the numerous potential image-to-graph transformation approaches forGNN-based graph-level anomaly detection (GLAD). In this study, wesystematically evaluate the efficacy of multiple segmentation schemes, edgeconstruction strategies, and node feature sets based on color, texture, andshape descriptors to produce suitable image-derived graph representations toperform graph-level anomaly detection. We conduct extensive experiments ondermoscopic images using state-of-the-art GLAD models, examining performanceand efficiency in purely unsupervised, weakly supervised, and fully supervisedregimes. Our findings reveal, for example, that color descriptors contributethe best standalone performance, while incorporating shape and texture featuresconsistently enhances detection efficacy. In particular, our best unsupervisedconfiguration using OCGTL achieves a competitive AUC-ROC score of up to 0.805without relying on pretrained backbones like comparable image-based approaches.With the inclusion of sparse labels, the performance increases substantially to0.872 and with full supervision to 0.914 AUC-ROC.</description>
      <author>example@mail.com (Dehn Xu, Tim Katzke, Emmanuel Müller)</author>
      <guid isPermaLink="false">2508.11826v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication</title>
      <link>http://arxiv.org/abs/2508.11733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages for main content, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SafeSieve是一种渐进式自适应多智能体剪枝算法，通过双机制动态优化智能体间通信，实现了从启发式初始化到经验驱动精细化的平滑过渡，在保持高准确率的同时显著减少了token使用和部署成本。&lt;h4&gt;背景&lt;/h4&gt;基于大语言模型的多智能体系统表现出强大的协作能力，但通常存在冗余通信和过多的token开销问题。现有方法通常通过预训练的GNN或贪心算法来提高效率，但缺乏统一的策略来优化任务前和任务后的过程。&lt;h4&gt;目的&lt;/h4&gt;提出SafeSieve，一种渐进式自适应多智能体剪枝算法，通过新的双机制动态优化智能体间通信，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;SafeSieve整合了基于LLM的初始语义评估和累积的性能反馈，采用0扩展聚类来保持结构连贯的智能体组，同时消除无效连接，实现了从启发式初始化到经验驱动精细化的平滑过渡。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，SafeSieve实现了94.01%的平均准确率，同时减少了12.4%-27.8%的token使用；在提示注入攻击下表现出鲁棒性（平均准确率仅下降1.23%）；在异构环境中，在保持性能的同时减少了13.3%的部署成本。&lt;h4&gt;结论&lt;/h4&gt;SafeSieve是一种鲁棒、高效和可扩展的框架，适用于实际多智能体系统。&lt;h4&gt;翻译&lt;/h4&gt;基于大语言模型的多智能体系统表现出强大的协作能力，但常常遭受冗余通信和过多的token开销。现有方法通常通过预训练的GNN或贪心算法来提高效率，但通常将任务前和任务后的优化分开，缺乏统一的策略。为此，我们提出了SafeSieve，一种渐进式自适应多智能体剪枝算法，通过一种新的双机制动态优化智能体间通信。SafeSieve整合了基于LLM的初始语义评估和累积的性能反馈，实现了从启发式初始化到经验驱动精细化的平滑过渡。与现有的贪心Top-k剪枝方法不同，SafeSieve采用0扩展聚类来保持结构连贯的智能体组，同时消除无效连接。在多个基准测试（SVAMP、HumanEval等）中的实验表明，SafeSieve实现了94.01%的平均准确率，同时减少了12.4%-27.8%的token使用。结果进一步表明在提示注入攻击下具有鲁棒性（平均准确率下降1.23%）。在异构环境中，SafeSieve在保持性能的同时减少了13.3%的部署成本。这些结果确立了SafeSieve作为实际多智能体系统的一种鲁棒、高效和可扩展的框架。我们的代码可以在https://anonymous.4open.science/r/SafeSieve-D8F2FFUN找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLM-based multi-agent systems exhibit strong collaborative capabilities butoften suffer from redundant communication and excessive token overhead.Existing methods typically enhance efficiency through pretrained GNNs or greedyalgorithms, but often isolate pre- and post-task optimization, lacking aunified strategy. To this end, we present SafeSieve, a progressive and adaptivemulti-agent pruning algorithm that dynamically refines the inter-agentcommunication through a novel dual-mechanism. SafeSieve integrates initialLLM-based semantic evaluation with accumulated performance feedback, enabling asmooth transition from heuristic initialization to experience-drivenrefinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs0-extension clustering to preserve structurally coherent agent groups whileeliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval,etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducingtoken usage by 12.4%-27.8%. Results further demonstrate robustness under promptinjection attacks (1.23% average accuracy drop). In heterogeneous settings,SafeSieve reduces deployment costs by 13.3% while maintaining performance.These results establish SafeSieve as a robust, efficient, and scalableframework for practical multi-agent systems. Our code can be found inhttps://anonymous.4open.science/r/SafeSieve-D8F2FFUN.</description>
      <author>example@mail.com (Ruijia Zhang, Xinyan Zhao, Ruixiang Wang, Sigen Chen, Guibin Zhang, An Zhang, Kun Wang, Qingsong Wen)</author>
      <guid isPermaLink="false">2508.11733v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data</title>
      <link>http://arxiv.org/abs/2508.11723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 32 figures, submitted to Environment and Planning B: Urban  Analytics and City Science&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SPLI的场地规划布局指标系统，这是一个数据驱动的框架，整合实证知识和多源异构数据，用于生成结构化的城市空间信息，支持多模态空间数据分析。&lt;h4&gt;背景&lt;/h4&gt;城市空间布局影响土地利用效率和空间组织，但传统场地规划依赖经验判断和单一数据源，限制了多功能布局的系统量化。&lt;h4&gt;目的&lt;/h4&gt;开发SPLI系统，通过整合多源数据支持城市空间分析，并扩展传统指标，包括建筑功能层次分类、空间组织、功能多样性、基本服务可达性和土地利用强度五个维度。&lt;h4&gt;方法&lt;/h4&gt;SPLI系统结合OpenStreetMap、兴趣点、建筑形态、土地利用和卫星影像等多源数据，通过关系图神经网络和图神经网络等深度学习技术解决数据缺口问题。&lt;h4&gt;主要发现&lt;/h4&gt;SPLI系统提高了功能分类的准确性，为自动化、数据驱动的城市空间分析提供了标准化基础。&lt;h4&gt;结论&lt;/h4&gt;SPLI系统通过整合多源数据和实证知识，为城市空间分析提供了更系统、量化的方法，克服了传统场地规划的局限性。&lt;h4&gt;翻译&lt;/h4&gt;城市场地的空间布局塑造了土地利用效率和空间组织。传统的场地规划往往依赖经验判断和单一数据源，限制了多功能布局的系统量化。我们提出了一个场地规划布局指标（SPLI）系统，这是一个数据驱动的框架，整合了实证知识和多源异构数据，以生成结构化的城市空间信息。SPLI通过结合OpenStreetMap（OSM）、兴趣点（POI）、建筑形态、土地利用和卫星影像，支持多模态空间数据系统进行分析、推理和检索。它通过五个维度扩展了传统指标：（1）层次化建筑功能分类，将实证系统细化为清晰的层次结构；（2）空间组织，量化七种布局模式（如对称型、同心型、轴向型）；（3）功能多样性，使用功能比率（FR）和辛普森指数（SI）将定性评估转化为可测量指标；（4）基本服务可达性，整合设施分布和交通网络，提供全面的可达性指标；（5）土地利用强度，使用容积率（FAR）和建筑覆盖率（BCR）评估利用效率。通过深度学习（包括关系图神经网络（RGNN）和图神经网络（GNN））解决数据缺口问题。实验表明，SPLI提高了功能分类的准确性，并为自动化、数据驱动的城市空间分析提供了标准化基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The spatial layout of urban sites shapes land-use efficiency and spatialorganization. Traditional site planning often relies on experiential judgmentand single-source data, limiting systematic quantification of multifunctionallayouts. We propose a Site Planning Layout Indicator (SPLI) system, adata-driven framework integrating empirical knowledge with heterogeneousmulti-source data to produce structured urban spatial information. The SPLIsupports multimodal spatial data systems for analytics, inference, andretrieval by combining OpenStreetMap (OSM), Points of Interest (POI), buildingmorphology, land use, and satellite imagery. It extends conventional metricsthrough five dimensions: (1) Hierarchical Building Function Classification,refining empirical systems into clear hierarchies; (2) Spatial Organization,quantifying seven layout patterns (e.g., symmetrical, concentric,axial-oriented); (3) Functional Diversity, transforming qualitative assessmentsinto measurable indicators using Functional Ratio (FR) and Simpson Index (SI);(4) Accessibility to Essential Services, integrating facility distribution andtransport networks for comprehensive accessibility metrics; and (5) Land UseIntensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) toassess utilization efficiency. Data gaps are addressed through deep learning,including Relational Graph Neural Networks (RGNN) and Graph Neural Networks(GNN). Experiments show the SPLI improves functional classification accuracyand provides a standardized basis for automated, data-driven urban spatialanalytics.</description>
      <author>example@mail.com (Qian Cao, Jielin Chen, Junchao Zhao, Rudi Stouffs)</author>
      <guid isPermaLink="false">2508.11723v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis</title>
      <link>http://arxiv.org/abs/2508.13028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Speech Synthesis Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种新的讽刺性语音合成方法，通过整合双模态讽刺检测模型的反馈损失到TTS训练过程，并利用迁移学习进行两阶段微调，有效提高了合成语音的质量、自然度和讽刺表达能力。&lt;h4&gt;背景&lt;/h4&gt;讽刺性语音合成对于增强娱乐和人机交互等应用中的自然互动至关重要。然而，由于讽刺的韵律微妙性以及标注的讽刺语音数据有限，合成讽刺性语音仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;解决讽刺性语音合成中的挑战，提高模型捕捉和传达讽刺的能力，生成更自然、更具讽刺意识的语音。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一种新方法，将双模态讽刺检测模型的反馈损失整合到TTS训练过程中。同时，利用迁移学习，首先在包含各种语音风格（包括讽刺语音）的多样化数据集上对预训练的语音合成模型进行微调，然后在专注于讽刺语音的数据集上进一步优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过客观和主观评估，证明提出的方法提高了合成语音的质量、自然度和讽刺表达能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效解决讽刺性语音合成中的挑战，生成更高质量的讽刺语音，适用于娱乐和人机交互等应用场景。&lt;h4&gt;翻译&lt;/h4&gt;讽刺性语音合成涉及生成能有效传达讽刺的语音，对于增强娱乐和人机交互等应用中的自然互动至关重要。然而，由于讽刺特有的微妙韵律以及标注的讽刺语音数据有限，合成讽刺性语音仍然是一个挑战。为解决这些挑战，本研究引入了一种新方法，将双模态讽刺检测模型的反馈损失整合到TTS训练过程中，增强模型捕捉和传达讽刺的能力。此外，通过利用迁移学习，在朗读语音上预训练的语音合成模型经历了两阶段微调过程。首先，它在包含各种语音风格（包括讽刺语音）的多样化数据集上进行微调。在第二阶段，模型使用专注于讽刺语音的数据集进一步优化，提高其生成讽刺意识语音的能力。客观和主观评估证明，我们提出的方法提高了合成语音的质量、自然度和讽刺意识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sarcastic speech synthesis, which involves generating speech that effectivelyconveys sarcasm, is essential for enhancing natural interactions inapplications such as entertainment and human-computer interaction. However,synthesizing sarcastic speech remains a challenge due to the nuanced prosodythat characterizes sarcasm, as well as the limited availability of annotatedsarcastic speech data. To address these challenges, this study introduces anovel approach that integrates feedback loss from a bi-modal sarcasm detectionmodel into the TTS training process, enhancing the model's ability to captureand convey sarcasm. In addition, by leveraging transfer learning, a speechsynthesis model pre-trained on read speech undergoes a two-stage fine-tuningprocess. First, it is fine-tuned on a diverse dataset encompassing variousspeech styles, including sarcastic speech. In the second stage, the model isfurther refined using a dataset focused specifically on sarcastic speech,enhancing its ability to generate sarcasm-aware speech. Objective andsubjective evaluations demonstrate that our proposed methods improve thequality, naturalness, and sarcasm-awareness of synthesized speech.</description>
      <author>example@mail.com (Zhu Li, Yuqing Zhang, Xiyuan Gao, Devraj Raghuvanshi, Nagendra Kumar, Shekhar Nayak, Matt Coler)</author>
      <guid isPermaLink="false">2508.13028v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian</title>
      <link>http://arxiv.org/abs/2508.12993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究发现图的代数连通性（Fiedler值）可以作为预测图卷积网络（GCN）性能的有效指标，具有相似Fiedler值的图结构特性类似，可使用相同超参数获得相似结果，且在这些图间迁移学习更有效。&lt;h4&gt;背景&lt;/h4&gt;在图卷积网络（GCN）文献中，一个常见的观察是堆叠GCN层可能也可能不会在节点分类和边预测等任务上带来更好的性能，需要更好的指标来预测GCN性能。&lt;h4&gt;目的&lt;/h4&gt;探索图的代数连通性（Fiedler值）作为GCN性能预测指标的可行性，并研究为什么Fiedler值是一个好的预测指标。&lt;h4&gt;方法&lt;/h4&gt;通过理论和实验方式研究，在合成和真实图数据（包括Cora、CiteSeer和Polblogs数据集）上进行实验，探索多种聚合图中连通分量Fiedler值的方法，并给出Fiedler值作为良好预测指标的理论解释。&lt;h4&gt;主要发现&lt;/h4&gt;1. 图的代数连通性（Fiedler值）是预测GCN性能的良好指标；2. 具有相似Fiedler值的图具有相似的结构特性；3. 在具有相似代数连通性的图之间进行迁移学习可能更有效；4. 可以通过多种方式聚合图中连通分量的Fiedler值来获得整个图的值。&lt;h4&gt;结论&lt;/h4&gt;Fiedler值可以作为预测GCN性能的有效指标，对于理解和优化GCN在不同图上的应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;图卷积网络（GCN）文献中的一个常见观察是，堆叠GCN层可能也可能不会在节点分类和边预测等任务上带来更好的性能。我们通过经验发现，图的代数连通性（称为Fiedler值）是预测GCN性能的良好指标。直观地说，具有相似Fiedler值的图具有类似的结构特性，这表明当与GCN一起使用时，相同的过滤器和超参数可能会产生相似的结果，并且在具有相似代数连通性的图之间的迁移学习可能更有效。我们在合成和真实图数据（包括Cora、CiteSeer和Polblogs数据集）上的实验中从理论和经验上探索了这一点。我们探索了多种方法来聚合图中连通分量的Fiedler值，以获得整个图的值，并展示它可以用于预测GCN性能。我们还提出了理论论据，解释为什么Fiedler值是一个好的预测指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A common observation in the Graph Convolutional Network (GCN) literature isthat stacking GCN layers may or may not result in better performance on taskslike node classification and edge prediction. We have found empirically that agraph's algebraic connectivity, which is known as the Fiedler value, is a goodpredictor of GCN performance. Intuitively, graphs with similar Fiedler valueshave analogous structural properties, suggesting that the same filters andhyperparameters may yield similar results when used with GCNs, and thattransfer learning may be more effective between graphs with similar algebraicconnectivity. We explore this theoretically and empirically with experiments onsynthetic and real graph data, including the Cora, CiteSeer and Polblogsdatasets. We explore multiple ways of aggregating the Fiedler value forconnected components in the graphs to arrive at a value for the entire graph,and show that it can be used to predict GCN performance. We also presenttheoretical arguments as to why the Fiedler value is a good predictor.</description>
      <author>example@mail.com (Shalima Binta Manir, Tim Oates)</author>
      <guid isPermaLink="false">2508.12993v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Neutrino Scattering: Domain Adaptation with GANs</title>
      <link>http://arxiv.org/abs/2508.12987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用迁移学习技术，将在合成中微子-碳散射数据上训练的GAN模型迁移应用于中微子-氩和反中微子-碳相互作用的散射事件生成，显著优于从头开始训练的方法，即使在数据量较小的情况下也能保持良好性能。&lt;h4&gt;背景&lt;/h4&gt;研究基于在合成的荷电流(CC)中微子-碳非弹性散射数据上训练的生成对抗网络(GAN)模型，关注中微子与不同原子核相互作用的散射事件生成问题。&lt;h4&gt;目的&lt;/h4&gt;利用迁移学习将GAN模型中的物理知识迁移到新的中微子-原子核相互作用类型，评估迁移学习在数据来自不同相互作用模型时的有效性，为实验数据稀少场景提供解决方案。&lt;h4&gt;方法&lt;/h4&gt;采用迁移学习技术，将基础GAN模型适应到新的相互作用类型，比较迁移学习与从头开始训练的性能，使用10,000和100,000两种不同规模的事件数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习显著优于从头开始训练生成模型；通过迁移学习获得的模型即使在较小的训练数据(10,000事件)下也能表现良好；该方法在实验数据稀少的场景中具有应用潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的迁移学习方法为构建中微子散射事件生成器提供了有希望的技术路线，特别适用于实验数据有限的粒子物理研究领域。&lt;h4&gt;翻译&lt;/h4&gt;我们利用迁移学习来推断在合成的荷电流(CC)中微子-碳非弹性散射数据上训练的生成对抗网络(GAN)模型中编码的物理知识。这个基础模型被调整为生成中微子-氩和反中微子-碳相互作用的CC非弹性散射事件(仅轻子运动学)。此外，我们评估了迁移学习在当新数据来自不同的中微子-原子核相互作用模型时重新优化自定义模型的有效性。我们的结果表明，迁移学习显著优于从头开始训练生成模型。为了研究这一点，我们考虑两个训练数据集：一个包含10,000个事件，另一个包含100,000个事件。通过迁移学习获得的模型即使在较小的训练数据下也能表现良好。所提出的方法为在实验数据稀少的场景中构建中微子散射事件生成器提供了一种有希望的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We utilize transfer learning to extrapolate the physics knowledge encoded ina Generative Adversarial Network (GAN) model trained on syntheticcharged-current (CC) neutrino-carbon inclusive scattering data. This base modelis adapted to generate CC inclusive scattering events (lepton kinematics only)for neutrino-argon and antineutrino-carbon interactions. Furthermore, we assessthe effectiveness of transfer learning in re-optimizing a custom model when newdata comes from a different neutrino-nucleus interaction model. Our resultsdemonstrate that transfer learning significantly outperforms traininggenerative models from scratch. To study this, we consider two training datasets: one with 10,000 and another with 100,000 events. The models obtained viatransfer learning perform well even with smaller training data. The proposedmethod provides a promising approach for constructing neutrino scattering eventgenerators in scenarios where experimental data is sparse.</description>
      <author>example@mail.com (Jose L. Bonilla, Krzysztof M. Graczyk, Artur M. Ankowski, Rwik Dharmapal Banerjee, Beata E. Kowal, Hemant Prasad, Jan T. Sobczyk)</author>
      <guid isPermaLink="false">2508.12987v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning</title>
      <link>http://arxiv.org/abs/2508.12877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MPS-Tuning的新型微调方法，用于预训练视觉-语言模型，通过保留和雕刻语义流形来提高模型性能同时保持数据分布的几何结构。&lt;h4&gt;背景&lt;/h4&gt;预训练的视觉-语言模型（如CLIP）在少样本图像分类中展现出巨大潜力，催生了多种迁移学习策略。这些方法利用VLM的预训练知识实现有效的领域适应，同时通过参数高效调整或基于实例的一致性约束减轻过拟合。&lt;h4&gt;目的&lt;/h4&gt;克服现有正则化方法忽略数据分布几何结构的问题，提出一种能够保留数据分布几何结构同时增强类别可分性的新型微调方法。&lt;h4&gt;方法&lt;/h4&gt;提出'流形保持与雕刻调优'（MPS-Tuning）方法，将特征空间中的数据分布视为语义流形，通过微调前后特征的Gram矩阵对齐保留原始流形的拓扑结构，并将图像和文本模态的特征配对优化以增强类别判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，MPS-Tuning显著提高了模型性能，同时有效保留了语义流形的结构。&lt;h4&gt;结论&lt;/h4&gt;MPS-Tuning是一种有效的微调方法，能够平衡模型性能和语义结构保持，代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;预训练的视觉-语言模型（如CLIP）在少样本图像分类中展现出巨大潜力，并催生了许多有效的迁移学习策略。这些方法利用VLM的预训练知识实现有效的领域适应，同时通过参数高效调整或基于实例的一致性约束来减轻过拟合。然而，这类正则化方法通常忽略了数据分布的几何结构，可能导致整体语义表示的扭曲。为克服这一局限，我们提出了一种新型微调方法——流形保持与雕刻调优（MPS-Tuning）。将特征空间中的数据分布视为语义流形，MPS-Tuning明确约束该流形的内在几何结构，同时进一步雕刻它以增强类别可分性。具体而言，MPS-Tuning通过微调前后特征的Gram矩阵对齐，保留了原始流形的宏观和微观拓扑结构。理论上，该约束被证明近似于Gromov-Wasserstein距离的上界。此外，图像和文本模态的特征被配对，并通过优化成对相似性来增强流形的类别判别能力。大量实验表明，MPS-Tuning显著提高了模型性能，同时有效保留了语义流形的结构。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained vision-language models (VLMs), such as CLIP, have shown remarkablepotential in few-shot image classification and led to numerous effectivetransfer learning strategies. These methods leverage the pretrained knowledgeof VLMs to enable effective domain adaptation while mitigating overfittingthrough parameter-efficient tuning or instance-based consistency constraints.However, such regularizations often neglect the geometric structure of datadistribution, which may lead to distortion of the overall semanticrepresentation. To overcome this limitation, we propose a novel fine-tuningmethod, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding thedata distribution in feature space as a semantic manifold, MPS-Tuningexplicitly constrains the intrinsic geometry of this manifold while furthersculpting it to enhance class separability. Specifically, MPS-Tuning preservesboth macroscopic and microscopic topological structures of the originalmanifold by aligning Gram matrices of features before and after fine-tuning.Theoretically, this constraint is shown to approximate an upper bound of theGromov-Wasserstein distance. Furthermore, features from the image and textmodalities are paired, and pairwise similarities are optimized to enhance themanifold's class discriminability. Extensive experiments demonstrate thatMPS-Tuning significantly improves model performance while effectivelypreserving the structure of the semantic manifold. The code will be released.</description>
      <author>example@mail.com (Dexia Chen, Qianjie Zhu, Weibing Li, Yue Yu, Tong Zhang, Ruixuan Wang)</author>
      <guid isPermaLink="false">2508.12877v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models</title>
      <link>http://arxiv.org/abs/2508.12861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对视觉语言模型在跨域少样本图像识别任务中的局限性，提出了一种名为一致性引导多视图协同优化的新型微调策略，并建立了相应的跨域少样本基准进行评估。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在自然图像和语言数据上预训练后，在少样本图像识别任务中展现出巨大潜力，各种迁移学习方法也取得了良好性能。然而，当面对与自然图像不同的成像域的跨域任务时，这些方法的有效性通常受到限制。&lt;h4&gt;目的&lt;/h4&gt;解决视觉语言模型在跨域少样本任务中的有效性受限问题，提高模型在非自然图像域的识别性能。&lt;h4&gt;方法&lt;/h4&gt;提出一致性引导多视图协同优化策略，该策略使用两个功能互补的专家模块提取多视图特征，并结合基于先验知识的一致性约束和基于信息几何的共识机制增强特征学习的鲁棒性。同时建立新的跨域少样本基准用于全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;在现有和新提出的基准上的大量经验评估表明，CoMuCo在少样本任务中始终优于当前方法，有效提升了视觉语言模型在跨域场景下的性能。&lt;h4&gt;结论&lt;/h4&gt;CoMuCo是一种有效的视觉语言模型微调策略，能够成功处理跨域少样本图像识别任务，相关代码和基准将公开发布以促进领域研究。&lt;h4&gt;翻译&lt;/h4&gt;在自然图像和语言数据上预训练的视觉语言模型，如CLIP，在少样本图像识别任务中展现出巨大潜力，促进了各种高效迁移学习方法的发展。这些方法利用视觉语言模型中固有的预学习知识，在标准图像数据集上取得了强大性能。然而，当面对与自然图像不同的成像域的跨域任务时，它们的有效性通常受到限制。为解决这一局限，我们提出了一致性引导多视图协同优化，一种视觉语言模型的新型微调策略。该策略采用两个功能互补的专家模块提取多视图特征，同时融入基于先验知识的一致性约束和基于信息几何的共识机制，以增强特征学习的鲁棒性。此外，还建立了一个新的跨域少样本基准，用于全面评估在不同于自然图像的成像域上的方法。在现有和新提出的基准上的大量经验评估表明，CoMuCo在少样本任务中持续优于当前方法。代码和基准将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) pre-trained on natural image and language data,such as CLIP, have exhibited significant potential in few-shot imagerecognition tasks, leading to development of various efficient transferlearning methods. These methods exploit inherent pre-learned knowledge in VLMsand have achieved strong performance on standard image datasets. However, theireffectiveness is often limited when confronted with cross-domain tasks whereimaging domains differ from natural images. To address this limitation, wepropose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), anovel fine-tuning strategy for VLMs. This strategy employs two functionallycomplementary expert modules to extract multi-view features, whileincorporating prior knowledge-based consistency constraints and informationgeometry-based consensus mechanisms to enhance the robustness of featurelearning. Additionally, a new cross-domain few-shot benchmark is established tohelp comprehensively evaluate methods on imaging domains distinct from naturalimages. Extensive empirical evaluations on both existing and newly proposedbenchmarks suggest CoMuCo consistently outperforms current methods in few-shottasks. The code and benchmark will be released.</description>
      <author>example@mail.com (Dexia Chen, Wentao Zhang, Qianjie Zhu, Ping Hu, Weibing Li, Tong Zhang, Ruixuan Wang)</author>
      <guid isPermaLink="false">2508.12861v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Contro</title>
      <link>http://arxiv.org/abs/2508.12738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, accepted for CDC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层次贝叶斯优化框架，用于在不同闭环任务中高效学习控制器参数，通过利用问题的结构知识实现知识转移和增强数据效率。&lt;h4&gt;背景&lt;/h4&gt;许多控制问题需要在不同闭环任务中重复调整和适应控制器，其中数据效率和适应性至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个分层次贝叶斯优化框架，专门针对顺序决策和控制场景中的高效控制器参数学习。&lt;h4&gt;方法&lt;/h4&gt;该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识（包括动态系统、控制律和相关的闭环成本函数）。使用高斯过程构建分层代理模型，捕捉不同参数化下的闭环状态演化，并通过已知闭式表达式精确计算特定任务的权重和累积到闭环成本中。这允许在不同闭环任务之间进行知识转移和增强数据效率。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架保留了与标准黑盒贝叶斯优化相当的次线性遗憾保证，同时支持多任务或迁移学习。&lt;h4&gt;结论&lt;/h4&gt;在模型预测控制的仿真实验中，与纯黑盒贝叶斯优化方法相比，在样本效率和适应性方面都有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;许多控制问题需要在不同闭环任务中重复调整和适应控制器，其中数据效率和适应性至关重要。我们提出了一种针对顺序决策和控制场景中不同任务高效控制器参数学习的分层次贝叶斯优化框架。该方法不将闭环成本视为黑盒，而是利用底层问题的结构知识，包括动态系统、控制律和相关的闭环成本函数。我们使用高斯过程构建分层代理模型，捕捉不同参数化下的闭环状态演化，同时通过已知闭式表达式精确计算特定任务的权重并累积到闭环成本中。这允许在不同闭环任务之间进行知识转移和增强数据效率。与标准黑盒贝叶斯优化相比，所提出的框架保留了次线性遗憾保证，同时支持多任务或迁移学习。模型预测控制的仿真实验表明，与纯黑盒贝叶斯优化方法相比，在样本效率和适应性方面都有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many control problems require repeated tuning and adaptation of controllersacross distinct closed-loop tasks, where data efficiency and adaptability arecritical. We propose a hierarchical Bayesian optimization (BO) framework thatis tailored to efficient controller parameter learning in sequentialdecision-making and control scenarios for distinct tasks. Instead of treatingthe closed-loop cost as a black-box, our method exploits structural knowledgeof the underlying problem, consisting of a dynamical system, a control law, andan associated closed-loop cost function. We construct a hierarchical surrogatemodel using Gaussian processes that capture the closed-loop state evolutionunder different parameterizations, while the task-specific weighting andaccumulation into the closed-loop cost are computed exactly via knownclosed-form expressions. This allows knowledge transfer and enhanced dataefficiency between different closed-loop tasks. The proposed framework retainssublinear regret guarantees on par with standard black-box BO, while enablingmulti-task or transfer learning. Simulation experiments with model predictivecontrol demonstrate substantial benefits in both sample efficiency andadaptability when compared to purely black-box BO approaches.</description>
      <author>example@mail.com (Sebastian Hirt, Lukas Theiner, Maik Pfefferkorn, Rolf Findeisen)</author>
      <guid isPermaLink="false">2508.12738v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>BUILDA: A Thermal Building Data Generation Framework for Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.12703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings can be accessed at:  https://annsim.org/2025-annsim-proceedings/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BuilDa，一个用于生成高质量、足够数量合成数据的建筑热数据生成框架，以支持迁移学习研究，无需深入的建筑模拟专业知识。&lt;h4&gt;背景&lt;/h4&gt;迁移学习可改善建筑热动力学数据驱动建模，但现有研究方向需要大量建筑热数据，而公共数据集和数据生成器在数据质量和数量上无法满足需求，且现有数据生成方法需要建筑模拟专业知识。&lt;h4&gt;目的&lt;/h4&gt;开发BuilDa框架，生成质量和数量足够的合成数据用于迁移学习研究，减少对建筑模拟专业知识的依赖。&lt;h4&gt;方法&lt;/h4&gt;BuilDa使用单区域Modelica模型，导出为功能mock-up unit (FMU)，在Python中模拟，无需深入的建筑模拟知识即可生成大量数据。&lt;h4&gt;主要发现&lt;/h4&gt;通过生成数据并用于预训练和微调迁移学习模型，成功演示了BuilDa框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;BuilDa框架能够生成满足迁移学习研究需求的高质量和足够数量的合成数据，降低了建筑热数据获取的门槛。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习可以改善建筑热动力学的数据驱动建模。因此，该领域出现了许多新的迁移学习研究方向，例如选择合适的迁移学习源模型。然而，这些研究方向目前需要大量的建筑热数据，而公共数据集和现有数据生成器在数据质量和数量上无法满足迁移学习研究的需要。此外，现有数据生成方法通常需要建筑模拟专业知识。我们提出了BuilDa，一个用于生成高质量和足够数量合成数据的建筑热数据生成框架，以支持迁移学习研究。该框架无需深厚的建筑模拟知识即可生成大量数据。BuilDa使用单区域Modelica模型，该模型被导出为功能mock-up unit (FMU)并在Python中模拟。我们通过生成数据并将其用于预训练和微调迁移学习模型来演示BuilDa。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning (TL) can improve data-driven modeling of building thermaldynamics. Therefore, many new TL research areas emerge in the field, such asselecting the right source model for TL. However, these research directionsrequire massive amounts of thermal building data which is lacking presently.Neither public datasets nor existing data generators meet the needs of TLresearch in terms of data quality and quantity. Moreover, existing datageneration approaches typically require expert knowledge in buildingsimulation. We present BuilDa, a thermal building data generation framework forproducing synthetic data of adequate quality and quantity for TL research. Theframework does not require profound building simulation knowledge to generatelarge volumes of data. BuilDa uses a single-zone Modelica model that isexported as a Functional Mock-up Unit (FMU) and simulated in Python. Wedemonstrate BuilDa by generating data and utilizing it for pretraining andfine-tuning TL models.</description>
      <author>example@mail.com (Thomas Krug, Fabian Raisch, Dominik Aimer, Markus Wirnsberger, Ferdinand Sigg, Benjamin Schäfer, Benjamin Tischler)</author>
      <guid isPermaLink="false">2508.12703v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification</title>
      <link>http://arxiv.org/abs/2508.12418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 7 figures. Submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Bi-Axial Transformer(BAT)的新型模型，用于处理电子健康记录(EHRs)的分类任务，该模型同时关注临床变量和时间点轴，能够学习更丰富的数据关系并解决数据稀疏性问题。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHRs)作为患者医疗历史的数字表示，是流行病学和临床研究的宝贵资源。然而，EHRs正变得越来越复杂，表现为更大的数据集、更长的时间序列和多模态集成。Transformer模型虽然因其能够建模长距离依赖和并行处理数据而适合处理这些挑战，但在EHR分类中的应用仍受限于数据表示方式，这可能降低性能或无法捕捉有意义的缺失信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理EHR数据复杂性和稀疏性的模型，提高EHR分类任务的性能，特别是在败血症预测和死亡率分类方面。&lt;h4&gt;方法&lt;/h4&gt;提出Bi-Axial Transformer(BAT)，该模型同时关注EHR数据的临床变量和时间点两个轴，学习更丰富的数据关系，并解决数据稀疏性问题。作者还重新实现了基线模型，使用PyTorch框架，使这些模型可在多个存储库中获得。&lt;h4&gt;主要发现&lt;/h4&gt;BAT在败血症预测任务上取得了最先进的性能，在死亡率分类方面与顶尖方法具有竞争力。与其他Transformer相比，BAT对数据缺失具有更强的鲁棒性，并能学习独特的传感器嵌入，可用于迁移学习。&lt;h4&gt;结论&lt;/h4&gt;BAT模型通过双轴注意力机制有效解决了EHR数据中的挑战，提高了分类性能并增强了鲁棒性。重新实现的基线模型为未来研究提供了可复现的基准。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHRs)是患者医疗历史的数字表示，是流行病学和临床研究的宝贵资源。它们也正变得越来越复杂，最近的趋势表明数据集更大、时间序列更长、多模态集成增加。由于在自然语言处理和其他领域的成功，Transformer迅速普及，它们非常适合解决这些挑战，因为它们能够建模长距离依赖和并行处理数据。但它们在EHR分类中的应用仍受限于数据表示，这可能降低性能或无法捕捉有意义的缺失信息。在本文中，我们提出了双轴Transformer(BAT)，它同时关注EHR数据的临床变量和时间点轴，以学习更丰富的数据关系并解决数据稀疏性问题。BAT在败血症预测上取得了最先进的性能，并且在死亡率分类方面与顶尖方法具有竞争力。与其他Transformer相比，BAT对数据缺失表现出更强的鲁棒性，并学习独特的传感器嵌入，可用于迁移学习。之前位于多个存储库或使用已弃用库的基线模型，已使用PyTorch重新实现，可供复现和未来基准测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs), the digital representation of a patient'smedical history, are a valuable resource for epidemiological and clinicalresearch. They are also becoming increasingly complex, with recent trendsindicating larger datasets, longer time series, and multi-modal integrations.Transformers, which have rapidly gained popularity due to their success innatural language processing and other domains, are well-suited to address thesechallenges due to their ability to model long-range dependencies and processdata in parallel. But their application to EHR classification remains limitedby data representations, which can reduce performance or fail to captureinformative missingness. In this paper, we present the Bi-Axial Transformer(BAT), which attends to both the clinical variable and time point axes of EHRdata to learn richer data relationships and address the difficulties of datasparsity. BAT achieves state-of-the-art performance on sepsis prediction and iscompetitive to top methods for mortality classification. In comparison to othertransformers, BAT demonstrates increased robustness to data missingness, andlearns unique sensor embeddings which can be used in transfer learning.Baseline models, which were previously located across multiple repositories orutilized deprecated libraries, were re-implemented with PyTorch and madeavailable for reproduction and future benchmarking.</description>
      <author>example@mail.com (Rachael DeVries, Casper Christensen, Marie Lisandra Zepeda Mendoza, Ole Winther)</author>
      <guid isPermaLink="false">2508.12418v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Dual-species atomic absorption image reconstruction using deep neural networks</title>
      <link>http://arxiv.org/abs/2508.12120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度学习的在线图像补全协议，用于减少双原子系统光学吸收信号中的干涉条纹。&lt;h4&gt;背景&lt;/h4&gt;光学成像在理解被困中性原子行为方面发挥着重要作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少双原子系统中光学吸收信号的干涉条纹。&lt;h4&gt;方法&lt;/h4&gt;采用深度学习方法和迁移学习方案，逐步更新先前学习的参数，实现在线图像补全。&lt;h4&gt;主要发现&lt;/h4&gt;该方法对于两种不同的原子物种（6Li和23Na）都能有效抑制干涉条纹，显示出稳健的解决方案。&lt;h4&gt;结论&lt;/h4&gt;提出的在线图像补全方法能够有效适应漂移的实验条件，可以轻松集成到实验室环境中，迁移学习可以加速图像分析。&lt;h4&gt;翻译&lt;/h4&gt;光学成像在理解被困中性原子行为方面发挥着重要作用。在本工作中，我们描述了一种基于深度学习的在线图像补全协议，用于减少双原子系统光学吸收信号中的干涉条纹。无论对于两种不同原子物种（6Li和23Na）的任务性质如何不同，该方法都能显示出抑制条纹的稳健解决方案。为了将其融入日常操作，需要一种迁移学习方案，逐步更新先前学习的参数。我们概述了一种在线图像补全方法，能够有效适应漂移的实验条件。我们的方法可以轻松集成到实验室环境中，迁移学习可以加速图像分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical imaging plays an instrumental role in understanding the behavior oftrapped neutral atoms. In this work, we describe a deep learning-based onlineimage completion protocol that reduces interference fringes in opticalabsorption signals for a dual-species atomic system. Regardless of the distinctnature of the task for two different atomic species, 6Li and 23Na, the methoddisplays a robust solution for suppressing fringes. To incorporate this intodaily operations, a transfer learning scheme is required that incrementallyupdates the previously learned parameters. We outline an online imagecompletion method that efficiently adapts to drifting experimental conditions.Our method can be easily integrated into lab settings, where transfer learningcan accelerate image analysis.</description>
      <author>example@mail.com (Kyuhwan Lee, Yong-il Shin)</author>
      <guid isPermaLink="false">2508.12120v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    <item>
      <title>Robust Data Fusion via Subsampling</title>
      <link>http://arxiv.org/abs/2508.12048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在数据污染情况下结合子抽样策略的迁移学习方法，以提高有限目标数据集的性能，并通过理论分析和实际应用验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;数据融合和迁移学习是快速发展领域，可通过利用相关数据源或任务提升目标人群模型性能，但面临数据异质性和实际集成挑战。&lt;h4&gt;目的&lt;/h4&gt;研究目标数据有限而外部数据量大但受异常值污染的情况，开发结合子抽样策略的迁移学习方法，解决数据污染下的迁移学习问题。&lt;h4&gt;方法&lt;/h4&gt;研究两种子抽样策略（减少偏差和最小化方差），提出结合策略提高估计器性能，提供非渐近误差边界，并通过模拟和A380飞机硬着陆风险分析验证方法。&lt;h4&gt;主要发现&lt;/h4&gt;异常值可能因任意均值偏移偏离真实模型；样本量、信号强度、采样率等因素影响估计器性能；所提方法在模拟中表现优越；健壮迁移学习可提高稀有类型数据的估计效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的结合子抽样策略的迁移学习方法能有效处理数据污染问题，提高估计器性能，并成功应用于稀有飞机类型的风险分析。&lt;h4&gt;翻译&lt;/h4&gt;数据融合和迁移学习是快速发展的领域，通过利用其他相关数据源或任务来提高目标人群的模型性能。挑战在于目标与外部数据之间各种潜在的异质性，以及阻止简单数据集成的实际问题。我们考虑一个现实场景，其中目标数据规模有限，而外部数据量大但被异常值污染；这种数据污染以及其他计算和操作约束，需要适当选择或对迁移学习的外部数据进行子抽样。据我们所知，数据污染下的迁移学习和子抽样尚未得到充分研究。我们通过研究使用外部数据子样本的各种迁移学习方法来解决这个问题，考虑了由于任意均值偏移而偏离底层真实模型的异常值。研究了两种子抽样策略：一种旨在减少偏差，另一种旨在最小化方差。还引入了结合这些策略的方法，以提高估计器的性能。我们提供了迁移学习估计器的非渐近误差边界，阐明了样本量、信号强度、采样率、异常值大小和模型误差分布尾部行为等因素的作用。广泛的模拟显示了所提出方法的优越性能。此外，我们通过利用其他飞机类型的数据，将方法应用于分析A380飞机的硬着陆风险，证明健壮的迁移学习可以在借助其他飞机类型数据的情况下，提高相对罕见飞机类型的估计效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data fusion and transfer learning are rapidly growing fields that enhancemodel performance for a target population by leveraging other related datasources or tasks. The challenges lie in the various potential heterogeneitiesbetween the target and external data, as well as various practical concernsthat prevent a na\"ive data integration. We consider a realistic scenario wherethe target data is limited in size while the external data is large butcontaminated with outliers; such data contamination, along with othercomputational and operational constraints, necessitates proper selection orsubsampling of the external data for transfer learning. To ourknowledge,transfer learning and subsampling under data contamination have notbeen thoroughly investigated. We address this gap by studying various transferlearning methods with subsamples of the external data, accounting for outliersdeviating from the underlying true model due to arbitrary mean shifts. Twosubsampling strategies are investigated: one aimed at reducing biases and theother at minimizing variances. Approaches to combine these strategies are alsointroduced to enhance the performance of the estimators. We providenon-asymptotic error bounds for the transfer learning estimators, clarifyingthe roles of sample sizes, signal strength, sampling rates, magnitude ofoutliers, and tail behaviors of model error distributions, among other factors.Extensive simulations show the superior performance of the proposed methods.Additionally, we apply our methods to analyze the risk of hard landings in A380airplanes by utilizing data from other airplane types,demonstrating that robusttransfer learning can improve estimation efficiency for relatively rareairplane types with the help of data from other types of airplanes.</description>
      <author>example@mail.com (Jing Wang, HaiYing Wang, Kun Chen)</author>
      <guid isPermaLink="false">2508.12048v1</guid>
      <pubDate>Tue, 19 Aug 2025 17:13:32 +0800</pubDate>
    </item>
    </channel>
</rss>