<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 14 May 2025 14:16:09 +0800</lastBuildDate>
    <item>
      <title>Integrating Machine Learning with Triboelectric Nanogenerators: Optimizing Electrode Materials and Doping Strategies for Intelligent Energy Harves</title>
      <link>http://arxiv.org/abs/2505.07414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将机器学习技术与摩擦纳米发电机（TENGs）相结合的方法，以优化能量收集技术。通过使用图神经网络，该研究框架能够预测并提升TENG电极材料和掺杂策略的性能。&lt;h4&gt;背景&lt;/h4&gt;将机器学习技术与TENGs结合，为优化能量收集技术提供了新的途径。&lt;h4&gt;目的&lt;/h4&gt;利用图神经网络预测和提升TENG电极材料和掺杂策略的性能。&lt;h4&gt;方法&lt;/h4&gt;通过利用大量实验和计算结果的数据集，模型有效地对电极材料进行分类，预测最佳的掺杂比例，并建立稳健的结构-性能关系。&lt;h4&gt;主要发现&lt;/h4&gt;模型发现，铝掺杂的PTFE能量密度提高了65.7%，氟掺杂的PTFE提高了85.7%。PTFE被证明是一种高效的负极材料，当使用铜作为正极时，通过7%的银掺杂，其能量密度可达1.12 J/cm²。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅加速了材料发现，还显著降低了实验成本，为理解影响TENG性能的基本因素提供了新的见解，为智能材料设计建立了一个稳健的平台，推动了可持续能源技术和自供电系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;The integration of machine learning techniques with triboelectricnanogenerators (TENGs) offers a transformative pathway for optimizing energyharvesting technologies. In this study, we propose a comprehensive frameworkthat utilizes graph neural networks to predict and enhance the performance ofTENG electrode materials and doping strategies. By leveraging an extensivedataset of experimental and computational results, the model effectivelyclassifies electrode materials, predicts optimal doping ratios, and establishesrobust structure-property relationships. Key findings include a 65.7% increasein energy density for aluminum-doped PTFE and an 85.7% improvement forfluorine-doped PTFE, highlighting the critical influence of doping materialsand their concentrations. The model further identifies PTFE as a highlyeffective negative electrode material, achieving a maximum energy density of1.12 J/cm$^2$ with 7% silver (Ag) doping when copper (Cu) is used as thepositive electrode. This data-driven approach not only accelerates materialdiscovery but also significantly reduces experimental costs, providing novelinsights into the fundamental factors influencing TENG performance. Theproposed methodology establishes a robust platform for intelligent materialdesign, advancing the development of sustainable energy technologies andself-powered systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of machine learning techniques with triboelectricnanogenerators (TENGs) offers a transformative pathway for optimizing energyharvesting technologies. In this study, we propose a comprehensive frameworkthat utilizes graph neural networks to predict and enhance the performance ofTENG electrode materials and doping strategies. By leveraging an extensivedataset of experimental and computational results, the model effectivelyclassifies electrode materials, predicts optimal doping ratios, and establishesrobust structure-property relationships. Key findings include a 65.7% increasein energy density for aluminum-doped PTFE and an 85.7% improvement forfluorine-doped PTFE, highlighting the critical influence of doping materialsand their concentrations. The model further identifies PTFE as a highlyeffective negative electrode material, achieving a maximum energy density of1.12 J/cm$^2$ with 7% silver (Ag) doping when copper (Cu) is used as thepositive electrode. This data-driven approach not only accelerates materialdiscovery but also significantly reduces experimental costs, providing novelinsights into the fundamental factors influencing TENG performance. Theproposed methodology establishes a robust platform for intelligent materialdesign, advancing the development of sustainable energy technologies andself-powered systems.</description>
      <author>example@mail.com (Guanping Xu, Zirui Zhao, Zhong Lin Wang, Hai-Feng Li)</author>
      <guid isPermaLink="false">2505.07414v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
  <item>
      <title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
      <link>http://arxiv.org/abs/2505.07301v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强3D人体运动预测的方法，通过使用估计的姿势和视频数据来减少对昂贵运动捕捉数据的依赖，以提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的3D人体运动预测方法依赖昂贵的运动捕捉数据，但数据收集成本高，导致数据多样性不足，模型泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;提高3D人体运动预测模型的泛化能力，使其能更好地处理未见过的运动或主体。&lt;h4&gt;方法&lt;/h4&gt;将易于获取的视频中的2D姿势通过特定流程转换为运动捕捉风格的3D运动，并通过这些运动进行额外的学习，以适应测试域。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在定量和定性方面都有显著影响。&lt;h4&gt;结论&lt;/h4&gt;通过使用估计的姿势和视频数据进行额外学习，可以有效提高3D人体运动预测模型的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D Human Motion Prediction (HMP), conventional methods train HMP modelswith expensive motion capture data. However, the data collection cost of suchmotion capture data limits the data diversity, which leads to poorgeneralizability to unseen motions or subjects. To address this issue, thispaper proposes to enhance HMP with additional learning using estimated posesfrom easily available videos. The 2D poses estimated from the monocular videosare carefully transformed into motion capture-style 3D motions through ourpipeline. By additional learning with the obtained motions, the HMP model isadapted to the test domain. The experimental results demonstrate thequantitative and qualitative impact of our method.</description>
      <author>example@mail.com (Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita)</author>
      <guid isPermaLink="false">2505.07301v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data</title>
      <link>http://arxiv.org/abs/2505.08736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages; 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于核物理的原型基础模型，该模型能够处理来自未来电子离子对撞机中成像切伦科夫探测器的低级探测器输入。&lt;h4&gt;背景&lt;/h4&gt;现有基于下一个标记预测的方法存在局限性，如VQ-VAE标记化导致的分辨率损失和条件生成能力不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了三种关键创新：分别对离散空间特征和连续变量使用不同的词汇表，并通过因果多头交叉注意力（CMHCA）结合；通过添加上下文嵌入实现连续动力学条件化；以及实现可扩展、简单且高分辨率的无联合词汇膨胀的连续变量标记化。&lt;h4&gt;方法&lt;/h4&gt;模型能够快速、高保真地生成切伦科夫光子的像素和时间序列，并通过高性能DIRC中的封闭测试进行验证。此外，模型在π介子和K介子识别等重建任务中表现出泛化能力，并展示了其微调的能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型通过使用不同的词汇表和因果多头交叉注意力，以及连续动力学条件化和无联合词汇膨胀的标记化方法，有效地解决了现有方法的局限性。&lt;h4&gt;结论&lt;/h4&gt;该模型在核物理领域具有广泛的应用前景，能够提高探测器数据处理的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于核物理的原型基础模型，该模型能够处理来自未来电子离子对撞机中成像切伦科夫探测器的低级探测器输入。为了解决现有基于下一个标记预测的方法的局限性，我们提出了三种关键创新：分别对离散空间特征和连续变量使用不同的词汇表，并通过因果多头交叉注意力（CMHCA）结合；通过添加上下文嵌入实现连续动力学条件化；以及实现可扩展、简单且高分辨率的无联合词汇膨胀的连续变量标记化。我们的模型能够快速、高保真地生成切伦科夫光子的像素和时间序列，并通过高性能DIRC中的封闭测试进行验证。我们还展示了我们的模型在π介子和K介子识别等重建任务中的泛化能力，并展示了其微调的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a (proto) Foundation Model for Nuclear Physics, capable ofoperating on low-level detector inputs from Imaging Cherenkov Detectors at thefuture Electron Ion Collider. To address limitations in existing next-tokenprediction approaches-namely resolution loss from VQ-VAE tokenization and lackof conditional generation-we propose three key innovations: (i) separatevocabularies for discrete spatial features and continuous variates, combinedvia Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematicconditioning through prepended context embeddings, and (iii) scalable andsimple, high-resolution continuous variate tokenization without jointvocabulary inflation. Our model enables fast, high-fidelity generation of pixeland time sequences for Cherenkov photons, validated through closure tests inthe High Performance DIRC. We also show our model generalizes to reconstructiontasks such as pion and kaon identification, in which we show its ability toleverage fine-tuning.</description>
      <author>example@mail.com (James Giroux, Cristiano Fanelli)</author>
      <guid isPermaLink="false">2505.08736v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.08725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The dataset and code will be released at  https://github.com/zc-zhao/DriveMonkey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NuInteract的大规模数据集和DriveMonkey框架，用于改进大型视觉语言模型（LVLMs）在场景理解方面的能力。&lt;h4&gt;背景&lt;/h4&gt;LVLMs在图像理解方面取得了显著进展，但在全面场景理解和3D感知方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有LVLMs在场景理解方面的不足，特别是3D感知和指令理解的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NuInteract数据集，包含超过150万张多视角图像语言对，以及DriveMonkey框架，该框架通过可学习的查询将LVLMs与空间处理器无缝集成，并使用预训练的3D检测器初始化空间处理器。&lt;h4&gt;主要发现&lt;/h4&gt;DriveMonkey在3D视觉地面实况任务上优于一般的LVLMs，实现了9.86%的显著提升。&lt;h4&gt;结论&lt;/h4&gt;NuInteract数据集和DriveMonkey框架能够有效提高LVLMs在场景理解方面的性能。&lt;h4&gt;翻译&lt;/h4&gt;The Large Visual-Language Models (LVLMs) have significantly advanced imageunderstanding. Their comprehension and reasoning capabilities enable promisingapplications in autonomous driving scenarios. However, existing researchtypically focuses on front-view perspectives and partial objects within scenes,struggling to achieve comprehensive scene understanding. Meanwhile, existingLVLMs suffer from the lack of mapping relationship between 2D and 3D andinsufficient integration of 3D object localization and instructionunderstanding. To tackle these limitations, we first introduce NuInteract, alarge-scale dataset with over 1.5M multi-view image language pairs spanningdense scene captions and diverse interactive tasks. Furthermore, we proposeDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMswith a spatial processor using a series of learnable queries. The spatialprocessor, designed as a plug-and-play component, can be initialized withpre-trained 3D detectors to improve 3D perception. Our experiments show thatDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notableimprovement on the 3D visual grounding task. The dataset and code will bereleased at https://github.com/zc-zhao/DriveMonkey.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Large Visual-Language Models (LVLMs) have significantly advanced imageunderstanding. Their comprehension and reasoning capabilities enable promisingapplications in autonomous driving scenarios. However, existing researchtypically focuses on front-view perspectives and partial objects within scenes,struggling to achieve comprehensive scene understanding. Meanwhile, existingLVLMs suffer from the lack of mapping relationship between 2D and 3D andinsufficient integration of 3D object localization and instructionunderstanding. To tackle these limitations, we first introduce NuInteract, alarge-scale dataset with over 1.5M multi-view image language pairs spanningdense scene captions and diverse interactive tasks. Furthermore, we proposeDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMswith a spatial processor using a series of learnable queries. The spatialprocessor, designed as a plug-and-play component, can be initialized withpre-trained 3D detectors to improve 3D perception. Our experiments show thatDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notableimprovement on the 3D visual grounding task. The dataset and code will bereleased at https://github.com/zc-zhao/DriveMonkey.</description>
      <author>example@mail.com (Zongchuang Zhao, Haoyu Fu, Dingkang Liang, Xin Zhou, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai)</author>
      <guid isPermaLink="false">2505.08725v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>GNN-based Precoder Design and Fine-tuning for Cell-free Massive MIMO with Real-world CSI</title>
      <link>http://arxiv.org/abs/2505.08788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了无细胞大规模MIMO（CF-mMIMO）技术，并探讨了基于图神经网络（GNN）的预编码方法，通过在合成数据集和真实世界传播环境中进行训练和验证，证明了GNN预编码技术在从合成到真实无线环境中的有效泛化能力。&lt;h4&gt;背景&lt;/h4&gt;CF-mMIMO技术在未来无线网络中提供均匀高质量覆盖具有巨大潜力，但其预编码在分布式系统中的挑战需要解决。&lt;h4&gt;目的&lt;/h4&gt;通过GNN方法解决CF-mMIMO中的预编码挑战，并验证其从合成数据集到真实世界传播环境的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用模拟的信道状态信息（CSI）数据对GNN进行预训练，结合标准传播模型和小尺度瑞利衰落。然后，在物理测试平台上收集的真实CSI测量数据上对模型进行微调，采用层冻结策略以平衡预训练特征和适应真实世界条件。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的GNN模型在20 dB信噪比（SNR）下实现了约8.2比特每信道使用的增益，对应15.7%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;转移学习在GNN预编码中起着关键作用，并表明GNN预编码技术在合成到真实无线环境中的泛化具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;Cell-free massive MIMO (CF-mMIMO) has emerged as a promising paradigm for delivering uniformly high-quality coverage in future wireless networks. To address the inherent challenges of precoding in such distributed systems, recent studies have explored the use of graph neural network (GNN)-based methods, using their powerful representation capabilities. However, these approaches have predominantly been trained and validated on synthetic datasets, leaving their generalizability to real-world propagation environments largely unverified. In this work, we initially pre-train the GNN using simulated channel state information (CSI) data, which incorporates standard propagation models and small-scale Rayleigh fading. Subsequently, we fine-tune the model on real-world CSI measurements collected from a physical testbed equipped with distributed access points (APs). To balance the retention of pre-trained features with adaptation to real-world conditions, we adopt a layer-freezing strategy during fine-tuning, wherein several GNN layers are frozen and only the later layers remain trainable. Numerical results demonstrate that the fine-tuned GNN significantly outperforms the pre-trained model, achieving an approximate 8.2 bits per channel use gain at 20 dB signal-to-noise ratio (SNR), corresponding to a 15.7 % improvement. These findings highlight the critical role of transfer learning and underscore the potential of GNN-based precoding techniques to effectively generalize from synthetic to real-world wireless environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cell-free massive MIMO (CF-mMIMO) has emerged as a promising paradigm fordelivering uniformly high-quality coverage in future wireless networks. Toaddress the inherent challenges of precoding in such distributed systems,recent studies have explored the use of graph neural network (GNN)-basedmethods, using their powerful representation capabilities. However, theseapproaches have predominantly been trained and validated on synthetic datasets,leaving their generalizability to real-world propagation environments largelyunverified. In this work, we initially pre-train the GNN using simulatedchannel state information (CSI) data, which incorporates standard propagationmodels and small-scale Rayleigh fading. Subsequently, we finetune the model onreal-world CSI measurements collected from a physical testbed equipped withdistributed access points (APs). To balance the retention of pre-trainedfeatures with adaptation to real-world conditions, we adopt a layer-freezingstrategy during fine-tuning, wherein several GNN layers are frozen and only thelater layers remain trainable. Numerical results demonstrate that thefine-tuned GNN significantly outperforms the pre-trained model, achieving anapproximate 8.2 bits per channel use gain at 20 dB signal-to-noise ratio (SNR),corresponding to a 15.7 % improvement. These findings highlight the criticalrole of transfer learning and underscore the potential of GNN-based precodingtechniques to effectively generalize from synthetic to real-world wirelessenvironments.</description>
      <author>example@mail.com (Tianzheng Miao, Thomas Feys, Gilles Callebaut, Jarne Van Mulders, Emanuele Peschiera, Md Arifur Rahman, François Rottenberg)</author>
      <guid isPermaLink="false">2505.08788v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.08614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WaveGuard的主动水印框架，旨在解决Deepfake技术带来的隐私侵犯和身份盗窃风险。&lt;h4&gt;背景&lt;/h4&gt;Deepfake技术存在隐私侵犯和身份盗窃等风险。&lt;h4&gt;目的&lt;/h4&gt;提出WaveGuard框架以应对Deepfake技术的威胁。&lt;h4&gt;方法&lt;/h4&gt;WaveGuard通过频域嵌入和基于图的结构一致性来增强鲁棒性和不可见性。具体来说，使用双树复小波变换（DT-CWT）将水印嵌入到高频子带，并采用结构一致性图神经网络（SC-GNN）来保持视觉质量。此外，还设计了一个注意力模块来提高嵌入精度。&lt;h4&gt;主要发现&lt;/h4&gt;在人脸交换和重演任务上的实验结果表明，WaveGuard在鲁棒性和视觉质量方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WaveGuard框架在应对Deepfake技术威胁方面具有良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake technology poses increasing risks such as privacy invasion andidentity theft. To address these threats, we propose WaveGuard, a proactivewatermarking framework that enhances robustness and imperceptibility viafrequency-domain embedding and graph-based structural consistency.Specifically, we embed watermarks into high-frequency sub-bands using Dual-TreeComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency GraphNeural Network (SC-GNN) to preserve visual quality. We also design an attentionmodule to refine embedding precision. Experimental results on face swap andreenactment tasks demonstrate that WaveGuard outperforms state-of-the-artmethods in both robustness and visual quality. Code is available athttps://github.com/vpsg-research/WaveGuard.</description>
      <author>example@mail.com (Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma)</author>
      <guid isPermaLink="false">2505.08614v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>SAR-GTR: Attributed Scattering Information Guided SAR Graph Transformer Recognition Algorithm</title>
      <link>http://arxiv.org/abs/2505.08547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用电磁散射信息进行SAR数据解释的方法，提出了SAR图变换识别算法（SAR-GTR），通过结合GNN和Transformer机制，有效提升了SAR解释的性能。&lt;h4&gt;背景&lt;/h4&gt;电磁散射信息在SAR解释领域是一个重要的研究方向，而GNN能够有效整合专业知识和先验知识，解决SAR解释中的样本限制和泛化问题。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在深入探究单通道SAR的电磁逆散射信息，并重新审视GNN在SAR解释中的应用限制。&lt;h4&gt;方法&lt;/h4&gt;提出了SAR图变换识别算法（SAR-GTR），该算法通过区分离散和连续参数的映射方法，避免信息混淆和损失。GTR结合了GNN和Transformer机制，并引入边缘信息增强通道，以促进节点和边缘特征的交互式学习，捕捉目标的鲁棒和全局结构特征。此外，GTR通过全局节点编码和边缘位置编码构建了层次拓扑感知系统，充分利用目标的层次结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;SAR-GTR能够有效处理电磁散射参数的属性和特征，并通过边缘信息增强和层次拓扑结构提升SAR解释的性能。&lt;h4&gt;结论&lt;/h4&gt;SAR-GTR算法在ATRNet-STAR大规模车辆数据集上验证了其有效性，为SAR数据解释提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当前，利用电磁散射信息进行SAR数据解释是SAR解释领域的一个重要研究方向。图神经网络（GNN）能够有效地整合领域特定的物理知识和人类先验知识，从而缓解了SAR解释中样本可用性有限和泛化能力差等挑战。在本研究中，我们深入研究了单通道SAR的电磁逆散射信息，并重新审视了将GNN应用于SAR解释的限制。我们提出了SAR图变换识别算法（SAR-GTR）。SAR-GTR通过区分离散和连续参数的映射方法，仔细考虑了不同电磁散射参数的属性和特征，从而避免了信息混淆和损失。此外，GTR结合了GNN和Transformer机制，并引入了边缘信息增强通道，以促进节点和边缘特征的交互式学习，从而能够捕捉目标的鲁棒和全局结构特征。此外，GTR通过全局节点编码和边缘位置编码构建了一个层次拓扑感知系统，充分利用了目标的层次结构信息。最后，通过使用ATRNet-STAR大规模车辆数据集验证了该算法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Utilizing electromagnetic scattering information for SAR data interpretationis currently a prominent research focus in the SAR interpretation domain. GraphNeural Networks (GNNs) can effectively integrate domain-specific physicalknowledge and human prior knowledge, thereby alleviating challenges such aslimited sample availability and poor generalization in SAR interpretation. Inthis study, we thoroughly investigate the electromagnetic inverse scatteringinformation of single-channel SAR and re-examine the limitations of applyingGNNs to SAR interpretation. We propose the SAR Graph Transformer RecognitionAlgorithm (SAR-GTR). SAR-GTR carefully considers the attributes andcharacteristics of different electromagnetic scattering parameters bydistinguishing the mapping methods for discrete and continuous parameters,thereby avoiding information confusion and loss. Furthermore, the GTR combinesGNNs with the Transformer mechanism and introduces an edge informationenhancement channel to facilitate interactive learning of node and edgefeatures, enabling the capture of robust and global structural characteristicsof targets. Additionally, the GTR constructs a hierarchical topology-awaresystem through global node encoding and edge position encoding, fullyexploiting the hierarchical structural information of targets. Finally, theeffectiveness of the algorithm is validated using the ATRNet-STAR large-scalevehicle dataset.</description>
      <author>example@mail.com (Xuying Xiong, Xinyu Zhang, Weidong Jiang, Li Liu, Yongxiang Liu, Tianpeng Liu)</author>
      <guid isPermaLink="false">2505.08547v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation</title>
      <link>http://arxiv.org/abs/2505.08665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SkillFormer，一种用于从自视角和外视角视频中统一多视角技能水平估计的参数高效架构。&lt;h4&gt;背景&lt;/h4&gt;评估复杂活动中的人类技能水平是一个具有应用在体育、康复和培训中的挑战性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的架构，能够从多视角视频中准确评估技能水平。&lt;h4&gt;方法&lt;/h4&gt;SkillFormer基于TimeSformer骨干网络，引入了CrossViewFusion模块，该模块通过多头交叉注意力、可学习门控和自适应自校准融合视图特定特征。此外，利用低秩适应来微调参数，显著降低训练成本。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoExo4D数据集上，SkillFormer在多视角设置中实现了最先进的准确性，同时表现出卓越的计算效率，参数数量比先前基线减少了4.5倍，训练轮数减少了3.75倍。&lt;h4&gt;结论&lt;/h4&gt;SkillFormer在多个结构化任务中表现出色，证实了多视角集成对于精细技能评估的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Assessing human skill levels in complex activities is a challenging problemwith applications in sports, rehabilitation, and training. In this work, wepresent SkillFormer, a parameter-efficient architecture for unified multi-viewproficiency estimation from egocentric and exocentric videos. Building on theTimeSformer backbone, SkillFormer introduces a CrossViewFusion module thatfuses view-specific features using multi-head cross-attention, learnablegating, and adaptive self-calibration. We leverage Low-Rank Adaptation tofine-tune only a small subset of parameters, significantly reducing trainingcosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achievesstate-of-the-art accuracy in multi-view settings while demonstrating remarkablecomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewertraining epochs than prior baselines. It excels in multiple structured tasks,confirming the value of multi-view integration for fine-grained skillassessment.</description>
      <author>example@mail.com (Edoardo Bianchi, Antonio Liotta)</author>
      <guid isPermaLink="false">2505.08665v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.08361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WM3C的强化学习新框架，通过学习利用组合因果组件来提升强化学习在未知环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习在遇到具有未知动态的新环境时，泛化能力仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在通过借鉴人类的组合推理能力，提高强化学习在未知环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;WM3C框架通过识别和利用可组合元素之间的因果动力学，将语言作为一种组合模式，将潜在空间分解为有意义的组件，并使用掩码自动编码器和自适应稀疏正则化来捕获高级语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;WM3C在识别潜在过程、改进策略学习和泛化到未见任务方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;WM3C为强化学习在未知环境中的泛化提供了一种有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalization in reinforcement learning (RL) remains a significantchallenge, especially when agents encounter novel environments with unseendynamics. Drawing inspiration from human compositional reasoning -- where knowncomponents are reconfigured to handle new situations -- we introduce WorldModeling with Compositional Causal Components (WM3C). This novel frameworkenhances RL generalization by learning and leveraging compositional causalcomponents. Unlike previous approaches focusing on invariant representationlearning or meta-learning, WM3C identifies and utilizes causal dynamics amongcomposable elements, facilitating robust adaptation to new tasks. Our approachintegrates language as a compositional modality to decompose the latent spaceinto meaningful components and provides theoretical guarantees for their uniqueidentification under mild assumptions. Our practical implementation uses amasked autoencoder with mutual information constraints and adaptive sparsityregularization to capture high-level semantic information and effectivelydisentangle transition dynamics. Experiments on numerical simulations andreal-world robotic manipulation tasks demonstrate that WM3C significantlyoutperforms existing methods in identifying latent processes, improving policylearning, and generalizing to unseen tasks.</description>
      <author>example@mail.com (Xinyue Wang, Biwei Huang)</author>
      <guid isPermaLink="false">2505.08361v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding</title>
      <link>http://arxiv.org/abs/2505.08194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLTP的语言触觉预训练框架，用于实现触觉感知与视觉-语言模型（VLMs）的整合，以增强机器人多模态感知能力。&lt;h4&gt;背景&lt;/h4&gt;现有的触觉描述主要限于表面属性，如纹理，忽略了机器人操作中关键的接触状态。&lt;h4&gt;目的&lt;/h4&gt;提出CLTP框架，以实现接触状态感知的触觉语言理解，从而提高机器人操作任务的效果。&lt;h4&gt;方法&lt;/h4&gt;收集了包含50k个触觉3D点云-语言对的创新数据集，其中描述从触觉传感器的视角明确捕捉了多维接触状态（如接触位置、形状和力）。CLTP利用预对齐和冻结的视觉-语言特征空间来桥接整体文本和触觉模态。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了CLTP在零样本3D分类、接触状态分类和触觉3D大型语言模型（LLM）交互等三个下游任务中的优越性。&lt;h4&gt;结论&lt;/h4&gt;CLTP是第一个从接触状态的角度对触觉和语言表示进行对齐的研究，为触觉-语言-动作模型学习提供了巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;近期将触觉感知与视觉-语言模型（VLMs）整合的研究取得了显著进展，展示了在机器人多模态感知方面的巨大潜力。然而，现有的触觉描述仅限于表面属性，如纹理，忽略了机器人操作中至关重要的接触状态。为了弥合这一差距，我们提出了CLTP，一个直观且有效的语言触觉预训练框架，它将触觉3D点云与自然语言对齐于各种接触场景中，从而实现接触状态感知的触觉语言理解，为富含接触操作的机器人任务提供支持。我们首先收集了一个包含50k个触觉3D点云-语言对的创新数据集，其中描述从触觉传感器的视角明确捕捉了多维接触状态（例如，接触位置、形状和力）。CLTP利用预对齐和冻结的视觉-语言特征空间来桥接整体的文本和触觉模态。实验验证了它在三个下游任务中的优越性：零样本3D分类、接触状态分类和触觉3D大型语言模型（LLM）交互。据我们所知，这是第一个从接触状态的角度对触觉和语言表示进行对齐的研究，为触觉-语言-动作模型学习提供了巨大潜力。代码和数据集已开源，网址为https://sites.google.com/view/cltp/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in integrating tactile sensing with vision-languagemodels (VLMs) have demonstrated remarkable potential for robotic multimodalperception. However, existing tactile descriptions remain limited tosuperficial attributes like texture, neglecting critical contact statesessential for robotic manipulation. To bridge this gap, we propose CLTP, anintuitive and effective language tactile pretraining framework that alignstactile 3D point clouds with natural language in various contact scenarios,thus enabling contact-state-aware tactile language understanding forcontact-rich manipulation tasks. We first collect a novel dataset of 50k+tactile 3D point cloud-language pairs, where descriptions explicitly capturemultidimensional contact states (e.g., contact location, shape, and force) fromthe tactile sensor's perspective. CLTP leverages a pre-aligned and frozenvision-language feature space to bridge holistic textual and tactilemodalities. Experiments validate its superiority in three downstream tasks:zero-shot 3D classification, contact state classification, and tactile 3D largelanguage model (LLM) interaction. To the best of our knowledge, this is thefirst study to align tactile and language representations from the contactstate perspective for manipulation tasks, providing great potential fortactile-language-action model learning. Code and datasets are open-sourced athttps://sites.google.com/view/cltp/.</description>
      <author>example@mail.com (Wenxuan Ma, Xiaoge Cao, Yixiang Zhang, Chaofan Zhang, Shaobo Yang, Peng Hao, Bin Fang, Yinghao Cai, Shaowei Cui, Shuo Wang)</author>
      <guid isPermaLink="false">2505.08194v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI</title>
      <link>http://arxiv.org/abs/2505.08430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的邻近上下文聚合框架（GNCAF），用于对全切片图像（WSI）中的三级淋巴结构（TLS）进行语义分割，以改善TLS成熟度和面积的量化，从而提高预后任务的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的TLS评估方法通常依赖于细胞代理任务，并需要额外的后处理步骤。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的TLS语义分割任务（TLS-SS），以端到端的方式在WSI中分割TLS的区域和成熟阶段。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于GNN的邻近上下文聚合框架（GNCAF），该框架通过逐步聚合目标及其邻近区域的多跳上下文信息，并使用自注意力机制来指导目标区域的分割。&lt;h4&gt;主要发现&lt;/h4&gt;GNCAF能够与各种分割模型集成，增强其感知超出局部区域上下文信息的能力。在TCGA-COAD和INHOUSE-PAAD数据集上的实验表明，GNCAF在mF1和mIoU方面分别实现了22.08%和26.57%的最大提升。此外，GNCAF在淋巴结转移分割任务中也验证了其任务的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;GNCAF是一种有效的TLS语义分割方法，能够显著提高TLS分割的准确性，并具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tertiary lymphoid structures (TLS) are organized clusters of immune cells,whose maturity and area can be quantified in whole slide image (WSI) forvarious prognostic tasks. Existing methods for assessing these characteristicstypically rely on cell proxy tasks and require additional post-processingsteps. In this work, We focus on a novel task-TLS Semantic Segmentation(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI inan end-to-end manner. Due to the extensive scale of WSI and patch-basedsegmentation strategies, TLS-SS necessitates integrating from neighboringpatches to guide target patch (target) segmentation. Previous techniques oftenemploy on multi-resolution approaches, constraining the capacity to leveragethe broader neighboring context while tend to preserve coarse-grainedinformation. To address this, we propose a GNN-based Neighboring ContextAggregation Framework (GNCAF), which progressively aggregates multi-hopneighboring context from the target and employs a self-attention mechanism toguide the segmentation of the target. GNCAF can be integrated with varioussegmentation models to enhance their ability to perceive contextual informationoutside of the patch. We build two TLS-SS datasets, called TCGA-COAD andINHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publiclyavailable. Experiments on these datasets demonstrate the superiority of GNCAF,achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,respectively. Additionally, we also validate the task scalability of GNCAF onsegmentation of lymph node metastases.</description>
      <author>example@mail.com (Lei Su)</author>
      <guid isPermaLink="false">2505.08430v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing</title>
      <link>http://arxiv.org/abs/2505.08101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的蒸馏框架，用于在资源受限环境中部署高性能模型，如Point Transformer V3，通过拓扑感知表示和梯度引导的知识蒸馏，有效地将知识从高容量教师模型转移到轻量级学生模型。&lt;h4&gt;背景&lt;/h4&gt;点云处理在自动驾驶和3D物体识别等应用中扮演着关键角色，但将高性能模型部署在资源受限环境中由于计算和内存需求高而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的蒸馏框架，以在资源受限环境中部署高性能点云处理模型。&lt;h4&gt;方法&lt;/h4&gt;该方法利用拓扑感知表示和梯度引导的知识蒸馏，同时捕捉点云的底层几何结构，并通过基于梯度的特征对齐指导学生模型的训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在Nuscenes、SemanticKITTI和Waymo数据集上的实验结果表明，该方法在模型大小减少约16倍和推理时间减少近1.9倍的同时，达到了与教师模型相当的性能。在NuScenes数据集上，该方法在仅基于LiDAR数据进行训练的知识蒸馏技术中取得了最先进的性能，超越了先前知识蒸馏基线在分割性能上的表现。&lt;h4&gt;结论&lt;/h4&gt;提出的蒸馏框架在保持高性能的同时，显著降低了模型的计算和内存需求，适用于资源受限的环境。&lt;h4&gt;翻译&lt;/h4&gt;点云处理由于在自动驾驶和3D物体识别等应用中的关键作用而受到广泛关注。然而，由于高性能模型如Point Transformer V3的计算和内存需求高，在资源受限的环境中部署这些模型仍然具有挑战性。本研究引入了一种新颖的蒸馏框架，该框架利用拓扑感知表示和梯度引导的知识蒸馏，有效地将知识从高容量教师模型传递到轻量级学生模型。我们的方法捕捉了点云的底层几何结构，并通过基于梯度的特征对齐有选择地指导学生模型的训练过程。在Nuscenes、SemanticKITTI和Waymo数据集上的实验结果表明，所提出的方法在模型大小减少约16倍和推理时间减少近1.9倍的同时，达到了与教师模型相当的性能。值得注意的是，在NuScenes数据集上，我们的方法在仅基于LiDAR数据进行训练的知识蒸馏技术中取得了最先进的性能，超越了先前知识蒸馏基线在分割性能上的表现。我们的实现代码已公开，可在https://github.com/HySonLab/PointDistill上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hysonlab/pointdistill&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud processing has gained significant attention due to its criticalrole in applications such as autonomous driving and 3D object recognition.However, deploying high-performance models like Point Transformer V3 inresource-constrained environments remains challenging due to their highcomputational and memory demands. This work introduces a novel distillationframework that leverages topology-aware representations and gradient-guidedknowledge distillation to effectively transfer knowledge from a high-capacityteacher to a lightweight student model. Our approach captures the underlyinggeometric structures of point clouds while selectively guiding the studentmodel's learning process through gradient-based feature alignment. Experimentalresults in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that theproposed method achieves competitive performance, with an approximately 16xreduction in model size and a nearly 1.9x decrease in inference time comparedto its teacher model. Notably, on NuScenes, our method achievesstate-of-the-art performance among knowledge distillation techniques trainedsolely on LiDAR data, surpassing prior knowledge distillation baselines insegmentation performance. Our implementation is available publicly at:  https://github.com/HySonLab/PointDistill</description>
      <author>example@mail.com (Luu Tung Hai, Thinh D. Le, Zhicheng Ding, Qing Tian, Truong-Son Hy)</author>
      <guid isPermaLink="false">2505.08101v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</title>
      <link>http://arxiv.org/abs/2505.08552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了生成式AI工具在视觉内容创作中的应用，特别是视觉艺术品领域，以及由此引发的版权侵权和伪造问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，生成式AI工具在视觉内容创作中的应用日益增多，尤其是用于视觉艺术品的创作，这引起了关于版权侵权和伪造的严重担忧。&lt;h4&gt;目的&lt;/h4&gt;提出了一种名为DFA-CON的对比学习框架，旨在检测侵犯版权或伪造的AI生成艺术。&lt;h4&gt;方法&lt;/h4&gt;该框架学习一个具有判别性的表征空间，在对比学习框架中，对原创艺术品及其伪造品之间的亲和力进行建模。模型在多种攻击类型下进行训练，包括修复、风格迁移、对抗性扰动和cutmix。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，该模型在大多数攻击类型上表现出稳健的检测性能，优于最近的预训练基础模型。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一个有效的方法来检测AI生成的艺术作品中的版权侵权问题，并将相关代码和模型检查点公开。&lt;h4&gt;翻译&lt;/h4&gt;The paper discusses the application of generative AI tools in visual content creation, especially in the context of visual art, and the serious concerns about copyright infringement and forgery that this has raised. In recent years, the application of generative AI tools in visual content creation has increased, especially in the creation of visual art, which has raised serious concerns about copyright infringement and forgery. This paper proposes a contrastive learning framework named DFA-CON, designed to detect copyright-infringing or forged AI-generated art. The framework learns a discriminative representation space, modeling affinity between original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. The code and model checkpoints will be publicly released upon acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent proliferation of generative AI tools for visual contentcreation-particularly in the context of visual artworks-has raised seriousconcerns about copyright infringement and forgery. The large-scale datasetsused to train these models often contain a mixture of copyrighted andnon-copyrighted artworks. Given the tendency of generative models to memorizetraining patterns, they are susceptible to varying degrees of copyrightviolation. Building on the recently proposed DeepfakeArt Challenge benchmark,this work introduces DFA-CON, a contrastive learning framework designed todetect copyright-infringing or forged AI-generated art. DFA-CON learns adiscriminative representation space, posing affinity among original artworksand their forged counterparts within a contrastive learning framework. Themodel is trained across multiple attack types, including inpainting, styletransfer, adversarial perturbation, and cutmix. Evaluation results demonstraterobust detection performance across most attack types, outperforming recentpretrained foundation models. Code and model checkpoints will be releasedpublicly upon acceptance.</description>
      <author>example@mail.com (Haroon Wahab, Hassan Ugail, Irfan Mehmood)</author>
      <guid isPermaLink="false">2505.08552v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Revealing economic facts: LLMs know more than they say</title>
      <link>http://arxiv.org/abs/2505.08662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型（LLMs）的隐藏状态是否可以用于估计和推断经济和金融统计数据。&lt;h4&gt;背景&lt;/h4&gt;研究聚焦于县级行政区和公司层面的经济和财务变量，如失业率和总资产。&lt;h4&gt;目的&lt;/h4&gt;探究隐藏状态是否比LLMs的直接响应包含更丰富的经济信息。&lt;h4&gt;方法&lt;/h4&gt;使用简单线性模型对开源LLMs的隐藏状态进行训练，并与模型的文本输出进行比较。此外，还提出了一个迁移学习方法，该方法在不要求目标变量有标记数据的情况下提高估计精度。&lt;h4&gt;主要发现&lt;/h4&gt;隐藏状态模型在估计经济和金融统计数据方面优于模型的文本输出，且仅需少量标记样本即可进行训练。&lt;h4&gt;结论&lt;/h4&gt;隐藏状态表示在超分辨率和数据推断任务中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了大型语言模型的隐藏状态是否可以用来估计和推断经济和金融统计数据。我们专注于县级行政区（例如失业率）和公司层面（例如总资产）的变量，我们发现基于开源LLMs隐藏状态的简单线性模型优于模型的文本输出。这表明隐藏状态比LLMs直接揭示的响应包含更丰富的经济信息。学习曲线分析表明，仅需要几十个标记样本就足以进行训练。我们还提出了一种迁移学习方法，该方法在不要求目标变量有标记数据的情况下提高了估计精度。最后，我们展示了隐藏状态表示在超分辨率和数据推断任务中的实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate whether the hidden states of large language models (LLMs) canbe used to estimate and impute economic and financial statistics. Focusing oncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,we show that a simple linear model trained on the hidden states of open-sourceLLMs outperforms the models' text outputs. This suggests that hidden statescapture richer economic information than the responses of the LLMs revealdirectly. A learning curve analysis indicates that only a few dozen labelledexamples are sufficient for training. We also propose a transfer learningmethod that improves estimation accuracy without requiring any labelled datafor the target variable. Finally, we demonstrate the practical utility ofhidden-state representations in super-resolution and data imputation tasks.</description>
      <author>example@mail.com (Marcus Buckmann, Quynh Anh Nguyen, Edward Hill)</author>
      <guid isPermaLink="false">2505.08662v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification</title>
      <link>http://arxiv.org/abs/2505.08265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了使用大型语言模型（LLMs）作为特征增强器优化节点表示，并将其作为图神经网络（GNNs）输入的潜力，并基于互换干预方法进行了深入研究。&lt;h4&gt;背景&lt;/h4&gt;LLMs在图表示学习中的应用展现出巨大潜力，但其基本性质尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;通过更深入的分析，探究LLMs增强器和GNNs的深层性质及其内部机制。&lt;h4&gt;方法&lt;/h4&gt;构建了一个具有可控因果关系的合成图数据集，并使用互换干预方法进行分析。基于分析结果，设计了一个即插即用的优化模块。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了LLMs增强器和GNNs的内在逻辑和内部机制，并验证了优化模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效提高LLMs增强器和GNNs之间的信息传递效率。&lt;h4&gt;翻译&lt;/h4&gt;The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of large language models (LLMs) as feature enhancers to optimize noderepresentations, which are then used as inputs for graph neural networks(GNNs), has shown significant potential in graph representation learning.However, the fundamental properties of this approach remain underexplored. Toaddress this issue, we propose conducting a more in-depth analysis of thisissue based on the interchange intervention method. First, we construct asynthetic graph dataset with controllable causal relationships, enablingprecise manipulation of semantic relationships and causal modeling to providedata for analysis. Using this dataset, we conduct interchange interventions toexamine the deeper properties of LLM enhancers and GNNs, uncovering theirunderlying logic and internal mechanisms. Building on the analytical results,we design a plug-and-play optimization module to improve the informationtransfer between LLM enhancers and GNNs. Experiments across multiple datasetsand models validate the proposed module.</description>
      <author>example@mail.com (Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.08265v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models</title>
      <link>http://arxiv.org/abs/2505.08455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Video-based long-form Causal Reasoning (VCRBench)这一新的基准，用于评估大型视频语言模型（LVLMs）在视频因果推理方面的能力，并提出了一种名为Recognition-Reasoning Decomposition (RRD)的模块化方法来提高LVLMs在视频因果推理任务上的准确率。&lt;h4&gt;背景&lt;/h4&gt;尽管视频理解取得了进展，但LVLMs在视频因果推理方面的能力尚未得到充分探索，主要原因是缺乏相关和专门的基准来评估在视觉基础和目标驱动环境中的因果推理。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文旨在提出一个名为VCRBench的新基准，用于评估LVLMs在视频因果推理方面的能力。&lt;h4&gt;方法&lt;/h4&gt;VCRBench使用日常活动的程序视频创建，其中步骤被故意打乱，每个视频片段捕捉一个关键因果事件，以测试LVLMs是否能够识别、推理和正确排序实现特定目标所需的事件。此外，该基准被精心设计，以防止LVLMs利用语言捷径，同时避免开放式问答评估的挑战。作者还提出了RRD方法，将视频因果推理分解为视频识别和因果推理两个子任务。&lt;h4&gt;主要发现&lt;/h4&gt;在VCRBench上的评估表明，最先进的LVLMs在视频因果推理方面存在困难，主要因为它们难以直接从视觉观察中建模长距离因果依赖关系。RRD方法显著提高了VCRBench上的准确率，最高提升达25.2%。&lt;h4&gt;结论&lt;/h4&gt;LVLMs主要依赖于语言知识来完成复杂的视频因果推理任务。&lt;h4&gt;翻译&lt;/h4&gt;Despite recent advances in video understanding, the capabilities of LargeVideo Language Models (LVLMs) to perform video-based causal reasoning remainsunderexplored, largely due to the absence of relevant and dedicated benchmarksfor evaluating causal reasoning in visually grounded and goal-driven settings.To fill this gap, we introduce a novel benchmark named Video-based long-formCausal Reasoning (VCRBench). We create VCRBench using procedural videos ofsimple everyday activities, where the steps are deliberately shuffled with eachclip capturing a key causal event, to test whether LVLMs can identify, reasonabout, and correctly sequence the events needed to accomplish a specific goal.Moreover, the benchmark is carefully designed to prevent LVLMs from exploitinglinguistic shortcuts, as seen in multiple-choice or binary QA formats, whilealso avoiding the challenges associated with evaluating open-ended QA. Ourevaluation of state-of-the-art LVLMs on VCRBench suggests that these modelsstruggle with video-based long-form causal reasoning, primarily due to theirdifficulty in modeling long-range causal dependencies directly from visualobservations. As a simple step toward enabling such capabilities, we proposeRecognition-Reasoning Decomposition (RRD), a modular approach that breaksvideo-based causal reasoning into two sub-tasks of video recognition and causalreasoning. Our experiments on VCRBench show that RRD significantly boostsaccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysisreveals interesting insights, for instance, that LVLMs primarily rely onlanguage knowledge for complex video-based long-form causal reasoning tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pritamqu/vcrbench&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in video understanding, the capabilities of LargeVideo Language Models (LVLMs) to perform video-based causal reasoning remainsunderexplored, largely due to the absence of relevant and dedicated benchmarksfor evaluating causal reasoning in visually grounded and goal-driven settings.To fill this gap, we introduce a novel benchmark named Video-based long-formCausal Reasoning (VCRBench). We create VCRBench using procedural videos ofsimple everyday activities, where the steps are deliberately shuffled with eachclip capturing a key causal event, to test whether LVLMs can identify, reasonabout, and correctly sequence the events needed to accomplish a specific goal.Moreover, the benchmark is carefully designed to prevent LVLMs from exploitinglinguistic shortcuts, as seen in multiple-choice or binary QA formats, whilealso avoiding the challenges associated with evaluating open-ended QA. Ourevaluation of state-of-the-art LVLMs on VCRBench suggests that these modelsstruggle with video-based long-form causal reasoning, primarily due to theirdifficulty in modeling long-range causal dependencies directly from visualobservations. As a simple step toward enabling such capabilities, we proposeRecognition-Reasoning Decomposition (RRD), a modular approach that breaksvideo-based causal reasoning into two sub-tasks of video recognition and causalreasoning. Our experiments on VCRBench show that RRD significantly boostsaccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysisreveals interesting insights, for instance, that LVLMs primarily rely onlanguage knowledge for complex video-based long-form causal reasoning tasks.</description>
      <author>example@mail.com (Pritam Sarkar, Ali Etemad)</author>
      <guid isPermaLink="false">2505.08455v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A computer vision-based model for occupancy detection using low-resolution thermal images</title>
      <link>http://arxiv.org/abs/2505.08336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用低分辨率热图像和计算机视觉技术进行人员占用检测，以提高HVAC系统的能效和操作。&lt;h4&gt;背景&lt;/h4&gt;传统的HVAC系统通常不考虑占用情况，而先进的以用户为中心的控制（OCC）系统则考虑了占用状态。然而，使用RGB图像和计算机视觉技术进行占用检测存在隐私问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个利用低分辨率热图像和计算机视觉技术的人员占用检测模型，以解决隐私问题并降低计算资源需求。&lt;h4&gt;方法&lt;/h4&gt;使用转移学习技术微调YOLOv5模型，以实现基于低分辨率热图像的占用检测。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在占用检测方面取得了满意的性能，精确度、召回率、mAP50和mAP50值接近1.000。&lt;h4&gt;结论&lt;/h4&gt;该模型不仅解决了隐私问题，还降低了计算资源的需求，对HVAC系统的能效和操作有积极影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occupancy plays an essential role in influencing the energy consumption andoperation of heating, ventilation, and air conditioning (HVAC) systems.Traditional HVAC typically operate on fixed schedules without consideringoccupancy. Advanced occupant-centric control (OCC) adopted occupancy status inregulating HVAC operations. RGB images combined with computer vision (CV)techniques are widely used for occupancy detection, however, the detailedfacial and body features they capture raise significant privacy concerns.Low-resolution thermal images offer a non-invasive solution that mitigatesprivacy issues. The study developed an occupancy detection model utilizinglow-resolution thermal images and CV techniques, where transfer learning wasapplied to fine-tune the You Only Look Once version 5 (YOLOv5) model. Thedeveloped model ultimately achieved satisfactory performance, with precision,recall, mAP50, and mAP50 values approaching 1.000. The contributions of thismodel lie not only in mitigating privacy concerns but also in reducingcomputing resource demands.</description>
      <author>example@mail.com (Xue Cui, Vincent Gbouna Zakka, Minhyun Lee)</author>
      <guid isPermaLink="false">2505.08336v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
      <link>http://arxiv.org/abs/2505.08316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for full publication at CogSci 2025  (https://cognitivesciencesociety.org/cogsci-2025/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究VVS（腹侧视觉通路）的功能，提出了一种结合相对位置预测（RP）学习与对比学习的新的无监督任务驱动方法来建模VVS，并证明了VVS在计算层面与位置感知（尤其是RP预测）有关。&lt;h4&gt;背景&lt;/h4&gt;现有的无监督任务驱动方法通过对比学习建模VVS，主要关注物体识别，但作者认为VVS的功能不仅仅局限于物体识别。&lt;h4&gt;目的&lt;/h4&gt;引入相对位置预测作为VVS的附加功能，并设计一种新的无监督任务驱动方法来建模VVS。&lt;h4&gt;方法&lt;/h4&gt;理论上解释了对比学习可能无法实现RP预测的能力，并提出将RP学习与对比学习结合的新方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在提高物体识别性能的同时增强了RP预测能力，且RP预测能力普遍提高了模型的大脑相似度。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明VVS在位置感知中起着重要作用，特别是在RP预测方面。&lt;h4&gt;翻译&lt;/h4&gt;Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rdz98/unsup-vvs&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on the concept that ventral visual stream (VVS) mainly functions forobject recognition, current unsupervised task-driven methods model VVS bycontrastive learning, and have achieved good brain similarity. However, webelieve functions of VVS extend beyond just object recognition. In this paper,we introduce an additional function involving VVS, named relative position (RP)prediction. We first theoretically explain contrastive learning may be unableto yield the model capability of RP prediction. Motivated by this, wesubsequently integrate RP learning with contrastive learning, and propose a newunsupervised task-driven method to model VVS, which is more inline withbiological reality. We conduct extensive experiments, demonstrating that: (i)our method significantly improves downstream performance of object recognitionwhile enhancing RP predictivity; (ii) RP predictivity generally improves themodel brain similarity. Our results provide strong evidence for the involvementof VVS in location perception (especially RP prediction) from a computationalperspective.</description>
      <author>example@mail.com (Dazhong Rong, Hao Dong, Xing Gao, Jiyu Wei, Di Hong, Yaoyao Hao, Qinming He, Yueming Wang)</author>
      <guid isPermaLink="false">2505.08316v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People</title>
      <link>http://arxiv.org/abs/2505.08215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了影响语音清晰度预测性能的关键设计因素，并提出了针对有听力障碍人群的语音清晰度预测方法。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型（SFMs）在多种下游任务中表现出色，但针对有听力障碍人群的语音清晰度预测（SIP-HI）的优化研究不足。&lt;h4&gt;目的&lt;/h4&gt;通过研究，确定影响SIP-HI性能的关键设计因素，并提出有效的语音清晰度预测方法。&lt;h4&gt;方法&lt;/h4&gt;对5个SFMs进行综合研究，关注编码器层选择、预测头架构和集成配置，并探讨关键SFM属性与其对SIP-HI性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;与传统使用所有层的方法不同，选择单个编码器层可以获得更好的结果。此外，时间建模对于有效的预测头至关重要。集成多个SFMs可以提高性能，更强个体的模型提供更大的益处。&lt;h4&gt;结论&lt;/h4&gt;研究为有效适应SFMs进行有听力障碍人群的语音清晰度预测提供了实践见解。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型（SFMs）在众多下游任务中表现出色，包括为听力受损人群进行语音清晰度预测（SIP-HI）。然而，针对SIP-HI对SFMs进行优化的研究尚不充分。在本文中，我们进行了一项全面的研究，以确定影响SIP-HI性能的关键设计因素，使用5个SFMs，重点关注编码器层选择、预测头架构和集成配置。我们的研究结果表明，与传统使用所有层的方法相反，选择单个编码器层可以获得更好的结果。此外，时间建模对于有效的预测头至关重要。我们还证明了集成多个SFMs可以提高性能，更强的个体模型提供更大的益处。最后，我们探讨了关键SFM属性及其对SIP-HI性能影响之间的关系。我们的研究为有效调整SFMs以进行有听力障碍人群的语音清晰度预测提供了实践见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models (SFMs) have demonstrated strong performance across avariety of downstream tasks, including speech intelligibility prediction forhearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has beeninsufficiently explored. In this paper, we conduct a comprehensive study toidentify key design factors affecting SIP-HI performance with 5 SFMs, focusingon encoder layer selection, prediction head architecture, and ensembleconfigurations. Our findings show that, contrary to traditional use-all-layersmethods, selecting a single encoder layer yields better results. Additionally,temporal modeling is crucial for effective prediction heads. We alsodemonstrate that ensembling multiple SFMs improves performance, with strongerindividual models providing greater benefit. Finally, we explore therelationship between key SFM attributes and their impact on SIP-HI performance.Our study offers practical insights into effectively adapting SFMs for speechintelligibility prediction for hearing-impaired populations.</description>
      <author>example@mail.com (Haoshuai Zhou, Boxuan Cao, Changgeng Mo, Linkai Li, Shan Xiang Wang)</author>
      <guid isPermaLink="false">2505.08215v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing</title>
      <link>http://arxiv.org/abs/2505.08302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Full version of the paper will be appearing at the Proceedings of the  Thirty-Third International Joint Conference on Artificial Intelligence  (IJCAI-25), Special Track on AI for Good&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为KIIM的新型灌溉方法映射方法，通过利用Swin-Transformer模型和多种信息处理技术，实现了更准确和高效的灌溉方法映射。&lt;h4&gt;背景&lt;/h4&gt;现有的灌溉方法映射模型依赖于卫星图像的光谱特征，但由于农业景观的复杂性和有限的训练数据，这些模型效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的灌溉方法映射方法，以实现可持续农业实践和食物系统的精准灌溉。&lt;h4&gt;方法&lt;/h4&gt;KIIM方法利用了以下技术：(i) 特殊的投影矩阵来编码作物到灌溉概率；(ii) 空间注意力图来识别农业用地；(iii) 双向交叉注意力来关注不同模态的互补信息；(iv) 加权集成来结合图像和作物信息的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在五个美国州进行的实验中，KIIM方法相对于基线模型提高了22.9%的IoU（交并比），对于难以分类的滴灌，IoU提高了71.4%。此外，通过两阶段迁移学习方法，在数据有限的州中，IoU提升了51%。&lt;h4&gt;结论&lt;/h4&gt;KIIM方法通过仅使用40%的训练数据即可实现基线性能，提高了灌溉方法映射的效率和可行性，减少了大量手动标记的需求，使大规模自动化灌溉映射更加经济高效。&lt;h4&gt;翻译&lt;/h4&gt;Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate mapping of irrigation methods is crucial for sustainableagricultural practices and food systems. However, existing models that relysolely on spectral features from satellite imagery are ineffective due to thecomplexity of agricultural landscapes and limited training data, making this achallenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), anovel Swin-Transformer based approach that uses (i) a specialized projectionmatrix to encode crop to irrigation probability, (ii) a spatial attention mapto identify agricultural lands from non-agricultural lands, (iii)bi-directional cross-attention to focus complementary information fromdifferent modalities, and (iv) a weighted ensemble for combining predictionsfrom images and crop information. Our experimentation on five states in the USshows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU)improvement for hard-to-classify drip irrigation. In addition, we propose atwo-phase transfer learning approach to enhance cross-state irrigation mapping,achieving a 51% IoU boost in a state with limited labeled data. The ability toachieve baseline performance with only 40% of the training data highlights itsefficiency, reducing the dependency on extensive manual labeling efforts andmaking large-scale, automated irrigation mapping more feasible andcost-effective.</description>
      <author>example@mail.com (Oishee Bintey Hoque, Nibir Chandra Mandal, Abhijin Adiga, Samarth Swarup, Sayjro Kossi Nouwakpo, Amanda Wilson, Madhav Marathe)</author>
      <guid isPermaLink="false">2505.08302v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2505.07396v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了城市数字孪生（UDTs）在城市管理中的重要性，并提出了一个综合的多模态城市数字孪生基准数据集TUM2TWIN，以解决城市数字孪生创建过程中的挑战。&lt;h4&gt;背景&lt;/h4&gt;城市数字孪生在城市管理中扮演着关键角色，但创建过程中存在多个阶段的挑战，如获取精确的3D数据、重建高保真模型、维护模型更新和确保数据兼容性。&lt;h4&gt;目的&lt;/h4&gt;旨在解决城市数字孪生创建中的挑战，并推动相关研究和实际应用。&lt;h4&gt;方法&lt;/h4&gt;提出了TUM2TWIN数据集，该数据集包含地理参照的、语义对齐的3D模型和网络，以及多种地面、移动、空中和卫星观测数据，支持对传感器进行稳健分析和发展先进的重建方法。&lt;h4&gt;主要发现&lt;/h4&gt;TUM2TWIN数据集支持新型视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建等下游任务，展示了其在城市数字孪生创建中的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;TUM2TWIN数据集为克服现有城市数字孪生创建的局限性奠定了基础，并为更智能、数据驱动的城市环境的研究和实际解决方案提供了支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：城市数字孪生（UDTs）已成为城市管理不可或缺的工具，并能够整合来自不同来源的复杂、异构数据。创建城市数字孪生涉及多个过程阶段的挑战，包括获取精确的3D源数据、重建高保真3D模型、维护模型更新以及确保下游任务的无缝互操作性。当前的数据库通常仅限于处理链的一部分，阻碍了综合城市数字孪生的验证。为了解决这些挑战，我们引入了第一个综合的多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，拥有32个数据子集，覆盖约100,000平方米，目前数据量为767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，该基准支持对传感器进行稳健分析和发展先进的重建方法。此外，我们还探讨了下游任务，展示了TUM2TWIN的潜力，包括NeRF和高斯喷溅的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。我们相信这一贡献为克服现有城市数字孪生创建的局限性，促进新的研究方向和实践解决方案奠定了基础。项目可在以下网址获取：https://tum2t.win&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Digital Twins (UDTs) have become essential for managing cities andintegrating complex, heterogeneous data from diverse sources. Creating UDTsinvolves challenges at multiple process stages, including acquiring accurate 3Dsource data, reconstructing high-fidelity 3D models, maintaining models'updates, and ensuring seamless interoperability to downstream tasks. Currentdatasets are usually limited to one part of the processing chain, hamperingcomprehensive UDTs validation. To address these challenges, we introduce thefirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.This dataset includes georeferenced, semantically aligned 3D models andnetworks along with various terrestrial, mobile, aerial, and satelliteobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, highaccuracy, and multimodal data integration, the benchmark supports robustanalysis of sensors and the development of advanced reconstruction methods.Additionally, we explore downstream tasks demonstrating the potential ofTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solarpotential analysis, point cloud semantic segmentation, and LoD3 buildingreconstruction. We are convinced this contribution lays a foundation forovercoming current limitations in UDT creation, fostering new researchdirections and practical solutions for smarter, data-driven urban environments.The project is available under: https://tum2t.win</description>
      <author>example@mail.com (Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi)</author>
      <guid isPermaLink="false">2505.07396v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors</title>
      <link>http://arxiv.org/abs/2505.08111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference publication submitted to IEEE I2MTC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用基于床的压力敏感垫（PSM）监测睡眠中的患者睡眠姿势，并使用深度学习模型进行睡眠姿势分类。&lt;h4&gt;背景&lt;/h4&gt;睡眠姿势对睡眠质量和睡眠障碍（如呼吸暂停）的发生率有影响。&lt;h4&gt;目的&lt;/h4&gt;通过使用深度学习模型，准确估计睡眠姿势，克服临床环境中训练深度学习模型所需的大量标记数据的挑战。&lt;h4&gt;方法&lt;/h4&gt;在睡眠诊所的床上放置PSM收集数据，使用迁移学习来适应预训练的深度学习模型，包括Vision Transformer模型（ViTMAE）和预训练的人体姿态估计模型（ViTPose），并在低分辨率PSM数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;该方法优于基于PSM的睡眠姿势分类的先前工作，包括使用深度学习（TCN）和传统机器学习模型（SVM、XGBoost、随机森林）的方法，并在112个夜晚的患者记录和13个患者的更高分辨率数据集上进行了评估和验证。&lt;h4&gt;结论&lt;/h4&gt;尽管从低分辨率PSM数据中区分睡眠姿势存在挑战，但该方法在临床环境中的实际部署显示出希望。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way ofmonitoring patients during sleep. We focus on four-way sleep positionclassification using data collected from a PSM placed under a mattress in asleep clinic. Sleep positions can affect sleep quality and the prevalence ofsleep disorders, such as apnea. Measurements were performed on patients withsuspected sleep disorders referred for assessments at a sleep clinic. Trainingdeep learning models can be challenging in clinical settings due to the needfor large amounts of labeled data. To overcome the shortage of labeled trainingdata, we utilize transfer learning to adapt pre-trained deep learning models toaccurately estimate sleep positions from a low-resolution PSM dataset collectedin a polysomnography sleep lab. Our approach leverages Vision Transformermodels pre-trained on ImageNet using masked autoencoding (ViTMAE) and apre-trained model for human pose estimation (ViTPose). These approachesoutperform previous work from PSM-based sleep pose classification using deeplearning (TCN) as well as traditional machine learning models (SVM, XGBoost,Random Forest) that use engineered features. We evaluate the performance ofsleep position classification from 112 nights of patient recordings andvalidate it on a higher resolution 13-patient dataset. Despite the challengesof differentiating between sleep positions from low-resolution PSM data, ourapproach shows promise for real-world deployment in clinical settings</description>
      <author>example@mail.com (Olivier Papillon, Rafik Goubran, James Green, Julien Larivière-Chartier, Caitlin Higginson, Frank Knoefel, Rébecca Robillard)</author>
      <guid isPermaLink="false">2505.08111v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series</title>
      <link>http://arxiv.org/abs/2505.08723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TiMo是一种针对卫星图像时间序列分析的新型分层视觉Transformer基础模型，通过引入时空陀螺仪注意力机制，能够动态捕捉时空变化的多尺度模式，并在多个时空任务中展现出优越性。&lt;h4&gt;背景&lt;/h4&gt;现有的时空基础模型依赖于平面视觉Transformer，无法显式捕捉土地物体之间的多尺度时空关系，限制了其在下游任务中的有效性。&lt;h4&gt;目的&lt;/h4&gt;提出TiMo模型，以克服现有模型的局限性，提高时空基础模型在环境管理和灾害评估等应用中的效果。&lt;h4&gt;方法&lt;/h4&gt;引入时空陀螺仪注意力机制，动态捕捉时空变化的多尺度模式；构建MillionST数据集，包含100万张图像，涵盖五年内的10个时间相和100,000个地理位置；利用Masked Image Modeling对TiMo进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;TiMo在森林砍伐监测、土地覆盖分割、作物类型分类和洪水检测等时空任务中，表现优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;TiMo模型能够有效学习并编码可推广的时空表示，适用于多种时空分析任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要：卫星图像时间序列（SITS）提供了对地球表面的连续观测，对于环境管理和灾害评估等应用至关重要。然而，现有的时空基础模型依赖于平面视觉Transformer，无法显式捕捉土地物体之间的多尺度时空关系，这限制了它们在下游任务中的有效性。为了克服这一挑战，我们提出了TiMo，一种针对SITS分析的分层视觉Transformer基础模型。在核心部分，我们引入了一种时空陀螺仪注意力机制，能够动态地捕捉时间和空间上的演变多尺度模式。为了预训练，我们精心制作了MillionST数据集，包含从100,000个地理位置捕获的100万张图像，这些图像在五年内的10个时间相中捕捉到了多样化的地理空间变化和季节性变化。利用这个数据集，我们将掩码图像建模应用于预训练TiMo，使其能够有效地学习并编码可推广的时空表示。在多个时空任务（包括森林砍伐监测、土地覆盖分割、作物类型分类和洪水检测）上的广泛实验表明，TiMo在性能上优于最先进的方法。代码、模型和数据集将在https://github.com/MiliLab/TiMo发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mililab/timo&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Satellite image time series (SITS) provide continuous observations of theEarth's surface, making them essential for applications such as environmentalmanagement and disaster assessment. However, existing spatiotemporal foundationmodels rely on plain vision transformers, which encode entire temporalsequences without explicitly capturing multiscale spatiotemporal relationshipsbetween land objects. This limitation hinders their effectiveness in downstreamtasks. To overcome this challenge, we propose TiMo, a novel hierarchical visiontransformer foundation model tailored for SITS analysis. At its core, weintroduce a spatiotemporal gyroscope attention mechanism that dynamicallycaptures evolving multiscale patterns across both time and space. Forpre-training, we curate MillionST, a large-scale dataset of one million imagesfrom 100,000 geographic locations, each captured across 10 temporal phases overfive years, encompassing diverse geospatial changes and seasonal variations.Leveraging this dataset, we adapt masked image modeling to pre-train TiMo,enabling it to effectively learn and encode generalizable spatiotemporalrepresentations.Extensive experiments across multiple spatiotemporaltasks-including deforestation monitoring, land cover segmentation, crop typeclassification, and flood detection-demonstrate TiMo's superiority overstate-of-the-art methods. Code, model, and dataset will be released athttps://github.com/MiliLab/TiMo.</description>
      <author>example@mail.com (Xiaolei Qin, Di Wang, Jing Zhang, Fengxiang Wang, Xin Su, Bo Du, Liangpei Zhang)</author>
      <guid isPermaLink="false">2505.08723v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Training Strategies for Efficient Embodied Reasoning</title>
      <link>http://arxiv.org/abs/2505.08243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了机器人思维链推理（CoT）在提高机器人策略泛化能力和性能方面的作用，特别是视觉-语言-动作模型（VLAs）。提出了新的机器人推理方法，并测试了其效果。&lt;h4&gt;背景&lt;/h4&gt;现有的机器人CoT推理方法在性能和泛化能力上有提升，但存在局限性，如需要特定的机器人推理数据集和较慢的推理速度。&lt;h4&gt;目的&lt;/h4&gt;设计新的机器人推理方法，解决现有方法的局限性，并深入理解推理如何帮助策略性能提升。&lt;h4&gt;方法&lt;/h4&gt;提出了三种机器人推理提升策略，并设计了简单变体来测试每种策略。通过学习生成推理和关注推理，来提升VLA模型的表示学习和动作预测。&lt;h4&gt;主要发现&lt;/h4&gt;学习生成推理可以提升VLA表示，关注推理有助于利用这些特征进行更好的动作预测。新方法在LIBERO-90基准测试中取得了显著性能提升，推理速度比标准方法快3倍。&lt;h4&gt;结论&lt;/h4&gt;CoT推理有助于提升VLAs，提出了两种简单轻量级的替代方法，这些方法在性能和推理速度上都有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpfulintermediate representations before choosing actions -- provides an effectivemethod for improving the generalization and performance of robot policies,especially vision-language-action models (VLAs). While such approaches havebeen shown to improve performance and generalization, they suffer from corelimitations, like needing specialized robot reasoning data and slow inferencespeeds. To design new robot reasoning approaches that address these issues, amore complete characterization of why reasoning helps policy performance iscritical. We hypothesize several mechanisms by which robot reasoning improvespolicies -- (1) better representation learning, (2) improved learningcurricularization, and (3) increased expressivity -- then devise simplevariants of robot CoT reasoning to isolate and test each one. We find thatlearning to generate reasonings does lead to better VLA representations, whileattending to the reasonings aids in actually leveraging these features forimproved action prediction. Our results provide us with a better understandingof why CoT reasoning helps VLAs, which we use to introduce two simple andlightweight alternative recipes for robot reasoning. Our proposed approachesachieve significant performance gains over non-reasoning policies,state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedupcompared to standard robot reasoning.</description>
      <author>example@mail.com (William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, Sergey Levine)</author>
      <guid isPermaLink="false">2505.08243v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction</title>
      <link>http://arxiv.org/abs/2505.08266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraphVision Network (GVN)的有效框架，以及其高效的变体E-GVN，旨在通过视觉感知增强来提高链接预测任务中的MPNNs性能。&lt;h4&gt;背景&lt;/h4&gt;MPNNs和结构特征（SFs）是链接预测任务的基础，但视觉感知在MPNN社区中尚未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够利用视觉感知的框架，以提升MPNNs在链接预测任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了GraphVision Network (GVN)和E-GVN两种框架，并通过七个链接预测数据集进行了实证研究。&lt;h4&gt;主要发现&lt;/h4&gt;GVN在七个链接预测数据集上，包括挑战性的大规模图数据集，都实现了视觉增强带来的性能提升，这些改进与现有最先进（SOTA）方法兼容，并且GVN达到了新的SOTA结果。&lt;h4&gt;结论&lt;/h4&gt;GVN和E-GVN为链接预测任务提供了一个有希望的新的研究方向，证明了视觉感知在MPNNs中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called GraphVision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message-passing graph neural networks (MPNNs) and structural features (SFs)are cornerstones for the link prediction task. However, as a common andintuitive mode of understanding, the potential of visual perception has beenoverlooked in the MPNN community. For the first time, we equip MPNNs withvision structural awareness by proposing an effective framework called GraphVision Network (GVN), along with a more efficient variant (E-GVN). Extensiveempirical results demonstrate that with the proposed frameworks, GVNconsistently benefits from the vision enhancement across seven link predictiondatasets, including challenging large-scale graphs. Such improvements arecompatible with existing state-of-the-art (SOTA) methods and GVNs achieve newSOTA results, thereby underscoring a promising novel direction for linkprediction.</description>
      <author>example@mail.com (Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, Yu Zhang, James Kwok)</author>
      <guid isPermaLink="false">2505.08266v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation</title>
      <link>http://arxiv.org/abs/2505.08157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于双曲对比学习的知识感知推荐方法，旨在解决现有对比学习方法在捕获用户-物品二分图和知识图中的层次结构以及生成正样本时的偏好学习偏移问题。&lt;h4&gt;背景&lt;/h4&gt;基于图神经网络（GNNs）和对比学习的知识感知推荐已成为主流，但现有方法在有效捕获用户-物品二分图和知识图中的层次结构以及生成正样本时存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的双曲对比学习方法，通过模型增强来提高知识感知推荐的效果。&lt;h4&gt;方法&lt;/h4&gt;设计了一种新的洛伦兹知识聚合机制来捕获内在的层次图结构，并提出了三种模型级增强技术来辅助双曲对比学习，避免增强正样本对之间的偏好偏移。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在实验中显示出优于现有基线的优越性，最大提升了11.03%。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提高知识感知推荐的效果，为解决现有对比学习方法中的问题提供了一种新的思路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Benefiting from the effectiveness of graph neural networks (GNNs) andcontrastive learning, GNN-based contrastive learning has become mainstream forknowledge-aware recommendation. However, most existing contrastivelearning-based methods have difficulties in effectively capturing theunderlying hierarchical structure within user-item bipartite graphs andknowledge graphs. Moreover, they commonly generate positive samples forcontrastive learning by perturbing the graph structure, which may lead to ashift in user preference learning. To overcome these limitations, we proposehyperbolic contrastive learning with model-augmentation for knowledge-awarerecommendation. To capture the intrinsic hierarchical graph structures, wefirst design a novel Lorentzian knowledge aggregation mechanism, which enablesmore effective representations of users and items. Then, we propose threemodel-level augmentation techniques to assist Hyperbolic contrastive learning.Different from the classical structure-level augmentation (e.g., edgedropping), the proposed model-augmentations can avoid preference shifts betweenthe augmented positive pair. Finally, we conduct extensive experiments todemonstrate the superiority (maximum improvement of $11.03\%$) of proposedmethods over existing baselines.</description>
      <author>example@mail.com (Shengyin Sun, Chen Ma)</author>
      <guid isPermaLink="false">2505.08157v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)</title>
      <link>http://arxiv.org/abs/2505.08086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习（TL）的多模态AI模型，用于伤口分类，旨在提高伤口诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;有效的诊断急性难以愈合的伤口对于提供有效的患者护理至关重要。不良的临床结果通常与感染、周围血管疾病和伤口深度增加有关。&lt;h4&gt;目的&lt;/h4&gt;通过结合两种最先进的架构Xception和GMRNN，开发一个多模态网络，以改善伤口类型（糖尿病、压力、手术和静脉溃疡）的分类。&lt;h4&gt;方法&lt;/h4&gt;该模型通过连接迁移学习算法提取的特征和位置特征来分类伤口类型，并与深度神经网络（DNN）进行综合比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法的伤口分类准确率从78.77%到100%不等，证明了其在准确分类最常见伤口类型方面的卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了所提出的方法在利用伤口图像及其相应位置准确分类最常见伤口类型方面的优异表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The effective diagnosis of acute and hard-to-heal wounds is crucial for woundcare practitioners to provide effective patient care. Poor clinical outcomesare often linked to infection, peripheral vascular disease, and increasingwound depth, which collectively exacerbate these comorbidities. However,diagnostic tools based on Artificial Intelligence (AI) speed up theinterpretation of medical images and improve early detection of disease. Inthis article, we propose a multi-modal AI model based on transfer learning(TL), which combines two state-of-the-art architectures, Xception and GMRNN,for wound classification. The multi-modal network is developed by concatenatingthe features extracted by a transfer learning algorithm and location featuresto classify the wound types of diabetic, pressure, surgical, and venous ulcers.The proposed method is comprehensively compared with deep neural networks (DNN)for medical image analysis. The experimental results demonstrate a notablewound-class classifications (containing only diabetic, pressure, surgical, andvenous) vary from 78.77 to 100\% in various experiments. The results presentedin this study showcase the exceptional accuracy of the proposed methodology inaccurately classifying the most commonly occurring wound types using woundimages and their corresponding locations.</description>
      <author>example@mail.com (Ramin Mousa, Ehsan Matbooe, Hakimeh Khojasteh, Amirali Bengari, Mohammadmahdi Vahediahmar)</author>
      <guid isPermaLink="false">2505.08086v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</title>
      <link>http://arxiv.org/abs/2505.08607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BooSTer的新框架，用于解决立体匹配方法中标签获取困难、数据稀缺以及合成图像与真实世界图像之间的领域差异问题。&lt;h4&gt;背景&lt;/h4&gt;立体匹配方法需要密集的像素级地面真实标签，这在实际数据集中非常耗时，而且合成图像与真实世界图像之间的数据稀少和领域差距也带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，以利用视觉基础模型和大规模混合图像源（包括合成、真实和单视图图像）来解决上述问题。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一种数据生成策略，结合单目深度估计和扩散模型，从单视图图像中生成密集的立体匹配数据。2. 通过使用伪单目深度标签和动态尺度及平移不变损失，从单目深度估计模型中迁移知识，以解决现实世界数据集中的稀疏标签问题。3. 将视觉基础模型作为编码器，提取鲁棒且可迁移的特征，以提高准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，该方法在准确率方面取得了显著的提升，特别是在数据标签有限和领域迁移的场景中。&lt;h4&gt;结论&lt;/h4&gt;BooSTer框架在解决立体匹配中的挑战方面是有效的，尤其是在标签稀缺和领域迁移的情况下，可以显著提高准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stereo matching methods rely on dense pixel-wise ground truth labels, whichare laborious to obtain, especially for real-world datasets. The scarcity oflabeled data and domain gaps between synthetic and real-world images also posenotable challenges. In this paper, we propose a novel framework,\textbf{BooSTer}, that leverages both vision foundation models and large-scalemixed image sources, including synthetic, real, and single-view images. First,to fully unleash the potential of large-scale single-view images, we design adata generation strategy combining monocular depth estimation and diffusionmodels to generate dense stereo matching data from single-view images. Second,to tackle sparse labels in real-world datasets, we transfer knowledge frommonocular depth estimation models, using pseudo-mono depth labels and a dynamicscale- and shift-invariant loss for additional supervision. Furthermore, weincorporate vision foundation model as an encoder to extract robust andtransferable features, boosting accuracy and generalization. Extensiveexperiments on benchmark datasets demonstrate the effectiveness of ourapproach, achieving significant improvements in accuracy over existing methods,particularly in scenarios with limited labeled data and domain shifts.</description>
      <author>example@mail.com (Yuran Wang, Yingping Liang, Ying Fu)</author>
      <guid isPermaLink="false">2505.08607v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2505.08199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多粒度信息的MLP预测框架，用于解决长期时间序列预测中的关键问题，并在八个基准数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;长期时间序列预测在能源消耗和天气预报等领域具有广泛的应用价值，但准确预测长期变化面临复杂的时间模式和内在的多尺度变化。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种高效的基于MLP的预测框架，解决长期时间序列预测中的多粒度信息利用不足、忽略通道特定属性以及趋势和季节成分的独特性质等问题。&lt;h4&gt;方法&lt;/h4&gt;该方法能够清晰并同步地预测不同尺度的复杂时间动态，并通过一个动态分配不同粒度信息重要性的系统将多尺度预测巧妙地整合。同时，使用双叉结构独立建模趋势和季节性元素。&lt;h4&gt;主要发现&lt;/h4&gt;在八个长期时间序列预测基准数据集上的实验结果表明，与最新的基于MLP的方法（TimeMixer）相比，MDMixer将平均MAE性能提高了4.64%，同时实现了训练效率和模型可解释性之间的有效平衡。&lt;h4&gt;结论&lt;/h4&gt;MDMixer是一种有效的长期时间序列预测方法，在保持训练效率的同时提高了预测准确性，并且具有良好的模型可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term time series forecasting (LTSF) offers broad utility in practicalsettings like energy consumption and weather prediction. Accurately predictinglong-term changes, however, is demanding due to the intricate temporal patternsand inherent multi-scale variations within time series. This work confronts keyissues in LTSF, including the suboptimal use of multi-granularity information,the neglect of channel-specific attributes, and the unique nature of trend andseasonal components, by introducing a proficient MLP-based forecastingframework. Our method adeptly disentangles complex temporal dynamics usingclear, concurrent predictions across various scales. These multi-scaleforecasts are then skillfully integrated through a system that dynamicallyassigns importance to information from different granularities, sensitive toindividual channel characteristics. To manage the specific features of temporalpatterns, a two-pronged structure is utilized to model trend and seasonalelements independently. Experimental results on eight LTSF benchmarksdemonstrate that MDMixer improves average MAE performance by 4.64% compared tothe recent state-of-the-art MLP-based method (TimeMixer), while achieving aneffective balance between training efficiency and model interpretability.</description>
      <author>example@mail.com (Boshi Gao, Qingjian Ni, Fanbo Ju, Yu Chen, Ziqi Zhao)</author>
      <guid isPermaLink="false">2505.08199v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models</title>
      <link>http://arxiv.org/abs/2505.08590v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了将RAG增强型LLM与病理学基础模型结合应用于甲状腺细胞学诊断，通过利用精确的知识库，提高诊断效率和可解释性。&lt;h4&gt;背景&lt;/h4&gt;人工智能的进步正在通过整合大型语言模型（LLM）与检索增强生成（RAG）和特定领域的基座模型来改变病理学。&lt;h4&gt;目的&lt;/h4&gt;解决细胞学解释、标准化和诊断准确性的挑战。&lt;h4&gt;方法&lt;/h4&gt;利用RAG动态检索相关案例研究、诊断标准和专家解释，同时利用病理学基础模型在高清病理图像上进行特征提取和分类。&lt;h4&gt;主要发现&lt;/h4&gt;这些AI驱动的方法提高了诊断一致性，减少了变异性，并支持病理学家区分良性甲状腺病变和恶性病变。结果显示，将RAG与特定领域的LLM结合显著提高了诊断效率和可解释性。&lt;h4&gt;结论&lt;/h4&gt;AI辅助的甲状腺细胞学诊断具有潜力，其中基础模型UNI在从甲状腺细胞学样本中预测手术病理诊断方面实现了AUC 0.73-0.93的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in artificial intelligence (AI) are transforming pathology byintegrat-ing large language models (LLMs) with retrieval-augmented generation(RAG) and domain-specific foundation models. This study explores theapplication of RAG-enhanced LLMs coupled with pathology foundation models forthyroid cytology diagnosis, addressing challenges in cytologicalinterpretation, standardization, and diagnostic accuracy. By leveraging acurated knowledge base, RAG facilitates dy-namic retrieval of relevant casestudies, diagnostic criteria, and expert interpreta-tion, improving thecontextual understanding of LLMs. Meanwhile, pathology foun-dation models,trained on high-resolution pathology images, refine feature extrac-tion andclassification capabilities. The fusion of these AI-driven approaches en-hancesdiagnostic consistency, reduces variability, and supports pathologists indis-tinguishing benign from malignant thyroid lesions. Our results demonstratethat integrating RAG with pathology-specific LLMs significantly improvesdiagnostic efficiency and interpretability, paving the way for AI-assistedthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 forcorrect prediction of surgi-cal pathology diagnosis from thyroid cytologysamples.</description>
      <author>example@mail.com (Hussien Al-Asi, Jordan P Reynolds, Shweta Agarwal, Bryan J Dangott, Aziza Nassar, Zeynettin Akkus)</author>
      <guid isPermaLink="false">2505.08590v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification</title>
      <link>http://arxiv.org/abs/2505.08173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了现有因果模型在CNN和ViT变体上的影响，并提出了TSCNet，一种两阶段因果建模方法，以通过多尺度因果干预发现细粒度因果关联，以减轻长尾分类中的偏差。&lt;h4&gt;背景&lt;/h4&gt;因果推理成为减轻长尾分类偏差的可行方法，但随着从CNN到ViT的高级骨干模型的变化，现有的因果模型可能无法实现预期的性能提升。&lt;h4&gt;目的&lt;/h4&gt;研究现有因果模型对CNN和ViT变体的影响，并提出一种新的方法来解决尾类分类中的难题。&lt;h4&gt;方法&lt;/h4&gt;提出TSCNet，包含两个阶段：1. 层次因果表示学习阶段（HCRL），通过解耦背景和对象，在补丁和特征级别应用后门干预，防止模型使用与类别无关的区域来推断标签；2. 反事实对数偏置校准阶段（CLBC），通过自适应构建反事实平衡数据分布来精炼模型决策边界的优化。&lt;h4&gt;主要发现&lt;/h4&gt;ViT的全局特征表示使得因果方法难以建模细粒度特征与预测之间的关联，导致在具有相似视觉外观的尾类分类中存在困难。&lt;h4&gt;结论&lt;/h4&gt;TSCNet可以消除数据不平衡引入的多个偏差，在长尾基准测试中优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：因果推理已成为减轻长尾分类偏差的有前途的方法，通过处理由类别不平衡引入的偏差。然而，随着从卷积神经网络（CNN）到视觉Transformer（ViT）的先进骨干模型的变化，现有的因果模型可能无法实现预期的性能提升。本文研究了现有因果模型对CNN和ViT变体的影响，强调ViT的全局特征表示使得因果方法难以建模细粒度特征与预测之间的关联，这导致了在具有相似视觉外观的尾类分类中的困难。为了解决这些问题，本文提出了TSCNet，一种两阶段因果建模方法，通过多尺度因果干预来发现细粒度因果关联。具体来说，在层次因果表示学习阶段（HCRL）中，它解耦背景和对象，在补丁和特征级别应用后门干预，防止模型使用与类别无关的区域来推断标签，从而增强细粒度因果表示。在反事实对数偏置校准阶段（CLBC）中，它通过自适应构建反事实平衡数据分布来精炼模型决策边界的优化。在各个长尾基准上进行的广泛实验表明，所提出的TSCNet可以消除数据不平衡引入的多个偏差，优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal inference has emerged as a promising approach to mitigate long-tailclassification by handling the biases introduced by class imbalance. However,along with the change of advanced backbone models from Convolutional NeuralNetworks (CNNs) to Visual Transformers (ViT), existing causal models may notachieve an expected performance gain. This paper investigates the influence ofexisting causal models on CNNs and ViT variants, highlighting that ViT's globalfeature representation makes it hard for causal methods to model associationsbetween fine-grained features and predictions, which leads to difficulties inclassifying tail classes with similar visual appearance. To address theseissues, this paper proposes TSCNet, a two-stage causal modeling method todiscover fine-grained causal associations through multi-scale causalinterventions. Specifically, in the hierarchical causal representation learningstage (HCRL), it decouples the background and objects, applying backdoorinterventions at both the patch and feature level to prevent model from usingclass-irrelevant areas to infer labels which enhances fine-grained causalrepresentation. In the counterfactual logits bias calibration stage (CLBC), itrefines the optimization of model's decision boundary by adaptive constructingcounterfactual balanced data distribution to remove the spurious associationsin the logits caused by data distribution. Extensive experiments conducted onvarious long-tail benchmarks demonstrate that the proposed TSCNet can eliminatemultiple biases introduced by data imbalance, which outperforms existingmethods.</description>
      <author>example@mail.com (Xiaoshuo Yan, Zhaochuan Li, Lei Meng, Zhuang Qi, Wei Wu, Zixuan Li, Xiangxu Meng)</author>
      <guid isPermaLink="false">2505.08173v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection</title>
      <link>http://arxiv.org/abs/2505.08561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为轨迹感知自适应标记采样（TATS）的新型预训练策略，用于视觉基础模型，并通过在四个数据集上的实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;MVM（掩码视频建模）是一种有效的视觉基础模型预训练策略，但选择合适的掩码策略是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的掩码策略，以优化视频中的掩码标记选择，并提高预训练模型的性能。&lt;h4&gt;方法&lt;/h4&gt;引入了TATS，该策略可以建模标记的运动动态，并集成到掩码自动编码器（MAE）框架中。同时，提出了一种统一的训练策略，使用近端策略优化（PPO）对MAE和TATS进行联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;TATS模型允许进行激进的掩码，同时在不影响动作识别等下游任务性能的情况下保持预训练的内存效率。&lt;h4&gt;结论&lt;/h4&gt;在四个数据集上的实验表明，与现有方法相比，TATS在有效性、迁移性、泛化性和效率方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：掩码视频建模（MVM）已成为视觉基础模型的高效预训练策略，其中模型使用可见标记的信息重建掩码时空标记。然而，这类方法的一个关键挑战在于选择合适的掩码策略。以往的研究探索了预定义的掩码技术，包括随机和基于管道的掩码，以及利用关键运动先验、光流和来自外部预训练模型的语义线索的方法。在本研究中，我们引入了一种新颖且通用的轨迹感知自适应标记采样（TATS），该策略可以建模标记的运动动态，并且可以无缝集成到掩码自动编码器（MAE）框架中以选择视频中的运动中心标记。此外，我们提出了一种统一的训练策略，通过近端策略优化（PPO）从零开始联合优化MAE和TATS。我们表明，我们的模型允许进行激进的掩码，同时在不损害动作识别等下游任务性能的情况下确保预训练保持内存效率。在四个基准数据集（包括Something-Something v2、Kinetics-400、UCF101和HMDB51）上对所提出方法的大量实验表明，与最先进的方法相比，我们的工作在有效性、迁移性、泛化性和效率方面具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked video modeling~(MVM) has emerged as a highly effective pre-trainingstrategy for visual foundation models, whereby the model reconstructs maskedspatiotemporal tokens using information from visible tokens. However, a keychallenge in such approaches lies in selecting an appropriate masking strategy.Previous studies have explored predefined masking techniques, including randomand tube-based masking, as well as approaches that leverage key motion priors,optical flow and semantic cues from externally pre-trained models. In thiswork, we introduce a novel and generalizable Trajectory-Aware Adaptive TokenSampler (TATS), which models the motion dynamics of tokens and can beseamlessly integrated into the masked autoencoder (MAE) framework to selectmotion-centric tokens in videos. Additionally, we propose a unified trainingstrategy that enables joint optimization of both MAE and TATS from scratchusing Proximal Policy Optimization (PPO). We show that our model allows foraggressive masking without compromising performance on the downstream task ofaction recognition while also ensuring that the pre-training remains memoryefficient. Extensive experiments of the proposed approach across fourbenchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,demonstrate the effectiveness, transferability, generalization, and efficiencyof our work compared to other state-of-the-art methods.</description>
      <author>example@mail.com (Ayush K. Rai, Kyle Min, Tarun Krishna, Feiyan Hu, Alan F. Smeaton, Noel E. O'Connor)</author>
      <guid isPermaLink="false">2505.08561v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic</title>
      <link>http://arxiv.org/abs/2505.08021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络（GNNs）在处理图结构数据中的应用，特别是它们如何处理不同大小的输入图和在图同构下的不变性。文章通过将有限模型理论的方法和工具应用于图表示学习领域，揭示了GNNs的逻辑表达能力。&lt;h4&gt;背景&lt;/h4&gt;GNNs解决了在图结构数据上应用深度学习时遇到的两个主要挑战：处理不同大小的输入图和确保图同构下的不变性。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs的表达能力，特别是有限GNN架构与一阶逻辑（FO）特定片段的关系。&lt;h4&gt;方法&lt;/h4&gt;应用一阶和模态逻辑的有限模型理论方法于图表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;有限GNN架构对应于一阶逻辑的特定片段，包括模态逻辑（ML）、分级模态逻辑（GML）、带有普遍模态的模态逻辑（ML(A)）、二变量片段（FO2）及其扩展计数量词（C2）。&lt;h4&gt;结论&lt;/h4&gt;这一研究提供了一个统一框架，以理解GNNs在FO中的逻辑表达能力。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we show that bounded GNN architectures correspond to specific fragments of first-order logic (FO), including modal logic (ML), graded modal logic (GML), modal logic with the universal modality (ML(A)), the two-variable fragment (FO2) and its extension with counting quantifiers (C2). To establish these results, we apply methods and tools from finite model theory of first-order and modal logics to the domain of graph representation learning. This provides a unifying framework for understanding the logical expressiveness of GNNs within FO.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) address two key challenges in applying deeplearning to graph-structured data: they handle varying size input graphs andensure invariance under graph isomorphism. While GNNs have demonstrated broadapplicability, understanding their expressive power remains an importantquestion. In this paper, we show that bounded GNN architectures correspond tospecific fragments of first-order logic (FO), including modal logic (ML),graded modal logic (GML), modal logic with the universal modality (ML(A)), thetwo-variable fragment (FO2) and its extension with counting quantifiers (C2).To establish these results, we apply methods and tools from finite model theoryof first-order and modal logics to the domain of graph representation learning.This provides a unifying framework for understanding the logical expressivenessof GNNs within FO.</description>
      <author>example@mail.com (Bernardo Cuenca Grau, Przemysław A. Wałęga)</author>
      <guid isPermaLink="false">2505.08021v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain</title>
      <link>http://arxiv.org/abs/2505.08550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于线性多变量时间序列预测模型OLinear，该模型在正交变换域中运行，旨在通过改进特征域的编码和解码来提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;当前预测模型通常采用时间域的时序预测（TF）范式，直接在时间域中编码和解码时间序列。然而，时间序列数据中的交错步骤依赖关系可能会阻碍TF的性能。&lt;h4&gt;目的&lt;/h4&gt;为了解决TF的性能问题，本文旨在提出一种更有效的编码和解码方法，并增强多变量时间序列的表示学习。&lt;h4&gt;方法&lt;/h4&gt;本文采用基于正交矩阵的数据自适应变换OrthoTrans，对时间序列的时序皮尔逊相关矩阵进行对角化，从而在去相关的特征域中进行更有效的编码和解码。此外，引入了定制的线性层NormLin，使用归一化权重矩阵来捕捉多变量依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，NormLin模块在性能上优于多头自注意力机制，同时所需的浮点运算数（FLOPs）大约是其一半。在24个基准和140个预测任务上的广泛实验表明，OLinear模型在效率上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;作为自注意力机制的插件替代品，NormLin模块能够持续增强基于Transformer的预测器。代码和数据集可通过指定链接获取。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于线性多变量时间序列预测模型OLinear，该模型在正交变换域中运行，旨在通过改进特征域的编码和解码来提高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/jackyue1994/OLinear&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-basedmultivariate time series forecasting model that operates in an$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typicallyadopt the temporal forecast (TF) paradigm, which directly encode and decodetime series in the time domain. However, the entangled step-wise dependenciesin series data can hinder the performance of TF. To address this, someforecasters conduct encoding and decoding in the transformed domain usingfixed, dataset-independent bases (e.g., sine and cosine signals in the Fouriertransform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptivetransformation based on an orthogonal matrix that diagonalizes the series'temporal Pearson correlation matrix. This approach enables more effectiveencoding and decoding in the decorrelated feature domain and can serve as aplug-in module to enhance existing forecasters. To enhance the representationlearning for multivariate time series, we introduce a customized linear layer,$\mathbf{NormLin}$, which employs a normalized weight matrix to capturemultivariate dependencies. Empirically, the NormLin module shows a surprisingperformance advantage over multi-head self-attention, while requiring nearlyhalf the FLOPs. Extensive experiments on 24 benchmarks and 140 forecastingtasks demonstrate that OLinear consistently achieves state-of-the-artperformance with high efficiency. Notably, as a plug-in replacement forself-attention, the NormLin module consistently enhances Transformer-basedforecasters. The code and datasets are available athttps://anonymous.4open.science/r/OLinear</description>
      <author>example@mail.com (Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi)</author>
      <guid isPermaLink="false">2505.08550v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Equivariant graph neural network surrogates for predicting the properties of relaxed atomic configurations</title>
      <link>http://arxiv.org/abs/2505.08121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文使用密度泛函理论（DFT）计算确定结构的最小形成能量，并提出了等变图神经网络（EGNN）模型来预测对特定结构的DFT计算结果。&lt;h4&gt;背景&lt;/h4&gt;传统的簇展开方法在处理固定晶格以外的结构，如间隙原子、非晶材料和多结构材料时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出EGNN模型，以更灵活地处理结构变化，同时尊重系统的对称性。&lt;h4&gt;方法&lt;/h4&gt;在锂钴氧化物（LCO）的多种锂含量和锂原子排列组合下，使用EGNN模型进行数学框架构建和训练。&lt;h4&gt;主要发现&lt;/h4&gt;EGNN模型能够准确预测训练集之外的量，包括最大原子位移、应变张量和能量，以及形成能量。&lt;h4&gt;结论&lt;/h4&gt;EGNN模型提供了对研究系统的深入理解，无需进行更多的DFT计算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Density functional theory (DFT) calculations determine the relaxed atomicpositions and lattice parameters that minimize the formation energy of astructure. We present an equivariant graph neural network (EGNN) model topredict the outcome of DFT calculations for structures of interest. Clusterexpansions are a well established approach for representing the formationenergies. However, traditional cluster expansions are limited in their abilityto handle variations from a fixed lattice, including interstitial atoms,amorphous materials, and materials with multiple structures. EGNNs offer a moreflexible framework that inherently respects the symmetry of the system withoutbeing reliant on a particular lattice. In this work, we present themathematical framework and the results of training for lithium cobalt oxide(LCO) at various compositions of lithium and arrangements of the lithium atoms.Our results demonstrate that the EGNN can accurately predict quantities outsidethe training set including the largest atomic displacements, the strain tensorand energy, and the formation energy providing greater insight into the systembeing studied without the need for more DFT calculations.</description>
      <author>example@mail.com (Jamie Holber, Krishna Garikipati)</author>
      <guid isPermaLink="false">2505.08121v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks</title>
      <link>http://arxiv.org/abs/2505.07895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HGNN-IMA的新型模型，用于多模态异构网络中的节点分类，通过捕捉信息传播过程中的多模态相互影响，实现自适应多模态融合。&lt;h4&gt;背景&lt;/h4&gt;当前在线平台如豆瓣电影网络和亚马逊产品评论网络等可以描述为多模态异构网络（MMHNs），在这些网络中准确分类节点对于分析对应实体至关重要，需要有效的节点表示学习方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的节点分类模型，以解决现有多模态融合方法在早期融合和晚期融合中的不足。&lt;h4&gt;方法&lt;/h4&gt;模型名为HGNN-IMA，采用异构图Transformer框架，集成了嵌套的跨模态注意力机制，并考虑了模态对齐以促进具有跨模态一致相似性的节点间的传播，同时增加了注意力损失以减轻缺失模态的影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验验证了该模型在节点分类任务中的优越性，为处理多模态数据提供了创新视角，尤其是在伴随网络结构的情况下。&lt;h4&gt;结论&lt;/h4&gt;HGNN-IMA模型能够有效地处理多模态数据，特别是在节点分类任务中具有显著优势，为多模态网络分析提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：如今，众多在线平台可以描述为多模态异构网络（MMHNs），如豆瓣的电影网络和亚马逊的产品评论网络。在这些网络中对节点进行准确分类对于分析相应的实体至关重要，这需要节点上的有效表示学习方法。然而，现有的多模态融合方法通常采用早期融合策略，这可能会导致丢失各个模态的独特特征，或者采用晚期融合方法，忽略了基于GNN的信息传播中的跨模态指导。在本文中，我们提出了一种名为异构图神经网络与跨模态注意力（HGNN-IMA）的新型模型，用于MMHNs中的节点分类。它通过在异构图Transformer框架内捕捉信息传播过程中的多模态相互影响来学习节点表示。具体来说，将嵌套的跨模态注意力机制整合到节点间注意力中，以实现自适应多模态融合，并考虑模态对齐以促进具有所有模态一致相似性的节点间的传播。此外，还增加了注意力损失以减轻缺失模态的影响。大量实验验证了该模型在节点分类任务中的优越性，为处理多模态数据提供了一个创新的方法，尤其是在伴随网络结构的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nowadays, numerous online platforms can be described as multi-modalheterogeneous networks (MMHNs), such as Douban's movie networks and Amazon'sproduct review networks. Accurately categorizing nodes within these networks iscrucial for analyzing the corresponding entities, which requires effectiverepresentation learning on nodes. However, existing multi-modal fusion methodsoften adopt either early fusion strategies which may lose the uniquecharacteristics of individual modalities, or late fusion approaches overlookingthe cross-modal guidance in GNN-based information propagation. In this paper,we propose a novel model for node classification in MMHNs, named HeterogeneousGraph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns noderepresentations by capturing the mutual influence of multiple modalities duringthe information propagation process, within the framework of heterogeneousgraph transformer. Specifically, a nested inter-modal attention mechanism isintegrated into the inter-node attention to achieve adaptive multi-modalfusion, and modality alignment is also taken into account to encourage thepropagation among nodes with consistent similarities across all modalities.Moreover, an attention loss is augmented to mitigate the impact of missingmodalities. Extensive experiments validate the superiority of the model in thenode classification task, providing an innovative view to handle multi-modaldata, especially when accompanied with network structures.</description>
      <author>example@mail.com (Jiafan Li, Jiaqi Zhu, Liang Chang, Yilin Li, Miaomiao Li, Yang Wang, Hongan Wang)</author>
      <guid isPermaLink="false">2505.07895v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>ExEBench: Benchmarking Foundation Models on Extreme Earth Events</title>
      <link>http://arxiv.org/abs/2505.08529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ExE-Bench，一个针对极端事件的基准数据集，旨在评估机器学习模型在极端事件管理中的可靠性。&lt;h4&gt;背景&lt;/h4&gt;地球正面临越来越频繁的极端事件，这些事件对人类生活和生态系统构成重大风险。机器学习在提取特征和灾难管理方面展现出潜力，但模型可能存在训练数据中的偏差。&lt;h4&gt;目的&lt;/h4&gt;ExEBench旨在（1）评估机器学习模型在多样化、高影响任务和领域中的泛化能力，（2）促进有助于灾难管理的创新机器学习方法的发展，（3）提供一个分析极端事件相互作用和级联效应的平台，以加深我们对地球系统，尤其是未来几十年气候变化预期下的理解。&lt;h4&gt;方法&lt;/h4&gt;ExEBench包含七个极端事件类别，包括洪水、野火、风暴、热带气旋、极端降水、热浪和冷浪，具有全球覆盖、不同数据量和多样化的数据来源。&lt;h4&gt;主要发现&lt;/h4&gt;ExEBench旨在解决机器学习模型在极端事件管理中的挑战，并提供一个评估模型性能的基准。&lt;h4&gt;结论&lt;/h4&gt;ExEBench是一个公共数据集和代码库，旨在推动机器学习在极端事件管理中的应用和发展。&lt;h4&gt;翻译&lt;/h4&gt;Our planet is facing increasingly frequent extreme events, which pose major risks to human lives and ecosystems. Recent advances in machine learning (ML), especially with foundation models (FMs) trained on extensive datasets, excel in extracting features and show promise in disaster management. Nevertheless, these models often inherit biases from training data, challenging their performance over extreme values. To explore the reliability of FM in the context of extreme events, we introduce extbf{ExE}Bench (extbf{Ex}tremeextbf{E}arth Benchmark), a collection of seven extreme event categories across floods, wildfires, storms, tropical cyclones, extreme precipitation, heatwaves, and cold waves. The dataset features global coverage, varying datavolumes, and diverse data sources with different spatial, temporal, and spectral characteristics. To broaden the real-world impact of FMs, we includemultiple challenging ML tasks that are closely aligned with operational needs in extreme events detection, monitoring, and forecasting. ExEBench aims to (1) assess FM generalizability across diverse, high-impact tasks and domains, (2) promote the development of novel ML methods that benefit disaster management, and (3) offer a platform for analyzing the interactions and cascading effects of extreme events to advance our understanding of Earth system, especially under the climate change expected in the decades to come. The dataset and code are public https://github.com/zhaoshan2/EarthExtreme-Bench.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our planet is facing increasingly frequent extreme events, which pose majorrisks to human lives and ecosystems. Recent advances in machine learning (ML),especially with foundation models (FMs) trained on extensive datasets, excel inextracting features and show promise in disaster management. Nevertheless,these models often inherit biases from training data, challenging theirperformance over extreme values. To explore the reliability of FM in thecontext of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme\textbf{E}arth Benchmark), a collection of seven extreme event categoriesacross floods, wildfires, storms, tropical cyclones, extreme precipitation,heatwaves, and cold waves. The dataset features global coverage, varying datavolumes, and diverse data sources with different spatial, temporal, andspectral characteristics. To broaden the real-world impact of FMs, we includemultiple challenging ML tasks that are closely aligned with operational needsin extreme events detection, monitoring, and forecasting. ExEBench aims to (1)assess FM generalizability across diverse, high-impact tasks and domains, (2)promote the development of novel ML methods that benefit disaster management,and (3) offer a platform for analyzing the interactions and cascading effectsof extreme events to advance our understanding of Earth system, especiallyunder the climate change expected in the decades to come. The dataset and codeare public https://github.com/zhaoshan2/EarthExtreme-Bench.</description>
      <author>example@mail.com (Shan Zhao, Zhitong Xiong, Jie Zhao, Xiao Xiang Zhu)</author>
      <guid isPermaLink="false">2505.08529v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Symbolically-Guided Visual Plan Inference from Uncurated Video Data</title>
      <link>http://arxiv.org/abs/2505.08444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vis2Plan是一个高效的、可解释的、基于符号指导的视觉规划框架，它在长周期操作任务中实现了良好的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉规划方法通常依赖于视频生成模型来获取子目标，但存在模型幻觉和计算成本高的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够自动提取任务符号并构建高级符号转换图，以实现多目标、多阶段规划的框架。&lt;h4&gt;方法&lt;/h4&gt;Vis2Plan从原始的无标签游戏数据中提取一组紧凑的任务符号，并在符号级别进行规划，生成一系列基于符号表示的物理一致的中间子目标图像。&lt;h4&gt;主要发现&lt;/h4&gt;Vis2Plan在真实机器人环境中比基于扩散视频生成模型的视觉规划器有53%更高的成功率，并且生成视觉计划的效率提高了35倍。&lt;h4&gt;结论&lt;/h4&gt;Vis2Plan能够生成物理一致的图像目标，并提供了完全可检查的推理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual planning, by offering a sequence of intermediate visual subgoals to agoal-conditioned low-level policy, achieves promising performance onlong-horizon manipulation tasks. To obtain the subgoals, existing methodstypically resort to video generation models but suffer from model hallucinationand computational cost. We present Vis2Plan, an efficient, explainable andwhite-box visual planning framework powered by symbolic guidance. From raw,unlabeled play data, Vis2Plan harnesses vision foundation models toautomatically extract a compact set of task symbols, which allows building ahigh-level symbolic transition graph for multi-goal, multi-stage planning. Attest time, given a desired task goal, our planner conducts planning at thesymbolic level and assembles a sequence of physically consistent intermediatesub-goal images grounded by the underlying symbolic representation. OurVis2Plan outperforms strong diffusion video generation-based visual planners bydelivering 53\% higher aggregate success rate in real robot settings whilegenerating visual plans 35$\times$ faster. The results indicate that Vis2Planis able to generate physically consistent image goals while offering fullyinspectable reasoning steps.</description>
      <author>example@mail.com (Wenyan Yang, Ahmet Tikna, Yi Zhao, Yuying Zhang, Luigi Palopoli, Marco Roveri, Joni Pajarinen)</author>
      <guid isPermaLink="false">2505.08444v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care</title>
      <link>http://arxiv.org/abs/2505.08414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Meta-EyeFM是一个多功能的底层模型，集成了大语言模型和视觉基础模型，用于眼科疾病的评估。该模型通过路由机制实现基于文本查询的准确任务分析，并通过低秩适应对视觉基础模型进行微调，以提高疾病的检测、严重程度区分和常见眼部标志识别的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前的深度学习模型大多是针对特定任务的，并且缺乏用户友好的界面进行操作。&lt;h4&gt;目的&lt;/h4&gt;提出Meta-EyeFM模型，旨在为眼科疾病的评估提供一种高效、准确的方法。&lt;h4&gt;方法&lt;/h4&gt;Meta-EyeFM利用路由机制，根据文本查询进行任务特定的分析。通过低秩适应对视觉基础模型进行微调，以检测眼部和全身疾病，区分眼部疾病的严重程度，并识别常见眼部标志。&lt;h4&gt;主要发现&lt;/h4&gt;Meta-EyeFM在将眼底图像路由到适当的视觉基础模型方面达到了100%的准确率，在疾病检测、严重程度区分和标志识别方面的准确率分别达到了≥82.2%、≥89%和≥76%。该模型在检测各种眼病方面的准确率比Gemini-1.5-flash和ChatGPT-4oLMMs高11%到43%，并且与眼科医生的水平相当。&lt;h4&gt;结论&lt;/h4&gt;Meta-EyeFM系统提供了增强的可用性和诊断性能，成为初级眼科护理或在线大语言模型眼底评估的有价值决策支持工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当前的深度学习模型大多是针对特定任务的，并且缺乏用户友好的界面进行操作。我们提出了Meta-EyeFM，一个集成了大型语言模型（LLM）和视觉基础模型（VFMs）的多功能基础模型，用于眼科疾病的评估。Meta-EyeFM利用路由机制，根据文本查询进行任务特定的分析。使用低秩适应，我们微调了我们的VFMs以检测眼部和全身疾病，区分眼部疾病严重程度，并识别常见眼部标志。该模型在将眼底图像路由到适当的VFMs方面达到了100%的准确率，VFMs在疾病检测方面的准确率达到了≥82.2%，在严重程度区分方面达到了≥89%，在标志识别方面达到了≥76%。与Gemini-1.5-flash和ChatGPT-4oLMMs相比，Meta-EyeFM在检测各种眼病方面的准确率提高了11%到43%，并且与眼科医生的水平相当。该系统提供了增强的可用性和诊断性能，成为初级眼科护理或在线LLM眼底评估的有价值决策支持工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current deep learning models are mostly task specific and lack auser-friendly interface to operate. We present Meta-EyeFM, a multi-functionfoundation model that integrates a large language model (LLM) with visionfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages arouting mechanism to enable accurate task-specific analysis based on textqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular andsystemic diseases, differentiate ocular disease severity, and identify commonocular signs. The model achieved 100% accuracy in routing fundus images toappropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection,$\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification.Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4oLMMs in detecting various eye diseases and comparable to an ophthalmologist.This system offers enhanced usability and diagnostic performance, making it avaluable decision support tool for primary eye care or an online LLM for fundusevaluation.</description>
      <author>example@mail.com (Zhi Da Soh, Yang Bai, Kai Yu, Yang Zhou, Xiaofeng Lei, Sahil Thakur, Zann Lee, Lee Ching Linette Phang, Qingsheng Peng, Can Can Xue, Rachel Shujuan Chong, Quan V. Hoang, Lavanya Raghavan, Yih Chung Tham, Charumathi Sabanayagam, Wei-Chi Wu, Ming-Chih Ho, Jiangnan He, Preeti Gupta, Ecosse Lamoureux, Seang Mei Saw, Vinay Nangia, Songhomitra Panda-Jonas, Jie Xu, Ya Xing Wang, Xinxing Xu, Jost B. Jonas, Tien Yin Wong, Rick Siow Mong Goh, Yong Liu, Ching-Yu Cheng)</author>
      <guid isPermaLink="false">2505.08414v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>BLAB: Brutally Long Audio Bench</title>
      <link>http://arxiv.org/abs/2505.03054v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为BLAB的音频基准测试，用于评估音频语言模型在长音频内容上的理解能力。&lt;h4&gt;背景&lt;/h4&gt;理解多样化的语音交互对于语言技术的发展至关重要，而当前研究主要关注短音频片段，缺乏对长对话音频片段的探索。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理长音频内容的音频语言模型，提高语言技术在不同用户群体中的可访问性。&lt;h4&gt;方法&lt;/h4&gt;BLAB包含超过833小时的多样化全长度音频剪辑，平均长度为51分钟，并配以基于文本的标注问题及答案。对六个开源和专有音频语言模型进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有模型在BLAB上的表现都存在困难，尤其是在定位、时长估计、情感和计数任务上。长音频理解能力存在挑战，模型性能随音频时长增加而下降，依赖提示而非音频内容。&lt;h4&gt;结论&lt;/h4&gt;BLAB为评估和开发具有强大长音频理解能力的音频语言模型提供了一个挑战性的框架。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Developing large audio language models (LMs) capable of understanding diverse spoken interactions is essential for accommodating the multimodal nature of human communication and can increase the accessibility of language technologies across different user populations. Recent work on audio LMs has primarily evaluated their performance on short audio segments, typically under 30 seconds, with limited exploration of long-form conversational speech segments that more closely reflect natural user interactions with these models. We introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio benchmark that evaluates audio LMs on localization, duration estimation, emotion, and counting tasks using audio segments averaging 51 minutes in length. BLAB consists of 833+ hours of diverse, full-length audio clips, each paired with human-annotated, text-based natural language questions and answers. Our audio data were collected from permissively licensed sources and underwent a human-assisted filtering process to ensure task compliance. We evaluate six open-source and proprietary audio LMs on BLAB and find that all of them, including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the tasks in BLAB. Our comprehensive analysis reveals key insights into the trade-offs between task difficulty and audio duration. In general, we find that audio LMs struggle with long-form speech, with performance declining as duration increases. They perform poorly on localization, temporal reasoning, counting, and struggle to understand non-phonemic information, relying more on prompts than audio content. BLAB serves as a challenging evaluation framework to develop audio LMs with robust long-form audio understanding capabilities.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing large audio language models (LMs) capable of understanding diversespoken interactions is essential for accommodating the multimodal nature ofhuman communication and can increase the accessibility of language technologiesacross different user populations. Recent work on audio LMs has primarilyevaluated their performance on short audio segments, typically under 30seconds, with limited exploration of long-form conversational speech segmentsthat more closely reflect natural user interactions with these models. Weintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audiobenchmark that evaluates audio LMs on localization, duration estimation,emotion, and counting tasks using audio segments averaging 51 minutes inlength. BLAB consists of 833+ hours of diverse, full-length audio clips, eachpaired with human-annotated, text-based natural language questions and answers.Our audio data were collected from permissively licensed sources and underwenta human-assisted filtering process to ensure task compliance. We evaluate sixopen-source and proprietary audio LMs on BLAB and find that all of them,including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with thetasks in BLAB. Our comprehensive analysis reveals key insights into thetrade-offs between task difficulty and audio duration. In general, we find thataudio LMs struggle with long-form speech, with performance declining asduration increases. They perform poorly on localization, temporal reasoning,counting, and struggle to understand non-phonemic information, relying more onprompts than audio content. BLAB serves as a challenging evaluation frameworkto develop audio LMs with robust long-form audio understanding capabilities.</description>
      <author>example@mail.com (Orevaoghene Ahia, Martijn Bartelds, Kabir Ahuja, Hila Gonen, Valentin Hofmann, Siddhant Arora, Shuyue Stella Li, Vishal Puttagunta, Mofetoluwa Adeyemi, Charishma Buchireddy, Ben Walls, Noah Bennett, Shinji Watanabe, Noah A. Smith, Yulia Tsvetkov, Sachin Kumar)</author>
      <guid isPermaLink="false">2505.03054v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</title>
      <link>http://arxiv.org/abs/2505.06699v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模型引导的新兴学习范式，通过使用训练好的模型作为参考来指导并增强目标模型的训练，称为模型引导。文章通过理论分析，提出了一个基于分布鲁棒优化的理论框架DRRho风险最小化，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;模型引导在训练大型基础模型等场景中已有所应用，但其内在原理理解不足，导致性能欠佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个理论框架以指导模型引导，提高模型训练的泛化能力和数据效率。&lt;h4&gt;方法&lt;/h4&gt;提出DRRho风险最小化理论框架，通过分布鲁棒优化（DRO）来分析模型引导的原理，并引入DRRho-CLIP方法进行对比语言-图像预训练。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了模型引导方法比无参考模型训练在泛化能力和数据效率方面的优势，并通过实验验证了这一发现。&lt;h4&gt;结论&lt;/h4&gt;模型引导方法在提高模型训练效果方面具有潜力，且理论框架DRRho风险最小化为其提供了理论支持。&lt;h4&gt;翻译&lt;/h4&gt;本文正式化了一种新兴的学习范式，即使用训练好的模型作为参考来指导并增强目标模型的训练，称为模型引导。尽管模型引导在诸如大型基础模型训练的各种场景中已有所应用，但其背后的原理仍然没有得到充分的理解，这导致了性能的不优化。在这项工作中，我们提出了一种基于分布鲁棒优化（DRO）的理论驱动框架，称为DRRho风险最小化。通过泛化分析，我们提供了理论见解，说明了为什么这种方法比没有参考模型训练提高了泛化能力和数据效率。据我们所知，这是首次为这种新的学习范式提供这样的理论见解，这大大增强了我们对于模型引导的理解和实践。基于这些见解以及对比学习和DRO之间的联系，我们引入了一种名为DRRho-CLIP的对比语言-图像预训练的新方法。广泛的实验验证了理论见解，揭示了与没有参考模型的CLIP相比的优越扩展规律，并展示了其相对于现有启发式方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper formalizes an emerging learning paradigm that uses a trained modelas a reference to guide and enhance the training of a target model throughstrategic data selection or weighting, named $\textbf{model steering}$. Whilead-hoc methods have been used in various contexts, including the training oflarge foundation models, its underlying principles remain insufficientlyunderstood, leading to sub-optimal performance. In this work, we propose atheory-driven framework for model steering called $\textbf{DRRho riskminimization}$, which is rooted in Distributionally Robust Optimization (DRO).Through a generalization analysis, we provide theoretical insights into whythis approach improves generalization and data efficiency compared to trainingwithout a reference model. To the best of our knowledge, this is the first timesuch theoretical insights are provided for the new learning paradigm, whichsignificantly enhance our understanding and practice of model steering.Building on these insights and the connection between contrastive learning andDRO, we introduce a novel method for Contrastive Language-Image Pretraining(CLIP) with a reference model, termed DRRho-CLIP. Extensive experimentsvalidate the theoretical insights, reveal a superior scaling law compared toCLIP without a reference model, and demonstrate its strength over existingheuristic approaches.</description>
      <author>example@mail.com (Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. Thai, Tianbao Yang)</author>
      <guid isPermaLink="false">2505.06699v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>DSADF: Thinking Fast and Slow for Decision Making</title>
      <link>http://arxiv.org/abs/2505.08189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DSADF的双系统自适应决策框架，通过结合快速直觉决策和深度分析推理，提高强化学习代理在动态环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理在静态环境中表现良好，但在动态环境中泛化能力有限，因为它们依赖于试错交互。&lt;h4&gt;目的&lt;/h4&gt;为了解决强化学习代理在动态环境中泛化能力不足的问题，提出了一种新的决策框架。&lt;h4&gt;方法&lt;/h4&gt;DSADF框架包含两个互补模块：System 1（由强化学习代理和记忆空间组成，用于快速直觉决策）和System 2（由视觉语言模型驱动，用于深度分析推理）。&lt;h4&gt;主要发现&lt;/h4&gt;DSADF在视频游戏环境Crafter和Housekeep中的实证研究表明，该方法在未知和已知任务中均显著提高了决策能力。&lt;h4&gt;结论&lt;/h4&gt;DSADF通过结合直觉和深度推理，实现了高效和自适应的决策，为强化学习代理在动态环境中的泛化提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管强化学习代理在定义良好的环境中效果显著，但它们往往难以将学到的策略泛化到动态环境中，这是因为它们依赖于试错交互。最近的研究探索了通过策略优化指导或先验知识应用大型语言模型（LLMs）或视觉语言模型（VLMs）来提高强化学习代理的泛化能力。然而，这些方法通常缺乏强化学习代理和基础模型之间的无缝协调，导致在不熟悉的环境中的不合理决策和效率瓶颈。充分利用基础模型的推理能力、强化学习代理的快速响应能力，并增强两者之间的交互以形成一个双系统，仍然是一个悬而未决的科学问题。为了解决这个问题，我们借鉴了Kahneman的快速思考（系统1）和慢速思考（系统2）的理论，证明了在复杂世界中平衡直觉和深度推理可以实现敏捷的决策。在本研究中，我们提出了一种双系统自适应决策框架（DSADF），它集成了两个互补的模块：系统1，包括一个强化学习代理和一个用于快速直觉决策的记忆空间；系统2，由一个视觉语言模型驱动，用于深度分析推理。DSADF通过结合两个系统的优势，促进了高效和自适应的决策。在视频游戏环境Crafter和Housekeep中的实证研究证明了我们提出的方法的有效性，显示出在未知和已知任务中决策能力的显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Reinforcement Learning (RL) agents are effective in well-definedenvironments, they often struggle to generalize their learned policies todynamic settings due to their reliance on trial-and-error interactions. Recentwork has explored applying Large Language Models (LLMs) or Vision LanguageModels (VLMs) to boost the generalization of RL agents through policyoptimization guidance or prior knowledge. However, these approaches often lackseamless coordination between the RL agent and the foundation model, leading tounreasonable decision-making in unfamiliar environments and efficiencybottlenecks. Making full use of the inferential capabilities of foundationmodels and the rapid response capabilities of RL agents and enhancing theinteraction between the two to form a dual system is still a lingeringscientific question. To address this problem, we draw inspiration fromKahneman's theory of fast thinking (System 1) and slow thinking (System 2),demonstrating that balancing intuition and deep reasoning can achieve nimbledecision-making in a complex world. In this study, we propose a Dual-SystemAdaptive Decision Framework (DSADF), integrating two complementary modules:System 1, comprising an RL agent and a memory space for fast and intuitivedecision making, and System 2, driven by a VLM for deep and analyticalreasoning. DSADF facilitates efficient and adaptive decision-making bycombining the strengths of both systems. The empirical study in the video gameenvironment: Crafter and Housekeep demonstrates the effectiveness of ourproposed method, showing significant improvements in decision abilities forboth unseen and known tasks.</description>
      <author>example@mail.com (Alex Zhihao Dou, Dongfei Cui, Jun Yan, Weida Wang, Benteng Chen, Haoming Wang, Zeke Xie, Shufei Zhang)</author>
      <guid isPermaLink="false">2505.08189v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast</title>
      <link>http://arxiv.org/abs/2505.08151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了锂离子电池容量退化准确估计的重要性，提出了一种针对时间序列基础模型的自适应微调策略，以增强电池退化预测的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统专家模型适用于特定场景，而数据驱动技术的发展为电池容量退化预测提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于电池退化预测的时间序列基础模型的微调策略，实现零样本泛化。&lt;h4&gt;方法&lt;/h4&gt;采用自适应微调策略对Timer模型进行微调，并在约10GB的公开电池充放电数据上进行应用；提出知识蒸馏框架，将预训练基础模型的知识转移到紧凑的专家模型中。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的Battery-Timer在容量退化预测方面具有强大的零样本泛化能力；知识蒸馏框架显著提高了专家模型的多条件泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效提升电池容量退化预测的准确性，为电池可靠性和安全性提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sjtu-chan-joey/battery-timer&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate estimation of lithium-ion battery capacity degradation is criticalfor enhancing the reliability and safety of battery operations. Traditionalexpert models, tailored to specific scenarios, provide isolated estimations.With the rapid advancement of data-driven techniques, a series ofgeneral-purpose time-series foundation models have been developed. However,foundation models specifically designed for battery capacity degradation remainlargely unexplored. To enable zero-shot generalization in battery degradationprediction using large model technology, this study proposes adegradation-aware fine-tuning strategy for time-series foundation models. Weapply this strategy to fine-tune the Timer model on approximately 10 GB ofopen-source battery charge discharge data. Validation on our releasedCycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timerpossesses strong zero-shot generalization capability in capacity degradationforecasting. To address the computational challenges of deploying large models,we further propose a knowledge distillation framework that transfers theknowledge of pre-trained foundation models into compact expert models.Distillation results across several state-of-the-art time-series expert modelsconfirm that foundation model knowledge significantly improves themulti-condition generalization of expert models.</description>
      <author>example@mail.com (Joey Chan, Zhen Chen, Ershun Pan)</author>
      <guid isPermaLink="false">2505.08151v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Games for graded modal substitution calculus</title>
      <link>http://arxiv.org/abs/2505.07966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了针对分级模态替换演算（GMSC）及其变体的两种语义游戏和公式大小游戏，用于研究计算模型的表达能力。&lt;h4&gt;背景&lt;/h4&gt;GMSC及其变体被用于逻辑描述各种计算框架，如图神经网络、普通神经网络和分布式计算。&lt;h4&gt;目的&lt;/h4&gt;引入语义游戏和公式大小游戏，以研究GMSC的等价类以及这些类在给定的GMSC程序大小下的等价性。&lt;h4&gt;方法&lt;/h4&gt;通过引入新的语义游戏和公式大小游戏，展示了这些游戏如何描述GMSC程序等价类之间的关系。&lt;h4&gt;主要发现&lt;/h4&gt;公式大小游戏可以用来研究被描述的计算机模型的表达能力；GMSC在词上具有与确定性线性带限制图灵机（确定性线性界限自动机）相同的表达能力。&lt;h4&gt;结论&lt;/h4&gt;公式大小游戏是研究GMSC及其变体表达能力的一个有效工具，同时GMSC的强大表达能力与确定性线性带限制图灵机相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graded modal substitution calculus (GMSC) and its variants has been used forlogical characterizations of various computing frameworks such as graph neuralnetworks, ordinary neural networks and distributed computing. In this paper weintroduce two different semantic games and formula size game for graded modalsubstitution calculus and its variants. Ultimately, we show that the formulasize game characterizes the equivalence of classes of pointed Kripke models upto programs of GMSC of given size. Thus, the formula size game can be used tostudy the expressive power mentioned characterized classes of computing models.Moreover, we show that over words GMSC has the same expressive power asdeterministic linearly tape-bounded Turing machines also known as deterministiclinear bounded automata.</description>
      <author>example@mail.com (Veeti Ahvonen, Reijo Jaakkola, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07966v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem</title>
      <link>http://arxiv.org/abs/2505.08148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了14,904个定制GPT模型，评估了它们对七种可利用威胁的易受攻击性，并引入了一个多指标排名系统来考察定制GPT的流行度与其关联的安全风险之间的关系。&lt;h4&gt;背景&lt;/h4&gt;许多用户使用基于GPT的语言模型执行各种任务，定制GPT模型因其特殊需求而越来越受欢迎，但同时也引发了安全漏洞的担忧。&lt;h4&gt;目的&lt;/h4&gt;研究定制GPT模型的安全风险，并分析其与流行度的关系。&lt;h4&gt;方法&lt;/h4&gt;分析了14,904个定制GPT模型，评估了其对七种威胁的易受攻击性，并引入了多指标排名系统。&lt;h4&gt;主要发现&lt;/h4&gt;超过95%的定制GPT缺乏适当的安全保护，最常见的漏洞包括角色扮演攻击（96.51%）、系统提示泄露（92.20%）和钓鱼（91.22%）。此外，OpenAI的基础模型存在固有的安全弱点。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了增强安全措施和严格内容审查的紧迫需求，以确保GPT应用的安全部署。&lt;h4&gt;翻译&lt;/h4&gt;Millions of users leverage generative pretrained transformer (GPT)-based language models developed by leading model providers for a wide range of tasks. To support enhanced user interaction and customization, many platforms-such as OpenAI-now enable developers to create and publish tailored model instances, known as custom GPTs, via dedicated repositories or application stores. These custom GPTs empower users to browse and interact with specialized applications designed to meet specific needs. However, as custom GPTs see growing adoption, concerns regarding their security vulnerabilities have intensified. Existing research on these vulnerabilities remains largely theoretical, often lacking empirical, large-scale, and statistically rigorous assessments of associated risks. In this study, we analyze 14,904 custom GPTs to assess their susceptibility to seven exploitable threats, such as roleplay-based attacks, system prompt leakage, phishing content generation, and malicious code synthesis, across various categories and popularity tiers within the OpenAI marketplace. We introduce a multi-metric ranking system to examine the relationship between a custom GPT's popularity and its associated security risks. Our findings reveal that over 95% of custom GPTs lack adequate security protections. The most prevalent vulnerabilities include roleplay-based vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing (91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit inherent security weaknesses, which are often inherited or amplified in custom GPTs. These results highlight the urgent need for enhanced security measures and stricter content moderation to ensure the safe deployment of GPT-based applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/customgptvulnerability/custom-gpt-vulnerability-assessment&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millions of users leverage generative pretrained transformer (GPT)-basedlanguage models developed by leading model providers for a wide range of tasks.To support enhanced user interaction and customization, many platforms-such asOpenAI-now enable developers to create and publish tailored model instances,known as custom GPTs, via dedicated repositories or application stores. Thesecustom GPTs empower users to browse and interact with specialized applicationsdesigned to meet specific needs. However, as custom GPTs see growing adoption,concerns regarding their security vulnerabilities have intensified. Existingresearch on these vulnerabilities remains largely theoretical, often lackingempirical, large-scale, and statistically rigorous assessments of associatedrisks.  In this study, we analyze 14,904 custom GPTs to assess their susceptibilityto seven exploitable threats, such as roleplay-based attacks, system promptleakage, phishing content generation, and malicious code synthesis, acrossvarious categories and popularity tiers within the OpenAI marketplace. Weintroduce a multi-metric ranking system to examine the relationship between acustom GPT's popularity and its associated security risks.  Our findings reveal that over 95% of custom GPTs lack adequate securityprotections. The most prevalent vulnerabilities include roleplay-basedvulnerabilities (96.51%), system prompt leakage (92.20%), and phishing(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibitinherent security weaknesses, which are often inherited or amplified in customGPTs. These results highlight the urgent need for enhanced security measuresand stricter content moderation to ensure the safe deployment of GPT-basedapplications.</description>
      <author>example@mail.com (Sunday Oyinlola Ogundoyin, Muhammad Ikram, Hassan Jameel Asghar, Benjamin Zi Hao Zhao, Dali Kaafar)</author>
      <guid isPermaLink="false">2505.08148v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines</title>
      <link>http://arxiv.org/abs/2505.07857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 10 figures(including 6 graphs)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对乌尔都语特定意图检测的独特对比学习方法，该方法利用未标记的乌尔都语数据重新训练预训练的语言模型，以提高乌尔都语意图检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;尽管多种语言都开发了意图检测预测器，但乌尔都语这一第十大语言在该领域仍处于发展阶段。&lt;h4&gt;目的&lt;/h4&gt;提高乌尔都语意图检测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于预训练语言模型的对比学习方法，并探索了6种不同的语言模型和13种不同的相似度计算方法，构建了一个综合的端到端LLMPIA意图检测流程。&lt;h4&gt;主要发现&lt;/h4&gt;在ATIS和Web Queries数据集上，LLMPIA在4-way 1 shot和4-way 5 shot实验设置下分别达到了83.28%和98.25%的F1-Score，在Web Queries数据集上达到了76.23%和84.42%的F1-Score，并且在一个额外的案例研究中，LLMPIA比最先进的预测器高出53.55%的F1-Score。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有效提高了乌尔都语意图检测的性能，为乌尔都语在意图检测领域的进一步发展奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multifarious intent detection predictors are developed for differentlanguages, including English, Chinese and French, however, the field remainsunderdeveloped for Urdu, the 10th most spoken language. In the realm ofwell-known languages, intent detection predictors utilize the strategy offew-shot learning and prediction of unseen classes based on the model trainingon seen classes. However, Urdu language lacks few-shot strategy based intentdetection predictors and traditional predictors are focused on prediction ofthe same classes which models have seen in the train set. To empower Urdulanguage specific intent detection, this introduces a unique contrastivelearning approach that leverages unlabeled Urdu data to re-train pre-trainedlanguage models. This re-training empowers LLMs representation learning for thedownstream intent detection task. Finally, it reaps the combined potential ofpre-trained LLMs and the prototype-informed attention mechanism to create acomprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigmof proposed predictive pipeline, it explores the potential of 6 distinctlanguage models and 13 distinct similarity computation methods. The proposedframework is evaluated on 2 public benchmark datasets, namely ATIS encompassing5836 samples and Web Queries having 8519 samples. Across ATIS dataset under4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,respectively. In an additional case study on the Web Queries dataset under sameclasses train and test set settings, LLMPIA outperformed state-of-the-artpredictor by 53.55% F1-Score.</description>
      <author>example@mail.com (Faiza Hassan, Summra Saleem, Kashif Javed, Muhammad Nabeel Asim, Abdur Rehman, Andreas Dengel)</author>
      <guid isPermaLink="false">2505.07857v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey</title>
      <link>http://arxiv.org/abs/2505.08034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE COMPSAC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文主要探讨了生成式人工智能（GenAI）在智慧城市中的应用，特别是基于对话界面的GenAI在市民、运营者和规划者等关键用户类型中的应用。&lt;h4&gt;背景&lt;/h4&gt;随着物联网（IoT）和数字孪生技术的普及，为智慧城市提供了丰富的数据基础，旨在改善城市生活和运营。&lt;h4&gt;目的&lt;/h4&gt;研究GenAI在智慧城市中的具体应用，并探讨如何利用GenAI模型和技术在智慧城市的不同子系统内为不同用户类型提供定制化服务和统一界面。&lt;h4&gt;方法&lt;/h4&gt;本文对已提出的GenAI模型和技术进行了识别和回顾，并考虑了如何基于现有的城市记录、IoT数据流和城市数字孪生来构建GenAI。&lt;h4&gt;主要发现&lt;/h4&gt;GenAI能够通过处理多模态内容生成新的输出，如文本和模拟，从而显著提升传统AI分析预测的能力。&lt;h4&gt;结论&lt;/h4&gt;本文认为，这项工作代表了从智慧城市关键用户视角对GenAI技术在智慧城市中应用的首次全面总结。&lt;h4&gt;翻译&lt;/h4&gt;The paper mainly discusses the application of Generative AI (GenAI) in smart cities, especially the application of GenAI based on conversational interfaces for different user types such as citizens, operators, and planners.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of IoT in cities, combined with Digital Twins, creates arich data foundation for Smart Cities aimed at improving urban life andoperations. Generative AI (GenAI) significantly enhances this potential, movingbeyond traditional AI analytics and predictions by processing multimodalcontent and generating novel outputs like text and simulations. Usingspecialized or foundational models, GenAI's natural language abilities such asNatural Language Understanding (NLU) and Natural Language Generation (NLG) canpower tailored applications and unified interfaces, dramatically loweringbarriers for users interacting with complex smart city systems. In this paper,we focus on GenAI applications based on conversational interfaces within thecontext of three critical user archetypes in a Smart City - Citizens, Operatorsand Planners. We identify and review GenAI models and techniques that have beenproposed or deployed for various urban subsystems in the contexts of these userarchetypes. We also consider how GenAI can be built on the existing datafoundation of official city records, IoT data streams and Urban Digital Twins.We believe this work represents the first comprehensive summarization of GenAItechniques for Smart Cities from the lens of the critical users in a SmartCity.</description>
      <author>example@mail.com (Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj)</author>
      <guid isPermaLink="false">2505.08034v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Model Embedding-Based Semantic Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for the Workshop "Safely Leveraging Vision-Language  Foundation Models in Robotics: Challenges and Opportunities" at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了语义异常检测，通过利用最先进视觉基础模型的语义先验知识，直接在图像上进行操作。提出了一种框架，将运行时图像的局部视觉嵌入与被认为是安全且性能良好的名义场景数据库进行比较。&lt;h4&gt;背景&lt;/h4&gt;语义异常是自主系统中常见的视觉元素组合异常，可能导致系统推理失败。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，用于检测自主系统中的语义异常。&lt;h4&gt;方法&lt;/h4&gt;提出两种框架变体：一种使用基于原始网格的嵌入，另一种利用实例分割进行对象中心表示。为了提高鲁棒性，引入了一种简单的过滤机制来抑制假阳性。&lt;h4&gt;主要发现&lt;/h4&gt;在CARLA模拟的异常评估中，基于实例的方法结合过滤机制的性能与GPT-4o相当，同时提供了精确的异常定位。&lt;h4&gt;结论&lt;/h4&gt;基础模型中的视觉嵌入对于自主系统中的实时异常检测具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic anomalies are contextually invalid or unusual combinations offamiliar visual elements that can cause undefined behavior and failures insystem-level reasoning for autonomous systems. This work explores semanticanomaly detection by leveraging the semantic priors of state-of-the-art visionfoundation models, operating directly on the image. We propose a framework thatcompares local vision embeddings from runtime images to a database of nominalscenarios in which the autonomous system is deemed safe and performant. In thiswork, we consider two variants of the proposed framework: one using rawgrid-based embeddings, and another leveraging instance segmentation forobject-centric representations. To further improve robustness, we introduce asimple filtering mechanism to suppress false positives. Our evaluations onCARLA-simulated anomalies show that the instance-based method with filteringachieves performance comparable to GPT-4o, while providing precise anomalylocalization. These results highlight the potential utility of visionembeddings from foundation models for real-time anomaly detection in autonomoussystems.</description>
      <author>example@mail.com (Max Peter Ronecker, Matthew Foutter, Amine Elhafsi, Daniele Gammelli, Ihor Barakaiev, Marco Pavone, Daniel Watzenig)</author>
      <guid isPermaLink="false">2505.07998v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Development of a WAZOBIA-Named Entity Recognition System</title>
      <link>http://arxiv.org/abs/2505.07884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个针对尼日利亚三种主要语言（豪萨语、约鲁巴语和伊博语）的WAZOBIA-NER实体识别系统。&lt;h4&gt;背景&lt;/h4&gt;尽管计算语言学对非洲语言越来越感兴趣，但现有的NER系统主要关注英语、欧洲语言和一些全球语言，对资源匮乏的语言关注不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于尼日利亚三种主要语言的WAZOBIA-NER系统，以解决数据稀缺和语言多样性挑战。&lt;h4&gt;方法&lt;/h4&gt;研究首先为每种语言编制了标注数据集，然后探索了最新的机器学习技术，如条件随机场（CRF）和深度学习模型（如双向长短期记忆网络（BiLSTM）、双向编码器表示的Transformer（Bert）和循环神经网络（RNN）的微调）。系统还利用光学字符识别（OCR）技术将文本图像转换为机器可读文本。&lt;h4&gt;主要发现&lt;/h4&gt;系统在识别人、组织和地点三个实体方面取得了0.9511的精确度、0.9400的召回率、0.9564的F1分数和0.9301的准确率。模型在三种语言上进行了评估，精确度、召回率、F1分数和准确率是关键评估指标。&lt;h4&gt;结论&lt;/h4&gt;Wazobia-NER系统表明，使用当前的NLP框架和迁移学习为资源匮乏的非洲语言构建稳健的NER工具是可行的。&lt;h4&gt;翻译&lt;/h4&gt;Named Entity Recognition (NER) is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Named Entity Recognition NER is very crucial for various natural languageprocessing applications, including information extraction, machine translation,and sentiment analysis. Despite the ever-increasing interest in Africanlanguages within computational linguistics, existing NER systems focus mainlyon English, European, and a few other global languages, leaving a significantgap for under-resourced languages. This research presents the development of aWAZOBIA-NER system tailored for the three most prominent Nigerian languages:Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilationof annotated datasets for each language, addressing data scarcity andlinguistic diversity challenges. Exploring the state-of-the-art machinelearning technique, Conditional Random Fields (CRF) and deep learning modelssuch as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional EncoderRepresentation from Transformers (Bert) and fine-tune with a Recurrent NeuralNetwork (RNN), the study evaluates the effectiveness of these approaches inrecognizing three entities: persons, organizations, and locations. The systemutilizes optical character recognition (OCR) technology to convert textualimages into machine-readable text, thereby enabling the Wazobia system toaccept both input text and textual images for extraction purposes. The systemachieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 inF1-score, and 0.9301 in accuracy. The model's evaluation was conducted acrossthree languages, with precision, recall, F1-score, and accuracy as keyassessment metrics. The Wazobia-NER system demonstrates that it is feasible tobuild robust NER tools for under-resourced African languages using current NLPframeworks and transfer learning.</description>
      <author>example@mail.com (S. E Emedem, I. E Onyenwe, E. G Onyedinma)</author>
      <guid isPermaLink="false">2505.07884v1</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>COMRECGC: Global Graph Counterfactual Explainer through Common Recourse</title>
      <link>http://arxiv.org/abs/2505.07081v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络（GNNs）的预测解释方法，特别是针对全局反事实解释中的共同回溯问题，并提出了一种有效的算法COMRECGC。&lt;h4&gt;背景&lt;/h4&gt;GNNs在多个领域如社交网络、分子生物学和推荐系统中得到广泛应用，但其黑盒性质需要通过解释方法来补充。&lt;h4&gt;目的&lt;/h4&gt;提出并解决GNNs全局反事实解释中的共同回溯问题，设计有效的算法来生成与所有输入拒绝图相关的接受图。&lt;h4&gt;方法&lt;/h4&gt;本文正式化了共同回溯解释问题，并设计了COMRECGC算法来解决。&lt;h4&gt;主要发现&lt;/h4&gt;COMRECGC算法在四个真实世界图数据集上与强基线进行了基准测试，表现出优于竞争对手的性能。&lt;h4&gt;结论&lt;/h4&gt;共同回溯解释与图反事实解释相比，在药物发现或计算生物学等应用中具有可比性或优越性，值得考虑。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ssggreg/comrecgc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have been widely used in various domains such associal networks, molecular biology, or recommendation systems. Concurrently,different explanations methods of GNNs have arisen to complement its black-boxnature. Explanations of the GNNs' predictions can be categorized into twotypes--factual and counterfactual. Given a GNN trained on binary classificationinto ''accept'' and ''reject'' classes, a global counterfactual explanationconsists in generating a small set of ''accept'' graphs relevant to all of theinput ''reject'' graphs. The transformation of a ''reject'' graph into an''accept'' graph is called a recourse. A common recourse explanation is a smallset of recourse, from which every ''reject'' graph can be turned into an''accept'' graph. Although local counterfactual explanations have been studiedextensively, the problem of finding common recourse for global counterfactualexplanation remains unexplored, particularly for GNNs. In this paper, weformalize the common recourse explanation problem, and design an effectivealgorithm, COMRECGC, to solve it. We benchmark our algorithm against strongbaselines on four different real-world graphs datasets and demonstrate thesuperior performance of COMRECGC against the competitors. We also compare thecommon recourse explanations to the graph counterfactual explanation, showingthat common recourse explanations are either comparable or superior, makingthem worth considering for applications such as drug discovery or computationalbiology.</description>
      <author>example@mail.com (Gregoire Fournier, Sourav Medya)</author>
      <guid isPermaLink="false">2505.07081v2</guid>
      <pubDate>Wed, 14 May 2025 14:16:09 +0800</pubDate>
    </item>
    <item>
      <title>Recent results of (semi)leptonic decays of charm hadrons at BESIII</title>
      <link>http://arxiv.org/abs/2505.05123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  contribution to the 2025 Electroweak session of the 59th Rencontres  de Moriond&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文报告了BESIII实验最近对 charm 等离子子 (半)轻子衰变的测量结果。&lt;h4&gt;背景&lt;/h4&gt;包括 D^+ → μ^+ν_μ, D^+ → τ^+ν_τ, D^+ → η′ℓ^+ν_ℓ, 以及 D^{0(+)} → K_ℓ^+ν_ℓ (其中 ℓ=e, μ) 的衰变。&lt;h4&gt;目的&lt;/h4&gt;这些测量提供了最精确或首次对 CKM 矩阵元素 |V_{cs(d)}|，衰变常数 f_{D^+}，以及强子形式因子 f_+^{D^+→η′}(0) 和 f_+^{D→K}(0) 的确定。&lt;h4&gt;方法&lt;/h4&gt;利用了机器学习中的图神经网络首次观察到了稀有β衰变 Λ_c^+ → n e^+ν_e。&lt;h4&gt;主要发现&lt;/h4&gt;测量提供了对 CKM 矩阵元素、衰变常数和强子形式因子的精确测定，并测试了电子-μ子以及τ-μ的轻子味 universality。&lt;h4&gt;结论&lt;/h4&gt;论文通过BESIII实验的测量结果，对 charm 等离子子的轻子衰变特性有了更深入的了解，并通过机器学习方法实现了对稀有β衰变的首次观察。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this talk, we report the recent measurements of (semi)leptonic decays ofcharm mesons from the BESIII experiment, including $D^+\to\mu^+\nu_\mu$,$D^+\to\tau^+\nu_\tau$, $D^+\to\eta^\prime\ell^+\nu_\ell$, and $D^{0(+)}\to\barK\ell^+\nu_\ell$ (where $\ell=e, \mu$). These measurements provide the mostprecise or first determinations to date of the CKM matrix elements$|V_{cs(d)}|$, the decay constant $f_{D^+}$, and the hadronic form factors$f_+^{D^+\to \eta^\prime}(0)$ and $f_+^{D\to \bar K}(0)$. Lepton flavoruniversality of $e-\mu$ and $\tau-\mu$ are also tested with these decays.Additionally, we present the first observation of the rare beta decay$\Lambda_c^+\to ne^+\nu_e$ with machine learning of Graph Neural Network.</description>
      <author>example@mail.com (Xiang Pan)</author>
      <guid isPermaLink="false">2505.05123v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
  <item>
      <title>Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions</title>
      <link>http://arxiv.org/abs/2505.07611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了147项关于基于视觉的交通事故预测（Vision-TAA）的研究，重点关注监督学习、无监督学习和混合深度学习模型在事故预测中的应用，以及使用真实世界和合成数据集的方法。&lt;h4&gt;背景&lt;/h4&gt;交通事故预测和检测对于提高道路安全至关重要，基于视觉的交通事故预测在深度学习时代成为一种有前景的方法。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供关于Vision-TAA系统发展的基础参考，以促进道路安全和交通管理。&lt;h4&gt;方法&lt;/h4&gt;本文将现有方法分为四种主要方法：基于图像和视频特征的预测、基于时空特征的预测、场景理解和多模态数据融合。&lt;h4&gt;主要发现&lt;/h4&gt;尽管这些方法显示出巨大的潜力，但数据稀缺、在复杂场景中泛化能力有限和实时性能限制等问题仍然普遍存在。&lt;h4&gt;结论&lt;/h4&gt;本文强调了未来研究的机遇，包括多模态数据融合、自监督学习和基于Transformer架构的集成，以提高预测准确性和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic accident prediction and detection are critical for enhancing roadsafety,and vision-based traffic accident anticipation (Vision-TAA) has emergedas a promising approach in the era of deep learning.This paper reviews 147recent studies,focusing on the application of supervised,unsupervised,andhybrid deep learning models for accident prediction,alongside the use ofreal-world and synthetic datasets.Current methodologies are categorized intofour key approaches: image and video feature-based prediction, spatiotemporalfeature-based prediction, scene understanding,and multimodal data fusion.Whilethese methods demonstrate significant potential,challenges such as datascarcity,limited generalization to complex scenarios,and real-time performanceconstraints remain prevalent. This review highlights opportunities for futureresearch,including the integration of multimodal data fusion, self-supervisedlearning,and Transformer-based architectures to enhance prediction accuracy andscalability.By synthesizing existing advancements and identifying criticalgaps, this paper provides a foundational reference for developing robust andadaptive Vision-TAA systems,contributing to road safety and traffic management.</description>
      <author>example@mail.com (Yi Zhang, Wenye Zhou, Ruonan Lin, Xin Yang, Hao Zheng)</author>
      <guid isPermaLink="false">2505.07611v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DanceGRPO: Unleashing GRPO on Visual Generation</title>
      <link>http://arxiv.org/abs/2505.07818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://dancegrpo.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DanceGRPO是一个统一的框架，将Group Relative Policy Optimization (GRPO)应用于视觉生成，能够跨多种生成范式、任务、基础模型和奖励模型无缝适应。&lt;h4&gt;背景&lt;/h4&gt;视觉内容创建中，将模型输出与人类偏好对齐是一个关键挑战。现有的基于强化学习的视觉生成方法存在与ODEs采样范式不兼容、大规模训练不稳定和视频生成缺乏验证等问题。&lt;h4&gt;目的&lt;/h4&gt;提出DanceGRPO，旨在提供一个统一的强化学习算法，以解决上述问题，并实现视觉生成中的高效反馈。&lt;h4&gt;方法&lt;/h4&gt;DanceGRPO将GRPO应用于视觉生成，支持扩散模型和rectified flows两种生成范式，以及文本到图像、文本到视频、图像到视频三种任务。它使用四种基础模型和五种奖励模型，包括图像/视频美学、文本-图像对齐、视频运动质量等。&lt;h4&gt;主要发现&lt;/h4&gt;DanceGRPO在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准测试中优于基线，提升可达181%。它不仅稳定了复杂视频生成的策略优化，还能更好地捕捉去噪轨迹，并从稀疏的二进制反馈中学习。&lt;h4&gt;结论&lt;/h4&gt;DanceGRPO是一个鲁棒且通用的解决方案，可以扩展视觉生成中的强化学习从人类反馈（RLHF）任务，为强化学习和视觉合成之间的和谐提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;DanceGRPO是一个将Group Relative Policy Optimization (GRPO)应用于视觉生成的统一框架，它能够跨多种生成范式、任务、基础模型和奖励模型无缝适应。在视觉内容创建中，将模型输出与人类偏好对齐是一个关键挑战。现有的基于强化学习的视觉生成方法存在与ODEs采样范式不兼容、大规模训练不稳定和视频生成缺乏验证等问题。本文提出了DanceGRPO，旨在解决这些问题，并实现视觉生成中的高效反馈。DanceGRPO将GRPO应用于视觉生成，支持扩散模型和rectified flows两种生成范式，以及文本到图像、文本到视频、图像到视频三种任务。它使用四种基础模型和五种奖励模型，包括图像/视频美学、文本-图像对齐、视频运动质量等。在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准测试中，DanceGRPO优于基线，提升可达181%。它不仅稳定了复杂视频生成的策略优化，还能更好地捕捉去噪轨迹，并从稀疏的二进制反馈中学习。DanceGRPO是一个鲁棒且通用的解决方案，可以扩展视觉生成中的强化学习从人类反馈（RLHF）任务，为强化学习和视觉合成之间的和谐提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent breakthroughs in generative models-particularly diffusion models andrectified flows-have revolutionized visual content creation, yet aligning modeloutputs with human preferences remains a critical challenge. Existingreinforcement learning (RL)-based methods for visual generation face criticallimitations: incompatibility with modern Ordinary Differential Equations(ODEs)-based sampling paradigms, instability in large-scale training, and lackof validation for video generation. This paper introduces DanceGRPO, the firstunified framework to adapt Group Relative Policy Optimization (GRPO) to visualgeneration paradigms, unleashing one unified RL algorithm across two generativeparadigms (diffusion models and rectified flows), three tasks (text-to-image,text-to-video, image-to-video), four foundation models (Stable Diffusion,HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/videoaesthetics, text-image alignment, video motion quality, and binary reward). Toour knowledge, DanceGRPO is the first RL-based unified framework capable ofseamless adaptation across diverse generative paradigms, tasks, foundationalmodels, and reward models. DanceGRPO demonstrates consistent and substantialimprovements, which outperform baselines by up to 181% on benchmarks such asHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only canstabilize policy optimization for complex video generation, but also enablesgenerative policy to better capture denoising trajectories for Best-of-Ninference scaling and learn from sparse binary feedback. Our results establishDanceGRPO as a robust and versatile solution for scaling Reinforcement Learningfrom Human Feedback (RLHF) tasks in visual generation, offering new insightsinto harmonizing reinforcement learning and visual synthesis. The code will bereleased.</description>
      <author>example@mail.com (Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo)</author>
      <guid isPermaLink="false">2505.07818v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.06991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文报告了ACVLAB团队为ICRA 2025 GOOSE 2D语义分割挑战赛开发的语义分割框架，该框架在真实世界条件下将户外场景解析为九个语义类别。&lt;h4&gt;背景&lt;/h4&gt;该框架旨在解决户外场景中由于光照不一致导致的图像分割问题。&lt;h4&gt;目的&lt;/h4&gt;提高语义分割的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;该框架采用Swin Transformer骨干网络，并增强其空间泛化能力；引入了旋转位置编码（RoPE）以增强空间信息；设计了色彩偏移估计与校正模块以补偿自然环境中的光照不一致；采用基于分位数的方法进行降噪，降低误差最大的2.5%像素的影响。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在GOOSE官方测试集上实现了平均交并比（mIoU）0.848，证明了色彩校正、位置编码和误差感知降噪在鲁棒语义分割中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该框架结合了多种技术，在真实世界条件下的户外场景语义分割中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This report presents our semantic segmentation framework developed by teamACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, whichfocuses on parsing outdoor scenes into nine semantic categories underreal-world conditions. Our method integrates a Swin Transformer backboneenhanced with Rotary Position Embedding (RoPE) for improved spatialgeneralization, alongside a Color Shift Estimation-and-Correction moduledesigned to compensate for illumination inconsistencies in naturalenvironments. To further improve training stability, we adopt a quantile-baseddenoising strategy that downweights the top 2.5\% of highest-error pixels,treating them as noise and suppressing their influence during optimization.Evaluated on the official GOOSE test set, our approach achieved a meanIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness ofcombining color correction, positional encoding, and error-aware denoising inrobust semantic segmentation.</description>
      <author>example@mail.com (Chih-Chung Hsu, I-Hsuan Wu, Wen-Hai Tseng, Ching-Heng Cheng, Ming-Hsuan Wu, Jin-Hui Jiang, Yu-Jou Hsiao)</author>
      <guid isPermaLink="false">2505.06991v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A class of distributed automata that contains the modal mu-fragment</title>
      <link>http://arxiv.org/abs/2505.07816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文将分阶模态μ-演算的μ-片段翻译成一类分布式消息传递自动机。&lt;h4&gt;背景&lt;/h4&gt;在逻辑和计算模型领域，研究分阶模态μ-演算与分布式消息传递自动机之间的关系。&lt;h4&gt;目的&lt;/h4&gt;探索分阶模态μ-演算在分布式消息传递自动机中的表达能力和应用。&lt;h4&gt;方法&lt;/h4&gt;通过翻译技术将分阶模态μ-演算的μ-片段转换为分布式消息传递自动机。&lt;h4&gt;主要发现&lt;/h4&gt;作为推论，本文获得了一个关于循环图神经网络在实数和分阶模态替换演算下具有相同表达能力定理的替代证明。&lt;h4&gt;结论&lt;/h4&gt;分阶模态μ-演算和分布式消息传递自动机之间存在密切的联系，可以相互转换，且在逻辑单形二阶逻辑MSO的约束下具有相同的表达能力。&lt;h4&gt;翻译&lt;/h4&gt;本文实现了分阶模态μ-演算的μ-片段到分布式消息传递自动机的翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper gives a translation from the $\mu$-fragment of the graded modal$\mu$-calculus to a class of distributed message-passing automata. As acorollary, we obtain an alternative proof for a theorem from\cite{ahvonen_neurips} stating that recurrent graph neural networks workingwith reals and graded modal substitution calculus have the same expressivepower in restriction to the logic monadic second-order logic MSO.</description>
      <author>example@mail.com (Veeti Ahvonen, Damian Heiman, Antti Kuusisto)</author>
      <guid isPermaLink="false">2505.07816v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.06951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, International Conference on Robotics and  Automation(ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对热图像语义分割的跨光谱无监督领域自适应（UDA）方法，通过提高互补信息交换和增强夜间场景下的性能，解决了传统方法在领域自适应中的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶领域，热图像语义分割由于能在恶劣视觉条件下提供稳健的场景理解而成为关键研究领域。然而，由于缺乏标注的热图像数据集，无监督领域自适应方法成为解决这一问题的有效途径。&lt;h4&gt;目的&lt;/h4&gt;旨在通过无监督领域自适应方法解决热图像语义分割中标签数据不足的问题，并提高模型在不同领域的适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新型的掩码互学习策略，通过在光谱模型间选择性传递结果并屏蔽不确定区域，促进互补信息的交换。同时，引入了一种新型的原型自监督损失函数，用于增强夜间场景下热分割模型的表现，解决RGB预训练网络在低光照条件下知识迁移能力不足的问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在实验中表现出比之前UDA方法更高的性能，并且与最先进的监督方法具有可比的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法有效提高了热图像语义分割的性能，特别是在夜间场景和领域自适应方面。&lt;h4&gt;翻译&lt;/h4&gt;In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In autonomous driving, thermal image semantic segmentation has emerged as acritical research area, owing to its ability to provide robust sceneunderstanding under adverse visual conditions. In particular, unsuperviseddomain adaptation (UDA) for thermal image segmentation can be an efficientsolution to address the lack of labeled thermal datasets. Nevertheless, sincethese methods do not effectively utilize the complementary information betweenRGB and thermal images, they significantly decrease performance during domainadaptation. In this paper, we present a comprehensive study on cross-spectralUDA for thermal image semantic segmentation. We first propose a novel maskedmutual learning strategy that promotes complementary information exchange byselectively transferring results between each spectral model while masking outuncertain regions. Additionally, we introduce a novel prototypicalself-supervised loss designed to enhance the performance of the thermalsegmentation model in nighttime scenarios. This approach addresses thelimitations of RGB pre-trained networks, which cannot effectively transferknowledge under low illumination due to the inherent constraints of RGBsensors. In experiments, our method achieves higher performance over previousUDA methods and comparable performance to state-of-the-art supervised methods.</description>
      <author>example@mail.com (Seokjun Kwon, Jeongmin Shin, Namil Kim, Soonmin Hwang, Yukyung Choi)</author>
      <guid isPermaLink="false">2505.06951v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.07398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DepthFusion的深度感知混合特征融合策略，用于提高LiDAR相机3D目标检测器的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的3D目标检测器主要关注特征融合，但忽略了深度因素在设计融合策略时的重要性。&lt;h4&gt;目的&lt;/h4&gt;通过统计分析和可视化，观察不同模态在不同深度下的作用，并提出一种新的融合策略。&lt;h4&gt;方法&lt;/h4&gt;提出了一种DepthFusion策略，通过在全局和局部级别引入深度编码来引导点云和RGB图像模态的权重。具体包括Depth-GFusion模块和Depth-LFusion模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过统计分析和可视化发现，不同模态在深度变化时扮演不同的角色。&lt;h4&gt;结论&lt;/h4&gt;DepthFusion方法在nuScenes和KITTI数据集上的实验结果表明，该方法优于现有方法，并且对各种类型的损坏更加鲁棒。&lt;h4&gt;翻译&lt;/h4&gt;State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art LiDAR-camera 3D object detectors usually focus on featurefusion. However, they neglect the factor of depth while designing the fusionstrategy. In this work, we are the first to observe that different modalitiesplay different roles as depth varies via statistical analysis andvisualization. Based on this finding, we propose a Depth-Aware Hybrid FeatureFusion (DepthFusion) strategy that guides the weights of point cloud and RGBimage modalities by introducing depth encoding at both global and local levels.Specifically, the Depth-GFusion module adaptively adjusts the weights of imageBird's-Eye-View (BEV) features in multi-modal global features via depthencoding. Furthermore, to compensate for the information lost when transferringraw features to the BEV space, we propose a Depth-LFusion module, whichadaptively adjusts the weights of original voxel features and multi-view imagefeatures in multi-modal local features via depth encoding. Extensiveexperiments on the nuScenes and KITTI datasets demonstrate that our DepthFusionmethod surpasses previous state-of-the-art methods. Moreover, our DepthFusionis more robust to various kinds of corruptions, outperforming previous methodson the nuScenes-C dataset.</description>
      <author>example@mail.com (Mingqian Ji, Jian Yang, Shanshan Zhang)</author>
      <guid isPermaLink="false">2505.07398v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
      <link>http://arxiv.org/abs/2505.07396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了首个综合多模态城市数字孪生（UDT）基准数据集TUM2TWIN，旨在解决城市数字孪生创建中的挑战，并推动智能、数据驱动的城市环境的发展。&lt;h4&gt;背景&lt;/h4&gt;城市数字孪生（UDTs）对于城市管理及整合来自不同来源的复杂、异构数据变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了解决创建UDT过程中遇到的挑战，如获取精确的3D源数据、重建高保真3D模型、维护模型更新和确保与下游任务的互操作性等问题。&lt;h4&gt;方法&lt;/h4&gt;提出了TUM2TWIN数据集，包含地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，覆盖约100,000平方米，数据量达到767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，支持传感器分析及高级重建方法的发展。&lt;h4&gt;主要发现&lt;/h4&gt;TUM2TWIN数据集支持下游任务，如NeRF和Gaussian Splatting的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。&lt;h4&gt;结论&lt;/h4&gt;TUM2TWIN数据集为克服当前UDT创建的局限性、促进新的研究方向和实践解决方案奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：城市数字孪生（UDTs）已成为城市管理及整合来自不同来源的复杂、异构数据的关键。创建UDT涉及多个过程阶段的挑战，包括获取精确的3D源数据、重建高保真3D模型、维护模型更新和确保与下游任务的互操作性。当前数据集通常仅限于处理链的一部分，阻碍了全面UDT验证。为了解决这些挑战，我们引入了首个综合多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参照的、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测数据，涵盖约100,000平方米，数据量达到767GB。通过确保地理参照的室内外采集、高精度和多模态数据集成，该基准支持传感器分析及高级重建方法的发展。此外，我们探讨了下游任务，展示了TUM2TWIN的潜力，包括NeRF和Gaussian Splatting的新颖视图合成、太阳能潜力分析、点云语义分割和LoD3建筑重建。我们相信，这一贡献为克服当前UDT创建的局限性、促进新的研究方向和实践解决方案奠定了基础。项目可访问：https://tum2t.win&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Digital Twins (UDTs) have become essential for managing cities andintegrating complex, heterogeneous data from diverse sources. Creating UDTsinvolves challenges at multiple process stages, including acquiring accurate 3Dsource data, reconstructing high-fidelity 3D models, maintaining models'updates, and ensuring seamless interoperability to downstream tasks. Currentdatasets are usually limited to one part of the processing chain, hamperingcomprehensive UDTs validation. To address these challenges, we introduce thefirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.This dataset includes georeferenced, semantically aligned 3D models andnetworks along with various terrestrial, mobile, aerial, and satelliteobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, highaccuracy, and multimodal data integration, the benchmark supports robustanalysis of sensors and the development of advanced reconstruction methods.Additionally, we explore downstream tasks demonstrating the potential ofTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solarpotential analysis, point cloud semantic segmentation, and LoD3 buildingreconstruction. We are convinced this contribution lays a foundation forovercoming current limitations in UDT creation, fostering new researchdirections and practical solutions for smarter, data-driven urban environments.The project is available under: https://tum2t.win</description>
      <author>example@mail.com (Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi)</author>
      <guid isPermaLink="false">2505.07396v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models</title>
      <link>http://arxiv.org/abs/2505.07364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了跨模态医学图像翻译领域，探讨了生成合成数据在深度模型训练中的应用，并评估了生成数据在无监督异常检测任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，跨模态医学图像翻译领域的研究取得了丰硕成果，特别是基于生成对抗网络（GAN）的架构在处理大型多模态数据集稀缺的问题上表现出良好的性能。&lt;h4&gt;目的&lt;/h4&gt;旨在设计并比较不同的GAN框架，以从T1加权MRI数据生成合成[18F]氟代脱氧葡萄糖（FDG）PET图像，并评估这些合成数据在无监督异常检测模型训练中的应用。&lt;h4&gt;方法&lt;/h4&gt;设计了基于GAN的框架来生成合成FDG PET图像，进行了定性和定量视觉质量评估，并探索了这些合成数据在无监督异常检测模型训练中的影响。引入了针对无监督检测任务的合成FDG PET数据的诊断任务导向质量指标，并使用这些合成数据训练了一个结合Siamese自编码器深度表示学习和OC-SVM密度支持估计模型的无监督异常检测（UAD）模型。&lt;h4&gt;主要发现&lt;/h4&gt;最好的GAN模型能够生成与真实控制数据集在结构相似性（SSIM）和峰值信噪比（PSNR）值接近0.9和23.8的合成PET图像。在基于这些合成正常PET数据的最佳UAD模型上训练，达到了74%的敏感性。&lt;h4&gt;结论&lt;/h4&gt;基于GAN的模型最适合进行MR T1到FDG PET的翻译，优于transformer或扩散模型。此外，这些合成数据对UAD模型的训练和癫痫患者临床检查的评估也具有诊断价值。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了跨模态医学图像翻译领域，探讨了生成合成数据在深度模型训练中的应用，并评估了生成数据在无监督异常检测任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background and Objective. Research in the cross-modal medical imagetranslation domain has been very productive over the past few years in tacklingthe scarce availability of large curated multimodality datasets with thepromising performance of GAN-based architectures. However, only a few of thesestudies assessed task-based related performance of these synthetic data,especially for the training of deep models. Method. We design and comparedifferent GAN-based frameworks for generating synthetic brain[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We firstperform standard qualitative and quantitative visual quality evaluation. Then,we explore further impact of using these fake PET data in the training of adeep unsupervised anomaly detection (UAD) model designed to detect subtleepilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostictask-oriented quality metrics of the synthetic FDG PET data tailored to ourunsupervised detection task, then use these fake data to train a use case UADmodel combining a deep representation learning based on siamese autoencoderswith a OC-SVM density support estimation model. This model is trained on normalsubjects only and allows the detection of any variation from the pattern of thenormal population. We compare the detection performance of models trained on 35paired real MR T1 of normal subjects paired either on 35 true PET images or on35 synthetic PET images generated from the best performing generative models.Performance analysis is conducted on 17 exams of epilepsy patients undergoingsurgery. Results. The best performing GAN-based models allow generatingrealistic fake PET images of control subject with SSIM and PSNR values around0.9 and 23.8, respectively and in distribution (ID) with regard to the truecontrol dataset. The best UAD model trained on these synthetic normative PETdata allows reaching 74% sensitivity. Conclusion. Our results confirm thatGAN-based models are the best suited for MR T1 to FDG PET translation,outperforming transformer or diffusion models. We also demonstrate thediagnostic value of these synthetic data for the training of UAD models andevaluation on clinical exams of epilepsy patients. Our code and the normativeimage dataset are available.</description>
      <author>example@mail.com (Daria Zotova, Nicolas Pinon, Robin Trombetta, Romain Bouet, Julien Jung, Carole Lartizien)</author>
      <guid isPermaLink="false">2505.07364v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Gameplay Highlights Generation</title>
      <link>http://arxiv.org/abs/2505.07721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过自动生成引人注目的精彩片段，使玩家能够在社交媒体上分享他们的游戏体验，从而节省玩家时间并提高观众参与度。&lt;h4&gt;背景&lt;/h4&gt;传统的精彩片段检测技术如游戏引擎集成需要与游戏开发者进行昂贵的合作，而OCR技术需要针对每款游戏进行工程化，且可能无法跨游戏UI和不同语言通用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动检测游戏中的有趣事件并生成精彩片段的方法，同时提高检测准确率和跨游戏性能。&lt;h4&gt;方法&lt;/h4&gt;首先识别视频中发生有趣事件的区间，然后将这些区间拼接起来。开发了一个包含人类使用VIA视频标注器标注的有趣事件的内部游戏事件检测数据集。使用X-CLIP等多模态通用视频理解模型，并对其进行微调以提高分类性能。使用ONNX库实现跨平台推理，并提供后训练量化工具以减小模型大小和推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型可以从未见过的第一人称射击游戏游戏视频中以超过90%的准确率检测到有趣事件。此外，当与高资源游戏一起训练时，该模型在低资源游戏（小数据集）上表现显著更好，显示出迁移学习的迹象。&lt;h4&gt;结论&lt;/h4&gt;X-CLIP模型中的自然语言监督导致数据高效且性能优异的视频识别模型。&lt;h4&gt;翻译&lt;/h4&gt;本研究通过自动生成引人注目的精彩片段，使玩家能够在社交媒体上分享他们的游戏体验，从而节省玩家时间并提高观众参与度。传统的精彩片段检测技术如游戏引擎集成需要与游戏开发者进行昂贵的合作，而OCR技术需要针对每款游戏进行工程化，且可能无法跨游戏UI和不同语言通用。本研究开发了一种能够自动检测游戏中的有趣事件并生成精彩片段的方法，同时提高检测准确率和跨游戏性能。首先识别视频中发生有趣事件的区间，然后将这些区间拼接起来。开发了一个包含人类使用VIA视频标注器标注的有趣事件的内部游戏事件检测数据集。使用X-CLIP等多模态通用视频理解模型，并对其进行微调以提高分类性能。使用ONNX库实现跨平台推理，并提供后训练量化工具以减小模型大小和推理时间。微调后的模型可以从未见过的第一人称射击游戏游戏视频中以超过90%的准确率检测到有趣事件。此外，当与高资源游戏一起训练时，该模型在低资源游戏（小数据集）上表现显著更好，显示出迁移学习的迹象。X-CLIP模型中的自然语言监督导致数据高效且性能优异的视频识别模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we enable gamers to share their gaming experience on socialmedia by automatically generating eye-catching highlight reels from theirgameplay session Our automation will save time for gamers while increasingaudience engagement. We approach the highlight generation problem by firstidentifying intervals in the video where interesting events occur and thenconcatenate them. We developed an in-house gameplay event detection datasetcontaining interesting events annotated by humans using VIA video annotator.Traditional techniques for highlight detection such as game engine integrationrequires expensive collaboration with game developers. OCR techniques whichdetect patches of specific images or texts require expensive per gameengineering and may not generalize across game UI and different language. Wefinetuned a multimodal general purpose video understanding model such as X-CLIPusing our dataset which generalizes across multiple games in a genre withoutper game engineering. Prompt engineering was performed to improve theclassification performance of this multimodal model. Our evaluation showed thatsuch a finetuned model can detect interesting events in first person shootinggames from unseen gameplay footage with more than 90% accuracy. Moreover, ourmodel performed significantly better on low resource games (small dataset) whentrained along with high resource games, showing signs of transfer learning. Tomake the model production ready, we used ONNX libraries to enable crossplatform inference. These libraries also provide post training quantizationtools to reduce model size and inference time for deployment. ONNX runtimelibraries with DirectML backend were used to perform efficient inference onWindows OS. We show that natural language supervision in the X-CLIP model leadsto data efficient and highly performant video recognition models.</description>
      <author>example@mail.com (Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo)</author>
      <guid isPermaLink="false">2505.07721v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于对比学习的异构图异常检测模型EAGLE，通过对比异常节点与正常节点到局部上下文距离，提高了异常检测的效率。&lt;h4&gt;背景&lt;/h4&gt;图异常检测在多个实际场景中非常重要，已有基于深度学习的方法在性能上优于传统方法，但现有方法在效率上存在不足。&lt;h4&gt;目的&lt;/h4&gt;针对现有方法效率不足的问题，提出一种高效的异构图异常检测模型。&lt;h4&gt;方法&lt;/h4&gt;EAGLE模型首先在元路径级别上采样实例对进行对比学习，然后使用基于图自动编码器的模型无监督地学习节点嵌入，并结合判别器预测节点的异常分数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，EAGLE在三个异构网络数据集上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;EAGLE模型能够有效提高异构图异常检测的效率，并在实际数据集上取得了良好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/MIS.2022.3229147&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection is a popular and vital task in various real-worldscenarios, which has been studied for several decades. Recently, many studiesextending deep learning-based methods have shown preferable performance ongraph anomaly detection. However, existing methods are lack of efficiency thatis definitely necessary for embedded devices. Towards this end, we propose anEfficient Anomaly detection model on heterogeneous Graphs via contrastiveLEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms oftheir distances to the local context. The proposed method first samplesinstance pairs on meta path-level for contrastive learning. Then, a graphautoencoder-based model is applied to learn informative node embeddings in anunsupervised way, which will be further combined with the discriminator topredict the anomaly scores of nodes. Experimental results show that EAGLEoutperforms the state-of-the-art methods on three heterogeneous networkdatasets.</description>
      <author>example@mail.com (Jing Ren, Mingliang Hou, Zhixuan Liu, Xiaomei Bai)</author>
      <guid isPermaLink="false">2505.07508v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
      <link>http://arxiv.org/abs/2505.07301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过使用视频中的估计姿态来增强3D人体运动预测（HMP）模型的方法，以降低数据收集成本并提高模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的3D HMP模型训练需要昂贵的运动捕捉数据，而此类数据的收集成本限制了数据的多样性，导致模型在未见过的运动或主体上的泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过使用易于获取的视频中的估计姿态来增强HMP模型，以提高其泛化能力。&lt;h4&gt;方法&lt;/h4&gt;将来自单目视频的2D姿态经过处理转化为运动捕捉风格的3D运动，并通过额外学习使HMP模型适应测试域。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法对HMP模型有定性和定量的影响。&lt;h4&gt;结论&lt;/h4&gt;该方法可以有效地提高HMP模型的泛化能力，并减少对昂贵运动捕捉数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在3D人体运动预测（HMP）中，传统方法使用昂贵的运动捕捉数据训练HMP模型。然而，此类运动捕捉数据的数据收集成本限制了数据的多样性，导致对未见过的运动或主体的泛化能力差。为了解决这个问题，本文提出通过使用从易于获取的视频中估计的姿态来增强HMP模型的方法。通过从获得的运动中进行额外学习，HMP模型被适应到测试域。实验结果表明了我们的方法对HMP模型的定性和定量影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D Human Motion Prediction (HMP), conventional methods train HMP modelswith expensive motion capture data. However, the data collection cost of suchmotion capture data limits the data diversity, which leads to poorgeneralizability to unseen motions or subjects. To address this issue, thispaper proposes to enhance HMP with additional learning using estimated posesfrom easily available videos. The 2D poses estimated from the monocular videosare carefully transformed into motion capture-style 3D motions through ourpipeline. By additional learning with the obtained motions, the HMP model isadapted to the test domain. The experimental results demonstrate thequantitative and qualitative impact of our method.</description>
      <author>example@mail.com (Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita)</author>
      <guid isPermaLink="false">2505.07301v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Tagging fully hadronic exotic decays of the vectorlike $\mathbf{B}$ quark using a graph neural network</title>
      <link>http://arxiv.org/abs/2505.07769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对偶产生的矢量型B夸克衰变到新规范单态（伪）标量场Φ和b夸克的LHC前景，并使用混合深度学习模型来提高探测效率。&lt;h4&gt;背景&lt;/h4&gt;之前的研究中，作者已经探讨了机器学习增强的矢量型单态B夸克衰变到单态标量或伪标量场的研究。&lt;h4&gt;目的&lt;/h4&gt;旨在通过深度学习模型提高对这种衰变模式的探测能力。&lt;h4&gt;方法&lt;/h4&gt;采用包含图神经网络和深度神经网络的混合深度学习模型，以克服标准模型背景大和缺乏轻子末态的问题。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习分析流程的性能可以达到半轻子模式的水平，在B夸克完全异质衰变的情况下，即BR(B→bΦ) = 100%，在HL-LHC上可以达到约MB=1.8（2.4）TeV的发现（排除）范围。&lt;h4&gt;结论&lt;/h4&gt;混合深度学习模型能够显著提高对矢量型B夸克衰变到新规范单态场的研究效率，有望在HL-LHC上探测到这种衰变模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following up on our earlier study in [J. Bardhan et al., Machinelearning-enhanced search for a vectorlike singlet B quark decaying to a singletscalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], weinvestigate the LHC prospects of pair-produced vectorlike $B$ quarks decayingexotically to a new gauge-singlet (pseudo)scalar field $\Phi$ and a $b$ quark.After the electroweak symmetry breaking, the $\Phi$ decays predominantly to$gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature.Because of the large Standard Model background and the lack of leptonichandles, it is a difficult channel to probe. To overcome the challenge, weemploy a hybrid deep learning model containing a graph neural network followedby a deep neural network. We estimate that such a state-of-the-art deeplearning analysis pipeline can lead to a performance comparable to that in thesemi-leptonic mode, taking the discovery (exclusion) reach up to about$M_B=1.8\:(2.4)$~TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B\to b\Phi) = 100\%$.</description>
      <author>example@mail.com (Jai Bardhan, Tanumoy Mandal, Subhadip Mitra, Cyrin Neeraj, Mihir Rawat)</author>
      <guid isPermaLink="false">2505.07769v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Camera Control at the Edge with Language Models for Scene Understanding</title>
      <link>http://arxiv.org/abs/2505.06402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures. This work was presented and published at the 11th  IEEE International Conference on Control, Automation and Robotics (ICCAR) in  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了优化提示统一系统（OPUS），该系统利用大型语言模型（LLM）控制PTZ相机，并提供对自然环境的上下文理解。&lt;h4&gt;背景&lt;/h4&gt;传统的PTZ相机控制方法复杂且效率低。&lt;h4&gt;目的&lt;/h4&gt;OPUS系统旨在提高PTZ相机控制的成本效益，并实现与环境的高效交互。&lt;h4&gt;方法&lt;/h4&gt;OPUS系统通过高级相机控制API生成关键词，并使用合成数据通过监督微调（SFT）将知识从大型闭源语言模型转移到较小的模型。此外，通过将多个摄像头的数据进行文本描述，OPUS提高了环境意识。&lt;h4&gt;主要发现&lt;/h4&gt;在基准测试中，OPUS方法在传统语言模型技术和更复杂的提示方法中表现出色，比先进技术提高了35%，比像Gemini Pro这样的闭源模型任务准确率高出20%。&lt;h4&gt;结论&lt;/h4&gt;OPUS系统通过直观的自然语言界面简化了PTZ相机的操作，消除了显式编程的需要，并为与相机系统进行对话式交互提供了方法，这代表了用户控制和利用PTZ相机技术方面的重大进步。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种优化提示统一系统（OPUS），这是一个利用大型语言模型（LLM）控制PTZ相机、提供对自然环境的上下文理解的框架。为了实现这一目标，OPUS系统通过从高级相机控制API生成关键词并通过对合成数据的监督微调（SFT）将知识从大型闭源语言模型转移到较小的模型来提高成本效益。这使其能够在保持与GPT-4等大型模型相当性能的同时实现高效的边缘部署。OPUS通过将来自多个摄像头的数据进行文本描述来增强环境意识，消除了对专用感官标记的需求。在基准测试中，我们的方法在传统语言模型技术和更复杂的提示方法中表现出色，比先进技术提高了35%，比像Gemini Pro这样的闭源模型任务准确率高出20%。该系统展示了OPUS通过直观的自然语言界面简化PTZ相机操作的能力。这种方法消除了显式编程的需要，并为与相机系统进行对话式交互提供了方法，代表了用户控制和利用PTZ相机技术方面的重大进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present Optimized Prompt-based Unified System (OPUS), aframework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom(PTZ) cameras, providing contextual understanding of natural environments. Toachieve this goal, the OPUS system improves cost-effectiveness by generatingkeywords from a high-level camera control API and transferring knowledge fromlarger closed-source language models to smaller ones through SupervisedFine-Tuning (SFT) on synthetic data. This enables efficient edge deploymentwhile maintaining performance comparable to larger models like GPT-4. OPUSenhances environmental awareness by converting data from multiple cameras intotextual descriptions for language models, eliminating the need for specializedsensory tokens. In benchmark testing, our approach significantly outperformedboth traditional language model techniques and more complex prompting methods,achieving a 35% improvement over advanced techniques and a 20% higher taskaccuracy compared to closed-source models like Gemini Pro. The systemdemonstrates OPUS's capability to simplify PTZ camera operations through anintuitive natural language interface. This approach eliminates the need forexplicit programming and provides a conversational method for interacting withcamera systems, representing a significant advancement in how users can controland utilize PTZ camera technology.</description>
      <author>example@mail.com (Alexiy Buynitsky, Sina Ehsani, Bhanu Pallakonda, Pragyana Mishra)</author>
      <guid isPermaLink="false">2505.06402v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Generating Skyline Explanations for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.07635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，用于为图神经网络（GNN）生成子图解释，该方法同时优化多个可解释性度量。&lt;h4&gt;背景&lt;/h4&gt;现有的GNN解释方法通常计算子图（称为“解释子图”），以优化预定义的单个可解释性度量，如保真度或简洁性，这可能导致有偏的解释，无法全面解释GNN模型的输出。&lt;h4&gt;目的&lt;/h4&gt;引入天际线解释，这是一种GNN解释范式，旨在通过同时优化多个可解释性度量来识别k个解释子图。&lt;h4&gt;方法&lt;/h4&gt;1. 将天际线解释生成形式化为一个多目标优化问题，并追求逼近天际线集的解释子图。2. 设计了高效的算法，采用剥洋葱的方法，有策略地从节点邻居中移除边，随着它探索解释域，逐步改进解释，并保证质量。3. 进一步开发了一个算法来多样化解释，以提供更全面的视角。&lt;h4&gt;主要发现&lt;/h4&gt;1. 天际线解释生成问题是困难的。2. 所设计的算法在保证质量的同时，能够高效地生成解释。3. 通过多样化解释，可以提供更全面的视角。&lt;h4&gt;结论&lt;/h4&gt;使用真实世界的图，通过实验验证了所提出算法的有效性、效率和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法，用于为图神经网络（GNN）生成子图解释，该方法同时优化多个可解释性度量。现有GNN解释方法通常计算子图（称为“解释子图”），以优化预定义的单个可解释性度量，如保真度或简洁性，这可能导致有偏的解释，无法全面解释GNN模型的输出。我们引入天际线解释，这是一种GNN解释范式，旨在通过同时优化多个可解释性度量来识别k个解释子图。我们将天际线解释生成形式化为一个多目标优化问题，并追求逼近天际线集的解释子图。我们设计了一种高效的算法，采用剥洋葱的方法，有策略地从节点邻居中移除边，随着它探索解释域，逐步改进解释，并保证质量。我们进一步开发了一个算法来多样化解释，以提供更全面的视角。使用真实世界的图，我们通过实验验证了所提出算法的有效性、效率和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel approach to generate subgraph explanations forgraph neural networks GNNs that simultaneously optimize multiple measures forexplainability. Existing GNN explanation methods often compute subgraphs(called ``explanatory subgraphs'') that optimize a pre-defined, singleexplainability measure, such as fidelity or conciseness. This can lead tobiased explanations that cannot provide a comprehensive explanation to clarifythe output of GNN models. We introduce skyline explanation, a GNN explanationparadigm that aims to identify k explanatory subgraphs by simultaneouslyoptimizing multiple explainability measures. (1) We formulate skylineexplanation generation as a multi-objective optimization problem, and pursueexplanations that approximate a skyline set of explanatory subgraphs. We showthe hardness for skyline explanation generation. (2) We design efficientalgorithms with an onion-peeling approach that strategically removes edges fromneighbors of nodes of interests, and incrementally improves explanations as itexplores an interpretation domain, with provable quality guarantees. (3) Wefurther develop an algorithm to diversify explanations to provide morecomprehensive perspectives. Using real-world graphs, we empirically verify theeffectiveness, efficiency, and scalability of our algorithms.</description>
      <author>example@mail.com (Dazhuo Qiu, Haolai Che, Arijit Khan, Yinghui Wu)</author>
      <guid isPermaLink="false">2505.07635v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GLFM的多类点云异常检测方法，通过全局-局部特征匹配逐步分离不同类别间易混淆的数据。&lt;h4&gt;背景&lt;/h4&gt;随着产品类别的增加，单类无监督方法在计算和存储成本上的限制使得多类无监督方法成为必要。&lt;h4&gt;目的&lt;/h4&gt;为了解决正常点和异常点在不同类别数据中特征相似导致的多类方法性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;GLFM分为三个阶段：第一阶段提出异常合成管道，通过拉伸点云创建丰富的异常数据来优化特征提取器；第二阶段根据全局和局部特征分布建立全局和局部记忆库，减弱特征混淆对记忆库建立的影响；第三阶段利用测试数据与全局和局部记忆库的特征距离进行异常检测。&lt;h4&gt;主要发现&lt;/h4&gt;在MVTec 3D-AD、Real3D-AD和实际工业部件数据集上的实验表明，GLFM在点云异常检测方面具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;GLFM是一种有效的多类点云异常检测方法，能够提高异常检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：点云异常检测对于各种工业应用至关重要。由于产品类别的增加导致的巨大计算和存储成本限制了单类无监督方法的应用，需要发展多类无监督方法。然而，正常点和异常点在不同类别数据中的特征相似性导致了特征混淆问题，这极大地阻碍了多类方法的表现。因此，我们引入了一种名为GLFM的多类点云异常检测方法，利用全局-局部特征匹配逐步分离跨多个类别易混淆的数据。具体来说，GLFM分为三个阶段：第一阶段提出了一种异常合成管道，通过拉伸点云创建丰富的异常数据，用于优化点云特征提取器；第二阶段根据所有训练数据的全局和局部特征分布建立全局和局部记忆库，减弱了特征混淆对记忆库建立的影响；第三阶段利用测试数据通过其与全局和局部记忆库的特征距离进行异常检测。在MVTec 3D-AD、Real3D-AD和实际工业部件数据集上的大量实验展示了我们提出的GLFM在点云异常检测方面的优越性能。代码可在https://github.com/hustCYQ/GLFM-Multi-class-3DAD上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/hustCYQ/GLFM-Multi-class-3DAD&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud anomaly detection is essential for various industrialapplications. The huge computation and storage costs caused by the increasingproduct classes limit the application of single-class unsupervised methods,necessitating the development of multi-class unsupervised methods. However, thefeature similarity between normal and anomalous points from different classdata leads to the feature confusion problem, which greatly hinders theperformance of multi-class methods. Therefore, we introduce a multi-class pointcloud anomaly detection method, named GLFM, leveraging global-local featurematching to progressively separate data that are prone to confusion acrossmultiple classes. Specifically, GLFM is structured into three stages: Stage-Iproposes an anomaly synthesis pipeline that stretches point clouds to createabundant anomaly data that are utilized to adapt the point cloud featureextractor for better feature representation. Stage-II establishes the globaland local memory banks according to the global and local feature distributionsof all the training data, weakening the impact of feature confusion on theestablishment of the memory bank. Stage-III implements anomaly detection oftest data leveraging its feature distance from global and local memory banks.Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry partsdataset showcase our proposed GLFM's superior point cloud anomaly detectionperformance. The code is available athttps://github.com/hustCYQ/GLFM-Multi-class-3DAD.</description>
      <author>example@mail.com (Yuqi Cheng, Yunkang Cao, Dongfang Wang, Weiming Shen, Wenlong Li)</author>
      <guid isPermaLink="false">2505.07375v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation</title>
      <link>http://arxiv.org/abs/2505.07674v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究关注在复杂拓扑环境中预测网络流量的挑战，提出了一种结合图卷积网络（GCN）和门控循环单元（GRU）的时空建模方法，通过实验验证了该方法在复杂网络流量预测场景中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;网络流量预测在复杂拓扑环境中是一个挑战，需要有效的方法来捕捉空间依赖和时间演变。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合GCN和GRU的时空建模方法，以精确预测未来的网络流量模式。&lt;h4&gt;方法&lt;/h4&gt;该方法通过GCN捕捉网络节点间的空间依赖，GRU模拟流量数据的时间演变。通过在真实世界Abilene网络流量数据集上进行实验，将所提模型与多种深度学习方法进行比较，并进行了消融实验以检验不同组件对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提方法在多个指标上均优于其他方法，显示出在复杂网络流量预测场景中的稳健稳定性和强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的时空建模方法在复杂网络流量预测中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本研究关注于在复杂拓扑环境中预测网络流量的挑战。它引入了一种时空建模方法，该方法结合了图卷积网络（GCN）和门控循环单元（GRU）。GCN组件捕捉网络节点之间的空间依赖，而GRU组件模拟流量数据的时间演变。这种组合允许精确预测未来的流量模式。通过在真实世界的Abilene网络流量数据集上进行全面实验，验证了所提模型的有效性。该模型与几种流行的深度学习方法进行了基准测试。此外，进行了一系列消融实验来检验各种组件对性能的影响，包括图卷积层的数量变化、不同的时间建模策略以及构建邻接矩阵的方法。结果表明，所提方法在多个指标上均取得了优异的性能，显示出在复杂网络流量预测场景中的稳健稳定性和强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study focuses on the challenge of predicting network traffic withincomplex topological environments. It introduces a spatiotemporal modelingapproach that integrates Graph Convolutional Networks (GCN) with GatedRecurrent Units (GRU). The GCN component captures spatial dependencies amongnetwork nodes, while the GRU component models the temporal evolution of trafficdata. This combination allows for precise forecasting of future trafficpatterns. The effectiveness of the proposed model is validated throughcomprehensive experiments on the real-world Abilene network traffic dataset.The model is benchmarked against several popular deep learning methods.Furthermore, a set of ablation experiments is conducted to examine theinfluence of various components on performance, including changes in the numberof graph convolution layers, different temporal modeling strategies, andmethods for constructing the adjacency matrix. Results indicate that theproposed approach achieves superior performance across multiple metrics,demonstrating robust stability and strong generalization capabilities incomplex network traffic forecasting scenarios.</description>
      <author>example@mail.com (Nan Jiang, Wenxuan Zhu, Xu Han, Weiqiang Huang, Yumeng Sun)</author>
      <guid isPermaLink="false">2505.07674v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Across Fixed-Income Product Classes</title>
      <link>http://arxiv.org/abs/2505.07676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种在不同固定收益产品类别间进行折扣曲线迁移学习的框架。&lt;h4&gt;背景&lt;/h4&gt;由于从稀疏或噪声数据中估计折扣曲线的挑战，将核岭回归（KR）扩展到向量值设置。&lt;h4&gt;目的&lt;/h4&gt;提出了一种方法，通过经济原理引入额外的正则化项，以促进产品类别间扩散曲线的平滑性。&lt;h4&gt;方法&lt;/h4&gt;在向量值再生核希尔伯特空间（RKHS）中建立了一个凸优化问题，并展示了由可分离核引起的向量值RKHS范数的分解。此外，提供了向量值KR的高斯过程解释，以量化估计不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;引入的正则化项导致有效的可分离核结构，理论贡献是可分离核引起的向量值RKHS范数的分解。&lt;h4&gt;结论&lt;/h4&gt;示例表明，与单曲线估计相比，迁移学习显著提高了外推性能并缩小了置信区间。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种在不同固定收益产品类别间进行折扣曲线迁移学习的框架。由于从稀疏或噪声数据中估计折扣曲线的挑战，我们将核岭回归（KR）扩展到向量值设置。提出了一种方法，通过经济原理引入额外的正则化项，以促进产品类别间扩散曲线的平滑性。在向量值再生核希尔伯特空间（RKHS）中建立了一个凸优化问题，并展示了由可分离核引起的向量值RKHS范数的分解。此外，提供了向量值KR的高斯过程解释，以量化估计不确定性。示例表明，与单曲线估计相比，迁移学习显著提高了外推性能并缩小了置信区间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a framework for transfer learning of discount curves acrossdifferent fixed-income product classes. Motivated by challenges in estimatingdiscount curves from sparse or noisy data, we extend kernel ridge regression(KR) to a vector-valued setting, formulating a convex optimization problem in avector-valued reproducing kernel Hilbert space (RKHS). Each component of thesolution corresponds to the discount curve implied by a specific product class.We introduce an additional regularization term motivated by economicprinciples, promoting smoothness of spread curves between product classes, andshow that it leads to a valid separable kernel structure. A main theoreticalcontribution is a decomposition of the vector-valued RKHS norm induced byseparable kernels. We further provide a Gaussian process interpretation ofvector-valued KR, enabling quantification of estimation uncertainty.Illustrative examples demonstrate that transfer learning significantly improvesextrapolation performance and tightens confidence intervals compared tosingle-curve estimation.</description>
      <author>example@mail.com (Nicolas Camenzind, Damir Filipovic)</author>
      <guid isPermaLink="false">2505.07676v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning</title>
      <link>http://arxiv.org/abs/2505.07322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RealRep的通用SDR到HDR转换方法，用于处理现实场景中风格多样的SDR内容。&lt;h4&gt;背景&lt;/h4&gt;HDR-WCG技术越来越普及，对将SDR内容转换为HDR的需求日益增加。现有的方法主要依赖于固定的色调映射算子，对于处理具有多种风格的SDR输入不足。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，提出了一种名为RealRep的方法，可以处理现实场景中风格多样的SDR内容。&lt;h4&gt;方法&lt;/h4&gt;通过分离亮度（luminance）和色度（chrominance），分析不同风格内容之间的内在差异，并提出了一个解耦的多视角风格表示学习方法。该方法捕捉了不同风格中真实亮度和色度分布的先验指导，即使在SDR风格分布存在显著变化的情况下，从而建立了一个鲁棒的逆色调映射嵌入空间。此外，为了解决直接利用退化表示先验的困难，引入了退化域感知控制映射网络（DDACMNet），这是一个两阶段框架，通过控制感知归一化机制进行自适应分层映射。&lt;h4&gt;主要发现&lt;/h4&gt;RealRep在泛化能力和感知上忠实于HDR色域重建方面，一致优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;RealRep和DDACMNet能够有效地将SDR内容转换为HDR，为HDR内容的制作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高动态范围宽色域（HDR-WCG）技术越来越普及，对将标准动态范围（SDR）内容转换为HDR的需求日益增加。现有方法主要依赖于固定的色调映射算子，对于处理现实场景中常见的多种风格的SDR输入不足。为了解决这一挑战，我们提出了一种处理现实场景中风格多样的SDR内容的通用SDR到HDR方法，称为Realistic Style Disentangled Representation Learning（RealRep）。通过分离亮度（luminance）和色度（chrominance），我们分析了具有不同风格的内容之间的内在差异，并提出了一种解耦的多视角风格表示学习方法。这种方法捕捉了不同风格中真实亮度和色度分布的先验指导，即使在SDR风格分布存在显著变化的情况下，从而建立了一个鲁棒的逆色调映射嵌入空间。受直接利用退化表示先验的困难所启发，我们进一步引入了退化域感知控制映射网络（DDACMNet），这是一个两阶段框架，通过控制感知归一化机制进行自适应分层映射。大量的实验表明，RealRep在泛化能力和感知上忠实于HDR色域重建方面，一致优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becomingincreasingly prevalent, intensifying the demand for converting Standard DynamicRange (SDR) content to HDR. Existing methods primarily rely on fixed tonemapping operators, which are inadequate for handling SDR inputs with diversestyles commonly found in real-world scenarios. To address this challenge, wepropose a generalized SDR-to-HDR method that handles diverse styles inreal-world SDR content, termed Realistic Style Disentangled RepresentationLearning (RealRep). By disentangling luminance and chrominance, we analyze theintrinsic differences between contents with varying styles and propose adisentangled multi-view style representation learning method. This approachcaptures the guidance prior of true luminance and chrominance distributionsacross different styles, even when the SDR style distributions exhibitsignificant variations, thereby establishing a robust embedding space forinverse tone mapping. Motivated by the difficulty of directly utilizingdegradation representation priors, we further introduce the Degradation-DomainAware Controlled Mapping Network (DDACMNet), a two-stage framework thatperforms adaptive hierarchical mapping guided by a control-aware normalizationmechanism. DDACMNet dynamically modulates the mapping process viadegradation-conditioned hierarchical features, enabling robust adaptationacross diverse degradation domains. Extensive experiments show that RealRepconsistently outperforms state-of-the-art methods with superior generalizationand perceptually faithful HDR color gamut reconstruction.</description>
      <author>example@mail.com (Gang He, Siqi Wang, Kepeng Xu, Lin Zhang)</author>
      <guid isPermaLink="false">2505.07322v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes</title>
      <link>http://arxiv.org/abs/2505.07315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FederatedInvariant Features Learning (FedIFL)的联邦跨域诊断框架，用于解决工业数据稀缺导致的故障诊断模型训练难题。&lt;h4&gt;背景&lt;/h4&gt;由于工业数据稀缺，尤其是对于初创企业，独立训练全面的故障诊断模型存在困难；联邦学习能够在保证数据隐私的同时实现协同训练，因此成为一个理想的解决方案。&lt;h4&gt;目的&lt;/h4&gt;为了解决联邦诊断场景中标签空间不一致导致的问题，提高模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;FedIFL框架通过原型对比学习减轻客户端域偏移，同时确保本地模型能够以隐私友好的方式访问其他客户端的分布。此外，引入特征解耦机制来减轻跨客户端域偏移，并设计实例级联邦实例一致性损失来保证不同客户端之间不变特征的实例级一致性。还构建了联邦实例个性化损失和正交损失来区分特定特征与不变特征。&lt;h4&gt;主要发现&lt;/h4&gt;FedIFL框架在全局标签空间中实现了良好的泛化能力，能够在标签空间不一致的情况下为目标客户端的电动机驱动系统（MDS）提供准确的故障诊断。&lt;h4&gt;结论&lt;/h4&gt;FedIFL在联邦跨域诊断中表现出色，能够有效解决不一致故障模式的问题。&lt;h4&gt;翻译&lt;/h4&gt;Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of working conditions leads to variations in fault modes, resulting in inconsistent label spaces across different clients. In federated diagnostic scenarios, label space inconsistency leads to local models focusing on client-specific fault modes and causes local models from different clients to map different failure modes to similar feature representations, which weakens the aggregated global model's generalization. To tackle this issue, this article proposed a federated cross-domain diagnostic framework termed Federated Invariant Features Learning (FedIFL). In intra-client training, prototype contrastive learning mitigates intra-client domain shifts, subsequently, feature generating ensures local models can access distributions of other clients in a privacy-friendly manner. Besides, in cross-client training, a feature disentanglement mechanism is introduced to mitigate cross-client domain shifts, specifically, an instance-level federated instance consistency loss is designed to ensure the instance-level consistency of invariant features between different clients, furthermore, a federated instance personalization loss and an orthogonal loss are constructed to distinguish specific features from the invariant features. Eventually, the aggregated model achieves promising generalization among global label spaces, enabling accurate fault diagnosis for target clients' Motor Driven Systems (MDSs) with inconsistent label spaces. Experiments on real-world MDSs validate the effectiveness and superiority of FedIFL in federated cross-domain diagnosis with inconsistent fault modes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the scarcity of industrial data, individual equipment users,particularly start-ups, struggle to independently train a comprehensive faultdiagnosis model; federated learning enables collaborative training whileensuring data privacy, making it an ideal solution. However, the diversity ofworking conditions leads to variations in fault modes, resulting ininconsistent label spaces across different clients. In federated diagnosticscenarios, label space inconsistency leads to local models focus onclient-specific fault modes and causes local models from different clients tomap different failure modes to similar feature representations, which weakensthe aggregated global model's generalization. To tackle this issue, thisarticle proposed a federated cross-domain diagnostic framework termed FederatedInvariant Features Learning (FedIFL). In intra-client training, prototypecontrastive learning mitigates intra-client domain shifts, subsequently,feature generating ensures local models can access distributions of otherclients in a privacy-friendly manner. Besides, in cross-client training, afeature disentanglement mechanism is introduced to mitigate cross-client domainshifts, specifically, an instance-level federated instance consistency loss isdesigned to ensure the instance-level consistency of invariant features betweendifferent clients, furthermore, a federated instance personalization loss andan orthogonal loss are constructed to distinguish specific features that fromthe invariant features. Eventually, the aggregated model achieves promisinggeneralization among global label spaces, enabling accurate fault diagnosis fortarget clients' Motor Driven Systems (MDSs) with inconsistent label spaces.Experiments on real-world MDSs validate the effectiveness and superiority ofFedIFL in federated cross-domain diagnosis with inconsistent fault modes.</description>
      <author>example@mail.com (Zexiao Wang, Yankai Wang, Xiaoqiang Liao, Xinguo Ming, Weiming Shen)</author>
      <guid isPermaLink="false">2505.07315v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Chronocept: Instilling a Sense of Time in Machines</title>
      <link>http://arxiv.org/abs/2505.07637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 8 figures, 18 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Chronocept，这是一个用于建模时间有效性的基准，旨在解决人工智能在处理时间推理方面的困难。&lt;h4&gt;背景&lt;/h4&gt;人类认知与时间感（Chronoception）紧密相连，这种感知能力使我们能够判断事实的有效性和知识的时效性。尽管在视觉、语言和运动控制方面取得了进展，但人工智能在处理时间有效性方面仍然存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Chronocept，作为第一个将时间有效性建模为时间上的连续概率分布的基准。&lt;h4&gt;方法&lt;/h4&gt;Chronocept使用偏态正态曲线拟合语义分解的时间轴，捕捉出现、衰减和峰值相关性的细微模式。它包括两个数据集：Benchmark I（原子事实）和Benchmark II（多句段落）。通过标注显示高标注者间一致性（84%和89%）。基线预测曲线参数（位置、规模和偏度），实现可解释和可推广的学习，并优于基于分类的方法。&lt;h4&gt;主要发现&lt;/h4&gt;Chronocept在人工智能的时间推理方面填补了基础性的空白，支持知识基础、事实核查、检索增强生成（RAG）和主动代理等应用。&lt;h4&gt;结论&lt;/h4&gt;Chronocept通过提供对时间有效性的建模，为人工智能在时间推理方面的发展提供了新的可能性，并促进了相关领域的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;人类认知与时间感（Chronoception）紧密相连，这种感知能力使我们能够判断事实的有效性和知识的时效性。尽管在视觉、语言和运动控制方面取得了进展，但人工智能在处理时间有效性方面仍然存在挑战。本文介绍了Chronocept，这是一个用于建模时间有效性的基准，旨在解决人工智能在处理时间推理方面的困难。Chronocept使用偏态正态曲线拟合语义分解的时间轴，捕捉出现、衰减和峰值相关性的细微模式。它包括两个数据集：Benchmark I（原子事实）和Benchmark II（多句段落）。通过标注显示高标注者间一致性（84%和89%）。基线预测曲线参数（位置、规模和偏度），实现可解释和可推广的学习，并优于基于分类的方法。Chronocept在人工智能的时间推理方面填补了基础性的空白，支持知识基础、事实核查、检索增强生成（RAG）和主动代理等应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human cognition is deeply intertwined with a sense of time, known asChronoception. This sense allows us to judge how long facts remain valid andwhen knowledge becomes outdated. Despite progress in vision, language, andmotor control, AI still struggles to reason about temporal validity. Weintroduce Chronocept, the first benchmark to model temporal validity as acontinuous probability distribution over time. Using skew-normal curves fittedalong semantically decomposed temporal axes, Chronocept captures nuancedpatterns of emergence, decay, and peak relevance. It includes two datasets:Benchmark I (atomic facts) and Benchmark II (multi-sentence passages).Annotations show strong inter-annotator agreement (84% and 89%). Our baselinespredict curve parameters - location, scale, and skewness - enablinginterpretable, generalizable learning and outperforming classification-basedapproaches. Chronocept fills a foundational gap in AI's temporal reasoning,supporting applications in knowledge grounding, fact-checking,retrieval-augmented generation (RAG), and proactive agents. Code and data arepublicly available.</description>
      <author>example@mail.com (Krish Goel, Sanskar Pandey, KS Mahadevan, Harsh Kumar, Vishesh Khadaria)</author>
      <guid isPermaLink="false">2505.07637v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures</title>
      <link>http://arxiv.org/abs/2505.07070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了神经网络语言模型在训练用于预测下一个标记时如何获取语言结构。通过分析随机层次模型（RHM）生成的合成数据集，得出了神经网络性能的理论扩展规律。&lt;h4&gt;背景&lt;/h4&gt;研究者已经开发了一种基于数据相关性的表示学习理论，该理论解释了深度学习模型如何按层次结构逐层捕捉数据。&lt;h4&gt;目的&lt;/h4&gt;研究目的是扩展理论框架以考虑架构差异，并预测和验证卷积网络和Transformer模型在性能扩展方面的差异。&lt;h4&gt;方法&lt;/h4&gt;研究者通过随机层次模型（RHM）生成合成数据集，并比较了卷积网络和Transformer模型在这些数据集上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，卷积网络由于其结构通过局部性和权重共享与生成过程对齐，其性能扩展速度比依赖全局自注意力机制的Transformer模型更快。&lt;h4&gt;结论&lt;/h4&gt;这一发现阐明了神经网络扩展规律背后的架构偏差，并强调了表示学习是如何由模型架构和数据统计属性之间的相互作用所塑造的。&lt;h4&gt;翻译&lt;/h4&gt;摘要：神经网络语言模型在训练用于预测下一个标记时如何获取语言结构？我们通过推导神经网络在由随机层次模型（RHM）生成的合成数据集上的性能理论扩展规律来回答这个问题。RHM是一组概率上下文无关文法集合，旨在捕获自然语言的层次结构，同时保持可分析性。此前，我们已开发了一种基于数据相关性的表示学习理论，解释了深度学习模型如何按层次结构逐层捕捉数据。在这里，我们将我们的理论框架扩展以考虑架构差异。特别是，我们预测并经验性地验证了卷积网络（其结构通过局部性和权重共享与生成过程对齐）的性能扩展速度比依赖全局自注意力机制的Transformer模型更快。这一发现阐明了神经网络扩展规律背后的架构偏差，并突出了表示学习是如何由模型架构和数据统计属性之间的相互作用所塑造的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How do neural language models acquire a language's structure when trained fornext-token prediction? We address this question by deriving theoretical scalinglaws for neural network performance on synthetic datasets generated by theRandom Hierarchy Model (RHM) -- an ensemble of probabilistic context-freegrammars designed to capture the hierarchical structure of natural languagewhile remaining analytically tractable. Previously, we developed a theory ofrepresentation learning based on data correlations that explains how deeplearning models capture the hierarchical structure of the data sequentially,one layer at a time. Here, we extend our theoretical framework to account forarchitectural differences. In particular, we predict and empirically validatethat convolutional networks, whose structure aligns with that of the generativeprocess through locality and weight sharing, enjoy a faster scaling ofperformance compared to transformer models, which rely on global self-attentionmechanisms. This finding clarifies the architectural biases underlying neuralscaling laws and highlights how representation learning is shaped by theinteraction between model architecture and the statistical properties of data.</description>
      <author>example@mail.com (Francesco Cagnetta, Alessandro Favero, Antonio Sclocchi, Matthieu Wyart)</author>
      <guid isPermaLink="false">2505.07070v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Is MixIT Really Unsuitable for Correlated Sources? Exploring MixIT for Unsupervised Pre-training in Music Source Separation</title>
      <link>http://arxiv.org/abs/2505.07631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于MixIT的预训练在音乐源分离（MSS）中的应用，探讨了MixIT在MSS中的潜力和改进方法。&lt;h4&gt;背景&lt;/h4&gt;音乐源分离是一个高成本的过程，因此利用未标记数据进行预训练是一个有前景的方法。尽管MixIT这类无监督学习方法在一般声音分离中被探索，但在MSS中由于其隐含的源独立性假设而被忽视。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过MixIT预训练来提高MSS的性能，并探索MixIT在MSS中的潜力。&lt;h4&gt;方法&lt;/h4&gt;首先，使用MixIT在未经标记的野外部数据上进行模型预训练，然后在使用MUSDB18数据集进行监督的情况下进行微调。使用band-split TF-Locoformer模型进行实验，这是一种最先进MSS模型之一。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，尽管MixIT不假设任何源模型并且处理模糊性有困难，但它仍能在一定程度上分离乐器，显示出其在无监督预训练中的潜力。&lt;h4&gt;结论&lt;/h4&gt;MixIT预训练可以提高MSS的性能，优于从头开始训练的方法。&lt;h4&gt;翻译&lt;/h4&gt;In music source separation (MSS), obtaining isolated sources or stems is highly costly, making pre-training on unlabeled data a promising approach. Although source-agnostic unsupervised learning like mixture-invariant training (MixIT) has been explored in general sound separation, they have been largely overlooked in MSS due to its implicit assumption of source independence. We hypothesize, however, that the difficulty of applying MixIT to MSS arises from the ill-posed nature of MSS itself, where stem definitions are application-dependent and models lack explicit knowledge of what should or should not be separated, rather than from high inter-source correlation. While MixIT does not assume any source model and struggles with such ambiguities, our preliminary experiments show that it can still separate instruments to some extent, suggesting its potential for unsupervised pre-training. Motivated by these insights, this study investigates MixIT-based pre-training for MSS. We first pre-train a model on in-the-wild, unlabeled data from the Free Music Archive using MixIT, and then fine-tune it on MUSDB18 with supervision. Using the band-split TF-Locoformer, one of the state-of-the-art MSS models, we demonstrate that MixIT-based pre-training improves the performance over training from scratch.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In music source separation (MSS), obtaining isolated sources or stems ishighly costly, making pre-training on unlabeled data a promising approach.Although source-agnostic unsupervised learning like mixture-invariant training(MixIT) has been explored in general sound separation, they have been largelyoverlooked in MSS due to its implicit assumption of source independence. Wehypothesize, however, that the difficulty of applying MixIT to MSS arises fromthe ill-posed nature of MSS itself, where stem definitions areapplication-dependent and models lack explicit knowledge of what should orshould not be separated, rather than from high inter-source correlation. WhileMixIT does not assume any source model and struggles with such ambiguities, ourpreliminary experiments show that it can still separate instruments to someextent, suggesting its potential for unsupervised pre-training. Motivated bythese insights, this study investigates MixIT-based pre-training for MSS. Wefirst pre-train a model on in-the-wild, unlabeled data from the Free MusicArchive using MixIT, and then fine-tune it on MUSDB18 with supervision. Usingthe band-split TF-Locoformer, one of the state-of-the-art MSS models, wedemonstrate that MixIT-based pre-training improves the performance overtraining from scratch.</description>
      <author>example@mail.com (Kohei Saijo, Yoshiaki Bando)</author>
      <guid isPermaLink="false">2505.07631v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用自动检索合成CAD模型的方法，生成高质量的3D标注数据，用于训练深度学习模型，从而提高模型性能并降低标注成本。&lt;h4&gt;背景&lt;/h4&gt;高层次的3D场景理解在许多应用中至关重要，但生成准确的3D标注数据对深度学习模型的发展构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究如何利用自动检索合成CAD模型的方法来生成高质量的3D标注数据，并验证这种方法在训练深度学习模型中的有效性。&lt;h4&gt;方法&lt;/h4&gt;采用与之前用于自动标注ScanNet场景中对象9D姿态和CAD模型相似的流程，应用于ScanNet++ v1数据集，以生成自动标注数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，使用自动获得的标注数据训练的深度学习模型不仅可行，而且性能优于使用手动标注数据训练的模型。验证了该方法在点云补全和单视图CAD模型检索与对齐两个任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注有潜力提高模型性能，同时显著降低标注成本，并将发布相关标注数据和训练模型以支持未来的3D场景理解研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高级3D场景理解在许多应用中至关重要。然而，生成准确3D标注的挑战使得深度学习模型的发展变得困难。我们转向最近在自动检索合成CAD模型方面的进展，并表明这种方法生成的数据可以用作训练监督深度学习模型的高质量真实标签。更具体地说，我们采用了与之前用于自动标注ScanNet场景中对象9D姿态和CAD模型相似的流程。这次，我们将它应用于之前缺乏此类标注的ScanNet++ v1数据集。我们的发现表明，不仅可以在这些自动获得的标注数据上训练深度学习模型，而且所得到的模型在性能上优于在手动标注数据上训练的模型。我们在两个不同的任务上验证了这一点：点云补全和单视图CAD模型检索与对齐。我们的结果强调了自动3D标注在提高模型性能的同时显著降低标注成本的可能性。为了支持3D场景理解的未来研究，我们将发布我们的标注，我们称之为SCANnotate++，以及我们的训练模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v3</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Wireless Link Scheduling with State-Augmented Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.07598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模无线自组织网络中的最优链路调度问题，旨在在保证链路传输公平性的前提下，最大化长期平均性能。&lt;h4&gt;背景&lt;/h4&gt;针对无线自组织网络中链路调度的问题，本文提出了一种基于图神经网络的优化调度策略。&lt;h4&gt;目的&lt;/h4&gt;目标是实现链路调度的长期平均性能最大化，同时确保每个链路的最小传输需求，以保证公平性。&lt;h4&gt;方法&lt;/h4&gt;利用图结构来表示链路冲突，构建了一个受约束的优化问题，并通过图神经网络（GNN）参数化调度策略。使用状态增强技术应对长期性能的挑战，通过将拉格朗日对偶变量作为调度策略的动态输入，训练GNN逐渐调整调度决策以实现最小传输需求。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟验证了所提策略的有效性，并在各种网络设置中将其性能与多个基线进行了比较。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于GNN的链路调度策略在保证公平性的同时，实现了长期平均性能的最大化，具有良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;We consider the problem of optimal link scheduling in large-scale wireless adhoc networks. We specifically aim for the maximum long-term average performance, subject to a minimum transmission requirement for each link to ensure fairness. With a graph structure utilized to represent the conflicts of links, we formulate a constrained optimization problem to learn the scheduling policy, which is parameterized with a graph neural network (GNN). To address the challenge of long-term performance, we use the state-augmentation technique. In particular, by augmenting the Lagrangian dual variables as dynamic inputs to the scheduling policy, the GNN can be trained to gradually adapt the scheduling decisions to achieve the minimum transmission requirements. We verify the efficacy of our proposed policy through numerical simulations and compare its performance with several baselines in various network settings.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of optimal link scheduling in large-scale wireless adhoc networks. We specifically aim for the maximum long-term averageperformance, subject to a minimum transmission requirement for each link toensure fairness. With a graph structure utilized to represent the conflicts oflinks, we formulate a constrained optimization problem to learn the schedulingpolicy, which is parameterized with a graph neural network (GNN). To addressthe challenge of long-term performance, we use the state-augmentationtechnique. In particular, by augmenting the Lagrangian dual variables asdynamic inputs to the scheduling policy, the GNN can be trained to graduallyadapt the scheduling decisions to achieve the minimum transmissionrequirements. We verify the efficacy of our proposed policy through numericalsimulations and compare its performance with several baselines in variousnetwork settings.</description>
      <author>example@mail.com (Romina Garcia Camargo, Zhiyang Wang, Navid NaderiAlizadeh, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2505.07598v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP是一种用于单目3D目标检测的深度估计方法，通过链式预测（CoP）来提高深度估计的准确性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;3D属性预测对于单目3D目标检测至关重要，其中深度估计是最具挑战性的部分，因为将2D图像映射到3D空间存在固有的歧义。&lt;h4&gt;目的&lt;/h4&gt;提出MonoCoP方法，通过条件预测和特征链式传播来提高深度估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP采用以下设计：使用轻量级属性网络（AN）学习每个3D属性的特征；构建显式的特征传播链；使用残差连接聚合特征，确保后续属性预测基于所有已处理的属性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MonoCoP在KITTI排行榜上达到了最先进的性能，并在Waymo和nuScenes frontal数据集上超过了现有方法。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP是一种有效的单目3D目标检测方法，能够显著提高深度估计的准确性，且无需额外数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v3</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study</title>
      <link>http://arxiv.org/abs/2505.07576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种在半导体制造领域应用的视觉异常检测（VAD）方法，通过利用MIIC数据集建立了一个基准，验证了现代VAD方法在该领域的有效性。&lt;h4&gt;背景&lt;/h4&gt;半导体制造是一个复杂的多阶段过程，自动视觉检测对于减少设备停机时间和控制成本至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需大量异常标记样本的VAD方法，避免昂贵的缺陷收集阶段，同时提供预测的解释。&lt;h4&gt;方法&lt;/h4&gt;通过利用MIIC数据集，建立了一个VAD在半导体领域的基准，并展示了现代VAD方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;现代VAD方法在半导体领域显示出良好的效果。&lt;h4&gt;结论&lt;/h4&gt;VAD方法在半导体制造领域具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing is a complex, multistage process. Automatedvisual inspection of Scanning Electron Microscope (SEM) images is indispensablefor minimizing equipment downtime and containing costs. Most previous researchconsiders supervised approaches, assuming a sufficient number of anomalouslylabeled samples. On the contrary, Visual Anomaly Detection (VAD), an emergingresearch domain, focuses on unsupervised learning, avoiding the costly defectcollection phase while providing explanations of the predictions. We introducea benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.Our results demonstrate the efficacy of modern VAD approaches in this field.</description>
      <author>example@mail.com (Manuel Barusco, Francesco Borsatti, Youssef Ben Khalifa, Davide Dalle Pezze, Gian Antonio Susto)</author>
      <guid isPermaLink="false">2505.07576v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies</title>
      <link>http://arxiv.org/abs/2505.07552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a long paper at the Educational Data Mining (EDM)  Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了教师课堂中视觉注意力的分布对学生学习参与度、成就和专业教师培训的重要影响，并提出了一种自动化处理流程，利用先进的人脸检测和识别技术来减少手动标注数据的需求。&lt;h4&gt;背景&lt;/h4&gt;虽然教师对学生的关注点对学生学习有重要影响，但推断教师关注的位置和学生不是一件容易的事。移动眼动追踪可以提供帮助，但需要大量手动标注。&lt;h4&gt;目的&lt;/h4&gt;减少手动标注数据，以识别教师关注的特定学生。&lt;h4&gt;方法&lt;/h4&gt;使用最先进的面部检测模型和特征嵌入训练面部识别模型，结合移动眼动追踪数据，并在课堂环境中进行迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同的课堂环境中评估了该方法，结果表明在U形和较小的教室中取得了最佳结果，准确率分别约为0.7和0.9。&lt;h4&gt;结论&lt;/h4&gt;该方法可以在不要求大量手动标注数据的情况下，以非侵入的方式处理教师的视觉注意力，有助于改进教学策略、增强课堂管理和提供专业教师发展的反馈。&lt;h4&gt;翻译&lt;/h4&gt;摘要：教师对课堂学生的视觉注意力和其分布对学生参与度、成就以及专业教师培训具有重要作用。尽管如此，推断教师关注的位置和对象并非易事。移动眼动追踪可以提供重要帮助，但仅使用移动眼动追踪需要大量的手动标注。为了解决这一局限性，我们提出了一种自动化处理流程的概念，该流程需要最少的手动标注数据来识别教师关注的特定学生。为此，我们利用最先进的面部检测模型和面部识别特征嵌入，在课堂环境中进行迁移学习，以训练面部识别模型，并将这些模型与来自移动眼动追踪器的教师的注视结合。我们使用从四个不同课堂收集的数据评估了我们的方法，结果表明，虽然在我们的所有课堂设置中都可以以合理的表现估计视觉关注的学生，但在U形和较小的教室中取得了最佳结果，准确率分别约为0.7和0.9。虽然我们没有评估我们的方法在师生互动中的应用，并且专注于技术方法的合理性，但鉴于我们的方法不需要大量的手动标注数据，并以非侵入的方式处理教师的视觉注意力，它可以帮助改进教学策略，增强课堂管理，并为专业教师发展提供反馈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Teachers' visual attention and its distribution across the students inclassrooms can constitute important implications for student engagement,achievement, and professional teacher training. Despite that, inferring theinformation about where and which student teachers focus on is not trivial.Mobile eye tracking can provide vital help to solve this issue; however, theuse of mobile eye tracking alone requires a significant amount of manualannotations. To address this limitation, we present an automated processingpipeline concept that requires minimal manually annotated data to recognizewhich student the teachers focus on. To this end, we utilize state-of-the-artface detection models and face recognition feature embeddings to train facerecognition models with transfer learning in the classroom context and combinethese models with the teachers' gaze from mobile eye trackers. We evaluated ourapproach with data collected from four different classrooms, and our resultsshow that while it is possible to estimate the visually focused students withreasonable performance in all of our classroom setups, U-shaped and smallclassrooms led to the best results with accuracies of approximately 0.7 and0.9, respectively. While we did not evaluate our method for teacher-studentinteractions and focused on the validity of the technical approach, as ourmethodology does not require a vast amount of manually annotated data andoffers a non-intrusive way of handling teachers' visual attention, it couldhelp improve instructional strategies, enhance classroom management, andprovide feedback for professional teacher development.</description>
      <author>example@mail.com (Efe Bozkir, Christian Kosel, Tina Seidel, Enkelejda Kasneci)</author>
      <guid isPermaLink="false">2505.07552v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling</title>
      <link>http://arxiv.org/abs/2505.07157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为HAMLET的跨语言医疗主题建模的图驱动架构，该架构利用大型语言模型（LLMs）来解决传统主题模型在处理语境细微差别、多义词和罕见词时的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统主题模型在处理语境细微差别、多义词和罕见词时存在困难，导致生成的主题缺乏连贯性和质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的方法，通过使用LLMs生成初始主题，并通过神经网络增强语义融合来精炼这些主题嵌入。&lt;h4&gt;方法&lt;/h4&gt;使用BERT和图神经网络（GNN）进行主题嵌入的精炼，并引入了一种新的计算相似性的方法，以进一步精炼主题嵌入并提取前k个主题。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用两个医疗数据集（一个英语和一个法语）的六个集合，HAMLET在主题建模方面是有效的。&lt;h4&gt;结论&lt;/h4&gt;HAMLET通过结合LLMs、BERT、SBERT和GNN等技术，能够有效地提高主题建模的质量和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的主题模型往往难以处理语境细微差别，无法充分处理多义词和罕见词。这种限制通常会导致缺乏连贯性和质量的主题。大型语言模型（LLMs）可以通过生成一组初始主题来缓解这个问题。然而，这些原始主题通常缺乏精炼和代表性，导致冗余且缺乏词汇相似性，以及可解释性降低。本文介绍了一种名为HAMLET的跨语言医疗主题建模的图驱动架构，它使用LLMs。所提出的方法利用神经网络增强语义融合来精炼LLM生成的主题嵌入。这种方法不依赖于仅统计共现或人类解释来从文档语料库中提取主题，而是引入了一种主题嵌入精炼方法，该方法使用双向编码器表示从Transformer（BERT）和图神经网络（GNN）。在主题生成后，采用BERT和Sentence-BERT（SBERT）相结合的混合技术进行嵌入。使用GNN进一步精炼主题表示，GNN在文档、主题、单词、相似主题和相似单词之间建立连接。引入了一种新的计算相似性的方法。因此，精炼了主题嵌入，并提取了前k个主题。使用两个医疗数据集（一个英语和一个法语）的六个集合进行了实验。结果表明，HAMLET在主题建模方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional topic models often struggle with contextual nuances and fail toadequately handle polysemy and rare words. This limitation typically results intopics that lack coherence and quality. Large Language Models (LLMs) canmitigate this issue by generating an initial set of topics. However, these rawtopics frequently lack refinement and representativeness, which leads toredundancy without lexical similarity and reduced interpretability. This paperintroduces HAMLET, a graph-driven architecture for cross-lingual healthcaretopic modeling that uses LLMs. The proposed approach leverages neural-enhancedsemantic fusion to refine the embeddings of topics generated by the LLM.Instead of relying solely on statistical co-occurrence or human interpretationto extract topics from a document corpus, this method introduces a topicembedding refinement that uses Bidirectional Encoder Representations fromTransformers (BERT) and Graph Neural Networks (GNN). After topic generation, ahybrid technique that involves BERT and Sentence-BERT (SBERT) is employed forembedding. The topic representations are further refined using a GNN, whichestablishes connections between documents, topics, words, similar topics, andsimilar words. A novel method is introduced to compute similarities.Consequently, the topic embeddings are refined, and the top k topics areextracted. Experiments were conducted using two healthcare datasets, one inEnglish and one in French, from which six sets were derived. The resultsdemonstrate the effectiveness of HAMLET.</description>
      <author>example@mail.com (Hajar Sakai, Sarah S. Lam)</author>
      <guid isPermaLink="false">2505.07157v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Seed1.5-VL Technical Report</title>
      <link>http://arxiv.org/abs/2505.07062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Seed1.5-VL，这是一个用于提升通用多模态理解和推理能力的视觉语言基础模型。&lt;h4&gt;背景&lt;/h4&gt;Seed1.5-VL由一个532M参数的视觉编码器和20B参数的混合专家（MoE）LLM组成。&lt;h4&gt;目的&lt;/h4&gt;Seed1.5-VL旨在在公共VLM基准测试和内部评估套件中提供强大性能，并超越现有的多模态系统。&lt;h4&gt;方法&lt;/h4&gt;模型设计、数据构建和训练过程中的经验被综合回顾。&lt;h4&gt;主要发现&lt;/h4&gt;Seed1.5-VL在60个公共基准测试中的38个上达到了最先进的性能，并在GUI控制和游戏等以代理为中心的任务中优于OpenAICUA和Claude 3.7。&lt;h4&gt;结论&lt;/h4&gt;Seed1.5-VL的推理能力使其特别适用于多模态推理挑战，如视觉谜题，并有望在多个任务中应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出Seed1.5-VL，一个旨在提升通用多模态理解和推理能力的视觉语言基础模型。Seed1.5-VL由一个532M参数的视觉编码器和20B参数的混合专家（MoE）LLM组成。尽管其架构相对紧凑，但它能在广泛的公共VLM基准测试和内部评估套件中提供强大性能，在60个公共基准测试中的38个上达到了最先进的性能。此外，在以代理为中心的任务，如GUI控制和游戏玩法中，Seed1.5-VL超越了包括OpenAICUA和Claude 3.7在内的领先的多模态系统。除了视觉和视频理解之外，它还表现出强大的推理能力，使其特别适用于多模态推理挑战，如视觉谜题。我们相信这些能力将使它在多个任务中具有更广泛的应用。在本报告中，我们主要提供了在模型设计、数据构建和训练各阶段构建Seed1.5-VL的经验的综合回顾，希望这份报告能够激发进一步的研究。Seed1.5-VL现在可通过https://www.volcengine.com/（火山引擎模型ID：doubao-1-5-thinking-vision-pro-250428）获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Seed1.5-VL, a vision-language foundation model designed to advancegeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composedwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20Bactive parameters. Despite its relatively compact architecture, it deliversstrong performance across a wide spectrum of public VLM benchmarks and internalevaluation suites, achieving the state-of-the-art performance on 38 out of 60public benchmarks. Moreover, in agent-centric tasks such as GUI control andgameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAICUA and Claude 3.7. Beyond visual and video understanding, it also demonstratesstrong reasoning abilities, making it particularly effective for multimodalreasoning challenges such as visual puzzles. We believe these capabilities willempower broader applications across diverse tasks. In this report, we mainlyprovide a comprehensive review of our experiences in building Seed1.5-VL acrossmodel design, data construction, and training at various stages, hoping thatthis report can inspire further research. Seed1.5-VL is now accessible athttps://www.volcengine.com/ (Volcano Engine Model ID:doubao-1-5-thinking-vision-pro-250428)</description>
      <author>example@mail.com (Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, Zuquan Song)</author>
      <guid isPermaLink="false">2505.07062v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition</title>
      <link>http://arxiv.org/abs/2505.07166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in SIGIR-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了密集检索器中的预训练和微调的作用，发现预训练知识对检索性能至关重要，而微调主要调整神经元激活，而非重组知识。&lt;h4&gt;背景&lt;/h4&gt;密集检索器使用预训练的骨干语言模型（如BERT、LLaMA），通过对比学习进行微调，以执行将文本编码为可以进行比较的语义表示的任务。&lt;h4&gt;目的&lt;/h4&gt;重新审视密集检索器中预训练与微调的作用，特别是在BERT编码器使用DPR作为代表性密集检索器的情况下。&lt;h4&gt;方法&lt;/h4&gt;测试了不同的表示方法（比较使用CLS标记与平均池化）、骨干架构（仅编码器的BERT与仅解码器的LLaMA）和额外的数据集（MSMARCO和Natural Questions）。&lt;h4&gt;主要发现&lt;/h4&gt;在DPR微调中，预训练知识是检索性能的基础，微调主要调整神经元激活而非重组知识。但这种模式并不普遍，例如在平均池化（Contriever）和解码器基于（LLaMA）模型中。&lt;h4&gt;结论&lt;/h4&gt;确保了研究的可重复性，并将实现公开提供。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了密集检索器中的预训练和微调的作用，发现预训练知识对检索性能至关重要，而微调主要调整神经元激活，而非重组知识。背景是密集检索器使用预训练的骨干语言模型（如BERT、LLaMA），通过对比学习进行微调，以执行将文本编码为可以进行比较的语义表示的任务。研究目的是重新审视密集检索器中预训练与微调的作用，特别是在BERT编码器使用DPR作为代表性密集检索器的情况下。研究方法包括测试不同的表示方法、骨干架构和额外的数据集。主要发现是在DPR微调中，预训练知识是检索性能的基础，微调主要调整神经元激活而非重组知识。但这种模式并不普遍，例如在平均池化和解码器基于模型中。结论是确保了研究的可重复性，并将实现公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/ielab/denseretriever-knowledge-acquisition&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense retrievers utilize pre-trained backbone language models (e.g., BERT,LLaMA) that are fine-tuned via contrastive learning to perform the task ofencoding text into sense representations that can be then compared via ashallow similarity operation, e.g. inner product. Recent research hasquestioned the role of fine-tuning vs. that of pre-training within denseretrievers, specifically arguing that retrieval knowledge is primarily gainedduring pre-training, meaning knowledge not acquired during pre-training cannotbe sub-sequentially acquired via fine-tuning. We revisit this idea here as theclaim was only studied in the context of a BERT-based encoder using DPR asrepresentative dense retriever. We extend the previous analysis by testingother representation approaches (comparing the use of CLS tokens with that ofmean pooling), backbone architectures (encoder-only BERT vs. decoder-onlyLLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Ourstudy confirms that in DPR tuning, pre-trained knowledge underpins retrievalperformance, with fine-tuning primarily adjusting neuron activation rather thanreorganizing knowledge. However, this pattern does not hold universally, suchas in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure fullreproducibility and make our implementation publicly available athttps://github.com/ielab/DenseRetriever-Knowledge-Acquisition.</description>
      <author>example@mail.com (Zheng Yao, Shuai Wang, Guido Zuccon)</author>
      <guid isPermaLink="false">2505.07166v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Linux Kernel Configurations at Scale: A Dataset for Performance and Evolution Analysis</title>
      <link>http://arxiv.org/abs/2505.07487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LinuxData，一个涵盖多个Linux内核版本的配置数据集，用于研究Linux内核配置的复杂性和配置选项的影响。&lt;h4&gt;背景&lt;/h4&gt;配置Linux内核以满足特定要求（如二进制大小）非常具有挑战性，因为内核选项众多且版本间快速变化。&lt;h4&gt;目的&lt;/h4&gt;为了填补现有文献中缺乏综合大规模数据集的空白，LinuxData被创建出来，用于促进对内核配置空间分析的研究。&lt;h4&gt;方法&lt;/h4&gt;LinuxData通过自动化工具和构建过程收集了240,000多个内核配置，并系统地标注了编译结果和二进制大小。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集能够支持特征子集选择、基于机器学习的预测模型以及内核版本间的迁移学习。&lt;h4&gt;结论&lt;/h4&gt;LinuxData通过OpenML平台易于访问，并可以通过简单的Python代码来评估AI技术，如监督机器学习，从而提高研究的可重复性并促进对Linux内核配置和演化的新认识。&lt;h4&gt;翻译&lt;/h4&gt;本文提出LinuxData，一个包含多个Linux内核版本的配置数据集，用于研究Linux内核配置的复杂性和配置选项的影响。由于内核选项众多且版本间快速变化，配置Linux内核以满足特定要求（如二进制大小）具有很大挑战性。为了填补现有文献中缺乏综合大规模数据集的空白，LinuxData被创建出来，用于促进对内核配置空间分析的研究。该数据集通过自动化工具和构建过程收集了240,000多个内核配置，并系统地标注了编译结果和二进制大小。该数据集能够支持特征子集选择、基于机器学习的预测模型以及内核版本间的迁移学习。通过OpenML平台，LinuxData易于访问，并可以通过简单的Python代码来评估AI技术，如监督机器学习，从而提高研究的可重复性并促进对Linux内核配置和演化的新认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Configuring the Linux kernel to meet specific requirements, such as binarysize, is highly challenging due to its immense complexity-with over 15,000interdependent options evolving rapidly across different versions. Althoughseveral studies have explored sampling strategies and machine learning methodsto understand and predict the impact of configuration options, the literaturestill lacks a comprehensive and large-scale dataset encompassing multiplekernel versions along with detailed quantitative measurements. To bridge thisgap, we introduce LinuxData, an accessible collection of kernel configurationsspanning several kernel releases, specifically from versions 4.13 to 5.8. Thisdataset, gathered through automated tools and build processes, comprises over240,000 kernel configurations systematically labeled with compilation outcomesand binary sizes. By providing detailed records of configuration evolution andcapturing the intricate interplay among kernel options, our dataset enablesinnovative research in feature subset selection, prediction models based onmachine learning, and transfer learning across kernel versions. Throughout thispaper, we describe how the dataset has been made easily accessible via OpenMLand illustrate how it can be leveraged using only a few lines of Python code toevaluate AI-based techniques, such as supervised machine learning. Weanticipate that this dataset will significantly enhance reproducibility andfoster new insights into configuration-space analysis at a scale that presentsunique opportunities and inherent challenges, thereby advancing ourunderstanding of the Linux kernel's configurability and evolution.</description>
      <author>example@mail.com (Heraldo Borges, Juliana Alves Pereira, Djamel Eddine Khelladi, Mathieu Acher)</author>
      <guid isPermaLink="false">2505.07487v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark</title>
      <link>http://arxiv.org/abs/2505.06746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  supplementary material included&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为M$^3$CAD的新颖基准测试，旨在推进通用合作式自动驾驶研究。&lt;h4&gt;背景&lt;/h4&gt;M$^3$CAD包含204个序列和30k个帧，涵盖了各种合作驾驶场景。&lt;h4&gt;目的&lt;/h4&gt;M$^3$CAD旨在支持包括目标检测与跟踪、地图构建、运动预测、占用预测和路径规划在内的多种自动驾驶任务。&lt;h4&gt;方法&lt;/h4&gt;M$^3$CAD采用多种传感模态，如激光雷达点云、RGB图像和GPS/IMU，以支持单车和多车自动驾驶研究。&lt;h4&gt;主要发现&lt;/h4&gt;M$^3$CAD是迄今为止针对合作多任务自动驾驶研究最全面的基准测试。&lt;h4&gt;结论&lt;/h4&gt;M$^3$CAD及其基线模型和评估结果被发布以支持鲁棒的合作自动驾驶系统的开发。&lt;h4&gt;翻译&lt;/h4&gt;We introduce M$^3$CAD, a novel benchmark designed to advance research in generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with 30k frames, spanning a diverse range of cooperative driving scenarios. Each sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving tasks, including object detection and tracking, mapping, motion forecasting, occupancy prediction, and path planning. This rich multimodal setup enables M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving research, significantly broadening the scope of research in the field. To our knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored for cooperative multi-task autonomous driving research. We evaluate the state-of-the-art end-to-end solution on M$^3$CAD to establish baseline performance. To foster cooperative autonomous driving research, we also propose E2EC, a simple yet effective framework for cooperative driving solution that leverages inter-vehicle shared information for improved path planning. We release M$^3$CAD, along with our baseline models and evaluation results, to support the development of robust cooperative autonomous driving systems. All resources will be made publicly available on https://github.com/zhumorui/M3CAD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce M$^3$CAD, a novel benchmark designed to advance research ingeneric cooperative autonomous driving. M$^3$CAD comprises 204 sequences with30k frames, spanning a diverse range of cooperative driving scenarios. Eachsequence includes multiple vehicles and sensing modalities, e.g., LiDAR pointclouds, RGB images, and GPS/IMU, supporting a variety of autonomous drivingtasks, including object detection and tracking, mapping, motion forecasting,occupancy prediction, and path planning. This rich multimodal setup enablesM$^3$CAD to support both single-vehicle and multi-vehicle autonomous drivingresearch, significantly broadening the scope of research in the field. To ourknowledge, M$^3$CAD is the most comprehensive benchmark specifically tailoredfor cooperative multi-task autonomous driving research. We evaluate thestate-of-the-art end-to-end solution on M$^3$CAD to establish baselineperformance. To foster cooperative autonomous driving research, we also proposeE2EC, a simple yet effective framework for cooperative driving solution thatleverages inter-vehicle shared information for improved path planning. Werelease M$^3$CAD, along with our baseline models and evaluation results, tosupport the development of robust cooperative autonomous driving systems. Allresources will be made publicly available on https://github.com/zhumorui/M3CAD</description>
      <author>example@mail.com (Morui Zhu, Yongqi Zhu, Yihao Zhu, Qi Chen, Deyuan Qu, Song Fu, Qing Yang)</author>
      <guid isPermaLink="false">2505.06746v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection</title>
      <link>http://arxiv.org/abs/2505.06903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CheXLearner是一个统一了解剖区域检测、基于黎曼流形的结构对齐和细粒度区域语义引导的端到端框架，用于时间医学图像分析。&lt;h4&gt;背景&lt;/h4&gt;现有的医学图像分析方法要么在粗粒度上对齐图像和文本，导致潜在的语义不匹配，要么仅依赖于视觉信息，缺乏医学语义整合。&lt;h4&gt;目的&lt;/h4&gt;提出CheXLearner，以解决现有方法的问题，实现图像和文本的精确对齐，并整合医学语义。&lt;h4&gt;方法&lt;/h4&gt;CheXLearner使用超曲几何来对齐解剖结构，并捕获时间胸片中的病理学意义差异。通过引入区域进展描述作为监督，实现跨模态表示学习，并支持动态低级别特征优化。&lt;h4&gt;主要发现&lt;/h4&gt;CheXLearner在解剖区域进展检测上达到81.12%的平均准确率和80.32%的F1分数，显著优于现有基准。在下游疾病分类中，模型达到91.52%的平均AUC分数。&lt;h4&gt;结论&lt;/h4&gt;CheXLearner在时间医学图像分析中实现了优异的性能，验证了其在特征表示方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal medical image analysis is essential for clinical decision-making,yet existing methods either align images and text at a coarse level - causingpotential semantic mismatches - or depend solely on visual information, lackingmedical semantic integration. We present CheXLearner, the first end-to-endframework that unifies anatomical region detection, Riemannian manifold-basedstructure alignment, and fine-grained regional semantic guidance. Our proposedMed-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry torobustly align anatomical structures and capture pathologically meaningfuldiscrepancies across temporal chest X-rays. By introducing regional progressiondescriptions as supervision, CheXLearner achieves enhanced cross-modalrepresentation learning and supports dynamic low-level feature optimization.Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and80.32% (+11.05%) F1-score on anatomical region progression detection -substantially outperforming state-of-the-art baselines, especially instructurally complex regions. Additionally, our model attains a 91.52% averageAUC score in downstream disease classification, validating its superior featurerepresentation.</description>
      <author>example@mail.com (Yuanzhuo Wang, Junwen Duan, Xinyu Li, Jianxin Wang)</author>
      <guid isPermaLink="false">2505.06903v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>COMRECGC: Global Graph Counterfactual Explainer through Common Recourse</title>
      <link>http://arxiv.org/abs/2505.07081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了图神经网络（GNNs）及其解释方法，提出了一个有效的算法COMRECGC来求解全局反事实解释中的共同补救方法问题，并通过实验证明了其性能优于其他算法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）在社交网络、分子生物学、推荐系统等领域得到了广泛应用，但其黑盒性质需要通过解释方法来补充。&lt;h4&gt;目的&lt;/h4&gt;设计一个算法来求解全局反事实解释中的共同补救方法问题，并证明其性能优于其他算法。&lt;h4&gt;方法&lt;/h4&gt;本文正式化了共同补救方法解释问题，并设计了COMRECGC算法来解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;COMRECGC算法在四个不同的真实世界图数据集上进行了基准测试，表现优于其他算法。同时，共同补救方法解释与图反事实解释进行了比较，结果表明共同补救方法解释在药物发现或计算生物学等应用中具有可比性或优越性。&lt;h4&gt;结论&lt;/h4&gt;共同补救方法解释对于GNNs的全局反事实解释问题是一个有价值的解决方案，值得在药物发现或计算生物学等应用中进行考虑。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have been widely used in various domains such associal networks, molecular biology, or recommendation systems. Concurrently,different explanations methods of GNNs have arisen to complement its black-boxnature. Explanations of the GNNs' predictions can be categorized into twotypes--factual and counterfactual. Given a GNN trained on binary classificationinto ''accept'' and ''reject'' classes, a global counterfactual explanationconsists in generating a small set of ''accept'' graphs relevant to all of theinput ''reject'' graphs. The transformation of a ''reject'' graph into an''accept'' graph is called a recourse. A common recourse explanation is a smallset of recourse, from which every ''reject'' graph can be turned into an''accept'' graph. Although local counterfactual explanations have been studiedextensively, the problem of finding common recourse for global counterfactualexplanation remains unexplored, particularly for GNNs. In this paper, weformalize the common recourse explanation problem, and design an effectivealgorithm, COMRECGC, to solve it. We benchmark our algorithm against strongbaselines on four different real-world graphs datasets and demonstrate thesuperior performance of COMRECGC against the competitors. We also compare thecommon recourse explanations to the graph counterfactual explanation, showingthat common recourse explanations are either comparable or superior, makingthem worth considering for applications such as drug discovery or computationalbiology.</description>
      <author>example@mail.com (Gregoire Fournier, Sourav Medya)</author>
      <guid isPermaLink="false">2505.07081v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Dynamics in Continual Pre-Training for Large Language Models</title>
      <link>http://arxiv.org/abs/2505.07796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML2025 (spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在持续预训练（CPT）过程中大型语言模型的学习动态，特别关注了通用性能和下游领域性能在每一步训练中的演变。&lt;h4&gt;背景&lt;/h4&gt;CPT已成为将强大基础模型应用于特定下游任务的流行且有效的方法。&lt;h4&gt;目的&lt;/h4&gt;研究CPT过程中学习动态，特别是通用和下游领域性能的变化。&lt;h4&gt;方法&lt;/h4&gt;通过验证损失来衡量领域性能，并观察到CPT损失曲线描述了从一条曲线到另一条隐藏曲线的过渡，该曲线可以由解耦分布偏移和学习率衰减效应来描述。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种CPT扩展定律，结合了分布偏移和学习率衰减两个因素，能够预测任何（持续）训练步骤和CPT中的学习率计划（LRS）下的损失。&lt;h4&gt;结论&lt;/h4&gt;该定律在多种CPT数据集和训练超参数下均有效，可以用于调整训练超参数，以平衡通用性能和特定领域性能。&lt;h4&gt;翻译&lt;/h4&gt;Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Pre-Training (CPT) has become a popular and effective method toapply strong foundation models to specific downstream tasks. In this work, weexplore the learning dynamics throughout the CPT process for large languagemodels. We specifically focus on how general and downstream domain performanceevolves at each training step, with domain performance measured via validationlosses. We have observed that the CPT loss curve fundamentally characterizesthe transition from one curve to another hidden curve, and could be describedby decoupling the effects of distribution shift and learning rate annealing. Wederive a CPT scaling law that combines the two factors, enabling the predictionof loss at any (continual) training steps and across learning rate schedules(LRS) in CPT. Our formulation presents a comprehensive understanding of severalcritical factors in CPT, including loss potential, peak learning rate, trainingsteps, replay ratio, etc. Moreover, our approach can be adapted to customizetraining hyper-parameters to different CPT goals such as balancing general anddomain-specific performance. Extensive experiments demonstrate that our scalinglaw holds across various CPT datasets and training hyper-parameters.</description>
      <author>example@mail.com (Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng)</author>
      <guid isPermaLink="false">2505.07796v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
      <link>http://arxiv.org/abs/2505.06814v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了M4IVQA挑战赛，旨在进一步推动多模态、多语言和多跳医疗教学问答系统的研究。&lt;h4&gt;背景&lt;/h4&gt;继NLPCC 2023 Foshan的CMIVQA和NLPCC 2024 Hangzhou的MMIVQA挑战赛成功举办后，今年引入了新的M4IVQA任务。&lt;h4&gt;目的&lt;/h4&gt;M4IVQA挑战赛专注于评估能够从医疗教学视频中整合信息、理解多种语言并回答需要跨模态推理的多跳问题的模型。&lt;h4&gt;方法&lt;/h4&gt;挑战赛包括三个赛道：多模态、多语言和多跳视频中的时序答案定位（M4TAGSV）、多模态、多语言和多跳视频语料库检索（M4VCR）和多模态、多语言和多跳视频语料库中的时序答案定位（M4TAGVC）。参与者需要开发能够处理视频和文本数据、理解多语言查询并为多跳医疗问题提供相关答案的算法。&lt;h4&gt;主要发现&lt;/h4&gt;M4IVQA挑战赛将推动多模态推理系统在医疗场景中的应用创新，最终有助于构建更智能的应急响应系统和在多语言社区中更有效的医学教育平台。&lt;h4&gt;结论&lt;/h4&gt;M4IVQA挑战赛有望促进多模态推理系统在医疗领域的创新，对医疗教育有重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has beenintroduced to further advance research in multi-modal, multilingual, andmulti-hop medical instructional question answering (M4IVQA) systems, with aspecific focus on medical instructional videos. The M4IVQA challenge focuses onevaluating models that integrate information from medical instructional videos,understand multiple languages, and answer multi-hop questions requiringreasoning over various modalities. This task consists of three tracks:multi-modal, multilingual, and multi-hop Temporal Answer Grounding in SingleVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video CorpusRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal AnswerGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected todevelop algorithms capable of processing both video and text data,understanding multilingual queries, and providing relevant answers to multi-hopmedical questions. We believe the newly introduced M4IVQA challenge will driveinnovations in multimodal reasoning systems for healthcare scenarios,ultimately contributing to smarter emergency response systems and moreeffective medical education platforms in multilingual communities. Our officialwebsite is https://cmivqa.github.io/</description>
      <author>example@mail.com (Bin Li, Shenxi Liu, Yixuan Weng, Yue Du, Yuhang Tian, Shoujun Zhou)</author>
      <guid isPermaLink="false">2505.06814v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Inference for Small Cohorts via Transfer Learning and Weighted Integration of Multiple Datasets</title>
      <link>http://arxiv.org/abs/2505.07153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了东北部美国地区肺部感染问题，指出国家eICU协作数据库中该地区患者数据不足，强调了数据的代表性不足。提出了一个新的加权方法TRANSLATE，用于整合来自不同来源的数据，以提高对感染结果的推断准确性。&lt;h4&gt;背景&lt;/h4&gt;东北部美国地区的肺部感染是一个重要问题，但国家eICU协作数据库中该地区患者数据不足，表明数据代表性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的加权方法，以解决小样本问题，并提高对肺部感染结果的推断准确性。&lt;h4&gt;方法&lt;/h4&gt;使用了一种名为TRANSLATE的新加权方法，通过学习权重将外部数据与目标队列对齐，从而整合来自不同来源的数据。&lt;h4&gt;主要发现&lt;/h4&gt;TRANSLATE方法在模拟和实际数据应用中提高了对肺部感染结果的推断准确性，同时考虑了区域异质性。&lt;h4&gt;结论&lt;/h4&gt;TRANSLATE方法为提高对肺部感染结果的推断提供了理论保证，并适用于多种统计估计，如均值、方差和分布函数。&lt;h4&gt;翻译&lt;/h4&gt;Lung sepsis remains a significant concern in the Northeastern U.S., yet thenational eICU Collaborative Database includes only a small number of patientsfrom this region, highlighting underrepresentation. Understanding clinicalvariables such as FiO2, creatinine, platelets, and lactate, which reflectoxygenation, kidney function, coagulation, and metabolism, is crucial becausethese markers influence sepsis outcomes and may vary by sex. Transfer learninghelps address small sample sizes by borrowing information from larger datasets,although differences in covariates and outcome-generating mechanisms betweenthe target and external cohorts can complicate the process. We propose a novelweighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate datafrom various sources by incorporating domain-specific characteristics throughlearned weights that align external data with the target cohort. These weightsadjust for cohort differences, are proportional to each cohort's effectivesample size, and downweight dissimilar cohorts. TRANSLATE offers theoreticalguarantees for improved precision and applies to a wide range of estimands,including means, variances, and distribution functions. Simulations and areal-data application to sepsis outcomes in the Northeast cohort, using a muchlarger sample from other U.S. regions, show that the method enhances inferencewhile accounting for regional heterogeneity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung sepsis remains a significant concern in the Northeastern U.S., yet thenational eICU Collaborative Database includes only a small number of patientsfrom this region, highlighting underrepresentation. Understanding clinicalvariables such as FiO2, creatinine, platelets, and lactate, which reflectoxygenation, kidney function, coagulation, and metabolism, is crucial becausethese markers influence sepsis outcomes and may vary by sex. Transfer learninghelps address small sample sizes by borrowing information from larger datasets,although differences in covariates and outcome-generating mechanisms betweenthe target and external cohorts can complicate the process. We propose a novelweighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate datafrom various sources by incorporating domain-specific characteristics throughlearned weights that align external data with the target cohort. These weightsadjust for cohort differences, are proportional to each cohort's effectivesample size, and downweight dissimilar cohorts. TRANSLATE offers theoreticalguarantees for improved precision and applies to a wide range of estimands,including means, variances, and distribution functions. Simulations and areal-data application to sepsis outcomes in the Northeast cohort, using a muchlarger sample from other U.S. regions, show that the method enhances inferencewhile accounting for regional heterogeneity.</description>
      <author>example@mail.com (Subharup Guha, Mengqi Xu, Yi Li)</author>
      <guid isPermaLink="false">2505.07153v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</title>
      <link>http://arxiv.org/abs/2505.06575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRACE的新方法，用于估计人类与场景接触的几何级别，通过结合点云编码器-解码器架构和层次特征提取与融合模块，实现了对3D人类几何结构与2D图像交互语义的有效整合，从而准确建模接触区域。&lt;h4&gt;背景&lt;/h4&gt;现有的方法主要依赖于参数化人类模型（如SMPL），通过固定的SMPL顶点序列在图像和接触区域之间建立对应关系，但这种方法的泛化能力在不同的人类几何形状上受到限制。&lt;h4&gt;目的&lt;/h4&gt;提高人类与场景接触估计的准确性，并增强对不同人类几何形状的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;GRACE方法结合了点云编码器-解码器架构和层次特征提取与融合模块，通过视觉线索将几何特征映射到3D人类网格的顶点空间，从而实现接触区域的准确建模。&lt;h4&gt;主要发现&lt;/h4&gt;GRACE在多个基准数据集上的实验表明，其在接触估计方面达到了最先进的性能，并且其鲁棒性验证了其在非结构化人类点云上的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;GRACE是一种有效且具有良好泛化能力的3D人类接触估计方法，适用于人类行为分析、具身人工智能和AR/VR等应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：估算人类-场景接触的几何级别旨在将特定的接触表面点定位在3D人类几何上，这提供了一个空间先验，并架起了人类与场景之间的桥梁，支持人类行为分析、具身人工智能和AR/VR等应用。为了完成这项任务，现有的方法主要依赖于参数化人类模型（例如SMPL），通过固定的SMPL顶点序列在图像和接触区域之间建立对应关系。实际上，这种方法完成了从图像特征到有序序列的映射。然而，这种方法缺乏对几何形状的考虑，限制了其在不同人类几何形状上的泛化能力。在本文中，我们引入了GRACE（用于3D人类-场景接触估计的几何级别推理），这是一种新的3D人类接触估计范式。GRACE结合了点云编码器-解码器架构以及层次特征提取和融合模块，使得3D人类几何结构与从图像中提取的2D交互语义能够有效整合。在视觉线索的引导下，GRACE建立了从几何特征到3D人类网格顶点空间的隐式映射，从而实现了接触区域的准确建模。这种设计确保了高预测精度，并赋予了框架在多种人类几何形状上的强大泛化能力。在多个基准数据集上的大量实验表明，GRACE在接触估计方面达到了最先进的性能，进一步的结果进一步验证了其在非结构化人类点云上的鲁棒泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the geometry level of human-scene contact aims to ground specificcontact surface points at 3D human geometries, which provides a spatial priorand bridges the interaction between human and scene, supporting applicationssuch as human behavior analysis, embodied AI, and AR/VR. To complete the task,existing approaches predominantly rely on parametric human models (e.g., SMPL),which establish correspondences between images and contact regions throughfixed SMPL vertex sequences. This actually completes the mapping from imagefeatures to an ordered sequence. However, this approach lacks consideration ofgeometry, limiting its generalizability in distinct human geometries. In thispaper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene ContactEstimation), a new paradigm for 3D human contact estimation. GRACE incorporatesa point cloud encoder-decoder architecture along with a hierarchical featureextraction and fusion module, enabling the effective integration of 3D humangeometric structures with 2D interaction semantics derived from images. Guidedby visual cues, GRACE establishes an implicit mapping from geometric featuresto the vertex space of the 3D human mesh, thereby achieving accurate modelingof contact regions. This design ensures high prediction accuracy and endows theframework with strong generalization capability across diverse humangeometries. Extensive experiments on multiple benchmark datasets demonstratethat GRACE achieves state-of-the-art performance in contact estimation, withadditional results further validating its robust generalization to unstructuredhuman point clouds.</description>
      <author>example@mail.com (Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha)</author>
      <guid isPermaLink="false">2505.06575v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>MarkMatch: Same-Hand Stuffing Detection</title>
      <link>http://arxiv.org/abs/2505.07032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MarkMatch的检索系统，用于检测两个选票标记是否由同一只手填写。&lt;h4&gt;背景&lt;/h4&gt;与之前使用二分类方法处理独立标记对的SOTA方法BubbleSig不同，MarkMatch使用对比学习来评估查询标记与数据库中标记之间的风格相似度。&lt;h4&gt;目的&lt;/h4&gt;提高在书写变化和视觉噪声下的泛化能力，并增强对真实匹配的高置信度。&lt;h4&gt;方法&lt;/h4&gt;模型使用密集批相似度矩阵和双重损失目标进行训练，通过对比学习在每个批次中对每个样本与多个负样本进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;MarkMatch实现了0.943的F1分数，超过了BubbleSig的最佳性能。系统还集成了Segment Anything Model，通过框或点提示进行灵活的标记提取。&lt;h4&gt;结论&lt;/h4&gt;MarkMatch为选举审计员提供了一种实用的工具，用于对可疑选票进行视觉、非生物识别的调查。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为MarkMatch的检索系统，用于检测两个选票标记是否由同一只手填写。与之前使用二分类方法处理独立标记对的SOTA方法BubbleSig不同，MarkMatch使用对比学习来评估查询标记与数据库中标记之间的风格相似度。我们的模型使用密集批相似度矩阵和双重损失目标进行训练，通过对比学习在每个批次中对每个样本与多个负样本进行对比，从而使模型能够学习细微的书写差异，并提高在书写变化和视觉噪声下的泛化能力，而斜对角监督则加强了真实匹配的高置信度。模型实现了0.943的F1分数，超过了BubbleSig的最佳性能。MarkMatch还集成了Segment Anything Model，通过框或点提示进行灵活的标记提取。该系统为选举审计员提供了一种实用的工具，用于对可疑选票进行视觉、非生物识别的调查。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MarkMatch, a retrieval system for detecting whether two paperballot marks were filled by the same hand. Unlike the previous SOTA methodBubbleSig, which used binary classification on isolated mark pairs, MarkMatchranks stylistic similarity between a query mark and a mark in the databaseusing contrastive learning. Our model is trained with a dense batch similaritymatrix and a dual loss objective. Each sample is contrasted against manynegatives within each batch, enabling the model to learn subtle handwritingdifference and improve generalization under handwriting variation and visualnoise, while diagonal supervision reinforces high confidence on true matches.The model achieves an F1 score of 0.943, surpassing BubbleSig's bestperformance. MarkMatch also integrates Segment Anything Model for flexible markextraction via box- or point-based prompts. The system offers election auditorsa practical tool for visual, non-biometric investigation of suspicious ballots.</description>
      <author>example@mail.com (Fei Zhao, Runlin Zhang, Chengcui Zhang, Nitesh Saxena)</author>
      <guid isPermaLink="false">2505.07032v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</title>
      <link>http://arxiv.org/abs/2505.06886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，发现两者在上下文（群体水平）和自下而上（单细胞水平）的映射存在显著相似性，并通过添加神经响应归一化层（NeuRN）进一步增强了这种相似性，提高了模型在现实世界任务中的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;小鼠是系统神经科学中研究最广泛的动物模型之一。了解小鼠视觉皮层中由各种自然场景刺激引起的神经表征的普遍模式是计算视觉领域的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;研究小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，并提出一种新的框架来比较两者之间的功能架构。&lt;h4&gt;方法&lt;/h4&gt;引入了一种通用的表征学习策略，并添加了受视觉皮层中兴奋性和抑制性神经元激活特征启发的NeuRN层。&lt;h4&gt;主要发现&lt;/h4&gt;发现小鼠视觉皮层的功能映射与高性能深度学习模型在上下文和自下而上的场景中存在显著相似性；NeuRN层的添加显著提高了深度学习模型在领域泛化任务中对数据变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;研究结果对从小鼠视觉皮层中获得灵感的先进AI模型的发展具有重要意义，表明这些模型可以作为研究小鼠视觉皮层神经表征的有价值工具，并因此提高它们在现实世界任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The mouse is one of the most studied animal models in the field of systemsneuroscience. Understanding the generalized patterns and decoding the neuralrepresentations that are evoked by the diverse range of natural scene stimuliin the mouse visual cortex is one of the key quests in computational vision. Inrecent years, significant parallels have been drawn between the primate visualcortex and hierarchical deep neural networks. However, their generalizedefficacy in understanding mouse vision has been limited. In this study, weinvestigate the functional alignment between the mouse visual cortex and deeplearning models for object classification tasks. We first introduce ageneralized representational learning strategy that uncovers a strikingresemblance between the functional mapping of the mouse visual cortex andhigh-performing deep learning models on both top-down (population-level) andbottom-up (single cell-level) scenarios. Next, this representational similarityacross the two systems is further enhanced by the addition of Neural ResponseNormalization (NeuRN) layer, inspired by the activation profile of excitatoryand inhibitory neurons in the visual cortex. To test the performance effect ofNeuRN on real-world tasks, we integrate it into deep learning models andobserve significant improvements in their robustness against data shifts indomain generalization tasks. Our work proposes a novel framework for comparingthe functional architecture of the mouse visual cortex with deep learningmodels. Our findings carry broad implications for the development of advancedAI models that draw inspiration from the mouse visual cortex, suggesting thatthese models serve as valuable tools for studying the neural representations ofthe mouse visual cortex and, as a result, enhancing their performance onreal-world tasks.</description>
      <author>example@mail.com (Ahmed Qazi, Hamd Jalil, Asim Iqbal)</author>
      <guid isPermaLink="false">2505.06886v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>BodyGPS: Anatomical Positioning System</title>
      <link>http://arxiv.org/abs/2505.07744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的基础模型，用于解析医学图像中的人类解剖结构，适用于不同的模态，支持监督或无监督训练，能够进行匹配、配准、分类或分割，可带或不带用户交互。&lt;h4&gt;背景&lt;/h4&gt;当前解析医学图像中的人类解剖结构需要针对不同模态的特定模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适用于不同模态、支持多种任务和交互方式的基础模型。&lt;h4&gt;方法&lt;/h4&gt;通过训练一个神经网络估计器，将查询位置映射到图谱坐标，通过回归实现。通过稀疏采样输入数据，提高效率，实现小于1毫秒的响应时间，无需额外的加速硬件。&lt;h4&gt;主要发现&lt;/h4&gt;该算法在CT和MRI模态中均表现出良好的实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法和模型在医学图像解析中具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a new type of foundational model for parsing human anatomy inmedical images that works for different modalities. It supports supervised orunsupervised training and can perform matching, registration, classification,or segmentation with or without user interaction. We achieve this by training aneural network estimator that maps query locations to atlas coordinates viaregression. Efficiency is improved by sparsely sampling the input, enablingresponse times of less than 1 ms without additional accelerator hardware. Wedemonstrate the utility of the algorithm in both CT and MRI modalities.</description>
      <author>example@mail.com (Halid Ziya Yerebakan, Kritika Iyer, Xueqi Guo, Yoshihisa Shinagawa, Gerardo Hermosillo Valadez)</author>
      <guid isPermaLink="false">2505.07744v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors</title>
      <link>http://arxiv.org/abs/2505.06573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ElectricSight的系统，用于3D距离测量和监测电力传输线路潜在威胁，如大型起重机，以提高安全。&lt;h4&gt;背景&lt;/h4&gt;保护电力传输线路免受潜在危害是关键任务，其中之一是准确测量电力线路与潜在威胁之间的距离。现有的基于传感器的测量方法在平衡准确性和成本方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出ElectricSight系统，旨在解决现有方法的局限性，实现低成本且精确的3D距离测量。&lt;h4&gt;方法&lt;/h4&gt;ElectricSight系统结合实时图像和环境影响点云先验信息，采用单目深度估计方法，将3D点云数据集成到基于图像的估计中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ElectricSight在距离测量上实现了平均精度1.08米，在早期预警上实现了92%的准确率。&lt;h4&gt;结论&lt;/h4&gt;ElectricSight系统通过其创新的设计和实施，提供了有效的解决方案，以解决电力传输线路安全监测中的距离测量问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protecting power transmission lines from potential hazards involves criticaltasks, one of which is the accurate measurement of distances between powerlines and potential threats, such as large cranes. The challenge with this taskis that the current sensor-based methods face challenges in balancing accuracyand cost in distance measurement. A common practice is to install cameras ontransmission towers, which, however, struggle to measure true 3D distances dueto the lack of depth information. Although 3D lasers can provide accurate depthdata, their high cost makes large-scale deployment impractical.  To address this challenge, we present ElectricSight, a system designed for 3Ddistance measurement and monitoring of potential hazards to power transmissionlines. This work's key innovations lie in both the overall system framework anda monocular depth estimation method. Specifically, the system frameworkcombines real-time images with environmental point cloud priors, enablingcost-effective and precise 3D distance measurements. As a core component of thesystem, the monocular depth estimation method enhances the performance byintegrating 3D point cloud data into image-based estimates, improving both theaccuracy and reliability of the system.  To assess ElectricSight's performance, we conducted tests with data from areal-world power transmission scenario. The experimental results demonstratethat ElectricSight achieves an average accuracy of 1.08 m for distancemeasurements and an early warning accuracy of 92%.</description>
      <author>example@mail.com (Xingchen Li, LiDian Wang, Yu Sheng, ZhiPeng Tang, Haojie Ren, Guoliang You, YiFan Duan, Jianmin Ji, Yanyong Zhang)</author>
      <guid isPermaLink="false">2505.06573v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies</title>
      <link>http://arxiv.org/abs/2505.06855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Multi-Masking Strategy (MMS)的文本识别方法，通过结合随机块状和跨度掩码，优化了自监督学习在文本识别任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界标注数据集的稀缺，大多数文本识别方法都是在大规模合成数据集上训练的。合成图像无法真实反映现实场景，如不均匀光照、不规则布局、遮挡和退化等问题，导致在处理复杂现实图像时性能不佳。&lt;h4&gt;目的&lt;/h4&gt;为了缩小现实世界和合成数据集之间的差距，本文旨在通过自监督学习技术提高文本识别的性能。&lt;h4&gt;方法&lt;/h4&gt;本文分析了原始的Masked AutoEncoder (MAE)并指出，随机块状掩码主要捕捉低级纹理特征，但忽略了高级上下文表示。为了充分利用高级上下文表示，我们引入了随机块状和跨度掩码。Multi-Masking Strategy (MMS)将随机块状和跨度掩码整合到MIM框架中，共同学习低级和高级文本表示。&lt;h4&gt;主要发现&lt;/h4&gt;经过与真实数据的微调，MMS在文本识别、分割和文本图像超分辨率等文本相关任务中优于现有的自监督方法。&lt;h4&gt;结论&lt;/h4&gt;MMS通过引入新的掩码策略，显著提高了自监督学习在文本识别任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level textual representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing text recognition methods are trained on large-scale syntheticdatasets due to the scarcity of labeled real-world datasets. Synthetic images,however, cannot faithfully reproduce real-world scenarios, such as unevenillumination, irregular layout, occlusion, and degradation, resulting inperformance disparities when handling complex real-world images. Recentself-supervised learning techniques, notably contrastive learning and maskedimage modeling (MIM), narrow this domain gap by exploiting unlabeled real textimages. This study first analyzes the original Masked AutoEncoder (MAE) andobserves that random patch masking predominantly captures low-level texturalfeatures but misses high-level contextual representations. To fully exploit thehigh-level contextual representations, we introduce random blockwise and spanmasking in the text recognition task. These strategies can mask the continuousimage patches and completely remove some characters, forcing the model to inferrelationships among characters within a word. Our Multi-Masking Strategy (MMS)integrates random patch, blockwise, and span masking into the MIM frame, whichjointly learns low and high-level textual representations. After fine-tuningwith real data, MMS outperforms the state-of-the-art self-supervised methods invarious text-related tasks, including text recognition, segmentation, andtext-image super-resolution.</description>
      <author>example@mail.com (Zhengmi Tang, Yuto Mitsui, Tomo Miyazaki, Shinichiro Omachi)</author>
      <guid isPermaLink="false">2505.06855v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Survival Modeling in the Age of Foundation Models</title>
      <link>http://arxiv.org/abs/2505.07683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 7 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过利用基础模型和病理报告文本信息提取，研究了训练经典的多模态生存模型的可行性，并展示了其在预测癌症生存方面的优势。&lt;h4&gt;背景&lt;/h4&gt;TCGA数据库通过其基因组学、临床和图像数据的整合，为癌症研究提供了大规模参考。基础模型在生物医学深度学习中用于提取有意义特征嵌入，而病理报告文本在TCGA数据库中虽存在，但长期以来未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;研究通过使用基础模型从零样本嵌入中训练经典的多模态生存模型的可行性。&lt;h4&gt;方法&lt;/h4&gt;通过分析TCGA数据，结合基础模型提取的零样本嵌入和病理报告文本，构建多模态生存模型，并与单模态模型进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现多模态融合模型易于实现且效果优于单模态模型，同时加入病理报告文本也有助于提高模型的预测性能。&lt;h4&gt;结论&lt;/h4&gt;利用基础模型和病理报告文本信息提取可以现代化生存建模，提高癌症生存预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and imagedata. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as alarge-scale reference through its harmonized genomics, clinical, and imagedata. Prior studies have trained bespoke cancer survival prediction models fromunimodal or multimodal TCGA data. A modern paradigm in biomedical deep learningis the development of foundation models (FMs) to derive meaningful featureembeddings, agnostic to a specific modeling task. Biomedical text especiallyhas seen growing development of FMs. While TCGA contains free-text data aspathology reports, these have been historically underutilized. Here, weinvestigate the feasibility of training classical, multimodal survival modelsover zero-shot embeddings extracted by FMs. We show the ease and additiveeffect of multimodal fusion, outperforming unimodal models. We demonstrate thebenefit of including pathology report text and rigorously evaluate the effectof model-based text summarization and hallucination. Overall, we modernizesurvival modeling by leveraging FMs and information extraction from pathologyreports.</description>
      <author>example@mail.com (Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman)</author>
      <guid isPermaLink="false">2505.07683v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</title>
      <link>http://arxiv.org/abs/2505.06710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于弱监督方案的多实例学习（MIL）特征提取器预训练方法，旨在改善MIL在病理图像分析中的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的MIL方法强调了特征聚合器的重要性，但很大程度上忽视了实例级别的表征学习。同时，这些方法假设可以直接利用或微调预训练的特征提取器，但这种情况并不总是成立。&lt;h4&gt;目的&lt;/h4&gt;提出一种预训练MIL特征提取器的方法，通过传播弱标签到对应的实例来实现。&lt;h4&gt;方法&lt;/h4&gt;通过弱标签传播的方式预训练特征提取器，并深入研究了几个关键组件，包括强大的数据增强、非线性预测头和鲁棒的损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在常见的病理图像数据集上，该方法在不同下游任务中取得了比其他预训练方案（如ImageNet预训练和自监督学习）更好的性能。该方法还显示出了兼容性和可扩展性，适用于病理特定模型的微调和多个数据集的预训练。&lt;h4&gt;结论&lt;/h4&gt;这是首个专注于MIL表征学习的研究工作，提出了有效的预训练方法，并展示了其在实际应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Various multi-instance learning (MIL) based approaches have been developedand successfully applied to whole-slide pathological images (WSI). Existing MILmethods emphasize the importance of feature aggregators, but largely neglectthe instance-level representation learning. They assume that the availabilityof a pre-trained feature extractor can be directly utilized or fine-tuned,which is not always the case. This paper proposes to pre-train featureextractor for MIL via a weakly-supervised scheme, i.e., propagating the weakbag-level labels to the corresponding instances for supervised learning. Tolearn effective features for MIL, we further delve into several key components,including strong data augmentation, a non-linear prediction head and the robustloss function. We conduct experiments on common large-scale WSI datasets andfind it achieves better performance than other pre-training schemes (e.g.,ImageNet pre-training and self-supervised learning) in different downstreamtasks. We further show the compatibility and scalability of the proposed schemeby deploying it in fine-tuning the pathological-specific models andpre-training on merged multiple datasets. To our knowledge, this is the firstwork focusing on the representation learning for MIL.</description>
      <author>example@mail.com (Yicheng Song, Tiancheng Lin, Die Peng, Su Yang, Yi Xu)</author>
      <guid isPermaLink="false">2505.06710v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>NetSight: Graph Attention Based Traffic Forecasting in Computer Networks</title>
      <link>http://arxiv.org/abs/2505.07034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NetSight的新方法，用于预测网络中给定指标的值。NetSight能够同时学习全局和局部尺度的时空依赖关系，从而提高预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;当前网络交通受到节点交互和节点需求波动的影响，传统的统计预测方法已无法适应这种非线性动态时空依赖。&lt;h4&gt;目的&lt;/h4&gt;提出NetSight以解决传统方法在处理网络交通中的时空依赖关系方面的不足。&lt;h4&gt;方法&lt;/h4&gt;NetSight通过学习网络中各个节点的测量时间序列数据，同时考虑全局和局部尺度的时空依赖关系，实现网络指标的预测。&lt;h4&gt;主要发现&lt;/h4&gt;NetSight在两个大型真实网络的数据集上进行了广泛评估，结果显示其在预测准确性方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;NetSight通过同时学习全局和局部尺度的时空依赖关系，提高了网络指标预测的准确性，为网络交通预测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The traffic in today's networks is increasingly influenced by the interactions among network nodes as well as by the temporal fluctuations in the demands of the nodes. Traditional statistical prediction methods are becoming obsolete due to their inability to address the non-linear and dynamic spatio-temporal dependencies present in today's network traffic. The most promising direction of research today is graph neural networks (GNNs) based prediction approaches that are naturally suited to handle graph-structured data. Unfortunately, the state-of-the-art GNN approaches separate the modeling of spatial and temporal information, resulting in the loss of important information about joint dependencies. These GNN based approaches further do not model information at both local and global scales simultaneously, leaving significant room for improvement. To address these challenges, we propose NetSight. NetSight learns joint spatio-temporal dependencies simultaneously at both global and local scales from the time-series of measurements of any given network metric collected at various nodes in a network. Using the learned information, NetSight can then accurately predict the future values of the given network metric at those nodes in the network. We propose several new concepts and techniques in the design of NetSight, such as spatio-temporal adjacency matrix and node normalization. Through extensive evaluations and comparison with prior approaches using data from two large real-world networks, we show that NetSight significantly outperforms all prior state-of-the-art approaches. We will release the source code and data used in the evaluation of NetSight on the acceptance of this paper.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The traffic in today's networks is increasingly influenced by theinteractions among network nodes as well as by the temporal fluctuations in thedemands of the nodes. Traditional statistical prediction methods are becomingobsolete due to their inability to address the non-linear and dynamicspatio-temporal dependencies present in today's network traffic. The mostpromising direction of research today is graph neural networks (GNNs) basedprediction approaches that are naturally suited to handle graph-structureddata. Unfortunately, the state-of-the-art GNN approaches separate the modelingof spatial and temporal information, resulting in the loss of importantinformation about joint dependencies. These GNN based approaches further do notmodel information at both local and global scales simultaneously, leavingsignificant room for improvement. To address these challenges, we proposeNetSight. NetSight learns joint spatio-temporal dependencies simultaneously atboth global and local scales from the time-series of measurements of any givennetwork metric collected at various nodes in a network. Using the learnedinformation, NetSight can then accurately predict the future values of thegiven network metric at those nodes in the network. We propose several newconcepts and techniques in the design of NetSight, such as spatio-temporaladjacency matrix and node normalization. Through extensive evaluations andcomparison with prior approaches using data from two large real-world networks,we show that NetSight significantly outperforms all prior state-of-the-artapproaches. We will release the source code and data used in the evaluation ofNetSight on the acceptance of this paper.</description>
      <author>example@mail.com (Jinming Xing, Guoheng Sun, Hui Sun, Linchao Pan, Shakir Mahmood, Xuanhao Luo, Muhammad Shahzad)</author>
      <guid isPermaLink="false">2505.07034v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Vision-Language Foundation Model for Leaf Disease Identification</title>
      <link>http://arxiv.org/abs/2505.07019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCOLD的智能农业领域视觉-语言基础模型，用于叶病识别，通过结合图像和文本模态，解决了现有方法中模态整合不足和依赖预训练数据集的问题。&lt;h4&gt;背景&lt;/h4&gt;叶病识别在智能农业中至关重要，但现有研究在整合图像和文本模态以及使用预训练数据集方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出SCOLD模型，旨在解决农业任务中的模态整合和预训练数据集的局限性。&lt;h4&gt;方法&lt;/h4&gt;SCOLD模型使用超过186,000个植物叶片图像及其对应的症状描述进行训练，通过无任务预训练和软目标对比学习来提高模型的泛化能力和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SCOLD在零样本和少样本分类、图像-文本检索和图像分类等多个基准测试中优于现有视觉-语言模型，同时保持了有竞争力的参数规模。&lt;h4&gt;结论&lt;/h4&gt;SCOLD模型显著推进了农业视觉-语言基础模型的发展，以最小的或无需监督微调即可实现强大的性能，为未来研究长文本和简化上下文训练的模型、涉及类别模糊性的任务以及智能植物病害诊断的多模态系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：叶病识别在智能农业中发挥着关键作用。然而，许多现有研究仍然难以整合图像和文本模态以弥补彼此的局限性。此外，许多这些方法依赖于使用ImageNet等受限制的数据集进行预训练，这些数据集缺乏领域特定信息。我们提出了SCOLD（用于叶病识别的软目标对比学习），这是一种针对解决这些挑战的农业任务而定制的内容感知视觉-语言基础模型。SCOLD使用一个包含超过186,000个植物叶片图像及其对应症状描述的多样化语料库进行开发，这些图像-字幕对与97个独特概念相匹配。通过无任务预训练，SCOLD利用上下文软目标通过平滑标签来减轻对比学习中的过度自信，从而提高模型在细粒度分类任务上的泛化能力和鲁棒性。实验结果表明，SCOLD在多个基准测试中（包括零样本和少样本分类、图像-文本检索和图像分类）优于现有的视觉-语言模型（如OpenAI-CLIP-L、BioCLIP和SigLIP2），同时保持有竞争力的参数规模。消融研究进一步突出了SCOLD相对于其竞争对手的有效性。所提出的方法显著推进了农业视觉-语言基础模型，以最小的或无需监督微调即可实现强大的性能。这项工作为使用长文本和简化上下文训练的模型、涉及类别模糊性的任务以及智能植物病害诊断的多模态系统的研究奠定了坚实的基础。本研究的代码可在https://huggingface.co/enalis/scold获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leaf disease identification plays a pivotal role in smart agriculture.However, many existing studies still struggle to integrate image and textualmodalities to compensate for each other's limitations. Furthermore, many ofthese approaches rely on pretraining with constrained datasets such asImageNet, which lack domain-specific information. We propose SCOLD (Soft-targetCOntrastive learning for Leaf Disease identification), a context-awarevision-language foundation model tailored to address these challenges foragricultural tasks. SCOLD is developed using a diverse corpus of plant leafimages and corresponding symptom descriptions, comprising over 186,000image-caption pairs aligned with 97 unique concepts. Through task-agnosticpretraining, SCOLD leverages contextual soft targets to mitigate overconfidencein contrastive learning by smoothing labels, thereby improving modelgeneralization and robustness on fine-grained classification tasks.Experimental results demonstrate that SCOLD outperforms existingvision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 acrossseveral benchmarks, including zero-shot and few-shot classification, image-textretrieval, and image classification, while maintaining a competitive parameterfootprint. Ablation studies further highlight SCOLD's effectiveness in contrastto its counterparts. The proposed approach significantly advances theagricultural vision-language foundation model, offering strong performance withminimal or no supervised fine-tuning. This work lays a solid groundwork forfuture research on models trained with long-form and simplified contexts, tasksinvolving class ambiguity, and multi-modal systems for intelligent plantdisease diagnostics. The code for this study is available athttps://huggingface.co/enalis/scold</description>
      <author>example@mail.com (Khang Nguyen Quoc, Lan Le Thi Thu, Luyl-Da Quach)</author>
      <guid isPermaLink="false">2505.07019v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for Class Distribution Mismatch</title>
      <link>http://arxiv.org/abs/2505.06948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UCDM的无监督学习方法，用于解决训练数据与目标任务类别分布不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;以往的方法在半监督场景下设计分类器，将未知或新类别归为“其他”类别，但过分依赖标记数据，限制了其适用性和性能。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出UCDM方法，通过从无标签数据中构建正负样本对来进行分类器训练。&lt;h4&gt;方法&lt;/h4&gt;UCDM方法通过随机采样图像和使用扩散模型添加或删除语义类别来合成多样化的训练对。此外，引入基于置信度的标签机制，迭代地为有价值的数据分配伪标签，并将其纳入训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的大量实验表明，UCDM方法优于以往半监督方法。在Tiny-ImageNet数据集上，60%的类别分布不匹配比例下，UCDM方法无需依赖标记数据，在分类已知、未知和新类别上分别超越了OpenMatch（每类40个标签）的35.1%、63.7%和72.5%。&lt;h4&gt;结论&lt;/h4&gt;UCDM方法有效地解决了类别分布不匹配问题，在无需大量标记数据的情况下，显著提高了分类器的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：类别分布不匹配（CDM）是指训练数据中的类别分布与目标任务之间的差异。先前的方法通过设计分类器来对训练期间已知的类别进行分类，将未知或新类别分组到“其他”类别中。然而，它们侧重于半监督场景，并且高度依赖标记数据，限制了它们的适用性和性能。为了解决这个问题，我们提出了无监督学习用于类别分布不匹配（UCDM），该方法从无标签数据中为分类器训练构建正负样本对。我们的方法通过随机采样图像和使用扩散模型添加或删除语义类别来合成多样化的训练对。此外，我们引入了一种基于置信度的标签机制，迭代地为有价值的数据分配伪标签，并将它们纳入训练过程。在三个数据集上的大量实验表明UCDM方法优于先前半监督方法。具体而言，在Tiny-ImageNet数据集上，60%的类别分布不匹配比例下，我们的方法，不依赖于标记数据，在分类已知、未知和新类别上分别超过了OpenMatch（每类40个标签）的35.1%、63.7%和72.5%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Class distribution mismatch (CDM) refers to the discrepancy between classdistributions in training data and target tasks. Previous methods address thisby designing classifiers to categorize classes known during training, whilegrouping unknown or new classes into an "other" category. However, they focuson semi-supervised scenarios and heavily rely on labeled data, limiting theirapplicability and performance. To address this, we propose UnsupervisedLearning for Class Distribution Mismatch (UCDM), which constructspositive-negative pairs from unlabeled data for classifier training. Ourapproach randomly samples images and uses a diffusion model to add or erasesemantic classes, synthesizing diverse training pairs. Additionally, weintroduce a confidence-based labeling mechanism that iteratively assignspseudo-labels to valuable real-world data and incorporates them into thetraining process. Extensive experiments on three datasets demonstrate UCDM'ssuperiority over previous semi-supervised methods. Specifically, with a 60%mismatch proportion on Tiny-ImageNet dataset, our approach, without relying onlabeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,and 72.5% in classifying known, unknown, and new classes.</description>
      <author>example@mail.com (Pan Du, Wangbo Zhao, Xinai Lu, Nian Liu, Zhikai Li, Chaoyu Gong, Suyun Zhao, Hong Chen, Cuiping Li, Kai Wang, Yang You)</author>
      <guid isPermaLink="false">2505.06948v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics</title>
      <link>http://arxiv.org/abs/2505.06435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 30 figures. IEEE Transactions on Pattern Analysis and  Machine Intelligence (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对连续敏感属性的公平表示学习（FRL）算法，旨在解决现有FRL算法在处理连续敏感属性（如年龄或收入）时的局限性。&lt;h4&gt;背景&lt;/h4&gt;AI公平性，也称为算法公平性，旨在确保算法在操作过程中不对任何个人或群体产生偏见或歧视。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的FRL算法，使其能够应用于连续敏感属性。&lt;h4&gt;方法&lt;/h4&gt;引入了期望积分概率度量（EIPM）来评估连续敏感属性表示空间的公平性水平，并证明了低EIPM值的表示分布上的预测头构建将具有公平性。此外，EIPM可以通过有限样本的估计器进行准确估计。&lt;h4&gt;主要发现&lt;/h4&gt;提出的FRL算法FREM（基于EIPM和MMD的公平表示）在实验中优于其他基线方法。&lt;h4&gt;结论&lt;/h4&gt;FREM算法能够有效处理连续敏感属性，提高算法的公平性，并在实际应用中优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI fairness, also known as algorithmic fairness, aims to ensure thatalgorithms operate without bias or discrimination towards any individual orgroup. Among various AI algorithms, the Fair Representation Learning (FRL)approach has gained significant interest in recent years. However, existing FRLalgorithms have a limitation: they are primarily designed for categoricalsensitive attributes and thus cannot be applied to continuous sensitiveattributes, such as age or income. In this paper, we propose an FRL algorithmfor continuous sensitive attributes. First, we introduce a measure called theExpectation of Integral Probability Metrics (EIPM) to assess the fairness levelof representation space for continuous sensitive attributes. We demonstratethat if the distribution of the representation has a low EIPM value, then anyprediction head constructed on the top of the representation become fair,regardless of the selection of the prediction head. Furthermore, EIPM possessesa distinguished advantage in that it can be accurately estimated using ourproposed estimator with finite samples. Based on these properties, we propose anew FRL algorithm called Fair Representation using EIPM with MMD (FREM).Experimental evidences show that FREM outperforms other baseline methods.</description>
      <author>example@mail.com (Insung Kong, Kunwoong Kim, Yongdai Kim)</author>
      <guid isPermaLink="false">2505.06435v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A systematic review of challenges and proposed solutions in modeling multimodal data</title>
      <link>http://arxiv.org/abs/2505.06945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态数据建模在临床研究中已成为一种强大的方法，它能够整合多种数据类型，如影像、基因组、可穿戴传感器和电子健康记录。然而，建模这种异构数据存在技术挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态数据建模能够提高诊断准确性和支持个性化护理，但建模这种异构数据面临技术挑战。&lt;h4&gt;目的&lt;/h4&gt;本文通过综合69项研究的发现，旨在识别多模态数据建模中的常见障碍。&lt;h4&gt;方法&lt;/h4&gt;本文采用系统综述的方法，分析了69项相关研究。&lt;h4&gt;主要发现&lt;/h4&gt;主要障碍包括缺失的数据模式、样本量有限、维度不平衡、可解释性问题以及寻找最佳融合技术。&lt;h4&gt;结论&lt;/h4&gt;本文强调了转移学习、生成模型、注意力机制和神经架构搜索等最近的方法学进展，这些进展为解决这些问题提供了有希望的解决方案。此外，本文通过映射当前趋势和创新，为该领域的全面概述提供了实用的见解，以指导未来在多模态建模医学应用中的研究和开发。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态数据建模已成为临床研究中一种强大的方法，能够整合多种数据类型，如影像、基因组、可穿戴传感器和电子健康记录。尽管其能够提高诊断准确性和支持个性化护理，但建模这种异构数据仍然存在显著的技术挑战。本文通过综合69项研究的发现，识别出常见的障碍，包括缺失的数据模式、样本量有限、维度不平衡、可解释性问题以及寻找最佳融合技术。本文突出了转移学习、生成模型、注意力机制和神经架构搜索等最近的方法学进展，这些进展为解决这些问题提供了有希望的解决方案。通过映射当前趋势和创新，本文为该领域的全面概述提供了实用的见解，以指导未来在多模态建模医学应用中的研究和开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data modeling has emerged as a powerful approach in clinicalresearch, enabling the integration of diverse data types such as imaging,genomics, wearable sensors, and electronic health records. Despite itspotential to improve diagnostic accuracy and support personalized care,modeling such heterogeneous data presents significant technical challenges.This systematic review synthesizes findings from 69 studies to identify commonobstacles, including missing modalities, limited sample sizes, dimensionalityimbalance, interpretability issues, and finding the optimal fusion techniques.We highlight recent methodological advances, such as transfer learning,generative models, attention mechanisms, and neural architecture search thatoffer promising solutions. By mapping current trends and innovations, thisreview provides a comprehensive overview of the field and offers practicalinsights to guide future research and development in multimodal modeling formedical applications.</description>
      <author>example@mail.com (Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binde, Nadine Binder)</author>
      <guid isPermaLink="false">2505.06945v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Image Classification Using a Diffusion Model as a Pre-Training Model</title>
      <link>http://arxiv.org/abs/2505.06890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合了表示条件的扩散模型，通过使用Vision Transformer（ViT）的表示来调节基于Transformer的扩散模型内部过程，实现了表示条件的数据生成，并利用自监督学习在无标签数据上解决大规模标注数据集的挑战。&lt;h4&gt;背景&lt;/h4&gt;在图像分类任务中，大规模标注数据集的获取是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来生成表示条件的数据，并提高图像分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用Vision Transformer（ViT）的表示作为条件来调节扩散模型，通过零样本分类任务在脑部影像中检测血肿来评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与DINOv2等对比学习基线相比，该方法在准确性上提高了6.15%，在F1分数上提高了13.60%，显示了其在图像分类中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合表示条件和扩散模型，在图像分类任务中取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a diffusion model that integrates arepresentation-conditioning mechanism, where the representations derived from aVision Transformer (ViT) are used to condition the internal process of aTransformer-based diffusion model. This approach enablesrepresentation-conditioned data generation, addressing the challenge ofrequiring large-scale labeled datasets by leveraging self-supervised learningon unlabeled data. We evaluate our method through a zero-shot classificationtask for hematoma detection in brain imaging. Compared to the strongcontrastive learning baseline, DINOv2, our method achieves a notableimprovement of +6.15% in accuracy and +13.60% in F1-score, demonstrating itseffectiveness in image classification.</description>
      <author>example@mail.com (Kosuke Ukita, Ye Xiaolong, Tsuyoshi Okita)</author>
      <guid isPermaLink="false">2505.06890v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects</title>
      <link>http://arxiv.org/abs/2505.06363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了基于人类演示学习物体模型的方法，用于多自由度物体的精确控制。&lt;h4&gt;背景&lt;/h4&gt;随着机器人在多样化环境中应用，它们需要与具有多个独立关节或自由度的复杂物体交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法依赖先验知识、仅关注单自由度物体、无法处理遮挡关节和忽略获取它们所需的操作序列的问题。&lt;h4&gt;方法&lt;/h4&gt;通过学习人类演示的物体模型，引入了物体运动学序列机器（OKSMs）这一新表示，它捕捉了多自由度物体的运动学约束和操作顺序。为了从点云数据中估计这些模型，提出了Pokenet，这是一个在人类演示上训练的深度神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据上，Pokenet比现有方法提高了超过20%的关节轴和状态估计。OKSMs在Sawyer机器人上通过基于逆运动学规划的策略操作多自由度物体。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在模拟和真实世界的数据上均有效，能够提高机器人对多自由度物体的控制精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robots become more generalized and deployed in diverse environments, theymust interact with complex objects, many with multiple independent joints ordegrees of freedom (DoF) requiring precise control. A common strategy is objectmodeling, where compact state-space models are learned from real-worldobservations and paired with classical planning. However, existing methodsoften rely on prior knowledge or focus on single-DoF objects, limiting theirapplicability. They also fail to handle occluded joints and ignore themanipulation sequences needed to access them. We address this by learningobject models from human demonstrations. We introduce Object Kinematic SequenceMachines (OKSMs), a novel representation capturing both kinematic constraintsand manipulation order for multi-DoF objects. To estimate these models frompoint cloud data, we present Pokenet, a deep neural network trained on humandemonstrations. We validate our approach on 8,000 simulated and 1,600real-world annotated samples. Pokenet improves joint axis and state estimationby over 20 percent on real-world data compared to prior methods. Finally, wedemonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning tomanipulate multi-DoF objects.</description>
      <author>example@mail.com (Anmol Gupta, Weiwei Gu, Omkar Patil, Jun Ki Lee, Nakul Gopalan)</author>
      <guid isPermaLink="false">2505.06363v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Similarity Search in Automotive Production</title>
      <link>http://arxiv.org/abs/2505.07256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Procedia CIRP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合视觉基础模型和合成数据的图像分类流程，用于汽车生产中的视觉质量检测，以提高检测的准确性和降低数据收集成本。&lt;h4&gt;背景&lt;/h4&gt;视觉质量检测对于保证汽车的安全和可靠性至关重要，计算机视觉技术在成本效益和可靠性方面表现出色。&lt;h4&gt;目的&lt;/h4&gt;为了减少大量标注数据的收集需求，提出了一个创新的图像分类流程。&lt;h4&gt;方法&lt;/h4&gt;该方法利用DINOv2模型将输入图像转换为特征向量，然后使用余弦距离测量与预分类的参考图像进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用合成数据而非真实图像作为参考，该流程实现了高分类准确率，同时不依赖于真实数据。&lt;h4&gt;结论&lt;/h4&gt;在八个实际检测场景中评估了该方法，证明它满足了生产环境中的高性能要求。&lt;h4&gt;翻译&lt;/h4&gt;在汽车生产中，视觉质量检查对于确保车辆的安全和可靠性至关重要。由于成本效益和可靠性，计算机视觉（CV）已经成为这些检查的流行解决方案。然而，CV模型需要大量标注的数据集，收集这些数据集既昂贵又耗时。为了减少对大量训练数据的需求，我们提出了一种结合基于视觉的基础模型和合成数据的图像分类流程。我们的方法利用DINOv2模型将输入图像转换为特征向量，然后使用余弦距离测量与预分类的参考图像进行比较。通过使用合成数据而不是真实图像作为参考，我们的流程在不依赖于真实数据的情况下实现了高分类准确率。我们在八个现实世界的检测场景中评估了这种方法，证明了它满足生产环境的高性能要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual quality inspection in automotive production is essential for ensuringthe safety and reliability of vehicles. Computer vision (CV) has become apopular solution for these inspections due to its cost-effectiveness andreliability. However, CV models require large, annotated datasets, which arecostly and time-consuming to collect. To reduce the need for extensive trainingdata, we propose a novel image classification pipeline that combines similaritysearch using a vision-based foundation model with synthetic data. Our approachleverages a DINOv2 model to transform input images into feature vectors, whichare then compared to pre-classified reference images using cosine distancemeasurements. By utilizing synthetic data instead of real images as references,our pipeline achieves high classification accuracy without relying on realdata. We evaluate this approach in eight real-world inspection scenarios anddemonstrate that it meets the high performance requirements of productionenvironments.</description>
      <author>example@mail.com (Christoph Huber, Ludwig Schleeh, Dino Knoll, Michael Guthe)</author>
      <guid isPermaLink="false">2505.07256v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting</title>
      <link>http://arxiv.org/abs/2505.06862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了BIGBIRD-PEGASUS模型在长文档摘要任务上的表现，并提出了改进策略以解决模型处理超长文档时性能下降的问题。&lt;h4&gt;背景&lt;/h4&gt;BIGBIRD-PEGASUS模型在长文档摘要任务上取得了最先进的成果，但其处理能力有限，最大只能处理4,096个token的文档，导致在处理超长文档时性能下降。&lt;h4&gt;目的&lt;/h4&gt;旨在提高BIGBIRD-PEGASUS模型处理超长文档的能力。&lt;h4&gt;方法&lt;/h4&gt;通过微调预训练的BIGBIRD-PEGASUS模型，并在其他领域的数据集上进行训练。首先，筛选出长度超过20,000个token的文档，以专注于超长文档。为了解决领域迁移问题和数据集小导致的过拟合，通过将文档摘要训练对拆分成部分，以适应4,096个token的文档。&lt;h4&gt;主要发现&lt;/h4&gt;使用微调的方法可以有效提高模型处理超长文档的能力。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效提升BIGBIRD-PEGASUS模型在超长文档摘要任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：BIGBIRD-PEGASUS模型在长文档摘要任务上取得了最先进的成果。然而，其容量仍限于最多4,096个token，因此在处理超长文档时会导致性能下降。处理此问题的常见方法是对文档进行截断。在本研究中，我们将采用不同的方法。我们将使用预训练的BIGBIRD-PEGASUS模型，并通过在其它领域的数据集上微调模型来改进它。首先，我们将所有长度少于20,000个token的文档过滤掉，以专注于超长文档。为了避免领域迁移问题和由于数据集小导致的迁移学习过拟合，我们将数据集通过将文档摘要训练对拆分成部分来增强，以适应4,096个token的文档。源代码可在https://github.com/lhfazry/SPIN-summ上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lhfazry/spin-summ&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ onabstractive text summarization for long documents. However it's capacity stilllimited to maximum of $4,096$ tokens, thus caused performance degradation onsummarization for very long documents. Common method to deal with the issue isto truncate the documents. In this reasearch, we'll use different approach.We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned themodel on other domain dataset. First, we filter out all documents which lengthless than $20,000$ tokens to focus on very long documents. To prevent domainshifting problem and overfitting on transfer learning due to small dataset, weaugment the dataset by splitting document-summary training pair into parts, tofit the document into $4,096$ tokens. Source code available on$\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.</description>
      <author>example@mail.com (Lhuqita Fazry)</author>
      <guid isPermaLink="false">2505.06862v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning</title>
      <link>http://arxiv.org/abs/2505.06796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态假新闻检测方法，旨在应对deepfake建模攻击。&lt;h4&gt;背景&lt;/h4&gt;多模态新闻信息丰富，但易受deepfake建模攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;设计一个新的多模态假新闻检测数据集（MFND），并构建一个模型来检测和定位高度逼真的假新闻。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Shallow-Deep Multitask Learning（SDML）的模型，该模型利用单模态和互模态特征挖掘新闻的内在语义。在浅层推理中，采用动量蒸馏的轻惩罚对比学习进行细粒度空间图像和文本语义对齐，并引入自适应跨模态融合模块以增强互模态特征。在深层推理中，设计了两个分支框架，分别增强图像和文本的单模态特征，并合并互模态特征进行四个预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在主流和提出的数据集上均表现出优越性。&lt;h4&gt;结论&lt;/h4&gt;SDML模型在多模态假新闻检测方面具有显著效果。&lt;h4&gt;翻译&lt;/h4&gt;The abstract is about a new multimodal fake news detection method aimed at combating deepfake modeling attacks. The method is demonstrated to be effective in detecting and localizing highly authentic fake news.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yunan-wang33/sdml&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal news contains a wealth of information and is easily affected bydeepfake modeling attacks. To combat the latest image and text generationmethods, we present a new Multimodal Fake News Detection dataset (MFND)containing 11 manipulated types, designed to detect and localize highlyauthentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning(SDML) model for fake news, which fully uses unimodal and mutual modal featuresto mine the intrinsic semantics of news. Under shallow inference, we proposethe momentum distillation-based light punishment contrastive learning forfine-grained uniform spatial image and text semantic alignment, and an adaptivecross-modal fusion module to enhance mutual modal features. Under deepinference, we design a two-branch framework to augment the image and textunimodal features, respectively merging with mutual modalities features, forfour predictions via dedicated detection and localization projections.Experiments on both mainstream and our proposed datasets demonstrate thesuperiority of the model. Codes and dataset are released athttps://github.com/yunan-wang33/sdml.</description>
      <author>example@mail.com (Ye Zhu, Yunan Wang, Zitong Yu)</author>
      <guid isPermaLink="false">2505.06796v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards user-centered interactive medical image segmentation in VR with an assistive AI agent</title>
      <link>http://arxiv.org/abs/2505.07214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMIRA的对话式AI代理，旨在辅助用户在虚拟现实(VR)环境中对3D医学概念进行定位、分割和可视化。&lt;h4&gt;背景&lt;/h4&gt;手动分割医学扫描图像（如MRI、CT）工作量大，容易出错，难以掌握，而全自动算法可以从用户反馈中受益。&lt;h4&gt;目的&lt;/h4&gt;开发一种辅助工具，通过语音交互帮助用户理解影像学特征，定位临床目标，并生成可以快速细化分割掩码的3D医学概念。&lt;h4&gt;方法&lt;/h4&gt;结合最新的影像学AI基础模型和VR的直观数据交互，设计了SAMIRA，并比较了VR控制器指向、头部指向和眼动追踪作为输入模式，以确定最佳交互范式。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究显示，SAMIRA具有高可用性（SUS=90.0 ± 9.0），总体任务负荷低，以及对VR系统的指导、培训潜力和AI在影像学分割任务中集成的高度支持。&lt;h4&gt;结论&lt;/h4&gt;SAMIRA是一个有效且用户友好的工具，可以增强对患者的解剖理解，并可能提高影像学分割任务的效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crucial in disease analysis and surgical planning, manual segmentation ofvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, andchallenging to master, while fully automatic algorithms can benefit fromuser-feedback. Therefore, with the complementary power of the latestradiological AI foundation models and virtual reality (VR)'s intuitive datainteraction, we propose SAMIRA, a novel conversational AI agent that assistsusers with localizing, segmenting, and visualizing 3D medical concepts in VR.Through speech-based interaction, the agent helps users understand radiologicalfeatures, locate clinical targets, and generate segmentation masks that can berefined with just a few point prompts. The system also supports true-to-scale3D visualization of segmented pathology to enhance patient-specific anatomicalunderstanding. Furthermore, to determine the optimal interaction paradigm undernear-far attention-switching for refining segmentation masks in an immersive,human-in-the-loop workflow, we compare VR controller pointing, head pointing,and eye tracking as input modes. With a user study, evaluations demonstrated ahigh usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well asstrong support for the proposed VR system's guidance, training potential, andintegration of AI in radiological segmentation tasks.</description>
      <author>example@mail.com (Pascal Spiegler, Arash Harirpoush, Yiming Xiao)</author>
      <guid isPermaLink="false">2505.07214v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach</title>
      <link>http://arxiv.org/abs/2505.06853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the 6th BioSMART Conference, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种估计膝关节周围骨肉瘤手术安全边界的置信区间的方法。&lt;h4&gt;背景&lt;/h4&gt;拉丁美洲的癌症病例预计将在2022年达到420万，到2045年将增加到670万。骨肉瘤是一种常见的、对年轻人影响严重的骨癌，由于其独特的质地和强度，难以检测。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，以估计膝关节周围骨肉瘤手术的安全边界置信区间。&lt;h4&gt;方法&lt;/h4&gt;该方法使用来自开源仓库的MRI和X射线数据，结合数字处理技术和无监督学习算法（如K均值聚类）来定义肿瘤边界。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果突出了自动、针对患者特定情况确定安全边界的潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法有望提高骨肉瘤手术的安全性和精确性。&lt;h4&gt;翻译&lt;/h4&gt;根据泛美卫生组织，2022年拉丁美洲的癌症病例估计为420万，预计到2045年将增加到670万。骨肉瘤是影响年轻人的最常见和致命的骨癌之一，由于其独特的质地和强度，难以检测。手术切除骨肉瘤需要精确的安全边界以确保完全切除同时保留健康组织。因此，本研究提出了一种估计膝关节周围骨肉瘤手术安全边界的置信区间的方法。提出的方法使用来自开源仓库的MRI和X射线数据，数字处理技术和无监督学习算法（如K均值聚类）来定义肿瘤边界。实验结果突出了自动化、针对患者特定情况确定安全边界的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; According to the Pan American Health Organization, the number of cancer casesin Latin America was estimated at 4.2 million in 2022 and is projected to riseto 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bonecancers affecting young people, is difficult to detect due to its uniquetexture and intensity. Surgical removal of osteosarcoma requires precise safetymargins to ensure complete resection while preserving healthy tissue.Therefore, this study proposes a method for estimating the confidence intervalof surgical safety margins in osteosarcoma surgery around the knee. Theproposed approach uses MRI and X-ray data from open-source repositories,digital processing techniques, and unsupervised learning algorithms (such ask-means clustering) to define tumor boundaries. Experimental results highlightthe potential for automated, patient-specific determination of safety margins.</description>
      <author>example@mail.com (Carolina Vargas-Ecos, Edwin Salcedo)</author>
      <guid isPermaLink="false">2505.06853v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>QSeer: A Quantum-Inspired Graph Neural Network for Parameter Initialization in Quantum Approximate Optimization Algorithm Circuits</title>
      <link>http://arxiv.org/abs/2505.06810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为QSeer的量子灵感的图神经网络，旨在准确预测量子近似优化算法（QAOA）的参数初始化，以提高在NISQ时代的QAOA优化效果。&lt;h4&gt;背景&lt;/h4&gt;量子近似优化算法（QAOA）在优化过程中存在 barren plateau problem，即性能提升停滞的问题。有效的参数初始化对于在近期的有噪声中等规模量子（NISQ）时代优化QAOA至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有参数初始化方法的局限性，提高QAOA在NISQ时代的性能。&lt;h4&gt;方法&lt;/h4&gt;QSeer是一种量子灵感的图神经网络，它借鉴了先前优化的QAOA参数，并结合了关键的物理信息，如参数浓度、对称性和绝热进化原则。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的方法相比，QSeer提高了QAOA电路在多样化图上的初始近似比和收敛速度，分别提升了6%-68%和5x-10x。&lt;h4&gt;结论&lt;/h4&gt;QSeer作为一种新型量子灵感图神经网络，能够显著提高QAOA的优化性能，为NISQ时代的量子计算提供了一种有效的参数初始化方法。&lt;h4&gt;翻译&lt;/h4&gt;To mitigate the barren plateau problem, effective parameter initialization is crucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) in the near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-driven approaches leveraged the optimal parameter concentration phenomenon, utilizing medium values of previously optimized QAOA parameters stored in databases as initialization for new graphs. However, this medium-value-based strategy lacks generalization capability. Conversely, prior computer-science-based approaches employed graph neural networks (GNNs) trained on previously optimized QAOA parameters to predict initialization values for new graphs. However, these approaches neglect key physics-informed QAOA principles, such as parameter concentration, symmetry, and adiabatic evolution, resulting in suboptimal parameter predictions and limited performance improvements. Furthermore, no existing GNN-based methods support parameter initialization for QAOA circuits with variable depths or for solving weighted Max-Cut problems. This paper introduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameter prediction. Compared to prior physics- and computer-science-driven methods, QSeer improves the initial approximation ratio and convergence speed of QAOA circuits across diverse graphs by 6%-68% and 5x-10x, respectively.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To mitigate the barren plateau problem, effective parameter initialization iscrucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) inthe near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-drivenapproaches leveraged the optimal parameter concentration phenomenon, utilizingmedium values of previously optimized QAOA parameters stored in databases asinitialization for new graphs. However, this medium-value-based strategy lacksgeneralization capability. Conversely, prior computer-science-based approachesemployed graph neural networks (GNNs) trained on previously optimized QAOAparameters to predict initialization values for new graphs. However, theseapproaches neglect key physics-informed QAOA principles, such as parameterconcentration, symmetry, and adiabatic evolution, resulting in suboptimalparameter predictions and limited performance improvements. Furthermore, noexisting GNN-based methods support parameter initialization for QAOA circuitswith variable depths or for solving weighted Max-Cut problems. This paperintroduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameterprediction. Compared to prior physics- and computer-science-driven methods,QSeer improves the initial approximation ratio and convergence speed of QAOAcircuits across diverse graphs by 6%-68% and 5x-10x, respectively.</description>
      <author>example@mail.com (Lei Jiang, Chi Zhang, Fan Chen)</author>
      <guid isPermaLink="false">2505.06810v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Architectural Precedents for General Agents using Large Language Models</title>
      <link>http://arxiv.org/abs/2505.07087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 2 figures. Submitted to AGI25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文总结了在多种预Transformer AI架构中出现的认知设计模式，并探讨了这些模式在利用大型语言模型（LLMs）的系统中的应用，特别是对于推理和交互式（“代理”）用例。&lt;h4&gt;背景&lt;/h4&gt;人工智能和通用人工智能（AGI）的目标之一是识别和理解足以实现通用智能的具体机制和表示。在AI/AGI领域，许多认知架构已被探索。&lt;h4&gt;目的&lt;/h4&gt;识别和理解足以实现通用智能的具体机制和表示。&lt;h4&gt;方法&lt;/h4&gt;总结预Transformer AI架构中的认知设计模式，并分析这些模式在LLMs系统中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;不同研究组和不同研究传统独立地识别了相似/共同的过程和表示或认知设计模式，这些模式在现有架构中体现。LLMs提供了一种新的机制和表示组合，用于探索通用智能的可能性。&lt;h4&gt;结论&lt;/h4&gt;通过分析和应用这些重复出现的模式，可以预测今天代理LLM系统中的差距或不足，并确定使用LLMs和其他生成基础模型实现通用智能的未来的研究主题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One goal of AI (and AGI) is to identify and understand specific mechanismsand representations sufficient for general intelligence. Often, this workmanifests in research focused on architectures and many cognitive architectureshave been explored in AI/AGI. However, different research groups and evendifferent research traditions have somewhat independently identifiedsimilar/common patterns of processes and representations or cognitive designpatterns that are manifest in existing architectures. Today, AI systemsexploiting large language models (LLMs) offer a relatively new combination ofmechanism and representation available for exploring the possibilities ofgeneral intelligence. In this paper, we summarize a few recurring cognitivedesign patterns that have appeared in various pre-transformer AI architectures.We then explore how these patterns are evident in systems using LLMs,especially for reasoning and interactive ("agentic") use cases. By examiningand applying these recurring patterns, we can also predict gaps or deficienciesin today's Agentic LLM Systems and identify likely subjects of future researchtowards general intelligence using LLMs and other generative foundation models.</description>
      <author>example@mail.com (Robert E. Wray, James R. Kirk, John E. Laird)</author>
      <guid isPermaLink="false">2505.07087v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws</title>
      <link>http://arxiv.org/abs/2505.06699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模型引导的新兴学习范式，通过使用训练好的模型作为参考来指导并增强目标模型的训练，通过战略性地选择或加权数据。该方法名为DRRho风险最小化，基于分布鲁棒优化（DRO）。通过泛化分析，本文提供了理论上的见解，解释了为什么与没有参考模型训练相比，这种方法可以改善泛化能力和数据效率。&lt;h4&gt;背景&lt;/h4&gt;虽然模型引导在包括大型基础模型训练的各种场景中已经使用，但其基本原理理解不足，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于理论的框架DRRho风险最小化，用于模型引导，并引入一种名为DRRho-CLIP的新方法，用于具有参考模型的对比语言-图像预训练（CLIP）。&lt;h4&gt;方法&lt;/h4&gt;本文通过泛化分析，对模型引导提供了理论上的见解，并引入了DRRho-CLIP方法。&lt;h4&gt;主要发现&lt;/h4&gt;本文提供了模型引导的第一个理论见解，增强了我们对模型引导的理解和实践，并通过实验验证了理论见解的有效性。&lt;h4&gt;结论&lt;/h4&gt;DRRho-CLIP方法比没有参考模型的CLIP具有更好的扩展性和性能，优于现有的启发式方法。&lt;h4&gt;翻译&lt;/h4&gt;本文将摘要内容翻译成了中文，并按照要求进行了结构化表达。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper formalizes an emerging learning paradigm that uses a trained modelas a reference to guide and enhance the training of a target model throughstrategic data selection or weighting, named $\textbf{model steering}$. Whilead-hoc methods have been used in various contexts, including the training oflarge foundation models, its underlying principles remain insufficientlyunderstood, leading to sub-optimal performance. In this work, we propose atheory-driven framework for model steering called $\textbf{DRRho riskminimization}$, which is rooted in Distributionally Robust Optimization (DRO).Through a generalization analysis, we provide theoretical insights into whythis approach improves generalization and data efficiency compared to trainingwithout a reference model. To the best of our knowledge, this is the first timesuch theoretical insights are provided for the new learning paradigm, whichsignificantly enhance our understanding and practice of model steering.Building on these insights and the connection between contrastive learning andDRO, we introduce a novel method for Contrastive Language-Image Pretraining(CLIP) with a reference model, termed DRRho-CLIP. Extensive experimentsvalidate the theoretical insights, reveal a superior scaling law compared toCLIP without a reference model, and demonstrate its strength over existingheuristic approaches.</description>
      <author>example@mail.com (Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. That, Tianbao Yang)</author>
      <guid isPermaLink="false">2505.06699v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Deep Neural Networks for Cross-Energy Particle Identification at RHIC and LHC</title>
      <link>http://arxiv.org/abs/2505.06732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Journal of Physics G: Nuclear and Particle Physics on 30  April 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了在强横动量区域应用深度神经网络进行粒子识别的方法。&lt;h4&gt;背景&lt;/h4&gt;研究基于模拟的大型强子对撞机（LHC）质子-质子碰撞数据（sqrt(s) = 13TeV）。&lt;h4&gt;目的&lt;/h4&gt;目的是训练一个模型，使用七个动力学特征来区分九种不同的粒子。&lt;h4&gt;方法&lt;/h4&gt;模型在模拟数据上训练，并在高横动量RHIC数据上测试，未进行迁移学习、微调或权重调整。&lt;h4&gt;主要发现&lt;/h4&gt;模型在LHC和RHIC数据集上均保持了超过91%的准确率，在所有RHIC数据集上均达到了超过96%的准确率，包括横动量大于7 GeV/c的数据集，尽管模型未在RHIC数据上训练。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，该模型能够捕捉到高能碰撞的底层物理，而不仅仅是过拟合训练数据。&lt;h4&gt;翻译&lt;/h4&gt;这项工作强调了模拟训练模型在不同能量域中的应用潜力，特别是在数据较少或代表性不足的设置中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work demonstrates the application of a deep neural network for particleidentification in the high transverse momentum regime. A model trained onsimulated Large Hadron Collider (LHC) proton-proton collisions (sqrt(s) = 13TeV) is used to classify nine distinct particles using seven kinematic-levelfeatures. The model is then tested on high transverse momentum RHIC datawithout any transfer learning, fine-tuning, or weight adjustment. It maintainsaccuracy above 91% for both LHC and RHIC sets, while achieving above 96%accuracy for all RHIC sets, including the pT greater than 7 GeV/c set, despitenot having been trained on any RHIC data. These results indicate that the modelcaptures the underlying physics of high-energy collisions rather than justoverfitting to the training data. This work highlights the potential ofsimulation-trained models to be deployed in different energy domains,especially in underrepresented or data-limited settings.</description>
      <author>example@mail.com (Omar M. Khalaf, Ahmed M. Hamed)</author>
      <guid isPermaLink="false">2505.06732v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning Graph Representation of Agent Diffuser</title>
      <link>http://arxiv.org/abs/2505.06761v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AAMAS2025 International Conference on Autonomous Agents  and Multiagent Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LGR-AD的新型多智能体系统，旨在提高动态计算机视觉任务中的适应性。该系统通过模拟分布式系统中相互作用的智能体来建模生成过程，每个智能体代表一个专家子模型，并通过图神经网络进行协作。&lt;h4&gt;背景&lt;/h4&gt;扩散生成模型在文本到图像合成方面取得了显著进展，但静态模型参数可能无法最佳地处理生成过程的各个阶段。&lt;h4&gt;目的&lt;/h4&gt;提出LGR-AD系统，以提高动态计算机视觉任务中的适应性。&lt;h4&gt;方法&lt;/h4&gt;LGR-AD将生成过程建模为分布式系统，每个智能体代表一个专家子模型，并通过图神经网络进行协作。使用基于top-$k$最大生成树的协调机制，优化生成过程。每个智能体的决策由一个元模型指导，该模型最小化一个新颖的损失函数，平衡准确性和多样性。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析和大量实证评估表明，LGR-AD在多个基准测试中优于传统的扩散模型，显示出其在复杂图像生成任务中的可扩展性和灵活性。&lt;h4&gt;结论&lt;/h4&gt;LGR-AD系统在动态计算机视觉任务中具有提高适应性的潜力，并在图像生成任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于扩散的生成模型在文本到图像合成方面取得了显著进展，展示了令人印象深刻的文本理解和零样本泛化能力。这些模型根据文本提示从随机噪声中细化图像，初始对文本输入的依赖逐渐转向增强视觉保真度。这种转变表明，静态模型参数可能无法最佳地处理生成过程的各个阶段。我们引入了LGR-AD（学习智能体扩散器图表示），这是一种新型多智能体系统，旨在提高动态计算机视觉任务中的适应性。LGR-AD将生成过程建模为相互作用的智能体分布式系统，每个智能体代表一个专家子模型。这些智能体通过编码其关系和性能指标的图神经网络动态适应变化条件并协作。我们的方法采用基于top-$k$最大生成树的协调机制，优化生成过程。每个智能体的决策由一个元模型指导，该模型最小化一个新颖的损失函数，平衡准确性和多样性。理论分析和大量实证评估表明，LGR-AD在多个基准测试中优于传统的扩散模型，突出了其在复杂图像生成任务中的可扩展性和灵活性的潜力。代码可在以下链接获取：https://github.com/YousIA/LGR_AD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yousia/lgr_ad&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion-based generative models have significantly advanced text-to-imagesynthesis, demonstrating impressive text comprehension and zero-shotgeneralization. These models refine images from random noise based on textualprompts, with initial reliance on text input shifting towards enhanced visualfidelity over time. This transition suggests that static model parameters mightnot optimally address the distinct phases of generation. We introduce LGR-AD(Learning Graph Representation of Agent Diffusers), a novel multi-agent systemdesigned to improve adaptability in dynamic computer vision tasks. LGR-ADmodels the generation process as a distributed system of interacting agents,each representing an expert sub-model. These agents dynamically adapt tovarying conditions and collaborate through a graph neural network that encodestheir relationships and performance metrics. Our approach employs acoordination mechanism based on top-$k$ maximum spanning trees, optimizing thegeneration process. Each agent's decision-making is guided by a meta-model thatminimizes a novel loss function, balancing accuracy and diversity. Theoreticalanalysis and extensive empirical evaluations show that LGR-AD outperformstraditional diffusion models across various benchmarks, highlighting itspotential for scalable and flexible solutions in complex image generationtasks. Code is available at: https://github.com/YousIA/LGR_AD</description>
      <author>example@mail.com (Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan Dubiński, Ahmed Nabil Belbachir, Anis Yazidi)</author>
      <guid isPermaLink="false">2505.06761v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mixer-Informer-Based Two-Stage Transfer Learning for Long-Sequence Load Forecasting in Newly Constructed Electric Vehicle Charging Stations</title>
      <link>http://arxiv.org/abs/2505.06657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MIK-TST的创新两阶段迁移学习框架，用于精确预测电动汽车充电站的负荷，旨在提高智能电网效率和支撑可持续的电动汽车基础设施扩展。&lt;h4&gt;背景&lt;/h4&gt;随着电动汽车的快速普及，对充电站负荷的精确预测变得至关重要，但这一预测面临着长期时间依赖性和新设施数据有限的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效预测充电站负荷的MIK-TST框架，以提高充电站负荷预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;MIK-TST框架整合了Mixer、Informer和Kolmogorov-Arnold Networks (KAN)技术。Mixer融合了多源特征，Informer通过ProbSparse注意力机制捕捉长距离依赖性，而KAN则通过可学习的激活函数增强了非线性建模。该框架在大量数据上预训练，并在有限的目标数据上微调。&lt;h4&gt;主要发现&lt;/h4&gt;MIK-TST在MAE和MSE方面分别实现了4%和8%的降低，在Boulder，美国的一个包含26个充电站的数据库上优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;MIK-TST是一种可扩展的解决方案，能够提高智能电网的效率并支持可持续的电动汽车基础设施的扩展。&lt;h4&gt;翻译&lt;/h4&gt;The rapid rise in electric vehicle (EV) adoption demands precise charging station load forecasting, challenged by long-sequence temporal dependencies and limited data in new facilities. This study proposes MIK-TST, a novel two-stage transfer learning framework integrating Mixer, Informer, and Kolmogorov-Arnold Networks (KAN). The Mixer fuses multi-source features, Informer captures long-range dependencies via ProbSparse attention, and KAN enhances non-linear modeling with learnable activation functions. Pre-trained on extensive data and fine-tuned on limited target data, MIK-TST achieves 4% and 8% reductions in MAE and MSE, respectively, outperforming baselines on a dataset of 26 charging stations in Boulder, USA. This scalable solution enhances smart grid efficiency and supports sustainable EV infrastructure expansion.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid rise in electric vehicle (EV) adoption demands precise chargingstation load forecasting, challenged by long-sequence temporal dependencies andlimited data in new facilities. This study proposes MIK-TST, a novel two-stagetransfer learning framework integrating Mixer, Informer, and Kolmogorov-ArnoldNetworks (KAN). The Mixer fuses multi-source features, Informer captureslong-range dependencies via ProbSparse attention, and KAN enhances nonlinearmodeling with learnable activation functions. Pre-trained on extensive data andfine-tuned on limited target data, MIK-TST achieves 4% and 8% reductions in MAEand MSE, respectively, outperforming baselines on a dataset of 26 chargingstations in Boulder, USA. This scalable solution enhances smart grid efficiencyand supports sustainable EV infrastructure expansion.</description>
      <author>example@mail.com (Zhenhua Zhou, Bozhen Jiang, Qin Wang)</author>
      <guid isPermaLink="false">2505.06657v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Contrastive Federated Semi-Supervised Learning Intrusion Detection Framework for Internet of Robotic Things</title>
      <link>http://arxiv.org/abs/2505.06636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对物联网机器人（IoRT）网络入侵检测和防御的框架CFedSSL-NID，用于解决机器人本地没有标记数据且需要保护数据隐私的实际场景。&lt;h4&gt;背景&lt;/h4&gt;在智能工业和自动驾驶等环境中，物联网（IoT）与机器人高度集成形成物联网机器人（IoRT）。然而，IoRT的网络入侵可能导致数据泄露、服务中断，甚至通过控制机器人或车辆造成物理损害。&lt;h4&gt;目的&lt;/h4&gt;为了解决IoRT机器人本地缺乏标记数据且需要保护数据隐私的问题，提出了一种名为CFedSSL-NID的对比联邦半监督学习网络入侵检测框架。&lt;h4&gt;方法&lt;/h4&gt;CFedSSL-NID框架结合了随机弱和强数据增强、潜在对比学习和EMA更新，以整合监督信号，从而在机器人本地未标记数据上增强性能和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，CFedSSL-NID在基准数据集上优于现有的联邦半监督和全监督方法，并且具有更低的资源需求。&lt;h4&gt;结论&lt;/h4&gt;CFedSSL-NID是一种有效的IoRT入侵检测和防御框架，能够有效处理数据隐私保护问题，并具有较低的资源需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In intelligent industry, autonomous driving and other environments, theInternet of Things (IoT) highly integrated with robotic to form the Internet ofRobotic Things (IoRT). However, network intrusion to IoRT can lead to dataleakage, service interruption in IoRT and even physical damage by controllingrobots or vehicles. This paper proposes a Contrastive Federated Semi-SupervisedLearning Network Intrusion Detection framework (CFedSSL-NID) for IoRT intrusiondetection and defense, to address the practical scenario of IoRT where robotsdon't possess labeled data locally and the requirement for data privacypreserving. CFedSSL-NID integrates randomly weak and strong augmentation,latent contrastive learning, and EMA update to integrate supervised signals,thereby enhancing performance and robustness on robots' local unlabeled data.Extensive experiments demonstrate that CFedSSL-NID outperforms existingfederated semi-supervised and fully supervised methods on benchmark dataset andhas lower resource requirements.</description>
      <author>example@mail.com (Yifan Zeng)</author>
      <guid isPermaLink="false">2505.06636v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Reconstructing Brain Causal Dynamics for Subject and Task Fingerprints using fMRI Time-series Data</title>
      <link>http://arxiv.org/abs/2505.06392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种利用因果动力学进行fMRI基于主体和任务指纹识别的新方法，通过分析多尺度脑网络中的复杂关系，验证了其可行性和有效性。&lt;h4&gt;背景&lt;/h4&gt;系统神经科学因果模型近年来因其在多尺度脑网络中解析复杂关系的能力而重新引起关注。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法，利用因果动力学，以实现基于fMRI的个体和任务指纹识别。&lt;h4&gt;方法&lt;/h4&gt;应用隐式-显式离散化方案，开发了一种双时标线性状态空间模型。通过数据驱动识别其参数，该模型捕捉因果特征，包括从空间角度分析大脑区域之间的有向交互，以及从时间角度解耦大脑活动的快慢动态模式。这些因果特征随后与基于模型的主体识别的模态分解和投影方法以及用于基于学习的任务分类的图神经网络（GNN）框架相结合。此外，引入了大脑可达性景观的概念作为新的可视化工具，定量描述了在各种fMRI任务下大脑区域可能的最大激活水平。&lt;h4&gt;主要发现&lt;/h4&gt;使用人类连接组项目数据集评估了所提出的方法，并证明了其优于非因果方法的优点。获得的因果特征被可视化，并显示出与大脑功能已建立的理解的明确生物学相关性。&lt;h4&gt;结论&lt;/h4&gt;验证了利用大脑因果特征进行主体和任务指纹识别的可行性和有效性。此外，这项工作为因果指纹在健康对照和神经退行性疾病中的潜在应用的研究铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;The abstract of this paper is summarized as follows: Recently, there has been a renewed interest in system neuroscience causation models, driven by their unique capability to unravel complex relationships in multi-scale brain networks. In this paper, we present a novel method that leverages causal dynamics to achieve effective fMRI-based subject and task fingerprinting. By applying an implicit-explicit discretization scheme, we develop a two-timescale linear state-space model. Through data-driven identification of its parameters, the model captures causal signatures, including directed interactions among brain regions from a spatial perspective, and disentangled fast and slow dynamic modes of brain activity from a temporal perspective. These causal signatures are then integrated with: (i) a modal decomposition and projection method for model-based subject identification, and (ii) a Graph Neural Network (GNN) framework for learning-based task classification. Furthermore, we introduce the concept of the brain reachability landscape as a novel visualization tool, which quantitatively characterizes the maximum possible activation levels of brain regions under various fMRI tasks. We evaluate the proposed approach using the Human Connectome Project dataset and demonstrate its advantage over non-causality-based methods. The obtained causal signatures are visualized and demonstrate clear biological relevance with established understandings of brain function. We verified the feasibility and effectiveness of utilizing brain causal signatures for subject and task fingerprinting. Additionally, our work paves the way for further studies on causal fingerprints with potential applications in both healthy controls and neurodegenerative diseases.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Recently, there has been a revived interest in system neurosciencecausation models, driven by their unique capability to unravel complexrelationships in multi-scale brain networks. In this paper, we present a novelmethod that leverages causal dynamics to achieve effective fMRI-based subjectand task fingerprinting. Methods: By applying an implicit-explicitdiscretization scheme, we develop a two-timescale linear state-space model.Through data-driven identification of its parameters, the model captures causalsignatures, including directed interactions among brain regions from a spatialperspective, and disentangled fast and slow dynamic modes of brain activityfrom a temporal perspective. These causal signatures are then integrated with:(i) a modal decomposition and projection method for model-based subjectidentification, and (ii) a Graph Neural Network (GNN) framework forlearning-based task classification. Furthermore, we introduce the concept ofthe brain reachability landscape as a novel visualization tool, whichquantitatively characterizes the maximum possible activation levels of brainregions under various fMRI tasks. Results: We evaluate the proposed approachusing the Human Connectome Project dataset and demonstrate its advantage overnon-causality-based methods. The obtained causal signatures are visualized anddemonstrate clear biological relevance with established understandings of brainfunction. Conclusion: We verified the feasibility and effectiveness ofutilizing brain causal signatures for subject and task fingerprinting.Additionally, our work paves the way for further studies on causal fingerprintswith potential applications in both healthy controls and neurodegenerativediseases.</description>
      <author>example@mail.com (Dachuan Song, Li Shen, Duy Duong-Tran, Xuan Wang)</author>
      <guid isPermaLink="false">2505.06392v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines</title>
      <link>http://arxiv.org/abs/2505.06333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures, 2 tables, IJCAI 2025 (International Joint  Conferences on Artificial Intelligence) Special Track on AI4Tech: AI Enabling  Critical Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于神经符号AI和融合的多模态异常预测方法，用于现代装配管道中的产品质量和运营效率保障。&lt;h4&gt;背景&lt;/h4&gt;在复杂的多模态环境和高数据量的预测环境中，传统的单模态方法无法捕捉到精确异常预测所需的复杂关系。&lt;h4&gt;目的&lt;/h4&gt;确保装配管道中的产品质量和运营效率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于时间序列和图像的融合模型，并采用了决策级融合技术。研究基于三个主要创新方法：时间序列和图像的决策级融合建模、迁移学习用于融合和知识注入学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用迁移学习的神经符号AI融合方法能够有效地利用时间序列和图像数据的互补优势，为装配管道中的异常预测提供了一种稳健且可解释的方法，并提高了性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在公开的多模态数据集上进行了评估，并通过消融研究验证了预处理技术和融合模型的有效性。&lt;h4&gt;翻译&lt;/h4&gt;In modern assembly pipelines, identifying anomalies is crucial in ensuring product quality and operational efficiency. Conventional single-modality methods fail to capture the intricate relationships required for precise anomaly prediction in complex predictive environments with abundant data and multiple modalities. This paper proposes a neurosymbolic AI and fusion-based approach for multimodal anomaly prediction in assembly pipelines. We introduce a time series and image-based fusion model that leverages decision-level fusion techniques. Our research builds upon three primary novel approaches in multimodal learning: time series and image-based decision-level fusion modeling, transfer learning for fusion, and knowledge-infused learning. We evaluate the novel method using our derived and publicly available multimodal dataset and conduct comprehensive ablation studies to assess the impact of our preprocessing techniques and fusion model compared to traditional baselines. The results demonstrate that a neurosymbolic AI-based fusion approach that uses transfer learning can effectively harness the complementary strengths of time series and image data, offering a robust and interpretable approach for anomaly prediction in assembly pipelines with enhanced performance. The datasets, codes to reproduce the results, supplementary materials, and demo are available at https://github.com/ChathurangiShyalika/NSF-MAP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/chathurangishyalika/nsf-map&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern assembly pipelines, identifying anomalies is crucial in ensuringproduct quality and operational efficiency. Conventional single-modalitymethods fail to capture the intricate relationships required for preciseanomaly prediction in complex predictive environments with abundant data andmultiple modalities. This paper proposes a neurosymbolic AI and fusion-basedapproach for multimodal anomaly prediction in assembly pipelines. We introducea time series and image-based fusion model that leverages decision-level fusiontechniques. Our research builds upon three primary novel approaches inmultimodal learning: time series and image-based decision-level fusionmodeling, transfer learning for fusion, and knowledge-infused learning. Weevaluate the novel method using our derived and publicly available multimodaldataset and conduct comprehensive ablation studies to assess the impact of ourpreprocessing techniques and fusion model compared to traditional baselines.The results demonstrate that a neurosymbolic AI-based fusion approach that usestransfer learning can effectively harness the complementary strengths of timeseries and image data, offering a robust and interpretable approach for anomalyprediction in assembly pipelines with enhanced performance. \noindent Thedatasets, codes to reproduce the results, supplementary materials, and demo areavailable at https://github.com/ChathurangiShyalika/NSF-MAP.</description>
      <author>example@mail.com (Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, Amit Sheth)</author>
      <guid isPermaLink="false">2505.06333v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2505.06628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ACORN的算法，用于提高机器人在现实世界中的应用中的鲁棒性和安全性。&lt;h4&gt;背景&lt;/h4&gt;传统的Embodied AI研究过于关注成功率等性能指标，而忽略了在实际部署中出现的鲁棒性和安全性问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一差距，本文引入了四个新的以安全性为中心的指标，并提出了ACORN算法，以提高策略的鲁棒性而不牺牲性能。&lt;h4&gt;方法&lt;/h4&gt;ACORN利用对比学习同时将轨迹与专家演示对齐，并从可能的不安全行为中分离出来。它通过结构化高斯噪声注入生成信息丰富的负样本，同时使用双重扰动技术保持样本多样性，同时最小化计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;在多种操作环境中进行的综合实验验证了ACORN的有效性，与基线方法相比，在干扰下的安全性指标提高了高达23%。&lt;h4&gt;结论&lt;/h4&gt;ACORN在提高Embodied AI在安全性关键的实际应用中的可靠部署方面具有重大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Embodied AI研究传统上强调成功率等性能指标，而忽略了在实际部署中出现的鲁棒性和安全性问题。在实际情况中，智能体不断遇到不可预测的情况和分布变化，导致看似可靠的策略出现灾难性失败，尤其是在操作任务中。为了解决这一差距，我们引入了四个新的以安全性为中心的指标，以量化智能体对环境扰动的弹性。在这些指标的基础上，我们提出了自适应对比优化算法ACORN，这是一种即插即用的算法，它增强了策略的鲁棒性，而不牺牲性能。ACORN利用对比学习同时将轨迹与专家演示对齐，同时从可能的不安全行为中分离出来。我们的方法通过结构化高斯噪声注入高效地生成信息丰富的负样本，同时使用双重扰动技术保持样本多样性，同时最小化计算开销。在多种操作环境中进行的综合实验验证了ACORN的有效性，与基线方法相比，在干扰下的安全性指标提高了高达23%。这些发现强调了ACORN在使Embodied AI在安全性关键的实际应用中可靠部署方面的重要潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied AI research has traditionally emphasized performance metrics such assuccess rate and cumulative reward, overlooking critical robustness and safetyconsiderations that emerge during real-world deployment. In actualenvironments, agents continuously encounter unpredicted situations anddistribution shifts, causing seemingly reliable policies to experiencecatastrophic failures, particularly in manipulation tasks. To address this gap,we introduce four novel safety-centric metrics that quantify an agent'sresilience to environmental perturbations. Building on these metrics, wepresent Adaptive Contrastive Optimization for Robust Manipulation (ACORN), aplug-and-play algorithm that enhances policy robustness without sacrificingperformance. ACORN leverages contrastive learning to simultaneously aligntrajectories with expert demonstrations while diverging from potentially unsafebehaviors. Our approach efficiently generates informative negative samplesthrough structured Gaussian noise injection, employing a double perturbationtechnique that maintains sample diversity while minimizing computationaloverhead. Comprehensive experiments across diverse manipulation environmentsvalidate ACORN's effectiveness, yielding improvements of up to 23% in safetymetrics under disturbance compared to baseline methods. These findingsunderscore ACORN's significant potential for enabling reliable deployment ofembodied agents in safety-critical real-world applications.</description>
      <author>example@mail.com (Zhongquan Zhou, Shuhao Li, Zixian Yue)</author>
      <guid isPermaLink="false">2505.06628v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</title>
      <link>http://arxiv.org/abs/2505.06557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TCSVT 2025, doi at https://ieeexplore.ieee.org/document/10970001&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Positive Sample Mining (PSM)的框架，用于弱监督时间句子定位（WSTSG）任务，旨在从无剪辑视频中检测与语言描述相对应的时间区间。&lt;h4&gt;背景&lt;/h4&gt;现有的WSTSG方法通常通过生成负样本进行对比学习，但这些负样本与锚样本高度相似，直接将其作为负样本会导致优化困难，并忽略了这些相似样本与锚样本之间的相关性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出PSM框架，从训练集中挖掘正样本，提供更具区分性的监督。&lt;h4&gt;方法&lt;/h4&gt;PSM框架通过文本查询的相似性将剩余训练集划分为语义相似和不同子集。为了有效利用这些相关性，引入了PSM指导的对比损失和排名损失，以确保锚样本与相似样本更近，与不相似样本更远，并区分锚样本和负样本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PSM框架在WSTSG和grounded VideoQA任务上表现出有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;PSM框架能够有效提高WSTSG任务的性能，为视频理解提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The task of weakly supervised temporal sentence grounding (WSTSG) aims todetect temporal intervals corresponding to a language description fromuntrimmed videos with only video-level video-language correspondence. For ananchor sample, most existing approaches generate negative samples either fromother videos or within the same video for contrastive learning. However, sometraining samples are highly similar to the anchor sample, directly regardingthem as negative samples leads to difficulties for optimization and ignores thecorrelations between these similar samples and the anchor sample. To addressthis, we propose Positive Sample Mining (PSM), a novel framework that minespositive samples from the training set to provide more discriminativesupervision. Specifically, for a given anchor sample, we partition theremaining training set into semantically similar and dissimilar subsets basedon the similarity of their text queries. To effectively leverage thesecorrelations, we introduce a PSM-guided contrastive loss to ensure that theanchor proposal is closer to similar samples and further from dissimilar ones.Additionally, we design a PSM-guided rank loss to ensure that similar samplesare closer to the anchor proposal than to the negative intra-video proposal,aiming to distinguish the anchor proposal and the negative intra-videoproposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate theeffectiveness and superiority of our method.</description>
      <author>example@mail.com (Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang)</author>
      <guid isPermaLink="false">2505.06557v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Molecular Representation Learning via Structure Awareness</title>
      <link>http://arxiv.org/abs/2505.05877v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Image Processing (TIP) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该摘要介绍了一种名为MMSA的多模态自监督分子表示预训练框架，用于提升分子图表示的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确提取分子表示是药物发现过程中的关键步骤。近年来，分子表示学习方法取得了显著进展，特别是基于图像和2D/3D拓扑的多模态分子表示方法变得越来越主流。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有多模态方法直接融合不同模态信息，忽视模态间相互作用，未能充分捕捉分子之间的复杂高阶关系和不变特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于结构感知的多模态自监督分子表示预训练框架（MMSA），通过利用分子之间的不变知识来增强分子图表示。该框架包括两个主要模块：多模态分子表示学习模块和结构感知模块。&lt;h4&gt;主要发现&lt;/h4&gt;多模态分子表示学习模块协同处理同一种分子的不同模态信息，以克服模态差异并生成统一的分子嵌入。结构感知模块通过构建超图结构来建模分子之间的高阶相关性，并引入一个记忆机制来存储典型的分子表示，与记忆库中的记忆锚对齐，以整合不变知识，从而提高模型的一般化能力。&lt;h4&gt;结论&lt;/h4&gt;广泛的实验证明了MMSA的有效性，在MoleculeNet基准测试中取得了最先进的性能，与基线方法相比，平均ROC-AUC提高了1.8%至9.6%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate extraction of molecular representations is a critical step in thedrug discovery process. In recent years, significant progress has been made inmolecular representation learning methods, among which multi-modal molecularrepresentation methods based on images, and 2D/3D topologies have becomeincreasingly mainstream. However, existing these multi-modal approaches oftendirectly fuse information from different modalities, overlooking the potentialof intermodal interactions and failing to adequately capture the complexhigher-order relationships and invariant features between molecules. Toovercome these challenges, we propose a structure-awareness-based multi-modalself-supervised molecular representation pre-training framework (MMSA) designedto enhance molecular graph representations by leveraging invariant knowledgebetween molecules. The framework consists of two main modules: the multi-modalmolecular representation learning module and the structure-awareness module.The multi-modal molecular representation learning module collaborativelyprocesses information from different modalities of the same molecule toovercome intermodal differences and generate a unified molecular embedding.Subsequently, the structure-awareness module enhances the molecularrepresentation by constructing a hypergraph structure to model higher-ordercorrelations between molecules. This module also introduces a memory mechanismfor storing typical molecular representations, aligning them with memoryanchors in the memory bank to integrate invariant knowledge, thereby improvingthe model generalization ability. Extensive experiments have demonstrated theeffectiveness of MMSA, which achieves state-of-the-art performance on theMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to9.6% over baseline methods.</description>
      <author>example@mail.com (Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang)</author>
      <guid isPermaLink="false">2505.05877v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning</title>
      <link>http://arxiv.org/abs/2505.06321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用图学习来增强大型语言模型（LLMs）推理能力的框架。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型在各种领域取得了显著成功，但它们在训练成本和解决复杂推理问题方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，以实现LLMs更灵活和自适应的推理能力。&lt;h4&gt;方法&lt;/h4&gt;该框架将问题推理过程建模为图，并使用基于LLM的图学习来引导每个推理步骤的适应性生成。此外，引入了图神经网络（GNN）模块来对生成的推理过程进行表示学习，从而实现模型和提示的实时调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，这种方法在不要求额外训练或特定任务提示设计的情况下，显著提高了多个任务上的推理性能。&lt;h4&gt;结论&lt;/h4&gt;该框架为LLMs提供了更灵活和自适应的推理能力，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）在各个领域取得了显著的成功。然而，它们仍然面临着包括训练高计算成本和解决复杂推理问题限制在内的重大挑战。尽管现有方法通过结构化范式扩展了LLMs的推理能力，但这些方法通常依赖于特定任务的提示和预定义的推理过程，这限制了它们的灵活性和泛化能力。为了解决这些限制，我们提出了一种新颖的框架，该框架利用图学习来为LLMs提供更灵活和自适应的推理能力。具体而言，这种方法将问题推理过程建模为图，并使用基于LLM的图学习来引导每个推理步骤的适应性生成。为了进一步增强模型的适应性，我们引入了一个图神经网络（GNN）模块，对生成的推理过程进行表示学习，从而实现模型和提示的实时调整。实验结果表明，这种方法在多个任务上显著提高了推理性能，而不需要额外的训练或特定任务的提示设计。代码可在https://github.com/zch65458525/L2T上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zch65458525/l2t&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved remarkable success across variousdomains. However, they still face significant challenges, including highcomputational costs for training and limitations in solving complex reasoningproblems. Although existing methods have extended the reasoning capabilities ofLLMs through structured paradigms, these approaches often rely on task-specificprompts and predefined reasoning processes, which constrain their flexibilityand generalizability. To address these limitations, we propose a novelframework that leverages graph learning to enable more flexible and adaptivereasoning capabilities for LLMs. Specifically, this approach models thereasoning process of a problem as a graph and employs LLM-based graph learningto guide the adaptive generation of each reasoning step. To further enhance theadaptability of the model, we introduce a Graph Neural Network (GNN) module toperform representation learning on the generated reasoning process, enablingreal-time adjustments to both the model and the prompt. Experimental resultsdemonstrate that this method significantly improves reasoning performanceacross multiple tasks without requiring additional training or task-specificprompt design. Code can be found in https://github.com/zch65458525/L2T.</description>
      <author>example@mail.com (Hang Gao, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu)</author>
      <guid isPermaLink="false">2505.06321v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Contrastive Learning through Relative Similarity Preservation</title>
      <link>http://arxiv.org/abs/2505.05533v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI2025; full version including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图对比学习（GCL）在图数据上的应用，提出了一种新的框架RELGCL，通过保留自然相对相似性模式来提高学习效果。&lt;h4&gt;背景&lt;/h4&gt;GCL在计算机视觉领域取得了成功，但在图数据上由于图的离散性和非欧几里得性质，传统方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GCL框架，以解决图数据中的挑战，并提高学习效果。&lt;h4&gt;方法&lt;/h4&gt;通过分析11个真实世界的图，发现了一个普遍模式：随着结构距离的增加，标签一致性会系统地减少。利用随机游走理论对这一模式进行理论保证，并提出了RELGCL框架。&lt;h4&gt;主要发现&lt;/h4&gt;发现图自然地编码了相对相似性模式，结构上更接近的节点表现出更强的语义关系。&lt;h4&gt;结论&lt;/h4&gt;RELGCL框架在同质性和异质性图上均优于20种现有方法，验证了利用自然相对相似性比人工绝对相似性更有效。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) has achieved remarkable success by following the computer vision paradigm of preserving absolute similarity between augmented views. However, this approach faces fundamental challenges in graphs due to their discrete, non-Euclidean nature -- view generation often breaks semantic validity and similarity verification becomes unreliable. Through analyzing 11 real-world graphs, we discover a universal pattern transcending the homophily-heterophily dichotomy: label consistency systematically diminishes as structural distance increases, manifesting as smooth decay in homophily graphs and oscillatory decay in heterophily graphs. We establish theoretical guarantees for this pattern through random walk theory, proving label distribution convergence and characterizing the mechanisms behind different decay behaviors. This discovery reveals that graphs naturally encode relative similarity patterns, where structurally closer nodes exhibit collectively stronger semantic relationships. Leveraging this insight, we propose RELGCL, a novel GCL framework with complementary pairwise and listwise implementations that preserve these inherent patterns through collective similarity objectives. Extensive experiments demonstrate that our method consistently outperforms 20 existing approaches across both homophily and heterophily graphs, validating the effectiveness of leveraging natural relative similarity over artificial absolute similarity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) has achieved remarkable success by followingthe computer vision paradigm of preserving absolute similarity betweenaugmented views. However, this approach faces fundamental challenges in graphsdue to their discrete, non-Euclidean nature -- view generation often breakssemantic validity and similarity verification becomes unreliable. Throughanalyzing 11 real-world graphs, we discover a universal pattern transcendingthe homophily-heterophily dichotomy: label consistency systematicallydiminishes as structural distance increases, manifesting as smooth decay inhomophily graphs and oscillatory decay in heterophily graphs. We establishtheoretical guarantees for this pattern through random walk theory, provinglabel distribution convergence and characterizing the mechanisms behinddifferent decay behaviors. This discovery reveals that graphs naturally encoderelative similarity patterns, where structurally closer nodes exhibitcollectively stronger semantic relationships. Leveraging this insight, wepropose RELGCL, a novel GCL framework with complementary pairwise and listwiseimplementations that preserve these inherent patterns through collectivesimilarity objectives. Extensive experiments demonstrate that our methodconsistently outperforms 20 existing approaches across both homophily andheterophily graphs, validating the effectiveness of leveraging natural relativesimilarity over artificial absolute similarity.</description>
      <author>example@mail.com (Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou)</author>
      <guid isPermaLink="false">2505.05533v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence</title>
      <link>http://arxiv.org/abs/2505.06907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  On going work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了大型语言模型（LLMs）如ChatGPT、DeepSeek和Grok-3的兴起如何改变了人工智能领域，并提出了一种名为人工个性化智能（API）的愿景，旨在通过个性化定制来满足用户的具体需求，同时保持隐私和效率。&lt;h4&gt;背景&lt;/h4&gt;LLMs如ChatGPT等在生成类似人类内容方面表现出色，接近实现通用人工智能（AGI），但它们的大规模特性、对隐私的敏感性以及计算需求给个性化定制带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出人工个性化智能（API）的概念，旨在通过个性化联邦智能（PFI）将强大的模型适应于用户的具体需求，同时保持隐私和效率。&lt;h4&gt;方法&lt;/h4&gt;本文首先回顾了联邦学习（FL）和基础模型（FMs）的最新进展，并讨论了利用FMs增强联邦系统的潜力。然后，文章探讨了实现PFI的关键动机和这一领域的有希望的机会，包括高效的PFI、可信的PFI和由检索增强生成（RAG）赋能的PFI。&lt;h4&gt;主要发现&lt;/h4&gt;PFI结合了联邦学习的隐私保护优势以及基础模型的零样本泛化能力，使得在边缘进行个性化、高效且隐私保护的应用成为可能。&lt;h4&gt;结论&lt;/h4&gt;本文旨在为API作为AGI补充的发展奠定基础，特别关注PFI作为关键使能技术。&lt;h4&gt;翻译&lt;/h4&gt;本文讨论了大型语言模型（LLMs），如ChatGPT、DeepSeek和Grok-3的兴起如何改变了人工智能领域。作为构建在LLMs之上的基础模型（FMs）的突出例子，这些模型在生成类似人类内容方面表现出色，使我们更接近实现通用人工智能（AGI）。然而，它们的大规模特性、对隐私的敏感性和大量的计算需求给为最终用户进行个性化定制带来了重大挑战。为了弥合这一差距，本文提出了人工个性化智能（API）的愿景，重点在于将这些强大的模型适应于满足用户的具体需求，同时保持隐私和效率。具体而言，本文提出了个性化联邦智能（PFI），它将联邦学习的隐私保护优势与FMs的零样本泛化能力相结合，使得在边缘实现个性化、高效和隐私保护的应用成为可能。我们首先回顾了FL和FMs的最近进展，并讨论了利用FMs增强联邦系统的潜力。然后，我们提出了实现PFI的关键动机并探索了这一领域的有希望的机会，包括高效PFI、可信PFI以及由检索增强生成（RAG）赋能的PFI。最后，我们概述了在边缘部署FM驱动的FL系统时的关键挑战和未来研究方向，以实现改进的个性化、计算效率和隐私保证。总的来说，本文旨在为API作为AGI补充的发展奠定基础，特别关注PFI作为关键使能技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of large language models (LLMs), such as ChatGPT, DeepSeek, andGrok-3, has reshaped the artificial intelligence landscape. As prominentexamples of foundational models (FMs) built on LLMs, these models exhibitremarkable capabilities in generating human-like content, bringing us closer toachieving artificial general intelligence (AGI). However, their large-scalenature, sensitivity to privacy concerns, and substantial computational demandspresent significant challenges to personalized customization for end users. Tobridge this gap, this paper presents the vision of artificial personalizedintelligence (API), focusing on adapting these powerful models to meet thespecific needs and preferences of users while maintaining privacy andefficiency. Specifically, this paper proposes personalized federatedintelligence (PFI), which integrates the privacy-preserving advantages offederated learning (FL) with the zero-shot generalization capabilities of FMs,enabling personalized, efficient, and privacy-protective deployment at theedge. We first review recent advances in both FL and FMs, and discuss thepotential of leveraging FMs to enhance federated systems. We then present thekey motivations behind realizing PFI and explore promising opportunities inthis space, including efficient PFI, trustworthy PFI, and PFI empowered byretrieval-augmented generation (RAG). Finally, we outline key challenges andfuture research directions for deploying FM-powered FL systems at the edge withimproved personalization, computational efficiency, and privacy guarantees.Overall, this survey aims to lay the groundwork for the development of API as acomplement to AGI, with a particular focus on PFI as a key enabling technique.</description>
      <author>example@mail.com (Yu Qiao, Huy Q. Le, Avi Deb Raha, Phuong-Nam Tran, Apurba Adhikary, Mengchun Zhang, Loc X. Nguyen, Eui-Nam Huh, Dusit Niyato, Choong Seon Hong)</author>
      <guid isPermaLink="false">2505.06907v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>SynSHRP2: A Synthetic Multimodal Benchmark for Driving Safety-critical Events Derived from Real-world Driving Data</title>
      <link>http://arxiv.org/abs/2505.06276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a poster in CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了SynSHRP2，一个由SHRP 2 NDS数据合成的多模态驾驶数据集，用于解决SCEs数据难以获取的问题。&lt;h4&gt;背景&lt;/h4&gt;SCEs（与驾驶相关的安全关键事件）对于自动驾驶系统的发展和安全评估至关重要，但由于SCEs的稀有性和数据中存在的隐私信息，其获取存在挑战。&lt;h4&gt;目的&lt;/h4&gt;合成数据集以解决SCEs数据获取的难题，同时保护个人隐私。&lt;h4&gt;方法&lt;/h4&gt;使用StableDiffusion和ControlNet技术生成去识别化的关键帧，并包含详细标注，包括SCE类型、环境交通条件和事件前后的时序运动数据。&lt;h4&gt;主要发现&lt;/h4&gt;SynSHRP2数据集包含1874起碰撞和6924起险情，可用于事件属性分类和场景理解，为安全研究和自动驾驶系统开发提供潜力。&lt;h4&gt;结论&lt;/h4&gt;SynSHRP2数据集为安全研究提供了宝贵的资源，并展示了其在自动驾驶系统开发中的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;The paper introduces SynSHRP2, a synthetically created multimodal driving dataset derived from the SHRP 2 NDS, to address the challenge of accessing SCEs data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving-related safety-critical events (SCEs), including crashes andnear-crashes, provide essential insights for the development and safetyevaluation of automated driving systems. However, two major challenges limittheir accessibility: the rarity of SCEs and the presence of sensitive privacyinformation in the data. The Second Strategic Highway Research Program (SHRP 2)Naturalistic Driving Study (NDS), the largest NDS to date, collected millionsof hours of multimodal, high-resolution, high-frequency driving data fromthousands of participants, capturing thousands of SCEs. While this dataset isinvaluable for safety research, privacy concerns and data use restrictionssignificantly limit public access to the raw data. To address these challenges,we introduce SynSHRP2, a publicly available, synthetic, multimodal drivingdataset containing over 1874 crashes and 6924 near-crashes derived from theSHRP 2 NDS. The dataset features de-identified keyframes generated using StableDiffusion and ControlNet, ensuring the preservation of critical safety-relatedinformation while eliminating personally identifiable data. Additionally,SynSHRP2 includes detailed annotations on SCE type, environmental and trafficconditions, and time-series kinematic data spanning 5 seconds before and duringeach event. Synchronized keyframes and narrative descriptions further enhanceits usability. This paper presents two benchmarks for event attributeclassification and scene understanding, demonstrating the potentialapplications of SynSHRP2 in advancing safety research and automated drivingsystem development.</description>
      <author>example@mail.com (Liang Shi, Boyu Jiang, Zhenyuan Yuan, Miguel A. Perez, Feng Guo)</author>
      <guid isPermaLink="false">2505.06276v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders</title>
      <link>http://arxiv.org/abs/2505.06316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GRAPHCOMP的新方法，用于科学数据的错误受限有损压缩，该方法通过利用图神经网络技术来提高压缩比并控制数据失真。&lt;h4&gt;背景&lt;/h4&gt;科学数据的生成量巨大，对存储、传输和分析提出了挑战。现有的错误受限有损压缩方法往往忽略了科学数据中固有的空间和时间相关性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图基于方法，用于实现科学数据的错误受限有损压缩，同时保持较高的压缩比和较低的数据失真。&lt;h4&gt;方法&lt;/h4&gt;对原始网格数据进行不规则分割，生成保留空间和时间相关性的图表示。利用图神经网络设计一个时间图自编码器，学习压缩后的图表示。解压缩过程使用学习到的图模型和潜在表示来重建近似原始数据。&lt;h4&gt;主要发现&lt;/h4&gt;GRAPHCOMP在多个数据集上实现了最高的压缩比，比第二好的方法高出22%至50%。&lt;h4&gt;结论&lt;/h4&gt;GRAPHCOMP是一种有效的科学数据错误受限有损压缩方法，能够显著提高压缩比，同时满足用户定义的误差界限。&lt;h4&gt;翻译&lt;/h4&gt;The generation of large amounts of scientific data poses significant challenges for efficient storage, transfer, and analysis. Recently, error-bounded lossy compression methods have emerged due to their ability to achieve high compression ratios while controlling data distortion. However, they often overlook the inherent spatial and temporal correlations within scientific data, thus missing opportunities for higher compression. In this paper, we propose GRAPHCOMP, a novel graph-based method for error-bounded lossy compression of scientific data. We perform irregular segmentation of the original grid data and generate a graph representation that preserves the spatial and temporal correlations. Inspired by Graph Neural Networks (GNNs), we then propose a temporal graph autoencoder to learn latent representations that significantly reduce the size of the graph, effectively compressing the original data. Decompression reverses the process and utilizes the learned graph model together with the latent representation to reconstruct an approximation of the original data. The decompressed data are guaranteed to satisfy a user-defined point-wise error bound. We compare our method against the state-of-the-art error-bounded lossy methods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic data. GRAPHCOMP consistently achieves the highest compression ratio across most datasets, outperforming the second-best method by margins ranging from 22% to 50%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generation of voluminous scientific data poses significant challenges forefficient storage, transfer, and analysis. Recently, error-bounded lossycompression methods emerged due to their ability to achieve high compressionratios while controlling data distortion. However, they often overlook theinherent spatial and temporal correlations within scientific data, thus missingopportunities for higher compression. In this paper we propose GRAPHCOMP, anovel graph-based method for error-bounded lossy compression of scientificdata. We perform irregular segmentation of the original grid data and generatea graph representation that preserves the spatial and temporal correlations.Inspired by Graph Neural Networks (GNNs), we then propose a temporal graphautoencoder to learn latent representations that significantly reduce the sizeof the graph, effectively compressing the original data. Decompression reversesthe process and utilizes the learnt graph model together with the latentrepresentation to reconstruct an approximation of the original data. Thedecompressed data are guaranteed to satisfy a user-defined point-wise errorbound. We compare our method against the state-of-the-art error-bounded lossymethods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and syntheticdata. GRAPHCOMP consistently achieves the highest compression ratio across mostdatasets, outperforming the second-best method by margins ranging from 22% to50%.</description>
      <author>example@mail.com (Guozhong Li, Muhannad Alhumaidi, Spiros Skiadopoulos, Ibrahim Hoteit, Panos Kalnis)</author>
      <guid isPermaLink="false">2505.06316v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Improving Generalization of Medical Image Registration Foundation Model</title>
      <link>http://arxiv.org/abs/2505.06527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了可变形配准在医学图像处理中的重要性，以及如何通过结合Sharpness-Aware Minimization (SAM)技术来提升基础模型在医学图像配准中的泛化能力和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;可变形配准是医学图像处理中的基本任务，旨在通过建立非线性对应关系实现图像的精确对齐。传统方法在适应性和可解释性方面表现良好，但计算效率有限。深度学习方法提高了配准速度和准确性，但通常缺乏灵活性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型的局限性，本文提出将SAM技术整合到基础模型中，以提高其在医学图像配准中的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过优化损失函数的平坦度，SAM技术增强了模型在不同数据分布下的稳定性，并提升了其处理复杂临床场景的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，整合了SAM的基础模型在跨数据集配准性能方面取得了显著提升，为医学图像配准技术的进步提供了新的见解。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法通过结合SAM技术，有效提升了基础模型在医学图像配准中的性能，为该领域的研究提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;This paper discusses the importance of deformable registration in medical image processing and proposes the integration of Sharpness-Aware Minimization (SAM) technology to enhance the generalization and robustness of foundation models in medical image registration.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/promise13/fm_sam&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deformable registration is a fundamental task in medical image processing,aiming to achieve precise alignment by establishing nonlinear correspondencesbetween images. Traditional methods offer good adaptability andinterpretability but are limited by computational efficiency. Although deeplearning approaches have significantly improved registration speed andaccuracy, they often lack flexibility and generalizability across differentdatasets and tasks. In recent years, foundation models have emerged as apromising direction, leveraging large and diverse datasets to learn universalfeatures and transformation patterns for image registration, thus demonstratingstrong cross-task transferability. However, these models still face challengesin generalization and robustness when encountering novel anatomical structures,varying imaging conditions, or unseen modalities. To address these limitations,this paper incorporates Sharpness-Aware Minimization (SAM) into foundationmodels to enhance their generalization and robustness in medical imageregistration. By optimizing the flatness of the loss landscape, SAM improvesmodel stability across diverse data distributions and strengthens its abilityto handle complex clinical scenarios. Experimental results show that foundationmodels integrated with SAM achieve significant improvements in cross-datasetregistration performance, offering new insights for the advancement of medicalimage registration technology. Our code is available athttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.</description>
      <author>example@mail.com (Jing Hu, Kaiwei Yu, Hongjiang Xian, Shu Hu, Xin Wang)</author>
      <guid isPermaLink="false">2505.06527v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Climate in a Bottle: Towards a Generative Foundation Model for the Kilometer-Scale Global Atmosphere</title>
      <link>http://arxiv.org/abs/2505.06474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于生成扩散模型的气候模拟器cBottle，用于全球千米级气候模拟和再分析，旨在压缩、增强有限集成和改善与PB级气候预测数据交互的延迟。&lt;h4&gt;背景&lt;/h4&gt;现有的自回归范式在气候时间尺度上训练具有挑战性，因为存在漂移、不稳定性和组件耦合问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种条件生成模型作为替代方案，并展示cBottle框架在气候模拟和再分析中的应用。&lt;h4&gt;方法&lt;/h4&gt;cBottle由两个模型阶段组成：一个全局训练的粗分辨率图像生成器，用于生成基于月平均海面温度和太阳条件下的100公里（50k像素）字段；接着是一个局部训练的16倍超分辨率阶段，用于生成5公里（12.5M像素）字段；全局超分辨率通过重叠块多扩散方法实现。&lt;h4&gt;主要发现&lt;/h4&gt;cBottle在一系列气候模型诊断方面表现出潜力，包括日到季节尺度的变化、大型变化模式、热带气旋统计以及气候变化和极端天气的趋势。&lt;h4&gt;结论&lt;/h4&gt;cBottle不仅是一个模拟器，也是向基础模型迈进的一步，它通过桥接多个数据模态（再分析和模拟）以及对应的超越模拟的任务（如零样本偏差校正、气候降尺度以及信道填充）来实现这一目标。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于生成扩散模型的气候模拟器cBottle，用于全球千米级气候模拟和再分析，旨在压缩、增强有限集成和改善与PB级气候预测数据交互的延迟。然而，现有的自回归范式在气候时间尺度上训练具有挑战性，因为存在漂移、不稳定性和组件耦合问题。为此，本文提出条件生成模型作为替代方案，并展示了cBottle框架在气候模拟和再分析中的应用。cBottle由两个模型阶段组成：一个全局训练的粗分辨率图像生成器，用于生成基于月平均海面温度和太阳条件下的100公里（50k像素）字段；接着是一个局部训练的16倍超分辨率阶段，用于生成5公里（12.5M像素）字段；全局超分辨率通过重叠块多扩散方法实现。cBottle在一系列气候模型诊断方面表现出潜力，包括日到季节尺度的变化、大型变化模式、热带气旋统计以及气候变化和极端天气的趋势。cBottle不仅是一个模拟器，也是向基础模型迈进的一步，它通过桥接多个数据模态（再分析和模拟）以及对应的超越模拟的任务（如零样本偏差校正、气候降尺度以及信道填充）来实现这一目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI emulators offer a path to compressing, boosting limited ensembles, andimproving the latency of interacting with petabyte-scale climate predictiondata. However, prevailing auto-regressive paradigms offer limited flexibility,and are challenging to train on climate time horizons due to drifts,instabilities and component-coupling challenges. Conditionally generativemodels offer an appealing alternative. In this context we demonstrate agenerative diffusion-based framework -- Climate in a Bottle (cBottle) -- foremulating global km-scale climate simulations and reanalysis on the equal-areaHEALPix grid. cBottle consists of two model stages: a globally-trainedcoarse-resolution image generator that generates 100km (50k-pixel) fields givenmonthly average sea surface temperatures and solar conditioning, followed by alocally-trained 16x super-resolution stage that generates 5km (12.5M-pixel)fields; global super-resolution is made affordable using an overlappingpatch-based multi-diffusion. Overall, cBottle shows promise as an emulatoracross a battery of climate model diagnostics, including diurnal-to-seasonalscale variability, large-scale modes of variability, tropical cyclonestatistics, and trends of climate change and weather extremes. Moreover,cBottle is a step towards a foundation model, by bridging multiple datamodalities (reanalysis and simulation) with corresponding utility beyondemulation to tasks such as zero-shot bias correction, climate downscaling, andchannel in-filling.</description>
      <author>example@mail.com (Noah D. Brenowitz, Tao Ge, Akshay Subramaniam, Aayush Gupta, David M. Hall, Morteza Mardani, Arash Vahdat, Karthik Kashinath, Michael S. Pritchard)</author>
      <guid isPermaLink="false">2505.06474v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
      <link>http://arxiv.org/abs/2505.05181v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Stochastic Variational Propagation (SVP)方法，作为深度学习中的反向传播（BP）的替代方案，以提高可扩展性和减少内存占用。&lt;h4&gt;背景&lt;/h4&gt;反向传播（BP）依赖于全局梯度同步，这限制了其可扩展性并带来显著的内存开销。&lt;h4&gt;目的&lt;/h4&gt;提出SVP方法，将训练重新构造成层次化的变分推断，以实现可扩展的训练。&lt;h4&gt;方法&lt;/h4&gt;SVP将层激活视为潜在变量，并优化局部的证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。为了防止层间表示的崩溃，SVP使用固定随机矩阵将激活投影到低维空间，同时结合特征对齐损失以保持层间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP具有竞争力的精度，内存使用减少最多4倍，显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;SVP引入了概率视角到深度表示学习，为构建更模块化和可解释的神经网络设计开辟了途径。&lt;h4&gt;翻译&lt;/h4&gt;Backpropagation (BP) 是深度学习的基础，但其对全局梯度同步的依赖限制了可扩展性并带来显著的内存开销。我们提出了Stochastic Variational Propagation (SVP)，作为一种可扩展的替代方案，将训练重新构造成层次化的变分推断。SVP将层激活视为潜在变量，并优化局部的证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。然而，直接在层间ELBOs中应用KL散度可能导致层间表示的崩溃，因为过度压缩。为了防止这种情况，SVP通过固定随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。结合特征对齐损失以保持层间一致性，SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP具有竞争力的精度，内存使用减少最多4倍，显著提高了可扩展性。更广泛地说，SVP为深度表示学习引入了概率视角，为构建更模块化和可解释的神经网络设计开辟了途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backpropagation (BP) is the cornerstone of deep learning, but its reliance onglobal gradient synchronization limits scalability and imposes significantmemory overhead. We propose Stochastic Variational Propagation (SVP), ascalable alternative that reframes training as hierarchical variationalinference. SVP treats layer activations as latent variables and optimizes localEvidence Lower Bounds (ELBOs), enabling independent, local updates whilepreserving global coherence. However, directly applying KL divergence inlayer-wise ELBOs risks inter-layer's representation collapse due to excessivecompression. To prevent this, SVP projects activations into low-dimensionalspaces via fixed random matrices, ensuring information preservation andrepresentational diversity. Combined with a feature alignment loss forinter-layer consistency, SVP achieves competitive accuracy with BP acrossdiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST toImageNet), reduces memory usage by up to 4x, and significantly improvesscalability. More broadly, SVP introduces a probabilistic perspective to deeprepresentation learning, opening pathways toward more modular and interpretableneural network design.</description>
      <author>example@mail.com (Bojian Yin, Federico Corradi)</author>
      <guid isPermaLink="false">2505.05181v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A New DAPO Algorithm for Stock Trading</title>
      <link>http://arxiv.org/abs/2505.06408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE IDS 2025 Special Track: Financial Reinforcement  Learning and Foundation Models (FinRLFM). 3 pages, 2 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究强化学习在金融交易中的应用，设计了一种结合改进的GRPO算法、DAPO思想和LLM提取的风险和情绪信号的交易代理。&lt;h4&gt;背景&lt;/h4&gt;强化学习在语言模型配合下表现优异，但其在金融交易中的潜力尚待探索。&lt;h4&gt;目的&lt;/h4&gt;探讨是否可以通过强化学习实现金融交易中的类似增益。&lt;h4&gt;方法&lt;/h4&gt;设计了一种交易代理，结合改进的GRPO算法、DAPO思想和从金融新闻中提取的LLM风险和情绪信号。&lt;h4&gt;主要发现&lt;/h4&gt;在NASDAQ-100指数上，该代理实现了230.49%的累计回报和0.37的信息比率，优于CPPO-DeepSeek基准。同时，训练时间缩短至2.5小时，RAM使用量显著降低。&lt;h4&gt;结论&lt;/h4&gt;提出的RL-LLM框架为数据高效交易代理提供了一个可扩展的路径。&lt;h4&gt;翻译&lt;/h4&gt;The study explores the application of reinforcement learning in financial trading, designing a trading agent that combines an improved GRPO algorithm, DAPO ideas, and LLM-based risk and sentiment signals extracted from financial news.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in reinforcement learning, such as Dynamic Sampling PolicyOptimization (DAPO), show strong performance when paired with large languagemodels (LLMs). Motivated by this success, we ask whether similar gains can berealized in financial trading. We design a trading agent that combines animproved Group Relative Policy Optimization (GRPO) algorithm, augmented withideas from DAPO, with LLM-based risk and sentiment signals extracted fromfinancial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains acumulative return of 230.49 percent and an information ratio of 0.37,outperforming the CPPO-DeepSeek baseline. It also cuts training time from about8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. Theproposed RL-LLM framework offers a scalable path toward data-efficient tradingagents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/</description>
      <author>example@mail.com (Ruijian Zha, Bojun Liu)</author>
      <guid isPermaLink="false">2505.06408v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.06301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于边缘增强的图神经网络框架，用于解决人类活动识别（HAR）中的跨用户变异性问题。&lt;h4&gt;背景&lt;/h4&gt;由于传感器放置、身体动态和行为模式的不同，跨用户变异性在人类活动识别中仍然是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，能够捕捉用户间共有的生物力学不变量，从而提高传统方法的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;该方法将解剖学相关性知识整合到一个统一的图神经网络架构中，通过编码领域不变特征来解决用户特定变异性，并使用变分边缘特征提取器来处理这个问题。梯度反转层（GRL）用于执行对抗性领域泛化，确保对未见用户的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在OPPORTUNITY和DSADS数据集上的大量实验表明，该方法取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过信息融合技术，本文将生物力学原理与基于图的对抗性学习相结合，为跨用户HAR构建了一个统一和通用的模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在人类活动识别（HAR）中，由于传感器放置、身体动态和行为模式的不同，跨用户变异性仍然是一个关键挑战。传统的方 法往往无法捕捉用户间持续存在的生物力学不变量，限制了它们的泛化能力。我们提出了一种边缘增强的图神经网络（GNN）架构的对抗性领域泛化（EEG-ADG）框架，将解剖学相关性知识整合到统一框架中。通过建模三个生物力学动机关系——相互连接单元、类似单元和侧向单元——我们的方法编码了领域不变特征，并通过变分边缘特征提取器解决了用户特定变异性问题。梯度反转层（GRL）执行对抗性领域泛化，确保对未见用户的鲁棒性。在OPPORTUNITY和DSADS数据集上的大量实验证明了该方法的最佳性能。通过信息融合技术，我们的工作将生物力学原理与基于图的对抗性学习相结合，为跨用户HAR构建了一个统一和通用的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-user variability in Human Activity Recognition (HAR) remains a criticalchallenge due to differences in sensor placement, body dynamics, and behavioralpatterns. Traditional methods often fail to capture biomechanical invariantsthat persist across users, limiting their generalization capability. We proposean Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)framework that integrates anatomical correlation knowledge into a unified graphneural network (GNN) architecture. By modeling three biomechanically motivatedrelationships together-Interconnected Units, Analogous Units, and LateralUnits-our method encodes domain-invariant features while addressinguser-specific variability through Variational Edge Feature Extractor. AGradient Reversal Layer (GRL) enforces adversarial domain generalization,ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY andDSADS datasets demonstrate state-of-the-art performance. Our work bridgesbiomechanical principles with graph-based adversarial learning by integratinginformation fusion techniques. This fusion of information underpins our unifiedand generalized model for cross-user HAR.</description>
      <author>example@mail.com (Xiaozhou Ye, Kevin I-Kai Wang)</author>
      <guid isPermaLink="false">2505.06301v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.06282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, Accepted by SIGIR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IFL-GCL的图对比学习方法，旨在解决传统GCL在语义相似对被错误分类为负样本导致的采样偏差问题，并通过在图预训练框架和LLM增强器中实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图对比学习（GCL）在图基础模型或LLM作为图增强器的研究中发挥着关键作用。然而，传统的GCL方法通过数据增强定义自监督任务，导致语义相似对被错误分类为负样本，从而限制了性能。&lt;h4&gt;目的&lt;/h4&gt;将GCL视为正未标记（PU）学习问题，并通过语义指导的自监督任务定义来改进GCL的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为IFL-GCL的方法，使用InfoNCE来提取语义信息，并通过证明在InfoNCE下节点对的表示相似性与对应对比样本为正样本的概率一致，重新定义了基于校正样本的最大似然目标，从而得到一个新的InfoNCE损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在图预训练框架和LLM作为增强器的情况下，IFL-GCL在独立同分布（IID）和无关领域（OOD）场景中都实现了显著的性能提升，最高达到了9.05%的改进。&lt;h4&gt;结论&lt;/h4&gt;语义指导的IFL-GCL方法验证了其在图对比学习中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;As an important graph pre-training method, Graph Contrastive Learning (GCL) continues to play a crucial role in the ongoing surge of research on graph foundation models or LLM as enhancer for graphs. Traditional GCL optimizes InfoNCE by using augmentations to define self-supervised tasks, treating augmented pairs as positive samples and others as negative. However, this leads to semantically similar pairs being classified as negative, causing significant sampling bias and limiting performance. In this paper, we argue that GCL is essentially a Positive-Unlabeled (PU) learning problem, where the definition of self-supervised tasks should be semantically guided, i.e., augmented samples with similar semantics are considered positive, while others, with unknown semantics, are treated as unlabeled. From this perspective, the key lies in how to extract semantic information. To achieve this, we propose IFL-GCL, using InfoNCE as a 'free lunch' to extract semantic information. Specifically, we first prove that under InfoNCE, the representation similarity of node pairs aligns with the probability that the corresponding contrastive sample is positive. Then we redefine the maximum likelihood objective based on the corrected samples, leading to a new InfoNCE loss function. Extensive experiments on both the graph pretraining framework and LLM as an enhancer show significantly improvements of IFL-GCL in both IID and OOD scenarios, achieving up to a 9.05% improvement, validating the effectiveness of semantically guided. Code for IFL-GCL is publicly available at: https://github.com/Camel-Prince/IFL-GCL.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3726302.3730007&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As an important graph pre-training method, Graph Contrastive Learning (GCL)continues to play a crucial role in the ongoing surge of research on graphfoundation models or LLM as enhancer for graphs. Traditional GCL optimizesInfoNCE by using augmentations to define self-supervised tasks, treatingaugmented pairs as positive samples and others as negative. However, this leadsto semantically similar pairs being classified as negative, causing significantsampling bias and limiting performance. In this paper, we argue that GCL isessentially a Positive-Unlabeled (PU) learning problem, where the definition ofself-supervised tasks should be semantically guided, i.e., augmented sampleswith similar semantics are considered positive, while others, with unknownsemantics, are treated as unlabeled. From this perspective, the key lies in howto extract semantic information. To achieve this, we propose IFL-GCL, usingInfoNCE as a "free lunch" to extract semantic information. Specifically, Wefirst prove that under InfoNCE, the representation similarity of node pairsaligns with the probability that the corresponding contrastive sample ispositive. Then we redefine the maximum likelihood objective based on thecorrected samples, leading to a new InfoNCE loss function. Extensiveexperiments on both the graph pretraining framework and LLM as an enhancer showsignificantly improvements of IFL-GCL in both IID and OOD scenarios, achievingup to a 9.05% improvement, validating the effectiveness of semantically guided.Code for IFL-GCL is publicly available at:https://github.com/Camel-Prince/IFL-GCL.</description>
      <author>example@mail.com (Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng)</author>
      <guid isPermaLink="false">2505.06282v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume</title>
      <link>http://arxiv.org/abs/2505.06292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Graph Neural Network for Urban Interpolation (GNNUI)，一种新型的城市交通流量估计方法，并提出了两个新的开放的大规模城市交通流量基准。&lt;h4&gt;背景&lt;/h4&gt;交通流量数据对城市规划至关重要，但交通传感器部署和维护成本高，导致数据稀疏。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的城市交通流量估计方法，解决交通数据稀疏的问题。&lt;h4&gt;方法&lt;/h4&gt;GNNUI使用掩码算法学习插值，整合节点特征以捕捉功能角色，并使用针对零膨胀交通分布的损失函数。同时，提出了两个新的城市交通流量基准：柏林的Strava自行车数据和纽约市的出租车数据。&lt;h4&gt;主要发现&lt;/h4&gt;GNNUI在各种指标（MAE、RMSE、真实零率、Kullback-Leibler散度）上优于其他基于图的方法，并且在不同传感器覆盖率下保持鲁棒性。在Strava和出租车数据集上，GNNUI在极端数据稀缺的情况下表现良好，MAE仅略有增加。&lt;h4&gt;结论&lt;/h4&gt;GNNUI是一种有效且鲁棒的城市交通流量估计方法，有助于解决交通数据稀疏问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：可靠的地面级交通流量数据，涵盖多种交通方式，有助于城市规划，通过提供基础设施改善、交通管理和公共交通决策的信息。然而，由于部署和维护成本高，测量交通流量的传感器通常分布稀疏。为了解决这个问题，插值方法可以使用可用数据估计未观测位置的交通流量。图神经网络在交通流量预测中表现出强大的性能，尤其是在高速公路和主要干道网络上。然而，将它们应用于城市环境带来独特的挑战：城市网络表现出更大的结构多样性，交通流量高度过度分散，存在许多零值，最佳的空间依赖性建模方法尚不清楚，传感器覆盖率通常非常稀疏。我们引入了Graph Neural Network for Urban Interpolation（GNNUI），一种新颖的城市交通流量估计方法。GNNUI使用掩码算法学习插值，整合节点特征以捕捉功能角色，并使用针对零膨胀交通分布的损失函数。除了模型之外，我们还引入了两个新的公开的大规模城市交通流量基准，涵盖了不同的交通方式：柏林和纽约市的Strava自行车数据和出租车数据。GNNUI在各种指标（MAE、RMSE、真实零率、Kullback-Leibler散度）上优于最近的基于图的方法，并且在不同传感器覆盖率下保持鲁棒性。例如，在Strava上，MAE仅从7.1增加到10.5，在出租车数据上从23.0增加到40.4，表明在现实世界城市环境中常见的数据稀缺情况下，GNNUI表现良好。我们还考察了图连接选择如何影响模型精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable street-level traffic volume data, covering multiple modes oftransportation, helps urban planning by informing decisions on infrastructureimprovements, traffic management, and public transportation. Yet, trafficsensors measuring traffic volume are typically scarcely located, due to theirhigh deployment and maintenance costs. To address this, interpolation methodscan estimate traffic volumes at unobserved locations using available data.Graph Neural Networks have shown strong performance in traffic volumeforecasting, particularly on highways and major arterial networks. Applyingthem to urban settings, however, presents unique challenges: urban networksexhibit greater structural diversity, traffic volumes are highly overdispersedwith many zeros, the best way to account for spatial dependencies remainsunclear, and sensor coverage is often very sparse. We introduce the GraphNeural Network for Urban Interpolation (GNNUI), a novel urban traffic volumeestimation approach. GNNUI employs a masking algorithm to learn interpolation,integrates node features to capture functional roles, and uses a loss functiontailored to zero-inflated traffic distributions. In addition to the model, weintroduce two new open, large-scale urban traffic volume benchmarks, coveringdifferent transportation modes: Strava cycling data from Berlin and New YorkCity taxi data. GNNUI outperforms recent, some graph-based, interpolationmethods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAErises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strongperformance under extreme data scarcity, common in real-world urban settings.We also examine how graph connectivity choices influence model accuracy.</description>
      <author>example@mail.com (Silke K. Kaiser, Filipe Rodrigues, Carlos Lima Azevedo, Lynn H. Kaack)</author>
      <guid isPermaLink="false">2505.06292v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Soft causal learning for generalized molecule property prediction: An environment perspective</title>
      <link>http://arxiv.org/abs/2505.06283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 7 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种软因果学习框架，旨在解决分子科学中未解决的样本外分布（OOD）挑战。&lt;h4&gt;背景&lt;/h4&gt;分子图学习在AI科学中日益重要，但现有方法在处理样本外分布样本时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，以充分建模分子环境并绕过不变子图，从而解决分子科学中的样本外分布挑战。&lt;h4&gt;方法&lt;/h4&gt;1) 将化学理论融入图增长生成器以模仿扩展环境；2) 设计基于GIB的目标函数以将环境从整个图中分离出来；3) 引入基于交叉注意力的软因果交互，允许环境和不变性之间的动态交互。&lt;h4&gt;主要发现&lt;/h4&gt;1) 扩展的原子模式导致基于不变理性模型的失败；2) 发现的分子子图与对应属性之间的关联复杂，因果子结构无法完全解释标签；3) 环境和不变性之间的相互作用相互影响，难以建模。&lt;h4&gt;结论&lt;/h4&gt;实验表明，所提出的框架具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种软因果学习框架，旨在解决分子科学中未解决的样本外分布（OOD）挑战。在AI科学中，分子图学习已成为一个日益重要的主题，但现有方法在处理样本外分布样本时存在局限性。本文提出了一种框架，旨在通过充分建模分子环境并绕过不变子图来解决分子科学中的样本外分布挑战。具体来说，本文首先将化学理论融入图增长生成器以模仿扩展环境，然后设计了一种基于GIB的目标函数以将环境从整个图中分离出来，最后引入了一种基于交叉注意力的软因果交互，允许环境和不变性之间的动态交互。通过在七个数据集上进行的实验，包括模仿不同类型的样本外泛化场景，广泛的比较、消融实验以及可视化案例研究，证明了所提出框架的良好泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning on molecule graphs has become an increasingly important topic in AIfor science, which takes full advantage of AI to facilitate scientificdiscovery. Existing solutions on modeling molecules utilize Graph NeuralNetworks (GNNs) to achieve representations but they mostly fail to adapt modelsto out-of-distribution (OOD) samples. Although recent advances on OOD-orientedgraph learning have discovered the invariant rationale on graphs, they stillignore three important issues, i.e., 1) the expanding atom patterns regardingenvironments on graphs lead to failures of invariant rationale based models, 2)the associations between discovered molecular subgraphs and correspondingproperties are complex where causal substructures cannot fully interpret thelabels. 3) the interactions between environments and invariances can influencewith each other thus are challenging to be modeled. To this end, we propose asoft causal learning framework, to tackle the unresolved OOD challenge inmolecular science, from the perspective of fully modeling the moleculeenvironments and bypassing the invariant subgraphs. Specifically, we firstincorporate chemistry theories into our graph growth generator to imitateexpaned environments, and then devise an GIB-based objective to disentangleenvironment from whole graphs and finally introduce a cross-attention basedsoft causal interaction, which allows dynamic interactions between environmentsand invariances. We perform experiments on seven datasets by imitatingdifferent kinds of OOD generalization scenarios. Extensive comparison, ablationexperiments as well as visualized case studies demonstrate well generalizationability of our proposal.</description>
      <author>example@mail.com (Limin Li, Kuo Yang, Wenjie Du, Pengkun Wang, Zhengyang Zhou, Yang Wang)</author>
      <guid isPermaLink="false">2505.06283v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
      <link>http://arxiv.org/abs/2505.05472v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Mogao Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Mogao的统一框架，该框架通过因果方法实现交错的多模态生成，并在架构设计上进行了多项关键技术改进。&lt;h4&gt;背景&lt;/h4&gt;尽管统一模型在图像理解和生成方面取得了显著进展，但大多数方法仍然局限于基于多模态的单模态生成。&lt;h4&gt;目的&lt;/h4&gt;提出Mogao框架，以实现交错的多模态生成，并提升多模态理解和文本到图像生成的性能。&lt;h4&gt;方法&lt;/h4&gt;Mogao通过以下技术改进实现其目标：深度融合设计、双重视觉编码器、交错旋转位置嵌入和多模态无分类器引导。此外，还引入了一种高效的大规模数据集训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;Mogao在多模态理解和文本到图像生成方面达到了最先进的性能，同时擅长生成高质量、连贯的交错输出。其在零样本图像编辑和组合生成方面的能力，使其成为实用的全模态基础模型。&lt;h4&gt;结论&lt;/h4&gt;Mogao为统一多模态系统的发展开辟了道路，并具有未来扩展的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in unified models for image understanding and generation hasbeen impressive, yet most approaches remain limited to single-modal generationconditioned on multiple modalities. In this paper, we present Mogao, a unifiedframework that advances this paradigm by enabling interleaved multi-modalgeneration through a causal approach. Mogao integrates a set of key technicalimprovements in architecture design, including a deep-fusion design, dualvision encoders, interleaved rotary position embeddings, and multi-modalclassifier-free guidance, which allow it to harness the strengths of bothautoregressive models for text generation and diffusion models for high-qualityimage synthesis. These practical improvements also make Mogao particularlyeffective to process interleaved sequences of text and images arbitrarily. Tofurther unlock the potential of unified models, we introduce an efficienttraining strategy on a large-scale, in-house dataset specifically curated forjoint text and image generation. Extensive experiments show that Mogao not onlyachieves state-of-the-art performance in multi-modal understanding andtext-to-image generation, but also excels in producing high-quality, coherentinterleaved outputs. Its emergent capabilities in zero-shot image editing andcompositional generation highlight Mogao as a practical omni-modal foundationmodel, paving the way for future development and scaling the unifiedmulti-modal systems.</description>
      <author>example@mail.com (Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang)</author>
      <guid isPermaLink="false">2505.05472v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
      <link>http://arxiv.org/abs/2505.02350v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用稀疏椭圆径向基函数网络逼近点云符号距离函数（SDF）的机器学习方法，实现了紧凑且精确的表面表示。&lt;h4&gt;背景&lt;/h4&gt;点云表面表示是计算机图形学和视觉领域的一个基本问题。&lt;h4&gt;目的&lt;/h4&gt;目的是通过尽可能少的椭圆径向基函数（ERBFs）精确地逼近点云的SDF，以实现SDF的稀疏表示。&lt;h4&gt;方法&lt;/h4&gt;方法包括：引入动态多目标优化策略平衡稀疏性和逼近精度；采用基于最近邻的数据结构提高计算效率；在CUDA上并行化每个核的计算；以及设计基于八叉树的细化策略进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在准确性、鲁棒性和计算效率方面优于之前的稀疏表示方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为点云表面表示提供了一种高效且准确的新途径，其对应的可执行程序已公开。&lt;h4&gt;翻译&lt;/h4&gt;Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using a sparse ellipsoidal radial basis function network, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding executable program is publicly available at https://github.com/lianbobo/SE-RBFNet.git.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/lianbobo/se-rbfnet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud surface representation is a fundamental problem in computergraphics and vision. This paper presents a machine learning approach forapproximating the signed distance function (SDF) of a point cloud using asparse ellipsoidal radial basis function network, enabling a compact andaccurate surface representation. Given the SDF values defined on the gridpoints constructed from the point cloud, our method approximates the SDFaccurately with as few ellipsoidal radial basis functions (ERBFs) as possible,i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsityand approximation precision, a dynamic multi-objective optimization strategy isintroduced, which adaptively adds the regularization terms and jointlyoptimizes the weights, centers, shapes, and orientations of ERBFs. To improvecomputational efficiency, a nearest-neighbor-based data structure is employed,restricting function calculations to points near each Gaussian kernel center.The computations for each kernel are further parallelized on CUDA, whichsignificantly improves the optimization speed. Additionally, a hierarchicaloctree-based refinement strategy is designed for training. Specifically, theinitialization and optimization of network parameters are conducted usingcoarse grid points in the octree lattice structure. Subsequently, fine latticepoints are progressively incorporated to accelerate model convergence andenhance training efficiency. Extensive experiments on multiple benchmarkdatasets demonstrate that our method outperforms previous sparse representationapproaches in terms of accuracy, robustness, and computational efficiency. Thecorresponding executable program is publicly available athttps://github.com/lianbobo/SE-RBFNet.git.</description>
      <author>example@mail.com (Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen)</author>
      <guid isPermaLink="false">2505.02350v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</title>
      <link>http://arxiv.org/abs/2505.05396v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文旨在从临床理论角度研究疼痛评估过程，并探索和检验现有自动方法，在此基础上，开发创新计算方法以实现高性能的自动疼痛评估，并适用于真实临床环境。&lt;h4&gt;背景&lt;/h4&gt;论文从临床理论和现有自动疼痛评估方法出发，探讨了疼痛感知的影响因素。&lt;h4&gt;目的&lt;/h4&gt;主要目的是开发适用于不同场景的自动疼痛评估流程，并研究影响疼痛感知的显著因素。&lt;h4&gt;方法&lt;/h4&gt;通过计算方法研究影响疼痛感知的人口统计学元素，设计、开发、提出并提供了适用于单模态和多模态配置的自动疼痛评估流程。&lt;h4&gt;主要发现&lt;/h4&gt;论文中的研究展示了所提出方法的有效性，实现了最先进的结果，并为探索人工智能、基础模型和生成式人工智能的新方法铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;本文提出的自动疼痛评估方法在临床环境中具有应用潜力，并为人工智能领域的研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From the original abstract: This thesis initially aims to study the painassessment process from a clinical-theoretical perspective while exploring andexamining existing automatic approaches. Building on this foundation, theprimary objective of this Ph.D. project is to develop innovative computationalmethods for automatic pain assessment that achieve high performance and areapplicable in real clinical settings. A primary goal is to thoroughlyinvestigate and assess significant factors, including demographic elements thatimpact pain perception, as recognized in pain research, through a computationalstandpoint. Within the limits of the available data in this research area, ourgoal was to design, develop, propose, and offer automatic pain assessmentpipelines for unimodal and multimodal configurations that are applicable to thespecific requirements of different scenarios. The studies published in thisPh.D. thesis showcased the effectiveness of the proposed methods, achievingstate-of-the-art results. Additionally, they paved the way for exploring newapproaches in artificial intelligence, foundation models, and generativeartificial intelligence.</description>
      <author>example@mail.com (Stefanos Gkikas)</author>
      <guid isPermaLink="false">2505.05396v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>ALFEE: Adaptive Large Foundation Model for EEG Representation</title>
      <link>http://arxiv.org/abs/2505.06291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ALFEE的新型混合Transformer架构，用于EEG信号表示学习，以解决现有EEG模型在信号噪声比、个体差异和跨范式差异方面的问题。&lt;h4&gt;背景&lt;/h4&gt;虽然基础模型在文本、图像和视频领域表现出色，但关键生物信号，尤其是脑电图（EEG），仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;为了解决EEG模型在信号处理和泛化方面的局限性，提出ALFEE框架。&lt;h4&gt;方法&lt;/h4&gt;ALFEE采用混合注意力机制，将通道特征聚合与时间动态建模分离，具有两个学习阶段：预训练和微调。预训练阶段优化任务预测、通道和时间掩码重建以及时间预测，而微调阶段使用特定任务的标记字典和交叉注意力层。&lt;h4&gt;主要发现&lt;/h4&gt;ALFEE在六个下游EEG任务上表现出优于现有模型的效果，经过25,000小时的预训练后，在多任务上提升了性能。&lt;h4&gt;结论&lt;/h4&gt;ALFEE框架为生物信号分析提供了一个可扩展的基础，其实现在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然基础模型在文本、图像和视频领域表现出色，但关键的生物信号，尤其是脑电图（EEG），仍然没有得到充分的探索。EEG由于其高时间分辨率、操作实用性和安全性，为神经科学研究提供了益处。然而，低信号噪声比、个体差异和跨范式差异阻碍了现有模型的泛化。现有的方法通常采用简化的策略，如使用单个损失函数或通道-时间联合表示模块，并且在预训练和评估任务之间存在领域差距，这损害了效率和适应性。为了解决这些局限性，我们提出了自适应大型基础模型用于EEG信号表示（ALFEE）框架，这是一种新颖的混合Transformer架构，具有两个学习阶段，用于鲁棒的EEG表示学习。ALFEE采用混合注意力，将通道特征聚合与时间动态建模分离，使具有可变通道配置的EEG表示鲁棒。通道编码器自适应地压缩可变通道信息，时间编码器捕获任务指导的演变，混合解码器在时间和频率域中重建信号。在预训练期间，ALFEE优化任务预测、通道和时间掩码重建以及时间预测，以增强多尺度和多通道表示。在微调期间，使用特定任务的标记字典和交叉注意力层进行全模型适应性，以提升跨多个任务的表现。经过25,000小时的预训练后，在六个下游EEG任务上的大量实验结果表明，ALFEE的性能优于现有模型。我们的ALFEE框架为生物信号分析建立了一个可扩展的基础，其实现在https://github.com/xw1216/ALFEE上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models excel in text, image, and video domains, the criticalbiological signals, particularly electroencephalography(EEG), remainunderexplored. EEG benefits neurological research with its high temporalresolution, operational practicality, and safety profile. However, lowsignal-to-noise ratio, inter-subject variability, and cross-paradigmdifferences hinder the generalization of current models. Existing methods oftenemploy simplified strategies, such as a single loss function or achannel-temporal joint representation module, and suffer from a domain gapbetween pretraining and evaluation tasks that compromises efficiency andadaptability. To address these limitations, we propose the Adaptive LargeFoundation model for EEG signal representation(ALFEE) framework, a novel hybridtransformer architecture with two learning stages for robust EEG representationlearning. ALFEE employs a hybrid attention that separates channel-wise featureaggregation from temporal dynamics modeling, enabling robust EEG representationwith variable channel configurations. A channel encoder adaptively compressesvariable channel information, a temporal encoder captures task-guidedevolution, and a hybrid decoder reconstructs signals in both temporal andfrequency domains. During pretraining, ALFEE optimizes task prediction, channeland temporal mask reconstruction, and temporal forecasting to enhancemulti-scale and multi-channel representation. During fine-tuning, a full-modeladaptation with a task-specific token dictionary and a cross-attention layerboosts performance across multiple tasks. After 25,000 hours of pretraining,extensive experimental results on six downstream EEG tasks demonstrate thesuperior performance of ALFEE over existing models. Our ALFEE frameworkestablishes a scalable foundation for biological signal analysis withimplementation at https://github.com/xw1216/ALFEE.</description>
      <author>example@mail.com (Wei Xiong, Junming Lin, Jiangtong Li, Jie Li, Changjun Jiang)</author>
      <guid isPermaLink="false">2505.06291v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation</title>
      <link>http://arxiv.org/abs/2505.06288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的等距沉浸核学习方法（IIKL），用于从离散的非欧几里得数据中构建黎曼流形并等距地诱导黎曼度量，以保持数据内在的几何和拓扑属性。&lt;h4&gt;背景&lt;/h4&gt;在科学应用中，保留离散非欧几里得数据的内在几何和拓扑属性对于几何表示学习至关重要。传统方法通常将非欧几里得离散数据映射到欧几里得空间，可能导致关键几何信息的丢失。&lt;h4&gt;目的&lt;/h4&gt;提出IIKL方法，以保持离散非欧几里得数据的几何结构，并提高下游任务（如数据重建和分类）的准确性。&lt;h4&gt;方法&lt;/h4&gt;IIKL方法等距沉浸黎曼流形，并通过最大似然估计（MLE）导出参数化学习模型和交替训练方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用IIKL学习到的黎曼流形及其度量，模型在3D和高维数据集中成功地保持了数据的内在几何表示，显著提高了下游任务的准确性，如数据重建和分类。与最先进的（SOTA）方法相比，该方法可以减少超过90%的内积不变损失，平均提高了40%的下游重建准确性，并在涉及等距和共形的几何度量方面减少了90%的错误。&lt;h4&gt;结论&lt;/h4&gt;IIKL方法在保持离散非欧几里得数据的几何结构方面表现出色，对于下游任务如数据重建和分类有显著的提升效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric representation learning in preserving the intrinsic geometric andtopological properties for discrete non-Euclidean data is crucial in scientificapplications. Previous research generally mapped non-Euclidean discrete datainto Euclidean space during representation learning, which may lead to the lossof some critical geometric information. In this paper, we propose a novelIsometric Immersion Kernel Learning (IIKL) method to build Riemannian manifoldand isometrically induce Riemannian metric from discrete non-Euclidean data. Weprove that Isometric immersion is equivalent to the kernel function in thetangent bundle on the manifold, which explicitly guarantees the invariance ofthe inner product between vectors in the arbitrary tangent space throughout thelearning process, thus maintaining the geometric structure of the originaldata. Moreover, a novel parameterized learning model based on IIKL isintroduced, and an alternating training method for this model is derived usingMaximum Likelihood Estimation (MLE), ensuring efficient convergence.Experimental results proved that using the learned Riemannian manifold and itsmetric, our model preserved the intrinsic geometric representation of data inboth 3D and high-dimensional datasets successfully, and significantly improvedthe accuracy of downstream tasks, such as data reconstruction andclassification. It is showed that our method could reduce the inner productinvariant loss by more than 90% compared to state-of-the-art (SOTA) methods,also achieved an average 40% improvement in downstream reconstruction accuracyand a 90% reduction in error for geometric metrics involving isometric andconformal.</description>
      <author>example@mail.com (Zihao Chen, Wenyong Wang, Jiachen Yang, Yu Xiang)</author>
      <guid isPermaLink="false">2505.06288v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Learning Dynamics in Unsupervised Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.06279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种无监督强化学习（URL）智能体可解释性框架，旨在理解内在动机如何影响注意力、行为和表示学习。&lt;h4&gt;背景&lt;/h4&gt;分析了在程序生成环境中训练的五个智能体：DQN、RND、ICM、PPO和Transformer-RND变体。&lt;h4&gt;目的&lt;/h4&gt;理解智能体如何随时间感知和适应，并引入两个度量指标：注意力多样性和注意力变化率。&lt;h4&gt;方法&lt;/h4&gt;使用Grad-CAM、层级相关性传播（LRP）、探索指标和潜在空间聚类进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;好奇心驱动的智能体比外在动机的智能体表现出更广泛、更动态的注意力和探索行为。TransformerRND结合了广泛的注意力、高探索覆盖率和紧凑、结构化的潜在表示。&lt;h4&gt;结论&lt;/h4&gt;结果突出了架构归纳偏见和训练信号对智能体内部动态的影响。除了以奖励为中心的评估之外，该框架还提供了诊断工具，以探究强化学习智能体的感知和抽象，从而实现更具可解释性和泛化性的行为。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种针对无监督强化学习（URL）智能体的可解释性框架，旨在理解内在动机如何塑造注意力、行为和表示学习。我们分析了在程序生成环境中训练的五个智能体：DQN、RND、ICM、PPO和Transformer-RND变体。为了捕捉智能体如何随时间感知和适应，我们引入了两个指标：注意力多样性和注意力变化率。我们的研究结果表明，好奇心驱动的智能体比外在动机的智能体表现出更广泛的注意力范围和更动态的探索行为。其中，TransformerRND结合了广泛的注意力、高探索覆盖率和紧凑、结构化的潜在表示。我们的结果突出了架构归纳偏见和训练信号对智能体内部动态的影响。除了以奖励为中心的评估之外，所提出的框架还提供了诊断工具，以探究强化学习智能体的感知和抽象，从而实现更具可解释性和泛化性的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an interpretability framework for unsupervised reinforcementlearning (URL) agents, aimed at understanding how intrinsic motivation shapesattention, behavior, and representation learning. We analyze five agents DQN,RND, ICM, PPO, and a Transformer-RND variant trained on procedurally generatedenvironments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),exploration metrics, and latent space clustering. To capture how agentsperceive and adapt over time, we introduce two metrics: attention diversity,which measures the spatial breadth of focus, and attention change rate, whichquantifies temporal shifts in attention. Our findings show thatcuriosity-driven agents display broader, more dynamic attention and exploratorybehavior than their extrinsically motivated counterparts. Among them,TransformerRND combines wide attention, high exploration coverage, and compact,structured latent representations. Our results highlight the influence ofarchitectural inductive biases and training signals on internal agent dynamics.Beyond reward-centric evaluation, the proposed framework offers diagnostictools to probe perception and abstraction in RL agents, enabling moreinterpretable and generalizable behavior.</description>
      <author>example@mail.com (Shashwat Pandey)</author>
      <guid isPermaLink="false">2505.06279v1</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs</title>
      <link>http://arxiv.org/abs/2504.17040v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DyMU是一种高效的、无需训练的框架，能够在保持高任务性能的同时动态减少视觉语言模型（VLMs）的计算负担。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在处理图像和文本时存在计算效率低的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够动态减少视觉语言模型计算负担的方法，同时保持高任务性能。&lt;h4&gt;方法&lt;/h4&gt;DyMU包含两个关键组件：动态标记合并（DToMe）和虚拟标记解合并（VTU）。DToMe通过根据图像复杂度合并相似标记来减少视觉标记嵌入的数量，VTU则通过高效地重建完整序列的注意力动态来模拟大型语言模型（LLMs）的预期标记序列。&lt;h4&gt;主要发现&lt;/h4&gt;DyMU可以在减少32%-85%的平均视觉标记数量的同时，在多种VLM架构上实现与全长模型相当的性能，包括最近流行的AnyRes-based视觉编码器。通过定性分析，DToMe能够根据图像复杂度有效地调整标记减少，并且与现有系统不同，为用户提供更多控制计算成本的能力。&lt;h4&gt;结论&lt;/h4&gt;DyMU是一种适用于大多数最先进VLM架构的有效解决方案，它通过动态适应标记压缩到图像内容，并在不进行额外微调的情况下运行，从而提高了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种高效的、无需训练的框架DyMU，该框架在保持高任务性能的同时动态减少了视觉语言模型（VLMs）的计算负担。我们的方法包含两个关键组件。首先，动态标记合并（DToMe）通过根据图像复杂度合并相似标记来减少视觉标记嵌入的数量，解决了视觉转换器固定长度输出的固有低效问题。其次，虚拟标记解合并（VTU）通过高效地重建完整序列的注意力动态来模拟大型语言模型（LLMs）的预期标记序列，从而在不进行额外微调的情况下保持下游性能。与先前的方法不同，我们的方法动态地将标记压缩适应到图像内容，并且完全无需训练，使其适用于大多数最先进的VLM架构。在图像和视频理解任务上的大量实验表明，DyMU可以将平均视觉标记数量减少32%-85%，同时在多种VLM架构上实现与全长模型相当的性能，包括最近流行的AnyRes-based视觉编码器。此外，通过定性分析，我们证明了DToMe能够根据图像复杂度有效地调整标记减少，并且与现有系统不同，为用户提供更多控制计算成本的能力。项目页面：https://mikewangwzhl.github.io/dymu/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DyMU, an efficient, training-free framework that dynamicallyreduces the computational burden of vision-language models (VLMs) whilemaintaining high task performance. Our approach comprises two key components.First, Dynamic Token Merging (DToMe) reduces the number of visual tokenembeddings by merging similar tokens based on image complexity, addressing theinherent inefficiency of fixed-length outputs in vision transformers. Second,Virtual Token Unmerging (VTU) simulates the expected token sequence for largelanguage models (LLMs) by efficiently reconstructing the attention dynamics ofa full sequence, thus preserving the downstream performance without additionalfine-tuning. Unlike previous approaches, our method dynamically adapts tokencompression to the content of the image and operates completely training-free,making it readily applicable to most state-of-the-art VLM architectures.Extensive experiments on image and video understanding tasks demonstrate thatDyMU can reduce the average visual token count by 32%-85% while achievingcomparable performance to full-length models across diverse VLM architectures,including the recently popularized AnyRes-based visual encoders. Furthermore,through qualitative analyses, we demonstrate that DToMe effectively adaptstoken reduction based on image complexity and, unlike existing systems,provides users more control over computational costs. Project page:https://mikewangwzhl.github.io/dymu/.</description>
      <author>example@mail.com (Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu)</author>
      <guid isPermaLink="false">2504.17040v2</guid>
      <pubDate>Tue, 13 May 2025 14:27:28 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction</title>
      <link>http://arxiv.org/abs/2505.04918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  International Joint Conferences on Artificial Intelligence (IJCAI  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PASSAT是一种新的深度学习模型，用于天气预测，它结合了物理和地球表面拓扑信息，以改进预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的深度学习模型在天气预测中存在忽视物理和地球表面拓扑的问题。&lt;h4&gt;目的&lt;/h4&gt;开发PASSAT模型，以解决现有模型的不足，提高天气预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;PASSAT模型将天气演变归因于两个关键因素：平流过程和地球-大气相互作用。它考虑地球表面的拓扑结构，并在球面上数值求解平流方程和纳维-斯托克斯方程。使用球面图神经网络来捕捉地球-大气相互作用，并生成初始速度场。&lt;h4&gt;主要发现&lt;/h4&gt;在5.625°分辨率的ERA5数据集中，PASSAT优于现有的深度学习模型和IFS T42数值天气预报模型。&lt;h4&gt;结论&lt;/h4&gt;PASSAT模型通过结合物理和拓扑信息，在天气预测方面取得了显著的改进。&lt;h4&gt;翻译&lt;/h4&gt;尽管深度学习模型在天气预测中显示出巨大的潜力，但大多数模型忽略了底层天气演变的物理或地球表面的拓扑。鉴于这些缺点，我们开发了PASSAT，一种新的物理辅助和拓扑信息深度学习模型，用于天气预测。PASSAT将天气演变归因于两个关键因素：（i）可以由平流方程和纳维-斯托克斯方程表征的平流过程；（ii）难以建模和计算的地球-大气相互作用。PASSAT还考虑了地球表面的拓扑结构，而不仅仅是将其视为平面。在这些考虑的基础上，PASSAT在球面上数值求解平流方程和纳维-斯托克斯方程，利用球面图神经网络来捕捉地球-大气相互作用，并从同一球面图神经网络生成解决平流方程的关键初始速度场。在5.625°分辨率的ERA5数据集中，PASSAT优于基于深度学习的最先进的天气预测模型和IFS T42业务数值天气预报模型。代码和检查点可在https://github.com/Yumenomae/PASSAT_5p625上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yumenomae/passat_5p625&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although deep learning models have demonstrated remarkable potential inweather prediction, most of them overlook either the \textbf{physics} of theunderlying weather evolution or the \textbf{topology} of the Earth's surface.In light of these disadvantages, we develop PASSAT, a novel Physics-ASSistedAnd Topology-informed deep learning model for weather prediction. PASSATattributes the weather evolution to two key factors: (i) the advection processthat can be characterized by the advection equation and the Navier-Stokesequation; (ii) the Earth-atmosphere interaction that is difficult to both modeland calculate. PASSAT also takes the topology of the Earth's surface intoconsideration, other than simply treating it as a plane. With theseconsiderations, PASSAT numerically solves the advection equation and theNavier-Stokes equation on the spherical manifold, utilizes a spherical graphneural network to capture the Earth-atmosphere interaction, and generates theinitial velocity fields that are critical to solving the advection equationfrom the same spherical graph neural network. In the $5.625^\circ$-resolutionERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-basedweather prediction models and the operational numerical weather predictionmodel IFS T42. Code and checkpoint are available athttps://github.com/Yumenomae/PASSAT_5p625.</description>
      <author>example@mail.com (Jiaqi Zheng, Qing Ling, Yerong Feng)</author>
      <guid isPermaLink="false">2505.04918v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
  <item>
      <title>Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</title>
      <link>http://arxiv.org/abs/2505.06224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了模型表示评估的重要性，提出了超越下游探针的方法，并引入了一种标准化协议来量化模型表示中可变因素的信息量、等变性、不变性和解耦性。&lt;h4&gt;背景&lt;/h4&gt;下游探针是评估模型表示的主要方法，但在评估过程中忽略了等变性、不变性和解耦性等属性，这些属性对表示的可解释性、适应性和实用性至关重要。&lt;h4&gt;目的&lt;/h4&gt;强调模型表示评估的重要性，并提出一种统一的评估框架，以量化模型表示中可变因素的信息量、等变性、不变性和解耦性。&lt;h4&gt;方法&lt;/h4&gt;引入了一种标准化协议，用于评估图像和语音领域不同模型、不同架构和预训练方法下的表示，并识别可控的可变因素。&lt;h4&gt;主要发现&lt;/h4&gt;发现具有相似下游性能的模型在信息量、等变性、不变性和解耦性等方面可能表现出显著差异，暗示其下游性能背后的机制功能不同。&lt;h4&gt;结论&lt;/h4&gt;提出新的研究方向，以理解和改进模型表示，并强调在评估模型表示时需要考虑更广泛的属性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Downstream probing has been the dominant method for evaluating modelrepresentations, an important process given the increasing prominence ofself-supervised learning and foundation models. However, downstream probingprimarily assesses the availability of task-relevant information in the model'slatent space, overlooking attributes such as equivariance, invariance, anddisentanglement, which contribute to the interpretability, adaptability, andutility of representations in real-world applications. While some attempts havebeen made to measure these qualities in representations, no unified evaluationframework with modular, generalizable, and interpretable metrics exists.  In this paper, we argue for the importance of representation evaluationbeyond downstream probing. We introduce a standardized protocol to quantifyinformativeness, equivariance, invariance, and disentanglement of factors ofvariation in model representations. We use it to evaluate representations froma variety of models in the image and speech domains using differentarchitectures and pretraining approaches on identified controllable factors ofvariation. We find that representations from models with similar downstreamperformance can behave substantially differently with regard to theseattributes. This hints that the respective mechanisms underlying theirdownstream performance are functionally different, prompting new researchdirections to understand and improve representations.</description>
      <author>example@mail.com (Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels)</author>
      <guid isPermaLink="false">2505.06224v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review</title>
      <link>http://arxiv.org/abs/2505.06118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了深度学习在淋巴结分割中的应用，并讨论了卷积神经网络、编码器-解码器网络和Transformer等不同深度学习架构在分析不同模态医学影像数据的方法。&lt;h4&gt;背景&lt;/h4&gt;自动淋巴结分割是计算机视觉任务中癌症早期检测和分期的基础。传统的分割方法受限于人工描绘和操作者技能的差异性，限制了其达到高准确性的能力。&lt;h4&gt;目的&lt;/h4&gt;评估深度学习在淋巴结分割中的应用，并探讨解决淋巴结形状多样性、标注数据集稀缺以及跨不同成像模态的鲁棒和泛化方法不足等挑战。&lt;h4&gt;方法&lt;/h4&gt;本文提供了对深度学习技术在淋巴结分割任务中应用的全面概述，并探索了包括多模态融合技术、迁移学习和使用大规模预训练模型等潜在的未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;尽管深度学习技术在淋巴结分割方面取得了进展，但仍然面临淋巴结形状多样性、标注数据集稀缺以及跨不同成像模态的鲁棒和泛化方法不足等挑战。&lt;h4&gt;结论&lt;/h4&gt;这是首次对深度学习技术在淋巴结分割任务中的应用进行综合概述的研究，并为未来的研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;Automatic lymph node segmentation is the cornerstone for advances in computer vision tasks for early detection and staging of cancer. Traditional segmentation methods are constrained by manual delineation and variability in operator proficiency, limiting their ability to achieve high accuracy. The introduction of deep learning technologies offers new possibilities for improving the accuracy of lymph node image analysis. This study evaluates the application of deep learning in lymph node segmentation and discusses the methodologies of various deep learning architectures such as convolutional neural networks, encoder-decoder networks, and transformers in analyzing medical imaging data across different modalities. Despite the advancements, it still confronts challenges like the shape diversity of lymph nodes, the scarcity of accurately labeled datasets, and the inadequate development of methods that are robust and generalizable across different imaging modalities. To the best of our knowledge, this is the first study that provides a comprehensive overview of the application of deep learning techniques in lymph node segmentation task. Furthermore, this study also explores potential future research directions, including multimodal fusion techniques, transfer learning, and the use of large-scale pre-trained models to overcome current limitations while enhancing cancer diagnosis and treatment planning strategies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic lymph node segmentation is the cornerstone for advances in computervision tasks for early detection and staging of cancer. Traditionalsegmentation methods are constrained by manual delineation and variability inoperator proficiency, limiting their ability to achieve high accuracy. Theintroduction of deep learning technologies offers new possibilities forimproving the accuracy of lymph node image analysis. This study evaluates theapplication of deep learning in lymph node segmentation and discusses themethodologies of various deep learning architectures such as convolutionalneural networks, encoder-decoder networks, and transformers in analyzingmedical imaging data across different modalities. Despite the advancements, itstill confronts challenges like the shape diversity of lymph nodes, thescarcity of accurately labeled datasets, and the inadequate development ofmethods that are robust and generalizable across different imaging modalities.To the best of our knowledge, this is the first study that provides acomprehensive overview of the application of deep learning techniques in lymphnode segmentation task. Furthermore, this study also explores potential futureresearch directions, including multimodal fusion techniques, transfer learning,and the use of large-scale pre-trained models to overcome current limitationswhile enhancing cancer diagnosis and treatment planning strategies.</description>
      <author>example@mail.com (Jingguo Qu, Xinyang Han, Man-Lik Chui, Yao Pu, Simon Takadiyi Gunda, Ziman Chen, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying)</author>
      <guid isPermaLink="false">2505.06118v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization</title>
      <link>http://arxiv.org/abs/2505.05851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted for presentation at the 7th Workshop on Long-term Human  Motion Prediction (LHMP) at International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在拥挤和遮挡频繁的环境中，应用超宽带（UWB）定位技术作为人类运动捕捉的可扩展替代方案的可能性。&lt;h4&gt;背景&lt;/h4&gt;随着机器人越来越多地融入人类环境，理解和预测人类运动对于安全高效的人机交互至关重要。目前，人类运动和活动预测方法需要高质量和数量的数据进行训练和评估，通常从运动捕捉系统、车载或固定传感器中收集。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种在更大和更复杂环境中（如仓库、机场或会议中心）评估传感方式（如UWB）的可扩展和准确的运动数据收集方法。&lt;h4&gt;方法&lt;/h4&gt;包括额外的传感方式，如眼动追踪、车载机器人激光雷达和雷达传感器，并记录运动捕捉数据作为评估和比较的基准。环境模拟博物馆设置，最多有四个参与者以自然的方式向随机目标移动，提供超过130分钟的多模态数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究为超越基于视觉的系统提供了可扩展和准确的运动数据收集的步骤。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为在更大和更复杂的环境中评估传感方式（如UWB）提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;With robots increasingly integrating into human environments, understanding and predicting human motion is essential for safe and efficient interactions. Modern human motion and activity prediction approaches require high quality and quantity of data for training and evaluation, usually collected from motion capture systems, onboard or stationary sensors. Setting up these systems is challenging due to the intricate setup of hardware components, extensive calibration procedures, occlusions, and substantial costs. These constraints make deploying such systems in new and large environments difficult and limit their usability for in-the-wild measurements. In this paper we investigate the possibility to apply the novel Ultra-Wideband (UWB) localization technology as a scalable alternative for human motion capture in crowded and occlusion-prone environments. We include additional sensing modalities such as eye-tracking, onboard robot LiDAR and radar sensors, and record motion capture data as ground truth for evaluation and comparison. The environment imitates a museum setup, with up to four active participants navigating toward random goals in a natural way, and offers more than 130 minutes of multi-modal data. Our investigation provides a step toward scalable and accurate motion data collection beyond vision-based systems, laying a foundation for evaluating sensing modalities like UWB in larger and complex environments like warehouses, airports, or convention centers.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With robots increasingly integrating into human environments, understandingand predicting human motion is essential for safe and efficient interactions.Modern human motion and activity prediction approaches require high quality andquantity of data for training and evaluation, usually collected from motioncapture systems, onboard or stationary sensors. Setting up these systems ischallenging due to the intricate setup of hardware components, extensivecalibration procedures, occlusions, and substantial costs. These constraintsmake deploying such systems in new and large environments difficult and limittheir usability for in-the-wild measurements. In this paper we investigate thepossibility to apply the novel Ultra-Wideband (UWB) localization technology asa scalable alternative for human motion capture in crowded and occlusion-proneenvironments. We include additional sensing modalities such as eye-tracking,onboard robot LiDAR and radar sensors, and record motion capture data as groundtruth for evaluation and comparison. The environment imitates a museum setup,with up to four active participants navigating toward random goals in a naturalway, and offers more than 130 minutes of multi-modal data. Our investigationprovides a step toward scalable and accurate motion data collection beyondvision-based systems, laying a foundation for evaluating sensing modalitieslike UWB in larger and complex environments like warehouses, airports, orconvention centers.</description>
      <author>example@mail.com (Janik Kaden, Maximilian Hilger, Tim Schreiter, Marius Schaab, Thomas Graichen, Andrey Rudenko, Ulrich Heinkel, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2505.05851v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2505.06113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用摄像头的感知框架，通过扩展Lift-Splat-Shoot架构生成鸟瞰图（BEV）地图，实现低成本且精确的环境感知。&lt;h4&gt;背景&lt;/h4&gt;传统的自动驾驶感知系统依赖昂贵的LiDAR传感器来生成精确的环境表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要LiDAR传感器的低成本感知框架，同时保持高精度。&lt;h4&gt;方法&lt;/h4&gt;结合YOLOv1和DepthAnythingV2技术，通过多摄像头输入实现物体检测和单目深度估计，以实现全面的360度场景理解。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenLane-V2和NuScenes数据集上评估，该方法在道路分割精度达到85%，车辆检测率在85-90%，平均位置误差限制在1.2米。&lt;h4&gt;结论&lt;/h4&gt;深度学习有潜力仅使用摄像头输入提取丰富的空间信息，从而实现低成本且不牺牲精度的自主导航。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自动驾驶车辆的感知系统传统上依赖昂贵的LiDAR传感器来生成精确的环境表示。在本文中，我们提出了一种仅使用摄像头的感知框架，通过扩展Lift-Splat-Shoot架构生成鸟瞰图（BEV）地图。我们的方法结合了基于YOLOv1的对象检测和DepthAnythingV2的单目深度估计，通过多摄像头输入实现全面的360度场景理解。我们在OpenLane-V2和NuScenes数据集上评估了我们的方法，与LiDAR地面真实值相比，实现了高达85%的道路分割精度和85-90%的车辆检测率，平均位置误差限制在1.2米。这些结果突出了深度学习仅使用摄像头输入提取丰富空间信息的潜力，使得低成本且不牺牲精度的自主导航成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicle perception systems have traditionally relied on costlyLiDAR sensors to generate precise environmental representations. In this paper,we propose a camera-only perception framework that produces Bird's Eye View(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combinesYOLOv11-based object detection with DepthAnythingV2 monocular depth estimationacross multi-camera inputs to achieve comprehensive 360-degree sceneunderstanding. We evaluate our approach on the OpenLane-V2 and NuScenesdatasets, achieving up to 85% road segmentation accuracy and 85-90% vehicledetection rates when compared against LiDAR ground truth, with averagepositional errors limited to 1.2 meters. These results highlight the potentialof deep learning to extract rich spatial information using only camera inputs,enabling cost-efficient autonomous navigation without sacrificing accuracy.</description>
      <author>example@mail.com (Anupkumar Bochare)</author>
      <guid isPermaLink="false">2505.06113v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective</title>
      <link>http://arxiv.org/abs/2505.05785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LRW-OOD的图Out-Of-Distribution（OOD）泛化学习方法，旨在解决图神经网络在分布偏移下的性能退化问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在分布偏移的情况下表现不佳，现有的图OOD方法通常基于不变风险最小化和结构因果模型，但它们可能不符合现实情况。&lt;h4&gt;目的&lt;/h4&gt;提出可学习的随机游走（LRW）作为不变知识的实例，以实现图OOD泛化学习。&lt;h4&gt;方法&lt;/h4&gt;LRW-OOD使用可学习的随机游走采样器和路径编码器来参数化转移矩阵，并提出基于核密度估计（KDE）的互信息（MI）损失来生成符合OOD原则的随机游走序列。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该模型可以在各种类型的分布偏移下有效地增强图OOD泛化，并且相对于最先进的图OOD泛化基线，准确率提高了3.87%。&lt;h4&gt;结论&lt;/h4&gt;LRW-OOD是一种有效的图OOD泛化学习方法，能够显著提高模型在分布偏移情况下的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-Of-Distribution (OOD) generalization has gained increasing attentions formachine learning on graphs, as graph neural networks (GNNs) often exhibitperformance degradation under distribution shifts. Existing graph OOD methodstend to follow the basic ideas of invariant risk minimization and structuralcausal models, interpreting the invariant knowledge across datasets undervarious distribution shifts as graph topology or graph spectrum. However, theseinterpretations may be inconsistent with real-world scenarios, as neitherinvariant topology nor spectrum is assured. In this paper, we advocate thelearnable random walk (LRW) perspective as the instantiation of invariantknowledge, and propose LRW-OOD to realize graph OOD generalization learning.Instead of employing fixed probability transition matrix (i.e.,degree-normalized adjacency matrix), we parameterize the transition matrix withan LRW-sampler and a path encoder. Furthermore, we propose the kernel densityestimation (KDE)-based mutual information (MI) loss to generate random walksequences that adhere to OOD principles. Extensive experiment demonstrates thatour model can effectively enhance graph OOD generalization under various typesof distribution shifts and yield a significant accuracy improvement of 3.87%over state-of-the-art graph OOD generalization baselines.</description>
      <author>example@mail.com (Henan Sun, Xunkai Li, Lei Zhu, Junyi Han, Guang Zeng, Ronghua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2505.05785v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Discovery of the Polar Ring Galaxies with deep learning</title>
      <link>http://arxiv.org/abs/2505.05890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 11 figures. The article is submitted to Astron. &amp; Astrophys&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究旨在创建一个强和良好候选行星状星云（PRGs）的目录，开发一种基于机器学习方法的图像搜索技术，用于在大规模巡天中搜索和发现PRGs，并探索CIGALE软件在确定其多波段特性的能力。&lt;h4&gt;背景&lt;/h4&gt;本研究首次将深度学习方法应用于PRGs的搜索。&lt;h4&gt;目的&lt;/h4&gt;创建PRGs目录，开发基于机器学习的图像搜索方法，探索CIGALE软件在确定PRGs多波段特性的能力。&lt;h4&gt;方法&lt;/h4&gt;使用深度学习方法搜索PRGs，对现有PRGs目录中的星系进行视觉检查以创建训练样本，采用数据增强、图像分割和集成学习方法。使用转移学习技术通过GALFIT生成的合成图像扩大训练样本。&lt;h4&gt;主要发现&lt;/h4&gt;发现了三个新的PRGs，并在SDSS环状星系目录中发现了四个PRGs。使用CIGALE软件对其中一个发现的新PRG进行了研究，确定了其红外到紫外波段的谱能量分布，并估计了其恒星形成率（SFR）和总恒星质量。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法在发现新的PRGs方面是有效的，且CIGALE软件能够确定PRGs的多波段特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of our research is to create a catalog of strong and good candidatesfor PRGs using existing catalogs of PRGs, develop an image-based approach withmachine learning methods for the search and discovery of PRGs in a big skysurvey, and explore the capability of the CIGALE software for determining theirmultiwavelength properties. For the first time, we applied a deep learningmethod to the search for PRGs. We visually inspected galaxies from existingcatalogs of PRGs to create a training sample based on high-quality SDSS images.Since the resulting training sample was extremely small (87 strong and goodPRGs), we applied augmentation, image segmentation, and ensemble learningtechniques. However, most effective method was transfer learning with itsability to enlarge the training sample by synthetic images generated by GALFIT.To examine deep learning approach for finding new PRGs we used the SDSS catalogof galaxies at z &lt; 0.1. The method with synthetic images showed that even withovertraining we were able to find galaxies with a ring pattern. Our deeplearning approach has resulted in the discovery of three PRGs (SDSSJ140644.42+471602.0; SDSS J133650.48+492745.3; SDSS J095717.30+364953.5). Also,we visually inspected the Catalog of the SDSS Ring galaxies at z &lt; 0.1 anddiscovered four PRGs among ~2,200 ring galaxies (SDSS J095851.32+320422.9; SDSSJ104211.05+234448.2; SDSS J162212.63+272032.2; SDSS J104600.10+090627.2). Oneof the discovered galaxies with transfer learning, SDSS J140644.42+471602.0,was studied with CIGALE software to determine its spectral energy distributionin IR-UV bands. The current SFR is 71 M_sun per year, although the lack of FUVdata limits this estimate. The total stellar mass is 8.34x10^{10} M_sun. Thepredominance of an old stellar population (two-thirds of the total mass)suggests that this PRG is undergoing interaction process.</description>
      <author>example@mail.com (D. V. Dobrycheva, O. O. Hetmantsev, I. B. Vavilova, A. Shportko, O. Gugnin, O. V. Kompaniiets)</author>
      <guid isPermaLink="false">2505.05890v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</title>
      <link>http://arxiv.org/abs/2505.05845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于机器学习的木材节子检测和配对自动化流程，提高了木材加工中的效率。&lt;h4&gt;背景&lt;/h4&gt;木材节子对木材的美观和结构完整性至关重要，传统手工标注节子费时费力。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级且全自动的节子检测和配对方法。&lt;h4&gt;方法&lt;/h4&gt;利用工业级相机采集木质板材的高分辨率表面图像，并手动标注和预处理大规模数据集。使用YOLOv8进行节子检测，采用三重神经网络进行特征映射和配对。&lt;h4&gt;主要发现&lt;/h4&gt;YOLOv8在检测阶段达到mAP@0.5的0.887，三重神经网络在配对阶段达到0.85的配对准确率。分析表明，节子起点和终点到底部的距离以及纵向坐标在提高配对准确性中起关键作用。&lt;h4&gt;结论&lt;/h4&gt;该方法有效验证了人工智能在推进木材科学和工业中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knots in wood are critical to both aesthetics and structural integrity,making their detection and pairing essential in timber processing. However,traditional manual annotation was labor-intensive and inefficient,necessitating automation. This paper proposes a lightweight and fully automatedpipeline for knot detection and pairing based on machine learning techniques.In the detection stage, high-resolution surface images of wooden boards werecollected using industrial-grade cameras, and a large-scale dataset wasmanually annotated and preprocessed. After the transfer learning, the YOLOv8lachieves an mAP@0.5 of 0.887. In the pairing stage, detected knots wereanalyzed and paired based on multidimensional feature extraction. A tripletneural network was used to map the features into a latent space, enablingclustering algorithms to identify and pair corresponding knots. The tripletnetwork with learnable weights achieved a pairing accuracy of 0.85. Furtheranalysis revealed that he distances from the knot's start and end points to thebottom of the wooden board, and the longitudinal coordinates play crucial rolesin achieving high pairing accuracy. Our experiments validate the effectivenessof the proposed solution, demonstrating the potential of AI in advancing woodscience and industry.</description>
      <author>example@mail.com (Guohao Lin, Shidong Pan, Rasul Khanbayov, Changxi Yang, Ani Khaloian-Sarnaghi, Andriy Kovryga)</author>
      <guid isPermaLink="false">2505.05845v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling</title>
      <link>http://arxiv.org/abs/2505.06184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at MisD @ AAAI ICWSM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型（LLM）的社交媒体用户画像方法，旨在解决现有画像技术的局限性，如可迁移性差、特征不可解释、需要大量标注数据集或依赖刚性预定义类别等。&lt;h4&gt;背景&lt;/h4&gt;社交媒体用户画像对于虚假信息检测、参与度预测、仇恨言论监控和用户行为建模等任务至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应不同领域、减少对大量标注数据集依赖、生成可解释的用户画像的新方法。&lt;h4&gt;方法&lt;/h4&gt;该方法采用两阶段方法：首先使用特定领域的知识库进行半监督过滤，然后生成抽象（合成描述）和提取（代表性推文选择）的用户画像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在创建灵活、适应性强和可解释的用户画像方面显著优于现有方法，提高了9.8%。&lt;h4&gt;结论&lt;/h4&gt;该方法通过利用LLM的内在知识，在最小化人工验证的同时，实现了跨领域的适应性，并有效利用LLM的推理和知识能力，为下游社交网络任务提供支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：通过内容分析进行社交媒体用户画像对于诸如虚假信息检测、参与度预测、仇恨言论监控和用户行为建模等任务至关重要。然而，现有的画像技术，包括推文摘要、基于属性的画像和潜在表示学习，面临着重大局限性：它们通常缺乏可迁移性，产生不可解释的特征，需要大量标注数据集，或者依赖于限制适应性的刚性预定义类别。我们提出了一种基于大型语言模型（LLM）的新方法，该方法利用领域定义语句，这些语句作为领域关键特征的轮廓，构成了领域画像的基础。我们的两阶段方法首先使用特定领域的知识库进行半监督过滤，然后生成抽象（合成描述）和提取（代表性推文选择）的用户画像。通过利用LLM的内在知识以及最小的人工验证，我们的方法在减少对大量标注数据集需求的同时，实现了跨领域的适应性。我们的方法生成可解释的自然语言用户画像，将大量的用户数据压缩到可以解锁LLM推理和知识能力的规模，从而为下游社交网络任务提供支持。我们贡献了一个波斯政治Twitter（X）数据集和一个基于LLM的评估框架，并进行了人工验证。实验结果表明，我们的方法在创建灵活、适应性强和可解释的用户画像方面显著优于最先进的LLM和传统方法，提高了9.8%，证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social media user profiling through content analysis is crucial for taskslike misinformation detection, engagement prediction, hate speech monitoring,and user behavior modeling. However, existing profiling techniques, includingtweet summarization, attribute-based profiling, and latent representationlearning, face significant limitations: they often lack transferability,produce non-interpretable features, require large labeled datasets, or rely onrigid predefined categories that limit adaptability. We introduce a novel largelanguage model (LLM)-based approach that leverages domain-defining statements,which serve as key characteristics outlining the important pillars of a domainas foundations for profiling. Our two-stage method first employssemi-supervised filtering with a domain-specific knowledge base, then generatesboth abstractive (synthesized descriptions) and extractive (representativetweet selections) user profiles. By harnessing LLMs' inherent knowledge withminimal human validation, our approach is adaptable across domains whilereducing the need for large labeled datasets. Our method generatesinterpretable natural language user profiles, condensing extensive user datainto a scale that unlocks LLMs' reasoning and knowledge capabilities fordownstream social network tasks. We contribute a Persian political Twitter (X)dataset and an LLM-based evaluation framework with human validation.Experimental results show our method significantly outperforms state-of-the-artLLM-based and traditional methods by 9.8%, demonstrating its effectiveness increating flexible, adaptable, and interpretable user profiles.</description>
      <author>example@mail.com (Vahid Rahimzadeh, Ali Hamzehpour, Azadeh Shakery, Masoud Asadpour)</author>
      <guid isPermaLink="false">2505.06184v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</title>
      <link>http://arxiv.org/abs/2505.05752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 15 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于点云数据的几何测量和合规性评估自动化框架，通过深度学习和信号处理技术自动化基础设施测绘任务。&lt;h4&gt;背景&lt;/h4&gt;自动化在提高基础设施测绘的效率、准确性和可扩展性方面可以发挥重要作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，用于利用点云数据自动评估道路边缘坡道是否符合美国残疾人法案（ADA）的要求。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了基于深度学习的检测和分割技术，以及几何和信号处理技术，并使用了一个新收集的大规模标注数据集进行模型训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在自动化测绘任务中具有较高的准确性和可靠性，可以显著减少手动工作并提高基础设施评估的一致性。&lt;h4&gt;结论&lt;/h4&gt;该框架为基础设施测绘和自动施工评估提供了基础，并促进了点云数据在这些领域的更广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自动化在提高基础设施测绘的效率、准确性及可扩展性方面扮演着重要角色。本文提出了一种基于点云数据的几何测量和合规性评估自动化框架。该框架集成了深度学习检测和分割技术与几何和信号处理技术，以自动化测绘任务。作为一种概念验证，我们应用该框架自动评估道路边缘坡道是否符合美国残疾人法案（ADA），展示了点云数据在测绘自动化中的实用性。该方法利用了一个新收集的大型标注数据集，作为本工作的一个部分公开，以促进稳健的模型训练和评估。包括与多个坡道的手动现场测量的比较在内的实验结果，验证了所提方法的准确性和可靠性，突出了其在显著减少手动工作并提高基础设施评估一致性方面的潜力。除了ADA合规性之外，该框架为基础设施测绘和自动施工评估的更广泛应用奠定了基础，并促进了点云数据在这些领域的更广泛应用。标注数据库、手动坡道调查数据和开发算法在项目的GitHub页面上公开：https://github.com/Soltanilara/SurveyAutomation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/soltanilara/surveyautomation&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automation can play a prominent role in improving efficiency, accuracy, andscalability in infrastructure surveying and assessing construction andcompliance standards. This paper presents a framework for automation ofgeometric measurements and compliance assessment using point cloud data. Theproposed approach integrates deep learning-based detection and segmentation, inconjunction with geometric and signal processing techniques, to automatesurveying tasks. As a proof of concept, we apply this framework toautomatically evaluate the compliance of curb ramps with the Americans withDisabilities Act (ADA), demonstrating the utility of point cloud data in surveyautomation. The method leverages a newly collected, large annotated dataset ofcurb ramps, made publicly available as part of this work, to facilitate robustmodel training and evaluation. Experimental results, including comparison withmanual field measurements of several ramps, validate the accuracy andreliability of the proposed method, highlighting its potential to significantlyreduce manual effort and improve consistency in infrastructure assessment.Beyond ADA compliance, the proposed framework lays the groundwork for broaderapplications in infrastructure surveying and automated construction evaluation,promoting wider adoption of point cloud data in these domains. The annotateddatabase, manual ramp survey data, and developed algorithms are publiclyavailable on the project's GitHub page:https://github.com/Soltanilara/SurveyAutomation.</description>
      <author>example@mail.com (Amin Ghafourian, Andrew Lee, Dechen Gao, Tyler Beer, Kin Yen, Iman Soltani)</author>
      <guid isPermaLink="false">2505.05752v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Adapting a Segmentation Foundation Model for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2505.06217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新框架，用于将Segment Anything Model (SAM) 应用于医学图像分类。&lt;h4&gt;背景&lt;/h4&gt;SAM在图像分割任务中表现出色，但其适应医学图像分类的研究还较少。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，以适应SAM进行医学图像分类。&lt;h4&gt;方法&lt;/h4&gt;使用SAM的图像编码器作为特征提取器，捕获图像的分割特征，并提出了一种新的空间局部化通道注意力（SLCA）机制来计算特征图的空间局部化注意力权重，从而增强分类模型对图像中空间相关或有意义区域的关注。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开的医学图像分类数据集上的实验结果表明，该方法有效且数据效率高。&lt;h4&gt;结论&lt;/h4&gt;该方法在医学图像分类中取得了良好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models, such as the Segment Anything Model(SAM), have shown strong performance in various vision tasks, particularlyimage segmentation, due to their impressive zero-shot segmentationcapabilities. However, effectively adapting such models for medical imageclassification is still a less explored topic. In this paper, we introduce anew framework to adapt SAM for medical image classification. First, we utilizethe SAM image encoder as a feature extractor to capture segmentation-basedfeatures that convey important spatial and contextual details of the image,while freezing its weights to avoid unnecessary overhead during training. Next,we propose a novel Spatially Localized Channel Attention (SLCA) mechanism tocompute spatially localized attention weights for the feature maps. Thefeatures extracted from SAM's image encoder are processed through SLCA tocompute attention weights, which are then integrated into deep learningclassification models to enhance their focus on spatially relevant ormeaningful regions of the image, thus improving classification performance.Experimental results on three public medical image classification datasetsdemonstrate the effectiveness and data-efficiency of our approach.</description>
      <author>example@mail.com (Pengfei Gu, Haoteng Tang, Islam A. Ebeid, Jose A. Nunez, Fabian Vazquez, Diego Adame, Marcus Zhan, Huimin Li, Bin Fu, Danny Z. Chen)</author>
      <guid isPermaLink="false">2505.06217v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder</title>
      <link>http://arxiv.org/abs/2505.05710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了基于Transformer的HyperspectralMAE模型，用于高光谱数据，通过双掩码策略和波长感知嵌入，提高了高光谱图像的重建和分析能力。&lt;h4&gt;背景&lt;/h4&gt;高光谱图像在空间和光谱维度上具有高维度，这给数据处理带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理高光谱数据的模型，并提高其重建和分析性能。&lt;h4&gt;方法&lt;/h4&gt;HyperspectralMAE模型采用双掩码策略，在预训练过程中随机遮挡50%的空间块和50%的光谱带，并引入基于波长的可学习谐波傅里叶位置嵌入来编码光谱顺序。重建目标结合均方误差（MSE）和光谱角映射（SAM）来平衡像素级精度和光谱形状保真度。&lt;h4&gt;主要发现&lt;/h4&gt;HyperspectralMAE在Indian Pines数据集上实现了最先进的迁移学习准确率，证明了掩码双维度预训练能够产生鲁棒的光谱-空间表示。&lt;h4&gt;结论&lt;/h4&gt;双掩码和波长感知嵌入技术能够提升高光谱图像的重建和下游分析能力。&lt;h4&gt;翻译&lt;/h4&gt;Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose HyperspectralMAE, a Transformer-based foundation model for hyperspectral data that employs a dual masking strategy: during pre-training we randomly occlude 50% of spatial patches and 50% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity. The resulting model contains about 1.8×10^8 parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion (~1,600 scenes, ~3×10^11 pixel spectra) and DLR EnMAP Level-0 (~1,300 scenes, ~3×10^11 pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imagery provides rich spectral detail but poses uniquechallenges because of its high dimensionality in both spatial and spectraldomains. We propose \textit{HyperspectralMAE}, a Transformer-based foundationmodel for hyperspectral data that employs a \textit{dual masking} strategy:during pre-training we randomly occlude 50\% of spatial patches and 50\% ofspectral bands. This forces the model to learn representations capable ofreconstructing missing information across both dimensions. To encode spectralorder, we introduce learnable harmonic Fourier positional embeddings based onwavelength. The reconstruction objective combines mean-squared error (MSE) withthe spectral angle mapper (SAM) to balance pixel-level accuracy andspectral-shape fidelity.  The resulting model contains about $1.8\times10^{8}$ parameters and produces768-dimensional embeddings, giving it sufficient capacity for transferlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra)and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixelspectra) -- and fine-tuned it for land-cover classification on the Indian Pinesbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learningaccuracy on Indian Pines, confirming that masked dual-dimensional pre-trainingyields robust spectral-spatial representations. These results demonstrate thatdual masking and wavelength-aware embeddings advance hyperspectral imagereconstruction and downstream analysis.</description>
      <author>example@mail.com (Wooyoung Jeong, Hyun Jae Park, Seonghun Jeong, Jong Wook Jang, Tae Hoon Lim, Dae Seoung Kim)</author>
      <guid isPermaLink="false">2505.05710v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Steepest Descent Density Control for Compact 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2505.05587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025, Project page: https://vita-group.github.io/SteepGS/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;3D Gaussian Splatting (3DGS) 是一种实时、高分辨率的新视角合成技术。为了优化场景覆盖和捕捉细节，3DGS 使用密化算法生成额外的点，但往往导致冗余点云，增加了内存使用、性能下降和存储需求。本文提出了一种理论框架，通过优化密化控制来解决这个问题。&lt;h4&gt;背景&lt;/h4&gt;3DGS 通过将场景表示为高斯原语混合体，利用 GPU 光栅化管道进行高效渲染和重建。&lt;h4&gt;目的&lt;/h4&gt;优化场景覆盖和捕捉细节，同时减少冗余点云，提高资源受限设备的部署效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种理论框架，分析密化过程，通过优化理论方法确定必要的条件，最小化后代的数量，确定最优的参数更新方向，并提供后代不透明度的解析解。基于这些发现，引入了 SteepGS，这是一种最陡密度控制策略，旨在最小化损失并保持紧凑的点云。&lt;h4&gt;主要发现&lt;/h4&gt;分析表明，分割对于逃离鞍点至关重要。通过优化理论方法，确定了密化的必要条件，确定了最小数量的后代高斯原语，确定了最优的参数更新方向，并为后代不透明度的标准化提供了解析解。&lt;h4&gt;结论&lt;/h4&gt;SteepGS 通过减少高斯点约50%，在不影响渲染质量的同时，显著提高了效率和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯喷射（3DGS）已经成为实时、高分辨率新视角合成的强大技术。通过将场景表示为高斯原语混合体，3DGS 利用 GPU 光栅化管道进行高效渲染和重建。为了优化场景覆盖和捕捉细节，3DGS 采用密化算法生成额外的点。然而，这个过程通常会导致冗余点云，从而增加内存使用、降低性能和大量存储需求，这对资源受限设备的部署构成了重大挑战。为了解决这一限制，我们提出了一种理论框架，揭示了 3DGS 中密度控制背后的原理并提高了其效率。我们的分析表明，分割对于逃离鞍点至关重要。通过一种优化理论方法，我们建立了密化的必要条件，确定了最小数量的后代高斯原语，确定了最优的参数更新方向，并提供了后代不透明度标准化的解析解。基于这些见解，我们引入了 SteepGS，这是一种采用最陡密度控制的策略，旨在最小化损失的同时保持紧凑的点云。SteepGS 通过减少高斯点约50%，在不影响渲染质量的同时，显著提高了效率和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a powerful technique forreal-time, high-resolution novel view synthesis. By representing scenes as amixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines forefficient rendering and reconstruction. To optimize scene coverage and capturefine details, 3DGS employs a densification algorithm to generate additionalpoints. However, this process often leads to redundant point clouds, resultingin excessive memory usage, slower performance, and substantial storage demands- posing significant challenges for deployment on resource-constrained devices.To address this limitation, we propose a theoretical framework that demystifiesand improves density control in 3DGS. Our analysis reveals that splitting iscrucial for escaping saddle points. Through an optimization-theoretic approach,we establish the necessary conditions for densification, determine the minimalnumber of offspring Gaussians, identify the optimal parameter update direction,and provide an analytical solution for normalizing off-spring opacity. Buildingon these insights, we introduce SteepGS, incorporating steepest densitycontrol, a principled strategy that minimizes loss while maintaining a compactpoint cloud. SteepGS achieves a ~50% reduction in Gaussian points withoutcompromising rendering quality, significantly enhancing both efficiency andscalability.</description>
      <author>example@mail.com (Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan)</author>
      <guid isPermaLink="false">2505.05587v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies</title>
      <link>http://arxiv.org/abs/2505.06145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合自适应微调、对比学习和正则化优化的策略，以提升基于Transformer模型的文本分类性能，并在FewRel 2.0数据集上进行了实验。&lt;h4&gt;背景&lt;/h4&gt;Few-shot文本分类在低资源环境中具有重要的应用价值。&lt;h4&gt;目的&lt;/h4&gt;提出一种策略来提升Transformer模型在少样本情况下的分类性能。&lt;h4&gt;方法&lt;/h4&gt;采用自适应微调、对比学习和正则化优化等方法。&lt;h4&gt;主要发现&lt;/h4&gt;T5-small、DeBERTa-v3和RoBERTa-base在少样本任务中表现良好，特别是5-shot设置可以更有效地捕捉文本特征并提高分类准确率。不同关系类别之间的分类难度存在显著差异，引入对比损失和正则化损失可以增强模型的泛化能力，有效缓解少样本环境中的过拟合问题。&lt;h4&gt;结论&lt;/h4&gt;使用Transformer模型或具有更强自注意力机制的生成架构可以提高少样本分类的稳定性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot text classification has important application value in low-resourceenvironments. This paper proposes a strategy that combines adaptivefine-tuning, contrastive learning, and regularization optimization to improvethe classification performance of Transformer-based models. Experiments on theFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base performwell in few-shot tasks, especially in the 5-shot setting, which can moreeffectively capture text features and improve classification accuracy. Theexperiment also found that there are significant differences in theclassification difficulty of different relationship categories. Some categorieshave fuzzy semantic boundaries or complex feature distributions, making itdifficult for the standard cross entropy loss to learn the discriminativeinformation required to distinguish categories. By introducing contrastive lossand regularization loss, the generalization ability of the model is enhanced,effectively alleviating the overfitting problem in few-shot environments. Inaddition, the research results show that the use of Transformer models orgenerative architectures with stronger self-attention mechanisms can helpimprove the stability and accuracy of few-shot classification.</description>
      <author>example@mail.com (Xu Han, Yumeng Sun, Weiqiang Huang, Hongye Zheng, Junliang Du)</author>
      <guid isPermaLink="false">2505.06145v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</title>
      <link>http://arxiv.org/abs/2505.05638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统地评估了最先进的运动预测器和运动规划器之间的相互作用，并研究了模型参数减少对驾驶性能的影响。&lt;h4&gt;背景&lt;/h4&gt;近年来，由于运动预测竞赛和基准的推动，基于学习的预测模型规模越来越大，参数数量可达数百万，旨在通过厘米级的精度提高开环预测的准确性。&lt;h4&gt;目的&lt;/h4&gt;评估这些模型在集成到自动驾驶堆栈后是否能够提高性能。&lt;h4&gt;方法&lt;/h4&gt;通过实验评估了最先进的运动预测器和运动规划器之间的相互作用，并研究了参数减少的模型在闭环驾驶性能上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;高开环准确性并不总是与更好的闭环驾驶行为相关，预测的时间一致性和规划器的兼容性等其他因素也起着关键作用。减少参数的模型在某些情况下表现出可比较甚至更优的闭环驾驶性能。&lt;h4&gt;结论&lt;/h4&gt;高开环准确性与闭环驾驶性能之间没有必然联系，模型参数的减少可能不会影响驾驶性能。&lt;h4&gt;翻译&lt;/h4&gt;Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at https://github.com/continental/pred2plan.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fueled by motion prediction competitions and benchmarks, recent years haveseen the emergence of increasingly large learning based prediction models, manywith millions of parameters, focused on improving open-loop prediction accuracyby mere centimeters. However, these benchmarks fail to assess whether suchimprovements translate to better performance when integrated into an autonomousdriving stack. In this work, we systematically evaluate the interplay betweenstate-of-the-art motion predictors and motion planners. Our results show thathigher open-loop accuracy does not always correlate with better closed-loopdriving behavior and that other factors, such as temporal consistency ofpredictions and planner compatibility, also play a critical role. Furthermore,we investigate downsized variants of these models, and, surprisingly, find thatin some cases models with up to 86% fewer parameters yield comparable or evensuperior closed-loop driving performance. Our code is available athttps://github.com/continental/pred2plan.</description>
      <author>example@mail.com (Mohamed-Khalil Bouzidi, Christian Schlauch, Nicole Scheuerer, Yue Yao, Nadja Klein, Daniel Göhring, Jörg Reichardt)</author>
      <guid isPermaLink="false">2505.05638v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Fast and Fourier Features for Transfer Learning of Interatomic Potentials</title>
      <link>http://arxiv.org/abs/2505.05652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了franken，一个可扩展且轻量级的迁移学习框架，用于训练机器学习原子间势，旨在提高计算效率和数据效率。&lt;h4&gt;背景&lt;/h4&gt;在原子模拟中，训练既计算高效又数据高效的机器学习原子间势是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提出franken框架，以实现原子间势的快速和准确适应，无需调整超参数或修改架构。&lt;h4&gt;方法&lt;/h4&gt;franken从预训练的图神经网络中提取原子描述符，并使用随机傅里叶特征将它们转移到新系统中。&lt;h4&gt;主要发现&lt;/h4&gt;在27种过渡金属的基准数据集上，franken在训练时间和准确性方面优于优化的核方法，将模型训练时间从数十小时缩短到几分钟。&lt;h4&gt;结论&lt;/h4&gt;franken框架展示了强大的数据效率，能够使用仅数十个训练结构稳定且准确地训练大量水以及Pt(111)/水界面的原子间势。&lt;h4&gt;翻译&lt;/h4&gt;Training machine learning interatomic potentials that are both computationally and data-efficient is a key challenge for enabling their routine use in atomistic simulations. To this effect, we introduce franken, a scalable and lightweight transfer learning framework that extracts atomic descriptors from pretrained graph neural networks and transfer them to new systems using random Fourier features-an efficient and scalable approximation of kernel methods. Franken enables fast and accurate adaptation of general-purpose potentials to new systems or levels of quantum mechanical theory without requiring hyperparameter tuning or architectural modifications. On a benchmark dataset of 27 transition metals, franken outperforms optimized kernel-based methods in both training time and accuracy, reducing model training from tens of hours to minutes on a single GPU. We further demonstrate the framework's strong data-efficiency by training stable and accurate potentials for bulk water and the Pt(111)/water interface using just tens of training structures. Our open-source implementation (https://franken.readthedocs.io) offers a fast and practical solution for training potentials and deploying them for molecular dynamics simulations across diverse systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training machine learning interatomic potentials that are bothcomputationally and data-efficient is a key challenge for enabling theirroutine use in atomistic simulations. To this effect, we introduce franken, ascalable and lightweight transfer learning framework that extracts atomicdescriptors from pretrained graph neural networks and transfer them to newsystems using random Fourier features-an efficient and scalable approximationof kernel methods. Franken enables fast and accurate adaptation ofgeneral-purpose potentials to new systems or levels of quantum mechanicaltheory without requiring hyperparameter tuning or architectural modifications.On a benchmark dataset of 27 transition metals, franken outperforms optimizedkernel-based methods in both training time and accuracy, reducing modeltraining from tens of hours to minutes on a single GPU. We further demonstratethe framework's strong data-efficiency by training stable and accuratepotentials for bulk water and the Pt(111)/water interface using just tens oftraining structures. Our open-source implementation(https://franken.readthedocs.io) offers a fast and practical solution fortraining potentials and deploying them for molecular dynamics simulationsacross diverse systems.</description>
      <author>example@mail.com (Pietro Novelli, Giacomo Meanti, Pedro J. Buigues, Lorenzo Rosasco, Michele Parrinello, Massimiliano Pontil, Luigi Bonati)</author>
      <guid isPermaLink="false">2505.05652v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Contrastive Learning through Relative Similarity Preservation</title>
      <link>http://arxiv.org/abs/2505.05533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCAI2025; full version including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图对比学习（GCL）在处理图数据时的挑战，提出了一种新的框架RELGCL，以解决传统方法在处理离散、非欧几里得性质图数据时的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统的GCL方法在保持增强视图之间的绝对相似性方面取得了成功，但在处理图数据时面临挑战，因为图数据具有离散和非欧几里得性质，导致视图生成破坏语义有效性和相似性验证的不可靠性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GCL框架，能够有效地利用图数据中的自然相对相似性模式。&lt;h4&gt;方法&lt;/h4&gt;通过分析11个现实世界的图，发现了一个超越同质性和异质性的普遍模式：随着结构距离的增加，标签一致性系统性地下降，在亲社会性图中表现为平滑衰减，在异质图中表现为振荡衰减。通过随机游走理论建立了对这个模式的保证，证明了标签分布收敛并描述了不同衰减行为背后的机制。&lt;h4&gt;主要发现&lt;/h4&gt;图自然地编码了相对相似性模式，结构上更接近的节点表现出更强的语义关系。&lt;h4&gt;结论&lt;/h4&gt;提出的RELGCL框架在亲社会性和异质图中均优于20种现有方法，验证了利用自然相对相似性而非人工绝对相似性的有效性。&lt;h4&gt;翻译&lt;/h4&gt;Graph contrastive learning (GCL) has achieved remarkable success by following the computer vision paradigm of preserving absolute similarity between augmented views. However, this approach faces fundamental challenges in graphs due to their discrete, non-Euclidean nature -- view generation often breaks semantic validity and similarity verification becomes unreliable. Through analyzing 11 real-world graphs, we discover a universal pattern transcending the homophily-heterophily dichotomy: label consistency systematically diminishes as structural distance increases, manifesting as smooth decay in homophily graphs and oscillatory decay in heterophily graphs. We establish theoretical guarantees for this pattern through random walk theory, proving label distribution convergence and characterizing the mechanisms behind different decay behaviors. This discovery reveals that graphs naturally encode relative similarity patterns, where structurally closer nodes exhibit collectively stronger semantic relationships. Leveraging this insight, we propose RELGCL, a novel GCL framework with complementary pairwise and listwise implementations that preserve these inherent patterns through collective similarity objectives. Extensive experiments demonstrate that our method consistently outperforms 20 existing approaches across both homophily and heterophily graphs, validating the effectiveness of leveraging natural relative similarity over artificial absolute similarity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) has achieved remarkable success by followingthe computer vision paradigm of preserving absolute similarity betweenaugmented views. However, this approach faces fundamental challenges in graphsdue to their discrete, non-Euclidean nature -- view generation often breakssemantic validity and similarity verification becomes unreliable. Throughanalyzing 11 real-world graphs, we discover a universal pattern transcendingthe homophily-heterophily dichotomy: label consistency systematicallydiminishes as structural distance increases, manifesting as smooth decay inhomophily graphs and oscillatory decay in heterophily graphs. We establishtheoretical guarantees for this pattern through random walk theory, provinglabel distribution convergence and characterizing the mechanisms behinddifferent decay behaviors. This discovery reveals that graphs naturally encoderelative similarity patterns, where structurally closer nodes exhibitcollectively stronger semantic relationships. Leveraging this insight, wepropose RELGCL, a novel GCL framework with complementary pairwise and listwiseimplementations that preserve these inherent patterns through collectivesimilarity objectives. Extensive experiments demonstrate that our methodconsistently outperforms 20 existing approaches across both homophily andheterophily graphs, validating the effectiveness of leveraging natural relativesimilarity over artificial absolute similarity.</description>
      <author>example@mail.com (Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou)</author>
      <guid isPermaLink="false">2505.05533v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous Information Networks</title>
      <link>http://arxiv.org/abs/2505.05989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究关注异构信息网络中的路径建模问题，并提出了一个多跳路径感知推荐框架。&lt;h4&gt;背景&lt;/h4&gt;研究背景是异构信息网络中的路径建模问题。&lt;h4&gt;目的&lt;/h4&gt;目的是提出一个能够有效捕获高阶交互语义的多跳路径感知推荐框架。&lt;h4&gt;方法&lt;/h4&gt;方法包括三个阶段：路径选择、语义表示和基于注意力的融合。路径选择阶段引入路径过滤机制去除冗余和噪声信息；表示学习阶段使用序列建模结构联合编码实体和关系；融合阶段使用注意力机制为每条路径分配不同权重以生成全局用户兴趣表示。&lt;h4&gt;主要发现&lt;/h4&gt;在亚马逊-图书等真实数据集上的实验表明，该方法在HR@10、Recall@10和Precision@10等多个评估指标上显著优于现有推荐模型。&lt;h4&gt;结论&lt;/h4&gt;结果表明，多跳路径在捕获高阶交互语义方面是有效的，并且该框架在异构推荐场景中具有强大的建模能力。该方法通过在异构网络中整合结构信息建模与推荐算法设计，提供了理论上的价值和实际应用的价值，为在复杂数据环境中学习用户偏好提供了一种更具有表达性和灵活性的范式。&lt;h4&gt;翻译&lt;/h4&gt;This study focuses on the problem of path modeling in heterogeneous information networks and proposes a multi-hop path-aware recommendation framework. The method centers on multi-hop paths composed of various types of entities and relations. It models user preferences through three stages: path selection, semantic representation, and attention-based fusion. In the path selection stage, a path filtering mechanism is introduced to remove redundant and noisy information. In the representation learning stage, a sequential modeling structure is used to jointly encode entities and relations, preserving the semantic dependencies within paths. In the fusion stage, an attention mechanism assigns different weights to each path to generate a global user interest representation. Experiments conducted on real-world datasets such as Amazon-Book show that the proposed method significantly outperforms existing recommendation models across multiple evaluation metrics, including HR@10, Recall@10, and Precision@10. The results confirm the effectiveness of multi-hop paths in capturing high-order interaction semantics and demonstrate the expressive modeling capabilities of the framework in heterogeneous recommendation scenarios. This method provides both theoretical and practical value by integrating structural information modeling in heterogeneous networks with recommendation algorithm design. It offers a more expressive and flexible paradigm for learning user preferences in complex data environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study focuses on the problem of path modeling in heterogeneousinformation networks and proposes a multi-hop path-aware recommendationframework. The method centers on multi-hop paths composed of various types ofentities and relations. It models user preferences through three stages: pathselection, semantic representation, and attention-based fusion. In the pathselection stage, a path filtering mechanism is introduced to remove redundantand noisy information. In the representation learning stage, a sequentialmodeling structure is used to jointly encode entities and relations, preservingthe semantic dependencies within paths. In the fusion stage, an attentionmechanism assigns different weights to each path to generate a global userinterest representation. Experiments conducted on real-world datasets such asAmazon-Book show that the proposed method significantly outperforms existingrecommendation models across multiple evaluation metrics, including HR@10,Recall@10, and Precision@10. The results confirm the effectiveness of multi-hoppaths in capturing high-order interaction semantics and demonstrate theexpressive modeling capabilities of the framework in heterogeneousrecommendation scenarios. This method provides both theoretical and practicalvalue by integrating structural information modeling in heterogeneous networkswith recommendation algorithm design. It offers a more expressive and flexibleparadigm for learning user preferences in complex data environments.</description>
      <author>example@mail.com (Hongye Zheng, Yue Xing, Lipeng Zhu, Xu Han, Junliang Du, Wanyu Cui)</author>
      <guid isPermaLink="false">2505.05989v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
      <link>http://arxiv.org/abs/2505.05736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First Draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MINT是一种通过偏好优化将单模态大型解码模型与多模态生物医学数据中的领域特定决策模式对齐的框架，用于提高预训练大型语言模型在生物医学任务中的微调能力。&lt;h4&gt;背景&lt;/h4&gt;高质量的多模态生物医学数据稀缺，限制了预训练大型语言模型在特定生物医学任务中的微调效果。&lt;h4&gt;目的&lt;/h4&gt;提出MINT框架，以解决高质量多模态生物医学数据稀缺的问题，提高预训练大型语言模型在生物医学任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;MINT通过偏好优化将单模态大型解码模型与多模态生物医学数据中的领域特定决策模式对齐，使用Odds Ratio Preference Optimization (ORPO)框架作为其核心优化技术，并利用上游的多模态机器学习模型来转移领域特定知识。&lt;h4&gt;主要发现&lt;/h4&gt;MINT在两个关键应用中展示了其有效性：(1) 从文本中预测罕见遗传疾病，MINT生成的模型在仅依赖文本输入的情况下优于使用SFT、RAG或DPO训练的模型；(2) 使用细胞核图像进行组织类型分类，MINT生成的模型显著提高了Llama 3.2-Vision-11B-Instruct在组织类型分类上的性能。&lt;h4&gt;结论&lt;/h4&gt;MINT通过偏好优化提供了一种有效的方法，将单模态大型语言模型与高质量的多模态专业知识对齐，从而提高其在生物医学任务中的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of high-quality multimodal biomedical data limits the ability toeffectively fine-tune pretrained Large Language Models (LLMs) for specializedbiomedical tasks. To address this challenge, we introduce MINT (MultimodalIntegrated kNowledge Transfer), a framework that aligns unimodal large decodermodels with domain-specific decision patterns from multimodal biomedical datathrough preference optimization. While MINT supports different optimizationtechniques, we primarily implement it with the Odds Ratio PreferenceOptimization (ORPO) framework as its backbone. This strategy enables thealigned LLMs to perform predictive tasks using text-only or image-only inputswhile retaining knowledge learnt from multimodal data. MINT leverages anupstream multimodal machine learning (MML) model trained on high-qualitymultimodal data to transfer domain-specific insights to downstream text-only orimage-only LLMs. We demonstrate its effectiveness through two key applications:(1) Rare genetic disease prediction from texts, where MINT uses a multimodalencoder model, trained on facial photos and clinical notes, to generate apreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despiterelying on text input only, the MINT-derived model outperforms models trainedwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissuetype classification using cell nucleus images, where MINT uses avision-language foundation model as the preference generator, containingknowledge learnt from both text and histopathological images to aligndownstream image-only models. The resulting MINT-derived model significantlyimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue typeclassification. In summary, MINT provides an effective strategy to alignunimodal LLMs with high-quality multimodal expertise through preferenceoptimization.</description>
      <author>example@mail.com (Da Wu, Zhanliang Wang, Quan Nguyen, Zhuoran Xu, Kai Wang)</author>
      <guid isPermaLink="false">2505.05736v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.13580v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://stefan-ainetter.github.io/SCANnotatepp; CVPR'25  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了在3D场景理解中的应用，提出了一种基于自动检索合成CAD模型的方法，以生成高质量的真实标签数据，用于训练深度学习模型。&lt;h4&gt;背景&lt;/h4&gt;高层次的3D场景理解在许多应用中至关重要，但生成精确的3D标注数据是深度学习模型开发的一大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究目的是利用自动检索合成CAD模型的方法，为监督式深度学习模型训练提供高质量的真实标签数据。&lt;h4&gt;方法&lt;/h4&gt;论文采用了与之前用于自动标注ScanNet场景中物体9D姿态和CAD模型相似的流程，并将其应用于ScanNet++ v1数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，基于自动获得的标注数据可以训练深度学习模型，且训练出的模型性能优于基于人工标注数据的模型。&lt;h4&gt;结论&lt;/h4&gt;自动3D标注具有提升模型性能的潜力，同时显著降低标注成本。论文将发布名为SCANnotate++的标注数据和训练模型，以支持未来3D场景理解的研究。&lt;h4&gt;翻译&lt;/h4&gt;High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stefan-ainetter/SCANnotatepp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level 3D scene understanding is essential in many applications. However,the challenges of generating accurate 3D annotations make development of deeplearning models difficult. We turn to recent advancements in automaticretrieval of synthetic CAD models, and show that data generated by such methodscan be used as high-quality ground truth for training supervised deep learningmodels. More exactly, we employ a pipeline akin to the one previously used toautomatically annotate objects in ScanNet scenes with their 9D poses and CADmodels. This time, we apply it to the recent ScanNet++ v1 dataset, whichpreviously lacked such annotations. Our findings demonstrate that it is notonly possible to train deep learning models on these automatically-obtainedannotations but that the resulting models outperform those trained on manuallyannotated data. We validate this on two distinct tasks: point cloud completionand single-view CAD model retrieval and alignment. Our results underscore thepotential of automatic 3D annotations to enhance model performance whilesignificantly reducing annotation costs. To support future research in 3D sceneunderstanding, we will release our annotations, which we call SCANnotate++,along with our trained models.</description>
      <author>example@mail.com (Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2504.13580v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Molecular Representation Learning via Structure Awareness</title>
      <link>http://arxiv.org/abs/2505.05877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Image Processing (TIP) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构感知的多模态自监督分子表示预训练框架（MMSA），旨在通过利用分子之间的不变知识来增强分子图表示。&lt;h4&gt;背景&lt;/h4&gt;分子表示的准确提取是药物发现过程中的关键步骤。近年来，分子表示学习方法取得了显著进展，其中基于图像和2D/3D拓扑的多模态分子表示方法已成为主流。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有多模态方法直接融合不同模态信息，忽视模态间交互作用和无法充分捕捉分子间复杂高阶关系和不变特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出的方法包括两个主要模块：多模态分子表示学习模块和结构感知模块。多模态分子表示学习模块协同处理同一分子的不同模态信息，以克服模态差异并生成统一的分子嵌入。结构感知模块通过构建超图结构来模拟分子之间的高阶相关性，并引入记忆机制来存储典型分子表示，与记忆库中的记忆锚对齐，以整合不变知识，从而提高模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;MMSA在MoleculeNet基准测试上实现了最先进的性能，平均ROC-AUC比基线方法提高了1.8%到9.6%。&lt;h4&gt;结论&lt;/h4&gt;MMSA框架有效提高了分子表示的准确性，对药物发现过程具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Accurate extraction of molecular representations is a critical step in the drug discovery process. In recent years, significant progress has been made in molecular representation learning methods, among which multi-modal molecular representation methods based on images, and 2D/3D topologies have become increasingly mainstream. However, existing these multi-modal approaches often directly fuse information from different modalities, overlooking the potential of intermodal interactions and failing to adequately capture the complex higher-order relationships and invariant features between molecules. To overcome these challenges, we propose a structure-awareness-based multi-modal self-supervised molecular representation pre-training framework (MMSA) designed to enhance molecular graph representations by leveraging invariant knowledge between molecules. The framework consists of two main modules: the multi-modal molecular representation learning module and the structure-awareness module. The multi-modal molecular representation learning module collaboratively processes information from different modalities of the same molecule to overcome intermodal differences and generate a unified molecular embedding. Subsequently, the structure-awareness module enhances the molecular representation by constructing a hypergraph structure to model higher-order correlations between molecules. This module also introduces a memory mechanism for storing typical molecular representations, aligning them with memory anchors in the memory bank to integrate invariant knowledge, thereby improving the model generalization ability. Extensive experiments have demonstrated the effectiveness of MMSA, which achieves state-of-the-art performance on the MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to 9.6% over baseline methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate extraction of molecular representations is a critical step in thedrug discovery process. In recent years, significant progress has been made inmolecular representation learning methods, among which multi-modal molecularrepresentation methods based on images, and 2D/3D topologies have becomeincreasingly mainstream. However, existing these multi-modal approaches oftendirectly fuse information from different modalities, overlooking the potentialof intermodal interactions and failing to adequately capture the complexhigher-order relationships and invariant features between molecules. Toovercome these challenges, we propose a structure-awareness-based multi-modalself-supervised molecular representation pre-training framework (MMSA) designedto enhance molecular graph representations by leveraging invariant knowledgebetween molecules. The framework consists of two main modules: the multi-modalmolecular representation learning module and the structure-awareness module.The multi-modal molecular representation learning module collaborativelyprocesses information from different modalities of the same molecule toovercome intermodal differences and generate a unified molecular embedding.Subsequently, the structure-awareness module enhances the molecularrepresentation by constructing a hypergraph structure to model higher-ordercorrelations between molecules. This module also introduces a memory mechanismfor storing typical molecular representations, aligning them with memoryanchors in the memory bank to integrate invariant knowledge, thereby improvingthe model generalization ability. Extensive experiments have demonstrated theeffectiveness of MMSA, which achieves state-of-the-art performance on theMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to9.6% over baseline methods.</description>
      <author>example@mail.com (Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang)</author>
      <guid isPermaLink="false">2505.05877v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Sensorimotor Learning for Open-world Robot Manipulation</title>
      <link>http://arxiv.org/abs/2505.06136v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ph.D. Dissertation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了开放世界机器人操作问题，即机器人必须能够泛化或快速适应它没有预先编程或预训练的新对象、场景或任务。通过有效的感觉运动学习方法，本文提出了新的观点和方法来提高数据效率，使机器人能够学习通用的操作技能。&lt;h4&gt;背景&lt;/h4&gt;开放世界机器人操作是一个挑战，因为机器人需要处理未知的新对象和任务。&lt;h4&gt;目的&lt;/h4&gt;通过有效的传感器运动学习方法解决开放世界机器人操作问题，提高机器人适应新环境和任务的能力。&lt;h4&gt;方法&lt;/h4&gt;利用有限的演示数据中的规律性，使机器人能够学习通用的操作技能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 引入方法赋予机器人以对象为中心的先验知识，使其能够从少量远程操作演示中学习通用的闭环传感器运动策略。2. 引入方法使机器人能够理解空间，从而从野外观测视频中模仿操作技能。3. 引入方法使机器人能够从以往的经验中识别可重用的技能，从而能够连续模仿多个任务。&lt;h4&gt;结论&lt;/h4&gt;本文的贡献为构建通用型个人机器人奠定了基础，这些机器人能够以低成本的数据收集快速适应新情况或任务，并且能够轻松与人类互动。通过使机器人能够从有限的数据中学习和泛化，本文向实现能够无缝集成到日常场景中的智能机器人助手的目标迈进。&lt;h4&gt;翻译&lt;/h4&gt;This dissertation considers Open-world Robot Manipulation, a manipulation problem where a robot must generalize or quickly adapt to new objects, scenes, or tasks for which it has not been pre-programmed or pre-trained. This dissertation tackles the problem using a methodology of efficient sensorimotor learning. The key to enabling efficient sensorimotor learning lies in leveraging regular patterns that exist in limited amounts of demonstration data. These patterns, referred to as ``regularity,'' enable the data-efficient learning of generalizable manipulation skills. This dissertation offers a new perspective on formulating manipulation problems through the lens of regularity. Building upon this notion, we introduce three major contributions. First, we introduce methods that endow robots with object-centric priors, allowing them to learn generalizable, closed-loop sensorimotor policies from a small number of teleoperation demonstrations. Second, we introduce methods that constitute robots' spatial understanding, unlocking their ability to imitate manipulation skills from in-the-wild video observations. Last but not least, we introduce methods that enable robots to identify reusable skills from their past experiences, resulting in systems that can continually imitate multiple tasks in a sequential manner. Altogether, the contributions of this dissertation help lay the groundwork for building general-purpose personal robots that can quickly adapt to new situations or tasks with low-cost data collection and interact easily with humans. By enabling robots to learn and generalize from limited data, this dissertation takes a step toward realizing the vision of intelligent robotic assistants that can be seamlessly integrated into everyday scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This dissertation considers Open-world Robot Manipulation, a manipulationproblem where a robot must generalize or quickly adapt to new objects, scenes,or tasks for which it has not been pre-programmed or pre-trained. Thisdissertation tackles the problem using a methodology of efficient sensorimotorlearning. The key to enabling efficient sensorimotor learning lies inleveraging regular patterns that exist in limited amounts of demonstrationdata. These patterns, referred to as ``regularity,'' enable the data-efficientlearning of generalizable manipulation skills. This dissertation offers a newperspective on formulating manipulation problems through the lens ofregularity. Building upon this notion, we introduce three major contributions.First, we introduce methods that endow robots with object-centric priors,allowing them to learn generalizable, closed-loop sensorimotor policies from asmall number of teleoperation demonstrations. Second, we introduce methods thatconstitute robots' spatial understanding, unlocking their ability to imitatemanipulation skills from in-the-wild video observations. Last but not least, weintroduce methods that enable robots to identify reusable skills from theirpast experiences, resulting in systems that can continually imitate multipletasks in a sequential manner. Altogether, the contributions of thisdissertation help lay the groundwork for building general-purpose personalrobots that can quickly adapt to new situations or tasks with low-cost datacollection and interact easily with humans. By enabling robots to learn andgeneralize from limited data, this dissertation takes a step toward realizingthe vision of intelligent robotic assistants that can be seamlessly integratedinto everyday scenarios.</description>
      <author>example@mail.com (Yifeng Zhu)</author>
      <guid isPermaLink="false">2505.06136v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Automated Learning of Semantic Embedding Representations for Diffusion Models</title>
      <link>http://arxiv.org/abs/2505.05732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended version of the paper published in SDM25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种改进的Denoising Diffusion Models（DDMs）的多级去噪自编码器框架，以提高其表示能力，并通过实验验证了其在图像嵌入和语义表示方面的优越性。&lt;h4&gt;背景&lt;/h4&gt;生成模型能够捕捉数据的真实分布，生成语义丰富的表示。尽管去噪扩散模型（DDMs）在生成能力上表现出色，但对其有效表示学习的需求尚未得到满足。&lt;h4&gt;目的&lt;/h4&gt;扩展DDMs的表示能力，并通过自条件扩散学习在去噪马尔可夫链上获取嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;采用多级去噪自编码器框架，引入序列一致的扩散Transformer和额外的时步依赖编码器，通过自条件扩散学习在去噪马尔可夫链上获取嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法生成的嵌入在多个数据集上进行了广泛的实验，结果表明，DDMs最优学习的嵌入在大多数情况下超过了最先进的自监督表示学习方法，达到了显著的判别语义表示质量。&lt;h4&gt;结论&lt;/h4&gt;DDMs不仅适合于生成任务，而且对于通用深度学习应用也可能具有潜在优势。&lt;h4&gt;翻译&lt;/h4&gt;Generative models capture the true distribution of data, yielding semantically rich representations. Denoising diffusion models (DDMs) exhibit superior generative capabilities, though efficient representation learning for them are lacking. In this work, we employ a multi-level denoising autoencoder framework to expand the representation capacity of DDMs, which introduces sequentially consistent Diffusion Transformers and an additional timestep-dependent encoder to acquire embedding representations on the denoising Markov chain through self-conditional diffusion learning. Intuitively, the encoder, conditioned on the entire diffusion process, compresses high-dimensional data into directional vectors in latent under different noise levels, facilitating the learning of image embeddings across all timesteps. To verify the semantic adequacy of embeddings generated through this approach, extensive experiments are conducted on various datasets, demonstrating that optimally learned embeddings by DDMs surpass state-of-the-art self-supervised representation learning methods in most cases, achieving remarkable discriminative semantic representation quality. Our work justifies that DDMs are not only suitable for generative tasks, but also potentially advantageous for general-purpose deep learning applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1137/1.9781611978520.1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models capture the true distribution of data, yieldingsemantically rich representations. Denoising diffusion models (DDMs) exhibitsuperior generative capabilities, though efficient representation learning forthem are lacking. In this work, we employ a multi-level denoising autoencoderframework to expand the representation capacity of DDMs, which introducessequentially consistent Diffusion Transformers and an additionaltimestep-dependent encoder to acquire embedding representations on thedenoising Markov chain through self-conditional diffusion learning.Intuitively, the encoder, conditioned on the entire diffusion process,compresses high-dimensional data into directional vectors in latent underdifferent noise levels, facilitating the learning of image embeddings acrossall timesteps. To verify the semantic adequacy of embeddings generated throughthis approach, extensive experiments are conducted on various datasets,demonstrating that optimally learned embeddings by DDMs surpassstate-of-the-art self-supervised representation learning methods in most cases,achieving remarkable discriminative semantic representation quality. Our workjustifies that DDMs are not only suitable for generative tasks, but alsopotentially advantageous for general-purpose deep learning applications.</description>
      <author>example@mail.com (Limai Jiang, Yunpeng Cai)</author>
      <guid isPermaLink="false">2505.05732v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos</title>
      <link>http://arxiv.org/abs/2505.05681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对卷尾猴的自然行为研究，使用预训练的视频文本基础模型，通过优化和调整，开发了一种从视频中检索有用片段的计算模型。&lt;h4&gt;背景&lt;/h4&gt;非人类灵长类动物的自然栖息地视频是研究其在野外行为的常见来源。&lt;h4&gt;目的&lt;/h4&gt;开发有用的计算模型，帮助研究人员从视频中检索有用片段。&lt;h4&gt;方法&lt;/h4&gt;使用原始、未标记的视频素材，结合弱音频描述，基于多模态大型语言模型（MLLMs）和视觉-语言模型（VLMs），提出了一种两阶段方法：数据预处理管道和微调过程。数据预处理管道自动提取干净且语义对齐的视频文本对，然后使用低秩适应（LoRA）方法微调预训练的Microsoft X-CLIP模型。&lt;h4&gt;主要发现&lt;/h4&gt;在领域数据上，16帧模型和8帧模型的$Hits@5$分别提升了167%和114%。基于$NDCG@K$结果，模型能够很好地对大多数考虑的行为进行排序，而测试的原始预训练模型则无法对它们进行排序。&lt;h4&gt;结论&lt;/h4&gt;研究成功开发了一种有效的计算模型，能够帮助研究人员从视频中检索到有用的片段，特别是在处理噪声视频和音频内容方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;This study focuses on the study of the natural behavior of capuchin monkeys using pre-trained video-text fundamental models, aiming to develop computational models that can help researchers retrieve useful clips from videos. The research successfully developed an effective computational model that can help researchers retrieve useful clips from videos, especially in dealing with noisy video and audio content.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video recordings of nonhuman primates in their natural habitat are a commonsource for studying their behavior in the wild. We fine-tune pre-trainedvideo-text foundational models for the specific domain of capuchin monkeys,with the goal of developing useful computational models to help researchers toretrieve useful clips from videos. We focus on the challenging problem oftraining a model based solely on raw, unlabeled video footage, using weak audiodescriptions sometimes provided by field collaborators. We leverage recentadvances in Multimodal Large Language Models (MLLMs) and Vision-Language Models(VLMs) to address the extremely noisy nature of both video and audio content.Specifically, we propose a two-folded approach: an agentic data treatmentpipeline and a fine-tuning process. The data processing pipeline automaticallyextracts clean and semantically aligned video-text pairs from the raw videos,which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP modelthrough Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of$167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame modelon our domain data. Moreover, based on $NDCG@K$ results, our model is able torank well most of the considered behaviors, while the tested raw pre-trainedmodels are not able to rank them at all. The code will be made available uponacceptance.</description>
      <author>example@mail.com (Giulio Cesare Mastrocinque Santo, Patrícia Izar, Irene Delval, Victor de Napole Gregolin, Nina S. T. Hirata)</author>
      <guid isPermaLink="false">2505.05681v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>3D Hand-Eye Calibration for Collaborative Robot Arm: Look at Robot Base Once</title>
      <link>http://arxiv.org/abs/2504.21619v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  updated&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的手眼标定方法，用于协作机器人领域，该方法简化了标定过程，提高了效率。&lt;h4&gt;背景&lt;/h4&gt;手眼标定是协作机器人领域的一个常见问题，涉及视觉传感器和机器人法兰之间的变换矩阵的确定。&lt;h4&gt;目的&lt;/h4&gt;目的是减少标定所需的时间和不便，特别是在需要频繁重新标定的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出了一个通用的数据集生成方法，用于点云配准，重点是使机器人基础点云与扫描数据对齐。进行了详细的模拟研究，并进行了工业环境中的实际实验。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在14个不同品牌的机器人臂上进行模拟和评估，包括KUKA、Universal Robots、UFACTORY和Franka Emika。物理实验表明，该方法在短短几秒钟内即可完成整个标定过程，性能与现有的商业手眼标定解决方案相当。&lt;h4&gt;结论&lt;/h4&gt;提供了一个用户友好的手眼标定解决方案，代码公开可在github.com/leihui6/LRBO上获取。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种改进的手眼标定方法，用于协作机器人领域，该方法简化了标定过程，提高了效率。手眼标定是协作机器人领域的一个常见问题，涉及视觉传感器和机器人法兰之间的变换矩阵的确定。目的是减少标定所需的时间和不便，特别是在需要频繁重新标定的情况下。提出了一个通用的数据集生成方法，用于点云配准，重点是使机器人基础点云与扫描数据对齐。进行了详细的模拟研究，并进行了工业环境中的实际实验。该方法在14个不同品牌的机器人臂上进行模拟和评估，包括KUKA、Universal Robots、UFACTORY和Franka Emika。物理实验表明，该方法在短短几秒钟内即可完成整个标定过程，性能与现有的商业手眼标定解决方案相当。提供了一个用户友好的手眼标定解决方案，代码公开可在github.com/leihui6/LRBO上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-eye calibration is a common problem in the field of collaborativerobotics, involving the determination of the transformation matrix between thevisual sensor and the robot flange to enable vision-based robotic tasks.However, this process typically requires multiple movements of the robot armand an external calibration object, making it both time-consuming andinconvenient, especially in scenarios where frequent recalibration isnecessary. In this work, we extend our previous method which eliminates theneed for external calibration objects such as a chessboard. We propose ageneric dataset generation approach for point cloud registration, focusing onaligning the robot base point cloud with the scanned data. Furthermore, a moredetailed simulation study is conducted involving several differentcollaborative robot arms, followed by real-world experiments in an industrialsetting. Our improved method is simulated and evaluated using a total of 14robotic arms from 9 different brands, including KUKA, Universal Robots,UFACTORY, and Franka Emika, all of which are widely used in the field ofcollaborative robotics. Physical experiments demonstrate that our extendedapproach achieves performance comparable to existing commercial hand-eyecalibration solutions, while completing the entire calibration procedure injust a few seconds. In addition, we provide a user-friendly hand-eyecalibration solution, with the code publicly available atgithub.com/leihui6/LRBO.</description>
      <author>example@mail.com (Leihui Li, Lixuepiao Wan, Volker Krueger, Xuping Zhang)</author>
      <guid isPermaLink="false">2504.21619v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction</title>
      <link>http://arxiv.org/abs/2505.05644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将月球图像的反射率参数估计和基于图像的3D重建作为多模态学习问题，并设计了一种统一的变压器架构，用于学习不同数据源之间的共享表示。&lt;h4&gt;背景&lt;/h4&gt;多模态学习是多学科新兴的研究主题，但在行星科学中应用较少。&lt;h4&gt;目的&lt;/h4&gt;将月球图像的反射率参数估计和基于图像的3D重建问题转化为多模态学习问题，并提出相应的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种统一的变压器架构，用于学习灰度图像、数字高程模型、表面法线和反照率图等多种数据源之间的共享表示。&lt;h4&gt;主要发现&lt;/h4&gt;该架构能够从灰度图像中同时预测数字高程模型和反照率图，解决了行星表面3D重建的任务，并解耦了光度参数和高度信息。&lt;h4&gt;结论&lt;/h4&gt;该基础模型能够在四种模态之间学习到物理上合理的关联关系，未来可以通过添加更多输入模态来实现诸如光度归一化和共定位等任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning is an emerging research topic across multiple disciplinesbut has rarely been applied to planetary science. In this contribution, weidentify that reflectance parameter estimation and image-based 3Dreconstruction of lunar images can be formulated as a multimodal learningproblem. We propose a single, unified transformer architecture trained to learnshared representations between multiple sources like grayscale images, digitalelevation models, surface normals, and albedo maps. The architecture supportsflexible translation from any input modality to any target modality. PredictingDEMs and albedo maps from grayscale images simultaneously solves the task of 3Dreconstruction of planetary surfaces and disentangles photometric parametersand height information. Our results demonstrate that our foundation modellearns physically plausible relations across these four modalities. Adding moreinput modalities in the future will enable tasks such as photometricnormalization and co-registration.</description>
      <author>example@mail.com (Tom Sander, Moritz Tenthoff, Kay Wohlfarth, Christian Wöhler)</author>
      <guid isPermaLink="false">2505.05644v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models</title>
      <link>http://arxiv.org/abs/2505.05577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 42nd International Conference on Machine Learning,  Vancouver, Canada. PMLR 267, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PyTDC，一个开源的机器学习平台，用于多模态生物AI模型的训练、评估和推理。PyTDC整合了分布式、异构、持续更新的数据源和模型权重，并标准化了基准测试和推理端点。&lt;h4&gt;背景&lt;/h4&gt;现有的生物医学基准未能提供整合多模态生物数据和广泛机器学习任务的模型训练、评估和推理的端到端基础设施。&lt;h4&gt;目的&lt;/h4&gt;提出PyTDC平台，以促进多模态、上下文感知的基础模型在生物医学AI开放问题上的研究。&lt;h4&gt;方法&lt;/h4&gt;讨论了PyTDC架构的组件，并进行了首个针对单细胞药物靶点提名机器学习任务的案例研究。&lt;h4&gt;主要发现&lt;/h4&gt;在图表示学习和图理论领域的最先进方法在该任务上表现不佳。虽然发现了一种上下文感知的几何深度学习方法，其性能优于评估的SoTA和领域特定基线方法，但该模型无法泛化到未见过的细胞类型或整合额外的模态。&lt;h4&gt;结论&lt;/h4&gt;PyTDC平台能够促进关于开发多模态、上下文感知的基础模型的研究，这些模型可以解决生物医学AI中的开放问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现有的生物医学基准未能提供整合多模态生物数据和广泛机器学习任务的模型训练、评估和推理的端到端基础设施。我们提出了PyTDC，一个开源的机器学习平台，提供了多模态生物AI模型的简化训练、评估和推理软件。PyTDC统一了分布式、异构、持续更新的数据源和模型权重，并标准化了基准测试和推理端点。本文讨论了PyTDC架构的组件，并进行了我们知识范围内的首个关于引入的单细胞药物靶点提名机器学习任务的案例研究。我们发现图表示学习和图理论领域的最先进方法在该任务上表现不佳。尽管我们发现了一种上下文感知的几何深度学习方法，其性能优于评估的SoTA和领域特定基线方法，但该模型无法泛化到未见过的细胞类型或整合额外的模态，这突显了PyTDC平台促进关于开发多模态、上下文感知的基础模型的研究的能力，这些模型可以解决生物医学AI中的开放问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/apliko-xyz/pytdc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing biomedical benchmarks do not provide end-to-end infrastructure fortraining, evaluation, and inference of models that integrate multimodalbiological data and a broad range of machine learning tasks in therapeutics. Wepresent PyTDC, an open-source machine-learning platform providing streamlinedtraining, evaluation, and inference software for multimodal biological AImodels. PyTDC unifies distributed, heterogeneous, continuously updated datasources and model weights and standardizes benchmarking and inferenceendpoints. This paper discusses the components of PyTDC's architecture and, toour knowledge, the first-of-its-kind case study on the introduced single-celldrug-target nomination ML task. We find state-of-the-art methods in graphrepresentation learning and domain-specific methods from graph theory performpoorly on this task. Though we find a context-aware geometric deep learningmethod that outperforms the evaluated SoTA and domain-specific baselinemethods, the model is unable to generalize to unseen cell types or incorporateadditional modalities, highlighting PyTDC's capacity to facilitate an excitingavenue of research developing multimodal, context-aware, foundation models foropen problems in biomedical AI.</description>
      <author>example@mail.com (Alejandro Velez-Arce, Marinka Zitnik)</author>
      <guid isPermaLink="false">2505.05577v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction</title>
      <link>http://arxiv.org/abs/2505.05612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;scDrugMap是一个集成的框架，用于预测单细胞数据中的药物反应，旨在解决癌症治疗中的药物耐药性问题。&lt;h4&gt;背景&lt;/h4&gt;药物耐药性是癌症治疗中的主要挑战，单细胞分析有助于了解细胞异质性，但大规模基础模型在预测单细胞数据药物反应中的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发scDrugMap框架，以预测单细胞数据中的药物反应，并建立大规模基准。&lt;h4&gt;方法&lt;/h4&gt;scDrugMap使用Python命令行界面和Web服务器进行药物反应预测。它评估了包括八种单细胞模型和两种大型语言模型在内的多种基础模型。使用超过326,000个细胞的原始数据集和18,800个细胞的验证集进行评估，涵盖了36个数据集和多种组织和癌症类型。模型性能在池化数据和跨数据评估设置下进行了基准测试，使用了层冻结和低秩适应（LoRA）微调策略。&lt;h4&gt;主要发现&lt;/h4&gt;在池化数据场景中，scFoundation模型在层冻结和微调情况下均表现出最佳性能，F1分数分别为0.971和0.947，超过表现最差的模型50%以上。在跨数据设置中，UCE模型在微调后表现突出（平均F1：0.774），而scGPT在零样本学习中领先（平均F1：0.858）。&lt;h4&gt;结论&lt;/h4&gt;scDrugMap为单细胞数据中药物反应预测的基础模型提供了第一个大规模基准，是一个用户友好且灵活的平台，用于推进药物发现和转化研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/qsong-github/scdrugmap&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug resistance presents a major challenge in cancer therapy. Single cellprofiling offers insights into cellular heterogeneity, yet the application oflarge-scale foundation models for predicting drug response in single cell dataremains underexplored. To address this, we developed scDrugMap, an integratedframework featuring both a Python command-line interface and a web server fordrug response prediction. scDrugMap evaluates a wide range of foundationmodels, including eight single-cell models and two large language models, usinga curated dataset of over 326,000 cells in the primary collection and 18,800cells in the validation set, spanning 36 datasets and diverse tissue and cancertypes. We benchmarked model performance under pooled-data and cross-dataevaluation settings, employing both layer freezing and Low-Rank Adaptation(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundationachieved the best performance, with mean F1 scores of 0.971 (layer freezing)and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMapprovides the first large-scale benchmark of foundation models for drug responseprediction in single-cell data and serves as a user-friendly, flexible platformfor advancing drug discovery and translational research.</description>
      <author>example@mail.com (Qing Wang, Yining Pan, Minghao Zhou, Zijia Tang, Yanfei Wang, Guangyu Wang, Qianqian Song)</author>
      <guid isPermaLink="false">2505.05612v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning</title>
      <link>http://arxiv.org/abs/2505.05192v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究如何结合长期观察数据和短期实验数据来估计长期因果关系，并提出了一种基于自然数据异质性的方法来识别潜在混杂因素，避免了对理想化假设的依赖。&lt;h4&gt;背景&lt;/h4&gt;在现实场景中，估计长期因果关系是一个关键但具有挑战性的问题，现有的方法通常依赖于理想化的假设，而这些假设在实际情况中往往不成立，限制了其实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要理想化假设的方法来估计长期个体因果关系。&lt;h4&gt;方法&lt;/h4&gt;利用数据的自然异质性，如来自多个来源的数据，来识别潜在混杂因素，并设计了一种基于潜在表示学习估计长期因果效应的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个合成和半合成数据集上进行的实验研究，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法可以有效地估计长期因果关系，且不依赖于理想化假设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/learnwjj/ICEVAE&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating long-term causal effects by combining long-term observational andshort-term experimental data is a crucial but challenging problem in manyreal-world scenarios. In existing methods, several ideal assumptions, e.g.latent unconfoundedness assumption or additive equi-confounding biasassumption, are proposed to address the latent confounder problem raised by theobservational data. However, in real-world applications, these assumptions aretypically violated which limits their practical effectiveness. In this paper,we tackle the problem of estimating the long-term individual causal effectswithout the aforementioned assumptions. Specifically, we propose to utilize thenatural heterogeneity of data, such as data from multiple sources, to identifylatent confounders, thereby significantly avoiding reliance on idealizedassumptions. Practically, we devise a latent representation learning-basedestimator of long-term causal effects. Theoretically, we establish theidentifiability of latent confounders, with which we further achieve long-termeffect identification. Extensive experimental studies, conducted on multiplesynthetic and semi-synthetic datasets, demonstrate the effectiveness of ourproposed method.</description>
      <author>example@mail.com (Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo)</author>
      <guid isPermaLink="false">2505.05192v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Griffin: Towards a Graph-Centric Relational Database Foundation Model</title>
      <link>http://arxiv.org/abs/2505.05568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Griffin，这是第一个专门为关系数据库（RDBs）设计的基座模型。Griffin通过统一数据编码器和任务解码器来处理各种任务，并集成了交叉注意力模块和新型聚合器，在单表和RDB数据集上进行预训练，展示了其在处理大规模、异构和时间图上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;目前存在一些小型模型专注于单一RDB任务，但它们无法处理多样化的任务。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够处理关系数据库多样化任务的基座模型。&lt;h4&gt;方法&lt;/h4&gt;Griffin模型集成了交叉注意力模块和新型聚合器，并在单表和RDB数据集上进行预训练，使用高级编码器处理分类、数值和元数据特征。&lt;h4&gt;主要发现&lt;/h4&gt;Griffin在大型、异构和时间图上表现出优异或相当的性能，擅长处理低数据场景，并在预训练新数据集和任务时显示出强大的迁移能力。&lt;h4&gt;结论&lt;/h4&gt;Griffin作为一个适用于关系数据库的通用基座模型具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;We introduce Griffin, the first foundation model attemptation designed specifically for Relational Databases (RDBs). Unlike previous smaller models focused on single RDB tasks, Griffin unifies the data encoder and task decoderto handle diverse tasks. Additionally, we enhance the architecture by incorporating a cross-attention module and a novel aggregator. Griffin utilizes pretraining on both single-table and RDB datasets, employing advanced encoders for categorical, numerical, and metadata features, along with innovative components such as cross-attention modules and enhanced message-passing neural networks (MPNNs) to capture the complexities of relational data. Evaluated on large-scale, heterogeneous, and temporal graphs extracted from RDBs across various domains (spanning over 150 million nodes), Griffin demonstratessuperior or comparable performance to individually trained models, excels in low-data scenarios, and shows strong transferability with similarity and diversity in pretraining across new datasets and tasks, highlighting its potential as a universally applicable foundation model for RDBs. Code available at https://github.com/yanxwb/Griffin.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Griffin, the first foundation model attemptation designedspecifically for Relational Databases (RDBs). Unlike previous smaller modelsfocused on single RDB tasks, Griffin unifies the data encoder and task decoderto handle diverse tasks. Additionally, we enhance the architecture byincorporating a cross-attention module and a novel aggregator. Griffin utilizespretraining on both single-table and RDB datasets, employing advanced encodersfor categorical, numerical, and metadata features, along with innovativecomponents such as cross-attention modules and enhanced message-passing neuralnetworks (MPNNs) to capture the complexities of relational data. Evaluated onlarge-scale, heterogeneous, and temporal graphs extracted from RDBs acrossvarious domains (spanning over 150 million nodes), Griffin demonstratessuperior or comparable performance to individually trained models, excels inlow-data scenarios, and shows strong transferability with similarity anddiversity in pretraining across new datasets and tasks, highlighting itspotential as a universally applicable foundation model for RDBs. Code availableat https://github.com/yanxwb/Griffin.</description>
      <author>example@mail.com (Yanbo Wang, Xiyuan Wang, Quan Gan, Minjie Wang, Qibin Yang, David Wipf, Muhan Zhang)</author>
      <guid isPermaLink="false">2505.05568v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.05049v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于贝叶斯熵公式的理论动机不确定性量化模型，用于解决Segment Anything Model (SAM)的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;SAM的引入为语义分割应用铺平了道路，但其不确定性量化存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时尊重随机、认知和任务不确定性的不确定性量化模型。&lt;h4&gt;方法&lt;/h4&gt;使用贝叶斯熵公式训练了USAM，这是一种轻量级的后处理不确定性量化方法。该模型将不确定性的根源追溯到欠参数化模型、不充分的提示或图像模糊性。&lt;h4&gt;主要发现&lt;/h4&gt;USAM在SA-V、MOSE、ADE20k、DAVIS和COCO数据集上显示出优越的预测能力，提供了一种计算成本低、易于使用的UQ替代方案。&lt;h4&gt;结论&lt;/h4&gt;USAM可以作为支持用户提示、增强半监督流程或平衡准确性和成本效率之间权衡的不确定性量化工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/GreenAutoML4FAS/UncertainSAM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The introduction of the Segment Anything Model (SAM) has paved the way fornumerous semantic segmentation applications. For several tasks, quantifying theuncertainty of SAM is of particular interest. However, the ambiguous nature ofthe class-agnostic foundation model SAM challenges current uncertaintyquantification (UQ) approaches. This paper presents a theoretically motivateduncertainty quantification model based on a Bayesian entropy formulationjointly respecting aleatoric, epistemic, and the newly introduced taskuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQmethod. Our model traces the root of uncertainty back to under-parameterisedmodels, insufficient prompts or image ambiguities. Our proposed deterministicUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQalternative that can support user-prompting, enhance semi-supervised pipelines,or balance the tradeoff between accuracy and cost efficiency.</description>
      <author>example@mail.com (Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2505.05049v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility</title>
      <link>http://arxiv.org/abs/2505.05518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于人工智能的跟踪模型，用于实时跟踪心脏内导管尖端的位置，以提高介入性电生理和结构性心脏病治疗中的操作效率和安全性。&lt;h4&gt;背景&lt;/h4&gt;心脏内超声心动图（ICE）在电生理和结构性心脏病介入治疗中扮演着关键角色，但由于手动操作ICE导管时需要频繁调整，持续监测导管尖端位置是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个AI驱动的跟踪模型，以估计ICE成像平面内的导管尖端入射角和通过点，确保连续的可视化并便于机器人ICE导管控制。&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合数据集生成策略，结合临床ICE序列和合成数据增强以增强模型鲁棒性。在水箱设置中收集ICE图像，并使用电磁传感器确定精确的真实位置。合成序列通过将导管尖端叠加到真实ICE图像上来创建，以保持运动连续性并模拟不同的解剖场景。最终数据集包含5,698个ICE尖端图像对。模型架构集成了预训练的超声基础模型，并使用基于变换器的网络处理连续的ICE帧。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法实现了3.32度的入口角度误差和12.76度的旋转角度误差，为实时机器人ICE导管调整奠定了基础，最小化了操作者的工作量，同时确保了治疗设备的连续可视性。&lt;h4&gt;结论&lt;/h4&gt;该AI驱动的框架有助于提高心脏介入治疗中的操作效率和安全性，未来工作将集中于扩大临床数据集以进一步提高模型泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Intra-cardiac Echocardiography (ICE) plays a critical role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing real-time visualization of intracardiac structures. However, maintaining continuous visibility of the therapy device tip remains a challenge due to frequent adjustments required during manual ICE catheter manipulation. To address this, we propose an AI-driven tracking model that estimates the device tip incident angle and passing point within the ICE imaging plane, ensuring continuous visibility and facilitating robotic ICE catheter control. A key innovation of our approach is the hybrid dataset generation strategy, which combines clinical ICE sequences with synthetic data augmentation to enhance model robustness. We collected ICE images in a water chamber setup, equipping both the ICE catheter and device tip with electromagnetic (EM) sensors to establish precise ground-truth locations. Synthetic sequences were created by overlaying catheter tips onto real ICE images, preserving motion continuity while simulating diverse anatomical scenarios. The final dataset consists of 5,698 ICE-tip image pairs, ensuring comprehensive training coverage. Our model architecture integrates a pretrained ultrasound (US) foundation model, trained on 37.4M echocardiography images, for feature extraction. A transformer-based network processes sequential ICE frames, leveraging historical passing points and incident angles to improve prediction accuracy. Experimental results demonstrate that our method achieves 3.32 degree entry angle error, 12.76 degree rotation angle error. This AI-driven framework lays the foundation for real-time robotic ICE catheter adjustments, minimizing operator workload while ensuring consistent therapy device visibility. Future work will focus on expanding clinical datasets to further enhance model generalization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intra-cardiac Echocardiography (ICE) plays a critical role inElectrophysiology (EP) and Structural Heart Disease (SHD) interventions byproviding real-time visualization of intracardiac structures. However,maintaining continuous visibility of the therapy device tip remains a challengedue to frequent adjustments required during manual ICE catheter manipulation.To address this, we propose an AI-driven tracking model that estimates thedevice tip incident angle and passing point within the ICE imaging plane,ensuring continuous visibility and facilitating robotic ICE catheter control.  A key innovation of our approach is the hybrid dataset generation strategy,which combines clinical ICE sequences with synthetic data augmentation toenhance model robustness. We collected ICE images in a water chamber setup,equipping both the ICE catheter and device tip with electromagnetic (EM)sensors to establish precise ground-truth locations. Synthetic sequences werecreated by overlaying catheter tips onto real ICE images, preserving motioncontinuity while simulating diverse anatomical scenarios. The final datasetconsists of 5,698 ICE-tip image pairs, ensuring comprehensive trainingcoverage.  Our model architecture integrates a pretrained ultrasound (US) foundationmodel, trained on 37.4M echocardiography images, for feature extraction. Atransformer-based network processes sequential ICE frames, leveraginghistorical passing points and incident angles to improve prediction accuracy.  Experimental results demonstrate that our method achieves 3.32 degree entryangle error, 12.76 degree rotation angle error. This AI-driven framework laysthe foundation for real-time robotic ICE catheter adjustments, minimizingoperator workload while ensuring consistent therapy device visibility. Futurework will focus on expanding clinical datasets to further enhance modelgeneralization.</description>
      <author>example@mail.com (Jaeyoung Huh, Ankur Kapoor, Young-Ho Kim)</author>
      <guid isPermaLink="false">2505.05518v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>AI-powered virtual eye: perspective, challenges and opportunities</title>
      <link>http://arxiv.org/abs/2505.05516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 Pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为‘虚拟眼’的下一代AI平台，利用互联的基础模型模拟眼睛的复杂结构和生物功能。&lt;h4&gt;背景&lt;/h4&gt;AI、成像和多组学技术的进步为构建高保真的人类眼睛数字复制品提供了肥沃的土壤。&lt;h4&gt;目的&lt;/h4&gt;追溯从早期机械和基于规则的模型到当代AI驱动方法的发展历程，并整合一个具有多模态、多尺度、动态预测能力和嵌入式反馈机制的综合模型。&lt;h4&gt;方法&lt;/h4&gt;提出一个发展路线图，强调大规模多模态数据集、生成AI、基础模型、基于代理的架构和交互式界面的作用。&lt;h4&gt;主要发现&lt;/h4&gt;尽管存在可解释性、伦理、数据处理和评估的挑战，虚拟眼有潜力革命性地改变个性化眼科护理并加速眼健康和疾病的研究。&lt;h4&gt;结论&lt;/h4&gt;虚拟眼有望在眼科护理和眼健康研究领域带来重大变革。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We envision the "virtual eye" as a next-generation, AI-powered platform thatuses interconnected foundation models to simulate the eye's intricate structureand biological function across all scales. Advances in AI, imaging, andmultiomics provide a fertile ground for constructing a universal, high-fidelitydigital replica of the human eye. This perspective traces the evolution fromearly mechanistic and rule-based models to contemporary AI-driven approaches,integrating in a unified model with multimodal, multiscale, dynamic predictivecapabilities and embedded feedback mechanisms. We propose a development roadmapemphasizing the roles of large-scale multimodal datasets, generative AI,foundation models, agent-based architectures, and interactive interfaces.Despite challenges in interpretability, ethics, data processing and evaluation,the virtual eye holds the potential to revolutionize personalized ophthalmiccare and accelerate research into ocular health and disease.</description>
      <author>example@mail.com (Yue Wu, Yibo Guo, Yulong Yan, Jiancheng Yang, Xin Zhou, Ching-Yu Cheng, Danli Shi, Mingguang He)</author>
      <guid isPermaLink="false">2505.05516v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience</title>
      <link>http://arxiv.org/abs/2505.05515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个基于神经科学的自主推理框架，旨在提高智能系统的自主推理能力。&lt;h4&gt;背景&lt;/h4&gt;随着自主人工智能的发展，研究如何使智能代理真正实现自主性成为一个关键问题。&lt;h4&gt;目的&lt;/h4&gt;探究智能代理自主性的核心，即代理推理，并建立一个新的神经科学启发框架。&lt;h4&gt;方法&lt;/h4&gt;基于神经科学的三个定义，提出了一个统一的框架，包括感知、维度、逻辑和交互四种核心推理类型，并应用于分类和分析现有的AI推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;该框架有助于理解和评估现有AI推理方法的理论基础、计算设计和实践限制，并提出了构建更具通用性和认知一致性的智能代理的新方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合认知神经科学和人工智能，该研究为推进智能系统中的代理推理提供了理论基础和实践路线。&lt;h4&gt;翻译&lt;/h4&gt;Autonomous AI is no longer a hard-to-reach concept, it enables the agents to move beyond executing tasks to independently addressing complex problems, adapting to change while handling the uncertainty of the environment. However, what makes the agents truly autonomous? It is agentic reasoning, that is crucial for foundation models to develop symbolic logic, statistical correlations, or large-scale pattern recognition to process information, draw inferences, and make decisions. However, it remains unclear why and how existing agentic reasoning approaches work, in comparison to biological reasoning, which instead is deeply rooted in neural mechanisms involving hierarchical cognition, multimodal integration, and dynamic interactions. In this work, we propose a novel neuroscience-inspired framework for agentic reasoning. Grounded in three neuroscience-based definitions and supported by mathematical and biological foundations, we propose a unified framework modeling reasoning from perception to action, encompassing four core types, perceptual, dimensional, logical, and interactive, inspired by distinct functional roles observed in the human brain. We apply this framework to systematically classify and analyze existing AI reasoning methods, evaluating their theoretical foundations, computational designs, and practical limitations. We also explore its implications for building more generalizable, cognitively aligned agents in physical and virtual environments. Finally, building on our framework, we outline future directions and propose new neural-inspired reasoning methods, analogous to chain-of-thought prompting. By bridging cognitive neuroscience and AI, this work offers a theoretical foundation and practical roadmap for advancing agentic reasoning in intelligent systems. The associated project can be found at: https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous AI is no longer a hard-to-reach concept, it enables the agents tomove beyond executing tasks to independently addressing complex problems,adapting to change while handling the uncertainty of the environment. However,what makes the agents truly autonomous? It is agentic reasoning, that iscrucial for foundation models to develop symbolic logic, statisticalcorrelations, or large-scale pattern recognition to process information, drawinferences, and make decisions. However, it remains unclear why and howexisting agentic reasoning approaches work, in comparison to biologicalreasoning, which instead is deeply rooted in neural mechanisms involvinghierarchical cognition, multimodal integration, and dynamic interactions. Inthis work, we propose a novel neuroscience-inspired framework for agenticreasoning. Grounded in three neuroscience-based definitions and supported bymathematical and biological foundations, we propose a unified frameworkmodeling reasoning from perception to action, encompassing four core types,perceptual, dimensional, logical, and interactive, inspired by distinctfunctional roles observed in the human brain. We apply this framework tosystematically classify and analyze existing AI reasoning methods, evaluatingtheir theoretical foundations, computational designs, and practicallimitations. We also explore its implications for building more generalizable,cognitively aligned agents in physical and virtual environments. Finally,building on our framework, we outline future directions and propose newneural-inspired reasoning methods, analogous to chain-of-thought prompting. Bybridging cognitive neuroscience and AI, this work offers a theoreticalfoundation and practical roadmap for advancing agentic reasoning in intelligentsystems. The associated project can be found at:https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .</description>
      <author>example@mail.com (Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun Tang, Lin Wang)</author>
      <guid isPermaLink="false">2505.05515v1</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAISY的运动感知图像合成方法，用于消除医学图像采集过程中的运动模糊，并通过实验证明其性能优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;患者运动会导致医学图像模糊、鬼影和器官变形，从而增加图像解读的难度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来消除医学图像中的运动伪影，并提高图像质量。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法首先对运动进行特征化，然后利用Segment Anything Model (SAM)动态学习解剖边界处的空间模式，并引入Variance-Selective SSIM (VS-SSIM)损失函数来强调像素方差高的区域，以保留关键解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;与现有技术相比，MAISY在胸部和头部CT数据集上的Peak Signal-to-Noise Ratio (PSNR)、SSIM和Dice指数均有显著提升。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法在消除医学图像中的运动伪影方面表现优异，有效提高了图像质量。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterizes motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging. Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v3</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</title>
      <link>http://arxiv.org/abs/2407.06188v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://yukangcao.github.io/CrowdMoGen&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrowdMoGen是一个零样本框架，用于集体运动生成，能够从文本提示中有效分组个体并生成与事件对齐的运动序列。&lt;h4&gt;背景&lt;/h4&gt;当前文本到运动生成技术通常假设所有个体作为一个单一单元，难以扩展到更大的人群并确保个体对特定事件做出适当的反应。&lt;h4&gt;目的&lt;/h4&gt;提出CrowdMoGen，以解决上述挑战，实现集体运动生成。&lt;h4&gt;方法&lt;/h4&gt;1) 利用预训练的大语言模型（LLMs）组织个体成不同群体；2) 集成基于SMPL的关节先验生成与情境相符的活动，包括关节轨迹和文本描述；3) 引入集体运动生成器，将活动整合到基于transformer的网络中，在多步去噪过程中保持空间约束。&lt;h4&gt;主要发现&lt;/h4&gt;CrowdMoGen在实验中显著优于先前方法，能够生成逼真、由事件驱动且空间上连贯的运动序列。&lt;h4&gt;结论&lt;/h4&gt;CrowdMoGen作为第一个集体运动生成框架，有望推动城市模拟、人群规划和其他大规模交互环境中的应用。&lt;h4&gt;翻译&lt;/h4&gt;While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-07-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advances in text-to-motion generation have shown promisingresults, they typically assume all individuals are grouped as a single unit.Scaling these methods to handle larger crowds and ensuring that individualsrespond appropriately to specific events remains a significant challenge. Thisis primarily due to the complexities of scene planning, which involvesorganizing groups, planning their activities, and coordinating interactions,and controllable motion generation. In this paper, we present CrowdMoGen, thefirst zero-shot framework for collective motion generation, which effectivelygroups individuals and generates event-aligned motion sequences from textprompts. 1) Being limited by the available datasets for training an effectivescene planning module in a supervised manner, we instead propose a crowd sceneplanner that leverages pre-trained large language models (LLMs) to organizeindividuals into distinct groups. While LLMs offer high-level guidance forgroup divisions, they lack the low-level understanding of human motion. Toaddress this, we further propose integrating an SMPL-based joint prior togenerate context-appropriate activities, which consists of both jointtrajectories and textual descriptions. 2) Secondly, to incorporate the assignedactivities into the generative network, we introduce a collective motiongenerator that integrates the activities into a transformer-based network in ajoint-wise manner, maintaining the spatial constraints during the multi-stepdenoising process. Extensive experiments demonstrate that CrowdMoGensignificantly outperforms previous approaches, delivering realistic,event-driven motion sequences that are spatially coherent. As the firstframework of collective motion generation, CrowdMoGen has the potential toadvance applications in urban simulation, crowd planning, and other large-scaleinteractive environments.</description>
      <author>example@mail.com (Yukang Cao, Xinying Guo, Mingyuan Zhang, Haozhe Xie, Chenyang Gu, Ziwei Liu)</author>
      <guid isPermaLink="false">2407.06188v2</guid>
      <pubDate>Mon, 12 May 2025 14:12:33 +0800</pubDate>
    </item>
    <item>
      <title>Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach</title>
      <link>http://arxiv.org/abs/2505.03299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the MORSE workshop of CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型在计算机视觉领域的应用，提出了一个基于“能力编码”的方法来预测模型在多个下游任务上的性能，旨在简化模型选择并提供未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;在地球观测领域，过去四年内开发了超过75个遥感视觉基础模型，但它们在所有任务上并未表现出一致的优越性。&lt;h4&gt;目的&lt;/h4&gt;为了促进模型间的比较，提出了一种成本效益高的方法，用于预测模型在多个下游任务上的性能，而无需对每个任务进行微调。&lt;h4&gt;方法&lt;/h4&gt;该方法基于“能力编码”，旨在简化选择适用于特定新任务的基础模型，并用于分析现有文献，提出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;通过“能力编码”方法，可以简化模型选择，并为现有文献提供新的视角。&lt;h4&gt;结论&lt;/h4&gt;该方法为选择和评估基础模型提供了新的途径，并为未来的研究指明了方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基础模型在计算机视觉领域取得了显著进步：经过一次昂贵但有效的训练阶段后，它们可以处理各种任务。在地球观测领域，过去四年中已经开发了75多个遥感视觉基础模型。然而，没有一个模型在所有可用的下游任务上始终优于其他模型。为了促进它们的比较，我们提出了一种成本效益高的方法，用于预测模型在多个下游任务上的性能，而无需对每个任务进行微调。这种方法基于我们所说的“能力编码”。这种新颖方法的效用有两方面：我们展示了它在简化选择特定新任务的基础模型方面的潜力，并使用它来提供对现有文献的新视角，提出了未来研究的途径。代码可在https://github.com/pierreadorni/capabilities-encoding上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pierreadorni/capabilities-encoding&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models constitute a significant advancement in computer vision:after a single, albeit costly, training phase, they can address a wide array oftasks. In the field of Earth observation, over 75 remote sensing visionfoundation models have been developed in the past four years. However, none hasconsistently outperformed the others across all available downstream tasks. Tofacilitate their comparison, we propose a cost-effective method for predictinga model's performance on multiple downstream tasks without the need forfine-tuning on each one. This method is based on what we call "capabilitiesencoding." The utility of this novel approach is twofold: we demonstrate itspotential to simplify the selection of a foundation model for a given new task,and we employ it to offer a fresh perspective on the existing literature,suggesting avenues for future research. Codes are available athttps://github.com/pierreadorni/capabilities-encoding.</description>
      <author>example@mail.com (Pierre Adorni, Minh-Tan Pham, Stéphane May, Sébastien Lefèvre)</author>
      <guid isPermaLink="false">2505.03299v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
  <item>
      <title>Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization</title>
      <link>http://arxiv.org/abs/2505.05343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is  available at https://github.com/swimmiing/ACL-SSL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉-语言模型在声音源定位中的应用，提出了一种无显式文本输入的自监督方法，通过音频驱动嵌入和对比音频-视觉对应目标，实现了对声音源的高效定位。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉-语言模型在多模态对齐和泛化方面表现出色，其中CLIP模型尤为成功。&lt;h4&gt;目的&lt;/h4&gt;将CLIP模型应用于声音源定位，提出一种无需显式文本输入的自监督方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将音频映射到与CLIP文本编码器兼容的token的框架，生成音频驱动嵌入，用于生成声音区域掩码，并通过对比音频-视觉对应目标提取视觉特征与音频嵌入对齐。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的多模态基础模型的对齐知识使得该方法能够生成更完整和紧凑的声音源定位，进一步通过LLM引导扩展，将对象感知的音频-视觉场景理解提炼到模型中，增强了对齐。&lt;h4&gt;结论&lt;/h4&gt;在五个不同任务上的广泛实验表明，该方法在各种变体中都优于现有方法，并在零样本设置中实现了强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper studies the application of large-scale vision-language models in sound source localization, proposes a self-supervised method without explicit text input, and achieves efficient localization of sound sources through audio-driven embeddings and contrastive audio-visual correspondence objectives.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/swimmiing/ACL-SSL&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale vision-language models demonstrate strong multimodal alignmentand generalization across diverse tasks. Among them, CLIP stands out as one ofthe most successful approaches. In this work, we extend the application of CLIPto sound source localization, proposing a self-supervised method operateswithout explicit text input. We introduce a framework that maps audios intotokens compatible with CLIP's text encoder, producing audio-driven embeddings.These embeddings are used to generate sounding region masks, from which visualfeatures are extracted and aligned with the audio embeddings through acontrastive audio-visual correspondence objective. Our findings show thatalignment knowledge of pre-trained multimodal foundation model enables ourmethod to generate more complete and compact localization for sounding objects.We further propose an LLM-guided extension that distills object-awareaudio-visual scene understanding into the model during training to enhancealignment. Extensive experiments across five diverse tasks demonstrate that ourmethod, in all variants, outperforms state-of-the-art approaches and achievesstrong generalization in zero-shot settings.</description>
      <author>example@mail.com (Sooyoung Park, Arda Senocak, Joon Son Chung)</author>
      <guid isPermaLink="false">2505.05343v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning</title>
      <link>http://arxiv.org/abs/2505.05192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合长期观察数据和短期实验数据来估计长期因果效应的方法，以解决现实场景中因果推断的挑战。&lt;h4&gt;背景&lt;/h4&gt;在现有方法中，通常假设潜在无混淆性或加性等混淆偏倚，但这些假设在现实应用中往往不成立，限制了方法的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需上述假设的长期个体因果效应估计方法。&lt;h4&gt;方法&lt;/h4&gt;利用数据自然异质性，如来自多个来源的数据，来识别潜在混淆因子，从而避免对理想化假设的依赖。具体方法是设计一个基于潜在表示学习的长期因果效应估计器。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析，建立了潜在混淆因子的可识别性，并在此基础上实现了长期效应识别。实验结果表明，该方法在多个合成和半合成数据集上有效。&lt;h4&gt;结论&lt;/h4&gt;该方法在估计长期因果效应时，能够有效避免对理想化假设的依赖，提高了在现实世界中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating long-term causal effects by combining long-term observational andshort-term experimental data is a crucial but challenging problem in manyreal-world scenarios. In existing methods, several ideal assumptions, e.g.latent unconfoundedness assumption or additive equi-confounding biasassumption, are proposed to address the latent confounder problem raised by theobservational data. However, in real-world applications, these assumptions aretypically violated which limits their practical effectiveness. In this paper,we tackle the problem of estimating the long-term individual causal effectswithout the aforementioned assumptions. Specifically, we propose to utilize thenatural heterogeneity of data, such as data from multiple sources, to identifylatent confounders, thereby significantly avoiding reliance on idealizedassumptions. Practically, we devise a latent representation learning-basedestimator of long-term causal effects. Theoretically, we establish theidentifiability of latent confounders, with which we further achieve long-termeffect identification. Extensive experimental studies, conducted on multiplesynthetic and semi-synthetic datasets, demonstrate the effectiveness of ourproposed method.</description>
      <author>example@mail.com (Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo)</author>
      <guid isPermaLink="false">2505.05192v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model</title>
      <link>http://arxiv.org/abs/2505.05397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对路边感知在智能交通系统和车辆到一切（V2X）任务中的应用，提出了一种基于Mamba模型的PillarMamba框架，用于路边点云感知，并通过混合状态空间块（HSB）解决了空间模型在扫描方向限制下的局部连接中断和历史关系遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;路边感知在智能交通系统和车辆到一切（V2X）任务中越来越受到重视，因为它可以扩展连接车辆的感知范围并提高交通安全。然而，基于路边点云的3D目标检测尚未得到有效探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Mamba的PillarMamba框架，用于路边点云感知，并解决空间模型在扫描方向限制下的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入Mamba模型到路边点云感知，并基于交叉阶段状态空间组（CSG）提出PillarMamba框架。通过混合状态空间块（HSB）增强网络的表示能力，并通过交叉阶段特征融合实现高效计算。HSB通过局部卷积增强邻域连接，通过残差注意力保留历史记忆。&lt;h4&gt;主要发现&lt;/h4&gt;PillarMamba在DAIR-V2X-I这个流行的路边大规模数据集上优于现有方法，表明了所提出的方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;PillarMamba框架能够有效地进行路边点云感知，并通过HSB解决了空间模型在扫描方向限制下的局限性，为智能交通系统和V2X任务提供了有效的感知支持。&lt;h4&gt;翻译&lt;/h4&gt;Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything (V2X) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. However, roadside point cloud-oriented 3D object detection has not been effectively explored. To some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. The recent emergence of Mamba, based on State Space Model (SSM), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. In this work, we introduce Mamba to pillar-based roadside point cloud perception and propose a framework based on Cross-stage State-space Group (CSG), called PillarMamba. It enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. However, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. To address this, we propose the Hybrid State-space Block (HSB) to obtain the local-global context of roadside point cloud. Specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. The proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: DAIR-V2X-I. The code will be released soon.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything(V2X) tasks, roadside perception has received increasing attention in recentyears, as it can extend the perception range of connected vehicles and improvetraffic safety. However, roadside point cloud oriented 3D object detection hasnot been effectively explored. To some extent, the key to the performance of apoint cloud detector lies in the receptive field of the network and the abilityto effectively utilize the scene context. The recent emergence of Mamba, basedon State Space Model (SSM), has shaken up the traditional convolution andtransformers that have long been the foundational building blocks, due to itsefficient global receptive field. In this work, we introduce Mamba topillar-based roadside point cloud perception and propose a framework based onCross-stage State-space Group (CSG), called PillarMamba. It enhances theexpressiveness of the network and achieves efficient computation throughcross-stage feature fusion. However, due to the limitations of scan directions,state space model faces local connection disrupted and historical relationshipforgotten. To address this, we propose the Hybrid State-space Block (HSB) toobtain the local-global context of roadside point cloud. Specifically, itenhances neighborhood connections through local convolution and preserveshistorical memory through residual attention. The proposed method outperformsthe state-of-the-art methods on the popular large scale roadside benchmark:DAIR-V2X-I. The code will be released soon.</description>
      <author>example@mail.com (Zhang Zhang, Chao Sun, Chao Yue, Da Wen, Tianze Wang, Jianghao Leng)</author>
      <guid isPermaLink="false">2505.05397v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning</title>
      <link>http://arxiv.org/abs/2505.05208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE IJCNN 2025 has accepted the paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于模糊sigmoid卷积（FSC）的肿瘤检测方法，通过减少可训练参数数量来提高模型性能，同时保持分类精度。&lt;h4&gt;背景&lt;/h4&gt;早期检测和准确诊断对于提高患者预后至关重要，卷积神经网络（CNN）在肿瘤检测中显示出潜力，但现有模型往往存在过参数化问题。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种高效、准确且参数较少的肿瘤检测模型。&lt;h4&gt;方法&lt;/h4&gt;提出的方法包括模糊sigmoid卷积（FSC）、漏斗顶部模块和漏斗中部模块。FSC通过有效的感受野扩展和保持输入数据完整性，实现高效的特征图减少和模型肿瘤检测能力提升。模糊sigmoid激活函数被引入卷积层以提高特征提取和分类。&lt;h4&gt;主要发现&lt;/h4&gt;FSC模型在三个基准数据集上实现了99.17%、99.75%和99.89%的分类精度，参数数量比大规模迁移学习架构少100倍。&lt;h4&gt;结论&lt;/h4&gt;FSC模型是一种高效、轻量级的深度学习模型，适用于医学影像应用中的脑肿瘤早期检测。&lt;h4&gt;翻译&lt;/h4&gt;摘要：早期检测和准确诊断对于改善患者预后至关重要。卷积神经网络（CNN）在肿瘤检测中显示出希望，但现有模型通常受到过参数化的限制，这限制了其性能提升。在本研究中，引入了模糊sigmoid卷积（FSC）以及两个附加模块：漏斗顶部和中部。所提出的方法显著减少了可训练参数的数量，而没有损害分类精度。这种方法的核心是一个新颖的卷积算子，它有效地扩展了感受野同时保持输入数据完整性。这使高效的特征图减少和增强模型肿瘤检测能力成为可能。在基于FSC的模型中，模糊sigmoid激活函数被整合到卷积层中，以改进特征提取和分类。将模糊逻辑纳入架构提高了其适应性和鲁棒性。在三个基准数据集上进行的广泛实验表明了所提出模型的优越性能和效率。基于FSC的架构在三个不同的数据集上达到了99.17%、99.75%和99.89%的分类精度。该模型使用的参数数量比大规模迁移学习架构少100倍，突出了其计算效率和适用于早期检测脑肿瘤的适用性。这项研究为医学影像应用提供了轻量级、高性能的深度学习模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection and accurate diagnosis are essential to improving patientoutcomes. The use of convolutional neural networks (CNNs) for tumor detectionhas shown promise, but existing models often suffer from overparameterization,which limits their performance gains. In this study, fuzzy sigmoid convolution(FSC) is introduced along with two additional modules: top-of-the-funnel andmiddle-of-the-funnel. The proposed methodology significantly reduces the numberof trainable parameters without compromising classification accuracy. A novelconvolutional operator is central to this approach, effectively dilating thereceptive field while preserving input data integrity. This enables efficientfeature map reduction and enhances the model's tumor detection capability. Inthe FSC-based model, fuzzy sigmoid activation functions are incorporated withinconvolutional layers to improve feature extraction and classification. Theinclusion of fuzzy logic into the architecture improves its adaptability androbustness. Extensive experiments on three benchmark datasets demonstrate thesuperior performance and efficiency of the proposed model. The FSC-basedarchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%on three different datasets. The model employs 100 times fewer parameters thanlarge-scale transfer learning architectures, highlighting its computationalefficiency and suitability for detecting brain tumors early. This researchoffers lightweight, high-performance deep-learning models for medical imagingapplications.</description>
      <author>example@mail.com (Muhammad Irfan, Anum Nawaz, Riku Klen, Abdulhamit Subasi, Tomi Westerlund, Wei Chen)</author>
      <guid isPermaLink="false">2505.05208v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Generalization Analysis for Contrastive Representation Learning under Non-IID Settings</title>
      <link>http://arxiv.org/abs/2505.04937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To Appear in ICML, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对比表示学习（CRL）的泛化行为，并针对非独立同分布（non-$i.i.d.$）数据集提供了泛化分析。&lt;h4&gt;背景&lt;/h4&gt;尽管CRL在各种领域取得了显著成功，但其泛化行为的理论理解有限，且现有文献仅分析了在数据元对独立同分布假设下的泛化界限。&lt;h4&gt;目的&lt;/h4&gt;本文旨在对CRL框架在非独立同分布设置下的泛化行为进行理论分析，并提供更符合实际情况的泛化界限。&lt;h4&gt;方法&lt;/h4&gt;本文借鉴U统计学的文献，推导了泛化界限，表明每个类中所需样本的数量与每个类关联的可学习特征表示类的覆盖数的对数成比例。&lt;h4&gt;主要发现&lt;/h4&gt;本文推导了针对线性映射和神经网络等常见函数类的过量风险界限。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为CRL在非独立同分布数据集上的应用提供了理论支持，有助于理解和优化CRL模型的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL is limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to each class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Representation Learning (CRL) has achieved impressive success invarious domains in recent years. Nevertheless, the theoretical understanding ofthe generalization behavior of CRL is limited. Moreover, to the best of ourknowledge, the current literature only analyzes generalization bounds under theassumption that the data tuples used for contrastive learning are independentlyand identically distributed. However, in practice, we are often limited to afixed pool of reusable labeled data points, making it inevitable to recycledata across tuples to create sufficiently large datasets. Therefore, thetuple-wise independence condition imposed by previous works is invalidated. Inthis paper, we provide a generalization analysis for the CRL framework undernon-$i.i.d.$ settings that adheres to practice more realistically. Drawinginspiration from the literature on U-statistics, we derive generalizationbounds which indicate the required number of samples in each class scales asthe logarithm of the covering number of the class of learnable featurerepresentations associated to each class. Next, we apply our main results toderive excess risk bounds for common function classes such as linear maps andneural networks.</description>
      <author>example@mail.com (Nong Minh Hieu, Antoine Ledent)</author>
      <guid isPermaLink="false">2505.04937v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PADriver: Towards Personalized Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.05240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PADriver的新型闭环框架，用于个性化自动驾驶（PAD）。该框架基于多模态大型语言模型（MLLM），以流式帧和个人化文本提示为输入，自动执行场景理解、危险水平估计和行动决策。&lt;h4&gt;背景&lt;/h4&gt;目前自动驾驶领域的研究主要集中在基于规则的系统和基于数据的方法，但缺乏对个性化需求的考虑。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种能够根据个性化需求进行自动驾驶的框架，提高自动驾驶系统的适应性和安全性。&lt;h4&gt;方法&lt;/h4&gt;PADriver使用多模态大型语言模型处理输入数据，自动进行场景理解、危险水平估计和行动决策。同时，构建了基于Highway-Env模拟器的闭环基准PAD-Highway，以评估系统在遵守交通规则下的决策性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PADriver在构建的基准上优于现有方法，并支持多种驾驶模式。&lt;h4&gt;结论&lt;/h4&gt;PADriver能够有效提高个性化自动驾驶系统的性能，为自动驾驶技术的发展提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为PADriver的新型闭环框架，用于个性化自动驾驶（PAD）。该框架基于多模态大型语言模型（MLLM），以流式帧和个人化文本提示为输入，自动执行场景理解、危险水平估计和行动决策。实验结果表明，PADriver在构建的基准上优于现有方法，并支持多种驾驶模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose PADriver, a novel closed-loop framework forpersonalized autonomous driving (PAD). Built upon Multi-modal Large LanguageModel (MLLM), PADriver takes streaming frames and personalized textual promptsas inputs. It autoaggressively performs scene understanding, danger levelestimation and action decision. The predicted danger level reflects the risk ofthe potential action and provides an explicit reference for the final action,which corresponds to the preset personalized prompt. Moreover, we construct aclosed-loop benchmark named PAD-Highway based on Highway-Env simulator tocomprehensively evaluate the decision performance under traffic rules. Thedataset contains 250 hours videos with high-quality annotation to facilitatethe development of PAD behavior analysis. Experimental results on theconstructed benchmark show that PADriver outperforms state-of-the-artapproaches on different evaluation metrics, and enables various driving modes.</description>
      <author>example@mail.com (Genghua Kou, Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Ziheng Zhang, Osamu Yoshie, Tiancai Wang, Ying Li, Xiangyu Zhang)</author>
      <guid isPermaLink="false">2505.05240v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
      <link>http://arxiv.org/abs/2505.05181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Stochastic Variational Propagation（SVP）的深度学习算法，作为反向传播（BP）的替代方案，旨在提高深度学习的可扩展性和降低内存开销。&lt;h4&gt;背景&lt;/h4&gt;反向传播在深度学习中至关重要，但其依赖于全局梯度同步，限制了可扩展性并增加了内存开销。&lt;h4&gt;目的&lt;/h4&gt;提出SVP算法，通过将训练重新定义为分层变分推理，以实现可扩展性和降低内存开销。&lt;h4&gt;方法&lt;/h4&gt;SVP将层激活视为潜在变量，并优化局部证据下界（ELBOs），允许独立、局部的更新同时保持全局一致性。为了避免层间表示崩溃，SVP通过固定的随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。&lt;h4&gt;主要发现&lt;/h4&gt;SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上与BP实现了具有竞争力的精度，将内存使用量降低了多达4倍，并显著提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;SVP引入了概率视角到深度表示学习，为更模块化和可解释的神经网络设计开辟了途径。&lt;h4&gt;翻译&lt;/h4&gt;Backpropagation (BP) 是深度学习的基础，但其对全局梯度同步的依赖限制了可扩展性并造成了显著的内存开销。我们提出了Stochastic Variational Propagation (SVP)，一种可扩展的替代方案，将训练重新定义为分层变分推理。SVP将层激活视为潜在变量，并优化局部证据下界 (ELBOs)，允许独立、局部的更新同时保持全局一致性。然而，直接在层间ELBOs中应用KL散度可能会由于过度压缩而导致层间表示崩溃。为了防止这种情况，SVP通过固定的随机矩阵将激活投影到低维空间，确保信息保留和表示多样性。结合用于层间一致性的特征对齐损失，SVP在多种架构（MLPs、CNNs、Transformers）和数据集（MNIST到ImageNet）上实现了与BP具有竞争力的精度，将内存使用量降低了多达4倍，并显著提高了可扩展性。更广泛地说，SVP为深度表示学习引入了概率视角，为更模块化和可解释的神经网络设计开辟了途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backpropagation (BP) is the cornerstone of deep learning, but its reliance onglobal gradient synchronization limits scalability and imposes significantmemory overhead. We propose Stochastic Variational Propagation (SVP), ascalable alternative that reframes training as hierarchical variationalinference. SVP treats layer activations as latent variables and optimizes localEvidence Lower Bounds (ELBOs), enabling independent, local updates whilepreserving global coherence. However, directly applying KL divergence inlayer-wise ELBOs risks inter-layer's representation collapse due to excessivecompression. To prevent this, SVP projects activations into low-dimensionalspaces via fixed random matrices, ensuring information preservation andrepresentational diversity. Combined with a feature alignment loss forinter-layer consistency, SVP achieves competitive accuracy with BP acrossdiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST toImageNet), reduces memory usage by up to 4x, and significantly improvesscalability. More broadly, SVP introduces a probabilistic perspective to deeprepresentation learning, opening pathways toward more modular and interpretableneural network design.</description>
      <author>example@mail.com (Bojian Yin, Federico Corradi)</author>
      <guid isPermaLink="false">2505.05181v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP通过预测链（CoP）和三个关键设计，提高了单目3D物体检测的深度估计准确性。&lt;h4&gt;背景&lt;/h4&gt;深度估计在单目3D物体检测中是一个挑战，因为2D图像到3D空间的映射存在固有模糊性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法MonoCoP，以解决深度估计问题并提高整体准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP使用轻量级属性网络（AN）学习每个3D属性的特征，构建一个显式的预测链来传播这些特征，并使用残差连接聚合特征。&lt;h4&gt;主要发现&lt;/h4&gt;MonoCoP在KITTI、Waymo和nuScenes数据集上实现了最先进的性能，且无需额外数据。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP通过条件预测和特征传播，提高了单目3D物体检测的深度估计准确性。&lt;h4&gt;翻译&lt;/h4&gt;Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects</title>
      <link>http://arxiv.org/abs/2505.04962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE/RSJ IROS 2022 Workshop on Mobile Manipulation and  Embodied Intelligence (MOMA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种系统，用于从有序或无序堆积中自主精确地拾取立方体物体。该系统采用了一种高效的方法来估计立方体物体的姿态，旨在以时间高效的方式减少目标姿态误差。&lt;h4&gt;背景&lt;/h4&gt;传统的姿态估计方法，如全局点云注册，容易产生微小的姿态误差。为了提高姿态精度，通常使用局部注册算法，但这些方法存在执行时间开销和最终姿态误差不确定的问题。&lt;h4&gt;目的&lt;/h4&gt;降低目标姿态误差，并以时间高效的方式进行姿态估计和校正。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的线性时间方法来估计和校正姿态误差，并详细描述了算法的各个模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过新的线性时间方法，可以在保持姿态估计精度的同时，减少执行时间和提高姿态校正的准确性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地减少姿态估计误差，适用于立方体物体的自主拾取场景。&lt;h4&gt;翻译&lt;/h4&gt;The proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. This paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. Typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. However, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. This paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proposed system outlined in this paper is a solution to a use case thatrequires the autonomous picking of cuboidal objects from an organized orunorganized pile with high precision. This paper presents an efficient methodfor precise pose estimation of cuboid-shaped objects, which aims to reduceerrors in target pose in a time-efficient manner. Typical pose estimationmethods like global point cloud registrations are prone to minor pose errorsfor which local registration algorithms are generally used to improve poseaccuracy. However, due to the execution time overhead and uncertainty in theerror of the final achieved pose, an alternate, linear time approach isproposed for pose error estimation and correction. This paper presents anoverview of the solution followed by a detailed description of individualmodules of the proposed algorithm.</description>
      <author>example@mail.com (Utsav Rai, Hardik Mehta, Vismay Vakharia, Aditya Choudhary, Amit Parmar, Rolif Lima, Kaushik Das)</author>
      <guid isPermaLink="false">2505.04962v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow</title>
      <link>http://arxiv.org/abs/2505.05089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025. Project Page:  https://wynelio.github.io/E-NMSTFlow&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为E-NMSTFlow的新型无监督事件驱动光流网络，旨在提高长时间序列中的光流估计准确性。&lt;h4&gt;背景&lt;/h4&gt;现有基于学习的事件光流方法多采用基于帧的技术，忽略了事件的空间时间特性，并假设事件间的运动是线性的，这导致长时间序列中的光流误差增加。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文旨在通过利用丰富的时间和空间信息以及事件间的非线性运动，提高事件驱动光流估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种Spatio-Temporal Motion Feature Aware (STMFA)模块和一种Adaptive Motion Feature Enhancement (AMFE)模块，这两个模块都利用丰富的时空信息来学习时空数据关联。同时，提出了一种非线性运动补偿损失函数，利用事件间的准确非线性运动来提高网络的无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在MVSEC和DSEC-Flow数据集上优于其他无监督学习方法，排名第一。&lt;h4&gt;结论&lt;/h4&gt;E-NMSTFlow网络在长时间序列光流估计方面表现出色，为事件驱动光流估计提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. However, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. Additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. In this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel unsupervised event-based optical flow network focusing on long-time sequences. We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an Adaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. Meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. Extensive experiments demonstrate the effectiveness and superiority of our method. Remarkably, our method ranks first among unsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project page is available at https://wynelio.github.io/E-NMSTFlow.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras have the potential to capture continuous motion informationover time and space, making them well-suited for optical flow estimation.However, most existing learning-based methods for event-based optical flowadopt frame-based techniques, ignoring the spatio-temporal characteristics ofevents. Additionally, these methods assume linear motion between consecutiveevents within the loss time window, which increases optical flow errors inlong-time sequences. In this work, we observe that rich spatio-temporalinformation and accurate nonlinear motion between events are crucial forevent-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novelunsupervised event-based optical flow network focusing on long-time sequences.We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and anAdaptive Motion Feature Enhancement (AMFE) module, both of which utilize richspatio-temporal information to learn spatio-temporal data associations.Meanwhile, we propose a nonlinear motion compensation loss that utilizes theaccurate nonlinear motion between events to improve the unsupervised learningof our network. Extensive experiments demonstrate the effectiveness andsuperiority of our method. Remarkably, our method ranks first amongunsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our projectpage is available at https://wynelio.github.io/E-NMSTFlow.</description>
      <author>example@mail.com (Zuntao Liu, Hao Zhuang, Junjie Jiang, Yuhang Song, Zheng Fang)</author>
      <guid isPermaLink="false">2505.05089v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks</title>
      <link>http://arxiv.org/abs/2505.04981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的深度强化学习（DRL）算法，用于动态THz无人机网络中的资源分配，以最大化资源效率。&lt;h4&gt;背景&lt;/h4&gt;THz无人机网络具有灵活的拓扑结构和超高速数据传输能力，在安全监控、灾害响应和环境保护等领域具有广泛应用前景。然而，动态拓扑结构给无人机之间THz链路的长期联合功率和天线阵列资源分配带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出GLOVE算法，通过GNN学习无人机与其邻近无人机之间的关系，同时强调无人机自身的特征，以实现资源效率的最大化。&lt;h4&gt;方法&lt;/h4&gt;GLOVE算法通过图神经网络（GNN）学习无人机与其邻近无人机之间的交互关系，并利用多任务结构协同训练所有无人机功率和子阵列的资源分配决策。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GLOVE算法在资源效率最高和延迟最低方面优于基准方案，并且在整个训练过程中保持了零数据包丢失，显示出其在高度动态THz无人机网络中的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GLOVE算法能够有效解决动态THz无人机网络中的资源分配问题，为无人机网络的应用提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexibletopologies and ultra-high data rates are expected to empower numerousapplications in security surveillance, disaster response, and environmentalmonitoring, among others. However, the dynamic topologies hinder the efficientlong-term joint power and antenna array resource allocation for THz links amongUAVs. Furthermore, the continuous nature of power and the discrete nature ofantennas cause this joint resource allocation problem to be a mixed-integernonlinear programming (MINLP) problem with non-convexity and NP-hardness.Inspired by recent rapid advancements in deep reinforcement learning (DRL), agraph neural network (GNN) aided DRL algorithm for resource allocation in thedynamic THz UAV network with an emphasis on self-node features (GLOVE) isproposed in this paper, with the aim of resource efficiency (RE) maximization.When training the allocation policy for each UAV, GLOVE learns the relationshipbetween this UAV and its neighboring UAVs via GNN, while also emphasizing theimportant self-node features of this UAV. In addition, a multi-task structureis leveraged by GLOVE to cooperatively train resource allocation decisions forthe power and sub-arrays of all UAVs. Experimental results illustrate thatGLOVE outperforms benchmark schemes in terms of the highest RE and the lowestlatency. Moreover, unlike the benchmark methods with severe packet loss, GLOVEmaintains zero packet loss during the entire training process, demonstratingits better robustness under the highly dynamic THz UAV network.</description>
      <author>example@mail.com (Zhifeng Hu, Chong Han)</author>
      <guid isPermaLink="false">2505.04981v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant</title>
      <link>http://arxiv.org/abs/2505.05467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamBridge是一种将离线视频语言模型（Video-LLMs）转换为流式模型的简单而有效的框架。&lt;h4&gt;背景&lt;/h4&gt;将现有模型应用于在线场景时，存在两个主要挑战：多轮实时理解和缺乏主动响应机制。&lt;h4&gt;目的&lt;/h4&gt;StreamBridge旨在解决上述挑战，提高离线视频语言模型在流式场景下的理解能力。&lt;h4&gt;方法&lt;/h4&gt;StreamBridge包含以下特点：(1) 结合内存缓冲区和圆周衰减压缩策略，支持长上下文的多轮交互；(2) 解耦的轻量级激活模型，易于集成到现有的Video-LLMs中，实现连续的主动响应。同时，构建了大规模数据集Stream-IT，用于流式视频理解。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，StreamBridge在多种任务中显著提高了离线视频语言模型的流式理解能力，甚至在某些方面超过了GPT-4o和Gemini1.5 Pro等专有模型。在标准视频理解基准测试中，它也实现了有竞争力的或优越的性能。&lt;h4&gt;结论&lt;/h4&gt;StreamBridge是一个有效的框架，可以显著提升离线视频语言模型的流式理解能力，并在多个任务中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present StreamBridge, a simple yet effective framework that seamlesslytransforms offline Video-LLMs into streaming-capable models. It addresses twofundamental challenges in adapting existing models into online scenarios: (1)limited capability for multi-turn real-time understanding, and (2) lack ofproactive response mechanisms. Specifically, StreamBridge incorporates (1) amemory buffer combined with a round-decayed compression strategy, supportinglong-context multi-turn interactions, and (2) a decoupled, lightweightactivation model that can be effortlessly integrated into existing Video-LLMs,enabling continuous proactive responses. To further support StreamBridge, weconstruct Stream-IT, a large-scale dataset tailored for streaming videounderstanding, featuring interleaved video-text sequences and diverseinstruction formats. Extensive experiments show that StreamBridge significantlyimproves the streaming understanding capabilities of offline Video-LLMs acrossvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini1.5 Pro. Simultaneously, it achieves competitive or superior performance onstandard video understanding benchmarks.</description>
      <author>example@mail.com (Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge, Afshin Dehghan, Meng Cao, Ping Huang)</author>
      <guid isPermaLink="false">2505.05467v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.04907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为VaCDA的多源域适应框架，旨在解决可穿戴设备产生的海量未标注数据带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着科技的发展，可穿戴设备中的传感器持续监控用户活动，生成大量未标注数据。这些数据难以解释，手动标注既费时又易出错。数据分布因设备放置、类型和使用行为的不同而异，传统迁移学习方法效果不佳，难以识别日常活动。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，VaCDA结合了变分自编码器（VAE）和对比学习，以改善特征表示并减少源域和目标域之间的异质性。&lt;h4&gt;方法&lt;/h4&gt;VaCDA使用VAE从可用传感器数据中学习一个共享的低维潜在空间，该空间可以泛化不同传感器之间的数据。同时，通过对比学习，将同一类别的实例在域间对齐，同时分离不同类别，以增强特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公开数据集上，VaCDA在跨位置和跨设备场景中优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;VaCDA在处理可穿戴设备数据方面表现出色，特别是在解决数据异质性和提升特征表示方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：技术进步导致了带有传感器的可穿戴设备的兴起，这些设备持续监控用户活动，生成大量未标记数据。这些数据难以解释，手动标注既费力又容易出错。由于设备放置、类型和使用行为的差异，数据分布往往异质。因此，传统的迁移学习方法表现不佳，难以识别日常活动。为了解决这些挑战，我们使用变分自编码器（VAE）从可用的传感器数据中学习一个共享的低维潜在空间。这个空间可以泛化不同传感器之间的数据，减轻异质性，并帮助目标域的鲁棒适应。我们通过对比学习来增强特征表示，通过对齐跨域的同一类实例同时分离不同类别。我们提出了变分对比域适应（VaCDA），这是一个结合VAE和对比学习的多源域适应框架，旨在改善特征表示并减少源域和目标域之间的异质性。我们在三个异质场景（跨个人、跨位置和跨设备）的多个公开数据集上评估了VaCDA。在跨位置和跨设备场景中，VaCDA优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Technological advancements have led to the rise of wearable devices withsensors that continuously monitor user activities, generating vast amounts ofunlabeled data. This data is challenging to interpret, and manual annotation islabor-intensive and error-prone. Additionally, data distribution is oftenheterogeneous due to device placement, type, and user behavior variations. As aresult, traditional transfer learning methods perform suboptimally, making itdifficult to recognize daily activities. To address these challenges, we use avariational autoencoder (VAE) to learn a shared, low-dimensional latent spacefrom available sensor data. This space generalizes data across diverse sensors,mitigating heterogeneity and aiding robust adaptation to the target domain. Weintegrate contrastive learning to enhance feature representation by aligninginstances of the same class across domains while separating different classes.We propose Variational Contrastive Domain Adaptation (VaCDA), a multi-sourcedomain adaptation framework combining VAEs and contrastive learning to improvefeature representation and reduce heterogeneity between source and targetdomains. We evaluate VaCDA on multiple publicly available datasets across threeheterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDAoutperforms the baselines in cross-position and cross-device scenarios.</description>
      <author>example@mail.com (Soham Khisa, Avijoy Chakma)</author>
      <guid isPermaLink="false">2505.04907v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction</title>
      <link>http://arxiv.org/abs/2505.05094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究旨在通过结合联合图学习和网络分析来解决高血压共病问题的早期识别挑战。&lt;h4&gt;背景&lt;/h4&gt;高血压的共病给患者和社会带来了沉重的负担，早期识别对于及时干预至关重要，但这一任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一个名为Conjoint Graph Representation Learning (CGRL)的框架，以预测糖尿病和冠心病患者的风险。&lt;h4&gt;方法&lt;/h4&gt;该方法包括：a) 基于疾病编码构建两个网络，包括患者网络和疾病差异网络；b) 生成三个共病网络特征以捕捉共病与风险疾病之间的潜在关系；c) 结合计算结构干预和学习特征表示，以预测糖尿病和冠心病患者的风险。&lt;h4&gt;主要发现&lt;/h4&gt;基于差异网络提取的网络特征非常重要，所提出的框架在准确性方面比其他强大模型提供了更准确的预测。&lt;h4&gt;结论&lt;/h4&gt;CGRL框架有助于揭示糖尿病和冠心病的共病模式和疾病进展途径，并可能揭示其病理发病机制。&lt;h4&gt;翻译&lt;/h4&gt;本研究旨在通过结合联合图学习与网络分析解决高血压共病问题的早期识别挑战。研究背景指出，高血压的共病给患者和社会带来了沉重的负担，早期识别对于及时干预至关重要，但这一任务仍然具有挑战性。研究目的在于开发一个名为Conjoint Graph Representation Learning (CGRL)的框架，以预测糖尿病和冠心病患者的风险。研究方法包括基于疾病编码构建患者网络和疾病差异网络，生成共病网络特征，并结合计算结构干预和学习特征表示。主要发现表明，基于差异网络提取的网络特征非常重要，所提出的框架在准确性方面比其他强大模型提供了更准确的预测。研究结论指出，CGRL框架有助于揭示糖尿病和冠心病的共病模式和疾病进展途径，并可能揭示其病理发病机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The comorbidities of hypertension impose a heavy burden on patients andsociety. Early identification is necessary to prompt intervention, but itremains a challenging task. This study aims to address this challenge bycombining joint graph learning with network analysis. Motivated by thisdiscovery, we develop a Conjoint Graph Representation Learning (CGRL) frameworkthat: a) constructs two networks based on disease coding, including the patientnetwork and the disease difference network. Three comorbidity network featureswere generated based on the basic difference network to capture the potentialrelationship between comorbidities and risk diseases; b) incorporatescomputational structure intervention and learning feature representation, CGRLwas developed to predict the risks of diabetes and coronary heart disease inpatients; and c) analysis the comorbidity patterns and exploring the pathwaysof disease progression, the pathological pathogenesis of diabetes and coronaryheart disease may be revealed. The results show that the network featuresextracted based on the difference network are important, and the framework weproposed provides more accurate predictions than other strong models in termsof accuracy.</description>
      <author>example@mail.com (Leming Zhou, Zuo Wang, Zhixuan Duan)</author>
      <guid isPermaLink="false">2505.05094v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Does CLIP perceive art the same way we do?</title>
      <link>http://arxiv.org/abs/2505.05229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了CLIP模型在提取艺术作品的高层次语义和风格信息方面的能力，并评估了其在内容、场景理解、艺术风格、历史时期以及视觉变形或伪影等方面的感知能力，同时探讨了其与人类感知和上下文理解的一致性。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一个能够通过联合嵌入连接图像和文本的强大多模态模型。&lt;h4&gt;目的&lt;/h4&gt;研究CLIP在提取绘画（包括人类和AI生成的图像）中的高层次语义和风格信息的能力。&lt;h4&gt;方法&lt;/h4&gt;设计针对性的探测任务，比较CLIP的响应与人类标注和专家基准，以探索其与人类感知和上下文理解的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了CLIP在视觉表示中的优势和局限性，特别是在与美学线索和艺术意图相关方面。&lt;h4&gt;结论&lt;/h4&gt;讨论了这些发现对于将CLIP用作生成过程（如风格迁移或基于提示的图像合成）中的指导机制的影响，并强调了在多模态系统中进行更深层次可解释性的必要性，特别是在应用于以细微差别和主观性为中心的创意领域时。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了CLIP模型在提取艺术作品的高层次语义和风格信息方面的能力，并评估了其在内容、场景理解、艺术风格、历史时期以及视觉变形或伪影等方面的感知能力。通过设计针对性的探测任务，比较CLIP的响应与人类标注和专家基准，探讨了其与人类感知和上下文理解的一致性。研究揭示了CLIP在视觉表示中的优势和局限性，特别是在与美学线索和艺术意图相关方面。进一步讨论了这些发现对于将CLIP用作生成过程中的指导机制的影响，并强调了在多模态系统中进行更深层次可解释性的必要性，特别是在应用于以细微差别和主观性为中心的创意领域时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CLIP has emerged as a powerful multimodal model capable of connecting imagesand text through joint embeddings, but to what extent does it "see" the sameway humans do - especially when interpreting artworks? In this paper, weinvestigate CLIP's ability to extract high-level semantic and stylisticinformation from paintings, including both human-created and AI-generatedimagery. We evaluate its perception across multiple dimensions: content, sceneunderstanding, artistic style, historical period, and the presence of visualdeformations or artifacts. By designing targeted probing tasks and comparingCLIP's responses to human annotations and expert benchmarks, we explore itsalignment with human perceptual and contextual understanding. Our findingsreveal both strengths and limitations in CLIP's visual representations,particularly in relation to aesthetic cues and artistic intent. We furtherdiscuss the implications of these insights for using CLIP as a guidancemechanism during generative processes, such as style transfer or prompt-basedimage synthesis. Our work highlights the need for deeper interpretability inmultimodal systems, especially when applied to creative domains where nuanceand subjectivity play a central role.</description>
      <author>example@mail.com (Andrea Asperti, Leonardo Dessì, Maria Chiara Tonetti, Nico Wu)</author>
      <guid isPermaLink="false">2505.05229v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
      <link>http://arxiv.org/abs/2505.04846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted at the Platform for Advanced Scientific  Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HiPerRAG的RAG工作流程，通过高性能计算技术索引和检索超过360万篇科学文献中的知识，旨在解决科学文献量增长带来的问题。&lt;h4&gt;背景&lt;/h4&gt;科学文献量呈指数增长，导致发现未被充分利用、重复工作和跨学科合作受限。&lt;h4&gt;目的&lt;/h4&gt;通过提高大语言模型处理信息的事实性来协助科学家。&lt;h4&gt;方法&lt;/h4&gt;HiPerRAG利用高性能计算，包括Oreo模型进行多模态文档解析和ColTrast查询感知编码器算法来增强检索准确性。&lt;h4&gt;主要发现&lt;/h4&gt;HiPerRAG在现有的科学问答基准和两个新基准上表现出色，SciQ准确率达到90%，PubMedQA准确率达到76%，超过了PubMedGPT和GPT-4等特定领域的模型和商业LLM。&lt;h4&gt;结论&lt;/h4&gt;HiPerRAG在Polaris、Sunspot和Frontier超级计算机上扩展到数千个GPU，提供了百万文档规模的RAG工作流程，以统一科学知识和促进跨学科创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3732775.3733586&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The volume of scientific literature is growing exponentially, leading tounderutilized discoveries, duplicated efforts, and limited cross-disciplinarycollaboration. Retrieval Augmented Generation (RAG) offers a way to assistscientists by improving the factuality of Large Language Models (LLMs) inprocessing this influx of information. However, scaling RAG to handle millionsof articles introduces significant challenges, including the high computationalcosts associated with parsing documents and embedding scientific knowledge, aswell as the algorithmic complexity of aligning these representations with thenuanced semantics of scientific content. To address these issues, we introduceHiPerRAG, a RAG workflow powered by high performance computing (HPC) to indexand retrieve knowledge from more than 3.6 million scientific articles. At itscore are Oreo, a high-throughput model for multimodal document parsing, andColTrast, a query-aware encoder fine-tuning algorithm that enhances retrievalaccuracy by using contrastive learning and late-interaction techniques.HiPerRAG delivers robust performance on existing scientific question answeringbenchmarks and two new benchmarks introduced in this work, achieving 90%accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific modelslike PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUson the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers milliondocument-scale RAG workflows for unifying scientific knowledge and fosteringinterdisciplinary innovation.</description>
      <author>example@mail.com (Ozan Gokdemir, Carlo Siebenschuh, Alexander Brace, Azton Wells, Brian Hsu, Kyle Hippe, Priyanka V. Setty, Aswathy Ajith, J. Gregory Pauloski, Varuni Sastry, Sam Foreman, Huihuo Zheng, Heng Ma, Bharat Kale, Nicholas Chia, Thomas Gibbs, Michael E. Papka, Thomas Brettin, Francis J. Alexander, Anima Anandkumar, Ian Foster, Rick Stevens, Venkatram Vishwanath, Arvind Ramanathan)</author>
      <guid isPermaLink="false">2505.04846v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</title>
      <link>http://arxiv.org/abs/2505.05001v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:  text overlap with arXiv:2403.06378&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的视频拼接框架StabStitch++，用于解决视频拼接中出现的扭曲抖动问题，并通过实验证明其在性能、鲁棒性和效率方面优于现有解决方案。&lt;h4&gt;背景&lt;/h4&gt;视频拼接过程中，由于图像扭曲不连续，即使输入视频稳定，拼接后的视频也可能出现扭曲抖动，影响视觉体验。&lt;h4&gt;目的&lt;/h4&gt;提出一种视频拼接框架，实现空间拼接和时序稳定性的同时优化。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一个可微分的双向分解模块，将单应性变换解耦并融入空间扭曲中，平衡两个视图的对齐负担和投影畸变。2. 从视频稳定中的摄像机路径得到视频拼接轨迹的数学表达式。3. 提出了一个扭曲平滑模型，通过混合损失函数同时促进内容对齐、轨迹平滑和在线协作。&lt;h4&gt;主要发现&lt;/h4&gt;StabStitch++在在线模式下同时优化了对齐和稳定性，优于牺牲对齐以换取稳定性的StabStitch。&lt;h4&gt;结论&lt;/h4&gt;StabStitch++通过构建实时在线视频拼接系统，在视频拼接性能、鲁棒性和效率方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nie-lang/stabstitch2&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We retarget video stitching to an emerging issue, named warping shake, whichunveils the temporal content shakes induced by sequentially unsmooth warps whenextending image stitching to video stitching. Even if the input videos arestable, the stitched video can inevitably cause undesired warping shakes andaffect the visual experience. To address this issue, we propose StabStitch++, anovel video stitching framework to realize spatial stitching and temporalstabilization with unsupervised learning simultaneously. First, different fromexisting learning-based image stitching solutions that typically warp one imageto align with another, we suppose a virtual midplane between original imageplanes and project them onto it. Concretely, we design a differentiablebidirectional decomposition module to disentangle the homography transformationand incorporate it into our spatial warp, evenly spreading alignment burdensand projective distortions across two views. Then, inspired by camera paths invideo stabilization, we derive the mathematical expression of stitchingtrajectories in video stitching by elaborately integrating spatial and temporalwarps. Finally, a warp smoothing model is presented to produce stable stitchedvideos with a hybrid loss to simultaneously encourage content alignment,trajectory smoothness, and online collaboration. Compared with StabStitch thatsacrifices alignment for stabilization, StabStitch++ makes no compromise andoptimizes both of them simultaneously, especially in the online mode. Toestablish an evaluation benchmark and train the learning framework, we build avideo stitching dataset with a rich diversity in camera motions and scenes.Experiments exhibit that StabStitch++ surpasses current solutions in stitchingperformance, robustness, and efficiency, offering compelling advancements inthis field by building a real-time online video stitching system.</description>
      <author>example@mail.com (Lang Nie, Chunyu Lin, Kang Liao, Yun Zhang, Shuaicheng Liu, Yao Zhao)</author>
      <guid isPermaLink="false">2505.05001v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
      <link>http://arxiv.org/abs/2505.05472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Mogao Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Mogao，一个通过因果方法实现交错多模态生成的统一框架，在图像理解和生成方面取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;当前统一模型在图像理解和生成方面取得了一定的进步，但大多数方法仍然局限于基于多模态的单模态生成。&lt;h4&gt;目的&lt;/h4&gt;提出Mogao框架，通过交错多模态生成来推进这一范式。&lt;h4&gt;方法&lt;/h4&gt;Mogao集成了架构设计方面的多项关键技术改进，包括深度融合设计、双视觉编码器、交错旋转位置嵌入和多模态无分类器引导，以及在大规模内部数据集上的高效训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;Mogao在多模态理解和文本到图像生成方面达到了最先进的性能，并且在产生高质量、连贯的交错输出方面表现出色。它在零样本图像编辑和组合生成方面的能力突显了Mogao作为一个实用的全模态基础模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;Mogao作为一个实用的全模态基础模型，为未来统一多模态系统的发展铺平了道路，并有望进一步扩展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in unified models for image understanding and generation hasbeen impressive, yet most approaches remain limited to single-modal generationconditioned on multiple modalities. In this paper, we present Mogao, a unifiedframework that advances this paradigm by enabling interleaved multi-modalgeneration through a causal approach. Mogao integrates a set of key technicalimprovements in architecture design, including a deep-fusion design, dualvision encoders, interleaved rotary position embeddings, and multi-modalclassifier-free guidance, which allow it to harness the strengths of bothautoregressive models for text generation and diffusion models for high-qualityimage synthesis. These practical improvements also make Mogao particularlyeffective to process interleaved sequences of text and images arbitrarily. Tofurther unlock the potential of unified models, we introduce an efficienttraining strategy on a large-scale, in-house dataset specifically curated forjoint text and image generation. Extensive experiments show that Mogao not onlyachieves state-of-the-art performance in multi-modal understanding andtext-to-image generation, but also excels in producing high-quality, coherentinterleaved outputs. Its emergent capabilities in zero-shot image editing andcompositional generation highlight Mogao as a practical omni-modal foundationmodel, paving the way for future development and scaling the unifiedmulti-modal systems.</description>
      <author>example@mail.com (Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang)</author>
      <guid isPermaLink="false">2505.05472v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation</title>
      <link>http://arxiv.org/abs/2505.05085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了通过传递和Koopman算子方法将复杂非线性动力学系统线性化表示的框架，并提出了一种通过机器学习优化线性算子的近似表示方法。&lt;h4&gt;背景&lt;/h4&gt;传递和Koopman算子方法为表示非线性动力学系统提供了一种线性变换的框架，但直接从数据中高效地估计这些算子的谱是具有挑战性的。&lt;h4&gt;目的&lt;/h4&gt;目的是通过一般算子和表征学习的方法，使用高效的有限维表示来近似这些线性算子。&lt;h4&gt;方法&lt;/h4&gt;使用机器学习技术学习正交归一且局部支撑的基函数，这些基函数根据系统的动力学特性动态定制。这种学习到的基函数提供了算子操作的准确近似以及几乎不变的有限维子空间。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法，可以检索估计算子的谱属性，并强调机器学习基函数的动态自适应特性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种基于机器学习的方法来优化线性算子的近似表示，为理解复杂非线性动力学系统的动力学提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;Transfer and Koopman operator methods offer a framework for representing complex, nonlinear dynamical systems via linear transformations, enabling for a deeper understanding of the underlying dynamics. The spectrum of these operators provide important insights into system predictability and emergent behaviour, although efficiently estimating them from data can be challenging. We tackle this issue through the lens of general operator and representational learning, in which we approximate these linear operators using efficient finite-dimensional representations. Specifically, we machine-learn orthonormal, locally supported basis functions that are dynamically tailored to the system. This learned basis provides a particularly accurate approximation of the operator's action as well as a nearly invariant finite-dimensional subspace. We illustrate our approach with examples that showcase the retrieval of spectral properties from the estimated operator, and emphasize the dynamically adaptive quality of the machine-learned basis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer and Koopman operator methods offer a framework for representingcomplex, nonlinear dynamical systems via linear transformations, enabling for adeeper understanding of the underlying dynamics. The spectrum of theseoperators provide important insights into system predictability and emergentbehaviour, although efficiently estimating them from data can be challenging.We tackle this issue through the lens of general operator and representationallearning, in which we approximate these linear operators using efficientfinite-dimensional representations. Specifically, we machine-learn orthonormal,locally supported basis functions that are dynamically tailored to the system.This learned basis provides a particularly accurate approximation of theoperator's action as well as a nearly invariant finite-dimensional subspace. Weillustrate our approach with examples that showcase the retrieval of spectralproperties from the estimated operator, and emphasise the dynamically adaptivequality of the machine-learned basis.</description>
      <author>example@mail.com (Gary Froyland, Kevin Kühl)</author>
      <guid isPermaLink="false">2505.05085v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes</title>
      <link>http://arxiv.org/abs/2505.04659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GSsplat的可泛化语义高斯混合方法，用于高效地生成未见过的场景的语义合成。&lt;h4&gt;背景&lt;/h4&gt;在3D场景理解的研究中，从多个视角对未见过的场景进行语义合成至关重要。当前的方法在渲染新视角图像和语义图方面表现良好，但速度和分割性能存在局限性。&lt;h4&gt;目的&lt;/h4&gt;为了提高合成速度和分割性能，提出了GSsplat方法。&lt;h4&gt;方法&lt;/h4&gt;GSsplat通过一次输入预测场景自适应高斯分布的位置和属性，取代了传统场景特定的高斯混合中的密度化和修剪过程。在多任务框架中，设计了一个混合网络来提取颜色和语义信息，并预测高斯参数。为了增强高斯的空间感知能力以实现高质量的渲染，通过基于群体的监督和具有空间单元聚类的点级交互模块提出了一个新颖的偏移学习模块。&lt;h4&gt;主要发现&lt;/h4&gt;GSsplat在多视角输入变化的情况下，以最快的速度实现了语义合成中的最先进性能。&lt;h4&gt;结论&lt;/h4&gt;GSsplat方法在提高合成速度和分割性能方面取得了显著的成果，为3D场景理解研究提供了有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：未见场景的语义合成从多个视角对3D场景理解研究至关重要。当前的方法能够通过重建可泛化的神经辐射场来渲染新视角的图像和语义图。然而，它们往往在速度和分割性能方面存在限制。我们提出了一种可泛化的语义高斯混合方法（GSsplat）用于高效的novel-view合成。我们的模型通过一次输入预测场景自适应高斯分布的位置和属性，取代了传统场景特定高斯混合的密度化和修剪过程。在多任务框架中，设计了一个混合网络来提取颜色和语义信息并预测高斯参数。为了增强高斯的空间感知能力以实现高质量的渲染，通过基于群体的监督和具有空间单元聚类的点级交互模块提出了一个新颖的偏移学习模块。当用变化数量的多视角输入进行评估时，GSsplat在语义合成方面达到了最快的速度和最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The semantic synthesis of unseen scenes from multiple viewpoints is crucialfor research in 3D scene understanding. Current methods are capable ofrendering novel-view images and semantic maps by reconstructing generalizableNeural Radiance Fields. However, they often suffer from limitations in speedand segmentation performance. We propose a generalizable semantic GaussianSplatting method (GSsplat) for efficient novel-view synthesis. Our modelpredicts the positions and attributes of scene-adaptive Gaussian distributionsfrom once input, replacing the densification and pruning processes oftraditional scene-specific Gaussian Splatting. In the multi-task framework, ahybrid network is designed to extract color and semantic information andpredict Gaussian parameters. To augment the spatial perception of Gaussians forhigh-quality rendering, we put forward a novel offset learning module throughgroup-based supervision and a point-level interaction module with spatial unitaggregation. When evaluated with varying numbers of multi-view inputs, GSsplatachieves state-of-the-art performance for semantic synthesis at the fastestspeed.</description>
      <author>example@mail.com (Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang)</author>
      <guid isPermaLink="false">2505.04659v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Contextual Embedding for Robust Far-View Borehole Detection</title>
      <link>http://arxiv.org/abs/2505.05008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对远视图像中密集分布的小孔检测的适应性检测方法，旨在提高爆破作业的安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;现有的检测方法在处理小尺度目标、高度密集的排列和有限的小孔视觉特征时存在困难。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了基于现有架构（如YOLO）的适应性检测方法，该方法通过指数移动平均（EMA）统计更新显式利用一致的嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;该方法引入了三个协同组件：(1)自适应增强，利用动态更新的图像统计信息来鲁棒地处理光照和纹理变化；(2)嵌入稳定化，确保一致且可靠的特征提取；(3)上下文细化，利用空间上下文提高检测精度。&lt;h4&gt;主要发现&lt;/h4&gt;EMA在本方法中的广泛应用特别有利，鉴于小孔的有限视觉复杂性和小尺度，即使在具有挑战性的视觉条件下也能实现稳定和鲁棒的表示学习。&lt;h4&gt;结论&lt;/h4&gt;在具有挑战性的私有采石场数据集上的实验表明，与基线YOLO架构相比，该方法有显著改进，突出了该方法在实际和复杂工业场景中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在控制的爆破作业中，从远视图像中准确检测密集分布的小孔对于操作安全和效率至关重要。然而，现有的检测方法由于小目标尺度、高度密集的排列和小孔的有限视觉特征而常常遇到困难。为了解决这些挑战，我们提出了一种基于现有架构（例如YOLO）的适应性检测方法，该方法通过通过指数移动平均（EMA）统计更新显式利用一致的嵌入表示。我们的方法引入了三个协同组件：（1）自适应增强，利用动态更新的图像统计信息来鲁棒地处理光照和纹理变化；（2）嵌入稳定化，确保一致且可靠的特征提取；（3）上下文细化，利用空间上下文提高检测精度。在本方法中EMA的广泛应用特别有利，鉴于小孔的有限视觉复杂性和小尺度，即使在具有挑战性的视觉条件下也能实现稳定和鲁棒的表示学习。在具有挑战性的私有采石场数据集上的实验表明，与基线YOLO架构相比，该方法有显著改进，突出了该方法在实际和复杂工业场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In controlled blasting operations, accurately detecting densely distributedtiny boreholes from far-view imagery is critical for operational safety andefficiency. However, existing detection methods often struggle due to smallobject scales, highly dense arrangements, and limited distinctive visualfeatures of boreholes. To address these challenges, we propose an adaptivedetection approach that builds upon existing architectures (e.g., YOLO) byexplicitly leveraging consistent embedding representations derived throughexponential moving average (EMA)-based statistical updates.  Our method introduces three synergistic components: (1) adaptive augmentationutilizing dynamically updated image statistics to robustly handle illuminationand texture variations; (2) embedding stabilization to ensure consistent andreliable feature extraction; and (3) contextual refinement leveraging spatialcontext for improved detection accuracy. The pervasive use of EMA in our methodis particularly advantageous given the limited visual complexity and smallscale of boreholes, allowing stable and robust representation learning evenunder challenging visual conditions. Experiments on a challenging proprietaryquarry-site dataset demonstrate substantial improvements over baselineYOLO-based architectures, highlighting our method's effectiveness in realisticand complex industrial scenarios.</description>
      <author>example@mail.com (Xuesong Liu, Tianyu Hao, Emmett J. Ientilucci)</author>
      <guid isPermaLink="false">2505.05008v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
      <link>http://arxiv.org/abs/2505.04485v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FA-KPConv，一种基于KPConv的神经网络架构，用于3D点云分析。通过帧平均技术，FA-KPConv使网络对点云的平移、旋转和反射具有精确的不变性和/或等变性，提高了点云分类和点云注册的性能。&lt;h4&gt;背景&lt;/h4&gt;KPConv是一种广泛用于3D点云分析的骨干网络，但其对欧几里得变换的不变性和/或等变性只能在大数据集或数据增强的情况下近似实现。&lt;h4&gt;目的&lt;/h4&gt;通过引入帧平均技术，使点云神经网络对平移、旋转和/或反射具有精确的不变性和/或等变性。&lt;h4&gt;方法&lt;/h4&gt;FA-KPConv通过在现有的KPConv网络基础上添加帧平均技术，将几何先验知识嵌入到网络中，同时保持可学习参数的数量，不牺牲任何输入信息。&lt;h4&gt;主要发现&lt;/h4&gt;FA-KPConv在点云分类和点云注册任务中展现出优势，尤其是在训练数据稀缺或测试数据随机旋转的挑战性情况下。&lt;h4&gt;结论&lt;/h4&gt;FA-KPConv通过引入帧平均技术，提高了点云神经网络对欧几里得变换的不变性和/或等变性，从而提高了点云分类和点云注册的性能。&lt;h4&gt;翻译&lt;/h4&gt;We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.</description>
      <author>example@mail.com (Ali Alawieh, Alexandru P. Condurache)</author>
      <guid isPermaLink="false">2505.04485v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks</title>
      <link>http://arxiv.org/abs/2505.04894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE DCOSS-IoT 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TH-GCN的图卷积网络方法，用于优化密集5G网络中的切换管理，旨在提高网络稳定性。&lt;h4&gt;背景&lt;/h4&gt;5G技术的快速发展改变了车载网络，提供了高带宽、低延迟和快速数据率，这些特性对于智能城市和车辆中的实时应用至关重要，同时也提高了交通安全和娱乐服务。&lt;h4&gt;目的&lt;/h4&gt;解决5G网络在覆盖范围有限和频繁切换导致的网络不稳定问题，尤其是在高移动性环境中。&lt;h4&gt;方法&lt;/h4&gt;TH-GCN使用图神经网络（GNN）将车辆和基站建模为动态图中的节点，该图包含信号质量、吞吐量、车辆速度和基站负载等特征。通过整合用户设备和基站的角度，实现自适应的实时切换决策。&lt;h4&gt;主要发现&lt;/h4&gt;TH-GCN将切换次数减少了多达78%，并将信号质量提高了10%，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;TH-GCN通过优化切换管理，显著提高了5G网络的稳定性和性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of 5G has transformed vehicular networks, offering highbandwidth, low latency, and fast data rates essential for real-timeapplications in smart cities and vehicles. These improvements enhance trafficsafety and entertainment services. However, the limited coverage and frequenthandovers in 5G networks cause network instability, especially in high-mobilityenvironments due to the ping-pong effect. This paper presents TH-GCN(Throughput-oriented Graph Convolutional Network), a novel approach foroptimizing handover management in dense 5G networks. Using graph neuralnetworks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamicgraph enriched with features such as signal quality, throughput, vehicle speed,and base station load. By integrating both user equipment and base stationperspectives, this dual-centric approach enables adaptive, real-time handoverdecisions that improve network stability. Simulation results show that TH-GCNreduces handovers by up to 78 percent and improves signal quality by 10percent, outperforming existing methods.</description>
      <author>example@mail.com (Nazanin Mehregan, Robson E. De Grande)</author>
      <guid isPermaLink="false">2505.04894v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Graffe: Graph Representation Learning via Diffusion Probabilistic Models</title>
      <link>http://arxiv.org/abs/2505.04956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 4 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Graffe，一种用于图表示学习的自监督扩散模型。该模型通过图编码器将源图压缩成紧凑表示，该表示作为条件引导扩散解码器的去噪过程。实验表明，Graffe在节点和图分类任务上取得了竞争性结果，并在11个真实世界数据集的9个上达到了最先进的表现。&lt;h4&gt;背景&lt;/h4&gt;扩散概率模型（DPMs）因其生成高质量样本的潜力而广为人知，但在表示学习领域往往被忽视。尽管最近的研究突出了其在捕获视觉语义方面的潜力，但将DPMs应用于图表示学习仍然处于初级阶段。&lt;h4&gt;目的&lt;/h4&gt;提出Graffe模型，旨在将扩散模型应用于图表示学习，并评估其有效性。&lt;h4&gt;方法&lt;/h4&gt;Graffe模型包括一个图编码器和一个扩散解码器。图编码器将源图压缩成紧凑表示，该表示作为条件来引导扩散解码器的去噪过程。通过理论证明和实践验证来评估模型的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明表明，去噪目标隐式最大化了数据和其表示之间的条件互信息。在实证研究中，Graffe在节点和图分类任务上取得了优异的性能，在11个真实世界数据集的9个上达到了最先进的表现。&lt;h4&gt;结论&lt;/h4&gt;扩散模型，尤其是扩散概率模型，是图表示学习的有效工具，特别是Graffe模型在图表示学习方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;Diffusion probabilistic models (DPMs), widely recognized for their potential to generate high-quality samples, tend to go unnoticed in representation learning. While recent progress has highlighted their potential for capturing visual semantics, adapting DPMs to graph representation learning remains in its infancy. In this paper, we introduce Graffe, a self-supervised diffusion model proposed for graph representation learning. It features a graph encoder that distills a source graph into a compact representation, which, in turn, serves as the condition to guide the denoising process of the diffusion decoder. To evaluate the effectiveness of our model, we first explore the theoretical foundations of applying diffusion models to representation learning, proving that the denoising objective implicitly maximizes the conditional mutual information between data and its representation. Specifically, we prove that the negative logarithm of the denoising score matching loss is a tractable lower bound for the conditional mutual information. Empirically, we conduct a series of case studies to validate our theoretical insights. In addition, Graffe delivers competitive results under the linear probing setting on node and graph classification tasks, achieving state-of-the-art performance on 9 of the 11 real-world datasets. These findings indicate that powerful generative models, especially diffusion models, serve as an effective tool for graph representation learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion probabilistic models (DPMs), widely recognized for their potentialto generate high-quality samples, tend to go unnoticed in representationlearning. While recent progress has highlighted their potential for capturingvisual semantics, adapting DPMs to graph representation learning remains in itsinfancy. In this paper, we introduce Graffe, a self-supervised diffusion modelproposed for graph representation learning. It features a graph encoder thatdistills a source graph into a compact representation, which, in turn, servesas the condition to guide the denoising process of the diffusion decoder. Toevaluate the effectiveness of our model, we first explore the theoreticalfoundations of applying diffusion models to representation learning, provingthat the denoising objective implicitly maximizes the conditional mutualinformation between data and its representation. Specifically, we prove thatthe negative logarithm of the denoising score matching loss is a tractablelower bound for the conditional mutual information. Empirically, we conduct aseries of case studies to validate our theoretical insights. In addition,Graffe delivers competitive results under the linear probing setting on nodeand graph classification tasks, achieving state-of-the-art performance on 9 ofthe 11 real-world datasets. These findings indicate that powerful generativemodels, especially diffusion models, serve as an effective tool for graphrepresentation learning.</description>
      <author>example@mail.com (Dingshuo Chen, Shuchen Xue, Liuji Chen, Yingheng Wang, Qiang Liu, Shu Wu, Zhi-Ming Ma, Liang Wang)</author>
      <guid isPermaLink="false">2505.04956v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</title>
      <link>http://arxiv.org/abs/2505.05396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文旨在从临床理论角度研究疼痛评估过程，并探索和检验现有自动方法。在此基础上，主要目标是开发高性能、适用于实际临床环境的自动疼痛评估计算方法。&lt;h4&gt;背景&lt;/h4&gt;论文从临床理论角度出发，对疼痛评估过程进行了研究，并考察了现有的自动方法。&lt;h4&gt;目的&lt;/h4&gt;开发创新计算方法，实现高性能的自动疼痛评估，并在实际临床环境中应用。&lt;h4&gt;方法&lt;/h4&gt;通过计算方法，深入研究和评估影响疼痛感知的显著因素，包括人口统计学元素，并设计、开发、提出适用于不同场景需求的自动疼痛评估流程。&lt;h4&gt;主要发现&lt;/h4&gt;论文中提出的方法在疼痛评估方面取得了最先进的结果，并展示了其有效性。&lt;h4&gt;结论&lt;/h4&gt;论文为探索人工智能、基础模型和生成式人工智能的新方法铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;This thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computational methods for automatic pain assessment that achieve high performance and are applicable in real clinical settings. A primary goal is to thoroughly investigate and assess significant factors, including demographic elements that impact pain perception, as recognized in pain research, through a computational standpoint. Within the limits of the available data in this research area, our goal was to design, develop, propose, and offer automatic pain assessment pipelines for unimodal and multimodal configurations that are applicable to the specific requirements of different scenarios. The studies published in this Ph.D. thesis showcased the effectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they paved the way for exploring new approaches in artificial intelligence, foundation models, and generative artificial intelligence.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; From the original abstract:  This thesis initially aims to study the pain assessment process from aclinical-theoretical perspective while exploring and examining existingautomatic approaches. Building on this foundation, the primary objective ofthis Ph.D. project is to develop innovative computational methods for automaticpain assessment that achieve high performance and are applicable in realclinical settings. A primary goal is to thoroughly investigate and assesssignificant factors, including demographic elements that impact painperception, as recognized in pain research, through a computational standpoint.Within the limits of the available data in this research area, our goal was todesign, develop, propose, and offer automatic pain assessment pipelines forunimodal and multimodal configurations that are applicable to the specificrequirements of different scenarios. The studies published in this Ph.D. thesisshowcased the effectiveness of the proposed methods, achieving state-of-the-artresults. Additionally, they paved the way for exploring new approaches inartificial intelligence, foundation models, and generative artificialintelligence.</description>
      <author>example@mail.com (Stefanos Gkikas)</author>
      <guid isPermaLink="false">2505.05396v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>An Unsupervised Learning Method for Radio Interferometry Deconvolution</title>
      <link>http://arxiv.org/abs/2505.04887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了如何利用压缩感知（CS）理论解决射电干涉仪空间频率采样不完整的问题，提出了一种基于深度字典的全新无监督学习方法，以实现对天体信息的高精度恢复。&lt;h4&gt;背景&lt;/h4&gt;射电干涉仪的空间频率采样不完整导致天体信息恢复困难，压缩感知理论提供了一种稳定且唯一恢复天空中亮度分布的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于压缩感知理论的方法，以实现对射电干涉仪观测到的天空中亮度分布的精确恢复。&lt;h4&gt;方法&lt;/h4&gt;开发了一个深度字典（通过卷积神经网络实现），该字典是多分辨率和过完备的，以实现稀疏表示，并将其整合到压缩感知框架中。在去卷积过程中，模型图像和深度字典交替更新。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地从噪声测量中恢复具有复杂形态的扩展源，与现有算法相比，动态范围（DR）几乎提高了45到100倍。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法结合了压缩感知的数学严谨性和深度神经网络的表 达能力，在解决射电干涉仪观测数据恢复问题上取得了显著成果。&lt;h4&gt;翻译&lt;/h4&gt;Given the incomplete sampling of spatial frequencies by radiointerferometers, achieving precise restoration of astrophysical information remains challenging. To address this ill-posed problem, compressive sensing (CS) provides a robust framework for stable and unique recovery of sky brightness distributions in noisy environments, contingent upon satisfying specific conditions. We explore the applicability of CS theory and find that for radiointerferometric telescopes, the conditions can be simplified to sparse representation. Building on this insight, we develop a deep dictionary (realized through a convolutional neural network), which is designed to be multi-resolution and overcomplete, to achieve sparse representation and integrate it within the CS framework. The resulting method is a novel, fully interpretable unsupervised learning approach that combines the mathematical rigor of CS with the expressive power of deep neural networks, effectively bridging the gap between deep learning and classical dictionary methods. During the deconvolution process, the model image and the deep dictionary are updated alternately. This approach enables efficient and accurate recovery of extended sources with complex morphologies from noisy measurements. Comparative analyses with state-of-the-art algorithms demonstrate the outstanding performance of our method, i.e., achieving a dynamic range (DR) nearly 45 to 100 times higher than that of multiscale CLEAN (MS-CLEAN).&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3847/1538-4365/add1b7&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the incomplete sampling of spatial frequencies by radiointerferometers, achieving precise restoration of astrophysical informationremains challenging. To address this ill-posed problem, compressive sensing(CS)provides a robust framework for stable and unique recovery of sky brightnessdistributions in noisy environments, contingent upon satisfying specificconditions. We explore the applicability of CS theory and find that for radiointerferometric telescopes, the conditions can be simplified to sparserepresentation. {{Building on this insight, we develop a deep dictionary(realized through a convolutional neural network), which is designed to bemulti-resolution and overcomplete, to achieve sparse representation andintegrate it within the CS framework. The resulting method is a novel, fullyinterpretable unsupervised learning approach that combines}} the mathematicalrigor of CS with the expressive power of deep neural networks, effectivelybridging the gap between deep learning and classical dictionary methods.{{During the deconvolution process, the model image and the deep dictionary areupdated alternatively.}} This approach enables efficient and accurate recoveryof extended sources with complex morphologies from noisy measurements.Comparative analyses with state-of-the-art algorithms demonstrate theoutstanding performance of our method, i.e., achieving a dynamic range (DR)nearly 45 to 100 times higher than that of multiscale CLEAN (MS-CLEAN).</description>
      <author>example@mail.com (Lei Yu, Bin Liu, Cheng-Jin Jin, Ru-Rong Chen, Hong-Wei Xi, Bo Peng)</author>
      <guid isPermaLink="false">2505.04887v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</title>
      <link>http://arxiv.org/abs/2505.05291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在视网膜成像中，基于自监督学习的Vision Transformers（ViTs）在AMD识别任务上的表现，并探讨了领域内预训练的必要性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习使ViTs能够从大规模自然图像数据集中学习鲁棒表示，增强了其在不同领域的泛化能力。在视网膜成像中，基于自然或眼科数据的预训练模型显示出潜力，但领域内预训练的好处尚未确定。&lt;h4&gt;目的&lt;/h4&gt;为了调查领域内预训练的好处，本研究在七个包含70,000张专家标注的DFI数据集上对六个SSL预训练的ViTs进行了基准测试，以识别中晚期AMD。&lt;h4&gt;方法&lt;/h4&gt;研究者对自然图像预训练的iBOT、领域特定模型以及未预训练的ViT-L进行了比较，并发布了BRAMD，一个包含巴西DFI和AMD标签的公开数据集。&lt;h4&gt;主要发现&lt;/h4&gt;自然图像预训练的iBOT在分布外泛化方面表现最佳，其AUROCs为0.80-0.97，优于领域特定模型（AUROCs为0.78-0.96）和未预训练的ViT-L（AUROCs为0.68-0.91）。这些发现强调了基础模型在提高AMD识别价值中的重要性，并挑战了领域内预训练的必要性。&lt;h4&gt;结论&lt;/h4&gt;领域内预训练可能不是必要的，基础模型能够有效提高AMD识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) tolearn robust representations from large-scale natural image datasets, enhancingtheir generalization across domains. In retinal imaging, foundation modelspretrained on either natural or ophthalmic data have shown promise, but thebenefits of in-domain pretraining remain uncertain. To investigate this, webenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasetstotaling 70,000 expert-annotated images for the task of moderate-to-lateage-related macular degeneration (AMD) identification. Our results show thatiBOT pretrained on natural images achieves the highest out-of-distributiongeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,which achieved AUROCs of 0.68-0.91. These findings highlight the value offoundation models in improving AMD identification and challenge the assumptionthat in-domain pretraining is necessary. Furthermore, we release BRAMD, anopen-access dataset (n=587) of DFIs with AMD labels from Brazil.</description>
      <author>example@mail.com (Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar)</author>
      <guid isPermaLink="false">2505.05291v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Network Digital Twin for Route Optimization in 5G/B5G Transport Slicing with What-If Analysis</title>
      <link>http://arxiv.org/abs/2505.04879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication at IEEE International  Conference on Communications. \c{opyright}2025 IEEE. Personal use of this  material is permitted. Permission from IEEE must be obtained for all other  uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了5G和B5G网络中动态监控和高级解决方案的需求，以保障服务质量。提出了网络数字孪生（NDT）作为虚拟网络测试的解决方案，并在传输网络领域设计了一个实验平台，以实现智能决策和动态路由优化。&lt;h4&gt;背景&lt;/h4&gt;5G和B5G网络的多样化服务需求，如超低延迟和高带宽，要求动态监控和高级解决方案来确保服务质量。&lt;h4&gt;目的&lt;/h4&gt;设计一个实验平台，利用网络数字孪生（NDT）在传输网络领域进行配置和算法测试，以实现智能决策和动态路由优化。&lt;h4&gt;方法&lt;/h4&gt;构建了一个由图神经网络（GNN）组成的NDT，并在包含8、16和30个节点的三种不同网络拓扑中进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;NDT在实现后的延迟预测中取得了较低的MAPE值，表明其具有较高的准确性，能够为网络性能提供精确的见解。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在生成网络性能的精确见解方面是有效的，为5G/B5G场景中的动态路由优化问题提供了智能决策支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：第五代（5G）和超越5G（B5G）网络的出现引入了从超低延迟到高带宽的多样化服务需求，需要动态监控和高级解决方案来确保服务质量（QoS）。负责连接无线接入网络和核心网络的传输网络将越来越多地面临管理复杂流量模式的不利挑战。网络数字孪生（NDT）概念作为在虚拟网络中测试配置和算法的解决方案而出现。在这种情况下，这项工作在传输网络领域设计了一个包含NDT的实验平台，与虚拟对应物和推荐系统同步，以实现5G/B5G场景中动态路由优化问题的智能决策。我们的NDT由一个图神经网络（GNN）组成，在由8、16和30个节点组成的三种不同网络拓扑中进行了评估。与实施解决方案后的实际延迟相比，它实现了URLLC和eMBB切片的较低MAPE值。这些值表明了高精度，证明了该解决方案在生成特定解决方案实施时网络性能精确见解方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of fifth-generation (5G) and Beyond 5G (B5G) networks introducesdiverse service requirements, from ultra-low latency to high bandwidth,demanding dynamic monitoring and advanced solutions to ensure Quality ofService (QoS). The transport network - responsible for interconnecting theradio access network and core networks - will increasingly face challenges inefficiently managing complex traffic patterns. The Network Digital Twin (NDT)concept emerges as a promising solution for testing configurations andalgorithms in a virtual network before real-world deployment. In this context,this work designs an experimental platform with NDT in a transport networkdomain, synchronizing with the virtual counterpart and a recommendation systemfor what-if analysis, enabling intelligent decision-making for dynamic routeoptimization problems in 5G/B5G scenarios. Our NDT, composed of a Graph NeuralNetwork (GNN), was evaluated across three different network topologiesconsisting of 8, 16, and 30 nodes. It achieved lower MAPE values for URLLC andeMBB slices, comparing latency predictions with actual latency after thesolution implementation. These values indicate high accuracy, demonstrating thesolution's effectiveness in generating precise insights into networkperformance if a particular solution were implemented.</description>
      <author>example@mail.com (Rebecca Aben-Athar, Heitor Anglada, Lucas Costa, João Albuquerque, Abrahão Ferreira, Cristiano Bonato Both, Kleber Cardoso, Silvia Lins, Andrey Silva, Glauco Gonçalves, Ilan Correa, Aldebaro Klautau)</author>
      <guid isPermaLink="false">2505.04879v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Accurate Prediction of Sequential Tensor Properties Using Equivariant Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了利用光学光谱研究材料与光相互作用的强大工具，并展示了其在光电子器件开发中的应用，同时提出了一种新的神经网络模型StepENN，用于预测材料的光学响应。&lt;h4&gt;背景&lt;/h4&gt;光学光谱对于揭示材料的电子结构至关重要，特别是在光电子器件如太阳能电池、发光二极管和光电探测器中，电子结构的理解直接影响器件性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预测各向异性块体材料中光学响应的模型，以支持光电子器件的数据驱动设计。&lt;h4&gt;方法&lt;/h4&gt;提出了序列张量属性等变神经网络（StepENN），这是一种图神经网络架构，可以将晶体结构直接映射到不同光子频率下的完整光学张量。StepENN通过将各向同性的序列标量分量和各向异性的序列张量分量编码为l=0和l=2球张量分量，确保了与晶体系统内在对称性约束一致的对称感知序列张量预测。&lt;h4&gt;主要发现&lt;/h4&gt;在基于1,432种块体半导体频率相关介电张量数据集上训练的模型，在预测张量光谱方面达到了平均绝对误差（MAE）为24.216毫法拉每米（mF/m），其中85.7%的预测相对误差小于10%，展示了模型在推导其他光谱相关性质（如光学电导率）的潜力。&lt;h4&gt;结论&lt;/h4&gt;StepENN框架为具有工程化各向异性光学响应的材料的数据驱动设计开辟了新的途径，加速了光电子应用中的材料进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：光学光谱是研究材料与光相互作用的强大工具，揭示了复杂的电子结构，如平坦能带和非平凡拓扑特征。这些见解对于光子器件（包括太阳能电池、发光二极管和光电探测器）的开发和优化至关重要，其中对电子结构的理解直接影响到器件的性能。此外，在各向异性块体材料中，光学响应是方向依赖的，由于其固有的复杂性和晶体对称性的限制，预测这些响应张量仍然具有计算上的挑战。为了解决这一挑战，我们引入了序列张量属性等变神经网络（StepENN），这是一种将晶体结构直接映射到不同光子频率下的完整光学张量的图神经网络架构。通过将各向同性的序列标量分量和各向异性的序列张量分量编码为l=0和l=2球张量分量，StepENN确保了与晶体系统内在对称性约束一致的对称感知序列张量预测。在基于从第一性原理方法计算出的1,432种块体半导体频率相关介电张量数据集上训练的模型，在预测张量光谱方面达到了平均绝对误差（MAE）为24.216毫法拉每米（mF/m），其中85.7%的预测相对误差小于10%，展示了其在推导其他光谱相关性质（如光学电导率）的潜力。这一框架为具有工程化各向异性光学响应的材料的数据驱动设计开辟了新的途径，加速了光电子应用中的材料进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Optical spectra serve as a powerful tool for probing the interactions betweenmaterials and light, unveiling complex electronic structures such as flat bandsand nontrivial topological features. These insights are crucial for thedevelopment and optimization of photonic devices, including solar cells,light-emitting diodes, and photodetectors, where understanding the electronicstructure directly impacts device performance. Moreover, in anisotropic bulkmaterials, the optical responses are direction-dependent, and predicting thoseresponse tensors still remains computationally demanding due to its inherentcomplexity and the constraint from crystal symmetry. To address this challenge,we introduce the sequential tensorial properties equivariant neural network(StepENN), a graph neural network architecture that maps crystal structuresdirectly to their full optical tensors across different photon frequencies. Byencoding the isotropic sequential scalar components and anisotropic sequentialtensor components into l=0 and l=2 spherical tensor components, StepENN ensuressymmetry-aware sequential tensor predictions that are consistent with theinherent symmetry constraints of crystal systems. Trained on a dataset offrequency-dependent permittivity tensors for 1,432 bulk semiconductors computedfrom first-principles methods, our model achieves a mean absolute error (MAE)of 24.216 millifarads per meter (mF/m) on the predicted tensorial spectra with85.7% of its predictions exhibiting less than 10% relative error, demonstratingits potential for deriving other spectrum-related properties, such as opticalconductivity. This framework opens new avenues for the data-driven design ofmaterials with engineered anisotropic optical responses, accelerating materialadvances in optoelectronic applications.</description>
      <author>example@mail.com (Ting-Wei Hsu, Zhenyao Fang, Arun Bansil, Qimin Yan)</author>
      <guid isPermaLink="false">2505.04862v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Piecewise Constant Spectral Graph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to TMLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PieCoN（分段常数谱图神经网络），这是一种结合常数谱滤波器和多项式滤波器的图神经网络，旨在更灵活地利用图结构，并通过自适应地划分谱区间来提高学习谱特性的范围。&lt;h4&gt;背景&lt;/h4&gt;现有的谱GNN使用低阶多项式滤波器捕获图谱特性，但可能无法充分识别图的谱特性，因为多项式阶数低；同时，增加多项式阶数会导致计算成本上升，且超过一定阈值会带来性能平台期或下降。&lt;h4&gt;目的&lt;/h4&gt;设计PieCoN以解决现有谱GNN的这些挑战，提供一种更灵活的方法来利用图结构。&lt;h4&gt;方法&lt;/h4&gt;PieCoN结合常数谱滤波器和多项式滤波器，并通过自适应地划分谱区间来提高学习谱特性的范围。&lt;h4&gt;主要发现&lt;/h4&gt;在包括同质和异质图的九个基准数据集上的实验表明，PieCoN在异质数据集上尤其有效，突出了其在广泛领域的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;PieCoN通过结合不同类型的滤波器并自适应地处理谱区间，有效地提高了图神经网络学习谱特性的能力，尤其是在异质图数据集上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have achieved significant success across various domains by leveraging graph structures in data. Existing spectral GNNs, which use low-degree polynomial filters to capture graph spectral properties, may not fully identify the graph's spectral characteristics because of the polynomial's small degree. However, increasing the polynomial degree is computationally expensive and beyond certain thresholds leads to performance plateaus or degradation. In this paper, we introduce the Piecewise Constant Spectral Graph Neural Network (PieCoN) to address these challenges. PieCoN combines constant spectral filters with polynomial filters to provide a more flexible way to leverage the graph structure. By adaptively partitioning the spectrum into intervals, our approach increases the range of spectral properties that can be effectively learned. Experiments on nine benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that PieCoN is particularly effective on heterophilic datasets, highlighting its potential for a wide range of applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved significant success across variousdomains by leveraging graph structures in data. Existing spectral GNNs, whichuse low-degree polynomial filters to capture graph spectral properties, may notfully identify the graph's spectral characteristics because of the polynomial'ssmall degree. However, increasing the polynomial degree is computationallyexpensive and beyond certain thresholds leads to performance plateaus ordegradation. In this paper, we introduce the Piecewise Constant Spectral GraphNeural Network(PieCoN) to address these challenges. PieCoN combines constantspectral filters with polynomial filters to provide a more flexible way toleverage the graph structure. By adaptively partitioning the spectrum intointervals, our approach increases the range of spectral properties that can beeffectively learned. Experiments on nine benchmark datasets, including bothhomophilic and heterophilic graphs, demonstrate that PieCoN is particularlyeffective on heterophilic datasets, highlighting its potential for a wide rangeof applications.</description>
      <author>example@mail.com (Vahan Martirosyan, Jhony H. Giraldo, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2505.04808v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.02393v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IEF-VAD的视频异常检测框架，该框架通过图像-事件融合的方法，从RGB视频中直接合成事件表示，并通过一种原则性的、具有不确定性的过程将其与图像特征融合，以解决现有视频异常检测器依赖RGB帧而缺乏时间分辨率的问题。&lt;h4&gt;背景&lt;/h4&gt;大多数现有的视频异常检测器依赖于RGB帧，这些帧无法捕捉到异常事件的突然或短暂的运动线索，而这些线索是异常事件的关键指标。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的视频异常检测方法，以提高检测的准确性和鲁棒性，并减少对特定事件传感器的需求。&lt;h4&gt;方法&lt;/h4&gt;IEF-VAD框架包括：(i) 使用Student-t似然模型对重尾传感器噪声进行建模，并通过拉普拉斯近似推导值级逆方差权重；(ii) 应用类似卡尔曼风格的帧级更新来平衡模态随时间的变化；(iii) 通过迭代地细化融合的潜在状态来消除残留的跨模态噪声。&lt;h4&gt;主要发现&lt;/h4&gt;IEF-VAD在多个真实世界的异常检测基准测试中达到了新的水平，无需专用的事件传感器或帧级标签。&lt;h4&gt;结论&lt;/h4&gt;合成事件表示在强调运动线索方面具有效用，这些线索在RGB帧中通常没有得到充分的代表，从而使IEF-VAD能够在不需要专用事件传感器的情况下，实现准确和鲁棒的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数现有的视频异常检测器仅依赖于RGB帧，这缺乏捕捉突然或短暂运动线索的时间分辨率，而运动线索是异常事件的关键指标。为了解决这一局限性，我们提出了图像-事件融合视频异常检测（IEF-VAD）框架，该框架直接从RGB视频中合成事件表示，并通过一种原则性的、具有不确定性的过程将其与图像特征融合。该系统（i）使用Student-t似然对重尾传感器噪声进行建模，通过拉普拉斯近似推导值级逆方差权重；（ii）应用类似卡尔曼风格的帧级更新来平衡模态随时间的变化；（iii）通过迭代地细化融合的潜在状态来消除残留的跨模态噪声。在没有专用事件传感器或帧级标签的情况下，IEF-VAD在多个真实世界的异常检测基准测试中设定了新的水平。这些发现突出了合成事件表示在强调运动线索方面的效用，这些线索在RGB帧中通常没有得到充分的代表，从而使IEF-VAD能够在不需要专用事件传感器的情况下，实现准确和鲁棒的视频理解。代码和模型可在https://github.com/EavnJeong/IEF-VAD上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/eavnjeong/ief-vad&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing video anomaly detectors rely solely on RGB frames, which lackthe temporal resolution needed to capture abrupt or transient motion cues, keyindicators of anomalous events. To address this limitation, we proposeImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework thatsynthesizes event representations directly from RGB videos and fuses them withimage features through a principled, uncertainty-aware process. The system (i)models heavy-tailed sensor noise with a Student`s-t likelihood, derivingvalue-level inverse-variance weights via a Laplace approximation; (ii) appliesKalman-style frame-wise updates to balance modalities over time; and (iii)iteratively refines the fused latent state to erase residual cross-modal noise.Without any dedicated event sensor or frame-level labels, IEF-VAD sets a newstate of the art across multiple real-world anomaly detection benchmarks. Thesefindings highlight the utility of synthetic event representations inemphasizing motion cues that are often underrepresented in RGB frames, enablingaccurate and robust video understanding across diverse applications withoutrequiring dedicated event sensors. Code and models are available athttps://github.com/EavnJeong/IEF-VAD.</description>
      <author>example@mail.com (Sungheon Jeong, Jihong Park, Mohsen Imani)</author>
      <guid isPermaLink="false">2505.02393v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MTL-UE: Learning to Learn Nothing for Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2505.05279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MTL-UE的统一框架，用于生成多任务数据和多任务学习模型不可学习的示例。&lt;h4&gt;背景&lt;/h4&gt;现有的不可学习策略主要集中在防止未经授权的用户使用个人数据训练单任务学习模型。然而，近年来，研究重点已转向多任务数据和多任务学习，旨在开发能够同时处理多个任务的通用和基础模型。&lt;h4&gt;目的&lt;/h4&gt;尽管多任务数据和模型日益重要，但在追求不可学习策略时，它们却被大量忽视。本文旨在提出一种针对多任务数据和模型的不可学习策略。&lt;h4&gt;方法&lt;/h4&gt;MTL-UE通过设计一个基于生成器的结构，引入标签先验和类别的特征嵌入，来优化扰动，从而提高攻击性能。此外，它还结合了任务内和任务间嵌入正则化，以增加类间分离并抑制类内方差，从而大大增强攻击的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;MTL-UE对密集预测任务中的多任务学习提供了良好的支持，并且可以即插即用，允许与现有的依赖代理的不可学习方法轻松集成。实验表明，MTL-UE在4个多任务数据集、3种基础不可学习方法和5种模型骨干以及5种多任务任务加权策略上均实现了优越的攻击性能。&lt;h4&gt;结论&lt;/h4&gt;MTL-UE是一种有效的方法，可以生成多任务数据和模型不可学习的示例，并在多个数据集和模型上表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing unlearnable strategies focus on preventing unauthorized usersfrom training single-task learning (STL) models with personal data.Nevertheless, the paradigm has recently shifted towards multi-task data andmulti-task learning (MTL), targeting generalist and foundation models that canhandle multiple tasks simultaneously. Despite their growing importance, MTLdata and models have been largely neglected while pursuing unlearnablestrategies. This paper presents MTL-UE, the first unified framework forgenerating unlearnable examples for multi-task data and MTL models. Instead ofoptimizing perturbations for each sample, we design a generator-based structurethat introduces label priors and class-wise feature embeddings which leads tomuch better attacking performance. In addition, MTL-UE incorporates intra-taskand inter-task embedding regularization to increase inter-class separation andsuppress intra-class variance which enhances the attack robustness greatly.Furthermore, MTL-UE is versatile with good supports for dense prediction tasksin MTL. It is also plug-and-play allowing integrating existingsurrogate-dependent unlearnable methods with little adaptation. Extensiveexperiments show that MTL-UE achieves superior attacking performanceconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5MTL task-weighting strategies.</description>
      <author>example@mail.com (Yi Yu, Song Xia, Siyuan Yang, Chenqi Kong, Wenhan Yang, Shijian Lu, Yap-Peng Tan, Alex C. Kot)</author>
      <guid isPermaLink="false">2505.05279v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
      <link>http://arxiv.org/abs/2505.04899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Organ-Wise Tokenization (OWT)的框架，通过Token Group-based Reconstruction (TGR)训练范式来解决图像表示学习中的可解释性和泛化性问题。&lt;h4&gt;背景&lt;/h4&gt;当前代表学习依赖黑盒嵌入，难以解释和泛化，这在医学图像分析中尤为重要。&lt;h4&gt;目的&lt;/h4&gt;提出OWT框架以解决上述局限性，提高医学图像分析的解可释性、泛化性和效率。&lt;h4&gt;方法&lt;/h4&gt;OWT将图像显式地分解为可分离的token组，每组对应一个独特的器官或语义实体。&lt;h4&gt;主要发现&lt;/h4&gt;OWT在CT和MRI数据集上的实验表明，其在图像重建和分割方面的性能强，并且能够实现新型语义级生成和检索应用。&lt;h4&gt;结论&lt;/h4&gt;OWT作为语义解耦表示学习的基础框架，具有广泛的适用性和可扩展性，适用于实际医学图像分析场景及其他领域。&lt;h4&gt;翻译&lt;/h4&gt;近期在表示学习方面取得的进展常常依赖于整体、黑盒的嵌入，这会混杂多个语义组件，限制了可解释性和泛化性。这些问题在医学影像分析中尤为关键。为了解决这些限制，我们提出了一种基于器官的标记化（OWT）框架和基于标记组重建（TGR）训练范式。与产生整体特征的常规方法不同，OWT明确地将图像分解为可分离的标记组，每个组对应一个独特的器官或语义实体。我们的设计确保每个标记组封装了器官特定的信息，从而提高了可解释性、泛化性和效率，同时在下游任务中允许细粒度的控制。在CT和MRI数据集上的实验表明，OWT不仅实现了强大的图像重建和分割性能，而且还能实现标准整体嵌入方法无法达到的新型语义级生成和检索应用。这些发现强调了OWT作为语义解耦表示学习基础框架的潜力，它提供了广泛的扩展性和适用性，可以应用于现实世界的医学图像分析场景及其它领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in representation learning often rely on holistic, black-boxembeddings that entangle multiple semantic components, limitinginterpretability and generalization. These issues are especially critical inmedical imaging. To address these limitations, we propose an Organ-WiseTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)training paradigm. Unlike conventional approaches that produce holisticfeatures, OWT explicitly disentangles an image into separable token groups,each corresponding to a distinct organ or semantic entity. Our design ensureseach token group encapsulates organ-specific information, boostinginterpretability, generalization, and efficiency while allowing fine-grainedcontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate theeffectiveness of OWT in not only achieving strong image reconstruction andsegmentation performance, but also enabling novel semantic-level generation andretrieval applications that are out of reach for standard holistic embeddingmethods. These findings underscore the potential of OWT as a foundationalframework for semantically disentangled representation learning, offering broadscalability and applicability to real-world medical imaging scenarios andbeyond.</description>
      <author>example@mail.com (Sifan Song, Siyeop Yoon, Pengfei Jin, Sekeun Kim, Matthew Tivnan, Yujin Oh, Runqi Meng, Ling Chen, Zhiliang Lyu, Dufan Wu, Ning Guo, Xiang Li, Quanzheng Li)</author>
      <guid isPermaLink="false">2505.04899v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Representing spherical tensors with scalar-based machine-learning models</title>
      <link>http://arxiv.org/abs/2505.05404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在物理学中旋转对称性的重要作用，以及如何通过旋转群的结构来描述三维物体在刚体旋转作用下的性质变化。&lt;h4&gt;背景&lt;/h4&gt;旋转对称性在物理学中扮演核心角色，它提供了一个优雅的框架来描述从原子到宏观尺度三维物体的性质在刚体旋转作用下的变换。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索一种新的方法来解决旋转对称性学习问题，该方法通过将等变函数表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积来实现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的方法，其中等变函数被表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积，同时提出了对通用表达式的近似，这些近似虽然缺乏通用逼近能力，但速度快、易于实现且在实际应用中准确。&lt;h4&gt;主要发现&lt;/h4&gt;发现了一种新的等变函数表示方法，该方法通过将标量函数和具有适当对称性的小张量基相乘来处理旋转对称性学习问题。&lt;h4&gt;结论&lt;/h4&gt;本文提出的近似方法在处理旋转对称性学习问题时既快速又简单，同时在实际应用中保持了较高的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：旋转对称性在物理学中起着核心作用，提供了一个优雅的框架来描述三维物体（从原子到宏观尺度）在刚体旋转作用下的性质变化。三维点云的等变模型能够以与旋转群结构完全一致的方式近似结构-性质关系，通过结合自身为球张量的中间表示。然而，对称性约束使得这种方法在计算上具有挑战性，且实现起来繁琐，这促使越来越受欢迎的无约束架构在训练过程中学习近似对称性。在本工作中，我们探索了第三种解决学习问题的途径，其中等变函数被表示为点云坐标的标量函数与具有适当对称性的小张量基的乘积。我们还提出了对通用表达式的近似，虽然缺乏通用逼近能力，但这些近似方法快速、易于实现，且在实际设置中准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rotational symmetry plays a central role in physics, providing an elegantframework to describe how the properties of 3D objects -- from atoms to themacroscopic scale -- transform under the action of rigid rotations. Equivariantmodels of 3D point clouds are able to approximate structure-property relationsin a way that is fully consistent with the structure of the rotation group, bycombining intermediate representations that are themselves spherical tensors.The symmetry constraints however make this approach computationally demandingand cumbersome to implement, which motivates increasingly popular unconstrainedarchitectures that learn approximate symmetries as part of the trainingprocess. In this work, we explore a third route to tackle this learningproblem, where equivariant functions are expressed as the product of a scalarfunction of the point cloud coordinates and a small basis of tensors with theappropriate symmetry. We also propose approximations of the general expressionsthat, while lacking universal approximation properties, are fast, simple toimplement, and accurate in practical settings.</description>
      <author>example@mail.com (Michelangelo Domina, Filippo Bigi, Paolo Pegolo, Michele Ceriotti)</author>
      <guid isPermaLink="false">2505.05404v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Clustering with Communication: A Variational Framework for Single Cell Representation Learning</title>
      <link>http://arxiv.org/abs/2505.04891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CCCVAE的新型变分自动编码器框架，该框架将细胞间通讯信号纳入单细胞表征学习中，通过实验证明其能提高单细胞分析的聚类性能。&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序（scRNA-seq）揭示了细胞异质性，但理解生物功能还需要考虑细胞间通讯（CCC），即通过配体-受体对介导的信号交互来协调细胞行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将细胞间通讯信号纳入单细胞表征学习的方法，以更好地理解细胞行为。&lt;h4&gt;方法&lt;/h4&gt;提出CCCVAE框架，利用从配体-受体相互作用中导出的通讯感知核和稀疏高斯过程，将生物学信息先验编码到潜在空间中。与传统的独立处理每个细胞的VAE不同，CCCVAE鼓励潜在嵌入反映转录相似性和细胞间通讯背景。&lt;h4&gt;主要发现&lt;/h4&gt;在四个scRNA-seq数据集上的实证结果表明，CCCVAE提高了聚类性能，其评估分数高于标准的VAE基线。&lt;h4&gt;结论&lt;/h4&gt;将生物学先验嵌入深度生成模型对于无监督单细胞分析具有价值。&lt;h4&gt;翻译&lt;/h4&gt;Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular heterogeneity, but recent studies emphasize that understanding biological function also requires modeling cell-cell communication (CCC), the signaling interactions mediated by ligand-receptor pairs that coordinate cellular behavior. Tools like CellChat have demonstrated that CCC plays a critical role in processes such as cell differentiation, tissue regeneration, and immune response, and that transcriptomic data inherently encodes rich information about intercellular signaling. We propose CCCVAE, a novel variational autoencoder framework that incorporates CCC signals into single-cell representation learning. By leveraging a communication-aware kernel derived from ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes biologically informed priors into the latent space. Unlike conventional VAEs that treat each cell independently, CCCVAE encourages latent embeddings to reflect both transcriptional similarity and intercellular signaling context. Empirical results across four scRNA-seq datasets show that CCCVAE improves clustering performance, achieving higher evaluation scores than standard VAE baselines. This work demonstrates the value of embedding biological priors into deep generative models for unsupervised single-cell analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) has revealed complex cellularheterogeneity, but recent studies emphasize that understanding biologicalfunction also requires modeling cell-cell communication (CCC), the signalinginteractions mediated by ligand-receptor pairs that coordinate cellularbehavior. Tools like CellChat have demonstrated that CCC plays a critical rolein processes such as cell differentiation, tissue regeneration, and immuneresponse, and that transcriptomic data inherently encodes rich informationabout intercellular signaling. We propose CCCVAE, a novel variationalautoencoder framework that incorporates CCC signals into single-cellrepresentation learning. By leveraging a communication-aware kernel derivedfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodesbiologically informed priors into the latent space. Unlike conventional VAEsthat treat each cell independently, CCCVAE encourages latent embeddings toreflect both transcriptional similarity and intercellular signaling context.Empirical results across four scRNA-seq datasets show that CCCVAE improvesclustering performance, achieving higher evaluation scores than standard VAEbaselines. This work demonstrates the value of embedding biological priors intodeep generative models for unsupervised single-cell analysis.</description>
      <author>example@mail.com (Cong Qi, Yeqing Chen, Jie Zhang, Wei Zhi)</author>
      <guid isPermaLink="false">2505.04891v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning</title>
      <link>http://arxiv.org/abs/2505.05062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉基础模型在长尾半监督学习（LTSSL）中的影响，提出了一种新的轻量级微调策略ULFine，以降低训练成本并提高预测准确性。&lt;h4&gt;背景&lt;/h4&gt;基于CLIP等大规模视觉基础模型在下游任务中的成功，本文旨在探索这些模型对LTSSL的影响。&lt;h4&gt;目的&lt;/h4&gt;分析大规模视觉基础模型对LTSSL的影响，并提出一种新的微调策略。&lt;h4&gt;方法&lt;/h4&gt;采用三种策略（线性探测（LP）、轻量级微调（LFT）和全量微调（FFT））对基础模型进行探索。&lt;h4&gt;主要发现&lt;/h4&gt;FFT导致模型性能下降，LP和LFT虽然提高了整体模型性能，但对长尾类别的影响微乎其微。LP由于训练数据未充分学习产生大量错误伪标签，而LFT减少了这些错误标签的数量，但训练数据偏差导致对它们过度自信。&lt;h4&gt;结论&lt;/h4&gt;ULFine策略通过文本原型自信度感知的适应性调整和双对数互补融合，减轻了过度自信，并对抗了伪标签和分类器偏差，显著降低了训练成本并提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;基于大规模视觉基础模型（如CLIP）在各种下游任务中的成功，本文最初尝试通过采用基础模型的三种策略（线性探测（LP）、轻量级微调（LFT）和全量微调（FFT））来探索其对长尾半监督学习（LTSSL）的影响。我们的分析提出了以下见解：i）与从头开始训练的LTSSL算法相比，FFT导致模型性能下降，而LP和LFT虽然提高了整体模型性能，但对长尾类别的益处微乎其微。ii）LP由于未充分学习的训练数据而产生大量错误伪标签，而LFT可以减少这些错误标签的数量，但由于训练数据偏差，对它们变得过度自信。这加剧了LTSSL中固有的伪标签和分类器偏差，限制了长尾类别的性能提升。基于这些见解，我们提出了一种无偏轻量级微调策略，extbf{ULFine}，通过文本原型自信度感知的适应性调整减轻过度自信，并通过双对数互补融合对抗伪标签和分类器偏差。大量实验表明，与最先进的方法相比，ULFine显著降低了训练成本超过十倍，并大幅提高了预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Based on the success of large-scale visual foundation models like CLIP invarious downstream tasks, this paper initially attempts to explore their impacton Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundationmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning(LFT), and Full Fine-Tuning (FFT). Our analysis presents the followinginsights: i) Compared to LTSSL algorithms trained from scratch, FFT results ina decline in model performance, whereas LP and LFT, although boosting overallmodel performance, exhibit negligible benefits to tail classes. ii) LP producesnumerous false pseudo-labels due to \textit{underlearned} training data, whileLFT can reduce the number of these false labels but becomes overconfident aboutthem owing to \textit{biased fitting} training data. This exacerbates thepseudo-labeled and classifier biases inherent in LTSSL, limiting performanceimprovement in the tail classes. With these insights, we propose a UnbiasedLightweight Fine-tuning strategy, \textbf{ULFine}, which mitigates theoverconfidence via confidence-aware adaptive fitting of textual prototypes andcounteracts the pseudo-labeled and classifier biases via complementary fusionof dual logits. Extensive experiments demonstrate that ULFine markedlydecreases training costs by over ten times and substantially increasesprediction accuracies compared to state-of-the-art methods.</description>
      <author>example@mail.com (Enhao Zhang, Chaohua Li, Chuanxing Geng, Songcan Chen)</author>
      <guid isPermaLink="false">2505.05062v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.05049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于贝叶斯熵公式的理论动机不确定性量化模型，用于解决Segment Anything Model（SAM）的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;SAM模型在语义分割应用中取得了成功，但其不确定性量化（UQ）方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的不确定性量化模型，用于量化SAM的不确定性。&lt;h4&gt;方法&lt;/h4&gt;基于贝叶斯熵公式，提出了一种名为USAM的轻量级后处理UQ方法，并分析了不确定性来源。&lt;h4&gt;主要发现&lt;/h4&gt;USAM模型在SA-V、MOSE、ADE20k、DAVIS和COCO数据集上表现出优异的预测能力，为UQ提供了一种计算成本低、易于使用的替代方案。&lt;h4&gt;结论&lt;/h4&gt;USAM模型能够支持用户提示、增强半监督流程或平衡准确性与成本效率之间的权衡。&lt;h4&gt;翻译&lt;/h4&gt;The introduction of the Segment Anything Model (SAM) has paved the way for numerous semantic segmentation applications. For several tasks, quantifying the uncertainty of SAM is of particular interest. However, the ambiguous nature of the class-agnostic foundation model SAM challenges current uncertainty quantification (UQ) approaches. This paper presents a theoretically motivated uncertainty quantification model based on a Bayesian entropy formulation jointly respecting aleatoric, epistemic, and the newly introduced task uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ method. Our model traces the root of uncertainty back to under-parameterised models, insufficient prompts or image ambiguities. Our proposed deterministic USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ alternative that can support user-prompting, enhance semi-supervised pipelines, or balance the tradeoff between accuracy and cost efficiency.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The introduction of the Segment Anything Model (SAM) has paved the way fornumerous semantic segmentation applications. For several tasks, quantifying theuncertainty of SAM is of particular interest. However, the ambiguous nature ofthe class-agnostic foundation model SAM challenges current uncertaintyquantification (UQ) approaches. This paper presents a theoretically motivateduncertainty quantification model based on a Bayesian entropy formulationjointly respecting aleatoric, epistemic, and the newly introduced taskuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQmethod. Our model traces the root of uncertainty back to under-parameterisedmodels, insufficient prompts or image ambiguities. Our proposed deterministicUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQalternative that can support user-prompting, enhance semi-supervised pipelines,or balance the tradeoff between accuracy and cost efficiency.</description>
      <author>example@mail.com (Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2505.05049v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes</title>
      <link>http://arxiv.org/abs/2505.05288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech report. Project page: https://nianticlabs.github.io/placeit3d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的任务：在真实3D场景中进行语言引导的物体放置。该任务通过文本提示来指导3D资产的放置，并提出了新的基准和评估协议。&lt;h4&gt;背景&lt;/h4&gt;在3D场景中进行语言引导的定位任务存在挑战，如放置位置的模糊性和对3D几何关系和自由空间的理解需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够根据文本提示在真实3D场景中放置3D资产的模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的基准和评估协议，引入了一个新的数据集用于训练3D语言大模型，并提出了第一个非平凡基线方法。&lt;h4&gt;主要发现&lt;/h4&gt;该任务具有多解性，需要考虑3D几何关系和自由空间。&lt;h4&gt;结论&lt;/h4&gt;该任务和新的基准有望成为评估和比较通用3D语言大模型的标准之一。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了在真实3D场景中进行语言引导的物体放置的新任务。我们的模型被给定一个3D场景的点云、一个3D资产和一个文本提示，该提示广泛描述了3D资产应该放置的位置。这里的任务是找到尊重提示的有效放置。与3D场景中的其他语言引导定位任务（如基础）相比，这个任务具有特定的挑战：它是不确定的，因为它有多个有效解，并且需要推理3D几何关系和自由空间。我们通过提出一个新的基准和评估协议来启动这个任务。我们还介绍了一个新的数据集，用于在这个任务上训练3D LLMs，以及第一个作为非平凡基线的方法。我们认为这个具有挑战性的任务和我们的新基准可以成为用于评估和比较通用3D LLM模型的标准工具包的一部分。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the novel task of Language-Guided Object Placement in Real 3DScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textualprompt broadly describing where the 3D asset should be placed. The task here isto find a valid placement for the 3D asset that respects the prompt. Comparedwith other language-guided localization tasks in 3D scenes such as grounding,this task has specific challenges: it is ambiguous because it has multiplevalid solutions, and it requires reasoning about 3D geometric relationships andfree space. We inaugurate this task by proposing a new benchmark and evaluationprotocol. We also introduce a new dataset for training 3D LLMs on this task, aswell as the first method to serve as a non-trivial baseline. We believe thatthis challenging task and our new benchmark could become part of the suite ofbenchmarks used to evaluate and compare generalist 3D LLM models.</description>
      <author>example@mail.com (Ahmed Abdelreheem, Filippo Aleotti, Jamie Watson, Zawar Qureshi, Abdelrahman Eldesokey, Peter Wonka, Gabriel Brostow, Sara Vicente, Guillermo Garcia-Hernando)</author>
      <guid isPermaLink="false">2505.05288v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and Task-Scalable Control Architecture</title>
      <link>http://arxiv.org/abs/2505.04980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的视觉-语言模型（LVLM）与模型预测控制（MPC）集成框架，该框架为自动驾驶（AD）提供了任务可扩展性和安全性。&lt;h4&gt;背景&lt;/h4&gt;LVLM在多样化的驾驶场景中擅长高级任务规划，但这些基础模型并非专门为驾驶设计，其推理与低级运动规划的可行性不一致，因此存在安全和任务切换流畅性的担忧。&lt;h4&gt;目的&lt;/h4&gt;集成LVLM与MPC Builder，自动根据LVLM生成的符号任务命令生成MPC，同时确保最优性和安全性。&lt;h4&gt;方法&lt;/h4&gt;生成的MPC可以提供关于任务可行性的反馈，并生成任务切换感知的MPC，从而有效地辅助或拒绝LVLM驱动的任务切换。&lt;h4&gt;主要发现&lt;/h4&gt;该方法提供了一个安全、灵活和适应性强的控制框架，弥合了尖端基础模型与可靠车辆操作之间的差距。&lt;h4&gt;结论&lt;/h4&gt;通过仿真实验验证了该方法的有效性，结果表明该系统可以在高速公路驾驶中安全有效地操作，同时保持LVLM的灵活性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的视觉-语言模型（LVLM）与模型预测控制（MPC）集成框架，该框架为自动驾驶（AD）提供了任务可扩展性和安全性。LVLM在多样化的驾驶场景中擅长高级任务规划，但这些基础模型并非专门为驾驶设计，其推理与低级运动规划的可行性不一致，因此存在安全和任务切换流畅性的担忧。本文集成LVLM与MPC Builder，自动根据LVLM生成的符号任务命令生成MPC，同时确保最优性和安全性。生成的MPC可以提供关于任务可行性的反馈，并生成任务切换感知的MPC，从而有效地辅助或拒绝LVLM驱动的任务切换。该方法提供了一个安全、灵活和适应性强的控制框架，弥合了尖端基础模型与可靠车辆操作之间的差距。通过仿真实验验证了该方法的有效性，结果表明该系统可以在高速公路驾驶中安全有效地操作，同时保持LVLM的灵活性和适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel Large Vision-Language Model (LVLM) and ModelPredictive Control (MPC) integration framework that delivers both taskscalability and safety for Autonomous Driving (AD). LVLMs excel at high-leveltask planning across diverse driving scenarios. However, since these foundationmodels are not specifically designed for driving and their reasoning is notconsistent with the feasibility of low-level motion planning, concerns remainregarding safety and smooth task switching. This paper integrates LVLMs withMPC Builder, which automatically generates MPCs on demand, based on symbolictask commands generated by the LVLM, while ensuring optimality and safety. Thegenerated MPCs can strongly assist the execution or rejection of LVLM-driventask switching by providing feedback on the feasibility of the given tasks andgenerating task-switching-aware MPCs. Our approach provides a safe, flexible,and adaptable control framework, bridging the gap between cutting-edgefoundation models and reliable vehicle operation. We demonstrate theeffectiveness of our approach through a simulation experiment, showing that oursystem can safely and effectively handle highway driving while maintaining theflexibility and adaptability of LVLMs.</description>
      <author>example@mail.com (Kazuki Atsuta, Kohei Honda, Hiroyuki Okuda, Tatsuya Suzuki)</author>
      <guid isPermaLink="false">2505.04980v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2505.04965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DenseGrounding的新方法，用于使智能代理通过自然语言理解和交互3D环境，该方法在ego-centric 3D visual grounding任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;理解3D环境并通过自然语言与之互动对于推动机器人和人机交互至关重要。ego-centric 3D visual grounding任务要求代理根据口头描述在真实世界的3D空间中定位目标对象。&lt;h4&gt;目的&lt;/h4&gt;解决ego-centric 3D visual grounding任务中存在的两个主要挑战：(1) 由于点云与ego-centric多视图图像融合的稀疏性导致的细粒度视觉语义丢失；(2) 由于任意语言描述而导致的有限的文本语义上下文。&lt;h4&gt;方法&lt;/h4&gt;DenseGrounding通过增强视觉和文本语义来解决上述问题。对于视觉特征，引入了层次场景语义增强器（Hierarchical Scene Semantic Enhancer），通过捕捉细粒度的全局场景特征并促进跨模态对齐来保留密集的语义。对于文本描述，提出了语言语义增强器（Language Semantic Enhancer），利用大型语言模型提供丰富的上下文和多样化的语言描述，并在模型训练期间增加额外的上下文。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，DenseGrounding在整体准确率方面显著优于现有方法，在全数据集和较小的迷你子集上分别提高了5.81%和7.56%，进一步推动了ego-centric 3D visual grounding的SOTA（最先进的技术水平）。该方法在CVPR 2024自主挑战赛的多视图3D visual grounding赛道中获得了第一名并获得了创新奖，验证了其有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;DenseGrounding是一种有效的ego-centric 3D visual grounding方法，能够显著提高准确率，并在实际竞赛中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling intelligent agents to comprehend and interact with 3D environmentsthrough natural language is crucial for advancing robotics and human-computerinteraction. A fundamental task in this field is ego-centric 3D visualgrounding, where agents locate target objects in real-world 3D spaces based onverbal descriptions. However, this task faces two significant challenges: (1)loss of fine-grained visual semantics due to sparse fusion of point clouds withego-centric multi-view images, (2) limited textual semantic context due toarbitrary language descriptions. We propose DenseGrounding, a novel approachdesigned to address these issues by enhancing both visual and textualsemantics. For visual features, we introduce the Hierarchical Scene SemanticEnhancer, which retains dense semantics by capturing fine-grained global scenefeatures and facilitating cross-modal alignment. For text descriptions, wepropose a Language Semantic Enhancer that leverages large language models toprovide rich context and diverse language descriptions with additional contextduring model training. Extensive experiments show that DenseGroundingsignificantly outperforms existing methods in overall accuracy, withimprovements of 5.81% and 7.56% when trained on the comprehensive full datasetand smaller mini subset, respectively, further advancing the SOTA in egocentric3D visual grounding. Our method also achieves 1st place and receives theInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3DVisual Grounding Track, validating its effectiveness and robustness.</description>
      <author>example@mail.com (Henry Zheng, Hao Shi, Qihang Peng, Yong Xien Chng, Rui Huang, Yepeng Weng, Zhongchao Shi, Gao Huang)</author>
      <guid isPermaLink="false">2505.04965v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization</title>
      <link>http://arxiv.org/abs/2505.04880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为GroverGPT-2的基于大型语言模型（LLM）的方法，用于模拟Grover算法，并通过实验验证了经典模型能够捕捉量子算法结构的能力。&lt;h4&gt;背景&lt;/h4&gt;量子计算在特定任务上理论上优于经典计算，但实际量子优势的边界尚不明确。研究经典机器学习模拟量子算法的能力对于理解这一边界至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究经典机器学习模拟量子算法的能力，特别是探索大型语言模型在模拟Grover算法方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出GroverGPT-2，一种基于LLM的方法，使用思维链（CoT）推理和量子本地标记化来模拟Grover算法。GroverGPT-2能够直接从量子电路表示中执行模拟，并生成逻辑结构和可解释的输出。&lt;h4&gt;主要发现&lt;/h4&gt;GroverGPT-2能够通过有效处理量子本地标记来学习和内化量子电路逻辑，提供了经典模型如LLM能够捕捉量子算法结构的直接证据。GroverGPT-2的输出将电路数据与自然语言交织，将明确的推理嵌入到模拟中。此外，还发现GroverGPT-2的实证缩放定律与量子比特数量增加有关，为可扩展的经典模拟提供了一条路径。&lt;h4&gt;结论&lt;/h4&gt;这些发现为探索经典模拟的极限、提高量子教育与研究，以及为量子计算中的未来基础模型奠定基础开辟了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：量子计算在特定任务上相对于经典计算具有理论优势，然而实际量子优势的边界仍是一个未解之谜。为了研究这个边界，了解经典机器是否能以及如何学习模拟量子算法至关重要。近年来大型语言模型（LLM）在推理能力上的进步，促使人们探索其在这一挑战中的潜力。在这项工作中，我们引入了GroverGPT-2，这是一种基于LLM的方法，利用思维链（CoT）推理和量子本地标记化来模拟Grover算法。在先前的作品基础上，GroverGPT-2可以直接从量子电路表示中进行模拟，并产生逻辑结构化和可解释的输出。我们的结果表明，GroverGPT-2可以通过高效处理量子本地标记来学习和内化量子电路逻辑，为经典模型如LLM能够捕捉量子算法结构提供了直接证据。此外，GroverGPT-2将电路数据与自然语言交织，将明确的推理嵌入到模拟中。这种双重能力使GroverGPT-2成为提高机器对量子算法理解和建模量子电路逻辑的原型。我们还确定了GroverGPT-2随量子比特数量增加的实证缩放定律，为可扩展的经典模拟指明了一条路径。这些发现为探索经典模拟的极限、增强量子教育和研究，以及为量子计算中的未来基础模型奠定基础开辟了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum computing offers theoretical advantages over classical computing forspecific tasks, yet the boundary of practical quantum advantage remains an openquestion. To investigate this boundary, it is crucial to understand whether,and how, classical machines can learn and simulate quantum algorithms. Recentprogress in large language models (LLMs) has demonstrated strong reasoningabilities, prompting exploration into their potential for this challenge. Inthis work, we introduce GroverGPT-2, an LLM-based method for simulatingGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-nativetokenization. Building on its predecessor, GroverGPT-2 performs simulationdirectly from quantum circuit representations while producing logicallystructured and interpretable outputs. Our results show that GroverGPT-2 canlearn and internalize quantum circuit logic through efficient processing ofquantum-native tokens, providing direct evidence that classical models likeLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2outputs interleave circuit data with natural language, embedding explicitreasoning into the simulation. This dual capability positions GroverGPT-2 as aprototype for advancing machine understanding of quantum algorithms andmodeling quantum circuit logic. We also identify an empirical scaling law forGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalableclassical simulation. These findings open new directions for exploring thelimits of classical simulatability, enhancing quantum education and research,and laying groundwork for future foundation models in quantum computing.</description>
      <author>example@mail.com (Min Chen, Jinglei Cheng, Pingzhi Li, Haoran Wang, Tianlong Chen, Junyu Liu)</author>
      <guid isPermaLink="false">2505.04880v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model</title>
      <link>http://arxiv.org/abs/2505.04861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mix-QSAM的混合精度后训练量化框架，用于提高Segment Anything Model (SAM)在资源受限设备上的部署性能。&lt;h4&gt;背景&lt;/h4&gt;SAM是一个流行的视觉基础模型，但由于其计算和内存需求高，在资源受限设备上的部署面临挑战。&lt;h4&gt;目的&lt;/h4&gt;针对现有后训练量化方法依赖于固定位宽量化导致的精度和效率问题，提出Mix-QSAM框架以提高模型的性能。&lt;h4&gt;方法&lt;/h4&gt;Mix-QSAM引入了层间重要性评分和跨层协同度量，用于量化层贡献和捕捉相邻层之间的依赖关系。通过这些度量，构建了整数二次规划问题，以确定模型大小和位操作约束下的最优位宽分配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Mix-QSAM在实例分割和目标检测任务上优于现有PTQ方法，在6位和4位混合精度设置下，平均精度提高可达20%，同时保持了计算效率。&lt;h4&gt;结论&lt;/h4&gt;Mix-QSAM通过优化位宽分配，有效提高了SAM在资源受限设备上的部署性能，同时保持了模型的精度和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Segment Anything Model (SAM) is a popular vision foundation model;however, its high computational and memory demands make deployment onresource-constrained devices challenging. While Post-Training Quantization(PTQ) is a practical approach for reducing computational overhead, existing PTQmethods rely on fixed bit-width quantization, leading to suboptimal accuracyand efficiency. To address this limitation, we propose Mix-QSAM, amixed-precision PTQ framework for SAM. First, we introduce a layer-wiseimportance score, derived using Kullback-Leibler (KL) divergence, to quantifyeach layer's contribution to the model's output. Second, we introducecross-layer synergy, a novel metric based on causal mutual information, tocapture dependencies between adjacent layers. This ensures that highlyinterdependent layers maintain similar bit-widths, preventing abrupt precisionmismatches that degrade feature propagation and numerical stability. Usingthese metrics, we formulate an Integer Quadratic Programming (IQP) problem todetermine optimal bit-width allocation under model size and bit-operationconstraints, assigning higher precision to critical layers while minimizingbit-width in less influential layers. Experimental results demonstrate thatMix-QSAM consistently outperforms existing PTQ methods on instance segmentationand object detection tasks, achieving up to 20% higher average precision under6-bit and 4-bit mixed-precision settings, while maintaining computationalefficiency.</description>
      <author>example@mail.com (Navin Ranjan, Andreas Savakis)</author>
      <guid isPermaLink="false">2505.04861v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
      <link>http://arxiv.org/abs/2504.21435v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 15 figures, CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SeriesBench的基准测试，用于评估多模态大型语言模型在理解叙事驱动视频系列方面的能力。&lt;h4&gt;背景&lt;/h4&gt;随着多模态大型语言模型的快速发展，现有的基准测试主要关注独立视频和视觉元素，如人类动作和物体状态，而忽略了视频系列中的复杂连续叙事。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，提出了SeriesBench，旨在评估模型对叙事驱动视频系列的理解能力。&lt;h4&gt;方法&lt;/h4&gt;SeriesBench包含105个精心挑选的叙事驱动系列，涵盖28个需要深度叙事理解的专项任务。它采用了新颖的长篇叙事标注方法和全信息转换方法，以及一个名为PC-DCoT的叙事推理框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，现有的MLLMs在理解叙事驱动系列方面仍面临重大挑战，而PC-DCoT能够帮助这些MLLMs实现性能提升。&lt;h4&gt;结论&lt;/h4&gt;SeriesBench和PC-DCoT强调了提升模型理解叙事驱动系列能力的重要性，为MLLMs的未来发展提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess 'visual elements' like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zackhxn/seriesbench-cvpr2025&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onstandalone videos and mainly assess "visual elements" like human actions andobject states. In reality, contemporary videos often encompass complex andcontinuous narratives, typically presented as a series. To address thischallenge, we propose SeriesBench, a benchmark consisting of 105 carefullycurated narrative-driven series, covering 28 specialized tasks that requiredeep narrative understanding. Specifically, we first select a diverse set ofdrama series spanning various genres. Then, we introduce a novel long-spannarrative annotation method, combined with a full-information transformationapproach to convert manual annotations into diverse task formats. To furtherenhance model capacity for detailed analysis of plot structures and characterrelationships within series, we propose a novel narrative reasoning framework,PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs stillface significant challenges in understanding narrative-driven series, whilePC-DCoT enables these MLLMs to achieve performance improvements. Overall, ourSeriesBench and PC-DCoT highlight the critical necessity of advancing modelcapabilities to understand narrative-driven series, guiding the futuredevelopment of MLLMs. SeriesBench is publicly available athttps://github.com/zackhxn/SeriesBench-CVPR2025.</description>
      <author>example@mail.com (Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2504.21435v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</title>
      <link>http://arxiv.org/abs/2505.04813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://threedle.github.io/wir3d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WIR3D的技术，通过在三维空间中提取一组视觉上有意义的曲线来抽象三维形状。&lt;h4&gt;背景&lt;/h4&gt;WIR3D技术旨在通过曲线来表示三维形状的几何和视觉特征。&lt;h4&gt;目的&lt;/h4&gt;目的是为了能够从任意视角忠实地表示形状的几何和显著视觉特征，如纹理。&lt;h4&gt;方法&lt;/h4&gt;方法包括优化贝塞尔曲线的参数，利用预训练的基础模型（CLIP）的中间激活来指导优化过程，将优化分为两个阶段：一个用于捕捉形状的粗略几何形状，另一个用于表示细粒度特征。第二个阶段通过一个新颖的局部关键点损失进行空间引导，确保对原始表面的忠实度通过神经SDF损失。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，该方法能够成功应用于具有不同复杂度、几何结构和纹理的形状的大数据集，并展示了特征控制和形状变形的下游应用。&lt;h4&gt;结论&lt;/h4&gt;结论是WIR3D技术能够有效地抽象三维形状，并在实际应用中表现出良好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present WIR3D, a technique for abstracting 3D shapes through a sparse setof visually meaningful curves in 3D. We optimize the parameters of Beziercurves such that they faithfully represent both the geometry and salient visualfeatures (e.g. texture) of the shape from arbitrary viewpoints. We leverage theintermediate activations of a pre-trained foundation model (CLIP) to guide ouroptimization process. We divide our optimization into two phases: one forcapturing the coarse geometry of the shape, and the other for representingfine-grained features. Our second phase supervision is spatially guided by anovel localized keypoint loss. This spatial guidance enables user control overabstracted features. We ensure fidelity to the original surface through aneural SDF loss, which allows the curves to be used as intuitive deformationhandles. We successfully apply our method for shape abstraction over a broaddataset of shapes with varying complexity, geometric structure, and texture,and demonstrate downstream applications for feature control and shapedeformation.</description>
      <author>example@mail.com (Richard Liu, Daniel Fu, Noah Tan, Itai Lang, Rana Hanocka)</author>
      <guid isPermaLink="false">2505.04813v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</title>
      <link>http://arxiv.org/abs/2505.04802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ORBIT-2，一种用于全球超分辨率气候降尺度的可扩展基础模型，旨在解决现有方法在变量和地理范围泛化能力有限以及ViT自注意力二次复杂度约束的问题。&lt;h4&gt;背景&lt;/h4&gt;稀疏观测和粗分辨率气候模型限制了区域决策的有效性，强调了稳健降尺度的必要性。&lt;h4&gt;目的&lt;/h4&gt;提出ORBIT-2模型，以实现高效、鲁棒的预测，并降低自注意力复杂度，支持长序列处理和大规模并行计算。&lt;h4&gt;方法&lt;/h4&gt;ORBIT-2包含两个关键创新：(1) Residual Slim ViT (Reslim)，一种轻量级架构，结合残差学习和贝叶斯正则化；(2) TILES，一种分块序列缩放算法，将自注意力复杂度从二次降低到线性。&lt;h4&gt;主要发现&lt;/h4&gt;ORBIT-2可以扩展到10亿参数，在32,768个GPU上运行，实现高达1.8 ExaFLOPS的持续吞吐量和92-98%的强扩展效率。它可以支持0.9公里全球分辨率的降尺度，并处理高达42亿个标记的序列。在7公里分辨率的基准测试中，ORBIT-2与观测数据相比，实现了0.98到0.99的R^2分数的高精度。&lt;h4&gt;结论&lt;/h4&gt;ORBIT-2模型在提高气候降尺度精度和效率方面具有显著优势，为区域决策提供了强有力的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse observations and coarse-resolution climate models limit effectiveregional decision-making, underscoring the need for robust downscaling.However, existing AI methods struggle with generalization across variables andgeographies and are constrained by the quadratic complexity of VisionTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundationmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporatestwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecturewith residual learning and Bayesian regularization for efficient, robustprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reducesself-attention complexity from quadratic to linear, enabling long-sequenceprocessing and massive parallelism. ORBIT-2 scales to 10 billion parametersacross 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and92-98% strong scaling efficiency. It supports downscaling to 0.9 km globalresolution and processes sequences up to 4.2 billion tokens. On 7 km resolutionbenchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98to 0.99 against observation data.</description>
      <author>example@mail.com (Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, Ming Fan, Nasik Muhammad Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu)</author>
      <guid isPermaLink="false">2505.04802v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2505.04911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了SpatialPrompting，这是一种新型框架，利用现成多模态大型语言模型的涌现推理能力，在三维（3D）环境中实现零样本空间推理。&lt;h4&gt;背景&lt;/h4&gt;现有方法依赖昂贵的3D特定微调，使用如点云或基于体素的特征等专业的3D输入。&lt;h4&gt;目的&lt;/h4&gt;旨在提供一种利用直观视觉和位置线索的灵活空间推理新范式。&lt;h4&gt;方法&lt;/h4&gt;SpatialPrompting采用关键帧驱动的提示生成策略，使用视觉语言相似度、马氏距离、视场和图像锐度等指标，从图像序列中选择多样化的关键帧，并与相应的相机位姿数据集成，以有效地抽象空间关系并推断复杂的3D结构。&lt;h4&gt;主要发现&lt;/h4&gt;该框架不仅建立了一种利用直观视觉和位置线索的灵活空间推理新范式，而且在基准数据集（如ScanQA和SQA3D）上实现了最先进的零样本性能，并在多个指标上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地消除了对专业3D输入和微调的需求，提供了一种比传统方法更简单、更可扩展的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large languagemodels to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces SpatialPrompting, a novel framework that harnesses theemergent reasoning capabilities of off-the-shelf multimodal large languagemodels to achieve zero-shot spatial reasoning in three-dimensional (3D)environments. Unlike existing methods that rely on expensive 3D-specificfine-tuning with specialized 3D inputs such as point clouds or voxel-basedfeatures, SpatialPrompting employs a keyframe-driven prompt generationstrategy. This framework uses metrics such as vision-language similarity,Mahalanobis distance, field of view, and image sharpness to select a diverseand informative set of keyframes from image sequences and then integrates themwith corresponding camera pose data to effectively abstract spatialrelationships and infer complex 3D structures. The proposed framework not onlyestablishes a new paradigm for flexible spatial reasoning that utilizesintuitive visual and positional cues but also achieves state-of-the-artzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, acrossseveral metrics. The proposed method effectively eliminates the need forspecialized 3D inputs and fine-tuning, offering a simpler and more scalablealternative to conventional approaches.</description>
      <author>example@mail.com (Shun Taguchi, Hideki Deguchi, Takumi Hamazaki, Hiroyuki Sakai)</author>
      <guid isPermaLink="false">2505.04911v1</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MAISY的新方法，用于消除医学图像采集过程中因患者运动导致的模糊、鬼影和器官扭曲问题，从而提高图像解释的准确性。&lt;h4&gt;背景&lt;/h4&gt;患者运动导致医学图像模糊，影响图像解读。现有的基于生成对抗网络（GAN）的算法虽然能生成无运动图像，但存在忽视局部特征和难以处理像素强度、亮度因素及方差变化的问题。&lt;h4&gt;目的&lt;/h4&gt;提出MAISY方法，旨在更好地处理运动模糊，并保留关键病理信息。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法通过以下步骤实现：（a）利用Segment Anything Model（SAM）动态学习运动伪影最明显的解剖边界处的空间模式；（b）引入变差选择性的结构相似性指数（VS-SSIM）损失，自适应地强调高像素变差的区域，以保留必要的解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;在胸部和头部CT数据集上的实验表明，MAISY模型在峰值信噪比（PSNR）、结构相似性（SSIM）和Dice系数上均优于现有算法，PSNR提高40%，SSIM提高10%，Dice系数提高16%。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法在处理运动模糊和提高医学图像质量方面表现出色，为医学图像分析提供了有效工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging. Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v2</guid>
      <pubDate>Fri, 09 May 2025 14:15:34 +0800</pubDate>
    </item>
    <item>
      <title>HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems</title>
      <link>http://arxiv.org/abs/2505.03140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Hamiltonian-Masked Autoencoding (HMAE)框架，用于解决量子机器学习中标签数据稀缺和模拟计算昂贵的问题。&lt;h4&gt;背景&lt;/h4&gt;量子机器学习在处理自旋和分子系统时面临数据稀缺和计算成本高的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出HMAE框架，通过在未标记的量子哈密顿量上预训练transformer，实现高效的少样本迁移学习。&lt;h4&gt;方法&lt;/h4&gt;HMAE采用基于量子信息理论的物理信息策略，根据哈密顿项的物理意义进行选择性掩码。&lt;h4&gt;主要发现&lt;/h4&gt;在12,500个量子哈密顿量上的实验表明，HMAE在相分类中达到85.3% ± 1.5%的准确率，在基态能量预测中达到0.15 ± 0.02 eV的MAE，仅使用10个标记示例。&lt;h4&gt;结论&lt;/h4&gt;HMAE具有出色的样本效率，比基线方法减少了3-5倍的标记示例需求，但该方法目前限于小型量子系统，不能直接应用于材料科学和量子化学中感兴趣的大系统。&lt;h4&gt;翻译&lt;/h4&gt;Quantum machine learning for spin and molecular systems faces criticalchallenges of scarce labeled data and computationally expensive simulations. Toaddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),a novel self-supervised framework that pre-trains transformers on unlabeledquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike randommasking approaches, HMAE employs a physics-informed strategy based onquantum information theory to selectively mask Hamiltonian terms based on theirphysical significance. Experiments on 12,500 quantum Hamiltonians (60%real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% ± 1.5%accuracy in phase classification and 0.15 ± 0.02 eV MAE in ground stateenergy prediction with merely 10 labeled examples - a statistically significantimprovement (p &lt; 0.01) over classical graph neural networks (78.1% ± 2.1%)and quantum neural networks (76.8% ± 2.3%). Our method's primary advantageis exceptional sample efficiency - reducing required labeled examples by 3-5xcompared to baseline methods - though we emphasize that ground truth values forfine-tuning and evaluation still require exact diagonalization or tensornetworks. We explicitly acknowledge that our current approach is limited tosmall quantum systems (specifically limited to 12 qubits during training, withlimited extension to 16-20 qubits in testing) and that, while promising withinthis regime, this size restriction prevents immediate application to largersystems of practical interest in materials science and quantum chemistry.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum machine learning for spin and molecular systems faces criticalchallenges of scarce labeled data and computationally expensive simulations. Toaddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),a novel self-supervised framework that pre-trains transformers on unlabeledquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlikerandom masking approaches, HMAE employs a physics-informed strategy based onquantum information theory to selectively mask Hamiltonian terms based on theirphysical significance. Experiments on 12,500 quantum Hamiltonians (60%real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground stateenergy prediction with merely 10 labeled examples - a statistically significantimprovement (p &lt; 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantageis exceptional sample efficiency - reducing required labeled examples by 3-5xcompared to baseline methods - though we emphasize that ground truth values forfine-tuning and evaluation still require exact diagonalization or tensornetworks. We explicitly acknowledge that our current approach is limited tosmall quantum systems (specifically limited to 12 qubits during training, withlimited extension to 16-20 qubits in testing) and that, while promising withinthis regime, this size restriction prevents immediate application to largersystems of practical interest in materials science and quantum chemistry.</description>
      <author>example@mail.com (Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma)</author>
      <guid isPermaLink="false">2505.03140v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
  <item>
      <title>Geospatial Mechanistic Interpretability of Large Language Models</title>
      <link>http://arxiv.org/abs/2505.03368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究大型语言模型（LLMs）在地理信息处理方面的能力和内部工作机制。&lt;h4&gt;背景&lt;/h4&gt;LLMs在自然语言处理任务中表现出色，并在地理领域被用于地理知识库和推理工具，但其内部处理地理信息的方式尚不明确。&lt;h4&gt;目的&lt;/h4&gt;建立一个新的框架来研究地理空间机制的可解释性，通过空间分析来揭示LLMs处理地理信息的方式。&lt;h4&gt;方法&lt;/h4&gt;使用探针技术揭示LLMs内部的内部结构，引入机制可解释性领域，讨论叠加假设和稀疏自动编码器在分解LLMs的多义内部表示为可解释的单义特征中的作用。&lt;h4&gt;主要发现&lt;/h4&gt;实验中，通过空间自相关展示了地名特征如何显示与地理位置相关的空间模式，从而可以地理空间地解释，为LLMs如何处理地理信息提供了洞察。&lt;h4&gt;结论&lt;/h4&gt;本文提出的框架有助于地理学中基础模型的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）在各种自然语言处理任务中展现出了前所未有的能力。它们处理和生成有效文本和代码的能力使它们在许多领域变得无处不在，而它们作为知识库和“推理”工具的应用仍是一个持续研究的领域。在地理学中，越来越多的文献开始关注评估LLMs的地理知识和它们执行空间推理的能力。然而，关于这些模型内部工作方式的了解仍然非常有限，特别是关于它们如何处理地理信息。在本章中，我们建立了一个研究地理空间机制可解释性的新框架——使用空间分析来逆向工程LLMs如何处理地理信息。我们的目标是深化我们对这些复杂模型在处理地理信息时生成的内部表示的理解——如果这样的表述不是过度拟人化的话，我们可以称之为“LLMs如何思考地理信息”。我们首先概述了使用探针揭示LLMs内部结构的方法。然后，我们介绍了机制可解释性的领域，讨论了叠加假设和稀疏自动编码器在将LLMs的多义内部表示分解为更可解释的单义特征中的作用。在我们的实验中，我们使用空间自相关来展示地名特征如何显示出与地理位置相关的空间模式，从而可以地理空间地解释，为这些模型如何处理地理信息提供了洞察。我们最后讨论了我们的框架如何有助于塑造地理学中基础模型的研究和应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/sdesabbata/geospatial-mechanistic-interpretability&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated unprecedented capabilitiesacross various natural language processing tasks. Their ability to process andgenerate viable text and code has made them ubiquitous in many fields, whiletheir deployment as knowledge bases and "reasoning" tools remains an area ofongoing research. In geography, a growing body of literature has been focusingon evaluating LLMs' geographical knowledge and their ability to perform spatialreasoning. However, very little is still known about the internal functioningof these models, especially about how they process geographical information.  In this chapter, we establish a novel framework for the study of geospatialmechanistic interpretability - using spatial analysis to reverse engineer howLLMs handle geographical information. Our aim is to advance our understandingof the internal representations that these complex models generate whileprocessing geographical information - what one might call "how LLMs think aboutgeographic information" if such phrasing was not an undue anthropomorphism.  We first outline the use of probing in revealing internal structures withinLLMs. We then introduce the field of mechanistic interpretability, discussingthe superposition hypothesis and the role of sparse autoencoders indisentangling polysemantic internal representations of LLMs into moreinterpretable, monosemantic features. In our experiments, we use spatialautocorrelation to show how features obtained for placenames display spatialpatterns related to their geographic location and can thus be interpretedgeospatially, providing insights into how these models process geographicalinformation. We conclude by discussing how our framework can help shape thestudy and use of foundation models in geography.</description>
      <author>example@mail.com (Stef De Sabbata, Stefano Mizzaro, Kevin Roitero)</author>
      <guid isPermaLink="false">2505.03368v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Registration of 3D Point Sets Using Exponential-based Similarity Matrix</title>
      <link>http://arxiv.org/abs/2505.04540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的迭代最近点（ICP）算法，用于解决点云配准问题，特别是在存在大旋转差异或数据受传感器噪声严重腐蚀的情况下。&lt;h4&gt;背景&lt;/h4&gt;点云注册是计算机视觉和机器人领域的一个基本问题，涉及使用如激光雷达或结构光等深度传感器从不同视点捕获的3D点集的对齐。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有注册技术在大旋转差异或数据腐蚀情况下性能不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为指数相似矩阵ICP（ESM-ICP）的方法，该方法通过引入高斯启发式的指数加权方案来动态地构建相似矩阵，从而提高对齐过程中的旋转和平移组件的估计。&lt;h4&gt;主要发现&lt;/h4&gt;ESM-ICP在两个具有挑战性的场景中表现出了鲁棒性：(i) 源点云和目标点云之间存在大的旋转差异；(ii) 数据被非高斯噪声腐蚀。&lt;h4&gt;结论&lt;/h4&gt;ESM-ICP在性能上优于传统的几何注册技术以及几种最近基于学习的方法，并且其完整实现已公开在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种改进的迭代最近点（ICP）算法，用于解决点云配准问题，特别是在存在大旋转差异或数据受传感器噪声严重腐蚀的情况下。本文提出了一种称为指数相似矩阵ICP（ESM-ICP）的方法，该方法通过引入高斯启发式的指数加权方案来动态地构建相似矩阵，从而提高对齐过程中的旋转和平移组件的估计。ESM-ICP在两个具有挑战性的场景中表现出了鲁棒性：(i) 源点云和目标点云之间存在大的旋转差异；(ii) 数据被非高斯噪声腐蚀。ESM-ICP在性能上优于传统的几何注册技术以及几种最近基于学习的方法，并且其完整实现已公开在GitHub上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/aralab-unr/esm_icp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a fundamental problem in computer vision androbotics, involving the alignment of 3D point sets captured from varyingviewpoints using depth sensors such as LiDAR or structured light. In modernrobotic systems, especially those focused on mapping, it is essential to mergemultiple views of the same environment accurately. However, state-of-the-artregistration techniques often struggle when large rotational differences existbetween point sets or when the data is significantly corrupted by sensor noise.These challenges can lead to misalignments and, consequently, to inaccurate ordistorted 3D reconstructions. In this work, we address both these limitationsby proposing a robust modification to the classic Iterative Closest Point (ICP)algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),integrates a Gaussian-inspired exponential weighting scheme to construct asimilarity matrix that dynamically adapts across iterations. This matrixfacilitates improved estimation of both rotational and translational componentsduring alignment. We demonstrate the robustness of ESM-ICP in two challengingscenarios: (i) large rotational discrepancies between the source and targetpoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results showthat ESM-ICP outperforms traditional geometric registration techniques as wellas several recent learning-based methods. To encourage reproducibility andcommunity engagement, our full implementation is made publicly available onGitHub. https://github.com/aralab-unr/ESM_ICP</description>
      <author>example@mail.com (Ashutosh Singandhupe, Sanket Lokhande, Hung Manh La)</author>
      <guid isPermaLink="false">2505.04540v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.04594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MonoCoP是一种基于预测链（CoP）的3D属性预测方法，旨在解决单目3D目标检测中的深度估计问题，通过条件预测提高准确性。&lt;h4&gt;背景&lt;/h4&gt;深度估计是单目3D目标检测中最具挑战性的问题，因为将2D图像映射到3D空间存在固有的歧义。&lt;h4&gt;目的&lt;/h4&gt;提出MonoCoP以通过条件预测提高3D属性预测的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;MonoCoP采用三个关键设计：1）使用轻量级的属性网络（AN）学习每个3D属性的特征；2）构建显式链来传播这些特征；3）使用残差连接聚合特征，确保后续属性预测基于先前处理的所有属性。&lt;h4&gt;主要发现&lt;/h4&gt;MonoCoP在KITTI排行榜上实现了最先进的性能，且无需额外数据，在Waymo和nuScenes前向数据集上超越了现有方法。&lt;h4&gt;结论&lt;/h4&gt;MonoCoP通过条件预测链提高了单目3D目标检测的深度估计准确性，为该领域提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description>
      <author>example@mail.com (Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu)</author>
      <guid isPermaLink="false">2505.04594v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>On Path to Multimodal Generalist: General-Level and General-Bench</title>
      <link>http://arxiv.org/abs/2505.04620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML'25, 305 pages, 115 tables, 177 figures, project page:  https://generalist.top/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态大型语言模型（MLLM）的快速发展及其在多模态理解与生成方面的能力提升。&lt;h4&gt;背景&lt;/h4&gt;MLLM的发展推动了多模态通用主义范式的出现，从最初的多模态理解发展到跨模态生成，能力从粗粒度扩展到细粒度，支持的模态从有限扩展到任意。&lt;h4&gt;目的&lt;/h4&gt;评估MLLM的性能和泛化能力，推动向更强大的多模态通用主义和通用人工智能（AGI）的发展。&lt;h4&gt;方法&lt;/h4&gt;引入了“通用级别”评估框架，定义了MLLM性能和泛化能力的5个级别，以及“通用基准”（General-Bench），包含700多个任务和325,800个实例。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，尽管存在许多基准，但高任务性能并不一定意味着更强的MLLM能力，且达到真正的人工智能仍面临挑战。&lt;h4&gt;结论&lt;/h4&gt;该项目为下一代多模态基础模型的研究铺平了道路，为加速实现AGI提供了坚实的框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态大型语言模型（MLLM）目前正在经历快速增长，这得益于LLM的先进能力。与早期的专家不同，现有的MLLM正在向多模态通用主义范式进化。最初仅限于理解多个模态，这些模型已经发展到不仅理解而且能够跨模态生成。它们的能力已从粗粒度扩展到细粒度的多模态理解，从支持有限的模态扩展到任意的模态。尽管存在许多基准来评估MLLM，但一个关键问题出现了：我们能否简单地假设在任务上的更高性能意味着更强的MLLM能力，从而让我们更接近人类水平的人工智能？我们认为答案并不像它看起来那么简单。本项目引入了通用级别，这是一个定义MLLM性能和泛化能力的5个级别的评估框架，提供了一种比较MLLM和衡量现有系统向更强大的多模态通用主义以及最终向AGI发展的方法。该框架的核心是协同的概念，它衡量模型是否在理解和生成之间以及在不同模态之间保持一致的能力。为了支持这种评估，我们提出了通用基准，它包含更广泛的技能、模态、格式和能力，包括700多个任务和325,800个实例。涉及100多个现有最先进MLLM的评估结果揭示了通用主义者的能力排名，突显了达到真正人工智能的挑战。我们期望这个项目为下一代多模态基础模型的研究铺平道路，为加速实现AGI提供坚实的基础。项目页面：https://generalist.top/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Multimodal Large Language Model (MLLM) is currently experiencing rapidgrowth, driven by the advanced capabilities of LLMs. Unlike earlierspecialists, existing MLLMs are evolving towards a Multimodal Generalistparadigm. Initially limited to understanding multiple modalities, these modelshave advanced to not only comprehend but also generate across modalities. Theircapabilities have expanded from coarse-grained to fine-grained multimodalunderstanding and from supporting limited modalities to arbitrary ones. Whilemany benchmarks exist to assess MLLMs, a critical question arises: Can wesimply assume that higher performance across tasks indicates a stronger MLLMcapability, bringing us closer to human-level AI? We argue that the answer isnot as straightforward as it seems. This project introduces General-Level, anevaluation framework that defines 5-scale levels of MLLM performance andgenerality, offering a methodology to compare MLLMs and gauge the progress ofexisting systems towards more robust multimodal generalists and, ultimately,towards AGI. At the core of the framework is the concept of Synergy, whichmeasures whether models maintain consistent capabilities across comprehensionand generation, and across multiple modalities. To support this evaluation, wepresent General-Bench, which encompasses a broader spectrum of skills,modalities, formats, and capabilities, including over 700 tasks and 325,800instances. The evaluation results that involve over 100 existingstate-of-the-art MLLMs uncover the capability rankings of generalists,highlighting the challenges in reaching genuine AI. We expect this project topave the way for future research on next-generation multimodal foundationmodels, providing a robust infrastructure to accelerate the realization of AGI.Project page: https://generalist.top/</description>
      <author>example@mail.com (Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang)</author>
      <guid isPermaLink="false">2505.04620v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution</title>
      <link>http://arxiv.org/abs/2505.04384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TMM on 17-Jan-2025; Submitted to IEEE TMM on  11-Jul-2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于解耦和对比学习的多类别人脸操纵技术分类框架，用于提升开放世界半监督深度伪造归属（OSS-DFA）任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度伪造归属（DFA）旨在对不同的人脸操纵技术进行多分类，以减轻伪造内容对社会秩序和个人声誉的负面影响。然而，先前的方法仅关注特定方法的线索，容易导致过拟合，并忽略了通用伪造特征的重要性。此外，它们在更实际的开世界场景中难以区分不确定的新类别。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的基于多解耦的对比学习框架，以增强在开放世界半监督深度伪造归属任务中对新类别的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;DATA框架首次定义了'正交归一深度伪造基'的概念，并利用它来解耦特定方法特征，减少对伪造无关信息的过拟合。此外，设计了一种增强记忆机制以帮助新类别发现和对比学习，并通过实例级别的解耦来获得新类别的清晰边界。DATA还使用基础对比损失和中心对比损失作为辅助模块，以增强特征的标准化和区分度。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验评估表明，DATA在OSS-DFA基准上取得了最先进的性能，与现有方法相比，在不同设置下准确率提高了2.55% / 5.7%。&lt;h4&gt;结论&lt;/h4&gt;DATA框架在OSS-DFA任务中表现优异，能够有效提升对新类别的泛化能力，并具有更好的标准化和区分度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake attribution (DFA) aims to perform multiclassification on differentfacial manipulation techniques, thereby mitigating the detrimental effects offorgery content on the social order and personal reputations. However, previousmethods focus only on method-specific clues, which easily lead to overfitting,while overlooking the crucial role of common forgery features. Additionally,they struggle to distinguish between uncertain novel classes in more practicalopen-world scenarios. To address these issues, in this paper we propose aninnovative multi-DisentAnglement based conTrastive leArning framework, DATA, toenhance the generalization ability on novel classes for the open-worldsemi-supervised deepfake attribution (OSS-DFA) task. Specifically, since allgeneration techniques can be abstracted into a similar architecture, DATAdefines the concept of 'Orthonormal Deepfake Basis' for the first time andutilizes it to disentangle method-specific features, thereby reducing theoverfitting on forgery-irrelevant information. Furthermore, an augmented-memorymechanism is designed to assist in novel class discovery and contrastivelearning, which aims to obtain clear class boundaries for the novel classesthrough instance-level disentanglements. Additionally, to enhance thestandardization and discrimination of features, DATA uses bases contrastiveloss and center contrastive loss as auxiliaries for the aforementioned modules.Extensive experimental evaluations show that DATA achieves state-of-the-artperformance on the OSS-DFA benchmark, e.g., there are notable accuracyimprovements in 2.55% / 5.7% under different settings, compared with theexisting methods.</description>
      <author>example@mail.com (Ming-Hui Liu, Xiao-Qian Liu, Xin Luo, Xin-Shun Xu)</author>
      <guid isPermaLink="false">2505.04384v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Granular Attention based Heterogeneous Hypergraph Neural Network</title>
      <link>http://arxiv.org/abs/2505.04340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MGA-HHN的多粒度注意力异构超图神经网络，用于异构图表示学习，并通过实验证明了其在节点分类、节点聚类和可视化任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的异构图神经网络（HeteGNNs）通过元路径基于的消息传递学习潜在节点表示，但存在无法捕捉高阶关系和信息扭曲等问题。&lt;h4&gt;目的&lt;/h4&gt;提出MGA-HHN以解决HeteGNNs中的上述局限性。&lt;h4&gt;方法&lt;/h4&gt;MGA-HHN引入了两项关键创新：(1) 一种基于元路径构建异构超图的新方法，通过多视图显式地建模异构图中的高阶语义信息；(2) 一个多粒度注意力机制，在节点和超边级别上操作，以捕捉具有相同语义上下文的节点之间的精细粒度交互，同时保留不同超边类型之间的语义多样性。&lt;h4&gt;主要发现&lt;/h4&gt;MGA-HHN有效地缓解了长程消息扭曲，并生成了更丰富的节点表示。&lt;h4&gt;结论&lt;/h4&gt;MGA-HHN在真实世界基准数据集上的实验表明，它优于现有的模型，展示了其在节点分类、节点聚类和可视化任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：异构图神经网络（HeteGNNs）通过有效地提取异构图中的复杂结构和语义信息，在学习节点表示方面显示出强大的能力。大多数现存的HeteGNNs遵循邻域聚合范式，利用基于元路径的消息传递来学习潜在节点表示。然而，由于元路径的成对性质，这些模型无法捕捉节点间的高阶关系，导致性能次优。此外，由于HeteGNNs中的长程消息传递导致的“过度压缩”，进一步限制了这些模型的有效性。为了解决这些局限性，本文提出了一种基于多粒度注意力的异构超图神经网络MGA-HHN，用于异构图表示学习。MGA-HHN引入了两项关键创新：(1) 一种基于元路径构建异构超图的新方法，通过多视图显式地建模异构图中的高阶语义信息；(2) 一个多粒度注意力机制，在节点和超边级别上操作。这种机制使得模型能够捕捉具有相同语义上下文的节点之间的精细粒度交互，同时保留不同超边类型之间的语义多样性。因此，MGA-HHN有效地缓解了长程消息扭曲，并生成了更丰富的节点表示。在真实世界基准数据集上的大量实验表明，MGA-HHN优于现有模型，展示了其在节点分类、节点聚类和可视化任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous graph neural networks (HeteGNNs) have demonstrated strongabilities to learn node representations by effectively extracting complexstructural and semantic information in heterogeneous graphs. Most of theprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveragingmeta-path based message passing to learn latent node representations. However,due to the pairwise nature of meta-paths, these models fail to capturehigh-order relations among nodes, resulting in suboptimal performance.Additionally, the challenge of ``over-squashing'', where long-range messagepassing in HeteGNNs leads to severe information distortion, further limits theefficacy of these models. To address these limitations, this paper proposesMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph NeuralNetwork for heterogeneous graph representation learning. MGA-HHN introduces twokey innovations: (1) a novel approach for constructing meta-path basedheterogeneous hypergraphs that explicitly models higher-order semanticinformation in heterogeneous graphs through multiple views, and (2) amulti-granular attention mechanism that operates at both the node and hyperedgelevels. This mechanism enables the model to capture fine-grained interactionsamong nodes sharing the same semantic context within a hyperedge type, whilepreserving the diversity of semantics across different hyperedge types. Assuch, MGA-HHN effectively mitigates long-range message distortion and generatesmore expressive node representations. Extensive experiments on real-worldbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-artmodels, showcasing its effectiveness in node classification, node clusteringand visualization tasks.</description>
      <author>example@mail.com (Hong Jin, Kaicheng Zhou, Jie Yin, Lan You, Zhifeng Zhou)</author>
      <guid isPermaLink="false">2505.04340v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>RAFT: Robust Augmentation of FeaTures for Image Segmentation</title>
      <link>http://arxiv.org/abs/2505.04529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为RAFT的新框架，用于通过数据增强、特征增强和主动学习，利用最少量的标注数据对图像分割模型进行适应，以解决合成数据训练的深度神经网络在实际应用中性能不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;图像分割是场景理解的有力计算机视觉技术，但在实际应用中，高质量的标注数据集是必需的，而手动数据收集和标注成本高昂。合成数据虽然提供了高质量的标签，但深度神经网络在合成数据上训练后，往往在真实世界部署时面临Syn2Real问题，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述图像分割的差距，提出RAFT框架，旨在通过数据增强、特征增强和主动学习，使用最少量的标注数据对图像分割模型进行适应。&lt;h4&gt;方法&lt;/h4&gt;RAFT框架通过数据增强、特征增强和主动学习，利用少量标注数据对图像分割模型进行训练和优化。&lt;h4&gt;主要发现&lt;/h4&gt;在合成到真实的“SYNTHIA-&gt;Cityscapes”和“GTAV-&gt;Cityscapes”基准测试中，RAFT框架超过了之前的最先进方法HALO，分别实现了mIoU提升2.1%/79.9%和0.4%/78.2%。在真实到真实的基准测试“Cityscapes-&gt;ACDC”中，RAFT同样超越了HALO，mIoU提升1.3%/73.2%。此外，还考察了分配的标注预算和RAFT框架的各个组件对最终迁移mIoU的影响。&lt;h4&gt;结论&lt;/h4&gt;RAFT框架有效提高了图像分割模型在真实世界中的应用性能，并通过实验验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图像分割是场景理解的有力计算机视觉技术。然而，由于需要高质量、精心标注的数据集，实际部署受到了阻碍。合成数据提供了高质量的标签，同时减少了手动数据收集和标注的需求。然而，在合成数据上训练的深度神经网络往往面临Syn2Real问题，导致在真实世界部署中性能不佳。为了缓解上述图像分割的差距，我们提出了RAFT，一种新的框架，通过数据增强、特征增强以及主动学习，利用最少量的标注数据对图像分割模型进行适应。为了验证RAFT，我们在合成到真实“SYNTHIA-&gt;Cityscapes”和“GTAV-&gt;Cityscapes”基准测试上进行了实验。我们成功超过了之前的最佳水平HALO。在“SYNTHIA-&gt;Cityscapes”中，经过领域适应后，mIoU提高了2.1%/79.9%，在“GTAV-&gt;Cityscapes”中，mIoU提高了0.4%/78.2%。此外，我们在真实到真实基准测试“Cityscapes-&gt;ACDC”上也测试了我们的方法，再次超过了HALO，在适应后mIoU提高了1.3%/73.2%。最后，我们还考察了分配的标注预算和RAFT框架的各个组件对最终迁移mIoU的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image segmentation is a powerful computer vision technique for sceneunderstanding. However, real-world deployment is stymied by the need forhigh-quality, meticulously labeled datasets. Synthetic data provideshigh-quality labels while reducing the need for manual data collection andannotation. However, deep neural networks trained on synthetic data often facethe Syn2Real problem, leading to poor performance in real-world deployments.  To mitigate the aforementioned gap in image segmentation, we propose RAFT, anovel framework for adapting image segmentation models using minimal labeledreal-world data through data and feature augmentations, as well as activelearning. To validate RAFT, we perform experiments on the synthetic-to-real"SYNTHIA-&gt;Cityscapes" and "GTAV-&gt;Cityscapes" benchmarks. We managed to surpassthe previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences animprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV-&gt;Cityscapesexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approachon the real-to-real benchmark of "Cityscapes-&gt;ACDC", and again surpass HALO,with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine theeffect of the allocated annotation budget and various components of RAFT uponthe final transfer mIoU.</description>
      <author>example@mail.com (Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin)</author>
      <guid isPermaLink="false">2505.04529v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning</title>
      <link>http://arxiv.org/abs/2505.04601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OpenVision，这是一个完全开放的、成本效益高的视觉编码器家族，其性能与OpenAI的CLIP相当甚至更优，适用于构建多模态基础模型。&lt;h4&gt;背景&lt;/h4&gt;OpenAI的CLIP自2021年初发布以来，一直是构建多模态基础模型的首选视觉编码器。尽管最近出现了如SigLIP等替代品，但它们并未完全开放，其训练数据和训练方法未公开。&lt;h4&gt;目的&lt;/h4&gt;本文旨在填补这一空白，提出OpenVision，一个完全开放的视觉编码器，以提升多模态模型的质量。&lt;h4&gt;方法&lt;/h4&gt;OpenVision基于现有的工作，如CLIPS训练框架和Recap-DataComp-1B训练数据，同时揭示了多个关键见解，以增强编码器的质量，并展示了在推进多模态模型方面的实际效益。&lt;h4&gt;主要发现&lt;/h4&gt;OpenVision提供了从5.9M到632.1M参数范围的视觉编码器，使得构建多模态模型时可以在容量和效率之间进行灵活的权衡：较大的模型提供增强的多模态性能，而较小的版本则支持轻量级、边缘就绪的多模态部署。&lt;h4&gt;结论&lt;/h4&gt;OpenVision为多模态模型构建提供了新的选择，有助于推动多模态技术的发展和应用。&lt;h4&gt;翻译&lt;/h4&gt;OpenAI的CLIP，自2021年初发布以来，一直是构建多模态基础模型的首选视觉编码器。尽管最近出现了如SigLIP等替代品，但据我们所知，它们都没有完全开放：它们的训练数据仍然是专有的，或者它们的训练方法并未公开。本文通过提出OpenVision，一个完全开放、成本效益高的视觉编码器家族，来填补这一空白。OpenVision基于现有工作，例如CLIPS训练框架和Recap-DataComp-1B训练数据，同时揭示了多个关键见解，以增强编码器的质量，并展示了在推进多模态模型方面的实际效益。通过发布从5.9M到632.1M参数范围的视觉编码器，OpenVision为实践者提供了在构建多模态模型时在容量和效率之间进行灵活权衡的选择：较大的模型提供增强的多模态性能，而较小的版本则支持轻量级、边缘就绪的多模态部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; OpenAI's CLIP, released in early 2021, have long been the go-to choice ofvision encoder for building multimodal foundation models. Although recentalternatives such as SigLIP have begun to challenge this status quo, to ourknowledge none are fully open: their training data remains proprietary and/ortheir training recipes are not released. This paper fills this gap withOpenVision, a fully-open, cost-effective family of vision encoders that matchor surpass the performance of OpenAI's CLIP when integrated into multimodalframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS fortraining framework and Recap-DataComp-1B for training data -- while revealingmultiple key insights in enhancing encoder quality and showcasing practicalbenefits in advancing multimodal models. By releasing vision encoders spanningfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexibletrade-off between capacity and efficiency in building multimodal models: largermodels deliver enhanced multimodal performance, while smaller versions enablelightweight, edge-ready multimodal deployments.</description>
      <author>example@mail.com (Xianhang Li, Yanqing Liu, Haoqin Tu, Hongru Zhu, Cihang Xie)</author>
      <guid isPermaLink="false">2505.04601v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems</title>
      <link>http://arxiv.org/abs/2505.04596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 Figures, Accepted at AIRC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种优化动态监控环境中PTZ（全向俯仰缩放）相机调度和控制的新方法。&lt;h4&gt;背景&lt;/h4&gt;PTZ相机在动态监控环境中的应用需要高效的视频捕获。&lt;h4&gt;目的&lt;/h4&gt;提高动态监控环境中PTZ相机的调度和控制效率。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了卡尔曼滤波器进行运动预测和动态网络流模型来增强实时视频捕获效率。通过将卡尔曼滤波器分配给跟踪对象，系统预测未来位置，实现精确的相机任务调度。此外，引入基于价值的系统来优先处理相机动作，关注关键事件的及时捕获。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的主从相机系统相比，该方法提高了覆盖范围，减少了平均等待时间，并最小化了遗漏事件。&lt;h4&gt;结论&lt;/h4&gt;该方法显著提高了监控系统的效率、可扩展性和有效性，尤其是在动态和拥挤的环境中。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法来优化动态监控环境中PTZ相机的调度和控制。该方法结合了卡尔曼滤波器进行运动预测和动态网络流模型以增强实时视频捕获效率。通过将卡尔曼滤波器分配给跟踪对象，系统预测未来位置，从而实现精确的相机任务调度。此外，还引入了一个基于价值的系统来优先处理相机动作，关注关键事件的及时捕获。广泛的模拟表明，与传统的主从相机系统相比，该方法提高了覆盖范围，减少了平均等待时间，并最小化了遗漏事件。总的来说，该方法显著提高了监控系统的效率、可扩展性和有效性，尤其是在动态和拥挤的环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel approach for optimizing the scheduling andcontrol of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.The proposed method integrates Kalman filters for motion prediction with adynamic network flow model to enhance real-time video capture efficiency. Byassigning Kalman filters to tracked objects, the system predicts futurelocations, enabling precise scheduling of camera tasks. This prediction-drivenapproach is formulated as a network flow optimization, ensuring scalability andadaptability to various surveillance scenarios. To further reduce redundantmonitoring, we also incorporate group-tracking nodes, allowing multiple objectsto be captured within a single camera focus when appropriate. In addition, avalue-based system is introduced to prioritize camera actions, focusing on thetimely capture of critical events. By adjusting the decay rates of these valuesover time, the system ensures prompt responses to tasks with imminentdeadlines. Extensive simulations demonstrate that this approach improvescoverage, reduces average wait times, and minimizes missed events compared totraditional master-slave camera systems. Overall, our method significantlyenhances the efficiency, scalability, and effectiveness of surveillancesystems, particularly in dynamic and crowded environments.</description>
      <author>example@mail.com (Mohammad Merati, David Castañón)</author>
      <guid isPermaLink="false">2505.04596v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces</title>
      <link>http://arxiv.org/abs/2505.04335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Filtration-based Hyperbolic Fuzzy C-Means (HypeFCM)的聚类算法，用于在非欧几里得空间中更好地表示数据关系。&lt;h4&gt;背景&lt;/h4&gt;传统的聚类技术在处理复杂、高维和非欧几里得数据集时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;克服传统聚类技术在非欧几里得空间中的局限性。&lt;h4&gt;方法&lt;/h4&gt;HypeFCM算法结合了模糊聚类原理和双曲几何，使用基于权重的过滤机制来提高性能。算法使用狄利克雷分布初始化权重，并根据Poincaré圆盘模型中的双曲度量迭代地优化聚类中心和成员分配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HypeFCM在非欧几里得设置中显著优于传统的模糊聚类方法，证明了其鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;HypeFCM是一种有效的聚类算法，特别适用于在非欧几里得空间中处理复杂数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering algorithms play a pivotal role in unsupervised learning byidentifying and grouping similar objects based on shared characteristics. Whiletraditional clustering techniques, such as hard and fuzzy center-basedclustering, have been widely used, they struggle with complex,high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibitsnotable limitations in non-Euclidean spaces. Euclidean spaces assume linearseparability and uniform distance scaling, limiting their effectiveness incapturing complex, hierarchical, or non-Euclidean structures in fuzzyclustering. To overcome these challenges, we introduce Filtration-basedHyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored forbetter representation of data relationships in non-Euclidean spaces. HypeFCMintegrates the principles of fuzzy clustering with hyperbolic geometry andemploys a weight-based filtering mechanism to improve performance. Thealgorithm initializes weights using a Dirichlet distribution and iterativelyrefines cluster centroids and membership assignments based on a hyperbolicmetric in the Poincar\'e Disc model. Extensive experimental evaluationsdemonstrate that HypeFCM significantly outperforms conventional fuzzyclustering methods in non-Euclidean settings, underscoring its robustness andeffectiveness.</description>
      <author>example@mail.com (Swagato Das, Arghya Pratihar, Swagatam Das)</author>
      <guid isPermaLink="false">2505.04335v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
      <link>http://arxiv.org/abs/2505.04485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, accepted at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Frame-Averaging Kernel-Point Convolution (FA-KPConv)，这是一种基于知名KPConv架构的神经网络架构，适用于3D点云分析。FA-KPConv通过FrameAveraging技术使得基于KPConv层的点云神经网络在平移、旋转和/或反射变换下具有精确的不变性和/或等变性，同时不增加可学习参数数量，不损失任何输入信息，并在点云分类和点云注册任务中显示出优势。&lt;h4&gt;背景&lt;/h4&gt;KPConv是一种广泛用于3D点云分析的骨干网络，但其基于KPConv的网络在训练大数据集或进行数据增强时，只能近似实现欧几里得变换的不变性和/或等变性。&lt;h4&gt;目的&lt;/h4&gt;设计一个神经网络架构，使得基于KPConv层的点云神经网络在平移、旋转和/或反射变换下具有精确的不变性和/或等变性。&lt;h4&gt;方法&lt;/h4&gt;使用FrameAveraging技术，将几何先验知识嵌入到KPConv网络中，通过简单地封装现有的KPConv网络来实现。&lt;h4&gt;主要发现&lt;/h4&gt;FA-KPConv在网络结构上对现有的KPConv进行了改进，使其具有精确的不变性和/或等变性，同时保持了参数数量和输入信息的完整性。&lt;h4&gt;结论&lt;/h4&gt;FA-KPConv在点云分类和点云注册任务中显示出优势，尤其是在训练数据稀缺或测试数据随机旋转的挑战性情况下。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为Frame-Averaging Kernel-Point Convolution (FA-KPConv)的神经网络架构，该架构基于著名的KPConv，被广泛用于3D点云分析。尽管许多常见任务需要欧几里得变换的不变性和/或等变性，但基于KPConv的网络在训练大数据集或进行显著数据增强时只能近似实现这些特性。通过使用FrameAveraging，我们允许灵活定制使用KPConv层构建的点云神经网络，使它们对输入点云的平移、旋转和/或反射具有精确的不变性和/或等变性。通过简单地封装现有的基于KPConv的网络，FA-KPConv将几何先验知识嵌入其中，同时保持可学习参数的数量，不牺牲任何输入信息。我们展示了这种引入的偏差对点云分类和点云注册的好处，尤其是在训练数据稀缺或测试数据随机旋转的困难情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neuralnetwork architecture built on top of the well-known KPConv, a widely adoptedbackbone for 3D point cloud analysis. Even though invariance and/orequivariance to Euclidean transformations are required for many common tasks,KPConv-based networks can only approximately achieve such properties whentraining on large datasets or with significant data augmentations. Using FrameAveraging, we allow to flexibly customize point cloud neural networks builtwith KPConv layers, by making them exactly invariant and/or equivariant totranslations, rotations and/or reflections of the input point clouds. By simplywrapping around an existing KPConv-based network, FA-KPConv embeds geometricalprior knowledge into it while preserving the number of learnable parameters andnot compromising any input information. We showcase the benefit of such anintroduced bias for point cloud classification and point cloud registration,especially in challenging cases such as scarce training data or randomlyrotated test data.</description>
      <author>example@mail.com (Ali Alawieh, Alexandru P. Condurache)</author>
      <guid isPermaLink="false">2505.04485v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality</title>
      <link>http://arxiv.org/abs/2505.03440v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, submitted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为manvr3d的VR平台，用于交互式人机细胞追踪。&lt;h4&gt;背景&lt;/h4&gt;生命科学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平的发育历史。&lt;h4&gt;目的&lt;/h4&gt;通过结合VR控制器和眼动追踪硬件，加速深度学习细胞追踪模型的真值生成和校对。&lt;h4&gt;方法&lt;/h4&gt;将深度学习模型的增量标注、训练和校对循环提升到三维空间，并应用手势和眼动追踪等自然用户界面来加速细胞追踪流程。&lt;h4&gt;主要发现&lt;/h4&gt;该系统桥接了基于深度学习的细胞追踪软件和3D/VR可视化之间的差距，提高了细胞追踪的效率和空间理解。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有助于加速细胞追踪工作流程，并提升生命科学家的研究效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为manvr3d的VR平台，用于交互式人机细胞追踪。我们利用VR控制器和眼动追踪硬件来促进基于深度学习的细胞追踪模型的快速真值生成和校对。生命科学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平的发育历史。传统上，这种细胞谱系树的重建涉及通过所有记录的时间点追踪单个细胞，手动标注其位置，然后将它们随时间链接起来以创建完整的轨迹。基于深度学习的算法加速了这一过程，但严重依赖于手动标注的高质量真值数据和整理。在此过程中，图像数据的视觉表示仍然主要依赖于2D渲染，这极大地限制了空间理解和导航。在本研究中，我们通过将深度学习模型的增量标注、训练和校对循环提升到第三维度，并应用手势和眼动追踪等自然用户界面，来加速生命科学家的细胞追踪工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose manvr3d, a novel VR-ready platform for interactivehuman-in-the-loop cell tracking. We utilize VR controllers and eye-trackinghardware to facilitate rapid ground truth generation and proofreading for deeplearning-based cell tracking models. Life scientists reconstruct thedevelopmental history of organisms on the cellular level by analyzing 3Dtime-lapse microscopy images acquired at high spatio-temporal resolution. Thereconstruction of such cell lineage trees traditionally involves trackingindividual cells through all recorded time points, manually annotating theirpositions, and then linking them over time to create complete trajectories.Deep learning-based algorithms accelerate this process, yet depend heavily onmanually-annotated high-quality ground truth data and curation. Visualrepresentation of the image data in this process still relies primarily on 2Drenderings, which greatly limits spatial understanding and navigation. In thiswork, we bridge the gap between deep learning-based cell tracking software and3D/VR visualization to create a human-in-the-loop cell tracking system. We liftthe incremental annotation, training and proofreading loop of the deep learningmodel into the 3rd dimension and apply natural user interfaces like handgestures and eye tracking to accelerate the cell tracking workflow for lifescientists.</description>
      <author>example@mail.com (Samuel Pantze, Jean-Yves Tinevez, Matthew McGinity, Ulrik Günther)</author>
      <guid isPermaLink="false">2505.03440v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities</title>
      <link>http://arxiv.org/abs/2505.04461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCAI 2025 Survey Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了时间交互图（TIGs）及其在复杂动态系统行为建模中的应用，并重点介绍了时间交互图表示学习（TIGRL）的发展现状。&lt;h4&gt;背景&lt;/h4&gt;时间交互图（TIGs）由于其能够模拟复杂动态系统行为的能力，在现实世界中得到了广泛应用。&lt;h4&gt;目的&lt;/h4&gt;时间交互图表示学习（TIGRL）的目标是将TIGs中的节点嵌入到低维度的表示中，以有效地保留结构和时间信息，从而提高分类、预测和聚类等下游任务在动态数据环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;本文首先介绍了TIGs的基础概念，强调了时间依赖性的关键作用；接着提出了一个全面的分类法，根据学习过程中利用的信息类型对最先进的TIGRL方法进行系统分类；同时整理了数据集和基准测试的资源，为实证研究提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;本文系统地分类了TIGRL方法，并探讨了TIGRL领域的关键开放挑战和有前景的研究方向。&lt;h4&gt;结论&lt;/h4&gt;本文为TIGRL领域未来的发展奠定了基础，有望推动该领域的发展进程。&lt;h4&gt;翻译&lt;/h4&gt;Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal interaction graphs (TIGs), defined by sequences of timestampedinteraction events, have become ubiquitous in real-world applications due totheir capability to model complex dynamic system behaviors. As a result,temporal interaction graph representation learning (TIGRL) has garneredsignificant attention in recent years. TIGRL aims to embed nodes in TIGs intolow-dimensional representations that effectively preserve both structural andtemporal information, thereby enhancing the performance of downstream taskssuch as classification, prediction, and clustering within constantly evolvingdata environments. In this paper, we begin by introducing the foundationalconcepts of TIGs and emphasize the critical role of temporal dependencies. Wethen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,systematically categorizing them based on the types of information utilizedduring the learning process to address the unique challenges inherent to TIGs.To facilitate further research and practical applications, we curate the sourceof datasets and benchmarks, providing valuable resources for empiricalinvestigations. Finally, we examine key open challenges and explore promisingresearch directions in TIGRL, laying the groundwork for future advancementsthat have the potential to shape the evolution of this field.</description>
      <author>example@mail.com (Pengfei Jiao, Hongjiang Chen, Xuan Guo, Zhidong Zhao, Dongxiao He, Di Jin)</author>
      <guid isPermaLink="false">2505.04461v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Vision Graph Prompting via Semantic Low-Rank Decomposition</title>
      <link>http://arxiv.org/abs/2505.04121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision GNN (ViG)通过将图像表示为图结构，提供了一种更自然的方式来捕捉超越传统网格或序列表示的复杂语义模式。为了高效地将ViG应用于下游任务，参数高效的微调技术如视觉提示变得日益重要。&lt;h4&gt;背景&lt;/h4&gt;现有的提示方法主要针对基于Transformer的模型设计，忽视了图表示中节点和边之间的丰富拓扑关系，限制了它们建模复杂语义的能力。&lt;h4&gt;目的&lt;/h4&gt;提出Vision Graph Prompting (VGP)，一个针对视觉图结构的全新框架。&lt;h4&gt;方法&lt;/h4&gt;VGP的核心洞察是图中语义上连接的组件表现出低秩特性。基于这一观察，引入了一种语义低秩提示方法，该方法分解低秩语义特征，并将其与视觉图拓扑结构上的提示相结合，捕捉全局结构模式和细粒度语义依赖。&lt;h4&gt;主要发现&lt;/h4&gt;VGP显著提高了ViG在多种下游任务上的迁移性能，在保持参数效率的同时，达到了与完全微调相当的结果。&lt;h4&gt;结论&lt;/h4&gt;VGP在视觉图结构上的应用为提高ViG的迁移性能提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;Vision GNN (ViG)通过将图像表示为图结构，以更自然的方式捕捉超越传统网格或序列表示的复杂语义模式。为了高效地将ViG应用于下游任务，参数高效的微调技术如视觉提示变得日益重要。然而，现有的提示方法主要针对基于Transformer的模型设计，忽视了图表示中节点和边之间的丰富拓扑关系，限制了它们建模复杂语义的能力。在本文中，我们提出了Vision Graph Prompting (VGP)，一个针对视觉图结构的全新框架。我们的核心洞察是图中语义上连接的组件表现出低秩特性。基于这一观察，我们引入了一种语义低秩提示方法，该方法分解低秩语义特征，并将其与视觉图拓扑结构上的提示相结合，捕捉全局结构模式和细粒度语义依赖。广泛的实验表明，我们的方法在多种下游任务上显著提高了ViG的迁移性能，在保持参数效率的同时，达到了与完全微调相当的结果。我们的代码可在https://github.com/zhoujiahuan1991/ICML2025-VGP上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhoujiahuan1991/icml2025-vgp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision GNN (ViG) demonstrates superior performance by representing images asgraph structures, providing a more natural way to capture irregular semanticpatterns beyond traditional grid or sequence-based representations. Toefficiently adapt ViG to downstream tasks, parameter-efficient fine-tuningtechniques like visual prompting become increasingly essential. However,existing prompting methods are primarily designed for Transformer-based models,neglecting the rich topological relationships among nodes and edges ingraph-based representations, limiting their capacity to model complexsemantics. In this paper, we propose Vision Graph Prompting (VGP), a novelframework tailored for vision graph structures. Our core insight reveals thatsemantically connected components in the graph exhibit low-rank properties.Building on this observation, we introduce a semantic low-rank prompting methodthat decomposes low-rank semantic features and integrates them with prompts onvision graph topologies, capturing both global structural patterns andfine-grained semantic dependencies. Extensive experiments demonstrate ourmethod significantly improves ViG's transfer performance on diverse downstreamtasks, achieving results comparable to full fine-tuning while maintainingparameter efficiency. Our code is available athttps://github.com/zhoujiahuan1991/ICML2025-VGP.</description>
      <author>example@mail.com (Zixiang Ai, Zichen Liu, Jiahuan Zhou)</author>
      <guid isPermaLink="false">2505.04121v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception</title>
      <link>http://arxiv.org/abs/2505.04410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeCLIP是一个新型的视觉语言模型框架，通过解耦CLIP的自注意力模块来增强其在密集预测任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;密集视觉预测任务受限于预定义类别，限制了其在实际场景中的应用，而VLMs如CLIP在开放词汇任务中表现良好，但在密集预测中存在局部特征表示的限制。&lt;h4&gt;目的&lt;/h4&gt;提出DeCLIP框架以解决CLIP在图像token中难以有效聚合空间或语义相关区域信息的问题。&lt;h4&gt;方法&lt;/h4&gt;DeCLIP通过解耦自注意力模块获得“内容”和“上下文”特征，其中“内容”特征与图像裁剪表示对齐以增强局部可判别性，“上下文”特征在视觉基础模型如DINO的指导下学习保留空间相关性。&lt;h4&gt;主要发现&lt;/h4&gt;DeCLIP在多个开放词汇密集预测任务中（如目标检测和语义分割）显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;DeCLIP通过改进局部特征表示和空间一致性，提高了VLMs在密集预测任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;This abstract discusses a novel framework called DeCLIP, which enhances the performance of Vision-Language Models (VLMs) like CLIP in dense prediction tasks. The framework addresses limitations in local feature representation by decoupling the self-attention module to obtain 'content' and 'context' features, which are aligned with image crop representations and guided by vision foundation models to improve local discriminability and spatial consistency. Extensive experiments demonstrate its superiority over existing methods in tasks like object detection and semantic segmentation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense visual prediction tasks have been constrained by their reliance onpredefined categories, limiting their applicability in real-world scenarioswhere visual concepts are unbounded. While Vision-Language Models (VLMs) likeCLIP have shown promise in open-vocabulary tasks, their direct application todense prediction often leads to suboptimal performance due to limitations inlocal feature representation. In this work, we present our observation thatCLIP's image tokens struggle to effectively aggregate information fromspatially or semantically related regions, resulting in features that lacklocal discriminability and spatial consistency. To address this issue, wepropose DeCLIP, a novel framework that enhances CLIP by decoupling theself-attention module to obtain ``content'' and ``context'' featuresrespectively. The ``content'' features are aligned with image croprepresentations to improve local discriminability, while ``context'' featureslearn to retain the spatial correlations under the guidance of visionfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIPsignificantly outperforms existing methods across multiple open-vocabularydense prediction tasks, including object detection and semantic segmentation.Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.</description>
      <author>example@mail.com (Junjie Wang, Bin Chen, Yulin Li, Bin Kang, Yichi Chen, Zhuotao Tian)</author>
      <guid isPermaLink="false">2505.04410v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal cascade feature transfer for polymer property prediction</title>
      <link>http://arxiv.org/abs/2505.03704v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为多模态级联模型特征迁移的聚合物性质预测的新颖迁移学习方法。&lt;h4&gt;背景&lt;/h4&gt;聚合物数据以多种格式存在，包括分子描述符、添加剂信息以及化学结构。&lt;h4&gt;目的&lt;/h4&gt;提高聚合物物理性质的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;该模型通过结合由图卷积神经网络（GCN）从化学结构中提取的特征，以及分子描述符和添加剂信息等特征，实现更准确的预测。&lt;h4&gt;主要发现&lt;/h4&gt;使用多个聚合物数据集进行实证评估表明，与使用单一特征的传统方法相比，该方法表现出更高的预测性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在聚合物性质预测方面具有较高的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;In this paper, we propose a novel transfer learning approach called multi-modal cascade model with feature transfer for polymer property prediction. Polymers are characterized by a composite of data in several different formats, including molecular descriptors and additive information as well as chemical structures. However, in conventional approaches, prediction models were often constructed using each type of data separately. Our model enables more accurate prediction of physical properties for polymers by combining features extracted from the chemical structure by graph convolutional neural networks (GCN) with features such as molecular descriptors and additive information. The predictive performance of the proposed method is empirically evaluated using several polymer datasets. We report that the proposed method shows high predictive performance compared to the baseline conventional approach using a single feature.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel transfer learning approach calledmulti-modal cascade model with feature transfer for polymer propertyprediction.Polymers are characterized by a composite of data in severaldifferent formats, including molecular descriptors and additive information aswell as chemical structures. However, in conventional approaches, predictionmodels were often constructed using each type of data separately. Our modelenables more accurate prediction of physical properties for polymers bycombining features extracted from the chemical structure by graph convolutionalneural networks (GCN) with features such as molecular descriptors and additiveinformation. The predictive performance of the proposed method is empiricallyevaluated using several polymer datasets. We report that the proposed methodshows high predictive performance compared to the baseline conventionalapproach using a single feature.</description>
      <author>example@mail.com (Kiichi Obuchi, Yuta Yahagi, Kiyohiko Toyama, Shukichi Tanaka, Kota Matsui)</author>
      <guid isPermaLink="false">2505.03704v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization</title>
      <link>http://arxiv.org/abs/2505.04412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 11 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自编码器的方法，旨在从高维数据中发现和表示低维结构，同时保留关键拓扑和几何属性。该方法在点云数据集上的实验表明，其在发现噪声数据中的流形结构并通过降维保持这些结构方面优于t-SNE、UMAP和拓扑自编码器等基线方法。&lt;h4&gt;背景&lt;/h4&gt;流形学习旨在发现和表示高维数据中的低维结构，但现有方法往往无法同时捕捉局部细节和全局拓扑完整性，或者无法构建平衡的降维。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的流形学习方法，以从噪声数据中发现流形结构，并通过降维保持这些结构。&lt;h4&gt;方法&lt;/h4&gt;该方法集成了一个流形重建层，该层从噪声点云中揭示潜在流形结构，并在降维过程中提供拓扑和几何属性的规范化。这两个组件在训练过程中相互促进。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在发现噪声数据中的流形结构并保持这些结构方面优于基线方法，这一结论通过可视化和定量指标得到了验证。&lt;h4&gt;结论&lt;/h4&gt;结合流形重建和流形学习对于实现可靠地表示潜在流形具有重要意义，尤其是在处理噪声数据时。&lt;h4&gt;翻译&lt;/h4&gt;Manifold learning aims to discover and represent low-dimensional structures underlying high-dimensional data while preserving critical topological and geometric properties. Existing methods often fail to capture local details with global topological integrity from noisy data or construct a balanced dimensionality reduction, resulting in distorted or fractured embeddings. We present an AutoEncoder-based method that integrates a manifold reconstruction layer, which uncovers latent manifold structures from noisy point clouds, and further provides regularizations on topological and geometric properties during dimensionality reduction, whereas the two components promote each other during training. Experiments on point cloud datasets demonstrate that our method outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in discovering manifold structures from noisy data and preserving them through dimensionality reduction, as validated by visualization and quantitative metrics. This work demonstrates the significance of combining manifold reconstruction with manifold learning to achieve reliable representation of the latent manifold, particularly when dealing with noisy real-world data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/thanatorika/mrtg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manifold learning aims to discover and represent low-dimensional structuresunderlying high-dimensional data while preserving critical topological andgeometric properties. Existing methods often fail to capture local details withglobal topological integrity from noisy data or construct a balanceddimensionality reduction, resulting in distorted or fractured embeddings. Wepresent an AutoEncoder-based method that integrates a manifold reconstructionlayer, which uncovers latent manifold structures from noisy point clouds, andfurther provides regularizations on topological and geometric properties duringdimensionality reduction, whereas the two components promote each other duringtraining. Experiments on point cloud datasets demonstrate that our methodoutperforms baselines like t-SNE, UMAP, and Topological AutoEncoders indiscovering manifold structures from noisy data and preserving them throughdimensionality reduction, as validated by visualization and quantitativemetrics. This work demonstrates the significance of combining manifoldreconstruction with manifold learning to achieve reliable representation of thelatent manifold, particularly when dealing with noisy real-world data. Coderepository: https://github.com/Thanatorika/mrtg.</description>
      <author>example@mail.com (Ren Wang, Pengcheng Zhou)</author>
      <guid isPermaLink="false">2505.04412v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MFSeg: Efficient Multi-frame 3D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.04408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种高效的多帧3D语义分割框架MFSeg。&lt;h4&gt;背景&lt;/h4&gt;当前多帧3D语义分割方法在保持高精度的同时，计算开销较大。&lt;h4&gt;目的&lt;/h4&gt;减少计算开销，同时保持高精度。&lt;h4&gt;方法&lt;/h4&gt;通过在特征级别聚合点云序列，并正则化特征提取和聚合过程。使用轻量级的基于MLP的点解码器，消除了从过去帧中上采样冗余点的需求。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Waymo数据集上的实验表明，MFSeg优于现有方法，证明了其有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;MFSeg是一个有效且高效的3D语义分割框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose MFSeg, an efficient multi-frame 3D semantic segmentationframework. By aggregating point cloud sequences at the feature level andregularizing the feature extraction and aggregation process, MFSeg reducescomputational overhead while maintaining high accuracy. Moreover, by employinga lightweight MLP-based point decoder, our method eliminates the need toupsample redundant points from past frames. Experiments on the nuScenes andWaymo datasets show that MFSeg outperforms existing methods, demonstrating itseffectiveness and efficiency.</description>
      <author>example@mail.com (Chengjie Huang, Krzysztof Czarnecki)</author>
      <guid isPermaLink="false">2505.04408v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training</title>
      <link>http://arxiv.org/abs/2505.04083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Plexus的三维并行方法，用于全图训练，以解决大规模图数据在GPU内存容量限制、采样和数据传输速度慢、分布式全图训练通信开销大和负载不均衡等问题。&lt;h4&gt;背景&lt;/h4&gt;由于现实世界图数据规模庞大，很多图数据超过了GPU的内存容量，使用图神经网络（GNN）处理这些数据需要采用如小批量采样等技术进行扩展。&lt;h4&gt;目的&lt;/h4&gt;旨在解决大规模图数据在训练过程中的内存限制、计算效率低下和分布式训练中的通信开销问题。&lt;h4&gt;方法&lt;/h4&gt;Plexus是一种三维并行方法，包括负载平衡的排列方案和性能模型以预测最优的三维配置。&lt;h4&gt;主要发现&lt;/h4&gt;Plexus在多个图数据集上进行了评估，并在Perlmutter上扩展到2048个GPU，在Frontier上扩展到2048个GCD。Plexus相比现有方法实现了2.3x-12.5x的速度提升，并在Perlmutter上减少了5.2-8.7倍的时间到解，在Frontier上减少了7-54.2倍的时间到解。&lt;h4&gt;结论&lt;/h4&gt;Plexus实现了前所未有的加速效果，显著提高了大规模图数据的处理效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have emerged as a potent class of neural networkscapable of leveraging the connectivity and structure of real-world graphs tolearn intricate properties and relationships between nodes. Many real-worldgraphs exceed the memory capacity of a GPU due to their sheer size, and usingGNNs on them requires techniques such as mini-batch sampling to scale. However,this can lead to reduced accuracy in some cases, and sampling and data transferfrom the CPU to the GPU can also slow down training. On the other hand,distributed full-graph training suffers from high communication overhead andload imbalance due to the irregular structure of graphs. We propose Plexus, athree-dimensional (3D) parallel approach for full-graph training that tacklesthese issues and scales to billion-edge graphs. Additionally, we introduceoptimizations such as a permutation scheme for load balancing, and aperformance model to predict the optimal 3D configuration. We evaluate Plexuson several graph datasets and show scaling results for up to 2048 GPUs onPerlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexusachieves unprecedented speedups of 2.3x-12.5x over existing methods and areduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x onFrontier.</description>
      <author>example@mail.com (Aditya K. Ranjan, Siddharth Singh, Cunyang Wei, Abhinav Bhatele)</author>
      <guid isPermaLink="false">2505.04083v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows</title>
      <link>http://arxiv.org/abs/2505.04354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从专家依赖的优化问题解决流程转变为进化代理工作流程的观点。&lt;h4&gt;背景&lt;/h4&gt;传统的优化实践依赖于人类专家进行问题制定、算法选择和超参数调整，这导致了工业界对先进方法的采纳障碍。&lt;h4&gt;目的&lt;/h4&gt;本文主张通过基于基础模型和进化搜索的进化代理工作流程，可以自主地在优化空间中导航，包括问题、制定、算法和超参数空间。&lt;h4&gt;方法&lt;/h4&gt;通过云计算资源调度和ADMM参数适应的案例研究，展示了这一方法如何连接学术界创新与工业实施之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;本文挑战了以人为中心的优化工作流程的现状，并倡导采用更可扩展、适应性强的方法来解决现实世界的优化问题。&lt;h4&gt;结论&lt;/h4&gt;本文提出了优化问题解决流程转变的新思路，并提供了实际案例支持其观点。&lt;h4&gt;翻译&lt;/h4&gt;本文主张优化问题解决流程从专家依赖向进化代理工作流程转变。传统的优化实践依赖人类专家，导致工业界采纳先进方法的障碍。本文认为，基于基础模型和进化搜索的进化代理工作流程可以自主地在优化空间中导航，包括问题、制定、算法和超参数空间。通过云计算资源调度和ADMM参数适应的案例研究，本文展示了这一方法如何连接学术界创新与工业实施之间的差距。本文挑战了以人为中心的优化工作流程的现状，并倡导采用更可扩展、适应性强的方法来解决现实世界的优化问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This position paper argues that optimization problem solving can transitionfrom expert-dependent to evolutionary agentic workflows. Traditionaloptimization practices rely on human specialists for problem formulation,algorithm selection, and hyperparameter tuning, creating bottlenecks thatimpede industrial adoption of cutting-edge methods. We contend that anevolutionary agentic workflow, powered by foundation models and evolutionarysearch, can autonomously navigate the optimization space, comprising problem,formulation, algorithm, and hyperparameter spaces. Through case studies incloud resource scheduling and ADMM parameter adaptation, we demonstrate howthis approach can bridge the gap between academic innovation and industrialimplementation. Our position challenges the status quo of human-centricoptimization workflows and advocates for a more scalable, adaptive approach tosolving real-world optimization problems.</description>
      <author>example@mail.com (Wenhao Li, Bo Jin, Mingyi Hong, Changhong Lu, Xiangfeng Wang)</author>
      <guid isPermaLink="false">2505.04354v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</title>
      <link>http://arxiv.org/abs/2505.04105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了医学图像采集过程中患者运动导致的图像模糊、鬼影和器官变形问题，提出了一种新的运动感知图像合成方法MAISY，有效提升了图像质量。&lt;h4&gt;背景&lt;/h4&gt;医学图像采集过程中患者运动会导致图像模糊、鬼影和器官变形，影响图像解读。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来校正医学图像中的运动伪影，提升图像质量。&lt;h4&gt;方法&lt;/h4&gt;MAISY方法首先通过Segment Anything Model (SAM)动态学习解剖边界处的空间模式，然后利用引入的Variance-Selective SSIM (VS-SSIM)损失函数来强调像素方差高的区域，以保留关键解剖细节。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MAISY模型在胸部和头部CT数据集上优于现有方法，PSNR提高了40%，SSIM提高了10%，Dice提高了16%。&lt;h4&gt;结论&lt;/h4&gt;MAISY方法能够有效校正医学图像中的运动伪影，提高图像质量。&lt;h4&gt;翻译&lt;/h4&gt;During medical image acquisition, patient motion causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. The current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods, with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss, effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterizes motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patient motion during medical image acquisition causes blurring, ghosting,and distorts organs, which makes image interpretation challenging.Currentstate-of-the-art algorithms using Generative Adversarial Network (GAN)-basedmethods with their ability to learn the mappings between corrupted images andtheir ground truth via Structural Similarity Index Measure (SSIM) losseffectively generate motion-free images. However, we identified the followinglimitations: (i) they mainly focus on global structural characteristics andtherefore overlook localized features that often carry critical pathologicalinformation, and (ii) the SSIM loss function struggles to handle images withvarying pixel intensities, luminance factors, and variance. In this study, wepropose Motion-Aware Image SYnthesis (MAISY) which initially characterizemotion and then uses it for correction by: (a) leveraging the foundation modelSegment Anything Model (SAM), to dynamically learn spatial patterns alonganatomical boundaries where motion artifacts are most pronounced and, (b)introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptivelyemphasizes spatial regions with high pixel variance to preserve essentialanatomical details during artifact correction. Experiments on chest and head CTdatasets demonstrate that our model outperformed the state-of-the-artcounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by10%, and Dice by 16%.</description>
      <author>example@mail.com (Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim)</author>
      <guid isPermaLink="false">2505.04105v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning: a Lecture Note</title>
      <link>http://arxiv.org/abs/2505.03861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本讲义旨在为数据科学或相关学科的低年级硕士生和博士生提供机器学习的基础理念。&lt;h4&gt;背景&lt;/h4&gt;讲义从现代机器学习的基本理念开始，以分类作为主要目标任务。&lt;h4&gt;目的&lt;/h4&gt;帮助学生掌握损失函数、反向传播、随机梯度下降、泛化、模型选择以及人工神经网络的基本模块。&lt;h4&gt;方法&lt;/h4&gt;基于这些基本理念，讲义深入探讨了无监督学习的概率方法，包括有向潜在变量模型、专家乘积、生成对抗网络和自回归模型。&lt;h4&gt;主要发现&lt;/h4&gt;讲义还涵盖了强化学习、集成方法和元学习等多种进一步的话题。&lt;h4&gt;结论&lt;/h4&gt;阅读完本讲义后，学生应准备好开始研究机器学习以及更广泛的人工智能领域的更高级主题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This lecture note is intended to prepare early-year master's and PhD studentsin data science or a related discipline with foundational ideas in machinelearning. It starts with basic ideas in modern machine learning withclassification as a main target task. These basic ideas include lossformulation, backpropagation, stochastic gradient descent, generalization,model selection as well as fundamental blocks of artificial neural networks.Based on these basic ideas, the lecture note explores in depth the probablisticapproach to unsupervised learning, covering directed latent variable models,product of experts, generative adversarial networks and autoregressive models.Finally, the note ends by covering a diverse set of further topics, such asreinforcement learning, ensemble methods and meta-learning. After reading thislecture note, a student should be ready to embark on studying and researchingmore advanced topics in machine learning and more broadly artificialintelligence.</description>
      <author>example@mail.com (Kyunghyun Cho)</author>
      <guid isPermaLink="false">2505.03861v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
      <link>http://arxiv.org/abs/2505.02366v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的无监督对比学习方法，用于自然语言处理中的文本语义嵌入，该方法通过联合张量表示模量约束和交叉注意力机制，提高了对比学习的效果。&lt;h4&gt;背景&lt;/h4&gt;无监督对比学习在自然语言处理中成为热门研究主题，现有工作通常关注正负样本在高维语义空间中的方向分布，但忽略了模量特征，导致对比学习效果不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的训练目标，旨在对语义表示张量施加模量约束，以增强对比学习中正样本的对齐。&lt;h4&gt;方法&lt;/h4&gt;1. 提出一种新的训练目标，用于对语义表示张量施加模量约束。2. 提出了一种交叉注意力结构，增强模型对CLS token的关注，优化CLS Pooling的质量。3. 结合上述两种动机，提出了一种新的联合张量表示模量约束和交叉注意力无监督对比学习文本嵌入框架JTCSE。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，JTCSE的孪塔集成模型和单塔蒸馏模型优于其他基线，成为当前SOTA。此外，JTCSE在超过130个下游零样本任务上整体优于其他基线。&lt;h4&gt;结论&lt;/h4&gt;JTCSE框架通过联合张量表示模量约束和交叉注意力机制，有效提高了无监督对比学习在自然语言处理中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new Joint Tensor representation modulus constraint and Cross-attention unsupervised contrastive learning Sentence Embedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/tianyuzong/jtcse&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised contrastive learning has become a hot research topic in naturallanguage processing. Existing works usually aim at constraining the orientationdistribution of the representations of positive and negative samples in thehigh-dimensional semantic space in contrastive learning, but the semanticrepresentation tensor possesses both modulus and orientation features, and theexisting works ignore the modulus feature of the representations and causeinsufficient contrastive learning. % Therefore, we firstly propose a trainingobjective that aims at modulus constraints on the semantic representationtensor, to strengthen the alignment between the positive samples in contrastivelearning. Therefore, we first propose a training objective that is designed toimpose modulus constraints on the semantic representation tensor, to strengthenthe alignment between positive samples in contrastive learning. Then, theBERT-like model suffers from the phenomenon of sinking attention, leading to alack of attention to CLS tokens that aggregate semantic information. Inresponse, we propose a cross-attention structure among the twin-tower ensemblemodels to enhance the model's attention to CLS token and optimize the qualityof CLS Pooling. Combining the above two motivations, we propose a new\textbf{J}oint \textbf{T}ensor representation modulus constraint and\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence\textbf{E}mbedding representation framework JTCSE, which we evaluate in sevensemantic text similarity computation tasks, and the experimental results showthat JTCSE's twin-tower ensemble model and single-tower distillation modeloutperform the other baselines and become the current SOTA. In addition, wehave conducted an extensive zero-shot downstream task evaluation, which showsthat JTCSE outperforms other baselines overall on more than 130 tasks.</description>
      <author>example@mail.com (Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu)</author>
      <guid isPermaLink="false">2505.02366v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</title>
      <link>http://arxiv.org/abs/2505.04119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GAPrompt的新方法，用于提高3D视觉模型在点云数据上的性能。&lt;h4&gt;背景&lt;/h4&gt;预训练的3D视觉模型在点云数据上表现出色，但完全微调这些模型对于下游任务来说计算成本高且存储密集。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来提高3D视觉模型的可适应性和性能，同时减少计算和存储成本。&lt;h4&gt;方法&lt;/h4&gt;GAPrompt利用几何线索来增强3D视觉模型的适应性，包括引入点提示和点平移提示器，以及Prompt传播机制。&lt;h4&gt;主要发现&lt;/h4&gt;GAPrompt在多个基准测试中显著优于现有的参数高效微调方法，并且与全微调相比，只使用了2.19%的可训练参数。&lt;h4&gt;结论&lt;/h4&gt;GAPrompt是一种有效的参数高效微调方法，能够提高3D视觉模型在点云数据上的性能。&lt;h4&gt;翻译&lt;/h4&gt;Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhoujiahuan1991/icml2025-vgp&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained 3D vision models have gained significant attention for theirpromising performance on point cloud data. However, fully fine-tuning thesemodels for downstream tasks is computationally expensive and storage-intensive.Existing parameter-efficient fine-tuning (PEFT) approaches, which focusprimarily on input token prompting, struggle to achieve competitive performancedue to their limited ability to capture the geometric information inherent inpoint clouds. To address this challenge, we propose a novel Geometry-AwarePoint Cloud Prompt (GAPrompt) that leverages geometric cues to enhance theadaptability of 3D vision models. First, we introduce a Point Prompt thatserves as an auxiliary input alongside the original point cloud, explicitlyguiding the model to capture fine-grained geometric details. Additionally, wepresent a Point Shift Prompter designed to extract global shape informationfrom the point cloud, enabling instance-specific geometric adjustments at theinput level. Moreover, our proposed Prompt Propagation mechanism incorporatesthe shape information into the model's feature extraction process, furtherstrengthening its ability to capture essential geometric characteristics.Extensive experiments demonstrate that GAPrompt significantly outperformsstate-of-the-art PEFT methods and achieves competitive results compared to fullfine-tuning on various benchmarks, while utilizing only 2.19% of trainableparameters. Our code is available athttps://github.com/zhoujiahuan1991/ICML2025-VGP.</description>
      <author>example@mail.com (Zixiang Ai, Zichen Liu, Yuanhang Lei, Zhenyu Cui, Xu Zou, Jiahuan Zhou)</author>
      <guid isPermaLink="false">2505.04119v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers</title>
      <link>http://arxiv.org/abs/2505.04018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的深度学习框架，用于结构健康监测和控制，通过结合图神经网络（GNN）、变换器和物理信息损失函数，实现了对结构群体进行模态分解和识别。&lt;h4&gt;背景&lt;/h4&gt;模态识别对于结构健康监测和控制至关重要，它提供了对结构动力学和性能的关键见解。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种能够准确分解动态响应并识别模态特性的模型，无需标签数据，且不受外部载荷或结构配置变化的影响。&lt;h4&gt;方法&lt;/h4&gt;该模型通过将图神经网络（GNN）和变换器模块结合，将多自由度（MDOF）结构动态测量分解为单自由度（SDOF）模态响应，同时利用GNN捕捉结构配置并识别与分解的SDOF模态响应相对应的模态形状。模型以纯物理信息和无监督的方式进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟和实验室实验验证，该模型在从稀疏结构动态测量中准确分解动态响应和识别模态特性方面表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;与现有的模态识别技术和模型变体相比，该模型在性能上具有优越性，是一种有利的基于群体的结构健康监测方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：模态识别对于结构健康监测和控制至关重要，它提供了对结构动力学和性能的关键见解。本研究提出了一种新型的深度学习框架，该框架结合了图神经网络（GNN）、变换器和物理信息损失函数，以实现对结构群体进行模态分解和识别。变换器模块将多自由度（MDOF）结构动态测量分解为单自由度（SDOF）模态响应，从而便于识别自然频率和阻尼比。同时，GNN捕捉结构配置并识别与分解的SDOF模态响应相对应的模态形状。所提出的模型以纯物理信息和无监督的方式进行训练，利用模态分解理论和结构模态的独立性来指导学习，无需标签数据。通过数值模拟和实验室实验的验证表明，该模型在从稀疏结构动态测量中准确分解动态响应和识别模态特性方面具有有效性。与现有的模态识别技术和模型变体进行的比较分析进一步强调了其优越性能，使其成为基于群体的结构健康监测的有利方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modal identification is crucial for structural health monitoring andstructural control, providing critical insights into structural dynamics andperformance. This study presents a novel deep learning framework thatintegrates graph neural networks (GNNs), transformers, and a physics-informedloss function to achieve modal decomposition and identification across apopulation of structures. The transformer module decomposesmulti-degrees-of-freedom (MDOF) structural dynamic measurements intosingle-degree-of-freedom (SDOF) modal responses, facilitating theidentification of natural frequencies and damping ratios. Concurrently, the GNNcaptures the structural configurations and identifies mode shapes correspondingto the decomposed SDOF modal responses. The proposed model is trained in apurely physics-informed and unsupervised manner, leveraging modal decompositiontheory and the independence of structural modes to guide learning without theneed for labeled data. Validation through numerical simulations and laboratoryexperiments demonstrates its effectiveness in accurately decomposing dynamicresponses and identifying modal properties from sparse structural dynamicmeasurements, regardless of variations in external loads or structuralconfigurations. Comparative analyses against established modal identificationtechniques and model variations further underscore its superior performance,positioning it as a favorable approach for population-based structural healthmonitoring.</description>
      <author>example@mail.com (Xudong Jian, Kiran Bacsa, Gregory Duthé, Eleni Chatzi)</author>
      <guid isPermaLink="false">2505.04018v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models</title>
      <link>http://arxiv.org/abs/2505.03821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Dataset:  https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型（VLMs）在执行视觉视角推理的能力，通过一组新的视觉任务进行评估。&lt;h4&gt;背景&lt;/h4&gt;研究者受到人类测试的启发，设计了新的视觉任务来测试VLMs。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在场景理解、空间推理和视觉视角推理三个层面的认知能力。&lt;h4&gt;方法&lt;/h4&gt;研究者利用精心控制的场景，其中一个人形迷你人物与一个物体配对，通过改变物体位置、人形迷你人物的方向以及使用鸟瞰图和表面视图，创建了144个独特的视觉任务。每个任务都与一系列7个诊断问题相匹配，以评估上述三个认知层面。&lt;h4&gt;主要发现&lt;/h4&gt;评估了包括GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct和Claude Sonnet变体在内的多个最先进模型。结果显示，这些模型在场景理解方面表现良好，但在空间推理方面表现显著下降，在视角推理方面进一步恶化。&lt;h4&gt;结论&lt;/h4&gt;分析表明，在表面级别物体识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这指出了在未来的VLM发展中整合显式几何表示和定制训练协议的必要性。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了视觉语言模型（VLMs）执行视觉视角推理的能力，通过一组受人类测试启发的全新视觉任务进行探究。我们的方法利用了精心控制的场景，其中一个人形迷你人物与一个物体配对。通过系统地改变空间配置，如物体相对于人形迷你人物的位置和人形迷你人物的方向，以及使用鸟瞰图和表面视图，我们创建了144个独特的视觉任务。每个视觉任务都与一系列7个诊断问题相匹配，旨在评估三个层次的视觉认知：场景理解、空间推理和视觉视角推理。我们对包括GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct和Claude Sonnet变体在内的多个最先进模型进行了评估，发现尽管它们在场景理解方面表现出色，但在空间推理方面的表现显著下降，在视角推理方面的表现进一步恶化。我们的分析表明，在表面级别物体识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这指出了在未来的VLM发展中整合显式几何表示和定制训练协议的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the ability of Vision Language Models (VLMs) to perform visualperspective taking using a novel set of visual tasks inspired by establishedhuman tests. Our approach leverages carefully controlled scenes, in which asingle humanoid minifigure is paired with a single object. By systematicallyvarying spatial configurations - such as object position relative to thehumanoid minifigure and the humanoid minifigure's orientation - and using bothbird's-eye and surface-level views, we created 144 unique visual tasks. Eachvisual task is paired with a series of 7 diagnostic questions designed toassess three levels of visual cognition: scene understanding, spatialreasoning, and visual perspective taking. Our evaluation of severalstate-of-the-art models, including GPT-4-Turbo, GPT-4o,Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals thatwhile they excel in scene understanding, the performance declines significantlyon spatial reasoning and further deteriorates on perspective-taking. Ouranalysis suggests a gap between surface-level object recognition and the deeperspatial and perspective reasoning required for complex visual tasks, pointingto the need for integrating explicit geometric representations and tailoredtraining protocols in future VLM development.</description>
      <author>example@mail.com (Gracjan Góral, Alicja Ziarko, Piotr Miłoś, Michał Nauman, Maciej Wołczyk, Michał Kosiński)</author>
      <guid isPermaLink="false">2505.03821v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems</title>
      <link>http://arxiv.org/abs/2505.03946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了高性能计算（HPC）环境中的资源分配问题，提出了一种基于DD-PPO算法的新型调度器，以解决传统调度算法在异构和大规模系统中的效率与灵活性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;高性能计算环境中的资源分配是一个复杂的问题，调度算法不仅要高效分配系统资源，还要优化多个性能指标，如作业等待时间和系统利用率。&lt;h4&gt;目的&lt;/h4&gt;开发一种更加适应性和智能的调度策略，以解决传统调度算法在异构和大规模系统中的效率与灵活性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于DD-PPO算法的调度器，该算法支持大规模分布式训练，且无需在每一步进行参数同步。&lt;h4&gt;主要发现&lt;/h4&gt;与传统调度器和现有的基于RL的调度算法相比，DD-PPO调度器在调度性能上有所提升。&lt;h4&gt;结论&lt;/h4&gt;DD-PPO调度器通过消除对集中更新共享策略的依赖，提高了可扩展性、训练效率和样本利用率，从而在HPC调度中展现出更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高性能计算（HPC）环境中的资源分配对作业调度算法提出了复杂且多方面的挑战。除了高效分配系统资源之外，调度器还必须考虑并优化多个性能指标，包括作业等待时间和系统利用率。虽然基于规则的调度算法在当前的HPC系统部署中占主导地位，但随着这些系统的异构性和规模的增加，预计将挑战这些算法在最小化作业等待时间和最大化利用率方面的效率和灵活性。最近的研究努力集中在利用强化学习（RL）的进步来开发更适应性和智能的调度策略。最近基于RL的调度方法探索了从深度Q网络（DQN）到近端策略优化（PPO）等多种算法，以及最近将图神经网络与RL技术相结合的混合方法。然而，这些方法的共同局限性是它们依赖于相对较小的数据集，并且当使用大数据集时，这些方法面临着可扩展性问题。本研究介绍了一种基于DD-PPO算法的新型RL调度器，该算法支持跨多个工作者的分布式训练，而无需在每一步进行参数同步。通过消除对共享策略集中更新的依赖，DD-PPO调度器提高了可扩展性、训练效率和样本利用率。验证数据集利用了超过1150万个真实的HPC作业跟踪数据，用于比较DD-PPO在传统和先进调度方法之间的性能，实验结果表明，与基于规则的调度器和现有的基于RL的调度算法相比，DD-PPO调度器的调度性能有所改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Resource allocation in High Performance Computing (HPC) environments presentsa complex and multifaceted challenge for job scheduling algorithms. Beyond theefficient allocation of system resources, schedulers must account for andoptimize multiple performance metrics, including job wait time and systemutilization. While traditional rule-based scheduling algorithms dominate thecurrent deployments of HPC systems, the increasing heterogeneity and scale ofthose systems is expected to challenge the efficiency and flexibility of thosealgorithms in minimizing job wait time and maximizing utilization. Recentresearch efforts have focused on leveraging advancements in ReinforcementLearning (RL) to develop more adaptable and intelligent scheduling strategies.Recent RL-based scheduling approaches have explored a range of algorithms, fromDeep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,hybrid methods that integrate Graph Neural Networks with RL techniques.However, a common limitation across these methods is their reliance onrelatively small datasets, and these methods face scalability issues when usinglarge datasets. This study introduces a novel RL-based scheduler utilizing theDecentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,which supports large-scale distributed training across multiple workers withoutrequiring parameter synchronization at every step. By eliminating reliance oncentralized updates to a shared policy, the DD-PPO scheduler enhancesscalability, training efficiency, and sample utilization. The validationdataset leveraged over 11.5 million real HPC job traces for comparing DD-PPOperformance between traditional and advanced scheduling approaches, and theexperimental results demonstrate improved scheduling performance in comparisonto both rule-based schedulers and existing RL-based scheduling algorithms.</description>
      <author>example@mail.com (Matthew Sgambati, Aleksandar Vakanski, Matthew Anderson)</author>
      <guid isPermaLink="false">2505.03946v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype</title>
      <link>http://arxiv.org/abs/2505.03853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GRAPE的基因调控网络构建方法，通过结合预训练的大型语言模型和DNA序列模型，提高基因调控网络的构建效率。&lt;h4&gt;背景&lt;/h4&gt;预测基因扰动有助于在实验室实验之前识别关键基因，提高实验效率。构建基因调控网络对于理解和预测基因扰动的影响至关重要。&lt;h4&gt;目的&lt;/h4&gt;提高基因调控网络的构建效率和准确性，并能够捕捉基因间的潜在相互作用。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的大型语言模型和DNA序列模型提取基因描述和DNA序列数据中的特征，引入基因生物型信息，并通过图结构学习动态优化基因调控网络。&lt;h4&gt;主要发现&lt;/h4&gt;GRAPE方法在公开数据集上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;GRAPE方法通过结合多源信息和图结构学习，能够有效构建基因调控网络，提高基因扰动预测的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting genetic perturbations enables the identification of potentiallycrucial genes prior to wet-lab experiments, significantly improving overallexperimental efficiency. Since genes are the foundation of cellular life,building gene regulatory networks (GRN) is essential to understand and predictthe effects of genetic perturbations. However, current methods fail to fullyleverage gene-related information, and solely rely on simple evaluation metricsto construct coarse-grained GRN. More importantly, they ignore functionaldifferences between biotypes, limiting the ability to capture potential geneinteractions. In this work, we leverage pre-trained large language model andDNA sequence model to extract features from gene descriptions and DNA sequencedata, respectively, which serve as the initialization for gene representations.Additionally, we introduce gene biotype information for the first time ingenetic perturbation, simulating the distinct roles of genes with differentbiotypes in regulating cellular processes, while capturing implicit generelationships through graph structure learning (GSL). We propose GRAPE, aheterogeneous graph neural network (HGNN) that leverages gene representationsinitialized with features from descriptions and sequences, models the distinctroles of genes with different biotypes, and dynamically refines the GRN throughGSL. The results on publicly available datasets show that our method achievesstate-of-the-art performance.</description>
      <author>example@mail.com (Changxi Chi, Jun Xia, Jingbo Zhou, Jiabei Cheng, Chang Yu, Stan Z. Li)</author>
      <guid isPermaLink="false">2505.03853v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>QStore: Quantization-Aware Compressed Model Storage</title>
      <link>http://arxiv.org/abs/2505.04081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了QStore，一种用于高效存储模型两种精度（高精度和低精度）的无损压缩格式。&lt;h4&gt;背景&lt;/h4&gt;现代应用广泛使用大型多模态基础模型，这些应用通常具有复杂的流程，需要存储和使用多个精度的相似模型。&lt;h4&gt;目的&lt;/h4&gt;降低存储成本，同时不牺牲模型加载速度。&lt;h4&gt;方法&lt;/h4&gt;QStore存储低精度模型以及重建高精度模型所需的残差信息，而不是分别存储低精度和高精度模型。&lt;h4&gt;主要发现&lt;/h4&gt;QStore在压缩多个精度的流行基础模型时，可以将整体存储占用减少高达2.2倍（原大小的45%），同时模型保存和加载速度比现有方法快1.7倍和1.8倍。&lt;h4&gt;结论&lt;/h4&gt;QStore是一种有效的解决方案，可以同时存储模型的两种精度，同时显著降低存储成本并提高加载速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern applications commonly leverage large, multi-modal foundation models.These applications often feature complex workflows that demand the storage andusage of similar models in multiple precisions. A straightforward approach isto maintain a separate file for each model precision (e.g., INT8, BF16), whichis indeed the approach taken by many model providers such as HuggingFace andOllama. However, this approach incurs excessive storage costs since a higherprecision model (e.g., BF16) is a strict superset of a lower precision model(e.g., INT8) in terms of information. Unfortunately, simply maintaining onlythe higher-precision model and requiring every user to dynamically convert themodel precision is not desirable because every user of lower precision modelsmust pay the cost for model download and precision conversion.  In this paper, we present QStore, a unified, lossless compression format forsimultaneously storing a model in two (high and low) precisions efficiently.Instead of storing low-precision and high-precision models separately, QStorestores low-precision model and only the residual information needed toreconstruct high-precision models. The size of residual information issignificantly smaller than the original high-precision models, thus achievinghigh savings in storage cost. Moreover, QStore does not compromise the speed ofmodel loading. The low-precision models can be loaded quickly just like before.The high-precision models can also be reconstructed efficiently in memory bymerging low-precision data and the residual with QStore's lightweight decodinglogic. We evaluate QStore for compressing multiple precisions of popularfoundation models, and show that QStore reduces overall storage footprint by upto 2.2x (45% of the original size) while enabling up to 1.7x and 1.8x fastermodel saving and loading versus existing approaches.</description>
      <author>example@mail.com (Raunak Shah, Zhaoheng Li, Yongjoo Park)</author>
      <guid isPermaLink="false">2505.04081v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>BuildingBlock: A Hybrid Approach for Structured Building Generation</title>
      <link>http://arxiv.org/abs/2505.04051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH 2025 (Conference Track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BuildingBlock的混合方法，用于生成多样化、结构化且层次一致的建筑模型，适用于游戏、虚拟现实和数字孪生等领域。&lt;h4&gt;背景&lt;/h4&gt;当前三维建筑生成方法面临生成多样化、结构化和层次一致建筑模型的挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出BuildingBlock方法来解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;BuildingBlock方法包括两个阶段：布局生成阶段（LGP）和建筑构建阶段（BCP）。LGP将基于箱子的布局生成重新定义为点云生成任务，使用新构建的建筑数据集和基于Transformer的扩散模型创建全局一致的布局。利用LLM将这些布局扩展为基于规则的层次化设计，无缝整合组件风格和空间结构。BCP利用这些布局来指导程序内容生成（PCG），实现局部可定制、高质量的结构化建筑生成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，BuildingBlock方法在生成多样化、层次化结构化的建筑方面是有效的，在多个基准测试中达到了最先进的结果，并为进一步的可扩展和直观的建筑工作流程铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;BuildingBlock方法为三维建筑生成提供了一种新的有效途径，有助于推动相关领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3721238.3730705&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional building generation is vital for applications in gaming,virtual reality, and digital twins, yet current methods face challenges inproducing diverse, structured, and hierarchically coherent buildings. Wepropose BuildingBlock, a hybrid approach that integrates generative models,procedural content generation (PCG), and large language models (LLMs) toaddress these limitations. Specifically, our method introduces a two-phasepipeline: the Layout Generation Phase (LGP) and the Building Construction Phase(BCP).  LGP reframes box-based layout generation as a point-cloud generation task,utilizing a newly constructed architectural dataset and a Transformer-baseddiffusion model to create globally consistent layouts. With LLMs, these layoutsare extended into rule-based hierarchical designs, seamlessly incorporatingcomponent styles and spatial structures.  The BCP leverages these layouts to guide PCG, enabling local-customizable,high-quality structured building generation. Experimental results demonstrateBuildingBlock's effectiveness in generating diverse and hierarchicallystructured buildings, achieving state-of-the-art results on multiplebenchmarks, and paving the way for scalable and intuitive architecturalworkflows.</description>
      <author>example@mail.com (Junming Huang, Chi Wang, Letian Li, Changxin Huang, Qiang Dai, Weiwei Xu)</author>
      <guid isPermaLink="false">2505.04051v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>VideoLLM Benchmarks and Evaluation: A Survey</title>
      <link>http://arxiv.org/abs/2505.03829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对视频大型语言模型（VideoLLMs）的基准和评估方法进行了全面分析。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）的快速发展推动了视频理解技术的进步。&lt;h4&gt;目的&lt;/h4&gt;旨在为研究人员提供如何有效评估VideoLLMs的结构化理解，并识别视频理解领域的发展方向。&lt;h4&gt;方法&lt;/h4&gt;分析了视频理解基准的特点、评估协议和局限性，包括闭集、开集和针对时间和时空理解任务的专业评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;指出了最先进VideoLLMs在这些基准上的性能趋势，并确定了当前评估框架中的关键挑战。&lt;h4&gt;结论&lt;/h4&gt;提出了未来研究方向，包括改进基准设计、评估指标和协议，需要更多样化、多模态和可解释性强的基准。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大型语言模型（LLMs）的快速发展推动了视频理解技术的显著进步。本文对专为视频大型语言模型（VideoLLMs）设计或使用的基准和评估方法进行了全面分析。我们考察了视频理解基准的现状，讨论了其特点、评估协议和局限性。论文分析了各种评估方法，包括闭集、开集以及针对时间和时空理解任务的专业评估。我们强调了最先进VideoLLMs在这些基准上的性能趋势，并确定了当前评估框架中的关键挑战。此外，我们提出了未来研究方向，包括改进基准设计、评估指标和协议，包括需要更多样化、多模态和可解释性强的基准。本调查旨在为研究人员提供如何有效评估VideoLLMs的结构化理解，并识别利用大型语言模型推进视频理解领域的前景方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Large Language Models (LLMs) has catalyzedsignificant advancements in video understanding technologies. This surveyprovides a comprehensive analysis of benchmarks and evaluation methodologiesspecifically designed or used for Video Large Language Models (VideoLLMs). Weexamine the current landscape of video understanding benchmarks, discussingtheir characteristics, evaluation protocols, and limitations. The paperanalyzes various evaluation methodologies, including closed-set, open-set, andspecialized evaluations for temporal and spatiotemporal understanding tasks. Wehighlight the performance trends of state-of-the-art VideoLLMs across thesebenchmarks and identify key challenges in current evaluation frameworks.Additionally, we propose future research directions to enhance benchmarkdesign, evaluation metrics, and protocols, including the need for more diverse,multimodal, and interpretability-focused benchmarks. This survey aims to equipresearchers with a structured understanding of how to effectively evaluateVideoLLMs and identify promising avenues for advancing the field of videounderstanding with large language models.</description>
      <author>example@mail.com (Yogesh Kumar)</author>
      <guid isPermaLink="false">2505.03829v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques</title>
      <link>http://arxiv.org/abs/2505.03848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  46 pages, 22 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种集成了拓扑数据分析（TDA）、自监督学习和迁移学习的高级聚类框架，用于处理半导体制造中生成的大量图像数据，以提高缺陷识别和产量优化。&lt;h4&gt;背景&lt;/h4&gt;半导体制造产生的图像数据量巨大，对于缺陷识别和产量优化至关重要，但通常超过了人工检查的能力。&lt;h4&gt;目的&lt;/h4&gt;旨在通过一种新的无监督图像聚类方法，提高高维、未标记数据在缺陷识别和产量优化方面的有效性。&lt;h4&gt;方法&lt;/h4&gt;该框架结合了TDA捕获内在拓扑特征，自监督学习从未标记数据中提取有意义的表示，以及迁移学习提高框架的适应性和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和开源半导体图像数据集上验证，该框架成功识别了与缺陷模式和工艺变化一致的聚类。&lt;h4&gt;结论&lt;/h4&gt;本研究突出了结合TDA、自监督学习和迁移学习的变革潜力，为半导体制造和其他具有大规模图像数据集的领域提供了可扩展的主动过程监控和质量控制解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces an advanced clustering framework that integrates deep Topological Data Analysis (TDA) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. Transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. Validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. This study highlights the transformative potential of combining TDA, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing generates vast amounts of image data, crucial fordefect identification and yield optimization, yet often exceeds manualinspection capabilities. Traditional clustering techniques struggle withhigh-dimensional, unlabeled data, limiting their effectiveness in capturingnuanced patterns. This paper introduces an advanced clustering framework thatintegrates deep Topological Data Analysis (TDA) with self-supervised andtransfer learning techniques, offering a novel approach to unsupervised imageclustering. TDA captures intrinsic topological features, while self-supervisedlearning extracts meaningful representations from unlabeled data, reducingreliance on labeled datasets. Transfer learning enhances the framework'sadaptability and scalability, allowing fine-tuning to new datasets withoutretraining from scratch. Validated on synthetic and open-source semiconductorimage datasets, the framework successfully identifies clusters aligned withdefect patterns and process variations. This study highlights thetransformative potential of combining TDA, self-supervised learning, andtransfer learning, providing a scalable solution for proactive processmonitoring and quality control in semiconductor manufacturing and other domainswith large-scale image datasets.</description>
      <author>example@mail.com (Janhavi Giri, Attila Lengyel, Don Kent, Edward Kibardin)</author>
      <guid isPermaLink="false">2505.03848v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</title>
      <link>http://arxiv.org/abs/2505.01880v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9pages, 5figures. This paper has been accepted for IJCAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种渐进式音频-语言共学习网络（LOCO），用于定位部分伪造音频的伪造区域，通过协同学习和自监督方式在弱监督场景下提升定位性能。&lt;h4&gt;背景&lt;/h4&gt;现有的音频时间伪造定位方法依赖于使用细粒度注释训练高效网络，但在实际场景中获取这些注释成本高且具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文旨在提出一种能够有效定位伪造区域的方法。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了音频-语言共学习模块，通过时间维度和全局维度的语义对齐来捕捉伪造的一致性特征；2. 使用语句级注释和可学习提示构建伪造感知提示，动态地将语义先验纳入时间内容特征；3. 应用伪造定位模块，基于融合的伪造类激活序列生成伪造提议；4. 引入渐进式细化策略，生成伪帧级标签，并利用监督语义对比学习增强真实内容与伪造内容之间的语义区别，从而持续优化伪造感知特征。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公开基准数据集上的实验表明，提出的LOCO实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在音频时间伪造定位任务上取得了显著的成果，为解决现实场景中的挑战提供了一种有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio temporal forgery localization (ATFL) aims to find the precise forgeryregions of the partial spoof audio that is purposefully modified. Existing ATFLmethods rely on training efficient networks using fine-grained annotations,which are obtained costly and challenging in real-world scenarios. To meet thischallenge, in this paper, we propose a progressive audio-language co-learningnetwork (LOCO) that adopts co-learning and self-supervision manners to promptlocalization performance under weak supervision scenarios. Specifically, anaudio-language co-learning module is first designed to capture forgeryconsensus features by aligning semantics from temporal and global perspectives.In this module, forgery-aware prompts are constructed by using utterance-levelannotations together with learnable prompts, which can incorporate semanticpriors into temporal content features dynamically. In addition, a forgerylocalization module is applied to produce forgery proposals based on fusedforgery-class activation sequences. Finally, a progressive refinement strategyis introduced to generate pseudo frame-level labels and leverage supervisedsemantic contrastive learning to amplify the semantic distinction between realand fake content, thereby continuously optimizing forgery-aware features.Extensive experiments show that the proposed LOCO achieves SOTA performance onthree public benchmarks.</description>
      <author>example@mail.com (Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo)</author>
      <guid isPermaLink="false">2505.01880v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
      <link>http://arxiv.org/abs/2505.02831v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Self-Representation Alignment for Diffusion Transformers&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为Self-Representation Alignment (SRA)的方法，通过自我蒸馏的方式，在不引入额外复杂框架或依赖外部预训练模型的情况下，提高扩散变换器的内部表示学习，从而加速生成训练并提高生成质量。&lt;h4&gt;背景&lt;/h4&gt;现有方法要么需要引入额外的复杂表示训练框架，要么依赖大规模预训练的表示基础模型来在原始生成训练过程中提供表示指导。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，使扩散变换器能够在不依赖外部表示组件的情况下提供表示指导。&lt;h4&gt;方法&lt;/h4&gt;SRA通过自我蒸馏的方式，将扩散变换器早期层的高噪声输出潜在表示与后期层的低噪声输出潜在表示进行对齐，以逐步增强仅在生成训练过程中的整体表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，将SRA应用于DiTs和SiTs可以带来一致的性能提升。SRA不仅显著优于依赖于辅助复杂表示训练框架的方法，而且其性能与高度依赖强大外部表示先验的方法相当。&lt;h4&gt;结论&lt;/h4&gt;SRA是一种简单而直接的方法，可以有效地提高扩散变换器的表示学习，并显著提升生成质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated that learning a meaningful internalrepresentation can both accelerate generative training and enhance generationquality of the diffusion transformers. However, existing approaches necessitateto either introduce an additional and complex representation training frameworkor rely on a large-scale, pre-trained representation foundation model toprovide representation guidance during the original generative trainingprocess. In this study, we posit that the unique discriminative processinherent to diffusion transformers enables them to offer such guidance withoutrequiring external representation components. We therefore proposeSelf-Representation Alignment (SRA), a simple yet straightforward method thatobtain representation guidance through a self-distillation manner.Specifically, SRA aligns the output latent representation of the diffusiontransformer in earlier layer with higher noise to that in later layer withlower noise to progressively enhance the overall representation learning duringonly generative training process. Experimental results indicate that applyingSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRAnot only significantly outperforms approaches relying on auxiliary, complexrepresentation training frameworks but also achieves performance comparable tomethods that heavily dependent on powerful external representation priors.</description>
      <author>example@mail.com (Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang)</author>
      <guid isPermaLink="false">2505.02831v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments</title>
      <link>http://arxiv.org/abs/2505.03825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in Expert Systems with Applications (DOI pending)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IntelligentlyAugmented Contrastive Tensor Factorization (ITA-CTF)的框架，用于从多维时间序列中学习有效的表示，并解决了低训练数据环境下学习复杂特征的问题。&lt;h4&gt;背景&lt;/h4&gt;在现实世界系统中对多维时间序列进行分类需要精细学习复杂特征，如跨维依赖和类内变化，同时面临训练数据不足的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种既灵活又高效的方法来学习多维时间序列的有效表示，并提高分类性能。&lt;h4&gt;方法&lt;/h4&gt;ITA-CTF框架包括CTF模块和ITA模块。CTF模块学习时间序列的核心解释成分及其联合依赖，并通过对比损失优化来提高分类性能。ITA模块生成针对性强且信息丰富的增强数据，以强调原始数据中的类内模式，同时保留类属性。&lt;h4&gt;主要发现&lt;/h4&gt;与标准TF和多个深度学习基准相比，ITA-CTF在五个不同的分类任务上实现了显著的性能提升，最高可达18.7%。&lt;h4&gt;结论&lt;/h4&gt;ITA-CTF是一种有效的框架，可以在低训练数据环境下对多维时间序列进行分类，并通过智能增强和对比学习显著提高分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification of multi-dimensional time series from real-world systemsrequire fine-grained learning of complex features such as cross-dimensionaldependencies and intra-class variations-all under the practical challenge oflow training data availability. However, standard deep learning (DL) strugglesto learn generalizable features in low-data environments due to modeloverfitting. We propose a versatile yet data-efficient framework, IntelligentlyAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effectiverepresentations from multi-dimensional time series. The CTF module learns coreexplanatory components of the time series (e.g., sensor factors, temporalfactors), and importantly, their joint dependencies. Notably, unlike standardtensor factorization (TF), the CTF module incorporates a new contrastive lossoptimization to induce similarity learning and class-awareness into the learntrepresentations for better classification performance. To strengthen thiscontrastive learning, the preceding ITA module generates targeted butinformative augmentations that highlight realistic intra-class patterns in theoriginal data, while preserving class-wise properties. This is achieved bydynamically sampling a "soft" class prototype to guide the warping of eachquery data sample, which results in an augmentation that is intelligentlypattern-mixed between the "soft" class prototype and the query sample. Theseaugmentations enable the CTF module to recognize complex intra-class variationsdespite the limited original training data, and seek out invariant class-wiseproperties for accurate classification performance. The proposed method iscomprehensively evaluated on five different classification tasks. Compared tostandard TF and several DL benchmarks, notable performance improvements up to18.7% were achieved.</description>
      <author>example@mail.com (Anushiya Arunan, Yan Qin, Xiaoli Li, Yuen Chau)</author>
      <guid isPermaLink="false">2505.03825v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering</title>
      <link>http://arxiv.org/abs/2505.02173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时间序列聚类方法，用于将具有相似行为的时间序列数据分类到不同的组中。&lt;h4&gt;背景&lt;/h4&gt;时间序列聚类是一种无监督学习方法，已在医疗保健、金融、经济、能源和气候科学等领域得到应用。已有多种时间序列聚类方法被提出并使用超过四十年，大多数方法集中于测量时间序列之间的欧几里得距离或关联差异。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的相似性度量方法，称为排序皮尔逊相关差异（RDPC），并将其应用于层次聚类，以评估和比较现有聚类算法的性能。&lt;h4&gt;方法&lt;/h4&gt;RDPC方法结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异。&lt;h4&gt;主要发现&lt;/h4&gt;RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。&lt;h4&gt;结论&lt;/h4&gt;通过将泰国的电力消耗时间序列数据集的随机样本聚类成具有独特特征的七个组，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：时间序列聚类是一种无监督学习方法，用于将具有相似行为的时间序列数据分类到具有相似行为的组中。它在医疗保健、金融、经济、能源和气候科学等领域得到应用。已经提出了多种时间序列聚类方法，并且已经使用了超过四十年。大多数方法集中于测量时间序列之间的欧几里得距离或关联差异。在这项工作中，我们提出了一种新的差异度量方法，称为排序皮尔逊相关差异（RDPC），它结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异。它被纳入层次聚类中。性能被评估并与现有聚类算法进行了比较。结果表明，RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。最后，我们通过将泰国的电力消耗时间序列数据集的随机样本聚类成具有独特特征的七个组，证明了我们的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series clustering is an unsupervised learning method for classifyingtime series data into groups with similar behavior. It is used in applicationssuch as healthcare, finance, economics, energy, and climate science. Severaltime series clustering methods have been introduced and used for over fourdecades. Most of them focus on measuring either Euclidean distances orassociation dissimilarities between time series. In this work, we propose a newdissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),which combines a weighted average of a specified fraction of the largestelement-wise differences with the well-known Pearson correlation dissimilarity.It is incorporated into hierarchical clustering. The performance is evaluatedand compared with existing clustering algorithms. The results show that theRDPC algorithm outperforms others in complicated cases involving differentseasonal patterns, trends, and peaks. Finally, we demonstrate our method byclustering a random sample of customers from a Thai electricity consumptiontime series dataset into seven groups with unique characteristics.</description>
      <author>example@mail.com (Chutiphan Charoensuk, Nathakhun Wiroonsri)</author>
      <guid isPermaLink="false">2505.02173v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>PointExplainer: Towards Transparent Parkinson's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2505.03833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointExplainer的可解释诊断策略，用于分析手绘信号以早期诊断帕金森病，并通过实验证明其能够提供直观的解释而不会降低诊断性能。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在分析数字化手绘信号方面显示出潜力，但现有诊断方法的可解释性不足，这给临床信任带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出PointExplainer，旨在识别推动模型诊断的手绘区域，并量化它们对模型决策的相对贡献。&lt;h4&gt;方法&lt;/h4&gt;PointExplainer包括一个诊断模块和一个解释模块。诊断模块将手绘信号编码为3D点云以表示手绘轨迹；解释模块训练一个可解释的代理模型来近似黑盒诊断模型的行为。此外，还引入了一致性度量来解决解释中的忠实度问题。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集和一个新构建的数据集上的实验表明，PointExplainer能够提供直观的解释，并且没有诊断性能的下降。&lt;h4&gt;结论&lt;/h4&gt;PointExplainer是一种有效的可解释诊断策略，有助于提高临床对帕金森病早期诊断的信任。&lt;h4&gt;翻译&lt;/h4&gt;Deep neural networks have shown potential in analyzing digitized hand-drawnsignals for early diagnosis of Parkinson's disease. However, the lack of clearinterpretability in existing diagnostic methods presents a challenge toclinical trust. In this paper, we propose PointExplainer, an explainablediagnostic strategy to identify hand-drawn regions that drive model diagnosis.Specifically, PointExplainer assigns discrete attribution values to hand-drawnsegments, explicitly quantifying their relative contributions to the model'sdecision. Its key components include: (i) a diagnosis module, which encodeshand-drawn signals into 3D point clouds to represent hand-drawn trajectories,and (ii) an explanation module, which trains an interpretable surrogate modelto approximate the local behavior of the black-box diagnostic model. We alsointroduce consistency measures to further address the issue of faithfulness inexplanations. Extensive experiments on two benchmark datasets and a newlyconstructed dataset show that PointExplainer can provide intuitive explanationswith no diagnostic performance degradation. The source code is available athttps://github.com/chaoxuewang/PointExplainer.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks have shown potential in analyzing digitized hand-drawnsignals for early diagnosis of Parkinson's disease. However, the lack of clearinterpretability in existing diagnostic methods presents a challenge toclinical trust. In this paper, we propose PointExplainer, an explainablediagnostic strategy to identify hand-drawn regions that drive model diagnosis.Specifically, PointExplainer assigns discrete attribution values to hand-drawnsegments, explicitly quantifying their relative contributions to the model'sdecision. Its key components include: (i) a diagnosis module, which encodeshand-drawn signals into 3D point clouds to represent hand-drawn trajectories,and (ii) an explanation module, which trains an interpretable surrogate modelto approximate the local behavior of the black-box diagnostic model. We alsointroduce consistency measures to further address the issue of faithfulness inexplanations. Extensive experiments on two benchmark datasets and a newlyconstructed dataset show that PointExplainer can provide intuitive explanationswith no diagnostic performance degradation. The source code is available athttps://github.com/chaoxuewang/PointExplainer.</description>
      <author>example@mail.com (Xuechao Wang, Sven Nomm, Junqing Huang, Kadri Medijainen, Aaro Toomela, Michael Ruzhansky)</author>
      <guid isPermaLink="false">2505.03833v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating the Impact of Electrode Shift on Classification Performance in Electromyography-Based Motion Prediction Using Sliding-Window Normalization</title>
      <link>http://arxiv.org/abs/2504.03196v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要讨论了电磁肌电图（EMG）信号在假肢、辅助服装和康复等领域的应用，提出了滑动窗口归一化（SWN）技术来减少电极偏移引起的性能下降。&lt;h4&gt;背景&lt;/h4&gt;EMG信号广泛应用于多种领域，但在跨个体泛化、电极偏移和日常变化等方面仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;通过提出SWN技术，旨在减少电极偏移对分类性能的影响，从而提高EMG信号的实用性。&lt;h4&gt;方法&lt;/h4&gt;采用滑动窗口归一化方法，结合z分数归一化和滑动窗口技术，在实时预测场景下减少电极偏移导致的分类性能下降。&lt;h4&gt;主要发现&lt;/h4&gt;SWN技术在肘关节运动（休息、屈曲和伸展）的EMG信号分类中，将分类准确率的下降减少了1.0%，相比无归一化的情况提高了6.6%。当SWN与多电极位置混合策略结合使用时，分类准确率比基准提高了2.4%。&lt;h4&gt;结论&lt;/h4&gt;SWN技术能够有效减少电极偏移引起的性能下降，增强基于EMG的运动估计系统的实用性。&lt;h4&gt;翻译&lt;/h4&gt;Electromyography (EMG) signals are used in many applications, including prosthetic hands, assistive suits, and rehabilitation. Recent advances in motion estimation have improved performance, yet challenges remain in cross-subject generalization, electrode shift, and daily variations. When electrode shift occurs, both transfer learning and adversarial domain adaptation improve classification performance by reducing the performance gap to -1% (eight-class scenario). However, additional data are needed for re-training in transfer learning or for training in adversarial domain adaptation. To address this issue, we investigated a sliding-window normalization (SWN) technique in a real-time prediction scenario. This method combines z-score normalization with a sliding-window approach to reduce the decline in classification performance caused by electrode shift. We validated the effectiveness of SWN using experimental data from a target trajectory tracking task involving the right arm. For three motions classification (rest, flexion, and extension of the elbow) obtained from EMG signals, our offline analysis showed that SWN reduced the differential classification accuracy to -1.0%, representing a 6.6% improvement compared to the case without normalization (-7.6%). Furthermore, when SWN was combined with a strategy that uses a mixture of multiple electrode positions, classification accuracy improved by an additional 2.4% over the baseline. These results suggest that SWN can effectively reduce the performance degradation caused by electrode shift, thereby enhancing the practicality of EMG-based motion estimation systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electromyography (EMG) signals are used in many applications, includingprosthetic hands, assistive suits, and rehabilitation. Recent advances inmotion estimation have improved performance, yet challenges remain incross-subject generalization, electrode shift, and daily variations. Whenelectrode shift occurs, both transfer learning and adversarial domainadaptation improve classification performance by reducing the performance gapto -1\% (eight-class scenario). However, additional data are needed forre-training in transfer learning or for training in adversarial domainadaptation. To address this issue, we investigated a sliding-windownormalization (SWN) technique in a real-time prediction scenario. This methodcombines z-score normalization with a sliding-window approach to reduce thedecline in classification performance caused by electrode shift. We validatedthe effectiveness of SWN using experimental data from a target trajectorytracking task involving the right arm. For three motions classification (rest,flexion, and extension of the elbow) obtained from EMG signals, our offlineanalysis showed that SWN reduced the differential classification accuracy to-1.0\%, representing a 6.6\% improvement compared to the case withoutnormalization (-7.6\%). Furthermore, when SWN was combined with a strategy thatuses a mixture of multiple electrode positions, classification accuracyimproved by an additional 2.4\% over the baseline. These results suggest thatSWN can effectively reduce the performance degradation caused by electrodeshift, thereby enhancing the practicality of EMG-based motion estimationsystems.</description>
      <author>example@mail.com (Taichi Tanaka, Isao Nambu, Yasuhiro Wada)</author>
      <guid isPermaLink="false">2504.03196v2</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective</title>
      <link>http://arxiv.org/abs/2505.03828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 tables, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文全面回顾了从自然语言处理角度的带有情感感知的推荐系统，涵盖了2023年到2025年初的进展。&lt;h4&gt;背景&lt;/h4&gt;电子商务平台产生了大量的用户反馈，如星级评分、书面评论和评论，但大多数推荐引擎主要依赖数值评分，往往忽略了自由文本中嵌入的细微意见。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过将情感分析集成到电子商务推荐器中，通过详细的意见提取来提高预测准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;本文将近期的工作分为四种主要方法：结合情感嵌入与用户物品交互的深度学习分类器、基于transformer的细微特征提取方法、传播情感信号的图神经网络和实时适应用户反馈的对话式推荐器。&lt;h4&gt;主要发现&lt;/h4&gt;本文总结了模型架构，并展示了情感如何通过推荐管道传递，影响基于对话的建议。关键挑战包括处理嘈杂或讽刺性文本、动态用户偏好和偏见缓解。&lt;h4&gt;结论&lt;/h4&gt;本文概述了研究差距，并提供了开发更智能、更公平、更以用户为中心的推荐工具的路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; E-commerce platforms generate vast volumes of user feedback, such as starratings, written reviews, and comments. However, most recommendation enginesrely primarily on numerical scores, often overlooking the nuanced opinionsembedded in free text. This paper comprehensively reviews sentiment-awarerecommendation systems from a natural language processing perspective, coveringadvancements from 2023 to early 2025. It highlights the benefits of integratingsentiment analysis into e-commerce recommenders to enhance prediction accuracyand explainability through detailed opinion extraction. Our survey categorizesrecent work into four main approaches: deep learning classifiers that combinesentiment embeddings with user item interactions, transformer based methods fornuanced feature extraction, graph neural networks that propagate sentimentsignals, and conversational recommenders that adapt in real time to userfeedback. We summarize model architectures and demonstrate how sentiment flowsthrough recommendation pipelines, impacting dialogue-based suggestions. Keychallenges include handling noisy or sarcastic text, dynamic user preferences,and bias mitigation. Finally, we outline research gaps and provide a roadmapfor developing smarter, fairer, and more user-centric recommendation tools.</description>
      <author>example@mail.com (Yogesh Gajula)</author>
      <guid isPermaLink="false">2505.03828v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling</title>
      <link>http://arxiv.org/abs/2505.03799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in International Joint Conference on Neural Networks  (IJCNN), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDM-InstructGLM的新型指令调整图语言模型框架，用于解决大规模图中LLMs在处理图结构时的可扩展性和效率问题。&lt;h4&gt;背景&lt;/h4&gt;LLMs在自然语言处理任务中表现出色，但在处理图相关问题时，由于可扩展性限制和缺乏针对图结构的专用机制，其应用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;旨在提高LLMs在图结构处理中的可扩展性和效率，同时避免依赖GNNs。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于相似度-度数偏差的随机游走机制，该机制根据节点特征相似度和度中心性选择性地采样和编码图信息，确保在LLM中实现自适应和结构化的表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了令牌效率，减少了随机采样引起的信息损失，并在节点分类和链接预测等图相关任务上提高了性能。结果表明，仅使用LLMs进行图处理是可行的。&lt;h4&gt;结论&lt;/h4&gt;该研究为无GNN的图学习方法铺平了道路，利用LLMs作为独立的图推理模型。&lt;h4&gt;翻译&lt;/h4&gt;Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated strong capabilities in variousnatural language processing tasks; however, their application to graph-relatedproblems remains limited, primarily due to scalability constraints and theabsence of dedicated mechanisms for processing graph structures. Existingapproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),using GNNs as feature encoders or auxiliary components. However, directlyencoding graph structures within LLMs has been underexplored, particularly inthe context of large-scale graphs where token limitations hinder effectiverepresentation. To address these challenges, we propose SDM-InstructGLM, anovel instruction-tuned Graph Language Model (InstructGLM) framework thatenhances scalability and efficiency without relying on GNNs. Our methodintroduces a similarity-degree-based biased random walk mechanism, whichselectively samples and encodes graph information based on node-featuresimilarity and degree centrality, ensuring an adaptive and structuredrepresentation within the LLM. This approach significantly improves tokenefficiency, mitigates information loss due to random sampling, and enhancesperformance on graph-based tasks such as node classification and linkprediction. Furthermore, our results demonstrate the feasibility of LLM-onlygraph processing, enabling scalable and interpretable Graph Language Models(GLMs) optimized through instruction-based fine-tuning. This work paves the wayfor GNN-free approaches to graph learning, leveraging LLMs as standalone graphreasoning models. Our source code is available on GitHub.</description>
      <author>example@mail.com (Hyun Lee, Chris Yi, Maminur Islam, B. D. S. Aritra)</author>
      <guid isPermaLink="false">2505.03799v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation</title>
      <link>http://arxiv.org/abs/2505.03844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用空间条件技术将卫星SAR图像转换为机载SAR表示的新方法，旨在解决SAR图像数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，合成孔径雷达（SAR）卫星图像的获取能力显著提高，但高分辨率SAR图像的获取仍然昂贵且有限。缺乏开源、标注良好的SAR文本图像数据集限制了现有基础模型在遥感应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;通过合成图像生成技术来扩充稀缺的SAR图像数据，以促进更广泛的应用。&lt;h4&gt;方法&lt;/h4&gt;利用ONERA超过15年的航空数据，创建了包含11万张SAR图像的全面训练数据集，并利用一个预训练的包含35亿参数的潜在扩散模型进行探索。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效地将模拟图像与ONERA基于物理的模拟器EMPRISE生成的图像的现实感相连接，探索了AI在推进SAR成像技术中的应用。&lt;h4&gt;结论&lt;/h4&gt;本研究首次在文献中引入了这种新方法，为SAR图像处理领域提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The availability of Synthetic Aperture Radar (SAR) satellite imagery hasincreased considerably in recent years, with datasets commercially available.However, the acquisition of high-resolution SAR images in airborneconfigurations, remains costly and limited. Thus, the lack of open source,well-labeled, or easily exploitable SAR text-image datasets is a barrier to theuse of existing foundation models in remote sensing applications. In thiscontext, synthetic image generation is a promising solution to augment thisscarce data, enabling a broader range of applications. Leveraging over 15 yearsof ONERA's extensive archival airborn data from acquisition campaigns, wecreated a comprehensive training dataset of 110 thousands SAR images to exploita 3.5 billion parameters pre-trained latent diffusion model. In this work, wepresent a novel approach utilizing spatial conditioning techniques within afoundation model to transform satellite SAR imagery into airborne SARrepresentations. Additionally, we demonstrate that our pipeline is effectivefor bridging the realism of simulated images generated by ONERA's physics-basedsimulator EMPRISE. Our method explores a key application of AI in advancing SARimaging technology. To the best of our knowledge, we are the first to introducethis approach in the literature.</description>
      <author>example@mail.com (Solène Debuysère, Nicolas Trouvé, Nathan Letheule, Olivier Lévêque, Elise Colin)</author>
      <guid isPermaLink="false">2505.03844v1</guid>
      <pubDate>Thu, 08 May 2025 14:13:06 +0800</pubDate>
    </item>
    <item>
      <title>TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2505.02052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于压力传感器的活动识别方法，通过使用生成式基础模型和压力特定的人体活动识别技术，实现了压力数据的自然语言解释，从而提高了人体活动识别的性能。&lt;h4&gt;背景&lt;/h4&gt;人体活动识别（HAR）主要关注惯性测量单元和视觉数据，而忽略了压力传感器在捕捉身体动态和重心变化方面的独特能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决压力传感器在HAR领域应用不足的问题，提出利用生成式基础模型和压力特定技术来处理压力数据。&lt;h4&gt;方法&lt;/h4&gt;提出了一个双向的文本×压力（Text$imes$Pressure，简称TxP）模型，该模型使用生成式基础模型将压力数据解释为自然语言，并实现两个任务：将活动文本描述转换为压力序列（Text2Pressure）和从动态压力图中生成活动描述和分类（Pressure2Text）。TxP在包含超过81,100个文本-压力对的合成PressLang数据集上训练，并使用预训练模型如CLIP和LLaMA 2 13B Chat。&lt;h4&gt;主要发现&lt;/h4&gt;TxP在真实世界数据上验证，如瑜伽和日常任务，提供了基于原子动作的数据增强和分类的新方法，将宏观F1分数提高了最多12.4%，与现有技术相比，显著提升了基于压力的HAR性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过使用生成式基础模型和压力特定技术，实现了对压力数据的自然语言解释，从而提高了人体活动识别的性能，为压力传感器的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;基于传感器的活动识别（HAR）主要关注惯性测量单元和视觉数据，往往忽略了压力传感器在捕捉细微的身体动态和重心变化方面的独特能力。尽管压力传感器在姿势和平衡相关活动方面具有潜力，但由于数据集有限，在HAR领域仍然没有得到充分利用。为了弥合这一差距，我们提出利用生成式基础模型和压力特定的HAR技术。具体来说，我们提出了一种双向文本×压力（Text×Pressure）模型，该模型使用生成式基础模型将压力数据解释为自然语言。TxP实现了两个任务：（1）文本2压力，将活动文本描述转换为压力序列；（2）压力2文本，从动态压力图中生成活动描述和分类。利用预训练模型如CLIP和LLaMA 2 13B Chat，TxP在包含超过81,100个文本-压力对的合成PressLang数据集上训练。在真实世界数据（如瑜伽和日常任务）上验证后，TxP提供了基于原子动作的数据增强和分类的新方法。这使宏观F1分数与现有技术相比提高了最多12.4%，推动了基于压力的HAR的应用和人类运动的深入理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sensor-based human activity recognition (HAR) has predominantly focused onInertial Measurement Units and vision data, often overlooking the capabilitiesunique to pressure sensors, which capture subtle body dynamics and shifts inthe center of mass. Despite their potential for postural and balance-basedactivities, pressure sensors remain underutilized in the HAR domain due tolimited datasets. To bridge this gap, we propose to exploit generativefoundation models with pressure-specific HAR techniques. Specifically, wepresent a bidirectional Text$\times$Pressure model that uses generativefoundation models to interpret pressure data as natural language. TxPaccomplishes two tasks: (1) Text2Pressure, converting activity textdescriptions into pressure sequences, and (2) Pressure2Text, generatingactivity descriptions and classifications from dynamic pressure maps.Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained onour synthetic PressLang dataset, containing over 81,100 text-pressure pairs.Validated on real-world data for activities such as yoga and daily tasks, TxPprovides novel approaches to data augmentation and classification grounded inatomic actions. This consequently improved HAR performance by up to 12.4\% inmacro F1 score compared to the state-of-the-art, advancing pressure-based HARwith broader applications and deeper insights into human movement.</description>
      <author>example@mail.com (Lala Shakti Swarup Ray, Lars Krupp, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz)</author>
      <guid isPermaLink="false">2505.02052v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
  <item>
      <title>Joint Generalized Cosine Similarity: A Novel Method for N-Modal Semantic Alignment Based on Contrastive Learning</title>
      <link>http://arxiv.org/abs/2505.03532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对多模态深度学习中的对齐任务，提出了一种新型的相似度测量方法，并应用于对比学习中。&lt;h4&gt;背景&lt;/h4&gt;对齐是多模态深度学习中的一个关键任务，而对比学习在该领域被广泛应用。然而，当存在多于两种模态时，现有方法通常计算成对损失函数并聚合它们作为复合损失函数以优化模型参数。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统相似度测量方法的局限性（即它们只能计算两个向量之间的相似度），提出了一个新的相似度测量方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的相似度测量方法：联合广义余弦相似度（JGCS），它围绕从Gram行列式导出的角度进行。基于此，引入了相应的对比学习损失函数GHA Loss以及新的跨模态对比学习范式。&lt;h4&gt;主要发现&lt;/h4&gt;在Derm7pt数据集和模拟数据集上的实验表明，该方法在具有噪声鲁棒性、计算效率和可扩展性等显著优势的同时，实现了优异的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的联合广义余弦相似度不仅适用于对比学习，而且可以轻松扩展到其他领域。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a new similarity measurement method, Joint Generalized Cosine Similarity (JGCS), for alignment tasks in multi-modal deep learning, and applies it in contrastive learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alignment remains a crucial task in multi-modal deep learning, andcontrastive learning has been widely applied in this field. However, when thereare more than two modalities, existing methods typically calculate pairwiseloss function and aggregate them into a composite loss function for theoptimization of model parameters. This limitation mainly stems from thedrawbacks of traditional similarity measurement method (i.e. they can onlycalculate the similarity between two vectors). To address this issue, wepropose a novel similarity measurement method: the Joint Generalized CosineSimilarity (JGCS). Unlike traditional pairwise methods (e.g., dot product orcosine similarity), JGCS centers around the angle derived from the Gramdeterminant. To the best of our knowledge, this is the first similaritymeasurement method capable of handling tasks involving an arbitrary number ofvectors. Based on this, we introduce the corresponding contrastive learningloss function , GHA Loss, and the new inter-modal contrastive learningparadigm. Additionally, comprehensive experiments conducted on the Derm7ptdataset and simulated datasets demonstrate that our method achieves superiorperformance while exhibiting remarkable advantages such as noise robustness,computational efficiency, and scalability. Finally, it is worth mentioning thatthe Joint Generalized Cosine Similarity proposed by us can not only be appliedin contrastive learning, but also be easily extended to other domains.</description>
      <author>example@mail.com (Yiqiao Chen, Zijian Huang)</author>
      <guid isPermaLink="false">2505.03532v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis</title>
      <link>http://arxiv.org/abs/2505.03123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个多模态时空图神经网络框架，用于预测结直肠癌肝转移（CRLM）的进展。&lt;h4&gt;背景&lt;/h4&gt;现有的临床模型未能有效整合肿瘤的空间异质性、动态演变和复杂的多模态数据关系，限制了其预测精度。&lt;h4&gt;目的&lt;/h4&gt;提高CRLM进展的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;将术前CT影像和临床数据结合成异构图结构，通过空间拓扑和跨模态边进行联合建模。使用GraphSAGE聚合时空邻域信息，并利用监督和对比学习策略来增强模型捕捉时间特征和鲁棒性。还提供了一个参数减少78.55%的轻量级模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，模型在MSKCC CRLM数据集上达到了85%的时间邻近准确率，平均绝对误差为1.1005，显著优于现有方法。创新性异构图构建和时空解耦机制有效地揭示了动态肿瘤微环境变化与预后的关联。&lt;h4&gt;结论&lt;/h4&gt;该模型为个性化治疗决策提供了可靠的定量支持。&lt;h4&gt;翻译&lt;/h4&gt;We propose a multimodal spatiotemporal graph neural network (STG) framework to predict colorectal cancer liver metastasis (CRLM) progression. Current clinical models do not effectively integrate the tumor's spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. Our STG framework combines preoperative CT imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the model's ability to capture temporal features and improve robustness. A lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. The model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. The innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a multimodal spatiotemporal graph neural network (STG) frameworkto predict colorectal cancer liver metastasis (CRLM) progression. Currentclinical models do not effectively integrate the tumor's spatial heterogeneity,dynamic evolution, and complex multimodal data relationships, limiting theirpredictive accuracy. Our STG framework combines preoperative CT imaging andclinical data into a heterogeneous graph structure, enabling joint modeling oftumor distribution and temporal evolution through spatial topology andcross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporalneighborhood information and leverages supervised and contrastive learningstrategies to enhance the model's ability to capture temporal features andimprove robustness. A lightweight version of the model reduces parameter countby 78.55%, maintaining near-state-of-the-art performance. The model jointlyoptimizes recurrence risk regression and survival analysis tasks, withcontrastive loss improving feature representational discriminability andcross-modal consistency. Experimental results on the MSKCC CRLM dataset show atime-adjacent accuracy of 85% and a mean absolute error of 1.1005,significantly outperforming existing methods. The innovative heterogeneousgraph construction and spatiotemporal decoupling mechanism effectively uncoverthe associations between dynamic tumor microenvironment changes and prognosis,providing reliable quantitative support for personalized treatment decisions.</description>
      <author>example@mail.com (Yiran Zhu, Wei Yang, Yan su, Zesheng Li, Chengchang Pan, Honggang Qi)</author>
      <guid isPermaLink="false">2505.03123v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing</title>
      <link>http://arxiv.org/abs/2505.03621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysLLM是一个协同优化框架，结合了LLMs和rPPG领域的特定组件，以提高rPPG信号的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;rPPG技术虽然可以实现非接触生理测量，但易受光照变化、运动伪影和时间建模限制。&lt;h4&gt;目的&lt;/h4&gt;提出PhysLLM框架，以解决LLMs在处理rPPG信号时的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 采用Text Prototype Guidance (TPG)策略，通过将生理特征投影到LLM可理解的语义空间，实现跨模态对齐。2. 提出Dual-Domain Stationary (DDS)算法，通过自适应时频域特征重加权来解决问题的不稳定性。3. 通过生理统计、环境上下文回答和任务描述，将生理先验系统地注入rPPG任务，利用跨模态学习整合视觉和文本信息。&lt;h4&gt;主要发现&lt;/h4&gt;PhysLLM在四个基准数据集上实现了最先进的准确性和鲁棒性，证明了其在光照变化和运动场景中的优越泛化能力。&lt;h4&gt;结论&lt;/h4&gt;PhysLLM通过结合LLMs和rPPG特定组件，有效提高了rPPG信号的测量精度和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;Remote photoplethysmography (rPPG) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rPPG signals due to their text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative optimization framework that synergizes LLMs with domain-specific rPPG components. Specifically, the Text Prototype Guidance (TPG) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into LLM-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. Finally, rPPG task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. Evaluation on four benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote photoplethysmography (rPPG) enables non-contact physiologicalmeasurement but remains highly susceptible to illumination changes, motionartifacts, and limited temporal modeling. Large Language Models (LLMs) excel atcapturing long-range dependencies, offering a potential solution but strugglewith the continuous, noise-sensitive nature of rPPG signals due to theirtext-centric design. To bridge this gap, we introduce PhysLLM, a collaborativeoptimization framework that synergizes LLMs with domain-specific rPPGcomponents. Specifically, the Text Prototype Guidance (TPG) strategy isproposed to establish cross-modal alignment by projecting hemodynamic featuresinto LLM-interpretable semantic space, effectively bridging therepresentational gap between physiological signals and linguistic tokens.Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed forresolving signal instability through adaptive time-frequency domain featurere-weighting. Finally, rPPG task-specific cues systematically injectphysiological priors through physiological statistics, environmental contextualanswering, and task description, leveraging cross-modal learning to integrateboth visual and textual information, enabling dynamic adaptation to challengingscenarios like variable illumination and subject movements. Evaluation on fourbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,demonstrating superior generalization across lighting variations and motionscenarios.</description>
      <author>example@mail.com (Yiping Xie, Bo Zhao, Mingtong Dai, Jian-Ping Zhou, Yue Sun, Tao Tan, Weicheng Xie, Linlin Shen, Zitong Yu)</author>
      <guid isPermaLink="false">2505.03621v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Behavior Intelligence: Technology, Challenges, and Future Directions</title>
      <link>http://arxiv.org/abs/2505.03315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures, Pre-print for IWIS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了人工行为智能（ABI）的技术框架，全面分析了人类姿态、面部表情、情绪、行为序列和上下文线索，并探讨了在自动驾驶、智能医疗、监控系统和社会机器人等领域应用ABI的潜力。&lt;h4&gt;背景&lt;/h4&gt;理解和预测人类行为已成为人工智能应用领域的关键能力。&lt;h4&gt;目的&lt;/h4&gt;定义人工行为智能（ABI）的技术框架，并分析其组成部分，同时探讨如何通过大规模预训练模型提高行为识别的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;本文详细阐述了ABI的必要组成部分，包括姿态估计、面部和情绪识别、序列行为分析和上下文感知建模。此外，还介绍了近期在大型语言模型（LLMs）、视觉基础模型和多模态集成模型方面的进展，以及如何通过优化策略解决ABI在实际应用中的技术挑战。&lt;h4&gt;主要发现&lt;/h4&gt;本文指出，为了将ABI应用于实际场景，需要解决从有限数据中学习行为智能、量化复杂行为预测中的不确定性以及优化模型结构以适应低功耗、实时推理等问题。&lt;h4&gt;结论&lt;/h4&gt;研究团队对ABI领域有浓厚兴趣，并正在积极研究，特别是开发能够高效推断复杂人类行为的轻量级智能模型。同时，探索了包括轻量级Transformer、基于图的识别架构、能量感知损失函数和多模态知识蒸馏在内的多种优化策略，并在实时环境中验证其适用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：理解和预测人类行为已成为人工智能应用领域的关键能力。本文定义了人工行为智能（ABI）的技术框架，全面分析了人类姿态、面部表情、情绪、行为序列和上下文线索。它详细介绍了ABI的基本组成部分，包括姿态估计、面部和情绪识别、序列行为分析和上下文感知建模。此外，我们强调了近期在大型语言模型（LLMs）、视觉基础模型和多模态集成模型方面的进展，这些进展在显著提高行为识别的准确性和可解释性方面具有变革性的潜力。我们的研究团队对ABI领域有浓厚兴趣，并正在积极进行研究，特别是专注于开发能够高效推断复杂人类行为的智能轻量级模型。本文确定了在现实应用中部署ABI必须解决的一些技术挑战，包括从有限数据中学习行为智能、量化复杂行为预测中的不确定性以及优化模型结构以适应低功耗、实时推理。为了应对这些挑战，我们的团队正在探索各种优化策略，包括轻量级Transformer、基于图的识别架构、能量感知损失函数和多模态知识蒸馏，同时验证它们在实时环境中的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and predicting human behavior has emerged as a core capabilityin various AI application domains such as autonomous driving, smart healthcare,surveillance systems, and social robotics. This paper defines the technicalframework of Artificial Behavior Intelligence (ABI), which comprehensivelyanalyzes and interprets human posture, facial expressions, emotions, behavioralsequences, and contextual cues. It details the essential components of ABI,including pose estimation, face and emotion recognition, sequential behavioranalysis, and context-aware modeling. Furthermore, we highlight thetransformative potential of recent advances in large-scale pretrained models,such as large language models (LLMs), vision foundation models, and multimodalintegration models, in significantly improving the accuracy andinterpretability of behavior recognition. Our research team has a stronginterest in the ABI domain and is actively conducting research, particularlyfocusing on the development of intelligent lightweight models capable ofefficiently inferring complex human behaviors. This paper identifies severaltechnical challenges that must be addressed to deploy ABI in real-worldapplications including learning behavioral intelligence from limited data,quantifying uncertainty in complex behavior prediction, and optimizing modelstructures for low-power, real-time inference. To tackle these challenges, ourteam is exploring various optimization strategies including lightweighttransformers, graph-based recognition architectures, energy-aware lossfunctions, and multimodal knowledge distillation, while validating theirapplicability in real-time environments.</description>
      <author>example@mail.com (Kanghyun Jo, Jehwan Choi, Kwanho Kim, Seongmin Kim, Duy-Linh Nguyen, Xuan-Thuy Vo, Adri Priadana, Tien-Dat Tran)</author>
      <guid isPermaLink="false">2505.03315v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense</title>
      <link>http://arxiv.org/abs/2505.03424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为GNN-AID的开源框架，旨在解决机器学习模型中可解释性和鲁棒性的问题，特别针对图数据。&lt;h4&gt;背景&lt;/h4&gt;随着对可信人工智能（TAI）需求的增长，机器学习模型的可解释性和鲁棒性变得尤为重要。然而，许多现有工具忽视了图数据，且很少将这两个方面结合在一起。&lt;h4&gt;目的&lt;/h4&gt;提出GNN-AID框架，用于分析、解释和防御图神经网络（GNNs），以解决上述问题。&lt;h4&gt;方法&lt;/h4&gt;GNN-AID是一个基于Python库的框架，支持高级信任方法和架构层，允许用户通过攻击、防御和可解释性方法来分析图数据集和GNN的行为。它建立在PyTorch-Geometric之上，提供预加载的数据集、模型和自定义接口，以支持任何GNNs。还包括一个Web界面，具有图形可视化和无需编码的功能，如交互式模型构建器，简化了GNNs的探索和分析。此外，GNN-AID还支持MLOps技术，确保可重复性和结果版本化。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-AID是一个灵活的工具，可以帮助开发者创建、分析和定制图模型，同时提供预构建的数据集和模型进行快速实验。研究人员可以使用该框架探索可解释性和鲁棒性之间的关系，测试防御策略，并组合方法以保护免受不同类型攻击。此外，还发现了针对规避和中毒攻击的防御策略在图数据上可能产生冲突，突出了防御策略之间的复杂联系。&lt;h4&gt;结论&lt;/h4&gt;GNN-AID是一个强大的工具，旨在帮助开发者和研究人员在机器学习模型的可解释性和鲁棒性方面取得进展，特别针对图数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing need for Trusted AI (TAI) highlights the importance ofinterpretability and robustness in machine learning models. However, manyexisting tools overlook graph data and rarely combine these two aspects into asingle solution. Graph Neural Networks (GNNs) have become a popular approach,achieving top results across various tasks. We introduce GNN-AID (Graph NeuralNetwork Analysis, Interpretation, and Defense), an open-source frameworkdesigned for graph data to address this gap. Built as a Python library, GNN-AIDsupports advanced trust methods and architectural layers, allowing users toanalyze graph datasets and GNN behavior using attacks, defenses, andinterpretability methods.  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,and support for any GNNs through customizable interfaces. It also includes aweb interface with tools for graph visualization and no-code features like aninteractive model builder, simplifying the exploration and analysis of GNNs.The framework also supports MLOps techniques, ensuring reproducibility andresult versioning to track and revisit analyses efficiently.  GNN-AID is a flexible tool for developers and researchers. It helpsdevelopers create, analyze, and customize graph models, while also providingaccess to prebuilt datasets and models for quick experimentation. Researcherscan use the framework to explore advanced topics on the relationship betweeninterpretability and robustness, test defense strategies, and combine methodsto protect against different types of attacks.  We also show how defenses against evasion and poisoning attacks can conflictwhen applied to graph data, highlighting the complex connections betweendefense strategies.  GNN-AID is available at\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}</description>
      <author>example@mail.com (Kirill Lukyanov, Mikhail Drobyshevskiy, Georgii Sazonov, Mikhail Soloviov, Ilya Makarov)</author>
      <guid isPermaLink="false">2505.03424v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Optimization and Program Search using Language Models for Task and Motion Planning</title>
      <link>http://arxiv.org/abs/2505.03725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 8 figures, under review for the 9th Annual Conference on  Robot Learning (CoRL 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的TAMP方法，通过元优化技术解决高级规划和低级控制之间的接口问题。&lt;h4&gt;背景&lt;/h4&gt;智能交互需要机器人代理联合推理高级计划和低级控制。TAMP通过结合符号规划和连续轨迹生成来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;寻找高级规划和低级运动生成之间的最佳接口。&lt;h4&gt;方法&lt;/h4&gt;提出的方法通过：(i) 使用程序搜索作为基础模型和机器人控制之间的接口，来优化轨迹优化问题；(ii) 利用零阶方法优化基础模型输出的数值参数。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的物体操作和绘图任务上的结果证实，该方法优于之前的TAMP方法。&lt;h4&gt;结论&lt;/h4&gt;该方法通过元优化技术提高了TAMP方法的效果，为智能交互提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：与真实世界的智能交互需要机器人代理联合推理高级计划和低级控制。任务和运动规划（TAMP）通过结合符号规划和连续轨迹生成来解决这个问题。最近，TAMP的基础模型方法取得了令人印象深刻的成果，包括快速规划和执行自然语言指令。然而，高级规划和低级运动生成之间的最佳接口仍然是一个未解决的问题：先前的方法要么过于抽象（例如，链式简化技能原语），要么缺乏抽象（例如，直接预测关节角度）。我们的方法通过以下方式引入了一种新颖的技术来解决这些问题：(i) 使用程序搜索作为基础模型和机器人控制之间的接口，用于优化轨迹优化问题；(ii) 利用零阶方法优化基础模型输出的数值参数。在具有挑战性的物体操作和绘图任务上的结果证实，我们提出的方法优于先前的TAMP方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligent interaction with the real world requires robotic agents tojointly reason over high-level plans and low-level controls. Task and motionplanning (TAMP) addresses this by combining symbolic planning and continuoustrajectory generation. Recently, foundation model approaches to TAMP havepresented impressive results, including fast planning times and the executionof natural language instructions. Yet, the optimal interface between high-levelplanning and low-level motion generation remains an open question: priorapproaches are limited by either too much abstraction (e.g., chainingsimplified skill primitives) or a lack thereof (e.g., direct joint angleprediction). Our method introduces a novel technique employing a form ofmeta-optimization to address these issues by: (i) using program search overtrajectory optimization problems as an interface between a foundation model androbot control, and (ii) leveraging a zero-order method to optimize numericalparameters in the foundation model output. Results on challenging objectmanipulation and drawing tasks confirm that our proposed method improves overprior TAMP approaches.</description>
      <author>example@mail.com (Denis Shcherba, Eckart Cobo-Briesewitz, Cornelius V. Braun, Marc Toussaint)</author>
      <guid isPermaLink="false">2505.03725v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning</title>
      <link>http://arxiv.org/abs/2505.03703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了新的衡量方法和有效技术来解决视觉-语言模型中的模态差距问题，并通过实验验证了其在下游任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型（VLMs）可以将文本和图像嵌入到共享表示空间中，但存在模态差距现象，即不同模态的嵌入在表示空间中存在明显分离。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种通用的、实用的方法来精确评估和减少模态差距。&lt;h4&gt;方法&lt;/h4&gt;提出了基于光谱和最优传输方法的新的衡量措施和有效技术。&lt;h4&gt;主要发现&lt;/h4&gt;在多个图像-文本数据集和模型上进行的实验表明，这些方法对下游任务（如多模态检索、多模态聚类或零样本分类等）具有积极的影响。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法和技术在减少模态差距和提高下游任务性能方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language models (VLMs) allow to embed texts and images in a shared representation space. However, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. While this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc., no generic and practical methods have so far been proposed to assess it precisely and even reduce it. We therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. Extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. Our code is available at the URL provided in the paper's abstract.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) allow to embed texts and images in a sharedrepresentation space. However, it has been shown that these models are subjectto a modality gap phenomenon meaning there exists a clear separation betweenthe embeddings from one modality and another in the embedding space. While thismisalignment is detrimental for downstream tasks such as multimodal retrieval,multimodal clustering or zero-shot classification, etc. no generic andpractical methods have so far been proposed to assess it precisely and evenreduce it. We therefore propose novel measures and effective techniques(spectral- and optimal transport-based methods) to achieve this goal. Extensiveexperiments conducted on several image-text datasets and models demonstratetheir effectiveness and beneficial effects on downstream tasks. Our code isavailable at the URL provided in the paper's abstract.</description>
      <author>example@mail.com (François Role, Sébastien Meyer, Victor Amblard)</author>
      <guid isPermaLink="false">2505.03703v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.03721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于太阳能传感器的可持续智能农场网络，通过深度强化学习和决策理论优化监控效果和能源效率，以应对网络攻击和能量供应波动等挑战。&lt;h4&gt;背景&lt;/h4&gt;太阳能传感器监测系统在农业领域的应用日益广泛，但系统对网络攻击的抵御能力和适应动态能量供应的能力尚不明确。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些挑战，论文旨在设计一种可持续的智能农场网络，以在各种网络攻击和能量供应波动条件下保持高质量的动物监测。&lt;h4&gt;方法&lt;/h4&gt;该研究采用深度强化学习（DRL）来制定最佳策略，同时利用迁移学习（TL）和决策理论（DT）加速学习过程，并通过决策理论指导的策略优化监控质量和能源可持续性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，基于决策理论的DRL模型在性能上优于仅使用迁移学习的DRL模型，系统性能得到提升，训练时间缩短了47.5%。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效提高太阳能传感器监测系统的性能和能源效率，为农业领域的智能监测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于太阳能传感器的监测系统已成为农业创新的关键，通过集成传感器技术、物联网、边缘和云计算，推动了农场管理和动物福利的进步。然而，这些系统对网络攻击的抵抗力及其对动态和受限能源供应的适应性仍鲜有研究。为了解决这些挑战，我们提出了一种可持续的智能农场网络，旨在在各种网络和对抗威胁以及波动的能源条件下保持高质量的动物监测。我们的方法利用深度强化学习（DRL）来制定最佳策略，以最大化监控效果和能源效率。为了克服DRL固有的收敛速度慢的挑战，我们整合了迁移学习（TL）和决策理论（DT）来加速学习过程。通过整合决策理论指导的策略，我们优化了监控质量和能源可持续性，显著减少了训练时间，同时实现了可比较的性能回报。我们的实验结果表明，决策理论指导的DRL优于迁移学习增强的DRL模型，提高了系统性能，并将训练时间缩短了47.5%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solar sensor-based monitoring systems have become a crucial agriculturalinnovation, advancing farm management and animal welfare through integratingsensor technology, Internet-of-Things, and edge and cloud computing. However,the resilience of these systems to cyber-attacks and their adaptability todynamic and constrained energy supplies remain largely unexplored. To addressthese challenges, we propose a sustainable smart farm network designed tomaintain high-quality animal monitoring under various cyber and adversarialthreats, as well as fluctuating energy conditions. Our approach utilizes deepreinforcement learning (DRL) to devise optimal policies that maximize bothmonitoring effectiveness and energy efficiency. To overcome DRL's inherentchallenge of slow convergence, we integrate transfer learning (TL) and decisiontheory (DT) to accelerate the learning process. By incorporating DT-guidedstrategies, we optimize monitoring quality and energy sustainability,significantly reducing training time while achieving comparable performancerewards. Our experimental results prove that DT-guided DRL outperformsTL-enhanced DRL models, improving system performance and reducing trainingruntime by 47.5%.</description>
      <author>example@mail.com (Dian Chen, Zelin Wan, Dong Sam Ha, Jin-Hee Cho)</author>
      <guid isPermaLink="false">2505.03721v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>GNN-enabled Precoding for Massive MIMO LEO Satellite Communications</title>
      <link>http://arxiv.org/abs/2505.03311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了针对LEO卫星通信中大规模MIMO预编码挑战的解决方案。&lt;h4&gt;背景&lt;/h4&gt;低地球轨道卫星通信是6G网络发展的关键部分，而大规模MIMO技术的集成正在积极探索以增强其性能。&lt;h4&gt;目的&lt;/h4&gt;提出的方法旨在解决LEO卫星通信在受限功率条件下提高通信能效的挑战。&lt;h4&gt;方法&lt;/h4&gt;1. 引入端到端图神经网络（GNN）框架以降低传统预编码方法的计算复杂度；2. 提出Dinkelbach算法的深度展开和加权最小均方误差（WMMSE）方法以实现增强的能效；3. 将泰勒展开法应用于GNN中矩阵求逆，以增强方法的可解释性和性能。&lt;h4&gt;主要发现&lt;/h4&gt;数值实验证明了所提出方法在复杂性和鲁棒性方面的有效性，并显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在提高LEO卫星通信能效方面具有显著潜力，有助于实现更高效和可持续的通信。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low Earth Orbit (LEO) satellite communication is a critical component in thedevelopment of sixth generation (6G) networks. The integration of massivemultiple-input multiple-output (MIMO) technology is being actively explored toenhance the performance of LEO satellite communications. However, the limitedpower of LEO satellites poses a significant challenge in improvingcommunication energy efficiency (EE) under constrained power conditions.Artificial intelligence (AI) methods are increasingly recognized as promisingsolutions for optimizing energy consumption while enhancing system performance,thus enabling more efficient and sustainable communications. This paperproposes approaches to address the challenges associated with precoding inmassive MIMO LEO satellite communications. First, we introduce an end-to-endgraph neural network (GNN) framework that effectively reduces the computationalcomplexity of traditional precoding methods. Next, we introduce a deepunfolding of the Dinkelbach algorithm and the weighted minimum mean squareerror (WMMSE) approach to achieve enhanced EE, transforming iterativeoptimization processes into a structured neural network, thereby improvingconvergence speed and computational efficiency. Furthermore, we incorporate theTaylor expansion method to approximate matrix inversion within the GNN,enhancing both the interpretability and performance of the proposed method.Numerical experiments demonstrate the validity of our proposed method in termsof complexity and robustness, achieving significant improvements overstate-of-the-art methods.</description>
      <author>example@mail.com (Huibin Zhou, Xinrui Gong, Christos G. Tsinos, Li You, Xiqi Gao, Björn Ottersten)</author>
      <guid isPermaLink="false">2505.03311v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality</title>
      <link>http://arxiv.org/abs/2505.03440v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, submitted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为manvr3d的VR平台，用于交互式细胞追踪，结合VR控制器和眼动追踪设备，以加速深度学习细胞追踪模型的校对和验证。&lt;h4&gt;背景&lt;/h4&gt;生物学家通过分析高时空分辨率的3D时间推移显微镜图像来重建生物体在细胞水平上的发育历史。传统的细胞谱系树重建涉及手动标注细胞位置，并随时间链接它们以创建完整轨迹。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合深度学习细胞追踪软件和3D/VR可视化的系统，以提升细胞追踪的效率和科学家的工作流程。&lt;h4&gt;方法&lt;/h4&gt;利用VR控制器和眼动追踪硬件，将深度学习模型的标注、训练和校对流程提升到三维空间，并应用自然用户界面如手势和眼动追踪。&lt;h4&gt;主要发现&lt;/h4&gt;该系统通过将交互式细胞追踪提升到三维空间，提高了细胞追踪的工作效率和科学家对数据的理解。&lt;h4&gt;结论&lt;/h4&gt;manvr3d平台能够有效加速深度学习细胞追踪模型的训练和验证过程，提高生命科学家的研究效率。&lt;h4&gt;翻译&lt;/h4&gt;We propose manvr3d, a novel VR-ready platform for interactive human-in-the-loop cell tracking. We utilize VR controllers and eye-tracking hardware to facilitate rapid ground truth generation and proofreading for deep learning-based cell tracking models. Life scientists reconstruct the developmental history of organisms on the cellular level by analyzing 3D time-lapse microscopy images acquired at high spatio-temporal resolution. The reconstruction of such cell lineage trees traditionally involves tracking individual cells through all recorded time points, manually annotating their positions, and then linking them over time to create complete trajectories. Deep learning-based algorithms accelerate this process, yet depend heavily on manually-annotated high-quality ground truth data and curation. Visual representation of the image data in this process still relies primarily on 2D renderings, which greatly limits spatial understanding and navigation. In this work, we bridge the gap between deep learning-based cell tracking software and 3D/VR visualization to create a human-in-the-loop cell tracking system. We lift the incremental annotation, training and proofreading loop of the deep learning model into the 3rd dimension and apply natural user interfaces like hand gestures and eye tracking to accelerate the cell tracking workflow for life scientists.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose manvr3d, a novel VR-ready platform for interactivehuman-in-the-loop cell tracking. We utilize VR controllers and eye-trackinghardware to facilitate rapid ground truth generation and proofreading for deeplearning-based cell tracking models. Life scientists reconstruct thedevelopmental history of organisms on the cellular level by analyzing 3Dtime-lapse microscopy images acquired at high spatio-temporal resolution. Thereconstruction of such cell lineage trees traditionally involves trackingindividual cells through all recorded time points, manually annotating theirpositions, and then linking them over time to create complete trajectories.Deep learning-based algorithms accelerate this process, yet depend heavily onmanually-annotated high-quality ground truth data and curation. Visualrepresentation of the image data in this process still relies primarily on 2Drenderings, which greatly limits spatial understanding and navigation. In thiswork, we bridge the gap between deep learning-based cell tracking software and3D/VR visualization to create a human-in-the-loop cell tracking system. We liftthe incremental annotation, training and proofreading loop of the deep learningmodel into the 3rd dimension and apply natural user interfaces like handgestures and eye tracking to accelerate the cell tracking workflow for lifescientists.</description>
      <author>example@mail.com (Samuel Pantze, Jean-Yves Tinevez, Matthew McGinity, Ulrik Günther)</author>
      <guid isPermaLink="false">2505.03440v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph</title>
      <link>http://arxiv.org/abs/2505.03173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RAVU是一个用于增强视频理解的新框架，通过在时空图上的组合推理来提升对长视频的理解能力。&lt;h4&gt;背景&lt;/h4&gt;目前的大多模态模型在处理几分钟到几小时的视频时存在困难，因为它们缺乏明确的记忆和检索机制。&lt;h4&gt;目的&lt;/h4&gt;为了解决LMMs在处理长视频时的局限性，提出了RAVU框架。&lt;h4&gt;方法&lt;/h4&gt;RAVU构建了视频的图表示，捕捉实体之间的空间和时间关系，作为长期记忆来追踪对象及其随时间的变化。为了回答复杂查询，将查询分解为一系列推理步骤，并在图上执行这些步骤，检索相关关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;RAVU方法在两个主要的视频问答数据集NExT-QA和EgoSchema上，与SOTA方法和基线相比，在有限的检索帧（5-10帧）内表现出优越的性能。&lt;h4&gt;结论&lt;/h4&gt;RAVU在理解长视频，特别是需要多跳推理和跨帧追踪对象查询方面，能够实现更准确的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehending long videos remains a significant challenge for LargeMulti-modal Models (LMMs). Current LMMs struggle to process even minutes tohours videos due to their lack of explicit memory and retrieval mechanisms. Toaddress this limitation, we propose RAVU (Retrieval Augmented VideoUnderstanding), a novel framework for video understanding enhanced by retrievalwith compositional reasoning over a spatio-temporal graph. We construct a graphrepresentation of the video, capturing both spatial and temporal relationshipsbetween entities. This graph serves as a long-term memory, allowing us to trackobjects and their actions across time. To answer complex queries, we decomposethe queries into a sequence of reasoning steps and execute these steps on thegraph, retrieving relevant key information. Our approach enables more accurateunderstanding of long videos, particularly for queries that require multi-hopreasoning and tracking objects across frames. Our approach demonstrate superiorperformances with limited retrieved frames (5-10) compared with other SOTAmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.</description>
      <author>example@mail.com (Sameer Malik, Moyuru Yamada, Ayush Singh, Dishank Aggarwal)</author>
      <guid isPermaLink="false">2505.03173v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>HCOA*: Hierarchical Class-ordered A* for Navigation in Semantic Environments</title>
      <link>http://arxiv.org/abs/2505.03128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对混合几何和语义3D环境中的机器人导航问题进行研究，提出了一种高效的路径规划算法HCOA*，通过利用环境层次结构，显著降低了计算成本。&lt;h4&gt;背景&lt;/h4&gt;在混合几何和语义3D环境中，机器人导航需要考虑环境的几何和语义信息，这对计算资源提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;在保持计算效率的同时，实现从起点到目标点的导航。&lt;h4&gt;方法&lt;/h4&gt;提出了HCOA*算法，该算法利用环境层次结构进行路径规划，并证明了其理论性能保障。同时，引入了两种高层节点分类方法：基于图神经网络的分类方法和基于多数类分类方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验，HCOA*算法在uHumans2 3DSG数据集上能够找到最优路径，同时减少了25%的扩展节点数量，并降低了16%的计算时间。&lt;h4&gt;结论&lt;/h4&gt;HCOA*算法在混合几何和语义3D环境中表现出色，有效提高了导航效率。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了在混合几何和语义3D环境中的机器人导航问题。在给定环境层次结构的情况下，旨在通过最小化计算成本，从起点导航到目标点。我们引入了分层类有序A*（HCOA*）算法，该算法利用环境层次结构进行语义图中的高效路径规划，显著减少了计算工作量。我们使用语义类的一个全序，并证明了该算法的理论性能保证。我们提出了两种基于最低层节点语义的高层节点分类方法：基于图神经网络的分类方法和基于多数类的分类方法。我们通过在3D场景图（3DSG）上的模拟评估了我们的方法，将其与最先进的方法进行了比较，并评估了其分类方法的性能。结果表明，HCOA*能够在3DSG数据集上找到最优路径，同时减少了25%的扩展节点数量，并在uHumans2 3DSG数据集上实现了16%的计算时间降低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the problem of robot navigation in mixed geometric andsemantic 3D environments. Given a hierarchical representation of theenvironment, the objective is to navigate from a start position to a goal whileminimizing the computational cost. We introduce Hierarchical Class-ordered A*(HCOA*), an algorithm that leverages the environmental hierarchy for efficientpath-planning in semantic graphs, significantly reducing computational effort.We use a total order over the semantic classes and prove theoreticalperformance guarantees for the algorithm. We propose two approaches forhigher-layer node classification based on the node semantics of the lowestlayer: a Graph Neural Network-based method and a Majority-Class method. Weevaluate our approach through simulations on a 3D Scene Graph (3DSG), comparingit to the state-of-the-art and assessing its performance against ourclassification approaches. Results show that HCOA* can find the optimal pathwhile reducing the number of expanded nodes by 25% and achieving a 16%reduction in computational time on the uHumans2 3DSG dataset.</description>
      <author>example@mail.com (Evangelos Psomiadis, Panagiotis Tsiotras)</author>
      <guid isPermaLink="false">2505.03128v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>BLAB: Brutally Long Audio Bench</title>
      <link>http://arxiv.org/abs/2505.03054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为BLAB的音频语言模型评估基准，用于评估音频模型在处理长音频段时的性能。&lt;h4&gt;背景&lt;/h4&gt;为了适应人类通信的多模态特性，开发能够理解多样化口语交互的大型音频语言模型（LMs）至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探索音频语言模型在处理长音频对话片段时的表现，并评估其在定位、持续时间估计、情感和计数任务上的能力。&lt;h4&gt;方法&lt;/h4&gt;BLAB包含833小时以上的不同音频片段，平均长度为51分钟，并配有人工标注的文本问答。研究人员评估了六种开源和专有音频LMs在BLAB上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;所有测试的音频LMs在BLAB上都面临挑战，包括高级模型如Gemini 2.0 Pro和GPT-4o。研究发现，音频LMs在处理长音频时表现不佳，性能随音频时长增加而下降，在定位、时间推理、计数任务上表现不佳，且难以理解非音素信息，更依赖于提示而非音频内容。&lt;h4&gt;结论&lt;/h4&gt;BLAB为开发具有稳健长音频理解能力的音频LMs提供了一个具有挑战性的评估框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要：开发能够理解多样化口语交互的大型音频语言模型（LMs）对于适应人类通信的多模态特性至关重要，并且可以增加语言技术在不同用户群体中的可及性。最近关于音频LMs的研究主要评估了它们在短音频片段上的性能，通常长度不超过30秒，对更接近自然用户交互的长音频对话片段的探索有限。我们引入了Brutally Long Audio Bench（BLAB），这是一个具有挑战性的长音频基准，用于评估音频LMs在定位、持续时间估计、情感和计数任务上的表现，使用的音频片段平均长度为51分钟。BLAB由超过833小时的多样化完整音频剪辑组成，每个剪辑都与基于文本的自然语言问答配对。我们的音频数据来自许可来源，并经过人工辅助过滤过程以确保任务符合性。我们在BLAB上评估了六种开源和专有音频LMs，并发现所有这些模型，包括如Gemini 2.0 Pro和GPT-4o等高级模型，在BLAB任务上都面临挑战。我们的综合分析揭示了任务难度与音频时长之间的权衡。总的来说，我们发现音频LMs在处理长音频时表现不佳，随着音频时长的增加，性能下降。它们在定位、时间推理、计数任务上表现不佳，难以理解非音素信息，更多地依赖于提示而非音频内容。BLAB作为一个具有挑战性的评估框架，为开发具有稳健长音频理解能力的音频LMs提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing large audio language models (LMs) capable of understanding diversespoken interactions is essential for accommodating the multimodal nature ofhuman communication and can increase the accessibility of language technologiesacross different user populations. Recent work on audio LMs has primarilyevaluated their performance on short audio segments, typically under 30seconds, with limited exploration of long-form conversational speech segmentsthat more closely reflect natural user interactions with these models. Weintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audiobenchmark that evaluates audio LMs on localization, duration estimation,emotion, and counting tasks using audio segments averaging 51 minutes inlength. BLAB consists of 833+ hours of diverse, full-length audio clips, eachpaired with human-annotated, text-based natural language questions and answers.Our audio data were collected from permissively licensed sources and underwenta human-assisted filtering process to ensure task compliance. We evaluate sixopen-source and proprietary audio LMs on BLAB and find that all of them,including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with thetasks in BLAB. Our comprehensive analysis reveals key insights into thetrade-offs between task difficulty and audio duration. In general, we find thataudio LMs struggle with long-form speech, with performance declining asduration increases. They perform poorly on localization, temporal reasoning,counting, and struggle to understand non-phonemic information, relying more onprompts than audio content. BLAB serves as a challenging evaluation frameworkto develop audio LMs with robust long-form audio understanding capabilities.</description>
      <author>example@mail.com (Orevaoghene Ahia, Martijn Bartelds, Kabir Ahuja, Hila Gonen, Valentin Hofmann, Siddhant Arora, Shuyue Stella Li, Vishal Puttagunta, Mofetoluwa Adeyemi, Charishma Buchireddy, Ben Walls, Noah Bennett, Shinji Watanabe, Noah A. Smith, Yulia Tsvetkov, Sachin Kumar)</author>
      <guid isPermaLink="false">2505.03054v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2505.03692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多视角点云配准中的位姿图构建和运动同步问题。&lt;h4&gt;背景&lt;/h4&gt;多视角点云配准在机器人、自动化和计算机视觉领域具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的方法来构建可靠的位姿图并进行运动同步。&lt;h4&gt;方法&lt;/h4&gt;设计了一个网络模型，通过提取点云对之间的匹配距离信息来识别可靠的配对；提出另一个神经网络模型，以数据驱动的方式计算绝对位姿，避免优化手工设计的损失函数；模型考虑了几何分布信息，并使用改进的注意力机制以促进灵活可靠的特征交互。&lt;h4&gt;主要发现&lt;/h4&gt;在室内和室外数据集上的实验结果表明，该方法有效且具有泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在多视角点云配准中具有良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. This paper concentrates on pose graph construction and motion synchronization within multiview registration. Previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. To identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. For motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. Our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. Experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2024.3455783&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiview point cloud registration plays a crucial role in robotics,automation, and computer vision fields. This paper concentrates on pose graphconstruction and motion synchronization within multiview registration. Previousmethods for pose graph construction often pruned fully connected graphs orconstructed sparse graph using global feature aggregated from localdescriptors, which may not consistently yield reliable results. To identifydependable pairs for pose graph construction, we design a network model thatextracts information from the matching distance between point cloud pairs. Formotion synchronization, we propose another neural network model to calculatethe absolute pose in a data-driven manner, rather than optimizing inaccuratehandcrafted loss functions. Our model takes into account geometric distributioninformation and employs a modified attention mechanism to facilitate flexibleand reliable feature interaction. Experimental results on diverse indoor andoutdoor datasets confirm the effectiveness and generalizability of ourapproach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.</description>
      <author>example@mail.com (Shiqi Li, Jihua Zhu, Yifan Xie, Naiwen Hu, Di Wang)</author>
      <guid isPermaLink="false">2505.03692v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation</title>
      <link>http://arxiv.org/abs/2505.03114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于路径和骨轮廓正则化的无监督MRI到CT转换方法，用于提高骨结构在转换过程中的准确性。&lt;h4&gt;背景&lt;/h4&gt;MRI和CT扫描的结合对于医学影像学具有重要意义，但获取配对的MRI和CT扫描存在实际挑战，因此需要开发能够利用无配对数据集的稳健方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确转换骨结构的方法，适用于需要精确骨表示的放射治疗等应用。&lt;h4&gt;方法&lt;/h4&gt;提出的方法将MRI和CT图像投影到共享的潜在空间，通过神经常微分方程建模MRI到CT的映射，并最小化流的转换路径长度。此外，引入一个可训练的神经网络生成骨轮廓，并实施机制以直接和间接鼓励模型关注骨轮廓及其邻近区域。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的评估表明，该方法优于现有的无监督MRI到CT转换方法，实现了更低的总体误差率。在下游骨分割任务中，该方法在保持骨结构保真度方面表现出优异的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在MRI到CT的无监督转换中表现出良好的性能，尤其是在骨结构的准确转换方面。&lt;h4&gt;翻译&lt;/h4&gt;我们的代码可在https://github.com/kennysyp/PaBoT找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate MRI-to-CT translation promises the integration of complementaryimaging information without the need for additional imaging sessions. Given thepractical challenges associated with acquiring paired MRI and CT scans, thedevelopment of robust methods capable of leveraging unpaired datasets isessential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CTtranslation methods, which predominantly rely on cycle consistency andcontrastive learning frameworks, frequently encounter challenges in accuratelytranslating anatomical features that are highly discernible on CT but lessdistinguishable on MRI, such as bone structures. This limitation renders theseapproaches less suitable for applications in radiation therapy, where precisebone representation is essential for accurate treatment planning. To addressthis challenge, we propose a path- and bone-contour regularized approach forunpaired MRI-to-CT translation. In our method, MRI and CT images are projectedto a shared latent space, where the MRI-to-CT mapping is modeled as acontinuous flow governed by neural ordinary differential equations. The optimalmapping is obtained by minimizing the transition path length of the flow. Toenhance the accuracy of translated bone structures, we introduce a trainableneural network to generate bone contours from MRI and implement mechanisms todirectly and indirectly encourage the model to focus on bone contours and theiradjacent regions. Evaluations conducted on three datasets demonstrate that ourmethod outperforms existing unpaired MRI-to-CT translation approaches,achieving lower overall error rates. Moreover, in a downstream bonesegmentation task, our approach exhibits superior performance in preserving thefidelity of bone structures. Our code is available at:https://github.com/kennysyp/PaBoT.</description>
      <author>example@mail.com (Teng Zhou, Jax Luo, Yuping Sun, Yiheng Tan, Shun Yao, Nazim Haouchine, Scott Raymond)</author>
      <guid isPermaLink="false">2505.03114v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Gene finding revisited: improved robustness through structured decoding from learned embeddings</title>
      <link>http://arxiv.org/abs/2505.03377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的基因定位方法，通过结合原始遗传序列的学习嵌入和潜条件随机场的精确解码，提高了基因发现的效果和训练鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;基因发现是识别基因组中编码序列位置的任务，随着原始基因组序列数量的不断增加，基因发现成为了解释（新）生物遗传信息以及学习不同物种间共有的进化模式的重要途径。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出一种能够利用深度学习技术在基因发现中取得更好性能的方法。&lt;h4&gt;方法&lt;/h4&gt;本文提出的方法结合了原始遗传序列的学习嵌入和潜条件随机场的精确解码，以提高基因发现的性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在性能上达到了现有最佳水平，同时增加了训练的鲁棒性，并消除了手动拟合长度分布的需求。&lt;h4&gt;结论&lt;/h4&gt;随着DNA语言模型的改进，这种方法为更有效的跨物种基因发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基因定位是识别基因组中编码序列位置的任务。随着原始基因组序列数量的不断增长，基因定位成为了理解（新型）生物遗传信息以及学习进化多样物种间共有模式的 重要途径。当前最先进的方法通常是针对每个生物训练的图形模型，并且需要手动管理的数据集。然而，这些模型缺乏灵活性，无法融入近年来在蛋白质序列分析中具有革命性的深度学习表示学习方法，而这些方法可能会帮助基因发现者利用越来越多的已测序基因组来提高跨多个生物的性能。在此，我们提出了一种新的方法，结合了原始遗传序列的学习嵌入和潜条件随机场的精确解码。我们表明，该模型在性能上达到了现有最佳水平，同时提高了训练的鲁棒性，并消除了需要手动拟合长度分布的需求。随着DNA语言模型的改进，这为更高效的跨物种基因发现开辟了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gene finding is the task of identifying the locations of coding sequenceswithin the vast amount of genetic code contained in the genome. With an everincreasing quantity of raw genome sequences, gene finding is an importantavenue towards understanding the genetic information of (novel) organisms, aswell as learning shared patterns across evolutionarily diverse species. Thecurrent state of the art are graphical models usually trained per organism andrequiring manually curated datasets. However, these models lack the flexibilityto incorporate deep learning representation learning techniques that have inrecent years been transformative in the analysis of pro tein sequences, andwhich could potentially help gene finders exploit the growing number of thesequenced genomes to expand performance across multiple organisms. Here, wepropose a novel approach, combining learned embeddings of raw genetic sequenceswith exact decoding using a latent conditional random field. We show that themodel achieves performance matching the current state of the art, whileincreasing training robustness, and removing the need for manually fittedlength distributions. As language models for DNA improve, this paves the wayfor more performant cross-organism gene-finders.</description>
      <author>example@mail.com (Frederikke I. Marin, Dennis Pultz, Wouter Boomsma)</author>
      <guid isPermaLink="false">2505.03377v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal cascade feature transfer for polymer property prediction</title>
      <link>http://arxiv.org/abs/2505.03704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为多模态级联模型及特征迁移的聚合物性质预测的新颖迁移学习方法。&lt;h4&gt;背景&lt;/h4&gt;聚合物性质预测依赖于多种数据格式，如分子描述符、添加剂信息及化学结构。&lt;h4&gt;目的&lt;/h4&gt;提高聚合物物理性质的预测精度。&lt;h4&gt;方法&lt;/h4&gt;通过结合由图卷积神经网络（GCN）从化学结构中提取的特征以及分子描述符和添加剂信息等特征。&lt;h4&gt;主要发现&lt;/h4&gt;与仅使用单一特征的基线传统方法相比，所提出的方法在多个聚合物数据集上表现出更高的预测性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在聚合物性质预测方面显示出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种名为多模态级联模型及特征迁移的聚合物性质预测的新颖迁移学习方法。聚合物具有多种不同格式的数据特征，包括分子描述符、添加剂信息以及化学结构。然而，在传统方法中，预测模型通常使用每种类型的数据分别构建。我们的模型通过结合由图卷积神经网络（GCN）从化学结构中提取的特征以及分子描述符和添加剂信息等特征，使聚合物物理性质的预测更加准确。使用多个聚合物数据集对所提出的方法的预测性能进行了实证评估。我们报告说，与仅使用单一特征的基线传统方法相比，所提出的方法表现出更高的预测性能。该方法在聚合物性质预测方面显示出优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel transfer learning approach calledmulti-modal cascade model with feature transfer for polymer propertyprediction.Polymers are characterized by a composite of data in severaldifferent formats, including molecular descriptors and additive information aswell as chemical structures. However, in conventional approaches, predictionmodels were often constructed using each type of data separately. Our modelenables more accurate prediction of physical properties for polymers bycombining features extracted from the chemical structure by graph convolutionalneural networks (GCN) with features such as molecular descriptors and additiveinformation. The predictive performance of the proposed method is empiricallyevaluated using several polymer datasets. We report that the proposed methodshows high predictive performance compared to the baseline conventionalapproach using a single feature.</description>
      <author>example@mail.com (Kiichi Obuchi, Yuta Yahagi, Kiyohiko Toyama, Shukichi Tanaka, Kota Matsui)</author>
      <guid isPermaLink="false">2505.03704v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting</title>
      <link>http://arxiv.org/abs/2505.03679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at RA-L 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于相机-雷达融合的物体分割框架，通过结合相机和雷达传感器的信息，提高在恶劣天气条件下的语义分割性能。&lt;h4&gt;背景&lt;/h4&gt;物体分割在自动驾驶和机器人领域至关重要，相机传感器在恶劣天气下易受影响，而雷达传感器虽然鲁棒但数据稀疏且噪声大。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，通过融合相机和雷达传感器的信息，增强仅使用相机的分割基准，并提高在恶劣天气条件下的分割性能。&lt;h4&gt;方法&lt;/h4&gt;将扩散模型集成到相机-雷达融合架构中，利用雷达点特征和Segment-Anything模型创建伪掩码，并引入噪声降低单元对伪掩码进行去噪，最终生成补全原始图像缺失信息的重绘图像。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在Waterscenes数据集上，将仅使用相机的分割基准的mIoU提高了2.63%，并将相机-雷达融合架构的mIoU提高了1.48%。&lt;h4&gt;结论&lt;/h4&gt;该方法在恶劣天气条件下使用相机-雷达融合进行语义分割是有效的。&lt;h4&gt;翻译&lt;/h4&gt;Segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. Although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. In contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. Therefore, a promising approach is to fuse information from both sensors. In this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. We leverage radar point features to create pseudo-masks using the Segment-Anything model, treating the projected radar points as point prompts. Additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. Our method improves the camera-only segmentation baseline by 2.63% in mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the Waterscenes dataset. This demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segmenting objects in an environment is a crucial task for autonomous drivingand robotics, as it enables a better understanding of the surroundings of eachagent. Although camera sensors provide rich visual details, they are vulnerableto adverse weather conditions. In contrast, radar sensors remain robust undersuch conditions, but often produce sparse and noisy data. Therefore, apromising approach is to fuse information from both sensors. In this work, wepropose a novel framework to enhance camera-only baselines by integrating adiffusion model into a camera-radar fusion architecture. We leverage radarpoint features to create pseudo-masks using the Segment-Anything model,treating the projected radar points as point prompts. Additionally, we proposea noise reduction unit to denoise these pseudo-masks, which are further used togenerate inpainted images that complete the missing information in the originalimages. Our method improves the camera-only segmentation baseline by 2.63% inmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on theWaterscenes dataset. This demonstrates the effectiveness of our approach forsemantic segmentation using camera-radar fusion under adverse weatherconditions.</description>
      <author>example@mail.com (Huawei Sun, Bora Kunter Sahin, Georg Stettinger, Maximilian Bernhard, Matthias Schubert, Robert Wille)</author>
      <guid isPermaLink="false">2505.03679v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices</title>
      <link>http://arxiv.org/abs/2505.03303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 10 figures, 4 tables, submitted to Springer - Pattern  Recognition and Image Analysis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文全面评估了轻量级深度学习模型在图像分类中的应用，特别强调其在资源受限环境（如低内存设备）中的适用性。&lt;h4&gt;背景&lt;/h4&gt;在资源受限的环境中，如低内存设备，对轻量级深度学习模型的需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;评估轻量级深度学习模型在图像分类任务中的性能，并探究其在资源受限环境中的适用性。&lt;h4&gt;方法&lt;/h4&gt;对五种最先进的架构（MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S和ShuffleNetV2）在三个不同数据集（CIFAR-10、CIFAR-100和Tiny ImageNet）上的表现进行了基准测试。使用四个关键性能指标进行评估：分类准确率、推理时间、浮点运算次数（FLOPs）和模型大小。此外，通过比较预训练模型与从头开始训练的模型，研究了超参数调整、数据增强和训练方法的影响，重点关注MobileNetV3 Small。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习显著提高了模型准确率和计算效率，尤其是在Tiny ImageNet等复杂数据集上。EfficientNetV2在准确率上表现最佳，MobileNetV3在准确率和效率之间提供了最佳平衡，SqueezeNet在推理速度和紧凑性方面表现突出。&lt;h4&gt;结论&lt;/h4&gt;本研究的发现突出了准确性和效率之间的关键权衡，为在计算资源有限的现实世界应用中部署轻量级模型提供了有价值的见解。通过解决这些挑战，本研究有助于优化边缘计算和移动平台上的深度学习系统。&lt;h4&gt;翻译&lt;/h4&gt;本文提出对轻量级深度学习模型进行全面的评估，着重于其在资源受限环境（如低内存设备）中的适用性。五项最先进的架构——MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S和ShuffleNetV2——在三个不同的数据集（CIFAR-10、CIFAR-100和Tiny ImageNet）上进行了基准测试。使用四个关键性能指标进行评估：分类准确率、推理时间、浮点运算次数（FLOPs）和模型大小。此外，通过比较预训练模型与从头开始训练的模型，研究了超参数调整、数据增强和训练方法的影响，重点关注MobileNetV3 Small。我们的发现表明，迁移学习显著提高了模型准确率和计算效率，尤其是在Tiny ImageNet等复杂数据集上。EfficientNetV2在准确率上始终表现最佳，而MobileNetV3在准确率和效率之间提供了最佳平衡，SqueezeNet在推理速度和紧凑性方面表现突出。本研究强调了准确性和效率之间的关键权衡，为在计算资源有限的现实世界应用中部署轻量级模型提供了可操作的见解。通过解决这些挑战，本研究有助于优化边缘计算和移动平台上的深度学习系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a comprehensive evaluation of lightweight deep learningmodels for image classification, emphasizing their suitability for deploymentin resource-constrained environments such as low-memory devices. Fivestate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diversedatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed usingfour key performance metrics: classification accuracy, inference time,floating-point operations (FLOPs), and model size. Additionally, we investigatethe impact of hyperparameter tuning, data augmentation, and training paradigmsby comparing pretrained models with scratch-trained counterparts, focusing onMobileNetV3 Small. Our findings reveal that transfer learning significantlyenhances model accuracy and computational efficiency, particularly for complexdatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highestaccuracy, while MobileNetV3 offers the best balance between accuracy andefficiency, and SqueezeNet excels in inference speed and compactness. Thisstudy highlights critical trade-offs between accuracy and efficiency, offeringactionable insights for deploying lightweight models in real-world applicationswhere computational resources are limited. By addressing these challenges, thisresearch contributes to optimizing deep learning systems for edge computing andmobile platforms.</description>
      <author>example@mail.com (Tasnim Shahriar)</author>
      <guid isPermaLink="false">2505.03303v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Fast Large Deformation Matching with the Energy Distance Kernel</title>
      <link>http://arxiv.org/abs/2505.03342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种高效的点云与度量注册框架，使用双Lipschitz同胚，达到O(n log n)的复杂度。&lt;h4&gt;背景&lt;/h4&gt;现有方法在处理点云和度量注册时，存在计算复杂度高和超参数调优困难的问题。&lt;h4&gt;目的&lt;/h4&gt;旨在解决点云和度量注册的复杂性和超参数调优问题。&lt;h4&gt;方法&lt;/h4&gt;使用Energy-Distance (ED)内核，并通过其一维切片投影近似，每个投影的计算复杂度为O(n log n)，从而避免了超参数调优并实现了高效的大规模优化。引入了两种模型来正则化变形，同时保持低计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;提出的第一种模型依赖于TV正则化，第二种模型通过限制其使用范围到测度空间或点云空间，避免了非光滑的TV正则化。&lt;h4&gt;结论&lt;/h4&gt;在合成数据和真实数据上展示了模型的数值鲁棒性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;We propose an efficient framework for point cloud and measure registration using bi-Lipschitz homeomorphisms, achieving O(n log n) complexity, where n is the number of points. By leveraging the Energy-Distance (ED) kernel, which can be approximated by its sliced one-dimensional projections, each computable in O(n log n), our method avoids hyperparameter tuning and enables efficient large-scale optimization. The main issue to be solved is the lack of regularity of the ED kernel. To this goal, we introduce two models that regularize the deformations and retain a low computational footprint. The first model relies on TV regularization, while the second model avoids the non-smooth TV regularization at the cost of restricting its use to the space of measures, or cloud of points. Last, we demonstrate the numerical robustness and scalability of our models on synthetic and real data.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an efficient framework for point cloud and measure registrationusing bi-Lipschitz homeomorphisms, achieving O(n log n) complexity, where n isthe number of points. By leveraging the Energy-Distance (ED) kernel, which canbe approximated by its sliced one-dimensional projections, each computable inO(n log n), our method avoids hyperparameter tuning and enables efficientlarge-scale optimization. The main issue to be solved is the lack of regularityof the ED kernel. To this goal, we introduce two models that regularize thedeformations and retain a low computational footprint. The first model relieson TV regularization, while the second model avoids the non-smooth TVregularization at the cost of restricting its use to the space of measures, orcloud of points. Last, we demonstrate the numerical robustness and scalabilityof our models on synthetic and real data.</description>
      <author>example@mail.com (Siwan Boufadene, François-Xavier Vialard, Jean Feydy)</author>
      <guid isPermaLink="false">2505.03342v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Fairness of Automatic Speech Recognition in Cleft Lip and Palate Speech</title>
      <link>http://arxiv.org/abs/2505.03697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Digital Signal Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了裂唇裂腭（CLP）患者产生的语音在自动语音识别（ASR）系统中的公平性问题，并通过实验证实了公共ASR系统对CLP语音的公平性存在降低。研究提出了通过增强策略来提高ASR对CLP语音的识别公平性的方法。&lt;h4&gt;背景&lt;/h4&gt;CLP患者的语音由于结构异常通常表现为高度鼻音和呼吸声，这影响了ASR的性能和公平性。&lt;h4&gt;目的&lt;/h4&gt;验证公共ASR系统对CLP语音的公平性，并提出通过增强策略来提高ASR对CLP语音的识别公平性。&lt;h4&gt;方法&lt;/h4&gt;研究系统地探索了在不同严重程度下用正常语音增强CLP语音的方法，并在AIISH和NMCPC数据集上测试了GMM-HMM、Whisper和XLS-R三种ASR模型。&lt;h4&gt;主要发现&lt;/h4&gt;增强CLP语音的方法在ASR中表现出了良好的效果，如GMM-HMM模型在AIISH数据集上的词错误率（WER）从22.64%下降到18.76%，Whisper在NMCPC数据集上的WER从28.45%下降到18.89%。GMM-HMM在AIISH数据集上表现优于XLS-R和Whisper，这可能是因为GMM-HMM更适合识别卡纳达语儿童的语音。通过引入公平性分数，研究显示在增强后，AIISH和NMCPC数据集的公平性分别提高了17.89%和47.50%。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，通过使用正常语音增强CLP语音可以显著提高ASR系统的公平性和识别准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech produced by individuals with cleft lip and palate (CLP) is oftenhighly nasalized and breathy due to structural anomalies, causing shifts informant structure that affect automatic speech recognition (ASR) performanceand fairness. This study hypothesizes that publicly available ASR systemsexhibit reduced fairness for CLP speech and confirms this through experiments.Despite formant disruptions, mild and moderate CLP speech retains somespectro-temporal alignment with normal speech, motivating augmentationstrategies to enhance fairness. The study systematically explores augmentingCLP speech with normal speech across severity levels and evaluates its impacton ASR fairness. Three ASR models-GMM-HMM, Whisper, and XLS-R-were tested onAIISH and NMCPC datasets. Results indicate that training with normal speech andtesting on mixed data improves word error rate (WER). Notably, WER decreasedfrom $22.64\%$ to $18.76\%$ (GMM-HMM, AIISH) and $28.45\%$ to $18.89\%$(Whisper, NMCPC). The superior performance of GMM-HMM on AIISH may be due toits suitability for Kannada children's speech, a challenge for foundationmodels like XLS-R and Whisper. To assess fairness, a fairness score wasintroduced, revealing improvements of $17.89\%$ (AIISH) and $47.50\%$ (NMCPC)with augmentation.</description>
      <author>example@mail.com (Susmita Bhattacharjee, Jagabandhu Mishra, H. S. Shekhawat, S. R. Mahadeva Prasanna)</author>
      <guid isPermaLink="false">2505.03697v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2505.03581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DyGEnc的新方法，用于编码动态图，旨在解决动态环境中事件分析的问题。&lt;h4&gt;背景&lt;/h4&gt;动态环境中事件分析对智能代理和机器人的开发构成了挑战，当前方法主要利用视觉模型，但往往缺乏可解释的空间时间对象表示。&lt;h4&gt;目的&lt;/h4&gt;DyGEnc的目的是通过集成压缩的空间时间结构观察表示和大型语言模型的认知能力，以实现基于文本场景图的先进问答。&lt;h4&gt;方法&lt;/h4&gt;DyGEnc结合了压缩的空间时间结构观察表示和大型语言模型的能力，并通过STAR和AGQA数据集上的扩展评估证明了其优越性。&lt;h4&gt;主要发现&lt;/h4&gt;DyGEnc在处理有关人类与对象交互历史的问题时，比现有视觉方法高15-25%。此外，该方法可以无缝扩展到使用基础模型提取文本场景图以处理原始输入图像。&lt;h4&gt;结论&lt;/h4&gt;DyGEnc有望为长时程推理实现鲁棒和压缩的基于图的机器人记忆系统做出贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要分析了在动态环境中对事件进行分析的挑战，主要介绍了DyGEnc这一新型动态图编码方法，强调了其基于文本场景图的问答能力，并通过实验证明了其性能优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The analysis of events in dynamic environments poses a fundamental challengein the development of intelligent agents and robots capable of interacting withhumans. Current approaches predominantly utilize visual models. However, thesemethods often capture information implicitly from images, lacking interpretablespatial-temporal object representations. To address this issue we introduceDyGEnc - a novel method for Encoding a Dynamic Graph. This method integratescompressed spatial-temporal structural observation representation with thecognitive capabilities of large language models. The purpose of thisintegration is to enable advanced question answering based on a sequence oftextual scene graphs. Extended evaluations on the STAR and AGQA datasetsindicate that DyGEnc outperforms existing visual methods by a large margin of15-25% in addressing queries regarding the history of human-to-objectinteractions. Furthermore, the proposed method can be seamlessly extended toprocess raw input images utilizing foundational models for extracting explicittextual scene graphs, as substantiated by the results of a robotic experimentconducted with a wheeled manipulator platform. We hope that these findings willcontribute to the implementation of robust and compressed graph-based roboticmemory for long-horizon reasoning. Code is available atgithub.com/linukc/DyGEnc.</description>
      <author>example@mail.com (Sergey Linok, Vadim Semenov, Anastasia Trunova, Oleg Bulichev, Dmitry Yudin)</author>
      <guid isPermaLink="false">2505.03581v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges</title>
      <link>http://arxiv.org/abs/2505.03556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对大型人工智能模型（LAMs）在通信领域的应用进行了全面回顾，包括其基础、应用和面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;6G无线通信旨在建立一个智能的、无处不在的连接世界，提供前所未有的通信体验。LAMs与典型的AI模型相比，具有显著更大的规模（例如，数十亿或数万亿的参数），并展现出出色的认知能力。&lt;h4&gt;目的&lt;/h4&gt;研究LAMs在通信中的应用，以解决未来无线通信系统中的复杂挑战。&lt;h4&gt;方法&lt;/h4&gt;介绍了基于AI的通信系统的当前状态，强调了将LAMs集成到通信中的动机，并总结了关键贡献。概述了LAMs在通信中的基本概念，包括主要架构、分类、训练方法和评估技术。还介绍了优化策略，如思维链（CoT）、检索增强生成（RAG）和代理系统。&lt;h4&gt;主要发现&lt;/h4&gt;LAMs在通信中具有强大的泛化能力和处理未在训练中看到的任务的能力，对于提供多样化的AI服务至关重要。&lt;h4&gt;结论&lt;/h4&gt;本文分析了LAMs在通信领域的应用进展，讨论了当前研究的挑战，并提供了未来研究方向的见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：6G无线通信旨在建立一个智能的、无处不在的连接世界，提供前所未有的通信体验。大型人工智能模型（LAMs）与典型的AI模型相比，具有显著更大的规模（例如，数十亿或数万亿的参数），并展现出出色的认知能力，包括强大的泛化能力以微调下游任务，以及处理训练期间未见任务的涌现能力。因此，LAMs高效地为各种通信应用提供AI服务，成为解决未来无线通信系统中复杂挑战的关键工具。本研究对LAMs在通信中的基础、应用和挑战进行了全面回顾。首先，我们介绍了基于AI的通信系统的当前状态，强调了将LAMs集成到通信中的动机，并总结了关键贡献。然后，我们概述了LAMs在通信中的基本概念。这包括对LAMs主要架构的介绍，如transformer、扩散模型和mamba。我们还探讨了LAMs的分类，包括大型语言模型（LLMs）、大型视觉模型（LVMs）、大型多模态模型（LMMs）和世界模型，并考察了它们在通信中的潜在应用。此外，我们还涵盖了LAMs在通信系统中的训练方法和评估技术。最后，我们介绍了优化策略，如思维链（CoT）、检索增强生成（RAG）和代理系统。在此之后，我们讨论了LAMs在各种通信场景中的研究进展。最后，我们分析了当前研究的挑战，并提供了对未来研究方向的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 6G wireless communications aim to establish an intelligent world ofubiquitous connectivity, providing an unprecedented communication experience.Large artificial intelligence models (LAMs) are characterized by significantlylarger scales (e.g., billions or trillions of parameters) compared to typicalartificial intelligence (AI) models. LAMs exhibit outstanding cognitiveabilities, including strong generalization capabilities for fine-tuning todownstream tasks, and emergent capabilities to handle tasks unseen duringtraining. Therefore, LAMs efficiently provide AI services for diversecommunication applications, making them crucial tools for addressing complexchallenges in future wireless communication systems. This study provides acomprehensive review of the foundations, applications, and challenges of LAMsin communication. First, we introduce the current state of AI-basedcommunication systems, emphasizing the motivation behind integrating LAMs intocommunications and summarizing the key contributions. We then present anoverview of the essential concepts of LAMs in communication. This includes anintroduction to the main architectures of LAMs, such as transformer, diffusionmodels, and mamba. We also explore the classification of LAMs, including largelanguage models (LLMs), large vision models (LVMs), large multimodal models(LMMs), and world models, and examine their potential applications incommunication. Additionally, we cover the training methods and evaluationtechniques for LAMs in communication systems. Lastly, we introduce optimizationstrategies such as chain of thought (CoT), retrieval augmented generation(RAG), and agentic systems. Following this, we discuss the researchadvancements of LAMs across various communication scenarios. Finally, weanalyze the challenges in the current research and provide insights intopotential future research directions.</description>
      <author>example@mail.com (Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Merouane Debbah, Dusit Niyato, Zhu Han)</author>
      <guid isPermaLink="false">2505.03556v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization</title>
      <link>http://arxiv.org/abs/2505.03186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了CoGenAV模型，一个用于学习跨多种语音和音频视觉任务的多功能表示的模型，通过优化来自自然音频视觉同步、对比特征对齐和生成文本预测的双目标，使用仅223小时标记数据从LRS2数据集进行训练，有效提高了多种语音处理任务的表现。&lt;h4&gt;背景&lt;/h4&gt;语音处理在传统音频-only系统在挑战性条件下失效时面临困难，而说话者的唇部动作、声音和底层语言内容之间的内在同步为改进语音处理任务提供了丰富的信息来源。&lt;h4&gt;目的&lt;/h4&gt;开发一个数据高效的模型，能够学习通用的音频视觉表示，适用于广泛的语音和音频视觉任务。&lt;h4&gt;方法&lt;/h4&gt;CoGenAV通过优化双目标进行训练，双目标来源于自然音频视觉同步、对比特征对齐和生成文本预测，使用LRS2数据集的223小时标记数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;CoGenAV在多个基准测试中展示了其有效性和多功能性，在LRS2数据集上应用于音频视觉语音识别（AVSR）时，实现了最先进的Word Error Rate（WER）为1.27，在视觉语音识别（VSR）上也表现出色，WER为22.0，在嘈杂环境中的性能提高了超过70%，并且对语音重建任务也有益处，提高了语音增强和分离的性能，在音频视觉同步任务如主动说话者检测（ASD）中也取得了有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;CoGenAV模型将被开源，以促进学术界和工业界进一步的开发和合作。&lt;h4&gt;翻译&lt;/h4&gt;摘要：说话者的唇部动作、声音和底层语言内容之间的内在同步为改进语音处理任务提供了丰富的信息来源，特别是在传统音频-only系统在挑战性条件下失效的情况下。我们介绍了CoGenAV，一个强大且数据高效的模型，旨在学习通用的音频视觉表示，适用于广泛的语音和音频视觉任务。CoGenAV通过优化来自自然音频视觉同步、对比特征对齐和生成文本预测的双目标进行训练，仅使用LRS2数据集的223小时标记数据。这种对比生成同步策略有效地捕捉了跨模态的基本相关性。我们在多个基准测试中展示了所学习的CoGenAV表示的有效性和多功能性。当在LRS2上用于音频视觉语音识别（AVSR）时，这些表示有助于实现最先进的Word Error Rate（WER）为1.27。它们还在视觉语音识别（VSR）上表现出色，LRS2上的WER为22.0，并且通过超过70%的提高在嘈杂环境中显著提高了性能。此外，CoGenAV表示对语音重建任务也有益处，提高了语音增强和分离的性能，并在音频视觉同步任务如主动说话者检测（ASD）中取得了有竞争力的结果。我们的模型将被开源，以促进学术界和工业界进一步的开发和合作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The inherent synchronization between a speaker's lip movements, voice, andthe underlying linguistic content offers a rich source of information forimproving speech processing tasks, especially in challenging conditions wheretraditional audio-only systems falter. We introduce CoGenAV, a powerful anddata-efficient model designed to learn versatile audio-visual representationsapplicable across a wide range of speech and audio-visual tasks. CoGenAV istrained by optimizing a dual objective derived from natural audio-visualsynchrony, contrastive feature alignment and generative text prediction, usingonly 223 hours of labeled data from the LRS2 dataset. Thiscontrastive-generative synchronization strategy effectively capturesfundamental cross-modal correlations. We showcase the effectiveness andversatility of the learned CoGenAV representations on multiple benchmarks. Whenutilized for Audio-Visual Speech Recognition (AVSR) on LRS2, theserepresentations contribute to achieving a state-of-the-art Word Error Rate(WER) of 1.27. They also enable strong performance in Visual Speech Recognition(VSR) with a WER of 22.0 on LRS2, and significantly improve performance innoisy environments by over 70%. Furthermore, CoGenAV representations benefitspeech reconstruction tasks, boosting performance in Speech Enhancement andSeparation, and achieve competitive results in audio-visual synchronizationtasks like Active Speaker Detection (ASD). Our model will be open-sourced tofacilitate further development and collaboration within both academia andindustry.</description>
      <author>example@mail.com (Detao Bai, Zhiheng Ma, Xihan Wei, Liefeng Bo)</author>
      <guid isPermaLink="false">2505.03186v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal Delivery Based on Agentic UAVs</title>
      <link>http://arxiv.org/abs/2505.03460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多模态大型语言模型（MLLMs）的物流无人机（UAV）递送系统，旨在解决精细粒度末端递送的需求。&lt;h4&gt;背景&lt;/h4&gt;随着智能物流需求的增长，特别是精细粒度末端递送的需求，需要自主无人机递送系统。然而，现有研究主要依赖于地面机器人，而基于视觉-语言导航（VLN）的任务主要关注粗粒度、长距离目标，不适合精确的末端递送。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一差距，本文提出了LogisticsVLN系统，一个可扩展的空中递送系统，用于自主终端递送。&lt;h4&gt;方法&lt;/h4&gt;LogisticsVLN系统集成了轻量级大型语言模型（LLMs）和视觉-语言模型（VLMs），在一个模块化管道中用于请求理解、楼层定位、目标检测和动作决策。为了支持这一新环境的研究和评估，构建了Vision-Language Delivery（VLD）数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在VLD数据集上的实验结果表明LogisticsVLN系统的可行性。此外，对系统每个模块进行了子任务级别的评估，为提高基于基础模型视觉-语言递送系统的鲁棒性和实际部署提供了宝贵见解。&lt;h4&gt;结论&lt;/h4&gt;LogisticsVLN系统为智能物流的精细粒度末端递送提供了一种可行的解决方案，并通过实验验证了其有效性和可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing demand for intelligent logistics, particularly fine-grainedterminal delivery, underscores the need for autonomous UAV (Unmanned AerialVehicle)-based delivery systems. However, most existing last-mile deliverystudies rely on ground robots, while current UAV-based Vision-LanguageNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,making them unsuitable for precise terminal delivery. To bridge this gap, wepropose LogisticsVLN, a scalable aerial delivery system built on multimodallarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLNintegrates lightweight Large Language Models (LLMs) and Visual-Language Models(VLMs) in a modular pipeline for request understanding, floor localization,object detection, and action-decision making. To support research andevaluation in this new setting, we construct the Vision-Language Delivery (VLD)dataset within the CARLA simulator. Experimental results on the VLD datasetshowcase the feasibility of the LogisticsVLN system. In addition, we conductsubtask-level evaluations of each module of our system, offering valuableinsights for improving the robustness and real-world deployment of foundationmodel-based vision-language delivery systems.</description>
      <author>example@mail.com (Xinyuan Zhang, Yonglin Tian, Fei Lin, Yue Liu, Jing Ma, Kornélia Sára Szatmáry, Fei-Yue Wang)</author>
      <guid isPermaLink="false">2505.03460v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds</title>
      <link>http://arxiv.org/abs/2505.02972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个名为GeoERM的几何感知多任务学习框架，通过将共享表示嵌入其自然的黎曼流形上并利用显式流形操作来优化它，以提升统计能力和学习效率。&lt;h4&gt;背景&lt;/h4&gt;现有的多任务学习（MTL）方法通常将潜在表示矩阵视为普通欧几里得空间中的一个点，忽略了它往往具有非欧几里得几何，因此在任务异质或对抗性时鲁棒性不足。&lt;h4&gt;目的&lt;/h4&gt;提升统计能力和学习效率，提高多任务学习的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;GeoERM框架通过在每个训练周期进行（1）遵循搜索空间内在曲率的黎曼梯度步，随后（2）高效的极向收缩以保持在流形上，保证了每一步的几何保真度。该方法适用于广泛的矩阵分解MTL模型，且每次迭代的成本与欧几里得基线相同。&lt;h4&gt;主要发现&lt;/h4&gt;在合成实验和可穿戴传感器活动识别基准测试中，GeoERM一致性地提高了估计准确性，减少了负面迁移，并且在对抗性标签噪声下保持稳定，优于现有的MTL和单任务方法。&lt;h4&gt;结论&lt;/h4&gt;GeoERM是一个有效且鲁棒的多任务学习方法，通过利用几何结构提升了学习效率并降低了负面迁移和噪声的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-Task Learning (MTL) seeks to boost statistical power and learningefficiency by discovering structure shared across related tasks.State-of-the-art MTL representation methods, however, usually treat the latentrepresentation matrix as a point in ordinary Euclidean space, ignoring itsoften non-Euclidean geometry, thus sacrificing robustness when tasks areheterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTLframework that embeds the shared representation on its natural Riemannianmanifold and optimizes it via explicit manifold operations. Each training cycleperforms (i) a Riemannian gradient step that respects the intrinsic curvatureof the search space, followed by (ii) an efficient polar retraction to remainon the manifold, guaranteeing geometric fidelity at every iteration. Theprocedure applies to a broad class of matrix-factorized MTL models and retainsthe same per-iteration cost as Euclidean baselines. Across a set of syntheticexperiments with task heterogeneity and on a wearable-sensoractivity-recognition benchmark, GeoERM consistently improves estimationaccuracy, reduces negative transfer, and remains stable under adversarial labelnoise, outperforming leading MTL and single-task alternatives.</description>
      <author>example@mail.com (Aoran Chen, Yang Feng)</author>
      <guid isPermaLink="false">2505.02972v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.00507v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in CVPRw2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HeAL的启发式增强主动学习算法，用于3D物体检测，通过结合启发式特征、定位和分类，选择对模型训练最有贡献的样本。&lt;h4&gt;背景&lt;/h4&gt;主动学习在自动驾驶模型训练的样本选择中得到了应用，但在3D物体检测中，未控制场景下的样本选择具有挑战性，且现有方法主要关注理论方面，忽视了实际应用中的洞察。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的主动学习方法，以解决3D物体检测中样本选择的问题，并提高检测模型的性能。&lt;h4&gt;方法&lt;/h4&gt;HeAL算法通过整合启发式特征（如物体距离和点数）来估计不确定性，从而增强所选样本对训练检测模型的有用性。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI数据集上的定量评估显示，HeAL在mAP（平均精度）方面与现有最佳方法具有竞争力，并且只使用24%的样本就达到了与全监督基线相同的mAP。&lt;h4&gt;结论&lt;/h4&gt;HeAL算法在3D物体检测中通过提高样本选择的质量，显著提升了模型的性能，同时减少了训练所需的样本数量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Active Learning has proved to be a relevant approach to perform sampleselection for training models for Autonomous Driving. Particularly, previousworks on active learning for 3D object detection have shown that selection ofsamples in uncontrolled scenarios is challenging. Furthermore, currentapproaches focus exclusively on the theoretical aspects of the sample selectionproblem but neglect the practical insights that can be obtained from theextensive literature and application of 3D detection models. In this paper, weintroduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)which integrates those heuristical features together with Localization andClassification to deliver the most contributing samples to the model'straining. In contrast to previous works, our approach integrates heuristicalfeatures such as object distance and point-quantity to estimate theuncertainty, which enhance the usefulness of selected samples to traindetection models. Our quantitative evaluation on KITTI shows that HeAL presentscompetitive mAP with respect to the State-of-the-Art, and achieves the same mAPas the full-supervised baseline with only 24% of the samples.</description>
      <author>example@mail.com (Esteban Rivera, Surya Prabhakaran, Markus Lienkamp)</author>
      <guid isPermaLink="false">2505.00507v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2505.03300v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IV2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的3D语义分割流程，用于自动驾驶和基础设施管理，该流程通过监督学习实现，避免了直接3D标注或依赖其他模态（如相机图像）的需求。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR点云的语义分割对于自动驾驶和基础设施管理至关重要，但需要大量的标注数据集并面临领域迁移问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的3D语义分割方法，无需直接3D标注或依赖其他模态，并用于伪标签生成。&lt;h4&gt;方法&lt;/h4&gt;该流程利用对齐的场景和最先进的2D分割方法，从LiDAR扫描中生成2D视图，并使用相机域预训练模型对这些视图进行2D语义分割。然后，通过简单的基于投票的估计器将分割的2D输出回投影到3D点上。&lt;h4&gt;主要发现&lt;/h4&gt;该研究提出了一个全局的3D语义分割流程，无需先前的3D标注或其他模态进行推理，并展示了生成的伪标签在无监督领域自适应任务中的潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法为3D语义分割提供了一种新的解决方案，适用于自动驾驶和基础设施管理等领域，且无需大量3D标注数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IV55156.2024.10588443&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of 3D LiDAR point clouds, essential for autonomousdriving and infrastructure management, is best achieved by supervised learning,which demands extensive annotated datasets and faces the problem of domainshifts. We introduce a new 3D semantic segmentation pipeline that leveragesaligned scenes and state-of-the-art 2D segmentation methods, avoiding the needfor direct 3D annotation or reliance on additional modalities such as cameraimages at inference time. Our approach generates 2D views from LiDAR scanscolored by sensor intensity and applies 2D semantic segmentation to these viewsusing a camera-domain pretrained model. The segmented 2D outputs are thenback-projected onto the 3D points, with a simple voting-based estimator thatmerges the labels associated to each 3D point. Our main contribution is aglobal pipeline for 3D semantic segmentation requiring no prior 3D annotationand not other modality for inference, which can be used for pseudo-labelgeneration. We conduct a thorough ablation study and demonstrate the potentialof the generated pseudo-labels for the Unsupervised Domain Adaptation task.</description>
      <author>example@mail.com (Andrew Caunes, Thierry Chateau, Vincent Frémont)</author>
      <guid isPermaLink="false">2505.03300v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2505.03406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型（LLMs）在医疗保健领域的应用，特别是通过整合医院特定数据和量化低秩适应（QLoRA）微调的检索增强生成（RAG）来增强医疗决策支持。&lt;h4&gt;背景&lt;/h4&gt;医疗决策支持需要准确的信息检索和生成，而LLMs在处理自然语言方面具有优势。&lt;h4&gt;目的&lt;/h4&gt;提高医疗决策的准确性和效率，通过使用LLMs和RAG技术。&lt;h4&gt;方法&lt;/h4&gt;使用Llama 3.2-3B-Instruct作为基础模型，通过嵌入和检索与医疗保健相关的信息，并使用QLoRA进行参数效率和内存优化。&lt;h4&gt;主要发现&lt;/h4&gt;该系统在多个医疗基准测试中表现良好，可以用于提供基本的医疗建议。系统还支持疾病预测、治疗建议和复杂医疗报告的总结。&lt;h4&gt;结论&lt;/h4&gt;LLMs在医疗保健领域具有广泛的应用潜力，但需要考虑患者隐私、数据安全和临床验证等伦理和实际挑战。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了大型语言模型（LLMs）在医疗保健中的应用，特别是通过整合医院特定数据和量化低秩适应（QLoRA）微调的检索增强生成（RAG）来增强医疗决策支持。该系统以Llama 3.2-3B-Instruct作为其基础模型。通过嵌入和检索与医疗保健相关的上下文信息，该系统显著提高了响应准确性。QLoRA促进了参数效率和内存优化，通过专门的量化技术保留了医疗信息的完整性。我们的研究还表明，我们的模型在各种医疗基准测试中表现相对良好，表明它可以用于提供基本的医疗建议。本文详细介绍了系统的技术组件，包括其架构、量化方法和关键医疗保健应用，如从患者症状和病史中增强疾病预测、治疗建议和复杂医疗报告的有效总结。我们触及了伦理考虑——患者隐私、数据安全以及严格临床验证的需求——以及将此类系统整合到现实世界医疗工作流程中的实际挑战。此外，轻量级的量化权重确保了可扩展性和部署的简便性，即使在资源有限的医院环境中也是如此。最后，本文分析了LLMs对医疗保健的更广泛影响，并概述了LLMs在医疗环境中的未来方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research paper investigates the application of Large Language Models(LLMs) in healthcare, specifically focusing on enhancing medical decisionsupport through Retrieval-Augmented Generation (RAG) integrated withhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. Byembedding and retrieving context-relevant healthcare information, the systemsignificantly improves response accuracy. QLoRA facilitates notable parameterefficiency and memory optimization, preserving the integrity of medicalinformation through specialized quantization techniques. Our research alsoshows that our model performs relatively well on various medical benchmarks,indicating that it can be used to make basic medical suggestions. This paperdetails the system's technical components, including its architecture,quantization methods, and key healthcare applications such as enhanced diseaseprediction from patient symptoms and medical history, treatment suggestions,and efficient summarization of complex medical reports. We touch on the ethicalconsiderations-patient privacy, data security, and the need for rigorousclinical validation-as well as the practical challenges of integrating suchsystems into real-world healthcare workflows. Furthermore, the lightweightquantized weights ensure scalability and ease of deployment even inlow-resource hospital environments. Finally, the paper concludes with ananalysis of the broader impact of LLMs on healthcare and outlines futuredirections for LLMs in medical settings.</description>
      <author>example@mail.com (Mohammad Shoaib Ansari, Mohd Sohail Ali Khan, Shubham Revankar, Aditya Varma, Anil S. Mokhade)</author>
      <guid isPermaLink="false">2505.03406v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera</title>
      <link>http://arxiv.org/abs/2505.03093v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用消费级360度视频摄像头进行森林资源测量的低成本替代方案。&lt;h4&gt;背景&lt;/h4&gt;森林资源调查依赖于胸径（DBH）的准确测量，而基于LiDAR的技术虽然精度高，但成本高且操作复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种低成本、易于操作的替代方法来测量DBH。&lt;h4&gt;方法&lt;/h4&gt;开发了一个半自动化的流程，包括使用Agisoft Metashape软件进行密集点云重建，通过将SAM掩模投影到3D云上执行语义树干分割，以及使用基于RANSAC的稳健技术来估计横截面形状和DBH。&lt;h4&gt;主要发现&lt;/h4&gt;在61个不同条件下的43棵树的测量中，该方法相对于人工测量的“真实值”的中值绝对相对误差为5-9%，仅比基于LiDAR的估计高出2-4%，同时使用成本低得多的单台360度摄像头，设置简单，且易于获取。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种经济高效的DBH测量方法，适用于森林资源调查和碳核算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forest inventories rely on accurate measurements of the diameter at breastheight (DBH) for ecological monitoring, resource management, and carbonaccounting. While LiDAR-based techniques can achieve centimeter-levelprecision, they are cost-prohibitive and operationally complex. We present alow-cost alternative that only needs a consumer-grade 360 video camera. Oursemi-automated pipeline comprises of (i) a dense point cloud reconstructionusing Structure from Motion (SfM) photogrammetry software called AgisoftMetashape, (ii) semantic trunk segmentation by projecting Grounded SegmentAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-basedtechnique to estimate cross section shape and DBH. We introduce an interactivevisualization tool for inspecting segmented trees and their estimated DBH. On61 acquisitions of 43 trees under a variety of conditions, our method attainsmedian absolute relative errors of 5-9% with respect to "ground-truth" manualmeasurements. This is only 2-4% higher than LiDAR-based estimates, whileemploying a single 360 camera that costs orders of magnitude less, requiresminimal setup, and is widely available.</description>
      <author>example@mail.com (Siming He, Zachary Osman, Fernando Cladera, Dexter Ong, Nitant Rai, Patrick Corey Green, Vijay Kumar, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2505.03093v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Early Prediction of Sepsis: Feature-Aligned Transfer Learning</title>
      <link>http://arxiv.org/abs/2505.02889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A project implemented for MACHINE LEARNING IN HEALTH AND BIOMEDICAL  SCIENCE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为特征对齐迁移学习（FATL）的方法，旨在通过机器学习预测早期脓毒症，以帮助医疗提供者更早地进行干预，从而改善患者预后、降低医疗成本并支持更公平的医疗保健服务。&lt;h4&gt;背景&lt;/h4&gt;脓毒症是一种危及生命的疾病，当身体对感染产生极端反应时，会导致全身炎症、器官衰竭甚至死亡。由于脓毒症进展迅速，早期检测至关重要。然而，现有的诊断方法往往在严重损害发生后才能识别脓毒症。&lt;h4&gt;目的&lt;/h4&gt;本项目旨在通过开发一个基于机器学习的系统来预测早期脓毒症，为医疗提供者提供更多干预时间。&lt;h4&gt;方法&lt;/h4&gt;论文提出的方法FATL通过识别和关注多个研究中最重要的和最常报告的特征来解决现有模型中患者信息或特征变异大的问题，确保模型的一致性和临床相关性。FATL通过结合来自不同人群的模型的知识的加权方法来解决训练在狭窄患者群体上的模型导致的群体偏差问题。&lt;h4&gt;主要发现&lt;/h4&gt;FATL提供了一种实用且可扩展的早期脓毒症检测解决方案，尤其适用于资源有限的医院，并有可能改善患者预后、降低医疗成本并支持更公平的医疗保健服务。&lt;h4&gt;结论&lt;/h4&gt;FATL方法能够提高脓毒症早期检测的准确性和有效性，有助于改善患者治疗结果，降低医疗成本，并促进医疗服务的公平性。&lt;h4&gt;翻译&lt;/h4&gt;Sepsis is a life-threatening medical condition that occurs when the body has an extreme response to infection, leading to widespread inflammation, organ failure, and potentially death. Because sepsis can worsen rapidly, early detection is critical to saving lives. However, current diagnostic methods often identify sepsis only after significant damage has already occurred. Our project aims to address this challenge by developing a machine learning based system to predict sepsis in its early stages, giving healthcare providers more time to intervene. A major problem with existing models is the wide variability in the patient information or features they use, such as heart rate, temperature, and lab results. This inconsistency makes models difficult to compare and limits their ability to work across different hospitals and settings. To solve this, we propose a method called Feature Aligned Transfer Learning (FATL), which identifies and focuses on the most important and commonly reported features across multiple studies, ensuring the model remains consistent and clinically relevant. Most existing models are trained on narrow patient groups, leading to population bias. FATL addresses this by combining knowledge from models trained on diverse populations, using a weighted approach that reflects each model's contribution. This makes the system more generalizable and effective across different patient demographics and clinical environments. FATL offers a practical and scalable solution for early sepsis detection, particularly in hospitals with limited resources, and has the potential to improve patient outcomes, reduce healthcare costs, and support more equitable healthcare delivery.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sepsis is a life threatening medical condition that occurs when the body hasan extreme response to infection, leading to widespread inflammation, organfailure, and potentially death. Because sepsis can worsen rapidly, earlydetection is critical to saving lives. However, current diagnostic methodsoften identify sepsis only after significant damage has already occurred. Ourproject aims to address this challenge by developing a machine learning basedsystem to predict sepsis in its early stages, giving healthcare providers moretime to intervene.  A major problem with existing models is the wide variability in the patientinformation or features they use, such as heart rate, temperature, and labresults. This inconsistency makes models difficult to compare and limits theirability to work across different hospitals and settings. To solve this, wepropose a method called Feature Aligned Transfer Learning (FATL), whichidentifies and focuses on the most important and commonly reported featuresacross multiple studies, ensuring the model remains consistent and clinicallyrelevant.  Most existing models are trained on narrow patient groups, leading topopulation bias. FATL addresses this by combining knowledge from models trainedon diverse populations, using a weighted approach that reflects each modelscontribution. This makes the system more generalizable and effective acrossdifferent patient demographics and clinical environments. FATL offers apractical and scalable solution for early sepsis detection, particularly inhospitals with limited resources, and has the potential to improve patientoutcomes, reduce healthcare costs, and support more equitable healthcaredelivery.</description>
      <author>example@mail.com (Oyindolapo O. Komolafe, Zhimin Mei, David Morales Zarate, Gregory William Spangenberg)</author>
      <guid isPermaLink="false">2505.02889v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights</title>
      <link>http://arxiv.org/abs/2505.02845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了浮动车观测器（FCO）在微观交通模拟中的应用，通过集成车载传感器检测和定位其他交通参与者，提供更丰富、更详细的交通数据。通过仿真实验，探讨了不同建模方法的效果，并引入了一种基于神经网络的仿真技术，以评估FCO在智能交通系统（ITS）中的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;浮动车观测器（FCO）通过集成车载传感器，扩展了传统的浮动车数据（FCD），能够检测和定位其他交通参与者，提供更详细的交通数据。&lt;h4&gt;目的&lt;/h4&gt;探索各种建模方法，以评估FCO在微观交通模拟中的应用潜力，特别是在智能交通系统（ITS）中的应用。&lt;h4&gt;方法&lt;/h4&gt;采用了从二维光线追踪到高保真协同模拟的多种建模方法，后者模拟真实世界传感器并集成3D目标检测算法，以更精确地复制FCO的检测过程。此外，还引入了一种基于神经网络的仿真技术，以近似高保真协同模拟的结果。&lt;h4&gt;主要发现&lt;/h4&gt;即使在20%的渗透率下，基于LiDAR检测的FCO能够在各种交叉口和交通需求场景中识别65%的车辆。通过整合时间洞察，可以恢复之前检测但现在不可见的车辆，数据驱动方法能够恢复超过80%的这些车辆，位置偏差最小。&lt;h4&gt;结论&lt;/h4&gt;FCO在ITS中具有巨大潜力，尤其是在增强交通状态估计和监测方面，尤其是在不同渗透率和交通条件下。&lt;h4&gt;翻译&lt;/h4&gt;This paper investigates the application of Floating Car Observers (FCO) in microscopic traffic simulations, which integrate onboard sensors to detect and localize other traffic participants, providing richer and more detailed traffic data. Through simulation experiments, various modeling methods are explored to evaluate their potential for Intelligent Transportation System (ITS) applications. These methods range from 2D raytracing to high-fidelity co-simulations that emulate real-world sensors and integrate 3D object detection algorithms to closely replicate FCO detections. Additionally, a neural network-based emulation technique is introduced to effectively approximate the results of high-fidelity co-simulations. This approach captures the unique characteristics of FCO detections while offering a fast and scalable solution for modeling. Using this emulation method, the impact of FCO data in a digital twin of a traffic network modeled in SUMO is investigated. Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based detections can identify 65% of vehicles across various intersections and traffic demand scenarios. Further potential emerges when temporal insights are integrated, enabling the recovery of previously detected but currently unseen vehicles. By employing data-driven methods, over 80% of these vehicles are recovered with minimal positional deviations. These findings underscore the potential of FCOs for ITS, particularly in enhancing traffic state estimation and monitoring under varying penetration rates and traffic conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) byintegrating onboard sensors to detect and localize other traffic participants,providing richer and more detailed traffic data. In this work, we explorevarious modeling approaches for FCO detections within microscopic trafficsimulations to evaluate their potential for Intelligent Transportation System(ITS) applications. These approaches range from 2D raytracing to high-fidelityco-simulations that emulate real-world sensors and integrate 3D objectdetection algorithms to closely replicate FCO detections. Additionally, weintroduce a neural network-based emulation technique that effectivelyapproximates the results of high-fidelity co-simulations. This approachcaptures the unique characteristics of FCO detections while offering a fast andscalable solution for modeling. Using this emulation method, we investigate theimpact of FCO data in a digital twin of a traffic network modeled in SUMO.Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-baseddetections can identify 65% of vehicles across various intersections andtraffic demand scenarios. Further potential emerges when temporal insights areintegrated, enabling the recovery of previously detected but currently unseenvehicles. By employing data-driven methods, we recover over 80% of thesevehicles with minimal positional deviations. These findings underscore thepotential of FCOs for ITS, particularly in enhancing traffic state estimationand monitoring under varying penetration rates and traffic conditions.</description>
      <author>example@mail.com (Jeremias Gerner, Klaus Bogenberger, Stefanie Schmidtner)</author>
      <guid isPermaLink="false">2505.02845v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection</title>
      <link>http://arxiv.org/abs/2505.03359v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMBC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用基于语音的AI模型检测抑郁症和创伤后应激障碍（PTSD）的方法，并提出了针对性别偏见问题的解决方案。&lt;h4&gt;背景&lt;/h4&gt;基于语音的AI模型在心理健康评估中显示出潜力，但存在性别偏见问题，可能导致不公平和不准确的预测。&lt;h4&gt;目的&lt;/h4&gt;通过引入领域对抗训练方法，考虑性别差异，提高基于语音的抑郁症和PTSD检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;将不同性别视为不同的领域，并将这些信息整合到预训练的语音基础模型中，并在E-DAIC数据集上验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著提高了检测性能，与基线相比，F1分数提高了高达13.29个百分点。&lt;h4&gt;结论&lt;/h4&gt;解决AI驱动的心理健康评估中的性别差异问题至关重要。&lt;h4&gt;翻译&lt;/h4&gt;Speech-based AI models are emerging as powerful tools for detecting depression and the presence of Post-traumatic stress disorder (PTSD), offering a non-invasive and cost-effective way to assess mental health. However, these models often struggle with gender bias, which can lead to unfair and inaccurate predictions. In this study, our study addresses this issue by introducing a domain adversarial training approach that explicitly considers gender differences in speech-based depression and PTSD detection. Specifically, we treat different genders as distinct domains and integrate this information into a pretrained speech foundation model. We then validate its effectiveness on the E-DAIC dataset to assess its impact on performance. Experimental results show that our method notably improves detection performance, increasing the F1-score by up to 13.29 percentage points compared to the baseline. This highlights the importance of addressing demographic disparities in AI-driven mental health assessment.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech-based AI models are emerging as powerful tools for detectingdepression and the presence of Post-traumatic stress disorder (PTSD), offeringa non-invasive and cost-effective way to assess mental health. However, thesemodels often struggle with gender bias, which can lead to unfair and inaccuratepredictions. In this study, our study addresses this issue by introducing adomain adversarial training approach that explicitly considers genderdifferences in speech-based depression and PTSD detection. Specifically, wetreat different genders as distinct domains and integrate this information intoa pretrained speech foundation model. We then validate its effectiveness on theE-DAIC dataset to assess its impact on performance. Experimental results showthat our method notably improves detection performance, increasing the F1-scoreby up to 13.29 percentage points compared to the baseline. This highlights theimportance of addressing demographic disparities in AI-driven mental healthassessment.</description>
      <author>example@mail.com (June-Woo Kim, Haram Yoon, Wonkyo Oh, Dawoon Jung, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang)</author>
      <guid isPermaLink="false">2505.03359v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</title>
      <link>http://arxiv.org/abs/2505.03233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了使用大规模合成动作数据训练视觉-语言-动作模型的可能性，并提出了一种名为GraspVLA的模型，该模型在合成动作数据上进行了预训练，以促进抓取任务的零样本泛化和小样本适应性。&lt;h4&gt;背景&lt;/h4&gt;实体基础模型因其零样本泛化、可扩展性和通过少量后训练适应新任务的能力而受到越来越多的关注。然而，现有模型严重依赖真实世界数据，收集成本高且劳动密集。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一差距，研究旨在探索使用大规模合成动作数据完全训练视觉-语言-动作模型的可行性。&lt;h4&gt;方法&lt;/h4&gt;研究人员创建了SynGrasp-1B，这是一个包含十亿帧机器人抓取数据的集，通过模拟生成，具有逼真的渲染和广泛的领域随机化。基于此，他们提出了GraspVLA模型，该模型在大型合成动作数据上进行了预训练，并将自回归感知任务和基于流匹配的动作生成整合到一个统一的思维链过程中，以实现合成动作数据和互联网语义数据的联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;GraspVLA在真实世界和模拟基准测试中表现出先进的零样本泛化能力和小样本适应性，能够将学习到的动作转移到更广泛的互联网覆盖对象上，实现了抓取任务的开放词汇泛化。&lt;h4&gt;结论&lt;/h4&gt;研究证明了使用合成数据训练视觉-语言-动作模型的可行性，并提出了一个能够实现零样本泛化和小样本适应性的模型，这将对机器人抓取任务的研究和应用产生积极影响。&lt;h4&gt;翻译&lt;/h4&gt;Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied foundation models are gaining increasing attention for theirzero-shot generalization, scalability, and adaptability to new tasks throughfew-shot post-training. However, existing models rely heavily on real-worlddata, which is costly and labor-intensive to collect. Synthetic data offers acost-effective alternative, yet its potential remains largely underexplored. Tobridge this gap, we explore the feasibility of training Vision-Language-Actionmodels entirely with large-scale synthetic action data. We curate SynGrasp-1B,a billion-frame robotic grasping dataset generated in simulation withphotorealistic rendering and extensive domain randomization. Building on this,we present GraspVLA, a VLA model pretrained on large-scale synthetic actiondata as a foundational model for grasping tasks. GraspVLA integratesautoregressive perception tasks and flow-matching-based action generation intoa unified Chain-of-Thought process, enabling joint training on synthetic actiondata and Internet semantics data. This design helps mitigate sim-to-real gapsand facilitates the transfer of learned actions to a broader range ofInternet-covered objects, achieving open-vocabulary generalization in grasping.Extensive evaluations across real-world and simulation benchmarks demonstrateGraspVLA's advanced zero-shot generalizability and few-shot adaptability tospecific human preferences. We will release SynGrasp-1B dataset and pre-trainedweights to benefit the community.</description>
      <author>example@mail.com (Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2505.03233v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis</title>
      <link>http://arxiv.org/abs/2505.03132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了VISLIX，一个用于视觉分析的新框架，旨在帮助领域专家分析计算机视觉模型中的数据切片，以支持机器学习操作的生命周期。&lt;h4&gt;背景&lt;/h4&gt;在实际部署机器学习模型之前，特别是在自动驾驶和监控等安全关键领域，需要对模型进行严格的评估。数据切片是评估机器学习模型常用的方法，但存在一些挑战。&lt;h4&gt;目的&lt;/h4&gt;克服数据切片在视觉模型验证中的挑战，并支持机器学习操作的生命周期。&lt;h4&gt;方法&lt;/h4&gt;VISLIX使用最先进的基座模型，无需图像元数据或视觉概念，自动生成自然语言洞察，并允许用户交互式地测试数据切片假设。&lt;h4&gt;主要发现&lt;/h4&gt;VISLIX通过专家研究和三个用例评估，证明了该工具在为验证目标检测模型提供全面洞察方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;VISLIX是一个有效的工具，可以帮助领域专家在计算机视觉模型中分析数据切片，从而提高机器学习模型在安全关键领域的评估效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world machine learning models require rigorous evaluation beforedeployment, especially in safety-critical domains like autonomous driving andsurveillance. The evaluation of machine learning models often focuses on dataslices, which are subsets of the data that share a set of characteristics. Dataslice finding automatically identifies conditions or data subgroups wheremodels underperform, aiding developers in mitigating performance issues.Despite its popularity and effectiveness, data slicing for vision modelvalidation faces several challenges. First, data slicing often needs additionalimage metadata or visual concepts, and falls short in certain computer visiontasks, such as object detection. Second, understanding data slices is alabor-intensive and mentally demanding process that heavily relies on theexpert's domain knowledge. Third, data slicing lacks a human-in-the-loopsolution that allows experts to form hypothesis and test them interactively. Toovercome these limitations and better support the machine learning operationslifecycle, we introduce VISLIX, a novel visual analytics framework that employsstate-of-the-art foundation models to help domain experts analyze slices incomputer vision models. Our approach does not require image metadata or visualconcepts, automatically generates natural language insights, and allows usersto test data slice hypothesis interactively. We evaluate VISLIX with an expertstudy and three use cases, that demonstrate the effectiveness of our tool inproviding comprehensive insights for validating object detection models.</description>
      <author>example@mail.com (Xinyuan Yan, Xiwei Xuan, Jorge Piazentin Ono, Jiajing Guo, Vikram Mohanty, Shekar Arvind Kumar, Liang Gou, Bei Wang, Liu Ren)</author>
      <guid isPermaLink="false">2505.03132v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion</title>
      <link>http://arxiv.org/abs/2505.03118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种自适应阈值机制的多标签分类方法，该方法在重类别不平衡和噪声条件下预测多个标签，并实现了在AmazonCat-13K基准测试中的显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;多标签分类（MLC）通常需要在重类别不平衡和噪声条件下预测多个标签，传统方法忽略了上下文和全局稀有性。&lt;h4&gt;目的&lt;/h4&gt;提高多标签分类在重类别不平衡和噪声条件下的预测准确率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种自适应阈值机制，融合了全局（基于IDF）和局部（基于KNN）信号来产生每个标签、每个实例的阈值。这些阈值被处理为损失函数中的可微分惩罚，提供平滑的监督和更好的校准。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在AmazonCat-13K基准测试中实现了0.1712的宏观F1值，显著优于基于树和预训练转换器的传统方法。&lt;h4&gt;结论&lt;/h4&gt;该方法具有轻量级、可解释性和高度模块化等特点，并且代码已公开发布以确保可重复性和未来的扩展。&lt;h4&gt;翻译&lt;/h4&gt;We introduce an adaptive thresholding mechanism for multi-label classification (MLC) that predicts multiple labels per sample, often under heavy class imbalance and noisy conditions. Traditional approaches apply fixed thresholds or treat labels independently, overlooking context and global rarity. We introduce an adaptive thresholding mechanism that fuses global (IDF-based) and local (KNN-based) signals to produce per-label, per-instance thresholds. Instead of applying these as hard cutoffs, we treat them as differentiable penalties in the loss, providing smooth supervision and better calibration. Our architecture is lightweight, interpretable, and highly modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712, substantially outperforming tree-based and pretrained transformer-based methods. We release full code for reproducibility and future extensions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-label classification (MLC) requires predicting multiple labels persample, often under heavy class imbalance and noisy conditions. Traditionalapproaches apply fixed thresholds or treat labels independently, overlookingcontext and global rarity. We introduce an adaptive thresholding mechanism thatfuses global (IDF-based) and local (KNN-based) signals to produce per-label,per-instance thresholds. Instead of applying these as hard cutoffs, we treatthem as differentiable penalties in the loss, providing smooth supervision andbetter calibration. Our architecture is lightweight, interpretable, and highlymodular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,substantially outperforming tree-based and pretrained transformer-basedmethods. We release full code for reproducibility and future extensions.</description>
      <author>example@mail.com (Dmytro Shamatrin)</author>
      <guid isPermaLink="false">2505.03118v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs</title>
      <link>http://arxiv.org/abs/2505.03112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合传统信号处理技术与大型语言模型（LLMs）的创新框架，用于自动调制分类（AMC），以应对信号干扰和噪声的复杂交互。&lt;h4&gt;背景&lt;/h4&gt;AMC对于高效频谱管理和稳健的无线通信至关重要，但由于信号干扰和噪声的复杂交互，AMC仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合传统信号处理技术与LLMs的创新框架，以解决AMC的挑战。&lt;h4&gt;方法&lt;/h4&gt;该方法利用高阶统计和累积量估计将定量信号特征转换为结构化自然语言提示，并通过将示例上下文纳入这些提示中，利用LLM对经典信号处理的熟悉程度，实现有效的单次分类，无需额外的训练或预处理。&lt;h4&gt;主要发现&lt;/h4&gt;在包括无噪声和噪声条件在内的合成数据集上的实验评估表明，该框架在多种调制方案和信噪比（SNR）下实现了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法为在不同信道条件下的无线通信中的稳健基础模型铺平了道路，显著降低了开发特定信道模型的开支，为下一代无线网络中的可扩展、可解释和通用的信号分类系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;自动调制分类（AMC）对于高效频谱管理和稳健的无线通信至关重要。然而，由于信号干扰和噪声的复杂交互，AMC仍然具有挑战性。在本研究中，我们提出了一种将传统信号处理技术与大型语言模型（LLMs）相结合的创新框架，以应对AMC的挑战。我们的方法利用高阶统计和累积量估计将定量信号特征转换为结构化自然语言提示。通过将这些提示中的示例上下文纳入，我们的方法利用了LLM对经典信号处理的内在熟悉程度，从而实现有效的单次分类，无需额外的训练或预处理（例如去噪）。在包括无噪声和噪声条件在内的合成数据集上的实验评估表明，我们的框架在多种调制方案和信噪比（SNR）下实现了具有竞争力的性能。此外，我们的方法为在不同信道条件下的无线通信中的稳健基础模型铺平了道路，显著降低了开发特定信道模型的开支。这项工作为下一代无线网络中的可扩展、可解释和通用的信号分类系统奠定了基础。源代码可在https://github.com/RU-SIT/context-is-king找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic Modulation Classification (AMC) is critical for efficient spectrummanagement and robust wireless communications. However, AMC remains challengingdue to the complex interplay of signal interference and noise. In this work, wepropose an innovative framework that integrates traditional signal processingtechniques with Large-Language Models (LLMs) to address AMC. Our approachleverages higher-order statistics and cumulant estimation to convertquantitative signal features into structured natural language prompts. Byincorporating exemplar contexts into these prompts, our method exploits theLLM's inherent familiarity with classical signal processing, enabling effectiveone-shot classification without additional training or preprocessing (e.g.,denoising). Experimental evaluations on synthetically generated datasets,spanning both noiseless and noisy conditions, demonstrate that our frameworkachieves competitive performance across diverse modulation schemes andSignal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robustfoundation models in wireless communications across varying channel conditions,significantly reducing the expense associated with developing channel-specificmodels. This work lays the foundation for scalable, interpretable, andversatile signal classification systems in next-generation wireless networks.The source code is available at https://github.com/RU-SIT/context-is-king</description>
      <author>example@mail.com (Mohammad Rostami, Atik Faysal, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Yu-Dong Yao)</author>
      <guid isPermaLink="false">2505.03112v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning</title>
      <link>http://arxiv.org/abs/2505.03035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MORE的新方法，用于增强语言模型解决零样本移动操作规划问题的能力，以应对场景动态、未知区域和错误恢复等挑战。&lt;h4&gt;背景&lt;/h4&gt;自主长时域移动操作涉及众多挑战，包括场景动态、未知区域和错误恢复。现有的基于基础模型的方法在处理大量对象和大规模环境时性能下降。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，提高移动操作规划的能力。&lt;h4&gt;方法&lt;/h4&gt;MORE利用场景图来表示环境，结合实例区分，并引入了一种主动过滤方案，提取与任务相关的对象和区域实例的子图，从而将规划问题限定在有限范围内，有效减轻幻觉并提高可靠性。此外，还引入了几个增强功能，以实现室内和室外环境的规划。&lt;h4&gt;主要发现&lt;/h4&gt;在BEHAVIOR-1K基准测试的81个多样化的重组任务中，MORE成为第一个成功解决基准测试中很大一部分问题的方法，并优于基于基础模型的方法。此外，该方法在模拟日常活动的几个复杂真实任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;MORE是一种有效的移动操作规划方法，能够处理多种挑战，并在实际应用中显示出良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Autonomous long-horizon mobile manipulation encompasses a multitude of challenges, including scene dynamics, unexplored areas, and error recovery. Recent works have leveraged foundation models for scene-level robotic reasoning and planning. However, the performance of these methods degrades when dealing with a large number of objects and large-scale environments. To address these limitations, we propose MORE, a novel approach for enhancing the capabilities of language models to solve zero-shot mobile manipulation planning for rearrangement tasks. MORE leverages scene graphs to represent environments, incorporates instance differentiation, and introduces an active filtering scheme that extracts task-relevant subgraphs of object and region instances. These steps yield a bounded planning problem, effectively mitigating hallucinations and improving reliability. Additionally, we introduce several enhancements that enable planning across both indoor and outdoor environments. We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K benchmark, where it becomes the first approach to successfully solve a significant share of the benchmark, outperforming recent foundation model-based approaches. Furthermore, we demonstrate the capabilities of our approach in several complex real-world tasks, mimicking everyday activities. We make the code publicly available at https://more-model.cs.uni-freiburg.de.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous long-horizon mobile manipulation encompasses a multitude ofchallenges, including scene dynamics, unexplored areas, and error recovery.Recent works have leveraged foundation models for scene-level robotic reasoningand planning. However, the performance of these methods degrades when dealingwith a large number of objects and large-scale environments. To address theselimitations, we propose MORE, a novel approach for enhancing the capabilitiesof language models to solve zero-shot mobile manipulation planning forrearrangement tasks. MORE leverages scene graphs to represent environments,incorporates instance differentiation, and introduces an active filteringscheme that extracts task-relevant subgraphs of object and region instances.These steps yield a bounded planning problem, effectively mitigatinghallucinations and improving reliability. Additionally, we introduce severalenhancements that enable planning across both indoor and outdoor environments.We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1Kbenchmark, where it becomes the first approach to successfully solve asignificant share of the benchmark, outperforming recent foundation model-basedapproaches. Furthermore, we demonstrate the capabilities of our approach inseveral complex real-world tasks, mimicking everyday activities. We make thecode publicly available at https://more-model.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Mohammad Mohammadi, Daniel Honerkamp, Martin Büchner, Matteo Cassinelli, Tim Welschehold, Fabien Despinoy, Igor Gilitschenski, Abhinav Valada)</author>
      <guid isPermaLink="false">2505.03035v1</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>DyTTP: Trajectory Prediction with Normalization-Free Transformers</title>
      <link>http://arxiv.org/abs/2504.05356v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于提高自动驾驶系统中轨迹预测准确性的方法，该方法结合了动态Tanh（DyT）和快照集成策略，以解决传统Transformer架构中正常化层带来的计算开销和训练不稳定问题。&lt;h4&gt;背景&lt;/h4&gt;准确的轨迹预测对自动驾驶系统的安全运行至关重要，而理解周围动态环境的行为是关键。Transformer架构在捕捉复杂的时空依赖关系方面展现出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;提高轨迹预测的准确性、推理速度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 将DyT集成到Transformer架构中，替换传统的层归一化，简化网络架构并提高推理稳定性。2. 采用快照集成策略，通过循环学习率调度在单个训练运行中捕获多个模型快照，并在推理时通过简单平均聚合这些快照。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse数据集上的实验表明，该方法显著提高了预测准确性、推理速度和在不同驾驶场景中的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;无归一化的Transformer设计结合轻量级集成技术在推进自动驾驶车辆轨迹预测方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;精准的轨迹预测是自动驾驶系统安全运行的基础，理解周围环境的动态行为至关重要。基于Transformer的架构在捕捉复杂的时空依赖关系方面显示出巨大的潜力。然而，它们对归一化层的依赖可能导致计算开销和训练不稳定。在本研究中，我们提出了一种两阶段的方法来应对这些挑战。首先，我们将最新的促进Transformer的方法——动态Tanh（DyT）集成到主干网络中，取代了传统的层归一化。这种修改简化了网络架构并提高了推理的稳定性。我们是第一个将DyT应用于轨迹预测任务的工作。作为补充，我们采用了快照集成策略，进一步提升了轨迹预测性能。使用循环学习率调度，在单个训练运行中捕获多个模型快照。这些快照在推理时通过简单平均进行聚合，使得模型能够从多种假设中受益，而不会产生额外的计算成本。在Argoverse数据集上的大量实验表明，我们的结合方法显著提高了预测准确性、推理速度和在不同驾驶场景中的鲁棒性。这项工作强调了在推进自动驾驶车辆轨迹预测方面，无归一化Transformer设计结合轻量级集成技术的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate trajectory prediction is a cornerstone for the safe operation ofautonomous driving systems, where understanding the dynamic behavior ofsurrounding agents is crucial. Transformer-based architectures havedemonstrated significant promise in capturing complex spatio-temporalitydependencies. However, their reliance on normalization layers can lead tocomputation overhead and training instabilities. In this work, we present atwo-fold approach to address these challenges. First, we integrate DynamicTanh(DyT), which is the latest method to promote transformers, into the backbone,replacing traditional layer normalization. This modification simplifies thenetwork architecture and improves the stability of the inference. We are thefirst work to deploy the DyT to the trajectory prediction task. Complementingthis, we employ a snapshot ensemble strategy to further boost trajectoryprediction performance. Using cyclical learning rate scheduling, multiple modelsnapshots are captured during a single training run. These snapshots are thenaggregated via simple averaging at inference time, allowing the model tobenefit from diverse hypotheses without incurring substantial additionalcomputational cost. Extensive experiments on Argoverse datasets demonstratethat our combined approach significantly improves prediction accuracy,inference speed and robustness in diverse driving scenarios. This workunderscores the potential of normalization-free transformer designs augmentedwith lightweight ensemble techniques in advancing trajectory forecasting forautonomous vehicles.</description>
      <author>example@mail.com (JianLin Zhu, HongKuo Niu)</author>
      <guid isPermaLink="false">2504.05356v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2504.21254v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为ABG-NAS的自动图神经网络架构搜索框架，旨在解决现有图神经网络在适应复杂图结构时的局限性，从而提高图表示学习的效率和效果。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）在节点分类、链接预测和子图搜索等下游任务中至关重要，但现有的GNN架构往往难以适应不同的复杂图结构，限制了其生成结构感知和任务判别性表示的能力。&lt;h4&gt;目的&lt;/h4&gt;提出ABG-NAS框架，以实现高效的图表示学习。&lt;h4&gt;方法&lt;/h4&gt;ABG-NAS包括三个关键组件：全面架构搜索空间（CASS）、自适应遗传优化策略（AGOS）和贝叶斯引导调优模块（BGTM）。CASS探索多样化的传播和转换操作，AGOS动态平衡探索和利用，BGTM定期优化超参数。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集（Cora、PubMed、Citeseer和CoraFull）上的实验表明，ABG-NAS在性能上优于手动设计的GNN和最先进的神经架构搜索（NAS）方法。&lt;h4&gt;结论&lt;/h4&gt;ABG-NAS具有提供可扩展和自适应解决方案的潜力，可以推动图表示学习的发展。&lt;h4&gt;翻译&lt;/h4&gt;Effective and efficient graph representation learning is essential for enabling critical downstream tasks, such as node classification, link prediction, and subgraph search. However, existing graph neural network (GNN) architectures often struggle to adapt to diverse and complex graph structures, limiting their ability to produce structure-aware and task-discriminative representations. To address this challenge, we propose ABG-NAS, a novel framework for automated graph neural network architecture search tailored for efficient graph representation learning. ABG-NAS encompasses three key components: a Comprehensive Architecture Search Space (CASS), an Adaptive Genetic Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS systematically explores diverse propagation (P) and transformation (T) operations, enabling the discovery of GNN architectures capable of capturing intricate graph characteristics. AGOS dynamically balances exploration and exploitation, ensuring search efficiency and preserving solution diversity. BGTM further optimizes hyperparameters periodically, enhancing the scalability and robustness of the resulting architectures. Empirical evaluations on benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that ABG-NAS consistently outperforms both manually designed GNNs and state-of-the-art neural architecture search (NAS) methods. These results highlight the potential of ABG-NAS to advance graph representation learning by providing scalable and adaptive solutions for diverse graph structures. Our code is publicly available at https://github.com/sserranw/ABG-NAS.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective and efficient graph representation learning is essential forenabling critical downstream tasks, such as node classification, linkprediction, and subgraph search. However, existing graph neural network (GNN)architectures often struggle to adapt to diverse and complex graph structures,limiting their ability to produce structure-aware and task-discriminativerepresentations. To address this challenge, we propose ABG-NAS, a novelframework for automated graph neural network architecture search tailored forefficient graph representation learning. ABG-NAS encompasses three keycomponents: a Comprehensive Architecture Search Space (CASS), an AdaptiveGenetic Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module(BGTM). CASS systematically explores diverse propagation (P) and transformation(T) operations, enabling the discovery of GNN architectures capable ofcapturing intricate graph characteristics. AGOS dynamically balancesexploration and exploitation, ensuring search efficiency and preservingsolution diversity. BGTM further optimizes hyperparameters periodically,enhancing the scalability and robustness of the resulting architectures.Empirical evaluations on benchmark datasets (Cora, PubMed, Citeseer, andCoraFull) demonstrate that ABG-NAS consistently outperforms both manuallydesigned GNNs and state-of-the-art neural architecture search (NAS) methods.These results highlight the potential of ABG-NAS to advance graphrepresentation learning by providing scalable and adaptive solutions fordiverse graph structures. Our code is publicly available athttps://github.com/sserranw/ABG-NAS.</description>
      <author>example@mail.com (Sixuan Wang, Jiao Yin, Jinli Cao, MingJian Tang, Hua Wang, Yanchun Zhang)</author>
      <guid isPermaLink="false">2504.21254v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID</title>
      <link>http://arxiv.org/abs/2504.19244v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SALCR的无监督可见光-红外行人重识别框架，旨在通过优化特定细粒度模式，实现不同模态标签分布的互补对齐。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通过标签关联算法统一跨模态图像的伪标签，并设计对比学习框架进行全局特征学习，但忽略了特征表示和伪标签分布中的跨模态变化。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中由于仅优化全局特征而导致的模态共享学习不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出SALCR框架，包括DAGI模块统一跨模态实例的伪标签，FGSAL模块探索跨模态实例中各模态强调的部分级语义对齐模式，以及GPCR模块动态挖掘可靠的正样本集以优化实例间关系。&lt;h4&gt;主要发现&lt;/h4&gt;SALCR框架通过强调细粒度模式，实现了不同模态标签分布的互补对齐，并通过优化实例间关系来减轻噪声伪标签的副作用。&lt;h4&gt;结论&lt;/h4&gt;实验表明，SALCR方法在性能上优于现有方法，并提供了可用的代码。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework for unsupervised visible-infrared person re-identification, which aims to achieve complementary alignment between the label distributions of different modalities by optimizing specific fine-grained patterns. The existing methods unify pseudo-labels of cross-modality images through label association algorithms and design contrastive learning frameworks for global feature learning, but overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. To address this issue, the SALCR framework is proposed, which includes a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bidirectional manner, a Fine-Grained Semantic-Aligned Learning (FGSAL) module to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances, and a Global-Part Collaborative Refinement (GPCR) module to dynamically mine reliable positive sample sets for the global and part features and optimize the inter-instance relationships. Extensive experiments demonstrate that the proposed method achieves superior performance compared to existing methods, and the code is available.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks tomatch pedestrian images of the same individual across different modalitieswithout human annotations for model learning. Previous methods unifypseudo-labels of cross-modality images through label association algorithms andthen design contrastive learning framework for global feature learning.However, these methods overlook the cross-modality variations in featurerepresentation and pseudo-label distributions brought by fine-grained patterns.This insight results in insufficient modality-shared learning when only globalfeatures are optimized. To address this issue, we propose a Semantic-AlignedLearning with Collaborative Refinement (SALCR) framework, which builds upoptimization objective for specific fine-grained patterns emphasized by eachmodality, thereby achieving complementary alignment between the labeldistributions of different modalities. Specifically, we first introduce a DualAssociation with Global Learning (DAGI) module to unify the pseudo-labels ofcross-modality instances in a bi-directional manner. Afterward, a Fine-GrainedSemantic-Aligned Learning (FGSAL) module is carried out to explore part-levelsemantic-aligned patterns emphasized by each modality from cross-modalityinstances. Optimization objective is then formulated based on thesemantic-aligned features and their corresponding label space. To alleviate theside-effects arising from noisy pseudo-labels, we propose a Global-PartCollaborative Refinement (GPCR) module to mine reliable positive sample setsfor the global and part features dynamically and optimize the inter-instancerelationships. Extensive experiments demonstrate the effectiveness of theproposed method, which achieves superior performances to state-of-the-artmethods. Our code is available at\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</description>
      <author>example@mail.com (De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao)</author>
      <guid isPermaLink="false">2504.19244v2</guid>
      <pubDate>Wed, 07 May 2025 14:15:45 +0800</pubDate>
    </item>
    <item>
      <title>DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion</title>
      <link>http://arxiv.org/abs/2505.01857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DualDiff的双重分支条件扩散模型，用于增强多视角驾驶场景生成，旨在提高场景重建的准确性和高保真度。&lt;h4&gt;背景&lt;/h4&gt;现有的驾驶场景重建方法主要依赖3D边界框和前景背景的二值图，这些方法无法充分捕捉场景的复杂性并整合多模态信息。&lt;h4&gt;目的&lt;/h4&gt;提高场景重建的准确性和高保真度，增强多视角驾驶场景生成。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种名为Occupancy Ray Sampling (ORS)的语义丰富的3D表示方法，以实现全面的前景和背景控制。2. 设计了语义融合注意力（SFA）机制，以改善跨模态信息整合。3. 设计了前景感知掩码（FGM）损失，以增强小对象的生成。&lt;h4&gt;主要发现&lt;/h4&gt;DualDiff在FID评分中达到了最先进的性能，并在下游的BEV分割和3D目标检测任务中取得了持续的良好结果。&lt;h4&gt;结论&lt;/h4&gt;DualDiff模型在驾驶场景重建方面具有显著优势，能够有效提升重建的准确性和细节表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and high-fidelity driving scene reconstruction relies on fullyleveraging scene information as conditioning. However, existing approaches,which primarily use 3D bounding boxes and binary maps for foreground andbackground control, fall short in capturing the complexity of the scene andintegrating multi-modal information. In this paper, we propose DualDiff, adual-branch conditional diffusion model designed to enhance multi-view drivingscene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3Drepresentation, alongside numerical driving scene representation, forcomprehensive foreground and background control. To improve cross-modalinformation integration, we propose a Semantic Fusion Attention (SFA) mechanismthat aligns and fuses features across modalities. Furthermore, we design aforeground-aware masked (FGM) loss to enhance the generation of tiny objects.DualDiff achieves state-of-the-art performance in FID score, as well asconsistently better results in downstream BEV segmentation and 3D objectdetection tasks.</description>
      <author>example@mail.com (Haoteng Li, Zhao Yang, Zezhong Qian, Gongpeng Zhao, Yuqi Huang, Jun Yu, Huazheng Zhou, Longjun Liu)</author>
      <guid isPermaLink="false">2505.01857v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
  <item>
      <title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
      <link>http://arxiv.org/abs/2505.02831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Self-Representation Alignment for Diffusion Transformers. arXiv admin  note: text overlap with arXiv:2410.06940 by other authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了Self-Representation Alignment（SRA）方法，通过自我蒸馏的方式，在仅生成训练过程中，提升扩散变换器的内部表示学习，从而加速生成训练并提高生成质量。&lt;h4&gt;背景&lt;/h4&gt;现有方法要么需要引入额外的复杂表示训练框架，要么依赖大规模预训练的表示基础模型来提供表示指导。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要外部表示组件的简单方法，利用扩散变换器自身的独特判别过程来提供表示指导。&lt;h4&gt;方法&lt;/h4&gt;SRA方法通过将扩散变换器早期层输出高噪声的潜在表示与后期层低噪声的潜在表示进行对齐，以逐步增强仅在生成训练过程中的整体表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，将SRA应用于DiTs和SiTs可以获得一致的性能提升。SRA不仅显著优于依赖于辅助的复杂表示训练框架的方法，而且达到了依赖于强大外部表示先验的方法的性能。&lt;h4&gt;结论&lt;/h4&gt;SRA方法能够有效提升扩散变换器的生成训练速度和生成质量，是一种简单且有效的内部表示学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vvvvvjdy/SRA&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated that learning a meaningful internalrepresentation can both accelerate generative training and enhance generationquality of the diffusion transformers. However, existing approaches necessitateto either introduce an additional and complex representation training frameworkor rely on a large-scale, pre-trained representation foundation model toprovide representation guidance during the original generative trainingprocess. In this study, we posit that the unique discriminative processinherent to diffusion transformers enables them to offer such guidance withoutrequiring external representation components. We therefore proposeSelf-Representation A}lignment (SRA), a simple yet straightforward method thatobtain representation guidance through a self-distillation manner.Specifically, SRA aligns the output latent representation of the diffusiontransformer in earlier layer with higher noise to that in later layer withlower noise to progressively enhance the overall representation learning duringonly generative training process. Experimental results indicate that applyingSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRAnot only significantly outperforms approaches relying on auxiliary, complexrepresentation training frameworks but also achieves performance comparable tomethods that heavily dependent on powerful external representation priors.</description>
      <author>example@mail.com (Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang)</author>
      <guid isPermaLink="false">2505.02831v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter</title>
      <link>http://arxiv.org/abs/2505.02664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 Pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Grasp the Graph 2.0（GtG 2.0）方法，这是一种轻量级且高效的机器人抓取框架，通过集成图神经网络从点云数据中进行高效的几何推理。&lt;h4&gt;背景&lt;/h4&gt;在杂乱的真实环境中进行抓取姿态检测是一个重大挑战，因为噪声和不完整的感觉数据与复杂对象几何形状相结合。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理噪声和不完整数据，并适应复杂对象几何形状的抓取姿态检测方法。&lt;h4&gt;方法&lt;/h4&gt;GtG 2.0方法利用传统的抓取姿态生成器高效地生成7自由度的抓取候选者，并通过包含夹爪爪口内点和周围环境点的集成图神经网络模型对这些候选者进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;GtG 2.0在GraspNet-1Billion基准测试中，与基于假设和测试以及基于图神经网络的方法相比，平均精度提高了35%，并排名前三。&lt;h4&gt;结论&lt;/h4&gt;GtG 2.0在3自由度Delta并联机器人和Kinect-v1摄像头上的实验中，成功率达到91%，杂乱完成率为100%，证明了其灵活性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在杂乱的真实环境中进行抓取姿态检测仍然是一个重大挑战，因为噪声和不完整的感觉数据与复杂对象几何形状相结合。本文介绍了Grasp the Graph 2.0（GtG 2.0）方法，这是一种轻量级且高效的机器人抓取框架，通过集成图神经网络从点云数据中进行高效的几何推理。基于GtG 1.0的成功，它证明了图神经网络在抓取检测中的潜力，但受限于完整、无噪声的点云和4自由度抓取的假设，GtG 2.0采用传统的抓取姿态生成器来高效地生成7自由度的抓取候选者。候选者通过包含夹爪爪口内点和周围环境点的集成图神经网络模型进行评估。这种改进的表示提高了抓取检测性能，与使用相同生成器的先前方法相比，GtG 2.0在GraspNet-1Billion基准测试中的平均精度提高了35%，并排名前三。使用3自由度Delta并联机器人和Kinect-v1摄像头进行的实验表明，成功率为91%，杂乱完成率为100%，证明了其灵活性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasp pose detection in cluttered, real-world environments remains asignificant challenge due to noisy and incomplete sensory data combined withcomplex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)method, a lightweight yet highly effective hypothesis-and-test roboticsgrasping framework which leverages an ensemble of Graph Neural Networks forefficient geometric reasoning from point cloud data. Building on the success ofGtG 1.0, which demonstrated the potential of Graph Neural Networks for graspdetection but was limited by assumptions of complete, noise-free point cloudsand 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator toefficiently produce 7-Dof grasp candidates. Candidates are assessed with anensemble Graph Neural Network model which includes points within the gripperjaws (inside points) and surrounding contextual points (outside points). Thisimproved representation boosts grasp detection performance over previousmethods using the same generator. GtG 2.0 shows up to a 35% improvement inAverage Precision on the GraspNet-1Billion benchmark compared tohypothesis-and-test and Graph Neural Network-based methods, ranking it amongthe top three frameworks. Experiments with a 3-Dof Delta Parallel robot andKinect-v1 camera show a success rate of 91% and a clutter completion rate of100%, demonstrating its flexibility and reliability.</description>
      <author>example@mail.com (Ali Rashidi Moghadam, Sayedmohammadreza Rastegari, Mehdi Tale Masouleh, Ahmad Kalhor)</author>
      <guid isPermaLink="false">2505.02664v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration</title>
      <link>http://arxiv.org/abs/2505.02787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的无监督描述符学习方法，用于眼底图像配准，无需依赖关键点检测，并在多个测试中展示了其准确性和性能。&lt;h4&gt;背景&lt;/h4&gt;当前眼底图像配准方法受限于缺乏标记数据，尤其在医疗领域更为显著，这促使了无监督学习技术的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖关键点检测的无监督描述符学习方法，以实现眼底图像配准。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的无监督描述符学习方法，并在公共眼底图像注册数据集上进行了广泛和全面的比较。同时，测试了多种不同类型的关键点检测器，并提出了新的检测器。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在注册精度上不亚于监督方法，且在不同关键点检测器下均表现出准确性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作在医疗领域利用无监督学习方面迈出了重要一步。&lt;h4&gt;翻译&lt;/h4&gt;Current color fundus image registration approaches are limited, among other things, by the lack of labeled data, which is even more significant in the medical domain, motivating the use of unsupervised learning. Therefore, in this work, we develop a novel unsupervised descriptor learning method that does not rely on keypoint detection. This enables the resulting descriptor network to be agnostic to the keypoint detector used during the registration inference. To validate this approach, we perform an extensive and comprehensive comparison on the reference public retinal image registration dataset. Additionally, we test our method with multiple keypoint detectors of varied nature, even proposing some novel ones. Our results demonstrate that the proposed approach offers accurate registration, not incurring in any performance loss versus supervised methods. Additionally, it demonstrates accurate performance regardless of the keypoint detector used. Thus, this work represents a notable step towards leveraging unsupervised learning in the medical domain.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current color fundus image registration approaches are limited, among otherthings, by the lack of labeled data, which is even more significant in themedical domain, motivating the use of unsupervised learning. Therefore, in thiswork, we develop a novel unsupervised descriptor learning method that does notrely on keypoint detection. This enables the resulting descriptor network to beagnostic to the keypoint detector used during the registration inference.  To validate this approach, we perform an extensive and comprehensivecomparison on the reference public retinal image registration dataset.Additionally, we test our method with multiple keypoint detectors of variednature, even proposing some novel ones. Our results demonstrate that theproposed approach offers accurate registration, not incurring in anyperformance loss versus supervised methods. Additionally, it demonstratesaccurate performance regardless of the keypoint detector used. Thus, this workrepresents a notable step towards leveraging unsupervised learning in themedical domain.</description>
      <author>example@mail.com (David Rivas-Villar, Álvaro S. Hervella, José Rouco, Jorge Novo)</author>
      <guid isPermaLink="false">2505.02787v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.02393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为IEF-VAD的视频异常检测框架，通过融合图像和事件表示来提高检测精度。&lt;h4&gt;背景&lt;/h4&gt;现有的视频异常检测器主要依赖于RGB帧，这限制了捕捉突发事件中的快速或短暂运动。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架，该框架能够从RGB视频中直接合成事件表示，并通过不确定性感知的过程与图像特征融合。&lt;h4&gt;方法&lt;/h4&gt;IEF-VAD框架包括：(i) 使用Student-t分布来建模传感器噪声，并通过Laplace近似得到价值级别的逆方差权重；(ii) 应用类似Kalman的帧级更新来平衡不同模态随时间的变化；(iii) 通过迭代细化融合的潜在状态来消除残存的跨模态噪声。&lt;h4&gt;主要发现&lt;/h4&gt;IEF-VAD在多个真实世界的异常检测基准测试中达到了新的水平，并且无需专用的事件传感器或帧级标签。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了合成事件表示在强调通常在RGB帧中未被充分表示的运动线索中的效用，这使得在不同应用中实现准确和鲁棒的视频理解成为可能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数现有的视频异常检测器仅依赖于RGB帧，这缺乏捕捉突变或短暂运动线索所需的时间分辨率，而这些线索是异常事件的关键指标。为了解决这一限制，我们提出了用于视频异常检测的图像-事件融合（IEF-VAD）框架，该框架直接从RGB视频中合成事件表示，并通过一种原则性、不确定性感知的过程与图像特征融合。该系统（i）使用Student-t分布来建模传感器噪声，通过Laplace近似推导出价值级别的逆方差权重；（ii）应用类似Kalman的帧级更新来平衡模态随时间的变化；（iii）迭代细化融合的潜在状态，以消除残留的跨模态噪声。无需专用的事件传感器或帧级标签，IEF-VAD在多个真实世界的异常检测基准测试中达到了新的水平。这些发现突出了合成事件表示在强调通常在RGB帧中未被充分表示的运动线索中的效用，使得在不同应用中实现准确和鲁棒的视频理解成为可能。代码和模型可在https://github.com/EavnJeong/IEF-VAD上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing video anomaly detectors rely solely on RGB frames, which lackthe temporal resolution needed to capture abrupt or transient motion cues, keyindicators of anomalous events. To address this limitation, we proposeImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework thatsynthesizes event representations directly from RGB videos and fuses them withimage features through a principled, uncertainty-aware process. The system (i)models heavy-tailed sensor noise with a Student`s-t likelihood, derivingvalue-level inverse-variance weights via a Laplace approximation; (ii) appliesKalman-style frame-wise updates to balance modalities over time; and (iii)iteratively refines the fused latent state to erase residual cross-modal noise.Without any dedicated event sensor or frame-level labels, IEF-VAD sets a newstate of the art across multiple real-world anomaly detection benchmarks. Thesefindings highlight the utility of synthetic event representations inemphasizing motion cues that are often underrepresented in RGB frames, enablingaccurate and robust video understanding across diverse applications withoutrequiring dedicated event sensors. Code and models are available athttps://github.com/EavnJeong/IEF-VAD.</description>
      <author>example@mail.com (Sungheon Jeong, Jihong Park, Mohsen Imani)</author>
      <guid isPermaLink="false">2505.02393v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Lidar Point Cloud Sampling via Colorization and Super-Resolution of Lidar Imagery</title>
      <link>http://arxiv.org/abs/2505.02049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages. arXiv admin note: substantial text overlap with  arXiv:2409.11532&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要介绍了激光雷达技术的最新进展，特别是在点云分辨率提高和生成360度低分辨率图像方面的突破。这些图像的应用使深度学习技术能够在激光雷达系统中替代传统方法，提高了在恶劣环境下的鲁棒性，并解决了点云几何信息退化的问题。&lt;h4&gt;背景&lt;/h4&gt;背景提到了激光雷达技术的进步，以及由此带来的点云分辨率提升和生成360度低分辨率图像的能力。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出一个新颖的框架，利用基于深度学习的彩色化和超分辨率技术在激光雷达图像上提取可靠样本，以提高里程计估计的精度。&lt;h4&gt;方法&lt;/h4&gt;该方法采用深度学习技术对激光雷达图像进行彩色化和超分辨率处理，以提高关键点检测的准确性，进而进行有效的点云降采样，从而提升点云注册精度并减少由于几何信息不足或误点引起的误差。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在翻译和旋转误差方面优于先前的方法，并且在使用更少点的情况下取得了更好的效果。&lt;h4&gt;结论&lt;/h4&gt;结论强调了该研究提出的框架在提高激光雷达里程计估计精度方面的有效性，并指出了其在减少错误和提高数据处理效率方面的优势。&lt;h4&gt;翻译&lt;/h4&gt;Recent advancements in lidar technology have led to improved point cloud resolution as well as the generation of 360 degrees, low-resolution images by encoding depth, reflectivity, or near-infrared light within each pixel. These images enable the application of deep learning (DL) approaches, originally developed for RGB images from cameras to lidar-only systems, eliminating other efforts, such as lidar-camera calibration. Compared with conventional RGB images, lidar imagery demonstrates greater robustness in adverse environmental conditions, such as low light and foggy weather. Moreover, the imaging capability addresses the challenges in environments where the geometric information in point clouds may be degraded, such as long corridors, and dense point clouds may be misleading, potentially leading to drift errors. Therefore, this paper proposes a novel framework that leverages DL-based colorization and super-resolution techniques on lidar imagery to extract reliable samples from lidar point clouds for odometry estimation. The enhanced lidar images, enriched with additional information, facilitate improved keypoint detection, which is subsequently employed for more effective point cloud downsampling. The proposed method enhances point cloud registration accuracy and mitigates mismatches arising from insufficient geometric information or misleading extra points. Experimental results indicate that our approach surpasses previous methods, achieving lower translation and rotation errors while using fewer points.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in lidar technology have led to improved point cloudresolution as well as the generation of 360 degrees, low-resolution images byencoding depth, reflectivity, or near-infrared light within each pixel. Theseimages enable the application of deep learning (DL) approaches, originallydeveloped for RGB images from cameras to lidar-only systems, eliminating otherefforts, such as lidar-camera calibration. Compared with conventional RGBimages, lidar imagery demonstrates greater robustness in adverse environmentalconditions, such as low light and foggy weather. Moreover, the imagingcapability addresses the challenges in environments where the geometricinformation in point clouds may be degraded, such as long corridors, and densepoint clouds may be misleading, potentially leading to drift errors.  Therefore, this paper proposes a novel framework that leverages DL-basedcolorization and super-resolution techniques on lidar imagery to extractreliable samples from lidar point clouds for odometry estimation. The enhancedlidar images, enriched with additional information, facilitate improvedkeypoint detection, which is subsequently employed for more effective pointcloud downsampling. The proposed method enhances point cloud registrationaccuracy and mitigates mismatches arising from insufficient geometricinformation or misleading extra points. Experimental results indicate that ourapproach surpasses previous methods, achieving lower translation and rotationerrors while using fewer points.</description>
      <author>example@mail.com (Sier Ha, Honghao Du, Xianjia Yu, Tomi Westerlund)</author>
      <guid isPermaLink="false">2505.02049v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
      <link>http://arxiv.org/abs/2505.02366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的无监督对比学习方法JTCSE，用于自然语言处理中的文本语义嵌入。&lt;h4&gt;背景&lt;/h4&gt;无监督对比学习在自然语言处理领域受到关注，但现有方法忽略了语义表示张量的模量特征，导致对比学习效果不足。&lt;h4&gt;目的&lt;/h4&gt;旨在增强对比学习中正样本之间的对齐，并优化模型对CLS token的注意力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种训练目标，旨在对语义表示张量施加模量约束，并设计了交叉注意力结构以增强模型对CLS token的注意力。&lt;h4&gt;主要发现&lt;/h4&gt;JTCSE在七个语义文本相似度计算任务中表现出色，其双塔集成模型和单塔蒸馏模型优于其他基线，成为当前SOTA。在超过130个零样本下游任务评估中，JTCSE整体优于其他基线。&lt;h4&gt;结论&lt;/h4&gt;JTCSE通过结合模量约束和交叉注意力机制，在文本语义嵌入方面取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised contrastive learning has become a hot research topic in naturallanguage processing. Existing works usually aim at constraining the orientationdistribution of the representations of positive and negative samples in thehigh-dimensional semantic space in contrastive learning, but the semanticrepresentation tensor possesses both modulus and orientation features, and theexisting works ignore the modulus feature of the representations and causeinsufficient contrastive learning. % Therefore, we firstly propose a trainingobjective that aims at modulus constraints on the semantic representationtensor, to strengthen the alignment between the positive samples in contrastivelearning. Therefore, we first propose a training objective that is designed toimpose modulus constraints on the semantic representation tensor, to strengthenthe alignment between positive samples in contrastive learning. Then, theBERT-like model suffers from the phenomenon of sinking attention, leading to alack of attention to CLS tokens that aggregate semantic information. Inresponse, we propose a cross-attention structure among the twin-tower ensemblemodels to enhance the model's attention to CLS token and optimize the qualityof CLS Pooling. Combining the above two motivations, we propose a new\textbf{J}oint \textbf{T}ensor representation modulus constraint and\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence\textbf{E}mbedding representation framework JTCSE, which we evaluate in sevensemantic text similarity computation tasks, and the experimental results showthat JTCSE's twin-tower ensemble model and single-tower distillation modeloutperform the other baselines and become the current SOTA. In addition, wehave conducted an extensive zero-shot downstream task evaluation, which showsthat JTCSE outperforms other baselines overall on more than 130 tasks.</description>
      <author>example@mail.com (Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu)</author>
      <guid isPermaLink="false">2505.02366v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework</title>
      <link>http://arxiv.org/abs/2505.02712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过深度强化学习控制布尔网络模型，以探索细胞重编程策略，并展示该方法在处理复杂生物系统中的有效性。&lt;h4&gt;背景&lt;/h4&gt;细胞重编程在治疗复杂疾病方面具有潜在的治疗价值，但传统的实验方法耗时且成本高。&lt;h4&gt;目的&lt;/h4&gt;利用深度强化学习来控制布尔网络模型，以发现细胞重编程的策略。&lt;h4&gt;方法&lt;/h4&gt;提出了布尔网络模型在异步更新模式下的新型控制问题，引入了伪吸引子的概念并改进了伪吸引子状态识别的流程，并设计了计算框架来解决控制问题。将图神经网络和图卷积集成到人工神经网络近似器中，以利用生物系统的结构。&lt;h4&gt;主要发现&lt;/h4&gt;在多个来自文献的大规模真实世界生物网络上进行了实验，证明了该方法的可扩展性和有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法在细胞重编程中具有实际应用潜力，并展示了深度强化学习在处理复杂生物系统中的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cellular reprogramming, the artificial transformation of one cell type intoanother, has been attracting increasing research attention due to itstherapeutic potential for complex diseases. However, discovering reprogrammingstrategies through classical wet-lab experiments is hindered by lengthy timecommitments and high costs. In this study, we explore the use of deepreinforcement learning (DRL) to control Boolean network models of complexbiological systems, such as gene regulatory networks and signalling pathwaynetworks. We formulate a novel control problem for Boolean network models underthe asynchronous update mode in the context of cellular reprogramming. Tofacilitate scalability, we consider our previously introduced concept of apseudo-attractor and we improve our procedure for effective identification ofpseudo-attractor states. Finally, we devise a computational framework to solvethe control problem. To leverage the structure of biological systems, weincorporate graph neural networks with graph convolutions into the artificialneural network approximator for the action-value function learned by the DRLagent. Experiments on a number of large real-world biological networks fromliterature demonstrate the scalability and effectiveness of our approach.</description>
      <author>example@mail.com (Andrzej Mizera, Jakub Zarzycki)</author>
      <guid isPermaLink="false">2505.02712v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>fastabx: A library for efficient computation of ABX discriminability</title>
      <link>http://arxiv.org/abs/2505.02692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了名为fastabx的高性能Python库，用于构建ABXdiscrimination任务。&lt;h4&gt;背景&lt;/h4&gt;ABX是一种衡量感兴趣的一般类别之间分离程度的度量，广泛用于评估自监督语音表示中的语音可辨性。然而，由于其缺乏适当的工具，其更广泛的应用受到了限制。&lt;h4&gt;目的&lt;/h4&gt;fastabx通过提供一个能够构建任何类型ABX任务的框架，同时提供快速开发周期所需的效率，来解决这一差距。&lt;h4&gt;方法&lt;/h4&gt;fastabx提供了一种构建ABX任务的方法，并能够计算表示之间的距离。&lt;h4&gt;主要发现&lt;/h4&gt;fastabx将成为更广泛的表示学习社区的有价值资源，使研究人员能够系统地研究可以从学习到的表示中直接提取哪些信息，而不仅限于语音处理领域。&lt;h4&gt;结论&lt;/h4&gt;fastabx的源代码可在https://github.com/bootphon/fastabx上获取。&lt;h4&gt;翻译&lt;/h4&gt;We introduce fastabx, a high-performance Python library for building ABXdiscrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce fastabx, a high-performance Python library for building ABXdiscrimination tasks. ABX is a measure of the separation between genericcategories of interest. It has been used extensively to evaluate phoneticdiscriminability in self-supervised speech representations. However, itsbroader adoption has been limited by the absence of adequate tools. fastabxaddresses this gap by providing a framework capable of constructing any type ofABX task while delivering the efficiency necessary for rapid developmentcycles, both in task creation and in calculating distances betweenrepresentations. We believe that fastabx will serve as a valuable resource forthe broader representation learning community, enabling researchers tosystematically investigate what information can be directly extracted fromlearned representations across several domains beyond speech processing. Thesource code is available at https://github.com/bootphon/fastabx.</description>
      <author>example@mail.com (Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux)</author>
      <guid isPermaLink="false">2505.02692v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation</title>
      <link>http://arxiv.org/abs/2505.02476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print for IEEE IAVVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为点云重组的方法，用于增强现实世界点云数据，以解决智能移动系统在开放世界应用中基于LiDAR感知的验证问题。&lt;h4&gt;背景&lt;/h4&gt;由于真实环境条件的可变性，基于LiDAR的感知验证是一个挑战。虚拟模拟可以生成任意场景，但缺乏物理传感器特性；而真实世界数据提供真实的传感器特性，但难以控制影响因素。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，通过整合实验室环境中测量的物理目标对象的点云，系统地增强捕获的点云场景，以创建大量和多样化的可重复、物理准确的测试场景。&lt;h4&gt;方法&lt;/h4&gt;点云重组方法通过将实验室环境中测量的物理目标对象的点云与真实世界场景中的点云进行整合，从而增强真实世界点云数据。&lt;h4&gt;主要发现&lt;/h4&gt;使用Ouster OS1-128 Rev7传感器，该方法展示了如何将具有不同服装和姿势的人形目标添加到真实世界的城市和乡村场景中，以实现可重复的位置定位。重组的场景与真实传感器的输出非常接近，从而实现了有针对性的测试、可扩展的故障分析和系统安全性的提高。&lt;h4&gt;结论&lt;/h4&gt;通过提供受控且传感器真实的数据，该方法使得对特定传感器及其算法局限性的结论更加可靠，例如物体检测。&lt;h4&gt;翻译&lt;/h4&gt;摘要：由于真实环境条件的变化，基于LiDAR的智能移动系统在开放世界应用中的感知验证仍然是一个挑战。虚拟模拟允许在受控条件下生成任意场景，但缺乏物理传感器特性，如强度响应或材料依赖效应。相比之下，真实世界数据提供了真实的传感器特性，但提供了较少的控制影响因素，阻碍了充分的验证。现有的方法通过在场景之间转移对象来增强真实世界点云数据，但这些问题没有考虑验证，并且由于依赖于经验数据而在可控性方面有限。我们通过提出点云重组来解决这些限制，该方法通过整合在受控实验室环境中测量的物理目标对象的点云，系统地增强捕获的点云场景。因此，能够创建大量和多样化的可重复、物理准确的测试场景，与具有注册3D网格的现象感知遮挡相关。使用Ouster OS1-128 Rev7传感器，我们展示了如何通过添加具有不同服装和姿势的人形目标来增强真实世界的城市和乡村场景，以实现可重复的位置定位。我们表明，重组的场景与真实传感器的输出非常接近，从而实现了有针对性的测试、可扩展的故障分析和系统安全性的提高。通过提供受控但传感器真实的数据，我们的方法使得对特定传感器及其算法局限性的结论更加可靠，例如物体检测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The validation of LiDAR-based perception of intelligent mobile systemsoperating in open-world applications remains a challenge due to the variabilityof real environmental conditions. Virtual simulations allow the generation ofarbitrary scenes under controlled conditions but lack physical sensorcharacteristics, such as intensity responses or material-dependent effects. Incontrast, real-world data offers true sensor realism but provides less controlover influencing factors, hindering sufficient validation. Existing approachesaddress this problem with augmentation of real-world point cloud data bytransferring objects between scenes. However, these methods do not considervalidation and remain limited in controllability because they rely on empiricaldata. We solve these limitations by proposing Point Cloud Recombination, whichsystematically augments captured point cloud scenes by integrating point cloudsacquired from physical target objects measured in controlled laboratoryenvironments. Thus enabling the creation of vast amounts and varieties ofrepeatable, physically accurate test scenes with respect to phenomena-awareocclusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, wedemonstrate the augmentation of real-world urban and rural scenes with humanoidtargets featuring varied clothing and poses, for repeatable positioning. Weshow that the recombined scenes closely match real sensor outputs, enablingtargeted testing, scalable failure analysis, and improved system safety. Byproviding controlled yet sensor-realistic data, our method enables trustworthyconclusions about the limitations of specific sensors in compound with theiralgorithms, e.g., object detection.</description>
      <author>example@mail.com (Hubert Padusinski, Christian Steinhauser, Christian Scherl, Julian Gaal, Jacob Langner)</author>
      <guid isPermaLink="false">2505.02476v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2505.02634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于迁移学习、多目标深度强化学习（DRL）的方法，能够根据气动和结构标准优化任何机翼的几何形状。&lt;h4&gt;背景&lt;/h4&gt;研究背景未在摘要中提及。&lt;h4&gt;目的&lt;/h4&gt;旨在展示该方法，通过最大化升阻比（$C_L/C_D$）同时保持机翼的结构完整性（通过最大厚度建模），并使用不同的迁移学习（TL）策略训练DRL智能体。&lt;h4&gt;方法&lt;/h4&gt;将DRL智能体的性能与粒子群优化（PSO）进行比较，PSO是一种传统的无梯度优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;DRL智能体能够执行多目标形状优化，DRL方法在计算效率和形状优化性能方面优于PSO，迁移学习增强的DRL智能体在性能上与DRL相当，同时节省了大量计算资源。&lt;h4&gt;结论&lt;/h4&gt;DRL方法在多目标形状优化中表现优异，迁移学习可以显著提高DRL的性能并节省计算资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The main objective of this paper is to introduce a transferlearning-enhanced, multi-objective, deep reinforcement learning (DRL)methodology that is able to optimise the geometry of any airfoil based onconcomitant aerodynamic and structural criteria. To showcase the method, we aimto maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structuralintegrity of the airfoil -- as modelled by its maximum thickness -- and trainthe DRL agent using a list of different transfer learning (TL) strategies. Theperformance of the DRL agent is compared with Particle Swarm Optimisation(PSO), a traditional gradient-free optimisation method. Results indicate thatDRL agents are able to perform multi-objective shape optimisation, that the DRLapproach outperforms PSO in terms of computational efficiency and shapeoptimisation performance, and that the TL-enhanced DRL agent achievesperformance comparable to the DRL one, while further saving substantialcomputational resources.</description>
      <author>example@mail.com (David Ramos, Lucas Lacasa, Eusebio Valero, Gonzalo Rubio)</author>
      <guid isPermaLink="false">2505.02634v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection</title>
      <link>http://arxiv.org/abs/2505.02331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Source code and pre-trained models will be available at  https://github.com/MSA-LMC/VAEmo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VAEmo是一种高效的两阶段框架，用于基于情感联合视觉-听觉表示学习，并通过外部知识注入解决情感识别的挑战。&lt;h4&gt;背景&lt;/h4&gt;音频视觉情感识别（AVER）旨在从非语言视觉-听觉线索中推断人类情感，具有模态互补和语言无关的优势，但面临情感表达的不确定性、跨模态表达差异和可靠标注数据稀缺等问题。&lt;h4&gt;目的&lt;/h4&gt;提出VAEmo，以解决AVER中的挑战，实现高效的跨模态情感语义建模。&lt;h4&gt;方法&lt;/h4&gt;第一阶段，通过掩码重建和对比性目标，在大型以说话者为中心的VA语料库上预训练一个统一且轻量级的表示网络；第二阶段，使用多模态大型语言模型根据设计的思维链提示生成详细的情感描述，并通过双路径对比学习将丰富的文本语义注入到VA表示中。&lt;h4&gt;主要发现&lt;/h4&gt;VAEmo在多个下游AVER基准测试中实现了最先进的性能，证明了统一跨模态编码和情感感知语义指导对高效、通用VA情感表示的益处。&lt;h4&gt;结论&lt;/h4&gt;VAEmo通过统一跨模态编码和情感感知语义指导，为高效、通用的VA情感表示提供了新的解决方案，并实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audiovisual emotion recognition (AVER) aims to infer human emotions fromnonverbal visual-audio (VA) cues, offering modality-complementary andlanguage-agnostic advantages. However, AVER remains challenging due to theinherent ambiguity of emotional expressions, cross-modal expressivedisparities, and the scarcity of reliably annotated data. Recentself-supervised AVER approaches have introduced strong multimodalrepresentations, yet they predominantly rely on modality-specific encoders andcoarse content-level alignment, limiting fine-grained emotional semanticmodeling. To address these issues, we propose VAEmo, an efficient two-stageframework for emotion-centric joint VA representation learning with externalknowledge injection. In Stage 1, a unified and lightweight representationnetwork is pre-trained on large-scale speaker-centric VA corpora via maskedreconstruction and contrastive objectives, mitigating the modality gap andlearning expressive, complementary representations without emotion labels. InStage 2, multimodal large language models automatically generate detailedaffective descriptions according to our well-designed chain-of-thoughtprompting for only a small subset of VA samples; these rich textual semanticsare then injected by aligning their corresponding embeddings with VArepresentations through dual-path contrastive learning, further bridging theemotion gap. Extensive experiments on multiple downstream AVER benchmarks showthat VAEmo achieves state-of-the-art performance with a compact design,highlighting the benefit of unified cross-modal encoding and emotion-awaresemantic guidance for efficient, generalizable VA emotion representations.</description>
      <author>example@mail.com (Hao Cheng, Zhiwei Zhao, Yichao He, Zhenzhen Hu, Jia Li, Meng Wang, Richang Hong)</author>
      <guid isPermaLink="false">2505.02331v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
      <link>http://arxiv.org/abs/2505.02350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用稀疏椭球径向基函数网络逼近点云上签名的距离函数（SDF）的机器学习方法，实现了紧凑且精确的表面表示。&lt;h4&gt;背景&lt;/h4&gt;点云表面表示是计算机图形学和视觉中的基本问题。&lt;h4&gt;目的&lt;/h4&gt;通过使用尽可能少的椭球径向基函数（ERBFs）来逼近SDF，实现点云的紧凑和精确的表面表示。&lt;h4&gt;方法&lt;/h4&gt;引入了一种动态多目标优化策略，自适应地添加正则化项，并联合优化ERBFs的权重、中心、形状和方向。为了提高计算效率，采用了基于最近邻的数据结构，并将每个核的计算并行化在CUDA上。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个基准数据集上的广泛实验，证明了该方法在准确性、鲁棒性和计算效率方面优于之前的稀疏表示方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为点云表面表示提供了一种高效且准确的解决方案，并公开了相应的代码。&lt;h4&gt;翻译&lt;/h4&gt;摘要：点云表面表示是计算机图形学和视觉中的基本问题。本文提出了一种机器学习方法，使用稀疏椭球径向基函数网络逼近点云的签名距离函数（SDF），从而实现紧凑且精确的表面表示。给定由点云构建的网格点上的SDF值，我们的方法尽可能准确地逼近SDF，即使用尽可能少的椭球径向基函数（ERBFs）来表示点云的SDF。为了平衡稀疏性和逼近精度，引入了一种动态多目标优化策略，该策略自适应地添加正则化项，并联合优化ERBFs的权重、中心、形状和方向。为了提高计算效率，采用了一种基于最近邻的数据结构，将函数计算限制在每个高斯核中心附近的点。进一步地，每个核的计算在CUDA上进行了并行化，这显著提高了优化速度。此外，设计了一种基于分层八叉树的细化策略进行训练。具体来说，使用八叉树晶格结构中的粗网格点初始化和优化网络参数。随后，逐步引入细晶格点以加速模型收敛并提高训练效率。在多个基准数据集上的广泛实验表明，我们的方法在准确性、鲁棒性和计算效率方面优于之前的稀疏表示方法。相应的代码可在https://github.com/lianbobo/SE-RBFNet.git上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud surface representation is a fundamental problem in computergraphics and vision. This paper presents a machine learning approach forapproximating the signed distance function (SDF) of a point cloud using sparseellipsoidal radial basis function networks, enabling a compact and accuratesurface representation. Given the SDF values defined on the grid pointsconstructed from the point cloud, our method approximates the SDF accuratelywith as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,represent the SDF of a point cloud by sparse ERBFs. To balance sparsity andapproximation precision, a dynamic multi-objective optimization strategy isintroduced, which adaptively adds the regularization terms and jointlyoptimizes the weights, centers, shapes, and orientations of ERBFs. To improvecomputational efficiency, a nearest-neighbor-based data structure is employed,restricting function calculations to points near each Gaussian kernel center.The computations for each kernel are further parallelized on CUDA, whichsignificantly improves the optimization speed. Additionally, a hierarchicaloctree-based refinement strategy is designed for training. Specifically, theinitialization and optimization of network parameters are conducted usingcoarse grid points in the octree lattice structure. Subsequently, fine latticepoints are progressively incorporated to accelerate model convergence andenhance training efficiency. Extensive experiments on multiple benchmarkdatasets demonstrate that our method outperforms previous sparse representationapproaches in terms of accuracy, robustness, and computational efficiency. Thecorresponding code is publicly available athttps://github.com/lianbobo/SE-RBFNet.git.</description>
      <author>example@mail.com (Bobo Lian, Dandan Wang, Chenjian Wu, Minxin Chen)</author>
      <guid isPermaLink="false">2505.02350v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action</title>
      <link>http://arxiv.org/abs/2505.01583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TEMPURA的视频理解框架，旨在提高视觉语言模型对视频的时序理解和因果事件关系处理能力。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在处理视频时，要么压缩视频标记以降低时间分辨率，要么将视频视为未分割的流，这导致无法精确识别事件边界和建模因果依赖关系。&lt;h4&gt;目的&lt;/h4&gt;提高视频时序理解能力，实现细粒度的时间定位。&lt;h4&gt;方法&lt;/h4&gt;TEMPURA采用两阶段训练框架：首先，通过掩码事件预测推理重建缺失事件并生成因果解释；其次，学习视频分割和密集描述，将视频分解为非重叠事件，并配以详细的时间戳对齐描述。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPURA在时序定位和突出检测基准测试中优于强基线模型，证明了将因果推理与细粒度时序分割相结合可以提升视频理解能力。&lt;h4&gt;结论&lt;/h4&gt;TEMPURA框架通过结合因果推理和细粒度时序分割，有效提高了视觉语言模型对视频的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：理解因果事件关系和实现视频中的细粒度时间定位对视觉语言模型来说仍然具有挑战性。现有方法要么压缩视频标记以降低时间分辨率，要么将视频视为未分割的流，这模糊了细粒度事件边界并限制了因果依赖关系的建模。我们提出了TEMPURA（用于动作推理的时序事件掩码预测和理解），一个两阶段训练框架，用于增强视频时序理解。TEMPURA首先应用掩码事件预测推理来重建缺失事件，并从密集事件注释中生成逐步的因果解释，从中汲取有效的填充技术。然后，TEMPURA学习执行视频分割和密集描述，将视频分解为非重叠事件，并配以详细的时间戳对齐描述。我们在我们自己编纂的VER数据集上训练TEMPURA，该数据集包含100万个训练实例和50万个具有时间对齐事件描述和结构化推理步骤的视频。在时序定位和突出检测基准测试中进行的实验表明，TEMPURA优于强基线模型，证实了将因果推理与细粒度时序分割相结合可提高视频理解能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding causal event relationships and achieving fine-grained temporalgrounding in videos remain challenging for vision-language models. Existingmethods either compress video tokens to reduce temporal resolution, or treatvideos as unsegmented streams, which obscures fine-grained event boundaries andlimits the modeling of causal dependencies. We propose TEMPURA (Temporal EventMasked Prediction and Understanding for Reasoning in Action), a two-stagetraining framework that enhances video temporal understanding. TEMPURA firstapplies masked event prediction reasoning to reconstruct missing events andgenerate step-by-step causal explanations from dense event annotations, drawinginspiration from effective infilling techniques. TEMPURA then learns to performvideo segmentation and dense captioning to decompose videos intonon-overlapping events with detailed, timestamp-aligned descriptions. We trainTEMPURA on VER, a large-scale dataset curated by us that comprises 1M traininginstances and 500K videos with temporally aligned event descriptions andstructured reasoning steps. Experiments on temporal grounding and highlightdetection benchmarks demonstrate that TEMPURA outperforms strong baselinemodels, confirming that integrating causal reasoning with fine-grained temporalsegmentation leads to improved video understanding.</description>
      <author>example@mail.com (Jen-Hao Cheng, Vivian Wang, Huayu Wang, Huapeng Zhou, Yi-Hao Peng, Hou-I Liu, Hsiang-Wei Huang, Kuang-Ming Chen, Cheng-Yen Yang, Wenhao Chai, Yi-Ling Chen, Vibhav Vineet, Qin Cai, Jenq-Neng Hwang)</author>
      <guid isPermaLink="false">2505.01583v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation</title>
      <link>http://arxiv.org/abs/2505.02075v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了视觉基础模型（VFMs）在密集预测任务中的有效性，并提出了一种任务无关的特征上采样模块来提高VFM特征分辨率。&lt;h4&gt;背景&lt;/h4&gt;随着VFMs的流行，人们对其在密集预测任务中的效果越来越感兴趣，但由于VFM通常产生低分辨率特征，限制了其直接应用。&lt;h4&gt;目的&lt;/h4&gt;为了评估特征上采样方法在VFM上的有效性，本研究以交互式分割（IS）作为新的基准。&lt;h4&gt;方法&lt;/h4&gt;通过使用任务无关的特征上采样模块来提高VFM特征分辨率，并利用交互式分割作为评估环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，选择合适的上采样策略可以显著提高VFM特征的质量。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的方法可以有效地提高VFM在密集预测任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;Vision Foundation Models (VFMs) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. As VFMs' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. However, VFMs typically produce low-resolution features, limiting their direct applicability in this context. One way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines VFM features resolution. To assess the effectiveness of this approach, we investigate Interactive Segmentation (IS) as a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, IS creates a challenging environment that demands comprehensive visual scene understanding. Our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves VFM features quality. The code is released at https://github.com/havrylovv/iSegProbe&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Foundation Models (VFMs) are large-scale, pre-trained models thatserve as general-purpose backbones for various computer vision tasks. As VFMs'popularity grows, there is an increasing interest in understanding theireffectiveness for dense prediction tasks. However, VFMs typically producelow-resolution features, limiting their direct applicability in this context.One way to tackle this limitation is by employing a task-agnostic featureupsampling module that refines VFM features resolution. To assess theeffectiveness of this approach, we investigate Interactive Segmentation (IS) asa novel benchmark for evaluating feature upsampling methods on VFMs. Due to itsinherent multimodal input, consisting of an image and a set of user-definedclicks, as well as its dense mask output, IS creates a challenging environmentthat demands comprehensive visual scene understanding. Our benchmarkingexperiments show that selecting appropriate upsampling strategies significantlyimproves VFM features quality. The code is released athttps://github.com/havrylovv/iSegProbe</description>
      <author>example@mail.com (Volodymyr Havrylov, Haiwen Huang, Dan Zhang, Andreas Geiger)</author>
      <guid isPermaLink="false">2505.02075v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery</title>
      <link>http://arxiv.org/abs/2505.02829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 10 figures, 19 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为LISAt的视觉-语言模型，用于描述复杂遥感场景、回答相关问题并分割感兴趣的对象。&lt;h4&gt;背景&lt;/h4&gt;现有的分割模型可以识别图像中的预定义对象，但难以处理涉及多个对象的复杂用户查询。&lt;h4&gt;目的&lt;/h4&gt;开发LISAt模型，使其能够描述复杂遥感场景，回答相关问题，并分割感兴趣的对象。&lt;h4&gt;方法&lt;/h4&gt;在新的地理空间推理-分割数据集GRES上训练LISAt，该数据集包含27,615个标注和9,205张图像，以及包含超过100万个问答对的模态预训练数据集PreGRES。&lt;h4&gt;主要发现&lt;/h4&gt;LISAt在遥感描述任务上优于现有的地理空间基础模型RS-GPT4V，在推理分割任务上超越了最先进的开放域模型，分别提高了10.04%和143.36%。&lt;h4&gt;结论&lt;/h4&gt;LISAt模型、数据集和代码可在https://lisat-bair.github.io/LISAt/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segmentation models can recognize a pre-defined set of objects in images.However, models that can reason over complex user queries that implicitly referto multiple objects of interest are still in their infancy. Recent advances inreasoning segmentation--generating segmentation masks from complex, implicitquery text--demonstrate that vision-language models can operate across an opendomain and produce reasonable outputs. However, our experiments show that suchmodels struggle with complex remote-sensing imagery. In this work, we introduceLISAt, a vision-language model designed to describe complex remote-sensingscenes, answer questions about them, and segment objects of interest. Wetrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,with 27,615 annotations over 9,205 images, and a multimodal pretrainingdataset, PreGRES, containing over 1 million question-answer pairs. LISAtoutperforms existing geospatial foundation models such as RS-GPT4V by over10.04 % (BLEU-4) on remote-sensing description tasks, and surpassesstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %(gIoU). Our model, datasets, and code are available athttps://lisat-bair.github.io/LISAt/</description>
      <author>example@mail.com (Jerome Quenum, Wen-Han Hsieh, Tsung-Han Wu, Ritwik Gupta, Trevor Darrell, David M. Chan)</author>
      <guid isPermaLink="false">2505.02829v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design</title>
      <link>http://arxiv.org/abs/2505.02168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025 (https://openreview.net/forum?id=rbnf7oe6JQ)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CircuitFusion，这是一种多模态和实现感知的电路编码器，能够将电路编码成支持不同下游电路设计任务的一般表示。&lt;h4&gt;背景&lt;/h4&gt;AI的快速发展依赖于IC的支持，但数字IC的日益复杂使得传统的IC设计过程成本高昂且耗时。&lt;h4&gt;目的&lt;/h4&gt;提出CircuitFusion，以解决传统IC设计过程的成本和时间问题，并提高电路设计效率。&lt;h4&gt;方法&lt;/h4&gt;CircuitFusion融合了三种电路模态：硬件代码、结构图和功能摘要，并识别了电路的四个独特属性：并行执行、功能等效变换、多个设计阶段和电路可重用性。&lt;h4&gt;主要发现&lt;/h4&gt;CircuitFusion在五个不同的电路设计任务上表现优于特定于每个任务的SOTA监督方法，证明了其泛化能力和学习电路固有属性的能力。&lt;h4&gt;结论&lt;/h4&gt;CircuitFusion为电路设计提供了一种新的、高效的方法，能够显著提高设计效率和性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancements of AI rely on the support of ICs. However, the growingcomplexity of digital ICs makes the traditional IC design process costly andtime-consuming. In recent years, AI-assisted IC design methods havedemonstrated great potential, but most methods are task-specific or focussolely on the circuit structure in graph format, overlooking other circuitmodalities with rich functional information. In this paper, we introduceCircuitFusion, the first multimodal and implementation-aware circuit encoder.It encodes circuits into general representations that support differentdownstream circuit design tasks. To learn from circuits, we propose to fusethree circuit modalities: hardware code, structural graph, and functionalitysummary. More importantly, we identify four unique properties of circuits:parallel execution, functional equivalent transformation, multiple designstages, and circuit reusability. Based on these properties, we propose newstrategies for both the development and application of CircuitFusion: 1) Duringcircuit preprocessing, utilizing the parallel nature of circuits, we split eachcircuit into multiple sub-circuits based on sequential-element boundaries, eachsub-circuit in three modalities. 2) During CircuitFusion pre-training, weintroduce three self-supervised tasks that utilize equivalent transformationsboth within and across modalities. 3) When applying CircuitFusion to downstreamtasks, we propose a new retrieval-augmented inference method, which retrievessimilar known circuits as a reference for predictions. It improves fine-tuningperformance and even enables zero-shot inference. Evaluated on five differentcircuit design tasks, CircuitFusion consistently outperforms the SOTAsupervised method specifically developed for every single task, demonstratingits generalizability and ability to learn circuits' inherent properties.</description>
      <author>example@mail.com (Wenji Fang, Shang Liu, Jing Wang, Zhiyao Xie)</author>
      <guid isPermaLink="false">2505.02168v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2505.02304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GSP-MC的生成式手语描述提示多正对比学习方法，用于手语识别，通过结合检索增强生成和领域特定的大型语言模型，以及多步提示工程和专家验证的手语语料库，生成精确的多部分描述。&lt;h4&gt;背景&lt;/h4&gt;手语识别（SLR）在创建准确标注方面面临挑战，因为同时手动和非手动信号固有的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，将生成式大型语言模型（LLMs）集成到手语识别任务中。&lt;h4&gt;方法&lt;/h4&gt;GSP-MC方法利用检索增强生成（RAG）与领域特定LLMs，结合多步提示工程和专家验证的手语语料库。它还采用双编码器架构，通过概率匹配双向对齐层次骨骼特征与多个文本描述（全局、同义词和部分级别）。该方法结合全局和部分级别的损失，优化KL散度，确保所有相关文本-骨骼对之间的鲁棒对齐，同时捕获手势级别的语义和详细的部件动态。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在中文SLR500（达到97.1%）和土耳其AUTSL数据集（97.07%准确率）上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法的多语言有效性突显了其在开发包容性通信技术方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：手语识别（SLR）在创建准确标注方面面临基本挑战，因为同时手动和非手动信号固有的复杂性。据我们所知，这是首次将生成式大型语言模型（LLMs）集成到手语识别任务中的工作。我们提出了一种名为GSP-MC的新颖的生成式手语描述提示多正对比学习方法，该方法利用检索增强生成（RAG）与领域特定LLMs，结合多步提示工程和专家验证的手语语料库，以生成精确的多部分描述。GSP-MC方法还采用双编码器架构，通过概率匹配双向对齐层次骨骼特征与多个文本描述（全局、同义词和部分级别）。我们的方法结合全局和部分级别的损失，优化KL散度，以确保所有相关文本-骨骼对之间的鲁棒对齐，同时捕获手势级别的语义和详细的部件动态。实验表明，该方法在中文SLR500（达到97.1%）和土耳其AUTSL数据集（97.07%准确率）上优于现有方法。该方法的多语言有效性突显了其在开发包容性通信技术方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language recognition (SLR) faces fundamental challenges in creatingaccurate annotations due to the inherent complexity of simultaneous manual andnon-manual signals. To the best of our knowledge, this is the first work tointegrate generative large language models (LLMs) into SLR tasks. We propose anovel Generative Sign-description Prompts Multi-positive Contrastive learning(GSP-MC) method that leverages retrieval-augmented generation (RAG) withdomain-specific LLMs, incorporating multi-step prompt engineering andexpert-validated sign language corpora to produce precise multipartdescriptions. The GSP-MC method also employs a dual-encoder architecture tobidirectionally align hierarchical skeleton features with multiple textdescriptions (global, synonym, and part level) through probabilistic matching.Our approach combines global and part-level losses, optimizing KL divergence toensure robust alignment across all relevant text-skeleton pairs while capturingboth sign-level semantics and detailed part dynamics. Experiments demonstratestate-of-the-art performance against existing methods on the Chinese SLR500(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method'scross-lingual effectiveness highlight its potential for developing inclusivecommunication technologies.</description>
      <author>example@mail.com (Siyu Liang, Yunan Li, Wentian Xin, Huizhou Chen, Xujie Liu, Kang Liu, Qiguang Miao)</author>
      <guid isPermaLink="false">2505.02304v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering</title>
      <link>http://arxiv.org/abs/2505.02173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时间序列聚类方法，用于将具有相似行为的时间序列数据分类成组。&lt;h4&gt;背景&lt;/h4&gt;时间序列聚类是一种无监督学习方法，用于将时间序列数据分类。它广泛应用于医疗保健、金融、经济、能源和气候科学等领域。已有多种时间序列聚类方法被提出并使用了四十多年。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的相似性度量方法，并将其应用于时间序列聚类。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为“排序皮尔逊相关差异度”（RDPC）的新差异度量方法，该方法结合了加权平均的指定分数的最大元素差异与已知的皮尔逊相关差异度。该方法被整合到层次聚类中，并与其他聚类算法的性能进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。&lt;h4&gt;结论&lt;/h4&gt;通过将泰国的随机样本客户聚类到具有独特特征的七个组中，展示了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：时间序列聚类是一种无监督学习方法，用于将具有相似行为的时间序列数据分类成组。它在医疗保健、金融、经济、能源和气候科学等应用中得到了使用。几十年来，已经提出了几种时间序列聚类方法。大多数方法都集中在测量时间序列之间的欧几里得距离或关联差异度。在这项工作中，我们提出了一种新的差异度量方法，称为排序皮尔逊相关差异度（RDPC），它将指定分数的最大元素差异的加权平均与已知的皮尔逊相关差异度相结合。它被整合到层次聚类中。性能得到了评估，并与现有的聚类算法进行了比较。结果表明，RDPC算法在涉及不同季节模式、趋势和峰值的复杂情况下优于其他算法。最后，我们通过将来自泰国电力消耗时间序列数据集的随机样本客户聚类到具有独特特征的七个组中，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series clustering is an unsupervised learning method for classifyingtime series data into groups with similar behavior. It is used in applicationssuch as healthcare, finance, economics, energy, and climate science. Severaltime series clustering methods have been introduced and used for over fourdecades. Most of them focus on measuring either Euclidean distances orassociation dissimilarities between time series. In this work, we propose a newdissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),which combines a weighted average of a specified fraction of the largestelement-wise differences with the well-known Pearson correlation dissimilarity.It is incorporated into hierarchical clustering. The performance is evaluatedand compared with existing clustering algorithms. The results show that theRDPC algorithm outperforms others in complicated cases involving differentseasonal patterns, trends, and peaks. Finally, we demonstrate our method byclustering a random sample of customers from a Thai electricity consumptiontime series dataset into seven groups with unique characteristics.</description>
      <author>example@mail.com (Chutiphan Charoensuk, Nathakhun Wiroonsri)</author>
      <guid isPermaLink="false">2505.02173v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction</title>
      <link>http://arxiv.org/abs/2505.02126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GarmentGS的新方法，用于高保真地重建服装表面，并生成非密封的单层网格。该方法通过密集点云引导，实现快速重建和高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统的3D服装制作需要大量的人工操作，导致时间和劳动力成本高。3D高斯Splatting在3D场景重建和渲染方面取得了突破性进展，为3D服装重建开辟了新的途径。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，以解决传统方法中由于高斯原元的无结构和不规则性质导致的难以重建高保真、非密封的3D服装的问题。&lt;h4&gt;方法&lt;/h4&gt;GarmentGS方法引入了一个快速密集点云重建模块，可以在10分钟内完成服装点云重建，并使用密集点云引导高斯原元的移动、展平和旋转，以在服装表面上实现更好的分布，达到优异的渲染效果和几何精度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了快速训练和实时渲染，同时保持了有竞争力的质量。&lt;h4&gt;结论&lt;/h4&gt;GarmentGS方法通过密集点云引导，能够高效地重建高保真服装表面，为3D服装设计提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的3D服装制作需要大量的人工操作，导致时间和劳动力成本高。近年来，3D高斯Splatting在3D场景重建和渲染方面取得了突破性进展，引起了广泛关注，并为3D服装重建开辟了新的途径。然而，由于高斯原元的无结构和不规则性质，难以重建高保真、非密封的3D服装。在本文中，我们提出了一种名为GarmentGS的密集点云引导方法，可以重建具有高几何精度的服装表面，并生成非密封的单层网格。我们的方法引入了一个快速密集点云重建模块，可以在10分钟内完成服装点云重建，比传统方法所需的几个小时要快。此外，我们使用密集点云来引导高斯原元的移动、展平和旋转，以在服装表面上实现更好的分布，从而实现优异的渲染效果和几何精度。通过数值和可视化比较，我们的方法在保持有竞争力的质量的同时，实现了快速训练和实时渲染。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional 3D garment creation requires extensive manual operations,resulting in time and labor costs. Recently, 3D Gaussian Splatting has achievedbreakthrough progress in 3D scene reconstruction and rendering, attractingwidespread attention and opening new pathways for 3D garment reconstruction.However, due to the unstructured and irregular nature of Gaussian primitives,it is difficult to reconstruct high-fidelity, non-watertight 3D garments. Inthis paper, we present GarmentGS, a dense point cloud-guided method that canreconstruct high-fidelity garment surfaces with high geometric accuracy andgenerate non-watertight, single-layer meshes. Our method introduces a fastdense point cloud reconstruction module that can complete garment point cloudreconstruction in 10 minutes, compared to traditional methods that requireseveral hours. Furthermore, we use dense point clouds to guide the movement,flattening, and rotation of Gaussian primitives, enabling better distributionon the garment surface to achieve superior rendering effects and geometricaccuracy. Through numerical and visual comparisons, our method achieves fasttraining and real-time rendering while maintaining competitive quality.</description>
      <author>example@mail.com (Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao)</author>
      <guid isPermaLink="false">2505.02126v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications</title>
      <link>http://arxiv.org/abs/2505.02179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为ProDisc-VAD的弱监督视频异常检测框架，通过两个协同组件解决标签模糊性问题，提高了特征学习的区分度。&lt;h4&gt;背景&lt;/h4&gt;现有的基于MIL的WS-VAD方法在处理标签模糊性时，难以进行有效的特征学习。&lt;h4&gt;目的&lt;/h4&gt;提出ProDisc-VAD框架，旨在解决标签模糊性问题，提高异常检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;ProDisc-VAD包含原型交互层（PIL）和伪实例判别增强（PIDE）损失。PIL使用少量可学习的原型进行正常性建模，而PIDE损失通过对比学习增强最可靠的极端评分实例的区分度。&lt;h4&gt;主要发现&lt;/h4&gt;ProDisc-VAD在ShanghaiTech和UCF-Crime数据集上取得了优异的AUC值（97.98%和87.12%），且参数数量仅为0.4M，远低于基于ViT的方法。&lt;h4&gt;结论&lt;/h4&gt;ProDisc-VAD框架在效率和性能方面表现出色，代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;摘要：使用多个实例学习（MIL）的弱监督视频异常检测（WS-VAD）由于标签模糊性而受到阻碍，影响了特征学习的区分度。我们提出了ProDisc-VAD，一个高效的框架，通过两个协同组件来解决这个问题。原型交互层（PIL）通过使用一组可学习的原型提供受控的正常性建模，建立了一个稳健的基线，而不会被主导的正常数据所淹没。伪实例判别增强（PIDE）损失通过仅对最可靠的极端评分实例（最高/最低评分）应用有针对性的对比学习来增强可分性。ProDisc-VAD仅使用0.4M参数就实现了强大的AUCs（ShanghaiTech：97.98%，UCF-Crime：87.12%），比最近的基于ViT的方法如VadCLIP少800多倍，展示了卓越的效率以及最先进的性能。代码可在https://github.com/modadundun/ProDisc-VAD上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly-supervised video anomaly detection (WS-VAD) using Multiple InstanceLearning (MIL) suffers from label ambiguity, hindering discriminative featurelearning. We propose ProDisc-VAD, an efficient framework tackling this via twosynergistic components. The Prototype Interaction Layer (PIL) providescontrolled normality modeling using a small set of learnable prototypes,establishing a robust baseline without being overwhelmed by dominant normaldata. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boostsseparability by applying targeted contrastive learning exclusively to the mostreliable extreme-scoring instances (highest/lowest scores). ProDisc-VADachieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4Mparameters, over 800x fewer than recent ViT-based methods like VadCLIP,demonstrating exceptional efficiency alongside state-of-the-art performance.Code is available at https://github.com/modadundun/ProDisc-VAD.</description>
      <author>example@mail.com (Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu)</author>
      <guid isPermaLink="false">2505.02179v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking</title>
      <link>http://arxiv.org/abs/2505.02139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对订单簿（LOB）表示学习进行了系统性比较研究，以识别提取可迁移、紧凑特征的有效方法，这些特征能够捕捉订单簿的基本属性。&lt;h4&gt;背景&lt;/h4&gt;订单簿是金融市场最基本的数据之一，提供了市场动态的精细视图，但由于其强烈的自相关性、交叉特征约束和特征尺度差异，处理深度模型时面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在分析现有方法中代表学习与特定下游任务的紧密耦合，并提出一种有效的方法来提取可迁移的特征。&lt;h4&gt;方法&lt;/h4&gt;引入了LOBench，一个使用真实中国A股市场数据的标准化基准，提供精心制作的数据集、统一的预处理、一致的评估指标和强大的基线。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了订单簿表示对于各种下游任务的充分性和必要性，并突出了其相对于传统任务特定端到端模型和高级表示学习模型在通用时间序列上的优势。&lt;h4&gt;结论&lt;/h4&gt;本研究建立了一个可重复的框架，并为未来的研究提供了明确的指导，数据集和代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constraints, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at https://github.com/financial-simulation-lab/LOBench.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Limit Order Book (LOB), the mostly fundamental data of the financialmarket, provides a fine-grained view of market dynamics while poses significantchallenges in dealing with the esteemed deep models due to its strongautocorrelation, cross-feature constrains, and feature scale disparity.Existing approaches often tightly couple representation learning with specificdownstream tasks in an end-to-end manner, failed to analyze the learnedrepresentations individually and explicitly, limiting their reusability andgeneralization. This paper conducts the first systematic comparative study ofLOB representation learning, aiming to identify the effective way of extractingtransferable, compact features that capture essential LOB properties. Weintroduce LOBench, a standardized benchmark with real China A-share marketdata, offering curated datasets, unified preprocessing, consistent evaluationmetrics, and strong baselines. Extensive experiments validate the sufficiencyand necessity of LOB representations for various downstream tasks and highlighttheir advantages over both the traditional task-specific end-to-end models andthe advanced representation learning models for general time series. Our workestablishes a reproducible framework and provides clear guidelines for futureresearch. Datasets and code will be publicly available athttps://github.com/financial-simulation-lab/LOBench.</description>
      <author>example@mail.com (Muyao Zhong, Yushi Lin, Peng Yang)</author>
      <guid isPermaLink="false">2505.02139v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora</title>
      <link>http://arxiv.org/abs/2505.02147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的深度学习方法，使用卷积神经网络和迁移学习技术对60种不同的草药进行分类，旨在解决在生物多样性丰富的地区（如尼泊尔）中草药分类的挑战。&lt;h4&gt;背景&lt;/h4&gt;草药分类在植物研究中是一个关键的挑战，特别是在像尼泊尔这样生物多样性丰富的地区。&lt;h4&gt;目的&lt;/h4&gt;开发一个鲁棒的机器学习模型，用于对草药进行分类，以解决现有草药识别方法中的局限性。&lt;h4&gt;方法&lt;/h4&gt;研究使用了12000张草药图片的手动整理数据集，并采用了多种模型架构，包括DenseNet121、ResNet50、VGG16、InceptionV3、EfficientNetV2和Vision Transformer（VIT），其中DenseNet121最终表现出最佳性能。还应用了数据增强和正则化技术来减轻过拟合并提高模型的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet121模型在草药分类任务中表现出优越的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作推动了草药分类技术的发展，同时保护了传统的植物学知识并促进了草药的可持续利用。&lt;h4&gt;翻译&lt;/h4&gt;Herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as Nepal. This study introduces a novel deep learning approach for classifying 60 different herb species using Convolutional Neural Networks (CNNs) and transfer learning techniques. Using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. Our research employed multiple model architectures, including DenseNet121, 50-layer Residual Network (ResNet50), 16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2, and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating superior performance. Data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. This work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Herb classification presents a critical challenge in botanical research,particularly in regions with rich biodiversity such as Nepal. This studyintroduces a novel deep learning approach for classifying 60 different herbspecies using Convolutional Neural Networks (CNNs) and transfer learningtechniques. Using a manually curated dataset of 12,000 herb images, wedeveloped a robust machine learning model that addresses existing limitationsin herb recognition methodologies. Our research employed multiple modelarchitectures, including DenseNet121, 50-layer Residual Network (ResNet50),16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,and Vision Transformer (VIT), with DenseNet121 ultimately demonstratingsuperior performance. Data augmentation and regularization techniques wereapplied to mitigate overfitting and enhance the generalizability of the model.This work advances herb classification techniques, preserving traditionalbotanical knowledge and promoting sustainable herb utilization.</description>
      <author>example@mail.com (Prajwal Thapa, Mridul Sharma, Jinu Nyachhyon, Yagya Raj Pandeya)</author>
      <guid isPermaLink="false">2505.02147v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</title>
      <link>http://arxiv.org/abs/2505.01880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9pages, 5figures. This paper has been accepted for IJCAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种渐进式音频-语言共学习网络（LOCO），用于定位音频时间伪造区域，旨在解决现有方法依赖昂贵且难以获取的精细标注的问题。&lt;h4&gt;背景&lt;/h4&gt;音频时间伪造定位（ATFL）旨在识别部分伪造音频中被故意修改的精确伪造区域。现有的ATFL方法依赖于使用精细标注来训练高效的网络，而这种标注在现实场景中获取成本高昂且具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;为了克服上述挑战，本文旨在提出一种能够在弱监督场景下提升定位性能的方法。&lt;h4&gt;方法&lt;/h4&gt;本文提出的LOCO网络采用共学习和自监督的方法，具体包括：1）设计了一个音频-语言共学习模块，通过从时间和全局视角对齐语义来捕获伪造一致性特征；2）通过结合语音级别的标注和可学习提示构建伪造感知提示，动态地将语义先验融入时间内容特征；3）应用伪造定位模块，基于融合的伪造类别激活序列生成伪造提案；4）引入渐进式细化策略，生成伪帧级标签，利用监督语义对比学习增强真实内容与伪造内容之间的语义区别，从而不断优化伪造感知特征。&lt;h4&gt;主要发现&lt;/h4&gt;大量的实验表明，提出的LOCO在网络性能上达到了在三个公开基准测试上的SOTA（最先进的技术水平）。&lt;h4&gt;结论&lt;/h4&gt;本文提出的LOCO方法能够有效地定位音频时间伪造区域，并且在公开基准测试中取得了优异的性能，为音频伪造检测领域提供了新的思路和方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio temporal forgery localization (ATFL) aims to find the precise forgeryregions of the partial spoof audio that is purposefully modified. Existing ATFLmethods rely on training efficient networks using fine-grained annotations,which are obtained costly and challenging in real-world scenarios. To meet thischallenge, in this paper, we propose a progressive audio-language co-learningnetwork (LOCO) that adopts co-learning and self-supervision manners to promptlocalization performance under weak supervision scenarios. Specifically, anaudio-language co-learning module is first designed to capture forgeryconsensus features by aligning semantics from temporal and global perspectives.In this module, forgery-aware prompts are constructed by using utterance-levelannotations together with learnable prompts, which can incorporate semanticpriors into temporal content features dynamically. In addition, a forgerylocalization module is applied to produce forgery proposals based on fusedforgery-class activation sequences. Finally, a progressive refinement strategyis introduced to generate pseudo frame-level labels and leverage supervisedsemantic contrastive learning to amplify the semantic distinction between realand fake content, thereby continuously optimizing forgery-aware features.Extensive experiments show that the proposed LOCO achieves SOTA performance onthree public benchmarks.</description>
      <author>example@mail.com (Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo)</author>
      <guid isPermaLink="false">2505.01880v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Deep Representation Learning for Electronic Design Automation</title>
      <link>http://arxiv.org/abs/2505.02105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了在电子设计自动化（EDA）领域中，表示学习作为一种有效技术，如何利用自然表示工作流程元素作为图像、网格和图，通过解决电路复杂性增加和严格的功率、性能和面积（PPA）要求，自动从复杂数据格式中提取有意义的特征。&lt;h4&gt;背景&lt;/h4&gt;表示学习已成为EDA算法中的一种有效技术，它利用工作流程元素的自然表示，如图像、网格和图。&lt;h4&gt;目的&lt;/h4&gt;本文旨在分析表示学习在EDA中的应用，包括基础概念，以及时间预测、可布线分析和自动化布局等任务的先前工作和案例研究。&lt;h4&gt;方法&lt;/h4&gt;本文介绍了基于图像的方法、基于图的方法和混合多模态解决方案等关键技术，以展示表示学习在路由、时序和寄生预测方面的改进。&lt;h4&gt;主要发现&lt;/h4&gt;本文提供的进步展示了表示学习在提高当前集成电路设计流程中的效率、准确性和可扩展性方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;表示学习在EDA领域有广泛的应用前景，可以显著提高集成电路设计的效率和质量。&lt;h4&gt;翻译&lt;/h4&gt;Representation learning has become an effective technique utilized by electronic design automation (EDA) algorithms, which leverage the natural representation of workflow elements as images, grids, and graphs. By addressing challenges related to the increasing complexity of circuits and stringent power, performance, and area (PPA) requirements, representation learning facilitates the automatic extraction of meaningful features from complex data formats, including images, grids, and graphs. This paper examines the application of representation learning in EDA, covering foundational concepts and analyzing prior work and case studies on tasks that include timing prediction, routability analysis, and automated placement. Key techniques, including image-based methods, graph-based approaches, and hybrid multimodal solutions, are presented to illustrate the improvements provided in routing, timing, and parasitic prediction. The provided advancements demonstrate the potential of representation learning to enhance efficiency, accuracy, and scalability in current integrated circuit design flows.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning has become an effective technique utilized byelectronic design automation (EDA) algorithms, which leverage the naturalrepresentation of workflow elements as images, grids, and graphs. By addressingchallenges related to the increasing complexity of circuits and stringentpower, performance, and area (PPA) requirements, representation learningfacilitates the automatic extraction of meaningful features from complex dataformats, including images, grids, and graphs. This paper examines theapplication of representation learning in EDA, covering foundational conceptsand analyzing prior work and case studies on tasks that include timingprediction, routability analysis, and automated placement. Key techniques,including image-based methods, graph-based approaches, and hybrid multimodalsolutions, are presented to illustrate the improvements provided in routing,timing, and parasitic prediction. The provided advancements demonstrate thepotential of representation learning to enhance efficiency, accuracy, andscalability in current integrated circuit design flows.</description>
      <author>example@mail.com (Pratik Shrestha, Saran Phatharodom, Alec Aversa, David Blankenship, Zhengfeng Wu, Ioannis Savidis)</author>
      <guid isPermaLink="false">2505.02105v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Segment Any RGB-Thermal Model with Language-aided Distillation</title>
      <link>http://arxiv.org/abs/2505.01950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2412.04220 by other authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SARTM是一个针对RGB-T语义分割的框架，通过改进SAM模型，并结合语义理解模块，提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;SAM模型在RGB数据上表现良好，但在RGB-T语义分割上有局限性。&lt;h4&gt;目的&lt;/h4&gt;提出SARTM框架，以增强SAM模型在RGB-T语义分割中的应用。&lt;h4&gt;方法&lt;/h4&gt;1. 通过添加LoRA层微调SAM模型，保留其泛化能力和分割能力。2. 引入语言信息作为训练指导。3. 引入跨模态知识蒸馏模块（CMKD）以实现模态适应。4. 调整SAM的分割头，并加入辅助语义分割头，融合多尺度特征。&lt;h4&gt;主要发现&lt;/h4&gt;SARTM在三个多模态RGB-T语义分割基准测试中（MFNET、PST900、FMB）表现出色，优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;SARTM显著提升了RGB-T语义分割的性能，适用于各种视觉条件。&lt;h4&gt;翻译&lt;/h4&gt;The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low-light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation (CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent Segment Anything Model (SAM) demonstrates strong instancesegmentation performance across various downstream tasks. However, SAM istrained solely on RGB data, limiting its direct applicability to RGB-thermal(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution forscene understanding in adverse weather and lighting conditions, such as lowlight and overexposure, we propose a novel framework, SARTM, which customizesthe powerful SAM for RGB-T semantic segmentation. Our key idea is to unleashthe potential of SAM while introduce semantic understanding modules for RGB-Tdata pairs. Specifically, our framework first involves fine tuning the originalSAM by adding extra LoRA layers, aiming at preserving SAM's stronggeneralization and segmentation capabilities for downstream tasks. Secondly, weintroduce language information as guidance for training our SARTM. To addresscross-modal inconsistencies, we introduce a Cross-Modal KnowledgeDistillation(CMKD) module that effectively achieves modality adaptation whilemaintaining its generalization capabilities. This semantic module enables theminimization of modality gaps and alleviates semantic ambiguity, facilitatingthe combination of any modality under any visual conditions. Furthermore, weenhance the segmentation performance by adjusting the segmentation head of SAMand incorporating an auxiliary semantic segmentation head, which integratesmulti-scale features for effective fusion. Extensive experiments are conductedacross three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,and FMB. Both quantitative and qualitative results consistently demonstratethat the proposed SARTM significantly outperforms state-of-the-art approachesacross a variety of conditions.</description>
      <author>example@mail.com (Dong Xing, Xianxun Zhu, Wei Zhou, Qika Lin, Hang Yang, Yuqing Wang)</author>
      <guid isPermaLink="false">2505.01950v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Using Knowledge Graphs to harvest datasets for efficient CLIP model training</title>
      <link>http://arxiv.org/abs/2505.02746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用智能网络搜索策略和知识图谱来训练高质量CLIP模型的方法，显著减少了所需数据量，从而降低了训练成本。&lt;h4&gt;背景&lt;/h4&gt;训练高质量的CLIP模型通常需要大量的数据集，这限制了特定领域模型的发展，尤其是在CLIP模型覆盖不佳的领域，同时也增加了训练成本。&lt;h4&gt;目的&lt;/h4&gt;为了解决需要对CLIP模型训练过程进行精细控制的科学研究的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过采用智能网络搜索策略和知识图谱，从少量数据中从头开始训练一个鲁棒的CLIP模型。&lt;h4&gt;主要发现&lt;/h4&gt;只需要10M张图片就可以构建一个专门针对生物有机体的专家基础模型，并引入了包含3300万张图片和4600万文本描述的EntityNet数据集，这显著缩短了训练通用CLIP模型所需的时间。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地降低了训练高质量CLIP模型的数据需求，为特定领域模型的发展提供了新的可能性，并减少了训练成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training high-quality CLIP models typically requires enormous datasets, whichlimits the development of domain-specific models -- especially in areas thateven the largest CLIP models do not cover well -- and drives up training costs.This poses challenges for scientific research that needs fine-grained controlover the training procedure of CLIP models. In this work, we show that byemploying smart web search strategies enhanced with knowledge graphs, a robustCLIP model can be trained from scratch with considerably less data.Specifically, we demonstrate that an expert foundation model for livingorganisms can be built using just 10M images. Moreover, we introduce EntityNet,a dataset comprising 33M images paired with 46M text descriptions, whichenables the training of a generic CLIP model in significantly reduced time.</description>
      <author>example@mail.com (Simon Ging, Sebastian Walter, Jelena Bratulić, Johannes Dienert, Hannah Bast, Thomas Brox)</author>
      <guid isPermaLink="false">2505.02746v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Federated Graph Learning: A Data Condensation Perspective</title>
      <link>http://arxiv.org/abs/2505.02573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了联邦图学习（Federated Graph Learning）的一种新范式FedGM，旨在解决现有联邦图学习中数据异质性和隐私风险的问题。&lt;h4&gt;背景&lt;/h4&gt;联邦图学习通过多客户端的图促进图神经网络（GNNs）的协作训练，但现有方法在联邦优化中过于依赖模型参数或梯度的通信，未能有效处理由复杂多变的图分布引入的数据异质性。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为压缩图的优化载体，以及新的FGL范式FedGM，以解决FGL的数据异质性和隐私风险问题。&lt;h4&gt;方法&lt;/h4&gt;FedGM利用广义的压缩图共识聚合分布式图的综合知识，同时通过单次传输压缩数据来最小化通信成本和隐私风险。&lt;h4&gt;主要发现&lt;/h4&gt;在六个公开数据集上的大量实验表明，FedGM优于最先进的基础方案，突显了其在新型FGL范式中的潜力。&lt;h4&gt;结论&lt;/h4&gt;FedGM是一种有效的联邦图学习新方法，可以解决数据异质性和隐私风险问题，具有潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated graph learning is a widely recognized technique that promotescollaborative training of graph neural networks (GNNs) by multi-clientgraphs.However, existing approaches heavily rely on the communication of modelparameters or gradients for federated optimization and fail to adequatelyaddress the data heterogeneity introduced by intricate and diverse graphdistributions. Although some methods attempt to share additional messages amongthe server and clients to improve federated convergence during communication,they introduce significant privacy risks and increase communication overhead.To address these issues, we introduce the concept of a condensed graph as anovel optimization carrier to address FGL data heterogeneity and propose a newFGL paradigm called FedGM. Specifically, we utilize a generalized condensationgraph consensus to aggregate comprehensive knowledge from distributed graphs,while minimizing communication costs and privacy risks through a singletransmission of the condensed data. Extensive experiments on six publicdatasets consistently demonstrate the superiority of FedGM overstate-of-the-art baselines, highlighting its potential for a novel FGLparadigm.</description>
      <author>example@mail.com (Hao Zhang, Xunkai Li, Yinlin Zhu, Lianglin Hu)</author>
      <guid isPermaLink="false">2505.02573v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding</title>
      <link>http://arxiv.org/abs/2505.01743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Llambda的劳动力节约系统，旨在支持低分辨率的人机行为理解（HBU）。该系统通过利用有限的标记数据和大量的未标记数据来指导大视觉语言模型（LVLM）生成信息丰富的字幕，从而有效微调LVLM模型以理解低分辨率视频。&lt;h4&gt;背景&lt;/h4&gt;随着大视觉语言模型（LVLMs）的快速发展，它们在生成关于低分辨率视觉系统（如深度、热成像和红外）的人机行为理解（HBU）描述方面具有超越传统标注的潜力。然而，现有的LVLM方法无法很好地理解低分辨率数据，因为它们主要设计用于处理高分辨率数据，如RGB图像。&lt;h4&gt;目的&lt;/h4&gt;提出一种劳动力节约的系统Llambda，以支持低分辨率HBU。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种对比性导向的数据标注器，可以从长低分辨率视频中捕获与行为相关的信息，并通过对比学习生成高质量的伪标签。2. 提出了一种物理知识引导的字幕生成器，利用空间和时间一致性检查来减轻伪标签中的错误。3. 使用基于LoRA的高效微调技术来确保设备上的部署。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Llambda在区域尺度真实测试平台上使用三个不同的低分辨率数据集进行评估时，平均在Bert-Score上优于几个最先进的LVLM系统，提高了40.03%。&lt;h4&gt;结论&lt;/h4&gt;Llambda系统通过有效利用有限标记数据和大量未标记数据，能够显著提高LVLM模型对低分辨率视频的理解能力，并在实际应用中展现出优异的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancements in Large Vision Language Models (LVLMs) offer thepotential to surpass conventional labeling by generating richer, more detaileddescriptions of on-device human behavior understanding (HBU) in low-resolutionvision systems, such as depth, thermal, and infrared. However, existing largevision language model (LVLM) approaches are unable to understand low-resolutiondata well as they are primarily designed for high-resolution data, such as RGBimages. A quick fixing approach is to caption a large amount of low-resolutiondata, but it requires a significant amount of labor-intensive annotationefforts. In this paper, we propose a novel, labor-saving system, Llambda,designed to support low-resolution HBU. The core idea is to leverage limitedlabeled data and a large amount of unlabeled data to guide LLMs in generatinginformative captions, which can be combined with raw data to effectivelyfine-tune LVLM models for understanding low-resolution videos in HBU. First, wepropose a Contrastive-Oriented Data Labeler, which can capturebehavior-relevant information from long, low-resolution videos and generatehigh-quality pseudo labels for unlabeled data via contrastive learning. Second,we propose a Physical-Knowledge Guided Captioner, which utilizes spatial andtemporal consistency checks to mitigate errors in pseudo labels. Therefore, itcan improve LLMs' understanding of sequential data and then generatehigh-quality video captions. Finally, to ensure on-device deployability, weemploy LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.We evaluate Llambda using a region-scale real-world testbed and three distinctlow-resolution datasets, and the experiments show that Llambda outperformsseveral state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.</description>
      <author>example@mail.com (Siyang Jiang, Bufang Yang, Lilin Xu, Mu Yuan, Yeerzhati Abudunuer, Kaiwei Liu, Liekang Zeng, Hongkai Chen, Zhenyu Yan, Xiaofan Jiang, Guoliang Xing)</author>
      <guid isPermaLink="false">2505.01743v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Embracing Diffraction: A Paradigm Shift in Wireless Sensing and Communication</title>
      <link>http://arxiv.org/abs/2505.01625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将电磁衍射作为无线信号传播中的一个重要且未被充分利用的机制，并探讨了其在环境感知和通信中的应用。&lt;h4&gt;背景&lt;/h4&gt;无线信号在现代社会中至关重要，而电磁衍射现象通常被视为次要效应或校正因子。&lt;h4&gt;目的&lt;/h4&gt;通过理解和利用衍射效应，提高无线系统的感知能力和通信策略的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入了一个通用的优化框架，以形式化利用衍射的概念，并讨论了边缘衍射和凯勒的几何衍射理论（GTD）在射频感知和通信中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;衍射诱导的元素具有丰富的信息，可以揭示其几何特性等底层属性。&lt;h4&gt;结论&lt;/h4&gt;本文为将衍射系统地纳入未来无线系统的设计和运行提供了愿景，为增强感知能力和更鲁棒的通信策略铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：无线信号是现代社会不可或缺的一部分，它既支持通信，也越来越多地用于环境感知。尽管存在各种传播模型，从经验方法到全波模拟，电磁衍射现象通常被视为次要效应或校正因子。本文将衍射定位为一个基本重要且未被充分利用的机制，它富含关于物理环境的丰富信息。具体来说，衍射诱导的元素产生了丰富的特征，这些特征蕴含了它们底层属性（如几何形状）的信息。我们进一步论证，通过理解和利用这些关系，衍射可以被战略性地利用。我们引入了一个通用的优化框架来形式化这一概念，说明了如何利用衍射来解决逆问题（如从测量场中感知场景细节，如物体几何形状）和正问题（通过配置衍射元素来塑造射频场以实现通信目标）。主要关注边缘衍射和凯勒的几何衍射理论（GTD），我们讨论了在射频感知中用于场景理解以及在通信中用于射频场编程的具体应用，借鉴了最近的研究成果。总的来说，本文为将衍射系统地纳入未来无线系统的设计和运行提供了愿景，为增强感知能力和更鲁棒的通信策略铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless signals are integral to modern society, enabling both communicationand increasingly, environmental sensing. While various propagation modelsexist, ranging from empirical methods to full-wave simulations, the phenomenonof electromagnetic diffraction is often treated as a secondary effect or acorrection factor. This paper positions diffraction as a fundamentallyimportant and underutilized mechanism that is rich with information about thephysical environment. Specifically, diffraction-inducing elements generatedistinct signatures that are rich with information about their underlyingproperties such as their geometries. We then argue that by understanding andexploiting these relationships, diffraction can be harnessed strategically. Weintroduce a general optimization framework to formalize this concept,illustrating how diffraction can be leveraged for both inverse problems(sensing scene details such as object geometries from measured fields) andforward problems (shaping RF fields for communication objectives by configuringdiffracting elements). Focusing primarily on edge diffraction and Keller'sGeometrical Theory of Diffraction (GTD), we discuss specific applications in RFsensing for scene understanding and in communications for RF field programming,drawing upon recent work. Overall, this paper lays out a vision forsystematically incorporating diffraction into the design and operation offuture wireless systems, paving the way for enhanced sensing capabilities andmore robust communication strategies.</description>
      <author>example@mail.com (Anurag Pallaprolu, Winston Hurst, Yasamin Mostofi)</author>
      <guid isPermaLink="false">2505.01625v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning</title>
      <link>http://arxiv.org/abs/2505.02071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为COCA的紧凑聚类注意力层，用于对象中心表示学习，并解决单图像中的无监督对象发现任务。&lt;h4&gt;背景&lt;/h4&gt;在多对象场景中提取对象中心表示，并构建了COCA-Net，一个自下而上的层次网络架构。&lt;h4&gt;目的&lt;/h4&gt;提高对象检测和分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;COCA使用一种新的聚类算法，利用紧凑性的物理概念来突出场景中的不同对象质心，并提供空间归纳偏差。&lt;h4&gt;主要发现&lt;/h4&gt;COCA-Net在解码器和编码器端都生成高质量的分割掩码，且不受预定义对象掩码数量的限制，比竞争对手更好地处理背景元素的分割。&lt;h4&gt;结论&lt;/h4&gt;在六个广泛采用的数据集上，COCA-Net在九个不同的评估指标上实现了优于或与最先进模型相当的结果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种紧凑聚类注意力层（COCA），这是一种有效的构建块，它引入了一种面向对象表示学习的高级策略，同时解决了单图像中的无监督对象发现任务。COCA是一个基于注意力的聚类模块，能够从多对象场景中提取对象中心表示，当级联到自下而上的层次网络架构中时，被称为COCA-Net。在其核心，COCA利用了一种新的聚类算法，该算法利用紧凑性的物理概念来突出场景中的不同对象质心，从而提供空间归纳偏差。多亏了这种策略，COCA-Net在其管道的解码器和编码器端都生成了高质量的分割掩码。此外，COCA-Net不受它生成的预定义对象掩码数量的限制，并且比竞争对手更好地处理背景元素的分割。我们在六个广泛采用的数据集上展示了COCA-Net的分割性能，在九个不同的评估指标上与最先进模型相比实现了优越或竞争性的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Compact Clustering Attention (COCA) layer, an effectivebuilding block that introduces a hierarchical strategy for object-centricrepresentation learning, while solving the unsupervised object discovery taskon single images. COCA is an attention-based clustering module capable ofextracting object-centric representations from multi-object scenes, whencascaded into a bottom-up hierarchical network architecture, referred to asCOCA-Net. At its core, COCA utilizes a novel clustering algorithm thatleverages the physical concept of compactness, to highlight distinct objectcentroids in a scene, providing a spatial inductive bias. Thanks to thisstrategy, COCA-Net generates high-quality segmentation masks on both thedecoder side and, notably, the encoder side of its pipeline. Additionally,COCA-Net is not bound by a predetermined number of object masks that itgenerates and handles the segmentation of background elements better than itscompetitors. We demonstrate COCA-Net's segmentation performance on six widelyadopted datasets, achieving superior or competitive results against thestate-of-the-art models across nine different evaluation metrics.</description>
      <author>example@mail.com (Can Küçüksözen, Yücel Yemez)</author>
      <guid isPermaLink="false">2505.02071v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</title>
      <link>http://arxiv.org/abs/2505.02707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 7 figures, Website: https://voila.maitrix.org&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Voila是一个大型语音语言基础模型系列，旨在实现一个能够无缝融入日常生活、自主、实时且情感表达的语音AI代理。&lt;h4&gt;背景&lt;/h4&gt;目前语音AI代理主要依赖于命令反应，缺乏自主性和情感表达。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现流畅、动态和情感共鸣的交互的语音AI代理。&lt;h4&gt;方法&lt;/h4&gt;Voila采用了一种新的端到端架构，实现了全双工、低延迟的对话，并保留了丰富的语音细微差别。它集成了大型语言模型（LLMs）的推理能力和强大的声学建模，支持自然、个性化的语音生成。此外，Voila支持超过一百万个预构建的语音和基于简短音频样本的高效定制。&lt;h4&gt;主要发现&lt;/h4&gt;Voila实现了195毫秒的响应延迟，超过了人类的平均反应时间。它能够通过文本指令定义说话者的身份、语气和其他特征，支持自动语音识别（ASR）、文本到语音（TTS）和多语言语音翻译。&lt;h4&gt;结论&lt;/h4&gt;Voila是一个统一模型，适用于各种基于语音的应用，并且是完全开源的，以支持开放研究和加速下一代人机交互的进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要：一个能够无缝融入日常生活的语音AI代理将以自主、实时和情感表达的方式与人类互动。它不仅会响应命令，还会持续监听、推理并主动响应，从而促进流畅、动态和情感共鸣的交互。我们介绍了Voila，这是一系列大型语音语言基础模型，朝着这个愿景迈出了第一步。Voila超越了传统的管道系统，采用了一种新的端到端架构，实现了全双工、低延迟的对话，同时保留了丰富的语音细微差别，如音调、节奏和情感。它实现了仅195毫秒的响应延迟，超过了人类的平均反应时间。其分层多尺度Transformer集成了大型语言模型（LLMs）的推理能力和强大的声学建模，实现了自然、个性化的语音生成——用户可以通过简单的文本指令来定义说话者的身份、语气和其他特征。此外，Voila支持超过一百万个预构建的语音和基于10秒短音频样本的高效定制。除了语音对话外，Voila还被设计为一个统一模型，适用于广泛的基于语音的应用，包括自动语音识别（ASR）、文本到语音（TTS）和，经过最小适应，多语言语音翻译。Voila是完全开源的，以支持开放研究和加速向下一代人机交互的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A voice AI agent that blends seamlessly into daily life would interact withhumans in an autonomous, real-time, and emotionally expressive manner. Ratherthan merely reacting to commands, it would continuously listen, reason, andrespond proactively, fostering fluid, dynamic, and emotionally resonantinteractions. We introduce Voila, a family of large voice-language foundationmodels that make a step towards this vision. Voila moves beyond traditionalpipeline systems by adopting a new end-to-end architecture that enablesfull-duplex, low-latency conversations while preserving rich vocal nuances suchas tone, rhythm, and emotion. It achieves a response latency of just 195milliseconds, surpassing the average human response time. Its hierarchicalmulti-scale Transformer integrates the reasoning capabilities of large languagemodels (LLMs) with powerful acoustic modeling, enabling natural, persona-awarevoice generation -- where users can simply write text instructions to definethe speaker's identity, tone, and other characteristics. Moreover, Voilasupports over one million pre-built voices and efficient customization of newones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,Voila is designed as a unified model for a wide range of voice-basedapplications, including automatic speech recognition (ASR), Text-to-Speech(TTS), and, with minimal adaptation, multilingual speech translation. Voila isfully open-sourced to support open research and accelerate progress towardnext-generation human-machine interactions.</description>
      <author>example@mail.com (Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, Zhiting Hu)</author>
      <guid isPermaLink="false">2505.02707v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Robustness questions the interpretability of graph neural networks: what to do?</title>
      <link>http://arxiv.org/abs/2505.02566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出一个全面的基准，用于系统分析影响图神经网络（GNN）可解释性的各种因素，包括增强鲁棒性的防御机制的影响。&lt;h4&gt;背景&lt;/h4&gt;GNN在生物信息学、社交网络和推荐系统等领域得到广泛应用，但其模型可解释性与鲁棒性之间的相互作用在对抗性场景（如中毒和逃避攻击）下理解不足。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过防御中毒和逃避攻击来影响GNN的可解释性，并强调鲁棒性与可解释性之间的关键权衡。&lt;h4&gt;方法&lt;/h4&gt;评估基于GCN、SAGE、GIN和GAT的六种GNN架构，在两个不同领域的五个数据集上使用四个可解释性指标（保真度、稳定性、一致性和稀疏性）。&lt;h4&gt;主要发现&lt;/h4&gt;根据选择的防御方法和模型架构特征，可解释性存在显著差异。&lt;h4&gt;结论&lt;/h4&gt;通过建立标准化基准，本研究为开发既具有鲁棒性又可解释的GNN提供了基础，有助于在敏感应用中部署这些模型时建立信任。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms. We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across five datasets from two distinct domains, employing four interpretability metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how defenses against poisoning and evasion attacks, applied before and during model training, affect interpretability and highlights critical trade-offs between robustness and interpretability. The framework will be published as open source. The results reveal significant variations in interpretability depending on the chosen defense methods and model architecture characteristics. By establishing a standardized benchmark, this work provides a foundation for developing GNNs that are both robust to adversarial threats and interpretable, facilitating trust in their deployment in sensitive applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a cornerstone in graph-based dataanalysis, with applications in diverse domains such as bioinformatics, socialnetworks, and recommendation systems. However, the interplay between modelinterpretability and robustness remains poorly understood, especially underadversarial scenarios like poisoning and evasion attacks. This paper presents acomprehensive benchmark to systematically analyze the impact of various factorson the interpretability of GNNs, including the influence ofrobustness-enhancing defense mechanisms.  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT acrossfive datasets from two distinct domains, employing four interpretabilitymetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines howdefenses against poisoning and evasion attacks, applied before and during modeltraining, affect interpretability and highlights critical trade-offs betweenrobustness and interpretability. The framework will be published as opensource.  The results reveal significant variations in interpretability depending onthe chosen defense methods and model architecture characteristics. Byestablishing a standardized benchmark, this work provides a foundation fordeveloping GNNs that are both robust to adversarial threats and interpretable,facilitating trust in their deployment in sensitive applications.</description>
      <author>example@mail.com (Kirill Lukyanov, Georgii Sazonov, Serafim Boyarsky, Ilya Makarov)</author>
      <guid isPermaLink="false">2505.02566v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach</title>
      <link>http://arxiv.org/abs/2505.01947v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 29th International Conference on Engineering of  Complex Computer Systems (ICECCS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RADD，一种结合规则挖掘和无监督学习的无人机异常检测方法。&lt;h4&gt;背景&lt;/h4&gt;无人机（UAVs）因其多用途而日益受到关注，但传感器故障可能导致物理不稳定和安全问题。&lt;h4&gt;目的&lt;/h4&gt;为了减少这些风险，本文旨在提出一种能够检测无人机异常并允许操作者在运行时采取预防措施的方法。&lt;h4&gt;方法&lt;/h4&gt;RADD结合了规则挖掘和无监督学习，利用规则捕获传感器和执行器之间的预期关系，并使用无监督学习技术覆盖规则可能遗漏的微妙关系。&lt;h4&gt;主要发现&lt;/h4&gt;RADD在Gazebo模拟器中实现了基于ArduPilot无人机软件的方法，使用44条规则和五个无监督学习模型，成功检测了93.84%的异常，错误警报率低（2.33%）。&lt;h4&gt;结论&lt;/h4&gt;RADD在检测无人机故障方面优于基于LSTM的最新方法，并且可以有效地在运行时部署。&lt;h4&gt;翻译&lt;/h4&gt;UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; UAVs, commonly referred to as drones, have witnessed a remarkable surge inpopularity due to their versatile applications. These cyber-physical systemsdepend on multiple sensor inputs, such as cameras, GPS receivers,accelerometers, and gyroscopes, with faults potentially leading to physicalinstability and serious safety concerns. To mitigate such risks, anomalydetection has emerged as a crucial safeguarding mechanism, capable ofidentifying the physical manifestations of emerging issues and allowingoperators to take preemptive action at runtime. Recent anomaly detectionmethods based on LSTM neural networks have shown promising results, but threechallenges persist: the need for models that can generalise across the diversemission profiles of drones; the need for interpretability, enabling operatorsto understand the nature of detected problems; and the need for capturingdomain knowledge that is difficult to infer solely from log data. Motivated bythese challenges, this paper introduces RADD, an integrated approach to anomalydetection in drones that combines rule mining and unsupervised learning. Inparticular, we leverage rules (or invariants) to capture expected relationshipsbetween sensors and actuators during missions, and utilise unsupervisedlearning techniques to cover more subtle relationships that the rules may havemissed. We implement this approach using the ArduPilot drone software in theGazebo simulator, utilising 44 rules derived across the main phases of dronemissions, in conjunction with an ensemble of five unsupervised learning models.We find that our integrated approach successfully detects 93.84% of anomaliesover six types of faults with a low false positive rate (2.33%), and can bedeployed effectively at runtime. Furthermore, RADD outperforms astate-of-the-art LSTM-based method in detecting the different types of faultsevaluated in our study.</description>
      <author>example@mail.com (Ivan Tan, Wei Minn, Christopher M. Poskitt, Lwin Khin Shar, Lingxiao Jiang)</author>
      <guid isPermaLink="false">2505.01947v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</title>
      <link>http://arxiv.org/abs/2505.02677v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了利用视网膜图像和临床数据对中风进行检测和风险评估的影响。&lt;h4&gt;背景&lt;/h4&gt;中风是全球主要的公共卫生问题，深度学习在提高中风诊断和风险评估方面显示出潜力，但现有方法依赖于昂贵的医学成像技术。&lt;h4&gt;目的&lt;/h4&gt;研究视网膜成像在识别高风险患者和改善长期预后方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多模态深度神经网络，处理光学相干断层扫描（OCT）和红外反射视网膜扫描，并结合临床数据，如人口统计学、生命体征和诊断代码。使用自监督学习框架和包含37k扫描的现实世界数据集进行预训练，然后使用较小的标记子集进行微调和评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与仅使用单模态图像的基线相比，该框架实现了5%的AUROC提升，与现有最先进的基础模型相比提高了8%。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了视网膜成像在识别高风险患者和改善长期预后方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：中风是全球主要的公共卫生问题，影响着数百万人的健康。深度学习最近在提高中风诊断和风险评估方面显示出希望。然而，现有方法依赖于昂贵的医学成像技术，如计算机断层扫描。最近的研究表明，视网膜成像可以作为一种成本效益高的替代方案，用于评估脑血管健康，因为视网膜和大脑之间存在共同的临床途径。因此，本研究探讨了利用视网膜图像和临床数据对中风检测和风险评估的影响。我们提出了一种多模态深度神经网络，处理光学相干断层扫描（OCT）和红外反射视网膜扫描，并结合临床数据，如人口统计学、生命体征和诊断代码。我们使用自监督学习框架和包含37k扫描的现实世界数据集进行预训练，然后使用较小的标记子集进行微调和评估。我们的实证发现确立了所考虑模态在检测与急性中风相关的视网膜持久效应和预测特定时间范围内的未来风险方面的预测能力。实验结果表明，与仅使用单模态图像的基线相比，我们的提议框架实现了5%的AUROC提升，与现有最先进的基础模型相比提高了8%。总之，我们的研究突出了视网膜成像在识别高风险患者和改善长期预后方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stroke is a major public health problem, affecting millions worldwide. Deeplearning has recently demonstrated promise for enhancing the diagnosis and riskprediction of stroke. However, existing methods rely on costly medical imagingmodalities, such as computed tomography. Recent studies suggest that retinalimaging could offer a cost-effective alternative for cerebrovascular healthassessment due to the shared clinical pathways between the retina and thebrain. Hence, this study explores the impact of leveraging retinal images andclinical data for stroke detection and risk prediction. We propose a multimodaldeep neural network that processes Optical Coherence Tomography (OCT) andinfrared reflectance retinal scans, combined with clinical data, such asdemographics, vital signs, and diagnosis codes. We pretrained our model using aself-supervised learning framework using a real-world dataset consisting of$37$ k scans, and then fine-tuned and evaluated the model using a smallerlabeled subset. Our empirical findings establish the predictive ability of theconsidered modalities in detecting lasting effects in the retina associatedwith acute stroke and forecasting future risk within a specific time horizon.The experimental results demonstrate the effectiveness of our proposedframework by achieving $5$\% AUROC improvement as compared to the unimodalimage-only baseline, and $8$\% improvement compared to an existingstate-of-the-art foundation model. In conclusion, our study highlights thepotential of retinal imaging in identifying high-risk patients and improvinglong-term outcomes.</description>
      <author>example@mail.com (Saeed Shurrab, Aadim Nepal, Terrence J. Lee-St. John, Nicola G. Ghazi, Bartlomiej Piechowski-Jozwiak, Farah E. Shamout)</author>
      <guid isPermaLink="false">2505.02677v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction</title>
      <link>http://arxiv.org/abs/2505.02628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepSparse的深度学习模型，用于稀疏视图锥束CT（CBCT）重建，以减少辐射暴露，并通过实验证明其优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;CBCT是医学领域的重要3D成像技术，但高质量成像所需的辐射暴露对易受伤害的人群引起担忧。稀疏视图重建通过使用较少的X射线投影来减少辐射，但现有方法面临计算需求高和泛化性差的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的深度学习模型DeepSparse，以解决现有稀疏视图CBCT重建方法的问题，并提高重建质量。&lt;h4&gt;方法&lt;/h4&gt;DeepSparse模型采用DiCE网络，该网络整合了多视图2D特征和多尺度3D特征。此外，引入了HyViP框架进行预训练，并在新数据集上进行两步微调。&lt;h4&gt;主要发现&lt;/h4&gt;DeepSparse在重建质量上优于现有方法，为更安全、更高效的CBCT成像铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;DeepSparse模型能够有效减少CBCT成像的辐射暴露，同时保持高质量的重建图像。&lt;h4&gt;翻译&lt;/h4&gt;Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in the medical field, while the high radiation exposure required for high-quality imaging raises significant concerns, particularly for vulnerable populations. Sparse-view reconstruction reduces radiation by using fewer X-ray projections while maintaining image quality, yet existing methods face challenges such as high computational demands and poor generalizability to different datasets. To overcome these limitations, we propose DeepSparse, the first foundation model for sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional Cross-Scale Embedding), a novel network that integrates multi-view 2D features and multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View Sampling Pretraining) framework, which pretrains the model on large datasets with both sparse-view and dense-view projections, and a two-step finetuning strategy to adapt and refine the model for new datasets. Extensive experiments and ablation studies demonstrate that our proposed DeepSparse achieves superior reconstruction quality compared to state-of-the-art methods, paving the way for safer and more efficient CBCT imaging.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cone-beam computed tomography (CBCT) is a critical 3D imaging technology inthe medical field, while the high radiation exposure required for high-qualityimaging raises significant concerns, particularly for vulnerable populations.Sparse-view reconstruction reduces radiation by using fewer X-ray projectionswhile maintaining image quality, yet existing methods face challenges such ashigh computational demands and poor generalizability to different datasets. Toovercome these limitations, we propose DeepSparse, the first foundation modelfor sparse-view CBCT reconstruction, featuring DiCE (Dual-DimensionalCross-Scale Embedding), a novel network that integrates multi-view 2D featuresand multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid ViewSampling Pretraining) framework, which pretrains the model on large datasetswith both sparse-view and dense-view projections, and a two-step finetuningstrategy to adapt and refine the model for new datasets. Extensive experimentsand ablation studies demonstrate that our proposed DeepSparse achieves superiorreconstruction quality compared to state-of-the-art methods, paving the way forsafer and more efficient CBCT imaging.</description>
      <author>example@mail.com (Yiqun Lin, Hualiang Wang, Jixiang Chen, Jiewen Yang, Jiarong Guo, Xiaomeng Li)</author>
      <guid isPermaLink="false">2505.02628v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition</title>
      <link>http://arxiv.org/abs/2505.01783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为C-PP-COAD的上下文感知预测驱动的在线异常检测框架，用于解决网络安全、医疗保健和工业监控等领域中实时识别行为偏差的问题。&lt;h4&gt;背景&lt;/h4&gt;在线异常检测在网络安全、医疗保健和工业监控等领域至关重要，这些领域需要及时识别偏差以避免关键故障或安全漏洞。&lt;h4&gt;目的&lt;/h4&gt;为了解决有限的真实校准数据带来的挑战，本文旨在提出一种新的异常检测方法，以减少对真实校准数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;C-PP-COAD框架利用合成校准数据来缓解数据稀缺问题，并基于上下文线索自适应地整合真实数据。它使用符合性p值、主动p值统计和在线FDR控制机制来保持严格的异常检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的实验表明，C-PP-COAD显著减少了对外部校准数据的依赖，同时保证了FDR控制的可靠性。&lt;h4&gt;结论&lt;/h4&gt;C-PP-COAD是一种有效的在线异常检测方法，能够减少对真实校准数据的依赖，同时保持对FDR的保证控制。&lt;h4&gt;翻译&lt;/h4&gt;Online anomaly detection is essential in fields such as cybersecurity, healthcare, and industrial monitoring, where promptly identifying deviations from expected behavior can avert critical failures or security breaches. While numerous anomaly scoring methods based on supervised or unsupervised learning have been proposed, current approaches typically rely on a continuous stream of real-world calibration data to provide assumption-free guarantees on the false discovery rate (FDR). To address the inherent challenges posed by limited real calibration data, we introduce context-aware prediction-powered conformal online anomaly detection (C-PP-COAD). Our framework strategically leverages synthetic calibration data to mitigate data scarcity, while adaptively integrating real data based on contextual cues. C-PP-COAD utilizes conformal p-values, active p-value statistics, and online FDR control mechanisms to maintain rigorous and reliable anomaly detection performance over time. Experiments conducted on both synthetic and real-world datasets demonstrate that C-PP-COAD significantly reduces dependency on real calibration data without compromising guaranteed FDR control.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online anomaly detection is essential in fields such as cybersecurity,healthcare, and industrial monitoring, where promptly identifying deviationsfrom expected behavior can avert critical failures or security breaches. Whilenumerous anomaly scoring methods based on supervised or unsupervised learninghave been proposed, current approaches typically rely on a continuous stream ofreal-world calibration data to provide assumption-free guarantees on the falsediscovery rate (FDR). To address the inherent challenges posed by limited realcalibration data, we introduce context-aware prediction-powered conformalonline anomaly detection (C-PP-COAD). Our framework strategically leveragessynthetic calibration data to mitigate data scarcity, while adaptivelyintegrating real data based on contextual cues. C-PP-COAD utilizes conformalp-values, active p-value statistics, and online FDR control mechanisms tomaintain rigorous and reliable anomaly detection performance over time.Experiments conducted on both synthetic and real-world datasets demonstratethat C-PP-COAD significantly reduces dependency on real calibration datawithout compromising guaranteed FDR control.</description>
      <author>example@mail.com (Amirmohammad Farzaneh, Osvaldo Simeone)</author>
      <guid isPermaLink="false">2505.01783v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks</title>
      <link>http://arxiv.org/abs/2505.02022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了NbBench，第一个用于纳米抗体表征学习的全面基准套件，旨在提升纳米抗体建模的标准化和可重复性。&lt;h4&gt;背景&lt;/h4&gt;纳米抗体作为单域抗体片段，具有体积小、稳定性高和结合亲和力强的特点，在治疗和诊断领域具有潜在价值。&lt;h4&gt;目的&lt;/h4&gt;为了解决纳米抗体特定建模未充分探索且缺乏统一基准的问题，引入NbBench。&lt;h4&gt;方法&lt;/h4&gt;NbBench包含八个生物意义明确的任务，涵盖结构注释、结合预测和开发性评估，并对十一种代表性模型进行了系统评估。&lt;h4&gt;主要发现&lt;/h4&gt;抗体语言模型在抗原相关任务中表现优秀，但在回归任务（如热稳定性和亲和力）上所有模型的性能都面临挑战，没有单一模型在所有任务中都优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;NbBench通过标准化数据集、任务定义和评估协议，为评估和推进纳米抗体建模提供了一个可重复的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nanobodies, single-domain antibody fragments derived from camelidheavy-chain-only antibodies, exhibit unique advantages such as compact size,high stability, and strong binding affinity, making them valuable tools intherapeutics and diagnostics. While recent advances in pretrained protein andantibody language models (PPLMs and PALMs) have greatly enhanced biomolecularunderstanding, nanobody-specific modeling remains underexplored and lacks aunified benchmark. To address this gap, we introduce NbBench, the firstcomprehensive benchmark suite for nanobody representation learning. Spanningeight biologically meaningful tasks across nine curated datasets, NbBenchencompasses structure annotation, binding prediction, and developabilityassessment. We systematically evaluate eleven representative models--includinggeneral-purpose protein LMs, antibody-specific LMs, and nanobody-specificLMs--in a frozen setting. Our analysis reveals that antibody language modelsexcel in antigen-related tasks, while performance on regression tasks such asthermostability and affinity remains challenging across all models. Notably, nosingle model consistently outperforms others across all tasks. By standardizingdatasets, task definitions, and evaluation protocols, NbBench offers areproducible foundation for assessing and advancing nanobody modeling.</description>
      <author>example@mail.com (Yiming Zhang, Koji Tsuda)</author>
      <guid isPermaLink="false">2505.02022v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Low-Complexity Acoustic Scene Classification with Device Information in the DCASE 2025 Challenge</title>
      <link>http://arxiv.org/abs/2505.01747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Task Description Page:  https://dcase.community/challenge2025/task-low-complexity-acoustic-scene-classification-with-device-information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了DCASE 2025挑战赛中低复杂度声场景分类任务及其基线系统，继续关注低复杂度模型、数据效率和设备不匹配问题，并引入了新的关键变化。&lt;h4&gt;背景&lt;/h4&gt;前几届DCASE挑战赛（2022-2024年）已经关注了低复杂度模型、数据效率和设备不匹配问题。&lt;h4&gt;目的&lt;/h4&gt;开发设备特定的模型，利用设备特性，反映实际部署场景中模型对底层硬件的了解。&lt;h4&gt;方法&lt;/h4&gt;今年的任务引入了新的变化，即在推理时提供记录设备信息。训练集与DCASE 2024挑战赛使用的25%子集相匹配，没有对外部数据使用的限制，突出了迁移学习的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;基线系统在十个类别的任务中，使用设备通用模型达到了50.72%的准确率，当使用可用的设备信息时，准确率提高到了51.89%。&lt;h4&gt;结论&lt;/h4&gt;通过提供设备信息，可以开发出更适应特定设备的模型，从而提高声场景分类的准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Low-Complexity Acoustic Scene Classification withDevice Information Task of the DCASE 2025 Challenge and its baseline system.Continuing the focus on low-complexity models, data efficiency, and devicemismatch from previous editions (2022--2024), this year's task introduces a keychange: recording device information is now provided at inference time. Thisenables the development of device-specific models that leverage devicecharacteristics -- reflecting real-world deployment scenarios in which a modelis designed with awareness of the underlying hardware. The training set matchesthe 25% subset used in the corresponding DCASE 2024 challenge, with norestrictions on external data use, highlighting transfer learning as a centraltopic. The baseline achieves 50.72% accuracy on this ten-class problem with adevice-general model, improving to 51.89% when using the available deviceinformation.</description>
      <author>example@mail.com (Florian Schmid, Paul Primus, Toni Heittola, Annamaria Mesaros, Irene Martín-Morató, Gerhard Widmer)</author>
      <guid isPermaLink="false">2505.01747v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction</title>
      <link>http://arxiv.org/abs/2505.02043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从点云数据直接预测并重建CAD模型的网络方法，通过改进的Transformer直接检测和预测草图曲线，重建拓扑结构并优化曲线参数，实现高精度的CAD模型重建。&lt;h4&gt;背景&lt;/h4&gt;现有的CAD模型重建方法通常使用隐式场来表示草图，导致曲线形状的重建精度不高。&lt;h4&gt;目的&lt;/h4&gt;提高从点云数据重建CAD模型的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Point2Primitive的CAD重建网络，该网络通过直接预测扩展原语的所有元素来生成可编辑的CAD模型。使用改进的Transformer直接从点云中检测和预测草图曲线，并将草图曲线参数作为位置查询进行自回归优化，重建拓扑结构并通过结合预测曲线和计算出的扩展操作来恢复每个扩展参数。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在原语预测精度和CAD重建方面表现出色，重建的形状具有较高的几何精度。&lt;h4&gt;结论&lt;/h4&gt;该方法为从点云数据重建CAD模型提供了一种有效且准确的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：从点云恢复CAD模型，尤其是草图扩展过程，可以看作是重建拓扑结构和扩展原语的过程。先前的方法利用隐式场来表示草图，导致曲线形状的重建。在本文中，我们提出了一种CAD重建网络（Point2Primitive），它通过直接预测扩展原语的每个元素从输入点云生成可编辑的CAD模型。Point2Primitive基于改进的Transformer直接从点云中检测和预测草图曲线（类型和参数）。草图曲线参数被表示为位置查询并以自回归方式进行优化，从而实现高参数精度。拓扑结构通过扩展分割重建，每个扩展参数（草图和扩展操作）通过结合预测曲线和计算出的扩展操作来恢复。大量实验表明，我们的方法在原语预测精度和CAD重建方面优于现有方法。重建的形状具有高几何保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering CAD models from point clouds, especially the sketch-extrusionprocess, can be seen as the process of rebuilding the topology and extrusionprimitives. Previous methods utilize implicit fields for sketch representation,leading to shape reconstruction of curved edges. In this paper, we proposed aCAD reconstruction network that produces editable CAD models from input pointclouds (Point2Primitive) by directly predicting every element of the extrusionprimitives. Point2Primitive can directly detect and predict sketch curves (typeand parameter) from point clouds based on an improved transformer. The sketchcurve parameters are formulated as position queries and optimized in anautoregressive way, leading to high parameter accuracy. The topology is rebuiltby extrusion segmentation, and each extrusion parameter (sketch and extrusionoperation) is recovered by combining the predicted curves and the computedextrusion operation. Extensive experiments demonstrate that our method issuperior in primitive prediction accuracy and CAD reconstruction. Thereconstructed shapes are of high geometrical fidelity.</description>
      <author>example@mail.com (Cheng Wang, Xinzhu Ma, Bin Wang, Shixiang Tang, Yuan Meng, Ping Jiang)</author>
      <guid isPermaLink="false">2505.02043v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>MolQAE: Quantum Autoencoder for Molecular Representation Learning</title>
      <link>http://arxiv.org/abs/2505.01875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为量子分子自动编码器的新方法，该方法将量子计算与分子表示学习相结合，旨在解决传统分子表示方法在处理高维数据时遇到的计算瓶颈。&lt;h4&gt;背景&lt;/h4&gt;传统分子表示方法在处理高维数据时存在计算瓶颈，而量子计算通过其固有的并行性和量子叠加特性提供了有希望的替代方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于量子电路的自动编码器架构，将SMILES分子表示映射到量子状态空间，并利用SWAP测试评估编码质量。&lt;h4&gt;方法&lt;/h4&gt;量子自动编码器架构通过参数化量子电路进行降维，并使用SWAP测试来评估编码质量。&lt;h4&gt;主要发现&lt;/h4&gt;理论研究表明，该方法可以在指数级较小的空间中保留分子特征，同时保持分子之间的相似性关系。实验结果表明，量子自动编码器有效地捕捉了分子结构和化学性质。&lt;h4&gt;结论&lt;/h4&gt;该框架不仅为分子表示学习建立了量子途径，也为药物发现和材料设计开辟了新的可能性。作为分子表示学习与量子计算交叉领域的首次研究，本研究为化学信息学的发展奠定了理论和实践基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为量子分子自动编码器的新方法，该方法将量子计算与分子表示学习相结合。传统分子表示方法在处理高维数据时存在计算瓶颈，而量子计算通过其固有的并行性和量子叠加特性提供了有希望的替代方案。本文提出了一种基于量子电路的自动编码器架构，将SMILES分子表示映射到量子状态空间，并利用SWAP测试评估编码质量。理论研究表明，该方法可以在指数级较小的空间中保留分子特征，同时保持分子之间的相似性关系。实验结果表明，量子自动编码器有效地捕捉了分子结构和化学性质。该框架不仅为分子表示学习建立了量子途径，也为药物发现和材料设计开辟了新的可能性。作为分子表示学习与量子计算交叉领域的首次研究，本研究为化学信息学的发展奠定了理论和实践基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces the Quantum Molecular Autoencoder, a novel approachthat integrates quantum computing with molecular representation learning. Whileconventional molecular representation methods face computational bottleneckswhen processing high-dimensional data, quantum computing offers a promisingalternative through its inherent parallelism and quantum superpositionproperties. We present a quantum circuit-based autoencoder architecture thatmaps SMILES molecular representations into quantum state space, employsparameterized quantum circuits for dimensional reduction, and utilizes SWAPtests to evaluate encoding quality. Theoretically, our approach preservesessential molecular features in exponentially smaller spaces while maintainingsimilarity relationships between molecules. Experimental results demonstratethat quantum autoencoders effectively capture molecular structures and chemicalproperties. The proposed framework not only establishes a quantum pathway formolecular representation learning but also opens new possibilities forapplications in drug discovery and materials design. As the first investigationat the intersection of molecular representation learning and quantum computing,this research lays both theoretical and practical foundations for theadvancement of cheminformatics.</description>
      <author>example@mail.com (Yi Pan, Hanqi Jiang, Wei Ruan, Dajiang Zhu, Xiang Li, Yohannes Abate, Yingfeng Wang, Tianming Liu)</author>
      <guid isPermaLink="false">2505.01875v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments</title>
      <link>http://arxiv.org/abs/2505.01632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的神经网络框架，用于解决非平稳环境噪声对自动语音识别的影响。&lt;h4&gt;背景&lt;/h4&gt;非平稳环境噪声对自动语音识别系统的影响是一个持续且重要的研究焦点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的神经网络框架，以克服非平稳环境噪声带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;该方法结合了鲁棒的前端处理，并在Aurora-2语音数据库上使用基于ResNet的迁移学习方法，对Mel频率声学特征集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与卷积神经网络和长短期记忆网络相比，该方法在识别准确率上有显著提升，在干净和噪声环境下的准确率分别为98.94%和91.21%。&lt;h4&gt;结论&lt;/h4&gt;该神经网络框架在自动语音识别中有效提高了识别准确率，特别是在噪声环境下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICTIS62692.2024.10894239&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Addressing the detrimental impact of non-stationary environmental noise onautomatic speech recognition (ASR) has been a persistent and significantresearch focus. Despite advancements, this challenge continues to be a majorconcern. Recently, data-driven supervised approaches, such as deep neuralnetworks, have emerged as promising alternatives to traditional unsupervisedmethods. With extensive training, these approaches have the potential toovercome the challenges posed by diverse real-life acoustic environments. Inthis light, this paper introduces a novel neural framework that incorporates arobust frontend into ASR systems in both clean and noisy environments.Utilizing the Aurora-2 speech database, the authors evaluate the effectivenessof an acoustic feature set for Mel-frequency, employing the approach oftransfer learning based on Residual neural network (ResNet). The experimentalresults demonstrate a significant improvement in recognition accuracy comparedto convolutional neural networks (CNN) and long short-term memory (LSTM)networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.</description>
      <author>example@mail.com (Noussaiba Djeffal, Djamel Addou, Hamza Kheddar, Sid Ahmed Selouani)</author>
      <guid isPermaLink="false">2505.01632v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2505.01969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages of main text, 3 pages of appendix, accepted to IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的多类别3D异常检测（MC3D-AD）模型，旨在利用局部和全局几何感知信息来重建所有类别的正常表示，以提高检测效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的3D异常检测方法通常需要针对每个类别独立训练模型，导致成本高、效率低和泛化能力弱。&lt;h4&gt;目的&lt;/h4&gt;设计一个统一的模型，实现多类别3D异常检测，利用局部和全局几何感知信息来重建正常表示。&lt;h4&gt;方法&lt;/h4&gt;1. 提出自适应几何感知掩码注意力模块，提取几何变化信息以引导掩码注意力；2. 引入局部几何感知编码器，增强掩码注意力以编码组级特征标记；3. 设计全局查询解码器，利用点云位置嵌入来改善解码过程和重建能力。&lt;h4&gt;主要发现&lt;/h4&gt;MC3D-AD在Real3D-AD和Anomaly-ShapeNet数据集上评估，在对象级AUROC上分别比现有单类别方法提高了3.1%和9.3%。&lt;h4&gt;结论&lt;/h4&gt;MC3D-AD模型在多类别3D异常检测任务中表现出显著优越性，能够有效提高检测效率和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;3D Anomaly Detection (AD) is a promising means of controlling the quality of manufactured products. However, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. Therefore, this paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. First, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. Then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. Finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. This leads to local and global geometry-aware reconstructed feature tokens for the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and Anomaly-ShapeNet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1% and 9.3% improvement in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The source code will be released upon acceptance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Anomaly Detection (AD) is a promising means of controlling the quality ofmanufactured products. However, existing methods typically require carefullytraining a task-specific model for each category independently, leading to highcost, low efficiency, and weak generalization. Therefore, this paper presents anovel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aimsto utilize both local and global geometry-aware information to reconstructnormal representations of all categories. First, to learn robust andgeneralized features of different categories, we propose an adaptivegeometry-aware masked attention module that extracts geometry variationinformation to guide mask attention. Then, we introduce a local geometry-awareencoder reinforced by the improved mask attention to encode group-level featuretokens. Finally, we design a global query decoder that utilizes point cloudposition embeddings to improve the decoding process and reconstruction ability.This leads to local and global geometry-aware reconstructed feature tokens forthe AD task. MC3D-AD is evaluated on two publicly available Real3D-AD andAnomaly-ShapeNet datasets, and exhibits significant superiority over currentstate-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvementin object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. Thesource code will be released upon acceptance.</description>
      <author>example@mail.com (Jiayi Cheng, Can Gao, Jie Zhou, Jiajun Wen, Tao Dai, Jinbao Wang)</author>
      <guid isPermaLink="false">2505.01969v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Machine-learning interatomic potentials from a users perspective: A comparison of accuracy, speed and data efficiency</title>
      <link>http://arxiv.org/abs/2505.02503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习原子间势（MLIPs）显著改变了原子建模领域，提高了密度泛函理论在大规模模拟中的精度，同时接近经典原子间势的运行速度。&lt;h4&gt;背景&lt;/h4&gt;近年来，开发了多种类型的MLIPs，但对于特定问题设置，判断哪种方法最佳往往很困难。&lt;h4&gt;目的&lt;/h4&gt;针对结构和化学上复杂的固体（如Al-Cu-Zr和Si-O），比较了多种MLIPs方法，包括GAP、HDNNP、MTP、线性和非线性ACE、NequIP、Allegro和MACE。&lt;h4&gt;方法&lt;/h4&gt;进行了基准测试，并分析了非线性ACE、NequIP和MACE在精度与计算成本之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;非线性ACE、NequIP和MACE在精度与计算成本之间达到Pareto最优。对于Al-Cu-Zr系统，MACE和Allegro提供最高精度，而NequIP在Si-O系统中表现更优。GPU可以大幅加速MLIPs，使其在可访问的时间尺度上与未加速的经典原子间势相当甚至更优。&lt;h4&gt;结论&lt;/h4&gt;探讨了相应势的外推行为，研究了势能表面的平滑性，并评估了相应的拟合代码和分子动力学界面的用户友好性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：机器学习原子间势（MLIPs）极大地改变了原子建模领域。它们在大型模拟中提高了密度泛函理论的精度，同时几乎与经典原子间势一样快。在过去的几年里，开发了各种类型的MLIPs，但对于给定的问题设置，判断哪种方法最佳往往很困难。对于结构和化学上复杂的固体，即Al-Cu-Zr和Si-O，我们对一系列机器学习原子间势方法进行了基准测试，特别是高斯近似势（GAP）、高维神经网络势（HDNNP）、矩张量势（MTP）、线性和非线性原子簇展开（ACE）、神经网络等变原子间势（NequIP）、Allegro和MACE。我们发现非线性ACE、等变的消息传递图神经网络NequIP和MACE在精度与计算成本权衡中形成了Pareto前沿。在Al-Cu-Zr系统中，我们发现MACE和Allegro提供了最高的精度，而NequIP在Si-O中表现更优。此外，GPU可以大幅加速MLIPs，使它们在可访问的时间尺度上与未加速的经典原子间势相当甚至更优。最后，我们探讨了相应势的外推行为，探究了势能表面的平滑性，并最终估计了相应的拟合代码和分子动力学界面的用户友好性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning interatomic potentials (MLIPs) have massively changed thefield of atomistic modeling. They enable the accuracy of density functionaltheory in large-scale simulations while being nearly as fast as classicalinteratomic potentials. Over the last few years, a wide range of differenttypes of MLIPs have been developed, but it is often difficult to judge whichapproach is the best for a given problem setting. For the case of structurallyand chemically complex solids, namely Al-Cu-Zr and Si-O, we benchmark a rangeof machine learning interatomic potential approaches, in particular, theGaussian approximation potential (GAP), high-dimensional neural networkpotentials (HDNNP), moment tensor potentials (MTP), the atomic clusterexpansion (ACE) in its linear and nonlinear version, neural equivariantinteratomic potentials (NequIP), Allegro, and MACE. We find that nonlinear ACEand the equivariant, message-passing graph neural networks NequIP and MACE formthe Pareto front in the accuracy vs. computational cost trade-off. In case ofthe Al-Cu-Zr system we find that MACE and Allegro offer the highest accuracy,while NequIP outperforms them for Si-O. Furthermore, GPUs can massivelyaccelerate the MLIPs, bringing them on par with and even ahead ofnon-accelerated classical interatomic potentials (IPs) with regards toaccessible timescales. Finally, we explore the extrapolation behavior of thecorresponding potentials, probe the smoothness of the potential energysurfaces, and finally estimate the user friendliness of the correspondingfitting codes and molecular dynamics interfaces.</description>
      <author>example@mail.com (Niklas Leimeroth, Linus C. Erhard, Karsten Albe, Jochen Rohrer)</author>
      <guid isPermaLink="false">2505.02503v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement</title>
      <link>http://arxiv.org/abs/2505.01831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review at Neural Networks&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MTRL-FIE的多尺度目标感知表示学习框架，用于高效的眼底图像增强。&lt;h4&gt;背景&lt;/h4&gt;高质量的眼底图像对于临床筛查和眼科疾病诊断至关重要，但由于硬件限制、操作可变性和患者依从性，眼底图像往往存在分辨率低和信噪比低的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，提出了一种新的图像增强框架，以恢复全面的多尺度信息，并针对图像增强的目标，如病变，进行优化。&lt;h4&gt;方法&lt;/h4&gt;该方法包括一个多尺度特征编码器（MFE），使用小波分解嵌入低频结构信息和高频细节，以及一个结构保持的层次解码器（SHD），用于融合多尺度特征嵌入以实现真实眼底图像的恢复。SHD集成了层次融合和组注意力机制，同时使用目标感知特征聚合（TFA）模块来增强病理区域并减少伪影。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MTRL-FIE在多个眼底图像数据集上展现了有效性和泛化能力，与现有方法相比，MTRL-FIE在更轻量级的架构下实现了更优越的增强性能。&lt;h4&gt;结论&lt;/h4&gt;MTRL-FIE不仅能够有效增强眼底图像，而且可以泛化到其他眼科图像处理任务，无需监督微调，具有临床应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a multi-scale target-aware representation learning framework named MTRL-FIE for efficient fundus image enhancement.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality fundus images provide essential anatomical information forclinical screening and ophthalmic disease diagnosis. Yet, due to hardwarelimitations, operational variability, and patient compliance, fundus imagesoften suffer from low resolution and signal-to-noise ratio. Recent years havewitnessed promising progress in fundus image enhancement. However, existingworks usually focus on restoring structural details or global characteristicsof fundus images, lacking a unified image enhancement framework to recovercomprehensive multi-scale information. Moreover, few methods pinpoint thetarget of image enhancement, e.g., lesions, which is crucial for medicalimage-based diagnosis. To address these challenges, we propose a multi-scaletarget-aware representation learning framework (MTRL-FIE) for efficient fundusimage enhancement. Specifically, we propose a multi-scale feature encoder (MFE)that employs wavelet decomposition to embed both low-frequency structuralinformation and high-frequency details. Next, we design a structure-preservinghierarchical decoder (SHD) to fuse multi-scale feature embeddings for realfundus image restoration. SHD integrates hierarchical fusion and groupattention mechanisms to achieve adaptive feature fusion while retaining localstructural smoothness. Meanwhile, a target-aware feature aggregation (TFA)module is used to enhance pathological regions and reduce artifacts.Experimental results on multiple fundus image datasets demonstrate theeffectiveness and generalizability of MTRL-FIE for fundus image enhancement.Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancementperformance with a more lightweight architecture. Furthermore, our approachgeneralizes to other ophthalmic image processing tasks without supervisedfine-tuning, highlighting its potential for clinical applications.</description>
      <author>example@mail.com (Haofan Wu, Yin Huang, Yuqing Wu, Qiuyu Yang, Bingfang Wang, Li Zhang, Muhammad Fahadullah Khan, Ali Zia, M. Saleh Memon, Syed Sohail Bukhari, Abdul Fattah Memon, Daizong Ji, Ya Zhang, Ghulam Mustafa, Yin Fang)</author>
      <guid isPermaLink="false">2505.01831v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Text to Image Generation and Editing: A Survey</title>
      <link>http://arxiv.org/abs/2505.02527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  49 pages,3 figures,3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对2021年至2024年间141篇关于文本到图像生成（T2I）的研究进行了全面综述。&lt;h4&gt;背景&lt;/h4&gt;近年来，T2I技术受到广泛关注，涌现出大量研究成果。&lt;h4&gt;目的&lt;/h4&gt;本文旨在为未来研究者提供有价值的指导，并推动T2I领域的发展。&lt;h4&gt;方法&lt;/h4&gt;首先介绍了T2I的四种基础模型架构（自回归、非自回归、GAN和扩散）以及常用的关键技术（自动编码器、注意力和无分类器引导）。然后，系统比较了这些研究在T2I生成和T2I编辑两个方向上的方法，包括编码器及其使用的关键技术。此外，还从数据集、评估指标、训练资源和推理速度等方面对比了这些研究的性能。除了四种基础模型外，还调查了T2I领域的其他工作，如基于能量的模型、最新的Mamba和多模态模型。最后，探讨了T2I的社会影响并提供了一些解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;本文提出了提高T2I模型性能的独特见解和可能的未来发展方向。&lt;h4&gt;结论&lt;/h4&gt;本文是T2I领域的首次系统性和全面性概述，为未来研究者提供了宝贵的参考，并刺激了该领域的持续进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-image generation (T2I) refers to the text-guided generation ofhigh-quality images. In the past few years, T2I has attracted widespreadattention and numerous works have emerged. In this survey, we comprehensivelyreview 141 works conducted from 2021 to 2024. First, we introduce fourfoundation model architectures of T2I (autoregression, non-autoregression, GANand diffusion) and the commonly used key technologies (autoencoder, attentionand classifier-free guidance). Secondly, we systematically compare the methodsof these studies in two directions, T2I generation and T2I editing, includingthe encoders and the key technologies they use. In addition, we also comparethe performance of these researches side by side in terms of datasets,evaluation metrics, training resources, and inference speed. In addition to thefour foundation models, we survey other works on T2I, such as energy-basedmodels and recent Mamba and multimodality. We also investigate the potentialsocial impact of T2I and provide some solutions. Finally, we propose uniqueinsights of improving the performance of T2I models and possible futuredevelopment directions. In summary, this survey is the first systematic andcomprehensive overview of T2I, aiming to provide a valuable guide for futureresearchers and stimulate continued progress in this field.</description>
      <author>example@mail.com (Pengfei Yang, Ngai-Man Cheung, Xinda Ma)</author>
      <guid isPermaLink="false">2505.02527v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis</title>
      <link>http://arxiv.org/abs/2505.01785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TV-SurvCaus的新框架，用于估计时间变化治疗对生存结果的影响，并证明了其在动态治疗制度中的有效性。&lt;h4&gt;背景&lt;/h4&gt;在医学等领域，治疗协议会随时间变化，这使得估计时间变化治疗对生存结果的影响成为一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过引入TV-SurvCaus框架，扩展表示平衡技术，以处理时间变化治疗设置下的生存分析。&lt;h4&gt;方法&lt;/h4&gt;TV-SurvCaus框架通过以下方法提供理论保证：(1) 对估计异质性效应的时间变化精度提供了一般性界限；(2) 通过顺序平衡权重控制方差；(3) 对动态治疗制度的一致性结果；(4) 对于具有时间依赖性的表示学习提供了收敛速率；(5) 对治疗-混杂因素反馈的偏差提供了正式界限。&lt;h4&gt;主要发现&lt;/h4&gt;通过在合成和真实世界数据集上的广泛实验，证明了TV-SurvCaus在估计具有时间变化协变量和治疗的个体化治疗效果方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该框架通过在动态、纵向设置中更准确地估计治疗效应，推动了因果推断领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：估计时间变化治疗对生存结果的影响在许多领域都是一个具有挑战性的任务，尤其是在医学领域，治疗协议会随时间变化。尽管最近在表示学习方面的进展已经改善了静态治疗中的因果推断，但将这些方法扩展到具有生存结果的时间变化治疗制度仍然没有得到充分探索。在本文中，我们引入了TV-SurvCaus，这是一种新颖的框架，它将表示平衡技术扩展到时间变化治疗设置下的生存分析。我们通过以下方式提供理论保证：(1) 对估计异质性效应的时间变化精度提供了一般性界限；(2) 通过顺序平衡权重控制方差；(3) 对动态治疗制度的一致性结果；(4) 对于具有时间依赖性的表示学习提供了收敛速率；(5) 对治疗-混杂因素反馈的偏差提供了正式界限。我们的神经架构通过序列建模来处理时间依赖性，同时平衡时间依赖性表示。通过在合成和真实世界数据集上的广泛实验，我们证明了TV-SurvCaus在估计具有时间变化协变量和治疗的个体化治疗效果方面优于现有方法。我们的框架通过在动态、纵向设置中更准确地估计治疗效应，推动了因果推断领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the causal effect of time-varying treatments on survival outcomesis a challenging task in many domains, particularly in medicine where treatmentprotocols adapt over time. While recent advances in representation learninghave improved causal inference for static treatments, extending these methodsto dynamic treatment regimes with survival outcomes remains under-explored. Inthis paper, we introduce TV-SurvCaus, a novel framework that extendsrepresentation balancing techniques to the time-varying treatment setting forsurvival analysis. We provide theoretical guarantees through (1) a generalizedbound for time-varying precision in estimation of heterogeneous effects, (2)variance control via sequential balancing weights, (3) consistency results fordynamic treatment regimes, (4) convergence rates for representation learningwith temporal dependencies, and (5) a formal bound on the bias due totreatment-confounder feedback. Our neural architecture incorporates sequencemodeling to handle temporal dependencies while balancing time-dependentrepresentations. Through extensive experiments on both synthetic and real-worlddatasets, we demonstrate that TV-SurvCaus outperforms existing methods inestimating individualized treatment effects with time-varying covariates andtreatments. Our framework advances the field of causal inference by enablingmore accurate estimation of treatment effects in dynamic, longitudinal settingswith survival outcomes.</description>
      <author>example@mail.com (Ayoub Abraich)</author>
      <guid isPermaLink="false">2505.01785v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Beyond the model: Key differentiators in large language models and multi-agent services</title>
      <link>http://arxiv.org/abs/2505.02489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大语言模型（LLMs）不再是生成AI的唯一定义因素，优化周边生态系统成为关键。&lt;h4&gt;背景&lt;/h4&gt;随着DeepSeek、Manus AI和Llama 4等基础模型的推出，LLMs不再独占鳌头。&lt;h4&gt;目的&lt;/h4&gt;分析确保现代AI服务高效和盈利的关键不同点。&lt;h4&gt;方法&lt;/h4&gt;探讨数据质量和管理、计算效率、延迟和评估框架等方面的优化。&lt;h4&gt;主要发现&lt;/h4&gt;优化周边生态系统对于生成AI至关重要。&lt;h4&gt;结论&lt;/h4&gt;除了模型大小，数据管理、计算效率等因素对AI服务的效率与盈利性具有决定性影响。&lt;h4&gt;翻译&lt;/h4&gt;With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, ithas become evident that large language models (LLMs) are no longer the soledefining factor in generative AI. As many now operate at comparable levels ofcapability, the real race is not about having the biggest model but optimizingthe surrounding ecosystem, including data quality and management, computationalefficiency, latency, and evaluation frameworks. This review article delves intothese critical differentiators that ensure modern AI services are efficient andprofitable.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.30574/wjarr.2025.26.1.1295&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, ithas become evident that large language models (LLMs) are no longer the soledefining factor in generative AI. As many now operate at comparable levels ofcapability, the real race is not about having the biggest model but optimizingthe surrounding ecosystem, including data quality and management, computationalefficiency, latency, and evaluation frameworks. This review article delves intothese critical differentiators that ensure modern AI services are efficient andprofitable.</description>
      <author>example@mail.com (Muskaan Goyal, Pranav Bhasin)</author>
      <guid isPermaLink="false">2505.02489v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Real-time Spatial Retrieval Augmented Generation for Urban Environments</title>
      <link>http://arxiv.org/abs/2505.02271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了生成式人工智能，特别是大型语言模型在城市化应用中的潜力，并提出了一个基于FIWARE的实时空间RAG架构，以解决传统RAG架构在城市化环境中的不足。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能在城市化应用中具有变革性潜力，但基础模型存在局限性，更新耗时且成本高。&lt;h4&gt;目的&lt;/h4&gt;提出一种实时空间RAG架构，以有效整合生成式人工智能到城市中，并解决传统RAG架构在城市化环境中的不足。&lt;h4&gt;方法&lt;/h4&gt;使用FIWARE开发实时空间RAG架构，利用链接数据的时空过滤能力。&lt;h4&gt;主要发现&lt;/h4&gt;该架构在Madrid市的旅游助手用例中得到了验证，证明了通过RAG架构正确整合基础模型的可行性。&lt;h4&gt;结论&lt;/h4&gt;提出的实时空间RAG架构能够有效解决城市化环境中的信息注入问题，为生成式人工智能在城市化中的应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;The paper discusses the potential of generative artificial intelligence, especially large language models, in urban applications, and proposes a real-time spatial RAG architecture based on FIWARE to address the limitations of traditional RAG architectures in urban contexts.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of Generative Artificial Ingelligence (AI), especiallyLarge Language Models, presents transformative opportunities for urbanapplications through Urban Foundation Models. However, base models facelimitations, as they only contain the knowledge available at the time oftraining, and updating them is both time-consuming and costly. RetrievalAugmented Generation (RAG) has emerged in the literature as the preferredapproach for injecting contextual information into Foundation Models. Itprevails over techniques such as fine-tuning, which are less effective indynamic, real-time scenarios like those found in urban environments. However,traditional RAG architectures, based on semantic databases, knowledge graphs,structured data, or AI-powered web searches, do not fully meet the demands ofurban contexts. Urban environments are complex systems characterized by largevolumes of interconnected data, frequent updates, real-time processingrequirements, security needs, and strong links to the physical world. This workproposes a real-time spatial RAG architecture that defines the necessarycomponents for the effective integration of generative AI into cities,leveraging temporal and spatial filtering capabilities through linked data. Theproposed architecture is implemented using FIWARE, an ecosystem of softwarecomponents to develop smart city solutions and digital twins. The design andimplementation are demonstrated through the use case of a tourism assistant inthe city of Madrid. The use case serves to validate the correct integration ofFoundation Models through the proposed RAG architecture.</description>
      <author>example@mail.com (David Nazareno Campo, Javier Conde, Álvaro Alonso, Gabriel Huecas, Joaquín Salvachúa, Pedro Reviriego)</author>
      <guid isPermaLink="false">2505.02271v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder</title>
      <link>http://arxiv.org/abs/2505.01938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HybridGS的3D Gaussian Splatting压缩框架，旨在提高3DGS数据的压缩效率。&lt;h4&gt;背景&lt;/h4&gt;现有的3DGS压缩方案主要通过隐式数据嵌入来生成紧凑的表示，但存在编码时间长、数据格式高度定制化的问题，难以广泛应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个既能够生成紧凑表示，又能进行标准化点云数据编码的3DGS压缩框架。&lt;h4&gt;方法&lt;/h4&gt;HybridGS首先生成紧凑且显式的3DGS数据，引入双通道稀疏表示来监督基本位置和特征位深。然后，使用规范化的点云编码器进行进一步的数据压缩，形成标准输出位流。此外，提出了一种简单有效的速率控制方案来调整可解释的数据压缩方案。&lt;h4&gt;主要发现&lt;/h4&gt;目前，HybridGS不包含旨在提高3DGS生成质量的模块。然而，实验结果表明，它仍然提供了与现有方法相当的重构性能，并且编码和解码速度明显更高。&lt;h4&gt;结论&lt;/h4&gt;HybridGS是一种高效且易于部署的3DGS压缩框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要：大多数现有的3D高斯分层（3DGS）压缩方案都侧重于通过隐式数据嵌入来生成紧凑的3DGS表示。它们具有较长的编码时间和高定制化的数据格式，这使得它们难以广泛应用。本文提出了一种新的3DGS压缩框架，称为HybridGS，它利用了紧凑生成和标准化点云数据编码的优势。HybridGS首先生成紧凑且显式的3DGS数据。引入双通道稀疏表示来监督基本位置和特征位深。然后，利用规范化的点云编码器进行进一步的数据压缩，形成标准输出位流。提出了一种简单有效的速率控制方案来调整可解释的数据压缩方案。目前，HybridGS不包含旨在提高3DGS生成质量的模块。但实验结果表明，它仍然提供了与现有方法相当的重构性能，并且编码和解码速度明显更高。代码可在https://github.com/Qi-Yangsjtu/HybridGS上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing 3D Gaussian Splatting (3DGS) compression schemes focus onproducing compact 3DGS representation via implicit data embedding. They havelong coding times and highly customized data format, making it difficult forwidespread deployment. This paper presents a new 3DGS compression frameworkcalled HybridGS, which takes advantage of both compact generation andstandardized point cloud data encoding. HybridGS first generates compact andexplicit 3DGS data. A dual-channel sparse representation is introduced tosupervise the primitive position and feature bit depth. It then utilizes acanonical point cloud encoder to perform further data compression and formstandard output bitstreams. A simple and effective rate control scheme isproposed to pivot the interpretable data compression scheme. At the currentstage, HybridGS does not include any modules aimed at improving 3DGS qualityduring generation. But experiment results show that it still providescomparable reconstruction performance against state-of-the-art methods, withevidently higher encoding and decoding speed. The code is publicly available athttps://github.com/Qi-Yangsjtu/HybridGS.</description>
      <author>example@mail.com (Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li)</author>
      <guid isPermaLink="false">2505.01938v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation</title>
      <link>http://arxiv.org/abs/2505.02247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的解释方法，专门针对3D Geometric Graph Neural Networks（GNNs），旨在解决3D GNNs在分子数据建模中的可解释性问题。&lt;h4&gt;背景&lt;/h4&gt;3D GNNs在分子数据建模中具有强大的预测能力，但通常缺乏可解释性，这在需要可靠和透明洞察的科学应用中引起担忧。&lt;h4&gt;目的&lt;/h4&gt;提高3D GNNs的可解释性，使其在科学应用中更加可靠和透明。&lt;h4&gt;方法&lt;/h4&gt;该方法将解释局部化到每个节点的3D空间中的直接邻域，并为每个节点分配一个影响半径，定义了消息传递捕获空间和结构交互的局部化区域。&lt;h4&gt;主要发现&lt;/h4&gt;通过限制子图到局部化的影响半径，这种方法不仅增强了可解释性，而且与3D图应用中典型的物理和结构依赖性相一致，如分子学习。&lt;h4&gt;结论&lt;/h4&gt;该方法利用了3D图固有的空间和几何特征，为3D GNNs提供了一种新的解释方法，有助于提高模型的可解释性和透明度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Geometric Graph Neural Networks (GNNs) have emerged as transformativetools for modeling molecular data. Despite their predictive power, these modelsoften suffer from limited interpretability, raising concerns for scientificapplications that require reliable and transparent insights. While existingmethods have primarily focused on explaining molecular substructures in 2DGNNs, the transition to 3D GNNs introduces unique challenges, such as handlingthe implicit dense edge structures created by a cut-off radius. To tackle this,we introduce a novel explanation method specifically designed for 3D GNNs,which localizes the explanation to the immediate neighborhood of each nodewithin the 3D space. Each node is assigned an radius of influence, defining thelocalized region within which message passing captures spatial and structuralinteractions crucial for the model's predictions. This method leverages thespatial and geometric characteristics inherent in 3D graphs. By constrainingthe subgraph to a localized radius of influence, the approach not only enhancesinterpretability but also aligns with the physical and structural dependenciestypical of 3D graph applications, such as molecular learning.</description>
      <author>example@mail.com (Jingxiang Qu, Wenhan Gao, Jiaxing Zhang, Xufeng Liu, Hua Wei, Haibin Ling, Yi Liu)</author>
      <guid isPermaLink="false">2505.02247v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning</title>
      <link>http://arxiv.org/abs/2505.02232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合提示式基础模型和强化学习的方法，使机器人能够以提示响应的方式执行灵活的操纵任务。&lt;h4&gt;背景&lt;/h4&gt;构建对输入提示有响应的模型是机器学习领域的一次变革，这种方法在机器人领域，如杂乱环境中的针对性操作等问题上具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以将高级命令与精细的灵活控制相联系的问题。&lt;h4&gt;方法&lt;/h4&gt;采用了一种记忆增强的学生-教师学习框架，使用Segment-Anything 2 (SAM 2)模型作为感知骨干，从用户提示中推断出感兴趣的对象。虽然检测可能不完美，但它们的时序为记忆增强模型提供了丰富的隐含状态估计信息。&lt;h4&gt;主要发现&lt;/h4&gt;该方法成功地学习了提示响应策略，并在从杂乱场景中拾取物体等任务中得到了演示。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在机器人灵活操纵任务中表现出色，并展示了其在实际应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building models responsive to input prompts represents a transformative shiftin machine learning. This paradigm holds significant potential for roboticsproblems, such as targeted manipulation amidst clutter. In this work, wepresent a novel approach to combine promptable foundation models withreinforcement learning (RL), enabling robots to perform dexterous manipulationtasks in a prompt-responsive manner. Existing methods struggle to linkhigh-level commands with fine-grained dexterous control. We address this gapwith a memory-augmented student-teacher learning framework. We use theSegment-Anything 2 (SAM 2) model as a perception backbone to infer an object ofinterest from user prompts. While detections are imperfect, their temporalsequence provides rich information for implicit state estimation bymemory-augmented models. Our approach successfully learns prompt-responsivepolicies, demonstrated in picking objects from cluttered scenes. Videos andcode are available at https://memory-student-teacher.github.io</description>
      <author>example@mail.com (Malte Mosbach, Sven Behnke)</author>
      <guid isPermaLink="false">2505.02232v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement</title>
      <link>http://arxiv.org/abs/2505.01766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Information Fusion&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于图的鲁棒多模态方法，以整合视觉和运动数据，提高手术流程识别的准确性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;手术流程识别对于自动化任务、支持决策和培训新外科医生至关重要，但数据损坏可能导致性能下降。&lt;h4&gt;目的&lt;/h4&gt;旨在提高手术流程识别的鲁棒性，特别是在有领域偏移或数据损坏的复杂场景中。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为GRAD的多模态图表示网络，用于捕捉精细的视觉信息，并通过基于图的消息建模显式地建模视觉和运动嵌入之间的复杂关系。还提出了一个视觉-运动对抗框架和上下文校准解码器。&lt;h4&gt;主要发现&lt;/h4&gt;模型和模块的有效性通过广泛的比较和消融实验得到了证明。鲁棒性实验表明，该方法能够有效处理存储和传输过程中的数据损坏，表现出良好的稳定性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该方法旨在推进自动化手术流程识别，解决手术程序固有的复杂性和动态性。&lt;h4&gt;翻译&lt;/h4&gt;Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical workflow recognition is vital for automating tasks, supportingdecision-making, and training novice surgeons, ultimately improving patientsafety and standardizing procedures. However, data corruption can lead toperformance degradation due to issues like occlusion from bleeding or smoke insurgical scenes and problems with data storage and transmission. In this case,we explore a robust graph-based multimodal approach to integrating vision andkinematic data to enhance accuracy and reliability. Vision data capturesdynamic surgical scenes, while kinematic data provides precise movementinformation, overcoming limitations of visual recognition under adverseconditions. We propose a multimodal Graph Representation network withAdversarial feature Disentanglement (GRAD) for robust surgical workflowrecognition in challenging scenarios with domain shifts or corrupted data.Specifically, we introduce a Multimodal Disentanglement Graph Network thatcaptures fine-grained visual information while explicitly modeling the complexrelationships between vision and kinematic embeddings through graph-basedmessage modeling. To align feature spaces across modalities, we propose aVision-Kinematic Adversarial framework that leverages adversarial training toreduce modality gaps and improve feature consistency. Furthermore, we design aContextual Calibrated Decoder, incorporating temporal and contextual priors toenhance robustness against domain shifts and corrupted data. Extensivecomparative and ablation experiments demonstrate the effectiveness of our modeland proposed modules. Moreover, our robustness experiments show that our methodeffectively handles data corruption during storage and transmission, exhibitingexcellent stability and robustness. Our approach aims to advance automatedsurgical workflow recognition, addressing the complexities and dynamisminherent in surgical procedures.</description>
      <author>example@mail.com (Long Bai, Boyi Ma, Ruohan Wang, Guankun Wang, Beilei Cui, Zhongliang Jiang, Mobarakol Islam, Zhe Min, Jiewen Lai, Nassir Navab, Hongliang Ren)</author>
      <guid isPermaLink="false">2505.01766v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Exploring new Approaches for Information Retrieval through Natural Language Processing</title>
      <link>http://arxiv.org/abs/2505.02199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, comprehensive literature review covering six key  IR-NLP papers, plus keywords and full reference list&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了信息检索（IR）在自然语言处理（NLP）中的应用的最新进展和新兴方法。&lt;h4&gt;背景&lt;/h4&gt;文章回顾了传统的IR模型，包括布尔模型、向量空间模型、概率模型和推理网络模型。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探讨现代技术，如深度学习、强化学习和预训练的转换器模型（如BERT）。&lt;h4&gt;方法&lt;/h4&gt;文章讨论了用于高效文本索引和搜索的关键工具和库，如Lucene、Anserini和Pyserini。&lt;h4&gt;主要发现&lt;/h4&gt;文章对稀疏、密集和混合检索方法进行了比较分析，并展示了它们在网页搜索引擎、跨语言IR、论点挖掘、私人信息检索和仇恨言论检测中的应用。&lt;h4&gt;结论&lt;/h4&gt;最后，文章确定了提高检索精度、可扩展性和考虑伦理问题的开放挑战和未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This review paper explores recent advancements and emerging approaches inInformation Retrieval (IR) applied to Natural Language Processing (NLP). Weexamine traditional IR models such as Boolean, vector space, probabilistic, andinference network models, and highlight modern techniques including deeplearning, reinforcement learning, and pretrained transformer models like BERT.We discuss key tools and libraries - Lucene, Anserini, and Pyserini - forefficient text indexing and search. A comparative analysis of sparse, dense,and hybrid retrieval methods is presented, along with applications in websearch engines, cross-language IR, argument mining, private informationretrieval, and hate speech detection. Finally, we identify open challenges andfuture research directions to enhance retrieval accuracy, scalability, andethical considerations.</description>
      <author>example@mail.com (Manak Raj, Nidhi Mishra)</author>
      <guid isPermaLink="false">2505.02199v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Contextures: Representations from Contexts</title>
      <link>http://arxiv.org/abs/2505.01557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025, longer version. arXiv admin note: substantial text overlap  with arXiv:2504.19792&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了上下文结构理论，用以描述大型模型学习到的表示，并证明了许多流行的学习方法都可以通过学习输入与上下文变量之间的关联来表征。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型在实证上取得了成功，但我们缺乏对这些模型学习到的表示的系统描述。&lt;h4&gt;目的&lt;/h4&gt;建立上下文结构理论，以系统地描述大型模型学习到的表示。&lt;h4&gt;方法&lt;/h4&gt;通过证明许多流行的学习方法可以描述为从输入和上下文变量之间的关联中学习，以及上下文结构理论在监督学习、自监督学习和流形学习等不同学习范式中的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;上下文结构理论表明，学习上下文结构的表示在兼容上下文的任务上是最优的；模型规模达到可以近似最高奇异函数的程度后，进一步扩大模型规模将带来递减的回报；提出了一种评估上下文有用性的指标，并通过实验证明它与编码器在实际数据集上的性能有很好的相关性。&lt;h4&gt;结论&lt;/h4&gt;上下文结构理论为理解大型模型的学习提供了新的视角，并指出除了规模扩大外，提高上下文质量也是进一步提升模型性能的关键。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管基础模型在实证上取得了成功，但我们缺乏对这些模型学习到的表示的系统描述。在本文中，我们建立了上下文结构理论。它表明，一类广泛的表示学习方法可以表征为从输入与上下文变量之间的关联中学习。具体来说，我们表明许多流行的方法旨在逼近由上下文诱导的期望算子的最高奇异函数，在这种情况下，我们说表示学习上下文结构。我们通过证明表示学习在各种学习范式（监督学习、自监督学习和流形学习）中都可以从这种角度进行研究，来演示上下文结构理论的普遍性。我们还证明，学习上下文结构的表示在兼容上下文的任务上是最佳的。上下文结构理论的一个重要含义是，一旦模型足够大，可以逼近最高奇异函数，进一步扩大模型规模将带来递减的回报。因此，规模扩大并不是我们所需要的全部，进一步的改进需要更好的上下文。为此，我们研究了如何在不知道下游任务的情况下评估上下文的有用性。我们提出了一种指标，并通过实验表明，它与编码器在许多真实数据集上的实际性能有很好的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the empirical success of foundation models, we do not have asystematic characterization of the representations that these models learn. Inthis paper, we establish the contexture theory. It shows that a large class ofrepresentation learning methods can be characterized as learning from theassociation between the input and a context variable. Specifically, we showthat many popular methods aim to approximate the top-d singular functions ofthe expectation operator induced by the context, in which case we say that therepresentation learns the contexture. We demonstrate the generality of thecontexture theory by proving that representation learning within variouslearning paradigms -- supervised, self-supervised, and manifold learning -- canall be studied from such a perspective. We also prove that the representationsthat learn the contexture are optimal on those tasks that are compatible withthe context. One important implication of the contexture theory is that oncethe model is large enough to approximate the top singular functions, furtherscaling up the model size yields diminishing returns. Therefore, scaling is notall we need, and further improvement requires better contexts. To this end, westudy how to evaluate the usefulness of a context without knowing thedownstream tasks. We propose a metric and show by experiments that itcorrelates well with the actual performance of the encoder on many realdatasets.</description>
      <author>example@mail.com (Runtian Zhai, Kai Yang, Che-Ping Tsai, Burak Varici, Zico Kolter, Pradeep Ravikumar)</author>
      <guid isPermaLink="false">2505.01557v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>Sparfels: Fast Reconstruction from Sparse Unposed Imagery</title>
      <link>http://arxiv.org/abs/2505.02178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page : https://shubhendu-jena.github.io/Sparfels/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用表面元素平铺的稀疏视图重建方法，该方法在消费级GPU上运行时间少于3分钟。&lt;h4&gt;背景&lt;/h4&gt;目前关于从噪声或未定位的稀疏相机学习稀疏辐射场的方法较少，在此设置下的形状恢复相对较少研究。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且简单的流程，利用最新的3D基础模型，以实现稀疏视图重建。&lt;h4&gt;方法&lt;/h4&gt;利用3D基础模型的多个任务头，特别是点图和相机初始化，构建一个2D高斯平铺（2DGS）模型，并通过图像对应关系指导2DGS训练中的相机优化。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种新的沿射线平铺颜色变差的公式，该公式可以高效计算。在训练中降低这一时刻可以导致更准确的形状重建。&lt;h4&gt;结论&lt;/h4&gt;在稀疏未校准设置中的重建和基于已建立的多视图数据集的新视角基准测试中，该方法展示了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种稀疏视图重建方法，该方法使用表面元素平铺，在消费级GPU上运行时间少于3分钟。虽然很少有方法解决从噪声或未定位的稀疏相机学习稀疏辐射场的问题，但在此设置下的形状恢复相对较少被探索。一些辐射场和形状学习测试时间优化方法通过学习数据先验或使用外部单目几何先验的组合来解决稀疏定位设置。不同之处在于，我们提出了一种高效且简单的流程，利用单个最新的3D基础模型。我们利用其各种任务头，特别是点图和相机初始化来实例化一个2D高斯平铺（2DGS）模型，并通过图像对应关系引导2DGS训练中的相机优化。我们贡献的关键是一个新的沿射线平铺颜色变差的公式，该公式可以高效计算。在训练中降低这一时刻可以导致更准确的形状重建。我们在稀疏未校准设置中的重建和基于已建立的多视图数据集的新视角基准测试中展示了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a method for Sparse view reconstruction with surface elementsplatting that runs within 3 minutes on a consumer grade GPU. While few methodsaddress sparse radiance field learning from noisy or unposed sparse cameras,shape recovery remains relatively underexplored in this setting. Severalradiance and shape learning test-time optimization methods address the sparseposed setting by learning data priors or using combinations of externalmonocular geometry priors. Differently, we propose an efficient and simplepipeline harnessing a single recent 3D foundation model. We leverage itsvarious task heads, notably point maps and camera initializations toinstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and imagecorrespondences to guide camera optimization midst 2DGS training. Key to ourcontribution is a novel formulation of splatted color variance along rays,which can be computed efficiently. Reducing this moment in training leads tomore accurate shape reconstructions. We demonstrate state-of-the-artperformances in the sparse uncalibrated setting in reconstruction and novelview benchmarks based on established multi-view datasets.</description>
      <author>example@mail.com (Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma)</author>
      <guid isPermaLink="false">2505.02178v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:42 +0800</pubDate>
    </item>
    <item>
      <title>3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment</title>
      <link>http://arxiv.org/abs/2505.01809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的弱监督视觉定位方法，用于在点云中根据自然语言描述定位定向3D框，无需标注来指导模型学习。&lt;h4&gt;背景&lt;/h4&gt;该任务面临两个主要挑战：类别级别的模糊性和实例级别的复杂性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了一个能够明确区分类别和实例的新方法。&lt;h4&gt;方法&lt;/h4&gt;在类别级别分支中，利用预训练的外部检测器的大量类别知识，将对象提议特征与句子级别的类别特征对齐，从而增强类别意识。在实例级别分支中，利用语言查询中的空间关系描述来细化对象提议特征，确保对象之间的清晰区分。&lt;h4&gt;主要发现&lt;/h4&gt;这些设计使得模型能够准确识别目标类别对象，同时区分同一类别内的实例。&lt;h4&gt;结论&lt;/h4&gt;与先前的方法相比，该方法在三个广泛使用的基准测试（Nr3D、Sr3D和ScanRef）上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 3D weakly-supervised visual grounding task aims to localize oriented 3Dboxes in point clouds based on natural language descriptions without requiringannotations to guide model learning. This setting presents two primarychallenges: category-level ambiguity and instance-level complexity.Category-level ambiguity arises from representing objects of fine-grainedcategories in a highly sparse point cloud format, making category distinctionchallenging. Instance-level complexity stems from multiple instances of thesame category coexisting in a scene, leading to distractions during grounding.To address these challenges, we propose a novel weakly-supervised groundingapproach that explicitly differentiates between categories and instances. Inthe category-level branch, we utilize extensive category knowledge from apre-trained external detector to align object proposal features withsentence-level category features, thereby enhancing category awareness. In theinstance-level branch, we utilize spatial relationship descriptions fromlanguage queries to refine object proposal features, ensuring cleardifferentiation among objects. These designs enable our model to accuratelyidentify target-category objects while distinguishing instances within the samecategory. Compared to previous methods, our approach achieves state-of-the-artperformance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.</description>
      <author>example@mail.com (Xiaoqi Li, Jiaming Liu, Nuowei Han, Liang Heng, Yandong Guo, Hao Dong, Yang Liu)</author>
      <guid isPermaLink="false">2505.01809v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data</title>
      <link>http://arxiv.org/abs/2505.02130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML2025 Accept&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;注意力机制对大型语言模型（LLMs）的成功至关重要，但在处理图结构数据时，与图神经网络（GNNs）中使用的基于固定链接的消息传递机制相比，存在不足。&lt;h4&gt;背景&lt;/h4&gt;对于图结构数据，需要强调拓扑连接，而现有的注意力机制在处理这类数据时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;从注意力机制的角度进行实证研究，探索LLMs如何处理图结构数据，以深入了解LLMs在图结构上的注意力行为。&lt;h4&gt;方法&lt;/h4&gt;分析LLMs在图结构数据上的注意力应用，并评估不同注意力模型的效果。&lt;h4&gt;主要发现&lt;/h4&gt;1) LLMs可以识别图数据并捕捉文本节点之间的交互，但由于固有架构限制，难以建模图结构内部的节点关系。2) LLMs在图节点上的注意力分布不符合理想的结构模式，表明其未能适应图拓扑的细微差别。3) 无论是全连接注意力还是固定连接性都不理想；每种方法在其应用场景中都有特定的局限性。相反，中间状态注意力窗口可以提高LLMs的训练性能，并在推理过程中无缝过渡到全连接窗口。&lt;h4&gt;结论&lt;/h4&gt;LLMs在处理图结构数据时存在局限性，需要改进模型以更好地处理此类数据。&lt;h4&gt;翻译&lt;/h4&gt;注意力机制对于大型语言模型（LLMs）的成功至关重要，然而，在处理需要强调拓扑连接的图结构数据时，与在固定链接上使用的消息传递机制（如图神经网络GNNs所采用的）相比，它们存在不足。然而，对于图结构数据，需要强调拓扑连接，而现有的注意力机制在处理这类数据时存在局限性。受这些观察的启发，我们从注意力机制的角度进行了实证研究，以探索LLMs如何处理图结构数据。目的是深入了解LLMs在图结构上的注意力行为。我们发现了一些关于LLMs如何将注意力应用于图结构数据的独特现象，并分析了这些发现以改进LLMs对这类数据的建模。研究的主要发现是：1）虽然LLMs可以识别图数据并捕捉文本节点之间的交互，但由于固有的架构限制，它们难以建模图结构内部的节点关系。2）LLMs在图节点上的注意力分布不符合理想的结构模式，表明其未能适应图拓扑的细微差别。3）无论是全连接注意力还是固定连接性都不理想；每种方法在其应用场景中都有特定的局限性。相反，中间状态注意力窗口可以提高LLMs的训练性能，并在推理过程中无缝过渡到全连接窗口。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention mechanisms are critical to the success of large language models(LLMs), driving significant advancements in multiple fields. However, forgraph-structured data, which requires emphasis on topological connections, theyfall short compared to message-passing mechanisms on fixed links, such as thoseemployed by Graph Neural Networks (GNNs). This raises a question: ``Doesattention fail for graphs in natural language settings?'' Motivated by theseobservations, we embarked on an empirical study from the perspective ofattention mechanisms to explore how LLMs process graph-structured data. Thegoal is to gain deeper insights into the attention behavior of LLMs over graphstructures. We uncovered unique phenomena regarding how LLMs apply attention tograph-structured data and analyzed these findings to improve the modeling ofsuch data by LLMs. The primary findings of our research are: 1) While LLMs canrecognize graph data and capture text-node interactions, they struggle to modelinter-node relationships within graph structures due to inherent architecturalconstraints. 2) The attention distribution of LLMs across graph nodes does notalign with ideal structural patterns, indicating a failure to adapt to graphtopology nuances. 3) Neither fully connected attention nor fixed connectivityis optimal; each has specific limitations in its application scenarios.Instead, intermediate-state attention windows improve LLM training performanceand seamlessly transition to fully connected windows during inference. Sourcecode: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}</description>
      <author>example@mail.com (Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan)</author>
      <guid isPermaLink="false">2505.02130v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</title>
      <link>http://arxiv.org/abs/2505.02152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Interleave-VLA框架，该框架能够理解交错图像-文本指令并在物理世界中直接生成连续的动作序列。&lt;h4&gt;背景&lt;/h4&gt;现有的VLA模型仅限于处理机器人的观察和文本指令，缺乏数字世界中由基础模型进展带来的交错多模态指令的灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出一个灵活的、模型无关的范式，通过最小修改扩展最先进的VLA模型，并实现强大的零样本泛化。&lt;h4&gt;方法&lt;/h4&gt;开发了一个自动管道，将Open X-Embodiment中的真实世界数据集的纯文本指令转换为交错图像-文本指令，构建了第一个包含210k个场景的大型真实世界交错具身数据集。&lt;h4&gt;主要发现&lt;/h4&gt;Interleave-VLA在模拟基准和真实机器人实验中表现出显著优势：1）与最先进的基线相比，它将域外泛化能力提高了2-3倍；2）支持灵活的任务接口；3）以零样本方式处理多样化的用户提供的图像指令，如手绘草图。&lt;h4&gt;结论&lt;/h4&gt;Interleave-VLA通过交错范式有效地利用了异构数据集和多样化的指令图像，包括来自互联网的图像，显示出强大的扩展潜力。模型和数据集将开源。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型在物理世界的通用机器人操作方面展现出巨大潜力。然而，现有的模型局限于机器人观察和纯文本指令，缺乏由数字世界中基础模型进展带来的交错多模态指令的灵活性。在本文中，我们提出了Interleave-VLA，这是第一个能够理解交错图像-文本指令并在物理世界中直接生成连续动作序列的框架。它提供了一个灵活的、模型无关的范式，通过最小修改扩展了最先进的VLA模型，并实现了强大的零样本泛化。实现Interleave-VLA的一个关键挑战是缺乏大规模的交错具身数据集。为了弥合这一差距，我们开发了一个自动管道，将Open X-Embodiment中的真实世界数据集的纯文本指令转换为交错图像-文本指令，从而产生了第一个包含210k个场景的大型真实世界交错具身数据集。通过在模拟基准和真实机器人实验上的综合评估，我们证明了Interleave-VLA的优势：1）与最先进的基线相比，它将域外泛化能力提高了2-3倍；2）支持灵活的任务接口；3）以零样本方式处理多样化的用户提供的图像指令，如手绘草图。我们进一步分析了Interleave-VLA强大零样本性能背后的因素，表明交错范式有效地利用了异构数据集和多样化的指令图像，包括来自互联网的图像，这表明了强大的扩展潜力。我们的模型和数据集将开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have shown great promise for generalistrobotic manipulation in the physical world. However, existing models arerestricted to robot observations and text-only instructions, lacking theflexibility of interleaved multimodal instructions enabled by recent advancesin foundation models in the digital world. In this paper, we presentInterleave-VLA, the first framework capable of comprehending interleavedimage-text instructions and directly generating continuous action sequences inthe physical world. It offers a flexible, model-agnostic paradigm that extendsstate-of-the-art VLA models with minimal modifications and strong zero-shotgeneralization. A key challenge in realizing Interleave-VLA is the absence oflarge-scale interleaved embodied datasets. To bridge this gap, we develop anautomatic pipeline that converts text-only instructions from real-worlddatasets in Open X-Embodiment into interleaved image-text instructions,resulting in the first large-scale real-world interleaved embodied dataset with210k episodes. Through comprehensive evaluation on simulation benchmarks andreal-robot experiments, we demonstrate that Interleave-VLA offers significantbenefits: 1) it improves out-of-domain generalization to unseen objects by 2-3xcompared to state-of-the-art baselines, 2) supports flexible task interfaces,and 3) handles diverse user-provided image instructions in a zero-shot manner,such as hand-drawn sketches. We further analyze the factors behindInterleave-VLA's strong zero-shot performance, showing that the interleavedparadigm effectively leverages heterogeneous datasets and diverse instructionimages, including those from the Internet, which demonstrates strong potentialfor scaling up. Our model and dataset will be open-sourced.</description>
      <author>example@mail.com (Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding)</author>
      <guid isPermaLink="false">2505.02152v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes</title>
      <link>http://arxiv.org/abs/2505.01726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025 Proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NPISeg3D的新颖概率框架，用于解决交互式3D分割中的两个关键挑战：从稀疏用户点击生成准确分割以及量化预测不确定性。&lt;h4&gt;背景&lt;/h4&gt;交互式3D分割通过结合用户点击在复杂3D场景中生成准确的对象掩码，但存在两个未充分探索的挑战：从稀疏用户点击到准确分割的有效泛化，以及量化预测不确定性以帮助用户识别不可靠区域。&lt;h4&gt;目的&lt;/h4&gt;提出NPISeg3D框架，以解决上述两个挑战，实现更准确的分割和可靠的预测不确定性估计。&lt;h4&gt;方法&lt;/h4&gt;NPISeg3D利用神经过程（NPs）构建了一个具有场景特异性和对象特异性潜在变量的分层潜在变量结构，以增强少样本泛化能力。此外，设计了一个概率原型调制器，通过对象特异性潜在变量自适应地调节点击原型，提高模型捕捉对象感知上下文和量化预测不确定性的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在四个3D点云数据集上的实验表明，NPISeg3D在较少点击的情况下实现了优越的分割性能，并提供了可靠的预测不确定性估计。&lt;h4&gt;结论&lt;/h4&gt;NPISeg3D框架能够有效地解决交互式3D分割中的关键挑战，为生成准确的对象掩码提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D segmentation has emerged as a promising solution forgenerating accurate object masks in complex 3D scenes by incorporatinguser-provided clicks. However, two critical challenges remain underexplored:(1) effectively generalizing from sparse user clicks to produce accuratesegmentation, and (2) quantifying predictive uncertainty to help users identifyunreliable regions. In this work, we propose NPISeg3D, a novel probabilisticframework that builds upon Neural Processes (NPs) to address these challenges.Specifically, NPISeg3D introduces a hierarchical latent variable structure withscene-specific and object-specific latent variables to enhance few-shotgeneralization by capturing both global context and object-specificcharacteristics. Additionally, we design a probabilistic prototype modulatorthat adaptively modulates click prototypes with object-specific latentvariables, improving the model's ability to capture object-aware context andquantify predictive uncertainty. Experiments on four 3D point cloud datasetsdemonstrate that NPISeg3D achieves superior segmentation performance with fewerclicks while providing reliable uncertainty estimations.</description>
      <author>example@mail.com (Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves)</author>
      <guid isPermaLink="false">2505.01726v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>TeMTG: Text-Enhanced Multi-Hop Temporal Graph Modeling for Audio-Visual Video Parsing</title>
      <link>http://arxiv.org/abs/2505.02096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICMR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TeMTG的多模态优化框架，用于解析视频中的事件类别和发生时间。&lt;h4&gt;背景&lt;/h4&gt;现有的音频-视觉视频解析方法通常通过弱标签隐式建模音频和视觉特征，而没有挖掘不同模态之间的语义关系以及显式建模事件的时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出TeMTG框架以更准确地解析在弱监督下每个段落的 eventos 信息。&lt;h4&gt;方法&lt;/h4&gt;TeMTG框架结合了文本增强和多跳时间图建模。具体来说，利用预训练的多模态模型生成特定模态的文本嵌入，并将其与音频-视觉特征融合以增强这些特征的语义表示。此外，引入了多跳时间图神经网络，它显式地建模了段落之间的局部时间关系，捕捉了短期和长期事件的时间连续性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在LLP数据集的多个关键指标上，该方法达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;TeMTG框架在音频-视觉视频解析任务中取得了显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Video Parsing (AVVP) task aims to parse the event categories andoccurrence times from audio and visual modalities in a given video. Existingmethods usually focus on implicitly modeling audio and visual features throughweak labels, without mining semantic relationships for different modalities andexplicit modeling of event temporal dependencies. This makes it difficult forthe model to accurately parse event information for each segment under weaksupervision, especially when high similarity between segmental modal featuresleads to ambiguous event boundaries. Hence, we propose a multimodaloptimization framework, TeMTG, that combines text enhancement and multi-hoptemporal graph modeling. Specifically, we leverage pre-trained multimodalmodels to generate modality-specific text embeddings, and fuse them withaudio-visual features to enhance the semantic representation of these features.In addition, we introduce a multi-hop temporal graph neural network, whichexplicitly models the local temporal relationships between segments, capturingthe temporal continuity of both short-term and long-range events. Experimentalresults demonstrate that our proposed method achieves state-of-the-art (SOTA)performance in multiple key indicators in the LLP dataset.</description>
      <author>example@mail.com (Yaru Chen, Peiliang Zhang, Fei Li, Faegheh Sardari, Ruohao Guo, Zhenbo Li, Wenwu Wang)</author>
      <guid isPermaLink="false">2505.02096v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency</title>
      <link>http://arxiv.org/abs/2505.02133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大型语言模型（LLMs）在自动代码生成中的应用，探讨了提升代码生成功能、可靠性和实用性的方法。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在代码生成领域的应用已成为人工智能研究的重要焦点，随着模型的发展，其在理解和生成复杂代码结构方面的能力为自动化编程任务提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;旨在通过结合多代理协作和基于运行时执行信息调试两种方法，提升代码生成的功能、可靠性和实用性。&lt;h4&gt;方法&lt;/h4&gt;进行实证研究，评估了单独策略以及两种策略组合的效果，使用了19个LLM来检验策略的性能，并分析了不同编程活动组合和训练范式对代码生成有效性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实现了结合两种策略的链式系统，使用两个常用的代码生成基准数据集评估了功能准确性、代码可靠性和生成延迟。&lt;h4&gt;结论&lt;/h4&gt;研究结果为寻求稳健的AI驱动编码解决方案的组织提供了有价值的见解，指导他们选择能够更好地适应复杂后训练策略的模型，从而促进更有效和可靠的代码生成技术的采用。&lt;h4&gt;翻译&lt;/h4&gt;The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study uses 19 LLMs to examine the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of large language models (LLMs) for automated code generation hasemerged as a significant focus within AI research. As these pretrained modelscontinue to evolve, their ability to understand and generate complex codestructures has opened new possibilities for automating intricate programmingtasks for the sake of accurate code generation. Although contemporaryfoundational models demonstrate promoting results, researchers continue toexplore optimal post-training strategies to enhance code quality. These includesupervised fine-tuning, retrieval-augmented generation (RAG), debugging, andmany others. In this paper, we combine two widely used approaches namelymulti-agent collaboration and runtime execution information-based debugging,for improving code generation functionality, reliability, and practicalapplicability. We perform an empirical study in order to extend the evaluationof the individual strategies as well as the proposed composition of theactivities of both strategies. Our study use 19 LLMs to examines theperformance of individual and the proposed strategies, offering comprehensiveinsights into how different programming activities compositions and trainingparadigms influence code generation effectiveness. In particular, we implementa chained system that combines both strategies to assess their combined impacton functional accuracy, code reliability, and generation latency using twobenchmark datasets commonly used for code generation. Our findings providevaluable insights for organizations seeking robust AI-driven coding solutionsby guiding them in selecting models that can better adapt to complexpost-training strategies, ultimately fostering the adoption of more effectiveand reliable code generation technologies.</description>
      <author>example@mail.com (Nazmus Ashrafi, Salah Bouktif, Mohammed Mediani)</author>
      <guid isPermaLink="false">2505.02133v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation</title>
      <link>http://arxiv.org/abs/2505.01615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于跨注意力机制的Transformer方法，用于多模态传感器融合，以构建船舶周围环境的鸟瞰图，支持更安全的自主航行。&lt;h4&gt;背景&lt;/h4&gt;为了提高航行准确性和鲁棒性，需要详细可靠的场景表示。&lt;h4&gt;目的&lt;/h4&gt;通过深度融合多视角RGB图像、长波红外图像和稀疏LiDAR点云，以及X波段雷达和电子海图数据，提高导航准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用跨注意力Transformer模型进行多模态传感器融合，结合多种数据源进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够提供详细的可靠场景表示，即使在恶劣天气和复杂的航海环境中也能有效工作。&lt;h4&gt;结论&lt;/h4&gt;该方法在真实世界的海试中得到了验证，证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于跨注意力机制的Transformer方法，用于多模态传感器融合，以构建船舶周围环境的鸟瞰图，支持更安全的自主航行。该模型深度融合多视角RGB图像和长波红外图像与稀疏LiDAR点云。训练还整合了X波段雷达和电子海图数据以提供预测信息。所得到的视图提供了一个详细的可靠场景表示，提高了导航的准确性和鲁棒性。现实世界的海试证实了该方法的有效性，即使在恶劣天气和复杂的航海环境中也是如此。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a cross attention transformer based method for multimodal sensorfusion to build a birds eye view of a vessels surroundings supporting saferautonomous marine navigation. The model deeply fuses multiview RGB and longwave infrared images with sparse LiDAR point clouds. Training also integrates Xband radar and electronic chart data to inform predictions. The resulting viewprovides a detailed reliable scene representation improving navigationalaccuracy and robustness. Real world sea trials confirm the methodseffectiveness even in adverse weather and complex maritime settings.</description>
      <author>example@mail.com (Dimitrios Dagdilelis, Panagiotis Grigoriadis, Roberto Galeazzi)</author>
      <guid isPermaLink="false">2505.01615v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Defense Against Adversarial Attacks in Time Series Classification</title>
      <link>http://arxiv.org/abs/2505.02073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures. Accepted at RAFDA Workshop, PAKDD 2025  (Springer, EI &amp; Scopus indexed). Code:  https://github.com/Yi126/Lightweight-Defence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对时间序列分类（TSC）领域，提出了五种基于数据增强的时间序列对抗防御方法，以降低计算成本，并提升了模型的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;随着时间序列分类在计算机视觉领域的兴起，保证TSC模型对抗攻击的鲁棒性变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发高效、计算成本低的对抗防御方法，以提高TSC模型的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出五种基于数据增强的防御方法，其中最计算密集的方法相比原始TSC模型仅增加14.07%的计算资源。创建了两种结合方法：一种是所有提出技术的集成，另一种是集成方法与PGD-based AT方法的对比。&lt;h4&gt;主要发现&lt;/h4&gt;提出的集成方法不仅提供了比基于PGD的AT方法更好的防御性能，而且增强了TSC模型的泛化能力，且所需的计算资源仅为PGD-based AT的三分之一以下。&lt;h4&gt;结论&lt;/h4&gt;这些方法推进了数据挖掘中鲁棒TSC的发展，并为将数据增强对抗防御与大规模预训练模型结合的未来研究提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As time series classification (TSC) gains prominence, ensuring robust TSCmodels against adversarial attacks is crucial. While adversarial defense iswell-studied in Computer Vision (CV), the TSC field has primarily relied onadversarial training (AT), which is computationally expensive. In this paper,five data augmentation-based defense methods tailored for time series aredeveloped, with the most computationally intensive method among them increasingthe computational resources by only 14.07% compared to the original TSC model.Moreover, the deployment process for these methods is straightforward. Byleveraging these advantages of our methods, we create two combined methods. Oneof these methods is an ensemble of all the proposed techniques, which not onlyprovides better defense performance than PGD-based AT but also enhances thegeneralization ability of TSC models. Moreover, the computational resourcesrequired for our ensemble are less than one-third of those required forPGD-based AT. These methods advance robust TSC in data mining. Furthermore, asfoundation models are increasingly explored for time series feature learning,our work provides insights into integrating data augmentation-based adversarialdefense with large-scale pre-trained models in future research.</description>
      <author>example@mail.com (Yi Han)</author>
      <guid isPermaLink="false">2505.02073v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup</title>
      <link>http://arxiv.org/abs/2505.01902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种机器学习框架，用于预测FIFA世界杯比赛的胜者。该框架结合了球队历史数据和球员个人表现指标，并使用分类技术以及降维和超参数优化，以创建鲁棒的预测模型。&lt;h4&gt;背景&lt;/h4&gt;准确预测FIFA世界杯比赛结果对分析师、教练、赌徒和球迷都有重要价值。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习框架，用于预测FIFA世界杯比赛的胜者。&lt;h4&gt;方法&lt;/h4&gt;通过整合球队历史数据和球员个人表现指标，创建年度球队档案，采用分类技术、降维和超参数优化来提高预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在FIFA 2022世界杯数据上的预测准确性优于基线方法，强调了结合个人球员属性和球队构成的重要性。&lt;h4&gt;结论&lt;/h4&gt;本文强调了丰富、以球员为中心的数据在体育分析中的变革潜力，并为未来探索高级学习架构如图神经网络来模拟复杂团队互动奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of FIFA World Cup match outcomes holds significant valuefor analysts, coaches, bettors, and fans. This paper presents a machinelearning framework specifically designed to forecast match winners in FIFAWorld Cup. By integrating both team-level historical data and player-specificperformance metrics such as goals, assists, passing accuracy, and tackles, wecapture nuanced interactions often overlooked by traditional aggregate models.Our methodology processes multi-year data to create year-specific team profilesthat account for evolving rosters and player development. We employclassification techniques complemented by dimensionality reduction andhyperparameter optimization, to yield robust predictive models. Experimentalresults on data from the FIFA 2022 World Cup demonstrate our approach'ssuperior accuracy compared to baseline method. Our findings highlight theimportance of incorporating individual player attributes and team-levelcomposition to enhance predictive performance, offering new insights intoplayer synergy, strategic match-ups, and tournament progression scenarios. Thiswork underscores the transformative potential of rich, player-centric data insports analytics, setting a foundation for future exploration of advancedlearning architectures such as graph neural networks to model complex teaminteractions.</description>
      <author>example@mail.com (Ali Al-Bustami, Zaid Ghazal)</author>
      <guid isPermaLink="false">2505.01902v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation</title>
      <link>http://arxiv.org/abs/2505.01984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ADaFGrad的方法，用于增强全切片图像（WSI）分析中的终身学习能力，以提高癌症诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;全切片图像在癌症诊断和预后中起着关键作用，但由于其巨大的数据量，存储、处理和模型训练都面临挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种终身学习的方法，以利用分布在多个机构的切片来开发一个统一的在线模型，作为临床和医院环境中癌症诊断的计算工具。&lt;h4&gt;方法&lt;/h4&gt;ADaFGrad方法利用病理视觉语言基础模型，开发了一个框架，允许切片的区域组织特征与预定义的基于文本的原型缓冲区进行交互。此外，还提出了一种梯度蒸馏机制，模拟了在持续学习环境中，过去和当前迭代中logit相对于分类头参数的梯度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ADaFGrad在仅经过几个训练周期后，在类增量学习场景中比最先进的WSI特定方法和传统持续学习方法表现更好，提高了5.068%。此外，ADaFGrad的准确率比基线提高了40.084%，显示出所提出模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;ADaFGrad方法在癌症诊断中的终身学习能力方面具有显著优势，能够提高诊断的准确性并减少知识遗忘。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosisand prognosis, as they provide tissue details at the cellular level. However,the rapid growth of computational tasks involving WSIs poses significantchallenges. Given that WSIs are gigapixels in size, they present difficultiesin terms of storage, processing, and model training. Therefore, it is essentialto develop lifelong learning approaches for WSI analysis. In scenarios whereslides are distributed across multiple institutes, we aim to leverage them todevelop a unified online model as a computational tool for cancer diagnosis inclinical and hospital settings. In this study, we introduce ADaFGrad, a methoddesigned to enhance lifelong learning for whole-slide image (WSI) analysis.First, we leverage pathology vision-language foundation models to develop aframework that enables interaction between a slide's regional tissue featuresand a predefined text-based prototype buffer. Additionally, we propose agradient-distillation mechanism that mimics the gradient of a logit withrespect to the classification-head parameters across past and currentiterations in a continual-learning setting. We construct a sequence of six TCGAdatasets for training and evaluation. Experimental results show that ADaFGradoutperforms both state-of-the-art WSI-specific and conventionalcontinual-learning methods after only a few training epochs, exceeding them byup to +5.068% in the class-incremental learning scenario while exhibiting theleast forgetting (i.e., retaining the most knowledge from previous tasks).Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy,further demonstrating the effectiveness of the proposed modules.</description>
      <author>example@mail.com (Doanh C. Bui, Hoai Luan Pham, Vu Trung Duong Le, Tuan Hai Vu, Van Duy Tran, Khang Nguyen, Yasuhiko Nakashima)</author>
      <guid isPermaLink="false">2505.01984v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models</title>
      <link>http://arxiv.org/abs/2505.01912v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了在分子发现领域，基于深度学习和生成模型的数据驱动发现流程的发展。研究了机器学习模型在过滤和设计新型分子时的应用，同时评估了模型在分子性质预测中的泛化能力，并提出一个新的开源基准研究BOOM（Benchmark for Out-of-Distribution Molecular Property Predictions）。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习和生成模型的发展，数据驱动分子发现管道受到广泛关注，这类管道利用机器学习模型过滤和设计新型分子，而无需进行昂贵的第一性原理模拟。然而，发现新的分子需要准确的超出分布（OOD）预测，而ML模型在泛化OOD方面往往存在困难。&lt;h4&gt;目的&lt;/h4&gt;研究并提出一个基准（BOOM）来评估基于属性的超出分布分子性质预测模型，通过评估多个模型和性质预测任务组合的泛化能力，为深度学习模型在OOD性能上提供基准。&lt;h4&gt;方法&lt;/h4&gt;研究人员评估了超过140种模型和性质预测任务组合，以在多个任务上基准测试深度学习模型的OOD性能，并通过消融实验研究了数据生成、预训练、超参数优化、模型架构和分子表示对OOD性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现没有现存的模型在所有任务上都能实现强的OOD泛化；即使表现最好的模型，其平均OOD误差也比分布内的误差大3倍。此外，具有高归纳偏置的深度学习模型在具有简单、特定性质的OOD任务上表现良好。化学基础模型虽然在有限的训练数据场景中提供了有希望的解决方案，但目前的模型并未展现出强大的OOD外推能力。&lt;h4&gt;结论&lt;/h4&gt;提出发展具有强大OOD泛化的机器学习模型是化学机器学习模型开发的新前沿挑战，并且开源的BOOM基准研究将被发布在Github上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in deep learning and generative modeling have driven interest indata-driven molecule discovery pipelines, whereby machine learning (ML) modelsare used to filter and design novel molecules without requiring prohibitivelyexpensive first-principles simulations. Although the discovery of novelmolecules that extend the boundaries of known chemistry requires accurateout-of-distribution (OOD) predictions, ML models often struggle to generalizeOOD. Furthermore, there are currently no systematic benchmarks for molecularOOD prediction tasks. We present BOOM, $\boldsymbol{b}$enchmarks for$\boldsymbol{o}$ut-$\boldsymbol{o}$f-distribution $\boldsymbol{m}$olecularproperty predictions -- a benchmark study of property-based out-of-distributionmodels for common molecular property prediction models. We evaluate more than140 combinations of models and property prediction tasks to benchmark deeplearning models on their OOD performance. Overall, we do not find any existingmodels that achieve strong OOD generalization across all tasks: even the topperforming model exhibited an average OOD error 3x larger than in-distribution.We find that deep learning models with high inductive bias can perform well onOOD tasks with simple, specific properties. Although chemical foundation modelswith transfer and in-context learning offer a promising solution for limitedtraining data scenarios, we find that current foundation models do not showstrong OOD extrapolation capabilities. We perform extensive ablationexperiments to highlight how OOD performance is impacted by data generation,pre-training, hyperparameter optimization, model architecture, and molecularrepresentation. We propose that developing ML models with strong OODgeneralization is a new frontier challenge in chemical ML model development.This open-source benchmark will be made available on Github.</description>
      <author>example@mail.com (Evan R. Antoniuk, Shehtab Zaman, Tal Ben-Nun, Peggy Li, James Diffenderfer, Busra Demirci, Obadiah Smolenski, Tim Hsu, Anna M. Hiszpanski, Kenneth Chiu, Bhavya Kailkhura, Brian Van Essen)</author>
      <guid isPermaLink="false">2505.01912v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>ReLI: A Language-Agnostic Approach to Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2505.01862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ReLI的语言无关框架，旨在使自主代理能够在不同语言中自然交流、语义推理并执行任务，解决了跨语言应用中与环境的交互和执行人类指令的问题。&lt;h4&gt;背景&lt;/h4&gt;在工业、家庭和其他日常任务中，自适应自主代理的应用正在增加，但在全球或跨语言的应用环境中，确保自主代理能够有效与环境互动并执行多样化语言的指令仍然是一个未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;提出ReLI框架，以实现自主代理在不同语言环境中自然交流、语义推理并执行任务，不受指令语言起源的限制。&lt;h4&gt;方法&lt;/h4&gt;ReLI框架首先将大规模预训练的基础模型转化为语言到行动模型，这些模型可以通过自然的人类-机器人对话直接提供常识推理和高级机器人控制。此外，还进行了跨语言的模型固化，以确保ReLI能够跨全球语言泛化。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的模拟和真实世界实验，包括零样本和少样本的时空导航、场景信息检索和查询导向任务，ReLI在140种语言的超过70K次多轮对话中进行了性能评估。平均而言，ReLI在跨语言指令解析和任务执行成功率方面达到了90%±0.2的准确率。&lt;h4&gt;结论&lt;/h4&gt;ReLI有望增强现实世界中的人机自然交互，同时支持语言多样性。演示和资源将在https://linusnep.github.io/ReLI/上公开提供。&lt;h4&gt;翻译&lt;/h4&gt;摘要翻译：Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to conversenaturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction's linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI's robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%±0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI's potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at https://linusnep.github.io/ReLI/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adapting autonomous agents to industrial, domestic, and other daily tasks iscurrently gaining momentum. However, in the global or cross-lingual applicationcontexts, ensuring effective interaction with the environment and executingunrestricted human task-specified instructions in diverse languages remains anunsolved problem. To address this challenge, we propose ReLI, alanguage-agnostic framework designed to enable autonomous agents to conversenaturally, semantically reason about the environment, and to perform downstreamtasks, regardless of the task instruction's linguistic origin. First, we groundlarge-scale pre-trained foundation models and transform them intolanguage-to-action models that can directly provide common-sense reasoning andhigh-level robot control through natural, free-flow human-robot conversationalinteractions. Further, we perform cross-lingual grounding of the models toensure that ReLI generalises across the global languages. To demonstrate theReLI's robustness, we conducted extensive simulated and real-world experimentson various short- and long-horizon tasks, including zero-shot and few-shotspatial navigation, scene information retrieval, and query-oriented tasks. Webenchmarked the performance on 140 languages involving over 70K multi-turnconversations. On average, ReLI achieved over 90%$\pm$0.2 accuracy incross-lingual instruction parsing and task execution success rates. Theseresults demonstrate the ReLI's potential to enhance natural human-robotinteraction in the real world while championing linguistic diversity.Demonstrations and resources will be publicly available athttps://linusnep.github.io/ReLI/.</description>
      <author>example@mail.com (Linus Nwankwo, Bjoern Ellensohn, Ozan Özdenizci, Elmar Rueckert)</author>
      <guid isPermaLink="false">2505.01862v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2</title>
      <link>http://arxiv.org/abs/2505.01854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Short-Long Memory SAM 2（SLM-SAM 2）的新型架构，用于提高医学图像分割的自动化标注准确性。&lt;h4&gt;背景&lt;/h4&gt;医学图像（如MRI和CT）的手动标注是劳动密集型且耗时的过程，而视频对象分割的基础模型（如SAM 2）有望加快标注速度。&lt;h4&gt;目的&lt;/h4&gt;为了解决SAM 2在边界区域易出错的问题，提出SLM-SAM 2以提升分割精度。&lt;h4&gt;方法&lt;/h4&gt;SLM-SAM 2结合了短时和长时记忆银行以及独立的注意力模块，并在三个公开数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;SLM-SAM 2在5个体积和1个体积可用的情况下，与默认SAM 2相比，分别实现了Dice相似系数平均提升0.14和0.11。SLM-SAM 2对过度传播具有更强的抵抗力。&lt;h4&gt;结论&lt;/h4&gt;SLM-SAM 2在医学图像分割的自动化标注方面取得了显著进展，有助于分割模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manual annotation of volumetric medical images, such as magnetic resonanceimaging (MRI) and computed tomography (CT), is a labor-intensive andtime-consuming process. Recent advancements in foundation models for videoobject segmentation, such as Segment Anything Model 2 (SAM 2), offer apotential opportunity to significantly speed up the annotation process bymanually annotating one or a few slices and then propagating target masksacross the entire volume. However, the performance of SAM 2 in this contextvaries. Our experiments show that relying on a single memory bank and attentionmodule is prone to error propagation, particularly at boundary regions wherethe target is present in the previous slice but absent in the current one. Toaddress this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novelarchitecture that integrates distinct short-term and long-term memory bankswith separate attention modules to improve segmentation accuracy. We evaluateSLM-SAM 2 on three public datasets covering organs, bones, and muscles acrossMRI and CT modalities. We show that the proposed method markedly outperformsthe default SAM 2, achieving average Dice Similarity Coefficient improvement of0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available forthe initial adaptation, respectively. SLM-SAM 2 also exhibits strongerresistance to over-propagation, making a notable step toward more accurateautomated annotation of medical images for segmentation model development.</description>
      <author>example@mail.com (Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski)</author>
      <guid isPermaLink="false">2505.01854v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings</title>
      <link>http://arxiv.org/abs/2505.01711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CXR-TextInter的新框架，用于通过利用大型语言模型（LLMs）来提高胸部X光片（CXR）的自动解释能力，同时结合医学知识模块增强临床推理。&lt;h4&gt;背景&lt;/h4&gt;自动解释胸部X光片是提高临床工作流程和患者护理的关键任务。尽管多模态基础模型有潜力，但有效利用大型语言模型（LLMs）进行视觉任务仍是一个未充分探索的领域。&lt;h4&gt;目的&lt;/h4&gt;开发CXR-TextInter框架，通过结构化文本表示和医学知识模块，实现CXR的准确解释。&lt;h4&gt;方法&lt;/h4&gt;开发了一个名为MediInstruct-CXR的数据集，其中包含结构化图像表示和多样化的临床指令-响应示例，以及CXR-ClinEval基准，用于综合评估各种解释任务。通过在CXR-ClinEval上进行大量实验，评估CXR-TextInter的性能。&lt;h4&gt;主要发现&lt;/h4&gt;CXR-TextInter在病理检测、报告生成和视觉问答等任务上达到了最先进的性能，超过了现有的多模态基础模型。消融实验证实了知识集成模块的关键作用。盲法评估表明，认证放射科医生对CXR-TextInter生成的输出质量有显著偏好。&lt;h4&gt;结论&lt;/h4&gt;本文验证了医学图像AI的替代范式，展示了当视觉信息得到有效结构化且领域知识得到整合时，利用高级LLM能力的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Automated interpretation of chest X-rays (CXR) is a critical task with the potential to significantly improve clinical workflow and patient care. While recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (LLMs) for this visual task remains an underexplored area. This paper introduces CXR-TextInter, a novel framework that repurposes powerful text-centric LLMs for CXR interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. We augment this LLM-centric approach with an integrated medical knowledge module to enhance clinical reasoning. To facilitate training and evaluation, we developed the MediInstruct-CXR dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the CXR-ClinEval benchmark for comprehensive assessment across various interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that CXR-TextInter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. Ablation studies confirm the critical contribution of the knowledge integration module. Furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by CXR-TextInter. Our work validates an alternative paradigm for medical image AI, showcasing the potential of harnessing advanced LLM capabilities when visual information is effectively structured and domain knowledge is integrated.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated interpretation of chest X-rays (CXR) is a critical task with thepotential to significantly improve clinical workflow and patient care. Whilerecent advances in multimodal foundation models have shown promise, effectivelyleveraging the full power of large language models (LLMs) for this visual taskremains an underexplored area. This paper introduces CXR-TextInter, a novelframework that repurposes powerful text-centric LLMs for CXR interpretation byoperating solely on a rich, structured textual representation of the imagecontent, generated by an upstream image analysis pipeline. We augment thisLLM-centric approach with an integrated medical knowledge module to enhanceclinical reasoning. To facilitate training and evaluation, we developed theMediInstruct-CXR dataset, containing structured image representations pairedwith diverse, clinically relevant instruction-response examples, and theCXR-ClinEval benchmark for comprehensive assessment across variousinterpretation tasks. Extensive experiments on CXR-ClinEval demonstrate thatCXR-TextInter achieves state-of-the-art quantitative performance acrosspathology detection, report generation, and visual question answering,surpassing existing multimodal foundation models. Ablation studies confirm thecritical contribution of the knowledge integration module. Furthermore, blindedhuman evaluation by board-certified radiologists shows a significant preferencefor the clinical quality of outputs generated by CXR-TextInter. Our workvalidates an alternative paradigm for medical image AI, showcasing thepotential of harnessing advanced LLM capabilities when visual information iseffectively structured and domain knowledge is integrated.</description>
      <author>example@mail.com (Alexander Davis, Rafael Souza, Jia-Hao Lim)</author>
      <guid isPermaLink="false">2505.01711v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification</title>
      <link>http://arxiv.org/abs/2505.01660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Focal-SAM的新方法，用于解决现实世界数据集中长尾分布导致的泛化困难问题。&lt;h4&gt;背景&lt;/h4&gt;现实世界数据集通常遵循长尾分布，使得对尾部类别的泛化变得困难。&lt;h4&gt;目的&lt;/h4&gt;提高模型对尾部类别的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;Focal-SAM通过为类间锐度分配不同的惩罚，在不进行额外反向传播的情况下实现细粒度控制，从而保持效率。&lt;h4&gt;主要发现&lt;/h4&gt;Focal-SAM在提高泛化能力的同时，避免了ImbSAM和CC-SAM在计算效率和损失景观控制之间的权衡。&lt;h4&gt;结论&lt;/h4&gt;Focal-SAM在传统和基础模型上的广泛实验验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Real-world datasets often follow a long-tailed distribution, making generalization to tail classes difficult. Recent methods resorted to long-tail variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to improve generalization by flattening the loss landscape. However, these attempts face a trade-off between computational efficiency and control over the loss landscape. On the one hand, ImbSAM is efficient but offers only coarse control as it excludes head classes from the SAM process. On the other hand, CC-SAM provides fine-grained control through class-dependent perturbations but at the cost of efficiency due to multiple backpropagations. Seeing this dilemma, we introduce Focal-SAM, which assigns different penalties to class-wise sharpness, achieving fine-grained control without extra backpropagations, thus maintaining efficiency. Furthermore, we theoretically analyze Focal-SAM's generalization ability and derive a sharper generalization bound. Extensive experiments on both traditional and foundation models validate the effectiveness of Focal-SAM.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world datasets often follow a long-tailed distribution, makinggeneralization to tail classes difficult. Recent methods resorted to long-tailvariants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, toimprove generalization by flattening the loss landscape. However, theseattempts face a trade-off between computational efficiency and control over theloss landscape. On the one hand, ImbSAM is efficient but offers only coarsecontrol as it excludes head classes from the SAM process. On the other hand,CC-SAM provides fine-grained control through class-dependent perturbations butat the cost of efficiency due to multiple backpropagations. Seeing thisdilemma, we introduce Focal-SAM, which assigns different penalties toclass-wise sharpness, achieving fine-grained control without extrabackpropagations, thus maintaining efficiency. Furthermore, we theoreticallyanalyze Focal-SAM's generalization ability and derive a sharper generalizationbound. Extensive experiments on both traditional and foundation models validatethe effectiveness of Focal-SAM.</description>
      <author>example@mail.com (Sicong Li, Qianqian Xu, Zhiyong Yang, Zitai Wang, Linchao Zhang, Xiaochun Cao, Qingming Huang)</author>
      <guid isPermaLink="false">2505.01660v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
      <link>http://arxiv.org/abs/2505.01651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages including references and appendix, 25 pages core text, 3  figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HAIG框架，用于分析人类-人工智能关系中的信任动态。&lt;h4&gt;背景&lt;/h4&gt;现有的分类框架（如“人机交互”模型）不足以捕捉人工智能系统如何从工具发展到伙伴，尤其是在基础模型展现出涌现能力，多智能体系统表现出自主目标设定行为时。&lt;h4&gt;目的&lt;/h4&gt;HAIG框架旨在更好地描述系统发展过程中代理权的复杂分配模式，以及信任关系的维护。&lt;h4&gt;方法&lt;/h4&gt;HAIG框架在三个层面运作：维度（决策权分配、过程自主性和问责配置）、连续体（每个维度的渐进变化）和阈值（需要治理适应的关键点）。&lt;h4&gt;主要发现&lt;/h4&gt;HAIG框架采用信任-效用导向，关注维持适当的信任关系，以最大化效用并确保足够的保障措施。分析揭示了自我监督、推理权限和分布式决策在非均匀信任演化中的作用，这些演化是在情境变化和技术进步的驱动下发生的。&lt;h4&gt;结论&lt;/h4&gt;案例研究在医疗保健和欧洲法规中展示了HAIG框架如何补充现有框架，并为预测治理挑战的替代方法提供基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为HAIG的框架，用于分析人类与人工智能之间信任动态的发展。现有的分类框架，如“人机交互”模型，无法充分捕捉人工智能系统从工具发展到伙伴的过程，尤其是在基础模型展现出涌现能力，多智能体系统表现出自主目标设定行为时。HAIG框架在三个层面运作：维度（决策权分配、过程自主性和问责配置）、连续体（每个维度的渐进变化）和阈值（需要治理适应的关键点）。该框架采用信任-效用导向，关注维持适当的信任关系，以最大化效用并确保足够的保障措施。研究发现，自我监督、推理权限和分布式决策在非均匀信任演化中发挥了作用，这些演化是在情境变化和技术进步的驱动下发生的。在医疗保健和欧洲法规的案例研究中，HAIG框架展示了其如何补充现有框架，并为预测治理挑战的替代方法提供基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces the HAIG framework for analysing trust dynamics acrossevolving human-AI relationships. Current categorical frameworks (e.g.,"human-in-the-loop" models) inadequately capture how AI systems evolve fromtools to partners, particularly as foundation models demonstrate emergentcapabilities and multi-agent systems exhibit autonomous goal-settingbehaviours. As systems advance, agency redistributes in complex patterns thatare better represented as positions along continua rather than discretecategories, though progression may include both gradual shifts and significantstep changes. The HAIG framework operates across three levels: dimensions(Decision Authority Distribution, Process Autonomy, and AccountabilityConfiguration), continua (gradual shifts along each dimension), and thresholds(critical points requiring governance adaptation). Unlike risk-based orprinciple-based approaches, HAIG adopts a trust-utility orientation, focusingon maintaining appropriate trust relationships that maximise utility whileensuring sufficient safeguards. Our analysis reveals how technical advances inself-supervision, reasoning authority, and distributed decision-making drivenon-uniform trust evolution across both contextual variation and technologicaladvancement. Case studies in healthcare and European regulation demonstrate howHAIG complements existing frameworks while offering a foundation foralternative approaches that anticipate governance challenges before theyemerge.</description>
      <author>example@mail.com (Zeynep Engin)</author>
      <guid isPermaLink="false">2505.01651v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>PainFormer: a Vision Foundation Model for Automatic Pain Assessment</title>
      <link>http://arxiv.org/abs/2505.01571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PainFormer的自动疼痛评估系统，通过多任务学习在14个任务/数据集上训练，能够有效提取多种输入模态的高质量嵌入，并在疼痛评估方面取得卓越表现。&lt;h4&gt;背景&lt;/h4&gt;疼痛是一个影响大量人群的复杂状况，准确可靠的疼痛评估对于开发有效的疼痛管理方案至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够进行连续监测并支持决策过程的自动疼痛评估系统，以减轻痛苦并预防功能下降。&lt;h4&gt;方法&lt;/h4&gt;PainFormer是一个基于视觉的多任务学习基础模型，同时训练于14个任务/数据集，共计1090万个样本。它作为各种输入模态的嵌入提取器，为基于Transformer的Embedding-Mixer模块提供特征表示，该模块执行最终的疼痛评估。&lt;h4&gt;主要发现&lt;/h4&gt;使用包括RGB、合成热成像和估计深度视频在内的行为模态以及ECG、EMG、GSR和fNIRS在内的生理模态的广泛实验表明，PainFormer能够从不同的输入模态中有效提取高质量嵌入。&lt;h4&gt;结论&lt;/h4&gt;在单模态和多模态设置中进行的实验证明了该框架在各个模态中均达到最先进的性能，为通用自动疼痛评估模型的发展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;The study proposes an automatic pain assessment system named PainFormer, which is a vision foundation model based on multi-task learning principles trained on 14 tasks/datasets with a total of 10.9 million samples. It acts as an embedding extractor for various input modalities, providing feature representations to the Embedding-Mixer module, which is based on Transformer and performs the final pain assessment. Extensive experiments using behavioral modalities such as RGB, synthetic thermal, and estimated depth videos, as well as physiological modalities such as ECG, EMG, GSR, and fNIRS, show that PainFormer can effectively extract high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 73 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pain is a manifold condition that impacts a significant percentage of thepopulation. Accurate and reliable pain evaluation for the people suffering iscrucial to developing effective and advanced pain management protocols.Automatic pain assessment systems provide continuous monitoring and supportdecision-making processes, ultimately aiming to alleviate distress and preventfunctionality decline. This study introduces PainFormer, a vision foundationmodel based on multi-task learning principles trained simultaneously on 14tasks/datasets with a total of 10.9 million samples. Functioning as anembedding extractor for various input modalities, the foundation model providesfeature representations to the Embedding-Mixer, a transformer-based module thatperforms the final pain assessment. Extensive experiments employing behavioralmodalities-including RGB, synthetic thermal, and estimated depth videos-andphysiological modalities such as ECG, EMG, GSR, and fNIRS revealed thatPainFormer effectively extracts high-quality embeddings from diverse inputmodalities. The proposed framework is evaluated on two pain datasets, BioVidand AI4Pain, and directly compared to 73 different methodologies documented inthe literature. Experiments conducted in unimodal and multimodal settingsdemonstrate state-of-the-art performances across modalities and pave the waytoward general-purpose models for automatic pain assessment.</description>
      <author>example@mail.com (Stefanos Gkikas, Raul Fernandez Rojas, Manolis Tsiknakis)</author>
      <guid isPermaLink="false">2505.01571v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning</title>
      <link>http://arxiv.org/abs/2505.01558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the 2025 CVPR Workshop on Foundation and Large Vision  Models in Remote Sensing, to appear in CVPR 2025 Workshop Proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用新兴地理空间基础模型进行领域泛化的方法，通过结合软对齐伪标签和源到目标生成预训练，以提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;遥感技术在土地覆盖和土地利用制图、作物产量预测以及环境监测等方面有广泛应用。尽管卫星技术的发展扩大了遥感数据集，但高性能分割模型仍然依赖于大量标注数据，受到标注稀缺性和传感器、光照和地理差异性的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种领域泛化方法，以改善模型泛化能力。&lt;h4&gt;方法&lt;/h4&gt;结合软对齐伪标签和源到目标生成预训练，利用新兴地理空间基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;新的数学洞察力被应用于基于MAE的生成学习，以实现领域不变特征学习。&lt;h4&gt;结论&lt;/h4&gt;在超光谱和多光谱遥感数据集上的实验证实了该方法在提高适应性和分割效果方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：遥感技术使多种关键应用成为可能，如土地覆盖和土地利用制图、作物产量预测和环境监测。卫星技术的进步扩大了遥感数据集，但高性能分割模型仍然依赖于大量标注数据，面临着标注稀缺性和传感器、光照和地理差异性的挑战。领域自适应提供了一种改善模型泛化能力的有希望的方法。本文介绍了一种利用新兴地理空间基础模型进行领域泛化的方法，通过结合软对齐伪标签与源到目标生成预训练。我们进一步对基于MAE的生成学习进行了新的数学洞察，以实现领域不变特征学习。在超光谱和多光谱遥感数据集上的实验证实了我们的方法在提高适应性和分割效果方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing enables a wide range of critical applications such as landcover and land use mapping, crop yield prediction, and environmentalmonitoring. Advances in satellite technology have expanded remote sensingdatasets, yet high-performance segmentation models remain dependent onextensive labeled data, challenged by annotation scarcity and variabilityacross sensors, illumination, and geography. Domain adaptation offers apromising solution to improve model generalization. This paper introduces adomain generalization approach to leveraging emerging geospatial foundationmodels by combining soft-alignment pseudo-labeling with source-to-targetgenerative pre-training. We further provide new mathematical insights intoMAE-based generative learning for domain-invariant feature learning.Experiments with hyperspectral and multispectral remote sensing datasetsconfirm our method's effectiveness in enhancing adaptability and segmentation.</description>
      <author>example@mail.com (Anan Yaghmour, Melba M. Crawford, Saurabh Prasad)</author>
      <guid isPermaLink="false">2505.01558v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos</title>
      <link>http://arxiv.org/abs/2505.01481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoHallu，一个用于评估合成视频真实性的基准，并探讨了现有模型在常识和物理法则上的幻觉问题。&lt;h4&gt;背景&lt;/h4&gt;合成视频生成技术因其真实性和广泛应用而受到关注，但现有模型在生成内容时往往忽视常识和物理法则，导致异常内容。&lt;h4&gt;目的&lt;/h4&gt;提出VideoHallu基准，用于检测合成视频中的异常内容，并研究如何提高多模态大型语言模型（MLLMs）在合成视频中的推理能力。&lt;h4&gt;方法&lt;/h4&gt;设计包含多种类别和专家设计的问答任务的基准，评估多个MLLMs模型，并通过Group Relative Policy Optimization（GRPO）对模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;尽管在MVBench和MovieChat等任务上表现良好，这些模型在合成视频中的常识和物理任务上仍存在幻觉问题。微调后，模型在推理能力上取得了显著提升。&lt;h4&gt;结论&lt;/h4&gt;VideoHallu基准有助于评估合成视频的真实性，微调MLLMs可以提升其在合成视频中的推理能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces VideoHallu, a benchmark for evaluating the authenticity of synthetic videos, and explores the hallucination problems of existing models in common sense and physical laws.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic video generation with foundation models has gained attention forits realism and wide applications. While these models produce high-qualityframes, they often fail to respect common sense and physical laws, resulting inabnormal content. Existing metrics like VideoScore emphasize general qualitybut ignore such violations and lack interpretability. A more insightfulapproach is using multi-modal large language models (MLLMs) as interpretableevaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalitiesin synthetic videos remains underexplored. To address this, we introduceVideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora,and Kling, paired with expert-designed QA tasks solvable via human-levelreasoning across various categories. We assess several SoTA MLLMs, includingGPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 andVideoChat-R1. Despite strong real-world performance on MVBench and MovieChat,these models still hallucinate on basic commonsense and physics tasks insynthetic settings, underscoring the challenge of hallucination. We furtherfine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on realand synthetic commonsense/physics data. Results show notable accuracy gains,especially with counterexample integration, advancing MLLMs' reasoningcapabilities. Our data is available at https://github.com/zli12321/VideoHallu.</description>
      <author>example@mail.com (Zongxia Li, Xiyang Wu, Yubin Qin, Guangyao Shi, Hongyang Du, Dinesh Manocha, Tianyi Zhou, Jordan Lee Boyd-Graber)</author>
      <guid isPermaLink="false">2505.01481v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials</title>
      <link>http://arxiv.org/abs/2505.01438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在动态加载下，两相随机材料（TRMs）中的全局应力演变和时空超分辨率问题。&lt;h4&gt;背景&lt;/h4&gt;材料应力分析对材料和性能优化至关重要。在动态加载下，材料的全局应力演变表现出复杂的时空特性，特别是在两相随机材料中。这种材料的失效通常与应力集中有关，相界面是应力集中的关键位置。&lt;h4&gt;目的&lt;/h4&gt;解决在实际工程应用中，由于微结构数据的时空分辨率有限，深学习方法在生成高分辨率时空应力场方面面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种动态加载下两相随机材料全局应力生成和时空超分辨率的框架。首先，引入了一种基于扩散模型的时空应力扩散（STS-diffusion）方法来生成全局时空应力数据。其次，开发了一种物理信息网络，称为时空超分辨率物理信息算子（ST-SRPINN），用于时空超分辨率。ST-SRPINN是一种无监督学习方法，详细探讨了数据驱动和物理信息损失函数权重对模型准确性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架通过物理约束，可以在训练过程中仅需要低分辨率的应力场数据，并将其时空分辨率升级到任意放大倍数。&lt;h4&gt;结论&lt;/h4&gt;该研究为两相随机材料在动态加载下的应力分析和超分辨率提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;Material stress analysis is a critical aspect of material design and performance optimization. Under dynamic loading, the global stress evolution in materials exhibits complex spatiotemporal characteristics, especially in two-phase random materials (TRMs). Such kind of material failure is often associated with stress concentration, and the phase boundaries are key locations where stress concentration occurs. In practical engineering applications, the spatiotemporal resolution of acquired microstructural data and its dynamic stress evolution is often limited. This poses challenges for deep learning methods in generating high-resolution spatiotemporal stress fields, particularly for accurately capturing stress concentration regions. In this study, we propose a framework for global stress generation and spatiotemporal super-resolution in TRMs under dynamic loading. First, we introduce a diffusion model-based approach, named as Spatiotemporal Stress Diffusion (STS-diffusion), for generating global spatiotemporal stress data. This framework incorporates Space-Time U-Net (STU-net), and we systematically investigate the impact of different attention positions on model accuracy. Next, we develop a physics-informed network for spatiotemporals super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning method. The influence of data-driven and physics-informed loss function weights on model accuracy is explored in detail. Benefiting from physics-based constraints, ST-SRPINN requires only low-resolution stress field data during training and can upscale the spatiotemporal resolution of stress fields to arbitrary magnifications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Material stress analysis is a critical aspect of material design andperformance optimization. Under dynamic loading, the global stress evolution inmaterials exhibits complex spatiotemporal characteristics, especially intwo-phase random materials (TRMs). Such kind of material failure is oftenassociated with stress concentration, and the phase boundaries are keylocations where stress concentration occurs. In practical engineeringapplications, the spatiotemporal resolution of acquired microstructural dataand its dynamic stress evolution is often limited. This poses challenges fordeep learning methods in generating high-resolution spatiotemporal stressfields, particularly for accurately capturing stress concentration regions. Inthis study, we propose a framework for global stress generation andspatiotemporal super-resolution in TRMs under dynamic loading. First, weintroduce a diffusion model-based approach, named as Spatiotemporal StressDiffusion (STS-diffusion), for generating global spatiotemporal stress data.This framework incorporates Space-Time U-Net (STU-net), and we systematicallyinvestigate the impact of different attention positions on model accuracy.Next, we develop a physics-informed network for spatiotemporalsuper-resolution, termed as Spatiotemporal Super-Resolution Physics-InformedOperator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learningmethod. The influence of data-driven and physics-informed loss function weightson model accuracy is explored in detail. Benefiting from physics-basedconstraints, ST-SRPINN requires only low-resolution stress field data duringtraining and can upscale the spatiotemporal resolution of stress fields toarbitrary magnifications.</description>
      <author>example@mail.com (Tengfei Xing, Xiaodan Ren, Jie Li)</author>
      <guid isPermaLink="false">2505.01438v1</guid>
      <pubDate>Tue, 06 May 2025 14:25:43 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Enabled System Diagnosis in Microgrids: A Feature-Feedback GAN Approach</title>
      <link>http://arxiv.org/abs/2505.01366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对逆变器微电网的两阶段故障和网络安全攻击检测框架。&lt;h4&gt;背景&lt;/h4&gt;逆变器资源和通信网络的集成带来了现代化，同时也引入了新的电力系统基础设施脆弱性，特别是虚假数据注入（FDI）攻击。&lt;h4&gt;目的&lt;/h4&gt;开发一种检测逆变器微电网内部故障和网络安全攻击的方法。&lt;h4&gt;方法&lt;/h4&gt;第一阶段引入了无监督学习模型F2GAN来区分微电网中的真实内部故障和由网络引起的异常。第二阶段应用了支持向量机（SVM）、k-最近邻（KNN）、决策树（DT）和人工神经网络（ANN）来定位和分类逆变器开关中的故障。&lt;h4&gt;主要发现&lt;/h4&gt;F2GAN在系统诊断和适应零日攻击方面优于传统的GAN架构。所提出的框架在模拟微电网环境中得到了验证，表明其在检测和分类物理和网络安全干扰方面具有稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架能够有效检测和分类逆变器微电网中的故障和网络安全攻击，对电力电子主导系统中的干扰具有识别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing integration of inverter-based resources (IBRs) andcommunication networks has brought both modernization and new vulnerabilitiesto the power system infrastructure. These vulnerabilities expose the system tointernal faults and cyber threats, particularly False Data Injection (FDI)attacks, which can closely mimic real fault scenarios. Hence, this workpresents a two-stage fault and cyberattack detection framework tailored forinverter-based microgrids. Stage 1 introduces an unsupervised learning modelFeature Feedback Generative Adversarial Network (F2GAN), to distinguish betweengenuine internal faults and cyber-induced anomalies in microgrids. Compared toconventional GAN architectures, F2GAN demonstrates improved system diagnosisand greater adaptability to zero-day attacks through its feature-feedbackmechanism. In Stage 2, supervised machine learning techniques, includingSupport Vector Machines (SVM), k-Nearest Neighbors (KNN), Decision Trees (DT),and Artificial Neural Networks (ANN) are applied to localize and classifyfaults within inverter switches, distinguishing between single-switch andmulti-switch faults. The proposed framework is validated on a simulatedmicrogrid environment, illustrating robust performance in detecting andclassifying both physical and cyber-related disturbances in powerelectronic-dominated systems.</description>
      <author>example@mail.com (Swetha Rani Kasimalla, Kuchan Park, Junho Hong, Young-Jin Kim, HyoJong Lee)</author>
      <guid isPermaLink="false">2505.01366v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
  <item>
      <title>Tightly Coupled Range Inertial Odometry and Mapping with Exact Point Cloud Downsampling</title>
      <link>http://arxiv.org/abs/2505.01017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE International Conference on Robotics and Automation (ICRA2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于因子图的点云下采样算法，用于实时处理多扫描配准误差最小化，并设计了一个完整的SLAM框架，该框架在标准CPU上实时运行，实验结果表明，该框架优于现有的基于CPU的SLAM框架。&lt;h4&gt;背景&lt;/h4&gt;为了实时处理多扫描配准误差最小化问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种算法，以减少需要评估的残差数量，同时保持采样点的无近似误差。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于核心集提取的点云下采样算法，该算法从输入点的残差中提取子集，使得子集在给定姿态下产生与原始集相同的二次误差函数。此外，还设计了一个完整的SLAM框架，包括基于滑动窗口优化的里程计估计和基于整个地图注册误差最小化的全局轨迹优化。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在标准CPU上实时运行，且实验结果表明，该框架优于现有的基于CPU的SLAM框架，无需使用GPU加速。&lt;h4&gt;结论&lt;/h4&gt;提出的算法和SLAM框架能够有效减少计算量，提高处理速度，且性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;In this work, to facilitate the real-time processing of multi-scan registration error minimization on factor graphs, we devise a point cloud downsampling algorithm based on coreset extraction. This algorithm extracts a subset of the residuals of input points such that the subset yields exactly the same quadratic error function as that of the original set for a given pose. This enables a significant reduction in the number of residuals to be evaluated without approximation errors at the sampling point. Using this algorithm, we devise a complete SLAM framework that consists of odometry estimation based on sliding window optimization and global trajectory optimization based on registration error minimization over the entire map, both of which can run in real time on a standard CPU. The experimental results demonstrate that the proposed framework outperforms state-of-the-art CPU-based SLAM frameworks without the use of GPU acceleration.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, to facilitate the real-time processing of multi-scanregistration error minimization on factor graphs, we devise a point clouddownsampling algorithm based on coreset extraction. This algorithm extracts asubset of the residuals of input points such that the subset yields exactly thesame quadratic error function as that of the original set for a given pose.This enables a significant reduction in the number of residuals to be evaluatedwithout approximation errors at the sampling point. Using this algorithm, wedevise a complete SLAM framework that consists of odometry estimation based onsliding window optimization and global trajectory optimization based onregistration error minimization over the entire map, both of which can run inreal time on a standard CPU. The experimental results demonstrate that theproposed framework outperforms state-of-the-art CPU-based SLAM frameworkswithout the use of GPU acceleration.</description>
      <author>example@mail.com (Kenji Koide, Aoki Takanose, Shuji Oishi, Masashi Yokozuka)</author>
      <guid isPermaLink="false">2505.01017v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades</title>
      <link>http://arxiv.org/abs/2505.01415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在佛罗里达州的艾维尔吉斯地区进行水位预测的模型，探讨了多种模型在预测洪水和干旱调节、水资源规划和生态系统管理方面的应用。&lt;h4&gt;背景&lt;/h4&gt;艾维尔吉斯在洪水和干旱调节、水资源规划和生态系统管理中扮演着关键角色，但传统的预测方法存在计算成本高和适应性差的问题。&lt;h4&gt;目的&lt;/h4&gt;研究十二种特定任务的模型和五种时间序列基础模型，以解决艾维尔吉斯地区水位预测的挑战。&lt;h4&gt;方法&lt;/h4&gt;在艾维尔吉斯地区进行了水位预测的实际应用研究，测试了多种模型，包括Chronos等基础模型和特定任务的模型。&lt;h4&gt;主要发现&lt;/h4&gt;Chronos基础模型在所有模型中表现最佳，其他基础模型表现相对较差。特定模型的表现因模型架构而异。&lt;h4&gt;结论&lt;/h4&gt;本文的研究结果表明，深度学习和基础模型在艾维尔吉斯地区的水位预测中具有潜力，但不同模型的表现差异较大。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Everglades在洪水和干旱调节、水资源规划以及周边地区的生态系统管理中发挥着至关重要的作用。然而，传统的基于物理和统计的方法在预测水位时往往面临重大挑战，包括高计算成本和有限的适应不同或不可预见条件的能力。最近在大型时间序列模型方面的进步表明，它们有潜力解决这些限制，最先进的深度学习和基础模型在各种领域的时间序列预测中取得了显著的成功。尽管取得了这些进展，但它们在关键环境系统，如Everglades中的应用仍然很少被探索。在这项研究中，我们通过调查十二种特定任务的模型和五种时间序列基础模型，填补了这一空白，这些模型涵盖了六个类别，以关注Everglades水位预测的实际应用。我们的主要结果表明，基础模型Chronos显著优于所有其他模型，而其他基础模型表现出相对较差的性能。此外，特定模型的表现因模型架构而异。最后，我们讨论了模型性能差异的可能原因。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Everglades play a crucial role in flood and drought regulation, waterresource planning, and ecosystem management in the surrounding regions.However, traditional physics-based and statistical methods for predicting waterlevels often face significant challenges, including high computational costsand limited adaptability to diverse or unforeseen conditions. Recentadvancements in large time series models have demonstrated the potential toaddress these limitations, with state-of-the-art deep learning and foundationmodels achieving remarkable success in time series forecasting across variousdomains. Despite this progress, their application to critical environmentalsystems, such as the Everglades, remains underexplored. In this study, we fillthe gap by investigating twelve task-specific models and five time seriesfoundation models across six categories for a real-world application focused onwater level prediction in the Everglades. Our primary results show that thefoundation model, Chronos, significantly outperforms all other models while theremaining foundation models exhibit relatively poor performance. Moreover, theperformance of task-specific models varies with the model architectures.Lastly, we discuss the possible reasons for the varying performance of models.</description>
      <author>example@mail.com (Rahuul Rangaraj, Jimeng Shi, Azam Shirali, Rajendra Paudel, Yanzhao Wu, Giri Narasimhan)</author>
      <guid isPermaLink="false">2505.01415v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Qracle: A Graph-Neural-Network-based Parameter Initializer for Variational Quantum Eigensolvers</title>
      <link>http://arxiv.org/abs/2505.01236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的参数初始化方法Qracle，用于解决变分量子本征求解器（VQEs）在复杂问题中的优化挑战。&lt;h4&gt;背景&lt;/h4&gt;VQEs是NISQ算法中的领先类别，在量子物理和量子化学中有广泛应用。但随着系统规模的增加，VQE优化受到 barren plateau现象的阻碍，导致梯度消失和损失函数陷入局部最小值。&lt;h4&gt;目的&lt;/h4&gt;提出Qracle以解决VQE优化中的挑战，提高初始损失、加速收敛并改善最终性能。&lt;h4&gt;方法&lt;/h4&gt;Qracle将哈密顿量和相关基函数电路编码为统一的图表示，并利用GNN学习从VQE问题图到优化基函数参数的映射。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进的初始化技术相比，Qracle将初始损失减少了10.86，通过减少优化步骤加速收敛高达64.42%，并使对称平均绝对百分比误差（SMAPE）降低了26.43%。&lt;h4&gt;结论&lt;/h4&gt;Qracle是一种有效的VQE参数初始化方法，能够显著提高VQE问题的优化性能。&lt;h4&gt;翻译&lt;/h4&gt;Variational Quantum Eigensolvers (VQEs) 是一种在量子物理和量子化学中具有广泛应用的前沿噪声中等规模量子（NISQ）算法。然而，随着系统规模的增加，VQE优化越来越受到 barren plateau 现象的阻碍，其中梯度消失，损失函数陷入局部最小值。虽然已经提出了基于机器学习的参数初始化方法来应对这一挑战，但它们在复杂的 VQE 问题中往往效果有限。这主要是因为它们无法充分模拟嵌入在哈密顿量结构和相关基函数电路中的复杂相关性。在本文中，我们提出了一种基于图神经网络（GNN）的 VQE 参数初始化方法 Qracle。Qracle 系统地将哈密顿量和相关基函数电路编码为统一的图表示，并利用 GNN 学习从 VQE 问题图到优化基函数参数的映射。与最先进的初始化技术相比，Qracle 实现了初始损失的减少高达 10.86，通过减少优化步骤加速收敛高达 64.42%，并使对称平均绝对百分比误差（SMAPE）降低了 26.43%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variational Quantum Eigensolvers (VQEs) are a leading class of noisyintermediate-scale quantum (NISQ) algorithms with broad applications in quantumphysics and quantum chemistry. However, as system size increases, VQEoptimization is increasingly hindered by the barren plateau phenomenon, wheregradients vanish and the loss function becomes trapped in local minima. Whilemachine learning-based parameter initialization methods have been proposed toaddress this challenge, they often show limited effectiveness in complex VQEproblems. This is primarily due to their inadequate ability to model theintricate correlations embedded in the Hamiltonian structure and the associatedansatz circuits. In this paper, we propose \textit{Qracle}, a graph neuralnetwork (GNN)-based parameter initializer for VQEs. \textit{Qracle}systematically encodes both the Hamiltonian and the associated ansatz circuitinto a unified graph representation and leverages a GNN to learn a mapping fromVQE problem graphs to optimized ansatz parameters. Compared to state-of-the-artinitialization techniques, \textit{Qracle} achieves a reduction in initial lossof up to $10.86$, accelerates convergence by decreasing optimization steps byup to $64.42\%$, and improves final performance with up to a $26.43\%$reduction in Symmetric Mean Absolute Percentage Error (SMAPE).</description>
      <author>example@mail.com (Chi Zhang, Lei Jiang, Fan Chen)</author>
      <guid isPermaLink="false">2505.01236v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>SMSAT: A Multimodal Acoustic Dataset and Deep Contrastive Learning Framework for Affective and Physiological Modeling of Spiritual Meditation</title>
      <link>http://arxiv.org/abs/2505.00839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了听觉刺激如何影响情绪和生理状态，提出了一个多模态评估方法，并引入了新的数据集和深度学习框架，用于分析精神冥想、音乐和自然寂静三种听觉条件下的情感和生理影响。&lt;h4&gt;背景&lt;/h4&gt;理解听觉刺激如何影响情绪和生理状态对于情感计算和心理健康技术的发展至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估三种听觉条件（精神冥想、音乐和自然寂静）对情感和生理状态的影响。&lt;h4&gt;方法&lt;/h4&gt;使用生物特征信号测量方法，引入了Spiritual, Music, Silence Acoustic Time Series (SMSAT)数据集，并开发了一个基于对比学习的SMSAT音频编码器以及一个集成25个手工和学习的特征的Calmness Analysis Model (CAM)。&lt;h4&gt;主要发现&lt;/h4&gt;SMSAT音频编码器实现了99.99%的分类准确率，CAM在情感状态分类中达到了99.99%的分类准确率。SM分析通过ANOVA显示显著的生理波动。与现有方法的最高90%的准确率相比，本文提出的方法性能提升显著，最高达到99%。&lt;h4&gt;结论&lt;/h4&gt;本文为情感计算应用提供了经过验证的多模态数据集和可扩展的深度学习框架，可用于压力监测、心理健康和基于音频的治疗干预。&lt;h4&gt;翻译&lt;/h4&gt;Understanding how auditory stimuli influence emotional and physiological states is fundamental to advancing affective computing and mental health technologies. In this paper, we present a multimodal evaluation of the affective and physiological impacts of three auditory conditions, that is, spiritual meditation (SM), music (M), and natural silence (NS), using a comprehensive suite of biometric signal measures. To facilitate this analysis, we introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT) dataset, a novel benchmark comprising acoustic time series (ATS) signals recorded under controlled exposure protocols, with careful attention to demographic diversity and experimental consistency. To model the auditory induced states, we develop a contrastive learning based SMSAT audio encoder that extracts highly discriminative embeddings from ATS data, achieving 99.99% classification accuracy in interclass and intraclass evaluations. Furthermore, we propose the Calmness Analysis Model (CAM), a deep learning framework integrating 25 handcrafted and learned features for affective state classification across auditory conditions, attaining robust 99.99% classification accuracy. In contrast, pairwise t tests reveal significant deviations in cardiac response characteristics (CRC) between SM analysis via ANOVA inducing more significant physiological fluctuations. Compared to existing state of the art methods reporting accuracies up to 90%, the proposed model demonstrates substantial performance gains (up to 99%). This work contributes a validated multimodal dataset and a scalable deep learning framework for affective computing applications in stress monitoring, mental well-being, and therapeutic audio-based interventions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how auditory stimuli influence emotional and physiologicalstates is fundamental to advancing affective computing and mental healthtechnologies. In this paper, we present a multimodal evaluation of theaffective and physiological impacts of three auditory conditions, that is,spiritual meditation (SM), music (M), and natural silence (NS), using acomprehensive suite of biometric signal measures. To facilitate this analysis,we introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT)dataset, a novel benchmark comprising acoustic time series (ATS) signalsrecorded under controlled exposure protocols, with careful attention todemographic diversity and experimental consistency. To model the auditoryinduced states, we develop a contrastive learning based SMSAT audio encoderthat extracts highly discriminative embeddings from ATS data, achieving 99.99%classification accuracy in interclass and intraclass evaluations. Furthermore,we propose the Calmness Analysis Model (CAM), a deep learning frameworkintegrating 25 handcrafted and learned features for affective stateclassification across auditory conditions, attaining robust 99.99%classification accuracy. In contrast, pairwise t tests reveal significantdeviations in cardiac response characteristics (CRC) between SM analysis viaANOVA inducing more significant physiological fluctuations. Compared toexisting state of the art methods reporting accuracies up to 90%, the proposedmodel demonstrates substantial performance gains (up to 99%). This workcontributes a validated multimodal dataset and a scalable deep learningframework for affective computing applications in stress monitoring, mentalwell-being, and therapeutic audio-based interventions.</description>
      <author>example@mail.com (Ahmad Suleman, Yazeed Alkhrijah, Misha Urooj Khan, Hareem Khan, Muhammad Abdullah Husnain Ali Faiz, Mohamad A. Alawad, Zeeshan Kaleem, Guan Gui)</author>
      <guid isPermaLink="false">2505.00839v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing User Sequence Modeling through Barlow Twins-based Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2505.00953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Barlow Twins的自监督学习方法，用于用户序列建模，旨在减少对大量负样本的需求，从而在有限的标记数据和负样本情况下实现有效的表示学习。&lt;h4&gt;背景&lt;/h4&gt;用户序列建模对于现代大规模推荐系统至关重要，因为它可以从用户的历史交互中提取用户和项目的有用表示。然而，学习这些表示的一个关键挑战是缺乏标记的训练数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种适应Barlow Twins的自监督学习方法，通过结合合适的增强方法，减少对大量负样本的需求，以实现有效的表示学习。&lt;h4&gt;方法&lt;/h4&gt;在用户序列建模中应用Barlow Twins，并引入适当的增强方法，以减少对负样本批次的需求，从而在较小的批次大小和有限的标记数据下进行有效的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在MovieLens-1M、MovieLens-20M和Yelp数据集上评估了该方法，结果显示在三个下游任务中，该方法始终优于广泛使用的双编码器模型，准确率提高了8%-20%。&lt;h4&gt;结论&lt;/h4&gt;该方法在提取用户建模中的有价值序列信息方面非常有效，特别是在标记数据稀缺和负样本有限的情况下。&lt;h4&gt;翻译&lt;/h4&gt;摘要：用户序列建模对于现代大规模推荐系统至关重要，因为它能够从用户的历史交互中提取用户和物品的有用表示。这些用户表示被广泛用于各种下游任务，以增强用户的在线体验。学习这些表示的一个关键挑战是缺乏标记的训练数据。虽然自监督学习（SSL）方法已经出现，作为一种从无标签数据中学习表示的有希望解决方案，但许多现有方法依赖于大量的负样本采样，这可能是计算上昂贵的，并且在现实场景中可能并不总是可行。在本工作中，我们提出了一种Barlow Twins（一种最先进的SSL方法）的改编，用于用户序列建模，通过结合适当的增强方法。我们的方法旨在减少对大量负样本批次的需求，从而使用较小的批次大小和有限的标记数据实现有效的表示学习。我们在MovieLens-1M、MovieLens-20M和Yelp数据集上评估了我们的方法，表明我们的方法在三个下游任务中始终优于广泛使用的双编码器模型，实现了8%-20%的准确率提升。我们的发现强调了我们的方法在提取用户建模的有价值序列信息方面的有效性，尤其是在标记数据稀缺和负样本有限的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User sequence modeling is crucial for modern large-scale recommendationsystems, as it enables the extraction of informative representations of usersand items from their historical interactions. These user representations arewidely used for a variety of downstream tasks to enhance users' onlineexperience. A key challenge for learning these representations is the lack oflabeled training data. While self-supervised learning (SSL) methods haveemerged as a promising solution for learning representations from unlabeleddata, many existing approaches rely on extensive negative sampling, which canbe computationally expensive and may not always be feasible in real-worldscenario. In this work, we propose an adaptation of Barlow Twins, astate-of-the-art SSL methods, to user sequence modeling by incorporatingsuitable augmentation methods. Our approach aims to mitigate the need for largenegative sample batches, enabling effective representation learning withsmaller batch sizes and limited labeled data. We evaluate our method on theMovieLens-1M, MovieLens-20M, and Yelp datasets, demonstrating that our methodconsistently outperforms the widely-used dual encoder model across threedownstream tasks, achieving an 8%-20% improvement in accuracy. Our findingsunderscore the effectiveness of our approach in extracting valuablesequence-level information for user modeling, particularly in scenarios wherelabeled data is scarce and negative examples are limited.</description>
      <author>example@mail.com (Yuhan Liu, Lin Ning, Neo Wu, Karan Singhal, Philip Andrew Mansfield, Devora Berlowitz, Sushant Prakash, Bradley Green)</author>
      <guid isPermaLink="false">2505.00953v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>A Physics-preserved Transfer Learning Method for Differential Equations</title>
      <link>http://arxiv.org/abs/2505.01281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对微分方程的通用迁移学习方法，该方法能够自适应地纠正领域偏移并保留物理信息。&lt;h4&gt;背景&lt;/h4&gt;尽管基于数据的神经网络算子等在求解微分方程方面取得了成功，但它们受到不同学习环境（数据偏差或方程变化）导致的领域偏移问题的影响，这些问题可以通过迁移学习（TL）来缓解。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有迁移学习方法在微分方程问题中缺乏泛化能力或训练过程中的物理信息保留的问题。&lt;h4&gt;方法&lt;/h4&gt;将数据域表征为产品分布，将基本问题表征为分布偏差和算子偏差。提出了一种物理保留最优张量传输（POTT）方法，该方法同时允许对常见微分方程的泛化性和对特定问题的物理信息保留，利用POTT映射诱导的推前分布将数据驱动模型适应到目标域。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，证明了所提出的POTT方法在性能、泛化性和物理信息保留方面的优越性。&lt;h4&gt;结论&lt;/h4&gt;POTT方法能够有效地解决微分方程中的领域偏移问题，同时保持物理信息的准确性。&lt;h4&gt;翻译&lt;/h4&gt;While data-driven methods such as neural operator have achieved great success in solving differential equations (DEs), they suffer from domain shift problems caused by different learning environments (with data bias or equation changes), which can be alleviated by transfer learning (TL). However, existing TL methods adopted in DEs problems lack either generalizability in general DEs problems or physics preservation during training. In this work, we focus on a general transfer learning method that adaptively correct the domain shift and preserve physical information. Mathematically, we characterize the data domain as product distribution and the essential problems as distribution bias and operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that simultaneously admits generalizability to common DEs and physics preservation of specific problem is proposed to adapt the data-driven model to target domain utilizing the push-forward distribution induced by the POTT map. Extensive experiments demonstrate the superior performance, generalizability and physics preservation of the proposed POTT method.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While data-driven methods such as neural operator have achieved great successin solving differential equations (DEs), they suffer from domain shift problemscaused by different learning environments (with data bias or equation changes),which can be alleviated by transfer learning (TL). However, existing TL methodsadopted in DEs problems lack either generalizability in general DEs problems orphysics preservation during training. In this work, we focus on a generaltransfer learning method that adaptively correct the domain shift and preservephysical information. Mathematically, we characterize the data domain asproduct distribution and the essential problems as distribution bias andoperator bias. A Physics-preserved Optimal Tensor Transport (POTT) method thatsimultaneously admits generalizability to common DEs and physics preservationof specific problem is proposed to adapt the data-driven model to target domainutilizing the push-forward distribution induced by the POTT map. Extensiveexperiments demonstrate the superior performance, generalizability and physicspreservation of the proposed POTT method.</description>
      <author>example@mail.com (Hao-Ran Yang, Chuan-Xian Ren)</author>
      <guid isPermaLink="false">2505.01281v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors</title>
      <link>http://arxiv.org/abs/2505.01322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FreeInsert的框架，用于在3D场景中通过自然语言实现直观的场景编辑，无需依赖空间先验信息。&lt;h4&gt;背景&lt;/h4&gt;现有的基于2D编辑的方法通常依赖于空间先验，如2D掩膜或3D边界框，难以保证插入对象的一致性，限制了其在现实应用中的灵活性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，使对象生成与空间放置分离，实现3D场景中无空间先验的灵活和自动化的对象插入。&lt;h4&gt;方法&lt;/h4&gt;FreeInsert利用基础模型，包括MLLMs、LGMs和扩散模型，从用户指令中提取结构化语义，包括对象类型、空间关系和附加区域。这些语义指导插入对象的3D一致性重建和自由度学习。MLLMs的空间推理能力用于初始化对象的姿态和比例。一个分层、空间感知的细化阶段进一步整合空间语义和MLLM推断的先验，以增强放置。最后，使用插入对象的图像来提高外观，以增强视觉保真度。&lt;h4&gt;主要发现&lt;/h4&gt;FreeInsert在不依赖空间先验的情况下，实现了语义上连贯、空间上精确和视觉上逼真的3D插入。&lt;h4&gt;结论&lt;/h4&gt;FreeInsert提供了一个用户友好且灵活的编辑体验，为3D场景编辑提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在3D场景中进行文本驱动对象插入是一个新兴的任务，它通过自然语言实现直观的场景编辑。然而，现有的基于2D编辑的方法通常依赖于空间先验，如2D掩膜或3D边界框，并且难以确保插入对象的一致性。这些限制阻碍了其在现实应用中的灵活性和可扩展性。在本文中，我们提出了一种名为FreeInsert的新框架，该框架利用包括MLLMs、LGMs和扩散模型在内的基础模型，将对象生成与空间放置分离。这可以实现3D场景中无空间先验的灵活和自动化的对象插入。FreeInsert从基于MLLM的解析器开始，该解析器从用户指令中提取结构化语义，包括对象类型、空间关系和附加区域。这些语义指导插入对象的3D一致性重建和自由度学习。我们利用MLLMs的空间推理能力来初始化对象的位置和比例。一个分层、空间感知的细化阶段进一步整合空间语义和MLLM推断的先验，以增强放置。最后，使用插入对象的图像来提高外观，以增强视觉保真度。实验结果表明，FreeInsert在不依赖空间先验的情况下，实现了语义上连贯、空间上精确和视觉上逼真的3D插入，提供了一种用户友好且灵活的编辑体验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-driven object insertion in 3D scenes is an emerging task that enablesintuitive scene editing through natural language. However, existing 2Dediting-based methods often rely on spatial priors such as 2D masks or 3Dbounding boxes, and they struggle to ensure consistency of the inserted object.These limitations hinder flexibility and scalability in real-worldapplications. In this paper, we propose FreeInsert, a novel framework thatleverages foundation models including MLLMs, LGMs, and diffusion models todisentangle object generation from spatial placement. This enables unsupervisedand flexible object insertion in 3D scenes without spatial priors. FreeInsertstarts with an MLLM-based parser that extracts structured semantics, includingobject types, spatial relationships, and attachment regions, from userinstructions. These semantics guide both the reconstruction of the insertedobject for 3D consistency and the learning of its degrees of freedom. Weleverage the spatial reasoning capabilities of MLLMs to initialize object poseand scale. A hierarchical, spatially aware refinement stage further integratesspatial semantics and MLLM-inferred priors to enhance placement. Finally, theappearance of the object is improved using the inserted-object image to enhancevisual fidelity. Experimental results demonstrate that FreeInsert achievessemantically coherent, spatially precise, and visually realistic 3D insertionswithout relying on spatial priors, offering a user-friendly and flexibleediting experience.</description>
      <author>example@mail.com (Chenxi Li, Weijie Wang, Qiang Li, Bruno Lepri, Nicu Sebe, Weizhi Nie)</author>
      <guid isPermaLink="false">2505.01322v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L</title>
      <link>http://arxiv.org/abs/2505.00757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于4D雷达的3D物体检测模型，在Hailo-8L AI加速器上实现了芯片级部署，解决了传统3D卷积神经网络架构与Hailo-8L 4D张量支持的兼容性问题。&lt;h4&gt;背景&lt;/h4&gt;4D雷达在自动驾驶领域受到关注，因为它能够在恶劣天气条件下实现鲁棒的3D物体检测。&lt;h4&gt;目的&lt;/h4&gt;在低功耗嵌入式环境中实现4D雷达技术的实时处理。&lt;h4&gt;方法&lt;/h4&gt;提出了一种张量变换方法，在编译过程中将5D输入转换为4D格式，以适应Hailo-8L的4D张量支持。&lt;h4&gt;主要发现&lt;/h4&gt;该系统在Hailo-8L上实现了46.47%的AP_3D和52.75%的AP_BEV，保持了与基于GPU的模型相当的准确性，同时达到了13.76 Hz的推理速度。&lt;h4&gt;结论&lt;/h4&gt;4D雷达感知技术适用于自动驾驶系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D radar has attracted attention in autonomous driving due to its ability toenable robust 3D object detection even under adverse weather conditions. Topractically deploy such technologies, it is essential to achieve real-timeprocessing within low-power embedded environments. Addressing this, we presentthe first on-chip implementation of a 4D radar-based 3D object detection modelon the Hailo-8L AI accelerator. Although conventional 3D convolutional neuralnetwork (CNN) architectures require 5D inputs, the Hailo-8L only supports 4Dtensors, posing a significant challenge. To overcome this limitation, weintroduce a tensor transformation method that reshapes 5D inputs into 4Dformats during the compilation process, enabling direct deployment withoutaltering the model structure. The proposed system achieves 46.47% AP_3D and52.75% AP_BEV, maintaining comparable accuracy to GPU-based models whileachieving an inference speed of 13.76 Hz. These results demonstrate theapplicability of 4D radar-based perception technologies to autonomous drivingsystems.</description>
      <author>example@mail.com (Woong-Chan Byun, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong)</author>
      <guid isPermaLink="false">2505.00757v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-based structural classification of glass-forming liquids and its interpretation via Self-Attention mechanism</title>
      <link>http://arxiv.org/abs/2505.00993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9pages, 5 figures for main text, 12 pages for Supplementary Material&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用机器学习中的图神经网络（GNNs）和自注意力机制，探索了玻璃形成液体在不同温度下的结构变化及其机理。&lt;h4&gt;背景&lt;/h4&gt;玻璃形成液体在熔点以下表现出缓慢的动力学，并保持类似正常液体的非晶结构。区分超冷却和高温下的微观结构是一个有争议的话题。&lt;h4&gt;目的&lt;/h4&gt;研究目的是通过机器学习方法揭示玻璃形成液体结构变化的基本机制。&lt;h4&gt;方法&lt;/h4&gt;研究使用图神经网络自动提取特征，并通过自注意力机制生成注意力系数，量化图节点之间连接的重要性，同时使用物理定义的结构描述符来探索结构随温度降低的变化。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，高注意力系数与更无序的结构之间存在强相关性，这被认为是玻璃形成液体变化的关键指标。&lt;h4&gt;结论&lt;/h4&gt;该研究通过GNN+自注意力方法揭示了玻璃形成液体结构变化的内在机理，为理解这些液体的性质提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;摘要：玻璃形成液体在其熔点以下表现出缓慢的动力学，保持与非晶态液体相似的结构。区分超冷却和高温下的微观结构是一个有争议的话题。基于最近在机器学习，尤其是图神经网络（GNNs）方面的进展，我们的研究自动提取特征，揭示了在不同温度下驱动结构变化的根本机制。我们采用自注意力机制来生成注意力系数，量化图节点之间连接的重要性，为GNN预测背后的原因提供洞察。通过使用物理定义的结构描述符，包括键取向序参数、Voronoi单元体积和配位数，以及GNN+自注意力方法，我们发现了高注意力系数与更无序结构之间的强相关性，这被认为是玻璃形成液体变化的关键指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Glass-forming liquids exhibit slow dynamics below their melting temperatures,maintaining an amorphous structure reminiscent of normal liquids.Distinguishing microscopic structures in the supercooled and high-temperatureregimes remains a debated topic. Building on recent advances in machinelearning, particularly Graph Neural Networks (GNNs), our study automaticallyextracts features, unveiling fundamental mechanisms driving structural changesat varying temperatures. We employ the Self-Attention mechanism to generateattention coefficients that quantify the importance of connections betweengraph nodes, providing insights into the rationale behind GNN predictions.Exploring structural changes with decreasing temperature through theGNN+Self-Attention using physically-defined structural descriptors, includingthe bond-orientational order parameter, Voronoi cell volume, and coordinationnumber, we identify strong correlations between high attention coefficients andmore disordered structures as a key indicator of variations in glass-formingliquids.</description>
      <author>example@mail.com (Kohei Yoshikawa, Kentaro Yano, Shota Goto, Kang Kim, Nobuyuki Matubayasi)</author>
      <guid isPermaLink="false">2505.00993v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging</title>
      <link>http://arxiv.org/abs/2505.01239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对基于深度学习的肺肿瘤分割模型进行了全面基准分析，比较了传统架构如U-Net和DeepLabV3，自配置模型如nnUNet，以及基础模型如MedSAM和MedSAM~2，评估了在不同学习范式下的分割准确性和计算效率，发现基础模型在准确性和计算效率上优于传统模型。&lt;h4&gt;背景&lt;/h4&gt;精确的肺肿瘤分割对于提高肿瘤学中的诊断、治疗规划和患者预后至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估基于深度学习的肺肿瘤分割模型在不同学习范式下的性能，特别是分割准确性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;对U-Net、DeepLabV3、nnUNet、MedSAM和MedSAM~2等模型进行了比较，并在两个肺肿瘤分割数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;传统模型在肿瘤分割方面存在困难，而基础模型MedSAM~2在准确性和计算效率上都优于传统模型。&lt;h4&gt;结论&lt;/h4&gt;基础模型在肺肿瘤分割中具有潜力，有助于改善临床工作流程和患者预后。&lt;h4&gt;翻译&lt;/h4&gt;Accurate lung tumor segmentation is crucial for improving diagnosis, treatment planning, and patient outcomes in oncology. However, the complexity of tumor morphology, size, and location poses significant challenges for automated segmentation. This study presents a comprehensive benchmarking analysis of deep learning-based segmentation models, comparing traditional architectures such as U-Net and DeepLabV3, self-configuring models like nnUNet, and foundation models like MedSAM, and MedSAM~2. Evaluating performance across two lung tumor segmentation datasets, we assess segmentation accuracy and computational efficiency under various learning paradigms, including few-shot learning and fine-tuning. The results reveal that while traditional models struggle with tumor delineation, foundation models, particularly MedSAM~2, outperform them in both accuracy and computational efficiency. These findings underscore the potential of foundation models for lung tumor segmentation, highlighting their applicability in improving clinical workflows and patient outcomes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate lung tumor segmentation is crucial for improving diagnosis,treatment planning, and patient outcomes in oncology. However, the complexityof tumor morphology, size, and location poses significant challenges forautomated segmentation. This study presents a comprehensive benchmarkinganalysis of deep learning-based segmentation models, comparing traditionalarchitectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,and foundation models like MedSAM, and MedSAM~2. Evaluating performance acrosstwo lung tumor segmentation datasets, we assess segmentation accuracy andcomputational efficiency under various learning paradigms, including few-shotlearning and fine-tuning. The results reveal that while traditional modelsstruggle with tumor delineation, foundation models, particularly MedSAM~2,outperform them in both accuracy and computational efficiency. These findingsunderscore the potential of foundation models for lung tumor segmentation,highlighting their applicability in improving clinical workflows and patientoutcomes.</description>
      <author>example@mail.com (Elena Mulero Ayllón, Massimiliano Mantegna, Linlin Shen, Paolo Soda, Valerio Guarrasi, Matteo Tortora)</author>
      <guid isPermaLink="false">2505.01239v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Identifying Root Cause of bugs by Capturing Changed Code Lines with Relational Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.00990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种即时缺陷预测模型，旨在帮助开发团队提高软件质量和效率，通过实时评估开发者提交的代码变更是否可能引入缺陷，以便在提交阶段及时识别潜在问题。&lt;h4&gt;背景&lt;/h4&gt;由于所有删除和添加的代码行可能与引入的缺陷的根本原因相关，当前工作中存在两个主要挑战：1）缺乏有效集成异构图信息；2）缺乏更改代码行之间的语义关系。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了RC-Detection方法，该方法利用关系图卷积网络来捕捉更改代码行之间的语义关系，并用于检测更改代码行中的根本原因删除行，从而识别修复提交中引入的缺陷的根本原因。&lt;h4&gt;方法&lt;/h4&gt;RC-Detection方法用于检测更改代码行中的根本原因删除行，并通过收集来自87个开源项目的数据（包括675个修复提交）来评估其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与最先进的根本原因检测方法相比，RC-Detection在Recall@1、Recall@2、Recall@3和MFR方面分别提高了4.107%、5.113%、4.289%和24.536%。&lt;h4&gt;结论&lt;/h4&gt;RC-Detection方法在实时缺陷预测中显示出显著的性能提升，有助于提高软件质量和开发效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：即时缺陷预测模型通过评估开发者提交的代码变更是否可能引入缺陷，帮助开发团队提高软件质量和效率，允许在提交阶段及时识别潜在问题。然而，由于所有删除和添加的代码行可能与引入的缺陷的根本原因相关，当前工作中存在两个主要挑战：1）缺乏有效集成异构图信息；2）缺乏更改代码行之间的语义关系。为了解决这些挑战，我们提出了一种名为RC-Detection的方法，该方法利用关系图卷积网络来捕捉更改代码行之间的语义关系。RC-Detection用于检测更改代码行中的根本原因删除行，从而识别修复提交中引入的缺陷的根本原因。为了评估RC-Detection的有效性，我们使用了包含高质量修复和引入提交的三个数据集。我们通过收集来自87个开源项目的数据（包括675个修复提交）进行了广泛的实验，以评估我们模型的表现。实验结果表明，与最先进的根本原因检测方法相比，RC-Detection在Recall@1、Recall@2、Recall@3和MFR方面分别提高了4.107%、5.113%、4.289%和24.536%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Just-In-Time defect prediction model helps development teams improvesoftware quality and efficiency by assessing whether code changes submitted bydevelopers are likely to introduce defects in real-time, allowing timelyidentification of potential issues during the commit stage. However, two mainchallenges exist in current work due to the reality that all deleted and addedlines in bug-fixing commits may be related to the root cause of the introducedbug: 1) lack of effective integration of heterogeneous graph information, and2) lack of semantic relationships between changed code lines. To address thesechallenges, we propose a method called RC-Detection, which utilizes relationalgraph convolutional network to capture the semantic relationships betweenchanged code lines. RC-Detection is used to detect root-cause deletion lines inchanged code lines, thereby identifying the root cause of introduced bugs inbug-fixing commits. To evaluate the effectiveness of RC-Detection, we usedthree datasets that contain high-quality bug-fixing and bug-introducingcommits. Extensive experiments were conducted to evaluate the performance ofour model by collecting data from 87 open-source projects, including 675bug-fix commits. The experimental results show that, compared to the mostadvanced root cause detection methods, RC-Detection improved Recall@1,Recall@2, Recall@3, and MFR by at 4.107%, 5.113%, 4.289%, and 24.536%,respectively.</description>
      <author>example@mail.com (Jiaqi Zhang, Shikai Guo, Hui Li, Chenchen Li, Yu Chai, Rong Chen)</author>
      <guid isPermaLink="false">2505.00990v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>TSTMotion: Training-free Scene-awarenText-to-motion Generation</title>
      <link>http://arxiv.org/abs/2505.01182v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICME2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TSTMotion的无需训练的场感知文本到动作生成框架，用于在空白背景中生成人类动作序列，并有效利用预训练的运动生成器。&lt;h4&gt;背景&lt;/h4&gt;当前文本到动作生成研究主要集中于空白背景中的人类动作序列生成，但现实中的动作通常发生在多样化的3D场景中。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需使用大量真实3D场景动作数据集的场感知文本到动作生成方法，以降低成本。&lt;h4&gt;方法&lt;/h4&gt;采用基础模型进行推理、预测和验证场景感知的运动引导，并将该引导融入空白背景动作生成器，通过两种修改实现场感知文本驱动的动作序列。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法有效且具有泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TSTMotion框架在无需训练的情况下实现了高效的场感知文本到动作生成。&lt;h4&gt;翻译&lt;/h4&gt;Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a Training-free Scene-aware Text-to-Motion framework, dubbed as TSMotion, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in ProjectPage.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-motion generation has recently garnered significant researchinterest, primarily focusing on generating human motion sequences in blankbackgrounds. However, human motions commonly occur within diverse 3D scenes,which has prompted exploration into scene-aware text-to-motion generationmethods. Yet, existing scene-aware methods often rely on large-scaleground-truth motion sequences in diverse 3D scenes, which poses practicalchallenges due to the expensive cost. To mitigate this challenge, we are thefirst to propose a \textbf{T}raining-free \textbf{S}cene-aware\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, thatefficiently empowers pre-trained blank-background motion generators with thescene-aware capability. Specifically, conditioned on the given 3D scene andtext description, we adopt foundation models together to reason, predict andvalidate a scene-aware motion guidance. Then, the motion guidance isincorporated into the blank-background motion generators with twomodifications, resulting in scene-aware text-driven motion sequences. Extensiveexperiments demonstrate the efficacy and generalizability of our proposedframework. We release our code in \href{https://tstmotion.github.io/}{ProjectPage}.</description>
      <author>example@mail.com (Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu)</author>
      <guid isPermaLink="false">2505.01182v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Toward Data-centric Directed Graph Learning: An Entropy-driven Approach</title>
      <link>http://arxiv.org/abs/2505.00983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EDEN的熵驱动的有向图知识蒸馏方法，旨在提高有向图神经网络（DiGNNs）的性能。&lt;h4&gt;背景&lt;/h4&gt;有向图（digraph）在复杂拓扑系统建模中具有优越的表示能力，但现有的DiGNNs未能充分利用有向图中的数据知识，导致模型预测性能不佳。&lt;h4&gt;目的&lt;/h4&gt;通过探索有向边（拓扑）和节点特征（特征和标签）之间的潜在相关性，增强模型中心的神经网络编码能力。&lt;h4&gt;方法&lt;/h4&gt;EDEN采用层次编码理论，首先利用拓扑视角的定向结构度量构建粗粒度层次知识树（HKT），然后量化节点特征的互信息以优化知识流。&lt;h4&gt;主要发现&lt;/h4&gt;EDEN在14个（有向）图数据集和4个下游任务上进行了广泛评估，结果表明EDEN达到了最先进的性能，并显著提高了常见的DiGNNs的性能。&lt;h4&gt;结论&lt;/h4&gt;EDEN作为一种数据中心的图学习范式或模型无关的知识蒸馏模块，能够有效地提升有向图神经网络的学习效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The directed graph (digraph), as a generalization of undirected graphs,exhibits superior representation capability in modeling complex topologysystems and has garnered considerable attention in recent years. Despite thenotable efforts made by existing DiGraph Neural Networks (DiGNNs) to leveragedirected edges, they still fail to comprehensively delve into the abundant dataknowledge concealed in the digraphs. This data-level limitation results inmodel-level sub-optimal predictive performance and underscores the necessity offurther exploring the potential correlations between the directed edges(topology) and node profiles (feature and labels) from a data-centricperspective, thereby empowering model-centric neural networks with strongerencoding capabilities.  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraphknowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as adata-centric digraph learning paradigm or a model-agnostic hot-and-plugdata-centric Knowledge Distillation (KD) module. The core idea is to achievedata-centric ML, guided by our proposed hierarchical encoding theory forstructured data. Specifically, EDEN first utilizes directed structuralmeasurements from a topology perspective to construct a coarse-grainedHierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutualinformation of node profiles to refine knowledge flow in the HKT, enablingdata-centric KD supervision within model training. As a general framework, EDENcan also naturally extend to undirected scenarios and demonstrate satisfactoryperformance. In our experiments, EDEN has been widely evaluated on 14 (di)graphdatasets (homophily and heterophily) and across 4 downstream tasks. The resultsdemonstrate that EDEN attains SOTA performance and exhibits strong improvementfor prevalent (Di)GNNs.</description>
      <author>example@mail.com (Xunkai Li, Zhengyu Wu, Kaichi Yu, Hongchao Qin, Guang Zeng, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2505.00983v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study</title>
      <link>http://arxiv.org/abs/2505.01109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the Journal of Medical Imaging (SPIE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MIL（多实例学习）是全切片图像分类的最佳解决方案，本文通过实验验证了基于实例的MIL方法在特定条件下可以媲美复杂的基于嵌入的MIL方法。&lt;h4&gt;背景&lt;/h4&gt;MIL通过将切片划分为多个补丁，每个补丁作为一个包含全局标签的实例的包进行处理。MIL主要有两种方法：基于实例和基于嵌入。尽管基于实例的方法更易于解释，但基于嵌入的方法由于对特征提取器的鲁棒性而更受欢迎。&lt;h4&gt;目的&lt;/h4&gt;研究基于实例的MIL方法是否可以通过使用自监督学习（SSL）提高性能。&lt;h4&gt;方法&lt;/h4&gt;作者在4个数据集上进行了710次实验，比较了10种MIL策略，6种自监督方法与4个骨干网络，4个基础模型以及各种病理适应性技术。此外，作者引入了4种在病理学领域从未使用过的基于实例的MIL方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过这些广泛的实验，作者发现使用良好的SSL特征提取器，简单的基于实例的MIL方法，参数非常少，可以获得与复杂的、最先进的（SOTA）基于嵌入的MIL方法相似甚至更好的性能，并在BRACS和Camelyon16数据集上设定了新的SOTA结果。&lt;h4&gt;结论&lt;/h4&gt;简单基于实例的MIL方法由于更易于解释和向临床医生解释，因此建议在WSI中投入更多努力进行良好的SSL方法适配，而不是复杂的基于嵌入的MIL方法。&lt;h4&gt;翻译&lt;/h4&gt;Multiple Instance Learning (MIL) has emerged as the best solution for WholeSlide Image (WSI) classification. It consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. MIL includes two main approaches: instance-based and embedding-based. In the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. In the latter, bag classification is performed after aggregating patch embeddings. Even if instance-based methods are naturally more interpretable, embedding-based MILs have usually been preferred in the past due to their robustness to poor feature extractors. However, recently, the quality of feature embeddings has drastically increased using self-supervised learning (SSL). Nevertheless, many authors continue to endorse the superiority of embedding-based MIL. To investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL methods never used before in the pathology domain. Through these extensive experiments, we show that with a good SSL feature extractor, simple instance-based MILs, with very few parameters, obtain similar or better performance than complex, state-of-the-art (SOTA) embedding-based MIL methods, setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple instance-based MIL methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted SSL methods for WSI rather than into complex embedding-based MIL methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiple Instance Learning (MIL) has emerged as the best solution for WholeSlide Image (WSI) classification. It consists of dividing each slide intopatches, which are treated as a bag of instances labeled with a global label.MIL includes two main approaches: instance-based and embedding-based. In theformer, each patch is classified independently, and then the patch scores areaggregated to predict the bag label. In the latter, bag classification isperformed after aggregating patch embeddings. Even if instance-based methodsare naturally more interpretable, embedding-based MILs have usually beenpreferred in the past due to their robustness to poor feature extractors.However, recently, the quality of feature embeddings has drastically increasedusing self-supervised learning (SSL). Nevertheless, many authors continue toendorse the superiority of embedding-based MIL. To investigate this further, weconduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6self-supervised methods with 4 backbones, 4 foundation models, and variouspathology-adapted techniques. Furthermore, we introduce 4 instance-based MILmethods never used before in the pathology domain. Through these extensiveexperiments, we show that with a good SSL feature extractor, simpleinstance-based MILs, with very few parameters, obtain similar or betterperformance than complex, state-of-the-art (SOTA) embedding-based MIL methods,setting new SOTA results on the BRACS and Camelyon16 datasets. Since simpleinstance-based MIL methods are naturally more interpretable and explainable toclinicians, our results suggest that more effort should be put intowell-adapted SSL methods for WSI rather than into complex embedding-based MILmethods.</description>
      <author>example@mail.com (Ali Mammadov, Loic Le Folgoc, Julien Adam, Anne Buronfosse, Gilles Hayem, Guillaume Hocquet, Pietro Gori)</author>
      <guid isPermaLink="false">2505.01109v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction</title>
      <link>http://arxiv.org/abs/2505.00625v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SA-GAT-SR的新计算范式，它结合了图神经网络和符号回归的优势，以提高材料科学中的预测精度和可解释性。&lt;h4&gt;背景&lt;/h4&gt;深度学习，特别是图神经网络（GNNs），在材料科学中显示出巨大的实用价值，但复杂的模型往往缺乏物理可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合GNNs和符号回归的新框架，以实现材料属性的高通量预测，同时保持物理可解释性。&lt;h4&gt;方法&lt;/h4&gt;开发了一种自适应性图注意力网络（SA-GAT）和符号回归（SR）的集成框架，其中SA-GAT自动识别和调整注意力权重，SR模块将特征转化为简明的分析表达式。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在180维特征空间中筛选关键特征，同时保持O(n)的计算规模，实现了比传统SR方法23倍的速度提升，并揭示了量子力学意义上的关系。&lt;h4&gt;结论&lt;/h4&gt;SA-GAT-SR框架为计算材料科学提供了一种新方法，在预测精度和物理可解释性之间架起桥梁，为材料行为提供了有价值的物理见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，机器学习的进步展示了深度学习方法，尤其是图神经网络（GNNs）在材料科学中的巨大效用。这些方法已成为预测材料属性的高通量工具，为传统的第一性原理计算提供了令人信服的增强和替代方案。尽管社区主要关注开发越来越复杂和通用的模型以提高预测精度，但这些方法往往缺乏物理可解释性和对材料行为的洞察。在这里，我们引入了一种新的计算范式，即与符号回归集成的自适应性图注意力网络（SA-GAT-SR），它协同结合了GNNs的预测能力和符号回归的解释能力。我们的框架采用了一种自适应性编码算法，该算法自动识别和调整注意力权重，以便从庞大的180维特征空间中筛选关键特征，同时保持O(n)的计算规模。集成的SR模块随后将这些特征提炼成紧凑的分析表达式，这些表达式明确揭示了量子力学意义上的关系，比严重依赖第一性原理计算得出的特征作为输入的传统SR实现快23倍。这项工作在计算材料科学中提出了一种新框架，在预测精度和物理可解释性之间架起了桥梁，为材料行为提供了有价值的物理见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/MustBeOne/SA-GAT-SR&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in machine learning have demonstrated an enormous utility ofdeep learning approaches, particularly Graph Neural Networks (GNNs) formaterials science. These methods have emerged as powerful tools forhigh-throughput prediction of material properties, offering a compellingenhancement and alternative to traditional first-principles calculations. Whilethe community has predominantly focused on developing increasingly complex anduniversal models to enhance predictive accuracy, such approaches often lackphysical interpretability and insights into materials behavior. Here, weintroduce a novel computational paradigm, Self-Adaptable Graph AttentionNetworks integrated with Symbolic Regression (SA-GAT-SR), that synergisticallycombines the predictive capability of GNNs with the interpretative power ofsymbolic regression. Our framework employs a self-adaptable encoding algorithmthat automatically identifies and adjust attention weights so as to screencritical features from an expansive 180-dimensional feature space whilemaintaining O(n) computational scaling. The integrated SR module subsequentlydistills these features into compact analytical expressions that explicitlyreveal quantum-mechanically meaningful relationships, achieving 23 timesacceleration compared to conventional SR implementations that heavily rely onfirst principle calculations-derived features as input. This work suggests anew framework in computational materials science, bridging the gap betweenpredictive accuracy and physical interpretability, offering valuable physicalinsights into material behavior.</description>
      <author>example@mail.com (Liu Junchi, Tang Ying, Tretiak Sergei, Duan Wenhui, Zhou Liujiang)</author>
      <guid isPermaLink="false">2505.00625v2</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References</title>
      <link>http://arxiv.org/abs/2505.01325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 6 figures, submitted to Springer Nature Computer Science&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了TRAVELER，一个用于评估模型处理时间参照能力的合成基准数据集，并探讨了不同类型时间参照对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;在自然语言理解中，理解和解决时间参照是至关重要的，因为我们在日常交流中经常提到过去或未来。&lt;h4&gt;目的&lt;/h4&gt;为了弥补现有基准在系统处理时间参照能力上的系统性评估不足，本文提出了TRAVELER。&lt;h4&gt;方法&lt;/h4&gt;TRAVELER是一个基于问答范式的合成数据集，包含涉及时间参照的问题及其正确答案。它评估模型处理明确、隐含相对说话时间和模糊时间参照的能力。对于模糊时间参照，通过在Prolific上进行的问卷调查确定了真实答案。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，尽管基准LLMs在处理少量事件和明确时间参照的问题时表现良好，但随着事件集长度的增加和时间参照的减少，性能明显下降。模糊问题类别在所有模型中表现最差。&lt;h4&gt;结论&lt;/h4&gt;TRAVELER基准可以用于评估不同类型时间参照对模型性能的影响，并提供了对模型处理复杂时间参照能力的深入了解。&lt;h4&gt;翻译&lt;/h4&gt;Understanding and resolving temporal references is essential in NaturalLanguage Understanding as we often refer to the past or future in dailycommunication. Although existing benchmarks address a system's ability toreason about and resolve temporal references, systematic evaluation of specifictemporal references remains limited. Towards closing this gap, we introduceTRAVELER, a novel synthetic benchmark dataset that follows a Question Answeringparadigm and consists of questions involving temporal references with thecorresponding correct answers. TRAVELER assesses models' abilities to resolveexplicit, implicit relative to speech time, and vague temporal references.Beyond investigating the performance of state-of-the-art LLMs depending on thetype of temporal reference, our benchmark also allows evaluation of performancein relation to the length of the set of events. For the category of vaguetemporal references, ground-truth answers were established via human surveys onProlific, following a procedure similar to the one from Kenneweg et al. Todemonstrate the benchmark's applicability, we evaluate four state-of-the-artLLMs using a question-answering task encompassing 3,300 questions. Our findingsshow that while the benchmarked LLMs can answer questions over event sets witha handful of events and explicit temporal references successfully, performanceclearly deteriorates with larger event set length and when temporal referencesget less explicit. Notably, the vague question category exhibits the lowestperformance across all models. The benchmark is publicly available at:https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and resolving temporal references is essential in NaturalLanguage Understanding as we often refer to the past or future in dailycommunication. Although existing benchmarks address a system's ability toreason about and resolve temporal references, systematic evaluation of specifictemporal references remains limited. Towards closing this gap, we introduceTRAVELER, a novel synthetic benchmark dataset that follows a Question Answeringparadigm and consists of questions involving temporal references with thecorresponding correct answers. TRAVELER assesses models' abilities to resolveexplicit, implicit relative to speech time, and vague temporal references.Beyond investigating the performance of state-of-the-art LLMs depending on thetype of temporal reference, our benchmark also allows evaluation of performancein relation to the length of the set of events. For the category of vaguetemporal references, ground-truth answers were established via human surveys onProlific, following a procedure similar to the one from Kenneweg et al. Todemonstrate the benchmark's applicability, we evaluate four state-of-the-artLLMs using a question-answering task encompassing 3,300 questions. Our findingsshow that while the benchmarked LLMs can answer questions over event sets witha handful of events and explicit temporal references successfully, performanceclearly deteriorates with larger event set length and when temporal referencesget less explicit. Notably, the vague question category exhibits the lowestperformance across all models.  The benchmark is publicly available at:https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER</description>
      <author>example@mail.com (Svenja Kenneweg, Jörg Deigmöller, Philipp Cimiano, Julian Eggert)</author>
      <guid isPermaLink="false">2505.01325v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>High Dynamic Range Novel View Synthesis with Single Exposure</title>
      <link>http://arxiv.org/abs/2505.01212v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  It has been accepted by ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的HDR-NVS方法，旨在从低动态范围（LDR）图像中建立3D场景的HDR模型。&lt;h4&gt;背景&lt;/h4&gt;传统的HDR-NVS方法使用多曝光LDR图像来捕捉场景中的广泛亮度级别，但这种方法存在易受运动伪影影响、捕获和存储成本高等问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，本文首次提出单曝光HDR-NVS问题，并引入了一种名为Mono-HDR-3D的新方法，该方法通过两个专门模块实现，一个用于将LDR颜色转换为HDR颜色，另一个用于将HDR图像转换为LDR格式，从而实现闭环的无监督学习。&lt;h4&gt;方法&lt;/h4&gt;Mono-HDR-3D是一个元算法，可以无缝集成到现有的NVS模型中。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Mono-HDR-3D在性能上显著优于之前的方法。&lt;h4&gt;结论&lt;/h4&gt;Mono-HDR-3D是一种有效的方法，可以用于从单曝光LDR图像中合成高质量的HDR图像，并将在未来发布源代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3Dscene HDR model from Low Dynamic Range (LDR) imagery. Typically,multiple-exposure LDR images are employed to capture a wider range ofbrightness levels in a scene, as a single LDR image cannot represent both thebrightest and darkest regions simultaneously. While effective, thismultiple-exposure HDR-NVS approach has significant limitations, includingsusceptibility to motion artifacts (e.g., ghosting and blurring), high captureand storage costs. To overcome these challenges, we introduce, for the firsttime, the single-exposure HDR-NVS problem, where only single exposure LDRimages are available during training. We further introduce a novel approach,Mono-HDR-3D, featuring two dedicated modules formulated by the LDR imageformation principles, one for converting LDR colors to HDR counterparts, andthe other for transforming HDR images to LDR format so that unsupervisedlearning is enabled in a closed loop. Designed as a meta-algorithm, ourapproach can be seamlessly integrated with existing NVS models. Extensiveexperiments show that Mono-HDR-3D significantly outperforms previous methods.Source code will be released.</description>
      <author>example@mail.com (Kaixuan Zhang, Hu Wang, Minxian Li, Mingwu Ren, Mao Ye, Xiatian Zhu)</author>
      <guid isPermaLink="false">2505.01212v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
      <link>http://arxiv.org/abs/2505.00254v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, AVAS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为AVAS的基于视频语言模型（VLM）的系统，用于开放式的视频分析，解决了现有系统在处理超长视频内容时的挑战。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的视频分析在多个领域变得至关重要，但现有系统通常局限于特定任务，限制了其在开放式分析场景中的适应性。&lt;h4&gt;目的&lt;/h4&gt;开发AVAS系统，以实现开放式的视频理解、推理和分析。&lt;h4&gt;方法&lt;/h4&gt;AVAS系统包含两个关键创新：(1) 构建事件知识图谱（EKGs）以高效索引长视频流；(2) 利用EKGs进行检索生成，以处理复杂和多样化的查询。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试LVBench和VideoMME-Long上，AVAS取得了最先进的性能，分别达到62.3%和64.1%的准确率，显著超过现有的VLM和视频检索增强生成（RAG）系统。在新的基准AVAS-100上，AVAS也取得了优异的性能，准确率达到75.8%。&lt;h4&gt;结论&lt;/h4&gt;AVAS系统在超长和开放式视频场景中的视频分析方面表现出色，为视频分析领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：AI驱动的视频分析在多个领域变得至关重要。然而，现有系统通常局限于特定任务，限制了其在开放式分析场景中的适应性。最近出现的视频语言模型（VLMs）作为颠覆性技术，为开放式的视频理解、推理和分析提供了巨大的潜力。尽管如此，它们有限的上下文窗口在处理超长视频内容时带来了挑战，这在现实世界应用中很常见。为了解决这个问题，我们引入了AVAS，这是一个由VLM驱动的系统，旨在进行开放式的、高级视频分析。AVAS包含两个关键创新：(1) 近实时构建事件知识图谱（EKGs）以高效索引长或连续视频流；(2) 利用EKGs进行检索生成，以处理复杂和多样化的查询。在公共基准测试LVBench和VideoMME-Long上的全面评估表明，AVAS实现了最先进的性能，分别达到62.3%和64.1%的准确率，显著优于现有的VLM和视频检索增强生成（RAG）系统。此外，为了评估超长和开放式视频场景中的视频分析，我们引入了一个新的基准，AVAS-100。该基准包含8个视频，每个视频的时长超过10小时，以及120对手动标注的、多样化的、复杂的问答对。在AVAS-100上，AVAS取得了顶级性能，准确率达到75.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-driven video analytics has become increasingly pivotal across diversedomains. However, existing systems are often constrained to specific,predefined tasks, limiting their adaptability in open-ended analyticalscenarios. The recent emergence of Video-Language Models (VLMs) astransformative technologies offers significant potential for enablingopen-ended video understanding, reasoning, and analytics. Nevertheless, theirlimited context windows present challenges when processing ultra-long videocontent, which is prevalent in real-world applications. To address this, weintroduce AVAS, a VLM-powered system designed for open-ended, advanced videoanalytics. AVAS incorporates two key innovations: (1) the near real-timeconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long orcontinuous video streams, and (2) an agentic retrieval-generation mechanismthat leverages EKGs to handle complex and diverse queries. Comprehensiveevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate thatAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,respectively, significantly surpassing existing VLM and videoRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate videoanalytics in ultra-long and open-world video scenarios, we introduce a newbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hoursin duration, along with 120 manually annotated, diverse, and complexquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with anaccuracy of 75.8%.</description>
      <author>example@mail.com (Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu)</author>
      <guid isPermaLink="false">2505.00254v2</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Federated Adapter on Foundation Models: An Out-Of-Distribution Approach</title>
      <link>http://arxiv.org/abs/2505.01075v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedOA，一种用于联邦学习框架中隐私保护模型微调的方法，以解决联邦基础模型（FedFM）中的分布外（OOD）泛化问题。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型在联邦学习中的广泛应用，FedFM作为一种隐私保护的方法应运而生。然而，FedFM面临的一个关键挑战是如何处理分布外泛化问题，即未见过的新任务或客户端可能表现出分布偏移，导致性能不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文旨在提出一种新的方法来增强FedFM中的OOD泛化能力。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一种基于适配器的参数高效微调方法，并引入了基于特征距离正则化的个性化适配器，以对齐分布并保证每个客户端的OOD泛化。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了传统的FedFM全局聚合模型本身具有OOD泛化能力，并且通过全局模型提供的信息进行正则化，可以增强个性化模型的OOD泛化，且在一般非凸设置下已证明收敛。&lt;h4&gt;结论&lt;/h4&gt;实证结果表明，所提出的方法在各种NLP任务上的基准数据集上有效。&lt;h4&gt;翻译&lt;/h4&gt;As foundation models gain prominence, Federated Foundation Models (FedFM) have emerged as a privacy-preserving approach to collaboratively fine-tune models in federated learning (FL) frameworks using distributed datasets across clients. A key challenge for FedFM, given the versatile nature of foundation models, is addressing out-of-distribution (OOD) generalization, where unseen tasks or clients may exhibit distribution shifts leading to suboptimal performance. Although numerous studies have explored OOD generalization in conventional FL, these methods are inadequate for FedFM due to the challenges posed by large parameter scales and increased data heterogeneity. To address these, we propose FedOA, which employs adapter-based parameter-efficient fine-tuning methods for efficacy and introduces personalized adapters with feature distance-based regularization to align distributions and guarantee OOD generalization for each client. Theoretically, we demonstrate that the conventional aggregated global model in FedFM inherently retains OOD generalization capabilities, and our proposed method enhances the personalized model's OOD generalization through regularization informed by the global model, with proven convergence under general non-convex settings. Empirically, the effectiveness of the proposed method is validated on benchmark datasets across various NLP tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As foundation models gain prominence, Federated Foundation Models (FedFM)have emerged as a privacy-preserving approach to collaboratively fine-tunemodels in federated learning (FL) frameworks using distributed datasets acrossclients. A key challenge for FedFM, given the versatile nature of foundationmodels, is addressing out-of-distribution (OOD) generalization, where unseentasks or clients may exhibit distribution shifts leading to suboptimalperformance. Although numerous studies have explored OOD generalization inconventional FL, these methods are inadequate for FedFM due to the challengesposed by large parameter scales and increased data heterogeneity. To addressthese, we propose FedOA, which employs adapter-based parameter-efficientfine-tuning methods for efficacy and introduces personalized adapters withfeature distance-based regularization to align distributions and guarantee OODgeneralization for each client. Theoretically, we demonstrate that theconventional aggregated global model in FedFM inherently retains OODgeneralization capabilities, and our proposed method enhances the personalizedmodel's OOD generalization through regularization informed by the global model,with proven convergence under general non-convex settings. Empirically, theeffectiveness of the proposed method is validated on benchmark datasets acrossvarious NLP tasks.</description>
      <author>example@mail.com (Yiyuan Yang, Guodong Long, Tianyi Zhou, Qinghua Lu, Shanshan Ye, Jing Jiang)</author>
      <guid isPermaLink="false">2505.01075v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2505.01064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint; earlier version accepted at NeurIPS 2024 Workshop on  Adaptive Foundation Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NeaR的新方法，用于解决Vocabulary-Free Fine-grained Visual Recognition（VF-FGVR）问题，即在没有预先标签信息的情况下，从不受约束的输出空间中预测标签。&lt;h4&gt;背景&lt;/h4&gt;Fine-grained Visual Recognition（FGVR）在医学成像等领域由于隐私和标注成本问题，缺乏专家标注的数据集。VF-FGVR模型需要从不受约束的输出空间预测标签，而现有的Multimodal Large Language Models（MLLMs）查询成本高，推理时间长。&lt;h4&gt;目的&lt;/h4&gt;提出NeaR方法，以解决VF-FGVR中的成本和推理时间问题。&lt;h4&gt;方法&lt;/h4&gt;NeaR通过使用MLLM生成的标签微调下游CLIP模型，并从少量未标注训练集中构建弱监督数据集。&lt;h4&gt;主要发现&lt;/h4&gt;NeaR能够处理MLLM生成标签中的噪声、随机性和开放性，并建立了高效VF-FGVR的新基准。&lt;h4&gt;结论&lt;/h4&gt;NeaR是一种有效的VF-FGVR方法，能够降低成本和推理时间。&lt;h4&gt;翻译&lt;/h4&gt;Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce NeaRest-Neighbor Label Refinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-grained Visual Recognition (FGVR) involves distinguishing betweenvisually similar categories, which is inherently challenging due to subtleinter-class differences and the need for large, expert-annotated datasets. Indomains like medical imaging, such curated datasets are unavailable due toissues like privacy concerns and high annotation costs. In such scenarioslacking labeled data, an FGVR model cannot rely on a predefined set of traininglabels, and hence has an unconstrained output space for predictions. We referto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predictlabels from an unconstrained output space without prior label information.While recent Multimodal Large Language Models (MLLMs) show potential forVF-FGVR, querying these models for each test input is impractical because ofhigh costs and prohibitive inference times. To address these limitations, weintroduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novelapproach that fine-tunes a downstream CLIP model using labels generated by anMLLM. Our approach constructs a weakly supervised dataset from a small,unlabeled training set, leveraging MLLMs for label generation. NeaR is designedto handle the noise, stochasticity, and open-endedness inherent in labelsgenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.</description>
      <author>example@mail.com (Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian)</author>
      <guid isPermaLink="false">2505.01064v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Fast and Low-Cost Genomic Foundation Models via Outlier Removal</title>
      <link>http://arxiv.org/abs/2505.00598v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  International Conference on Machine Learning (ICML) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GERM是一种基因组基础模型，具有强大的压缩性能和快速适应能力，旨在解决基因组建模中计算资源稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;基因组建模需要大量的计算资源，而现有的模型如DNABERT-2在适应性和鲁棒性方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出GERM模型，以改善基因组建模的效率和质量。&lt;h4&gt;方法&lt;/h4&gt;GERM通过消除异常值来增强低秩适应和后训练量化，使用受联想记忆模型启发的无异常值机制替换了传统的注意力层。同时，提出了GERM-T策略，利用小步持续学习在无异常值框架内，避免从头开始重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;GERM在微调性能上比基线模型提高了37.98%，在量化上提高了64.34%，同时减少了平均峰度和最大无穷范数的百分比。&lt;h4&gt;结论&lt;/h4&gt;GERM在资源受限的环境中为基因组建模提供了一个实用的解决方案，并在性能上优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;为了解决基因组建模中计算资源稀缺的挑战，我们引入了GERM，这是一种具有强大压缩性能和快速适应能力的基因组基础模型。GERM通过消除阻碍低秩适应和后训练量化的异常值，改进了如DNABERT-2等模型，增强了效率和鲁棒性。我们用受联想记忆模型启发的无异常值机制替换了常规的注意力层。通过在预训练和微调过程中移除异常值，这种方法加速了适应过程，降低了计算成本，并在可接受的损失范围内增强了量化的鲁棒性。此外，我们还提出了GERM-T策略，它利用无异常值框架内的小步持续学习，利用原始检查点以避免从头开始重新训练。实证表明，GERM在微调性能上比基线模型提高了37.98%，在量化上提高了64.34%，同时平均峰度降低了92.14%，最大无穷范数降低了82.77%。与领先方法相比，GERM在性能上始终优于其他方法，为资源受限环境中的基因组建模提供了一个实用的解决方案。代码可在https://github.com/MAGICS-LAB/GERM上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/MAGICS-LAB/GERM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To address the challenge of scarce computational resources in genomicmodeling, we introduce GERM, a genomic foundation model with strong compressionperformance and fast adaptability. GERM improves upon models like DNABERT-2 byeliminating outliers that hinder low-rank adaptation and post-trainingquantization, enhancing both efficiency and robustness. We replace the vanillaattention layer with an outlier-free mechanism inspired by associative memorymodels. By removing outliers during both pre-training and fine-tuning, thisapproach accelerates adaptation, reduces computational costs, and enhancesquantization robustness within acceptable loss margins. Additionally, wepropose GERM-T, a strategy that employs small-step continual learning withinthe outlier-free framework, leveraging original checkpoints to avoid retrainingfrom scratch. Empirically, GERM improves fine-tuning performance by 37.98% andquantization by 64.34% over the baseline model. It also reduces averagekurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leadingmethods, GERM consistently delivers superior performance, offering a practicalsolution for genomic modeling in resource-constrained settings. Code isavailable at https://github.com/MAGICS-LAB/GERM.</description>
      <author>example@mail.com (Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry Yao-Chieh Hu, Han Liu)</author>
      <guid isPermaLink="false">2505.00598v2</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery</title>
      <link>http://arxiv.org/abs/2504.19684v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合CycleGAN域适应和对比学习的框架，以增强天气分类，特别是在低光夜间条件下的分类性能。&lt;h4&gt;背景&lt;/h4&gt;恶劣天气对交通安全构成挑战，需要从交通摄像头图像中进行稳健的实时天气检测。&lt;h4&gt;目的&lt;/h4&gt;提高夜间条件下天气分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用轻量级的SigLIP-2模型，并结合CycleGAN将夜间图像转换为类似白天的表示，同时保留天气信息。使用对比学习方法提高分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;在Iowa Department of Transportation数据集上，CycleGAN辅助的EVA-02模型在三类天气条件（无降水、雨、雪）下的整体准确率达到96.55%，白天/夜间整体准确率达到96.55%，但存在显著的日夜差距。改进后的模型Vision-SigLIP-2 + Text-SigLIP-2 + CycleGAN + Contrastive在夜间场景中表现优异，达到最高夜间准确率85.90%，整体准确率达到93.35%，同时大幅减少了训练和推理时间。&lt;h4&gt;结论&lt;/h4&gt;该框架提供了一种可扩展、高效的解决方案，可以缩小日夜性能差距，利用现有摄像头基础设施进行全天候分类。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种结合CycleGAN域适应和高效对比学习的框架，以增强天气分类，特别是在低光夜间条件下的分类性能。背景是恶劣天气对交通安全构成挑战，需要从交通摄像头图像中进行稳健的实时天气检测。目的是提高夜间条件下天气分类的准确性。方法包括采用轻量级的SigLIP-2模型，并结合CycleGAN将夜间图像转换为类似白天的表示，同时保留天气信息。使用对比学习方法提高分类性能。主要发现是在Iowa Department of Transportation数据集上，CycleGAN辅助的EVA-02模型在三类天气条件（无降水、雨、雪）下的整体准确率达到96.55%，白天/夜间整体准确率达到96.55%，但存在显著的日夜差距。改进后的模型Vision-SigLIP-2 + Text-SigLIP-2 + CycleGAN + Contrastive在夜间场景中表现优异，达到最高夜间准确率85.90%，整体准确率达到93.35%，同时大幅减少了训练和推理时间。结论是该框架提供了一种可扩展、高效的解决方案，可以缩小日夜性能差距，利用现有摄像头基础设施进行全天候分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adverse weather conditions challenge safe transportation, necessitatingrobust real-time weather detection from traffic camera imagery. We propose anovel framework combining CycleGAN-based domain adaptation with efficientcontrastive learning to enhance weather classification, particularly inlow-light nighttime conditions. Our approach leverages the lightweight SigLIP-2model, which employs pairwise sigmoid loss to reduce computational demands,integrated with CycleGAN to transform nighttime images into day-likerepresentations while preserving weather cues. Evaluated on an Iowa Departmentof Transportation dataset, the baseline EVA-02 model with CLIP achieves aper-class overall accuracy of 96.55\% across three weather conditions (NoPrecipitation, Rain, Snow) and a day/night overall accuracy of 96.55\%, butshows a significant day-night gap (97.21\% day vs.\ 63.40\% night). WithCycleGAN, EVA-02 improves to 97.01\% per-class accuracy and 96.85\% day/nightaccuracy, boosting nighttime performance to 82.45\%. Our Vision-SigLIP-2 +Text-SigLIP-2 + CycleGAN + Contrastive configuration excels in nighttimescenarios, achieving the highest nighttime accuracy of 85.90\%, with 94.00\%per-class accuracy and 93.35\% day/night accuracy. This model reduces trainingtime by 89\% (from 6 hours to 40 minutes) and inference time by 80\% (from 15seconds to 3 seconds) compared to EVA-02. By narrowing the day-nightperformance gap from 33.81 to 8.90 percentage points, our framework provides ascalable, efficient solution for all-weather classification using existingcamera infrastructure.</description>
      <author>example@mail.com (Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, Anuj Sharma)</author>
      <guid isPermaLink="false">2504.19684v2</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations</title>
      <link>http://arxiv.org/abs/2505.00745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Sensys 2025 final version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MOCHA的新型框架，用于优化移动视频分析系统中模型的适应性响应。&lt;h4&gt;背景&lt;/h4&gt;移动视频分析系统在部署过程中面临各种环境变化，对模型的适应性响应提出了更高的要求。&lt;h4&gt;目的&lt;/h4&gt;提高模型在环境变化时的适应性响应速度。&lt;h4&gt;方法&lt;/h4&gt;MOCHA通过移动设备和云资源之间的分层协作实现这一点，具体方法包括：在请求云模型检索和端到端重新训练之前，在设备上执行模型重用和快速微调；通过云基础模型分析的领域语义，将历史专家模型组织成结构化的分类法，以加速检索；维护设备上的专家模型缓存，以便频繁场景的本地模型重用，并主动从云模型数据库中预取模型权重。&lt;h4&gt;主要发现&lt;/h4&gt;在三个深度神经网络任务上进行的广泛评估表明，MOCHA在适应过程中提高了模型准确性，最多提高了6.8%，同时将响应延迟和重新训练时间分别减少了35.5倍和3.0倍。&lt;h4&gt;结论&lt;/h4&gt;MOCHA框架有效地提高了移动视频分析系统中模型的适应性响应速度，同时提高了模型准确性并减少了响应延迟和重新训练时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile video analysis systems often encounter various deploying environments,where environment shifts present greater demands for responsiveness inadaptations of deployed "expert DNN models". Existing model adaptationframeworks primarily operate in a cloud-centric way, exhibiting degradedperformance during adaptation and delayed reactions to environment shifts.Instead, this paper proposes MOCHA, a novel framework optimizing theresponsiveness of continuous model adaptation through hierarchicalcollaborations between mobile and cloud resources. Specifically, MOCHA (1)reduces adaptation response delays by performing on-device model reuse and fastfine-tuning before requesting cloud model retrieval and end-to-end retraining;(2) accelerates history expert model retrieval by organizing them into astructured taxonomy utilizing domain semantics analyzed by a cloud foundationmodel as indices; (3) enables efficient local model reuse by maintainingonboard expert model caches for frequent scenes, which proactively prefetchmodel weights from the cloud model database. Extensive evaluations withreal-world videos on three DNN tasks show MOCHA improves the model accuracyduring adaptation by up to 6.8% while saving the response delay and retrainingtime by up to 35.5x and 3.0x respectively.</description>
      <author>example@mail.com (Maozhe Zhao, Shengzhong Liu, Fan Wu, Guihai Chen)</author>
      <guid isPermaLink="false">2505.00745v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection</title>
      <link>http://arxiv.org/abs/2505.00739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了MoSAM，一种结合运动引导提示和时空记忆选择机制的对象分割模型，用于解决Segment Anything Model 2（SAM2）在视频分割中存在的长期跟踪能力和记忆准确性问题。&lt;h4&gt;背景&lt;/h4&gt;SAM2在交互式对象分割方面表现出色，但其基于过去六帧的掩码记忆进行分割的方式存在两个主要挑战：无法处理视频中的对象消失和遮挡，以及缺乏运动信息导致的长距离跟踪限制。&lt;h4&gt;目的&lt;/h4&gt;提出MoSAM模型，旨在解决SAM2在视频分割中的这两个挑战，提高模型在跟踪和记忆准确性方面的性能。&lt;h4&gt;方法&lt;/h4&gt;MoSAM模型采用了两种关键策略：运动引导提示（MGP）和时空记忆选择（ST-MS）机制。MGP通过多种方式表示物体运动，并引导模型关注运动方向；ST-MS机制动态识别可能包含准确分割的帧，从而提高记忆的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MoSAM在视频对象分割和视频实例分割的多个基准测试中取得了优于其他竞争者的最新水平的结果。&lt;h4&gt;结论&lt;/h4&gt;MoSAM模型通过引入运动引导和时空记忆选择机制，有效提高了SAM2在视频分割任务中的性能，尤其是在长期跟踪和记忆准确性方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent Segment Anything Model 2 (SAM2) has demonstrated exceptionalcapabilities in interactive object segmentation for both images and videos.However, as a foundational model on interactive segmentation, SAM2 performssegmentation directly based on mask memory from the past six frames, leading totwo significant challenges. Firstly, during inference in videos, objects maydisappear since SAM2 relies solely on memory without accounting for objectmotion information, which limits its long-range object tracking capabilities.Secondly, its memory is constructed from fixed past frames, making itsusceptible to challenges associated with object disappearance or occlusion,due to potentially inaccurate segmentation results in memory. To address theseproblems, we present MoSAM, incorporating two key strategies to integrateobject motion cues into the model and establish more reliable feature memory.Firstly, we propose Motion-Guided Prompting (MGP), which represents the objectmotion in both sparse and dense manners, then injects them into SAM2 through aset of motion-guided prompts. MGP enables the model to adjust its focus towardsthe direction of motion, thereby enhancing the object tracking capabilities.Furthermore, acknowledging that past segmentation results may be inaccurate, wedevise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamicallyidentifies frames likely to contain accurate segmentation in both pixel- andframe-level. By eliminating potentially inaccurate mask predictions frommemory, we can leverage more reliable memory features to exploit similarregions for improving segmentation results. Extensive experiments on variousbenchmarks of video object segmentation and video instance segmentationdemonstrate that our MoSAM achieves state-of-the-art results compared to othercompetitors.</description>
      <author>example@mail.com (Qiushi Yang, Yuan Yao, Miaomiao Cui, Liefeng Bo)</author>
      <guid isPermaLink="false">2505.00739v1</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT</title>
      <link>http://arxiv.org/abs/2501.05817v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE VTC2025-Spring, Copyright IEEE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文评估了可重构智能表面（RIS）优化算法在基于射线追踪（RT）模拟的城市数字孪生环境中的性能，并通过实际实验强调了在接近真实环境中的算法验证的重要性。&lt;h4&gt;背景&lt;/h4&gt;研究背景为城市数字孪生环境中的RIS优化算法。&lt;h4&gt;目的&lt;/h4&gt;目的是评估RIS优化算法在不同部署条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;方法包括在Sionna的RT模拟中实现和基准测试基于信道估计的额外RIS优化算法，并生成RIS辅助通信系统的覆盖图。&lt;h4&gt;主要发现&lt;/h4&gt;主要发现是，在接近真实环境的模拟中验证算法的必要性，因为测量设置中的微小变化可能会显著影响性能。&lt;h4&gt;结论&lt;/h4&gt;结论是，RIS优化算法的性能在接近真实环境的模拟中得到了验证。&lt;h4&gt;翻译&lt;/h4&gt;This paper evaluates the performance of reconfigurable intelligent surface (RIS) optimization algorithms, which utilize channel estimation methods, in raytracing (RT) simulations within urban digital twin environments. Beyond Sionna's native capabilities, we implement and benchmark additional RIS optimization algorithms based on channel estimation, enabling an evaluation of RIS strategies under various deployment conditions. Coverage maps for RIS-assisted communication systems are generated through the integration of Sionna's RT simulations. Moreover, real-world experimentation underscores the necessity of validating algorithms in near-realistic simulation environments, as minor variations in measurement setups can significantly affect performance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-01-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper evaluates the performance of reconfigurable intelligent surface(RIS) optimization algorithms, which utilize channel estimation methods, in raytracing (RT) simulations within urban digital twin environments. BeyondSionna's native capabilities, we implement and benchmark additional RISoptimization algorithms based on channel estimation, enabling an evaluation ofRIS strategies under various deployment conditions. Coverage maps forRIS-assisted communication systems are generated through the integration ofSionna's RT simulations. Moreover, real-world experimentation underscores thenecessity of validating algorithms in near-realistic simulation environments,as minor variations in measurement setups can significantly affect performance.</description>
      <author>example@mail.com (Ahmet Esad Güneşer, Berkay Şekeroğlu, Sefa Kayraklık, Erhan Karakoca, İbrahim Hökelek, Sultan Aldirmaz-Colak, Ali Görçin)</author>
      <guid isPermaLink="false">2501.05817v2</guid>
      <pubDate>Mon, 05 May 2025 14:09:37 +0800</pubDate>
    </item>
    <item>
      <title>Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction</title>
      <link>http://arxiv.org/abs/2505.00237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE RA-L&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于在动态和不确定环境中安全高效控制移动机器人的集成方法。&lt;h4&gt;背景&lt;/h4&gt;该方法由两个关键步骤组成，一个是一步式多模态运动预测，用于预测动态障碍物的运动，另一个是模型预测控制，将这些预测纳入运动规划过程。&lt;h4&gt;目的&lt;/h4&gt;该方法旨在使移动机器人在动态环境中有效导航。&lt;h4&gt;方法&lt;/h4&gt;运动预测由一个基于能量的神经网络驱动，在单次操作中生成高分辨率、多步预测。预测结果被进一步用于创建几何形状，作为数学约束。预测的障碍物通过非监督方式根据邻近性分组，以改进性能和效率。整体无碰撞导航由针对主动动态障碍物避免的特定设计的模型预测控制处理。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在代表典型仓库设置的多种场景中对各种情况进行了性能评估。结果显示，该方法优于其他现有的动态障碍物避免方法。&lt;h4&gt;结论&lt;/h4&gt;该集成方法使移动机器人在动态环境中有效导航，并在实际应用中显示出优异的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes an integrated approach for the safe and efficient controlof mobile robots in dynamic and uncertain environments. The approach consistsof two key steps: one-shot multimodal motion prediction to anticipate motionsof dynamic obstacles and model predictive control to incorporate thesepredictions into the motion planning process. Motion prediction is driven by anenergy-based neural network that generates high-resolution, multi-steppredictions in a single operation. The prediction outcomes are further utilizedto create geometric shapes formulated as mathematical constraints. Instead oftreating each dynamic obstacle individually, predicted obstacles are grouped byproximity in an unsupervised way to improve performance and efficiency. Theoverall collision-free navigation is handled by model predictive control with aspecific design for proactive dynamic obstacle avoidance. The proposed approachallows mobile robots to navigate effectively in dynamic environments. Itsperformance is accessed across various scenarios that represent typicalwarehouse settings. The results demonstrate that the proposed approachoutperforms other existing dynamic obstacle avoidance methods.</description>
      <author>example@mail.com (Ze Zhang, Georg Hess, Junjie Hu, Emmanuel Dean, Lennart Svensson, Knut Åkesson)</author>
      <guid isPermaLink="false">2505.00237v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:44 +0800</pubDate>
    </item>
  <item>
      <title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
      <link>http://arxiv.org/abs/2505.00254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了基于AI的视频分析技术，特别是在开放性分析场景中的重要性以及存在的问题，并提出了一种名为AVA的系统，用于解决超长视频内容处理的挑战。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的视频分析在多个领域变得至关重要，但现有系统通常受限于特定任务，限制了其在开放性分析场景中的适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为AVA的系统，旨在实现开放性、高级视频分析，解决超长视频内容处理的挑战。&lt;h4&gt;方法&lt;/h4&gt;AVA系统包括两个关键创新：(1) 实时构建事件知识图谱（EKGs）以高效索引长视频流，(2) 利用EKGs的智能检索-生成机制处理复杂和多样化的查询。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试LVBench和VideoMME-Long上，AVA系统取得了最先进的性能，分别达到62.3%和64.1%的准确率，显著优于现有的VLM和视频检索增强生成（RAG）系统。此外，AVA系统在新的基准测试AVA-100上也表现出色，该基准测试包含超过10小时的视频和120个复杂的问题-答案对，准确率达到75.8%。&lt;h4&gt;结论&lt;/h4&gt;AVA系统为超长和开放世界视频场景的视频分析提供了有效的解决方案，并在相关基准测试中取得了优异的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-driven video analytics has become increasingly pivotal across diversedomains. However, existing systems are often constrained to specific,predefined tasks, limiting their adaptability in open-ended analyticalscenarios. The recent emergence of Video-Language Models (VLMs) astransformative technologies offers significant potential for enablingopen-ended video understanding, reasoning, and analytics. Nevertheless, theirlimited context windows present challenges when processing ultra-long videocontent, which is prevalent in real-world applications. To address this, weintroduce AVA, a VLM-powered system designed for open-ended, advanced videoanalytics. AVA incorporates two key innovations: (1) the near real-timeconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long orcontinuous video streams, and (2) an agentic retrieval-generation mechanismthat leverages EKGs to handle complex and diverse queries. Comprehensiveevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate thatAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,respectively, significantly surpassing existing VLM and videoRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate videoanalytics in ultra-long and open-world video scenarios, we introduce a newbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hoursin duration, along with 120 manually annotated, diverse, and complexquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with anaccuracy of 75.8%.</description>
      <author>example@mail.com (Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu)</author>
      <guid isPermaLink="false">2505.00254v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:44 +0800</pubDate>
    </item>
    <item>
      <title>V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2505.00156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为V3LMA的新方法，通过结合大型语言模型（LLMs）和大型视觉语言模型（LVLMs）来增强3D场景理解，以提高自动驾驶系统在复杂交通场景中的情境意识和决策能力。&lt;h4&gt;背景&lt;/h4&gt;LVLMs在理解与分析不同领域的视觉场景方面表现出强大的能力，但在自动驾驶领域，它们对3D环境的理解有限，限制了其在动态环境中实现全面安全理解的有效性。&lt;h4&gt;目的&lt;/h4&gt;提出V3LMA方法，旨在通过整合LLMs和LVLMs来提升3D场景理解能力，以实现更安全的自动驾驶。&lt;h4&gt;方法&lt;/h4&gt;V3LMA利用从对象检测和视频输入生成的文本描述，通过一个专门的预处理流程提取3D对象数据，从而在不需微调的情况下显著提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在LingoQA基准测试中取得了0.56的分数，证明了其在复杂交通场景中的情境意识和决策能力得到了提升。&lt;h4&gt;结论&lt;/h4&gt;通过探索不同的融合策略和标记组合，V3LMA有望进一步推进对交通场景的解释，从而最终实现更安全的自动驾驶系统。&lt;h4&gt;翻译&lt;/h4&gt;Large Vision Language Models (LVLMs) have shown strong capabilities in understanding and analyzing visual scenes across various domains. However, in the context of autonomous driving, their limited comprehension of 3D environments restricts their effectiveness in achieving a complete and safe understanding of dynamic surroundings. To address this, we introduce V3LMA, a novel approach that enhances 3D scene understanding by integrating Large Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions generated from object detections and video inputs, significantly boosting performance without requiring fine-tuning. Through a dedicated preprocessing pipeline that extracts 3D object data, our method improves situational awareness and decision-making in complex traffic scenarios, achieving a score of 0.56 on the LingoQA benchmark. We further explore different fusion strategies and token combinations with the goal of advancing the interpretation of traffic scenes, ultimately enabling safer autonomous driving systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision Language Models (LVLMs) have shown strong capabilities inunderstanding and analyzing visual scenes across various domains. However, inthe context of autonomous driving, their limited comprehension of 3Denvironments restricts their effectiveness in achieving a complete and safeunderstanding of dynamic surroundings. To address this, we introduce V3LMA, anovel approach that enhances 3D scene understanding by integrating LargeLanguage Models (LLMs) with LVLMs. V3LMA leverages textual descriptionsgenerated from object detections and video inputs, significantly boostingperformance without requiring fine-tuning. Through a dedicated preprocessingpipeline that extracts 3D object data, our method improves situationalawareness and decision-making in complex traffic scenarios, achieving a scoreof 0.56 on the LingoQA benchmark. We further explore different fusionstrategies and token combinations with the goal of advancing the interpretationof traffic scenes, ultimately enabling safer autonomous driving systems.</description>
      <author>example@mail.com (Jannik Lübberstedt, Esteban Rivera, Nico Uhlemann, Markus Lienkamp)</author>
      <guid isPermaLink="false">2505.00156v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:44 +0800</pubDate>
    </item>
    <item>
      <title>Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets</title>
      <link>http://arxiv.org/abs/2505.00584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究自动驾驶中的目标检测和跟踪，提出了一种针对相机雷达自动驾驶数据集的合成数据增强方法，以增强检测和跟踪管道的鲁棒性，并评估了一种轻量级噪声识别神经网络的效果。&lt;h4&gt;背景&lt;/h4&gt;过去几十年中，使用神经网络在各种数据集上进行目标检测取得了有希望的结果，但大多数方法关注性能指标，而较少关注提高检测和跟踪管道的鲁棒性，尤其是针对传感器故障。&lt;h4&gt;目的&lt;/h4&gt;通过创建一个逼真的合成数据增强管道，模拟传感器故障和数据退化，提高自动驾驶车辆中目标检测和跟踪的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;开发了一个针对相机雷达自动驾驶数据集的合成数据增强管道，并训练了一个轻量级噪声识别神经网络，在增强的数据集上进行训练和测试。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的轻量级噪声识别神经网络在增强的数据集上达到了54.4%的整体识别准确率，覆盖了11个类别，共包含10086张图像和2145个雷达点云。&lt;h4&gt;结论&lt;/h4&gt;该方法提高了目标检测和跟踪的鲁棒性，为自动驾驶系统中的传感器故障处理提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：检测和跟踪对象是任何自主导航方法的关键组成部分。在过去几十年中，使用神经网络在各种数据集上进行目标检测取得了有希望的结果。虽然许多方法关注性能指标，但很少有项目关注提高这些检测和跟踪管道的鲁棒性，特别是针对传感器故障。在本文中，我们试图通过为相机雷达自动驾驶数据集创建一个逼真的合成数据增强管道来解决这个问题。我们的目标是准确地模拟传感器故障和由于现实世界的干扰而导致的数据退化。我们还展示了在增强的数据集上训练和测试的基线轻量级噪声识别神经网络的结果，在11个类别中，10086张图像和2145个雷达点云上达到了54.4%的整体识别准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting and tracking objects is a crucial component of any autonomousnavigation method. For the past decades, object detection has yielded promisingresults using neural networks on various datasets. While many methods focus onperformance metrics, few projects focus on improving the robustness of thesedetection and tracking pipelines, notably to sensor failures. In this paper weattempt to address this issue by creating a realistic synthetic dataaugmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Ourgoal is to accurately simulate sensor failures and data deterioration due toreal-world interferences. We also present our results of a baseline lightweightNoise Recognition neural network trained and tested on our augmented dataset,reaching an overall recognition accuracy of 54.4\% on 11 categories across10086 images and 2145 radar point-clouds.</description>
      <author>example@mail.com (Mathis Morales, Golnaz Habibi)</author>
      <guid isPermaLink="false">2505.00584v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:44 +0800</pubDate>
    </item>
    <item>
      <title>HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2505.00507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HeAL的主动学习方法，用于3D目标检测中的样本选择，通过结合启发式特征和定位与分类来提高模型的训练效果。&lt;h4&gt;背景&lt;/h4&gt;主动学习在自动驾驶模型训练的样本选择方面已证明是相关的方法。3D目标检测在未控制场景下的样本选择具有挑战性，当前方法主要关注样本选择问题的理论方面，而忽略了从大量文献和应用中获得的实用见解。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合启发式特征的方法，用于3D目标检测的样本选择，以提高模型的训练效果。&lt;h4&gt;方法&lt;/h4&gt;引入HeAL方法，该方法集成启发式特征如物体距离和点数量来估计不确定性，增强所选样本对检测模型训练的有用性。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI数据集上的定量评估表明，HeAL在mAP（平均精度）方面与现有最佳方法具有竞争力，并且只需24%的样本就能达到与全监督基线相同的mAP。&lt;h4&gt;结论&lt;/h4&gt;HeAL方法能够有效地提高3D目标检测模型的性能，并且相较于传统方法在样本利用率上有了显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Active Learning has proved to be a relevant approach to perform sampleselection for training models for Autonomous Driving. Particularly, previousworks on active learning for 3D object detection have shown that selection ofsamples in uncontrolled scenarios is challenging. Furthermore, currentapproaches focus exclusively on the theoretical aspects of the sample selectionproblem but neglect the practical insights that can be obtained from theextensive literature and application of 3D detection models. In this paper, weintroduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)which integrates those heuristical features together with Localization andClassification to deliver the most contributing samples to the model'straining. In contrast to previous works, our approach integrates heuristicalfeatures such as object distance and point-quantity to estimate theuncertainty, which enhance the usefulness of selected samples to traindetection models. Our quantitative evaluation on KITTI shows that HeAL presentscompetitive mAP with respect to the State-of-the-Art, and achieves the same mAPas the full-supervised baseline with only 24% of the samples.</description>
      <author>example@mail.com (Esteban Rivera, Surya Prabhakaran, Markus Lienkamp)</author>
      <guid isPermaLink="false">2505.00507v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:44 +0800</pubDate>
    </item>
    <item>
      <title>Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis</title>
      <link>http://arxiv.org/abs/2505.00627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAM-Brain3D的脑部特定基础模型和Hypergraph Dynamic Adapter（HyDA）适配器，用于脑部疾病的分割和分类任务，通过多模态、多尺度动态建模提供了一种新的脑部疾病分析方法。&lt;h4&gt;背景&lt;/h4&gt;脑部疾病如阿尔茨海默病和脑瘤因其复杂性和社会影响而带来巨大挑战。现有的脑部基础模型在任务和数据同质性、泛化能力以及适应不同临床任务方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的脑部疾病分析范式，通过改进脑部基础模型和适配器，提高脑部疾病分割和分类任务的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了SAM-Brain3D，一个在超过66,000个脑部图像-标签对上训练的脑部特定基础模型，覆盖14种MRI亚模态。2. 开发了Hypergraph Dynamic Adapter（HyDA），一个轻量级的适配器，用于高效且有效地进行下游适应。&lt;h4&gt;主要发现&lt;/h4&gt;SAM-Brain3D能够捕捉详细的脑部解剖和模态先验知识，用于分割各种脑部目标和更广泛的下游任务。HyDA利用超图融合互补的多模态数据，并动态生成针对患者的卷积核，以实现多尺度特征融合和个性化适应。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，该方法在广泛的脑部疾病分割和分类任务中优于现有最先进的方法，为脑部疾病分析提供了一种新的范式。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Brain diseases, such as Alzheimer's disease and brain tumors, present profound challenges due to their complexity and societal impact. Recent advancements in brain foundation models have shown significant promise in addressing a range of brain-related tasks. However, current brain foundation models are limited by task and data homogeneity, restricted generalization beyond segmentation or classification, and inefficient adaptation to diverse clinical tasks. In this work, we propose SAM-Brain3D, a brain-specific foundation model trained on over 66,000 brain image-label pairs across 14 MRI sub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter for efficient and effective downstream adaptation. SAM-Brain3D captures detailed brain-specific anatomical and modality priors for segmenting diverse brain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse complementary multi-modal data and dynamically generate patient-specific convolutional kernels for multi-scale feature fusion and personalized patient-wise adaptation. Together, our framework excels across a broad spectrum of brain disease segmentation and classification tasks. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art approaches, offering a new paradigm for brain disease analysis through multi-modal, multi-scale, and dynamic foundation modeling.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain diseases, such as Alzheimer's disease and brain tumors, presentprofound challenges due to their complexity and societal impact. Recentadvancements in brain foundation models have shown significant promise inaddressing a range of brain-related tasks. However, current brain foundationmodels are limited by task and data homogeneity, restricted generalizationbeyond segmentation or classification, and inefficient adaptation to diverseclinical tasks. In this work, we propose SAM-Brain3D, a brain-specificfoundation model trained on over 66,000 brain image-label pairs across 14 MRIsub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapterfor efficient and effective downstream adaptation. SAM-Brain3D capturesdetailed brain-specific anatomical and modality priors for segmenting diversebrain targets and broader downstream tasks. HyDA leverages hypergraphs to fusecomplementary multi-modal data and dynamically generate patient-specificconvolutional kernels for multi-scale feature fusion and personalizedpatient-wise adaptation. Together, our framework excels across a broad spectrumof brain disease segmentation and classification tasks. Extensive experimentsdemonstrate that our method consistently outperforms existing state-of-the-artapproaches, offering a new paradigm for brain disease analysis throughmulti-modal, multi-scale, and dynamic foundation modeling.</description>
      <author>example@mail.com (Zhongying Deng, Haoyu Wang, Ziyan Huang, Lipei Zhang, Angelica I. Aviles-Rivero, Chaoyu Liu, Junjun He, Zoe Kourtzi, Carola-Bibiane Schönlieb)</author>
      <guid isPermaLink="false">2505.00627v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:44 +0800</pubDate>
    </item>
    <item>
      <title>OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification</title>
      <link>http://arxiv.org/abs/2505.00650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at: https://github.com/Atahanka/OmicsCL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OmicsCL，一种模块化的对比学习框架，用于从多组学数据中无监督地学习疾病亚型，以推进个性化医学。&lt;h4&gt;背景&lt;/h4&gt;无监督地从多组学数据中学习疾病亚型为个性化医学提供了巨大的机会。&lt;h4&gt;目的&lt;/h4&gt;开发OmicsCL，一个能够联合嵌入异质组学模态（如基因表达、DNA甲基化和miRNA表达）到统一潜在空间的对比学习框架。&lt;h4&gt;方法&lt;/h4&gt;OmicsCL包含一个生存感知的对比损失，该损失鼓励模型学习与生存相关模式对齐的表示，而不依赖于标记的输出。该方法在TCGA BRCA数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;OmicsCL揭示了具有临床意义的聚类，并在患者生存方面实现了强大的无监督一致性。该框架在超参数配置方面表现出鲁棒性，并且可以调整以优先考虑亚型一致性或生存分层。消融研究表明，整合生存感知损失显著增强了学习嵌入的预测能力。&lt;h4&gt;结论&lt;/h4&gt;这些结果突出了对比目标在从高维、异质组学数据中揭示生物洞察力方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised learning of disease subtypes from multi-omics data presents asignificant opportunity for advancing personalized medicine. We introduceOmicsCL, a modular contrastive learning framework that jointly embedsheterogeneous omics modalities-such as gene expression, DNA methylation, andmiRNA expression-into a unified latent space. Our method incorporates asurvival-aware contrastive loss that encourages the model to learnrepresentations aligned with survival-related patterns, without relying onlabeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncoversclinically meaningful clusters and achieves strong unsupervised concordancewith patient survival. The framework demonstrates robustness acrosshyperparameter configurations and can be tuned to prioritize either subtypecoherence or survival stratification. Ablation studies confirm that integratingsurvival-aware loss significantly enhances the predictive power of learnedembeddings. These results highlight the promise of contrastive objectives forbiological insight discovery in high-dimensional, heterogeneous omics data.</description>
      <author>example@mail.com (Atahan Karagoz)</author>
      <guid isPermaLink="false">2505.00650v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:44 +0800</pubDate>
    </item>
    <item>
      <title>SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction</title>
      <link>http://arxiv.org/abs/2505.00625v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的计算材料科学框架，结合了图神经网络（GNNs）的预测能力和符号回归（SR）的解释能力，以实现材料性质的高通量预测。&lt;h4&gt;背景&lt;/h4&gt;深度学习，尤其是图神经网络（GNNs），在材料科学中显示出巨大的效用，特别是在材料性质的高通量预测方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的计算范式，Self-Adaptable Graph Attention Networks integrated with Symbolic Regression (SA-GAT-SR)，以增强预测准确性并提高物理可解释性。&lt;h4&gt;方法&lt;/h4&gt;该方法使用自适应性编码算法来自动识别和调整注意力权重，以从180维特征空间中筛选关键特征，同时保持O(n)的计算规模。符号回归模块随后将这些特征提炼成紧凑的分析表达式，揭示量子力学上有意义的关联。&lt;h4&gt;主要发现&lt;/h4&gt;与依赖第一性原理计算特征的传统符号回归实现相比，该框架实现了23倍的加速，并提供了对材料行为的宝贵物理见解。&lt;h4&gt;结论&lt;/h4&gt;SA-GAT-SR框架在计算材料科学中提供了一个新的框架，弥合了预测准确性和物理可解释性之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;Recent advances in machine learning have demonstrated an enormous utility of deep learning approaches, particularly Graph Neural Networks (GNNs) for materials science. These methods have emerged as powerful tools for high-throughput prediction of material properties, offering a compelling enhancement and alternative to traditional first-principles calculations. While the community has predominantly focused on developing increasingly complex and universal models to enhance predictive accuracy, such approaches often lack physical interpretability and insights into materials behavior. Here, we introduce a novel computational paradigm, Self-Adaptable Graph Attention Networks integrated with Symbolic Regression (SA-GAT-SR), that synergistically combines the predictive capability of GNNs with the interpretative power of symbolic regression. Our framework employs a self-adaptable encoding algorithm that automatically identifies and adjust attention weights so as to screen critical features from an expansive 180-dimensional feature space while maintaining O(n) computational scaling. The integrated SR module subsequently distills these features into compact analytical expressions that explicitly reveal quantum-mechanically meaningful relationships, achieving 23 times acceleration compared to conventional SR implementations that heavily rely on first principle calculations-derived features as input. This work suggests a new framework in computational materials science, bridging the gap between predictive accuracy and physical interpretability, offering valuable physical insights into material behavior.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/MustBeOne/SA-GAT-SR&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in machine learning have demonstrated an enormous utility ofdeep learning approaches, particularly Graph Neural Networks (GNNs) formaterials science. These methods have emerged as powerful tools forhigh-throughput prediction of material properties, offering a compellingenhancement and alternative to traditional first-principles calculations. Whilethe community has predominantly focused on developing increasingly complex anduniversal models to enhance predictive accuracy, such approaches often lackphysical interpretability and insights into materials behavior. Here, weintroduce a novel computational paradigm, Self-Adaptable Graph AttentionNetworks integrated with Symbolic Regression (SA-GAT-SR), that synergisticallycombines the predictive capability of GNNs with the interpretative power ofsymbolic regression. Our framework employs a self-adaptable encoding algorithmthat automatically identifies and adjust attention weights so as to screencritical features from an expansive 180-dimensional feature space whilemaintaining O(n) computational scaling. The integrated SR module subsequentlydistills these features into compact analytical expressions that explicitlyreveal quantum-mechanically meaningful relationships, achieving 23 timesacceleration compared to conventional SR implementations that heavily rely onfirst principle calculations-derived features as input. This work suggests anew framework in computational materials science, bridging the gap betweenpredictive accuracy and physical interpretability, offering valuable physicalinsights into material behavior.</description>
      <author>example@mail.com (Liu Junchi, Tang Ying, Tretiak Sergei, Duan Wenhui, Zhou Liujiang)</author>
      <guid isPermaLink="false">2505.00625v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics</title>
      <link>http://arxiv.org/abs/2505.00555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在生物统计学因果推断的背景下，将机制可解释性技术应用于神经网络的方法。&lt;h4&gt;背景&lt;/h4&gt;在生物统计学中，从预测模型中提取可解释的洞察力对于评估因果性至关重要，而神经网络虽然在建模复杂生物数据方面具有强大的能力，但其传统的“黑盒”特性给验证和高风险健康应用中的信任带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索如何利用机制可解释性技术来提升神经网络在生物统计学因果推断中的应用。&lt;h4&gt;方法&lt;/h4&gt;研究通过以下方式实现：1) 探测和验证神经网络学习到的内部表示，例如在目标最小损失估计（TMLE）框架中估计混杂函数；2) 发现和可视化网络处理不同类型输入时采用的独特计算路径，可能揭示混杂因素和治疗方法是如何处理的；3) 提供比较学习机制和提取的洞察力跨统计、机器学习和神经网络模型的方法，以加深对它们各自优势和劣势的理解。&lt;h4&gt;主要发现&lt;/h4&gt;机制可解释性工具可以用于探测和验证神经网络学习到的内部表示，发现和可视化计算路径，以及比较不同模型的学习机制。&lt;h4&gt;结论&lt;/h4&gt;机制可解释性技术有助于提升神经网络在生物统计学因果推断中的应用，增强了模型的可信度和理解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：从预测模型中提取可解释的洞察力在生物统计学中仍然至关重要，特别是在评估因果性时，经典统计和机器学习方法往往提供内在的清晰性。虽然神经网络（NN）在建模复杂生物数据方面提供了强大的能力，但它们的传统“黑盒”特性给验证和高风险健康应用中的信任带来了挑战。近期在机制可解释性（MI）方面的进展旨在解析这些网络学习到的内部计算。本研究调查了MI技术在生物统计学因果推断背景下应用于神经网络的方法。我们证明，MI工具可以用来：1) 探测和验证神经网络学习到的内部表示，如估计目标最小损失估计（TMLE）框架中的混杂函数等内部表示；2) 发现和可视化网络处理不同类型输入时采用的独特计算路径，可能揭示混杂因素和治疗方法是如何处理的；3) 提供比较学习机制和提取的洞察力跨统计、机器学习和神经网络模型的方法，促进对这些模型各自优势和劣势的深入理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interpretable insights from predictive models remain critical inbio-statistics, particularly when assessing causality, where classicalstatistical and machine learning methods often provide inherent clarity. WhileNeural Networks (NNs) offer powerful capabilities for modeling complexbiological data, their traditional "black-box" nature presents challenges forvalidation and trust in high-stakes health applications. Recent advances inMechanistic Interpretability (MI) aim to decipher the internal computationslearned by these networks. This work investigates the application of MItechniques to NNs within the context of causal inference for bio-statistics.  We demonstrate that MI tools can be leveraged to: (1) probe and validate theinternal representations learned by NNs, such as those estimating nuisancefunctions in frameworks like Targeted Minimum Loss-based Estimation (TMLE); (2)discover and visualize the distinct computational pathways employed by thenetwork to process different types of inputs, potentially revealing howconfounders and treatments are handled; and (3) provide methodologies forcomparing the learned mechanisms and extracted insights across statistical,machine learning, and NN models, fostering a deeper understanding of theirrespective strengths and weaknesses for causal bio-statistical analysis.</description>
      <author>example@mail.com (Jean-Baptiste A. Conan)</author>
      <guid isPermaLink="false">2505.00555v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations</title>
      <link>http://arxiv.org/abs/2505.00500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为INR-DOM的新方法，用于解决在现实场景中操作可变形物体，特别是弹性带的问题。&lt;h4&gt;背景&lt;/h4&gt;由于可变形物体的无限自由度（DoF）和密集但部分可观测的观测（如图像或点云），可变形物体操作（DOM）需要在大状态空间上工作的策略，这增加了采样复杂性和策略学习的不确定性。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现实场景中可变形物体操作的问题，特别是弹性带。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的隐式神经网络表示（INR）学习方法，用于弹性DOM，称为INR-DOM。该方法学习与部分可观测的弹性物体相关的一致状态表示，重建一个表示为符号距离函数的完整且隐式的表面。此外，通过强化学习（RL）进行探索性表示微调，使RL算法能够有效地学习可利用的表示，同时高效地获得DOM策略。&lt;h4&gt;主要发现&lt;/h4&gt;通过构建三个模拟环境和使用Franka Emika Panda机械臂进行的真实世界操作研究，对方法进行了定量和定性分析。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效地处理可变形物体的操作问题，为现实场景中的DOM策略学习提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;We aim to solve the problem of manipulating deformable objects, particularly elastic bands, in real-world scenarios. However, deformable object manipulation (DOM) requires a policy that works on a large state space due to the unlimited degree of freedom (DoF) of deformable objects. Further, their dense but partial observations (e.g., images or point clouds) may increase the sampling complexity and uncertainty in policy learning. To figure it out, we propose a novel implicit neural-representation (INR) learning for elastic DOMs, called INR-DOM. Our method learns consistent state representations associated with partially observable elastic objects reconstructing a complete and implicit surface represented as a signed distance function. Furthermore, we perform exploratory representation fine-tuning through reinforcement learning (RL) that enables RL algorithms to effectively learn exploitable representations while efficiently obtaining a DOM policy. We perform quantitative and qualitative analyses building three simulated environments and real-world manipulation studies with a Franka Emika Panda arm. Videos are available at http://inr-dom.github.io.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We aim to solve the problem of manipulating deformable objects, particularlyelastic bands, in real-world scenarios. However, deformable object manipulation(DOM) requires a policy that works on a large state space due to the unlimiteddegree of freedom (DoF) of deformable objects. Further, their dense but partialobservations (e.g., images or point clouds) may increase the samplingcomplexity and uncertainty in policy learning. To figure it out, we propose anovel implicit neural-representation (INR) learning for elastic DOMs, calledINR-DOM. Our method learns consistent state representations associated withpartially observable elastic objects reconstructing a complete and implicitsurface represented as a signed distance function. Furthermore, we performexploratory representation fine-tuning through reinforcement learning (RL) thatenables RL algorithms to effectively learn exploitable representations whileefficiently obtaining a DOM policy. We perform quantitative and qualitativeanalyses building three simulated environments and real-world manipulationstudies with a Franka Emika Panda arm. Videos are available athttp://inr-dom.github.io.</description>
      <author>example@mail.com (Minseok Song, JeongHo Ha, Bonggyeong Park, Daehyung Park)</author>
      <guid isPermaLink="false">2505.00500v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>AnimalMotionCLIP: Embedding motion in CLIP for Animal Behavior Analysis</title>
      <link>http://arxiv.org/abs/2505.00569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures,Accepted for the poster session at the CV4Animals  workshop: Computer Vision for Animal Behavior Tracking and Modeling In  conjunction with Computer Vision and Pattern Recognition 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AnimalMotionCLIP，用于动物行为识别，通过结合视频帧和光流信息，以及多种时间建模方案，提高了动物行为分析的准确性。&lt;h4&gt;背景&lt;/h4&gt;近年来，深度学习技术在动物行为识别中的应用受到关注，尤其是利用预训练的视觉语言模型如CLIP，因为它们在各类下游任务中表现出优异的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决将深度学习模型应用于动物行为识别时遇到的两个主要挑战：整合运动信息和设计有效的时序建模方案。&lt;h4&gt;方法&lt;/h4&gt;在CLIP框架中，通过交错视频帧和光流信息，以及使用密集、半密集和稀疏的分类器聚合方法来构建时间建模方案。&lt;h4&gt;主要发现&lt;/h4&gt;AnimalMotionCLIP能够在Animal Kingdom数据集上实现比现有方法更优的性能，能够正确识别精细的时序动作，这对动物行为分析至关重要。&lt;h4&gt;结论&lt;/h4&gt;AnimalMotionCLIP为动物行为识别提供了一种有效的方法，并展示了在动物行为分析中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, there has been a surge of interest in applying deep learningtechniques to animal behavior recognition, particularly leveraging pre-trainedvisual language models, such as CLIP, due to their remarkable generalizationcapacity across various downstream tasks. However, adapting these models to thespecific domain of animal behavior recognition presents two significantchallenges: integrating motion information and devising an effective temporalmodeling scheme. In this paper, we propose AnimalMotionCLIP to address thesechallenges by interleaving video frames and optical flow information in theCLIP framework. Additionally, several temporal modeling schemes using anaggregation of classifiers are proposed and compared: dense, semi dense, andsparse. As a result, fine temporal actions can be correctly recognized, whichis of vital importance in animal behavior analysis. Experiments on the AnimalKingdom dataset demonstrate that AnimalMotionCLIP achieves superior performancecompared to state-of-the-art approaches.</description>
      <author>example@mail.com (Enmin Zhong, Carlos R. del-Blanco, Daniel Berjón, Fernando Jaureguizar, Narciso García)</author>
      <guid isPermaLink="false">2505.00569v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction</title>
      <link>http://arxiv.org/abs/2505.00615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Website: https://simongiebenhain.github.io/pixel3dmm/ ;  Video: https://www.youtube.com/watch?v=BwxwEXJwUDc&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种从单张RGB图像中重建3D人脸的方法。&lt;h4&gt;背景&lt;/h4&gt;针对3D人脸重建问题。&lt;h4&gt;目的&lt;/h4&gt;为了提高3D人脸重建的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出Pixel3DMM，一套高度通用的视觉Transformer，预测每个像素的几何线索。2. 利用DINO基础模型的潜在特征，并引入定制的表面法线和uv坐标预测头。3. 通过将三个高质量的3D人脸数据集与FLAME网格拓扑进行配准来训练模型。4. 提出FLAME fitting优化，从uv坐标和法线估计中求解3DMM参数。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过新基准评估了单图像人脸重建。2. 该基准首次评估了姿态和自然面部几何。3. 在姿态面部表达方面，方法在几何精度上优于最具有竞争力的基线超过15%。&lt;h4&gt;结论&lt;/h4&gt;该方法在3D人脸重建方面表现优于现有基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address the 3D reconstruction of human faces from a single RGB image. Tothis end, we propose Pixel3DMM, a set of highly-generalized vision transformerswhich predict per-pixel geometric cues in order to constrain the optimizationof a 3D morphable face model (3DMM). We exploit the latent features of the DINOfoundation model, and introduce a tailored surface normal and uv-coordinateprediction head. We train our model by registering three high-quality 3D facedatasets against the FLAME mesh topology, which results in a total of over1,000 identities and 976K images. For 3D face reconstruction, we propose aFLAME fitting opitmization that solves for the 3DMM parameters from theuv-coordinate and normal estimates. To evaluate our method, we introduce a newbenchmark for single-image face reconstruction, which features high diversityfacial expressions, viewing angles, and ethnicities. Crucially, our benchmarkis the first to evaluate both posed and neutral facial geometry. Ultimately,our method outperforms the most competitive baselines by over 15% in terms ofgeometric accuracy for posed facial expressions.</description>
      <author>example@mail.com (Simon Giebenhain, Tobias Kirschstein, Martin Rünz, Lourdes Agapito, Matthias Nießner)</author>
      <guid isPermaLink="false">2505.00615v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic</title>
      <link>http://arxiv.org/abs/2505.00534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于深度学习的多目标多摄像头跟踪（MO-MCT）框架，用于解决城市规模交通场景中的自动目标跟踪问题。&lt;h4&gt;背景&lt;/h4&gt;随着网络摄像头的数量增加，视觉传感器在智能交通系统（ITS）中的重要性日益提高，但手动对象跟踪和匹配在多个非重叠摄像头之间存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效且经济的深度学习框架，以解决城市规模交通场景中的手动对象跟踪和匹配问题。&lt;h4&gt;方法&lt;/h4&gt;该框架利用Mask R-CNN进行目标检测，使用非极大值抑制（NMS）从重叠检测中选择目标对象，并采用迁移学习进行再识别。此外，利用适当的损失函数和距离度量和ResNet-152进行特征提取，结合Deep SORT进行车辆跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在5th AI City Challenge数据集（Track 3）上实现了竞争性能，IDF1分数为0.8289，精确率和召回率分别为0.9026和0.8527。&lt;h4&gt;结论&lt;/h4&gt;该框架在鲁棒和准确的车辆跟踪方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着网络摄像头数量的持续增加，视觉传感器在智能交通系统（ITS）中对于交通监测、管理和优化变得日益重要。然而，在跨多个非重叠摄像头的城市规模交通场景中进行手动对象跟踪和匹配存在重大挑战。这些挑战包括处理多样化的车辆属性、遮挡、光照变化、阴影以及不同的视频分辨率。为了解决这些问题，我们提出了一种高效且经济的基于深度学习的多目标多摄像头跟踪（MO-MCT）框架。所提出的框架利用Mask R-CNN进行目标检测，并采用非极大值抑制（NMS）从重叠检测中选择目标对象。迁移学习用于再识别，能够实现跨多个摄像头的车辆轨迹块的关联和生成。此外，我们利用适当的损失函数和距离度量和ResNet-152进行特征提取，结合Deep SORT进行车辆跟踪。所提出的框架在5th AI City Challenge数据集（Track 3）上进行了评估，包括46个摄像头流。在这些46个摄像头流中，40个用于模型训练和验证，其余6个用于模型测试。所提出的框架实现了具有竞争力的性能，IDF1分数为0.8289，精确率和召回率分别为0.9026和0.8527，证明了其在鲁棒和准确的车辆跟踪方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s11042-023-16243-7&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision sensors are becoming more important in Intelligent TransportationSystems (ITS) for traffic monitoring, management, and optimization as thenumber of network cameras continues to rise. However, manual object trackingand matching across multiple non-overlapping cameras pose significantchallenges in city-scale urban traffic scenarios. These challenges includehandling diverse vehicle attributes, occlusions, illumination variations,shadows, and varying video resolutions. To address these issues, we propose anefficient and cost-effective deep learning-based framework for Multi-ObjectMulti-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN forobject detection and employs Non-Maximum Suppression (NMS) to select targetobjects from overlapping detections. Transfer learning is employed forre-identification, enabling the association and generation of vehicle trackletsacross multiple cameras. Moreover, we leverage appropriate loss functions anddistance measures to handle occlusion, illumination, and shadow challenges. Thefinal solution identification module performs feature extraction usingResNet-152 coupled with Deep SORT based vehicle tracking. The proposedframework is evaluated on the 5th AI City Challenge dataset (Track 3),comprising 46 camera feeds. Among these 46 camera streams, 40 are used formodel training and validation, while the remaining six are utilized for modeltesting. The proposed framework achieves competitive performance with an IDF1score of 0.8289, and precision and recall scores of 0.9026 and 0.8527respectively, demonstrating its effectiveness in robust and accurate vehicletracking.</description>
      <author>example@mail.com (Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, Rana Hammad Raza)</author>
      <guid isPermaLink="false">2505.00534v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</title>
      <link>http://arxiv.org/abs/2505.00562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICML2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TeLoGraF的方法，通过利用图神经网络编码器和流程匹配来学习通用信号时序逻辑(STL)规范，以解决复杂任务。&lt;h4&gt;背景&lt;/h4&gt;由于缺乏多样化的STL数据集和编码器来有效提取时序逻辑信息，大多数先前的工作仅考虑了固定或参数化的STL规范。&lt;h4&gt;目的&lt;/h4&gt;提高对通用STL规范的学习能力，解决实际应用中的复杂任务。&lt;h4&gt;方法&lt;/h4&gt;提出TeLoGraF方法，使用图神经网络编码器识别四种常用的STL模板，并收集了包含200K规范及其配对演示的数据集。在五个模拟环境中进行了实验，包括2D空间中的简单动态模型和高维7DoF Franka Panda机械臂以及Ant四足导航。&lt;h4&gt;主要发现&lt;/h4&gt;TeLoGraF在STL满意度率上优于其他基线方法。与经典STL规划算法相比，TeLoGraF在推理速度上快10-100倍，并且可以处理任何系统动力学。此外，该方法在解决复杂STL和抵抗分布外STL规范方面表现出强大的能力。&lt;h4&gt;结论&lt;/h4&gt;TeLoGraF是一种高效且通用的STL解决方案，可以应用于各种动态系统。&lt;h4&gt;翻译&lt;/h4&gt;Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to solve complex tasks with signal temporal logic (STL)specifications is crucial to many real-world applications. However, mostprevious works only consider fixed or parametrized STL specifications due tothe lack of a diverse STL dataset and encoders to effectively extract temporallogic information for downstream tasks. In this paper, we propose TeLoGraF,Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)encoder and flow-matching to learn solutions for general STL specifications. Weidentify four commonly used STL templates and collect a total of 200Kspecifications with paired demonstrations. We conduct extensive experiments infive simulation environments ranging from simple dynamical models in the 2Dspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadrupednavigation. Results show that our method outperforms other baselines in the STLsatisfaction rate. Compared to classical STL planning algorithms, our approachis 10-100X faster in inference and can work on any system dynamics. Besides, weshow our graph-encoding method's capability to solve complex STLs androbustness to out-distribution STL specifications. Code is available athttps://github.com/mengyuest/TeLoGraF</description>
      <author>example@mail.com (Yue Meng, Chuchu Fan)</author>
      <guid isPermaLink="false">2505.00562v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass</title>
      <link>http://arxiv.org/abs/2505.00389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by SIGIR 2025 (Full)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的句子表示方法CSE-SFP，用于高效的文本表示。&lt;h4&gt;背景&lt;/h4&gt;句子表示在信息检索和计算语言学中是一个基本任务，对文本聚类、内容分析、问答系统和网络搜索等多种应用有重要影响。&lt;h4&gt;目的&lt;/h4&gt;为了解决由于时间和计算限制，将无监督句子表示与生成性PLMs结合的困难，尤其是针对参数规模更大的生成性PLMs。&lt;h4&gt;方法&lt;/h4&gt;CSE-SFP方法利用生成模型的特性，通过单次前向传递进行有效的无监督对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;CSE-SFP不仅生成高质量的嵌入，而且显著减少了训练时间和内存消耗。此外，引入了两个比率指标，用于评估编码模型的语义空间属性。&lt;h4&gt;结论&lt;/h4&gt;CSE-SFP是一种针对解码器PLM的高效无监督文本表示框架，为评估编码模型的语义空间属性提供了更可靠的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a fundamental task in Information Retrieval and Computational Linguistics,sentence representation has profound implications for a wide range of practicalapplications such as text clustering, content analysis, question-answeringsystems, and web search. Recent advances in pre-trained language models (PLMs)have driven remarkable progress in this field, particularly throughunsupervised embedding derivation methods centered on discriminative PLMs likeBERT. However, due to time and computational constraints, few efforts haveattempted to integrate unsupervised sentence representation with generativePLMs, which typically possess much larger parameter sizes. Given thatstate-of-the-art models in both academia and industry are predominantly basedon generative architectures, there is a pressing need for an efficientunsupervised text representation framework tailored to decoder-only PLMs. Toaddress this concern, we propose CSE-SFP, an innovative method that exploitsthe structural characteristics of generative models. Compared to existingstrategies, CSE-SFP requires only a single forward pass to perform effectiveunsupervised contrastive learning. Rigorous experimentation demonstrates thatCSE-SFP not only produces higher-quality embeddings but also significantlyreduces both training time and memory consumption. Furthermore, we introducetwo ratio metrics that jointly assess alignment and uniformity, therebyproviding a more robust means for evaluating the semantic spatial properties ofencoding models.</description>
      <author>example@mail.com (Bowen Zhang, Zixin Song, Chunping Li)</author>
      <guid isPermaLink="false">2505.00389v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Fast and Low-Cost Genomic Foundation Models via Outlier Removal</title>
      <link>http://arxiv.org/abs/2505.00598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  International Conference on Machine Learning (ICML) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了首个针对基因组基础模型（GFMs）的统一对抗攻击基准，名为GERM，用于系统地评估GFMs对对抗攻击的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;现有GFM基准未能全面评估GFMs对对抗攻击的脆弱性。&lt;h4&gt;目的&lt;/h4&gt;建立一种全面的评估框架，分析GFMs的脆弱性，包括模型架构、量化方案和训练数据集。&lt;h4&gt;方法&lt;/h4&gt;使用四种广泛采用的攻击算法和三种防御策略，对五种最先进的GFMs进行对抗鲁棒性评估。&lt;h4&gt;主要发现&lt;/h4&gt;基于transformer的模型比HyenaDNA具有更强的对抗扰动鲁棒性，强调了架构设计对脆弱性的影响；对抗攻击通常针对生物上重要的基因组区域，表明这些模型有效地捕捉到了有意义的序列特征。&lt;h4&gt;结论&lt;/h4&gt;GERM基准为评估GFMs的脆弱性提供了一个可访问和全面的框架，揭示了架构设计和序列特征在模型鲁棒性中的作用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了首个针对基因组基础模型（GFMs）的统一对抗攻击基准，名为GERM。与现有的GFM基准不同，GERM提供了首个全面评估GFMs对抗攻击脆弱性的评估框架。在方法论上，我们使用四种广泛采用的攻击算法和三种防御策略评估了五种最先进的GFMs的对抗鲁棒性。重要的是，我们的基准提供了一个可访问且全面的框架，以分析GFMs的脆弱性与模型架构、量化方案和训练数据集的关系。经验表明，基于transformer的模型与HyenaDNA相比，对对抗扰动的鲁棒性更强，突出了架构设计对脆弱性的影响。此外，对抗攻击通常针对生物上重要的基因组区域，表明这些模型有效地捕捉到了有意义的序列特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the first unified adversarial attack benchmark for GenomicFoundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERMoffers the first comprehensive evaluation framework to systematically assessthe vulnerability of GFMs to adversarial attacks. Methodologically, we evaluatethe adversarial robustness of five state-of-the-art GFMs using four widelyadopted attack algorithms and three defense strategies. Importantly, ourbenchmark provides an accessible and comprehensive framework to analyze GFMvulnerabilities with respect to model architecture, quantization schemes, andtraining datasets. Empirically, transformer-based models exhibit greaterrobustness to adversarial perturbations compared to HyenaDNA, highlighting theimpact of architectural design on vulnerability. Moreover, adversarial attacksfrequently target biologically significant genomic regions, suggesting thatthese models effectively capture meaningful sequence features.</description>
      <author>example@mail.com (Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry Yao-Chieh Hu, Han Liu)</author>
      <guid isPermaLink="false">2505.00598v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly</title>
      <link>http://arxiv.org/abs/2505.00426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 12 figures, Accepted by IJCAI-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种零样本3D零件组装方法，通过使用预训练的点云扩散模型作为组装过程中的判别器，指导零件操作形成真实形状。&lt;h4&gt;背景&lt;/h4&gt;随着自主组装的需求增长，3D零件组装技术对于机器人至关重要。现有方法主要通过监督学习训练神经网络来估计每个零件的变换，但需要大量手动标注数据，成本高且不适用于大规模应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种零样本零件组装方法，降低数据收集成本，提高组装的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 利用预训练的点云扩散模型作为判别器；2. 将扩散模型应用于零样本组装，转化为迭代最近点（ICP）过程；3. 提出新的推离策略处理重叠部分。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过实验和定量比较，证明其有效性，甚至在某些方面超过了监督学习方法。&lt;h4&gt;结论&lt;/h4&gt;该方法有效提高了零样本3D零件组装的鲁棒性，适用于大规模应用。&lt;h4&gt;翻译&lt;/h4&gt;3D零件组装旨在理解零件关系并预测其6自由度姿态以构建逼真的3D形状，满足自主组装的需求，这对机器人至关重要。现有方法主要通过在监督下训练神经网络来估计每个零件的变换，这需要大量的手动标注数据。然而，数据收集的高成本和现实世界中形状和零件的巨大可变性使得传统方法不适用于大规模应用。在本文中，我们首先提出了一种零样本零件组装方法，该方法利用预训练的点云扩散模型作为组装过程中的判别器，指导零件操作形成真实形状。具体来说，我们理论证明了利用扩散模型进行零样本零件组装可以转化为迭代最近点（ICP）过程。然后，我们提出了一种新的推离策略来解决重叠部分，从而进一步提高方法的鲁棒性。为了验证我们的工作，我们进行了广泛的实验和与几个强基线方法的定量比较，证明了所提出方法的有效性，该方法甚至在某些方面超过了监督学习方法。代码已发布在https://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D part assembly aims to understand part relationships and predict their6-DoF poses to construct realistic 3D shapes, addressing the growing demand forautonomous assembly, which is crucial for robots. Existing methods mainlyestimate the transformation of each part by training neural networks undersupervision, which requires a substantial quantity of manually labeled data.However, the high cost of data collection and the immense variability ofreal-world shapes and parts make traditional methods impractical forlarge-scale applications. In this paper, we propose first a zero-shot partassembly method that utilizes pre-trained point cloud diffusion models asdiscriminators in the assembly process, guiding the manipulation of parts toform realistic shapes. Specifically, we theoretically demonstrate thatutilizing a diffusion model for zero-shot part assembly can be transformed intoan Iterative Closest Point (ICP) process. Then, we propose a novel pushing-awaystrategy to address the overlap parts, thereby further enhancing the robustnessof the method. To verify our work, we conduct extensive experiments andquantitative comparisons to several strong baseline methods, demonstrating theeffectiveness of the proposed approach, which even surpasses the supervisedlearning method. The code has been released onhttps://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.</description>
      <author>example@mail.com (Ruiyuan Zhang, Qi Wang, Jiaxiang Liu, Yu Zhang, Yuchi Huo, Chao Wu)</author>
      <guid isPermaLink="false">2505.00426v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors</title>
      <link>http://arxiv.org/abs/2505.00580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  to appear in Proceedings of the 2025 International Joint Conference  on Artificial Intelligence (IJCAI-2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过分解方法降低复杂度的模型微调方法，并解决了非方阵微调权重的问题，实验表明该方法在多个任务上表现良好，同时减少了浮点运算和可训练参数的数量。&lt;h4&gt;背景&lt;/h4&gt;基础模型在不同领域取得了巨大成功，但它们的计算和存储复杂度高，使得模型难以微调并在实际应用中受限。&lt;h4&gt;目的&lt;/h4&gt;提出一种降低模型微调复杂度的方法，同时提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;通过分解方法，使用交错循环矩阵和斜对角矩阵的乘积，解决非方阵微调权重问题，并使用1D快速傅里叶变换（FFT）代替2D FFT。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个任务上实现了与现有方法相似或更好的性能，同时显著减少了浮点运算和可训练参数的数量。&lt;h4&gt;结论&lt;/h4&gt;提出的方法有效降低了模型微调的复杂度，并提高了实际应用中的适用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基础模型在不同领域取得了巨大成功。然而，它们的巨大计算和存储复杂度使得这些模型难以微调，并且在实际应用中受限。最近的研究表明，在傅里叶域进行训练可以是一种在模型性能和训练参数数量方面都有效的微调方法。在这项工作中，我们通过交错循环矩阵和斜对角矩阵的乘积的分解进一步降低了复杂性。此外，我们通过将循环矩阵划分为块来解决非方阵微调权重的情况。我们的方法避免了权重变化矩阵的构建，并使用1D快速傅里叶变换（FFT）而不是2D FFT。实验结果表明，我们的方法在各种任务上实现了类似或更好的性能，同时大幅减少了浮点运算（FLOPs）和可训练参数的数量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have achieved tremendous success in different domains.However, their huge computation and storage complexity make these modelsdifficult to fine-tune and also less applicable in practice. Recent study showstraining in Fourier domain can be an effective fine-tuning method in terms ofboth model performance and number of training parameters. In this work, wepropose to further reduce the complexity by the factorization through theproduct of interleaved circulant and diagonal matrices. In addition, we addressthe case of non-square fine-tuning weights by partitioning the circulant matrixinto blocks. Our method avoids the construction of weight change matrix andutilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimentalresults show that our method achieves similar or better performance acrossvarious tasks with much less floating-point operations (FLOPs) and the numberof trainable parameters.</description>
      <author>example@mail.com (Xinyu Ding, Lexuan Chen, Siyu Liao, Zhongfeng Wang)</author>
      <guid isPermaLink="false">2505.00580v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior</title>
      <link>http://arxiv.org/abs/2505.00439v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 tables, 3 figures, 3 algorithms&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种动态生成验证集的方法，以提升域内泛化动作策略的缩放行为，并通过实验证明这种方法在9个领域中提升了图神经网络策略的缩放行为。&lt;h4&gt;背景&lt;/h4&gt;现有工作已表明，成功的域内泛化动作策略可以通过学习获得，从小规模训练实例到大规模测试实例的缩放行为是关键目标。&lt;h4&gt;目的&lt;/h4&gt;旨在通过使用比训练实例更大的验证实例来提升策略的缩放行为。&lt;h4&gt;方法&lt;/h4&gt;提出了一种动态生成验证集的方法，并在评估缩放行为时，通过系统地生成测试实例，以确保对每个实例规模覆盖性能的置信度。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，动态验证方法在所有9个测试领域中都提升了图神经网络策略的缩放行为。&lt;h4&gt;结论&lt;/h4&gt;动态验证是一种有效的提升域内泛化动作策略缩放行为的方法。&lt;h4&gt;翻译&lt;/h4&gt;Recent work has shown that successful per-domain generalizing action policies can be learned. Scaling behavior, from small training instances to large test instances, is the key objective; and the use of validation instances larger than training instances is one key to achieve it. Prior work has used fixed validation sets. Here, we introduce a method generating the validation set dynamically, on the fly, increasing instance size so long as informative and feasible. We also introduce refined methodology for evaluating scaling behavior, generating test instances systematically to guarantee a given confidence in coverage performance for each instance size. In experiments, dynamic validation improves scaling behavior of GNN policies in all 9 domains used.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent work has shown that successful per-domain generalizing action policiescan be learned. Scaling behavior, from small training instances to large testinstances, is the key objective; and the use of validation instances largerthan training instances is one key to achieve it. Prior work has used fixedvalidation sets. Here, we introduce a method generating the validation setdynamically, on the fly, increasing instance size so long as informative andfeasible.We also introduce refined methodology for evaluating scaling behavior,generating test instances systematically to guarantee a given confidence incoverage performance for each instance size. In experiments, dynamic validationimproves scaling behavior of GNN policies in all 9 domains used.</description>
      <author>example@mail.com (Timo P. Gros, Nicola J. Müller, Daniel Fiser, Isabel Valera, Verena Wolf, Jörg Hoffmann)</author>
      <guid isPermaLink="false">2505.00439v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</title>
      <link>http://arxiv.org/abs/2505.00308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于深度学习的质量评估方法，用于评估放射治疗中的自动生成的轮廓（auto-contours），特别关注在线自适应放射治疗（OART）。该方法利用贝叶斯有序分类（BOC）和校准的不确定性阈值，能够在不依赖真实轮廓或大量人工标注的情况下进行有信心的质量预测。&lt;h4&gt;背景&lt;/h4&gt;放射治疗中自动生成的轮廓质量评估是一个重要的任务，特别是对于在线自适应放射治疗（OART），需要一种无需真实轮廓或大量人工标注的方法来进行质量评估。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种基于深度学习的质量评估方法，用于评价放射治疗中自动生成的轮廓质量，并特别关注在线自适应放射治疗（OART）。&lt;h4&gt;方法&lt;/h4&gt;研究人员开发了一个BOC模型来对自动轮廓的质量进行分类，并量化预测的不确定性。通过校准步骤优化不确定性阈值以满足临床准确性的需求。方法在无手动标注、有限标注和广泛标注的三种数据场景下进行了验证。对于前列腺癌中的直肠轮廓，当没有手动标注时，使用几何代理标签；当有限时，使用迁移学习；当有充足标注时，使用直接监督。&lt;h4&gt;主要发现&lt;/h4&gt;BOC模型在所有场景下都表现出稳健的性能。仅使用30个手动标注和通过34名受试者校准后，在测试数据上达到了超过90%的准确率。使用校准阈值，超过98%的情况下，自动轮廓的质量被准确地预测，准确率超过93%，从而减少了不必要的手动审查并突出了需要纠正的案例。&lt;h4&gt;结论&lt;/h4&gt;提出的质量评估模型通过减少人工工作量并允许快速、有信息量的临床决策，提高了OART中的轮廓效率。通过不确定性量化，确保了更安全、更可靠的放射治疗工作流程。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种基于深度学习的质量评估方法，用于评估放射治疗中的自动生成的轮廓（auto-contours），重点在于在线自适应放射治疗（OART）。该方法利用贝叶斯有序分类（BOC）和校准的不确定性阈值，可以在不依赖真实轮廓或大量人工标注的情况下进行有信心的质量预测。我们开发了一个BOC模型来对自动轮廓的质量进行分类并量化预测的不确定性。使用校准步骤优化不确定性阈值以满足临床准确性的需求。该方法在无手动标注、有限标注和广泛标注的三种数据场景下进行了验证。对于前列腺癌中的直肠轮廓，在没有手动标注时应用几何代理标签；在有限标注时使用迁移学习；在有充足标注时使用直接监督。BOC模型在所有场景下都表现出稳健的性能。仅使用30个手动标注和通过34名受试者校准后，在测试数据上达到了超过90%的准确率。使用校准阈值，超过98%的情况下，自动轮廓的质量被准确地预测，准确率超过93%，从而减少了不必要的手动审查并突出了需要纠正的案例。提出的质量评估模型通过减少人工工作量并允许快速、有信息量的临床决策，提高了OART中的轮廓效率。通过不确定性量化，确保了更安全、更可靠的放射治疗工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: This study presents a Deep Learning (DL)-based quality assessment(QA) approach for evaluating auto-generated contours (auto-contours) inradiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). LeveragingBayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,the method enables confident QA predictions without relying on ground truthcontours or extensive manual labeling. Methods: We developed a BOC model toclassify auto-contour quality and quantify prediction uncertainty. Acalibration step was used to optimize uncertainty thresholds that meet clinicalaccuracy needs. The method was validated under three data scenarios: no manuallabels, limited labels, and extensive labels. For rectum contours in prostatecancer, we applied geometric surrogate labels when manual labels were absent,transfer learning when limited, and direct supervision when ample labels wereavailable. Results: The BOC model delivered robust performance across allscenarios. Fine-tuning with just 30 manual labels and calibrating with 34subjects yielded over 90% accuracy on test data. Using the calibratedthreshold, over 93% of the auto-contours' qualities were accurately predictedin over 98% of cases, reducing unnecessary manual reviews and highlightingcases needing correction. Conclusion: The proposed QA model enhances contouringefficiency in OART by reducing manual workload and enabling fast, informedclinical decisions. Through uncertainty quantification, it ensures safer, morereliable radiotherapy workflows.</description>
      <author>example@mail.com (Biling Wang, Austen Maniscalco, Ti Bai, Siqiu Wang, Michael Dohopolski, Mu-Han Lin, Chenyang Shen, Dan Nguyen, Junzhou Huang, Steve Jiang, Xinlei Wang)</author>
      <guid isPermaLink="false">2505.00308v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions</title>
      <link>http://arxiv.org/abs/2505.00402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2023&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepSTA的深度时空注意力模型，用于预测快递员的准时送达率，以应对异常情况下的物流挑战。&lt;h4&gt;背景&lt;/h4&gt;预测快递员准时送达率对于物流行业至关重要，特别是在疫情等异常情况下，快递员准时送达率会显著下降且波动较大。&lt;h4&gt;目的&lt;/h4&gt;针对现有研究对物流场景关注不足、异常事件建模不明确、传统数据驱动方法在异常情况下表现不佳等问题，提出一种新的预测模型。&lt;h4&gt;方法&lt;/h4&gt;设计了一种异常时空学习模块，利用循环神经网络来建模事件信息；使用Node2Vec模型来建模道路区域之间的相关性；采用图神经网络和长短期记忆网络来捕捉快递员的时空依赖性；提出了一种异常模式注意力模块，利用记忆网络通过注意力机制存储快递员的异常特征模式。&lt;h4&gt;主要发现&lt;/h4&gt;在2022年COVID-19疫情期间的真实物流数据集上的实验表明，该模型在MAE和MSE方面分别优于最佳基线12.11%和13.71%，证明了其在多个竞争基线中的优越性能。&lt;h4&gt;结论&lt;/h4&gt;DeepSTA模型在预测快递员准时送达率方面表现出色，能够有效应对异常情况下的物流挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3583780.3614671&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prediction of couriers' delivery timely rates in advance is essential to thelogistics industry, enabling companies to take preemptive measures to ensurethe normal operation of delivery services. This becomes even more criticalduring anomaly conditions like the epidemic outbreak, during which couriers'delivery timely rate will decline markedly and fluctuates significantly.Existing studies pay less attention to the logistics scenario. Moreover, manyworks focusing on prediction tasks in anomaly scenarios fail to explicitlymodel abnormal events, e.g., treating external factors equally with otherfeatures, resulting in great information loss. Further, since some anomalousevents occur infrequently, traditional data-driven methods perform poorly inthese scenarios. To deal with them, we propose a deep spatial-temporalattention model, named DeepSTA. To be specific, to avoid information loss, wedesign an anomaly spatio-temporal learning module that employs a recurrentneural network to model incident information. Additionally, we utilize Node2vecto model correlations between road districts, and adopt graph neural networksand long short-term memory to capture the spatial-temporal dependencies ofcouriers. To tackle the issue of insufficient training data in abnormalcircumstances, we propose an anomaly pattern attention module that adopts amemory network for couriers' anomaly feature patterns storage via attentionmechanisms. The experiments on real-world logistics datasets during theCOVID-19 outbreak in 2022 show the model outperforms the best baselines by12.11% in MAE and 13.71% in MSE, demonstrating its superior performance overmultiple competitive baselines.</description>
      <author>example@mail.com (Jinhui Yi, Huan Yan, Haotian Wang, Jian Yuan, Yong Li)</author>
      <guid isPermaLink="false">2505.00402v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices</title>
      <link>http://arxiv.org/abs/2505.00365v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TNNLS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SacFL的新型联邦持续学习框架，旨在解决移动设备在持续学习过程中的存储资源限制、任务切换检测自主性差以及对抗新任务困难等问题。&lt;h4&gt;背景&lt;/h4&gt;随着终端设备的普及，分布式计算模式中，设备上机器学习模型持续处理由这些设备生成的多样化数据。数据的动态特性，即连续变化或数据漂移，对设备上的模型构成了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，本文提出了联邦持续学习（FCL），在保护用户数据隐私的同时，通过协作更新来增强模型。&lt;h4&gt;方法&lt;/h4&gt;SacFL采用编码器-解码器架构，将任务鲁棒和任务敏感组件分离，通过保留轻量级任务敏感组件来显著减少存储需求。此外，SacFL利用对比学习引入了自主数据漂移检测机制，能够识别新任务的出现以及其是否为良性任务。&lt;h4&gt;主要发现&lt;/h4&gt;在多个文本和图像数据集（如Cifar100和THUCNews）上进行的综合实验验证了SacFL在类增量学习和领域增量场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;SacFL框架在解决移动设备持续学习中的挑战方面表现出色，并已开发出演示系统以验证其实用性。&lt;h4&gt;翻译&lt;/h4&gt;The proliferation of end devices has led to a distributed computing paradigm, wherein on-device machine learning models continuously process diverse data generated by these devices. The dynamic nature of this data, characterized by continuous changes or data drift, poses significant challenges for on-device models. To address this issue, continual learning (CL) is proposed, enabling machine learning models to incrementally update their knowledge and mitigate catastrophic forgetting. However, the traditional centralized approach to CL is unsuitable for end devices due to privacy and data volume concerns. In this context, federated continual learning (FCL) emerges as a promising solution, preserving user data locally while enhancing models through collaborative updates. Aiming at the challenges of limited storage resources for CL, poor autonomy in task shift detection, and difficulty in coping with new adversarial tasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL employs an Encoder-Decoder architecture to separate task-robust and task-sensitive components, significantly reducing storage demands by retaining lightweight task-sensitive components for resource-constrained end devices. Moreover, SacFL leverages contrastive learning to introduce an autonomous data shift detection mechanism, enabling it to discern whether a new task has emerged and whether it is a benign task. This capability ultimately allows the device to autonomously trigger CL or attack defense strategy without additional information, which is more practical for end devices. Comprehensive experiments conducted on multiple text and image datasets, such as Cifar100 and THUCNews, have validated the effectiveness of SacFL in both class-incremental and domain-incremental scenarios. Furthermore, a demo system has been developed to verify its practicality.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of end devices has led to a distributed computing paradigm,wherein on-device machine learning models continuously process diverse datagenerated by these devices. The dynamic nature of this data, characterized bycontinuous changes or data drift, poses significant challenges for on-devicemodels. To address this issue, continual learning (CL) is proposed, enablingmachine learning models to incrementally update their knowledge and mitigatecatastrophic forgetting. However, the traditional centralized approach to CL isunsuitable for end devices due to privacy and data volume concerns. In thiscontext, federated continual learning (FCL) emerges as a promising solution,preserving user data locally while enhancing models through collaborativeupdates. Aiming at the challenges of limited storage resources for CL, poorautonomy in task shift detection, and difficulty in coping with new adversarialtasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFLemploys an Encoder-Decoder architecture to separate task-robust andtask-sensitive components, significantly reducing storage demands by retaininglightweight task-sensitive components for resource-constrained end devices.Moreover, $\rm{SacFL}$ leverages contrastive learning to introduce anautonomous data shift detection mechanism, enabling it to discern whether a newtask has emerged and whether it is a benign task. This capability ultimatelyallows the device to autonomously trigger CL or attack defense strategy withoutadditional information, which is more practical for end devices. Comprehensiveexperiments conducted on multiple text and image datasets, such as Cifar100 andTHUCNews, have validated the effectiveness of $\rm{SacFL}$ in bothclass-incremental and domain-incremental scenarios. Furthermore, a demo systemhas been developed to verify its practicality.</description>
      <author>example@mail.com (Zhengyi Zhong, Weidong Bao, Ji Wang, Jianguo Chen, Lingjuan Lyu, Wei Yang Bryan Lim)</author>
      <guid isPermaLink="false">2505.00365v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation</title>
      <link>http://arxiv.org/abs/2505.00378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Information Fusion&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Cues3D的紧凑型方法，用于Open-vocabulary 3D panoptic segmentation，该方法仅依赖于NeRF，避免了预关联，并通过NeRF的隐式3D场实现全局一致的几何结构，从而有效地区分物体。&lt;h4&gt;背景&lt;/h4&gt;Open-vocabulary 3D panoptic segmentation成为研究热点，现有方法结合2D分割和几何感知3D原语，但NeRF类方法因缺乏高保真3D点云而受限。&lt;h4&gt;目的&lt;/h4&gt;提出Cues3D方法，解决NeRF类方法在保持观察一致性方面的不足。&lt;h4&gt;方法&lt;/h4&gt;Cues3D利用NeRF的隐式3D场建立全局一致的几何结构，并提出一个三阶段训练框架：初始化-去歧义-细化，以及实例去歧义方法来匹配NeRF渲染的3D掩码。&lt;h4&gt;主要发现&lt;/h4&gt;Cues3D在ScanNet v2、ScanNet200、ScanNet++和Replica数据集上表现出色，优于其他2D图像方法，与最新的2D-3D融合方法竞争，并在使用额外3D点云时甚至超越。&lt;h4&gt;结论&lt;/h4&gt;Cues3D是一种有效的3D panoptic segmentation方法，能够提供高度一致和独特的3D实例ID。&lt;h4&gt;翻译&lt;/h4&gt;Open-vocabulary 3D panoptic segmentation最近成为了一个显著的研究趋势。目前表现最好的方法将2D分割与几何感知的3D原语相结合。然而，如果没有高保真的3D点云，如基于NeRF的方法，这种优势就会丧失。这些方法受限于在部分观察中保持一致性的能力不足。为了解决这个问题，最近的工作利用对比损失或跨视图关联预处理来进行视图一致性。与它们相反，我们提出了Cues3D，一种仅依赖于NeRF而不是预关联的紧凑方法。其核心思想是NeRF的隐式3D场内在地建立了一个全局一致的几何结构，从而在没有显式跨视图监督的情况下有效地区分物体。我们提出了一个针对NeRF的三阶段训练框架，即初始化-去歧义-细化，其中使用最初学习到的知识来纠正实例ID。此外，还提出了一种实例去歧义方法来匹配NeRF渲染的3D掩码，并确保全局唯一的3D实例身份。借助Cues3D，我们使用平衡版本的NeRF在视图中为每个对象获得了高度一致和独特的3D实例ID。我们的实验在ScanNet v2、ScanNet200、ScanNet++和Replica数据集上进行了3D实例、panoptic和语义分割任务。Cues3D优于其他基于2D图像的方法，并与最新的2D-3D融合方法竞争，甚至在使用额外的3D点云时甚至超过了它们。代码链接可在附录中找到，并将发布在github上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D panoptic segmentation has recently emerged as asignificant trend. Top-performing methods currently integrate 2D segmentationwith geometry-aware 3D primitives. However, the advantage would be lost withouthigh-fidelity 3D point clouds, such as methods based on Neural Radiance Field(NeRF). These methods are limited by the insufficient capacity to maintainconsistency across partial observations. To address this, recent works haveutilized contrastive loss or cross-view association pre-processing for viewconsensus. In contrast to them, we present Cues3D, a compact approach thatrelies solely on NeRF instead of pre-associations. The core idea is that NeRF'simplicit 3D field inherently establishes a globally consistent geometry,enabling effective object distinction without explicit cross-view supervision.We propose a three-phase training framework for NeRF,initialization-disambiguation-refinement, whereby the instance IDs arecorrected using the initially-learned knowledge. Additionally, an instancedisambiguation method is proposed to match NeRF-rendered 3D masks and ensureglobally unique 3D instance identities. With the aid of Cues3D, we obtainhighly consistent and unique 3D instance ID for each object across views with abalanced version of NeRF. Our experiments are conducted on ScanNet v2,ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, andsemantic segmentation tasks. Cues3D outperforms other 2D image-based methodsand competes with the latest 2D-3D merging based methods, while even surpassingthem when using additional 3D point clouds. The code link could be found in theappendix and will be released on\href{https://github.com/mRobotit/Cues3D}{github}</description>
      <author>example@mail.com (Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe)</author>
      <guid isPermaLink="false">2505.00378v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>R&amp;B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training</title>
      <link>http://arxiv.org/abs/2505.00358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为R&amp;B的框架，用于改进数据混合策略，以降低训练语言模型的成本。&lt;h4&gt;背景&lt;/h4&gt;数据混合策略在降低语言模型训练成本方面取得了成功，但存在两个主要缺陷：依赖预定的数据域和计算效率问题。&lt;h4&gt;目的&lt;/h4&gt;旨在解决数据混合策略的缺陷，提出一种更有效的框架。&lt;h4&gt;方法&lt;/h4&gt;R&amp;B框架通过基于语义相似性重新划分训练数据（Regroup）来创建更细粒度的领域，并通过利用训练过程中获得的领域梯度诱导的Gram矩阵来有效优化数据组合（Balance）。&lt;h4&gt;主要发现&lt;/h4&gt;R&amp;B框架无需额外的计算即可获得评估信息，如损失或梯度，并且与现有非自适应混合方法相比，在理论和实证上都表现出更高的有效性。&lt;h4&gt;结论&lt;/h4&gt;R&amp;B框架在五个不同数据集上证明了其有效性，且只需极小的额外计算开销（0.01%）即可匹配或超越最先进的数据混合策略的性能。&lt;h4&gt;翻译&lt;/h4&gt;Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&amp;B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&amp;B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&amp;B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&amp;B matches or exceeds the performance of state-of-the-art data mixing strategies.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data mixing strategies have successfully reduced the costs involved intraining language models. While promising, such methods suffer from two flaws.First, they rely on predetermined data domains (e.g., data sources, tasktypes), which may fail to capture critical semantic nuances, leavingperformance on the table. Second, these methods scale with the number ofdomains in a computationally prohibitive way. We address these challenges viaR&amp;B, a framework that re-partitions training data based on semantic similarity(Regroup) to create finer-grained domains, and efficiently optimizes the datacomposition (Balance) by leveraging a Gram matrix induced by domain gradientsobtained throughout training. Unlike prior works, it removes the need foradditional compute to obtain evaluation information such as losses orgradients. We analyze this technique under standard regularity conditions andprovide theoretical insights that justify R&amp;B's effectiveness compared tonon-adaptive mixing approaches. Empirically, we demonstrate the effectivenessof R&amp;B on five diverse datasets ranging from natural language to reasoning andmultimodal tasks. With as little as 0.01% additional compute overhead, R&amp;Bmatches or exceeds the performance of state-of-the-art data mixing strategies.</description>
      <author>example@mail.com (Albert Ge, Tzu-Heng Huang, John Cooper, Avi Trost, Ziyi Chu, Satya Sai Srinath Namburi GNVV, Ziyang Cai, Kendall Park, Nicholas Roberts, Frederic Sala)</author>
      <guid isPermaLink="false">2505.00358v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2505.00364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TIF的新颖的图分类可解释框架，通过将GNN转换为层次树，以不同粒度的粗化图作为树节点，提供多粒度的可解释性。&lt;h4&gt;背景&lt;/h4&gt;现有的基于子图的可解释方法过于强调局部结构，可能忽略了图中的长距离依赖关系。虽然基于图粗化的方法对全局可解释性有益，但它们不可避免地将图简化为固定的粒度。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，能够在不同粒度上捕捉现实世界图任务中的关系，同时保持模型的可解释性。&lt;h4&gt;方法&lt;/h4&gt;TIF通过迭代采用图粗化模块将原始图压缩为越来越粗的子图，并通过特定的图扰动模块保持不同分支中树节点的多样性。此外，还引入了自适应路由模块来识别最有信息的根到叶路径。&lt;h4&gt;主要发现&lt;/h4&gt;TIF在可解释性方面表现出优越性，同时在预测性能上与最先进的模型相当。&lt;h4&gt;结论&lt;/h4&gt;TIF为图分类提供了一个既可解释又有竞争力的预测性能的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interpretable Graph Neural Networks (GNNs) aim to reveal the underlyingreasoning behind model predictions, attributing their decisions to specificsubgraphs that are informative. However, existing subgraph-based interpretablemethods suffer from an overemphasis on local structure, potentially overlookinglong-range dependencies within the entire graphs. Although recent efforts thatrely on graph coarsening have proven beneficial for global interpretability,they inevitably reduce the graphs to a fixed granularity. Such an inflexibleway can only capture graph connectivity at a specific level, whereas real-worldgraph tasks often exhibit relationships at varying granularities (e.g.,relevant interactions in proteins span from functional groups, to amino acids,and up to protein domains). In this paper, we introduce a novel Tree-likeInterpretable Framework (TIF) for graph classification, where plain GNNs aretransformed into hierarchical trees, with each level featuring coarsened graphsof different granularity as tree nodes. Specifically, TIF iteratively adopts agraph coarsening module to compress original graphs (i.e., root nodes of trees)into increasingly coarser ones (i.e., child nodes of trees), while preservingdiversity among tree nodes within different branches through a dedicated graphperturbation module. Finally, we propose an adaptive routing module to identifythe most informative root-to-leaf paths, providing not only the finalprediction but also the multi-granular interpretability for the decision-makingprocess. Extensive experiments on the graph classification benchmarks with bothsynthetic and real-world datasets demonstrate the superiority of TIF ininterpretability, while also delivering a competitive prediction performanceakin to the state-of-the-art counterparts.</description>
      <author>example@mail.com (Jie Yang, Yuwen Wang, Kaixuan Chen, Tongya Zheng, Yihe Zhou, Zhenbang Xiao, Ji Cao, Mingli Song, Shunyu Liu)</author>
      <guid isPermaLink="false">2505.00364v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction</title>
      <link>http://arxiv.org/abs/2505.00259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的PTQ方法，称为Pack-PTQ，用于压缩复杂模型。&lt;h4&gt;背景&lt;/h4&gt;PTQ已成为压缩复杂模型的重要解决方案，它倡导使用小的校准数据集并避免端到端重训练。&lt;h4&gt;目的&lt;/h4&gt;解决现有PTQ方法中块状重建忽略跨块依赖和低比特率下准确率下降的问题。&lt;h4&gt;方法&lt;/h4&gt;Pack-PTQ首先设计了一种Hessian引导的自适应打包机制，将块分为非重叠的打包单元，作为重建的基本单位，从而保留跨块依赖并实现准确的量化参数估计。其次，基于打包配置，提出了一种混合精度量化方法，根据打包单元的不同敏感性分配不同的比特宽度，从而进一步提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;在2D图像和3D点云分类任务上进行的广泛实验表明，该方法在性能上优于现有的PTQ方法。&lt;h4&gt;结论&lt;/h4&gt;Pack-PTQ是一种有效的PTQ方法，可以显著提高压缩模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;Post-training quantization (PTQ) has evolved as a prominent solution for compressing complex models, which advocates a small calibration dataset and avoids end-to-end retraining. However, most existing PTQ methods employ block-wise reconstruction, which neglects cross-block dependency and exhibits an noticeable accuracy drop in low-bit cases. To address these limitations, this paper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a Hessian-guided adaptive packing mechanism to partition blocks into non-overlapping packs, which serve as the base unit for reconstruction, thereby preserving the cross-block dependency and enabling accurate quantization parameters estimation. Second, based on the pack configuration, we propose a mixed-precision quantization approach to assign varied bit-widths to packs according to their distinct sensitivities, thereby further enhancing performance. Extensive experiments on 2D image and 3D point cloud classification tasks, using various network architectures, demonstrate the superiority of our method over the state-of-the-art PTQ methods.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Post-training quantization (PTQ) has evolved as a prominent solution forcompressing complex models, which advocates a small calibration dataset andavoids end-to-end retraining. However, most existing PTQ methods employblock-wise reconstruction, which neglects cross-block dependency and exhibits anotable accuracy drop in low-bit cases. To address these limitations, thispaper presents a novel PTQ method, dubbed Pack-PTQ. First, we design aHessian-guided adaptive packing mechanism to partition blocks intonon-overlapping packs, which serve as the base unit for reconstruction, therebypreserving the cross-block dependency and enabling accurate quantizationparameters estimation. Second, based on the pack configuration, we propose amixed-precision quantization approach to assign varied bit-widths to packsaccording to their distinct sensitivities, thereby further enhancingperformance. Extensive experiments on 2D image and 3D point cloudclassification tasks, using various network architectures, demonstrate thesuperiority of our method over the state-of-the-art PTQ methods.</description>
      <author>example@mail.com (Changjun Li, Runqing Jiang, Zhuo Song, Pengpeng Yu, Ye Zhang, Yulan Guo)</author>
      <guid isPermaLink="false">2505.00259v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution</title>
      <link>http://arxiv.org/abs/2505.00334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的图像超分辨率（SR）框架ResQu，它结合了四元数小波预处理框架和潜在扩散模型，以提高从低分辨率图像重建高分辨率图像的质量。&lt;h4&gt;背景&lt;/h4&gt;图像超分辨率是计算机视觉中的基本问题，它在医学成像、卫星分析等领域有广泛的应用。从低分辨率输入重建高分辨率图像对于增强下游任务如目标检测和分割至关重要。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出一种能够平衡感知质量和结构保真度的图像超分辨率方法，特别是在高放大倍数的情况下。&lt;h4&gt;方法&lt;/h4&gt;ResQu框架结合了四元数小波预处理和潜在扩散模型，并引入了一种新的四元数小波和时间感知的编码器。此外，该方法还利用了如Stable Diffusion等基础模型的生成先验。&lt;h4&gt;主要发现&lt;/h4&gt;在特定领域的数据集上的大量实验表明，ResQu方法在感知质量和标准评估指标上优于许多现有方法。&lt;h4&gt;结论&lt;/h4&gt;ResQu框架能够实现出色的图像超分辨率结果，在感知质量和结构保真度方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces a novel image super-resolution (SR) framework called ResQu, which integrates a quaternion wavelet preprocessing framework with latent diffusion models to enhance the quality of high-resolution image reconstruction from low-resolution inputs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image Super-Resolution is a fundamental problem in computer vision with broadapplications spacing from medical imaging to satellite analysis. The ability toreconstruct high-resolution images from low-resolution inputs is crucial forenhancing downstream tasks such as object detection and segmentation. Whiledeep learning has significantly advanced SR, achieving high-qualityreconstructions with fine-grained details and realistic textures remainschallenging, particularly at high upscaling factors. Recent approachesleveraging diffusion models have demonstrated promising results, yet they oftenstruggle to balance perceptual quality with structural fidelity. In this work,we introduce ResQu a novel SR framework that integrates a quaternion waveletpreprocessing framework with latent diffusion models, incorporating a newquaternion wavelet- and time-aware encoder. Unlike prior methods that simplyapply wavelet transforms within diffusion models, our approach enhances theconditioning process by exploiting quaternion wavelet embeddings, which aredynamically integrated at different stages of denoising. Furthermore, we alsoleverage the generative priors of foundation models such as Stable Diffusion.Extensive experiments on domain-specific datasets demonstrate that our methodachieves outstanding SR results, outperforming in many cases existingapproaches in perceptual quality and standard evaluation metrics. The code willbe available after the revision process.</description>
      <author>example@mail.com (Luigi Sigillo, Christian Bianchi, Danilo Comminiello)</author>
      <guid isPermaLink="false">2505.00334v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Repetition Makes Perfect: Recurrent Sum-GNNs Match Message Passing Limit</title>
      <link>http://arxiv.org/abs/2505.00291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提供了有限精度参数的循环图神经网络（recurrent GNNs）的精确表达能力界限。&lt;h4&gt;背景&lt;/h4&gt;GNNs的表达能力受到由颜色细化（或Weisfeiler-Leman）算法引起的自然消息传递不变性的限制。&lt;h4&gt;目的&lt;/h4&gt;研究循环GNNs在有限精度参数下的表达能力。&lt;h4&gt;方法&lt;/h4&gt;通过证明循环GNNs（使用求和聚合和ReLU激活）可以模拟遵循颜色细化算法消息传递不变性的任何图算法。&lt;h4&gt;主要发现&lt;/h4&gt;循环GNNs可以达到由上述不变性限制的表达能力极限，这与非循环GNNs形成对比，后者只能在非常弱的形式下实现Weisfeiler-Leman的能力。&lt;h4&gt;结论&lt;/h4&gt;通过引入随机初始化，循环GNNs可以模拟所有图算法，特别是任何多项式时间复杂度的图算法都可以由随机初始化的循环GNN在多项式时间内模拟。&lt;h4&gt;翻译&lt;/h4&gt;我们为具有有限精度参数的循环图神经网络（recurrent GNNs）的表达能力提供了首次精确界限。我们证明，使用求和聚合和ReLU激活的循环GNNs可以模拟遵循颜色细化（或Weisfeiler-Leman）算法诱导的自然消息传递不变性的任何图算法。虽然众所周知，GNNs的表达能力受到这种不变性的限制[Morris等人，AAAI2019；Xu等人，ICLR 2019]，但我们证明循环GNNs实际上可以达到这个极限。这与非循环GNNs形成对比，后者只有在非常弱的形式下才具有Weisfeiler-Leman的能力，即每个图大小都需要不同的GNN模型来计算。我们构建的模拟只在时间和空间上引入了多项式开销。此外，我们展示了通过引入随机初始化，循环GNNs可以模拟所有图算法，这特别意味着任何具有多项式时间复杂度的图算法都可以由随机初始化的循环GNN在多项式时间内模拟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We provide first tight bounds for the expressivity of Recurrent Graph NeuralNetworks (recurrent GNNs) with finite-precision parameters. We prove thatrecurrent GNNs, with sum aggregation and ReLU activation, can emulate any graphalgorithm that respects the natural message-passing invariance induced by thecolor refinement (or Weisfeiler-Leman) algorithm. While it is well known thatthe expressive power of GNNs is limited by this invariance [Morris et al., AAAI2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actuallyreach this limit. This is in contrast to non-recurrent GNNs, which have thepower of Weisfeiler-Leman only in a very weak, "non-uniform", sense where everygraph size requires a different GNN model to compute with. The emulation weconstruct introduces only a polynomial overhead in both time and space.  Furthermore, we show that by incorporating random initialization, recurrentGNNs can emulate all graph algorithms, implying in particular that any graphalgorithm with polynomial-time complexity can be emulated by a recurrent GNNwith random initialization, running in polynomial time.</description>
      <author>example@mail.com (Eran Rosenbluth, Martin Grohe)</author>
      <guid isPermaLink="false">2505.00291v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Explorative Curriculum Learning for Strongly Correlated Electron Systems</title>
      <link>http://arxiv.org/abs/2505.00233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了基于转移学习和课程学习框架的NQS，用于高效稳定地探索量子多体系统的大参数空间，并通过Pairing-Net架构验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;神经网络量子态（NQS）在预测复杂量子多体系统方面取得了进展，但计算成本高，探索参数效率低。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于转移学习和课程学习框架的NQS，以解决计算成本高和参数探索效率低的问题。&lt;h4&gt;方法&lt;/h4&gt;设计了基于转移学习的课程学习框架，并通过Pairing-Net架构在强关联电子系统中实现这一策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法相较于传统方法在计算速度上提高了约200倍，并在优化稳定性上有所改善。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法可以有效地提高NQS的计算效率和稳定性，为探索量子多体系统提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，神经网络量子态（NQS）在复杂量子多体系统，如强关联电子系统的预测中取得了显著的进展。然而，其计算成本仍然很高，这使得探索各种相互作用强度和其他物理参数变得效率低下。尽管提出了迁移学习来缓解这一挑战，但将泛化到大规模系统和多样化的参数范围内仍然困难。为了解决这一局限性，我们提出了一种基于迁移学习的NQS课程学习框架的新方法。这有助于高效稳定地在量子多体系统的大参数空间中进行探索。此外，通过通过微扰的角度来解释NQS迁移学习，我们展示了如何将先前的物理知识灵活地纳入课程学习过程中。我们还提出了Pairing-Net架构，这是一种在强关联电子系统中实际实现这一策略的方法，并通过实证验证了其有效性。我们的结果表明，与传统的计算方法相比，该方法在计算速度上提高了大约200倍，在优化稳定性方面也有所改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in neural network quantum states (NQS) have enabledhigh-accuracy predictions for complex quantum many-body systems such asstrongly correlated electron systems. However, the computational cost remainsprohibitive, making exploration of the diverse parameters of interactionstrengths and other physical parameters inefficient. While transfer learninghas been proposed to mitigate this challenge, achieving generalization tolarge-scale systems and diverse parameter regimes remains difficult. To addressthis limitation, we propose a novel curriculum learning framework based ontransfer learning for NQS. This facilitates efficient and stable explorationacross a vast parameter space of quantum many-body systems. In addition, byinterpreting NQS transfer learning through a perturbative lens, we demonstratehow prior physical knowledge can be flexibly incorporated into the curriculumlearning process. We also propose Pairing-Net, an architecture to practicallyimplement this strategy for strongly correlated electron systems, andempirically verify its effectiveness. Our results show an approximately200-fold speedup in computation and a marked improvement in optimizationstability compared to conventional methods.</description>
      <author>example@mail.com (Kimihiro Yamazaki, Takuya Konishi, Yoshinobu Kawahara)</author>
      <guid isPermaLink="false">2505.00233v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models as AI Agents for Digital Atoms and Molecules: Catalyzing a New Era in Computational Biophysics</title>
      <link>http://arxiv.org/abs/2505.00270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了计算生物物理学领域中的LLMs和基于代理的系统如何改变该领域，并介绍了ADAM框架，一个基于多代理的LLMs框架，用于生物物理计算。&lt;h4&gt;背景&lt;/h4&gt;计算生物物理学领域分子数据迅速增长，系统复杂性呈指数增长，LLMs和基于代理的系统正在重塑该领域。&lt;h4&gt;目的&lt;/h4&gt;考察LLMs、智能代理和科学计算交叉领域的近期进展，并介绍ADAM框架。&lt;h4&gt;方法&lt;/h4&gt;ADAM采用模块化设计，结合LLM驱动的语义工具和确定性符号计算，并引入ADAM工具协议（ATP）以实现异步、数据库中心的工具编排。&lt;h4&gt;主要发现&lt;/h4&gt;ADAM通过混合神经符号架构和模块化设计重塑科学工作流程，并促进社区驱动的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;尽管取得了显著进展，但建立基准标准、优化基础模型和代理以及构建开放协作生态系统仍需进一步努力。&lt;h4&gt;翻译&lt;/h4&gt;在计算生物物理学领域，随着分子数据的快速增长和系统复杂性的指数增长，大型语言模型（LLMs）和基于代理的系统正在从根本上改变该领域。本文从LLMs、智能代理和科学计算的交叉领域考察了最近的研究进展，并在此基础上介绍了ADAM（数字原子和分子的代理），一个创新的基于多代理的LLMs框架。ADAM利用先进的AI架构通过模块化设计重塑科学工作流程。它采用混合神经符号架构，结合LLM驱动的语义工具和确定性符号计算。此外，它的ADAM工具协议（ATP）实现了异步、数据库中心的工具编排，促进了社区驱动的可扩展性。尽管取得了显著的进展，但持续的挑战需要进一步努力来建立基准标准、优化基础模型和代理以及构建开放的协作生态系统。ADAM可在https://sidereus-ai.com访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In computational biophysics, where molecular data is expanding rapidly andsystem complexity is increasing exponentially, large language models (LLMs) andagent-based systems are fundamentally reshaping the field. This perspectivearticle examines the recent advances at the intersection of LLMs, intelligentagents, and scientific computation, with a focus on biophysical computation.Building on these advancements, we introduce ADAM (Agent for Digital Atoms andMolecules), an innovative multi-agent LLM-based framework. ADAM employscutting-edge AI architectures to reshape scientific workflows through a modulardesign. It adopts a hybrid neural-symbolic architecture that combinesLLM-driven semantic tools with deterministic symbolic computations. Moreover,its ADAM Tool Protocol (ATP) enables asynchronous, database-centric toolorchestration, fostering community-driven extensibility. Despite thesignificant progress made, ongoing challenges call for further efforts inestablishing benchmarking standards, optimizing foundational models and agents,and building an open collaborative ecosystem. ADAM is accessible athttps://sidereus-ai.com.</description>
      <author>example@mail.com (Yijie Xia, Xiaohan Lin, Zicheng Ma, Jinyuan Hu, Yanheng Li, Zhaoxin Xie, Hao Li, Li Yang, Zhiqiang Zhao, Lijiang Yang, Zhenyu Chen, Yi Qin Gao)</author>
      <guid isPermaLink="false">2505.00270v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation</title>
      <link>http://arxiv.org/abs/2505.00257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Heterogeneous Federated Graph Neural Network (HFGNN)的方法，旨在解决跨境金融数据共享中的隐私问题。&lt;h4&gt;背景&lt;/h4&gt;金融机构对共享外部数据有强烈需求，但隐私问题导致不同平台间互联困难，数据开放程度低。&lt;h4&gt;目的&lt;/h4&gt;为了有效解决跨境金融数据共享中的隐私问题，确保数据可用但不可见，实现不同行业业务组织的异构数据联合画像。&lt;h4&gt;方法&lt;/h4&gt;该方法将跨境组织的异构业务数据分布作为子图，通过中央服务器将子图之间的共享和流通过程构建为统计异构的全局图。每个子图通过本地训练学习相应的个性化服务模型，选择和更新带有聚合参数的相关子图子集，有效分离和结合子图之间的拓扑和特征信息。&lt;h4&gt;主要发现&lt;/h4&gt;模拟实验结果表明，所提出的方法在准确性和收敛速度方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;HFGNN方法能够有效解决跨境金融数据共享中的隐私问题，提高了数据共享的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The sharing of external data has become a strong demand of financialinstitutions, but the privacy issue has led to the difficulty ofinterconnecting different platforms and the low degree of data openness. Toeffectively solve the privacy problem of financial data in trans-border flowand sharing, to ensure that the data is available but not visible, to realizethe joint portrait of all kinds of heterogeneous data of business organizationsin different industries, we propose a Heterogeneous Federated Graph NeuralNetwork (HFGNN) approach. In this method, the distribution of heterogeneousbusiness data of trans-border organizations is taken as subgraphs, and thesharing and circulation process among subgraphs is constructed as astatistically heterogeneous global graph through a central server. Eachsubgraph learns the corresponding personalized service model through localtraining to select and update the relevant subset of subgraphs with aggregatedparameters, and effectively separates and combines topological and featureinformation among subgraphs. Finally, our simulation experimental results showthat the proposed method has higher accuracy performance and faster convergencespeed than existing methods.</description>
      <author>example@mail.com (Zhizhong Tan, Jiexin Zheng, Kevin Qi Zhang, Wenyong Wang)</author>
      <guid isPermaLink="false">2505.00257v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction</title>
      <link>http://arxiv.org/abs/2505.00236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于分层图表示学习的成分-疾病关联预测模型（Node2Vec-DGI-EL），用于预测中药成分与疾病之间的潜在关联，并验证了该模型的有效性。&lt;h4&gt;背景&lt;/h4&gt;中药作为传统医学的重要组成部分，包含对现代药物开发至关重要的活性成分，具有巨大的治疗潜力和开发价值。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效预测中药成分与疾病之间关联的模型。&lt;h4&gt;方法&lt;/h4&gt;使用Node2Vec算法提取网络节点的嵌入向量作为初始特征，利用DGI算法对网络节点进行深层表示和学习，并采用集成学习方法提高预测准确性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;该模型显著优于现有方法，AUC达到0.9987，AUPR达到0.9545，表明其具有优越的预测能力。消融实验进一步揭示了各模块的贡献和重要性。案例研究探讨了潜在的关联，如三尖杉酯碱与高血压视网膜病变，甲基牛磺酸与结直肠癌。分子对接实验验证了这些发现。&lt;h4&gt;结论&lt;/h4&gt;Node2Vec-DGI-EL模型专注于中药数据集，并有效地预测了成分-疾病关联，克服了对节点语义信息的依赖。&lt;h4&gt;翻译&lt;/h4&gt;This research proposes an ingredient-disease association prediction model (Node2Vec-DGI-EL) based on hierarchical graph representation learning, for effectively predicting potential associations between traditional Chinese medicine ingredients and diseases, and verifies the effectiveness of the model.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Chinese medicine, as an essential component of traditionalmedicine, contains active ingredients that serve as a crucial source for moderndrug development, holding immense therapeutic potential and development value.A multi-layered and complex network is formed from Chinese medicine to diseasesand used to predict the potential associations between Chinese medicineingredients and diseases. This study proposes an ingredient-disease associationprediction model (Node2Vec-DGI-EL) based on hierarchical graph representationlearning. First, the model uses the Node2Vec algorithm to extract nodeembedding vectors from the network as the initial features of the nodes. Next,the network nodes are deeply represented and learned using the DGI algorithm toenhance the model's expressive power. To improve prediction accuracy androbustness, an ensemble learning method is incorporated to achieve moreaccurate ingredient-disease association predictions. The effectiveness of themodel is then evaluated through a series of theoretical verifications. Theresults demonstrated that the proposed model significantly outperformedexisting methods, achieving an AUC of 0.9987 and an AUPR of 0.9545, therebyindicating superior predictive capability. Ablation experiments furtherrevealed the contribution and importance of each module. Additionally, casestudies explored potential associations, such as triptonide with hypertensiveretinopathy and methyl ursolate with colorectal cancer. Molecular dockingexperiments validated these findings, showing the triptonide-PGR interactionand the methyl ursolate-NFE2L2 interaction can bind stable. In conclusion, theNode2Vec-DGI-EL model focuses on TCM datasets and effectively predictsingredient-disease associations, overcoming the reliance on node semanticinformation.</description>
      <author>example@mail.com (Leifeng Zhang, Xin Dong, Shuaibing Jia, Jianhua Zhang)</author>
      <guid isPermaLink="false">2505.00236v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation</title>
      <link>http://arxiv.org/abs/2504.19174v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH 2025 (Patent Protected); Project page:  https://vcc.tech/research/2025/CLRWire&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CLR-Wire是一种新的3D曲线线框生成框架，通过将几何和拓扑集成到一个统一的连续潜在表示中，实现了曲线的编码。&lt;h4&gt;背景&lt;/h4&gt;传统的线框生成方法将顶点、边和面解耦，而CLR-Wire将曲线及其拓扑连接编码为连续且固定长度的潜在空间。&lt;h4&gt;目的&lt;/h4&gt;CLR-Wire旨在提供一种能够联合学习几何和拓扑的方法，以生成高质量的线框。&lt;h4&gt;方法&lt;/h4&gt;CLR-Wire使用注意力驱动的变分自编码器（VAE）将曲线编码到潜在空间中，并使用流匹配模型将高斯噪声映射到这些潜在空间，最终解码成完整的3D线框。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进的生成方法相比，CLR-Wire在准确性、新颖性和多样性方面取得了显著改进，为CAD设计、几何重建和3D内容创作提供了一个高效且全面的解决方案。&lt;h4&gt;结论&lt;/h4&gt;CLR-Wire提供了一种细粒度的复杂形状和不规则拓扑建模方法，并支持无条件和基于点云或图像输入的条件生成。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种名为CLR-Wire的新型3D曲线线框生成框架，该框架将几何和拓扑集成到一个统一的连续潜在表示中。与将顶点、边和面解耦的传统方法不同，CLR-Wire通过使用注意力驱动的变分自编码器（VAE）将曲线及其拓扑连接编码为连续且固定长度的潜在空间。这种统一的方法促进了几何和拓扑的联合学习和生成。为了生成线框，我们采用流匹配模型将高斯噪声逐步映射到这些潜在空间，然后解码成完整的3D线框。我们的方法提供了复杂形状和不规则拓扑的细粒度建模，并支持无条件和基于点云或图像输入的条件生成。实验结果表明，与最先进的生成方法相比，我们的方法在准确性、新颖性和多样性方面取得了显著的改进，为CAD设计、几何重建和3D内容创作提供了一个高效且全面的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce CLR-Wire, a novel framework for 3D curve-based wireframegeneration that integrates geometry and topology into a unified ContinuousLatent Representation. Unlike conventional methods that decouple vertices,edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves alongwith their topological connectivity into a continuous and fixed-length latentspace using an attention-driven variational autoencoder (VAE). This unifiedapproach facilitates joint learning and generation of both geometry andtopology. To generate wireframes, we employ a flow matching model toprogressively map Gaussian noise to these latents, which are subsequentlydecoded into complete 3D wireframes. Our method provides fine-grained modelingof complex shapes and irregular topologies, and supports both unconditionalgeneration and generation conditioned on point cloud or image inputs.Experimental results demonstrate that, compared with state-of-the-artgenerative approaches, our method achieves substantial improvements inaccuracy, novelty, and diversity, offering an efficient and comprehensivesolution for CAD design, geometric reconstruction, and 3D content creation.</description>
      <author>example@mail.com (Xueqi Ma, Yilin Liu, Tianlong Gao, Qirui Huang, Hui Huang)</author>
      <guid isPermaLink="false">2504.19174v2</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Towards Autonomous Micromobility through Scalable Urban Simulation</title>
      <link>http://arxiv.org/abs/2505.00690v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 Highlight. Project page:  https://metadriverse.github.io/urban-sim/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了利用人工智能辅助微移动设备（如配送机器人和电动滑板车）在繁忙的城市环境中提高安全性和效率的方法。&lt;h4&gt;背景&lt;/h4&gt;微移动设备作为一种替代传统车辆移动的方案，目前主要依赖人工操作，这在充满不确定性和行人的城市环境中存在安全隐患和效率问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种可扩展的城市模拟解决方案，以推进自主微移动。&lt;h4&gt;方法&lt;/h4&gt;构建了URBAN-SIM，一个用于在大规模训练具身智能体在交互式城市场景中的高性能机器人学习平台。URBAN-SIM包含三个关键模块：分层城市生成流水线、交互式动力学生成策略和异步场景采样方案。同时，提出了URBAN-BENCH，一套用于评估智能体实现自主微移动能力的基本任务和基准。&lt;h4&gt;主要发现&lt;/h4&gt;URBAN-BENCH包括基于智能体三个核心技能（城市移动、城市导航和城市穿越）的八个任务。实验评估了四种不同具身形式的机器人（如轮式和腿式机器人）在这些任务中的表现，揭示了每种机器人的优势和局限性。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了所提出的方法在提高微移动设备在复杂城市环境中的安全性和效率方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Micromobility, which utilizes lightweight mobile machines moving in urbanpublic spaces, such as delivery robots and mobility scooters, emerges as apromising alternative to vehicular mobility. Current micromobility dependsmostly on human manual operation (in-person or remote control), which raisessafety and efficiency concerns when navigating busy urban environments full ofunpredictable obstacles and pedestrians. Assisting humans with AI agents inmaneuvering micromobility devices presents a viable solution for enhancingsafety and efficiency. In this work, we present a scalable urban simulationsolution to advance autonomous micromobility. First, we build URBAN-SIM - ahigh-performance robot learning platform for large-scale training of embodiedagents in interactive urban scenes. URBAN-SIM contains three critical modules:Hierarchical Urban Generation pipeline, Interactive Dynamics Generationstrategy, and Asynchronous Scene Sampling scheme, to improve the diversity,realism, and efficiency of robot learning in simulation. Then, we proposeURBAN-BENCH - a suite of essential tasks and benchmarks to gauge variouscapabilities of the AI agents in achieving autonomous micromobility.URBAN-BENCH includes eight tasks based on three core skills of the agents:Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robotswith heterogeneous embodiments, such as the wheeled and legged robots, acrossthese tasks. Experiments on diverse terrains and urban structures reveal eachrobot's strengths and limitations.</description>
      <author>example@mail.com (Wayne Wu, Honglin He, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, Bolei Zhou)</author>
      <guid isPermaLink="false">2505.00690v1</guid>
      <pubDate>Fri, 02 May 2025 14:11:45 +0800</pubDate>
    </item>
    <item>
      <title>Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning</title>
      <link>http://arxiv.org/abs/2504.21707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将现代表征学习目标重新定义为局部条件分布上的递归散度对齐过程的方法。&lt;h4&gt;背景&lt;/h4&gt;现有框架如信息对比学习（I-Con）通过固定邻域条件间的KL散度统一了多个学习范式，但忽视了学习过程中的关键递归结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Recursive KLDivergence Optimization（RKDO）的动态公式，将表征学习视为KL散度在数据邻域上的演变。&lt;h4&gt;方法&lt;/h4&gt;RKDO捕捉对比聚类和降维方法作为静态切片，同时提供了一条新的路径以实现模型稳定性和局部适应。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，RKDO提供了双重效率优势：与静态方法相比，在三个不同的数据集上损失值降低约30%，并且所需计算资源减少了60%至80%。&lt;h4&gt;结论&lt;/h4&gt;RKDO的递归更新机制为表征学习提供了一种更有效的优化景观，对资源受限的应用具有重大意义。&lt;h4&gt;翻译&lt;/h4&gt;We propose a generalization of modern representation learning objectives by reframing them as recursive divergence alignment processes over localized conditional distributions. While recent frameworks like Information Contrastive Learning (I-Con) unify multiple learning paradigms through KL divergence between fixed neighborhood conditionals, we argue this view underplays a crucial recursive structure inherent in the learning process. We introduce Recursive KLDivergence Optimization (RKDO), a dynamic formalism where representation learning is framed as the evolution of KL divergences across data neighborhoods. This formulation captures contrastive clustering and dimensionality reduction methods as static slices while offering a new path to model stability and local adaptation. Our experiments demonstrate that RKDO offers dual efficiency advantages: approximately 30 percent lower loss values compared to static approaches across three different datasets and 60 to 80 percent reduction in computational resources needed to achieve comparable results. This suggests that RKDO's recursive updating mechanism provides a fundamentally more efficient optimization landscape for representation learning with significant implications for resource constrained applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/anthonymartin/RKDO-recursive-kl-divergence-optimization&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a generalization of modern representation learning objectives byreframing them as recursive divergence alignment processes over localizedconditional distributions While recent frameworks like Information ContrastiveLearning I-Con unify multiple learning paradigms through KL divergence betweenfixed neighborhood conditionals we argue this view underplays a crucialrecursive structure inherent in the learning process. We introduce Recursive KLDivergence Optimization RKDO a dynamic formalism where representation learningis framed as the evolution of KL divergences across data neighborhoods. Thisformulation captures contrastive clustering and dimensionality reductionmethods as static slices while offering a new path to model stability and localadaptation. Our experiments demonstrate that RKDO offers dual efficiencyadvantages approximately 30 percent lower loss values compared to staticapproaches across three different datasets and 60 to 80 percent reduction incomputational resources needed to achieve comparable results. This suggeststhat RKDOs recursive updating mechanism provides a fundamentally more efficientoptimization landscape for representation learning with significantimplications for resource constrained applications.</description>
      <author>example@mail.com (Anthony D Martin)</author>
      <guid isPermaLink="false">2504.21707v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
  <item>
      <title>REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining</title>
      <link>http://arxiv.org/abs/2504.21699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究传感器退化对自动驾驶的影响，特别是在大雨天气下，如何通过新的数据集和模型来改善LiDAR点云的去雨处理。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统在恶劣天气下可能会因为传感器退化而导致安全问题，如LiDAR点云质量下降。&lt;h4&gt;目的&lt;/h4&gt;发布一个新的、大规模的多模态模拟雨数据集REHEARSE-3D，以促进3D点云去雨研究。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含高分辨率LiDAR数据和4D雷达数据的点云去雨数据集，并利用该数据集进行雨滴检测和去除的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;REHEARSE-3D数据集是最大的点标注数据集，具有高分辨率LiDAR数据，并记录了白天和夜晚的4D雷达点云数据，同时包含了雨特征信息。&lt;h4&gt;结论&lt;/h4&gt;通过REHEARSE-3D数据集，可以评估和比较不同的统计和深度学习模型在融合LiDAR和4D雷达点云的去雨性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传感器退化对自动驾驶构成了重大挑战。在暴雨天气中，雨滴的干扰会严重影响LiDAR点云的质量，导致不准确的点测量。这反过来可能导致安全担忧，如果自动驾驶系统没有意识到这种变化，即如果它们无法识别这种变化。在本研究中，我们发布了一个新的、大规模的多模态模拟雨数据集REHEARSE-3D，以促进3D点云去雨研究。与最相关的竞争对手相比，我们的数据集在几个方面是独特的。首先，它是最大的点标注数据集，其次，它是唯一一个包含高分辨率LiDAR数据（LiDAR-256）的数据集，该数据集在受控的天气环境中记录了白天和夜晚的4D雷达点云。此外，REHEARSE-3D涉及雨特征信息，这对于传感器噪声建模以及分析点级别的天气影响都具有重要意义。利用REHEARSE-3D，我们基准测试了融合LiDAR和4D雷达点云中的雨滴检测和去除。我们的综合研究进一步评估了各种统计和深度学习模型。在发表后，数据集和基准模型将在以下网址公开：https://sporsho.github.io/REHEARSE3D。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sensor degradation poses a significant challenge in autonomous driving.During heavy rainfall, the interference from raindrops can adversely affect thequality of LiDAR point clouds, resulting in, for instance, inaccurate pointmeasurements. This, in turn, can potentially lead to safety concerns ifautonomous driving systems are not weather-aware, i.e., if they are unable todiscern such changes. In this study, we release a new, large-scale, multi-modalemulated rain dataset, REHEARSE-3D, to promote research advancements in 3Dpoint cloud de-raining. Distinct from the most relevant competitors, ourdataset is unique in several respects. First, it is the largest point-wiseannotated dataset, and second, it is the only one with high-resolution LiDARdata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime andnighttime conditions in a controlled weather environment. Furthermore,REHEARSE-3D involves rain-characteristic information, which is of significantvalue not only for sensor noise modeling but also for analyzing the impact ofweather at a point level. Leveraging REHEARSE-3D, we benchmark raindropdetection and removal in fused LiDAR and 4D Radar point clouds. Ourcomprehensive study further evaluates the performance of various statisticaland deep-learning models. Upon publication, the dataset and benchmark modelswill be made publicly available at: https://sporsho.github.io/REHEARSE3D.</description>
      <author>example@mail.com (Abu Mohammed Raisuddin, Jesper Holmblad, Hamed Haghighi, Yuri Poledna, Maikol Funk Drechsler, Valentina Donzella, Eren Erdal Aksoy)</author>
      <guid isPermaLink="false">2504.21699v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</title>
      <link>http://arxiv.org/abs/2504.21716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Austrian Robotics Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于LLM驱动的机器人系统，用于自主家庭物品管理，通过内存增强的任务规划实现机器人执行高级用户命令并跟踪过去的行为。&lt;h4&gt;背景&lt;/h4&gt;现有的家庭物品管理机器人缺乏有效的任务规划和长期物体跟踪能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自主管理家庭物品的机器人系统，并提高其任务规划和记忆召回能力。&lt;h4&gt;方法&lt;/h4&gt;该系统采用LLM驱动的代理编排架构，包括一个路由代理、一个任务规划代理和一个知识库代理，每个代理都由特定于任务的LLM提供支持。系统利用情境学习避免显式模型训练，并利用RAG（关系图模型）从过去的交互中检索上下文，增强长期物体跟踪。结合Grounded SAM和LLaMa3.2-Vision提供鲁棒的物体检测，以促进任务规划中的语义场景理解。&lt;h4&gt;主要发现&lt;/h4&gt;系统在三个家庭场景中的评估显示，任务规划准确率很高，并且由于RAG的引入，记忆召回能力得到了改善。Qwen2.5在专用代理中表现最佳，而LLaMa3.1在路由任务中表现突出。&lt;h4&gt;结论&lt;/h4&gt;该系统有效地提高了家庭物品管理的自主性，并展示了在多个场景中实现高任务规划准确率的能力。&lt;h4&gt;翻译&lt;/h4&gt;We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMa3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an embodied robotic system with an LLM-driven agent-orchestrationarchitecture for autonomous household object management. The system integratesmemory-augmented task planning, enabling robots to execute high-level usercommands while tracking past actions. It employs three specialized agents: arouting agent, a task planning agent, and a knowledge base agent, each poweredby task-specific LLMs. By leveraging in-context learning, our system avoids theneed for explicit model training. RAG enables the system to retrieve contextfrom past interactions, enhancing long-term object tracking. A combination ofGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitatingsemantic scene understanding for task planning. Evaluation across threehousehold scenarios demonstrates high task planning accuracy and an improvementin memory recall due to RAG. Specifically, Qwen2.5 yields best performance forspecialized agents, while LLaMA3.1 excels in routing tasks. The source code isavailable at: https://github.com/marc1198/chat-hsr.</description>
      <author>example@mail.com (Marc Glocker, Peter Hönig, Matthias Hirschmanner, Markus Vincze)</author>
      <guid isPermaLink="false">2504.21716v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>LRBO2: Improved 3D Vision Based Hand-Eye Calibration for Collaborative Robot Arm</title>
      <link>http://arxiv.org/abs/2504.21619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的手眼标定方法，用于协作机器人领域，该方法可以减少对外部标定对象的需求，并实现快速标定。&lt;h4&gt;背景&lt;/h4&gt;手眼标定在协作机器人领域是常见问题，涉及视觉传感器与机器人法兰之间的变换矩阵确定，但传统方法耗时且不便。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需外部标定对象的新方法，以减少标定时间和提高效率。&lt;h4&gt;方法&lt;/h4&gt;改进了LRBO方法，通过生成通用数据集进行点云注册，以对齐机器人底座点云和扫描数据。同时，通过模拟和实际工业环境中的实验进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过模拟14个品牌的机器人臂进行测试，表现与现有商业手眼标定解决方案相当，且标定过程仅需几秒钟。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一个用户友好的手眼标定解决方案，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了在协作机器人领域，一种新的手眼标定方法，通过减少对外部标定对象的需求和缩短标定时间，提高了标定的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-eye calibration is a common problem in the field of collaborativerobotics, involving the determination of the transformation matrix between thevisual sensor and the robot flange to enable vision-based robotic tasks.However, this process typically requires multiple movements of the robot armand an external calibration object, making it both time-consuming andinconvenient, especially in scenarios where frequent recalibration isnecessary. In this work, we extend our previous method, Look at Robot Base Once(LRBO), which eliminates the need for external calibration objects such as achessboard. We propose a generic dataset generation approach for point cloudregistration, focusing on aligning the robot base point cloud with the scanneddata. Furthermore, a more detailed simulation study is conducted involvingseveral different collaborative robot arms, followed by real-world experimentsin an industrial setting. Our improved method is simulated and evaluated usinga total of 14 robotic arms from 9 different brands, including KUKA, UniversalRobots, UFACTORY, and Franka Emika, all of which are widely used in the fieldof collaborative robotics. Physical experiments demonstrate that our extendedapproach achieves performance comparable to existing commercial hand-eyecalibration solutions, while completing the entire calibration procedure injust a few seconds. In addition, we provide a user-friendly hand-eyecalibration solution, with the code publicly available atgithub.com/leihui6/LRBO2.</description>
      <author>example@mail.com (Leihui Li, Lixuepiao Wan, Volker Krueger, Xuping Zhang)</author>
      <guid isPermaLink="false">2504.21619v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Convergence rate for Nearest Neighbour matching: geometry of the domain and higher-order regularity</title>
      <link>http://arxiv.org/abs/2504.21633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了从部分观察数据和缺失结果中估计某些数学期望的问题，特别是在迁移学习、反事实分析或因果推断等领域。论文分析了匹配估计器的性质，特别是其偏差的高阶特性。&lt;h4&gt;背景&lt;/h4&gt;匹配估计器，基于k-近邻的估计器，在上述领域中广泛应用。已知这类估计器的方差可以以参数速率收敛到零，但当协变量的维度大于2时，其偏差可能以较慢的速率收敛，这使得偏差分析尤为重要。&lt;h4&gt;目的&lt;/h4&gt;提供匹配估计器偏差的高阶性质，并分析避免边界偏差问题的几何条件。&lt;h4&gt;方法&lt;/h4&gt;不假设协变量目标分布的支持严格包含在源分布的支持中，分析了两个几何条件，并展示了这些条件比通常的凸支持假设更为一般，从而改进了现有结果。&lt;h4&gt;主要发现&lt;/h4&gt;证明了当协变量的维度小于4时，Abadie和Imbens（2006）研究的匹配估计器对于平均处理效应可以渐近有效，这一结果在维度1时已知。&lt;h4&gt;结论&lt;/h4&gt;本文提供了一种更通用的方法来分析匹配估计器的偏差，并展示了在特定条件下，匹配估计器可以达到渐近有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：从部分观察数据和缺失结果中估计某些数学期望是一个在许多领域（如迁移学习、反事实分析或因果推断）中遇到的核心问题。匹配估计器，基于k-近邻的估计器，在此背景下被广泛使用。已知此类估计器的方差可以以参数速率收敛到零，但当协变量的维度大于2时，其偏差可能以较慢的速率收敛，这使得偏差分析尤为重要。本文提供了偏差的高阶性质。与现有文献相比，本文不假设协变量目标分布的支持严格包含在源分布的支持中，并分析了两个几何条件以避免此类边界偏差问题。本文表明，这些条件比通常的凸支持假设更为一般，从而改进了现有结果。此外，本文表明，当协变量的维度小于4时，Abadie和Imbens（2006）研究的匹配估计器对于平均处理效应可以渐近有效，这一结果在维度1时已知。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating some mathematical expectations from partially observed data and inparticular missing outcomes is a central problem encountered in numerous fieldssuch as transfer learning, counterfactual analysis or causal inference.Matching estimators, estimators based on k-nearest neighbours, are widely usedin this context. It is known that the variance of such estimators can convergeto zero at a parametric rate, but their bias can have a slower rate when thedimension of the covariates is larger than 2. This makes analysis of this biasparticularly important. In this paper, we provide higher order properties ofthe bias. In contrast to the existing literature related to this problem, we donot assume that the support of the target distribution of the covariates isstrictly included in that of the source, and we analyse two geometricconditions on the support that avoid such boundary bias problems. We show thatthese conditions are much more general than the usual convex supportassumption, leading to an improvement of existing results. Furthermore, we showthat the matching estimator studied by Abadie and Imbens (2006) for the averagetreatment effect can be asymptotically efficient when the dimension of thecovariates is less than 4, a result only known in dimension 1.</description>
      <author>example@mail.com (Simon Viel, Lionel Truquet, Ikko Yamane)</author>
      <guid isPermaLink="false">2504.21633v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields</title>
      <link>http://arxiv.org/abs/2504.21814v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了AIGC基础模型在图像压缩领域的应用潜力，通过实验证明了结合结构扫描提示和GPT-4o图像生成功能在超低比特率下的优异性能。&lt;h4&gt;背景&lt;/h4&gt;AIGC基础模型的快速发展改变了图像压缩的范式，使得大部分像素级的转换和编码可以被放弃。&lt;h4&gt;目的&lt;/h4&gt;探究GPT-4o图像生成在图像压缩领域的潜在应用，并比较其与现有图像压缩技术的性能。&lt;h4&gt;方法&lt;/h4&gt;研究了文本编码和多模态编码（即文本+极低分辨率图像）两种压缩范式，通过GPT-4o图像生成功能生成所有/大部分像素级信息，而非压缩。&lt;h4&gt;主要发现&lt;/h4&gt;提出的结构扫描提示机制能够保持解码过程中的语义和结构一致性，实验结果显示在超低比特率下，结合结构扫描提示和GPT-4o的图像生成功能表现出色。&lt;h4&gt;结论&lt;/h4&gt;AIGC生成在图像压缩领域具有巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：AIGC基础模型的高速发展颠覆了图像压缩的范式，为放弃大多数像素级转换和编码铺平了道路，迫使我们思考：如果AIGC基础模型能够从一些紧凑的描述符（如文本或线索）中忠实生成复杂结构和精细细节，那么为什么还要压缩可以生成的内容？幸运的是，OpenAI最近推出的GPT-4o图像生成在跨模态生成、编辑和设计能力方面取得了令人印象深刻的成果，这激励我们通过探索其在图像压缩领域的潜力来回答上述问题。在本研究中，我们调查了两种典型的压缩范式：文本编码和多模态编码（即文本+极低分辨率图像），其中所有/大部分像素级信息是通过GPT-4o图像生成功能生成，而不是通过先进的GPT-4o图像生成功能进行压缩。核心挑战在于如何在解码过程中保持语义和结构一致性。为了克服这一挑战，我们提出了一种结构扫描提示工程机制，将图像转换为文本空间，作为GPT-4o图像生成的条件。大量实验表明，我们设计的结构扫描提示与GPT-4o图像生成功能的结合在超低比特率下实现了令人印象深刻的性能，进一步表明了AIGC生成在图像压缩领域的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of AIGC foundation models has revolutionized theparadigm of image compression, which paves the way for the abandonment of mostpixel-level transform and coding, compelling us to ask: why compress what youcan generate if the AIGC foundation model is powerful enough to faithfullygenerate intricate structure and fine-grained details from nothing more thansome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4oimage generation of OpenAI has achieved impressive cross-modality generation,editing, and design capabilities, which motivates us to answer the abovequestion by exploring its potential in image compression fields. In this work,we investigate two typical compression paradigms: textual coding and multimodalcoding (i.e., text + extremely low-resolution image), where all/mostpixel-level information is generated instead of compressing via the advancedGPT-4o image generation function. The essential challenge lies in how tomaintain semantic and structure consistency during the decoding process. Toovercome this, we propose a structure raster-scan prompt engineering mechanismto transform the image into textual space, which is compressed as the conditionof GPT-4o image generation. Extensive experiments have shown that thecombination of our designed structural raster-scan prompts and GPT-4o's imagegeneration function achieved the impressive performance compared with recentmultimodal/generative image compression at ultra-low bitrate, furtherindicating the potential of AIGC generation in image compression fields.</description>
      <author>example@mail.com (Yixin Gao, Xiaohan Pan, Xin Li, Zhibo Chen)</author>
      <guid isPermaLink="false">2504.21814v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</title>
      <link>http://arxiv.org/abs/2504.21650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project homepage: https://zhouhyocean.github.io/holotime/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HoloTime框架通过集成视频扩散模型和360度4D场景重建方法，能够从单张图片生成全景视频，并转化为4D资产，提供沉浸式4D体验。&lt;h4&gt;背景&lt;/h4&gt;现有的扩散模型主要关注静态3D场景或对象级别的动态，限制了提供真正沉浸式体验的能力。&lt;h4&gt;目的&lt;/h4&gt;提出HoloTime框架，解决现有扩散模型在提供沉浸式体验方面的不足。&lt;h4&gt;方法&lt;/h4&gt;引入360World数据集，提出Panoramic Animator模型将全景图像转换为高质量全景视频，以及Panoramic Space-Time Reconstruction方法将全景视频转换为4D点云。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，HoloTime在全景视频生成和4D场景重建方面表现出优越性，能够创建更吸引人、更逼真的沉浸式环境。&lt;h4&gt;结论&lt;/h4&gt;HoloTime框架能够提升VR和AR应用中的用户体验，实现更加沉浸式的4D体验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of diffusion models holds the promise ofrevolutionizing the application of VR and AR technologies, which typicallyrequire scene-level 4D assets for user experience. Nonetheless, existingdiffusion models predominantly concentrate on modeling static 3D scenes orobject-level dynamics, constraining their capacity to provide truly immersiveexperiences. To address this issue, we propose HoloTime, a framework thatintegrates video diffusion models to generate panoramic videos from a singleprompt or reference image, along with a 360-degree 4D scene reconstructionmethod that seamlessly transforms the generated panoramic video into 4D assets,enabling a fully immersive 4D experience for users. Specifically, to tame videodiffusion models for generating high-fidelity panoramic videos, we introducethe 360World dataset, the first comprehensive collection of panoramic videossuitable for downstream 4D scene reconstruction tasks. With this curateddataset, we propose Panoramic Animator, a two-stage image-to-video diffusionmodel that can convert panoramic images into high-quality panoramic videos.Following this, we present Panoramic Space-Time Reconstruction, which leveragesa space-time depth estimation method to transform the generated panoramicvideos into 4D point clouds, enabling the optimization of a holistic 4DGaussian Splatting representation to reconstruct spatially and temporallyconsistent 4D scenes. To validate the efficacy of our method, we conducted acomparative analysis with existing approaches, revealing its superiority inboth panoramic video generation and 4D scene reconstruction. This demonstratesour method's capability to create more engaging and realistic immersiveenvironments, thereby enhancing user experiences in VR and AR applications.</description>
      <author>example@mail.com (Haiyang Zhou, Wangbo Yu, Jiawen Guan, Xinhua Cheng, Yonghong Tian, Li Yuan)</author>
      <guid isPermaLink="false">2504.21650v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Visual Layer Selection in Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2504.21447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, submitted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多模态大语言模型（MLLMs）中的视觉层选择问题，提出了一种分层表示相似度方法来对CLIP-ViT的层进行分类，并通过实验验证了不同层对MLLM性能的影响。&lt;h4&gt;背景&lt;/h4&gt;现有的MLLMs通常使用CLIP-ViT作为视觉编码器，但不同层捕获的信息类型不同，浅层关注细节，深层与文本语义更接近。目前，大多数MLLMs基于经验选择视觉特征，而非系统分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来对CLIP-ViT层进行分类，并评估其对MLLM性能的影响，从而为视觉表示学习提供原则性的研究。&lt;h4&gt;方法&lt;/h4&gt;提出分层表示相似度方法，对CLIP-ViT层进行分组，并训练了不同参数规模的LLaVA-style模型，通过10个数据集和4个任务的实验来评估不同层对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;发现深层层对OCR任务至关重要；浅层和中层在涉及计数、定位和对象定位的推理任务中显著优于深层层；跨浅层、中层和深层层的轻量级特征融合在10个数据集中有9个实现了性能提升。&lt;h4&gt;结论&lt;/h4&gt;本文首次对MLLM中的视觉层选择进行了原则性研究，为深入探究MLLM的视觉表示学习奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;Multimodal large language models (MLLMs) have achieved impressive performance across a wide range of tasks, typically using CLIP-ViT as their visual encoder due to its strong text-image alignment capabilities. While prior studies suggest that different CLIP-ViT layers capture different types of information, with shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, most MLLMs still select visual features based on empirical heuristics rather than systematic analysis. In this work, we propose a Layer-wise Representation Similarity approach to group CLIP-ViT layers with similar behaviors into {shallow, middle, and deep} categories and assess their impact on MLLM performance. Building on this foundation, we revisit the visual layer selection problem in MLLMs at scale, training LLaVA-style models ranging from 1.4B to 7B parameters. Through extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep layers are essential for OCR tasks; (2) shallow and middle layers substantially outperform deep layers on reasoning tasks involving counting, positioning, and object localization; (3) a lightweight fusion of features across shallow, middle, and deep layers consistently outperforms specialized fusion baselines and single-layer selections, achieving gains on 9 out of 10 datasets. Our work offers the first principled study of visual layer selection in MLLMs, laying the groundwork for deeper investigations into visual representation learning for MLLMs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have achieved impressive performanceacross a wide range of tasks, typically using CLIP-ViT as their visual encoderdue to its strong text-image alignment capabilities. While prior studiessuggest that different CLIP-ViT layers capture different types of information,with shallower layers focusing on fine visual details and deeper layersaligning more closely with textual semantics, most MLLMs still select visualfeatures based on empirical heuristics rather than systematic analysis. In thiswork, we propose a Layer-wise Representation Similarity approach to groupCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}categories and assess their impact on MLLM performance. Building on thisfoundation, we revisit the visual layer selection problem in MLLMs at scale,training LLaVA-style models ranging from 1.4B to 7B parameters. Throughextensive experiments across 10 datasets and 4 tasks, we find that: (1) deeplayers are essential for OCR tasks; (2) shallow and middle layers substantiallyoutperform deep layers on reasoning tasks involving counting, positioning, andobject localization; (3) a lightweight fusion of features across shallow,middle, and deep layers consistently outperforms specialized fusion baselinesand single-layer selections, achieving gains on 9 out of 10 datasets. Our workoffers the first principled study of visual layer selection in MLLMs, layingthe groundwork for deeper investigations into visual representation learningfor MLLMs.</description>
      <author>example@mail.com (Haoran Chen, Junyan Lin, Xinhao Chen, Yue Fan, Xin Jin, Hui Su, Jianfeng Dong, Jinlan Fu, Xiaoyu Shen)</author>
      <guid isPermaLink="false">2504.21447v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Overlapping data in network protocols: bridging OS and NIDS reassembly gap</title>
      <link>http://arxiv.org/abs/2504.21618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了网络入侵检测系统（NIDS）在处理重叠数据块攻击时的抵抗能力，提出了新的分析方法，并分析了操作系统和网络入侵检测系统的重组行为。&lt;h4&gt;背景&lt;/h4&gt;IPv4、IPv6和TCP协议允许将数据包分割成多个块，这些块可能包含重叠的数据部分，操作系统网络堆栈的实现可能会以不同的方式重新组装这些重叠部分。&lt;h4&gt;目的&lt;/h4&gt;研究NIDS在处理重叠数据块攻击时的抵抗能力，并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;1. 扩展了插入和规避攻击的表征，以解决重叠上下文中的局限性。2. 提出使用艾伦的时间区间代数来建模重叠类型。3. 分析了操作系统和网络入侵检测系统在处理建模的重叠测试案例时的重组行为。&lt;h4&gt;主要发现&lt;/h4&gt;1. 操作系统的重组策略会随时间变化。2. 所有测试的网络入侵检测系统都容易受到基于重叠的规避和插入攻击。&lt;h4&gt;结论&lt;/h4&gt;NIDS在处理重叠数据块攻击时存在局限性，需要进一步的研究和改进来提高其安全性。&lt;h4&gt;翻译&lt;/h4&gt;This paper discusses the resistance of Network Intrusion Detection Systems (NIDS) to overlapping data chunk attacks, proposes new analytical methods, and analyzes the reassembly behavior of operating systems and NIDS when processing modeled overlapping test cases.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; IPv4, IPv6, and TCP have a common mechanism allowing one to split an originaldata packet into several chunks. Such chunked packets may have overlapping dataportions and, OS network stack implementations may reassemble these overlapsdifferently. A Network Intrusion Detection System (NIDS) that tries toreassemble a given flow data has to use the same reassembly policy as themonitored host OS; otherwise, the NIDS or the host may be subject to attack. Inthis paper, we provide several contributions that enable us to analyze NIDSresistance to overlapping data chunks-based attacks. First, we extendstate-of-the-art insertion and evasion attack characterizations to addresstheir limitations in an overlap-based context. Second, we propose a new way tomodel overlap types using Allen's interval algebra, a spatio-temporalreasoning. This new modeling allows us to formalize overlap test cases, whichensures exhaustiveness in overlap coverage and eases the reasoning about anduse of reassembly policies. Third, we analyze the reassembly behavior ofseveral OSes and NIDSes when processing the modeled overlap test cases. We showthat 1) OS reassembly policies evolve over time and 2) all the tested NIDSesare (still) vulnerable to overlap-based evasion and insertion attacks.</description>
      <author>example@mail.com (Lucas Aubard, Johan Mazel, Gilles Guette, Pierre Chifflier)</author>
      <guid isPermaLink="false">2504.21618v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2504.21844v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 10 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的异构图神经网络（HGNN）架构，用于解决大型强子对撞机中粒子碰撞事件重建和分析的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着大型强子对撞机中亮度边界的增长，粒子碰撞事件的重建和分析面临挑战，包括数据采集阶段的延迟和存储需求增加，以及背景水平升高和粒子顶点误关联等问题。&lt;h4&gt;目的&lt;/h4&gt;开发更全面和可扩展的重建方法，利用机器学习的最新进展来解决上述问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的HGNN架构，具有不同粒子碰撞关系的独特表示，并集成了图剪枝层以提高可扩展性。该网络在模拟LHCb实验的环境中，采用多任务范式进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;HGNN显著提高了美子介子重建性能，并在单个框架内同时执行粒子顶点关联和图剪枝。该网络量化了重建和剪枝性能，展示了随着事件复杂性的增加而增强的推理时间可扩展性，并使用加权消息传递方案来减轻潜在的性能损失。&lt;h4&gt;结论&lt;/h4&gt;HGNN是一种有效的工具，可以解决大型强子对撞机中粒子碰撞事件重建和分析的挑战，并有望提高实验的准确性和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着大型强子对撞机亮度边界的增长，对粒子碰撞事件的重建和分析提出了挑战。增加的粒子多重性正在考验数据采集阶段的延迟和存储需求，同时出现了新的复杂问题，包括更高的背景水平和更频繁的粒子顶点误关联。这反过来又需要开发更全面和可扩展的重建方法，这些方法可以利用机器学习的最新进展。我们提出了一种新的异构图神经网络（HGNN）架构，具有不同粒子碰撞关系的独特表示，并集成了图剪枝层以提高可扩展性。在模拟LHCb实验的环境中，采用多任务范式进行训练的该HGNN显著提高了美子介子重建性能。值得注意的是，它同时在单个框架内执行粒子顶点关联和图剪枝。我们量化了重建和剪枝性能，展示了随着事件复杂性的增加而增强的推理时间可扩展性，并使用加权消息传递方案来减轻潜在的性能损失。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing luminosity frontier at the Large Hadron Collider is challengingthe reconstruction and analysis of particle collision events. Increasedparticle multiplicities are straining latency and storage requirements at thedata acquisition stage, while new complications are emerging, including higherbackground levels and more frequent particle vertex misassociations. This inturn necessitates the development of more holistic and scalable reconstructionmethods that take advantage of recent advances in machine learning. We proposea novel Heterogeneous Graph Neural Network (HGNN) architecture featuring uniquerepresentations for diverse particle collision relationships and integratedgraph pruning layers for scalability. Trained with a multi-task paradigm in anenvironment mimicking the LHCb experiment, this HGNN significantly improvesbeauty hadron reconstruction performance. Notably, it concurrently performsparticle vertex association and graph pruning within a single framework. Wequantify reconstruction and pruning performance, demonstrate enhanced inferencetime scaling with event complexity, and mitigate potential performance lossusing a weighted message passing scheme.</description>
      <author>example@mail.com (William Sutcliffe, Marta Calvi, Simone Capelli, Jonas Eschle, Julián García Pardiñas, Abhijit Mathad, Azusa Uzuki, Nicola Serra)</author>
      <guid isPermaLink="false">2504.21844v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space</title>
      <link>http://arxiv.org/abs/2504.21467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 19 figures, IEEE Transactions on Image Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为POLAR的多视角点云刚性配准方法，该方法能够高效处理大量视图，同时对高水平的退化和大初始角度具有鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;点云刚性配准是3D计算机视觉中的一个基本问题，现有方法在处理大量视图和高水平退化时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;旨在寻找一种能够高效处理大量视图，同时鲁棒于高水平退化和大初始角度的多视角点云刚性配准方法。&lt;h4&gt;方法&lt;/h4&gt;将配准问题转化为预训练自动编码器的潜在空间，设计考虑退化的损失函数，并开发了一种高效的多次启动优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;POLAR在合成和真实数据上显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;POLAR是一种有效的多视角点云刚性配准方法，可通过github.com/pypolar/polar或pip install polaregistration进行获取和使用。&lt;h4&gt;翻译&lt;/h4&gt;点云刚性配准是多维计算机视觉中的基本问题。在多视角情况下，我们旨在找到一组6D姿态来对齐一组对象。基于成对配准的方法依赖于后续同步算法，这使得它们随着视图数量的增加而扩展性较差。生成方法克服了这一限制，但基于高斯混合模型并使用期望最大化算法，因此它们不适合处理大变换。此外，大多数现有方法无法处理高水平的退化。在本文中，我们引入了POLAR（POint cloud LAtent Registration），这是一种多视角配准方法，能够有效地处理大量视图，同时对高水平的退化和大的初始角度具有鲁棒性。为了实现这一点，我们将配准问题转化为预训练自动编码器的潜在空间，设计了一个考虑退化的损失函数，并开发了一种高效的多次启动优化策略。我们提出的方法在合成和真实数据上显著优于现有方法。POLAR可在github.com/pypolar/polar或作为独立的包通过pip install polaregistration安装。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TIP.2025.3565998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud rigid registration is a fundamental problem in 3D computervision. In the multiview case, we aim to find a set of 6D poses to align a setof objects. Methods based on pairwise registration rely on a subsequentsynchronization algorithm, which makes them poorly scalable with the number ofviews. Generative approaches overcome this limitation, but are based onGaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,they are not well suited to handle large transformations. Moreover, mostexisting methods cannot handle high levels of degradations. In this paper, weintroduce POLAR (POint cloud LAtent Registration), a multiview registrationmethod able to efficiently deal with a large number of views, while beingrobust to a high level of degradations and large initial angles. To achievethis, we transpose the registration problem into the latent space of apretrained autoencoder, design a loss taking degradations into account, anddevelop an efficient multistart optimization strategy. Our proposed methodsignificantly outperforms state-of-the-art approaches on synthetic and realdata. POLAR is available at github.com/pypolar/polar or as a standalone packagewhich can be installed with pip install polaregistration.</description>
      <author>example@mail.com (Luc Vedrenne, Sylvain Faisan, Denis Fortun)</author>
      <guid isPermaLink="false">2504.21467v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality</title>
      <link>http://arxiv.org/abs/2504.21360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ImaginateAR，一个基于AI的移动AR内容创作系统，旨在让非专业人士通过语音输入实现AR内容的创作。&lt;h4&gt;背景&lt;/h4&gt;当前AR内容的创作对于非专业人士来说较为困难，通常需要专业工具和时间。&lt;h4&gt;目的&lt;/h4&gt;ImaginateAR的目标是让任何人都能在任何地方通过语音表达他们的想象力来构建AR内容。&lt;h4&gt;方法&lt;/h4&gt;ImaginateAR采用离线场景理解、快速3D资产生成和基于LLM的语音交互技术。&lt;h4&gt;主要发现&lt;/h4&gt;技术评估表明，ImaginateAR产生的户外场景图更准确，并且比先前的方法生成3D网格更快。用户研究揭示了AI在创作中的作用、用户在自由创作中创造的内容以及未来AR创作工具的设计启示。&lt;h4&gt;结论&lt;/h4&gt;ImaginateAR为非专业人士提供了更便捷的AR内容创作方式，并揭示了AI在AR内容创作中的重要作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While augmented reality (AR) enables new ways to play, tell stories, andexplore ideas rooted in the physical world, authoring personalized AR contentremains difficult for non-experts, often requiring professional tools and time.Prior systems have explored AI-driven XR design but typically rely onmanually-defined environments and fixed asset libraries, limiting creativeflexibility and real-world relevance. We introduce ImaginateAR, a mobileAI-assisted AR authoring system that aims to let anyone build anything,anywhere -- simply by speaking their imagination. ImaginateAR is powered bycustom pipelines for offline scene understanding, fast 3D asset generation, andLLM-driven speech interaction. Users might say "a dragon enjoying a campfire"(P7) and iteratively refine the scene using both AI and manual tools. Ourtechnical evaluation shows that ImaginateAR produces more accurate outdoorscene graphs and generates 3D meshes faster than prior methods. A three-partuser study (N=20) revealed preferred roles for AI in authoring, what and howusers create in free-form use, and design implications for future AR authoringtools.</description>
      <author>example@mail.com (Jaewook Lee, Filippo Aleotti, Diego Mazala, Guillermo Garcia-Hernando, Sara Vicente, Oliver James Johnston, Isabel Kraus-Liang, Jakub Powierza, Donghoon Shin, Jon E. Froehlich, Gabriel Brostow, Jessica Van Brummelen)</author>
      <guid isPermaLink="false">2504.21360v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>MAGNET: an open-source library for mesh agglomeration by Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2504.21780v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MAGNET是一个开源的Python库，用于二维和三维网格的聚合，基于图神经网络（GNN）。&lt;h4&gt;背景&lt;/h4&gt;MAGNET旨在为训练各种GNN模型提供全面解决方案，结合深度学习和其他高级算法如METIS和k-means，以促进网格聚合和质量指标计算。&lt;h4&gt;目的&lt;/h4&gt;MAGNET旨在提高网格聚合和GNN模型训练的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;MAGNET采用图二分法，利用SAGE卷积层利用连通性和几何网格信息，与Antonietti等人（2024）提出的方法一致。此外，MAGNET结合了强化学习来提高模型预测粗划分的准确性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;MAGNET通过提供详细的教程，指导用户进行网格聚合和GNN二分模型的训练。MAGNET在多种场景中的应用得到展示，其性能与METIS和k-means相比，在划分质量和计算效率方面具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;MAGNET的接口具有多功能性，通过集成Lymph库（一个实现多物理场微分问题数值离散化的开源库）展示了其接口的灵活性。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个名为MAGNET的开源Python库，该库旨在用于二维和三维网格的聚合，基于图神经网络（GNN）。MAGNET作为训练各种GNN模型的全面解决方案，结合深度学习和其他高级算法，如METIS和k-means，以促进网格聚合和质量指标计算。通过其代码结构和主要功能概述了库的引入。GNN框架采用图二分法，通过SAGE卷积层利用连通性和几何网格信息，与Antonietti等人（2024）提出的方法一致。此外，提出的MAGNET库结合了强化学习，以提高模型在多级框架内预测粗划分的准确性和鲁棒性。提供了一个详细的教程，指导用户通过网格聚合和GNN二分模型的训练过程。我们展示了MAGNET执行的网格聚合的几个示例，证明了该库在各种场景中的应用。此外，新引入的模型性能与METIS和k-means进行了对比，表明所提出的GNN模型在划分质量和计算效率方面具有竞争力。最后，通过其与Lymph库的集成展示了MAGNET接口的灵活性，Lymph是一个在多顶点网格上实现不连续伽辽金方法的开源库，用于多物理场微分问题的数值离散化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MAGNET, an open-source Python library designed for meshagglomeration in both two- and three-dimensions, based on employing GraphNeural Networks (GNN). MAGNET serves as a comprehensive solution for training avariety of GNN models, integrating deep learning and other advanced algorithmssuch as METIS and k-means to facilitate mesh agglomeration and quality metriccomputation. The library's introduction is outlined through its code structureand primary features. The GNN framework adopts a graph bisection methodologythat capitalizes on connectivity and geometric mesh information via SAGEconvolutional layers, in line with the methodology proposed by Antonietti etal. (2024). Additionally, the proposed MAGNET library incorporatesreinforcement learning to enhance the accuracy and robustness of the model forpredicting coarse partitions within a multilevel framework. A detailed tutorialis provided to guide the user through the process of mesh agglomeration and thetraining of a GNN bisection model. We present several examples of meshagglomeration conducted by MAGNET, demonstrating the library's applicabilityacross various scenarios. Furthermore, the performance of the newly introducedmodels is contrasted with that of METIS and k-means, illustrating that theproposed GNN models are competitive regarding partition quality andcomputational efficiency. Finally, we exhibit the versatility of MAGNET'sinterface through its integration with Lymph, an open-source libraryimplementing discontinuous Galerkin methods on polytopal grids for thenumerical discretization of multiphysics differential problems.</description>
      <author>example@mail.com (Paola F. Antonietti, Matteo Caldana, Ilario Mazzieri, Andrea Re Fraschini)</author>
      <guid isPermaLink="false">2504.21780v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Robust Orthogonal NMF with Label Propagation for Image Clustering</title>
      <link>http://arxiv.org/abs/2504.21472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为RONMF（鲁棒正交非负矩阵分解）的新方法，用于图像聚类，旨在克服现有NMF方法对噪声敏感和无法有效利用有限监督信息的缺点。&lt;h4&gt;背景&lt;/h4&gt;NMF在图像聚类中是一种流行的无监督学习方法，但在实际应用中，大多数现有方法对噪声污染敏感，且无法有效利用有限的监督信息。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些缺点，提出了一种名为RONMF的统一非凸框架，结合标签传播，以提高鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;RONMF方法不仅考虑了图拉普拉斯和标签传播作为正则化项，还引入了一种更有效的非凸结构来衡量重建误差，并对基矩阵施加正交约束以减少噪声污染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在八个公共图像数据集上，所提出的RONMF在各种标准指标上优于最先进的NMF方法，并显示出卓越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;RONMF是一种有效且鲁棒的图像聚类方法，能够处理噪声污染并有效利用有限的监督信息。&lt;h4&gt;翻译&lt;/h4&gt;摘要：非负矩阵分解（NMF）是一种流行的无监督学习方法，广泛应用于图像聚类。然而，在现实世界的聚类场景中，大多数现有的NMF方法对噪声污染非常敏感，并且无法有效地利用有限的监督信息。为了克服这些缺点，我们提出了一种名为鲁棒正交非负矩阵分解（RONMF）的统一非凸框架，称为标签传播。该方法不仅将图拉普拉斯和标签传播作为正则化项，还引入了一种更有效的非凸结构来衡量重建误差，并对基矩阵施加正交约束以减少噪声污染，从而提高了鲁棒性。为了解决RONMF，我们开发了一种基于交替方向乘子法（ADMM）的优化算法。特别是，所有子问题都有封闭形式的解，这确保了其效率。在八个公共图像数据集上的实验评估表明，所提出的RONMF在各种标准指标上优于最先进的NMF方法，并显示出卓越的鲁棒性。代码可在https://github.com/slinda-liu上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-negative matrix factorization (NMF) is a popular unsupervised learningapproach widely used in image clustering. However, in real-world clusteringscenarios, most existing NMF methods are highly sensitive to noise corruptionand are unable to effectively leverage limited supervised information. Toovercome these drawbacks, we propose a unified non-convex framework with labelpropagation called robust orthogonal nonnegative matrix factorization (RONMF).This method not only considers the graph Laplacian and label propagation asregularization terms but also introduces a more effective non-convex structureto measure the reconstruction error and imposes orthogonal constraints on thebasis matrix to reduce the noise corruption, thereby achieving higherrobustness. To solve RONMF, we develop an alternating direction method ofmultipliers (ADMM)-based optimization algorithm. In particular, all subproblemshave closed-form solutions, which ensures its efficiency. Experimentalevaluations on eight public image datasets demonstrate that the proposed RONMFoutperforms state-of-the-art NMF methods across various standard metrics andshows excellent robustness. The code will be available athttps://github.com/slinda-liu.</description>
      <author>example@mail.com (Jingjing Liu, Nian Wu, Xianchao Xiu, Jianhua Zhang)</author>
      <guid isPermaLink="false">2504.21472v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>A simple and effective approach for body part recognition on CT scans based on projection estimation</title>
      <link>http://arxiv.org/abs/2504.21810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于2D X射线类似估计的3D CT扫描体区域识别方法，旨在通过简化标注过程提高医学数据集的质量。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型需要大量标注数据才能获得最佳性能，而CT数据标注由于其体积性和缺失或不完整的元数据而特别具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;通过估计2D图像来识别14个不同的身体区域，为构建高质量的医学数据集提供有价值的信息。&lt;h4&gt;方法&lt;/h4&gt;本研究提出了一种基于2D X射线类似估计的3D CT扫描体区域识别方法，并与2.5D、3D和基础模型（MI2）方法进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在统计显著性方面优于其他方法，其最佳模型EffNet-B0的F1-Score为0.980 ± 0.016，而其他方法的F1-Score分别为0.840 ± 0.114（2.5D DenseNet-161）、0.854 ± 0.096（3D VoxCNN）和0.852 ± 0.104（MI2基础模型）。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的基于2D X射线类似估计的3D CT扫描体区域识别方法在医学数据集构建中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;众所周知，机器学习模型需要大量标注数据以获得最佳性能。由于CT数据的体积性以及通常缺失或不完整的关联元数据，标注CT数据可能是一项特别具有挑战性的任务。即使是检查一张CT扫描，也需要额外的计算机软件，或者在编程语言的情况下，还需要额外的编程库。本研究提出了一种简单但有效的方法，基于2D X射线类似估计的3D CT扫描体区域识别。尽管体区域通常与CT扫描相关联，但它通常只描述了主要关注的体区域，而忽略了观察到的CT中存在的其他解剖区域。在所提出的方法中，利用估计的2D图像来识别14个不同的身体区域，为构建高质量医学数据集提供了有价值的信息。为了评估所提出方法的有效性，将其与2.5D、3D和基础模型（MI2）方法进行了比较。我们的方法优于其他方法，其中以统计显著性和最佳性能模型EffNet-B0的F1-Score为0.980 ± 0.016，相比之下，其他方法的F1-Score分别为0.840 ± 0.114（2.5D DenseNet-161）、0.854 ± 0.096（3D VoxCNN）和0.852 ± 0.104（MI2基础模型）。所使用的数据集由三个不同的临床中心组成，包括15,622张CT扫描（44,135个标签）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; It is well known that machine learning models require a high amount ofannotated data to obtain optimal performance. Labelling Computed Tomography(CT) data can be a particularly challenging task due to its volumetric natureand often missing and$/$or incomplete associated meta-data. Even inspecting oneCT scan requires additional computer software, or in the case of programminglanguages $-$ additional programming libraries. This study proposes a simple,yet effective approach based on 2D X-ray-like estimation of 3D CT scans forbody region identification. Although body region is commonly associated withthe CT scan, it often describes only the focused major body region neglectingother anatomical regions present in the observed CT. In the proposed approach,estimated 2D images were utilized to identify 14 distinct body regions,providing valuable information for constructing a high-quality medical dataset.To evaluate the effectiveness of the proposed method, it was compared against2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformedthe others, where it came on top with statistical significance and F1-Score forthe best-performing model EffNet-B0 of 0.980 $\pm$ 0.016 in comparison to the0.840 $\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\pm$ 0.096 (3D VoxCNN), and 0.852$\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised threedifferent clinical centers and counted 15,622 CT scans (44,135 labels).</description>
      <author>example@mail.com (Franko Hrzic, Mohammadreza Movahhedi, Ophelie Lavoie-Gagne, Ata Kiapour)</author>
      <guid isPermaLink="false">2504.21810v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2504.21254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ABG-NAS的自动图神经网络架构搜索框架，旨在提高图表示学习的效率和效果。&lt;h4&gt;背景&lt;/h4&gt;现有的图神经网络架构难以适应多样化的复杂图结构，限制了其提供稳健和可泛化表示的能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，提出ABG-NAS框架，以实现高效的图表示学习。&lt;h4&gt;方法&lt;/h4&gt;ABG-NAS包含三个关键组件：全面架构搜索空间（CASS）、自适应遗传优化策略（AGOS）和贝叶斯引导调优模块（BGTM）。CASS系统地探索了不同的传播和转换操作，AGOS动态平衡探索和利用，BGTM定期优化超参数。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的实验表明，ABG-NAS在性能上优于手动设计的GNN和最先进的神经架构搜索（NAS）方法。&lt;h4&gt;结论&lt;/h4&gt;ABG-NAS通过提供可扩展和自适应的解决方案，有望推动图表示学习的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：有效的图表示学习对于实现关键下游任务（如节点分类、链接预测和子图搜索）至关重要。然而，现有的图神经网络（GNN）架构往往难以适应多样化的复杂图结构，限制了它们提供稳健和可泛化表示的能力。为了应对这一挑战，我们提出了ABG-NAS，这是一种针对高效图表示学习的自动图神经网络架构搜索的新框架。ABG-NAS包含三个关键组件：一个全面的架构搜索空间（CASS）、一个自适应遗传优化策略（AGOS）和一个贝叶斯引导调优模块（BGTM）。CASS系统地探索了不同的传播（P）和转换（T）操作，使得能够发现能够捕捉复杂图特征的GNN架构。AGOS动态平衡探索和利用，确保搜索效率并保持解决方案的多样性。BGTM进一步定期优化超参数，增强了结果的架构的可扩展性和鲁棒性。在基准数据集（Cora、PubMed、Citeseer和CoraFull）上的实证评估表明，ABG-NAS在性能上始终优于手动设计的GNN和最先进的神经架构搜索（NAS）方法。这些结果突出了ABG-NAS通过为多样化的图结构提供可扩展和自适应的解决方案，在推进图表示学习方面的潜力。我们的代码在https://github.com/sserranw/ABG-NAS上公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective and efficient graph representation learning is essential forenabling critical downstream tasks, such as node classification, linkprediction, and subgraph search. However, existing graph neural network (GNN)architectures often struggle to adapt to diverse and complex graph structures,limiting their ability to provide robust and generalizable representations. Toaddress this challenge, we propose ABG-NAS, a novel framework for automatedgraph neural network architecture search tailored for efficient graphrepresentation learning. ABG-NAS encompasses three key components: aComprehensive Architecture Search Space (CASS), an Adaptive GeneticOptimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASSsystematically explores diverse propagation (P) and transformation (T)operations, enabling the discovery of GNN architectures capable of capturingintricate graph characteristics. AGOS dynamically balances exploration andexploitation, ensuring search efficiency and preserving solution diversity.BGTM further optimizes hyperparameters periodically, enhancing the scalabilityand robustness of the resulting architectures. Empirical evaluations onbenchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate thatABG-NAS consistently outperforms both manually designed GNNs andstate-of-the-art neural architecture search (NAS) methods. These resultshighlight the potential of ABG-NAS to advance graph representation learning byproviding scalable and adaptive solutions for diverse graph structures. Ourcode is publicly available at https://github.com/sserranw/ABG-NAS.</description>
      <author>example@mail.com (Sixuan Wang, Jiao Yin, Jinli Cao, MingJian Tang, Hua Wang, Yanchun Zhang)</author>
      <guid isPermaLink="false">2504.21254v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Path Planning on Multi-level Point Cloud with a Weighted Traversability Graph</title>
      <link>http://arxiv.org/abs/2504.21622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对多层次地形情况的新路径规划方法，该方法在三个层面进行了创新。&lt;h4&gt;背景&lt;/h4&gt;路径规划在复杂地形中的有效性是研究的关键。&lt;h4&gt;目的&lt;/h4&gt;解决多层次地形情况下的路径规划问题。&lt;h4&gt;方法&lt;/h4&gt;1) 使用多级跳表结构和数据精简算法预处理点云地图，以实现地图的有序化和简化；2) 通过车辆和点云的交互分析直接获取局部可通行性指数，以节省表面拟合工作；3) 在多层次连通图上分配可通行性指数，生成加权可通行性图，用于基于搜索的路径规划；4) 修改A*算法以利用可通行性图生成短且安全的路径。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过室内和室外实验在各种环境中验证了其有效性和可靠性，包括多层建筑、林地和崎岖山区。&lt;h4&gt;结论&lt;/h4&gt;该方法可以有效地解决地面车辆在广泛情况下的3D路径规划问题。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的路径规划方法来解决多层次地形情况。该方法在三个方面进行了创新：1) 使用多级跳表结构和数据精简算法对点云地图进行预处理，以实现地图的有序化和简化；2) 通过车辆和点云的交互分析直接获取局部可通行性指数，从而节省表面拟合工作；3) 在多层次连通图上分配可通行性指数，生成加权可通行性图，用于基于搜索的路径规划；4) 修改A*算法以利用可通行性图生成短且安全的路径。通过在多种环境中的室内和室外实验验证了该方法的有效性和可靠性，包括多层建筑、林地和崎岖山区。结果表明，该方法可以有效地解决地面车辆在广泛情况下的3D路径规划问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article proposes a new path planning method for addressing multi-levelterrain situations. The proposed method includes innovations in three aspects:1) the pre-processing of point cloud maps with a multi-level skip-liststructure and data-slimming algorithm for well-organized and simplified mapformalization and management, 2) the direct acquisition of local traversabilityindexes through vehicle and point cloud interaction analysis, which saves workin surface fitting, and 3) the assignment of traversability indexes on amulti-level connectivity graph to generate a weighted traversability graph forgenerally search-based path planning. The A* algorithm is modified to utilizethe traversability graph to generate a short and safe path. The effectivenessand reliability of the proposed method are verified through indoor and outdoorexperiments conducted in various environments, including multi-floor buildings,woodland, and rugged mountainous regions. The results demonstrate that theproposed method can properly address 3D path planning problems for groundvehicles in a wide range of situations.</description>
      <author>example@mail.com (Yujie Tang, Quan Li, Hao Geng, Yangmin Xie, Hang Shi, Yusheng Yang)</author>
      <guid isPermaLink="false">2504.21622v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning</title>
      <link>http://arxiv.org/abs/2504.21375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Multi-modal, Multi-modal Representation Learning, Missing Modality,  Missing Modality Reconstruction, Speech and Multi-modality, Vision and  Language&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Synergy-CLIP的新框架，用于增强多模态表示学习，通过整合视觉、文本和音频模态，并验证了其在多种下游任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;多模态表示学习在人工智能领域变得至关重要，但现有方法主要关注双模态交互，且在等规模环境中整合模态的研究尚不充分。&lt;h4&gt;目的&lt;/h4&gt;提出Synergy-CLIP框架，以增强多模态表示学习，并解决构建大规模平衡数据集的挑战。&lt;h4&gt;方法&lt;/h4&gt;Synergy-CLIP扩展了CLIP架构，通过整合视觉、文本和音频模态，并引入了VGG-sound+数据集，以提供等规模的多模态数据表示。&lt;h4&gt;主要发现&lt;/h4&gt;Synergy-CLIP在零样本分类等任务中优于现有基线，并展示了其在实际应用场景中提取模态间协同作用的能力。&lt;h4&gt;结论&lt;/h4&gt;Synergy-CLIP为多模态表示学习的进步和新的研究方向提供了坚实的基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态表示学习已成为人工智能领域的关键领域，它使视觉、文本和音频等不同模态的集成成为可能，以解决复杂问题。然而，现有方法主要关注双模态交互，如图像-文本对，这限制了它们充分利用多模态数据丰富性的能力。此外，由于构建大规模平衡数据集的挑战，等规模环境中模态的整合仍被探索不足。在本研究中，我们提出了Synergy-CLIP，这是一个新颖的框架，它扩展了对比语言-图像预训练（CLIP）架构，通过整合视觉、文本和音频模态来增强多模态表示学习。与现有方法不同，Synergy-CLIP通过平等地对齐和捕获三个模态的潜在信息。为了解决构建大规模多模态数据集的高成本，我们引入了VGG-sound+数据集，该数据集旨在提供视觉、文本和音频数据的等规模表示。Synergy-CLIP在各种下游任务中得到了验证，包括零样本分类，其中它优于现有基线。此外，我们引入了一个缺失模态重建任务，展示了Synergy-CLIP在现实应用场景中提取模态间协同作用的能力。这些贡献为多模态表示学习的进步和新的研究方向提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal representation learning has become a pivotal area in artificialintelligence, enabling the integration of diverse modalities such as vision,text, and audio to solve complex problems. However, existing approachespredominantly focus on bimodal interactions, such as image-text pairs, whichlimits their ability to fully exploit the richness of multi-modal data.Furthermore, the integration of modalities in equal-scale environments remainsunderexplored due to the challenges of constructing large-scale, balanceddatasets. In this study, we propose Synergy-CLIP, a novel framework thatextends the contrastive language-image pre-training (CLIP) architecture toenhance multi-modal representation learning by integrating visual, textual, andaudio modalities. Unlike existing methods that focus on adapting individualmodalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent informationacross three modalities equally. To address the high cost of constructinglarge-scale multi-modal datasets, we introduce VGG-sound+, a triple-modaldataset designed to provide equal-scale representation of visual, textual, andaudio data. Synergy-CLIP is validated on various downstream tasks, includingzero-shot classification, where it outperforms existing baselines.Additionally, we introduce a missing modality reconstruction task,demonstrating Synergy-CLIP's ability to extract synergy among modalities inrealistic application scenarios. These contributions provide a robustfoundation for advancing multi-modal representation learning and exploring newresearch directions.</description>
      <author>example@mail.com (Sangyeon Cho, Jangyeong Jeon, Mingi Kim, Junyeong Kim)</author>
      <guid isPermaLink="false">2504.21375v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
      <link>http://arxiv.org/abs/2504.21435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 15 figures, CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的视频理解基准SeriesBench，用于评估多模态大型语言模型在理解叙事驱动系列视频方面的能力。&lt;h4&gt;背景&lt;/h4&gt;现有的视频理解基准主要关注独立视频，评估视觉元素如人类动作和物体状态，而现实中的视频往往包含复杂连续的叙事，通常以系列形式呈现。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，提出SeriesBench，包含105个精心挑选的叙事驱动系列，涵盖28个需要深度叙事理解的专项任务。&lt;h4&gt;方法&lt;/h4&gt;首先选择涵盖各种类型的戏剧系列，然后引入一种新的长跨度叙事标注方法，结合全信息转换方法将人工标注转换为不同的任务格式。此外，提出了一种新的叙事推理框架PC-DCoT，以增强模型对剧情结构和人物关系分析的详细分析能力。&lt;h4&gt;主要发现&lt;/h4&gt;在SeriesBench上的广泛结果表明，现有的MLLMs在理解叙事驱动系列视频方面仍面临重大挑战，而PC-DCoT使这些MLLMs能够实现性能提升。&lt;h4&gt;结论&lt;/h4&gt;SeriesBench和PC-DCoT强调了提高模型理解叙事驱动系列视频能力的重要性，为MLLMs的未来发展提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus onextbf{standalone} videos and mainly assess ``visual elements'' like humanactions and object states. In reality, contemporary videos often encompasscomplex and continuous narratives, typically presented as a extbf{series}. Toaddress this challenge, we propose extbf{SeriesBench}, a benchmark consistingof 105 carefully curated narrative-driven series, covering 28 specialized tasksthat require deep narrative understanding. Specifically, we first select adiverse set of drama series spanning various genres. Then, we introduce a novellong-span narrative annotation method, combined with a full-informationtransformation approach to convert manual annotations into diverse taskformats. To further enhance model capacity for detailed analysis of plotstructures and character relationships within series, we propose a novelnarrative reasoning framework, extbf{PC-DCoT}. Extensive results onextbf{SeriesBench} indicate that existing MLLMs still face significantchallenges in understanding narrative-driven series, while extbf{PC-DCoT}enables these MLLMs to achieve performance improvements. Overall, ourextbf{SeriesBench} and extbf{PC-DCoT} highlight the critical necessity ofadvancing model capabilities to understand narrative-driven series, guiding thefuture development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of Multi-modal Large Language Models (MLLMs), anincreasing number of benchmarks have been established to evaluate the videounderstanding capabilities of these models. However, these benchmarks focus on\textbf{standalone} videos and mainly assess ``visual elements'' like humanactions and object states. In reality, contemporary videos often encompasscomplex and continuous narratives, typically presented as a \textbf{series}. Toaddress this challenge, we propose \textbf{SeriesBench}, a benchmark consistingof 105 carefully curated narrative-driven series, covering 28 specialized tasksthat require deep narrative understanding. Specifically, we first select adiverse set of drama series spanning various genres. Then, we introduce a novellong-span narrative annotation method, combined with a full-informationtransformation approach to convert manual annotations into diverse taskformats. To further enhance model capacity for detailed analysis of plotstructures and character relationships within series, we propose a novelnarrative reasoning framework, \textbf{PC-DCoT}. Extensive results on\textbf{SeriesBench} indicate that existing MLLMs still face significantchallenges in understanding narrative-driven series, while \textbf{PC-DCoT}enables these MLLMs to achieve performance improvements. Overall, our\textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity ofadvancing model capabilities to understand narrative-driven series, guiding thefuture development of MLLMs. SeriesBench is publicly available athttps://github.com/zackhxn/SeriesBench-CVPR2025.</description>
      <author>example@mail.com (Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, ShaoGuo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2504.21435v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Phi-4-reasoning Technical Report</title>
      <link>http://arxiv.org/abs/2504.21318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为Phi-4-reasoning的推理模型，在复杂推理任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;Phi-4-reasoning是一个拥有140亿参数的推理模型，它在推理任务上取得了显著成绩。&lt;h4&gt;目的&lt;/h4&gt;开发Phi-4-reasoning模型和Phi-4-reasoning-plus变体，以实现更高的推理性能。&lt;h4&gt;方法&lt;/h4&gt;通过在精心挑选的“可教”提示集上对Phi-4进行监督微调，并使用o3-mini生成推理演示来训练模型。Phi-4-reasoning-plus通过基于结果的强化学习进一步优化。&lt;h4&gt;主要发现&lt;/h4&gt;Phi-4-reasoning和Phi-4-reasoning-plus在数学、科学推理、编码、算法问题解决、规划和空间理解等多个推理任务上优于更大的开放模型，并接近DeepSeek-R1模型的表现。&lt;h4&gt;结论&lt;/h4&gt;精心数据整理对监督微调有益，且可以通过强化学习进一步增强。评估表明，有改进推理模型性能和鲁棒性的评估方法的机会。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种名为Phi-4-reasoning的推理模型，它是一个拥有140亿参数的推理模型，在复杂推理任务上取得了显著成绩。通过在精心挑选的“可教”提示集上对Phi-4进行监督微调，并使用o3-mini生成推理演示来训练模型。Phi-4-reasoning-plus通过基于结果的强化学习进一步优化。在数学、科学推理、编码、算法问题解决、规划和空间理解等多个推理任务上，这两个模型都优于更大的开放模型，如DeepSeek-R1-Distill-Llama-70B模型，并接近DeepSeek-R1模型的表现。我们的全面评估涵盖了这些基准测试。有趣的是，我们还观察到这些改进对通用基准测试也有非平凡的迁移。在本报告中，我们提供了关于我们的训练数据、训练方法和评估的见解。我们表明，精心数据整理对监督微调有益，并且可以通过强化学习进一步增强。最后，我们的评估指出了改进推理模型性能和鲁棒性的评估方法的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Phi-4-reasoning, a 14-billion parameter reasoning model thatachieves strong performance on complex reasoning tasks. Trained via supervisedfine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selectedfor the right level of complexity and diversity-and reasoning demonstrationsgenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chainsthat effectively leverage inference-time compute. We further developPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-basedreinforcement learning that offers higher performance by generating longerreasoning traces. Across a wide range of reasoning tasks, both modelsoutperform significantly larger open-weight models such asDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of fullDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math andscientific reasoning, coding, algorithmic problem solving, planning, andspatial understanding. Interestingly, we observe a non-trivial transfer ofimprovements to general-purpose benchmarks as well. In this report, we provideinsights into our training data, our training methodologies, and ourevaluations. We show that the benefit of careful data curation for supervisedfine-tuning (SFT) extends to reasoning language models, and can be furtheramplified by reinforcement learning (RL). Finally, our evaluation points toopportunities for improving how we assess the performance and robustness ofreasoning models.</description>
      <author>example@mail.com (Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng)</author>
      <guid isPermaLink="false">2504.21318v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image</title>
      <link>http://arxiv.org/abs/2504.21281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Mamba模型的三维医学图像肿瘤分割方法，该方法结合了特征提取和自适应多级特征融合技术，旨在提高多模态医学图像分割的准确性。&lt;h4&gt;背景&lt;/h4&gt;多模态3D医学图像分割在识别肿瘤区域时面临图像强度和肿瘤形态变化带来的挑战。传统的基于CNN的方法难以捕捉全局特征，而基于Transformers的方法虽然能有效地捕捉全局上下文，但在3D医学图像分割中计算成本高。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Mamba模型的特征提取和自适应多级特征融合方法，用于三维肿瘤分割，以提高分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 开发特定模态的Mamba编码器，以高效提取代表各模态中解剖和病理结构的远程相关特征。2. 设计双向协同集成块，通过模态注意力和通道注意力学习动态融合多模态和多级互补特征。3. 解码器结合深度语义信息和细粒度细节生成肿瘤分割图。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有的CNN、Transformer和基于Mamba的方法相比，该方法在PET/CT和MRI多序列医学图像数据集上取得了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地提高多模态3D医学图像肿瘤分割的准确性，为临床应用提供了新的技术支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal 3D medical image segmentation aims to accurately identify tumorregions across different modalities, facing challenges from variations in imageintensity and tumor morphology. Traditional convolutional neural network(CNN)-based methods struggle with capturing global features, whileTransformers-based methods, despite effectively capturing global context,encounter high computational costs in 3D medical image segmentation. The Mambamodel combines linear scalability with long-distance modeling, making it apromising approach for visual representation learning. However, Mamba-based 3Dmulti-modal segmentation still struggles to leverage modality-specific featuresand fuse complementary information effectively. In this paper, we propose aMamba based feature extraction and adaptive multilevel feature fusion for 3Dtumor segmentation using multi-modal medical image. We first develop thespecific modality Mamba encoder to efficiently extract long-range relevantfeatures that represent anatomical and pathological structures present in eachmodality. Moreover, we design an bi-level synergistic integration block thatdynamically merges multi-modal and multi-level complementary features by themodality attention and channel attention learning. Lastly, the decoder combinesdeep semantic information with fine-grained details to generate the tumorsegmentation map. Experimental results on medical image datasets (PET/CT andMRI multi-sequence) show that our approach achieve competitive performancecompared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.</description>
      <author>example@mail.com (Zexin Ji, Beiji Zou, Xiaoyan Kui, Hua Li, Pierre Vera, Su Ruan)</author>
      <guid isPermaLink="false">2504.21281v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Anti-Intercept OFDM Waveform Design with Secure Coding for Satellite Networks</title>
      <link>http://arxiv.org/abs/2504.21446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究低地球轨道卫星网络中物理层安全设计，旨在提高对窃听威胁的抵抗能力。&lt;h4&gt;背景&lt;/h4&gt;低地球轨道卫星网络对下一代通信系统至关重要，但它们的特点，如有限的机载资源、视距传播和易受广泛覆盖区域监听攻击，给物理层安全带来挑战。&lt;h4&gt;目的&lt;/h4&gt;设计适用于卫星-地面链路的抗截获波形，以提高在正交频分复用（OFDM）系统中的保密性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于二分搜索激活神经网络（BSA-Net）的解决方案，结合无监督学习进行安全编码优化和二分搜索进行动态功率分配。&lt;h4&gt;主要发现&lt;/h4&gt;该方案分为两个阶段，第一阶段在功率限制下优化安全编码，第二阶段在窃听限制下对子载波进行功率分配。仿真结果表明，该方法在保密率性能方面有显著提升。&lt;h4&gt;结论&lt;/h4&gt;所提出的抗截获波形设计方法有效提高了卫星网络对抗窃听威胁的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low Earth Orbit (LEO) satellite networks are integral to next-generationcommunication systems, providing global coverage, low latency, and minimalsignal loss. However, their unique characteristics, such as constrained onboardresources, Line-of-Sight (LoS) propagation, and vulnerability to eavesdroppingover wide coverage areas, present significant challenges to physical layersecurity. To address these challenges, this paper focuses on the design ofanti-intercept waveforms for satellite-ground links within Orthogonal FrequencyDivision Multiplexing (OFDM) systems, aiming to enhance security againsteavesdropping threats. We formulate a secrecy rate maximization problem thataims to balance secrecy performance and communication reliability undereavesdropping constraints and sub-carrier power limitations. To solve thisnon-convex optimization problem, we propose a bisection search-activated neuralnetwork (BSA-Net) that integrates unsupervised learning for secure codingoptimization and bisection search for dynamic power allocation. The proposedmethod is structured in two stages: the first optimizes secure coding underpower constraints, while the second allocates power across sub-carriers undereavesdropping constraints. Extensive simulation results demonstrate theefficacy of our approach, showcasing significant improvements in secrecy rateperformance.</description>
      <author>example@mail.com (Zhisheng Yin, Yonghong Liu, Dongbo Li, Nan Cheng, Linlin Liang, Changle Li, Jie Liu)</author>
      <guid isPermaLink="false">2504.21446v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Visual Text Processing: A Comprehensive Review and Unified Evaluation</title>
      <link>http://arxiv.org/abs/2504.21682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了视觉文本处理领域的最新进展，重点关注文本特征的适用性和有效整合方法，并提出了新的基准和评估指标。&lt;h4&gt;背景&lt;/h4&gt;视觉文本在文档和场景图像中扮演关键角色，传递丰富的语义信息，并在计算机视觉领域受到广泛关注。&lt;h4&gt;目的&lt;/h4&gt;探讨适用于不同视觉文本处理任务的文本特征，以及如何将这些特征有效整合到处理框架中。&lt;h4&gt;方法&lt;/h4&gt;提出了VTPBench基准和VTPScore评估指标，并通过实证研究验证了现有技术的改进空间。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，在视觉文本处理中，文本特征的有效捕捉和利用对于开发鲁棒的模型至关重要。&lt;h4&gt;结论&lt;/h4&gt;本工作旨在为视觉文本处理领域提供基础资源，促进未来在该领域的探索和创新。&lt;h4&gt;翻译&lt;/h4&gt;This paper reviews the latest advancements in the field of visual text processing, focusing on the suitability of text features for different tasks and how these distinctive text features can be effectively integrated into processing frameworks. The paper introduces VTPBench, a new benchmark that encompasses a wide range of visual text processing datasets, and VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. An empirical study with more than 20 specific models reveals substantial room for improvement in current techniques. The aim of this work is to establish a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual text is a crucial component in both document and scene images,conveying rich semantic information and attracting significant attention in thecomputer vision community. Beyond traditional tasks such as text detection andrecognition, visual text processing has witnessed rapid advancements driven bythe emergence of foundation models, including text image reconstruction andtext image manipulation. Despite significant progress, challenges remain due tothe unique properties that differentiate text from general objects. Effectivelycapturing and leveraging these distinct textual characteristics is essentialfor developing robust visual text processing models. In this survey, we presenta comprehensive, multi-perspective analysis of recent advancements in visualtext processing, focusing on two key questions: (1) What textual features aremost suitable for different visual text processing tasks? (2) How can thesedistinctive text features be effectively incorporated into processingframeworks? Furthermore, we introduce VTPBench, a new benchmark thatencompasses a broad range of visual text processing datasets. Leveraging theadvanced visual quality assessment capabilities of multimodal large languagemodels (MLLMs), we propose VTPScore, a novel evaluation metric designed toensure fair and reliable evaluation. Our empirical study with more than 20specific models reveals substantial room for improvement in the currenttechniques. Our aim is to establish this work as a fundamental resource thatfosters future exploration and innovation in the dynamic field of visual textprocessing. The relevant repository is available athttps://github.com/shuyansy/Visual-Text-Processing-survey.</description>
      <author>example@mail.com (Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, Xu-Cheng Yin, Nicu Sebe)</author>
      <guid isPermaLink="false">2504.21682v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Database and deep-learning scalability of anharmonic phonon properties by automated brute-force first-principles calculations</title>
      <link>http://arxiv.org/abs/2504.21245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过开发自动化的第一性原理工作流程，计算了晶体化合物的非谐和声子性质，并构建了一个包含6000多种无机化合物的数据库，以优化其热传输行为，并分析其光学、电子和磁特性。&lt;h4&gt;背景&lt;/h4&gt;理解晶体化合物的非谐和声子性质（如声子寿命和热导率）对于研究和优化其热传输行为至关重要，这些性质还通过声子与其他准粒子和场的相互作用影响光学、电子和磁特性。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化的第一性原理工作流程，构建一个包含多种无机化合物的数据库，并利用此数据库预测热导率和光谱。&lt;h4&gt;方法&lt;/h4&gt;建立了一个包含超过6000种无机化合物的数据库，并训练了一个图神经网络模型，使用结构参数预测热导率和光谱值。&lt;h4&gt;主要发现&lt;/h4&gt;预测准确性随着训练数据量的增加而提高，模型能够识别出具有极端热导率的材料（无论是高还是低）。数据库提供了对声子非谐和行为的宝贵见解。&lt;h4&gt;结论&lt;/h4&gt;该研究加速了先进功能材料的设计和开发，为理解和优化材料的热传输特性提供了新的工具和资源。&lt;h4&gt;翻译&lt;/h4&gt;摘要：理解晶体化合物的非谐和声子性质（如声子寿命和热导率）对于研究和优化它们的热传输行为至关重要。这些性质还通过声子与其他准粒子和场的相互作用影响光学、电子和磁特性。在本研究中，我们开发了一个自动化的第一性原理工作流程来计算非谐和声子性质，并构建了一个包含6000多种无机化合物的综合数据库。利用这个数据集，我们训练了一个图神经网络模型，从结构参数预测热导率和光谱值，证明了预测精度随着训练数据量的增加而提高的缩放规律。使用该模型进行高通量筛选，可以识别出具有极端热导率的材料——无论是高还是低。所得到的数据库为声子的非谐和行为提供了有价值的见解，从而加速了先进功能材料的设计和开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the anharmonic phonon properties of crystal compounds -- suchas phonon lifetimes and thermal conductivities -- is essential forinvestigating and optimizing their thermal transport behaviors. Theseproperties also impact optical, electronic, and magnetic characteristicsthrough interactions between phonons and other quasiparticles and fields. Inthis study, we develop an automated first-principles workflow to calculateanharmonic phonon properties and build a comprehensive database encompassingmore than 6,000 inorganic compounds. Utilizing this dataset, we train a graphneural network model to predict thermal conductivity values and spectra fromstructural parameters, demonstrating a scaling law in which prediction accuracyimproves with increasing training data size. High-throughput screening with themodel enables the identification of materials exhibiting extreme thermalconductivities -- both high and low. The resulting database offers valuableinsights into the anharmonic behavior of phonons, thereby accelerating thedesign and development of advanced functional materials.</description>
      <author>example@mail.com (Masato Ohnishi, Tianqi Deng, Pol Torres, Zhihao Xu, Terumasa Tadano, Haoming Zhang, Wei Nong, Masatoshi Hanai, Zhiting Tian, Ming Hu, Xiulin Ruan, Ryo Yoshida, Toyotaro Suzumura, Lucas Lindsay, Alan J. H. McGaughey, Tengfei Luo, Kedar Hippalgaonkar, Junichiro Shiomi)</author>
      <guid isPermaLink="false">2504.21245v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2504.21206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Federated Graph Learning (FGL) 通过保护数据隐私，允许客户端以分布式方式协作训练图神经网络 (GNNs)。然而，现有的FGL方法通常要求所有客户端拥有的图数据同质，以保证节点邻居分布模式的相似性。本文提出了FedHERO，一个旨在有效利用和共享异质图洞察的FGL框架，以解决节点邻居分布模式差异导致的问题。&lt;h4&gt;背景&lt;/h4&gt;FGL方法需要客户端拥有的图数据同质，以保证学习到的知识在所有客户端的本地模型中保持一致，从而在全局模型中正确聚合。&lt;h4&gt;目的&lt;/h4&gt;解决节点邻居分布模式差异导致的全局模型性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;FedHERO使用一个双通道GNN，并配备结构学习者，以识别和在学习不同节点邻居分布模式下的图中的通用模式。&lt;h4&gt;主要发现&lt;/h4&gt;FedHERO通过利用本地和共享的结构洞察，提高了单个客户端模型的表现，并为此领域处理具有各种节点邻居分布模式的图数据树立了新的先例。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了FedHERO相较于现有方法的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;Federated Graph Learning (FGL) 使客户端能够以保护数据隐私的方式协作训练图神经网络 (GNNs)。然而，现有的FGL方法通常要求所有客户端拥有的图数据同质，以确保节点邻居分布模式的相似性。因此，这些本地模型可以正确地聚合为全局模型，而不会损害整体性能。尽管如此，当节点的邻居分布模式在不同客户端之间发生变化时（例如，当客户端持有具有不同异质性的图时），它们的本地模型可能会从节点级别的预测任务中获得不同甚至冲突的知识。因此，通常会导致全局模型性能的灾难性下降。为了解决这个挑战，我们提出了FedHERO，一个旨在有效利用和共享异质图洞察的FGL框架。FedHERO的核心是一个双通道GNN，配备了一个结构学习者，旨在识别本地图中编码的结构知识。借助这个专门的组件，FedHERO使每个客户端的本地模型能够识别和学习适用于具有不同节点邻居分布模式图的通用模式。FedHERO不仅通过利用本地和共享的结构洞察来提高单个客户端模型的表现，而且在该领域树立了处理具有各种节点邻居分布模式图数据的新先例。我们进行了广泛的实验，以验证FedHERO相较于现有方法的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Graph Learning (FGL) empowers clients to collaboratively trainGraph neural networks (GNNs) in a distributed manner while preserving dataprivacy. However, FGL methods usually require that the graph data owned by allclients is homophilic to ensure similar neighbor distribution patterns ofnodes. Such an assumption ensures that the learned knowledge is consistentacross the local models from all clients. Therefore, these local models can beproperly aggregated as a global model without undermining the overallperformance. Nevertheless, when the neighbor distribution patterns of nodesvary across different clients (e.g., when clients hold graphs with differentlevels of heterophily), their local models may gain different and even conflictknowledge from their node-level predictive tasks. Consequently, aggregatingthese local models usually leads to catastrophic performance deterioration onthe global model. To address this challenge, we propose FedHERO, an FGLframework designed to harness and share insights from heterophilic graphseffectively. At the heart of FedHERO is a dual-channel GNN equipped with astructure learner, engineered to discern the structural knowledge encoded inthe local graphs. With this specialized component, FedHERO enables the localmodel for each client to identify and learn patterns that are universallyapplicable across graphs with different patterns of node neighbordistributions. FedHERO not only enhances the performance of individual clientmodels by leveraging both local and shared structural insights but also sets anew precedent in this field to effectively handle graph data with various nodeneighbor distribution patterns. We conduct extensive experiments to validatethe superior performance of FedHERO against existing alternatives.</description>
      <author>example@mail.com (Zihan Chen, Xingbo Fu, Yushun Dong, Jundong Li, Cong Shen)</author>
      <guid isPermaLink="false">2504.21206v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-level datasets training method in Physics-Informed Neural Networks</title>
      <link>http://arxiv.org/abs/2504.21328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 12figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Physics-Informed Neural Networks (PINNs) 在解决偏微分方程 (PDEs) 方面表现出巨大潜力，但面临难以解决的挑战和高频成分问题，影响准确性和收敛性。&lt;h4&gt;背景&lt;/h4&gt;PINNs 能够结合物理定律进行多领域应用，但存在计算成本增加、精度损失或解发散的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来减轻上述问题，提高训练准确性。&lt;h4&gt;方法&lt;/h4&gt;受CFD领域的多网格方法启发，通过不同级别的训练样本训练，有效去除不同频率的错误，从而在不调整神经网络结构、损失权重和超参数的情况下提高训练精度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在1D高频率常微分方程和2D对流扩散方程中表现出效，并且在稳态Lid-driven cavity flows的经典基准问题中应用，显示了对高频多模式问题的适用性和效果。通过不同的训练序列模式，预测精度提高了30%至60%。&lt;h4&gt;结论&lt;/h4&gt;该方法与迁移学习技术结合，可以解决更复杂的问题（如更高的Reynolds数）。结果表明，该方法能够在Re=5000的情况下产生良好的预测，展示了解决复杂高频PDEs的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于物理的神经网络（PINNs）作为一种解决偏微分方程（PDEs）的有前景的方法，在计算机科学和各个物理相关领域受到广泛关注。尽管已证明其能够结合物理定律进行多种应用，PINNs 仍面临难以解决的挑战，这些挑战涉及难以解决的问题和/或具有高频成分的解，导致精度和收敛性问题。这不仅会增加计算成本，还可能导致精度损失或解发散。本研究提出了一种替代方法来减轻上述问题。受CFD社区中的多网格方法的启发，当前方法的底层思想是通过使用不同级别的训练样本进行训练，有效地去除不同频率的错误，从而以更简单的方式提高训练精度，而不需要在微调神经网络结构、损失权重以及超参数上花费时间。为了证明当前方法的效力，我们首先研究了具有高频成分的1D常微分方程和具有V循环训练策略的2D对流扩散方程。最后，当前方法被用于不同雷诺数下的经典基准问题——稳态Lid-driven cavity flows，以研究该问题涉及多模式高频的适用性和效果。通过各种训练序列模式，预测精度提高了30%至60%。我们还研究了当前方法与迁移学习技术之间的协同作用，以解决更复杂的问题（即更高的Re）。从目前的结果来看，当前框架甚至对于Re=5000的情况也能产生良好的预测，展示了其解决复杂高频PDEs的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physics-Informed Neural Networks have emerged as a promising methodology forsolving PDEs, gaining significant attention in computer science and variousphysics-related fields. Despite being demonstrated the ability to incorporatethe physics of laws for versatile applications, PINNs still struggle with thechallenging problems which are stiff to be solved and/or have high-frequencycomponents in the solutions, resulting in accuracy and convergence issues. Itmay not only increase computational costs, but also lead to accuracy loss orsolution divergence. In this study, an alternative approach is proposed tomitigate the above-mentioned problems. Inspired by the multi-grid method in CFDcommunity, the underlying idea of the current approach is to efficiently removedifferent frequency errors via training with different levels of trainingsamples, resulting in a simpler way to improve the training accuracy withoutspending time in fine-tuning of neural network structures, loss weights as wellas hyperparameters. To demonstrate the efficacy of current approach, we firstinvestigate canonical 1D ODE with high-frequency component and 2Dconvection-diffusion equation with V-cycle training strategy. Finally, thecurrent method is employed for the classical benchmark problem of steadyLid-driven cavity flows at different Reynolds numbers, to investigate theapplicability and efficacy for the problem involved multiple modes of high andlow frequency. By virtue of various training sequence modes, improvementthrough predictions lead to 30% to 60% accuracy improvement. We alsoinvestigate the synergies between current method and transfer learningtechniques for more challenging problems (i.e., higher Re). From the presentresults, it also revealed that the current framework can produce goodpredictions even for the case of Re=5000, demonstrating the ability to solvecomplex high-frequency PDEs.</description>
      <author>example@mail.com (Yao-Hsuan Tsai, Hsiao-Tung Juan, Pao-Hsiung Chiu, Chao-An Lin)</author>
      <guid isPermaLink="false">2504.21328v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes</title>
      <link>http://arxiv.org/abs/2504.21562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于神经细胞自动机（NCA）的无线胶囊内窥镜图像处理方法，用于病理检测和深度估计，并在ESP32微控制器上实现，以实现胶囊内窥镜的精确定位。&lt;h4&gt;背景&lt;/h4&gt;无线胶囊内窥镜是一种非侵入性的胃肠道成像方法，但其视频数据量大，定位胶囊后需要大量时间审查，且现有技术如出血检测和深度估计方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种在小型化设备上可靠进行出血分割和深度估计的方法，以实现胶囊内窥镜的精确定位。&lt;h4&gt;方法&lt;/h4&gt;使用NCA进行出血分割和深度估计，将大型基础模型精简为轻量级NCA架构，并将训练好的NCA模型部署到ESP32微控制器上。&lt;h4&gt;主要发现&lt;/h4&gt;NCA在出血分割和深度估计方面比其他便携式分割模型更准确，同时内存参数存储量比其他小型化模型低100倍以上。NCA的深度估计视觉效果令人信服，有时甚至优于伪地面真实值。对ESP32-S3的运行时优化显著提高了平均推理速度。&lt;h4&gt;结论&lt;/h4&gt;本研究首次实现了在小型化设备上进行可靠出血分割和深度估计，为结合视觉里程计进行胶囊精确定位的精确诊断开辟了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：无线胶囊内窥镜是一种非侵入性的整个胃肠道成像方法，是传统内窥镜的无痛替代品。它生成大量的视频数据，需要大量审查时间，并且在吞咽后定位胶囊是一个挑战。出血检测和深度估计等技术有助于病理定位，但深度学习模型通常太大，无法直接在胶囊上运行。针对出血分割和深度估计的神经网络细胞自动机（NCA）在胶囊内窥镜图像上进行了训练。对于单目深度估计，我们将大型基础模型精简为轻量级的NCA架构，将基础模型的输出作为伪地面真实值。然后，我们将训练好的NCA移植到ESP32微控制器上，使小型化到摄像头胶囊的硬件上实现高效的图像处理。NCA比其他便携式分割模型更准确（Dice），同时比其他小型化模型在内存中存储的参数少100倍以上。NCA的深度估计视觉效果令人信服，有时甚至优于伪地面真实值。在ESP32-S3上的运行时优化显著提高了平均推理速度，提高了3倍以上。通过几个算法调整和蒸馏，可以将NCA模型封装到适合无线胶囊内窥镜的微控制器中。这是首次在小型化设备上实现可靠出血分割和深度估计的工作，为结合视觉里程计作为胶囊精确定位手段的精确诊断铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless Capsule Endoscopy is a non-invasive imaging method for the entiregastrointestinal tract, and is a pain-free alternative to traditionalendoscopy. It generates extensive video data that requires significant reviewtime, and localizing the capsule after ingestion is a challenge. Techniqueslike bleeding detection and depth estimation can help with localization ofpathologies, but deep learning models are typically too large to run directlyon the capsule. Neural Cellular Automata (NCA) for bleeding segmentation anddepth estimation are trained on capsule endoscopic images. For monocular depthestimation, we distill a large foundation model into the lean NCA architecture,by treating the outputs of the foundation model as pseudo ground truth. We thenport the trained NCA to the ESP32 microcontroller, enabling efficient imageprocessing on hardware as small as a camera capsule. NCA are more accurate(Dice) than other portable segmentation models, while requiring more than 100xfewer parameters stored in memory than other small-scale models. The visualresults of NCA depth estimation look convincing, and in some cases beat therealism and detail of the pseudo ground truth. Runtime optimizations on theESP32-S3 accelerate the average inference speed significantly, by more thanfactor 3. With several algorithmic adjustments and distillation, it is possibleto eNCApsulate NCA models into microcontrollers that fit into wireless capsuleendoscopes. This is the first work that enables reliable bleeding segmentationand depth estimation on a miniaturized device, paving the way for precisediagnosis combined with visual odometry as a means of precise localization ofthe capsule -- on the capsule.</description>
      <author>example@mail.com (Henry John Krumb, Anirban Mukhopadhyay)</author>
      <guid isPermaLink="false">2504.21562v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>VideoMultiAgents: A Multi-Agent Framework for Video Question Answering</title>
      <link>http://arxiv.org/abs/2504.20091v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为VideoMultiAgents的视频问答框架，该框架通过整合视觉、场景图分析和文本处理等专门代理，增强了视频理解能力，并通过问题引导的标题生成提高了答案的准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的视频问答方法往往依赖于将帧级字幕输入到单一模型中，这难以充分捕捉时间和交互式上下文。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述局限性，提出了VideoMultiAgents框架，旨在通过多模态推理提高视频理解。&lt;h4&gt;方法&lt;/h4&gt;VideoMultiAgents框架包括视觉、场景图分析和文本处理等专门代理，并通过问题引导的标题生成来生成与查询直接相关的字幕。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在Intent-QA、EgoSchema子集和NExT-QA数据集上均取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;VideoMultiAgents框架通过多模态推理和问题引导的标题生成，显著提高了视频问答的准确性。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces a video question answering framework called VideoMultiAgents, which integrates specialized agents for vision, scene graph analysis, and text processing to enhance video understanding and improve answer accuracy through question-guided caption generation. Experimental results demonstrate that this method achieves state-of-the-art performance on the Intent-QA, EgoSchema subset, and NExT-QA datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VQA) inherently relies on multimodal reasoning,integrating visual, temporal, and linguistic cues to achieve a deeperunderstanding of video content. However, many existing methods rely on feedingframe-level captions into a single model, making it difficult to adequatelycapture temporal and interactive contexts. To address this limitation, weintroduce VideoMultiAgents, a framework that integrates specialized agents forvision, scene graph analysis, and text processing. It enhances videounderstanding leveraging complementary multimodal reasoning from independentlyoperating agents. Our approach is also supplemented with a question-guidedcaption generation, which produces captions that highlight objects, actions,and temporal transitions directly relevant to a given query, thus improving theanswer accuracy. Experimental results demonstrate that our method achievesstate-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%). The source code isavailable at https://github.com/PanasonicConnect/VideoMultiAgents.</description>
      <author>example@mail.com (Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Yasunori Ishii, Masamoto Tanabiki, Kazuki Kozuka, Ehsan Adeli)</author>
      <guid isPermaLink="false">2504.20091v2</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild</title>
      <link>http://arxiv.org/abs/2504.21248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用多模态迁移学习来提高基于视频的动态面部表情识别（DFEW）数据集上的面部表情识别（FER）性能。&lt;h4&gt;背景&lt;/h4&gt;面部表情识别是计算机视觉的一个子集，在人类-计算机交互、医疗保健和客户服务中具有重要作用。FER是一个具有挑战性的问题领域，因为准确的分类需要模型能够区分面部特征的微妙变化。&lt;h4&gt;目的&lt;/h4&gt;探索使用预训练的ResNets、OpenPose和OmniVec网络组合，以及跨时间、多模态特征对分类准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;本文使用预训练的ResNets、OpenPose和OmniVec网络，结合多模态特征生成器，来提高基于变换器的分类模型的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，这些精心调整的多模态特征生成器适度提高了基于变换器的分类模型的准确性。&lt;h4&gt;结论&lt;/h4&gt;多模态迁移学习可以有效地提高视频基础上面部表情识别的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial expression recognition (FER) is a subset of computer vision withimportant applications for human-computer-interaction, healthcare, and customerservice. FER represents a challenging problem-space because accurateclassification requires a model to differentiate between subtle changes infacial features. In this paper, we examine the use of multi-modal transferlearning to improve performance on a challenging video-based FER dataset,Dynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrainedResNets, OpenPose, and OmniVec networks, we explore the impact ofcross-temporal, multi-modal features on classification accuracy. Ultimately, wefind that these finely-tuned multi-modal feature generators modestly improveaccuracy of our transformer-based classification model.</description>
      <author>example@mail.com (Ezra Engel, Lishan Li, Chris Hudy, Robert Schleusner)</author>
      <guid isPermaLink="false">2504.21248v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision</title>
      <link>http://arxiv.org/abs/2504.21423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Diff-Prompt的Prompt学习新方法，通过扩散模型生成丰富且精细的提示信息，用于复杂下游任务，并取得了显著的效果。&lt;h4&gt;背景&lt;/h4&gt;Prompt学习在微调预训练的多模态模型中显示出有前景的结果，但在更复杂和精细的任务中，性能提升有限。&lt;h4&gt;目的&lt;/h4&gt;旨在使用扩散模型为复杂下游任务生成丰富和精细的提示信息。&lt;h4&gt;方法&lt;/h4&gt;方法包括三个阶段：首先训练一个Mask-VAE来压缩掩码到潜在空间；其次，利用改进的Diffusion Transformer在潜在空间中训练提示生成器；最后，将提示生成器的去噪过程与预训练模型在语义空间中对齐，并使用生成的提示来微调模型。&lt;h4&gt;主要发现&lt;/h4&gt;在复杂像素级下游任务，即指代表达理解任务上，Diff-Prompt相较于基础模型在R@1和R@5上分别提升了8.87和14.05，并在多个指标上优于其他最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了该方法的有效性，并突出了使用生成模型进行提示生成的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Prompt学习在微调预训练的多模态模型中已显示出有前景的结果。然而，当应用于更复杂和精细的任务时，性能提升有限。原因是大多数现有方法直接通过损失反向传播优化提示生成过程中涉及的参数，这限制了提示表示的丰富性和特异性。在本文中，我们提出了扩散驱动提示生成器（Diff-Prompt），旨在使用扩散模型为复杂下游任务生成丰富和精细的提示信息。具体来说，我们的方法包括三个阶段。在第一阶段，我们训练一个Mask-VAE将掩码压缩到潜在空间。在第二阶段，我们利用改进的扩散Transformer（DiT）在潜在空间中训练提示生成器，使用掩码进行监督。在第三阶段，我们将提示生成器的去噪过程与预训练模型在语义空间中对齐，并使用生成的提示来微调模型。我们在一个复杂的像素级下游任务，即指代表达理解任务上进行了实验，并将我们的方法与各种参数高效的微调方法进行了比较。与基础模型相比，Diff-Prompt在R@1和R@5上分别提高了8.87和14.05，并在多个指标上优于其他最先进的方法。实验结果验证了我们的方法的有效性，并突出了使用生成模型进行提示生成的潜力。代码可在https://github.com/Kelvin-ywc/diff-prompt上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Prompt learning has demonstrated promising results in fine-tuning pre-trainedmultimodal models. However, the performance improvement is limited when appliedto more complex and fine-grained tasks. The reason is that most existingmethods directly optimize the parameters involved in the prompt generationprocess through loss backpropagation, which constrains the richness andspecificity of the prompt representations. In this paper, we proposeDiffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusionmodel to generate rich and fine-grained prompt information for complexdownstream tasks. Specifically, our approach consists of three stages. In thefirst stage, we train a Mask-VAE to compress the masks into latent space. Inthe second stage, we leverage an improved Diffusion Transformer (DiT) to traina prompt generator in the latent space, using the masks for supervision. In thethird stage, we align the denoising process of the prompt generator with thepre-trained model in the semantic space, and use the generated prompts tofine-tune the model. We conduct experiments on a complex pixel-level downstreamtask, referring expression comprehension, and compare our method with variousparameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximumimprovement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation modeland also outperforms other state-of-the-art methods across multiple metrics.The experimental results validate the effectiveness of our approach andhighlight the potential of using generative models for prompt generation. Codeis available at https://github.com/Kelvin-ywc/diff-prompt.</description>
      <author>example@mail.com (Weicai Yan, Wang Lin, Zirun Guo, Ye Wang, Fangming Feng, Xiaoda Yang, Zehan Wang, Tao Jin)</author>
      <guid isPermaLink="false">2504.21423v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions</title>
      <link>http://arxiv.org/abs/2504.21189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了人工智能在个性化阿尔茨海默病进展预测中的应用方法。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病的进展存在显著的个体差异，这给准确预测和个性化护理规划带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发能够预测患者特定疾病轨迹的预测模型。&lt;h4&gt;方法&lt;/h4&gt;文章回顾了包括状态空间模型、深度学习技术（如循环神经网络）、图神经网络以及人工智能驱动的数字孪生等关键方法。&lt;h4&gt;主要发现&lt;/h4&gt;讨论了数据限制带来的挑战，如高维度、缺失数据和数据集不平衡，并提出了合成数据生成策略。&lt;h4&gt;结论&lt;/h4&gt;强调了多模态集成、模型可解释性和泛化性的趋势，并指出了开放性挑战和未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：阿尔茨海默病（AD）的特征是个体间进展的显著差异，这给准确的预测和个性化的护理规划带来了复杂性。这种异质性强调了开发能够预测患者特定疾病轨迹的预测模型的迫切需要。人工智能（AI）通过分析复杂、多模态和纵向患者数据，提供了强大的工具来应对这一挑战。本文全面概述了应用于个性化AD进展预测的AI方法。我们回顾了包括用于捕捉时间动态的状态空间模型、用于序列建模的深度学习技术（如循环神经网络）、用于利用网络结构的图神经网络（GNNs）以及人工智能驱动的数字孪生等新兴概念。认识到数据限制往往阻碍进展，我们检查了常见挑战，如高维度、缺失数据和数据集不平衡。我们进一步讨论了人工智能驱动的缓解策略，特别是使用变分自编码器（VAEs）和生成对抗网络（GANs）进行数据增强和平衡。该综述综合了当前方法的优点和局限性，强调了向多模态集成和持续需要模型可解释性和泛化性的趋势。最后，我们确定了关键开放性挑战，包括稳健的外部验证、临床整合和伦理考虑，并概述了混合模型、因果推理和联邦学习等有希望的未来研究方向。本综述旨在巩固现有知识并指导未来在开发与临床相关的AI工具以实现个性化AD预测方面的努力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alzheimer's Disease (AD) is marked by significant inter-individualvariability in its progression, complicating accurate prognosis andpersonalized care planning. This heterogeneity underscores the critical needfor predictive models capable of forecasting patient-specific diseasetrajectories. Artificial Intelligence (AI) offers powerful tools to addressthis challenge by analyzing complex, multi-modal, and longitudinal patientdata. This paper provides a comprehensive survey of AI methodologies applied topersonalized AD progression prediction. We review key approaches includingstate-space models for capturing temporal dynamics, deep learning techniqueslike Recurrent Neural Networks for sequence modeling, Graph Neural Networks(GNNs) for leveraging network structures, and the emerging concept of AI-drivendigital twins for individualized simulation. Recognizing that data limitationsoften impede progress, we examine common challenges such as highdimensionality, missing data, and dataset imbalance. We further discussAI-driven mitigation strategies, with a specific focus on synthetic datageneration using Variational Autoencoders (VAEs) and Generative AdversarialNetworks (GANs) to augment and balance datasets. The survey synthesizes thestrengths and limitations of current approaches, emphasizing the trend towardsmultimodal integration and the persistent need for model interpretability andgeneralizability. Finally, we identify critical open challenges, includingrobust external validation, clinical integration, and ethical considerations,and outline promising future research directions such as hybrid models, causalinference, and federated learning. This review aims to consolidate currentknowledge and guide future efforts in developing clinically relevant AI toolsfor personalized AD prognostication.</description>
      <author>example@mail.com (Gulsah Hancerliogullari Koksalmis, Bulent Soykan, Laura J. Brattain, Hsin-Hsiung Huang)</author>
      <guid isPermaLink="false">2504.21189v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Galvatron: An Automatic Distributed System for Efficient Foundation Model Training</title>
      <link>http://arxiv.org/abs/2504.21411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Galvatron是一个用于高效训练大规模基础模型的分布式系统。&lt;h4&gt;背景&lt;/h4&gt;传统模型训练过程中，选择最优并行策略非常复杂。&lt;h4&gt;目的&lt;/h4&gt;Galvatron旨在自动识别最有效的混合策略，提高大规模模型训练的效率。&lt;h4&gt;方法&lt;/h4&gt;系统包括硬件和模型分析的性能分析器，决策树和动态规划策略的搜索引擎，以及执行这些策略的高效运行时环境。&lt;h4&gt;主要发现&lt;/h4&gt;在各种集群上的基准测试表明，Galvatron相比现有框架具有更高的吞吐量。&lt;h4&gt;结论&lt;/h4&gt;Galvatron是一个开源系统，提供了用户友好的界面和全面的文档，使复杂的分布式训练变得可行和高效。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Galvatron is a distributed system for efficiently training large-scaleFoundation Models. It overcomes the complexities of selecting optimalparallelism strategies by automatically identifying the most efficient hybridstrategy, incorporating data, tensor, pipeline, sharded data, and sequenceparallelism, along with recomputation. The system's architecture includes aprofiler for hardware and model analysis, a search engine for strategyoptimization using decision trees and dynamic programming, and a runtime forexecuting these strategies efficiently. Benchmarking on various clustersdemonstrates Galvatron's superior throughput compared to existing frameworks.This open-source system offers user-friendly interfaces and comprehensivedocumentation, making complex distributed training accessible and efficient.The source code of Galvatron is available athttps://github.com/PKU-DAIR/Hetu-Galvatron.</description>
      <author>example@mail.com (Xinyi Liu, Yujie Wang, Shenhan Zhu, Fangcheng Fu, Qingshuo Liu, Guangming Lin, Bin Cui)</author>
      <guid isPermaLink="false">2504.21411v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning</title>
      <link>http://arxiv.org/abs/2504.21187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型（LLM）的HLS编码助手LIFT，用于自动生成性能关键pragma，以解决FPGA编程中性能优化的问题。&lt;h4&gt;背景&lt;/h4&gt;FPGA在数据中心环境中因其可重构性和能效而被广泛采用。尽管HLS工具提高了编程抽象级别，但实现高性能仍需要专家知识和手动插入优化pragma。&lt;h4&gt;目的&lt;/h4&gt;提出一种自动生成性能关键pragma的方法，以简化FPGA编程并提高性能。&lt;h4&gt;方法&lt;/h4&gt;LIFT通过将LLM与图神经网络（GNN）紧密集成和监督训练过程来优化。它结合了LLM的序列建模能力和GNN的结构和语义理解，以处理代码及其控制/数据依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;LIFT在性能上优于现有的AutoDSE和HARP，以及GPT-4o，平均性能提升分别为3.52倍、2.16倍和66倍。&lt;h4&gt;结论&lt;/h4&gt;LIFT是一种有效的编码助手，可以自动生成性能关键pragma，从而提高FPGA设计的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; FPGAs are increasingly adopted in datacenter environments for theirreconfigurability and energy efficiency. High-Level Synthesis (HLS) tools haveeased FPGA programming by raising the abstraction level from RTL to untimedC/C++, yet attaining high performance still demands expert knowledge anditerative manual insertion of optimization pragmas to modify themicroarchitecture. To address this challenge, we propose LIFT, a large languagemodel (LLM)-based coding assistant for HLS that automatically generatesperformance-critical pragmas given a C/C++ design. We fine-tune the LLM bytightly integrating and supervising the training process with a graph neuralnetwork (GNN), combining the sequential modeling capabilities of LLMs with thestructural and semantic understanding of GNNs necessary for reasoning over codeand its control/data dependencies. On average, LIFT produces designs thatimprove performance by 3.52x and 2.16x than prior state-of the art AutoDSE andHARP respectively, and 66x than GPT-4o.</description>
      <author>example@mail.com (Neha Prakriya, Zijian Ding, Yizhou Sun, Jason Cong)</author>
      <guid isPermaLink="false">2504.21187v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>JAvaScript Multimodal INformation Explorer</title>
      <link>http://arxiv.org/abs/2504.21393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Jasmine，一个基于JavaScript的多模态信息探索器，用于处理和分析大量天文数据。&lt;h4&gt;背景&lt;/h4&gt;天文数据量庞大、信息丰富，处理这些数据是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决处理天文数据的问题。&lt;h4&gt;方法&lt;/h4&gt;引入了Jasmine，它允许用户打开不同的数据查看模态，显示集合中的特定数据点。目前支持图像数据和点云对象。用户可以选择显示哪些数据点信息。点云具有交互性，允许缩放、滚动和旋转。通过自动编码实现数据集的键属性排列。&lt;h4&gt;主要发现&lt;/h4&gt;Jasmine支持多种数据视图，具有交互性，并通过自动编码优化了数据点的显示。&lt;h4&gt;结论&lt;/h4&gt;Jasmine是一个有效的工具，可以用于分析、查看、探索和通信天文数据。&lt;h4&gt;翻译&lt;/h4&gt;Astronomical data is rich in volume, information and facets. Although this offers multiple research perspectives, processing the data remains a challenge. Infrastructures for analyzing, inspecting, exploring and communicating with data are mandatory. To address this issue, we introduce Jasmine, the JAvaScript Multimodal INformation Explorer. Jasmine allows users to open different dataviewer modals that show a specific data point from a set. The viewer currently supports image data, as well as point cloud objects. Users can decide on which information about the data point they like to have displayed. Point clouds are interactive and allow for zooming, tossing, and turning. Picking a data point is enabled by providing a structured view of the set, arranged by a key property. This arrangement is achieved by autoencoding.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Astronomical data is rich in volume, information and facets. Although thisoffers multiple research perspectives, processing the data remains a challenge.Infrastructures for analyzing, inspecting, exploring and communicating withdata are mandatory. To address this issue, we introduce Jasmine, the JAvaScriptMultimodal INformation Explorer. Jasmine allows users to open different dataviewer modals that show a specific data point from a set. The viewer currentlysupports image data, as well as point cloud objects. Users can decide on whichinformation about the data point they like to have displayed. Point clouds areinteractive and allow for zooming, tossing, and turning. Picking a data pointis enabled by providing a structured view of the set, arranged by a keyproperty. This arrangement is achieved by autoencoding.</description>
      <author>example@mail.com (Fenja Schweder, Sebastian Trujillo-Gomez, Kai Polsterer)</author>
      <guid isPermaLink="false">2504.21393v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection</title>
      <link>http://arxiv.org/abs/2504.21066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在边缘设备上训练和部署深度伪造检测模型，探讨了通过压缩技术和迁移学习方法来降低计算需求和训练开销，以提高数据隐私和保密性。&lt;h4&gt;背景&lt;/h4&gt;在边缘设备上处理数据可以保护数据隐私，但受限于边缘设备的计算和内存资源。&lt;h4&gt;目的&lt;/h4&gt;通过压缩技术和迁移学习方法，解决边缘设备资源受限的问题。&lt;h4&gt;方法&lt;/h4&gt;使用Synthbuster、RAISE和ForenSynths数据集，评估了剪枝、知识蒸馏（KD）、量化、微调和基于适配器的技术。&lt;h4&gt;主要发现&lt;/h4&gt;压缩和迁移学习在高达90%的压缩率下仍能有效实现，且当训练和验证数据来自同一深度伪造模型时，性能保持不变。然而，当测试数据集由训练集中未出现的深度伪造模型生成时，出现了领域泛化问题。&lt;h4&gt;结论&lt;/h4&gt;压缩和迁移学习是提高边缘设备上深度伪造检测模型性能的有效方法，但在处理未在训练集中出现的深度伪造模型时存在挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training and deploying deepfake detection models on edge devices offers theadvantage of maintaining data privacy and confidentiality by processing itclose to its source. However, this approach is constrained by the limitedcomputational and memory resources available at the edge. To address thischallenge, we explore compression techniques to reduce computational demandsand inference time, alongside transfer learning methods to minimize trainingoverhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluatethe effectiveness of pruning, knowledge distillation (KD), quantization,fine-tuning, and adapter-based techniques. Our experimental results demonstratethat both compression and transfer learning can be effectively achieved, evenwith a high compression level of 90%, remaining at the same performance levelwhen the training and validation data originate from the same DeepFake model.However, when the testing dataset is generated by DeepFake models not presentin the training set, a domain generalization issue becomes evident.</description>
      <author>example@mail.com (Andreas Karathanasis, John Violos, Ioannis Kompatsiaris, Symeon Papadopoulos)</author>
      <guid isPermaLink="false">2504.21066v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*</title>
      <link>http://arxiv.org/abs/2504.11014v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted (Poster) to the 3rd CV4MR Workshop at CVPR 2025:  https://openreview.net/forum?id=00RQ8Cv3ia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GATE3D的新型弱监督框架，专门用于通用单目3D目标检测，通过一致性损失在2D和3D预测之间架起桥梁，以解决多领域训练中标注数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉领域正趋向于开发能够同时处理多种不同任务的通用模型，这通常需要跨多领域数据集进行联合训练以确保有效的泛化。然而，由于标注有准确3D地面真实标签的数据集稀缺，尤其是在典型的基于道路的自动驾驶环境之外，单目3D目标检测在多领域训练中面临独特挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，提出一种能够有效处理非道路环境中的行人检测问题的弱监督框架。&lt;h4&gt;方法&lt;/h4&gt;GATE3D通过使用伪标签和一致性损失在2D和3D预测之间进行联合训练，以实现跨领域的泛化。&lt;h4&gt;主要发现&lt;/h4&gt;GATE3D在KITTI基准数据集和作者收集的室内办公数据集上取得了有竞争力的性能，表明该框架能够显著加速从有限标注数据中的学习过程。&lt;h4&gt;结论&lt;/h4&gt;GATE3D在机器人、增强现实和虚拟现实应用中具有广泛的应用潜力，通过有效的预训练策略，显著加速了从有限标注数据中的学习。&lt;h4&gt;翻译&lt;/h4&gt;The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging trend in computer vision emphasizes developing universal modelscapable of simultaneously addressing multiple diverse tasks. Such universalitytypically requires joint training across multi-domain datasets to ensureeffective generalization. However, monocular 3D object detection presentsunique challenges in multi-domain training due to the scarcity of datasetsannotated with accurate 3D ground-truth labels, especially beyond typicalroad-based autonomous driving contexts. To address this challenge, we introducea novel weakly supervised framework leveraging pseudo-labels. Currentpretrained models often struggle to accurately detect pedestrians in non-roadenvironments due to inherent dataset biases. Unlike generalized image-based 2Dobject detection models, achieving similar generalization in monocular 3Ddetection remains largely unexplored. In this paper, we propose GATE3D, a novelframework designed specifically for generalized monocular 3D object detectionvia weak supervision. GATE3D effectively bridges domain gaps by employingconsistency losses between 2D and 3D predictions. Remarkably, our modelachieves competitive performance on the KITTI benchmark as well as on anindoor-office dataset collected by us to evaluate the generalizationcapabilities of our framework. Our results demonstrate that GATE3Dsignificantly accelerates learning from limited annotated data througheffective pre-training strategies, highlighting substantial potential forbroader impacts in robotics, augmented reality, and virtual realityapplications. Project page: https://ies0411.github.io/GATE3D/</description>
      <author>example@mail.com (Eunsoo Im, Changhyun Jee, Jung Kwon Lee)</author>
      <guid isPermaLink="false">2504.11014v4</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation</title>
      <link>http://arxiv.org/abs/2504.21336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first universal foundation model for grounded biomedical image  interpretation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniBiomed是一种新型的基于多模态大语言模型和Segment Anything Model的通用基础模型，用于生物医学图像的 grounded interpretation，显著提高了生物医学图像分析的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;传统的AI方法在生物医学图像分析中通常依赖于分离的训练，如大型语言模型用于临床文本生成和分割模型用于目标提取，这导致在实际应用中缺乏灵活性，无法充分利用整体生物医学信息。&lt;h4&gt;目的&lt;/h4&gt;提出UniBiomed，旨在解决传统方法的问题，实现生物医学图像的全面、灵活的grounded interpretation。&lt;h4&gt;方法&lt;/h4&gt;UniBiomed结合了多模态大语言模型（MLLM）和Segment Anything Model（SAM），能够同时生成临床文本和分割相应的生物医学对象，实现grounded interpretation。为了开发UniBiomed，研究人员创建了一个包含超过2700万对图像、注释和文本描述的大规模数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在84个内部和外部数据集上的广泛验证表明，UniBiomed在分割、疾病识别、区域感知诊断、视觉问答和报告生成等方面均达到了最先进的性能。与依赖临床专家进行预诊断和手动制作精确文本或视觉提示的先前模型不同，UniBiomed能够提供自动化的端到端grounded interpretation。&lt;h4&gt;结论&lt;/h4&gt;UniBiomed代表了生物医学AI的一个新突破，为更准确和高效的生物医学图像分析打开了强大的grounded interpretation能力，代表了临床工作流程中的一个新颖范式转变，将显著提高诊断效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态生物医学图像的解释为生物医学图像分析开辟了新的机遇。传统的AI方法通常依赖于分离的训练，即大型语言模型（LLMs）用于临床文本生成和分割模型用于目标提取，这导致在实际应用中缺乏灵活性，无法充分利用整体生物医学信息。为此，我们引入了UniBiomed，这是第一个用于grounded生物医学图像解释的通用基础模型。UniBiomed基于多模态大语言模型（MLLM）和Segment Anything Model（SAM）的全新集成，有效地统一了临床文本的生成和对应生物医学对象的分割，以实现grounded interpretation。这样，UniBiomed能够处理跨十个不同生物医学成像模态的广泛生物医学任务。为了开发UniBiomed，我们创建了一个包含超过2700万对图像、注释和文本描述的大规模数据集。在84个内部和外部数据集上的广泛验证表明，UniBiomed在分割、疾病识别、区域感知诊断、视觉问答和报告生成等方面均达到了最先进的性能。此外，与依赖临床专家进行预诊断和手动制作精确文本或视觉提示的先前模型不同，UniBiomed能够提供自动化的端到端grounded interpretation。这代表了临床工作流程中的一个新颖范式转变，将显著提高诊断效率。总之，UniBiomed代表了生物医学AI的一个新突破，为更准确和高效的生物医学图像分析打开了强大的grounded interpretation能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal interpretation of biomedical images opens up novel opportunitiesin biomedical image analysis. Conventional AI approaches typically rely ondisjointed training, i.e., Large Language Models (LLMs) for clinical textgeneration and segmentation models for target extraction, which results ininflexible real-world deployment and a failure to leverage holistic biomedicalinformation. To this end, we introduce UniBiomed, the first universalfoundation model for grounded biomedical image interpretation. UniBiomed isbased on a novel integration of Multi-modal Large Language Model (MLLM) andSegment Anything Model (SAM), which effectively unifies the generation ofclinical texts and the segmentation of corresponding biomedical objects forgrounded interpretation. In this way, UniBiomed is capable of tackling a widerange of biomedical tasks across ten diverse biomedical imaging modalities. Todevelop UniBiomed, we curate a large-scale dataset comprising over 27 milliontriplets of images, annotations, and text descriptions across ten imagingmodalities. Extensive validation on 84 internal and external datasetsdemonstrated that UniBiomed achieves state-of-the-art performance insegmentation, disease recognition, region-aware diagnosis, visual questionanswering, and report generation. Moreover, unlike previous models that rely onclinical experts to pre-diagnose images and manually craft precise textual orvisual prompts, UniBiomed can provide automated and end-to-end groundedinterpretation for biomedical image analysis. This represents a novel paradigmshift in clinical workflows, which will significantly improve diagnosticefficiency. In summary, UniBiomed represents a novel breakthrough in biomedicalAI, unlocking powerful grounded interpretation capabilities for more accurateand efficient biomedical image analysis.</description>
      <author>example@mail.com (Linshan Wu, Yuxiang Nie, Sunan He, Jiaxin Zhuang, Hao Chen)</author>
      <guid isPermaLink="false">2504.21336v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>NEP89: Universal neuroevolution potential for inorganic and organic materials across 89 elements</title>
      <link>http://arxiv.org/abs/2504.21286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为NEP89的机器学习势能模型，该模型基于神经进化势能架构，在89种化学元素上实现了经验势能的速度和高度准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的机器学习势能模型往往针对特定材料或计算密集，限制了其更广泛的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效且准确的机器学习势能模型，以促进原子尺度模拟的广泛应用。&lt;h4&gt;方法&lt;/h4&gt;通过描述符空间子采样和迭代主动学习过程，构建了一个涵盖无机和有机材料的89种元素的紧凑且全面的训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;NEP89在预测性能上经过严格评估，与代表性基础模型相比，展现出可靠性和竞争力。NEP89的计算效率比同类模型高3-4个数量级，使得大规模原子尺度模拟成为可能，并支持在小型数据集上进行微调。&lt;h4&gt;结论&lt;/h4&gt;NEP89是机器学习势能领域的一项重大进步，能够支持高性能的原子尺度模拟，适用于多个研究领域和社区。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learned potentials (MLPs) offer near-first-principles accuracy foratomistic simulations, but many models are material-specific or computationallyintensive, limiting their broader use. Here, we introduce NEP89, a foundationmodel based on the neuroevolution potential (NEP) architecture, deliveringempirical-potential-like speed and high accuracy across 89 chemical elements. Acompact yet comprehensive training dataset covering inorganic and organicmaterials across 89 elements was curated through descriptor-space subsamplingand an iterative active-learning-like process applied to multiple datasets. Werigorously evaluated NEP89's predictive performance against representativefoundation models, demonstrating its reliability and competitive accuracyacross diverse benchmark studies. NEP89 is 3-4 orders of magnitude morecomputationally efficient than comparable models, enabling previouslyimpractical large-scale atomistic simulations for both inorganic and organicsystems. It also supports fine-tuning on small datasets, allowing rapidadaptation to user-specific applications. This work marks a significantadvancement in MLPs, enabling high-performance atomistic simulations acrossdiverse research fields and communities.</description>
      <author>example@mail.com (Ting Liang, Ke Xu, Eric Lindgren, Zherui Chen, Rui Zhao, Jiahui Liu, Benrui Tang, Bohan Zhang, Yanzhou Wang, Keke Song, Penghua Ying, Haikuan Dong, Shunda Chen, Paul Erhart, Zheyong Fan, Tapio Ala-Nissila, Jianbin Xu)</author>
      <guid isPermaLink="false">2504.21286v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks</title>
      <link>http://arxiv.org/abs/2504.20869v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络在应对图对抗攻击时的鲁棒性问题，并提出了基于噪声和分类边界的攻击策略。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在利用局部信息方面具有较强的学习能力，但在对抗攻击下不够鲁棒。&lt;h4&gt;目的&lt;/h4&gt;量化对抗攻击中每个扰动的强度，提高攻击策略的可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了噪声概念来量化对抗链接的攻击强度，并基于噪声和分类边界提出了三种攻击策略，通过单步和多步优化进行攻击。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上进行的实验证明了所提出的攻击策略的有效性，并分析了有效对抗扰动的偏好模式。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效量化攻击强度，并提供具有较高解释性的攻击策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have been widely utilized to solve graph-related tasksbecause of their strong learning power in utilizing the local information ofneighbors. However, recent studies on graph adversarial attacks have proventhat current graph neural networks are not robust against malicious attacks.Yet much of the existing work has focused on the optimization objective basedon attack performance to obtain (near) optimal perturbations, but paid lessattention to the strength quantification of each perturbation such as theinjection of a particular node/link, which makes the choice of perturbations ablack-box model that lacks interpretability. In this work, we propose theconcept of noise to quantify the attack strength of each adversarial link.Furthermore, we propose three attack strategies based on the defined noise andclassification margins in terms of single and multiple steps optimization.Extensive experiments conducted on benchmark datasets against threerepresentative graph neural networks demonstrate the effectiveness of theproposed attack strategies. Particularly, we also investigate the preferredpatterns of effective adversarial perturbations by analyzing the correspondingproperties of the selected perturbation nodes.</description>
      <author>example@mail.com (Junyuan Fang, Han Yang, Haixian Wen, Jiajing Wu, Zibin Zheng, Chi K. Tse)</author>
      <guid isPermaLink="false">2504.20869v2</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model</title>
      <link>http://arxiv.org/abs/2504.21186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图基础模型（GFM）的零样本图域外的检测方法，解决了图结构数据中的零样本OD检测问题。&lt;h4&gt;背景&lt;/h4&gt;零样本OD检测对于确保机器学习系统在动态和开放世界环境中的安全和可靠性至关重要。在视觉和文本领域，大规模预训练模型如视觉-语言模型（VLMs）和大型语言模型（LLMs）已经使零样本OD检测取得了显著进展。然而，由于图结构数据的复杂关系结构和缺乏针对图的强大、大规模预训练模型，图域外的零样本OD检测尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过利用图基础模型（GFM）实现零样本图域外的检测，并针对OD标签名称不可用的情况，提出一种新的框架GLIP-OOD。&lt;h4&gt;方法&lt;/h4&gt;1. 利用GFM在只有类别标签名称的情况下进行OD检测，无需节点级监督，在多个数据集上优于现有的监督方法。2. 引入GLIP-OOD框架，使用LLMs从无标签数据中生成语义信息丰富的伪OD标签，使GFM能够捕捉ID和OD类别之间的细微语义边界，并执行细粒度OD检测。&lt;h4&gt;主要发现&lt;/h4&gt;本文提出的方法是第一个在完全零样本设置中实现节点级图域外检测的方法，并在四个基准文本属性图数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为图结构数据中的零样本OD检测提供了新的思路，对于提高机器学习系统的安全和可靠性具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution (OOD) detection is critical for ensuring the safety andreliability of machine learning systems, particularly in dynamic and open-worldenvironments. In the vision and text domains, zero-shot OOD detection - whichrequires no training on in-distribution (ID) data - has made significantprogress through the use of large-scale pretrained models such asvision-language models (VLMs) and large language models (LLMs). However,zero-shot OOD detection in graph-structured data remains largely unexplored,primarily due to the challenges posed by complex relational structures and theabsence of powerful, large-scale pretrained models for graphs. In this work, wetake the first step toward enabling zero-shot graph OOD detection by leveraginga graph foundation model (GFM). We show that, when provided only with classlabel names, the GFM can perform OOD detection without any node-levelsupervision - outperforming existing supervised methods across multipledatasets. To address the more practical setting where OOD label names areunavailable, we introduce GLIP-OOD, a novel framework that employs LLMs togenerate semantically informative pseudo-OOD labels from unlabeled data. Theselabels enable the GFM to capture nuanced semantic boundaries between ID and OODclasses and perform fine-grained OOD detection - without requiring any labelednodes. Our approach is the first to enable node-level graph OOD detection in afully zero-shot setting, and achieves state-of-the-art performance on fourbenchmark text-attributed graph datasets.</description>
      <author>example@mail.com (Haoyan Xu, Zhengtao Yao, Xuzhi Zhang, Ziyi Wang, Langzhou He, Yushun Dong, Philip S. Yu, Mengyuan Li, Yue Zhao)</author>
      <guid isPermaLink="false">2504.21186v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning</title>
      <link>http://arxiv.org/abs/2504.21099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  survey paper, under updating&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了在联邦学习环境中整合参数高效微调（PEFT）技术的进展。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过在大规模数据集上预训练提供了强大而通用的架构，但针对特定任务进行微调需要大量计算资源。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供PEFT技术在联邦学习环境中的全面回顾，并分析其在数据异质性、通信效率、计算限制和隐私问题等挑战上的应用。&lt;h4&gt;方法&lt;/h4&gt;将现有方法分为三类：添加PEFT（引入新的可训练参数）、选择性PEFT（仅微调现有参数的子集）和重新参数化PEFT（转换模型架构以实现高效更新）。&lt;h4&gt;主要发现&lt;/h4&gt;分析了不同方法如何解决联邦设置中的独特挑战，并按应用领域组织文献，包括自然语言处理和计算机视觉任务。&lt;h4&gt;结论&lt;/h4&gt;讨论了有前景的研究方向，包括扩展到更大的基础模型、联邦PEFT方法的理论分析和资源受限环境中的可持续方法。&lt;h4&gt;翻译&lt;/h4&gt;本文综述了在联邦学习环境中整合参数高效微调（PEFT）技术的进展。基础模型通过在大规模数据集上预训练提供了强大而通用的架构，但针对特定任务进行微调需要大量计算资源。本文旨在提供PEFT技术在联邦学习环境中的全面回顾，并分析其在数据异质性、通信效率、计算限制和隐私问题等挑战上的应用。将现有方法分为三类：添加PEFT（引入新的可训练参数）、选择性PEFT（仅微调现有参数的子集）和重新参数化PEFT（转换模型架构以实现高效更新）。分析了不同方法如何解决联邦设置中的独特挑战，并按应用领域组织文献，包括自然语言处理和计算机视觉任务。讨论了有前景的研究方向，包括扩展到更大的基础模型、联邦PEFT方法的理论分析和资源受限环境中的可持续方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have revolutionized artificial intelligence by providingrobust, versatile architectures pre-trained on large-scale datasets. However,adapting these massive models to specific downstream tasks requiresfine-tuning, which can be prohibitively expensive in computational resources.Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge byselectively updating only a small subset of parameters. Meanwhile, FederatedLearning (FL) enables collaborative model training across distributed clientswithout sharing raw data, making it ideal for privacy-sensitive applications.This survey provides a comprehensive review of the integration of PEFTtechniques within federated learning environments. We systematically categorizeexisting approaches into three main groups: Additive PEFT (which introduces newtrainable parameters), Selective PEFT (which fine-tunes only subsets ofexisting parameters), and Reparameterized PEFT (which transforms modelarchitectures to enable efficient updates). For each category, we analyze howthese methods address the unique challenges of federated settings, includingdata heterogeneity, communication efficiency, computational constraints, andprivacy concerns. We further organize the literature based on applicationdomains, covering both natural language processing and computer vision tasks.Finally, we discuss promising research directions, including scaling to largerfoundation models, theoretical analysis of federated PEFT methods, andsustainable approaches for resource-constrained environments.</description>
      <author>example@mail.com (Jieming Bian, Yuanzhe Peng, Lei Wang, Yin Huang, Jie Xu)</author>
      <guid isPermaLink="false">2504.21099v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Learning Hierarchical Interaction for Accurate Molecular Property Prediction</title>
      <link>http://arxiv.org/abs/2504.20127v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的深度学习模型HimNet，用于发现具有理想分子属性的分子，并通过在多个基准数据集上的实验证明其在分子属性预测任务中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;药物发现过程中，了解分子的吸收、分布、代谢、排泄和毒性（ADMET）属性至关重要。目前常用的深度学习模型如GNN和Transformer在处理分子结构的多层次特性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型的局限性，本文旨在提出一种能够有效捕捉分子结构层次特性，并通过有效特征交互来预测分子属性的深度学习模型。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一个分层交互消息传递机制，作为新模型HimNet的基础。该方法通过分层注意力引导的消息传递，使模型能够在原子、基序和分子层面进行交互感知的表示学习，平衡全局和局部信息，以实现对下游属性预测任务的丰富特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，HimNet在大多数分子属性预测任务中达到了最佳或接近最佳的性能，并展现出良好的层次可解释性，与化学直觉对代表性分子的理解相吻合。&lt;h4&gt;结论&lt;/h4&gt;HimNet为分子活性和ADMET属性预测提供了一个准确高效的方法，对药物发现早期阶段的决策有重大贡献。&lt;h4&gt;翻译&lt;/h4&gt;Discovering molecules with desirable molecular properties, including ADMET profiles, is of great importance in drug discovery. Existing approaches typically employ deep learning models, such as Graph Neural Networks (GNNs) and Transformers, to predict these molecular properties by learning from diverse chemical information. However, these models often fail to efficiently capture and utilize the hierarchical nature of molecular structures, and lack mechanisms for effective interaction among multi-level features. To address these limitations, we propose a Hierarchical Interaction Message Passing Mechanism, which serves as the foundation of our novel model, HimNet. Our method enables interaction-aware representation learning across atomic, motif, and molecular levels via hierarchical attention-guided message passing. This design allows HimNet to effectively balance global and local information, ensuring rich and task-relevant feature extraction for downstream property prediction tasks, such as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple benchmark datasets demonstrate that HimNet achieves the best or near-best performance in most molecular property prediction tasks. Furthermore, our method exhibits promising hierarchical interpretability, aligning well with chemical intuition on representative molecules. We believe that HimNet offers an accurate and efficient solution for molecular activity and ADMET property prediction, contributing significantly to advanced decision-making in the early stages of drug discovery.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discovering molecules with desirable molecular properties, including ADMET(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is ofgreat importance in drug discovery. Existing approaches typically employ deeplearning models, such as Graph Neural Networks (GNNs) and Transformers, topredict these molecular properties by learning from diverse chemicalinformation. However, these models often fail to efficiently capture andutilize the hierarchical nature of molecular structures, and lack mechanismsfor effective interaction among multi-level features. To address theselimitations, we propose a Hierarchical Interaction Message Passing Mechanism,which serves as the foundation of our novel model, HimNet. Our method enablesinteraction-aware representation learning across atomic, motif, and molecularlevels via hierarchical attention-guided message passing. This design allowsHimNet to effectively balance global and local information, ensuring rich andtask-relevant feature extraction for downstream property prediction tasks, suchas Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiplebenchmark datasets demonstrate that HimNet achieves the best or near-bestperformance in most molecular property prediction tasks. Furthermore, ourmethod exhibits promising hierarchical interpretability, aligning well withchemical intuition on representative molecules. We believe that HimNet offersan accurate and efficient solution for molecular activity and ADMET propertyprediction, contributing significantly to advanced decision-making in the earlystages of drug discovery.</description>
      <author>example@mail.com (Huiyang Hong, Xinkai Wu, Hongyu Sun, Chaoyang Xie, Qi Wang, Yuquan Li)</author>
      <guid isPermaLink="false">2504.20127v2</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS</title>
      <link>http://arxiv.org/abs/2504.21064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于图神经网络（GNN）的抑郁症诊断新方法，利用离散傅里叶变换（DFT）和时序图卷积网络（TGCN）构建模型，并通过倾向得分匹配（PSM）提高了数据集质量。&lt;h4&gt;背景&lt;/h4&gt;数据驱动的抑郁症诊断成为神经医学的研究重点，GNN模型因能捕捉大脑通道的时空联系而广泛应用，但缺乏鲁棒的时间生物标志物限制了其效果。&lt;h4&gt;目的&lt;/h4&gt;引入一个新型有效的时间生物标志物，提高抑郁症诊断的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用DFT方法设计时间生物标志物，构建基于TGCN的定制图网络架构，使用包含1086个受试者的数据集进行训练，并进行了倾向得分匹配（PSM）来创建更精细的数据集。使用SHAP方法进行模型可解释性验证。&lt;h4&gt;主要发现&lt;/h4&gt;新设计的生物标志物增强了大脑通道的时间特征表示，提高了F1分数，在现实世界数据和PSM数据集上均有提升。&lt;h4&gt;结论&lt;/h4&gt;该方法有助于开发更有效的抑郁症诊断工具，并通过SHAP方法验证了模型的解释性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：数据驱动的抑郁症诊断方法已成为神经医学的研究重点，这一进展得益于相关数据集的发展。近年来，基于图神经网络（GNN）的模型因其能够从时空角度捕捉大脑通道的功能连通性而得到了广泛应用。然而，它们的效果受到了缺乏鲁棒时间生物标志物的限制。在本研究中，我们通过利用离散傅里叶变换（DFT）提出了一种新颖且有效的抑郁症诊断生物标志物，并提出了一种基于时序图卷积网络（TGCN）的定制图网络架构。我们的模型在一个包含1086个受试者的数据集上进行训练，这个数据集比该领域之前的所有数据集都要大10倍以上。此外，为了满足医学要求，我们对数据进行倾向得分匹配（PSM），创建了一个更精细的数据子集，称为PSM数据集。实验结果表明，结合我们的新设计的生物标志物增强了大脑通道的时间特征表示，从而在现实世界数据和PSM数据集上都提高了F1分数。这一进展有可能为更有效的抑郁症诊断工具的开发做出贡献。此外，我们还使用了SHapley Additive exPlanation（SHAP）方法来验证我们模型的解释性，确保其在医疗环境中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data-driven approaches for depression diagnosis have emerged as a significantresearch focus in neuromedicine, driven by the development of relevantdatasets. Recently, graph neural network (GNN)-based models have gainedwidespread adoption due to their ability to capture brain channel functionalconnectivity from both spatial and temporal perspectives. However, theireffectiveness is hindered by the absence of a robust temporal biomarker. Inthis paper, we introduce a novel and effective biomarker for depressiondiagnosis by leveraging the discrete Fourier transform (DFT) and propose acustomized graph network architecture based on Temporal Graph ConvolutionalNetwork (TGCN). Our model was trained on a dataset comprising 1,086 subjects,which is over 10 times larger than previous datasets in the field of depressiondiagnosis. Furthermore, to align with medical requirements, we performedpropensity score matching (PSM) to create a refined subset, referred to as thePSM dataset. Experimental results demonstrate that incorporating our newlydesigned biomarker enhances the representation of temporal characteristics inbrain channels, leading to improved F1 scores in both the real-world datasetand the PSM dataset. This advancement has the potential to contribute to thedevelopment of more effective depression diagnostic tools. In addition, we usedSHapley Additive exPlaination (SHAP) to validate the interpretability of ourmodel, ensuring its practical applicability in medical settings.</description>
      <author>example@mail.com (Chengkai Yang, Xingping Dong, Xiaofen Zong)</author>
      <guid isPermaLink="false">2504.21064v1</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>LLMs for Engineering: Teaching Models to Design High Powered Rockets</title>
      <link>http://arxiv.org/abs/2504.19394v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了大型语言模型（LLMs）在火箭工程设计中的应用，发现虽然LLMs在工程知识方面表现良好，但通过强化学习（RL）增强后，模型的表现优于现有技术和人类专家。&lt;h4&gt;背景&lt;/h4&gt;LLMs在软件工程领域已有显著应用，但在物理工程领域的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;评估LLMs在火箭工程设计中的能力。&lt;h4&gt;方法&lt;/h4&gt;通过RocketBench，一个将LLMs与高保真火箭模拟连接的基准，测试模型在两个越来越复杂的设计任务上的表现：目标高度优化和精确着陆挑战。&lt;h4&gt;主要发现&lt;/h4&gt;先进的LLMs表现出强大的工程基础知识，但在给定模拟结果后难以迭代其设计，最终表现低于人类水平。然而，通过强化学习增强后，一个7B参数的模型在表现上优于现有的SoTA基础模型和人类专家。&lt;h4&gt;结论&lt;/h4&gt;强化学习训练的LLMs可以成为复杂工程优化的有效工具，有望改变软件开发之外的工程领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have transformed software engineering, but theirapplication to physical engineering domains remains underexplored. This paperevaluates LLMs' capabilities in high-powered rocketry design throughRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.We test models on two increasingly complex design tasks: target altitudeoptimization and precision landing challenges. Our findings reveal that whilestate-of-the-art LLMs demonstrate strong baseline engineering knowledge, theystruggle to iterate on their designs when given simulation results andultimately plateau below human performance levels. However, when enhanced withreinforcement learning (RL), we show that a 7B parameter model outperforms bothSoTA foundation models and human experts. This research demonstrates thatRL-trained LLMs can serve as effective tools for complex engineeringoptimization, potentially transforming engineering domains beyond softwaredevelopment.</description>
      <author>example@mail.com (Toby Simonds)</author>
      <guid isPermaLink="false">2504.19394v2</guid>
      <pubDate>Thu, 01 May 2025 14:15:22 +0800</pubDate>
    </item>
    <item>
      <title>Comments on the minimal training set for CNN: a case study of the frustrated $J_1$-$J_2$ Ising model on the square lattice</title>
      <link>http://arxiv.org/abs/2504.19795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  46 figures, 16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文详细探讨了训练工作CNN所需的最小训练集，并针对挫败的$J_1$-$J_2$伊辛模型在正方形格子上进行了研究。&lt;h4&gt;背景&lt;/h4&gt;考虑的模型是挫败的$J_1$-$J_2$伊辛模型，其中$J_1 &lt; 0$和$J_2 &gt; 0$分别是最近邻和次近邻耦合。&lt;h4&gt;目的&lt;/h4&gt;研究训练CNN所需的最小训练集，并利用训练好的CNN研究$g = 0.8$的相变。&lt;h4&gt;方法&lt;/h4&gt;使用$g = J_2/|J_1| = 0.7$的配置来训练CNN，并用训练好的CNN来研究$g = 0.8$的相变。发现转移学习是成功的，只需要两种温度下的配置（一个低于临界温度$T_c$，一个高于临界温度$T_c$）即可准确确定$g = 0.8$的$T_c$。&lt;h4&gt;主要发现&lt;/h4&gt;发现使用单个自旋翻转算法在低温区域采样配置时效率低下，因此训练集中关联的两个温度不应与$g = 0.7$的$T_c$相距太远，否则获得的CNN性能不高，无法准确确定$g = 0.8$的$T_c$。同时，揭示了只考虑两种温度配置作为训练集时训练成功CNN的条件。&lt;h4&gt;结论&lt;/h4&gt;通过转移学习，使用两种温度的配置即可准确确定$g = 0.8$的$T_c$，但需注意配置的温度不应与$g = 0.7$的$T_c$相差太远，以保证CNN的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The minimal training set to train a working CNN is explored in detail. Theconsidered model is the frustrated $J_1$-$J_2$ Ising model on the squarelattice. Here $J_1 &lt; 0$ and $J_2 &gt; 0$ are the nearest and next-to-nearestneighboring couplings, respectively. We train the CNN using the configurationsof $g \stackrel{\text{def}}{=} J_2/|J_1| = 0.7$ and employ the resulting CNN tostudy the phase transition of $g = 0.8$. We find that this transfer learning issuccessful. In particular, only configurations of two temperatures, one isbelow and one is above the critical temperature $T_c$ of $g=0.7$, are needed toreach accurately determination of the $T_c$ of $g=0.8$. However, it may besubtle to use this strategy for the training. Specifically, for the consideredmodel, due to the inefficiency of the single spin flip algorithm used insampling the configurations at the low-temperature region, the two temperaturesassociated with the training set should not be too far away from the $T_c$ of$g=0.7$, otherwise, the performance of the obtained CNN is not of high quality,hence cannot determine the $T_c$ of $g=0.8$ accurately. For the consideredmodel, we also uncover the condition for training a successful CNN when onlyconfigurations of two temperatures are considered as the training set.</description>
      <author>example@mail.com (Shang-Wei Li, Yuan-Heng Tseng, Ming-Che Hsieh, Fu-Jiun Jiang)</author>
      <guid isPermaLink="false">2504.19795v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
  <item>
      <title>ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery</title>
      <link>http://arxiv.org/abs/2504.19684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合生成域适应和高效对比学习的可扩展框架，以提升从低质量交通摄像头图像中准确分类天气的能力，特别是在夜间恶劣条件下。&lt;h4&gt;背景&lt;/h4&gt;准确从低质量交通摄像头图像中分类天气是一个具有挑战性的任务，尤其是在夜间不利条件下。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效提升夜间天气分类准确性的框架。&lt;h4&gt;方法&lt;/h4&gt;使用基于CycleGAN的域转换技术提升夜间图像质量，结合SigLIP-2（Sigmoid对比损失）进行对比学习，以及结合Vision-SigLIP-2、Text-SigLIP-2、CycleGAN和对比训练。&lt;h4&gt;主要发现&lt;/h4&gt;使用CLIP对比损失的EVA-02模型在整体准确率上达到96.55%，但白天（97.21%）和夜间（63.40%）存在性能差距。替换CLIP为SigLIP-2后，整体准确率提升至94.00%，夜间准确率提高至85.90%。结合所有技术的模型在夜间准确率上达到85.90%，而EVA-02结合CycleGAN在整体准确率和每类准确率上保持最高（97.01%）。&lt;h4&gt;结论&lt;/h4&gt;域适应和高效对比学习结合的方法有望构建实用、资源高效的天气分类系统，适用于智能交通基础设施。&lt;h4&gt;翻译&lt;/h4&gt;Accurate weather classification from low-quality traffic camera imagery remains a challenging task, particularly under adverse nighttime conditions. In this study, we propose a scalable framework that combines generative domain adaptation with efficient contrastive learning to enhance classification performance. Using CycleGAN-based domain translation, we improve the quality of nighttime images, enabling better feature extraction by downstream models. While the baseline EVA-02 model employing CLIP-based contrastive loss achieves an overall accuracy of 96.55%, it exhibits a significant performance gap between daytime (97.21%) and nighttime conditions (63.40%). Replacing CLIP with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive overall accuracy of 94.00%, with substantial improvements in nighttime performance (85.90% accuracy). The combination of Vision-SigLIP-2, Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime accuracy (85.90%) among all models tested, while EVA-02 with CycleGAN maintains the highest overall accuracy (97.01%) and per-class accuracies. These findings demonstrate the potential of combining domain adaptation and efficient contrastive learning to build practical, resource-efficient weather classification systems for intelligent transportation infrastructure.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate weather classification from low-quality traffic camera imageryremains a challenging task, particularly under adverse nighttime conditions. Inthis study, we propose a scalable framework that combines generative domainadaptation with efficient contrastive learning to enhance classificationperformance. Using CycleGAN-based domain translation, we improve the quality ofnighttime images, enabling better feature extraction by downstream models.While the baseline EVA-02 model employing CLIP-based contrastive loss achievesan overall accuracy of 96.55\%, it exhibits a significant performance gapbetween daytime (97.21\%) and nighttime conditions (63.40\%). Replacing CLIPwith the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitiveoverall accuracy of 94.00\%, with substantial improvements in nighttimeperformance (85.90\% accuracy). The combination of Vision-SigLIP-2,Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttimeaccuracy (85.90\%) among all models tested, while EVA-02 with CycleGANmaintains the highest overall accuracy (97.01\%) and per-class accuracies.These findings demonstrate the potential of combining domain adaptation andefficient contrastive learning to build practical, resource-efficient weatherclassification systems for intelligent transportation infrastructure.</description>
      <author>example@mail.com (Anush Lakshman Sivaraman, Kojo Adu-Gyamfi, Ibne Farabi Shihab, Anuj Sharma)</author>
      <guid isPermaLink="false">2504.19684v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification</title>
      <link>http://arxiv.org/abs/2504.20930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ChestX-Reasoner的放射学诊断多模态大型语言模型（MLLM），该模型通过利用从临床报告中挖掘的过程监督来增强推理能力，并提出了RadRBench-CXR基准和RadRScore评估指标，显著提升了诊断准确性和推理能力。&lt;h4&gt;背景&lt;/h4&gt;尽管推理增强的大型语言模型和多模态大型语言模型在复杂任务中的性能有了显著提升，但医疗AI模型往往忽略了临床实践中固有的结构化推理过程。&lt;h4&gt;目的&lt;/h4&gt;设计ChestX-Reasoner，以利用从临床报告中挖掘的过程监督，反映放射科医生逐步推理的过程。&lt;h4&gt;方法&lt;/h4&gt;通过从常规放射学报告中提取和细化推理链来构建大型数据集。采用两阶段训练框架，结合监督微调和由过程奖励引导的强化学习，以更好地使模型推理与临床标准对齐。&lt;h4&gt;主要发现&lt;/h4&gt;ChestX-Reasoner在诊断准确性和推理能力方面优于现有的医疗和通用领域MLLM，与最佳医疗MLLM、最佳通用MLLM及其基模型相比，推理能力分别提高了16%、5.9%和18%，在结果准确性方面分别提高了3.3%、24%和27%。&lt;h4&gt;结论&lt;/h4&gt;所有资源均已开源，以促进医疗推理MLLM的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces ChestX-Reasoner, a radiology diagnosis multimodal large language model (MLLM) designed to enhance reasoning capability by leveraging process supervision mined directly from clinical reports, and reflects the step-by-step reasoning followed by radiologists. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in reasoning-enhanced large language models (LLMs) andmultimodal LLMs (MLLMs) have significantly improved performance in complextasks, yet medical AI models often overlook the structured reasoning processesinherent in clinical practice. In this work, we present ChestX-Reasoner, aradiology diagnosis MLLM designed to leverage process supervision mineddirectly from clinical reports, reflecting the step-by-step reasoning followedby radiologists. We construct a large dataset by extracting and refiningreasoning chains from routine radiology reports. Our two-stage trainingframework combines supervised fine-tuning and reinforcement learning guided byprocess rewards to better align model reasoning with clinical standards. Weintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visualquestion answering samples with 301K clinically validated reasoning steps, andpropose RadRScore, a metric evaluating reasoning factuality, completeness, andeffectiveness. ChestX-Reasoner outperforms existing medical and general-domainMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,and 18% improvements in reasoning ability compared to the best medical MLLM,the best general MLLM, and its base model, respectively, as well as 3.3%, 24%,and 27% improvements in outcome accuracy. All resources are open-sourced tofacilitate further research in medical reasoning MLLMs.</description>
      <author>example@mail.com (Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie)</author>
      <guid isPermaLink="false">2504.20930v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation</title>
      <link>http://arxiv.org/abs/2504.20830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CMT的CAD生成框架，用于解决现有CAD方法在复杂设计要求下的不足。同时，开发了一个大规模的多模态CAD数据集mmABC。&lt;h4&gt;背景&lt;/h4&gt;现有CAD方法在复杂设计要求下存在表现不足，原因在于其过于简化的表示或架构无法支持多模态设计需求。&lt;h4&gt;目的&lt;/h4&gt;通过改进方法和数据集来提高CAD生成的准确性和用户友好性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于边界表示（B-Rep）的多模态CAD生成框架CMT，包括拓扑预测器。同时，开发了一个包含超过130万个B-Rep模型的大规模数据集mmABC，包含点云、文本描述和多视角图像等多模态标注。&lt;h4&gt;主要发现&lt;/h4&gt;CMT在条件和无条件CAD生成任务中均表现出优越性。与现有方法相比，CMT在无条件的生成任务中提高了覆盖率和有效率的10.68%和10.3%，在mmABC上的图像条件CAD生成任务中Chamfer距离减少了4.01。&lt;h4&gt;结论&lt;/h4&gt;CMT框架和数据集mmABC能够显著提高CAD生成的质量和效率，并将相关资源（数据集、代码和预训练网络）公开。&lt;h4&gt;翻译&lt;/h4&gt;While accurate and user-friendly Computer-Aided Design (CAD) is crucial for industrial design and manufacturing, existing methods still struggle to achieve this due to their over-simplified representations or architectures incapable of supporting multimodal design requirements. In this paper, we attempt to tackle this problem from both methods and datasets aspects. First, we propose a cascade MAR with topology predictor (CMT), the first multimodal framework for CAD generation based on Boundary Representation (B-Rep). Specifically, the cascade MAR can effectively capture the 'edge-counters-surface' priors that are essential in B-Reps, while the topology predictor directly estimates topology in B-Reps from the compact tokens in MAR. Second, to facilitate large-scale training, we develop a large-scale multimodal CAD dataset, mmABC, which includes over 1.3 million B-Rep models with multimodal annotations, including point clouds, text descriptions, and multi-view images. Extensive experiments show the superior of CMT in both conditional and unconditional CAD generation tasks. For example, we improve Coverage and Valid ratio by +10.68% and +10.3%, respectively, compared to state-of-the-art methods on ABC in unconditional generation. CMT also improves +4.01 Chamfer on image conditioned CAD generation on mmABC. The dataset, code and pretrained network shall be released.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While accurate and user-friendly Computer-Aided Design (CAD) is crucial forindustrial design and manufacturing, existing methods still struggle to achievethis due to their over-simplified representations or architectures incapable ofsupporting multimodal design requirements. In this paper, we attempt to tacklethis problem from both methods and datasets aspects. First, we propose acascade MAR with topology predictor (CMT), the first multimodal framework forCAD generation based on Boundary Representation (B-Rep). Specifically, thecascade MAR can effectively capture the ``edge-counters-surface'' priors thatare essential in B-Reps, while the topology predictor directly estimatestopology in B-Reps from the compact tokens in MAR. Second, to facilitatelarge-scale training, we develop a large-scale multimodal CAD dataset, mmABC,which includes over 1.3 million B-Rep models with multimodal annotations,including point clouds, text descriptions, and multi-view images. Extensiveexperiments show the superior of CMT in both conditional and unconditional CADgeneration tasks. For example, we improve Coverage and Valid ratio by +10.68%and +10.3%, respectively, compared to state-of-the-art methods on ABC inunconditional generation. CMT also improves +4.01 Chamfer on image conditionedCAD generation on mmABC. The dataset, code and pretrained network shall bereleased.</description>
      <author>example@mail.com (Jianyu Wu, Yizhou Wang, Xiangyu Yue, Xinzhu Ma, Jingyang Guo, Dongzhan Zhou, Wanli Ouyang, Shixiang Tang)</author>
      <guid isPermaLink="false">2504.20830v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2504.20501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RRL-MedSAM的新型框架，用于解决单次医学图像分割的问题，通过利用SAM编码器的泛化能力来学习更好的特征表示。&lt;h4&gt;背景&lt;/h4&gt;单次医学图像分割对于医学分析至关重要，但传统方法依赖于专家手动标注，且计算成本高。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，以适应单次3D医学图像分割，减少对专家标注的依赖和降低计算成本。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了RRL-MedSAM框架，利用SAM编码器的泛化能力。2. 采用双阶段知识蒸馏策略，从基础模型中提取自然和医学图像之间的通用知识。3. 使用互指数移动平均（mutual-EMA）更新通用轻量级编码器和医学特定编码器的权重。4. 引入自动提示（AP）分割解码器，利用通用轻量级模型生成的掩码来辅助医学特定模型。5. 使用伪标签进行互监督。&lt;h4&gt;主要发现&lt;/h4&gt;在OASIS、CT-lung等三个公开数据集上进行的实验表明，RRL-MedSAM在分割和配准任务上优于现有的单次医学图像分割方法。特别是，与SAM-Base的编码器相比，我们的轻量级编码器仅使用了3%的参数。&lt;h4&gt;结论&lt;/h4&gt;RRL-MedSAM框架能够有效提高单次医学图像分割的性能，同时减少计算资源的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One-shot medical image segmentation (MIS) is crucial for medical analysis dueto the burden of medical experts on manual annotation. The recent emergence ofthe segment anything model (SAM) has demonstrated remarkable adaptation in MISbut cannot be directly applied to one-shot medical image segmentation (MIS) dueto its reliance on labor-intensive user interactions and the high computationalcost. To cope with these limitations, we propose a novel SAM-guided robustrepresentation learning framework, named RRL-MedSAM, to adapt SAM to one-shot3D MIS, which exploits the strong generalization capabilities of the SAMencoder to learn better feature representation. We devise a dual-stageknowledge distillation (DSKD) strategy to distill general knowledge betweennatural and medical images from the foundation model to train a lightweightencoder, and then adopt a mutual exponential moving average (mutual-EMA) toupdate the weights of the general lightweight encoder and medical-specificencoder. Specifically, pseudo labels from the registration network are used toperform mutual supervision for such two encoders. Moreover, we introduce anauto-prompting (AP) segmentation decoder which adopts the mask generated fromthe general lightweight model as a prompt to assist the medical-specific modelin boosting the final segmentation performance. Extensive experiments conductedon three public datasets, i.e., OASIS, CT-lung demonstrate that the proposedRRL-MedSAM outperforms state-of-the-art one-shot MIS methods for bothsegmentation and registration tasks. Especially, our lightweight encoder usesonly 3\% of the parameters compared to the encoder of SAM-Base.</description>
      <author>example@mail.com (Jia Wang, Yunan Mei, Jiarui Liu, Xin Fan)</author>
      <guid isPermaLink="false">2504.20501v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Dual Explanations via Subgraph Matching for Malware Detection</title>
      <link>http://arxiv.org/abs/2504.20904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型双原型驱动的可解释框架，用于解释基于图神经网络（GNN）的恶意软件检测决策，在保持高检测性能的同时显著提高了恶意软件分析的可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统可解释方法在图神经网络中突出显示重要区域，但无法将它们与已知的良性或恶意行为模式关联起来，这在安全环境中限制了其效用。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，以解释基于GNN的恶意软件检测决策，并提高其在安全环境中的可解释性。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新的双可解释框架，该框架结合了基线解释器和一种名为SubMatch解释器的二级解释器。SubMatch解释器通过子图匹配技术为节点分配可解释的分数，从而在良性和恶意区域之间提供更精细的区别。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在保持高检测性能的同时，显著提高了恶意软件分析的可解释性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为恶意软件分析提供了一种更加可解释、行为对齐的解释机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interpretable malware detection is crucial for understanding harmfulbehaviors and building trust in automated security systems. Traditionalexplainable methods for Graph Neural Networks (GNNs) often highlight importantregions within a graph but fail to associate them with known benign ormalicious behavioral patterns. This limitation reduces their utility insecurity contexts, where alignment with verified prototypes is essential. Inthis work, we introduce a novel dual prototype-driven explainable frameworkthat interprets GNN-based malware detection decisions. This dual explainableframework integrates a base explainer (a state-of-the-art explainer) with anovel second-level explainer which is designed by subgraph matching technique,called SubMatch explainer. The proposed explainer assigns interpretable scoresto nodes based on their association with matched subgraphs, offering afine-grained distinction between benign and malicious regions. Thisprototype-guided scoring mechanism enables more interpretable, behavior-alignedexplanations. Experimental results demonstrate that our method preserves highdetection performance while significantly improving interpretability in malwareanalysis.</description>
      <author>example@mail.com (Hossein Shokouhinejad, Roozbeh Razavi-Far, Griffin Higgins, Ali A. Ghorbani)</author>
      <guid isPermaLink="false">2504.20904v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks</title>
      <link>http://arxiv.org/abs/2504.20869v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ubder Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于量化攻击强度的噪声概念，并提出基于此的攻击策略，通过实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络因其强大的学习能力被广泛应用于解决图相关任务，但近年来研究表明它们对恶意攻击不够鲁棒。&lt;h4&gt;目的&lt;/h4&gt;为了提高图神经网络的鲁棒性，本文旨在提出一种方法来量化攻击强度并选择合适的攻击策略。&lt;h4&gt;方法&lt;/h4&gt;本文提出了噪声的概念来量化每个对抗链接的攻击强度，并基于噪声和分类边界定义了三种攻击策略。&lt;h4&gt;主要发现&lt;/h4&gt;通过在基准数据集上进行的实验，验证了所提出的攻击策略的有效性，并分析了有效对抗扰动节点的偏好模式。&lt;h4&gt;结论&lt;/h4&gt;本文提出的噪声概念和攻击策略能够有效提高图神经网络的鲁棒性，并提供了对抗扰动选择的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks have been widely utilized to solve graph-related tasks because of their strong learning power in utilizing the local information of neighbors. However, recent studies on graph adversarial attacks have proved that current graph neural networks are not robust against malicious attacks. Yet much of the existing work has focused on the optimization objective based on attack performance to obtain (near) optimal perturbations, but paid less attention to the strength quantification of each perturbation such as the injection of a particular node/link, which makes the choice of perturbations a black-box model that lacks interpretability. In this work, we propose the concept of noise to quantify the attack strength of each adversarial link. Furthermore, we propose three attack strategies based on the defined noise and classification margins in terms of single and multiple steps optimization. Extensive experiments conducted on benchmark datasets against three representative graph neural networks demonstrate the effectiveness of the proposed attack strategies. Particularly, we also investigate the preferred patterns of effective adversarial perturbations by analyzing the corresponding properties of the selected perturbation nodes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have been widely utilized to solve graph-related tasksbecause of their strong learning power in utilizing the local information ofneighbors. However, recent studies on graph adversarial attacks have proventhat current graph neural networks are not robust against malicious attacks.Yet much of the existing work has focused on the optimization objective basedon attack performance to obtain (near) optimal perturbations, but paid lessattention to the strength quantification of each perturbation such as theinjection of a particular node/link, which makes the choice of perturbations ablack-box model that lacks interpretability. In this work, we propose theconcept of noise to quantify the attack strength of each adversarial link.Furthermore, we propose three attack strategies based on the defined noise andclassification margins in terms of single and multiple steps optimization.Extensive experiments conducted on benchmark datasets against threerepresentative graph neural networks demonstrate the effectiveness of theproposed attack strategies. Particularly, we also investigate the preferredpatterns of effective adversarial perturbations by analyzing the correspondingproperties of the selected perturbation nodes.</description>
      <author>example@mail.com (Junyuan Fang, Han Yang, Haixian Wen, Jiajing Wu, Zibin Zheng, Chi K. Tse)</author>
      <guid isPermaLink="false">2504.20869v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability</title>
      <link>http://arxiv.org/abs/2504.20667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ILLUME的灵活且可解释的框架，用于理解黑盒机器学习模型，该框架可以与各种代理模型集成，提供对任何黑盒分类器的解释。&lt;h4&gt;背景&lt;/h4&gt;后验可解释性对于理解黑盒机器学习模型至关重要。基于代理的技术在局部和全局模型无关的解释中广泛使用，但存在显著局限性。&lt;h4&gt;目的&lt;/h4&gt;提出ILLUME框架，以提供准确、鲁棒且忠实于黑盒的特徵归因和决策规则，从而有效解决传统代理方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;ILLUME结合了一个全局训练的代理模型和通过元编码器学习的实例特定线性变换，以生成局部和全局解释。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实证评估，证明了ILLUME在生成特征归因和决策规则方面的有效性，这些规则不仅准确，而且鲁棒且忠实于黑盒。&lt;h4&gt;结论&lt;/h4&gt;ILLUME是一个统一的解释框架，能够有效解决传统代理方法的局限性，为理解黑盒机器学习模型提供了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Post-hoc explainability is essential for understanding black-box machinelearning models. Surrogate-based techniques are widely used for local andglobal model-agnostic explanations but have significant limitations. Localsurrogates capture non-linearities but are computationally expensive andsensitive to parameters, while global surrogates are more efficient butstruggle with complex local behaviors. In this paper, we present ILLUME, aflexible and interpretable framework grounded in representation learning, thatcan be integrated with various surrogate models to provide explanations for anyblack-box classifier. Specifically, our approach combines a globally trainedsurrogate with instance-specific linear transformations learned with ameta-encoder to generate both local and global explanations. Through extensiveempirical evaluations, we demonstrate the effectiveness of ILLUME in producingfeature attributions and decision rules that are not only accurate but alsorobust and faithful to the black-box, thus providing a unified explanationframework that effectively addresses the limitations of traditional surrogatemethods.</description>
      <author>example@mail.com (Simone Piaggesi, Riccardo Guidotti, Fosca Giannotti, Dino Pedreschi)</author>
      <guid isPermaLink="false">2504.20667v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>DB-GNN: Dual-Branch Graph Neural Network with Multi-Level Contrastive Learning for Jointly Identifying Within- and Cross-Frequency Coupled Brain Networks</title>
      <link>http://arxiv.org/abs/2504.20744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双重分支图神经网络（DB-GNN），用于联合识别脑网络中的频率内耦合（WFC）和跨频率耦合（CFC），并通过实验验证了其在情感识别任务中的高效性。&lt;h4&gt;背景&lt;/h4&gt;脑网络中的WFC和CFC分别反映了同一频率带内的神经同步和跨带振荡相互作用，它们的协同作用有助于理解认知状态如情感背后的神经机制。&lt;h4&gt;目的&lt;/h4&gt;提出DB-GNN以更全面地利用WFC和CFC的互补特性，从而更好地理解认知状态。&lt;h4&gt;方法&lt;/h4&gt;DB-GNN利用独特的双重分支学习架构来高效挖掘全局协作信息和局部跨频率及频率内耦合信息；采用Transformer架构来增强对全局信息的感知；通过整合先验的WFC和CFC信息来防止过拟合；引入多尺度图对比学习正则化项来增强全局和局部感知分支的联合感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证表明，DB-GNN在情感识别数据集上实现了97.88%的测试准确率和97.87%的F1分数，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;DB-GNN能够有效识别WFC和CFC，并显著提高情感识别任务的性能，为认知状态的理解提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：脑网络中的频率内耦合（WFC）和跨频率耦合（CFC）分别反映了同一频率带内的神经同步和跨带振荡相互作用。它们的协同作用为理解认知状态（如情感）背后的神经机制提供了全面的视角。然而，现有的多通道脑电图（EEG）研究通常分别分析WFC或CFC，未能充分利用它们的互补特性。本研究提出了一种双重分支图神经网络（DB-GNN）来联合识别频率内和跨频率耦合的脑网络。首先，DB-GNN利用其独特的双重分支学习架构高效挖掘全局协作信息和局部跨频率及频率内耦合信息。其次，为了更全面地感知跨频率和频率内耦合的全局信息，DB-GNN的全局感知分支采用了Transformer架构。为了避免Transformer架构的过拟合，本研究将先验的WFC和CFC信息整合到Transformer推理过程中，从而增强了DB-GNN的泛化能力。最后，引入了多尺度图对比学习正则化项，以约束DB-GNN的全局和局部感知分支在图级别和节点级别，从而增强了其联合感知能力，并进一步提高了其泛化性能。在情感识别数据集上的实验验证表明，DB-GNN实现了97.88%的测试准确率和97.87%的F1分数，达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Within-frequency coupling (WFC) and cross-frequency coupling (CFC) in brainnetworks reflect neural synchronization within the same frequency band andcross-band oscillatory interactions, respectively. Their synergy provides acomprehensive understanding of neural mechanisms underlying cognitive statessuch as emotion. However, existing multi-channel EEG studies often analyze WFCor CFC separately, failing to fully leverage their complementary properties.This study proposes a dual-branch graph neural network (DB-GNN) to jointlyidentify within- and cross-frequency coupled brain networks. Firstly, DBGNNleverages its unique dual-branch learning architecture to efficiently mineglobal collaborative information and local cross-frequency and within-frequencycoupling information. Secondly, to more fully perceive the global informationof cross-frequency and within-frequency coupling, the global perception branchof DB-GNN adopts a Transformer architecture. To prevent overfitting of theTransformer architecture, this study integrates prior within- andcross-frequency coupling information into the Transformer inference process,thereby enhancing the generalization capability of DB-GNN. Finally, amulti-scale graph contrastive learning regularization term is introduced toconstrain the global and local perception branches of DB-GNN at bothgraph-level and node-level, enhancing its joint perception ability and furtherimproving its generalization performance. Experimental validation on theemotion recognition dataset shows that DB-GNN achieves a testing accuracy of97.88% and an F1- score of 97.87%, reaching the state-of-the-art performance.</description>
      <author>example@mail.com (Xiang Wang, Hui Xu, Jing Cai, Ta Zhou, Xibei Yang, Wei Xue)</author>
      <guid isPermaLink="false">2504.20744v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study</title>
      <link>http://arxiv.org/abs/2504.20541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于深度学习的框架，用于从WiFi信道状态信息（CSI）数据生成点云。&lt;h4&gt;背景&lt;/h4&gt;研究背景为从无线信号数据中重建环境点云。&lt;h4&gt;目的&lt;/h4&gt;研究目的是开发一种方法，能够从WiFi数据中准确重建环境点云。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段自动编码器方法：使用具有卷积层的PointNet自动编码器进行点云生成，以及使用卷积神经网络自动编码器将CSI数据映射到匹配的潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法有效，突出了其在无线传感和环境制图应用中的潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法能够通过WiFi数据准确重建环境点云，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces a deep learning framework for generating point clouds from WiFi Channel State Information data. We employ a two-stage autoencoder approach: a PointNet autoencoder with convolutional layers for point cloud generation, and a Convolutional Neural Network autoencoder to map CSI data to a matching latent space. By aligning these latent spaces, our method enables accurate environmental point cloud reconstruction from WiFi data. Experimental results validate the effectiveness of our approach, highlighting its potential for wireless sensing and environmental mapping applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a deep learning framework for generating point cloudsfrom WiFi Channel State Information data. We employ a two-stage autoencoderapproach: a PointNet autoencoder with convolutional layers for point cloudgeneration, and a Convolutional Neural Network autoencoder to map CSI data to amatching latent space. By aligning these latent spaces, our method enablesaccurate environmental point cloud reconstruction from WiFi data. Experimentalresults validate the effectiveness of our approach, highlighting its potentialfor wireless sensing and environmental mapping applications.</description>
      <author>example@mail.com (Daniele Pannone, Danilo Avola)</author>
      <guid isPermaLink="false">2504.20541v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2504.20464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对基于多模态大型语言模型（MLLMs）的图形用户界面（GUI）代理的最新进展进行了结构化总结，重点关注通过强化学习（RL）增强的架构。&lt;h4&gt;背景&lt;/h4&gt;GUI代理作为实现与数字系统智能交互的有前景的方法，近年来得到了快速发展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供对GUI代理领域最近进展的全面概述，特别是那些通过强化学习增强的架构。&lt;h4&gt;方法&lt;/h4&gt;首先，将GUI代理任务形式化为马尔可夫决策过程，并讨论了典型的执行环境和评估指标。接着，回顾了基于（M）LLM的GUI代理的模块化架构，包括感知、规划和行动模块，并追踪了它们的演变过程。此外，将GUI代理的训练方法分为基于提示、基于监督微调（SFT）和基于RL的方法，强调了从简单的提示工程到通过RL进行动态策略学习的进展。&lt;h4&gt;主要发现&lt;/h4&gt;本文说明了在多模态感知、决策推理和自适应动作生成方面的最新创新如何显著提高了GUI代理在复杂真实世界环境中的泛化能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;最后，本文确定了构建更强大和可靠的GUI代理的关键挑战和未来方向。&lt;h4&gt;翻译&lt;/h4&gt;Graphical User Interface (GUI) agents, driven by Multi-modal Large Language Models (MLLMs), have emerged as a promising paradigm for enabling intelligent interaction with digital systems. This paper provides a structured summary of recent advances in GUI agents, focusing on architectures enhanced by Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov Decision Processes and discuss typical execution environments and evaluation metrics. We then review the modular architecture of (M)LLM-based GUI agents, covering Perception, Planning, and Acting modules, and trace their evolution through representative works. Furthermore, we categorize GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, highlighting the progression from simple prompt engineering to dynamic policy learning via RL. Our summary illustrates how recent innovations in multimodal perception, decision reasoning, and adaptive action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. We conclude by identifying key challenges and future directions for building more capable and reliable GUI agents.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical User Interface (GUI) agents, driven by Multi-modal Large LanguageModels (MLLMs), have emerged as a promising paradigm for enabling intelligentinteraction with digital systems. This paper provides a structured summary ofrecent advances in GUI agents, focusing on architectures enhanced byReinforcement Learning (RL). We first formalize GUI agent tasks as MarkovDecision Processes and discuss typical execution environments and evaluationmetrics. We then review the modular architecture of (M)LLM-based GUI agents,covering Perception, Planning, and Acting modules, and trace their evolutionthrough representative works. Furthermore, we categorize GUI agent trainingmethodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, andRL-based approaches, highlighting the progression from simple promptengineering to dynamic policy learning via RL. Our summary illustrates howrecent innovations in multimodal perception, decision reasoning, and adaptiveaction generation have significantly improved the generalization and robustnessof GUI agents in complex real-world environments. We conclude by identifyingkey challenges and future directions for building more capable and reliable GUIagents.</description>
      <author>example@mail.com (Jiahao Li, Kaer Huang)</author>
      <guid isPermaLink="false">2504.20464v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features</title>
      <link>http://arxiv.org/abs/2504.20970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint submitted to IEEE International Workshop on Machine Learning  for Signal Processing (MLSP), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于奇异值分解的最小二乘法（SVD-LS）框架，用于多类肺炎分类，旨在通过高效的诊断工具辅助放射科医生做出更可靠和高效的决策。&lt;h4&gt;背景&lt;/h4&gt;准确且早期通过X光成像诊断肺炎对于有效治疗和改善患者预后至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的肺炎分类方法，以辅助放射科医生进行更准确的诊断。&lt;h4&gt;方法&lt;/h4&gt;利用最先进的自监督和迁移学习模型，采用闭式、非迭代的方法进行分类，避免了梯度优化的计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;SVD-LS在保持准确性的同时，显著降低了计算成本，在实验中表现出与现有方法相竞争的性能。&lt;h4&gt;结论&lt;/h4&gt;SVD-LS是一个可行的实时医学成像应用替代方案，能够提高肺炎诊断的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and early diagnosis of pneumonia through X-ray imaging is essentialfor effective treatment and improved patient outcomes. Recent advancements inmachine learning have enabled automated diagnostic tools that assistradiologists in making more reliable and efficient decisions. In this work, wepropose a Singular Value Decomposition-based Least Squares (SVD-LS) frameworkfor multi-class pneumonia classification, leveraging powerful featurerepresentations from state-of-the-art self-supervised and transfer learningmodels. Rather than relying on computationally expensive gradient basedfine-tuning, we employ a closed-form, non-iterative classification approachthat ensures efficiency without compromising accuracy. Experimental resultsdemonstrate that SVD-LS achieves competitive performance while offeringsignificantly reduced computational costs, making it a viable alternative forreal-time medical imaging applications.</description>
      <author>example@mail.com (Mete Erdogan, Sebnem Demirtas)</author>
      <guid isPermaLink="false">2504.20970v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding</title>
      <link>http://arxiv.org/abs/2504.20384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FiLA-Video的轻量级视频理解框架，旨在解决长视频理解中的复杂性和处理限制问题。&lt;h4&gt;背景&lt;/h4&gt;尽管视觉大语言模型（VLLMs）在视频理解方面取得了显著进展，但视频数据的复杂性和上下文处理限制仍然阻碍了长视频的理解。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，提出了一种新的方法，即视频特征压缩，以减少大型语言模型的token输入，但许多方法要么未能优先考虑关键特征，导致冗余的帧间信息，要么引入了计算成本高的模块。&lt;h4&gt;方法&lt;/h4&gt;FiLA-Video采用了一种轻量级的动态权重多帧融合策略，自适应地将多个帧整合成一个单一表示，同时保留关键视频信息并降低计算成本。为了提高融合中的帧选择，引入了一种关键帧选择策略，有效地从更大的帧池中识别出信息丰富的帧，以改善总结。此外，还提出了一种简单而有效的长视频训练数据生成策略，无需大量手动标注即可提高模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有方法相比，FiLA-Video在长视频理解方面实现了更高的效率和准确性。&lt;h4&gt;结论&lt;/h4&gt;FiLA-Video框架为长视频理解提供了一种高效且准确的方法，有望在视频理解领域得到广泛应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in video understanding within visual large languagemodels (VLLMs) have led to notable progress. However, the complexity of videodata and contextual processing limitations still hinder long-videocomprehension. A common approach is video feature compression to reduce tokeninput to large language models, yet many methods either fail to prioritizeessential features, leading to redundant inter-frame information, or introducecomputationally expensive modules.To address these issues, we proposeFiLA(Fine-grained Vision Language Model)-Video, a novel framework thatleverages a lightweight dynamic-weight multi-frame fusion strategy, whichadaptively integrates multiple frames into a single representation whilepreserving key video information and reducing computational costs. To enhanceframe selection for fusion, we introduce a keyframe selection strategy,effectively identifying informative frames from a larger pool for improvedsummarization. Additionally, we present a simple yet effective long-videotraining data generation strategy, boosting model performance without extensivemanual annotation. Experimental results demonstrate that FiLA-Video achievessuperior efficiency and accuracy in long-video comprehension compared toexisting methods.</description>
      <author>example@mail.com (Yanan Guo, Wenhui Dong, Jun Song, Shiding Zhu, Xuan Zhang, Hanqing Yang, Yingbo Wang, Yang Du, Xianing Chen, Bo Zheng)</author>
      <guid isPermaLink="false">2504.20384v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating the Structural Bias in Graph Adversarial Defenses</title>
      <link>http://arxiv.org/abs/2504.20848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种防御策略，旨在缓解图神经网络（GNNs）在对抗攻击下的结构偏差，并提高其鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;近年来，图神经网络在处理与图结构相关的下游任务中展现出巨大潜力，但现有的GNNs易受恶意对抗攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;提出一种防御策略，以减少GNNs在对抗攻击下的结构偏差，特别是针对低度节点（尾节点）的防御能力。&lt;h4&gt;方法&lt;/h4&gt;策略包括异同质增强图构建、kNN增强图构建和多视图节点级注意力模块。异同质增强图通过移除异质链接（特征不同的节点之间的链接）和为低度节点添加同质链接（特征相似的节点之间的链接）来构建。此外，采用注意力机制以自适应地结合两种图视图的表示。&lt;h4&gt;主要发现&lt;/h4&gt;该策略在基准数据集上展示了防御和去偏效果，表明其能有效缓解GNNs的结构偏差。&lt;h4&gt;结论&lt;/h4&gt;提出的防御策略能够有效提升GNNs在对抗攻击下的鲁棒性，特别是在低度节点方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：近年来，图神经网络（GNNs）在解决各种与图结构相关的下游任务中显示出巨大的潜力。然而，最近的研究发现，当前的GNNs容易受到恶意对抗攻击的影响。鉴于对抗攻击在现实世界中的不可避免性，已经提出了各种防御方法来对抗这些攻击并提高GNNs的鲁棒性。尽管这些防御方法表现出可嘉的性能，但我们观察到它们在防御能力上往往表现出对低度节点（即尾节点）的结构偏差，这与传统GNNs在干净图中对低度节点的结构偏差相似。因此，在本工作中，我们提出了一种防御策略，通过包括异同质增强图构建、kNN增强图构建和多视图节点级注意力模块来缓解GNNs对抗攻击的结构偏差。值得注意的是，异同质增强图由全局移除异质链接（即连接具有不同特征的节点的链接）和为低度节点添加同质链接（即连接具有相似特征的节点的链接）组成。为了进一步增强防御能力，采用了一种注意力机制来自适应地结合上述两种图视图的表示。我们进行了广泛的实验，以证明所提出的策略在基准数据集上的防御和去偏效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, graph neural networks (GNNs) have shown great potential inaddressing various graph structure-related downstream tasks. However, recentstudies have found that current GNNs are susceptible to malicious adversarialattacks. Given the inevitable presence of adversarial attacks in the realworld, a variety of defense methods have been proposed to counter these attacksand enhance the robustness of GNNs. Despite the commendable performance ofthese defense methods, we have observed that they tend to exhibit a structuralbias in terms of their defense capability on nodes with low degree (i.e., tailnodes), which is similar to the structural bias of traditional GNNs on nodeswith low degree in the clean graph. Therefore, in this work, we propose adefense strategy by including hetero-homo augmented graph construction, $k$NNaugmented graph construction, and multi-view node-wise attention modules tomitigate the structural bias of GNNs against adversarial attacks. Notably, thehetero-homo augmented graph consists of removing heterophilic links (i.e.,links connecting nodes with dissimilar features) globally and adding homophiliclinks (i.e., links connecting nodes with similar features) for nodes with lowdegree. To further enhance the defense capability, an attention mechanism isadopted to adaptively combine the representations from the above two kinds ofgraph views. We conduct extensive experiments to demonstrate the defense anddebiasing effect of the proposed strategy on benchmark datasets.</description>
      <author>example@mail.com (Junyuan Fang, Huimin Liu, Han Yang, Jiajing Wu, Zibin Zheng, Chi K. Tse)</author>
      <guid isPermaLink="false">2504.20848v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes</title>
      <link>http://arxiv.org/abs/2504.20303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了DeepAndes，一个专门为安第斯考古学设计的基于Transformer的视觉基础模型，通过大规模的自监督预训练，在考古遥感领域取得了显著成效。&lt;h4&gt;背景&lt;/h4&gt;利用遥感数据在大规模上对遗址进行映射，可以帮助考古学家深入了解长期人口趋势、区域间社会网络以及过去对气候变化的适应。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理多光谱卫星图像的深度学习模型，以解决传统监督学习方法在标注考古特征时遇到的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出DeepAndes模型，该模型基于三百万张多光谱卫星图像进行训练，并采用了定制化的DINOv2自监督学习算法，针对8波段多光谱图像进行了优化。&lt;h4&gt;主要发现&lt;/h4&gt;DeepAndes在图像理解任务中表现出色，包括不平衡图像分类、图像实例检索和像素级语义分割，其F1分数、平均精度和Dice分数在少量样本学习场景中优于从头开始训练或在小数据集上预训练的模型。&lt;h4&gt;结论&lt;/h4&gt;大规模自监督预训练在考古遥感领域是有效的，DeepAndes模型为这一领域提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;通过在大规模上使用遥感数据对遗址进行映射，考古学家可以生成关于长期人口趋势、区域间社会网络和过去对气候变化适应的独特见解。遥感调查补充了基于现场的方法，当与深度学习和计算机视觉技术结合时，其范围可以特别广泛。然而，传统的监督深度学习方法在标注大规模精细考古特征时面临挑战。尽管最近的视觉基础模型在利用最小标注学习大规模遥感数据方面取得了显著的成功，但大多数现成的解决方案是为RGB图像而不是多光谱卫星图像（如我们研究中使用的8波段数据）设计的。在本文中，我们介绍了DeepAndes，这是一个基于Transformer的视觉基础模型，在300万张多光谱卫星图像上进行了训练，专门针对安第斯考古学进行了定制。DeepAndes集成了针对8波段多光谱图像优化的定制化DINOv2自监督学习算法，标志着第一个专门为安第斯地区设计的基座模型。我们通过不平衡图像分类、图像实例检索和像素级语义分割任务评估了其图像理解性能。我们的实验表明，DeepAndes在少量样本学习场景中实现了优越的F1分数、平均精度和Dice分数，显著优于从头开始训练或在小数据集上预训练的模型。这强调了大规模自监督预训练在考古遥感中的有效性。代码将在https://github.com/geopacha/DeepAndes上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By mapping sites at large scales using remotely sensed data, archaeologistscan generate unique insights into long-term demographic trends, inter-regionalsocial networks, and past adaptations to climate change. Remote sensing surveyscomplement field-based approaches, and their reach can be especially great whencombined with deep learning and computer vision techniques. However,conventional supervised deep learning methods face challenges in annotatingfine-grained archaeological features at scale. While recent vision foundationmodels have shown remarkable success in learning large-scale remote sensingdata with minimal annotations, most off-the-shelf solutions are designed forRGB images rather than multi-spectral satellite imagery, such as the 8-banddata used in our study. In this paper, we introduce DeepAndes, atransformer-based vision foundation model trained on three millionmulti-spectral satellite images, specifically tailored for Andean archaeology.DeepAndes incorporates a customized DINOv2 self-supervised learning algorithmoptimized for 8-band multi-spectral imagery, marking the first foundation modeldesigned explicitly for the Andes region. We evaluate its image understandingperformance through imbalanced image classification, image instance retrieval,and pixel-level semantic segmentation tasks. Our experiments show thatDeepAndes achieves superior F1 scores, mean average precision, and Dice scoresin few-shot learning scenarios, significantly outperforming models trained fromscratch or pre-trained on smaller datasets. This underscores the effectivenessof large-scale self-supervised pre-training in archaeological remote sensing.Codes will be available on https://github.com/geopacha/DeepAndes.</description>
      <author>example@mail.com (Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Siqi Lu, Quan Liu, Ruining Deng, Can Cui, Jialin Yue, Yizhe Lin, Tianyuan Yao, Juming Xiong, Junchao Zhu, Chongyu Qu, Yuechen Yang, Mitchell Wilkes, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, Yuankai Huo)</author>
      <guid isPermaLink="false">2504.20303v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>DRO: Doppler-Aware Direct Radar Odometry</title>
      <link>http://arxiv.org/abs/2504.20339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at RSS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于旋转频率调制连续波雷达的SE(2)里程计方法，用于移动机器人应用。&lt;h4&gt;背景&lt;/h4&gt;雷达在移动机器人应用中的重要性正在兴起，尤其是在需要穿透障碍物和恶劣天气条件的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要特征或点云提取的直接扫描到局部地图注册的方法，以提高移动机器人的定位精度。&lt;h4&gt;方法&lt;/h4&gt;该方法利用雷达强度信息进行直接注册，并考虑了运动和多普勒畸变。在特定频率调制模式下，还引入了基于多普勒的约束来提高速度估计。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在超过250公里的公共数据集上进行了验证，与现有方法相比，平均相对位置误差降低了0.26%。在具有适当多普勒调制模式的场景中，位置误差进一步降低至0.18%。&lt;h4&gt;结论&lt;/h4&gt;该方法在几何特征稀缺的场景中表现良好，并且实时实现已经公开。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于旋转频率调制连续波雷达的SE(2)里程计方法，用于移动机器人应用。与相机或激光雷达相比，毫米波雷达能够在薄墙、植被和恶劣天气条件下（如大雨、雾、雪和灰尘）进行探测。本文提出了一种新的SE(2)里程计方法，用于旋转频率调制连续波雷达。该方法以直接方式使用所有雷达强度信息进行输入雷达数据的扫描到局部地图注册，无需特征或点云提取。该方法执行局部连续轨迹估计，并考虑雷达扫描的运动和多普勒畸变。如果雷达具有使径向多普勒速度可观测的特定频率调制模式，则还制定了基于多普勒的约束，以提高速度估计并使里程计在几何特征稀缺的场景（例如无特征隧道）中成为可能。该方法已在超过250公里的公共数据集（Boreas和MulRan）上进行了验证，这些数据是通过我们的汽车平台收集的。在有陀螺仪的帮助下，它优于最先进的方法，在Boreas排行榜上实现了0.26%的平均相对位置误差。当使用具有适当多普勒启用频率调制模式的数据时，在类似环境中，位置误差降低至0.18%。我们还使用在具有不同结构级别的越野环境中收集的1.5小时数据对该算法进行了基准测试，以展示其多功能性。我们的实时实现已经公开：https://github.com/utiasASRL/dro。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A renaissance in radar-based sensing for mobile robotic applications isunderway. Compared to cameras or lidars, millimetre-wave radars have theability to `see' through thin walls, vegetation, and adversarial weatherconditions such as heavy rain, fog, snow, and dust. In this paper, we propose anovel SE(2) odometry approach for spinning frequency-modulated continuous-waveradars. Our method performs scan-to-local-map registration of the incomingradar data in a direct manner using all the radar intensity information withoutthe need for feature or point cloud extraction. The method performs locallycontinuous trajectory estimation and accounts for both motion and Dopplerdistortion of the radar scans. If the radar possesses a specific frequencymodulation pattern that makes radial Doppler velocities observable, anadditional Doppler-based constraint is formulated to improve the velocityestimate and enable odometry in geometrically feature-deprived scenarios (e.g.,featureless tunnels). Our method has been validated on over 250km of on-roaddata sourced from public datasets (Boreas and MulRan) and collected using ourautomotive platform. With the aid of a gyroscope, it outperformsstate-of-the-art methods and achieves an average relative translation error of0.26% on the Boreas leaderboard. When using data with the appropriateDoppler-enabling frequency modulation pattern, the translation error is reducedto 0.18% in similar environments. We also benchmarked our algorithm using 1.5hours of data collected with a mobile robot in off-road environments withvarious levels of structure to demonstrate its versatility. Our real-timeimplementation is publicly available: https://github.com/utiasASRL/dro.</description>
      <author>example@mail.com (Cedric Le Gentil, Leonardo Brizi, Daniil Lisus, Xinyuan Qiao, Giorgio Grisetti, Timothy D. Barfoot)</author>
      <guid isPermaLink="false">2504.20339v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects</title>
      <link>http://arxiv.org/abs/2504.20579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了利用神经网络解决观察数据中治疗效应估计的挑战，包括隐藏混杂因素和协变量不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;由于隐藏混杂因素和协变量不匹配，从观察数据中估计治疗效应具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出神经网络架构，旨在学习有效的调整表示，并满足协变量匹配约束。&lt;h4&gt;方法&lt;/h4&gt;结合了基于梯度匹配的跨域神经网络和协变量匹配变换的神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;证明了近似不变表示可以产生近似有效的调整集，从而能够估计真实因果效应的区间。&lt;h4&gt;结论&lt;/h4&gt;该方法在ATE和PEHE误差方面优于多种基线模型，并在IHDP、Jobs、Cattaneo和基于图像的Crowd Management数据集等因果基准测试中表现良好。&lt;h4&gt;翻译&lt;/h4&gt;摘要：从观察数据中估计治疗效应具有挑战性，主要由于隐藏混杂因素和协变量不匹配。针对这些问题，本文提出了神经网络架构，旨在学习有效的调整表示，并满足协变量匹配约束。结合了基于梯度匹配的跨域神经网络和协变量匹配变换的神经网络。证明了近似不变表示可以产生近似有效的调整集，从而能够估计真实因果效应的区间。在ATE和PEHE误差方面，该方法优于多种基线模型，并在多个因果基准测试中表现良好。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating treatment effects from observational data is challenging due totwo main reasons: (a) hidden confounding, and (b) covariate mismatch (controland treatment groups not having identical distributions). Long lines of worksexist that address only either of these issues. To address the former,conventional techniques that require detailed knowledge in the form of causalgraphs have been proposed. For the latter, covariate matching and importanceweighting methods have been used. Recently, there has been progress incombining testable independencies with partial side information for tacklinghidden confounding. A common framework to address both hidden confounding andselection bias is missing. We propose neural architectures that aim to learn arepresentation of pre-treatment covariates that is a valid adjustment and alsosatisfies covariate matching constraints. We combine two different neuralarchitectures: one based on gradient matching across domains created bysubsampling a suitable anchor variable that assumes causal side information,followed by the other, a covariate matching transformation. We prove thatapproximately invariant representations yield approximate valid adjustment setswhich would enable an interval around the true causal effect. In contrast tousual sensitivity analysis, where an unknown nuisance parameter is varied, wehave a testable approximation yielding a bound on the effect estimate. We alsooutperform various baselines with respect to ATE and PEHE errors on causalbenchmarks that include IHDP, Jobs, Cattaneo, and an image-based CrowdManagement dataset.</description>
      <author>example@mail.com (Praharsh Nanavati, Ranjitha Prasad, Karthikeyan Shanmugam)</author>
      <guid isPermaLink="false">2504.20579v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Under High-Dimensional Network Convolutional Regression Model</title>
      <link>http://arxiv.org/abs/2504.19979v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于网络卷积回归（NCR）的高维迁移学习框架，用于处理网络数据中的依赖性，并通过仿真和实际应用证明了其在预测准确性上的提升。&lt;h4&gt;背景&lt;/h4&gt;迁移学习通过利用相关领域的知识来提高模型性能，尤其是在标注数据稀缺的情况下。然而，在处理网络数据中的依赖性时，现有的研究仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决网络数据中依赖性的处理问题，本文提出了一个基于NCR的迁移学习框架，旨在提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;该方法包括一个两步迁移学习算法，用于解决源网络和目标网络之间的领域偏移，并包含一个源检测机制来识别信息丰富的领域。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，在Erdos-Renyi模型假设下的随机图背景下，当存在信息丰富的源时，迁移学习可以改善收敛速度。实证评估显示，在目标域的标注数据有限的情况下，该方法在预测准确性上有了显著提升。&lt;h4&gt;结论&lt;/h4&gt;提出的NCR迁移学习框架在处理网络数据依赖性方面有效，并在预测准确性上取得了显著成果。&lt;h4&gt;翻译&lt;/h4&gt;摘要：通过利用相关领域的知识，迁移学习可以增强模型性能，尤其是在标注数据稀缺的情况下。尽管现有研究在独立设置下解决了各种分布偏移的迁移学习问题，但处理网络数据中的依赖性仍然具有挑战性。为了应对这一挑战，我们提出了一种基于网络卷积回归（NCR）的高维迁移学习框架，该框架受到图卷积网络（GCNs）成功应用的启发。NCR模型通过允许每个节点的响应依赖于其特征及其邻居的聚合特征，有效地捕捉了局部依赖性。我们的方法包括一个两步迁移学习算法，用于解决源网络和目标网络之间的领域偏移，以及一个源检测机制来识别信息丰富的领域。从理论上讲，我们在基于Erdos-Renyi模型假设的随机图背景下分析了lasso估计量，证明了当存在信息丰富的源时，迁移学习可以改善收敛速度。实证评估，包括模拟和利用SinaWeibo数据的实际应用，证明了在目标域的标注数据有限的情况下，该方法在预测准确性上有了显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning enhances model performance by utilizing knowledge fromrelated domains, particularly when labeled data is scarce. While existingresearch addresses transfer learning under various distribution shifts inindependent settings, handling dependencies in networked data remainschallenging. To address this challenge, we propose a high-dimensional transferlearning framework based on network convolutional regression (NCR), inspired bythe success of graph convolutional networks (GCNs). The NCR model incorporatesrandom network structure by allowing each node's response to depend on itsfeatures and the aggregated features of its neighbors, capturing localdependencies effectively. Our methodology includes a two-step transfer learningalgorithm that addresses domain shift between source and target networks, alongwith a source detection mechanism to identify informative domains.Theoretically, we analyze the lasso estimator in the context of a random graphbased on the Erdos-Renyi model assumption, demonstrating that transfer learningimproves convergence rates when informative sources are present. Empiricalevaluations, including simulations and a real-world application using SinaWeibo data, demonstrate substantial improvements in prediction accuracy,particularly when labeled data in the target domain is limited.</description>
      <author>example@mail.com (Liyuan Wang, Jiachen Chen, Kathryn L. Lunetta, Danyang Huang, Huimin Cheng, Debarghya Mukherjee)</author>
      <guid isPermaLink="false">2504.19979v2</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting</title>
      <link>http://arxiv.org/abs/2504.20630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为ISDrama的多模态沉浸式空间戏剧生成模型，该模型通过多模态提示创建连续多说话者的双耳语音，并具有戏剧性的韵律，应用于AR、VR等领域。&lt;h4&gt;背景&lt;/h4&gt;多模态沉浸式空间戏剧生成需要同时模拟空间信息和戏剧韵律，且数据收集成本高，目前尚无解决这些挑战的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够通过多模态提示生成沉浸式空间戏剧的模型，并构建相应的数据集。&lt;h4&gt;方法&lt;/h4&gt;1) 构建了名为MRSDrama的多模态记录空间戏剧数据集，包含双耳戏剧音频、剧本、视频、几何姿态和文本提示。2) 提出了ISDrama模型，该模型包含以下主要组件：多模态姿态编码器（考虑移动说话者引起的多普勒效应，提取统一姿态信息）、沉浸式戏剧Transformer（基于流的mamba-transformer模型，结合Drama-MOE以增强韵律和姿态控制）以及上下文一致的分类器无关的指导策略以生成连贯的戏剧。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ISDrama在客观和主观指标上优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;ISDrama模型能够有效地生成沉浸式空间戏剧，为相关应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a multimodal immersive spatial drama generation model named ISDrama, which creates continuous binaural speech with dramatic prosody based on multimodal prompts and has potential applications in AR and VR. The model simultaneously models spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. The paper constructs the MRSDrama dataset, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, it proposes ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama consists of the following main components: 1) Multimodal Pose Encoder, which uses contrastive learning to consider the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. It also designs a context-consistent classifier-free guidance strategy to generate coherent drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal immersive spatial drama generation focuses on creating continuousmulti-speaker binaural speech with dramatic prosody based on multimodalprompts, with potential applications in AR, VR, and others. This task requiressimultaneous modeling of spatial information and dramatic prosody based onmultimodal inputs, with high data collection costs. To the best of ourknowledge, our work is the first attempt to address these challenges. Weconstruct MRSDrama, the first multimodal recorded spatial drama dataset,containing binaural drama audios, scripts, videos, geometric poses, and textualprompts. Then, we propose ISDrama, the first immersive spatial drama generationmodel through multimodal prompting. ISDrama comprises these primary components:1) Multimodal Pose Encoder, based on contrastive learning, considering theDoppler effect caused by moving speakers to extract unified pose informationfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-basedmamba-transformer model that generates high-quality drama, incorporatingDrama-MOE to select proper experts for enhanced prosody and pose control. Wealso design a context-consistent classifier-free guidance strategy tocoherently generate complete drama. Experimental results show that ISDramaoutperforms baseline models on objective and subjective metrics. The demos anddataset are available at https://aaronz345.github.io/ISDramaDemo.</description>
      <author>example@mail.com (Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao)</author>
      <guid isPermaLink="false">2504.20630v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations</title>
      <link>http://arxiv.org/abs/2504.20222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025 CV4Metaverse Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FreBIS是一种新的神经网络隐式表面表示方法，用于解决复杂场景中的表面建模问题。&lt;h4&gt;背景&lt;/h4&gt;神经网络隐式表面表示技术在增强现实/虚拟现实、数字孪生、自主导航等领域需求增加。&lt;h4&gt;目的&lt;/h4&gt;提出FreBIS方法以克服传统3D表面重建方法在处理复杂场景时的局限性。&lt;h4&gt;方法&lt;/h4&gt;FreBIS通过根据表面频率分层场景，并为每个频率级别（或一组频率级别）分配专用编码器。它还通过一个新颖的冗余感知加权模块鼓励编码器捕获互补信息。&lt;h4&gt;主要发现&lt;/h4&gt;在BlendedMVS数据集上的实证评估表明，使用FreBIS的频率分层编码器替换标准编码器，可以显著提高重建3D表面的质量和渲染的保真度。&lt;h4&gt;结论&lt;/h4&gt;FreBIS在处理复杂场景的表面建模方面具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：神经网络隐式表面表示技术在增强现实/虚拟现实、数字孪生、自主导航等多个技术领域需求日益增长。这种技术能够将场景中的物体表面建模为连续函数，近年来在超越经典3D表面重建方法（如使用体素或点云的方法）方面取得了显著进展。然而，这些方法在处理具有多样化和复杂表面的场景时存在困难，主要是因为它们使用单个编码网络来同时捕获场景中从低到高表面频率的全部信息。在这项工作中，我们提出了一种名为FreBIS的新型神经网络隐式表面表示方法，以克服这一挑战。FreBIS通过根据表面频率将场景分层到多个频率级别，并为每个级别（或一组级别）分配一个专门的编码器。此外，FreBIS通过一种新颖的冗余感知加权模块促进编码特征的相互差异，从而鼓励这些编码器捕获互补信息。在具有挑战性的BlendedMVS数据集上的实证评估表明，用我们的频率分层编码器替换标准编码器，可以显著提高3D表面重建的质量及其从任何视角的渲染保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural implicit surface representation techniques are in high demand foradvancing technologies in augmented reality/virtual reality, digital twins,autonomous navigation, and many other fields. With their ability to modelobject surfaces in a scene as a continuous function, such techniques have maderemarkable strides recently, especially over classical 3D surfacereconstruction methods, such as those that use voxels or point clouds. However,these methods struggle with scenes that have varied and complex surfacesprincipally because they model any given scene with a single encoder networkthat is tasked to capture all of low through high-surface frequency informationin the scene simultaneously. In this work, we propose a novel, neural implicitsurface representation approach called FreBIS to overcome this challenge.FreBIS works by stratifying the scene based on the frequency of surfaces intomultiple frequency levels, with each level (or a group of levels) encoded by adedicated encoder. Moreover, FreBIS encourages these encoders to capturecomplementary information by promoting mutual dissimilarity of the encodedfeatures via a novel, redundancy-aware weighting module. Empirical evaluationson the challenging BlendedMVS dataset indicate that replacing the standardencoder in an off-the-shelf neural surface reconstruction method with ourfrequency-stratified encoders yields significant improvements. Theseenhancements are evident both in the quality of the reconstructed 3D surfacesand in the fidelity of their renderings from any viewpoint.</description>
      <author>example@mail.com (Naoko Sawada, Pedro Miraldo, Suhas Lohit, Tim K. Marks, Moitreya Chatterjee)</author>
      <guid isPermaLink="false">2504.20222v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection</title>
      <link>http://arxiv.org/abs/2504.20498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript submitted to IEEE Transactions on Multimedia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Style-Adaptive Detection Transformer (SA-DETR)的检测器，用于解决单源域泛化（SDG）在目标检测中的问题，该检测器能够使用源域数据实现强泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的基于CNN的检测器主要通过精心设计的数据增强策略和特征对齐技术来提高鲁棒性，但数据增强方法存在局限性，且DETR在域适应任务中表现出色，但在SDG任务中的应用潜力尚未探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够使用源域数据在未见过的目标域上展示强泛化能力的目标检测器。&lt;h4&gt;方法&lt;/h4&gt;提出了一种域风格适配器，将未见目标域的风格表示投影到训练域，实现动态风格适配；并提出了一种对象感知对比学习模块，通过对比学习引导检测器提取域不变特征；使用对象感知门控掩码在空间和语义维度上约束特征聚合，实现跨域实例级特征的对比，从而增强泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;SA-DETR在五个不同的天气场景下表现出了优越的性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SA-DETR是一种有效的SDG检测器，能够显著提高目标检测的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Single-source Domain Generalization (SDG) in object detection aims to develop a detector using only data from a source domain that can exhibit strong generalization capability when applied to unseen target domains. Existing methods are built upon CNN-based detectors and primarily improve robustness by employing carefully designed data augmentation strategies integrated with feature alignment techniques. However, data augmentation methods have inherent drawbacks; they are only effective when the augmented sample distribution approximates or covers the unseen scenarios, thus failing to enhance generalization across all unseen domains. Furthermore, while the recent Detection Transformer (DETR) has demonstrated superior generalization capability in domain adaptation tasks due to its efficient global information extraction, its potential in SDG tasks remains unexplored. To this end, we introduce a strong DETR-based detector named the Style-Adaptive Detection Transformer (SA-DETR) for SDG in object detection. Specifically, we present a domain style adapter that projects the style representation of the unseen target domain into the training domain, enabling dynamic style adaptation. Then, we propose an object-aware contrastive learning module to guide the detector in extracting domain-invariant features through contrastive learning. By using object-aware gating masks to constrain feature aggregation in both spatial and semantic dimensions, this module achieves cross-domain contrast of instance-level features, thereby enhancing generalization. Extensive experiments demonstrate the superior performance and generalization capability of SA-DETR across five different weather scenarios. Code is released at https://github.com/h751410234/SA-DETR.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-source Domain Generalization (SDG) in object detection aims to developa detector using only data from a source domain that can exhibit stronggeneralization capability when applied to unseen target domains. Existingmethods are built upon CNN-based detectors and primarily improve robustness byemploying carefully designed data augmentation strategies integrated withfeature alignment techniques. However, data augmentation methods have inherentdrawbacks; they are only effective when the augmented sample distributionapproximates or covers the unseen scenarios, thus failing to enhancegeneralization across all unseen domains. Furthermore, while the recentDetection Transformer (DETR) has demonstrated superior generalizationcapability in domain adaptation tasks due to its efficient global informationextraction, its potential in SDG tasks remains unexplored. To this end, weintroduce a strong DETR-based detector named the Style-Adaptive DetectionTransformer (SA-DETR) for SDG in object detection. Specifically, we present adomain style adapter that projects the style representation of the unseentarget domain into the training domain, enabling dynamic style adaptation.Then, we propose an object-aware contrastive learning module to guide thedetector in extracting domain-invariant features through contrastive learning.By using object-aware gating masks to constrain feature aggregation in bothspatial and semantic dimensions, this module achieves cross-domain contrast ofinstance-level features, thereby enhancing generalization. Extensiveexperiments demonstrate the superior performance and generalization capabilityof SA-DETR across five different weather scenarios. Code is released athttps://github.com/h751410234/SA-DETR.</description>
      <author>example@mail.com (Jianhong Han, Yupei Wang, Liang Chen)</author>
      <guid isPermaLink="false">2504.20498v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>VideoMultiAgents: A Multi-Agent Framework for Video Question Answering</title>
      <link>http://arxiv.org/abs/2504.20091v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为VideoMultiAgents的视频问答框架，该框架通过整合专门用于视觉、场景图分析和文本处理的代理，利用多模态推理来增强视频内容的理解，并通过问题引导的标题生成来提高答案的准确性。&lt;h4&gt;背景&lt;/h4&gt;视频问答（VQA）需要多模态推理，结合视觉、时间和语言线索来深入理解视频内容。然而，许多现有方法依赖于将帧级标题输入到单个模型中，这使得难以充分捕捉时间和交互式上下文。&lt;h4&gt;目的&lt;/h4&gt;提出VideoMultiAgents框架，以解决现有方法在捕捉时间和交互式上下文方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;VideoMultiAgents框架通过整合专门代理，包括视觉代理、场景图分析代理和文本处理代理，以及问题引导的标题生成来提高视频问答的性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在Intent-QA（79.0%，比之前的SOTA高6.2%）、EgoSchema子集（75.4%，比之前的SOTA高3.4%）和NExT-QA（79.6%，比之前的SOTA高0.4%）上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;VideoMultiAgents框架通过多模态推理和问题引导的标题生成，显著提高了视频问答的性能，并在多个数据集上取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VQA) inherently relies on multimodal reasoning,integrating visual, temporal, and linguistic cues to achieve a deeperunderstanding of video content. However, many existing methods rely on feedingframe-level captions into a single model, making it difficult to adequatelycapture temporal and interactive contexts. To address this limitation, weintroduce VideoMultiAgents, a framework that integrates specialized agents forvision, scene graph analysis, and text processing. It enhances videounderstanding leveraging complementary multimodal reasoning from independentlyoperating agents. Our approach is also supplemented with a question-guidedcaption generation, which produces captions that highlight objects, actions,and temporal transitions directly relevant to a given query, thus improving theanswer accuracy. Experimental results demonstrate that our method achievesstate-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).</description>
      <author>example@mail.com (Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Masamoto Tanabiki, Kazuki Kozuka, Ehsan Adeli)</author>
      <guid isPermaLink="false">2504.20091v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Learning a General Model: Folding Clothing with Topological Dynamics</title>
      <link>http://arxiv.org/abs/2504.20720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种拓扑动力学模型，用于折叠复杂的服装。&lt;h4&gt;背景&lt;/h4&gt;服装具有高自由度和复杂结构，对服装操作提出了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种方法来折叠复杂的服装。&lt;h4&gt;方法&lt;/h4&gt;利用可见的折叠结构作为拓扑骨架，设计了一种新的拓扑图来表示服装状态。应用语义分割分析遮挡关系，并分解服装结构，结合关键点检测生成拓扑图。使用改进的图神经网络（GNN）学习一般动力学，预测服装变形并计算变形雅可比矩阵。&lt;h4&gt;主要发现&lt;/h4&gt;拓扑图可以指示服装的约束，并预测服装的运动。&lt;h4&gt;结论&lt;/h4&gt;实验验证了算法在识别和折叠具有遮挡的复杂服装方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;The abstract of the paper is summarized as follows: The high degrees of freedom and complex structure of garments present significant challenges for clothing manipulation. In this paper, we propose a general topological dynamics model to fold complex clothing. By utilizing the visible folding structure as the topological skeleton, we design a novel topological graph to represent the clothing state. This topological graph is low-dimensional and applied for complex clothing in various folding states. It indicates the constraints of clothing and enables predictions regarding clothing movement. To extract graphs from self-occlusion, we apply semantic segmentation to analyze the occlusion relationships and decompose the clothing structure. The decomposed structure is then combined with keypoint detection to generate the topological graph. To analyze the behavior of the topological graph, we employ an improved Graph Neural Network (GNN) to learn the general dynamics. The GNN model can predict the deformation of clothing and is employed to calculate the deformation Jacobi matrix for control. Experiments using jackets validate the algorithm's effectiveness to recognize and fold complex clothing with self-occlusion.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The high degrees of freedom and complex structure of garments presentsignificant challenges for clothing manipulation. In this paper, we propose ageneral topological dynamics model to fold complex clothing. By utilizing thevisible folding structure as the topological skeleton, we design a noveltopological graph to represent the clothing state. This topological graph islow-dimensional and applied for complex clothing in various folding states. Itindicates the constraints of clothing and enables predictions regardingclothing movement. To extract graphs from self-occlusion, we apply semanticsegmentation to analyze the occlusion relationships and decompose the clothingstructure. The decomposed structure is then combined with keypoint detection togenerate the topological graph. To analyze the behavior of the topologicalgraph, we employ an improved Graph Neural Network (GNN) to learn the generaldynamics. The GNN model can predict the deformation of clothing and is employedto calculate the deformation Jacobi matrix for control. Experiments usingjackets validate the algorithm's effectiveness to recognize and fold complexclothing with self-occlusion.</description>
      <author>example@mail.com (Yiming Liu, Lijun Han, Enlin Gu, Hesheng Wang)</author>
      <guid isPermaLink="false">2504.20720v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>FALCO: a Foundation model of Astronomical Light Curves for time dOmain astronomy</title>
      <link>http://arxiv.org/abs/2504.20290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FALCO是一种用于时域天文观测的光曲线分析基础模型，通过自监督学习在未标记的开普勒光曲线上训练，表现出强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;时域天文观测揭示了多种可变现象，但数据规模和复杂性以及快速分类的需求对分析提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理时域天文观测数据并具有良好泛化能力的模型。&lt;h4&gt;方法&lt;/h4&gt;使用基于Transformer的架构，通过自监督学习在未标记的开普勒光曲线上训练FALCO模型。&lt;h4&gt;主要发现&lt;/h4&gt;FALCO在三个不同任务上表现出色：在八类恒星可变性分类中达到95%的准确率，表面重力估计的RMSE为0.1305 dex，在耀斑识别中达到87%的精确度。&lt;h4&gt;结论&lt;/h4&gt;FALCO模型能够从光曲线中学习可泛化的表示，并能够轻松适应不同的任务。模型性能随着模型规模和输入序列长度的增加而提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-domain surveys have advanced astronomical research by revealing diversevariable phenomena, from stellar flares to transient events. The scale andcomplexity of survey data, along with the demand for rapid classification,present significant challenges for analysis. While machine learning offerssolutions, most existing models are tailored to single tasks, struggle togeneralize, and depend heavily on large, accurately labeled datasets. Weintroduce FALCO, a foundation model for astronomical light curve analysis intime-domain astronomy. This work presents the initial version of FALCO trainedvia self-supervised learning on unlabeled Kepler light curves using aTransformer-based architecture. The model has been evaluated on three distincttasks and demonstrates strong generalization: achieving 95 percent accuracy instellar variability classification across eight classes, an overall RMSE of0.1305 dex in surface gravity estimation (notably improved to below 0.08 dexwhen log g is less than 1, and approximately 0.02 dex near log g equals 3), and87 percent precision in flare identification. These results highlight themodel's versatility and ability to learn generalizable representations fromlight curves, enabling straightforward adaptation to diverse tasks. We furtheranalyze the impact of model scaling and sequence length, finding performanceimproves with larger models and longer input sequences. We also apply FALCO toderive surface gravity (log g) measurements for 179,732 Kepler stars from theirlight curves.</description>
      <author>example@mail.com (Xiaoxiong Zuo, Yihan Tao, Yang Huang, Zhixuan Kang, Huaxi Chen, Chenzhou Cui, Jiashu Pan, Xiao Kong, Xiaoyu Tang, Henggeng Han, Haiyang Mu, Yunfei Xu, Dongwei Fan, Guirong Xue, Ali Luo, Jifeng Liu)</author>
      <guid isPermaLink="false">2504.20290v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>SFi-Former: Sparse Flow Induced Attention for Graph Transformer</title>
      <link>http://arxiv.org/abs/2504.20666v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICMR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的注意力机制SFi-attention，以及基于此的SFi-Former模型，用于处理具有长距离依赖的图数据，以提高图神经网络（GNN）的性能并解决传统GNN存在的问题。&lt;h4&gt;背景&lt;/h4&gt;Graph Transformers在处理具有长距离依赖的图数据时表现出色，但传统的密集注意力机制导致诱导偏置弱、过拟合和过全局化等问题。&lt;h4&gt;目的&lt;/h4&gt;设计一种新的注意力机制以缓解密集注意力带来的问题，并提高GNN模型在处理长距离依赖图数据时的性能。&lt;h4&gt;方法&lt;/h4&gt;引入SFi-attention，通过最小化基于网络流的能量函数来学习稀疏模式，并设计SFi-Former模型利用稀疏注意力模式生成稀疏网络流，从而从其他节点选择性地聚合特征。&lt;h4&gt;主要发现&lt;/h4&gt;SFi-Former在GNN基准数据集上获得了有竞争力的性能，在长距离图基准（LRGB）数据集上实现了SOTA性能，并且具有更小的泛化差距，表明它更不容易过拟合。&lt;h4&gt;结论&lt;/h4&gt;SFi-attention和SFi-Former模型为处理长距离依赖的图数据提供了一种有效的方法，并有望提高GNN模型的整体性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph Transformers在许多研究中显示出比传统消息传递图神经网络（GNN）更优越的性能，尤其是在处理具有长距离依赖的图数据时。然而，由于密集注意力，GTs往往会遭受弱诱导偏置、过拟合和过全局化问题。在本文中，我们引入了SFi-attention，这是一种新的注意力机制，通过最小化基于网络流的能量函数（带有l1范数正则化）来学习稀疏模式，以缓解由密集注意力引起的问题。此外，相应地设计了SFi-Former，该模型可以利用SFi-attention的稀疏注意力模式在图数据的邻接矩阵之外生成稀疏网络流。具体来说，SFi-Former通过灵活地调整稀疏注意力来选择性地从其他节点聚合特征，从而得到一个更稳健的模型。我们在各种图数据集上验证了我们的SFi-Former，特别是那些表现出长距离依赖的图数据。实验结果表明，我们的SFi-Former在GNN基准数据集上获得了有竞争力的性能，在长距离图基准（LRGB）数据集上实现了SOTA性能。此外，我们的模型产生了更小的泛化差距，这表明它不太可能过拟合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have demonstrated superior performance compared totraditional message-passing graph neural networks in many studies, especiallyin processing graph data with long-range dependencies. However, GTs tend tosuffer from weak inductive bias, overfitting and over-globalizing problems dueto the dense attention. In this paper, we introduce SFi-attention, a novelattention mechanism designed to learn sparse pattern by minimizing an energyfunction based on network flows with l1-norm regularization, to relieve thoseissues caused by dense attention. Furthermore, SFi-Former is accordinglydevised which can leverage the sparse attention pattern of SFi-attention togenerate sparse network flows beyond adjacency matrix of graph data.Specifically, SFi-Former aggregates features selectively from other nodesthrough flexible adaptation of the sparse attention, leading to a more robustmodel. We validate our SFi-Former on various graph datasets, especially thosegraph data exhibiting long-range dependencies. Experimental results show thatour SFi-Former obtains competitive performance on GNN Benchmark datasets andSOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,our model gives rise to smaller generalization gaps, which indicates that it isless prone to over-fitting. Click here for codes.</description>
      <author>example@mail.com (Zhonghao Li, Ji Shi, Xinming Zhang, Miao Zhang, Bo Li)</author>
      <guid isPermaLink="false">2504.20666v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Exploring internal representation of self-supervised networks: few-shot learning abilities and comparison with human semantics and recognition of objects</title>
      <link>http://arxiv.org/abs/2504.20364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了自监督学习在机器学习和神经科学领域的最新进展，并研究了自监督学习方法在训练人工神经网络方面的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;自监督学习方法不需要标注的监督信息，可以应用于不需要大量数据集的神经网络训练，并可能为大脑如何无监督地适应环境提供见解。&lt;h4&gt;目的&lt;/h4&gt;研究使用自监督对比学习算法训练的DCNN内部表示与人类语义和识别之间的对应关系。&lt;h4&gt;方法&lt;/h4&gt;采用少量样本学习评估程序，测量DCNN识别新概念的能力，并使用两种比较方法将少量样本学习结果与人类语义和识别联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;通过对比学习获得的表示与人类认知高度一致，表明自监督对比学习框架在无法获得明确监督的情况下，如人类婴儿在语言习得之前，具有模拟人类大脑学习机制的可能性。&lt;h4&gt;结论&lt;/h4&gt;自监督对比学习框架在模拟人类大脑学习机制方面具有潜力，尤其是在无法获得明确监督的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in self-supervised learning have attracted significantattention from both machine learning and neuroscience. This is primarilybecause self-supervised methods do not require annotated supervisoryinformation, making them applicable to training artificial networks withoutrelying on large amounts of curated data, and potentially offering insightsinto how the brain adapts to its environment in an unsupervised manner.Although several previous studies have elucidated the correspondence betweenneural representations in deep convolutional neural networks (DCNNs) andbiological systems, the extent to which unsupervised or self-supervisedlearning can explain the human-like acquisition of categorically structuredinformation remains less explored. In this study, we investigate thecorrespondence between the internal representations of DCNNs trained using aself-supervised contrastive learning algorithm and human semantics andrecognition. To this end, we employ a few-shot learning evaluation procedure,which measures the ability of DCNNs to recognize novel concepts from limitedexposure, to examine the inter-categorical structure of the learnedrepresentations. Two comparative approaches are used to relate the few-shotlearning outcomes to human semantics and recognition, with results suggestingthat the representations acquired through contrastive learning are well alignedwith human cognition. These findings underscore the potential ofself-supervised contrastive learning frameworks to model learning mechanismssimilar to those of the human brain, particularly in scenarios where explicitsupervision is unavailable, such as in human infants prior to languageacquisition.</description>
      <author>example@mail.com (Asaki Kataoka, Yoshihiro Nagano, Masafumi Oizumi)</author>
      <guid isPermaLink="false">2504.20364v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2504.20403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于创建可编辑3D虚拟形象的框架，旨在为普通用户提供精确的区域定位、几何适应性和逼真的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;个性化3D形象编辑因其用户友好性和在AR/VR和虚拟试穿等应用中的可用性而具有巨大潜力。然而，先前的研究在生成视觉上令人满意的结果方面遇到了困难，这可能是由于在复杂重建场景中混合优化几何和纹理下的不稳定表示学习。&lt;h4&gt;目的&lt;/h4&gt;旨在为普通用户提供一种创建精确区域定位、几何适应性和逼真渲染的3D可编辑形象的可访问解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一个精心设计的框架，该框架将编辑过程分解为局部空间适应和逼真外观学习，使用混合四面体约束高斯分层（TetGS）作为底层表示。TetGS结合了四面体网格的可控显式结构和3D高斯分层的精确渲染能力，并经过三个阶段的优化：从现实世界的单目视频实例化3D形象以提供TetGS初始化的准确先验；使用显式划分的四面体进行局部空间适应以引导高斯核的重新分配；以及基于几何的外观生成，采用由粗到细的激活策略。&lt;h4&gt;主要发现&lt;/h4&gt;定性和定量实验都证明了该方法在生成逼真的3D可编辑形象方面的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地生成逼真的3D可编辑形象，为普通用户提供了创建个性化虚拟形象的新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized 3D avatar editing holds significant promise due to itsuser-friendliness and availability to applications such as AR/VR and virtualtry-ons. Previous studies have explored the feasibility of 3D editing, butoften struggle to generate visually pleasing results, possibly due to theunstable representation learning under mixed optimization of geometry andtexture in complicated reconstructed scenarios. In this paper, we aim toprovide an accessible solution for ordinary users to create their editable 3Davatars with precise region localization, geometric adaptability, andphotorealistic renderings. To tackle this challenge, we introduce ameticulously designed framework that decouples the editing process into localspatial adaptation and realistic appearance learning, utilizing a hybridTetrahedron-constrained Gaussian Splatting (TetGS) as the underlyingrepresentation. TetGS combines the controllable explicit structure oftetrahedral grids with the high-precision rendering capabilities of 3D GaussianSplatting and is optimized in a progressive manner comprising three stages: 3Davatar instantiation from real-world monocular videos to provide accuratepriors for TetGS initialization; localized spatial adaptation with explicitlypartitioned tetrahedrons to guide the redistribution of Gaussian kernels; andgeometry-based appearance generation with a coarse-to-fine activation strategy.Both qualitative and quantitative experiments demonstrate the effectiveness andsuperiority of our approach in generating photorealistic 3D editable avatars.</description>
      <author>example@mail.com (Hanxi Liu, Yifang Men, Zhouhui Lian)</author>
      <guid isPermaLink="false">2504.20403v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>What Causes Knowledge Loss in Multilingual Language Models?</title>
      <link>http://arxiv.org/abs/2504.20356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了自然语言处理模型中的跨语言迁移问题，通过实验验证了参数共享在减轻遗忘和保留先验知识方面的效果。&lt;h4&gt;背景&lt;/h4&gt;跨语言迁移在自然语言处理中通过利用共享的语言知识来增强多语言性能。然而，传统方法在处理所有数据时往往无法模拟真实场景，导致灾难性遗忘等问题。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过实验评估参数共享通过适配器能否减轻遗忘同时保留先验知识。&lt;h4&gt;方法&lt;/h4&gt;研究在52种语言中使用不同阶数的LoRA适配器，对非共享、部分共享和完全共享的参数进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现使用非拉丁文字的语言更容易受到灾难性遗忘的影响，而使用拉丁文字的语言则更有利于有效的跨语言迁移。&lt;h4&gt;结论&lt;/h4&gt;参数共享通过适配器可以有效减轻遗忘并保留先验知识，特别是在使用拉丁文字的语言中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-lingual transfer in natural language processing (NLP) models enhancesmultilingual performance by leveraging shared linguistic knowledge. However,traditional methods that process all data simultaneously often fail to mimicreal-world scenarios, leading to challenges like catastrophic forgetting, wherefine-tuning on new tasks degrades performance on previously learned ones. Ourstudy explores this issue in multilingual contexts, focusing on linguisticdifferences affecting representational learning rather than just modelparameters. We experiment with 52 languages using LoRA adapters of varyingranks to evaluate non-shared, partially shared, and fully shared parameters.Our aim is to see if parameter sharing through adapters can mitigate forgettingwhile preserving prior knowledge. We find that languages using non-Latinscripts are more susceptible to catastrophic forgetting, whereas those writtenin Latin script facilitate more effective cross-lingual transfer.</description>
      <author>example@mail.com (Maria Khelli, Samuel Cahyawijaya, Ayu Purwarianti, Genta Indra Winata)</author>
      <guid isPermaLink="false">2504.20356v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning on a Random Lattice</title>
      <link>http://arxiv.org/abs/2504.20197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Proceedings of ILIAD (2024),  https://www.iliadconference.com/proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了将深度神经网络学习到的表示分解为可解释特征，以提高其安全性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;为了更好地理解特征，本文采用了几何视角，将特征视为学习到的坐标系统，用于映射嵌入数据分布。&lt;h4&gt;目的&lt;/h4&gt;通过分析数据分布的模型，提高对深度神经网络特征的理解。&lt;h4&gt;方法&lt;/h4&gt;将通用数据分布模型为一个随机晶格，并使用渗透理论分析其性质。&lt;h4&gt;主要发现&lt;/h4&gt;将学习到的特征分为上下文、组件和表面特征，模型与机制可解释性领域的近期发现相一致。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为未来研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;Decomposing a deep neural network's learned representations into interpretable features could greatly enhance its safety and reliability. To better understand features, we adopt a geometric perspective, viewing them as a learned coordinate system for mapping an embedded data distribution. We motivate a model of a generic data distribution as a random lattice and analyze its properties using percolation theory. Learned features are categorized into context, component, and surface features. The model is qualitatively consistent with recent findings in mechanistic interpretability and suggests directions for future research.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decomposing a deep neural network's learned representations intointerpretable features could greatly enhance its safety and reliability. Tobetter understand features, we adopt a geometric perspective, viewing them as alearned coordinate system for mapping an embedded data distribution. Wemotivate a model of a generic data distribution as a random lattice and analyzeits properties using percolation theory. Learned features are categorized intocontext, component, and surface features. The model is qualitatively consistentwith recent findings in mechanistic interpretability and suggests directionsfor future research.</description>
      <author>example@mail.com (Aryeh Brill)</author>
      <guid isPermaLink="false">2504.20197v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Triadic Closure-Heterogeneity-Harmony GCN for Link Prediction</title>
      <link>http://arxiv.org/abs/2504.20492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了TriHetGCN模型，用于复杂网络的链接预测，通过结合拓扑指示符，提高了预测准确性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;链接预测在复杂网络分析中应用广泛，但传统方法依赖于预定义的节点连接假设，而基于GNN的方法常忽视节点属性和节点对之间的结构关系。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够有效处理节点属性和节点对之间结构关系的链接预测模型。&lt;h4&gt;方法&lt;/h4&gt;TriHetGCN模型包含拓扑特征构建、图结构表示和连接概率预测三个模块，利用三度闭包和度异质性等拓扑指示符来提高预测能力。&lt;h4&gt;主要发现&lt;/h4&gt;在九个真实世界数据集上的评估中，TriHetGCN模型取得了最先进的性能，超越了主流方法，表现出较强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TriHetGCN模型提供了一个连接统计物理和图深度学习的有希望的框架，具有良好的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction aims to estimate the likelihood of connections between pairsof nodes in complex networks, which is beneficial to many applications fromfriend recommendation to metabolic network reconstruction. Traditionalheuristic-based methodologies in the field of complex networks typically dependon predefined assumptions about node connectivity, limiting theirgeneralizability across diverse networks. While recent graph neural network(GNN) approaches capture global structural features effectively, they oftenneglect node attributes and intrinsic structural relationships between nodepairs. To address this, we propose TriHetGCN, an extension of traditional GraphConvolutional Networks (GCNs) that incorporates explicit topological indicators-- triadic closure and degree heterogeneity. TriHetGCN consists of threemodules: topology feature construction, graph structural representation, andconnection probability prediction. The topology feature module constructs nodefeatures using shortest path distances to anchor nodes, enhancing globalstructure perception. The graph structural module integrates topologicalindicators into the GCN framework to model triadic closure and heterogeneity.The connection probability module uses deep learning to predict links.Evaluated on nine real-world datasets, from traditional networks without nodeattributes to large-scale networks with rich features, TriHetGCN achievesstate-of-the-art performance, outperforming mainstream methods. This highlightsits strong generalization across diverse network types, offering a promisingframework that bridges statistical physics and graph deep learning.</description>
      <author>example@mail.com (Ke-ke Shang, Junfan Yi, Michael Small, Yijie Zhou)</author>
      <guid isPermaLink="false">2504.20492v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation</title>
      <link>http://arxiv.org/abs/2504.17365v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeSoccer是一种新的足球视频多模态大型语言模型，用于全场比赛视频的密集视频字幕生成（SDVC），通过引入MoFA-Select模块和联合预测时间戳和字幕，实现了长视频理解并达到先进性能。&lt;h4&gt;背景&lt;/h4&gt;足球是一项全球流行的运动赛事，通常具有长时间的赛程和独特的精彩时刻。现有的足球多模态大型语言模型在字幕生成时依赖时间先验知识，无法端到端处理足球视频。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型的问题，提出TimeSoccer，旨在实现全场比赛视频的端到端密集视频字幕生成，并生成高质量、时间对齐准确、语义相关的评论。&lt;h4&gt;方法&lt;/h4&gt;TimeSoccer联合预测时间戳和字幕，一次生成，并在45分钟的比赛中进行全局上下文建模。MoFA-Select是一个无训练的动觉帧压缩模块，通过粗到细的策略自适应选择代表性帧，并采用补充训练范式加强模型处理长时序的能力。&lt;h4&gt;主要发现&lt;/h4&gt;TimeSoccer在SDVC任务上实现了最先进（SoTA）的性能，能够生成高质量、时间对齐准确、语义相关的评论。&lt;h4&gt;结论&lt;/h4&gt;TimeSoccer在足球视频字幕生成方面取得了显著进步，为长视频理解提供了有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soccer is a globally popular sporting event, typically characterized by longmatches and distinctive highlight moments. Recent advances in Multimodal LargeLanguage Models (MLLMs) offer promising capabilities in temporal grounding andvideo understanding, soccer commentary generation often requires precisetemporal localization and semantically rich descriptions over long-form video.However, existing soccer MLLMs often rely on the temporal a priori for captiongeneration, so they cannot process the soccer video end-to-end. While sometraditional approaches follow a two-step paradigm that is complex and fails tocapture the global context to achieve suboptimal performance. To solve theabove issues, we present TimeSoccer, the first end-to-end soccer MLLM forSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.TimeSoccer jointly predicts timestamps and generates captions in a single pass,enabling global context modeling across 45-minute matches. To support longvideo understanding of soccer matches, we introduce MoFA-Select, atraining-free, motion-aware frame compression module that adaptively selectsrepresentative frames via a coarse-to-fine strategy, and incorporatescomplementary training paradigms to strengthen the model's ability to handlelong temporal sequences. Extensive experiments demonstrate that our TimeSoccerachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-endform, generating high-quality commentary with accurate temporal alignment andstrong semantic relevance.</description>
      <author>example@mail.com (Ling You, Wenxuan Huang, Xinni Xie, Xiangyi Wei, Bangyan Li, Shaohui Lin, Yang Li, Changbo Wang)</author>
      <guid isPermaLink="false">2504.17365v3</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Learning Laplacian Positional Encodings for Heterophilous Graphs</title>
      <link>http://arxiv.org/abs/2504.20430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AISTATS 2025; version with full appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文从理论上证明了当前图位置编码（PEs）在处理异质图任务时可能不利于性能，甚至可能损害性能。为了解决这一局限性，提出了可学习的拉普拉斯位置编码（LLPE），它能够利用图拉普拉斯的全谱，从而在亲缘性和异质图上捕捉图结构。&lt;h4&gt;背景&lt;/h4&gt;许多现实世界的网络表现出异质性，即使是高度同质性的图也可能包含局部强异质性的区域。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的PE方法，以有效捕捉异质图中的复杂结构。&lt;h4&gt;方法&lt;/h4&gt;提出了Learnable Laplacian Positional Encodings (LLPE)，并通过理论证明和实证评估来验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;LLPE能够近似一类通用的图距离，并在12个基准数据集上对多种GNN（包括图变换器）进行了评估，结果表明LLPE在合成和现实世界的图上分别提高了35%和14%的准确率。&lt;h4&gt;结论&lt;/h4&gt;LLPE是向开发能够有效捕捉异质图中复杂结构的位置编码方法迈出的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;In this work, we theoretically demonstrate that current graph positional encodings (PEs) are not beneficial and could potentially hurt performance in tasks involving heterophilous graphs, where nodes that are close tend to have different labels. This limitation is critical as many real-world networks exhibit heterophily, and even highly homophilous graphs can contain local regions of strong heterophily. To address this limitation, we propose Learnable Laplacian Positional Encodings (LLPE), a new PE that leverages the full spectrum of the graph Laplacian, enabling them to capture graph structure on both homophilous and heterophilous graphs. Theoretically, we prove LLPE's ability to approximate a general class of graph distances and demonstrate its generalization properties. Empirically, our evaluation on 12 benchmarks demonstrates that LLPE improves accuracy across a variety of GNNs, including graph transformers, by up to 35% and 14% on synthetic and real-world graphs, respectively. Going forward, our work represents a significant step towards developing PEs that effectively capture complex structures in heterophilous graphs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we theoretically demonstrate that current graph positionalencodings (PEs) are not beneficial and could potentially hurt performance intasks involving heterophilous graphs, where nodes that are close tend to havedifferent labels. This limitation is critical as many real-world networksexhibit heterophily, and even highly homophilous graphs can contain localregions of strong heterophily. To address this limitation, we propose LearnableLaplacian Positional Encodings (LLPE), a new PE that leverages the fullspectrum of the graph Laplacian, enabling them to capture graph structure onboth homophilous and heterophilous graphs. Theoretically, we prove LLPE'sability to approximate a general class of graph distances and demonstrate itsgeneralization properties. Empirically, our evaluation on 12 benchmarksdemonstrates that LLPE improves accuracy across a variety of GNNs, includinggraph transformers, by up to 35% and 14% on synthetic and real-world graphs,respectively. Going forward, our work represents a significant step towardsdeveloping PEs that effectively capture complex structures in heterophilousgraphs.</description>
      <author>example@mail.com (Michael Ito, Jiong Zhu, Dexiong Chen, Danai Koutra, Jenna Wiens)</author>
      <guid isPermaLink="false">2504.20430v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Understanding GNNs and Homophily in Dynamic Node Classification</title>
      <link>http://arxiv.org/abs/2504.20421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AISTATS 2025; version with full appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了动态图中的同质性，提出了一个新的同质性度量方法，并探讨了其在图神经网络（GNN）性能上的应用。&lt;h4&gt;背景&lt;/h4&gt;同质性作为度量，对于理解图神经网络（GNN）至关重要，但至今为止，这一度量仅在静态图环境中进行分析。&lt;h4&gt;目的&lt;/h4&gt;探索动态环境中的同质性，并提出一种适用于动态环境的新同质性度量方法。&lt;h4&gt;方法&lt;/h4&gt;重点关注图卷积网络（GCN），理论上证明了在动态环境中，当前GCN的判别性能由节点未来标签与其邻居当前标签相同性的概率决定。&lt;h4&gt;主要发现&lt;/h4&gt;提出了动态同质性，这一新度量与GNN的判别性能相关，并揭示了如何设计更强大的GNN以适应动态图。&lt;h4&gt;结论&lt;/h4&gt;本文的研究工作对于理解同质性和GNN在动态节点分类中的性能具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;Homophily, as a measure, has been critical to increasing our understanding of graph neural networks (GNNs). However, to date this measure has only been analyzed in the context of static graphs. In our work, we explore homophily in dynamic settings. Focusing on graph convolutional networks (GCNs), we demonstrate theoretically that in dynamic settings, current GCN discriminative performance is characterized by the probability that a node's future label is the same as its neighbors' current labels. Based on this insight, we propose dynamic homophily, a new measure of homophily that applies in the dynamic setting. This new measure correlates with GNN discriminative performance and sheds light on how to potentially design more powerful GNNs for dynamic graphs. Leveraging a variety of dynamic node classification datasets, we demonstrate that popular GNNs are not robust to low dynamic homophily. Going forward, our work represents an important step towards understanding homophily and GNN performance in dynamic node classification.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Homophily, as a measure, has been critical to increasing our understanding ofgraph neural networks (GNNs). However, to date this measure has only beenanalyzed in the context of static graphs. In our work, we explore homophily indynamic settings. Focusing on graph convolutional networks (GCNs), wedemonstrate theoretically that in dynamic settings, current GCN discriminativeperformance is characterized by the probability that a node's future label isthe same as its neighbors' current labels. Based on this insight, we proposedynamic homophily, a new measure of homophily that applies in the dynamicsetting. This new measure correlates with GNN discriminative performance andsheds light on how to potentially design more powerful GNNs for dynamic graphs.Leveraging a variety of dynamic node classification datasets, we demonstratethat popular GNNs are not robust to low dynamic homophily. Going forward, ourwork represents an important step towards understanding homophily and GNNperformance in dynamic node classification.</description>
      <author>example@mail.com (Michael Ito, Danai Koutra, Jenna Wiens)</author>
      <guid isPermaLink="false">2504.20421v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs</title>
      <link>http://arxiv.org/abs/2504.20406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用大型语言模型和公开的脚本指南构建离线模拟框架的方法，以自动化任务和定制软件工作流程，旨在解决传统脚本创建的障碍。&lt;h4&gt;背景&lt;/h4&gt;脚本接口允许用户自动化任务和定制软件工作流程，但创建脚本通常需要编程专长和对特定API的熟悉度，这对许多用户来说构成了障碍。&lt;h4&gt;目的&lt;/h4&gt;为了解决运行时代码生成的问题，如未验证的代码、安全风险、较长的响应时间和较高的计算成本，本文旨在通过构建一个离线模拟框架来创建软件特定的技能集。&lt;h4&gt;方法&lt;/h4&gt;该框架包括两个组件：（1）任务创建，使用自上而下的功能指导和自下而上的API协同探索来生成有用的任务；（2）技能生成，通过试验、优化和验证脚本，基于执行反馈来生成技能。为了高效地导航广泛的API景观，引入了一个基于图神经网络（GNN）的链接预测模型来捕捉API协同，从而生成涉及未充分利用的API的技能，并扩展技能集的多样性。&lt;h4&gt;主要发现&lt;/h4&gt;Adobe Illustrator的实验表明，与传统的运行时代码生成相比，该框架显著提高了自动化成功率，减少了响应时间，并节省了运行时令牌成本。&lt;h4&gt;结论&lt;/h4&gt;这是首次尝试将软件脚本接口作为基于LLM系统的测试平台，强调了在受控环境中利用执行反馈的优势，并为将AI能力与用户需求对齐提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scripting interfaces enable users to automate tasks and customize softwareworkflows, but creating scripts traditionally requires programming expertiseand familiarity with specific APIs, posing barriers for many users. While LargeLanguage Models (LLMs) can generate code from natural language queries, runtimecode generation is severely limited due to unverified code, security risks,longer response times, and higher computational costs. To bridge the gap, wepropose an offline simulation framework to curate a software-specific skillset,a collection of verified scripts, by exploiting LLMs and publicly availablescripting guides. Our framework comprises two components: (1) task creation,using top-down functionality guidance and bottom-up API synergy exploration togenerate helpful tasks; and (2) skill generation with trials, refining andvalidating scripts based on execution feedback. To efficiently navigate theextensive API landscape, we introduce a Graph Neural Network (GNN)-based linkprediction model to capture API synergy, enabling the generation of skillsinvolving underutilized APIs and expanding the skillset's diversity.Experiments with Adobe Illustrator demonstrate that our framework significantlyimproves automation success rates, reduces response time, and saves runtimetoken costs compared to traditional runtime code generation. This is the firstattempt to use software scripting interfaces as a testbed for LLM-basedsystems, highlighting the advantages of leveraging execution feedback in acontrolled environment and offering valuable insights into aligning AIcapabilities with user needs in specialized software domains.</description>
      <author>example@mail.com (Paiheng Xu, Gang Wu, Xiang Chen, Tong Yu, Chang Xiao, Franck Dernoncourt, Tianyi Zhou, Wei Ai, Viswanathan Swaminathan)</author>
      <guid isPermaLink="false">2504.20406v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Generative Diffusion Models for Resource Allocation in Wireless Networks</title>
      <link>http://arxiv.org/abs/2504.20277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于生成扩散模型（GDMs）的监督训练算法，用于学习随机资源分配策略。&lt;h4&gt;背景&lt;/h4&gt;资源分配问题被表述为在满足稳态服务质量（QoS）约束下最大化稳态效用函数。&lt;h4&gt;目的&lt;/h4&gt;目的是训练一个GDM策略来模仿专家政策，并从最优分布中生成新的样本。&lt;h4&gt;方法&lt;/h4&gt;通过序列执行生成的样本来达到近最优性能。为了使算法适用于多种网络配置，使用图神经网络（GNN）架构参数化反向扩散过程。&lt;h4&gt;主要发现&lt;/h4&gt;在多用户干扰网络中的功率控制案例研究中，展示了数值结果。&lt;h4&gt;结论&lt;/h4&gt;该算法能够通过生成扩散模型学习到近最优的资源分配策略，并能够适应不同的网络配置。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a supervised training algorithm for learning stochasticresource allocation policies with generative diffusion models (GDMs). Weformulate the allocation problem as the maximization of an ergodic utilityfunction subject to ergodic Quality of Service (QoS) constraints. Given samplesfrom a stochastic expert policy that yields a near-optimal solution to theproblem, we train a GDM policy to imitate the expert and generate new samplesfrom the optimal distribution. We achieve near-optimal performance throughsequential execution of the generated samples. To enable generalization to afamily of network configurations, we parameterize the backward diffusionprocess with a graph neural network (GNN) architecture. We present numericalresults in a case study of power control in multi-user interference networks.</description>
      <author>example@mail.com (Yigit Berkay Uslu, Samar Hadou, Shirin Saeedi Bidokhti, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2504.20277v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Learning Hierarchical Interaction for Accurate Molecular Property Prediction</title>
      <link>http://arxiv.org/abs/2504.20127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HimNet的新型模型，用于预测分子的ADMET属性，通过层次交互信息传递机制，实现了对分子结构的有效捕捉和特征提取，提高了分子属性预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;在药物发现过程中，发现具有理想分子属性的分子至关重要。现有的方法通常使用深度学习模型，如图神经网络（GNNs）和Transformer，通过学习化学信息来预测这些分子属性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型在捕捉分子结构层次性和多级特征交互方面存在的不足，提出了一种新的层次交互信息传递机制。&lt;h4&gt;方法&lt;/h4&gt;该方法通过层次注意力引导的信息传递，实现了在原子、基序和分子层面的交互感知表示学习，从而有效平衡全局和局部信息。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验表明，HimNet在大多数分子属性预测任务中取得了最佳或接近最佳的性能，并且表现出良好的层次可解释性，与化学直觉相符。&lt;h4&gt;结论&lt;/h4&gt;HimNet为分子活性和ADMET属性预测提供了一种准确且高效的方法，对药物发现早期阶段的决策具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在药物发现中，发现具有理想分子属性（包括ADMET特性）的分子至关重要。现有方法通常采用深度学习模型，如图神经网络（GNNs）和Transformer，通过学习多样化的化学信息来预测这些分子属性。然而，这些模型往往无法有效地捕捉和利用分子结构的层次性，并且缺乏多级特征之间有效交互的机制。为了解决这些限制，我们提出了一种层次交互信息传递机制，作为我们新颖模型HimNet的基础。我们的方法通过层次注意力引导的信息传递，实现了在原子、基序和分子层面的交互感知表示学习。这种设计使得HimNet能够有效地平衡全局和局部信息，确保为下游属性预测任务（如血脑屏障通透性BBBP）提供丰富且与任务相关的特征提取。在多个基准数据集上的大量实验表明，HimNet在大多数分子属性预测任务中实现了最佳或接近最佳的性能。此外，我们的方法表现出有希望的结构可解释性，与代表性分子的化学直觉相吻合。我们相信，HimNet为分子活性和ADMET属性预测提供了一种准确且高效的方法，对药物发现早期阶段的决策具有重大贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discovering molecules with desirable molecular properties, including ADMET(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is ofgreat importance in drug discovery. Existing approaches typically employ deeplearning models, such as Graph Neural Networks (GNNs) and Transformers, topredict these molecular properties by learning from diverse chemicalinformation. However, these models often fail to efficiently capture andutilize the hierarchical nature of molecular structures, and lack mechanismsfor effective interaction among multi-level features. To address theselimitations, we propose a Hierarchical Interaction Message Passing Mechanism,which serves as the foundation of our novel model, HimNet. Our method enablesinteraction-aware representation learning across atomic, motif, and molecularlevels via hierarchical attention-guided message passing. This design allowsHimNet to effectively balance global and local information, ensuring rich andtask-relevant feature extraction for downstream property prediction tasks, suchas Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiplebenchmark datasets demonstrate that HimNet achieves the best or near-bestperformance in most molecular property prediction tasks. Furthermore, ourmethod exhibits promising hierarchical interpretability, aligning well withchemical intuition on representative molecules. We believe that HimNet offersan accurate and efficient solution for molecular activity and ADMET propertyprediction, contributing significantly to advanced decision-making in the earlystages of drug discovery.</description>
      <author>example@mail.com (Huiyang Hong, Xinkai Wu, Hongyu Sun, Qi Wang, Yuquan Li)</author>
      <guid isPermaLink="false">2504.20127v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>If Concept Bottlenecks are the Question, are Foundation Models the Answer?</title>
      <link>http://arxiv.org/abs/2504.19774v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了概念瓶颈模型（CBMs），这是一种结合高性能和先验可解释性的神经网络。&lt;h4&gt;背景&lt;/h4&gt;CBMs通过将输入（如图像）映射到高级概念（如可见物体及其属性），然后以可解释的方式使用这些概念解决下游任务（如图像标签或评分）。然而，这些模型的表现和可解释性依赖于它们学习到的概念质量。&lt;h4&gt;目的&lt;/h4&gt;研究使用VLM-CBM架构代替手动标注（专家标注）的方法，以及这种做法对学习到的概念质量的影响。&lt;h4&gt;方法&lt;/h4&gt;研究者通过实证分析，使用一系列重要的指标来测试最先进的VLM-CBMs，并评估其学习到的概念。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，VLM监督与专家标注在性能上可能存在明显差异，并且概念准确性和质量并不强烈相关。&lt;h4&gt;结论&lt;/h4&gt;本文的结论是，VLM监督在概念质量上可能不如专家标注，但其在某些任务中可以提供合理的性能。&lt;h4&gt;翻译&lt;/h4&gt;Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high-performance with ante-hoc interpretability. CBMs work by first mapping inputs (e.g., images) to high-level concepts (e.g., visible objects and their properties) and then use these to solve a downstream task (e.g., tagging or scoring an image) in an interpretable manner. Their performance and interpretability, however, hinge on the quality of the concepts they learn. The go-to strategy for ensuring good quality concepts is to leverage expert annotations, which are expensive to collect and seldom available in applications. Researchers have recently addressed this issue by introducing 'VLM-CBM' architectures that replace manual annotations with weak supervision from foundation models. It is, however, unclear what is the impact of doing so on the quality of the learned concepts. To answer this question, we put state-of-the-art VLM-CBMs to the test, analyzing their learned concepts empirically using a selection of significant metrics. Our results show that, depending on the task, VLM supervision can sensibly differ from expert annotations, and that concept accuracy and quality are not strongly correlated. Our code is available at https://github.com/debryu/CQA.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept Bottleneck Models (CBMs) are neural networks designed to conjoin highperformance with ante-hoc interpretability. CBMs work by first mapping inputs(e.g., images) to high-level concepts (e.g., visible objects and theirproperties) and then use these to solve a downstream task (e.g., tagging orscoring an image) in an interpretable manner. Their performance andinterpretability, however, hinge on the quality of the concepts they learn. Thego-to strategy for ensuring good quality concepts is to leverage expertannotations, which are expensive to collect and seldom available inapplications. Researchers have recently addressed this issue by introducing"VLM-CBM" architectures that replace manual annotations with weak supervisionfrom foundation models. It is however unclear what is the impact of doing so onthe quality of the learned concepts. To answer this question, we putstate-of-the-art VLM-CBMs to the test, analyzing their learned conceptsempirically using a selection of significant metrics. Our results show that,depending on the task, VLM supervision can sensibly differ from expertannotations, and that concept accuracy and quality are not strongly correlated.Our code is available at https://github.com/debryu/CQA.</description>
      <author>example@mail.com (Nicola Debole, Pietro Barbiero, Francesco Giannini, Andrea Passerini, Stefano Teso, Emanuele Marconato)</author>
      <guid isPermaLink="false">2504.19774v2</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier</title>
      <link>http://arxiv.org/abs/2504.20124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于人工智能的诊断流程，利用Google的Health Acoustic Representations (HeAR)模型从儿童呼吸声音中检测哮喘的早期迹象。&lt;h4&gt;背景&lt;/h4&gt;早期检测儿童哮喘对于预防长期呼吸并发症和减少紧急干预至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够快速、非侵入性地筛查哮喘的方法。&lt;h4&gt;方法&lt;/h4&gt;使用SPRSound数据集，该数据集是第一个公开的、标注了1个月至18岁儿童呼吸声音的集合。数据集中的每个2秒音频片段被嵌入到512维度的表示中，使用的是在3亿个与健康相关的音频剪辑（包括1亿个咳嗽声音）上预训练的HeAR模型。然后，使用SVM、随机森林和MLP等多个分类器在这些嵌入上进行训练，以区分哮喘指示性声音和正常声音。&lt;h4&gt;主要发现&lt;/h4&gt;该系统在分类任务上达到了超过91%的准确率，在正例的精确度-召回率指标上表现良好。通过PCA可视化学习到的嵌入，通过波形回放分析误分类，并提供了ROC和混淆矩阵的见解。&lt;h4&gt;结论&lt;/h4&gt;这种方法表明，由基础音频模型驱动的短、低资源的儿童录音可以实现快速、非侵入性的哮喘筛查，特别适用于远程或服务不足的医疗保健环境。&lt;h4&gt;翻译&lt;/h4&gt;摘要：儿童哮喘的早期检测对于预防长期呼吸并发症和减少紧急干预至关重要。本研究提出了一种基于人工智能的诊断流程，利用Google的Health Acoustic Representations (HeAR)模型从儿童呼吸声音中检测哮喘的早期迹象。使用SPRSound数据集，该数据集是第一个公开的、标注了1个月至18岁儿童呼吸声音的集合。数据集中的每个2秒音频片段被嵌入到512维度的表示中，使用的是在3亿个与健康相关的音频剪辑（包括1亿个咳嗽声音）上预训练的HeAR模型。然后，使用SVM、随机森林和MLP等多个分类器在这些嵌入上进行训练，以区分哮喘指示性声音和正常声音。该系统在分类任务上达到了超过91%的准确率，在正例的精确度-召回率指标上表现良好。通过PCA可视化学习到的嵌入，通过波形回放分析误分类，并提供了ROC和混淆矩阵的见解。这种方法表明，由基础音频模型驱动的短、低资源的儿童录音可以实现快速、非侵入性的哮喘筛查，特别适用于远程或服务不足的医疗保健环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection of asthma in children is crucial to prevent long-termrespiratory complications and reduce emergency interventions. This workpresents an AI-powered diagnostic pipeline that leverages Googles HealthAcoustic Representations (HeAR) model to detect early signs of asthma frompediatric respiratory sounds. The SPRSound dataset, the first open-accesscollection of annotated respiratory sounds in children aged 1 month to 18years, is used to extract 2-second audio segments labeled as wheeze, crackle,rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensionalrepresentation using HeAR, a foundation model pretrained on 300 millionhealth-related audio clips, including 100 million cough sounds. Multipleclassifiers, including SVM, Random Forest, and MLP, are trained on theseembeddings to distinguish between asthma-indicative and normal sounds. Thesystem achieves over 91\% accuracy, with strong performance on precision-recallmetrics for positive cases. In addition to classification, learned embeddingsare visualized using PCA, misclassifications are analyzed through waveformplayback, and ROC and confusion matrix insights are provided. This methoddemonstrates that short, low-resource pediatric recordings, when powered byfoundation audio models, can enable fast, noninvasive asthma screening. Theapproach is especially promising for digital diagnostics in remote orunderserved healthcare settings.</description>
      <author>example@mail.com (Abul Ehtesham, Saket Kumar, Aditi Singh, Tala Talaei Khoei)</author>
      <guid isPermaLink="false">2504.20124v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Informed Neural Operator Transformer</title>
      <link>http://arxiv.org/abs/2504.19452v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GINOT的几何信息神经网络算子Transformer，该模型结合了transformer架构和神经网络算子框架，实现了对任意几何形状的前向预测。&lt;h4&gt;背景&lt;/h4&gt;基于机器学习的代理模型在计算效率方面优于传统的数值方法，尤其在需要重复评估偏微分方程的问题中。&lt;h4&gt;目的&lt;/h4&gt;提出GINOT的目的是为了实现任意几何形状的前向预测。&lt;h4&gt;方法&lt;/h4&gt;GINOT通过采样和分组机制结合注意力机制对几何形状的点云进行编码，确保对点顺序和填充的不变性，同时保持对点密度变化的鲁棒性。几何信息通过注意力机制与解解码器中的查询点无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;GINOT在多个具有挑战性的数据集上进行了验证，展示了其在复杂和任意2D和3D几何形状上的高精度和强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;GINOT是一种高效且准确的方法，可以用于对任意几何形状进行前向预测。&lt;h4&gt;翻译&lt;/h4&gt;Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions for arbitrary geometries. GINOT encodes the surface point cloud of a geometry using a sampling and grouping mechanism combined with an attention mechanism, ensuring invariance to point order and padding while maintaining robustness to variations in point density. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learning-based surrogate models offer significant computationalefficiency and faster simulations compared to traditional numerical methods,especially for problems requiring repeated evaluations of partial differentialequations. This work introduces the Geometry-Informed Neural OperatorTransformer (GINOT), which integrates the transformer architecture with theneural operator framework to enable forward predictions for arbitrarygeometries. GINOT encodes the surface points cloud of a geometry using asampling and grouping mechanism combined with an attention mechanism, ensuringinvariance to point order and padding while maintaining robustness tovariations in point density. The geometry information is seamlessly integratedwith query points in the solution decoder through the attention mechanism. Theperformance of GINOT is validated on multiple challenging datasets, showcasingits high accuracy and strong generalization capabilities for complex andarbitrary 2D and 3D geometries.</description>
      <author>example@mail.com (Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric, Philippe Geubelle)</author>
      <guid isPermaLink="false">2504.19452v2</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Pretraining for Material Property Prediction</title>
      <link>http://arxiv.org/abs/2504.20112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 7 figures, 2 algorithms, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的监督预训练方法，用于材料性能预测，通过使用可用的类别信息作为代理标签，提高了模型的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;准确预测材料性能对于发现具有定制功能的新型材料至关重要。深度学习模型在捕捉结构-性能关系方面表现出色，但通常依赖于需要大量、标注良好的数据集的监督学习。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过使用自我监督学习（SSL）和监督预训练来预测材料性能，以减少对大量标注数据集的依赖。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了监督预训练，其中可用的类别信息作为代理标签来指导学习，并评估了这种方法在两个最先进的SSL模型上的效果。此外，他们还提出了一种基于图的增强技术，通过向材料图注入噪声来提高鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法在六个具有挑战性的材料性能预测任务上进行了微调，平均绝对误差（MAE）提高了2%至6.67%，并建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;这项研究是第一次探索在材料性能预测中使用代理标签进行监督预训练，推动了该领域的方法和应用的进展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：准确预测材料属性有助于发现具有定制功能的新型材料。最近，深度学习模型在捕捉结构-属性关系方面显示出优异的准确性和灵活性。然而，这些模型通常依赖于需要大量、标注良好的数据集的监督学习。自我监督学习（SSL）通过在大规模无标签数据集上预训练来开发可以微调用于材料属性预测的基础模型，提供了一种有希望的选择。在本工作中，我们提出了监督预训练，其中可用的类别信息作为代理标签来引导学习，即使下游任务涉及无关的材料属性。我们评估了这种策略在两个最先进的SSL模型上的效果，并介绍了一种新的监督预训练框架。为了进一步提高表示学习，我们提出了一种基于图的增强技术，通过向材料图注入噪声来提高鲁棒性，而不破坏材料图的结构。所得到的基模模型用于六个具有挑战性的材料性能预测任务，与基线相比，平均绝对误差（MAE）提高了2%至6.67%，并在材料性能预测中建立了新的基准。这项研究是第一次在材料性能预测中探索使用代理标签进行监督预训练，推动了该领域的方法和应用的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of material properties facilitates the discovery of novelmaterials with tailored functionalities. Deep learning models have recentlyshown superior accuracy and flexibility in capturing structure-propertyrelationships. However, these models often rely on supervised learning, whichrequires large, well-annotated datasets an expensive and time-consumingprocess. Self-supervised learning (SSL) offers a promising alternative bypretraining on large, unlabeled datasets to develop foundation models that canbe fine-tuned for material property prediction. In this work, we proposesupervised pretraining, where available class information serves as surrogatelabels to guide learning, even when downstream tasks involve unrelated materialproperties. We evaluate this strategy on two state-of-the-art SSL models andintroduce a novel framework for supervised pretraining. To further enhancerepresentation learning, we propose a graph-based augmentation technique thatinjects noise to improve robustness without structurally deforming materialgraphs. The resulting foundation models are fine-tuned for six challengingmaterial property predictions, achieving significant performance gains overbaselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)and establishing a new benchmark in material property prediction. This studyrepresents the first exploration of supervised pertaining with surrogate labelsin material property prediction, advancing methodology and application in thefield.</description>
      <author>example@mail.com (Chowdhury Mohammad Abid Rahman, Aldo H. Romero, Prashnna K. Gyawali)</author>
      <guid isPermaLink="false">2504.20112v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning</title>
      <link>http://arxiv.org/abs/2504.20103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种异构网络药物靶点相互作用预测框架，结合图神经网络和多尺度信号处理技术，构建了一个既高效预测又具有多层次可解释性的模型。&lt;h4&gt;背景&lt;/h4&gt;药物-靶点相互作用预测在生物医学领域的药物开发和精准医学中是一个核心任务。然而，传统的机器学习方法通常存在黑盒问题，这使得揭示模型决策机制与生物分子间相互作用模式之间的深层相关性变得困难。&lt;h4&gt;目的&lt;/h4&gt;研究旨在提供一种从黑盒预测到机制解码的完整解决方案，以发现药物靶点。&lt;h4&gt;方法&lt;/h4&gt;该框架的技术突破主要体现在以下三个方面：1. 设计了基于异构图卷积神经网络（HGCN）的多阶邻域聚合策略；2. 提出了多尺度图信号分解和生物解释模块；3. 结合多维度视角和层次表示的对比学习，通过比较学习模型，将HGCN和GWT两个视角的节点表示对齐和融合，使模型能够整合多维度信息并提高预测鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架在所有数据集上均表现出优异的预测性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为药物靶点发现提供了一种完整的解决方案，其方法论对于建模复杂的生物分子相互作用系统具有重要参考价值。&lt;h4&gt;翻译&lt;/h4&gt;This study proposes a heterogeneous network drug target interaction prediction framework, integrating graph neural network and multi scale signal processing technology to construct a model with both efficient prediction and multi level interpretability. The technical breakthroughs are mainly reflected in the following three dimensions: Local global feature collaborative perception module based on Heterogeneous Graph Convolutional Neural Network (HGCN), multi scale graph signal decomposition and biological interpretation module, and contrastive learning combining multi dimensional perspectives and hierarchical representations. The experimental results show that our framework shows excellent prediction performance on all datasets. This study provides a complete solution for drug target discovery from black box prediction to mechanism decoding, and its methodology has important reference value for modeling complex biomolecular interaction systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug-target interaction (DTI) prediction is a core task in drug developmentand precision medicine in the biomedical field. However, traditional machinelearning methods generally have the black box problem, which makes it difficultto reveal the deep correlation between the model decision mechanism and theinteraction pattern between biological molecules. This study proposes aheterogeneous network drug target interaction prediction framework, integratinggraph neural network and multi scale signal processing technology to constructa model with both efficient prediction and multi level interpretability. Itstechnical breakthroughs are mainly reflected in the following threedimensions:Local global feature collaborative perception module. Based onheterogeneous graph convolutional neural network (HGCN), a multi order neighboraggregation strategy is designed.Multi scale graph signal decomposition andbiological interpretation module. A deep hierarchical node feature transform(GWT) architecture is proposed.Contrastive learning combining multi dimensionalperspectives and hierarchical representations. By comparing the learningmodels, the node representations from the two perspectives of HGCN and GWT arealigned and fused, so that the model can integrate multi dimensionalinformation and improve the prediction robustness. Experimental results showthat our framework shows excellent prediction performance on all datasets. Thisstudy provides a complete solution for drug target discovery from black boxprediction to mechanism decoding, and its methodology has important referencevalue for modeling complex biomolecular interaction systems.</description>
      <author>example@mail.com (Wenfeng Dai, Yanhong Wang, Shuai Yan, Qingzhi Yu, Xiang Cheng)</author>
      <guid isPermaLink="false">2504.20103v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling</title>
      <link>http://arxiv.org/abs/2504.20110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自监督几何表示学习方法，用于从非参数3D模型中捕捉精细几何特征，以支持数据稀缺场景下的代理建模。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的代理建模正成为3D设计、分析和制造中替代基于物理模拟的有效方法，但这些方法通常需要大量的标记CAD到模拟数据集。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉精细几何特征的自监督几何表示学习方法，以支持需要精细几何细节的复杂应用。&lt;h4&gt;方法&lt;/h4&gt;该方法将几何特征提取与下游物理任务解耦，通过几何重建损失来学习一个潜在空间嵌入。它使用近零级采样和创新的批量自适应注意力加权损失函数，以增强对复杂设计特征的编码。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在结构力学案例研究中得到了验证，表现出在捕捉设计特征和实现准确的少样本物理预测方面的强大性能。&lt;h4&gt;结论&lt;/h4&gt;与传统参数化代理建模相比，该方法有望弥合几何和基于物理的表示之间的差距，为数据稀缺场景下的代理建模提供有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;AI驱动的代理建模已经成为3D设计、分析和制造中替代基于物理模拟的有效方法。这些模型利用数据驱动的方法来预测通常需要计算昂贵的模拟的物理量。然而，标记的CAD到模拟数据集的稀缺推动了自监督和基础模型最近的进步，其中几何表示学习是在线进行的，然后针对特定的下游任务进行微调。尽管这些方法显示出希望，但它们在需要精细尺度几何细节保留的应用中效果有限。这项工作介绍了一种自监督几何表示学习方法，旨在从非参数3D模型中捕获精细几何特征。与传统的端到端代理模型不同，这种方法将几何特征提取与下游物理任务解耦，通过几何重建损失学习一个潜在空间嵌入。关键元素包括近零级采样的基本使用和创新的批量自适应注意力加权损失函数，这些函数增强了复杂设计特征的编码。该方法通过结构力学案例研究得到了验证，证明了在捕捉设计特征和实现准确的少样本物理预测方面的强大性能。与传统参数化代理建模的比较突出了其弥合几何和基于物理表示之间差距的潜力，为数据稀缺场景下的代理建模提供了有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-driven surrogate modeling has become an increasingly effective alternativeto physics-based simulations for 3D design, analysis, and manufacturing. Thesemodels leverage data-driven methods to predict physical quantitiestraditionally requiring computationally expensive simulations. However, thescarcity of labeled CAD-to-simulation datasets has driven recent advancementsin self-supervised and foundation models, where geometric representationlearning is performed offline and later fine-tuned for specific downstreamtasks. While these approaches have shown promise, their effectiveness islimited in applications requiring fine-scale geometric detail preservation.This work introduces a self-supervised geometric representation learning methoddesigned to capture fine-scale geometric features from non-parametric 3Dmodels. Unlike traditional end-to-end surrogate models, this approach decouplesgeometric feature extraction from downstream physics tasks, learning a latentspace embedding guided by geometric reconstruction losses. Key elements includethe essential use of near-zero level sampling and the innovative batch-adaptiveattention-weighted loss function, which enhance the encoding of intricatedesign features. The proposed method is validated through case studies instructural mechanics, demonstrating strong performance in capturing designfeatures and enabling accurate few-shot physics predictions. Comparisons withtraditional parametric surrogate modeling highlight its potential to bridge thegap between geometric and physics-based representations, providing an effectivesolution for surrogate modeling in data-scarce scenarios.</description>
      <author>example@mail.com (Yu-hsuan Chen, Jing Bi, Cyril Ngo Ngoc, Victor Oancea, Jonathan Cagan, Levent Burak Kara)</author>
      <guid isPermaLink="false">2504.20110v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics</title>
      <link>http://arxiv.org/abs/2504.20099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review at the International Journal of Interactive  Multimedia and Artificial Intelligence (IJIMAI)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了时间序列基础模型产生的潜在空间的可解释性，重点关注其在视觉分析任务中的潜力。&lt;h4&gt;背景&lt;/h4&gt;该研究评估了MOMENT系列模型，这是一组基于Transformer的预训练架构，用于处理多元时间序列任务，如数据插补、预测、分类和异常检测。&lt;h4&gt;目的&lt;/h4&gt;研究旨在评估这些模型在五个数据集上的能力，以捕捉时间序列数据在其潜在空间投影中的底层结构，并验证微调是否可以改善结果嵌入空间的清晰度。&lt;h4&gt;方法&lt;/h4&gt;研究人员对MOMENT模型进行了评估，并进行了微调以观察性能改进，同时进行了可视化分析以评估嵌入的可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;微调后观察到损失减少方面的显著性能提升。可视化分析显示，嵌入的可解释性改善有限，需要进一步研究。&lt;h4&gt;结论&lt;/h4&gt;尽管MOMENT等时间序列基础模型稳健，但其潜在空间可能需要额外的方法论改进才能得到充分解释，例如替代投影技术、损失函数或数据预处理策略。尽管MOMENT存在局限性，但基础模型在执行时间上提供了大幅减少，这对于交互式可视化分析是一个重大进步。&lt;h4&gt;翻译&lt;/h4&gt;The present study explores the interpretability of latent spaces produced by time series foundation models, focusing on their potential for visual analysis tasks. Specifically, we evaluate the MOMENT family of models, a set of transformer-based, pre-trained architectures for multivariate time series tasks such as: imputation, prediction, classification, and anomaly detection. We evaluate the capacity of these models on five datasets to capture the underlying structures in time series data within their latent space projection and validate whether fine tuning improves the clarity of the resulting embedding spaces. Notable performance improvements in terms of loss reduction were observed after fine tuning. Visual analysis shows limited improvement in the interpretability of the embeddings, requiring further work. Results suggest that, although Time Series Foundation Models such as MOMENT are robust, their latent spaces may require additional methodological refinements to be adequately interpreted, such as alternative projection techniques, loss functions, or data preprocessing strategies. Despite the limitations of MOMENT, foundation models supose a big reduction in execution time and so a great advance for interactive visual analytics.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The present study explores the interpretability of latent spaces produced bytime series foundation models, focusing on their potential for visual analysistasks. Specifically, we evaluate the MOMENT family of models, a set oftransformer-based, pre-trained architectures for multivariate time series taskssuch as: imputation, prediction, classification, and anomaly detection. Weevaluate the capacity of these models on five datasets to capture theunderlying structures in time series data within their latent space projectionand validate whether fine tuning improves the clarity of the resultingembedding spaces. Notable performance improvements in terms of loss reductionwere observed after fine tuning. Visual analysis shows limited improvement inthe interpretability of the embeddings, requiring further work. Results suggestthat, although Time Series Foundation Models such as MOMENT are robust, theirlatent spaces may require additional methodological refinements to beadequately interpreted, such as alternative projection techniques, lossfunctions, or data preprocessing strategies. Despite the limitations of MOMENT,foundation models supose a big reduction in execution time and so a greatadvance for interactive visual analytics.</description>
      <author>example@mail.com (Inmaculada Santamaria-Valenzuela, Victor Rodriguez-Fernandez, Javier Huertas-Tato, Jong Hyuk Park, David Camacho)</author>
      <guid isPermaLink="false">2504.20099v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction</title>
      <link>http://arxiv.org/abs/2504.20102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为HyboWaveNet的深度学习框架，用于蛋白质-蛋白质相互作用（PPI）的预测，该框架结合了双曲图神经网络和多尺度图小波变换，以解决现有方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;蛋白质-蛋白质相互作用对于理解细胞功能、疾病途径和药物发现至关重要。现有方法在PPI预测方面虽然准确，但缺乏可解释性和难以捕捉蛋白质之间的层次几何和动态相互作用模式。&lt;h4&gt;目的&lt;/h4&gt;提出HyboWaveNet以解决现有PPI预测方法的局限性，提高预测结果的因果解释性和对多尺度动态相互作用模式的捕捉能力。&lt;h4&gt;方法&lt;/h4&gt;HyboWaveNet通过将蛋白质特征映射到洛伦兹空间，利用双曲距离度量模拟生物分子之间的层次拓扑关系。它结合了图神经网络和图小波变换，以自适应地提取不同分辨率下的局部和全局交互特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，HyboWaveNet在公共数据集上优于现有方法，并且多尺度图小波变换模块提高了HyboWaveNet的预测性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;HyboWaveNet将几何深度学习和信号处理相结合，为分析复杂生物系统提供了一种原则性的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein-protein interactions (PPIs) are fundamental for deciphering cellularfunctions,disease pathways,and drug discovery.Although existing neural networksand machine learning methods have achieved high accuracy in PPIprediction,their black-box nature leads to a lack of causal interpretation ofthe prediction results and difficulty in capturing hierarchical geometries andmulti-scale dynamic interaction patterns among proteins.To address thesechallenges, we propose HyboWaveNet,a novel deep learning framework thatcollaborates with hyperbolic graphical neural networks (HGNNs) and multiscalegraphical wavelet transform for robust PPI prediction. Mapping protein featuresto Lorentz space simulates hierarchical topological relationships amongbiomolecules via a hyperbolic distance metric,enabling node featurerepresentations that better fit biological a priori.HyboWaveNet inherentlysimulates hierarchical and scale-free biological relationships, while theintegration of wavelet transforms enables adaptive extraction of local andglobal interaction features across different resolutions. Our frameworkgenerates node feature representations via a graph neural network under theLorenz model and generates pairs of positive samples under multiple differentviews for comparative learning, followed by further feature extraction viamulti-scale graph wavelet transforms to predict potential PPIs. Experiments onpublic datasets show that HyboWaveNet improves over both existingstate-of-the-art methods. We also demonstrate through ablation experimentalstudies that the multi-scale graph wavelet transform module improves thepredictive performance and generalization ability of HyboWaveNet. This worklinks geometric deep learning and signal processing to advance PPI prediction,providing a principled approach for analyzing complex biological systems</description>
      <author>example@mail.com (Qingzhi Yu, Shuai Yan, Wenfeng Dai, Xiang Cheng)</author>
      <guid isPermaLink="false">2504.20102v1</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>Perception Encoder: The best visual embeddings are not at the output of the network</title>
      <link>http://arxiv.org/abs/2504.13181v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Updated refs, fixed typos, and added new COCO SotA: 66.0 val mAP!  Code, models, and data at  https://github.com/facebookresearch/perception_models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为感知编码器（PE）的先进视觉编码器，用于图像和视频理解。通过简单的视觉-语言学习进行训练，该编码器在多种下游任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;传统的视觉编码器依赖于多种预训练目标，针对分类、字幕或定位等特定任务进行优化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够通过简单的视觉-语言学习训练，并在多个下游任务上取得优异表现的视觉编码器。&lt;h4&gt;方法&lt;/h4&gt;采用精心调整的图像预训练食谱和强大的视频数据引擎进行训练，并通过语言对齐和空间对齐两种方法提取隐藏在中间层的嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;对比视觉-语言训练单独就能产生强大的通用嵌入，适用于多种下游任务。PE系列模型在多种任务上达到最佳水平，包括零样本图像和视频分类与检索，文档、图像和视频问答，以及空间任务如检测、跟踪和深度估计。&lt;h4&gt;结论&lt;/h4&gt;PE系列模型在多个视觉理解任务上取得了最先进的成果，并发布了相关模型、代码和新型数据集。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为感知编码器（PE）的先进视觉编码器，用于图像和视频理解。通过简单的视觉-语言学习进行训练，该编码器在多种下游任务上表现出色。传统的视觉编码器依赖于多种预训练目标，针对分类、字幕或定位等特定任务进行优化。本文的目的是开发一种能够通过简单的视觉-语言学习训练，并在多个下游任务上取得优异表现的视觉编码器。采用精心调整的图像预训练食谱和强大的视频数据引擎进行训练，并通过语言对齐和空间对齐两种方法提取隐藏在中间层的嵌入。研究发现，对比视觉-语言训练单独就能产生强大的通用嵌入，适用于多种下游任务。PE系列模型在多种任务上达到最佳水平，包括零样本图像和视频分类与检索，文档、图像和视频问答，以及空间任务如检测、跟踪和深度估计。结论是，PE系列模型在多个视觉理解任务上取得了最先进的成果，并发布了相关模型、代码和新型数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Perception Encoder (PE), a state-of-the-art vision encoder forimage and video understanding trained via simple vision-language learning.Traditionally, vision encoders have relied on a variety of pretrainingobjectives, each tailored to specific downstream tasks such as classification,captioning, or localization. Surprisingly, after scaling our carefully tunedimage pretraining recipe and refining with our robust video data engine, wefind that contrastive vision-language training alone can produce strong,general embeddings for all of these downstream tasks. There is only one caveat:these embeddings are hidden within the intermediate layers of the network. Todraw them out, we introduce two alignment methods: language alignment formultimodal language modeling, and spatial alignment for dense prediction.Together, our PE family of models achieves best-in-class results on a widevariety of tasks, including (1) zero-shot image and video classification andretrieval, simultaneously obtaining 86.6 average zero-shot ImageNet robustnessand 76.9 zero-shot Kinetics-400 video classification; (2) document, image, andvideo Q&amp;A, enabling 94.6 DocVQA, 80.9 InfographicVQA, and 82.7 PerceptionTestwith an 8B LLM; and (3) spatial tasks such as detection, tracking, and depthestimation, setting a new COCO state-of-the-art of 66.0 box mAP. To fosterfurther research, we release our models, code, and novel dataset ofsynthetically and human-annotated videos:https://github.com/facebookresearch/perception_models</description>
      <author>example@mail.com (Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollár, Christoph Feichtenhofer)</author>
      <guid isPermaLink="false">2504.13181v2</guid>
      <pubDate>Wed, 30 Apr 2025 14:15:18 +0800</pubDate>
    </item>
    <item>
      <title>QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds</title>
      <link>http://arxiv.org/abs/2504.19716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级的解析方法，用于机器人抓取规划，特别是针对抗对称抓取，这种方法在六自由度空间中几乎不进行采样。&lt;h4&gt;背景&lt;/h4&gt;抓取一直是机器人与环境之间最终接口的一个长期挑战。随着环境和任务的复杂化，嵌入更高智能以从周围环境中推断并对其采取行动的需求变得必要。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决现有抓取规划方法在现实生活中的泛化能力差、生成抓取计划所需时间过长以及缺乏可重复性等问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于优化的抓取规划算法，通过在物体表面上估计抓取点来解决问题，而不是直接估计末端执行器的位置。此外，提出了一种软区域生长算法，用于有效分割平面，即使在曲面上也能有效工作。&lt;h4&gt;主要发现&lt;/h4&gt;该抓取框架在多个模拟物体上与现有的抓取规划方法Grasp Pose Detection (GPD)进行了比较，并使用图像和点云数据在现实世界中进行了评估，结果显示了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;与GPD相比，本文提出的方法在模拟和现实世界的抓取规划中表现出更高的效率和质量。&lt;h4&gt;翻译&lt;/h4&gt;摘要：抓取一直是机器人与环境之间最终接口的一个长期挑战。随着环境和任务的复杂化，嵌入更高智能以从周围环境中推断并对其采取行动的需求变得必要。尽管大多数方法利用技术来通过在六自由度空间中采用纯采样方法或作为学习问题来处理估计抓取位姿的问题，但它们通常由于跨领域泛化能力差而在现实生活中的设置中失败。此外，由于采样效率低下和现有抓取规划方法的概率性质，生成抓取计划所需的时间以及缺乏可重复性严重限制了它们在现实世界任务中的应用。本文提出了一种轻量级的解析方法，用于机器人抓取规划，特别是抗对称抓取，这种方法在六自由度空间中几乎不进行采样。所提出的抓取规划算法被表述为一个优化问题，用于估计物体表面上的抓取点，而不是直接估计末端执行器的位置。为此，提出了一种软区域生长算法，用于有效分割平面，即使在曲面上也能有效工作。然后使用基于优化的质量指标来评估抓取点，以确保间接力闭合。所提出的抓取框架与现有的最先进的抓取规划方法Grasp Pose Detection (GPD)在多个模拟物体上进行了比较，作为一个基线。与GPD相比，本文提出的方法在模拟和现实世界的抓取规划中表现出更高的效率和质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasping has been a long-standing challenge in facilitating the finalinterface between a robot and the environment. As environments and tasks becomecomplicated, the need to embed higher intelligence to infer from thesurroundings and act on them has become necessary. Although most methodsutilize techniques to estimate grasp pose by treating the problem via puresampling-based approaches in the six-degree-of-freedom space or as a learningproblem, they usually fail in real-life settings owing to poor generalizationacross domains. In addition, the time taken to generate the grasp plan and thelack of repeatability, owing to sampling inefficiency and the probabilisticnature of existing grasp planning approaches, severely limits their applicationin real-world tasks. This paper presents a lightweight analytical approachtowards robotic grasp planning, particularly antipodal grasps, with little tono sampling in the six-degree-of-freedom space. The proposed grasp planningalgorithm is formulated as an optimization problem towards estimating grasppoints on the object surface instead of directly estimating the end-effectorpose. To this extent, a soft-region-growing algorithm is presented foreffective plane segmentation, even in the case of curved surfaces. Anoptimization-based quality metric is then used for the evaluation of grasppoints to ensure indirect force closure. The proposed grasp framework iscompared with the existing state-of-the-art grasp planning approach, Grasp posedetection (GPD), as a baseline over multiple simulated objects. Theeffectiveness of the proposed approach in comparison to GPD is also evaluatedin a real-world setting using image and point-cloud data, with the plannedgrasps being executed using a ROBOTIQ gripper and UR5 manipulator.</description>
      <author>example@mail.com (Navin Sriram Ravie, Keerthi Vasan M, Asokan Thondiyath, Bijo Sebastian)</author>
      <guid isPermaLink="false">2504.19716v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
  <item>
      <title>WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System</title>
      <link>http://arxiv.org/abs/2504.18870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in IEEE TIM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的宽视场3D激光雷达车辆舱自动定位系统，以提高物流自动化中的操作效率和安全性。&lt;h4&gt;背景&lt;/h4&gt;自动化装载系统是物流自动化的重要组成部分，但在不同尺寸的货车舱中进行精确自动定位存在挑战，包括适应性、统一坐标系和可靠性问题。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有方法的局限性，实现大、中、小尺寸围栏式货车舱在杂乱环境中的关键点精确自动定位。&lt;h4&gt;方法&lt;/h4&gt;提出了一种利用LiDAR生成宽视场范围内高密度点云的方法，结合停车区域约束进行车辆点云分割，以及利用货车舱的几何特征进行关键点定位。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该系统具有可靠的定位精度和降低的计算资源消耗。&lt;h4&gt;结论&lt;/h4&gt;该系统能够应用于相关领域，提升物流自动化水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As an essential component of logistics automation, the automated loadingsystem is becoming a critical technology for enhancing operational efficiencyand safety. Precise automatic positioning of the truck compartment, whichserves as the loading area, is the primary step in automated loading. However,existing methods have difficulty adapting to truck compartments of varioussizes, do not establish a unified coordinate system for LiDAR and mobilemanipulators, and often exhibit reliability issues in cluttered environments.To address these limitations, our study focuses on achieving precise automaticpositioning of key points in large, medium, and small fence-style truckcompartments in cluttered scenarios. We propose an innovative widefield-of-view 3-D LiDAR vehicle compartment automatic localization system. Forvehicles of various sizes, this system leverages the LiDAR to generatehigh-density point clouds within an extensive field-of-view range. Byincorporating parking area constraints, our vehicle point cloud segmentationmethod more effectively segments vehicle point clouds within the scene. Ourcompartment key point positioning algorithm utilizes the geometric features ofthe compartments to accurately locate the corner points, providing stackablespatial regions. Extensive experiments on our collected data and publicdatasets demonstrate that this system offers reliable positioning accuracy andreduced computational resource consumption, leading to its application andpromotion in relevant fields.</description>
      <author>example@mail.com (Guodong Sun, Mingjing Li, Dingjie Liu, Mingxuan Liu, Bo Wu, Yang Zhang)</author>
      <guid isPermaLink="false">2504.18870v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2504.20024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://spatial-reasoner.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为SpatialReasoner的新颖的大视觉语言模型（LVLM），用于解决3D空间推理问题，并通过显式的3D表示在3D感知、计算和推理阶段之间共享，从而提高了空间推理的性能，并研究了LVLM在3D空间推理中可能出现的错误。&lt;h4&gt;背景&lt;/h4&gt;近期研究探索了基于数据驱动的3D空间推理方法，并使用强化学习（RL）实现了空间推理性能的提升。然而，这些方法通常以隐式的方式进行空间推理，且未充分研究在训练过程中获得的3D知识是否能够泛化到未见过的问答类型。&lt;h4&gt;目的&lt;/h4&gt;提出SpatialReasoner模型，旨在通过显式的3D表示来增强3D空间推理，并研究LVLM在3D空间推理中可能出现的错误。&lt;h4&gt;方法&lt;/h4&gt;SpatialReasoner模型采用了大视觉语言模型，并在3D感知、计算和推理阶段之间共享显式的3D表示，以支持高级3D空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;SpatialReasoner在多种空间推理基准测试中实现了改进的性能，并且在评估新颖的3D空间推理问题时表现出更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究将先前视觉基础模型的3D解析能力与大型语言模型的强大推理能力相结合，为3D空间推理开辟了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;Recent studies in 3D spatial reasoning explore data-driven approaches and achieve enhanced spatial reasoning performance with reinforcement learning (RL). However, these methods typically perform spatial reasoning in an implicit manner, and it remains underexplored whether the acquired 3D knowledge generalizes to unseen question types at any stage of the training. In this work we introduce SpatialReasoner, a novel large vision-language model (LVLM) that address 3D spatial reasoning with explicit 3D representations shared between stages -- 3D perception, computation, and reasoning. Explicit 3D representations provide a coherent interface that supports advanced 3D spatial reasoning and enable us to study the factual errors made by LVLMs. Results show that our SpatialReasoner achieve improved performance on a variety of spatial reasoning benchmarks and generalizes better when evaluating on novel 3D spatial reasoning questions. Our study bridges the 3D parsing capabilities of prior visual foundation models with the powerful reasoning abilities of large language models, opening new directions for 3D spatial reasoning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies in 3D spatial reasoning explore data-driven approaches andachieve enhanced spatial reasoning performance with reinforcement learning(RL). However, these methods typically perform spatial reasoning in an implicitmanner, and it remains underexplored whether the acquired 3D knowledgegeneralizes to unseen question types at any stage of the training. In this workwe introduce SpatialReasoner, a novel large vision-language model (LVLM) thataddress 3D spatial reasoning with explicit 3D representations shared betweenstages -- 3D perception, computation, and reasoning. Explicit 3Drepresentations provide a coherent interface that supports advanced 3D spatialreasoning and enable us to study the factual errors made by LVLMs. Results showthat our SpatialReasoner achieve improved performance on a variety of spatialreasoning benchmarks and generalizes better when evaluating on novel 3D spatialreasoning questions. Our study bridges the 3D parsing capabilities of priorvisual foundation models with the powerful reasoning abilities of largelanguage models, opening new directions for 3D spatial reasoning.</description>
      <author>example@mail.com (Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jieneng Chen, Jianwen Xie, Alan Yuille)</author>
      <guid isPermaLink="false">2504.20024v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Taming the Randomness: Towards Label-Preserving Cropping in Contrastive Learning</title>
      <link>http://arxiv.org/abs/2504.19824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对比学习（CL）在自监督学习（SSL）中的应用，提出了一种新的参数化裁剪方法，以提高自标签的鲁棒性和模型性能。&lt;h4&gt;背景&lt;/h4&gt;自监督学习（SSL）在深度学习，特别是计算机视觉（CV）领域，通过利用大量未标记数据来促进学习，对比学习（CL）是SSL方法中的一个成功分支。&lt;h4&gt;目的&lt;/h4&gt;为了提高自标签的鲁棒性和模型在下游任务中的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入了两种新的参数化裁剪方法，并与非参数化的随机裁剪方法进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用这些方法在CIFAR-10分类任务上，模型的准确性提高了2.7%到12.4%，具体取决于裁剪大小。&lt;h4&gt;结论&lt;/h4&gt;提出的参数化裁剪方法能够有效提高对比学习在自监督学习中的应用效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning (CL) approaches have gained great recognition as a verysuccessful subset of self-supervised learning (SSL) methods. SSL enableslearning from unlabeled data, a crucial step in the advancement of deeplearning, particularly in computer vision (CV), given the plethora of unlabeledimage data. CL works by comparing different random augmentations (e.g.,different crops) of the same image, thus achieving self-labeling. Nevertheless,randomly augmenting images and especially random cropping can result in animage that is semantically very distant from the original and therefore leadsto false labeling, hence undermining the efficacy of the methods. In thisresearch, two novel parameterized cropping methods are introduced that increasethe robustness of self-labeling and consequently increase the efficacy. Theresults show that the use of these methods significantly improves the accuracyof the model by between 2.7\% and 12.4\% on the downstream task of classifyingCIFAR-10, depending on the crop size compared to that of the non-parameterizedrandom cropping method.</description>
      <author>example@mail.com (Mohamed Hassan, Mohammad Wasil, Sebastian Houben)</author>
      <guid isPermaLink="false">2504.19824v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Category-Level and Open-Set Object Pose Estimation for Robotics</title>
      <link>http://arxiv.org/abs/2504.19572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Austrian Robotics Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文比较了用于解决类别级6D姿态估计的算法、数据集和精度指标，并分析了如何将类别级和开放集对象姿态估计连接起来以实现泛化，并提供有针对性的建议。&lt;h4&gt;背景&lt;/h4&gt;对象姿态估计在计算机视觉和机器人学中扮演重要角色，但类别级和开放集方法在处理未知纹理、形状和大小等基本材料属性时存在挑战。&lt;h4&gt;目的&lt;/h4&gt;比较不同方法在类别级6D姿态估计中的表现，并分析如何实现泛化。&lt;h4&gt;方法&lt;/h4&gt;本文通过比较不同的数据集、精度指标和算法，对类别级6D姿态估计进行了研究。&lt;h4&gt;主要发现&lt;/h4&gt;类别级和开放集对象姿态估计面临复杂性和不确定性，需要多种数据集、精度指标和算法解决方案。&lt;h4&gt;结论&lt;/h4&gt;研究提供了将类别级和开放集对象姿态估计连接起来的方法，以实现更广泛的泛化，并为实际应用提供指导。&lt;h4&gt;翻译&lt;/h4&gt;摘要：对象姿态估计在计算机视觉和机器人学中能够实现多种任务，包括场景理解和机器人抓取。姿态估计任务的复杂性取决于与目标对象相关的未知变量。尽管实例级方法在处理不透明和朗伯物体方面已经表现出色，但类别级和开放集方法（其中纹理、形状和大小部分或全部未知）在这些基本材料属性上仍然存在困难。在这些场景中，由于纹理未知，无法用于区分对象对称性，这是6D对象姿态估计的另一个核心挑战。估计具有如此众多未知因素的6D姿态的复杂性导致了各种数据集、精度指标和算法解决方案的出现。本文比较了用于解决类别级6D姿态估计的算法、数据集和精度指标。基于这种比较，我们分析了如何将类别级和开放集对象姿态估计连接起来以实现泛化，并提供有针对性的建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object pose estimation enables a variety of tasks in computer vision androbotics, including scene understanding and robotic grasping. The complexity ofa pose estimation task depends on the unknown variables related to the targetobject. While instance-level methods already excel for opaque and Lambertianobjects, category-level and open-set methods, where texture, shape, and sizeare partially or entirely unknown, still struggle with these basic materialproperties. Since texture is unknown in these scenarios, it cannot be used fordisambiguating object symmetries, another core challenge of 6D object poseestimation. The complexity of estimating 6D poses with such a manifold ofunknowns led to various datasets, accuracy metrics, and algorithmic solutions.This paper compares datasets, accuracy metrics, and algorithms for solving 6Dpose estimation on the category-level. Based on this comparison, we analyze howto bridge category-level and open-set object pose estimation to reachgeneralization and provide actionable recommendations.</description>
      <author>example@mail.com (Peter Hönig, Matthias Hirschmanner, Markus Vincze)</author>
      <guid isPermaLink="false">2504.19572v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Prediction of Nonlinear Optical Properties</title>
      <link>http://arxiv.org/abs/2504.19987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了非线性光学材料，通过深度学习技术预测其非线性光学性质，以加速新型光学材料的发现和设计。&lt;h4&gt;背景&lt;/h4&gt;非线性光学材料在激光生成领域有广泛应用，但发现具有显著二次谐波产生（SHG）性质的新材料困难重重，因为实验和理论计算耗时且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于深度学习的方法，使用原子线图神经网络（ALIGNN）预测非线性光学材料的性质，以加快新型光学材料的发现和设计。&lt;h4&gt;方法&lt;/h4&gt;从新型光电材料发现数据库（NOEMD）中获取数据，以库尔特斯-佩里（KP）系数作为关键目标，构建了一个鲁棒的模型，该模型能够准确估计非线性光学响应。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在可接受的绝对误差不超过1 pm/V和相对误差不超过0.5的条件下，实现了82.5%的准确率。&lt;h4&gt;结论&lt;/h4&gt;这项工作突出了深度学习在加速具有所需特性的先进光学材料发现和设计中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;This abstract is a summary of a study that uses deep learning to predict nonlinear optical properties of materials for laser generation. The research aims to accelerate the discovery and design of new optical materials. Data from the NOEMD database is used, and the Kurtz-Perry coefficient is targeted. The model achieves high accuracy and highlights the potential of deep learning in this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nonlinear optical (NLO) materials for generating lasers via second harmonicgeneration (SHG) are highly sought in today's technology. However, discoveringnovel materials with considerable SHG is challenging due to the time-consumingand costly nature of both experimental methods and first-principlescalculations. In this study, we present a deep learning approach using theAtomistic Line Graph Neural Network (ALIGNN) to predict NLO properties.Sourcing data from the Novel Opto-Electronic Materials Discovery (NOEMD)database and using the Kurtz-Perry (KP) coefficient as the key target, wedeveloped a robust model capable of accurately estimating nonlinear opticalresponses. Our results demonstrate that the model achieves 82.5% accuracy at atolerated absolute error up to 1 pm/V and relative error not exceeding 0.5.This work highlights the potential of deep learning in accelerating thediscovery and design of advanced optical materials with desired properties.</description>
      <author>example@mail.com (Yomn Alkabakibi, Congwei Xie, Artem R. Oganov)</author>
      <guid isPermaLink="false">2504.19987v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models</title>
      <link>http://arxiv.org/abs/2504.20020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模块化机器学习（MML）的新颖学习范式，旨在解决大型语言模型（LLMs）在推理、事实一致性和可解释性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;尽管LLMs在自然语言处理、计算机视觉、数据挖掘等领域推动了机器学习研究，但它们在推理、事实一致性和可解释性方面仍存在关键局限性。&lt;h4&gt;目的&lt;/h4&gt;通过引入MML，旨在增强LLMs的逆事实推理能力、减轻幻觉现象，并促进公平性、安全性和透明度。&lt;h4&gt;方法&lt;/h4&gt;MML将LLMs的复杂结构分解为三个相互依存的组件：模块化表示、模块化模型和模块化推理。通过使用解耦表示学习、神经架构搜索和神经符号学习等技术实现MML。&lt;h4&gt;主要发现&lt;/h4&gt;MML范式可以：1）通过解耦语义组件来阐明LLMs的内部工作机制；2）允许灵活且适应任务的模型设计；3）实现可解释且基于逻辑的决策过程。&lt;h4&gt;结论&lt;/h4&gt;MML与LLMs的结合有望弥合统计（深度）学习和形式（逻辑）推理之间的差距，为广泛实际应用中的稳健、适应性强和值得信赖的AI系统铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have dramatically advanced machine learningresearch including natural language processing, computer vision, data mining,etc., yet they still exhibit critical limitations in reasoning, factualconsistency, and interpretability. In this paper, we introduce a novel learningparadigm -- Modular Machine Learning (MML) -- as an essential approach towardnew-generation LLMs. MML decomposes the complex structure of LLMs into threeinterdependent components: modular representation, modular model, and modularreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,mitigating hallucinations, as well as promoting fairness, safety, andtransparency. Specifically, the proposed MML paradigm can: i) clarify theinternal working mechanism of LLMs through the disentanglement of semanticcomponents; ii) allow for flexible and task-adaptive model design; iii) enableinterpretable and logic-driven decision-making process. We present a feasibleimplementation of MML-based LLMs via leveraging advanced techniques such asdisentangled representation learning, neural architecture search andneuro-symbolic learning. We critically identify key challenges, such as theintegration of continuous neural and discrete symbolic processes, jointoptimization, and computational scalability, present promising future researchdirections that deserve further exploration. Ultimately, the integration of theMML paradigm with LLMs has the potential to bridge the gap between statistical(deep) learning and formal (logical) reasoning, thereby paving the way forrobust, adaptable, and trustworthy AI systems across a wide range of real-worldapplications.</description>
      <author>example@mail.com (Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu)</author>
      <guid isPermaLink="false">2504.20020v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Uncertainty-Aware Graph Neural Network</title>
      <link>http://arxiv.org/abs/2504.19820v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为HU-GNN的新型图神经网络架构，该架构通过统一多尺度表示学习、原理性不确定性估计和自监督嵌入多样性，有效缓解了数据稀疏性和利用结构属性。&lt;h4&gt;背景&lt;/h4&gt;近年来，图神经网络（GNNs）的研究探索了捕获局部不确定性和利用图层次结构来缓解数据稀疏性和利用结构属性的方法，但这两者之间的协同集成尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出HU-GNN的目的是为了解决上述问题，实现多尺度表示学习、不确定性估计和嵌入多样性的统一。&lt;h4&gt;方法&lt;/h4&gt;HU-GNN通过自适应形成节点簇，并在多个结构尺度上从单个节点到更高级别估计不确定性。这些不确定性估计引导了一个鲁棒的消息传递机制和注意力权重，从而有效缓解了噪声和对抗性扰动，同时保持了节点和图级别任务的预测精度。&lt;h4&gt;主要发现&lt;/h4&gt;本文提出了概率公式、严格的不确定性校准保证和正式的鲁棒性界限等关键理论贡献。通过结合图对比学习的最新进展，HU-GNN保持了多样性和结构上忠实嵌入。&lt;h4&gt;结论&lt;/h4&gt;在标准基准上的大量实验表明，该模型实现了最先进的鲁棒性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;最近关于图神经网络（GNNs）的研究探讨了捕获局部不确定性和利用图层次结构来缓解数据稀疏性和利用结构属性的方法。然而，这两种方法的协同集成尚未得到充分探索。在本工作中，我们引入了一种新的架构，即分层不确定性感知图神经网络（HU-GNN），它在一个端到端的框架内统一了多尺度表示学习、原理性不确定性估计和自监督嵌入多样性。具体来说，HU-GNN自适应地形成节点簇，并从单个节点到更高级别估计多个结构尺度的不确定性。这些不确定性估计引导了一个鲁棒的消息传递机制和注意力权重，有效地缓解了噪声和对抗性扰动，同时在节点和图级别任务上保持了预测精度。我们还提供了关键的理论贡献，包括概率公式、严格的不确定性校准保证和正式的鲁棒性界限。最后，通过结合图对比学习的最新进展，HU-GNN保持了多样性和结构上忠实嵌入。在标准基准上的大量实验表明，我们的模型实现了最先进的鲁棒性和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research on graph neural networks (GNNs) has explored mechanisms forcapturing local uncertainty and exploiting graph hierarchies to mitigate datasparsity and leverage structural properties. However, the synergisticintegration of these two approaches remains underexplored. In this work, weintroduce a novel architecture, the Hierarchical Uncertainty-Aware Graph NeuralNetwork (HU-GNN), which unifies multi-scale representation learning, principleduncertainty estimation, and self-supervised embedding diversity within a singleend-to-end framework. Specifically, HU-GNN adaptively forms node clusters andestimates uncertainty at multiple structural scales from individual nodes tohigher levels. These uncertainty estimates guide a robust message-passingmechanism and attention weighting, effectively mitigating noise and adversarialperturbations while preserving predictive accuracy on both node- andgraph-level tasks. We also offer key theoretical contributions, including aprobabilistic formulation, rigorous uncertainty-calibration guarantees, andformal robustness bounds. Finally, by incorporating recent advances in graphcontrastive learning, HU-GNN maintains diverse, structurally faithfulembeddings. Extensive experiments on standard benchmarks demonstrate that ourmodel achieves state-of-the-art robustness and interpretability.</description>
      <author>example@mail.com (Yoonhyuk Choi, Chong-Kwon Kim)</author>
      <guid isPermaLink="false">2504.19820v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity</title>
      <link>http://arxiv.org/abs/2504.19581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMBLE的方法，用于学习特定形状的点云采样策略，以实现更精确和高效的3D数据表示。&lt;h4&gt;背景&lt;/h4&gt;随着对3D数据精确和高效表示的需求增加，点云采样已成为3D计算机视觉中的一个关键研究课题。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有学习型采样方法存在的问题，如产生不可识别的采样模式或偏置采样结果，同时忽略不同形状点分布的自然变化。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于稀疏注意力图和分箱的SAMBLE方法，用于学习特定形状的点云采样策略。&lt;h4&gt;主要发现&lt;/h4&gt;SAMBLE有效地在采样边缘点以获得局部细节和保持全局形状均匀性之间实现了平衡，在多个常见的点云下游任务中取得了优越的性能，即使在少数点采样的情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;SAMBLE方法能够显著提高点云采样在3D数据表示中的应用效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driven by the increasing demand for accurate and efficient representation of3D data in various domains, point cloud sampling has emerged as a pivotalresearch topic in 3D computer vision. Recently, learning-to-sample methods havegarnered growing interest from the community, particularly for their ability tobe jointly trained with downstream tasks. However, previous learning-basedsampling methods either lead to unrecognizable sampling patterns by generatinga new point cloud or biased sampled results by focusing excessively on sharpedge details. Moreover, they all overlook the natural variations in pointdistribution across different shapes, applying a similar sampling strategy toall point clouds. In this paper, we propose a Sparse Attention Map andBin-based Learning method (termed SAMBLE) to learn shape-specific samplingstrategies for point cloud shapes. SAMBLE effectively achieves an improvedbalance between sampling edge points for local details and preservinguniformity in the global shape, resulting in superior performance acrossmultiple common point cloud downstream tasks, even in scenarios with few-pointsampling.</description>
      <author>example@mail.com (Chengzhi Wu, Yuxin Wan, Hao Fu, Julius Pfrommer, Zeyun Zhong, Junwei Zheng, Jiaming Zhang, Jürgen Beyerer)</author>
      <guid isPermaLink="false">2504.19581v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Under High-Dimensional Network Convolutional Regression Model</title>
      <link>http://arxiv.org/abs/2504.19979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于网络卷积回归（NCR）的高维迁移学习框架，用于解决网络数据中的依赖性问题，并通过仿真和真实世界应用验证了其在预测精度上的提升。&lt;h4&gt;背景&lt;/h4&gt;迁移学习通过利用相关领域的知识提升模型性能，特别是在标注数据稀缺的情况下。然而，现有研究主要关注独立设置下的分布变化，而处理网络数据中的依赖性仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种高维迁移学习框架，解决网络数据中的依赖性问题，并提高预测精度。&lt;h4&gt;方法&lt;/h4&gt;1. 基于网络卷积回归（NCR）模型，允许节点的响应依赖于其特征和邻居的聚合特征，有效捕获局部依赖性；2. 设计了两个步骤的迁移学习算法，处理源和目标网络之间的领域差异；3. 引入源检测机制来识别信息丰富的领域；4. 理论上分析基于Erdős-Rényi模型的随机图上的lasso估计器，证明在存在信息源时，迁移学习可以提高收敛速度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在预测精度上取得了显著提升，特别是在目标域的标注数据有限的情况下。&lt;h4&gt;结论&lt;/h4&gt;提出的NCR迁移学习框架能够有效处理网络数据中的依赖性，并显著提高预测精度，特别是在数据稀缺的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning enhances model performance by utilizing knowledge fromrelated domains, particularly when labeled data is scarce. While existingresearch addresses transfer learning under various distribution shifts inindependent settings, handling dependencies in networked data remainschallenging. To address this challenge, we propose a high-dimensional transferlearning framework based on network convolutional regression (NCR), inspired bythe success of graph convolutional networks (GCNs). The NCR model incorporatesrandom network structure by allowing each node's response to depend on itsfeatures and the aggregated features of its neighbors, capturing localdependencies effectively. Our methodology includes a two-step transfer learningalgorithm that addresses domain shift between source and target networks, alongwith a source detection mechanism to identify informative domains.Theoretically, we analyze the lasso estimator in the context of a random graphbased on the Erdos-Renyi model assumption, demonstrating that transfer learningimproves convergence rates when informative sources are present. Empiricalevaluations, including simulations and a real-world application using SinaWeibo data, demonstrate substantial improvements in prediction accuracy,particularly when labeled data in the target domain is limited.</description>
      <author>example@mail.com (Liyuan Wang, Jiachen Chen, Kathryn L. Lunetta, Danyang Huang, Huimin Cheng, Debarghya Mukherjee)</author>
      <guid isPermaLink="false">2504.19979v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Streaming Video Representation via Multitask Training</title>
      <link>http://arxiv.org/abs/2504.20041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report. Project Page:  https://go2heart.github.io/streamformer&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了StreamFormer，一种新的流式视频处理框架，用于实时应用，如具身AI和自动驾驶。StreamFormer通过引入因果时序注意力机制，在预训练视觉Transformer中实现了高效的流式视频处理，同时保持了图像表示能力。&lt;h4&gt;背景&lt;/h4&gt;在实时应用中，理解连续视频流非常重要。与离线视频理解不同，流式视频理解需要逐帧处理视频流，保留历史信息，并做出低延迟决策。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决流式视频理解中的挑战，如高效处理视频流、保留历史信息和低延迟决策。&lt;h4&gt;方法&lt;/h4&gt;（i）开发了一种新的流式视频主干网络StreamFormer，通过将因果时序注意力机制整合到预训练视觉Transformer中；（ii）提出将多种时空视频理解任务统一到一个多任务视觉-语言对齐框架中，以训练StreamFormer；（iii）在在线动作检测、在线视频实例分割和视频问答等任务上进行了广泛的实验。&lt;h4&gt;主要发现&lt;/h4&gt;StreamFormer在保持效率的同时，在在线动作检测、在线视频实例分割和视频问答等任务上实现了有竞争力的结果，证明了其在实时应用中的潜力。&lt;h4&gt;结论&lt;/h4&gt;StreamFormer是一种高效的流式视频处理框架，适用于实时应用，如具身AI和自动驾驶，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;Understanding continuous video streams plays a fundamental role in real-time applications including embodied AI and autonomous driving. Unlike offline video understanding, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions. To address these challenges, our main contributions are three-fold. (i) We develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability. (ii) To train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) We conduct extensive experiments on online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive results while maintaining efficiency, demonstrating its potential for real-time applications.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding continuous video streams plays a fundamental role in real-timeapplications including embodied AI and autonomous driving. Unlike offline videounderstanding, streaming video understanding requires the ability to processvideo streams frame by frame, preserve historical information, and makelow-latency decisions.To address these challenges, our main contributions arethree-fold. (i) We develop a novel streaming video backbone, termed asStreamFormer, by incorporating causal temporal attention into a pre-trainedvision transformer. This enables efficient streaming video processing whilemaintaining image representation capability.(ii) To train StreamFormer, wepropose to unify diverse spatial-temporal video understanding tasks within amultitask visual-language alignment framework. Hence, StreamFormer learnsglobal semantics, temporal dynamics, and fine-grained spatial relationshipssimultaneously. (iii) We conduct extensive experiments on online actiondetection, online video instance segmentation, and video question answering.StreamFormer achieves competitive results while maintaining efficiency,demonstrating its potential for real-time applications.</description>
      <author>example@mail.com (Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie)</author>
      <guid isPermaLink="false">2504.20041v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>A Review of 3D Object Detection with Vision-Language Models</title>
      <link>http://arxiv.org/abs/2504.18738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对3D对象检测与视觉语言模型（VLMs）的综述进行了系统分析。&lt;h4&gt;背景&lt;/h4&gt;3D对象检测是3D视觉与多模态AI交叉领域的快速发展的一个领域。&lt;h4&gt;目的&lt;/h4&gt;提供首个针对3D对象检测与视觉语言模型的系统分析。&lt;h4&gt;方法&lt;/h4&gt;通过审查超过100篇研究论文，分析3D对象检测的独特挑战，比较传统方法和现代视觉语言框架，并回顾关键架构、预训练策略和提示工程方法。&lt;h4&gt;主要发现&lt;/h4&gt;分析了3D对象检测与视觉语言模型在空间推理和数据复杂性方面的差异，并讨论了性能和评估基准。&lt;h4&gt;结论&lt;/h4&gt;强调了当前挑战，如有限的3D语言数据集和计算需求，并提出了未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;This review provides a systematic analysis of comprehensive survey of 3Dobject detection with vision-language models(VLMs), a rapidly advancing areaat the intersection of 3D vision and multimodal AI. By examining over 100research papers, we provide the first systematic analysis dedicated to 3Dobject detection with vision-language models. We begin by outlining the uniquechallenges of 3D object detection with vision-language models, emphasizingdifferences from 2D detection in spatial reasoning and data complexity.Traditional approaches using point clouds and voxel grids are compared tomodern vision-language frameworks like CLIP and 3D LLMs, which enableopen-vocabulary detection and zero-shot generalization. We review keyarchitectures, pretraining strategies, and prompt engineering methods thatalign textual and 3D features for effective 3D object detection withvision-language models. Visualization examples and evaluation benchmarks arediscussed to illustrate performance and behavior. Finally, we highlight currentchallenges, such as limited 3D-language datasets and computational demands, andpropose future research directions to advance 3D object detection withvision-language models. &gt;Object Detection, Vision-Language Models, Agents,VLMs, LLMs, AI&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This review provides a systematic analysis of comprehensive survey of 3Dobject detection with vision-language models(VLMs) , a rapidly advancing areaat the intersection of 3D vision and multimodal AI. By examining over 100research papers, we provide the first systematic analysis dedicated to 3Dobject detection with vision-language models. We begin by outlining the uniquechallenges of 3D object detection with vision-language models, emphasizingdifferences from 2D detection in spatial reasoning and data complexity.Traditional approaches using point clouds and voxel grids are compared tomodern vision-language frameworks like CLIP and 3D LLMs, which enableopen-vocabulary detection and zero-shot generalization. We review keyarchitectures, pretraining strategies, and prompt engineering methods thatalign textual and 3D features for effective 3D object detection withvision-language models. Visualization examples and evaluation benchmarks arediscussed to illustrate performance and behavior. Finally, we highlight currentchallenges, such as limited 3D-language datasets and computational demands, andpropose future research directions to advance 3D object detection withvision-language models. &gt;Object Detection, Vision-Language Models, Agents,VLMs, LLMs, AI</description>
      <author>example@mail.com (Ranjan Sapkota, Konstantinos I Roumeliotis, Rahul Harsha Cheppally, Marco Flores Calero, Manoj Karkee)</author>
      <guid isPermaLink="false">2504.18738v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>HJRNO: Hamilton-Jacobi Reachability with Neural Operators</title>
      <link>http://arxiv.org/abs/2504.19989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了HJRNO，一种基于神经操作符的框架，用于高效准确地解决向后可达管（BRTs），以保障自主系统在不确定环境下的安全性。&lt;h4&gt;背景&lt;/h4&gt;确保自主系统在不确定性下的安全性是关键挑战。传统的HJR分析方法在提供安全性保证的同时，受到维度灾难的限制，限制了其在高维系统或变化环境条件下的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，即HJRNO，以解决传统HJR方法在处理高维系统时的可扩展性问题，并实现高效准确的安全分析。&lt;h4&gt;方法&lt;/h4&gt;HJRNO利用傅里叶神经网络操作符（FNO）学习值函数之间的映射，通过这种方式，可以实现快速推理并具有良好的泛化能力，能够适应不同的障碍物形状、系统配置和超参数。&lt;h4&gt;主要发现&lt;/h4&gt;HJRNO在随机障碍物场景中实现了低误差，并在不同的系统动力学下表现出有效的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;HJRNO为自主系统提供了一种可扩展且实时的安全分析方法，是一种有前景的基础模型方法。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于神经操作符的框架HJRNO，用于解决向后可达管问题，以提高自主系统在不确定性环境下的安全性。传统方法在处理高维系统时受到限制，而HJRNO通过傅里叶神经网络操作符实现了高效和准确的解决方案，展现出良好的泛化能力，为自主系统的实时安全分析提供了有希望的基础模型方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring the safety of autonomous systems under uncertainty is a criticalchallenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used methodfor guaranteeing safety under worst-case disturbances. Traditional HJR methodsprovide safety guarantees but suffer from the curse of dimensionality, limitingtheir scalability to high-dimensional systems or varying environmentalconditions. In this work, we propose HJRNO, a neural operator-based frameworkfor solving backward reachable tubes (BRTs) efficiently and accurately. Byleveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping betweenvalue functions, enabling fast inference with strong generalization acrossdifferent obstacle shapes, system configurations, and hyperparameters. Wedemonstrate that HJRNO achieves low error on random obstacle scenarios andgeneralizes effectively across varying system dynamics. These results suggestthat HJRNO offers a promising foundation model approach for scalable, real-timesafety analysis in autonomous systems.</description>
      <author>example@mail.com (Yankai Li, Mo Chen)</author>
      <guid isPermaLink="false">2504.19989v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2504.19500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MPEC的新型掩码点-实体对比学习方法，用于开放词汇的3D语义分割，旨在通过实体-语言对齐和点-实体一致性来提升实体特征表示，实现了在ScanNet上的最先进性能和零样本场景理解能力。&lt;h4&gt;背景&lt;/h4&gt;开放词汇的3D场景理解对于增强物理智能至关重要，因为它使具身智能体能够解释和动态交互于现实世界环境中。&lt;h4&gt;目的&lt;/h4&gt;提出MPEC方法，旨在改善语义区分能力，增强独特实例的区分度，并展示该方法在零样本场景理解上的优势。&lt;h4&gt;方法&lt;/h4&gt;MPEC方法结合了3D实体-语言对齐和点-实体一致性，用于开放词汇的3D语义分割，并在8个数据集上进行了广泛的微调实验，这些数据集涵盖了从低级感知到高级推理任务。&lt;h4&gt;主要发现&lt;/h4&gt;MPEC方法在ScanNet上取得了最先进的结果，并在多种3D场景理解任务上展示了稳定的表现。&lt;h4&gt;结论&lt;/h4&gt;MPEC方法展示了3D特征的潜力，并能够在不同的3D场景理解任务中带来持续的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;摘要：开放式词汇的3D场景理解对于提高物理智能至关重要，因为它允许具身智能体解释和动态地与真实世界环境互动。本文提出了一种名为MPEC的新型掩码点-实体对比学习方法，用于开放式词汇的3D语义分割，它利用了3D实体-语言对齐和不同点云视图中的点-实体一致性来培养特定于实体的特征表示。该方法提高了语义区分度，增强了独特实例的区分能力，在ScanNet开放式词汇的3D语义分割任务上取得了最先进的成果，并展示了卓越的零样本场景理解能力。在从低级感知到高级推理任务的8个数据集上进行的广泛微调实验展示了学习到的3D特征的潜力，推动在不同3D场景理解任务中的性能持续提升。项目网站：https://mpec-3d.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D scene understanding is pivotal for enhancing physicalintelligence, as it enables embodied agents to interpret and interactdynamically within real-world environments. This paper introduces MPEC, a novelMasked Point-Entity Contrastive learning method for open-vocabulary 3D semanticsegmentation that leverages both 3D entity-language alignment and point-entityconsistency across different point cloud views to foster entity-specificfeature representations. Our method improves semantic discrimination andenhances the differentiation of unique instances, achieving state-of-the-artresults on ScanNet for open-vocabulary 3D semantic segmentation anddemonstrating superior zero-shot scene understanding capabilities. Extensivefine-tuning experiments on 8 datasets, spanning from low-level perception tohigh-level reasoning tasks, showcase the potential of learned 3D features,driving consistent performance gains across varied 3D scene understandingtasks. Project website: https://mpec-3d.github.io/</description>
      <author>example@mail.com (Yan Wang, Baoxiong Jia, Ziyu Zhu, Siyuan Huang)</author>
      <guid isPermaLink="false">2504.19500v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Heterophily-informed Message Passing</title>
      <link>http://arxiv.org/abs/2504.19785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appearing in Transactions on Machine Learning Research (TMLR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图神经网络（GNN）方法，旨在解决GNN过度平滑的问题，该方法通过调节消息的聚合来保护信息中的低频和高频成分。&lt;h4&gt;背景&lt;/h4&gt;GNNs由于其隐含的同质性假设而容易受到过度平滑的影响。&lt;h4&gt;目的&lt;/h4&gt;缓解GNN的过度平滑问题，同时保留信息中的低频和高频成分。&lt;h4&gt;方法&lt;/h4&gt;该方法依赖于学习到的嵌入，无需辅助标签，从而将异质性感知嵌入的好处扩展到更广泛的应用，例如生成建模。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在各种数据集和GNN架构上都有性能提升，并揭示了标准分类基准中的异质性模式。此外，在分子生成中的应用在化学信息学基准上展示了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;提出的方案有效地减轻了GNN的过度平滑问题，并提高了其性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due to their implicit homophily assumption. We mitigate this problem with a novel scheme that regulates the aggregation of messages, modulating the type and extent of message passing locally thereby preserving both the low and high-frequency components of information. Our approach relies solely on learned embeddings, obviating the need for auxiliary labels, thus extending the benefits of heterophily-aware embeddings to broader applications, e.g., generative modelling. Our experiments, conducted across various data sets and GNN architectures, demonstrate performance enhancements and reveal heterophily patterns across standard classification benchmarks. Furthermore, application to molecular generation showcases notable performance improvements on chemoinformatics benchmarks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are known to be vulnerable to oversmoothing dueto their implicit homophily assumption. We mitigate this problem with a novelscheme that regulates the aggregation of messages, modulating the type andextent of message passing locally thereby preserving both the low andhigh-frequency components of information. Our approach relies solely on learntembeddings, obviating the need for auxiliary labels, thus extending thebenefits of heterophily-aware embeddings to broader applications, e.g.,generative modelling. Our experiments, conducted across various data sets andGNN architectures, demonstrate performance enhancements and reveal heterophilypatterns across standard classification benchmarks. Furthermore, application tomolecular generation showcases notable performance improvements onchemoinformatics benchmarks.</description>
      <author>example@mail.com (Haishan Wang, Arno Solin, Vikas Garg)</author>
      <guid isPermaLink="false">2504.19785v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes</title>
      <link>http://arxiv.org/abs/2504.19557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in 2025 IEEE/CVF Conference on Computer Vision and Pattern  Recognition Workshops (CVPRW)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的基于神经的点云合成方法，用于解决大规模自动驾驶场景中新型视图合成的问题。&lt;h4&gt;背景&lt;/h4&gt;当前基于点的三维点云地图在用于新型视图合成（NVS）时，由于可扩展性和渲染质量受限，导致可视化效果下降。&lt;h4&gt;目的&lt;/h4&gt;为了解决渲染质量低的问题，提出了一种名为CE-NPBG的新方法，用于大规模自动驾驶场景中的新型视图合成。&lt;h4&gt;方法&lt;/h4&gt;该方法利用两种模态：摆姿势的图像（相机）和同步的原始3D点云（LiDAR）。它首先使用外观和几何之间的连接关系图，从当前相机视角检索3D点云图中的点进行渲染。此外，该方法将神经网络描述符与点关联，并使用它们来合成视图。为了提高描述符的编码质量和渲染质量，提出了联合对抗和点光栅化训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用连接关系图，该方法显著提高了渲染质量，并通过仅使用大型3D点云图的一小部分点来增强运行时间和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法对于提高渲染质量和可扩展性具有显著优势，并已集成到最近的3D高斯喷溅工作中。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当前基于点的三维点云地图在用于新型视图合成（NVS）时，由于可扩展性和渲染质量受限，导致可视化效果下降。我们识别出这些低质量渲染背后的主要问题是几何和外观之间的可见性不匹配，这是由于同时使用这两种模态造成的。为了解决这个问题，我们提出了CE-NPBG，这是一种用于大规模自动驾驶场景中新型视图合成（NVS）的新方法。我们的方法是一种神经点云技术，利用两种模态：摆姿势的图像（相机）和同步的原始3D点云（LiDAR）。我们首先使用外观和几何之间的连接关系图，从当前相机视角检索3D点云图中的点进行渲染。通过利用这种连接关系，我们的方法显著提高了渲染质量，并通过仅使用大型3D点云图的一小部分点来增强运行时间和可扩展性。我们的方法将神经网络描述符与点关联，并使用它们来合成视图。为了提高这些描述符的编码质量和渲染质量，我们提出了联合对抗和点光栅化训练。在训练期间，我们将图像合成网络与多分辨率判别器配对。在推理期间，我们将它们解耦，并使用图像合成网络生成新型视图。我们还把我们提出的方案集成到最近的3D高斯喷溅工作中，以突出其在提高渲染和可扩展性方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current point-based approaches encounter limitations in scalability andrendering quality when using large 3D point cloud maps because using themdirectly for novel view synthesis (NVS) leads to degraded visualizations. Weidentify the primary issue behind these low-quality renderings as a visibilitymismatch between geometry and appearance, stemming from using these twomodalities together. To address this problem, we present CE-NPBG, a newapproach for novel view synthesis (NVS) in large-scale autonomous drivingscenes. Our method is a neural point-based technique that leverages twomodalities: posed images (cameras) and synchronized raw 3D point clouds(LiDAR). We first employ a connectivity relationship graph between appearanceand geometry, which retrieves points from a large 3D point cloud map observedfrom the current camera perspective and uses them for rendering. By leveragingthis connectivity, our method significantly improves rendering quality andenhances run-time and scalability by using only a small subset of points fromthe large 3D point cloud map. Our approach associates neural descriptors withthe points and uses them to synthesize views. To enhance the encoding of thesedescriptors and elevate rendering quality, we propose a joint adversarial andpoint rasterization training. During training, we pair an image-synthesizernetwork with a multi-resolution discriminator. At inference, we decouple themand use the image-synthesizer to generate novel views. We also integrate ourproposal into the recent 3D Gaussian Splatting work to highlight its benefitsfor improved rendering and scalability.</description>
      <author>example@mail.com (Mohammad Altillawi, Fengyi Shen, Liudi Yang, Sai Manoj Prakhya, Ziyuan Liu)</author>
      <guid isPermaLink="false">2504.19557v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction</title>
      <link>http://arxiv.org/abs/2504.19545v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Point2Quad的基于学习的四边形网格生成方法，用于从点云中生成四边形网格。&lt;h4&gt;背景&lt;/h4&gt;尽管基于学习的三角形网格生成方法取得了显著进展，但由于确保共面性、凸性和仅使用四边形的挑战，四边形网格生成仍然相对较少探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的学习算法，能够从点云中生成符合要求的四边形网格。&lt;h4&gt;方法&lt;/h4&gt;Point2Quad通过以下步骤实现：使用k-NN生成候选四边形，考虑共面性和正方形性；使用两个编码器提取几何和拓扑特征；结合四边形特定的特性来解决四边形相关约束问题；融合提取的特征以训练分类器，并设计复合损失函数；最后，通过四边形特定的后处理进行细化。&lt;h4&gt;主要发现&lt;/h4&gt;在清晰和噪声数据上进行的广泛实验证明了Point2Quad的有效性和优越性，与基线方法相比，在综合指标下表现更优。&lt;h4&gt;结论&lt;/h4&gt;Point2Quad是一种有效的基于学习的方法，可以用于从点云中生成高质量的四边形网格。&lt;h4&gt;翻译&lt;/h4&gt;摘要：四边形网格在几何建模和计算力学中至关重要。尽管基于学习的三角形网格生成方法取得了显著进展，但由于确保共面性、凸性和仅使用四边形的挑战，四边形网格生成仍然相对较少探索。在本文中，我们提出了Point2Quad，这是第一个从点云生成仅四边形网格的基于学习的方法。其关键思想是学习识别融合了点级和面级特征的四边形网格。具体来说，Point2Quad从基于k-NN的候选生成开始，考虑共面性和正方形性。然后，跟随两个编码器提取几何和拓扑特征，以解决四边形相关约束的挑战，特别是通过结合深入的特定于四边形的特性。随后，提取的特征被融合以训练具有设计复合损失的分类器。最后，通过四边形特定的后处理进行细化。在清晰和噪声数据上的广泛实验证明了Point2Quad的有效性和优越性，与基线方法相比，在综合指标下表现更优。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TCSVT.2025.3556130&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quad meshes are essential in geometric modeling and computational mechanics.Although learning-based methods for triangle mesh demonstrate considerableadvancements, quad mesh generation remains less explored due to the challengeof ensuring coplanarity, convexity, and quad-only meshes. In this paper, wepresent Point2Quad, the first learning-based method for quad-only meshgeneration from point clouds. The key idea is learning to identify quad meshwith fused pointwise and facewise features. Specifically, Point2Quad beginswith a k-NN-based candidate generation considering the coplanarity andsquareness. Then, two encoders are followed to extract geometric andtopological features that address the challenge of quad-related constraints,especially by combining in-depth quadrilaterals-specific characteristics.Subsequently, the extracted features are fused to train the classifier with adesigned compound loss. The final results are derived after the refinement by aquad-specific post-processing. Extensive experiments on both clear and noisedata demonstrate the effectiveness and superiority of Point2Quad, compared tobaseline methods under comprehensive metrics.</description>
      <author>example@mail.com (Zezeng Li, Zhihui Qi, Weimin Wang, Ziliang Wang, Junyi Duan, Na Lei)</author>
      <guid isPermaLink="false">2504.19545v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Do You Know the Way? Human-in-the-Loop Understanding for Fast Traversability Estimation in Mobile Robotics</title>
      <link>http://arxiv.org/abs/2504.19851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by RA-L. Code is available at  https://github.com/andreschreiber/CHUNGUS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于在非结构化环境中进行机器导航的可穿越性估计方法，旨在解决现有方法的局限性和提高可穿越性预测的性能。&lt;h4&gt;背景&lt;/h4&gt;随着机器人在非结构化环境中应用的增加，有效的感知和导航策略变得至关重要，特别是在可穿越性估计方面。&lt;h4&gt;目的&lt;/h4&gt;设计一种新的方法来改善机器人在环境中移动的能力，通过准确地预测它们可以或不可以到达的区域。&lt;h4&gt;方法&lt;/h4&gt;提出了一个结合人工参与（HiL）的可穿越性估计方法，该方法根据需要向人类请求标注，并使用基础模型快速学习新标注，即使在标注数量较少的情况下也能提供准确的预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在仿真和实际数据集上进行了广泛验证，证明了其能提供业界领先的可穿越性预测性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合人工智能和人工参与，在提高可穿越性估计准确性方面具有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3563819&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing use of robots in unstructured environments necessitates thedevelopment of effective perception and navigation strategies to enable fieldrobots to successfully perform their tasks. In particular, it is key for suchrobots to understand where in their environment they can and cannot travel -- atask known as traversability estimation. However, existing geometric approachesto traversability estimation may fail to capture nuanced representations oftraversability, whereas vision-based approaches typically either involvemanually annotating a large number of images or require robot experience. Inaddition, existing methods can struggle to address domain shifts as theytypically do not learn during deployment. To this end, we propose ahuman-in-the-loop (HiL) method for traversability estimation that prompts ahuman for annotations as-needed. Our method uses a foundation model to enablerapid learning on new annotations and to provide accurate predictions even whentrained on a small number of quickly-provided HiL annotations. We extensivelyvalidate our method in simulation and on real-world data, and demonstrate thatit can provide state-of-the-art traversability prediction performance.</description>
      <author>example@mail.com (Andre Schreiber, Katherine Driggs-Campbell)</author>
      <guid isPermaLink="false">2504.19851v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Physical Reach: Comparing Head- and Cane-Mounted Cameras for Last-Mile Navigation by Blind Users</title>
      <link>http://arxiv.org/abs/2504.19345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过调查和实验研究，探讨了盲人在导航过程中面临的挑战，并提出了基于多传感器融合的导航辅助系统设计方案。&lt;h4&gt;背景&lt;/h4&gt;盲人在导航过程中存在困难，如定位入口、识别障碍物以及导航复杂或拥挤的空间。&lt;h4&gt;目的&lt;/h4&gt;为了填补现有辅助系统设计的空白，本文通过两部分研究来指导设计：调查盲人的导航策略和偏好，以及通过实验研究不同视角下的导航效果。&lt;h4&gt;方法&lt;/h4&gt;首先，通过调查10位经验丰富的盲人使用拐杖的用户，了解他们的导航策略、痛点和技术偏好；其次，通过实验，让一位盲人使用同步的头戴式和拐杖式摄像头在五个真实环境中导航，以视角放置为变量，评估不同视角对空间感知的支持，包括SLAM性能和基于NeRF的3D重建。&lt;h4&gt;主要发现&lt;/h4&gt;头戴式传感器提供了更高的定位精度，而拐杖式视角提供了更全面的地面覆盖和丰富的环境重建。头戴式和拐杖式结合的配置在性能上优于单独使用。&lt;h4&gt;结论&lt;/h4&gt;不同传感器的放置位置具有互补的优势，为开发感知能力强、鲁棒且用户友好的混合导航辅助系统提供了实际指导。&lt;h4&gt;翻译&lt;/h4&gt;摘要：盲人在最后一英里导航中面临持续挑战，包括定位入口、识别障碍物以及导航复杂或拥挤的空间。尽管可穿戴摄像头在辅助系统中越来越被使用，但还没有系统性的、以视角为焦点的比较来指导其设计。本文通过两部分研究来填补这一空白。首先，我们调查了10位有经验的盲人拐杖使用者，揭示了导航策略、痛点和技术偏好。参与者强调了多感官整合、以目的地为导向的旅行和补充（而不是替代）拐杖触觉功能辅助工具的重要性。其次，我们进行了一项受控数据收集，让一位盲人在使用同步的头戴式和拐杖式摄像头在五个真实环境中导航，将视角放置作为主要变量。为了评估每个视角如何支持空间感知，我们评估了SLAM性能（用于定位和制图）和基于NeRF的3D重建（用于下游场景理解）。头戴式传感器提供了优越的定位精度，而拐杖式视角提供了更广泛的地面覆盖和更丰富的环境重建。头戴式和拐杖式结合的配置在性能上始终优于单独使用。这些结果突出了不同传感器放置位置的互补优势，并为开发感知能力强、鲁棒且用户友好的混合导航辅助系统提供了实际指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Blind individuals face persistent challenges in last-mile navigation,including locating entrances, identifying obstacles, and navigating complex orcluttered spaces. Although wearable cameras are increasingly used in assistivesystems, there has been no systematic, vantage-focused comparison to guidetheir design. This paper addresses that gap through a two-part investigation.First, we surveyed ten experienced blind cane users, uncovering navigationstrategies, pain points, and technology preferences. Participants stressed theimportance of multi-sensory integration, destination-focused travel, andassistive tools that complement (rather than replace) the cane's tactileutility. Second, we conducted controlled data collection with a blindparticipant navigating five real-world environments using synchronized head-and cane-mounted cameras, isolating vantage placement as the primary variable.To assess how each vantage supports spatial perception, we evaluated SLAMperformance (for localization and mapping) and NeRF-based 3D reconstruction(for downstream scene understanding). Head-mounted sensors delivered superiorlocalization accuracy, while cane-mounted views offered broader ground-levelcoverage and richer environmental reconstructions. A combined (head+cane)configuration consistently outperformed both. These results highlight thecomplementary strengths of different sensor placements and offer actionableguidance for developing hybrid navigation aids that are perceptive, robust, anduser-aligned.</description>
      <author>example@mail.com (Apurv Varshney, Lucas Nadolskis, Tobias Höllerer, Michael Beyeler)</author>
      <guid isPermaLink="false">2504.19345v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Contextures: The Mechanism of Representation Learning</title>
      <link>http://arxiv.org/abs/2504.19792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD Dissertation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文建立了情境理论，以数学方式描述了表示学习或预训练的机制。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型在实证上取得了显著成功，但它们学习到的表示以及为什么这些表示对各种下游任务有用尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;对表示学习有一个科学理解，特别是在模型规模扩大导致回报递减，设计新的预训练方法对于进一步进步至关重要的时候。&lt;h4&gt;方法&lt;/h4&gt;情境理论提供了一个统一的框架来分析不同的表示学习方法，其核心论点是表示是通过输入X与情境变量A之间的关联来学习的。&lt;h4&gt;主要发现&lt;/h4&gt;如果编码器捕捉到这种关联的最大信息，即编码器学习到情境，那么它将在与情境兼容的任务类别中表现最优。此外，当X和A之间的关联既不过强也不过弱时，情境最有用。&lt;h4&gt;结论&lt;/h4&gt;情境理论的重要含义是，仅增加模型规模将导致回报递减，进一步的进步需要更好的情境。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了情境理论，用以数学化地刻画表示学习（预训练）的机制。尽管基础模型在实证上取得了显著的成功，但它们学习到的表示以及这些表示为何对各种下游任务有用尚不明确。对表示学习有一个科学理解至关重要，尤其是在模型规模扩大导致回报递减，设计新的预训练方法对于进一步进步变得至关重要的当下。先前的研究对不同的表示学习方法采取了不同的处理方式，而情境理论提供了一个统一的框架来分析这些方法。其核心论点是，表示是通过输入X与情境变量A之间的关联来学习的。我们证明了，如果编码器捕捉到这种关联的最大信息，在这种情况下我们说编码器学习到了情境，那么它将在与情境兼容的任务类别中表现最优。我们还表明，当X和A之间的关联既不过强也不过弱时，情境最有用。情境理论的重要含义是，仅增加模型规模将导致回报递减，进一步的进步需要更好的情境。我们证明了包括监督学习、自监督学习、生成模型等许多预训练目标都可以学习情境。然后，我们引入了两个用于学习情境的通用目标——SVME和KISE。我们还展示了如何混合多个情境，这是一种从现有情境中轻松创建更好情境的方法。然后，我们为表示学习证明了统计学习界限。最后，我们讨论了从预训练到下游任务的数据分布变化的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This dissertation establishes the contexture theory to mathematicallycharacterize the mechanism of representation learning, or pretraining. Despitethe remarkable empirical success of foundation models, it is not very clearwhat representations they learn, and why these representations are useful forvarious downstream tasks. A scientific understanding of representation learningis critical, especially at this point when scaling up the model size isproducing diminishing returns, and designing new pretraining methods isimperative for further progress.  Prior work treated different representation learning methods quitedifferently, whereas the contexture theory provides a unified framework foranalyzing these methods. The central argument is that a representation islearned from the association between the input X and a context variable A. Weprove that if an encoder captures the maximum information of this association,in which case we say that the encoder learns the contexture, then it will beoptimal on the class of tasks that are compatible with the context. We alsoshow that a context is the most useful when the association between X and A isneither too strong nor too weak. The important implication of the contexturetheory is that increasing the model size alone will achieve diminishingreturns, and further advancements require better contexts.  We demonstrate that many pretraining objectives can learn the contexture,including supervised learning, self-supervised learning, generative models,etc. Then, we introduce two general objectives -- SVME and KISE, for learningthe contexture. We also show how to mix multiple contexts together, aneffortless way to create better contexts from existing ones. Then, we provestatistical learning bounds for representation learning. Finally, we discussthe effect of the data distribution shift from pretraining to the downstreamtask.</description>
      <author>example@mail.com (Runtian Zhai)</author>
      <guid isPermaLink="false">2504.19792v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning</title>
      <link>http://arxiv.org/abs/2504.19627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  VCM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VCM是一种端到端自监督视觉概念建模框架，旨在提高大型视觉语言模型（LVLMs）在现实世界应用中的效率和性能。&lt;h4&gt;背景&lt;/h4&gt;当前LVLMs在处理图像时效率低下，因为它们在标记级别处理整个图像，而人类可以在概念级别分析信息和生成内容。&lt;h4&gt;目的&lt;/h4&gt;提出VCM框架，以解决LVLMs缺乏视觉概念模型导致的效率低下问题。&lt;h4&gt;方法&lt;/h4&gt;VCM利用跨多个采样实例的隐式对比学习和视觉语言微调来构建视觉概念模型，无需昂贵的概念级别标注。&lt;h4&gt;主要发现&lt;/h4&gt;VCM显著降低了计算成本（例如，LLaVA-1.5-7B的FLOPs减少了85%）同时保持了在多种图像理解任务上的强大性能。此外，VCM增强了视觉编码器在经典视觉概念感知任务中的能力。&lt;h4&gt;结论&lt;/h4&gt;广泛的定量和定性实验验证了VCM的有效性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks likeembodied intelligence due to their strong vision-language reasoning abilities.However, current LVLMs process entire images at the token level, which isinefficient compared to humans who analyze information and generate content atthe conceptual level, extracting relevant visual concepts with minimal effort.This inefficiency, stemming from the lack of a visual concept model, limitsLVLMs' usability in real-world applications. To address this, we propose VCM,an end-to-end self-supervised visual concept modeling framework. VCM leveragesimplicit contrastive learning across multiple sampled instances andvision-language fine-tuning to construct a visual concept model withoutrequiring costly concept-level annotations. Our results show that VCMsignificantly reduces computational costs (e.g., 85\% fewer FLOPs forLLaVA-1.5-7B) while maintaining strong performance across diverse imageunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities inclassic visual concept perception tasks. Extensive quantitative and qualitativeexperiments validate the effectiveness and efficiency of VCM.</description>
      <author>example@mail.com (Run Luo, Renke Shan, Longze Chen, Ziqiang Liu, Lu Wang, Min Yang, Xiaobo Xia)</author>
      <guid isPermaLink="false">2504.19627v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>OpenFusion++: An Open-vocabulary Real-time Scene Understanding System</title>
      <link>http://arxiv.org/abs/2504.19266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenFusion++是一种基于TSDF的实时3D语义-几何重建系统，旨在解决现有方法在实例分割、语义更新和复杂查询处理方面的不足。&lt;h4&gt;背景&lt;/h4&gt;实时开放词汇场景理解对于视觉语言导航、具身智能和增强现实等应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的问题，如不精确的实例分割、静态语义更新和复杂查询处理能力有限。&lt;h4&gt;方法&lt;/h4&gt;OpenFusion++通过融合基础模型中的置信图来优化3D点云，使用基于实例面积的自适应缓存动态更新全局语义标签，并采用双路径编码框架将对象属性与环境上下文集成，以实现精确的查询响应。&lt;h4&gt;主要发现&lt;/h4&gt;在ICL、Replica、ScanNet和ScanNet++数据集上的实验表明，OpenFusion++在语义准确性和查询响应速度方面均显著优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;OpenFusion++能够有效提升3D感知的效率和准确性，为视觉语言导航、具身智能和增强现实等应用提供强有力的支持。&lt;h4&gt;翻译&lt;/h4&gt;实时开放词汇场景理解对于高效的三维感知至关重要，对于如视觉语言导航、具身智能和增强现实等应用至关重要。然而，现有方法在实例分割、静态语义更新和复杂查询处理方面存在不足。为了解决这些问题，我们提出了一种基于TSDF的实时3D语义-几何重建系统，称为OpenFusion++。我们的方法通过融合基础模型的置信图来优化3D点云，通过基于实例面积的自适应缓存动态更新全局语义标签，并采用双路径编码框架将对象属性与环境上下文集成，以实现精确的查询响应。在ICL、Replica、ScanNet和ScanNet++数据集上的实验表明，OpenFusion++在语义准确性和查询响应速度方面均显著优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time open-vocabulary scene understanding is essential for efficient 3Dperception in applications such as vision-language navigation, embodiedintelligence, and augmented reality. However, existing methods suffer fromimprecise instance segmentation, static semantic updates, and limited handlingof complex queries. To address these issues, we present OpenFusion++, aTSDF-based real-time 3D semantic-geometric reconstruction system. Our approachrefines 3D point clouds by fusing confidence maps from foundational models,dynamically updates global semantic labels via an adaptive cache based oninstance area, and employs a dual-path encoding framework that integratesobject attributes with environmental context for precise query responses.Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstratethat OpenFusion++ significantly outperforms the baseline in both semanticaccuracy and query responsiveness.</description>
      <author>example@mail.com (Xiaofeng Jin, Matteo Frosi, Matteo Matteucci)</author>
      <guid isPermaLink="false">2504.19266v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Fourier Transformer with Structure-Frequency Information</title>
      <link>http://arxiv.org/abs/2504.19740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Grafourierformer，一种结合了图傅里叶变换的图变换器（GTs），通过考虑节点频率信息来优化注意力机制，以提升图结构任务的性能。&lt;h4&gt;背景&lt;/h4&gt;虽然图变换器在图结构任务中表现出优势，但其自注意力机制忽略了图的一般化偏差，现有方法主要通过位置编码、注意力偏差和相对距离等方面进行补偿，但性能仍不理想。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理图的一般化偏差的方法，并提升图分类和节点分类任务的性能。&lt;h4&gt;方法&lt;/h4&gt;Grafourierformer通过将图傅里叶变换应用于注意力矩阵，使用图拉普拉斯矩阵的特征值构建特征值矩阵掩码，并通过傅里叶逆变换提取节点的高频和低频特征，构建节点频率能量矩阵，以实现注意力头对图结构信息和节点频率信息的优化。&lt;h4&gt;主要发现&lt;/h4&gt;Grafourierformer在多个基准测试中均优于GNN和基于GT的模型，消融实验进一步验证了该方法的有效性和必要性。&lt;h4&gt;结论&lt;/h4&gt;Grafourierformer通过结合图傅里叶变换和节点频率信息，有效地抑制了冗余信息干扰，提升了图结构任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;Graph Transformers have shown advantages in numerous graph structure tasks but their self-attention mechanism ignores the generalization bias of graphs, with existing methods mainly compensating for this bias from aspects like position encoding, attention bias and relative distance yet still having sub-optimal performance and being insufficient by only considering the structural perspective of generalization bias. To address this, this paper proposes Grafourierformer, which innovatively combines GT with inductive bias containing Frequency-Structure information by applying Graph Fourier Transform to the Attention Matrix: specifically, eigenvalues from the Graph Laplacian matrix are used to construct an Eigenvalue matrix mask (reflecting node positions and structural relationships with neighboring nodes to enable consideration of node range structural characteristics and focus on local graph details), and inverse Fourier transform is employed to extract node high-frequency and low-frequency features, calculate low-frequency and high-frequency energy, and construct a node frequency-energy matrix to filter the eigenvalue matrix mask, allowing attention heads to incorporate both graph structural information and node frequency information optimization, adaptively distinguish global trends from local details, and effectively suppress redundant information interference. Extensive experiments on various benchmarks show Grafourierformer consistently outperforms GNN and GT-based models in graph classification and node classification tasks, with ablation experiments further validating the effectiveness and necessity of the method. Codes are available at https://github.com/Arichibald/Grafourierformer.git&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have shown advantages in numerous graph structuretasks but their self-attention mechanism ignores the generalization bias ofgraphs, with existing methods mainly compensating for this bias from aspectslike position encoding, attention bias and relative distance yet still havingsub-optimal performance and being insufficient by only considering thestructural perspective of generalization bias. To address this, this paperproposes Grafourierformer, which innovatively combines GT with inductive biascontaining Frequency-Structure information by applying Graph Fourier Transformto the Attention Matrix: specifically, eigenvalues from the Graph Laplacianmatrix are used to construct an Eigenvalue matrix mask (reflecting nodepositions and structural relationships with neighboring nodes to enableconsideration of node range structural characteristics and focus on local graphdetails), and inverse Fourier transform is employed to extract nodehigh-frequency and low-frequency features, calculate low-frequency andhigh-frequency energy, and construct a node frequency-energy matrix to filterthe eigenvalue matrix mask, allowing attention heads to incorporate both graphstructural information and node frequency information optimization, adaptivelydistinguish global trends from local details, and effectively suppressredundant information interference. Extensive experiments on various benchmarksshow Grafourierformer consistently outperforms GNN and GT-based models in graphclassification and node classification tasks, with ablation experiments furthervalidating the effectiveness and necessity of the method. Codes are availableat https://github.com/Arichibald/Grafourierformer.git</description>
      <author>example@mail.com (Yonghui Zhai, Yang Zhang, Minghao Shang, Lihua Pang, Yaxin Ren)</author>
      <guid isPermaLink="false">2504.19740v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model</title>
      <link>http://arxiv.org/abs/2504.19739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AffectVLM，这是一种用于从3D/4D数据中理解面部情感的多视角视觉语言模型。&lt;h4&gt;背景&lt;/h4&gt;该模型旨在通过集成多视角来提供语义丰富和视觉全面的情感理解。&lt;h4&gt;目的&lt;/h4&gt;目的是开发一个能够有效捕捉视觉特征并具有良好语言能力的模型，同时能够进行实时交互推理和分布式学习。&lt;h4&gt;方法&lt;/h4&gt;方法包括提出一个联合表示学习框架和新的梯度友好损失函数，引入增强文本提示和混合视角增强，以及开发一个Streamlit应用程序用于实时交互推理。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验验证了AffectVLM在多个基准测试中的优越性能。&lt;h4&gt;结论&lt;/h4&gt;AffectVLM在面部情感理解方面表现出色，具有广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce AffectVLM, a vision-language model designed tointegrate multiviews for a semantically rich and visually comprehensiveunderstanding of facial emotions from 3D/4D data. To effectively capture visualfeatures, we propose a joint representation learning framework paired with anovel gradient-friendly loss function that accelerates model convergencetowards optimal feature representation. Additionally, we introduce augmentedtextual prompts to enhance the model's linguistic capabilities and employ mixedview augmentation to expand the visual dataset. We also develop a Streamlit appfor a real-time interactive inference and enable the model for distributedlearning. Extensive experiments validate the superior performance of AffectVLMacross multiple benchmarks.</description>
      <author>example@mail.com (Muzammil Behzad, Guoying Zhao)</author>
      <guid isPermaLink="false">2504.19739v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis</title>
      <link>http://arxiv.org/abs/2504.19223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为CARL的模型，用于跨不同成像模态（如RGB、多光谱和超光谱成像）进行相机无关的表征学习，以解决光谱成像在人工智能应用中的挑战。&lt;h4&gt;背景&lt;/h4&gt;光谱成像在医学和城市场景理解等领域具有广泛应用前景，但在遥感领域已确立为关键模态。然而，不同光谱相机在通道维度和捕获波长上的差异阻碍了AI驱动方法的发展，导致具有有限泛化能力和不足跨相机适用性的相机特定模型。&lt;h4&gt;目的&lt;/h4&gt;提出CARL模型，旨在解决光谱成像在AI应用中的瓶颈，实现不同通道维度的光谱图像到相机无关嵌入的转换。&lt;h4&gt;方法&lt;/h4&gt;CARL模型通过波长位置编码和自注意力-交叉注意力机制，将光谱信息压缩到学习的查询表示中。采用一种新的基于JEPA的谱自监督策略进行光谱-空间预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模实验中，CARL模型在医学成像、自动驾驶和卫星成像等领域显示出对光谱异质性的独特鲁棒性，在具有模拟和真实世界跨相机光谱变化的数据库上表现优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;该模型的可扩展性和多功能性使其成为未来光谱基础模型的骨干。&lt;h4&gt;翻译&lt;/h4&gt;Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing. However, variability in channel dimensionality and captured wavelengths among spectral cameras impedes the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability. To address this bottleneck, we introduce $extbf{CARL}$, a model for $extbf{C}$amera-$extbf{A}$gnostic $extbf{R}$epresentation$extbf{L}$earning across RGB, multispectral, and hyperspectral imaging modalities. To enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic embedding, we introduce wavelength positional encoding and a self-attention-cross-attention mechanism to compress spectral information into learned query representations. Spectral-spatial pre-training is achieved with a novel spectral self-supervised JEPA-inspired strategy tailored to CARL. Large-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. The scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral imaging offers promising applications across diverse domains,including medicine and urban scene understanding, and is already established asa critical modality in remote sensing. However, variability in channeldimensionality and captured wavelengths among spectral cameras impede thedevelopment of AI-driven methodologies, leading to camera-specific models withlimited generalizability and inadequate cross-camera applicability. To addressthis bottleneck, we introduce $\textbf{CARL}$, a model for$\textbf{C}$amera-$\textbf{A}$gnostic $\textbf{R}$epresentation$\textbf{L}$earning across RGB, multispectral, and hyperspectral imagingmodalities. To enable the conversion of a spectral image with any channeldimensionality to a camera-agnostic embedding, we introduce wavelengthpositional encoding and a self-attention-cross-attention mechanism to compressspectral information into learned query representations. Spectral-spatialpre-training is achieved with a novel spectral self-supervised JEPA-inspiredstrategy tailored to CARL. Large-scale experiments across the domains ofmedical imaging, autonomous driving, and satellite imaging demonstrate ourmodel's unique robustness to spectral heterogeneity, outperforming on datasetswith simulated and real-world cross-camera spectral variations. The scalabilityand versatility of the proposed approach position our model as a backbone forfuture spectral foundation models.</description>
      <author>example@mail.com (Alexander Baumann, Leonardo Ayala, Silvia Seidlitz, Jan Sellner, Alexander Studier-Fischer, Berkin Özdemir, Lena Maier-Hein, Slobodan Ilic)</author>
      <guid isPermaLink="false">2504.19223v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning</title>
      <link>http://arxiv.org/abs/2504.19030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究旨在提高语音命令识别系统的准确性和效率，通过使用预训练的YAMNet模型和迁移学习技术，显著提升了语音命令识别效果。&lt;h4&gt;背景&lt;/h4&gt;语音命令识别是智能应用中用户交互的关键组成部分，目前存在准确性和效率的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，以增强语音命令识别系统的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的YAMNet模型和迁移学习技术，对语音命令进行检测和解释，并在Speech Commands数据集上进行了细致的数据增强和特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法，模型达到了95.28%的识别准确率，证明了高级机器学习技术在语音命令识别中的影响。&lt;h4&gt;结论&lt;/h4&gt;这一成果在音频处理技术方面取得了实质性进展，并为该领域未来的研究设定了新的基准。&lt;h4&gt;翻译&lt;/h4&gt;This work addresses the need for enhanced accuracy and efficiency in speech command recognition systems, a critical component for improving user interaction in various smart applications. Leveraging the robust pretrained YAMNet model and transfer learning, this study develops a method that significantly improves speech command recognition. We adapt and train a YAMNet deep learning model to effectively detect and interpret speech commands from audio signals. Using the extensively annotated Speech Commands dataset (speech_commands_v0.01), our approach demonstrates the practical application of transfer learning to accurately recognize a predefined set of speech commands. The dataset is meticulously augmented, and features are strategically extracted to boost model performance. As a result, the final model achieved a recognition accuracy of 95.28%, underscoring the impact of advanced machine learning techniques on speech command recognition. This achievement marks substantial progress in audio processing technologies and establishes a new benchmark for future research in the field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICTIS62692.2024.10894266&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work addresses the need for enhanced accuracy and efficiency in speechcommand recognition systems, a critical component for improving userinteraction in various smart applications. Leveraging the robust pretrainedYAMNet model and transfer learning, this study develops a method thatsignificantly improves speech command recognition. We adapt and train a YAMNetdeep learning model to effectively detect and interpret speech commands fromaudio signals. Using the extensively annotated Speech Commands dataset(speech_commands_v0.01), our approach demonstrates the practical application oftransfer learning to accurately recognize a predefined set of speech commands.The dataset is meticulously augmented, and features are strategically extractedto boost model performance. As a result, the final model achieved a recognitionaccuracy of 95.28%, underscoring the impact of advanced machine learningtechniques on speech command recognition. This achievement marks substantialprogress in audio processing technologies and establishes a new benchmark forfuture research in the field.</description>
      <author>example@mail.com (Sidahmed Lachenani, Hamza Kheddar, Mohamed Ouldzmirli)</author>
      <guid isPermaLink="false">2504.19030v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID</title>
      <link>http://arxiv.org/abs/2504.19244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SALCR的无监督可见光-红外行人重识别框架，该框架通过语义对齐学习和协作优化来提高跨模态图像的重识别性能。&lt;h4&gt;背景&lt;/h4&gt;当前方法在统一跨模态图像的伪标签和设计对比学习框架时，忽略了特征表示和伪标签分布的跨模态变化。&lt;h4&gt;目的&lt;/h4&gt;旨在解决现有方法中由于只优化全局特征而导致的模态共享学习不足的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 提出DAGI模块双向统一跨模态实例的伪标签；2. 使用FGSAL模块探索跨模态实例中每种模态强调的语义对齐模式；3. 基于语义对齐特征及其对应的标签空间构建优化目标；4. 提出GPCR模块动态挖掘可靠的正样本集，优化实例间关系。&lt;h4&gt;主要发现&lt;/h4&gt;SALCR框架通过强调特定细粒度模式，实现了不同模态标签分布的互补对齐。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，该方法在性能上优于现有方法，并且代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;The proposed SALCR framework for unsupervised visible-infrared person re-identification addresses the insufficient modality-shared learning issue in existing methods by employing semantic-aligned learning and collaborative refinement. It achieves complementary alignment between the label distributions of different modalities by emphasizing specific fine-grained patterns. Extensive experiments demonstrate its superiority over state-of-the-art methods, and the code is publicly available.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks tomatch pedestrian images of the same individual across different modalitieswithout human annotations for model learning. Previous methods unifypseudo-labels of cross-modality images through label association algorithms andthen design contrastive learning framework for global feature learning.However, these methods overlook the cross-modality variations in featurerepresentation and pseudo-label distributions brought by fine-grained patterns.This insight results in insufficient modality-shared learning when only globalfeatures are optimized. To address this issue, we propose a Semantic-AlignedLearning with Collaborative Refinement (SALCR) framework, which builds upoptimization objective for specific fine-grained patterns emphasized by eachmodality, thereby achieving complementary alignment between the labeldistributions of different modalities. Specifically, we first introduce a DualAssociation with Global Learning (DAGI) module to unify the pseudo-labels ofcross-modality instances in a bi-directional manner. Afterward, a Fine-GrainedSemantic-Aligned Learning (FGSAL) module is carried out to explore part-levelsemantic-aligned patterns emphasized by each modality from cross-modalityinstances. Optimization objective is then formulated based on thesemantic-aligned features and their corresponding label space. To alleviate theside-effects arising from noisy pseudo-labels, we propose a Global-PartCollaborative Refinement (GPCR) module to mine reliable positive sample setsfor the global and part features dynamically and optimize the inter-instancerelationships. Extensive experiments demonstrate the effectiveness of theproposed method, which achieves superior performances to state-of-the-artmethods. Our code is available at\href{https://github.com/FranklinLingfeng/code-for-SALCR}.</description>
      <author>example@mail.com (De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao)</author>
      <guid isPermaLink="false">2504.19244v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration</title>
      <link>http://arxiv.org/abs/2504.19847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Seg2HOI的新框架，该框架将基于分割的视觉基础模型与人类-物体交互任务相结合，区别于传统的基于检测的人类-物体交互方法。&lt;h4&gt;背景&lt;/h4&gt;在人类-物体交互（HOI）任务中，传统方法依赖于检测，而本文提出的方法采用分割技术。&lt;h4&gt;目的&lt;/h4&gt;提高HOI检测的准确性，并通过引入四元组扩展三元组，包括人类-物体对的分割掩码。&lt;h4&gt;方法&lt;/h4&gt;Seg2HOI继承了视觉基础模型（如可提示和交互机制）的特性，并包含一个解码器，将这些属性应用于HOI任务。尽管仅针对HOI进行训练，但没有对这些属性进行额外的训练机制，该框架仍然表现出高效的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共基准数据集上的大量实验表明，Seg2HOI即使在零样本场景下也能达到与最先进方法相当的性能。&lt;h4&gt;结论&lt;/h4&gt;Seg2HOI可以从未在训练中使用的新型文本和视觉提示中生成HOI四元组和交互式HOI分割，这使得它能够通过这种灵活性在广泛的应用中发挥作用。&lt;h4&gt;翻译&lt;/h4&gt;In this work, we introduce Segmentation to Human-Object Interaction (Seg2HOI) approach, a novel framework that integrates segmentation-based vision foundation models with the human-object interaction task, distinguished from traditional detection-based Human-Object Interaction (HOI) methods. Our approach enhances HOI detection by not only predicting the standard triplets but also introducing quadruplets, which extend HOI triplets by including segmentation masks for human-object pairs. More specifically, Seg2HOI inherits the properties of the vision foundation model (e.g., promptable and interactive mechanisms) and incorporates a decoder that applies these attributes to the HOI task. Despite training only for HOI, without additional training mechanisms for these properties, the framework demonstrates that such features still operate efficiently. Extensive experiments on two public benchmark datasets demonstrate that Seg2HOI achieves performance comparable to state-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that Seg2HOI can generate HOI quadruplets and interactive HOI segmentation from novel text and visual prompts that were not used during training, making it versatile for a wide range of applications by leveraging this flexibility.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce Segmentation to Human-Object Interaction(\textit{\textbf{Seg2HOI}}) approach, a novel framework that integratessegmentation-based vision foundation models with the human-object interactiontask, distinguished from traditional detection-based Human-Object Interaction(HOI) methods. Our approach enhances HOI detection by not only predicting thestandard triplets but also introducing quadruplets, which extend HOI tripletsby including segmentation masks for human-object pairs. More specifically,Seg2HOI inherits the properties of the vision foundation model (e.g.,promptable and interactive mechanisms) and incorporates a decoder that appliesthese attributes to HOI task. Despite training only for HOI, without additionaltraining mechanisms for these properties, the framework demonstrates that suchfeatures still operate efficiently. Extensive experiments on two publicbenchmark datasets demonstrate that Seg2HOI achieves performance comparable tostate-of-the-art methods, even in zero-shot scenarios. Lastly, we propose thatSeg2HOI can generate HOI quadruplets and interactive HOI segmentation fromnovel text and visual prompts that were not used during training, making itversatile for a wide range of applications by leveraging this flexibility.</description>
      <author>example@mail.com (Juhan Park, Kyungjae Lee, Hyung Jin Chang, Jungchan Cho)</author>
      <guid isPermaLink="false">2504.19847v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Stress in Two-phase Random Materials and Super-Resolution Method for Stress Images by Embedding Physical Information</title>
      <link>http://arxiv.org/abs/2504.18854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了两相随机材料（TRM）的应力分析，提出了一种基于深度学习的应力预测框架。&lt;h4&gt;背景&lt;/h4&gt;应力分析是材料设计的重要部分，对于具有复杂微结构的材料，如两相随机材料，应力集中是材料失效的常见原因，而相界面是应力集中的关键。&lt;h4&gt;目的&lt;/h4&gt;减少相界面的应力预测误差，提高应力图像的分辨率，并实现多尺度分析。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为MC U-net的神经网络来预测低分辨率材料微结构中的应力，并引入了相界面信息以减少预测误差；同时，提出了一种基于物理信息混合的神经网络（MPINN）的方法，用于应力图像超分辨率，无需成对训练数据，并能提高应力图像的分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效减少相界面的预测误差，提高应力图像的分辨率，并实现多尺度分析。&lt;h4&gt;结论&lt;/h4&gt;所提出的应力预测框架具有较高的准确性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Stress analysis is an important part of material design. For materials with complex microstructures, such as two-phase random materials (TRMs), material failure is often accompanied by stress concentration. Phase interfaces in two-phase materials are critical for stress concentration. Therefore, the prediction error of stress at phase boundaries is crucial. In practical engineering, the pixels of the obtained material microstructure images are limited, which limits the resolution of stress images generated by deep learning methods, making it difficult to observe stress concentration regions. Existing Image Super-Resolution (ISR) technologies are all based on data-driven supervised learning. However, stress images have natural physical constraints, which provide new ideas for new ISR technologies. In this study, we constructed a stress prediction framework for TRMs. First, the framework uses a proposed Multiple Compositions U-net (MC U-net) to predict stress in low-resolution material microstructures. By considering the phase interface information of the microstructure, the MC U-net effectively reduces the problem of excessive prediction errors at phase boundaries. Secondly, a Mixed Physics-Informed Neural Network (MPINN) based method for stress ISR (SRPINN) was proposed. By introducing the constraints of physical information, the new method does not require paired stress images for training and can increase the resolution of stress images to any multiple. This enables a multiscale analysis of the stress concentration regions at phase boundaries. Finally, we performed stress analysis on TRMs with different phase volume fractions and loading states through transfer learning. The results show the proposed stress prediction framework has satisfactory accuracy and generalization ability.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.2139/ssrn.5096177&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stress analysis is an important part of material design. For materials withcomplex microstructures, such as two-phase random materials (TRMs), materialfailure is often accompanied by stress concentration. Phase interfaces intwo-phase materials are critical for stress concentration. Therefore, theprediction error of stress at phase boundaries is crucial. In practicalengineering, the pixels of the obtained material microstructure images arelimited, which limits the resolution of stress images generated by deeplearning methods, making it difficult to observe stress concentration regions.Existing Image Super-Resolution (ISR) technologies are all based on data-drivensupervised learning. However, stress images have natural physical constraints,which provide new ideas for new ISR technologies. In this study, we constructeda stress prediction framework for TRMs. First, the framework uses a proposedMultiple Compositions U-net (MC U-net) to predict stress in low-resolutionmaterial microstructures. By considering the phase interface information of themicrostructure, the MC U-net effectively reduces the problem of excessiveprediction errors at phase boundaries. Secondly, a Mixed Physics-InformedNeural Network (MPINN) based method for stress ISR (SRPINN) was proposed. Byintroducing the constraints of physical information, the new method does notrequire paired stress images for training and can increase the resolution ofstress images to any multiple. This enables a multiscale analysis of the stressconcentration regions at phase boundaries. Finally, we performed stressanalysis on TRMs with different phase volume fractions and loading statesthrough transfer learning. The results show the proposed stress predictionframework has satisfactory accuracy and generalization ability.</description>
      <author>example@mail.com (Tengfei Xing, Xiaodan Ren, Jie Li)</author>
      <guid isPermaLink="false">2504.18854v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Relative Contrastive Learning for Sequential Recommendation with Similarity-based Positive Pair Selection</title>
      <link>http://arxiv.org/abs/2504.19178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The code can be found at https://github.com/Cloudcatcher888/RCL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为RCL的相对对比学习框架，用于序列推荐模型的训练，以改善对比学习的效果。&lt;h4&gt;背景&lt;/h4&gt;现有的对比学习方法通常依赖于数据增强策略来创建正样本和促进表示不变性，但可能会无意中改变用户意图。&lt;h4&gt;目的&lt;/h4&gt;解决监督对比学习（SCL）方法由于同目标序列稀缺而缺乏足够信号的问题。&lt;h4&gt;方法&lt;/h4&gt;提出使用相似序列（具有不同目标项目）作为额外的正样本，并引入RCL框架。RCL包括双级正样本选择模块和相对对比学习模块，分别用于选择强正样本和弱正样本，以及确保每个序列更接近其强正样本。&lt;h4&gt;主要发现&lt;/h4&gt;将RCL应用于两个主流的深度学习序列推荐模型，实验结果表明，在五个公共数据集和一个私有数据集上，RCL的平均性能比最先进的序列推荐方法提高了4.88%。&lt;h4&gt;结论&lt;/h4&gt;RCL能够有效提高序列推荐模型的性能，是一个有潜力的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3627673.3679681&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Learning (CL) enhances the training of sequential recommendation(SR) models through informative self-supervision signals. Existing methodsoften rely on data augmentation strategies to create positive samples andpromote representation invariance. Some strategies such as item reordering anditem substitution may inadvertently alter user intent. Supervised ContrastiveLearning (SCL) based methods find an alternative to augmentation-based CLmethods by selecting same-target sequences (interaction sequences with the sametarget item) to form positive samples. However, SCL-based methods suffer fromthe scarcity of same-target sequences and consequently lack enough signals forcontrastive learning. In this work, we propose to use similar sequences (withdifferent target items) as additional positive samples and introduce a RelativeContrastive Learning (RCL) framework for sequential recommendation. RCLcomprises a dual-tiered positive sample selection module and a relativecontrastive learning module. The former module selects same-target sequences asstrong positive samples and selects similar sequences as weak positive samples.The latter module employs a weighted relative contrastive loss, ensuring thateach sequence is represented closer to its strong positive samples than itsweak positive samples. We apply RCL on two mainstream deep learning-based SRmodels, and our empirical results reveal that RCL can achieve 4.88% improvementaveragely than the state-of-the-art SR methods on five public datasets and oneprivate dataset.</description>
      <author>example@mail.com (Zhikai Wang, Yanyan Shen, Zexi Zhang, Li He, Yichun Li, Hao Gu, Yinghua Zhang)</author>
      <guid isPermaLink="false">2504.19178v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>FiberKAN: Kolmogorov-Arnold Networks for Nonlinear Fiber Optics</title>
      <link>http://arxiv.org/abs/2504.18833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Kolmogorov-Arnold网络（KAN）的人工智能科学（AI4S）框架FiberKAN，用于非线性光纤的科学研究与动态特性描述。&lt;h4&gt;背景&lt;/h4&gt;尽管许多系统动力学已经从严格的原理推导出理论，但仍有大量复杂的动力学尚未被发现和描述，这阻碍了相关领域科学进步。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了一种新的AI4S框架，用于科学发现和动态特性描述。&lt;h4&gt;方法&lt;/h4&gt;提出了FiberKAN框架，该框架使用KAN而非传统的多层感知器（MLP）结构，并具有可训练和透明的激活函数，增强了网络的物理可解释性和非线性特征描述能力。&lt;h4&gt;主要发现&lt;/h4&gt;KAN可以有效地发现和描述不同影响下的显式、隐式和非解析解，并且在与MLP具有等效可训练参数规模的情况下，性能更优。&lt;h4&gt;结论&lt;/h4&gt;这项工作突出了KAN的变革潜力，将其确立为AI4S的先驱范式，推动了非线性光纤光学领域的进步，并在广泛的科学和工程学科中促进了创新。&lt;h4&gt;翻译&lt;/h4&gt;Scientific discovery and dynamic characterization of the physical system play a critical role in understanding, learning, and modeling the physical phenomena and behaviors in various fields. Although theories and laws of many system dynamics have been derived from rigorous first principles, there are still a considerable number of complex dynamics that have not yet been discovered and characterized, which hinders the progress of science in corresponding fields. To address these challenges, artificial intelligence for science (AI4S) has emerged as a burgeoning research field. In this paper, a Kolmogorov-Arnold Network (KAN)-based AI4S framework named FiberKAN is proposed for scientific discovery and dynamic characterization of nonlinear fiber optics. Unlike the classic multi-layer perceptron (MLP) structure, the trainable and transparent activation functions in KAN make the network have stronger physical interpretability and nonlinear characterization abilities. Multiple KANs are established for fiber-optic system dynamics under various physical effects. Results show that KANs can well discover and characterize the explicit, implicit, and non-analytical solutions under different effects, and achieve better performance than MLPs with the equivalent scale of trainable parameters. Moreover, the effectiveness, computational cost, interactivity, noise resistance, transfer learning ability, and comparison between related algorithms in fiber-optic systems are also studied and analyzed. This work highlights the transformative potential of KAN, establishing it as a pioneering paradigm in AI4S that propels advancements in nonlinear fiber optics, and fosters groundbreaking innovations across a broad spectrum of scientific and engineering disciplines.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific discovery and dynamic characterization of the physical system playa critical role in understanding, learning, and modeling the physical phenomenaand behaviors in various fields. Although theories and laws of many systemdynamics have been derived from rigorous first principles, there are still aconsiderable number of complex dynamics that have not yet been discovered andcharacterized, which hinders the progress of science in corresponding fields.To address these challenges, artificial intelligence for science (AI4S) hasemerged as a burgeoning research field. In this paper, a Kolmogorov-ArnoldNetwork (KAN)-based AI4S framework named FiberKAN is proposed for scientificdiscovery and dynamic characterization of nonlinear fiber optics. Unlike theclassic multi-layer perceptron (MLP) structure, the trainable and transparentactivation functions in KAN make the network have stronger physicalinterpretability and nonlinear characterization abilities. Multiple KANs areestablished for fiber-optic system dynamics under various physical effects.Results show that KANs can well discover and characterize the explicit,implicit, and non-analytical solutions under different effects, and achievebetter performance than MLPs with the equivalent scale of trainable parameters.Moreover, the effectiveness, computational cost, interactivity, noiseresistance, transfer learning ability, and comparison between relatedalgorithms in fiber-optic systems are also studied and analyzed. This workhighlights the transformative potential of KAN, establishing it as a pioneeringparadigm in AI4S that propels advancements in nonlinear fiber optics, andfosters groundbreaking innovations across a broad spectrum of scientific andengineering disciplines.</description>
      <author>example@mail.com (Xiaotian Jiang, Min Zhang, Xiao Luo, Zelai Yu, Yiming Meng, Danshi Wang)</author>
      <guid isPermaLink="false">2504.18833v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Informed Neural Operator Transformer</title>
      <link>http://arxiv.org/abs/2504.19452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为GINOT的机器学习代理模型，该模型结合了transformer架构和神经算子框架，能够对任意几何形状进行正向预测，具有计算效率高、模拟速度快的特点。&lt;h4&gt;背景&lt;/h4&gt;传统的数值方法在需要重复求解偏微分方程的问题上计算效率较低，而基于机器学习的代理模型则提供了显著的计算效率提升。&lt;h4&gt;目的&lt;/h4&gt;提出GINOT模型，以实现对任意几何形状的高精度正向预测。&lt;h4&gt;方法&lt;/h4&gt;GINOT通过采样和分组机制以及注意力机制对几何形状的点云进行编码，确保对点顺序和填充的不变性，同时保持对点密度变化的鲁棒性。几何信息通过注意力机制与解的查询点无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;GINOT在多个具有挑战性的数据集上进行了验证，展示了其在复杂和任意2D和3D几何形状上的高精度和强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;GINOT模型在计算效率和预测精度方面具有显著优势，适用于需要重复计算偏微分方程的问题。&lt;h4&gt;翻译&lt;/h4&gt;Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions for arbitrary geometries. GINOT encodes the surface points cloud of a geometry using a sampling and grouping mechanism combined with an attention mechanism, ensuring invariance to point order and padding while maintaining robustness to variations in point density. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learning-based surrogate models offer significant computationalefficiency and faster simulations compared to traditional numerical methods,especially for problems requiring repeated evaluations of partial differentialequations. This work introduces the Geometry-Informed Neural OperatorTransformer (GINOT), which integrates the transformer architecture with theneural operator framework to enable forward predictions for arbitrarygeometries. GINOT encodes the surface points cloud of a geometry using asampling and grouping mechanism combined with an attention mechanism, ensuringinvariance to point order and padding while maintaining robustness tovariations in point density. The geometry information is seamlessly integratedwith query points in the solution decoder through the attention mechanism. Theperformance of GINOT is validated on multiple challenging datasets, showcasingits high accuracy and strong generalization capabilities for complex andarbitrary 2D and 3D geometries.</description>
      <author>example@mail.com (Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric, Philippe Geubelle)</author>
      <guid isPermaLink="false">2504.19452v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Efficiency Meets Symmetry Breaking</title>
      <link>http://arxiv.org/abs/2504.19738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于图神经网络的学习型规划器，该规划器能够学习适用于大搜索空间的搜索指导，但其在处理对称性方面的潜力尚未得到充分探索。&lt;h4&gt;背景&lt;/h4&gt;现有的学习型规划器虽然能够学习适用于大搜索空间的搜索指导，但尚未探索如何处理搜索空间中的对称性。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够检测对称性并提高学习效率的图表示方法，以及两种剪枝方法（动作剪枝和状态剪枝）来管理搜索过程中的对称性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种图表示规划问题的方法，并整合了动作剪枝和状态剪枝技术，以提高规划器的学习效率和对称性处理能力。&lt;h4&gt;主要发现&lt;/h4&gt;将提出的方法整合到Fast Downward中，在最新的IPC学习轨迹数据集上首次成功超越LAMA。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在处理规划问题的对称性方面取得了显著成效。&lt;h4&gt;翻译&lt;/h4&gt;This paper introduces a learning-based planner leveraging Graph Neural Networks, which can learn search guidance applicable to large search spaces, yet the potential to address symmetries remains largely unexplored. In this paper, we introduce a graph representation of planning problems, which combines learning efficiency with the ability to detect symmetries, along with two pruning methods, action pruning and state pruning, designed to manage symmetries during search. The integration of these techniques into Fast Downward achieves a first-time success over LAMA on the latest IPC learning track dataset. Code is released at: https://github.com/bybeye/Distincter.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning-based planners leveraging Graph Neural Networks can learn searchguidance applicable to large search spaces, yet their potential to addresssymmetries remains largely unexplored. In this paper, we introduce a graphrepresentation of planning problems allying learning efficiency with theability to detect symmetries, along with two pruning methods, action pruningand state pruning, designed to manage symmetries during search. The integrationof these techniques into Fast Downward achieves a first-time success over LAMAon the latest IPC learning track dataset. Code is released at:https://github.com/bybeye/Distincter.</description>
      <author>example@mail.com (Yingbin Bai, Sylvie Thiebaux, Felipe Trevizan)</author>
      <guid isPermaLink="false">2504.19738v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser Understanding</title>
      <link>http://arxiv.org/abs/2504.18785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ALF的多模态Transformer架构，用于理解广告商在文本、图像、视频和结构化数据模态中的行为和意图。&lt;h4&gt;背景&lt;/h4&gt;研究者们在分析广告商行为和意图方面面临挑战，需要一种能够处理多种数据模态的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效理解广告商行为和意图的模型，以应用于欺诈检测、违规政策识别和广告商相似度匹配等任务。&lt;h4&gt;方法&lt;/h4&gt;ALF通过对比学习和多任务优化，创建统一的广告商表示，捕捉内容和行为模式。其架构结合了多模态变换、样本间注意力机制、频谱归一化投影和校准概率输出。&lt;h4&gt;主要发现&lt;/h4&gt;ALF在关键任务上实现了最先进的性能，如欺诈检测、违规政策识别和广告商相似度匹配。在生产部署中，ALF将滥用检测任务中的误报率降低了90%，同时保持了99.8%的精确度。&lt;h4&gt;结论&lt;/h4&gt;ALF的有效性源于其新颖的多模态变换组合、样本间注意力机制、频谱归一化投影和校准概率输出的结合。&lt;h4&gt;翻译&lt;/h4&gt;We present ALF (Advertiser Large Foundation model), a multi-modal transformer architecture for understanding advertiser behavior and intent across text, image, video and structured data modalities. Through contrastive learning and multi-task optimization, ALF creates unified advertiser representations that capture both content and behavioral patterns. Our model achieves state-of-the-art performance on critical tasks including fraud detection, policy violation identification, and advertiser similarity matching. In production deployment, ALF reduces false positives by 90% while maintaining 99.8% precision on abuse detection tasks. The architecture's effectiveness stems from its novel combination of multi-modal transformations, inter-sample attention mechanism, spectrally normalized projections, and calibrated probabilistic outputs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ALF (Advertiser Large Foundation model), a multi-modal transformerarchitecture for understanding advertiser behavior and intent across text,image, video and structured data modalities. Through contrastive learning andmulti-task optimization, ALF creates unified advertiser representations thatcapture both content and behavioral patterns. Our model achievesstate-of-the-art performance on critical tasks including fraud detection,policy violation identification, and advertiser similarity matching. Inproduction deployment, ALF reduces false positives by 90% while maintaining99.8% precision on abuse detection tasks. The architecture's effectivenessstems from its novel combination of multi-modal transformations, inter-sampleattention mechanism, spectrally normalized projections, and calibratedprobabilistic outputs.</description>
      <author>example@mail.com (Santosh Rajagopalan, Jonathan Vronsky, Songbai Yan, S. Alireza Golestaneh, Shubhra Chandra, Min Zhou)</author>
      <guid isPermaLink="false">2504.18785v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>A Real-Time Event-Based Normal Flow Estimator</title>
      <link>http://arxiv.org/abs/2504.19417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种实时、异步的事件驱动正常流估计器，通过优化实现提升了性能。&lt;h4&gt;背景&lt;/h4&gt;传统方法将事件切片视为3D点云，编码局部几何为固定长度向量，使用多层感知器预测正常流，但计算复杂度较高。&lt;h4&gt;目的&lt;/h4&gt;开发一种更高效的事件相机正常流预测方法。&lt;h4&gt;方法&lt;/h4&gt;该方法利用事件坐标为整数的特性，将表示步骤重构为池化操作，降低计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;该方法支持实时正常流预测，使用1 GB的CUDA内存，在RTX 3070上每秒处理400万个正常流，在RTX A5000上每秒处理600万个。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种高效的事件相机正常流预测方法，并发布了CUDA实现和Python接口。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种实时、异步的事件驱动正常流估计器。它遵循与《直接从事件邻域学习正常流》相同的算法，但具有更优化的实现。原始方法将事件切片视为3D点云，将每个事件的局部几何编码为固定长度的向量，并使用多层感知器来预测正常流。它通过将邻接矩阵与特征矩阵相乘来构建表示，从而在事件数量上具有二次时间复杂度。相比之下，我们利用事件坐标是整数的事实，将表示步骤重构成池化操作。这实现了与邻接矩阵相同的效果，但具有更低的计算成本。因此，我们的方法支持事件相机的实时正常流预测。我们的估计器使用1 GB的CUDA内存，在RTX 3070上每秒处理4百万个正常流，在RTX A5000上每秒处理600万个。我们在https://github.com/dhyuan99/VecKM_flow_cpp上发布了CUDA实现和Python接口。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a real-time, asynchronous, event-based normal flowestimator. It follows the same algorithm as Learning Normal Flow Directly FromEvent Neighborhoods, but with a more optimized implementation. The originalmethod treats event slices as 3D point clouds, encodes each event's localgeometry into a fixed-length vector, and uses a multi-layer perceptron topredict normal flow. It constructs representations by multiplying an adjacencymatrix with a feature matrix, resulting in quadratic time complexity withrespect to the number of events. In contrast, we leverage the fact that eventcoordinates are integers and reformulate the representation step as a poolingoperation. This achieves the same effect as the adjacency matrix but with muchlower computational cost. As a result, our method supports real-time normalflow prediction on event cameras. Our estimator uses 1 GB of CUDA memory andruns at 4 million normal flows per second on an RTX 3070, or 6 million persecond on an RTX A5000. We release the CUDA implementation along with a Pythoninterface at https://github.com/dhyuan99/VecKM_flow_cpp.</description>
      <author>example@mail.com (Dehao Yuan, Cornelia Fermüller)</author>
      <guid isPermaLink="false">2504.19417v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Segmenting Objectiveness and Task-awareness Unknown Region for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2504.19183v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SOTA的新框架，用于自动驾驶场景中的道路场景分割，该框架通过语义融合块（SFB）增强客观性分割，并通过场景理解引导的提示上下文适配器（SG-PCA）过滤与道路导航任务无关的异常。&lt;h4&gt;背景&lt;/h4&gt;随着基于transformer架构和大型语言模型（LLMs）的出现，道路场景感知的准确性得到了显著提高。然而，现有的道路场景分割方法主要在闭集数据上训练，导致对分布外（OOD）对象的检测能力不足。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一局限性，本文提出了SOTA框架，旨在提高自动驾驶场景中道路场景分割的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;SOTA通过语义融合块（SFB）增强分割的客观性，并通过场景理解引导的提示上下文适配器（SG-PCA）过滤与道路导航任务无关的异常。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的广泛实证评估表明，SOTA在多个检测器上持续提高OOD检测性能，实现了鲁棒和准确的分割结果。&lt;h4&gt;结论&lt;/h4&gt;SOTA框架在自动驾驶场景中的道路场景分割方面具有显著优势，能够有效提高异常检测的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;With the emergence of transformer-based architectures and large language models (LLMs), the accuracy of road scene perception has substantially advanced. Nonetheless, current road scene segmentation approaches are predominantly trained on closed-set data, resulting in insufficient detection capabilities for out-of-distribution (OOD) objects. To overcome this limitation, road anomaly detection methods have been proposed. However, existing methods primarily depend on image inpainting and OOD distribution detection techniques, facing two critical issues: (1) inadequate consideration of the objectiveness attributes of anomalous regions, causing incomplete segmentation when anomalous objects share similarities with known classes, and (2) insufficient attention to environmental constraints, leading to the detection of anomalies irrelevant to autonomous driving tasks. In this paper, we propose a novel framework termed Segmenting Objectiveness and Task-Awareness (SOTA) for autonomous driving scenes. Specifically, SOTA enhances the segmentation of objectiveness through a Semantic Fusion Block (SFB) and filters anomalies irrelevant to road navigation tasks using a Scene-understanding Guided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations on multiple benchmark datasets, including Fishyscapes Lost and Found, Segment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTA consistently improves OOD detection performance across diverse detectors, achieving robust and accurate segmentation outcomes.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emergence of transformer-based architectures and large languagemodels (LLMs), the accuracy of road scene perception has substantiallyadvanced. Nonetheless, current road scene segmentation approaches arepredominantly trained on closed-set data, resulting in insufficient detectioncapabilities for out-of-distribution (OOD) objects. To overcome thislimitation, road anomaly detection methods have been proposed. However,existing methods primarily depend on image inpainting and OOD distributiondetection techniques, facing two critical issues: (1) inadequate considerationof the objectiveness attributes of anomalous regions, causing incompletesegmentation when anomalous objects share similarities with known classes, and(2) insufficient attention to environmental constraints, leading to thedetection of anomalies irrelevant to autonomous driving tasks. In this paper,we propose a novel framework termed Segmenting Objectiveness and Task-Awareness(SOTA) for autonomous driving scenes. Specifically, SOTA enhances thesegmentation of objectiveness through a Semantic Fusion Block (SFB) and filtersanomalies irrelevant to road navigation tasks using a Scene-understandingGuided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations onmultiple benchmark datasets, including Fishyscapes Lost and Found,Segment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTAconsistently improves OOD detection performance across diverse detectors,achieving robust and accurate segmentation outcomes.</description>
      <author>example@mail.com (Mi Zheng, Guanglei Yang, Zitong Huang, Zhenhua Guo, Kevin Han, Wangmeng Zuo)</author>
      <guid isPermaLink="false">2504.19183v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification</title>
      <link>http://arxiv.org/abs/2504.19682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures, accepted for presentation at  xAI-World-Conference 2025, code is available at  https://github.com/nickhaidos/Vision-GNNs-Explainer&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络（GNNs）在视觉任务中的应用，分析了GNN图像分类器在不同层形成的图的语义一致性，并比较了标准与对抗环境下的解释性，通过可视化技术揭示了模型的解释性。&lt;h4&gt;背景&lt;/h4&gt;GNNs作为卷积方法在图像分类等视觉任务中的替代方案，利用图像块表示，通过块相似性或分类相关性建立边。&lt;h4&gt;目的&lt;/h4&gt;分析GNN图像分类器在不同层形成的图的语义一致性，评估其保留物体结构和有意义关系的能力。&lt;h4&gt;方法&lt;/h4&gt;通过量化层间图连接反映语义相似性和空间一致性的程度，比较标准与对抗环境下的解释性，使用基于热图的可视化技术展示信息流动。&lt;h4&gt;主要发现&lt;/h4&gt;模型的决策过程可以有效地解释，但其推理过程不一定与人类感知一致，尤其是在深层。&lt;h4&gt;结论&lt;/h4&gt;GNNs在视觉任务中的应用具有较高的解释性，但其推理过程与人类感知存在差异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as an efficient alternative toconvolutional approaches for vision tasks such as image classification,leveraging patch-based representations instead of raw pixels. These methodsconstruct graphs where image patches serve as nodes, and edges are establishedbased on patch similarity or classification relevance. Despite theirefficiency, the explainability of GNN-based vision models remainsunderexplored, even though graphs are naturally interpretable. In this work, weanalyze the semantic consistency of the graphs formed at different layers ofGNN-based image classifiers, focusing on how well they preserve objectstructures and meaningful relationships. A comprehensive analysis is presentedby quantifying the extent to which inter-layer graph connections reflectsemantic similarity and spatial coherence. Explanations from standard andadversarial settings are also compared to assess whether they reflect theclassifiers' robustness. Additionally, we visualize the flow of informationacross layers through heatmap-based visualization techniques, therebyhighlighting the models' explainability. Our findings demonstrate that thedecision-making processes of these models can be effectively explained, whilealso revealing that their reasoning does not necessarily align with humanperception, especially in deeper layers.</description>
      <author>example@mail.com (Nikolaos Chaidos, Angeliki Dimitriou, Nikolaos Spanos, Athanasios Voulodimos, Giorgos Stamou)</author>
      <guid isPermaLink="false">2504.19682v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning</title>
      <link>http://arxiv.org/abs/2504.18666v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Journal of the Brazilian Computer Society (JBCS)  [https://journals-sol.sbc.org.br]&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为active-DeepFA的方法，用于在标注数据稀缺、未标注数据丰富的情况下训练深度学习模型。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型的训练面临的一个主要挑战是准确标注数据的有限可用性，特别是在数据标注耗时且易出错的任务中。&lt;h4&gt;目的&lt;/h4&gt;提出active-DeepFA方法，以有效地结合聚类（CL）、基于教师-学生的元伪标签（meta-pseudo-labeling）和主动学习（AL）来训练非预训练的CNN架构，用于图像分类。&lt;h4&gt;方法&lt;/h4&gt;该方法将DeepFA集成到协同训练设置中，实现两个合作网络以减轻伪标签的确认偏差。它通过监督CL预热网络，然后定期进行标签传播和伪标签交换，同时将最有意义的样本标注并添加到标注集中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在三个具有挑战性的生物图像数据集上进行了评估，仅使用5%的标注样本，提高了基线并优于六种其他SoTA方法。此外，它通过仅使用3%的标注数据就达到了与对手相当的结果，从而减少了标注工作量。&lt;h4&gt;结论&lt;/h4&gt;active-DeepFA方法在标注数据稀缺的情况下，能够有效提高图像分类的准确率，并显著减少标注工作的工作量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major challenge that prevents the training of DL models is the limitedavailability of accurately labeled data. This shortcoming is highlighted inareas where data annotation becomes a time-consuming and error-prone task. Inthis regard, SSL tackles this challenge by capitalizing on scarce labeled andabundant unlabeled data; however, SoTA methods typically depend on pre-trainedfeatures and large validation sets to learn effective representations forclassification tasks. In addition, the reduced set of labeled data is oftenrandomly sampled, neglecting the selection of more informative samples. Here,we present active-DeepFA, a method that effectively combines CL,teacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNNarchitectures for image classification in scenarios of scarcity of labeled andabundance of unlabeled data. It integrates DeepFA into a co-training setup thatimplements two cooperative networks to mitigate confirmation bias frompseudo-labels. The method starts with a reduced set of labeled samples bywarming up the networks with supervised CL. Afterward and at regular epochintervals, label propagation is performed on the 2D projections of thenetworks' deep features. Next, the most reliable pseudo-labels are exchangedbetween networks in a cross-training fashion, while the most meaningful samplesare annotated and added into the labeled set. The networks independentlyminimize an objective loss function comprising supervised contrastive,supervised and semi-supervised loss components, enhancing the representationstowards image classification. Our approach is evaluated on three challengingbiological image datasets using only 5% of labeled samples, improving baselinesand outperforming six other SoTA methods. In addition, it reduces annotationeffort by achieving comparable results to those of its counterparts with only3% of labeled data.</description>
      <author>example@mail.com (David Aparco-Cardenas, Jancarlo F. Gomes, Alexandre X. Falcão, Pedro J. de Rezende)</author>
      <guid isPermaLink="false">2504.18666v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Intelligent4DSE: Optimizing High-Level Synthesis Design Space Exploration with Graph Neural Networks and Large Language Models</title>
      <link>http://arxiv.org/abs/2504.19649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoGNNs-LLMEA的框架，用于优化HLS设计空间探索过程，通过结合图神经网络、任务自适应消息传递和大型语言模型增强的进化算法，提高预测准确性，减少预测误差。&lt;h4&gt;背景&lt;/h4&gt;HLS设计空间探索是电子设计自动化中的一个优化过程，旨在通过系统性地探索高级设计配置来实现性能、面积和功耗（PPA）平衡的硬件实现。&lt;h4&gt;目的&lt;/h4&gt;优化HLS预测任务，提高预测准确性，减少预测误差，同时减少对领域特定知识的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为CoGNNs-LLMEA的框架，该框架集成了图神经网络、任务自适应消息传递和大型语言模型增强的进化算法。CoGNNs直接利用编译器前端处理后的源代码生成的中间表示，预测结果质量（QoR），无需调用HLS工具。&lt;h4&gt;主要发现&lt;/h4&gt;CoGNNs在HLS后的QoR预测中达到了最先进的预测准确性，与基线模型相比，平均预测误差在延迟方面降低了2.8倍，在资源利用率方面降低了3.4倍。&lt;h4&gt;结论&lt;/h4&gt;CoGNNs-LLMEA框架能够有效提高HLS设计空间探索的预测准确性，减少预测误差，并降低对领域特定知识的依赖。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高级综合（HLS）设计空间探索（DSE）是电子设计自动化（EDA）中的一个优化过程，它系统地探索高级设计配置，以实现平衡性能、面积和功耗（PPA）的帕累托最优硬件实现。为了优化此过程，HLS预测任务通常采用消息传递神经网络（MPNN），利用复杂的架构以实现高精度。这些预测器作为DSE过程中的评估者，有效地绕过了传统上由HLS工具要求的耗时估计。然而，现有的模型往往优先考虑结构复杂性和训练损失的优化，而忽略了特定任务的特性。此外，尽管进化算法在DSE中得到广泛应用，但它们通常需要大量的领域特定知识来设计有效的交叉和变异算子。为了解决这些限制，我们提出了一种名为CoGNNs-LLMEA的框架，该框架将图神经网络与任务自适应消息传递和大型语言模型增强的进化算法相结合。作为预测模型，CoGNNs直接利用源代码在编译器前端处理后的中间表示，能够在不调用HLS工具的情况下预测结果质量（QoR）。由于其强大的任务适应性，CoGNNs可以调整以预测HLS后和实现后的结果，有效地弥合了高级抽象和物理实现特性之间的差距。CoGNNs在HLS后的QoR预测中实现了最先进的预测准确性，与基线模型相比，平均预测误差在延迟方面降低了2.8倍，在资源利用率方面降低了3.4倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-level synthesis (HLS) design space exploration (DSE) is an optimizationprocess in electronic design automation (EDA) that systematically exploreshigh-level design configurations to achieve Pareto-optimal hardwareimplementations balancing performance, area, and power (PPA). To optimize thisprocess, HLS prediction tasks often employ message-passing neural networks(MPNNs), leveraging complex architectures to achieve high accuracy. Thesepredictors serve as evaluators in the DSE process, effectively bypassing thetime-consuming estimations traditionally required by HLS tools. However,existing models often prioritize structural complexity and minimization oftraining loss, overlooking task-specific characteristics. Additionally, whileevolutionary algorithms are widely used in DSE, they typically requireextensive domain-specific knowledge to design effective crossover and mutationoperators. To address these limitations, we propose CoGNNs-LLMEA, a frameworkthat integrates a graph neural network with task-adaptive message passing and alarge language model-enhanced evolutionary algorithm. As a predictive model,CoGNNs directly leverages intermediate representations generated from sourcecode after compiler front-end processing, enabling prediction of quality ofresults (QoR) without invoking HLS tools. Due to its strong adaptability totasks, CoGNNs can be tuned to predict post-HLS and post-implementationoutcomes, effectively bridging the gap between high-level abstractions andphysical implementation characteristics. CoGNNs achieves state-of-the-artprediction accuracy in post-HLS QoR prediction, reducing mean prediction errorsby 2.8$\times$ for latency and 3.4$\times$ for resource utilization compared tobaseline models.</description>
      <author>example@mail.com (Lei Xu, Shanshan Wang, Emmanuel Casseau, Chenglong Xiao)</author>
      <guid isPermaLink="false">2504.19649v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation</title>
      <link>http://arxiv.org/abs/2504.19174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SIGGRAPH 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CLR-Wire是一种基于3D曲线的线框生成框架，它将几何和拓扑整合到一个统一的连续潜在表示中。&lt;h4&gt;背景&lt;/h4&gt;传统方法将顶点、边和面解耦，而CLR-Wire将曲线及其拓扑连接编码为神经网络参数曲线，并使用注意力驱动的变分自编码器（VAE）将它们映射到连续且固定长度的潜在空间。&lt;h4&gt;目的&lt;/h4&gt;CLR-Wire旨在提供一种能够同时学习几何和拓扑的统一方法，并生成高质量的线框。&lt;h4&gt;方法&lt;/h4&gt;CLR-Wire使用流匹配模型将高斯噪声映射到潜在空间，然后解码为完整的3D线框。它支持无条件生成和基于点云或图像输入的生成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与最先进的生成方法相比，CLR-Wire在准确性、新颖性和多样性方面有显著提升。&lt;h4&gt;结论&lt;/h4&gt;CLR-Wire为CAD设计、几何重建和3D内容创建提供了一种高效且全面的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种名为CLR-Wire的新型框架，该框架用于基于3D曲线的线框生成，它将几何和拓扑整合到一个统一的连续潜在表示中。与将顶点、边和面解耦的传统方法不同，CLR-Wire将曲线及其拓扑连接编码为神经网络参数曲线，并使用注意力驱动的变分自编码器（VAE）将它们映射到连续且固定长度的潜在空间。这种统一的方法促进了几何和拓扑的联合学习和生成。为了生成线框，我们使用流匹配模型将高斯噪声逐步映射到这些潜在空间，然后解码为完整的3D线框。我们的方法提供了对复杂形状和不规则拓扑的精细建模，并支持无条件生成和基于点云或图像输入的生成。实验结果表明，与最先进的生成方法相比，我们的方法在准确性、新颖性和多样性方面取得了显著提高，为CAD设计、几何重建和3D内容创作提供了一种高效且全面的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce CLR-Wire, a novel framework for 3D curve-based wireframegeneration that integrates geometry and topology into a unified ContinuousLatent Representation. Unlike conventional methods that decouple vertices,edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves alongwith their topological connectivity into a continuous and fixed-length latentspace using an attention-driven variational autoencoder (VAE). This unifiedapproach facilitates joint learning and generation of both geometry andtopology. To generate wireframes, we employ a flow matching model toprogressively map Gaussian noise to these latents, which are subsequentlydecoded into complete 3D wireframes. Our method provides fine-grained modelingof complex shapes and irregular topologies, and supports both unconditionalgeneration and generation conditioned on point cloud or image inputs.Experimental results demonstrate that, compared with state-of-the-artgenerative approaches, our method achieves substantial improvements inaccuracy, novelty, and diversity, offering an efficient and comprehensivesolution for CAD design, geometric reconstruction, and 3D content creation.</description>
      <author>example@mail.com (Xueqi Ma, Yilin Liu, Tianlong Gao, Qirui Huang, Hui Huang)</author>
      <guid isPermaLink="false">2504.19174v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>If Concept Bottlenecks are the Question, are Foundation Models the Answer?</title>
      <link>http://arxiv.org/abs/2504.19774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了概念瓶颈模型（CBMs），这是一种结合高性能和先验可解释性的神经网络。通过将输入映射到高级概念并使用这些概念解决下游任务，CBMs旨在提高可解释性。然而，模型性能和可解释性依赖于学习到的概念质量，而高质量概念的学习往往依赖于昂贵的专家标注。本文通过实验分析了使用弱监督的VLM-CBM架构对学习到的概念质量的影响。&lt;h4&gt;背景&lt;/h4&gt;概念瓶颈模型（CBMs）旨在结合高性能和可解释性，通过将输入映射到高级概念来解决下游任务。&lt;h4&gt;目的&lt;/h4&gt;研究使用弱监督的VLM-CBM架构对学习到的概念质量的影响，并分析其与专家标注的差异。&lt;h4&gt;方法&lt;/h4&gt;通过实验分析VLM-CBMs架构，使用一系列显著指标评估学习到的概念。&lt;h4&gt;主要发现&lt;/h4&gt;VLM监督与专家标注在任务上可能存在显著差异，概念准确性和质量之间没有强相关性。&lt;h4&gt;结论&lt;/h4&gt;VLM监督可以作为一种替代专家标注的方法，但需要根据具体任务调整以保持概念质量。&lt;h4&gt;翻译&lt;/h4&gt;Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high-performance with ante-hoc interpretability. CBMs work by first mapping inputs (e.g., images) to high-level concepts (e.g., visible objects and their properties) and then use these to solve a downstream task (e.g., tagging or scoring an image) in an interpretable manner. Their performance and interpretability, however, hinge on the quality of the concepts they learn. The go-to strategy for ensuring good quality concepts is to leverage expert annotations, which are expensive to collect and seldom available in applications. Researchers have recently addressed this issue by introducing 'VLM-CBM' architectures that replace manual annotations with weak supervision from foundation models. It is however unclear what is the impact of doing so on the quality of the learned concepts. To answer this question, we put state-of-the-art VLM-CBMs to the test, analyzing their learned concepts empirically using a selection of significant metrics. Our results show that, depending on the task, VLM supervision can sensibly differ from expert annotations, and that concept accuracy and quality are not strongly correlated. Our code is available at https://github.com/debryu/CQA.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept Bottleneck Models (CBMs) are neural networks designed to conjoin highperformance with ante-hoc interpretability. CBMs work by first mapping inputs(e.g., images) to high-level concepts (e.g., visible objects and theirproperties) and then use these to solve a downstream task (e.g., tagging orscoring an image) in an interpretable manner. Their performance andinterpretability, however, hinge on the quality of the concepts they learn. Thego-to strategy for ensuring good quality concepts is to leverage expertannotations, which are expensive to collect and seldom available inapplications. Researchers have recently addressed this issue by introducing"VLM-CBM" architectures that replace manual annotations with weak supervisionfrom foundation models. It is however unclear what is the impact of doing so onthe quality of the learned concepts. To answer this question, we putstate-of-the-art VLM-CBMs to the test, analyzing their learned conceptsempirically using a selection of significant metrics. Our results show that,depending on the task, VLM supervision can sensibly differ from expertannotations, and that concept accuracy and quality are not strongly correlated.Our code is available at https://github.com/debryu/CQA.</description>
      <author>example@mail.com (Nicola Debole, Pietro Barbiero, Francesco Giannini, Andrea Passeggini, Stefano Teso, Emanuele Marconato)</author>
      <guid isPermaLink="false">2504.19774v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>An SE(3) Noise Model for Range-Azimuth-Elevation Sensors</title>
      <link>http://arxiv.org/abs/2504.19009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文讨论了点云配准在状态估计中的应用，特别是Scan matching技术。分析了当前方法中协方差表示不准确和忽略传感器与车辆外部不确定性以及里程计不确定性对权重的影响。&lt;h4&gt;背景&lt;/h4&gt;Scan matching是一种在状态估计中广泛使用的技术，其中点云配准是一种加权最小二乘问题，其权重由测量点的逆协方差确定。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过开发一个基于矩阵李群的测距-方位-高度传感器模型来解决上述问题。&lt;h4&gt;方法&lt;/h4&gt;该方法允许无缝地结合外部和里程计不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;文章展示了模拟示例和实际水下激光扫描收集的点云子图的结果，说明了新模型的有效性。&lt;h4&gt;结论&lt;/h4&gt;新模型能够更准确地处理Scan matching中的不确定性，提高估计的可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scan matching is a widely used technique in state estimation. Point-cloudalignment, one of the most popular methods for scan matching, is a weightedleast-squares problem in which the weights are determined from the inversecovariance of the measured points. An inaccurate representation of thecovariance will affect the weighting of the least-squares problem. For example,if ellipsoidal covariance bounds are used to approximate the curved,"banana-shaped" noise characteristics of many scanning sensors, the weightingin the least-squares problem may be overconfident. Additionally,sensor-to-vehicle extrinsic uncertainty and odometry uncertainty during submapformation are two sources of uncertainty that are often overlooked in scanmatching applications, also likely contributing to overconfidence on the scanmatching estimate. This paper attempts to address these issues by developing amodel for range-azimuth-elevation sensors on matrix Lie groups. The modelallows for the seamless incorporation of extrinsic and odometry uncertainty.Illustrative results are shown both for a simulated example and for a realpoint-cloud submap collected with an underwater laser scanner.</description>
      <author>example@mail.com (Thomas Hitchcox, James Richard Forbes)</author>
      <guid isPermaLink="false">2504.19009v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge</title>
      <link>http://arxiv.org/abs/2504.18961v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A technical report for the MMCTR Challenge held by EReL@MIR Workshop  at WWW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了在Multimodal Large Language Models (MLLMs)快速发展的背景下，研究人员在推荐系统中的应用探索，以及针对提高信息检索任务中多模态表示学习效率的各种方法的实验。&lt;h4&gt;背景&lt;/h4&gt;MLLMs的快速发展促使研究人员探索其在推荐系统中的应用，但大型模型的高延迟给此类应用带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;EREL@MIR研讨会为提高多模态表示学习效率提供了实验机会，要求参赛者提交技术报告详细说明他们的方法和发现。&lt;h4&gt;方法&lt;/h4&gt;我们的团队在竞赛中获得了Task 2 - Winner (Multimodal CTR Prediction)的奖项，并在报告中详细介绍了我们的方法和关键发现。&lt;h4&gt;主要发现&lt;/h4&gt;提出了如何有效地将推荐信号整合到多模态表示中的几个研究方向。&lt;h4&gt;结论&lt;/h4&gt;公开了我们的实现代码库和训练模型权重。&lt;h4&gt;翻译&lt;/h4&gt;With the rapid advancement of Multimodal Large Language Models (MLLMs), an increasing number of researchers are exploring their application in recommendation systems. However, the high latency associated with large models presents a significant challenge for such use cases. The EReL@MIR workshop provided a valuable opportunity to experiment with various approaches aimed at improving the efficiency of multimodal representation learning for information retrieval tasks. As part of the competition's requirements, participants were mandated to submit a technical report detailing their methodologies and findings. Our team was honored to receive the award for Task 2 - Winner (Multimodal CTR Prediction). In this technical report, we present our methods and key findings. Additionally, we propose several directions for future work, particularly focusing on how to effectively integrate recommendation signals into multimodal representations. The codebase for our implementation is publicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the trained model weights can be accessed at: https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of Multimodal Large Language Models (MLLMs), anincreasing number of researchers are exploring their application inrecommendation systems. However, the high latency associated with large modelspresents a significant challenge for such use cases. The EReL@MIR workshopprovided a valuable opportunity to experiment with various approaches aimed atimproving the efficiency of multimodal representation learning for informationretrieval tasks. As part of the competition's requirements, participants weremandated to submit a technical report detailing their methodologies andfindings. Our team was honored to receive the award for Task 2 - Winner(Multimodal CTR Prediction). In this technical report, we present our methodsand key findings. Additionally, we propose several directions for future work,particularly focusing on how to effectively integrate recommendation signalsinto multimodal representations. The codebase for our implementation ispublicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and thetrained model weights can be accessed at:https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.</description>
      <author>example@mail.com (Junjie Zhou)</author>
      <guid isPermaLink="false">2504.18961v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>A Universal Spin-Orbit-Coupled Hamiltonian Model for Accelerated Quantum Material Discovery</title>
      <link>http://arxiv.org/abs/2504.19586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages,8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Uni-HamGNN的通用自旋轨道耦合（SOC）哈密顿量图神经网络，用于准确模拟复杂系统中的SOC效应，同时解决了传统密度泛函理论（DFT）计算需求高和现有机器学习框架可迁移性有限的问题。&lt;h4&gt;背景&lt;/h4&gt;在模拟复杂系统中的SOC效应时，DFT的计算需求高，且现有机器学习框架的可迁移性有限，导致准确建模成为一大挑战。&lt;h4&gt;目的&lt;/h4&gt;通过引入Uni-HamGNN，旨在解决高计算需求和有限的可迁移性问题，实现复杂系统中SOC效应的准确建模。&lt;h4&gt;方法&lt;/h4&gt;该方法将SOC哈密顿量分解为自旋无关项和SOC校正项，以保持SU(2)对称性，并显著降低参数需求。基于这种分解，提出了delta-learning策略来分别拟合两个组成部分，从而解决由它们之间的幅度差异引起的训练困难，并实现高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在SOC相关组件上实现了显著的准确性（平均绝对误差为0.0025 meV），并通过GNoME数据集的高通量筛选和2D谷电子材料和过渡金属二硫化物（TMD）异质结构的精确预测，展示了其广泛的应用性。&lt;h4&gt;结论&lt;/h4&gt;这一突破消除了对系统特定重新训练和高成本的SOC-DFT计算的需求，为量子材料的快速发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;The accurate modeling of spin-orbit coupling (SOC) effects in diverse complex systems remains a significant challenge due to the high computational demands of density functional theory (DFT) and the limited transferability of existing machine-learning frameworks. This study addresses these limitations by introducing Uni-HamGNN, a universal SOC Hamiltonian graph neural network that is applicable across the periodic table. By decomposing the SOC Hamiltonian into spin-independent and SOC correction terms, our approach preserves SU(2) symmetry while significantly reducing parameter requirements. Based on this decomposition, we propose a delta-learning strategy to separately fit the two components, thereby addressing the training difficulties caused by magnitude discrepancies between them and enabling efficient training. The model achieves remarkable accuracy (mean absolute error of 0.0025 meV for the SOC-related component) and demonstrates broad applicability through high-throughput screening of the GNoME dataset for topological insulators, as well as precise predictions for 2D valleytronic materials and transition metal dichalcogenide (TMD) heterostructures. This breakthrough eliminates the need for system-specific retraining and costly SOC-DFT calculations, paving the way for rapid discovery of quantum materials.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate modeling of spin-orbit coupling (SOC) effects in diverse complexsystems remains a significant challenge due to the high computational demandsof density functional theory (DFT) and the limited transferability of existingmachine-learning frameworks. This study addresses these limitations byintroducing Uni-HamGNN, a universal SOC Hamiltonian graph neural network thatis applicable across the periodic table. By decomposing the SOC Hamiltonianinto spin-independent and SOC correction terms, our approach preserves SU(2)symmetry while significantly reducing parameter requirements. Based on thisdecomposition, we propose a delta-learning strategy to separately fit the twocomponents, thereby addressing the training difficulties caused by magnitudediscrepancies between them and enabling efficient training. The model achievesremarkable accuracy (mean absolute error of 0.0025 meV for the SOC-relatedcomponent) and demonstrates broad applicability through high-throughputscreening of the GNoME dataset for topological insulators, as well as precisepredictions for 2D valleytronic materials and transition metal dichalcogenide(TMD) heterostructures. This breakthrough eliminates the need forsystem-specific retraining and costly SOC-DFT calculations, paving the way forrapid discovery of quantum materials.</description>
      <author>example@mail.com (Yang Zhong, Rui Wang, Xingao Gong, Hongjun Xiang)</author>
      <guid isPermaLink="false">2504.19586v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks</title>
      <link>http://arxiv.org/abs/2504.19499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in the proceedings of the 2025 IEEE International  Conference on Communications (ICC), Seventh Workshop on Data Driven  Intelligence for Networks and Systems (DDINS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图强化学习的QoS感知负载均衡方法，用于优化多频段O-RAN中GBR和BE流量的性能，同时满足QoS和资源约束。&lt;h4&gt;背景&lt;/h4&gt;下一代无线蜂窝网络需要提供卓越的服务质量，以支持新兴无线应用，这要求严格的性能保证，如链路层数据速率。&lt;h4&gt;目的&lt;/h4&gt;解决满足QoS要求的关键挑战，即防止小区拥塞，通过平衡负载确保每个小区有足够的无线资源来服务其指定的用户设备。&lt;h4&gt;方法&lt;/h4&gt;提出的方法基于图强化学习（GRL），将QoS感知负载均衡建模为马尔可夫决策过程，状态表示为图。QoS考虑因素集成到状态表示和奖励信号设计中。使用基于GNN架构的离策略对抗深度Q网络（DQN）训练负载均衡代理。&lt;h4&gt;主要发现&lt;/h4&gt;与两种基线方法相比，GRL解决方案的性能显著提高，包括QoS违规减少了53%，BE流量的第5百分位数速率提高了四倍。&lt;h4&gt;结论&lt;/h4&gt;该方法确保了负载均衡策略对节点（UE或小区）的顺序不变，能够灵活处理各种网络大小，并在负载均衡决策中考虑空间节点依赖性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation wireless cellular networks are expected to provideunparalleled Quality-of-Service (QoS) for emerging wireless applications,necessitating strict performance guarantees, e.g., in terms of link-level datarates. A critical challenge in meeting these QoS requirements is the preventionof cell congestion, which involves balancing the load to ensure sufficientradio resources are available for each cell to serve its designated UserEquipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approachis developed to optimize the performance of Guaranteed Bit Rate (GBR) and BestEffort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoSand resource constraints. The proposed solution builds on Graph ReinforcementLearning (GRL), a powerful framework at the intersection of Graph NeuralNetwork (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process,with states represented as graphs. QoS consideration are integrated into bothstate representations and reward signal design. The LB agent is then trainedusing an off-policy dueling Deep Q Network (DQN) that leverages a GNN-basedarchitecture. This design ensures the LB policy is invariant to the ordering ofnodes (UE or cell), flexible in handling various network sizes, and capable ofaccounting for spatial node dependencies in LB decisions. Performance of theGRL-based solution is compared with two baseline methods. Results showsubstantial performance gains, including a $53\%$ reduction in QoS violationsand a fourfold increase in the 5th percentile rate for BE traffic.</description>
      <author>example@mail.com (Omid Semiari, Hosein Nikopour, Shilpa Talwar)</author>
      <guid isPermaLink="false">2504.19499v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation</title>
      <link>http://arxiv.org/abs/2504.19718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 4 figures, to be published in Eurographics 2025 as a short  paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的方法，用于在3D人脸扫描中准确区分皮肤和非皮肤几何形状，以提高人脸注册的质量。&lt;h4&gt;背景&lt;/h4&gt;由于在非皮肤区域（如头发、胡须、配饰）中扫描质量通常下降，现有的人脸注册方法难以处理这些问题。&lt;h4&gt;目的&lt;/h4&gt;提高人脸注册的质量，通过在扫描网格上准确区分皮肤和非皮肤区域。&lt;h4&gt;方法&lt;/h4&gt;该方法通过使用冻结的图像基础模型从多视角图像中提取特征，并在3D空间中聚合这些特征。然后，将这些提升的2D特征与从扫描网格中提取的3D几何特征融合，以在扫描网格上直接预测分割掩码。&lt;h4&gt;主要发现&lt;/h4&gt;该方法的分割结果比纯2D或3D分割方法分别提高了8.89%和14.3%的注册准确性。&lt;h4&gt;结论&lt;/h4&gt;尽管仅用合成数据进行训练，但该模型对真实数据具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;This paper proposes a novel method for accurately separating skin from non-skin geometry on 3D human head scans to improve the quality of face registration.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Face registration deforms a template mesh to closely fit a 3D face scan, thequality of which commonly degrades in non-skin regions (e.g., hair, beard,accessories), because the optimized template-to-scan distance pulls thetemplate mesh towards the noisy scan surface. Improving registration qualityrequires a clean separation of skin and non-skin regions on the scan mesh.Existing image-based (2D) or scan-based (3D) segmentation methods howeverperform poorly. Image-based segmentation outputs multi-view inconsistent masks,and they cannot account for scan inaccuracies or scan-image misalignment, whilescan-based methods suffer from lower spatial resolution compared to images. Inthis work, we introduce a novel method that accurately separates skin fromnon-skin geometry on 3D human head scans. For this, our method extractsfeatures from multi-view images using a frozen image foundation model andaggregates these features in 3D. These lifted 2D features are then fused with3D geometric features extracted from the scan mesh, to then predict asegmentation mask directly on the scan mesh. We show that our segmentationsimprove the registration accuracy over pure 2D or 3D segmentation methods by8.89% and 14.3%, respectively. Although trained only on synthetic data, ourmodel generalizes well to real data.</description>
      <author>example@mail.com (Victoria Yue Chen, Daoye Wang, Stephan Garbin, Sebastian Winberg, Timo Bolkart, Thabo Beeler)</author>
      <guid isPermaLink="false">2504.19718v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>UNILoc: Unified Localization Combining Model-Based Geometry and Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2504.17676v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, submitted to IEEE conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的定位方法，该方法结合了基于模型和基于机器学习的方法，通过利用可用的地图信息来发挥各自的优势。&lt;h4&gt;背景&lt;/h4&gt;精确的移动设备定位对于新兴的5G/6G应用（如自动驾驶和增强现实）至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种统一的方法，以提高定位精度，并避免监督学习。&lt;h4&gt;方法&lt;/h4&gt;通过融合几何估计和建筑布局来生成训练标签，避免监督学习；使用基于光线追踪的模拟来验证方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在视线（LoS）用户和非视线（NLoS）用户的位置精度上均有显著提升；与完全监督的指纹识别相比，该方法在整体性能上具有竞争力，同时消除了繁琐的标签数据测量和收集的需求。&lt;h4&gt;结论&lt;/h4&gt;提出的统一方法能够显著提高定位精度，同时避免了传统方法的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate mobile device localization is critical for emerging 5G/6Gapplications such as autonomous vehicles and augmented reality. In this paper,we propose a unified localization method that integrates model-based andmachine learning (ML)-based methods to reap their respective advantages byexploiting available map information. In order to avoid supervised learning, wegenerate training labels automatically via optimal transport (OT) by fusinggeometric estimates with building layouts. Ray-tracing based simulations arecarried out to demonstrate that the proposed method significantly improvespositioning accuracy for both line-of-sight (LoS) users (compared to ML-basedmethods) and non-line-of-sight (NLoS) users (compared to model-based methods).Remarkably, the unified method is able to achieve competitive overallperformance with the fully-supervised fingerprinting, while eliminating theneed for cumbersome labeled data measurement and collection.</description>
      <author>example@mail.com (Yuhao Zhang, Guangjin Pan, Musa Furkan Keskin, Ossi Kaltiokallio, Mikko Valkama, Henk Wymeersch)</author>
      <guid isPermaLink="false">2504.17676v2</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>ReLU integral probability metric and its applications</title>
      <link>http://arxiv.org/abs/2504.18897v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  49 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种参数化的积分概率度量（IPM）来衡量两个概率测度之间的差异，并在多个任务中展示了其有效性和优越性能。&lt;h4&gt;背景&lt;/h4&gt;在衡量两个概率测度之间的差异时，需要一种有效的度量方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的IPM，用于测量两个概率测度之间的差异，并应用于高维设置。&lt;h4&gt;方法&lt;/h4&gt;使用特定的参数化判别器家族，如具有ReLU激活的单节点神经网络，来区分分布，并通过优化所选判别器的参数来提高估计器的收敛速度。&lt;h4&gt;主要发现&lt;/h4&gt;提出的IPM在多个任务中提供了强大的理论保证，并且实证实验表明，其性能与其他方法相当甚至更优。&lt;h4&gt;结论&lt;/h4&gt;该IPM在处理概率测度差异时表现出色，具有高效算法和较少的超参数，适用于高维设置，并在因果推断和公平表示学习等任务中具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种参数化的积分概率度量（IPM）来衡量两个概率测度之间的差异。所提出的IPM利用特定的参数化判别器家族，例如具有ReLU激活的单节点神经网络，以有效地区分分布，使其适用于高维设置。通过优化所选判别器类的参数，所提出的IPM表明其估计器具有良好的收敛速度，可以成为使用平滑非参数判别器类的其他IPM的替代品。我们提出了一种高效的算法用于实际计算，提供了一种简单的实现，并需要较少的超参数。此外，我们探讨了其在各种任务中的应用，如因果推断的协变量平衡和公平表示学习。在如此多样的应用中，我们证明了所提出的IPM提供了强大的理论保证，并且实证实验表明，它实现了与其他方法相当甚至更优的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a parametric integral probability metric (IPM) to measure thediscrepancy between two probability measures. The proposed IPM leverages aspecific parametric family of discriminators, such as single-node neuralnetworks with ReLU activation, to effectively distinguish betweendistributions, making it applicable in high-dimensional settings. By optimizingover the parameters of the chosen discriminator class, the proposed IPMdemonstrates that its estimators have good convergence rates and can serve as asurrogate for other IPMs that use smooth nonparametric discriminator classes.We present an efficient algorithm for practical computation, offering a simpleimplementation and requiring fewer hyperparameters. Furthermore, we explore itsapplications in various tasks, such as covariate balancing for causal inferenceand fair representation learning. Across such diverse applications, wedemonstrate that the proposed IPM provides strong theoretical guarantees, andempirical experiments show that it achieves comparable or even superiorperformance to other methods.</description>
      <author>example@mail.com (Yuha Park, Kunwoong Kim, Insung Kong, Yongdai Kim)</author>
      <guid isPermaLink="false">2504.18897v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>MASR: Self-Reflective Reasoning through Multimodal Hierarchical Attention Focusing for Agent-based Video Understanding</title>
      <link>http://arxiv.org/abs/2504.17213v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于代理的视频理解框架MASR，该框架在视频理解方面取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;尽管大模型在快速发展的时代，视频理解仍然是一个极具挑战的任务。&lt;h4&gt;目的&lt;/h4&gt;针对视频信息丰富且冗余的问题，提出MASR框架以提高视频理解的全面性和准确性。&lt;h4&gt;方法&lt;/h4&gt;MASR通过多模态粗到细的相关性感知（MCRS）和膨胀时间扩展（DTE）来检测和优先处理与查询高度相关的视频片段，并在自反推理过程中迭代应用MCRS和DTE，以自适应调整注意力。&lt;h4&gt;主要发现&lt;/h4&gt;MASR在EgoSchema数据集上比之前的方法提高了5%的性能，在Next-QA和IntentQA数据集上分别超过了最先进的标准的0.2%和0.3%，在包含长期视频的Video-MME数据集上，MASR也优于其他基于代理的方法。&lt;h4&gt;结论&lt;/h4&gt;MASR框架在视频理解任务中表现优异，为提高视频理解性能提供了有效的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Even in the era of rapid advances in large models, video understandingremains a highly challenging task. Compared to texts or images, videos commonlycontain more information with redundancy, requiring large models to properlyallocate attention at a global level for comprehensive and accurateunderstanding. To address this, we propose a Multimodal hierarchical Attentionfocusing Self-reflective Reasoning (MASR) framework for agent-based videounderstanding. The key innovation lies in its ability to detect and prioritizesegments of videos that are highly relevant to the query. Firstly, MASRrealizes Multimodal Coarse-to-fine Relevance Sensing (MCRS) which enhances thecorrelation between the acquired contextual information and the query.Secondly, MASR employs Dilated Temporal Expansion (DTE) to mitigate the risk ofmissing crucial details when extracting semantic information from the focusedframes selected through MCRS. By iteratively applying MCRS and DTE in theself-reflective reasoning process, MASR is able to adaptively adjust theattention to extract highly query-relevant context and therefore improve theresponse accuracy. In the EgoSchema dataset, MASR achieves a remarkable 5%performance gain over previous leading approaches. In the Next-QA and IntentQAdatasets, it outperforms the state-of-the-art standards by 0.2% and 0.3%respectively. In the Video-MME dataset that contains long-term videos, MASRalso performs better than other agent-based methods.</description>
      <author>example@mail.com (Shiwen Cao, Zhaoxing Zhang, Junming Jiao, Juyi Qiao, Guowen Song, Rong Shen, Xiangbing Meng)</author>
      <guid isPermaLink="false">2504.17213v2</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>HeartSimSage: Attention-Enhanced Graph Neural Networks for Accelerating Cardiac Mechanics Modeling</title>
      <link>http://arxiv.org/abs/2504.18968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于注意力增强图神经网络的心脏生物力学有限元分析模拟器HeartSimSage，用于快速预测心脏被动双心室心肌位移。&lt;h4&gt;背景&lt;/h4&gt;有限元分析（FEA）是心脏生物力学建模的基础，但其计算成本高，限制了其在数字孪生创建中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发HeartSimSage以解决现有模拟器的限制，快速预测患者特定几何形状、心室压力和材料属性下的被动双心室心肌位移。&lt;h4&gt;方法&lt;/h4&gt;HeartSimSage能够有效处理不同的三维双心室几何形状、网格拓扑、纤维方向、基于结构的本构模型和生理边界条件。它支持可变节点数、排序和单元连接的灵活网格结构。通过设计受GraphSAGE启发的邻近连接策略，优化信息传播，并采用基于子集的训练提高效率。HeartSimSage集成了注意力机制，自适应地权衡邻居贡献并过滤无关信息，从而提高预测精度。&lt;h4&gt;主要发现&lt;/h4&gt;HeartSimSage在GPU上实现了约13,000倍的加速，在CPU上实现了190倍的加速，同时保持预测双心室位移的平均误差为0.13% ± 0.12%。&lt;h4&gt;结论&lt;/h4&gt;通过使用HeartSimSage，可以在不牺牲精度的情况下显著提高心脏生物力学模拟的计算效率。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Finite element analysis (FEA) forms the cornerstone of modeling cardiac biomechanics but is computationally expensive, limiting its clinical application for digital twin creation, which often requires tens to hundreds of simulations to estimate tissue parameters. We developed an attention-enhanced graph neural network (GNN)-based FEA emulator, HeartSimSage, to rapidly predict passive biventricular myocardial displacements from patient-specific geometries, chamber pressures, and material properties. HeartSimSage addresses the limitations of current emulators by effectively handling diverse three-dimensional (3D) biventricular geometries, mesh topologies, fiber directions, structurally based constitutive models, and physiological boundary conditions. It supports flexible mesh structures with variable node counts, orderings, and element connectivity. To optimize information propagation, we designed a neighboring connection strategy inspired by Graph Sample and Aggregate (GraphSAGE) that prioritizes local interactions while maintaining mid-to-long-range dependencies. We further incorporated Laplace-Dirichlet solutions for enhanced spatial encoding and employed subset-based training for improved efficiency. By integrating an attention mechanism, HeartSimSage adaptively weighs neighbor contributions and filters irrelevant information, enhancing prediction accuracy. HeartSimSage achieves approximately 13,000x speedup on GPU and 190x on CPU compared to traditional FEA, while maintaining a nominal averaged error of 0.13% ± 0.12% in predicting biventricular displacements. We validated our model using a published left ventricle dataset and conducted sensitivity analyses on hyperparameters, neighboring strategies, and the attention mechanism.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Finite element analysis (FEA) forms the cornerstone of modeling cardiacbiomechanics but is computationally expensive, limiting its clinicalapplication for digital twin creation, which often requires tens to hundreds ofsimulations to estimate tissue parameters. We developed an attention-enhancedgraph neural network (GNN)-based FEA emulator, HeartSimSage, to rapidly predictpassive biventricular myocardial displacements from patient-specificgeometries, chamber pressures, and material properties. HeartSimSage addressesthe limitations of current emulators by effectively handling diversethree-dimensional (3D) biventricular geometries, mesh topologies, fiberdirections, structurally based constitutive models, and physiological boundaryconditions. It supports flexible mesh structures with variable node counts,orderings, and element connectivity. To optimize information propagation, wedesigned a neighboring connection strategy inspired by Graph Sample andAggregate (GraphSAGE) that prioritizes local interactions while maintainingmid-to-long-range dependencies. We further incorporated Laplace-Dirichletsolutions for enhanced spatial encoding and employed subset-based training forimproved efficiency. By integrating an attention mechanism, HeartSimSageadaptively weighs neighbor contributions and filters irrelevant information,enhancing prediction accuracy. HeartSimSage achieves approximately 13,000xspeedup on GPU and 190x on CPU compared to traditional FEA, while maintaining anominal averaged error of 0.13% +- 0.12% in predicting biventriculardisplacements. We validated our model using a published left ventricle datasetand conducted sensitivity analyses on hyperparameters, neighboring strategies,and the attention mechanism.</description>
      <author>example@mail.com (Lei Shi, Yurui Chen, Vijay Vedula)</author>
      <guid isPermaLink="false">2504.18968v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy</title>
      <link>http://arxiv.org/abs/2504.18829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Robotics: Science and Systems (RSS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的抓取合成方法，能够为任何抓取类型、物体和机械手合成丰富的接触、无穿透和物理上可行的抓取。&lt;h4&gt;背景&lt;/h4&gt;智能机器人需要掌握多样化的抓取技能，但收集涵盖多种抓取类型的海量高质量数据集极具挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够合成适用于任何抓取类型、物体和机械手的抓取的方法。&lt;h4&gt;方法&lt;/h4&gt;从每个手型和抓取类型的一个单一人工标注模板开始，通过两个阶段进行合成：首先优化物体以适应手模板，然后在模拟中对手进行局部细化以适应物体。引入了一种接触感知控制策略来验证合成的抓取，并允许手在接触点对物体施加适当的力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在模拟中显著优于之前的类型无关抓取合成基线。构建了一个包含10.7k个物体和9.5M个抓取的数据集，涵盖了GRASP分类法中的31种抓取类型。在现实世界实验中，通过训练的类型条件生成模型，从单视图物体点云中成功执行所需的抓取类型，成功率达到82.3%。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地解决了抓取合成问题，为智能机器人提供了强大的抓取能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：具有广泛适用性的灵活抓取是智能机器人的一项基本技能。开发这种技能需要一个大规模且高质量的数据集，该数据集涵盖了多种抓取类型（即至少那些由GRASP分类法归类的类型），但收集此类数据极为困难。现有的自动抓取合成方法通常局限于特定的抓取类型或物体类别，阻碍了可扩展性。这项工作提出了一种高效的管道，能够为任何抓取类型、物体和机械手合成丰富的接触、无穿透和物理上可行的抓取。从每个手型和抓取类型的一个单一人工标注模板开始，我们的管道通过两个阶段解决复杂的合成问题：首先优化物体以适应手模板，然后在模拟中对手进行局部细化以适应物体。为了验证合成的抓取，我们引入了一种接触感知控制策略，该策略允许手在每个接触点对物体施加适当的力。这些经过验证的抓取也可以用作新的抓取模板，以促进未来的合成。实验表明，我们的方法在模拟中显著优于之前的类型无关抓取合成基线。使用我们的算法，我们构建了一个包含10.7k个物体和9.5M个抓取的数据集，涵盖了GRASP分类法中的31种抓取类型。最后，我们训练了一个类型条件生成模型，该模型成功地从单视图物体点云中执行所需的抓取类型，在现实世界实验中的成功率为82.3%。项目页面：https://pku-epic.github.io/Dexonomy。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizable dexterous grasping with suitable grasp types is a fundamentalskill for intelligent robots. Developing such skills requires a large-scale andhigh-quality dataset that covers numerous grasp types (i.e., at least thosecategorized by the GRASP taxonomy), but collecting such data is extremelychallenging. Existing automatic grasp synthesis methods are often limited tospecific grasp types or object categories, hindering scalability. This workproposes an efficient pipeline capable of synthesizing contact-rich,penetration-free, and physically plausible grasps for any grasp type, object,and articulated hand. Starting from a single human-annotated template for eachhand and grasp type, our pipeline tackles the complicated synthesis problemwith two stages: optimize the object to fit the hand template first, and thenlocally refine the hand to fit the object in simulation. To validate thesynthesized grasps, we introduce a contact-aware control strategy that allowsthe hand to apply the appropriate force at each contact point to the object.Those validated grasps can also be used as new grasp templates to facilitatefuture synthesis. Experiments show that our method significantly outperformsprevious type-unaware grasp synthesis baselines in simulation. Using ouralgorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,covering 31 grasp types in the GRASP taxonomy. Finally, we train atype-conditional generative model that successfully performs the desired grasptype from single-view object point clouds, achieving an 82.3% success rate inreal-world experiments. Project page: https://pku-epic.github.io/Dexonomy.</description>
      <author>example@mail.com (Jiayi Chen, Yubin Ke, Lin Peng, He Wang)</author>
      <guid isPermaLink="false">2504.18829v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities</title>
      <link>http://arxiv.org/abs/2504.19596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysioOmni是一种用于多模态生理信号分析的基础模型，旨在解决现有方法在处理不同数据集和缺失模态时的局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态生理信号（如脑电图、心电图、眼电图和肌电图）在医疗和脑机接口领域至关重要，但现有方法依赖于特定架构和针对数据集的融合策略，难以学习通用的表示形式。&lt;h4&gt;目的&lt;/h4&gt;提出PhysioOmni模型，以实现跨数据集的泛化能力并处理推理时的缺失模态。&lt;h4&gt;方法&lt;/h4&gt;PhysioOmni训练了一个解耦的多模态分词器，通过模态不变和模态特定的目标进行掩码信号预训练。此外，通过原型对齐在下游数据集上进行鲁棒的微调，以确保对不同和缺失模态组合的适应性。&lt;h4&gt;主要发现&lt;/h4&gt;在情绪识别、睡眠阶段分类、运动预测和心理负荷检测等四个下游任务上，PhysioOmni实现了最先进的性能，同时保持了对抗缺失模态的强大鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;PhysioOmni模型有效解决了多模态生理信号分析中的挑战，并将在未来发布代码和模型权重。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucialfor healthcare and brain-computer interfaces. While existing methods rely onspecialized architectures and dataset-specific fusion strategies, they struggleto learn universal representations that generalize across datasets and handlemissing modalities at inference time. To address these issues, we proposePhysioOmni, a foundation model for multimodal physiological signal analysisthat models both homogeneous and heterogeneous features to decouple multimodalsignals and extract generic representations while maintaining compatibilitywith arbitrary missing modalities. PhysioOmni trains a decoupled multimodaltokenizer, enabling masked signal pre-training via modality-invariant andmodality-specific objectives. To ensure adaptability to diverse and incompletemodality combinations, the pre-trained encoders undergo resilient fine-tuningwith prototype alignment on downstream datasets. Extensive experiments on fourdownstream tasks, emotion recognition, sleep stage classification, motorprediction, and mental workload detection, demonstrate that PhysioOmni achievesstate-of-the-art performance while maintaining strong robustness to missingmodalities. Our code and model weights will be released.</description>
      <author>example@mail.com (Xi Fu, Wei-Bang Jiang, Yi Ding, Cuntai Guan)</author>
      <guid isPermaLink="false">2504.19596v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation</title>
      <link>http://arxiv.org/abs/2504.18878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为时间序列表示模型（TSRM）的时序特征编码架构，用于多变量时间序列的预测和插补。&lt;h4&gt;背景&lt;/h4&gt;在多变量时间序列预测和插补领域，现有方法存在复杂度高的问题。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够高效学习时间序列模式并进行预测和插补的架构。&lt;h4&gt;方法&lt;/h4&gt;架构基于CNN的表示层，每个层负责独立的学习任务，以捕获多样化的时间模式。之后通过注意力机制提取特征，并使用合并层聚合提取的特征。架构灵感来源于Transformer编码器，核心机制为自注意力。&lt;h4&gt;主要发现&lt;/h4&gt;TSRM在大多数七个已建立的基准数据集上优于现有方法，同时显著减少了可学习参数的数量。&lt;h4&gt;结论&lt;/h4&gt;TSRM是一种高效且性能优越的时间序列预测和插补模型。&lt;h4&gt;翻译&lt;/h4&gt;We introduce a temporal feature encoding architecture called Time SeriesRepresentation Model (TSRM) for multivariate time series forecasting andimputation. The architecture is structured around CNN-based representationlayers, each dedicated to an independent representation learning task anddesigned to capture diverse temporal patterns, followed by an attention-basedfeature extraction layer and a merge layer, designed to aggregate extractedfeatures. The architecture is fundamentally based on a configuration that isinspired by a Transformer encoder, with self-attention mechanisms at its core.The TSRM architecture outperforms state-of-the-art approaches on most of theseven established benchmark datasets considered in our empirical evaluation forboth forecasting and imputation tasks. At the same time, it significantlyreduces complexity in the form of learnable parameters. The source code isavailable at https://github.com/RobertLeppich/TSRM.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a temporal feature encoding architecture called Time SeriesRepresentation Model (TSRM) for multivariate time series forecasting andimputation. The architecture is structured around CNN-based representationlayers, each dedicated to an independent representation learning task anddesigned to capture diverse temporal patterns, followed by an attention-basedfeature extraction layer and a merge layer, designed to aggregate extractedfeatures. The architecture is fundamentally based on a configuration that isinspired by a Transformer encoder, with self-attention mechanisms at its core.The TSRM architecture outperforms state-of-the-art approaches on most of theseven established benchmark datasets considered in our empirical evaluation forboth forecasting and imputation tasks. At the same time, it significantlyreduces complexity in the form of learnable parameters. The source code isavailable at https://github.com/RobertLeppich/TSRM.</description>
      <author>example@mail.com (Robert Leppich, Michael Stenger, Daniel Grillmeyer, Vanessa Borst, Samuel Kounev)</author>
      <guid isPermaLink="false">2504.18878v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Kinship Verification through a Forest Neural Network</title>
      <link>http://arxiv.org/abs/2504.18910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的概念的新方法，用于人脸表征在血缘关系验证中的应用，该方法在准确性上与从头开始学习的父母和子女人脸图像的联合表征算法相当。&lt;h4&gt;背景&lt;/h4&gt;早期方法在血缘关系验证中使用人脸表征，但这些表征的准确性不如从头开始学习的父母和子女人脸图像的联合表征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，以改善血缘关系验证中人脸表征的准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种结合图神经网络概念的算法，并设计了分类模块的结构，引入了一种新的损失组合，以在训练网络时逐渐引入中心损失。&lt;h4&gt;主要发现&lt;/h4&gt;在KinFaceW-I和II数据集上进行了实验，证明了该方法的有效性，并在KinFaceW-II上取得了最佳结果，对所有血缘类型平均提高了近1.6，在KinFaceW-I上接近最佳。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在血缘关系验证中取得了显著的性能提升，并提供了可用的代码实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early methods used face representations in kinship verification, which areless accurate than joint representations of parents' and children's facialimages learned from scratch. We propose an approach featuring graph neuralnetwork concepts to utilize face representations and have comparable results tojoint representation algorithms. Moreover, we designed the structure of theclassification module and introduced a new combination of losses to engage thecenter loss gradually in training our network. Additionally, we conductedexperiments on KinFaceW-I and II, demonstrating the effectiveness of ourapproach. We achieved the best result on KinFaceW-II, an average improvement ofnearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. Thecode is available at https://github.com/ali-nazari/Kinship-Verification</description>
      <author>example@mail.com (Ali Nazari, Mohsen Ebrahimi Moghaddam, Omidreza Borzoei)</author>
      <guid isPermaLink="false">2504.18910v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance</title>
      <link>http://arxiv.org/abs/2504.18866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Transactions on Pattern Analysis and Machine  Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PiercingEye的新颖的弱监督视频暴力检测（VVD）方法，该方法结合了欧几里得和双曲几何，以增强特征表示的判别性。&lt;h4&gt;背景&lt;/h4&gt;现有的弱监督视频暴力检测方法主要依赖于欧几里得表示学习，但往往难以区分视觉相似但语义不同的事件，这是由于有限的层次建模和不足的模糊训练样本。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，提出了一种新的双空间学习框架。&lt;h4&gt;方法&lt;/h4&gt;PiercingEye引入了一种层敏感的双曲聚合策略，并使用双曲Dirichlet能量约束来逐步建模事件层次，同时引入了一种跨空间注意力机制，以促进欧几里得和双曲空间之间的互补特征交互。此外，为了缓解模糊样本的稀缺性，利用大型语言模型生成逻辑引导的模糊事件描述，并通过双曲视觉-语言对比损失实现显式监督，该损失通过动态相似度感知加权优先考虑高混淆样本。&lt;h4&gt;主要发现&lt;/h4&gt;在XD-Violence和UCF-Crime数据集上的大量实验表明，PiercingEye达到了最先进的性能，尤其是在新创建的模糊事件子集上，验证了其在细粒度暴力检测中的优越能力。&lt;h4&gt;结论&lt;/h4&gt;PiercingEye在视频暴力检测领域取得了显著的成果，为未来的研究提供了新的方向和方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现有的弱监督视频暴力检测方法主要依赖于欧几里得表示学习，这往往由于有限的层次建模和不足的模糊训练样本，难以区分视觉相似但语义不同的事件。为了解决这一挑战，我们提出了PiercingEye，一种新的双空间学习框架，它结合了欧几里得和双曲几何来增强判别性特征表示。具体来说，PiercingEye引入了一种层敏感的双曲聚合策略，并使用双曲Dirichlet能量约束来逐步建模事件层次，同时引入了一种跨空间注意力机制，以促进欧几里得和双曲空间之间的互补特征交互。此外，为了缓解模糊样本的稀缺性，我们利用大型语言模型生成逻辑引导的模糊事件描述，通过双曲视觉-语言对比损失实现显式监督，该损失通过动态相似度感知加权优先考虑高混淆样本。在XD-Violence和UCF-Crime数据集上的大量实验表明，PiercingEye达到了最先进的性能，尤其是在新创建的模糊事件子集上，验证了其在细粒度暴力检测中的优越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing weakly supervised video violence detection (VVD) methods primarilyrely on Euclidean representation learning, which often struggles to distinguishvisually similar yet semantically distinct events due to limited hierarchicalmodeling and insufficient ambiguous training samples. To address thischallenge, we propose PiercingEye, a novel dual-space learning framework thatsynergizes Euclidean and hyperbolic geometries to enhance discriminativefeature representation. Specifically, PiercingEye introduces a layer-sensitivehyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints toprogressively model event hierarchies, and a cross-space attention mechanism tofacilitate complementary feature interactions between Euclidean and hyperbolicspaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leveragelarge language models to generate logic-guided ambiguous event descriptions,enabling explicit supervision through a hyperbolic vision-language contrastiveloss that prioritizes high-confusion samples via dynamic similarity-awareweighting. Extensive experiments on XD-Violence and UCF-Crime benchmarksdemonstrate that PiercingEye achieves state-of-the-art performance, withparticularly strong results on a newly curated ambiguous event subset,validating its superior capability in fine-grained violence detection.</description>
      <author>example@mail.com (Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Mengjingcheng Mo, Jiankang Zheng, Qingqing Li, Ji Gan, Xinbo Gao)</author>
      <guid isPermaLink="false">2504.18866v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal graph representation learning for website generation based on visual sketch</title>
      <link>http://arxiv.org/abs/2504.18729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用多模态图表示学习的新方法，用于将数字设计转换为功能性源代码，以解决设计到代码转换中的复杂性和耗时问题。&lt;h4&gt;背景&lt;/h4&gt;设计到代码转换是一个在软件开发中具有挑战性的问题，传统方法在准确解析网页设计的复杂视觉细节和结构关系方面存在困难，导致自动化和效率受限。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过整合设计草图中的视觉和结构信息，提高代码生成的准确性和效率，特别是生成语义正确和结构良好的HTML代码。&lt;h4&gt;方法&lt;/h4&gt;采用多模态图表示学习，结合视觉和结构信息，以增强代码生成的准确性和效率。&lt;h4&gt;主要发现&lt;/h4&gt;对方法进行了全面评估，与现有技术相比，在准确性和效率方面均有显著提升，多模态图学习在现有技术中表现出显著的优势，突显了该方法在自动化设计到代码转换中的潜力。&lt;h4&gt;结论&lt;/h4&gt;该方法有望革新设计到代码的自动化过程，相关代码可在https://github.com/HySonLab/Design2Code获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Design2Code problem, which involves converting digital designs intofunctional source code, is a significant challenge in software development dueto its complexity and time-consuming nature. Traditional approaches oftenstruggle with accurately interpreting the intricate visual details andstructural relationships inherent in webpage designs, leading to limitations inautomation and efficiency. In this paper, we propose a novel method thatleverages multimodal graph representation learning to address these challenges.By integrating both visual and structural information from design sketches, ourapproach enhances the accuracy and efficiency of code generation, particularlyin producing semantically correct and structurally sound HTML code. We presenta comprehensive evaluation of our method, demonstrating significantimprovements in both accuracy and efficiency compared to existing techniques.Extensive evaluation demonstrates significant improvements of multimodal graphlearning over existing techniques, highlighting the potential of our method torevolutionize design-to-code automation. Code available athttps://github.com/HySonLab/Design2Code</description>
      <author>example@mail.com (Tung D. Vu, Chung Hoang, Truong-Son Hy)</author>
      <guid isPermaLink="false">2504.18729v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Faster and More Compact Foundation Models for Molecular Property Prediction</title>
      <link>http://arxiv.org/abs/2504.19538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了通过减少模型尺寸来提高分子性质预测中机器学习模型的效率，同时保持性能。&lt;h4&gt;背景&lt;/h4&gt;尽管机器学习在分子性质预测方面的准确性有所提高，但代价是更高的计算成本和更长的训练时间。&lt;h4&gt;目的&lt;/h4&gt;提高JMP模型在分子数据集上的微调效率，同时保持或提高性能。&lt;h4&gt;方法&lt;/h4&gt;分析了JMP模型的层贡献，通过剪枝预训练模型来探索模型压缩策略，并评估其对微调期间效率和准确性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;移除两个交互块可以最小化性能下降，将模型尺寸减少32%，同时将推理吞吐量提高1.3倍。&lt;h4&gt;结论&lt;/h4&gt;JMP-L模型存在过度参数化问题，一个更小、更高效的变体可以实现可比的性能，同时降低计算成本。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在分子性质预测的机器学习方面，尽管准确率有所提高，但代价是更高的计算成本和更长的训练时间。最近，联合多域预训练（JMP）基础模型在各种下游任务中表现出强大的性能，且训练时间低于之前的模型。尽管JMP具有优势，但将其在从小规模到大规模的分子数据集上进行微调需要相当多的时间和计算资源。在这项工作中，我们研究了通过减少模型尺寸来提高效率的策略，同时保持性能。为了更好地理解模型的效率，我们分析了JMP的层贡献，发现后续交互块提供的回报逐渐减少，这表明存在模型压缩的机会。我们通过剪枝预训练模型来探索块减少策略，并在微调期间评估其对效率和准确性的影响。我们的分析表明，移除两个交互块会导致最小的性能下降，将模型尺寸减少32%，同时将推理吞吐量提高1.3倍。这些结果表明，JMP-L模型过度参数化，一个更小、更高效的变体可以以更低的计算成本实现可比的性能。我们的研究为开发更轻量级、更快、更可扩展的基础模型提供了见解，用于分子和材料发现。代码可在以下链接公开获取：https://github.com/Yasir-Ghunaim/efficient-jmp。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in machine learning for molecular property prediction haveimproved accuracy but at the expense of higher computational cost and longertraining times. Recently, the Joint Multi-domain Pre-training (JMP) foundationmodel has demonstrated strong performance across various downstream tasks withreduced training time over previous models. Despite JMP's advantages,fine-tuning it on molecular datasets ranging from small-scale to large-scalerequires considerable time and computational resources. In this work, weinvestigate strategies to enhance efficiency by reducing model size whilepreserving performance. To better understand the model's efficiency, we analyzethe layer contributions of JMP and find that later interaction blocks providediminishing returns, suggesting an opportunity for model compression. Weexplore block reduction strategies by pruning the pre-trained model andevaluating its impact on efficiency and accuracy during fine-tuning. Ouranalysis reveals that removing two interaction blocks results in a minimalperformance drop, reducing the model size by 32% while increasing inferencethroughput by 1.3x. These results suggest that JMP-L is over-parameterized andthat a smaller, more efficient variant can achieve comparable performance withlower computational cost. Our study provides insights for developing lighter,faster, and more scalable foundation models for molecular and materialsdiscovery. The code is publicly available at:https://github.com/Yasir-Ghunaim/efficient-jmp.</description>
      <author>example@mail.com (Yasir Ghunaim, Andrés Villa, Gergo Ignacz, Gyorgy Szekely, Motasem Alfarra, Bernard Ghanem)</author>
      <guid isPermaLink="false">2504.19538v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations</title>
      <link>http://arxiv.org/abs/2504.18591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了基于神经场的 enf2enf 方法，用于预测具有非参数化几何变异性稳态偏微分方程的解。&lt;h4&gt;背景&lt;/h4&gt;神经场在处理偏微分方程解的近似学习方面取得了进展，能够处理一般几何形状。&lt;h4&gt;目的&lt;/h4&gt;提出 enf2enf 方法，以预测具有非参数化几何变异性稳态偏微分方程的解。&lt;h4&gt;方法&lt;/h4&gt;enf2enf 方法通过将输入几何编码为潜在点云嵌入，保留几何基础并捕捉局部现象，然后将这些表示与全局参数结合，直接解码为连续输出场，从而有效地模拟几何与物理之间的耦合。&lt;h4&gt;主要发现&lt;/h4&gt;该方法利用局部性和平移不变性的归纳偏置，能够捕捉精细的物理特征和复杂的形状变化，从而提高泛化能力和物理合规性。&lt;h4&gt;结论&lt;/h4&gt;在高质量空气动力学数据集、超弹性材料基准和多元素翼型几何形状上的实验表明，所提出的模型与最先进的基于图、操作学习和神经场的方法相比，实现了优越或具有竞争力的性能。该方法支持实时推理和零样本超分辨率，能够在低分辨率网格上高效训练，同时在全尺度离散化上保持高精度。&lt;h4&gt;翻译&lt;/h4&gt;摘要：最近在神经场方面的进展使得学习神经算子，该算子可以近似求解一般几何形状上的偏微分方程（PDEs）的解，成为了一种强大的、离散化不变的方法。基于这些进展，我们介绍了 enf2enf，这是一种编码器-解码器方法，用于预测具有非参数化几何变异性稳态偏微分方程。在 enf2enf 中，输入几何被编码为保留几何基础并捕捉局部现象的潜在点云嵌入。然后，这些表示与全局参数结合，直接解码为连续输出场，从而有效地模拟几何与物理之间的耦合。通过利用局部性和平移不变性的归纳偏置，我们的方法能够捕捉精细的物理特征以及复杂的形状变化，从而增强泛化能力和物理合规性。在高质量空气动力学数据集、超弹性材料基准和多元素翼型几何形状上的广泛实验表明，与最先进的基于图、操作学习和神经场的方法相比，所提出的模型实现了优越或具有竞争力的性能。值得注意的是，我们的方法支持实时推理和零样本超分辨率，能够在低分辨率网格上高效训练，同时在全尺度离散化上保持高精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Neural Fields have enabled powerful,discretization-invariant methods for learning neural operators that approximatesolutions of Partial Differential Equations (PDEs) on general geometries.Building on these developments, we introduce enf2enf, an encoder--decodermethodology for predicting steady-state Partial Differential Equations withnon-parameterized geometric variability, based on recently proposed EquivariantNeural Field architectures. In enf2enf, input geometries are encoded intolatent point cloud embeddings that inherently preserve geometric grounding andcapture local phenomena. The resulting representations are then combined withglobal parameters and directly decoded into continuous output fields, thusefficiently modeling the coupling between geometry and physics. By leveragingthe inductive biases of locality and translation invariance, our approach isable to capture fine-scale physical features as well as complex shapevariations, thereby enhancing generalization and physical compliance. Extensiveexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic materialbenchmark, and multi-element airfoil geometries, demonstrate that the proposedmodel achieves superior or competitive performance compared to state-of-the-artgraph based, operator learning, and neural field methods. Notably, our methodsupports real time inference and zero-shot super-resolution, enabling efficienttraining on low-resolution meshes while maintaining high accuracy on full-scalediscretizations.</description>
      <author>example@mail.com (Giovanni Catalani, Michael Bauerheim, Frédéric Tost, Xavier Bertrand, Joseph Morlier)</author>
      <guid isPermaLink="false">2504.18591v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>High-order Graph Neural Networks with Common Neighbor Awareness for Link Prediction</title>
      <link>http://arxiv.org/abs/2504.18758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted By ICAIS&amp;ISAS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于高阶图神经网络（HGNN-CNA）的动态图学习中的链接预测方法，通过考虑多跳共同邻居来捕捉节点间的复杂交互，并直接在动态图学习（DGL）中融合这种交互，从而显著提高了链接预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;链接预测是动态图学习中的一个基本任务，动态图神经网络的进步主要通过对节点间关系进行消息传递来建模，但它们依赖于成对节点交互，忽略了DGL中的共同邻居交互。&lt;h4&gt;目的&lt;/h4&gt;提出HGNN-CNA以解决DGL中共同邻居交互被忽略的问题，从而提高链接预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;HGNN-CNA通过以下两种方法实现：a) 通过考虑多跳共同邻居来估计相关性分数，以捕捉节点间的复杂交互；b) 将这种相关性融合到消息传递过程中，直接在DGL中考虑共同邻居交互。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实动态图上的实验结果表明，与几个最先进的模型相比，所提出的HGNN-CNA在链接预测任务上取得了显著的准确性提升。&lt;h4&gt;结论&lt;/h4&gt;HGNN-CNA通过直接考虑DGL中的共同邻居交互，显著提高了链接预测的准确性，为动态图学习中的链接预测提供了一种新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;链接预测是动态图学习中的基础任务，它被动态图（DG）的拓扑结构所塑造。动态图神经网络（DGNN）的最近进展，主要通过消息传递方案建模节点之间的关系，已经显著提高了链接预测的性能。然而，DGNNs高度依赖于成对节点交互，这忽略了DGL中的共同邻居交互。为了解决这一局限性，我们提出了一种具有共同邻居感知能力的高阶图神经网络（HGNN-CNA）用于链接预测，具有两个方面的想法：a）通过考虑多跳共同邻居来估计相关性分数，以捕捉节点之间的复杂交互；b）将相关性融合到消息传递过程中，直接在DGL中考虑共同邻居交互。在三个真实动态图上的实验结果表明，所提出的HGNN-CNA在链接预测任务上相对于几个最先进的模型取得了显著的准确性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is a fundamental task in dynamic graph learning (DGL),inherently shaped by the topology of the DG. Recent advancements in dynamicgraph neural networks (DGNN), primarily by modeling the relationships amongnodes via a message passing scheme, have significantly improved link predictionperformance. However, DGNNs heavily rely on the pairwise node interactions,which neglect the common neighbor interaction in DGL. To address thislimitation, we propose a High-order Graph Neural Networks with Common NeighborAwareness (HGNN-CNA) for link prediction with two-fold ideas: a) estimatingcorrelation score by considering multi-hop common neighbors for capturing thecomplex interaction between nodes; b) fusing the correlation into themessage-passing process to consider common neighbor interaction directly inDGL. Experimental results on three real DGs demonstrate that the proposedHGNN-CNA acquires a significant accuracy gain over several state-of-the-artmodels on the link prediction task.</description>
      <author>example@mail.com (Ling Wang, Minglian Han)</author>
      <guid isPermaLink="false">2504.18758v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>LLMs for Engineering: Teaching Models to Design High Powered Rockets</title>
      <link>http://arxiv.org/abs/2504.19394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文评估了大型语言模型（LLMs）在火箭设计中的应用能力，发现RL训练的LLMs在复杂工程优化中具有潜力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）已经改变了软件工程，但其在物理工程领域的应用仍不充分。&lt;h4&gt;目的&lt;/h4&gt;评估LLMs在火箭设计中的能力。&lt;h4&gt;方法&lt;/h4&gt;通过RocketBench基准测试连接LLMs与高保真火箭模拟，测试模型在两个设计任务上的表现：目标高度优化和精度着陆挑战。&lt;h4&gt;主要发现&lt;/h4&gt;最先进的LLMs显示出强大的工程基础知识，但在给定模拟结果后难以迭代设计，最终性能低于人类水平。然而，通过强化学习（RL）增强后，一个7B参数的模型优于最先进的基础模型和人类专家。&lt;h4&gt;结论&lt;/h4&gt;RL训练的LLMs可以成为复杂工程优化的有效工具，有可能改变软件以外的工程领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have transformed software engineering, but theirapplication to physical engineering domains remains underexplored. This paperevaluates LLMs' capabilities in high-powered rocketry design throughRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.We test models on two increasingly complex design tasks: target altitudeoptimization and precision landing challenges. Our findings reveal that whilestate-of-the-art LLMs demonstrate strong baseline engineering knowledge, theystruggle to iterate on their designs when given simulation results andultimately plateau below human performance levels. However, when enhanced withreinforcement learning (RL), we show that a 7B parameter model outperforms bothSoTA foundation models and human experts. This research demonstrates thatRL-trained LLMs can serve as effective tools for complex engineeringoptimization, potentially transforming engineering domains beyond softwaredevelopment.</description>
      <author>example@mail.com (Toby Simonds)</author>
      <guid isPermaLink="false">2504.19394v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Deep-Learning Based Potentially Hazardous Asteroids Classification Using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2504.18605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的近地小行星（PHA）分类方法，用于行星防御和深空导航。&lt;h4&gt;背景&lt;/h4&gt;传统方法在近地小行星分类中往往忽略了小行星之间的动态关系。&lt;h4&gt;目的&lt;/h4&gt;通过模型识别具有潜在危害的小行星。&lt;h4&gt;方法&lt;/h4&gt;将小行星作为节点，利用轨道和物理特征建模，节点之间通过表示相似性的边连接。使用NASA的958,524条记录的数据集进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;模型在数据集极度不平衡（仅0.22%为危险标签）的情况下，实现了99%的总体准确率和0.99的AUC。应用合成少数类过采样技术后，对于危险小行星的召回率为78%，F1分数为37%。特征重要性分析显示反照率、近日点和半长轴为主要预测因子。&lt;h4&gt;结论&lt;/h4&gt;该框架支持行星防御任务，并证实了人工智能在实现未来任务如NASA的近地天体调查员和ESA的拉姆塞斯自主导航方面的潜力，为小行星危害评估提供了一种可解释和可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Abstract: Classifying potentially hazardous asteroids (PHAs) is crucial for planetary defense and deep space navigation, yet traditional methods often overlook the dynamical relationships among asteroids. We introduce a Graph Neural Network (GNN) approach that models asteroids as nodes with orbital and physical features, connected by edges representing their similarities, using a NASA dataset of 958,524 records. Despite an extreme class imbalance with only 0.22% of the dataset with the hazardous label, our model achieves an overall accuracy of 99% and an AUC of 0.99, with a recall of 78% and an F1-score of 37% for hazardous asteroids after applying the Synthetic Minority Oversampling Technique. Feature importance analysis highlights albedo, perihelion distance, and semi-major axis as main predictors. This framework supports planetary defense missions and confirms AI's potential in enabling autonomous navigation for future missions such as NASA's NEO Surveyor and ESA's Ramses, offering an interpretable and scalable solution for asteroid hazard assessment.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classifying potentially hazardous asteroids (PHAs) is crucial for planetarydefense and deep space navigation, yet traditional methods often overlook thedynamical relationships among asteroids. We introduce a Graph Neural Network(GNN) approach that models asteroids as nodes with orbital and physicalfeatures, connected by edges representing their similarities, using a NASAdataset of 958,524 records. Despite an extreme class imbalance with only 0.22%of the dataset with the hazardous label, our model achieves an overall accuracyof 99% and an AUC of 0.99, with a recall of 78% and an F1-score of 37% forhazardous asteroids after applying the Synthetic Minority OversamplingTechnique. Feature importance analysis highlights albedo, perihelion distance,and semi-major axis as main predictors. This framework supports planetarydefense missions and confirms AI's potential in enabling autonomous navigationfor future missions such as NASA's NEO Surveyor and ESA's Ramses, offering aninterpretable and scalable solution for asteroid hazard assessment.</description>
      <author>example@mail.com (Baimam Boukar Jean Jacques)</author>
      <guid isPermaLink="false">2504.18605v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Anyprefer: An Agentic Framework for Preference Data Synthesis</title>
      <link>http://arxiv.org/abs/2504.19276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Anyprefer的框架，旨在合成高质量偏好数据以对齐目标模型，并通过实验证明其能显著提升模型在不同应用中的对齐性能。&lt;h4&gt;背景&lt;/h4&gt;高质量偏好数据对于通过偏好学习使基础模型与人类价值观保持一致至关重要，但手动标注此类数据耗时且成本高。&lt;h4&gt;目的&lt;/h4&gt;提出Anyprefer框架，以合成高质量偏好数据，解决手动标注数据耗时且成本高的问题，并减少由于奖励模型与目标模型共享权重而导致的偏差。&lt;h4&gt;方法&lt;/h4&gt;Anyprefer将数据合成过程视为一个合作的双玩家马尔可夫博弈，其中目标模型和裁判模型协作。引入一系列外部工具协助裁判模型准确奖励目标模型的响应，并引入反馈机制优化模型的提示，以增强合作并提高数据质量。&lt;h4&gt;主要发现&lt;/h4&gt;Anyprefer-V1数据集包含58K高质量偏好对，实验表明Anyprefer在四个主要应用中显著提升了模型的对齐性能，平均提升18.55%在五个自然语言生成数据集，3.66%在九个视觉-语言理解数据集，30.05%在三个医学图像分析数据集，以及16.00%在四个视觉-运动控制任务中。&lt;h4&gt;结论&lt;/h4&gt;Anyprefer框架能够有效合成高质量偏好数据，并显著提升模型在不同应用中的对齐性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality preference data is essential for aligning foundation models withhuman values through preference learning. However, manual annotation of suchdata is often time-consuming and costly. Recent methods often adopt aself-rewarding approach, where the target model generates and annotates its ownpreference data, but this can lead to inaccuracies since the reward modelshares weights with the target model, thereby amplifying inherent biases. Toaddress these issues, we propose Anyprefer, a framework designed to synthesizehigh-quality preference data for aligning the target model. Anyprefer framesthe data synthesis process as a cooperative two-player Markov Game, where thetarget model and the judge model collaborate together. Here, a series ofexternal tools are introduced to assist the judge model in accurately rewardingthe target model's responses, mitigating biases in the rewarding process. Inaddition, a feedback mechanism is introduced to optimize prompts for bothmodels, enhancing collaboration and improving data quality. The synthesizeddata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58Khigh-quality preference pairs. Extensive experiments show that Anyprefersignificantly improves model alignment performance across four mainapplications, covering 21 datasets, achieving average improvements of 18.55% infive natural language generation datasets, 3.66% in nine vision-languageunderstanding datasets, 30.05% in three medical image analysis datasets, and16.00% in four visuo-motor control tasks.</description>
      <author>example@mail.com (Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao)</author>
      <guid isPermaLink="false">2504.19276v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks</title>
      <link>http://arxiv.org/abs/2504.19274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted to the Privacy Enhancing Technologies  Symposium (PETS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TeleSparse是一种ZK-friendly后处理机制，用于解决将ZK-SNARKs应用于现代神经网络时的计算开销问题。&lt;h4&gt;背景&lt;/h4&gt;验证深度学习推理的完整性对于确保模型应用正确至关重要，但通常需要访问模型权重和训练数据，这可能导致敏感信息泄露。&lt;h4&gt;目的&lt;/h4&gt;提出TeleSparse以解决将ZK-SNARKs应用于现代神经网络时的计算开销问题。&lt;h4&gt;方法&lt;/h4&gt;TeleSparse通过两种方式解决挑战：减少电路约束和最小化查找表的大小。具体包括：1）通过稀疏化神经网络模型来减少约束；2）通过神经遥传来优化激活范围。&lt;h4&gt;主要发现&lt;/h4&gt;TeleSparse在相同模型上减少了证明生成时间46%，内存使用减少了67%，精度损失约为1%。&lt;h4&gt;结论&lt;/h4&gt;TeleSparse为ZK-friendly模型设计开辟了新方向，推动了可扩展、资源高效的验证深度学习。&lt;h4&gt;翻译&lt;/h4&gt;摘要：验证深度学习推理的完整性对于理解模型是否被正确应用至关重要。然而，这种验证通常需要访问模型权重和（可能敏感或私有的）训练数据。所谓的零知识简明非交互式知识证明（ZK-SNARKs）似乎提供了在不访问此类敏感数据的情况下验证模型推理的能力。然而，将ZK-SNARKs应用于现代神经网络（如transformers和大型视觉模型）会引入显著的计算开销。我们提出了TeleSparse，这是一种ZK-friendly的后处理机制，用于提供实际的解决方案。TeleSparse解决了将ZK-SNARKs应用于现代神经网络时固有的两个基本挑战：（1）减少电路约束：过参数化的模型导致ZK-SNARK验证的约束数量众多，从而增加了内存和证明生成成本。我们通过应用稀疏化来解决这一问题，在不损害精度或安全性的情况下提高了证明效率。（2）通过优化激活范围，最小化非线性函数所需的查找表大小，这是一种新颖的神经网络遥传方法，用于缩小激活函数的范围。TeleSparse在相同模型上减少了证明生成时间46%，内存使用减少了67%，精度损失约为1%。我们使用Halo2证明系统实现了我们的框架，并在多个架构（Vision-transformer、ResNet、MobileNet）和数据集（ImageNet、CIFAR-10、CIFAR-100）上证明了其有效性。这项工作为ZK-friendly模型设计开辟了新方向，推动了可扩展、资源高效的验证深度学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Verification of the integrity of deep learning inference is crucial forunderstanding whether a model is being applied correctly. However, suchverification typically requires access to model weights and (potentiallysensitive or private) training data. So-called Zero-knowledge SuccinctNon-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide thecapability to verify model inference without access to such sensitive data.However, applying ZK-SNARKs to modern neural networks, such as transformers andlarge vision models, introduces significant computational overhead.  We present TeleSparse, a ZK-friendly post-processing mechanisms to producepractical solutions to this problem. TeleSparse tackles two fundamentalchallenges inherent in applying ZK-SNARKs to modern neural networks: (1)Reducing circuit constraints: Over-parameterized models result in numerousconstraints for ZK-SNARK verification, driving up memory and proof generationcosts. We address this by applying sparsification to neural network models,enhancing proof efficiency without compromising accuracy or security. (2)Minimizing the size of lookup tables required for non-linear functions, byoptimizing activation ranges through neural teleportation, a novel adaptationfor narrowing activation functions' range.  TeleSparse reduces prover memory usage by 67% and proof generation time by46% on the same model, with an accuracy trade-off of approximately 1%. Weimplement our framework using the Halo2 proving system and demonstrate itseffectiveness across multiple architectures (Vision-transformer, ResNet,MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens newdirections for ZK-friendly model design, moving toward scalable,resource-efficient verifiable deep learning.</description>
      <author>example@mail.com (Mohammad M Maheri, Hamed Haddadi, Alex Davidson)</author>
      <guid isPermaLink="false">2504.19274v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Speaker Diarization for Low-Resource Languages Through Wav2vec Fine-Tuning</title>
      <link>http://arxiv.org/abs/2504.18582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对低资源语言如库尔德语的说话人分割问题进行了研究，提出了一种基于Wav2Vec 2.0模型的方法，通过迁移学习提高了低资源语言的说话人分割性能。&lt;h4&gt;背景&lt;/h4&gt;说话人分割是语音处理中的基础任务，但在低资源语言如库尔德语中由于数据有限、方言多样和代码转换频繁，存在独特挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在解决低资源语言库尔德语的说话人分割问题，提高其分割性能。&lt;h4&gt;方法&lt;/h4&gt;研究者通过在专门为库尔德语准备的语料库上训练Wav2Vec 2.0自监督学习模型，并利用迁移学习，将其他语言学到的多语言表示应用于库尔德语语音的语音学和声学特征。&lt;h4&gt;主要发现&lt;/h4&gt;与基线方法相比，该方法将分割错误率降低了7.2%，并将聚类纯度提高了13%。&lt;h4&gt;结论&lt;/h4&gt;这些发现表明，对现有模型的改进可以显著提高低资源语言的说话人分割性能，为其他研究不足的语言构建有效的说话人分割系统奠定了基础，有助于提高语音技术的公平性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：说话人分割是语音处理中的基本任务，它涉及将音频流分割成说话人。尽管最先进的模型在高资源语言上取得了先进的性能，但像库尔德语这样的低资源语言由于有限的注释数据、多种方言和频繁的代码转换而提出了独特的挑战。在本研究中，我们通过在专门的库尔德语语料库上训练Wav2Vec 2.0自监督学习模型来解决这些问题。通过迁移学习，我们将从其他语言学习到的多语言表示调整为捕获库尔德语语音的语音学和声学特征。与基线方法相比，我们的方法将分割错误率降低了7.2%，并将聚类纯度提高了13%。这些发现表明，对现有模型的改进可以显著提高低资源语言的分割性能。我们的工作对开发库尔德语媒体转录服务以及在多语言呼叫中心、电话会议和视频会议系统中的说话人分割具有实际意义。这些结果为构建其他研究不足的语言的有效分割系统奠定了基础，有助于提高语音技术的公平性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speaker diarization is a fundamental task in speech processing that involvesdividing an audio stream by speaker. Although state-of-the-art models haveadvanced performance in high-resource languages, low-resource languages such asKurdish pose unique challenges due to limited annotated data, multiple dialectsand frequent code-switching. In this study, we address these issues by trainingthe Wav2Vec 2.0 self-supervised learning model on a dedicated Kurdish corpus.By leveraging transfer learning, we adapted multilingual representationslearned from other languages to capture the phonetic and acousticcharacteristics of Kurdish speech. Relative to a baseline method, our approachreduced the diarization error rate by seven point two percent and improvedcluster purity by thirteen percent. These findings demonstrate thatenhancements to existing models can significantly improve diarizationperformance for under-resourced languages. Our work has practical implicationsfor developing transcription services for Kurdish-language media and forspeaker segmentation in multilingual call centers, teleconferencing andvideo-conferencing systems. The results establish a foundation for buildingeffective diarization systems in other understudied languages, contributing togreater equity in speech technology.</description>
      <author>example@mail.com (Abdulhady Abas Abdullah, Sarkhel H. Taher Karim, Sara Azad Ahmed, Kanar R. Tariq, Tarik A. Rashid)</author>
      <guid isPermaLink="false">2504.18582v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:01 +0800</pubDate>
    </item>
    <item>
      <title>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</title>
      <link>http://arxiv.org/abs/2504.19056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub  Repository:  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了生成式人工智能在艺术、游戏和动画领域的应用，特别是动画内容生产的成本和时间减少。文章全面分析了面部动画、表情渲染、图像合成、角色创建、手势建模、运动合成、物体生成和纹理合成等领域的最新进展。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能在动画领域的应用越来越广泛，但现有综述往往缺乏综合性，未能全面涵盖所有相关技术。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提供一个综合性的视角，全面概述生成式人工智能在角色动画中的应用，并帮助新进入该领域的研究者和开发者。&lt;h4&gt;方法&lt;/h4&gt;文章首先回顾了面部动画、表情渲染等领域的最新研究，然后讨论了实际应用、常用数据集和新兴趋势。此外，还提供了基础模型和评估指标的相关背景介绍。&lt;h4&gt;主要发现&lt;/h4&gt;近年来，生成式人工智能在动画领域的应用取得了显著进展，包括面部动画、表情渲染、图像合成等方面的技术革新。&lt;h4&gt;结论&lt;/h4&gt;本文为生成式人工智能动画领域的研究者和开发者提供了一个资源，并指出了未来研究方向，以推动人工智能驱动的角色动画技术发展。&lt;h4&gt;翻译&lt;/h4&gt;Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, imagesynthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative AI is reshaping art, gaming, and most notably animation. Recentbreakthroughs in foundation and diffusion models have reduced the time and costof producing animated content. Characters are central animation components,involving motion, emotions, gestures, and facial expressions. The pace andbreadth of advances in recent months make it difficult to maintain a coherentview of the field, motivating the need for an integrative review. Unlikeearlier overviews that treat avatars, gestures, or facial animation inisolation, this survey offers a single, comprehensive perspective on all themain generative AI applications for character animation. We begin by examiningthe state-of-the-art in facial animation, expression rendering, imagesynthesis, avatar creation, gesture modeling, motion synthesis, objectgeneration, and texture synthesis. We highlight leading research, practicaldeployments, commonly used datasets, and emerging trends for each area. Tosupport newcomers, we also provide a comprehensive background section thatintroduces foundational models and evaluation metrics, equipping readers withthe knowledge needed to enter the field. We discuss open challenges and mapfuture research directions, providing a roadmap to advance AI-drivencharacter-animation technologies. This survey is intended as a resource forresearchers and developers entering the field of generative AI animation oradjacent fields. Resources are available at:https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.</description>
      <author>example@mail.com (Mohammad Mahdi Abootorabi, Omid Ghahroodi, Pardis Sadat Zahraei, Hossein Behzadasl, Alireza Mirrokni, Mobina Salimipanah, Arash Rasouli, Bahar Behzadipour, Sara Azarnoush, Benyamin Maleki, Erfan Sadraiye, Kiarash Kiani Feriz, Mahdi Teymouri Nahad, Ali Moghadasi, Abolfazl Eshagh Abianeh, Nizi Nazar, Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Meisam Ahmadi, Ehsaneddin Asgari)</author>
      <guid isPermaLink="false">2504.19056v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:02 +0800</pubDate>
    </item>
    <item>
      <title>PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data</title>
      <link>http://arxiv.org/abs/2504.18770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 13 figures, Published at ICLR 2025 - Machine Learning for  Remote Sensing (ML4RS) Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为PyViT-FUSE的基础模型，专门设计用于处理多模态图像，通过注意力机制将任意数量的混合分辨率输入波段融合成一个单一表示。&lt;h4&gt;背景&lt;/h4&gt;针对地球观测数据的多模态图像处理需求。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够融合多模态图像的基础模型。&lt;h4&gt;方法&lt;/h4&gt;使用注意力机制融合不同分辨率的输入波段，并通过具有新颖金字塔结构的视觉Transformer对融合后的块状标记进行处理。采用SwAV算法的核心概念进行自监督训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过可视化注意力分数展示了融合机制的可解释性，并证明了模型在下游任务中的应用能力。&lt;h4&gt;结论&lt;/h4&gt;PyViT-FUSE模型在地球观测数据的多模态图像处理方面具有良好的表现。&lt;h4&gt;翻译&lt;/h4&gt;We propose PyViT-FUSE, a foundation model for earth observation data explicitly designed to handle multi-modal imagery by learning to fuse an arbitrary number of mixed-resolution input bands into a single representation through an attention mechanism. The learned patch tokens are further processed by a stack of vision transformers with a novel pyramidal structure. We train the model on a globally sampled dataset in a self-supervised manner, leveraging core concepts of the SwAV algorithm. We show the interpretability of the fusion mechanism by visualization of the attention scores and the models applicability to downstream tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose PyViT-FUSE, a foundation model for earth observation dataexplicitly designed to handle multi-modal imagery by learning to fuse anarbitrary number of mixed-resolution input bands into a single representationthrough an attention mechanism. The learned patch tokens are further processedby a stack of vision transformers with a novel pyramidal structure. We trainthe model on a globally sampled dataset in a self-supervised manner, leveragingcore concepts of the SwAV algorithm. We show the interpretability of the fusionmechanism by visualization of the attention scores and the models applicabilityto downstream tasks.</description>
      <author>example@mail.com (Manuel Weber, Carly Beneke)</author>
      <guid isPermaLink="false">2504.18770v1</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:02 +0800</pubDate>
    </item>
    <item>
      <title>EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception</title>
      <link>http://arxiv.org/abs/2504.16616v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EHGCN的新方法，用于在欧几里得和双曲空间中感知事件流，通过自适应采样策略、运动感知超边生成方法和欧几里得-双曲GCN融合，实现了混合事件感知。&lt;h4&gt;背景&lt;/h4&gt;事件相机具有微秒级时间分辨率和高清特性，尽管基于图神经网络（GNN）的感知方法有进展，但它们在纯欧几里得空间中使用简单的成对连接机制，难以捕捉长距离依赖关系，也无法有效表征非均匀分布事件流的内在层次结构。&lt;h4&gt;目的&lt;/h4&gt;提出EHGCN方法，以改善事件流感知，尤其是在欧几里得和双曲空间中。&lt;h4&gt;方法&lt;/h4&gt;EHGCN中引入了自适应采样策略来动态调节采样率，通过保留判别性事件来降低噪声。此外，提出了基于运动状态转移概率的马尔可夫向量场（MVF）驱动的运动感知超边生成方法，以消除跨目标的虚假关联并提供拓扑先验，同时捕捉事件之间的长距离依赖关系。最后，提出了欧几里得-双曲GCN来融合在欧几里得和双曲空间中局部聚合和全局层次建模的信息。&lt;h4&gt;主要发现&lt;/h4&gt;EHGCN能够有效捕捉事件之间的长距离依赖关系，并通过融合欧几里得和双曲空间的信息实现混合事件感知。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，EHGCN在事件感知任务（如目标检测和识别）中是有效的。&lt;h4&gt;翻译&lt;/h4&gt;摘要：事件相机，具有微秒级时间分辨率和高动态范围（HDR）特性，为感知任务提供高速事件流。尽管基于图神经网络（GNN）的感知方法最近有了进展，但它们容易使用简单的成对连接机制在纯欧几里得空间中，这使它们难以捕捉长距离依赖关系，并且无法有效表征非均匀分布事件流的固有层次结构。为此，本文提出了一种名为EHGCN的新方法，这是首次在欧几里得和双曲空间中感知事件流。在EHGCN中，我们引入了一种自适应采样策略来动态调节采样率，在保留判别性事件的同时降低噪声。然后，我们提出了一种基于运动状态转移概率的马尔可夫向量场（MVF）驱动的运动感知超边生成方法，从而消除跨目标的虚假关联并提供关键拓扑先验，同时捕捉事件之间的长距离依赖关系。最后，我们提出了一个欧几里得-双曲GCN，以融合在欧几里得和双曲空间中分别局部聚合和全局层次建模的信息，以实现混合事件感知。在事件感知任务（如目标检测和识别）上的实验结果验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-04-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras, with microsecond temporal resolution and high dynamic range(HDR) characteristics, emit high-speed event stream for perception tasks.Despite the recent advancement in GNN-based perception methods, they are proneto use straightforward pairwise connectivity mechanisms in the pure Euclideanspace where they struggle to capture long-range dependencies and fail toeffectively characterize the inherent hierarchical structures of non-uniformlydistributed event stream. To this end, in this paper we propose a novelapproach named EHGCN, which is a pioneer to perceive event stream in bothEuclidean and hyperbolic spaces for event vision. In EHGCN, we introduce anadaptive sampling strategy to dynamically regulate sampling rates, retainingdiscriminative events while attenuating chaotic noise. Then we present a MarkovVector Field (MVF)-driven motion-aware hyperedge generation method based onmotion state transition probabilities, thereby eliminating cross-targetspurious associations and providing critically topological priors whilecapturing long-range dependencies between events. Finally, we propose aEuclidean-Hyperbolic GCN to fuse the information locally aggregated andglobally hierarchically modeled in Euclidean and hyperbolic spaces,respectively, to achieve hybrid event perception. Experimental results on eventperception tasks such as object detection and recognition validate theeffectiveness of our approach.</description>
      <author>example@mail.com (Haosheng Chen, Lian Luo, Mengjingcheng Mo, Zhanjie Wu, Guobao Xiao, Ji Gan, Jiaxu Leng, Xinbo Gao)</author>
      <guid isPermaLink="false">2504.16616v2</guid>
      <pubDate>Tue, 29 Apr 2025 14:21:02 +0800</pubDate>
    </item>
    </channel>
</rss>