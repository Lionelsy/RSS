<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 27 Oct 2025 14:45:35 +0800</lastBuildDate>
    <item>
      <title>Modest-Align: Data-Efficient Alignment for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Modest-Align是一个轻量级跨模态对齐框架，通过随机扰动和嵌入平滑两种策略解决资源受限场景下的过度自信问题，在保持高性能的同时大幅减少训练数据和计算资源需求。&lt;h4&gt;背景&lt;/h4&gt;跨模态对齐旨在将异构模态映射到共享潜在空间，CLIP等模型通过大规模预训练获得强大识别能力，但在资源受限、数据有限或质量低的情况下，这些模型常因模糊或弱相关的图像-文本对而出现过度自信和性能下降。&lt;h4&gt;目的&lt;/h4&gt;设计一个轻量级的对齐框架，提高在资源受限场景下的鲁棒性和效率，解决模型的过度自信问题。&lt;h4&gt;方法&lt;/h4&gt;提出Modest-Align框架，采用两种互补策略：随机扰动引入受控噪声模拟不确定性，嵌入平滑校准嵌入空间中的相似度分布，共同减少过度自信并提高对噪声或弱对齐样本的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验表明，Modest-Align在检索任务中优于最先进方法，使用超过100倍少的训练数据和600倍少的GPU时间达到与CLIP竞争的结果。&lt;h4&gt;结论&lt;/h4&gt;Modest-Align为现实世界中资源受限的跨模态对齐问题提供了实用且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;跨模态对齐旨在将异构模态映射到共享的潜在空间，如CLIP等模型所示，这些模型通过大规模图像-文本预训练获得强大的识别能力。然而，在资源受限、数据有限或质量低的环境中，由于模糊或弱相关的图像-文本对普遍存在，这些模型常常过度自信且性能下降。当前依赖单一正样本对的对比学习方法进一步加剧了这一问题，通过强化对不确定样本的过度自信。为应对这些挑战，我们提出了Modest-Align，一个为鲁棒性和效率而设计的轻量级对齐框架。我们的方法采用两种互补策略——随机扰动，引入受控噪声来模拟不确定性；以及嵌入平滑，校准嵌入空间中的相似度分布。这些机制共同减少过度自信并提高对噪声或弱对齐样本的性能。在多个基准数据集上的广泛实验表明，Modest-Align在检索任务中优于最先进方法，使用超过100倍少的训练数据和600倍少的GPU时间达到与CLIP竞争的结果。我们的方法为现实世界中资源受限的跨模态对齐问题提供了实用且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-modal alignment aims to map heterogeneous modalities into a sharedlatent space, as exemplified by models like CLIP, which benefit fromlarge-scale image-text pretraining for strong recognition capabilities.However, when operating in resource-constrained settings with limited orlow-quality data, these models often suffer from overconfidence and degradedperformance due to the prevalence of ambiguous or weakly correlated image-textpairs. Current contrastive learning approaches, which rely on single positivepairs, further exacerbate this issue by reinforcing overconfidence on uncertainsamples. To address these challenges, we propose Modest-Align, a lightweightalignment framework designed for robustness and efficiency. Our approachleverages two complementary strategies -- Random Perturbation, which introducescontrolled noise to simulate uncertainty, and Embedding Smoothing, whichcalibrates similarity distributions in the embedding space. These mechanismscollectively reduce overconfidence and improve performance on noisy or weaklyaligned samples. Extensive experiments across multiple benchmark datasetsdemonstrate that Modest-Align outperforms state-of-the-art methods in retrievaltasks, achieving competitive results with over 100x less training data and 600xless GPU time than CLIP. Our method offers a practical and scalable solutionfor cross-modal alignment in real-world, low-resource scenarios.</description>
      <author>example@mail.com (Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Mingkun Xu, Zuozhu Liu)</author>
      <guid isPermaLink="false">2510.21606v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
  <item>
      <title>Visual Diffusion Models are Geometric Solvers</title>
      <link>http://arxiv.org/abs/2510.21697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://kariander1.github.io/visual-geo-solver/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了视觉扩散模型可以作为有效的几何求解器，通过在像素空间工作直接解决几何问题。研究者将这种方法应用于三个著名的几何难题：内接正方形问题、斯坦纳树问题和简单多边形问题。&lt;h4&gt;背景&lt;/h4&gt;几何问题求解一直是挑战性问题，尤其是内接正方形问题（询问每个约旦曲线是否包含四个点形成正方形）等长期未解决的难题。前期工作需要专门的架构和领域特定的适应来将扩散应用于参数化几何表示。&lt;h4&gt;目的&lt;/h4&gt;展示视觉扩散模型作为几何求解器的有效性，探索在像素空间直接推理几何问题的可能性，并开发一种通用框架来近似解决著名的几何难题。&lt;h4&gt;方法&lt;/h4&gt;将每个问题实例视为图像，训练标准视觉扩散模型将高斯噪声转换为代表有效近似解的图像。模型学习将嘈杂的几何结构转换为正确配置，将几何推理重新表述为图像生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;视觉扩散模型能够有效解决内接正方形问题、斯坦纳树问题和简单多边形问题；模型能够将嘈杂的几何结构转换为正确配置；生成模型与几何问题解决之间存在联系；在图像空间操作提供了一种通用框架来近似解决著名难题。&lt;h4&gt;结论&lt;/h4&gt;视觉扩散模型可以作为有效的几何求解器，在图像空间操作为近似解决著名难题提供了通用且实用的框架，为解决更广泛的挑战性几何任务开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们展示了视觉扩散模型可以作为有效的几何求解器：它们通过在像素空间工作，能够直接推理几何问题。我们首先在内接正方形问题上证明了这一点，这是几何学中的一个长期未解决的问题，询问每个约旦曲线是否包含四个点形成正方形。然后我们将这种方法扩展到其他两个著名的难解几何问题：斯坦纳树问题和简单多边形问题。我们的方法将每个问题实例视为图像，并训练一个标准的视觉扩散模型，该模型将高斯噪声转换为代表有效近似解的图像，该解与精确解非常匹配。模型学习将嘈杂的几何结构转换为正确配置，有效地将几何推理重新表述为图像生成。与之前需要专门架构和领域特定适应的工作不同，我们使用在问题视觉表示上操作的标准视觉扩散模型。这种简单性突显了生成模型与几何问题解决之间令人惊讶的联系。除了这里研究的具体问题外，我们的结果指向一个更广泛的范式：在图像空间操作为近似解决著名难题提供了通用且实用的框架，并为解决更广泛的挑战性几何任务开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we show that visual diffusion models can serve as effectivegeometric solvers: they can directly reason about geometric problems by workingin pixel space. We first demonstrate this on the Inscribed Square Problem, along-standing problem in geometry that asks whether every Jordan curve containsfour points forming a square. We then extend the approach to two otherwell-known hard geometric problems: the Steiner Tree Problem and the SimplePolygon Problem.  Our method treats each problem instance as an image and trains a standardvisual diffusion model that transforms Gaussian noise into an imagerepresenting a valid approximate solution that closely matches the exact one.The model learns to transform noisy geometric structures into correctconfigurations, effectively recasting geometric reasoning as image generation.  Unlike prior work that necessitates specialized architectures anddomain-specific adaptations when applying diffusion to parametric geometricrepresentations, we employ a standard visual diffusion model that operates onthe visual representation of the problem. This simplicity highlights asurprising bridge between generative modeling and geometric problem solving.Beyond the specific problems studied here, our results point toward a broaderparadigm: operating in image space provides a general and practical frameworkfor approximating notoriously hard problems, and opens the door to tackling afar wider class of challenging geometric tasks.</description>
      <author>example@mail.com (Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or)</author>
      <guid isPermaLink="false">2510.21697v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields</title>
      <link>http://arxiv.org/abs/2510.21441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出OpenHype方法，使用连续双曲潜在空间表示3D场景层次结构，实现了更高效的3D场景理解，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;建模3D对象和3D场景的内在层次结构对自主代理理解环境至关重要，但使用隐式表示如神经辐射场实现这一目标仍面临挑战。现有显式建模层次结构的方法存在局限性：要么需要多次渲染增加推理时间，要么依赖预定义的封闭集离散层次结构，难以泛化到真实世界的多样化结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效表示3D场景层次结构的方法，解决现有方法在推理效率和泛化能力方面的局限性，实现对3D场景更全面、高效的理解。&lt;h4&gt;方法&lt;/h4&gt;提出OpenHype，使用连续双曲潜在空间表示场景层次结构。利用双曲几何特性自然编码多尺度关系，通过潜在空间中的测地线路径实现层次结构的平滑遍历。&lt;h4&gt;主要发现&lt;/h4&gt;OpenHype在标准基准测试中优于最先进的方法，展示了在3D场景理解方面卓越的效率和适应性。&lt;h4&gt;结论&lt;/h4&gt;通过利用双曲几何性质，OpenHype提供了表示和探索3D场景层次结构的有效方式，解决了现有方法的效率和泛化局限性，为自主代理的环境理解提供了更强大工具。&lt;h4&gt;翻译&lt;/h4&gt;建模3D对象和3D场景的内在层次结构是非常可取的，因为它能够使自主代理更全面地理解环境。使用隐式表示（如神经辐射场）来实现这一点仍然是一个未被探索的挑战。明确建模层次结构的现有方法通常面临显著限制：它们要么需要多次渲染传递来捕获不同粒度级别的嵌入，显著增加了推理时间；要么依赖于预定义的封闭集离散层次结构，难以泛化到代理在真实世界中遇到的多样化且细微的结构。为解决这些挑战，我们提出了OpenHype，一种使用连续双曲潜在空间表示场景层次结构的新方法。通过利用双曲几何的特性，OpenHype自然编码了多尺度关系，并能够通过潜在空间中的测地线路径实现层次结构的平滑遍历。我们的方法在标准基准测试中优于最先进的方法，展示了在3D场景理解方面卓越的效率和适应性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效表示3D场景中的层次结构问题，特别是在使用神经辐射场(NeRF)等隐式表示时。这个问题很重要，因为理解3D场景的层次结构对自主代理全面理解环境至关重要，例如物体由多个部分组成，也可以在更高层次上语义分组，这种层次组织对于语义分割、场景重建和物体检测等应用非常关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考，发现现有方法要么需要多次渲染增加推理时间，要么依赖预定义的离散层次结构泛化能力差。他们借鉴了双曲几何的思想，因为双曲空间的指数扩展特性能够自然编码多尺度关系。方法借鉴了现有工作如使用CLIP提取语言特征、使用中性词减少噪声等，但创新性地将双曲几何应用于3D场景层次表示，实现了连续层次遍历。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用双曲空间的几何特性自然编码3D场景的层次结构，实现连续的多尺度关系表示。整体流程包括：1)双曲自编码器训练：将语言对齐特征转换为双曲空间表示，高层对象靠近原点，低层对象靠近边界；2)NeRF训练：监督模型预测双曲特征，使用双曲距离作为损失函数；3)层次遍历：通过沿测地线路径连续遍历层次，解码特征并计算与文本提示的相似度，使用softmax加权聚合结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在3D场景理解中应用双曲空间自然编码层次结构；2)实现连续层次遍历，只需一次渲染而非多次；3)提出特征外推技术解决多视图一致性问题；4)改进的softmax加权聚合方法处理复杂查询。相比之前工作，OpenHype无需预定义离散层次或多次渲染，能连续遍历层次结构，在处理组合查询时表现更好，解决了现有方法的'词袋效应'问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenHype通过双曲空间几何特性实现了在神经辐射场中连续、高效地表示和遍历3D场景层次结构，显著提升了开放词汇3D场景理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling the inherent hierarchical structure of 3D objects and 3D scenes ishighly desirable, as it enables a more holistic understanding of environmentsfor autonomous agents. Accomplishing this with implicit representations, suchas Neural Radiance Fields, remains an unexplored challenge. Existing methodsthat explicitly model hierarchical structures often face significantlimitations: they either require multiple rendering passes to captureembeddings at different levels of granularity, significantly increasinginference time, or rely on predefined, closed-set discrete hierarchies thatgeneralize poorly to the diverse and nuanced structures encountered by agentsin the real world. To address these challenges, we propose OpenHype, a novelapproach that represents scene hierarchies using a continuous hyperbolic latentspace. By leveraging the properties of hyperbolic geometry, OpenHype naturallyencodes multi-scale relationships and enables smooth traversal of hierarchiesthrough geodesic paths in latent space. Our method outperforms state-of-the-artapproaches on standard benchmarks, demonstrating superior efficiency andadaptability in 3D scene understanding.</description>
      <author>example@mail.com (Lisa Weijler, Sebastian Koch, Fabio Poiesi, Timo Ropinski, Pedro Hermosilla)</author>
      <guid isPermaLink="false">2510.21441v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZING-3D是一个创新的框架，能够生成丰富语义表示的3D场景图，支持增量更新和几何基础，适用于机器人应用。&lt;h4&gt;背景&lt;/h4&gt;理解和推理复杂的3D环境需要结构化的场景表示，捕获物体及其语义和空间关系。现有3D场景图生成工作利用预训练VLMs但存在局限：局限于单视图设置、不支持增量更新、缺乏3D空间几何基础。&lt;h4&gt;目的&lt;/h4&gt;提出ZING-3D框架，利用预训练基础模型实现开放词汇识别，零样本生成丰富场景语义表示，支持增量更新和3D空间几何基础，适用于下游机器人应用。&lt;h4&gt;方法&lt;/h4&gt;利用VLM推理生成丰富2D场景图，使用深度信息与3D空间关联；节点表示开放词汇对象(含特征、3D位置、语义上下文)，边捕获空间和语义关系(含对象间距离)。&lt;h4&gt;主要发现&lt;/h4&gt;在Replica和HM3D数据集上的实验表明，ZING-3D能有效捕获空间和关系知识，无需特定任务训练。&lt;h4&gt;结论&lt;/h4&gt;ZING-3D解决了现有方法的局限性，适用于下游机器人应用。&lt;h4&gt;翻译&lt;/h4&gt;理解和推理复杂的3D环境需要结构化的场景表示，这些表示不仅要捕获物体，还要捕获它们的语义和空间关系。虽然最近关于3D场景图生成的工作利用了没有针对特定任务微调的预训练VLMs，但它们主要局限于单视图设置，无法支持随着新观察到来时的增量更新，并且缺乏在3D空间中的明确几何基础，所有这些对于具身场景都是必不可少的。在本文中，我们提出了ZING-3D框架，它利用预训练基础模型的丰富知识，实现开放词汇识别，并以零样本方式生成丰富的场景语义表示，同时支持在3D空间中进行增量更新和几何基础，使其适用于下游机器人应用。我们的方法利用VLM推理生成丰富的2D场景图，并使用深度信息将其与3D关联。节点表示具有特征、3D位置和语义上下文的开放词汇对象，而边捕获具有对象间距离的空间和语义关系。我们在来自Replica和HM3D数据集的场景上的实验表明，ZING-3D能够在无需特定任务训练的情况下有效地捕获空间和关系知识。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D场景图生成方法的三个限制：依赖特定词汇表、只能在单张2D图像上操作、无法增量更新。这个问题在现实中很重要，因为机器人需要在动态环境中在线构建和更新对3D环境的理解，而现有方法无法处理现实世界中的新物体或关系，也无法捕捉不同视角间的空间一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从嵌入式AI代理在环境中探索的角度设计系统，考虑如何随着新观测的加入逐步构建场景理解。他们借鉴了Vision-Language Models(VLMs)的进步，特别是像Open-World SGG和Pixels-to-Graphs等利用预训练模型进行零样本关系推理的工作。同时，他们结合了深度信息实现3D几何定位，并使用Grounded-SAM2提供精确的物体分割掩码，将2D场景表示提升到3D空间。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练视觉-语言模型的广泛知识，实现零样本开放词汇表识别，同时支持场景图的增量更新和3D空间中的几何定位。整体流程包括：1)使用VLM进行开放词汇表物体检测；2)构建2D场景图，捕获物体间的空间和语义关系；3)使用Grounded-SAM2获取精确分割掩码，结合深度信息将物体投影到3D空间；4)随着机器人探索，增量更新全局3D场景图；5)根据导航任务需求进行场景图剪枝。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)零样本嵌入式场景图生成，无需任务特定训练；2)丰富的语义信息，节点包含物体特征、3D位置和房间类型，边表示精确的空间关系；3)支持增量更新，场景图随探索过程动态演进。相比之前工作，ZING-3D的独特之处在于结合了2D视觉推理与3D几何信息，支持增量更新，实现了真正的开放词汇表识别，并提供了任务导向的场景图剪枝功能，更适合机器人实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ZING-3D通过结合视觉-语言模型与3D几何信息，实现了零样本增量式3D场景图生成，为机器人在复杂环境中的导航和交互提供了结构化的语义-空间表示。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and reasoning about complex 3D environments requires structuredscene representations that capture not only objects but also their semantic andspatial relationships. While recent works on 3D scene graph generation haveleveraged pretrained VLMs without task-specific fine-tuning, they are largelyconfined to single-view settings, fail to support incremental updates as newobservations arrive and lack explicit geometric grounding in 3D space, all ofwhich are essential for embodied scenarios. In this paper, we propose, ZING-3D,a framework that leverages the vast knowledge of pretrained foundation modelsto enable open-vocabulary recognition and generate a rich semanticrepresentation of the scene in a zero-shot manner while also enablingincremental updates and geometric grounding in 3D space, making it suitable fordownstream robotics applications. Our approach leverages VLM reasoning togenerate a rich 2D scene graph, which is grounded in 3D using depthinformation. Nodes represent open-vocabulary objects with features, 3Dlocations, and semantic context, while edges capture spatial and semanticrelations with inter-object distances. Our experiments on scenes from theReplica and HM3D dataset show that ZING-3D is effective at capturing spatialand relational knowledge without the need of task-specific training.</description>
      <author>example@mail.com (Pranav Saxena, Jimmy Chiun)</author>
      <guid isPermaLink="false">2510.21069v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2510.20198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 24 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过五项任务测试了大型语言模型在文本输入上的空间推理能力，发现模型在简单任务中表现中等，但随着复杂性和规模增加，性能显著下降，平均准确率损失42.7%，最高达84%，表明LLMs在空间推理方面存在明显局限性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言处理方面表现出色，但其空间推理能力尚未被充分研究。&lt;h4&gt;目的&lt;/h4&gt;探究大型语言模型在文本输入上的空间理解和计算能力，识别其在空间推理方面的优势和局限。&lt;h4&gt;方法&lt;/h4&gt;设计并实施了五项空间推理任务：象限识别、几何变换、距离评估、单词搜索和滑块拼图。这些任务在结构化网格环境中进行，通过增加网格尺寸来提高复杂性，要求模型从简单模式识别扩展到抽象空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模型在复杂性和规模较小的任务中表现中等；2) 随着规模增加，性能迅速下降，准确率平均损失42.7%，最高达84%；3) 所有初始准确率超过50%的测试显示至少48%的性能损失；4) 模型在扩展复杂性方面的挣扎暗示其底层架构中缺乏强大的空间表示。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型在语言推理和空间推理之间存在明显差距，本研究揭示了其当前局限性，并为未来在语言和几何交叉领域的集成基准研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文通过一套五项任务，探究了大型语言模型对文本输入的空间推理能力，旨在测试它们的空间理解和计算能力。模型在基于结构化网格环境中的基本空间推理和多步问题解决方面接受了测试，使用了象限识别、几何变换、距离评估、单词搜索和滑块拼图等任务。每个任务通过增加网格尺寸来提高复杂性，要求模型超越简单的模式识别，进入抽象空间推理。我们的结果显示，虽然大型语言模型在复杂性和规模较小的所有任务中表现出中等成功，但随着规模增加，性能迅速下降，准确率平均损失42.7%，最高达到84%。所有初始准确率超过50%的测试都显示出至少48%的损失，说明了性能下降的一致性。此外，模型在扩展复杂性方面的挣扎暗示其底层架构中缺乏强大的空间表示。本文强调了大型语言模型中语言推理和空间推理之间的差距，提供了对其当前局限性的见解，并为未来在语言和几何交叉领域的集成基准研究奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the spatial reasoning capability of large language models(LLMs) over textual input through a suite of five tasks aimed at probing theirspatial understanding and computational abilities. The models were tested onboth fundamental spatial reasoning and multi-step problem-solving withinstructured grid-based environments using tasks such as quadrant identification,geometric transformations, distance evaluation, word searches, and tilesliding. Each task was scaled in complexity through increasing grid dimensions,requiring models to extend beyond simple pattern recognition into abstractspatial reasoning. Our results reveal that while LLMs demonstrate moderatesuccess in all tasks with small complexity and size, performance drops offrapidly as scale increases, with an average loss in accuracy of 42.7%, andreaching as high as 84%. Every test that began with over 50% accuracy showed aloss of at least 48%, illustrating the consistent nature of the deterioration.Furthermore, their struggles with scaling complexity hint at a lack of robustspatial representations in their underlying architectures. This paperunderscores the gap between linguistic and spatial reasoning in LLMs, offeringinsights into their current limitations, and laying the groundwork for futureintegrative benchmarks at the intersection of language and geometry.</description>
      <author>example@mail.com (Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum)</author>
      <guid isPermaLink="false">2510.20198v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty evaluation of segmentation models for Earth observation</title>
      <link>http://arxiv.org/abs/2510.19586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了从卫星影像中估计语义分割预测不确定性的方法，针对遥感地球观测应用对现有方法进行了基准测试，评估了不确定性度量的实际效用，并提出了实用建议。&lt;h4&gt;背景&lt;/h4&gt;与标准图像分类相比，分割中的不确定性估计面临独特挑战，需要可扩展的方法来生成逐像素估计。大多数相关研究集中在场景理解或医学影像领域。&lt;h4&gt;目的&lt;/h4&gt;专门针对遥感地球观测应用对不确定性估计方法进行基准测试，评估不确定性度量的实际效用，测试它们识别预测错误和噪声损坏的输入图像区域的能力。&lt;h4&gt;方法&lt;/h4&gt;在两个遥感数据集PASTIS和ForTy上进行实验，这些数据集在规模、地理覆盖范围和标签置信度方面存在差异。评估包括多种模型（如随机分割网络和集成方法）与多种神经网络架构和不确定性度量相结合的广泛评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验评估了不同不确定性估计方法在遥感应用中的表现，确定了哪些方法更适合识别预测错误和噪声损坏区域。&lt;h4&gt;结论&lt;/h4&gt;基于研究结果提出了若干实用建议，为遥感影像语义分割中的不确定性估计提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了从卫星影像中估计语义分割预测不确定性的方法。与标准图像分类相比，分割中的不确定性估计面临独特挑战，需要可扩展的方法来生成逐像素估计。虽然大多数关于此主题的研究集中在场景理解或医学影像上，但这项工作专门针对遥感地球观测应用对现有方法进行了基准测试。我们的评估侧重于不确定性度量的实际效用，测试它们识别预测错误和噪声损坏的输入图像区域的能力。实验在两个遥感数据集PASTIS和ForTy上进行，这两个数据集在规模、地理覆盖范围和标签置信度方面存在差异。我们进行了广泛的评估，结合了多种模型（如随机分割网络和集成方法）与多种神经网络架构和不确定性度量。根据我们的发现，我们提出了若干实用建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates methods for estimating uncertainty in semanticsegmentation predictions derived from satellite imagery. Estimating uncertaintyfor segmentation presents unique challenges compared to standard imageclassification, requiring scalable methods producing per-pixel estimates. Whilemost research on this topic has focused on scene understanding or medicalimaging, this work benchmarks existing methods specifically for remote sensingand Earth observation applications. Our evaluation focuses on the practicalutility of uncertainty measures, testing their ability to identify predictionerrors and noise-corrupted input image regions. Experiments are conducted ontwo remote sensing datasets, PASTIS and ForTy, selected for their differencesin scale, geographic coverage, and label confidence. We perform an extensiveevaluation featuring several models, such as Stochastic Segmentation Networksand ensembles, in combination with a number of neural architectures anduncertainty metrics. We make a number of practical recommendations based on ourfindings.</description>
      <author>example@mail.com (Melanie Rey, Andriy Mnih, Maxim Neumann, Matt Overlan, Drew Purves)</author>
      <guid isPermaLink="false">2510.19586v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</title>
      <link>http://arxiv.org/abs/2510.19400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The project and benchmark are publicly available at  https://github.com/microsoft/MV-RoboBench&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MV-RoboBench基准测试，用于评估视觉语言模型在机器人操作中的多视图空间推理能力。研究显示当前最先进模型表现远低于人类水平，并发现空间智能与机器人任务执行呈正相关，但单视图基准表现不能可靠预测多视图机器人任务表现。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型对具身人工智能至关重要，是视觉语言动作模型的基础。然而大多数VLM评估集中在单视图设置，对多视图信息整合能力的探索不足。多摄像头设置在机器人平台上越来越标准，能提供互补视角以缓解遮挡和深度模糊问题。&lt;h4&gt;目的&lt;/h4&gt;填补VLMs多视图空间推理能力评估的空白，专门设计一个基准测试来评估VLMs在机器人操作中的多视图空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建MV-RoboBench基准测试，包含8个子任务中的1.7k个手动筛选的问答项目，分为空间理解和机器人执行两个主要类别。评估多种现有VLMs（包括开源和闭源模型）以及采用CoT启发技术的增强版本。&lt;h4&gt;主要发现&lt;/h4&gt;(i)在多视图机器人场景中，空间智能和机器人任务执行呈正相关；(ii)在现有通用单视图空间理解基准上的良好表现并不能可靠地转化为在机器人空间任务中的成功。&lt;h4&gt;结论&lt;/h4&gt;当前最先进的VLMs在多视图机器人感知方面仍面临重大挑战。作者发布MV-RoboBench作为开放资源，旨在促进空间感知VLMs和VLAs的进步，提供数据和多视图具身推理的标准化评估协议。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型对具身人工智能至关重要，使机器人能够感知、推理并在复杂环境中行动。它们也是最近视觉语言动作模型的基础。然而，大多数VLM评估集中在单视图设置，对其整合多视图信息的能力探索不足。与此同时，多摄像头设置在机器人平台上越来越标准，因为它们提供互补视角以缓解遮挡和深度模糊问题。因此，VLMs是否能有效利用此类多视图输入进行机器人推理仍然是一个开放问题。为填补这一空白，我们引入MV-RoboBench，一个专门设计用于评估VLMs在机器人操作中多视图空间推理能力的基准测试。MV-RoboBench包含8个子任务中的1.7k个手动筛选的问答项目，分为两个主要类别：空间理解和机器人执行。我们评估了多种现有的VLMs，包括开源和闭源模型，以及采用CoT启发技术的增强版本。结果显示，最先进的模型表现远低于人类水平，突显了VLMs在多视图机器人感知方面面临的重大挑战。此外，我们的分析揭示了两个关键发现：(i)在多视图机器人场景中，空间智能和机器人任务执行呈正相关；(ii)在现有通用单视图空间理解基准上的良好表现并不能可靠地转化为在我们基准评估的机器人空间任务中的成功。我们发布MV-RoboBench作为开放资源，旨在促进空间感知VLMs和VLAs的进步，不仅提供数据，还提供多视图具身推理的标准化评估协议。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是评估视觉语言模型（VLMs）在机器人场景中的多视图空间推理能力。这个问题很重要，因为现有的VLM评估大多集中在单视图设置，而机器人平台越来越多地采用多摄像头系统来提供互补视角以克服遮挡和深度模糊问题。理解VLM能否有效整合这些多视图信息对提升机器人在复杂环境中的感知和决策能力至关重要，也是实现先进视觉语言动作（VLA）模型的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有空间推理基准的局限性，发现它们大多专注于单视图数据或非具身任务，而机器人操作场景需要多视图感知能力。他们借鉴了ShareRobot（具身机器人任务但无多视图）、All-Angles Bench和Ego3D-Bench（多视图但仅限导航或照片对齐）等工作，设计了MV-RoboBench，一个专门针对机器人操作场景中多视图空间推理的基准。作者构建了多阶段管道：数据收集（从AgiWorld和BridgeV2数据集筛选）、问答生成（为八个子任务设计模板）和人工质保审查，确保基准质量和多样性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是评估VLMs能否有效整合多个摄像头视图的互补信息，支持机器人在现实世界中的决策。基准包含1700多个人工策划的问答项目，分为空间理解（跨视图匹配、距离判断、视角识别、3D空间一致性）和机器人执行（动作规划、步骤执行、轨迹选择、功能识别）两大类。实现流程包括：1)数据收集（规则过滤+GPT-4.1辅助筛选+人工验证）；2)问答生成（任务特定模板+五选一问答对构建）；3)人工质保审查（迭代审查+内容修正+答案分布平衡）；4)模型评估（统一零样本提示+准确率作为指标）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合空间和机器人推理与多视图输入的机器人操作基准；2)系统性评估VLMs整合多视图信息的能力；3)发现空间智能与机器人执行在多视图场景中正相关；4)揭示单视图基准性能不能可靠转移到多视图机器人任务。相比之前工作，MV-RoboBench专注于具身多视图推理而非抽象任务；使用真实机器人演示而非模板生成；同时评估空间理解和机器人执行；强调多视图互补信息整合而非单一视角分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MV-RoboBench是首个专门针对机器人操作场景中多视图空间推理能力的基准测试，通过系统评估现有视觉语言模型的表现，揭示了它们在整合多视图信息进行机器人决策方面的显著不足，并为未来具身人工智能和多视图感知研究提供了新的评估标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) are essential to Embodied AI, enabling robotsto perceive, reason, and act in complex environments. They also serve as thefoundation for the recent Vision-Language-Action (VLA) models. Yet mostevaluations of VLMs focus on single-view settings, leaving their ability tointegrate multi-view information underexplored. At the same time, multi-camerasetups are increasingly standard in robotic platforms, as they providecomplementary perspectives to mitigate occlusion and depth ambiguity. WhetherVLMs can effectively leverage such multi-view inputs for robotic reasoningtherefore remains an open question. To bridge this gap, we introduceMV-RoboBench, a benchmark specifically designed to evaluate the multi-viewspatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBenchconsists of 1.7k manually curated QA items across eight subtasks, divided intotwo primary categories: spatial understanding and robotic execution. Weevaluate a diverse set of existing VLMs, including both open-source andclosed-source models, along with enhanced versions incorporating CoT-inspiredtechniques. The results show that state-of-the-art models remain far belowhuman performance, underscoring the substantial challenges VLMs face inmulti-view robotic perception. Additionally, our analysis uncovers two keyfindings: (i) spatial intelligence and robotic task execution are positivelycorrelated in multi-view robotic scenarios; and (ii) strong performance onexisting general-purpose single-view spatial understanding benchmarks does notreliably translate to success in the robotic spatial tasks assessed by ourbenchmark. We release MV-RoboBench as an open resource to foster progress inspatially grounded VLMs and VLAs, providing not only data but also astandardized evaluation protocol for multi-view embodied reasoning.</description>
      <author>example@mail.com (Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo)</author>
      <guid isPermaLink="false">2510.19400v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization</title>
      <link>http://arxiv.org/abs/2510.19330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了人群定位中的尺度偏移问题及其在域泛化场景下的影响，提出了Catto算法来减轻尺度偏移的影响，并建立了ScaleBench基准测试。&lt;h4&gt;背景&lt;/h4&gt;人群定位在视觉场景理解中扮演关键角色，但现有方法因训练和测试数据之间头部尺度分布差异（尺度偏移）导致性能显著下降，这一问题被称为域泛化挑战。&lt;h4&gt;目的&lt;/h4&gt;理解人群定位模型在域泛化背景下尺度偏移的本质，解决四个关键问题：尺度偏移如何影响人群定位、如何量化这种影响、产生原因以及如何减轻影响。&lt;h4&gt;方法&lt;/h4&gt;系统检查不同尺度偏移水平下人群定位性能变化；建立ScaleBench基准测试，重现20种先进域泛化算法；提供尺度偏移的严格理论分析；提出因果特征分解和各向异性处理（Catto）算法。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验展示了现有算法的局限性；强调了尺度偏移的重要性和复杂性；提供了四个对未来研究有重要意义的见解。&lt;h4&gt;结论&lt;/h4&gt;强调了'尺度偏移域泛化'这一新颖且适用的研究方向的重要性。&lt;h4&gt;翻译&lt;/h4&gt;人群定位在视觉场景理解中扮演关键角色，用于预测人群中每个行人的位置，因此适用于各种下游任务。然而，由于训练和测试数据之间头部尺度分布的差异（尺度偏移），现有方法性能显著下降，这一挑战被称为域泛化（DG）。本文旨在理解在人群定位模型的域泛化背景下尺度偏移的本质。为此，我们解决了四个关键问题：(i) 尺度偏移如何在域泛化场景中影响人群定位？(ii) 如何量化这种影响？(iii) 产生这种影响的原因是什么？(iv) 如何减轻这种影响？首先，我们系统地检查了人群定位性能如何随不同水平的尺度偏移而变化。然后，我们建立了一个基准ScaleBench，重现了20种先进的域泛化算法来量化这种影响。通过大量实验，我们展示了现有算法的局限性，并强调了尺度偏移的重要性和复杂性，这是一个尚未充分探索的主题。为了加深理解，我们对尺度偏移提供了严格的理论分析。基于这些见解，我们进一步提出了一种名为因果特征分解和各向异性处理（Catto）的有效算法，以减轻域泛化设置中尺度偏移的影响。随后，我们还提供了大量的分析实验，揭示了四个对未来研究有重要意义的见解。我们的结果强调了这一新颖且适用的研究方向的重要性，我们称之为尺度偏移域泛化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crowd localization plays a crucial role in visual scene understanding towardspredicting each pedestrian location in a crowd, thus being applicable tovarious downstream tasks. However, existing approaches suffer from significantperformance degradation due to discrepancies in head scale distributions (scaleshift) between training and testing data, a challenge known as domaingeneralization (DG). This paper aims to comprehend the nature of scale shiftwithin the context of domain generalization for crowd localization models. Tothis end, we address four critical questions: (i) How does scale shiftinfluence crowd localization in a DG scenario? (ii) How can we quantify thisinfluence? (iii) What causes this influence? (iv) How to mitigate theinfluence? Initially, we conduct a systematic examination of how crowdlocalization performance varies with different levels of scale shift. Then, weestablish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms toquantify the influence. Through extensive experiments, we demonstrate thelimitations of existing algorithms and underscore the importance and complexityof scale shift, a topic that remains insufficiently explored. To deepen ourunderstanding, we provide a rigorous theoretical analysis on scale shift.Building on these insights, we further propose an effective algorithm calledCausal Feature Decomposition and Anisotropic Processing (Catto) to mitigate theinfluence of scale shift in DG settings. Later, we also provide extensiveanalytical experiments, revealing four significant insights for futureresearch. Our results emphasize the importance of this novel and applicableresearch direction, which we term Scale Shift Domain Generalization.</description>
      <author>example@mail.com (Juncheng Wang, Lei Shang, Ziqi Liu, Wang Lu, Xixu Hu, Zhe Hu, Jindong Wang, Shujun Wang)</author>
      <guid isPermaLink="false">2510.19330v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning</title>
      <link>http://arxiv.org/abs/2510.18337v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MoTVLA是一种基于混合变压器的视觉-语言-动作模型，结合了快速-慢速统一推理与行为策略学习，解决了现有方法中语言控制能力有限和推理延迟显著的问题。&lt;h4&gt;背景&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中是增强机器人开放世界泛化能力的热门研究方向。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够平衡语言控制能力和执行效率的模型，解决现有方法中的两个主要挑战。&lt;h4&gt;方法&lt;/h4&gt;MoTVLA模型保留了预训练视觉语言模型的通用智能，同时引入一个与预训练模型共享知识的领域专家transformer，生成领域特定的快速推理，并将动作专家基于分解的运动指令进行条件化。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛评估，MoTVLA在快速-慢速推理和操作任务性能方面表现出优越性，能够学习多样化行为并显著提高语言控制能力。&lt;h4&gt;结论&lt;/h4&gt;MoTVLA成功整合了快速-慢速统一推理与行为策略学习，有效解决了现有方法中的局限性，为机器人学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中正在增强机器人学习的开放世界泛化能力方面获得动力。尽管有 promising 的进展，现有方法面临两个挑战：在不使用生成推理作为条件时，语言控制能力有限，或者在整合推理时推理延迟显著。在这项工作中，我们引入了MoTVLA，一种基于混合变压器(MoT)的视觉-语言-动作(VLA)模型，它整合了快速-慢速统一推理与行为策略学习。MoTVLA保留了预训练VLMs的通用智能（作为通用者）用于感知、场景理解和语义规划等任务，同时整合了一个领域专家（第二个transformer），它与预训练VLM共享知识，以生成领域特定的快速推理（例如机器人运动分解），从而提高策略执行效率。通过将动作专家基于分解的运动指令进行条件化，MoTVLA能够学习多样化行为并显著提高语言控制能力。在自然语言处理基准、机器人仿真环境和真实世界实验中的广泛评估证实了MoTVLA在快速-慢速推理和操作任务性能方面的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在机器人学习中整合视觉语言指令时面临的两个挑战：一是当不使用生成的推理作为条件时语言控制能力有限，二是当整合推理时推理延迟显著。这个问题很重要，因为它限制了机器人在开放世界中的泛化能力和实时应用，影响了机器人在需要快速响应和精确控制的环境中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：传统视觉语言动作模型在连续动作表示上存在问题，扩散策略虽然适合连续动作空间但语言控制能力有限。他们提出通过'分解-组合-再分解'的混合变换器架构统一快速和慢速推理。该方法借鉴了混合变换器架构、预训练视觉语言模型、扩散策略等现有工作，并参考了BAGEL模型中的Vision Transformer和Qwen2.5 LLM的文本分词器。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合变换器架构统一快速和慢速推理，在一个模型中同时保留通用智能和领域特定知识，使用'分解-组合-再分解'流程实现知识共享。整体流程包括：输入空间设计(处理语言、RGB图像和可学习查询)；推理骨干设计(通用专家负责慢速推理，领域专家负责快速推理)；推理输出设计(统一在文本空间但分为两种功能)；动作专家设计(使用扩散变换器生成动作)；训练流程(领域专家微调和动作专家扩散策略训练)；推理流程(支持对话模式和动作模式两种交互方式)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一快速-慢速推理的MoT架构；2)基于分解运动条件的策略学习；3)支持对话和动作的双模式操作。相比之前的工作，MoTVLA解决了连续动作表示的精度问题，显式生成推理内容提高语言控制能力，显著降低推理延迟，并通过知识共享避免了灾难性遗忘，实现了更好的知识保留和执行效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MoTVLA通过混合变换器架构统一了快速和慢速推理，在保持通用视觉语言模型智能的同时，实现了高效、可解释的机器人操作策略学习，显著提升了语言控制能力和任务执行效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating visual-language instructions into visuomotor policies is gainingmomentum in robot learning for enhancing open-world generalization. Despitepromising advances, existing approaches face two challenges: limited languagesteerability when no generated reasoning is used as a condition, or significantinference latency when reasoning is incorporated. In this work, we introduceMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)model that integrates fast-slow unified reasoning with behavior policylearning. MoTVLA preserves the general intelligence of pre-trained VLMs(serving as the generalist) for tasks such as perception, scene understanding,and semantic planning, while incorporating a domain expert, a secondtransformer that shares knowledge with the pretrained VLM, to generatedomain-specific fast reasoning (e.g., robot motion decomposition), therebyimproving policy execution efficiency. By conditioning the action expert ondecomposed motion instructions, MoTVLA can learn diverse behaviors andsubstantially improve language steerability. Extensive evaluations acrossnatural language processing benchmarks, robotic simulation environments, andreal-world experiments confirm the superiority of MoTVLA in both fast-slowreasoning and manipulation task performance.</description>
      <author>example@mail.com (Wenhui Huang, Changhe Chen, Han Qi, Chen Lv, Yilun Du, Heng Yang)</author>
      <guid isPermaLink="false">2510.18337v3</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14828v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RoboGPT-R1的两阶段微调框架，用于提高具身智能体在长期操作任务中的推理能力，显著提升了模型在EmbodiedBench基准测试上的表现。&lt;h4&gt;背景&lt;/h4&gt;提高具身智能体的推理能力对机器人在长期操作任务中成功完成复杂人类指令至关重要。尽管基于监督微调的大语言模型和视觉语言模型在规划任务中取得成功，但在复杂真实环境中的长期操作任务仍面临挑战，这是由于它们有限的常识和推理能力。&lt;h4&gt;目的&lt;/h4&gt;解决通用视觉语言模型通过监督微调对机器人规划任务的泛化能力差和物理理解不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出RoboGPT-R1框架，第一阶段通过监督训练使用专家序列获取基础知识，第二阶段使用强化学习解决模型在视觉空间理解和推理方面的不足。设计基于规则的奖励函数，同时考虑长期性能和环境中的动作约束，以实现多步推理任务中的物理理解和动作序列一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，提高了21.33%，超越了在Qwen2.5-VL-7B上训练的其他工作，提高了20.33%。&lt;h4&gt;结论&lt;/h4&gt;RoboGPT-R1框架有效提高了具身智能体的推理能力和长期操作任务表现。&lt;h4&gt;翻译&lt;/h4&gt;提高具身智能体的推理能力对于机器人在长期操作任务中成功完成复杂的人类指令至关重要。尽管基于监督微调的大语言模型和视觉语言模型在规划任务中取得了成功，但由于其有限的常识和推理能力，它们在复杂真实环境中执行长期操作任务时仍面临挑战。考虑到通过监督微调将通用视觉语言模型与机器人规划任务对齐存在泛化能力差和物理理解不足的问题，我们提出了RoboGPT-R1，一个用于具身规划的两阶段微调框架。在该框架中，监督训练通过专家序列获取基础知识，随后使用强化学习来解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，同时考虑长期性能和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，提高了21.33%，并超越了在Qwen2.5-VL-7B上训练的其他工作，提高了20.33%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improving the reasoning capabilities of embodied agents is crucial for robotsto complete complex human instructions in long-view manipulation taskssuccessfully. Despite the success of large language models and vision languagemodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continuefacing challenges in performing long-horizon manipulation tasks in complexreal-world environments, owing to their restricted common sense and reasoningcapabilities. Considering that aligning general-purpose vision language modelsto robotic planning tasks via supervised fine-tuning suffers from poorgeneralization and insufficient physical understanding, we propose RoboGPT-R1,a two-stage fine-tuning framework for embodied planning. In this framework,supervised training acquires foundational knowledge through expert sequences,followed by RL to address the model's shortcomings in visual-spatialunderstanding and reasoning. To achieve physical understanding and actionsequence consistency in multi-step reasoning tasks, we design a rule-basedreward function that simultaneously considers long-horizon performance andaction constraint in the environment. The reasoning model, trained onQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on theEmbodiedBench benchmark.</description>
      <author>example@mail.com (Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li)</author>
      <guid isPermaLink="false">2510.14828v2</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning</title>
      <link>http://arxiv.org/abs/2510.21635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAP-MAE的领域自适应点云掩码自编码器方法，用于解决跨领域点云数据整合问题，提高下游3D点云分析任务性能。&lt;h4&gt;背景&lt;/h4&gt;与2D数据相比，可用于训练的点云数据在不同领域中规模有限，研究人员尝试结合不同领域数据进行MAE预训练以缓解数据稀缺问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自适应整合跨领域数据集知识的方法，以改善通用点云分析任务性能。&lt;h4&gt;方法&lt;/h4&gt;设计了异构领域适配器，在预训练阶段使用适配模式学习跨领域点云信息，在微调阶段采用融合模式增强特征；同时引入领域特征生成器指导点云特征适应下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;仅通过一次预训练，DAP-MAE在四种点云分析任务上表现优异，在ScanObjectNN上的目标分类达到95.18%，在Bosphorus上的面部表情识别达到88.45%。&lt;h4&gt;结论&lt;/h4&gt;DAP-MAE有效解决了跨领域点云数据整合问题，提高了下游任务性能，为点云分析提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;与2D数据相比，可用于训练的点云数据在不同领域的规模相当有限。研究人员一直在尝试结合这些不同领域的数据进行掩码自编码器预训练，以利用这种数据稀缺问题。然而，从混合领域学到的先验知识可能与下游3D点云分析任务不太匹配，导致性能下降。为解决这一问题，我们提出了领域自适应点云掩码自编码器，这是一种MAE预训练方法，可以自适应地整合跨领域数据集的知识，用于通用点云分析。在DAP-MAE中，我们设计了一个异构领域适配器，在预训练期间使用适配模式，使模型能够全面学习来自不同领域点云的信息，同时在微调阶段采用融合模式以增强点云特征。同时，DAP-MAE集成了一个领域特征生成器，指导点云特征适应各种下游任务。仅通过一次预训练，DAP-MAE在四种不同的点云分析任务上取得了优异的性能，在ScanObjectNN上的目标分类达到95.18%，在Bosphorus上的面部表情识别达到88.45%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云数据在不同领域（物体、人脸、场景）之间的迁移学习问题。在现实中，3D点云数据的收集和标注需要大量资源，导致各领域可用数据有限。现有方法通常只在单一领域内进行预训练，当应用于不同领域任务时性能显著下降。解决这个问题对于实现通用3D点云分析至关重要，可应用于自动驾驶、机器人、增强/虚拟现实等领域，有效利用有限的数据资源并提高模型泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云MAE方法的局限性：单一领域预训练导致跨领域性能下降，简单组合多领域数据也会因信息误解为噪声而降低性能。基于此，他们设计了DAP-MAE框架，包含异构领域适配器(HDA)和领域特征生成器(DFG)两个核心组件。该方法借鉴了掩码自编码器(MAE)的自监督学习框架、Transformer架构、PointNet的点云处理方法以及对比学习技术，但创新性地将其应用于跨领域点云学习场景，实现了单模态一次预训练适应多任务的目标。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过异构领域适配器和领域特征生成器，使模型能够协作学习来自不同领域的点云数据，实现一次预训练适应多种下游任务。整体流程分为两阶段：1)预训练阶段：使用来自物体、人脸、场景三个领域的数据，通过HDA的适应模式分别处理各领域数据，使用Transformer编码器-解码器架构进行掩码重建，同时DFG提取领域特征并通过对比损失训练；2)微调阶段：针对特定下游任务，使用HDA的融合模式整合多领域信息，DFG提取领域和类别特征，输入任务头进行训练。这种方法既保留了各领域的特性，又实现了跨领域知识的有效迁移。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出DAP-MAE框架，实现单模态一次预训练适应多任务；2)设计异构领域适配器(HDA)，预训练时使用适应模式分别处理不同领域，微调时使用融合模式整合信息；3)引入领域特征生成器(DFG)提取多样化领域特征指导下游任务。相比之前工作：与传统MAE不同，DAP-MAE能跨领域学习；与简单组合多领域数据的方法不同，它避免将跨域信息误解为噪声；与跨模态方法不同，它专注于单模态点云数据降低训练成本；在多个下游任务上实现了优于其他自监督方法的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAP-MAE通过异构领域适配器和领域特征生成器实现了跨领域点云数据的有效协作学习，仅需一次预训练就能在多种3D点云分析任务上达到顶尖性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compared to 2D data, the scale of point cloud data in different domainsavailable for training, is quite limited. Researchers have been trying tocombine these data of different domains for masked autoencoder (MAE)pre-training to leverage such a data scarcity issue. However, the priorknowledge learned from mixed domains may not align well with the downstream 3Dpoint cloud analysis tasks, leading to degraded performance. To address such anissue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),an MAE pre-training method, to adaptively integrate the knowledge ofcross-domain datasets for general point cloud analysis. In DAP-MAE, we design aheterogeneous domain adapter that utilizes an adaptation mode duringpre-training, enabling the model to comprehensively learn information frompoint clouds across different domains, while employing a fusion mode in thefine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates adomain feature generator to guide the adaptation of point cloud features tovarious downstream tasks. With only one pre-training, DAP-MAE achievesexcellent performance across four different point cloud analysis tasks,reaching 95.18% in object classification on ScanObjectNN and 88.45% in facialexpression recognition on Bosphorus.</description>
      <author>example@mail.com (Ziqi Gao, Qiufu Li, Linlin Shen)</author>
      <guid isPermaLink="false">2510.21635v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization</title>
      <link>http://arxiv.org/abs/2510.20974v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PCA点云(PPC)框架，用于解决点云强化学习中的相机姿态不匹配问题，通过将点云映射到规范姿态，显著提高了对视角变化的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;从原始视觉输入的强化学习近年来取得了显著成功，但它对分布外变化(如光照、颜色和视角变化)仍然很脆弱。点云强化学习提供了一种有前景的替代方案，减轻了基于外观的脆弱性，但其对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;解决点云强化学习对相机姿态不匹配敏感的挑战，提高在现实场景中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出PCA点云(PPC)框架，这是一个专门为下游机器人控制设计的规范化框架，它将任意刚体变换下的点云映射到唯一的规范姿态，将观测结果对齐到一致的坐标系。&lt;h4&gt;主要发现&lt;/h4&gt;PPC显著减少了视角引起的不一致性，在实验中提高了在具有挑战性的机器人任务中对未见过的相机姿态的鲁棒性，为域随机化提供了有原则的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PPC框架有效地解决了点云强化学习中的相机姿态不匹配问题，提高了在现实场景中的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;从原始视觉输入的强化学习近年来取得了显著成功，但它对分布外变化(如光照、颜色和视角变化)仍然很脆弱。点云强化学习提供了一种有前景的替代方案，减轻了基于外观的脆弱性，但其对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。为应对这一挑战，我们提出了PCA点云(PPC)，这是一个专门为下游机器人控制设计的规范化框架。PPC将任意刚体变换下的点云映射到唯一的规范姿态，将观测结果对齐到一致的坐标系，从而显著减少了视角引起的不一致性。在我们的实验中，我们证明了PPC提高了在具有挑战性的机器人任务中对未见过的相机姿态的鲁棒性，为域随机化提供了有原则的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) from raw visual input has achieved impressivesuccesses in recent years, yet it remains fragile to out-of-distributionvariations such as changes in lighting, color, and viewpoint. Point CloudReinforcement Learning (PC-RL) offers a promising alternative by mitigatingappearance-based brittleness, but its sensitivity to camera pose mismatchescontinues to undermine reliability in realistic settings. To address thischallenge, we propose PCA Point Cloud (PPC), a canonicalization frameworkspecifically tailored for downstream robotic control. PPC maps point cloudsunder arbitrary rigid-body transformations to a unique canonical pose, aligningobservations to a consistent frame, thereby substantially decreasingviewpoint-induced inconsistencies. In our experiments, we show that PPCimproves robustness to unseen camera poses across challenging robotic tasks,providing a principled alternative to domain randomization.</description>
      <author>example@mail.com (Michael Bezick, Vittorio Giammarino, Ahmed H. Qureshi)</author>
      <guid isPermaLink="false">2510.20974v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Fractional harmonic transform on point cloud manifolds</title>
      <link>http://arxiv.org/abs/2510.20842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数扩展了传统的点云流形谐波变换(PMHT)，构建了空间域和频率域之间可连续调节的中间分数阶谱域，实现了更灵活的变换和滤波操作。&lt;h4&gt;背景&lt;/h4&gt;点云可被视为光滑流形的离散样本，可使用拉普拉斯-贝尔特拉米算子进行谱分析。然而，传统的PMHT受限于固定基函数和单一谱表示，难以捕获复杂几何特征。&lt;h4&gt;目的&lt;/h4&gt;提出PMFHT来克服传统PMHT的局限性，通过引入分数阶参数构建连续可调的中间谱域。&lt;h4&gt;方法&lt;/h4&gt;引入分数阶参数，构建空间域和频率域之间的中间分数阶谱域，支持更灵活的变换和滤波操作。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，选择不同的变换顺序可以丰富点云的谱表示，在滤波和特征增强等任务中取得优异结果。&lt;h4&gt;结论&lt;/h4&gt;PMFHT扩展了点云谱分析的理论框架，为流形几何处理提供了强大的新工具。&lt;h4&gt;翻译&lt;/h4&gt;三维点云可以被视为光滑流形的离散样本，允许使用拉普拉斯-贝尔特拉米算子进行谱分析。然而，传统的点云流形谐波变换(PMHT)受其固定基函数和单一谱表示的限制，限制了其捕获复杂几何特征的能力。本文提出了一种点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数推广了PMHT，并构建了空间域和频率域之间可连续调节的中间分数阶谱域。这种分数阶框架支持更灵活的变换和滤波操作。实验表明，选择不同的变换顺序可以丰富点云的谱表示，并在滤波和特征增强等任务中取得优异结果。因此，PMFHT不仅扩展了点云谱分析的理论框架，还为流形几何处理提供了强大的新工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决传统点云流形谐波变换(PMHT)的局限性，即其固定的基函数和单一频谱表示无法充分捕捉复杂几何特征。这一问题很重要，因为三维点云是3D场景最常见的数据表示形式之一，广泛应用于LiDAR、结构光扫描和立体重建等领域，有效的几何特征提取对点云处理至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统PMHT的局限性，然后从信号处理中的分数傅里叶变换(FRFT)获得启发，后者通过引入分数阶参数解决了类似限制，提供了时域和频域之间的连续中间表示。作者借鉴了PMHT的基础框架、FRFT的分数阶参数思想以及LBO在点云上的离散化方法，将流形谐波扩展为分数阶形式，通过非线性缩放LBO特征值创建连续可调的中间频谱域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入分数阶参数α，将传统PMHT扩展为分数阶形式(PMFHT)，创建空间域和频率域之间可连续调整的中间分数阶频谱域。实现流程包括：1)构建离散拉普拉斯-贝尔特拉米算子；2)求解广义特征值问题获得点云流形谐波基；3)定义分数阶傅里叶矩阵和点云流形分数阶谐波变换；4)应用变换进行不同类型的滤波操作，如特征增强或平滑处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建点云流形分数阶谐波变换的统一框架；2)提供简单高效的PMFHT分数幂公式；3)证明PMFHT能提供更丰富的频谱表示并在点云处理任务中表现出色。相比传统PMHT，PMFHT引入分数阶参数提供连续可调的中间频谱域，能捕捉多尺度几何特征，通过选择不同变换阶数丰富频谱表示，为流形几何处理提供新工具。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数扩展了传统点云流形谐波变换，提供了空间域和频率域之间的连续可调中间表示，丰富了点云的频谱分析能力，并在点云处理任务中展现出优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional point clouds can be viewed as discrete samples of smoothmanifolds, allowing spectral analysis using the Laplace-Beltrami operator(LBO). However, the traditional point cloud manifold harmonic transform (PMHT)is limited by its fixed basis functions and single spectral representation,which restricts its ability to capture complex geometric features. This paperproposes a point cloud manifold fractional harmonic transform (PMFHT), whichgeneralizes PMHT by introducing fractional-order parameters and constructs acontinuously adjustable intermediate fractional-order spectral domain betweenthe spatial domain and the frequency domain. This fractional-order frameworksupports more flexible transformation and filtering operations. Experimentsshow that choosing different transformation orders can enrich the spectralrepresentation of point clouds and achieve excellent results in tasks such asfiltering and feature enhancement. Therefore, PMFHT not only expands thetheoretical framework of point cloud spectral analysis, but also provides apowerful new tool for manifold geometry processing.</description>
      <author>example@mail.com (Jiamian Li, Bing-Zhao Li)</author>
      <guid isPermaLink="false">2510.20842v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects</title>
      <link>http://arxiv.org/abs/2510.21585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at: https://brain-bzh.github.io/reve/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了REVE模型，一个专为EEG信号设计的基础模型，能够处理不同长度和电极排列的EEG信号，并在多种下游任务中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过大规模预训练减少了任务特定数据的依赖，在语言和视觉领域取得成功，但在EEG领域的应用滞后。公共EEG数据集的异质性（不同协议、设备和电极配置）导致现有EEG基础模型难以泛化，现有模型通常限制在单一设置下预训练，导致次优性能，特别是在线性探测下。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多样化EEG信号泛化的预训练模型REVE（Representation for EEG with Versatile Embeddings）。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新颖的4D位置编码方案，使其能够处理任意长度和电极排列的信号；使用掩码自编码目标函数进行预训练；在来自92个数据集、25,000名受试者的超过60,000小时EEG数据上预训练REVE，这是迄今为止最大的EEG预训练工作。&lt;h4&gt;主要发现&lt;/h4&gt;REVE在10个下游EEG任务上取得了最先进的结果，包括运动想象分类、癫痫检测、睡眠分期、认知负荷估计和情绪识别；几乎不需要微调的情况下，REVE展示了强大的泛化能力和细致的时空建模能力。&lt;h4&gt;结论&lt;/h4&gt;REVE为EEG信号处理提供了新的基础模型，能够处理多样化的EEG数据；研究团队发布了代码、预训练权重和教程，以支持标准化的EEG研究并加速临床神经科学的进展。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过大规模预训练减少了对任务特定数据的依赖，从而改变了人工智能领域。尽管在语言和视觉领域取得了成功，但由于公共数据集的异质性（收集于不同的协议、设备和电极配置下），它们在EEG中的应用一直滞后。现有的EEG基础模型难以在这些变化中泛化，通常将预训练限制在单一设置中，导致次优性能，特别是在线性探测下。我们提出了REVE（Representation for EEG with Versatile Embeddings），一个明确设计为能够泛化到多样化EEG信号的预训练模型。REVE引入了一种新颖的4D位置编码方案，使其能够处理任意长度和电极排列的信号。使用掩码自编码目标函数，我们在来自92个数据集、25,000名受试者的超过60,000小时EEG数据上预训练了REVE，这是迄今为止最大的EEG预训练工作。REVE在10个下游EEG任务上取得了最先进的结果，包括运动想象分类、癫痫检测、睡眠分期、认知负荷估计和情绪识别。几乎不需要微调的情况下，它展示了强大的泛化能力和细致的时空建模能力。我们发布了代码、预训练权重和教程，以支持标准化的EEG研究并加速临床神经科学的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have transformed AI by reducing reliance on task-specificdata through large-scale pretraining. While successful in language and vision,their adoption in EEG has lagged due to the heterogeneity of public datasets,which are collected under varying protocols, devices, and electrodeconfigurations. Existing EEG foundation models struggle to generalize acrossthese variations, often restricting pretraining to a single setup, resulting insuboptimal performance, in particular under linear probing. We present REVE(Representation for EEG with Versatile Embeddings), a pretrained modelexplicitly designed to generalize across diverse EEG signals. REVE introduces anovel 4D positional encoding scheme that enables it to process signals ofarbitrary length and electrode arrangement. Using a masked autoencodingobjective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasetsspanning 25,000 subjects, representing the largest EEG pretraining effort todate. REVE achieves state-of-the-art results on 10 downstream EEG tasks,including motor imagery classification, seizure detection, sleep staging,cognitive load estimation, and emotion recognition. With little to nofine-tuning, it demonstrates strong generalization, and nuanced spatio-temporalmodeling. We release code, pretrained weights, and tutorials to supportstandardized EEG research and accelerate progress in clinical neuroscience.</description>
      <author>example@mail.com (Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi)</author>
      <guid isPermaLink="false">2510.21585v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence</title>
      <link>http://arxiv.org/abs/2510.21406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 D&amp;B Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了多模态未修剪视频检索(MUVR)任务及相应基准数据集，旨在推进长视频平台上的视频检索技术。该研究构建了实用的检索范式、多层次视觉对应和全面的评估标准，并对多种先进模型进行了评估，揭示了当前方法在处理未修剪视频和多模态查询方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着长视频平台的普及，视频检索技术面临新的挑战。现有的视频检索方法主要针对修剪过的短视频，而长视频平台上的视频通常包含多个相关片段，需要更灵活的检索方式来满足用户需求。&lt;h4&gt;目的&lt;/h4&gt;提出并构建一个专门针对长视频平台的多模态未修剪视频检索任务和基准数据集，以促进该领域的研究和发展，并评估现有方法在这一新任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;设计了MUVR基准数据集，包含53K个来自Bilibili的未修剪视频、1,050个多模态查询和84K个匹配。构建了以视频为中心的多模态查询支持长文本描述、视频标签提示和掩码提示。建立了六个级别的多层次视觉对应标准（副本、事件、场景、实例、动作和其他）。开发了三个版本的评估基准（基础版、过滤版、问答版），并提出了重新排序分数评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，当前的视频检索模型在处理未修剪视频和多模态查询方面存在明显局限性；MLLMs在多视频理解和重新排序能力上也表现出不足，这为未来研究指明了方向。&lt;h4&gt;结论&lt;/h4&gt;MUVR基准为长视频平台上的视频检索研究提供了新的评估框架，揭示了现有方法的不足，并为未来改进提供了方向。该研究有助于推动多模态未修剪视频检索领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了多模态未修剪视频检索任务，并创建了一个新的基准(MUVR)以推进长视频平台的视频检索。MUVR旨在使用多模态查询检索包含相关片段的未修剪视频。它具有以下特点：1)实用的检索范式：MUVR支持以视频为中心的多模态查询，通过长文本描述、视频标签提示和掩码提示表达细粒度检索需求。它采用一对多检索范式，专注于未修剪视频，专为长视频平台应用定制。2)多层次视觉对应：为了涵盖常见的视频类别（如新闻、旅行、舞蹈）并精确定义检索匹配标准，我们基于用户感兴趣且想要检索的核心视频内容（如新闻事件、旅行地点、舞蹈动作）构建了多层次视觉对应。它涵盖六个级别：副本、事件、场景、实例、动作和其他。3)全面的评估标准：我们开发了3个版本的MUVR（即基础版、过滤版、问答版）。MUVR-Base/Filter评估检索模型，而MUVR-QA以问答格式评估MLLMs。我们还提出了重新排序分数来评估MLLMs的重新排序能力。MUVR包含来自Bilibili视频平台的53K个未修剪视频，有1,050个多模态查询和84K个匹配。我们对3个最先进的视频检索模型、6个基于图像的VLMs和10个MLLMs进行了广泛评估。MUVR揭示了检索方法在处理未修剪视频和多模态查询方面的局限性，以及MLLMs在多视频理解和重新排序方面的局限性。我们的代码和基准可在https://github.com/debby-0527/MUVR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Multi-modal Untrimmed Video Retrieval task, along with a newbenchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aimsto retrieve untrimmed videos containing relevant segments using multi-modalqueries. It has the following features: 1) Practical retrieval paradigm: MUVRsupports video-centric multi-modal queries, expressing fine-grained retrievalneeds through long text descriptions, video tag prompts, and mask prompts. Itadopts a one-to-many retrieval paradigm and focuses on untrimmed videos,tailored for long-video platform applications. 2) Multi-level visualcorrespondence: To cover common video categories (e.g., news, travel, dance)and precisely define retrieval matching criteria, we construct multi-levelvisual correspondence based on core video content (e.g., news events, travellocations, dance moves) which users are interested in and want to retrieve. Itcovers six levels: copy, event, scene, instance, action, and others. 3)Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QAassesses MLLMs in a question-answering format. We also propose a RerankingScore to evaluate the reranking ability of MLLMs. MUVR consists of 53Kuntrimmed videos from the video platform Bilibili, with 1,050 multi-modalqueries and 84K matches. Extensive evaluations of 3 state-of-the-art videoretrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR revealsthe limitations of retrieval methods in processing untrimmed videos andmulti-modal queries, as well as MLLMs in multi-video understanding andreranking. Our code and benchmark is available athttps://github.com/debby-0527/MUVR.</description>
      <author>example@mail.com (Yue Feng, Jinwei Hu, Qijia Lu, Jiawei Niu, Li Tan, Shuo Yuan, Ziyi Yan, Yizhen Jia, Qingzhi He, Shiping Ge, Ethan Q. Chen, Wentong Li, Limin Wang, Jie Qin)</author>
      <guid isPermaLink="false">2510.21406v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation</title>
      <link>http://arxiv.org/abs/2510.21026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 11 figures and 3 tables. Project page is available at  \url{https://irvlutd.github.io/HRT1/}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个新颖的人机轨迹传递系统，使机器人能够通过学习人类示范视频来操作物体，系统包含四个模块，实现了机器人观看一次人类示范后就能在不同环境中重复相同操作任务的能力。&lt;h4&gt;背景&lt;/h4&gt;机器人操作任务通常需要精确的编程和大量调整，而人类能够直观地通过示范学习操作任务。如何让机器人从人类示范中学习操作技能是一个重要研究方向。&lt;h4&gt;目的&lt;/h4&gt;开发一个系统，使机器人能够通过观看人类示范视频来学习操作任务，并能在不同环境中重复这些任务，即使物体放置方式与示范不同。&lt;h4&gt;方法&lt;/h4&gt;系统包含四个模块：1)使用AR头戴设备从机器人视角收集人类示范视频的数据收集模块；2)从示范视频中检测物体并提取3D人类手部轨迹的视频理解模块；3)将人类手部轨迹转换为机器人末端执行器参考轨迹的轨迹传递模块；4)利用轨迹优化算法解决机器人配置空间中轨迹问题的轨迹优化模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该系统能够使移动机械臂观看一次人类示范视频后，就能在不同的环境中重复相同的移动操作任务，即使物体放置方式与示范不同。&lt;h4&gt;结论&lt;/h4&gt;该系统有效地实现了从人类示范到机器人操作的技能传递，为机器人学习人类操作任务提供了一种直观、灵活的方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种新颖的人机轨迹传递系统，使机器人能够通过学习人类示范视频来操作物体。该系统由四个模块组成。第一个模块是数据收集模块，旨在使用AR头戴设备从机器人视角收集人类示范视频。第二个模块是视频理解模块，从示范视频中检测物体并提取3D人类手部轨迹。第三个模块将人类手部轨迹转换为3D空间中机器人末端执行器的参考轨迹。最后一个模块利用轨迹优化算法解决机器人配置空间中的轨迹问题，使其能够遵循从人类示范传递而来的末端执行器轨迹。因此，这些模块使机器人能够观看一次人类示范视频，然后在不同环境中重复相同的移动操作任务，即使物体放置方式与示范不同。我们在移动机械臂上进行了不同操作任务的实验，以验证我们系统的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让机器人通过观看一次人类演示视频，就能在不同环境中重复执行相同的移动操作任务。这个问题在现实中很重要，因为传统的机器人操作需要大量编程和调参，而现有的基于学习的方法通常需要大量机器人遥操作数据或多次演示，收集成本高。此外，现有方法在物体被手部遮挡时表现不佳，且大多不支持移动操作，限制了机器人在日常环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于人类演示的机器人操作方法，发现模仿学习方法需要多次演示，强化学习方法需要构建任务空间的数字孪生，而训练免费方法在物体遮挡或噪声处理方面存在局限。作者借鉴了多个现有工作：使用AR头显收集数据类似iTeach框架；使用GroundingDINO和SAMv2进行物体检测；使用HaMeR进行手部姿态估计；使用统一夹持器坐标系空间(UGCS)进行抓取转移；使用BundleSDF进行物体姿态估计。基于这些分析，作者设计了HRT1系统，专注于手部轨迹转移而非物体轨迹，并加入了轨迹优化算法以提高鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分析人类演示视频中的手部动作，将其转换为机器人的执行轨迹，使机器人能够一次性学习并执行相同的操作任务，即使在不同环境和物体摆放情况下也能成功。整体流程分为四个模块：1)数据收集模块：使用HoloLens 2从机器人视角收集人类演示视频；2)视频理解模块：检测物体并提取3D人类手部轨迹；3)人类到机器人抓取转移模块：使用UGCS表示将人类手部轨迹转换为机器人夹持器轨迹；4)任务执行的轨迹对齐模块：使用BundleSDF估计物体姿态变换，并通过两阶段轨迹优化算法(先优化机器人基座位置，再优化关节空间轨迹)使机器人精确执行任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于手部轨迹而非物体轨迹的转移，在物体被遮挡时更可靠；2)使用统一夹持器坐标系空间(UGCS)进行抓取转移，支持不同类型夹持器；3)两阶段轨迹优化算法，能处理转移轨迹中的噪声；4)支持移动操作，是首个支持移动的训练免费方法；5)改进的3D手部姿态估计，提高深度准确性。相比之前工作，HRT1不依赖物体姿态估计，使用轨迹优化而非简单逆运动学，支持移动操作，且只需一次演示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HRT1通过分析人类演示视频中的手部动作并转换为机器人执行轨迹，实现了机器人仅需观看一次演示就能在不同环境中成功执行移动操作任务的能力，显著提高了轨迹转移的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel system for human-to-robot trajectory transfer thatenables robots to manipulate objects by learning from human demonstrationvideos. The system consists of four modules. The first module is a datacollection module that is designed to collect human demonstration videos fromthe point of view of a robot using an AR headset. The second module is a videounderstanding module that detects objects and extracts 3D human-handtrajectories from demonstration videos. The third module transfers a human-handtrajectory into a reference trajectory of a robot end-effector in 3D space. Thelast module utilizes a trajectory optimization algorithm to solve a trajectoryin the robot configuration space that can follow the end-effector trajectorytransferred from the human demonstration. Consequently, these modules enable arobot to watch a human demonstration video once and then repeat the same mobilemanipulation task in different environments, even when objects are placeddifferently from the demonstrations. Experiments of different manipulationtasks are conducted on a mobile manipulator to verify the effectiveness of oursystem</description>
      <author>example@mail.com (Sai Haneesh Allu, Jishnu Jaykumar P, Ninad Khargonkar, Tyler Summers, Jian Yao, Yu Xiang)</author>
      <guid isPermaLink="false">2510.21026v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Integrated Bayesian State Space Models for Multimodal Time-Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.20952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了LLM集成的贝叶斯状态空间模型(LBS)，一种用于多模态时间预测的新概率框架，解决了现有方法在架构上的限制，实现了灵活的时间窗口、不确定性量化和改进的时间泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现实世界预测需要整合结构化时间序列数据与非结构化文本信息，但现有方法受固定输入/输出时间跨度限制，无法建模或量化不确定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时处理数值和文本信息的时间预测方法，提供灵活的时间窗口、不确定性量化和更好的时间泛化能力。&lt;h4&gt;方法&lt;/h4&gt;LBS包含两个主要组件：(1)状态空间模型(SSM)骨干，捕获生成数值和文本观测的潜在状态的时间动态；(2)预训练大型语言模型(LLM)，用于编码文本输入进行后验状态估计和解码文本预测。&lt;h4&gt;主要发现&lt;/h4&gt;LBS在TextTimeCorpus基准测试上比之前的最先进方法提高13.20%，同时为每个预测提供可读的人类摘要，实现了灵活的回看和预测窗口、原则性不确定性量化和改进的时间泛化。&lt;h4&gt;结论&lt;/h4&gt;该研究首次统一LLM和SSM进行数值和文本联合预测，为多模态时间推理提供了新的基础框架。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的预测需要整合结构化的时间序列数据和非结构化的文本信息，但现有方法在架构上受到固定输入/输出时间跨度的限制，无法建模或量化不确定性。我们通过引入LLM集成的贝叶斯状态空间模型(LBS)来解决这一挑战，这是一种用于多模态时间预测的新概率框架。总体而言，LBS包含两个组件：(1)状态空间模型(SSM)骨干，捕获生成数值和文本观测的潜在状态的时间动态；(2)预训练的大型语言模型(LLM)，调整后用于编码文本输入进行后验状态估计和解码与潜在轨迹一致的文本预测。这种设计能够提供灵活的回看和预测窗口，原则性的不确定性量化，并改善时间泛化能力。在TextTimeCorpus基准测试上的实验表明，LBS比之前的最先进方法提高13.20%，同时为每个预测提供可读的人类摘要。我们的工作是首次统一LLM和SSM进行数值和文本联合预测，为多模态时间推理提供了新的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting in the real world requires integrating structured time-seriesdata with unstructured textual information, but existing methods arearchitecturally limited by fixed input/output horizons and are unable to modelor quantify uncertainty. We address this challenge by introducingLLM-integrated Bayesian State space models (LBS), a novel probabilisticframework for multimodal temporal forecasting. At a high level, LBS consists oftwo components: (1) a state space model (SSM) backbone that captures thetemporal dynamics of latent states from which both numerical and textualobservations are generated and (2) a pretrained large language model (LLM) thatis adapted to encode textual inputs for posterior state estimation and decodetextual forecasts consistent with the latent trajectory. This design enablesflexible lookback and forecast windows, principled uncertainty quantification,and improved temporal generalization thanks to the well-suited inductive biasof SSMs toward modeling dynamical systems. Experiments on the TextTimeCorpusbenchmark demonstrate that LBS improves the previous state-of-the-art by 13.20%while providing human-readable summaries of each forecast. Our work is thefirst to unify LLMs and SSMs for joint numerical and textual prediction,offering a novel foundation for multimodal temporal reasoning.</description>
      <author>example@mail.com (Sungjun Cho, Changho Shin, Suenggwan Jo, Xinya Yan, Shourjo Aditya Chaudhuri, Frederic Sala)</author>
      <guid isPermaLink="false">2510.20952v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.20622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SeViCES框架，通过语义-视觉共识证据选择方法解决长视频理解中的挑战，在准确性和鲁棒性方面超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;长视频理解因其复杂、多样且时间分散的内容而具有挑战性。现有的Video-LLMs可处理数十分钟的视频，但应用于真正长序列时计算成本高，且推理往往不聚焦或不一致。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效可靠的长视频理解框架，解决现有方法中忽略时间依赖性和依赖单模态证据的局限性。&lt;h4&gt;方法&lt;/h4&gt;SeViCES是一个无需训练且与模型无关的框架，包含两个关键组件：(1)语义-视觉共识帧选择(SVCFS)模块，通过时间感知语义分支和聚类引导视觉分支选择信息量最大的帧；(2)答案共识精炼(ACR)模块，通过融合证据和约束答案空间解决语义和视觉预测间的不一致。&lt;h4&gt;主要发现&lt;/h4&gt;在长视频理解基准上的大量实验表明，SeViCES在准确性和鲁棒性方面均持续优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;共识驱动的证据选择对Video-LLMs的长视频理解能力至关重要。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解由于其复杂、多样且时间分散的内容而仍然具有挑战性。尽管视频大语言模型(Video-LLMs)可以处理长达数十分钟的视频，但将它们应用于真正长序列在计算上是禁止的，并且往往导致不聚焦或不一致的推理。一个有希望的解决方案是只选择信息量最大的帧，然而现有方法通常忽略时间依赖性或依赖单模态证据，限制了它们提供完整且与查询相关上下文的能力。我们提出了一个用于有效可靠长视频理解的语义-视觉共识证据选择(SeViCES)框架。SeViCES无需训练且与模型无关，并引入了两个关键组件。语义-视觉共识帧选择(SVCFS)模块通过(1)利用LLM对字幕进行推理的时间感知语义分支，和(2)通过互信息将嵌入与语义分数对齐的聚类引导视觉分支来选择帧。答案共识精炼(ACR)模块通过融合证据和约束答案空间，进一步解决基于语义和视觉的预测之间的一致性问题。在长视频理解基准上的大量实验表明，SeViCES在准确性和鲁棒性方面均持续优于最先进的方法，证明了共识驱动的证据选择对Video-LLMs的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video understanding remains challenging due to its complex, diverse, andtemporally scattered content. Although video large language models (Video-LLMs)can process videos lasting tens of minutes, applying them to truly longsequences is computationally prohibitive and often leads to unfocused orinconsistent reasoning. A promising solution is to select only the mostinformative frames, yet existing approaches typically ignore temporaldependencies or rely on unimodal evidence, limiting their ability to providecomplete and query-relevant context. We propose a Semantic-Visual ConsensusEvidence Selection (SeViCES) framework for effective and reliable long videounderstanding. SeViCES is training-free and model-agnostic, and introduces twokey components. The Semantic-Visual Consensus Frame Selection (SVCFS) moduleselects frames through (1) a temporal-aware semantic branch that leverages LLMreasoning over captions, and (2) a cluster-guided visual branch that alignsembeddings with semantic scores via mutual information. The Answer ConsensusRefinement (ACR) module further resolves inconsistencies between semantic- andvisual-based predictions by fusing evidence and constraining the answer space.Extensive experiments on long video understanding benchmarks show that SeViCESconsistently outperforms state-of-the-art methods in both accuracy androbustness, demonstrating the importance of consensus-driven evidence selectionfor Video-LLMs.</description>
      <author>example@mail.com (Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He)</author>
      <guid isPermaLink="false">2510.20622v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>
      <link>http://arxiv.org/abs/2510.20579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Open-o3 Video是一个非智能体框架，将显式时空证据整合到视频推理中，通过专门的数据集和训练策略实现了在多个视频理解基准测试上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大多数视频推理模型只生成文本推理轨迹而不指示关键证据出现的时间和位置。将图像证据中心推理扩展到视频更具挑战性，因为它需要在动态场景中联合时空跟踪和定位。&lt;h4&gt;目的&lt;/h4&gt;引入Open-o3 Video框架，解决视频推理中时空证据整合的挑战，通过收集训练数据和设计训练策略来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;创建了两个高质量数据集STGR-CoT-30k用于SFT和STGR-RL-36k用于RL，包含精心构建的时空注释；采用冷启动强化学习策略，使用多种专门设计的奖励来鼓励答案准确性、时间对齐和空间精度。&lt;h4&gt;主要发现&lt;/h4&gt;在V-STAR基准测试上，Open-o3 Video实现了最先进性能，相比Qwen2.5-VL基线，mAM提高14.4%，mLGM提高24.2%；在VideoMME、WorldSense、VideoMMMU和TVGBench等多个视频理解基准测试上观察到一致改进。&lt;h4&gt;结论&lt;/h4&gt;Open-o3 Video的推理轨迹为测试时扩展提供了有价值的信号，支持置信感知的验证，提高答案可靠性。&lt;h4&gt;翻译&lt;/h4&gt;大多数视频推理模型只生成文本推理轨迹而不指示关键证据出现的时间和位置。最近的模型如OpenAI-o3在图像证据中心推理方面引起了广泛兴趣，但将这种能力扩展到视频更具挑战性，因为它需要在动态场景中联合时空跟踪和定位。我们引入了Open-o3 Video，一个将显式时空证据整合到视频推理中的非智能体框架，并仔细收集训练数据和设计训练策略来解决上述挑战。模型在答案旁边突出显示关键时间戳、对象和边界框，使推理能够基于具体的视觉观察。为实现这一功能，我们首先策划并构建了两个高质量数据集：用于SFT的STGR-CoT-30k和用于RL的STGR-RL-36k，包含精心构建的时间和空间注释，因为大多数现有数据集只提供视频的时间跨度或图像的空间框，缺乏统一的时空监督和推理轨迹。然后，我们采用冷启动强化学习策略，使用多种专门设计的奖励，共同鼓励答案准确性、时间对齐和空间精度。在V-STAR基准测试上，Open-o3 Video实现了最先进的性能，相比Qwen2.5-VL基线，mAM提高了14.4%，mLGM提高了24.2%。在广泛的视频理解基准测试上，包括VideoMME、WorldSense、VideoMMMU和TVGBench，也观察到一致改进。除了准确性，Open-o3 Video产生的推理轨迹还为测试时扩展提供了有价值的信号，使置信感知的验证成为可能，并提高答案可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most video reasoning models only generate textual reasoning traces withoutindicating when and where key evidence appears. Recent models such as OpenAI-o3have sparked wide interest in evidence-centered reasoning for images, yetextending this ability to videos is more challenging, as it requires jointtemporal tracking and spatial localization across dynamic scenes. We introduceOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporalevidence into video reasoning, and carefully collect training data and designtraining strategies to address the aforementioned challenges. The modelhighlights key timestamps, objects, and bounding boxes alongside its answers,allowing reasoning to be grounded in concrete visual observations. To enablethis functionality, we first curate and build two high-quality datasets,STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructedtemporal and spatial annotations, since most existing datasets offer eithertemporal spans for videos or spatial boxes on images, lacking unifiedspatio-temporal supervision and reasoning traces. Then, we adopt a cold-startreinforcement learning strategy with multiple specially designed rewards thatjointly encourage answer accuracy, temporal alignment, and spatial precision.On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistentimprovements are also observed on a broad range of video understandingbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyondaccuracy, the reasoning traces produced by Open-o3 Video also provide valuablesignals for test-time scaling, enabling confidence-aware verification andimproving answer reliability.</description>
      <author>example@mail.com (Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang)</author>
      <guid isPermaLink="false">2510.20579v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</title>
      <link>http://arxiv.org/abs/2510.20470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Conan，一个基于证据的多步视频推理框架，通过识别上下文和证据帧、跨帧线索推理以及自适应决策机制，解决了现有视频推理方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;视频推理需要跨帧多步推理，对多模态大语言模型(MLLMs)仍是主要挑战。基于强化学习的方法依赖文本链导致结论缺乏基础，帧检索方法则难以准确进行证据定位。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效进行多步视频推理的框架，解决现有方法在推理准确性和证据定位方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建Conan-91K数据集，包含自动生成的推理轨迹；设计多阶段渐进式冷启动策略和识别-推理-行动(AIR)RLVR训练框架，共同增强多步视觉推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在六个多步推理基准测试上，Conan的准确性平均超过基线Qwen2.5-VL-7B-Instruct模型10%以上，达到最先进性能；且能有效泛化到长视频理解任务，展示出良好的可扩展性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Conan框架通过结合证据基础和多步推理，有效解决了视频推理中的挑战，实现了更准确可靠的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;视频推理需要跨帧多步推理，这对多模态大语言模型(MLLMs)仍然是一个主要挑战。虽然基于强化学习(RL)的方法增强了推理能力，但它们通常只依赖文本链，导致结论缺乏基础或产生幻觉。相反，帧检索方法引入了视觉基础，但仍然难以准确进行证据定位。为了解决这些挑战，我们提出了Conan，一个用于基于证据的多步视频推理框架。Conan能够识别上下文和证据帧，跨帧线索进行推理，并自适应地决定何时得出结论或进一步探索。为此，我们(1)构建了Conan-91K，这是一个大规模的自动生成推理轨迹数据集，包括帧识别、证据推理和行动决策，以及(2)设计了一个多阶段渐进式冷启动策略，结合识别-推理-行动(AIR)RLVR训练框架，共同增强多步视觉推理。在六个多步推理基准测试上的广泛实验表明，Conan在准确性上平均超过基线Qwen2.5-VL-7B-Instruct模型10%以上，达到了最先进的性能。此外，Conan能有效地泛化到长视频理解任务，验证了其强大的可扩展性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video reasoning, which requires multi-step deduction across frames, remains amajor challenge for multimodal large language models (MLLMs). Whilereinforcement learning (RL)-based methods enhance reasoning capabilities, theyoften rely on text-only chains that yield ungrounded or hallucinatedconclusions. Conversely, frame-retrieval approaches introduce visual groundingbut still struggle with inaccurate evidence localization. To address thesechallenges, we present Conan, a framework for evidence-grounded multi-stepvideo reasoning. Conan identifies contextual and evidence frames, reasons overcross-frame clues, and adaptively decides when to conclude or explore further.To achieve this, we (1) construct Conan-91K, a large-scale dataset ofautomatically generated reasoning traces that includes frame identification,evidence reasoning, and action decision, and (2) design a multi-stageprogressive cold-start strategy combined with anIdentification-Reasoning-Action (AIR) RLVR training framework to jointlyenhance multi-step visual reasoning. Extensive experiments on six multi-stepreasoning benchmarks demonstrate that Conan surpasses the baselineQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achievingstate-of-the-art performance. Furthermore, Conan generalizes effectively tolong-video understanding tasks, validating its strong scalability androbustness.</description>
      <author>example@mail.com (Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun)</author>
      <guid isPermaLink="false">2510.20470v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling</title>
      <link>http://arxiv.org/abs/2510.20302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InvDec的混合架构，用于多元时间序列预测，有效结合了时间建模和跨变量依赖关系建模，特别在高维数据集上表现优异。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列预测需要同时建模时间模式和跨变量依赖关系。现有方法存在局限性：通道独立方法如PatchTST擅长时间建模但忽略变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合架构，实现时间编码和变量级解码的原则性分离，以同时捕捉时间模式和跨变量依赖关系。&lt;h4&gt;方法&lt;/h4&gt;提出InvDec架构，结合基于补丁的时间编码器和通过变量级自注意力操作的倒置解码器；引入延迟变量嵌入，在时间编码后才丰富变量特定表示；采用自适应残差融合机制动态平衡时间信息和变量信息；将InvDec与PatchTST结合形成InvDec-PatchTST。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准测试上，InvDec-PatchTST在高维数据集上表现显著：Electricity数据集（321个变量）MSE降低20.9%，Weather数据集提升4.3%，Traffic数据集提升2.7%；在低维ETT数据集上保持竞争力；消融研究验证了各组件有效性；InvDec的优势随数据集维度增长而增长。&lt;h4&gt;结论&lt;/h4&gt;InvDec有效地结合了时间建模和跨变量相关性建模，特别适合高维数据集，随着变量数量增加，跨变量建模变得至关重要。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列预测需要同时建模时间模式和跨变量依赖关系。通道独立方法如PatchTST擅长时间建模但忽略了变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码。我们提出了InvDec（倒置解码器），一种混合架构，实现了时间编码和变量级解码的原则性分离。InvDec结合了基于补丁的时间编码器和一个通过变量级自注意力操作在变量维度上运行的倒置解码器。我们引入了延迟变量嵌入，仅在时间编码后丰富变量特定表示，保持时间特征完整性。自适应残差融合机制动态平衡不同维度数据集的时间信息和变量信息。将InvDec与PatchTST实例化得到InvDec-PatchTST。在七个基准测试上的广泛实验表明，在高维数据集上取得了显著提升：Electricity（321个变量）上MSE降低20.9%，Weather上提升4.3%，Traffic上提升2.7%，同时在低维ETT数据集上保持竞争力。消融研究验证了每个组件，分析显示InvDec的优势随数据集维度增长而增长，证实了随着变量数量增加，跨变量建模变得关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series forecasting requires simultaneously modelingtemporal patterns and cross-variate dependencies. Channel-independent methodssuch as PatchTST excel at temporal modeling but ignore variable correlations,while pure variate-attention approaches such as iTransformer sacrifice temporalencoding. We proposeInvDec (Inverted Decoder), a hybrid architecture thatachieves principled separation between temporal encoding and variate-leveldecoding. InvDec combines a patch-based temporal encoder with an inverteddecoder operating on the variate dimension through variate-wise self-attention.We introduce delayed variate embeddings that enrich variable-specificrepresentations only after temporal encoding, preserving temporal featureintegrity. An adaptive residual fusion mechanism dynamically balances temporaland variate information across datasets of varying dimensions. InstantiatingInvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on sevenbenchmarks demonstrate significant gains on high-dimensional datasets: 20.9%MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and2.7% gain on Traffic compared to PatchTST, while maintaining competitiveperformance on low-dimensional ETT datasets. Ablation studies validate eachcomponent, and analysis reveals that InvDec's advantage grows with datasetdimensionality, confirming that cross-variate modeling becomes critical as thenumber of variables increases.</description>
      <author>example@mail.com (Yuhang Wang)</author>
      <guid isPermaLink="false">2510.20302v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</title>
      <link>http://arxiv.org/abs/2510.20285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双模态反事实对比构建（DMC³）框架，用于解决以第一人称视角视频为基础的问题回答任务中的独特挑战，如理解多个事件和识别手部物体交互。&lt;h4&gt;背景&lt;/h4&gt;以第一人称视角视频为基础的问题回答（Egocentric VideoQA）在以第一人称视频理解中扮演着重要角色。现有的预训练和微调方法忽略了第一人称视角带来的独特挑战，如理解多个事件和识别手部物体交互。&lt;h4&gt;目的&lt;/h4&gt;为了解决第一人称视角视频理解中的独特挑战，特别是理解多个事件和识别手部物体交互，作者提出了一种新的框架DMC³。&lt;h4&gt;方法&lt;/h4&gt;DMC³框架包含三个主要部分：开发一个反事实样本构建模块，通过事件描述重述和核心交互挖掘分别为文本和视觉模态生成正负样本；将这些样本与原始样本一起输入基线模型；在反事实样本参与的对比优化模块中应用对比损失，最小化原始样本特征与正样本特征之间的距离，同时最大化与负样本的距离。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在EgoTaskQA数据集的normal和indirect分割上分别达到了52.51%和46.04%的性能，在QAEGO4D上达到了13.2%的性能，均达到了最先进的水平。&lt;h4&gt;结论&lt;/h4&gt;通过提出DMC³框架，有效解决了第一人称视角视频理解中的独特挑战，特别是在理解多个事件和识别手部物体交互方面取得了显著进展，并在多个基准测试中达到了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;以第一人称视频问答（Egocentric VideoQA）在以第一人称视频理解中发挥着重要作用，它指的是基于第一人称视频回答问题。尽管现有方法通过预训练和微调的范式已经取得了进展，但它们忽略了第一人称视角带来的独特挑战，如理解多个事件和识别手部物体交互。为了应对这些挑战，我们提出了一个双模态反事实对比构建（DMC³）框架，该框架包含一个以第一人称视频问答的基线模型、一个反事实样本构建模块和一个反事实样本参与的对比优化。具体来说，我们首先开发了一个反事实样本构建模块，通过事件描述重述和核心交互挖掘分别为文本和视觉模态生成正负样本。然后，我们将这些样本与原始样本一起输入基线模型。最后，在反事实样本参与的对比优化模块中，我们应用对比损失来最小化原始样本特征与正样本特征之间的距离，同时最大化与负样本的距离。实验表明，我们的方法在EgoTaskQA的normal和indirect分割上分别达到了52.51%和46.04%，在QAEGO4D上达到了13.2%，均达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755085&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Egocentric Video Question Answering (Egocentric VideoQA) plays an importantrole in egocentric video understanding, which refers to answering questionsbased on first-person videos. Although existing methods have made progressthrough the paradigm of pre-training and fine-tuning, they ignore the uniquechallenges posed by the first-person perspective, such as understandingmultiple events and recognizing hand-object interactions. To deal with thesechallenges, we propose a Dual-Modal Counterfactual Contrastive Construction(DMC$^3$) framework, which contains an egocentric videoqa baseline, acounterfactual sample construction module and a counterfactual sample-involvedcontrastive optimization. Specifically, We first develop a counterfactualsample construction module to generate positive and negative samples fortextual and visual modalities through event description paraphrasing and coreinteraction mining, respectively. Then, We feed these samples together with theoriginal samples into the baseline. Finally, in the counterfactualsample-involved contrastive optimization module, we apply contrastive loss tominimize the distance between the original sample features and the positivesample features, while maximizing the distance from the negative samples.Experiments show that our method achieve 52.51\% and 46.04\% on the\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% onQAEGO4D, both reaching the state-of-the-art performance.</description>
      <author>example@mail.com (Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu)</author>
      <guid isPermaLink="false">2510.20285v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching</title>
      <link>http://arxiv.org/abs/2510.20178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PPMStereo的新方法，通过引入内存缓冲区和两阶段决策过程（选择和播放），实现了从立体视频中估计时间上一致的深度信息，在保持计算效率的同时提高了时空一致性。&lt;h4&gt;背景&lt;/h4&gt;从立体视频中估计时间上一致的深度信息对增强现实等实际应用至关重要，因为深度估计的不一致会破坏用户沉浸感。然而，这项任务具有挑战性，因为很难以计算高效的方式建模长期的时间一致性。之前的方法在时空建模的广度和计算效率之间存在权衡。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决立体视频中时间一致深度估计的挑战，特别是如何在保持计算效率的同时建模长程时空一致性，开发一种能够实现高效信息聚合且保持深度估计时间一致性的方法。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种名为PPMStereo的方法，引入了内存缓冲区用于建模长程时空一致性。受人类两阶段决策过程的启发，PPMStereo包含一个'选择'过程（识别最相关的帧）和一个'播放'过程（为时空聚合自适应地加权所选帧），形成一种两阶段协作过程。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验验证了PPMStereo的有效性，表明其在准确性和时间一致性方面达到了最先进的性能。在Sintel clean/final数据集上，PPMStereo实现了0.62/1.11 TEPE的性能，比BiDAStereo分别提高了17.3%和9.02%，且计算成本更低。&lt;h4&gt;结论&lt;/h4&gt;PPMStereo通过创新的内存缓冲区和两阶段决策过程，成功解决了立体视频中时间一致深度估计的挑战，在保持计算效率的同时提高了时空一致性，为增强现实等实际应用提供了更可靠的深度估计技术。&lt;h4&gt;翻译&lt;/h4&gt;从立体视频中估计时间上一致的深度信息对于增强现实等实际应用至关重要，因为不一致的深度估计会破坏用户的沉浸感。尽管如此，由于难以以计算高效的方式建模长期时间一致性，这项任务仍然具有挑战性。先前的方法试图通过聚合时空信息来解决这一问题，但面临一个基本的权衡：有限的时空建模只能带来适度的提升，而捕捉长程依赖关系则会显著增加计算成本。为了解决这一限制，我们引入了一个内存缓冲区，用于建模长程时空一致性，同时实现高效的动态立体匹配。受人类两阶段决策过程的启发，我们提出了一种用于动态立体匹配的选择并播放记忆(PPM)构建模块，称为PPMStereo。PPM包括一个'选择'过程，用于识别最相关的帧，以及一个'播放'过程，用于为时空聚合自适应地加权所选帧。这种两阶段协作过程保持了一个紧凑但信息丰富的内存缓冲区，同时实现了时间上一致的信息聚合。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态场景中的立体匹配问题，即在视频序列中保持深度估计的时间一致性，避免出现闪烁和模糊等不一致现象。这个问题在现实中非常重要，因为像增强现实、自动驾驶和机器人等应用需要时间一致的深度估计来提供稳定可靠的用户体验，而不一致的深度估计会严重影响用户体验和系统性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：一些方法使用小时间窗口限制了信息传播，而扩大窗口则计算成本过高且不考虑帧可靠性差异。作者借鉴了人类决策的两阶段过程（'选择'和'播放'），并参考了视频任务中的记忆模型（如XMem和RMem），但针对立体匹配任务进行了专门改进。作者设计了一个质量评估模块来评估帧的价值，并采用动态选择机制来构建高效的记忆缓冲区。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入记忆缓冲区来建模长期时空一致性，同时保持计算效率。方法包含两个关键过程：1）'选择'过程：使用质量评估模块识别最相关的K帧，评估标准包括置信度、冗余性和相似性；2）'播放'过程：通过动态记忆调制机制自适应加权选定帧的特征，并使用注意力机制读取记忆缓冲区。整体流程包括特征提取、成本体积构建、上下文编码、记忆缓冲区更新和迭代细化等步骤，通过GRU模块逐步优化视差估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首次将记忆缓冲区引入动态立体匹配任务，实现高效长期建模；2）提出'选择和播放'记忆构建方法，动态选择并加权关键帧；3）引入质量评估模块联合评估帧的置信度、冗余性和相似性；4）设计动态记忆调制机制自适应调整特征权重。相比之前工作，PPMStereo不再使用固定窗口或平等对待所有帧，而是根据帧质量和相关性动态选择和加权，在保持计算效率的同时显著提高了时间一致性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PPMStereo通过创新的'选择和播放'记忆构建机制，实现了在计算高效的同时保持时间一致性的动态立体匹配，显著提高了深度估计的准确性和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporally consistent depth estimation from stereo video is critical forreal-world applications such as augmented reality, where inconsistent depthestimation disrupts the immersion of users. Despite its importance, this taskremains challenging due to the difficulty in modeling long-term temporalconsistency in a computationally efficient manner. Previous methods attempt toaddress this by aggregating spatio-temporal information but face a fundamentaltrade-off: limited temporal modeling provides only modest gains, whereascapturing long-range dependencies significantly increases computational cost.To address this limitation, we introduce a memory buffer for modelinglong-range spatio-temporal consistency while achieving efficient dynamic stereomatching. Inspired by the two-stage decision-making process in humans, wepropose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) constructionmodule for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPMconsists of a `pick' process that identifies the most relevant frames and a`play' process that weights the selected frames adaptively for spatio-temporalaggregation. This two-stage collaborative process maintains a compact yethighly informative memory buffer while achieving temporally consistentinformation aggregation. Extensive experiments validate the effectiveness ofPPMStereo, demonstrating state-of-the-art performance in both accuracy andtemporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on theSintel clean/final (17.3\% \&amp; 9.02\% improvements over BiDAStereo) with fewercomputational costs. Codes are available at\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.</description>
      <author>example@mail.com (Yun Wang, Junjie Hu, Qiaole Dong, Yongjian Zhang, Yanwei Fu, Tin Lun Lam, Dapeng Wu)</author>
      <guid isPermaLink="false">2510.20178v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency</title>
      <link>http://arxiv.org/abs/2510.19980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 4 figures. Accepted as Spotlight poster in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对时间序列预测中的冗余特征学习问题，提出了一种名为AMRC的创新解决方案，通过动态掩码损失和表示一致性约束提高了预测性能，挑战了传统的时间序列建模假设。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在能源管理和金融市场等关键领域发挥着重要作用。尽管基于深度学习的方法（如MLP、RNN、Transformer）已取得显著进展，但现有的'长序列信息增益假设'存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在解决现有模型在训练过程中学习大量冗余特征（如噪声或不相关波动）的问题，从而影响有效信号提取，提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'具有表示一致性的自适应掩码损失'（AMRC）的创新解决方案，包含两个核心组件：1) 动态掩码损失，自适应识别高判别性时间段以指导梯度下降；2) 表示一致性约束，稳定输入、标签和预测之间的映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过系统实验发现了一个反直觉现象：适当截断历史数据可以 paradoxically 提高预测准确性，表明现有模型在训练过程中学习了大量冗余特征，损害了有效信号的提取。&lt;h4&gt;结论&lt;/h4&gt;AMRC能有效抑制冗余特征学习，同时显著提高模型性能。这项工作不仅挑战了时间建模中的传统假设，还为开发高效和稳健的预测模型提供了新的理论见解和方法突破。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在能源管理和金融市场等关键领域发挥着关键作用。尽管基于深度学习的方法（如MLP、RNN、Transformer）已取得显著进展，但现有的'长序列信息增益假设'存在固有局限性。通过系统实验，本研究揭示了一个反直觉现象：适当截断历史数据可以 paradoxically 提高预测准确性，表明现有模型在训练过程中学习了大量冗余特征（如噪声或不相关波动），从而损害了有效信号的提取。基于信息瓶颈理论，我们提出了一种名为'具有表示一致性的自适应掩码损失'（AMRC）的创新解决方案，包含两个核心组件：1) 动态掩码损失，在模型训练过程中自适应识别高判别性时间段以指导梯度下降；2) 表示一致性约束，稳定输入、标签和预测之间的映射关系。实验结果表明，AMRC能有效抑制冗余特征学习，同时显著提高模型性能。这项工作不仅挑战了时间建模中的传统假设，还为开发高效和稳健的预测模型提供了新的理论见解和方法突破。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting plays a pivotal role in critical domains such asenergy management and financial markets. Although deep learning-basedapproaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, theprevailing "long-sequence information gain hypothesis" exhibits inherentlimitations. Through systematic experimentation, this study reveals acounterintuitive phenomenon: appropriately truncating historical data canparadoxically enhance prediction accuracy, indicating that existing modelslearn substantial redundant features (e.g., noise or irrelevant fluctuations)during training, thereby compromising effective signal extraction. Buildingupon information bottleneck theory, we propose an innovative solution termedAdaptive Masking Loss with Representation Consistency (AMRC), which featurestwo core components: 1) Dynamic masking loss, which adaptively identifiedhighly discriminative temporal segments to guide gradient descent during modeltraining; 2) Representation consistency constraint, which stabilized themapping relationships among inputs, labels, and predictions. Experimentalresults demonstrate that AMRC effectively suppresses redundant feature learningwhile significantly improving model performance. This work not only challengesconventional assumptions in temporal modeling but also provides noveltheoretical insights and methodological breakthroughs for developing efficientand robust forecasting models.</description>
      <author>example@mail.com (Renzhao Liang, Sizhe Xu, Chenggang Xie, Jingru Chen, Feiyang Ren, Shu Yang, Takahiro Yabe)</author>
      <guid isPermaLink="false">2510.19980v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks</title>
      <link>http://arxiv.org/abs/2510.20683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Spikachu是一种基于脉冲神经网络的脑机接口解码框架，具有可扩展性、因果性和高能效性，解决了现有方法在实时应用和能源效率方面的限制。&lt;h4&gt;背景&lt;/h4&gt;脑机接口对神经运动障碍人群具有重要意义，但现有解码方法要么简单但缺乏泛化能力，要么复杂但难以实时应用，且都依赖于高能耗的人工神经网络，难以集成到资源有限的现实系统中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于脉冲神经网络的、可扩展、因果且节能的神经解码框架，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;Spikachu直接处理分箱脉冲，将其投影到共享潜在空间，利用适应输入时序的脉冲模块提取特征，然后整合这些潜在表示并解码生成行为预测。研究在6只非人灵长类动物的113个记录会话（总计43小时）上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与因果基线相比，Spikachu在使用少2.26到418.81倍能源的情况下表现更好；将训练扩展到多个会话和受试者可提高性能，并实现向未见过的会话、受试者和任务的少样本迁移。&lt;h4&gt;结论&lt;/h4&gt;Spikachu引入了一种基于SNN的可扩展、在线兼容的神经解码框架，其性能与最先进模型相当，同时能耗低几个数量级。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口(BCIs)有望为神经运动障碍个体实现言语和假肢控制等关键功能。其成功的关键是神经解码器，即将神经活动映射到预期行为的模型。当前基于学习的解码方法分为两类：简单但缺乏泛化能力的因果模型，或复杂但难以实时应用的非因果模型。两者都面临一个共同挑战，它们依赖于能耗高的人工神经网络骨干，这使得集成到现实世界的资源有限系统中变得困难。脉冲神经网络(SNNs)提供了一种有前景的替代方案。由于它们以因果方式运行，这些模型适合实时使用，并且它们的低能耗需求使其成为电池受限环境的理想选择。为此，我们引入了Spikachu：一种基于SNN的可扩展、因果且节能的神经解码框架。我们的方法通过将分箱脉冲直接投影到共享潜在空间来处理它们，在该空间中，适应输入时序的脉冲模块提取相关特征；然后将这些潜在表示整合和解码以生成行为预测。我们在6只非人灵长类动物的113个记录会话上评估了我们的方法，总计43小时的记录。与因果基线相比，我们的方法在使用少2.26到418.81倍能源的情况下表现更好。此外，我们证明将训练扩展到多个会话和受试者可以提高性能，并实现向未见过的会话、受试者和任务的少样本迁移。总体而言，Spikachu引入了一种基于SNN的可扩展、在线兼容的神经解码框架，其性能与最先进模型相当，同时能耗低几个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-computer interfaces (BCIs) promise to enable vital functions, such asspeech and prosthetic control, for individuals with neuromotor impairments.Central to their success are neural decoders, models that map neural activityto intended behavior. Current learning-based decoding approaches fall into twoclasses: simple, causal models that lack generalization, or complex, non-causalmodels that generalize and scale offline but struggle in real-time settings.Both face a common challenge, their reliance on power-hungry artificial neuralnetwork backbones, which makes integration into real-world, resource-limitedsystems difficult. Spiking neural networks (SNNs) offer a promisingalternative. Because they operate causally these models are suitable forreal-time use, and their low energy demands make them ideal forbattery-constrained environments. To this end, we introduce Spikachu: ascalable, causal, and energy-efficient neural decoding framework based on SNNs.Our approach processes binned spikes directly by projecting them into a sharedlatent space, where spiking modules, adapted to the timing of the input,extract relevant features; these latent representations are then integrated anddecoded to generate behavioral predictions. We evaluate our approach on 113recording sessions from 6 non-human primates, totaling 43 hours of recordings.Our method outperforms causal baselines when trained on single sessions usingbetween 2.26 and 418.81 times less energy. Furthermore, we demonstrate thatscaling up training to multiple sessions and subjects improves performance andenables few-shot transfer to unseen sessions, subjects, and tasks. Overall,Spikachu introduces a scalable, online-compatible neural decoding frameworkbased on SNNs, whose performance is competitive relative to state-of-the-artmodels while consuming orders of magnitude less energy.</description>
      <author>example@mail.com (Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale)</author>
      <guid isPermaLink="false">2510.20683v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
  <item>
      <title>Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists</title>
      <link>http://arxiv.org/abs/2510.20158v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对关节式自行车和骑行者的类别级8D姿态估计方法，能够从单张RGB图像估计自行车的3D平移、旋转以及转向手柄和踏板相对于车身的旋转，从而提供更细粒度的自行车姿态状态和行驶方向估计。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶中，骑行者属于安全关键类弱势道路使用者(VRU)，准确估计其姿态对过马路意图分类、行为预测和碰撞避免至关重要。与刚性物体不同，关节式自行车由通过关节连接的可移动刚性部件组成，6D姿态方法在自行车转向/踏板角度变化时变得不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够估计自行车完整关节状态的方法，包括3D位置、旋转以及转向手柄和踏板的相对旋转，以提供更准确的骑行者行为预测和碰撞避免能力。&lt;h4&gt;方法&lt;/h4&gt;提出联合估计关节式自行车8D姿态和3D关键点的模型，使用合成和真实图像数据的混合进行训练，以在真实图像上实现良好的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的8D姿态估计方法能够提供更细粒度的自行车姿态状态和行驶方向估计，与使用刚性规范对象模板的最先进6D姿态估计器相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该方法在处理关节式自行车姿态变化方面表现出色，能够更好地理解骑行者的实际行驶意图，为自动驾驶系统提供更可靠的骑行者行为预测能力。&lt;h4&gt;翻译&lt;/h4&gt;在自动驾驶中，骑行者属于安全关键类弱势道路使用者(VRU)，准确估计他们的姿态对于骑行者过马路意图分类、行为预测和碰撞避免至关重要。与刚性物体不同，关节式自行车由通过关节连接的可移动刚性部件组成，并受运动学结构约束。6D姿态方法可以估计刚性自行车的3D旋转和平移，但当自行车的转向/踏板角度变化时，6D方法变得不足。这是因为：1)自行车关节姿态的变化会导致其3D边界框也发生变化，2)3D框的方向不一定与决定实际预期行驶方向的转向方向对齐。在这项工作中，我们介绍了一种针对关节式自行车和骑行者的类别级8D姿态估计方法，可以从单张RGB图像进行估计。除了能够从单张图像估计自行车的3D平移和旋转外，我们的方法还估计其转向手柄和踏板相对于自行车车身的旋转。这两个新参数能够估计更细粒度的自行车姿态状态和行驶方向。我们提出的模型联合估计关节式自行车的8D姿态和3D关键点，并使用合成和真实图像数据的混合进行训练，以在真实图像上泛化。我们包含了一个评估部分，评估了估计的8D姿态参数的准确性，与使用刚性规范对象模板进行匹配的最先进类别级6D姿态估计器相比，我们的方法取得了具有竞争力的分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Autonomous Driving, cyclists belong to the safety-critical class ofVulnerable Road Users (VRU), and accurate estimation of their pose is criticalfor cyclist crossing intention classification, behavior prediction, andcollision avoidance. Unlike rigid objects, articulated bicycles are composed ofmovable rigid parts linked by joints and constrained by a kinematic structure.6D pose methods can estimate the 3D rotation and translation of rigid bicycles,but 6D becomes insufficient when the steering/pedals angles of the bicyclevary. That is because: 1) varying the articulated pose of the bicycle causesits 3D bounding box to vary as well, and 2) the 3D box orientation is notnecessarily aligned to the orientation of the steering which determines theactual intended travel direction. In this work, we introduce a method forcategory-level 8D pose estimation for articulated bicycles and cyclists from asingle RGB image. Besides being able to estimate the 3D translation androtation of a bicycle from a single image, our method also estimates therotations of its steering handles and pedals with respect to the bicycle bodyframe. These two new parameters enable the estimation of a more fine-grainedbicycle pose state and travel direction. Our proposed model jointly estimatesthe 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mixof synthetic and real image data to generalize on real images. We include anevaluation section where we evaluate the accuracy of our estimated 8D poseparameters, and our method shows promising results by achieving competitivescores when compared against state-of-the-art category-level 6D pose estimatorsthat use rigid canonical object templates for matching.</description>
      <author>example@mail.com (Eduardo R. Corral-Soto, Yang Liu, Yuan Ren, Bai Dongfeng, Liu Bingbing)</author>
      <guid isPermaLink="false">2510.20158v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</title>
      <link>http://arxiv.org/abs/2510.19789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OmniMotion-X，一个用于全身人体运动生成的多模态框架，采用自回归扩散变换器以统一的序列到序列方式工作。该框架支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势以及全局时空控制场景，并能灵活组合这些任务。&lt;h4&gt;背景&lt;/h4&gt;人体运动生成领域需要能够处理多种模态输入并生成连贯、可控运动的系统。现有方法在处理多模态任务组合和保持生成内容一致性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的多模态框架，能够处理多种运动生成任务，提高生成内容的一致性、风格和时序动态，并实现长时间的真实、连贯、可控运动生成。&lt;h4&gt;方法&lt;/h4&gt;使用自回归扩散变换器作为核心架构，引入参考运动作为条件信号增强一致性，采用渐进式弱到强混合条件训练策略处理多模态冲突，并构建了OmniMoCap-X数据集，整合28个公开MoCap来源，使用GPT-4o自动生成结构化字幕。&lt;h4&gt;主要发现&lt;/h4&gt;OmniMotion-X在多个多模态任务上显著超越现有方法，实现了最先进的性能，能够生成交互式的真实、连贯、可控的长时间运动。&lt;h4&gt;结论&lt;/h4&gt;OmniMotion-X通过统一的多模态框架和创新的条件训练策略，解决了人体运动生成中的多模态整合问题，为动画制作和人机交互提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了OmniMotion-X，一个用于全身人体运动生成的多模态框架，利用自回归扩散变换器以统一的序列到序列方式工作。OmniMotion-X有效支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势和全局时空控制场景（如运动预测、中间帧生成、补全和关节/轨迹引导合成），以及这些任务的灵活组合。具体而言，我们提出使用参考运动作为新的条件信号，显著提高了生成内容、风格和时序动态的一致性，这对真实动画至关重要。为处理多模态冲突，我们引入了渐进式弱到强混合条件训练策略。为支持高质量多模态训练，我们构建了OmniMoCap-X，这是迄今为止最大的统一多模态运动数据集，整合了10个不同任务中的28个公开MoCap来源，标准化为30fps的SMPL-X格式。为确保详细且一致的标注，我们将序列渲染为视频并使用GPT-4o自动生成结构化和层次化字幕，捕捉低级行动和高层语义。大量实验评估证实，OmniMotion-X显著超越现有方法，在多个多模态任务上展示了最先进的性能，并能实现真实、连贯、可控的长时间运动的交互式生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces OmniMotion-X, a versatile multimodal framework forwhole-body human motion generation, leveraging an autoregressive diffusiontransformer in a unified sequence-to-sequence manner. OmniMotion-X efficientlysupports diverse multimodal tasks, including text-to-motion, music-to-dance,speech-to-gesture, and global spatial-temporal control scenarios (e.g., motionprediction, in-betweening, completion, and joint/trajectory-guided synthesis),as well as flexible combinations of these tasks. Specifically, we propose theuse of reference motion as a novel conditioning signal, substantially enhancingthe consistency of generated content, style, and temporal dynamics crucial forrealistic animations. To handle multimodal conflicts, we introduce aprogressive weak-to-strong mixed-condition training strategy. To enablehigh-quality multimodal training, we construct OmniMoCap-X, the largest unifiedmultimodal motion dataset to date, integrating 28 publicly available MoCapsources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.To ensure detailed and consistent annotations, we render sequences into videosand use GPT-4o to automatically generate structured and hierarchical captions,capturing both low-level actions and high-level semantics. Extensiveexperimental evaluations confirm that OmniMotion-X significantly surpassesexisting methods, demonstrating state-of-the-art performance across multiplemultimodal tasks and enabling the interactive generation of realistic,coherent, and controllable long-duration motions.</description>
      <author>example@mail.com (Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu)</author>
      <guid isPermaLink="false">2510.19789v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling</title>
      <link>http://arxiv.org/abs/2510.19364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is submitted to IEEE International Conference on Robotics  and Automation (ICRA) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种有效的概率框架，用于处理非结构化越野环境中机器人运动预测的不确定性，通过建模地形参数的空间相关偶然不确定性并传播到轨迹预测中，显著提高了预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;不确定性感知的机器人运动预测对于非结构化越野环境中的下游可通行性估计和自主导航至关重要，因为在这种环境中地形是异构的且感知不确定性很高。&lt;h4&gt;目的&lt;/h4&gt;引入一个有效的概率框架，明确地对地形参数的空间相关偶然不确定性建模为概率世界模型，并通过可微分物理引擎传播这种不确定性，实现概率轨迹预测。&lt;h4&gt;方法&lt;/h4&gt;利用结构化卷积算子，提供高分辨率多变量预测，同时保持可管理的计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集上的实验评估显示，与偶然不确定性估计基线相比，该方法的不确定性估计和轨迹预测准确性显著提高。&lt;h4&gt;结论&lt;/h4&gt;通过明确建模和传播空间相关的偶然不确定性，该方法能够提供更可靠的机器人轨迹预测，适用于复杂的越野环境。&lt;h4&gt;翻译&lt;/h4&gt;不确定性感知的机器人运动预测对于非结构化越野环境中的下游可通行性估计和安全自主导航至关重要，在这种环境中地形是异构的且感知不确定性很高。大多数现有方法假设确定性或空间独立的地面不确定性，忽略了3D空间数据的固有局部相关性，并且通常产生不可靠的预测。在这项工作中，我们引入了一个有效的概率框架，明确地将地形参数的空间相关偶然不确定性建模为概率世界模型，并通过可微分物理引擎传播这种不确定性以实现概率轨迹预测。通过利用结构化卷积算子，我们的方法在可管理的计算成本下提供了高分辨率的多变量预测。在公开可用数据集上的实验评估表明，与偶然不确定性估计基线相比，不确定性估计和轨迹预测准确性显著提高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人越野导航中不确定性感知的运动预测问题。具体来说，现有方法大多假设地形是确定性的或空间独立的不确定性，忽略了3D空间数据的固有局部相关性，导致预测不可靠。这个问题在现实中非常重要，因为越野环境地形异构、感知不确定性高，不确定性感知的运动预测对可通行性评估和安全的自主导航至关重要，而传统方法忽略了关键的物理地形参数、空间相关性和环境不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括确定性方法、空间独立的不确定性估计方法和物理信息方法。他们借鉴了MonoForce的物理信息世界建模框架，但扩展了它以包含空间相关的不确定性。方法设计考虑了计算效率，通过结构化卷积算子避免显式构造高维协方差矩阵，结合深度学习与可微分物理引擎，实现了从感知到轨迹预测的端到端不确定性传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个端到端的概率框架，显式建模地形参数上的空间相关偶然不确定性，并通过可微分物理引擎传播这种不确定性用于概率轨迹预测，利用结构化卷积算子以可管理的计算成本提供高分辨率多变量预测。整体流程：1)输入车载摄像头图像；2)使用Lift-Splat-Shoot将特征投影到鸟瞰图网格；3)通过卷积层预测地形参数的平均图和方差图；4)将方差图与固定高斯核卷积形成结构化多元高斯分布；5)从概率世界模型中采样；6)通过可微分物理引擎进行轨迹预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)具有空间相关偶然不确定性的地形端到端概率世界建模；2)基于卷积的结构化协方差估计的可扩展损失公式；3)通过可微分物理引擎进行不确定性感知的轨迹预测；4)在真实世界数据集上的广泛验证。相比之前工作，不同之处在于：与确定性方法相比显式建模了不确定性；与空间独立方法相比考虑了空间相关性；与现有物理信息方法相比在模型中明确包含空间相关不确定性；与高斯过程等方法相比通过结构化卷积提高了计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ProTerrain通过引入基于结构化卷积的高效概率框架，解决了越野导航中地形参数空间相关不确定性建模的挑战，实现了从感知到轨迹预测的端到端不确定性传播，显著提高了机器人在复杂环境中的导航安全性和可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Uncertainty-aware robot motion prediction is crucial for downstreamtraversability estimation and safe autonomous navigation in unstructured,off-road environments, where terrain is heterogeneous and perceptualuncertainty is high. Most existing methods assume deterministic or spatiallyindependent terrain uncertainties, ignoring the inherent local correlations of3D spatial data and often producing unreliable predictions. In this work, weintroduce an efficient probabilistic framework that explicitly models spatiallycorrelated aleatoric uncertainty over terrain parameters as a probabilisticworld model and propagates this uncertainty through a differentiable physicsengine for probabilistic trajectory forecasting. By leveraging structuredconvolutional operators, our approach provides high-resolution multivariatepredictions at manageable computational cost. Experimental evaluation on apublicly available dataset shows significantly improved uncertainty estimationand trajectory prediction accuracy over aleatoric uncertainty estimationbaselines.</description>
      <author>example@mail.com (Golnaz Raja, Ruslan Agishev, Miloš Prágr, Joni Pajarinen, Karel Zimmermann, Arun Kumar Singh, Reza Ghabcheloo)</author>
      <guid isPermaLink="false">2510.19364v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>HumanCM: One Step Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2510.16709v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HumanCM是一种基于一致性模型的人体运动预测框架，能够实现高效的单步生成，无需依赖多步去噪过程。&lt;h4&gt;背景&lt;/h4&gt;现有基于扩散模型的人体运动预测方法通常需要多步去噪过程，计算效率较低。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的单步人体运动预测框架，在减少计算步骤的同时保持或提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;HumanCM学习嘈杂和清洁运动状态之间的自一致映射，采用基于Transformer的时空架构，使用时间嵌入来建模长程依赖并保持运动一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM能够达到与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少最多两个数量级。&lt;h4&gt;结论&lt;/h4&gt;HumanCM是一种高效的人体运动预测方法，能够在单步生成中实现高准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了HumanCM，一种基于一致性模型构建的单步人体运动预测框架。HumanCM不依赖于扩散模型中的多步去噪，而是通过学习嘈杂和清洁运动状态之间的自一致映射，执行高效的单步生成。该框架采用基于Transformer的时空架构，使用时间嵌入来建模长程依赖并保持运动一致性。在Human3.6M和HumanEva-I上的实验表明，HumanCM能够达到与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少最多两个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present HumanCM, a one-step human motion prediction framework built uponconsistency models. Instead of relying on multi-step denoising as indiffusion-based methods, HumanCM performs efficient single-step generation bylearning a self-consistent mapping between noisy and clean motion states. Theframework adopts a Transformer-based spatiotemporal architecture with temporalembeddings to model long-range dependencies and preserve motion coherence.Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achievescomparable or superior accuracy to state-of-the-art diffusion models whilereducing inference steps by up to two orders of magnitude.</description>
      <author>example@mail.com (Liu Haojie, Gao Suixiang)</author>
      <guid isPermaLink="false">2510.16709v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.20795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于贝叶斯图深度学习的框架，用于从宇宙微波背景图中估算原始磁场宇宙学参数，并结合贝叶斯神经网络实现不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;深度学习已成为现代宇宙学中的变革性方法，为从复杂数据集中提取物理信息提供了强大工具。&lt;h4&gt;目的&lt;/h4&gt;实现一种新颖的贝叶斯图深度学习框架，用于从模拟的宇宙微波背景(CMB)图中直接估算原始磁场(PMF)宇宙学中的关键宇宙学参数。&lt;h4&gt;方法&lt;/h4&gt;使用DeepSphere球形卷积神经网络架构，通过HEALPix像素化尊重CMB数据的球形几何特性，并集成贝叶斯神经网络(BNNs)来捕获偶然性和认知性不确定性，实现稳健的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法表现出卓越的性能，在磁场参数估计中实现了超过0.89的R²分数，并通过方差缩放和GPNormal后训练技术获得了校准良好的不确定性估计。&lt;h4&gt;结论&lt;/h4&gt;集成的DeepSphere-BNNs框架不仅提供了来自带有PMF贡献的CMB图的准确参数估计，还提供了可靠的不确定性量化，为精确宇宙学时代的稳健宇宙学推理提供了必要工具。&lt;h4&gt;翻译&lt;/h4&gt;深度学习已成为现代宇宙学中的变革性方法，为从复杂数据集中提取有意义的物理信息提供了强大工具。本文实现了一种新颖的贝叶斯图深度学习框架，用于从模拟的宇宙微波背景(CMB)图中直接估算原始磁场(PMF)宇宙学中的关键宇宙学参数。我们的方法利用了DeepSphere，这是一种专门设计用于通过HEALPix像素化尊重CMB数据球形几何形状的球形卷积神经网络架构。为了超越确定性点估计并实现稳健的不确定性量化，我们将贝叶斯神经网络(BNNs)集成到框架中，捕获反映模型对其预测置信度的偶然性和认知性不确定性。所提出的方法表现出卓越的性能，在磁场参数估计中实现了超过0.89的R²分数。我们通过方差缩放和GPNormal等后训练技术获得了校准良好的不确定性估计。这种集成的DeepSphere-BNNs框架不仅提供了来自带有PMF贡献的CMB图的准确参数估计，还提供了可靠的不确定性量化，为精确宇宙学时代的稳健宇宙学推理提供了必要工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has emerged as a transformative methodology in moderncosmology, providing powerful tools to extract meaningful physical informationfrom complex astronomical datasets. This paper implements a novel Bayesiangraph deep learning framework for estimating key cosmological parameters in aprimordial magnetic field (PMF) cosmology directly from simulated CosmicMicrowave Background (CMB) maps. Our methodology utilizes DeepSphere, aspherical convolutional neural network architecture specifically designed torespect the spherical geometry of CMB data through HEALPix pixelization. Toadvance beyond deterministic point estimates and enable robust uncertaintyquantification, we integrate Bayesian Neural Networks (BNNs) into theframework, capturing aleatoric and epistemic uncertainties that reflect themodel confidence in its predictions. The proposed approach demonstratesexceptional performance, achieving $R^{2}$ scores exceeding 0.89 for themagnetic parameter estimation. We further obtain well-calibrated uncertaintyestimates through post-hoc training techniques including Variance Scaling andGPNormal. This integrated DeepSphere-BNNs framework not only delivers accurateparameter estimation from CMB maps with PMF contributions but also providesreliable uncertainty quantification, providing the necessary tools for robustcosmological inference in the era of precision cosmology.</description>
      <author>example@mail.com (Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado)</author>
      <guid isPermaLink="false">2510.20795v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages</title>
      <link>http://arxiv.org/abs/2510.20739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何使用机器学习技术优先处理程序分析工具报告的漏洞，通过在Node.js包上的实验表明，机器学习模型能有效减少人工审查工作量，同时保持高准确率。&lt;h4&gt;背景&lt;/h4&gt;程序分析工具会产生大量候选漏洞报告，需要昂贵的人工审查，这给安全分析师带来了如何优先处理最可能是真实漏洞的报告的实际挑战。&lt;h4&gt;目的&lt;/h4&gt;研究机器学习是否可以应用于优先处理程序分析工具报告的漏洞，以减轻安全分析师的工作负担。&lt;h4&gt;方法&lt;/h4&gt;收集了1,883个Node.js包的基准测试数据集，每个包包含一个报告的ACE或ACI漏洞；评估了多种机器学习方法，包括经典模型、图神经网络、大型语言模型以及混合模型；所有模型都基于动态程序分析工具的输出数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;顶级大型语言模型达到F1分数0.915，最佳图神经网络和经典机器学习模型达到F1分数0.904；在低于7%的假阴性率下，领先模型可消除66.9%的良性包无需人工审查，每个包处理时间约60毫秒；当模型调整为0.8精度水平时，可检测99.2%的可利用污染流，仅遗漏0.8%。&lt;h4&gt;结论&lt;/h4&gt;该机器学习方法在现实世界漏洞分类中显示出强大的潜力，能显著减少人工审查工作量，同时保持高检测率。&lt;h4&gt;翻译&lt;/h4&gt;程序分析工具经常产生大量候选漏洞报告，需要昂贵的人工审查，这带来了一个实际挑战：安全分析师如何优先处理最可能是真实漏洞的报告？本文研究了机器学习是否可以应用于优先处理程序分析工具报告的漏洞。我们专注于Node.js包，收集了1,883个Node.js包的基准测试，每个包包含一个报告的ACE或ACI漏洞。我们评估了多种机器学习方法，包括经典模型、图神经网络、大型语言模型以及结合GNN和LLMs的混合模型，这些模型都基于动态程序分析工具的输出数据进行训练。顶级LLM达到F1分数0.915，而最佳GNN和经典ML模型达到F1分数0.904。在低于7%的假阴性率下，领先模型消除了66.9%需要人工审查的良性包，每个包处理时间约60毫秒。如果将最佳模型调整为在0.8的精度水平下运行（即在所有警告中允许20%的假阳性），我们的方法可以检测99.2%的可利用污染流，仅遗漏0.8%，这表明在现实世界漏洞分类方面具有强大的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Program analysis tools often produce large volumes of candidate vulnerabilityreports that require costly manual review, creating a practical challenge: howcan security analysts prioritize the reports most likely to be truevulnerabilities?  This paper investigates whether machine learning can be applied toprioritizing vulnerabilities reported by program analysis tools. We focus onNode.js packages and collect a benchmark of 1,883 Node.js packages, eachcontaining one reported ACE or ACI vulnerability. We evaluate a variety ofmachine learning approaches, including classical models, graph neural networks(GNNs), large language models (LLMs), and hybrid models that combine GNN andLLMs, trained on data based on a dynamic program analysis tool's output. Thetop LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML modelsreaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leadingmodel eliminates 66.9% of benign packages from manual review, taking around 60ms per package. If the best model is tuned to operate at a precision level of0.8 (i.e., allowing 20% false positives amongst all warnings), our approach candetect 99.2% of exploitable taint flows while missing only 0.8%, demonstratingstrong potential for real-world vulnerability triage.</description>
      <author>example@mail.com (Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang)</author>
      <guid isPermaLink="false">2510.20739v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series</title>
      <link>http://arxiv.org/abs/2510.20718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 27 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了半导体制造中的异常预测问题，提出了两种新方法：基于N-BEATS的单变量预测模型和基于图神经网络(GNN)的多变量关系模型。研究显示GNN在性能和效率上均优于N-BEATS模型，为制造环境中的在线异常预测提供了有前景的解决方案。&lt;h4&gt;背景&lt;/h4&gt;半导体制造是极其复杂且精度要求高的过程，涉及数千个相互依赖的参数。多变量时间序列分析在此类环境中至关重要，但异常预测面临传感器数据高维度、真实故障稀有导致的类别不平衡，以及变量间复杂相互依赖关系等挑战。&lt;h4&gt;目的&lt;/h4&gt;推动半导体制造领域从异常检测向异常预测发展，为实现实时工艺校正和主动故障预防提供技术支持。&lt;h4&gt;方法&lt;/h4&gt;提出包含两个阶段的异常预测框架：首先在假设无异常的数据集上训练预测模型，然后对未见时间序列数据进行预测。两种方法差异在于：第一种使用N-BEATS模型进行单变量时间序列预测，假设变量间独立；第二种使用图神经网络捕捉变量间关系，取消独立性假设。&lt;h4&gt;主要发现&lt;/h4&gt;两种模型在20个时间点的预测范围内表现出色，并在50个时间点范围内保持稳定的异常预测能力。GNN持续优于N-BEATS模型，同时需要更少的可训练参数和更低的计算成本。&lt;h4&gt;结论&lt;/h4&gt;图神经网络(GNN)作为在线异常预测解决方案具有显著优势，适合在制造环境中部署，为半导体制造业的实时监控和故障预防提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;半导体制造是一个极其复杂且精度要求高的过程，其特点是在各种工具和工艺步骤中收集数千个相互依赖的参数。多变量时间序列分析已成为这类环境中实时监控和故障检测的关键领域。然而，半导体制造中的异常预测面临几个关键挑战，包括传感器数据的高维度和由于真实故障的稀有性导致的严重类别不平衡。此外，变量之间复杂的相互依赖关系使异常预测和根本原因分析都变得复杂。本文提出了两种新颖的方法，推动该领域从异常检测向异常预测发展，这是实现实时工艺校正和主动故障预防的关键步骤。提出的异常预测框架包含两个主要阶段：在假设不含异常的数据集上训练预测模型，然后对未见的时间序列数据进行预测。将预测结果与训练信号的预测进行比较，超出预定阈值的偏差被标记为异常。这两种方法在采用的预测模型上有所不同。第一种利用N-BEATS模型进行单变量时间序列预测，假设变量间相互独立。第二种利用图神经网络捕捉变量间关系，取消了这一假设。两种模型在长达20个时间点的预测范围内都表现出强大的预测性能，并在长达50个时间点的范围内保持稳定的异常预测能力。图神经网络始终优于N-BEATS模型，同时需要显著更少的可训练参数和更低的计算成本。这些结果将图神经网络定位为一种有前景的在线异常预测解决方案，适合在制造环境中部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing is an extremely complex and precision-drivenprocess, characterized by thousands of interdependent parameters collectedacross diverse tools and process steps. Multi-variate time-series analysis hasemerged as a critical field for real-time monitoring and fault detection insuch environments. However, anomaly prediction in semiconductor fabricationpresents several critical challenges, including high dimensionality of sensordata and severe class imbalance due to the rarity of true faults. Furthermore,the complex interdependencies between variables complicate both anomalyprediction and root-cause-analysis. This paper proposes two novel approaches toadvance the field from anomaly detection to anomaly prediction, an essentialstep toward enabling real-time process correction and proactive faultprevention. The proposed anomaly prediction framework contains two main stages:(a) training a forecasting model on a dataset assumed to contain no anomalies,and (b) performing forecast on unseen time series data. The forecast iscompared with the forecast of the trained signal. Deviations beyond apredefined threshold are flagged as anomalies. The two approaches differ in theforecasting model employed. The first assumes independence between variables byutilizing the N-BEATS model for univariate time series forecasting. The secondlifts this assumption by utilizing a Graph Neural Network (GNN) to captureinter-variable relationships. Both models demonstrate strong forecastingperformance up to a horizon of 20 time points and maintain stable anomalyprediction up to 50 time points. The GNN consistently outperforms the N-BEATSmodel while requiring significantly fewer trainable parameters and lowercomputational cost. These results position the GNN as promising solution foronline anomaly forecasting to be deployed in manufacturing environments.</description>
      <author>example@mail.com (Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder)</author>
      <guid isPermaLink="false">2510.20718v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: GRaph-based Addiction Care prEdiction</title>
      <link>http://arxiv.org/abs/2510.20671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GRACE的图神经网络框架，用于自动确定成瘾患者的适当护理场所，解决了传统方法中存在的类别不平衡问题，并在真实数据中取得显著改进。&lt;h4&gt;背景&lt;/h4&gt;确定成瘾患者的适当护理场所是影响治疗效果和资源利用的关键临床决策。由于专科治疗资源不足，且现有决策方法在成瘾数据集中存在严重类别不平衡问题，需要开发自动化框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化框架来确定成瘾患者的适当护理场所，并解决成瘾数据集中的类别不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出新的图神经网络框架GRACE，将护理场所预测形式化为结构化学习问题；进行广泛特征工程；提出获取无偏元图的新方法训练图神经网络以克服类别不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据上的实验结果表明，与竞争基线相比，少数类别的F1分数提高了11-35%。&lt;h4&gt;结论&lt;/h4&gt;GRACE框架在解决成瘾患者护理场所预测问题上有效，特别是在处理类别不平衡问题时表现优异。&lt;h4&gt;翻译&lt;/h4&gt;确定成瘾患者的适当护理场所是影响患者治疗效果和资源有效利用的最关键临床决策之一。由于缺乏足够的专科治疗资源，如住院床位或人员，开发自动化框架的需求尚未得到满足。当前的决策方法在成瘾数据集中存在严重的类别不平衡问题。为解决这一限制，我们提出了一个新的图神经网络框架，将护理场所预测形式化为一个结构化学习问题。此外，我们进行了广泛的特征工程，并提出了一种获得无偏元图的新方法来训练图神经网络，以克服类别不平衡问题。真实世界数据的实验结果表明，与竞争基线相比，少数类别的F1分数提高了11-35%。代码和注释嵌入可在https://anonymous.4open.science/r/GRACE-F8E1/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Determining the appropriate locus of care for addiction patients is one ofthe most critical clinical decisions that affects patient treatment outcomesand effective use of resources. With a lack of sufficient specialized treatmentresources, such as inpatient beds or staff, there is an unmet need to developan automated framework for the same. Current decision-making approaches sufferfrom severe class imbalances in addiction datasets. To address this limitation,we propose a novel graph neural network (GRACE) framework that formalizes locusof care prediction as a structured learning problem. Further, we performextensive feature engineering and propose a new approach of obtaining anunbiased meta-graph to train a GNN to overcome the class imbalance problem.Experimental results in real-world data show an improvement of 11-35% in termsof the F1 score of the minority class over competitive baselines. The codes andnote embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.</description>
      <author>example@mail.com (Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee)</author>
      <guid isPermaLink="false">2510.20671v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting</title>
      <link>http://arxiv.org/abs/2510.20591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的电网拓扑优化(NTO)加速方法，用于解决输电网拥堵管理问题。该方法通过母线分裂技术，能够实现大规模系统的近实时优化，并具有良好的泛化能力和跨系统可转移性。&lt;h4&gt;背景&lt;/h4&gt;电网拓扑优化(NTO)通过母线分裂可以缓解输电网拥堵并减少重新调度成本。然而，对于大规模系统，使用现有求解器解决这种混合整数非线性问题在近实时情况下目前是不可行的。虽然机器学习方法是一种有前景的替代方案，但它们对未见的拓扑、变化的运行条件和不同系统的泛化能力有限，限制了其实际应用。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在开发一种能够实现大规模系统近实时电网拓扑优化的方法，同时解决现有方法在泛化能力和跨系统可转移性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;本文考虑线性化交流潮流(AC PF)，将NTO公式化为拥堵管理问题，并提出了一种图神经网络(GNN)加速方法。研究人员开发了一种异构边缘感知消息传递神经网络，以预测有效的母线分裂行为作为候选NTO解决方案。该方法能够捕获局部流模式，实现对未见的拓扑变化的泛化，并提高跨系统的可转移性。&lt;h4&gt;主要发现&lt;/h4&gt;案例研究显示，所提出的GNN方法在速度上提高了4个数量级，能够在GOC 2000母线系统上一分钟内提供交流可行解，优化间隙为2.3%。这表明该方法在近实时优化方面取得了显著进展。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，所提出的GNN方法在具有拓扑和跨系统泛化能力的大规模系统近实时NTO方面取得了重要进展，为解决电网拥堵管理问题提供了新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;电网拓扑优化(NTO)通过母线分裂可以缓解输电网拥堵并减少重新调度成本。然而，使用现有求解器解决大规模系统的这种混合整数非线性问题在近实时情况下目前是不可行的。机器学习方法已成为一种有前景的替代方案，但它们对未见的拓扑、变化的运行条件和不同系统的泛化能力有限，这限制了它们的实际应用。本文考虑线性化交流潮流(AC PF)，将NTO公式化为拥堵管理问题，并提出了一种图神经网络(GNN)加速方法。我们开发了一种异构边缘感知消息传递神经网络，以预测有效的母线分裂行为作为候选NTO解决方案。所提出的GNN捕获局部流模式，实现对未见的拓扑变化的泛化，并提高了跨系统的可转移性。案例研究显示速度提高了4个数量级，在GOC 2000母线系统上在一分钟内提供交流可行解，优化间隙为2.3%。这些结果表明，在具有拓扑和跨系统泛化能力的大规模系统近实时NTO方面取得了重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network topology optimization (NTO) via busbar splitting can mitigatetransmission grid congestion and reduce redispatch costs. However, solving thismixed-integer non-linear problem for large-scale systems in near-real-time iscurrently intractable with existing solvers. Machine learning (ML) approacheshave emerged as a promising alternative, but they have limited generalizationto unseen topologies, varying operating conditions, and different systems,which limits their practical applicability. This paper formulates NTO forcongestion management problem considering linearized AC PF, and proposes agraph neural network (GNN)-accelerated approach. We develop a heterogeneousedge-aware message passing NN to predict effective busbar splitting actions ascandidate NTO solutions. The proposed GNN captures local flow patterns,achieves generalization to unseen topology changes, and improvestransferability across systems. Case studies show up to 4 orders-of-magnitudespeed-up, delivering AC-feasible solutions within one minute and a 2.3%optimality gap on the GOC 2000-bus system. These results demonstrate asignificant step toward near-real-time NTO for large-scale systems withtopology and cross-system generalization.</description>
      <author>example@mail.com (Ali Rajaei, Peter Palensky, Jochen L. Cremer)</author>
      <guid isPermaLink="false">2510.20591v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics</title>
      <link>http://arxiv.org/abs/2510.20556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 5 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图重连是减轻图神经网络和图变换器中过度压缩的关键技术，通过修改图拓扑改善信息流动，但会改变图结构，可能扭曲重要拓扑信号。本研究首次系统分析了重连对图结构指标的影响及其与下游任务性能的关系。&lt;h4&gt;背景&lt;/h4&gt;图重连已成为减轻图神经网络（GNNs）和图变换器（Graph Transformers）中过度压缩的关键技术，通过修改图拓扑来改善信息流动。然而，重连会改变图的结构，有扭曲重要拓扑相关信号的风险。&lt;h4&gt;目的&lt;/h4&gt;提供关于重连如何影响各种图结构指标的系统性分析，以及这些变化如何与下游任务性能相关联，明确需要保留哪些结构属性以确保性能提升和结构保真度。&lt;h4&gt;方法&lt;/h4&gt;研究七种不同的重连策略，并将局部和全局图属性的变化与节点分类准确性进行关联分析。&lt;h4&gt;主要发现&lt;/h4&gt;成功的重连方法倾向于保留局部结构，同时在全局连接方面保持灵活性。这一模式在研究中呈现出一致性。&lt;h4&gt;结论&lt;/h4&gt;这些发现为设计有效的重连策略提供了新的见解，弥合了图理论与实际GNN优化之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;图重连已成为减轻图神经网络（GNNs）和图变换器（Graph Transformers）中过度压缩的关键技术，通过修改图拓扑来改善信息流动。虽然有效，但重连本质上改变了图的结构，带来了扭曲重要拓扑相关信号的风险。然而，尽管重连的使用日益增多，人们尚不清楚需要保留哪些结构属性以确保性能提升和结构保真度。在本工作中，我们首次提供了关于重连如何影响各种图结构指标的系统性分析，以及这些变化如何与下游任务性能相关联。我们研究了七种不同的重连策略，并将局部和全局图属性的变化与节点分类准确性进行关联。我们的结果揭示了一个一致的规律：成功的重连方法倾向于保留局部结构，同时在全局连接方面保持灵活性。这些发现为设计有效的重连策略提供了新的见解，弥合了图理论与实际GNN优化之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph rewiring has emerged as a key technique to alleviate over-squashing inGraph Neural Networks (GNNs) and Graph Transformers by modifying the graphtopology to improve information flow. While effective, rewiring inherentlyalters the graph's structure, raising the risk of distorting importanttopology-dependent signals. Yet, despite the growing use of rewiring, little isknown about which structural properties must be preserved to ensure bothperformance gains and structural fidelity. In this work, we provide the firstsystematic analysis of how rewiring affects a range of graph structuralmetrics, and how these changes relate to downstream task performance. We studyseven diverse rewiring strategies and correlate changes in local and globalgraph properties with node classification accuracy. Our results reveal aconsistent pattern: successful rewiring methods tend to preserve localstructure while allowing for flexibility in global connectivity. These findingsoffer new insights into the design of effective rewiring strategies, bridgingthe gap between graph theory and practical GNN optimization.</description>
      <author>example@mail.com (Alexandre Benoit, Catherine Aitken, Yu He)</author>
      <guid isPermaLink="false">2510.20556v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach</title>
      <link>http://arxiv.org/abs/2510.20454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的网球比赛预测方法，专门处理非传递性玩家优势现象，并发现博彩市场在此类比赛中存在效率低下的问题。&lt;h4&gt;背景&lt;/h4&gt;非传递性玩家优势（即A击败B，B击败C，但C击败A）在网球比赛中很常见，但很少有预测方法能处理这种复杂关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够建模和利用非传递性关系的预测方法，以识别和利用博彩市场中的效率低下点。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络方法，通过时序有向图建模玩家关系，其中玩家作为节点，历史比赛结果作为有向边。&lt;h4&gt;主要发现&lt;/h4&gt;博彩公司Pinnacle Sports在处理高非传递性复杂度的比赛时表现不佳；基于图的方法能有效捕捉这些场景中的关系动态；在1903次投注中，使用Kelly下注策略获得了3.26%的显著正回报率。&lt;h4&gt;结论&lt;/h4&gt;博彩市场在处理非传递性比赛时存在效率低下的问题，而所提出的图神经网络方法能够成功利用这种市场效率低下问题。&lt;h4&gt;翻译&lt;/h4&gt;非传递性玩家优势，即玩家A击败B，B击败C，但C击败A，在竞争性网球比赛中很常见。然而，很少有已知的方法尝试将其纳入预测方法中。我们通过图神经网络方法解决了这个问题，该方法通过时序有向图明确建模这些非传递性关系，玩家作为节点，他们的历史比赛结果作为有向边。我们发现博彩公司Pinnacle Sports在处理高非传递性复杂度的比赛时表现不佳，并认为我们的基于图的方法能够捕捉这些场景中的关系动态。当有选择地使用我们的模型对高非传递性比赛进行下注时（准确率65.7%，0.215 Brier分数），在1903次投注中，使用Kelly下注策略获得了3.26%的显著正回报率，表明市场在处理非传递性比赛时存在效率低下的问题，而我们的方法成功地利用了这一点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intransitive player dominance, where player A beats B, B beats C, but C beatsA, is common in competitive tennis. Yet, there are few known attempts toincorporate it within forecasting methods. We address this problem with a graphneural network approach that explicitly models these intransitive relationshipsthrough temporal directed graphs, with players as nodes and their historicalmatch outcomes as directed edges. We find the bookmaker Pinnacle Sports poorlyhandles matches with high intransitive complexity and posit that ourgraph-based approach is uniquely positioned to capture relational dynamics inthese scenarios. When selectively betting on higher intransitivity matchupswith our model (65.7% accuracy, 0.215 Brier Score), we achieve significantpositive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting amarket inefficiency in handling intransitive matchups that our approachsuccessfully exploits.</description>
      <author>example@mail.com (Lawrence Clegg, John Cartlidge)</author>
      <guid isPermaLink="false">2510.20454v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization</title>
      <link>http://arxiv.org/abs/2510.20295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种不需要不变风险最小化框架的方法，用于捕获因果子图，解决分布偏移下图神经网络的分布外泛化挑战。通过识别因果子图在不同环境中具有较小分布变化的特性，建立了不变分布标准，并基于此提出了范数引导的不变分布目标方法，实验证明该方法优于现有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;分布偏移下图神经网络的分布外泛化仍然是一个关键挑战。现有方法通常采用不变风险最小化框架，需要昂贵的环境注释或启发式生成的合成分割。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要不变风险最小化框架的方法，用于捕获因果子图，以克服现有方法的限制。&lt;h4&gt;方法&lt;/h4&gt;基于不变分布标准，系统地揭示分布偏移与表示范数之间的定量关系，用于识别因果子图；提出一种范数引导的不变分布目标方法，用于因果子图发现和预测。&lt;h4&gt;主要发现&lt;/h4&gt;因果子图在不同环境中表现出比非因果成分小得多的分布变化，这被形式化为不变分布标准，并从理论上得到了证明。&lt;h4&gt;结论&lt;/h4&gt;在两个广泛使用的基准测试上，所提出的方法在图泛化任务中始终优于现有的最先进方法，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在分布偏移下的分布外泛化对于图神经网络仍然是一个关键挑战。现有方法通常采用不变风险最小化框架，需要昂贵的环境注释或启发式生成的合成分割。为避免这些限制，本文旨在开发一种不需要不变风险最小化的方法来捕获因果子图。我们首先确定因果子图在不同环境中表现出比非因果成分小得多的分布变化，这被形式化为不变分布标准，并在本文中从理论上进行了证明。基于这一标准，我们系统地揭示了分布偏移与表示范数之间的定量关系，用于识别因果子图，并深入研究了其潜在机制。最后，我们通过引入一个基于范数的不变分布目标，提出了一种不需要不变风险最小化的方法，用于因果子图发现和预测。在两个广泛使用的基准测试上的大量实验表明，我们的方法在图泛化方面始终优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution generalization under distributional shifts remains acritical challenge for graph neural networks. Existing methods generally adoptthe Invariant Risk Minimization (IRM) framework, requiring costly environmentannotations or heuristically generated synthetic splits. To circumvent theselimitations, in this work, we aim to develop an IRM-free method for capturingcausal subgraphs. We first identify that causal subgraphs exhibit substantiallysmaller distributional variations than non-causal components across diverseenvironments, which we formalize as the Invariant Distribution Criterion andtheoretically prove in this paper. Building on this criterion, wesystematically uncover the quantitative relationship between distributionalshift and representation norm for identifying the causal subgraph, andinvestigate its underlying mechanisms in depth. Finally, we propose an IRM-freemethod by introducing a norm-guided invariant distribution objective for causalsubgraph discovery and prediction. Extensive experiments on two widely usedbenchmarks demonstrate that our method consistently outperformsstate-of-the-art methods in graph generalization.</description>
      <author>example@mail.com (Yang Qiu, Yixiong Zou, Jun Wang, Wei Liu, Xiangyu Fu, Ruixuan Li)</author>
      <guid isPermaLink="false">2510.20295v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction</title>
      <link>http://arxiv.org/abs/2510.20236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为层到层知识混合(LKM)的自知识蒸馏方法，能够在不显著增加计算复杂性的情况下提高图神经网络(GNNs)预测分子性质的准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)是目前预测分子性质最有效的方法，但仍需要更准确的模型。增加模型复杂度可以提高准确性，但也会增加训练和推理过程中的计算成本和内存需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种提高GNN准确性的方法，同时不显著增加计算复杂性和内存需求。&lt;h4&gt;方法&lt;/h4&gt;提出层到层知识混合(LKM)方法，通过最小化GNN层现有隐藏嵌入之间的平均绝对距离，有效聚合多跳和多尺度信息，改进局部和全局分子特征的表示。&lt;h4&gt;主要发现&lt;/h4&gt;LKM方法在不显著增加训练和推理复杂性的情况下，提高了最先进GNN的准确性。使用三种不同的GNN架构(DimeNet++、MXMNet和PAMNet)和三个数据集(QM9、MD17和Chignolin)进行评估，LKM将量子化学和生物物理性质预测的平均绝对误差分别降低了最高9.8%(QM9)、45.3%(MD17能量)和22.9%(Chignolin)。&lt;h4&gt;结论&lt;/h4&gt;LKM有潜力显著提高GNN预测化学性质的准确性，而不会显著增加训练和推理成本。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是目前预测分子性质最有效的方法，但仍需要更准确的模型。通过增加模型复杂度可以提高GNN的准确性，但这也会增加训练和推理过程中的计算成本和内存需求。在本研究中，我们开发了层到层知识混合(LKM)，一种新颖的自知识蒸馏方法，它在提高最先进GNN准确性的同时，在训练和推理过程中只增加了微不足道的计算复杂度。通过最小化GNN层现有隐藏嵌入之间的平均绝对距离，LKM有效地聚合了多跳和多尺度信息，实现了对局部和全局分子特征的改进表示。我们使用三种不同的GNN架构(DimeNet++、MXMNet和PAMNet)和量子化学性质数据集(QM9、MD17和Chignolin)评估了LKM。我们发现LKM方法将量子化学和生物物理性质预测的平均绝对误差分别降低了高达9.8%(QM9)、45.3%(MD17能量)和22.9%(Chignolin)。这项工作证明了LKM在不显著增加训练和推理成本的情况下，显著提高GNN化学性质预测准确性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are the currently most effective methods forpredicting molecular properties but there remains a need for more accuratemodels. GNN accuracy can be improved by increasing the model complexity butthis also increases the computational cost and memory requirement duringtraining and inference. In this study, we develop Layer-to-Layer KnowledgeMixing (LKM), a novel self-knowledge distillation method that increases theaccuracy of state-of-the-art GNNs while adding negligible computationalcomplexity during training and inference. By minimizing the mean absolutedistance between pre-existing hidden embeddings of GNN layers, LKM efficientlyaggregates multi-hop and multi-scale information, enabling improvedrepresentation of both local and global molecular features. We evaluated LKMusing three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) usingdatasets of quantum chemical properties (QM9, MD17 and Chignolin). We foundthat the LKM method effectively reduces the mean absolute error of quantumchemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM tosignificantly improve the accuracy of GNNs for chemical property predictionwithout any substantial increase in training and inference cost.</description>
      <author>example@mail.com (Teng Jiek See, Daokun Zhang, Mario Boley, David K. Chalmers)</author>
      <guid isPermaLink="false">2510.20236v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Extending machine learning model for implicit solvation to free energy calculations</title>
      <link>http://arxiv.org/abs/2510.20103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络的隐式溶剂模型LSNN，通过结合力匹配和化学变量导数匹配，实现了与显式溶剂模型相当的自由能预测准确性，同时提高了计算效率，为药物发现应用奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;隐式溶剂方法在分子模拟中计算效率高，但与显式溶剂模型相比准确性不足，限制了其在精确热力学计算中的应用。基于机器学习的方法目前主要依靠力匹配，可能导致能量预测相差任意常数，不适合绝对自由能比较。&lt;h4&gt;目的&lt;/h4&gt;开发更精确的隐式溶剂势能，解决当前基于机器学习的方法仅依靠力匹配的缺点，确保不同化学物种的溶剂化自由能可以进行有意义的比较。&lt;h4&gt;方法&lt;/h4&gt;引入基于图神经网络(GNN)的隐式溶剂模型Lambda Solvation Neural Network (LSNN)，除了力匹配外，还训练网络匹配化学变量的导数。在约30万个小分子的数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;LSNN实现了与显式溶剂化学模拟相当的自由能预测准确性，同时提供了计算速度提升，为药物发现中的未来应用建立了基础框架。&lt;h4&gt;结论&lt;/h4&gt;LSNN克服了传统隐式溶剂方法的局限性，通过结合力匹配和化学变量导数匹配，确保了不同化学物种的溶剂化自由能可以进行有意义的比较。&lt;h4&gt;翻译&lt;/h4&gt;隐式溶剂方法为分子模拟中的溶剂化效应建模提供了计算效率高的框架。然而，与显式溶剂模型相比，其准确性往往不足，限制了其在精确热力学计算中的应用。机器学习(ML)的最新进展提供了一种克服这些局限性的机会，通过利用神经网络为各种应用开发更精确的隐式溶剂势能。当前基于ML方法的一个主要缺点是其仅依靠力匹配，这可能导致能量预测相差任意常数，因此不适合绝对自由能比较。在此，我们介绍了一种新颖的方法，即基于图神经网络(GNN)的隐式溶剂模型，称为Lambda Solvation Neural Network (LSNN)。除了力匹配外，该网络还被训练以匹配化学变量的导数，确保不同化学物种的溶剂化自由能可以进行有意义的比较。在约30万个小分子的数据集上训练后，LSNN实现了与显式溶剂化学模拟相当的自由能预测准确性，同时提供了计算加速，并为药物发现中的未来应用建立了基础框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The implicit solvent approach offers a computationally efficient framework tomodel solvation effects in molecular simulations. However, its accuracy oftenfalls short compared to explicit solvent models, limiting its use in precisethermodynamic calculations. Recent advancements in machine learning (ML)present an opportunity to overcome these limitations by leveraging neuralnetworks to develop more precise implicit solvent potentials for diverseapplications. A major drawback of current ML-based methods is their reliance onforce-matching alone, which can lead to energy predictions that differ by anarbitrary constant and are therefore unsuitable for absolute free energycomparisons. Here, we introduce a novel methodology with a graph neural network(GNN)-based implicit solvent model, dubbed Lambda Solvation Neural Network(LSNN). In addition to force-matching, this network was trained to match thederivatives of alchemical variables, ensuring that solvation free energies canbe meaningfully compared across chemical species.. Trained on a dataset ofapproximately 300,000 small molecules, LSNN achieves free energy predictionswith accuracy comparable to explicit-solvent alchemical simulations, whileoffering a computational speedup and establishing a foundational framework forfuture applications in drug discovery.</description>
      <author>example@mail.com (Rishabh Dey, Michael Brocidiacono, Kushal Koirala, Alexander Tropsha, Konstantin I. Popov)</author>
      <guid isPermaLink="false">2510.20103v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</title>
      <link>http://arxiv.org/abs/2510.19954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了RELATE，一种与模式无关的特征编码器，用于处理关系型多表数据，能与任何通用图神经网络一起使用，在保持性能的同时大幅减少参数数量。&lt;h4&gt;背景&lt;/h4&gt;关系型多表数据在电子商务、医疗健康和科学研究等领域很常见，可表示为具有多模态节点属性的异质时间图。现有图神经网络依赖特定模式的特征编码器，需为每种节点类型和特征列设计单独模块，限制了可扩展性和参数共享。&lt;h4&gt;目的&lt;/h4&gt;开发一种与模式无关、即插即用的特征编码器，能与任何通用图神经网络配合使用，解决现有方法的可扩展性和参数共享问题。&lt;h4&gt;方法&lt;/h4&gt;RELATE采用针对分类、数值、文本和时间属性的共享模态特定编码器，后接Perceiver风格的交叉注意力模块，将特征聚合成固定大小、排列不变的节点表示。&lt;h4&gt;主要发现&lt;/h4&gt;在RelBench基准测试中，RELATE实现了与特定模式编码器相当的性能（差距在3%以内），同时将参数数量减少了高达5倍。&lt;h4&gt;结论&lt;/h4&gt;RELATE设计支持变化的数据模式，并支持通用图神经网络的多数据集预训练，为关系图数据的基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;关系型多表数据在电子商务、医疗健康和科学研究等领域很常见，可以自然地表示为具有多模态节点属性的异质时间图。现有的图神经网络依赖于特定模式的特征编码器，需要为每种节点类型和特征列设计单独的模块，这阻碍了可扩展性和参数共享。我们引入了RELATE（关系型实体潜在聚合编码器），这是一种与模式无关的、即插即用的特征编码器，可以与任何通用图神经网络一起使用。RELATE采用针对分类、数值、文本和时间属性的共享模态特定编码器，然后是一个Perceiver风格的交叉注意力模块，将特征聚合成固定大小、排列不变的节点表示。我们在RelBench基准测试的ReLGNN和HGT上评估了RELATE，结果显示它实现了与特定模式编码器相当的性能（差距在3%以内），同时将参数数量减少了高达5倍。这种设计支持变化的数据模式，并支持通用图神经网络的多数据集预训练，为关系图数据的基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational multi-table data is common in domains such as e-commerce,healthcare, and scientific research, and can be naturally represented asheterogeneous temporal graphs with multi-modal node attributes. Existing graphneural networks (GNNs) rely on schema-specific feature encoders, requiringseparate modules for each node type and feature column, which hindersscalability and parameter sharing. We introduce RELATE (Relational Encoder forLatent Aggregation of Typed Entities), a schema-agnostic, plug-and-play featureencoder that can be used with any general purpose GNN. RELATE employs sharedmodality-specific encoders for categorical, numerical, textual, and temporalattributes, followed by a Perceiver-style cross-attention module thataggregates features into a fixed-size, permutation-invariant noderepresentation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,where it achieves performance within 3% of schema-specific encoders whilereducing parameter counts by up to 5x. This design supports varying schemas andenables multi-dataset pretraining for general-purpose GNNs, paving the waytoward foundation models for relational graph data.</description>
      <author>example@mail.com (Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski)</author>
      <guid isPermaLink="false">2510.19954v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>FnRGNN: Distribution-aware Fairness in Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.19257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FnRGNN，一种用于图神经网络节点回归的公平性感知处理框架，通过在结构、表示和预测三个层面进行干预，有效减少了组间差异而不牺牲性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在结构化数据学习中表现出色，但在回归任务中的公平性研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;解决图神经网络在节点级回归任务中的公平性问题，特别是处理现有方法无法解决的连续特性挑战。&lt;h4&gt;方法&lt;/h4&gt;FnRGNN框架采用三级干预策略：结构层面的边重加权、表示层面的MMD对齐、以及预测层面的Sinkhorn分布匹配归一化。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的实验表明，FnRGNN能够有效减少组间差异，同时保持模型性能。&lt;h4&gt;结论&lt;/h4&gt;多层级干预策略确保了FnRGNN在复杂图拓扑结构下的鲁棒公平性，为图神经网络回归任务中的公平性问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在结构化数据学习中表现出色，但在回归任务中的公平性研究仍然不足。现有方法主要针对分类任务和表示层面的去偏见，无法完全处理节点级回归的连续特性。我们提出了FnRGNN，一个基于GNN的节点回归的公平性感知处理框架，在三个层面进行干预：(i)结构层面的边重加权，(ii)表示层面的MMD对齐，(iii)预测层面的Sinkhorn分布匹配归一化。这种多层级策略确保了在复杂图拓扑结构下的鲁棒公平性。在四个真实世界数据集上的实验表明，FnRGNN减少了组间差异而不牺牲性能。代码可在https://github.com/sybeam27/FnRGNN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760796&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) excel at learning from structured data, yetfairness in regression tasks remains underexplored. Existing approaches mainlytarget classification and representation-level debiasing, which cannot fullyaddress the continuous nature of node-level regression. We propose FnRGNN, afairness-aware in-processing framework for GNN-based node regression thatapplies interventions at three levels: (i) structure-level edge reweighting,(ii) representation-level alignment via MMD, and (iii) prediction-levelnormalization through Sinkhorn-based distribution matching. This multi-levelstrategy ensures robust fairness under complex graph topologies. Experiments onfour real-world datasets demonstrate that FnRGNN reduces group disparitieswithout sacrificing performance. Code is available athttps://github.com/sybeam27/FnRGNN.</description>
      <author>example@mail.com (Soyoung Park, Sungsu Lim)</author>
      <guid isPermaLink="false">2510.19257v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Graph Neural Networks: A Mutual Learning Approach</title>
      <link>http://arxiv.org/abs/2510.19223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种图神经网络之间的协作学习框架，在没有预训练教师模型的情况下，让简单浅层的GNN架构相互教学，从而提升模型在推理时的性能，特别是在处理多个任务时。&lt;h4&gt;背景&lt;/h4&gt;知识蒸馏技术是将复杂教师模型专业知识转移到轻量级学生模型的有效工具，特别适合在资源受限设备上部署高性能模型。该方法已成功应用于图神经网络，利用其表达能力生成捕获结构和特征相关信息的节点嵌入。&lt;h4&gt;目的&lt;/h4&gt;探索GNNs之间协作学习的潜力，使相对简单和浅层的GNN架构能够协同学习高效模型，在推理时表现更好，特别是在处理多个任务时。&lt;h4&gt;方法&lt;/h4&gt;提出一个协作学习框架，其中学生GNN集合在整个训练过程中相互教学；引入自适应logit加权单元促进模型间的高效知识交换；采用熵增强技术改进相互学习；这些组件动态赋能模型在训练过程中调整学习策略，优化下游任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;简单浅层的GNN架构能够协同学习高效模型；这些模型在推理时表现更好，特别是在处理多个任务时；提出的自适应logit加权单元和熵增强技术有效促进了模型间的知识交换和相互学习。&lt;h4&gt;结论&lt;/h4&gt;通过协作学习框架，学生GNN能够在没有预训练教师模型的情况下相互教学；提出的方法在节点分类和图分类任务上表现出色；为资源受限环境中的高效模型部署提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;知识蒸馏技术已成为一种强大的工具，用于将复杂教师模型的专业知识转移到轻量级学生模型中，特别适合在资源受限设备上部署高性能模型。这种方法已成功应用于图神经网络，利用其表达能力生成捕获结构和特征相关信息的节点嵌入。在本研究中，我们通过探索GNNs之间协作学习的潜力，偏离了传统的KD方法。在没有预训练教师模型的情况下，我们证明相对简单和浅层的GNN架构能够协同学习高效模型，在推理时表现更好，特别是在处理多个任务时。我们提出了一个协作学习框架，其中学生GNN集合在整个训练过程中相互教学。我们引入了自适应logit加权单元以促进模型间的高效知识交换，以及熵增强技术以改进相互学习。这些组件动态赋能模型在训练过程中调整学习策略，优化下游任务性能。在三个节点分类和图分类数据集上进行的广泛实验证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge distillation (KD) techniques have emerged as a powerful tool fortransferring expertise from complex teacher models to lightweight studentmodels, particularly beneficial for deploying high-performance models inresource-constrained devices. This approach has been successfully applied tograph neural networks (GNNs), harnessing their expressive capabilities togenerate node embeddings that capture structural and feature-relatedinformation. In this study, we depart from the conventional KD approach byexploring the potential of collaborative learning among GNNs. In the absence ofa pre-trained teacher model, we show that relatively simple and shallow GNNarchitectures can synergetically learn efficient models capable of performingbetter during inference, particularly in tackling multiple tasks. We propose acollaborative learning framework where ensembles of student GNNs mutually teacheach other throughout the training process. We introduce an adaptive logitweighting unit to facilitate efficient knowledge exchange among models and anentropy enhancement technique to improve mutual learning. These componentsdynamically empower the models to adapt their learning strategies duringtraining, optimizing their performance for downstream tasks. Extensiveexperiments conducted on three datasets each for node and graph classificationdemonstrate the effectiveness of our approach.</description>
      <author>example@mail.com (Paul Agbaje, Akajyoti Mitra, Afia Anjum, Pranali Khose, Ebelechukwu Nwafor, Habeeb Olufowobi)</author>
      <guid isPermaLink="false">2510.19223v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>An Active Diffusion Neural Network for Graphs</title>
      <link>http://arxiv.org/abs/2510.19202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ADGNN的新型图神经网络，通过整合多个外部信息源实现主动扩散，解决了传统扩散GNN的过平滑问题，使节点能保留独特特征同时获取全局图结构信息。&lt;h4&gt;背景&lt;/h4&gt;热扩散类比促进了图信息流理解和图神经网络发展，但大多数扩散GNN模拟被动热扩散，存在过平滑问题，限制了捕获全局图信息的能力。这类似于宇宙热寂理论，即封闭系统中能量分布随时间变得均匀，导致节点表示收敛到相同特征向量。&lt;h4&gt;目的&lt;/h4&gt;解决传统扩散GNN中的过平滑问题，使节点能保留独特特征同时有效获取图全局结构信息。&lt;h4&gt;方法&lt;/h4&gt;提出ADGNN（主动扩散图神经网络），通过整合多个外部信息源实现主动扩散，动态影响扩散过程克服过平滑问题。通过直接计算主动扩散迭代公式的闭式解，实现真正的无限扩散。&lt;h4&gt;主要发现&lt;/h4&gt;ADGNN在多种图任务上与最先进GNN模型相比，显著提高了准确性和效率，有效捕获全局图信息并保持节点独特性。&lt;h4&gt;结论&lt;/h4&gt;ADGNN通过主动扩散机制解决了传统扩散GNN的过平滑问题，使节点既能保持独特特征又能获取全局图结构信息，在多种图任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;热扩散的类比增强了对图中信息流的理解，并启发了图神经网络(GNNs)的发展。然而，大多数基于扩散的GNN模拟被动热扩散，仍然存在过平滑问题，限制了它们捕获全局图信息的能力。受宇宙热寂理论的启发，该理论认为在封闭系统中能量分布随时间变得均匀，我们认识到，在没有外部输入的情况下，节点表示会随着扩散过程收敛到相同的特征向量。为解决这个问题，我们提出了主动扩散图神经网络(ADGNN)。ADGNN通过整合多个外部信息源实现主动扩散，这些信息源动态影响扩散过程，有效克服了过平滑问题。此外，我们的方法通过直接计算主动扩散迭代公式的闭式解，实现了真正的无限扩散。这使得节点能够保留其独特特征，同时有效地获取对图全局结构的全面理解。我们在各种图任务上将ADGNN与几个最先进的GNN模型进行了评估。结果表明，ADGNN显著提高了准确性和效率，突显了其在捕获全局图信息和保持节点独特性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The analogy to heat diffusion has enhanced our understanding of informationflow in graphs and inspired the development of Graph Neural Networks (GNNs).However, most diffusion-based GNNs emulate passive heat diffusion, which stillsuffers from over-smoothing and limits their ability to capture global graphinformation. Inspired by the heat death of the universe, which posits thatenergy distribution becomes uniform over time in a closed system, we recognizethat, without external input, node representations in a graph converge toidentical feature vectors as diffusion progresses. To address this issue, wepropose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achievesactive diffusion by integrating multiple external information sources thatdynamically influence the diffusion process, effectively overcoming theover-smoothing problem. Furthermore, our approach realizes true infinitediffusion by directly calculating the closed-form solution of the activediffusion iterative formula. This allows nodes to preserve their uniquecharacteristics while efficiently gaining comprehensive insights into thegraph's global structure. We evaluate ADGNN against several state-of-the-artGNN models across various graph tasks. The results demonstrate that ADGNNsignificantly improves both accuracy and efficiency, highlighting itseffectiveness in capturing global graph information and maintaining nodedistinctiveness.</description>
      <author>example@mail.com (Mengying Jiang)</author>
      <guid isPermaLink="false">2510.19202v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning noisy tissue dynamics across time scales</title>
      <link>http://arxiv.org/abs/2510.19090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一种仿生机器学习框架，能够直接从实验电影中推断噪声多细胞动力学，成功应用于上皮组织、果蝇翅膀发育和ERK波介导的细胞信号过程。&lt;h4&gt;背景&lt;/h4&gt;组织动力学在从伤口愈合到形态发生的生物过程中起着关键作用，但这些噪声多细胞动力学难以预测。&lt;h4&gt;目的&lt;/h4&gt;引入一个能够直接从实验电影中推断噪声多细胞动力学的仿生机器学习框架。&lt;h4&gt;方法&lt;/h4&gt;该生成模型结合了图神经网络、归一化流和WaveNet算法，将组织表示为神经随机微分方程，其中细胞是 evolving graph 的边。&lt;h4&gt;主要发现&lt;/h4&gt;该机器学习架构反映了底层生物组织的架构，大大减少了训练所需的数据量；该模型能捕捉随机细胞运动并预测细胞在分裂周期中状态的演变；可以准确生成发育系统和细胞信号过程的实验动力学。&lt;h4&gt;结论&lt;/h4&gt;该方法为在生物工程和临床环境中作为数字孪生使用铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;组织动力学在从伤口愈合到形态发生的生物过程中起着关键作用。然而，这些噪声多细胞动力学难以预测。在此，我们引入了一种仿生机器学习框架，能够直接从实验电影中推断噪声多细胞动力学。该生成模型结合了图神经网络、归一化流和WaveNet算法，将组织表示为神经随机微分方程，其中细胞是 evolving graph 的边。与卷积或全连接神经网络相比，该机器学习架构反映了底层生物组织的架构，大大减少了训练所需的数据量。以上皮组织实验为例，我们表明该模型不仅能捕捉随机细胞运动，还能预测细胞在分裂周期中状态的演变。最后，我们证明了该方法可以准确生成发育系统（如果蝇翅膀）和由随机ERK波介导的细胞信号过程的实验动力学，为在生物工程和临床环境中作为数字孪生使用铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tissue dynamics play a crucial role in biological processes ranging fromwound healing to morphogenesis. However, these noisy multicellular dynamics arenotoriously hard to predict. Here, we introduce a biomimetic machine learningframework capable of inferring noisy multicellular dynamics directly fromexperimental movies. This generative model combines graph neural networks,normalizing flows and WaveNet algorithms to represent tissues as neuralstochastic differential equations where cells are edges of an evolving graph.This machine learning architecture reflects the architecture of the underlyingbiological tissues, substantially reducing the amount of data needed to trainit compared to convolutional or fully-connected neural networks. Takingepithelial tissue experiments as a case study, we show that our model not onlycaptures stochastic cell motion but also predicts the evolution of cell statesin their division cycle. Finally, we demonstrate that our method can accuratelygenerate the experimental dynamics of developmental systems, such as the flywing, and cell signaling processes mediated by stochastic ERK waves, paving theway for its use as a digital twin in bioengineering and clinical contexts.</description>
      <author>example@mail.com (Ming Han, John Devany, Michel Fruchart, Margaret L. Gardel, Vincenzo Vitelli)</author>
      <guid isPermaLink="false">2510.19090v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Committors without Descriptors</title>
      <link>http://arxiv.org/abs/2510.18018v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于committor的增强采样方法，用于解决原子模拟中的稀有事件研究挑战。通过结合图神经网络技术，实现了对系统亚稳态之间频繁转换的促进，并对过程过渡态集合进行了广泛采样。&lt;h4&gt;背景&lt;/h4&gt;稀有事件研究是原子模拟中的主要挑战之一，已经提出了几种增强采样方法来解决这一问题。最近有研究建议使用committor（提供对稀有事件的精确形式化描述）来解决这一挑战。&lt;h4&gt;目的&lt;/h4&gt;进一步自动化基于committor的方法，通过将其与图神经网络的强大表达能力相结合，使方法能够直接处理原子坐标而非描述符，并展示基于图的方法在描述溶剂分子在离子对解离或配体结合等系统中作用方面的优势。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于committor的方法，该方法促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。该方法的特点是自洽和半自动，利用变分准则迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入。进一步通过将先前方法与图神经网络的强大表达能力相结合，直接处理原子坐标而非描述符。&lt;h4&gt;主要发现&lt;/h4&gt;基于committor的方法能够促进系统亚稳态之间的频繁转换；该方法允许对过程过渡态集合进行广泛采样；结合图神经网络使方法更加自动化；基于图的方法在描述溶剂分子在离子对解离或配体结合等系统中作用方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;通过将基于committor的方法与图神经网络相结合，研究成功实现了方法的进一步自动化，并展示了基于图的方法在描述溶剂分子在特定系统中的角色方面的优势，为原子模拟中稀有事件的研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;稀有事件研究是原子模拟中的主要挑战之一，已经提出了几种解决这一问题的增强采样方法。最近有研究建议使用committor（提供对稀有事件的精确形式化描述）来解决这一挑战。我们最近跟进这一建议，提出了一种基于committor的方法，该方法促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。我们方法的优势之一是自洽和半自动，利用变分准则迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入。在这里，我们通过将先前方法与图神经网络的强大表达能力相结合，进一步自动化了这一过程，图神经网络可以直接处理原子坐标而非描述符。除了在基准系统上的应用外，我们强调了基于图的方法在描述离子对解离或配体结合等系统中溶剂分子作用方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of rare events is one of the major challenges in atomisticsimulations, and several enhanced sampling methods towards its solution havebeen proposed. Recently, it has been suggested that the use of the committor,which provides a precise formal description of rare events, could be of use inthis context. We have recently followed up on this suggestion and proposed acommittor-based method that promotes frequent transitions between themetastable states of the system and allows extensive sampling of the processtransition state ensemble. One of the strengths of our approach is beingself-consistent and semi-automatic, exploiting a variational criterion toiteratively optimize a neural-network-based parametrization of the committor,which uses a set of physical descriptors as input. Here, we further automatethis procedure by combining our previous method with the expressive power ofgraph neural networks, which can directly process atomic coordinates ratherthan descriptors. Besides applications on benchmark systems, we highlight theadvantages of a graph-based approach in describing the role of solventmolecules in systems, such as ion pair dissociation or ligand binding.</description>
      <author>example@mail.com (Peilin Kang, Jintu Zhang, Enrico Trizio, TingJun Hou, Michele Parrinello)</author>
      <guid isPermaLink="false">2510.18018v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AutoScape: Geometry-Consistent Long-Horizon Scene Generation</title>
      <link>http://arxiv.org/abs/2510.20726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://auto-scape.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AutoScape是一个长时程驾驶场景生成框架，通过创新的RGB-D扩散模型生成几何一致的关键帧，并使用视频扩散模型生成连贯的驾驶视频。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶和场景生成领域，需要生成长时间、几何一致的驾驶场景，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成长时间（超过20秒）、真实且几何一致的驾驶视频的框架，解决现有方法在长时程场景生成中的局限性。&lt;h4&gt;方法&lt;/h4&gt;AutoScape框架包含一个RGB-D扩散模型，该模型在共享潜在空间中联合处理图像和深度，基于先前生成的关键帧场景几何条件，并使用一致的引导来引导采样过程。给定高质量RGB-D关键帧后，视频扩散模型在关键帧之间进行插值。&lt;h4&gt;主要发现&lt;/h4&gt;AutoScape能够生成超过20秒的真实且几何一致的驾驶视频，相比之前的最先进方法，在长时程FID和FVD评分上分别提高了48.6%和43.0%。&lt;h4&gt;结论&lt;/h4&gt;AutoScape通过创新的RGB-D扩散模型和视频插值方法，显著提高了长时程驾驶场景生成的质量和一致性，为自动驾驶模拟和训练提供了更真实的数据来源。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了AutoScape，一个长时程驾驶场景生成框架。其核心是一个创新的RGB-D扩散模型，迭代生成稀疏的、几何一致的关键帧，作为场景外观和几何的可靠锚点。为了保持长距离几何一致性，模型1)在共享潜在空间中联合处理图像和深度，2)显式地基于先前生成的关键帧的场景几何（即渲染的点云）进行条件化，3)使用一致的引导来引导采样过程。给定高质量的RGB-D关键帧后，视频扩散模型在它们之间进行插值，生成密集且连贯的视频帧。AutoScape生成超过20秒的真实且几何一致的驾驶视频，相比之前的最先进方法，长时程FID和FVD评分分别提高了48.6%和43.0%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成长时间（20秒以上）且保持3D几何一致性的驾驶场景视频问题。这个问题在自动驾驶领域至关重要，因为自动驾驶系统需要大量高质量、长时间一致的场景数据进行安全可靠的测试和验证，而当前方法在长时间生成时难以保持几何一致性，限制了实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将长时间场景生成问题分解为稀疏关键帧生成和密集帧插值两个子问题，采用层次化方法解决。核心洞察是几何一致性退化是长时间生成的关键瓶颈，因此需要显式建模几何信息。方法借鉴了扩散模型在图像生成中的成功应用、RGB-D联合建模、ControlNet的条件控制机制以及视频扩散模型，但创新性地设计了RGB-D扩散模型、几何条件机制和Warp Consistent Guidance来提高几何一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次化方法解决长时间场景生成：先生成稀疏但几何一致的关键帧作为场景锚点，然后在关键帧间插值生成密集视频帧。整体流程分为两个阶段：1)关键帧生成阶段：从输入图像反投影为3D点云，投影到下一视角生成渲染点和掩码，使用RGB-D扩散模型生成新关键帧，迭代添加到点云集合，并用Warp Consistent Guidance提高一致性；2)插值阶段：使用视频扩散模型在关键帧间生成密集视频帧，条件于从关键帧渲染的3D点云确保几何一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)层次化框架，分解为关键帧生成和插值；2)RGB-D扩散模型，联合建模颜色和深度；3)显式几何条件，将渲染点云作为条件输入；4)Warp Consistent Guidance，减少误差累积；5)两阶段训练策略。相比WonderJourney和Vista等之前工作，AutoScape专门设计的RGB-D扩散模型具有更好的几何感知能力，通过关键帧锚点确保长期一致性，并显式利用几何信息而非仅依赖时间一致性模块。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AutoScape提出了一种创新的层次化框架，通过RGB-D扩散模型生成几何一致的关键帧，并结合视频插值实现了长达20秒的高质量、3D一致的驾驶场景视频生成，显著提升了长时间场景生成的质量和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes AutoScape, a long-horizon driving scene generationframework. At its core is a novel RGB-D diffusion model that iterativelygenerates sparse, geometrically consistent keyframes, serving as reliableanchors for the scene's appearance and geometry. To maintain long-rangegeometric consistency, the model 1) jointly handles image and depth in a sharedlatent space, 2) explicitly conditions on the existing scene geometry (i.e.,rendered point clouds) from previously generated keyframes, and 3) steers thesampling process with a warp-consistent guidance. Given high-quality RGB-Dkeyframes, a video diffusion model then interpolates between them to producedense and coherent video frames. AutoScape generates realistic andgeometrically consistent driving videos of over 20 seconds, improving thelong-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and43.0\%, respectively.</description>
      <author>example@mail.com (Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker)</author>
      <guid isPermaLink="false">2510.20726v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</title>
      <link>http://arxiv.org/abs/2510.20708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ALICE-LRI的新型方法，实现了从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件。该方法通过自动推断LiDAR传感器的内几何参数，实现了零点损失的点云投影和重建。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR传感器在自主导航、环境监测和遥感应用中至关重要。为了高效处理这些传感器产生的大量点云数据，LiDAR数据通常被投影到2D距离图像中，这些图像根据点的角度位置和距离来组织点。然而，传统的投影方法存在基本的几何不一致性，导致不可逆的信息丢失，影响高保真应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的、与传感器无关的方法，能够从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件，并保持几何精度在传感器精度范围内。&lt;h4&gt;方法&lt;/h4&gt;ALICE-LRI是一种自动LiDAR内标定估计方法，能够自动逆向工程任何旋转LiDAR传感器的内几何。该方法通过推断关键参数，包括激光束配置、角度分布和每束校准校正，实现无损投影和完整的点云重建，零点损失。&lt;h4&gt;主要发现&lt;/h4&gt;在完整的KITTI和DurLAR数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，所有点云中都没有点丢失。几何精度保持在传感器精度范围内，建立了具有实时性能的几何无损性。压缩案例研究验证了显著的下游效益，展示了实际应用中的显著质量改进。&lt;h4&gt;结论&lt;/h4&gt;从近似到无损LiDAR投影的范式转变，为需要完整几何保存的高精度遥感应用开辟了新的可能性。ALICE-LRI方法代表了LiDAR数据处理领域的重要进展，能够在不损失信息的情况下实现高效处理。&lt;h4&gt;翻译&lt;/h4&gt;3D LiDAR传感器对于自主导航、环境监测和遥感应用中的精密制图至关重要。为了高效处理这些传感器产生的大量点云数据，LiDAR数据通常被投影到2D距离图像中，这些图像根据点的角度位置和距离来组织点。虽然这些距离图像表示能够实现高效处理，但传统的投影方法存在基本的几何不一致性，导致不可逆的信息丢失，影响高保真应用。我们提出了ALICE-LRI（无损距离图像的自动LiDAR内标定估计），这是第一个通用的、与传感器无关的方法，能够从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件。我们的算法通过推断关键参数（包括激光束配置、角度分布和每束校准校正）来自动逆向工程任何旋转LiDAR传感器的内几何，实现无损投影和零点损失的完整点云重建。在完整的KITTI和DurLAR数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，所有点云中都没有点丢失。几何精度保持在传感器精度范围内，建立了具有实时性能的几何无损性。我们还介绍了压缩案例研究，验证了显著的下游效益，展示了实际应用中的显著质量改进。从近似到无损LiDAR投影的范式转变，为需要完整几何保存的高精度遥感应用开辟了新的可能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是如何在不依赖制造商提供的校准元数据的情况下，从旋转式激光雷达(LiDAR)点云生成无损的2D距离图像。这个问题很重要，因为LiDAR传感器在自动驾驶、环境监测和遥感等领域至关重要，而传统投影方法存在几何不一致性导致信息损失，会影响高精度应用的质量。许多实际场景中我们只有已校准的点云数据，而没有原始传感器数据或校准文件，限制了现有方法的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了理想传感器模型和实际传感器模型之间的差异，认识到需要推断出厂校准的几何参数。他们设计了ALICE-LRI方法，包含参数估计和无损投影两个主要阶段。参数估计又分为垂直和水平参数估计，使用Hough变换识别候选扫描线，加权最小二乘法进行拟合，以及冲突解决机制确保一致性。作者借鉴了Hough变换用于特征提取的技术和加权最小二乘法处理异方差噪声，但解决了现有校准研究的逆问题——从已校准点云推断参数而非从原始数据估计参数。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是自动反向推断LiDAR传感器的内在几何结构，无需制造商元数据，直接从点云数据推断关键参数，生成与传感器几何完全一致的距离图像，实现点云完全重建。整体流程分为：1)垂直参数估计：使用Hough变换识别候选扫描线参数，选择一致点，加权最小二乘拟合，解决冲突；2)水平参数估计：对每束进行分辨率穷举搜索，计算水平和方位角偏移，对点数不足的扫描线使用启发式方法；3)距离图像生成：使用估计参数将点云投影到2D图像，确保双射性和完全重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个通用方法实现从已校准点云生成无损距离图像，无需元数据；2)自动推断传感器内在几何参数；3)实现完全零点损失和几何无损；4)具有实时性能；5)提供开源实现。相比之前工作，传统方法依赖制造商提供的查找表或数据包信息，现有校准研究处理正向问题而非逆问题，大多数方法接受轻微几何失真，而ALICE-LRI实现了完全无损且通用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALICE-LRI首次实现了无需传感器元数据即可从旋转式LiDAR点云生成完全无损的距离图像，通过自动推断传感器内在几何参数解决了传统投影方法中的信息损失问题，为高精度遥感应用提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D LiDAR sensors are essential for autonomous navigation, environmentalmonitoring, and precision mapping in remote sensing applications. Toefficiently process the massive point clouds generated by these sensors, LiDARdata is often projected into 2D range images that organize points by theirangular positions and distances. While these range image representations enableefficient processing, conventional projection methods suffer from fundamentalgeometric inconsistencies that cause irreversible information loss,compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDARIntrinsic Calibration Estimation for Lossless Range Images), the first general,sensor-agnostic method that achieves lossless range image generation fromspinning LiDAR point clouds without requiring manufacturer metadata orcalibration files. Our algorithm automatically reverse-engineers the intrinsicgeometry of any spinning LiDAR sensor by inferring critical parametersincluding laser beam configuration, angular distributions, and per-beamcalibration corrections, enabling lossless projection and complete point cloudreconstruction with zero point loss. Comprehensive evaluation across thecomplete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfectpoint preservation, with zero points lost across all point clouds. Geometricaccuracy is maintained well within sensor precision limits, establishinggeometric losslessness with real-time performance. We also present acompression case study that validates substantial downstream benefits,demonstrating significant quality improvements in practical applications. Thisparadigm shift from approximate to lossless LiDAR projections opens newpossibilities for high-precision remote sensing applications requiring completegeometric preservation.</description>
      <author>example@mail.com (Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera)</author>
      <guid isPermaLink="false">2510.20708v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</title>
      <link>http://arxiv.org/abs/2510.20406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了PointMapPolicy，一种新颖的机器人操作方法，通过将点云组织成结构化网格并结合RGB数据，实现了高效的多模态感知，在多种操作任务中达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;机器人操作系统受益于互补的传感模态，其中每种模态提供独特的环境信息。点云能捕获详细的几何结构，而RGB图像提供丰富的语义上下文。然而，当前点云方法难以捕获细粒度细节，特别是对于复杂任务；而RGB方法缺乏几何意识，限制了其精度和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型方法，结合点云和RGB图像的优势，解决现有方法在几何细节和语义理解方面的局限性，提高机器人操作系统的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了PointMapPolicy，一种将扩散策略基于未下采样的结构化点网格的新方法。这种方法创建的数据类型更容易从观测中提取形状和空间关系，并且可以在参考帧之间转换。由于点网格的结构规整，可以直接使用成熟的计算机视觉技术处理3D数据。模型使用xLSTM作为骨干网络，高效融合点图与RGB数据以增强多模态感知。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboCasa和CALVIN基准测试以及真实机器人评估上的大量实验表明，该方法在各种操作任务中实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;PointMapPolicy有效结合了点云和RGB数据的优势，通过结构化点网格和多模态融合，显著提升了机器人操作系统的性能，特别是在需要精细几何理解和语义上下文的复杂任务中。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作系统受益于互补的传感模态，每种模态提供独特的环境信息。点云捕获详细的几何结构，而RGB图像提供丰富的语义上下文。当前点云方法难以捕获细粒度细节，特别是对于复杂任务，而RGB方法缺乏几何意识，这限制了它们的精度和泛化能力。我们引入了PointMapPolicy，一种新颖的方法，将扩散策略基于未下采样的结构化点网格。产生的数据类型更容易从观测中提取形状和空间关系，并且可以在参考帧之间转换。但由于它们在规则网格中的结构，我们能够直接使用成熟的计算机视觉技术处理3D数据。使用xLSTM作为骨干网络，我们的模型高效地将点图与RGB数据融合以增强多模态感知。在RoboCasa和CALVIN基准测试以及真实机器人评估的大量实验中，我们证明我们的方法在各种操作任务中实现了最先进的性能。概述和演示可在我们的项目页面查看：https://point-map.github.io/Point-Map/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人系统中多模态感知的挑战，即如何同时利用点云提供的几何信息和RGB图像提供的语义信息。这个问题很重要，因为机器人执行复杂任务时需要同时理解场景的几何结构和语义内容，而现有的方法要么缺乏几何细节（仅用RGB图像），要么难以处理精细细节（仅用点云），限制了机器人在复杂环境中的精确操作能力和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到计算机视觉社区在立体重建方面的最新进展启发，提出将点云转换为结构化的点图表示。他们借鉴了立体重建技术中的点图方法，将其应用于机器人模仿学习领域。设计过程中，他们考虑了点云和RGB图像的优缺点，创建了可以与标准视觉架构兼容的点图表示，并探索了多种融合策略来结合几何和语义信息。同时，他们借鉴了xLSTM架构作为骨干网络，平衡了时间建模能力和计算效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将无结构的点云转换为结构化的点图表示，使其能够与标准视觉架构兼容，同时融合点图的几何信息和RGB图像的语义信息。整体实现流程包括：1)将深度图转换为结构化的点图表示，编码为2D网格中的XYZ坐标；2)使用预训练的视觉编码器处理RGB图像和点图；3)采用晚期融合策略(如拼接)来结合多模态信息；4)使用xLSTM作为骨干网络处理多模态输入；5)基于EDM框架应用扩散策略生成动作序列；6)通过少量去噪步骤(4步)生成最终动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出点图(point maps)这一新的观察模态，首次在基于扩散的模仿学习中使用；2)将点云结构化为规则的2D网格，可直接应用标准视觉架构，无需KNN和FPS等昂贵操作；3)设计高效的多模态融合策略，平衡几何精度和语义理解；4)使用xLSTM作为骨干网络，相比Transformer具有更高的计算效率。相比之前的工作，PointMapPolicy不需要复杂的点云处理步骤，能同时利用几何和语义信息，且计算效率更高，在多个基准测试中取得了最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointMapPolicy通过结构化点云处理和多模态融合，在机器人模仿学习中实现了几何精度与语义理解的平衡，并在多个基准测试中取得了最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulation systems benefit from complementary sensing modalities,where each provides unique environmental information. Point clouds capturedetailed geometric structure, while RGB images provide rich semantic context.Current point cloud methods struggle to capture fine-grained detail, especiallyfor complex tasks, which RGB methods lack geometric awareness, which hinderstheir precision and generalization. We introduce PointMapPolicy, a novelapproach that conditions diffusion policies on structured grids of pointswithout downsampling. The resulting data type makes it easier to extract shapeand spatial relationships from observations, and can be transformed betweenreference frames. Yet due to their structure in a regular grid, we enable theuse of established computer vision techniques directly to 3D data. Using xLSTMas a backbone, our model efficiently fuses the point maps with RGB data forenhanced multi-modal perception. Through extensive experiments on the RoboCasaand CALVIN benchmarks and real robot evaluations, we demonstrate that ourmethod achieves state-of-the-art performance across diverse manipulation tasks.The overview and demos are available on our project page:https://point-map.github.io/Point-Map/</description>
      <author>example@mail.com (Xiaogang Jia, Qian Wang, Anrui Wang, Han A. Wang, Balázs Gyenes, Emiliyan Gospodinov, Xinkai Jiang, Ge Li, Hongyi Zhou, Weiran Liao, Xi Huang, Maximilian Beck, Moritz Reuss, Rudolf Lioutikov, Gerhard Neumann)</author>
      <guid isPermaLink="false">2510.20406v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>NeuralTouch: Neural Descriptors for Precise Sim-to-Real Tactile Robot Control</title>
      <link>http://arxiv.org/abs/2510.20390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了NeuralTouch，一个集成神经描述场（NDF）和触觉传感的多模态框架，通过轻柔的物理交互实现准确、可推广的机器人抓取。&lt;h4&gt;背景&lt;/h4&gt;抓取精度对精确物体操作至关重要，通常需要机器人手与物体仔细对齐。NDF是一种基于视觉的生成抓取姿势的方法，但单独使用时可能因相机校准不完美、点云不完整和物体变化而产生不准确姿势。触觉传感虽能实现更精确接触，但现有方法通常仅限于简单、预定义的接触几何形状。&lt;h4&gt;目的&lt;/h4&gt;开发一个集成NDF和触觉传感的框架，通过轻柔的物理交互实现准确、可推广的抓取，解决视觉方法在精确抓取方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用NDF隐式表示目标接触几何形状，训练深度强化学习策略使用触觉反馈来优化抓取。该策略以神经描述符为条件，无需明确指定接触类型。通过模拟中的消融研究和零样本迁移到现实世界任务（如销钉插入孔和瓶盖开启）进行验证，无需额外微调。&lt;h4&gt;主要发现&lt;/h4&gt;NeuralTouch显著提高了抓取精度和鲁棒性，优于基线方法，为精确、接触丰富的机器人操作提供了一个通用框架。&lt;h4&gt;结论&lt;/h4&gt;通过结合视觉（NDF）和触觉传感，NeuralTouch实现了准确、可推广的抓取，为需要精确接触的机器人操作任务提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;抓取精度是精确物体操作的关键前提，通常需要机器人手与物体之间的仔细对齐。神经描述场（NDF）提供了一种有前景的基于视觉的方法，可生成跨物体类别的抓取姿势。然而，仅靠NDF可能因不完美的相机校准、不完整的点云和物体变化而产生不准确姿势。同时，触觉传感能实现更精确接触，但现有方法通常学习限于简单、预定义接触几何形状的策略。在这项工作中，我们介绍了NeuralTouch，一个集成NDF和触觉传感的多模态框架，通过轻柔的物理交互实现准确、可推广的抓取。我们的方法利用NDF隐式表示目标接触几何形状，基于此训练深度强化学习策略，使用触觉反馈来优化抓取。该策略以神经描述符为条件，不需要明确指定接触类型。我们通过模拟中的消融研究和零样本迁移到现实世界操作任务（如销钉插入孔和瓶盖开启）来验证NeuralTouch，无需额外微调。结果表明，NeuralTouch显著提高了抓取精度和鲁棒性，优于基线方法，为精确、接触丰富的机器人操作提供了一个通用框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasping accuracy is a critical prerequisite for precise object manipulation,often requiring careful alignment between the robot hand and object. NeuralDescriptor Fields (NDF) offer a promising vision-based method to generategrasping poses that generalize across object categories. However, NDF alone canproduce inaccurate poses due to imperfect camera calibration, incomplete pointclouds, and object variability. Meanwhile, tactile sensing enables more precisecontact, but existing approaches typically learn policies limited to simple,predefined contact geometries. In this work, we introduce NeuralTouch, amultimodal framework that integrates NDF and tactile sensing to enableaccurate, generalizable grasping through gentle physical interaction. Ourapproach leverages NDF to implicitly represent the target contact geometry,from which a deep reinforcement learning (RL) policy is trained to refine thegrasp using tactile feedback. This policy is conditioned on the neuraldescriptors and does not require explicit specification of contact types. Wevalidate NeuralTouch through ablation studies in simulation and zero-shottransfer to real-world manipulation tasks--such as peg-out-in-hole and bottlelid opening--without additional fine-tuning. Results show that NeuralTouchsignificantly improves grasping accuracy and robustness over baseline methods,offering a general framework for precise, contact-rich robotic manipulation.</description>
      <author>example@mail.com (Yijiong Lin, Bowen Deng, Chenghua Lu, Max Yang, Efi Psomopoulou, Nathan F. Lepora)</author>
      <guid isPermaLink="false">2510.20390v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AnyPcc: Compressing Any Point Cloud with a Single Universal Model</title>
      <link>http://arxiv.org/abs/2510.20331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为AnyPcc的通用点云压缩框架，解决了深度学习点云几何压缩中的泛化性问题。该框架通过通用上下文模型和实例自适应微调策略，有效处理了分布外数据，并在15个数据集上实现了最先进的压缩性能。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的点云几何压缩面临泛化性挑战，主要原因是缺乏鲁棒的上下文模型和对分布外数据的低效处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用点云压缩框架AnyPcc，以解决点云几何压缩中的泛化性问题，特别是上下文建模和分布外数据处理方面的挑战。&lt;h4&gt;方法&lt;/h4&gt;AnyPcc包含两个主要组件：1) 通用上下文模型，利用空间和通道分组的先验信息捕获鲁棒的上下文依赖关系；2) 实例自适应微调策略，通过协同显式和隐式压缩范式处理分布外数据，为每个实例微调一小部分网络权重并整合到位流中。&lt;h4&gt;主要发现&lt;/h4&gt;在15个不同数据集上的广泛实验表明，AnyPcc在点云压缩方面设立了新的最先进水平，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;AnyPcc成功解决了点云几何压缩中的泛化性问题，通过创新的通用上下文模型和实例自适应微调策略，实现了更好的压缩性能，为点云压缩领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;泛化性是基于深度学习的点云几何压缩的一个关键挑战。我们认为这源于两个关键限制：缺乏鲁棒的上下文模型和对分布外数据的低效处理。为解决这两个问题，我们引入了AnyPcc，一个通用的点云压缩框架。AnyPcc首先采用通用上下文模型，利用空间和通道分组的先验信息来捕获鲁棒的上下文依赖关系。其次，我们新颖的实例自适应微调策略通过协同显式和隐式压缩范式来处理分布外数据。它为每个实例微调一小部分网络权重，并将它们整合到位流中，其中权重的边际位成本远小于几何压缩带来的节省。在15个不同数据集组成的基准测试上的大量实验证实，AnyPcc在点云压缩方面设立了新的最先进水平。我们的代码和数据集将被发布以鼓励可重复研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云压缩中的泛化挑战。现有方法通常针对特定点云密度设计，无法在真实世界中各种密度的点云上保持稳定性能，并且在遇到分布外数据时压缩效率急剧下降。这个问题很重要，因为随着自动驾驶和虚拟现实等应用中3D内容的普及，点云已成为主要数据表示形式，对高效压缩算法有迫切需求，而真实世界的点云数据具有广泛的多样性，许多关键类型（如医学扫描、3D高斯溅射）通常没有专用的训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：空间上下文模型在密集数据上表现好但在稀疏场景中性能差，而通道级模型虽能处理稀疏输入但牺牲了空间信息。为了解决这一权衡，他们设计了通用上下文模型(UCM)协同整合空间和通道先验。对于分布外数据问题，他们借鉴了参数高效微调技术，结合了隐式神经表示的优点和预训练模型的效率。作者借鉴了图像压缩中的上下文建模技术，但将其应用于几何占用码，还参考了参数高效微调方法，但这些技术在点云压缩领域尚未被探索。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合通用强大的预训练模型与快速的实例特定适应。整体流程包括：1)UCM采用从粗到细的层次结构，通过空间-通道上下文分解处理点云数据；2)使用3D棋盘模式将体素分为两组(G1和G2)，并在每组内将8位占用码分解为两个4位子符号；3)通过协同特征聚合增强上下文；4)IAFT策略只微调网络的一小部分参数(最终预测头)，实现快速实例适应；5)最终压缩位流包含微调后的权重和几何组件两部分；6)解码时先重建模型参数，再对几何数据进行解码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AnyPcc框架是首个使用单一统一模型实现高压缩率和跨多样化点云类型鲁棒性能的方法；2)UCM通过创新的棋盘分组策略协同整合空间和通道先验，建立跨越所有点云密度的鲁棒上下文建模；3)IAFT策略通过只微调预训练模型的一小部分参数，实现快速实例特定适应；4)实现了统一的无损和有损压缩。相比之前的工作，AnyPcc解决了类别特定方法的局限性，避免了Unicorn等泛化尝试的非统一架构问题，克服了隐式压缩的高计算成本，并解决了现有上下文模型在密集和稀疏数据之间的权衡问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AnyPcc通过引入通用上下文模型和实例自适应微调策略，首次实现了使用单一统一模型在各种点云类型上达到最先进的压缩性能，解决了点云压缩中长期存在的泛化挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalization remains a critical challenge for deep learning-based pointcloud geometry compression. We argue this stems from two key limitations: thelack of robust context models and the inefficient handling ofout-of-distribution (OOD) data. To address both, we introduce AnyPcc, auniversal point cloud compression framework. AnyPcc first employs a UniversalContext Model that leverages priors from both spatial and channel-wise groupingto capture robust contextual dependencies. Second, our novel Instance-AdaptiveFine-Tuning (IAFT) strategy tackles OOD data by synergizing explicit andimplicit compression paradigms. It fine-tunes a small subset of network weightsfor each instance and incorporates them into the bitstream, where the marginalbit cost of the weights is dwarfed by the resulting savings in geometrycompression. Extensive experiments on a benchmark of 15 diverse datasetsconfirm that AnyPcc sets a new state-of-the-art in point cloud compression. Ourcode and datasets will be released to encourage reproducible research.</description>
      <author>example@mail.com (Kangli Wang, Qianxi Yi, Yuqi Ye, Shihao Li, Wei Gao)</author>
      <guid isPermaLink="false">2510.20331v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种纯视觉单视图扩散策略学习方法(VO-DP)，通过融合预训练视觉基础模型的语义和几何特征，在机器人操作任务中取得了优异性能，特别是在真实世界任务中明显优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习领域，基于视觉运动的扩散策略学习是机器人操作的主要方向。现有方法多依赖点云作为输入，通过点云特征学习构建场景表示，但缺乏对纯视觉解决方案的深入探索，而纯视觉方案具有显著潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一种纯视觉且单视图的扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合，以提高机器人操作的性能。&lt;h4&gt;方法&lt;/h4&gt;利用VGGT的中间特征，结合来自DINOv2的语义特征和来自交替注意力块的几何特征，通过交叉注意力融合特征，并使用CNN进行空间压缩，形成策略头的输入。&lt;h4&gt;主要发现&lt;/h4&gt;VO-DP在仿真任务中达到64.6%的平均成功率，与基于点云的方法DP3(64.0%)相当，远高于纯视觉基线DP(34.8%)；在真实世界任务中达到87.9%的成功率，显著优于DP3(67.5%)和DP(11.2%)。鲁棒性评估表明VO-DP在各种变化条件下保持高度稳定。&lt;h4&gt;结论&lt;/h4&gt;提出的VO-DP方法在机器人操作任务中表现出色，特别是在真实世界任务中。研究团队还开源了一个支持多机器和多GPU并行训练的机器人操作训练库，兼容多种视觉运动策略。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习的背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数方法依赖点云作为观察输入，通过点云特征学习构建场景表示，从而实现显著的准确性。然而，现有文献缺乏对具有显著潜力的纯视觉解决方案的深入探索。在本文中，我们提出了一种纯视觉且单视图的扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合。我们利用VGGT的中间特征，结合来自DINOv2的语义特征和来自交替注意力块的几何特征。特征通过交叉注意力融合，并使用CNN进行空间压缩，形成策略头的输入。大量实验证明，VO-DP不仅显著优于纯视觉基线DP，而且与基于点云的方法DP3相比表现出明显的性能趋势：在仿真任务中，VO-DP的平均成功率为64.6%，与DP3的64.0%相当，远高于DP的34.8%；而在真实世界任务中，它达到87.9%，明显优于DP3的67.5%和DP的11.2%。进一步的鲁棒性评估证实，VO-DP在颜色、大小、背景和光照等变化条件下保持高度稳定。最后，我们开源了一个机器人操作训练库。该库基于Accelerate构建，支持多机器和多GPU并行训练以及混合精度训练。它与DP、DP3和VO-DP等视觉运动策略兼容，并支持RoboTwin模拟器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是如何仅使用RGB图像（vision-only）作为输入，实现与基于点云的方法相媲美的机器人操作性能。这个问题重要是因为当前主流机器人操作方法依赖昂贵的深度传感器（如深度相机或LiDAR），导致硬件成本高、系统复杂（需要多传感器校准），且在复杂场景中表现受限。相比之下，仅使用RGB图像的方法成本低得多、实用性高，但现有研究显示其性能通常不如基于点云的方法。提升vision-only方法的性能具有显著的实际应用价值，可以大幅降低机器人系统的部署成本和复杂度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前机器人操作领域的研究现状，指出vision-only方法成本低但性能不足，而基于点云的方法性能好但成本高。他们评估了现有方法，发现vision-only方法的瓶颈主要在于表示学习模块不够发达。作者认识到视觉基础模型（如VGGT、DINOv2）的潜力，这些模型可以直接从RGB图像中提取几何信息。因此，他们提出将语义特征和几何特征进行有效融合的思路，并在不依赖3D传感器的情况下获得丰富的场景理解。该方法借鉴了多个现有工作：使用预训练的VGGT模型作为视觉编码器，利用DINOv2进行语义特征提取，采用Alternating Attention网络进行几何特征提取，并借鉴了扩散模型（Diffusion Policy）的思想进行动作生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过预训练的视觉基础模型，从单视角RGB图像中同时提取语义和几何特征，并将这些特征自适应融合，从而在不依赖3D传感器的情况下实现高性能的机器人操作。整体实现流程包括四个主要步骤：1) 视觉编码：使用DINOv2提取语义特征，通过VGGT模型的Alternating Attention网络提取几何特征；2) 特征融合：使用残差交叉注意力机制将语义和几何特征融合，并通过前馈网络进一步处理；3) 场景表示压缩：使用轻量级ResNet对融合特征进行下采样和空间压缩，然后将压缩后的空间特征与本体感受信息连接，形成紧凑的场景表示；4) 动作生成：使用基于去噪扩散概率模型（DDPM）的策略头，以压缩后的场景表示为条件，通过迭代去噪过程预测动作轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次证明了仅使用RGB图像的vision-only方法可以达到与基于点云方法相媲美的性能水平；2) 设计了基于交叉注意力的特征融合模块，能够根据任务需求自适应地融合语义和几何特征；3) 通过空间特征压缩模块，从高维特征中提取关键信息，实现高效的单视角场景表示；4) 开源了DRRM训练库，支持多机多GPU并行训练和混合精度训练。相比之前的工作，VO-DP不再依赖点云或RGB-D等3D输入，仅使用RGB图像；利用预训练的视觉基础模型提取更丰富的特征；设计了专门的特征融合机制；在保持高性能的同时，显著降低了硬件成本和系统复杂度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VO-DP通过创新性地融合预训练视觉模型的语义和几何特征，首次实现了仅使用低成本RGB图像输入的机器人操作方法达到与基于昂贵3D传感器方法相媲美的性能，同时显著提升了在真实世界环境中的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v3</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>On Optimal Hyperparameters for Differentially Private Deep Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.20616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 30 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了差分隐私迁移学习中的关键超参数选择问题，特别关注裁剪边界C和批量大小B，揭示了理论理解与实际结果之间的不匹配，并提出改进方法。&lt;h4&gt;背景&lt;/h4&gt;差分隐私迁移学习（即在私有数据上微调预训练模型）是当前在隐私约束下训练大型模型的最先进方法。然而，在关键超参数选择方面存在理论与实践的脱节。&lt;h4&gt;目的&lt;/h4&gt;研究差分隐私迁移学习中两个关键超参数（裁剪边界C和批量大小B）的最优选择方法，解决当前理论理解与实证结果之间的不匹配问题。&lt;h4&gt;方法&lt;/h4&gt;分析裁剪边界C和批量大小B对模型性能的影响，考察梯度分布的变化，在固定计算预算（固定周期）下评估现有启发式方法，分析累积DP噪声对批量大小选择的影响，研究跨任务使用单一(C,B)设置的效果，并分析裁剪作为梯度重加权形式和累积DP噪声的作用。&lt;h4&gt;主要发现&lt;/h4&gt;1. 当前关于如何选择最优C的理论理解（更强的隐私需要更小的C）与实证结果（更强的隐私下更大的C表现更好）之间存在明显不匹配，这是由梯度分布变化引起的。2. 在计算预算有限的情况下，现有的调整批量大小B的启发式方法不适用，而累积DP噪声能更好地解释较小或较大批量的表现差异。3. 跨任务使用单一(C,B)设置会导致次优性能，特别是在从宽松隐私转向严格隐私以及从充足计算转向有限计算的情况下。&lt;h4&gt;结论&lt;/h4&gt;差分隐私迁移学习中的超参数选择需要考虑梯度分布变化和累积DP噪声的影响，不应简单地跨任务应用相同的超参数设置，而应根据隐私需求和计算资源进行针对性调整。&lt;h4&gt;翻译&lt;/h4&gt;差分隐私（DP）迁移学习，即在私有数据上微调预训练模型，是当前在隐私约束下训练大型模型的最先进方法。我们关注此设置中的两个关键超参数：裁剪边界C和批量大小B。我们展示了当前关于如何选择最优C的理论理解（更强的隐私需要更小的C）与实证结果（更强的隐私下更大的C表现更好）之间的明显不匹配，这是由梯度分布变化引起的。假设计算预算有限（固定周期），我们证明了现有的调整B的启发式方法不适用，而累积DP噪声能更好地解释较小或较大批量的表现差异。我们还强调了跨任务使用单一(C,B)设置的常见做法可能导致次优性能。我们发现，当在宽松与严格隐私之间转换以及在充足与有限计算之间转换时，性能下降特别明显，我们通过分析裁剪作为梯度重加权形式和检查累积DP噪声来解释这一现象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Differentially private (DP) transfer learning, i.e., fine-tuning a pretrainedmodel on private data, is the current state-of-the-art approach for traininglarge models under privacy constraints. We focus on two key hyperparameters inthis setting: the clipping bound $C$ and batch size $B$. We show a clearmismatch between the current theoretical understanding of how to choose anoptimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes(larger $C$ performs better under strong privacy), caused by changes in thegradient distributions. Assuming a limited compute budget (fixed epochs), wedemonstrate that the existing heuristics for tuning $B$ do not work, whilecumulative DP noise better explains whether smaller or larger batches performbetter. We also highlight how the common practice of using a single $(C,B)$setting across tasks can lead to suboptimal performance. We find thatperformance drops especially when moving between loose and tight privacy andbetween plentiful and limited compute, which we explain by analyzing clippingas a form of gradient re-weighting and examining cumulative DP noise.</description>
      <author>example@mail.com (Aki Rehn, Linzh Zhao, Mikko A. Heikkilä, Antti Honkela)</author>
      <guid isPermaLink="false">2510.20616v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Reproducible Demographic Inference for Fairness in Face Analysis</title>
      <link>http://arxiv.org/abs/2510.20482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个模块化迁移学习方法的人口统计属性推断流水线，以提高面部分析系统公平性评估的可靠性。该方法通过整合预训练的人脸识别编码器与非线性分类头，在性别和种族推断任务上超越了基线方法，特别是在更具挑战性的种族属性上表现优异。研究还引入了通过身份一致性定义的鲁棒性指标，适用于任何人口统计分割方案。&lt;h4&gt;背景&lt;/h4&gt;面部分析系统中的公平性评估通常依赖于自动人口统计属性推断，而人口统计属性推断又依赖于预定义的人口统计分割。公平性审计的有效性取决于DAI过程的可靠性，但这一问题在以往研究中未得到充分重视。&lt;h4&gt;目的&lt;/h4&gt;提高DAI的可靠性，从而获得更少偏差和更低方差的面部分析系统公平性估计；提出一个完全可复现的DAI流水线；为公平审计中的人口统计推断提供可靠基础。&lt;h4&gt;方法&lt;/h4&gt;用模块化迁移学习方法替代传统的端到端训练；整合预训练的人脸识别编码器与非线性分类头；从准确性、公平性和新引入的鲁棒性（通过身份一致性定义）三个维度评估流水线；在多个数据集和训练设置上对性别和种族推断进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模块化迁移学习方法在性别和种族推断上优于强大的基线方法；在更具挑战性的种族属性上表现尤其出色；新引入的鲁棒性指标适用于任何人口统计分割方案。&lt;h4&gt;结论&lt;/h4&gt;该工作为公平审计中的人口统计推断提供了可靠的基础；通过公开训练数据集元数据、完整代码库、预训练模型和评估工具包，促进了研究的透明度和可复现性。&lt;h4&gt;翻译&lt;/h4&gt;面部分析系统中的公平性评估通常依赖于自动人口统计属性推断，而人口统计属性推断本身又依赖于预定义的人口统计分割。然而，公平性审计的有效性取决于DAI过程的可靠性。我们首先提供了这种依赖关系的理论动机，表明提高DAI可靠性可以减少偏差并降低面部分析系统公平性估计的方差。为此，我们提出了一个完全可复现的DAI流水线，用模块化迁移学习方法替代传统的端到端训练。我们的设计整合了预训练的人脸识别编码器与非线性分类头。我们从三个维度评估了这个流水线：准确性、公平性，以及通过身份一致性定义的新引入的鲁棒性概念。所提出的鲁棒性指标适用于任何人口统计分割方案。我们在多个数据集和训练设置上对性别和种族推断进行了基准测试。我们的结果表明，所提出的方法优于强大的基线方法，特别是在更具挑战性的种族属性上。为了促进透明度和可复现性，我们将公开训练数据集元数据、完整代码库、预训练模型和评估工具包。这项工作为公平审计中的人口统计推断提供了可靠的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fairness evaluation in face analysis systems (FAS) typically depends onautomatic demographic attribute inference (DAI), which itself relies onpredefined demographic segmentation. However, the validity of fairness auditinghinges on the reliability of the DAI process. We begin by providing atheoretical motivation for this dependency, showing that improved DAIreliability leads to less biased and lower-variance estimates of FAS fairness.To address this, we propose a fully reproducible DAI pipeline that replacesconventional end-to-end training with a modular transfer learning approach. Ourdesign integrates pretrained face recognition encoders with non-linearclassification heads. We audit this pipeline across three dimensions: accuracy,fairness, and a newly introduced notion of robustness, defined viaintra-identity consistency. The proposed robustness metric is applicable to anydemographic segmentation scheme. We benchmark the pipeline on gender andethnicity inference across multiple datasets and training setups. Our resultsshow that the proposed method outperforms strong baselines, particularly onethnicity, which is the more challenging attribute. To promote transparency andreproducibility, we will publicly release the training dataset metadata, fullcodebase, pretrained models, and evaluation toolkit. This work contributes areliable foundation for demographic inference in fairness auditing.</description>
      <author>example@mail.com (Alexandre Fournier-Montgieux, Hervé Le Borgne, Adrian Popescu, Bertrand Luvison)</author>
      <guid isPermaLink="false">2510.20482v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning identification of fractional-order vortex beam diffraction process</title>
      <link>http://arxiv.org/abs/2510.20245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于ResNet的深度学习方法，用于在衍射条件下准确识别分数阶涡旋光束的传播距离和拓扑荷，考虑了大气湍流的影响，实现了高精度的识别。&lt;h4&gt;背景&lt;/h4&gt;分数阶涡旋光束具有分数阶轨道角动量(FOAM)模式，理论上可以无限增加传输容量，在测量、光通信和微粒操纵等领域有重要应用前景。然而，当分数阶涡旋光束在自由空间传播时，其螺旋相位的连续性使其在实际应用中容易受到衍射的影响，从而影响OAM模式识别的准确性，严重限制了基于FOAM的光通信的使用。&lt;h4&gt;目的&lt;/h4&gt;实现衍射条件下分数阶涡旋光束的机器学习识别，解决目前尚未报道的紧急问题。&lt;h4&gt;方法&lt;/h4&gt;基于ResNet，提出了一种深度学习方法。利用实验测量和数值模拟的强度分布，创建了大气湍流环境中涡旋光束衍射强度模式的数据集。采用基于迁移学习的改进101层ResNet结构，实现不同传播距离下FOAM模型的准确高效识别。&lt;h4&gt;主要发现&lt;/h4&gt;该方法可以在湍流条件下准确识别传播距离为100厘米、间距为5厘米、模式间距为0.1的FOAM模式，准确率达到99.69%。该方法考虑了空间传输过程中大气湍流的影响，使得识别方案即使在特殊环境中也能实现高精度。它具有区分超细FOAM模式和传播距离的能力，这是传统方法无法实现的。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法解决了分数阶涡旋光束在衍射条件下识别的难题，特别是在考虑大气湍流影响的情况下，实现了高精度的FOAM模式识别，为实际应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;分数阶涡旋光束具有分数阶轨道角动量(FOAM)模式，理论上具有无限增加传输容量的潜力。因此，它们在测量、光通信和微粒操纵领域具有重要的应用前景。然而，当分数阶涡旋光束在自由空间传播时，螺旋相位的连续性使它们在实际应用中容易受到衍射的影响，从而影响OAM模式识别的准确性，严重限制了基于FOAM的光通信的使用。实现衍射条件下分数阶涡旋光束的机器学习识别目前是一个紧迫且尚未报道的问题。在本工作中，基于ResNet，提出了一种深度学习(DL)方法，用于准确识别分数阶涡旋光束衍射过程中的传播距离和拓扑荷。利用实验测量和数值模拟的强度分布，创建了大气湍流环境中涡旋光束衍射强度模式的数据集。采用基于迁移学习的改进101层ResNet结构，实现不同传播距离下FOAM模型的准确高效识别。实验结果表明，所提出的方法可以在湍流条件下准确识别传播距离为100厘米、间距为5厘米、模式间距为0.1的FOAM模式，准确率为99.69%。该方法考虑了空间传输过程中大气湍流的影响，使得识别方案即使在特殊环境中也能实现高精度。它具有区分超细FOAM模式和传播距离的能力，这是传统方法无法实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.7498/aps.74.20241458&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fractional-order vortex beams possess fractional orbital angular momentum(FOAM) modes, which theoretically have the potential to increase transmissioncapacity infinitely. Therefore, they have significant application prospects inthe fields of measurement, optical communication and micro-particlemanipulation. However, when fractional-order vortex beams propagate in freespace, the discontinuity of the helical phase makes them susceptible todiffraction in practical applications, thereby affecting the accuracy of OAMmode recognition and severely limiting the use of FOAM-based opticalcommunication. Achieving machine learning recognition of fractional-ordervortex beams under diffraction conditions is currently an urgent and unreportedissue. Based on ResNet, a deep learning (DL) method of accurately recognizingthe propagation distance and topological charge of fractional-order vortex beamdiffraction process is proposed in this work. Utilizing both experimentallymeasured and numerically simulated intensity distributions, a dataset of vortexbeam diffraction intensity patterns in atmospheric turbulence environments iscreated. An improved 101-layer ResNet structure based on transfer learning isemployed to achieve accurate and efficient recognition of the FOAM model atdifferent propagation distances. Experimental results show that the proposedmethod can accurately recognize FOAM modes with a propagation distance of 100cm, a spacing of 5 cm, and a mode spacing of 0.1 under turbulent conditions,with an accuracy of 99.69%. This method considers the effect of atmosphericturbulence during spatial transmission, allowing the recognition scheme toachieve high accuracy even in special environments. It has the ability todistinguish ultra-fine FOAM modes and propagation distances, which cannot beachieved by traditional methods.</description>
      <author>example@mail.com (Yan Guo, Heng Lyu, Chunling Ding, Chenzhi Yuan, Ruibo Jin)</author>
      <guid isPermaLink="false">2510.20245v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models</title>
      <link>http://arxiv.org/abs/2510.20033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇博士论文提出了三种改进序列标注任务迁移学习的方法，通过调整预训练神经语言模型来提高性能&lt;h4&gt;背景&lt;/h4&gt;序列标注任务的迁移学习需要更有效的预训练语言模型适应方法&lt;h4&gt;目的&lt;/h4&gt;改进序列标注任务的迁移学习方法，使预训练神经语言模型能够更好地适应特定任务&lt;h4&gt;方法&lt;/h4&gt;提出了三种改进方法：1) 引入额外信号的多任务模型；2) 修改自回归大语言模型架构以实现层间双向信息流；3) 利用监督上下文微调结合响应导向适应策略的序列标注框架&lt;h4&gt;主要发现&lt;/h4&gt;预训练神经语言模型在有针对性的迁移学习范式下，在序列标注任务上能够达到最佳性能&lt;h4&gt;结论&lt;/h4&gt;通过有针对性的迁移学习范式调整预训练神经语言模型，可以显著提高其在序列标注任务上的性能&lt;h4&gt;翻译&lt;/h4&gt;这篇博士论文通过调整预训练的神经语言模型，改进了序列标注任务的迁移学习。所提出的迁移学习改进包括引入一个额外信号的多任务模型、基于自回归大语言模型架构修改的方法，以及利用监督上下文微调结合响应导向适应策略的自回归大语言模型序列标注框架。第一个改进是在事件触发检测任务的领域迁移背景下提出的，通过将独立于领域的文本处理系统获得的额外信号整合到多任务模型中来改进领域迁移。第二个改进涉及修改模型架构，为此提出了一个方法，使自回归大语言模型的层之间能够实现双向信息流。第三个改进利用自回归大语言模型作为文本生成器，通过生成式监督上下文微调框架实现。所提出的模型、方法和框架表明，当通过有针对性的迁移学习范式进行调整时，预训练的神经语言模型在序列标注任务上能够达到最佳性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This doctoral thesis improves the transfer learning for sequence labelingtasks by adapting pre-trained neural language models. The proposed improvementsin transfer learning involve introducing a multi-task model that incorporatesan additional signal, a method based on architectural modifications inautoregressive large language models, and a sequence labeling framework forautoregressive large language models utilizing supervised in-contextfine-tuning combined with response-oriented adaptation strategies. The firstimprovement is given in the context of domain transfer for the event triggerdetection task. The domain transfer of the event trigger detection task can beimproved by incorporating an additional signal obtained from adomain-independent text processing system into a multi-task model. The secondimprovement involves modifying the model's architecture. For that purpose, amethod is proposed to enable bidirectional information flow across layers ofautoregressive large language models. The third improvement utilizesautoregressive large language models as text generators through a generativesupervised in-context fine-tuning framework. The proposed model, method, andframework demonstrate that pre-trained neural language models achieve theirbest performance on sequence labeling tasks when adapted through targetedtransfer learning paradigms.</description>
      <author>example@mail.com (David Dukić)</author>
      <guid isPermaLink="false">2510.20033v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title>
      <link>http://arxiv.org/abs/2510.13307v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于因果表示和推理的联合学习方法，用于解决点云分割中的新类别发现问题&lt;h4&gt;背景&lt;/h4&gt;在点云分割任务中，需要学习一个模型，仅使用已标记的基础类别监督信息，来分割未标记的新类别点云&lt;h4&gt;目的&lt;/h4&gt;建立点表示与基础类别标签之间的精确相关性，以及基础类别与新类别点之间的表示相关性&lt;h4&gt;方法&lt;/h4&gt;引入结构因果模型(SCM)重新形式化3D-NCD问题，分析基础类别表示中的隐藏混杂因素，设计消除混杂因素的因果表示原型，并使用图结构建模基础类别与新类别之间的因果关系&lt;h4&gt;主要发现&lt;/h4&gt;粗略或统计相关性学习可能导致新类别推理的混淆，而施加因果关系作为强相关约束可以准确揭示对应类别的本质点云表示&lt;h4&gt;结论&lt;/h4&gt;在3D和2D NCD语义分割任务上的大量实验和可视化结果证明了该方法的优势&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们专注于点云分割的新类别发现(3D-NCD)，旨在学习一个模型，仅使用已标记的基础3D类别的监督信息，来分割未标记的新3D类别。此任务的关键在于建立点表示与其基础类别标签之间的精确相关性，以及基础类别与新类别点之间的表示相关性。粗略或统计相关性学习可能导致新类别推理的混淆。如果我们将因果关系作为强相关约束强加于学习过程，应该能够准确揭示对应于类别的本质点云表示。为此，我们引入结构因果模型(SCM)重新形式化3D-NCD问题，并提出一种新方法，即因果表示和推理的联合学习。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别与新类别之间的因果关系。我们设计了一个消除混杂因素的因果表示原型，以捕获基础类别的因果表示。然后使用图结构建模基础类别的因果表示原型与新类别原型之间的因果关系，实现从基础类别到新类别的因果推理。在3D和2D NCD语义分割上的大量实验和可视化结果证明了我们方法的优势&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决点云分割中的新类别发现问题，即如何仅使用已标记的基础类别数据来分割未标记的新类别。这个问题在自动驾驶、机器人感知等实际应用中非常重要，因为这些场景中环境是动态开放的，可能会突然出现新的物体类别，而传统方法无法处理这些未预先定义的类别，限制了系统在真实世界中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法的局限性：它们本质上是统计模型，倾向于学习'捷径特征'而非本质特征，且难以处理新类别。因此，作者引入结构因果模型(SCM)重新形式化问题，识别出混杂因素对基础类别学习的干扰，以及基础到新类别的因果路径。方法设计借鉴了点云分割领域常用的MinkowskiUNet架构、因果学习理论中的独立因果机制原则、对抗训练思想以及图神经网络技术，将它们创新性地结合来解决3D-NCD问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过因果表示学习识别和去除点云中的非因果特征(混杂因素)，学习到能准确对应类别的本质表示，并利用基础类别的因果表示通过图结构建模基础到新类别的因果关系，实现因果推理。整体流程包括：1)因果表示原型学习，通过对抗训练去除混杂因素，提取基础类别的因果表示并生成原型；2)构建因果推理图，使用自注意力机制调整边权重，应用因果剪枝和推理方向一致性约束；3)基于图卷积网络生成高质量伪标签，实现新类别的分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：1)首次将因果学习引入3D-NCD领域；2)提出因果表示原型学习方法，通过对抗机制消除混杂因素；3)设计基于图的因果推理框架，显式建模类别间因果关系。相比之前工作，不同之处在于：传统方法依赖统计相似性且易受捷径特征干扰，而本文方法通过因果学习提取本质特征；传统方法直接测量类别相似性，而本文使用图结构建模复杂的高阶依赖关系；实验表明本文方法在多个数据集上显著优于现有方法，特别是在新类别分割任务上。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合因果表示学习和推理的新方法，通过识别和去除点云中的非因果特征并建模基础到新类别的因果关系，显著提升了点云分割中新类别的发现和分割能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classesusing only the supervision from labeled (base) 3D classes. The key to this taskis to setup the exact correlations between the point representations and theirbase class labels, as well as the representation correlations between thepoints from base and novel classes. A coarse or statistical correlationlearning may lead to the confusion in novel class inference. lf we impose acausal relationship as a strong correlated constraint upon the learningprocess, the essential point cloud representations that accurately correspondto the classes should be uncovered. To this end, we introduce a structuralcausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,i.e., Joint Learning of Causal Representation and Reasoning. Specifically, wefirst analyze hidden confounders in the base class representations and thecausal relationships between the base and novel classes through SCM. We devisea causal representation prototype that eliminates confounders to capture thecausal representations of base classes. A graph structure is then used to modelthe causal relationships between the base classes' causal representationprototypes and the novel class prototypes, enabling causal reasoning from baseto novel classes. Extensive experiments and visualization results on 3D and 2DNCD semantic segmentation demonstrate the superiorities of our method.</description>
      <author>example@mail.com (Yang Li, Aming Wu, Zihao Zhang, Yahong Han)</author>
      <guid isPermaLink="false">2510.13307v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process</title>
      <link>http://arxiv.org/abs/2510.20736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeruIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于狄利克雷过程(DP)的多模态学习框架，通过DP的'富者愈富'特性自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡，有效解决了多模态融合中保持特征表达能力和学习跨模态交互的挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态融合在医疗保健和金融等现实世界场景中变得越来越重要，关键挑战是如何在保持每个模态特征表达能力的同时学习跨模态交互。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多模态学习方法，避免过度强调模态边缘分布对齐带来的问题，同时保持每个模态内的有意义表示。&lt;h4&gt;方法&lt;/h4&gt;提出DP驱动的多模态学习框架，假设每个模态遵循多元高斯分布的混合，并采用狄利克雷过程计算所有组件的混合权重，利用其'富者愈富'特性动态分配特征贡献并选择最突出的特征。&lt;h4&gt;主要发现&lt;/h4&gt;在多个多模态数据集上的实验表明，该模型优于其他竞争方法；消融分析验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;DP驱动的多模态学习框架能够自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡，有效解决了多模态融合中的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;开发有效的多模态融合方法在医疗保健和金融等现实世界场景中变得越来越重要。关键挑战是如何在保持每个模态特征表达能力的同时学习跨模态交互。先前的方法主要关注跨模态对齐，但过度强调模态边缘分布的对齐可能会施加过度的正则化，并阻碍每个模态内的有意义表示。狄利克雷过程(DP)混合模型是一种强大的贝叶斯非参数方法，可以通过其'富者愈富'特性放大最突出的特征，为它们分配不断增加的权重。受DP这一独特特性的启发，我们提出了一种新的DP驱动的多模态学习框架，自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡。具体而言，我们假设每个模态遵循多元高斯分布的混合，并进一步采用DP计算所有组件的混合权重。这种范式允许DP动态分配特征的贡献并选择最突出的特征，利用其'富者愈富'特性，从而促进多模态特征融合。在多个多模态数据集上的广泛实验证明了我们的模型优于其他竞争模型。消融分析进一步验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。代码已在https://github.com/HKU-MedAI/DPMM.git匿名提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing effective multimodal fusion approaches has become increasinglyessential in many real-world scenarios, such as health care and finance. Thekey challenge is how to preserve the feature expressiveness in each modalitywhile learning cross-modal interactions. Previous approaches primarily focus onthe cross-modal alignment, while over-emphasis on the alignment of marginaldistributions of modalities may impose excess regularization and obstructmeaningful representations within each modality. The Dirichlet process (DP)mixture model is a powerful Bayesian non-parametric method that can amplify themost prominent features by its richer-gets-richer property, which allocatesincreasing weights to them. Inspired by this unique characteristic of DP, wepropose a new DP-driven multimodal learning framework that automaticallyachieves an optimal balance between prominent intra-modal representationlearning and cross-modal alignment. Specifically, we assume that each modalityfollows a mixture of multivariate Gaussian distributions and further adopt DPto calculate the mixture weights for all the components. This paradigm allowsDP to dynamically allocate the contributions of features and select the mostprominent ones, leveraging its richer-gets-richer property, thus facilitatingmultimodal feature fusion. Extensive experiments on several multimodal datasetsdemonstrate the superior performance of our model over other competitors.Ablation analysis further validates the effectiveness of DP in aligningmodality distributions and its robustness to changes in key hyperparameters.Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git</description>
      <author>example@mail.com (Tsai Hor Chan, Feng Wu, Yihang Chen, Guosheng Yin, Lequan Yu)</author>
      <guid isPermaLink="false">2510.20736v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>From Masks to Worlds: A Hitchhiker's Guide to World Models</title>
      <link>http://arxiv.org/abs/2510.20668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github: https://github.com/M-E-AGI-Lab/Awesome-World-Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文不是典型的世界模型综述，而是面向世界构建者的指南，聚焦于从早期掩码模型到记忆增强系统的世界模型发展路径&lt;h4&gt;背景&lt;/h4&gt;现有关于世界模型的文献分散且缺乏系统性，许多论文仅提及'世界模型'概念但未深入探讨&lt;h4&gt;目的&lt;/h4&gt;提供一条清晰的世界模型发展路径，专注于核心组件而非列举所有相关研究&lt;h4&gt;方法&lt;/h4&gt;遵循从跨模态表示学习的掩码模型，到统一架构，再到交互式生成模型，最后到记忆增强系统的发展脉络&lt;h4&gt;主要发现&lt;/h4&gt;世界模型的核心在于三个关键组件：生成核心、交互循环和记忆系统&lt;h4&gt;结论&lt;/h4&gt;通过关注这三个核心组件构建的系统是实现真正世界模型的最有前途路径&lt;h4&gt;翻译&lt;/h4&gt;这不是一篇典型的世界模型综述；这是一份面向那些想要构建世界的人的指南。我们的目标不是罗列所有曾经提及'世界模型'的论文。相反，我们遵循一条清晰的道路：从早期跨模态统一表示学习的掩码模型，到共享单一范式的统一架构，再到闭合动作-感知循环的交互式生成模型，最后到随时间保持一致世界的记忆增强系统。我们绕过了松散相关的分支，专注于核心：生成核心、交互循环和记忆系统。我们表明这是实现真正世界模型的最有前途的路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何构建真正的世界模型的问题。尽管有数百篇相关论文，但对于如何实际构建一个真正的世界模型还没有明确共识。这个问题很重要，因为真正的世界模型可以模拟整个环境，用于强化学习、智能体规划、大型语言模型模拟社会等多个领域。它能从预测引擎转变为'活的世界'，具有持久性、代理性和涌现性，对理解复杂系统、进行科学实验和创造交互体验具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析世界模型的历史发展，提出了一条'狭窄的道路'，将世界模型发展分为五个阶段：基于掩码的模型、统一模型、交互式生成模型、记忆与一致性系统，以及真正的世界模型。论文大量借鉴了现有工作，每个阶段都列举了代表性模型和方法，如BERT、MAE等（第一阶段），EMU3、Chameleon等（第二阶段），Genie系列等（第三阶段），RETRO、MemGPT等（第四阶段）。作者通过分析这些工作的演进，提出了构建真正世界模型的路径。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：真正的世界模型不是单一实体，而是由三个核心子系统合成的系统：生成核心（产生世界状态）、交互循环（实时关闭行动-感知循环）和持久记忆系统（随时间维持一致的世界）。整体实现流程遵循五个阶段：首先建立基于掩码的模型，为跨模态表示学习提供通用范式；然后统一架构，使单一架构能处理和生成多种模态；接着引入交互循环，将静态生成器转变为实时模拟器；然后添加记忆系统，使模拟能随时间持续；最后将这些组件合成为自主整体，实现真正的世界模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出了清晰的世界模型五阶段发展路线图；2）定义了真正世界模型的三个核心子系统；3）指出了构建真正世界模型的三个基本挑战：一致性问题、压缩问题和对齐问题；4）提出了真正世界模型的三个定义属性：持久性、代理性和涌现性。与之前工作相比，不同之处在于：它不是简单罗列论文，而是提供清晰发展路径；不仅关注技术细节，还关注哲学意义和潜在影响；将世界模型视为综合系统而非孤立组件；提出了构建真正世界模型的具体挑战和未来方向。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提供了一个从基于掩码的模型到真正世界模型的五阶段发展路线图，定义了真正世界模型的三个核心子系统和三个关键属性，并指出了构建真正世界模型的三个基本挑战，为构建能够持久、交互和涌现的'活的世界'提供了清晰的指南。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is not a typical survey of world models; it is a guide for those whowant to build worlds. We do not aim to catalog every paper that has evermentioned a ``world model". Instead, we follow one clear road: from earlymasked models that unified representation learning across modalities, tounified architectures that share a single paradigm, then to interactivegenerative models that close the action-perception loop, and finally tomemory-augmented systems that sustain consistent worlds over time. We bypassloosely related branches to focus on the core: the generative heart, theinteractive loop, and the memory system. We show that this is the mostpromising path towards true world models.</description>
      <author>example@mail.com (Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang)</author>
      <guid isPermaLink="false">2510.20668v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning</title>
      <link>http://arxiv.org/abs/2510.20644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025. Code available at  https://github.com/ReubenDo/JSDlowerbound/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了互信息(MI)与Jensen-Shannon散度(JSD)之间的关系，通过推导一个新的紧密且可处理的Kullback-Leibler散度(KLD)下界作为JSD的函数，建立了两者之间的理论联系。研究证明最大化基于JSD的信息会增加对互信息的保证下界，并通过实验验证了该方法的有效性和实用性，为在基于MI的表示学习中使用判别学习提供了理论依据和实证支持。&lt;h4&gt;背景&lt;/h4&gt;互信息(MI)是表示学习中广泛使用的基本统计依赖性度量。然而，直接通过其定义为Kullback-Leibler散度(KLD)来优化MI通常是不可行的。因此，最近的方法转而最大化替代的依赖性度量，特别是Jensen-Shannon散度(JSD)作为判别损失。但这些替代目标与MI之间的联系尚未被充分理解。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在填补替代目标(特别是基于JSD的目标)与互信息之间理论理解的空白，通过建立它们之间的严格数学关系，为在表示学习中使用判别学习提供理论依据。&lt;h4&gt;方法&lt;/h4&gt;研究通过推导一个新的、紧密且可处理的KLD下界作为JSD的函数来建立MI与JSD之间的理论联系。通过将这一边界专门应用于联合分布和边缘分布，证明了最大化基于JSD的信息会增加对互信息的保证下界。此外，研究重新审视了基于JSD目标的实际实现，并观察到最小化二元分类器的交叉熵损失可以恢复JSD的已知变分下界。&lt;h4&gt;主要发现&lt;/h4&gt;1. 推导出了一个新的、紧密且可处理的KLD下界作为JSD的函数；2. 证明了最大化基于JSD的信息会增加对互信息的保证下界；3. 最小化区分联合分布与边缘分布对的二元分类器的交叉熵损失可以恢复JSD的已知变分下界；4. 实验表明该下界应用于MI估计时是紧密的；5. 与最先进的神经变分下界估计器相比，该下界估计器提供了稳定的低方差估计；6. 在信息瓶颈框架中展示了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;研究的结果为在基于互信息的表示学习中使用判别学习提供了新的理论依据和强有力的实证证据。所提出的下界估计器能够提供对互信息的稳定、低方差估计，并且在信息瓶颈框架中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;互信息(MI)是表示学习中广泛使用的基本统计依赖性度量。虽然直接通过其定义为Kullback-Leibler散度(KLD)来优化MI通常是不可行的，但最近的方法转而最大化替代的依赖性度量，特别是通过判别损失来最大化联合分布与边缘分布乘积之间的Jensen-Shannon散度(JSD)。然而，这些替代目标与MI之间的联系尚未被充分理解。在本工作中，我们通过推导一个新的、紧密且可处理的KLD下界作为JSD的函数来填补这一空白。通过将这一边界专门应用于联合分布和边缘分布，我们证明了最大化基于JSD的信息会增加对互信息的保证下界。此外，我们重新审视了基于JSD目标的实际实现，并观察到最小化训练以区分联合分布与边缘分布对的二元分类器的交叉熵损失可以恢复JSD的已知变分下界。广泛的实验表明该下界应用于MI估计时是紧密的。我们将该下界与一系列既定参考场景中最先进的神经变分下界估计器进行了比较。我们的下界估计器一致地提供了对互信息紧密下界的稳定、低方差估计。我们还展示了其在信息瓶颈框架中的实际实用性。综上所述，我们的结果为在基于MI的表示学习中使用判别学习提供了新的理论依据和强有力的实证证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mutual Information (MI) is a fundamental measure of statistical dependencewidely used in representation learning. While direct optimization of MI via itsdefinition as a Kullback-Leibler divergence (KLD) is often intractable, manyrecent methods have instead maximized alternative dependence measures, mostnotably, the Jensen-Shannon divergence (JSD) between joint and product ofmarginal distributions via discriminative losses. However, the connectionbetween these surrogate objectives and MI remains poorly understood. In thiswork, we bridge this gap by deriving a new, tight, and tractable lower bound onKLD as a function of JSD in the general case. By specializing this bound tojoint and marginal distributions, we demonstrate that maximizing the JSD-basedinformation increases a guaranteed lower bound on mutual information.Furthermore, we revisit the practical implementation of JSD-based objectivesand observe that minimizing the cross-entropy loss of a binary classifiertrained to distinguish joint from marginal pairs recovers a known variationallower bound on the JSD. Extensive experiments demonstrate that our lower boundis tight when applied to MI estimation. We compared our lower bound tostate-of-the-art neural estimators of variational lower bound across a range ofestablished reference scenarios. Our lower bound estimator consistentlyprovides a stable, low-variance estimate of a tight lower bound on MI. We alsodemonstrate its practical usefulness in the context of the InformationBottleneck framework. Taken together, our results provide new theoreticaljustifications and strong empirical evidence for using discriminative learningin MI-based representation learning.</description>
      <author>example@mail.com (Reuben Dorent, Polina Golland, William Wells III)</author>
      <guid isPermaLink="false">2510.20644v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences</title>
      <link>http://arxiv.org/abs/2510.20595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为Diffusion Autoencoder with Perceivers (daep)的新架构，用于处理科学领域中不规则、多模态序列数据。该架构通过将异构测量值标记化，使用Perceiver编码器压缩，并使用Perceiver-IO扩散解码器重建，实现了在不同数据设置中的可扩展学习。研究还建立了maep作为基线模型，实验表明daep在重建误差、判别性潜在空间保存和精细结构保留方面均优于VAE和maep基线模型。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为表征学习的中心策略，但大多数用于编码数据的架构仅在规则采样的输入（如图像、音频和视频）上得到验证。然而，在许多科学领域中，数据是以长序列、不规则和多模态的形式出现的。&lt;h4&gt;目的&lt;/h4&gt;为了从这些不规则、多模态序列数据中提取语义信息，作者提出了daep架构，并建立了一个强基线模型maep，以评估daep的性能。&lt;h4&gt;方法&lt;/h4&gt;daep架构通过以下步骤工作：将异构测量值标记化，使用Perceiver编码器进行压缩，使用Perceiver-IO扩散解码器进行重建。为了评估daep，作者将掩码自编码器调整为Perceiver编码器/解码器设计，建立了maep基线模型。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的光谱和光度天文数据集上，daep实现了比VAE和maep基线模型更低的重建误差，产生了更具判别性的潜在空间，并更好地保留了精细结构。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明daep是处理科学领域中不规则、异构序列数据的有效框架。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为表征学习的中心策略，但大多数用于编码数据的架构仅在规则采样的输入（如图像、音频和视频）上得到验证。在许多科学领域中，数据实际上是以长序列、不规则和多模态的形式出现的。为了从这些数据中提取语义信息，我们引入了带有Perceiver的扩散自编码器（daep）。daep将异构测量值标记化，使用Perceiver编码器压缩它们，并使用Perceiver-IO扩散解码器重建它们，从而能够在各种数据设置中实现可扩展学习。为了对daep架构进行基准测试，我们将掩码自编码器调整为Perceiver编码器/解码器设计，并在与daep相同的架构家族中建立了强基线（maep）。在多样化的光谱和光度天文数据集上，daep实现了比VAE和maep基线模型更低的重建误差，产生更具判别性的潜在空间，并更好地保留了精细结构。这些结果确立了daep作为科学领域中数据以不规则、异构序列形式出现的有效框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has become a central strategy for representationlearning, but the majority of architectures used for encoding data have onlybeen validated on regularly-sampled inputs such as images, audios. and videos.In many scientific domains, data instead arrive as long, irregular, andmultimodal sequences. To extract semantic information from these data, weintroduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizesheterogeneous measurements, compresses them with a Perceiver encoder, andreconstructs them with a Perceiver-IO diffusion decoder, enabling scalablelearning in diverse data settings. To benchmark the daep architecture, we adaptthe masked autoencoder to a Perceiver encoder/decoder design, and establish astrong baseline (maep) in the same architectural family as daep. Across diversespectroscopic and photometric astronomical datasets, daep achieves lowerreconstruction errors, produces more discriminative latent spaces, and betterpreserves fine-scale structure than both VAE and maep baselines. These resultsestablish daep as an effective framework for scientific domains where dataarrives as irregular, heterogeneous sequences.</description>
      <author>example@mail.com (Yunyi Shen, Alexander Gagliano)</author>
      <guid isPermaLink="false">2510.20595v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval</title>
      <link>http://arxiv.org/abs/2510.20393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM Multimedia 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的因果方法，用于解决图像到食谱检索中的表示偏差问题，通过预测并注入图像中可能被忽视的烹饪元素，提高了检索性能。&lt;h4&gt;背景&lt;/h4&gt;现有图像到食谱检索方法假设食物图像能完全捕捉食谱细节，但实际上图像只反映烹饪结果而非过程。这导致跨模态表示学习忽略视觉上不明显但对检索关键的细节，使表示偏向主要视觉元素，难以区分相似食谱。当训练数据包含不同菜系时，这种偏差更严重。&lt;h4&gt;目的&lt;/h4&gt;提出一种因果方法，预测图像中可能被忽视的烹饪元素，并将这些元素明确注入跨模态表示学习中，以减轻偏差。&lt;h4&gt;方法&lt;/h4&gt;采用因果方法预测图像中可能被忽视的烹饪元素，并将这些元素注入跨模态表示学习过程。在标准单语Recipe1M数据集和新策划的多语言多文化菜系数据集上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;提出的因果表示学习能够揭示细微的成分和烹饪动作，在单语和多语言多文化数据集上都取得了令人印象深刻的检索性能。&lt;h4&gt;结论&lt;/h4&gt;通过因果方法预测并注入图像中可能被忽视的烹饪元素，可以有效减轻跨模态表示学习中的偏差，特别是在处理不同菜系的图像和食谱时效果显著。&lt;h4&gt;翻译&lt;/h4&gt;现有的图像到食谱检索方法隐含假设食物图像可以完全捕捉其食谱中文本记录的细节。然而，食物图像只反映了烹饪菜肴的视觉结果，而不是底层的烹饪过程。因此，学习跨模态表示来弥合图像和食谱之间的模态差距时，往往会忽略那些在视觉上不明显但对食谱检索至关重要的细微、特定于食谱的细节。具体来说，这些表示偏向于捕捉主要的视觉元素，导致难以对在使用成分和烹饪方法上有细微差异的相似食谱进行排序。当训练数据混合来自不同菜系的图像和食谱时，表示学习中的偏差预计会更严重。本文提出了一种新的因果方法，预测图像中可能被忽视的烹饪元素，同时明确地将这些元素注入跨模态表示学习中以减轻偏差。实验在标准的单语Recipe1M数据集和一个新策划的多语言多文化菜系数据集上进行。结果表明，提出的因果表示学习能够揭示细微的成分和烹饪动作，并在单语和多语言多文化数据集上都取得了令人印象深刻的检索性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing approaches for image-to-recipe retrieval have the implicitassumption that a food image can fully capture the details textually documentedin its recipe. However, a food image only reflects the visual outcome of acooked dish and not the underlying cooking process. Consequently, learningcross-modal representations to bridge the modality gap between images andrecipes tends to ignore subtle, recipe-specific details that are not visuallyapparent but are crucial for recipe retrieval. Specifically, therepresentations are biased to capture the dominant visual elements, resultingin difficulty in ranking similar recipes with subtle differences in use ofingredients and cooking methods. The bias in representation learning isexpected to be more severe when the training data is mixed of images andrecipes sourced from different cuisines. This paper proposes a novel causalapproach that predicts the culinary elements potentially overlooked in images,while explicitly injecting these elements into cross-modal representationlearning to mitigate biases. Experiments are conducted on the standardmonolingual Recipe1M dataset and a newly curated multilingual multiculturalcuisine dataset. The results indicate that the proposed causal representationlearning is capable of uncovering subtle ingredients and cooking actions andachieves impressive retrieval performance on both monolingual and multilingualmulticultural datasets.</description>
      <author>example@mail.com (Qing Wang, Chong-Wah Ngo, Yu Cao, Ee-Peng Lim)</author>
      <guid isPermaLink="false">2510.20393v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GUSL-Dehaze: A Green U-Shaped Learning Approach to Image Dehazing</title>
      <link>http://arxiv.org/abs/2510.20266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GUSL-Dehaze是一种绿色U型学习方法的图像去雾技术，结合了基于物理的模型与绿色学习框架，避免了深度学习的高计算成本和大参数量问题。&lt;h4&gt;背景&lt;/h4&gt;图像去雾是恢复清晰图像的任务，传统方法依赖统计先验和物理模型，而最先进的方法主要基于深度学习，但这些方法计算成本高且参数量大，不适合资源受限设备。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、透明的图像去雾方法，避免深度学习的高计算成本，同时保持与最先进方法相当的性能。&lt;h4&gt;方法&lt;/h4&gt;GUSL-Dehaze采用改进的暗通道先验进行初始去雾，然后通过U型架构实现绿色学习流程，使用无监督表示学习进行特征提取，并结合相关特征测试和最小二乘归一化变换等特征工程技术，最后通过透明的监督学习策略获得去雾图像。&lt;h4&gt;主要发现&lt;/h4&gt;GUSL-Dehaze显著减少了参数数量，同时确保了数学可解释性，并取得了与最先进深度学习模型相当的性能。&lt;h4&gt;结论&lt;/h4&gt;GUSL-Dehaze为图像去雾提供了一种轻量级、透明的替代方案，避免了深度学习的计算负担，同时保持了高性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;图像去雾是一项恢复任务，旨在从单幅有雾输入中恢复清晰图像。传统方法依赖于统计先验和基于物理的大气散射模型来重建无雾图像。虽然最近最先进的方法主要基于深度学习架构，但这些模型通常涉及高计算成本和大参数量，使其不适合资源受限设备。在本工作中，我们提出了GUSL-Dehaze，一种绿色U型学习方法的图像去雾技术。我们的方法将基于物理的模型与绿色学习框架相结合，提供了比传统深度学习技术更轻量、更透明的替代方案。与基于神经网络的解决方案不同，GUSL-Dehaze完全避免了深度学习。相反，我们首先使用改进的暗通道先验进行初始去雾步骤，然后通过U型架构实现绿色学习流程。该架构采用无监督表示学习进行有效特征提取，并结合相关特征测试和最小二乘归一化变换等特征工程技术来保持紧凑的模型大小。最后，通过透明的监督学习策略获得去雾图像。GUSL-Dehaze显著减少了参数数量，同时确保了数学可解释性，并取得了与最先进深度学习模型相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image dehazing is a restoration task that aims to recover a clear image froma single hazy input. Traditional approaches rely on statistical priors and thephysics-based atmospheric scattering model to reconstruct the haze-free image.While recent state-of-the-art methods are predominantly based on deep learningarchitectures, these models often involve high computational costs and largeparameter sizes, making them unsuitable for resource-constrained devices. Inthis work, we propose GUSL-Dehaze, a Green U-Shaped Learning approach to imagedehazing. Our method integrates a physics-based model with a green learning(GL) framework, offering a lightweight, transparent alternative to conventionaldeep learning techniques. Unlike neural network-based solutions, GUSL-Dehazecompletely avoids deep learning. Instead, we begin with an initial dehazingstep using a modified Dark Channel Prior (DCP), which is followed by a greenlearning pipeline implemented through a U-shaped architecture. Thisarchitecture employs unsupervised representation learning for effective featureextraction, together with feature-engineering techniques such as the RelevantFeature Test (RFT) and the Least-Squares Normal Transform (LNT) to maintain acompact model size. Finally, the dehazed image is obtained via a transparentsupervised learning strategy. GUSL-Dehaze significantly reduces parameter countwhile ensuring mathematical interpretability and achieving performance on parwith state-of-the-art deep learning models.</description>
      <author>example@mail.com (Mahtab Movaheddrad, Laurence Palmer, C. -C. Jay Kuo)</author>
      <guid isPermaLink="false">2510.20266v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</title>
      <link>http://arxiv.org/abs/2510.20214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the preprint version of the manuscript submitted to IEEE  Journal of Biomedical and Health Informatics (JBHI) for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CURL的新型自监督学习框架，用于从胎儿超声视频中准确检测胎儿运动，解决了传统方法的主观性和准确性有限的问题。&lt;h4&gt;背景&lt;/h4&gt;准确的胎儿运动检测对于评估产前健康至关重要，异常的运动模式可能表明存在潜在的并发症，如胎盘功能障碍或胎儿窘迫。传统方法包括母亲感知和胎心宫缩监护图，但这些方法存在主观性和准确性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统胎儿运动检测方法的挑战，研究人员提出了一种新型自监督学习框架CURL，用于从延长的胎儿超声视频记录中检测胎儿运动。&lt;h4&gt;方法&lt;/h4&gt;CURL方法利用双重对比损失，结合空间和时间对比学习，来学习鲁棒的运动表示。此外，研究还引入了一种特定任务的采样策略，确保在自监督训练过程中有效分离运动和非运动段，同时通过概率微调方法实现对任意长度的超声记录的灵活推断。&lt;h4&gt;主要发现&lt;/h4&gt;在包含92名受试者（每人进行30分钟超声检查）的内部数据集上评估，CURL达到了78.01%的敏感性和81.60%的AUROC，证明了其在胎儿运动分析方面的可靠性和客观性。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了自监督对比学习在胎儿运动分析中的潜力，为改进产前监测和临床决策铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;准确的胎儿运动检测对于评估产前健康至关重要，因为异常的运动模式可能表明存在潜在的并发症，如胎盘功能障碍或胎儿窘迫。传统方法，包括母亲感知和胎心宫缩监护图，存在主观性和准确性有限的问题。为了解决这些挑战，我们提出了对比超声视频表示学习，这是一种新颖的自监督学习框架，用于从延长的胎儿超声视频记录中检测胎儿运动。我们的方法利用双重对比损失，结合空间和时间对比学习，来学习鲁棒的运动表示。此外，我们引入了一种特定任务的采样策略，确保在自监督训练过程中有效分离运动和非运动段，同时通过概率微调方法实现对任意长度的超声记录的灵活推断。在包含92名受试者（每人进行30分钟超声检查）的内部数据集上评估，CURL达到了78.01%的敏感性和81.60%的AUROC，证明了其在胎儿运动分析方面的可靠性和客观性。这些结果突显了自监督对比学习在胎儿运动分析中的潜力，为改进产前监测和临床决策铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate fetal movement (FM) detection is essential for assessing prenatalhealth, as abnormal movement patterns can indicate underlying complicationssuch as placental dysfunction or fetal distress. Traditional methods, includingmaternal perception and cardiotocography (CTG), suffer from subjectivity andlimited accuracy. To address these challenges, we propose ContrastiveUltrasound Video Representation Learning (CURL), a novel self-supervisedlearning framework for FM detection from extended fetal ultrasound videorecordings. Our approach leverages a dual-contrastive loss, incorporating bothspatial and temporal contrastive learning, to learn robust motionrepresentations. Additionally, we introduce a task-specific sampling strategy,ensuring the effective separation of movement and non-movement segments duringself-supervised training, while enabling flexible inference on arbitrarily longultrasound recordings through a probabilistic fine-tuning approach. Evaluatedon an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating itspotential for reliable and objective FM analysis. These results highlight thepotential of self-supervised contrastive learning for fetal movement analysis,paving the way for improved prenatal monitoring and clinical decision-making.</description>
      <author>example@mail.com (Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad)</author>
      <guid isPermaLink="false">2510.20214v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development</title>
      <link>http://arxiv.org/abs/2510.20196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了54个公开可用的脑部MRI数据集，评估了数据规模、多样性和一致性对基础模型开发的影响，发现数据集间存在显著不平衡和异质性，预处理无法完全消除数据集间的偏差，强调了设计脑部MRI基础模型时需要考虑预处理感知和领域自适应策略的必要性。&lt;h4&gt;背景&lt;/h4&gt;脑部MRI基础模型的发展依赖于可用数据的规模、多样性和一致性，然而对这些因素的系统评估仍然很少见。&lt;h4&gt;目的&lt;/h4&gt;分析54个公开可用的脑部MRI数据集（包含超过538,031个样本），为脑部MRI基础模型开发提供结构化、多层次的概述。&lt;h4&gt;方法&lt;/h4&gt;在数据集层面分析模态组成、疾病覆盖范围和数据集规模；在图像层面量化15个代表性数据集中的体素间距、方向和强度分布；评估预处理步骤（强度归一化、偏置场校正、颅骨剥离、空间配准和插值）对体素统计和几何形状的影响；使用3D DenseNet121进行特征空间案例研究，评估标准化预处理后的协变量偏移。&lt;h4&gt;主要发现&lt;/h4&gt;数据集层面存在大型健康队列与较小临床人群之间的严重不平衡；图像层面存在显著的异质性，可能影响表示学习；预处理步骤虽提高了数据集内部一致性，但数据集间的残余差异仍然存在；标准化预处理后仍可测量到残余协变量偏移，确认仅靠调和无法消除数据集间的偏差。&lt;h4&gt;结论&lt;/h4&gt;这些分析提供了公共脑部MRI资源中变异性的统一表征，强调在设计可推广的脑部MRI基础模型时需要考虑预处理感知和领域自适应策略。&lt;h4&gt;翻译&lt;/h4&gt;脑部MRI基础模型的发展在很大程度上取决于可用数据的规模、多样性和一致性，然而对这些因素的系统评估仍然很少。在本研究中，我们分析了54个公开可用的脑部MRI数据集，包含超过538,031个样本，为基础模型开发提供了结构化、多层次的概述。在数据集层面，我们表征了模态组成、疾病覆盖范围和数据集规模，揭示了大型健康队列与较小临床人群之间的严重不平衡。在图像层面，我们量化了15个代表性数据集中的体素间距、方向和强度分布，证明了可能影响表示学习的显著异质性。然后，我们对预处理变异性进行了定量评估，检查了强度归一化、偏置场校正、颅骨剥离、空间配准和插值如何改变体素统计和几何形状。虽然这些步骤提高了数据集内部的一致性，但数据集之间的残余差异仍然存在。最后，使用3D DenseNet121进行的特征空间案例研究显示，在标准化预处理后仍可测量到残余协变量偏移，确认仅靠调和无法消除数据集间的偏差。总之，这些分析提供了公共脑部MRI资源中变异性的统一表征，并强调了在设计可推广的脑部MRI基础模型时需要考虑预处理感知和领域自适应策略的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of foundation models for brain MRI depends critically on thescale, diversity, and consistency of available data, yet systematic assessmentsof these factors remain scarce. In this study, we analyze 54 publiclyaccessible brain MRI datasets encompassing over 538,031 to provide astructured, multi-level overview tailored to foundation model development. Atthe dataset level, we characterize modality composition, disease coverage, anddataset scale, revealing strong imbalances between large healthy cohorts andsmaller clinical populations. At the image level, we quantify voxel spacing,orientation, and intensity distributions across 15 representative datasets,demonstrating substantial heterogeneity that can influence representationlearning. We then perform a quantitative evaluation of preprocessingvariability, examining how intensity normalization, bias field correction,skull stripping, spatial registration, and interpolation alter voxel statisticsand geometry. While these steps improve within-dataset consistency, residualdifferences persist between datasets. Finally, feature-space case study using a3D DenseNet121 shows measurable residual covariate shift after standardizedpreprocessing, confirming that harmonization alone cannot eliminateinter-dataset bias. Together, these analyses provide a unified characterizationof variability in public brain MRI resources and emphasize the need forpreprocessing-aware and domain-adaptive strategies in the design ofgeneralizable brain MRI foundation models.</description>
      <author>example@mail.com (Minh Sao Khue Luu, Margaret V. Benedichuk, Ekaterina I. Roppert, Roman M. Kenzhin, Bair N. Tuchinov)</author>
      <guid isPermaLink="false">2510.20196v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks</title>
      <link>http://arxiv.org/abs/2510.20165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the Thirty Fifth AAAI Conference on  Artificial Intelligence (AAAI 2021), paper number 7926&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于GAN的无监督解纠缠表征学习模型IB-GAN，利用信息瓶颈框架优化GAN，通过生成器的中间层约束输入与生成输出之间的互信息，实现了对潜在空间的解纠缠和可解释性利用。&lt;h4&gt;背景&lt;/h4&gt;解纠缠表征学习是机器学习领域的重要研究方向，现有的方法如InfoGAN和β-VAEs存在一定局限性，需要改进模型架构以获得更好的解纠缠能力和样本质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于GAN的无监督解纠缠表征学习模型IB-GAN，利用信息瓶颈框架优化GAN，实现更好的解纠缠能力和样本质量。&lt;h4&gt;方法&lt;/h4&gt;IB-GAN架构与InfoGAN部分相似但有关键差异：利用生成器的中间层约束输入与生成输出之间的互信息；中间随机层可作为可学习的潜在分布，与生成器端到端联合训练；使生成器能够以解纠缠和可解释的方式利用潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;在dSprites和Color-dSprites数据集上的实验表明，IB-GAN实现了与最先进的β-VAEs相当的解纠缠分数，并优于InfoGAN；在CelebA和3D Chairs数据集上，IB-GAN在FID分数方面通常比β-VAEs和Info-GAN生成的样本具有更好的视觉质量和多样性。&lt;h4&gt;结论&lt;/h4&gt;IB-GAN通过利用信息瓶颈框架优化GAN，有效提升了模型的解纠缠能力和样本生成质量，是一种有效的无监督解纠缠表征学习方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的基于GAN的无监督解纠缠表征学习模型。这一新模型是在尝试将信息瓶颈框架应用于GAN优化的过程中发现的，因此命名为IB-GAN。IB-GAN的架构与InfoGAN部分相似，但有一个关键区别：利用生成器的中间层来约束输入与生成输出之间的互信息。中间随机层可以作为可学习的潜在分布，与生成器以端到端的方式联合训练。因此，IB-GAN的生成器能够以解纠缠和可解释的方式利用潜在空间。在dSprites和Color-dSprites数据集上的实验表明，IB-GAN实现了与最先进的β-VAEs相当的解纠缠分数，并优于InfoGAN。此外，在CelebA和3D Chairs数据集上，IB-GAN生成的样本在FID分数方面通常比β-VAEs和Info-GAN具有更好的视觉质量和多样性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v35i9.16967&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new GAN-based unsupervised model for disentangled representationlearning. The new model is discovered in an attempt to utilize the InformationBottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. Thearchitecture of IB-GAN is partially similar to that of InfoGAN but has acritical difference; an intermediate layer of the generator is leveraged toconstrain the mutual information between the input and the generated output.The intermediate stochastic layer can serve as a learnable latent distributionthat is trained with the generator jointly in an end-to-end fashion. As aresult, the generator of IB-GAN can harness the latent space in a disentangledand interpretable manner. With the experiments on dSprites and Color-dSpritesdataset, we demonstrate that IB-GAN achieves competitive disentanglement scoresto those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover,the visual quality and the diversity of samples generated by IB-GAN are oftenbetter than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebAand 3D Chairs dataset.</description>
      <author>example@mail.com (Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim)</author>
      <guid isPermaLink="false">2510.20165v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</title>
      <link>http://arxiv.org/abs/2510.20162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的组合零样本学习方法，通过无监督数据积累多模态知识并更新原型，解决了测试时分布偏移的问题，在多个基准数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;组合零样本学习旨在基于已学习知识识别新的属性-对象组合。现有方法在测试时由于标签空间分布偏移导致性能下降，这种偏移源于包含了从未见过的属性和对象重新组合的样本。&lt;h4&gt;目的&lt;/h4&gt;克服测试时标签空间分布偏移带来的挑战，提出一种方法来更新多模态原型，使模型能够灵活适应测试时的分布偏移。&lt;h4&gt;方法&lt;/h4&gt;提出一种新方法，通过无监督数据积累文本和视觉模态的综合知识；设计自适应更新权重控制原型调整程度；引入动态优先队列存储高置信度图像，从历史图像获取视觉知识；通过多模态协同表示学习对齐文本和视觉原型。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上，无论是在封闭世界还是开放世界设置下，该方法都达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过更新多模态原型和自适应权重，有效解决了组合零样本学习中的分布偏移问题，代码将在https://github.com/xud-yan/TOMCAT上提供。&lt;h4&gt;翻译&lt;/h4&gt;组合零样本学习旨在基于已学习知识识别新的属性-对象组合。现有方法在测试时由于标签空间分布偏移导致性能下降，这源于包含了从未见过的属性和对象重新组合的样本。为克服这一挑战，我们提出了一种新方法，通过无监督数据在文本和视觉模态中积累综合知识，以在测试时更新多模态原型。基于此，我们进一步设计了自适应更新权重来控制原型调整程度，使模型能够在测试过程中灵活适应分布偏移。此外，我们引入了动态优先队列，存储高置信度图像，以便从历史图像获取视觉知识进行推理。考虑到多模态知识的语义一致性，我们通过多模态协同表示学习对齐文本和视觉原型。大量实验表明，我们的方法在四个基准数据集上，无论是在封闭世界还是开放世界设置下，都达到了最先进的性能。代码将在https://github.com/xud-yan/TOMCAT上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compositional Zero-Shot Learning (CZSL) aims to recognize novelattribute-object compositions based on the knowledge learned from seen ones.Existing methods suffer from performance degradation caused by the distributionshift of label space at test time, which stems from the inclusion of unseencompositions recombined from attributes and objects. To overcome the challenge,we propose a novel approach that accumulates comprehensive knowledge in bothtextual and visual modalities from unsupervised data to update multimodalprototypes at test time. Building on this, we further design an adaptive updateweight to control the degree of prototype adjustment, enabling the model toflexibly adapt to distribution shift during testing. Moreover, a dynamicpriority queue is introduced that stores high-confidence images to acquirevisual knowledge from historical images for inference. Considering the semanticconsistency of multimodal knowledge, we align textual and visual prototypes bymultimodal collaborative representation learning. Extensive experimentsindicate that our approach achieves state-of-the-art performance on fourbenchmark datasets under both closed-world and open-world settings. Code willbe available at https://github.com/xud-yan/TOMCAT .</description>
      <author>example@mail.com (Xudong Yan, Songhe Feng)</author>
      <guid isPermaLink="false">2510.20162v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Improving Predictive Confidence in Medical Imaging via Online Label Smoothing</title>
      <link>http://arxiv.org/abs/2510.20011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and presented in International Conference on Advancing  Science and Technologies in Health Science&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了在线标签平滑(OLS)在医学图像分类中的应用，结果显示OLS能提高分类准确率并改善模型校准性。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型，特别是卷积神经网络，在医学图像分类中取得了显著成果，但这些模型经常产生过度自信的预测，影响在关键医疗环境中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;研究使用在线标签平滑(OLS)，一种基于模型自身预测模式动态调整软标签的方法，以提高医学图像分类模型的性能和可靠性。&lt;h4&gt;方法&lt;/h4&gt;在大型RadImageNet数据集上使用三种架构评估OLS：ResNet-50、MobileNetV2和VGG-19，并与标准训练方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;OLS相比标准训练方法持续提高了Top-1和Top-5分类准确率，并产生更紧凑和良好分离的特征嵌入，表明表示学习得到改善。&lt;h4&gt;结论&lt;/h4&gt;OLS不仅增强了预测性能，还提高了校准性，使其成为医学成像领域开发可信AI系统的实用有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型，特别是卷积神经网络，在医学图像分类中已取得了令人印象深刻的结果。然而，这些模型通常会产生过度自信的预测，这可能削弱它们在关键医疗环境中的可靠性。虽然传统的标签平滑提供了一种减少这种过度自信的简单方法，但它未能考虑类别之间的关系，将所有非目标类别同等对待。在本研究中，我们探索了在线标签平滑(OLS)的使用，这是一种动态方法，基于模型自身的预测模式在训练过程中调整软标签。我们在大型RadImageNet数据集上使用三种广泛使用的架构评估了OLS：ResNet-50、MobileNetV2和VGG-19。我们的结果表明，与标准训练方法（包括硬标签、传统标签平滑和无教师知识蒸馏）相比，OLS持续提高了Top-1和Top-5分类准确率。除了准确率的提升外，OLS还导致更紧凑和良好分离的特征嵌入，表明表示学习得到改善。这些发现表明，OLS不仅增强了预测性能，还提高了校准性，使其成为医学成像领域开发可信AI系统的实用有效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models, especially convolutional neural networks, have achievedimpressive results in medical image classification. However, these models oftenproduce overconfident predictions, which can undermine their reliability incritical healthcare settings. While traditional label smoothing offers a simpleway to reduce such overconfidence, it fails to consider relationships betweenclasses by treating all non-target classes equally. In this study, we explorethe use of Online Label Smoothing (OLS), a dynamic approach that adjusts softlabels throughout training based on the model's own prediction patterns. Weevaluate OLS on the large-scale RadImageNet dataset using three widely usedarchitectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLSconsistently improves both Top-1 and Top-5 classification accuracy compared tostandard training methods, including hard labels, conventional label smoothing,and teacher-free knowledge distillation. In addition to accuracy gains, OLSleads to more compact and well-separated feature embeddings, indicatingimproved representation learning. These findings suggest that OLS not onlystrengthens predictive performance but also enhances calibration, making it apractical and effective solution for developing trustworthy AI systems in themedical imaging domain.</description>
      <author>example@mail.com (Kushan Choudhury, Shubhrodeep Roy, Ankur Chanda, Shubhajit Biswas, Somenath Kuiry)</author>
      <guid isPermaLink="false">2510.20011v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Transformed Multi-view 3D Shape Features with Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.19955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了3D形状特征表示学习的挑战，通过将Vision Transformers架构与对比学习目标相结合，在3D形状理解方面取得了良好效果&lt;h4&gt;背景&lt;/h4&gt;计算机视觉方法在从2D图像识别3D物体方面存在困难，通常需要大量标记数据，且依赖的卷积神经网络可能会忽略关键的形状关系&lt;h4&gt;目的&lt;/h4&gt;解决3D形状特征表示学习中的挑战，探索减少对大量标记数据依赖的方法，克服CNNs在捕获关键形状关系方面的局限性&lt;h4&gt;方法&lt;/h4&gt;使用Vision Transformers (ViTs)架构与现代对比目标相结合，进行多视图3D分析，结合ViTs理解整体形状的能力和对比学习的有效性&lt;h4&gt;主要发现&lt;/h4&gt;监督对比损失在ModelNet10上达到了约90.6%的准确率；ViTs能够捕获全局形状语义，而对比优化能够完善局部判别特征&lt;h4&gt;结论&lt;/h4&gt;通过结合ViTs与对比目标，成功实现了3D表示学习，这种方法基于大量实验评估，证明了其有效性&lt;h4&gt;翻译&lt;/h4&gt;这篇论文通过研究最先进的骨干网络与对比监督和自监督学习目标的组合，解决了3D形状特征表示学习中的挑战。计算机视觉方法在从2D图像识别3D物体方面存在困难，通常需要大量标记数据，并依赖于卷积神经网络(CNNs)，而这些网络可能会忽略关键的形状关系。我们的研究表明，当Vision Transformers (ViTs)架构与现代对比目标配对时，在我们的下游任务中多视图3D分析取得了有希望的结果，统一了对比学习和3D形状理解的流程。例如，监督对比损失在ModelNet10上达到了约90.6%的准确率。ViTs和对比学习的应用，利用了ViTs理解整体形状的能力和对比学习的有效性，克服了对大量标记数据的需求以及CNNs在捕获关键形状关系方面的局限性。成功的原因在于通过ViTs捕获全局形状语义，并通过对比优化完善局部判别特征。重要的是，我们的方法是经验性的，因为它基于大量的实验评估来验证将ViTs与对比目标相结合用于3D表示学习的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D形状特征表示学习中的挑战，特别是计算机视觉方法从2D图像识别3D物体的困难。这个问题很重要，因为3D形状理解对机器人、虚拟现实等应用至关重要，而当前方法需要大量标记数据且依赖CNN，这些网络可能忽略关键的形状关系，限制了模型在真实世界应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CNN在3D形状理解中的局限性，注意到ViT架构在视觉识别任务中表现优异，并观察到对比学习在利用未标记数据方面的潜力。他们借鉴了MVCNN(首次使用CNN进行3D形状理解)、ViT架构以及多种对比学习方法(如InfoNCE、SimCLR、SupCon)，创新性地将这些技术结合应用于3D多视图形状理解，这是之前研究较少探索的组合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用Vision Transformer的注意力机制捕获全局形状语义，通过对比学习优化细化局部判别特征，统一对比学习和3D形状理解流程。整体流程分为两个阶段：第一阶段是多视图渲染和对比学习，从3D网格生成12个视图图像，使用多种对比损失函数训练ViT和CNN骨干；第二阶段是下游任务评估，包括分类(线性评估、k-NN分类、t-SNE可视化)和检索任务(基于余弦相似度排序，使用mAP评估)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将ViT与对比学习结合用于3D多视图形状理解；系统评估四种ViT骨干和五种对比损失函数；统一对比学习和3D形状理解流程；在多个下游任务进行全面评估。相比之前工作，本文主要使用ViT而非CNN骨干；将对比学习从2D扩展到3D多视图数据；提供更全面的评估；在ModelNet10上达到90.6%分类准确率和95.5%的mAP，超越之前方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将Vision Transformer与先进的对比学习目标相结合，显著提升了3D多视图形状理解的性能，同时减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges in representation learning of 3D shapefeatures by investigating state-of-the-art backbones paired with bothcontrastive supervised and self-supervised learning objectives. Computer visionmethods struggle with recognizing 3D objects from 2D images, often requiringextensive labeled data and relying on Convolutional Neural Networks (CNNs) thatmay overlook crucial shape relationships. Our work demonstrates that VisionTransformers (ViTs) based architectures, when paired with modern contrastiveobjectives, achieve promising results in multi-view 3D analysis on ourdownstream tasks, unifying contrastive and 3D shape understanding pipelines.For example, supervised contrastive losses reached about 90.6% accuracy onModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' abilityto understand overall shapes and contrastive learning's effectiveness,overcomes the need for extensive labeled data and the limitations of CNNs incapturing crucial shape relationships. The success stems from capturing globalshape semantics via ViTs and refining local discriminative features throughcontrastive optimization. Importantly, our approach is empirical, as it isgrounded on extensive experimental evaluation to validate the effectiveness ofcombining ViTs with contrastive objectives for 3D representation learning.</description>
      <author>example@mail.com (Márcus Vinícius Lobo Costa, Sherlon Almeida da Silva, Bárbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti)</author>
      <guid isPermaLink="false">2510.19955v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation</title>
      <link>http://arxiv.org/abs/2510.20596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2021&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于相似性原型的跨模态分割新框架，通过在嵌入空间中学习类别原型并引入相似性约束，以及使用字典存储不同图像中提取的原型，解决了深度学习模型在未见数据上性能下降的问题，实验证明该方法优于其他最先进方法。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型在各种视觉挑战中取得了巨大成功，但训练好的模型在应用于未见过的数据时性能会急剧下降，模型对域偏移敏感。&lt;h4&gt;目的&lt;/h4&gt;减少域差距，避免对未见域的昂贵标注，提高模型在跨模态分割任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于相似性原型的跨模态分割框架，在嵌入空间中学习类别的代表性原型，引入相似性约束使原型对每个语义类别具有代表性且不同类别间可分离，使用字典存储从不同图像中提取的原型防止类别缺失问题，实现原型的对比学习以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过原型学习和对比学习的方法可以有效解决域偏移问题，提高跨模态分割性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于相似性原型的跨模态分割框架比其他最先进方法取得了更好的结果。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型在各种视觉挑战中取得了巨大成功，但训练好的模型在应用于未见过的数据时性能会急剧下降。由于模型对域偏移敏感，无监督域适应尝试减少域差距并避免对未见域的昂贵标注。本文提出了一种基于相似性原型的跨模态分割新框架。具体来说，我们在嵌入空间中学习类别的原型，然后引入相似性约束，使这些原型对每个语义类别具有代表性，同时不同类别之间可分离。此外，我们使用字典存储从不同图像中提取的原型，这可以防止类别缺失问题，并实现原型的对比学习，进一步提高性能。大量实验表明，我们的方法比其他最先进方法取得了更好的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have achieved great success on various visionchallenges, but a well-trained model would face drastic performance degradationwhen applied to unseen data. Since the model is sensitive to domain shift,unsupervised domain adaptation attempts to reduce the domain gap and avoidcostly annotation of unseen domains. This paper proposes a novel framework forcross-modality segmentation via similarity-based prototypes. In specific, welearn class-wise prototypes within an embedding space, then introduce asimilarity constraint to make these prototypes representative for each semanticclass while separable from different classes. Moreover, we use dictionaries tostore prototypes extracted from different images, which prevents theclass-missing problem and enables the contrastive learning of prototypes, andfurther improves performance. Extensive experiments show that our methodachieves better results than other state-of-the-art methods.</description>
      <author>example@mail.com (Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang)</author>
      <guid isPermaLink="false">2510.20596v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment</title>
      <link>http://arxiv.org/abs/2510.20540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SheafAlign是一种基于层理论的多模态对齐框架，适用于分布式场景，不要求所有模态相互冗余，能有效保留共享和独特信息，并在多个方面表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;传统多模态对齐方法假设所有模态之间存在相互冗余，这一假设在现实世界的分布式场景中并不成立。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SheafAlign的框架，用于去中心化的多模态对齐，用多个比较空间替代单一空间对齐。&lt;h4&gt;方法&lt;/h4&gt;使用层理论框架，通过层结构建模成对模态关系，并利用基于去中心化对比学习的目标进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;Sheaf克服了先前方法的局限性，不需要所有模态之间存在相互冗余；在多模态传感数据集上表现出优越的零样本泛化能力、优秀的跨模态对齐能力、对缺失模态具有鲁棒性，与最先进的基线相比，通信成本降低50%。&lt;h4&gt;结论&lt;/h4&gt;SheafAlign是一种有效的去中心化多模态对齐方法，能够在分布式场景中更好地处理模态间关系。&lt;h4&gt;翻译&lt;/h4&gt;传统多模态对齐方法假设所有模态之间存在相互冗余，这一假设在现实世界的分布式场景中并不成立。我们提出了SheafAlign，一种基于层理论的去中心化多模态对齐框架，它用多个比较空间替代了单一空间对齐。这种方法通过层结构建模成对模态关系，并利用基于去中心化对比学习的目标进行训练。SheafAlign通过不要求所有模态之间存在相互冗余，克服了先前方法的局限性，同时保留了共享信息和独特信息。在多模态传感数据集上的实验显示，它在零样本泛化、跨模态对齐和对缺失模态的鲁棒性方面具有优越性，并且与最先进的基线相比，通信成本降低了50%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional multimodal alignment methods assume mutual redundancy across allmodalities, an assumption that fails in real-world distributed scenarios. Wepropose SheafAlign, a sheaf-theoretic framework for decentralized multimodalalignment that replaces single-space alignment with multiple comparison spaces.This approach models pairwise modality relations through sheaf structures andleverages decentralized contrastive learning-based objectives for training.SheafAlign overcomes the limitations of prior methods by not requiring mutualredundancy among all modalities, preserving both shared and unique information.Experiments on multimodal sensing datasets show superior zero-shotgeneralization, cross-modal alignment, and robustness to missing modalities,with 50\% lower communication cost than state-of-the-art baselines.</description>
      <author>example@mail.com (Abdulmomen Ghalkha, Zhuojun Tian, Chaouki Ben Issaid, Mehdi Bennis)</author>
      <guid isPermaLink="false">2510.20540v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ViTacGen: Robotic Pushing with Vision-to-Touch Generation</title>
      <link>http://arxiv.org/abs/2510.14117v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViTacGen是一种新颖的机器人操作框架，通过视觉到触觉生成在强化学习中消除对高分辨率真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署，在模拟和真实世界实验中展现出高达86%的成功率。&lt;h4&gt;背景&lt;/h4&gt;机器人推动是一种需要触觉反馈来捕捉末端执行器与物体间细微接触力和动力学的基本操作任务。真实触觉传感器面临高成本、脆弱性、校准和传感器差异等挑战，而仅依赖视觉的策略性能有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从视觉推断触觉状态的机器人操作框架，减少对昂贵且脆弱的真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署。&lt;h4&gt;方法&lt;/h4&gt;ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化触觉表示），以及一个使用对比学习融合视觉-触觉数据的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中验证了ViTacGen的有效性，其性能优于传统方法，成功率达到86%。&lt;h4&gt;结论&lt;/h4&gt;ViTacGen成功实现了从视觉到触觉的生成，使仅视觉的机器人系统能够在没有真实触觉传感器的情况下执行有效的机器人推动任务。&lt;h4&gt;翻译&lt;/h4&gt;机器人推动是一种基本操作任务，需要触觉反馈来捕捉末端执行器与物体之间的细微接触力和动力学特性。然而，真实触觉传感器通常面临高成本和脆弱性等硬件限制，以及涉及校准和不同传感器间差异的部署挑战，而仅视觉的策略难以获得令人满意的性能。受人类从视觉推断触觉状态能力的启发，我们提出了ViTacGen，一种专为视觉机器人推动设计的新颖机器人操作框架，在强化学习中使用视觉到触觉生成，消除对高分辨率真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署。具体来说，ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化的触觉表示），随后是一个基于视觉和生成触觉观察使用对比学习融合视觉-触觉数据的强化学习策略。我们在模拟和真实世界实验中都验证了我们方法的有效性，展示了其优越的性能，成功率高达86%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic pushing is a fundamental manipulation task that requires tactilefeedback to capture subtle contact forces and dynamics between the end-effectorand the object. However, real tactile sensors often face hardware limitationssuch as high costs and fragility, and deployment challenges involvingcalibration and variations between different sensors, while vision-onlypolicies struggle with satisfactory performance. Inspired by humans' ability toinfer tactile states from vision, we propose ViTacGen, a novel robotmanipulation framework designed for visual robotic pushing with vision-to-touchgeneration in reinforcement learning to eliminate the reliance onhigh-resolution real tactile sensors, enabling effective zero-shot deploymenton visual-only robotic systems. Specifically, ViTacGen consists of anencoder-decoder vision-to-touch generation network that generates contact depthimages, a standardized tactile representation, directly from visual imagesequence, followed by a reinforcement learning policy that fuses visual-tactiledata with contrastive learning based on visual and generated tactileobservations. We validate the effectiveness of our approach in both simulationand real world experiments, demonstrating its superior performance andachieving a success rate of up to 86\%.</description>
      <author>example@mail.com (Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo)</author>
      <guid isPermaLink="false">2510.14117v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Real Deep Research for AI, Robotics and Beyond</title>
      <link>http://arxiv.org/abs/2510.20809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  website: https://realdeepresearch.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Real Deep Research (RDR)的全面框架，用于系统分析AI和机器人研究领域，帮助研究人员识别新兴趋势、发现跨领域机会并为新研究提供起点。&lt;h4&gt;背景&lt;/h4&gt;AI和机器人研究快速增长，每年发表超过10,000篇论文，使得研究人员难以跟上最新发展。快速发展的趋势、跨学科工作的兴起以及需要探索专业领域之外的知识都构成了这一挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个通用流程，能够系统分析任何研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。&lt;h4&gt;方法&lt;/h4&gt;开发了Real Deep Research (RDR)全面框架，应用于AI和机器人领域，特别关注基础模型和机器人进展，同时简要扩展到其他科学领域。主论文详细介绍了RDR流程的构建，附录提供了各分析主题的广泛结果。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体的研究发现。&lt;h4&gt;结论&lt;/h4&gt;希望这项工作能为AI领域及更广泛领域的研究人员提供启示，帮助他们应对信息过载的挑战。&lt;h4&gt;翻译&lt;/h4&gt;随着AI和机器人研究的快速增长，现在每年产生超过10,000篇论文，研究人员越来越难以跟上最新发展。快速发展的趋势、跨学科工作的兴起以及探索专业领域之外知识的需求都构成了这一挑战。为解决这些问题，我们提出了一种能够系统分析任何研究领域的通用流程：识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。在本工作中，我们介绍了Real Deep Research (RDR)，这是一个应用于AI和机器人领域的全面框架，特别关注基础模型和机器人进展。我们还简要扩展了对其他科学领域的分析。主论文详细介绍了RDR流程的构建，附录提供了每个分析主题的广泛结果。我们希望这项工作能为AI领域及其他领域的研究人员提供启示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid growth of research in AI and robotics now producing over10,000 papers annually it has become increasingly difficult for researchers tostay up to date. Fast evolving trends, the rise of interdisciplinary work, andthe need to explore domains beyond one's expertise all contribute to thischallenge. To address these issues, we propose a generalizable pipeline capableof systematically analyzing any research area: identifying emerging trends,uncovering cross domain opportunities, and offering concrete starting pointsfor new inquiry. In this work, we present Real Deep Research (RDR) acomprehensive framework applied to the domains of AI and robotics, with aparticular focus on foundation models and robotics advancements. We alsobriefly extend our analysis to other areas of science. The main paper detailsthe construction of the RDR pipeline, while the appendix provides extensiveresults across each analyzed topic. We hope this work sheds light forresearchers working in the field of AI and beyond.</description>
      <author>example@mail.com (Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang)</author>
      <guid isPermaLink="false">2510.20809v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</title>
      <link>http://arxiv.org/abs/2510.20578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了EmbodiedBrain，一种新型的视觉语言基础模型，解决了当前具身AI模型在模型设计、实时性能评估和离线指标方面的局限性，实现了在所有评估指标上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;实现通用人工智能(AGI)需要能够在物理环境中进行稳健的空间感知、有效任务规划和自适应执行的具身AI智能体。然而，当前用于具身任务的大型语言模型和多模态大型语言模型存在关键局限性。&lt;h4&gt;目的&lt;/h4&gt;解决当前具身AI模型的局限性，提出一种新的视觉语言基础模型，提高具身智能体在物理环境中的感知、规划和执行能力。&lt;h4&gt;方法&lt;/h4&gt;开发了EmbodiedBrain模型(7B和32B参数规模)，采用与智能体对齐的数据结构，结合大规模监督微调(SFT)和步骤增强组相对策略优化(Step-GRPO)训练方法，引入包含生成奖励模型(GRM)的全面奖励系统，并建立三部分评估体系(通用、规划和端到端模拟基准测试)。&lt;h4&gt;主要发现&lt;/h4&gt;EmbodiedBrain在所有评估指标上实现了卓越性能，为具身基础模型建立了新的最先进水平，有效解决了模型设计与智能体需求之间的差距、实时延迟与性能的权衡问题，以及不真实的离线评估指标问题。&lt;h4&gt;结论&lt;/h4&gt;EmbodiedBrain为下一代通用具身智能体的发展铺平了道路，所有数据、模型权重和评估方法均已开源，可供研究社区使用。&lt;h4&gt;翻译&lt;/h4&gt;实现通用人工智能(AGI)需要能够在物理环境中进行稳健的空间感知、有效任务规划和自适应执行的具身AI智能体。然而，当前用于具身任务的大型语言模型(LLMs)和多模态大型语言模型(MLLMs)存在关键局限性，包括模型设计与智能体需求之间的显著差距、实时延迟与性能之间的不可避免权衡，以及使用不真实的离线评估指标。为解决这些挑战，我们提出了EmbodiedBrain，一种有7B和32B两种参数规模的新型视觉语言基础模型。我们的框架具有与智能体对齐的数据结构，采用结合大规模监督微调(SFT)和步骤增强组相对策略优化(Step-GRPO)的强大训练方法，通过将前序步骤整合为引导前体来提高长距离任务成功率。此外，我们引入了包含在基础设施层面加速的生成奖励模型(GRM)的全面奖励系统，以提高训练效率。为进行彻底验证，我们建立了包含通用、规划和端到端模拟基准测试的三部分评估体系，并提出了一个具有挑战性的新模拟环境并开源。实验结果表明，EmbodiedBrain在所有指标上都实现了卓越性能，为具身基础模型建立了新的最先进水平。为铺平下一代通用具身智能体的发展道路，我们开源了所有数据、模型权重和评估方法，可在https://zterobot.github.io/EmbodiedBrain.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The realization of Artificial General Intelligence (AGI) necessitatesEmbodied AI agents capable of robust spatial perception, effective taskplanning, and adaptive execution in physical environments. However, currentlarge language models (LLMs) and multimodal LLMs (MLLMs) for embodied taskssuffer from key limitations, including a significant gap between model designand agent requirements, an unavoidable trade-off between real-time latency andperformance, and the use of unauthentic, offline evaluation metrics. To addressthese challenges, we propose EmbodiedBrain, a novel vision-language foundationmodel available in both 7B and 32B parameter sizes. Our framework features anagent-aligned data structure and employs a powerful training methodology thatintegrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented GroupRelative Policy Optimization (Step-GRPO), which boosts long-horizon tasksuccess by integrating preceding steps as Guided Precursors. Furthermore, weincorporate a comprehensive reward system, including a Generative Reward Model(GRM) accelerated at the infrastructure level, to improve training efficiency.For enable thorough validation, we establish a three-part evaluation systemencompassing General, Planning, and End-to-End Simulation Benchmarks,highlighted by the proposal and open-sourcing of a novel, challengingsimulation environment. Experimental results demonstrate that EmbodiedBrainachieves superior performance across all metrics, establishing a newstate-of-the-art for embodied foundation models. Towards paving the way for thenext generation of generalist embodied agents, we open-source all of our data,model weight, and evaluating methods, which are available athttps://zterobot.github.io/EmbodiedBrain.github.io.</description>
      <author>example@mail.com (Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu)</author>
      <guid isPermaLink="false">2510.20578v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Framework for Zero-Shot Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.20542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了零样本强化学习的第一个统一框架，引入了一致的符号和分类法，将现有方法组织为直接表示和组合表示两大类，为该领域提供了原则性基础和研究方向。&lt;h4&gt;背景&lt;/h4&gt;零样本强化学习允许代理在不监督的情况下开发通用能力，无需额外训练或规划即可解决下游任务。与传统RL优化固定奖励不同，零样本RL需要代理编码足够丰富的表示以立即适应任何目标，类似于视觉和语言基础模型。尽管该领域兴趣增长，但缺乏共同的分析视角。&lt;h4&gt;目的&lt;/h4&gt;提出零样本强化学习的第一个统一框架，引入一致的符号和分类法，组织现有方法并允许直接比较不同方法，为该领域提供原则性基础。&lt;h4&gt;方法&lt;/h4&gt;框架将算法分为两类：直接表示（学习从奖励到策略的端到端映射）和组合表示（利用值函数的子结构分解表示）。在此框架内，突出方法的共同原则和关键差异，为后续特征方法推导扩展界限，提供其在零样本环境中的新视角。&lt;h4&gt;主要发现&lt;/h4&gt;通过共同视角整合了现有工作，揭示了不同方法间的共享原则和关键差异，为后续特征方法提供了新的理论视角，表明组合表示可能更适合零样本场景。&lt;h4&gt;结论&lt;/h4&gt;该框架为零样本强化学习的未来研究提供了原则性基础，并指明了开发更通用代理的明确路径，有助于推动通用人工智能代理的发展。&lt;h4&gt;翻译&lt;/h4&gt;零样本强化学习(RL)已成为一种在不监督情况下开发通用代理的设置，能够在测试时无需额外训练或规划的情况下解决下游任务。与传统优化固定奖励的RL不同，零样本RL需要代理编码足够丰富的表示以支持立即适应任何目标，这与视觉和语言基础模型相类似。尽管兴趣日益增长，该领域仍缺乏共同的分析视角。我们提出了零样本RL的第一个统一框架，我们的引入了一致的符号和分类法，组织了现有方法并允许直接比较它们。我们框架的核心是将算法分为两个家族：直接表示，学习从奖励到策略的端到端映射；以及组合表示，利用值函数的子结构分解表示。在此框架内，我们突出了跨方法的共同原则和关键差异，并为后续特征方法推导了扩展界限，提供了它们在零样本环境中性能的新视角。通过在共同视角下整合现有工作，我们的框架为未来零样本RL研究提供了原则性基础，并概述了开发更通用代理的明确路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot reinforcement learning (RL) has emerged as a setting for developinggeneral agents in an unsupervised manner, capable of solving downstream taskswithout additional training or planning at test-time. Unlike conventional RL,which optimizes policies for a fixed reward, zero-shot RL requires agents toencode representations rich enough to support immediate adaptation to anyobjective, drawing parallels to vision and language foundation models. Despitegrowing interest, the field lacks a common analytical lens.  We present the first unified framework for zero-shot RL. Our formulationintroduces a consistent notation and taxonomy that organizes existingapproaches and allows direct comparison between them. Central to our frameworkis the classification of algorithms into two families: direct representations,which learn end-to-end mappings from rewards to policies, and compositionalrepresentations, which decompose the representation leveraging the substructureof the value function. Within this framework, we highlight shared principlesand key differences across methods, and we derive an extended bound forsuccessor-feature methods, offering a new perspective on their performance inthe zero-shot regime. By consolidating existing work under a common lens, ourframework provides a principled foundation for future research in zero-shot RLand outlines a clear path toward developing more general agents.</description>
      <author>example@mail.com (Jacopo Di Ventura, Jan Felix Kleuker, Aske Plaat, Thomas Moerland)</author>
      <guid isPermaLink="false">2510.20542v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking</title>
      <link>http://arxiv.org/abs/2510.20335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code is at  https://github.com/ChampagneAndfragrance/Dino_Diffusion_Parking_Official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Dino-Diffusion Parking (DDP)，一种结合视觉基础模型与基于扩散规划的自动停车流水线，解决了在不同环境条件下停车的鲁棒性问题。&lt;h4&gt;背景&lt;/h4&gt;停车是驾驶安全的关键支柱，尽管端到端方法在领域内取得了良好结果，但在天气和光照变化等条件下的鲁棒性仍是主要挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种领域无关的自动停车流水线，实现分布变化下的通用感知和鲁棒运动规划。&lt;h4&gt;方法&lt;/h4&gt;提出Dino-Diffusion Parking (DDP)，将视觉基础模型与基于扩散的规划相结合，在CARLA常规设置中训练，然后以零样本方式转移到更具挑战性的环境。&lt;h4&gt;主要发现&lt;/h4&gt;模型在所有测试的分布外场景中停车成功率 consistently保持在90%以上，消融研究证实网络架构和算法设计显著提高了跨域性能，在3D高斯飞溅环境中的测试显示了有希望的模拟到现实迁移能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的DDP方法在不同环境条件下都能实现高成功率的自动停车，并且具有从模拟到现实的迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;停车是驾驶安全的关键支柱。尽管最近的端到端方法在领域内取得了有希望的结果，但在领域变化（如天气和光照变化）下的鲁棒性仍然是一个关键挑战。我们提出Dino-Diffusion Parking (DDP)，一个领域无关的自动停车流水线，它将视觉基础模型与基于扩散的规划相结合，以实现分布变化下的通用感知和鲁棒运动规划。我们在CARLA的常规设置中训练我们的流水线，并以零样本方式将其转移到更具挑战性的设置中。我们的模型在所有测试的分布外场景中停车成功率始终保持在90%以上，消融研究证实，网络架构和算法设计都显著提高了跨域性能，优于现有基线。此外，在从真实停车场重建的3D高斯飞溅环境中的测试显示了有希望的模拟到现实迁移。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶停车系统在不同环境条件下的跨域适应问题。当系统从训练环境（如晴天）转移到部署环境（如雨天、雾天或不同光照条件）时，性能会显著下降。这个问题很重要，因为停车占美国车辆事故的20%，且91%与倒车操作相关，准确的感知、规划和控制对安全至关重要。此外，传统解决方案需要大量收集不同条件下的数据，成本高昂，而本文方法无需额外数据就能适应各种环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将自动驾驶停车问题分解为感知、规划和控制三个模块，而非使用单一端到端模型。他们借鉴了多个现有工作：使用DINOv2视觉基础模型实现鲁棒感知，参考'Lift, Splat, Shoot'和'BEVFormer'进行BEV转换，借鉴机器人领域的扩散模型进行运动规划，并采用经典的Stanley控制器进行轨迹跟踪。作者特别设计了'后视目标重标记'的数据增强策略，通过人工扰动目标位置增强数据多样性，提高目标识别的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用模块化设计解耦感知和规划，利用视觉基础模型实现跨环境鲁棒感知，通过扩散模型减少运动规划中的累积误差，并结合经典控制器实现精确跟踪。整体流程：1)使用DINOv2处理摄像头图像生成鲁棒特征；2)将特征转换为鸟瞰图(BEV)表示；3)通过交叉注意力和FiLM结构将目标位置信息融合到BEV特征；4)扩散模型基于融合特征和目标位置预测轨迹；5)Stanley控制器根据预测轨迹生成控制命令执行停车。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次研究跨域自动驾驶停车问题，实现零样本迁移；2)模块化设计解耦感知和规划，避免过拟合；3)目标重标记数据增强技术提高目标识别鲁棒性；4)在SE(2)空间进行扩散运动规划，包含位置和方向信息；5)在3D高斯飞溅环境中验证模拟到真实世界的迁移能力。相比之前工作，本文方法在跨域场景中表现更好，不需要额外收集不同条件的数据，结合了模块化设计和扩散模型优势，并针对停车任务进行了专门优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于视觉基础模型和扩散模型的模块化自动驾驶停车框架，实现了在无需额外数据的情况下跨环境零样本迁移的能力，显著提升了自动驾驶系统在不同天气、光照条件下的停车鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parking is a critical pillar of driving safety. While recent end-to-end (E2E)approaches have achieved promising in-domain results, robustness under domainshifts (e.g., weather and lighting changes) remains a key challenge. Ratherthan relying on additional data, in this paper, we propose Dino-DiffusionParking (DDP), a domain-agnostic autonomous parking pipeline that integratesvisual foundation models with diffusion-based planning to enable generalizedperception and robust motion planning under distribution shifts. We train ourpipeline in CARLA at regular setting and transfer it to more adversarialsettings in a zero-shot fashion. Our model consistently achieves a parkingsuccess rate above 90% across all tested out-of-distribution (OOD) scenarios,with ablation studies confirming that both the network architecture andalgorithmic design significantly enhance cross-domain performance over existingbaselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environmentreconstructed from a real-world parking lot demonstrates promising sim-to-realtransfer.</description>
      <author>example@mail.com (Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz, Xinyu Huang, Liu Ren)</author>
      <guid isPermaLink="false">2510.20335v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Breakdance Video classification in the age of Generative AI</title>
      <link>http://arxiv.org/abs/2510.20287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了现代视频基础模型在霹雳舞这一小众但流行的舞蹈体育中的应用性，发现视频编码器模型在预测任务上表现优于最先进的视频语言模型。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型已在多个体育用例中得到广泛应用，但大多数研究仅针对足球、板球、篮球等流行体育项目，专注于视觉问答和精彩片段生成等生成任务。&lt;h4&gt;目的&lt;/h4&gt;分析现代视频基础模型（包括编码器和解码器）在霹雳舞这一非常小众但极受欢迎的舞蹈体育中的应用性。&lt;h4&gt;方法&lt;/h4&gt;评估视频编码器模型和视频语言模型在霹雳舞视频分类任务上的表现，并提供编码器模型选择和微调解码器模型分析的见解。&lt;h4&gt;主要发现&lt;/h4&gt;视频编码器模型在预测任务上继续优于最先进的视频语言模型，研究提供了如何选择编码器模型的见解，并对微调后的解码器模型在霹雳舞视频分类中的工作机制进行了详细分析。&lt;h4&gt;结论&lt;/h4&gt;视频编码器模型在特定体育应用（如霹雳舞）中可能比视频语言模型更有效。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型最近在多个体育用例中得到了广泛应用。这些工作大多针对足球、板球、篮球等流行体育项目的一个有限子集，专注于视觉问答、精彩片段生成等生成任务。这项工作分析了现代视频基础模型（包括编码器和解码器）在霹雳舞这种非常小众但极受欢迎的舞蹈体育中的应用性。我们的结果表明，视频编码器模型在预测任务上继续优于最先进的视频语言模型。我们提供了如何选择编码器模型的见解，并对微调后的解码器模型在霹雳舞视频分类中的工作机制进行了详细分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision Language models have seen huge application in several sportsuse-cases recently. Most of these works have been targeted towards a limitedsubset of popular sports like soccer, cricket, basketball etc; focusing ongenerative tasks like visual question answering, highlight generation. Thiswork analyzes the applicability of the modern video foundation models (bothencoder and decoder) for a very niche but hugely popular dance sports -breakdance. Our results show that Video Encoder models continue to outperformstate-of-the-art Video Language Models for prediction tasks. We provideinsights on how to choose the encoder model and provide a thorough analysisinto the workings of a finetuned decoder model for breakdance videoclassification.</description>
      <author>example@mail.com (Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson)</author>
      <guid isPermaLink="false">2510.20287v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Optimistic Task Inference for Behavior Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpTI-BFM是一种改进的行为基础模型方法，通过直接对奖励函数不确定性建模和乐观决策，实现了在测试时仅通过环境交互来高效推断和优化奖励函数，显著减少了数据需求。&lt;h4&gt;背景&lt;/h4&gt;行为基础模型(BFMs)能够在测试时直接检索针对任何指定奖励函数的高性能策略，实现零样本强化学习。尽管这种方法在计算上高效，但在数据方面效率较低，因为它通常需要在非平凡的推断数据集上计算奖励，假设可以访问奖励的功能形式或需要大量标注工作。&lt;h4&gt;目的&lt;/h4&gt;解决BFMs在数据效率方面的问题，使模型能够通过在测试时仅与环境交互来进行任务推断，避免对奖励函数功能形式的依赖或大量标注工作。&lt;h4&gt;方法&lt;/h4&gt;提出OpTI-BFM，一种乐观决策标准，直接对奖励函数的不确定性进行建模，并指导BFMs进行任务推断的数据收集。通过与线性bandit的上置信度算法的直接连接，为训练良好的BFMs提供了遗憾界限。&lt;h4&gt;主要发现&lt;/h4&gt;在既定的零样本基准上评估OpTI-BFM后，观察到它使基于后继特征的BFMs能够在少量回合中识别和优化未见过的奖励函数，且计算开销最小。&lt;h4&gt;结论&lt;/h4&gt;OpTI-BFM解决了传统BFMs在数据效率方面的限制，使其能够在测试时仅通过环境交互来推断和优化任务，显著减少了数据需求。&lt;h4&gt;翻译&lt;/h4&gt;行为基础模型(BFMs)能够检索针对任何在测试时直接指定的奖励函数的高性能策略，通常被称为零样本强化学习(RL)。虽然这在计算方面是一个非常高效的过程，但在数据方面可能效率较低：作为标准假设，BFMs需要在非平凡的推断数据集上计算奖励，假设可以访问奖励的功能形式或需要大量的标注工作。为了减轻这些限制，我们解决了在测试时仅通过与环境交互来进行任务推断的问题。我们提出了OpTI-BFM，一种乐观决策标准，直接对奖励函数的不确定性进行建模，并指导BFMs进行任务推断的数据收集。形式上，我们通过与线性bandit的上置信度算法的直接连接，为训练良好的BFMs提供了遗憾界限。经验上，我们在既定的零样本基准上评估了OpTI-BFM，并观察到它使基于后继特征的BFMs能够在少量回合中识别和优化未见过的奖励函数，且计算开销最小。代码可在https://github.com/ThomasRupf/opti-bfm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavior Foundation Models (BFMs) are capable of retrieving high-performingpolicy for any reward function specified directly at test-time, commonlyreferred to as zero-shot reinforcement learning (RL). While this is a veryefficient process in terms of compute, it can be less so in terms of data: as astandard assumption, BFMs require computing rewards over a non-negligibleinference dataset, assuming either access to a functional form of rewards, orsignificant labeling efforts. To alleviate these limitations, we tackle theproblem of task inference purely through interaction with the environment attest-time. We propose OpTI-BFM, an optimistic decision criterion that directlymodels uncertainty over reward functions and guides BFMs in data collection fortask inference. Formally, we provide a regret bound for well-trained BFMsthrough a direct connection to upper-confidence algorithms for linear bandits.Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, andobserve that it enables successor-features-based BFMs to identify and optimizean unseen reward function in a handful of episodes with minimal computeoverhead. Code is available at https://github.com/ThomasRupf/opti-bfm.</description>
      <author>example@mail.com (Thomas Rupf, Marco Bagatella, Marin Vlastelica, Andreas Krause)</author>
      <guid isPermaLink="false">2510.20264v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance</title>
      <link>http://arxiv.org/abs/2510.20119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)与轻量级监督基线模型和经典模型性能相当，差距源于简单导入NLP或CV流程。时间序列数据不像图像和文本那样直接捕捉人类概念，因此'在线抓取一切'的范式不适用。进步需要从机会性聚合转向原则性设计，构建系统跨越保持时间语义不变性空间的数据集，并基于第一原理构建时间序列不变性本体论，以确保表示完整性，使TSFMs实现泛化、推理和真正涌现行为所需的对齐结构。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)数量不断增加，但轻量级监督基线模型甚至经典模型通常与它们表现相当。这种差距源于简单导入NLP或CV的流程。&lt;h4&gt;目的&lt;/h4&gt;提出需要从机会性聚合转向原则性设计：构建数据集，系统性地跨越保持时间语义的不变性空间。&lt;h4&gt;方法&lt;/h4&gt;建议基于第一原理构建时间序列不变性的本体论，通过不变性覆盖确保表示的完整性。&lt;h4&gt;主要发现&lt;/h4&gt;在语言和视觉领域，大规模网络语料库密集捕捉人类概念，但时间序列数据没有直接对应的概念，因此'在线抓取一切'的范式对时间序列不适用。&lt;h4&gt;结论&lt;/h4&gt;只有通过不变性覆盖确保表示的完整性，TSFMs才能实现泛化、推理和真正涌现行为所需的对齐结构。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)不断增加，然而轻量级监督基线和甚至经典模型常常与它们匹敌。我们认为这种差距源于简单导入NLP或CV流程。在语言和视觉中，大规模网络语料库密集捕捉人类概念，即有无数的苹果图像和文本。相比之下，时间序列数据设计用来补充图像和文本模态。没有包含'苹果'概念的时间序列数据集。因此，'在线抓取一切'的范式对时间序列不适用。我们认为进步需要从机会性转向原则性设计：构建数据集，系统性地跨越保持时间语义的不变性空间。为此，我们建议时间序列不变性的本体论应基于第一原理构建。只有通过不变性覆盖确保表示的完整性，TSFMs才能实现泛化、推理和真正涌现行为所需的对齐结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timeseries foundation models (TSFMs) have multiplied, yet lightweightsupervised baselines and even classical models often match them. We argue thisgap stems from the naive importation of NLP or CV pipelines. In language andvision, large web-scale corpora densely capture human concepts i.e. there arecountless images and text of apples. In contrast, timeseries data is built tocomplement the image and text modalities. There are no timeseries dataset thatcontains the concept apple. As a result, the scrape-everything-online paradigmfails for TS. We posit that progress demands a shift from opportunisticaggregation to principled design: constructing datasets that systematicallyspan the space of invariance that preserve temporal semantics. To this end, wesuggest that the ontology of timeseries invariances should be built based onfirst principles. Only by ensuring representational completeness throughinvariance coverage can TSFMs achieve the aligned structure necessary forgeneralisation, reasoning, and truly emergent behaviour.</description>
      <author>example@mail.com (Arian Prabowo, Flora D. Salim)</author>
      <guid isPermaLink="false">2510.20119v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://imageomics.github.io/biocap/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了描述性字幕作为生物多模态基础模型的额外监督来源，通过使用多模态大语言模型生成合成字幕，训练出BIOCAP模型，在物种分类和文本图像检索方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;图像和字幕可以被视为物种潜在形态空间中的互补样本，各自捕捉特定的生物学特征。在训练中加入字幕可以促进与共享潜在结构的对齐，强调可能有诊断价值的特征，同时抑制虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;解决生物有机体生物学中大规模获取忠实、实例特定字幕的挑战，以充分利用自然语言监督在生物多模态基础模型中的应用。&lt;h4&gt;方法&lt;/h4&gt;使用多模态大语言模型(MLLMs)生成合成字幕，这些字幕由维基百科衍生的视觉信息和特定于分类群的格式示例指导，以减少幻觉并产生准确的描述性字幕。然后使用这些字幕训练BIOCAP(即带有字幕的BIOCLIP)模型。&lt;h4&gt;主要发现&lt;/h4&gt;BIOCAP模型能够捕捉丰富的语义，并在物种分类和文本图像检索方面取得强大的性能。&lt;h4&gt;结论&lt;/h4&gt;描述性字幕在连接生物图像与多模态基础模型方面具有超越标签的价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了描述性字幕作为生物多模态基础模型的额外监督来源。图像和字幕可以被视为物种潜在形态空间中的互补样本，每种都捕捉了特定的生物学特征。在训练中加入字幕可以鼓励与这种共享潜在结构的对齐，强调可能有诊断价值的特征，同时抑制虚假相关性。然而，主要挑战在于大规模获取忠实、实例特定的字幕。这一要求限制了自然语言监督在生物有机体生物学中的应用，与其他许多科学领域相比。我们通过使用多模态大语言模型(MLLMs)生成合成字幕来弥补这一差距，这些字幕由维基百科衍生的视觉信息和特定于分类群的格式示例指导。这些特定领域的上下文有助于减少幻觉，并产生准确、基于实例的描述性字幕。使用这些字幕，我们训练了BIOCAP(即带有字幕的BIOCLIP)，这是一个能够捕捉丰富语义并在物种分类和文本图像检索方面取得强大性能的生物基础模型。这些结果证明了描述性字幕在连接生物图像与多模态基础模型方面超越标签的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates descriptive captions as an additional source ofsupervision for biological multimodal foundation models. Images and captionscan be viewed as complementary samples from the latent morphospace of aspecies, each capturing certain biological traits. Incorporating captionsduring training encourages alignment with this shared latent structure,emphasizing potentially diagnostic characters while suppressing spuriouscorrelations. The main challenge, however, lies in obtaining faithful,instance-specific captions at scale. This requirement has limited theutilization of natural language supervision in organismal biology compared withmany other scientific domains. We complement this gap by generating syntheticcaptions with multimodal large language models (MLLMs), guided byWikipedia-derived visual information and taxon-tailored format examples. Thesedomain-specific contexts help reduce hallucination and yield accurate,instance-based descriptive captions. Using these captions, we train BIOCAP(i.e., BIOCLIP with Captions), a biological foundation model that captures richsemantics and achieves strong performance in species classification andtext-image retrieval. These results demonstrate the value of descriptivecaptions beyond labels in bridging biological images with multimodal foundationmodels.</description>
      <author>example@mail.com (Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu)</author>
      <guid isPermaLink="false">2510.20095v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</title>
      <link>http://arxiv.org/abs/2510.19949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 9 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Surfer 2是一个统一的架构，仅从视觉观察操作，在Web、桌面和移动三种环境中实现了最先进的性能，无需任务特定微调即可超越人类表现。&lt;h4&gt;背景&lt;/h4&gt;构建能够在网络、桌面和移动环境中通用的代理仍然是一个开放的挑战，因为之前的系统依赖于特定环境的接口，限制了跨平台部署。&lt;h4&gt;目的&lt;/h4&gt;介绍Surfer 2，一个统一的架构，仅从视觉观察操作，在所有三种环境中实现最先进的性能。&lt;h4&gt;方法&lt;/h4&gt;Surfer 2集成了分层上下文管理、解耦的规划和执行，以及自适应恢复的自验证， enabling可靠操作在长任务范围内。&lt;h4&gt;主要发现&lt;/h4&gt;在WebVoyager上达到97.1%的准确率，在WebArena上达到69.6%的准确率，在OSWorld上达到60.1%的准确率，在AndroidWorld上达到87.1%的准确率，超越了所有之前的系统，无需任务特定的微调，多次尝试后，Surfer 2在所有基准测试中超过了人类性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，系统编排增强了基础模型的能力，仅通过视觉交互实现了通用计算机控制，同时呼吁新一代视觉语言模型以实现帕累托最优的成本效益。&lt;h4&gt;翻译&lt;/h4&gt;构建能够在网络、桌面和移动环境中通用的代理仍然是一个开放的挑战，因为之前的系统依赖于特定环境的接口，这限制了跨平台部署。我们介绍了Surfer 2，一个统一的架构，仅从视觉观察操作，在所有三种环境中实现最先进的性能。Surfer 2集成了分层上下文管理、解耦的规划和执行，以及自适应恢复的自验证， enabling可靠操作在长任务范围内。我们的系统在WebVoyager上达到97.1%的准确率，在WebArena上达到69.6%，在OSWorld上达到60.1%，在AndroidWorld上达到87.1%，超越了所有之前的系统，无需任务特定的微调。多次尝试后，Surfer 2在所有基准测试中超过了人类性能。这些结果表明，系统编排增强了基础模型的能力，仅通过视觉交互实现了通用计算机控制，同时呼吁新一代视觉语言模型以实现帕累托最优的成本效益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building agents that generalize across web, desktop, and mobile environmentsremains an open challenge, as prior systems rely on environment-specificinterfaces that limit cross-platform deployment. We introduce Surfer 2, aunified architecture operating purely from visual observations that achievesstate-of-the-art performance across all three environments. Surfer 2 integrateshierarchical context management, decoupled planning and execution, andself-verification with adaptive recovery, enabling reliable operation over longtask horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% onWebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all priorsystems without task-specific fine-tuning. With multiple attempts, Surfer 2exceeds human performance on all benchmarks. These results demonstrate thatsystematic orchestration amplifies foundation model capabilities and enablesgeneral-purpose computer control through visual interaction alone, whilecalling for a next-generation vision language model to achieve Pareto-optimalcost-efficiency.</description>
      <author>example@mail.com (Mathieu Andreux, Märt Bakler, Yanael Barbier, Hamza Ben Chekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Nathan Bout, Matthias Brunel, Aleix Cambray, Pierre-Louis Cedoz, Antoine Chassang, Gautier Cloix, Ethan Connelly, Alexandra Constantinou, Ramzi De Coster, Hubert de la Jonquiere, Aurélien Delfosse, Maxime Delpit, Alexis Deprez, Augustin Derupti, Mathieu Diaz, Shannon D'Souza, Julie Dujardin, Abai Edmund, Michael Eickenberg, Armand Fatalot, Wissem Felissi, Isaac Herring, Xavier Koegler, Erwan Le Jumeau de Kergaradec, Aurélien Lac, Maxime Langevin, Corentin Lauverjat, Antonio Loison, Avshalom Manevich, Axel Moyal, Axel Nguyen Kerbel, Marinela Parovic, Julien Revelle, Guillaume Richard, Mats Richter, Ronan Riochet, María Santos, Romain Savidan, Laurent Sifre, Maxime Theillard, Marc Thibault, Ivan Valentini, Tony Wu, Laura Yie, Kai Yuan, Jevgenij Zubovskij)</author>
      <guid isPermaLink="false">2510.19949v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets</title>
      <link>http://arxiv.org/abs/2510.19944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Seed3D 1.0 Technical Report; Official Page on  https://seed.bytedance.com/seed3d&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Seed3D 1.0是一个基础模型，可以从单张图像生成可用于仿真的3D资产，解决了具身AI代理训练环境中的可扩展性问题，同时保持物理准确性。&lt;h4&gt;背景&lt;/h4&gt;开发具身AI代理需要平衡内容多样性和物理准确性的可扩展训练环境。现有世界模拟器存在局限：基于视频的方法内容多样但缺乏实时物理反馈，基于物理的引擎物理准确但受限于昂贵的手动资产创建。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单张图像生成仿真就绪3D资产的基础模型，解决可扩展性挑战，同时保持物理严谨性。&lt;h4&gt;方法&lt;/h4&gt;提出Seed3D 1.0基础模型，生成具有准确几何、良好对齐纹理和真实物理材质的3D资产，可直接集成到物理引擎中，支持机器人操作和仿真训练，并能扩展到完整场景生成。&lt;h4&gt;主要发现&lt;/h4&gt;Seed3D 1.0生成的3D资产具有准确的几何结构、对齐良好的纹理和真实的物理材质，可直接用于物理引擎，支持机器人操作和仿真训练，并能扩展到完整场景生成。&lt;h4&gt;结论&lt;/h4&gt;Seed3D 1.0通过实现可扩展的仿真就绪内容创建，为推进基于物理的世界模拟器提供了基础，现已可在指定网址获取。&lt;h4&gt;翻译&lt;/h4&gt;开发具身AI代理需要可扩展的训练环境，这些环境需要在内容多样性和物理准确性之间取得平衡。世界模拟器提供了这样的环境，但面临不同的限制：基于视频的方法可以生成多样化的内容，但缺乏实时物理反馈以支持交互式学习；而基于物理的引擎能提供准确的动力学，但由于昂贵的手动资产创建而面临可扩展性限制。我们提出了Seed3D 1.0，这是一个基础模型，可以从单张图像生成仿真就绪的3D资产，解决了可扩展性挑战，同时保持物理严谨性。与现有的3D生成模型不同，我们的系统生成具有准确几何、对齐良好的纹理和真实物理材质的资产。这些资产可以直接集成到物理引擎中，只需最少的配置，支持在机器人操作和仿真训练中部署。除了单个对象外，系统还能通过将对象组装成连贯的环境来扩展到完整场景生成。通过实现可扩展的仿真就绪内容创建，Seed3D 1.0为推进基于物理的世界模拟器提供了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从单张图像生成高质量的、可直接用于物理仿真的3D资产的问题。这个问题重要是因为开发具身AI代理需要可扩展的训练环境，平衡内容多样性和物理准确性，而现有世界模拟器面临根本性权衡：基于视频的方法缺乏实时物理反馈，基于物理的引擎则受限于手动资产创建的可扩展性，制约了训练环境的多样性和规模。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有世界模拟器的局限性，认识到需要结合生成多样性和物理严谨性，设计了Seed3D 1.0基础模型。系统借鉴了现有工作：几何生成部分采用VAE和基于修正流的扩散Transformer架构；使用DINOv2和RADIO作为图像编码器；纹理生成部分借鉴多模态扩散Transformer；数据预处理参考3DShape2VecSet设计；训练基础设施采用FlashAttention和混合分片数据并行等技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合生成多样性和物理严谨性，解决3D资产创建的可扩展性问题，生成可直接用于物理仿真的高质量3D资产。整体流程包括：1)几何生成：使用Seed3D-VAE学习紧凑潜在表示，Seed3D-DiT合成3D形状；2)纹理生成：Seed3D-MV生成多视图图像，Seed3D-PBR分解为PBR材质图，Seed3D-UV补全UV纹理；3)数据处理：自动化预处理管道、格式标准化和质量过滤；4)训练和推理：采用渐进式策略训练模型，通过多阶段处理生成最终3D资产。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)高质量资产生成：生成具有精确几何、高分辨率纹理和真实物理材质的3D资产；2)物理引擎兼容性：资产可直接集成到物理引擎中；3)可扩展场景合成：从室内到城市环境实现连贯场景；4)技术创新：开发了Seed3D-VAE、Seed3D-DiT、Seed3D-MV、Seed3D-PBR和Seed3D-UV五个核心组件。相比之前工作，解决了几何伪影和纹理错位问题，通过UV纹理补全解决自遮挡，采用混合架构平衡跨模态学习和模态特定处理，使用长度感知时间步长维持生成质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Seed3D 1.0通过从单张图像生成高质量的、物理兼容的3D资产，解决了具身AI训练环境中内容多样性和物理准确性之间的权衡问题，为物理驱动的世界模拟器提供了可扩展的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing embodied AI agents requires scalable training environments thatbalance content diversity with physics accuracy. World simulators provide suchenvironments but face distinct limitations: video-based methods generatediverse content but lack real-time physics feedback for interactive learning,while physics-based engines provide accurate dynamics but face scalabilitylimitations from costly manual asset creation. We present Seed3D 1.0, afoundation model that generates simulation-ready 3D assets from single images,addressing the scalability challenge while maintaining physics rigor. Unlikeexisting 3D generation models, our system produces assets with accurategeometry, well-aligned textures, and realistic physically-based materials.These assets can be directly integrated into physics engines with minimalconfiguration, enabling deployment in robotic manipulation and simulationtraining. Beyond individual objects, the system scales to complete scenegeneration through assembling objects into coherent environments. By enablingscalable simulation-ready content creation, Seed3D 1.0 provides a foundationfor advancing physics-based world simulators. Seed3D 1.0 is now available onhttps://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&amp;tab=Gen3D</description>
      <author>example@mail.com (Jiashi Feng, Xiu Li, Jing Lin, Jiahang Liu, Gaohong Liu, Weiqiang Lou, Su Ma, Guang Shi, Qinlong Wang, Jun Wang, Zhongcong Xu, Xuanyu Yi, Zihao Yu, Jianfeng Zhang, Yifan Zhu, Rui Chen, Jinxin Chi, Zixian Du, Li Han, Lixin Huang, Kaihua Jiang, Yuhan Li, Guan Luo, Shuguang Wang, Qianyi Wu, Fan Yang, Junyang Zhang, Xuanmeng Zhang)</author>
      <guid isPermaLink="false">2510.19944v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning</title>
      <link>http://arxiv.org/abs/2510.19893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为FairGRPO的分层强化学习方法，用于提高医学人工智能系统在不同人口统计群体中的诊断公平性，通过自适应重要性加权和无监督聚类处理缺失标签问题，实验表明该方法显著提高了预测平等性和F1分数。&lt;h4&gt;背景&lt;/h4&gt;医学人工智能系统在诊断方面取得显著成就，但在不同人口统计群体中表现出明显的性能差异，对代表性不足人群造成实际伤害。多模态推理基础模型推动了临床诊断，但通过强化学习进行的推理训练继承了并放大了训练数据集中的偏见。&lt;h4&gt;目的&lt;/h4&gt;提出一种促进跨异质临床人群公平学习的方法，解决临床领域常见的缺乏人口统计标签的问题。&lt;h4&gt;方法&lt;/h4&gt;引入Fairness-aware Group Relative Policy Optimization (FairGRPO)，一种分层强化学习方法，采用基于代表性、任务难度和数据源的自适应优势重要性加权。采用无监督聚类来处理缺失的人口统计标签，当标签不可用时自动发现潜在的人口统计群体。&lt;h4&gt;主要发现&lt;/h4&gt;在7个涵盖5种临床模态的临床诊断数据集上，FairGRPO与所有普通和偏见缓解的强化学习基线相比，将预测平等性提高了27.2%，同时F1分数提高了12.49%。训练动态分析显示，FairGRPO在整个优化过程中逐步改善公平性，而基线强化学习方法在训练过程中表现出公平性恶化。基于FairGRPO，发布了FairMedGemma-4B，一个公平感知的临床VLLM，在实现最先进性能的同时显著减少了不同人口统计群体之间的差异。&lt;h4&gt;结论&lt;/h4&gt;FairGRPO是一种有效的医学人工智能系统公平性提升方法，能够在不牺牲性能的情况下提高跨人群的诊断公平性，解决了临床数据中缺乏人口统计标签的常见问题。&lt;h4&gt;翻译&lt;/h4&gt;医学人工智能系统已取得显著的诊断能力，然而它们在不同人口统计群体中持续表现出性能差异，对代表性不足的人群造成实际伤害。虽然最近的多模态推理基础模型通过整合分析多样化的医疗数据推动了临床诊断的发展，但通过强化学习进行的推理训练继承了主导多数人群的训练数据集中存在的偏见，并往往放大这些偏见。我们引入了公平感知的群体相对策略优化（FairGRPO），这是一种分层强化学习方法，促进跨异质临床人群的公平学习。FairGRPO基于代表性、任务难度和数据源采用自适应的优势重要性加权。为解决临床领域中常见的人口统计标签缺失问题，我们进一步采用无监督聚类，当标签不可用时自动发现潜在的人口统计群体。在跨越X光、CT扫描、皮肤镜检查、乳腺X光检查和超声波5种临床模态的7个临床诊断数据集上进行综合实验，我们证明FairGRPO与所有普通和偏见缓解的强化学习基线相比，将预测平等性提高了27.2%，同时F1分数提高了12.49%。此外，训练动态分析显示，FairGRPO在整个优化过程中逐步改善公平性，而基线强化学习方法在训练过程中表现出公平性恶化。基于FairGRPO，我们发布了FairMedGemma-4B，一个公平感知的临床VLLM，在实现最先进性能的同时，显著减少了不同人口统计群体之间的差异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical artificial intelligence systems have achieved remarkable diagnosticcapabilities, yet they consistently exhibit performance disparities acrossdemographic groups, causing real-world harm to underrepresented populations.While recent multimodal reasoning foundation models have advanced clinicaldiagnosis through integrated analysis of diverse medical data, reasoningtrainings via reinforcement learning inherit and often amplify biases presentin training datasets dominated by majority populations. We introduceFairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchicalreinforcement learning approach that promotes equitable learning acrossheterogeneous clinical populations. FairGRPO employs adaptive importanceweighting of advantages based on representation, task difficulty, and datasource. To address the common issue of missing demographic labels in theclinical domain, we further employ unsupervised clustering, which automaticallydiscovers latent demographic groups when labels are unavailable. Throughcomprehensive experiments across 7 clinical diagnostic datasets spanning 5clinical modalities across X-ray, CT scan, dermoscropy, mammography andultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%against all vanilla and bias mitigated RL baselines, while improving F1 scoreby 12.49%. Furthermore, training dynamics analysis reveals that FairGRPOprogressively improves fairness throughout optimization, while baseline RLmethods exhibit deteriorating fairness as training progresses. Based onFairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM thatachieves state-of-the-art performance while demonstrating significantly reduceddisparities across demographic groups.</description>
      <author>example@mail.com (Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang)</author>
      <guid isPermaLink="false">2510.19893v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>SEMPO: Lightweight Foundation Models for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.19710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SEMPO是一种新型轻量级时间序列预测基础模型，通过两个创新模块在减少预训练数据规模和模型大小的同时实现强大的预测性能。&lt;h4&gt;背景&lt;/h4&gt;现有的时间序列基础模型虽然性能出色，但网络架构庞大，需要大规模数据集进行预训练，难以在资源受限环境中部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种多功能且经济实惠的时间序列基础模型，解决现有模型在多功能性和可负担性之间的矛盾。&lt;h4&gt;方法&lt;/h4&gt;SEMPO包含两个关键模块：(1)能量感知的频谱分解模块，同时建模高能量和低能量但信息丰富的频率信号；(2)基于提示混合的Transformer，通过小型数据集特定提示学习异构时间模式，实现参数高效模型适应。&lt;h4&gt;主要发现&lt;/h4&gt;SEMPO在两个大规模基准测试(包含16个数据集)上的实验表明，与最先进方法相比，它在零样本和少样本预测场景中表现出优越性能，同时显著减少了预训练数据规模和模型大小。&lt;h4&gt;结论&lt;/h4&gt;SEMPO成功实现了时间序列预测领域多功能性和可负担性的平衡，为资源受限环境中的时间序列预测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近大型预训练模型的兴起见证了在时间序列预测领域开发基础模型的显著成功。尽管在各种下游预测任务中表现出令人印象深刻的性能，但现有时间序列基础模型拥有庞大的网络架构，需要在大规模数据集上进行大量预训练，这严重阻碍了它们在资源受限环境中的部署。为了应对多功能性和可负担性之间日益加剧的矛盾，我们提出了SEMPO，一种新型轻量级基础模型，它只需要在相对小规模的数据上进行预训练，却表现出强大的通用时间序列预测能力。具体而言，SEMPO包含两个关键模块：1)能量感知的频谱分解模块，通过不仅建模高能量频率信号，还建模当前方法中被忽略的低能量但信息丰富的频率信号，显著提高了预训练数据的利用率；以及2)基于提示混合的Transformer，通过小型数据集特定的提示学习异构时间模式，并将时间序列标记自适应路由到基于提示的专家，实现跨不同数据集和领域的参数高效模型适应。配备这些模块后，SEMPO显著减少了预训练数据规模和模型大小，同时实现了强大的泛化能力。在覆盖16个数据集的两个大规模基准测试上进行的广泛实验表明，与最先进的方法相比，SEMPO在零样本和少样本预测场景中表现出优越性能。代码和数据可在https://github.com/mala-lab/SEMPO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent boom of large pre-trained models witnesses remarkable success indeveloping foundation models (FMs) for time series forecasting. Despiteimpressive performance across diverse downstream forecasting tasks, existingtime series FMs possess massive network architectures and require substantialpre-training on large-scale datasets, which significantly hinders theirdeployment in resource-constrained environments. In response to this growingtension between versatility and affordability, we propose SEMPO, a novellightweight foundation model that requires pretraining on relativelysmall-scale data, yet exhibits strong general time series forecasting.Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctraldecomposition module, that substantially improves the utilization ofpre-training data by modeling not only the high-energy frequency signals butalso the low-energy yet informative frequency signals that are ignored incurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learnsheterogeneous temporal patterns through small dataset-specific prompts andadaptively routes time series tokens to prompt-based experts forparameter-efficient model adaptation across different datasets and domains.Equipped with these modules, SEMPO significantly reduces both pre-training datascale and model size, while achieving strong generalization. Extensiveexperiments on two large-scale benchmarks covering 16 datasets demonstrate thesuperior performance of SEMPO in both zero-shot and few-shot forecastingscenarios compared with state-of-the-art methods. Code and data are availableat https://github.com/mala-lab/SEMPO.</description>
      <author>example@mail.com (Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang)</author>
      <guid isPermaLink="false">2510.19710v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</title>
      <link>http://arxiv.org/abs/2510.19585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review. Both the dataset and code will be published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一项从混合语言历史文档中提取拉丁语片段的新任务，并评估了大型基础模型在此任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;历史文档通常包含多种语言和不同的布局，从中提取特定语言片段具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;评估大型基础模型在从混合语言历史文档中提取拉丁语片段任务上的能力和局限性。&lt;h4&gt;方法&lt;/h4&gt;使用包含724个标注页面的多模态数据集，对大型基础模型进行了基准测试和性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;当代模型能够可靠地检测和提取拉丁语片段。&lt;h4&gt;结论&lt;/h4&gt;该研究首次全面分析了大型基础模型在从混合语言历史文档中提取拉丁语片段任务上的能力和局限性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一项从混合语言历史文档中提取拉丁语片段的新任务，这些文档具有不同的布局。我们使用一个包含724个标注页面的多模态数据集，对大型基础模型进行了基准测试和性能评估。结果表明，使用当代模型进行可靠的拉丁语检测是可行的。我们的研究首次对这些模型在此任务上的能力和局限性进行了全面分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel task of extracting Latin fragments frommixed-language historical documents with varied layouts. We benchmark andevaluate the performance of large foundation models against a multimodaldataset of 724 annotated pages. The results demonstrate that reliable Latindetection with contemporary models is achievable. Our study provides the firstcomprehensive analysis of these models' capabilities and limits for this task.</description>
      <author>example@mail.com (Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu Mäkelä, Mikko Tolonen)</author>
      <guid isPermaLink="false">2510.19585v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GigaBrain-0: A World Model-Powered Vision-Language-Action Model</title>
      <link>http://arxiv.org/abs/2510.19430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://gigabrain0.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了GigaBrain-0，一个利用世界模型生成数据的新型视觉-语言-动作(VLA)基础模型，减少了对真实机器人数据的依赖，提高了跨任务泛化能力和策略鲁棒性，在灵巧操作、长视野和移动操作任务中实现了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;为通用机器人训练视觉-语言-动作(VLA)模型通常需要大规模的真实世界机器人数据，这些数据的收集既昂贵又耗时。物理数据收集的低效严重限制了当前VLA系统的可扩展性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决物理数据收集低效的问题，减少对真实机器人数据的依赖，同时提高VLA系统的跨任务泛化能力和策略鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 引入GigaBrain-0，由世界模型生成数据(如视频生成、真实到真实转移、人类转移、视角转移、仿真到真实转移数据)赋能的新型VLA基础模型；2. 利用世界模型大规模生成多样化数据；3. 通过RGBD输入建模和具身思维链(CoT)监督提高策略鲁棒性；4. 开发了GigaBrain-0-Small，一个优化的轻量级变体，可在NVIDIA Jetson AGX Orin等设备上高效运行。&lt;h4&gt;主要发现&lt;/h4&gt;1. GigaBrain-0在灵巧操作、长视野和移动操作任务中实现了显著的性能提升；2. 广泛的实验证明GigaBrain-0在外观(如纹理、颜色)、物体放置和摄像机视点变化方面具有优越的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过利用世界模型生成数据，GigaBrain-0显著减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力和策略鲁棒性，为通用机器人提供了一个更高效、更可扩展的VLA解决方案。&lt;h4&gt;翻译&lt;/h4&gt;为通用机器人训练视觉-语言-动作(VLA)模型通常需要大规模的真实世界机器人数据，这些数据的收集既昂贵又耗时。物理数据收集的低效严重限制了当前VLA系统的可扩展性和泛化能力。为应对这一挑战，我们引入了GigaBrain-0，一个由世界模型生成数据(如视频生成、真实到真实转移、人类转移、视角转移、仿真到真实转移数据)赋能的新型VLA基础模型。通过利用世界模型大规模生成多样化数据，GigaBrain-0显著减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力。我们的方法通过RGBD输入建模和具身思维链(CoT)监督进一步提高了策略鲁棒性，使模型能够在任务执行过程中推理空间几何、物体状态和长视野依赖关系。这导致在灵巧操作、长视野和移动操作任务中的实际性能显著提升。广泛的实验证明，GigaBrain-0在外观(如纹理、颜色)、物体放置和摄像机视点变化方面实现了优越的泛化能力。此外，我们提出了GigaBrain-0-Small，一个优化的轻量级变体，设计用于在NVIDIA Jetson AGX Orin等设备上高效运行。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决训练视觉-语言-行动（VLA）模型需要大规模真实世界机器人数据的问题，而收集这些数据既昂贵又耗时。这个问题很重要，因为它严重限制了当前VLA系统的可扩展性和泛化能力，阻碍了通用机器人在多样化环境中的应用。缺乏足够多样性的训练数据导致模型在现实世界中表现不佳，限制了机器人技术的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到真实世界机器人数据收集的局限性，然后提出利用世界模型生成多样化训练数据的解决方案。他们设计了混合transformer架构，结合预训练视觉语言模型和动作扩散变换器，并引入RGB-D输入和具身思维链机制。作者借鉴了多项现有工作，包括π0等VLA模型架构、世界模型作为数据生成器、视觉语言模型如PaliGemma2、扩散模型用于视频生成，以及思维链推理和知识隔离技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用世界模型生成多样化、逼真的合成训练数据，减少对真实世界机器人数据的依赖，并通过RGB-D输入和具身思维链增强模型的感知和推理能力。整体流程包括：1）收集真实世界数据并利用GigaWorld生成多种合成数据（Real2Real转移、视图转移等）；2）采用混合transformer架构处理RGB-D输入和语言指令；3）训练模型生成具身思维链作为中间表示；4）基于思维链输出连续动作序列；5）提供轻量级版本适配边缘设备。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）整合多种世界模型生成的数据源，增强数据多样性；2）RGB-D输入建模提升3D空间理解；3）具身思维链监督机制改善推理能力；4）混合架构与知识隔离技术提高训练效率；5）高效数据生成与质量评估。相比之前工作，GigaBrain-0利用了更多样化的数据源（包括视图转移和Real2Real转移），具有更强的3D空间理解能力，能显式生成中间推理步骤，训练效率更高，在变化条件下泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GigaBrain-0通过创新性地整合世界模型生成的多样化训练数据和具身思维链推理机制，显著提升了视觉-语言-行动模型在真实世界任务中的泛化能力和执行效率，同时大幅减少了对昂贵真实世界机器人数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training Vision-Language-Action (VLA) models for generalist robots typicallyrequires large-scale real-world robot data, which is expensive andtime-consuming to collect. The inefficiency of physical data collectionseverely limits the scalability, and generalization capacity of current VLAsystems. To address this challenge, we introduce GigaBrain-0, a novel VLAfoundation model empowered by world model-generated data (e.g., videogeneration, real2real transfer, human transfer, view transfer, sim2realtransfer data). By leveraging world models to generate diverse data at scale,GigaBrain-0 significantly reduces reliance on real robot data while improvingcross-task generalization. Our approach further improves policy robustnessthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,enabling the model to reason about spatial geometry, object states, andlong-horizon dependencies during task execution. This leads to substantialgains in real-world performance on dexterous, long-horizon, and mobilemanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achievessuperior generalization across variations in appearances (e.g., textures,colors), object placements, and camera viewpoints. Additionally, we presentGigaBrain-0-Small, an optimized lightweight variant designed to run efficientlyon devices such as the NVIDIA Jetson AGX Orin.</description>
      <author>example@mail.com (GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yilong Li, Yiran Ding, Yuan Xu, Yun Ye, Yukun Zhou, Zhehao Dong, Zhenan Wang, Zhichao Liu, Zheng Zhu)</author>
      <guid isPermaLink="false">2510.19430v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets</title>
      <link>http://arxiv.org/abs/2510.19373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单的策略训练采样方法，用于缓解机器人任务数据集中物理动作序列的不平衡问题。&lt;h4&gt;背景&lt;/h4&gt;随着越来越多的机器人动作和感官观测数据集被收集用于训练大型神经网络，发现许多基于不同描述的任务实际上涉及非常相似的身体动作序列，导致数据集在物理机器人动作方面存在严重不平衡。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单的采样策略来缓解机器人任务数据集中的动作不平衡问题，提高模型在多任务场景下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单的策略训练采样方法，只需几行代码即可集成到现有代码库中，并在预训练小型模型和微调大型基础模型上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;与之前最先进的方法相比，该方法在低资源任务上取得了显著改进，同时没有降低高资源任务上的性能，使得模型容量能够更有效地用于多任务策略。&lt;h4&gt;结论&lt;/h4&gt;在Franka Panda机械臂上的多样化任务中进一步验证了该方法的有效性，证明了其在实际应用中的可行性。&lt;h4&gt;翻译&lt;/h4&gt;越来越多的机器人动作和感官观测数据集被收集起来，用于训练日益庞大的神经网络。这些数据集是基于任务收集的，尽管这些任务在描述上可能不同，但许多任务涉及非常相似的身体动作序列（例如，'拿起苹果'与'拿起橙子'）。因此，许多机器人任务数据集在所代表的物理机器人动作方面存在严重不平衡。在这项工作中，我们提出了一种简单的策略训练采样方法来缓解这种不平衡。我们的方法只需要几行代码就可以集成到现有代码库中，并提高了泛化能力。我们在预训练小型模型和微调大型基础模型上都评估了我们的方法。结果表明，与之前最先进的方法相比，在低资源任务上取得了显著改进，同时没有降低高资源任务上的性能。这使得模型容量能够更有效地用于多任务策略。我们还进一步在Franka Panda机械臂上的多样化任务设置中验证了我们的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increasingly large datasets of robot actions and sensory observations arebeing collected to train ever-larger neural networks. These datasets arecollected based on tasks and while these tasks may be distinct in theirdescriptions, many involve very similar physical action sequences (e.g., 'pickup an apple' versus 'pick up an orange'). As a result, many datasets of robotictasks are substantially imbalanced in terms of the physical robotic actionsthey represent. In this work, we propose a simple sampling strategy for policytraining that mitigates this imbalance. Our method requires only a few lines ofcode to integrate into existing codebases and improves generalization. Weevaluate our method in both pre-training small models and fine-tuning largefoundational models. Our results show substantial improvements on low-resourcetasks compared to prior state-of-the-art methods, without degrading performanceon high-resource tasks. This enables more effective use of model capacity formulti-task policies. We also further validate our approach in a real-worldsetup on a Franka Panda robot arm across a diverse set of tasks.</description>
      <author>example@mail.com (Basavasagar Patil, Sydney Belt, Jayjun Lee, Nima Fazeli, Bernadette Bucher)</author>
      <guid isPermaLink="false">2510.19373v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AMAuT: A Flexible and Efficient Multiview Audio Transformer Framework Trained from Scratch</title>
      <link>http://arxiv.org/abs/2510.19368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AMAuT框架，一个无需预训练权重且支持任意采样率和音频长度的音频模型，在多个基准测试中达到高准确度同时大幅减少计算资源消耗。&lt;h4&gt;背景&lt;/h4&gt;最近的SSAST、EAT、HuBERT、Qwen-Audio和AudioFlamingo等基础模型在标准音频基准测试中表现优异，但受限于固定的输入速率和持续时间，影响了它们的重用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需依赖预训练权重、支持任意采样率和音频长度的音频分类框架，提高模型的灵活性和效率。&lt;h4&gt;方法&lt;/h4&gt;AMAuT集成了四个关键组件：增强驱动的多视图学习提高鲁棒性；conv1+conv7+conv1一维CNN瓶颈实现稳定的时间编码；双CLS+TAL令牌进行双向上下文表示；测试时自适应/增强(TTA²)提高推理可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;在AudioMNIST、SpeechCommands V1&amp;V2、VocalSound和CochlScene五个公共基准测试上，AMAuT准确度高达99.8%，同时消耗的GPU小时数不到可比预训练模型的3%。&lt;h4&gt;结论&lt;/h4&gt;AMAuT为大型预训练模型提供了一个高效且灵活的替代方案，使最先进的音频分类在计算受限环境中变得可行。&lt;h4&gt;翻译&lt;/h4&gt;最近的SSAST、EAT、HuBERT、Qwen-Audio和AudioFlamingo等基础模型在标准音频基准测试中取得了顶尖结果，但受限于固定的输入速率和持续时间，阻碍了它们的重用性。本文引入了增强驱动多视图音频变换器(AMAuT)，这是一个从头开始训练的框架，消除对预训练权重的依赖，同时支持任意采样率和音频长度。AMAuT集成了四个关键组件：(1)增强驱动的多视图学习，提高鲁棒性；(2)conv1+conv7+conv1一维CNN瓶颈，用于稳定的时间编码；(3)双CLS+TAL令牌，用于双向上下文表示；(4)测试时自适应/增强(TTA²)，提高推理可靠性。在AudioMNIST、SpeechCommands V1&amp;V2、VocalSound和CochlScene五个公共基准测试上的实验表明，AMAuT准确度高达99.8%，同时消耗的GPU小时数不到可比预训练模型的3%。因此，AMAuT为大型预训练模型提供了一个高效且灵活的替代方案，使最先进的音频分类在计算受限环境中变得可行。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent foundational models, SSAST, EAT, HuBERT, Qwen-Audio, and AudioFlamingo, achieve top-tier results across standard audio benchmarks but arelimited by fixed input rates and durations, hindering their reusability. Thispaper introduces the Augmentation-driven Multiview Audio Transformer (AMAuT), atraining-from-scratch framework that eliminates the dependency on pre-trainedweights while supporting arbitrary sample rates and audio lengths. AMAuTintegrates four key components: (1) augmentation-driven multiview learning forrobustness, (2) a conv1 + conv7 + conv1 one-dimensional CNN bottleneck forstable temporal encoding, (3) dual CLS + TAL tokens for bidirectional contextrepresentation, and (4) test-time adaptation/augmentation (TTA^2) to improveinference reliability. Experiments on five public benchmarks, AudioMNIST,SpeechCommands V1 &amp; V2, VocalSound, and CochlScene, show that AMAuT achievesaccuracies up to 99.8% while consuming less than 3% of the GPU hours requiredby comparable pre-trained models. Thus, AMAuT presents a highly efficient andflexible alternative to large pre-trained models, making state-of-the-art audioclassification accessible in computationally constrained settings.</description>
      <author>example@mail.com (Weichuang Shao, Iman Yi Liao, Tomas Henrique Bode Maul, Tissa Chandesa)</author>
      <guid isPermaLink="false">2510.19368v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model Forecasts: Form and Function</title>
      <link>http://arxiv.org/abs/2510.19345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)虽然预测准确性高，但预测形式（点预测、分位数预测、参数化预测或轨迹集合）决定了其实际应用价值。研究发现大多数TSFMs只能提供点或参数化预测，而实际操作任务常需要保留时间依赖性的轨迹集合。研究确定了预测类型间的转换条件，证明边际分布无法确定路径相关事件概率，并将六个基本预测任务映射到最小充分预测类型，表明预测类型而非准确性才是区分模型实用价值的关键。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)在预测准确性方面表现出色，但准确性并不完全决定其实际价值。&lt;h4&gt;目的&lt;/h4&gt;研究不同预测形式对实际操作任务的支持能力，确定预测类型间的转换条件，并提供任务对齐的评估框架。&lt;h4&gt;方法&lt;/h4&gt;分析现有TSFMs的预测类型，研究预测类型之间的转换条件，证明边际分布与联合分布的关系，并将基本预测任务映射到最小充分预测类型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 三分之二的TSFMs只产生点或参数化预测，而许多操作任务需要保留时间依赖性的轨迹集合；2. 轨迹集合可通过边际化转换为简单形式，但反向转换需要额外方法；3. 边际分布无法确定路径相关事件概率，无限多联合分布可具有相同边际分布但给出不同操作答案；4. 六个基本预测任务可映射到最小充分预测类型。&lt;h4&gt;结论&lt;/h4&gt;在实际应用中，预测类型而非准确性是区分模型实用价值的关键因素。选择适当的预测形式对于支持特定操作任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)实现了强大的预测准确性，然而准确性本身并不决定实际价值。预测的形式——点预测、分位数预测、参数化预测或轨迹集合——从根本上限制了它能够支持的操作任务。我们调查了最近的TSFMs，发现三分之二只产生点预测或参数化预测，而许多操作任务需要保留时间依赖性的轨迹集合。我们确定了预测类型何时可以转换、何时不可以转换：轨迹集合可以通过边际化转换为更简单的形式而无需额外假设，但反向转换则需要通过copulas或conformal方法施加时间依赖性。我们证明了边际分布无法确定路径相关事件概率——无限多的联合分布具有相同的边际分布，但对操作问题给出不同的答案。我们将六个基本预测任务映射到最小充分预测类型，并提供了任务对齐的评估框架。我们的分析阐明了当预测类型而非准确性区分实用价值时的情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series foundation models (TSFMs) achieve strong forecast accuracy, yetaccuracy alone does not determine practical value. The form of a forecast --point, quantile, parametric, or trajectory ensemble -- fundamentally constrainswhich operational tasks it can support. We survey recent TSFMs and find thattwo-thirds produce only point or parametric forecasts, while many operationaltasks require trajectory ensembles that preserve temporal dependence. Weestablish when forecast types can be converted and when they cannot: trajectoryensembles convert to simpler forms via marginalization without additionalassumptions, but the reverse requires imposing temporal dependence throughcopulas or conformal methods. We prove that marginals cannot determinepath-dependent event probabilities -- infinitely many joint distributions shareidentical marginals but yield different answers to operational questions. Wemap six fundamental forecasting tasks to minimal sufficient forecast types andprovide a task-aligned evaluation framework. Our analysis clarifies whenforecast type, not accuracy, differentiates practical utility.</description>
      <author>example@mail.com (Alvaro Perez-Diaz, James C. Loach, Danielle E. Toutoungi, Lee Middleton)</author>
      <guid isPermaLink="false">2510.19345v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Slot Filling as a Reasoning Task for SpeechLLMs</title>
      <link>http://arxiv.org/abs/2510.19326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将推理能力整合到语音大语言模型中用于端到端槽填充任务，通过链式思维框架分解任务并创建推理数据集，实验表明混合语音LLM结合直接和推理模式表现最佳。&lt;h4&gt;背景&lt;/h4&gt;受到最近推理大语言模型发展的启发，研究者尝试将推理能力引入语音大语言模型。&lt;h4&gt;目的&lt;/h4&gt;通过链式思维框架将槽填充任务分解为多个推理步骤，创建推理数据集，并应用监督微调策略到语音大语言模型中。&lt;h4&gt;方法&lt;/h4&gt;区分常规和推理语音大语言模型，实验不同类型和大小的LLM作为文本基础模型，通过引入推理步骤展示性能改进。&lt;h4&gt;主要发现&lt;/h4&gt;引入推理步骤可提升性能；主要为数学、逻辑和编码领域开发的推理文本LLM作为基础模型时表现不佳；混合语音LLM结合直接和推理操作模式比单一模式微调的模型性能更好。&lt;h4&gt;结论&lt;/h4&gt;混合语音LLM（结合直接和推理模式）在性能上优于仅使用一种模式的模型，是更优的选择。&lt;h4&gt;翻译&lt;/h4&gt;我们提出将推理整合到语音大语言模型中用于端到端槽填充任务。受推理大语言模型最近发展的启发，我们使用链式思维框架将槽填充任务分解为多个推理步骤，创建推理数据集，并应用监督微调策略到语音大语言模型中。我们区分常规和推理语音大语言模型，并实验不同类型和大小的LLM作为它们的文本基础模型。我们通过引入推理（中间）步骤展示了性能改进。然而，我们表明主要为数学、逻辑和编码领域开发的推理文本LLM作为推理语音LLM的基础模型时可能表现不佳。我们进一步表明，构建在混合文本基础LLM上并微调以保留直接和推理操作模式的混合语音LLM，比仅使用一种操作模式微调的模型有更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose integration of reasoning into speech large language models(speechLLMs) for the end-to-end slot-filling task. Inspired by the recentdevelopment of reasoning LLMs, we use a chain-of-thought framework to decomposethe slot-filling task into multiple reasoning steps, create a reasoning datasetand apply the supervised fine-tuning strategy to a speechLLM. We distinguishbetween regular and reasoning speechLLMs and experiment with different typesand sizes of LLMs as their text foundation models. We demonstrate performanceimprovements by introducing reasoning (intermediate) steps. However, we showthat a reasoning textual LLM developed mainly for math, logic and codingdomains might be inferior as a foundation model for a reasoning speechLLM. Wefurther show that hybrid speechLLMs, built on a hybrid text foundation LLM andfine-tuned to preserve both direct and reasoning modes of operation, havebetter performance than those fine-tuned employing only one mode of operation.</description>
      <author>example@mail.com (Kadri Hacioglu, Manjunath K E, Andreas Stolcke)</author>
      <guid isPermaLink="false">2510.19326v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization</title>
      <link>http://arxiv.org/abs/2510.19325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为超体积优化(HVO)的新策略，用于解决大型语言模型在文本摘要任务中的多目标优化问题，通过动态调整奖励过程中的分数，使模型逐步逼近帕累托前沿，生成在多个维度上平衡的摘要。&lt;h4&gt;背景&lt;/h4&gt;文本摘要需要同时优化一致性、连贯性、相关性和流畅性等多个目标，这带来了很大挑战。虽然大型语言模型通过强化学习已展现出卓越性能，但很少有研究关注基于LLMs通过RL优化摘要的多目标问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的优化策略，用于解决基于大型语言模型的文本摘要任务中的多目标优化问题，生成在多个维度上更加平衡的摘要。&lt;h4&gt;方法&lt;/h4&gt;提出超体积优化(HVO)方法，在强化学习的奖励过程中使用超体积方法动态调整组之间的分数，引导模型优化逐步逼近帕累托前沿，从而在多个目标上生成平衡的摘要。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性摘要数据集上的实验表明，HVO在总体得分上优于组相对策略优化(GRPO)，且在不同维度上表现更平衡。通过HVO增强的7B基础模型在摘要任务中表现与GPT-4相当，同时保持更短的生成长度。&lt;h4&gt;结论&lt;/h4&gt;HVO是一种有效的多目标优化方法，能够生成在多个维度上平衡的摘要，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;文本摘要是一项关键任务，需要同时优化一致性、连贯性、相关性和流畅性等多个目标，这带来了相当大的挑战。尽管大型语言模型已经展示了卓越的性能，并通过强化学习得到了增强，但很少有研究关注基于LLMs通过RL优化摘要的多目标问题。在本文中，我们引入了超体积优化(HVO)，一种新颖的优化策略，通过使用超体积方法在强化学习的奖励过程中动态调整组之间的分数。这种方法引导模型的优化逐步逼近帕累托前沿，从而在多个目标上生成平衡的摘要。在几个代表性摘要数据集上的实验结果表明，我们的方法在总体得分上优于组相对策略优化(GRPO)，并在不同维度上表现出更平衡的性能。此外，通过HVO增强的7B基础模型在摘要任务中表现与GPT-4相当，同时保持更短的生成长度。我们的代码已在https://github.com/ai4business-LiAuto/HVO.git公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text summarization is a crucial task that requires the simultaneousoptimization of multiple objectives, including consistency, coherence,relevance, and fluency, which presents considerable challenges. Although largelanguage models (LLMs) have demonstrated remarkable performance, enhanced byreinforcement learning (RL), few studies have focused on optimizing themulti-objective problem of summarization through RL based on LLMs. In thispaper, we introduce hypervolume optimization (HVO), a novel optimizationstrategy that dynamically adjusts the scores between groups during the rewardprocess in RL by using the hypervolume method. This method guides the model'soptimization to progressively approximate the pareto front, thereby generatingbalanced summaries across multiple objectives. Experimental results on severalrepresentative summarization datasets demonstrate that our method outperformsgroup relative policy optimization (GRPO) in overall scores and shows morebalanced performance across different dimensions. Moreover, a 7B foundationmodel enhanced by HVO performs comparably to GPT-4 in the summarization task,while maintaining a shorter generation length. Our code is publicly availableat https://github.com/ai4business-LiAuto/HVO.git</description>
      <author>example@mail.com (Junjie Song, Yiwen Liu, Dapeng Li, Yin Sun, Shukun Fu, Siqi Chen, Yuji Cao)</author>
      <guid isPermaLink="false">2510.19325v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Advances in 4D Representation: Geometry, Motion, and Interaction</title>
      <link>http://arxiv.org/abs/2510.19255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages. Project Page: https://mingrui-zhao.github.io/4DRep-GMI/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于4D生成和重建的调查论文，从4D表示的独特视角出发，帮助读者了解如何选择和定制适合自己任务的4D表示方法。&lt;h4&gt;背景&lt;/h4&gt;4D生成和重建是计算机图形学中一个快速发展的子领域，其发展受到神经场、几何和深度学习以及3D生成人工智能(GenAI)最近进展的推动。&lt;h4&gt;目的&lt;/h4&gt;帮助读者了解如何选择和定制适合自己任务的4D表示方法，以建模随时间演变的3D几何并展示运动和交互。&lt;h4&gt;方法&lt;/h4&gt;采用选择性方法，重点关注代表性工作，以突出不同计算、应用和数据场景下每种表示的理想特性和随之而来的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;将4D表示基于几何、运动和交互三个关键支柱进行分类；涵盖当前流行的表示方法如NeRFs和3DGS，以及相对未被充分探索的表示；讨论大型语言模型和视频基础模型在4D应用中的作用及其局限性；分析当前可用的4D数据集及推动领域发展所需的更多数据集。&lt;h4&gt;结论&lt;/h4&gt;选择和定制适当的4D表示对于完成特定任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了关于4D生成和重建的调查，这是计算机图形学中一个快速发展的子领域，其发展受到神经场、几何和深度学习以及3D生成人工智能(GenAI)最近进展的推动。虽然我们的调查不是首创，但我们从4D表示的独特视角构建了该领域的覆盖范围，用于建模随时间演变的3D几何并展示运动和交互。具体而言，我们没有提供大量工作的详尽列举，而是采用更选择性的方法，重点关注代表性工作，以突出不同计算、应用和数据场景下每种表示的理想特性和随之而来的挑战。我们旨在传达给读者的主要信息是如何为他们的任务选择和定制适当的4D表示。在组织上，我们基于三个关键支柱来区分4D表示：几何、运动和交互。我们的讨论不仅将涵盖当今最受欢迎的表示，如神经辐射场(NeRFs)和3D高斯溅射(3DGS)，还将引起对4D背景下相对未被充分探索的表示的关注，如结构化模型和长程运动。在整个调查中，我们将回顾大型语言模型(LLMs)和视频基础模型(VFMs)在多种4D应用中的作用，同时引导讨论它们当前的局限性以及如何解决这些局限性。我们还专门介绍了当前可用的4D数据集，以及推动该子领域发展所缺乏的数据集。项目页面：https://mingrui-zhao.github.io/4DRep-GMI/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D表示（随时间变化的3D几何形状）的系统分类和分析问题。这个问题在现实中非常重要，因为随着计算机图形学应用扩展到电影视觉效果、虚拟现实、自主机器人、医学成像和电子商务等领域，能够捕获、表示和操作4D内容已成为连接图形学、视觉和机器学习的基本挑战。4D表示技术能够帮助我们理解和建模动态世界，为各种应用提供基础支撑。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用以表示为中心的独特视角，而非像之前综述那样按应用或方法分类。他们借鉴了Cao等人、Fan等人、Miao等人的工作，但认为这些综述未能充分涵盖所有相关表示方法，特别是结构化表示、运动和交互方面。作者通过三个关键支柱（几何、运动和交互）构建分析框架，并在几何部分进一步区分结构化和非结构化表示，从而提供了一个更全面、更有条理的分析视角。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为一篇综述论文，其核心思想是从表示角度系统分类和分析4D表示方法，帮助研究人员理解不同表示的特性、优势和局限性。整体流程分为六个部分：1）引言介绍背景和问题；2）几何建模分析非结构化表示（网格、点云、NeRF、3D高斯飞溅）和结构化表示（模板、部件、图）；3）运动建模分析不同运动类型与表示的交互；4）交互建模讨论多实体交互表示；5）数据集、评估指标和基准测试；6）整体分析和未来方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）采用表示中心视角而非应用或方法分类；2）提出几何、运动、交互三支柱框架；3）明确区分结构化与非结构化表示；4）全面分析不同运动类型与表示选择的相互作用；5）专门讨论交互表示问题；6）探讨大型语言模型和视频基础模型在4D应用中的作用。相比之前工作，这篇论文提供了更全面的表示分类，更深入分析表示方法的优缺点和适用场景，并提供了如何为特定任务选择表示的实用指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过从几何、运动和交互三个关键支柱对4D表示方法进行系统分类和分析，为研究人员提供了如何为特定4D任务选择和定制适当表示的全面指导框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a survey on 4D generation and reconstruction, a fast-evolvingsubfield of computer graphics whose developments have been propelled by recentadvances in neural fields, geometric and motion deep learning, as well 3Dgenerative artificial intelligence (GenAI). While our survey is not the firstof its kind, we build our coverage of the domain from a unique and distinctiveperspective of 4D representations\/}, to model 3D geometry evolving over timewhile exhibiting motion and interaction. Specifically, instead of offering anexhaustive enumeration of many works, we take a more selective approach byfocusing on representative works to highlight both the desirable properties andensuing challenges of each representation under different computation,application, and data scenarios. The main take-away message we aim to convey tothe readers is on how to select and then customize the appropriate 4Drepresentations for their tasks. Organizationally, we separate the 4Drepresentations based on three key pillars: geometry, motion, and interaction.Our discourse will not only encompass the most popular representations oftoday, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),but also bring attention to relatively under-explored representations in the 4Dcontext, such as structured models and long-range motions. Throughout oursurvey, we will reprise the role of large language models (LLMs) and videofoundational models (VFMs) in a variety of 4D applications, while steering ourdiscussion towards their current limitations and how they can be addressed. Wealso provide a dedicated coverage on what 4D datasets are currently available,as well as what is lacking, in driving the subfield forward. Projectpage:https://mingrui-zhao.github.io/4DRep-GMI/</description>
      <author>example@mail.com (Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang)</author>
      <guid isPermaLink="false">2510.19255v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>TinyUSFM: Towards Compact and Efficient Ultrasound Foundation Models</title>
      <link>http://arxiv.org/abs/2510.19239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submit to JBHI, 14 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TinyUSLM是一种通过知识蒸馏技术开发的轻量级超声基础模型，能够在保持优异性能的同时显著减少计算资源需求，使其适用于资源有限的临床环境。&lt;h4&gt;背景&lt;/h4&gt;医学成像的基础模型在多样化的解剖结构和临床应用中表现出优越的泛化能力，但其出色的性能依赖于大量计算资源，限制了在资源有限的临床环境中的部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级超声基础模型，能够在保持大规模超声基础模型(USFM)的优异器官多样性和任务适应性的同时，实现显著的计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出特征梯度驱动的核心集选择策略筛选高质量训练数据；开发域分离的掩码图像建模辅助一致性驱动的动态蒸馏保留空间和频域特性；建立包含8个分类和10个分割数据集的UniUS-Bench超声基准。&lt;h4&gt;主要发现&lt;/h4&gt;TinyUSLM仅使用20万张图像进行蒸馏，就能以仅6.36%的参数和6.40%的GFLOPs达到与USFM相当的性能；在分类和分割任务上分别比普通模型高出9.45%和7.72%；超越了所有最先进的轻量级模型；实现了84.91%的平均分类准确率和85.78%的平均分割Dice分数。&lt;h4&gt;结论&lt;/h4&gt;TinyUSLM成功实现了轻量级超声基础模型的开发，在保持优异性能的同时显著降低了计算资源需求，使其适用于资源有限的临床环境，为医学成像领域提供了实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学成像基础模型在多样化的解剖结构和临床应用中表现出优越的泛化能力。它们的出色性能依赖于大量计算资源，限制了在资源有限的临床环境中的部署。本文提出了TinyUSLM，这是第一个轻量级超声基础模型，通过使用精心筛选的小型数据集进行知识蒸馏，保持了我们的大规模超声基础模型(USFM)的卓越器官多样性和任务适应性，在不牺牲性能的情况下提供了显著的计算效率。考虑到轻量级模型的有限容量和表示能力，我们提出了一个特征梯度驱动的核心集选择策略，用于筛选高质量的紧凑训练数据，避免因低质量冗余图像导致的训练退化。为了在知识转移过程中保留基本的空间和频域特性，我们开发了域分离的掩码图像建模辅助一致性驱动的动态蒸馏。这个新颖的框架通过利用教师模型在不同域掩码上的一致性，自适应地从大型基础模型转移知识，专门针对超声解释进行定制。为了评估，我们建立了UniUS-Bench，这是最大的公开可用超声基准，包含跨15个器官的8个分类和10个分割数据集。仅使用20万张图像进行蒸馏，TinyUSLM就能以仅6.36%的参数和6.40%的GFLOPs达到USLM的性能。TinyUSLM在分类和分割任务上分别比普通模型高出9.45%和7.72%，超越了所有最先进的轻量级模型，并在各种医疗设备和中心实现了84.91%的平均分类准确率和85.78%的平均分割Dice分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for medical imaging demonstrate superior generalizationcapabilities across diverse anatomical structures and clinical applications.Their outstanding performance relies on substantial computational resources,limiting deployment in resource-constrained clinical environments. This paperpresents TinyUSFM, the first lightweight ultrasound foundation model thatmaintains superior organ versatility and task adaptability of our large-scaleUltrasound Foundation Model (USFM) through knowledge distillation withstrategically curated small datasets, delivering significant computationalefficiency without sacrificing performance. Considering the limited capacityand representation ability of lightweight models, we propose a feature-gradientdriven coreset selection strategy to curate high-quality compact training data,avoiding training degradation from low-quality redundant images. To preservethe essential spatial and frequency domain characteristics during knowledgetransfer, we develop domain-separated masked image modeling assistedconsistency-driven dynamic distillation. This novel framework adaptivelytransfers knowledge from large foundation models by leveraging teacher modelconsistency across different domain masks, specifically tailored for ultrasoundinterpretation. For evaluation, we establish the UniUS-Bench, the largestpublicly available ultrasound benchmark comprising 8 classification and 10segmentation datasets across 15 organs. Using only 200K images in distillation,TinyUSFM matches USFM's performance with just 6.36% of parameters and 6.40% ofGFLOPs. TinyUSFM significantly outperforms the vanilla model by 9.45% inclassification and 7.72% in segmentation, surpassing all state-of-the-artlightweight models, and achieving 84.91% average classification accuracy and85.78% average segmentation Dice score across diverse medical devices andcenters.</description>
      <author>example@mail.com (Chen Ma, Jing Jiao, Shuyu Liang, Junhu Fu, Qin Wang, Zeju Li, Yuanyuan Wang, Yi Guo)</author>
      <guid isPermaLink="false">2510.19239v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Implicit Biases of Design Choices for Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.19236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过理论和实证方法分析了时间序列基础模型(TSFMs)中设计选择的影响，揭示了不同设计如何导致模型中的隐式偏置，以及这些偏置如何影响模型行为，研究结果对于理解和改进未来TSFMs的发展具有重要意义。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)是一类强大的通用工具，用于时间序列预测和相关时间任务，但这些模型的行为受到其设计中微妙归纳偏置的强烈影响。&lt;h4&gt;目的&lt;/h4&gt;理解训练过程中的各种'旋钮'如何影响模型质量，而非开发一个声称比现有TSFMs更好的新模型；探讨设计选择如何导致模型基本属性中的隐式偏置。&lt;h4&gt;方法&lt;/h4&gt;使用理论和受控经验评估相结合的方法；识别几种设计选择(如patch大小、嵌入选择、训练目标等)；研究这些设计选择如何影响模型的基本属性。&lt;h4&gt;主要发现&lt;/h4&gt;不同的设计选择会导致模型基本属性中的隐式偏置；这些偏置可能是直观的或非常违反直觉的，取决于模型和数据的特性；在异常值处理的案例研究中，展示了多种偏置如何以复杂方式相互作用；讨论了研究结果对学习'苦涩教训'和构建TSFMs的启示。&lt;h4&gt;结论&lt;/h4&gt;理解设计选择对模型行为的影响对于构建有效的TSFMs至关重要；模型中的隐式偏置可以是有益的，但也可能导致意想不到的行为。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)是一类潜在的强大通用工具，用于时间序列预测和相关时间任务，但它们的行为受到其设计中微妙归纳偏置的强烈影响。我们不是开发一个新模型并声称它比现有的TSFMs更好，例如通过在现有成熟的基准测试中获胜，我们的目标是理解训练过程中的各种'旋钮'如何影响模型质量。结合理论和受控经验评估，我们确定了几个设计选择(补丁大小、嵌入选择、训练目标等)，并展示了它们如何导致模型基本属性中的隐式偏置(时间行为、几何结构、模型回归到均值的激进程度等)；我们展示了这些偏置如何可能是直观的或非常违反直觉的，这取决于模型和数据的特性。我们还在异常值处理的案例研究中说明了多种偏置如何以复杂方式相互作用；我们讨论了我们的结果对学习苦涩教训和构建TSFMs的启示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (TSFMs) are a class of potentially powerful,general-purpose tools for time series forecasting and related temporal tasks,but their behavior is strongly shaped by subtle inductive biases in theirdesign. Rather than developing a new model and claiming that it is better thanexisting TSFMs, e.g., by winning on existing well-established benchmarks, ourobjective is to understand how the various ``knobs'' of the training processaffect model quality. Using a mix of theory and controlled empiricalevaluation, we identify several design choices (patch size, embedding choice,training objective, etc.) and show how they lead to implicit biases infundamental model properties (temporal behavior, geometric structure, howaggressively or not the model regresses to the mean, etc.); and we show howthese biases can be intuitive or very counterintuitive, depending on propertiesof the model and data. We also illustrate in a case study on outlier handlinghow multiple biases can interact in complex ways; and we discuss implicationsof our results for learning the bitter lesson and building TSFMs.</description>
      <author>example@mail.com (Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang)</author>
      <guid isPermaLink="false">2510.19236v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</title>
      <link>http://arxiv.org/abs/2510.19060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 9 figures. Metric/benchmark available at  https://github.com/amith-ananthram/posh&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PoSh的新型评估指标，用于评估视觉语言模型生成的详细图像描述，并引入了DOCENT数据集作为基准测试。PoSh使用场景图作为结构化评分指南，能够更好地模拟人类评分行为，并且在多个方面优于现有评估方法。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)已发展到能够生成详细的图像描述，但评估这些描述仍然面临挑战。现有标准评估指标(如CIDEr、SPICE)是为短文本设计的，主要针对现在已不常见的错误类型(如对象识别错误)进行调整，无法有效评估长文本中属性和关系的连接性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够评估详细图像描述的新指标，特别关注长文本中属性和关系的连接性，并能将错误定位到特定文本跨度。同时创建一个具有挑战性的新数据集来验证该指标的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出PoSh评估指标，利用场景图作为结构化评分指南指导大语言模型作为评判者，产生基于细粒度错误的聚合分数。同时创建DOCENT数据集，包含艺术品、专家参考描述和模型生成描述，配有艺术史学生的质量评估。通过PoSh评估开放和封闭模型在描述绘画、素描和雕像方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;PoSh在DOCENT上与人类判断的相关性比最佳开源替代方案更强，对图像类型具有鲁棒性，且作为奖励函数优于标准监督微调。研究发现基础模型难以实现对具有丰富场景动态的图像的完整、无错误覆盖，确立了评估VLM进展的新任务标准。&lt;h4&gt;结论&lt;/h4&gt;PoSh和DOCENT为评估详细图像描述提供了新工具，有望促进辅助文本生成等重要领域的进步，为视觉语言模型的发展提供更准确的评估方法。&lt;h4&gt;翻译&lt;/h4&gt;虽然视觉语言模型已经发展到能够进行详细的图像描述，但评估仍然是一个挑战。标准指标是为短文本设计的，并且调整为识别现在不常见的错误，如对象识别错误。相比之下，长文本需要对属性和关系连接的敏感性，以及将错误定位到特定文本跨度的评分。在这项工作中，我们介绍了PoSh，一种用于详细图像描述的指标，它使用场景图作为结构化评分指南来指导大语言模型作为评判者，产生基于细粒度错误的聚合分数。PoSh是可复制的、可解释的，并且比现有指标更好地模拟人类评分者的行为。为了验证PoSh，我们引入了一个具有挑战性的新数据集DOCENT。这个新的基准包含艺术品，配以专家撰写的参考文本和模型生成的描述，并附有艺术史学生对它们质量的细致和粗略判断。因此，DOCENT能够在一个具有挑战性的新领域评估详细的图像描述指标和详细的图像描述本身。我们表明，PoSh在DOCENT上与人类判断的相关性比最佳开源替代方案更强，对图像类型具有鲁棒性，并且是一个有效的奖励函数，优于标准的监督微调。然后，使用PoSh，我们描述了开放和封闭模型在描述绘画、素描和雕像方面的性能，发现基础模型难以实现对具有丰富场景动态的图像的完整、无错误的覆盖，从而确立了一个具有挑战性的新任务来衡量VLM的进展。通过PoSh和DOCENT，我们希望能够在辅助文本生成等重要领域取得进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While vision-language models (VLMs) have advanced into detailed imagedescription, evaluation remains a challenge. Standard metrics (e.g. CIDEr,SPICE) were designed for short texts and tuned to recognize errors that are nowuncommon, such as object misidentification. In contrast, long texts requiresensitivity to attribute and relation attachments and scores that localizeerrors to particular text spans. In this work, we introduce PoSh, a metric fordetailed image description that uses scene graphs as structured rubrics toguide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grainederrors (e.g. mistakes in compositional understanding). PoSh is replicable,interpretable and a better proxy for human raters than existing metrics(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging newdataset, DOCENT. This novel benchmark contains artwork, paired withexpert-written references, and model-generated descriptions, augmented withgranular and coarse judgments of their quality from art history students. Thus,DOCENT enables evaluating both detailed image description metrics and detailedimage description itself in a challenging new domain. We show that PoShachieves stronger correlations (+0.05 Spearman $\rho$) with the human judgmentsin DOCENT than the best open-weight alternatives, is robust to image type(using CapArena, an existing dataset of web imagery) and is a capable rewardfunction, outperforming standard supervised fine-tuning. Then, using PoSh, wecharacterize the performance of open and closed models in describing thepaintings, sketches and statues in DOCENT and find that foundation modelsstruggle to achieve full, error-free coverage of images with rich scenedynamics, establishing a demanding new task to gauge VLM progress. Through bothPoSh and DOCENT, we hope to enable advances in important areas such asassistive text generation.</description>
      <author>example@mail.com (Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown)</author>
      <guid isPermaLink="false">2510.19060v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为QKCV的注意力机制，通过融入静态类别嵌入来增强传统QKV框架，提高时间序列预测准确性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的时间序列预测任务中，类别信息在捕捉固有数据模式方面起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用类别信息提高预测准确性的注意力机制。&lt;h4&gt;方法&lt;/h4&gt;引入QKCV（Query-Key-Category-Value）注意力机制，作为传统QKV框架的扩展，融入静态类别嵌入C来强调特定类别的信息。&lt;h4&gt;主要发现&lt;/h4&gt;QKCV作为即插即用模块能增强多种基于注意力的模型在现实数据集上的预测准确性；在微调单变量时间序列基础模型时，只需更新静态嵌入C，保留预训练权重，减少计算开销并提高微调性能。&lt;h4&gt;结论&lt;/h4&gt;QKCV注意力机制能有效利用类别信息提高时间序列预测的准确性，具有良好的适应性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的时间序列预测任务中，类别信息在捕捉固有数据模式方面起着关键作用。本文引入了QKCV（查询-键-类别-值）注意力，这是传统QKV框架的扩展，融入了静态类别嵌入C来强调特定类别的信息。作为一个通用的即插即用模块，QKCV增强了基于注意力的模型（如普通Transformer、Informer、PatchTST、TFT）在各种现实世界数据集上的预测准确性。此外，QKCV在通过仅更新静态嵌入C同时保留预训练权重来微调单变量时间序列基础模型时表现出显著的适应性，从而减少了计算开销并实现了更好的微调性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world time series forecasting tasks, category information plays apivotal role in capturing inherent data patterns. This paper introduces QKCV(Query-Key-Category-Value) attention, an extension of the traditional QKVframework that incorporates a static categorical embedding C to emphasizecategory-specific information. As a versatile plug-in module, QKCV enhances theforecasting accuracy of attention-based models (e.g., Vanilla Transformer,Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCVdemonstrates remarkable adaptability in fine-tuning univariate time seriesfoundation model by solely updating the static embedding C while preservingpretrained weights, thereby reducing computational overhead and achievingsuperior fine-tuning performance.</description>
      <author>example@mail.com (Hao Wang, Baojun Ma)</author>
      <guid isPermaLink="false">2510.20222v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.19273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个轻量级的MAV动作识别框架MobiAct，实现了高精度与低计算成本的平衡，在保持92.12%平均识别准确率的同时，仅消耗136.16 pJ能量并以每秒8.84个动作的速度处理，解码速度比领先方法快2倍。&lt;h4&gt;背景&lt;/h4&gt;微型飞行器(MAV)的精确高效运动识别对于自主空中群体的实时感知和协调至关重要。然而，现有方法大多依赖于大型、计算密集型模型，不适合资源有限的MAV平台，导致识别精度和推理速度之间的权衡。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级的MAV动作识别框架MobiAct，旨在以低计算成本实现高精度的MAV动作识别。&lt;h4&gt;方法&lt;/h4&gt;采用MobileNetV4作为骨干网络；引入分阶段正交知识蒸馏(SOKD)策略将教师网络(ResNet18)的MAV运动特征有效转移到学生网络；集成无参数注意力机制提高识别精度而不增加模型复杂度；开发混合损失训练策略结合多个损失目标确保训练过程的稳定和鲁棒优化。&lt;h4&gt;主要发现&lt;/h4&gt;MobiAct实现了低能耗、低计算的MAV动作识别；在所有三个自收集数据集上，平均识别准确率达到92.12%；仅消耗136.16 pJ能量，处理速度为每秒8.84个动作；动作解码速度比领先方法快2倍，同时保持高度相当的识别精度。&lt;h4&gt;结论&lt;/h4&gt;MobiAct在MAV动作识别方面展现出卓越的效率，成功解决了识别精度与计算资源消耗之间的权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;微型飞行器(MAV)运动的精确高效识别对于自主空中群体的实时感知和协调至关重要。然而，大多数现有方法依赖于大型、计算密集型模型，不适合资源有限的MAV平台，这导致了识别精度和推理速度之间的权衡。为解决这些挑战，本文提出了一个轻量级的MAV动作识别框架MobiAct，旨在以低计算成本实现高精度。具体而言，MobiAct采用MobileNetV4作为骨干网络，并引入分阶段正交知识蒸馏(SOKD)策略，将MAV运动特征从教师网络(ResNet18)有效转移到学生网络，从而提高知识转移效率。此外，架构中集成了无参数注意力机制，在不增加模型复杂度的情况下提高识别精度。此外，还开发了混合损失训练策略，结合多个损失目标，确保训练过程中的稳定和鲁棒优化。实验结果表明，所提出的MobiAct实现了低能耗、低计算的MAV动作识别，同时在比较的方法中保持最快的动作解码速度。在所有三个自收集数据集上，MobiAct平均识别准确率达到92.12%，而仅消耗136.16 pJ的能量，并以每秒8.84个动作的速度进行识别。值得注意的是，MobiAct的动作解码速度比领先方法快2倍，同时具有高度相当的识别精度，突显了其在MAV动作识别方面的卓越效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient recognition of Micro Air Vehicle (MAV) motion isessential for enabling real-time perception and coordination in autonomousaerial swarm. However, most existing approaches rely on large, computationallyintensive models that are unsuitable for resource-limited MAV platforms, whichresults in a trade-off between recognition accuracy and inference speed. Toaddress these challenges, this paper proposes a lightweight MAV actionrecognition framework, MobiAct, designed to achieve high accuracy with lowcomputational cost. Specifically, MobiAct adopts MobileNetV4 as the backbonenetwork and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD)strategy to effectively transfer MAV motion features from a teacher network(ResNet18) to a student network, thereby enhancing knowledge transferefficiency. Furthermore, a parameter-free attention mechanism is integratedinto the architecture to improve recognition accuracy without increasing modelcomplexity. In addition, a hybrid loss training strategy is developed tocombine multiple loss objectives, which ensures stable and robust optimizationduring training. Experimental results demonstrate that the proposed MobiActachieves low-energy and low-computation MAV action recognition, whilemaintaining the fastest action decoding speed among compared methods. Acrossall three self-collected datasets, MobiAct achieves an average recognitionaccuracy of 92.12%, while consuming only 136.16 pJ of energy and processingrecognition at a rate of 8.84 actions per second. Notably, MobiAct decodesactions up to 2 times faster than the leading method, with highly comparablerecognition accuracy, highlighting its superior efficiency in MAV actionrecognition.</description>
      <author>example@mail.com (Zhang Nengbo, Ho Hann Woei)</author>
      <guid isPermaLink="false">2510.19273v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
  <item>
      <title>X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning</title>
      <link>http://arxiv.org/abs/2510.19150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了X-Ego-CS基准数据集和交叉自我中心对比学习(CECL)方法，用于研究电竞游戏中的多智能体决策和团队战术学习。&lt;h4&gt;背景&lt;/h4&gt;人类团队战术源于个人视角及其预测、解释和适应队友意图的能力。现有视频理解研究虽改善了体育中团队互动建模，但大多依赖第三方广播视角，忽视了多智能体学习的同步、自我中心特性。&lt;h4&gt;目的&lt;/h4&gt;引入X-Ego-CS基准数据集，促进复杂3D环境中多智能体决策的研究，并提供交叉自我中心视角来捕捉团队互动。&lt;h4&gt;方法&lt;/h4&gt;X-Ego-CS数据集包含45场专业级《反恐精英2》比赛的124小时游戏录像，提供所有玩家的同步第一人称视角和状态-行动轨迹。提出CECL方法，对齐队友的自我中心视觉流，培养团队战术情境意识。&lt;h4&gt;主要发现&lt;/h4&gt;CECL能有效增强智能体从单一第一人称视图推断队友和对手位置的能力，使用最先进的视频编码器实现了有效性能。&lt;h4&gt;结论&lt;/h4&gt;X-Ego-CS和CECL为电竞中的交叉自我中心多智能体基准测试奠定基础，将游戏理解定位为多智能体建模和战术学习的测试平台，对虚拟和现实领域中的时空推理和人类-AI团队协作具有启示意义。&lt;h4&gt;翻译&lt;/h4&gt;人类团队战术源于每个球员的个人视角及其预测、解释和适应队友意图的能力。尽管视频理解方面的进展已改善了体育中团队互动的建模，但大多数现有工作依赖第三方广播视角，并忽视了多智能体学习的同步、自我中心特性。我们引入X-Ego-CS基准数据集，包含来自45场专业级流行电竞游戏《反恐精英2》的124小时游戏录像，旨在促进复杂3D环境中多智能体决策的研究。X-Ego-CS提供交叉自我中心视频流，同步捕捉所有玩家的第一人称视角以及状态-行动轨迹。基于此资源，我们提出交叉自我中心对比学习(CECL)，对齐队友的自我中心视觉流，从个人视角培养团队层面的战术情境意识。我们在队友-对手位置预测任务上评估CECL，证明了其有效性，能够增强智能体使用最先进的视频编码器从单一第一人称视图推断队友和对手位置的能力。X-Ego-CS和CECL共同为电竞中的交叉自我中心多智能体基准测试奠定基础。更广泛地说，我们的工作将游戏理解定位为多智能体建模和战术学习的测试平台，对虚拟和现实领域中的时空推理以及人类-AI团队协作具有启示意义。代码和数据集可在https://github.com/HATS-ICT/x-ego获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让AI系统从团队成员的第一人称视角中获得团队层面的战术态势感知能力。这个问题在现实中很重要，因为真实世界的团队协作（如体育竞技、军事行动、应急响应等）需要参与者能够根据队友和对手的意图来协调行动，而现有方法大多依赖第三人称视角，无法捕捉个体感知和协调的第一人称特性，限制了智能体在部分可观测环境中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有团队行为建模方法的局限性，特别是在处理部分可观测环境中的团队协调时。他们借鉴了体育理解中的第三人称分析方法、游戏理解中的人-AI协作研究以及对比学习在计算机视觉和多智能体学习中的应用。作者选择使用第一人称射击游戏（反恐精英）作为研究平台，因为它提供了丰富的游戏状态和决策复杂性。基于此，他们创建了X-Ego-CS数据集并设计了跨自我中心对比学习（CECL）方法，通过对比学习对齐队友的第一人称视觉表征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习对齐队友的第一人称视觉表征，使模型能够从有限的第一人称视角中推断团队层面的战术态势。整体流程包括：1) 收集和处理专业级反恐精英比赛数据，提取第一人称视频流和状态-动作轨迹；2) 使用时空视频编码器处理每个玩家的视角；3) 应用对比学习目标函数，使同一时间点的队友视角产生相似表征；4) 设计下游任务（队友和对手位置预测）来评估模型性能；5) 结合对比损失和分类损失进行训练，使模型能够从单个视角推断团队态势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) X-Ego-CS数据集：首个包含同步第一人称视频流和结构化状态-动作轨迹的专业电子竞技数据集；2) 跨自我中心对比学习（CECL）方法：通过对比学习对齐队友视角，实现团队态势感知；3) 队友-对手位置预测任务：为评估团队理解能力提供标准化基准。相比之前工作，本文方法使用同步第一人称视角而非第三人称视角，提供完整的第一人称视频流和精确轨迹数据，并通过对比学习模拟人类心智理论能力，在复杂3D环境中验证而非简化环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文贡献了一个包含同步第一人称视频流的专业电子竞技数据集和一个通过对比学习对齐队友视角的方法，使AI系统能够从有限的第一人称视角中获取团队层面的战术态势感知能力，为多智能体系统中的团队协作研究建立了新的基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human team tactics emerge from each player's individual perspective and theirability to anticipate, interpret, and adapt to teammates' intentions. Whileadvances in video understanding have improved the modeling of team interactionsin sports, most existing work relies on third-person broadcast views andoverlooks the synchronous, egocentric nature of multi-agent learning. Weintroduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplayfootage from 45 professional-level matches of the popular e-sports gameCounter-Strike 2, designed to facilitate research on multi-agentdecision-making in complex 3D environments. X-Ego-CS provides cross-egocentricvideo streams that synchronously capture all players' first-person perspectivesalong with state-action trajectories. Building on this resource, we proposeCross-Ego Contrastive Learning (CECL), which aligns teammates' egocentricvisual streams to foster team-level tactical situational awareness from anindividual's perspective. We evaluate CECL on a teammate-opponent locationprediction task, demonstrating its effectiveness in enhancing an agent'sability to infer both teammate and opponent positions from a singlefirst-person view using state-of-the-art video encoders. Together, X-Ego-CS andCECL establish a foundation for cross-egocentric multi-agent benchmarking inesports. More broadly, our work positions gameplay understanding as a testbedfor multi-agent modeling and tactical learning, with implications forspatiotemporal reasoning and human-AI teaming in both virtual and real-worlddomains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.</description>
      <author>example@mail.com (Yunzhe Wang, Soham Hans, Volkan Ustun)</author>
      <guid isPermaLink="false">2510.19150v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.19078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniHPR的统一人体姿态表示学习管道，通过创新的基于奇异值的对比学习损失函数，实现了图像、2D和3D人体姿态表示的有效对齐，并在人体姿态估计和检索任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，开发有效的对齐管道以从不同模态生成统一表示受到越来越多的关注。人体姿态表示作为以人为中心应用的关键组成部分，在人体姿态估计、动作识别、人机交互、目标跟踪等下游任务中至关重要。然而，目前很少有研究使用对比范式清晰研究多种人体姿态表示之间的相关性。&lt;h4&gt;目的&lt;/h4&gt;提出UniHPR，一个统一的人体姿态表示学习管道，用于对齐来自图像、2D和3D人体姿态的人体姿态嵌入。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的基于奇异值的对比学习损失函数，用于同时对齐超过两种数据表示，更好地对齐不同模态并进一步提高性能。选择2D和3D人体姿态估计作为评估任务，以验证对齐表示的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;使用简单的3D人体姿态解码器，UniHPR在Human3.6M数据集上实现了49.9mm的MPJPE性能指标，在3DPW数据集上实现了跨域评估的51.6mm PA-MPJPE性能指标。此外，在Human3.6M数据集上，使用统一的人体姿态表示实现了2D和3D姿态检索，检索误差为9.24mm的MPJPE。&lt;h4&gt;结论&lt;/h4&gt;UniHPR能够有效对齐不同模态的人体姿态表示，并在多种下游任务中展现出优异的性能，为多模态人体姿态表示的学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近年来，人们越来越关注开发有效的对齐管道，从不同模态生成统一表示，用于多模态融合和生成。作为以人为中心应用的重要组成部分，人体姿态表示在许多下游任务中至关重要，如人体姿态估计、动作识别、人机交互、目标跟踪等。人体姿态表示或嵌入可以从图像、2D关键点、3D骨架、网格模型等多种模态中提取。然而，使用对比范式清晰研究所有这些表示之间相关性的实例有限。在本文中，我们提出UniHPR，一个统一的人体姿态表示学习管道，用于对齐来自图像、2D和3D人体姿态的人体姿态嵌入。为了同时对齐超过两种数据表示，我们提出了一种新颖的基于奇异值的对比学习损失函数，更好地对齐不同模态并进一步提高性能。为了评估对齐表示的有效性，我们选择2D和3D人体姿态估计(HPE)作为评估任务。在我们的评估中，使用简单的3D人体姿态解码器，UniHPR在Human3.6M数据集上实现了49.9mm的MPJPE性能指标，在3DPW数据集上实现了跨域评估的51.6mm的PA-MPJPE性能指标。同时，我们能够在Human3.6M数据集上使用统一的人体姿态表示实现2D和3D姿态检索，检索误差为9.24mm的MPJPE。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been a growing interest in developing effectivealignment pipelines to generate unified representations from differentmodalities for multi-modal fusion and generation. As an important component ofHuman-Centric applications, Human Pose representations are critical in manydownstream tasks, such as Human Pose Estimation, Action Recognition,Human-Computer Interaction, Object tracking, etc. Human Pose representations orembeddings can be extracted from images, 2D keypoints, 3D skeletons, meshmodels, and lots of other modalities. Yet, there are limited instances wherethe correlation among all of those representations has been clearly researchedusing a contrastive paradigm. In this paper, we propose UniHPR, a unified HumanPose Representation learning pipeline, which aligns Human Pose embeddings fromimages, 2D and 3D human poses. To align more than two data representations atthe same time, we propose a novel singular value-based contrastive learningloss, which better aligns different modalities and further boosts performance.To evaluate the effectiveness of the aligned representation, we choose 2D and3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, witha simple 3D human pose decoder, UniHPR achieves remarkable performance metrics:MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW datasetwith cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D poseretrieval with our unified human pose representations in Human3.6M dataset,where the retrieval error is 9.24mm in MPJPE.</description>
      <author>example@mail.com (Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang)</author>
      <guid isPermaLink="false">2510.19078v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</title>
      <link>http://arxiv.org/abs/2510.18795v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 fiugres&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ProCLIP框架，解决CLIP文本编码器在处理长文本和多语言输入方面的局限性，通过课程学习实现CLIP图像编码器与LLM嵌入器的有效对齐。&lt;h4&gt;背景&lt;/h4&gt;原始CLIP文本编码器受限于77个token的最大输入长度，不支持多语言输入，这些限制显著阻碍了其在更广泛任务中的应用。虽然近期研究尝试用基于LLM的嵌入器替代CLIP文本编码器，但由于LLM和CLIP的表示空间独立预训练且缺乏先验对齐，直接使用对比学习会破坏CLIP图像编码器中固有的视觉-语言对齐。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来有效对齐CLIP图像编码器与基于LLM的嵌入器，同时保留CLIP的预训练知识，从而增强模型在处理长文本、多语言理解和细粒度语义理解方面的能力。&lt;h4&gt;方法&lt;/h4&gt;ProCLIP采用课程学习的渐进式视觉-语言对齐框架：首先从CLIP文本编码器中蒸馏知识到LLM嵌入器建立初始对齐；然后通过图像-文本对比微调进一步对齐，并使用自蒸馏正则化避免过拟合；在表示继承和对比微调过程中采用实例语义对齐损失和嵌入结构对齐损失以实现更有效的对齐。&lt;h4&gt;主要发现&lt;/h4&gt;直接对齐LLM和CLIP的表示空间会破坏CLIP图像编码器中固有的视觉-语言对齐，导致预训练知识利用不足；而ProCLIP框架能够有效对齐两者并保留CLIP的预训练知识。&lt;h4&gt;结论&lt;/h4&gt;ProCLIP通过课程学习的渐进式对齐方法，解决了LLM嵌入器和CLIP图像编码器之间的对齐问题，同时保留了CLIP的预训练知识，显著提升了模型在长文本处理、多语言理解和细粒度语义理解方面的能力。&lt;h4&gt;翻译&lt;/h4&gt;原始的CLIP文本编码器受限于最大77个token的输入长度，这妨碍了它有效处理长文本和进行细粒度语义理解的能力。此外，CLIP文本编码器不支持多语言输入。所有这些限制显著限制了它在更广泛任务中的应用性。最近的研究尝试用基于LLM的嵌入器替换CLIP文本编码器，以增强其处理长文本、多语言理解和细粒度语义理解的能力。然而，由于LLM的表示空间和CLIP的视觉-语言空间是独立预训练且没有对齐先验，直接使用对比学习对齐会破坏CLIP图像编码器中固有的视觉-语言对齐，导致预训练知识利用不足。为解决这一挑战，我们提出ProCLIP，一种基于课程学习的渐进式视觉-语言对齐框架，以有效对齐CLIP图像编码器和基于LLM的嵌入器。具体而言，ProCLIP首先从CLIP的文本编码器中蒸馏知识到基于LLM的嵌入器，利用CLIP丰富的预训练知识，同时建立LLM嵌入器和CLIP图像编码器之间的初始对齐。随后，ProCLIP通过图像-文本对比微调进一步对齐CLIP图像编码器和基于LLM的嵌入器，采用自蒸馏正则化来避免过拟合。为了实现更有效的对齐，在表示继承和对比微调过程中采用了实例语义对齐损失和嵌入结构对齐损失。代码可在https://github.com/VisionXLab/ProCLIP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The original CLIP text encoder is limited by a maximum input length of 77tokens, which hampers its ability to effectively process long texts and performfine-grained semantic understanding. In addition, the CLIP text encoder lackssupport for multilingual inputs. All these limitations significantly restrictits applicability across a broader range of tasks. Recent studies haveattempted to replace the CLIP text encoder with an LLM-based embedder toenhance its ability in processing long texts, multilingual understanding, andfine-grained semantic comprehension. However, because the representation spacesof LLMs and the vision-language space of CLIP are pretrained independentlywithout alignment priors, direct alignment using contrastive learning candisrupt the intrinsic vision-language alignment in the CLIP image encoder,leading to an underutilization of the knowledge acquired during pre-training.To address this challenge, we propose ProCLIP, a curriculum learning-basedprogressive vision-language alignment framework to effectively align the CLIPimage encoder with an LLM-based embedder. Specifically, ProCLIP first distillsknowledge from CLIP's text encoder into the LLM-based embedder to leverageCLIP's rich pretrained knowledge while establishing initial alignment betweenthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further alignsthe CLIP image encoder with the LLM-based embedder through image-textcontrastive tuning, employing self-distillation regularization to avoidoverfitting. To achieve a more effective alignment, instance semantic alignmentloss and embedding structure alignment loss are employed during representationinheritance and contrastive tuning. The Code is available athttps://github.com/VisionXLab/ProCLIP.</description>
      <author>example@mail.com (Xiaoxing Hu, Kaicheng Yang, Ziyang Gong, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang)</author>
      <guid isPermaLink="false">2510.18795v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</title>
      <link>http://arxiv.org/abs/2510.19592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://www.jshyun.me/projects/decaf&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DecAF的分解注意力融合方法，用于在无需重新训练多模态大语言模型(MLLMs)的情况下实现视频理解与定位，通过改进注意力图实现了与需要训练的方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)能够通过关注与文本查询相关的视觉标记来展示强大的视频理解能力，但直接将其应用于定位任务存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练的方法，将MLLMs的视频理解能力直接适应于视频推理分割任务。&lt;h4&gt;方法&lt;/h4&gt;将视频推理分割视为视频问答任务并通过展开机制提取注意力图；提出DecAF方法，通过对比对象-背景融合和互补视频帧融合两种机制改进原始注意力图；引入注意力引导的SAM2提示获取精细掩码。&lt;h4&gt;主要发现&lt;/h4&gt;DecAF能够抑制不相关的激活并增强对象聚焦的线索，使注意力图可以直接转换为粗略分割掩码；无需训练的方法实现了与需要训练的方法相当的性能。&lt;h4&gt;结论&lt;/h4&gt;DecAF优于现有的无需训练的方法，并在指代和推理VOS基准测试上达到了与基于训练方法相当的性能；与现有的将MLLMs与SAM联合训练的方法不同，DecAF完全无需重新训练。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)通过关注与文本查询相关的视觉标记展示了强大的视频理解能力。为了直接以无需训练的方式将其适应于定位任务，我们将视频推理分割视为视频问答任务，并通过展开机制提取注意力图。然而，原始注意力图嘈杂且与对象区域对齐不良。我们提出了分解注意力融合(DecAF)，通过两种机制改进这些图：(1)对比对象-背景融合和(2)互补视频帧融合。此方法抑制了不相关的激活并增强了对象聚焦的线索，使注意力图可以直接转换为粗略分割掩码。此外，我们引入了注意力引导的SAM2提示来获取精细掩码。与现有的将MLLMs与SAM联合训练的方法不同，我们的方法完全无需重新训练。DecAF优于无需训练的方法，并在指代和推理VOS基准测试上实现了与基于训练方法相当的性能。代码将在https://github.com/HYUNJS/DecAF上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) demonstrate strong videounderstanding by attending to visual tokens relevant to textual queries. Todirectly adapt this for localization in a training-free manner, we cast videoreasoning segmentation as a video QA task and extract attention maps viarollout mechanism. However, raw attention maps are noisy and poorly alignedwith object regions. We propose Decomposed Attention Fusion (DecAF), whichrefines these maps through two mechanisms: (1) contrastive object-backgroundfusion and (2) complementary video-frame fusion. This method suppressesirrelevant activations and enhances object-focused cues, enabling directconversion of attention maps into coarse segmentation masks. In addition, weintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.Unlike existing methods that jointly train MLLMs with SAM, our method operatesentirely without retraining. DecAF outperforms training-free methods andachieves performance comparable to training-based methods on both referring andreasoning VOS benchmarks. The code will be available athttps://github.com/HYUNJS/DecAF.</description>
      <author>example@mail.com (Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim)</author>
      <guid isPermaLink="false">2510.19592v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>A Matter of Time: Revealing the Structure of Time in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.19559v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉语言模型的时间感知能力，提出了TIME10k基准数据集，并发现时间信息在VLM嵌入空间中沿低维非线性流形结构化，基于此提出了时间线表示方法，该方法在计算效率高的同时实现了优异的时间推理性能。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉语言模型如CLIP因其可泛化和表达性的多模态表示而受到欢迎。这些模型通过利用具有多样化文本元数据的大规模训练数据，获得了开放词汇能力，能够解决超出其训练范围的任务。&lt;h4&gt;目的&lt;/h4&gt;研究视觉语言模型的时间感知能力，评估它们将视觉内容定位在时间中的能力。&lt;h4&gt;方法&lt;/h4&gt;引入TIME10k基准数据集（包含超过10,000张图像的时间基准数据），通过一种新方法评估37个VLMs的时间感知能力，并基于发现提出从嵌入空间推导显式'时间线'表示的方法。&lt;h4&gt;主要发现&lt;/h4&gt;时间信息在VLM嵌入空间中沿着低维、非线性的流形结构化，基于此可以推导出显式的'时间线'表示。&lt;h4&gt;结论&lt;/h4&gt;提出的时间线表示方法能够模拟时间及其时间进展，促进时间推理任务，在计算效率高的同时，实现了与基于提示的基线相当或更优的准确性。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉语言模型如CLIP因其可泛化和表达性的多模态表示而受到欢迎。通过利用具有多样化文本元数据的大规模训练数据，VLMs获得了开放词汇能力，能够解决超出其训练范围的任务。本文研究了VLMs的时间感知能力，评估它们将视觉内容定位在时间中的能力。我们引入了TIME10k，一个包含超过10,000张图像的时间基准数据集，并通过一种新方法评估了37个VLMs的时间感知能力。我们的研究揭示，时间信息在VLM嵌入空间中沿着低维、非线性的流形结构化。基于这一见解，我们提出了从嵌入空间推导显式'时间线'表示的方法。这些表示模拟时间及其时间进展，从而促进时间推理任务。我们的时间线方法在计算效率高的同时，实现了与基于提示的基线相当或更优的准确性。所有代码和数据都在https://tekayanidham.github.io/timeline-page/上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758163&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale vision-language models (VLMs) such as CLIP have gained popularityfor their generalizable and expressive multimodal representations. Byleveraging large-scale training data with diverse textual metadata, VLMsacquire open-vocabulary capabilities, solving tasks beyond their trainingscope. This paper investigates the temporal awareness of VLMs, assessing theirability to position visual content in time. We introduce TIME10k, a benchmarkdataset of over 10,000 images with temporal ground truth, and evaluate thetime-awareness of 37 VLMs by a novel methodology. Our investigation revealsthat temporal information is structured along a low-dimensional, non-linearmanifold in the VLM embedding space. Based on this insight, we propose methodsto derive an explicit ``timeline'' representation from the embedding space.These representations model time and its chronological progression and therebyfacilitate temporal reasoning tasks. Our timeline approaches achievecompetitive to superior accuracy compared to a prompt-based baseline whilebeing computationally efficient. All code and data are available athttps://tekayanidham.github.io/timeline-page/.</description>
      <author>example@mail.com (Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer)</author>
      <guid isPermaLink="false">2510.19559v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.19475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 6 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PRGCN的新型框架，通过跨序列模式检索和适应来解决单目3D人体姿态估计中的深度模糊性问题。该方法利用图记忆库存储姿态原型，并通过注意力机制动态检索，结合双流混合架构实现了最先进的性能和跨域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;单目3D人体姿态估计是一个不适定的逆问题，因为从2D到3D的提升中存在固有的深度模糊性。现有基于视频的方法虽然利用时间上下文增强空间推理，但独立处理每个序列，未能充分利用跨序列中人类运动的强结构规律性和重复运动模式。&lt;h4&gt;目的&lt;/h4&gt;解决单目3D人体姿态估计中的深度模糊性问题，突破现有方法仅独立处理每个序列的局限，通过跨序列模式重用机制提升姿态估计的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出PRGCN框架，将姿态估计形式化为模式检索和适应问题；引入图记忆库学习和存储姿态原型；通过注意力机制动态检索提供结构化先验；通过内存驱动的图卷积将先验与解剖约束融合；设计双流混合架构，结合Mamba的局部时间建模和自注意力的全局关系能力。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和MPI-INF-3DHP基准测试上，PRGCN实现了37.1mm和13.4mm的MPJPE，建立了新的最先进水平，同时表现出增强的跨域泛化能力。&lt;h4&gt;结论&lt;/h4&gt;跨序列模式重用机制对推进人体姿态估计领域至关重要，将研究范式从每序列优化转向累积知识学习。&lt;h4&gt;翻译&lt;/h4&gt;单目3D人体姿态估计由于2D到3D提升中的固有深度模糊性，仍然是一个根本性的不适定逆问题。虽然当代基于视频的方法利用时间上下文来增强空间推理，但它们在关键范式限制下运行：独立处理每个序列，因此未能充分利用跨序列中普遍存在的强结构规律性和重复运动模式。这项工作引入了模式重用图卷积网络，一个将姿态估计形式化为模式检索和适应问题的新型框架。其核心是，PRGCN具有一个图记忆库，学习和存储一组紧凑的姿态原型，编码为关系图，这些原型通过注意力机制动态检索以提供结构化先验。这些先验通过内存驱动的图卷积与硬编码的解剖约束自适应融合，确保几何合理性。为了用鲁棒的空间-时间特征支持这一检索过程，我们设计了一个双流混合架构，协同结合了基于Mamba的状态空间模型的线性复杂度局部时间建模与自注意力的全局关系能力。在Human3.6M和MPI-INF-3DHP基准测试上的广泛评估表明，PRGCN建立了新的最先进水平，分别实现了37.1mm和13.4mm的MPJPE，同时表现出增强的跨域泛化能力。我们的研究认为，长期以来被忽视的跨序列模式重用机制对推进该领域至关重要，将范式从每序列优化转向累积知识学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D human pose estimation remains a fundamentally ill-posed inverseproblem due to the inherent depth ambiguity in 2D-to-3D lifting. Whilecontemporary video-based methods leverage temporal context to enhance spatialreasoning, they operate under a critical paradigm limitation: processing eachsequence in isolation, thereby failing to exploit the strong structuralregularities and repetitive motion patterns that pervade human movement acrosssequences. This work introduces the Pattern Reuse Graph Convolutional Network(PRGCN), a novel framework that formalizes pose estimation as a problem ofpattern retrieval and adaptation. At its core, PRGCN features a graph memorybank that learns and stores a compact set of pose prototypes, encoded asrelational graphs, which are dynamically retrieved via an attention mechanismto provide structured priors. These priors are adaptively fused with hard-codedanatomical constraints through a memory-driven graph convolution, ensuringgeometrical plausibility. To underpin this retrieval process with robustspatiotemporal features, we design a dual-stream hybrid architecture thatsynergistically combines the linear-complexity, local temporal modeling ofMamba-based state-space models with the global relational capacity ofself-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarksdemonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPEof 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domaingeneralization capability. Our work posits that the long-overlooked mechanismof cross-sequence pattern reuse is pivotal to advancing the field, shifting theparadigm from per-sequence optimization towards cumulative knowledge learning.</description>
      <author>example@mail.com (Zhuoyang Xie, Yibo Zhao, Hui Huang, Riwei Wang, Zan Gao)</author>
      <guid isPermaLink="false">2510.19475v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>$Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction</title>
      <link>http://arxiv.org/abs/2510.19003v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了Time-Aware Δt-Mamba3D，一种新型的状态空间架构，专门用于纵向医学图像分析。该模型能够有效编码不规则访问间隔和丰富的时空上下文，同时保持计算效率，在乳腺癌风险预测任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;纵向放射图像分析面临一个基本数据挑战：如何有效建模在非规则时间间隔采集的高分辨率图像序列。这种数据结构包含重要的空间和时间线索，但当前方法无法充分利用。现有方法通常要么将空间信息压缩为向量，要么使用计算效率低下且与非均匀时间步不兼容的时空模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理不规则时间间隔采集的图像序列的模型，同时充分利用空间和时间信息，并保持计算效率，应用于纵向医学图像分析，特别是乳腺癌风险预测。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了Time-Aware Δt-Mamba3D，一种专为纵向医学成像设计的新的状态空间架构。该模型的核心创新是一个连续时间选择性扫描机制，明确地将检查之间的真实时间差异整合到状态转换中。此外，还采用了多尺度3D邻域融合模块，稳健地捕获时空关系。&lt;h4&gt;主要发现&lt;/h4&gt;在乳腺癌风险预测基准测试中，该模型表现出色，验证c-index提高了2-5个百分点，相比现有的循环、变压器和状态空间模型的变体，实现了更高的1-5年AUC分数。由于具有线性复杂度，该模型能够高效处理长期复杂的患者筛查历史。&lt;h4&gt;结论&lt;/h4&gt;Time-Aware Δt-Mamba3D为纵向图像分析形成了一个新框架，能够有效处理不规则时间间隔采集的图像序列，充分利用时空信息，同时保持计算效率，在医学图像分析任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;纵向连续放射图像分析受到一个基本数据挑战的阻碍：如何有效建模在非规则时间间隔采集的高分辨率图像序列。这种数据结构包含了当前方法无法充分利用的必不可少的空间和时间线索。模型通常要么将空间信息压缩为向量，要么使用计算效率低下且与非均匀时间步不兼容的时空模型。我们通过Time-Aware Δt-Mamba3D解决了这一挑战，这是一种专为纵向医学成像设计的新型状态空间架构。我们的模型同时编码不规则访问间隔和丰富的时空上下文，同时保持计算效率。其核心创新是一个连续时间选择性扫描机制，明确地将检查之间的真实时间差异整合到其状态转换中。这辅以一个多尺度3D邻域融合模块，稳健地捕获时空关系。在使用连续筛查乳腺X光检查的乳腺癌风险预测综合基准中，我们的模型表现出卓越性能，相比现有的循环、变压器和状态空间模型的变体，将验证c-index提高了2-5个百分点，并实现了更高的1-5年AUC分数。由于其线性复杂度，该模型能够高效处理长期复杂的患者筛查历史，为纵向图像分析形成了一个新框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效建模在不同时间间隔捕获的高分辨率图像序列的问题，特别是在乳腺癌筛查中不规则时间间隔的纵向放射学图像分析。这个问题很重要，因为乳腺癌是全球女性最常见的癌症之一，早期风险预测可以提高筛查效率；现有方法未能充分利用不规则时间间隔这一重要预测因素；医生评估风险时会考虑多次检查的比较，而大多数深度学习系统仍只处理单次检查；开发能够处理不规则时间间隔的高效模型对临床应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到乳腺癌筛查是纵向的，患者会定期返回检查，乳房随年龄变化，病变可能逐渐显现。他们指出大多数深度学习系统忽略了时序背景，现有方法要么将空间信息压缩为向量，要么使用计算效率低下的模型，或者无法处理非均匀时间步长。作者分析了各种处理不规则时间序列的方法，发现它们都有局限性，而状态空间模型如Mamba虽能捕获长期依赖关系，但尚未显式编码不规则时间间隔。作者借鉴了Mamba的状态空间架构、3D卷积网络处理空间信息的思想、时间感知模型处理时间间隔的方法，以及视频视觉 transformers处理时空信息的思路，但都进行了改进以适应不规则时间间隔的医学成像数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 时间感知选择性扫描：将真实时间差Δt直接注入选择性扫描中，使模型能根据实际时间间隔调整状态更新；2) 多尺度3D邻域融合：使用深度3D卷积捕获空间和时间依赖关系，同时保持计算效率；3) 线性复杂度：能高效处理长期复杂的患者筛查历史。整体流程：1) 输入处理：每个患者的纵向成像序列，使用Swin-V2处理每个图像并融合特征；2) Δt-Mamba3D块处理：将特征展平为标记序列，运行Mamba选择性扫描，状态更新由真实Δt调制，然后重塑回3D格式并应用3D邻域融合；3) 患者嵌入和风险模块：跨空间和时间聚合特征获得患者嵌入，使用加性危害模型估计未来乳腺癌风险；4) 处理可变长度序列：左填充序列到固定长度，使用掩码处理填充标记。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 时间感知选择性扫描机制：将真实时间差Δt注入选择性扫描，实现连续时间记忆衰减或累积；2) 多尺度深度3D融合模块：使用深度3D卷积捕获空间和时间依赖关系；3) 线性复杂度设计：能高效处理长期复杂历史。相比之前工作的不同：1) 与时间感知模型(如GRU-D)相比，使用连续时间状态空间模型，能更好地建模观测间的演变风险；2) 与连续时间模型(如Neural ODEs)相比，专为高维医学成像数据设计，处理长时间间隔；3) 与视频视觉transformers相比，明确编码不规则时间间隔，计算效率更高；4) 与视觉状态空间模型相比，引入真实时间间隔信息，结合3D邻域融合；5) 与标准Mamba相比，显式编码真实时间间隔，添加3D邻域融合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Time-Aware Δt-Mamba3D通过将真实时间间隔和多尺度3D空间-时间信息整合到高效的状态空间模型中，显著提高了乳腺癌风险预测的准确性，同时保持了线性计算复杂度，为纵向医学图像分析提供了新框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Longitudinal analysis of sequential radiological images is hampered by afundamental data challenge: how to effectively model a sequence ofhigh-resolution images captured at irregular time intervals. This datastructure contains indispensable spatial and temporal cues that current methodsfail to fully exploit. Models often compromise by either collapsing spatialinformation into vectors or applying spatio-temporal models that arecomputationally inefficient and incompatible with non-uniform time steps. Weaddress this challenge with Time-Aware $\Delta$t-Mamba3D, a novel state-spacearchitecture adapted for longitudinal medical imaging. Our model simultaneouslyencodes irregular inter-visit intervals and rich spatio-temporal context whileremaining computationally efficient. Its core innovation is a continuous-timeselective scanning mechanism that explicitly integrates the true timedifference between exams into its state transitions. This is complemented by amulti-scale 3D neighborhood fusion module that robustly capturesspatio-temporal relationships. In a comprehensive breast cancer risk predictionbenchmark using sequential screening mammogram exams, our model shows superiorperformance, improving the validation c-index by 2-5 percentage points andachieving higher 1-5 year AUC scores compared to established variants ofrecurrent, transformer, and state-space models. Thanks to its linearcomplexity, the model can efficiently process long and complex patientscreening histories of mammograms, forming a new framework for longitudinalimage analysis.</description>
      <author>example@mail.com (Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Shandong Wu)</author>
      <guid isPermaLink="false">2510.19003v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion</title>
      <link>http://arxiv.org/abs/2510.19215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Pattern Recognition&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SFGFusion的新型相机-4D成像雷达检测网络，通过表面拟合引导来解决3D物体检测中的多模态融合问题。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测对自动驾驶至关重要，4D成像雷达作为一种新兴传感器具有低成本、长距离检测和精确速度测量的优势，但其稀疏点云和低分辨率限制了物体的几何表示和跨模态融合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效融合相机和4D成像雷达数据的方法，提高3D物体检测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;SFGFusion通过估计物体的二次曲面参数增强空间表示和跨模态交互，预测细粒度密集深度用于图像特征转换和伪点云生成，采用基于支柱的方法处理雷达点云，并在BEV空间中进行特征融合和检测。&lt;h4&gt;主要发现&lt;/h4&gt;SFGFusion有效融合了相机和4D雷达特征，在TJ4DRadSet和view-of-delft(VoD)物体检测基准上取得了优越性能。&lt;h4&gt;结论&lt;/h4&gt;基于表面拟合的SFGFusion网络能够有效解决4D成像雷达的稀疏性问题，提升多模态融合效果，提高3D物体检测性能。&lt;h4&gt;翻译&lt;/h4&gt;3D物体检测对自动驾驶至关重要。作为一种新兴传感器，4D成像雷达具有低成本、长距离检测和精确速度测量的优势，使其非常适合物体检测。然而，其稀疏点云和低分辨率限制了物体的几何表示并阻碍了多模态融合。在本研究中，我们引入了SFGFusion，一种基于表面拟合引导的新型相机-4D成像雷达检测网络。通过从图像和雷达数据估计物体的二次曲面参数，显式表面拟合模型增强了空间表示和跨模态交互，实现了对细粒度密集深度更可靠的预测。预测的深度有两个用途：1)在图像分支中引导图像特征从透视视图(PV)转换为统一的鸟瞰图(BEV)用于多模态融合，提高空间映射准确性；2)在表面伪点分支中生成密集伪点云，减轻雷达点稀疏性。原始雷达点云也在单独的雷达分支中编码。这两个点云分支采用基于支柱的方法，然后将特征转换为BEV空间。最后，使用标准的2D主干和检测头从BEV特征预测物体标签和边界框。实验结果表明，SFGFusion有效融合了相机和4D雷达特征，在TJ4DRadSet和view-of-delft(VoD)物体检测基准上取得了优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D成像雷达点云稀疏性和低分辨率导致的物体几何表示不足，以及由此带来的多模态融合困难问题。这个问题在现实中非常重要，因为3D物体检测是自动驾驶的核心技术，而4D成像雷达具有成本低、远距离检测和精确测速的优势，能有效弥补相机在深度信息上的不足。解决这一问题可以提升自动驾驶系统的环境感知能力，增强在复杂场景下的检测精度和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多模态3D检测框架的局限性，特别是图像特征从2D到3D转换过程中因雷达点云稀疏导致的几何约束不足问题，提出了表面拟合模型作为解决方案。作者借鉴了基于图像的3D检测中的特征投影方法、基于点云的3D检测中的柱状处理方法(PointPillars)，以及多模态融合中的BEV特征融合技术，但针对4D成像雷达的特点进行了专门优化和创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用表面拟合模型估计物体表面深度，通过结合图像语义和雷达几何信息来增强深度预测精度，然后用这些深度信息指导图像特征转换和生成密集伪点云。整体流程包括：1)表面拟合模型融合图像和雷达信息预测物体深度；2)图像分支在深度指导下将特征从透视视图转换为鸟瞰视图；3)雷达分支处理原始4D雷达点云；4)表面伪点分支利用预测深度生成密集伪点云；5)多分支特征融合后通过检测头输出3D物体检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出表面拟合模型增强跨模态交互和深度估计；2)利用拟合深度指导图像特征视图变换提高空间映射精度；3)生成密集伪点云缓解雷达点云稀疏问题；4)设计针对4D雷达特点的多维特征提取方法。相比之前工作，本文专门针对4D成像雷达而非LiDAR进行优化，解决了雷达点云稀疏不规则带来的挑战，同时结合了图像语义和雷达几何信息的优势，实现了更准确的特征对齐和物体表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SFGFusion通过表面拟合模型有效融合相机和4D成像雷达数据，解决了雷达点云稀疏性导致的几何表示不足问题，显著提升了3D物体检测的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is essential for autonomous driving. As an emergingsensor, 4D imaging radar offers advantages as low cost, long-range detection,and accurate velocity measurement, making it highly suitable for objectdetection. However, its sparse point clouds and low resolution limit objectgeometric representation and hinder multi-modal fusion. In this study, weintroduce SFGFusion, a novel camera-4D imaging radar detection network guidedby surface fitting. By estimating quadratic surface parameters of objects fromimage and radar data, the explicit surface fitting model enhances spatialrepresentation and cross-modal interaction, enabling more reliable predictionof fine-grained dense depth. The predicted depth serves two purposes: 1) in animage branch to guide the transformation of image features from perspectiveview (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improvingspatial mapping accuracy; and 2) in a surface pseudo-point branch to generatedense pseudo-point cloud, mitigating the radar point sparsity. The originalradar point cloud is also encoded in a separate radar branch. These two pointcloud branches adopt a pillar-based method and subsequently transform thefeatures into the BEV space. Finally, a standard 2D backbone and detection headare used to predict object labels and bounding boxes from BEV features.Experimental results show that SFGFusion effectively fuses camera and 4D radarfeatures, achieving superior performance on the TJ4DRadSet and view-of-delft(VoD) object detection benchmarks.</description>
      <author>example@mail.com (Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang)</author>
      <guid isPermaLink="false">2510.19215v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.19661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgentSense是一种混合的、无需训练的框架，通过多智能体进化系统将大型语言模型集成到参与式城市感知中，解决了现有系统在多样化城市场景中泛化能力差和决策解释性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;基于网络的参与式城市感知已成为现代城市管理的重要方法，通过利用移动个体作为分布式传感器收集城市数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应动态城市条件和异构工作者偏好的城市感知系统，同时提供自然语言解释以提高透明度和信任度。&lt;h4&gt;方法&lt;/h4&gt;AgentSense首先使用经典规划器生成基线解决方案，然后通过多智能体进化系统迭代优化这些解决方案，使感知任务分配适应动态变化，并生成自然语言解释。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模移动数据集和七种动态干扰上的实验表明，AgentSense在适应性和可解释性方面明显优于传统方法，且比单智能体LLM基线在性能和鲁棒性方面表现更好。&lt;h4&gt;结论&lt;/h4&gt;AgentSense代表了在网络上部署自适应和可解释的城市感知系统的重要进展，为现代城市管理提供了更有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;基于网络的参与式城市感知已通过利用移动个体作为分布式传感器成为现代城市管理的重要方法。然而，现有的城市感知系统难以在多样化的城市场景中泛化，并且在决策过程中解释性差。在这项工作中，我们介绍了AgentSense，一个混合的、无需训练的框架，通过多智能体进化系统将大型语言模型集成到参与式城市感知中。AgentSense最初使用经典规划器生成基线解决方案，然后迭代优化它们，使感知任务分配适应动态城市条件和异构工作者偏好，同时产生自然语言解释以提高透明度和信任度。在两个大规模移动数据集和七种动态干扰上的大量实验表明，AgentSense在适应性和可解释性方面比传统方法具有明显优势。此外，与单智能体LLM基线相比，我们的方法在性能和鲁棒性方面表现更好，并提供更合理和透明的解释。这些结果表明AgentSense是在网络上部署自适应和可解释的城市感知系统的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Web-based participatory urban sensing has emerged as a vital approach formodern urban management by leveraging mobile individuals as distributedsensors. However, existing urban sensing systems struggle with limitedgeneralization across diverse urban scenarios and poor interpretability indecision-making. In this work, we introduce AgentSense, a hybrid, training-freeframework that integrates large language models (LLMs) into participatory urbansensing through a multi-agent evolution system. AgentSense initially employsclassical planner to generate baseline solutions and then iteratively refinesthem to adapt sensing task assignments to dynamic urban conditions andheterogeneous worker preferences, while producing natural language explanationsthat enhance transparency and trust. Extensive experiments across twolarge-scale mobility datasets and seven types of dynamic disturbancesdemonstrate that AgentSense offers distinct advantages in adaptivity andexplainability over traditional methods. Furthermore, compared to single-agentLLM baselines, our approach outperforms in both performance and robustness,while delivering more reasonable and transparent explanations. These resultsposition AgentSense as a significant advancement towards deploying adaptive andexplainable urban sensing systems on the web.</description>
      <author>example@mail.com (Xusen Guo, Mingxing Peng, Xixuan Hao, Xingchen Zou, Qiongyan Wang, Sijie Ruan, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.19661v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Conditions for Catastrophic Forgetting in Multilingual Translation</title>
      <link>http://arxiv.org/abs/2510.19546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Multilingual Representation Learning (MRL) Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过系统实证研究，探讨了多语言模型微调中的灾难性遗忘现象，发现模型与数据规模比例是遗忘的主要决定因素，指令遵循能力比架构更重要，参数高效微调无明显优势，而跨语言对齐可有效减轻遗忘并促进知识迁移。&lt;h4&gt;背景&lt;/h4&gt;在多语言基础模型上针对特定语言进行微调通常会导致灾难性遗忘，降低在微调中未见语言的性能。虽然这种现象有广泛记录，但文献中关于何时发生遗忘的结果是零散的。&lt;h4&gt;目的&lt;/h4&gt;解决关于灾难性遗忘发生条件的模糊性，进行系统的实证研究，使用机器翻译作为测试平台，以识别多语言微调中触发灾难性遗忘的条件。&lt;h4&gt;方法&lt;/h4&gt;使用机器翻译作为测试平台，进行受控实验，跨越不同的模型架构、数据规模和微调方法。&lt;h4&gt;主要发现&lt;/h4&gt;模型和数据规模之间的相对规模是遗忘的主要决定因素；模型的指令遵循能力对于保留多语言知识比其架构更重要；与假设相反，参数高效微调在减轻遗忘方面没有明显优于完全微调；跨语言对齐可以减轻遗忘，同时促进对未见目标语言的积极迁移。&lt;h4&gt;结论&lt;/h4&gt;跨语言对齐是一种有效的策略，可以减轻灾难性遗忘，并促进知识迁移到未见语言。&lt;h4&gt;翻译&lt;/h4&gt;在多语言基础模型上针对特定语言进行微调通常会导致灾难性遗忘，降低在微调中未见语言的性能。虽然这种现象有广泛记录，但文献中关于何时发生遗忘的结果是零散的。为解决这一模糊性，我们使用机器翻译作为测试平台进行系统实证研究，以识别多语言微调中触发灾难性遗忘的条件。通过跨越不同模型架构、数据规模和微调方法的受控实验，我们揭示了模型与数据规模之间的相对比例是遗忘的主要决定因素。此外，我们证明模型的指令遵循能力对于保留多语言知识比其架构更为关键。与假设相反，参数高效微调在减轻遗忘方面并未显示出比完全微调的明显优势。最后，我们表明跨语言对齐可以减轻遗忘，同时促进对未见目标语言的积极迁移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning multilingual foundation models on specific languages ofteninduces catastrophic forgetting, degrading performance on languages unseen infine-tuning. While this phenomenon is widely-documented, the literaturepresents fragmented results about when forgetting occurs. To address thisambiguity, we conduct a systematic empirical study using machine translation asa testbed to identify the conditions that trigger catastrophic forgetting inmultilingual fine-tuning. Through controlled experiments across different modelarchitectures, data scales, and fine-tuning approaches, we reveal that therelative scale between model and data size is a primary determinant offorgetting. Moreover, we demonstrate that a model's instruction-followingability is more critical for retaining multilingual knowledge than itsarchitecture. Contrary to assumptions, parameter-efficient fine-tuning offersno clear advantage over full fine-tuning in mitigating forgetting. Lastly, weshow that cross-lingual alignment can mitigate forgetting while alsofacilitating positive transfer to unseen target languages.</description>
      <author>example@mail.com (Danni Liu, Jan Niehues)</author>
      <guid isPermaLink="false">2510.19546v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment</title>
      <link>http://arxiv.org/abs/2510.19509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  57 pages (26 main, 25 appendix, 6 references)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的分类法，用于解决语音基础模型评估的匹配问题，通过三个正交轴对现有评估方法进行系统分类，为模型与合适评估方法的匹配提供了原则性框架，并揭示了未来基准设计的优先事项。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型最近在广泛任务中取得了显著能力，但其评估在不同任务和模型类型之间仍然分散，不同模型在语音处理的不同方面表现出色，因此需要不同的评估协议。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的分类法，解决'哪种评估适合哪种模型'的问题，为选择、解释和扩展语音模型评估提供概念基础和实践指南。&lt;h4&gt;方法&lt;/h4&gt;定义三个正交轴：测量的评估方面、尝试任务所需的模型能力、执行任务所需的任务或协议要求，沿着这些轴对现有评估和基准进行分类，涵盖表示学习、语音生成和交互式对话等领域。&lt;h4&gt;主要发现&lt;/h4&gt;通过将每个评估映射到模型展示的能力和方法论需求，该分类法揭示了系统性的差距，如韵律、交互或推理覆盖有限，突显了未来基准设计的优先事项。&lt;h4&gt;结论&lt;/h4&gt;该统一的分类法为模型与合适评估方法的匹配提供了原则性框架，为语音模型评估领域提供了概念基础和实践指导。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型最近在广泛任务中取得了显著能力。然而，它们的评估在不同任务和模型类型之间仍然分散。不同的模型在语音处理的不同方面表现出色，因此需要不同的评估协议。本文提出了一种统一的分类法，解决'哪种评估适合哪种模型'的问题。该分类法定义了三个正交轴：测量的评估方面、尝试任务所需的模型能力、执行任务所需的任务或协议要求。我们沿着这些轴对广泛的现有评估和基准进行分类，涵盖表示学习、语音生成和交互式对话等领域。通过将每个评估映射到模型展示的能力（如语音生成、实时处理）及其方法论需求（如微调数据、人工判断），该分类法为模型与合适评估方法的匹配提供了原则性框架。它还揭示了系统性的差距，如韵律、交互或推理覆盖有限，突显了未来基准设计的优先事项。总体而言，这项工作为选择、解释和扩展语音模型评估提供了概念基础和实践指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models have recently achieved remarkable capabilitiesacross a wide range of tasks. However, their evaluation remains disjointedacross tasks and model types. Different models excel at distinct aspects ofspeech processing and thus require different evaluation protocols. This paperproposes a unified taxonomy that addresses the question: Which evaluation isappropriate for which model? The taxonomy defines three orthogonal axes: the\textbf{evaluation aspect} being measured, the model capabilities required toattempt the task, and the task or protocol requirements needed to perform it.We classify a broad set of existing evaluations and benchmarks along theseaxes, spanning areas such as representation learning, speech generation, andinteractive dialogue. By mapping each evaluation to the capabilities a modelexposes (e.g., speech generation, real-time processing) and to itsmethodological demands (e.g., fine-tuning data, human judgment), the taxonomyprovides a principled framework for aligning models with suitable evaluationmethods. It also reveals systematic gaps, such as limited coverage of prosody,interaction, or reasoning, that highlight priorities for future benchmarkdesign. Overall, this work offers a conceptual foundation and practical guidefor selecting, interpreting, and extending evaluations of speech models.</description>
      <author>example@mail.com (Maureen de Seyssel, Eeshan Gunesh Dhekane)</author>
      <guid isPermaLink="false">2510.19509v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems</title>
      <link>http://arxiv.org/abs/2510.19444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种概率系统的定量抽象统一理论，结合了范畴论、最优传输和定量模态逻辑。核心是一个具有普遍性质的规范ε-商，在所有ε-抽象中最为信息丰富且满足值损失上限。该理论建立了抽象与实现函子之间的伴随关系，揭示了度量结构与逻辑语义的范畴对偶。研究还引入了定量模态μ演算，证明了其在逻辑可表示系统中的表达完整性，并分析了接口细化下的组合性。通过在有限马尔可夫决策过程上的验证，证实了理论的收缩性、值损失界限等性质，为状态聚合和表示学习提供了数学精确的保证。&lt;h4&gt;背景&lt;/h4&gt;概率系统的抽象和近似是计算机科学和人工智能中的重要问题，特别是在处理复杂系统时。现有的抽象方法往往缺乏统一的数学框架，难以保证近似的质量和性质。范畴论提供了描述系统结构和关系的强大工具，最优传输提供了度量概率空间之间距离的方法，而定量模态逻辑则允许对系统的行为进行精确描述。这些理论领域的结合为概率系统的定量抽象提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在构建一个统一的概率系统定量抽象理论，该理论能够：1) 提供一个信息丰富且满足值损失上限的抽象方法；2) 建立抽象与实现之间的数学关系；3) 刻画行为伪度量的性质；4) 开发表达完整的定量模态μ演算；5) 分析系统组合性；6) 为状态聚合和表示学习提供数学保证。&lt;h4&gt;方法&lt;/h4&gt;本研究采用了以下方法：1) 构建具有普遍性质的规范ε-商作为核心抽象机制；2) 应用范畴论建立抽象与实现函子之间的伴随关系；3) 使用贝尔曼风格算子刻画行为伪度量并证明其不动点性质；4) 在余代数框架中证明收缩性和利普希茨性质；5) 引入定量模态μ演算并证明其表达完整性；6) 分析接口细化下的组合性质；7) 在有限马尔可夫决策过程上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;研究的主要发现包括：1) 规范ε-商在所有ε-抽象中是最能保留信息且满足值损失上限的；2) 抽象与实现函子之间存在伴随关系，揭示了度量结构与逻辑语义的范畴对偶；3) 行为伪度量是贝尔曼风格算子的唯一不动点，具有收缩性和利普希茨性质；4) 定量模态μ演算在逻辑可表示系统中具有表达完整性，行为距离与最大逻辑偏差一致；5) 在接口细化下，抽象具有组合性质，能够清晰描述系统边界处的交互；6) 通过实验验证，理论具有收缩性、值损失界限、扰动稳定性、对抗区分性和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的概率系统定量抽象统一理论为复杂系统的抽象和近似提供了坚实的数学基础。该理论通过结合范畴论、最优传输和定量模态逻辑，建立了抽象与实现之间的严格关系，并确保了抽象的质量和性质。定量模态μ演算的表达完整性以及行为距离与逻辑偏差的一致性，为系统分析和验证提供了有力工具。实验验证证明了理论的鲁棒性和计算可行性。该框架不仅为状态聚合和表示学习提供了有原则的目标，还在随机域中为值函数近似提供了数学上精确的保证，对概率系统的建模、分析和应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种概率系统的定量抽象统一理论，该理论将范畴论、最优传输和定量模态逻辑联系起来。其核心是一个具有普遍性质的规范ε-商：在所有ε-抽象中，它是最能保留信息且满足规定值损失上限的。这种构造通过特殊伴随函子定理在抽象函子和实现函子之间诱导了一个伴随关系，揭示了度量结构和逻辑语义之间的范畴对偶。行为伪度量被刻画为贝尔曼风格算子的唯一不动点，并在余代数框架中证明了其收缩性和利普希茨性质。引入了定量模态μ演算并证明其在逻辑可表示系统中具有表达完整性，使得行为距离与最大逻辑偏差一致。分析了在接口细化下的组合性，阐明了抽象如何在系统边界处交互。在有限马尔可夫决策过程上的精确验证套件证实了收缩性、值损失界限、扰动下的稳定性、对抗区分性和可扩展性，展示了鲁棒性和计算可行性。由此产生的框架为状态聚合和表示学习提供了有原则的目标，并在随机域中为值函数近似提供了数学上精确的保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A unified theory of quantitative abstraction is presented for probabilisticsystems that links category theory, optimal transport, and quantitative modallogic. At its core is a canonical $ \varepsilon $-quotient endowed with auniversal property: among all $ \varepsilon $-abstractions, it is the mostinformative one that respects a prescribed bound on value loss. Thisconstruction induces an adjunction between abstraction and realization functors$ (Q_{\varepsilon} \dashv R_{\varepsilon}) $, established via the SpecialAdjoint Functor Theorem, revealing a categorical duality between metricstructure and logical semantics. A behavioral pseudometric is characterized asthe unique fixed point of a Bellman-style operator, with contraction andLipschitz properties proved in a coalgebraic setting. A quantitative modal $\mu $-calculus is introduced and shown to be expressively complete forlogically representable systems, so that behavioral distance coincides withmaximal logical deviation. Compositionality under interface refinement isanalyzed, clarifying how abstractions interact across system boundaries. Anexact validation suite on finite Markov decision processes corroborates thecontraction property, value-loss bounds, stability under perturbation,adversarial distinguishability, and scalability, demonstrating both robustnessand computational feasibility. The resulting framework provides principledtargets for state aggregation and representation learning, with mathematicallyprecise guarantees for value-function approximation in stochastic domains.</description>
      <author>example@mail.com (Nivar Anwer)</author>
      <guid isPermaLink="false">2510.19444v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment</title>
      <link>http://arxiv.org/abs/2510.19384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ADAligner，一个动态、质量感知的图文本对齐框架，解决了现有CLIP风格图文本对齐器在处理多对多关系和适应不同数据质量方面的局限性。ADAligner能够根据监督质量在表达性的多对多和保守的一对一目标之间动态调整，在多个任务上表现出色，并具有更强的鲁棒性和更快的预训练速度。&lt;h4&gt;背景&lt;/h4&gt;在文本属性图上预训练图基础模型对于搜索、推荐和知识发现等网络规模应用至关重要。然而，现有的CLIP风格图文本对齐器面临两个关键限制：假设节点和文本之间存在严格的一对一对应关系，忽略了现实世界图中的固有多对多关系；以及依赖于静态对齐目标，无法适应不同的数据质量，在有噪声监督下变得脆弱。&lt;h4&gt;目的&lt;/h4&gt;解决现有图文本对齐器的局限性，提出一个动态、质量感知的图文本对齐框架，能够根据监督质量在表达性的多对多和保守的一对一目标之间动态调整。&lt;h4&gt;方法&lt;/h4&gt;提出了ADAligner框架，实时估计批次级别的对齐可靠性，并相应地调整优化过程：当监督干净时，促进软的、子图级别的多对多对齐；在噪声下，通过动态过滤低置信度配对来强调可靠的一对一对齐。理论上证明了这种动态机制形成一个稳定的负反馈过程，确保收敛性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在九个不同的TAG数据集上的实验表明，ADAligner在零样本/少样本节点分类、链接预测和跨模态检索任务上一致优于先前的图文本对齐器。在有噪声监督下保持强大的鲁棒性，与多模态基线相比，预训练速度加快约2到3倍。&lt;h4&gt;结论&lt;/h4&gt;ADAligner为现实网络环境中的图文本表示学习建立了一个可扩展和可靠的基础。&lt;h4&gt;翻译&lt;/h4&gt;在文本属性图上预训练图基础模型对于搜索、推荐和知识发现等网络规模应用至关重要。然而，现有的CLIP风格图文本对齐器面临两个关键限制：它们假设节点和文本之间存在严格的一对一对应关系，忽略了现实世界图中的固有多对多关系；并且它们依赖于静态对齐目标，无法适应不同的数据质量，在有噪声监督下变得脆弱。总之，这些限制暴露了一个核心困境：拥抱表达性的多对多对齐会放大噪声，而恢复到严格的一对一策略则会牺牲语义多样性，无法处理本质上不匹配的配对。为了应对这些挑战，我们提出了ADAligner，一个动态、质量感知的图文本对齐框架，根据监督质量在表达性的多对多和保守的一对一目标之间动态调整。ADAligner实时估计批次级别的对齐可靠性，并相应地调整其优化过程，在监督干净时促进软的、子图级别的多对多对齐，同时在噪声下通过动态过滤低置信度配对来强调可靠的一对一对齐。理论上，我们证明这种动态机制形成一个稳定的负反馈过程，确保收敛性和鲁棒性。在九个不同的TAG数据集上的综合实验表明，ADAligner在零样本/少样本节点分类、链接预测和跨模态检索任务上一致地优于先前的图文本对齐器。它在有噪声监督下保持强大的鲁棒性，与多模态基线相比，预训练速度加快约2到3倍，为现实网络环境中的图文本表示学习建立了一个可扩展和可靠的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs)is central to web-scale applications such as search, recommendation, andknowledge discovery. However, existing CLIP-style graph-text aligners face twokey limitations: they assume strict one-to-one correspondences between nodesand texts, overlooking the inherent many-to-many relations in real-worldgraphs; and they rely on static alignment objectives that cannot adapt tovarying data quality, making them brittle under noisy supervision. Together,these limitations expose a core dilemma: embracing expressive many-to-manyalignment amplifies noise, while reverting to strict one-to-one strategiessacrifices semantic diversity and fails to handle inherently mismatched pairs.To address these challenges, we propose ADAligner, a dynamic, quality-awaregraph-text alignment framework that dynamically adjusts between expressivemany-to-many and conservative one-to-one objectives according to supervisionquality. ADAligner estimates batch-level alignment reliability in real time andadapts its optimization accordingly, promoting soft, subgraph-levelmany-to-many alignment when supervision is clean, while emphasizing reliableone-to-one alignment by dynamically filtering low-confidence pairs under noise.Theoretically, we prove that this dynamic mechanism forms a stable negativefeedback process, ensuring convergence and robustness. Comprehensiveexperiments on nine diverse TAG datasets demonstrate that ADAlignerconsistently outperforms prior graph-text aligners on zero-/few-shot nodeclassification, link prediction and cross-modal retrieval tasks. It maintainsstrong robustness under noisy supervision and accelerates pre-training byapproximately 2 to 3 times compared to multimodal baselines, establishing ascalable and reliable foundation for graph-text representation learning inreal-world web environments.</description>
      <author>example@mail.com (Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li)</author>
      <guid isPermaLink="false">2510.19384v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>From Newborn to Impact: Bias-Aware Citation Prediction</title>
      <link>http://arxiv.org/abs/2510.19246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种偏差感知引用预测框架，通过多智能体特征提取和鲁棒图表示学习，解决了新生论文引用预测中的两个关键研究空白，实验证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;引用动态是获取研究影响的关键，支撑研究评估、学术推荐和知识扩散研究。引用预测对新生论文尤为重要，因为在没有引用信号和高度长尾分布的情况下，必须进行早期评估。&lt;h4&gt;目的&lt;/h4&gt;解决两个关键研究空白：一是对科学影响的隐含因素建模不足，导致依赖粗略代理指标；二是缺乏偏差感知学习，无法在低引用论文上提供稳定预测。&lt;h4&gt;方法&lt;/h4&gt;提出偏差感知引用预测框架，结合多智能体特征提取和鲁棒图表示学习。多智能体图共学习模块从元数据和外部资源中提取细粒度可解释信号，并与异构网络嵌入融合；同时采用鲁棒机制，包括两阶段前向过程、GroupDRO优化和正则化头。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的综合实验证明了所提模型的有效性。模型实现了约百分之十三的错误指标降低和百分之五点五的排名指标显著改善。&lt;h4&gt;结论&lt;/h4&gt;提出的偏差感知引用预测框架能够有效解决现有研究空白，提高引用预测的准确性和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;作为获取研究影响的关键，引用动态支撑着研究评估、学术推荐和知识扩散研究。引用预测对新生论文尤为重要，因为在没有引用信号和高度长尾分布的情况下，必须进行早期评估。我们确定了两个关键研究空白：一是对科学影响的隐含因素建模不足，导致依赖粗略代理指标；二是缺乏偏差感知学习，无法在低引用论文上提供稳定预测。我们通过提出偏差感知引用预测框架来解决这些空白，该框架结合了多智能体特征提取和鲁棒图表示学习。首先，多智能体图共学习模块从元数据和外部资源中推导出细粒度、可解释的信号，如可重复性、协作网络和文本质量，并将它们与异构网络嵌入融合，即使在缺乏早期引用信号的情况下也能提供丰富的监督。其次，我们加入了一套鲁棒机制：一个将显性因素通过中间曝光估计路由的两阶段前向过程，用于优化跨环境最坏情况组风险的GroupDRO，以及在单调性和平滑性约束下对可控因素执行假设分析的正则化头。在两个真实世界数据集上的综合实验证明了我们提出的模型的有效性。具体而言，我们的模型在错误指标上实现了约百分之十三的降低，在排名指标上比基线方法有显著的百分之五点五的改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a key to accessing research impact, citation dynamics underpins researchevaluation, scholarly recommendation, and the study of knowledge diffusion.Citation prediction is particularly critical for newborn papers, where earlyassessment must be performed without citation signals and under highlylong-tailed distributions. We identify two key research gaps: (i) insufficientmodeling of implicit factors of scientific impact, leading to reliance oncoarse proxies; and (ii) a lack of bias-aware learning that can deliver stablepredictions on lowly cited papers. We address these gaps by proposing aBias-Aware Citation Prediction Framework, which combines multi-agent featureextraction with robust graph representation learning. First, a multi-agent xgraph co-learning module derives fine-grained, interpretable signals, such asreproducibility, collaboration network, and text quality, from metadata andexternal resources, and fuses them with heterogeneous-network embeddings toprovide rich supervision even in the absence of early citation signals. Second,we incorporate a set of robust mechanisms: a two-stage forward process thatroutes explicit factors through an intermediate exposure estimate, GroupDRO tooptimize worst-case group risk across environments, and a regularization headthat performs what-if analyses on controllable factors under monotonicity andsmoothness constraints. Comprehensive experiments on two real-world datasetsdemonstrate the effectiveness of our proposed model. Specifically, our modelachieves around a 13% reduction in error metrics (MALE and RMSLE) and a notable5.5% improvement in the ranking metric (NDCG) over the baseline methods.</description>
      <author>example@mail.com (Mingfei Lu, Mengjia Wu, Jiawei Xu, Weikai Li, Feng Liu, Ying Ding, Yizhou Sun, Jie Lu, Yi Zhang)</author>
      <guid isPermaLink="false">2510.19246v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence</title>
      <link>http://arxiv.org/abs/2510.19212v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人工智能的理论和方法基础实际上是统计学，而非仅仅来自计算机科学。统计学为机器学习和现代AI提供了不可或缺的基础。&lt;h4&gt;背景&lt;/h4&gt;人工智能的快速发展通常被描述为来自计算机科学和工程的革命，但这种描述掩盖了一个基本事实：AI的理论和方法核心一直是统计学。&lt;h4&gt;目的&lt;/h4&gt;系统性地论证统计学为机器学习和现代AI提供了不可或缺的基础，并呼吁教育、研究和实践重新拥抱这一统计学基础。&lt;h4&gt;方法&lt;/h4&gt;将AI分解为九个基础支柱（推断、密度估计、序列学习、泛化、表示学习、可解释性、因果性、优化和统一），展示每个支柱都建立在百年统计原理之上。&lt;h4&gt;主要发现&lt;/h4&gt;AI的九个基础支柱都建立在统计原理之上；从假设检验和估计的推断框架到聚类和生成式AI的密度估计根源；从启发循环网络的时间序列分析到提供真正理解的因果模型；统计学提供了理论框架、不确定性量化等'大脑'功能，而计算机科学提供了可扩展算法和硬件等'肌肉'功能。&lt;h4&gt;结论&lt;/h4&gt;承认统计学的基础对于开发更强大、可解释和值得信赖的智能系统是必要的步骤。没有统计学习就没有机器学习；没有统计思维就没有人工智能。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的迅速崛起通常被描述为一场源于计算机科学和工程学的革命。然而，这种叙事掩盖了一个基本事实：AI的理论和方法核心，并且一直是，统计学的。本文系统性地论证统计学领域为机器学习和现代AI提供了不可或缺的基础。我们将AI分解为九个基础支柱——推断、密度估计、序列学习、泛化、表示学习、可解释性、因果性、优化和统一——证明每一个都建立在百年统计原理之上。从支撑模型评估的假设检验和估计推断框架，到聚类和生成式AI的密度估计根源；从启发循环网络的时间序列分析到提供真正理解的因果模型，我们追溯了一条不间断的统计谱系。在庆祝推动现代AI的计算引擎的同时，我们认为统计学提供了'大脑'——理论框架、不确定性量化和推断目标——而计算机科学提供了'肌肉'——可扩展算法和硬件。认识到这一统计基础不仅仅是一个学术练习，而是开发更强大、可解释和值得信赖的智能系统的必要步骤。我们呼吁教育、研究和实践重新拥抱这一统计基础。忽视这些根基可能会构建一个脆弱的未来；拥抱它们才是通向真正智能机器的道路。没有统计学习就没有机器学习；没有统计思维就没有人工智能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid ascent of artificial intelligence (AI) is often portrayed as arevolution born from computer science and engineering. This narrative, however,obscures a fundamental truth: the theoretical and methodological core of AI is,and has always been, statistical. This paper systematically argues that thefield of statistics provides the indispensable foundation for machine learningand modern AI. We deconstruct AI into nine foundational pillars-Inference,Density Estimation, Sequential Learning, Generalization, RepresentationLearning, Interpretability, Causality, Optimization, andUnification-demonstrating that each is built upon century-old statisticalprinciples. From the inferential frameworks of hypothesis testing andestimation that underpin model evaluation, to the density estimation roots ofclustering and generative AI; from the time-series analysis inspiring recurrentnetworks to the causal models that promise true understanding, we trace anunbroken statistical lineage. While celebrating the computational engines thatpower modern AI, we contend that statistics provides the brain-the theoreticalframeworks, uncertainty quantification, and inferential goals-while computerscience provides the brawn-the scalable algorithms and hardware. Recognizingthis statistical backbone is not merely an academic exercise, but a necessarystep for developing more robust, interpretable, and trustworthy intelligentsystems. We issue a call to action for education, research, and practice tore-embrace this statistical foundation. Ignoring these roots risks building afragile future; embracing them is the path to truly intelligent machines. Thereis no machine learning without statistical learning; no artificial intelligencewithout statistical thought.</description>
      <author>example@mail.com (Ernest Fokoué)</author>
      <guid isPermaLink="false">2510.19212v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version</title>
      <link>http://arxiv.org/abs/2510.18998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages. An extended version of "An Encode-then-Decompose Approach  to Unsupervised Time Series Anomaly Detection on Contaminated Training Data"  accepted at ICDE 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的编码-分解范式和基于互信息的度量方法，用于时间序列异常检测，提高了对污染时间序列的鲁棒性，并在多个基准测试上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列异常检测在现代大规模系统中至关重要，应用于多个领域分析和监控系统运行。无监督方法因不需要异常标签而受到广泛关注，避免了高成本并具有更广泛的应用。&lt;h4&gt;目的&lt;/h4&gt;解决自动编码器学习到的表示对训练时间序列中的异常敏感导致准确性降低的问题，提高方法在污染数据上的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出编码-分解范式，将编码表示分解为稳定表示和辅助表示；同时提出基于互信息的新度量方法替代重构误差来识别异常。&lt;h4&gt;主要发现&lt;/h4&gt;在八个常用的多变量和单变量时间序列基准测试上展示了具有竞争力或最先进的性能，对不同污染比例的时间序列表现出鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;新方法通过分解编码表示和使用互信息度量，有效提高了时间序列异常检测的准确性和鲁棒性，特别是在训练数据存在异常污染的情况下。&lt;h4&gt;翻译&lt;/h4&gt;时间序列异常检测在现代大规模系统中很重要，并应用于各种领域以分析和监控不同系统的运行。无监督方法引起了广泛关注，因为它们在训练期间不需要异常标签，从而避免了潜在的高成本并具有更广泛的应用。其中，自动编码器受到了广泛关注。它们使用来自压缩表示的重构误差来定义异常分数。然而，自动编码器学习到的表示对训练时间序列中的异常敏感，导致准确性降低。我们提出了一种新颖的编码-分解范式，将编码表示分解为稳定表示和辅助表示，从而在使用污染时间序列进行训练时增强鲁棒性。此外，我们提出了一种基于互信息的新指标来替代重构误差以识别异常。我们的提案在八个常用的多变量和单变量时间序列基准测试上展示了具有竞争力或最先进的性能，并对具有不同污染比例的时间序列表现出鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series anomaly detection is important in modern large-scale systems andis applied in a variety of domains to analyze and monitor the operation ofdiverse systems. Unsupervised approaches have received widespread interest, asthey do not require anomaly labels during training, thus avoiding potentiallyhigh costs and having wider applications. Among these, autoencoders havereceived extensive attention. They use reconstruction errors from compressedrepresentations to define anomaly scores. However, representations learned byautoencoders are sensitive to anomalies in training time series, causingreduced accuracy. We propose a novel encode-then-decompose paradigm, where wedecompose the encoded representation into stable and auxiliary representations,thereby enhancing the robustness when training with contaminated time series.In addition, we propose a novel mutual information based metric to replace thereconstruction errors for identifying anomalies. Our proposal demonstratescompetitive or state-of-the-art performance on eight commonly used multi- andunivariate time series benchmarks and exhibits robustness to time series withdifferent contamination ratios.</description>
      <author>example@mail.com (Buang Zhang, Tung Kieu, Xiangfei Qiu, Chenjuan Guo, Jilin Hu, Aoying Zhou, Christian S. Jensen, Bin Yang)</author>
      <guid isPermaLink="false">2510.18998v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>SBAN: A Framework \&amp; Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining</title>
      <link>http://arxiv.org/abs/2510.18936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为SBAN的大规模多维度数据集，用于推进大型语言模型在软件代码分析方面的预训练和评估。&lt;h4&gt;背景&lt;/h4&gt;软件代码分析领域需要大规模、多模态的数据集来支持大型语言模型的训练和评估，特别是在安全分析和软件理解方面。&lt;h4&gt;目的&lt;/h4&gt;创建一个包含源代码、二进制代码、汇编指令和自然语言描述的多维度数据集，以支持跨表示学习、软件语义理解和自动化恶意软件检测等研究。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含超过300万个样本的数据集，其中包括290万个良性样本和672,000个恶意软件样本，每个样本都通过四个互补层表示：二进制代码、汇编指令、自然语言描述和源代码。&lt;h4&gt;主要发现&lt;/h4&gt;这种独特的多模态结构支持跨表示学习研究，并且可以应用于安全分析、代码翻译、代码解释和其他涉及异构数据的软件挖掘任务。&lt;h4&gt;结论&lt;/h4&gt;SBAN数据集通过桥接低级机器表示和高级人类语义，为构建能够推理代码的智能系统提供了坚实的基础，为挖掘软件行为、改进安全分析和增强大型语言模型在软件代码挖掘方面的能力开辟了新的机会。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了SBAN（源代码、二进制代码、汇编指令和自然语言描述），这是一个大规模、多维度数据集，旨在推进大型语言模型在软件代码分析方面的预训练和评估。SBAN包含超过300万个样本，其中包括290万个良性样本和672,000个恶意软件样本，每个样本都通过四个互补层表示：二进制代码、汇编指令、自然语言描述和源代码。这种独特的多模态结构支持跨表示学习研究、软件语义理解和自动化恶意软件检测。除了安全应用外，SBAN还支持更广泛的任务，如代码翻译、代码解释和其他涉及异构数据的软件挖掘任务。它特别适合深度模型的可扩展训练，包括变压器和其他大型语言模型架构。通过桥接低级机器表示和高级人类语义，SBAN为构建能够推理代码的智能系统提供了坚实的基础。我们相信，这个数据集为挖掘软件行为、改进安全分析和增强大型语言模型在软件代码挖掘的预训练和微调任务方面的能力开辟了新的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SBAN (Source code, Binary, Assembly, and NaturalLanguage Description), a large-scale, multi-dimensional dataset designed toadvance the pre-training and evaluation of large language models (LLMs) forsoftware code analysis. SBAN comprises more than 3 million samples, including2.9 million benign and 672,000 malware respectively, each represented acrossfour complementary layers: binary code, assembly instructions, natural languagedescriptions, and source code. This unique multimodal structure enablesresearch on cross-representation learning, semantic understanding of software,and automated malware detection. Beyond security applications, SBAN supportsbroader tasks such as code translation, code explanation, and other softwaremining tasks involving heterogeneous data. It is particularly suited forscalable training of deep models, including transformers and other LLMarchitectures. By bridging low-level machine representations and high-levelhuman semantics, SBAN provides a robust foundation for building intelligentsystems that reason about code. We believe that this dataset opens newopportunities for mining software behavior, improving security analytics, andenhancing LLM capabilities in pre-training and fine-tuning tasks for softwarecode mining.</description>
      <author>example@mail.com (Hamed Jelodar, Mohammad Meymani, Samita Bai, Roozbeh Razavi-Far, Ali A. Ghorbani)</author>
      <guid isPermaLink="false">2510.18936v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks</title>
      <link>http://arxiv.org/abs/2510.19764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 9 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的灵活框架，用于实现GPU加速的结构可塑性规则，展示了如何训练高效稀疏的脉冲神经网络分类器并学习拓扑图，稀疏模型可比密集模型训练速度快10倍。&lt;h4&gt;背景&lt;/h4&gt;大多数人工神经网络和生物大脑学习研究集中在突触可塑性上，而生物大脑中的结构可塑性（创建和移除连接）对有效学习、损伤恢复和资源优化同样重要。尽管受此启发，机器学习中常使用剪枝移除弱连接，但现有框架针对密集连接优化，无法降低大型模型的训练成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种支持结构可塑性规则的GPU加速框架，用于训练高效稀疏的SNN分类器，并在无监督学习背景下实现拓扑图形成，探索稀疏性的计算优势。&lt;h4&gt;方法&lt;/h4&gt;基于GeNN模拟器，使用e-prop监督学习规则和DEEP R训练稀疏SNN分类器，然后在无监督学习场景中应用该框架学习拓扑图。&lt;h4&gt;主要发现&lt;/h4&gt;稀疏分类器比基准密集模型训练时间减少高达10倍，同时通过DEEP R重连线保持与原始模型相当的性能；在比实时更快的模拟中成功展示了拓扑图形成，提供了连接演变的见解，并测量了模拟速度与网络规模的关系。&lt;h4&gt;结论&lt;/h4&gt;该框架使研究人员能够探索网络结构和神经通信中的稀疏性维持，以及在各种神经形态应用中稀疏性的计算优势。&lt;h4&gt;翻译&lt;/h4&gt;关于人工神经网络训练和生物大脑学习建模的大多数研究都集中在突触可塑性上，其中学习等同于改变现有连接的强度。然而，在生物大脑中，结构可塑性——创建新连接和移除其他连接——同样重要，不仅对有效学习至关重要，还有助于从损伤中恢复和优化资源使用。受结构可塑性启发，剪枝常用于机器学习以从训练好的模型中移除弱连接，从而降低推理的计算需求。然而，通常用于基于反向传播训练ANN和SNN的机器学习框架针对密集连接进行了优化，这意味着剪枝无法帮助降低不断增长的模型的训练成本。GeNN模拟器已经支持稀疏SNN的高效GPU加速模拟，用于计算神经科学和机器学习。在这里，我们提出了一个新的灵活框架，用于实现GPU加速的结构可塑性规则，并首先使用e-prop监督学习规则和DEEP R训练高效稀疏的SNN分类器，然后在无监督学习背景下学习拓扑图。与基准密集模型相比，我们的稀疏分类器将训练时间减少了高达10倍，而DEEP R重连线使它们能够与原始模型一样好地执行。我们在比实时更快的模拟中展示了拓扑图的形成，提供了连接演变的见解，并测量了模拟速度与网络规模的关系。所提出的框架将使进一步研究能够在网络结构和神经通信中实现和保持稀疏性，以及探索稀疏性在各种神经形态应用中的计算优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The majority of research in both training Artificial Neural Networks (ANNs)and modeling learning in biological brains focuses on synaptic plasticity,where learning equates to changing the strength of existing connections.However, in biological brains, structural plasticity - where new connectionsare created and others removed - is also vital, not only for effective learningbut also for recovery from damage and optimal resource usage. Inspired bystructural plasticity, pruning is often used in machine learning to remove weakconnections from trained models to reduce the computational requirements ofinference. However, the machine learning frameworks typically used forbackpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)are optimized for dense connectivity, meaning that pruning does not help reducethe training costs of ever-larger models. The GeNN simulator already supportsefficient GPU-accelerated simulation of sparse SNNs for computationalneuroscience and machine learning. Here, we present a new flexible frameworkfor implementing GPU-accelerated structural plasticity rules and demonstratethis first using the e-prop supervised learning rule and DEEP R to trainefficient, sparse SNN classifiers and then, in an unsupervised learningcontext, to learn topographic maps. Compared to baseline dense models, oursparse classifiers reduce training time by up to 10x while the DEEP R rewiringenables them to perform as well as the original models. We demonstratetopographic map formation in faster-than-realtime simulations, provide insightsinto the connectivity evolution, and measure simulation speed versus networksize. The proposed framework will enable further research into achieving andmaintaining sparsity in network structure and neural communication, as well asexploring the computational benefits of sparsity in a range of neuromorphicapplications.</description>
      <author>example@mail.com (James C. Knight, Johanna Senk, Thomas Nowotny)</author>
      <guid isPermaLink="false">2510.19764v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Study of Training Dynamics for Memory-Constrained Fine-Tuning</title>
      <link>http://arxiv.org/abs/2510.19675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了TraDy，一种内存高效的深度神经网络迁移学习方案，通过动态通道选择和层重要性预判实现严格内存约束下的高性能训练。&lt;h4&gt;背景&lt;/h4&gt;随着深度神经网络模型规模不断增大，而部署环境对资源有严格限制，内存高效训练变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在严格内存约束下实现高效训练的深度神经网络训练方法。&lt;h4&gt;方法&lt;/h4&gt;TraDy利用两个关键洞察：更新的层重要性依赖于架构且可预先确定；动态随机通道选择相比静态方法能提供更好的梯度近似。引入动态通道选择方法，在预选层内周期性地随机重新采样通道。&lt;h4&gt;主要发现&lt;/h4&gt;TraDy在各种下游任务和架构上取得最先进性能；实现高达99%的激活稀疏性；实现95%的权重导数稀疏性；权重导数计算的计算量减少97%。&lt;h4&gt;结论&lt;/h4&gt;TraDy是一种有效的内存高效训练方法，能够在保持性能的同时显著减少内存使用和计算需求。&lt;h4&gt;翻译&lt;/h4&gt;随着模型规模扩大而部署环境施加严格的资源限制，深度神经网络的内存高效训练变得越来越重要。我们提出了TraDy，一种利用两个关键洞察的新型迁移学习方案：更新的层重要性依赖于架构且可预先确定，而动态随机通道选择相比静态方法能提供更好的梯度近似。我们引入了一种动态通道选择方法，在预选层内周期性地随机重新采样通道。大量实验表明，TraDy在各种下游任务和架构上取得了最先进性能，同时保持严格的内存约束，实现了高达99%的激活稀疏性、95%的权重导数稀疏性，以及97%的权重导数计算FLOPs减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Memory-efficient training of deep neural networks has become increasinglyimportant as models grow larger while deployment environments impose strictresource constraints. We propose TraDy, a novel transfer learning schemeleveraging two key insights: layer importance for updates isarchitecture-dependent and determinable a priori, while dynamic stochasticchannel selection provides superior gradient approximation compared to staticapproaches. We introduce a dynamic channel selection approach thatstochastically resamples channels between epochs within preselected layers.Extensive experiments demonstrate TraDy achieves state-of-the-art performanceacross various downstream tasks and architectures while maintaining strictmemory constraints, achieving up to 99% activation sparsity, 95% weightderivative sparsity, and 97% reduction in FLOPs for weight derivativecomputation.</description>
      <author>example@mail.com (Aël Quélennec, Nour Hezbri, Pavlo Mozharovskyi, Van-Tam Nguyen, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2510.19675v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Beyond the Standard Model</title>
      <link>http://arxiv.org/abs/2510.19168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4+8 pages, 7 figures. Accepted at NeurIPS 2025 Workshop: Machine  Learning and the Physical Sciences&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了如何通过迁移学习减少宇宙学模拟成本，使用标准ΛCDM模型预训练，然后在各种超越ΛCDM场景上微调，发现预训练可以显著减少所需模拟数量，但也存在负迁移风险，特别是当参数间存在强物理简并时。&lt;h4&gt;背景&lt;/h4&gt;机器学习能够实现强大的宇宙学推断，但通常需要大量高保真模拟来覆盖多种宇宙学模型，这带来了高昂的计算成本。&lt;h4&gt;目的&lt;/h4&gt;探索通过迁移学习重用不同宇宙学模型间的知识，以减少模拟成本并提高推断效率。&lt;h4&gt;方法&lt;/h4&gt;在标准ΛCDM宇宙学模型上进行预训练，然后在各种超越ΛCDM场景(包括大质量中微子、修正引力、原始非高斯性)上进行微调，并测试包含瓶颈结构的不同迁移架构。&lt;h4&gt;主要发现&lt;/h4&gt;预训练可以在使用显著更少的超越ΛCDM模拟的情况下实现推断；当ΛCDM和超越ΛCDM参数之间存在强物理简并时，可能会发生负迁移；包含瓶颈结构的迁移架构提供了最佳性能。&lt;h4&gt;结论&lt;/h4&gt;预训练可以加速宇宙学推断过程，但也可能阻碍对新物理的学习，基础模型方法在物理学应用中既带来机会也存在潜在陷阱。&lt;h4&gt;翻译&lt;/h4&gt;机器学习能够实现强大的宇宙学推断，但通常需要覆盖多种宇宙学模型的大量高保真模拟。迁移学习提供了一种通过跨模型重用知识来减少模拟成本的方法。我们展示了在标准宇宙学模型ΛCDM上进行预训练，并在各种超越ΛCDM场景(包括大质量中微子、修正引力和原始非高斯性)上进行微调，可以使用显著更少的超越ΛCDM模拟实现推断。然而，我们也表明当ΛCDM和超越ΛCDM参数之间存在强物理简并时，可能会发生负迁移。我们考虑了各种迁移架构，发现包含瓶颈结构的架构提供最佳性能。我们的研究结果阐明了基础模型方法在物理学中的机会和陷阱：预训练可以加速推断，但也可能阻碍对新物理的学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning enables powerful cosmological inference but typicallyrequires many high-fidelity simulations covering many cosmological models.Transfer learning offers a way to reduce the simulation cost by reusingknowledge across models. We show that pre-training on the standard model ofcosmology, $\Lambda$CDM, and fine-tuning on various beyond-$\Lambda$CDMscenarios -- including massive neutrinos, modified gravity, and primordialnon-Gaussianities -- can enable inference with significantly fewerbeyond-$\Lambda$CDM simulations. However, we also show that negative transfercan occur when strong physical degeneracies exist between $\Lambda$CDM andbeyond-$\Lambda$CDM parameters. We consider various transfer architectures,finding that including bottleneck structures provides the best performance. Ourfindings illustrate the opportunities and pitfalls of foundation-modelapproaches in physics: pre-training can accelerate inference, but may alsohinder learning new physics.</description>
      <author>example@mail.com (Veena Krishnaraj, Adrian E. Bayer, Christian Kragh Jespersen, Peter Melchior)</author>
      <guid isPermaLink="false">2510.19168v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.14810v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为结构投影Hebbian表示（SPHeRe）的新型无监督学习方法，通过整合正交性和结构信息保留，解决了传统Hebbian学习在机器学习中的局限性。实验表明，该方法在图像分类、持续学习、迁移学习和图像重建任务中均表现出色，证明了Hebbian无监督学习在现代深度学习框架中的竞争力和潜力。&lt;h4&gt;背景&lt;/h4&gt;Hebbian学习是一种生物学原理，描述了神经元如何通过重复刺激调整其连接。然而，在机器学习中应用时，由于连接更新不受约束且缺乏反馈介导，它存在严重问题，限制了其在复杂网络架构和任务中的有效扩展。&lt;h4&gt;目的&lt;/h4&gt;解决Hebbian学习在机器学习中的局限性，提出一种能够有效扩展到复杂网络架构和任务的无监督学习方法。&lt;h4&gt;方法&lt;/h4&gt;引入结构投影Hebbian表示（SPHeRe），一种新的无监督学习方法，通过局部的辅助非线性块整合正交性和结构信息保留。结构信息保留的损失通过辅助的轻量级投影反向传播到输入，充当反馈介导，正交性约束则考虑了更新幅度的有界性。&lt;h4&gt;主要发现&lt;/h4&gt;SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试上，在无监督突触可塑性方法中达到了最先进的性能。该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Hebbian无监督学习规则在现代深度学习框架中具有竞争力和潜力，展示了不依赖于严格反向传播的高效和生物启发式学习算法的可能性。&lt;h4&gt;翻译&lt;/h4&gt;Hebbian学习是一种生物学原理，直观地描述了神经元如何通过重复刺激来调整其连接。然而，当应用于机器学习时，由于连接更新不受约束且缺乏反馈介导，它存在严重问题。这些缺点限制了其有效扩展到复杂的网络架构和任务。为此，我们在这里引入了结构投影Hebbian表示（SPHeRe），一种新的无监督学习方法，它通过一个局部的辅助非线性块整合了正交性和结构信息保留。结构信息保留的损失通过一个辅助的轻量级投影反向传播到输入，该投影在概念上充当反馈介导，而正交性约束则考虑了更新幅度的有界性。大量的实验结果表明，SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试中，在无监督突触可塑性方法中达到了最先进的性能。此外，该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。这项工作证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖于严格反向传播的高效和生物启发式学习算法的可能性。我们的代码可在https://github.com/brain-intelligence-lab/SPHeRe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hebbian learning is a biological principle that intuitively describes howneurons adapt their connections through repeated stimuli. However, when appliedto machine learning, it suffers serious issues due to the unconstrained updatesof the connections and the lack of accounting for feedback mediation. Suchshortcomings limit its effective scaling to complex network architectures andtasks. To this end, here we introduce the Structural Projection HebbianRepresentation (SPHeRe), a novel unsupervised learning method that integratesorthogonality and structural information preservation through a local auxiliarynonlinear block. The loss for structural information preservationbackpropagates to the input through an auxiliary lightweight projection thatconceptually serves as feedback mediation while the orthogonality constraintsaccount for the boundedness of updating magnitude. Extensive experimentalresults show that SPHeRe achieves SOTA performance among unsupervised synapticplasticity approaches on standard image classification benchmarks, includingCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strongeffectiveness in continual learning and transfer learning scenarios, and imagereconstruction tasks show the robustness and generalizability of the extractedfeatures. This work demonstrates the competitiveness and potential of Hebbianunsupervised learning rules within modern deep learning frameworks,demonstrating the possibility of efficient and biologically inspired learningalgorithms without the strong dependence on strict backpropagation. Our code isavailable at https://github.com/brain-intelligence-lab/SPHeRe.</description>
      <author>example@mail.com (Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu)</author>
      <guid isPermaLink="false">2510.14810v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>$\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual</title>
      <link>http://arxiv.org/abs/2510.18999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为∇-SDF的混合方法，用于从点云数据估计符号距离函数(SDF)，结合了显式先验和隐式神经残差，实现了高效率、高准确性和可微性的SDF重建。&lt;h4&gt;背景&lt;/h4&gt;从点云数据估计符号距离函数(SDF)对机器人自主能力(如定位、建图、运动规划和控制)有很多好处。现有方法中，支持在线和大规模SDF重建的体积方法会影响SDF估计的连续性和可微性；而神经网络方法虽然能提供高保真度和可微的SDF重建，但效率较低，在大环境中可能面临灾难性遗忘和内存限制，且通常仅限于截断的SDF。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现非截断(欧几里得)SDF重建的方法，同时具有体积方法的计算和内存效率以及神经网络方法的可微性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出∇-SDF，一种混合方法，结合了从梯度增强八叉树插值获得的显式先验和隐式神经残差。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，∇-SDF在准确性和效率方面优于现有最先进的技术，为机器人技术和计算机视觉中的下游任务提供了可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;∇-SDF为机器人自主能力中的SDF估计提供了一个高效、准确且可微的解决方案，克服了现有方法的局限性，为下游任务提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从点云数据估计符号距离函数(SDF)已被证明有益于许多机器人自主能力，包括定位、建图、运动规划和控制。支持在线和大规模SDF重建的方法往往依赖于离散的体积数据结构，这会影响SDF估计的连续性和可微性。最近，使用隐式特征的神经网络方法展示了高保真度和可微的SDF重建，但它们往往效率较低，在大环境中可能会经历灾难性遗忘和内存限制，并且通常仅限于截断的SDF。本文提出了∇-SDF，一种混合方法，结合了从梯度增强八叉树插值获得的显式先验和隐式神经残差。我们的方法实现了非截断(欧几里得)的SDF重建，计算和内存效率与体积方法相当，可微性和准确性可与神经网络方法相媲美。大量实验证明，∇-SDF在准确性和效率方面优于最先进的技术，为机器人技术和计算机视觉中的下游任务提供了可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从点云数据在线学习欧几里得符号距离函数（SDF）的问题。这个问题在机器人自主和计算机视觉领域非常重要，因为准确且可微分的几何环境表示对机器人定位、建图、运动规划和控制等关键功能至关重要。快速更新环境模型和获取梯度信息能让机器人更安全、精确地导航和交互环境，而小内存占用对表示在大场景中的可扩展性很重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有SDF重建方法的优缺点：体积方法实时性好但不可微；神经网络方法可微分但效率低且易遗忘；高斯过程方法连续但计算复杂。作者借鉴了H2-Mapping的八叉树先验和神经网络残差思想，以及HIO-SDF的全局SDF表示方法。作者设计了一种混合方法，结合显式八叉树先验和隐式神经残差，使用半稀疏八叉树结构和梯度增强插值提高精度，并设计了三种损失函数加速训练，从而克服了现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合显式八叉树先验和隐式神经残差的混合模型，实现高效、可微分且全局准确的SDF重建。整体流程包括：1)使用半稀疏八叉树存储SDF值和梯度，通过梯度增强插值获得SDF先验；2)使用多分辨率哈希网格编码器和MLP解码器预测SDF残差修正；3)选择关键帧保持训练数据代表性；4)生成表面点、扰动点和自由空间点三种训练样本；5)使用重建损失、Eikonal损失和投影损失训练模型；6)最终SDF预测为八叉树先验与神经网络残差之和。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)梯度增强的八叉树插值方法，在顶点存储SDF值和梯度，提高先验精度；2)半稀疏八叉树结构，平衡内存和精度；3)混合显式-隐式模型，实现全空间而非仅近表面的SDF学习；4)三种精心设计的损失函数加速收敛。相比H2-Mapping，∇-SDF实现非截断SDF重建；相比HIO-SDF，直接优化八叉参数学习更准确先验；相比体积方法，提供可微SDF；相比纯神经网络方法，解决了大环境中的遗忘问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ∇-SDF通过梯度增强的八叉树插值与神经残差相结合，实现了高效、可微分且全局准确的在线符号距离函数重建，结合了体积方法和神经网络方法的优点，同时克服了它们的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimation of signed distance functions (SDFs) from point cloud data has beenshown to benefit many robot autonomy capabilities, including localization,mapping, motion planning, and control. Methods that support online andlarge-scale SDF reconstruction tend to rely on discrete volumetric datastructures, which affect the continuity and differentiability of the SDFestimates. Recently, using implicit features, neural network methods havedemonstrated high-fidelity and differentiable SDF reconstruction but they tendto be less efficient, can experience catastrophic forgetting and memorylimitations in large environments, and are often restricted to truncated SDFs.This work proposes $\nabla$-SDF, a hybrid method that combines an explicitprior obtained from gradient-augmented octree interpolation with an implicitneural residual. Our method achieves non-truncated (Euclidean) SDFreconstruction with computational and memory efficiency comparable tovolumetric methods and differentiability and accuracy comparable to neuralnetwork methods. Extensive experiments demonstrate that \methodname{}outperforms the state of the art in terms of accuracy and efficiency, providinga scalable solution for downstream tasks in robotics and computer vision.</description>
      <author>example@mail.com (Zhirui Dai, Qihao Qian, Tianxing Fan, Nikolay Atanasov)</author>
      <guid isPermaLink="false">2510.18999v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception</title>
      <link>http://arxiv.org/abs/2510.17568v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PAGE-4D是一个扩展到动态场景的前馈模型，解决了现有3D前馈模型在处理动态元素时的局限性，通过动态感知聚合器实现了无需后处理的相机姿态估计、深度预测和点云重建。&lt;h4&gt;背景&lt;/h4&gt;最新的3D前馈模型（如VGGT）在推断静态场景的3D属性方面表现出色，但这些模型通常在静态数据集上训练，因此在涉及移动人类或可变形物体等复杂动态元素的真实场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;引入PAGE-4D模型，将VGGT扩展到动态场景，实现相机姿态估计、深度预测和点云重建，无需后处理。&lt;h4&gt;方法&lt;/h4&gt;提出一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息，对于姿态估计抑制运动线索，对于几何重建则增强这些线索，从而解决多任务4D重建中任务间的固有冲突。&lt;h4&gt;主要发现&lt;/h4&gt;PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越的结果。&lt;h4&gt;结论&lt;/h4&gt;PAGE-4D成功解决了多任务4D重建中任务之间的固有冲突，通过动态感知聚合器有效分离了静态和动态信息，在动态场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D前馈模型，如视觉几何基础变换器（VGGT），在推断静态场景的3D属性方面表现出强大能力。然而，由于它们通常在静态数据集上训练，这些模型在涉及复杂动态元素的真实场景中往往表现不佳，例如移动的人或像伞这样的可变形物体。为解决这一局限性，我们引入了PAGE-4D，一种将VGGT扩展到动态场景的前馈模型，能够实现相机姿态估计、深度预测和点云重建，且无需后处理。多任务4D重建的一个核心挑战是任务之间的内在冲突：准确的相机姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。为解决这一矛盾，我们提出了一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息——在姿态估计中抑制运动线索，而在几何重建中增强它们。大量实验表明，PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在动态场景（包含移动人或可变形物体如伞的场景）中进行准确的3D重建问题。这个问题在现实中非常重要，因为我们的世界本质上是动态的，包含大量移动的物体和人。能够在动态场景中进行准确的3D重建对于机器人导航、增强现实、自动驾驶、视频编辑等多个应用领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到现有的静态3D重建模型在动态场景中表现不佳，尤其是在相机姿态估计和几何重建之间存在冲突：姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。他们借鉴了VGGT作为基础模型，但针对动态场景进行了改进。通过分析VGGT在动态条件下的行为，发现它会忽略动态内容。基于这些观察，作者设计了一个动态感知聚合器，通过预测掩码来分离静态和动态信息，并采用针对性的微调策略，只更新对动态最敏感的中间层。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; PAGE-4D的核心思想是解耦动态区域在不同任务中的作用：在相机姿态估计时抑制动态区域，而在几何重建时利用这些区域的动态信息。整体实现流程包括：1) 使用预训练编码器提取图像特征；2) 通过动态感知聚合器整合空间和时间线索，包括帧间注意、帧内注意和动态感知全局注意；3) 使用轻量级解码器进行深度和点图重建；4) 专门的相机姿态估计解码器。特别的是，PAGE-4D预测一个动态掩码，通过交叉注意机制应用：过滤相机姿态令牌的动态内容，同时为几何令牌强调这些内容。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; PAGE-4D的关键创新点包括：1) 动态感知聚合器，通过预测掩码分离静态和动态信息；2) 针对性的微调策略，只更新对动态最敏感的中间层；3) 任务特定的动态处理，在不同任务中不同方式处理动态区域；4) 统一高效的框架，能在单一前向传递中同时完成多个任务。相比之前的工作，PAGE-4D不需要后处理，运行速度快，在动态场景中表现更好，能产生更密集和准确的点云重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAGE-4D通过解耦姿态和几何估计中的动态信息处理，首次实现了在单一前向模型中对动态场景的高效准确4D感知，显著超越了之前静态模型在动态环境中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent 3D feed-forward models, such as the Visual Geometry GroundedTransformer (VGGT), have shown strong capability in inferring 3D attributes ofstatic scenes. However, since they are typically trained on static datasets,these models often struggle in real-world scenarios involving complex dynamicelements, such as moving humans or deformable objects like umbrellas. Toaddress this limitation, we introduce PAGE-4D, a feedforward model that extendsVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, andpoint cloud reconstruction -- all without post-processing. A central challengein multi-task 4D reconstruction is the inherent conflict between tasks:accurate camera pose estimation requires suppressing dynamic regions, whilegeometry reconstruction requires modeling them. To resolve this tension, wepropose a dynamics-aware aggregator that disentangles static and dynamicinformation by predicting a dynamics-aware mask -- suppressing motion cues forpose estimation while amplifying them for geometry reconstruction. Extensiveexperiments show that PAGE-4D consistently outperforms the original VGGT indynamic scenarios, achieving superior results in camera pose estimation,monocular and video depth estimation, and dense point map reconstruction.</description>
      <author>example@mail.com (Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang)</author>
      <guid isPermaLink="false">2510.17568v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VO-DP的纯视觉单视角扩散策略学习方法，利用预训练视觉基础模型融合语义和几何特征，在模拟和真实世界任务中均表现出色，并开源了机器人操作训练库。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习中，基于视觉运动的扩散策略学习是机器人操作的主要方向。现有方法大多依赖点云作为输入并通过点云特征学习构建场景表示，但对纯视觉解决方案的探索不足。&lt;h4&gt;目的&lt;/h4&gt;探索一种纯视觉且单视角的扩散策略学习方法，以克服对点云输入的依赖，并发挥视觉基础模型在机器人操作中的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出VO-DP方法，利用VGGT的中间特征结合DINOv2的语义特征和交替注意力块的几何特征，通过交叉注意力融合特征，并用CNN空间压缩后输入策略头。同时开源基于Accelerate的机器人操作训练库，支持多GPU并行训练和混合精度训练。&lt;h4&gt;主要发现&lt;/h4&gt;模拟任务中VO-DP成功率达64.6%，与点云方法DP3(64.0%)相当，远高于基线DP(34.8%)；真实世界任务中达到87.9%，显著优于DP3(67.5%)和DP(11.2%)。VO-DP在颜色、尺寸、背景和光照变化条件下保持高度稳定。&lt;h4&gt;结论&lt;/h4&gt;VO-DP证明了纯视觉解决方案在机器人操作中的巨大潜力，特别是在真实世界任务中表现出色。开源的训练库为机器人操作研究提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数方法依赖点云作为观察输入，通过点云特征学习构建场景表示，实现显著准确性。然而，现有文献缺乏对具有巨大潜力的纯视觉解决方案的深入探索。本文提出纯视觉和单视角扩散策略学习方法(VO-DP)，利用预训练视觉基础模型实现语义和几何特征有效融合。使用VGGT中间特征，结合DINOv2语义特征和交替注意力块几何特征。特征通过交叉注意力融合，用CNN空间压缩后输入策略头。大量实验表明，VO-DP不仅显著优于纯视觉基线DP，且与点云方法DP3表现不同：模拟任务中VO-DP平均成功率达64.6%，与DP3的64.0%相当，远高于DP的34.8%；真实世界任务中达87.9%，显著优于DP3的67.5%和DP的11.2%。进一步鲁棒性评估证实VO-DP在颜色、尺寸、背景和光照变化条件下保持高度稳定。最后开源机器人操作训练库，基于Accelerate构建，支持多机器多GPU并行训练和混合精度训练，兼容DP、DP3和VO-DP等视觉运动策略，支持RoboTwin模拟器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作领域中纯视觉（仅RGB图像）模仿学习方法性能不足的问题。这个问题很重要，因为现有基于点云或RGB-D图像的方法虽然精度高，但依赖昂贵的深度传感器，而纯视觉方法成本低、实用性强，但性能通常不如基于点云的方法。探索纯视觉方法的潜力可以显著降低机器人系统的硬件成本和复杂度，避免多传感器校准问题，并更接近生物感知-行动系统，具有广泛的应用前景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点，指出纯视觉方法的性能瓶颈主要在于表示学习模块不完善。他们借鉴了多项现有工作：利用预训练的VGGT模型提取几何信息，使用DINOv2提取语义特征，采用DP中的扩散策略框架，以及Transformer中的cross-attention机制进行特征融合。在此基础上，他们创新设计了语义-几何自适应融合模块和空间特征压缩模块，实现了从单目RGB图像中同时提取和融合语义与几何信息，为下游策略学习提供高质量输入。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的视觉基础模型，从单目RGB图像中同时提取语义和几何特征，并通过自适应融合机制将这些特征有效结合，为下游策略学习提供高质量的输入。整体流程包括：1) 输入处理：接收单视图RGB图像序列；2) 特征提取：使用DINOv2提取语义特征，用VGGT的Alternating Attention网络提取几何特征；3) 特征融合：通过残差交叉注意力机制自适应融合语义和几何特征；4) 场景表示压缩：使用轻量级ResNet压缩融合后的特征，并与机器人关节状态连接形成场景表示；5) 动作生成：基于DDPM的策略头根据场景表示生成动作轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次实现纯视觉方法达到点云级别的性能；2) 创新设计语义-几何自适应融合机制；3) 高效的单视图表示学习方法；4) 开源DRRM训练框架。相比之前的工作：1) 相比传统纯视觉方法（如DP），性能显著提升；2) 相比基于点云的方法（如DP3），不需要昂贵深度传感器，在真实世界任务中表现更好；3) 相比其他纯视觉方法，更注重语义和几何特征的融合，在复杂场景中表现更好，对环境变化具有更强鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VO-DP通过创新性地融合预训练视觉模型的语义和几何特征，首次实现了仅使用RGB图像的机器人操作方法达到与基于点云方法相当的精度，同时大幅降低了硬件成本和系统复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning</title>
      <link>http://arxiv.org/abs/2510.17772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种基于图册的流形学习方法，克服了传统方法在低维嵌入时丢失关键特征的局限性，实现了在潜在流形上的直接机器学习。&lt;h4&gt;背景&lt;/h4&gt;尽管流形假设很流行，但当前流形学习方法主要将数据降维到R^D空间，当嵌入维度D接近流形真实维度d时会丢失关键特征。直接将潜在流形学习为可微图册的方法相对未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;旨在证明基于图册方法的有效性和潜力，为流形学习提供新思路。&lt;h4&gt;方法&lt;/h4&gt;实现了一个通用数据结构来维护可微图册，支持在流形上进行黎曼优化；补充了从点云数据学习可微图册的无监督启发式方法。&lt;h4&gt;主要发现&lt;/h4&gt;在选定的设置中，该方法在效率和准确性方面具有优势；在克莱因瓶上的监督分类任务和造血数据的RNA速度分析中，展示了改进的可解释性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于图册的流形学习方法为直接在潜在流形上进行机器学习提供了有效途径，提高了模型的可解释性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;尽管流形假设很受欢迎，但当前的流形学习方法不支持直接在潜在d维数据流形上进行机器学习，因为它们主要旨在将数据降维到R^D空间，当嵌入维度D接近d时会丢失关键流形特征。另一方面，直接将潜在流形学习为可微图册的方法相对未被充分探索。在本文中，我们旨在证明基于图册方法的有效性和潜力。为此，我们实现了一个通用的数据结构来维护可微图册，使得能够在流形上进行黎曼优化。我们补充了一种无监督启发式方法，从点云数据中学习可微图册。我们在选定的设置中实验证明了这种方法在效率和准确性方面的优势。此外，在克莱因瓶上的监督分类任务和造血数据的RNA速度分析中，我们展示了我们方法在可解释性和鲁棒性方面的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the popularity of the manifold hypothesis, current manifold-learningmethods do not support machine learning directly on the latent $d$-dimensionaldata manifold, as they primarily aim to perform dimensionality reduction into$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$approaches $d$.  On the other hand, methods that directly learn the latent manifold as adifferentiable atlas have been relatively underexplored.  In this paper, we aim to give a proof of concept of the effectiveness andpotential of atlas-based methods. To this end, we implement a generic datastructure to maintain a differentiable atlas that enables Riemannianoptimization over the manifold. We complement this with an unsupervisedheuristic that learns a differentiable atlas from point cloud data. Weexperimentally demonstrate that this approach has advantages in terms ofefficiency and accuracy in selected settings. Moreover, in a supervisedclassification task over the Klein bottle and in RNA velocity analysis ofhematopoietic data, we showcase the improved interpretability and robustness ofour approach.</description>
      <author>example@mail.com (Ryan A. Robinett, Sophia A. Madejski, Kyle Ruark, Samantha J. Riesenfeld, Lorenzo Orecchia)</author>
      <guid isPermaLink="false">2510.17772v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
  <item>
      <title>Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance</title>
      <link>http://arxiv.org/abs/2510.17237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'Pole-Image'的新型规范表示方法，利用电线杆作为高精度锚点来生成周围环境的签名，通过对比学习实现稳健的自定位和可靠的地图维护。&lt;h4&gt;背景&lt;/h4&gt;移动机器人的长期自主性需要稳健的自定位和可靠的地图维护，而传统的基于地标的方法面临高可检测性但低独特性与高独特性但难以稳定检测之间的权衡。&lt;h4&gt;目的&lt;/h4&gt;解决如何描述性地识别独特的'签名'（局部点云）的挑战，通过利用可检测的高精度'锚点'（如电线杆）。&lt;h4&gt;方法&lt;/h4&gt;提出'Pole-Image'作为混合方法，将检测到的类似电线杆的地标及其周围环境表示为以电线杆本身为原点的2D极坐标图像，利用电线杆易于检测的特点实现多样观测数据的自动大规模收集，并应用对比学习学习视角不变且高度判别性的描述符。&lt;h4&gt;主要发现&lt;/h4&gt;电线杆地标'检测'极其容易，使得机器人可以轻松跟踪同一电线杆，实现多样观测数据的自动大规模收集；通过对比学习获得的描述符克服了感知别名问题，实现了稳健的自定位；高精度编码实现了高灵敏度变化检测，有助于地图维护。&lt;h4&gt;结论&lt;/h4&gt;所提出的'Pole-Image'方法和对比学习应用能够有效解决传统地标方法面临的权衡问题，为移动机器人的长期自主性提供了稳健的自定位和可靠的地图维护方案。&lt;h4&gt;翻译&lt;/h4&gt;移动机器人的长期自主性需要稳健的自定位和可靠的地图维护。传统的基于地标的方法面临一个基本权衡：高可检测性但低独特性的地标（如电线杆）与高独特性但难以稳定检测的地标（如局部点云结构）之间的权衡。本文通过利用可检测的高精度'锚点'（如电线杆）来解决描述性识别独特'签名'（局部点云）的挑战。为此，我们提出了一种名为'Pole-Image'的新型规范表示方法，作为混合方法，使用电线杆作为锚点来生成周围3D结构的签名。Pole-Image将检测到的类似电线杆的地标及其周围环境表示为以电线杆本身为原点的2D极坐标图像。这种表示利用了电线杆作为高精度参考点的特性，明确编码了稳定电线杆与可变周围点云之间的'相对几何关系'。电线杆地标的主要优势是'检测'极其容易。这种检测的简便性使得机器人可以轻松跟踪同一电线杆，从而实现多样观测数据（正样本对）的自动大规模收集。这种数据获取可行性使得'对比学习（CL）'得以应用。通过应用CL，模型学会了视角不变且高度判别性的描述符。贡献有两方面：1）描述符克服了感知别名问题，实现了稳健的自定位。2）高精度编码实现了高灵敏度变化检测，有助于地图维护。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决移动机器人在长期自主运行中的鲁棒自定位和可靠地图维护问题。传统地标方法面临一个根本权衡：电线杆等地标容易检测但缺乏独特性，而高区分度的局部点云结构又难以稳定检测。这个问题很重要，因为它关系到机器人在动态环境中能否长期稳定工作，特别是在城市网格状环境中，地标排列高度重复会导致定位模糊，环境变化也会影响系统稳定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统地标方法的局限性，特别是感知别名问题。他们注意到电线杆等垂直结构有独特性质：检测容易精确但个体识别困难。作者思考如何利用电线杆作为'锚点'来描述周围环境的独特'签名'。他们发现电线杆允许检测和识别任务解耦，使跟踪同一电线杆变得容易，从而能自动收集多样化观测数据，使对比学习可行。作者借鉴了LiDAR-Iris的2D表示方法、对比学习与监督学习范式、点云描述符从手工设计到深度学习的演进思路，以及视觉位置识别网络中的特征聚合技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用电线杆作为高精度锚点，将电线杆及其周围环境表示为以电线杆为中心的2D极坐标图像（Pole-Image），然后训练编码器生成具有视角不变性和高区分度的描述符，实现鲁棒自定位和高灵敏度变化检测。整体流程：1）检测系统从LiDAR点云提取电线杆；2）将电线杆及其周围环境转换为Pole-Image表示；3）使用轻量级编码器将2D图像转换为紧凑向量描述符；4）通过对比学习或监督学习训练编码器，区分同一地标的图像和不同地标的图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）提出Pole-Image表示法，使用电线杆作为锚点生成签名；2）利用电线杆易检测性收集多样化数据，使对比学习可行；3）通过高精度编码实现高灵敏度变化检测；4）设计轻量级编码器生成紧凑描述符。不同之处：传统方法使用几何配置或手工设计描述符，本文学习数据驱动描述符；传统方法将地标视为'匿名点'，本文强调个体识别；传统方法难以处理感知别名，本文通过区分性描述符解决；传统方法只关注定位，本文同时解决定位和地图维护；对比学习方法比监督学习表现更好，泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于电线杆锚点的自监督描述符Pole-Image，通过对比学习生成具有高区分度和视角不变性的特征，同时解决了移动机器人在长期自主运行中的鲁棒自定位和高灵敏度地图维护问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term autonomy for mobile robots requires both robust self-localizationand reliable map maintenance. Conventional landmark-based methods face afundamental trade-off between landmarks with high detectability but lowdistinctiveness (e.g., poles) and those with high distinctiveness but difficultstable detection (e.g., local point cloud structures). This work addresses thechallenge of descriptively identifying a unique "signature" (local point cloud)by leveraging a detectable, high-precision "anchor" (like a pole). To solvethis, we propose a novel canonical representation, "Pole-Image," as a hybridmethod that uses poles as anchors to generate signatures from the surrounding3D structure. Pole-Image represents a pole-like landmark and its surroundingenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate imagewith the pole itself as the origin. This representation leverages the pole'snature as a high-precision reference point, explicitly encoding the "relativegeometry" between the stable pole and the variable surrounding point cloud. Thekey advantage of pole landmarks is that "detection" is extremely easy. Thisease of detection allows the robot to easily track the same pole, enabling theautomatic and large-scale collection of diverse observational data (positivepairs). This data acquisition feasibility makes "Contrastive Learning (CL)"applicable. By applying CL, the model learns a viewpoint-invariant and highlydiscriminative descriptor. The contributions are twofold: 1) The descriptorovercomes perceptual aliasing, enabling robust self-localization. 2) Thehigh-precision encoding enables high-sensitivity change detection, contributingto map maintenance.</description>
      <author>example@mail.com (Wuhao Xie, Kanji Tanaka)</author>
      <guid isPermaLink="false">2510.17237v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression</title>
      <link>http://arxiv.org/abs/2510.15337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 4 figures, 2 tables, 1 algorithm; camera-ready version  accepted at NeurIPS 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了迁移学习与最小二范数插值器(MNI)的结合，提出了新颖的两步式Transfer MNI方法，分析了其性能优势和适用条件，并通过实验验证了其鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;迁移学习是现代机器学习的关键组成部分，通过利用多样化数据源增强目标任务性能。同时，过参数化模型如高维线性回归中的MNI因其卓越的泛化能力（良性过拟合）而受到关注。然而，迁移学习与MNI的交集尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;填补迁移学习与MNI研究空白，提出Transfer MNI方法，分析其性能特征，开发数据驱动程序检测信息源，并通过实验验证方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出新颖的两步式Transfer MNI方法，表征其非渐近超额风险，开发数据驱动程序检测信息源，引入整合多个信息性Transfer MNIs的集成方法。&lt;h4&gt;主要发现&lt;/h4&gt;确定了Transfer MNI优于仅目标MNI的条件；揭示了自由协变量转移机制，利用异构数据可在有限成本下获得知识转移收益；有限样本实验证明了方法对模型和数据异质性的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Transfer MNI方法在适当条件下能有效结合迁移学习和MNI的优势，为利用异构数据源提供了一种有效途径，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习是现代机器学习的关键组成部分，通过利用多样化数据源增强目标任务性能。同时，高维线性回归中的最小二范数插值器等过参数化模型因其卓越的泛化能力而受到关注，这种现象被称为良性过拟合。尽管迁移学习和MNI各自都很重要，但它们的交集尚未被充分探索。我们的研究通过提出新颖的两步式Transfer MNI方法并分析其权衡关系来填补这一空白。我们表征了其非渐近超额风险，并确定了它优于仅目标MNI的条件。我们的分析揭示了自由协变量转移机制，在这种机制下，利用异构数据可以在有限成本下获得知识转移的收益。为了实现研究发现，我们开发了数据驱动程序来检测信息源，并引入了整合多个信息性Transfer MNIs的集成方法。有限样本实验证明了我们的方法对模型和数据异质性的鲁棒性，确认了它们的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is a key component of modern machine learning, enhancingthe performance of target tasks by leveraging diverse data sources.Simultaneously, overparameterized models such as the minimum-$\ell_2$-norminterpolator (MNI) in high-dimensional linear regression have garneredsignificant attention for their remarkable generalization capabilities, aproperty known as benign overfitting. Despite their individual importance, theintersection of transfer learning and MNI remains largely unexplored. Ourresearch bridges this gap by proposing a novel two-step Transfer MNI approachand analyzing its trade-offs. We characterize its non-asymptotic excess riskand identify conditions under which it outperforms the target-only MNI. Ouranalysis reveals free-lunch covariate shift regimes, where leveragingheterogeneous data yields the benefit of knowledge transfer at limited cost. Tooperationalize our findings, we develop a data-driven procedure to detectinformative sources and introduce an ensemble method incorporating multipleinformative Transfer MNIs. Finite-sample experiments demonstrate the robustnessof our methods to model and data heterogeneity, confirming their advantage.</description>
      <author>example@mail.com (Yeichan Kim, Ilmun Kim, Seyoung Park)</author>
      <guid isPermaLink="false">2510.15337v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients</title>
      <link>http://arxiv.org/abs/2510.17172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种混合预测框架，整合心电图基础模型与XGBoost分类器，提高了急性心肌梗死后恶性心律失常的预测准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;急性心肌梗死后发生的恶性室性心律失常是住院死亡的主要原因，但早期识别仍是临床挑战。传统风险评分表现有限，而端到端深度学习模型缺乏临床所需的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合预测框架，整合大规模心电图基础模型（ECGFounder）和可解释的XGBoost分类器，以提高预测准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;分析6,634例AMI患者的心电图记录（其中175例发生住院VT/VF），使用ECGFounder模型提取150维诊断概率特征，通过特征选择优化后训练XGBoost分类器，使用AUC和F1-score评估性能，并应用SHAP方法进行可解释性分析。&lt;h4&gt;主要发现&lt;/h4&gt;ECGFounder+XGBoost混合模型AUC达到0.801，优于KNN（0.677）、RNN（0.676）和端到端1D-CNN（0.720）。SHAP分析显示模型识别的关键特征（如'室性早搏'作为风险预测因子，'正常窦性心律'作为保护因素）与临床知识高度一致。&lt;h4&gt;结论&lt;/h4&gt;该混合框架为VT/VF风险预测提供了新范式，验证了基础模型输出作为有效自动化特征工程的用途，为构建可信、可解释的AI临床决策支持系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;急性心肌梗死后发生的恶性室性心律失常是住院死亡的主要原因，但早期识别仍是临床挑战。传统风险评分表现有限，而端到端深度学习模型缺乏临床所需的可解释性。本研究旨在开发一种混合预测框架，整合大规模心电图基础模型与可解释的XGBoost分类器，以提高预测准确性和可解释性。我们分析了6,634例AMI患者的心电图记录，其中175例发生住院VT/VF。使用ECGFounder模型提取150维诊断概率特征，通过特征选择优化后训练XGBoost分类器。模型性能通过AUC和F1-score评估，并使用SHAP方法进行可解释性分析。ECGFounder+XGBoost混合模型AUC达到0.801，优于其他模型。SHAP分析显示模型识别的关键特征与临床知识高度一致。我们得出结论，该混合框架为VT/VF风险预测提供了新范式，验证了基础模型输出作为有效自动化特征工程的用途，为构建可信、可解释的AI临床决策支持系统奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malignant ventricular arrhythmias (VT/VF) following acute myocardialinfarction (AMI) are a major cause of in-hospital death, yet earlyidentification remains a clinical challenge. While traditional risk scores havelimited performance, end-to-end deep learning models often lack theinterpretability needed for clinical trust. This study aimed to develop ahybrid predictive framework that integrates a large-scale electrocardiogram(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier toimprove both accuracy and interpretability. We analyzed 6,634 ECG recordingsfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFoundermodel was used to extract 150-dimensional diagnostic probability features ,which were then refined through feature selection to train the XGBoostclassifier. Model performance was evaluated using AUC and F1-score , and theSHAP method was used for interpretability. The ECGFounder + XGBoost hybridmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed thatmodel-identified key features, such as "premature ventricular complexes" (riskpredictor) and "normal sinus rhythm" (protective factor), were highlyconsistent with clinical knowledge. We conclude that this hybrid frameworkprovides a novel paradigm for VT/VF risk prediction by validating the use offoundation model outputs as effective, automated feature engineering forbuilding trustworthy, explainable AI-based clinical decision support systems.</description>
      <author>example@mail.com (Shun Huang, Wenlu Xing, Shijia Geng, Hailong Wang, Guangkun Nie, Gongzheng Tang, Chenyang He, Shenda Hong)</author>
      <guid isPermaLink="false">2510.17172v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.17289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究社交媒体多方对话环境中的反社会行为(ASB)检测与分析，使用多模态融合方法提升检测效果&lt;h4&gt;背景&lt;/h4&gt;社交媒体上的反社会行为(包括仇恨言论、骚扰和网络欺凌)对平台安全和社会福祉构成风险，而多方对话环境因数据有限研究不足&lt;h4&gt;目的&lt;/h4&gt;填补多方对话环境中反社会行为研究的空白，使用专门数据集评估相关任务的性能&lt;h4&gt;方法&lt;/h4&gt;使用法语开放数据集CyberAgressionAdo-Large，评估虐待检测、欺凌行为分析和欺凌同伴群体识别三个任务，对比六种基于文本和八种基于图的表示学习方法，分析词汇线索、互动动态及其多模态融合&lt;h4&gt;主要发现&lt;/h4&gt;多模态模型优于单模态基线；晚期融合模型mBERT + WD-SGCN表现最佳，在虐待检测(0.718)、同伴群体识别(0.286)和欺凌分析(0.606)方面均有良好成绩；能有效处理隐含攻击、角色转换和上下文相关的敌意等细微反社会行为现象&lt;h4&gt;结论&lt;/h4&gt;多模态融合方法在检测和分析多方对话中的反社会行为方面表现优异，特别是在处理复杂和微妙的反社会行为现象时具有优势&lt;h4&gt;翻译&lt;/h4&gt;社交媒体上的反社会行为(ASB)包括仇恨言论、骚扰和网络欺凌，对平台安全和社会福祉构成日益增长的风险。先前研究主要集中在X和Reddit等网络平台，而多方对话环境由于数据有限而研究不足。为解决这一差距，我们使用CyberAgressionAdo-Large(法语开放访问数据集)模拟多方对话中的ASB，并评估三个任务：虐待检测、欺凌行为分析和欺凌同伴群体识别。我们对比了六种基于文本和八种基于图的表示学习方法，分析词汇线索、互动动态及其多模态融合。结果显示多模态模型优于单模态基线模型。晚期融合模型mBERT + WD-SGCN获得最佳整体结果，在虐待检测方面表现最佳(0.718)，在同伴群体识别(0.286)和欺凌分析(0.606)方面具有竞争力。错误分析显示其在处理细微ASB现象方面的有效性，如隐含攻击、角色转换和上下文相关的敌意。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antisocial behavior (ASB) on social media -- including hate speech,harassment, and cyberbullying -- poses growing risks to platform safety andsocietal well-being. Prior research has focused largely on networks such as Xand Reddit, while \textit{multi-party conversational settings} remainunderexplored due to limited data. To address this gap, we use\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASBin multi-party conversations, and evaluate three tasks: \textit{abusedetection}, \textit{bullying behavior analysis}, and \textit{bullyingpeer-group identification}. We benchmark six text-based and eight graph-based\textit{representation-learning methods}, analyzing lexical cues, interactionaldynamics, and their multimodal fusion. Results show that multimodal modelsoutperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}achieves the best overall results, with top performance on abuse detection(0.718) and competitive scores on peer-group identification (0.286) andbullying analysis (0.606). Error analysis highlights its effectiveness inhandling nuanced ASB phenomena such as implicit aggression, role transitions,and context-dependent hostility.</description>
      <author>example@mail.com (Hajar Bakarou, Mohamed Sinane El Messoussi, Anaïs Ollagnier)</author>
      <guid isPermaLink="false">2510.17289v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding</title>
      <link>http://arxiv.org/abs/2510.17034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为W2R2的新型训练框架，用于解决多模态3D定位中的2D语义偏差问题，通过解耦表示学习和目标性快捷方式抑制，显著提升了定位准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;多模态3D定位在视觉语言模型(VLMs)中受到关注，用于推进复杂环境中的空间推理能力，但现有模型存在严重的'2D语义偏差'问题。&lt;h4&gt;目的&lt;/h4&gt;解决模型过度依赖2D图像特征进行粗略定位而忽略3D几何输入的问题，提高多模态融合性能。&lt;h4&gt;方法&lt;/h4&gt;提出What-Where Representation Re-Forming (W2R2)训练框架，将2D特征作为'What'识别的语义信标，3D特征作为'Where'定位的空间锚点，包含双目标损失函数：对齐损失监督融合预测，伪标签损失惩罚过度有效的2D主导伪输出。&lt;h4&gt;主要发现&lt;/h4&gt;W2R2方法在ScanRefer和ScanQA数据集上表现出色，在定位准确性和鲁棒性方面有显著提升，尤其在杂乱的室外场景中表现突出。&lt;h4&gt;结论&lt;/h4&gt;W2R2能够在不修改推理架构的情况下，通过重新塑造模型内部空间，实现精确的3D定位。&lt;h4&gt;翻译&lt;/h4&gt;多模态3D定位在视觉语言模型(VLMs)中引起了相当大的关注，用于推进复杂环境中的空间推理。然而，这些模型存在严重的'2D语义偏差'问题，这源于过度依赖2D图像特征进行粗略定位， largely忽视了3D几何输入，导致融合性能不佳。在本文中，我们提出了一种名为What-Where Representation Re-Forming (W2R2)的新型训练框架，通过解耦表示学习和目标性快捷方式抑制来解决此问题。我们的方法通过将2D特征指定为'What'识别的语义信标，将3D特征指定为'Where'定位的空间锚点，从根本上重塑了模型的内部空间，从而能够在不修改推理架构的情况下实现精确的3D定位。关键组件包括一个双目标损失函数，其中对齐损失使用自适应交叉熵监督融合预测以实现多模态协同，伪标签损失通过基于边界的机制惩罚过度有效的2D主导伪输出。在ScanRefer和ScanQA上进行的实验证明了W2R2的有效性，在定位准确性和鲁棒性方面有显著提升，尤其是在杂乱的室外场景中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态3D定位任务中的'2D语义偏差'问题，即模型过度依赖2D图像特征进行粗略定位而忽略3D几何输入的问题。这个问题很重要，因为人类生活在3D世界中并使用自然语言与之交互，准确的3D定位对于空间推理在复杂环境(如自动驾驶、机器人导航、增强现实等)中至关重要，而2D偏差限制了模型在这些场景中的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过诊断实验验证了2D语义偏差的存在：行为诊断显示移除3D输入后模型仍能保持较高准确率；表示空间分析(t-SNE可视化)显示融合特征更接近2D特征而非3D特征。作者借鉴了现有工作如VG-LLM(将2D与3D编码器配对)和ULIP(通过共享嵌入空间对齐模态)等思想，但发现它们仍存在2D偏差。基于这些观察，作者设计了W2R2框架，通过'拉-推'训练策略重构表示空间，将2D特征用于语义识别('what')，3D特征用于空间定位('where')。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'What-Where表示重塑'(W2R2)训练框架解耦2D语义和3D几何的表示学习，不修改推理架构只改变训练方式。整体流程包括：1)基线设置，使用多视图图像生成2D语义特征和3D几何特征并融合；2)正式化2D捷径，定义仅使用2D特征的预测路径；3)实施'拉-推'训练，'拉'目标对齐融合输出与真实标签，'推'目标惩罚过好的2D仅解决方案；4)通过总损失函数Ltotal = Lalign + λ Ldeterrence优化模型，使2D特征负责语义识别，3D特征负责空间定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出W2R2训练框架，通过解耦表示学习解决2D语义偏差；2)设计双目标损失函数，包括对齐损失和伪标签损失；3)采用'拉-推'训练策略重构表示空间；4)保持2D语义能力的同时增强3D几何利用。相比之前工作，不同之处在于：不优化特征融合而是重构表示空间；不消除2D语义而是明确分离其职责；通过训练策略而非架构修改解决问题；将评估层面的2D/3D表示解耦延伸到训练过程中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 论文提出了W2R2训练框架，通过解耦2D语义和3D几何的表示学习，有效解决了多模态3D定位中的2D语义偏差问题，显著提升了定位准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal 3D grounding has garnered considerable interest in Vision-LanguageModels (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complexenvironments. However, these models suffer from a severe "2D semantic bias"that arises from over-reliance on 2D image features for coarse localization,largely disregarding 3D geometric inputs and resulting in suboptimal fusionperformance. In this paper, we propose a novel training framework calledWhat-Where Representation Re-Forming (W2R2) to tackle this issue viadisentangled representation learning and targeted shortcut suppression. Ourapproach fundamentally reshapes the model's internal space by designating 2Dfeatures as semantic beacons for "What" identification and 3D features asspatial anchors for "Where" localization, enabling precise 3D grounding withoutmodifying inference architecture. Key components include a dual-objective lossfunction with an Alignment Loss that supervises fused predictions using adaptedcross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizesoverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness ofW2R2, with significant gains in localization accuracy and robustness,particularly in cluttered outdoor scenes.</description>
      <author>example@mail.com (Yutong Zhong)</author>
      <guid isPermaLink="false">2510.17034v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization</title>
      <link>http://arxiv.org/abs/2510.18539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  1st place at the IROS'25 RoboSense Challenge, Track #3: Cross-Sensor  Placement 3D Object Detection&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇技术报告概述了RoboSense 2025:Track 3的顶级解决方案，在各种传感器配置下实现了3D目标检测的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D检测器在使用传统全局特征（即绝对笛卡尔坐标）进行训练时，常常受到'几何捷径'的影响，导致模型主要依赖物体绝对位置而非形状和外观特征，限制了在不同传感器配置下的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来克服3D目标检测中的几何捷径问题，提高模型在不同传感器配置下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用GBlobs（一种局部点云特征描述符）作为网络输入特征，有效绕过几何捷径，迫使网络学习强大的、以物体为中心的表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过GBlobs方法，模型能够学习更加鲁棒的物体中心表示，显著提高了模型泛化能力，在本挑战中展示了卓越性能。&lt;h4&gt;结论&lt;/h4&gt;GBlobs方法成功解决了3D目标检测中的几何捷径问题，使模型能够更好地适应不同的传感器配置，在各种条件下实现优异的3D目标检测性能。&lt;h4&gt;翻译&lt;/h4&gt;这篇技术报告概述了RoboSense 2025:Track 3的顶级解决方案，在各种传感器配置下实现了3D目标检测的最先进性能。我们的提交使用了GBlobs，这是一种局部点云特征描述符，专门设计用于增强模型在不同LiDAR配置上的泛化能力。当前的基于LiDAR的3D检测器在使用传统全局特征（即绝对笛卡尔坐标）进行训练时，常常受到'几何捷径'的影响。这引入了位置偏差，导致模型主要依赖物体的绝对位置，而不是区分形状和外观特征。虽然对领域内数据有效，但这种捷径在遇到不同点分布时（例如由不同传感器放置引起的）会严重限制泛化能力。通过使用GBlobs作为网络输入特征，我们有效地绕过了这种几何捷径，迫使网络学习强大的、以物体为中心的表示。这种方法显著提高了模型的泛化能力，从而在本挑战中展示了卓越的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达3D物体检测模型在不同传感器位置下泛化能力不足的问题。现有模型过度依赖物体的绝对位置信息，当传感器位置改变时，模型性能会大幅下降。这个问题在现实中很重要，因为自动驾驶车辆可能需要使用不同位置配置的激光雷达传感器，如果模型无法适应这种变化，每次更换传感器位置都需要重新训练，限制了系统的灵活性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有检测器的局限性（即'几何捷径'问题）来设计方法。他们发现使用全局坐标导致模型只关注物体位置而非形状特征，因此决定采用局部几何信息。作者借鉴了已有的GBlobs表示方法，将点云局部区域表示为高斯斑点，通过均值和协方差描述局部结构。同时，考虑到激光雷达数据在远距离的稀疏性，设计了双模型方法：一个基于GBlobs处理近距离，一个基于全局坐标处理远距离。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用局部几何特征替代全局坐标，迫使模型学习物体的形状和外观特征而非绝对位置，从而提高泛化能力。实现流程：1）将点云局部区域表示为GBlobs；2）分别训练基于GBlobs的主模型和基于全局坐标的辅助模型；3）推理时对输入应用测试时增强（随机旋转、翻转和缩放）；4）两个模型分别进行推理，然后反转增强并应用非极大值抑制；5）基于30米距离阈值融合结果：近距离用GBlobs模型，远距离用全局坐标模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）将GBlobs作为输入特征解决几何捷径问题；2）设计双模型融合策略处理数据稀疏性；3）应用测试时增强技术提高鲁棒性；4）基于距离的预测融合方法。相比之前工作：传统检测器依赖全局坐标，而本文使用局部几何特征减轻位置偏见；之前工作未专门针对传感器位置变化优化；本文采用双模型而非单一模型适应不同距离的检测需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于局部几何特征GBlobs的3D物体检测方法，通过减轻几何捷径问题和采用双模型融合策略，显著提高了模型在不同传感器位置下的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This technical report outlines the top-ranking solution for RoboSense 2025:Track 3, achieving state-of-the-art performance on 3D object detection undervarious sensor placements. Our submission utilizes GBlobs, a local point cloudfeature descriptor specifically designed to enhance model generalization acrossdiverse LiDAR configurations. Current LiDAR-based 3D detectors often sufferfrom a \enquote{geometric shortcut} when trained on conventional globalfeatures (\ie, absolute Cartesian coordinates). This introduces a position biasthat causes models to primarily rely on absolute object position rather thandistinguishing shape and appearance characteristics. Although effective forin-domain data, this shortcut severely limits generalization when encounteringdifferent point distributions, such as those resulting from varying sensorplacements. By using GBlobs as network input features, we effectivelycircumvent this geometric shortcut, compelling the network to learn robust,object-centric representations. This approach significantly enhances themodel's ability to generalize, resulting in the exceptional performancedemonstrated in this challenge.</description>
      <author>example@mail.com (Dušan Malić, Christian Fruhwirth-Reisinger, Alexander Prutsch, Wei Lin, Samuel Schulter, Horst Possegger)</author>
      <guid isPermaLink="false">2510.18539v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</title>
      <link>http://arxiv.org/abs/2510.18341v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种在自动驾驶闭环仿真中进行真实视角外推的方法，在 ICCV 2025 RealADSim Workshop NVS 赛道中获得第一名。方法采用四阶段流水线，包括数据驱动初始化、几何先验注入、生成先验利用和数据驱动自适应，解决了街道视角外推的核心挑战，在 RealADSim-NVS 基准测试上获得 0.441 的最高分。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶闭环仿真中的真实视角外推是一个重要挑战，当前的 NovelView Synthesis (NVS) 方法在原始轨迹之外常常产生扭曲和不一致的图像。&lt;h4&gt;目的&lt;/h4&gt;提出一个解决方案，解决街道视角外推的核心挑战，并在 RealADSim Workshop NVS 赛道中取得领先成绩。&lt;h4&gt;方法&lt;/h4&gt;引入了一个全面的四阶段流水线：1) 使用数据驱动初始化策略生成稳健的伪激光雷达点云，避免局部最小值；2) 通过建模道路表面引入强几何先验，使用称为 2D-SDF 的新型降维 SDF；3) 利用生成先验为外推视点创建伪真实值，提供辅助监督；4) 使用数据驱动自适应网络移除时间特定伪影。&lt;h4&gt;主要发现&lt;/h4&gt;在 RealADSim-NVS 基准测试上，该方法获得了 0.441 的最终得分，在所有参与者中排名第一。&lt;h4&gt;结论&lt;/h4&gt;该方法成功解决了自动驾驶中视角外推的挑战，通过综合的四阶段流水线实现了高质量的视角外推，在基准测试中取得了最佳成绩。&lt;h4&gt;翻译&lt;/h4&gt;真实的视角外推对自动驾驶的闭环仿真至关重要，然而对于当前的 NovelView Synthesis (NVS) 方法来说，这仍然是一个重大挑战，这些方法通常在原始轨迹之外产生扭曲和不一致的图像。本报告提出了我们的获奖解决方案，在 ICCV 2025 RealADSim Workshop NVS 赛道中荣获第一名。为解决街道视角外推的核心挑战，我们引入了一个全面的四阶段流水线。首先，我们采用数据驱动初始化策略生成稳健的伪激光雷达点云，避免局部最小值。其次，我们通过使用称为 2D-SDF 的新型降维 SDF 对道路表面建模，注入强几何先验。第三，我们利用生成先验为外推视点创建伪真实值，提供辅助监督。最后，数据驱动自适应网络移除时间特定伪影。在 RealADSim-NVS 基准测试上，我们的方法获得了 0.441 的最终得分，在所有参与者中排名第一。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶场景中的街景视角外推问题，即从原始轨迹之外生成新视角的图像。这个问题在现实中很重要，因为高保真的模拟是验证自动驾驶算法的关键技术，而传统模拟器存在领域差距，真实日志回放又无法支持交互式闭环评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性分析街景外推的核心挑战，设计了一个四阶段流程。他们借鉴了3D高斯泼溅(3DGS)和神经辐射场(NeRF)等体积基元方法，但发现这些方法在外推视角下存在几何扭曲问题。同时参考了结构化运动(SfM)、视觉几何变换(VGGT)技术，并引入了生成模型如Difix3D+作为伪真实数据生成器，创新性地提出了2D-SDF来专门处理道路表面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统化的四阶段流程实现鲁棒且几何一致的街景外推。整体流程包括：(1)使用VGGT生成视觉伪激光雷达点云进行3D场景初始化；(2)使用2D-SDF表示道路表面，结合3D高斯泼溅表示地面以上物体；(3)利用扩散模型为外推视角生成伪真实数据，提供对未观察区域的监督；(4)训练时不变自适应网络去除时间特定特征，确保在不同条件下保持一致的渲染结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)鲁棒的无需激光雷达的初始化策略，避免局部最小值问题；(2)创新的2D-SDF表示，强制道路表面为局部平面先验；(3)迭代式伪真实数据框架，利用生成先验为未观察区域提供监督；(4)时不变自适应网络，去除时间特定特征。相比之前的工作，这个方法特别关注外推而非插值，同时结合了几何约束和生成模型的优势，既保证了几何一致性，又提供了视觉真实感。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种系统化的四阶段视觉街景外推方法，通过结合几何先验和生成模型，实现了在自动驾驶场景中从原始轨迹外推到新视角的鲁棒且几何一致的图像合成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Realistic view extrapolation is critical for closed-loop simulation inautonomous driving, yet it remains a significant challenge for current NovelView Synthesis (NVS) methods, which often produce distorted and inconsistentimages beyond the original trajectory. This report presents our winningsolution which ctook first place in the RealADSim Workshop NVS track at ICCV2025. To address the core challenges of street view extrapolation, we introducea comprehensive four-stage pipeline. First, we employ a data-driveninitialization strategy to generate a robust pseudo-LiDAR point cloud, avoidinglocal minima. Second, we inject strong geometric priors by modeling the roadsurface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage agenerative prior to create pseudo ground truth for extrapolated viewpoints,providing auxilary supervision. Finally, a data-driven adaptation networkremoves time-specific artifacts. On the RealADSim-NVS benchmark, our methodachieves a final score of 0.441, ranking first among all participants.</description>
      <author>example@mail.com (Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye)</author>
      <guid isPermaLink="false">2510.18341v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining</title>
      <link>http://arxiv.org/abs/2510.18244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BlendCLIP的多模态预训练框架，用于解决零样本3D物体分类中合成数据与真实数据之间的领域差距问题，实现了在自动驾驶等实际应用中的高效3D物体识别。&lt;h4&gt;背景&lt;/h4&gt;零样本3D物体分类对自动驾驶等实际应用至关重要，但面临合成训练数据与真实世界稀疏、嘈杂的LiDAR扫描数据之间的显著领域差距。仅使用合成数据训练的方法无法泛化到户外场景，而仅使用真实数据训练的方法缺乏语义多样性，难以识别罕见或未见过的物体。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够弥合合成数据与真实数据之间领域差距的方法，实现高效准确的零样本3D物体分类，特别是在自动驾驶等实际应用场景中。&lt;h4&gt;方法&lt;/h4&gt;提出BlendCLIP多模态预训练框架，通过以下步骤实现：1) 从真实世界驾驶数据和人工标注的3D框中生成大规模物体级三元组数据集（点云、图像和文本描述）；2) 采用基于课程的数据混合策略，首先将模型建立在语义丰富的合成CAD数据基础上，然后逐步适应真实世界扫描的特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法具有很高的标签效率：每批次仅引入1.5%的真实世界样本就能将nuScenes基准上的零样本准确率提高27%。最终模型在nuScenes和TruckScenes等户外数据集上实现了最先进的性能，比最佳先前方法提高了19.3%，同时在多样化合成基准上保持强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;有效的领域适应而非大规模真实世界标注是解锁强大开放词汇3D感知的关键。该研究为解决3D物体识别中的领域差距提供了新思路，代码和数据集将在论文被接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;零样本3D物体分类对自动驾驶等实际应用至关重要，但常常受到用于训练的合成数据与真实世界中遇到的稀疏、嘈杂的LiDAR扫描之间的显著领域差距的阻碍。仅使用合成数据训练的方法无法泛化到户外场景，而仅使用真实数据训练的方法缺乏语义多样性，无法识别罕见或未见过的物体。我们引入了BlendCLIP，一个多模态预训练框架，通过战略性地结合两个领域的优势来弥合这一合成到真实的差距。我们首先提出了一种管道，从真实世界驾驶数据和人工标注的3D框中直接生成大规模物体级三元组数据集——包括点云、图像和文本描述。我们的核心贡献是基于课程的数据混合策略，首先将模型建立在语义丰富的合成CAD数据基础上，然后逐步使其适应真实世界扫描的特定特性。实验表明我们的方法具有很高的标签效率：每批次引入仅1.5%的真实世界样本就能将nuScenes基准上的零样本准确率提高27%。因此，我们的最终模型在nuScenes和TruckScenes等具有挑战性的户外数据集上实现了最先进的性能，在nuScenes上比最佳先前方法提高了19.3%，同时在多样化的合成基准上保持强大的泛化能力。我们的研究结果表明，有效的领域适应而非大规模真实世界标注是解锁强大开放词汇3D感知的关键。我们的代码和数据集将在论文被接受后发布在https://github.com/kesu1/BlendCLIP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D物体分类中的合成数据（如CAD模型）和真实世界数据（如LiDAR扫描）之间的域差距问题。这个问题在自动驾驶等现实应用中非常重要，因为系统需要识别各种可能遇到的物体，包括训练中未见过的类别。现有方法要么完全依赖合成数据但无法很好地泛化到真实世界的稀疏、嘈杂环境，要么完全依赖真实数据但缺乏语义多样性来识别罕见物体。同时，真实世界数据标注成本高昂，大规模标注不现实。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法的两条路径各有局限：纯合成数据无法泛化到真实世界，而纯真实数据缺乏语义多样性。因此他们思考如何结合两种数据的优势。设计方法时借鉴了ULIP-2的预训练策略，学习3D编码器并与CLIP嵌入对齐；采用了课程学习思想，从简单到复杂逐步引入数据；参考了多模态表示学习，使用点云-图像-文本三元组进行训练。核心思路是先让模型在语义丰富的合成数据上建立基础，然后逐步适应真实世界数据特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用课程学习策略，先在语义丰富的合成CAD数据上建立基础，然后逐步适应真实世界LiDAR数据的特性，通过战略性地结合两个领域优势来弥合域差距。整体流程包括：1)数据准备，使用合成数据集和从真实数据集中构建的三元组；2)三元组生成，通过多扫描融合和运动补偿提取密集点云，投影边界框获取图像，用视觉-语言模型生成描述；3)课程训练，先仅用合成数据训练，再逐步引入真实世界数据；4)评估，在合成和真实数据集上评估零样本分类性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的课程学习数据混合策略；2)构建大规模户外三元组数据集；3)展示只需少量真实世界样本就能显著提高性能；4)在多个数据集上实现最先进的零样本分类。相比之前工作的不同：与纯合成数据方法相比能更好泛化到真实世界；与纯真实数据方法相比保留更好的语义多样性；相比简单数据混合策略避免了模型过度拟合到域标识符；相比需要大量标注的方法标注效率高，只需少量真实样本。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BlendCLIP通过课程学习策略战略性地结合合成数据和真实世界数据，以最小的标注成本实现了强大的零样本3D物体分类，显著缩小了合成到真实世界的域差距。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot 3D object classification is crucial for real-world applicationslike autonomous driving, however it is often hindered by a significant domaingap between the synthetic data used for training and the sparse, noisy LiDARscans encountered in the real-world. Current methods trained solely onsynthetic data fail to generalize to outdoor scenes, while those trained onlyon real data lack the semantic diversity to recognize rare or unseen objects.  We introduce BlendCLIP, a multimodal pretraining framework that bridges thissynthetic-to-real gap by strategically combining the strengths of both domains.We first propose a pipeline to generate a large-scale dataset of object-leveltriplets -- consisting of a point cloud, image, and text description -- mineddirectly from real-world driving data and human annotated 3D boxes. Our corecontribution is a curriculum-based data mixing strategy that first grounds themodel in the semantically rich synthetic CAD data before progressively adaptingit to the specific characteristics of real-world scans.  Our experiments show that our approach is highly label-efficient: introducingas few as 1.5\% real-world samples per batch into training boosts zero-shotaccuracy on the nuScenes benchmark by 27\%. Consequently, our final modelachieves state-of-the-art performance on challenging outdoor datasets likenuScenes and TruckScenes, improving over the best prior method by 19.3\% onnuScenes, while maintaining strong generalization on diverse syntheticbenchmarks. Our findings demonstrate that effective domain adaptation, notfull-scale real-world annotation, is the key to unlocking robustopen-vocabulary 3D perception. Our code and dataset will be released uponacceptance on https://github.com/kesu1/BlendCLIP.</description>
      <author>example@mail.com (Ajinkya Khoche, Gergő László Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt)</author>
      <guid isPermaLink="false">2510.18244v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Generalizable Light Transport 3D Embedding for Global Illumination</title>
      <link>http://arxiv.org/abs/2510.18189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种可泛化的3D光传输嵌入方法，直接从3D场景配置近似全局光照，无需使用光栅化或路径追踪的线索。&lt;h4&gt;背景&lt;/h4&gt;全局光照对真实感渲染至关重要，但由于模拟间接光传输的复杂性，计算成本很高。现有神经方法主要依赖场景优化，跨场景泛化努力停留在2D屏幕空间，存在视图不一致性和有限空间理解问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个可泛化的3D光传输嵌入方法，直接从3D场景配置近似全局光照，不使用光栅化或路径追踪的线索。&lt;h4&gt;方法&lt;/h4&gt;将场景表示为具有几何和材质特征点云；使用可扩展transformer模型对全局点与点交互建模，编码为神经基元；渲染时通过最近邻搜索检索附近基元，通过交叉注意力聚合潜在特征预测渲染量。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化室内场景中实现了漫反射全局光照预测；训练的嵌入可通过有限微调快速适应新渲染任务；展示了有光泽材质的空间-方向辐射场估计初步结果；归一化场可加速无偏路径引导。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了将学习先验整合到渲染管道中的路径，无需显式光线追踪光照线索。&lt;h4&gt;翻译&lt;/h4&gt;全局光照(GI)对于真实感渲染至关重要，但由于模拟间接光传输的复杂性，计算成本仍然很高。最近的神经方法主要依赖于场景优化，有时扩展到处理相机或几何变化。跨场景泛化的努力主要停留在2D屏幕空间，如神经去噪或基于G缓冲区的GI预测，这些方法常常存在视图不一致性和有限的空间理解问题。我们提出了一种可泛化的3D光传输嵌入，直接从3D场景配置近似全局光照，不使用光栅化或路径追踪的线索。每个场景表示为具有几何和材质特征点云。可扩展的transformer模型对全局点与点之间的交互进行建模，将这些特征编码为神经基元。渲染时，每个查询点通过最近邻搜索检索附近基元，并通过交叉注意力聚合它们的潜在特征，以预测所需的渲染量。我们在不同布局、几何形状和材质的多样化室内场景中展示了漫反射全局光照预测的结果。为辐照度估计训练的嵌入可以通过有限的微调快速适应新的渲染任务。我们还展示了针对有光泽材质的空间-方向辐射场估计的初步结果，并展示了归一化场如何加速无偏路径引导。这种方法展示了将学习先验整合到渲染管道中的路径，而无需显式的光线追踪光照线索。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决全局照明的通用化问题。当前的全局照明方法计算成本高昂，而现有神经方法通常针对单个场景优化，缺乏跨场景泛化能力。这个问题在游戏开发、电影制作、虚拟现实和建筑设计等领域至关重要，因为这些领域需要高质量的全局照明效果，但又面临实时渲染和资源限制的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到光传输算子和注意力机制之间的相似性，启发他们使用transformer架构来模拟光传输。他们意识到传统方法在处理复杂场景时的局限性，特别是视角一致性和泛化能力问题。他们借鉴了transformer架构在处理长距离依赖关系上的成功、PointTransformerV3作为点云编码的基础架构、传统的光传输方程和渲染方程，以及irradiance caching等传统技术的思想，但用神经网络方法进行了改进和创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D点云作为场景的中间表示，利用transformer架构编码场景点之间的长距离交互来模拟光传输，并设计可学习的局部查询解码器通过注意力机制聚合邻近点特征。整体流程包括：将3D场景转换为点云表示；使用transformer编码器处理场景点生成光传输嵌入；对于渲染时的查询点，通过k近邻搜索获取邻近场景点嵌入；使用基于交叉注意力的解码器聚合特征并预测渲染量；使用路径追踪的地面真实值进行端到端训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出通用的3D光传输嵌入方法直接从3D场景配置近似全局照明；使用点云作为中间表示避免屏幕空间限制；设计可扩展的transformer编码器捕获长距离交互；提出基于交叉注意力的局部查询解码器实现自适应特征聚合；展示如何重用预训练编码器适应不同渲染任务。与之前工作不同：相比单场景优化的神经方法，实现了跨场景泛化；相比2D屏幕空间方法，保持了多视角一致性；相比传统预计算方法，学习通用嵌入可跨场景重用；针对光传输模拟专门优化了点云处理方法；在3D空间直接预测入射辐射场，而非在2D渲染图像上训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于transformer的通用3D光传输嵌入方法，可以直接从场景几何、材质和光源配置中学习并泛化到复杂室内场景的全局照明效果，无需针对每个场景进行优化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global illumination (GI) is essential for realistic rendering but remainscomputationally expensive due to the complexity of simulating indirect lighttransport. Recent neural methods have mainly relied on per-scene optimization,sometimes extended to handle changes in camera or geometry. Efforts towardcross-scene generalization have largely stayed in 2D screen space, such asneural denoising or G-buffer based GI prediction, which often suffer from viewinconsistency and limited spatial understanding. We propose a generalizable 3Dlight transport embedding that approximates global illumination directly from3D scene configurations, without using rasterized or path-traced cues. Eachscene is represented as a point cloud with geometric and material features. Ascalable transformer models global point-to-point interactions to encode thesefeatures into neural primitives. At render time, each query point retrievesnearby primitives via nearest-neighbor search and aggregates their latentfeatures through cross-attention to predict the desired rendering quantity. Wedemonstrate results on diffuse global illumination prediction across diverseindoor scenes with varying layouts, geometry, and materials. The embeddingtrained for irradiance estimation can be quickly adapted to new rendering taskswith limited fine-tuning. We also present preliminary results forspatial-directional radiance field estimation for glossy materials and show howthe normalized field can accelerate unbiased path guiding. This approachhighlights a path toward integrating learned priors into rendering pipelineswithout explicit ray-traced illumination cues.</description>
      <author>example@mail.com (Bing Xu, Mukund Varma T, Cheng Wang, Tzumao Li, Lifan Wu, Bartlomiej Wronski, Ravi Ramamoorthi, Marco Salvi)</author>
      <guid isPermaLink="false">2510.18189v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields</title>
      <link>http://arxiv.org/abs/2510.18122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为HyperDiffusionFields (HyDiF)的框架，将3D分子构象建模为连续场而非离散原子坐标或图，使用分子方向场和分子神经场表示，并通过超网络和去噪扩散模型实现生成能力。&lt;h4&gt;背景&lt;/h4&gt;传统分子建模方法使用离散原子坐标或图表示，而本文提出了一种基于连续场的建模方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将3D分子构象表示为连续场的框架，支持分子生成和性质预测任务。&lt;h4&gt;方法&lt;/h4&gt;使用分子方向场(MDF)将空间中的任何点映射到特定类型最近原子的方向；通过分子特定的神经隐式场(MNF)表示MDF；采用超网络条件化为分子生成MNF权重；将超网络训练为去噪扩散模型以实现生成能力；扩展到掩码扩散机制支持结构条件生成任务。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够支持空间细粒度特征提取，这是基于图或点云的方法难以实现的；该方法可扩展到更大的生物分子。&lt;h4&gt;结论&lt;/h4&gt;基于场的分子建模是一个有前途的方向，能够有效处理分子生成和性质预测任务。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了HyperDiffusionFields (HyDiF)框架，将3D分子构象建模为连续场而非离散原子坐标或图。我们方法的核心是分子方向场(MDF)，它将空间中的任何点映射到特定类型最近原子的方向。我们使用分子特定的神经隐式场表示MDF，称为分子神经场(MNF)。为了实现跨分子学习和促进泛化，我们采用了一种方法，其中共享的超网络条件化为分子，生成给定分子MNF的权重。为了赋予模型生成能力，我们将超网络训练为去噪扩散模型，能够在分子场的函数空间中进行采样。我们的设计自然扩展到掩码扩散机制，通过选择性对场区域加噪来支持结构条件生成任务，如分子修复。除了生成任务外，MDF的局部和连续特性使得能够进行空间细粒度的特征提取用于分子性质预测，这是基于图或点云的方法难以实现的。此外，我们证明了我们的方法可以扩展到更大的生物分子，展示了基于场的分子建模的一个有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce HyperDiffusionFields (HyDiF), a framework that models 3Dmolecular conformers as continuous fields rather than discrete atomiccoordinates or graphs. At the core of our approach is the Molecular DirectionalField (MDF), a vector field that maps any point in space to the direction ofthe nearest atom of a particular type. We represent MDFs usingmolecule-specific neural implicit fields, which we call Molecular Neural Fields(MNFs). To enable learning across molecules and facilitate generalization, weadopt an approach where a shared hypernetwork, conditioned on a molecule,generates the weights of the given molecule's MNF. To endow the model withgenerative capabilities, we train the hypernetwork as a denoising diffusionmodel, enabling sampling in the function space of molecular fields. Our designnaturally extends to a masked diffusion mechanism to supportstructure-conditioned generation tasks, such as molecular inpainting, byselectively noising regions of the field. Beyond generation, the localized andcontinuous nature of MDFs enables spatially fine-grained feature extraction formolecular property prediction, something not easily achievable with graph orpoint cloud based methods. Furthermore, we demonstrate that our approach scalesto larger biomolecules, illustrating a promising direction for field-basedmolecular modeling.</description>
      <author>example@mail.com (Sudarshan Babu, Phillip Lo, Xiao Zhang, Aadi Srivastava, Ali Davariashtiyani, Jason Perera, Michael Maire, Aly A. Khan)</author>
      <guid isPermaLink="false">2510.18122v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework</title>
      <link>http://arxiv.org/abs/2510.18825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 (Poster)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的分层掩码框架，揭示了模型架构和注意力掩码构建之间的等价性，并基于此设计了M3Dphormer模型，该模型结合了多级掩码和双注意力计算，在多个基准测试上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;图变换器(Graph Transformers)在图表示学习中表现出色，但现有方法往往依赖针对特定交互的复杂架构设计，缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决现有图变换器架构灵活性不足的问题，提出一个统一的框架来建模多样化的节点交互。&lt;h4&gt;方法&lt;/h4&gt;提出统一的分层掩码框架，揭示模型架构与注意力掩码构建的等价性；设计M3Dphormer模型，包含三种理论支持的分层掩码和双层专家路由机制；引入双注意力计算方案，根据局部掩码稀疏性动态切换模式。&lt;h4&gt;主要发现&lt;/h4&gt;正确分类的概率与感受野大小和标签一致性呈正相关；有效的注意力掩码应确保足够大的感受野和高水平的标签一致性；分层掩码在不同场景下具有互补优势。&lt;h4&gt;结论&lt;/h4&gt;M3Dphormer通过统一框架和多级掩码设计有效解决了图变换器的灵活性限制，在多个基准测试上实现了最先进的性能，验证了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(Graph Transformers)因其能够建模多样化的节点交互，已成为图表示学习的强大范式。然而，现有的图变换器通常依赖于针对特定交互的复杂架构设计，限制了其灵活性。为此，我们提出了一个统一的分层掩码框架，揭示了模型架构与注意力掩码构建之间的基本等价性。该框架通过精心设计的注意力掩码捕获多样化交互，实现了一致性的建模范式。在该框架下的理论分析表明，正确分类的概率与感受野大小和标签正相关，导致了一个基本设计原则：有效的注意力掩码应确保足够大的感受野和高水平的标签一致性。虽然没有单一现有的掩码能在所有场景下满足这一原则，但我们的分析显示分层掩码提供了互补优势，促使它们的有效集成。随后，我们引入了M3Dphormer，这是一种基于专家混合的图变换器，具有多级掩码和双注意力计算。M3Dphormer包含三种理论支持的分层掩码，并采用双层专家路由机制来自适应地集成多级交互信息。为确保可扩展性，我们进一步引入了双注意力计算方案，根据局部掩码稀疏性在密集和稀疏模式之间动态切换。在多个基准测试上的广泛实验证明，M3Dphormer达到了最先进的性能，验证了我们的统一框架和模型设计的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have emerged as a powerful paradigm for graphrepresentation learning due to their ability to model diverse nodeinteractions. However, existing GTs often rely on intricate architecturaldesigns tailored to specific interactions, limiting their flexibility. Toaddress this, we propose a unified hierarchical mask framework that reveals anunderlying equivalence between model architecture and attention maskconstruction. This framework enables a consistent modeling paradigm bycapturing diverse interactions through carefully designed attention masks.Theoretical analysis under this framework demonstrates that the probability ofcorrect classification positively correlates with the receptive field size andlabel consistency, leading to a fundamental design principle: an effectiveattention mask should ensure both a sufficiently large receptive field and ahigh level of label consistency. While no single existing mask satisfies thisprinciple across all scenarios, our analysis reveals that hierarchical masksoffer complementary strengths, motivating their effective integration. Then, weintroduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer withMulti-Level Masking and Dual Attention Computation. M3Dphormer incorporatesthree theoretically grounded hierarchical masks and employs a bi-level expertrouting mechanism to adaptively integrate multi-level interaction information.To ensure scalability, we further introduce a dual attention computation schemethat dynamically switches between dense and sparse modes based on local masksparsity. Extensive experiments across multiple benchmarks demonstrate thatM3Dphormer achieves state-of-the-art performance, validating the effectivenessof our unified framework and model design.</description>
      <author>example@mail.com (Yujie Xing, Xiao Wang, Bin Wu, Hai Huang, Chuan Shi)</author>
      <guid isPermaLink="false">2510.18825v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</title>
      <link>http://arxiv.org/abs/2510.18703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: this https://linyq17.github.io/VC2L/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VC2L是一种统一的以视觉为中心的对比学习框架，通过在像素空间操作处理文本、图像及其组合，解决了传统对比视觉语言模型处理复杂网页文档的局限性，并在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;对比视觉语言模型如CLIP通过学习对齐的图像-文本对在各种多模态任务中表现出色，但它们处理复杂的真实世界网页文档的能力仍然有限，特别是在文本和图像交错、松散对齐或嵌入视觉形式的情况下。&lt;h4&gt;目的&lt;/h4&gt;解决对比视觉语言模型处理复杂网页文档的局限性，提出一种统一的框架来有效处理文本、图像及其组合。&lt;h4&gt;方法&lt;/h4&gt;提出以视觉为中心的对比学习（VC2L）框架，使用单一的视觉变换器建模文本、图像及其组合；完全在像素空间操作，将所有输入渲染为图像，消除OCR、文本分词或模态融合的需要；采用片段级对比学习目标，对齐连续的多模态片段，利用文档的固有连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;引入了三个检索基准：AnyCIR（跨模态检索）、SeqCIR（细粒度顺序理解）和CSR（泛化到未见数据）；实验结果表明，VC2L在提出的基准和已建立的datasets（如M-BEIR和MTEB）上与CLIP风格模型相比具有竞争性或更优的性能。&lt;h4&gt;结论&lt;/h4&gt;多模态网页数据作为对比学习宝贵训练资源具有潜力；统一的、以视觉为中心的方法在多模态表示学习中具有良好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;对比视觉语言模型（如CLIP）通过学习对齐的图像-文本对在各种多模态任务中表现出色。然而，它们处理复杂的真实世界网页文档的能力仍然有限，特别是在文本和图像交错、松散对齐或嵌入视觉形式的情况下。为解决这些挑战，我们提出了以视觉为中心的对比学习（VC2L），这是一个统一框架，使用单一的视觉变换器对文本、图像及其组合进行建模。VC2L完全在像素空间操作，通过将所有输入（文本、视觉或组合）渲染为图像，从而消除了OCR、文本分词或模态融合策略的需要。为捕捉多模态网页文档中的复杂跨模态关系，VC2L采用片段级对比学习目标，对齐连续的多模态片段，利用文档的固有连贯性，不需要明确配对的图像-文本数据。为评估这种方法的有效性，我们引入了三个检索基准：AnyCIR、SeqCIR和CSR，分别用于评估跨模态检索、细粒度顺序理解和泛化到未见数据的能力。实验结果表明，与CLIP风格模型相比，VC2L在提出的基准和已建立的datasets（如M-BEIR和MTEB）上取得了竞争性或更优的性能。这些发现强调了多模态网页数据作为对比学习宝贵训练资源的潜力，并说明了统一的、以视觉为中心的方法在多模态表示学习中的可扩展性。代码和模型可在https://github.com/showlab/VC2L获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive vision-language models such as CLIP have demonstrated strongperformance across a wide range of multimodal tasks by learning from alignedimage-text pairs. However, their ability to handle complex, real-world webdocuments remains limited, particularly in scenarios where text and images areinterleaved, loosely aligned, or embedded in visual form. To address thesechallenges, we propose Vision-Centric Contrastive Learning (VC2L), a unifiedframework that models text, images, and their combinations using a singlevision transformer. VC2L operates entirely in pixel space by rendering allinputs, whether textual, visual, or combined, as images, thus eliminating theneed for OCR, text tokenization, or modality fusion strategy. To capturecomplex cross-modal relationships in multimodal web documents, VC2L employs asnippet-level contrastive learning objective that aligns consecutive multimodalsegments, leveraging the inherent coherence of documents without requiringexplicitly paired image-text data. To assess the effectiveness of thisapproach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,designed to evaluate cross-modal retrieval, fine-grained sequentialunderstanding, and generalization to unseen data, respectively. Empiricalresults show that VC2L achieves competitive or superior performance compared toCLIP-style models on both the proposed benchmarks and established datasets suchas M-BEIR and MTEB. These findings underscore the potential of multimodal webdata as a valuable training resource for contrastive learning and illustratethe scalability of a unified, vision-centric approach for multimodalrepresentation learning. Code and models are available at:https://github.com/showlab/VC2L.</description>
      <author>example@mail.com (Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou)</author>
      <guid isPermaLink="false">2510.18703v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification</title>
      <link>http://arxiv.org/abs/2510.18530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于锚点的分阶段学习策略，用于在嘈杂条件下学习稳健的说话人表示。&lt;h4&gt;背景&lt;/h4&gt;在嘈杂条件下学习稳健的说话人表示面临重大挑战，需要谨慎处理区分性和噪声不变性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在嘈杂环境中保持说话人识别准确性的方法。&lt;h4&gt;方法&lt;/h4&gt;采用基于锚点的分阶段学习策略，首先训练基础模型建立区分性的说话人边界，然后从模型中提取锚嵌入作为稳定参考，最后在嘈杂输入上对基础模型的副本进行微调，通过强制接近固定的锚嵌入来保持失真情况下的说话人身份。&lt;h4&gt;主要发现&lt;/h4&gt;这种策略相比传统的联合优化方法有优势，特别是在保持区分性的同时提高噪声鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在各种噪声条件下都表现出一致的改进，这可能是由于其能够分别处理边界稳定性和变化抑制。&lt;h4&gt;翻译&lt;/h4&gt;在嘈杂条件下学习稳健的说话人表示面临重大挑战，这需要谨慎处理区分性和噪声不变性。在这项工作中，我们提出了一种基于锚点的分阶段学习策略用于稳健的说话人表示学习。具体来说，我们的方法首先训练一个基础模型来建立区分性的说话人边界，然后从该模型中提取锚嵌入作为稳定参考。最后，在嘈杂输入上对基础模型的副本进行微调，通过强制接近其对应的固定锚嵌入来保持失真情况下的说话人身份。实验结果表明，这种策略相比传统的联合优化方法具有优势，特别是在保持区分性的同时提高噪声鲁棒性。提出的方法在各种噪声条件下都表现出一致的改进，这可能是由于它能够分别处理边界稳定性和变化抑制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning robust speaker representations under noisy conditions presentssignificant challenges, which requires careful handling of both discriminativeand noise-invariant properties. In this work, we proposed an anchor-basedstage-wise learning strategy for robust speaker representation learning.Specifically, our approach begins by training a base model to establishdiscriminative speaker boundaries, and then extract anchor embeddings from thismodel as stable references. Finally, a copy of the base model is fine-tuned onnoisy inputs, regularized by enforcing proximity to their corresponding fixedanchor embeddings to preserve speaker identity under distortion. Experimentalresults suggest that this strategy offers advantages over conventional jointoptimization, particularly in maintaining discrimination while improving noiserobustness. The proposed method demonstrates consistent improvements acrossvarious noise conditions, potentially due to its ability to handle boundarystabilization and variation suppression separately.</description>
      <author>example@mail.com (Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei)</author>
      <guid isPermaLink="false">2510.18530v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Simple and Efficient Heterogeneous Temporal Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.18467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Neurips 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种简单高效的异质时序图神经网络(SE-HTGNN)，通过动态注意力机制将时间建模集成到空间学习中，并利用大型语言模型增强模型理解能力。&lt;h4&gt;背景&lt;/h4&gt;异质时序图(HTGs)是现实世界中普遍存在的数据结构，现有基于注意力机制的神经网络方法采用解耦的时空学习范式，削弱了时空信息交互并导致模型复杂度高。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的HTGs学习范式，解决现有方法的时空信息交互弱和模型复杂度高的问题。&lt;h4&gt;方法&lt;/h4&gt;通过创新的动态注意力机制将时间建模集成到空间学习中，保留历史快图的注意力信息指导后续计算；同时利用大型语言模型提示SE-HTGNN，捕获节点类型的隐含属性作为先验知识。&lt;h4&gt;主要发现&lt;/h4&gt;SE-HTGNN比最先进和最新的基线方法快达10倍，同时保持最佳的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;SE-HTGNN在提高计算效率的同时保持了准确性，是一种有效的异质时序图表示学习方法。&lt;h4&gt;翻译&lt;/h4&gt;异质时序图(HTGs)是现实世界中普遍存在的数据结构。最近，为了增强HTGs的表示学习，已提出许多基于注意力机制的神经网络。尽管取得了这些成功，现有方法依赖于解耦的时空学习范式，这削弱了时空信息的交互作用并导致模型复杂度高。为了弥合这一差距，我们提出了一种名为简单高效的异质时序图神经网络(SE-HTGNN)的新型HTGs学习范式。具体来说，我们通过创新的动态注意力机制将时间建模集成到空间学习中，该机制保留来自历史图快图的注意力信息以指导后续注意力计算，从而提高HTGs的整体判别性表示学习能力。此外，为了全面且自适应地理解HTGs，我们利用大型语言模型提示SE-HTGNN，使模型能够捕获节点类型的隐含属性作为先验知识。大量实验证明，SE-HTGNN比最先进和最新的基线方法快达10倍，同时保持最佳的预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in thereal world. Recently, to enhance representation learning on HTGs, numerousattention-based neural networks have been proposed. Despite these successes,existing methods rely on a decoupled temporal and spatial learning paradigm,which weakens interactions of spatio-temporal information and leads to a highmodel complexity. To bridge this gap, we propose a novel learning paradigm forHTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network(SE-HTGNN). Specifically, we innovatively integrate temporal modeling intospatial learning via a novel dynamic attention mechanism, which retainsattention information from historical graph snapshots to guide subsequentattention computation, thereby improving the overall discriminativerepresentations learning of HTGs. Additionally, to comprehensively andadaptively understand HTGs, we leverage large language models to promptSE-HTGNN, enabling the model to capture the implicit properties of node typesas prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves upto 10x speed-up over the state-of-the-art and latest baseline while maintainingthe best forecasting accuracy.</description>
      <author>example@mail.com (Yili Wang, Tairan Huang, Changlong He, Qiutong Li, Jianliang Gao)</author>
      <guid isPermaLink="false">2510.18467v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ProLAP: Probabilistic Language-Audio Pre-Training</title>
      <link>http://arxiv.org/abs/2510.18423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了概率语言-音频预训练模型(ProLAP)，用于解决语言-音频关系中的多对多对应问题，通过概率分布扩散建模多样性，并引入层次包含损失和掩码排斥损失来提高学习效率。&lt;h4&gt;背景&lt;/h4&gt;现有的语言-音频联合表征学习框架通常依赖确定性嵌入，假设音频和文本之间存在一一对应关系。然而在现实世界中，语言-音频关系本质上是多对多的：一个音频片段可以用多个字幕描述，反之亦然。&lt;h4&gt;目的&lt;/h4&gt;解决语言-音频关系中的多对多对应问题，提出能够从小数据集中学习数据固有层次结构的模型。&lt;h4&gt;方法&lt;/h4&gt;提出概率语言-音频预训练模型(ProLAP)，将多样性建模为联合语言-音频嵌入空间中概率分布的扩散。同时引入两个训练目标：层次包含损失促进对输入的语义层次理解，掩码排斥损失提高学习效率。&lt;h4&gt;主要发现&lt;/h4&gt;ProLAP在音频-文本检索任务上优于现有的确定性方法。通过音频遍历任务实验，证明ProLAP能够捕捉合理的语义层次结构，即使从小数据集也能有效学习。&lt;h4&gt;结论&lt;/h4&gt;ProLAP成功解决了语言-音频关系中的多对多对应问题，能够从小数据集中学习数据固有的层次结构，在检索任务和语义层次捕捉方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;语言-音频联合表征学习框架通常依赖于确定性嵌入，假设音频和文本之间存在一一对应关系。然而在现实世界中，语言-音频关系本质上是多对多的：一个音频片段可以用多个字幕描述，反之亦然。为解决这一问题，我们提出了概率语言-音频预训练(ProLAP)，将多样性建模为联合语言-音频嵌入空间中概率分布的扩散。为有效训练模态内的层次关系，我们还引入了两个目标：(i)层次包含损失促进对输入的语义层次理解，(ii)掩码排斥损失优化层次包含损失时提高学习效率。通过这种训练策略，我们的模型能够从小数据集中学习数据固有的层次结构，这与依赖大规模数据集的先前概率方法形成对比。在我们的实验中，ProLAP在音频-文本检索任务上优于现有的确定性方法。此外，通过本文介绍的音频遍历任务实验，我们证明了ProLAP捕捉了合理的语义层次。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language-audio joint representation learning frameworks typically depend ondeterministic embeddings, assuming a one-to-one correspondence between audioand text. In real-world settings, however, the language-audio relationship isinherently many-to-many: one audio segment can be described by multiplecaptions and vice versa. To address this, we propose ProbabilisticLanguage-Audio Pre-training (ProLAP), which models multiplicity as the spreadof probability distributions in a joint language-audio embedding space. Totrain the intra-modal hierarchical relationship effectively, we also introducetwo objectives: (i) hierarchical inclusion loss to promote semantichierarchical understanding of inputs and (ii) mask repulsive loss to improvethe efficiency of learning when optimizing the hierarchical inclusion loss.With this training strategy, our model can learn the hierarchical structureinherent in the data even from small datasets, in contrast to priorprobabilistic approaches that rely on large-scale datasets. In our experiments,ProLAP outperforms existing deterministic approaches on audio-text retrievaltasks. Moreover, through experiments on the audio traversal task introduced inthis paper, we demonstrate that ProLAP captures the plausible semantichierarchy.</description>
      <author>example@mail.com (Toranosuke Manabe, Yuchi Ishikawa, Hokuto Munakata, Tatsuya Komatsu)</author>
      <guid isPermaLink="false">2510.18423v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Towards Identifiability of Hierarchical Temporal Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.18310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CHiLD的因果层次化潜在动态识别框架，能够有效捕捉时间序列数据中的多层次时间依赖关系，解决了现有方法无法从单时间步观测变量中恢复层次化潜在变量联合分布的问题。&lt;h4&gt;背景&lt;/h4&gt;对时间序列数据中的层次化潜在动态进行建模对于捕捉现实世界任务中多层次的抽象时间依赖关系至关重要，但现有方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉时间序列数据中层次化潜在动态的框架，解决现有时间序列因果表征学习方法的不足。&lt;h4&gt;方法&lt;/h4&gt;提出CHiLD识别框架，首先利用时间上下文观测变量识别多层潜在变量的联合分布，然后利用层次结构的自然稀疏性识别每层内的潜在变量，并基于变分推断开发时间序列生成模型，包含上下文编码器和归一化流层次化先验网络。&lt;h4&gt;主要发现&lt;/h4&gt;使用三个条件独立的观测可以唯一确定层次化潜在变量的联合分布，这一发现为构建新方法提供了理论基础。&lt;h4&gt;结论&lt;/h4&gt;在合成和真实世界数据集上的经验评估验证了理论主张，证明了CHiLD在建模层次化潜在动态方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;对时间序列数据背后的层次化潜在动态进行建模，对于捕捉现实世界任务中多层次抽象的时间依赖关系至关重要。然而，现有的时间序列因果表征学习方法无法捕捉此类动态，因为它们无法从单时间步观测变量中恢复层次化潜在变量的联合分布。有趣的是，我们发现使用三个条件独立的观测可以唯一确定层次化潜在变量的联合分布。基于这一见解，我们提出了一个因果层次化潜在动态（CHiLD）识别框架。我们的方法首先利用时间上下文观测变量来识别多层潜在变量的联合分布。随后，我们利用层次结构中潜在变量的自然稀疏性来识别每层内的潜在变量。在理论结果的指导下，我们开发了一个基于变分推断的时间序列生成模型。该模型包含一个上下文编码器来重建多层潜在变量，以及基于归一化流的层次化先验网络，以施加层次化潜在动态的独立噪声条件。在合成和真实世界数据集上的经验评估验证了我们的理论主张，并证明了CHiLD在建模层次化潜在动态方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling hierarchical latent dynamics behind time series data is critical forcapturing temporal dependencies across multiple levels of abstraction inreal-world tasks. However, existing temporal causal representation learningmethods fail to capture such dynamics, as they fail to recover the jointdistribution of hierarchical latent variables from \textit{single-timestepobserved variables}. Interestingly, we find that the joint distribution ofhierarchical latent variables can be uniquely determined using threeconditionally independent observations. Building on this insight, we propose aCausally Hierarchical Latent Dynamic (CHiLD) identification framework. Ourapproach first employs temporal contextual observed variables to identify thejoint distribution of multi-layer latent variables. Sequentially, we exploitthe natural sparsity of the hierarchical structure among latent variables toidentify latent variables within each layer. Guided by the theoretical results,we develop a time series generative model grounded in variational inference.This model incorporates a contextual encoder to reconstruct multi-layer latentvariables and normalize flow-based hierarchical prior networks to impose theindependent noise condition of hierarchical latent dynamics. Empiricalevaluations on both synthetic and real-world datasets validate our theoreticalclaims and demonstrate the effectiveness of CHiLD in modeling hierarchicallatent dynamics.</description>
      <author>example@mail.com (Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang)</author>
      <guid isPermaLink="false">2510.18310v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Universal Spectral Tokenization via Self-Supervised Panchromatic Representation Learning</title>
      <link>http://arxiv.org/abs/2510.17959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 Machine Learning and the Physical Sciences  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种深度学习模型，能够以自监督方式学习异构天文光谱数据，生成统一且物理上有意义的表示，可作为天文领域基础模型的强大构建块，并可能扩展到其他科学领域。&lt;h4&gt;背景&lt;/h4&gt;连续科学数据跨越多个分辨率和领域，将其统一为共同表示是开发科学基础模型的关键步骤。天文光谱体现了这一挑战：大规模调查已收集数百万个跨越广泛波长和分辨率的光谱，但分析仍分散在光谱域（如光学与红外）和天体类型（如恒星与星系）中，限制了跨数据集信息整合的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够统一不同分辨率和领域光谱数据的模型，创建物理上有意义的表示，并适应各种下游任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种深度学习模型，以自监督方式联合学习异构光谱数据。该通用光谱标记器直接在原生波长网格上处理多种天体类型和分辨率的光谱，产生内在对齐、同质且物理上有意义的表示。&lt;h4&gt;主要发现&lt;/h4&gt;首次证明单个模型可以统一不同分辨率和领域的光谱数据。该模型能够高效适应，在各种下游任务中实现具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;该模型可作为天文领域基础模型的强大构建块，并可能扩展到其他具有异构连续数据的科学领域，如气候和医疗保健。&lt;h4&gt;翻译&lt;/h4&gt;连续科学数据跨越多个分辨率和领域，将其统一为共同表示是开发科学基础模型的关键步骤。天文光谱体现了这一挑战：大规模调查已收集了数百万个跨越广泛波长和分辨率的光谱，但分析仍分散在光谱域（如光学与红外）和天体类型（如恒星与星系）中，限制了跨数据集信息整合的能力。我们提出了一种深度学习模型，能够以自监督方式联合学习异构光谱数据。我们的通用光谱标记器直接在原生波长网格上处理多种天体类型和分辨率的光谱，产生内在对齐、同质且物理上有意义的表示，这些表示可以高效适应，在各种下游任务中实现具有竞争力的性能。我们首次证明，单个模型可以统一不同分辨率和领域的光谱数据，这表明我们的模型可以作为天文领域基础模型的强大构建块——并可能扩展到其他具有异构连续数据的科学领域，如气候和医疗保健。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential scientific data span many resolutions and domains, and unifyingthem into a common representation is a key step toward developing foundationmodels for the sciences. Astronomical spectra exemplify this challenge: massivesurveys have collected millions of spectra across a wide range of wavelengthsand resolutions, yet analyses remain fragmented across spectral domains (e.g.,optical vs. infrared) and object types (e.g., stars vs. galaxies), limiting theability to pool information across datasets. We present a deep learning modelthat jointly learns from heterogeneous spectra in a self-supervised manner. Ouruniversal spectral tokenizer processes spectra from a variety of object typesand resolutions directly on their native wavelength grids, producingintrinsically aligned, homogeneous, and physically meaningful representationsthat can be efficiently adapted to achieve competitive performance across arange of downstream tasks. For the first time, we demonstrate that a singlemodel can unify spectral data across resolutions and domains, suggesting thatour model can serve as a powerful building block for foundation models inastronomy -- and potentially extend to other scientific domains withheterogeneous sequential data, such as climate and healthcare.</description>
      <author>example@mail.com (Jeff Shen, Francois Lanusse, Liam Holden Parker, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Cassereau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho)</author>
      <guid isPermaLink="false">2510.17959v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation</title>
      <link>http://arxiv.org/abs/2510.17914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了NeuCo-Bench，一个用于评估地球观测背景下神经压缩和表示学习的新型基准框架。&lt;h4&gt;背景&lt;/h4&gt;地球观测领域需要有效的神经压缩和表示学习方法，但目前缺乏标准化的评估框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个社区驱动、标准化的评估框架，用于评估地球观测领域的神经嵌入，平衡准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;基于固定大小的嵌入作为紧凑、任务无关的表示，包含三个核心组件：可重用嵌入的评估管道、隐藏任务排行榜的挑战模式、平衡准确性和稳定性的评分系统，并提供多光谱多时相的地球观测数据集。&lt;h4&gt;主要发现&lt;/h4&gt;通过公开挑战和与最先进基础模型的消融研究，验证了NeuCo-Bench框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;NeuCo-Bench为地球观测及更广泛领域的神经嵌入的社区驱动、标准化评估提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;我们引入NeuCo-Bench，一个用于评估地球观测背景下（有损）神经压缩和表示学习的新型基准框架。我们的方法基于固定大小的嵌入，这些嵌入作为紧凑的、与任务无关的表示，适用于广泛的下游任务。NeuCo-Bench包含三个核心组件：(i)围绕可重用嵌入构建的评估管道，(ii)具有隐藏任务排行榜的新挑战模式，旨在减轻预训练偏差，(iii)平衡准确性和稳定性的评分系统。为了支持可重现性，我们发布了SSL4EO-S12-downstream，这是一个精心策划的多光谱、多时相的地球观测数据集。我们展示了在2025年CVPR EARTHVISION研讨会上公开挑战的初步结果，并使用最先进的基础模型进行了消融研究。NeuCo-Bench为地球观测及更广泛领域的神经嵌入的社区驱动、标准化评估迈出了第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)neural compression and representation learning in the context of EarthObservation (EO). Our approach builds on fixed-size embeddings that act ascompact, task-agnostic representations applicable to a broad range ofdownstream tasks. NeuCo-Bench comprises three core components: (i) anevaluation pipeline built around reusable embeddings, (ii) a new challenge modewith a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)a scoring system that balances accuracy and stability. To supportreproducibility, we release SSL4EO-S12-downstream, a curated multispectral,multitemporal EO dataset. We present initial results from a public challenge atthe 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-artfoundation models. NeuCo-Bench provides a first step towards community-driven,standardized evaluation of neural embeddings for EO and beyond.</description>
      <author>example@mail.com (Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht)</author>
      <guid isPermaLink="false">2510.17914v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability</title>
      <link>http://arxiv.org/abs/2510.17040v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为多样化影响成分分析(DICA)的新框架，用于从未知非线性混合中识别潜在成分，无需依赖辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设。&lt;h4&gt;背景&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习中的一个基础性挑战，应用于解缠表示学习和因果推断等任务。先前在线性独立成分分析方面的工作表明辅助信号可以支持潜在成分的可识别性，而较新方法则通过结构假设（如混合函数雅可比矩阵稀疏性）来放宽要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从未知非线性混合中识别潜在成分的方法，同时减少对辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设的依赖。&lt;h4&gt;方法&lt;/h4&gt;研究提出了多样化影响成分分析(DICA)框架，利用混合函数雅可比矩阵的凸几何特性，并引入雅可比体积最大化(J-VolMax)准则，通过鼓励潜在成分对观测变量的影响多样化来实现潜在成分的识别。&lt;h4&gt;主要发现&lt;/h4&gt;在合理条件下，所提出的方法无需依赖辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设即可实现潜在成分的可识别性。&lt;h4&gt;结论&lt;/h4&gt;研究结果扩展了可识别性分析的范围，为现有方法提供了互补的视角，为从非线性混合中识别潜在成分提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习中的一个基础性挑战，应用于解缠表示学习和因果推断等任务。先前在线性独立成分分析(nICA)方面的工作表明，辅助信号（如弱监督）可以支持条件独立潜在成分的可识别性。较新的方法则探索结构假设，例如混合函数的雅可比矩阵中的稀疏性，以放宽这些要求。在这项工作中，我们引入了多样化影响成分分析(DICA)框架，该框架利用混合函数雅可比矩阵的凸几何特性。我们提出了雅可比体积最大化(J-VolMax)准则，通过鼓励潜在成分对观测变量的影响多样化，实现潜在成分的识别。在合理条件下，这种方法无需依赖辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设即可实现可识别性。这些结果扩展了可识别性分析的范围，并为现有方法提供了互补的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent component identification from unknown nonlinear mixtures is afoundational challenge in machine learning, with applications in tasks such asdisentangled representation learning and causal inference. Prior work innonlinear independent component analysis (nICA) has shown that auxiliarysignals -- such as weak supervision -- can support identifiability ofconditionally independent latent components. More recent approaches explorestructural assumptions, e.g., sparsity in the Jacobian of the mixing function,to relax such requirements. In this work, we introduce Diverse InfluenceComponent Analysis (DICA), a framework that exploits the convex geometry of themixing function's Jacobian. We propose a Jacobian Volume Maximization(J-VolMax) criterion, which enables latent component identification byencouraging diversity in their influence on the observed variables. Underreasonable conditions, this approach achieves identifiability without relyingon auxiliary information, latent component independence, or Jacobian sparsityassumptions. These results extend the scope of identifiability analysis andoffer a complementary perspective to existing methods.</description>
      <author>example@mail.com (Hoang-Son Nguyen, Xiao Fu)</author>
      <guid isPermaLink="false">2510.17040v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Triangle Multiplication Is All You Need For Biomolecular Structure Representations</title>
      <link>http://arxiv.org/abs/2510.18870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Pairmixer是一种替代AlphaFold3中Pairformer主干网络的高效解决方案，消除了计算密集型的三角形注意力机制，同时保留了高阶几何推理能力，显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;AlphaFold已变革蛋白质结构预测，但虚拟配体筛选、全蛋白质组折叠和从头结合剂设计等新兴应用需要大规模预测，面临运行时间和内存成本过高的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代方案，消除三角形注意力机制，同时保留对结构预测至关重要的高阶几何推理能力，提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出Pairmixer，一种简化的替代方案，消除三角形注意力机制，同时保留高阶几何推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;Pairmixer在折叠和对接基准测试中匹配最先进结构预测器性能，实现长序列上高达4倍的更快推理速度，训练成本降低34%；在BoltzDesign中提供超过2倍的更快采样速度，可扩展到比Pairformer内存限制长约30%的序列。&lt;h4&gt;结论&lt;/h4&gt;Pairmixer解决了大规模蛋白质结构预测的计算瓶颈问题，为建模大型蛋白质复合物、高通量配体和结合剂筛选等下游应用提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;AlphaFold已经改变了蛋白质结构预测，但新兴应用如虚拟配体筛选、全蛋白质组折叠和从头结合剂设计需要大规模预测，此时运行时间和内存成本变得过高。主要瓶颈在于AlphaFold3类模型的Pairformer主干网络，它依赖于计算密集型的三角形原语——特别是三角形注意力机制——用于成对推理。我们引入Pairmixer，一种简化的替代方案，消除了三角形注意力同时保留了对结构预测至关重要的高阶几何推理能力。Pairmixer显著提高了计算效率，在折叠和对接基准测试中匹配最先进的结构预测器，在长序列上实现高达4倍的更快推理速度，同时将训练成本降低34%。其效率减轻了下游应用的计算负担，如建模大型蛋白质复合物、高通量配体和结合剂筛选以及基于幻觉的设计。例如，在BoltzDesign中，Pairmixer提供超过2倍的更快采样速度，并可扩展到比Pairformer内存限制长约30%的序列。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AlphaFold has transformed protein structure prediction, but emergingapplications such as virtual ligand screening, proteome-wide folding, and denovo binder design demand predictions at a massive scale, where runtime andmemory costs become prohibitive. A major bottleneck lies in the Pairformerbackbone of AlphaFold3-style models, which relies on computationally expensivetriangular primitives-especially triangle attention-for pairwise reasoning. Weintroduce Pairmixer, a streamlined alternative that eliminates triangleattention while preserving higher-order geometric reasoning capabilities thatare critical for structure prediction. Pairmixer substantially improvescomputational efficiency, matching state-of-the-art structure predictors acrossfolding and docking benchmarks, delivering up to 4x faster inference on longsequences while reducing training cost by 34%. Its efficiency alleviates thecomputational burden of downstream applications such as modeling large proteincomplexes, high-throughput ligand and binder screening, and hallucination-baseddesign. Within BoltzDesign, for example, Pairmixer delivers over 2x fastersampling and scales to sequences ~30% longer than the memory limits ofPairformer.</description>
      <author>example@mail.com (Jeffrey Ouyang-Zhang, Pranav Murugan, Daniel J. Diaz, Gianluca Scarpellini, Richard Strong Bowen, Nate Gruver, Adam Klivans, Philipp Krähenbühl, Aleksandra Faust, Maruan Al-Shedivat)</author>
      <guid isPermaLink="false">2510.18870v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations</title>
      <link>http://arxiv.org/abs/2510.18697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to RA-L&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了事件基础图（EGG）框架，通过将事件交互与场景空间特征关联，增强了机器人对环境的理解和交互能力，使机器人能够感知、推理和响应复杂的时空查询。&lt;h4&gt;背景&lt;/h4&gt;构建能够协助人类日常生活的智能自主机器人需要丰富的环境表示。虽然语义场景表示的进步已丰富了机器人的场景理解，但当前方法缺乏空间特征与动态事件之间的联系，例如无法将蓝色杯子与'洗杯子'事件联系起来。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将事件交互与场景空间特征联系起来的框架，使机器人能够更好地理解和响应环境中的动态事件。&lt;h4&gt;方法&lt;/h4&gt;引入事件基础图（EGG）框架，这是一种将事件交互与场景空间特征关联的环境表示方法，允许机器人感知、推理和响应复杂的时空查询。&lt;h4&gt;主要发现&lt;/h4&gt;使用真实机器人数据的实验证明EGG能够检索相关信息并准确响应对环境和事件的询问，展示了其在实际应用中的有效性。&lt;h4&gt;结论&lt;/h4&gt;EGG框架通过开源源代码和评估数据集（https://github.com/aalto-intelligent-robotics/EGG）为机器人环境理解和交互提供了新的解决方案，促进了该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;构建能够协助人类日常生活的智能自主机器人的一个基本方面是构建丰富的环境表示。尽管语义场景表示的进步已经丰富了机器人的场景理解，但当前方法缺乏空间特征与动态事件之间的联系；例如，无法将蓝色杯子与'洗杯子'事件联系起来。在这项工作中，我们引入了事件基础图（EGG），这是一个将事件交互与场景空间特征关联的框架。这种表示允许机器人感知、推理和响应复杂的时空查询。使用真实机器人数据的实验证明了EGG能够检索相关信息并准确响应对环境和事件的询问。此外，EGG框架的源代码和评估数据集已在以下地址开源：https://github.com/aalto-intelligent-robotics/EGG。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何构建一个能够将空间特征与动态事件连接起来的统一场景表示方法。这个问题很重要，因为当前的场景表示方法要么只关注静态空间元素(如3D场景图)，要么只记录事件描述(如视频字幕)，但无法将两者有效连接。这种连接缺失使得机器人无法理解空间变化与导致这些变化的交互之间的关系，限制了它们回答复杂时空查询的能力，如'你最后一次看到我的杯子是什么时候？'或'找到一台咖啡机'。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计方法时借鉴了现有工作的优点并试图解决其局限性。他们参考了3D场景图(3DSGs)的层次结构来表示空间元素，但增加了动态事件的表示；借鉴了记忆表示方法(如ReMEmbR和Embodied-RAG)记录事件交互的思想，但解决了它们缺乏空间特征连接的问题；还参考了利用大型语言模型进行信息检索的方法(如SayPlan和H-EMV)，但专注于改进基础表示。作者将这些方法的优势整合，创建了一个统一的图结构，通过边将事件与参与的空间元素连接起来。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个统一的图结构(EGG)同时表示场景中的空间元素和动态事件，并通过边将事件与参与事件的物体连接起来。整体流程包括：1) 构建EGG图，包含空间组件(类似3D场景图)和事件组件(描述观察到的活动)，以及连接事件与空间元素的事件边；2) 当接收到查询时，根据查询的四个维度(时间、位置、空间元素和事件)修剪图，提取相关子图；3) 将修剪后的子图序列化为JSON格式，输入到大型语言模型中生成回答。这种方法使机器人能够同时理解场景的空间布局和其中发生的事件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 创建了统一的时空场景表示，同时包含空间组件和事件组件；2) 通过事件边将动态事件与参与的空间元素连接起来，解决了现有方法中空间特征与动态事件之间的连接缺失问题；3) 开发了基于查询的图修剪策略，减少计算负担并提高回答质量；4) 使用真实机器人数据进行了全面的实验验证。相比之前的工作，EGG的主要不同在于：与传统3D场景图相比，增加了动态事件的表示；与纯事件表示方法相比，保留了空间特征的精确表示；与现有记忆表示方法相比，通过事件边明确地连接事件与空间元素，解决了多视角一致性和冗余信息问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了EGG框架，通过将动态事件与空间特征连接起来，创建了一个统一的时空场景图，使机器人能够感知、推理和解释场景中发生的事件，并准确回答复杂的时空查询。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A fundamental aspect for building intelligent autonomous robots that canassist humans in their daily lives is the construction of rich environmentalrepresentations. While advances in semantic scene representations have enrichedrobotic scene understanding, current approaches lack a connection betweenspatial features and dynamic events; e.g., connecting the blue mug to the eventwashing a mug. In this work, we introduce the event-grounding graph (EGG), aframework grounding event interactions to spatial features of a scene. Thisrepresentation allows robots to perceive, reason, and respond to complexspatio-temporal queries. Experiments using real robotic data demonstrate EGG'scapability to retrieve relevant information and respond accurately to humaninquiries concerning the environment and events within. Furthermore, the EGGframework's source code and evaluation dataset are released as open-source at:https://github.com/aalto-intelligent-robotics/EGG.</description>
      <author>example@mail.com (Phuoc Nguyen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2510.18697v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning</title>
      <link>http://arxiv.org/abs/2510.18337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出MoTVLA模型，一种基于混合变换器的视觉语言动作模型，整合快速-慢速统一推理与行为策略学习，解决机器人学习中语言可控性和推理效率的平衡问题。&lt;h4&gt;背景&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中是增强机器人学习在开放世界中泛化能力的重要方向，但现有方法面临语言可控性有限或推理延迟显著的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持预训练视觉语言模型的通用智能，又能提高策略执行效率和语言可控性的机器人学习模型。&lt;h4&gt;方法&lt;/h4&gt;提出MoTVLA模型，结合预训练视觉语言模型作为通用专家和领域特定专家，生成快速推理并基于分解的运动指令学习多样化行为。&lt;h4&gt;主要发现&lt;/h4&gt;MoTVLA在保持通用智能的同时，通过领域专家生成快速推理显著提高了策略执行效率，并通过条件化动作专家提升了语言可控性。&lt;h4&gt;结论&lt;/h4&gt;MoTVLA在自然语言处理基准、机器人仿真环境和真实世界实验中表现出优越的快速-慢速推理能力和操作任务性能。&lt;h4&gt;翻译&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中正在增强机器人学习在开放世界中的泛化能力。尽管取得了 promising 的进展，但现有方法面临两个挑战：在不使用生成推理作为条件时，语言可控性有限；当加入推理时，推理延迟显著。在这项工作中，我们引入了MoTVLA，一种基于混合变换器的视觉语言动作模型，整合了快速-慢速统一推理与行为策略学习。MoTVLA保留了预训练视觉语言模型的通用智能，用于感知、场景理解和语义规划等任务，同时引入领域专家（第二个与预训练VLM共享知识的Transformer）生成领域特定的快速推理（如机器人运动分解），从而提高策略执行效率。通过将动作专家基于分解的运动指令进行条件化，MoTVLA可以学习多样化行为并显著提高语言可控性。在自然语言处理基准、机器人仿真环境和真实世界实验中的广泛评估证实了MoTVLA在快速-慢速推理和操作任务性能方面的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言-行动(VLA)模型中的两个关键挑战：当不使用生成的推理作为条件时，语言可控性有限；当融入推理过程时，推理延迟显著。这个问题在机器人学习领域非常重要，因为机器人需要既能理解复杂语言指令，又能快速执行任务的能力。特别是在现实世界的应用中，机器人需要实时响应用户指令并准确执行任务，现有方法要么牺牲语言引导能力，要么牺牲推理速度，这限制了机器人在时间关键型应用中的实用性和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法要么缺乏语言可控性，要么推理速度慢，因此需要一种能够同时兼顾两者的方法。他们思考如何在一个统一架构中结合快速推理（用于高效执行）和慢速推理（用于复杂理解），同时保留预训练VLM的通用智能并添加领域特定知识以提高执行效率。作者借鉴了多种现有工作，包括：视觉-语言模型(VLMs)作为预训练基础，扩散策略(DPs)用于建模连续动作空间，混合变换器(MoT)架构用于整合不同功能组件，以及现有的VLA模型如RT-2、OpenVLA、π0.5等。在这些工作基础上，作者创新性地提出了统一的快速-慢速推理架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; MoTVLA的核心思想是在单一架构中统一快速和慢速推理，通过在输入端分解模态，在输出端分解功能，同时在中间保持共享的全局知识库。模型包含三个关键组件：通才(generalist)负责视觉-文本理解和慢速推理；领域专家(domain expert)专注于机器人任务的快速推理；行动专家(action expert)负责多任务策略学习。整体实现流程：1)输入处理：将输入分解为语言提示、RGB图像和可学习查询；2)推理主干：遵循'分解-组合-分解'范式，先独立处理多模态输入，再通过全局自注意力机制整合，最后在输出端解耦执行不同类型推理；3)推理输出：慢速推理用下一个token预测，快速推理用token-wise预测；4)行动专家：用扩散Transformer(DiT)在动作扩散框架内学习策略；5)训练：先进行领域专家监督微调，再训练行动专家；6)推理：支持对话模式(慢速推理)和行动模式(快速推理)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的快速-慢速推理架构：首次在单一模型中统一两种推理，保留通用智能同时高效学习领域知识；2)基于分解运动的策略学习：通过快速推理生成的分解运动来条件化策略学习，加快任务执行速度；3)分解-组合-分解设计：输入端分解模态，中间通过共享注意力整合，输出端分解功能；4)双模式推理：支持对话模式和行动模式，确保响应与语言提示一致；5)知识共享机制：通才和领域专家通过全局自注意力共享知识。相比之前工作的不同：1)与现有VLA模型相比：同时实现高语言可控性和低推理延迟；2)与基于扩散的策略相比：显式生成推理来条件化策略，提高语言可控性；3)与π0.5等模型相比：通过token-wise预测实现快速推理，大幅提高推理效率；4)架构设计不同：采用创新的MoT架构，明确分离功能但允许知识共享；5)训练策略不同：采用两阶段课程学习，保留预训练VLM的通用智能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了MoTVLA，一种基于混合变换器架构的视觉-语言-行动模型，通过统一的快速-慢速推理机制，在保留预训练视觉语言模型通用智能的同时，显著提高了语言可控性和推理效率，为机器人学习中的开放世界语言指导任务提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating visual-language instructions into visuomotor policies is gainingmomentum in robot learning for enhancing open-world generalization. Despitepromising advances, existing approaches face two challenges: limited languagesteerability when no generated reasoning is used as a condition, or significantinference latency when reasoning is incorporated.In this work, we introduceMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)model that integrates fast-slow unified reasoning with behavior policylearning. MoTVLA preserves the general intelligence of pre-trained VLMs(serving as the generalist) for tasks such as perception, scene understanding,and semantic planning, while incorporating a domain expert, a secondtransformer that shares knowledge with the pretrained VLM, to generatedomain-specific fast reasoning (e.g., robot motion decomposition), therebyimproving policy execution efficiency. By conditioning the action expert ondecomposed motion instructions, MoTVLA can learn diverse behaviors andsubstantially improve language steerability. Extensive evaluations acrossnatural language processing benchmarks, robotic simulation environments, andreal-world experiments confirm the superiority of MoTVLA in both fast-slowreasoning and manipulation task performance.</description>
      <author>example@mail.com (Wenhui Huang, Changhe Chen, Han Qi, Chen Lv, Yilun Du, Heng Yang)</author>
      <guid isPermaLink="false">2510.18337v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding</title>
      <link>http://arxiv.org/abs/2510.18262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  We have released V1, which only reports the test results. Our work is  still ongoing, and the next version will be coming soon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型视觉语言模型在自然场景理解方面取得成功，但在水下环境应用尚未充分探索。研究人员开发了UWBench基准数据集，包含15003张高分辨率水下图像及丰富标注，建立了三个水下视觉语言理解基准任务，实验表明水下理解仍具挑战性，该基准为水下环境视觉语言研究提供重要资源。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型在自然场景理解方面取得显著成功，但在水下环境中的应用仍被忽视。水下图像面临光衰减、色彩失真和悬浮颗粒散射等独特挑战，同时需要海洋生态系统和生物分类学专业知识。&lt;h4&gt;目的&lt;/h4&gt;填补大型视觉语言模型在水下环境应用的空白，引入专为水下视觉语言理解设计的综合基准UWBench，为水下环境中的视觉语言研究提供必要资源，支持海洋科学、生态监测和自主水下探索等应用。&lt;h4&gt;方法&lt;/h4&gt;构建UWBench基准，包含15003张不同水生环境的高分辨率水下图像，每张图像配有15281个对象指代表达式和124983个问答对。基于此建立三个综合基准：详细图像字幕生成、视觉定位和视觉问答，用于评估模型在水下环境中的表现。&lt;h4&gt;主要发现&lt;/h4&gt;在最先进视觉语言模型上的实验表明，水下理解仍然具有挑战性，存在大量改进空间。数据集捕捉了可见度、光照条件和水浊度的丰富变化，为模型评估提供了现实的测试平台。&lt;h4&gt;结论&lt;/h4&gt;UWBench基准为推进水下环境中的视觉语言研究提供了必要资源，支持海洋科学、生态监测和自主水下探索等应用。研究代码和基准将公开可用，促进相关领域发展。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型在自然场景理解方面取得了显著成功，但在水下环境中的应用仍然很大程度上未被探索。水下图像呈现独特的挑战，包括严重的光衰减、色彩失真和悬浮颗粒散射，同时需要海洋生态系统和生物分类学的专业知识。为了填补这一空白，我们引入UWBench，一个专门为水下视觉语言理解设计的综合基准。UWBench包含15003张在不同水生环境中捕获的高分辨率水下图像，涵盖海洋、珊瑚礁和深海栖息地。每张图像都经过人工验证的注释丰富，包括15281个精确描述海洋生物和水下结构的对象指代表达式，以及124983个涵盖从对象识别到生态关系理解的多样化推理能力的问答对。数据集捕捉到了可见度、光照条件和水浊度的丰富变化，为模型评估提供了现实的测试平台。基于UWBench，我们建立了三个综合基准：用于生成生态信息场景描述的详细图像字幕生成，用于精确定位海洋生物的视觉定位，以及用于水下环境多模态推理的视觉问答。在最先进的视觉语言模型上的广泛实验表明，水下理解仍然具有挑战性，有大量改进空间。我们的基准为推进水下环境中的视觉语言研究提供了必要的资源，支持海洋科学、生态监测和自主水下探索等应用。我们的代码和基准将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (VLMs) have achieved remarkable success innatural scene understanding, yet their application to underwater environmentsremains largely unexplored. Underwater imagery presents unique challengesincluding severe light attenuation, color distortion, and suspended particlescattering, while requiring specialized knowledge of marine ecosystems andorganism taxonomy. To bridge this gap, we introduce UWBench, a comprehensivebenchmark specifically designed for underwater vision-language understanding.UWBench comprises 15,003 high-resolution underwater images captured acrossdiverse aquatic environments, encompassing oceans, coral reefs, and deep-seahabitats. Each image is enriched with human-verified annotations including15,281 object referring expressions that precisely describe marine organismsand underwater structures, and 124,983 question-answer pairs covering diversereasoning capabilities from object recognition to ecological relationshipunderstanding. The dataset captures rich variations in visibility, lightingconditions, and water turbidity, providing a realistic testbed for modelevaluation. Based on UWBench, we establish three comprehensive benchmarks:detailed image captioning for generating ecologically informed scenedescriptions, visual grounding for precise localization of marine organisms,and visual question answering for multimodal reasoning about underwaterenvironments. Extensive experiments on state-of-the-art VLMs demonstrate thatunderwater understanding remains challenging, with substantial room forimprovement. Our benchmark provides essential resources for advancingvision-language research in underwater contexts and supporting applications inmarine science, ecological monitoring, and autonomous underwater exploration.Our code and benchmark will be available.</description>
      <author>example@mail.com (Da Zhang, Chenggang Rong, Bingyu Li, Feiyu Wang, Zhiyuan Zhao, Junyu Gao, Xuelong Li)</author>
      <guid isPermaLink="false">2510.18262v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion</title>
      <link>http://arxiv.org/abs/2510.18253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenInsGaussian是一种开放词汇实例高斯分割框架，通过上下文感知特征提取和注意力驱动的特征聚合两个模块解决了现有3D场景理解方法中的两个主要局限：预处理过程中单个掩码的上下文线索不足，以及融合多视图特征时存在的不一致性和缺失细节。&lt;h4&gt;背景&lt;/h4&gt;理解3D场景对自动驾驶、机器人和增强现实至关重要。现有的语义高斯飞溅方法利用大规模2D视觉模型将2D语义特征投影到3D场景上。&lt;h4&gt;目的&lt;/h4&gt;解决现有语义高斯飞溅方法的两个主要局限：预处理过程中单个掩码的上下文线索不足，以及融合多视图特征时存在的不一致性和缺失细节。&lt;h4&gt;方法&lt;/h4&gt;提出OpenInsGaussian框架，包含两个模块：1) 上下文感知特征提取，为每个掩码添加丰富的语义上下文；2) 注意力驱动的特征聚合，选择性融合多视图特征以减轻对齐错误和不完整性。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上进行的大量实验表明，OpenInsGaussian在开放词汇3D高斯分割方面取得了最先进的结果，大幅优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;OpenInsGaussian方法具有鲁棒性和通用性，标志着3D场景理解及其在各种实际场景中实际部署的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;理解3D场景对自动驾驶、机器人和增强现实至关重要。最近的语义高斯飞溅方法利用大规模2D视觉模型将2D语义特征投影到3D场景上。然而，它们存在两个主要局限：(1) 预处理过程中单个掩码的上下文线索不足；(2) 融合这些2D模型的多视图特征时存在不一致性和缺失细节。在本文中，我们介绍了OpenInsGaussian，一个具有上下文感知跨视图融合的开放词汇实例高斯分割框架。我们的方法包含两个模块：上下文感知特征提取，为每个掩码添加丰富的语义上下文；以及注意力驱动的特征聚合，选择性融合多视图特征以减轻对齐错误和不完整性。在基准数据集上的大量实验表明，OpenInsGaussian在开放词汇3D高斯分割方面取得了最先进的结果，大幅优于现有基线。这些发现强调了所提出方法的鲁棒性和通用性，标志着3D场景理解及其在各种实际场景中实际部署的重要一步。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景理解中的两个关键问题：1) 在预处理阶段从掩码提取特征时丢失上下文信息，特别是对小型或部分遮挡对象的识别；2) 多视图特征融合时由于光照、遮挡和视角变化导致的不一致问题。这些问题在自动驾驶、机器人和增强现实等应用中至关重要，因为它们直接影响3D场景语义理解的准确性和可靠性，而现有的语义高斯飞溅方法在这两方面存在明显局限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性，借鉴了3D高斯飞溅(3DGS)技术用于场景建模，利用CLIP等视觉语言模型提取语义特征，并参考了OpenGaussian的对比学习框架和SAM的分割能力。在此基础上，作者设计了两个关键模块：上下文感知特征提取模块，直接从CLIP中间特征图提取保留上下文的信息；以及注意力驱动的特征聚合策略，根据语义一致性选择性融合多视图特征。这种方法既利用了现有技术的优势，又针对性地解决了它们忽视的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过保留上下文信息和智能融合多视图特征来提升3D场景语义理解的准确性。整体流程分为四个阶段：1) 预处理：使用SAM生成对象掩码，同时提取局部特征和上下文感知特征并融合；2) 实例特征学习：通过对比学习训练3D高斯的类无关实例特征；3) 离散化：使用分层聚类策略将3D高斯分割为类无关簇；4) 语言特征聚合：通过注意力机制将语言特征与分割实例关联，根据视图间语义一致性加权融合特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 上下文感知特征提取，从CLIP中间特征图而非裁剪图像提取特征，保留空间上下文；2) 注意力驱动的特征聚合，基于余弦相似度而非复杂自注意力机制，高效融合多视图特征；3) 几何集成策略融合局部和上下文特征。相比之前工作，OpenInsGaussian解决了LangSplat和LEGaussians的模糊特征问题，也克服了OpenGaussian在处理运动模糊图像时的局限性，同时计算效率更高，无需额外训练即可直接应用于CLIP特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenInsGaussian通过创新的上下文感知特征提取和注意力驱动的多视图融合机制，显著提升了3D高斯飞溅在开放词汇场景理解中的性能，解决了现有方法中上下文丢失和多视图不一致的关键挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D scenes is pivotal for autonomous driving, robotics, andaugmented reality. Recent semantic Gaussian Splatting approaches leveragelarge-scale 2D vision models to project 2D semantic features onto 3D scenes.However, they suffer from two major limitations: (1) insufficient contextualcues for individual masks during preprocessing and (2) inconsistencies andmissing details when fusing multi-view features from these 2D models. In thispaper, we introduce \textbf{OpenInsGaussian}, an \textbf{Open}-vocabulary\textbf{Ins}tance \textbf{Gaussian} segmentation framework with Context-awareCross-view Fusion. Our method consists of two modules: Context-Aware FeatureExtraction, which augments each mask with rich semantic context, andAttention-Driven Feature Aggregation, which selectively fuses multi-viewfeatures to mitigate alignment errors and incompleteness. Through extensiveexperiments on benchmark datasets, OpenInsGaussian achieves state-of-the-artresults in open-vocabulary 3D Gaussian segmentation, outperforming existingbaselines by a large margin. These findings underscore the robustness andgenerality of our proposed approach, marking a significant step forward in 3Dscene understanding and its practical deployment across diverse real-worldscenarios.</description>
      <author>example@mail.com (Tianyu Huang, Runnan Chen, Dongting Hu, Fengming Huang, Mingming Gong, Tongliang Liu)</author>
      <guid isPermaLink="false">2510.18253v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>HouseTour: A Virtual Real Estate A(I)gent</title>
      <link>http://arxiv.org/abs/2510.18054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published on ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HouseTour，一种从描绘现有3D空间的图像集合中生成具有空间感知的3D相机轨迹和自然语言摘要的方法。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言模型在几何推理方面存在困难，缺乏能够同时处理3D相机轨迹生成和文本描述的集成方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成平滑视频轨迹并提供3D基础描述的方法，实现自动化、专业质量的视频创作，应用于房地产和旅游领域，且不需要专业知识或设备。&lt;h4&gt;方法&lt;/h4&gt;通过受已知相机位姿约束的扩散过程生成平滑视频轨迹；将这些信息整合到视觉语言模型中以生成基于3D的描述；使用3D高斯溅射技术合成最终视频，沿轨迹渲染新视图；提出包含1200多个房屋导览视频的HouseTour数据集，包括相机位姿、3D重建和房地产描述。&lt;h4&gt;主要发现&lt;/h4&gt;将3D相机轨迹整合到文本生成过程中，比独立处理每个任务的方法性能更好；研究评估了单独和端到端的性能，并引入了一种新的联合度量标准。&lt;h4&gt;结论&lt;/h4&gt;HouseTour方法实现了自动化、专业质量的视频创作，可应用于房地产和旅游领域，且不需要专业知识和设备。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了HouseTour，一种从描绘现有3D空间的图像集合中生成具有空间感知的3D相机轨迹和自然语言摘要的方法。与现有的视觉语言模型不同，后者在几何推理方面存在困难，我们的方法通过受已知相机位姿约束的扩散过程生成平滑的视频轨迹，并将这些信息整合到视觉语言模型中以生成基于3D的描述。我们使用3D高斯溅射技术合成最终视频，沿轨迹渲染新视图。为了支持这项任务，我们提出了HouseTour数据集，其中包含超过1200个带有相机位姿、3D重建和房地产描述的房屋导览视频。实验表明，将3D相机轨迹整合到文本生成过程中，比独立处理每个任务的方法性能更好。我们评估了单独和端到端的性能，并引入了一种新的联合度量标准。我们的工作实现了自动化、专业质量的视频创作，可用于房地产和旅游应用，无需专业知识或设备。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从一组带有已知相机位置的图像中自动生成类似专业房地产视频游览的平滑3D相机轨迹和描述性文本摘要的问题。这个问题在现实中很重要，因为房地产视频游览在YouTube上有超过6.24亿个视频，是美国价值3.43万亿美元房地产市场的关键工具，但目前制作这类视频需要专业人员和高昂设备，劳动密集且成本高。在研究上，这个问题也很重要，因为现有的视觉语言模型在几何推理方面存在困难，生成基于3D空间的视频并用结构化语言描述空间特性仍然是一个挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为两个主要任务：3D相机轨迹生成和文本摘要生成。在轨迹生成方面，作者借鉴了Diffuser的扩散过程，但提出了残差扩散器(Residual Diffuser)，将轨迹规划表示为样条插值的残差，这种方法更适合不同房地产布局的交互空间。在文本生成方面，作者基于Qwen2-VL模型构建了Qwen2-VL-3D，使用LoRA微调方法，并将3D空间信息作为第三种模态整合到视觉语言模型中。作者还构建了HouseTour数据集来支持这一任务。整体流程是从输入图像生成轨迹，然后结合轨迹信息和图像生成文本摘要，最后使用3D高斯飞溅技术渲染视频。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过空间感知的3D相机轨迹生成和多模态文本生成来创建类似专业房地产视频游览的系统。整体流程是：1)输入一组带有已知相机位置的图像；2)使用残差扩散器生成平滑的3D相机轨迹；3)使用Qwen2-VL-3D模型结合轨迹信息和图像生成描述性文本摘要；4)使用3D高斯飞溅技术沿着生成的轨迹渲染新视图，合成最终视频。这种方法确保了文本与空间路径对齐，并能生成描述空间布局、功能性和建筑特征的文本，而不仅仅是列出物体。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'空间感知的3D相机轨迹和文本摘要生成'这一新任务；2)开发残差扩散器，将轨迹规划表示为样条插值的残差；3)构建Qwen2-VL-3D模型，整合3D空间信息作为第三种模态；4)创建HouseTour数据集，包含专业房地产描述和真实世界视频轨迹。相比之前的工作，HouseTour的方法在轨迹生成上明确基于已知3D场景几何条件，整体性而非顺序地制定轨迹规划；在文本生成上能处理大量稀疏多图像数据，捕捉完整布局；在数据集上关注空间布局和建筑特征，而非仅关注家具和物体关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HouseTour提出了一种结合扩散过程生成的3D相机轨迹和整合空间信息的视觉语言模型，能够自动生成专业质量的房地产视频游览，无需专业知识或设备，并为此任务构建了一个新的数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce HouseTour, a method for spatially-aware 3D camera trajectory andnatural language summary generation from a collection of images depicting anexisting 3D space. Unlike existing vision-language models (VLMs), whichstruggle with geometric reasoning, our approach generates smooth videotrajectories via a diffusion process constrained by known camera poses andintegrates this information into the VLM for 3D-grounded descriptions. Wesynthesize the final video using 3D Gaussian splatting to render novel viewsalong the trajectory. To support this task, we present the HouseTour dataset,which includes over 1,200 house-tour videos with camera poses, 3Dreconstructions, and real estate descriptions. Experiments demonstrate thatincorporating 3D camera trajectories into the text generation process improvesperformance over methods handling each task independently. We evaluate bothindividual and end-to-end performance, introducing a new joint metric. Our workenables automated, professional-quality video creation for real estate andtouristic applications without requiring specialized expertise or equipment.</description>
      <author>example@mail.com (Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni)</author>
      <guid isPermaLink="false">2510.18054v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.16714v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://scenecot.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCENECOT的新框架和SCENECOT-185K数据集，通过将复杂推理任务分解并结合多模态专家模块，实现了3D场景中的类人推理，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有关于3D大型语言模型的研究仍然难以实现基于场景的问答，主要原因是人类场景-对象基础推理机制的探索不足。&lt;h4&gt;目的&lt;/h4&gt;填补3D场景基础推理研究的空白，通过提出新颖框架实现类人场景-对象推理。&lt;h4&gt;方法&lt;/h4&gt;引入SCENECOT方法将复杂推理任务分解为更简单的问题，基于多模态专家模块构建视觉线索，并开发包含185K个高质量实例的SCENECOT-185K数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在各种复杂的3D场景推理基准测试中，新框架在保持高基础问答连贯性的同时实现了强大的性能。&lt;h4&gt;结论&lt;/h4&gt;首次成功将思维链推理应用于3D场景理解，实现了逐步类人推理，并显示出扩展到更广泛3D场景理解场景的潜力。&lt;h4&gt;翻译&lt;/h4&gt;现有关于3D大型语言模型的研究仍然难以实现基于场景的问答，这主要是由于对类人场景-对象基础推理机制的探索不足。本文通过提出一个新颖框架来弥合这一差距。我们首先在3D场景中引入了一种基础的思维链推理方法，将复杂的推理任务解耦为更简单和可管理的问题，并基于多模态专家模块构建相应的视觉线索。为实现这种方法，我们开发了SCENECOT-185K，这是第一个大规模的基础CoT推理数据集，包含185K个高质量实例。在各种复杂的3D场景推理基准上进行的广泛实验表明，我们的新框架在保持高基础问答连贯性的同时实现了强大的性能。据我们所知，这是首次成功将CoT推理应用于3D场景理解，实现了逐步类人推理，并显示出扩展到更广泛的3D场景理解场景的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D大语言模型在实现基于场景的问答（grounded question-answering）方面的困难，特别是缺乏类人场景-对象推理机制的问题。这个问题在现实中很重要，因为3D场景理解是构建类人智能体的基础能力，而现有模型虽然能生成看似合理的答案，但无法将中间推理步骤与最终结果联系起来，导致grounding-QA一致性差，影响模型在复杂3D环境中的可靠性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了语言领域中的思维链（Chain-of-Thought, CoT）推理方法，观察到CoT通过将复杂问题分解为可管理的子问题，使语言模型在多种任务中表现出色。作者认为这种逐步推理的方式与人类认知过程相似，也符合3D场景中需要的多跳推理。然而，直接将CoT转移到3D场景具有挑战性，因为需要将基于语言的推理与多模态3D场景表示对齐。因此，作者设计了SCENECOT框架，将3D推理分解为四个阶段：任务识别、区域定位、实体定位和基于场景的推理。该方法借鉴了语言领域的CoT、2D视觉语言推理、多模态大型语言模型和专门的视觉定位模型等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂的3D场景推理任务分解为逐步的、基于场景的思维链推理过程，确保每个答案都有明确的基于场景的步骤支持，从而增强grounding-QA一致性。整体实现流程分为四个阶段：1）任务识别和分析，确定回答问题所需的底层任务和初始分析；2）任务相关区域定位，通过方向线索或基于时钟的参考系缩小推理空间；3）实体定位，使用多模态专家模块定位与问题相关的目标对象；4）基于场景的推理，获取候选对象信息并整合为最终答案。技术实现上，SCENECOT建立在多模态大型语言模型基础上，融入专门的3D-VL和2D-VL模型以及符号引擎来支持这种逐步推理结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出SCENECOT框架，首次将思维链推理应用于3D场景理解；2）构建SCENECOT-185K数据集，第一个大规模3D场景思维链推理数据集；3）显著提高grounding-QA一致性，在Beacon3D基准上达到34.7的良好一致性。相比之前工作，SCENECOT采用显式的逐步推理机制而非端到端训练，通过显式强制推理前的定位实现准确答案和高grounding-QA一致性，将3D问答视为多阶段任务而非单步任务，并利用大规模标注的推理轨迹数据集进行训练，使决策过程更透明、更接近人类推理方式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCENECOT通过引入首个大规模3D场景思维链推理数据集和框架，实现了类人、可解释且基于场景的3D推理，显著提高了复杂3D场景问答中的grounding-QA一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing research on 3D Large Language Models (LLMs) still struggles toachieve grounded question-answering, primarily due to the under-exploration ofthe mechanism of human-like scene-object grounded reasoning. This paper bridgesthe gap by presenting a novel framework. We first introduce a groundedChain-of-Thought reasoning method in 3D scenes (SCENECOT), decoupling a complexreasoning task into simpler and manageable problems, and building correspondingvisual clues based on multimodal expert modules. To enable such a method, wedevelop SCENECOT-185K, the first large-scale grounded CoT reasoning dataset,consisting of 185K high-quality instances. Extensive experiments across variouscomplex 3D scene reasoning benchmarks demonstrate that our new frameworkachieves strong performance with high grounding-QA coherence. To the best ofour knowledge, this is the first successful application of CoT reasoning to 3Dscene understanding, enabling step-by-step human-like reasoning and showingpotential for extension to broader 3D scene understanding scenarios.</description>
      <author>example@mail.com (Xiongkun Linghu, Jiangyong Huang, Ziyu Zhu, Baoxiong Jia, Siyuan Huang)</author>
      <guid isPermaLink="false">2510.16714v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>StreamingTOM: Streaming Token Compression for Efficient Video Understanding</title>
      <link>http://arxiv.org/abs/2510.18269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamingTOM是一个解决流式视频视觉语言模型因果性和累积性约束的两阶段框架，通过因果时间减少和在线量化存储技术，实现了高效的流式视频理解。&lt;h4&gt;背景&lt;/h4&gt;与离线处理不同，流式视频视觉语言模型面临两个基本约束：因果性（无法访问未来帧）和累积性（令牌无限增长）。现有方法仅调节后大语言模型的kv-cache，而未解决成本高昂的前大语言模型prefill问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需训练、即插即用的两阶段框架，解决流式视频视觉语言模型的前后大语言模型瓶颈，实现可预测延迟的高效处理。&lt;h4&gt;方法&lt;/h4&gt;StreamingTOM采用两阶段框架：1）因果时间减少：为每帧设定固定预算，基于相邻帧变化和令牌显著性选择令牌，只处理紧凑的视觉子集；2）在线量化存储：以4位格式存储令牌，按需检索相关组并解量化，保持活跃kv-cache有界。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，StreamingTOM实现了15.7倍的kv-cache压缩，1.2倍的更低峰值内存和2倍的更快TTFT。在无需训练的方法中保持最先进精度，在离线基准测试上平均达到63.8%，在RVS上达到55.8%/3.7。&lt;h4&gt;结论&lt;/h4&gt;两阶段方法在具有有界增长的流式视频理解中具有实际优势，有效解决了因果性和累积性约束带来的效率瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;与离线处理不同，流式视频视觉语言模型面临两个基本约束：因果性和累积性。因果性阻止了对离线方法利用的未来帧的访问，而累积性导致令牌无限增长，造成效率瓶颈。然而，现有方法只调节后大语言模型的kv-cache，而保持成本高昂的前大语言模型prefill不变。我们引入了StreamingTOM，这是一个无需训练、即插即用的两阶段框架，通过可预测延迟解决了前大语言模型和后大语言模型的瓶颈。因果时间减少设定了固定的每帧预算，并根据相邻帧变化和令牌显著性选择令牌，通过每帧只处理紧凑的视觉令牌子集而非所有视觉令牌，显著降低了每帧prefill成本。在线量化存储以4位格式存储令牌，按需检索相关组并解量化，无论流长度如何保持活跃kv-cache有界。实验证明，我们的方法相比先前最先进技术实现了15.7倍的kv-cache压缩，1.2倍的更低峰值内存和2倍的更快TTFT。在无需训练的方法中，StreamingTOM在离线基准测试上平均保持63.8%的精度，在RVS上保持55.8%/3.7的精度。这些结果突显了我们的两阶段方法在具有有界增长的流式视频理解中的实际优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unlike offline processing, streaming video vision-language models face twofundamental constraints: causality and accumulation. Causality prevents accessto future frames that offline methods exploit, while accumulation causes tokensto grow unbounded, creating efficiency bottlenecks. However, existingapproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefillunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stageframework that addresses both pre-LLM and post-LLM bottlenecks with predictablelatency. Causal Temporal Reduction imposes a fixed per-frame budget and selectstokens based on adjacent-frame changes and token saliency, drastically reducingper-frame prefill cost by processing only a compact subset of visual tokens perframe instead of all visual tokens. Online Quantized Memory stores tokens in4-bit format, retrieves relevant groups on demand, and dequantizes them,keeping the active kv-cache bounded regardless of stream length. Experimentsdemonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$lower peak memory and $2\times$ faster TTFT compared to prior SOTA.StreamingTOM maintains state-of-the-art accuracy among training-free methodswith an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS.These results highlight the practical benefits of our two-stage approach forefficient streaming video understanding with bounded growth.</description>
      <author>example@mail.com (Xueyi Chen, Keda Tao, Kele Shao, Huan Wang)</author>
      <guid isPermaLink="false">2510.18269v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</title>
      <link>http://arxiv.org/abs/2510.18016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ViBED-Net的新型深度学习框架，用于从视频数据中检测学生在在线学习环境中的参与度，通过结合面部表情和场景上下文信息，实现了73.43%的准确率，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要，但现有的检测方法仍有改进空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从视频数据中准确评估学生参与度的深度学习框架，通过结合面部表情和全场景上下文信息提高检测准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了ViBED-Net双流架构，使用EfficientNetV2进行空间特征提取，分别处理面部区域和整个视频帧；采用LSTM和Transformer编码器进行时间建模；在DAiSEE数据集上评估；应用有针对性的数据增强技术提高代表性不足类别的性能。&lt;h4&gt;主要发现&lt;/h4&gt;ViBED-Net与LSTM结合的变体达到73.43%的准确率，优于现有最先进方法；结合面部感知和场景感知的时空线索显著提高了参与度检测准确性；模块化设计使其可灵活应用于教育、用户体验研究和内容个性化。&lt;h4&gt;结论&lt;/h4&gt;ViBED-Net为视频情感计算提供了可扩展的高性能解决方案，可用于现实世界的参与度分析，推进了视频情感计算领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。我们提出了ViBED-Net（基于视频的参与度检测网络），这是一种新颖的深度学习框架，采用双流架构设计，用于从视频数据中评估学生参与度。ViBED-Net通过EfficientNetV2处理面部裁剪和整个视频帧来捕获面部表情和全场景上下文，进行空间特征提取。然后，使用两种时间建模策略分析这些特征：长短期记忆网络和Transformer编码器。我们的模型在DAiSEE数据集上进行了评估，这是一个大规模的电子学习情感状态识别基准。为了提高代表性不足的参与度类别的性能，我们应用了有针对性的数据增强技术。在测试的变体中，带有LSTM的ViBED-Net实现了73.43%的准确率，优于现有的最先进方法。ViBED-Net证明，结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。其模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。这项工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了视频情感计算的发展。该项目的源代码可在https://github.com/prateek-gothwal/ViBED-Net获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Engagement detection in online learning environments is vital for improvingstudent outcomes and personalizing instruction. We present ViBED-Net(Video-Based Engagement Detection Network), a novel deep learning frameworkdesigned to assess student engagement from video data using a dual-streamarchitecture. ViBED-Net captures both facial expressions and full-scene contextby processing facial crops and entire video frames through EfficientNetV2 forspatial feature extraction. These features are then analyzed over time usingtwo temporal modeling strategies: Long Short-Term Memory (LSTM) networks andTransformer encoders. Our model is evaluated on the DAiSEE dataset, alarge-scale benchmark for affective state recognition in e-learning. To enhanceperformance on underrepresented engagement classes, we apply targeted dataaugmentation techniques. Among the tested variants, ViBED-Net with LSTMachieves 73.43\% accuracy, outperforming existing state-of-the-art approaches.ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporalcues significantly improves engagement detection accuracy. Its modular designallows flexibility for application across education, user experience research,and content personalization. This work advances video-based affective computingby offering a scalable, high-performing solution for real-world engagementanalysis. The source code for this project is available onhttps://github.com/prateek-gothwal/ViBED-Net .</description>
      <author>example@mail.com (Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.18016v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding</title>
      <link>http://arxiv.org/abs/2510.17305v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ARR Rolling Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongInsightBench是首个专门评估模型理解长视频能力的基准测试，整合视觉、音频和文本多模态信息，包含长时长信息密集视频、多样任务场景和严格质量保证流程，实验显示全模态模型在精确时间定位和长距离因果推断任务中仍有挑战。&lt;h4&gt;背景&lt;/h4&gt;现有模型在理解长视频方面的能力缺乏系统评估基准，特别是对于包含丰富人类语言、视角、动作和其他上下文元素的长视频内容。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的基准测试来评估模型在理解长视频时整合视觉、音频和文本多模态信息的能力，重点关注人类语言、视角、动作等上下文元素。&lt;h4&gt;方法&lt;/h4&gt;1) 从开源数据集FineVideo精心筛选约1000个长时长、信息密集的视频，重点关注讲座、访谈和vlog等包含丰富语言元素的内容；2) 设计六种具有挑战性的任务场景，包括事件内任务和事件间任务；3) 开发三步半自动数据质量保证流程，确保合成问题和答案选项的难度与有效性。&lt;h4&gt;主要发现&lt;/h4&gt;全模态模型(OLMs)在需要精确时间定位和长距离因果推断的任务中面临挑战；扩展实验揭示了OLMs多模态融合中存在信息损失和处理偏差问题。&lt;h4&gt;结论&lt;/h4&gt;当前模型在长视频理解方面仍有明显局限性，特别是在需要精确时间定位和长距离因果推断的任务中，多模态融合过程中存在信息损失和处理偏差。&lt;h4&gt;翻译&lt;/h4&gt;我们引入LongInsightBench，这是首个专门用于评估模型理解长视频能力的基准测试，重点关注人类语言、视角、动作和其他上下文元素，同时整合视觉、音频和文本多模态信息。我们的基准在三个关键方面表现出色：a) 长时长、信息密集的视频：我们基于时长限制和视觉与音频模态的信息密度，从开源数据集FineVideo中精心选择了约1000个视频，重点关注包含丰富语言元素的内容，如讲座、访谈和vlog；b) 多样且有挑战性的任务场景：我们设计了六种具有挑战性的任务场景，包括事件内任务和事件间任务；c) 严格且全面的质量保证流程：我们开发了一个三步半自动数据质量保证流程，以确保合成问题和答案选项的难度和有效性。基于LongInsightBench，我们设计了一系列实验。实验结果表明，全模态模型(OLMs)在需要精确时间定位(T-Loc)和长距离因果推断(CE-Caus)的任务中仍面临挑战。扩展实验揭示了OLMs多模态融合中的信息损失和处理偏差。我们的数据集和代码可在https://anonymous.4open.science/r/LongInsightBench-910F/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce \textbf{LongInsightBench}, the first benchmark designed toassess models' ability to understand long videos, with a focus on humanlanguage, viewpoints, actions, and other contextual elements, while integrating\textbf{visual, audio, and text} modalities. Our benchmark excels in three keyareas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully selectapproximately 1,000 videos from open-source datasets FineVideo based onduration limit and the information density of both visual and audio modalities,focusing on content like lectures, interviews, and vlogs, which contain richlanguage elements. \textbf{b) Diverse and Challenging Task Scenarios:} We havedesigned six challenging task scenarios, including both Intra-Event andInter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality AssurancePipelines:} We have developed a three-step, semi-automated data qualityassurance pipeline to ensure the difficulty and validity of the synthesizedquestions and answer options. Based on LongInsightBench, we designed a seriesof experiments. Experimental results shows that Omni-modal models(OLMs) stillface challenge in tasks requiring precise temporal localization (T-Loc) andlong-range causal inference (CE-Caus). Extended experiments reveal theinformation loss and processing bias in multi-modal fusion of OLMs. Our datasetand code is available athttps://anonymous.4open.science/r/LongInsightBench-910F/.</description>
      <author>example@mail.com (ZhaoYang Han, Qihan Lin, Hao Liang, Bowen Chen, Zhou Liu, Wentao Zhang)</author>
      <guid isPermaLink="false">2510.17305v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</title>
      <link>http://arxiv.org/abs/2510.18795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 fiugres&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProCLIP是一种基于课程学习的渐进式视觉-语言对齐框架，旨在解决原始CLIP文本编码器在处理长文本、多语言理解和细粒度语义理解方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;原始CLIP文本编码器受限于77个token的最大输入长度，不支持多语言输入，限制了其在广泛任务中的应用。最近研究尝试用基于LLM的嵌入器替换CLIP文本编码器，但LLM和CLIP的表示空间独立预训练，直接对比学习会破坏CLIP图像编码器的视觉-语言对齐。&lt;h4&gt;目的&lt;/h4&gt;提出ProCLIP框架，有效对齐CLIP图像编码器和基于LLM的嵌入器，解决CLIP文本编码器的局限性。&lt;h4&gt;方法&lt;/h4&gt;ProCLIP首先从CLIP文本编码器向LLM嵌入器蒸馏知识，建立初始对齐；然后通过图像-文本对比调进一步对齐，使用自蒸馏正则化避免过拟合；采用实例语义对齐损失和嵌入结构对齐损失提高对齐效果。&lt;h4&gt;主要发现&lt;/h4&gt;结合知识蒸馏和对比学习的渐进式对齐方法能够有效利用CLIP的预训练知识，同时保持视觉-语言对齐质量。&lt;h4&gt;结论&lt;/h4&gt;ProCLIP框架能够解决CLIP文本编码器的局限性，实现更好的长文本处理、多语言理解和细粒度语义理解能力。&lt;h4&gt;翻译&lt;/h4&gt;原始CLIP文本编码器受限于77个token的最大输入长度，这阻碍了它有效处理长文本和执行细粒度语义理解的能力。此外，CLIP文本编码器不支持多语言输入。所有这些限制显著限制了它在更广泛任务中的适用性。最近的研究试图用基于LLM的嵌入器替换CLIP文本编码器，以增强其在处理长文本、多语言理解和细粒度语义理解方面的能力。然而，由于LLM和CLIP的表示空间是独立预训练的，没有先验对齐，使用对比学习直接对齐会破坏CLIP图像编码器中的内在视觉-语言对齐，导致预训练期间获取的知识利用不足。为应对这一挑战，我们提出了ProCLIP，一种基于课程学习的渐进式视觉-语言对齐框架，有效对齐CLIP图像编码器和基于LLM的嵌入器。具体来说，ProCLIP首先从CLIP文本编码器向基于LLM的嵌入器蒸馏知识，利用CLIP丰富的预训练知识，同时建立LLM嵌入器和CLIP图像编码器之间的初始对齐。随后，ProCLIP通过图像-文本对比调进一步对齐CLIP图像编码器和基于LLM的嵌入器，采用自蒸馏正则化避免过拟合。为了实现更有效的对齐，在表示继承和对比调优过程中使用了实例语义对齐损失和嵌入结构对齐损失。代码可在https://github.com/VisionXLab/ProCLIP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The original CLIP text encoder is limited by a maximum input length of 77tokens, which hampers its ability to effectively process long texts and performfine-grained semantic understanding. In addition, the CLIP text encoder lackssupport for multilingual inputs. All these limitations significantly restrictits applicability across a broader range of tasks. Recent studies haveattempted to replace the CLIP text encoder with an LLM-based embedder toenhance its ability in processing long texts, multilingual understanding, andfine-grained semantic comprehension. However, because the representation spacesof LLMs and the vision-language space of CLIP are pretrained independentlywithout alignment priors, direct alignment using contrastive learning candisrupt the intrinsic vision-language alignment in the CLIP image encoder,leading to an underutilization of the knowledge acquired during pre-training.To address this challenge, we propose ProCLIP, a curriculum learning-basedprogressive vision-language alignment framework to effectively align the CLIPimage encoder with an LLM-based embedder. Specifically, ProCLIP first distillsknowledge from CLIP's text encoder into the LLM-based embedder to leverageCLIP's rich pretrained knowledge while establishing initial alignment betweenthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further alignsthe CLIP image encoder with the LLM-based embedder through image-textcontrastive tuning, employing self-distillation regularization to avoidoverfitting. To achieve a more effective alignment, instance semantic alignmentloss and embedding structure alignment loss are employed during representationinheritance and contrastive tuning. The Code is available athttps://github.com/VisionXLab/ProCLIP</description>
      <author>example@mail.com (Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang)</author>
      <guid isPermaLink="false">2510.18795v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SEAL: Semantic-Aware Hierarchical Learning for Generalized Category Discovery</title>
      <link>http://arxiv.org/abs/2510.18740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了SEAL框架，通过利用自然层次结构和创新的对比学习方法解决了广义类别发现中的挑战，在多个基准测试上实现了最先进的性能，并展示了良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;广义类别发现在部分标记数据集上的目标是分类所有未标记图像，无论它们属于已知还是未知类别。现有方法通常依赖于单层语义或手动设计的抽象层次结构，限制了泛化能力和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的局限性，引入一个由自然存在且易于访问的层次结构指导的语义感知层次学习框架(SEAL)。&lt;h4&gt;方法&lt;/h4&gt;在SEAL框架中，提出层次语义引导的软对比学习方法，利用层次相似性生成信息丰富的软负样本；同时设计跨粒度一致性(CGC)模块，对齐不同粒度级别的预测。&lt;h4&gt;主要发现&lt;/h4&gt;SEAL在细粒度基准测试上持续实现了最先进的性能，包括SSB基准、Oxford-Pet和Herbarium19数据集，并在粗粒度数据集上进一步展示了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SEAL框架通过利用自然层次结构和创新的对比学习方法，有效解决了广义类别发现中的挑战，提高了模型的泛化能力和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文研究了广义类别发现(GCD)的问题。给定一个部分标记的数据集，GCD旨在对所有未标记图像进行分类，无论它们属于已知类别还是未知类别。现有方法通常依赖于单层语义或手动设计的抽象层次结构，这限制了它们的泛化能力和可扩展性。为了解决这些局限性，我们引入了一个由自然存在且易于访问的层次结构指导的语义感知层次学习框架(SEAL)。在SEAL中，我们提出了一种层次语义引导的软对比学习方法，利用层次相似性生成信息丰富的软负样本，解决了传统对比损失将所有负样本同等对待的局限性。此外，还设计了一个跨粒度一致性(CGC)模块，用于对齐不同粒度级别的预测。SEAL在细粒度基准测试上持续实现了最先进的性能，包括SSB基准、Oxford-Pet和Herbarium19数据集，并在粗粒度数据集上进一步展示了泛化能力。项目页面：https://visual-ai.github.io/seal/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates the problem of Generalized Category Discovery (GCD).Given a partially labelled dataset, GCD aims to categorize all unlabelledimages, regardless of whether they belong to known or unknown classes. Existingapproaches typically depend on either single-level semantics or manuallydesigned abstract hierarchies, which limit their generalizability andscalability. To address these limitations, we introduce a SEmantic-awarehierArchical Learning framework (SEAL), guided by naturally occurring andeasily accessible hierarchical structures. Within SEAL, we propose aHierarchical Semantic-Guided Soft Contrastive Learning approach that exploitshierarchical similarity to generate informative soft negatives, addressing thelimitations of conventional contrastive losses that treat all negativesequally. Furthermore, a Cross-Granularity Consistency (CGC) module is designedto align the predictions from different levels of granularity. SEALconsistently achieves state-of-the-art performance on fine-grained benchmarks,including the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, andfurther demonstrates generalization on coarse-grained datasets. Project page:https://visual-ai.github.io/seal/</description>
      <author>example@mail.com (Zhenqi He, Yuanpei Liu, Kai Han)</author>
      <guid isPermaLink="false">2510.18740v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</title>
      <link>http://arxiv.org/abs/2510.18637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages main text, 17 pages total&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ε-Seg是一种创新的分层变分自编码器方法，结合了中心区域掩蔽、稀疏标签对比学习、高斯混合模型先验和无聚类标签预测等技术，用于解决电子显微镜图像的生物样本语义分割挑战，特别是在标签稀疏的情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;电子显微镜(EM)图像的生物样本语义分割在生命科学中仍然是一个挑战。EM数据捕获生物结构的细节，有时复杂到即使是人类观察者也会感到难以处理。&lt;h4&gt;目的&lt;/h4&gt;引入一种名为ε-Seg的方法，用于解决EM图像的语义分割问题，特别是在标签稀疏的情况下(总图像数据的0.05%或更少)。&lt;h4&gt;方法&lt;/h4&gt;基于分层变分自编码器(HVAEs)的方法，采用中心区域掩蔽和修复损失来学习鲁棒和代表性的嵌入，使用对比学习和高斯混合模型先验来塑造潜在空间，并通过MLP语义分割头直接从潜在嵌入预测类标签，而不是聚类潜在嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;ε-Seg在两个密集的生物组织EM数据集上展示了经验结果，该方法也适用于荧光显微镜数据，能够在复杂的生物图像数据上实现具有竞争力的稀疏监督分割结果，即使只有有限的训练标签可用。&lt;h4&gt;结论&lt;/h4&gt;ε-Seg是一种有效的方法，可以在标签稀疏的情况下进行生物图像的语义分割，为生命科学领域的图像分析提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;生物样本的电子显微镜(EM)图像语义分割在生命科学中仍然是一个挑战。EM数据捕获生物结构的细节，有时复杂到即使是人类观察者也会感到难以处理。我们引入ε-Seg，一种基于分层变分自编码器(HVAEs)的方法，采用中心区域掩蔽、稀疏标签对比学习(CL)、高斯混合模型(GMM)先验和无聚类标签预测。中心区域掩蔽和修复损失鼓励模型学习鲁棒和代表性的嵌入来区分所需的类别，即使训练标签稀疏(占总图像数据的0.05%或更少)。为了获得最佳性能，我们采用CL和GMM先验来塑造HVAE的潜在空间，使得编码的输入斑块倾向于关于我们希望区分的语义类别进行聚类。最后，我们不是对潜在嵌入进行聚类以进行语义分割，而是提出一个MLP语义分割头来直接从潜在嵌入预测类标签。我们在两个密集的生物组织EM数据集上展示了ε-Seg和基线方法的经验结果，并证明了我们的方法在荧光显微镜数据上的适用性。我们的结果表明，即使在只有有限训练标签可用的情况下，ε-Seg也能够在复杂的生物图像数据上实现具有竞争力的稀疏监督分割结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of electron microscopy (EM) images of biologicalsamples remains a challenge in the life sciences. EM data captures details ofbiological structures, sometimes with such complexity that even human observerscan find it overwhelming. We introduce {\epsilon}-Seg, a method based onhierarchical variational autoencoders (HVAEs), employing center-region masking,sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior,and clustering-free label prediction. Center-region masking and the inpaintingloss encourage the model to learn robust and representative embeddings todistinguish the desired classes, even if training labels are sparse (0.05% ofthe total image data or less). For optimal performance, we employ CL and a GMMprior to shape the latent space of the HVAE such that encoded input patchestend to cluster wrt. the semantic classes we wish to distinguish. Finally,instead of clustering latent embeddings for semantic segmentation, we propose aMLP semantic segmentation head to directly predict class labels from latentembeddings. We show empirical results of {\epsilon}-Seg and baseline methods on2 dense EM datasets of biological tissues and demonstrate the applicability ofour method also on fluorescence microscopy data. Our results show that{\epsilon}-Seg is capable of achieving competitive sparsely-supervisedsegmentation results on complex biological image data, even if only limitedamounts of training labels are available.</description>
      <author>example@mail.com (Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug)</author>
      <guid isPermaLink="false">2510.18637v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder</title>
      <link>http://arxiv.org/abs/2510.18583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CovMatch是一种可扩展的多模态数据集蒸馏框架，通过联合优化两个编码器实现更强的跨模态对齐和改进的性能，在Flickr30K和COCO数据集上优于最先进方法，仅使用500个合成对即可实现高达6.8%的检索精度提升。&lt;h4&gt;背景&lt;/h4&gt;多模态数据集蒸馏旨在合成少量图像-文本对以高效训练大规模视觉-语言模型。虽然数据集蒸馏在单模态任务中显示出前景，但扩展到多模态对比学习面临关键挑战：学习跨模态对齐和管理大型编码器的高计算成本。&lt;h4&gt;目的&lt;/h4&gt;提出CovMatch框架，解决先前方法中语义对齐受限和性能扩展瓶颈的问题，实现更有效的多模态数据集蒸馏。&lt;h4&gt;方法&lt;/h4&gt;CovMatch通过使真实和合成特征的跨协方差对齐，同时正则化每个模态内的特征分布。与先前方法不同，CovMatch能够联合优化两个编码器，而非仅更新图像编码器和文本投影层。&lt;h4&gt;主要发现&lt;/h4&gt;先前方法通过冻结文本编码器来解决可扩展性问题，但这种方法严重限制了语义对齐，成为性能扩展的瓶颈。&lt;h4&gt;结论&lt;/h4&gt;在Flickr30K和COCO上评估，CovMatch优于最先进的多模态蒸馏方法，仅使用500个合成对即可实现高达6.8%的检索精度绝对提升。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据集蒸馏旨在合成一组小的图像-文本对，以实现大规模视觉-语言模型的高效训练。虽然数据集蒸馏在单模态任务中显示出前景，但将其扩展到多模态对比学习存在关键挑战：学习跨模态对齐和管理大型编码器的高计算成本。先前的方法通过冻结文本编码器并仅更新图像编码器和文本投影层来解决可扩展性问题。然而，我们发现这严重限制了语义对齐，成为性能扩展的瓶颈。我们提出CovMatch，一种可扩展的数据集蒸馏框架，使真实和合成特征的跨协方差对齐，同时正则化每个模态内的特征分布。与先前的方法不同，CovMatch能够联合优化两个编码器，从而实现更强的跨模态对齐和改进的性能。在Flickr30K和COCO上评估，CovMatch优于最先进的多模态蒸馏方法，仅使用500个合成对即可实现高达6.8%的检索精度绝对提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal dataset distillation aims to synthesize a small set of image-textpairs that enables efficient training of large-scale vision-language models.While dataset distillation has shown promise in unimodal tasks, extending it tomultimodal contrastive learning presents key challenges: learning cross-modalalignment and managing the high computational cost of large encoders. Priorapproaches address scalability by freezing the text encoder and update only theimage encoder and text projection layer. However, we find this severely limitssemantic alignment and becomes a bottleneck for performance scaling. We proposeCovMatch, a scalable dataset distillation framework that aligns thecross-covariance of real and synthetic features while regularizing featuredistributions within each modality. Unlike prior approaches, CovMatch enablesjoint optimization of both encoders, leading to stronger cross-modal alignmentand improved performance. Evaluated on Flickr30K and COCO, CovMatch outperformsstate-of-the-art multimodal distillation methods and achieves up to 6.8%absolute gains in retrieval accuracy using only 500 synthetic pairs.</description>
      <author>example@mail.com (Yongmin Lee, Hye Won Chung)</author>
      <guid isPermaLink="false">2510.18583v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>AWSPNet: Attention-based Dual-Tree Wavelet Scattering Prototypical Network for MIMO Radar Target Recognition and Jamming Suppression</title>
      <link>http://arxiv.org/abs/2510.18422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 figures, The code is available in  https://github.com/jiaxuanzhi/AwspNet&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AWSPNet的新型深度学习框架，用于雷达目标识别和干扰抑制。该方法结合了双树复小波变换、注意力机制、预训练网络和监督对比学习，在低信噪比环境下表现优异，具有良好的特征可分离性和泛化能力。通过与滑动窗口方法的集成，形成完整且实用的干扰识别与抑制系统。&lt;h4&gt;背景&lt;/h4&gt;基于数字射频存储器的电子对抗措施日益增多，对雷达系统的生存能力和有效性构成重大威胁。这些干扰器能生成大量欺骗性虚假目标，淹没雷达的处理能力并掩盖真实目标。因此，稳健地区分真实目标和复杂干扰信号的能力，特别是在低信噪比环境下，显得尤为重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够稳健区分真实目标和复杂干扰信号的框架，特别关注低信噪比环境下的性能，实现雷达目标识别和干扰抑制的同时处理。&lt;h4&gt;方法&lt;/h4&gt;提出了基于注意力的双树小波散射原型网络(AWSPNet)，利用双树复小波变换提取对噪声和信号平移具有内在鲁棒性的特征，通过注意力机制和预训练骨干网络进一步优化这些特征。采用监督对比学习策略解决标记数据有限的问题并增强泛化能力，使用原型网络进行分类。通过t-SNE可视化提供网络内部工作的物理解释，并将AWSPNet与时域滑动窗口方法集成形成完整算法。&lt;h4&gt;主要发现&lt;/h4&gt;在-6 dB信噪比条件下，AWSPNet达到了90.45%的准确率。t-SNE可视化显示模型不同阶段的特征可分离性良好，集成算法不仅能识别还能有效抑制各种类型的干扰。&lt;h4&gt;结论&lt;/h4&gt;AWSPNet在复杂电磁环境中具有实际应用潜力，能够有效处理低信噪比环境下的目标识别和干扰抑制问题。&lt;h4&gt;翻译&lt;/h4&gt;基于数字射频存储器的电子对抗措施日益增多，对雷达系统的生存能力和有效性构成重大威胁。这些干扰器能生成大量欺骗性虚假目标，淹没雷达的处理能力并掩盖真实目标。因此，稳健地区分真实目标和复杂干扰信号的能力，特别是在低信噪比环境下，显得尤为重要。本文介绍了基于注意力的双树小波散射原型网络(AWSPNet)，一种为同时进行雷达目标识别和干扰抑制而设计的深度学习框架。AWSPNet的核心是一个编码器，它利用双树复小波变换提取对噪声和信号平移具有内在鲁棒性的特征。这些特征通过注意力机制和预训练骨干网络得到进一步优化。为了解决标记数据有限的问题并增强泛化能力，我们在训练阶段采用了监督对比学习策略。分类由原型网络执行，该网络在少样本学习场景中特别有效，能够快速适应新的信号类型。我们通过大量实验证明了该方法的有效性。结果显示，AWSPNet在-6 dB信噪比下达到90.45%的准确率。此外，我们通过t-SNE可视化提供了网络内部工作的物理解释，分析了模型不同阶段特征的可分离性。最后，通过将AWSPNet与时域滑动窗口方法集成，我们提出了一个不仅能识别还能有效抑制各种类型干扰的完整算法，从而验证了其在复杂电磁环境中实际应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing of digital radio frequency memory based electroniccountermeasures poses a significant threat to the survivability andeffectiveness of radar systems. These jammers can generate a multitude ofdeceptive false targets, overwhelming the radar's processing capabilities andmasking targets. Consequently, the ability to robustly discriminate betweentrue targets and complex jamming signals, especially in low signal-to-noiseratio (SNR) environments, is of importance. This paper introduces theattention-based dual-tree wavelet scattering prototypical network (AWSPNet), adeep learning framework designed for simultaneous radar target recognition andjamming suppression. The core of AWSPNet is the encoder that leverages thedual-tree complex wavelet transform to extract features that are inherentlyrobust to noise and signal translations. These features are further refined byan attention mechanism and a pre-trained backbone network. To address thechallenge of limited labeled data and enhance generalization, we employ asupervised contrastive learning strategy during the training phase. Theclassification is performed by a prototypical network, which is particularlyeffective in few-shot learning scenarios, enabling rapid adaptation to newsignal types. We demonstrate the efficacy of our approach through extensiveexperiments. The results show that AWSPNet achieves 90.45\% accuracy at -6 dBSNR. Furthermore, we provide a physical interpretation of the network's innerworkings through t-SNE visualizations, which analyze the feature separabilityat different stages of the model. Finally, by integrating AWSPNet with atime-domain sliding window approach, we present a complete algorithm capable ofnot only identifying but also effectively suppressing various types of jamming,thereby validating its potential for practical application in complexelectromagnetic environments.</description>
      <author>example@mail.com (Yizhen Jia, Siyao Xiao, Wenkai Jia, Hui Chen, Wen-Qin Wang)</author>
      <guid isPermaLink="false">2510.18422v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net</title>
      <link>http://arxiv.org/abs/2510.18326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to a SN journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于注意力的Bhattacharyya-Hellinger特征聚合网络(ATTBHFA-Net)，用于解决灾害图像分类中的少样本学习问题。该方法通过线性组合Bhattacharyya系数和Hellinger距离来比较和聚合特征概率分布，形成鲁棒的原型，并提出了基于Bhattacharyya-Hellinger距离的对比损失。实验表明，该方法在四个FSL基准和两个灾害图像数据集上表现出优越的有效性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;自然和人为灾害的频率增加需要先进的视觉识别技术分析关键摄影数据。人工智能和弹性计算系统的进步使快速准确的灾害分类对有效救援行动变得至关重要。然而，灾害背景下的视觉识别面临数据有限且多样的挑战，难以收集和整理全面的高质量灾害图像。&lt;h4&gt;目的&lt;/h4&gt;解决灾害图像分类中的数据稀缺问题，提高少样本学习在灾害图像分类中的性能，克服灾害图像高类内变异和类间相似性的挑战。&lt;h4&gt;方法&lt;/h4&gt;引入了基于注意力的Bhattacharyya-Hellinger特征聚合网络(ATTBHFA-Net)，线性组合Bhattacharyya系数和Hellinger距离来比较和聚合特征概率分布形成鲁棒原型。Bhattacharyya系数作为对比边界增强类间可分性，Hellinger距离对同类对齐进行正则化。提出基于Bhattacharyya-Hellinger距离的对比损失作为余弦相似度损失的分布对应物，与分类交叉熵结合使用提高FSL性能。&lt;h4&gt;主要发现&lt;/h4&gt;在四个FSL基准和两个灾害图像数据集上的实验表明，ATTBHFA-Net与现有方法相比具有优越的有效性和泛化能力。该方法能够有效处理灾害图像的高类内变异和类间相似性问题，提高少样本学习在灾害图像分类中的性能。&lt;h4&gt;结论&lt;/h4&gt;ATTBHFA-Net为灾害图像分类中的少样本学习提供了有效解决方案。通过结合Bhattacharyya系数和Hellinger距离，该方法能够形成更鲁棒的原型，有效处理灾害图像的复杂特征分布，显著提高分类性能。&lt;h4&gt;翻译&lt;/h4&gt;自然和人为灾害频率的增加需要能够分析关键摄影数据的先进视觉识别技术。随着人工智能和弹性计算系统的进步，快速准确的灾害分类对高效救援行动变得至关重要。然而，由于数据有限且多样，难以收集和整理全面的高质量灾害图像，灾害背景下的视觉识别面临重大挑战。少样本学习为数据稀缺问题提供了有前景的方法，但现有的FSL研究主要依赖于缺乏遥感灾害图像的通用基准数据集，限制了其实际有效性。此外，灾害图像表现出高的类内变异和类间相似性，阻碍了基于度量的传统FSL方法的性能。为解决这些问题，本文引入了基于注意力的Bhattacharyya-Hellinger特征聚合网络(ATTBHFA-Net)，该网络线性组合Bhattacharyya系数和Hellinger距离来比较和聚合特征概率分布，形成鲁棒的原型。Bhattacharyya系数作为对比边界增强类间可分性，而Hellinger距离对同类对齐进行正则化。该框架类似于对比学习，但在概率分布上运行，而不是嵌入特征点。此外，提出了基于Bhattacharyya-Hellinger距离的对比损失，作为余弦相似度损失的分布对应物，与分类交叉熵结合使用，显著提高FSL性能。在四个FSL基准和两个灾害图像数据集上的实验表明，与现有方法相比，ATTBHFA-Net具有优越的有效性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing frequency of natural and human-induced disasters necessitatesadvanced visual recognition techniques capable of analyzing criticalphotographic data. With progress in artificial intelligence and resilientcomputational systems, rapid and accurate disaster classification has becomecrucial for efficient rescue operations. However, visual recognition indisaster contexts faces significant challenges due to limited and diverse datafrom the difficulties in collecting and curating comprehensive, high-qualitydisaster imagery. Few-Shot Learning (FSL) provides a promising approach to datascarcity, yet current FSL research mainly relies on generic benchmark datasetslacking remote-sensing disaster imagery, limiting its practical effectiveness.Moreover, disaster images exhibit high intra-class variation and inter-classsimilarity, hindering the performance of conventional metric-based FSL methods.To address these issues, this paper introduces the Attention-basedBhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net), whichlinearly combines the Bhattacharyya coefficient and Hellinger distances tocompare and aggregate feature probability distributions for robust prototypeformation. The Bhattacharyya coefficient serves as a contrastive margin thatenhances inter-class separability, while the Hellinger distance regularizessame-class alignment. This framework parallels contrastive learning butoperates over probability distributions rather than embedded feature points.Furthermore, a Bhattacharyya-Hellinger distance-based contrastive loss isproposed as a distributional counterpart to cosine similarity loss, usedjointly with categorical cross-entropy to significantly improve FSLperformance. Experiments on four FSL benchmarks and two disaster image datasetsdemonstrate the superior effectiveness and generalization of ATTBHFA-Netcompared to existing approaches.</description>
      <author>example@mail.com (Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong)</author>
      <guid isPermaLink="false">2510.18326v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title>
      <link>http://arxiv.org/abs/2510.16219v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SentinelNet，一个用于多智能体系统中主动检测和缓解恶意行为的去中心化框架，通过基于信用的检测器和对比学习，实现了高精度的恶意智能体检测和系统准确性恢复。&lt;h4&gt;背景&lt;/h4&gt;恶意智能体对基于大型语言模型的多智能体系统构成重大威胁，影响其可靠性和决策能力。现有防御措施通常采用被动设计或集中式架构，存在单点故障风险。&lt;h4&gt;目的&lt;/h4&gt;解决现有防御措施的不足，提出一个去中心化的框架，用于主动检测和缓解多智能体协作中的恶意行为。&lt;h4&gt;方法&lt;/h4&gt;提出SentinelNet框架，为每个智能体配备基于信用的检测器，通过对比学习在增强的对抗性辩论轨迹上训练，实现自主评估消息可信度，并通过bottom-k消除进行动态邻居排名，抑制恶意通信。同时生成模拟多样威胁的对抗性轨迹，以克服攻击数据稀缺问题。&lt;h4&gt;主要发现&lt;/h4&gt;在多智能体系统基准测试中，SentinelNet实现了近乎完美的恶意智能体检测，两轮辩论内检测率接近100%，从被破坏的基线恢复了95%的系统准确性，并在不同领域和攻击模式中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SentinelNet为保护协作式多智能体系统建立了一种新的范式，通过去中心化设计解决了单点故障问题，提高了系统的安全性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;恶意智能体对基于大型语言模型的多智能体系统的可靠性和决策能力构成重大威胁。现有防御措施通常因被动设计或引入单点故障风险的集中式架构而不足。为应对这些挑战，我们提出了SentinelNet，这是第一个用于主动检测和缓解多智能体协作中恶意行为的去中心化框架。SentinelNet为每个智能体配备基于信用的检测器，通过在增强的对抗性辩论轨迹上进行对比学习训练，实现自主评估消息可信度，并通过bottom-k消除进行动态邻居排名以抑制恶意通信。为克服攻击数据稀缺问题，它生成模拟多样威胁的对抗性轨迹，确保训练的鲁棒性。在多智能体系统基准测试中，SentinelNet实现了近乎完美的恶意智能体检测，两轮辩论内接近100%，并从被破坏的基线恢复了95%的系统准确性。通过在不同领域和攻击模式中表现出强大的泛化能力，SentinelNet为保护协作式多智能体系统建立了一种新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malicious agents pose significant threats to the reliability anddecision-making capabilities of Multi-Agent Systems (MAS) powered by LargeLanguage Models (LLMs). Existing defenses often fall short due to reactivedesigns or centralized architectures which may introduce single points offailure. To address these challenges, we propose SentinelNet, the firstdecentralized framework for proactively detecting and mitigating maliciousbehaviors in multi-agent collaboration. SentinelNet equips each agent with acredit-based detector trained via contrastive learning on augmented adversarialdebate trajectories, enabling autonomous evaluation of message credibility anddynamic neighbor ranking via bottom-k elimination to suppress maliciouscommunications. To overcome the scarcity of attack data, it generatesadversarial trajectories simulating diverse threats, ensuring robust training.Experiments on MAS benchmarks show SentinelNet achieves near-perfect detectionof malicious agents, close to 100% within two debate rounds, and recovers 95%of system accuracy from compromised baselines. By exhibiting stronggeneralizability across domains and attack patterns, SentinelNet establishes anovel paradigm for safeguarding collaborative MAS.</description>
      <author>example@mail.com (Yang Feng, Xudong Pan)</author>
      <guid isPermaLink="false">2510.16219v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction</title>
      <link>http://arxiv.org/abs/2510.18773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures. Accepted at the NeurIPS 2025 Workshop on  Tackling Climate Change with Machine Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了如何利用地理空间基础模型来预测城市热岛效应，并通过微调模型来评估缓解策略的有效性。&lt;h4&gt;背景&lt;/h4&gt;随着城市化和气候变化的发展，城市热岛效应变得越来越频繁和严重。传统机器学习模型因数据有限，尤其是在服务不足的地区，往往产生不准确的预测。&lt;h4&gt;目的&lt;/h4&gt;为了制定有效的城市热岛缓解计划，需要详细的气温数据，而地理空间基础模型提供了一种有希望的替代方案，表现出强大的泛化能力且只需少量微调。&lt;h4&gt;方法&lt;/h4&gt;研究通过量化绿色空间的冷却效果建立城市热模式的实证真实基准，并将其与模型预测比较以评估模型准确性。随后对基础模型进行微调，预测未来气候情景下的地表温度，并通过模拟修复演示其实际应用价值。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型为评估数据稀缺地区的城市热岛缓解策略提供了一种强大的方式。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以帮助支持更具气候适应能力的城市建设。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化和气候变化的推进，城市热岛效应正变得更加频繁和严重。为了制定有效的缓解计划，城市需要详细的气温数据，然而传统的机器学习模型因数据有限，尤其是在服务不足的地区，往往产生不准确的预测。基于全球非结构化数据训练的地理空间基础模型提供了一种有希望的替代方案，表现出强大的泛化能力且只需少量微调。在本研究中，通过量化绿色空间的冷却效果并建立城市热模式的实证真实基准，将其与模型预测进行比较以评估模型准确性。随后对基础模型进行微调以预测未来气候情景下的地表温度，并通过模拟修复演示其实际价值。结果表明，基础模型为评估数据稀缺地区的城市热岛缓解策略提供了一种强大方式，以支持更具气候适应能力的城市建设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As urbanization and climate change progress, urban heat island effects arebecoming more frequent and severe. To formulate effective mitigation plans,cities require detailed air temperature data, yet conventional machine learningmodels with limited data often produce inaccurate predictions, particularly inunderserved areas. Geospatial foundation models trained on global unstructureddata offer a promising alternative by demonstrating strong generalization andrequiring only minimal fine-tuning. In this study, an empirical ground truth ofurban heat patterns is established by quantifying cooling effects from greenspaces and benchmarking them against model predictions to evaluate the model'saccuracy. The foundation model is subsequently fine-tuned to predict landsurface temperatures under future climate scenarios, and its practical value isdemonstrated through a simulated inpainting that highlights its role formitigation support. The results indicate that foundation models offer apowerful way for evaluating urban heat island mitigation strategies indata-scarce regions to support more climate-resilient cities.</description>
      <author>example@mail.com (Jannis Fleckenstein, David Kreismann, Tamara Rosemary Govindasamy, Thomas Brunschwiler, Etienne Vos, Mattia Rigotti)</author>
      <guid isPermaLink="false">2510.18773v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Language Balance in Code-Switching Speech</title>
      <link>http://arxiv.org/abs/2510.18724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型基础模型在代码转换测试案例中表现不佳，原因可能是代码转换时刻不频繁且第二语言嵌入微妙。研究提出通过为训练过程提供标签并利用语言差异突出代码转换点，减轻生成过程中的上下文偏差，提高模型鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型在标准基准测试中取得了令人印象深刻的结果，但在代码转换测试案例中仍然表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提高大型基础模型在代码转换任务中的表现，解决模型在识别和预测代码转换点时的困难。&lt;h4&gt;方法&lt;/h4&gt;利用嵌入语言和主要语言之间的差异来突出代码转换点，为训练过程提供标签，采用简单有效的可微分替代方法减轻生成过程中的上下文偏差。&lt;h4&gt;主要发现&lt;/h4&gt;大型基础模型在代码转换测试案例中表现不佳的原因是代码转换时刻不频繁且第二语言嵌入微妙；通过提供标签和突出转换点，模型能够更正确地预测转换位置，替换错误减少。&lt;h4&gt;结论&lt;/h4&gt;通过为训练过程提供标签并利用语言差异突出代码转换点，可以有效提高大型基础模型在代码转换任务中的鲁棒性，减轻生成过程中的上下文偏差这一核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;尽管在标准基准测试上取得了令人印象深刻的结果，大型基础模型仍然难以应对代码转换测试案例。当数据稀缺不能作为性能不佳的通常理由时，原因可能在于代码转换时刻的不频繁出现，其中第二语言的嵌入显得微妙。与其期望模型自己学习这种不频繁性，不如为训练过程提供标签。评估模型在代码转换数据上的性能需要仔细定位代码转换点，在这些点上识别错误最为关键，以便分析强调在这些时刻发生的错误。基于这一观察，我们利用嵌入语言和主要语言之间的差异来突出这些代码转换点，从而强调在这些位置的学习。这种简单而有效的可微分替代方法减轻了生成过程中的上下文偏差——代码转换中的核心挑战，从而提高了模型的鲁棒性。我们在阿拉伯语和中文-英语方面的实验表明，模型能够更正确地预测转换位置，表现为替换错误的减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite achieving impressive results on standard benchmarks, largefoundational models still struggle against code-switching test cases. When datascarcity cannot be used as the usual justification for poor performance, thereason may lie in the infrequent occurrence of code-switched moments, where theembedding of the second language appears subtly. Instead of expecting themodels to learn this infrequency on their own, it might be beneficial toprovide the training process with labels. Evaluating model performance oncode-switching data requires careful localization of code-switching pointswhere recognition errors are most consequential, so that the analysisemphasizes mistakes occurring at those moments. Building on this observation,we leverage the difference between the embedded and the main language tohighlight those code-switching points and thereby emphasize learning at thoselocations. This simple yet effective differentiable surrogate mitigates contextbias during generation -- the central challenge in code-switching -- therebyimproving the model's robustness. Our experiments with Arabic andChinese-English showed that the models are able to predict the switching placesmore correctly, reflected by the reduced substitution error.</description>
      <author>example@mail.com (Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel)</author>
      <guid isPermaLink="false">2510.18724v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian Low-Rank Factorization for Robust Model Adaptation</title>
      <link>http://arxiv.org/abs/2510.18723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用贝叶斯因式分解适配器来适应语音基础模型，以处理代码切换场景，实现在特定领域适应的同时保留基础模型的通用性能。&lt;h4&gt;背景&lt;/h4&gt;大型语音基础模型在许多领域表现出色，但需要适应本地需求如代码切换(说话者在同一话语中混合使用多种语言)。直接微调这些模型存在过拟合风险，可能会覆盖基础模型的广泛能力。&lt;h4&gt;目的&lt;/h4&gt;解决语音基础模型在适应特定领域时保留通用性能的挑战，避免过拟合和灾难性遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;探索贝叶斯因式分解适配器用于语音基础模型，通过将先验设置接近零来实现更稀疏的适应矩阵，从而在适应特定领域的同时保留通用性能。将这种方法应用于Whisper模型，并在不同的多语言代码切换场景中进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示方法仅产生最小的适应损失，同时显著减少了基础模型的灾难性遗忘。与LoRA相比，该方法在新领域上仅下降4%，同时实现了54%的向后增益。&lt;h4&gt;结论&lt;/h4&gt;贝叶斯适应方法在微调语音基础模型时非常有效，可以在不牺牲泛化能力的情况下实现特定领域的适应。&lt;h4&gt;翻译&lt;/h4&gt;大型语音基础模型在许多领域实现了强大的性能，但它们通常需要适应处理本地需求，如代码切换，即说话者在同一话语中混合语言。直接微调这些模型存在对目标领域过拟合的风险，并可能覆盖基础模型的广泛能力。为解决这一挑战，我们探索了用于语音基础模型的贝叶斯因式分解适配器，它们将先验设置接近零，以实现更稀疏的适应矩阵，从而在适应特定领域的同时保留通用性能。我们将这种方法应用于Whisper模型，并在不同的多语言代码切换场景中进行评估。我们的结果显示，仅产生最小的适应损失，同时显著减少了基础模型的灾难性遗忘。与LoRA相比，我们的方法在新领域上仅下降4%，同时实现了54%的向后增益。这些发现强调了贝叶斯适应在微调语音基础模型而不牺牲泛化能力方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large speech foundation models achieve strong performance across manydomains, but they often require adaptation to handle local needs such ascode-switching, where speakers mix languages within the same utterance. Directfine-tuning of these models risks overfitting to the target domain andoverwriting the broad capabilities of the base model. To address thischallenge, we explore Bayesian factorized adapters for speech foundationmodels, which place priors near zero to achieve sparser adaptation matrices andthereby retain general performance while adapting to specific domains. We applyour approach to the Whisper model and evaluate on different multilingualcode-switching scenarios. Our results show only minimal adaptation loss whilesignificantly reducing catastrophic forgetting of the base model. Compared toLoRA, our method achieves a backward gain of 54% with only a 4% drop on the newdomain. These findings highlight the effectiveness of Bayesian adaptation forfine-tuning speech foundation models without sacrificing generalization.</description>
      <author>example@mail.com (Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel)</author>
      <guid isPermaLink="false">2510.18723v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</title>
      <link>http://arxiv.org/abs/2510.18632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了3DThinker框架，解决了视觉语言模型从有限视角理解3D空间关系的挑战，通过两阶段训练实现3D思维能力，无需3D先验输入或显式标记的3D数据，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在多模态任务中取得显著进展，但从有限视角理解3D空间关系仍是重大挑战。先前方法依赖纯文本或2D视觉线索，其有限表示能力阻碍了需要3D空间想象力的任务性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在3D空间关系理解上的局限性，开发一个能像人类一样在推理过程中利用图像中丰富几何信息的框架。&lt;h4&gt;方法&lt;/h4&gt;提出3DThinker框架，首个在推理过程中实现3D思维而无需任何3D先验输入的框架。训练分两阶段：首先通过监督训练将VLM生成的3D潜变量与3D基础模型对齐；然后仅基于结果信号优化整个推理轨迹，优化底层3D思维能力。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的广泛实验表明，3DThinker始终优于强大的基线模型，为将3D表示统一到多模态推理中提供了新视角。&lt;h4&gt;结论&lt;/h4&gt;3DThinker框架成功解决了从有限视角理解3D空间关系的挑战，不依赖3D先验输入或显式标记的3D数据，通过两阶段训练实现了有效的3D思维能力。&lt;h4&gt;翻译&lt;/h4&gt;尽管视觉语言模型的最新进展在广泛的多模态任务中取得了显著进步，但从有限视角理解3D空间关系仍然是一个重大挑战。先前的推理方法通常依赖纯文本（如拓扑认知图）或2D视觉线索。然而，它们有限的表示能力阻碍了需要3D空间想象力的特定任务性能。为解决这一限制，我们提出了3DThinker，一个能像人类一样在推理过程中有效利用图像中丰富几何信息的框架。我们的框架是首个在推理过程中实现3D思维而无需任何3D先验输入的框架，并且不依赖显式标记的3D数据进行训练。具体来说，我们的训练包括两个阶段。首先，我们进行监督训练，将VLM推理时生成的3D潜变量与3D基础模型（如VGGT）的3D潜变量对齐。然后，我们仅基于结果信号优化整个推理轨迹，从而优化底层的3D思维能力。在多个基准测试上的广泛实验表明，3DThinker始终优于强大的基线模型，并为将3D表示统一到多模态推理中提供了新视角。我们的代码将在https://github.com/zhangquanchen/3DThinker上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型(VLMs)从有限视角理解3D空间关系的挑战。这个问题很重要，因为空间理解是机器与真实3D世界交互(如具身AI、自动驾驶)的关键能力，这些系统通常依赖于多视角观察，需要从有限视角想象完整场景并进行空间推理。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人类认知机制的启发，特别是心理意象的认知机制。他们借鉴了Mirage框架利用图像嵌入进行监督训练的思想，以及现有的认知地图构建方法如MindCube和Ego3D。作者发现现有方法要么依赖纯文本或2D视觉线索，要么需要辅助模态或外部工具，因此设计3DThinker框架让VLMs能像人类一样在推理过程中进行3D空间想象。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让VLMs在推理过程中生成紧凑的3D潜在嵌入作为3D令牌，模拟人类在空间推理中想象的3D场景。整体实现流程分为两阶段：1)监督训练阶段，将VLM生成的3D潜在与3D基础模型对齐，使用3D对齐损失和交叉熵损失优化；2)强化训练阶段，使用基于结果的信号优化整个采样轨迹，保持3D潜在对齐的同时优化推理过程中的3D视觉令牌。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次引入'3D思维'框架，无需密集标注数据；2)提出两阶段训练框架，从特征对齐到基于结果信号学习几何感知；3)增强模型可解释性，能从潜在空间恢复3D表示；4)在多个基准测试中表现优异。相比之前工作，3DThinker不依赖纯文本或2D视觉线索，不需要辅助模态或外部工具，不依赖真实图像监督，且在推理时无需外部3D先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3DThinker首次让视觉语言模型能够在推理过程中进行3D空间想象，通过两阶段训练框架实现了从有限视角图像理解3D几何关系的能力，无需依赖外部3D先验或密集标注数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Though recent advances in vision-language models (VLMs) have achievedremarkable progress across a wide range of multimodal tasks, understanding 3Dspatial relationships from limited views remains a significant challenge.Previous reasoning methods typically rely on pure text (e.g., topologicalcognitive maps) or on 2D visual cues. However, their limited representationalcapacity hinders performance in specific tasks that require 3D spatialimagination. To address this limitation, we propose 3DThinker, a framework thatcan effectively exploits the rich geometric information embedded within imageswhile reasoning, like humans do. Our framework is the first to enable 3Dmentaling during reasoning without any 3D prior input, and it does not rely onexplicitly labeled 3D data for training. Specifically, our training consists oftwo stages. First, we perform supervised training to align the 3D latentgenerated by VLM while reasoning with that of a 3D foundation model (e.g.,VGGT). Then, we optimize the entire reasoning trajectory solely based onoutcome signals, thereby refining the underlying 3D mentaling. Extensiveexperiments across multiple benchmarks show that 3DThinker consistentlyoutperforms strong baselines and offers a new perspective toward unifying 3Drepresentations into multimodal reasoning. Our code will be available athttps://github.com/zhangquanchen/3DThinker.</description>
      <author>example@mail.com (Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang)</author>
      <guid isPermaLink="false">2510.18632v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents</title>
      <link>http://arxiv.org/abs/2510.18608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出应用持续学习和组合性原则，以促进开发更灵活、高效和智能的AI解决方案，解决基础模型在适应动态现实世界场景时需要重新训练整个模型的问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型在语言、视觉、机器人控制等多种任务中带来了前所未有的结果。这些模型能够处理大量数据，提取和开发丰富的表示，这些表示可以跨不同领域和模态使用。&lt;h4&gt;目的&lt;/h4&gt;提出应用持续学习和组合性原则，以促进开发更灵活、高效和智能的AI解决方案。&lt;h4&gt;方法&lt;/h4&gt;应用持续学习和组合性原则。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现。&lt;h4&gt;结论&lt;/h4&gt;通过应用持续学习和组合性原则，可以开发更灵活、高效和智能的AI解决方案，解决基础模型在适应动态场景时需要重新训练的问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型的诞生在从语言到视觉，再到机器人控制的广泛任务中带来了前所未有的结果。这些模型能够处理大量数据，并提取和开发丰富的表示，这些表示可以跨不同领域和模态使用。然而，它们在适应动态、现实世界场景方面仍然存在问题，需要从头开始重新训练整个模型。在这项工作中，我们提出应用持续学习和组合性原则，以促进开发更灵活、高效和智能的AI解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The birth of Foundation Models brought unprecedented results in a wide rangeof tasks, from language to vision, to robotic control. These models are able toprocess huge quantities of data, and can extract and develop richrepresentations, which can be employed across different domains and modalities.However, they still have issues in adapting to dynamic, real-world scenarioswithout retraining the entire model from scratch. In this work, we propose theapplication of Continual Learning and Compositionality principles to foster thedevelopment of more flexible, efficient and smart AI solutions.</description>
      <author>example@mail.com (Luigi Quarantiello, Elia Piccoli, Jack Bell, Malio Li, Giacomo Carfì, Eric Nuertey Coleman, Gerlando Gramaglia, Lanpei Li, Mauro Madeddu, Irene Testa, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2510.18608v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Decoding Dynamic Visual Experience from Calcium Imaging via Cell-Pattern-Aware SSL</title>
      <link>http://arxiv.org/abs/2510.18516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;POYO-SSL是一种创新的神经科学自监督学习方法，通过专注于可预测神经元并利用数据异质性，实现了比传统方法更好的性能和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在神经科学领域有很大潜力，因为缺乏大规模、标签一致的神经数据集。大多数神经数据集包含异构群体，混合了稳定、可预测的细胞和高度随机、刺激依赖的细胞，这使得在自监督学习中识别一致的活动模式变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督预训练方法，利用神经数据的异质性来改进预训练并实现规模效益。&lt;h4&gt;方法&lt;/h4&gt;POYO-SSL仅在可预测的(统计规律性的)神经元上进行预训练，这些神经元通过简单的更高阶统计量(偏度和峰度)在预训练分割中识别，然后在不可预测的群体上进行微调，用于下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;在Allen Brain Observatory数据集上，POYO-SSL比从头开始训练获得了约12-13%的相对提升，显示出与模型大小相关的平滑、单调的扩展性。相比之下，现有最先进的基线在模型大小增加时趋于平稳或不稳定。&lt;h4&gt;结论&lt;/h4&gt;通过将可预测性作为构建数据饮食的明确指标，POYO-SSL将异质性从负债转变为资产，为可扩展的神经解码提供了一种稳健的、基于生物学的方法，为神经动力学的基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习在神经科学应用中具有巨大潜力，这归因于缺乏大规模、标签一致的神经数据集。然而，大多数神经数据集包含异构群体，混合了稳定、可预测的细胞和高度随机、刺激依赖的细胞，这使得在自监督学习中识别一致的活动模式变得困难。因此，自监督预训练尚未在神经数据上显示出明显的规模效益。在这里，我们提出了一种自监督预训练的新方法POYO-SSL，它利用神经数据的异质性来改进预训练并实现规模效益。具体而言，在POYO-SSL中，我们仅在可预测的(统计规律性)神经元上进行预训练——这些神经元通过简单的更高阶统计量(偏度和峰度)在预训练分割中识别，然后在不可预测的群体上进行微调，用于下游任务。在Allen Brain Observatory数据集上，这种策略比从头开始训练获得了约12-13%的相对提升，并显示出与模型大小相关的平滑、单调的扩展性。相比之下，现有最先进的基线在模型大小增加时趋于平稳或不稳定。通过将可预测性作为构建数据饮食的明确指标，POYO-SSL将异质性从负债转变为资产，为可扩展的神经解码提供了一种稳健的、基于生物学的方法，并为神经动力学的基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) holds a great deal of promise for applicationsin neuroscience, due to the lack of large-scale, consistently labeled neuraldatasets. However, most neural datasets contain heterogeneous populations thatmix stable, predictable cells with highly stochastic, stimulus-contingent ones,which has made it hard to identify consistent activity patterns during SSL. Asa result, self-supervised pretraining has yet to show clear signs of benefitsfrom scale on neural data. Here, we present a novel approach to self-supervisedpretraining, POYO-SSL that exploits the heterogeneity of neural data to improvepre-training and achieve benefits of scale. Specifically, in POYO-SSL wepretrain only on predictable (statistically regular) neurons-identified on thepretraining split via simple higher-order statistics (skewness andkurtosis)-then we fine-tune on the unpredictable population for downstreamtasks. On the Allen Brain Observatory dataset, this strategy yieldsapproximately 12-13% relative gains over from-scratch training and exhibitssmooth, monotonic scaling with model size. In contrast, existingstate-of-the-art baselines plateau or destabilize as model size increases. Bymaking predictability an explicit metric for crafting the data diet, POYO-SSLturns heterogeneity from a liability into an asset, providing a robust,biologically grounded recipe for scalable neural decoding and a path towardfoundation models of neural dynamics.</description>
      <author>example@mail.com (Sangyoon Bae, Mehdi Azabou, Jiook Cha, Blake Richards)</author>
      <guid isPermaLink="false">2510.18516v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models</title>
      <link>http://arxiv.org/abs/2510.18457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and models available at: https://github.com/tianciB/VFM-VAE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种视觉基础模型变分自编码器(VFM-VAE)方法，解决了潜在扩散模型中视觉tokenizer与视觉基础模型对齐的鲁棒性问题，通过多尺度潜在融合和渐进分辨率重建技术，实现了高质量图像重建，并在较少训练epochs内取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;潜在扩散模型(LDMs)的性能严重依赖于其视觉tokenizer的质量。近期研究尝试通过蒸馏方法整合视觉基础模型(VFMs)，但这种方法会削弱与原始VFM的鲁棒性对齐，导致潜在表示在分布偏移下语义偏离。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接整合视觉基础模型到潜在扩散模型中的方法，避免蒸馏方法带来的对齐鲁棒性问题，同时实现高质量图像重建和高效训练。&lt;h4&gt;方法&lt;/h4&gt;提出视觉基础模型变分自编码器(VFM-VAE)，重新设计解码器结构，采用多尺度潜在融合和渐进分辨率重建模块，从空间上粗糙的VFM特征实现高质量重建。同时提供扩散训练期间表示动力学的综合分析，引入SE-CKNNA指标作为诊断工具，并开发联合tokenizer-扩散对齐策略。&lt;h4&gt;主要发现&lt;/h4&gt;通过重新设计VFM-VAE解码器结构和联合对齐策略，系统在仅80个epoch内达到2.20的gFID(无CFG)，比之前的tokenizer快10倍；继续训练至640个epoch后，进一步达到1.62的gFID(无CFG)。直接VFM整合被证明是LDMs的优越范式。&lt;h4&gt;结论&lt;/h4&gt;直接整合视觉基础模型到潜在扩散模型中的方法比传统的蒸馏方法更有效，能够保持与原始VFM的鲁棒性对齐，实现高质量图像重建，并显著加速训练过程。&lt;h4&gt;翻译&lt;/h4&gt;潜在扩散模型(LDMs)的性能严重依赖于其视觉tokenizer的质量。虽然近期工作已经探索通过蒸馏整合视觉基础模型(VFMs)，但我们发现这种方法存在一个根本缺陷：它不可避免地会削弱与原始VFM的对齐鲁棒性，导致对齐的潜在表示在分布偏移下发生语义偏离。在本文中，我们通过提出一种更直接的方法来绕过蒸馏：视觉基础模型变分自编码器(VFM-VAE)。为了解决VFM的语义焦点与像素级保真度需求之间的内在张力，我们使用多尺度潜在融合和渐进分辨率重建块重新设计了VFM-VAE解码器，使得从空间上粗糙的VFM特征中实现高质量重建成为可能。此外，我们提供了扩散训练期间表示动力学的全面分析，引入了所提出的SE-CKNNA指标作为这一诊断的更精确工具。这一分析使我们能够开发联合tokenizer-扩散对齐策略，显著加速了收敛。我们在tokenizer设计和训练策略方面的创新带来了卓越的性能和效率：我们的系统在仅80个epoch内达到2.20的gFID(无CFG)(比之前的tokenizer快10倍)。继续训练至640个epoch后，它进一步获得了1.62的gFID(无CFG)，确立了直接VFM整合作为LDMs的优越范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of Latent Diffusion Models (LDMs) is critically dependent onthe quality of their visual tokenizer. While recent works have exploredincorporating Vision Foundation Models (VFMs) via distillation, we identify afundamental flaw in this approach: it inevitably weakens the robustness ofalignment with the original VFM, causing the aligned latents to deviatesemantically under distribution shifts. In this paper, we bypass distillationby proposing a more direct approach: Vision Foundation Model VariationalAutoencoder (VFM-VAE). To resolve the inherent tension between the VFM'ssemantic focus and the need for pixel-level fidelity, we redesign the VFM-VAEdecoder with Multi-Scale Latent Fusion and Progressive ResolutionReconstruction blocks, enabling high-quality reconstruction from spatiallycoarse VFM features. Furthermore, we provide a comprehensive analysis ofrepresentation dynamics during diffusion training, introducing the proposedSE-CKNNA metric as a more precise tool for this diagnosis. This analysis allowsus to develop a joint tokenizer-diffusion alignment strategy that dramaticallyaccelerates convergence. Our innovations in tokenizer design and trainingstrategy lead to superior performance and efficiency: our system reaches a gFID(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).With continued training to 640 epochs, it further attains a gFID (w/o CFG) of1.62, establishing direct VFM integration as a superior paradigm for LDMs.</description>
      <author>example@mail.com (Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng)</author>
      <guid isPermaLink="false">2510.18457v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Automated urban waterlogging assessment and early warning through a mixture of foundation models</title>
      <link>http://arxiv.org/abs/2510.18425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Nature&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了UrbanWaterlogging Assessment (UWAssess)框架，利用基础模型自动识别监控图像中的内涝区域并生成结构化评估报告，通过半监督微调和思维链提示策略解决数据稀缺问题，显著提高了感知性能，并能够生成可靠的文本报告，支持城市管理和灾害应对。&lt;h4&gt;背景&lt;/h4&gt;气候变化加剧，城市内涝对全球公共安全和基础设施构成越来越严重的威胁。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动识别内涝区域并生成结构化评估报告的框架，替代依赖人工报告的传统监测方法。&lt;h4&gt;方法&lt;/h4&gt;设计UrbanWaterlogging Assessment (UWAssess)框架，采用半监督微调策略和思维链(CoT)提示策略，以解决标记数据稀缺问题，释放基础模型在数据稀缺下游任务中的潜力。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的视觉基准测试中评估显示感知性能有显著提高；基于GPT的评估确认了UWAssess能够生成可靠的文本报告，准确描述内涝范围、深度、风险和影响。&lt;h4&gt;结论&lt;/h4&gt;UWAssess的双重能力使内涝监测从感知转变为生成，多个基础模型的协作框架为智能和可扩展系统奠定了基础，支持城市管理、灾害应对和气候韧性。&lt;h4&gt;翻译&lt;/h4&gt;随着气候变化加剧，城市内涝对全球公共安全和基础设施构成越来越严重的威胁。然而，现有的监测方法主要依赖人工报告，无法提供及时和全面的评估。在本研究中，我们提出了UrbanWaterlogging Assessment (UWAssess)，这是一个基础模型驱动的框架，可以自动识别监控图像中的内涝区域并生成结构化评估报告。为解决标记数据稀缺的问题，我们设计了一种半监督微调策略和思维链(CoT)提示策略，以释放基础模型在数据稀缺下游任务中的潜力。在具有挑战性的视觉基准测试中的评估表明感知性能有显著提高。基于GPT的评估确认了UWAssess能够生成可靠的文本报告，准确描述内涝范围、深度、风险和影响。这种双重能力使内涝监测从感知转变为生成，而多个基础模型的协作框架为智能和可扩展系统奠定了基础，支持城市管理、灾害应对和气候韧性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With climate change intensifying, urban waterlogging poses an increasinglysevere threat to global public safety and infrastructure. However, existingmonitoring approaches rely heavily on manual reporting and fail to providetimely and comprehensive assessments. In this study, we present UrbanWaterlogging Assessment (UWAssess), a foundation model-driven framework thatautomatically identifies waterlogged areas in surveillance images and generatesstructured assessment reports. To address the scarcity of labeled data, wedesign a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)prompting strategy to unleash the potential of the foundation model fordata-scarce downstream tasks. Evaluations on challenging visual benchmarksdemonstrate substantial improvements in perception performance. GPT-basedevaluations confirm the ability of UWAssess to generate reliable textualreports that accurately describe waterlogging extent, depth, risk and impact.This dual capability enables a shift of waterlogging monitoring from perceptionto generation, while the collaborative framework of multiple foundation modelslays the groundwork for intelligent and scalable systems, supporting urbanmanagement, disaster response and climate resilience.</description>
      <author>example@mail.com (Chenxu Zhang, Fuxiang Huang, Lei Zhang)</author>
      <guid isPermaLink="false">2510.18425v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</title>
      <link>http://arxiv.org/abs/2510.18318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Earth AI是一种创新的地理空间AI系统，结合了三个关键领域的基础模型和Gemini驱动的推理引擎，能够有效处理大量多样的地理空间数据，提供深入的见解和预测能力，特别是在危机情境中表现突出。&lt;h4&gt;背景&lt;/h4&gt;地理空间数据为理解我们的星球提供了巨大潜力。然而，这些数据的巨大规模和多样性，以及不同的分辨率、时间尺度和稀疏性，给彻底分析和解释带来了显著挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍Earth AI，这是一种地理空间AI模型和智能代理推理系统，旨在显著提高我们从数据中解锁关于地球的新见解的能力。&lt;h4&gt;方法&lt;/h4&gt;基于三个关键领域的基础模型——全球规模图像、人口和环境，以及一个由Gemini驱动的智能推理引擎。还开发了一个由Gemini驱动的代理，能够对多个基础模型以及大型地理空间数据源和工具进行联合推理，以处理复杂的多步骤查询。&lt;h4&gt;主要发现&lt;/h4&gt;严格基准测试证明了基础模型的强大功能和新型能力；当这些模型一起使用时，它们为地理空间推理提供了互补价值，协同作用能够解锁卓越的预测能力；在真实世界危机场景基准测试中，代理展示了提供关键及时见解的能力，有效弥合了原始地理空间数据和可操作理解之间的差距。&lt;h4&gt;结论&lt;/h4&gt;Earth AI通过结合基础模型和智能代理推理，能够克服地理空间数据分析中的挑战，提供更深入的见解，特别是在危机情境中，能够将原始数据转化为可操作的理解。&lt;h4&gt;翻译&lt;/h4&gt;地理空间数据为理解我们的星球提供了巨大潜力。然而，这些数据的巨大规模和多样性，以及不同的分辨率、时间尺度和稀疏性，给彻底分析和解释带来了显著挑战。本文介绍了Earth AI，这是一种地理空间AI模型家族和智能代理推理系统，使我们在解锁关于地球的新颖而深刻见解的能力上取得了显著进步。该方法建立在三个关键领域的基础模型之上——全球规模图像、人口和环境，以及一个由Gemini驱动的智能推理引擎。我们展示了严格的基准测试，证明了我们基础模型的强大功能和新型能力，并验证了当一起使用时，它们为地理空间推理提供了互补价值，它们的协同作用能够解锁卓越的预测能力。为了处理复杂的多步骤查询，我们开发了一个由Gemini驱动的代理，能够对多个基础模型以及大型地理空间数据源和工具进行联合推理。在一个新的真实世界危机场景基准测试中，我们的代理展示了提供关键及时见解的能力，有效地弥合了原始地理空间数据和可操作理解之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geospatial data offers immense potential for understanding our planet.However, the sheer volume and diversity of this data along with its variedresolutions, timescales, and sparsity pose significant challenges for thoroughanalysis and interpretation. This paper introduces Earth AI, a family ofgeospatial AI models and agentic reasoning that enables significant advances inour ability to unlock novel and profound insights into our planet. Thisapproach is built upon foundation models across three key domains--Planet-scaleImagery, Population, and Environment--and an intelligent Gemini-poweredreasoning engine. We present rigorous benchmarks showcasing the power and novelcapabilities of our foundation models and validate that when used together,they provide complementary value for geospatial inference and their synergiesunlock superior predictive capabilities. To handle complex, multi-step queries,we developed a Gemini-powered agent that jointly reasons over our multiplefoundation models along with large geospatial data sources and tools. On a newbenchmark of real-world crisis scenarios, our agent demonstrates the ability todeliver critical and timely insights, effectively bridging the gap between rawgeospatial data and actionable understanding.</description>
      <author>example@mail.com (Aaron Bell, Amit Aides, Amr Helmy, Arbaaz Muslim, Aviad Barzilai, Aviv Slobodkin, Bolous Jaber, David Schottlander, George Leifman, Joydeep Paul, Mimi Sun, Nadav Sherman, Natalie Williams, Per Bjornsson, Roy Lee, Ruth Alcantara, Thomas Turnbull, Tomer Shekel, Vered Silverman, Yotam Gigi, Adam Boulanger, Alex Ottenwess, Ali Ahmadalipour, Anna Carter, Charles Elliott, David Andre, Elad Aharoni, Gia Jung, Hassler Thurston, Jacob Bien, Jamie McPike, Juliet Rothenberg, Kartik Hegde, Kel Markert, Kim Philipp Jablonski, Luc Houriez, Monica Bharel, Phing VanLee, Reuven Sayag, Sebastian Pilarski, Shelley Cazares, Shlomi Pasternak, Siduo Jiang, Stone Jiang, Thomas Colthurst, Yang Chen, Yehonathan Refael, Yochai Blau, Yuval Carny, Yael Maguire, Avinatan Hassidim, James Manyika, Tim Thelin, Genady Beryozkin, Gautam Prasad, Luke Barrington, Yossi Matias, Niv Efron, Shravya Shetty)</author>
      <guid isPermaLink="false">2510.18318v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>From Agent Simulation to Social Simulator: A Comprehensive Review (Part 1)</title>
      <link>http://arxiv.org/abs/2510.18271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是基于主体的建模(Agent-Based Modeling, ABM)综合评论的第一部分，重点介绍了ABM的历史发展和经典案例，包括基础模型和社会模拟案例的分类。&lt;h4&gt;背景&lt;/h4&gt;传统物理模拟方法在社会领域面临重大挑战，这促使了ABM的发展。ABM有着自己的发展历史和设计原则。&lt;h4&gt;目的&lt;/h4&gt;帮助读者理解传统物理模拟方法在社会领域面临的挑战，并介绍ABM的基础模型和经典案例。&lt;h4&gt;方法&lt;/h4&gt;详细介绍了模拟社会系统的基础模型，包括个体模型、环境模型和基于规则的模型。&lt;h4&gt;主要发现&lt;/h4&gt;社会模拟的经典案例可分为三类：思想实验、机制探索和平行优化。&lt;h4&gt;结论&lt;/h4&gt;ABM作为一种模拟社会系统的方法，有着自己的历史发展、设计原则、基础模型和经典应用案例。&lt;h4&gt;翻译&lt;/h4&gt;这是基于主体的建模(Agent-Based Modeling, ABM)综合评论的第一部分，重点介绍了ABM的历史发展和经典案例。它首先讨论了ABM的发展历史和设计原则，帮助读者理解传统物理模拟方法在社会领域面临的重大挑战。然后，它详细介绍了模拟社会系统的基础模型，包括个体模型、环境模型和基于规则的模型。最后，它介绍了社会模拟的经典案例，涵盖三种类型：思想实验、机制探索和平行优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is the first part of the comprehensive review, focusing on thehistorical development of Agent-Based Modeling (ABM) and its classic cases. Itbegins by discussing the development history and design principles ofAgent-Based Modeling (ABM), helping readers understand the significantchallenges that traditional physical simulation methods face in the socialdomain. Then, it provides a detailed introduction to foundational models forsimulating social systems, including individual models, environmental models,and rule-based models. Finally, it presents classic cases of social simulation,covering three types: thought experiments, mechanism exploration, and paralleloptimization.</description>
      <author>example@mail.com (Xiao Xue, Deyu Zhou, Ming Zhang, Fei-Yue Wang)</author>
      <guid isPermaLink="false">2510.18271v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
      <link>http://arxiv.org/abs/2510.18214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 4 tables. Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Vision Language Safety Understanding (VLSU)框架，通过细化的严重性分类和组合分析系统评估多模态安全性，揭示了当前模型在联合图像-文本理解方面的弱点。&lt;h4&gt;背景&lt;/h4&gt;当前多模态基础模型的安全评估通常将视觉和语言输入分开处理，忽略了良性内容组合后可能产生的风险；现有方法无法明确区分不安全内容和边缘情况，导致过度屏蔽或对真正有害内容拒绝不足。&lt;h4&gt;目的&lt;/h4&gt;提出一个综合框架来系统评估多模态安全性，通过细化的严重性分类和组合分析识别模型在联合理解方面的缺陷。&lt;h4&gt;方法&lt;/h4&gt;使用多阶段流程结合真实世界图像和人工注释，构建包含8,187个样本、涵盖15个危害类别的大规模基准数据集，评估11个最先进的模型。&lt;h4&gt;主要发现&lt;/h4&gt;模型在清晰的单模态安全信号上准确率达90%以上，但在需要联合图像-文本推理时性能显著下降至20-55%；34%的联合分类错误发生在正确分类单个模态的情况下；模型难以平衡拒绝不安全内容与回应边缘情况；指令框架可减少过度屏蔽但代价是降低对不安全内容的拒绝率。&lt;h4&gt;结论&lt;/h4&gt;该框架暴露了当前模型在联合图像-文本理解方面的弱点和对齐差距，为研究稳健的视觉-语言安全提供了关键的测试平台。&lt;h4&gt;翻译&lt;/h4&gt;多模态基础模型的安全评估通常将视觉和语言输入分开处理，忽略了良性内容组合后可能产生的风险。现有方法也无法明确区分不安全内容和边缘情况，导致对真正有害内容过度屏蔽或拒绝不足。我们提出了Vision Language Safety Understanding (VLSU)，一个通过细化的严重性分类和17种不同安全模式的组合分析来系统评估多模态安全的综合框架。使用多阶段流程结合真实世界图像和人工注释，我们构建了一个包含8,187个样本、跨越15个危害类别的大规模基准。我们对11个最先进模型的评估揭示了系统性的联合理解失败：尽管模型在清晰的单模态安全信号上达到90%以上的准确率，但当需要联合图像-文本推理来确定安全标签时，性能显著下降到20-55%。最关键的是，34%的联合图像-文本安全分类错误发生在正确分类单个模态的情况下，进一步证明了缺乏组合推理能力。此外，我们发现模型难以平衡拒绝不安全内容同时回应值得关注的边缘情况。例如，我们发现指令框架可以将Gemini-1.5对边缘内容的过度屏蔽率从62.4%降低到10.4%，但代价是对不安全内容的拒绝率从90.8%下降到53.9%。总体而言，我们的框架暴露了当前模型在联合图像-文本理解方面的弱点和对齐差距，并为研究稳健的视觉-语言安全的下一个里程碑提供了关键的测试平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety evaluation of multimodal foundation models often treats vision andlanguage inputs separately, missing risks from joint interpretation wherebenign content becomes harmful in combination. Existing approaches also fail todistinguish clearly unsafe content from borderline cases, leading toproblematic over-blocking or under-refusal of genuinely harmful content. Wepresent Vision Language Safety Understanding (VLSU), a comprehensive frameworkto systematically evaluate multimodal safety through fine-grained severityclassification and combinatorial analysis across 17 distinct safety patterns.Using a multi-stage pipeline with real-world images and human annotation, weconstruct a large-scale benchmark of 8,187 samples spanning 15 harm categories.Our evaluation of eleven state-of-the-art models reveals systematic jointunderstanding failures: while models achieve 90%-plus accuracy on clearunimodal safety signals, performance degrades substantially to 20-55% whenjoint image-text reasoning is required to determine the safety label. Mostcritically, 34% of errors in joint image-text safety classification occurdespite correct classification of the individual modalities, furtherdemonstrating absent compositional reasoning capabilities. Additionally, wefind that models struggle to balance refusing unsafe content while stillresponding to borderline cases that deserve engagement. For example, we findthat instruction framing can reduce the over-blocking rate on borderlinecontent from 62.4% to 10.4% in Gemini-1.5, but only at the cost ofunder-refusing on unsafe content with refusal rate dropping from 90.8% to53.9%. Overall, our framework exposes weaknesses in joint image-textunderstanding and alignment gaps in current models, and provides a criticaltest bed to enable the next milestones in research on robust vision-languagesafety.</description>
      <author>example@mail.com (Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng)</author>
      <guid isPermaLink="false">2510.18214v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation</title>
      <link>http://arxiv.org/abs/2510.18213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为EMA-SAM的新型模型，用于在甲状腺乳头状微小癌射频消融超声视频中实现更稳定的肿瘤分割。该模型通过在SAM-2基础上添加基于置信度的指数移动平均指针，解决了传统模型在介入超声应用中的不稳定预测和时间漂移问题。&lt;h4&gt;背景&lt;/h4&gt;甲状腺乳头状微小癌(PTMC)越来越多地使用射频消融(RFA)进行治疗，但在超声视频中准确分割病灶面临挑战，主要由于低对比度、探头引起的运动和热相关伪影等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有Segment Anything Model 2 (SAM-2)在介入超声视频中的不稳定预测和时间漂移问题，实现更稳定和准确的肿瘤分割。&lt;h4&gt;方法&lt;/h4&gt;开发了EMA-SAM，这是SAM-2的轻量级扩展，在记忆库中融入了基于置信度的指数移动平均指针，提供跨帧的稳定肿瘤潜在原型，保持时间一致性的同时能快速适应新情况。&lt;h4&gt;主要发现&lt;/h4&gt;在PTMC-RFA数据集上，EMA-SAM将maxDice从0.82提高到0.86，maxIoU从0.72提高到0.76，同时减少29%的假阳性；在外部基准测试中比SAM-2提高2-5个Dice点；计算开销增加不到0.1%，保持约30FPS的实时性能。&lt;h4&gt;结论&lt;/h4&gt;EMA-SAM是一个稳健高效的框架，能够实现稳定的肿瘤跟踪，弥合了基础模型和介入超声严格需求之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;甲状腺乳头状微小癌(PTMC)越来越多地采用射频消融(RFA)治疗，然而由于低对比度、探头引起的运动和热相关伪影，在超声视频中准确分割病灶仍然困难。最近的Segment Anything Model 2 (SAM-2)在静态图像上表现良好，但其帧独立设计在介入超声中会导致不稳定预测和时间漂移。我们引入了EMA-SAM，这是SAM-2的一个轻量级扩展，在记忆库中融入了基于置信度的指数移动平均指针，提供跨帧的稳定肿瘤潜在原型。这种设计能够在探头压力和气泡遮挡时保持时间一致性，并在清晰证据重新出现时快速适应。在我们精心准备的PTMC-RFA数据集(124分钟，13名患者)上，EMA-SAM将maxDice从0.82(SAM-2)提高到0.86，maxIoU从0.72提高到0.76，同时减少29%的假阳性。在外部基准测试中，包括VTUS和结肠镜视频息肉数据集，EMA-SAM比SAM-2一致提高了2-5个Dice点。重要的是，EMA指针增加了不到0.1%的FLOPs，在单个A100 GPU上保持约30FPS的实时吞吐量。这些结果确立了EMA-SAM作为稳定肿瘤跟踪的稳健高效框架，弥合了基础模型和介入超声严格需求之间的差距。代码可在以下网址获取：https://github.com/mdialameh/EMA-SAM&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Papillary thyroid microcarcinoma (PTMC) is increasingly managed withradio-frequency ablation (RFA), yet accurate lesion segmentation in ultrasoundvideos remains difficult due to low contrast, probe-induced motion, andheat-related artifacts. The recent Segment Anything Model 2 (SAM-2) generalizeswell to static images, but its frame-independent design yields unstablepredictions and temporal drift in interventional ultrasound. We introduce\textbf{EMA-SAM}, a lightweight extension of SAM-2 that incorporates aconfidence-weighted exponential moving average pointer into the memory bank,providing a stable latent prototype of the tumour across frames. This designpreserves temporal coherence through probe pressure and bubble occlusion whilerapidly adapting once clear evidence reappears. On our curated PTMC-RFA dataset(124 minutes, 13 patients), EMA-SAM improves \emph{maxDice} from 0.82 (SAM-2)to 0.86 and \emph{maxIoU} from 0.72 to 0.76, while reducing false positives by29\%. On external benchmarks, including VTUS and colonoscopy video polypdatasets, EMA-SAM achieves consistent gains of 2--5 Dice points over SAM-2.Importantly, the EMA pointer adds \textless0.1\% FLOPs, preserving real-timethroughput of $\sim$30\,FPS on a single A100 GPU. These results establishEMA-SAM as a robust and efficient framework for stable tumour tracking,bridging the gap between foundation models and the stringent demands ofinterventional ultrasound. Codes are available here \hyperref[code{https://github.com/mdialameh/EMA-SAM}.</description>
      <author>example@mail.com (Maryam Dialameh, Hossein Rajabzadeh, Jung Suk Sim, Hyock Ju Kwon)</author>
      <guid isPermaLink="false">2510.18213v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>MACE Foundation Models for Lattice Dynamics: A Benchmark Study on Double Halide Perovskites</title>
      <link>http://arxiv.org/abs/2510.18178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;材料信息学和人工智能的发展催生了MACE基础模型，为无机固体带来通用势能突破。研究使用DFT数据库对四种MACE变体进行基准测试，发现模型准确性随训练数据增加而提高，能更好地预测弱非谐性材料的动态稳定性，主要误差来源是原子力预测中的误差放大。&lt;h4&gt;背景&lt;/h4&gt;材料信息学和人工智能的最新发展催生了材料化学的基础能量模型，以MACE系列基础模型为代表，为无机固体带来了通用势能的重大突破。&lt;h4&gt;目的&lt;/h4&gt;对计算材料科学中的方法开发进行性能基准测试，理解模型局限性，促进模型改进，推动材料理论发展。&lt;h4&gt;方法&lt;/h4&gt;使用作者发表的DFT数据库，包含约2000种立方卤化物双钙钛矿的室温动态稳定性和振动非谐性，对四种MACE基础模型变体进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;模型准确性随训练数据增加而提高；基础模型能更准确地再现弱非谐性材料的动态稳定性；预测动态稳定性的主要误差来自原子力预测误差的放大，而非构型空间采样的差异。&lt;h4&gt;结论&lt;/h4&gt;希望研究结果能激励未来工作朝着更多物理启发的方向发展，以评估基础模型在原子建模中的准确性。&lt;h4&gt;翻译&lt;/h4&gt;材料信息学和人工智能的最新发展催生了材料化学的基础能量模型，如MACE基础模型系列，为无机固体带来了通用势能的重大突破。对于计算材料科学中的所有方法开发，都需要针对特定应用与现有高级数据进行性能基准测试，以理解模型的局限性，从而促进模型开发过程的持续改进，有时会导致材料理论的重大概念飞跃。在此，我们使用自己发表的DFT（密度泛函理论）数据库，包含约2000种立方卤化物双钙钛矿的室温动态稳定性和振动非谐性，对四种不同变体的MACE基础模型进行了基准测试，评估其筛选无机固体动态稳定性的性能。我们的分析表明，正如预期，模型准确性随着更多训练数据的加入而提高。基础模型能更准确地再现弱非谐性材料的动态稳定性（由DFT预测），而非高度非谐性和动力学不稳定的材料。预测动态稳定性的主要误差来源在于预测谐波声子特性时通过计算Hessian矩阵放大原子力误差，而非DFT和基础模型在分子动力学中采样构型空间的差异。我们希望当前的研究发现将激励未来工作朝着更多物理启发的方向发展，以评估基础模型在原子建模中的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments in materials informatics and artificial intelligence hasled to the emergence of foundational energy models for material chemistry, asrepresented by the suite of MACE-based foundation models, bringing asignificant breakthrough in universal potentials for inorganic solids. As toall method developments in computational materials science, performancebenchmarking against existing high-level data with focusing on specificapplications, is critically needed to understand the limitations in the models,thus facilitating the ongoing improvements in the model development process,and occasionally, leading to significant conceptual leaps in materials theory.Here, using our own published DFT (Density Functional Theory) database ofroom-temperature dynamic stability and vibrational anharmonicity for $\sim2000$cubic halide double perovskites, we benchmarked the performances of fourdifferent variants of the MACE foundation models for screening the dynamicstabilities of inorganic solids. Our analysis shows that, as anticipated, themodel accuracy improves with more training data. The dynamic stabilities ofweakly anharmonic materials (as predicted by DFT) are more accuratelyreproduced by the foundation model, than those highly anharmonic anddynamically unstable ones. The predominant source of error in predicting thedynamic stability arises predominantly from the amplification of errors inatomic forces when predicting the harmonic phonon properties through thecomputation of the Hessian matrix, less so is the contribution from possibledifferences in the range of the configurational spaces that are sampled by DFTand the foundation model in molecular dynamics. We hope that our presentfindings will stimulate future works towards more physics-inspired approachesin assessing the accuracy of foundation models for atomistic modelling.</description>
      <author>example@mail.com (Jack Yang, Ziqi Yin, Lei Ao, Sean Li)</author>
      <guid isPermaLink="false">2510.18178v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>MEG-GPT: A transformer-based foundation model for magnetoencephalography data</title>
      <link>http://arxiv.org/abs/2510.18080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MEG-GPT，一种基于transformer的基础模型，用于处理脑磁图数据。通过引入新颖的数据驱动标记化器和在大规模数据集上训练，该模型能生成具有真实脑电特性的数据，并在下游解码任务中表现出色，特别是在跨会话和跨受试者场景中。&lt;h4&gt;背景&lt;/h4&gt;神经科学领域需要建模大规模脑动力学的复杂时空模式，但传统方法无法捕捉脑磁图等模态中的丰富结构。同时，深度学习通过大规模基础模型在语言和视觉等领域已取得显著进展。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于transformer的基础模型MEG-GPT，用于处理脑磁图数据，解决传统方法在处理脑电生理数据方面的局限性，并探索其在计算神经科学和神经解码中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;提出MEG-GPT模型，使用时间注意力和下一时间点预测；引入数据驱动的标记化器处理连续MEG数据，保持高时间分辨率；在大规模MEG数据集上训练模型，包含612名闭眼休息状态的受试者数据；使用标记化的大脑区域时间序列进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;模型能生成具有真实时空频谱特性的数据，包括瞬态事件和群体变异性；在下游解码任务中表现良好，改善了监督预测任务；在跨会话准确率从0.54提高到0.59，跨受试者准确率从0.41提高到0.49；模型可在小标记数据集上高效微调，提升跨受试者解码性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为电生理数据建立了一个强大的基础模型，为计算神经科学和神经解码应用铺平了道路，展示了基础模型在神经科学领域的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;建模大规模脑动力学的复杂时空模式对神经科学至关重要，但传统方法无法捕捉脑磁图等模态中的丰富结构。深度学习的最新进展通过大规模基础模型在语言和视觉等领域实现了显著进步。在此，我们介绍了MEG-GPT，一个基于transformer的基础模型，使用时间注意力和下一时间点预测。为此，我们还引入了一种新颖的数据驱动标记化器用于连续MEG数据，它保留了连续MEG信号的高时间分辨率而无需有损变换。我们在从大规模MEG数据集提取的标记化大脑区域时间序列上训练MEG-GPT，并表明学习到的模型能够生成具有真实时空频谱特性的数据，包括瞬态事件和群体变异性。关键的是，它在下游解码任务中表现良好，改善了下游监督预测任务，在跨会话和跨受试者方面显示出改进的零样本泛化能力。此外，我们表明模型可以在较小的标记数据集上高效微调，以提高跨受试者解码场景中的性能。这项工作为电生理数据建立了一个强大的基础模型，为计算神经科学和神经解码应用铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modelling the complex spatiotemporal patterns of large-scale brain dynamicsis crucial for neuroscience, but traditional methods fail to capture the richstructure in modalities such as magnetoencephalography (MEG). Recent advancesin deep learning have enabled significant progress in other domains, such aslanguage and vision, by using foundation models at scale. Here, we introduceMEG-GPT, a transformer based foundation model that uses time-attention and nexttime-point prediction. To facilitate this, we also introduce a noveldata-driven tokeniser for continuous MEG data, which preserves the hightemporal resolution of continuous MEG signals without lossy transformations. Wetrained MEG-GPT on tokenised brain region time-courses extracted from alarge-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show thatthe learnt model can generate data with realistic spatio-spectral properties,including transient events and population variability. Critically, it performswell in downstream decoding tasks, improving downstream supervised predictiontask, showing improved zero-shot generalisation across sessions (improvingaccuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)compared to a baseline methods. Furthermore, we show the model can beefficiently fine-tuned on a smaller labelled dataset to boost performance incross-subject decoding scenarios. This work establishes a powerful foundationmodel for electrophysiological data, paving the way for applications incomputational neuroscience and neural decoding.</description>
      <author>example@mail.com (Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich)</author>
      <guid isPermaLink="false">2510.18080v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity</title>
      <link>http://arxiv.org/abs/2510.18037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 39th Conference on Neural Information Processing  Systems (NeurIPS 2025) Workshop: Data on the Brain &amp; Mind&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究系统评估了深度学习模型在神经活动预测中的性能，发现深度学习模型优于传统方法，最佳模型可预测未来1.5秒的神经活动，为神经系统的理解和控制提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;神经活动预测对于理解神经系统和实现闭环控制至关重要。虽然深度学习最近在时间序列预测领域取得了最先进进展，但其在神经活动预测中的应用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;弥合深度学习在神经活动预测中应用的差距，系统评估多种深度学习模型在神经活动预测中的性能。&lt;h4&gt;方法&lt;/h4&gt;系统评估了8种概率深度学习模型（包括2种基础模型），将其与4种经典统计模型和2种基线方法进行比较，使用宽场成像技术记录的小鼠皮层自发性神经活动作为数据，在不同的预测时间范围内进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在各种预测时间范围内，几种深度学习模型持续优于经典方法，最佳模型能够提供未来1.5秒内有信息量的预测。&lt;h4&gt;结论&lt;/h4&gt;研究结果指向未来的控制应用，为探索神经活动的内在时间结构开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;神经活动预测对于理解神经系统和实现闭环控制至关重要。虽然深度学习最近在时间序列预测文献中取得了最先进进展，但其在神经活动预测中的应用仍然有限。为了弥合这一差距，我们系统评估了八种概率深度学习模型（包括两种基础模型），这些模型在通用预测基准测试中表现出色。我们在通过宽场成像记录的小鼠皮层自发性神经活动上，将这些模型与四种经典统计模型和两种基线方法进行了比较。在各种预测时间范围内，几种深度学习模型持续优于经典方法，其中最佳模型能够提供未来1.5秒内有信息量的预测。我们的研究结果指向未来的控制应用，并为探索神经活动的内在时间结构开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural activity forecasting is central to understanding neural systems andenabling closed-loop control. While deep learning has recently advanced thestate-of-the-art in the time series forecasting literature, its application toneural activity forecasting remains limited. To bridge this gap, wesystematically evaluated eight probabilistic deep learning models, includingtwo foundation models, that have demonstrated strong performance on generalforecasting benchmarks. We compared them against four classical statisticalmodels and two baseline methods on spontaneous neural activity recorded frommouse cortex via widefield imaging. Across prediction horizons, several deeplearning models consistently outperformed classical approaches, with the bestmodel producing informative forecasts up to 1.5 seconds into the future. Ourfindings point toward future control applications and open new avenues forprobing the intrinsic temporal structure of neural activity.</description>
      <author>example@mail.com (Ziyu Lu, Anna J. Li, Alexander E. Ladd, Pascha Matveev, Aditya Deole, Eric Shea-Brown, J. Nathan Kutz, Nicholas A. Steinmetz)</author>
      <guid isPermaLink="false">2510.18037v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>AION-1: Omnimodal Foundation Model for Astronomical Sciences</title>
      <link>http://arxiv.org/abs/2510.17960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Neural Information Processing Systems (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AION-1，一个天文领域的大规模多模态基础模型家族，能够整合异构的成像、光谱和标量数据，并在多种天文任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在多个领域展现出潜力，但天文学仍缺乏一个统一框架来对其多样化的数据模态进行联合建模。&lt;h4&gt;目的&lt;/h4&gt;开发一个天文领域的大规模多模态基础模型，能够处理天文学中各种不同的数据类型。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段架构：首先进行模态特定的标记化，然后使用基于transformer的跨模态标记序列掩码建模。模型在五个大规模调查数据集上进行预训练，包括Legacy Survey、HSC、SDSS、DESI和Gaia，覆盖超过2亿个天体观测。&lt;h4&gt;主要发现&lt;/h4&gt;使用单个冻结编码器，AION-1在多种下游任务上取得优异表现，包括星系和恒星属性估计、星系形态分类、基于相似性的检索、星系图像分割和光谱超分辨率。&lt;h4&gt;结论&lt;/h4&gt;AION-1提供了构建多模态科学基础模型的可扩展蓝图，能够无缝集成嘈杂的、仪器特定的观测数据。研究团队发布了参数量从3亿到31亿不等的模型变体，并开源了所有代码、标记器、预训练权重和评估套件。&lt;h4&gt;翻译&lt;/h4&gt;尽管基础模型已在各种领域展现出前景，天文学仍然缺乏一个统一的框架来对其高度多样化的数据模态进行联合建模。在本文中，我们提出了AION-1，一个用于天文的大规模多模态基础模型家族。AION-1使用两阶段架构整合异构的成像、光谱和标量数据：模态特定的标记化，随后是基于transformer的跨模态标记序列掩码建模。该模型在五个大规模调查数据集上进行预训练：Legacy Survey、Hyper Suprime-Cam (HSC)、Sloan Digital Sky Survey (SDSS)、Dark Energy Spectroscopic Instrument (DESI)和Gaia。这些数据集涵盖了超过2亿颗恒星、星系和类星体的观测。使用单个冻结编码器，AION-1在广泛的下游任务上取得了强劲结果，包括星系和恒星属性估计、星系形态分类、基于相似性的检索、星系图像分割和光谱超分辨率。我们发布了参数量从3亿到31亿不等的AION-1模型变体。除天文学外，AION-1为多模态科学基础模型提供了可扩展的蓝图，能够无缝集成嘈杂的、仪器特定的观测。所有代码、标记器、预训练权重和轻量级评估套件均在开源许可下发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models have shown promise across a variety of fields,astronomy still lacks a unified framework for joint modeling across its highlydiverse data modalities. In this paper, we present AION-1, a family oflarge-scale multimodal foundation models for astronomy. AION-1 integratesheterogeneous imaging, spectroscopic, and scalar data using a two-stagearchitecture: modality-specific tokenization followed by transformer-basedmasked modeling of cross-modal token sequences. The model is pretrained on fivelarge-scale surveys: Legacy Survey, Hyper Suprime-Cam (HSC), Sloan Digital SkySurvey (SDSS), Dark Energy Spectroscopic Instrument (DESI), and Gaia. Thesespan more than 200 million observations of stars, galaxies, and quasars. With asingle frozen encoder, AION-1 achieves strong results on a broad suite ofdownstream tasks, including galaxy and stellar property estimation, galaxymorphology classification, similarity-based retrieval, galaxy imagesegmentation, and spectral super-resolution. We release AION-1 model variantsranging from 300 M to 3.1 B parameters. Beyond astronomy, AION-1 provides ascalable blueprint for multimodal scientific foundation models that canseamlessly integrate noisy, instrument-specific observations. All code,tokenizers, pretrained weights, and a lightweight evaluation suite are releasedunder an open-source license.</description>
      <author>example@mail.com (Liam Parker, Francois Lanusse, Jeff Shen, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Casserau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Regaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho)</author>
      <guid isPermaLink="false">2510.17960v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Trust in foundation models and GenAI: A geographic perspective</title>
      <link>http://arxiv.org/abs/2510.17942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型（特别是地理学领域）中的信任概念，将信任分为三类：对训练数据的认知信任、对模型功能的操作信任以及对模型开发者的人际信任。论文讨论了信任在地理应用中的含义、挑战、偏见问题、透明度和可解释性的重要性，以及地理信息科学家的独特视角。&lt;h4&gt;背景&lt;/h4&gt;大型预训练机器学习模型已经改变了多个领域对人工智能的理解，包括地理学领域。随着这些模型被越来越多地依赖并用于关键决策，信任已成为讨论中的重要议题，但同时也变得复杂且多方面。&lt;h4&gt;目的&lt;/h4&gt;论文旨在提供一个概念起点，帮助研究人员、从业者和政策制定者更好地理解（生成性）地理人工智能中的信任问题。&lt;h4&gt;方法&lt;/h4&gt;作者将信任概念分为三个类型进行分析：认知信任、操作信任和人际信任，并探讨这些信任类型在地理应用中的独特含义。&lt;h4&gt;主要发现&lt;/h4&gt;信任在基础模型中是一个多方面的概念；信任可分为三种类型：对训练数据的认知信任、对模型功能的操作信任以及对模型开发者的人际信任；文化背景、数据异质性和空间关系等主题对空间科学至关重要，并在发展信任中起重要作用；不同形式的偏见带来了挑战；透明度和可解释性很重要；模型开发中存在伦理责任；地理信息科学家提供了新的视角，呼吁进一步提高透明度、减少偏见并制定区域知情政策。&lt;h4&gt;结论&lt;/h4&gt;随着对基础模型依赖的增加，信任已成为一个复杂但至关重要的概念。通过将信任分类并考虑地理学特有的因素，论文为理解和建立对地理人工智能的信任提供了概念框架。&lt;h4&gt;翻译&lt;/h4&gt;大型预训练机器学习模型已经重塑了我们对多个领域人工智能的理解，包括我们自己的地理学领域。与任何新技术一样，信任在这一讨论中扮演着重要角色。在本章中，我们探讨了基础模型中信任的多方面概念，特别是在地理背景下。随着对这些模型的依赖增加并用于关键决策，信任虽然必不可少，但已成为一个分裂的概念。在这里，我们将信任分为三类：对训练数据的认知信任、对模型功能的操作信任以及对模型开发者的人际信任。每种信任类型都为地理应用带来了独特的含义。文化背景、数据异质性和空间关系等主题是空间科学的基础，并在发展信任中发挥重要作用。本章继续讨论了不同形式偏见带来的挑战、透明度和可解释性的重要性以及模型开发中的伦理责任。最后，强调了地理信息科学家的新颖视角，呼吁进一步提高透明度、减少偏见并制定区域知情政策。简而言之，本章旨在为研究人员、从业者和政策制定者提供一个概念起点，以更好地理解（生成性）地理人工智能中的信任。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-trained machine learning models have reshaped ourunderstanding of artificial intelligence across numerous domains, including ourown field of geography. As with any new technology, trust has taken on animportant role in this discussion. In this chapter, we examine the multifacetedconcept of trust in foundation models, particularly within a geographiccontext. As reliance on these models increases and they become relied upon forcritical decision-making, trust, while essential, has become a fracturedconcept. Here we categorize trust into three types: epistemic trust in thetraining data, operational trust in the model's functionality, andinterpersonal trust in the model developers. Each type of trust brings with itunique implications for geographic applications. Topics such as culturalcontext, data heterogeneity, and spatial relationships are fundamental to thespatial sciences and play an important role in developing trust. The chaptercontinues with a discussion of the challenges posed by different forms ofbiases, the importance of transparency and explainability, and ethicalresponsibilities in model development. Finally, the novel perspective ofgeographic information scientists is emphasized with a call for furthertransparency, bias mitigation, and regionally-informed policies. Simply put,this chapter aims to provide a conceptual starting point for researchers,practitioners, and policy-makers to better understand trust in (generative)GeoAI.</description>
      <author>example@mail.com (Grant McKenzie, Krzysztof Janowicz, Carsten Kessler)</author>
      <guid isPermaLink="false">2510.17942v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing</title>
      <link>http://arxiv.org/abs/2510.18591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络（GNNs）是学习图结构的主导架构，对抗攻击检测是一个重要问题。作者提出使用高效部分求解器替代传统强大求解器的方法，以提高结构鲁棒性，并在多种GNN变体和数据集上评估了其工具RobLight。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）是学习图结构的主导架构。与任何机器学习模型一样，检测对抗性攻击（对手通过小幅扰动输入来改变输出）是一个重要问题。解决对抗鲁棒性问题（确定是否存在此类攻击）的技术最初是为图像分类开发的，但也有适用于其他机器学习架构的变体。&lt;h4&gt;目的&lt;/h4&gt;提高图神经网络结构鲁棒性的最先进水平，通过替代传统方法中使用强大求解器的做法。&lt;h4&gt;方法&lt;/h4&gt;用高效的部分求解器（运行时间为多项式时间但不一定完整）替换强大求解器的使用，开发工具RobLight，并在多种GNN变体和数据集上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;可以通过使用高效的部分求解器替代强大求解器，来改进结构鲁棒性的最先进技术。&lt;h4&gt;结论&lt;/h4&gt;作者的工具RobLight在多种GNN变体和数据集上进行了评估，表明该方法在对抗攻击检测方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）是学习图结构的主导架构。与任何机器学习模型一样，一个重要问题是检测对抗性攻击，即对手可以通过对输入的小幅扰动来改变输出。解决对抗鲁棒性问题（确定是否存在此类攻击）的技术最初是为图像分类开发的，但也有适用于许多其他机器学习架构的变体。在图学习的情况下，攻击模型通常考虑对图结构的更改，而不仅仅是或代替输入的数值特征，该领域最先进的技术通过简化为约束求解来实现，基于强大的求解器（例如用于混合整数编程的求解器）。我们展示了可以通过用高效的部分求解器（运行时间为多项式时间但不一定完整）替换强大求解器的使用，来提高结构鲁棒性的最先进水平。我们在多种GNN变体和数据集上评估了我们的工具RobLight。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are the predominant architecture for learningover graphs. As with any machine learning model, and important issue is thedetection of adversarial attacks, where an adversary can change the output witha small perturbation of the input. Techniques for solving the adversarialrobustness problem - determining whether such an attack exists - wereoriginally developed for image classification, but there are variants for manyother machine learning architectures. In the case of graph learning, the attackmodel usually considers changes to the graph structure in addition to orinstead of the numerical features of the input, and the state of the arttechniques in the area proceed via reduction to constraint solving, working ontop of powerful solvers, e.g. for mixed integer programming. We show that it ispossible to improve on the state of the art in structural robustness byreplacing the use of powerful solvers by calls to efficient partial solvers,which run in polynomial time but may be incomplete. We evaluate our toolRobLight on a diverse set of GNN variants and datasets.</description>
      <author>example@mail.com (Chia-Hsuan Lu, Tony Tan, Michael Benedikt)</author>
      <guid isPermaLink="false">2510.18591v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2510.18473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了知识图谱上的公平感知图神经网络(GNNs)基准研究，从YAGO、DBpedia和Wikidata生成更大规模的数据集，评估不同GNN主干和早期停止条件下的预处理和内处理方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是学习图结构数据的强大工具，但通常对敏感属性产生有偏见的预测。尽管公平感知的GNNs已被研究用于减轻这种偏见，但之前的研究未在知识图谱这一重要应用领域进行评估。&lt;h4&gt;目的&lt;/h4&gt;评估公平感知的GNNs在知识图谱上的表现，并建立相关基准。&lt;h4&gt;方法&lt;/h4&gt;从三个知识图谱(YAGO、DBpedia和Wikidata)生成新的更大规模图数据集，在不同GNN主干和早期停止条件下对预处理和内处理方法进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;(i)知识图谱与现有数据集表现出不同趋势，在公平性与准确性间有更清晰的权衡；(ii)性能不仅受公平感知GNN方法影响，还受GNN主干和早期停止条件显著影响；(iii)预处理方法改善公平性指标，内处理方法提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;知识图谱上的公平性研究需要综合考虑数据集特性、模型架构和训练方法的选择，这些因素共同影响公平性与准确性的权衡。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是学习图结构数据的强大工具，但通常对敏感属性产生有偏见的预测。公平感知的GNNs已被积极研究用于减轻有偏见的预测。然而，之前的研究没有在知识图谱上评估公平感知的GNNs，而知识图谱是许多应用（如推荐系统）中最重要的图之一。因此，我们引入一个关于知识图谱的基准研究。我们从三个知识图谱（YAGO、DBpedia和Wikidata）生成新的图，这些图比公平性研究中使用的现有图数据集大得多。我们在不同的GNN主干和早期停止条件下对预处理和内处理方法进行基准测试。我们发现几个关键见解：(i)知识图谱显示出与现有数据集不同的趋势；在公平感知的GNNs中，与其他图相比，预测准确性和公平性指标之间有更清晰的权衡，(ii)性能不仅受到公平感知的GNN方法的影响，还受到GNN主干和早期停止条件的显著影响，以及(iii)预处理方法通常改善公平性指标，而内处理方法提高预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are powerful tools for learning fromgraph-structured data but often produce biased predictions with respect tosensitive attributes. Fairness-aware GNNs have been actively studied formitigating biased predictions. However, no prior studies have evaluatedfairness-aware GNNs on knowledge graphs, which are one of the most importantgraphs in many applications, such as recommender systems. Therefore, weintroduce a benchmarking study on knowledge graphs. We generate new graphs fromthree knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantlylarger than the existing graph datasets used in fairness studies. We benchmarkinprocessing and preprocessing methods in different GNN backbones and earlystopping conditions. We find several key insights: (i) knowledge graphs showdifferent trends from existing datasets; clearer trade-offs between predictionaccuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii)the performance is largely affected by not only fairness-aware GNN methods butalso GNN backbones and early stopping conditions, and (iii) preprocessingmethods often improve fairness metrics, while inprocessing methods improveprediction accuracy.</description>
      <author>example@mail.com (Yuya Sasaki)</author>
      <guid isPermaLink="false">2510.18473v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study</title>
      <link>http://arxiv.org/abs/2510.18370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究首次对图神经网络集成的专家级多样化技术进行了系统性实证研究，评估了20种多样化策略在14个节点分类基准上的表现，构建并分析了200多个集成变体，为专家训练和图数据上有效混合专家框架的设计提供了指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习关系数据的重要工具，但单一图神经网络在处理现实世界图中存在的异质性时性能受限。近期混合专家框架的进展表明，组合多个具有明显不同泛化模式的多样化图神经网络可以显著提高性能。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在对图神经网络集成的专家级多样化技术进行首个系统性实证研究，评估不同多样化策略的效果，并提供训练最大化多样化专家的机制见解。&lt;h4&gt;方法&lt;/h4&gt;研究评估了20种多样化策略，包括随机重新初始化、超参数调整、架构变化、方向性建模和训练数据分区等，在14个节点分类基准上构建并分析了200多个集成变体，从专家多样性、互补性和集成性能等方面全面评估了每种技术。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了训练最大化多样化专家的机制见解，发现不同多样化策略在产生专家多样性、互补性和提升集成性能方面有不同效果，为专家训练和有效混合专家框架设计提供了可操作的指导。&lt;h4&gt;结论&lt;/h4&gt;该研究通过大规模实证分析，为图神经网络集成的专家级多样化技术提供了系统性理解和实用指导，有助于开发更高效的混合专家框架来处理图数据中的异质性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为学习关系数据的重要工具，然而单一图神经网络的性能往往受到现实世界图中存在的异质性的限制。混合专家框架的最新进展表明，组合多个具有明显不同泛化模式的多样化图神经网络可以显著提高性能。在这项工作中，我们首次对图神经网络集成的专家级多样化技术进行了系统性实证研究。通过在14个节点分类基准上评估20种多样化策略——包括随机重新初始化、超参数调整、架构变化、方向性建模和训练数据分区等，我们构建并分析了200多个集成变体。我们的全面评估从专家多样性、互补性和集成性能等方面检验了每种技术。我们还揭示了训练最大化多样化专家的机制见解。这些发现为图数据上的专家训练和有效混合专家框架的设计提供了可操作的指导。我们的代码可在指定链接获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become essential tools for learning onrelational data, yet the performance of a single GNN is often limited by theheterogeneity present in real-world graphs. Recent advances inMixture-of-Experts (MoE) frameworks demonstrate that assembling multiple,explicitly diverse GNNs with distinct generalization patterns can significantlyimprove performance. In this work, we present the first systematic empiricalstudy of expert-level diversification techniques for GNN ensembles. Evaluating20 diversification strategies -- including random re-initialization,hyperparameter tuning, architectural variation, directionality modeling, andtraining data partitioning -- across 14 node classification benchmarks, weconstruct and analyze over 200 ensemble variants. Our comprehensive evaluationexamines each technique in terms of expert diversity, complementarity, andensemble performance. We also uncovers mechanistic insights into trainingmaximally diverse experts. These findings provide actionable guidance forexpert training and the design of effective MoE frameworks on graph data. Ourcode is available at https://github.com/Hydrapse/bench-gnn-diversification.</description>
      <author>example@mail.com (Gangda Deng, Yuxin Yang, Ömer Faruk Akgül, Hanqing Zeng, Yinglong Xia, Rajgopal Kannan, Viktor Prasanna)</author>
      <guid isPermaLink="false">2510.18370v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Committors without Descriptors</title>
      <link>http://arxiv.org/abs/2510.18018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络的改进版基于committor的方法，用于原子模拟中的稀有事件研究。&lt;h4&gt;背景&lt;/h4&gt;稀有事件的研究是原子模拟中的主要挑战之一，已提出几种增强采样方法。最近建议使用committor来提供稀有事件的精确形式描述。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于committor的方法，促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。&lt;h4&gt;方法&lt;/h4&gt;利用变分标准迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入，方法具有自洽和半自动的优势。&lt;h4&gt;主要发现&lt;/h4&gt;将之前的方法与图神经网络结合，可以直接处理原子坐标而不是描述符，进一步自动化该过程。&lt;h4&gt;结论&lt;/h4&gt;结合图神经网络增强了方法的能力，特别是在处理原子坐标和描述溶剂分子的作用方面，如离子对解离或配体结合。&lt;h4&gt;翻译&lt;/h4&gt;The study of rare events is one of the major challenges in atomistic simulations, and several enhanced sampling methods towards its solution have been proposed. Recently, it has been suggested that the use of the committor, which provides a precise formal description of rare events, could be of use in this context. We have recently followed up on this suggestion and proposed a committor-based method that promotes frequent transitions between the metastable states of the system and allows extensive sampling of the process transition state ensemble. One of the strengths of our approach is being self-consistent and semi-automatic, exploiting a variational criterion to iteratively optimize a neural-network-based parametrization of the committor, which uses a set of physical descriptors as input. Here, we further automate this procedure by combining our previous method with the expressive power of graph neural networks, which can directly process atomic coordinates rather than descriptors. Besides applications on benchmark systems, we highlight the advantages of a graph-based approach in describing the role of solvent molecules in systems, such as ion pair dissociation or ligand binding.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of rare events is one of the major challenges in atomisticsimulations, and several enhanced sampling methods towards its solution havebeen proposed. Recently, it has been suggested that the use of the committor,which provides a precise formal description of rare events, could be of use inthis context. We have recently followed up on this suggestion and proposed acommittor-based method that promotes frequent transitions between themetastable states of the system and allows extensive sampling of the processtransition state ensemble. One of the strengths of our approach is beingself-consistent and semi-automatic, exploiting a variational criterion toiteratively optimize a neural-network-based parametrization of the committor,which uses a set of physical descriptors as input. Here, we further automatethis procedure by combining our previous method with the expressive power ofgraph neural networks, which can directly process atomic coordinates ratherthan descriptors. Besides applications on benchmark systems, we highlight theadvantages of a graph-based approach in describing the role of solventmolecules in systems, such as ion pair dissociation or ligand binding.</description>
      <author>example@mail.com (Peilin Kang, Jintu Zhang, Enrico Trizio, TingJun Hou, Michele Parrinello)</author>
      <guid isPermaLink="false">2510.18018v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>QINNs: Quantum-Informed Neural Networks</title>
      <link>http://arxiv.org/abs/2510.17984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出量子信息神经网络(QINNs)框架，将量子信息概念引入经典模型，通过量子费舍尔信息矩阵(QFIM)作为粒子关联的紧凑表示，在喷注标记任务中提高模型性能，使粒子碰撞分析更加实用、可解释和可扩展。&lt;h4&gt;背景&lt;/h4&gt;经典深度神经网络能够学习强子对撞机数据中的丰富多粒子关联，但其归纳偏差很少基于物理结构。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用框架，将量子信息概念和量子可观测量引入纯经典模型中，增强粒子碰撞分析的量子信息处理能力。&lt;h4&gt;方法&lt;/h4&gt;研究QINNs的一个具体实现，将每个粒子编码为量子比特，使用量子费舍尔信息矩阵(QFIM)作为粒子关联的紧凑、基无关摘要，在图神经网络中将QFIM用作轻量级嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;QFIM能够区分QCD和强子顶喷注，显示出符合物理预期的不同模式，表明QINNs能够捕捉有意义的物理特征。&lt;h4&gt;结论&lt;/h4&gt;QINNs为粒子碰撞的量子信息分析(断层扫描)提供了一种实用、可解释和可扩展的途径，特别是通过增强现有的深度学习方法。&lt;h4&gt;翻译&lt;/h4&gt;经典深度神经网络可以学习强子对撞机数据中的丰富多粒子关联，但它们的归纳偏差很少锚定在物理结构上。我们提出了量子信息神经网络(QINNs)，这是一个通用框架，将量子信息概念和量子可观测量引入纯经典模型。虽然该框架很广泛，但在本文中，我们研究了一个具体实现，将每个粒子编码为量子比特，并使用量子费舍尔信息矩阵(QFIM)作为粒子关联的紧凑、基无关摘要。以喷注标记为案例研究，QFIM在图神经网络中充当轻量级嵌入，提高了模型的表型和可塑性。QFIM揭示了QCD和强子顶喷注的不同模式，这些模式符合物理预期。因此，QINNs为粒子碰撞的量子信息分析(即断层扫描)提供了一条实用、可解释和可扩展的途径，特别是通过增强成熟的深度学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical deep neural networks can learn rich multi-particle correlations incollider data, but their inductive biases are rarely anchored in physicsstructure. We propose quantum-informed neural networks (QINNs), a generalframework that brings quantum information concepts and quantum observables intopurely classical models. While the framework is broad, in this paper, we studyone concrete realisation that encodes each particle as a qubit and uses theQuantum Fisher Information Matrix (QFIM) as a compact, basis-independentsummary of particle correlations. Using jet tagging as a case study, QFIMs actas lightweight embeddings in graph neural networks, increasing modelexpressivity and plasticity. The QFIM reveals distinct patterns for QCD andhadronic top jets that align with physical expectations. Thus, QINNs offer apractical, interpretable, and scalable route to quantum-informed analyses, thatis, tomography, of particle collisions, particularly by enhancingwell-established deep learning approaches.</description>
      <author>example@mail.com (Aritra Bal, Markus Klute, Benedikt Maier, Melik Oughton, Eric Pezone, Michael Spannowsky)</author>
      <guid isPermaLink="false">2510.17984v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish</title>
      <link>http://arxiv.org/abs/2510.18725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SemiAdapt和SemiLoRA两种半监督推理高效方法，用于加强神经机器翻译中的领域适应，提高整体性能，特别是在低资源语言如爱尔兰语翻译方面。&lt;h4&gt;背景&lt;/h4&gt;微调被广泛用于定制大语言模型执行特定任务，但大型多语言模型的微调计算成本高，为研究低资源领域的研究人员设置了障碍。&lt;h4&gt;目的&lt;/h4&gt;解决低资源语言领域中高质量领域适应和微调的可访问性问题，使研究人员更容易进行这些工作。&lt;h4&gt;方法&lt;/h4&gt;介绍SemiAdapt和SemiLoRA作为半监督推理高效方法；利用参数高效微调和低秩适应技术；评估按数据集进行领域微调；开发基于嵌入的推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;SemiAdapt可以优于全领域微调；SemiLoRA可以使参数高效微调方法匹配甚至超过全模型微调的性能；基于嵌入的推理方法在更大和更嘈杂的语料库上表现特别好。&lt;h4&gt;结论&lt;/h4&gt;这些方法使高质量领域适应和微调更容易被低资源语言研究人员获取，所有爱尔兰语翻译模型都作为开放资源发布。&lt;h4&gt;翻译&lt;/h4&gt;本研究特别关注爱尔兰语翻译，开发的爱尔兰语翻译模型已作为开放资源发布，旨在促进低资源语言的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning is widely used to tailor large language models for specific taskssuch as neural machine translation (NMT). However, leveraging transfer learningis computationally expensive when fine-tuning large multilingual models withbillions of parameters, thus creating a barrier to entry for researchersworking on low-resource domains such as Irish translation. Parameter-efficientfine-tuning (PEFT) bridges this gap by training on a fraction of the originalmodel parameters, with the Low-Rank Adaptation (LoRA) approach introducingsmall, trainable adapter layers. We introduce SemiAdapt and SemiLoRA assemi-supervised inference-efficient approaches that strengthen domainadaptation and lead to improved overall performance in NMT. We demonstrate thatSemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRAcan propel PEFT methods to match or even outperform full-model fine-tuning. Wefurther evaluate domain-by-dataset fine-tuning and demonstrate that ourembedding-based inference methods perform especially well on larger and noisiercorpora. All Irish translation models developed in this work are released asopen resources. These methods aim to make high-quality domain adaptation andfine-tuning more accessible to researchers working with low-resource languages.</description>
      <author>example@mail.com (Josh McGiff, Nikola S. Nikolov)</author>
      <guid isPermaLink="false">2510.18725v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling</title>
      <link>http://arxiv.org/abs/2510.18405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 figures, 5 tables, submitted to the 11th IEEE International Women  in Engineering (WIE) Conference on Electrical and Computer Engineering 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种用于板球视频分析的自动化系统，该系统利用深度学习技术来提取导致球门被取的投球、检测板球并建立球轨迹模型。系统使用YOLOv8架构进行场地和球检测，结合光学字符识别技术提取记分卡信息来识别球门被取的时刻。通过全面的图像预处理，系统实现了从视频帧中稳健的文本提取。场地检测模型达到高精确度，球检测模型使用迁移学习也表现出色。该系统可以在检测到的场地上进行轨迹建模，为识别击球弱点提供数据驱动的洞察。&lt;h4&gt;背景&lt;/h4&gt;板球比赛产生大量视频数据，人工分析这些数据以提取战术信息是一个耗时且复杂的过程。随着深度学习技术的发展，自动分析板球视频以提取关键战术信息成为可能，这可以为教练团队和战略决策提供数据支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化系统，用于分析板球视频，提取关键信息（如导致球门被取的投球、球的检测和轨迹建模），从而为教练团队提供数据驱动的战术洞察，帮助识别击球弱点和改进比赛策略。&lt;h4&gt;方法&lt;/h4&gt;使用YOLOv8架构进行场地和球检测；结合光学字符识别技术提取记分卡信息；应用图像预处理技术，包括灰度转换、幂变换和形态学操作；使用迁移学习技术改进球检测模型；在检测到的场地上进行轨迹建模；在多个板球比赛视频上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;场地检测模型达到99.5%的平均精度均值和0.999的精确度；球检测模型使用迁移学习达到99.18%的平均精度均值、0.968的精确度和0.978的召回率；系统能够有效识别导致球门被取的时刻；轨迹建模能够提供识别击球弱点的数据驱动洞察；该系统在多个板球比赛视频上表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;该自动化板球视频分析系统利用深度学习技术实现了高精度的场地和球检测，能够有效提取关键战术信息。该系统不仅能够识别导致球门被取的时刻，还能通过轨迹建模提供数据驱动的战术洞察，为教练团队和战略决策提供重要支持，具有广阔的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种用于板球视频分析的自动化系统，该系统利用深度学习技术来提取导致球门被取的投球、检测板球并建立球轨迹模型。系统采用YOLOv8架构进行场地和球检测，结合光学字符识别技术提取记分卡信息以识别球门被取的时刻。通过全面的图像预处理，包括灰度转换、幂变换和形态学操作，系统实现了从视频帧中稳健的文本提取。场地检测模型达到99.5%的平均精度均值，精确度为0.999；而使用迁移学习的球检测模型达到99.18%的平均精度均值，精确度为0.968，召回率为0.978。该系统可在检测到的场地上进行轨迹建模，为识别击球弱点提供数据驱动的洞察。在多个板球比赛视频上的实验结果证明了这种方法在自动化板球分析中的有效性，为教练和战略决策提供了巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an automated system for cricket video analysis thatleverages deep learning techniques to extract wicket-taking deliveries, detectcricket balls, and model ball trajectories. The system employs the YOLOv8architecture for pitch and ball detection, combined with optical characterrecognition (OCR) for scorecard extraction to identify wicket-taking moments.Through comprehensive image preprocessing, including grayscale transformation,power transformation, and morphological operations, the system achieves robusttext extraction from video frames. The pitch detection model achieved 99.5%mean Average Precision at 50% IoU (mAP50) with a precision of 0.999, while theball detection model using transfer learning attained 99.18% mAP50 with 0.968precision and 0.978 recall. The system enables trajectory modeling on detectedpitches, providing data-driven insights for identifying batting weaknesses.Experimental results on multiple cricket match videos demonstrate theeffectiveness of this approach for automated cricket analytics, offeringsignificant potential for coaching and strategic decision-making.</description>
      <author>example@mail.com (Mst Jannatun Ferdous, Masum Billah, Joy Karmoker, Mohd Ruhul Ameen, Akif Islam, Md. Omar Faruqe)</author>
      <guid isPermaLink="false">2510.18405v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Aware Ellipsoidal Filtration for Persistent Homology of Recurrent Signals</title>
      <link>http://arxiv.org/abs/2510.17735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 13 figures. Extended version of the paper presented at  NOLTA 2025; prepared for journal submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为椭圆滤波的新型滤波方法，用于从动态光滑流中采样的点云分析，改进了循环信号的去噪和循环时间的估计。&lt;h4&gt;背景&lt;/h4&gt;持久同调通常用于探索点云的形状，这些点云假设是从几何对象中采样得到的。传统方法使用各向同性球体在不断增加的尺度上创建拓扑结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种假设点云从动态光滑流中采样的新型滤波方法，以改进信号处理和特征估计。&lt;h4&gt;方法&lt;/h4&gt;椭圆滤波基于局部流方差在点周围创建椭圆体，随着尺度增加来近似流的流形，而非使用传统的各向同性球体方法。&lt;h4&gt;主要发现&lt;/h4&gt;构建椭圆邻域能够改进循环信号的去噪和循环时间的估计，特别是在数据包含瓶颈的情况下效果更佳。&lt;h4&gt;结论&lt;/h4&gt;根据H1类的最大持久性选择椭圆体，可以为去噪和循环时间估计提供数据驱动的阈值。&lt;h4&gt;翻译&lt;/h4&gt;持久同调的一个常见用途是探索点云的形状，其中点假设是从几何对象中采样的。我们提出了一种新型滤波，称为椭圆滤波，它假设点云是从动态光滑流中采样的。椭圆滤波不是使用各向同性球体（例如Vietoris-Rips滤波）在不断增加的尺度上从点云创建拓扑结构，而是基于局部流方差在点周围创建椭圆体，随着尺度的增加来近似流的流形。我们表明，构建椭圆邻域可以改进循环信号的去噪和循环时间的估计，特别是当数据包含瓶颈时。根据H1类的最大持久性选择椭圆体，可以为去噪和循环时间估计提供数据驱动的阈值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统持久同调方法（如Vietoris-Rips过滤）在处理重复信号时的局限性，特别是在信号含有瓶颈结构和不同采样密度的情况下表现不佳的问题。这个问题在现实中很重要，因为重复信号在自然系统中很常见（如生理信号、气候数据、机械振动等），准确分析其拓扑结构对于理解系统动态特性、去噪和估计返回时间等任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到传统拓扑数据分析方法在处理时间序列数据时的局限性，特别是在处理瓶颈结构和不同采样密度时表现不佳。他们借鉴了持久同调的基本概念，特别是H1类的持久性分析；参考了Fernández等人的Fermat距离方法，该方法考虑了采样密度；受Kališnik等人的椭球过滤工作启发，但他们的工作针对的是静态点云而非时间序列数据。作者结合时空邻域概念，通过局部协方差估计和主成分分析构建自适应椭球体，使其能够根据局部流动方向和速度调整形状，从而更好地捕捉信号的动态特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云视为从动态光滑流中采样的样本，而非静态几何对象，通过使用自适应的椭球体邻域替代传统的各向同性球体邻域来更好地捕捉信号的局部流动特性。整体实现流程包括：1)局部协方差估计，结合时间邻域和空间邻域；2)基于PCA构建自适应椭球体；3)检测椭球体相交情况；4)构建椭球复形；5)进行持久同调分析；6)应用在信号去噪和返回时间估计上。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)时空邻域设计，同时考虑短期时间演化和空间重复性；2)自适应椭球体过滤，根据局部流动方向和速度调整形状；3)基于H1类最大持久性的数据驱动尺度选择方法；4)将拓扑分析方法应用于返回时间估计。相比之前的工作，本文方法超越了传统Vietoris-Rips过滤的各向同性限制，解决了Fermat距离方法未整合时间信息的问题，并改进了Kališnik等人针对静态点云的椭球过滤方法，使其能够处理时间序列数据的时空特性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于流动感知的自适应椭球过滤方法，通过结合时空邻域信息和局部流动几何，显著提高了持久同调在处理重复信号（特别是含有瓶颈结构和不同采样密度的信号）时的拓扑分析、去噪和返回时间估计性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One common use of persistent homology is to explore the shape of pointclouds, where points are assumed to be sampled from a geometric object. Wepropose a novel filtration, called ellipsoidal filtration, which assumes thatpoint clouds are sampled from a dynamic smooth flow. Instead of creatingtopologies from point clouds at increasing scales using isotropic balls (forexample, Vietoris-Rips filtration), ellipsoidal filtration creates ellipsoidsaround points based on local flow variances, approximating the flow's manifoldas the scale increases. We show that constructing ellipsoidal neighbourhoodsimproves the denoising of recurrent signals and the estimation of recurrencetimes, especially when the data contain bottlenecks. Choosing ellipsoidsaccording to the maximum persistence of the H1 class provides a data-driventhreshold for both denoising and recurrence-time estimation.</description>
      <author>example@mail.com (Omer Bahadir Eryilmaz, Cihan Katar, Max A. Little)</author>
      <guid isPermaLink="false">2510.17735v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
  <item>
      <title>Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions</title>
      <link>http://arxiv.org/abs/2510.17719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对雨滴条件下3D高斯散射(3DGS)的重建质量问题，提出了一个名为RaindropGS的综合基准测试，用于评估从受雨滴影响的非约束图像到清晰3D重建的完整流程。&lt;h4&gt;背景&lt;/h4&gt;3DGS在雨滴条件下因镜头上的雨滴污染而面临严重的遮挡和光学失真，导致重建质量显著下降。现有基准测试通常使用已知相机姿态的合成雨滴图像评估3DGS，假设理想条件。然而，在真实场景中，雨滴干扰相机姿态估计和点云初始化，且合成与真实雨滴间的域差距损害了泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决雨滴条件下3DGS评估的局限性，提出一个全面基准测试，评估从受雨滴影响的非约束图像到清晰3DGS重建的完整流程。&lt;h4&gt;方法&lt;/h4&gt;RaindropGS基准测试包含三部分：数据准备、数据处理和雨滴感知的3DGS评估。研究收集了真实世界雨滴重建数据集，每个场景包含三个对齐图像集：雨滴聚焦、背景聚焦和无雨滴真实地面，用于全面评估不同聚焦条件下的重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;现有3DGS方法在非约束雨滴图像上存在性能限制；相机聚焦位置显著影响3DGS重建性能；不准确的姿态估计和点云初始化对重建造成干扰。&lt;h4&gt;结论&lt;/h4&gt;研究为开发雨滴条件下更强大的3DGS方法提供了明确方向，通过全面评估揭示了不同流水线组件的影响和局限性。&lt;h4&gt;翻译&lt;/h4&gt;在雨滴条件下的3D高斯散射(3DGS)因相机镜头上雨滴污染造成的严重遮挡和光学失真而遭受显著重建质量下降。现有基准测试通常使用具有已知相机姿态的合成雨滴图像来评估3DGS，假设理想条件。然而，在真实场景中，雨滴常常干扰准确的相机姿态估计和点云初始化。此外，合成雨滴和真实雨滴之间的显著域差距进一步损害了泛化能力。为了解决这些问题，我们引入了RaindropGS，这是一个全面的基准测试，旨在评估从受雨滴影响的非约束图像到清晰的3DGS重建的完整3DGS流程。具体而言，整个基准测试流程包括三个部分：数据准备、数据处理和雨滴感知的3DGS评估，包括雨滴干扰类型、相机姿态估计和点云初始化、单图像雨滴去除比较以及3D高斯训练比较。首先，我们收集了一个真实世界的雨滴重建数据集，其中每个场景包含三个对齐的图像集：雨滴聚焦、背景聚焦和无雨滴真实地面，从而能够全面评估不同聚焦条件下的重建质量。通过全面的实验和分析，我们揭示了现有3DGS方法在非约束雨滴图像上的性能限制以及不同流水线组件的 varying 影响：相机聚焦位置对3DGS重建性能的影响，以及不准确姿态和点云初始化对重建造成的干扰。这些见解为在雨滴条件下开发更强大的3DGS方法指明了明确的方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在雨滴条件下进行3D高斯泼溅(3DGS)重建时，因相机镜头上雨滴造成的遮挡和光学失真导致的重建质量下降问题。这个问题在现实中很重要，因为雨滴是常见的环境干扰因素，会影响自动驾驶、增强现实等户外视觉应用；在研究中，现有方法在合成数据上表现良好但在真实场景中效果不佳，且缺乏针对真实雨滴条件的全面评估基准，无法准确衡量方法在实际应用中的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有研究的局限性：主要在合成数据上评估，假设了理想条件；认识到雨滴影响3DGS流程的多个阶段；注意到真实雨滴和合成雨滴间的差异；强调相机聚焦位置的重要性。作者设计了RaindropGS基准，包括数据准备(收集真实世界数据集)、数据处理(评估相机姿态估计、点云初始化和雨滴去除)和雨滴感知的3DGS评估。该方法借鉴了现有3DGS方法、COLMAP和VGGT进行姿态估计、以及Uformer、Restormer和IDT等雨滴去除模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全面基准，评估从受雨滴影响的未约束图像到清晰3DGS重建的完整流程，关注每个关键步骤(相机姿态估计、点云初始化、雨滴去除)对最终质量的影响。整体流程：1)数据收集-11个真实场景，每场景有雨滴聚焦、背景聚焦和无雨滴真实图像；2)数据处理-使用COLMAP和VGGT进行姿态估计和点云初始化，使用Uformer、Restormer和IDT进行雨滴去除；3)3DGS重建-使用原始3DGS、WeatherGS、GS-W和3DGS-MCMC四种方法；4)评估-使用PSNR、SSIM等指标比较不同聚焦条件和预处理策略的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个全面的3DGS雨滴重建基准，覆盖完整流程；2)首个真实世界3DGS雨滴重建数据集，包含三种对齐图像集和随机变化的雨滴；3)对现有方法的全面评估和见解。相比之前工作：1)评估从合成数据转向真实世界数据；2)关注点从仅关注3D高斯拟合扩展到整个流程；3)数据集从合成转向真实，考虑了雨滴在不同视角的变化；4)明确考虑了雨滴聚焦和背景聚焦两种条件，使评估更全面。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RaindropGS提供了一个全面的基准和真实世界数据集，用于评估在真实雨滴条件下3D高斯泼溅技术的完整重建流程，揭示了现有方法的局限性并指明了未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severeocclusions and optical distortions caused by raindrop contamination on thecamera lens, substantially degrading reconstruction quality. Existingbenchmarks typically evaluate 3DGS using synthetic raindrop images with knowncamera poses (constrained images), assuming ideal conditions. However, inreal-world scenarios, raindrops often interfere with accurate camera poseestimation and point cloud initialization. Moreover, a significant domain gapbetween synthetic and real raindrops further impairs generalization. To tacklethese issues, we introduce RaindropGS, a comprehensive benchmark designed toevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted imagesto clear 3DGS reconstructions. Specifically, the whole benchmark pipelineconsists of three parts: data preparation, data processing, and raindrop-aware3DGS evaluation, including types of raindrop interference, camera poseestimation and point cloud initialization, single image rain removalcomparison, and 3D Gaussian training comparison. First, we collect a real-worldraindrop reconstruction dataset, in which each scene contains three alignedimage sets: raindrop-focused, background-focused, and rain-free ground truth,enabling a comprehensive evaluation of reconstruction quality under differentfocus conditions. Through comprehensive experiments and analyses, we revealcritical insights into the performance limitations of existing 3DGS methods onunconstrained raindrop images and the varying impact of different pipelinecomponents: the impact of camera focus position on 3DGS reconstructionperformance, and the interference caused by inaccurate pose and point cloudinitialization on reconstruction. These insights establish clear directions fordeveloping more robust 3DGS methods under raindrop conditions.</description>
      <author>example@mail.com (Zhiqiang Teng, Beibei Lin, Tingting Chen, Zifeng Yuan, Xuanyi Li, Xuanyu Zhang, Shunli Zhang)</author>
      <guid isPermaLink="false">2510.17719v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards 3D Objectness Learning in an Open World</title>
      <link>http://arxiv.org/abs/2510.17686v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出OP3Det，一种无类别开放世界无提示3D检测器，能够检测3D场景中的任何物体，包括训练中未见过的物体，显著提升了开放世界3D物体检测性能。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测和新类别检测虽有进展，但学习泛化3D物体性的研究仍不足；传统封闭集3D检测器难以泛化到开放世界场景，而直接引入3D开放词汇模型面临词汇扩展和语义重叠的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究开放世界3D物体性，实现能够检测3D场景中所有物体（包括训练中未见过的物体）的泛化3D物体发现。&lt;h4&gt;方法&lt;/h4&gt;提出OP3Det检测器，引入2D基础模型的强泛化和零样本能力，利用2D语义先验和3D几何先验进行无类别提案，通过跨模态专家混合集成点云和RGB图像的互补信息，动态路由单模态和多模态特征以学习泛化的3D物体性。&lt;h4&gt;主要发现&lt;/h4&gt;OP3Det表现卓越，与现有开放世界3D检测器相比在AR指标上显著提高最多16.0%，与封闭世界3D检测器相比实现了13.5%的改进。&lt;h4&gt;结论&lt;/h4&gt;OP3Det成功实现了开放世界3D物体检测的目标，能够检测3D场景中的任何物体而不依赖手工制作的文本提示。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D物体检测和新类别检测研究取得了显著进展，然而关于学习泛化3D物体性的研究仍然不足。在本文中，我们深入研究开放世界3D物体性学习，专注于检测3D场景中的所有物体，包括训练中未见过的新物体。传统的封闭集3D检测器难以泛化到开放世界场景，而直接将3D开放词汇模型整合以获得开放世界能力又面临词汇扩展和语义重叠的挑战。为实现泛化的3D物体发现，我们提出了OP3Det，一种无类别开放世界无提示3D检测器，可以检测3D场景中的任何物体，而不依赖手工制作的文本提示。我们引入了2D基础模型的强泛化和零样本能力，利用2D语义先验和3D几何先验进行无类别提案，以扩展3D物体发现。然后，通过在跨模态专家混合中集成点云和RGB图像的互补信息，OP3Det动态路由单模态和多模态特征以学习泛化的3D物体性。大量实验证明了OP3Det的卓越性能，在AR指标上显著超越现有开放世界3D检测器最多16.0%，相比封闭世界3D检测器实现了13.5%的改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放世界中的3D物体检测问题，特别是学习通用的3D物体性，使模型能够检测3D场景中的所有物体，包括训练过程中未见过的物体类别。这个问题在现实世界中非常重要，因为自动驾驶、机器人等应用场景中物体类别可能会动态变化，传统封闭集3D检测器无法泛化到这些开放世界场景，而直接使用开放词汇模型又面临词汇扩展和语义重叠的问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先注意到现实世界环境中物体类别可能动态变化，导致需要开放世界3D检测能力。观察到2D领域在未知物体识别方面已有较多探索，但3D领域有限。意识到3D点云数据规模和标注类别有限，而2D领域有丰富的基础模型和训练数据。因此，作者将2D预训练模型的强大零样本能力转移到3D领域。借鉴了SAM模型用于提取类别无关物体掩码，以及多模态融合方法，但提出了创新的跨模态专家混合模块来解决现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型（特别是SAM）的强大泛化能力帮助3D物体发现，并通过多尺度点采样策略优化SAM产生的掩码，再通过跨模态专家混合模块动态融合单模态和多模态特征学习通用3D物体性。整体流程分为：1)3D物体发现阶段，使用SAM和多尺度点采样策略处理RGB图像，投影到3D空间；2)训练阶段，提取点云和图像特征，使用跨模态MoE融合特征；3)推理阶段，直接在点云-图像对上进行检测，完全无提示地进行类别无关的3D物体性推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次定义并解决3D领域的类别无关开放世界物体检测问题；2)设计多尺度点采样策略增强2D-3D关联，解决SAM碎片化掩码问题；3)提出跨模态专家混合模块动态选择单模态和多模态路径。相比传统封闭集3D检测器，它能检测未见过的物体类别；相比开放词汇3D检测器，它不需要预定义词汇表；相比现有多模态融合方法，它保留模态特定信息并动态选择最相关特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了OP3Det，首个类别无关的开放世界3D检测器，通过利用2D语义知识和创新的跨模态专家混合机制，实现了在开放世界中检测所有物体的能力，包括训练过程中未见过的物体类别。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in 3D object detection and novel category detection havemade significant progress, yet research on learning generalized 3D objectnessremains insufficient. In this paper, we delve into learning open-world 3Dobjectness, which focuses on detecting all objects in a 3D scene, includingnovel objects unseen during training. Traditional closed-set 3D detectorsstruggle to generalize to open-world scenarios, while directly incorporating 3Dopen-vocabulary models for open-world ability struggles with vocabularyexpansion and semantic overlap. To achieve generalized 3D object discovery, Wepropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detectany objects within 3D scenes without relying on hand-crafted text prompts. Weintroduce the strong generalization and zero-shot capabilities of 2D foundationmodels, utilizing both 2D semantic priors and 3D geometric priors forclass-agnostic proposals to broaden 3D object discovery. Then, by integratingcomplementary information from point cloud and RGB image in the cross-modalmixture of experts, OP3Det dynamically routes uni-modal and multi-modalfeatures to learn generalized 3D objectness. Extensive experiments demonstratethe extraordinary performance of OP3Det, which significantly surpasses existingopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvementcompared to closed-world 3D detectors.</description>
      <author>example@mail.com (Taichi Liu, Zhenyu Wang, Ruofeng Liu, Guang Wang, Desheng Zhang)</author>
      <guid isPermaLink="false">2510.17686v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation</title>
      <link>http://arxiv.org/abs/2510.17609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于机器学习的框架，用于自动分割3D点云，特别是在基础设施健康监测中的应用，结合了无人机扫描的真实数据和从BIM生成的合成数据。&lt;h4&gt;背景&lt;/h4&gt;无人机技术进步实现了高效、非接触式的结构健康监测，结合摄影测量可捕获高分辨率扫描并重建基础设施的详细3D模型，但从这些模型中分割特定结构组件仍面临挑战，传统方法依赖耗时且容易出错的手动标注。&lt;h4&gt;目的&lt;/h4&gt;解决从3D模型中分割特定结构组件的挑战，开发一种自动化分割方法替代耗时且易错的手动标注流程。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于机器学习的框架用于3D点云自动分割，利用真实无人机扫描点云和从建筑信息建模(BIM)生成的合成数据的互补优势，结合BIM数据克服手动标注的局限性。&lt;h4&gt;主要发现&lt;/h4&gt;在铁路轨道数据集验证中，该方法在识别和分割主要组件如铁轨和轨枕方面表现高准确性；通过使用较小的数据集并补充BIM数据，框架显著减少了训练时间同时保持了合理的分割准确性。&lt;h4&gt;结论&lt;/h4&gt;这种自动化方法提高了3D基础设施模型分割的精度和效率，推进了无人机和BIM技术在结构健康监测和基础设施管理中的集成应用。&lt;h4&gt;翻译&lt;/h4&gt;无人机技术的进步实现了高效、非接触式的结构健康监测。结合摄影测量技术，无人机可以捕获高分辨率扫描并重建基础设施的详细3D模型。然而，从这些模型中分割特定结构组件仍然是一个关键挑战，这一过程传统上依赖于耗时且容易出错的手动标注。为解决这一问题，我们提出了一种基于机器学习的框架，用于3D点云的自动分割。我们的方法利用真实无人机扫描点云和从建筑信息建模(BIM)生成的合成数据的互补优势，克服了与手动标注相关的局限性。在铁路轨道数据集上的验证显示，在识别和分割铁轨和轨枕等主要组件方面具有高准确性。此外，通过使用较小的数据集并补充BIM数据，该框架显著减少了训练时间，同时保持了合理的分割准确性。这种自动化方法提高了3D基础设施模型分割的精度和效率，并推进了无人机和BIM技术在结构健康监测和基础设施管理中的集成。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从3D点云中自动分割铁路基础设施的关键结构组件（如钢轨和轨枕）的问题。传统方法依赖耗时且易错的手动标注，这在大型数据集上尤其不切实际。这个问题很重要，因为铁路是社会经济支柱，组件损坏可能导致严重事故；人工检查既耗时又危险；而无人机虽高效安全，但点云分割仍需手动标注，限制了自动化监测的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到铁路基础设施健康监测的重要性及传统人工检查的局限性。他们借鉴了无人机技术用于基础设施检查的研究、摄影测量3D重建方法、以及PointNet++等深度学习网络用于点云分割的工作。在此基础上，他们创新性地提出结合真实世界无人机扫描点云和从建筑信息模型生成的合成数据，设计了一个机器学习框架，通过创建不同规模的训练数据集并测试不同旋转策略，实现了高效准确的自动分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合真实世界无人机扫描点云和从BIM生成的合成数据，减少对手动标注数据的依赖，实现铁路基础设施3D点云的自动分割。实现流程包括：1)使用无人机沿规划路径采集铁路图像并重建3D点云；2)对真实点云进行手动标注，同时创建BIM模型并自动生成标注点云；3)对数据进行下采样并创建不同规模的数据集；4)使用PointNet++架构训练分割模型；5)评估模型性能并测试在不同材料轨枕上的泛化能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创新性地融合真实点云与BIM合成数据；2)使用较小规模训练数据集结合BIM数据实现高效分割；3)系统研究数据增强策略对模型性能的影响；4)展示模型在不同材料轨枕上的泛化能力。相比之前工作，本文不仅减少了手动标注需求，还显著缩短了训练时间，实现了小规模数据集的高效利用，并证明了方法在不同材料铁路结构上的适用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的机器学习框架，通过结合无人机摄影测量的真实点云和BIM生成的合成数据，实现了铁路基础设施3D点云的高效自动分割，为铁路健康监测提供了一种可扩展且实用的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advancement of UAV technology has enabled efficient, non-contactstructural health monitoring. Combined with photogrammetry, UAVs can capturehigh-resolution scans and reconstruct detailed 3D models of infrastructure.However, a key challenge remains in segmenting specific structural componentsfrom these models-a process traditionally reliant on time-consuming anderror-prone manual labeling. To address this issue, we propose a machinelearning-based framework for automated segmentation of 3D point clouds. Ourapproach uses the complementary strengths of real-world UAV-scanned pointclouds and synthetic data generated from Building Information Modeling (BIM) toovercome the limitations associated with manual labeling. Validation on arailroad track dataset demonstrated high accuracy in identifying and segmentingmajor components such as rails and crossties. Moreover, by using smaller-scaledatasets supplemented with BIM data, the framework significantly reducedtraining time while maintaining reasonable segmentation accuracy. Thisautomated approach improves the precision and efficiency of 3D infrastructuremodel segmentation and advances the integration of UAV and BIM technologies instructural health monitoring and infrastructure management.</description>
      <author>example@mail.com (Siqi Chen, Shanyue Guan)</author>
      <guid isPermaLink="false">2510.17609v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception</title>
      <link>http://arxiv.org/abs/2510.17568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PAGE-4D的前馈模型，能够处理包含动态元素的3D场景，实现相机姿态估计、深度预测和点云重建，无需后处理处理。&lt;h4&gt;背景&lt;/h4&gt;现有的3D前馈模型（如VGGT）在静态场景的3D属性推断上表现良好，但由于主要在静态数据集上训练，在处理包含移动人类或可变形物体等复杂动态元素的真实世界场景时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;引入PAGE-4D模型，将VGGT扩展到动态场景，实现相机姿态估计、深度预测和点云重建的多功能4D重建系统。&lt;h4&gt;方法&lt;/h4&gt;提出了一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息，在相机姿态估计时抑制运动线索，在几何重建时增强这些线索，解决了多任务4D重建中任务间的固有冲突。&lt;h4&gt;主要发现&lt;/h4&gt;PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了更好的性能表现。&lt;h4&gt;结论&lt;/h4&gt;PAGE-4D成功解决了多任务4D重建中任务间的固有冲突，通过动态感知聚合器有效处理了静态和动态场景中的各种任务，无需后处理即可实现高质量的4D重建。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D前馈模型，如视觉几何基础变换器（VGGT），在推断静态场景的3D属性方面表现出强大能力。然而，由于它们通常在静态数据集上训练，这些模型在涉及复杂动态元素的真实世界场景中往往表现不佳，例如移动的人类或像雨伞这样的可变形物体。为了解决这一局限性，我们引入了PAGE-4D，一种将VGGT扩展到动态场景的前馈模型，能够实现相机姿态估计、深度预测和点云重建——所有这些都无需后处理。多任务4D重建的一个核心挑战是任务之间的固有冲突：准确的相机姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。为了解决这种张力，我们提出了一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息——为姿态估计抑制运动线索，同时为几何重建增强这些线索。大量实验表明，PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决将静态3D场景理解模型扩展到动态场景的问题。这个问题在现实中非常重要，因为动态场景（如包含移动人或物体的环境）在自动驾驶、机器人导航、增强现实等应用中非常普遍，准确理解和重建这些场景对现实应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到VGGT在静态场景中表现良好但在动态场景中性能下降，然后分析发现核心冲突：相机姿态估计需要抑制动态区域，而几何重建则需要利用动态信息。他们借鉴了VGGT的基础架构，引入了动态感知聚合器和注意力机制，并采用了针对性的微调策略，只调整对动态最敏感的中间层，而不是重新设计整个网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是解耦动态信息在不同任务中的作用：对相机姿态估计抑制动态区域，对几何重建则利用动态信息。整体流程包括：输入RGB图像序列；使用预训练编码器提取特征；通过三阶段动态感知聚合器处理特征；最后通过解码器输出深度图、3D点云和相机姿态估计结果。其中动态感知聚合器预测动态掩码，并根据任务类型有选择地应用这个掩码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 动态感知聚合器，解耦动态和静态信息；2) 针对性微调策略，只调整关键层；3) 动态掩码预测，自适应学习动态区域；4) 任务特定的注意力应用。相比之前的工作，PAGE-4D不需要大的架构改变，而是通过微调和动态感知注意力机制实现，在保持高效的同时提高了动态场景的性能，超越了VGGT和其他动态场景模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAGE-4D通过解耦动态信息在不同任务中的作用，有效地将静态3D场景理解模型扩展到动态场景，实现了高效的相机姿态估计和几何重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent 3D feed-forward models, such as the Visual Geometry GroundedTransformer (VGGT), have shown strong capability in inferring 3D attributes ofstatic scenes. However, since they are typically trained on static datasets,these models often struggle in real-world scenarios involving complex dynamicelements, such as moving humans or deformable objects like umbrellas. Toaddress this limitation, we introduce PAGE-4D, a feedforward model that extendsVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, andpoint cloud reconstruction -- all without post-processing. A central challengein multi-task 4D reconstruction is the inherent conflict between tasks:accurate camera pose estimation requires suppressing dynamic regions, whilegeometry reconstruction requires modeling them. To resolve this tension, wepropose a dynamics-aware aggregator that disentangles static and dynamicinformation by predicting a dynamics-aware mask -- suppressing motion cues forpose estimation while amplifying them for geometry reconstruction. Extensiveexperiments show that PAGE-4D consistently outperforms the original VGGT indynamic scenarios, achieving superior results in camera pose estimation,monocular and video depth estimation, and dense point map reconstruction.</description>
      <author>example@mail.com (Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang)</author>
      <guid isPermaLink="false">2510.17568v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS</title>
      <link>http://arxiv.org/abs/2510.17479v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A preprint paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究针对稀疏视图3D高斯溅射(3DGS)对训练视图过拟合导致的新视图渲染伪影问题，提出了一种基于改进初始化的解决方案。研究发现初始化是决定性能的关键因素，而非训练时约束。研究团队设计了频率感知SfM、3DGS自初始化和点云正则化三种方法，显著提升了稀疏视图设置下的渲染质量，并在LLFF和Mip-NeRF360数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;稀疏视图3D高斯溅射(3DGS)技术通常对训练视图过拟合，导致在新视图渲染时出现模糊等伪影。先前的研究主要通过增强初始化（即改进来自运动结构SfM的点云）或添加训练时约束（正则化）来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决稀疏视图3DGS的过拟合问题，通过改进初始化策略而非依赖训练时约束，提升新视图渲染质量，消除伪影。&lt;h4&gt;方法&lt;/h4&gt;研究团队设计了三种方法来改进初始化：(i) 频率感知SfM：通过低频视图增强和放宽的多视图对应关系提高低纹理区域的覆盖率；(ii) 3DGS自初始化：将光度监督提升为附加点，用学习的高斯中心补偿SfM未能充分覆盖的区域；(iii) 点云正则化：通过简单的几何/可见性先验强制执行多视图一致性和均匀空间覆盖，产生干净可靠的点云。&lt;h4&gt;主要发现&lt;/h4&gt;通过对照实验发现，初始化是决定稀疏视图3DGS性能的关键因素，它决定了可达到的性能范围，而训练时约束只能在该范围内带来适度改进，且需要额外计算成本。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的三种初始化改进方法在稀疏视图设置下展示了一致的性能提升，确立了一种更强初始化策略的有效性。代码已在GitHub开源，可供进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;稀疏视图3D高斯溅射(3DGS)通常对训练视图过拟合，导致新视图渲染时出现模糊等伪影。先前的研究通过增强初始化（即来自运动结构SfM的点云）或添加训练时约束（正则化）来解决3DGS优化问题。然而，我们的对照实验揭示初始化是决定性因素：它决定了稀疏视图3DGS可达到的性能范围，而训练时约束只能在该范围内带来适度改进且需要额外成本。鉴于初始化的首要地位，我们将设计重点放在那里。尽管SfM由于其依赖特征匹配在稀疏视图下表现不佳，但它仍能提供可靠的种子点。因此，我们在SfM基础上努力尽可能全面地补充它未能覆盖的区域。具体而言，我们设计了：(i)频率感知SfM，通过低频视图增强和放宽的多视图对应关系提高低纹理覆盖率；(ii)3DGS自初始化，将光度监督提升为附加点，用学习的高斯中心补偿SfM稀疏区域；(iii)点云正则化，通过简单的几何/可见性先验强制执行多视图一致性和均匀空间覆盖，产生干净可靠的点云。我们在LLFF和Mip-NeRF360上的实验展示了在稀疏视图设置中的一致性改进，确立了我们的方法作为一种更强的初始化策略。代码可在https://github.com/zss171999645/ItG-GS获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏视图3D高斯泼溅（3DGS）对训练视图过拟合的问题，导致在渲染新视图时出现模糊等伪影。这个问题在现实中非常重要，因为虚拟现实、增强现实、自由视点视频和数字内容创作等应用都需要从有限视点生成逼真新视图的能力。在实际场景中，由于硬件限制、成本或捕获条件，我们通常只能获取有限数量的视点图像，因此解决稀疏视图下的过拟合问题能显著提升这些应用的质量和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过受控实验发现初始化质量是决定稀疏视图3DGS性能的关键因素，而训练时正则化只能提供有限的改进。基于这一发现，他们将设计重点放在初始化阶段。他们借鉴了现有的SfM算法，尽管它在稀疏视图下表现不佳但仍能提供可靠的种子点。具体来说，他们参考了EAP-GS的工作，通过将最小轨道匹配要求从三个视图降低到两个视图来增加初始点集密度。此外，他们还利用了3DGS自身的学习信号，设计了自初始化方法，将像素级的光度约束转化为额外的3D点。最后，他们引入了点云正则化技术，结合了几何和可见性先验的概念来优化点云质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增强初始化点云的质量和覆盖范围来解决稀疏视图3DGS的过拟合问题。整体实现流程分为三个阶段：1）低频感知SfM：通过掩码高频图像区域，在增强的双图像集上执行SfM，从而改善低纹理区域的覆盖；2）3DGS自初始化：在输入视图上训练一个轻量级3DGS模型，并将所有原始高斯中心作为新的点云重用，这能补偿图像特征不足的区域；3）点云正则化：通过单视图点过滤（去除深度模糊的点）、聚类去噪（减少不稳定点和重复点）以及基于法线的一致性过滤（去除几何不一致的点），最终产生一个干净可靠的点云作为3DGS的初始化输入。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三个：1）频率感知SfM：通过掩码高频区域和放松多视图对应要求，显著改善低纹理区域的覆盖；2）3DGS自初始化：创新性地利用3DGS的学习信号，将像素级光度约束提升为额外的3D点，弥补了传统SfM在弱纹理区域的不足；3）点云正则化：结合单视图过滤、聚类去噪和基于法线的一致性过滤，产生干净可靠的点云。相比之前的工作，本文的不同之处在于：首先，它首次系统性地证明了初始化质量比训练时正则化对稀疏视图3DGS的性能影响更大；其次，它不是简单地改进单一组件，而是设计了一个完整的三阶段初始化管道；最后，它能够与现有的正则化方法（如DropGS）结合使用，进一步提升性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种三阶段初始化管道，通过显著增强稀疏视图3DGS的初始点云质量和覆盖范围，有效解决了过拟合问题，大幅提升了新视图合成的质量和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the trainingviews, leading to artifacts like blurring in novel view rendering. Prior workaddresses it either by enhancing the initialization (\emph{i.e.}, the pointcloud from Structure-from-Motion (SfM)) or by adding training-time constraints(regularization) to the 3DGS optimization. Yet our controlled ablations revealthat initialization is the decisive factor: it determines the attainableperformance band in sparse-view 3DGS, while training-time constraints yieldonly modest within-band improvements at extra cost. Given initialization'sprimacy, we focus our design there. Although SfM performs poorly under sparseviews due to its reliance on feature matching, it still provides reliable seedpoints. Thus, building on SfM, our effort aims to supplement the regions itfails to cover as comprehensively as possible. Specifically, we design: (i)frequency-aware SfM that improves low-texture coverage via low-frequency viewaugmentation and relaxed multi-view correspondences; (ii) 3DGSself-initialization that lifts photometric supervision into additional points,compensating SfM-sparse regions with learned Gaussian centers; and (iii)point-cloud regularization that enforces multi-view consistency and uniformspatial coverage through simple geometric/visibility priors, yielding a cleanand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrateconsistent gains in sparse-view settings, establishing our approach as astronger initialization strategy. Code is available athttps://github.com/zss171999645/ItG-GS.</description>
      <author>example@mail.com (Feng Zhou, Wenkai Guo, Pu Cao, Zhicheng Zhang, Jianqin Yin)</author>
      <guid isPermaLink="false">2510.17479v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding</title>
      <link>http://arxiv.org/abs/2510.17068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ProDAT的新型渐进式点云编码方法，通过密度感知尾部丢弃机制实现多比特率下的渐进式解码，并在编码效率上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;三维点云在自动驾驶、增强现实和沉浸式通信等应用中日益重要，需要实时处理和低延迟。但大数据量和带宽限制在资源有限环境中阻碍了高质量服务的部署。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于学习的点云几何编码方法固定潜在表示不支持渐进式解码的问题，实现高效的渐进式点云编码。&lt;h4&gt;方法&lt;/h4&gt;提出ProDAT，一种密度感知尾部丢弃机制，通过利用密度信息作为指导信号，使潜在特征和坐标根据其重要性进行自适应解码，从而使用单个模型在多个比特率下实现渐进式解码。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的实验结果表明，ProDAT不仅能够实现渐进式编码，而且与最先进的基于学习的编码技术相比，实现了更高的编码效率。在SemanticKITTI上，PSNR-D2的BD-rate改进超过28.6%，在ShapeNet上超过18.15%。&lt;h4&gt;结论&lt;/h4&gt;ProDAT成功填补了现有方法不支持渐进式解码的空白，同时显著提高了点云编码的效率。&lt;h4&gt;翻译&lt;/h4&gt;三维（3D）点云在自动驾驶、增强现实和沉浸式通信等应用中变得越来越重要，要求实时处理和低延迟。然而，其大数据量和带宽限制阻碍了在资源有限环境中部署高质量服务。渐进式编码允许在不同细节级别解码，通过允许初始部分解码随后进行细化提供了一种替代方案。尽管最近基于学习的点云几何编码方法取得了显著成功，但其固定的潜在表示不支持渐进式解码。为了填补这一空白，我们提出了ProDAT，一种用于渐进式点云编码的新型密度感知尾部丢弃机制。通过利用密度信息作为指导信号，潜在特征和坐标根据其重要性进行自适应解码，从而使用单个模型在多个比特率下实现渐进式解码。在基准数据集上的实验结果表明，所提出的ProDAT不仅能够实现渐进式编码，而且与最先进的基于学习的编码技术相比，实现了更高的编码效率，在SemanticKITTI上PSNR-D2的BD-rate改进超过28.6%，在ShapeNet上超过18.15%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云编码中不支持渐进式解码的问题。这个问题在现实中很重要，因为点云数据在自动驾驶、增强现实和沉浸式通信等应用中需求日益增长，但这些应用需要实时处理和低延迟。现有的学习点云编码方法产生单一比特流，必须完全解码才能重建，无法满足带宽受限环境下的渐进式质量提升需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云编码的现状，发现传统方法在大规模数据集上效率低，而学习-based方法虽性能好但不支持渐进式解码。他们借鉴了2D图像和视频领域的渐进式编码技术（如JPEG、JPEG2000和H.264 SVC），并参考了密度保留点云压缩方法和图像编码中的尾部丢弃技术。通过结合这些思想，作者设计了密度感知的尾部丢弃机制，利用点云密度信息指导特征选择，实现渐进式解码。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用密度信息作为指导信号，高密度区域通常表示复杂几何结构，稀疏区域对应简单结构，通过密度感知的尾部丢弃机制优先保留重要特征。整体流程包括：1)编码阶段提取特征并应用密度感知的尾部丢弃；2)根据渐进比例选择保留特征；3)解码器重建点云；4)训练时使用随机丢弃比例使模型适应不同级别的特征完整性，损失函数结合几何质量、密度保持和比特率约束。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)单一训练阶段的渐进式点云编码；2)密度感知的尾部丢弃机制；3)联合特征和坐标丢弃策略；4)动态密度归一化方法；5)结合全局方差和局部梯度的通道重要性计算。相比之前的工作，ProDAT不仅支持渐进式解码，还通过密度信息优化特征选择，在保持高质量重建的同时提高了编码效率，特别适合大规模点云数据，如SemanticKITTI和ShapeNet数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ProDAT通过引入密度感知的尾部丢弃机制，实现了单一训练阶段的渐进式点云编码，在保持高质量重建的同时显著提高了编码效率和灵活性，特别适合资源受限的实时应用场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional (3D) point clouds are becoming increasingly vital inapplications such as autonomous driving, augmented reality, and immersivecommunication, demanding real-time processing and low latency. However, theirlarge data volumes and bandwidth constraints hinder the deployment ofhigh-quality services in resource-limited environments. Progres- sive coding,which allows for decoding at varying levels of detail, provides an alternativeby allowing initial partial decoding with subsequent refinement. Althoughrecent learning-based point cloud geometry coding methods have achieved notablesuccess, their fixed latent representation does not support progressivedecoding. To bridge this gap, we propose ProDAT, a novel density-awaretail-drop mechanism for progressive point cloud coding. By leveraging densityinformation as a guidance signal, latent features and coordinates are decodedadaptively based on their significance, therefore achieving progressivedecoding at multiple bitrates using one single model. Experimental results onbenchmark datasets show that the proposed ProDAT not only enables progressivecoding but also achieves superior coding efficiency compared tostate-of-the-art learning-based coding techniques, with over 28.6% BD-rateimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet</description>
      <author>example@mail.com (Zhe Luo, Wenjing Jia, Stuart Perry)</author>
      <guid isPermaLink="false">2510.17068v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.16865v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于配准的旋转不变性特征提取框架，用于点云数据中的3D异常检测，解决了现有方法在特征转换一致性和判别能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;3D点云异常检测对工业质量控制至关重要，但当前基于内存库的方法常面临特征转换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些问题更加突出，导致不可靠的检测结果。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时解决点云配准和异常检测问题的框架，通过整合这两个任务的目标，实现旋转不变且局部判别性强的特征提取，提高异常检测的可靠性和有效性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种配准诱导的旋转不变性特征提取框架，将点云配准和基于内存的异常检测目标整合在一起。通过将特征提取嵌入到配准学习过程中，框架联合优化对齐和表示学习，使网络获得对旋转鲁棒且对异常检测高效的特征。&lt;h4&gt;主要发现&lt;/h4&gt;点云配准不仅在几何结构对齐中起关键作用，还引导特征提取实现旋转不变性和局部判别性表示。两个任务都依赖于建模局部几何结构和利用样本间的特征相似性。在Anomaly-ShapeNet和Real3D-AD数据集上的实验表明，该方法在有效性和泛化能力上一致优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过将特征提取与配准学习过程相结合，所提出的框架能够获得既对旋转具有鲁棒性又对异常检测高效的特征，显著提高了点云异常检测的性能和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;点云数据中的3D异常检测对工业质量控制至关重要，旨在以高可靠性识别结构缺陷。然而，当前基于内存库的方法常常遭受特征转换不一致和判别能力有限的困扰，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些局限性变得更加明显，导致不可靠的检测结果。我们认为点云配准不仅在几何结构对齐中起着关键作用，还引导特征提取朝向旋转不变和局部判别性表示方向发展。为此，我们提出了一种基于配准的旋转不变性特征提取框架，整合了点云配准和基于内存的异常检测目标。我们的关键见解是，这两个任务都依赖于建模局部几何结构和利用样本间的特征相似性。通过将特征提取嵌入到配准学习过程中，我们的框架联合优化了对齐和表示学习。这种整合使网络能够获得对旋转具有鲁棒性且对异常检测非常有效的特征。在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，我们的方法在有效性和泛化能力上一致优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云异常检测中的两个关键问题：现有基于内存库的方法在特征变换方面存在不一致性，以及在捕捉局部几何细节和实现旋转不变性方面能力有限。这个问题在现实中非常重要，因为3D异常检测对工业质量控制至关重要，可以可靠地识别结构缺陷。当物体以不同方向呈现时，旋转不变性变得尤为重要，而当前方法在注册失败时会产生不可靠的检测结果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析点云注册和基于内存库的异常检测之间的内在联系，发现这两个任务都依赖于对局部几何结构的建模和跨样本的特征相似性利用。他们提出将注册作为特征学习过程的一部分，而不是独立的预处理模块。作者借鉴了RIConv++、KPConv-FPN和Geometric Transformer等现有组件，应用了粗到细的点云注册策略、最优传输算法、RANSAC技术和内存库采样技术，但将它们整合到一个新的统一框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云注册过程整合到异常检测的特征学习中，通过注册任务强制源点云和目标点云之间的几何对齐和多尺度特征一致性，使网络能够获得对旋转具有鲁棒性且对异常检测有效的特征。整体流程分为两个阶段：1)注册诱导的特征学习阶段：生成变换点云对、多尺度下采样、构建补丁匹配、提取多尺度特征，并通过三个损失函数联合优化；2)注册诱导的异常检测阶段：点对齐、特征归一化和内存库构建、特征过滤和异常分数计算。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：将点云注册作为特征学习的一部分而非独立预处理步骤；提出统一的Reg2Inv框架用于注册诱导的旋转不变特征提取；联合优化原型-样本对齐和基于鲁棒局部几何特征的异常评分；通过注册任务同时学习旋转不变和局部判别性表示。相比之前工作，本文方法将注册整合到特征学习中而非作为预处理；保留了细粒度局部几何而非仅关注全局语义；在注册不完美时仍保持鲁棒性；同时优化空间对齐和特征学习而非分开处理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种将点云注册整合到特征学习过程中的统一框架Reg2Inv，通过联合优化几何对齐和多尺度特征一致性，实现了对旋转具有鲁棒性且对异常检测高度有效的3D点云特征提取，显著提升了工业质量控制中的异常检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D anomaly detection in point-cloud data is critical for industrial qualitycontrol, aiming to identify structural defects with high reliability. However,current memory bank-based methods often suffer from inconsistent featuretransformations and limited discriminative capacity, particularly in capturinglocal geometric details and achieving rotation invariance. These limitationsbecome more pronounced when registration fails, leading to unreliable detectionresults. We argue that point-cloud registration plays an essential role notonly in aligning geometric structures but also in guiding feature extractiontoward rotation-invariant and locally discriminative representations. To thisend, we propose a registration-induced, rotation-invariant feature extractionframework that integrates the objectives of point-cloud registration andmemory-based anomaly detection. Our key insight is that both tasks rely onmodeling local geometric structures and leveraging feature similarity acrosssamples. By embedding feature extraction into the registration learningprocess, our framework jointly optimizes alignment and representation learning.This integration enables the network to acquire features that are both robustto rotations and highly effective for anomaly detection. Extensive experimentson the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our methodconsistently outperforms existing approaches in effectiveness andgeneralizability.</description>
      <author>example@mail.com (Yuyang Yu, Zhengwei Chen, Xuemiao Xu, Lei Zhang, Haoxin Yang, Yongwei Nie, Shengfeng He)</author>
      <guid isPermaLink="false">2510.16865v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models</title>
      <link>http://arxiv.org/abs/2510.16706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于EaaS（嵌入即服务）模型的指纹框架，通过分析嵌入空间拓扑结构的几何特性来验证模型所有权，而非传统水印技术。该方法能有效抵抗旋转、缩放和平移（RST）攻击，并在视觉和文本嵌入任务中验证了其优越性。&lt;h4&gt;背景&lt;/h4&gt;特征嵌入已成为处理高维复杂数据的核心技术，导致EaaS模型在云环境中广泛部署。现有水印技术通过修改训练样本或网络参数注入后门触发器来保护知识产权，但这些方法易受语义分析和几何变换攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;解决现有EaaS模型水印技术易受语义分析和几何变换攻击的问题，开发一种更鲁棒的模型所有权验证方法。&lt;h4&gt;方法&lt;/h4&gt;提出一种指纹框架，将受害者模型和可疑模型的嵌入建模为点云，执行鲁棒的空间对齐和相似度测量，通过分析嵌入空间拓扑结构的几何特性建立EaaS模型所有权，而非依赖修改的训练样本或触发器。&lt;h4&gt;主要发现&lt;/h4&gt;将嵌入建模为点云的方法实现了鲁棒的空间对齐和相似度测量；该方法能有效抵抗RST攻击；在视觉和文本嵌入任务中验证了该方法的优越性和适用性。&lt;h4&gt;结论&lt;/h4&gt;该研究揭示了EaaS模型的固有特性，并为黑盒场景下EaaS模型的所有权验证提供了一种有前景的解决方案，通过几何分析而非传统水印技术提供更鲁棒的验证方法。&lt;h4&gt;翻译&lt;/h4&gt;特征嵌入已成为处理高维和复杂数据的核心技术，这使得嵌入即服务（EaaS）模型已在云环境中广泛部署。为了保护EaaS模型的知识产权，现有方法应用数字水印技术，通过修改训练样本或网络参数向EaaS模型注入特定的后门触发器。然而，这些方法不可避免地会产生可通过语义分析检测到的模式，并且容易受到几何变换（包括旋转、缩放和平移，RST）的影响。为了解决这个问题，我们提出了一种用于EaaS模型的指纹框架，而非仅仅改进现有的水印技术。与水印技术不同，所提出的方法通过分析嵌入空间拓扑结构的几何特性来建立EaaS模型所有权，而不是依赖修改的训练样本或触发器。关键创新在于将受害者模型和可疑模型的嵌入建模为点云，使我们能够执行鲁棒的空间对齐和相似度测量，这 inherently 抵抗RST攻击。在视觉和文本嵌入任务上评估的实验结果验证了其优越性和适用性。这项研究揭示了EaaS模型的固有特性，并为黑盒场景下EaaS模型的所有权验证提供了一种有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature embedding has become a cornerstone technology for processinghigh-dimensional and complex data, which results in that Embedding as a Service(EaaS) models have been widely deployed in the cloud. To protect theintellectual property of EaaS models, existing methods apply digitalwatermarking to inject specific backdoor triggers into EaaS models by modifyingtraining samples or network parameters. However, these methods inevitablyproduce detectable patterns through semantic analysis and exhibitsusceptibility to geometric transformations including rotation, scaling, andtranslation (RST). To address this problem, we propose a fingerprintingframework for EaaS models, rather than merely refining existing watermarkingtechniques. Different from watermarking techniques, the proposed methodestablishes EaaS model ownership through geometric analysis of embeddingspace's topological structure, rather than relying on the modified trainingsamples or triggers. The key innovation lies in modeling the victim andsuspicious embeddings as point clouds, allowing us to perform robust spatialalignment and similarity measurement, which inherently resists RST attacks.Experimental results evaluated on visual and textual embedding tasks verify thesuperiority and applicability. This research reveals inherent characteristicsof EaaS models and provides a promising solution for ownership verification ofEaaS models under the black-box scenario.</description>
      <author>example@mail.com (Hongjie Zhang, Zhiqi Zhao, Hanzhou Wu, Zhihua Xia, Athanasios V. Vasilakos)</author>
      <guid isPermaLink="false">2510.16706v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Bayesian Framework for Symmetry Inference in Chaotic Attractors</title>
      <link>http://arxiv.org/abs/2510.16509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种贝叶斯框架，用于从动力系统轨迹数据中检测对称性，解决了现有方法缺乏不确定性量化的问题，提高了对噪声的鲁棒性，并能够处理层次对称结构。&lt;h4&gt;背景&lt;/h4&gt;对称性检测是信号分析中的基本问题，能揭示底层结构和约束。当数据表现为动力系统轨迹时，对称性编码了动力系统的结构特性，实现模型简化、条件间比较和状态变化检测。现有最优传输方法依赖确定性阈值且缺乏不确定性量化，限制了鲁棒性和层次对称结构的解决能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个贝叶斯框架，将对称性检测构建为在候选子群格上的概率模型选择，使用基于Wasserstein距离的Gibbs后验，以解决不确定性量化问题，提高对噪声的鲁棒性，并处理层次对称结构。&lt;h4&gt;方法&lt;/h4&gt;提出贝叶斯框架，将对称性检测表述为候选子群格上的概率模型选择，使用基于Wasserstein距离的Gibbs后验。通过Metropolis-Hastings采样进行后验推断，建立在三个理论保证上：贝叶斯奥卡姆剃刀原理、共轭等变性和扰动下的稳定性界限。&lt;h4&gt;主要发现&lt;/h4&gt;建立了三个理论保证：(i)贝叶斯奥卡姆剃刀原理，倾向于与数据一致的最小对称性；(ii)共轭等变性确保框架独立性；(iii)扰动下的稳定性界限确保对噪声的鲁棒性。数值实验展示了高噪声和小样本量下准确的对称性恢复，应用人类步行动力学揭示了机械约束引起的对称性变化。&lt;h4&gt;结论&lt;/h4&gt;该贝叶斯框架有效解决了对称性检测中的不确定性量化问题，提高了对噪声的鲁棒性，并能够处理层次对称结构，在生物力学和动力系统的统计推断中具有实用价值。&lt;h4&gt;翻译&lt;/h4&gt;从数据中检测对称性是信号分析中的一个基本问题，它为底层结构和约束提供了见解。当数据表现为动力系统的轨迹时，对称性编码了动力系统的结构特性，这些特性能够实现模型简化、条件间的有原则比较以及检测状态变化。虽然最近的最优传输方法为这种情况下的数据驱动对称性检测提供了实用工具，但它们依赖于确定性阈值且缺乏不确定性量化，限制了它们对噪声的鲁棒性以及解决层次对称结构的能力。我们提出了一个贝叶斯框架，将对称性检测构建为在候选子群格上的概率模型选择，使用基于观测数据与群变换副本之间Wasserstein距离构建的Gibbs后验。我们建立了三个理论保证：(i)贝叶斯奥卡姆剃刀原理，倾向于与数据一致的最小对称性；(ii)共轭等变性确保了框架独立性；(iii)扰动下的稳定性界限确保了对噪声的鲁棒性。后验推断通过Metropolis-Hastings采样进行，在等变动力系统和合成点云上的数值实验展示了在高噪声和小样本量下准确的对称性恢复。在人类步行动力学的应用中揭示了机械约束引起的对称性变化，证明了该框架在生物力学和动力系统统计推断中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting symmetry from data is a fundamental problem in signal analysis,providing insight into underlying structure and constraints. When data emergeas trajectories of dynamical systems, symmetries encode structural propertiesof the dynamics that enable model reduction, principled comparison acrossconditions, and detection of regime changes. While recent optimal transportmethods provide practical tools for data-driven symmetry detection in thissetting, they rely on deterministic thresholds and lack uncertaintyquantification, limiting robustness to noise and ability to resolvehierarchical symmetry structures. We present a Bayesian framework thatformulates symmetry detection as probabilistic model selection over a latticeof candidate subgroups, using a Gibbs posterior constructed from Wassersteindistances between observed data and group-transformed copies. We establishthree theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimalsymmetry consistent with data, $(ii)$ conjugation equivariance ensuringframe-independence, and $(iii)$ stability bounds under perturbations forrobustness to noise. Posterior inference is performed via Metropolis-Hastingssampling and numerical experiments on equivariant dynamical systems andsynthetic point clouds demonstrate accurate symmetry recovery under high noiseand small sample sizes. An application to human gait dynamics reveals symmetrychanges induced by mechanical constraints, demonstrating the framework'sutility for statistical inference in biomechanical and dynamical systems.</description>
      <author>example@mail.com (Ziad Ghanem, Chang Hyunwoong, Preskella Mrad)</author>
      <guid isPermaLink="false">2510.16509v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data</title>
      <link>http://arxiv.org/abs/2510.16071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多尺度神经网络算子(MNO)架构，用于三维非结构化点云上的计算流体动力学，通过分解三个尺度的信息来提高精度和可扩展性，在多个基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;现有神经网络算子在求解偏微分方程时虽然比传统求解器快几个数量级，但在精度和可扩展性方面仍存在局限，特别是在不规则域上具有丰富多尺度结构的流体流动问题中。&lt;h4&gt;目的&lt;/h4&gt;引入多尺度神经网络算子(MNO)架构，解决三维非结构化点云上的计算流体动力学问题，提高预测精度和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;MNO明确分解三个尺度的信息：全局维度缩减注意力模块处理长程依赖关系，局部图注意力模块处理邻域级相互作用，微观逐点注意力模块处理细粒度细节，这种设计保留了多尺度归纳偏置同时保持计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;在四个涵盖稳态和非稳态流动场景的基准测试上(最多30万个点)，MNO始终优于最先进的基线方法，减少5%到40%的预测误差，并在具有挑战性的三维CFD问题中表现出更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;明确的多尺度设计对神经网络算子至关重要，MNO可作为不规则域上学习复杂流体动力学的可扩展框架。&lt;h4&gt;翻译&lt;/h4&gt;神经网络算子已成为求解偏微分方程的强大数据驱动范式，比传统求解器快几个数量级。然而，现有方法在精度和可扩展性方面仍然有限，特别是在流体流动表现出丰富多尺度结构的不规则域上。在这项工作中，我们引入了多尺度神经网络算子(MNO)，这是一种用于三维非结构化点云上计算流体动力学(CFD)的新架构。MNO明确分解三个尺度的信息：全局维度缩减注意力模块用于长程依赖关系，局部图注意力模块用于邻域级相互作用，微观逐点注意力模块用于细粒度细节。这种设计保留了多尺度归纳偏置，同时保持计算效率。我们在四个不同的基准测试上评估了MNO，涵盖了最多30万个点的稳态和非稳态流动场景。在所有任务中，MNO始终优于最先进的基线方法，减少5%到40%的预测误差，并在具有挑战性的三维CFD问题中表现出改进的鲁棒性。我们的结果强调了明确的多尺度设计对神经网络算子的重要性，并将MNO确立为不规则域上学习复杂流体动力学的可扩展框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有神经算子在计算流体动力学(CFD)中处理不规则域时的局限，特别是对流体流动中丰富多尺度结构的建模不足问题。这个问题很重要，因为CFD在工程设计、气象预测等领域有广泛应用，而传统CFD计算成本高，难以实现实时计算；神经算子虽能提供更快速度，但在精度上仍落后于传统方法，特别是在处理复杂几何形状和动态域时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到物理量在流场中表现出强烈的多尺度效应（大尺度全局趋势、局部相互作用和细粒度点变化），而现有方法要么过于依赖全局建模牺牲局部细节，要么细粒度注意力机制计算成本过高。因此设计了MNO，包含三个互补的并行模块分别处理不同尺度信息。该方法借鉴了Transolver和LNO的低秩投影策略（全局模块）、Point Transformer的思想（局部模块），以及Encoder-MNO-Decoder的常见架构，但针对点云数据和多尺度特性进行了专门设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是显式地将信息分解为全局、局部和微观三个尺度，通过三个并行注意力模块分别捕获不同尺度的特征，并在每个块后融合这些特征。整体流程包括：1)输入处理接收3D点云数据；2)编码器将输入嵌入到潜在标记空间；3)MNO块处理（包含全局维度收缩注意力、局部图注意力和微观点级注意力三个并行模块）；4)特征融合三个模块的输出；5)解码器将处理后的特征映射回目标物理量；6)输出预测流场中的关键物理量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个针对3D无结构点云的多尺度神经算子；2)显式的多尺度分解设计，包含三个互补的注意力模块；3)直接在点云上处理，避免网格约束；4)统一框架同时提取全局、局部和细粒度流场表示。相比之前工作的不同：与规则域方法相比，MNO可直接处理不规则域；与不规则域方法相比，MNO明确考虑多尺度特性并提供更平衡的表示能力；与多尺度方法相比，MNO直接在点云上工作且不使用重复下采样/上采样，避免了信息丢失。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MNO通过显式的多尺度注意力机制设计，首次实现了在3D点云数据上高效准确地捕获流体流动的全局趋势、局部相互作用和细粒度细节，显著提高了不规则域上计算流体动力学任务的预测精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators have emerged as a powerful data-driven paradigm for solvingPartial Differential Equations (PDEs), offering orders-of-magnitudeacceleration over traditional solvers. However, existing approaches stillsuffer from limited accuracy and scalability, particularly on irregular domainswhere fluid flows exhibit rich multiscale structures. In this work, weintroduce the Multiscale Neural Operator (MNO), a new architecture forComputational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured pointclouds. MNO explicitly decomposes information across three scales: a globaldimension-shrinkage attention module for long-range dependencies, a local graphattention module for neighborhood-level interactions, and a micro point-wiseattention module for fine-grained details. This design preserves multiscaleinductive biases while remaining computationally efficient. We evaluate MNO onfour diverse benchmarks, covering both steady-state and unsteady flow scenarioswith up to 300K points. Across all tasks, MNO consistently outperformsstate-of-the-art baselines, reducing prediction errors by 5% to 40% anddemonstrating improved robustness in challenging 3D CFD problems. Our resultshighlight the importance of explicit multiscale design for neural operators andestablish MNO as a scalable framework for learning complex fluid dynamics onirregular domains.</description>
      <author>example@mail.com (Qinxuan Wang, Chuang Wang, Mingyu Zhang, Jingwei Sun, Peipei Yang, Shuo Tang, Shiming Xiang)</author>
      <guid isPermaLink="false">2510.16071v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception</title>
      <link>http://arxiv.org/abs/2510.17363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS 2025). 8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Multi-Mono-Hydra (M2H)的新型多任务学习框架，用于从单目图像中进行语义分割和深度、边缘及表面法线估计，在边缘设备上实现高效实时空间感知。&lt;h4&gt;背景&lt;/h4&gt;在边缘设备上部署实时空间感知需要高效的多任务模型，这些模型需要能够利用互补任务信息同时最小化计算开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种优化的多任务学习框架，支持从单目图像中同时进行多种空间感知任务，并能在边缘设备上实时运行，为动态环境中的3D场景图构建提供基础。&lt;h4&gt;方法&lt;/h4&gt;M2H框架采用基于窗口的跨任务注意模块实现结构化特征交换，同时保留任务特定细节，提高预测一致性。框架基于轻量级ViT-based DINOv2主干网络构建，优化了实时部署性能。&lt;h4&gt;主要发现&lt;/h4&gt;M2H在NYUDv2数据集上超越最先进的多任务模型；在Hypersim上超越单任务深度和语义基线；在Cityscapes数据集上表现优异；同时保持笔记本电脑硬件上的计算效率；在真实世界数据验证中展现出实用性。&lt;h4&gt;结论&lt;/h4&gt;M2H通过创新的跨任务注意机制有效利用了任务间的互补信息，在多个基准测试和实际应用中表现出色，是一种适用于边缘设备的高效多任务空间感知解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在边缘设备上部署实时空间感知需要高效的多任务模型，这些模型能够利用互补任务信息同时最小化计算开销。本文介绍了Multi-Mono-Hydra (M2H)，一种新颖的多任务学习框架，专为从单目图像进行语义分割和深度、边缘及表面法线估计而设计。与传统依赖独立单任务模型或共享编码器-解码器架构的方法不同，M2H引入了基于窗口的跨任务注意模块，能够在保留任务特定细节的同时实现结构化特征交换，提高跨任务预测一致性。基于轻量级ViT-based DINOv2主干网络构建，M2H针对实时部署进行优化，并作为支持动态环境中3D场景图构建的单目空间感知系统的基础。全面评估显示，M2H在NYUDv2上超越最先进的多任务模型，在Hypersim上超越单任务深度和语义基线，在Cityscapes数据集上实现卓越性能，同时在笔记本电脑硬件上保持计算效率。除了基准测试外，M2H还在真实世界数据上得到验证，展示了其在空间感知任务中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在边缘设备上实现高效的多任务空间感知问题，特别是从单目图像同时进行语义分割和深度、边缘、表面法线估计。这个问题在现实中非常重要，因为自动驾驶系统、增强现实和机器人感知等应用需要实时理解环境，而边缘设备资源有限，需要高效的多任务模型来利用任务间的互补信息，同时最小化计算开销。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于Taskonomy的研究，认识到任务间关系可以减少监督需求并提高泛化能力。他们发现现有方法存在局限：局部交互保留了效率但限制了协同效应，全局注意力提供了丰富上下文但计算开销高。因此设计了一种平衡效率与协同效应的框架。作者借鉴了DINOv2的轻量级ViT主干网络、Swin Transformer的窗口注意力机制，并参考了注意力机制在其他多任务学习方法中的应用，如PAD-Net和MTI-Net。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入基于窗口的跨任务注意力模块实现结构化特征交换，同时保留任务特定细节，并使用轻量级ViT主干网络优化实时部署。整体流程：1) DINOv2编码器提取多尺度令牌表示；2) 通过MSTR块重组为空间特征图；3) MSF块生成任务特定特征；4) 双路径细化：WMCA捕获局部跨任务交互，GGFM聚合全局上下文；5) 融合局部和全局特征；6) 专用解码器头生成最终预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 窗口多任务交叉注意力(WMCA)模块，在局部窗口内高效交换信息；2) 全局门控特征合并(GGFM)模块，使用门控机制聚合全局上下文；3) 双路径细化策略，平衡特征交换深度与计算效率；4) 使用动态权重平均(DWA)平衡跨任务学习。相比之前工作，M2H在保持计算效率的同时实现了更好的性能，超越了多种最先进方法，特别适合边缘设备上的实时应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M2H通过创新的窗口化跨任务注意力和双路径特征融合机制，实现了在边缘设备上高效运行的多任务空间感知系统，在保持计算效率的同时超越了现有最先进方法在多个任务和数据集上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying real-time spatial perception on edge devices requires efficientmulti-task models that leverage complementary task information while minimizingcomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novelmulti-task learning framework designed for semantic segmentation and depth,edge, and surface normal estimation from a single monocular image. Unlikeconventional approaches that rely on independent single-task models or sharedencoder-decoder architectures, M2H introduces a Window-Based Cross-TaskAttention Module that enables structured feature exchange while preservingtask-specific details, improving prediction consistency across tasks. Built ona lightweight ViT-based DINOv2 backbone, M2H is optimized for real-timedeployment and serves as the foundation for monocular spatial perceptionsystems supporting 3D scene graph construction in dynamic environments.Comprehensive evaluations show that M2H outperforms state-of-the-art multi-taskmodels on NYUDv2, surpasses single-task depth and semantic baselines onHypersim, and achieves superior performance on the Cityscapes dataset, allwhile maintaining computational efficiency on laptop hardware. Beyondbenchmarks, M2H is validated on real-world data, demonstrating its practicalityin spatial perception tasks.</description>
      <author>example@mail.com (U. V. B. L Udugama, George Vosselman, Francesco Nex)</author>
      <guid isPermaLink="false">2510.17363v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2510.17274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In proceedings of IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'即插即预测'(PnF)的即插即用方法，通过多模态大语言模型增强现有运动预测模型，提高自动驾驶系统在多样化实际场景中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶系统依赖专用模型进行感知和运动预测，在标准条件下表现可靠，但在多样化实际场景中泛化且经济高效地适应仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法解决自动驾驶系统在多样化实际场景中的泛化问题。&lt;h4&gt;方法&lt;/h4&gt;PnF方法利用自然语言描述复杂场景的优势，设计提示从多模态大语言模型中提取结构化场景理解，并将这些信息蒸馏为可学习的嵌入，增强现有行为预测模型，无需微调即可实现性能提升。&lt;h4&gt;主要发现&lt;/h4&gt;该方法利用多模态大语言模型的零样本推理能力，显著提高了运动预测性能，同时不需要微调，使其易于实际采用。&lt;h4&gt;结论&lt;/h4&gt;在Waymo OpenMotion数据集和nuScenes数据集上使用两种最先进的运动预测模型验证了该方法，证明了在两个基准测试中都有一致的性能改进。&lt;h4&gt;翻译&lt;/h4&gt;当前的自动驾驶系统依赖于用于感知和预测运动的专用模型，这些模型在标准条件下表现出可靠的性能。然而，泛化到多样化的实际场景并保持成本效益仍然是一个重大挑战。为此，我们提出了即插即预测(PnF)，一种即插即用的方法，通过多模态大语言模型(MLLMs)增强现有的运动预测模型。PnF基于自然语言能更有效地描述和处理复杂场景的见解，实现了针对特定行为的快速适应。我们设计了提示从MLLMs中提取结构化的场景理解，并将这些信息蒸馏为可学习的嵌入，以增强现有的行为预测模型。我们的方法利用MLLMs的零样本推理能力，显著提高了运动预测性能，同时不需要微调，使其易于实际采用。我们在Waymo OpenMotion数据集和nuScenes数据集上使用两种最先进的运动预测模型验证了我们的方法，证明了在两个基准测试中都有一致的性能改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶系统在多样化现实场景中运动预测模型的泛化能力问题。这个问题很重要，因为自动驾驶车辆不可避免会遇到训练数据中未涵盖的罕见情况(长尾案例)，而持续收集大量数据和重新训练系统的成本过高。提高系统在复杂场景下的泛化能力对确保安全性和实用性至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从多模态大语言模型(MLLM)的进展获得灵感，认识到语言能更好地描述复杂场景。他们设计提示从MLLM中提取结构化场景理解，并将其转化为可学习的嵌入来增强现有预测模型。借鉴了现有运动预测模型(如Wayformer和MotionLM)的架构，以及MLLM在其他自动驾驶应用中的使用，但创新点在于采用零样本推理而非微调方式，保留了MLLM的通用能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用MLLM的零样本推理能力增强现有运动预测模型，通过自然语言描述更有效地处理复杂场景。整体流程包括：1)视觉语义分析器(VSA)提取代理特定语义；2)场景分类器(SC)提供场景级理解；3)通过学习的信息增益机制将MLLM提取的特征选择性整合到预测模型中。整个过程作为即插即用组件，无需对MLLM进行微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)即插即用框架PnF；2)利用MLLM零样本推理能力；3)多模态提示设计；4)选择性信息整合机制；5)双重分析组件(代理级和场景级)。相比之前工作，PnF不需要对MLLM进行微调，保留了其通用能力；不同于直接映射原始数据的EMMA模型，PnF专注于增强运动预测任务；相比其他需要修改架构的方法，PnF可作为轻量级组件集成到现有系统。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的Plug-and-Forecast方法通过即插即用多模态大语言模型的零样本推理能力，显著提升了自动驾驶系统中运动预测模型的性能，特别是在处理复杂和罕见场景时无需对语言模型进行微调。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current autonomous driving systems rely on specialized models for perceivingand predicting motion, which demonstrate reliable performance in standardconditions. However, generalizing cost-effectively to diverse real-worldscenarios remains a significant challenge. To address this, we proposePlug-and-Forecast (PnF), a plug-and-play approach that augments existing motionforecasting models with multimodal large language models (MLLMs). PnF builds onthe insight that natural language provides a more effective way to describe andhandle complex scenarios, enabling quick adaptation to targeted behaviors. Wedesign prompts to extract structured scene understanding from MLLMs and distillthis information into learnable embeddings to augment existing behaviorprediction models. Our method leverages the zero-shot reasoning capabilities ofMLLMs to achieve significant improvements in motion prediction performance,while requiring no fine-tuning -- making it practical to adopt. We validate ourapproach on two state-of-the-art motion forecasting models using the Waymo OpenMotion Dataset and the nuScenes Dataset, demonstrating consistent performanceimprovements across both benchmarks.</description>
      <author>example@mail.com (Katie Luo, Jingwei Ji, Tong He, Runsheng Xu, Yichen Xie, Dragomir Anguelov, Mingxing Tan)</author>
      <guid isPermaLink="false">2510.17274v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.16714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://scenecot.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的SCENECOT框架，首次将思维链推理应用于3D场景理解，解决了现有3D大型语言模型在实现基于场景的问答方面的困难。&lt;h4&gt;背景&lt;/h4&gt;现有关于3D大型语言模型的研究在实现基于场景的问答方面仍然存在困难，主要原因是对类人场景-对象基础推理机制的探索不足。&lt;h4&gt;目的&lt;/h4&gt;通过提出一个新颖的框架来填补3D场景理解中类人推理机制的空白。&lt;h4&gt;方法&lt;/h4&gt;作者在3D场景中引入了基础的思维链推理方法（SCENECOT），将复杂推理任务解耦为更简单的问题，并基于多模态专家模块构建视觉线索。同时开发了SCENECOT-185K数据集，包含185K个高质量实例。&lt;h4&gt;主要发现&lt;/h4&gt;在各种复杂的3D场景推理基准上的广泛实验表明，新框架在保持高基础问答连贯性的同时实现了强大的性能。&lt;h4&gt;结论&lt;/h4&gt;思维链推理首次成功应用于3D场景理解，实现了类人的逐步推理，并显示出扩展到更广泛的3D场景理解场景的潜力。&lt;h4&gt;翻译&lt;/h4&gt;现有关于3D大型语言模型的研究在实现基于场景的问答方面仍然存在困难，主要原因是对类人场景-对象基础推理机制的探索不足。本文通过提出一个新颖的框架来填补这一空白。我们首先在3D场景中引入了一种基础的思维链推理方法（SCENECOT），将复杂的推理任务解耦为更简单、更易管理的问题，并基于多模态专家模块构建相应的视觉线索。为此，我们开发了SCENECOT-185K，这是第一个大规模的基础思维链推理数据集，包含185K个高质量实例。在各种复杂的3D场景推理基准上的广泛实验表明，我们的新框架在保持高基础问答连贯性的同时实现了强大的性能。据我们所知，这是思维链推理首次成功应用于3D场景理解，实现了类人的逐步推理，并显示出扩展到更广泛的3D场景理解场景的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D大语言模型难以实现接地式问答的问题，即模型生成的回答与3D场景中的实际对象和空间关系缺乏明确联系。这个问题很重要，因为3D场景理解是构建人类级别具身智能体的基础能力，而现有模型往往产生看似合理但未与场景关联的答案，导致接地-问答连贯性差，阻碍了AI系统在需要精确空间理解的应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别3D推理的挑战（导航大空间、解释复杂空间关系、处理部分可观察性），借鉴了语言领域的思维链（CoT）推理方法，将复杂问题分解为可管理的子问题。他们还参考了2D视觉-语言模型中的CoT应用，将其扩展到3D场景理解。此外，作者结合了多模态大语言模型、专门的3D-VL和2D-VL模型以及符号引擎等现有技术，构建了SCENECOT框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂的3D场景推理任务分解为四个明确的阶段，每个阶段都引入明确的接地信号，确保逐步推理并提高接地-问答连贯性。整体流程包括：1）任务识别和分析，识别问题类型和初始分析；2）任务相关区域定位，缩小推理空间；3）实体和属性接地，使用多模态专家模块关联目标对象；4）接地推理，集成中间结果生成最终答案。整个过程使用特殊标记表示不同推理阶段，结合多种模型共同完成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）SCENECOT框架，首次将思维链推理应用于3D场景理解；2）SCENECOT-185K数据集，首个大规模接地CoT推理数据集；3）四阶段推理流程，明确分解复杂任务；4）显著提高接地-问答连贯性。相比之前工作，SCENECOT采用逐步推理而非单步任务，在每个阶段都引入明确的接地信号，专门设计了针对3D场景的推理数据集，并在接地-问答连贯性方面表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCENECOT通过引入首个专门为3D场景设计的接地式思维链推理框架和数据集，显著提升了AI系统在复杂3D环境中的类人推理能力和答案与场景的连贯性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing research on 3D Large Language Models (LLMs) still struggles toachieve grounded question-answering, primarily due to the under-exploration ofthe mech- anism of human-like scene-object grounded reasoning. This paperbridges the gap by presenting a novel framework. We first introduce a groundedChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling acomplex reasoning task into simpler and manageable problems, and buildingcorresponding visual clues based on multimodal expert modules. To enable such amethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoningdataset, consisting of 185K high-quality instances. Extensive experimentsacross various complex 3D scene reasoning benchmarks demonstrate that our newframework achieves strong performance with high grounding-QA coherence. To thebest of our knowledge, this is the first successful application of CoTreasoning to 3D scene understanding, enabling step-by-step human-like reasoningand showing potential for extension to broader 3D scene understandingscenarios.</description>
      <author>example@mail.com (Xiongkun Linghu, Jiangyong Huang, Ziyu Zhu, Baoxiong Jia, Siyuan Huang)</author>
      <guid isPermaLink="false">2510.16714v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Structured Interfaces for Automated Reasoning with 3D Scene Graphs</title>
      <link>http://arxiv.org/abs/2510.16643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合大型语言模型与三维场景图的新方法，通过检索增强生成技术选择相关场景图子集，并使用图数据库和Cypher查询语言接口作为工具，提高自然语言与机器人世界表示之间的连接效率。&lt;h4&gt;背景&lt;/h4&gt;为了使机器人能够理解和响应用户自然语言输入，需要将自然语言与机器人对世界的基础表示连接起来。目前，大型语言模型和三维场景图已成为热门选择，但现有方法将场景图编码为LLM上下文窗口内的序列化文本，无法有效处理大型或丰富的场景图。&lt;h4&gt;目的&lt;/h4&gt;解决使用大型语言模型与三维场景图连接自然语言时的可扩展性问题，特别是处理大型或丰富场景图的挑战。&lt;h4&gt;方法&lt;/h4&gt;采用检索增强生成技术选择与任务相关的三维场景图子集，将场景图编码到图数据库中，并提供Cypher查询语言接口作为大型语言模型的工具，使其能够检索与语言连接相关的数据。&lt;h4&gt;主要发现&lt;/h4&gt;在指令跟随和场景问答任务上的评估表明，使用Cypher作为三维场景图的接口能更好地扩展到大型、丰富的图形，显著提高语言连接任务性能，同时大幅减少场景图内容的token数量。&lt;h4&gt;结论&lt;/h4&gt;通过将三维场景图存储在图数据库并提供Cypher接口作为工具，可有效解决大型场景图的表示问题，提高自然语言与机器人世界表示之间的连接效率，减少计算资源消耗。&lt;h4&gt;翻译&lt;/h4&gt;为了使机器人具备理解和响应用户自然语言输入的能力，自然语言必须与机器人对世界的基础表示相连接。最近，大型语言模型和三维场景图已成为连接自然语言和表示世界的流行选择。在这项工作中，我们解决了使用大型语言模型与三维场景图连接自然语言的挑战。现有方法将场景图编码为大型语言模型上下文窗口内的序列化文本，但这种编码无法扩展到大型或丰富的三维场景图。相反，我们提议使用一种检索增强生成形式来选择与任务相关的三维场景图子集。我们将三维场景图编码在图数据库中，并提供Cypher查询语言接口作为大型语言模型的工具，使其能够检索与语言连接相关的数据。我们在指令跟随和场景问答任务上评估了我们的方法，并与基线上下文窗口和代码生成方法进行了比较。我们的结果表明，使用Cypher作为三维场景图的接口，在本地和基于云的模型上都能显著更好地扩展到大型、丰富的图形。这大大提高了语言连接任务的性能，同时大幅减少了场景图内容的token数量。视频补充材料可在https://www.youtube.com/watch?v=zY_YI9giZSA获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让大型语言模型（LLMs）有效地利用3D场景图（3DSGs）来理解并处理自然语言指令的问题。这个问题非常重要，因为它关系到机器人能否理解并执行人类的自然语言指令，对于人机交互至关重要。传统方法将3D场景图序列化为文本放入LLMs上下文窗口，但这种方法无法扩展到大型场景图、难以处理空间关系推理、且依赖LLMs进行定量推理（而这并非其强项）。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是序列化方法无法处理大型场景图的问题。他们考虑了两种替代方法：使用LLMs过滤节点和基于向量的RAG，但发现这些方法各有不足。作者借鉴了现有的检索增强生成（RAG）技术和图数据库技术，结合'代理AI'概念，设计出使用Cypher查询语言作为LLMs和3D场景图之间接口的新方法。他们特别受到GraphRAG技术的启发，将其专门应用于3D场景图和LLMs的结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图检索增强生成（GraphRAG）方法，通过Cypher查询语言作为大型语言模型（LLMs）和3D场景图（3DSGs）之间的接口。整体流程包括：1)将3D场景图编码到图数据库中；2)向LLMs提供Cypher查询作为工具；3)当用户输入自然语言时，LLMs决定是否需要查询场景图；4)如果需要，LLMs生成一个或多个Cypher查询；5)执行查询并获取结果；6)使用查询结果作为上下文生成最终响应（对于指令跟随任务转换为PDDL目标，对于问答任务直接回答问题）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用Cypher查询语言作为结构化接口；2)将RAG技术扩展到图数据结构（GraphRAG）；3)采用代理AI方法允许LLMs主动决定如何与场景图交互；4)利用图数据库的几何空间索引功能处理定量推理；5)有效处理大型、丰富的3D场景图。相比之前的工作，本文不再将整个场景图序列化为文本，而是按需查询；不依赖向量检索，而是使用结构化的图查询语言；不依赖LLMs进行定量推理，而是利用图数据库的专门功能；并采用更灵活的代理交互方式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种使用Cypher查询语言作为大型语言模型和3D场景图之间结构化接口的新方法，通过图检索增强生成技术，使机器人系统能够有效地理解和执行自然语言指令，同时解决了传统方法在处理大型场景图和定量推理方面的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In order to provide a robot with the ability to understand and react to auser's natural language inputs, the natural language must be connected to therobot's underlying representations of the world. Recently, large languagemodels (LLMs) and 3D scene graphs (3DSGs) have become a popular choice forgrounding natural language and representing the world. In this work, we addressthe challenge of using LLMs with 3DSGs to ground natural language. Existingmethods encode the scene graph as serialized text within the LLM's contextwindow, but this encoding does not scale to large or rich 3DSGs. Instead, wepropose to use a form of Retrieval Augmented Generation to select a subset ofthe 3DSG relevant to the task. We encode a 3DSG in a graph database and providea query language interface (Cypher) as a tool to the LLM with which it canretrieve relevant data for language grounding. We evaluate our approach oninstruction following and scene question-answering tasks and compare againstbaseline context window and code generation methods. Our results show thatusing Cypher as an interface to 3D scene graphs scales significantly better tolarge, rich graphs on both local and cloud-based models. This leads to largeperformance improvements in grounded language tasks while also substantiallyreducing the token count of the scene graph content. A video supplement isavailable at https://www.youtube.com/watch?v=zY_YI9giZSA.</description>
      <author>example@mail.com (Aaron Ray, Jacob Arkin, Harel Biggie, Chuchu Fan, Luca Carlone, Nicholas Roy)</author>
      <guid isPermaLink="false">2510.16643v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2510.16410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了REALM，一个创新的MLLM-agent框架，实现了开放世界的基于推理的3D物体分割，无需大量3D特定后训练。&lt;h4&gt;背景&lt;/h4&gt;在视觉和机器人领域，弥合复杂人类指令与精确3D物体定位之间的差距仍然是一个重大挑战。现有的3D分割方法难以解释模糊的、基于推理的指令，而擅长此类推理的2D视觉-语言模型则缺乏内在的3D空间理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够理解复杂人类指令并进行精确3D物体分割的方法，无需大量3D特定后训练。&lt;h4&gt;方法&lt;/h4&gt;直接在3D高斯溅射表示上进行分割，利用其渲染逼真新视图的能力。提出全局到局部空间定位策略：先并行输入多个全局视图到MLLM代理进行粗略定位，聚合响应识别目标物体；然后合成物体的特写新视图进行细粒度局部分割，获得准确且一致的3D掩码。&lt;h4&gt;主要发现&lt;/h4&gt;REALM在解释LERF、3D-OVS和REALM3D基准测试中的显式和隐式指令方面取得了显著性能，并能支持一系列3D交互任务。&lt;h4&gt;结论&lt;/h4&gt;REALM代理框架具有实用性和多功能性，能够无缝支持物体移除、替换和风格转换等多种3D交互任务。&lt;h4&gt;翻译&lt;/h4&gt;弥合复杂人类指令与精确3D物体定位之间的差距在视觉和机器人领域仍然是一个重大挑战。现有的3D分割方法往往难以解释模糊的、基于推理的指令，而擅长此类推理的2D视觉-语言模型则缺乏内在的3D空间理解。在本文中，我们介绍了REALM，一个创新的MLLM-agent框架，能够实现开放世界的基于推理的分割，而无需大量的3D特定后训练。我们直接在3D高斯溅射表示上进行分割，利用其能够渲染高度适合MLLM理解的逼真新视图的能力。由于直接将一个或多个渲染视图输入到MLLM可能导致对视角选择的高度敏感性，我们提出了一种新颖的全局到局部空间定位策略。具体来说，多个全局视图首先并行输入到MLLM代理中进行粗略定位，聚合响应以稳健地识别目标物体。然后，合成物体的几个特写新视图以执行细粒度的局部分割，从而获得准确且一致的3D掩码。大量实验表明，REALM在解释LERF、3D-OVS和我们新引入的REALM3D基准测试中的显式和隐式指令方面取得了显著性能。此外，我们的代理框架无缝支持一系列3D交互任务，包括物体移除、替换和风格转换，展示了其实用性和多功能性。项目页面：https://ChangyueShi.github.io/REALM。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何弥合复杂人类指令和精确3D物体定位之间的差距。这个问题重要是因为AI系统需要理解并能够通过自然语言与3D世界交互，这对未来机器人和人机协作至关重要。目前，3D分割方法难以解释模糊的推理指令，而2D视觉语言模型又缺乏3D空间理解能力，这限制了AI在现实场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法各有局限：3D分割方法擅长直接查询但缺乏推理能力，而2D视觉语言模型能推理但缺乏3D空间意识。作者借鉴了3D高斯溅射(3DGS)作为3D世界的高保真表示，结合SAM进行实例分割，并利用多模态大语言模型(MLLM)进行推理。为解决视角选择敏感性问题，作者设计了全局到局部空间定位策略，通过多视角聚合提高分割准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用MLLM的推理能力和3DGS的高保真表示，实现开放世界的3D推理分割和编辑。整体流程包括：1)构建3D特征场为每个高斯基元分配身份特征；2)使用MLLM基础视觉分割器(LMSeg)进行图像级推理；3)通过全局到局部空间定位(GLSpaG)策略聚合多视图结果，先从全局视角粗略定位目标，再从局部视角细粒度分割；4)基于分割结果执行各种3D编辑任务如移除、替换和风格转换。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)REALM框架实现无需大量3D特定后训练的开放世界3D推理分割；2)MLLM基础实例分割器结合MLLM和SAM的能力；3)全局到局部空间定位策略提高分割准确性；4)REALM3D基准数据集促进研究。相比之前工作，REALM能处理需要推理空间关系、语义属性或常识的模糊指令，将2D推理能力提升到3D领域，支持多种3D交互任务，解决了视角选择敏感性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; REALM通过将多模态大语言模型的推理能力与3D高斯溅射的高保真表示相结合，实现了开放世界中的3D推理分割和编辑，解决了现有方法在处理模糊、基于推理的3D指令时的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bridging the gap between complex human instructions and precise 3D objectgrounding remains a significant challenge in vision and robotics. Existing 3Dsegmentation methods often struggle to interpret ambiguous, reasoning-basedinstructions, while 2D vision-language models that excel at such reasoning lackintrinsic 3D spatial understanding. In this paper, we introduce REALM, aninnovative MLLM-agent framework that enables open-world reasoning-basedsegmentation without requiring extensive 3D-specific post-training. We performsegmentation directly on 3D Gaussian Splatting representations, capitalizing ontheir ability to render photorealistic novel views that are highly suitable forMLLM comprehension. As directly feeding one or more rendered views to the MLLMcan lead to high sensitivity to viewpoint selection, we propose a novelGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global viewsare first fed into the MLLM agent in parallel for coarse-level localization,aggregating responses to robustly identify the target object. Then, severalclose-up novel views of the object are synthesized to perform fine-grainedlocal segmentation, yielding accurate and consistent 3D masks. Extensiveexperiments show that REALM achieves remarkable performance in interpretingboth explicit and implicit instructions across LERF, 3D-OVS, and our newlyintroduced REALM3D benchmarks. Furthermore, our agent framework seamlesslysupports a range of 3D interaction tasks, including object removal,replacement, and style transfer, demonstrating its practical utility andversatility. Project page: https://ChangyueShi.github.io/REALM.</description>
      <author>example@mail.com (Changyue Shi, Minghao Chen, Yiping Mao, Chuxiao Yang, Xinyuan Hu, Jiajun Ding, Zhou Yu)</author>
      <guid isPermaLink="false">2510.16410v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with Thermal IR (LWIR/MWIR) and RGB</title>
      <link>http://arxiv.org/abs/2510.13404v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种通过合成短波红外（SWIR）图像来增强多模态融合框架的方法，以改善在能见度不良条件下的场景理解，同时保持实时性能。&lt;h4&gt;背景&lt;/h4&gt;在能见度不良条件下进行场景理解是监控和自主导航系统面临的重大挑战。传统成像模式（如RGB和热红外）在融合时往往无法提供全面的场景信息，特别是在大气干扰或照明不足的条件下。SWIR成像虽然能够穿透大气干扰并提供更清晰的材料区分能力，但其广泛应用面临主要障碍是缺乏公开可用的SWIR数据集。&lt;h4&gt;目的&lt;/h4&gt;解决SWIR数据集稀缺的问题，通过从现有LWIR数据合成生成类似SWIR的结构/对比度提示图像，并开发一种多模态融合框架，整合合成的SWIR、LWIR和RGB模式，以提高在不良能见度条件下的场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种从现有LWIR数据合成生成类似SWIR的结构/对比度提示图像的方法，然后提出了一种多模态融合框架，整合合成的SWIR、LWIR和RGB模式。该框架采用优化的编码器-解码器神经网络架构，具有模态特定的编码器和softmax门控融合头。&lt;h4&gt;主要发现&lt;/h4&gt;在公共RGB-LWIR基准测试集和额外的私有真实RGB-MWIR-SWIR数据集上的综合实验表明，合成SWIR增强融合框架提高了融合图像质量（对比度、边缘定义、结构保真度），同时保持实时性能。研究还添加了公平的三模态基线和级联三模态变体。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的合成SWIR增强融合框架在监控和自主系统等实际应用中具有巨大潜力，能够有效改善在能见度不良条件下的场景理解能力。&lt;h4&gt;翻译&lt;/h4&gt;在能见度不良条件下增强场景理解仍然是监控和自主导航系统的关键挑战。传统成像模式，如RGB和热红外（MWIR/LWIR），在融合时往往无法提供全面的场景信息，特别是在大气干扰或照明不足的条件下。为了解决这些局限性，短波红外（SWIR）成像已成为一种有前景的模式，因为它能够穿透大气干扰并提供更清晰的材料区分能力。然而，基于SWIR系统的进步和广泛应用面临重大障碍，主要是由于公开可用的SWIR数据集稀缺。为应对这一挑战，我们的研究提出了一种使用先进的对比度增强技术从现有LWIR数据合成生成类似SWIR的结构/对比度提示（不声称光谱再现）图像的方法。随后，我们提出了一种多模态融合框架，整合合成的SWIR、LWIR和RGB模式，采用优化的编码器-解码器神经网络架构，具有模态特定的编码器和softmax门控融合头。在公共RGB-LWIR基准（M3FD、TNO、CAMEL、MSRS、RoadScene）和额外的私有真实RGB-MWIR-SWIR数据集上的综合实验表明，我们的合成SWIR增强融合框架提高了融合图像质量（对比度、边缘定义、结构保真度），同时保持实时性能。我们还添加了公平的三模态基线（LP、LatLRR、GFF）和U2Fusion/SwinFusion的级联三模态变体，采用统一协议。结果突显了在监控和自主系统中实际应用的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enhancing scene understanding in adverse visibility conditions remains acritical challenge for surveillance and autonomous navigation systems.Conventional imaging modalities, such as RGB and thermal infrared (MWIR /LWIR), when fused, often struggle to deliver comprehensive scene information,particularly under conditions of atmospheric interference or inadequateillumination. To address these limitations, Short-Wave Infrared (SWIR) imaginghas emerged as a promising modality due to its ability to penetrate atmosphericdisturbances and differentiate materials with improved clarity. However, theadvancement and widespread implementation of SWIR-based systems facesignificant hurdles, primarily due to the scarcity of publicly accessible SWIRdatasets. In response to this challenge, our research introduces an approach tosynthetically generate SWIR-like structural/contrast cues (without claimingspectral reproduction) images from existing LWIR data using advanced contrastenhancement techniques. We then propose a multimodal fusion frameworkintegrating synthetic SWIR, LWIR, and RGB modalities, employing an optimizedencoder-decoder neural network architecture with modality-specific encoders anda softmax-gated fusion head. Comprehensive experiments on public RGB-LWIRbenchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private realRGB-MWIR-SWIR dataset demonstrate that our synthetic-SWIR-enhanced fusionframework improves fused-image quality (contrast, edge definition, structuralfidelity) while maintaining real-time performance. We also add fair trimodalbaselines (LP, LatLRR, GFF) and cascaded trimodal variants ofU2Fusion/SwinFusion under a unified protocol. The outcomes highlightsubstantial potential for real-world applications in surveillance andautonomous systems.</description>
      <author>example@mail.com (Muhammad Ishfaq Hussain, Ma Van Linh, Zubia Naz, Unse Fatima, Yeongmin Ko, Moongu Jeon)</author>
      <guid isPermaLink="false">2510.13404v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence</title>
      <link>http://arxiv.org/abs/2510.16555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Urban-R1框架，一种基于强化学习的后训练方法，用于解决城市基础模型中的地域偏见问题，有效提升了跨区域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;快速城市化加剧了对城市通用智能(UGI)的需求，即能够理解和推理复杂城市环境的AI系统。现有使用监督微调(SFT)构建的城市基础模型存在持续的地域偏见，产生区域倾斜的预测和有限的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出Urban-R1框架，使多模态大型语言模型(MLLMs)与UGI目标保持一致，缓解地域偏见并提高跨区域泛化能力。&lt;h4&gt;方法&lt;/h4&gt;Urban-R1采用组相对策略优化(GRPO)来优化跨地理群体的推理，并使用城市区域画像作为代理任务，从多模态城市数据提供可衡量的奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在不同地区和任务的广泛实验中，Urban-R1有效缓解了地域偏见，改善了跨区域泛化能力，性能优于监督微调训练和闭源模型。&lt;h4&gt;结论&lt;/h4&gt;强化学习对齐是迈向公平和可信城市智能的有前景的途径。&lt;h4&gt;翻译&lt;/h4&gt;快速城市化加剧了对城市通用智能(UGI)的需求，UGI指的是能够理解和推理复杂城市环境的AI系统。最近的研究使用监督微调(SFT)构建了城市基础模型，但这些模型存在持续的地域偏见，产生区域倾斜的预测和有限的泛化能力。为此，我们提出Urban-R1，一个基于强化学习的后训练框架，使MLLMs与UGI目标保持一致。Urban-R1采用组相对策略优化(GRPO)来优化跨地理群体的推理，并使用城市区域画像作为代理任务，从多模态城市数据提供可衡量的奖励。在不同地区和任务的广泛实验表明，Urban-R1有效缓解了地域偏见并改善了跨区域泛化能力，性能优于监督微调训练和闭源模型。我们的结果强调了强化学习对齐作为迈向公平和可信城市智能的有前景途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid urbanization intensifies the demand for Urban General Intelligence(UGI), referring to AI systems that can understand and reason about complexurban environments. Recent studies have built urban foundation models usingsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibitpersistent geospatial bias, producing regionally skewed predictions and limitedgeneralization. To this end, we propose Urban-R1, a reinforcementlearning-based post-training framework that aligns MLLMs with the objectives ofUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimizereasoning across geographic groups and employs urban region profiling as aproxy task to provide measurable rewards from multimodal urban data. Extensiveexperiments across diverse regions and tasks show that Urban-R1 effectivelymitigates geo-bias and improves cross-region generalization, outperforming bothSFT-trained and closed-source models. Our results highlight reinforcementlearning alignment as a promising pathway toward equitable and trustworthyurban intelligence.</description>
      <author>example@mail.com (Qiongyan Wang, Xingchen Zou, Yutian Jiang, Haomin Wen, Jiaheng Wei, Qingsong Wen, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.16555v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Lingua Custodi's participation at the WMT 2025 Terminology shared task</title>
      <link>http://arxiv.org/abs/2510.17504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究BERT-based跨语言句子嵌入方法，结合单语和跨语言表征学习技术，显著减少所需训练数据量，并在多语言任务上取得优异性能。&lt;h4&gt;背景&lt;/h4&gt;BERT在单语句子嵌入学习中表现有效，但BERT-based跨语言句子嵌入尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统研究学习多语言句子嵌入的方法，结合单语和跨语言表征的最佳方法。&lt;h4&gt;方法&lt;/h4&gt;结合掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax等方法。&lt;h4&gt;主要发现&lt;/h4&gt;引入预训练多语言语言模型可将实现良好性能所需的并行训练数据量减少80%；组合方法产生的模型在112种语言上达到83.7%的双语检索准确率，高于LASER的65.5%；在单语迁移学习基准测试中仍具竞争力；使用该方法挖掘的并行数据可训练出具有竞争力的NMT模型。&lt;h4&gt;结论&lt;/h4&gt;公开发布了109+语言的最佳多语言句子嵌入模型。&lt;h4&gt;翻译&lt;/h4&gt;虽然BERT是学习单语句子嵌入用于语义相似性和基于嵌入的迁移学习的有效方法，但基于BERT的跨语言句子嵌入尚未被探索。我们通过结合学习单语和跨语言表征的最佳方法，系统研究了学习多语言句子嵌入的方法，包括：掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax。我们证明，引入预训练多语言语言模型可将实现良好性能所需的并行训练数据量减少80%。组合这些最佳方法产生的模型在Tatoeba的112种语言上达到83.7%的双语检索准确率，远高于LASER的65.5%，同时在单语迁移学习基准测试中仍保持竞争力。使用我们的最佳模型从CommonCrawl挖掘的并行数据被证明可以训练出具有竞争力的NMT模型用于en-zh和en-de。我们在https://tfhub.dev/google/LaBSE上公开了我们的最佳多语言句子嵌入模型，适用于109+种语言。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While BERT is an effective method for learning monolingual sentenceembeddings for semantic similarity and embedding based transfer learning BERTbased cross-lingual sentence embeddings have yet to be explored. Wesystematically investigate methods for learning multilingual sentenceembeddings by combining the best methods for learning monolingual andcross-lingual representations including: masked language modeling (MLM),translation language modeling (TLM), dual encoder translation ranking, andadditive margin softmax. We show that introducing a pre-trained multilinguallanguage model dramatically reduces the amount of parallel training datarequired to achieve good performance by 80%. Composing the best of thesemethods produces a model that achieves 83.7% bi-text retrieval accuracy over112 languages on Tatoeba, well above the 65.5 achieved by LASER, while stillperforming competitively on monolingual transfer learning benchmarks. Paralleldata mined from CommonCrawl using our best model is shown to train competitiveNMT models for en-zh and en-de. We publicly release our best multilingualsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.</description>
      <author>example@mail.com (Jingshu Liu, Raheel Qader, Gaëtan Caillaut, Mariam Nakhlé)</author>
      <guid isPermaLink="false">2510.17504v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting</title>
      <link>http://arxiv.org/abs/2510.17408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的方法，将可信人工智能与节能机械臂相结合，用于智能垃圾分类和分拣。该系统通过使用MobileNetV2迁移学习增强的卷积神经网络，准确地将废物分为六类：塑料、玻璃、金属、纸张、纸板和垃圾。&lt;h4&gt;背景&lt;/h4&gt;城市废物管理需要智能化的解决方案来提高分类效率和可持续性。&lt;h4&gt;目的&lt;/h4&gt;开发一个结合可信人工智能和节能机械臂的系统，用于智能垃圾分类和分拣，提高废物管理的效率和可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用MobileNetV2迁移学习增强的卷积神经网络进行废物分类；实现机械臂模拟器进行虚拟分拣；使用欧几里得距离计算每个动作的能耗；融入可信人工智能的要素：透明度、鲁棒性、公平性和安全性。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了99.8%的训练准确率和80.5%的验证准确率；系统能够准确将废物分为六类；通过能耗计算优化了机械臂的移动路径，提高了能源效率。&lt;h4&gt;结论&lt;/h4&gt;该框架是一个可靠且可扩展的解决方案，适用于城市环境中的智能废物管理系统，结合了可信人工智能和节能机械臂的优势。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新颖的方法，将可信人工智能与节能机械臂相结合，用于智能垃圾分类和分拣。通过利用通过MobileNetV2迁移学习增强的卷积神经网络，系统准确地将废物分为六类：塑料、玻璃、金属、纸张、纸板和垃圾。该模型实现了99.8%的高训练准确率和80.5%的验证准确率，展示了强大的学习和泛化能力。实现了机械臂模拟器进行虚拟分拣，使用欧几里得距离计算每个动作的能耗，确保最佳和高效的移动。该框架融入了可信人工智能的关键要素，如透明度、鲁棒性、公平性和安全性，使其成为城市环境中智能废物管理系统的可靠且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel methodology that integrates trustworthyartificial intelligence (AI) with an energy-efficient robotic arm forintelligent waste classification and sorting. By utilizing a convolutionalneural network (CNN) enhanced through transfer learning with MobileNetV2, thesystem accurately classifies waste into six categories: plastic, glass, metal,paper, cardboard, and trash. The model achieved a high training accuracy of99.8% and a validation accuracy of 80.5%, demonstrating strong learning andgeneralization. A robotic arm simulator is implemented to perform virtualsorting, calculating the energy cost for each action using Euclidean distanceto ensure optimal and efficient movement. The framework incorporates keyelements of trustworthy AI, such as transparency, robustness, fairness, andsafety, making it a reliable and scalable solution for smart waste managementsystems in urban settings.</description>
      <author>example@mail.com (Halima I. Kure, Jishna Retnakumari, Augustine O. Nwajana, Umar M. Ismail, Bilyaminu A. Romo, Ehigiator Egho-Promise)</author>
      <guid isPermaLink="false">2510.17408v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Dictionary-Based Deblurring for Unpaired Data</title>
      <link>http://arxiv.org/abs/2510.16428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于字典学习的图像去模糊方法，能够在不同数据监督条件下实现鲁棒的去模糊效果，解决了传统方法对大量配对数据的依赖问题。&lt;h4&gt;背景&lt;/h4&gt;有效的图像去模糊通常依赖于大量完全配对的模糊和清晰图像数据集，但在现实世界中获取这种准确对齐的数据存在困难，限制了现有方法的有效性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决数据稀缺依赖问题，提出一种基于字典学习的去模糊方法，用于联合估计结构化的模糊矩阵和高分辨率图像字典。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的基于字典学习的去模糊方法，能够在不同程度的监督下实现鲁棒的图像去模糊，并在三种不同实验设置下进行了评估：完全监督、部分监督和无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;在CMU-Cornell iCoseg数据集和FocusPath数据集的合成模糊子集上的实验表明，所提出的框架相比传统耦合字典学习方法具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过准确的模糊建模和自适应字典表示，能够使用更少的训练样本，为数据受限场景下的图像去模糊提供了一种高效且鲁棒的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;有效的图像去模糊通常依赖于大量完全配对的模糊和对应清晰图像数据集。然而，在现实世界中获取这种准确对齐的数据存在许多困难，限制了现有去模糊方法的有效性和泛化能力。为解决这种数据稀缺依赖问题，我们提出了一种新颖的基于字典学习的去模糊方法，用于联合估计结构化的模糊矩阵和高分辨率图像字典。该框架能够在不同程度的监督下实现鲁棒的图像去模糊。我们在三种不同的实验设置下对方法进行了全面评估：(i) 涉及具有明确对应关系的配对数据的完全监督；(ii) 使用具有隐含关系的未配对数据的部分监督；(iii) 使用不存在直接配对的非对应数据的无监督学习。在CMU-Cornell iCoseg数据集和真实世界FocusPath数据集的合成模糊子集上进行的大量实验验证一致表明，与传统的耦合字典学习方法相比，所提出的框架具有优越的性能。结果验证了我们的方法通过准确的模糊建模和自适应字典表示，能够使用显著更少的训练样本，为数据受限场景下的图像去模糊提供了一种高效且鲁棒的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective image deblurring typically relies on large and fully paireddatasets of blurred and corresponding sharp images. However, obtaining suchaccurately aligned data in the real world poses a number of difficulties,limiting the effectiveness and generalizability of existing deblurring methods.To address this scarcity of data dependency, we present a novel dictionarylearning based deblurring approach for jointly estimating a structured blurmatrix and a high resolution image dictionary. This framework enables robustimage deblurring across different degrees of data supervision. Our method isthoroughly evaluated across three distinct experimental settings: (i) fullsupervision involving paired data with explicit correspondence, (ii) partialsupervision employing unpaired data with implicit relationships, and (iii)unsupervised learning using non-correspondence data where direct pairings areabsent. Extensive experimental validation, performed on synthetically blurredsubsets of the CMU-Cornell iCoseg dataset and the real-world FocusPath dataset,consistently shows that the proposed framework has superior performancecompared to conventional coupled dictionary learning approaches. The resultsvalidate that our approach provides an efficient and robust solution for imagedeblurring in data-constrained scenarios by enabling accurate blur modeling andadaptive dictionary representation with a notably smaller number of trainingsamples.</description>
      <author>example@mail.com (Alok Panigrahi, Jayaprakash Katual, Satish Mulleti)</author>
      <guid isPermaLink="false">2510.16428v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Semiparametric Gaussian Mixture Model with Spatial Dependence and Its Application to Whole-Slide Image Clustering Analysis</title>
      <link>http://arxiv.org/abs/2510.16421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种半参数高斯混合模型(SGMM)，用于考虑空间信息进行无监督学习，该模型比传统GMM更灵活，能够实现同类实例的空间聚类。&lt;h4&gt;背景&lt;/h4&gt;无监督学习中通常需要考虑空间信息，但传统高斯混合模型在这方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够考虑空间信息的半参数高斯混合模型，提高无监督学习的聚类性能。&lt;h4&gt;方法&lt;/h4&gt;提出半参数高斯混合模型(SGMM)，为每个实例假设随机位置，并基于此假设特征向量服从标准GMM；开发新的EM算法估计SGMM并建立渐近理论；进行数值模拟验证性能；将方法应用于CAMELYON16数据集进行乳腺癌检测。&lt;h4&gt;主要发现&lt;/h4&gt;SGMM比传统GMM更灵活，能使同类实例在空间上聚集；在数值模拟和实际应用中表现出色，尤其在乳腺癌检测任务中展现了卓越的聚类性能。&lt;h4&gt;结论&lt;/h4&gt;SGMM是一种有效的无监督学习方法，通过考虑空间信息提高了聚类性能，在理论和实际应用中都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们在这里开发了一种半参数高斯混合模型(SGMM)，用于考虑有价值空间信息的无监督学习。具体来说，我们为每个实例假设一个随机位置。然后，基于这个随机位置，我们为特征向量假设一个标准高斯混合模型(GMM)。所提出的SGMM允许混合概率与空间位置非参数相关。与传统GMM相比，SGMM更加灵活，并允许同一类的实例在空间上聚集。为了估计SGMM，开发了新的EM算法并建立了严格的渐近理论。进行了大量的数值模拟来证明我们的有限样本性能。对于实际应用，我们将SGMM方法应用于CAMELYON16数据集的全幻灯片图像(WSIs)进行乳腺癌检测。SGMM方法表现出卓越的聚类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We develop here a semiparametric Gaussian mixture model (SGMM) forunsupervised learning with valuable spatial information taken intoconsideration. Specifically, we assume for each instance a random location.Then, conditional on this random location, we assume for the feature vector astandard Gaussian mixture model (GMM). The proposed SGMM allows the mixingprobability to be nonparametrically related to the spatial location. Comparedwith a classical GMM, SGMM is considerably more flexible and allows theinstances from the same class to be spatially clustered. To estimate the SGMM,novel EM algorithms are developed and rigorous asymptotic theories areestablished. Extensive numerical simulations are conducted to demonstrate ourfinite sample performance. For a real application, we apply our SGMM method tothe CAMELYON16 dataset of whole-slide images (WSIs) for breast cancerdetection. The SGMM method demonstrates outstanding clustering performance.</description>
      <author>example@mail.com (Baichen Yu, Jin Liu, Hansheng Wang)</author>
      <guid isPermaLink="false">2510.16421v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Adversarially Robust Quantum Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.16301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This Book Chapter will publish in "Quantum Robustness in Artificial  Intelligence" Book by Springer and is currently in production. More  information about the Book is at:  https://link.springer.com/book/9783032111524?srsltid=AfmBOood7vZYc5xJYtLrQWND4pjedgfWAfAFFocjvnNS1lrNpVBwvJcO#accessibility-information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种量子迁移学习（QTL）模型，结合量子计算与迁移学习技术，用于高分辨率图像分类，在多个数据集上显示出优于传统和量子模型的性能，并通过对抗训练提高了模型鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;量子机器学习（QML）作为利用量子计算原理增强经典机器学习系统性能的有前景领域，受限于当前硬件约束（如量子比特数量有限和量子噪声），其实际部署仍然有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合量子-经典架构，结合量子计算优势与迁移学习技术，解决高分辨率图像分类问题，并提高模型在实际应用中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出量子迁移学习（QTL）模型，集成经典卷积特征提取与量子变分电路，并在Ants &amp; Bees、CIFAR-10和道路标志检测等数据集上进行模拟实验，同时研究模型对抗攻击的脆弱性并加入对抗训练提高鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;QTL在多个数据集上实现了比传统模型和未使用迁移学习的量子模型更优的分类性能；对抗训练显著增强了QTL的鲁棒性，提高了其在安全敏感应用中部署的潜力。&lt;h4&gt;结论&lt;/h4&gt;量子迁移学习模型结合了量子计算与迁移学习的优势，有效解决了量子机器学习在实际部署中的局限性，为高分辨率图像分类提供了一种有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;量子机器学习（QML）已成为一个有前景的研究领域，通过利用量子计算原理来增强经典机器学习系统的性能。然而，由于当前硬件限制（如量子比特数量有限和量子噪声），QML的实际部署仍然有限。本章介绍了一种混合量子-经典架构，结合量子计算的优势和迁移学习技术来解决高分辨率图像分类问题。具体而言，我们提出了一种量子迁移学习（QTL）模型，集成了经典卷积特征提取和量子变分电路。通过在Ants &amp; Bees、CIFAR-10和道路标志检测等多样化数据集上进行广泛模拟，我们证明QTL比传统模型和未使用迁移学习的量子模型实现了更好的分类性能。此外，我们还研究了模型对抗攻击的脆弱性，并证明加入对抗训练显著提高了QTL的鲁棒性，增强了其在安全敏感应用中部署的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum machine learning (QML) has emerged as a promising area of researchfor enhancing the performance of classical machine learning systems byleveraging quantum computational principles. However, practical deployment ofQML remains limited due to current hardware constraints such as limited numberof qubits and quantum noise. This chapter introduces a hybrid quantum-classicalarchitecture that combines the advantages of quantum computing with transferlearning techniques to address high-resolution image classification.Specifically, we propose a Quantum Transfer Learning (QTL) model thatintegrates classical convolutional feature extraction with quantum variationalcircuits. Through extensive simulations on diverse datasets including Ants \&amp;Bees, CIFAR-10, and Road Sign Detection, we demonstrate that QTL achievessuperior classification performance compared to both conventional and quantummodels trained without transfer learning. Additionally, we also investigate themodel's vulnerability to adversarial attacks and demonstrate that incorporatingadversarial training significantly boosts the robustness of QTL, enhancing itspotential for deployment in security sensitive applications.</description>
      <author>example@mail.com (Amena Khatun, Muhammad Usman)</author>
      <guid isPermaLink="false">2510.16301v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Orthology Networks</title>
      <link>http://arxiv.org/abs/2510.15837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了TRON（Transfer Orthology Networks），一种用于跨物种迁移学习的新型神经网络架构，利用直系同源关系来指导知识迁移，实现了从源物种到目标物种的高效知识传递。&lt;h4&gt;背景&lt;/h4&gt;跨物种迁移学习在生物信息学领域具有重要意义，特别是如何有效利用一个物种的知识来帮助理解另一个物种。现有的方法可能缺乏生物学解释性，且难以充分利用物种间的进化关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用物种间直系同源关系进行知识迁移的神经网络架构，提高跨物种预测的准确性和可解释性，并更有效地利用现有的转录组数据。&lt;h4&gt;方法&lt;/h4&gt;设计了TRON架构，通过在预训练的前馈神经网络前添加一个学习的物种转换层来实现知识迁移。该转换层的权重被物种间二分图的二元邻接矩阵掩码，学习一个线性变换将源物种的基因表达映射到目标物种的基因空间。&lt;h4&gt;主要发现&lt;/h4&gt;转换层的学习权重可以解释功能直系同源，提供不同物种基因如何对特定表型做出贡献的见解。这种方法为跨物种迁移学习提供了生物学基础和可解释的途径。&lt;h4&gt;结论&lt;/h4&gt;TRON为跨物种迁移学习提供了一种生物学基础扎实且可解释的方法，为更有效地利用现有转录组数据铺平了道路。研究团队正在收集跨物种转录组/表型数据，以获得对TRON架构的实验验证。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了TRON（Transfer Orthology Networks），一种用于跨物种迁移学习的新型神经网络架构。TRON利用直系同源关系（以物种之间的二分图表示）来指导知识迁移。具体来说，我们在一个预训练的前馈神经网络前添加一个学习的物种转换层，该层的权重被这个二分图的二元邻接矩阵掩码，该神经网络用于从源物种的基因表达数据预测表型。这通过学习一个线性变换来实现知识向目标物种的高效迁移，该变换将源物种的基因表达映射到目标物种的基因空间。这个转换层的学习权重为解释功能直系同源提供了潜在途径，提供了关于不同物种基因如何对感兴趣的表型做出贡献的见解。TRON为跨物种迁移学习提供了一种生物学基础扎实且可解释的方法，为更有效地利用现有转录组数据铺平了道路。我们正在收集跨物种转录组/表型数据，以获得对TRON架构的实验验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Transfer Orthology Networks (TRON), a novel neural networkarchitecture designed for cross-species transfer learning. TRON leveragesorthologous relationships, represented as a bipartite graph between species, toguide knowledge transfer. Specifically, we prepend a learned species conversionlayer, whose weights are masked by the biadjacency matrix of this bipartitegraph, to a pre-trained feedforward neural network that predicts a phenotypefrom gene expression data in a source species. This allows for efficienttransfer of knowledge to a target species by learning a linear transformationthat maps gene expression from the source to the target species' gene space.The learned weights of this conversion layer offer a potential avenue forinterpreting functional orthology, providing insights into how genes acrossspecies contribute to the phenotype of interest. TRON offers a biologicallygrounded and interpretable approach to cross-species transfer learning, pavingthe way for more effective utilization of available transcriptomic data. We arein the process of collecting cross-species transcriptomic/phenotypic data togain experimental validation of the TRON architecture.</description>
      <author>example@mail.com (Vikash Singh)</author>
      <guid isPermaLink="false">2510.15837v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI</title>
      <link>http://arxiv.org/abs/2510.15684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, BraTS GoAT 2025 challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型的多模态视觉变换器自编码器(MViT-AE)，用于在磁共振成像中进行脑肿瘤分割的无监督异常检测，无需依赖手动标注数据。&lt;h4&gt;背景&lt;/h4&gt;无监督异常检测(UAD)是脑肿瘤分割的一种替代方法，特别在标注数据有限、昂贵或不一致的情况下，可解决神经影像工作流程中的可扩展性瓶颈。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖手动标注的脑肿瘤分割方法，解决传统监督学习方法在数据获取方面的限制。&lt;h4&gt;方法&lt;/h4&gt;提出MViT-AE模型，仅在健康脑部MRI图像上训练，通过基于重建的误差图检测和定位肿瘤；采用多模态早期-晚期融合策略整合多种MRI序列信息；引入后处理流程整合分割任何模型(SAM)优化肿瘤轮廓预测。&lt;h4&gt;主要发现&lt;/h4&gt;在BraTS-GoAT 2025 Lighthouse数据集(包含胶质瘤、脑膜瘤和儿童脑肿瘤等)上评估，测试集上病变级别Dice相似系数：全肿瘤0.437，肿瘤核心0.316，增强肿瘤0.350；验证集上异常检测率为89.4%。&lt;h4&gt;结论&lt;/h4&gt;基于变换器的无监督模型有潜力成为神经肿瘤成像中可扩展、标签高效的工具，尽管在检测小或非增强病变方面仍存在挑战。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常检测(UAD)为磁共振成像(MRI)中的脑肿瘤分割提供了监督学习的替代方案，特别是在标注数据有限、昂贵或不一致的情况下。本研究提出了一种新型的多模态视觉变换器自编码器(MViT-AE)，仅在健康脑部MRI上进行训练，通过基于重建的误差图检测和定位肿瘤。这种无监督范式实现了不依赖手动标签的分割，解决了神经影像工作流程中的一个关键可扩展性瓶颈。我们的方法在BraTS-GoAT 2025 Lighthouse数据集上进行了评估，该数据集包含各种类型的肿瘤，如胶质瘤、脑膜瘤和儿童脑肿瘤。为提高性能，我们引入了多模态早期-晚期融合策略，利用多种MRI序列的互补信息，以及一个整合分割任何模型(SAM)的后处理流程，以优化预测的肿瘤轮廓。尽管UAD存在已知挑战，特别是在检测小或非增强病变方面，我们的方法仍实现了具有临床意义的肿瘤定位，测试集上的病变级别Dice相似系数为0.437(全肿瘤)、0.316(肿瘤核心)和0.350(增强肿瘤)，验证集上的异常检测率为89.4%。这些发现强调了基于变换器的无监督模型作为神经肿瘤成像中可扩展、标签高效工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomaly detection (UAD) presents a complementary alternative tosupervised learning for brain tumor segmentation in magnetic resonance imaging(MRI), particularly when annotated datasets are limited, costly, orinconsistent. In this work, we propose a novel Multimodal Vision TransformerAutoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect andlocalize tumors via reconstruction-based error maps. This unsupervised paradigmenables segmentation without reliance on manual labels, addressing a keyscalability bottleneck in neuroimaging workflows. Our method is evaluated inthe BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumorssuch as gliomas, meningiomas, and pediatric brain tumors. To enhanceperformance, we introduce a multimodal early-late fusion strategy thatleverages complementary information across multiple MRI sequences, and apost-processing pipeline that integrates the Segment Anything Model (SAM) torefine predicted tumor contours. Despite the known challenges of UAD,particularly in detecting small or non-enhancing lesions, our method achievesclinically meaningful tumor localization, with lesion-wise Dice SimilarityCoefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (EnhancingTumor) on the test set, and an anomaly Detection Rate of 89.4% on thevalidation set. These findings highlight the potential of transformer-basedunsupervised models to serve as scalable, label-efficient tools forneuro-oncological imaging.</description>
      <author>example@mail.com (Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier, Noel E. O'Connor, Ferran Marques)</author>
      <guid isPermaLink="false">2510.15684v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning</title>
      <link>http://arxiv.org/abs/2510.15372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分阶段自适应微调方法，用于解决微创手术中自动化工具检测面临的标注数据有限问题。该方法通过线性探测和渐进式冻结两个阶段，有效提高了手术工具检测的性能，在胆囊切除和眼科手术数据集上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;微创手术可以从自动化手术工具检测中获益，但手术环境中标注数据的有限性对训练强健的深度学习模型构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的分阶段自适应微调方法，提高手术工具检测的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;提出两步法微调策略：1)线性探测阶段，在预训练CNN架构上添加额外分类层；2)渐进式冻结阶段，动态减少可微调层数。该方法降低了网络复杂性，提高效率，只需单次训练循环。使用在ImageNet上预训练的ResNet-50和DenseNet-121架构，在Cholec80数据集上检测胆囊切除内窥镜视频中的手术工具。&lt;h4&gt;主要发现&lt;/h4&gt;该方法比现有方法和既定微调技术提高了检测性能，平均精度均值(mAP)达到96.4%。在CATARACTS数据集(眼科手术)上验证了该方法的可推广性。&lt;h4&gt;结论&lt;/h4&gt;渐进式冻结微调是提高不同手术过程中工具存在检测的一种有前途的技术，可能在一般图像分类任务中有更广泛的应用。&lt;h4&gt;翻译&lt;/h4&gt;微创手术可以从自动化手术工具检测中显著获益，实现高级分析和辅助。然而，手术环境中标注数据的有限性对训练强健的深度学习模型构成了挑战。本文引入了一种新颖的分阶段自适应微调方法，包含两个步骤：线性探测阶段，在预训练的基于CNN的架构上添加额外的分类层；渐进式冻结阶段，动态减少可微调层数，旨在调节对手术领域的适应。这种策略降低了网络复杂性并提高了效率，只需要单个训练循环，消除了多次迭代的必要性。我们在Cholec80数据集上验证了我们的方法，使用在ImageNet上预训练的CNN架构(ResNet-50和DenseNet-121)来检测胆囊切除内窥镜视频中的手术工具。我们的结果表明，与现有方法和既定的微调技术相比，我们的方法提高了检测性能，平均精度均值(mAP)达到96.4%。为了评估其更广泛的适用性，在CATARACTS数据集(一个不同的微创眼科手术领域)上进一步确认了微调策略的可推广性。这些发现表明，渐进式冻结微调是提高不同手术过程中工具存在检测的一种有前途的技术，可能在一般的图像分类任务中有更广泛的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/ima.70218&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Minimally invasive surgery can benefit significantly from automated surgicaltool detection, enabling advanced analysis and assistance. However, the limitedavailability of annotated data in surgical settings poses a challenge fortraining robust deep learning models. This paper introduces a novel stagedadaptive fine-tuning approach consisting of two steps: a linear probing stageto condition additional classification layers on a pre-trained CNN-basedarchitecture and a gradual freezing stage to dynamically reduce thefine-tunable layers, aiming to regulate adaptation to the surgical domain. Thisstrategy reduces network complexity and improves efficiency, requiring only asingle training loop and eliminating the need for multiple iterations. Wevalidated our method on the Cholec80 dataset, employing CNN architectures(ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgicaltools in cholecystectomy endoscopic videos. Our results demonstrate that ourmethod improves detection performance compared to existing approaches andestablished fine-tuning techniques, achieving a mean average precision (mAP) of96.4%. To assess its broader applicability, the generalizability of thefine-tuning strategy was further confirmed on the CATARACTS dataset, a distinctdomain of minimally invasive ophthalmic surgery. These findings suggest thatgradual freezing fine-tuning is a promising technique for improving toolpresence detection in diverse surgical procedures and may have broaderapplications in general image classification tasks.</description>
      <author>example@mail.com (Ana Davila, Jacinto Colan, Yasuhisa Hasegawa)</author>
      <guid isPermaLink="false">2510.15372v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization</title>
      <link>http://arxiv.org/abs/2510.15165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在连续时间线性二次调节器(LQRs)中使用策略迁移方法来提高强化学习效率，提供了连续时间RL策略迁移的第一个理论证明，并提出了新型策略学习算法。&lt;h4&gt;背景&lt;/h4&gt;强化学习使智能体能够通过与环境的交互学习最优决策策略，但在复杂任务上从零开始训练效率低下。迁移学习在大语言模型中非常成功，为提高强化学习效率提供了有前景的方向。&lt;h4&gt;目的&lt;/h4&gt;研究策略迁移方法，即在带熵正则化的连续时间线性二次调节器(LQRs)背景下，使用相关源任务的策略初始化目标RL任务的学习，并提供连续时间RL策略迁移的理论证明。&lt;h4&gt;方法&lt;/h4&gt;采用策略迁移方法，使用相关源任务的策略初始化目标RL任务的学习；提出针对连续时间LQRs的新型策略学习算法；分析连续时间LQRs与基于分数的扩散模型之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;证明了一个最优于一个LQR的策略可以作为紧密相关LQRs的近似最优初始化，同时保持原始算法的收敛速率；提出的策略学习算法实现了全局线性和局部超线性收敛；通过分析推导出一类基于分数的连续时间扩散模型的稳定性。&lt;h4&gt;结论&lt;/h4&gt;展示了迁移学习在连续时间RL中的理论保证和算法优势，弥补了现有文献中的空白，将先前的工作从离散时间扩展到连续时间设置。&lt;h4&gt;翻译&lt;/h4&gt;强化学习使智能体能够通过与环境的交互学习最优决策策略，但在复杂任务上从零开始训练可能效率极低。在大语言模型中广泛成功的迁移学习为利用预训练模型提高强化学习效率提供了有前景的方向。本文研究了策略迁移，这是一种迁移学习方法，在带熵正则化的连续时间线性二次调节器(LQRs)背景下，使用相关源任务的策略初始化目标RL任务的学习。我们首次提供了连续时间RL策略迁移的理论证明，证明了一个最优于一个LQR的策略可以作为紧密相关LQRs的近似最优初始化，同时保持原始算法的收敛速率。此外，我们提出了针对连续时间LQRs的新型策略学习算法，实现了全局线性和局部超线性收敛。我们的结果展示了连续时间RL中迁移学习的理论保证和算法优势，解决了现有文献中的空白，并将先前的工作从离散时间扩展到连续时间设置。作为我们分析的副产品，我们通过LQRs与基于分数的连续时间扩散模型之间的联系，推导出一类连续时间扩散模型的稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) enables agents to learn optimal decision-makingstrategies through interaction with an environment, yet training from scratchon complex tasks can be highly inefficient. Transfer learning (TL), widelysuccessful in large language models (LLMs), offers a promising direction forenhancing RL efficiency by leveraging pre-trained models.  This paper investigates policy transfer, a TL approach that initializeslearning in a target RL task using a policy from a related source task, in thecontext of continuous-time linear quadratic regulators (LQRs) with entropyregularization. We provide the first theoretical proof of policy transfer forcontinuous-time RL, proving that a policy optimal for one LQR serves as anear-optimal initialization for closely related LQRs, while preserving theoriginal algorithm's convergence rate. Furthermore, we introduce a novel policylearning algorithm for continuous-time LQRs that achieves global linear andlocal super-linear convergence. Our results demonstrate both theoreticalguarantees and algorithmic benefits of transfer learning in continuous-time RL,addressing a gap in existing literature and extending prior work from discreteto continuous time settings.  As a byproduct of our analysis, we derive the stability of a class ofcontinuous-time score-based diffusion models via their connection with LQRs.</description>
      <author>example@mail.com (Xin Guo, Zijiu Lyu)</author>
      <guid isPermaLink="false">2510.15165v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning strategies for accelerating reinforcement-learning-based flow control</title>
      <link>http://arxiv.org/abs/2510.16016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了迁移学习策略以加速深度强化学习在多保真度混沌流体流动控制中的应用，首次将渐进神经网络应用于基于DRL的流动控制，并评估了传统微调策略的性能。&lt;h4&gt;背景&lt;/h4&gt;在深度强化学习应用于混沌流体流动控制时，如何有效利用低保真度环境训练的知识到高保真度环境是一个挑战，传统微调方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;研究迁移学习策略，特别是渐进神经网络，来加速深度强化学习在多保真度混沌流体流动控制中的应用，并评估这些策略的性能、收敛行为和保留已转移知识的能力。&lt;h4&gt;方法&lt;/h4&gt;1)首次将渐进神经网络应用于基于DRL的流动控制；2)对传统微调策略进行全面基准测试；3)使用Kuramoto-Sivashinsky系统作为基准，研究知识转移；4)进行逐层敏感性分析，研究PNNs如何重用中间表示。&lt;h4&gt;主要发现&lt;/h4&gt;1)微调虽可加速收敛但对预训练时长敏感且易发生灾难性遗忘；2)PNNs通过保留先验知识实现稳定高效迁移；3)PNNs能动态重用源策略中间表示并逐步适应新任务；4)即使环境差异大，PNNs仍有效，而微调策略往往失败。&lt;h4&gt;结论&lt;/h4&gt;新型迁移学习框架在稳健、可扩展和计算高效的流动控制方面具有潜力，可应用于更复杂的流动配置。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了迁移学习策略以加速深度强化学习在多保真度混沌流体流动控制中的应用。渐进神经网络是一种模块化架构，旨在跨任务保留和重用知识，首次被应用于基于DRL的流动控制背景下。此外，还对传统的微调策略进行了全面的基准测试，评估了它们的性能、收敛行为以及保留已转移知识的能力。Kuramoto-Sivashinsky系统被用作基准，研究如何在低保真度环境中训练的控制策略知识有效地转移到高保真度设置中。系统评估表明，虽然微调可以加速收敛，但它对预训练时长非常敏感，且容易发生灾难性遗忘。相比之下，渐进神经网络通过保留先验知识实现稳定高效的迁移，提供一致的性能提升，并且在预训练阶段对过拟合具有显著的鲁棒性。逐层敏感性分析进一步揭示了渐进神经网络如何动态重用来自源策略的中间表示，同时逐步使更深层次层适应目标任务。此外，即使在源环境和目标环境差异很大的情况下，如物理机制不匹配或控制目标不同的情况下，渐进神经网络仍然有效，而微调策略往往导致次优适应或知识转移完全失败。这些结果突显了新型迁移学习框架在稳健、可扩展和计算高效的流动控制方面的潜力，可以应用于更复杂的流动配置。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates transfer learning strategies to accelerate deepreinforcement learning (DRL) for multifidelity control of chaotic fluid flows.Progressive neural networks (PNNs), a modular architecture designed to preserveand reuse knowledge across tasks, are employed for the first time in thecontext of DRL-based flow control. In addition, a comprehensive benchmarking ofconventional fine-tuning strategies is conducted, evaluating their performance,convergence behavior, and ability to retain transferred knowledge. TheKuramoto-Sivashinsky (KS) system is employed as a benchmark to examine howknowledge encoded in control policies, trained in low-fidelity environments,can be effectively transferred to high-fidelity settings. Systematicevaluations show that while fine-tuning can accelerate convergence, it ishighly sensitive to pretraining duration and prone to catastrophic forgetting.In contrast, PNNs enable stable and efficient transfer by preserving priorknowledge and providing consistent performance gains, and are notably robust tooverfitting during the pretraining phase. Layer-wise sensitivity analysisfurther reveals how PNNs dynamically reuse intermediate representations fromthe source policy while progressively adapting deeper layers to the targettask. Moreover, PNNs remain effective even when the source and targetenvironments differ substantially, such as in cases with mismatched physicalregimes or control objectives, where fine-tuning strategies often result insuboptimal adaptation or complete failure of knowledge transfer. The resultshighlight the potential of novel transfer learning frameworks for robust,scalable, and computationally efficient flow control that can potentially beapplied to more complex flow configurations.</description>
      <author>example@mail.com (Saeed Salehi)</author>
      <guid isPermaLink="false">2510.16016v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment</title>
      <link>http://arxiv.org/abs/2510.13023v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 6 page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种端到端的机器学习工作流程，用于解决工业环境中超声波焊接检测面临的训练数据有限和环境波动导致的信号损坏问题。&lt;h4&gt;背景&lt;/h4&gt;自动化超声波焊接检测在无损评估领域仍然是一个重大挑战，主要由于训练数据有限（实验标本整理或高保真模拟的复杂性）和工业环境的环境波动性导致的实时测量数据损坏。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端的机器学习工作流程，用于真实工业环境中的声学焊接检测，解决数据整理和信号损坏的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出的工作流程包括降阶建模方案、基于扩散的分布对齐和基于U-Net的分割与反演；使用基于Lamb波理论的降阶Helmholtz模型生成数据集；通过迁移学习使用有限的全3D弹性动力学模拟完善模型；使用引导扩散处理分布外真实世界测量数据。&lt;h4&gt;主要发现&lt;/h4&gt;低阶解决方案为反演模型提供了强大的训练数据集；引导扩散能有效处理具有不同且不可预测噪声分布的真实世界测量数据。&lt;h4&gt;结论&lt;/h4&gt;这种集成框架为真实数据上的自动化焊接检测提供了端到端解决方案。&lt;h4&gt;翻译&lt;/h4&gt;自动化超声波焊接检测由于训练数据有限（由于实验标本整理或高保真模拟的复杂性）和许多工业环境的环境波动性（导致实时测量数据损坏）等因素，在无损评估社区仍然是一个重大挑战。因此，用于真实（即工业）环境中声学焊接检测的端到端机器学习工作流程一直是一个难以实现的目标。这项工作通过提出包括降阶建模方案、基于扩散的分布对齐以及基于U-Net的分割和反演的工作流程，解决了数据整理和信号损坏的挑战。使用基于Lamb波理论的降阶Helmholtz模型生成涵盖不同焊接异质性和裂纹缺陷的综合数据集。相对廉价低阶解决方案为反演模型提供了强大的训练数据集，并通过使用有限的全3D弹性动力学模拟的迁移学习阶段进行完善。为了处理具有不同且不可预测噪声分布的分布外真实世界测量，即激光多普勒测振扫描，引导扩散生成OOD实验LDV扫描的分布内表示，随后由反演模型处理。这种集成框架为真实数据上的自动化焊接检测提供了端到端解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated ultrasonic weld inspection remains a significant challenge in thenondestructive evaluation (NDE) community to factors such as limited trainingdata (due to the complexity of curating experimental specimens or high-fidelitysimulations) and environmental volatility of many industrial settings(resulting in the corruption of on-the-fly measurements). Thus, an end-to-endmachine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,industrial) settings has remained an elusive goal. This work addresses thechallenges of data curation and signal corruption by proposing workflowconsisting of a reduced-order modeling scheme, diffusion based distributionalignment, and U-Net-based segmentation and inversion. A reduced-orderHelmholtz model based on Lamb wave theory is used to generate a comprehensivedataset over varying weld heterogeneity and crack defects. The relativelyinexpensive low-order solutions provide a robust training dateset for inversionmodels which are refined through a transfer learning stage using a limited setof full 3D elastodynamic simulations. To handle out-of-distribution (OOD)real-world measurements with varying and unpredictable noise distributions,i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distributionrepresentations of OOD experimental LDV scans which are subsequently processedby the inversion models. This integrated framework provides an end-to-endsolution for automated weld inspection on real data.</description>
      <author>example@mail.com (Joshua R. Tempelman, Adam J. Wachtor, Eric B. Flynn)</author>
      <guid isPermaLink="false">2510.13023v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues</title>
      <link>http://arxiv.org/abs/2510.17722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Website: https://github.com/NJU-LINK/MT-Video-Bench&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MT-Video-Bench，一个专门用于评估多模态大语言模型在多轮对话中视频理解能力的基准测试。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)的近期发展显著提升了AI理解视觉模态的能力，但现有评估基准仅限于单轮问答，忽视了现实场景中多轮对话的复杂性。&lt;h4&gt;目的&lt;/h4&gt;弥补现有评估基准的不足，引入一个专门用于评估MLLMs在多轮对话中表现的视频理解基准。&lt;h4&gt;方法&lt;/h4&gt;MT-Video-Bench主要评估六种关注感知性和互动性的核心能力，包含来自不同领域的987个精心策划的多轮对话，这些能力与现实应用如交互式体育分析和多轮视频智能辅导紧密对齐。&lt;h4&gt;主要发现&lt;/h4&gt;通过评估各种最先进的开源和闭源MLLMs，揭示了它们在处理多轮视频对话时的显著性能差异和局限性。&lt;h4&gt;结论&lt;/h4&gt;该基准将公开可用，以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的近期发展显著提升了AI理解视觉模态的能力。然而，现有评估基准仍仅限于单轮问答，忽视了现实场景中多轮对话的复杂性。为弥补这一差距，我们引入了MT-Video-Bench，一个用于评估MLLMs在多轮对话中表现的整体视频理解基准。具体而言，我们的MT-Video-Bench主要评估六种关注感知性和互动性的核心能力，包含来自不同领域的987个精心策划的多轮对话。这些能力与现实应用（如交互式体育分析和多轮视频智能辅导）紧密对齐。通过MT-Video-Bench，我们广泛评估了各种最先进的开源和闭源MLLMs，揭示了它们在处理多轮视频对话时的显著性能差异和局限性。该基准将公开可用以促进未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent development of Multimodal Large Language Models (MLLMs) hassignificantly advanced AI's ability to understand visual modalities. However,existing evaluation benchmarks remain limited to single-turn questionanswering, overlooking the complexity of multi-turn dialogues in real-worldscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic videounderstanding benchmark for evaluating MLLMs in multi-turn dialogues.Specifically, our MT-Video-Bench mainly assesses six core competencies thatfocus on perceptivity and interactivity, encompassing 987 meticulously curatedmulti-turn dialogues from diverse domains. These capabilities are rigorouslyaligned with real-world applications, such as interactive sports analysis andmulti-turn video-based intelligent tutoring. With MT-Video-Bench, weextensively evaluate various state-of-the-art open-source and closed-sourceMLLMs, revealing their significant performance discrepancies and limitations inhandling multi-turn video dialogues. The benchmark will be publicly availableto foster future research.</description>
      <author>example@mail.com (Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, Jiaheng Liu)</author>
      <guid isPermaLink="false">2510.17722v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Mimamsa Inspired Framework For Instruction Sequencing In AI Agents</title>
      <link>http://arxiv.org/abs/2510.17691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于印度Mimamsa哲学体系的AI智能体指令排序正式框架&lt;h4&gt;背景&lt;/h4&gt;灵感来源于印度哲学体系Mimamsa&lt;h4&gt;目的&lt;/h4&gt;建立可靠的指令排序机制，影响AI应用如任务规划和机器人技术&lt;h4&gt;方法&lt;/h4&gt;通过动作-对象对以三种方式形式化排序机制：直接断言(Srutikrama)用于时间先后顺序，目的驱动排序(Arthakrama)用于功能依赖关系，迭代过程(Pravrittikrama)用于区分重复任务中的并行和顺序执行。引入动作对象命令式逻辑的语法和语义，扩展MIRA形式化体系&lt;h4&gt;主要发现&lt;/h4&gt;建立了指令排序的正确性定理，基于连续指令间对象依赖关系，并证明了可靠性和完备性&lt;h4&gt;结论&lt;/h4&gt;形式化验证实现可靠的指令排序，解决时间推理和依赖建模问题&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了一个用于AI智能体指令排序的正式框架，灵感来源于印度哲学体系Mimamsa。该框架通过动作-对象对以三种不同方式形式化排序机制：直接断言(Srutikrama)用于时间先后顺序，目的驱动排序(Arthakrama)用于功能依赖关系，以及迭代过程(Pravrittikrama)用于区分重复任务中的并行和顺序执行。它引入了动作对象命令式逻辑的语法和语义，扩展了MIRA形式化体系，并添加了明确的排序演绎规则。指令排序的正确性通过一个验证定理建立，该定理基于连续指令间的对象依赖关系。这得到了可靠性和完备性的证明支持。这种形式化验证实现了可靠的指令排序，通过解决时间推理和依赖建模问题，影响了任务规划和机器人等AI应用领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a formal framework for sequencing instructions in AIagents, inspired by the Indian philosophical system of Mimamsa. The frameworkformalizes sequencing mechanisms through action object pairs in three distinctways: direct assertion (Srutikrama) for temporal precedence, purpose drivensequencing (Arthakrama) for functional dependencies, and iterative procedures(Pravrittikrama) for distinguishing between parallel and sequential executionin repetitive tasks. It introduces the syntax and semantics of an action objectimperative logic, extending the MIRA formalism (Srinivasan and Parthasarathi,2021) with explicit deduction rules for sequencing. The correctness ofinstruction sequencing is established through a validated theorem, which isbased on object dependencies across successive instructions. This is furthersupported by proofs of soundness and completeness. This formal verificationenables reliable instruction sequencing, impacting AI applications across areaslike task planning and robotics by addressing temporal reasoning and dependencymodeling.</description>
      <author>example@mail.com (Bama Srinivasan)</author>
      <guid isPermaLink="false">2510.17691v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding</title>
      <link>http://arxiv.org/abs/2510.17305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ARR Rolling Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongInsightBench是首个专门评估模型理解长视频能力的基准测试，整合视觉、音频和文本模态，包含长信息密集视频、多样化任务场景和严格质量保证流程。&lt;h4&gt;背景&lt;/h4&gt;目前缺乏专门评估模型理解长视频能力的基准测试，尤其关注人类语言、视角、动作等上下文元素。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试来评估模型对长视频的理解能力，特别关注多模态整合和复杂推理任务。&lt;h4&gt;方法&lt;/h4&gt;构建包含约1000个长信息密集视频的基准测试，设计六种挑战性任务场景，开发三步半自动数据质量保证流程，并进行一系列实验评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;全模态模型在需要精确时间定位和长距离因果推理的任务中面临挑战，多模态融合中存在信息损失和处理偏差。&lt;h4&gt;结论&lt;/h4&gt;LongInsightBench为评估长视频理解能力提供了有效工具，揭示了当前模型在特定任务上的局限性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了LongInsightBench，这是第一个专门评估模型理解长视频能力的基准测试，重点关注人类语言、视角、动作和其他上下文元素，同时整合视觉、音频和文本模态。我们的基准测试在三个关键方面表现出色：a) 长时长、信息密集的视频：我们从开源数据集FineVideo中根据时长限制和视觉及音频模态的信息密度精心选择了约1000个视频，重点关注包含丰富语言元素的内容，如讲座、访谈和vlog。b) 多样且具有挑战性的任务场景：我们设计了六种具有挑战性的任务场景，包括事件内部和事件之间的任务。c) 严格且全面的质量保证流程：我们开发了一个三步半自动数据质量保证流程，以确保合成问题和答案选项的难度和有效性。基于LongInsightBench，我们设计了一系列实验。实验结果表明，全模态模型在需要精确时间定位和长距离因果推理的任务中仍然面临挑战。扩展实验揭示了全模态模型多模态融合中的信息损失和处理偏差。我们的数据集和代码可在提供的链接获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce \textbf{LongInsightBench}, the first benchmark designed toassess models' ability to understand long videos, with a focus on humanlanguage, viewpoints, actions, and other contextual elements, while integrating\textbf{visual, audio, and text} modalities. Our benchmark excels in three keyareas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully selectapproximately 1,000 videos from open-source datasets FineVideo based onduration limit and the information density of both visual and audio modalities,focusing on content like lectures, interviews, and vlogs, which contain richlanguage elements. \textbf{b) Diverse and Challenging Task Scenarios:} We havedesigned six challenging task scenarios, including both Intra-Event andInter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality AssurancePipelines:} We have developed a three-step, semi-automated data qualityassurance pipeline to ensure the difficulty and validity of the synthesizedquestions and answer options. Based on LongInsightBench, we designed a seriesof experiments. Experimental results shows that Omni-modal models(OLMs) stillface challenge in tasks requiring precise temporal localization (T-Loc) andlong-range causal inference (CE-Caus). Extended experiments reveal theinformation loss and processing bias in multi-modal fusion of OLMs. Our datasetand code is available athttps://anonymous.4open.science/r/LongInsightBench-910F/.</description>
      <author>example@mail.com (ZhaoYang Han, Qihan Lin, Hao Liang, Bowen Chen, Zhou Liu, Wentao Zhang)</author>
      <guid isPermaLink="false">2510.17305v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Fair and Interpretable Deepfake Detection in Videos</title>
      <link>http://arxiv.org/abs/2510.17264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages (including References)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种公平感知的深度伪造检测框架，整合时间特征学习和人口感知数据增强，提高检测的公平性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;现有深度伪造检测方法存在偏见、缺乏透明度，无法捕捉时间信息，导致对不同人口统计群体做出有偏见的决策和不可靠结果。&lt;h4&gt;目的&lt;/h4&gt;开发一个公平感知的深度伪造检测框架，整合时间特征学习和人口感知数据增强，以提高公平性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;使用基于序列的聚类进行深度伪造视频的时间建模，利用概念提取提高检测可靠性并为非专业用户提供可解释决策；引入人口感知的数据增强方法，平衡代表性不足的群体，应用频域变换保留深度伪造伪影，减轻偏见并提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在FaceForensics++、DFD、Celeb-DF和DFDC数据集上的实验表明，所提出的方法在公平性和准确性之间取得了最佳平衡，优于现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的公平感知深度伪造检测框架通过整合时间特征学习和人口感知数据增强，有效提高了检测的公平性和可解释性，同时保持了高准确性。&lt;h4&gt;翻译&lt;/h4&gt;现有的深度伪造检测方法往往存在偏见、缺乏透明度，并且无法捕捉时间信息，导致对不同人口群体做出有偏见的决策和不可靠的结果。在本文中，我们提出了一个公平感知的深度伪造检测框架，整合时间特征学习和人口感知的数据增强，以提高公平性和可解释性。我们的方法利用基于序列的聚类对深度伪造视频进行时间建模，并通过概念提取提高检测可靠性，同时也为非专业用户提供可解释的决策。此外，我们引入了一种人口感知的数据增强方法，平衡代表性不足的群体，并应用频域变换来保留深度伪造伪影，从而减轻偏见并提高泛化能力。在FaceForensics++、DFD、Celeb-DF和DFDC数据集上使用最先进的架构（Xception、ResNet）进行的广泛实验证明了所提出方法在获得公平性和准确性之间最佳平衡方面的有效性，优于现有最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing deepfake detection methods often exhibit bias, lack transparency,and fail to capture temporal information, leading to biased decisions andunreliable results across different demographic groups. In this paper, wepropose a fairness-aware deepfake detection framework that integrates temporalfeature learning and demographic-aware data augmentation to enhance fairnessand interpretability. Our method leverages sequence-based clustering fortemporal modeling of deepfake videos and concept extraction to improvedetection reliability while also facilitating interpretable decisions fornon-expert users. Additionally, we introduce a demography-aware dataaugmentation method that balances underrepresented groups and appliesfrequency-domain transformations to preserve deepfake artifacts, therebymitigating bias and improving generalization. Extensive experiments onFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)architectures (Xception, ResNet) demonstrate the efficacy of the proposedmethod in obtaining the best tradeoff between fairness and accuracy whencompared to SoTA.</description>
      <author>example@mail.com (Akihito Yoshii, Ryosuke Sonoda, Ramya Srinivasan)</author>
      <guid isPermaLink="false">2510.17264v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>An empirical study of the effect of video encoders on Temporal Video Grounding</title>
      <link>http://arxiv.org/abs/2510.17007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究调查了不同视频特征对时序视频定位任务中经典架构性能的影响，发现仅通过更改视频编码器就能显著改变模型性能，并揭示了特征间可能存在的互补性。&lt;h4&gt;背景&lt;/h4&gt;时序视频定位是计算机视觉的基础任务，旨在长视频中定位自然语言查询。由于每天产生大量视频，该任务在科学界具有重要地位。&lt;h4&gt;目的&lt;/h4&gt;解决当前研究仅集中在少数视频表示上可能导致架构过拟合的问题，通过实证研究调查不同视频特征对经典架构的影响。&lt;h4&gt;方法&lt;/h4&gt;在三个基准测试（Charades-STA、ActivityNet-Captions和YouCookII）上提取特征，使用基于CNN、时序推理和transformers的视频编码器进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;仅通过更改视频编码器，模型的性能就显示出显著差异，同时揭示了使用某些特征产生的明显模式和错误。&lt;h4&gt;结论&lt;/h4&gt;不同视频特征对模型性能有显著影响，特征之间可能存在互补性，这为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;时序视频定位是计算机视觉中的一个基础任务，旨在定位长视频中未经修剪的自然语言查询。由于每天产生大量视频，它在科学界起着关键作用。尽管我们发现该领域有大量工作，但我们注意到研究仍然集中在少数几种视频表示上，长期来看可能导致架构过拟合。为了解决这个问题，我们提出了一项实证研究来调查不同视频特征对经典架构的影响。我们使用基于CNN、时序推理和transformers的视频编码器，为三个知名基准测试Charades-STA、ActivityNet-Captions和YouCookII提取特征。我们的结果表明，仅通过更改视频编码器，我们模型的性能就显示出显著差异，同时揭示了使用某些特征产生的明显模式和错误，最终表明特征间可能存在互补性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal video grounding is a fundamental task in computer vision, aiming tolocalize a natural language query in a long, untrimmed video. It has a key rolein the scientific community, in part due to the large amount of video generatedevery day. Although we find extensive work in this task, we note that researchremains focused on a small selection of video representations, which may leadto architectural overfitting in the long run. To address this issue, we proposean empirical study to investigate the impact of different video features on aclassical architecture. We extract features for three well-known benchmarks,Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based onCNNs, temporal reasoning and transformers. Our results show significantdifferences in the performance of our model by simply changing the videoencoder, while also revealing clear patterns and errors derived from the use ofcertain features, ultimately indicating potential feature complementarity.</description>
      <author>example@mail.com (Ignacio M. De la Jara, Cristian Rodriguez-Opazo, Edison Marrese-Taylor, Felipe Bravo-Marquez)</author>
      <guid isPermaLink="false">2510.17007v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display</title>
      <link>http://arxiv.org/abs/2510.16833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M2HVideo的视频生成框架，能够从人体模特影像生成身份可控、照片级真实的人体视频，解决了头部与身体运动不匹配以及时间建模导致的身份漂移问题。&lt;h4&gt;背景&lt;/h4&gt;人体模特展示是线上服装展示的经济有效替代方案，但缺乏真实感和表现细节，限制了其在时尚展示中的应用效果。&lt;h4&gt;目的&lt;/h4&gt;引入人体模特到人体(M2H)视频生成任务，旨在从人体模特影像中合成身份可控、照片级真实的人体视频，提高线上服装展示的真实感和表现力。&lt;h4&gt;方法&lt;/h4&gt;提出M2HVideo框架，包含动态姿态感知头部编码器融合面部语义和身体姿态，通过基于DDIM的一步去噪在像素空间应用镜像损失解决面部细节丢失问题，以及设计分布感知适配器对齐身份和服装特征统计分布增强时间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在UBC时尚数据集、自建的ASOS数据集和新收集的MannequinVideos数据集上的实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;M2HVideo框架有效解决了人体模特到人体视频生成的关键挑战，能够生成高质量、高保真度的服装展示视频，为线上时尚展示提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;人体模特展示为线上服装展示提供了比真人展示更具成本效益的替代方案，但缺乏真实感和表现细节。为克服这一局限，我们引入了一种称为人体模特到人体(M2H)视频生成的新任务，旨在从人体模特影像中合成身份可控、照片级真实的人体视频。我们提出M2HVideo，一个姿态感知和身份保持的视频生成框架，解决了两个关键挑战：头部和身体运动不匹配，以及时间建模导致的身份漂移。特别是，M2HVideo集成了动态姿态感知头部编码器，融合面部语义和身体姿态，产生跨帧一致的身份嵌入。为解决潜在空间压缩导致的面部细节丢失问题，我们引入了通过基于DDIM的一步去噪在像素空间应用的镜像损失。此外，我们设计了一个分布感知适配器，对齐身份和服装特征的统计分布，以增强时间一致性。在UBC时尚数据集、我们自建的ASOS数据集以及在现场收集的新MannequinVideos数据集上的大量实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面优于最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mannequin-based clothing displays offer a cost-effective alternative toreal-model showcases for online fashion presentation, but lack realism andexpressive detail. To overcome this limitation, we introduce a new task calledmannequin-to-human (M2H) video generation, which aims to synthesizeidentity-controllable, photorealistic human videos from footage of mannequins.We propose M2HVideo, a pose-aware and identity-preserving video generationframework that addresses two key challenges: the misalignment between head andbody motion, and identity drift caused by temporal modeling. In particular,M2HVideo incorporates a dynamic pose-aware head encoder that fuses facialsemantics with body pose to produce consistent identity embeddings acrossframes. To address the loss of fine facial details due to latent spacecompression, we introduce a mirror loss applied in pixel space through adenoising diffusion implicit model (DDIM)-based one-step denoising.Additionally, we design a distribution-aware adapter that aligns statisticaldistributions of identity and clothing features to enhance temporal coherence.Extensive experiments on the UBC fashion dataset, our self-constructed ASOSdataset, and the newly collected MannequinVideos dataset captured on-sitedemonstrate that M2HVideo achieves superior performance in terms of clothingconsistency, identity preservation, and video fidelity in comparison tostate-of-the-art methods.</description>
      <author>example@mail.com (Xiangyu Mu, Dongliang Zhou, Jie Hou, Haijun Zhang, Weili Guan)</author>
      <guid isPermaLink="false">2510.16833v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features</title>
      <link>http://arxiv.org/abs/2510.16781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需训练的视频理解框架，通过结合预训练视觉语言模型(VLMs)和经典机器学习算法，实现了视频内容的零样本、自动化结构分析。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉语言模型在静态图像上表现出显著的零样本推理能力，但这种能力尚未完全转移到视频领域。传统视频理解模型依赖于大量标注数据集的特定任务训练，过程昂贵且难以扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖的、无需训练的视频理解框架，避免端到端训练，协同结合预训练VLM的语义先验与经典机器学习算法的模式发现能力。&lt;h4&gt;方法&lt;/h4&gt;将视频理解重构为高维语义特征空间中的自监督时空聚类问题：使用预训练VLM的冻结视觉编码器将视频转换为语义特征轨迹；应用核时间分割(KTS)技术将特征流分割为语义连贯事件段；通过无监督密度聚类识别重复出现的宏观场景和主题；从每个聚类中选择代表性关键帧并利用VLM生成文本描述，最终形成结构化多模态摘要。&lt;h4&gt;主要发现&lt;/h4&gt;该方法为视频内容的零样本、自动化结构分析提供了有效、可解释且与模型无关的途径。&lt;h4&gt;结论&lt;/h4&gt;该框架无需训练即可实现视频理解，结合了预训练VLM和经典机器学习算法的优势，能够生成结构化的视频摘要。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉语言模型(VLMs)在静态图像上显著的零样本推理能力尚未完全转移到视频领域。传统视频理解模型通常依赖于在标注数据集上进行大量特定任务的训练，这一过程既昂贵又难以扩展。本文提出了一种新颖的、无需训练的视频理解框架，通过协同结合预训练VLM的丰富语义先验与经典机器学习算法的模式发现能力，避免了端到端训练。我们的核心思想是将视频理解重新构建为高维语义特征空间中的自监督时空聚类问题。所提出的管道首先使用预训练VLM的冻结视觉编码器将视频流转换为语义特征轨迹。随后，我们采用核时间分割(KTS)这一稳健的机器学习技术，将连续特征流分割为离散的语义连贯事件段。这些段随后接受无监督密度聚类，以识别视频中重复出现的宏观场景和主题。通过从每个发现的聚类中选择代表性关键帧，并利用VLM的生成能力进行文本描述，我们的框架自动生成视频内容的结构化、多模态摘要。这种方法为视频内容的零样本、自动化结构分析提供了有效、可解释且与模型无关的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The remarkable zero-shot reasoning capabilities of large-scale VisualLanguage Models (VLMs) on static images have yet to be fully translated to thevideo domain. Conventional video understanding models often rely on extensive,task-specific training on annotated datasets, a process that is both costly andlimited in scalability. This paper introduces a novel, training-free frameworkfor video understanding that circumvents end-to-end training by synergisticallycombining the rich semantic priors of pre-trained VLMs with classic machinelearning algorithms for pattern discovery. Our core idea is to reframe videounderstanding as a self-supervised spatio-temporal clustering problem within ahigh-dimensional semantic feature space. The proposed pipeline first transformsa video stream into a semantic feature trajectory using the frozen visualencoder of a pre-trained VLM. Subsequently, we employ Kernel TemporalSegmentation (KTS), a robust machine learning technique, to partition thecontinuous feature stream into discrete, semantically coherent event segments.These segments are then subjected to unsupervised density-based clustering toidentify recurring macroscopic scenes and themes throughout the video. Byselecting representative keyframes from each discovered cluster and leveragingthe VLM's generative capabilities for textual description, our frameworkautomatically produces a structured, multi-modal summary of the video content.This approach provides an effective, interpretable, and model-agnostic pathwayfor zero-shot, automated structural analysis of video content.</description>
      <author>example@mail.com (Shihao Ji, Zihui Song)</author>
      <guid isPermaLink="false">2510.16781v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey on World Models for Embodied AI</title>
      <link>http://arxiv.org/abs/2510.16732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Li-Zn-H/AwesomeWorldModels&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了具身AI中的世界模型，提出了一个统一的框架，包括三轴分类法、系统化的数据资源和评估指标，并对最先进模型进行了定量比较。&lt;h4&gt;背景&lt;/h4&gt;具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器捕捉环境动态，支持感知、预测和决策。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的具身AI中世界模型的框架，正式化问题设定和学习目标，并系统化相关资源和评估方法。&lt;h4&gt;方法&lt;/h4&gt;提出一个三轴分类法：(1)功能性：决策耦合型vs通用型；(2)时间建模：顺序模拟与推理vs全局差异预测；(3)空间表示：全局潜在向量、标记特征序列、空间潜在网格和分解渲染表示。系统化机器人、自动驾驶和一般视频环境的数据资源和评估指标，对最先进模型进行定量比较，并总结关键挑战。&lt;h4&gt;主要发现&lt;/h4&gt;世界模型在具身AI中具有统一框架，可通过三轴分类法进行系统化分类；当前研究面临统一数据集稀缺、评估指标需要更多关注物理一致性而非像素保真度、模型性能与计算效率之间的权衡，以及实现长时间一致性同时减轻错误累积等挑战。&lt;h4&gt;结论&lt;/h4&gt;世界模型研究仍面临多个开放挑战，包括需要统一的评估指标、平衡性能与计算效率、实现长时间一致性等。论文提供了一个精选的参考文献库供进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器捕捉环境动态，支持前向和反事实展开以支持感知、预测和决策。本综述提出了具身AI中世界模型的统一框架。具体而言，我们正式化了问题设定和学习目标，并提出了一个三轴分类法，包括：(1)功能性：决策耦合型vs通用型；(2)时间建模：顺序模拟与推理vs全局差异预测；(3)空间表示：全局潜在向量、标记特征序列、空间潜在网格和分解渲染表示。我们系统化了机器人、自动驾驶和一般视频环境的数据资源和评估指标，涵盖了像素预测质量、状态级理解和任务性能。此外，我们对最先进模型进行了定量比较，并总结了关键开放挑战，包括统一数据集的稀缺性、需要评估物理一致性而非像素保真度的评估指标、模型性能与实时控制所需计算效率之间的权衡，以及实现长时间一致性同时减轻错误累积的核心建模难度。最后，我们在https://github.com/Li-Zn-H/AwesomeWorldModels上维护了一个精选的参考文献库。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身AI领域中世界模型缺乏统一分类框架的问题。这个问题很重要，因为具身AI需要智能体能够感知环境、行动并预测行动如何改变未来状态，而世界模型作为内部模拟器是支持这些能力的关键组件。缺乏统一分类导致研究分散、术语不一致，难以进行有效比较和知识整合，阻碍了领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到具身AI中世界模型的重要性及当前研究的分类混乱问题。他们从认知科学中人类构建内部世界模型的方式获得启发，分析了世界模型的核心概念（模拟与规划、时间演化、空间表示）。作者借鉴了早期基于模型强化学习的研究、Ha和Schmidhuber的开创性工作、Dreamer系列模型以及Sora和V-JEPA 2等大型生成模型的研究成果，但发现现有综述采用功能导向或应用驱动的方法缺乏统一框架，因此提出了自己的三轴分类系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个统一的三轴分类框架来系统化组织具身AI中的世界模型研究。这三个轴分别是：功能（决策耦合vs通用目的）、时间建模（顺序模拟与推理vs全局差异预测）和空间表示（全局潜在向量、标记特征序列、空间潜在网格和分解渲染表示）。整体流程包括：介绍核心概念和理论基础、提出分类框架并映射方法、调查数据资源和评估指标、提供模型定量比较、讨论挑战和趋势、总结综述。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出统一的三轴分类框架；区分决策耦合和通用目的两种功能类型；区分顺序模拟与推理和全局差异预测两种时间建模策略；涵盖多种空间表示方法；系统整理跨领域数据资源和评估指标；提供最先进模型的定量比较；确定关键开放挑战。相比之前工作，本文提供了一个更全面、系统的分类框架，超越了之前功能导向或应用驱动的局限，覆盖更广泛的应用场景，并提供更全面的数据资源、评估指标和未来研究方向。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出一个统一的三轴分类框架，系统化地组织和分析了具身AI中的世界模型研究，为该领域提供了全面的知识图谱和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied AI requires agents that perceive, act, and anticipate how actionsreshape future world states. World models serve as internal simulators thatcapture environment dynamics, enabling forward and counterfactual rollouts tosupport perception, prediction, and decision making. This survey presents aunified framework for world models in embodied AI. Specifically, we formalizethe problem setting and learning objectives, and propose a three-axis taxonomyencompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)Temporal Modeling, Sequential Simulation and Inference vs. Global DifferencePrediction; (3) Spatial Representation, Global Latent Vector, Token FeatureSequence, Spatial Latent Grid, and Decomposed Rendering Representation. Wesystematize data resources and metrics across robotics, autonomous driving, andgeneral video settings, covering pixel prediction quality, state-levelunderstanding, and task performance. Furthermore, we offer a quantitativecomparison of state-of-the-art models and distill key open challenges,including the scarcity of unified datasets and the need for evaluation metricsthat assess physical consistency over pixel fidelity, the trade-off betweenmodel performance and the computational efficiency required for real-timecontrol, and the core modeling difficulty of achieving long-horizon temporalconsistency while mitigating error accumulation. Finally, we maintain a curatedbibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.</description>
      <author>example@mail.com (Xinqing Li, Xin He, Le Zhang, Yun Liu)</author>
      <guid isPermaLink="false">2510.16732v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Understanding under Deictic Frame of Reference</title>
      <link>http://arxiv.org/abs/2510.16685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了TUuD框架，评估大型语言模型(LLMs)在时间参考框架(t-FoR)下如何解释时间关系，发现LLMs表现出部分类人时间认知，但推理能力仍受参考框架转换和时间距离影响。&lt;h4&gt;背景&lt;/h4&gt;理解时间是人类认知的基础，时间经验常通过空间隐喻概念化。人类依赖参考框架(FoR)解释意义，而时间参考框架(t-FoR)定义了时间关系如何相对于'现在'被感知。尽管LLMs在自然语言理解上进展显著，但其时间解释和推理能力有限。&lt;h4&gt;目的&lt;/h4&gt;引入TUuD(Deictic t-FoR下的时间理解)框架，评估当'现在'的参考点沿时间线动态移动时，LLMs如何解释时间-事件和事件-事件关系。&lt;h4&gt;方法&lt;/h4&gt;提示LLMs对当前时刻和目标事件之间的相似性进行评分(0.00-1.00)，其中相似性量化了两个点之间的感知时间对齐。&lt;h4&gt;主要发现&lt;/h4&gt;四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似性评分在当前时刻达到峰值并向过去和未来事件递减。然而，这种适应在近期语境之外会减弱。&lt;h4&gt;结论&lt;/h4&gt;虽然LLMs显示出部分类人时间认知，但它们的时间推理仍然对参考框架的转换和时间距离敏感。&lt;h4&gt;翻译&lt;/h4&gt;理解时间是人类认知的基础，其中时间经验通常通过基于感官-运动经验的空间隐喻来概念化。例如，'夏天即将来临'与'我们正在接近夏天'是平行的表达。在这些表达中，人类依赖参考框架(FoR)来解释相对于特定视点的意义。将这一概念扩展到时间，时间参考框架(t-FoR)定义了时间关系如何相对于体验者的'现在'时刻被感知。虽然大型语言模型(LLMs)在自然语言理解方面显示出显著进展，但它们解释和推理时间的能力仍然有限。在这项工作中，我们引入了TUuD(Deictic t-FoR下的时间理解)框架，评估当'现在'的参考点沿时间线动态移动时，LLMs如何解释时间-事件和事件-事件关系。遵循最近关于时间认知的工作，提示LLMs对当前时刻和目标事件之间的相似性进行评分，从0.00(完全不同)到1.00(高度相似)，其中相似性量化了两个点之间的感知时间对齐。我们的结果表明，四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似性评分在当前时刻达到峰值并向过去和未来事件递减。然而，这种适应在近期语境之外会减弱，这表明虽然LLMs显示出部分类人时间认知，但它们的时间推理仍然对参考框架的转换和时间距离敏感。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding time is fundamental to human cognition, where temporalexperience is often conceptualized through spatial metaphors grounded insensory-motor experience. For example, "summer is approaching" parallels "Weare approaching the summer". In such expressions, humans rely on a frame ofreference (FoR) to interpret meaning relative to a particular viewpoint.Extending this concept to time, a temporal frame of reference (t-FoR) defineshow temporal relations are perceived relative to an experiencer's moment of"now". While Large Language Models (LLMs) have shown remarkable advances innatural language understanding, their ability to interpret and reason abouttime remains limited. In this work, we introduce TUuD (Temporal Understandingunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-eventand event-event relations when the reference point of "now" dynamically shiftsalong a timeline. Following recent work on temporal cognition\cite{li2025other}, LLMs are prompted to rate the similarity between thecurrent moment and a target event from 0.00 (completely dissimilar) to 1.00(highly similar), where similarity quantifies perceived temporal alignmentbetween the two points. Our results show that four evaluated LLMs exhibitmeasurable adaptation to a deictic t-FoR, with similarity ratings peakingaround the present and decreasing toward past and future events. Theadaptation, however, weakens beyond near-term contexts, suggesting that whileLLMs display partial human-like temporal cognition, their temporal reasoningremains sensitive to reference-frame shifts and temporal distance.</description>
      <author>example@mail.com (Damin Zhang, Julia Rayz)</author>
      <guid isPermaLink="false">2510.16685v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition</title>
      <link>http://arxiv.org/abs/2510.16541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GaitRDAE的区域感知动态聚合和激励框架，用于解决步态识别中动态建模运动区域的问题&lt;h4&gt;背景&lt;/h4&gt;深度学习步态识别在各种应用中取得了巨大成功，准确步态识别的关键在于考虑不同运动区域的独特和多样化的行为模式，特别是当协变量影响视觉外观时&lt;h4&gt;目的&lt;/h4&gt;解决现有方法使用预定义区域进行时间建模，为不同类型区域分配固定或等效时间尺度，难以处理随时间动态变化的运动区域的问题&lt;h4&gt;方法&lt;/h4&gt;提出GaitRDAE框架，包括两个核心模块：区域感知动态聚合（RDA）模块，为每个区域动态搜索最佳时间感受野；区域感知动态激励（RDE）模块，强调学习稳定行为模式的运动区域，抑制对易受协变量影响的静态区域的注意力&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GaitRDAE在几个基准数据集上取得了最先进的性能&lt;h4&gt;结论&lt;/h4&gt;GaitRDAE框架能够有效处理步态识别中动态变化的运动区域，提高了识别准确率&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的步态识别在各种应用中取得了巨大成功。准确步态识别的关键在于考虑不同运动区域中独特且多样化的行为模式，特别是当协变量影响视觉外观时。然而，现有方法通常使用预定义区域进行时间建模，为不同类型的区域分配固定或等效的时间尺度，这使得难以建模随时间动态变化的运动区域并适应其特定模式。为解决这个问题，我们引入了一个区域感知动态聚合和激励框架（GaitRDAE），该框架自动搜索运动区域，分配自适应时间尺度并应用相应的注意力机制。具体而言，该框架包括两个核心模块：区域感知动态聚合（RDA）模块，为每个区域动态搜索最佳时间感受野；区域感知动态激励（RDE）模块，强调学习包含更稳定行为模式的运动区域，同时抑制对更易受协变量影响的静态区域的注意力。实验结果表明，GaitRDAE在几个基准数据集上取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TMM.2025.3613158&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based gait recognition has achieved great success in variousapplications. The key to accurate gait recognition lies in considering theunique and diverse behavior patterns in different motion regions, especiallywhen covariates affect visual appearance. However, existing methods typicallyuse predefined regions for temporal modeling, with fixed or equivalent temporalscales assigned to different types of regions, which makes it difficult tomodel motion regions that change dynamically over time and adapt to theirspecific patterns. To tackle this problem, we introduce a Region-aware DynamicAggregation and Excitation framework (GaitRDAE) that automatically searches formotion regions, assigns adaptive temporal scales and applies correspondingattention. Specifically, the framework includes two core modules: theRegion-aware Dynamic Aggregation (RDA) module, which dynamically searches theoptimal temporal receptive field for each region, and the Region-aware DynamicExcitation (RDE) module, which emphasizes the learning of motion regionscontaining more stable behavior patterns while suppressing attention to staticregions that are more susceptible to covariates. Experimental results show thatGaitRDAE achieves state-of-the-art performance on several benchmark datasets.</description>
      <author>example@mail.com (Binyuan Huang, Yongdong Luo, Xianda Guo, Xiawu Zheng, Zheng Zhu, Jiahui Pan, Chengju Zhou)</author>
      <guid isPermaLink="false">2510.16541v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales</title>
      <link>http://arxiv.org/abs/2510.16209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为StretchySnake的灵活状态空间模型训练方法，解决了视频理解中时空不灵活性问题，使模型能够更好地处理不同分辨率和长度的视频，在多种动作识别基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;状态空间模型(SSMs)已成为transformers的有力替代方案，具有线性计算复杂度和隐藏状态递归特性，特别适合建模长序列。然而，当前视频理解训练方法针对transformers设计，未能充分利用SSMs的独特属性，导致模型在训练中未见过的空间和时间分辨率视频上性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种灵活的训练方法，利用并改进SSMs的固有适应性，使模型能够无缝处理从短时精细片段到长时间复杂活动的各种视频，解决时空不灵活性限制模型在短视频和长视频间保持性能的问题。&lt;h4&gt;方法&lt;/h4&gt;在训练过程中对视频进行不同时间和空间分辨率的采样，并动态插值模型权重以适应任何时空尺度。研究提出了五种不同的灵活训练变体，并确定了视频SSMs最有效的策略，创建了名为StretchySnake的模型。&lt;h4&gt;主要发现&lt;/h4&gt;StretchySnake在短动作(UCF-101, HMDB-51)和长动作(COIN, Breakfast)基准测试中，超越了transformer和SSM基线，性能提升高达28%，同时展现出对精细动作(SSV2, Diving-48)的强大适应能力。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种简单的即插即用训练方案，使视频SSMs在各种动作识别场景中更加健壮、分辨率无关且高效，解决了视频模型在时空变化下的性能退化问题。&lt;h4&gt;翻译&lt;/h4&gt;状态空间模型(SSMs)已成为各种任务中transformers的有竞争力的替代方案。它们的线性复杂度和隐藏状态递归特性使它们特别适合建模长序列，而注意力机制则变得二次方昂贵。然而，当前视频理解的训练方法是为transformers量身定制的，未能充分利用SSMs的独特属性。例如，视频模型通常在固定分辨率和视频长度下训练，以平衡注意力成本的二次方扩展与性能。因此，当在训练中未见过的空间和时间分辨率的视频上评估时，这些模型性能会下降；我们称这种特性为时空不灵活性。在动作识别的背景下，这严重限制了模型在短视频和长视频之间保持性能的能力。因此，我们提出了一种灵活的训练方法，利用并改进SSMs的固有适应性。我们的方法在训练过程中对视频进行不同时间和空间分辨率的采样，并动态插值模型权重以适应任何时空尺度。这使我们的SSM（我们称之为StretchySnake）具有时空灵活性，能够无缝处理从短时精细片段到长时间复杂活动的各种视频。我们介绍并比较了五种不同的灵活训练变体，确定了视频SSMs最有效的策略。在短动作(UCF-101, HMDB-51)和长动作(COIN, Breakfast)基准测试中，StretchySnake超越了transformer和SSM基线，性能提升高达28%，对精细动作(SSV2, Diving-48)具有强大的适应能力。因此，我们的方法提供了一种简单的即插即用训练方案，使视频SSMs在各种动作识别场景中更加健壮、分辨率无关且高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State space models (SSMs) have emerged as a competitive alternative totransformers in various tasks. Their linear complexity and hidden-staterecurrence make them particularly attractive for modeling long sequences,whereas attention becomes quadratically expensive. However, current trainingmethods for video understanding are tailored towards transformers and fail tofully leverage the unique attributes of SSMs. For example, video models areoften trained at a fixed resolution and video length to balance the quadraticscaling of attention cost against performance. Consequently, these modelssuffer from degraded performance when evaluated on videos with spatial andtemporal resolutions unseen during training; a property we call spatio-temporalinflexibility. In the context of action recognition, this severely limits amodel's ability to retain performance across both short- and long-form videos.Therefore, we propose a flexible training method that leverages and improvesthe inherent adaptability of SSMs. Our method samples videos at varyingtemporal and spatial resolutions during training and dynamically interpolatesmodel weights to accommodate any spatio-temporal scale. This instills our SSM,which we call StretchySnake, with spatio-temporal flexibility and enables it toseamlessly handle videos ranging from short, fine-grained clips to long,complex activities. We introduce and compare five different variants offlexible training, and identify the most effective strategy for video SSMs. Onshort-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,StretchySnake outperforms transformer and SSM baselines alike by up to 28%,with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,our method provides a simple drop-in training recipe that makes video SSMs morerobust, resolution-agnostic, and efficient across diverse action recognitionscenarios.</description>
      <author>example@mail.com (Nyle Siddiqui, Rohit Gupta, Sirnam Swetha, Mubarak Shah)</author>
      <guid isPermaLink="false">2510.16209v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Referential Consistency: Do LLMs Favor Sequences Over Absolute Time References?</title>
      <link>http://arxiv.org/abs/2510.15513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP Main Long Paper 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对大型语言模型(LLMs)在时间参考一致性方面的不足，提出了一个新的基准测试TEMP-ReCon和一种基于推理路径对齐的模型UnTRaP，以增强LLMs在时间敏感领域的时间推理能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)正越来越多地被作为知识来源的替代品，在法律、医疗保健和金融等时间敏感领域尤为显著。LLMs需要具备事实准确性和时间维度上的一致性，但目前确保LLMs时间一致性的努力仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;引入一个名为'temporal referential consistency'的新基准，并开发资源TEMP-ReCon，用于评估各种开源和闭源LLMs在不同语言环境(包括英语、法语和罗马尼亚语)中的时间参考一致性。&lt;h4&gt;方法&lt;/h4&gt;提出UnTRaP模型，这是一种基于推理路径对齐的模型，旨在提高LLMs的时间参考一致性。通过实验验证其与基线模型相比的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现大型语言模型确实表现出不足的时间参考一致性。通过引入新基准和资源，以及提出UnTRaP模型，可以有效解决这一问题。&lt;h4&gt;结论&lt;/h4&gt;UnTRaP模型相比几个基线模型更为有效，能够增强LLMs的时间参考一致性，为LLMs在时间敏感领域的应用提供了更好的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)作为知识来源替代品的日益普及标志着各领域的重要范式转变，包括法律、医疗保健和金融等时间敏感领域。为了满足这一扩展角色，LLMs不仅需要事实准确，还需要在时间维度上表现出一致性，这需要强大的时间推理能力。尽管有这一关键需求，确保LLMs时间一致性的努力仍然很少，包括在时间敏感查询中评估或增强LLMs时间参考方面的明显缺失。在本文中，我们通过引入一个名为'temporal referential consistency'的新基准以及资源TEMP-ReCon来填补这一空白，该资源用于评估各种开源和闭源LLMs在不同资源丰富度的语言环境(包括英语、法语和罗马尼亚语)中的表现。研究结果强调LLMs确实表现出不足的时间参考一致性。为此，我们提出了UnTRaP，一种基于推理路径对齐的模型，旨在提高LLMs的时间参考一致性。我们的实证实验证明了UnTRaP相比几个基线模型的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing acceptance of large language models (LLMs) as an alternativeto knowledge sources marks a significant paradigm shift across various domains,including time-sensitive fields such as law, healthcare, and finance. Tofulfill this expanded role, LLMs must not only be factually accurate but alsodemonstrate consistency across temporal dimensions, necessitating robusttemporal reasoning capabilities. Despite this critical requirement, efforts toensure temporal consistency in LLMs remain scarce including noticeable absenceof endeavors aimed at evaluating or augmenting LLMs across temporal referencesin time-sensitive inquiries. In this paper, we seek to address this gap byintroducing a novel benchmark entitled temporal referential consistency,accompanied by a resource TEMP-ReCon designed to benchmark a wide range of bothopen-source and closed-source LLMs with various linguistic contextscharacterized by differing resource richness (including English, French, andRomanian). The findings emphasis that LLMs do exhibit insufficient temporalreferent consistency. To address this, we propose \newmodel, a reasoning pathalignment-based model that aims to enhance the temporal referential consistencyof LLMs. Our empirical experiments substantiate the efficacy of UnTRaP comparedto several baseline models.</description>
      <author>example@mail.com (Ashutosh Bajpai, Tanmoy Chakraborty)</author>
      <guid isPermaLink="false">2510.15513v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models</title>
      <link>http://arxiv.org/abs/2510.15148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了XModBench，一个用于评估全模态大语言模型(OLLMs)跨模态一致性的基准测试。研究发现，即使是目前最强的模型如Gemini 2.5 Pro，在空间和时间推理方面表现不佳，存在模态差异和方向性不平衡问题，表明当前OLLMs距离真正的模态不变推理还有很大差距。&lt;h4&gt;背景&lt;/h4&gt;全模态大语言模型(OLLMs)旨在统一音频、视觉和文本理解于单一框架。现有基准主要评估通用跨模态问答能力，但不清楚OLLMs是否实现了模态不变推理或存在模态特定偏差。&lt;h4&gt;目的&lt;/h4&gt;引入XModBench，一个大规模的三模态基准，专门用于测量跨模态一致性，评估OLLMs的模态不变推理能力、模态差异和方向性不平衡。&lt;h4&gt;方法&lt;/h4&gt;XModBench包含60,828个多选题，涵盖五个任务家族，系统覆盖了问答对中的所有六种模态组合，能够对OLLM的模态不变推理、模态差异和方向性不平衡进行细粒度诊断。&lt;h4&gt;主要发现&lt;/h4&gt;即使是目前最强的模型Gemini 2.5 Pro，(i)在空间和时间推理方面表现不佳，准确率低于60%，(ii)存在持续的模态差异，当相同语义内容通过音频而非文本传达时性能显著下降，(iii)表现出系统性的方向性不平衡，当视觉作为上下文时一致性低于文本。&lt;h4&gt;结论&lt;/h4&gt;当前OLLMs距离真正的模态不变推理还有很长的路要走，XModBench可作为评估和改进跨模态能力的基本诊断工具。&lt;h4&gt;翻译&lt;/h4&gt;全模态大语言模型(OLLMs)旨在在单一框架内统一音频、视觉和文本理解。虽然现有基准主要评估通用的跨模态问答能力，但尚不清楚OLLMs是否实现了模态不变的推理或表现出模态特定的偏差。我们引入了XModBench，一个大规模的三模态基准，专门设计用于测量跨模态一致性。XModBench包含60,828个多选题，涵盖五个任务家族，并系统性地覆盖了问答对中的所有六种模态组合，能够对OLLM的模态不变推理、模态差异和方向性不平衡进行细粒度诊断。实验表明，即使是最强的模型Gemini 2.5 Pro，(i)在空间和时间推理方面表现不佳，准确率低于60%，(ii)表现出持续的模态差异，当相同语义内容通过音频而非文本传达时，性能显著下降，(iii)显示出系统性的方向性不平衡，当视觉作为上下文时，一致性低于文本。这些发现表明，当前的OLLMs距离真正的模态不变推理还有很长的路要走，并将XModBench定位为评估和改进跨模态能力的基本诊断工具。所有数据和评估工具将在https://xingruiwang.github.io/projects/XModBench/提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omni-modal large language models (OLLMs) aim to unify audio, vision, and textunderstanding within a single framework. While existing benchmarks primarilyevaluate general cross-modal question-answering ability, it remains unclearwhether OLLMs achieve modality-invariant reasoning or exhibit modality-specificbiases. We introduce XModBench, a large-scale tri-modal benchmark explicitlydesigned to measure cross-modal consistency. XModBench comprises 60,828multiple-choice questions spanning five task families and systematically coversall six modality compositions in question-answer pairs, enabling fine-graineddiagnosis of an OLLM's modality-invariant reasoning, modality disparity, anddirectional imbalance. Experiments show that even the strongest model, Gemini2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than60% accuracy, (ii) reveals persistent modality disparities, with performancedropping substantially when the same semantic content is conveyed through audiorather than text, and (iii) shows systematic directional imbalance, exhibitinglower consistency when vision serves as context compared to text. Thesefindings indicate that current OLLMs remain far from truly modality-invariantreasoning and position XModBench as a fundamental diagnostic tool forevaluating and improving cross-modal competence. All data and evaluation toolswill be available at https://xingruiwang.github.io/projects/XModBench/.</description>
      <author>example@mail.com (Xingrui Wang, Jiang Liu, Chao Huang, Xiaodong Yu, Ze Wang, Ximeng Sun, Jialian Wu, Alan Yuille, Emad Barsoum, Zicheng Liu)</author>
      <guid isPermaLink="false">2510.15148v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2510.16053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;交通预测是智能交通系统的核心技术，图神经网络已成为该领域的主流方法，但在处理事件信息方面仍面临挑战。&lt;h4&gt;背景&lt;/h4&gt;随着城市化进程加快，交通拥堵问题加剧，需要可靠且响应迅速的预测模型来改善城市资源分配和出行体验。&lt;h4&gt;目的&lt;/h4&gt;开发能够有效捕捉交通网络空间依赖关系和时间演化模式的预测模型，提高对复杂交通状况的响应能力。&lt;h4&gt;方法&lt;/h4&gt;采用图神经网络(GNNs)作为主要技术路线，结合图卷积结构和时间建模机制，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，并探索融入事件信息的方法。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs在捕捉周期性交通模式方面特别有效；早期基于人工特征的方法虽能提升对特定事件的响应，但严重依赖领域专家先验知识，难以泛化到复杂未知事件，且低维人工特征会导致语义细节丢失。&lt;h4&gt;结论&lt;/h4&gt;需要减少对人工特征的依赖，开发更有效的方法来处理交通预测中的事件信息，提高模型对未知事件的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确的交通预测是构建智能交通系统的核心技术，能够更好地进行城市资源分配和改善出行体验。随着城市化的发展，交通拥堵加剧，凸显了对可靠且响应迅速的预测模型的需求。近年来，深度学习，特别是图神经网络(GNNs)，已成为交通预测的主流范式。GNNs能够有效捕捉道路网络拓扑中的复杂空间依赖关系和交通流量数据中的动态时间演化模式。诸如STGCN和GraphWaveNet等基础模型，以及包括STWave和D2STGNN在内的最新发展，在标准交通数据集上取得了令人印象深刻的性能。这些方法结合了复杂的图卷积结构和时间建模机制，在捕捉和预测具有周期性规律的交通模式方面表现出特别的有效性。为了应对这一挑战，研究人员探索了多种融入事件信息的方式。早期尝试主要依赖人工设计的特征。例如，一些方法引入了人工定义的事件影响分数，或为不同事件引起的交通状况构建特定的子图。虽然这些方法在某种程度上增强了对特定事件的响应能力，但其主要缺点在于严重依赖领域专家的先验知识，使得对多样且复杂的未知事件的泛化变得困难，而低维人工特征往往导致丰富语义细节的丢失。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3748636.3762776&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate traffic forecasting is a core technology for building IntelligentTransportation Systems (ITS), enabling better urban resource allocation andimproved travel experiences. With growing urbanization, traffic congestion hasintensified, highlighting the need for reliable and responsive forecastingmodels. In recent years, deep learning, particularly Graph Neural Networks(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs caneffectively capture complex spatial dependencies in road network topology anddynamic temporal evolution patterns in traffic flow data. Foundational modelssuch as STGCN and GraphWaveNet, along with more recent developments includingSTWave and D2STGNN, have achieved impressive performance on standard trafficdatasets. These approaches incorporate sophisticated graph convolutionalstructures and temporal modeling mechanisms, demonstrating particulareffectiveness in capturing and forecasting traffic patterns characterized byperiodic regularities. To address this challenge, researchers have exploredvarious ways to incorporate event information. Early attempts primarily reliedon manually engineered event features. For instance, some approaches introducedmanually defined incident effect scores or constructed specific subgraphs fordifferent event-induced traffic conditions. While these methods somewhatenhance responsiveness to specific events, their core drawback lies in a heavyreliance on domain experts' prior knowledge, making generalization to diverseand complex unknown events difficult, and low-dimensional manual features oftenlead to the loss of rich semantic details.</description>
      <author>example@mail.com (Chenyang Yu, Xinpeng Xie, Yan Huang, Chenxi Qiu)</author>
      <guid isPermaLink="false">2510.16053v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model</title>
      <link>http://arxiv.org/abs/2510.17684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了IC-MoE模型，一种智能通信混合专家增强的医学图像分割基础模型，解决了现有微调方法中高级特征表示不足和预训练权重结构完整性受损的问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学图像分割领域已取得显著性能，自适应微调自然图像分割基础模型对医学图像分割任务至关重要。然而，现有微调方法存在两个局限性：高级特征表示不足和微调过程破坏预训练权重的结构完整性。&lt;h4&gt;目的&lt;/h4&gt;解决现有微调方法的局限性，提出一个能够增强高级特征表示同时保持预训练权重结构完整性的医学图像分割基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出IC-MoE模型，包含两个核心创新：1) 构建基础专家、语义专家和自适应专家，实现像素概率自适应投票策略，通过标签一致性和负载平衡进行专家选择和融合；2) 提出语义引导的对比学习方法，解决对比学习中的弱监督问题。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共医学图像分割数据集上的大量实验表明，IC-MoE优于其他最先进模型。IC-MoE有效地为基础医学图像分割模型补充了高级特征和预训练结构完整性，并在多样化医学图像分割场景中展现出优越的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;IC-MoE模型成功解决了现有微调方法的局限性，能够在增强高级特征表示的同时保持预训练权重的结构完整性，为医学图像分割任务提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割的基础模型已取得显著性能。自然图像分割基础模型的自适应微调对医学图像分割任务至关重要。然而，现有微调方法存在一些局限性：1) 高级特征表示不足；2) 微调过程破坏了预训练权重的结构完整性。受这些关键问题的启发，我们提出了一个智能通信混合专家增强的医学图像分割基础模型，名为IC-MoE，包含两个核心想法：1) 我们构建基础专家、语义专家和自适应专家。此外，我们实现了像素概率自适应投票策略，通过标签一致性和负载平衡实现专家选择和融合。这种方法初步增强了高级特征的表示能力，同时保留了预训练权重的结构完整性。2) 我们提出了一种语义引导的对比学习方法，解决了对比学习中弱监督的问题。这种方法进一步增强了高级特征的表示能力，同时保留了预训练权重的结构完整性。在三个公共医学图像分割数据集上的大量实验表明，IC-MoE优于其他最先进的模型。因此，所提出的IC-MoE有效地为基础医学图像分割模型补充了高级特征和预训练结构完整性。我们还验证了IC-MoE在多样化医学图像分割场景中的优越泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for medical image segmentation have achieved remarkableperformance. Adaptive fine-tuning of natural image segmentation foundationmodels is crucial for medical image segmentation tasks. However, somelimitations exist in existing fine-tuning methods: 1) insufficientrepresentation of high-level features and 2) the fine-tuning process disruptsthe structural integrity of pretrained weights. Inspired by these criticalproblems, we propose an intelligent communication mixture-of-expertsboosted-medical image segmentation foundation model, named IC-MoE, with twofoldideas: 1) We construct basic experts, semantic experts, and adaptive experts.Moreover, we implement a pixel probability adaptive voting strategy, whichenables expert selection and fusion through label consistency and loadbalancing. This approach preliminarily enhances the representation capabilityof high-level features while preserving the structural integrity of pretrainedweights. 2) We propose a semantic-guided contrastive learning method to addressthe issue of weak supervision in contrastive learning. This method furtherenhances the representation capability of high-level features while preservingthe structural integrity of pretrained weights. Extensive experiments acrossthree public medical image segmentation datasets demonstrate that the IC-MoEoutperforms other SOTA models. Consequently, the proposed IC-MoE effectivelysupplements foundational medical image segmentation models with high-levelfeatures and pretrained structural integrity. We also validate the superiorgeneralizability of the IC-MoE across diverse medical image segmentationscenarios.</description>
      <author>example@mail.com (Xinwei Zhang, Hu Chen, Zhe Yuan, Sukun Tian, Peng Feng)</author>
      <guid isPermaLink="false">2510.17684v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Curiosity-driven RL for symbolic equation solving</title>
      <link>http://arxiv.org/abs/2510.17022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the NeurIPS 2025 MATH-AI Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了增强的PPO算法在解决符号数学问题上的有效性，特别是能够处理涉及根式、指数和三角函数的非线性方程。&lt;h4&gt;背景&lt;/h4&gt;先前的研究表明对比学习可以解决单变量线性方程，但强化学习在符号数学领域的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;探索强化学习是否可以有效地应用于符号数学问题。&lt;h4&gt;方法&lt;/h4&gt;使用无模型PPO算法，并加入基于好奇心的探索机制和基于图的动作表示。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够解决涉及根式、指数和三角函数的非线性方程，而不仅仅是简单的线性方程。&lt;h4&gt;结论&lt;/h4&gt;基于好奇心的探索机制可能对解决一般符号推理任务具有实用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们探索强化学习是否可以用于符号数学。先前的工作表明对比学习可以解决单变量线性方程。我们展示了无模型PPO结合基于好奇心的探索和基于图的动作可以解决非线性方程，如涉及根式、指数和三角函数的方程。我们的研究表明基于好奇心的探索可能对一般符号推理任务有用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore if RL can be useful for symbolic mathematics. Previous work showedcontrastive learning can solve linear equations in one variable. We showmodel-free PPO \cite{schulman2017proximal} augmented with curiosity-basedexploration and graph-based actions can solve nonlinear equations such as thoseinvolving radicals, exponentials, and trig functions. Our work suggestscuriosity-based exploration may be useful for general symbolic reasoning tasks.</description>
      <author>example@mail.com (Kevin P. O Keeffe)</author>
      <guid isPermaLink="false">2510.17022v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.16797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MOSAIC是一种多阶段框架，用于句子嵌入模型的领域自适应，结合了领域特定的掩码监督。&lt;h4&gt;背景&lt;/h4&gt;大规模通用领域句子嵌入模型适应到专业领域面临的挑战。&lt;h4&gt;目的&lt;/h4&gt;有效学习领域相关表示，同时保持原始模型的强语义区分特性。&lt;h4&gt;方法&lt;/h4&gt;通过在统一训练流程中联合优化掩码语言模型(MLM)和对比目标。&lt;h4&gt;主要发现&lt;/h4&gt;在高资源和低资源领域都取得了显著提升，NDCG@10指标比基线提高最多13.4%。&lt;h4&gt;结论&lt;/h4&gt;平衡的联合监督和阶段适应对有效领域自适应至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了MOSAIC（具有选择性适应的掩码目标用于领域内对比学习），这是一种用于句子嵌入模型领域自适应的多阶段框架，它结合了联合领域特定的掩码监督。我们的方法解决了将大规模通用领域句子嵌入模型适应到专业领域的挑战。通过在统一的训练流程中联合优化掩码语言模型(MLM)和对比目标，我们的方法能够有效地学习领域相关表示，同时保持原始模型的强语义区分特性。我们在高资源和低资源领域上都经验性地验证了我们的方法，在NDCG@10（标准化折损累积增益）上比强大的通用领域基线提高了最多13.4%。全面的消融研究进一步证明了每个组件的有效性，强调了平衡联合监督和阶段适应的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domainContrastive learning), a multi-stage framework for domain adaptation ofsentence embedding models that incorporates joint domain-specific maskedsupervision. Our approach addresses the challenges of adapting large-scalegeneral-domain sentence embedding models to specialized domains. By jointlyoptimizing masked language modeling (MLM) and contrastive objectives within aunified training pipeline, our method enables effective learning ofdomain-relevant representations while preserving the robust semanticdiscrimination properties of the original model. We empirically validate ourapproach on both high-resource and low-resource domains, achieving improvementsup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over stronggeneral-domain baselines. Comprehensive ablation studies further demonstratethe effectiveness of each component, highlighting the importance of balancedjoint supervision and staged adaptation.</description>
      <author>example@mail.com (Vera Pavlova, Mohammed Makhlouf)</author>
      <guid isPermaLink="false">2510.16797v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization</title>
      <link>http://arxiv.org/abs/2510.16704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的领域连接对比学习(DCCL)方法，用于解决领域泛化(DG)中的问题。研究发现直接应用对比学习(CL)会降低DG性能，原因是缺乏类内连接。DCCL通过改进数据增强和跨域正样本，以及提出模型锚定和生成变换损失来增强类内连接，从而提升DG性能。实验表明，该方法在五个标准DG基准上优于最先进的基线方法，且不需要领域监督。&lt;h4&gt;背景&lt;/h4&gt;训练和测试样本之间的分布偏移在实践中经常发生，阻碍了模型的泛化性能。这促使了对领域泛化(DG)的研究，DG旨在仅使用源域数据来预测未见过的目标域数据的标签。&lt;h4&gt;目的&lt;/h4&gt;解决直接应用对比学习(CL)降低领域泛化(DG)性能的问题，增强跨领域的概念连接，获得适用于DG的可泛化表示。&lt;h4&gt;方法&lt;/h4&gt;提出领域连接对比学习(DCCL)范式。在数据方面，引入更积极的数据增强和跨域正样本以改善类内连接；在模型方面，提出模型锚定来利用预训练表示中的类内连接，并通过生成变换损失补充锚定。&lt;h4&gt;主要发现&lt;/h4&gt;在DG设置中缺乏类内连接是导致直接应用CL降低性能的原因；提出的DCCL方法通过增强类内连接，在五个标准DG基准上优于最先进的基线方法，且不需要领域监督。&lt;h4&gt;结论&lt;/h4&gt;领域连接对比学习(DCCL)是解决DG中分布偏移问题的有效方法，通过增强类内连接和跨领域概念连接，能够获得更好的可泛化表示。&lt;h4&gt;翻译&lt;/h4&gt;训练和测试样本之间的分布偏移在实践中经常发生，并阻碍了模型的泛化性能。这一关键挑战促使了对领域泛化(DG)的研究，DG旨在仅使用源域数据来预测未见过的目标域数据的标签。直观上，对比学习(CL)中学到的类分离表示应该能够改善DG，但实际情况恰恰相反：直接应用CL会降低性能。作者通过CL理论的见解分析了这一现象，发现在DG设置中缺乏类内连接导致了这一缺陷。因此，作者提出了一个新的范式——领域连接对比学习(DCCL)，以增强跨领域的概念连接并获得适用于DG的可泛化表示。在数据方面，引入更积极的数据增强和跨域正样本以改善类内连接；在模型方面，为更好地嵌入未见过的测试域，作者提出模型锚定来利用预训练表示中的类内连接，并通过生成变换损失补充锚定。在五个标准的DG基准上进行了大量实验，结果验证了DCCL优于最先进的基线方法，甚至不需要领域监督。详细的模型实现和代码可通过https://github.com/weitianxin/DCCL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3690624.3709280&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distribution shifts between training and testing samples frequently occur inpractice and impede model generalization performance. This crucial challengethereby motivates studies on domain generalization (DG), which aim to predictthe label on unseen target domain data by solely using data from sourcedomains. It is intuitive to conceive the class-separated representationslearned in contrastive learning (CL) are able to improve DG, while the realityis quite the opposite: users observe directly applying CL deteriorates theperformance. We analyze the phenomenon with the insights from CL theory anddiscover lack of intra-class connectivity in the DG setting causes thedeficiency. We thus propose a new paradigm, domain-connecting contrastivelearning (DCCL), to enhance the conceptual connectivity across domains andobtain generalizable representations for DG. On the data side, more aggressivedata augmentation and cross-domain positive samples are introduced to improveintra-class connectivity. On the model side, to better embed the unseen testdomains, we propose model anchoring to exploit the intra-class connectivity inpre-trained representations and complement the anchoring with generativetransformation loss. Extensive experiments on five standard DG benchmarks areperformed. The results verify that DCCL outperforms state-of-the-art baselineseven without domain supervision. The detailed model implementation and the codeare provided through https://github.com/weitianxin/DCCL</description>
      <author>example@mail.com (Tianxin Wei, Yifan Chen, Xinrui He, Wenxuan Bao, Jingrui He)</author>
      <guid isPermaLink="false">2510.16704v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions</title>
      <link>http://arxiv.org/abs/2510.16540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 (poster). This is the camera-ready version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出READ方法，通过添加两个辅助目标到对比学习中，增强视觉-语言模型的组合推理能力。READ-CLIP在五个主要组合推理基准测试中取得最先进性能，比传统微调基线高4.1%。&lt;h4&gt;背景&lt;/h4&gt;尽管近期有所进展，但使用标准对比目标训练的视觉-语言模型在组合推理（理解视觉和语言元素间结构关系）方面仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种微调方法，增强视觉-语言模型的组合推理能力，解决文本编码器关注单词而非单词间关系的问题。&lt;h4&gt;方法&lt;/h4&gt;引入READ方法，添加两个辅助目标到对比学习中：(1)令牌级重建目标，使用冻结预训练解码器重建替代标题；(2)句子级对齐目标，在嵌入空间中对齐释义句子。&lt;h4&gt;主要发现&lt;/h4&gt;READ-CLIP在五个组合推理基准测试中达到最先进性能，比最强基线高4.1%；READ应用于其他CLIP变体也提高了性能；重建和对齐目标提供互补好处。&lt;h4&gt;结论&lt;/h4&gt;READ方法有效增强了视觉-语言模型的组合推理能力，重建目标促进编码器捕获单词间关系，对齐目标确保不同措辞表达的释义具有一致表示。&lt;h4&gt;翻译&lt;/h4&gt;尽管近期有所进展，但使用标准对比目标训练的视觉-语言模型仍然在组合推理——即理解视觉和语言元素之间结构关系的能力——方面存在困难。这一缺点主要是由于文本编码器倾向于关注单个单词而非它们之间的关系，这种局限性通过主要将单词与视觉对象对齐的对比训练得到了强化。在本文中，我们引入了READ（文本描述的重建和对齐），这是一种微调方法，通过向对比学习中添加两个辅助目标来增强组合推理能力：(1)令牌级重建目标，其中冻结的预训练解码器基于原始标题的嵌入重建替代标题；(2)句子级对齐目标，在嵌入空间中明确对齐释义句子。我们表明，通过将READ方法应用于预训练的CLIP模型得到的READ-CLIP模型在五个主要的组合推理基准测试中取得了最先进的性能，比最强的传统微调基线高出最多4.1%。此外，将READ应用于现有的CLIP变体（包括NegCLIP和FSC-CLIP）也提高了这些基准测试的性能。定量和定性分析表明，我们提出的目标——重建和对齐——提供了互补的好处：前者鼓励编码器捕获标题中单词之间的关系，而后者确保用不同措辞表达的释义具有一致的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances, vision-language models trained with standardcontrastive objectives still struggle with compositional reasoning -- theability to understand structured relationships between visual and linguisticelements. This shortcoming is largely due to the tendency of the text encoderto focus on individual words rather than their relations, a limitationreinforced by contrastive training that primarily aligns words with visualobjects. In this paper, we introduce REconstruction and Alignment of textDescriptions (READ), a fine-tuning method designed to enhance compositionalreasoning by adding two auxiliary objectives to the contrastive learning: (1) atoken-level reconstruction objective, where a frozen pre-trained decoderreconstructs alternative captions based on the embedding of the originalcaption; and (2) a sentence-level alignment objective, which explicitly alignsparaphrased sentences in the embedding space. We show that READ-CLIP, a modelderived by applying the READ method to the pre-trained CLIP model, achieves thestate-of-the-art performance across five major compositional reasoningbenchmarks, outperforming the strongest conventional fine-tuning baseline by upto 4.1%. Furthermore, applying the READ to existing CLIP variants (includingNegCLIP and FSC-CLIP) also improves performance on these benchmarks.Quantitative and qualitative analyses reveal that our proposed objectives --reconstruction and alignment -- offer complementary benefits: the formerencourages the encoder to capture relationships between words within a caption,while the latter ensures consistent representations for paraphrases expressedwith different wording.</description>
      <author>example@mail.com (Jihoon Kwon, Kyle Min, Jy-yong Sohn)</author>
      <guid isPermaLink="false">2510.16540v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy</title>
      <link>http://arxiv.org/abs/2510.16450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种弱监督域适应方法，用于电子显微镜图像中线粒体的高效分割，通过多任务学习框架和实例感知的伪标签选择策略，显著提高了分割性能。&lt;h4&gt;背景&lt;/h4&gt;从各种电子显微镜图像中分割大量线粒体实例对生物和神经科学研究具有重要价值。无监督域适应方法虽然可以缓解域偏移并降低标注成本，但在实际应用中性能较低。&lt;h4&gt;目的&lt;/h4&gt;研究弱监督域适应(WDA)方法，利用目标域上的稀疏点标签，这些标签需要最少的标注工作和专业知识，以实现高效准确的线粒体分割。&lt;h4&gt;方法&lt;/h4&gt;引入一个多任务学习框架，同时进行分割和中心检测，采用新颖的交叉教学机制和面向类的跨域对比学习。提出分割自训练，使用实例感知的伪标签(IPL)选择策略，帮助选择语义上可靠和多样的伪标签。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的数据集上验证，该方法优于现有的UDA和WDA方法，显著缩小了与监督上限的性能差距。在UDA设置下，也显著优于其他UDA技术。&lt;h4&gt;结论&lt;/h4&gt;所提出的弱监督域适应方法通过有效利用稀疏点标注和实例感知的伪标签策略，实现了电子显微镜图像中线粒体的高效分割，为生物和神经科学研究提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;从各种电子显微镜图像中高效分割大量线粒体实例对生物和神经科学研究非常有价值。尽管无监督域适应方法可以帮助缓解域偏移并降低每个域的标注成本，但它们在实际应用中通常具有相对较低的性能。因此，我们研究了弱监督域适应(WDA)，它利用目标域上的额外稀疏点标签，这些标签需要最少的标注工作和最少的专家知识。为了充分利用不完整和不精确的点标注，我们引入了一个多任务学习框架，通过新颖的交叉教学机制和面向类的跨域对比学习共同进行分割和中心检测。虽然利用未标记的图像区域至关重要，我们引入了分割自训练，采用新颖的实例感知的伪标签(IPL)选择策略。与通常依赖像素级伪标签过滤的现有方法不同，IPL在检测任务的帮助下，在语义上选择可靠和多样的伪标签。在具有挑战性的数据集上进行的全面验证和比较表明，我们的方法优于现有的UDA和WDA方法，显著缩小了与监督上限的性能差距。此外，在UDA设置下，我们的方法也实现了对其他UDA技术的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotation-efficient segmentation of the numerous mitochondria instances fromvarious electron microscopy (EM) images is highly valuable for biological andneuroscience research. Although unsupervised domain adaptation (UDA) methodscan help mitigate domain shifts and reduce the high costs of annotating eachdomain, they typically have relatively low performance in practicalapplications. Thus, we investigate weakly supervised domain adaptation (WDA)that utilizes additional sparse point labels on the target domain, whichrequire minimal annotation effort and minimal expert knowledge. To take fulluse of the incomplete and imprecise point annotations, we introduce a multitasklearning framework that jointly conducts segmentation and center detection witha novel cross-teaching mechanism and class-focused cross-domain contrastivelearning. While leveraging unlabeled image regions is essential, we introducesegmentation self-training with a novel instance-aware pseudo-label (IPL)selection strategy. Unlike existing methods that typically rely on pixel-wisepseudo-label filtering, the IPL semantically selects reliable and diversepseudo-labels with the help of the detection task. Comprehensive validationsand comparisons on challenging datasets demonstrate that our method outperformsexisting UDA and WDA methods, significantly narrowing the performance gap withthe supervised upper bound. Furthermore, under the UDA setting, our method alsoachieves substantial improvements over other UDA techniques.</description>
      <author>example@mail.com (Shan Xiong, Jiabao Chen, Ye Wang, Jialin Peng)</author>
      <guid isPermaLink="false">2510.16450v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Toward General Digraph Contrastive Learning: A Dual Spatial Perspective</title>
      <link>http://arxiv.org/abs/2510.16311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;S2-DiGCL是一种针对有向图对比学习的新型框架，通过结合复域和实域的空间视角，构建高质量的正负样本，实现更通用和鲁棒的有向图对比学习。&lt;h4&gt;背景&lt;/h4&gt;现有图对比学习方法主要关注无向图，忽略了现实网络(如社交网络和推荐系统)中基本且不可或缺的方向信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从复杂和真实领域角度强调空间洞察力的有向图对比学习框架，以捕获方向信息。&lt;h4&gt;方法&lt;/h4&gt;S2-DiGCL从复域角度将个性化扰动引入磁拉普拉斯矩阵以自适应调整边相位和方向语义；从实域角度采用基于路径的子图增强策略来捕获细粒度的局部非对称性和拓扑依赖。&lt;h4&gt;主要发现&lt;/h4&gt;在7个真实有向图数据集上的广泛实验表明，S2-DiGCL方法具有优越性，在监督和非监督设置下，节点分类和链路预测任务均达到了SOTA性能，分别提高了4.41%和4.34%。&lt;h4&gt;结论&lt;/h4&gt;通过联合利用互补的空间视角，S2-DiGCL能够构建高质量的正负样本，实现更通用和鲁棒的有向图对比学习。&lt;h4&gt;翻译&lt;/h4&gt;图对比学习(GCL)已成为从图中提取一致表示的强大工具，独立于标记信息。然而，现有方法主要关注无向图，忽略了现实网络(如社交网络和推荐系统)中基本且不可或缺的方向信息。本文介绍了S2-DiGCL，一种新型框架，从复杂和真实领域角度强调有向图(有向图)对比学习的空间洞察力。从复域角度，S2-DiGCL将个性化扰动引入磁拉普拉斯矩阵，以自适应调整边相位和方向语义。从实域角度，它采用基于路径的子图增强策略来捕获细粒度的局部非对称性和拓扑依赖。通过联合利用这两个互补的空间视角，S2-DiGCL构建高质量的正负样本，实现更通用和鲁棒的有向图对比学习。在7个真实有向图数据集上的广泛实验证明了我们方法的优越性，在监督和非监督设置下，节点分类和链路预测任务均达到了SOTA性能，分别提高了4.41%和4.34%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Contrastive Learning (GCL) has emerged as a powerful tool forextracting consistent representations from graphs, independent of labeledinformation. However, existing methods predominantly focus on undirectedgraphs, disregarding the pivotal directional information that is fundamentaland indispensable in real-world networks (e.g., social networks andrecommendations).In this paper, we introduce S2-DiGCL, a novel framework thatemphasizes spatial insights from complex and real domain perspectives fordirected graph (digraph) contrastive learning. From the complex-domainperspective, S2-DiGCL introduces personalized perturbations into the magneticLaplacian to adaptively modulate edge phases and directional semantics. Fromthe real-domain perspective, it employs a path-based subgraph augmentationstrategy to capture fine-grained local asymmetries and topologicaldependencies. By jointly leveraging these two complementary spatial views,S2-DiGCL constructs high-quality positive and negative samples, leading to moregeneral and robust digraph contrastive learning. Extensive experiments on 7real-world digraph datasets demonstrate the superiority of our approach,achieving SOTA performance with 4.41% improvement in node classification and4.34% in link prediction under both supervised and unsupervised settings.</description>
      <author>example@mail.com (Daohan Su, Yang Zhang, Xunkai Li, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2510.16311v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title>
      <link>http://arxiv.org/abs/2510.16219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SentinelNet，首个用于多智能体系统中主动检测和减轻恶意行为的去中心化框架，通过基于信誉的检测器和动态邻居排名实现高效防御。&lt;h4&gt;背景&lt;/h4&gt;恶意智能体对基于大型语言模型的多智能体系统的可靠性和决策能力构成重大威胁，现有防御措施因反应式设计或集中式架构存在单点故障问题而效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个去中心化框架，能够主动检测并减轻多智能体协作中的恶意行为，提高系统安全性。&lt;h4&gt;方法&lt;/h4&gt;为每个智能体配备基于信誉的检测器，通过对比学习在增强的对抗辩论轨迹上进行训练，实现消息可信度自主评估和动态邻居排名，并通过生成对抗轨迹解决攻击数据稀缺问题。&lt;h4&gt;主要发现&lt;/h4&gt;SentinelNet在多智能体系统基准测试中实现了接近100%的恶意智能体检测率（两轮内），能从受损系统中恢复95%的准确性，并展现出跨领域和攻击模式的强泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SentinelNet为保护协作多智能体系统建立了新的防御范式，有效解决了现有防御机制的局限性。&lt;h4&gt;翻译&lt;/h4&gt;恶意智能体对由大型语言模型驱动的多智能体系统的可靠性和决策能力构成重大威胁。现有防御往往因反应式设计或集中式架构而不足，这些架构可能引入单点故障。为解决这些挑战，我们提出SentinelNet，首个用于主动检测和减轻多智能体协作中恶意行为的去中心化框架。SentinelNet为每个智能体配备基于信誉的检测器，通过在增强的对抗辩论轨迹上进行对比学习训练，使智能体能够自主评估消息可信度并通过bottom-k消除进行动态邻居排名，以抑制恶意通信。为克服攻击数据稀缺问题，它生成模拟各种威胁的对抗轨迹，确保稳健训练。在多智能体系统基准测试中，SentinelNet实现了对恶意智能体的近乎完美检测，在两轮辩论内接近100%，并从受损的基线系统中恢复95%的准确性。通过在不同领域和攻击模式中展现强大的泛化能力，SentinelNet为保护协作多智能体系统建立了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malicious agents pose significant threats to the reliability anddecision-making capabilities of Multi-Agent Systems (MAS) powered by LargeLanguage Models (LLMs). Existing defenses often fall short due to reactivedesigns or centralized architectures which may introduce single points offailure. To address these challenges, we propose SentinelNet, the firstdecentralized framework for proactively detecting and mitigating maliciousbehaviors in multi-agent collaboration. SentinelNet equips each agent with acredit-based detector trained via contrastive learning on augmented adversarialdebate trajectories, enabling autonomous evaluation of message credibility anddynamic neighbor ranking via bottom-k elimination to suppress maliciouscommunications. To overcome the scarcity of attack data, it generatesadversarial trajectories simulating diverse threats, ensuring robust training.Experiments on MAS benchmarks show SentinelNet achieves near-perfect detectionof malicious agents, close to 100% within two debate rounds, and recovers 95%of system accuracy from compromised baselines. By exhibiting stronggeneralizability across domains and attack patterns, SentinelNet establishes anovel paradigm for safeguarding collaborative MAS.</description>
      <author>example@mail.com (Yang Feng, Xudong Pan)</author>
      <guid isPermaLink="false">2510.16219v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</title>
      <link>http://arxiv.org/abs/2510.17790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了UltraCUA，一种基础模型，通过混合动作机制无缝集成GUI基本操作与高级程序化工具调用，解决了传统计算机使用代理(CUAs)仅依赖基本操作导致的级联故障和性能瓶颈问题。研究包含四个关键组件：自动化工具扩展管道、合成数据引擎、混合动作轨迹收集和两阶段训练流程。实验证明UltraCUA在多个基准测试上显著优于现有代理。&lt;h4&gt;背景&lt;/h4&gt;当前计算机使用多模态代理(CUAs)完全依赖基本操作(点击、输入、滚动)，这些操作需要准确的视觉定位和冗长的执行链，导致级联故障和性能瓶颈。与其他利用丰富程序化接口(API、MCP服务器、工具)的代理不同，CUAs仍然与这些能力隔离。&lt;h4&gt;目的&lt;/h4&gt;开发一种基础模型，弥合CUAs与其他代理之间的差距，通过混合动作无缝集成GUI基本操作与高级程序化工具调用，提高CUAs的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;研究方法包括四个关键组件：(1)自动化管道，从软件文档、开源代码库和代码生成扩展程序化工具；(2)合成数据引擎，生成超过17,000个可验证的任务，涵盖真实世界计算机使用场景；(3)大规模高质量混合动作轨迹收集，同时包含低级GUI动作和高级程序化工具调用；(4)两阶段训练流程，结合监督微调与在线强化学习，实现低级和高级动作之间的战略性交替。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在OSWorld基准测试中，UltraCUA模型比基础模型平均实现22%的相对改进，并且在步骤上快11%；2) 在WindowsAgentArena上的跨域评估显示，模型达到21.7%的成功率，优于在Windows数据上训练的基线模型；3) 混合动作机制被证明是关键的，它减少了错误传播，同时保持了执行效率。&lt;h4&gt;结论&lt;/h4&gt;UltraCUA成功解决了传统CUAs的局限性，通过混合动作机制将GUI基本操作与高级程序化工具调用相结合，显著提高了性能和效率。这种创新方法不仅减少了错误传播，还保持了执行效率，为计算机使用代理领域带来了重大进步。&lt;h4&gt;翻译&lt;/h4&gt;多模态计算机使用代理完全依赖基本操作(点击、输入、滚动)，这些操作需要准确的视觉定位和冗长的执行链，导致级联故障和性能瓶颈。而其他代理则利用丰富的程序化接口(API、MCP服务器、工具)，计算机使用代理(CUAs)仍然与这些能力隔离。我们提出了UltraCUA，一种基础模型，通过混合动作弥合这一差距——无缝集成GUI基本操作与高级程序化工具调用。为实现这一目标，我们的方法包含四个关键组件：(1)自动化管道，从软件文档、开源代码库和代码生成扩展程序化工具；(2)合成数据引擎，生成超过17,000个可验证的任务，涵盖真实世界计算机使用场景；(3)大规模高质量混合动作轨迹收集，同时包含低级GUI动作和高级程序化工具调用；(4)两阶段训练流程，结合监督微调与在线强化学习，实现低级和高级动作之间的战略性交替。我们的7B和32B模型的实验表明，比最先进的代理有显著改进。在OSWorld上，UltraCUA模型比基础模型平均实现22%的相对改进，并且在步骤上快11%。在WindowsAgentArena上的跨域评估显示，我们的模型达到21.7%的成功率，优于在Windows数据上训练的基线模型。混合动作机制被证明是关键的，它减少了错误传播，同时保持了执行效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal agents for computer use rely exclusively on primitive actions(click, type, scroll) that require accurate visual grounding and lengthyexecution chains, leading to cascading failures and performance bottlenecks.While other agents leverage rich programmatic interfaces (APIs, MCP servers,tools), computer-use agents (CUAs) remain isolated from these capabilities. Wepresent UltraCUA, a foundation model that bridges this gap through hybridaction -- seamlessly integrating GUI primitives with high-level programmatictool calls. To achieve this, our approach comprises four key components: (1) anautomated pipeline that scales programmatic tools from software documentation,open-source repositories, and code generation; (2) a synthetic data engineproducing over 17,000 verifiable tasks spanning real-world computer-usescenarios; (3) a large-scale high-quality hybrid action trajectory collectionwith both low-level GUI actions and high-level programmatic tool calls; and (4)a two-stage training pipeline combining supervised fine-tuning with onlinereinforcement learning, enabling strategic alternation between low-level andhigh-level actions. Experiments with our 7B and 32B models demonstratesubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUAmodels achieve an average 22% relative improvement over base models, whilebeing 11% faster in terms of steps. Out-of-domain evaluation onWindowsAgentArena shows our model reaches 21.7% success rate, outperformingbaselines trained on Windows data. The hybrid action mechanism proves critical,reducing error propagation while maintaining execution efficiency.</description>
      <author>example@mail.com (Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan)</author>
      <guid isPermaLink="false">2510.17790v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Elastic ViTs from Pretrained Models without Retraining</title>
      <link>http://arxiv.org/abs/2510.17700v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SnapViT，一种用于Vision Transformers的单次网络近似方法，通过结构化剪枝实现弹性推理，无需重新训练或标签数据，可适应各种计算预算。&lt;h4&gt;背景&lt;/h4&gt;现有视觉基础模型仅在有限的预定义尺寸中可用，导致在现实约束下无法做出最优部署选择。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预训练后结构化剪枝方法，使模型能够在连续的计算预算范围内进行弹性推理。&lt;h4&gt;方法&lt;/h4&gt;SnapViT结合梯度信息和跨网络结构相关性，通过进化算法近似，无需标记数据，适用于无分类头的模型，且无需重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在DINO、SigLIPv2、DeIT和AugReg模型上的实验表明，该方法在各种稀疏度下优于最先进方法，在单个A100 GPU上仅需不到五分钟即可生成可调整到任何计算预算的弹性模型。&lt;h4&gt;结论&lt;/h4&gt;SnapViT贡献包括：预训练Vision Transformers的有效剪枝策略、Hessian非对角结构的新进化近似方法，以及自监督重要性评分机制，无需重新训练或标签即可保持强性能。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型取得了显著的性能，但仅在有限的预定义尺寸中可用，这迫使在现实约束下做出次优的部署选择。我们介绍了SnapViT：用于剪枝Vision Transformers的单次网络近似，这是一种新的预训练后结构化剪枝方法，能够在连续的计算预算范围内实现弹性推理。我们的方法高效地结合了梯度信息和跨网络结构相关性，通过进化算法近似，不需要标记数据，适用于没有分类头的模型，且无需重新训练。在DINO、SigLIPv2、DeIT和AugReg模型上的实验表明，在各种稀疏度下优于最先进的方法，在单个A100 GPU上需要不到五分钟生成可调整到任何计算预算的弹性模型。我们的主要贡献包括：预训练Vision Transformers的有效剪策策略，Hessian非对角结构的新进化近似方法，以及无需重新训练或标签的自监督重要性评分机制。代码和剪枝模型可在以下网址获取：https://elastic.ashita.nl/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models achieve remarkable performance but are onlyavailable in a limited set of pre-determined sizes, forcing sub-optimaldeployment choices under real-world constraints. We introduce SnapViT:Single-shot network approximation for pruned Vision Transformers, a newpost-pretraining structured pruning method that enables elastic inferenceacross a continuum of compute budgets. Our approach efficiently combinesgradient information with cross-network structure correlations, approximatedvia an evolutionary algorithm, does not require labeled data, generalizes tomodels without a classification head, and is retraining-free. Experiments onDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance overstate-of-the-art methods across various sparsities, requiring less than fiveminutes on a single A100 GPU to generate elastic models that can be adjusted toany computational budget. Our key contributions include an efficient pruningstrategy for pretrained Vision Transformers, a novel evolutionary approximationof Hessian off-diagonal structures, and a self-supervised importance scoringmechanism that maintains strong performance without requiring retraining orlabels. Code and pruned models are available at: https://elastic.ashita.nl/</description>
      <author>example@mail.com (Walter Simoncini, Michael Dorkenwald, Tijmen Blankevoort, Cees G. M. Snoek, Yuki M. Asano)</author>
      <guid isPermaLink="false">2510.17700v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration</title>
      <link>http://arxiv.org/abs/2510.17670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种级联方法，结合大型预训练开放词汇目标检测模型与轻量级少样本分类器，解决了遥感等专业领域中开放词汇目标检测模型的零样本性能问题，显著提高了对细粒度类别的区分能力，并大幅降低了遥感图像标注成本。&lt;h4&gt;背景&lt;/h4&gt;开放词汇目标检测(OVD)模型能够通过任意文本查询检测物体，具有显著灵活性，但在遥感等专业领域的零样本性能常受自然语言固有歧义的影响，限制了关键下游应用，例如难以区分'渔船'和'游艇'等细粒度类别。&lt;h4&gt;目的&lt;/h4&gt;解决OVD模型在专业领域如遥感中的零样本性能问题，提高模型对细粒度类别的区分能力，降低遥感图像标注的高成本，实现即时适应特定用户需求。&lt;h4&gt;方法&lt;/h4&gt;提出一种级联方法，首先使用零样本模型生成高召回率的目标提案，然后通过仅用少量用户标注示例实时训练的紧凑分类器进行高精度精炼；引入FLAME作为框架核心，这是一种一步主动学习策略，使用密度识别决策边界附近的不确定边际候选样本，并通过聚类确保样本多样性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法无需昂贵的全模型微调即可实现高精度；能在不到一分钟内实现即时适应，比最先进的替代方案快得多；在遥感基准测试中一致超越最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;建立了一个实用且资源高效的框架，使基础模型能够适应特定用户需求，为开放词汇目标检测在专业领域的应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇目标检测(OVD)模型能够通过任意文本查询检测物体，提供显著灵活性。然而，它们在遥感等专业领域的零样本性能常因自然语言的固有歧义而受到影响，限制了关键的下游应用。例如，OVD模型可能难以区分'渔船'和'游艇'等细粒度类别，因为它们的嵌入相似且常常不可分割。这可能会通过产生不相关的检测来阻碍特定的用户目标，如监控非法捕鱼。为解决此问题，我们提出了一种级联方法，将大型预训练OVD模型的广泛泛化能力与轻量级少样本分类器相结合。我们的方法首先使用零样本模型生成高召回率的目标提案。然后，这些提案通过仅用少量用户标注示例实时训练的紧凑分类器进行高精度精炼，大幅降低了遥感图像标注的高成本。我们框架的核心是FLAME，一种一步主动学习策略，用于选择信息量最大的样本进行训练。FLAME实时识别决策边界附近的不确定边际候选样本，然后进行聚类以确保样本多样性。这种高效的采样技术无需昂贵的全模型微调即可实现高精度，并能在不到一分钟内实现即时适应，比最先进的替代方案快得多。我们的方法在遥感基准测试中一致超越最先进的性能，为将基础模型适应特定用户需求建立了实用且资源高效的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary object detection (OVD) models offer remarkable flexibility bydetecting objects from arbitrary text queries. However, their zero-shotperformance in specialized domains like Remote Sensing (RS) is oftencompromised by the inherent ambiguity of natural language, limiting criticaldownstream applications. For instance, an OVD model may struggle to distinguishbetween fine-grained classes such as "fishing boat" and "yacht" since theirembeddings are similar and often inseparable. This can hamper specific usergoals, such as monitoring illegal fishing, by producing irrelevant detections.To address this, we propose a cascaded approach that couples the broadgeneralization of a large pre-trained OVD model with a lightweight few-shotclassifier. Our method first employs the zero-shot model to generatehigh-recall object proposals. These proposals are then refined for highprecision by a compact classifier trained in real-time on only a handful ofuser-annotated examples - drastically reducing the high costs of RS imageryannotation.The core of our framework is FLAME, a one-step active learningstrategy that selects the most informative samples for training. FLAMEidentifies, on the fly, uncertain marginal candidates near the decisionboundary using density estimation, followed by clustering to ensure samplediversity. This efficient sampling technique achieves high accuracy withoutcostly full-model fine-tuning and enables instant adaptation, within less thena minute, which is significantly faster than state-of-the-art alternatives.Ourmethod consistently surpasses state-of-the-art performance on RS benchmarks,establishing a practical and resource-efficient framework for adaptingfoundation models to specific user needs.</description>
      <author>example@mail.com (Yehonathan Refael, Amit Aides, Aviad Barzilai, George Leifman, Genady Beryozkin, Vered Silverman, Bolous Jaber, Tomer Shekel)</author>
      <guid isPermaLink="false">2510.17670v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model</title>
      <link>http://arxiv.org/abs/2510.17662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DELULU是一个说话人感知的自监督语音基础模型，通过在外部监督集成到伪标签生成过程中，显著提升了在说话人相关任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;自监督语音模型在内容驱动任务上表现优异，但在捕捉说话人区分性特征方面有限，这对验证、说话人分割和档案应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉说话人区分性特征的自监督语音模型，解决现有模型在说话人相关任务上的局限性。&lt;h4&gt;方法&lt;/h4&gt;DELULU利用ReDimNet的帧级嵌入指导k-means聚类，引入说话人区分性归纳偏差；使用掩码预测和去噪的双重目标进行训练，增强鲁棒性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;DELULU在说话人验证任务上实现高达62%的相对EER改进，在零样本档案任务（性别、年龄、口音、说话人计数）上取得一致提升。&lt;h4&gt;结论&lt;/h4&gt;DELULU是说话人感知语音处理的强大通用编码器，无需任务特定微调即可实现卓越性能。&lt;h4&gt;翻译&lt;/h4&gt;自监督语音模型在内容驱动任务上取得了显著成功，但在捕捉对验证、说话人分割和档案应用至关重要的说话人区分性特征方面仍然有限。我们引入了DELULU，一个说话人感知的自监督基础模型，通过在外部监督集成到伪标签生成过程中来解决这一局限性。DELULU利用来自ReDimNet（最先进的说话人验证模型）的帧级嵌入来指导预训练期间的k-means聚类步骤，引入了强大的说话人区分性归纳偏差，使表示学习与说话人身份保持一致。该模型使用结合掩码预测和去噪的双重目标进行训练，进一步增强了鲁棒性和泛化能力。DELULU在一系列以说话人为中心的任务上显著优于先前的自监督学习模型，在说话人验证的等错误率上实现了高达62%的相对改进，并在零样本档案任务（如性别、年龄、口音和说话人计数）上取得了一致的提升。我们的研究结果表明，DELULU是说话人感知语音处理的强大通用编码器，即使在没有任务特定微调的情况下也能实现卓越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised speech models have achieved remarkable success oncontent-driven tasks, yet they remain limited in capturingspeaker-discriminative features critical for verification, diarization, andprofiling applications. We introduce DELULU, a speaker-aware self-supervisedfoundational model that addresses this limitation by integrating externalsupervision into the pseudo-label generation process. DELULU leveragesframe-level embeddings from ReDimNet, a state-of-the-art speaker verificationmodel, to guide the k-means clustering step during pre-training, introducing astrong speaker-discriminative inductive bias that aligns representationlearning with speaker identity. The model is trained using a dual objectivethat combines masked prediction and denoising, further enhancing robustness andgeneralization. DELULU significantly outperforms prior self-supervised learning(SSL) models across a range of speaker-centric tasks, achieving up to 62%relative improvement in equal error rate (EER) for speaker verification andconsistent gains on zero-shot profiling tasks such as gender, age, accent, andspeaker counting. Our findings demonstrate that DELULU is a strong universalencoder for speaker-aware speech processing, enabling superior performance evenwithout task-specific fine-tuning.</description>
      <author>example@mail.com (Massa Baali, Rita Singh, Bhiksha Raj)</author>
      <guid isPermaLink="false">2510.17662v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2510.17457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept by NeurIPS 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GBN的局部方法，通过自适应调整基于局部结构的消息传递来解决MPNNs的过平滑和过挤压问题。&lt;h4&gt;背景&lt;/h4&gt;MPNNs是图基础模型的构建块，但存在过平滑和过挤压问题。现有解决方案主要采用全局方法，导致表达能力不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种局部方法，能够自适应地调整消息传递，同时解决过平滑和过挤压问题，提高MPNNs的表达能力。&lt;h4&gt;方法&lt;/h4&gt;作者将局部黎曼几何与MPNNs连接，建立了新的非齐次边界条件，并设计了具有局部瓶颈调整的GBN网络，基于Robin条件构建。&lt;h4&gt;主要发现&lt;/h4&gt;谱间隙的增加会导致梯度消失，削弱消息传递效果；GBN在同类同质和异类异质图上表现出强大的表达能力，且在网络深度超过256层时仍保持性能。&lt;h4&gt;结论&lt;/h4&gt;局部方法比全局方法更有效地解决MPNNs的过平滑和过挤压问题，GBN网络提供了理论保证和优异的实验性能。&lt;h4&gt;翻译&lt;/h4&gt;消息传递神经网络是图基础模型的构建块，但 fundamentally suffer from 过平滑和过挤压问题。最近有很多研究试图解决这两个问题。现有工作主要采用全局方法，在某些区域可能有益，但在其他区域可能有害，最终导致表达能力不足。本文通过全局度量谱间隙重新审视过挤压问题，并证明谱间隙的增加会导致相对于输入特征的梯度消失，从而削弱消息传递的有效性。基于这些理论见解，我们提出了一种局部方法，根据局部结构自适应调整消息传递。为此，我们将局部黎曼几何与MPNNs连接，并建立了新的非齐次边界条件来解决过挤压和过平滑问题。基于Robin条件，我们设计了具有局部瓶颈调整的GBN网络，并提供了理论保证。在同类同质和异类异质图上的广泛实验表明GBN的表达能力。此外，即使网络深度超过256层，GBN也不会表现出性能下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message Passing Neural Networks (MPNNs) is the building block of graphfoundation models, but fundamentally suffer from oversmoothing andoversquashing. There has recently been a surge of interest in fixing bothissues. Existing efforts primarily adopt global approaches, which may bebeneficial in some regions but detrimental in others, ultimately leading to thesuboptimal expressiveness. In this paper, we begin by revisiting oversquashingthrough a global measure -- spectral gap $\lambda$ -- and prove that theincrease of $\lambda$ leads to gradient vanishing with respect to the inputfeatures, thereby undermining the effectiveness of message passing. Motivatedby such theoretical insights, we propose a \textbf{local} approach thatadaptively adjusts message passing based on local structures. To achieve this,we connect local Riemannian geometry with MPNNs, and establish a novelnonhomogeneous boundary condition to address both oversquashing andoversmoothing. Building on the Robin condition, we design a GBN network withlocal bottleneck adjustment, coupled with theoretical guarantees. Extensiveexperiments on homophilic and heterophilic graphs show the expressiveness ofGBN. Furthermore, GBN does not exhibit performance degradation even when thenetwork depth exceeds $256$ layers.</description>
      <author>example@mail.com (Li Sun, Zhenhao Huang, Ming Zhang, Philip S. Yu)</author>
      <guid isPermaLink="false">2510.17457v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</title>
      <link>http://arxiv.org/abs/2510.17439v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://falcon-vla.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FALCON的新型视觉-语言-行动模型，通过将3D空间令牌注入行动头，解决了现有VLA模型的空间推理差距问题，实现了在模拟和现实场景中的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言-行动模型在3D真实世界中运行，但通常基于2D编码器构建，导致空间推理差距，限制了泛化能力和适应性。近期VLA的3D集成技术要么需要专用传感器且跨模态迁移性差，要么注入缺乏几何信息的弱提示，导致视觉-语言对齐质量下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLA模型在空间表示、模态迁移性和对齐方面的局限性，提升模型在复杂环境中的表现和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入FALCON（From Spatial to Action）范式，将丰富的3D空间令牌注入到行动头中；利用空间基础模型仅从RGB提供强大的几何先验；包含一个可选择性融合深度或姿态的具身空间模型，无需重新训练或架构改变；为保留语言推理能力，空间令牌被空间增强行动头处理，而非连接到视觉-语言主干。&lt;h4&gt;主要发现&lt;/h4&gt;FALCON在三个模拟基准测试和十一个现实世界任务的综合评估中实现了最先进性能，一致超越竞争基线，并且在杂乱环境、空间提示条件和物体高度变化等情况下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;FALCON通过创新的空间令牌注入机制和灵活的多模态融合能力，有效解决了VLA模型在空间表示、模态迁移性和对齐方面的局限性，为3D环境中的智能行动提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现有的视觉-语言-行动模型在3D真实世界中运行，但通常基于2D编码器构建，留下了限制泛化和适应性的空间推理差距。近期VLA的3D集成技术要么需要专用传感器且跨模态迁移性差，要么注入缺乏几何信息的弱提示，导致视觉-语言对齐质量下降。在这项工作中，我们引入FALCON（From Spatial to Action），一种将丰富的3D空间令牌注入行动头的新颖范式。FALCON利用空间基础模型仅从RGB提供强大的几何先验，并包含一个可选择性融合深度或姿态的具身空间模型，当可用时提供更高保真度，无需重新训练或架构改变。为保留语言推理能力，空间令牌被空间增强行动头消耗，而非连接到视觉-语言主干。这些设计使FALCON能够解决空间表示、模态迁移性和对齐方面的局限性。在三个模拟基准测试和十一个现实世界任务的综合评估中，我们提出的FALCON实现了最先进性能，一致超越竞争基线，并且在杂乱环境、空间提示条件和物体高度变化等情况下保持鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉-语言-动作（VLA）模型的空间推理差距问题。这些模型虽然能在3D真实世界运行，但通常基于2D编码器构建，导致缺乏可靠的3D空间理解，限制了机器人在新场景、背景变化或物体变化时的泛化能力和适应性。这个问题非常重要，因为机器人需要与3D物理世界交互，而缺乏明确的3D意识使它们难以处理需要几何推理、深度感知或空间关系理解的任务，这已成为开发可靠通用机器人政策的主要瓶颈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先深入分析了现有VLA模型的局限性，注意到它们基于2D编码器但需要在3D世界运行，存在空间推理差距。作者借鉴了空间基础模型（如VGGT、DUSt3R）的思路，这些模型能将场景编码为令牌序列进行3D重建；同时受到大脑分工的启发，将VLM比作处理高级推理的大脑，动作头比作管理精细运动的小脑。作者还参考了现有VLA架构（如RT-2、OpenVLA），但改进了空间信息集成方式，并利用了现有的深度估计和相机姿态编码技术。最终设计了具身空间模型(ESM)和空间增强动作头，实现了空间与语义的有效融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将丰富的3D空间令牌注入到VLA模型的动作头中来增强空间理解能力，同时保持语言推理能力。整体流程包括：1)双路径处理 - VLM路径处理视觉和语言输入提取语义表示，ESM路径处理图像和可选几何输入提取空间令牌；2)ESM通过令牌化、空间编码和可选的深度/姿态注入来生成空间令牌；3)通过最大池化和MLP适配器将空间特征投影到VLM特征空间；4)使用元素级加法融合空间特征与语义动作令牌；5)融合后的特征输入动作预测器（MLP或LSTM）生成机器人动作序列；6)采用两阶段后训练方法确保训练稳定性和特征对齐。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间令牌注入新范式 - 将3D空间令牌注入动作头而非连接文本令牌；2)具身空间模型(ESM) - 可选择性整合深度和姿态等3D模态；3)空间增强动作头 - 直接将空间令牌整合到动作决策中；4)随机条件策略 - 确保模型在不同输入条件下都能有效工作。相比之前工作，FALCON不依赖特定3D传感器（区别于PointVLA、GeoVLA），不会破坏预训练的视觉-语言对齐（区别于3D-VLA、SpatialVLA），提供了显式3D理解（区别于传统2D VLA），并采用高效的两阶段训练方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FALCON通过将空间基础模型提供的丰富3D空间令牌注入到专门设计的空间增强动作头中，解决了现有视觉-语言-动作模型在3D空间理解上的局限，实现了强大的模态转移能力和在复杂空间任务中的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing vision-language-action (VLA) models act in 3D real-world but aretypically built on 2D encoders, leaving a spatial reasoning gap that limitsgeneralization and adaptability. Recent 3D integration techniques for VLAseither require specialized sensors and transfer poorly across modalities, orinject weak cues that lack geometry and degrade vision-language alignment. Inthis work, we introduce FALCON (From Spatial to Action), a novel paradigm thatinjects rich 3D spatial tokens into the action head. FALCON leverages spatialfoundation models to deliver strong geometric priors from RGB alone, andincludes an Embodied Spatial Model that can optionally fuse depth, or pose forhigher fidelity when available, without retraining or architectural changes. Topreserve language reasoning, spatial tokens are consumed by a Spatial-EnhancedAction Head rather than being concatenated into the vision-language backbone.These designs enable FALCON to address limitations in spatial representation,modality transferability, and alignment. In comprehensive evaluations acrossthree simulation benchmarks and eleven real-world tasks, our proposed FALCONachieves state-of-the-art performance, consistently surpasses competitivebaselines, and remains robust under clutter, spatial-prompt conditioning, andvariations in object scale and height.</description>
      <author>example@mail.com (Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou)</author>
      <guid isPermaLink="false">2510.17439v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion Models as Dataset Distillation Priors</title>
      <link>http://arxiv.org/abs/2510.17421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Diffusion As Priors (DAP)方法，通过利用扩散模型中的代表性先验，解决了数据集蒸馏中同时实现多样性、泛化能力和代表性的挑战。DAP在特征空间中使用Mercer核量化合成数据与真实数据的相似性，并将此先验作为指导引导反向扩散过程，无需重新训练即可提高蒸馏数据集质量。实验证明，DAP在ImageNet-1K等大型数据集上优于现有方法，实现了更好的跨架构泛化能力。&lt;h4&gt;背景&lt;/h4&gt;数据集蒸馏旨在从大型数据集中合成紧凑而信息丰富的数据集。该领域的一个重大挑战是在单个蒸馏数据集中同时实现多样性、泛化能力和代表性。虽然最近的生成式数据集蒸馏方法采用了强大的扩散模型作为基础模型，但这些方法忽略了扩散模型中固有的代表性先验，因此需要集成外部约束来提高数据质量。&lt;h4&gt;目的&lt;/h4&gt;解决数据集蒸馏中同时实现多样性、泛化能力和代表性的挑战，通过利用扩散模型中固有的代表性先验，提出一种无需重新训练即可提高蒸馏数据集质量的方法。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Diffusion As Priors (DAP)方法，该方法通过以下步骤实现：在特征空间中使用Mercer核量化合成数据与真实数据之间的相似性，将代表性形式化；将此先验作为指导来引导反向扩散过程；增强蒸馏样本的代表性，无需任何重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在ImageNet-1K及其子集等大规模数据集上的大量实验表明，DAP在生成高保真度数据集方面优于最先进的方法；DAP实现了更好的跨架构泛化能力；该研究在扩散先验与数据集蒸馏目标之间建立了理论联系。&lt;h4&gt;结论&lt;/h4&gt;Diffusion As Priors (DAP)方法为提高数据集蒸馏质量提供了一个实用的、无需训练的框架。该研究不仅在扩散先验与数据集蒸馏目标之间建立了理论联系，还为解决数据集蒸馏中的代表性挑战提供了有效的方法，同时实现了多样性和泛化能力的平衡。&lt;h4&gt;翻译&lt;/h4&gt;数据集蒸馏旨在从大型数据集中合成紧凑而信息丰富的数据集。该领域的一个重大挑战是在单个蒸馏数据集中同时实现多样性、泛化能力和代表性。虽然最近的生成式数据集蒸馏方法采用了强大的扩散模型作为基础模型，但这些方法忽略了扩散模型中固有的代表性先验，因此需要集成外部约束来提高数据质量。为此，我们提出了Diffusion As Priors (DAP)，该方法通过在特征空间中使用Mercer核量化合成数据与真实数据之间的相似性，将代表性形式化。然后，我们将此先验作为指导来引导反向扩散过程，无需任何重新训练即可增强蒸馏样本的代表性。在ImageNet-1K及其子集等大规模数据集上的大量实验表明，DAP在生成高保真度数据集方面优于最先进的方法，同时实现了更好的跨架构泛化能力。我们的研究不仅在扩散先验与数据集蒸馏目标之间建立了理论联系，还为提高蒸馏数据集质量提供了一个实用的、无需训练的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset distillation aims to synthesize compact yet informative datasets fromlarge ones. A significant challenge in this field is achieving a trifecta ofdiversity, generalization, and representativeness in a single distilleddataset. Although recent generative dataset distillation methods adopt powerfuldiffusion models as their foundation models, the inherent representativenessprior in diffusion models is overlooked. Consequently, these approaches oftennecessitate the integration of external constraints to enhance data quality. Toaddress this, we propose Diffusion As Priors (DAP), which formalizesrepresentativeness by quantifying the similarity between synthetic and realdata in feature space using a Mercer kernel. We then introduce this prior asguidance to steer the reverse diffusion process, enhancing therepresentativeness of distilled samples without any retraining. Extensiveexperiments on large-scale datasets, such as ImageNet-1K and its subsets,demonstrate that DAP outperforms state-of-the-art methods in generatinghigh-fidelity datasets while achieving superior cross-architecturegeneralization. Our work not only establishes a theoretical connection betweendiffusion priors and the objectives of dataset distillation but also provides apractical, training-free framework for improving the quality of the distilleddataset.</description>
      <author>example@mail.com (Duo Su, Huyu Wu, Huanran Chen, Yiming Shi, Yuzhu Wang, Xi Ye, Jun Zhu)</author>
      <guid isPermaLink="false">2510.17421v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Monitoring Horses in Stalls: From Object to Event Detection</title>
      <link>http://arxiv.org/abs/2510.17409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于视觉的监控系统，可自动化检测和跟踪马厩中的马匹和人，用于早期发现健康和福利问题，减少人工监控的劳动强度。&lt;h4&gt;背景&lt;/h4&gt;监控拴马的行为对于早期发现健康和福利问题至关重要，但目前的监控方法仍然劳动密集且耗时。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于视觉的原型监控系统，自动化检测和跟踪马厩内的马匹和人，实现实时行为监控。&lt;h4&gt;方法&lt;/h4&gt;使用目标检测和多目标跟踪技术，系统利用YOLOv11和BoT-SORT进行检测和跟踪，基于物体轨迹和空间关系推断事件状态，构建了使用CLIP和GroundingDINO标注的自定义数据集，系统能区分五种事件类型并考虑相机盲点。&lt;h4&gt;主要发现&lt;/h4&gt;定性评估表明系统在马匹相关事件检测方面表现可靠，但由于数据稀缺，在检测人方面存在局限性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为马匹设施的实时行为监控提供了基础，对动物福利和马厩管理有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;监控拴马的行为对于早期发现健康和福利问题至关重要，但仍然劳动密集且耗时。在本研究中，我们提出了一个基于视觉的原型监控系统，使用目标检测和多目标跟踪技术自动化检测和跟踪马厩内的马匹和人。系统利用YOLOv11和BoT-SORT进行检测和跟踪，同时基于马厩内物体的轨迹和空间关系推断事件状态。为支持开发，我们构建了一个使用基础模型CLIP和GroundingDINO协助标注的自定义数据集。系统能区分五种事件类型并考虑相机的盲点。定性评估表明系统在马匹相关事件检测方面表现可靠，同时指出由于数据稀缺，在检测人方面存在局限性。这项工作为马匹设施的实时行为监控提供了基础，对动物福利和马厩管理有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monitoring the behavior of stalled horses is essential for early detection ofhealth and welfare issues but remains labor-intensive and time-consuming. Inthis study, we present a prototype vision-based monitoring system thatautomates the detection and tracking of horses and people inside stables usingobject detection and multi-object tracking techniques. The system leveragesYOLOv11 and BoT-SORT for detection and tracking, while event states areinferred based on object trajectories and spatial relations within the stall.To support development, we constructed a custom dataset annotated withassistance from foundation models CLIP and GroundingDINO. The systemdistinguishes between five event types and accounts for the camera's blindspots. Qualitative evaluation demonstrated reliable performance forhorse-related events, while highlighting limitations in detecting people due todata scarcity. This work provides a foundation for real-time behavioralmonitoring in equine facilities, with implications for animal welfare andstable management.</description>
      <author>example@mail.com (Dmitrii Galimzianov, Viacheslav Vyshegorodtsev, Ivan Nezhivykh)</author>
      <guid isPermaLink="false">2510.17409v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine</title>
      <link>http://arxiv.org/abs/2510.17402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Ladder-base，首个使用组相对策略优化(GRPO)训练的中医领域大语言模型，在多个推理指标上表现优于通用大语言模型和特定中医模型。&lt;h4&gt;背景&lt;/h4&gt;中医拥有丰富且结构独特的知识体系，这对常规大语言模型的应用提出了挑战。虽然之前的中医特定LLM通过监督微调取得进展，但它们在对齐、数据质量和评估一致性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对中医领域的大语言模型，解决现有模型在一致性、数据质量和评估一致性方面的局限性，提高模型在中医领域的推理能力和事实一致性。&lt;h4&gt;方法&lt;/h4&gt;使用组相对策略优化(GRPO)强化学习方法训练Ladder-base模型，该方法通过基于组内比较优化响应选择来提高推理和事实一致性。模型基于Qwen2.5-7B-Instruct构建，在中医阶梯基准的文本子集上训练，使用80%数据训练，剩余20%平均分为验证和测试集。&lt;h4&gt;主要发现&lt;/h4&gt;通过标准化评估，Ladder-base在多个推理指标上表现优于GPT-4、Gemini 2.5、Claude 3、Qwen3等通用大语言模型，以及BenTsao、HuatuoGPT2、Zhongjing等特定中医模型。&lt;h4&gt;结论&lt;/h4&gt;GRPO为将大语言模型与中医领域专家级推理对齐提供了有效且高效的策略，支持开发可信且临床基础的中医人工智能系统。&lt;h4&gt;翻译&lt;/h4&gt;传统中医呈现了一个丰富且结构独特的知识体系，这对常规大语言模型的应用提出了挑战。尽管之前的中医特定LLM通过监督微调已经显示出进展，但它们常常在对齐、数据质量和评估一致性方面面临局限性。在本研究中，我们引入了Ladder-base，这是第一个使用组相对策略优化训练的中医领域LLM，这是一种通过基于组内比较优化响应选择来提高推理和事实一致性的强化学习方法。Ladder-base基于Qwen2.5-7B-Instruct基础模型构建，并仅在中医阶梯基准的文本子集上进行训练，使用80%的数据进行训练，剩余的20%平均分为验证集和测试集。通过标准化评估，Ladder-base在多个推理指标上表现出优于最先进的通用LLM和特定中医模型的性能。这些发现表明，GRPO为将LLM与中医领域专家级推理对齐提供了一种有效且高效的策略，支持开发可信且临床基础的中医人工智能系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Chinese Medicine (TCM) presents a rich and structurally uniqueknowledge system that challenges conventional applications of large languagemodels (LLMs). Although previous TCM-specific LLMs have shown progress throughsupervised fine-tuning, they often face limitations in alignment, data quality,and evaluation consistency. In this study, we introduce Ladder-base, the firstTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), areinforcement learning method that improves reasoning and factual consistencyby optimizing response selection based on intra-group comparisons. Ladder-baseis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusivelyon the textual subset of the TCM-Ladder benchmark, using 80 percent of the datafor training and the remaining 20 percent split evenly between validation andtest sets. Through standardized evaluation, Ladder-base demonstrates superiorperformance across multiple reasoning metrics when compared to bothstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, andQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, andZhongjing. These findings suggest that GRPO provides an effective and efficientstrategy for aligning LLMs with expert-level reasoning in traditional medicaldomains and supports the development of trustworthy and clinically grounded TCMartificial intelligence systems.</description>
      <author>example@mail.com (Jiacheng Xie, Shuai Zeng, Yang Yu, Xiaoting Tang, Guanghui An, Dong Xu)</author>
      <guid isPermaLink="false">2510.17402v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Exploring The Missing Semantics In Event Modality</title>
      <link>http://arxiv.org/abs/2510.17347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Semantic-E2VID框架，通过探索事件模态中缺失的视觉语义知识并利用其增强事件到视频重建，解决了事件相机无法捕捉静态物体和背景导致的语义信息缺失问题。&lt;h4&gt;背景&lt;/h4&gt;事件相机具有低延迟、高动态范围和高效运动捕捉等优势，但事件到视频重建任务面临重建和恢复语义信息的挑战。事件相机只捕捉强度变化，忽略静态物体和背景，导致捕获的事件模态中缺乏语义信息。&lt;h4&gt;目的&lt;/h4&gt;提出Semantic-E2VID框架，探索事件模态中缺失的视觉语义知识并利用其增强事件到视频重建。&lt;h4&gt;方法&lt;/h4&gt;引入跨模态特征对齐(CFA)模块将SAM模型的鲁棒视觉语义传输到事件编码器；提出语义感知特征融合(SFF)块整合学习到的语义信息；提出语义感知E2V监督利用SAM生成的类别标签帮助重建语义细节。&lt;h4&gt;主要发现&lt;/h4&gt;Semantic-E2VID显著提高了帧质量，在多个基准测试中优于最先进的E2V方法。&lt;h4&gt;结论&lt;/h4&gt;Semantic-E2VID有效解决了事件模态中语义信息缺失的问题，通过跨模态特征对齐和语义感知特征融合提升了事件到视频重建的质量。&lt;h4&gt;翻译&lt;/h4&gt;事件相机提供低延迟、高动态范围和高效运动捕捉等独特优势。然而，作为基础事件视觉任务的事件到视频重建(E2V)仍然具有挑战性，特别是在重建和恢复语义信息方面。这主要源于事件相机的本质，因为它只捕捉强度变化，忽略静态物体和背景，导致捕获的事件模态中缺乏语义信息。此外，语义信息在视频和帧重建中起着关键作用，但现有的E2V方法常常忽略了这一点。为了弥合这一差距，我们提出了Semantic-E2VID，这是一个E2V框架，探索事件模态中缺失的视觉语义知识，并利用它来增强事件到视频的重建。具体来说，Semantic-E2VID引入了跨模态特征对齐(CFA)模块，将基于帧的视觉基础模型(Segment Anything Model, SAM)的鲁棒视觉语义传输到事件编码器，同时对齐来自不同模态的高级特征。为了更好地利用学习到的语义特征，我们进一步提出了一个语义感知特征融合(SFF)块，将学习到的帧模态语义整合到事件表示中，形成具有丰富语义的事件表示，可被事件解码器解码。此外，为了促进语义信息的重建，我们提出了一种新颖的语义感知E2V监督，它通过利用SAM生成的类别标签帮助模型重建语义细节。大量实验证明，Semantic-E2VID显著提高了帧质量，在多个基准测试中优于最先进的E2V方法。示例代码包含在补充材料中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras offer distinct advantages such as low latency, high dynamicrange, and efficient motion capture. However, event-to-video reconstruction(E2V), a fundamental event-based vision task, remains challenging, particularlyfor reconstructing and recovering semantic information. This is primarily dueto the nature of the event camera, as it only captures intensity changes,ignoring static objects and backgrounds, resulting in a lack of semanticinformation in captured event modality. Further, semantic information plays acrucial role in video and frame reconstruction, yet is often overlooked byexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2Vframework that explores the missing visual semantic knowledge in event modalityand leverages it to enhance event-to-video reconstruction. Specifically,Semantic-E2VID introduces a cross-modal feature alignment (CFA) module totransfer the robust visual semantics from a frame-based vision foundationmodel, the Segment Anything Model (SAM), to the event encoder, while aligningthe high-level features from distinct modalities. To better utilize the learnedsemantic feature, we further propose a semantic-aware feature fusion (SFF)block to integrate learned semantics in frame modality to form eventrepresentations with rich semantics that can be decoded by the event decoder.Further, to facilitate the reconstruction of semantic information, we propose anovel Semantic Perceptual E2V Supervision that helps the model to reconstructsemantic details by leveraging SAM-generated categorical labels. Extensiveexperiments demonstrate that Semantic-E2VID significantly enhances framequality, outperforming state-of-the-art E2V methods across multiple benchmarks.The sample code is included in the supplementary material.</description>
      <author>example@mail.com (Jingqian Wu, Shengpeng Xu, Yunbo Jia, Edmund Y. Lam)</author>
      <guid isPermaLink="false">2510.17347v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Trading with the Devil: Risk and Return in Foundation Model Strategies</title>
      <link>http://arxiv.org/abs/2510.17165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种扩展的资本资产定价模型(CAPM)，用于分离基础模型引入的系统风险和特定微调带来的特定风险，帮助金融从业者更好地理解和评估基于基础模型的交易策略风险状况。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在自然语言处理等领域产生变革性影响，现开始应用于金融时间序列任务。这些预训练架构虽能提供多样化预测信号，但如何影响交易策略风险状况尚不明确，导致实践者不愿投入大量资本。&lt;h4&gt;目的&lt;/h4&gt;扩展资本资产定价模型，分离基础模型引入的系统风险（可能产生alpha）与特定微调带来的特定风险（通常不积累系统性溢价），并开发实用方法估计这些风险。&lt;h4&gt;方法&lt;/h4&gt;将风险分解与不确定性解耦概念对齐，将系统性风险视为本体不确定性，特定风险视为偶然不确定性。在偶然崩溃假设下，使用蒙特卡洛dropout等方法直接测量本体风险，将交易策略映射到更透明的风险-回报平面。&lt;h4&gt;主要发现&lt;/h4&gt;分离不同风险因素可更深入了解基于基础模型策略的性能限制、模型随时间的退化情况，以及有针对性的改进途径。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了在竞争性金融市场部署大型预训练模型的希望和陷阱，为金融从业者提供了更全面的风险评估框架。&lt;h4&gt;翻译&lt;/h4&gt;基础模型-已在自然语言处理等领域产生变革性影响-现正开始出现在金融时间序列任务中。虽然这些预训练架构承诺提供多样化的预测信号，但人们对其如何塑造构建于其上的交易策略的风险状况知之甚少，导致实践者不愿投入大量资本。在本文中，我们提出对资本资产定价模型(CAPM)的扩展，该模型分离了共享基础模型引入的系统风险-如果底层模型真正具有预测能力，则可能产生alpha-以及归因于自定义微调的特定风险，后者通常不积累系统性溢价。为了能够实际估计这些独立风险，我们将这种分解与不确定性解耦的概念对齐，将系统性风险视为本体不确定性（源于预训练模型），将特定风险视为偶然不确定性（在自定义适应过程中引入）。在偶然崩溃假设下，我们说明了如何使用蒙特卡洛dropout-以及其他不确定性量化工具包中的方法-直接测量本体风险，从而将交易策略映射到更透明的风险-回报平面。我们的实验表明，分离这些不同的风险因素可以更深入地了解基于基础模型的策略的性能限制、其随时间的模型退化情况，以及有针对性的改进途径。总的来说，我们的结果突出了在竞争性金融市场部署大型预训练模型的希望和陷阱。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models - already transformative in domains such as naturallanguage processing - are now starting to emerge for time-series tasks infinance. While these pretrained architectures promise versatile predictivesignals, little is known about how they shape the risk profiles of the tradingstrategies built atop them, leaving practitioners reluctant to commit seriouscapital. In this paper, we propose an extension to the Capital Asset PricingModel (CAPM) that disentangles the systematic risk introduced by a sharedfoundation model - potentially capable of generating alpha if the underlyingmodel is genuinely predictive - from the idiosyncratic risk attributable tocustom fine-tuning, which typically accrues no systematic premium. To enable apractical estimation of these separate risks, we align this decomposition withthe concepts of uncertainty disentanglement, casting systematic risk asepistemic uncertainty (rooted in the pretrained model) and idiosyncratic riskas aleatory uncertainty (introduced during custom adaptations). Under theAleatory Collapse Assumption, we illustrate how Monte Carlo dropout - amongother methods in the uncertainty-quantization toolkit - can directly measurethe epistemic risk, thereby mapping trading strategies to a more transparentrisk-return plane. Our experiments show that isolating these distinct riskfactors yields deeper insights into the performance limits offoundation-model-based strategies, their model degradation over time, andpotential avenues for targeted refinements. Taken together, our resultshighlight both the promise and the pitfalls of deploying large pretrainedmodels in competitive financial markets.</description>
      <author>example@mail.com (Jinrui Zhang)</author>
      <guid isPermaLink="false">2510.17165v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework</title>
      <link>http://arxiv.org/abs/2510.17163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型基础模型正在改变软件工程领域，但缺乏全面的可信度评估方法。研究团队提出了TREAT评估框架，通过多任务、多语言多模态、鲁棒性和严格评估方法四个改进点，对26个先进模型进行了评估，发现了模型在编程任务上的性能差异和多模态模型在UI代码生成方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型正在从根本上改变软件工程领域，在代码生成、调试和测试等任务上表现出色。然而，如何全面评估这些模型在真实软件工程场景中的可信度仍存在显著差距。现有基准测试存在任务范围有限、未包含模型鲁棒性和可靠性等关键评估方面的问题。&lt;h4&gt;目的&lt;/h4&gt;填补现有评估方法的不足，提供一个全面的模型性能评估框架，以评估大型基础模型在软件工程任务中的可信度和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出名为TREAT（Code LLMs Trustworthiness/Reliability Evaluation And Testing）的评估框架，包含四个主要改进：1) 多任务全面评估，涵盖多样化的软件工程活动；2) 多语言和多模态评估，包含多模态编码任务；3) 鲁棒性评估，评估模型在语义保持代码转换下的可靠性；4) 严格的评估方法，通过多样化的评估提示和自适应解决方案提取提高评估结果的可信度。基于此框架评估了26个最先进的模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 当前模型在编程任务上表现出显著的性能差异；2) 多模态语言模型在UI代码生成和编辑方面表现出特定的性能局限性。&lt;h4&gt;结论&lt;/h4&gt;TREAT框架为评估大型基础模型在软件工程任务中的可信度和可靠性提供了更全面的方法，有助于识别模型的优势和局限性，指导未来的模型改进方向。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型正在从根本上改变软件工程领域，在代码生成、调试和测试等多样化任务上表现出色。尽管进展迅速，但在如何全面评估这些模型在真实软件工程场景中的可信度方面仍存在显著差距。现有基准测试存在任务范围有限，未能纳入模型的鲁棒性和可靠性等关键评估方面的问题。为填补这一差距，我们提出了一个名为TREAT（Code LLMs Trustworthiness/Reliability Evaluation And Testing）的评估框架，该框架提供对模型在代码智能任务中性能的全面评估。我们的评估框架通过四个主要改进解决了现有方法的关键局限性：(1) 多任务全面评估，涵盖多样化的软件工程活动，而非有限的编码任务；(2) 多语言和多模态评估，超越传统的单语言、纯文本基准，包含多模态编码任务；(3) 鲁棒性评估，评估模型在语义保持代码转换下的可靠性；(4) 严格的评估方法，通过多样化的评估提示和自适应解决方案提取提高评估结果的可信度。基于此评估框架，我们评估了26个最先进的模型，发现了它们的优势和局限性，得出了几个关键见解：(1) 当前模型在编程任务上表现出显著的性能差异；(2) 多模态语言模型在UI代码生成和编辑方面表现出特定的性能局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large foundation models are fundamentally transforming the softwareengineering landscape, demonstrating exceptional capabilities across diversetasks such as code generation, debugging, and testing. Despite this rapidprogress, a significant gap remains in how to comprehensively evaluate thesemodels' trustworthiness in real-world software engineering scenarios. Existingbenchmarks suffer from limited task scope and fail to incorporate criticalevaluation aspects such as the robustness and reliability of models. To bridgethis gap, we present an evaluation framework called TREAT (Code LLMsTrustworthiness / Reliability Evaluation And Testing) that provides a holisticassessment of model performance in code intelligence tasks. Our evaluationframework addresses key limitations in existing approaches with four mainimprovements: (1) Multi-Task Holistic Evaluation that spans diverse softwareengineering activities rather than limited coding tasks; (2) Multi-Language andMulti-Modality Assessment that extends beyond traditional single-language,text-only benchmarks to include multi-modality coding tasks; (3) RobustnessAssessment that evaluates model reliability under semantically-preserving codetransformations; and (4) Rigorous Evaluation Methodology that enhances thetrustworthiness of evaluation results through diverse evaluation prompts andadaptive solution extraction. Based on this evaluation framework, we assess 26state-of-the-art models and uncover both their strengths and limitations,yielding several key insights:(1) Current models show substantial performancevariation across programming tasks; (2) Multi-modal language models demonstratespecific performance limitations in UI code generation and edit;</description>
      <author>example@mail.com (Shuzheng Gao, Eric John Li, Man Ho Lam, Jingyu Xiao, Yuxuan Wan, Chaozheng Wang, Ng Man Tik, Michael R. Lyu)</author>
      <guid isPermaLink="false">2510.17163v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Do Satellite Tasks Need Special Pretraining?</title>
      <link>http://arxiv.org/abs/2510.17014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究挑战了特定遥感基础模型比通用视觉基础模型更有用的观点，特别是在小规模应用中。作者设计了一个评估模型对低分辨率图像泛化能力的基准，并在卫星图像数据集上训练了iBOT模型，但发现没有预训练模型能比通用基线带来一致改进。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在多种模态中推动了机器学习的发展，最近多个团队训练了专门用于遥感应用的基础模型。这一研究方向受到遥感图像的独特特性、特定应用以及对卫星图像分析有用的鲁棒性类型的驱动。&lt;h4&gt;目的&lt;/h4&gt;系统性地挑战特定基础模型比通用视觉基础模型更有用的观点，至少在小规模情况下。&lt;h4&gt;方法&lt;/h4&gt;设计了一个简单的基准来衡量遥感模型对较低分辨率图像的泛化能力；在MillionAID（ImageNet规模的卫星图像数据集）上训练了iBOT（自监督视觉编码器），并进行了针对遥感的若干修改。&lt;h4&gt;主要发现&lt;/h4&gt;在ViT-B规模下，没有一个预训练模型能比通用基线带来一致的改进。&lt;h4&gt;结论&lt;/h4&gt;特定基础模型在小规模应用中并不比通用视觉基础模型更有优势。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已在各种模态中推动了机器学习的发展，包括图像。最近，多个团队训练了专门用于遥感应用的基础模型。这一研究方向受到遥感图像的独特特性、特定应用以及对卫星图像分析有用的鲁棒性类型的驱动。在这项工作中，我们系统地挑战了特定基础模型比通用视觉基础模型更有用的观点，至少在小规模情况下。首先，我们设计了一个简单的基准，用于衡量遥感模型在两个下游任务中对较低分辨率图像的泛化能力。其次，我们在MillionAID（一个ImageNet规模的卫星图像数据集）上训练了iBOT（一种自监督视觉编码器），并进行了针对遥感的若干修改。我们表明，在ViT-B规模下，这些预训练模型中没有哪一个比通用基线带来一致的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have advanced machine learning across various modalities,including images. Recently multiple teams trained foundation models specializedfor remote sensing applications. This line of research is motivated by thedistinct characteristics of remote sensing imagery, specific applications andtypes of robustness useful for satellite image analysis. In this work wesystematically challenge the idea that specific foundation models are moreuseful than general-purpose vision foundation models, at least in the smallscale. First, we design a simple benchmark that measures generalization ofremote sensing models towards images with lower resolution for two downstreamtasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,an ImageNet-scale satellite imagery dataset, with several modificationsspecific to remote sensing. We show that none of those pretrained models bringconsistent improvements upon general-purpose baselines at the ViT-B scale.</description>
      <author>example@mail.com (Ani Vanyan, Alvard Barseghyan, Hakob Tamazyan, Tigran Galstyan, Vahan Huroyan, Naira Hovakimyan, Hrant Khachatrian)</author>
      <guid isPermaLink="false">2510.17014v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Graph4MM: Weaving Multimodal Learning with Structural Information</title>
      <link>http://arxiv.org/abs/2510.16990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Graph4MM，一个基于图的多模态学习框架，通过Hop-Diffused Attention和MM-QFormer解决了多模态学习中的两个关键挑战：整合多跳邻居结构信息和融合模态特定信息。实验表明该方法显著优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;现实世界多模态数据具有复杂结构关系，跨模态实体通过上下文依赖和共指关系形成多样连接。图为建模模态内和模态间关系提供强大结构信息，但先前工作未能区分多跳邻居并将图视为独立模态，导致理解碎片化。&lt;h4&gt;目的&lt;/h4&gt;解决多模态学习中的两个关键挑战：(1)将多跳邻居的结构信息整合到基础模型中，(2)以原则性的方式融合模态特定信息。重新审视图在基础模型时代多模态学习中的作用。&lt;h4&gt;方法&lt;/h4&gt;提出Graph4MM框架，包含Hop-Diffused Attention（通过因果掩蔽和跳扩散将多跳结构信息整合到自注意力中）和MM-QFormer（用于跨模态融合的多映射查询transformer）。&lt;h4&gt;主要发现&lt;/h4&gt;利用结构整合模态内和模态间交互比将它们视为独立模态能更好地提升多模态理解。在生成性和判别性任务上，Graph4MM优于更大的VLMs、LLMs和多模态图基线，平均实现6.93%的改进。&lt;h4&gt;结论&lt;/h4&gt;Graph4MM框架有效解决了多模态学习中的关键挑战，通过整合多跳结构信息和跨模态融合，显著提升了多模态理解能力。&lt;h4&gt;翻译&lt;/h4&gt;现实世界多模态数据通常表现出超越传统图像-标题对等一对一映射的复杂结构关系。跨模态的实体以复杂的方式交互，图像和文本通过上下文依赖和共指关系形成多样的相互连接。图为建模模态内和模态间关系提供了强大的结构信息。然而，先前的工作未能区分多跳邻居，而是将图视为独立模态，这碎片化了整体理解。这一局限性给多模态学习带来了两个关键挑战：(1)将多跳邻居的结构信息整合到基础模型中，(2)以原则性的方式融合模态特定信息。为应对这些挑战，我们重新审视了基础模型时代图在多模态学习中的作用，并提出了Graph4MM，一个基于图的多模态学习框架。具体而言，我们引入了Hop-Diffused Attention，通过因果掩蔽和跳扩散将多跳结构信息整合到自注意力中。此外，我们设计了MM-QFormer，一个用于跨模态融合的多映射查询transformer。通过理论和经验分析，我们表明利用结构整合模态内和模态间交互，比将它们视为独立模态能更好地提升多模态理解。在生成性和判别性任务上的实验表明，Graph4MM优于更大的VLMs、LLMs和多模态图基线，实现了6.93%的平均改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world multimodal data usually exhibit complex structural relationshipsbeyond traditional one-to-one mappings like image-caption pairs. Entitiesacross modalities interact in intricate ways, with images and text formingdiverse interconnections through contextual dependencies and co-references.Graphs provide powerful structural information for modeling intra-modal andinter-modal relationships. However, previous works fail to distinguishmulti-hop neighbors and treat the graph as a standalone modality, whichfragments the overall understanding. This limitation presents two keychallenges in multimodal learning: (1) integrating structural information frommulti-hop neighbors into foundational models, and (2) fusing modality-specificinformation in a principled manner. To address these challenges, we revisit therole of graphs in multimodal learning within the era of foundation models andpropose Graph4MM, a graph-based multimodal learning framework. To be specific,we introduce Hop-Diffused Attention, which integrates multi-hop structuralinformation into self-attention through causal masking and hop diffusion.Furthermore, we design MM-QFormer, a multi-mapping querying transformer forcross-modal fusion. Through theoretical and empirical analysis, we show thatleveraging structures to integrate both intra- and inter-modal interactionsimproves multimodal understanding beyond treating them as a standalonemodality. Experiments on both generative and discriminative tasks show thatGraph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,achieving a 6.93% average improvement.</description>
      <author>example@mail.com (Xuying Ning, Dongqi Fu, Tianxin Wei, Wujiang Xu, Jingrui He)</author>
      <guid isPermaLink="false">2510.16990v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis</title>
      <link>http://arxiv.org/abs/2510.16973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述文章对医学图像分析中的基础模型(FMs)进行了全面和结构化的分析，系统性地分类了研究进展并评估了其临床应用价值。&lt;h4&gt;背景&lt;/h4&gt;人工智能特别是基础模型的最新进展彻底改变了医学图像分析，在多种医学影像任务中表现出强大的零样本和少样本性能。与传统特定任务AI模型不同，基础模型利用大量标记和非标记的多模态数据集学习通用表示，可通过微调适应各种下游临床应用。&lt;h4&gt;目的&lt;/h4&gt;弥补医学影像领域基础模型研究的碎片化现状，提供一个统一的综合分析，系统性地映射不同模态下架构、训练范式和临床应用的演变。&lt;h4&gt;方法&lt;/h4&gt;将研究按架构基础、训练策略和下游临床任务分为纯视觉基础模型和视觉语言基础模型；进行定量元分析，描述数据集利用和应用领域的时间趋势；批判性地讨论持续存在的挑战和新出现的解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可通过微调适应各种临床应用；持续存在的挑战包括领域适应、高效微调、计算限制和可解释性；新兴解决方案包括联邦学习、知识蒸馏和高级提示技术。&lt;h4&gt;结论&lt;/h4&gt;需要加强基础模型的鲁棒性、可解释性和临床集成研究，以加速这些模型转化为实际医疗实践。&lt;h4&gt;翻译&lt;/h4&gt;人工智能(AI)特别是基础模型(FMs)的最新进展彻底改变了医学图像分析，在从分割到报告生成的多种医学影像任务中表现出强大的零样本和少样本性能。与传统的特定任务AI模型不同，基础模型利用大量标记和非标记的多模态数据集学习通用表示，这些通用表示可以通过微调适应各种下游临床应用。然而，尽管医学影像中基础模型研究迅速增长，该领域仍然碎片化，缺乏一个统一的综合分析来系统性地映射不同模态下架构、训练范式和临床应用的演变。为解决这一差距，这篇综述文章对医学图像分析中的基础模型提供了全面和结构化的分析。我们根据架构基础、训练策略和下游临床任务将研究系统性地分为纯视觉基础模型和视觉语言基础模型。此外，还对研究进行了定量元分析，以描述数据集利用和应用领域的时间趋势。我们还批判性地讨论了持续存在的挑战，包括领域适应、高效微调、计算限制和可解释性，以及新兴的解决方案，如联邦学习、知识蒸馏和高级提示技术。最后，我们确定了旨在增强基础模型的鲁棒性、可解释性和临床集成的关键未来研究方向，从而加速它们转化为实际医疗实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in artificial intelligence (AI), particularly foundationmodels (FMs), have revolutionized medical image analysis, demonstrating strongzero- and few-shot performance across diverse medical imaging tasks, fromsegmentation to report generation. Unlike traditional task-specific AI models,FMs leverage large corpora of labeled and unlabeled multimodal datasets tolearn generalized representations that can be adapted to various downstreamclinical applications with minimal fine-tuning. However, despite the rapidproliferation of FM research in medical imaging, the field remains fragmented,lacking a unified synthesis that systematically maps the evolution ofarchitectures, training paradigms, and clinical applications across modalities.To address this gap, this review article provides a comprehensive andstructured analysis of FMs in medical image analysis. We systematicallycategorize studies into vision-only and vision-language FMs based on theirarchitectural foundations, training strategies, and downstream clinical tasks.Additionally, a quantitative meta-analysis of the studies was conducted tocharacterize temporal trends in dataset utilization and application domains. Wealso critically discuss persistent challenges, including domain adaptation,efficient fine-tuning, computational constraints, and interpretability alongwith emerging solutions such as federated learning, knowledge distillation, andadvanced prompting. Finally, we identify key future research directions aimedat enhancing the robustness, explainability, and clinical integration of FMs,thereby accelerating their translation into real-world medical practice.</description>
      <author>example@mail.com (Praveenbalaji Rajendran, Mojtaba Safari, Wenfeng He, Mingzhe Hu, Shansong Wang, Jun Zhou, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2510.16973v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Chem-R: Learning to Reason as a Chemist</title>
      <link>http://arxiv.org/abs/2510.16880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, 14 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Chem-R是一个通用的化学推理模型，通过三阶段训练框架实现先进化学推理能力，在综合基准测试上取得最先进性能，超越现有模型。&lt;h4&gt;背景&lt;/h4&gt;当前大语言模型在化学发现方面缺乏核心化学知识，推理轨迹不可靠，且在各类化学任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决现有大语言模型在化学领域的局限性，开发一个能模拟化学家深思熟虑过程的通用化学推理模型。&lt;h4&gt;方法&lt;/h4&gt;通过三阶段框架训练：1)化学基础训练建立核心知识；2)化学推理协议蒸馏融入结构化专家推理轨迹；3)多任务组相对策略优化模型在分子和反应级任务上的平衡性能。&lt;h4&gt;主要发现&lt;/h4&gt;Chem-R在综合基准测试上取得最先进性能，超越Gemini-2.5-Pro和DeepSeek-R1等领先模型，在分子任务上领先最多46%，在反应任务上领先最多66%，且在分子和反应级任务上都优于现有化学基础模型。&lt;h4&gt;结论&lt;/h4&gt;Chem-R具有强大的泛化能力和可解释性，有望成为下一代AI驱动化学发现的基础。&lt;h4&gt;翻译&lt;/h4&gt;尽管大型语言模型在化学发现方面具有巨大潜力，但当前模型缺乏核心化学知识，产生不可靠的推理轨迹，并在各种化学任务中表现不佳。为解决这些挑战，我们提出了Chem-R，一个通用的化学推理模型，旨在模拟化学家的深思熟虑过程。Chem-R通过三阶段框架进行训练，逐步构建高级推理能力：1)化学基础训练，建立核心化学知识；2)化学推理协议蒸馏，融入结构化、专家式的推理轨迹，引导系统化和可靠的问题解决；3)多任务组相对策略优化，优化模型在多样化的分子级和反应级任务上的平衡性能。这种结构化管道使Chem-R能够在综合基准测试上实现最先进的性能，超越包括Gemini-2.5-Pro和DeepSeek-R1在内的领先大型语言模型，在分子任务上领先高达46%，在反应任务上领先高达66%。同时，Chem-R在分子和反应级任务上也 consistently 优于现有的化学基础模型。这些结果突显了Chem-R的强大泛化能力、可解释性以及作为下一代AI驱动化学发现基础的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although large language models (LLMs) have significant potential to advancechemical discovery, current LLMs lack core chemical knowledge, produceunreliable reasoning trajectories, and exhibit suboptimal performance acrossdiverse chemical tasks. To address these challenges, we propose Chem-R, ageneralizable Chemical Reasoning model designed to emulate the deliberativeprocesses of chemists. Chem-R is trained through a three-phase framework thatprogressively builds advanced reasoning capabilities, including: 1) ChemicalFoundation Training, which establishes core chemical knowledge. 2) ChemicalReasoning Protocol Distillation, incorporating structured, expert-likereasoning traces to guide systematic and reliable problem solving. 3)Multi-task Group Relative Policy Optimization that optimizes the model forbalanced performance across diverse molecular- and reaction-level tasks. Thisstructured pipeline enables Chem-R to achieve state-of-the-art performance oncomprehensive benchmarks, surpassing leading large language models, includingGemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% onreaction tasks. Meanwhile, Chem-R also consistently outperforms the existingchemical foundation models across both molecular and reaction level tasks.These results highlight Chem-R's robust generalization, interpretability, andpotential as a foundation for next-generation AI-driven chemical discovery.</description>
      <author>example@mail.com (Weida Wang, Benteng Chen, Di Zhang, Wanhao Liu, Shuchen Pu, Ben Gao, Jin Zeng, Lei Bai, Wanli Ouyang, Xiaoyong Wei, Tianshu Yu, Tianfan Fu, Shuzhou Sun, Jiatong Li, Zifu Wang, Yuqiang Li, Shufei Zhang)</author>
      <guid isPermaLink="false">2510.16880v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation</title>
      <link>http://arxiv.org/abs/2510.16776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EMRRG的新型X光报告生成框架，该框架使用参数高效方法微调预训练的Mamba网络，结合具有混合解码器的LLM生成医学报告，在基准数据集上取得了良好效果。&lt;h4&gt;背景&lt;/h4&gt;X光图像医学报告生成(MRG)是人工智能的重要领域，可以显著减轻临床医生的诊断负担和患者等待时间。现有MRG模型主要依赖大型语言模型，对预训练视觉基础模型或高级微调技术的探索有限，且非Transformer架构在医学报告生成中研究不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的X光报告生成框架EMRRG，使用参数高效方法微调预训练的Mamba网络，探索非Transformer架构在医学报告生成中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;将X光图像分割成块并进行标记化处理，通过基于SSM的视觉主干网络进行特征提取，使用部分LoRA技术获得最佳性能。采用具有混合解码器的LLM生成医学报告，实现端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;在三个广泛使用的基准数据集上的大量实验验证了所提出策略在X光医学报告生成中的有效性。&lt;h4&gt;结论&lt;/h4&gt;EMRRG框架是一种有效的X光医学报告生成方法，结合了Mamba网络和参数高效微调技术，为医学报告生成领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray image-based medical report generation (MRG) is a pivotal area inartificial intelligence that can significantly reduce diagnostic burdens forclinicians and patient wait times. Existing MRG models predominantly rely onLarge Language Models (LLMs) to improve report generation, with limitedexploration of pre-trained vision foundation models or advanced fine-tuningtechniques. Mainstream frameworks either avoid fine-tuning or utilizesimplistic methods like LoRA, often neglecting the potential of enhancingcross-attention mechanisms. Additionally, while Transformer-based modelsdominate vision-language tasks, non-Transformer architectures, such as theMamba network, remain underexplored for medical report generation, presenting apromising avenue for future research. In this paper, we propose EMRRG, a novelX-ray report generation framework that fine-tunes pre-trained Mamba networksusing parameter-efficient methods. Specifically, X-ray images are divided intopatches, tokenized, and processed by an SSM-based vision backbone for featureextraction, with Partial LoRA yielding optimal performance. An LLM with ahybrid decoder generates the medical report, enabling end-to-end training andachieving strong results on benchmark datasets. Extensive experiments on threewidely used benchmark datasets fully validated the effectiveness of ourproposed strategies for the X-ray MRG. The source code of this paper will bereleased on https://github.com/Event-AHU/Medical_Image_Analysis.</description>
      <author>example@mail.com (Mingzheng Zhang, Jinfeng Gao, Dan Xu, Jiangrui Yu, Yuhan Qiao, Lan Chen, Jin Tang, Xiao Wang)</author>
      <guid isPermaLink="false">2510.16776v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge</title>
      <link>http://arxiv.org/abs/2510.16716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DistilLock是一个TEE辅助的微调框架，能够在边缘设备上实现隐私保护的知识蒸馏，同时保护数据隐私和模型知识产权。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在各种任务上表现出色，但微调通常依赖于基于云的集中式基础设施，需要数据所有者上传敏感数据，引发隐私问题；而在边缘设备上直接微调则存在模型知识产权泄露风险。&lt;h4&gt;目的&lt;/h4&gt;解决在保护数据隐私和模型知识产权的同时，在边缘设备上高效微调大型语言模型的困境。&lt;h4&gt;方法&lt;/h4&gt;提出DistilLock框架，在TEE中执行专有基础模型作为安全黑盒教师，并采用模型模糊化机制将模糊化权重卸载到不可信加速器上进行知识蒸馏。&lt;h4&gt;主要发现&lt;/h4&gt;DistilLock能够防止未授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率。&lt;h4&gt;结论&lt;/h4&gt;DistilLock为基于边缘的LLM个性化提供了一种安全且实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已在各种任务上展现出强大的性能，但对其进行微调通常依赖于基于云的集中式基础设施。这需要数据所有者将可能敏感的数据上传到外部服务器，引发严重的隐私问题。另一种替代方法是在边缘设备上使用本地数据直接微调LLMs；然而，这带来了新的挑战：模型所有者必须将专有模型传输到边缘设备，这存在知识产权泄露的风险。为了解决这一困境，我们提出了DistilLock，一个TEE辅助的微调框架，能够在边缘上实现隐私保护的知识蒸馏。在DistilLock中，专有基础模型在数据所有者设备上的可信执行环境enclave中执行，充当安全的黑盒教师。这种设置通过防止直接访问模型内部，既保护了数据隐私又保护了模型IP。此外，DistilLock采用模型模糊化机制，将模糊化的权重卸载到不可信的加速器上，以实现高效的知识蒸馏而不损害安全性。我们证明，DistilLock能够防止未授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率，为基于边缘的LLM个性化提供了一种安全且实用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated strong performance acrossdiverse tasks, but fine-tuning them typically relies on cloud-based,centralized infrastructures. This requires data owners to upload potentiallysensitive data to external servers, raising serious privacy concerns. Analternative approach is to fine-tune LLMs directly on edge devices using localdata; however, this introduces a new challenge: the model owner must transferproprietary models to the edge, which risks intellectual property (IP) leakage.To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuningframework that enables privacy-preserving knowledge distillation on the edge.In DistilLock, a proprietary foundation model is executed within a trustedexecution environment (TEE) enclave on the data owner's device, acting as asecure black-box teacher. This setup preserves both data privacy and model IPby preventing direct access to model internals. Furthermore, DistilLock employsa model obfuscation mechanism to offload obfuscated weights to untrustedaccelerators for efficient knowledge distillation without compromisingsecurity. We demonstrate that DistilLock prevents unauthorized knowledgedistillation processes and model-stealing attacks while maintaining highcomputational efficiency, but offering a secure and practical solution foredge-based LLM personalization.</description>
      <author>example@mail.com (Asmita Mohanty, Gezheng Kang, Lei Gao, Murali Annavaram)</author>
      <guid isPermaLink="false">2510.16716v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Universal and Transferable Attacks on Pathology Foundation Models</title>
      <link>http://arxiv.org/abs/2510.16660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 Pages, 8 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队提出了UTAP（通用可迁移对抗扰动）方法，揭示病理学基础模型的关键漏洞，该扰动能系统性地破坏多个模型的特征表示能力，导致下游任务性能下降。&lt;h4&gt;背景&lt;/h4&gt;病理学基础模型在医学诊断中应用广泛，但存在安全漏洞和鲁棒性问题，需要评估和防御机制。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的对抗扰动方法，评估病理学基础模型的鲁棒性，并推动防御机制的发展。&lt;h4&gt;方法&lt;/h4&gt;使用深度学习优化UTAP，创建一种固定且微弱的噪声模式，添加到病理图像中以破坏基础模型的特征表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;UTAP导致下游任务性能下降，具有通用性（可应用于不同视野范围，与开发数据集无关）和可迁移性（能降低各种黑盒病理学基础模型的性能），构成对多种病理学基础模型的广泛威胁。&lt;h4&gt;结论&lt;/h4&gt;UTAP为模型鲁棒性评估设定了高标准基准，突显了推进防御机制的必要性，可能为对抗训练提供资源，确保AI在病理学中的安全可靠部署。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了针对病理学基础模型的通用可迁移对抗扰动（UTAP），揭示了其能力中的关键漏洞。使用深度学习优化，UTAP由固定且微弱的噪声模式组成，当添加到病理图像中时，会系统性地破坏多个病理学基础模型的特征表示能力。因此，UTAP会导致利用基础模型的下游任务性能下降，包括在广泛未见过的数据分布上的错误分类。除了损害模型性能外，我们还证明了UTAP的两个关键特性：(1) 通用性：其扰动可以应用于不同的视野范围，且与开发UTAP的数据集无关；(2) 可迁移性：其扰动可以成功降低各种外部、黑盒病理学基础模型的性能——这些模型以前从未见过。这两个特性表明，UTAP不是与特定基础模型或图像数据集相关的专门攻击，而是对各种新兴病理学基础模型及其应用构成广泛威胁。我们在多个数据集上的各种最先进病理学基础模型上系统评估了UTAP，使用固定噪声模式对输入图像进行几乎不可见的修改，导致了其性能显著下降。这些强大攻击的建立为模型鲁棒性评估设定了关键的高标准基准，突显了推进防御机制的必要性，并可能为对抗训练提供必要资源，以确保AI在病理学中的安全可靠部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Universal and Transferable Adversarial Perturbations (UTAP) forpathology foundation models that reveal critical vulnerabilities in theircapabilities. Optimized using deep learning, UTAP comprises a fixed and weaknoise pattern that, when added to a pathology image, systematically disruptsthe feature representation capabilities of multiple pathology foundationmodels. Therefore, UTAP induces performance drops in downstream tasks thatutilize foundation models, including misclassification across a wide range ofunseen data distributions. In addition to compromising the model performance,we demonstrate two key features of UTAP: (1) universality: its perturbation canbe applied across diverse field-of-views independent of the dataset that UTAPwas developed on, and (2) transferability: its perturbation can successfullydegrade the performance of various external, black-box pathology foundationmodels - never seen before. These two features indicate that UTAP is not adedicated attack associated with a specific foundation model or image dataset,but rather constitutes a broad threat to various emerging pathology foundationmodels and their applications. We systematically evaluated UTAP across variousstate-of-the-art pathology foundation models on multiple datasets, causing asignificant drop in their performance with visually imperceptible modificationsto the input images using a fixed noise pattern. The development of thesepotent attacks establishes a critical, high-standard benchmark for modelrobustness evaluation, highlighting a need for advancing defense mechanisms andpotentially providing the necessary assets for adversarial training to ensurethe safe and reliable deployment of AI in pathology.</description>
      <author>example@mail.com (Yuntian Wang, Xilin Yang, Che-Yung Shen, Nir Pillar, Aydogan Ozcan)</author>
      <guid isPermaLink="false">2510.16660v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Hallucination Benchmark for Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2510.16567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SHALLOW，第一个系统分类和量化语音识别系统幻觉现象的基准框架，解决了传统评估指标无法区分幻觉与其他类型错误的问题。&lt;h4&gt;背景&lt;/h4&gt;自动语音识别系统中的幻觉现象指的是神经模型产生的流畅连贯的转录，但这些转录与底层声学输入完全无关。这些幻觉虽然类似于传统解码错误，但由于保留了句法和语义上合理的结构，可能更具危害性，特别是在医疗和法律等关键领域。传统评估指标主要基于错误指标，无法区分语音不准确和幻觉。&lt;h4&gt;目的&lt;/h4&gt;开发新的评估框架，能够有效识别和评估产生幻觉内容倾向更高的模型，并提供更细粒度的错误分析。&lt;h4&gt;方法&lt;/h4&gt;提出SHALLOW框架，系统性地沿着四个互补轴对ASR中的幻觉现象进行分类和量化：词汇、语音、形态和语义。在每个类别中定义有针对性的指标，以产生可解释的模型行为特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过评估各种架构和语音领域，发现当识别质量高（即低词错误率WER）时，SHALLOW指标与WER高度相关；但随着WER的增加，这种相关性显著减弱。SHALLOW能够捕获在降级和挑战性条件下WER无法区分的细粒度错误模式。&lt;h4&gt;结论&lt;/h4&gt;SHALLOW框架支持对模型弱点的具体诊断，并提供比总体错误率所能提供的更详细的模型改进反馈，有助于提高语音识别系统在关键领域的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;自动语音识别系统中的幻觉现象指的是神经ASR模型产生的流畅连贯的转录，这些转录与底层声学输入（即语音信号）完全无关。虽然幻觉可能类似于传统解码错误，在可能降低转录对下游应用的可用性方面，但幻觉由于其保留了句法和语义上合理的结构，可能更具危害性。这种明显的连贯性可能误导后续处理阶段并引入严重风险，特别是在医疗和法律等关键领域。传统评估指标主要基于错误指标，无法区分语音不准确和幻觉。因此，迫切需要新的评估框架，能够有效识别和评估产生幻觉内容倾向更高的模型。为此，我们引入了SHALLOW，这是第一个基准框架，系统性地沿着四个互补轴对ASR中的幻觉现象进行分类和量化：词汇、语音、形态和语义。我们在每个类别中定义有针对性的指标，以产生可解释的模型行为特征。通过评估各种架构和语音领域，我们发现当识别质量高（即低WER）时，SHALLOW指标与词错误率（WER）高度相关。然而，随着WER的增加，这种相关性显著减弱。因此，SHALLOW捕获了在降级和挑战性条件下WER无法区分的细粒度错误模式。我们的框架支持对模型弱点的具体诊断，并提供比总体错误率所能提供的更详细的模型改进反馈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hallucinations in automatic speech recognition (ASR) systems refer to fluentand coherent transcriptions produced by neural ASR models that are completelyunrelated to the underlying acoustic input (i.e., the speech signal). Whilesimilar to conventional decoding errors in potentially compromising theusability of transcriptions for downstream applications, hallucinations can bemore detrimental due to their preservation of syntactically and semanticallyplausible structure. This apparent coherence can mislead subsequent processingstages and introduce serious risks, particularly in critical domains such ashealthcare and law. Conventional evaluation metrics are primarily centered onerror-based metrics and fail to distinguish between phonetic inaccuracies andhallucinations. Consequently, there is a critical need for new evaluationframeworks that can effectively identify and assess models with a heightenedpropensity for generating hallucinated content. To this end, we introduceSHALLOW, the first benchmark framework that systematically categorizes andquantifies hallucination phenomena in ASR along four complementary axes:lexical, phonetic, morphological, and semantic. We define targeted metricswithin each category to produce interpretable profiles of model behavior.Through evaluation across various architectures and speech domains, we havefound that SHALLOW metrics correlate strongly with word error rate (WER) whenrecognition quality is high (i.e., low WER). Still, this correlation weakenssubstantially as WER increases. SHALLOW, therefore, captures fine-grained errorpatterns that WER fails to distinguish under degraded and challengingconditions. Our framework supports specific diagnosis of model weaknesses andprovides feedback for model improvement beyond what aggregate error rates canoffer.</description>
      <author>example@mail.com (Alkis Koudounas, Moreno La Quatra, Manuel Giollo, Sabato Marco Siniscalchi, Elena Baralis)</author>
      <guid isPermaLink="false">2510.16567v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>NeurIPT: Foundation Model for Neural Interfaces</title>
      <link>http://arxiv.org/abs/2510.16548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by The Thirty-Ninth Annual Conference on Neural Information  Processing Systems (NeurIPS 2025). Project Page:  https://ZzzitaoFang.github.io/projects/NeurIPT/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NeurIPT，一种专为多样化EEG神经接口设计的基础模型，通过结合基于幅度的掩码预训练和渐进式专家混合架构，有效捕捉EEG信号中的时空特征，在多个BCI数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)在临床诊断和脑机接口中有广泛应用，随着EEG数据量和多样性的增加，建立基础模型来扩展和泛化神经解码成为研究热点。然而，将基础模型应用于EEG仍面临受试者间、任务间和条件间变异性，以及不同电极配置带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理EEG数据多样性和变异性的基础模型，以提高神经解码的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出NeurIPT基础模型，采用预训练Transformer架构；时间维度引入基于信号幅度的掩码预训练(AAMP)和渐进式专家混合(PMoE)架构；空间维度利用电极的三维物理坐标实现跨设置的嵌入迁移，并开发脑叶内-间池化(IILP)以利用区域脑特征。&lt;h4&gt;主要发现&lt;/h4&gt;在八个下游BCI数据集上的实证评估表明，NeurIPT通过微调始终取得了最先进的性能，展示了其广泛的适用性和鲁棒的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作推动了EEG基础模型的前沿发展，并为可扩展和可泛化的神经信息处理系统提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)有广泛的应用，从临床诊断到脑机接口(BCIs)。随着EEG数据的数量和多样性的增加，人们越来越有兴趣建立基础模型(FMs)来扩展和泛化神经解码。尽管显示出早期潜力，但由于显著的受试者间、任务间和条件间变异性，以及不同记录设置中的多样化电极配置，将基础模型应用于EEG仍然具有挑战性。为了解决这些开放挑战，我们提出了NeurIPT，这是一种为多样化EEG神经接口开发的基础模型，通过捕捉EEG信号中固有的同质和异质时空特征，采用预训练Transformer架构。在时间维度上，我们引入了基于幅度的掩码预训练(AAMP)，基于信号幅度而非随机间隔进行掩码，以学习跨越不同信号强度的鲁棒表示，而不仅仅是局部插值。此外，这种时间表示通过渐进式专家混合(PMoE)架构得到增强，在更深层次逐步引入专门的专家子网络，有效适应EEG信号的多样化时间特征。在空间维度上，NeurIPT利用电极的三维物理坐标，实现不同EEG设置间的嵌入有效迁移，并在微调过程中开发脑叶内-间池化(IILP)，以高效利用区域脑特征。在八个下游BCI数据集上的实证评估，通过微调，证明了NeurIPT始终取得了最先进的性能，突显了其广泛的适用性和鲁棒的泛化能力。我们的工作推动了EEG基础模型的前沿发展，并为可扩展和可泛化的神经信息处理系统提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) has wide-ranging applications, from clinicaldiagnosis to brain-computer interfaces (BCIs). With the increasing volume andvariety of EEG data, there has been growing interest in establishing foundationmodels (FMs) to scale up and generalize neural decoding. Despite showing earlypotential, applying FMs to EEG remains challenging due to substantialinter-subject, inter-task, and inter-condition variability, as well as diverseelectrode configurations across recording setups. To tackle these openchallenges, we propose NeurIPT, a foundation model developed for diverseEEG-based Neural Interfaces with a Pre-trained Transformer by capturing bothhomogeneous and heterogeneous spatio-temporal characteristics inherent in EEGsignals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),masking based on signal amplitude rather than random intervals, to learn robustrepresentations across varying signal intensities beyond local interpolation.Moreover, this temporal representation is enhanced by a ProgressiveMixture-of-Experts (PMoE) architecture, where specialized expert subnetworksare progressively introduced at deeper layers, adapting effectively to thediverse temporal characteristics of EEG signals. Spatially, NeurIPT leveragesthe 3D physical coordinates of electrodes, enabling effective transfer ofembedding across varying EEG settings, and develops Intra-Inter Lobe Pooling(IILP) during fine-tuning to efficiently exploit regional brain features.Empirical evaluations across eight downstream BCI datasets, via fine-tuning,demonstrated NeurIPT consistently achieved state-of-the-art performance,highlighting its broad applicability and robust generalization. Our work pushesforward the state of FMs in EEG and offers insights into scalable andgeneralizable neural information processing systems.</description>
      <author>example@mail.com (Zitao Fang, Chenxuan Li, Hongting Zhou, Shuyang Yu, Guodong Du, Ashwaq Qasem, Yang Lu, Jing Li, Junsong Zhang, Sim Kuan Goh)</author>
      <guid isPermaLink="false">2510.16548v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion</title>
      <link>http://arxiv.org/abs/2510.16446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIPAMIN是一种视觉提示初始化策略，通过将提示与嵌入空间中的语义信息丰富区域对齐，并向预训练子空间注入新的表示方向，增强自监督模型的适应性，在各种任务和数据集大小上一致提高性能。&lt;h4&gt;背景&lt;/h4&gt;在大规模基础模型时代，为每个下游任务完全微调预训练网络通常需要大量资源。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉提示调整方法在专门化提示或丰富表示空间方面的局限性，特别是在自监督主干网络应用于具有挑战性任务和数据稀缺环境时。&lt;h4&gt;方法&lt;/h4&gt;提出VIPAMIN，一种视觉提示初始化策略，通过两种方式增强自监督模型的适应性：(1)将提示与嵌入空间中的语义信息丰富的区域对齐，(2)向预训练子空间注入新的表示方向。该方法仅需一次前向传播和轻量级操作。&lt;h4&gt;主要发现&lt;/h4&gt;VIPAMIN在各种任务和数据集大小上一致提高了性能，在视觉提示调整方面树立了新的最先进水平，特别是在具有挑战性的任务和数据稀缺环境中表现突出。&lt;h4&gt;结论&lt;/h4&gt;VIPAMIN是一种简单而有效的视觉提示初始化策略，能够显著提升自监督模型在下游任务中的适应性和性能。&lt;h4&gt;翻译&lt;/h4&gt;在大规模基础模型时代，为每个下游任务完全微调预训练网络通常需要大量资源。提示调整通过引入可调整提示同时保持主干网络冻结，提供了一种轻量级替代方案。然而，现有的视觉提示调整方法通常无法专门化提示或丰富表示空间——特别是当应用于自监督主干网络时。我们表明，这些限制在具有挑战性的任务和数据稀缺环境中变得尤为明显，而有效适应在这些环境中最为关键。在这项工作中，我们介绍了VIPAMIN，一种视觉提示初始化策略，通过(1)将提示与嵌入空间中的语义信息丰富区域对齐，和(2)向预训练子空间注入新的表示方向，来增强自监督模型的适应性。尽管简单——只需要一次前向传播和轻量级操作——VIPAMIN在不同任务和数据集大小上一致提高了性能，在视觉提示调整方面树立了新的最先进水平。我们的代码可在https://github.com/iamjaekyun/vipamin获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of large-scale foundation models, fully fine-tuning pretrainednetworks for each downstream task is often prohibitively resource-intensive.Prompt tuning offers a lightweight alternative by introducing tunable promptswhile keeping the backbone frozen. However, existing visual prompt tuningmethods often fail to specialize the prompts or enrich the representationspace--especially when applied to self-supervised backbones. We show that theselimitations become especially pronounced in challenging tasks and data-scarcesettings, where effective adaptation is most critical. In this work, weintroduce VIPAMIN, a visual prompt initialization strategy that enhancesadaptation of self-supervised models by (1) aligning prompts with semanticallyinformative regions in the embedding space, and (2) injecting novelrepresentational directions beyond the pretrained subspace. Despite itssimplicity--requiring only a single forward pass and lightweightoperations--VIPAMIN consistently improves performance across diverse tasks anddataset sizes, setting a new state of the art in visual prompt tuning. Our codeis available at https://github.com/iamjaekyun/vipamin.</description>
      <author>example@mail.com (Jaekyun Park, Hye Won Chung)</author>
      <guid isPermaLink="false">2510.16446v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment</title>
      <link>http://arxiv.org/abs/2510.16387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了Whisper模型在第二语言口语评估中的应用潜力，通过提取声学和语言特征，实现了超越现有方法的性能，并揭示了模型内在编码语言能力的特点。&lt;h4&gt;背景&lt;/h4&gt;Whisper是一个成熟的自动语音识别基础模型，先前研究主要分析其生成的转录文本，而对其潜在能力的探索不足。&lt;h4&gt;目的&lt;/h4&gt;探索Whisper模型在第二语言口语评估任务中的潜在能力，分析其内在编码的语言能力特点。&lt;h4&gt;方法&lt;/h4&gt;从Whisper的隐藏表示中提取声学和语言特征，在Whisper的中间和最终输出之上训练轻量级分类器，并融入图像和文本提示信息作为辅助线索。&lt;h4&gt;主要发现&lt;/h4&gt;Whisper模型在GEPT图片描述数据集上超越了现有最先进基线；融入图像和文本提示信息可进一步提升性能；即使没有任务特定微调，Whisper也能内在编码语言熟练程度的顺序模式和语音的语义方面。&lt;h4&gt;结论&lt;/h4&gt;Whisper模型具有作为第二语言口语评估和其他口语理解任务强大基础的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们探讨了Whisper这一成熟的自动语音识别基础模型在第二语言口语评估背景下的未开发潜力。与先前研究仅外在分析Whisper生成的转录文本不同，我们的方法更进一步，通过从隐藏表示中提取声学和语言特征来探测其潜在能力。仅通过在Whisper的中间和最终输出之上训练一个轻量级分类器，我们的方法在GEPT图片描述数据集上实现了强大的性能，超越了现有的最先进基线，包括一种多模态方法。此外，通过将图像和文本提示信息作为辅助相关性线索纳入，我们展示了额外的性能提升。最后，我们对Whisper的嵌入进行了深入分析，揭示出即使没有任务特定的微调，该模型也内在地编码了熟练程度的顺序模式和语音的语义方面，突显了其作为第二语言口语评估和其他口语理解任务强大基础的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore the untapped potential of Whisper, awell-established automatic speech recognition (ASR) foundation model, in thecontext of L2 spoken language assessment (SLA). Unlike prior studies thatextrinsically analyze transcriptions produced by Whisper, our approach goes astep further to probe its latent capabilities by extracting acoustic andlinguistic features from hidden representations. With only a lightweightclassifier being trained on top of Whisper's intermediate and final outputs,our method achieves strong performance on the GEPT picture-description dataset,outperforming existing cutting-edge baselines, including a multimodal approach.Furthermore, by incorporating image and text-prompt information as auxiliaryrelevance cues, we demonstrate additional performance gains. Finally, weconduct an in-depth analysis of Whisper's embeddings, which reveals that, evenwithout task-specific fine-tuning, the model intrinsically encodes both ordinalproficiency patterns and semantic aspects of speech, highlighting its potentialas a powerful foundation for SLA and other spoken language understanding tasks.</description>
      <author>example@mail.com (Fu-An Chao, Bi-Cheng Yan, Berlin Chen)</author>
      <guid isPermaLink="false">2510.16387v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning</title>
      <link>http://arxiv.org/abs/2510.16240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了Cosmos-Surg-dVRK，一种基于Cosmos世界基础模型的外科微调版本，结合视频分类器，实现了手术策略的全自动在线评估和基准测试，解决了在真实机器人平台上评估的高成本、时间和可重复性问题。&lt;h4&gt;背景&lt;/h4&gt;手术机器人和视觉-语言-动作模型的兴起推动了自主手术策略的发展，但在物理机器人平台（如dVRK）上直接评估这些策略面临高成本、时间需求、可重复性挑战和执行变异性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高保真度的模拟方法，用于评估复杂的现实世界手术任务，并提供全自动化的在线评估和基准测试平台。&lt;h4&gt;方法&lt;/h4&gt;引入Cosmos-Surg-dVRK（Cosmos世界基础模型的外科微调版本），结合训练好的视频分类器，使用两个不同的外科数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在桌面缝合垫任务上，Cosmos-Surg-dVRK中的在线运行与真实dVRK平台上的策略结果具有强相关性；人类标注者与V-JEPA2派生的视频分类器达成良好一致；离体猪胆囊切除术任务的初步实验显示与现实世界评估的良好一致性。&lt;h4&gt;结论&lt;/h4&gt;Cosmos-Surg-dVRK平台在更复杂的手术程序评估方面具有潜力，为手术策略的自动化评估提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;手术机器人和视觉-语言-动作模型的兴起加速了自主手术策略和高效评估策略的发展。然而，在da Vinci研究套件(dVRK)等物理机器人平台上直接评估这些策略仍然受到高成本、时间需求、可重复性挑战和执行变异性的阻碍。物理AI的世界基础模型(WFM)为模拟复杂的现实世界手术任务（如软组织变形）提供了具有高保真度的变革性方法。这项工作介绍了Cosmos-Surg-dVRK，这是Cosmos WFM的外科微调版本，它与训练好的视频分类器一起，实现了手术策略的完全自动化在线评估和基准测试。我们使用两个不同的外科数据集评估了Cosmos-Surg-dVRK。在桌面缝合垫任务上，自动化流程在Cosmos-Surg-dVRK中的在线运行与真实dVRK Si平台上的策略结果之间实现了强相关性，并且人类标注者与V-JEPA2派生的视频分类器之间达成良好一致。此外，在Cosmos-Surg-dVRK中使用离体猪胆囊切除术任务的初步实验显示出与现实世界评估的良好一致性，突显了该平台在更复杂手术程序方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决外科机器人策略评估的问题。传统上，直接在物理机器人平台（如dVRK）上评估策略存在成本高、耗时长、可重复性差和执行变异大等挑战。此外，现有模拟器难以准确模拟外科手术中的软组织变形等复杂物理现象。这个问题在现实中很重要，因为随着外科机器人和视觉-语言-动作模型的发展，自主外科策略的开发日益增多，但缺乏高效、可靠的评估方法严重制约了这一领域的进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了世界基础模型（WFM）的概念，特别是Cosmos WFM这一视频生成模型，将其作为可扩展的通用学习模拟器。他们针对外科手术领域对Cosmos WFM进行了微调，创建了Cosmos-Surg-dVRK。此外，他们还使用了V-JEPA 2视频分类器来自动评估策略执行的成功率。作者参考了早期世界模型工作（如Ha &amp; Schmidhuber, 2018）和基于扩散过程的大规模多模态世界基础模型，以及现有的视觉-语言-动作模型在外科机器人中的应用，但针对外科手术的特殊需求进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用微调后的Cosmos世界基础模型（Cosmos-Surg-dVRK）作为专门针对外科手术的学习模拟器，结合视频分类器实现完全自动化的策略评估。整体流程包括：1) 使用真实机器人记录的初始状态初始化策略；2) 策略和Cosmos-Surg-dVRK自回归地生成未来帧；3) 将生成的帧组合成视频；4) 使用V-JEPA 2视频分类器自动评估视频中的任务成功或失败；5) 根据评估结果选择最有前途的策略进行真实机器人测试。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 针对外科手术领域微调的Cosmos-Surg-dVRK学习模拟器；2) 使用V-JEPA 2视频分类器实现完全自动化的策略评估；3) 在真实外科任务（包括桌面缝合垫任务和离体猪胆囊切除术）上验证了方法有效性；4) 通过消融研究强调了失败数据在训练中的重要性。相比之前的工作，Cosmos-Surg-dVRK直接从外科数据中学习软组织动力学，不需要显式指定材料属性；使用标准的相对笛卡尔动作空间，实现即插即用的接口；专注于外科手术这一特定领域，而大多数现有方法专注于其他领域如视频游戏或通用机器人。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Cosmos-Surg-dVRK，一个基于世界基础模型的自动化外科机器人策略评估系统，通过在模拟环境中生成逼真的外科手术视频并自动评估策略性能，有效解决了传统评估中成本高、耗时长和难以模拟软组织变形等问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of surgical robots and vision-language-action models has acceleratedthe development of autonomous surgical policies and efficient assessmentstrategies. However, evaluating these policies directly on physical roboticplatforms such as the da Vinci Research Kit (dVRK) remains hindered by highcosts, time demands, reproducibility challenges, and variability in execution.World foundation models (WFM) for physical AI offer a transformative approachto simulate complex real-world surgical tasks, such as soft tissue deformation,with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetuneof the Cosmos WFM, which, together with a trained video classifier, enablesfully automated online evaluation and benchmarking of surgical policies. Weevaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletopsuture pad tasks, the automated pipeline achieves strong correlation betweenonline rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Siplatform, as well as good agreement between human labelers and the V-JEPA2-derived video classifier. Additionally, preliminary experiments with ex-vivoporcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promisingalignment with real-world evaluations, highlighting the platform's potentialfor more complex surgical procedures.</description>
      <author>example@mail.com (Lukas Zbinden, Nigel Nelson, Juo-Tung Chen, Xinhao Chen, Ji Woong, Kim, Mahdi Azizian, Axel Krieger, Sean Huver)</author>
      <guid isPermaLink="false">2510.16240v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Probing the Higgs Portal to a Strongly-Interacting Dark Sector at the FCC-ee</title>
      <link>http://arxiv.org/abs/2510.17675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figure, to be submitted to EPJC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在未来的圆形对撞机e+e-碰撞模式下可能出现的来自禁闭暗区的奇异信号，研究希格斯玻色子作为媒介产生的暗夸克及其导致的半可见喷注终态，并提出使用图神经网络喷注标记器提高信号探测灵敏度。&lt;h4&gt;背景&lt;/h4&gt;在未来的圆形对撞机中，e+e-碰撞模式下可能产生来自禁闭暗区的奇异信号，希格斯玻色子可能作为标准模型和暗区之间相互作用的媒介。&lt;h4&gt;目的&lt;/h4&gt;研究希格斯玻色子诱导的半可见喷注的探测方法，提高信号与背景的区分度，增强对希格斯玻色子到暗夸克稀有分支比的探测能力。&lt;h4&gt;方法&lt;/h4&gt;研究不同不可见状态比例的半可见喷注特性；当不可见成分较大时，基于运动学特征（如缺失能量）进行选择；当不可见成分较小时，采用图神经网络喷注标记器利用喷注亚结构差异进行信号识别。&lt;h4&gt;主要发现&lt;/h4&gt;对于不可见成分较大的情况，基于缺失能量的选择能提供良好的信背比；对于不可见成分较小的情况，图神经网络喷注标记器能有效提高信号探测灵敏度；所提策略可探测广泛参数空间，将希格斯玻色子到暗夸克的稀有分支比限制在千分之一水平。&lt;h4&gt;结论&lt;/h4&gt;提出的机器学习策略能有效探测希格斯玻色子诱导的半可见喷注，增强在未来的圆形对撞机上的发现前景，并限制希格斯玻色子的稀有分支比。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了在未来的圆形对撞机e+e-碰撞模式下可能出现的来自禁闭暗区的奇异信号。假设希格斯玻色子介导标准模型与暗区之间的相互作用，暗夸克可以在e+e-碰撞中产生。随后的强动力学可能导致包含可见和不可见粒子的半可见喷注终态。我们研究了不同不可见状态比例的半可见喷注，以及富含轻子和光子的喷注。当不可见成分较大时，基于运动学特征（如事件中的缺失能量）的选择已能提供良好的信背比。对于较小的不可见比例，缺失能量的减少使这些信号更类似于标准模型事件，因此我们采用利用喷注亚结构差异的图神经网络喷注标记器。这种机器学习策略提高了灵敏度，增强了在未来的圆形对撞机上发现希格斯玻色子诱导的半可见喷注的前景。我们的结果表明，所提出的策略可以有效探测所考虑模型的广泛参数空间和各种信号，将希格斯玻色子到暗夸克的稀有分支比限制在千分之一水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores exotic signatures from confining dark sectors that mayarise in the e+e- collision mode at the Future Circular Collider. Assuming theHiggs boson mediates the interaction between the Standard Model and the darksector, dark quarks can be produced in e+e- collisions. The ensuing strongdynamics may lead to semi-visible jet final states, containing both visible andinvisible particles. We investigate semi-visible jets with different fractionsof invisible states, and enriched in leptons and photons. When the invisiblecomponent is large, selections based on kinematic features, such as the missingenergy in the event, already provide good signal-to-background discrimination.For smaller invisible fractions, the reduced missing energy makes these signalsmore similar to Standard Model events, and we therefore employ a graph neuralnetwork jet tagger exploiting differences in jet substructure. This machinelearning strategy improves sensitivity and enhances the discovery prospects ofHiggs boson-induced semi-visible jets at the Future Circular Collider. Ourresults show that the proposed strategy can effectively probe a wide parameterspace for the models considered, and a variety of signatures, constraining theHiggs boson exotic branching ratios into dark quarks at the permille-level.</description>
      <author>example@mail.com (Cesare Cazzaniga, Annapaola de Cosa, Felix Kahlhoefer, Andrea S. Maria, Roberto Seidita, Emre Sitti)</author>
      <guid isPermaLink="false">2510.17675v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Model Metamers Reveal Invariances in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.17378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过元形态生成技术揭示了图神经网络(GNNs)中的过度不变性问题，并提出了改进方向和评估基准。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在感知系统中被广泛使用以学习具有不变性的表示，模仿人脑机制。然而，视觉和听觉领域的研究证实，人工神经网络的不变性属性与人脑之间仍存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;研究图神经网络(GNNs)中的不变性行为，探索其与人脑不变性机制的差异。&lt;h4&gt;方法&lt;/h4&gt;引入元形态生成技术，通过优化输入图使内部节点激活与参考图匹配，获得在表示空间中等效但在结构和特征上显著不同的图。理论研究聚焦于单个节点的局部元形态维度和元形态流形的激活诱导体积变化。&lt;h4&gt;主要发现&lt;/h4&gt;多种经典GNN架构表现出极端水平的表示不变性。虽然修改模型架构和训练策略可部分减轻这种过度不变性，但无法从根本上达到人脑水平的不变性。&lt;h4&gt;结论&lt;/h4&gt;量化元形态图与原始图之间的偏差，揭示了当前GNNs的独特失效模式，为模型评估提供了补充基准。&lt;h4&gt;翻译&lt;/h4&gt;近年来，深度神经网络已被广泛应用于感知系统，以学习具有不变性的表示，旨在模仿人脑中观察到的不变性机制。然而，视觉和听觉领域的研究证实，人工神经网络的不变性与人类之间仍存在显著差距。为了研究图神经网络(GNNs)中的不变性行为，我们引入了一种模型'元形态'生成技术。通过优化输入图使其内部节点激活与参考图相匹配，我们获得了在模型表示空间中等效但在结构和节点特征上显著不同的图。我们的理论分析聚焦于两个方面：单个节点的局部元形态维度和元形态流形的激活诱导体积变化。利用这种方法，我们在几种经典的GNN架构中发现了极端水平的表示不变性。虽然对模型架构和训练策略的有针对性的修改可以部分减轻这种过度不变性，但它们无法从根本上弥合与人脑相似的不变性之间的差距。最后，我们量化了元形态图与其原始对应物之间的偏差，揭示了当前GNNs的独特失效模式，并为模型评估提供了补充基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, deep neural networks have been extensively employed inperceptual systems to learn representations endowed with invariances, aiming toemulate the invariance mechanisms observed in the human brain. However, studiesin the visual and auditory domains have confirmed that significant gaps remainbetween the invariance properties of artificial neural networks and those ofhumans. To investigate the invariance behavior within graph neural networks(GNNs), we introduce a model ``metamers'' generation technique. By optimizinginput graphs such that their internal node activations match those of areference graph, we obtain graphs that are equivalent in the model'srepresentation space, yet differ significantly in both structure and nodefeatures. Our theoretical analysis focuses on two aspects: the local metamerdimension for a single node and the activation-induced volume change of themetamer manifold. Utilizing this approach, we uncover extreme levels ofrepresentational invariance across several classic GNN architectures. Althoughtargeted modifications to model architecture and training strategies canpartially mitigate this excessive invariance, they fail to fundamentally bridgethe gap to human-like invariance. Finally, we quantify the deviation betweenmetamer graphs and their original counterparts, revealing unique failure modesof current GNNs and providing a complementary benchmark for model evaluation.</description>
      <author>example@mail.com (Wei Xu, Xiaoyi Jiang, Lixiang Xu, Dechao Tang)</author>
      <guid isPermaLink="false">2510.17378v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses</title>
      <link>http://arxiv.org/abs/2510.17185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种统一的框架来评估图神经网络和大语言模型在文本属性图学习中的鲁棒性，发现了模型在文本和结构之间存在固有的鲁棒性权衡，并提出了SFT-auto框架来克服这些权衡。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)和大语言模型(LLMs)是学习文本属性图(TAGs)的强大方法，但目前对其鲁棒性的理解还不全面。现有的评估是零散的，未能系统地研究不同模型和攻击场景下文本和结构扰动的不同影响。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者引入了一个统一的框架来评估TAG学习的鲁棒性，旨在系统地研究不同类型的扰动对各种模型的影响，并提出解决方案来克服发现的权衡。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一个统一的框架，评估了经典GNNs、鲁棒GNNs(RGNNs)和GraphLLMs在四个领域的十个数据集上的性能，测试了基于文本、基于结构和混合扰动在投毒和规避场景下的影响。为了克服发现的权衡，他们引入了SFT-auto框架。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模型在文本和结构之间存在固有的鲁棒性权衡；2) GNNs和RGNNs的性能在很大程度上取决于文本编码器和攻击类型；3) GraphLLMs特别容易受到训练数据损坏的影响。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来的TAG安全研究奠定了基础，并为对抗环境中的鲁棒TAG学习提供了实用的解决方案。作者公开了他们的代码。&lt;h4&gt;翻译&lt;/h4&gt;尽管图神经网络(GNNs)和大语言模型(LLMs)是学习文本属性图(TAGs)的强大方法，但对其鲁棒性的全面理解仍然模糊。目前的评估是零散的，未能系统地研究不同模型和攻击场景下文本和结构扰动的不同影响。为了解决这些局限性，我们引入了一个统一的综合框架来评估TAG学习中的鲁棒性。我们的框架在四个领域的十个数据集上评估了经典GNNs、鲁棒GNNs(RGNNs)和GraphLLMs，在投毒和规避场景下，应对了多种基于文本、基于结构和混合的扰动。我们的广泛分析揭示了多个发现，其中三个特别值得注意：1) 模型在文本和结构之间存在固有的鲁棒性权衡；2) GNNs和RGNNs的性能在很大程度上取决于文本编码器和攻击类型；3) GraphLLMs特别容易受到训练数据损坏的影响。为了克服识别出的权衡，我们引入了SFT-auto，这是一个新颖的框架，在单一模型内提供针对文本和结构攻击的优越且平衡的鲁棒性。我们的研究为未来的TAG安全研究奠定了基础，并为对抗环境中的鲁棒TAG学习提供了实用的解决方案。我们的代码可在以下网址获取：https://github.com/Leirunlin/TGRB。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Graph Neural Networks (GNNs) and Large Language Models (LLMs) arepowerful approaches for learning on Text-Attributed Graphs (TAGs), acomprehensive understanding of their robustness remains elusive. Currentevaluations are fragmented, failing to systematically investigate the distincteffects of textual and structural perturbations across diverse models andattack scenarios. To address these limitations, we introduce a unified andcomprehensive framework to evaluate robustness in TAG learning. Our frameworkevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across tendatasets from four domains, under diverse text-based, structure-based, andhybrid perturbations in both poisoning and evasion scenarios. Our extensiveanalysis reveals multiple findings, among which three are particularlynoteworthy: 1) models have inherent robustness trade-offs between text andstructure, 2) the performance of GNNs and RGNNs depends heavily on the textencoder and attack type, and 3) GraphLLMs are particularly vulnerable totraining data corruption. To overcome the identified trade-offs, we introduceSFT-auto, a novel framework that delivers superior and balanced robustnessagainst both textual and structural attacks within a single model. Our workestablishes a foundation for future research on TAG security and offerspractical solutions for robust TAG learning in adversarial environments. Ourcode is available at: https://github.com/Leirunlin/TGRB.</description>
      <author>example@mail.com (Runlin Lei, Lu Yi, Mingguo He, Pengyu Qiu, Zhewei Wei, Yongchao Liu, Chuntao Hong)</author>
      <guid isPermaLink="false">2510.17185v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Based Extraction of Promising Material Groups and Common Features from High-Dimensional Data: A Case of Optical Spectra of Inorganic Crystals</title>
      <link>http://arxiv.org/abs/2510.17123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种深度学习模型的解释方法，用于处理材料科学中的高维光谱数据，并通过特征提取和聚类分析对材料进行分类，最终成功应用于光学吸收光谱预测模型的解释。&lt;h4&gt;背景&lt;/h4&gt;材料科学研究中需要处理高维光谱数据，而深度学习模型在处理这类数据时存在解释性挑战。传统方法难以同时考虑光谱数据和化学特性（如元素组成和原子排列）的相似性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维光谱数据的深度学习模型解释方法，并根据光谱数据和化学特性的相似性对材料进行分类。&lt;h4&gt;方法&lt;/h4&gt;使用特征提取和聚类分析技术，结合光谱数据和化学特性（元素组成和原子排列）对材料进行分类。将该方法应用于原子线图神经网络(ALIGNN)模型，该模型使用2,681种金属氧化物、硫属化物及相关化合物的第一性原理计算数据进行训练，用于预测光学吸收光谱。&lt;h4&gt;主要发现&lt;/h4&gt;分析揭示了影响光学吸收起始特性的关键元素种类及其配位环境，这些因素对材料的光学特性有重要影响。&lt;h4&gt;结论&lt;/h4&gt;提出的方法适用于各种光谱数据的分类和解释，不仅限于无机晶体的光学吸收光谱，为材料科学研究提供了一种新的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;我们报道了一种深度学习模型的解释方法，使我们能够处理材料科学中的高维光谱数据。所提出的方法使用特征提取和聚类分析，根据光谱数据以及化学特性（如元素组成和原子排列）的相似性将材料分类。作为演示，我们将此方法应用于原子线图神经网络(ALIGNN)模型，该模型使用2,681种金属氧化物、硫属化物及相关化合物的第一性原理计算数据进行训练，用于预测光学吸收光谱。我们的分析揭示了影响光学吸收起始特性的关键元素种类及其配位环境。本文提出的方法适用于各种光谱数据的分类和解释，超越了无机晶体的光学吸收光谱范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We report an interpretation method for deep learning models that allows us tohandle high-dimensional spectral data in materials science. The proposed methoduses feature extraction and clustering analysis to categorize materials intoclasses based on similarities in both spectral data and chemicalcharacteristics such as elemental composition and atomic arrangement. As ademonstration, we apply this method to an atomistic line graph neural network(ALIGNN) model trained on first-principles calculation data of 2,681 metaloxides, chalcogenides, and related compounds for optical absorption spectrumprediction. Our analysis reveals key elemental species and their coordinationenvironments that influence optical absorption onset characteristics. Themethod proposed herein is broadly applicable to the classification andinterpretation of diverse spectral data, extending beyond the opticalabsorption spectra of inorganic crystals.</description>
      <author>example@mail.com (Akira Takahashi, Yu Kumagai, Arata Takamatsu, Fumiyasu Oba)</author>
      <guid isPermaLink="false">2510.17123v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains</title>
      <link>http://arxiv.org/abs/2510.16885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniGTE是一种指令调优的编码器-解码器框架，通过整合图结构与大型语言模型语义，实现了无需任务特定监督的通用图推理能力，在多种图任务上达到最先进的零样本性能。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络通常与固定标签空间绑定，而大型语言模型难以捕捉图结构，使得在没有任务特定监督的情况下推广到未见过的图任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的框架，整合结构和语义推理能力，以解决图神经网络与大型语言模型各自的局限性，实现跨任务和跨域的通用图推理。&lt;h4&gt;方法&lt;/h4&gt;UniGTE采用增强预训练自回归大型语言模型的编码器，通过可学习对齐令牌和结构感知的图-文本注意力机制，使模型能同时处理标记化的图和自然语言任务提示，同时保持节点排列不变性；编码器生成紧凑的任务感知图表示，基于这些表示，冻结的大型语言模型解码器预测任务答案并重新表述输入图，重建目标正则化编码器保留结构线索。&lt;h4&gt;主要发现&lt;/h4&gt;UniGTE在五个涵盖不同领域节点级、边级和图级任务的数据集上进行指令调优后，推理时无需微调，在跨任务和跨域设置下的节点分类、链接预测、图分类和图回归任务上实现了新的最先进零样本结果。&lt;h4&gt;结论&lt;/h4&gt;图结构与大型语言模型语义的紧密集成能够实现强大且可迁移的图推理能力，为无需任务特定监督的通用图学习提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;在没有任务特定监督的情况下推广到未见过的图任务具有挑战性：传统图神经网络通常与固定标签空间绑定，而大型语言模型难以捕捉图结构。我们引入UniGTE，一种统一的指令调优编码器-解码器框架，整合了结构和语义推理能力。编码器通过可学习对齐令牌和结构感知的图-文本注意力机制增强预训练的自回归大型语言模型，使其能够同时处理标记化的图和自然语言任务提示，同时保持对节点排列的不变性。这产生了紧凑的、任务感知的图表示。仅基于这些表示，冻结的大型语言模型解码器进行预测和重建：输出任务答案并同时用自然语言重新表述输入图。重建目标正则化编码器以保留结构线索。UniGTE在五个涵盖不同领域节点级、边级和图级任务的数据集上进行指令调优，但推理时不需要微调。在跨任务和跨域设置下的节点分类、链接预测、图分类和图回归任务上，它实现了新的最先进零样本结果，表明图结构与大型语言模型语义的紧密集成能够实现强大且可迁移的图推理能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizing to unseen graph tasks without task-specific supervision ischallenging: conventional graph neural networks are typically tied to a fixedlabel space, while large language models (LLMs) struggle to capture graphstructure. We introduce UniGTE, an instruction-tuned encoder-decoder frameworkthat unifies structural and semantic reasoning. The encoder augments apretrained autoregressive LLM with learnable alignment tokens and astructure-aware graph-text attention mechanism, enabling it to attend jointlyto a tokenized graph and a natural-language task prompt while remainingpermutation-invariant to node order. This yields compact, task-aware graphrepresentations. Conditioned solely on these representations, a frozen LLMdecoder predicts and reconstructs: it outputs the task answer andsimultaneously paraphrases the input graph in natural language. Thereconstruction objective regularizes the encoder to preserve structural cues.UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,and graph-level tasks across diverse domains, yet requires no fine-tuning atinference. It achieves new state-of-the-art zero-shot results on nodeclassification, link prediction, graph classification, and graph regressionunder cross-task and cross-domain settings, demonstrating that tightintegration of graph structure with LLM semantics enables robust, transferablegraph reasoning.</description>
      <author>example@mail.com (Duo Wang, Yuan Zuo, Guangyue Lu, Junjie Wu)</author>
      <guid isPermaLink="false">2510.16885v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Accelerated First-Principles Quantum Transport Simulations at Nonequilibrium State</title>
      <link>http://arxiv.org/abs/2510.16878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepQT是一种深度学习框架，结合图神经网络和transformer架构，实现了电子结构和输运的多属性预测，无需手动特征工程，同时保持了第一性原理精度并大幅降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;非平衡格林函数方法结合密度泛函理论(NEGF-DFT)为纳米尺度电子输运模拟提供了严格框架，但其计算成本随系统规模急剧增加。现有的人工智能方法在加速此类模拟时存在局限性，包括缺乏原子分辨率、难以外推到更大系统以及无法同时预测多个属性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够加速纳米尺度电子输运模拟的深度学习方法，解决现有AI方法的局限性，实现多属性预测，并能够从小系统外推到更大系统。&lt;h4&gt;方法&lt;/h4&gt;引入DeepQT框架，结合图神经网络和transformer架构，通过学习NEGF-DFT的关键中间量(平衡哈密顿量和非平衡总势差)来重建哈密顿量，利用电子近视原理实现从小训练系统到更大系统的推广。&lt;h4&gt;主要发现&lt;/h4&gt;在石墨烯、MoS2和硅二极管(具有不同缺陷和掺杂剂)的基准测试中，DeepQT达到了第一性原理精度，同时将计算成本降低了几个数量级。&lt;h4&gt;结论&lt;/h4&gt;DeepQT是一个可扩展、可转移的框架，推进了AI辅助的量子输运研究，为下一代纳米电子器件设计提供了强大工具。&lt;h4&gt;翻译&lt;/h4&gt;非平衡格林函数方法结合密度泛函理论(NEGF-DFT)为纳米尺度电子输运模拟提供了严格的框架，但其计算成本随系统规模急剧增加。最近的人工智能方法试图加速此类模拟，但大多数依赖传统机器学习，缺乏原子分辨率，难以外推到更大系统，并且无法同时预测多个属性。在此我们引入DeepQT，一个深度学习框架，集成了图神经网络和transformer架构，实现了电子结构和输运的多属性预测，无需手动特征工程。通过学习NEGF-DFT的关键中间量——平衡哈密顿量和非平衡总势差，DeepQT重建了平衡和偏置条件下的哈密顿量，从而获得准确的输运预测。利用电子近视原理，DeepQT能够从小训练系统高保真地推广到更大系统。在石墨烯、MoS2和硅二极管(具有不同缺陷和掺杂剂)上的基准测试表明，DeepQT实现了第一性原理精度，同时将计算成本降低了几个数量级。这种可扩展、可转移的框架推动了AI辅助的量子输运发展，为下一代纳米电子器件设计提供了强大工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The non-equilibrium Green's function method combined with density functionaltheory (NEGF-DFT) provides a rigorous framework for simulating nanoscaleelectronic transport, but its computational cost scales steeply with systemsize. Recent artificial intelligence (AI) approaches have sought to acceleratesuch simulations, yet most rely on conventional machine learning, lack atomicresolution, struggle to extrapolate to larger systems, and cannot predictmultiple properties simultaneously. Here we introduce DeepQT, a deep-learningframework that integrates graph neural networks with transformer architecturesto enable multi-property predictions of electronic structure and transportwithout manual feature engineering. By learning key intermediate quantities ofNEGF-DFT, the equilibrium Hamiltonian and the non-equilibrium total potentialdifference, DeepQT reconstructs Hamiltonians under both equilibrium and biasconditions, yielding accurate transport predictions. Leveraging the principleof electronic nearsightedness, DeepQT generalizes from small training systemsto much larger ones with high fidelity. Benchmarks on graphene, MoS2, andsilicon diodes with varied defects and dopants show that DeepQT achievesfirst-principles accuracy while reducing computational cost by orders ofmagnitude. This scalable, transferable framework advances AI-assisted quantumtransport, offering a powerful tool for next-generation nanoelectronic devicedesign.</description>
      <author>example@mail.com (Zili Tang, Xiaoxin Xie, Guanwen Yao, Ligong Zhang, Xiaoyan Liu, Xing Zhang, Liu Fei)</author>
      <guid isPermaLink="false">2510.16878v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning</title>
      <link>http://arxiv.org/abs/2510.16824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProtoMol是一种原型引导的多模态框架，通过双分支层次编码器和层次双向跨模态注意力机制，实现分子图和文本描述之间的细粒度集成和一致语义对齐，解决了现有方法在跨模态交互和原型空间方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态分子表示学习通过联合建模分子图和文本描述，整合结构和语义信息，提高药物毒性、生物活性和理化性质的预测准确性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态方法的两个关键局限性：层次语义依赖被忽略和缺乏统一的原型空间，实现更稳健的跨模态对齐。&lt;h4&gt;方法&lt;/h4&gt;ProtoMol采用双分支层次编码器（图神经网络处理分子图，Transformer编码文本），引入层次双向跨模态注意力机制逐层对齐语义特征，并构建共享原型空间引导模态向一致且具有区分性的表示发展。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上，ProtoMol在各种分子性质预测任务中持续优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;ProtoMol通过细粒度的跨模态集成和一致的语义对齐，有效提升了分子性质预测的准确性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;多模态分子表示学习通过联合建模分子图和它们的文本描述，通过整合结构和语义信息，能够更稳健可靠地预测药物毒性、生物活性和理化性质，从而提高预测准确性和可解释性。然而，现有的多模态方法存在两个关键局限性：(1)它们通常只在最终编码器层执行跨模态交互，从而忽略了层次语义依赖；(2)它们缺乏统一的原型空间来实现模态间的稳健对齐。为了解决这些局限性，我们提出了ProtoMol，一种原型引导的多模态框架，能够实现分子图和文本描述之间的细粒度集成和一致语义对齐。ProtoMol采用双分支层次编码器，利用图神经网络处理结构化分子图，使用Transformer编码非结构化文本，从而生成全面的层次化表示。然后，ProtoMol引入了一种层次双向跨模态注意力机制，逐层对齐跨层的语义特征。此外，还构建了一个共享原型空间，包含可学习的、类别特定的锚点，引导两种模态向连贯且具有区分性的表示发展。在多个基准数据集上的广泛实验表明，在各种分子性质预测任务中，ProtoMol持续优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal molecular representation learning, which jointly models moleculargraphs and their textual descriptions, enhances predictive accuracy andinterpretability by enabling more robust and reliable predictions of drugtoxicity, bioactivity, and physicochemical properties through the integrationof structural and semantic information. However, existing multimodal methodssuffer from two key limitations: (1) they typically perform cross-modalinteraction only at the final encoder layer, thus overlooking hierarchicalsemantic dependencies; (2) they lack a unified prototype space for robustalignment between modalities. To address these limitations, we proposeProtoMol, a prototype-guided multimodal framework that enables fine-grainedintegration and consistent semantic alignment between molecular graphs andtextual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,utilizing Graph Neural Networks to process structured molecular graphs andTransformers to encode unstructured texts, resulting in comprehensivelayer-wise representations. Then, ProtoMol introduces a layer-wisebidirectional cross-modal attention mechanism that progressively alignssemantic features across layers. Furthermore, a shared prototype space withlearnable, class-specific anchors is constructed to guide both modalitiestoward coherent and discriminative representations. Extensive experiments onmultiple benchmark datasets demonstrate that ProtoMol consistently outperformsstate-of-the-art baselines across a variety of molecular property predictiontasks.</description>
      <author>example@mail.com (Yingxu Wang, Kunyu Zhang, Jiaxin Huang, Nan Yin, Siwei Liu, Eran Segal)</author>
      <guid isPermaLink="false">2510.16824v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network for Unified Electronic and Interatomic Potentials: Strain-tunable Electronic Structures in 2D Materials</title>
      <link>http://arxiv.org/abs/2510.16605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UEIPNet，一种等变图神经网络，用于预测原子结构的原子间势和紧束缚哈密顿量，实现物理上一致的机械-电子响应耦合建模，具有接近DFT的精度。&lt;h4&gt;背景&lt;/h4&gt;在原子结构模拟中，需要能够同时准确描述机械和电子响应的方法，传统DFT计算准确但计算成本高，而经典力场方法无法准确描述电子效应。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时预测原子间势和紧束缚哈密顿量的神经网络模型，实现物理一致的机械-电子响应耦合建模，并具有接近DFT的精度。&lt;h4&gt;方法&lt;/h4&gt;UEIPNet是一种等变图神经网络，使用密度泛函理论计算并结合Wannier投影进行训练，预测节点级别的能量和力作为目标，以及Wannier投影的TB矩阵作为边级别目标，在双层石墨烯和单层MoS2的DFT数据上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;在扭曲双层石墨烯中，UEIPNet揭示了层间间距、面内应变和平面外起伏如何驱动孤立平带的形成，并显示调制基底相互作用强度可以在非魔角处产生平带；对于单层MoS2，UEIPNet准确重现了声子色散、应变相关的带隙演化以及非均匀应变下的局部态密度调制。&lt;h4&gt;结论&lt;/h4&gt;UEIPNet提供了一个通用、高效且可扩展的框架，用于研究大规模原子系统中的变形-电子耦合，桥接了经典原子模拟和电子结构计算。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了UEIPNet，一种等变图神经网络，专为预测原子结构的原子间势和紧束缚哈密顿量而设计。UEIPNet使用密度泛函理论计算结合Wannier投影进行训练，预测能量和力作为节点级别目标，Wannier投影的TB矩阵作为边级别目标。这实现了物理上一致的机械-电子响应耦合建模，具有接近DFT的精度。在双层石墨烯和单层MoS2的DFT数据上训练后，UEIPNet捕捉了关键的变形-电子效应：在扭曲双层石墨烯中，它揭示了层间间距、面内应变和平面外起伏如何驱动孤立平带的形成，并进一步表明调制基底相互作用强度可以在非魔角处产生平带。对于单层MoS2，UEIPNet准确重现了声子色散、应变相关的带隙演化以及非均匀应变下的局部态密度调制。UEIPNet为研究大规模原子系统中的变形-电子耦合提供了一个通用、高效且可扩展的框架，桥接了经典原子模拟和电子结构计算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce UEIPNet, an equivariant graph neural network designed to predictboth interatomic potentials and tight-binding (TB) Hamiltonians for an atomicstructure. The UEIPNet is trained using density functional theory calculationsfollowed by Wannier projection to predict energies and forces as node-leveltargets and Wannier-projected TB matrices as edge-level targets. This enablesphysically consistent modeling of coupled mechanical electronic responses withnear-DFT accuracy. Trained on bilayer graphene and monolayer MoS2 DFT data,UEIPNet captures key deformation-electronic effects: in twisted bilayergraphene, it reveals how interlayer spacing, in-plane strain, and out-of-planecorrugation drive isolated flat-band formation, and further shows thatmodulating substrate interaction strength can generate flat bands even awayfrom the magic angle. For monolayer MoS2, the UEIPNet accurately reproducesphonon dispersions, strain-dependent band-gap evolution, and local density ofstates modulations under non-uniform strain. The UEIPNet offers a generalized,efficient, and scalable framework for studying deformation-electronic couplingin large-scale atomistic systems, bridging classical atomistic simulations andelectronic-structure calculations.</description>
      <author>example@mail.com (Moon-ki Choi, Daniel Palmer, Harley T. Johnson)</author>
      <guid isPermaLink="false">2510.16605v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations</title>
      <link>http://arxiv.org/abs/2510.16591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了深度学习模型中参数对称性和网络表达能力对泛化行为的影响，特别是在学习实空间重正化群变换时。研究发现对称性约束和表达能力之间存在竞争，过度复杂或受限的模型泛化能力较差。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型通过多层表示学习结构化数据特征非常成功。将物理对称性编码到模型中可提高困难任务性能，且参数对称性破坏和恢复原则被视为其分层学习动力学的统一机制。&lt;h4&gt;目的&lt;/h4&gt;评估参数对称性和网络表达能力在神经网络学习实空间重正化群(RG)变换时对泛化行为的作用，使用中心极限定理(CLT)作为测试案例映射。&lt;h4&gt;方法&lt;/h4&gt;研究简单多层感知器(MLPs)和图神经网络(GNNs)，在不同架构中改变权重对称性和激活函数。通过将CLT重新表述为累积量递归关系并利用既定框架分析MLPs的泛化行为，并验证该框架从MLPs到GNNs的扩展。&lt;h4&gt;主要发现&lt;/h4&gt;对称性约束和表达能力之间存在竞争，过于复杂或过度受限的模型泛化能力较差。分析揭示了这些复杂模型执行的信息处理过程。&lt;h4&gt;结论&lt;/h4&gt;这些发现为对称网络的学习动态提供了新见解，揭示了它们在建模结构化物理转换方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型已被证明在使用多层表示学习结构化数据的相关特征方面极为成功。将这些模型中的物理对称性编码可以提高困难任务上的性能，最近的工作提出了参数对称性破坏和恢复的原则，作为其分层学习动力学的统一机制。我们评估了参数对称性和网络表达能力在神经网络学习实空间重正化群(RG)变换时的泛化行为中的作用，使用中心极限定理(CLT)作为测试案例映射。我们考虑了简单的多层感知器(MLPs)和图神经网络(GNNs)，并在不同架构中改变权重对称性和激活函数。我们的结果表明对称性约束和表达能力之间存在竞争，过于复杂或过度受限的模型泛化能力较差。我们通过将CLT重新表述为累积量递归关系，并利用既定框架通过MLPs传播累积量，分析性地证明了某些受限MLP架构的这种 poor generalisation 行为。我们还经验性地验证了该框架从MLPs到GNNs的扩展，阐明了这些更复杂模型执行的信息处理过程。这些发现为对称网络的学习动态提供了新的见解，以及它们在建模结构化物理转换方面的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have proven enormously successful at using multiplelayers of representation to learn relevant features of structured data.Encoding physical symmetries into these models can improve performance ondifficult tasks, and recent work has motivated the principle of parametersymmetry breaking and restoration as a unifying mechanism underlying theirhierarchical learning dynamics. We evaluate the role of parameter symmetry andnetwork expressivity in the generalisation behaviour of neural networks whenlearning a real-space renormalisation group (RG) transformation, using thecentral limit theorem (CLT) as a test case map. We consider simple multilayerperceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetriesand activation functions across architectures. Our results reveal a competitionbetween symmetry constraints and expressivity, with overly complex oroverconstrained models generalising poorly. We analytically demonstrate thispoor generalisation behaviour for certain constrained MLP architectures byrecasting the CLT as a cumulant recursion relation and making use of anestablished framework to propagate cumulants through MLPs. We also empiricallyvalidate an extension of this framework from MLPs to GNNs, elucidating theinternal information processing performed by these more complex models. Thesefindings offer new insight into the learning dynamics of symmetric networks andtheir limitations in modelling structured physical transformations.</description>
      <author>example@mail.com (Cassidy Ashworth, Pietro Liò, Francesco Caso)</author>
      <guid isPermaLink="false">2510.16591v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching</title>
      <link>http://arxiv.org/abs/2510.16438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCVW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级的点和线段匹配器LightGlueStick，通过注意力线条消息传递(ALMP)实现高效通信，在基准测试中达到最先进水平。&lt;h4&gt;背景&lt;/h4&gt;线条和点是互补的局部特征，在SLAM和运动恢复结构等应用中有效。传统方法将点和线匹配视为独立任务，而GlueStick虽通过联合匹配降低计算复杂度，但架构过于复杂难以实时应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级的点和线段匹配器，适用于实时应用和边缘设备部署。&lt;h4&gt;方法&lt;/h4&gt;提出LightGlueStick，引入注意力线条消息传递(ALMP)组件，明确向网络暴露线条连接性，实现节点间高效通信。&lt;h4&gt;主要发现&lt;/h4&gt;LightGlueStick在不同基准测试中建立了新的最先进水平，同时保持了轻量级架构。&lt;h4&gt;结论&lt;/h4&gt;LightGlueStick实现了高效且轻量的点和线段匹配，适合实时应用和边缘设备部署。&lt;h4&gt;翻译&lt;/h4&gt;线条和点是互补的局部特征，它们的组合已被证明在SLAM和运动恢复结构等应用中有效。这些流程的核心是局部特征匹配器，用于在图像之间建立对应关系。传统上，点和线匹配被视为独立任务。最近，GlueStick提出了一种基于GNN的网络，同时处理点和线以建立匹配。虽然单一联合运行降低了整体计算复杂度，但复杂的架构阻碍了实时应用或边缘设备的部署。受点匹配最新进展的启发，我们提出了LightGlueStick，一种用于点和线段的轻量级匹配器。我们架构中的关键新颖组件是注意力线条消息传递(ALMP)，它明确地向网络暴露线条的连接性，允许节点之间进行高效通信。在彻底的实验中，我们表明LightGlueStick在不同基准测试中建立了新的最先进水平。代码可在https://github.com/aubingazhib/LightGlueStick获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lines and points are complementary local features, whose combination hasproven effective for applications such as SLAM and Structure-from-Motion. Thebackbone of these pipelines are the local feature matchers, establishingcorrespondences across images. Traditionally, point and line matching have beentreated as independent tasks. Recently, GlueStick proposed a GNN-based networkthat simultaneously operates on points and lines to establish matches. Whilerunning a single joint matching reduced the overall computational complexity,the heavy architecture prevented real-time applications or deployment to edgedevices.  Inspired by recent progress in point matching, we propose LightGlueStick, alightweight matcher for points and line segments. The key novel component inour architecture is the Attentional Line Message Passing (ALMP), whichexplicitly exposes the connectivity of the lines to the network, allowing forefficient communication between nodes. In thorough experiments we show thatLightGlueStick establishes a new state-of-the-art across different benchmarks.The code is available at https://github.com/aubingazhib/LightGlueStick.</description>
      <author>example@mail.com (Aidyn Ubingazhibov, Rémi Pautrat, Iago Suárez, Shaohui Liu, Marc Pollefeys, Viktor Larsson)</author>
      <guid isPermaLink="false">2510.16438v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites</title>
      <link>http://arxiv.org/abs/2510.16083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Elsevier Expert Systems with Applications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PassREfinder-FL的新框架，用于预测跨网站的凭证填充风险，解决了现有方法在可用性和实际部署方面的问题。&lt;h4&gt;背景&lt;/h4&gt;凭证填充攻击对经常在多个网站重复使用密码的在线用户造成了重大伤害。先前的研究方法虽然试图检测重复使用密码的用户或识别恶意登录尝试，但通常通过限制密码创建或网站访问影响可用性，且依赖复杂的账户共享机制，阻碍了实际部署。&lt;h4&gt;目的&lt;/h4&gt;提出PassREfinder-FL框架，预测跨网站的凭证填充风险，解决现有方法的局限性，同时保护用户隐私并提高可用性。&lt;h4&gt;方法&lt;/h4&gt;引入密码重用关系的概念，将其表示为网站图中的边；使用图神经网络(GNNs)执行链接预测任务，评估网站间的凭证重用风险；整合公共网站信息使方法可扩展到大量网站；采用联邦学习(FL)方法保护用户隐私，避免共享敏感信息。&lt;h4&gt;主要发现&lt;/h4&gt;在包含22,378个网站的3.6亿个泄露账户的真实数据集上评估，PassREfinder-FL在FL设置中实现了0.9153的F1分数；基于FL的GNN比其他最先进的GNN模型性能提升4-11%；预测结果可用于量化密码重用可能性，作为可操作的风险分数。&lt;h4&gt;结论&lt;/h4&gt;PassREfinder-FL是一个有效的框架，能够在保护用户隐私的同时准确预测跨网站的凭证填充风险，其预测结果可作为风险分数使用，帮助量化密码重用风险。&lt;h4&gt;翻译&lt;/h4&gt;凭证填充攻击对经常在多个网站重复使用密码的在线用户造成了重大伤害。虽然先前的研究试图检测有重复使用密码的用户或识别恶意登录尝试，但现有方法通常通过限制密码创建或网站访问来影响可用性，且它们依赖复杂的账户共享机制，阻碍了实际部署。为解决这些局限性，我们提出了PassREfinder-FL，一个预测跨网站凭证填充风险的新框架。我们引入了密码重用关系的概念——定义为用户在网站间重用密码的可能性——并将其表示为网站图中的边。使用图神经网络(GNNs)，我们执行链接预测任务来评估网站之间的凭证重用风险。我们的方法通过整合公共网站信息并将新观察到的网站作为图中的节点链接，可扩展到大量任意网站。为了保护用户隐私，我们使用联邦学习(FL)方法扩展了PassREfinder-FL，消除了跨管理员共享用户敏感信息的需求。在包含22,378个网站的3.6亿个泄露账户的真实世界数据集上的评估显示，PassREfinder-FL在FL设置中实现了0.9153的F1分数。我们进一步通过消融研究验证，基于FL的GNN比其他最先进的GNN模型实现了4-11%的性能提升。最后，我们证明预测结果可用于量化密码重用可能性，作为可操作的风险分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Credential stuffing attacks have caused significant harm to online users whofrequently reuse passwords across multiple websites. While prior research hasattempted to detect users with reused passwords or identify malicious loginattempts, existing methods often compromise usability by restricting passwordcreation or website access, and their reliance on complex account-sharingmechanisms hinders real-world deployment. To address these limitations, wepropose PassREfinder-FL, a novel framework that predicts credential stuffingrisks across websites. We introduce the concept of password reuse relations --defined as the likelihood of users reusing passwords between websites -- andrepresent them as edges in a website graph. Using graph neural networks (GNNs),we perform a link prediction task to assess credential reuse risk betweensites. Our approach scales to a large number of arbitrary websites byincorporating public website information and linking newly observed websites asnodes in the graph. To preserve user privacy, we extend PassREfinder-FL with afederated learning (FL) approach that eliminates the need to share usersensitive information across administrators. Evaluation on a real-world datasetof 360 million breached accounts from 22,378 websites shows thatPassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We furthervalidate that our FL-based GNN achieves a 4-11% performance improvement overother state-of-the-art GNN models through an ablation study. Finally, wedemonstrate that the predicted results can be used to quantify password reuselikelihood as actionable risk scores.</description>
      <author>example@mail.com (Jaehan Kim, Minkyoo Song, Minjae Seo, Youngjin Jin, Seungwon Shin, Jinwoo Kim)</author>
      <guid isPermaLink="false">2510.16083v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions</title>
      <link>http://arxiv.org/abs/2510.16064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于残差学习的电力系统优化方法，通过结合快速直流最优潮流解和图神经网络，解决了交流最优潮流计算效率低下的问题，实现了比传统方法更快的计算速度和更高的准确性。&lt;h4&gt;背景&lt;/h4&gt;解决非线性交流最优潮流(AC OPF)问题是实时电网运行中的主要计算瓶颈，传统AC OPF求解器计算复杂度高，难以满足实时决策需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可扩展的方法，能够快速提供交流可行的最优潮流解，以支持近实时电网运行决策。&lt;h4&gt;方法&lt;/h4&gt;提出残差学习范式，使用直流最优潮流解作为基线，通过拓扑感知图神经网络结合局部注意力和两级直流特征集成，学习非线性修正项，并采用物理信息损失函数强制执行交流潮流可行性和运行限制。&lt;h4&gt;主要发现&lt;/h4&gt;在57、118和2000总线系统上的评估表明，与传统AC OPF求解器相比，均方误差降低约25%，可行性误差减少高达3倍，运行时间加速高达13倍。模型在N-1 contingencies情况下保持准确性，并能高效扩展到大型网络。&lt;h4&gt;结论&lt;/h4&gt;残差学习是线性近似和交流可行最优潮流之间的实用且可扩展的桥梁，能够实现近实时运行决策。&lt;h4&gt;翻译&lt;/h4&gt;解决非线性交流最优潮流(AC OPF)问题仍然是实时电网运行的主要计算瓶颈。在本文中，我们提出了一种残差学习范式，使用快速的直流最优潮流(DC OPF)解作为基线，并学习仅提供完整AC-OPF解决方案所需的非线性修正。该方法利用了具有局部注意力和两级直流特征集成的拓扑感知图神经网络，使用强制执行交流潮流可行性和运行限制的物理信息损失函数进行训练。在57、118和2000总线系统上的OPFData评估显示，与传统AC OPF求解器相比，均方误差(MSE)降低约25%，可行性误差减少高达3倍，运行时间加速高达13倍。该模型在N-1 contingencies情况下保持准确性，并能高效扩展到大型网络。这些结果表明，残差学习是线性近似和交流可行最优潮流之间实用且可扩展的桥梁，能够实现近实时运行决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving the nonlinear AC optimal power flow (AC OPF) problem remains a majorcomputational bottleneck for real-time grid operations. In this paper, wepropose a residual learning paradigm that uses fast DC optimal power flow (DCOPF) solutions as a baseline, and learns only the nonlinear correctionsrequired to provide the full AC-OPF solution. The method utilizes atopology-aware Graph Neural Network with local attention and two-level DCfeature integration, trained using a physics-informed loss that enforces ACpower-flow feasibility and operational limits. Evaluations on OPFData for 57-,118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction infeasibility error, and up to 13X runtime speedup compared to conventional ACOPF solvers. The model maintains accuracy under N-1 contingencies and scalesefficiently to large networks. These results demonstrate that residual learningis a practical and scalable bridge between linear approximations andAC-feasible OPF, enabling near real-time operational decision making.</description>
      <author>example@mail.com (Muhy Eddin Za'ter, Bri-Mathias Hodge, Kyri Baker)</author>
      <guid isPermaLink="false">2510.16064v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks</title>
      <link>http://arxiv.org/abs/2510.16063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于变电站级电压估计的分层图神经网络方法，能够处理配电网中常见的低可观测性水平，并在实验中展现出比其他数据驱动模型更优的性能。&lt;h4&gt;背景&lt;/h4&gt;配电网中准确的电压估计对实时监控和提高电网可靠性至关重要。随着分布式能源渗透率和配电层电压变化性的增加，稳健的配电网状态估计(DSSE)对保持安全高效运行更加重要。然而，传统DSSE技术在处理稀疏测量和现代馈线规模方面存在困难，限制了它们在大规模网络中的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用电气拓扑和物理特征的分层图神经网络，用于变电站级电压估计，同时对现实配电网中常见的低可观测性水平保持稳健。&lt;h4&gt;方法&lt;/h4&gt;利用公开的SMART-DS数据集，在多个变电站和DER渗透场景的数千个总线上的模型进行训练和评估。提出了一种分层图神经网络方法，该方法能够有效处理配电网的拓扑结构和物理特性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法比其他数据驱动模型的RMSE低达2倍，即使在只有1%的测量覆盖率的情况下，也能保持高精度。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了图神经网络在实现可扩展、可重现和数据驱动的配电系统电压监控方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;配电网中的准确电压估计对实时监控和提高电网可靠性至关重要。随着分布式能源渗透率和配电层电压变化性的增加，稳健的配电网状态估计(DSSE)对保持安全高效运行变得更加必要。然而，传统的DSSE技术在处理稀疏测量和现代馈线规模方面存在困难，限制了它们在大规模网络中的可扩展性。本文提出了一种用于变电站级电压估计的分层图神经网络，它利用电气拓扑和物理特征，同时对现实配电网中常见的低可观测性水平保持稳健。利用公开的SMART-DS数据集，该模型在多个变电站和DER渗透场景的数千个总线上进行了训练和评估。全面的实验表明，所提出的方法比其他数据驱动模型的RMSE低达2倍，并且在只有1%的测量覆盖率的情况下仍能保持高精度。这些结果突出了图神经网络在实现可扩展、可重现和数据驱动的配电系统电压监控方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate voltage estimation in distribution networks is critical forreal-time monitoring and increasing the reliability of the grid. As DERpenetration and distribution level voltage variability increase, robustdistribution system state estimation (DSSE) has become more essential tomaintain safe and efficient operations. Traditional DSSE techniques, however,struggle with sparse measurements and the scale of modern feeders, limitingtheir scalability to large networks. This paper presents a hierarchical graphneural network for substation-level voltage estimation that exploits bothelectrical topology and physical features, while remaining robust to the lowobservability levels common to real-world distribution networks. Leveraging thepublic SMART-DS datasets, the model is trained and evaluated on thousands ofbuses across multiple substations and DER penetration scenarios. Comprehensiveexperiments demonstrate that the proposed method achieves up to 2 times lowerRMSE than alternative data-driven models, and maintains high accuracy with aslittle as 1\% measurement coverage. The results highlight the potential of GNNsto enable scalable, reproducible, and data-driven voltage monitoring fordistribution systems.</description>
      <author>example@mail.com (Muhy Eddin Za'ter, Bri-Mathias Hodge)</author>
      <guid isPermaLink="false">2510.16063v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs</title>
      <link>http://arxiv.org/abs/2510.15188v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the authors' extended version of the paper accepted for  publication at the ACM SIGSAC Conference on Computer and Communications  Security (CCS 2025). The final published version is available at  https://doi.org/10.1145/3719027.3765219&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了OCR-APT系统，用于高级持续性威胁(APTs)的检测和攻击故事重建，通过结合图神经网络和大型语言模型，实现了更准确的异常检测和更具可解释性的攻击报告。&lt;h4&gt;背景&lt;/h4&gt;高级持续性威胁(APTs)是一种隐蔽的网络攻击，通常能逃避系统级审计日志中的检测。现有的图异常检测系统存在高误报率和粗粒度警报问题，且依赖于文件路径或IP地址等节点属性，导致虚假关联，降低了检测的稳健性和可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成准确、类人叙述整个攻击的系统，帮助安全分析师完全理解攻击的进展和影响，提供更可靠、更具可解释性的APT检测方案。&lt;h4&gt;方法&lt;/h4&gt;OCR-APT系统采用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式而非脆弱的属性。然后使用大型语言模型(LLMs)迭代检测到的子图，重建多阶段攻击故事，并在每个阶段进行验证以减少幻觉并确保可解释的最终报告。&lt;h4&gt;主要发现&lt;/h4&gt;在DARPA TC3、OpTC和NODLINK数据集上的评估表明，OCR-APT在检测准确性和警报可解释性方面优于最先进的系统。OCR-APT能够重建类人报告，全面捕获攻击故事。&lt;h4&gt;结论&lt;/h4&gt;OCR-APT通过结合图神经网络和大型语言模型，解决了现有APT检测系统的局限性，提供了更准确、更可靠且更具可解释性的攻击检测和报告方案。&lt;h4&gt;翻译&lt;/h4&gt;高级持续性威胁(APTs)是一种隐蔽的网络攻击，通常能逃避系统级审计日志中的检测。来源图将这些日志建模为连接的实体和事件，揭示了线性日志表示中遗漏的关系。现有系统对这些图应用异常检测，但常常存在高误报率和粗粒度警报的问题。它们对文件路径或IP等节点属性的依赖导致虚假关联，降低了检测的稳健性和可靠性。为了完全理解攻击的进展和影响，安全分析师需要能够生成整个攻击的准确、类人叙述的系统。为了解决这些挑战，我们介绍了OCR-APT，一个用于APT检测和重建类人攻击故事的系统。OCR-APT使用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式而非文件路径或IP等脆弱属性。这种方法带来了更稳健的异常检测。然后使用大型语言模型(LLMs)迭代检测到的子图，重建多阶段攻击故事。每个阶段在继续之前都经过验证，减少了幻觉并确保了可解释的最终报告。我们在DARPA TC3、OpTC和NODLINK数据集上的评估表明，OCR-APT在检测准确性和警报可解释性方面优于最先进的系统。此外，OCR-APT重建的类人报告能够全面捕获攻击故事。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evadedetection in system-level audit logs. Provenance graphs model these logs asconnected entities and events, revealing relationships that are missed bylinear log representations. Existing systems apply anomaly detection to thesegraphs but often suffer from high false positive rates and coarse-grainedalerts. Their reliance on node attributes like file paths or IPs leads tospurious correlations, reducing detection robustness and reliability. To fullyunderstand an attack's progression and impact, security analysts need systemsthat can generate accurate, human-like narratives of the entire attack. Toaddress these challenges, we introduce OCR-APT, a system for APT detection andreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks(GNNs) for subgraph anomaly detection, learning behavior patterns around nodesrather than fragile attributes such as file paths or IPs. This approach leadsto a more robust anomaly detection. It then iterates over detected subgraphsusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.Each stage is validated before proceeding, reducing hallucinations and ensuringan interpretable final report. Our evaluations on the DARPA TC3, OpTC, andNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in bothdetection accuracy and alert interpretability. Moreover, OCR-APT reconstructshuman-like reports that comprehensively capture the attack story.</description>
      <author>example@mail.com (Ahmed Aly, Essam Mansour, Amr Youssef)</author>
      <guid isPermaLink="false">2510.15188v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction</title>
      <link>http://arxiv.org/abs/2510.16035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了首个针对基于图神经网络(GNN)的社交机器人检测器的对抗性多智能体强化学习框架(RoBCtrl)，通过扩散模型生成高保真机器人账户和多智能体强化学习优化攻击策略，实验证明该框架能有效削弱基于GNN的检测器性能。&lt;h4&gt;背景&lt;/h4&gt;社交网络已成为个人获取实时信息的关键来源。社交机器人在这些平台上的影响引起了研究者的广泛关注，导致了许多检测技术的发展。然而，这些检测方法的脆弱性和鲁棒性仍未得到充分探索。由于对社交代理的控制有限、机器人检测器的黑盒特性以及机器人的异构性问题，现有的基于图神经网络(GNN)的方法无法直接应用。&lt;h4&gt;目的&lt;/h4&gt;为应对现有GNN-based社交机器人检测方法面临的挑战，提出首个针对基于GNN的社交机器人检测器的对抗性多智能体强化学习框架，用于社交机器人控制攻击(RoBCtrl)。&lt;h4&gt;方法&lt;/h4&gt;使用扩散模型通过微小修改重构现有账户数据来生成高保真机器人账户；采用多智能体强化学习(MARL)方法模拟机器人的对抗行为；根据影响力和预算对社交账户进行分类；使用不同的智能体控制各类别的机器人账户，通过强化学习优化附着策略；设计基于结构熵的分层状态抽象以加速强化学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;据我们所知，这是首次应用扩散模型有效模拟 evolving social bots 的行为；在社交机器人检测数据集上的大量实验表明，该框架可以有效削弱基于GNN的检测器的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的RoBCtrl框架能够有效对抗基于GNN的社交机器人检测器，通过生成高保真机器人账户和智能化的攻击策略实现了这一目标。&lt;h4&gt;翻译&lt;/h4&gt;社交网络已成为个人获取实时信息的关键来源。社交机器人在这些平台上的影响引起了研究者的广泛关注，导致了许多检测技术的发展。然而，这些检测方法的脆弱性和鲁棒性仍未得到充分探索。由于对社交代理的控制有限、机器人检测器的黑盒特性以及机器人的异构性问题，现有的基于图神经网络(GNN)的方法无法直接应用。为应对这些挑战，本文提出了首个针对基于GNN的社交机器人检测器的对抗性多智能体强化学习框架，用于社交机器人控制攻击(RoBCtrl)。具体而言，我们使用扩散模型通过微小修改重构现有账户数据来生成高保真机器人账户，从而逃避社交平台的检测。据我们所知，这是首次应用扩散模型有效模拟 evolving social bots 的行为。随后，我们采用多智能体强化学习(MARL)方法模拟机器人的对抗行为。我们根据影响力和预算对社交账户进行分类，然后使用不同的智能体控制各类别的机器人账户，通过强化学习优化附着策略。此外，我们还设计了一种基于结构熵的分层状态抽象以加速强化学习。在社交机器人检测数据集上的大量实验表明，我们的框架可以有效削弱基于GNN的检测器的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social networks have become a crucial source of real-time information forindividuals. The influence of social bots within these platforms has garneredconsiderable attention from researchers, leading to the development of numerousdetection technologies. However, the vulnerability and robustness of thesedetection methods is still underexplored. Existing Graph Neural Network(GNN)-based methods cannot be directly applied due to the issues of limitedcontrol over social agents, the black-box nature of bot detectors, and theheterogeneity of bots. To address these challenges, this paper proposes thefirst adversarial multi-agent Reinforcement learning framework for social Botcontrol attacks (RoBCtrl) targeting GNN-based social bot detectors.Specifically, we use a diffusion model to generate high-fidelity bot accountsby reconstructing existing account data with minor modifications, therebyevading detection on social platforms. To the best of our knowledge, this isthe first application of diffusion models to mimic the behavior of evolvingsocial bots effectively. We then employ a Multi-Agent Reinforcement Learning(MARL) method to simulate bots adversarial behavior. We categorize socialaccounts based on their influence and budget. Different agents are thenemployed to control bot accounts across various categories, optimizing theattachment strategy through reinforcement learning. Additionally, ahierarchical state abstraction based on structural entropy is designed toaccelerate the reinforcement learning. Extensive experiments on social botdetection datasets demonstrate that our framework can effectively undermine theperformance of GNN-based detectors.</description>
      <author>example@mail.com (Yingguang Yang, Xianghua Zeng, Qi Wu, Hao Peng, Yutong Xia, Hao Liu, Bin Chong, Philip S. Yu)</author>
      <guid isPermaLink="false">2510.16035v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Plasma Shape Control via Zero-shot Generative Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.17531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合生成对抗模仿学习与希尔伯特空间表征学习的新框架，从历史PID控制数据中开发通用的零样本控制策略，用于等离子体形状控制，无需任务特定微调即可在各种场景下精确稳定地跟踪参考轨迹。&lt;h4&gt;背景&lt;/h4&gt;传统PID控制器在等离子体形状控制方面的适应性有限，而特定任务的强化学习方法存在泛化能力不足和需要重复重新训练的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，从大规模历史PID控制放电数据中开发通用的零样本控制策略，克服传统方法和特定任务强化学习方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;结合生成对抗模仿学习(GAIL)与希尔伯特空间表征学习，实现双重目标：模仿PID数据的稳定操作风格，构建几何结构的潜在空间以实现高效的目标导向控制。&lt;h4&gt;主要发现&lt;/h4&gt;基础策略可以零样本方式部署，无需任务特定的微调；在HL-3托卡马克模拟器上的评估表明，该策略能够精确且稳定地跟踪各种等离子体场景下关键形状参数的参考轨迹。&lt;h4&gt;结论&lt;/h4&gt;这项工作为未来聚变反应堆开发高度灵活和数据高效的智能控制系统提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;传统PID控制器在等离子体形状控制方面的适应性有限，而特定任务的强化学习方法存在泛化能力有限和需要重复重新训练的问题。为克服这些挑战，本文提出了一种新框架，用于从大规模历史PID控制放电数据中开发通用的零样本控制策略。我们的方法将生成对抗模仿学习(GAIL)与希尔伯特空间表征学习相结合，以实现双重目标：模仿PID数据的稳定操作风格，构建几何结构的潜在空间以实现高效的目标导向控制。所得基础策略可以零样本方式部署，用于各种轨迹跟踪任务，无需任务特定的微调。在HL-3托卡马克模拟器上的评估表明，该策略在多种等离子体场景下能够精确且稳定地跟踪关键形状参数的参考轨迹。这项工作为未来聚变反应堆开发高度灵活和数据高效的智能控制系统提供了可行途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional PID controllers have limited adaptability for plasma shapecontrol, and task-specific reinforcement learning (RL) methods suffer fromlimited generalization and the need for repetitive retraining. To overcomethese challenges, this paper proposes a novel framework for developing aversatile, zero-shot control policy from a large-scale offline dataset ofhistorical PID-controlled discharges. Our approach synergistically combinesGenerative Adversarial Imitation Learning (GAIL) with Hilbert spacerepresentation learning to achieve dual objectives: mimicking the stableoperational style of the PID data and constructing a geometrically structuredlatent space for efficient, goal-directed control. The resulting foundationpolicy can be deployed for diverse trajectory tracking tasks in a zero-shotmanner without any task-specific fine-tuning. Evaluations on the HL-3 tokamaksimulator demonstrate that the policy excels at precisely and stably trackingreference trajectories for key shape parameters across a range of plasmascenarios. This work presents a viable pathway toward developing highlyflexible and data-efficient intelligent control systems for future fusionreactors.</description>
      <author>example@mail.com (Niannian Wu, Rongpeng Li, Zongyu Yang, Yong Xiao, Ning Wei, Yihang Chen, Bo Li, Zhifeng Zhao, Wulyu Zhong)</author>
      <guid isPermaLink="false">2510.17531v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning</title>
      <link>http://arxiv.org/abs/2510.17489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DETree是一种新型AI参与文本检测方法，通过层次亲和树结构建模不同文本生成过程间的关系，并引入专门损失函数对齐文本表示。研究团队开发了RealBench基准数据集，显著提升了混合文本检测性能和分布外场景的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;检测AI参与的文本对打击错误信息、剽窃和学术不端行为至关重要。AI文本生成涉及多种协作过程（如AI生成文本由人类编辑、人类文本由AI编辑、AI文本由其他AI优化），不同过程生成的文本具有复杂特征，给检测带来巨大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发更有效的AI参与文本检测方法，准确识别不同人-AI协作过程生成的文本，提高检测性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出DETree方法，将不同文本生成过程间的关系建模为层次亲和树结构，并引入专门损失函数使文本表示与该树结构对齐。同时开发了RealBench基准数据集，自动整合各种人-AI协作过程产生的混合文本。&lt;h4&gt;主要发现&lt;/h4&gt;不同过程生成的文本表示表现出内在聚类关系；DETree方法在混合文本检测任务中提高了性能；显著增强了在分布外场景中的鲁棒性和泛化能力，特别是在少样本学习条件下。&lt;h4&gt;结论&lt;/h4&gt;基于训练的方法在分布外设置中具有潜力，DETree为AI参与文本检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;检测AI参与的文本对于打击错误信息、剽窃和学术不端行为至关重要。然而，AI文本生成包括多样的协作过程（AI生成文本由人类编辑、人类编写文本由AI编辑、AI生成文本由其他AI优化），其中可能涉及各种甚至新的LLM。这些不同过程生成的文本表现出复杂特征，给检测带来巨大挑战。当前方法对这些过程建模过于简单，主要采用二元分类（纯人类vs AI参与）或多分类（将人-AI协作视为新类别）。我们观察到，通过不同过程生成的文本表示表现出内在的聚类关系。因此，我们提出了DETree，一种新方法，将不同过程之间的关系建模为层次亲和树结构，并引入专门的损失函数使文本表示与此树对齐。为此，我们开发了RealBench，一个全面的基准数据集，自动整合通过各种人-AI协作过程产生的混合文本。我们的方法在混合文本检测任务中提高了性能，显著增强了在分布外场景中的鲁棒性和泛化能力，特别是在少样本学习条件下，进一步证明了基于训练的方法在OOD设置中的潜力。我们的代码和数据集可在https://github.com/heyongxin233/DETree获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting AI-involved text is essential for combating misinformation,plagiarism, and academic misconduct. However, AI text generation includesdiverse collaborative processes (AI-written text edited by humans,human-written text edited by AI, and AI-generated text refined by other AI),where various or even new LLMs could be involved. Texts generated through thesevaried processes exhibit complex characteristics, presenting significantchallenges for detection. Current methods model these processes rather crudely,primarily employing binary classification (purely human vs. AI-involved) ormulti-classification (treating human-AI collaboration as a new class). Weobserve that representations of texts generated through different processesexhibit inherent clustering relationships. Therefore, we propose DETree, anovel approach that models the relationships among different processes as aHierarchical Affinity Tree structure, and introduces a specialized lossfunction that aligns text representations with this tree. To facilitate thislearning, we developed RealBench, a comprehensive benchmark dataset thatautomatically incorporates a wide spectrum of hybrid texts produced throughvarious human-AI collaboration processes. Our method improves performance inhybrid text detection tasks and significantly enhances robustness andgeneralization in out-of-distribution scenarios, particularly in few-shotlearning conditions, further demonstrating the promise of training-basedapproaches in OOD settings. Our code and dataset are available athttps://github.com/heyongxin233/DETree.</description>
      <author>example@mail.com (Yongxin He, Shan Zhang, Yixuan Cao, Lei Ma, Ping Luo)</author>
      <guid isPermaLink="false">2510.17489v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition</title>
      <link>http://arxiv.org/abs/2510.17338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种后处理开放集识别方法，通过测量模型特征与预测logit之间的一致性来识别未知类别，无需重新训练预训练模型，在两个数据集上都取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的野生动物分类模型在封闭世界设置下训练，遇到未知类别时仍过于自信预测。&lt;h4&gt;目的&lt;/h4&gt;开发一种开放集识别方法，能够分类已知类别同时拒绝未知样本，无需重新训练预训练模型。&lt;h4&gt;方法&lt;/h4&gt;提出基于输入到最近类别均值(NCM)距离的概率分布，并与logit空间的softmax概率比较，测量NCM与分类头之间的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在两个评估数据集上排名前三，性能一致；在非洲和瑞典动物数据集上分别实现了93.41和95.35的AUROC。&lt;h4&gt;结论&lt;/h4&gt;该方法作为后处理技术应用，无需重新训练预训练模型，在开放集识别任务中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;当前最先进的野生动物分类模型是在封闭世界设置下训练的。当遇到未知类别时，它们对自己的预测仍然过于自信。开放集识别(OSR)旨在分类已知类别同时拒绝未知样本。本研究提出了一种后处理OSR方法，用于测量模型特征和预测logit之间的一致性。我们提出了一种基于输入到其最近类别均值(NCM)距离的概率分布。然后将基于NCM的分布与logit空间中的softmax概率进行比较，以测量NCM和分类头之间的一致性。所提出的策略在两个评估数据集中排名前三，且在两个数据集上表现一致。相比之下，当前最先进的方法在单个数据集上表现出色。我们在非洲和瑞典动物数据集上分别实现了93.41和95.35的AUROC。代码可在https://github.com/Applied-Representation-Learning-Lab/OSR找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current state-of-the-art Wildlife classification models are trained under theclosed world setting. When exposed to unknown classes, they remainoverconfident in their predictions. Open-set Recognition (OSR) aims to classifyknown classes while rejecting unknown samples. Several OSR methods have beenproposed to model the closed-set distribution by observing the feature, logit,or softmax probability space. A significant drawback of many existingapproaches is the requirement to retrain the pre-trained classification modelwith the OSR-specific strategy. This study contributes a post-processing OSRmethod that measures the agreement between the models' features and predictedlogits. We propose a probability distribution based on an input's distance toits Nearest Class Mean (NCM). The NCM-based distribution is then compared withthe softmax probabilities from the logit space to measure agreement between theNCM and the classification head. Our proposed strategy ranks within the topthree on two evaluated datasets, showing consistent performance across the twodatasets. In contrast, current state-of-the-art methods excel on a singledataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedishanimals. The code can be foundhttps://github.com/Applied-Representation-Learning-Lab/OSR.</description>
      <author>example@mail.com (Jiahao Huo, Mufhumudzi Muthivhi, Terence L. van Zyl, Fredrik Gustafsson)</author>
      <guid isPermaLink="false">2510.17338v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery</title>
      <link>http://arxiv.org/abs/2510.17188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted at NeurIPS (2025) Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HIDISC是一种双曲表示学习框架，用于解决领域泛化广义类别发现问题，无需片段模拟即可实现领域和类别级别的泛化。&lt;h4&gt;背景&lt;/h4&gt;广义类别发现(GCD)旨在对测试样本进行分类，分为训练期间可见的类别或新类别，无需标签监督。现有GCD方法假设训练期间可以同时访问有标签和无标签数据，且来自同一领域，限制了其在涉及分布偏移的开放世界场景中的应用。DG-GCD要求模型泛化到包含新类别的未见领域，且在训练期间不访问目标领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法，使模型能够泛化到未见领域中的新类别，同时避免现有方法的高计算成本和错误累积问题。&lt;h4&gt;方法&lt;/h4&gt;HIDISC框架使用GPT引导的扩散增强源领域，引入Tangent CutMix在切线空间中合成伪新样本，采用统一损失函数(惩罚Busemann对齐、混合双曲对比正则化和自适应异常值排斥)，并使用可学习的曲率参数使几何结构适应数据集复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;HIDISC在PACS、Office-Home和DomainNet数据集上实现了最先进的结果，一致性地优于现有的欧几里得和双曲(DG)-GCD基线。&lt;h4&gt;结论&lt;/h4&gt;HIDISC通过双曲表示学习框架有效解决了领域泛化广义类别发现问题，无需片段模拟即可实现领域和类别级别的泛化，同时保持了高效率。&lt;h4&gt;翻译&lt;/h4&gt;广义类别发现(GCD)旨在对测试样本进行分类，分为训练期间可见的类别或新类别，无需依赖标签监督。大多数现有GCD方法假设在训练期间可以同时访问有标签和无标签数据，并且来自同一领域，这限制了其在涉及分布偏移的开放世界场景中的应用。带有GCD的领域泛化(DG-GCD)通过要求模型泛化到包含新类别的未见领域，且在训练期间不访问目标领域数据，从而消除了这一限制。唯一的现有DG-GCD方法DG2CD-Net依赖于多合成领域的片段训练和任务向量聚合，导致高计算成本和错误累积。我们提出了HIDISC，一种双曲表示学习框架，无需片段模拟即可实现领域和类别级别的泛化。为了使模型接触到最小但多样化的领域变化，我们使用GPT引导的扩散增强源领域，避免过拟合并保持效率。为了构建表示空间，我们引入了Tangent CutMix，这是一种曲率感知的插值方法，在切线空间中合成伪新样本，保持流形一致性。统一的损失函数(结合惩罚Busemann对齐、混合双曲对比正则化和自适应异常值排斥)促进了紧凑、语义结构化的嵌入。可学习的曲率参数进一步使几何结构适应数据集的复杂性。HIDISC在PACS、Office-Home和DomainNet上实现了最先进的结果，一致性地优于现有的欧几里得和双曲(DG)-GCD基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized Category Discovery (GCD) aims to classify test-time samples intoeither seen categories** -- available during training -- or novel ones, withoutrelying on label supervision. Most existing GCD methods assume simultaneousaccess to labeled and unlabeled data during training and arising from the samedomain, limiting applicability in open-world scenarios involving distributionshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint byrequiring models to generalize to unseen domains containing novel categories,without accessing targetdomain data during training. The only prior DG-GCDmethod, DG2CD-Net, relies on episodic training with multiple synthetic domainsand task vector aggregation, incurring high computational cost and erroraccumulation. We propose HIDISC, a hyperbolic representation learning frameworkthat achieves domain and category-level generalization without episodicsimulation. To expose the model to minimal but diverse domain variations, weaugment the source domain using GPT-guided diffusion, avoiding overfittingwhile maintaining efficiency. To structure the representation space, weintroduce Tangent CutMix, a curvature-aware interpolation that synthesizespseudo-novel samples in tangent space, preserving manifold consistency. Aunified loss -- combining penalized Busemann alignment, hybrid hyperboliccontrastive regularization, and adaptive outlier repulsion -- **facilitatescompact, semantically structured embeddings. A learnable curvature parameterfurther adapts the geometry to dataset complexity. HIDISC achievesstate-of-the-art results on PACS , Office-Home , and DomainNet, consistentlyoutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.</description>
      <author>example@mail.com (Vaibhav Rathore, Divyam Gupta, Biplab Banerjee)</author>
      <guid isPermaLink="false">2510.17188v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DFNN: A Deep Fréchet Neural Network Framework for Learning Metric-Space-Valued Responses</title>
      <link>http://arxiv.org/abs/2510.17072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了深度Fréchet神经网络（DFNNs）框架，用于从欧几里得预测变量预测非欧几里得响应变量，并通过理论证明和实证研究展示了其优越性。&lt;h4&gt;背景&lt;/h4&gt;非欧几里得响应变量（如概率分布、网络、对称正定矩阵和组合）的回归在现代应用中变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种端到端的深度学习框架，用于预测被视为度量空间中随机对象的非欧几里得响应变量。&lt;h4&gt;方法&lt;/h4&gt;深度Fréchet神经网络（DFNNs），利用深度神经网络的表示学习能力来近似给定预测变量的响应的条件Fréchet均值，通过最小化Fréchet风险实现。&lt;h4&gt;主要发现&lt;/h4&gt;建立了DFNNs的通用近似定理，将神经网络近似理论推进到一般度量空间值响应，无需模型假设或局部平滑；在多种应用场景中，DFNNs始终优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;DFNNs是一种高度灵活的框架，能够适应不同的度量和高维预测变量，为非欧几里得响应变量的回归提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;回归非欧几里得响应变量——例如概率分布、网络、对称正定矩阵和组合——在现代应用中已变得越来越重要。在本文中，我们提出了深度Fréchet神经网络（DFNNs），一个用于从欧几里得预测变量预测非欧几里得响应变量的端到端深度学习框架——这些响应被视为度量空间中的随机对象。我们的方法利用深度神经网络的表示学习能力，通过最小化Fréchet风险来近似给定预测变量的响应的条件Fréchet均值——这是条件期望的度量空间类比。该框架非常灵活，能够适应不同的度量和高维预测变量。我们建立了DFNNs的通用近似定理，将神经网络近似理论的最先进水平推进到一般度量空间值响应，无需做出模型假设或依赖局部平滑。在合成分布和网络值响应以及预测就业职业构成的真实世界应用中的实证研究表明，DFNNs始终优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Regression with non-Euclidean responses -- e.g., probability distributions,networks, symmetric positive-definite matrices, and compositions -- has becomeincreasingly important in modern applications. In this paper, we propose deepFr\'echet neural networks (DFNNs), an end-to-end deep learning framework forpredicting non-Euclidean responses -- which are considered as random objects ina metric space -- from Euclidean predictors. Our method leverages therepresentation-learning power of deep neural networks (DNNs) to the task ofapproximating conditional Fr\'echet means of the response given the predictors,the metric-space analogue of conditional expectations, by minimizing aFr\'echet risk. The framework is highly flexible, accommodating diverse metricsand high-dimensional predictors. We establish a universal approximation theoremfor DFNNs, advancing the state-of-the-art of neural network approximationtheory to general metric-space-valued responses without making modelassumptions or relying on local smoothing. Empirical studies on syntheticdistributional and network-valued responses, as well as a real-worldapplication to predicting employment occupational compositions, demonstratethat DFNNs consistently outperform existing methods.</description>
      <author>example@mail.com (Kyum Kim, Yaqing Chen, Paromita Dubey)</author>
      <guid isPermaLink="false">2510.17072v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability</title>
      <link>http://arxiv.org/abs/2510.17040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Diverse Influence Component Analysis (DICA)框架，利用混合函数Jacobian的凸几何特性，通过Jacobian Volume Maximization (J-VolMax)准则实现潜在成分识别，无需依赖辅助信息、潜在成分独立性或Jacobian稀疏假设。&lt;h4&gt;背景&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习的基础挑战，应用于解纠缠表示学习和因果推断等领域。先前工作表明辅助信号可支持条件独立潜在成分的可识别性，而更新的方法则通过结构假设（如混合函数Jacobian的稀疏性）来放宽要求。&lt;h4&gt;目的&lt;/h4&gt;引入DICA框架，利用混合函数Jacobian的凸几何特性，开发一种新的潜在成分识别方法。&lt;h4&gt;方法&lt;/h4&gt;提出Jacobian Volume Maximization (J-VolMax)准则，通过鼓励潜在成分对观察变量的影响多样性来实现潜在成分识别。&lt;h4&gt;主要发现&lt;/h4&gt;在合理条件下，DICA方法无需依赖辅助信息、潜在成分独立性或Jacobian稀疏假设即可实现潜在成分的可识别性。&lt;h4&gt;结论&lt;/h4&gt;这些结果扩展了可识别性分析的范围，为现有方法提供了互补的视角。&lt;h4&gt;翻译&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习中的一个基础性挑战，应用于解纠缠表示学习和因果推断等任务。先前在非线性独立成分分析方面的工作表明，辅助信号（如弱监督）可以支持条件独立潜在成分的可识别性。更新的方法探索结构假设，例如混合函数的Jacobian稀疏性，以放宽这些要求。在这项工作中，我们引入了Diverse Influence Component Analysis (DICA)框架，利用混合函数Jacobian的凸几何特性。我们提出了Jacobian Volume Maximization (J-VolMax)准则，通过鼓励潜在成分对观察变量的影响多样性来实现潜在成分识别。在合理条件下，这种方法无需依赖辅助信息、潜在成分独立性或Jacobian稀疏假设即可实现可识别性。这些结果扩展了可识别性分析的范围，为现有方法提供了互补的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent component identification from unknown nonlinear mixtures is afoundational challenge in machine learning, with applications in tasks such asdisentangled representation learning and causal inference. Prior work innonlinear independent component analysis (nICA) has shown that auxiliarysignals -- such as weak supervision -- can support identifiability ofconditionally independent latent components. More recent approaches explorestructural assumptions, e.g., sparsity in the Jacobian of the mixing function,to relax such requirements. In this work, we introduce Diverse InfluenceComponent Analysis (DICA), a framework that exploits the convex geometry of themixing function's Jacobian. We propose a Jacobian Volume Maximization(J-VolMax) criterion, which enables latent component identification byencouraging diversity in their influence on the observed variables. Underreasonable conditions, this approach achieves identifiability without relyingon auxiliary information, latent component independence, or Jacobian sparsityassumptions. These results extend the scope of identifiability analysis andoffer a complementary perspective to existing methods.</description>
      <author>example@mail.com (Hoang-Son Nguyen, Xiao Fu)</author>
      <guid isPermaLink="false">2510.17040v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams</title>
      <link>http://arxiv.org/abs/2510.16988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CARE框架，通过序列-图像对比对齐方法解决了日常生活活动识别中的表征局限性，实现了高性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;从事件触发式环境传感器识别日常生活活动(ADLs)是环境辅助生活的关键任务，但现有方法存在表征层面的局限性。基于序列的方法对噪声敏感且缺乏空间感知，而基于图像的方法压缩了时间动态并扭曲了传感器布局。简单融合无法充分利用这两种方法的互补优势。&lt;h4&gt;目的&lt;/h4&gt;开发一个端到端框架，通过联合优化表征学习和分类，确保跨表征对齐和任务特定判别性，从而提高ADL识别的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出CARE(从事件触发式传感器流进行ADL识别的对比对齐)框架，集成时间感知、噪声鲁棒的序列编码与空间感知、频率敏感的图像表征，并采用联合对比-分类目标进行端到端学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个CASAS数据集上评估，CARE实现了最先进的性能：Milan上89.8%，Cairo上88.9%，Kyoto7上73.3%。同时，该方法展示了对传感器故障和布局变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CARE框架在智能家居环境中可靠的ADL识别具有显著潜力，其性能和鲁棒性证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;从事件触发式环境传感器识别日常生活活动(ADLs)是环境辅助生活(AAL)中的关键任务，然而现有方法仍受表征层面限制。基于序列的方法保留了传感器激活的时间顺序，但对噪声敏感且缺乏空间感知，而基于图像的方法捕捉全局模式和隐含的空间相关性，但压缩了细粒度时间动态并扭曲了传感器布局。简单融合(如特征连接)无法强制序列和图像表征视图之间的对齐，未能充分利用它们的互补优势。我们提出了CARE(从事件触发式传感器流进行ADL识别的对比对齐)，一个通过序列-图像对比对齐(SICA)和交叉熵联合优化表征学习的端到端框架，确保跨表征对齐和任务特定判别性。CARE集成(i)时间感知、噪声鲁棒的序列编码，(ii)空间感知和频率敏感的图像表征，并采用(iii)联合对比-分类目标进行对齐且具有判别性的嵌入的端到端学习。在三个CASAS数据集上评估，CARE实现了最先进的性能(Milan上89.8%，Cairo上88.9%，Kyoto7上73.3%)，并展示了对传感器故障和布局变化的鲁棒性，突显了其在智能家居中可靠ADL识别的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recognition of Activities of Daily Living (ADLs) from event-triggeredambient sensors is an essential task in Ambient Assisted Living, yet existingmethods remain constrained by representation-level limitations. Sequence-basedapproaches preserve temporal order of sensor activations but are sensitive tonoise and lack spatial awareness, while image-based approaches capture globalpatterns and implicit spatial correlations but compress fine-grained temporaldynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)fail to enforce alignment between sequence- and image-based representationviews, underutilizing their complementary strengths. We propose ContrastiveAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), anend-to-end framework that jointly optimizes representation learning viaSequence-Image Contrastive Alignment (SICA) and classification viacross-entropy, ensuring both cross-representation alignment and task-specificdiscriminability. CARE integrates (i) time-aware, noise-resilient sequenceencoding with (ii) spatially-informed and frequency-sensitive imagerepresentations, and employs (iii) a joint contrastive-classification objectivefor end-to-end learning of aligned and discriminative embeddings. Evaluated onthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% onMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness tosensor malfunctions and layout variability, highlighting its potential forreliable ADL recognition in smart homes.</description>
      <author>example@mail.com (Junhao Zhao, Zishuai Liu, Ruili Fang, Jin Lu, Linghan Zhang, Fei Dou)</author>
      <guid isPermaLink="false">2510.16988v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalizable Continual Learning</title>
      <link>http://arxiv.org/abs/2510.16914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为域可推广持续学习(DGCL)的新设置，以及自适应域变换(DoT)方法来解决智能系统在动态环境中学习新技能并推广到多样化场景的挑战。&lt;h4&gt;背景&lt;/h4&gt;智能系统需要不断获取新技能并将其推广到多样化、未见过的场景。现有持续学习方法假设每个任务的训练和测试域相同，在域变化场景下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出DGCL设置，使模型能够学习序列任务，每个任务涉及单一域，目标是模型在所有遇到的任务和域中表现良好。解决获取、保留和利用语义及域相关信息的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出自适应域变换(DoT)方法，基于预训练模型，受人类大脑分布式加枢纽理论启发，在表示学习中解耦语义和域相关信息，自适应转换跨域任务表示以实现输出对齐，确保平衡和泛化的预测。&lt;h4&gt;主要发现&lt;/h4&gt;DoT作为即插即用策略显著提升了最先进CL基线在DGCL下的性能，能够积累域可推广知识，具有轻量级实现确保资源效率，在全参数调整和参数高效调整范式下均有效。&lt;h4&gt;结论&lt;/h4&gt;DoT解决了DGCL中的独特挑战，通过解耦语义和域相关信息实现更好的泛化能力，使智能系统能够有效适应动态现实环境。&lt;h4&gt;翻译&lt;/h4&gt;为了有效适应动态现实环境，智能系统必须不断获取新技能，同时将其推广到多样化、未见过的场景。在此，我们引入一种名为域可推广持续学习(DGCL)的新颖且现实的设置：模型学习序列任务，每个任务涉及单一域，旨在在所有遇到的任务和域中表现良好。这种设置在获取、保留和利用语义和域相关信息以实现稳健泛化方面提出了独特挑战。尽管最先进的持续学习方法采用预训练模型来增强任务特定泛化，但它们通常假设每个任务的训练和测试域相同，因此在DGCL中表现不佳。为此，我们提出了自适应域变换(DoT)，这是一种专为DGCL设计的创新预训练模型方法。受人类大脑分布式加枢纽理论的启发，DoT在表示学习中解耦语义和域相关信息，并自适应地跨不同域转换任务表示以实现输出对齐，确保平衡和泛化的预测。DoT作为一种即插即用策略，在DGCL下极大地促进了最先进CL基线在全参数调整和参数高效调整范式中的性能，并通过大量实验得到验证。此外，DoT被证明能够从DGCL中积累域可推广知识，并通过轻量级实现确保资源效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To adapt effectively to dynamic real-world environments, intelligent systemsmust continually acquire new skills while generalizing them to diverse, unseenscenarios. Here, we introduce a novel and realistic setting named domaingeneralizable continual learning (DGCL): a model learns sequential tasks witheach involving a single domain, aiming to perform well across all encounteredtasks and domains. This setting poses unique challenges in acquiring,retaining, and leveraging both semantic- and domain-relevant information forrobust generalization. Although state-of-the-art continual learning (CL)methods have employed pre-trained models (PTMs) to enhance task-specificgeneralization, they typically assume identical training and testing domainsfor each task and therefore perform poorly in DGCL. To this end, we proposeadaptive Domain Transformation (DoT), an innovative PTMs-based approachtailored to DGCL. Inspired by the distributed-plus-hub theory of the humanbrain, DoT disentangles semantic- and domain-relevant information inrepresentation learning, and adaptively transforms task representations acrossvarious domains for output alignment, ensuring balanced and generalizedpredictions. DoT serves as a plug-in strategy that greatly facilitatesstate-of-the-art CL baselines under both full parameter tuning andparameter-efficient tuning paradigms in DGCL, validated by extensiveexperiments. Also, DoT is shown to accumulate domain-generalizable knowledgefrom DGCL, and ensure resource efficiency with a lightweight implementation.</description>
      <author>example@mail.com (Hongwei Yan, Guanglong Sun, Zhiqi Kang, Yi Zhong, Liyuan Wang)</author>
      <guid isPermaLink="false">2510.16914v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning</title>
      <link>http://arxiv.org/abs/2510.16877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Fly-CL是一种受果蝇嗅觉回路启发的生物启发框架，用于持续表征学习，解决了直接利用预训练特征时的多重共线性问题，同时显著减少了训练时间，性能达到或超过当前最先进方法。&lt;h4&gt;背景&lt;/h4&gt;持续表征学习范式将参数更新重新构建为相似度匹配问题以减轻灾难性遗忘，但直接利用预训练特征进行下游任务通常在相似度匹配阶段存在多重共线性问题，且更高级的方法可能对实时、低延迟应用计算成本过高。&lt;h4&gt;目的&lt;/h4&gt;解决直接利用预训练特征进行下游任务时存在的多重共线性问题，并提出一种计算效率高的方法，适用于实时、低延迟应用。&lt;h4&gt;方法&lt;/h4&gt;受果蝇嗅觉回路的启发，提出了Fly-CL框架，与各种预训练骨干网络兼容。从理论上展示了Fly-CL如何逐步解决多重共线性问题，实现更有效的相似度匹配，同时具有低时间复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;Fly-CL显著减少了训练时间，同时实现了与当前最先进方法相当或更好的性能。通过生物启发设计有效解决了多重共线性挑战。&lt;h4&gt;结论&lt;/h4&gt;Fly-CL是一种生物启发框架，与多种预训练骨干网络兼容。在不同网络架构和数据集上的广泛模拟实验验证了其有效性。代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;使用几乎冻结的预训练模型，持续表征学习范式将参数更新重新构建为相似度匹配问题，以减轻灾难性遗忘。然而，直接利用预训练特征进行下游任务通常在相似度匹配阶段存在多重共线性问题，更高级的方法可能对实时、低延迟应用来说计算成本过高。受果蝇嗅觉回路的启发，我们提出了Fly-CL，这是一种与多种预训练骨干网络兼容的生物启发框架。Fly-CL显著减少了训练时间，同时实现了与当前最先进方法相当或更好的性能。我们从理论上展示了Fly-CL如何逐步解决多重共线性问题，实现更有效的相似度匹配，同时具有低时间复杂度。在各种网络架构和数据集上的广泛模拟实验验证了Fly-CL通过生物启发设计解决这一挑战的有效性。代码可在https://github.com/gfyddha/Fly-CL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Using a nearly-frozen pretrained model, the continual representation learningparadigm reframes parameter updates as a similarity-matching problem tomitigate catastrophic forgetting. However, directly leveraging pretrainedfeatures for downstream tasks often suffers from multicollinearity in thesimilarity-matching stage, and more advanced methods can be computationallyprohibitive for real-time, low-latency applications. Inspired by the flyolfactory circuit, we propose Fly-CL, a bio-inspired framework compatible witha wide range of pretrained backbones. Fly-CL substantially reduces trainingtime while achieving performance comparable to or exceeding that of currentstate-of-the-art methods. We theoretically show how Fly-CL progressivelyresolves multicollinearity, enabling more effective similarity matching withlow time complexity. Extensive simulation experiments across diverse networkarchitectures and data regimes validate Fly-CL's effectiveness in addressingthis challenge through a biologically inspired design. Code is available athttps://github.com/gfyddha/Fly-CL.</description>
      <author>example@mail.com (Heming Zou, Yunliang Zang, Wutong Xu, Xiangyang Ji)</author>
      <guid isPermaLink="false">2510.16877v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding</title>
      <link>http://arxiv.org/abs/2510.16780v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为3D-GSRD的新型3D分子图自编码器，通过选择性重新掩码解码技术解决了将2D MGM成功扩展到3D MGM时面临的两个相互冲突的挑战。&lt;h4&gt;背景&lt;/h4&gt;掩码图模型(MGM)是分子表示学习(MRL)的一种有前景的方法，但将2D重新掩码解码的成功经验扩展到3D MGM面临两个相互冲突的挑战：避免2D结构信息泄漏到解码器，同时为重新掩码的原子重构提供足够的2D上下文。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理3D分子数据并解决2D结构信息泄漏与上下文提供之间矛盾的分子表示学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出3D-GSRD，其核心创新是选择性重新掩码解码(SRD)，该技术仅从编码器表示中重新掩码3D相关信息，同时保留2D图结构。SRD与3D关系转换器(3D-ReTrans)编码器和结构无关的解码器协同集成。&lt;h4&gt;主要发现&lt;/h4&gt;SRD与结构无关的解码器增强了编码器在分子表示学习中的作用。在MD17分子性质预测基准测试中，3D-GSRD在8个目标中的7个上达到了最新的最优性能。&lt;h4&gt;结论&lt;/h4&gt;3D-GSRD成功解决了将2D MGM扩展到3D MGM时面临的挑战，为分子表示学习提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;掩码图建模(MGM)是分子表示学习(MRL)的一种有前景的方法。然而，将重新掩码解码的成功从2D扩展到3D MGM并非易事，主要由于两个相互冲突的挑战：避免将2D结构信息泄漏到解码器，同时仍为重新掩码的原子重构提供足够的2D上下文。为解决这些挑战，我们提出了3D-GSRD：一种具有选择性重新掩码解码的3D分子图自编码器。3D-GSRD的核心创新在于其选择性重新掩码解码(SRD)，它仅从编码器表示中重新掩码3D相关信息，同时保留2D图结构。SRD与3D关系转换器(3D-ReTrans)编码器和结构无关的解码器协同集成。我们分析指出，SRD结合结构无关的解码器增强了编码器在MRL中的作用。大量实验表明，3D-GSRD实现了强大的下游性能，在广泛使用的MD17分子性质预测基准的8个目标中，有7个达到了最新的最优状态。代码已发布在https://github.com/WuChang0124/3D-GSRD。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked graph modeling (MGM) is a promising approach for molecularrepresentation learning (MRL).However, extending the success of re-maskdecoding from 2D to 3D MGM is non-trivial, primarily due to two conflictingchallenges: avoiding 2D structure leakage to the decoder, while still providingsufficient 2D context for reconstructing re-masked atoms.To address thesechallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder withSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in itsSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant informationfrom encoder representations while preserving the 2D graph structures.This SRDis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)encoder alongside a structure-independent decoder. We analyze that SRD,combined with the structure-independent decoder, enhances the encoder's role inMRL. Extensive experiments show that 3D-GSRD achieves strong downstreamperformance, setting a new state-of-the-art on 7 out of 8 targets in the widelyused MD17 molecular property prediction benchmark. The code is released athttps://github.com/WuChang0124/3D-GSRD.</description>
      <author>example@mail.com (Chang Wu, Zhiyuan Liu, Wen Shu, Liang Wang, Yanchen Luo, Wenqiang Lei, Yatao Bian, Junfeng Fang, Xiang Wang)</author>
      <guid isPermaLink="false">2510.16780v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning</title>
      <link>http://arxiv.org/abs/2510.16474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自适应核注意力机制的新预测建模方法，解决了传统方法在高维、异构数据处理中的局限性，实验证明该方法优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;高维、异构数据及其复杂特征交互对传统预测建模方法构成挑战。传统方法如投影到潜在结构(PLS)难以建模复杂非线性关系，特别是在具有高维相关结构的多变量系统中。多尺度上的同时交互使局部处理无法捕获跨组依赖，而静态特征加权限制了模型对上下文变化的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法，通过新颖的架构创新来增强预测性能，解决传统方法在高维、异构数据处理中的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的架构，引入基于自适应核的注意力机制。该机制分别处理不同的特征组，然后在集成之前进行整合，从而能够捕获局部模式同时保留全局关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与最先进的方法相比，作者提出的方法在各种数据集上的性能指标都有显著改进。&lt;h4&gt;结论&lt;/h4&gt;基于自适应核的注意力机制架构能够有效处理高维、异构数据中的复杂特征交互和多尺度交互问题，提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;高维、异构数据与复杂特征交互对传统预测建模方法构成了重大挑战。虽然投影到潜在结构(PLS)仍然是一种流行技术，但它难以建模复杂的非线性关系，特别是在具有高维相关结构的多变量系统中。多尺度上的同时交互进一步加剧了这一挑战，使得局部处理无法捕获跨组依赖关系。此外，静态特征加权限制了适应上下文变化的能力，因为它忽略了样本特定的相关性。为解决这些局限性，我们提出了一种通过新颖架构创新来增强预测性能的新方法。我们的架构引入了一个基于自适应核的注意力机制，它分别处理不同的特征组，然后在集成之前进行整合，从而能够捕获局部模式同时保留全局关系。实验结果表明，与最先进的方法相比，在各种数据集上的性能指标都有显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional, heterogeneous data with complex feature interactions posesignificant challenges for traditional predictive modeling approaches. WhileProjection to Latent Structures (PLS) remains a popular technique, it strugglesto model complex non-linear relationships, especially in multivariate systemswith high-dimensional correlation structures. This challenge is furthercompounded by simultaneous interactions across multiple scales, where localprocessing fails to capture crossgroup dependencies. Additionally, staticfeature weighting limits adaptability to contextual variations, as it ignoressample-specific relevance. To address these limitations, we propose a novelmethod that enhances predictive performance through novel architecturalinnovations. Our architecture introduces an adaptive kernel-based attentionmechanism that processes distinct feature groups separately before integration,enabling capture of local patterns while preserving global relationships.Experimental results show substantial improvements in performance metrics,compared to the state-of-the-art methods across diverse datasets.</description>
      <author>example@mail.com (Farwa Abbas, Hussain Ahmad, Claudia Szabo)</author>
      <guid isPermaLink="false">2510.16474v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Humanoid-inspired Causal Representation Learning for Domain Generalization</title>
      <link>http://arxiv.org/abs/2510.16382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种受人类智能启发的结构化因果模型HSCM，通过模仿人类视觉系统的分层处理和多级学习机制，专注于建模细粒度因果关系，从而提升模型在不同领域间的泛化能力和稳健性。&lt;h4&gt;背景&lt;/h4&gt;传统领域泛化模型存在局限性，它们通常依赖统计数据来捕获数据标签依赖和学习扭曲不变表示，无法充分捕捉人类视觉系统的层次化处理机制。&lt;h4&gt;目的&lt;/h4&gt;克服传统领域泛化模型的局限性，开发一种受人类智能启发的因果框架，提升模型在不同领域间的泛化能力，确保模型的稳健性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出Humanoid-inspired Structural Causal Model (HSCM)，模仿人类视觉系统的分层处理和多级学习，通过解耦和重新加权关键图像属性（如颜色、纹理和形状）来建模细粒度因果机制。&lt;h4&gt;主要发现&lt;/h4&gt;HSCM通过理论和实证评估证明优于现有领域泛化模型，提供了更规范的方法来捕获因果关系并提高模型稳健性，在动态复杂环境中实现更有效的迁移学习。&lt;h4&gt;结论&lt;/h4&gt;HSCM作为一种受人类智能启发的因果框架，能够有效提升模型跨领域的泛化能力，确保稳健性和可解释性，为领域泛化问题提供了新的解决思路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种受人类智能启发的结构化因果模型（HSCM），这是一种新颖的因果框架，旨在克服传统领域泛化模型的局限性。与依赖统计数据捕获数据标签依赖和学习扭曲不变表示的方法不同，HSCM模仿人类视觉系统的分层处理和多级学习，专注于建模细粒度因果机制。通过解耦和重新加权关键图像属性（如颜色、纹理和形状），HSCM增强了跨不同领域的泛化能力，确保了模型的稳健性和可解释性。利用人类智能的灵活性和适应性，我们的方法使模型在动态复杂环境中能够实现更有效的迁移和学习。通过理论和实证评估，我们证明了HSCM优于现有领域泛化模型，为捕获因果关系和提高模型稳健性提供了更规范的方法。代码可在https://github.com/lambett/HSCM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), anovel causal framework inspired by human intelligence, designed to overcome thelimitations of conventional domain generalization models. Unlike approachesthat rely on statistics to capture data-label dependencies and learndistortion-invariant representations, HSCM replicates the hierarchicalprocessing and multi-level learning of human vision systems, focusing onmodeling fine-grained causal mechanisms. By disentangling and reweighting keyimage attributes such as color, texture, and shape, HSCM enhancesgeneralization across diverse domains, ensuring robust performance andinterpretability. Leveraging the flexibility and adaptability of humanintelligence, our approach enables more effective transfer and learning indynamic, complex environments. Through both theoretical and empiricalevaluations, we demonstrate that HSCM outperforms existing domaingeneralization models, providing a more principled method for capturing causalrelationships and improving model robustness. The code is available athttps://github.com/lambett/HSCM.</description>
      <author>example@mail.com (Ze Tao, Jian Zhang, Haowei Li, Xianshuai Li, Yifei Peng, Xiyao Liu, Senzhang Wang, Chao Liu, Sheng Ren, Shichao Zhang)</author>
      <guid isPermaLink="false">2510.16382v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema</title>
      <link>http://arxiv.org/abs/2510.16357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.  HuggingFace:  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了多语言代码解析器数据集(MLCPD)，一个统一十种主要编程语言语法结构的大规模、语言无关数据集，包含超过七百万个标准化解析源文件，支持跨语言推理、结构学习和多语言软件分析。&lt;h4&gt;背景&lt;/h4&gt;现有代码语料库主要关注标记级代码或孤立解析器，缺乏跨语言的统一表示，需要一种能够统一不同编程语言语法结构的数据集。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、语言无关的数据集，统一十种主要编程语言的语法和结构表示，为跨语言表示学习和程序分析提供开放、可重现的基础。&lt;h4&gt;方法&lt;/h4&gt;提出通用抽象语法树(AST)模式标准化解析源文件，为每个文件提供分层树表示和丰富元数据，以Parquet格式存储，进行跨语言结构分析，并开发数据集复现、语法编译和可视化工具。&lt;h4&gt;主要发现&lt;/h4&gt;跨语言代码结构存在强大的规律性，差异很大的编程语言(如Python、Java和Go)的语法图可以在共享模式下对齐，提出的统一AST模式能够无损地表示不同语言的语法结构。&lt;h4&gt;结论&lt;/h4&gt;MLCPD为跨语言表示学习和程序分析提供了开放、可重现的基础，通过统一的数据表示和丰富的工具支持，促进了多语言软件分析和跨语言推理研究。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了多语言代码解析器数据集(MLCPD)，这是一个大规模、语言无关的数据集，统一了十种主要编程语言的代码语法和结构表示。MLCPD包含超过七百万个在我们提出的通用抽象语法树(AST)模式下标准化的解析源文件，实现了跨语言推理、结构学习和多语言软件分析的一致性。与仅关注标记级代码或孤立解析器的现有语料库不同，MLCPD为每个文件提供了分层树表示和丰富的元数据，确保无损的语法覆盖和结构一致性。每个条目包括标准化的模式、语言级元数据和抽象节点语义，以Parquet格式存储以便可扩展检索。经验分析揭示了强大的跨语言结构规律性，证明像Python、Java和Go这样差异很大的语言的语法图可以在共享模式下对齐。我们在Hugging Face上公开发布了该数据集，并在GitHub上提供了配套代码库，包括数据集复现、语法编译和探索跨语言统一AST的可视化工具。这些资源共同确立了MLCPD作为跨语言表示学习和程序分析未来研究的开放、可重现基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,language-agnostic dataset unifying syntactic and structural representations ofcode across ten major programming languages. MLCPD contains over seven millionparsed source files normalized under our proposed universal Abstract SyntaxTree (AST) schema, enabling consistent cross-language reasoning, structurallearning, and multilingual software analysis. Unlike existing corpora thatfocus purely on token-level code or isolated parsers, MLCPD provides bothhierarchical tree representations and rich metadata for every file, ensuringlossless syntactic coverage and structural uniformity. Each entry includes anormalized schema, language-level metadata, and abstracted node semanticsstored in Parquet format for scalable retrieval. Empirical analyses revealstrong cross-language structural regularities-demonstrating that syntacticgraphs from languages as diverse as Python, Java, and Go can be aligned under ashared schema. We release the dataset publicly on Hugging Face and theaccompanying codebase on GitHub, which includes complete pipelines for datasetreproduction, grammar compilation, and a visualization tool for exploring theunified AST across languages. Together, these resources establish MLCPD as anopen, reproducible foundation for future research in cross-languagerepresentation learning and program analysis.</description>
      <author>example@mail.com (Jugal Gajjar, Kamalasankari Subramaniakuppusamy)</author>
      <guid isPermaLink="false">2510.16357v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Disentangling Hyperedges through the Lens of Category Theory</title>
      <link>http://arxiv.org/abs/2510.16289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了超图结构数据中的超边解缠问题，从范畴论角度提出了一种新的解缠准则，并通过基因功能关系分析验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;尽管解缠表示学习在图结构数据分析中取得了进展，但针对超图结构数据的解缠研究较少，存在研究空白。&lt;h4&gt;目的&lt;/h4&gt;将超边解缠整合到超图神经网络中，使模型能够利用与标签相关的隐藏超边语义，如节点间未注释的关系。&lt;h4&gt;方法&lt;/h4&gt;从范畴论角度分析超边解缠，提出基于自然性条件的新解缠准则，并构建概念验证模型进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;概念验证模型成功捕获了基因通路中基因的功能关系，证明了所提准则的潜力。&lt;h4&gt;结论&lt;/h4&gt;基于自然性条件的解缠准则在超图结构数据中有效，特别是在分析基因功能关系方面表现出应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;尽管解缠表示学习在发现图结构数据中的潜在模式方面取得了有希望的结果，但很少有研究探索超图结构数据的解缠。将超边解缠整合到超图神经网络中，使模型能够利用与标签相关的隐藏超边语义，例如节点之间未注释的关系。本文从范畴论的角度对超边解缠进行了分析，并提出了一种从自然性条件推导出的新解缠准则。我们的概念验证模型通过成功捕获基因通路（超边）中基因（节点）的功能关系，实验性地展示了所提出准则的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the promising results of disentangled representation learning indiscovering latent patterns in graph-structured data, few studies have exploreddisentanglement for hypergraph-structured data. Integrating hyperedgedisentanglement into hypergraph neural networks enables models to leveragehidden hyperedge semantics, such as unannotated relations between nodes, thatare associated with labels. This paper presents an analysis of hyperedgedisentanglement from a category-theoretical perspective and proposes a novelcriterion for disentanglement derived from the naturality condition. Ourproof-of-concept model experimentally showed the potential of the proposedcriterion by successfully capturing functional relations of genes (nodes) ingenetic pathways (hyperedges).</description>
      <author>example@mail.com (Yoonho Lee, Junseok Lee, Sangwoo Seo, Sungwon Kim, Yeongmin Kim, Chanyoung Park)</author>
      <guid isPermaLink="false">2510.16289v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding</title>
      <link>http://arxiv.org/abs/2510.16273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MuseTok是一种创新的离散表示学习方法，专门针对符号音乐设计，结合了RQ-VAE和Transformer架构，在音乐生成和语义理解任务中均取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;离散表示学习在图像、语音和语言的生成和理解领域已显示出有前景的结果，这些进展启发了作者对音乐符号表示的研究。&lt;h4&gt;目的&lt;/h4&gt;提出MuseTok，一种用于符号音乐的标记化方法，研究其在音乐生成和理解任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;MuseTok采用基于残差向量量化-变分自编码器（RQ-VAE）的方法，在基于Transformer的编码器-解码器框架中，对小节音乐段进行处理，生成能够实现高保真音乐重建和准确音乐理论理解的音乐代码。&lt;h4&gt;主要发现&lt;/h4&gt;在音乐生成和语义理解任务的综合评估中，使用MuseTok的模型在语义理解方面优于先前的表示学习基线，在内容生成方面保持可比的性能；对MuseTok代码的定性分析表明，它能够从大型音乐集中有效捕捉潜在的音乐概念。&lt;h4&gt;结论&lt;/h4&gt;MuseTok是一种有效的符号音乐标记化方法，在音乐生成和理解任务中表现良好。&lt;h4&gt;翻译&lt;/h4&gt;离散表示学习在图像、语音和语言的生成和理解等多个领域已显示出有前景的结果。受这些进展的启发，我们提出了MuseTok，一种用于符号音乐的标记化方法，并研究了其在音乐生成和理解任务中的有效性。MuseTok在基于Transformer的编码器-解码器框架中，对小节音乐段应用残差向量量化-变分自编码器（RQ-VAE），生成能够实现高保真音乐重建和准确音乐理论理解的音乐代码。为了进行全面评估，我们将MuseTok应用于音乐生成和语义理解任务，包括旋律提取、和弦识别和情感识别。采用MuseTok的模型在语义理解方面优于先前的表示学习基线，同时在内容生成方面保持可比的性能。此外，使用真实类别和合成数据集对MuseTok代码进行的定性分析表明，MuseTok能够有效从大型音乐集中捕捉潜在的音乐概念。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discrete representation learning has shown promising results across variousdomains, including generation and understanding in image, speech and language.Inspired by these advances, we propose MuseTok, a tokenization method forsymbolic music, and investigate its effectiveness in both music generation andunderstanding tasks. MuseTok employs the residual vector quantized-variationalautoencoder (RQ-VAE) on bar-wise music segments within a Transformer-basedencoder-decoder framework, producing music codes that achieve high-fidelitymusic reconstruction and accurate understanding of music theory. Forcomprehensive evaluation, we apply MuseTok to music generation and semanticunderstanding tasks, including melody extraction, chord recognition, andemotion recognition. Models incorporating MuseTok outperform previousrepresentation learning baselines in semantic understanding while maintainingcomparable performance in content generation. Furthermore, qualitative analyseson MuseTok codes, using ground-truth categories and synthetic datasets, revealthat MuseTok effectively captures underlying musical concepts from large musiccollections.</description>
      <author>example@mail.com (Jingyue Huang, Zachary Novack, Phillip Long, Yupeng Hou, Ke Chen, Taylor Berg-Kirkpatrick, Julian McAuley)</author>
      <guid isPermaLink="false">2510.16273v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2510.16086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种因子引导的语义恢复框架(FSRF)，用于解决多模态情感分析中的模态缺失问题，通过去冗余的同质异质因子分解模块和分布对齐的自蒸馏模块，有效恢复了缺失模态的语义信息，实验证明该方法在不确定缺失模态的情况下具有显著的性能优势。&lt;h4&gt;背景&lt;/h4&gt;多模态情感分析(MSA)已成为研究热点，旨在利用多模态数据进行人类情感理解。以往研究主要关注完整多模态数据的交互和融合，忽略了实际应用中因遮挡、个人隐私限制和设备故障导致的模态缺失问题，导致泛化能力低。&lt;h4&gt;目的&lt;/h4&gt;提出一个因子引导的语义恢复框架(FSRF)，以缓解MSA任务中的模态缺失问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种去冗余的同质异质因子分解模块，将模态分解为模态同质、模态异质和噪声表示，并设计了表示学习的精细约束范式；设计了一种分布对齐的自蒸馏模块，通过利用双向知识转移完全恢复缺失的语义。&lt;h4&gt;主要发现&lt;/h4&gt;在两个数据集上的综合实验表明，与之前的方法相比，FSRF在不确定缺失模态的情况下具有显著的性能优势。&lt;h4&gt;结论&lt;/h4&gt;FSRF框架有效解决了多模态情感分析中的模态缺失问题，提高了模型在真实应用场景中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;近年来，多模态情感分析(MSA)已成为一个研究热点，旨在利用多模态数据进行人类情感理解。以往的MSA研究主要集中在完整多模态数据的交互和融合上，忽略了实际应用中因遮挡、个人隐私限制和设备故障导致的模态缺失问题，导致泛化能力低。为此，我们提出了一种因子引导的语义恢复框架(FSRF)，以缓解MSA任务中的模态缺失问题。具体而言，我们提出了一种去冗余的同质异质因子分解模块，将模态分解为模态同质、模态异质和噪声表示，并设计了表示学习的精细约束范式。此外，我们设计了一种分布对齐的自蒸馏模块，通过利用双向知识转移完全恢复缺失的语义。在两个数据集上的综合实验表明，与之前的方法相比，FSRF在不确定缺失模态的情况下具有显著的性能优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Multimodal Sentiment Analysis (MSA) has become a researchhotspot that aims to utilize multimodal data for human sentiment understanding.Previous MSA studies have mainly focused on performing interaction and fusionon complete multimodal data, ignoring the problem of missing modalities inreal-world applications due to occlusion, personal privacy constraints, anddevice malfunctions, resulting in low generalizability.  To this end, we propose a Factorization-guided Semantic Recovery Framework(FSRF) to mitigate the modality missing problem in the MSA task.  Specifically, we propose a de-redundant homo-heterogeneous factorizationmodule that factorizes modality into modality-homogeneous,modality-heterogeneous, and noisy representations and design elaborateconstraint paradigms for representation learning.  Furthermore, we design a distribution-aligned self-distillation module thatfully recovers the missing semantics by utilizing bidirectional knowledgetransfer.  Comprehensive experiments on two datasets indicate that FSRF has asignificant performance advantage over previous methods with uncertain missingmodalities.</description>
      <author>example@mail.com (Ziyang Liu, Pengjunfei Chu, Shuming Dong, Chen Zhang, Mingcheng Li, Jin Wang)</author>
      <guid isPermaLink="false">2510.16086v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.15430v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Withdrawn due to an accidental duplicate submission. This paper  (arXiv:2510.15430) was unintentionally submitted as a new entry instead of a  new version of our previous work (arXiv:2508.09201)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Learning to Detect (LoD)的通用框架，用于检测大视觉语言模型中的未知越狱攻击。&lt;h4&gt;背景&lt;/h4&gt;尽管进行了广泛的对齐努力，大视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来严重的安全风险。&lt;h4&gt;目的&lt;/h4&gt;解决现有检测方法的局限性，这些方法要么学习特定攻击参数（难以泛化到新攻击），要么依赖启发式原理（限制准确性和效率）。&lt;h4&gt;方法&lt;/h4&gt;提出Learning to Detect (LoD)框架，通过从攻击特定学习转向任务特定学习来检测未知越狱攻击。框架包括：1)多模态安全概念激活向量模块，用于安全导向的表征学习；2)安全模式自动编码器模块，用于无监督攻击分类。&lt;h4&gt;主要发现&lt;/h4&gt;广泛实验表明，该方法在各种未知攻击上实现了更高的一致性检测AUROC，同时提高了效率。&lt;h4&gt;结论&lt;/h4&gt;Learning to Detect框架有效解决了现有检测方法的局限性，能够准确检测未知越狱攻击并提高效率。&lt;h4&gt;翻译&lt;/h4&gt;尽管进行了广泛的对齐努力，大视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来了严重的安全风险。为了解决这个问题，现有的检测方法要么学习特定攻击的参数，这阻碍了对未见过攻击的泛化能力，要么依赖启发式原理，这限制了准确性和效率。为了克服这些限制，我们提出了Learning to Detect (LoD)框架，通过将重点从攻击特定学习转向任务特定学习，准确检测未知的越狱攻击。该框架包括一个用于安全导向表征学习的多模态安全概念激活向量模块和一个用于无监督攻击分类的安全模式自动编码器模块。广泛的实验表明，我们的方法在各种未知攻击上实现了更高的一致性检测AUROC，同时提高了效率。代码可在https://anonymous.4open.science/r/Learning-to-Detect-51CB获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)remain vulnerable to jailbreak attacks, posing serious safety risks. To addressthis, existing detection methods either learn attack-specific parameters, whichhinders generalization to unseen attacks, or rely on heuristically soundprinciples, which limit accuracy and efficiency. To overcome these limitations,we propose Learning to Detect (LoD), a general framework that accuratelydetects unknown jailbreak attacks by shifting the focus from attack-specificlearning to task-specific learning. This framework includes a Multi-modalSafety Concept Activation Vector module for safety-oriented representationlearning and a Safety Pattern Auto-Encoder module for unsupervised attackclassification. Extensive experiments show that our method achievesconsistently higher detection AUROC on diverse unknown attacks while improvingefficiency. The code is available athttps://anonymous.4open.science/r/Learning-to-Detect-51CB.</description>
      <author>example@mail.com (Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, Xiting Wang)</author>
      <guid isPermaLink="false">2510.15430v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale User Game Lifecycle Representation Learning</title>
      <link>http://arxiv.org/abs/2510.15412v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对在线游戏平台广告和推荐系统的需求，提出了一种称为用户游戏生命周期(UGL)的表示学习方法，以解决游戏数据稀疏和游戏不平衡问题，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;随着视频游戏的快速扩张，在线游戏平台需要有效的广告和推荐系统。现有的表示学习方法是为处理推荐系统中的数十亿个项目而设计的，但不适用于游戏广告和推荐，主要原因是游戏稀疏性（仅有数百个游戏不足以进行大规模用户表示学习）和游戏不平衡性（用户行为被少数热门游戏主导）。&lt;h4&gt;目的&lt;/h4&gt;解决游戏稀疏性和游戏不平衡性对游戏广告和推荐系统的影响，提高用户兴趣捕捉的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 引入用户游戏生命周期(UGL)来丰富用户在游戏中的行为，解决稀疏性问题；2. 提出两种创新策略来操纵用户行为，更有效地提取短期和长期兴趣；3. 提出逆概率掩码策略用于UGL表示学习，解决游戏不平衡挑战。&lt;h4&gt;主要发现&lt;/h4&gt;UGL表示显著提升了模型性能：对于游戏广告，平均AUC离线增加1.83%，CVR在线平均增加21.67%；对于游戏内物品推荐，平均AUC离线增加0.5%，ARPU在线平均增加0.82%。&lt;h4&gt;结论&lt;/h4&gt;UGL表示学习方法能够有效解决游戏稀疏性和不平衡性问题，显著提升游戏广告和推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;随着视频游戏生产的快速扩张，有必要为在线游戏平台开发有效的广告和推荐系统。向用户推荐和宣传游戏取决于捕捉他们对游戏的兴趣。然而，为处理推荐系统中的数十亿个项目而设计的现有表示学习方法不适用于游戏广告和推荐。这主要是由于游戏稀疏性，其中仅有的数百个游戏不足以进行大规模用户表示学习，以及游戏不平衡性，其中用户行为被少数热门游戏主导。为了解决稀疏性问题，我们引入了用户游戏生命周期(UGL)，旨在丰富用户在游戏中的行为。此外，我们提出了两种创新策略，旨在操纵用户行为以更有效地提取短期和长期兴趣。为了应对游戏不平衡挑战，我们提出了用于UGL表示学习的逆概率掩码策略。离线和在线实验结果表明，UGL表示显著增强了模型性能，在游戏广告方面平均实现1.83%的AUC离线增长和21.67%的CVR在线增长，在游戏内物品推荐方面平均实现0.5%的AUC离线增长和0.82%的ARPU在线增长。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid expansion of video game production necessitates the development ofeffective advertising and recommendation systems for online game platforms.Recommending and advertising games to users hinges on capturing their interestin games. However, existing representation learning methods crafted forhandling billions of items in recommendation systems are unsuitable for gameadvertising and recommendation. This is primarily due to game sparsity, wherethe mere hundreds of games fall short for large-scale user representationlearning, and game imbalance, where user behaviors are overwhelmingly dominatedby a handful of popular games. To address the sparsity issue, we introduce theUser Game Lifecycle (UGL), designed to enrich user behaviors in games.Additionally, we propose two innovative strategies aimed at manipulating userbehaviors to more effectively extract both short and long-term interests. Totackle the game imbalance challenge, we present an Inverse Probability Maskingstrategy for UGL representation learning. The offline and online experimentalresults demonstrate that the UGL representations significantly enhance model byachieving a 1.83% AUC offline increase on average and a 21.67% CVR onlineincrease on average for game advertising and a 0.5% AUC offline increase and a0.82% ARPU online increase for in-game item recommendation.</description>
      <author>example@mail.com (Yanjie Gou, Jiangming Liu, Kouying Xue, Yi Hu)</author>
      <guid isPermaLink="false">2510.15412v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>HumanCM: One Step Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2510.16709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了HumanCM，一个基于一致性模型的一步式人体运动预测框架，能够高效地单步生成人体运动。&lt;h4&gt;背景&lt;/h4&gt;现有的基于扩散模型的人体运动预测方法依赖多步去噪过程，计算效率较低。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效、准确地预测人体运动的单步生成方法，减少计算负担。&lt;h4&gt;方法&lt;/h4&gt;HumanCM采用基于Transformer的时空架构，通过学习嘈杂和清洁运动状态之间的自一致映射来实现单步生成，并使用时间嵌入来建模长程依赖关系和保持运动连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM实现了与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少了高达两个数量级。&lt;h4&gt;结论&lt;/h4&gt;HumanCM是一种高效的人体运动预测方法，能够在保持高准确性的同时显著减少计算负担。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了HumanCM，这是一个基于一致性模型的一步式人体运动预测框架。与依赖多步去噪的基于扩散的方法不同，HumanCM通过学习嘈杂和清洁运动状态之间的自一致映射来执行高效的单步生成。该框架采用基于Transformer的时空架构，并使用时间嵌入来建模长程依赖关系并保持运动连贯性。在Human3.6M和HumanEva-I上的实验表明，HumanCM实现了与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少了高达两个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present HumanCM, a one-step human motion prediction framework built uponconsistency models. Instead of relying on multi-step denoising as indiffusion-based methods, HumanCM performs efficient single-step generation bylearning a self-consistent mapping between noisy and clean motion states. Theframework adopts a Transformer-based spatiotemporal architecture with temporalembeddings to model long-range dependencies and preserve motion coherence.Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achievescomparable or superior accuracy to state-of-the-art diffusion models whilereducing inference steps by up to two orders of magnitude.</description>
      <author>example@mail.com (Liu Haojie, Gao Suixiang)</author>
      <guid isPermaLink="false">2510.16709v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks</title>
      <link>http://arxiv.org/abs/2510.14486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 8 figures, 2 supplementary figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种通过并行神经网络集合的交叉监督学习来实现表示学习的模型，每个网络通过与其他网络的交互来编码刺激到抽象表示空间。该模型在生物 plausible 性方面表现出色，学习到的表示易于解码，解码准确性与监督网络相当。研究发现小感受野性能最优，稀疏连接与全连接效果相近但计算量更少。&lt;h4&gt;背景&lt;/h4&gt;大脑通常通过弱监督从大量刺激中学习表示信息。无监督学习是探索生物神经网络设计和计算的天然方法。冗余减少已被建议作为神经编码的突出设计原则，但其机制性生物实现尚不清楚。人工神经网络的无监督训练产生的内部表示允许准确分类，但通常依赖生物上不合理的实现。&lt;h4&gt;目的&lt;/h4&gt;探索大脑中并行子网络间的相互作用如何支持学习，提出一个由神经网络集合组成的表示学习模型，其中每个网络通过与其他网络的交叉监督来学习将刺激编码到抽象表示空间。&lt;h4&gt;方法&lt;/h4&gt;提出一个模型，每个网络具有小的感受野，接收外部输入的固定部分，且网络间不共享权重。测试了不同类型的网络架构以及视觉或神经元刺激下的表现。&lt;h4&gt;主要发现&lt;/h4&gt;1. 交叉监督的网络学习到的语义表示易于解码；2. 单个网络和集合层面的解码准确性与监督网络相当；3. 小感受野性能最优；4. 网络间的稀疏连接几乎与全连接同样准确，但计算量更少。&lt;h4&gt;结论&lt;/h4&gt;稀疏交互的交叉监督网络集合可作为大脑中表示学习和集体计算的算法框架。&lt;h4&gt;翻译&lt;/h4&gt;大脑通过弱监督通常从大量刺激中学习表示信息。因此，无监督学习是探索生物神经网络设计和计算的天然方法。相应地，冗余减少已被建议作为神经编码的突出设计原则，但其机制性生物实现尚不清楚。类似地，人工神经网络的无监督训练产生的内部表示允许准确的刺激分类或解码，但通常依赖于生物上不合理的实现。作者认为大脑中并行子网络之间的相互作用可能支持这种学习：作者提出了一个由神经网络集合组成的表示学习模型，其中每个网络通过与其他网络的交叉监督来学习将刺激编码到抽象表示空间，这些网络接收同时或时间上接近的输入。为了生物 plausible 性，每个网络具有小的感受野，因此接收外部输入的固定部分，且网络间不共享权重。作者发现，对于不同类型的网络架构，以及视觉或神经元刺激，这些交叉监督的网络学习到的语义表示易于解码，且解码准确性与监督网络相当——无论是在单个网络层面还是集合层面。作者进一步表明，小感受野性能最优，且网络间的稀疏连接几乎与全连接同样准确，但计算量少得多。因此，作者提出一个稀疏交互的交叉监督网络集合作为大脑中表示学习和集体计算的算法框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brains learn to represent information from a large set of stimuli, typicallyby weak supervision. Unsupervised learning is therefore a natural approach forexploring the design of biological neural networks and their computations.Accordingly, redundancy reduction has been suggested as a prominent designprinciple of neural encoding, but its ``mechanistic'' biological implementationis unclear. Analogously, unsupervised training of artificial neural networksyields internal representations that allow for accurate stimulus classificationor decoding, but typically rely on biologically-implausible implementations. Wesuggest that interactions between parallel subnetworks in the brain mayunderlie such learning: we present a model of representation learning byensembles of neural networks, where each network learns to encode stimuli intoan abstract representation space by cross-supervising interactions with othernetworks, for inputs they receive simultaneously or in close temporalproximity. Aiming for biological plausibility, each network has a small``receptive field'', thus receiving a fixed part of the external input, and thenetworks do not share weights. We find that for different types of networkarchitectures, and for both visual or neuronal stimuli, these cross-supervisingnetworks learn semantic representations that are easily decodable and thatdecoding accuracy is comparable to supervised networks -- both at the level ofsingle networks and the ensemble. We further show that performance is optimalfor small receptive fields, and that sparse connectivity between networks isnearly as accurate as all-to-all interactions, with far fewer computations. Wethus suggest a sparsely interacting collective of cross-supervising networks asan algorithmic framework for representational learning and collectivecomputation in the brain.</description>
      <author>example@mail.com (Roy Urbach, Elad Schneidman)</author>
      <guid isPermaLink="false">2510.14486v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
  <item>
      <title>Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents</title>
      <link>http://arxiv.org/abs/2510.14438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出'探索到进化'范式构建可验证训练数据，增强网络代理信息聚合能力，创建了WebAggregatorQA数据集和WebAggregator系列模型，在信息聚合任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;现有开源深度研究网络代理主要关注信息获取能力，忽视信息聚合需求，限制了支持深入研究的能力。&lt;h4&gt;目的&lt;/h4&gt;解决网络代理在信息聚合方面的不足，开发有效聚合信息的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出'探索到进化'范式：1)主动在线探索从真实网络获取信息；2)自我进化聚合程序，通过选择、组合和细化12种高级逻辑类型操作合成可验证问答对。基于此构建WebAggregatorQA数据集，并使用SmolAgents框架开发WebAggregator模型。&lt;h4&gt;主要发现&lt;/h4&gt;WebAggregator-8B性能与GPT-4.1相当；32B变体在GAIA-text上比GPT-4.1高10%以上，接近Claude-3.7-sonnet；在信息聚合评估基准上，Claude-3.7-sonnet仅达28%，GPT-4.1为25.8%；即使能检索所有参考资料，代理仍表现不佳。&lt;h4&gt;结论&lt;/h4&gt;网络代理基础模型需要加强信息聚合能力，而不仅是信息获取能力。'探索到进化'范式和WebAggregator模型为解决这一问题提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;深度研究网络代理不仅从多种来源检索信息，更重要的是需要严格分析和聚合知识进行深入研究。现有开源代理主要关注信息获取而忽视信息聚合，限制了深入研究能力。我们提出'探索到进化'范式可扩展构建可验证训练数据。从主动在线探索开始，代理从真实网络获取信息；然后自我进化聚合程序，通过选择、组合和细化操作合成可验证问答对。这种进化使我们能生产包含50K网站和11个领域10K样本的WebAggregatorQA数据集。基于SmolAgents框架，我们开发了WebAggregator系列模型。WebAggregator-8B与GPT-4.1性能相当，32B变体在GAIA-text上比GPT-4.1高10%以上，接近Claude-3.7-sonnet。我们构建了WebAggregatorQA的人工注释评估分割作为测试集，Claude-3.7-sonnet仅达28%，GPT-4.1为25.8%，突显了加强网络代理信息聚合能力的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep research web agents not only retrieve information from diverse sourcessuch as web environments, files, and multimodal inputs, but more importantly,they need to rigorously analyze and aggregate knowledge for insightfulresearch. However, existing open-source deep research agents predominantlyfocus on enhancing information-seeking capabilities of web agents to locatespecific information, while overlooking the essential need for informationaggregation, which would limit their ability to support in-depth research. Wepropose an Explore to Evolve paradigm to scalably construct verifiable trainingdata for web agents. Begins with proactive online exploration, an agent sourcesgrounded information by exploring the real web. Using the collected evidence,the agent then self-evolves an aggregation program by selecting, composing, andrefining operations from 12 high-level logical types to synthesize a verifiableQA pair. This evolution from high-level guidance to concrete operations allowedus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50Kwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,we collect supervised fine-tuning trajectories to develop a series offoundation models, WebAggregator. WebAggregator-8B matches the performance ofGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-textand closely approaches Claude-3.7-sonnet. Moreover, given the limitedavailability of benchmarks that evaluate web agents' information aggregationabilities, we construct a human-annotated evaluation split of WebAggregatorQAas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve allreferences, they still struggle on WebAggregatorQA, highlighting the need tostrengthen the information aggregation capabilities of web agent foundations.</description>
      <author>example@mail.com (Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong)</author>
      <guid isPermaLink="false">2510.14438v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking</title>
      <link>http://arxiv.org/abs/2510.14824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对比分析了大型语言模型(LLMs)在重排序任务中对比学习(CL)与监督微调(SFT)两种训练目标的差异，发现SFT在LLM重排序中表现优于CL，主要原因是SFT提供了更强的权重方案。&lt;h4&gt;背景&lt;/h4&gt;在信息检索中，重排序模型训练主要关注两类目标：度量学习和分类。对于BERT编码器，对比学习更有效；而对于大型语言模型，通过监督微调进行分类似乎更有前景，因为它与LLMs的生成性质良好对齐。这种分歧引出了核心问题：哪个目标本质上更适合基于LLM的重排序，以及差异背后的机制是什么？&lt;h4&gt;目的&lt;/h4&gt;全面对比分析CL和SFT在重排序任务中的表现，并探究两者之间的差异机制，确定哪种目标更适合基于LLM的重排序模型。&lt;h4&gt;方法&lt;/h4&gt;以通用多模态检索(UMR)为实验平台，将训练目标分解为权重和方向两个组成部分，提出统一框架理解它们的相互作用，通过探测实验进行分析，进行大规模SFT训练并在MRB基准上评估，同时对SFT设置进行消融研究。&lt;h4&gt;主要发现&lt;/h4&gt;SFT提供了比CL明显更强的权重方案，而优选的评分方向没有明显差异；综合结果表明SFT在LLM重排序方面一致优于CL。&lt;h4&gt;结论&lt;/h4&gt;SFT比CL更适合LLM重排序任务，这一发现源于SFT的权重优势而非评分方向差异，研究结果有望促进该领域未来的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;在信息检索中，训练重排序模型主要关注两类目标：度量学习（如对比损失，提高相关查询-文档对的预测分数）和分类（相关性与不相关性的二元标签预测）。对于BERT风格的编码器，各种研究表明对比学习(CL)可以比判别性（分类）学习更有效。然而，对于大型语言模型(LLMs)，通过监督微调(SFT)进行分类（预测相关对为'是'，不相关对为'否'）似乎更有前景，因为它与LLMs的生成性质良好对齐。这种分歧引出了核心问题：哪个目标本质上更适合基于LLM的重排序，以及这种差异背后的机制是什么？在这项工作中，我们在重排序任务中对CL和SFT进行了全面的比较和分析，以通用多模态检索(UMR)为实验平台。我们首先将目标分解为两个组成部分：权重（控制更新的幅度）和方向（指导模型更新），然后提出了一个统一框架来理解它们的相互作用。通过探测实验，我们发现SFT提供了比CL明显更强的权重方案，而优选的评分方向没有明显胜者。综合这些结果，一致表明SFT在LLM重排序方面优于CL。为了进一步验证我们的发现，我们进行了大规模SFT训练，并在MRB基准上展示了新的最先进重排序器。我们还对SFT设置进行了消融研究，并期望我们的发现能够促进该领域未来的研究和应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In information retrieval, training reranking models mainly focuses on twotypes of objectives: metric learning (e.g. contrastive loss to increase thepredicted scores on relevant query-document pairs) and classification (binarylabel prediction of relevance vs. irrelevance). For BERT-style encoders,various studies have shown that contrastive learning (CL) can be more effectivethan discriminative (classification) learning. However, for large languagemodels (LLMs), classification via supervised fine-tuning (SFT), which predicts''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appearsmore promising as it aligns well with the generative nature of LLMs. Thisdivergence raises a central question: which objective is intrinsically bettersuited to LLM-based reranking, and what mechanism underlies the difference? Inthis work, we conduct a comprehensive comparison and analysis between CL andSFT for reranking, taking the universal multimodal retrieval (UMR) as theexperimental playground. We first decompose the objectives into two components:weight, which controls the magnitude of those updates, and direction, whichguides the model updates, then present a unified framework for understandingtheir interactions. Through probing experiments, we find that SFT provides asubstantially stronger weighting scheme than CL, whereas the preferred scoringdirection shows no clear winner. Taken together, these results point to aconsistent advantage of SFT over CL for LLM reranking. To further validate ourfindings, we conduct large-scale training with SFT and present newstate-of-the-art rerankers on the MRB benchmark. We also provide ablations onSFT settings and expect our findings to benefit future research andapplications in this area.</description>
      <author>example@mail.com (Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang)</author>
      <guid isPermaLink="false">2510.14824v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors</title>
      <link>http://arxiv.org/abs/2510.15547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Sensors Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模态超图对比注意力网络(MM-HCAN)，用于感应电机的鲁棒故障诊断，实现了高达99.82%的准确率，具有强大的跨域泛化能力和噪声鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;可靠的感应电机故障诊断对工业安全和运营连续性至关重要，但传统方法难以捕捉复杂的多模态信号关系，局限于单模态数据或单一故障类型，在嘈杂或跨域条件下性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的故障诊断框架，解决多模态传感器融合问题，实现轴承、定子和转子故障的同时诊断，提高诊断系统的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出多模态超图对比注意力网络(MM-HCAN)，首次将对比学习整合到专为多模态传感器融合设计的超图拓扑中，实现模态内和模态间依赖关系的联合建模，增强超越欧几里得嵌入空间的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界基准测试中，MM-HCAN实现了高达99.82%的准确率，具有强大的跨域泛化能力和对噪声的鲁棒性，消融研究验证了每个组件的有效贡献。&lt;h4&gt;结论&lt;/h4&gt;MM-HCAN为全面的多故障诊断提供了可扩展和鲁棒的解决方案，支持工业环境中的预测维护和资产寿命延长。&lt;h4&gt;翻译&lt;/h4&gt;可靠的感应电机故障诊断对工业安全和运营连续性至关重要，可减轻昂贵的意外停机时间。传统方法往往难以捕捉复杂的多模态信号关系，局限于单模态数据或单一故障类型，并在嘈杂或跨域条件下表现出性能下降。本文提出了多模态超图对比注意力网络(MM-HCAN)，一个用于鲁棒故障诊断的统一框架。据我们所知，MM-HCAN首次将对比学习整合到专为多模态传感器融合设计的超图拓扑中，实现了模态内和模态间依赖关系的联合建模，并增强了超越欧几里得嵌入空间的泛化能力。该模型支持轴承、定子和转子故障的同时诊断，满足了工程上对整合诊断能力的需求。在三个真实世界基准测试中评估，MM-HCAN实现了高达99.82%的准确率，具有强大的跨域泛化能力和对噪声的鲁棒性，证明了其适合实际部署。消融研究验证了每个组件的贡献。MM-HCAN为全面的多故障诊断提供了可扩展和鲁棒的解决方案，支持工业环境中的预测维护和资产寿命延长。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable induction motor (IM) fault diagnosis is vital for industrial safetyand operational continuity, mitigating costly unplanned downtime. Conventionalapproaches often struggle to capture complex multimodal signal relationships,are constrained to unimodal data or single fault types, and exhibit performancedegradation under noisy or cross-domain conditions. This paper proposes theMultimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unifiedframework for robust fault diagnosis. To the best of our knowledge, MM-HCAN isthe first to integrate contrastive learning within a hypergraph topologyspecifically designed for multimodal sensor fusion, enabling the jointmodelling of intra- and inter-modal dependencies and enhancing generalisationbeyond Euclidean embedding spaces. The model facilitates simultaneous diagnosisof bearing, stator, and rotor faults, addressing the engineering need forconsolidated di- agnostic capabilities. Evaluated on three real-worldbenchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domaingeneralisation and resilience to noise, demonstrating its suitability forreal-world deployment. An ablation study validates the contribution of eachcomponent. MM-HCAN provides a scalable and robust solution for comprehensivemulti-fault diagnosis, supporting predictive maintenance and extended assetlongevity in industrial environments.</description>
      <author>example@mail.com (Usman Ali, Ali Zia, Waqas Ali, Umer Ramzan, Abdul Rehman, Muhammad Tayyab Chaudhry, Wei Xiang)</author>
      <guid isPermaLink="false">2510.15547v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval</title>
      <link>http://arxiv.org/abs/2510.15543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种模态组合感知框架，以提高多模态大语言模型作为统一编码器时的鲁棒性，解决了传统对比学习训练的统一编码器容易学习模态捷径的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态检索支持跨模态内容检索，应用广泛。单独编码器方法如CLIP通过对比学习对齐模态特定嵌入，而多模态大语言模型(MLLMs)实现了处理组合输入的统一编码器。&lt;h4&gt;目的&lt;/h4&gt;解决统一编码器使用传统对比学习训练时容易学习模态捷径的问题，提高分布转移下的检索鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出模态组合感知框架，包含偏好损失强制多模态嵌入优于单模态对应部分，以及组合正则化目标将多模态嵌入与单模态部分组成的原型对齐，明确建模组合表示与单模态部分间的结构关系。&lt;h4&gt;主要发现&lt;/h4&gt;在各种基准测试上，该框架在分布外检索方面表现提升，证明了模态组合感知是利用MLLM作为统一编码器时鲁棒组合多模态检索的有效原则。&lt;h4&gt;结论&lt;/h4&gt;模态组合感知框架能有效提高多模态检索的鲁棒性，特别是在分布转移情况下，为多模态检索提供了新的有效原则。&lt;h4&gt;翻译&lt;/h4&gt;多模态检索寻求跨模态（如文本或图像）检索相关内容，支持从AI搜索到内容生成的应用。尽管像CLIP这样的单独编码器方法通过对比学习对齐模态特定嵌入取得了成功，但最近的多模态大语言模型(MLLMs)实现了可以直接处理组合输入的统一编码器。虽然灵活且先进，我们发现使用传统对比学习训练的统一编码器容易学习模态捷径，导致在分布转移下鲁棒性差。我们提出了一种模态组合感知框架来缓解这一问题。具体而言，偏好损失强制多模态嵌入优于其单模态对应部分，而组合正则化目标将多模态嵌入与其单模态部分组成的原型对齐。这些目标明确建模了组合表示与其单模态对应部分之间的结构关系。在各种基准测试上的实验显示分布外检索有所提升，突显了模态组合感知作为利用MLLM作为统一编码器时鲁棒组合多模态检索的有效原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal retrieval, which seeks to retrieve relevant content acrossmodalities such as text or image, supports applications from AI search tocontents production. Despite the success of separate-encoder approaches likeCLIP align modality-specific embeddings with contrastive learning, recentmultimodal large language models (MLLMs) enable a unified encoder that directlyprocesses composed inputs. While flexible and advanced, we identify thatunified encoders trained with conventional contrastive learning are prone tolearn modality shortcut, leading to poor robustness under distribution shifts.We propose a modality composition awareness framework to mitigate this issue.Concretely, a preference loss enforces multimodal embeddings to outperformtheir unimodal counterparts, while a composition regularization objectivealigns multimodal embeddings with prototypes composed from its unimodal parts.These objectives explicitly model structural relationships between the composedrepresentation and its unimodal counterparts. Experiments on various benchmarksshow gains in out-of-distribution retrieval, highlighting modality compositionawareness as a effective principle for robust composed multimodal retrievalwhen utilizing MLLMs as the unified encoder.</description>
      <author>example@mail.com (Qiyu Wu, Shuyang Cui, Satoshi Hayakawa, Wei-Yao Wang, Hiromi Wakaki, Yuki Mitsufuji)</author>
      <guid isPermaLink="false">2510.15543v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm</title>
      <link>http://arxiv.org/abs/2510.14321v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了大型推理嵌入模型(LREM)，通过将推理过程整合到表示学习中，解决了现有嵌入模型在处理困难查询时的性能下降问题，显著提高了电子商务搜索系统的检索准确性。&lt;h4&gt;背景&lt;/h4&gt;在现代电子商务搜索系统中，密集检索已成为不可或缺的组成部分。主流嵌入模型已从BERT转向大型语言模型(LLMs)以获得更准确的文本建模，但这些模型仍采用直接嵌入方法，语义准确性不足。现有模型通过对比学习实现语义对齐，但倾向于捕捉训练数据中的统计共现模式，偏向浅层词汇和语义匹配，导致对与目标物品词汇差异大的困难查询性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有嵌入模型在处理困难查询时的性能下降问题，通过整合推理过程到表示学习中，提高检索准确性，弥合原始查询和目标物品之间的语义差距。&lt;h4&gt;方法&lt;/h4&gt;提出大型推理嵌入模型(LREM)，创新性地将推理过程整合到表示学习中。对于困难查询，LREM首先进行推理以深入理解原始查询，然后生成推理增强的查询嵌入用于检索。采用两阶段训练过程：第一阶段在精心策划的查询-思维链-物品三元组上使用SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习进一步优化推理轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;推理过程有效地弥合了原始查询和目标物品之间的语义差距，显著提高了检索准确性。大量的离线和在线实验验证了LREM的有效性。&lt;h4&gt;结论&lt;/h4&gt;LREM已被部署在中国最大的电子商务平台上，自2025年8月起开始应用。&lt;h4&gt;翻译&lt;/h4&gt;在现代电子商务搜索系统中，密集检索已成为不可或缺的组成部分。通过计算查询和物品(产品)嵌入之间的相似性，它能够从大规模存储库中高效地选择候选产品。随着大型语言模型(LLMs)的突破，主流嵌入模型已经逐渐从BERT转向LLMs以实现更准确的文本建模。然而，这些模型仍然采用直接嵌入方法，嵌入的语义准确性仍然不足。因此，对比学习被大量使用来实现正对之间的紧密语义对齐。结果，这类模型往往会捕捉训练数据中的统计共现模式，使其偏向于浅层词汇和语义匹配。对于与目标物品表现出显著词汇差异的困难查询，性能会显著下降。在这项工作中，我们提出了大型推理嵌入模型(LREM)，创新性地将推理过程整合到表示学习中。对于困难查询，LREM首先进行推理以实现对原始查询的深入理解，然后生成推理增强的查询嵌入用于检索。这个推理过程有效地弥合了原始查询和目标物品之间的语义差距，显著提高了检索准确性。具体来说，我们采用两阶段训练过程：第一阶段在精心策划的查询-思维链-物品三元组上使用SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习(RL)进一步优化推理轨迹。大量的离线和在线实验验证了LREM的有效性，使其自2025年8月起被部署在中国最大的电子商务平台上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern e-commerce search systems, dense retrieval has become anindispensable component. By computing similarities between query and item(product) embeddings, it efficiently selects candidate products fromlarge-scale repositories. With the breakthroughs in large language models(LLMs), mainstream embedding models have gradually shifted from BERT to LLMsfor more accurate text modeling. However, these models still adoptdirect-embedding methods, and the semantic accuracy of embeddings remainsinadequate. Therefore, contrastive learning is heavily employed to achievetight semantic alignment between positive pairs. Consequently, such models tendto capture statistical co-occurrence patterns in the training data, biasingthem toward shallow lexical and semantic matches. For difficult queriesexhibiting notable lexical disparity from target items, the performancedegrades significantly. In this work, we propose the Large Reasoning EmbeddingModel (LREM), which novelly integrates reasoning processes into representationlearning. For difficult queries, LREM first conducts reasoning to achieve adeep understanding of the original query, and then produces areasoning-augmented query embedding for retrieval. This reasoning processeffectively bridges the semantic gap between original queries and target items,significantly improving retrieval accuracy. Specifically, we adopt a two-stagetraining process: the first stage optimizes the LLM on carefully curatedQuery-CoT-Item triplets with SFT and InfoNCE losses to establish preliminaryreasoning and embedding capabilities, and the second stage further refines thereasoning trajectories via reinforcement learning (RL). Extensive offline andonline experiments validate the effectiveness of LREM, leading to itsdeployment on China's largest e-commerce platform since August 2025.</description>
      <author>example@mail.com (Jianting Tang, Dongshuai Li, Tao Wen, Fuyu Lv, Dan Ou, Linli Xu)</author>
      <guid isPermaLink="false">2510.14321v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>BLIP3o-NEXT: Next Frontier of Native Image Generation</title>
      <link>http://arxiv.org/abs/2510.15857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BLIP3o-NEXT是一个全开源的基础模型，统一了文本到图像生成和图像编辑功能，采用自回归+扩散架构，在多种基准测试中表现优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;原生图像生成领域不断发展，需要能够同时处理图像生成和编辑的统一架构，以及探索影响模型性能的关键因素。&lt;h4&gt;目的&lt;/h4&gt;开发一个先进的全开源基础模型，推动原生图像生成的前沿发展，并探索影响模型性能的关键见解。&lt;h4&gt;方法&lt;/h4&gt;采用自回归+扩散架构，自回归模型基于多模态输入生成离散图像令牌，其隐藏状态作为扩散模型的条件信号生成高保真图像；结合后训练和数据引擎提高指令遵循能力和图像一致性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 架构选择对性能影响有限，有效扩展和快速推理是关键；2. 强化学习可进一步推动原生图像生成；3. 图像编辑具有挑战性，但可通过后训练和数据引擎改进；4. 数据质量和规模决定模型性能上限。&lt;h4&gt;结论&lt;/h4&gt;BLIP3o-NEXT通过创新的架构设计和关键见解的应用，实现了图像生成和编辑的新水平，在多种基准测试中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了BLIP3o-NEXT，这是BLIP3系列中的一个全开源基础模型，推动了原生图像生成的下一个前沿。BLIP3o-NEXT将文本到图像生成和图像编辑统一在一个架构中，展示了强大的图像生成和编辑能力。在开发最先进的原生图像生成模型过程中，我们确定了四个关键见解：(1) 大多数架构选择产生可比的性能；只要架构能有效扩展并支持快速推理，就可以被认为是有效的；(2) 强化学习的成功应用可以进一步推动原生图像生成的前沿；(3) 图像编辑仍然具有挑战性，但通过后训练和数据引擎可以显著提高指令遵循能力和生成图像与参考图像之间的一致性；(4) 数据质量和规模仍然是决定模型性能上限的决定性因素。基于这些见解，BLIP3o-NEXT采用自回归+扩散架构，其中自回归模型首先基于多模态输入生成离散图像令牌，然后其隐藏状态被用作扩散模型的条件信号以生成高保真图像。该架构结合了自回归模型的推理强度和指令遵循能力以及扩散模型的精细细节渲染能力，实现了新的连贯性和真实感水平。对各种文本到图像和图像编辑基准的广泛评估表明，BLIP3o-NEXT优于现有模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3series that advances the next frontier of native image generation. BLIP3o-NEXTunifies text-to-image generation and image editing within a singlearchitecture, demonstrating strong image generation and image editingcapabilities. In developing the state-of-the-art native image generation model,we identify four key insights: (1) Most architectural choices yield comparableperformance; an architecture can be deemed effective provided it scalesefficiently and supports fast inference; (2) The successful application ofreinforcement learning can further push the frontier of native imagegeneration; (3) Image editing still remains a challenging task, yet instructionfollowing and the consistency between generated and reference images can besignificantly enhanced through post-training and data engine; (4) Data qualityand scale continue to be decisive factors that determine the upper bound ofmodel performance. Building upon these insights, BLIP3o-NEXT leverages anAutoregressive + Diffusion architecture in which an autoregressive model firstgenerates discrete image tokens conditioned on multimodal inputs, whose hiddenstates are then used as conditioning signals for a diffusion model to generatehigh-fidelity images. This architecture integrates the reasoning strength andinstruction following of autoregressive models with the fine-detail renderingability of diffusion models, achieving a new level of coherence and realism.Extensive evaluations of various text-to-image and image-editing benchmarksshow that BLIP3o-NEXT achieves superior performance over existing models.</description>
      <author>example@mail.com (Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu)</author>
      <guid isPermaLink="false">2510.15857v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling</title>
      <link>http://arxiv.org/abs/2510.15851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于语音的大型语言模型（speech LLMs）在口语理解（SLU）槽填充任务中的应用，通过创建任务上界、识别性能差距并提出改进措施，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;槽填充是口语理解的关键子任务，传统实现方式为级联的语音识别后跟一个或多个自然语言理解组件。新兴的语音大型语言模型为理解任务提供了更统一、生成式和遵循指令的新途径，具有数据效率、计算效率和零样本能力。&lt;h4&gt;目的&lt;/h4&gt;创建槽填充任务的经验上界，确定性能、鲁棒性和泛化差距，并提出改进措施以缩小与上界结果的差距。&lt;h4&gt;方法&lt;/h4&gt;通过改进训练数据、架构和训练策略来提升模型性能，并评估这些改进措施的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;每项改进措施都显著提高了模型性能，同时研究还指出了实际应用中面临的挑战，并为利用这些新兴模型提供了经验指导。&lt;h4&gt;结论&lt;/h4&gt;基于语音的大型语言模型为槽填充任务提供了新的有效途径，但仍需解决实践挑战，进一步优化以实现更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;槽填充是口语理解（SLU）中的一个关键子任务，传统实现方式为级联的语音识别后跟一个或多个自然语言理解（NLU）组件。最近出现的基于语音的大型语言模型（speech LLMs），它整合了语音和文本基础模型，为以更统一、生成式和遵循指令的方式实现语音理解任务开辟了新途径，同时承诺具有数据和计算效率，具有零样本能力，可推广到未见过的槽标签。我们通过为槽填充任务创建经验上界，确定性能、鲁棒性和泛化差距，并提出改进训练数据、架构和训练策略的建议来缩小与上界结果的差距。我们证明这些措施中的每一项都显著提高了性能，同时突出了实践挑战，并为利用这些新兴模型提供了经验指导和见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Slot filling is a crucial subtask in spoken language understanding (SLU),traditionally implemented as a cascade of speech recognition followed by one ormore natural language understanding (NLU) components. The recent advent ofspeech-based large language models (speechLLMs), which integrate speech andtextual foundation models, has opened new avenues for achieving speechunderstanding tasks in a more unified, generative, and instruction-followingmanner while promising data and compute efficiency with zero-shot abilities,generalizing to unseen slot labels. We address the slot-filling task bycreating an empirical upper bound for the task, identifying performance,robustness, and generalization gaps, and proposing improvements to the trainingdata, architecture, and training strategies to narrow the gap with the upperbound result. We show that each of these measures improve performancesubstantially, while highlighting practical challenges and providing empiricalguidance and insights for harnessing these emerging models.</description>
      <author>example@mail.com (Kadri Hacioglu, Manjunath K E, Andreas Stolcke)</author>
      <guid isPermaLink="false">2510.15851v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training</title>
      <link>http://arxiv.org/abs/2510.15596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模模型训练（超过数万个GPU）中的性能变异性问题，提出了PRISM性能建模框架，考虑了大规模分布式训练的随机性质，为训练时间提供概率保证的量化度量。&lt;h4&gt;背景&lt;/h4&gt;大规模模型训练中，训练过程中的中断是必然发生的随机事件，随着训练规模扩大和GPU在受限环境下运行，动态运行时变异会变得更加频繁。在64k GPU规模下，已观察到9%的GPU时间变异性，GEMM工作负载上GPU性能最高有14%的变异。&lt;h4&gt;目的&lt;/h4&gt;理解性能变异性的潜在原因，并探索分布式训练的设计和优化空间，提出一种能考虑训练随机性质的性能建模框架。&lt;h4&gt;方法&lt;/h4&gt;提出PRISM性能建模框架，其核心是统计方法，为训练时间提供概率保证的量化度量。使用该框架探索并行化方法到下一代训练系统的设计和优化空间，并通过真实系统测量进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;1) PRISM框架的训练时间预测准确率为20.8%的Kolmogorov-Smirnov距离；2) 根据计算节点放置的不同，可获得高达1.26倍的性能提升潜力；3) 优化通信内核（如AllGather和ReduceScatter）对最小化训练步骤时间变异贡献最大。&lt;h4&gt;结论&lt;/h4&gt;PRISM框架能够有效建模和优化大规模分布式训练中的性能变异性，通过考虑并行化策略对变异的敏感性，可以显著提高训练效率。&lt;h4&gt;翻译&lt;/h4&gt;数万个GPU以上的大规模模型训练是一个未知领域。在这种规模下，训练过程中的中断不是是否会发生的问题，而是何时会发生的问题——这是一种降低训练生产力的随机过程。随着训练规模扩大和GPU在越来越受限的功率和热应力环境下运行，动态运行时变异将变得越来越频繁。在64k GPU规模下，我们已经观察到前沿基础模型训练有9%的GPU时间变异性。为了理解变异性的潜在原因，我们分析了各种平台上大规模的GPU微基准测试，显示在GEMM工作负载上，GPU性能最高有14%的变异，这取决于训练硬件和部署环境。受我们的分析和围绕性能变异性的广阔设计空间的启发，我们提出了PRISM——一个考虑大规模分布式训练随机性质的性能建模框架。PRISM的核心是一种统计方法，为训练时间提供概率保证的量化度量。使用PRISM，我们探索了分布式训练的设计和优化空间，从并行化方法到下一代训练系统。PRISM通过真实系统测量进行了验证，显示出训练时间预测准确率为20.8%的Kolmogorov-Smirnov距离。使用PRISM，我们证明，如果考虑并行化策略对变异的敏感性，根据计算节点放置的不同，可获得高达1.26倍的性能提升潜力。此外，我们使用PRISM识别了为减少性能变异而优化的内核，并预测了在变异被放大的大规模作业中减速的概率。我们发现优化通信内核，如AllGather和ReduceScatter，对最小化训练步骤时间变异贡献最大。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large model training beyond tens of thousands of GPUs is an unchartedterritory. At such scales, disruptions to the training process are not a matterof if, but a matter of when -- a stochastic process degrading trainingproductivity. Dynamic runtime variation will become increasingly more frequentas training scales up and GPUs are operated in increasingly power-limited andthermally-stressed environments. At the 64k GPU scale, we already observed 9%GPU time variability for frontier foundation model training. To understandpotential causes of variability, we analyze GPU microbenchmarks at scale acrossa variety of platforms, showing up to 14% variation in GPU performance on GEMMworkloads depending on training hardware and deployed environment.  Motivated by our analysis and the large design space around performancevariability, we present PRISM -- a performance modeling framework thatconsiders the stochastic nature of the large-scale distributed training. Thecore of PRISM is the statistical method that provides a quantifiable measurefor probabilistic guarantees on training time. Using PRISM, we explore thedesign and optimization space of distributed training, from parallelizationmethods to next-generation training systems. PRISM is validated withreal-system measurement, showing training time prediction accuracy with 20.8%Kolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending oncomputation node placement, up to 1.26x performance improvement potential isavailable if we factor in sensitivities of parallelization strategies tovariation. In addition, we use PRISM to identify kernels to optimize forreducing performance variability and predict probability of slow-down forlarge-scale jobs where variation is magnified. We find optimizing communicationkernels, such as AllGather and ReduceScatter, contribute most to minimizingvariability in training step time.</description>
      <author>example@mail.com (Alicia Golden, Michael Kuchnik, Samuel Hsia, Zachary DeVito, Gu-Yeon Wei, David Brooks, Carole-Jean Wu)</author>
      <guid isPermaLink="false">2510.15596v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VO-DP的视觉单视图扩散策略学习方法，利用预训练的视觉基础模型实现语义和几何特征的有效融合，在机器人操作任务中表现出色，特别是在真实世界任务中显著优于基于点云的方法。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习中，基于视觉运动扩散策略学习是机器人操作的主要方向之一。大多数这类方法依赖点云作为观察输入，并通过点云特征学习构建场景表示，从而实现显著精度。然而，现有文献对纯视觉解决方案的深入探索不足，尽管这些方案具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;探索一种仅依赖视觉的单视图扩散策略学习方法(VO-DP)，以充分利用视觉基础模型的能力，实现语义和几何特征的有效融合，并评估其在模拟和真实世界任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为VO-DP的视觉单视图扩散策略学习方法，利用预训练的视觉基础模型实现语义和几何特征的有效融合。具体包括：利用VGGT的中间特征，融合DINOv2的语义特征和交替注意力块的几何特征，通过交叉注意力融合特征，并使用CNN进行空间压缩形成策略头输入。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在模拟任务中，VO-DP平均成功率达到64.6%，与DP3(64.0%)相当，远高于DP(34.8%)；2) 在真实世界任务中，VO-DP达到87.9%的成功率，显著优于DP3(67.5%)和DP(11.2%)；3) VO-DP在颜色、大小、背景和光照等变化条件下表现出高度稳定性；4) VO-DP在真实世界任务中的性能明显优于基于点云的方法DP3。&lt;h4&gt;结论&lt;/h4&gt;VO-DP是一种有效的视觉单视图扩散策略学习方法，在模拟和真实世界任务中均表现出色，特别是在真实环境中显著优于现有方法。该方法为机器人操作领域提供了一种纯视觉解决方案，展示了视觉基础模型在机器人学习中的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习的背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数这些方法依赖点云作为观察输入，并通过点云特征学习构建场景表示，从而实现显著精度。然而，现有文献对纯视觉解决方案的深入探索不足，尽管这些方案具有巨大潜力。在本文中，我们提出了一种视觉单视图扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合。我们利用VGGT的中间特征，融合DINOv2的语义特征和交替注意力块的几何特征。特征通过交叉注意力融合，并通过CNN进行空间压缩，形成策略头的输入。大量实验证明，VO-DP不仅显著优于纯视觉基线DP，而且与基于点云的方法DP3表现出不同的性能趋势：在模拟任务中，VO-DP平均成功率达到64.6%，与DP3的64.0%相当，远高于DP的34.8%；而在真实世界任务中，它达到87.9%，以显著优势分别超过DP3的67.5%和DP的11.2%。进一步的鲁棒性评估证实，VO-DP在颜色、大小、背景和光照等变化条件下保持高度稳定。最后，我们开源了一个机器人操作训练库。该库基于Accelerate构建，支持多机多GPU并行训练和混合精度训练。它兼容DP、DP3和VO-DP等视觉运动策略，并支持RoboTwin模拟器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction</title>
      <link>http://arxiv.org/abs/2510.15386v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种名为PFGS的姿态感知3D高斯飞溅框架，用于从多姿态图像捕获中重建完整物体。该方法通过迭代融合辅助姿态的图像到主姿态的统一3DGS表示中，结合全局和局部配准策略，有效解决了现有方法在重建遮挡或自遮挡区域时的不完整问题。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅的最新进展已实现了从多视图图像生成高质量、实时的novel-view synthesis。然而，大多数现有方法假设物体以单一静态姿态被捕获，导致重建不完整，缺失了被遮挡或自遮挡区域。此外，最近的3D基础模型在提高配准鲁棒性和效率方面取得了进展，但仍受限于高内存需求和次优准确性。&lt;h4&gt;目的&lt;/h4&gt;解决从多姿态图像捕获中重建完整物体的实际挑战，克服现有方法在重建遮挡区域时的不完整性问题，并解决基础模型在配准过程中的高内存需求和次优准确性问题。&lt;h4&gt;方法&lt;/h4&gt;PFGS姿态感知3DGS框架，给定物体在一个主姿态和几个辅助姿态的图像，迭代地将每个辅助集融合到主姿态的统一3DGS表示中。采用姿态感知融合策略，结合全局和局部配准来有效合并视图并优化3DGS模型。通过更智能地将基础模型整合到配准过程中来克服挑战：利用背景特征进行每姿态相机姿态估计，并使用基础模型进行跨姿态配准。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PFGS在定性和定量评估中始终优于强大的基线方法，产生更完整的重建和更高保真度的3DGS模型。&lt;h4&gt;结论&lt;/h4&gt;PFGS通过智能整合基础模型到配准过程中，结合了两种方法的优势，同时解决了背景不一致问题，实现了从多姿态图像捕获中重建完整物体的目标。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D高斯飞溅(3DGS)进展已经能够从多视图图像实现高质量、实时的novel-view synthesis。然而，大多数现有方法假设物体以单一静态姿态被捕获，导致重建不完整，缺失了被遮挡或自遮挡区域。我们引入了PFGS，一种姿态感知的3DGS框架，解决了从多姿态图像捕获中重建完整物体的实际挑战。给定物体在一个主姿态和几个辅助姿态的图像，PFGS迭代地将每个辅助集融合到主姿态的统一3DGS表示中。我们的姿态感知融合策略结合了全局和局部配准，以有效合并视图并优化3DGS模型。虽然最近的3D基础模型进展提高了配准的鲁棒性和效率，但仍受限于高内存需求和次优准确性。PFGS通过更智能地将它们整合到配准过程中克服了这些挑战：它利用背景特征进行每姿态相机姿态估计，并使用基础模型进行跨姿态配准。这种设计结合了两种方法的优势，同时解决了背景不一致问题。实验结果表明，PFGS在定性和定量评估中始终优于强大的基线方法，产生更完整的重建和更高保真度的3DGS模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从多姿态图像捕获中重建完整3D物体的问题。现有的3D高斯泼溅方法假设物体在单一静态姿态下被捕获，导致重建不完整，特别是会错过被物体自身遮挡的区域。这个问题在现实中非常重要，因为完整3D重建对虚拟现实、增强现实、机器人技术和数字孪生等应用至关重要，而单一姿态无法获取物体的全部表面信息，特别是在自遮挡的情况下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有3D高斯泼溅方法在处理多姿态物体重建时的局限性，并识别出多姿态重建面临的技术挑战：物体姿态变化导致无法使用传统SfM技术、跨姿态变化破坏对应估计、合并独立重建模型会引入伪影。作者借鉴了3D基础模型（如Fast3R）来提高配准鲁棒性，采用轮廓共识融合策略对齐不同姿态相机，并使用背景特征进行姿态估计。他们设计了一个三阶段管道（全局配准、局部配准、3DGS模型完成）来解决这些问题，并通过实验验证了方法的有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用姿态感知融合策略，将不同姿态捕获的图像有效合并到一个统一的3D高斯泼溅表示中，结合全局和局部配准技术处理多姿态重建挑战。整体流程包括：1）预处理阶段构建初始3DGS并估计相机姿态；2）全局配准阶段选择混合姿态图像、使用3D基础模型估计姿态、通过两阶段轮廓共识融合对齐坐标系；3）局部配准阶段使用轮廓引导和光度目标优化进一步精炼对齐；4）3DGS模型完成阶段使用平衡采样策略微调模型；5）迭代过程逐步融入更多辅助姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出PFGS框架实现多姿态3D高斯泼溅物体的增量重建；2）设计有效的全局配准方法对齐不同姿态图像集；3）提出混合姿态图像选择策略确保几何一致性；4）开发两阶段轮廓共识融合策略统一坐标系；5）提出平衡采样策略处理图像数量不平衡问题。相比之前工作，PFGS能处理物体姿态变化（不同于传统SfM），解决了3D基础模型的内存和精度问题，专门针对多姿态重建优化（不同于其他3D高斯泼溅方法），不依赖顺序输入结构（不同于在线重建方法），专注于合并不同姿态的部分重建（不同于物体聚焦方法）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PFGS通过姿态感知融合策略，将多姿态图像捕获合并为完整3D高斯泼溅表示，解决了传统方法在处理物体自遮挡和跨姿态变化时的局限性，实现了更完整、更高保真度的3D物体重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,real-time novel-view synthesis from multi-view images. However, most existingmethods assume the object is captured in a single, static pose, resulting inincomplete reconstructions that miss occluded or self-occluded regions. Weintroduce PFGS, a pose-aware 3DGS framework that addresses the practicalchallenge of reconstructing complete objects from multi-pose image captures.Given images of an object in one main pose and several auxiliary poses, PFGSiteratively fuses each auxiliary set into a unified 3DGS representation of themain pose. Our pose-aware fusion strategy combines global and localregistration to merge views effectively and refine the 3DGS model. While recentadvances in 3D foundation models have improved registration robustness andefficiency, they remain limited by high memory demands and suboptimal accuracy.PFGS overcomes these challenges by incorporating them more intelligently intothe registration process: it leverages background features for per-pose camerapose estimation and employs foundation models for cross-pose registration. Thisdesign captures the best of both approaches while resolving backgroundinconsistency issues. Experimental results demonstrate that PFGS consistentlyoutperforms strong baselines in both qualitative and quantitative evaluations,producing more complete reconstructions and higher-fidelity 3DGS models.</description>
      <author>example@mail.com (Ting-Yu Yen, Yu-Sheng Chiu, Shih-Hsuan Hung, Peter Wonka, Hung-Kuo Chu)</author>
      <guid isPermaLink="false">2510.15386v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Symmetric Entropy-Constrained Video Coding for Machines</title>
      <link>http://arxiv.org/abs/2510.15347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is prepared to submit to the IEEE Transactions&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对称熵约束的视频编码框架(SEC-VCM)，通过建立视频编解码器与视觉主干之间的对称对齐，优化机器视觉系统的视频编码效果。&lt;h4&gt;背景&lt;/h4&gt;视频传输越来越多地服务于机器视觉系统(MVS)而非人类视觉系统(HVS)，视频编码为机器(VCM)已成为关键研究课题。现有VCM方法通常将编解码器绑定到特定下游模型，需要重新训练或有监督数据，限制了多任务场景中的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种对称熵约束的视频编码框架用于机器(SEC-VCM)，建立视频编解码器与视觉主干之间的对称对齐，使编解码器能够利用视觉主干的表示能力保留语义信息并丢弃机器视觉系统无关的信息。&lt;h4&gt;方法&lt;/h4&gt;1) 提出对称熵约束的视频编码框架(SEC-VCM)；2) 建立视频编解码器与视觉主干之间的对称对齐；3) 采用双向熵约束(BiEC)机制确保视频解码和视觉主干编码过程的对称性；4) 通过语义-像素双路径融合(SPDF)模块将像素级先验注入到最终重建中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架在速率-任务性能方面达到了最先进水平，与VTM相比，在视频实例分割(节省37.41%比特率)、视频对象分割(29.83%)、目标检测(46.22%)和多目标跟踪(44.94%)任务上实现了显著的比特率节省。&lt;h4&gt;结论&lt;/h4&gt;SEC-VCM框架通过建立视频编解码器与视觉主干之间的对称对齐，有效利用了视觉主干的表示能力，保留了语义信息并丢弃了机器视觉系统无关的信息，显著提高了机器导向的重建质量。&lt;h4&gt;翻译&lt;/h4&gt;随着视频传输越来越多地服务于机器视觉系统(MVS)而非人类视觉系统(HVS)，视频编码为机器(VCM)已成为关键研究课题。现有的VCM方法通常将编解码器绑定到特定下游模型，需要重新训练或有监督数据，从而限制了多任务场景中的泛化能力。最近，统一的VCM框架采用视觉主干(VB)和视觉基础模型(VFM)来支持单个编解码器完成多个视频理解任务。它们主要利用VB/VFM来保持语义一致性或抑制非语义信息，但很少探索如何在VB/VFM指导下直接将视频编码与理解联系起来。因此，我们提出了面向机器的对称熵约束视频编码框架(SEC-VCM)。它在视频编解码器和视觉主干之间建立了对称对齐，使编解码器能够利用视觉主干的表示能力来保留语义并丢弃机器视觉系统无关的信息。具体而言，双向熵约束(BiEC)机制通过抑制条件熵确保视频解码和视觉主干编码过程的对称性。这有助于编解码器明确处理对机器视觉系统有益的语义信息，同时压缩无用信息。此外，语义-像素双路径融合(SPDF)模块将像素级先验注入到最终重建中。通过语义-像素融合，它抑制了对机器视觉系统有害的伪影，并提高了机器导向的重建质量。实验结果表明，我们的框架在速率-任务性能方面达到了最先进水平，与VTM相比，在视频实例分割(节省37.41%)、视频对象分割(29.83%)、目标检测(46.22%)和多目标跟踪(44.94%)任务上实现了显著的比特率节省。我们将发布我们的代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As video transmission increasingly serves machine vision systems (MVS)instead of human vision systems (HVS), video coding for machines (VCM) hasbecome a critical research topic. Existing VCM methods often bind codecs tospecific downstream models, requiring retraining or supervised data and thuslimiting generalization in multi-task scenarios. Recently, unified VCMframeworks have employed visual backbones (VB) and visual foundation models(VFM) to support multiple video understanding tasks with a single codec. Theymainly utilize VB/VFM to maintain semantic consistency or suppress non-semanticinformation, but seldom explore how to directly link video coding withunderstanding under VB/VFM guidance. Hence, we propose a SymmetricEntropy-Constrained Video Coding framework for Machines (SEC-VCM). Itestablishes a symmetric alignment between the video codec and VB, allowing thecodec to leverage VB's representation capabilities to preserve semantics anddiscard MVS-irrelevant information. Specifically, a bi-directionalentropy-constraint (BiEC) mechanism ensures symmetry between the process ofvideo decoding and VB encoding by suppressing conditional entropy. This helpsthe codec to explicitly handle semantic information beneficial for MVS whilesqueezing useless information. Furthermore, a semantic-pixel dual-path fusion(SPDF) module injects pixel-level priors into the final reconstruction. Throughsemantic-pixel fusion, it suppresses artifacts harmful to MVS and improvesmachine-oriented reconstruction quality. Experimental results show ourframework achieves state-of-the-art (SOTA) in rate-task performance, withsignificant bitrate savings over VTM on video instance segmentation (37.41%),video object segmentation (29.83%), object detection (46.22%), and multipleobject tracking (44.94%). We will release our code.</description>
      <author>example@mail.com (Yuxiao Sun, Yao Zhao, Meiqin Liu, Chao Yao, Jian Jin, Weisi Lin)</author>
      <guid isPermaLink="false">2510.15347v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition</title>
      <link>http://arxiv.org/abs/2510.15280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨基础模型(FMs)如GPT-4和AlphaFold如何推动科学研究向新范式转变，提出三阶段框架描述这一演变过程，并回顾当前应用、识别风险与未来方向。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)如GPT-4和AlphaFold正在改变科学研究格局，它们不仅加速假设生成、实验设计和结果解释等任务，还引发了一个根本性问题：FMs是仅仅增强现有科学方法，还是重新定义科学研究的进行方式？&lt;h4&gt;目的&lt;/h4&gt;支持科学界理解基础模型(FMs)的变革作用，促进对科学发现未来的反思，并通过提出的三阶段框架帮助理解FMs如何催化科学研究的范式转变。&lt;h4&gt;方法&lt;/h4&gt;提出一个三阶段框架描述基础模型(FMs)在科学中的演变：(1)元科学整合阶段，FMs增强传统范式中的工作流程；(2)混合人机共创阶段，FMs成为问题制定、推理和发现的积极合作者；(3)自主科学发现阶段，FMs作为独立运行，能够在最少人类干预下生成新科学知识。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型(FMs)正在催化向新科学范式的转变，通过三个阶段逐步演变：从增强传统工作流程，到成为人机共创的积极合作者，最终实现自主科学发现。作者还回顾了FMs在现有科学范式中的应用和新兴能力，并确定了相关风险和未来发展方向。&lt;h4&gt;结论&lt;/h4&gt;基础模型(FMs)不仅仅是增强现有科学方法论的工具，而是正在重新定义科学研究的进行方式，推动科学向新范式转变。科学社区需要理解这一变革性作用，并反思科学发现的未来。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)如GPT-4和AlphaFold正在重塑科学研究格局。除了加速假设生成、实验设计和结果解释等任务外，它们还引发了一个更根本的问题：FMs仅仅是增强现有科学方法论，还是重新定义了科学研究的进行方式？在本文中，我们认为FMs正在催化向新科学范式的转变。我们引入了一个三阶段框架来描述这一演变：(1)元科学整合，FMs增强传统范式中的工作流程；(2)混合人机共创，FMs成为问题制定、推理和发现的积极合作者；(3)自主科学发现，FMs作为独立运行，能够在最少人类干预下生成新科学知识。通过这一视角，我们回顾了FMs在现有科学范式中的应用和新兴能力。我们进一步确定了FMs赋能的科学发现的风险和未来方向。这篇立场论文旨在支持科学界理解FMs的变革作用，并促进对科学发现未来的反思。我们的项目可在https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping thelandscape of scientific research. Beyond accelerating tasks such as hypothesisgeneration, experimental design, and result interpretation, they prompt a morefundamental question: Are FMs merely enhancing existing scientificmethodologies, or are they redefining the way science is conducted? In thispaper, we argue that FMs are catalyzing a transition toward a new scientificparadigm. We introduce a three-stage framework to describe this evolution: (1)Meta-Scientific Integration, where FMs enhance workflows within traditionalparadigms; (2) Hybrid Human-AI Co-Creation, where FMs become activecollaborators in problem formulation, reasoning, and discovery; and (3)Autonomous Scientific Discovery, where FMs operate as independent agentscapable of generating new scientific knowledge with minimal human intervention.Through this lens, we review current applications and emerging capabilities ofFMs across existing scientific paradigms. We further identify risks and futuredirections for FM-enabled scientific discovery. This position paper aims tosupport the scientific community in understanding the transformative role ofFMs and to foster reflection on the future of scientific discovery. Our projectis available athttps://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.</description>
      <author>example@mail.com (Fan Liu, Jindong Han, Tengfei Lyu, Weijia Zhang, Zhe-Rui Yang, Lu Dai, Cancheng Liu, Hao Liu)</author>
      <guid isPermaLink="false">2510.15280v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025</title>
      <link>http://arxiv.org/abs/2510.15217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;第六届健康、推理和学习年度会议(CHIL 2025)于2025年6月在美国加州大学伯克利分校举行，会议举办了8个研究圆桌会议，促进机器学习与医疗保健交叉领域的协作讨论。&lt;h4&gt;背景&lt;/h4&gt;会议由健康学习与推理协会(AHLI)主办，于2025年6月25-27日在美国加州大学伯克利分校举行。&lt;h4&gt;目的&lt;/h4&gt;促进机器学习和医疗保健交叉领域的协作小组对话，讨论关键挑战、探索新兴机会、构思可行方向。&lt;h4&gt;方法&lt;/h4&gt;举办研究圆桌会议，每个圆桌会议由高级和初级主席共同主持，强调开放交流、智力好奇心和包容性参与。&lt;h4&gt;主要发现&lt;/h4&gt;会议涵盖了8个主题：'可解释性、可解释性和透明度'、'不确定性、偏见和公平性'、'因果关系'、'领域适应'、'基础模型'、'从小型医疗数据学习'、'多模态方法'和'可扩展、可转化的医疗保健解决方案'。&lt;h4&gt;结论&lt;/h4&gt;通过8个由19位圆桌主席主持的圆桌会议，会议促进了该领域的严格讨论、机会探索和方向构思。&lt;h4&gt;翻译&lt;/h4&gt;第六届健康、推理和学习年度会议(CHIL 2025)由健康学习与推理协会(AHLI)主办，于2025年6月25-27日在美国加州大学伯克利分校举行。作为今年计划的一部分，我们举办了研究圆桌会议，以促进机器学习和医疗保健交叉领域关键及时话题的协作小组对话。每个圆桌会议由高级和初级主席团队主持，他们促进了开放交流、智力好奇心和包容性参与。会议强调对关键挑战的严格讨论、新兴机会的探索以及该领域可行方向的集体构思。总共有19位圆桌主席主持了8个圆桌会议，主题包括'可解释性、可解释性和透明度'、'不确定性、偏见和公平性'、'因果关系'、'领域适应'、'基础模型'、'从小型医疗数据学习'、'多模态方法'和'可扩展、可转化的医疗保健解决方案'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),hosted by the Association for Health Learning and Inference (AHLI), was held inperson on June 25-27, 2025, at the University of California, Berkeley, inBerkeley, California, USA. As part of this year's program, we hosted ResearchRoundtables to catalyze collaborative, small-group dialogue around critical,timely topics at the intersection of machine learning and healthcare. Eachroundtable was moderated by a team of senior and junior chairs who fosteredopen exchange, intellectual curiosity, and inclusive engagement. The sessionsemphasized rigorous discussion of key challenges, exploration of emergingopportunities, and collective ideation toward actionable directions in thefield. In total, eight roundtables were held by 19 roundtable chairs on topicsof "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learningfrom Small Medical Data," "Multimodal Methods," and "Scalable, TranslationalHealthcare Solutions."</description>
      <author>example@mail.com (Emily Alsentzer, Marie-Laure Charpignon, Bill Chen, Niharika D'Souza, Jason Fries, Yixing Jiang, Aparajita Kashyap, Chanwoo Kim, Simon Lee, Aishwarya Mandyam, Ashery Christopher Mbilinyi, Nikita Mehandru, Nitish Nagesh, Brighton Nuwagira, Emma Pierson, Arvind Pillai, Akane Sano, Tanveer Syeda-Mahmood, Shashank Yadav, Elias Adhanom, Muhammad Umar Afza, Amelia Archer, Suhana Bedi, Vasiliki Bikia, Trenton Chang, George H. Chen, Winston Chen, Erica Chiang, Edward Choi, Octavia Ciora, Paz Dozie-Nnamah, Shaza Elsharief, Matthew Engelhard, Ali Eshragh, Jean Feng, Josh Fessel, Scott Fleming, Kei Sen Fong, Thomas Frost, Soham Gadgil, Judy Gichoya, Leeor Hershkovich, Sujeong Im, Bhavya Jain, Vincent Jeanselme, Furong Jia, Qixuan, Jin, Yuxuan Jin, Daniel Kapash, Geetika Kapoor, Behdokht Kiafar, Matthias Kleiner, Stefan Kraft, Annika Kumar, Daeun Kyung, Zhongyuan Liang, Joanna Lin, Qianchu, Liu, Chang Liu, Hongzhou Luan, Chris Lunt, Leopoldo Julían Lechuga López, Matthew B. A. McDermott, Shahriar Noroozizadeh, Connor O'Brien, YongKyung Oh, Mixail Ota, Stephen Pfohl, Meagan Pi, Tanmoy Sarkar Pias, Emma Rocheteau, Avishaan Sethi, Toru Shirakawa, Anita Silver, Neha Simha, Kamile Stankeviciute, Max Sunog, Peter Szolovits, Shengpu Tang, Jialu Tang, Aaron Tierney, John Valdovinos, Byron Wallace, Will Ke Wang, Peter Washington, Jeremy Weiss, Daniel Wolfe, Emily Wong, Hye Sun Yun, Xiaoman Zhang, Xiao Yu Cindy Zhang, Hayoung Jeong, Kaveri A. Thakoor)</author>
      <guid isPermaLink="false">2510.15217v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection</title>
      <link>http://arxiv.org/abs/2510.15202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了分布外(OOD)检测中马氏距离方法的可靠性问题，分析了表示几何和规范化对性能的影响，并提出了一种新的径向缩放ℓ2规范化方法。&lt;h4&gt;背景&lt;/h4&gt;OOD检测对于深度学习模型的可靠部署至关重要，而马氏距离方法虽被广泛使用，但其表示几何和规范化对性能的影响尚未被充分理解，这可能限制其下游应用。&lt;h4&gt;目的&lt;/h4&gt;解决对马氏距离方法中表示几何和规范化影响理解不足的问题，通过全面的实证研究探索这些因素与OOD性能的关系。&lt;h4&gt;方法&lt;/h4&gt;研究进行了跨不同图像基础模型、数据集和距离规范化方案的实证分析，定义了数据表示的理想几何形状，分析了规范化对OOD性能的影响，并提出了径向缩放ℓ2规范化方法。&lt;h4&gt;主要发现&lt;/h4&gt;基于马氏距离的方法并非普遍可靠；光谱和内在维度指标可以准确预测模型的OOD性能；规范化对OOD性能有显著影响。&lt;h4&gt;结论&lt;/h4&gt;提出的径向缩放ℓ2规范化方法通过引入可调参数控制特征空间的径向几何，系统性收缩或扩展表示以显著提高OOD检测性能，为设计更有效和可靠的深度学习模型提供了新见解。&lt;h4&gt;翻译&lt;/h4&gt;该研究探讨了分布外(OOD)检测中马氏距离方法的可靠性问题，分析了表示几何和规范化对性能的影响，并提出了一种新的径向缩放ℓ2规范化方法。OOD检测对于深度学习模型的可靠部署至关重要，而马氏距离方法虽被广泛使用，但其表示几何和规范化对性能的影响尚未被充分理解，这可能限制其下游应用。研究旨在解决对马氏距离方法中表示几何和规范化影响理解不足的问题，通过全面的实证研究探索这些因素与OOD性能的关系。研究进行了跨不同图像基础模型、数据集和距离规范化方案的实证分析，定义了数据表示的理想几何形状，分析了规范化对OOD性能的影响，并提出了径向缩放ℓ2规范化方法。主要发现包括：基于马氏距离的方法并非普遍可靠；光谱和内在维度指标可以准确预测模型的OOD性能；规范化对OOD性能有显著影响。结论是，提出的径向缩放ℓ2规范化方法通过引入可调参数控制特征空间的径向几何，系统性收缩或扩展表示以显著提高OOD检测性能，为设计更有效和可靠的深度学习模型提供了新见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution (OOD) detection is critical for the reliable deploymentof deep learning models. hile Mahalanobis distance methods are widely used, theimpact of representation geometry and normalization on their performance is notfully understood, which may limit their downstream application. To address thisgap, we conducted a comprehensive empirical study across diverse imagefoundation models, datasets, and distance normalization schemes. First, ouranalysis shows that Mahalanobis-based methods aren't universally reliable.Second, we define the ideal geometry for data representations and demonstratethat spectral and intrinsic-dimensionality metrics can accurately predict amodel's OOD performance. Finally, we analyze how normalization impacts OODperformance. Building upon these studies, we propose radially scaled $\ell_2$normalization, a method that generalizes the standard $\ell_2$ normalizationrecently applied to Mahalanobis-based OOD detection. Our approach introduces atunable parameter to directly control the radial geometry of the feature space,systematically contracting or expanding representations to significantlyimprove OOD detection performance. By bridging the gap between representationgeometry, normalization, and OOD performance, our findings offer new insightsinto the design of more effective and reliable deep learning models.</description>
      <author>example@mail.com (Denis Janiak, Jakub Binkowski, Tomasz Kajdanowicz)</author>
      <guid isPermaLink="false">2510.15202v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>The Economics of AI Foundation Models: Openness, Competition, and Governance</title>
      <link>http://arxiv.org/abs/2510.15200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了基础模型生态系统中开放性的战略选择及其经济影响，发现开放性具有双重效应，并揭示了现有开发者的最优开放性策略与数据飞轮效应强度呈非单调关系，形成了'开放性陷阱'现象。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FM)生态系统中'开放性'的战略选择已成为一个关键问题，但其背后的经济驱动因素尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;分析开放性如何影响AI价值链中的竞争，包括现有开发者、下游部署者和新进入开发者之间的互动关系。&lt;h4&gt;方法&lt;/h4&gt;构建了一个两期博弈论模型，研究开放性对竞争格局的影响。&lt;h4&gt;主要发现&lt;/h4&gt;开放性具有双重效应：增强知识溢出到新进入者，同时通过'数据飞轮效应'增强现有开发者优势；现有开发者的最优开放性策略与数据飞轮效应强度呈非单调关系；中等数据飞轮效应下，现有开发者会战略性地限制开放性；形成了'开放性陷阱'，即透明度要求可能适得其反；垂直整合和政府补贴等干预措施也可能无效。&lt;h4&gt;结论&lt;/h4&gt;通过建模开发者对竞争和监管压力的战略反应，为分析复杂且快速发展的FM生态系统中的竞争和设计有效政策提供了稳健的框架。&lt;h4&gt;翻译&lt;/h4&gt;基础模型生态系统中'开放性'的战略选择已成为一个关键问题。虽然这一选择引发了激烈辩论，但其背后的经济驱动因素尚未得到充分探索。我们构建了一个两期博弈论模型，分析开放性如何影响AI价值链中的竞争，涉及现有开发者、下游部署者和新进入开发者。开放性产生双重效应：它增强了知识溢出到新进入者，但也通过'数据飞轮效应'增强了现有开发者的优势，即更大的用户参与度进一步降低了部署者未来的微调成本。我们的分析显示，现有开发者的第一期最优开放性强度与数据飞轮效应强度呈非单调关系。当数据飞轮效应较弱或非常强时，现有开发者偏好更高水平的开放性；然而，在中等范围内，它会战略性地限制开放性以损害新进入者的学习。这种动态导致了'开放性陷阱'，这是一个关键的政策悖论，即透明度要求可能适得其反，消除企业的战略灵活性，减少投资并降低福利。我们扩展了模型，表明其他常见的干预措施可能同样无效。例如，垂直整合只有在数据飞轮效应足够强到克服潜在更高效竞争对手的损失时才有利于生态系统。同样，旨在促进采用的政府补贴可能被现有开发者通过战略性的价格和开放性调整完全获取，使价值链的其他部分处境更差。通过建模开发者对竞争和监管压力的战略反应，我们为在复杂且快速发展的FM生态系统中分析竞争和设计有效政策提供了稳健的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The strategic choice of model "openness" has become a defining issue for thefoundation model (FM) ecosystem. While this choice is intensely debated, itsunderlying economic drivers remain underexplored. We construct a two-periodgame-theoretic model to analyze how openness shapes competition in an AI valuechain, featuring an incumbent developer, a downstream deployer, and an entrantdeveloper. Openness exerts a dual effect: it amplifies knowledge spillovers tothe entrant, but it also enhances the incumbent's advantage through a "dataflywheel effect," whereby greater user engagement today further lowers thedeployer's future fine-tuning cost. Our analysis reveals that the incumbent'soptimal first-period openness is surprisingly non-monotonic in the strength ofthe data flywheel effect. When the data flywheel effect is either weak or verystrong, the incumbent prefers a higher level of openness; however, for anintermediate range, it strategically restricts openness to impair the entrant'slearning. This dynamic gives rise to an "openness trap," a critical policyparadox where transparency mandates can backfire by removing firms' strategicflexibility, reducing investment, and lowering welfare. We extend the model toshow that other common interventions can be similarly ineffective. Verticalintegration, for instance, only benefits the ecosystem when the data flywheeleffect is strong enough to overcome the loss of a potentially more efficientcompetitor. Likewise, government subsidies intended to spur adoption can becaptured entirely by the incumbent through strategic price and opennessadjustments, leaving the rest of the value chain worse off. By modeling thedeveloper's strategic response to competitive and regulatory pressures, weprovide a robust framework for analyzing competition and designing effectivepolicy in the complex and rapidly evolving FM ecosystem.</description>
      <author>example@mail.com (Fasheng Xu, Xiaoyu Wang, Wei Chen, Karen Xie)</author>
      <guid isPermaLink="false">2510.15200v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Hyperparameter Optimization and Reproducibility in Deep Learning Model Training</title>
      <link>http://arxiv.org/abs/2510.15164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了病理学基础模型训练中的可重复性挑战，调查了软件随机性、硬件非确定性和超参数报告不一致性问题，并通过系统评估不同超参数设置和数据增强策略的影响，提供了实用规则来指导未来开发可重复的数字病理学基础模型。&lt;h4&gt;背景&lt;/h4&gt;基础模型训练在病理学领域面临可重复性挑战，这些挑战通常由软件随机性、硬件非确定性以及超参数报告不一致性所阻碍。&lt;h4&gt;目的&lt;/h4&gt;调查这些问题，通过系统评估不同超参数设置和数据增强策略对模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;在QUILT-1M数据集上训练了一个CLIP模型，并在三个下游病理学数据集（PatchCamelyon、LC25000-Lung和LC25000-Colon）上系统评估了不同超参数设置和数据增强策略的影响。&lt;h4&gt;主要发现&lt;/h4&gt;• 图像裁剪策略中，中等程度的随机裁剪比更激进或更保守的设置表现更好；• 不带局部损失的分布式训练提高了模型稳定性；• 较低的学习率在所有数据集上均降低了模型性能；• 结肠组织数据集提供了最可重复的基准测试结果。&lt;h4&gt;结论&lt;/h4&gt;计算病理学中的可重复性不仅依赖于透明的文档记录，还依赖于精心选择的实验配置，并提供了实用规则来指导未来开发可重复的数字病理学基础模型的工作。&lt;h4&gt;翻译&lt;/h4&gt;可重复性仍然是病理学基础模型训练中的一个关键挑战，常常受到软件随机性、硬件非确定性和不一致的超参数报告的阻碍。为了调查这些问题，我们在QUILT-1M数据集上训练了一个CLIP模型，并系统评估了不同超参数设置和数据增强策略在三个下游病理学数据集（PatchCamelyon、LC25000-Lung和LC25000-Colon）上的影响。尽管不同运行之间存在变异性，我们确定了明显的趋势：RandomResizedCrop值为0.7-0.8比更激进(0.6)或保守(0.9)的设置表现更好，不带局部损失的分布式训练提高了稳定性，学习率低于5.0e-5在所有数据集上一致降低了性能。LC25000(Colon)数据集始终提供了最可重复的基准。这些发现强调，计算病理学中的可重复性不仅依赖于透明的文档记录，还依赖于精心选择的实验配置，我们提供了实用规则来指导未来开发可重复的数字病理学基础模型的工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reproducibility remains a critical challenge in foundation model training forhistopathology, often hindered by software randomness, hardwarenon-determinism, and inconsistent hyperparameter reporting. To investigatethese issues, we trained a CLIP model on the QUILT-1M dataset andsystematically evaluated the impact of different hyperparameter settings andaugmentation strategies across three downstream histopathology datasets(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability acrossruns, we identified clear trends: RandomResizedCrop values of 0.7-0.8outperformed more aggressive (0.6) or conservative (0.9) settings, distributedtraining without local loss improved stability, and learning rates below 5.0e-5consistently degraded performance across all datasets. The LC25000 (Colon)dataset consistently provided the most reproducible benchmark. These findingshighlight that reproducibility in computational pathology depends not only ontransparent documentation but also on carefully chosen experimentalconfigurations, and we provide practical rules to guide future efforts indeveloping reproducible foundation models for digital pathology.</description>
      <author>example@mail.com (Usman Afzaal, Ziyu Su, Usama Sajjad, Hao Lu, Mostafa Rezapour, Metin Nafi Gurcan, Muhammad Khalid Khan Niazi)</author>
      <guid isPermaLink="false">2510.15164v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning</title>
      <link>http://arxiv.org/abs/2510.15026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MOBIUS是一个高效的基础模型家族，专为通用实例分割设计，能够在保持高性能的同时大幅减少计算需求，实现了从高端加速器到移动硬件的跨设备部署。&lt;h4&gt;背景&lt;/h4&gt;扩大模型规模和训练数据推动了基础模型在实例级感知方面的发展，在目标检测和分割任务中取得了最先进的性能，但高计算成本限制了它们在资源受限平台上的应用。&lt;h4&gt;目的&lt;/h4&gt;研究现有架构在实现高效边缘部署方面的局限性，同时不牺牲性能；引入MOBIUS基础模型家族，实现帕累托最优的缩放，支持跨设备部署。&lt;h4&gt;方法&lt;/h4&gt;提出瓶颈像素解码器用于高效多尺度多模态融合；提出语言引导的不确定性校准损失用于自适应解码器剪枝；提出简化的统一训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;MOBIUS将像素和transformer解码器的FLOPs分别减少了高达55%和75%，同时仅用三分之一训练迭代次数就保持了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;MOBIUS为高性能计算平台和移动设备上的高效分割建立了新的基准。&lt;h4&gt;翻译&lt;/h4&gt;扩大模型规模和训练数据推动了基础模型在实例级感知方面的发展，在目标检测和分割任务中实现了最先进的领域内和零样本性能。然而，它们的高计算成本限制了在资源受限平台上的应用。我们首先研究了现有架构在实现高效边缘部署而不牺牲性能方面的局限性。然后我们引入了MOBIUS，一个用于通用实例分割的基础模型家族，设计为帕累托最优的缩放，支持从高端加速器到移动硬件的跨设备部署。为了减少训练和推理需求，我们提出了：(i)用于高效多尺度多模态融合的瓶颈像素解码器，(ii)用于自适应解码器剪枝的语言引导不确定性校准损失，以及(iii)简化的统一训练策略。与权衡准确性和减少复杂性的高效基线不同，MOBIUS将像素和transformer解码器的FLOPs分别减少了高达55%和75%，同时在仅三分之一的训练迭代次数内保持了最先进的性能。MOBIUS为高性能计算平台和移动设备上的高效分割建立了新基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling up model size and training data has advanced foundation models forinstance-level perception, achieving state-of-the-art in-domain and zero-shotperformance across object detection and segmentation. However, their highcomputational cost limits adoption on resource-constrained platforms. We firstexamine the limitations of existing architectures in enabling efficient edgedeployment without compromising performance. We then introduce MOBIUS, a familyof foundation models for universal instance segmentation, designed forPareto-optimal downscaling to support deployment across devices ranging fromhigh-end accelerators to mobile hardware. To reduce training and inferencedemands, we propose: (i) a bottleneck pixel decoder for efficient multi-scaleand multi-modal fusion, (ii) a language-guided uncertainty calibration loss foradaptive decoder pruning, and (iii) a streamlined, unified training strategy.Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUSreduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,while maintaining state-of-the-art performance in just a third of the trainingiterations. MOBIUS establishes a new benchmark for efficient segmentation onboth high-performance computing platforms and mobile devices.</description>
      <author>example@mail.com (Mattia Segu, Marta Tintore Gazulla, Yongqin Xian, Luc Van Gool, Federico Tombari)</author>
      <guid isPermaLink="false">2510.15026v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Centric Activation and Coordination for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2510.14349v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VaCo是一种优化多模态大语言模型(MLLMs)表示的新方法，通过整合多个视觉基础模型(VFMs)的视觉中心激活和协调，显著提高了MLLMs在视觉理解方面的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)通过整合视觉编码器的图像特征与LLMs展示先进理解能力，但主流MLLMs仅受文本token的下一词预测监督，忽略了分析能力所需的关键视觉中心信息。&lt;h4&gt;目的&lt;/h4&gt;解决主流MLLMs忽略视觉中心信息的问题，提高MLLMs的视觉理解能力和分析能力。&lt;h4&gt;方法&lt;/h4&gt;引入VaCo方法，包括：1)视觉判别对齐整合VFMs提取的任务感知特征；2)在MLLMs中融入可学习的模块化任务查询(MTQs)和视觉对齐层(VALs)；3)设计令牌网关掩码(TGM)协调多个VFMs之间的表示冲突。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，VaCo在各种基准测试中显著提高了不同MLLMs的性能，展示了其在视觉理解方面的优越能力。&lt;h4&gt;结论&lt;/h4&gt;VaCo通过整合多个VFMs的视觉特征，有效解决了主流MLLMs忽略视觉中心信息的问题，提高了MLLMs的综合性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)整合视觉编码器中的图像特征与LLMs，展示了先进的理解能力。然而，主流MLLMs仅受文本token的下一词预测监督，忽略了分析能力所需的关键视觉中心信息。为解决这一困境，我们引入了VaCo，它通过多个视觉基础模型(VFMs)的视觉中心激活和协调来优化MLLM表示。VaCo引入视觉判别对齐来整合从VFMs提取的任务感知特征，从而统一MLLM中文本和视觉输出的优化。具体而言，我们将可学习的模块化任务查询(MTQs)和视觉对齐层(VALs)整合到MLLMs中，在多样化VFMs的监督下激活特定的视觉信号。为协调VFMs之间的表示冲突，精心设计的令牌网关掩码(TGM)限制了多组MTQs之间的信息流动。大量实验表明，VaCo在各种基准测试中显著提高了不同MLLMs的性能，展示了其在视觉理解方面的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) integrate image features from visualencoders with LLMs, demonstrating advanced comprehension capabilities. However,mainstream MLLMs are solely supervised by the next-token prediction of textualtokens, neglecting critical vision-centric information essential for analyticalabilities. To track this dilemma, we introduce VaCo, which optimizes MLLMrepresentations through Vision-Centric activation and Coordination frommultiple vision foundation models (VFMs). VaCo introduces visual discriminativealignment to integrate task-aware perceptual features extracted from VFMs,thereby unifying the optimization of both textual and visual outputs in MLLMs.Specifically, we incorporate the learnable Modular Task Queries (MTQs) andVisual Alignment Layers (VALs) into MLLMs, activating specific visual signalsunder the supervision of diverse VFMs. To coordinate representation conflictsacross VFMs, the crafted Token Gateway Mask (TGM) restricts the informationflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCosignificantly improves the performance of different MLLMs on variousbenchmarks, showcasing its superior capabilities in visual comprehension.</description>
      <author>example@mail.com (Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin)</author>
      <guid isPermaLink="false">2510.14349v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</title>
      <link>http://arxiv.org/abs/2510.15018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical report. Project page: https://urbanverseproject.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanVerse是一个数据驱动的真实到仿真系统，将众包城市旅游视频转换为具有物理感知能力的交互式仿真场景，包含10万多个带注释的城市3D资产库和自动场景提取管道，显著提高了城市AI代理训练的效果和导航策略的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;城市中的实体AI代理（如配送机器人、四足机器人等）日益增多，它们需要在混乱的城市街道中导航以提供最后一公里连接。训练这类代理需要多样化的、高保真的城市环境，但现有的人工制作或程序生成的仿真场景要么缺乏可扩展性，要么无法捕捉真实世界的复杂性。&lt;h4&gt;目的&lt;/h4&gt;引入UrbanVerse，一个数据驱动的真实到仿真系统，将众包的城市旅游视频转换为具有物理感知能力的交互式仿真场景。&lt;h4&gt;方法&lt;/h4&gt;UrbanVerse包括两个部分：UrbanVerse-100K（包含10万多个带注释的城市3D资产库，具有语义和物理属性）和UrbanVerse-Gen（一个自动管道，从视频中提取场景布局，并使用检索到的资产创建度量尺度的3D仿真）。在IsaacSim中运行，提供来自24个国家的160个高质量构建场景和10个艺术家设计的测试场景的精选基准。&lt;h4&gt;主要发现&lt;/h4&gt;UrbanVerse场景保留了真实世界的语义和布局，实现了与人工制作场景相当的人评估真实感。在城市导航中，在UrbanVerse中训练的策略显示出扩展幂律和强大的泛化能力，与先前的方法相比，在仿真中成功率提高了6.3%，在零样本仿真到真实迁移中提高了30.1%，仅用两次干预就完成了300米的真实世界任务。&lt;h4&gt;结论&lt;/h4&gt;UrbanVerse系统能够有效地将真实世界的城市环境转化为高质量的仿真场景，为训练城市AI代理提供了有效工具，并显著提高了导航策略的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;城市实体AI代理，从配送机器人到四足机器人，正日益增多地遍布我们的城市，在混乱的街道上导航以提供最后一公里的连接。训练此类代理需要多样化、高保真的城市环境来扩展规模，然而现有的人工制作或程序生成的仿真场景要么缺乏可扩展性，要么无法捕捉真实世界的复杂性。我们引入了UrbanVerse，一个数据驱动的真实到仿真系统，将众包的城市旅游视频转换为具有物理感知能力的交互式仿真场景。UrbanVerse包括：(i) UrbanVerse-100K，一个包含10万多个带注释的城市3D资产库，具有语义和物理属性，以及(ii) UrbanVerse-Gen，一个自动管道，从视频中提取场景布局，并使用检索到的资产创建度量尺度的3D仿真。在IsaacSim中运行，UrbanVerse提供了来自24个国家的160个高质量构建场景，以及10个艺术家设计的测试场景的精选基准。实验表明，UrbanVerse场景保留了真实世界的语义和布局，实现了与人工制作场景相当的人评估真实感。在城市导航中，在UrbanVerse中训练的策略显示出扩展幂律和强大的泛化能力，与先前的方法相比，在仿真中成功率提高了6.3%，在零样本仿真到真实迁移中提高了30.1%，仅用两次干预就完成了300米的真实世界任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决城市环境中具身AI体（如配送机器人、四足机器人等）训练所需的高保真、多样化城市环境不足的问题。这个问题很重要，因为随着城市中微型移动系统的兴起，这些AI体需要能够泛化到各种复杂的真实世界环境中，但现有的模拟场景要么缺乏可扩展性，要么无法捕捉真实世界的复杂性和动态变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过认识到需要两个关键元素来构建解决方案：大规模3D资产数据库和自动化场景生成管道。他们借鉴了现有工作如Objaverse等3D资产库，但解决了其中资产质量、相关性和标注不足的问题；同时结合了MASt3R、YoloWorld、SAM2等多种技术来构建UrbanVerse-Gen管道，实现从未校准视频中提取场景信息。还参考了数字孪生概念，但扩展到城市街道级别场景生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个数据驱动的真实到模拟系统，将真实世界城市旅游视频转换为具有物理感知的交互式模拟场景，保留真实世界的语义、布局和物理特性。整体流程分为两大部分：1) UrbanVerse-100K资产数据库：收集和标注102,530个高质量3D城市对象，每个带有33种属性；2) UrbanVerse-Gen管道：从视频中提取场景信息，检索匹配资产，并在IsaacSim中生成可交互的数字孪生场景，包括场景蒸馏、资产匹配和场景组装三个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) UrbanVerse-100K大规模高质量标注资产库；2) UrbanVerse-Gen从未校准视频中自动生成高保真城市场景的管道；3) 跨24个国家的160场景训练库。相比之前工作，不同之处在于：解决了现有模拟器场景真实性不足、资产多样性有限和物理标注缺乏的问题；超越了仅使用被动数据训练的方法，通过交互式模拟实现更好的障碍物避免；扩展了数字孪生概念到城市街道级别，而非仅限于室内环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanVerse通过将众源城市旅游视频转换为具有物理感知的交互式模拟场景，解决了城市环境中具身AI体训练所需的高保真、可扩展城市环境问题，实现了真实世界分布的保真度并显著提升了策略在模拟到真实世界零样本迁移中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban embodied AI agents, ranging from delivery robots to quadrupeds, areincreasingly populating our cities, navigating chaotic streets to providelast-mile connectivity. Training such agents requires diverse, high-fidelityurban environments to scale, yet existing human-crafted or procedurallygenerated simulation scenes either lack scalability or fail to capturereal-world complexity. We introduce UrbanVerse, a data-driven real-to-simsystem that converts crowd-sourced city-tour videos into physics-aware,interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, arepository of 100k+ annotated urban 3D assets with semantic and physicalattributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scenelayouts from video and instantiates metric-scale 3D simulations using retrievedassets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructedscenes from 24 countries, along with a curated benchmark of 10 artist-designedtest scenes. Experiments show that UrbanVerse scenes preserve real-worldsemantics and layouts, achieving human-evaluated realism comparable to manuallycrafted scenes. In urban navigation, policies trained in UrbanVerse exhibitscaling power laws and strong generalization, improving success by +6.3% insimulation and +30.1% in zero-shot sim-to-real transfer comparing to priormethods, accomplishing a 300 m real-world mission with only two interventions.</description>
      <author>example@mail.com (Mingxuan Liu, Honglin He, Elisa Ricci, Wayne Wu, Bolei Zhou)</author>
      <guid isPermaLink="false">2510.15018v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.15430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Learning to Detect (LoD)框架，用于有效检测大型视觉语言模型中的未知越狱攻击，解决了现有检测方法的泛化性和效率问题。&lt;h4&gt;背景&lt;/h4&gt;尽管进行了广泛的对齐努力，大型视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来严重的安全风险。&lt;h4&gt;目的&lt;/h4&gt;克服现有检测方法的局限性，提出一种能够准确检测未知越狱攻击的通用框架。&lt;h4&gt;方法&lt;/h4&gt;提出Learning to Detect (LoD)框架，将重点从特定攻击学习转向特定任务学习。该框架包括多模态安全概念激活向量模块用于安全导向的表征学习和安全模式自动编码器模块用于无监督攻击分类。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在各种未知攻击上实现了持续更高的检测AUROC，同时提高了效率。&lt;h4&gt;结论&lt;/h4&gt;LoD框架通过转变学习方式，有效解决了现有检测方法在泛化性和效率方面的局限性，能够更准确、高效地检测未知攻击。&lt;h4&gt;翻译&lt;/h4&gt;尽管进行了广泛的对齐努力，大型视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来严重的安全风险。为解决这个问题，现有的检测方法要么学习特定攻击的参数，这阻碍了它们对未见攻击的泛化能力；要么依赖于启发式原则，这限制了准确性和效率。为克服这些局限性，我们提出了Learning to Detect (LoD)框架，一种通过将重点从特定攻击学习转向特定任务学习来准确检测未知越狱攻击的通用框架。该框架包括用于安全导向表征学习的多模态安全概念激活向量模块和用于无监督攻击分类的安全模式自动编码器模块。大量实验表明，我们的方法在各种未知攻击上实现了持续更高的检测AUROC，同时提高了效率。代码可在https://anonymous.4open.science/r/Learning-to-Detect-51CB获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)remain vulnerable to jailbreak attacks, posing serious safety risks. To addressthis, existing detection methods either learn attack-specific parameters, whichhinders generalization to unseen attacks, or rely on heuristically soundprinciples, which limit accuracy and efficiency. To overcome these limitations,we propose Learning to Detect (LoD), a general framework that accuratelydetects unknown jailbreak attacks by shifting the focus from attack-specificlearning to task-specific learning. This framework includes a Multi-modalSafety Concept Activation Vector module for safety-oriented representationlearning and a Safety Pattern Auto-Encoder module for unsupervised attackclassification. Extensive experiments show that our method achievesconsistently higher detection AUROC on diverse unknown attacks while improvingefficiency. The code is available athttps://anonymous.4open.science/r/Learning-to-Detect-51CB.</description>
      <author>example@mail.com (Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, Xiting Wang)</author>
      <guid isPermaLink="false">2510.15430v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale User Game Lifecycle Representation Learning</title>
      <link>http://arxiv.org/abs/2510.15412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用户游戏生命周期(UGL)方法来解决游戏推荐系统中的稀疏性和不平衡性问题，通过创新策略显著提升了游戏广告和推荐的性能。&lt;h4&gt;背景&lt;/h4&gt;视频游戏产业快速发展，需要在线游戏平台开发有效的广告和推荐系统。现有的推荐系统表示学习方法不适合游戏场景，主要面临游戏稀疏性和不平衡性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决游戏推荐系统中的游戏稀疏性和不平衡性问题，提升游戏广告和推荐的性能。&lt;h4&gt;方法&lt;/h4&gt;引入用户游戏生命周期(UGL)丰富用户行为；提出两种创新策略操纵用户行为以提取短期和长期兴趣；采用逆概率掩码策略处理游戏不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;UGL表示显著提升了模型性能，离线实验显示游戏广告AUC平均增加1.83%，游戏内物品推荐AUC增加0.5%；在线实验显示游戏广告CVR平均增加21.67%，游戏内物品推荐ARPU增加0.82%。&lt;h4&gt;结论&lt;/h4&gt;UGL表示方法能有效解决游戏稀疏性和不平衡性问题，显著提升游戏广告和推荐的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;视频游戏生产的快速发展需要为在线游戏平台开发有效的广告和推荐系统。向用户推荐和宣传游戏取决于捕捉他们对游戏的兴趣。然而，为处理推荐系统中的数十亿物品而设计的现有表示学习方法不适合游戏广告和推荐。这主要是由于游戏稀疏性，其中仅有的几百个游戏不足以进行大规模用户表示学习，以及游戏不平衡性，其中用户行为被少数热门游戏主导。为解决稀疏性问题，我们引入了用户游戏生命周期(UGL)，旨在丰富用户在游戏中的行为。此外，我们提出了两种创新策略，旨在操纵用户行为以更有效地提取短期和长期兴趣。为解决游戏不平衡挑战，我们提出了用于UGL表示学习的逆概率掩码策略。离线和在线实验结果表明，UGL表示通过在游戏广告中平均实现1.83%的离线AUC增长和21.67%的在线CVR增长，以及在游戏内物品推荐中平均实现0.5%的离线AUC增长和0.82%的在线ARPU增长，显著增强了模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid expansion of video game production necessitates the development ofeffective advertising and recommendation systems for online game platforms.Recommending and advertising games to users hinges on capturing their interestin games. However, existing representation learning methods crafted forhandling billions of items in recommendation systems are unsuitable for gameadvertising and recommendation. This is primarily due to game sparsity, wherethe mere hundreds of games fall short for large-scale user representationlearning, and game imbalance, where user behaviors are overwhelmingly dominatedby a handful of popular games. To address the sparsity issue, we introduce theUser Game Lifecycle (UGL), designed to enrich user behaviors in games.Additionally, we propose two innovative strategies aimed at manipulating userbehaviors to more effectively extract both short and long-term interests. Totackle the game imbalance challenge, we present an Inverse Probability Maskingstrategy for UGL representation learning. The offline and online experimentalresults demonstrate that the UGL representations significantly enhance model byachieving a 1.83% AUC offline increase on average and a 21.67% CVR onlineincrease on average for game advertising and a 0.5% AUC offline increase and a0.82% ARPU online increase for in-game item recommendation.</description>
      <author>example@mail.com (Yanjie Gou, Jiangming Liu, Kouying Xue, Yi Hua)</author>
      <guid isPermaLink="false">2510.15412v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Zero-Shot Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.15382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Neurips 2025, 36 pages, 18 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BREEZE的新型零样本强化学习框架，该框架基于前向-后向表示(FB)方法进行升级，通过引入行为正则化、任务条件扩散模型和基于注意力的表达架构，解决了现有方法中建模表达能力不足和离分布动作导致的外推误差问题，显著提升了学习稳定性、策略提取能力和表示学习质量。&lt;h4&gt;背景&lt;/h4&gt;零样本强化学习的最新发展为学习预训练通用策略开辟了新途径，这些策略可以以零样本方式适应任意新任务。尽管流行的前向-后向表示(FB)及相关方法在零样本RL中显示出潜力，但它们的建模存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本RL方法中建模表达能力不足、离分布(OOD)动作在离线学习期间引起的外推误差导致有偏表示、最终造成次优性能的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了BREEZE(具有表达能力增强的行为正则化零样本RL)，这是一个升级的基于FB的框架，主要包括：1)在零样本RL策略学习中引入行为正则化，将策略优化转化为稳定的样本内学习范式；2)使用任务条件扩散模型提取策略，生成高质量和多模态的动作分布；3)采用基于注意力的表达架构进行表示建模，以捕捉环境动力学之间的复杂关系。&lt;h4&gt;主要发现&lt;/h4&gt;在ExORL和D4RL Kitchen上的大量实验表明，BREEZE实现了最佳或接近最佳的性能，并且比先前的离线零样本RL方法表现出更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;BREEZE通过结合行为正则化、任务条件扩散模型和基于注意力的表达架构，有效解决了现有零样本RL方法的局限性，显著提升了学习稳定性、策略提取能力和表示学习质量，为零样本强化学习领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近零样本强化学习(RL)的发展为学习预训练通用策略开辟了新途径，这些策略可以以零样本方式适应任意新任务。虽然流行的前向-后向表示(FB)及相关方法在零样本RL中显示出潜力，但我们通过实验发现它们的建模缺乏表达能力，并且离分布(OOD)动作在离线学习期间引起的外推误差有时会导致有偏表示，最终导致次优性能。为解决这些问题，我们提出了BREEZE(具有表达能力增强的行为正则化零样本RL)，这是一个升级的基于FB的框架，同时增强学习稳定性、策略提取能力和表示学习质量。BREEZE在零样本RL策略学习中引入行为正则化，将策略优化转化为稳定的样本内学习范式。此外，BREEZE使用任务条件扩散模型提取策略，能够在零样本RL设置中生成高质量和多模态的动作分布。而且，BREEZE采用基于注意力的表达架构进行表示建模，以捕捉环境动力学之间的复杂关系。在ExORL和D4RL Kitchen上的大量实验表明，BREEZE实现了最佳或接近最佳的性能，并且比先前的离线零样本RL方法表现出更强的鲁棒性。官方实现可在https://github.com/Whiterrrrr/BREEZE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent development of zero-shot reinforcement learning (RL) has opened anew avenue for learning pre-trained generalist policies that can adapt toarbitrary new tasks in a zero-shot manner. While the popular Forward-Backwardrepresentations (FB) and related methods have shown promise in zero-shot RL, weempirically found that their modeling lacks expressivity and that extrapolationerrors caused by out-of-distribution (OOD) actions during offline learningsometimes lead to biased representations, ultimately resulting in suboptimalperformance. To address these issues, we propose Behavior-REgularizEd Zero-shotRL with Expressivity enhancement (BREEZE), an upgraded FB-based framework thatsimultaneously enhances learning stability, policy extraction capability, andrepresentation learning quality. BREEZE introduces behavioral regularization inzero-shot RL policy learning, transforming policy optimization into a stablein-sample learning paradigm. Additionally, BREEZE extracts the policy using atask-conditioned diffusion model, enabling the generation of high-quality andmultimodal action distributions in zero-shot RL settings. Moreover, BREEZEemploys expressive attention-based architectures for representation modeling tocapture the complex relationships between environmental dynamics. Extensiveexperiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the bestor near-the-best performance while exhibiting superior robustness compared toprior offline zero-shot RL methods. The official implementation is availableat: https://github.com/Whiterrrrr/BREEZE.</description>
      <author>example@mail.com (Kexin Zheng, Lauriane Teyssier, Yinan Zheng, Yu Luo, Xiayuan Zhan)</author>
      <guid isPermaLink="false">2510.15382v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>DCMIL: A Progressive Representation Learning of Whole Slide Images for Cancer Prognosis Analysis</title>
      <link>http://arxiv.org/abs/2510.14403v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DCMIL（双课程对比多实例学习）的简单到难渐进式表示学习方法，用于高效处理全切片图像（WSIs）进行癌症预后预测，无需密集标注，可直接将千兆像素级WSIs转化为结果预测。&lt;h4&gt;背景&lt;/h4&gt;计算病理学是一个新兴学科，旨在利用全切片图像量化形态异质性并开发癌症客观预后模型，但受千兆像素级输入的计算瓶颈和密集手动标注稀缺性的阻碍，当前方法常忽略多倍率WSIs上的细粒度信息和肿瘤微环境变异。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效处理WSIs的癌症预后预测方法，解决现有方法的局限性，特别是计算瓶颈和标注稀缺问题。&lt;h4&gt;方法&lt;/h4&gt;提出DCMIL（dual-curriculum contrastive multi-instance learning）方法，这是一种简单到难渐进式表示学习技术，不依赖密集标注，可直接处理千兆像素级WSIs。&lt;h4&gt;主要发现&lt;/h4&gt;在十二种癌症类型（5,954名患者，1,254万张图像块）上的实验显示，DCMIL优于标准WSI预后模型，能识别预后显著区域，提供不确定性估计，捕捉正常与肿瘤组织形态差异，并可能产生新生物学见解。&lt;h4&gt;结论&lt;/h4&gt;DCMIL方法有效解决了计算病理学中的关键挑战，为癌症预后预测提供了强大工具，所有代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;蓬勃发展的计算病理学学科展现出利用全切片图像（WSIs）量化形态异异性并为人类癌症开发客观预后模型的潜力。然而，千兆像素级输入的计算瓶颈和密集手动标注的稀缺性阻碍了其进展。当前方法常常忽略多倍率WSIs上的细粒度信息和肿瘤微环境的变异。在此，我们提出了一种简单到难渐进式表示学习，称为双课程对比多实例学习（DCMIL），以高效处理WSIs用于癌症预后。该模型不依赖密集标注，能够直接将千兆像素级WSIs转化为结果预测。在十二种癌症类型（5,954名患者，1,254万张图像块）上的大量实验表明，DCMIL优于标准的基于WSI的预后模型。此外，DCMIL能够识别细粒度的预后显著区域，提供稳健的实例不确定性估计，并捕捉正常组织和肿瘤组织之间的形态差异，有可能产生新的生物学见解。所有代码已在https://github.com/tuuuc/DCMIL公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The burgeoning discipline of computational pathology shows promise inharnessing whole slide images (WSIs) to quantify morphological heterogeneityand develop objective prognostic modes for human cancers. However, progress isimpeded by the computational bottleneck of gigapixel-size inputs and thescarcity of dense manual annotations. Current methods often overlookfine-grained information across multi-magnification WSIs and variations intumor microenvironments. Here, we propose an easy-to-hard progressiverepresentation learning, termed dual-curriculum contrastive multi-instancelearning (DCMIL), to efficiently process WSIs for cancer prognosis. The modeldoes not rely on dense annotations and enables the direct transformation ofgigapixel-size WSIs into outcome predictions. Extensive experiments on twelvecancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMILoutperforms standard WSI-based prognostic models. Additionally, DCMILidentifies fine-grained prognosis-salient regions, provides robust instanceuncertainty estimation, and captures morphological differences between normaland tumor tissues, with the potential to generate new biological insights. Allcodes have been made publicly accessible at https://github.com/tuuuc/DCMIL.</description>
      <author>example@mail.com (Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning)</author>
      <guid isPermaLink="false">2510.14403v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations</title>
      <link>http://arxiv.org/abs/2510.14049v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了一个新的因果表示学习(CRL)基准，使用高保真模拟视觉数据，包含约20万张图像和300万视频帧，涵盖四个领域的24个子场景，提供对底层因果结构的灵活访问，评估了不同范式的代表性CRL方法，并提供了经验见解。&lt;h4&gt;背景&lt;/h4&gt;因果表示学习旨在揭示数据生成过程并识别潜在的因果变量和关系，但其评估具有固有挑战性，因为需要已知的真实因果变量和因果结构。现有评估通常依赖简单的合成数据集或真实世界任务的下游性能，面临真实性和评估精度之间的两难困境。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的CRL基准，使用高保真模拟视觉数据，既保持真实的视觉复杂性，又能访问真实的因果生成过程，提供全面的测试平台以弥合严格评估和实际应用之间的差距。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含约20万张图像和300万视频帧的数据集，涵盖四个领域(静态图像生成、动态物理模拟、机器人操作和交通情况分析)的24个子场景，从静态到动态设置，从简单到复杂结构，从单智能体到多智能体交互。提供对底层因果结构的灵活访问，允许用户修改或配置它们以符合CRL中的假设要求。&lt;h4&gt;主要发现&lt;/h4&gt;该基准提供了一个全面的测试平台，有望弥合严格评估和实际应用之间的差距；评估了不同范式的代表性CRL方法；提供了经验见解，帮助实践者和新手选择或扩展适当的CRL框架以解决特定类型的现实问题。&lt;h4&gt;结论&lt;/h4&gt;该基准有助于解决特定类型的现实问题，这些问题可以从CRL视角中受益；提供了项目页面和数据集的访问链接。&lt;h4&gt;翻译&lt;/h4&gt;因果表示学习(CRL)旨在揭示数据生成过程并识别潜在的因果变量和关系，其评估由于需要已知的真实因果变量和因果结构而仍然具有固有挑战性。现有评估通常要么依赖简单的合成数据集，要么依赖真实世界任务的下游性能，普遍面临真实性和评估精度之间的两难困境。在本文中，我们引入了一个使用高保真模拟视觉数据的新CRL基准，这些数据既保持了真实的视觉复杂性，更重要的是，可以访问真实的因果生成过程。该数据集包含四个领域(静态图像生成、动态物理模拟、机器人操作和交通情况分析)中24个子场景的约20万张图像和300万视频帧。这些场景从静态到动态设置，从简单到复杂结构，从单智能体到多智能体交互，提供了一个全面的测试平台，有望弥合严格评估和实际应用之间的差距。此外，我们提供对底层因果结构的灵活访问，允许用户修改或配置它们以符合CRL中的假设要求，如可用的领域标签、时间依赖性或干预历史。利用这个基准，我们评估了不同范式的代表性CRL方法，并提供了经验见解，帮助实践者和新手选择或扩展适当的CRL框架，以适当解决可以从CRL视角中受益的特定类型的现实问题。欢迎访问我们的：项目页面：https://causal-verse.github.io/，数据集：https://huggingface.co/CausalVerse。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal Representation Learning (CRL) aims to uncover the data-generatingprocess and identify the underlying causal variables and relations, whoseevaluation remains inherently challenging due to the requirement of knownground-truth causal variables and causal structure. Existing evaluations oftenrely on either simplistic synthetic datasets or downstream performance onreal-world tasks, generally suffering a dilemma between realism and evaluativeprecision. In this paper, we introduce a new benchmark for CRL usinghigh-fidelity simulated visual data that retains both realistic visualcomplexity and, more importantly, access to ground-truth causal generatingprocesses. The dataset comprises around 200 thousand images and 3 million videoframes across 24 sub-scenes in four domains: static image generation, dynamicphysical simulations, robotic manipulations, and traffic situation analysis.These scenarios range from static to dynamic settings, simple to complexstructures, and single to multi-agent interactions, offering a comprehensivetestbed that hopefully bridges the gap between rigorous evaluation andreal-world applicability. In addition, we provide flexible access to theunderlying causal structures, allowing users to modify or configure them toalign with the required assumptions in CRL, such as available domain labels,temporal dependencies, or intervention histories. Leveraging this benchmark, weevaluated representative CRL methods across diverse paradigms and offeredempirical insights to assist practitioners and newcomers in choosing orextending appropriate CRL frameworks to properly address specific types of realproblems that can benefit from the CRL perspective. Welcome to visit our:Project page:https://causal-verse.github.io/,Dataset:https://huggingface.co/CausalVerse.</description>
      <author>example@mail.com (Guangyi Chen, Yunlong Deng, Peiyuan Zhu, Yan Li, Yifan Shen, Zijian Li, Kun Zhang)</author>
      <guid isPermaLink="false">2510.14049v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Attn-JGNN: Attention Enhanced Join-Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.15583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于解决#SAT问题的注意力增强连接图神经网络(Attn-JGNN)模型，显著提高了求解准确性。&lt;h4&gt;背景&lt;/h4&gt;#SAT问题是计算机科学中的重要问题，涉及计算布尔公式满足元的数量，现有神经网络方法可能存在求解精度不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提高解决#SAT问题的准确性，通过引入注意力机制来优化连接图神经网络模型。&lt;h4&gt;方法&lt;/h4&gt;受迭代连接图传播算法启发，使用树分解将CNF公式编码为连接图，在连接图上进行迭代消息传递，通过学习分区函数近似模型数量，并在连接图的簇内和簇间应用注意力机制，使模型更关注关键变量和簇，减少冗余计算。&lt;h4&gt;主要发现&lt;/h4&gt;注意力机制使Attn-JGNN能够在概率推理中更关注关键变量和簇，减少冗余计算，实验表明该模型比其他神经网络方法取得了更好的结果。&lt;h4&gt;结论&lt;/h4&gt;Attn-JGNN模型通过结合注意力机制和连接图神经网络，有效提高了#SAT问题的求解准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于解决#SAT问题的注意力增强连接图神经网络(Attn-JGNN)模型，显著提高了求解准确性。受迭代连接图传播算法启发，Attn-JGNN使用树分解将CNF公式编码为连接图，然后在连接图上进行迭代消息传递，最后通过学习分区函数来近似模型数量。为了进一步提高求解准确性，我们在连接图的簇内和簇间应用注意力机制，这使得Attn-JGNN能够在概率推理中更关注关键变量和簇，并减少冗余计算。最后，我们的实验表明，Attn-JGNN模型比其他神经网络方法取得了更好的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an Attention Enhanced Join-Graph Neural Networks(Attn-JGNN) modelfor solving #SAT problems, which significantly improves the solving accuracy.Inspired by the Iterative Join Graph Propagation (IJGP) algorithm, Attn-JGNNuses tree decomposition to encode the CNF formula into a join-graph, thenperforms iterative message passing on the join-graph, and finally approximatesthe model number by learning partition functions. In order to further improvethe accuracy of the solution, we apply the attention mechanism in and betweenclusters of the join-graphs, which makes Attn-JGNN pay more attention to thekey variables and clusters in probabilistic inference, and reduces theredundant calculation. Finally, our experiments show that our Attn-JGNN modelachieves better results than other neural network methods.</description>
      <author>example@mail.com (Jixin Zhang, Yong Lai)</author>
      <guid isPermaLink="false">2510.15583v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs</title>
      <link>http://arxiv.org/abs/2510.15428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种过程感知框架，结合制造领域概念化与图神经网络推理，提高FMEA知识在不同制造生产线间的可重用性，有效解决了故障原因识别的挑战。&lt;h4&gt;背景&lt;/h4&gt;自动化制造线中的故障原因识别具有挑战性，主要由于系统复杂性、频繁重新配置以及现有FMEA知识的有限可重用性。FMEA工作表包含宝贵的专家见解，但由于自然语言变异性、术语不一致和工艺差异，在异构生产线之间的重用受到阻碍。&lt;h4&gt;目的&lt;/h4&gt;解决FMEA知识在不同生产线间重用的限制，提高故障原因识别的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出一个过程感知框架，首先通过本体引导的大语言模型提取，将多个生产线的FMEA工作表转换为统一的知识图谱；其次，使用带有过程感知评分函数的关系图卷积网络学习尊重语义关系和顺序流程的嵌入；最后，使用链接预测推断和排序与目标生产线流程一致的候选故障原因。&lt;h4&gt;主要发现&lt;/h4&gt;在汽车压力传感器装配线上的案例研究表明，所提出的方法优于最先进的检索增强生成基线（F1@20 = 0.267）和RGCN方法（0.400），实现了故障原因识别的最佳性能（0.523）。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架显著提高了FMEA知识在异构生产线之间的可转移性，支持操作人员更可靠地诊断故障，为未来智能制造中的领域自适应LLM应用铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;在自动化生产线中，由于系统复杂性、频繁重新配置以及现有故障模式与影响分析知识的有限可重用性，故障原因识别具有挑战性。尽管FMEA工作表包含宝贵的专家见解，但由于自然语言变异性、术语不一致和工艺差异，它们在异构生产线之间的重用受到阻碍。为解决这些限制，本研究提出了一种过程感知框架，通过结合制造领域概念化与图神经网络推理，提高FMEA的可重用性。首先，通过本体引导的大语言模型提取，将多个生产线的FMEA工作表转换为统一的知识图谱，捕获领域概念如动作、状态、组件和参数。其次，使用带有过程感知评分函数的关系图卷积网络学习尊重语义关系和顺序流程的嵌入。最后，使用链接预测来推断和排序与目标生产线流程一致的候选故障原因。在汽车压力传感器装配线上的案例研究表明，所提出的方法优于最先进的检索增强生成基线（F1@20 = 0.267）和RGCN方法（0.400），实现了故障原因识别的最佳性能（0.523）。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。这些结果表明，所提出的框架显著提高了FMEA知识在异构生产线之间的可转移性，从而支持操作人员更可靠地诊断故障，并为未来智能制造中领域自适应LLM的应用铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fault cause identification in automated manufacturing lines is challengingdue to the system's complexity, frequent reconfigurations, and the limitedreusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.Although FMEA worksheets contain valuable expert insights, their reuse acrossheterogeneous lines is hindered by natural language variability, inconsistentterminology, and process differences. To address these limitations, this studyproposes a process-aware framework that enhances FMEA reusability by combiningmanufacturing-domain conceptualization with graph neural network (GNN)reasoning. First, FMEA worksheets from multiple manufacturing lines aretransformed into a unified knowledge graph through ontology-guided largelanguage model (LLM) extraction, capturing domain concepts such as actions,states, components, and parameters. Second, a Relational Graph ConvolutionalNetwork (RGCN) with the process-aware scoring function learns embeddings thatrespect both semantic relationships and sequential process flows. Finally, linkprediction is employed to infer and rank candidate fault causes consistent withthe target line's process flow.  A case study on automotive pressure sensor assembly lines demonstrates thatthe proposed method outperforms a state-of-the-art retrieval-augmentedgeneration (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),achieving the best performance (0.523) in fault cause identification. Ablationstudies confirm the contributions of both LLM-driven domain conceptualizationand process-aware learning. These results indicate that the proposed frameworksignificantly improves the transferability of FMEA knowledge acrossheterogeneous lines, thereby supporting operators in diagnosing failures morereliably and paving the way for future domain-adaptive LLM applications insmart manufacturing.</description>
      <author>example@mail.com (Sho Okazaki, Kohei Kaminishi, Takuma Fujiu, Yusheng Wang, Jun Ota)</author>
      <guid isPermaLink="false">2510.15428v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Geometric Mixture Models for Electrolyte Conductivity Prediction</title>
      <link>http://arxiv.org/abs/2510.15403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了GeoMix框架，用于准确预测电解质系统中的离子电导率，解决了缺乏高质量基准和混合系统几何建模不足的挑战。&lt;h4&gt;背景&lt;/h4&gt;电解质系统中离子电导率的准确预测对科学和技术应用至关重要，但当前研究面临两个基本挑战：(1)缺乏高质量标准化基准，(2)对混合系统中几何结构和分子间相互作用的建模不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有研究的局限性，建立新的电解质研究基准，并提供通用的几何学习框架以推进混合系统建模。&lt;h4&gt;方法&lt;/h4&gt;重新组织和增强CALiSol和DiffMix电解质数据集，加入分子的几何图表示；提出GeoMix框架，保持Set-SE(3)等变性；设计几何交互网络(GIN)作为专门为分子间几何消息传递的等变模块。&lt;h4&gt;主要发现&lt;/h4&gt;GeoMix在两个数据集上都一致优于多种基线方法，证明了跨分子几何相互作用和等变消息传递对准确属性预测的重要性。&lt;h4&gt;结论&lt;/h4&gt;该工作不仅为电解质研究建立了新基准，还提供了通用的几何学习框架，可应用于能源材料、药物开发等领域的混合系统建模。&lt;h4&gt;翻译&lt;/h4&gt;电解质系统中离子电导率的准确预测对推进众多科学和技术应用至关重要。尽管已取得显著进展，但当前研究面临两个基本挑战：(1)缺乏高质量标准化基准，(2)对混合系统中几何结构和分子间相互作用的建模不足。为解决这些局限性，我们首先通过加入分子的几何图表示来重新组织和增强CALiSol和DiffMix电解质数据集。然后，我们提出了GeoMix，一种新型几何感知框架，保留了混合系统的重要但具有挑战性的Set-SE(3)等变性特性。GeoMix的核心是几何交互网络(GIN)，一个专门为分子间几何消息传递设计的等变模块。全面的实验证明，GeoMix在两个数据集上都一致优于多种基线方法(包括MLPs、GNNs和几何GNNs)，验证了跨分子几何相互作用和等变消息传递对准确属性预测的重要性。这项工作不仅为电解质研究建立了新基准，还提供了通用的几何学习框架，推进了能源材料、药物开发等领域中混合系统的建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of ionic conductivity in electrolyte systems is crucialfor advancing numerous scientific and technological applications. Whilesignificant progress has been made, current research faces two fundamentalchallenges: (1) the lack of high-quality standardized benchmarks, and (2)inadequate modeling of geometric structure and intermolecular interactions inmixture systems. To address these limitations, we first reorganize and enhancethe CALiSol and DiffMix electrolyte datasets by incorporating geometric graphrepresentations of molecules. We then propose GeoMix, a novel geometry-awareframework that preserves Set-SE(3) equivariance-an essential but challengingproperty for mixture systems. At the heart of GeoMix lies the GeometricInteraction Network (GIN), an equivariant module specifically designed forintermolecular geometric message passing. Comprehensive experiments demonstratethat GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,and geometric GNNs) across both datasets, validating the importance ofcross-molecular geometric interactions and equivariant message passing foraccurate property prediction. This work not only establishes new benchmarks forelectrolyte research but also provides a general geometric learning frameworkthat advances modeling of mixture systems in energy materials, pharmaceuticaldevelopment, and beyond.</description>
      <author>example@mail.com (Anyi Li, Jiacheng Cen, Songyou Li, Mingze Li, Yang Yu, Wenbing Huang)</author>
      <guid isPermaLink="false">2510.15403v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks</title>
      <link>http://arxiv.org/abs/2510.15333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于专家混合(MoE)架构的统一框架，用于防御图神经网络中的多种对抗性攻击，包括后门攻击、边操纵和节点注入攻击。&lt;h4&gt;背景&lt;/h4&gt;研究表明图神经网络容易受到多种对抗性攻击，包括操纵、节点注入和后门攻击。然而，现有防御方法通常只针对单一类型的攻击，缺乏能够同时防御多种威胁的统一方法。&lt;h4&gt;目的&lt;/h4&gt;设计一个可扩展的统一框架，能够同时防御后门、边操纵和节点注入等多种图对抗攻击。&lt;h4&gt;方法&lt;/h4&gt;利用专家混合(MoE)架构的灵活性，提出基于互信息的逻辑多样性损失，鼓励专家关注不同邻域结构；引入鲁棒性感知的路由器，识别扰动模式并将受扰节点路由到鲁棒专家。&lt;h4&gt;主要发现&lt;/h4&gt;在各种对抗设置下的广泛实验表明，该方法在抵御多种图对抗攻击方面表现出优越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效防御多种图对抗攻击，为图神经网络的安全提供了统一解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大量研究已经强调了图神经网络(GNNs)容易受到对抗性攻击的脆弱性，包括操纵、节点注入以及最近出现的后门攻击威胁。然而，现有的防御方法通常只关注单一类型的攻击，缺乏同时防御多种威胁的统一方法。在本工作中，我们利用专家混合(MoE)架构的灵活性，设计了一个可扩展的统一框架，用于防御后门、边操纵和节点注入攻击。具体来说，我们提出了一种基于互信息的逻辑多样性损失，鼓励各个专家在决策过程中关注不同的邻域结构，从而确保在局部结构受到扰动时，有足够数量的专家不受影响。此外，我们引入了一种鲁棒性感知的路由器，能够识别扰动模式并将受扰动的节点自适应地路由到相应的鲁棒专家。在各种对抗设置下进行的广泛实验表明，我们的方法在抵御多种图对抗攻击方面持续表现出优越的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extensive research has highlighted the vulnerability of graph neural networks(GNNs) to adversarial attacks, including manipulation, node injection, and therecently emerging threat of backdoor attacks. However, existing defensestypically focus on a single type of attack, lacking a unified approach tosimultaneously defend against multiple threats. In this work, we leverage theflexibility of the Mixture of Experts (MoE) architecture to design a scalableand unified framework for defending against backdoor, edge manipulation, andnode injection attacks. Specifically, we propose an MI-based logic diversityloss to encourage individual experts to focus on distinct neighborhoodstructures in their decision processes, thus ensuring a sufficient subset ofexperts remains unaffected under perturbations in local structures. Moreover,we introduce a robustness-aware router that identifies perturbation patternsand adaptively routes perturbed nodes to corresponding robust experts.Extensive experiments conducted under various adversarial settings demonstratethat our method consistently achieves superior robustness against multiplegraph adversarial attacks.</description>
      <author>example@mail.com (Yuyuan Feng, Bin Ma, Enyan Dai)</author>
      <guid isPermaLink="false">2510.15333v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.15215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于图神经网络的建模方法，用于解决分布式后端系统中的交通预测问题。通过将系统抽象为图结构，结合图卷积机制和门控循环结构，实现了空间结构与时间演化的联合建模，显著提高了预测的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统模型在捕捉分布式后端系统中的复杂依赖性和动态特征方面存在局限性，需要更先进的建模方法来提高交通预测的准确性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图神经网络的建模方法，克服传统模型的限制，提高分布式后端系统中交通预测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将系统抽象为包含节点和边的图，节点特征表示流量和资源状态，邻接关系描述服务交互；使用图卷积机制实现节点特征的多阶传播和聚合；采用门控循环结构动态建模历史序列；通过时空联合建模模块融合图表示与时间依赖性；使用解码器生成未来流量预测；使用均方误差进行模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在不同预测范围和模型深度下实现了稳定的性能和低误差，显著提高了分布式后端系统中交通预测的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;图神经网络在复杂系统建模中具有巨大潜力，能够有效捕捉分布式后端系统中的复杂依赖性和动态特征。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了分布式后端系统中的交通预测问题，并提出了一种基于图神经网络的建模方法，以克服传统模型在捕捉复杂依赖性和动态特征方面的局限性。该系统被抽象为一个包含节点和边的图，其中节点特征表示流量和资源状态，邻接关系描述服务交互。图卷积机制实现了节点特征的多阶传播和聚合，而门控循环结构动态建模历史序列，从而将空间结构与时间演化相结合。时空联合建模模块进一步融合图表示与时间依赖性，解码器生成未来流量预测。模型使用均方误差进行训练，以最小化与实际值的偏差。基于公共分布式系统日志的实验构建了节点特征、拓扑和序列的组合输入，并使用MSE、RMSE、MAE和MAPE指标将所提出的方法与主流基线进行比较。结果表明，所提出的方法在不同预测范围和模型深度下实现了稳定的性能和低误差，显著提高了分布式后端系统中交通预测的准确性和鲁棒性，验证了图神经网络在复杂系统建模中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the problem of traffic prediction in distributed backendsystems and proposes a graph neural network based modeling approach to overcomethe limitations of traditional models in capturing complex dependencies anddynamic features. The system is abstracted as a graph with nodes and edges,where node features represent traffic and resource states, and adjacencyrelations describe service interactions. A graph convolution mechanism enablesmulti order propagation and aggregation of node features, while a gatedrecurrent structure models historical sequences dynamically, thus integratingspatial structures with temporal evolution. A spatiotemporal joint modelingmodule further fuses graph representation with temporal dependency, and adecoder generates future traffic predictions. The model is trained with meansquared error to minimize deviations from actual values. Experiments based onpublic distributed system logs construct combined inputs of node features,topology, and sequences, and compare the proposed method with mainstreambaselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed methodachieves stable performance and low error across different prediction horizonsand model depths, significantly improving the accuracy and robustness oftraffic forecasting in distributed backend systems and verifying the potentialof graph neural networks in complex system modeling.</description>
      <author>example@mail.com (Zhimin Qiu, Feng Liu, Yuxiao Wang, Chenrui Hu, Ziyu Cheng, Di Wu)</author>
      <guid isPermaLink="false">2510.15215v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Structural Generalization for Microservice Routing Using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.15210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的端到端优化框架，用于微服务系统中的智能路由，旨在提高复杂拓扑结构下路由决策效率和整体系统性能。&lt;h4&gt;背景&lt;/h4&gt;微服务系统中的智能路由问题，需要处理复杂拓扑结构下的路由决策和系统性能优化。&lt;h4&gt;目的&lt;/h4&gt;提高路由决策效率和整体系统性能，特别是在复杂拓扑结构下。更好地评估路径质量，捕获服务通信中的不稳定性和瓶颈风险。&lt;h4&gt;方法&lt;/h4&gt;将微服务之间的调用关系建模为图，服务节点和通信链路作为图的节点和边；使用多维特征作为输入，包括节点状态、链路延迟和调用频率；采用多层图神经网络进行高阶信息聚合和结构建模；模型为每个候选服务路径输出分数，用于指导动态路由决策；引入边感知注意力机制提高模型评估路径质量的能力；在不同网络深度、拓扑密度和服务规模下进行系统性分析。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在多个关键指标上优于现有主流策略；能够有效处理高度动态和并发的微服务环境；展示了强大的性能、鲁棒性和结构泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的端到端优化框架在微服务系统智能路由方面表现优异，能够提高路由决策效率和系统整体性能。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文专注于微服务系统中的智能路由，并提出了一种基于图神经网络的端到端优化框架。目标是提高复杂拓扑结构下路由决策效率和整体系统性能。该方法将微服务之间的调用关系建模为图。在该图中，服务节点和通信链路被视为图的节点和边。使用节点状态、链路延迟和调用频率等多维特征作为输入。采用多层图神经网络执行高阶信息聚合和结构建模。模型为每个候选服务路径输出分数。然后使用这些分数来指导动态路由决策。为了提高模型评估路径质量的能力，引入了边感知注意力机制。该机制帮助模型更准确地捕获服务通信中的不稳定性和瓶颈风险。论文还对该模型在不同网络深度、拓扑密度和服务规模下的性能进行了系统性分析。从路由准确性、预测误差和系统稳定性等方面评估了该方法的有效性。实验结果表明，该方法在多个关键指标上优于现有的主流策略。它能够有效处理高度动态和并发的微服务环境，并表现出强大的性能、鲁棒性和结构泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper focuses on intelligent routing in microservice systems andproposes an end-to-end optimization framework based on graph neural networks.The goal is to improve routing decision efficiency and overall systemperformance under complex topologies. The method models invocationrelationships among microservices as a graph. In this graph, service nodes andcommunication links are treated as graph nodes and edges. Multi-dimensionalfeatures such as node states, link latency, and call frequency are used asinput. A multi-layer graph neural network is employed to perform high-orderinformation aggregation and structural modeling. The model outputs a score foreach candidate service path. These scores are then used to guide dynamicrouting decisions. To improve the model's ability to assess path quality, anedge-aware attention mechanism is introduced. This mechanism helps the modelcapture instability and bottleneck risks in service communications moreaccurately. The paper also conducts a systematic analysis of the model'sperformance under different network depths, topology densities, and servicescales. It evaluates the effectiveness of the method in terms of routingaccuracy, prediction error, and system stability. Experimental results showthat the proposed method outperforms existing mainstream strategies acrossmultiple key metrics. It handles highly dynamic and concurrent microserviceenvironments effectively and demonstrates strong performance, robustness, andstructural generalization.</description>
      <author>example@mail.com (Chenrui Hu, Ziyu Cheng, Di Wu, Yuxiao Wang, Feng Liu, Zhimin Qiu)</author>
      <guid isPermaLink="false">2510.15210v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs</title>
      <link>http://arxiv.org/abs/2510.15188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OCR-APT是一种创新的APT检测系统，通过结合图神经网络和大型语言模型，实现了更准确、更可解释的攻击检测和故事重建，解决了现有系统高误报率和粗粒度警报的问题。&lt;h4&gt;背景&lt;/h4&gt;高级持续性威胁(APTs)是隐蔽的网络攻击，通常能逃避系统级审计日志的检测。现有系统将异常检测应用于这些图，但常有高误报率和粗粒度警报，且依赖节点属性导致虚假关联，降低了检测的鲁棒性和可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发能够生成准确、类人的完整攻击叙述的系统，解决高误报率和粗粒度警报问题，提供更鲁棒的异常检测，并生成可解释的最终报告。&lt;h4&gt;方法&lt;/h4&gt;引入OCR-APT系统，使用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式而非脆弱属性；然后使用大型语言模型(LLMs)迭代处理检测到的子图重建多阶段攻击故事，每个阶段在继续前进行验证以减少幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;在DARPA TC3、OpTC和NODLINK数据集上的评估显示，OCR-APT在检测准确率和警报可解释性方面优于最先进的系统，且能重建全面捕获攻击故事的类人报告。&lt;h4&gt;结论&lt;/h4&gt;OCR-APT系统有效解决了现有APT检测系统的局限性，通过结合GNNs和LLMs，提供了更准确和可解释的APT检测与攻击故事重建能力。&lt;h4&gt;翻译&lt;/h4&gt;高级持续性威胁(APTs)是隐蔽的网络攻击，通常能逃避系统级审计日志中的检测。来源图将这些日志建模为连接的实体和事件，揭示了线性日志表示中遗漏的关系。现有系统将这些图应用于异常检测，但常常遭受高误报率和粗粒度警报的困扰。它们对文件路径或IP等节点属性的依赖导致虚假关联，降低了检测的鲁棒性和可靠性。为了完全理解攻击的进展和影响，安全分析师需要能够生成准确、类人的完整攻击叙述的系统。为解决这些挑战，我们引入了OCR-APT，一个用于APT检测和类人攻击故事重建的系统。OCR-APT使用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式，而不是依赖文件路径或IP等脆弱属性。这种方法导致更鲁棒的异常检测。然后，它使用大型语言模型(LLMs)迭代处理检测到的子图，重建多阶段攻击故事。每个阶段在继续前进行验证，减少幻觉并确保可解释的最终报告。我们在DARPA TC3、OpTC和NODLINK数据集上的评估表明，OCR-APT在检测准确率和警报可解释性方面优于最先进的系统。此外，OCR-APT重建的类人报告全面捕获了攻击故事。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evadedetection in system-level audit logs. Provenance graphs model these logs asconnected entities and events, revealing relationships that are missed bylinear log representations. Existing systems apply anomaly detection to thesegraphs but often suffer from high false positive rates and coarse-grainedalerts. Their reliance on node attributes like file paths or IPs leads tospurious correlations, reducing detection robustness and reliability. To fullyunderstand an attack's progression and impact, security analysts need systemsthat can generate accurate, human-like narratives of the entire attack. Toaddress these challenges, we introduce OCR-APT, a system for APT detection andreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks(GNNs) for subgraph anomaly detection, learning behavior patterns around nodesrather than fragile attributes such as file paths or IPs. This approach leadsto a more robust anomaly detection. It then iterates over detected subgraphsusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.Each stage is validated before proceeding, reducing hallucinations and ensuringan interpretable final report. Our evaluations on the DARPA TC3, OpTC, andNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in bothdetection accuracy and alert interpretability. Moreover, OCR-APT reconstructshuman-like reports that comprehensively capture the attack story.</description>
      <author>example@mail.com (Ahmed Aly, Essam Mansour, Amr Youssef)</author>
      <guid isPermaLink="false">2510.15188v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis</title>
      <link>http://arxiv.org/abs/2510.15750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 6 figures, 5 tables. Code available  at:https://github.com/SinghNayanKumar/DL-surrogate-modelling&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了图神经网络(GNNs)和3D U-Nets作为参数化I型梁有限元分析(FEA)替代模型的性能，引入了基于Navier-Cauchy方程的物理信息神经网络(PINN)框架，并证明课程学习策略对稳定训练至关重要。研究发现GNNs整体优于U-Net，MPNN和图变换器达到最高精度，PINN框架显著提高了泛化能力。&lt;h4&gt;背景&lt;/h4&gt;有限元分析(FEA)是产品设计生命周期中不可或缺的部分，但计算成本高，不适合许多设计优化问题。深度学习模型可能是一个很好的解决方案。&lt;h4&gt;目的&lt;/h4&gt;评估图神经网络(GNNs)和3D U-Nets作为FEA替代模型的性能，引入物理信息神经网络(PINN)框架，研究课程学习策略对训练稳定性的影响。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNNs)和3D U-Nets作为参数化I型梁FEA的替代模型，实现基于Navier-Cauchy方程的PINN框架，采用课程学习策略：先在数据上预训练，再进行物理信息微调。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs整体上优于U-Net；最差的GNN模型(GCN)相对L2误差为8.7%，而最好的U-Net模型得分为13.0%；MPNN和图变换器达到最高精度，相对L2得分分别为3.5%和2.6%；PINN框架显著提高了泛化能力，在高信号任务上减少了高达11.3%的误差；图变换器是最准确的模型但推理速度较慢；MPNN-PINN模型在性能和效率之间提供了最佳平衡。&lt;h4&gt;结论&lt;/h4&gt;图神经网络是有限元分析的有效替代方案；课程学习策略对稳定训练至关重要；物理信息神经网络框架提高了模型的泛化能力；MPNN-PINN模型在预测性能、模型大小和推理速度之间取得了良好平衡。&lt;h4&gt;翻译&lt;/h4&gt;虽然有限元分析(FEA)是产品设计生命周期中不可或缺的部分，但分析计算成本高，使其不适合许多设计优化问题。深度学习模型可以是一个很好的解决方案。然而，选择能够以高精度模拟FEA的架构是一个挑战。本文提出了对图神经网络(GNNs)和3D U-Nets作为参数化I型梁FEA替代模型的综合评估。我们引入了一个由Navier-Cauchy方程控制的物理信息神经网络(PINN)框架，以强制执行物理定律。关键的是，我们证明课程学习策略——先在数据上预训练，再进行物理信息微调——对于稳定训练至关重要。我们的结果表明，GNNs从根本上优于U-Net。即使在GNNs中最差的GCN框架也实现了8.7%的相对L2误差，而在U-Net中最好的框架(使用高分辨率数据训练的带注意力机制的U-Net)获得了13.0%的得分。在基于图的架构中，消息传递神经网络(MPNN)和图变换器达到了最高的准确性，分别实现了3.5%和2.6%的相对L2得分。包含物理基本定律(PINN)显著提高了泛化能力，在高信号任务上减少了高达11.3%的误差。虽然图变换器是最准确的模型，但在推理时比第二好的模型MPNN-PINN慢37.5%。PINN增强的MPNN(MPNN-PINN)提供了最实用的解决方案。它在预测性能、模型大小和推理速度之间提供了良好的平衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决有限元分析(FEA)计算成本高、不适合实时应用和设计优化的问题。这个问题在现实中很重要，因为FEA虽然广泛应用于产品设计，但计算时间长，难以用于需要快速反馈的场景，如数字孪生或多次迭代的设计优化。传统替代方法如降阶模型存在线性假设、侵入性要求等局限性，难以处理复杂的非线性物理现象。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了FEA的计算瓶颈和传统替代方法的局限性，认识到需要更灵活、非线性、非侵入性的解决方案。他们借鉴了现有工作中的图神经网络处理网格数据的方法，以及物理信息神经网络(PINN)嵌入物理定律的思路。在此基础上，作者设计了系统性比较多种GNN架构的方案，并创新性地采用课程学习策略来稳定PINN训练，通过两阶段训练(数据驱动预训练+物理信息微调)解决了训练不稳定的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络(GNN)直接处理FEA的非结构化网格，并通过物理信息神经网络(PINN)嵌入物理定律来提高泛化能力。整体流程包括：1)使用gmsh和DOLFINx生成I梁的FEA数据，创建低信号和高信号数据集；2)实现多种GNN架构(GCN、GAT、MPNN、图变换器)和3D U-Net基线；3)设计节点特征编码和消息传递机制；4)将纳维-柯西方程嵌入损失函数，采用课程学习和损失权重退火稳定训练；5)使用多种指标评估模型性能和计算效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统性比较多种GNN架构，证明GNN在FEA替代模型中的优越性；2)成功将物理定律嵌入GNN，显著提高泛化能力；3)提出稳健的PINN训练策略，通过课程学习和损失权重退火解决训练不稳定问题；4)进行全面的性能-效率分析，识别最优实用架构。相比之前工作，本文提供了更全面的架构比较，解决了PINN训练不稳定问题，并考虑了实际部署中的模型大小和推理速度等实用因素。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统比较多种图神经网络架构并成功集成物理信息学习，为有限元分析提供了一个高效、准确且泛化能力强的替代模型，同时提出了稳定的训练策略和实用的部署指南。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Finite Element Analysis (FEA) is an integral part of the productdesign lifecycle, the analysis is computationally expensive, making itunsuitable for many design optimization problems. The deep learning models canbe a great solution. However, selecting the architecture that emulates the FEAwith great accuracy is a challenge. This paper presents a comprehensiveevaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEAof parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)framework, governed by the Navier Cauchy equations, to enforce physical laws.Crucially, we demonstrate that a curriculum learning strategy, pretraining ondata followed by physics informed fine tuning, is essential for stabilizingtraining. Our results show that GNNs fundamentally outperform the U-Net. Eventhe worst performer among GNNs, the GCN framework, achieved a relative L2 errorof 8.7% while the best framework among U Net, U Net with attention mechanismtrained on high resolution data, achieved 13.0% score. Among the graph-basedarchitectures, the Message Passing Neural Networks (MPNN) and GraphTransformers achieved the highest accuracy, achieving a relative L2 score of3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)significantly improved the generalization, reducing error by up to 11.3% onhigh-signal tasks. While the Graph Transformer is the most accurate model, itis more 37.5% slower during inference when compared to second best model, MPNNPINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.It offers a good compromise between predictive performance, model size, andinference speed.</description>
      <author>example@mail.com (Nayan Kumar Singh)</author>
      <guid isPermaLink="false">2510.15750v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand</title>
      <link>http://arxiv.org/abs/2510.12328v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合物理信息的图神经网络与极值分析技术，用于改进泰国地区的站点降雨预测，特别是在极端事件预报方面取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍是一个重大挑战。传统方法在泰国地区的站点降雨预测中存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型预测模型，提高泰国地区站点降雨预测的准确性，特别是针对极端事件的预测能力，并提供高分辨率地图以支持长期水资源管理决策。&lt;h4&gt;方法&lt;/h4&gt;结合物理信息的图神经网络与极值分析技术，利用站点图的表示捕捉复杂时空模式，通过遥相关提供可解释性；预处理影响区域降雨的气候指标；应用基于图注意力网络和长短期记忆网络的模型；使用空间季节感知广义帕累托分布方法进行阈值超限映射以解决极端问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该方法在大多数地区优于成熟基线模型，包括易发生极端事件的区域；与最先进技术保持强竞争力；与业务预报系统SEAS5相比，显著改进了极端事件的预测；能够提供支持决策的高分辨率地图。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法是改进降雨预测特别是极端事件预测的实用增强工具，可为长期水资源管理提供决策支持。&lt;h4&gt;翻译&lt;/h4&gt;准确的降雨预报，特别是对极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。本文提出了结合物理信息的图神经网络与极值分析技术的新方法，以改进泰国地区的站点降雨预测。该模型利用站点图的表示来捕捉复杂的时空模式，并通过遥相关提供可解释性。我们预处理可能影响区域降雨的相关气候指标。提出的基于图注意力网络和长短期记忆网络的模型，使用简单地形降水物理公式推导的初始边特征应用注意力机制，嵌入随后由LSTM层处理。为解决极端问题，我们使用新颖的空间季节感知广义帕累托分布方法进行阈值超限映射，克服了传统机器学习模型的局限性。实验证明，我们的方法在大多数地区都优于成熟的基线模型，包括易发生极端事件的区域，并与最先进技术保持强竞争力。与业务预报系统SEAS5相比，我们的实际应用改进了极端事件的预测，并提供了实用的增强功能，可以生成支持长期水资源管理决策的高分辨率地图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate rainfall forecasting, particularly for extreme events, remains asignificant challenge in climatology and the Earth system. This paper presentsnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-valueanalysis techniques to improve gauge-station rainfall predictions acrossThailand. The model leverages a graph-structured representation of gaugestations to capture complex spatiotemporal patterns, and it offersexplainability through teleconnections. We preprocess relevant climate indicesthat potentially influence regional rainfall. The proposed Graph AttentionNetwork with Long Short-Term Memory (Attention-LSTM) applies the attentionmechanism using initial edge features derived from simpleorographic-precipitation physics formulation. The embeddings are subsequentlyprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold(POT) mapping using the novel Spatial Season-aware Generalized ParetoDistribution (GPD) method, which overcomes limitations of traditionalmachine-learning models. Experiments demonstrate that our method outperformswell-established baselines across most regions, including areas prone toextremes, and remains strongly competitive with the state of the art. Comparedwith the operational forecasting system SEAS5, our real-world applicationimproves extreme-event prediction and offers a practical enhancement to producehigh-resolution maps that support decision-making in long-term watermanagement.</description>
      <author>example@mail.com (Kiattikun Chobtham, Kanoksri Sarinnapakorn, Kritanai Torsri, Prattana Deeprasertkul, Jirawan Kamma)</author>
      <guid isPermaLink="false">2510.12328v3</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers</title>
      <link>http://arxiv.org/abs/2510.15385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FreqPDE的新方法，用于从多视图2D图像中准确检测3D物体，解决了现有方法中深度预测质量不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;从多视图2D图像中准确检测3D物体是自动驾驶领域的一项具有挑战性但至关重要的任务。当前方法依赖深度预测来恢复空间信息，但存在深度不连续和小物体不清晰等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供更准确深度信息的方法，解决现有深度预测中的边界不连续和小物体不清晰问题，并考虑跨视图一致性和尺度不变性。&lt;h4&gt;方法&lt;/h4&gt;提出频率感知位置深度嵌入（FreqPDE），包含三个主要模块：频率感知空间金字塔编码器（FSPE）构建特征金字塔；跨视图尺度不变深度预测器（CSDP）估计像素级深度分布；位置深度编码器（PDE）生成3D深度感知特征。同时采用混合深度监督进行互补深度学习。&lt;h4&gt;主要发现&lt;/h4&gt;现有深度预测方法存在物体边界深度不连续和小物体不清晰的问题，主要由投影点稀疏监督和高级图像特征使用引起。跨视图一致性和尺度不变性在先前方法中被忽视。&lt;h4&gt;结论&lt;/h4&gt;在nuScenes数据集上的广泛实验证明了所提出FreqPDE方法的有效性和优越性，能够显著提升3D物体检测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;从多视图2D图像中准确检测3D物体是自动驾驶领域一项具有挑战性但至关重要的任务。当前方法通过整合深度预测来恢复物体查询解码的空间信息，这需要在训练阶段使用LiDAR点进行显式监督。然而，预测的深度质量仍然不理想，如物体边界深度不连续和小物体不清晰，主要由投影点的稀疏监督和使用高级图像特征进行深度预测引起。此外，先前的方法也忽视了跨视图一致性和尺度不变性。本文引入了频率感知位置深度嵌入（FreqPDE）来为2D图像特征赋予空间信息，用于3D检测transformer解码器，这通过三个主要模块实现：频率感知空间金字塔编码器（FSPE）结合不同级别的高频边缘线索和低级语义构建特征金字塔；跨视图尺度不变深度预测器（CSDP）使用跨视图和高效通道注意力机制估计像素级深度分布；位置深度编码器（PDE）结合2D图像特征和3D位置嵌入，为查询解码生成3D深度感知特征。同时采用混合深度监督，从度量和分布方面进行互补深度学习。在nuScenes数据集上进行的广泛实验证明了所提出方法的有效性和优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从多视角2D图像中准确检测3D物体时深度预测质量不佳的问题，包括物体边界深度不连续、小物体不清晰等。这个问题在自动驾驶领域至关重要，因为准确的3D物体检测是确保自动驾驶系统安全感知周围环境的关键，而仅使用摄像头的方法比基于LiDAR的方法成本更低，但恢复3D空间信息是一个具有挑战性的病态问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前深度预测方法的三个主要缺陷：仅使用高级图像特征导致细节丢失、投影点云稀疏监督导致学习不完整、以及忽略了跨视图一致性和尺度不变性。基于这些问题，作者设计了FreqPDE方法，包含三个核心模块：频率感知空间金字塔编码器(FSPE)、跨视图尺度不变深度预测器(CSDP)和位置深度编码器(PDE)。该方法借鉴了现有工作，如BEVFormer和StreamPETR等基于Transformer的3D检测方法，以及BEVDepth等深度预测方法，同时引入了FreqFusion等频率域学习的方法，但进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用频率感知的位置深度嵌入为2D视觉特征提供高质量的空间信息，通过结合高频边缘细节和低频全局语义提高深度预测质量，并采用混合深度监督进行互补学习。整体流程包括：1)FSPE模块构建特征金字塔，结合不同级别的高频边缘线索和低频语义；2)CSDP模块进行分层深度预测，使用跨视图注意力和高效通道注意力确保一致性和尺度不变性；3)PDE模块结合2D图像特征和3D位置嵌入生成深度感知特征；4)采用混合深度监督，结合稀疏LiDAR和密集伪深度图；5)使用Transformer解码器生成最终检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将频率域信息引入多视图3D检测，同时利用高频和低频信息；2)FSPE模块通过低频语义提取和高频边界增强保留更多细节；3)CSDP模块引入跨视图注意力和相机感知通道注意力解决一致性和尺度不变性问题；4)混合深度监督结合稀疏LiDAR和密集伪深度图进行互补学习。相比之前工作，不同之处在于：之前方法主要使用单一频率信息，忽略了跨视图一致性和尺度不变性，且监督方式单一；而FreqPDE同时利用高频和低频信息，解决了跨视图一致性和尺度不变性问题，并引入了混合深度监督。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FreqPDE通过引入频率感知的位置深度嵌入和混合深度监督，显著提高了多视角3D物体检测的准确性，特别是在处理远距离和小物体方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting 3D objects accurately from multi-view 2D images is a challengingyet essential task in the field of autonomous driving. Current methods resortto integrating depth prediction to recover the spatial information for objectquery decoding, which necessitates explicit supervision from LiDAR pointsduring the training phase. However, the predicted depth quality is stillunsatisfactory such as depth discontinuity of object boundaries andindistinction of small objects, which are mainly caused by the sparsesupervision of projected points and the use of high-level image features fordepth prediction. Besides, cross-view consistency and scale invariance are alsooverlooked in previous methods. In this paper, we introduce Frequency-awarePositional Depth Embedding (FreqPDE) to equip 2D image features with spatialinformation for 3D detection transformer decoder, which can be obtained throughthree main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder(FSPE) constructs a feature pyramid by combining high-frequency edge clues andlow-frequency semantics from different levels respectively. Then the Cross-viewScale-invariant Depth Predictor (CSDP) estimates the pixel-level depthdistribution with cross-view and efficient channel attention mechanism.Finally, the Positional Depth Encoder (PDE) combines the 2D image features and3D position embeddings to generate the 3D depth-aware features for querydecoding. Additionally, hybrid depth supervision is adopted for complementarydepth learning from both metric and distribution aspects. Extensive experimentsconducted on the nuScenes dataset demonstrate the effectiveness and superiorityof our proposed method.</description>
      <author>example@mail.com (Haisheng Su, Junjie Zhang, Feixiang Song, Sanping Zhou, Wei Wu, Nanning Zheng, Junchi Yan)</author>
      <guid isPermaLink="false">2510.15385v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>ERNet: Efficient Non-Rigid Registration Network for Point Sequences</title>
      <link>http://arxiv.org/abs/2510.15800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025. Project Page: https://guangzhaohe.com/ernet&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为ERNet的高效前馈模型，用于解决将物体形状注册到经历非刚性变形的点云序列的挑战。该方法通过两阶段流程预测变形图序列，能有效处理嘈杂和部分输入，并利用时间信息实现准确和一致的序列注册。&lt;h4&gt;背景&lt;/h4&gt;将物体形状注册到经历非刚性变形的点云序列是一个长期存在的挑战。主要困难来自两个方面：目标函数非凸性导致的局部极小值（特别是在嘈杂或部分输入情况下）阻碍了准确和鲁棒的变形估计；长序列中的误差累积导致跟踪失败。&lt;h4&gt;目的&lt;/h4&gt;解决非刚性变形点云序列注册中的挑战，特别是局部极小值问题和误差累积问题，同时提高处理效率。&lt;h4&gt;方法&lt;/h4&gt;采用可扩展的数据驱动方法，提出ERNet模型，这是一种在大变形数据集上训练的高效前馈模型。关键设计是通过两阶段流程预测变形图序列：首先估计帧级粗略图节点实现鲁棒初始化，然后在滑动窗口方式下随时间细化它们的轨迹。该方法能有效处理嘈杂和部分输入，同时利用时间信息进行准确和一致的序列注册。&lt;h4&gt;主要发现&lt;/h4&gt;在Deforming Things4D和D-FAUST数据集上，所提出的方法优于之前的先进方法；与之前最好的方法相比，实现了4倍以上的速度提升，显著提高了处理效率。&lt;h4&gt;结论&lt;/h4&gt;ERNet模型有效解决了非刚性变形点云序列注册中的挑战，在准确性和效率方面都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;将物体形状注册到经历非刚性变形的点云序列是一个长期存在的挑战。关键困难源于两个因素：(i)由于目标函数的非凸性，特别是在嘈杂或部分输入的情况下，存在局部极小值，这阻碍了准确和鲁棒的变形估计；(ii)长序列中的误差累积导致跟踪失败。为应对这些挑战，我们采用可扩展的数据驱动方法，并提出了ERNet，一种在大变形数据集上训练的高效前馈模型。它旨在处理嘈杂和部分输入，同时有效利用时间信息进行准确和一致的序列注册。我们设计的关键是通过两阶段流程预测变形图序列，首先估计帧级粗略图节点以实现鲁棒初始化，然后在滑动窗口方式下随时间细化它们的轨迹。大量实验表明，我们提出的方法(i)在Deforming Things4D和D-FAUST数据集上都优于之前的先进方法，(ii)与之前最好的方法相比实现了4倍以上的速度提升，提供了显著的效率改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决将物体形状注册到一系列经历非刚性变形的点云上的问题。这个问题在计算机视觉和机器人领域至关重要，因为它涉及动态重建、场景理解和机器人操作等广泛应用。传统方法容易陷入局部最优解，特别是在处理噪声或部分输入的点云时，导致变形估计不准确；同时，长序列中的误差累积会导致跟踪失败，限制了这些方法在实际应用中的有效性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统优化方法容易陷入局部最优，基于神经变形场的方法难以泛化到噪声或部分输入，而预测密集对应关系的方法计算复杂度高。因此，作者设计了一个高效的前馈模型，采用数据驱动方法在大型变形数据集上训练。方法借鉴了变形图表示、三平面编码器、滑动窗口策略和局部刚性假设等现有技术，但创新性地将它们组合成一个两阶段管道：首先估计帧级粗略图节点，然后在滑动窗口方式下细化节点轨迹，实现了鲁棒且时间一致的序列配准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用变形图作为非刚性变形的紧凑表示，通过两阶段策略（粗略匹配和时空细化）实现高效配准，并利用局部刚性假设推断节点变换属性。整体流程包括：1) 使用三平面编码器将源点云和目标点云序列编码为特征；2) 从源点云采样节点，通过节点到帧匹配初始化节点位置；3) 使用时空变换器在滑动窗口中细化节点轨迹；4) 利用局部刚性假设和Procrustes分析估计节点变换；5) 使用径向基函数线性混合皮肤将变形图转换为密集变形场，完成配准。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 高效的前馈序列非刚性注册架构，采用两阶段策略提高鲁棒性和时间一致性；2) 变形图回归作为非刚性注册的高效表示，平衡了表达能力和计算效率；3) 利用局部刚性假设推断节点变换属性，避免直接预测高维非线性变换的困难。相比传统优化方法，ERNet不易陷入局部最优且能处理噪声输入；相比基于神经变形场的方法，它不需要每帧优化，效率更高；相比预测密集对应关系的方法，它计算效率更高，更适合处理长序列。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ERNet通过引入基于变形图的两阶段预测策略，实现了高效、准确且时间一致的非刚性点云序列配准，在保持高精度的同时实现了超过4倍的速度提升，显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Registering an object shape to a sequence of point clouds undergoingnon-rigid deformation is a long-standing challenge. The key difficulties stemfrom two factors: (i) the presence of local minima due to the non-convexity ofregistration objectives, especially under noisy or partial inputs, whichhinders accurate and robust deformation estimation, and (ii) error accumulationover long sequences, leading to tracking failures. To address these challenges,we introduce to adopt a scalable data-driven approach and propose ERNet, anefficient feed-forward model trained on large deformation datasets. It isdesigned to handle noisy and partial inputs while effectively leveragingtemporal information for accurate and consistent sequential registration. Thekey to our design is predicting a sequence of deformation graphs through atwo-stage pipeline, which first estimates frame-wise coarse graph nodes forrobust initialization, before refining their trajectories over time in asliding-window fashion. Extensive experiments show that our proposed approach(i) outperforms previous state-of-the-art on both the DeformingThings4D andD-FAUST datasets, and (ii) achieves more than 4x speedup compared to theprevious best, offering significant efficiency improvement.</description>
      <author>example@mail.com (Guangzhao He, Yuxi Xiao, Zhen Xu, Xiaowei Zhou, Sida Peng)</author>
      <guid isPermaLink="false">2510.15800v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes</title>
      <link>http://arxiv.org/abs/2510.15467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MRASfM的多相机重建和聚合运动结构框架，专门针对驾驶场景中的运动结构(SfM)问题，解决了姿态估计不可靠、道路表面重建异常值过多以及重建效率低等挑战。&lt;h4&gt;背景&lt;/h4&gt;Structure from Motion (SfM)估计相机姿态并重建点云，是各种任务的基础。然而，将SfM应用于多相机系统捕捉的驾驶场景存在显著困难，包括不可靠的姿态估计、道路表面重建中过多的异常值以及低重建效率。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些限制，提出专门为驾驶场景设计的多相机重建和聚合运动结构(MRASfM)框架。&lt;h4&gt;方法&lt;/h4&gt;MRASfM通过以下方法解决挑战：1)在注册过程中利用多相机系统内的固定空间关系提高姿态估计可靠性；2)采用平面模型有效去除道路表面重建中的错误点；3)在捆绑调整中将多相机集视为单一单元减少优化变量提高效率；4)通过场景关联和组装模块以从粗到细的方式实现多场景聚合。&lt;h4&gt;主要发现&lt;/h4&gt;在实际车辆上部署多相机系统验证了MRASfM在不同场景中的泛化能力和在具有挑战性条件下的鲁棒性。在公共数据集上的大规模验证结果显示，MRASfM达到了最先进的性能，实现了较低的绝对姿态误差。&lt;h4&gt;结论&lt;/h4&gt;MRASfM框架有效地解决了多相机系统在驾驶场景中应用SfM时面临的主要挑战，提高了姿态估计的可靠性、道路表面重建的质量和整体重建效率。&lt;h4&gt;翻译&lt;/h4&gt;运动结构(SfM)估计相机姿态并重建点云，形成各种任务的基础。然而，将SfM应用于由多相机系统捕捉的驾驶场景存在显著困难，包括不可靠的姿态估计、道路表面重建中过多的异常值以及低重建效率。为了解决这些限制，我们提出了一种专门为驾驶场景设计的多相机重建和聚合运动结构(MRASfM)框架。MRASfM通过在注册过程中利用多相机系统内的固定空间关系来提高相机姿态估计的可靠性。为了提高道路表面重建的质量，我们的框架采用平面模型有效去除三角测量道路表面中的错误点。此外，在捆绑调整(BA)中将多相机集视为单一单元有助于减少优化变量以提高效率。此外，MRASfM通过场景关联和组装模块以从粗到细的方式实现多场景聚合。我们在实际车辆上部署了多相机系统，以验证MRASfM在各种场景中的泛化能力以及在具有挑战性条件下的鲁棒性。此外，在公共数据集上的大规模验证结果显示了MRASfM的最先进性能，实现了较低的绝对姿态误差。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决将传统SfM技术应用于多相机系统捕获的驾驶场景时的三大挑战：不可靠的姿态估计、道路表面重建中过多的离群点和低重建效率。这个问题在现实中很重要，因为准确的驾驶场景重建是高清地图构建和新视角合成等关键下游任务的基础，而传统vSLAM存在累积漂移问题，直接应用SfM又面临上述挑战，限制了自动驾驶系统对环境的精确感知和自身定位能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先深入分析了传统SfM在驾驶场景中的局限性，然后有选择地借鉴了现有工作：参考了多相机vSLAM方法如BAMF-SLAM和MAVIS，但意识到它们需要精确校准；借鉴了COLMAP和MMA等SfM方法，但发现它们在处理多相机系统时有局限；受MCSfM启发但希望进一步提高效率。作者针对性地设计了解决方案：利用多相机固定空间关系提高姿态估计可靠性，应用平面模型过滤道路离群点，将相机集视为统一单位提高效率，并设计场景聚合模块整合碎片化场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将多相机系统视为刚性单元，利用相机间的固定空间关系作为先验约束，结合语义信息提高重建质量，通过分层优化提高效率，并以粗到细方式整合碎片化场景。整体流程分为单场景重建和多场景聚合：单场景重建包括多相机对应点搜索、先验初始化和迭代重建（相机集注册、语义辅助三角测量、相机集BA）；多场景聚合包括场景关联（使用GNSS定位）和场景组装（粗组装和精组装，通过SfM优化变换矩阵）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 相机集注册模块，将多相机视为刚性单元提高姿态估计鲁棒性，不同于传统方法忽略或简单利用相机间关系；2) 相机集BA模块，优化车辆姿态和内部相对姿态而非单个相机姿态，显著减少优化变量；3) 语义辅助三角测量，使用平面模型过滤道路离群点，专门处理道路表面特殊挑战；4) 多场景聚合模块，以粗到细方式整合无共享图像的碎片化场景，突破了传统SfM聚合方法的限制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MRASfM通过将多相机系统视为刚性单元、结合语义信息过滤离群点、优化重建流程以及设计多场景聚合方法，实现了驾驶场景中更准确、高效和鲁棒的三维重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Structure from Motion (SfM) estimates camera poses and reconstructs pointclouds, forming a foundation for various tasks. However, applying SfM todriving scenes captured by multi-camera systems presents significantdifficulties, including unreliable pose estimation, excessive outliers in roadsurface reconstruction, and low reconstruction efficiency. To address theselimitations, we propose a Multi-camera Reconstruction and AggregationStructure-from-Motion (MRASfM) framework specifically designed for drivingscenes. MRASfM enhances the reliability of camera pose estimation by leveragingthe fixed spatial relationships within the multi-camera system during theregistration process. To improve the quality of road surface reconstruction,our framework employs a plane model to effectively remove erroneous points fromthe triangulated road surface. Moreover, treating the multi-camera set as asingle unit in Bundle Adjustment (BA) helps reduce optimization variables toboost efficiency. In addition, MRASfM achieves multi-scene aggregation throughscene association and assembly modules in a coarse-to-fine fashion. We deployedmulti-camera systems on actual vehicles to validate the generalizability ofMRASfM across various scenes and its robustness in challenging conditionsthrough real-world applications. Furthermore, large-scale validation results onpublic datasets show the state-of-the-art performance of MRASfM, achieving0.124 absolute pose error on the nuScenes dataset.</description>
      <author>example@mail.com (Lingfeng Xuan, Chang Nie, Yiqing Xu, Zhe Liu, Yanzi Miao, Hesheng Wang)</author>
      <guid isPermaLink="false">2510.15467v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)</title>
      <link>http://arxiv.org/abs/2510.15219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究扩展了之前关于使用乘积系数增强3D LiDAR点云分类的工作，展示了将乘积系数与自编码器表示和KNN分类器结合可以带来性能提升。&lt;h4&gt;背景&lt;/h4&gt;研究基于之前的工作，该工作引入了乘积系数（一种测度论描述符）来补充原始的LiDAR空间特征。&lt;h4&gt;目的&lt;/h4&gt;探索将乘积系数与自编码器表示和KNN分类器结合的方法，以提升LiDAR分类性能。&lt;h4&gt;方法&lt;/h4&gt;结合乘积系数与自编码器表示和KNN分类器，并与基于PCA的基线方法以及早期框架进行比较。还研究了逐级添加乘积系数的效果。&lt;h4&gt;主要发现&lt;/h4&gt;将乘积系数与自编码器表示和KNN分类器结合，在PCA基线和早期框架上带来了一致的性能提升。逐级添加乘积系数显示出更丰富的系数集合系统性地改善了类别可分离性和整体准确性。&lt;h4&gt;结论&lt;/h4&gt;结合分层乘积系数特征与自编码器对提升LiDAR分类性能具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;这项工作扩展了我们之前关于使用乘积系数增强3D LiDAR点云分类的研究，乘积系数是一种补充原始空间LiDAR特征的测度论描述符。在这里，我们展示了将乘积系数与自编码器表示和KNN分类器相结合，在基于PCA的基线和我们早期的框架上都能带来一致的性能提升。我们还研究了逐级添加乘积系数的影响，揭示了一个明显的趋势：更丰富的系数集合系统性地改善了类别可分离性和整体准确性。结果强调了将分层乘积系数特征与自编码器相结合以进一步提升LiDAR分类性能的价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是改进3D LiDAR点云数据的分类性能。LiDAR技术广泛应用于数字高程模型更新、冰川和滑坡监测、海岸线分析和城市发展等领域，而将3D LiDAR点准确分类为语义类别（如植被、人造结构和水体）是这些应用中的关键步骤。提高分类准确率对于环境监测、城市规划、灾害评估等实际应用具有重要意义，特别是在气候变化研究中，如森林生长和碳封存能力的评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在之前工作[5]中引入了乘积系数作为度量论描述符，通过在原始空间Lidar特征基础上添加这些系数来增强分类性能。本研究进一步扩展了这个框架，借鉴了自编码器在表示学习方面的优势，用自编码器替代了之前工作使用的主成分分析(PCA)。作者认识到线性变换(如PCA)在捕获复杂特征依赖关系和减少冗余方面的局限性，因此引入了非线性表示学习方法。实验设计包括比较不同维度减少方法(PCA、自编码器、Nystroem)和不同分类器(KNN、随机森林)的性能，以验证新方法的有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 方法的核心思想是通过结合乘积系数和自编码器来增强3D LiDAR点云分类性能。乘积系数是基于度量论的特征，能够捕捉点云数据的局部结构信息，超越原始空间坐标。自编码器则学习非线性表示，能够更有效地捕获复杂特征依赖关系并减少冗余。整体实现流程包括：1) 特征生成：计算每个数据点周围局部邻域内的乘积系数，生成七个新特征；2) 特征标准化：将生成的特征标准化到单位立方体[0,1]^3；3) 维度减少：使用PCA或自编码器减少特征维度；4) 分类：使用KNN或随机森林分类器进行分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入自编码器替代PCA进行非线性表示学习，能够更有效地捕获复杂特征依赖关系；2) 系统地评估了不同级别乘积系数对分类性能的影响，发现更丰富的系数集能系统性地提高类别可分性和整体准确性；3) 实验证明结合乘积系数和自编码器的框架在分类准确率和F1分数上持续优于基于PCA的基线和之前的框架。相比之前的工作，主要不同在于使用了自编码器进行非线性表示学习，而不是使用PCA进行线性变换，以及更系统地评估了不同级别乘积系数的影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过结合基于度量论的乘积系数和自编码器非线性表示学习，显著提高了3D LiDAR点云分类的性能，为地理空间数据分析提供了一个更强大的框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work extends our previous study on enhancing 3D LiDAR point-cloudclassification with product coefficients\cite{medina2025integratingproductcoefficientsimproved}, measure-theoreticdescriptors that complement the original spatial Lidar features. Here, we showthat combining product coefficients with an autoencoder representation and aKNN classifier delivers consistent performance gains over both PCA-basedbaselines and our earlier framework. We also investigate the effect of addingproduct coefficients level by level, revealing a clear trend: richer sets ofcoefficients systematically improve class separability and overall accuracy.The results highlight the value of combining hierarchical product-coefficientfeatures with autoencoders to push LiDAR classification performance further.</description>
      <author>example@mail.com (Patricia Medina, Rasika Karkare)</author>
      <guid isPermaLink="false">2510.15219v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</title>
      <link>http://arxiv.org/abs/2510.12276v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为空间强制(SF)的对齐策略，用于增强视觉-语言-动作(VLA)模型的空间理解能力，无需依赖明确的3D输入或深度估计器。&lt;h4&gt;背景&lt;/h4&gt;大多数VLA模型构建于仅基于2D数据预训练的视觉语言模型上，缺乏准确的空间感知能力，影响在3D物理世界中的操作。现有解决方案面临传感器噪声、硬件异构性和深度覆盖不完整等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法使VLA模型能够获得空间理解能力，而不依赖于明确的3D输入或深度估计器。&lt;h4&gt;方法&lt;/h4&gt;提出空间强制(SF)对齐策略，通过将VLA的中间视觉嵌入与预训练的3D基础模型生成的几何表示对齐，隐式强制VLA模型发展空间理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;SF通过在中间层强制对齐，引导VLA编码更丰富的空间表示，提高动作精确度。实验表明SF取得了最先进的结果，超越了基于2D和3D的VLA，将训练速度提高了最多3.8倍，并提高了各种机器人任务的数据效率。&lt;h4&gt;结论&lt;/h4&gt;SF是一种简单有效的对齐策略，能够使VLA模型获得空间理解能力，提高性能和训练效率。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型最近在使机器人能够遵循语言指令和执行精确动作方面显示出强大的潜力。然而，大多数VLA构建于仅基于2D数据预训练的视觉语言模型上，这些模型缺乏准确的空间感知能力，阻碍了它们在3D物理世界中的操作能力。现有解决方案尝试整合明确的3D传感器输入，如深度图或点云，但由于传感器噪声、硬件异构性和现有数据集中的深度覆盖不完整，这些方法面临挑战。从2D图像估计3D线索的替代方法也受限于深度估计器的有限性能。我们提出了空间强制(SF)，这是一种简单而有效的对齐策略，隐式强制VLA模型发展空间理解能力，而不依赖于明确的3D输入或深度估计器。SF将VLA的中间视觉嵌入与预训练的3D基础模型生成的几何表示对齐。通过在中间层强制对齐，SF引导VLA编码更丰富的空间表示，提高动作精确度。在模拟和真实环境中的大量实验表明，SF取得了最先进的结果，超越了基于2D和3D的VLA。SF进一步将训练速度提高了最多3.8倍，并提高了各种机器人任务的数据效率。项目页面位于https://spatial-forcing.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉-语言-动作模型(VLA)缺乏准确空间感知能力的问题，因为这些模型大多仅基于2D数据预训练，无法有效适应3D物理世界。这个问题在现实中很重要，因为机器人操作需要在3D世界中整合语义推理和精确控制；在研究中重要是因为现有解决方案要么依赖昂贵的3D传感器(面临噪声和硬件兼容性问题)，要么从2D图像估计3D信息(受限于深度估计器性能)，而本文方法提供了一个无需这些依赖的替代方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过深度探测实验观察到当前VLA模型的视觉嵌入缺乏有意义的空间结构，然后提出一种隐式增强空间理解能力的方法。他们借鉴了表示监督领域的进展，特别是表示对齐策略，利用预训练的3D基础模型VGGT提供丰富的空间表示作为监督信号。作者还参考了Huang等人的工作，发现监督相对较深但不是最深的层(第24层)效果最佳，因为太浅的层可能无法获得足够的空间信息，而太深的层则会丢失视觉特定特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将VLA模型的中间视觉嵌入与预训练3D基础模型产生的几何表示进行对齐，隐式地强制模型发展空间理解能力，无需显式3D输入或深度估计器。实现流程是：1)输入多视角图像到VGGT模型生成空间表示；2)将VLA的中间视觉嵌入与这些空间表示进行对齐；3)使用余弦相似度作为对齐目标函数；4)选择第24层进行监督；5)将对齐损失与动作生成损失结合；6)推理阶段与标准VLA模型操作相同，无额外计算开销。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Spatial Forcing方法，通过隐式对齐增强空间感知；2)不依赖显式3D传感器输入或深度估计器；3)通过中间层对齐引导模型编码丰富空间表示；4)实现训练加速(最高3.8倍)和数据效率提升。相比之前工作，不同之处在于：与显式3D输入方法相比，SF无需额外传感器；与从2D估计3D的方法相比，不受深度估计器限制；与现有表示监督方法不同，SF专注于空间表示对齐，特别针对VLA的空间感知提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Spatial Forcing方法，通过将视觉-语言-动作模型的中间视觉嵌入与预训练3D基础模型的空间表示进行隐式对齐，显著提升了模型的空间感知能力、训练效率和数据利用率，无需依赖显式3D传感器输入或深度估计器。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have recently shown strong potential inenabling robots to follow language instructions and execute precise actions.However, most VLAs are built upon vision-language models pretrained solely on2D data, which lack accurate spatial awareness and hinder their ability tooperate in the 3D physical world. Existing solutions attempt to incorporateexplicit 3D sensor inputs such as depth maps or point clouds, but theseapproaches face challenges due to sensor noise, hardware heterogeneity, andincomplete depth coverage in existing datasets. Alternative methods thatestimate 3D cues from 2D images also suffer from the limited performance ofdepth estimators. We propose Spatial Forcing (SF), a simple yet effectivealignment strategy that implicitly forces VLA models to develop spatialcomprehension capabilities without relying on explicit 3D inputs or depthestimators. SF aligns intermediate visual embeddings of VLAs with geometricrepresentations produced by pretrained 3D foundation models. By enforcingalignment at intermediate layers, SF guides VLAs to encode richer spatialrepresentations that enhance action precision. Extensive experiments insimulation and real-world environments demonstrate that SF achievesstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF furtheraccelerates training by up to 3.8x and improves data efficiency across diverserobotic tasks. Project page is at https://spatial-forcing.github.io/</description>
      <author>example@mail.com (Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li)</author>
      <guid isPermaLink="false">2510.12276v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Programmatic Representation Learning with Language Models</title>
      <link>http://arxiv.org/abs/2510.14825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at https://github.com/gpoesia/leapr/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种称为'学习程序化表示'(LeaPR)的模型，它结合了决策树和通过大型语言模型(LLMs)合成的特征函数，能够在不依赖神经网络的情况下实现高质量的预测，同时保持模型的可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统监督机器学习模型（如决策树）是高效且可解释的预测器，但其质量高度依赖于输入特征的选择。虽然神经网络可以直接从原始数据（如图像或文本）学习有用的表示，但这以牺牲可解释性和需要专门硬件高效运行为代价。&lt;h4&gt;目的&lt;/h4&gt;探索一种新的模型类LeaPR，它将表示为代码（从数据点到标量的函数）的任意特征与决策树预测器堆叠，从而在保持可解释性的同时实现高质量的预测。&lt;h4&gt;方法&lt;/h4&gt;1. 使用大型语言模型(LLMs)合成特征函数，利用它们在广泛领域的丰富先验知识和使用现有领域特定库编写代码的能力；2. 提出两种算法从监督数据中学习LeaPR模型：设计了FunSearch的适配版本来学习特征而非直接生成预测器；开发了经典ID3算法的新变体用于决策树学习，在分割叶节点时按需生成新特征。&lt;h4&gt;主要发现&lt;/h4&gt;在从国际象棋位置评估到图像和文本分类的实验中，该方法学习了高质量的无神经网络预测器，通常可与神经网络相媲美。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种灵活的范式，用于端到端学习可解释的表示，其中特征和预测可以轻松检查和理解。&lt;h4&gt;翻译&lt;/h4&gt;传统监督机器学习的经典模型，如决策树，是高效且可解释的预测器，但其质量高度依赖于特定输入特征的选择。虽然神经网络可以直接从原始数据（例如图像或文本）学习有用的表示，但这以牺牲可解释性和需要专门硬件高效运行为代价。在本文中，我们探索了一个称为学习程序化表示的假设类，它将表示为代码的任意特征（从数据点到标量的函数）与决策树预测器堆叠。我们使用大型语言模型合成特征函数，这些模型在广泛领域拥有丰富的先验知识，并且使用现有领域特定库编写代码的能力令人瞩目。我们提出了两种算法从监督数据中学习LeaPR模型。首先，我们设计了FunSearch的适配版本来学习特征而非直接生成预测器。然后，我们开发了经典ID3算法用于决策树学习的新变体，其中在分割叶节点时按需生成新特征。从国际象棋位置评估到图像和文本分类的实验中，我们的方法学习了高质量的无神经网络预测器，通常可与神经网络相媲美。我们的研究提出了一种灵活的范式，用于端到端学习可解释的表示，其中特征和预测可以轻松检查和理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical models for supervised machine learning, such as decision trees, areefficient and interpretable predictors, but their quality is highly dependenton the particular choice of input features. Although neural networks can learnuseful representations directly from raw data (e.g., images or text), thiscomes at the expense of interpretability and the need for specialized hardwareto run them efficiently. In this paper, we explore a hypothesis class we callLearned Programmatic Representations (LeaPR) models, which stack arbitraryfeatures represented as code (functions from data points to scalars) anddecision tree predictors. We synthesize feature functions using Large LanguageModels (LLMs), which have rich prior knowledge in a wide range of domains and aremarkable ability to write code using existing domain-specific libraries. Wepropose two algorithms to learn LeaPR models from supervised data. First, wedesign an adaptation of FunSearch to learn features rather than directlygenerate predictors. Then, we develop a novel variant of the classical ID3algorithm for decision tree learning, where new features are generated ondemand when splitting leaf nodes. In experiments from chess position evaluationto image and text classification, our methods learn high-quality, neuralnetwork-free predictors often competitive with neural networks. Our worksuggests a flexible paradigm for learning interpretable representationsend-to-end where features and predictions can be readily inspected andunderstood.</description>
      <author>example@mail.com (Gabriel Poesia, Georgia Gabriela Sampaio)</author>
      <guid isPermaLink="false">2510.14825v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
  <item>
      <title>Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning</title>
      <link>http://arxiv.org/abs/2510.14819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PRTraj是一种新颖的轨迹表示学习框架，通过统一环境感知和路线选择建模来有效学习轨迹表示，解决了现有方法将轨迹视为孤立时空序列的局限。&lt;h4&gt;背景&lt;/h4&gt;现有轨迹表示学习方法将轨迹视为孤立的时空序列，忽略了形成轨迹的外部环境和内部路线选择行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够综合考虑外部环境和内部路线选择行为的轨迹表示学习框架，以生成更准确、更有效的轨迹嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;PRTraj框架包含环境感知模块和路线选择编码器：环境感知模块通过捕获周围POI分布的多粒度环境语义增强道路网络；路线选择编码器将轨迹的组成路段转换建模为决策序列来捕获路线选择行为；最后将路线选择感知表示聚合形成全局轨迹嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在3个真实世界数据集的5个下游任务上的广泛实验验证了PRTraj的有效性和泛化能力；PRTraj展现出强大的数据效率，在少样本场景下仍能保持稳健性能。&lt;h4&gt;结论&lt;/h4&gt;PRTraj通过结合环境感知和路线选择建模，显著提升了轨迹表示学习的效果，为各种下游任务提供了更高质量的轨迹嵌入。&lt;h4&gt;翻译&lt;/h4&gt;轨迹表示学习旨在将原始轨迹编码为低维向量，这些向量可在各种下游任务中利用，包括行程时间估计、位置预测和轨迹相似性分析。然而，现有的轨迹表示学习方法存在一个关键疏忽：将轨迹视为孤立的时空序列，而没有考虑支配其形成的外部环境和内部路线选择行为。为了弥合这一差距，我们提出了一种新颖的框架，统一了全面的环境感知和明确的路线选择建模，用于有效的轨迹表示学习，称为PRTraj。具体而言，PRTraj首先引入环境感知模块，通过捕获周围POI分布的多粒度环境语义来增强道路网络。基于这种环境感知骨干网络，路线选择编码器通过将轨迹的组成路段转换建模为决策序列来捕获每条轨迹固有的路线选择行为。这些路线选择感知表示最终被聚合形成全局轨迹嵌入。在3个真实世界数据集的5个下游任务上的广泛实验验证了PRTraj的有效性和泛化能力。此外，PRTraj展现出强大的数据效率，在少样本场景下保持稳健性能。我们的代码可在以下网址获取：https://anonymous.4open.science/r/PRTraj。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory Representation Learning (TRL) aims to encode raw trajectories intolow-dimensional vectors, which can then be leveraged in various downstreamtasks, including travel time estimation, location prediction, and trajectorysimilarity analysis. However, existing TRL methods suffer from a key oversight:treating trajectories as isolated spatio-temporal sequences, withoutconsidering the external environment and internal route choice behavior thatgovern their formation. To bridge this gap, we propose a novel framework thatunifies comprehensive environment \textbf{P}erception and explicit\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representationlearning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces anEnvironment Perception Module to enhance the road network by capturingmulti-granularity environmental semantics from surrounding POI distributions.Building on this environment-aware backbone, a Route Choice Encoder thencaptures the route choice behavior inherent in each trajectory by modeling itsconstituent road segment transitions as a sequence of decisions. Theseroute-choice-aware representations are finally aggregated to form the globaltrajectory embedding. Extensive experiments on 3 real-world datasets across 5downstream tasks validate the effectiveness and generalizability of PRTraj.Moreover, PRTraj demonstrates strong data efficiency, maintaining robustperformance under few-shot scenarios. Our code is available at:https://anonymous.4open.science/r/PRTraj.</description>
      <author>example@mail.com (Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song)</author>
      <guid isPermaLink="false">2510.14819v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval</title>
      <link>http://arxiv.org/abs/2510.14535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,3 figures, 3 tables. Accepted at 2025 IEEE International  Conference on Systems, Man, and Cybernetics (IEEE SMC 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PL-SE-ADA的域调和框架，通过双编码器结构和对抗训练实现医学图像的域调和与可解释表示学习，同时保留与疾病相关的信息。&lt;h4&gt;背景&lt;/h4&gt;医学图像（如磁共振扫描）常因扫描仪和协议差异在不同成像站点间表现出域偏移，降低了机器学习在疾病分类等任务中的性能。现有方法虽能提取域不变和域特定特征，但缺乏医学应用所需的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的域调和框架，实现可解释的表示学习，同时保留脑磁共振图像中与疾病相关的信息。&lt;h4&gt;方法&lt;/h4&gt;提出PL-SE-ADA框架，包含两个编码器分别提取域不变和域特定特征，一个解码器用于重建图像，以及一个域预测器。模型通过对抗训练学习，并通过将域不变和域特定特征的重建求和来重构输入图像。&lt;h4&gt;主要发现&lt;/h4&gt;PL-SE-ADA在图像重建、疾病分类和域识别方面实现了与先前方法相当或更好的性能，同时能够可视化域独立的脑特征和域特定成分，提供了高可解释性。&lt;h4&gt;结论&lt;/h4&gt;PL-SE-ADA是一种有效的域调和框架，不仅提高了医学图像处理任务的性能，还提供了必要的可解释性，解决了医学应用中的实际问题。&lt;h4&gt;翻译&lt;/h4&gt;医学图像如磁共振扫描通常因扫描仪和协议差异在不同成像站点间表现出域偏移，这降低了机器学习在疾病分类等任务中的性能。域调和因此成为关键研究焦点。近期方法将脑图像编码到低维潜在空间并分离为域不变和域特定成分，但往往缺乏医学应用所需的可解释性。我们提出PL-SE-ADA框架，包含两个编码器提取域不变和域特定特征，一个解码器用于重建图像，以及一个域预测器。模型通过对抗训练学习，并通过将域不变和域特定特征的重建求和来重构输入图像，确保调和效果和信息保留。与先前方法相比，PL-SE-ADA在图像重建、疾病分类和域识别方面表现相当或更好，同时提供了高可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical images like MR scans often show domain shifts across imaging sitesdue to scanner and protocol differences, which degrade machine learningperformance in tasks such as disease classification. Domain harmonization isthus a critical research focus. Recent approaches encode brain images$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, thendisentangle it into $\boldsymbol{z_u}$ (domain-invariant) and$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, thesemethods often lack interpretability$-$an essential requirement in medicalapplications$-$leaving practical issues unresolved. We proposePseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), ageneral framework for domain harmonization and interpretable representationlearning that preserves disease-relevant information in brain MR images.PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image$f_D$, and a domain predictor $g_D$. Beyond adversarial training between theencoder and domain predictor, the model learns to reconstruct the input image$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Comparedto prior methods, PL-SE-ADA achieves equal or better performance in imagereconstruction, disease classification, and domain recognition. It also enablesvisualization of both domain-independent brain features and domain-specificcomponents, offering high interpretability across the entire framework.</description>
      <author>example@mail.com (Keima Abe, Hayato Muraki, Shuhei Tomoshige, Kenichi Oishi, Hitoshi Iyatomi)</author>
      <guid isPermaLink="false">2510.14535v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Revisit Modality Imbalance at the Decision Layer</title>
      <link>http://arxiv.org/abs/2510.14411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Some Insights in Balanced Multimodal Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态学习面临模态不平衡问题，这种不平衡不仅存在于表示学习阶段，也在决策层显著存在。研究表明，即使在充分预训练后，模型仍表现出对某些模态的系统偏见，这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态导致。作者建议在决策层引入自适应权重分配机制以实现更平衡的模态融合。&lt;h4&gt;背景&lt;/h4&gt;多模态学习整合不同模态信息以增强模型性能，但常面临模态不平衡问题，即主导模态在联合优化过程中掩盖较弱模态。&lt;h4&gt;目的&lt;/h4&gt;揭示模态不平衡不仅在表示学习阶段存在，也在决策层显著表现，并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;在音频-视觉数据集（CREMAD和Kinetic-Sounds）上进行实验，分析模型在预训练和平衡优化后对模态的偏见，研究特征空间和决策权重分布的差异。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模态不平衡不仅存在于表示学习阶段，也在决策层显著存在；2) 即使在充分预训练和平衡优化后，模型仍表现出对某些模态（如音频）的系统偏见；3) 这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态导致；4) 在融合阶段聚合未校准的模态输出会导致决策层的加权偏差。&lt;h4&gt;结论&lt;/h4&gt;未来的多模态系统应该在决策层更多地纳入自适应权重分配机制，使各模态能够根据其能力实现相对平衡，从而有效利用较弱模态的贡献。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习整合来自不同模态的信息以增强模型性能，但它常常遭受模态不平衡的影响，在联合优化过程中主导模态会掩盖较弱的模态。本文揭示这种不平衡不仅发生在表示学习阶段，而且在决策层也显著表现。在音频-视觉数据集（CREMAD和Kinetic-Sounds）上的实验表明，即使在广泛的预训练和平衡优化后，模型仍然表现出对某些模态（如音频）的系统偏见。进一步分析表明，这种偏见源于特征空间和决策权重分布的内在差异，而不仅仅是优化动态。我们认为，在融合阶段聚合未校准的模态输出会导致决策层的加权偏差，阻碍较弱模态的有效贡献。为此，我们建议未来的多模态系统应该更注重在决策层纳入自适应权重分配机制，使各模态能够根据其能力实现相对平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning integrates information from different modalities toenhance model performance, yet it often suffers from modality imbalance, wheredominant modalities overshadow weaker ones during joint optimization. Thispaper reveals that such an imbalance not only occurs during representationlearning but also manifests significantly at the decision layer. Experiments onaudio-visual datasets (CREMAD and Kinetic-Sounds) show that even afterextensive pretraining and balanced optimization, models still exhibitsystematic bias toward certain modalities, such as audio. Further analysisdemonstrates that this bias originates from intrinsic disparities infeature-space and decision-weight distributions rather than from optimizationdynamics alone. We argue that aggregating uncalibrated modality outputs at thefusion stage leads to biased decision-layer weighting, hindering weakermodalities from contributing effectively. To address this, we propose thatfuture multimodal systems should focus more on incorporate adaptive weightallocation mechanisms at the decision layer, enabling relative balancedaccording to the capabilities of each modality.</description>
      <author>example@mail.com (Xiaoyu Ma, Hao Chen)</author>
      <guid isPermaLink="false">2510.14411v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis</title>
      <link>http://arxiv.org/abs/2510.14403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了DCMIL模型，用于处理全切片图像(WSI)进行癌症预后预测，解决了计算瓶颈和注释稀缺的问题，并在多种癌症类型中表现出色。&lt;h4&gt;背景&lt;/h4&gt;计算病理学新兴学科利用WSI量化形态异性和开发癌症预后模型，但受千兆像素级输入的计算瓶颈和密集手动注释稀缺的阻碍，且当前方法忽视了多倍率WSI中的细粒度信息和肿瘤微环境变异。&lt;h4&gt;目的&lt;/h4&gt;开发一个易于到难的正向表示学习模型(DCMIL)，高效处理WSI用于癌症预后预测，不依赖密集注释，并能直接将千兆像素级WSI转化为结果预测。&lt;h4&gt;方法&lt;/h4&gt;提出名为双课程对比多实例学习(DCMIL)的模型，是一种正向表示学习模型，能高效处理WSI，不需要密集注释，可直接将大型WSI图像转化为预后预测。&lt;h4&gt;主要发现&lt;/h4&gt;在12种癌症类型(5,954名患者，1,254万张图像块)的实验中，DCMIL优于标准WSI预后模型；能识别细粒度预后显著区域；提供稳健实例不确定性估计；捕获正常与肿瘤组织形态差异；有潜力产生新生物学见解。&lt;h4&gt;结论&lt;/h4&gt;DCMIL模型在癌症预后预测方面表现出色，不需要密集注释，能直接处理大型WSI图像，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;蓬勃发展的计算病理学学科显示出利用全切片图像(WSIs)量化形态异质性并为人类癌症开发客观预后模型的希望。然而，千兆像素级输入的计算瓶颈和密集手动注释的稀缺阻碍了进展。当前方法常常忽视了多倍率WSI中的细粒度信息和肿瘤微环境的变异。在这里，我们提出一个易于到难的正向表示学习模型，称为双课程对比多实例学习(DCMIL)，以高效处理WSI用于癌症预后。该模型不依赖于密集注释，并能将千兆像素级WSI直接转化为结果预测。在十二种癌症类型(5,954名患者，1,254万张图像块)的大量实验中证明，DCMIL优于标准的基于WSI的预后模型。此外，DCMIL能识别细粒度的预后显著区域，提供稳健的实例不确定性估计，并捕获正常组织和肿瘤组织之间的形态差异，有潜力产生新的生物学见解。所有代码已在https://github.com/tuuuc/DCMIL上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The burgeoning discipline of computational pathology shows promise inharnessing whole slide images (WSIs) to quantify morphological heterogeneityand develop objective prognostic modes for human cancers. However, progress isimpeded by the computational bottleneck of gigapixel-size inputs and thescarcity of dense manual annotations. Current methods often overlookfine-grained information across multi-magnification WSIs and variations intumor microenvironments. Here, we propose an easy-to-hard progressiverepresentation learning model, termed dual-curriculum contrastivemulti-instance learning (DCMIL), to efficiently process WSIs for cancerprognosis. The model does not rely on dense annotations and enables the directtransformation of gigapixel-size WSIs into outcome predictions. Extensiveexperiments on twelve cancer types (5,954 patients, 12.54 million tiles)demonstrate that DCMIL outperforms standard WSI-based prognostic models.Additionally, DCMIL identifies fine-grained prognosis-salient regions, providesrobust instance uncertainty estimation, and captures morphological differencesbetween normal and tumor tissues, with the potential to generate new biologicalinsights. All codes have been made publicly accessible athttps://github.com/tuuuc/DCMIL.</description>
      <author>example@mail.com (Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning)</author>
      <guid isPermaLink="false">2510.14403v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection</title>
      <link>http://arxiv.org/abs/2510.14344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了BINCTX，一种多模态学习方法，用于检测移动应用中的不良行为，通过结合代码级语义、行为触发方式和第三方库使用信息，实现了高准确率的检测。&lt;h4&gt;背景&lt;/h4&gt;移动应用市场有数百万个应用，但不良行为（如干扰性广告、非法重定向、支付欺诈）难以被发现，因为这些行为通常不依赖权限保护的API，且可通过UI或元数据编辑轻易伪装。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测移动应用中不良行为的机器学习方法，提高检测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;BINCTX构建应用的三种视图：全局字节码图像视图（捕获代码级语义和家族模式）、上下文视图（显示行为触发方式）和第三方库使用视图（总结组件间调用路径上的调用频率），然后将这三种视图嵌入并融合，训练上下文感知分类器。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界恶意软件和良性应用上，BINCTX达到94.73%的宏观F1值，比强大基线方法至少高出14.92%；在商业混淆下保持84%的F1值；比最先进的仅字节码系统更能抵抗对抗样本。&lt;h4&gt;结论&lt;/h4&gt;BINCTX通过多模态表示学习，有效结合了代码级语义、行为上下文和第三方库使用信息，显著提高了移动应用不良行为的检测性能，并增强了对混淆技术和对抗攻击的抵抗力。&lt;h4&gt;翻译&lt;/h4&gt;移动应用市场托管着数百万个应用，但不良行为（例如干扰性广告、非法重定向、支付欺诈）仍然难以被发现，因为它们通常不依赖于权限保护的API，并且可以通过UI或元数据编辑轻松伪装。我们提出了BINCTX，一种学习方法，它从(i)全局字节码图像视图捕获代码级语义和家族模式，(ii)上下文视图（显示的操作、组件、声明的权限、URL/IP常量）指示行为如何被触发，以及(iii)第三方库使用视图总结组件间调用路径上的调用频率，构建应用的多模态表示。这三个视图被嵌入并融合，训练一个上下文感知分类器。在真实世界的恶意软件和良性应用上，BINCTX实现了94.73%的宏观F1值，比强大的基线方法至少高出14.92%。它在商业混淆下保持鲁棒性（混淆后F1为84%），并且比最先进的仅字节码系统更能抵抗对抗样本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile app markets host millions of apps, yet undesired behaviors (e.g.,disruptive ads, illegal redirection, payment deception) remain hard to catchbecause they often do not rely on permission-protected APIs and can be easilycamouflaged via UI or metadata edits. We present BINCTX, a learning approachthat builds multi-modal representations of an app from (i) a globalbytecode-as-image view that captures code-level semantics and family-stylepatterns, (ii) a contextual view (manifested actions, components, declaredpermissions, URL/IP constants) indicating how behaviors are triggered, and(iii) a third-party-library usage view summarizing invocation frequencies alonginter-component call paths. The three views are embedded and fused to train acontextual-aware classifier. On real-world malware and benign apps, BINCTXattains a macro F1 of 94.73%, outperforming strong baselines by at least14.92%. It remains robust under commercial obfuscation (F1 84%post-obfuscation) and is more resistant to adversarial samples thanstate-of-the-art bytecode-only systems.</description>
      <author>example@mail.com (Zichen Liu, Shao Yang, Xusheng Xiao)</author>
      <guid isPermaLink="false">2510.14344v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm</title>
      <link>http://arxiv.org/abs/2510.14321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为大型推理嵌入模型(LREM)的新方法，通过将推理过程整合到表示学习中，解决了电子商务搜索系统中困难查询的语义匹配问题，显著提高了检索准确性。&lt;h4&gt;背景&lt;/h4&gt;在现代电子商务搜索系统中，密集检索是重要组成部分。主流嵌入模型已从BERT转向大型语言模型(LLMs)，但仍采用直接嵌入方法，语义准确性不足。对比学习虽被使用，但模型倾向于捕获统计共现模式，偏向浅层词汇和语义匹配，导致对困难查询的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出大型推理嵌入模型(LREM)，创新地将推理过程整合到表示学习中，以解决困难查询与目标物品之间的语义匹配问题，提高检索准确性。&lt;h4&gt;方法&lt;/h4&gt;LREM对困难查询先进行推理以深入理解查询，然后生成推理增强的查询嵌入用于检索。采用两阶段训练：第一阶段在Query-CoT-Item三元组上使用SFT和InfoNCE损失优化LLM；第二阶段通过强化学习进一步优化推理轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;推理过程有效桥接了原始查询和目标物品间的语义差距，显著提高了检索准确性。大量离线和在线实验验证了LREM的有效性。&lt;h4&gt;结论&lt;/h4&gt;LREM已被成功部署在中国最大的电子商务平台上，自2025年8月起，证明了其在实际应用中的价值。&lt;h4&gt;翻译&lt;/h4&gt;在现代电子商务搜索系统中，密集检索已成为不可或缺的组成部分。通过计算查询和物品(产品)嵌入之间的相似性，它能够从大规模存储库中高效地选择候选产品。随着大型语言模型(LLMs)的突破，主流嵌入模型已逐渐从BERT转向LLMs以实现更准确的文本建模。然而，这些模型仍采用直接嵌入方法，嵌入的语义准确性仍然不足。因此，对比学习被大量使用来实现正对之间的紧密语义对齐。结果，这些模型倾向于捕获训练数据中的统计共现模式，偏向于浅层词汇和语义匹配。对于与目标物品存在明显词汇差异的困难查询，性能显著下降。在这项工作中，我们提出了大型推理嵌入模型(LREM)，创新地将推理过程整合到表示学习中。对于困难查询，LREM首先进行推理以实现对原始查询的深入理解，然后生成推理增强的查询嵌入用于检索。这一推理过程有效地桥接了原始查询和目标物品之间的语义差距，显著提高了检索准确性。具体而言，我们采用两阶段训练过程：第一阶段在精心策划的查询-思维链-物品(Query-CoT-Item)三元组上使用SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习(RL)进一步优化推理轨迹。大量的离线和在线实验验证了LREM的有效性，使其自2025年8月起被部署在中国最大的电子商务平台上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern e-commerce search systems, dense retrieval has become anindispensable component. By computing similarities between query and item(product) embeddings, it efficiently selects candidate products fromlarge-scale repositories. With the breakthroughs in large language models(LLMs), mainstream embedding models have gradually shifted from BERT to LLMsfor more accurate text modeling. However, these models still adoptdirect-embedding methods, and the semantic accuracy of embeddings remainsinadequate. Therefore, contrastive learning is heavily employed to achievetight semantic alignment between positive pairs. Consequently, such models tendto capture statistical co-occurrence patterns in the training data, biasingthem toward shallow lexical and semantic matches. For difficult queriesexhibiting notable lexical disparity from target items, the performancedegrades significantly. In this work, we propose the Large Reasoning EmbeddingModel (LREM), which novelly integrates reasoning processes into representationlearning. For difficult queries, LREM first conducts reasoning to achieve adeep understanding of the original query, and then produces areasoning-augmented query embedding for retrieval. This reasoning processeffectively bridges the semantic gap between original queries and target items,significantly improving retrieval accuracy. Specifically, we adopt a two-stagetraining process: the first stage optimizes the LLM on carefully curatedQuery-CoT-Item triplets with SFT and InfoNCE losses to establish preliminaryreasoning and embedding capabilities, and the second stage further refines thereasoning trajectories via reinforcement learning (RL). Extensive offline andonline experiments validate the effectiveness of LREM, leading to itsdeployment on China's largest e-commerce platform since August 2025.</description>
      <author>example@mail.com (Jianting Tang, Dongshuai Li, Tao Wen, Fuyu Lv, Dan Ou, Linli Xu)</author>
      <guid isPermaLink="false">2510.14321v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks</title>
      <link>http://arxiv.org/abs/2510.14139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review in Frontiers in Bioinformatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;蛋白质-蛋白质相互作用的准确预测对于理解细胞功能和推进药物开发至关重要。现有的计算方法使用蛋白质语言模型(PLMs)的直接序列嵌入，或使用图神经网络(GNNs)处理3D蛋白质结构。本研究探索计算密集度较低的替代方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架用于下游PPI预测，通过链接预测实现。&lt;h4&gt;方法&lt;/h4&gt;引入一个两阶段的图表示学习框架ProtGram-DirectGCN。第一阶段开发ProtGram，将蛋白质的一级结构建模为全局推断的n-gram图层次结构，其中残基转移概率定义边权重。第二阶段提出DirectGCN，一种定制的有向图卷积神经网络，通过入向、出向和无向路径的转换处理信息，并通过可学习的门控机制结合这些路径。&lt;h4&gt;主要发现&lt;/h4&gt;DirectGCN在标准节点分类基准上表现良好，性能与已建立的方法相当，尤其在具有密集、异质结构的有向复杂图中表现出色。完整的ProtGram-DirectGCN框架应用于PPI预测时提供了强大的预测能力，即使在有限的训练数据下也能保持。&lt;h4&gt;结论&lt;/h4&gt;ProtGram-DirectGCN框架是一种有效的PPI预测方法，在计算资源有限的情况下也能保持良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;引言：准确预测蛋白质相互作用对于理解细胞功能和推进药物开发至关重要。现有的计算方法使用蛋白质语言模型的直接序列嵌入，或使用图神经网络处理3D蛋白质结构。本研究探索计算密集度较低的替代方法。我们引入了一种通过链接预测进行下游PPI预测的新框架。方法：我们引入了一个两阶段的图表示学习框架ProtGram-DirectGCN。首先，我们开发了ProtGram，该方法将蛋白质的一级结构建模为全局推断的n-gram图层次结构。在这些图中，残基转移概率定义边权重，每条边在有向图中连接一对残基，这些概率从大量序列集合中聚合。其次，我们提出了DirectGCN，一种定制的有向图卷积神经网络，该模型具有独特的卷积层，通过入向、出向和无向的特定路径转换处理信息，同时应用共享转换，这些路径通过可学习的门控机制结合。我们将DirectGCN应用于ProtGram图以学习残基级嵌入，并通过注意力池化生成蛋白质级嵌入进行预测。结果：我们首先在标准节点分类基准上建立了DirectGCN的有效性，其在一般数据集上的性能与已建立的方法相当，该模型在具有密集、异质结构的有向复杂图中表现出色。当应用于PPI预测时，完整的ProtGram-DirectGCN框架提供了强大的预测能力，即使在有限的训练数据下，这种强大的性能仍然保持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3389/fbinf.2025.1651623&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction Accurate prediction of protein-protein interactions (PPIs) iscrucial for understanding cellular functions and advancing drug development.Existing in-silico methods use direct sequence embeddings from Protein LanguageModels (PLMs). Others use Graph Neural Networks (GNNs) for 3D proteinstructures. This study explores less computationally intensive alternatives. Weintroduce a novel framework for downstream PPI prediction through linkprediction. Methods We introduce a two-stage graph representation learningframework, ProtGram-DirectGCN. First, we developed ProtGram. This approachmodels a protein's primary structure as a hierarchy of globally inferred n-gramgraphs. In these graphs, residue transition probabilities define edge weights.Each edge connects a pair of residues in a directed graph. The probabilitiesare aggregated from a large corpus of sequences. Second, we propose DirectGCN,a custom directed graph convolutional neural network. This model features aunique convolutional layer. It processes information through separatepath-specific transformations: incoming, outgoing, and undirected. A sharedtransformation is also applied. These paths are combined via a learnable gatingmechanism. We apply DirectGCN to ProtGram graphs to learn residue-levelembeddings. These embeddings are pooled via attention to generate protein-levelembeddings for prediction. Results We first established the efficacy ofDirectGCN on standard node classification benchmarks. Its performance matchesestablished methods on general datasets. The model excels at complex, directedgraphs with dense, heterophilic structures. When applied to PPI prediction, thefull ProtGram-DirectGCN framework delivers robust predictive power. This strongperformance holds even with limited training data.</description>
      <author>example@mail.com (Islam Akef Ebeid, Haoteng Tang, Pengfei Gu)</author>
      <guid isPermaLink="false">2510.14139v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title>
      <link>http://arxiv.org/abs/2510.14112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为STEMS的新型安全约束多智能体强化学习框架，用于协调建筑能源管理，有效解决了多建筑系统中时空依赖关系利用和操作安全性的挑战。&lt;h4&gt;背景&lt;/h4&gt;建筑能源管理对于实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。当前多建筑能源系统面临三个关键挑战：时空信息利用不足、缺乏严格的安全保证以及系统复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的安全约束多智能体强化学习框架，解决多建筑协调能源管理中的挑战，特别是在利用时空依赖关系和确保操作安全方面。&lt;h4&gt;方法&lt;/h4&gt;STEMS框架整合了两个核心组件：(1)时空图表示学习框架，使用GCN-Transformer融合架构捕捉建筑间关系和时间模式；(2)安全约束多智能体RL算法，结合控制屏障函数提供数学安全保证。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明STEMS相比现有方法具有优越性能，实现了21%的成本降低，18%的排放减少，将安全违规从35.1%大幅降低到5.6%，并保持最优舒适度，仅有0.13%的不舒适比例。该框架在极端天气条件下表现出强大的鲁棒性，并在不同类型建筑中保持有效性。&lt;h4&gt;结论&lt;/h4&gt;STEMS框架成功解决了多建筑能源管理中的关键挑战，通过整合时空信息利用和安全约束，实现了显著的能源成本降低和排放减少，同时确保了系统安全和居住者舒适度。&lt;h4&gt;翻译&lt;/h4&gt;建筑能源管理对于实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。协调建筑能源管理在利用时空依赖关系的同时确保多建筑系统运行安全方面面临关键挑战。当前多建筑能源系统面临三个关键挑战：时空信息利用不足、缺乏严格的安全保证以及系统复杂性。本文提出STEMS，一种用于协调建筑能源管理的新型安全约束多智能体强化学习框架。STEMS整合了两个核心组件：(1)使用GCN-Transformer融合架构的时空图表示学习框架，用于捕捉建筑间关系和时间模式；(2)结合控制屏障函数的安全约束多智能体RL算法，提供数学安全保证。在真实建筑数据集上的大量实验表明STEMS相比现有方法具有优越性能，实现了21%的成本降低，18%的排放减少，同时将安全违规从35.1%大幅降低到5.6%，并保持最优舒适度，仅有0.13%的不舒适比例。该框架在极端天气条件下也表现出强大的鲁棒性，并且在不同类型建筑中保持有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building energy management is essential for achieving carbon reduction goals,improving occupant comfort, and reducing energy costs. Coordinated buildingenergy management faces critical challenges in exploiting spatial-temporaldependencies while ensuring operational safety across multi-building systems.Current multi-building energy systems face three key challenges: insufficientspatial-temporal information exploitation, lack of rigorous safety guarantees,and system complexity. This paper proposes Spatial-Temporal Enhanced SafeMulti-Agent Coordination (STEMS), a novel safety-constrained multi-agentreinforcement learning framework for coordinated building energy management.STEMS integrates two core components: (1) a spatial-temporal graphrepresentation learning framework using a GCN-Transformer fusion architectureto capture inter-building relationships and temporal patterns, and (2) asafety-constrained multi-agent RL algorithm incorporating Control BarrierFunctions to provide mathematical safety guarantees. Extensive experiments onreal-world building datasets demonstrate STEMS's superior performance overexisting methods, showing that STEMS achieves 21% cost reduction, 18% emissionreduction, and dramatically reduces safety violations from 35.1% to 5.6% whilemaintaining optimal comfort with only 0.13 discomfort proportion. The frameworkalso demonstrates strong robustness during extreme weather conditions andmaintains effectiveness across different building types.</description>
      <author>example@mail.com (Huiliang Zhang, Di Wu, Arnaud Zinflou, Benoit Boulet)</author>
      <guid isPermaLink="false">2510.14112v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations</title>
      <link>http://arxiv.org/abs/2510.14049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了一个新的因果表征学习(CRL)基准，使用高保真模拟视觉数据，既保留真实视觉复杂性又能访问真实因果生成过程，包含约20万张图像和300万视频帧，涵盖四个领域的24个子场景。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习旨在揭示数据生成过程并识别潜在因果变量和关系，但评估具有挑战性，因为需要已知的真实因果变量和结构。现有评估方法要么依赖简化合成数据集，要么依赖现实世界任务中的下游性能，在真实性和评估精度间面临两难困境。&lt;h4&gt;目的&lt;/h4&gt;创建一个既保留真实视觉复杂性又能访问真实因果生成过程的新CRL基准，解决现有评估方法在真实性和评估精度之间的两难困境。&lt;h4&gt;方法&lt;/h4&gt;构建包含约20万张图像和300万视频帧的数据集，涵盖静态图像生成、动态物理模拟、机器人操作和交通情况分析四个领域的24个子场景，提供对底层因果结构的灵活访问，允许用户修改或配置以符合CRL假设要求。&lt;h4&gt;主要发现&lt;/h4&gt;利用此基准评估了不同范式的代表性CRL方法，提供了实证见解，帮助实践者和新手选择或扩展适当的CRL框架，以解决可以从CRL视角受益的现实问题。&lt;h4&gt;结论&lt;/h4&gt;该基准有望弥合严格评估和实际应用之间的差距，为CRL研究提供更全面、更真实的测试平台。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已为中文，无需额外翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal Representation Learning (CRL) aims to uncover the data-generatingprocess and identify the underlying causal variables and relations, whoseevaluation remains inherently challenging due to the requirement of knownground-truth causal variables and causal structure. Existing evaluations oftenrely on either simplistic synthetic datasets or downstream performance onreal-world tasks, generally suffering a dilemma between realism and evaluativeprecision. In this paper, we introduce a new benchmark for CRL usinghigh-fidelity simulated visual data that retains both realistic visualcomplexity and, more importantly, access to ground-truth causal generatingprocesses. The dataset comprises around 200 thousand images and 3 million videoframes across 24 sub-scenes in four domains: static image generation, dynamicphysical simulations, robotic manipulations, and traffic situation analysis.These scenarios range from static to dynamic settings, simple to complexstructures, and single to multi-agent interactions, offering a comprehensivetestbed that hopefully bridges the gap between rigorous evaluation andreal-world applicability. In addition, we provide flexible access to theunderlying causal structures, allowing users to modify or configure them toalign with the required assumptions in CRL, such as available domain labels,temporal dependencies, or intervention histories. Leveraging this benchmark, weevaluated representative CRL methods across diverse paradigms and offeredempirical insights to assist practitioners and newcomers in choosing orextending appropriate CRL frameworks to properly address specific types of realproblems that can benefit from the CRL perspective. Welcome to visit our:Project page:https://causal-verse.github.io/,Dataset:https://huggingface.co/CausalVerse.</description>
      <author>example@mail.com (Guangyi Chen, Yunlong Deng, Peiyuan Zhu, Yan Li, Yifan Sheng, Zijian Li, Kun Zhang)</author>
      <guid isPermaLink="false">2510.14049v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2510.14470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了结合语言模型的图基础模型在文本属性图上的安全漏洞，特别是后门攻击问题，并提出了一种双触发攻击框架。&lt;h4&gt;背景&lt;/h4&gt;图基础模型，特别是结合语言模型的模型，已革新图学习并在文本属性图上表现出色，但相比传统GNN引入了新的安全漏洞。&lt;h4&gt;目的&lt;/h4&gt;解决LM赋能的GFMs在无安全提示调整阶段的安全漏洞问题，特别是在属性不可访问的约束TAG系统中的后门攻击挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的双触发后门攻击框架，在文本层面和结构层面同时运作，通过利用预先建立的文本池实现无需显式优化触发节点文本属性的有效攻击。&lt;h4&gt;主要发现&lt;/h4&gt;传统图后门攻击在属性不可访问的约束TAG系统中性能显著下降；所提双触发攻击框架能保持优越的干净准确率并取得出色的攻击成功率。&lt;h4&gt;结论&lt;/h4&gt;LM赋能的GFMs在网络部署中存在关键后门风险，研究为基础模型时代开源平台开发更强大的监督机制提供了贡献。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型的出现，特别是那些结合语言模型的模型，已经革新了图学习并在文本属性图上表现出色。然而，与传统GNN相比，这些由语言模型赋能的图基础模型在无安全提示调整阶段引入了独特的安全漏洞，这些漏洞在当前研究中尚未得到充分研究。通过实证研究，我们发现在属性不可访问的约束文本属性图系统中，当没有显式优化触发节点属性时，传统图后门攻击的性能会显著下降。为此，我们提出了一种新的双触发后门攻击框架，在文本层面和结构层面同时运作，通过战略性地利用预先建立的文本池，无需显式优化触发节点文本属性即可实现有效攻击。大量实验评估表明，我们的攻击方法在保持优越的干净准确率的同时，取得了出色的攻击成功率，包括在高度隐蔽的单触发节点场景中。我们的工作强调了在网络上部署的由语言模型赋能的图基础模型中的关键后门风险，并为基础模型时代开源平台开发更强大的监督机制做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of graph foundation models (GFMs), particularly thoseincorporating language models (LMs), has revolutionized graph learning anddemonstrated remarkable performance on text-attributed graphs (TAGs). However,compared to traditional GNNs, these LM-empowered GFMs introduce unique securityvulnerabilities during the unsecured prompt tuning phase that remainunderstudied in current research. Through empirical investigation, we reveal asignificant performance degradation in traditional graph backdoor attacks whenoperating in attribute-inaccessible constrained TAG systems without explicittrigger node attribute optimization. To address this, we propose a noveldual-trigger backdoor attack framework that operates at both text-level andstruct-level, enabling effective attacks without explicit optimization oftrigger node text attributes through the strategic utilization of apre-established text pool. Extensive experimental evaluations demonstrate thatour attack maintains superior clean accuracy while achieving outstanding attacksuccess rates, including scenarios with highly concealed single-trigger nodes.Our work highlights critical backdoor risks in web-deployed LM-empowered GFMsand contributes to the development of more robust supervision mechanisms foropen-source platforms in the era of foundation models.</description>
      <author>example@mail.com (Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, Kai Zhou)</author>
      <guid isPermaLink="false">2510.14470v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis</title>
      <link>http://arxiv.org/abs/2510.14336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的图变换器架构DARTS-GT，通过不对称注意力和可微分架构搜索实现深度异质性，并开发了首个图变换器的定量可解释性框架。实验表明该方法在多个数据集上达到最先进水平，且发现的异构架构比基线更可解释，证明图变换器无需在性能和可解释性间做出取舍。&lt;h4&gt;背景&lt;/h4&gt;图变换器(GTs)是处理图结构数据的有力架构，但受限于刚性设计和缺乏可量化可解释性。当前最先进的GT在所有层中固定使用相同的GNN类型，错过了深度特定组件选择的优势，且复杂架构变得不透明，无法区分性能提升中的有意义模式和虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;重新设计GT注意力机制通过不对称性解耦结构编码与特征表示；使用DARTS在每层选择最优GNN算子；开发首个GT的定量可解释性框架；探索GT是否需要在性能和可解释性之间做出选择。&lt;h4&gt;方法&lt;/h4&gt;重新设计GT注意力：查询来自节点特征，键和值来自GNN变换；使用DARTS在transformer注意力内部实现深度异质性(DARTS-GT)；通过因果消融开发GT的定量可解释性框架；提出Head-deviation、Specialization和Focus指标；在8个基准数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;DARTS-GT在4个数据集上达到最先进水平，在其他数据集上保持竞争力；发现的架构揭示了数据集特定模式；可视化注意力和因果重要性并不总是相关，表明常用可视化方法可能忽略真正重要的组件；DARTS-GT发现的异构架构比基线产生更可解释的模型。&lt;h4&gt;结论&lt;/h4&gt;Graph Transformers不需要在性能和可解释性之间做出选择。异构架构可以同时提高性能和可解释性，证明性能和可解释性并非相互排斥的目标。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(GTs)已成为处理图结构数据的有力架构，但仍受限于刚性设计且缺乏可量化可解释性。当前最先进的GT在所有层中固定使用相同的GNN类型，错过了深度特定组件选择的优势，同时其复杂架构变得不透明，无法区分性能提升中的有意义模式和虚假相关性。我们通过不对称性重新设计GT注意力，解耦结构编码与特征表示：查询来自节点特征，而键和值来自GNN变换。在此框架内，我们使用可微分架构搜索(DARTS)在每层选择最优GNN算子，在transformer注意力内部实现深度异质性(DARTS-GT)。为了理解发现的架构，我们通过因果消融开发了首个GT的定量可解释性框架。我们的指标(Head-deviation、Specialization和Focus)识别出哪些头和节点驱动预测，同时实现模型比较。在八个基准数据集上的实验显示，DARTS-GT在四个数据集上达到最先进水平，在其他数据集上保持竞争力，且发现的架构揭示了数据集特定模式。我们的可解释性分析表明，可视化注意力和因果重要性并不总是相关，表明广泛使用的可视化方法可能忽略实际重要的组件。重要的是，DARTS-GT发现的异构架构始终比基线产生更可解释的模型，证明图变换器无需在性能和可解释性之间做出选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have emerged as powerful architectures forgraph-structured data, yet remain constrained by rigid designs and lackquantifiable interpretability. Current state-of-the-art GTs commit to fixed GNNtypes across all layers, missing potential benefits of depth-specific componentselection, while their complex architectures become opaque where performancegains cannot be distinguished between meaningful patterns and spuriouscorrelations. We redesign GT attention through asymmetry, decoupling structuralencoding from feature representation: queries derive from node features whilekeys and values come from GNN transformations. Within this framework, we useDifferentiable ARchiTecture Search (DARTS) to select optimal GNN operators ateach layer, enabling depth-wise heterogeneity inside transformer attentionitself (DARTS-GT). To understand discovered architectures, we develop the firstquantitative interpretability framework for GTs through causal ablation. Ourmetrics (Head-deviation, Specialization, and Focus), identify which heads andnodes drive predictions while enabling model comparison. Experiments acrosseight benchmarks show DARTS-GT achieves state-of-the-art on four datasets whileremaining competitive on others, with discovered architectures revealingdataset-specific patterns. Our interpretability analysis reveals that visualattention salience and causal importance do not always correlate, indicatingwidely used visualization approaches may miss components that actually matter.Crucially, heterogeneous architectures found by DARTS-GT consistently producedmore interpretable models than baselines, establishing that Graph Transformersneed not choose between performance and interpretability.</description>
      <author>example@mail.com (Shruti Sarika Chakraborty, Peter Minary)</author>
      <guid isPermaLink="false">2510.14336v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network</title>
      <link>http://arxiv.org/abs/2510.14243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submited to IEEE journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为空间计算通信(SCC)的框架，用于解决多用户沉浸式VR应用在分布式MEC网络中的延迟和能源效率问题。通过MO-CMPO算法，结合监督学习和强化学习，实现了帕累托最优的资源部署方案。&lt;h4&gt;背景&lt;/h4&gt;沉浸式VR应用对延迟、能源效率和计算资源有严格要求，特别是在多用户交互场景中。现有的分布式移动边缘计算(MEC)网络难以满足这些需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来满足多用户VR在分布式MEC网络上的延迟和能源需求，并实现资源的高效部署。&lt;h4&gt;方法&lt;/h4&gt;提出空间计算通信(SCC)框架，将资源部署任务表述为多目标组合优化问题，并设计MO-CMPO算法，结合监督学习和强化学习，利用稀疏图神经网络生成帕累托最优解。&lt;h4&gt;主要发现&lt;/h4&gt;MO-CMPO比基线方法实现了更好的超体积性能和显著更低的推理延迟。以延迟为导向的解决方案倾向于本地MEC执行，而以能源为导向的解决方案则最小化冗余部署。&lt;h4&gt;结论&lt;/h4&gt;SCC框架和MO-CMPO算法能够有效解决多用户VR应用在分布式MEC网络中的资源部署问题，平衡延迟和能源消耗。&lt;h4&gt;翻译&lt;/h4&gt;沉浸式虚拟现实(VR)应用对延迟、能源效率和计算资源有严格要求，特别是在多用户交互场景中。为应对这些挑战，我们引入了空间计算通信(SCC)的概念，这是一个旨在满足分布式移动边缘计算(MEC)网络上多用户VR延迟和能源需求的框架。SCC使用用户动态和资源需求的概率模型，联合表示由用户和基站定义的物理空间，以及代表共享沉浸式环境的虚拟空间。然后，资源部署任务被表述为多目标组合优化(MOCO)问题，同时最小化分布式MEC资源上的系统延迟和能源消耗。为解决这个问题，我们提出了MO-CMPO，这是一种基于策略优化的多目标一致性模型，集成了监督学习和由偏好权重引导的强化学习(RL)微调。利用稀疏图神经网络(GNN)，MO-CMPO有效生成帕累托最优解。使用真实的新无线电基站数据集进行的模拟表明，MO-CMPO比基线方法实现了更好的超体积性能和显著更低的推理延迟。此外，分析揭示了实际的部署模式：以延迟为导向的解决方案倾向于本地MEC执行以减少传输延迟，而以能源为导向的解决方案则最小化冗余部署以节省能源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Immersive virtual reality (VR) applications impose stringent requirements onlatency, energy efficiency, and computational resources, particularly inmulti-user interactive scenarios. To address these challenges, we introduce theconcept of spatial computing communications (SCC), a framework designed to meetthe latency and energy demands of multi-user VR over distributed mobile edgecomputing (MEC) networks. SCC jointly represents the physical space, defined byusers and base stations, and the virtual space, representing shared immersiveenvironments, using a probabilistic model of user dynamics and resourcerequirements. The resource deployment task is then formulated as amulti-objective combinatorial optimization (MOCO) problem that simultaneouslyminimizes system latency and energy consumption across distributed MECresources. To solve this problem, we propose MO-CMPO, a multi-objectiveconsistency model with policy optimization that integrates supervised learningand reinforcement learning (RL) fine-tuning guided by preference weights.Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generatesPareto-optimal solutions. Simulations with real-world New Radio base stationdatasets demonstrate that MO-CMPO achieves superior hypervolume performance andsignificantly lower inference latency than baseline methods. Furthermore, theanalysis reveals practical deployment patterns: latency-oriented solutionsfavor local MEC execution to reduce transmission delay, while energy-orientedsolutions minimize redundant placements to save energy.</description>
      <author>example@mail.com (Caolu Xu, Zhiyong Chen, Meixia Tao, Li Song, Wenjun Zhang)</author>
      <guid isPermaLink="false">2510.14243v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks</title>
      <link>http://arxiv.org/abs/2510.14137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了解耦图卷积网络(D-GCN)来解决异构多跳无线网络中吞吐量预测的挑战。D-GCN通过分离节点自身传输概率与邻居干扰效应，使用可学习注意力替代平均聚合，实现了更准确的预测和可解释性，实验表明其显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;p持续CSMA协议是随机接入MAC分析的核心，但在异构多跳无线网络中预测饱和吞吐量仍是一个难题。简化的单一共享干扰域模型会低估吞吐量48-62%，而精确的马尔可夫链分析计算复杂度高，对大型网络不实用。&lt;h4&gt;目的&lt;/h4&gt;开发可扩展的吞吐量预测方法，解决异构多跳无线网络中的计算障碍，适用于一般网络拓扑的结构化机器学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出解耦图卷积网络(D-GCN)，一种新型架构，明确分离节点自身的传输概率与邻居干扰效应的处理。用可学习的注意力替代平均聚合，产生可解释的每邻居贡献权重，同时捕获复杂的多跳干扰模式。&lt;h4&gt;主要发现&lt;/h4&gt;D-GCN实现了3.3%的归一化平均绝对误差(NMAE)，显著优于标准GCN的63.94% NMAE。D-GCN性能优于强基线方法，即使在精确分析方法计算上不可行的情况下仍然可扩展，且使基于梯度的网络优化达到理论最优值的1%以内。&lt;h4&gt;结论&lt;/h4&gt;D-GCN通过解耦处理和注意力机制，能够更准确地捕获网络中的复杂干扰模式，有效解决了异构多跳无线网络吞吐量预测问题。&lt;h4&gt;翻译&lt;/h4&gt;p持续CSMA协议、饱和吞吐量、异构多跳无线网络、干扰域、马尔可夫链分析、结构化机器学习、图卷积网络(GNNs)、图卷积网络(GCN)、归一化平均绝对误差(NMAE)、对称归一化、级联效应、解耦图卷积网络(D-GCN)、可学习注意力、多跳干扰模式、基于梯度的网络优化&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The p-persistent CSMA protocol is central to random-access MAC analysis, butpredicting saturation throughput in heterogeneous multi-hop wireless networksremains a hard problem. Simplified models that assume a single, sharedinterference domain can underestimate throughput by 48--62\% in sparsetopologies. Exact Markov-chain analyses are accurate but scale exponentially incomputation time, making them impractical for large networks. Thesecomputational barriers motivate structural machine learning approaches likeGNNs for scalable throughput prediction in general network topologies. Yetoff-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized meanabsolute error (NMAE) on heterogeneous networks because symmetric normalizationconflates a node's direct interference with higher-order, cascading effectsthat pertain to how interference propagates over the network graph.  Building on these insights, we propose the Decoupled Graph ConvolutionalNetwork (D-GCN), a novel architecture that explicitly separates processing of anode's own transmission probability from neighbor interference effects. D-GCNreplaces mean aggregation with learnable attention, yielding interpretable,per-neighbor contribution weights while capturing complex multihop interferencepatterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remainstractable even when exact analytical methods become computationally infeasible,and enables gradient-based network optimization that achieves within 1\% oftheoretical optima.</description>
      <author>example@mail.com (Faezeh Dehghan Tarzjani, Bhaskar Krishnamachari)</author>
      <guid isPermaLink="false">2510.14137v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>On the expressivity of sparse maxout networks</title>
      <link>http://arxiv.org/abs/2510.14068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了稀疏maxout网络的表达能力，建立了这类网络与虚拟多面体之间的对偶关系，分析了网络深度和宽度对表达能力的影响。&lt;h4&gt;背景&lt;/h4&gt;研究聚焦于稀疏maxout网络，其中每个神经元从前一层接收固定数量的输入并采用maxout激活函数，这种结构类似于卷积神经网络或图神经网络的关键特征。&lt;h4&gt;目的&lt;/h4&gt;目的是理解稀疏maxout网络的表达能力，特别是网络深度、宽度和稀疏性如何影响其计算能力。&lt;h4&gt;方法&lt;/h4&gt;通过建立稀疏maxout网络可计算函数与虚拟多面体之间的对偶关系，推导出相关多面体维度的紧界，并基于此构建深度层次结构序列。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现足够深的稀疏maxout网络具有通用性，但如果未达到所需深度，仅靠宽度无法弥补固定入度约束的稀疏性。&lt;h4&gt;结论&lt;/h4&gt;稀疏maxout网络的表达能力不仅取决于宽度，还与深度密切相关，深度不足时宽度无法完全补偿稀疏性的限制。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了稀疏maxout网络的表达能力，其中每个神经元从前一层接收固定数量的输入，并采用可能有多参数的maxout激活函数。这种设置捕捉了卷积神经网络或图神经网络的关键特征。我们建立了此类网络可计算的函数与一类虚拟多面体之间的对偶关系，将它们的几何形状与网络表达能力的问题联系起来。特别是，我们推导出相关多面体维度的紧界，作为我们分析的中心工具。在此基础上，我们构建了一个深度层次结构序列。虽然足够深的稀疏maxout网络是通用的，但我们证明，如果未达到所需深度，仅靠宽度无法弥补固定入度约束的稀疏性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the expressivity of sparse maxout networks, where each neuron takesa fixed number of inputs from the previous layer and employs a, possiblymulti-argument, maxout activation. This setting captures key characteristics ofconvolutional or graph neural networks. We establish a duality betweenfunctions computable by such networks and a class of virtual polytopes, linkingtheir geometry to questions of network expressivity. In particular, we derive atight bound on the dimension of the associated polytopes, which serves as thecentral tool for our analysis. Building on this, we construct a sequence ofdepth hierarchies. While sufficiently deep sparse maxout networks areuniversal, we prove that if the required depth is not reached, width alonecannot compensate for the sparsity of a fixed indegree constraint.</description>
      <author>example@mail.com (Moritz Grillo, Tobias Hofmann)</author>
      <guid isPermaLink="false">2510.14068v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations</title>
      <link>http://arxiv.org/abs/2510.14035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages content. 2 pages references&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GammaZero是一种以动作为中心的图表示框架，用于在部分可观察马尔可夫决策过程(POMDPs)中指导规划学习，解决了现有方法在可扩展性和泛化能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有方法需要特定领域的神经网络架构，并且难以处理大规模问题，限制了在POMDPs中的规划学习能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的图表示框架，使学习到的策略能够在不同规模的问题间泛化，并减少对领域特定架构的需求。&lt;h4&gt;方法&lt;/h4&gt;GammaZero将信念状态转换为以动作为中心的图，使用图神经网络结合解码器架构从专家演示中学习价值函数和策略，然后应用这些启发式指导蒙特卡洛树搜索。&lt;h4&gt;主要发现&lt;/h4&gt;在相同规模问题上，GammaZero性能与BetaZero相当；同时能够实现零样本泛化，处理比训练时所见大2-4倍的问题，并在减少搜索需求的同时保持解决方案质量。&lt;h4&gt;结论&lt;/h4&gt;GammaZero通过统一的图表示框架有效解决了POMDPs中的规划学习问题，实现了更好的泛化能力和可扩展性，为处理大规模部分可观察环境提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种以动作为中心的图表示框架，用于学习在部分可观察马尔可夫决策过程(POMDPs)中指导规划。与需要特定领域神经网络架构且难以扩展的现有方法不同，GammaZero利用统一的基于图的信念表示，使问题能够在领域内跨规模泛化。我们的关键见解是信念状态可以系统地转换为以动作为中心的图，其中在小问题上学习的结构模式可以转移到更大的实例上。我们采用具有解码器架构的图神经网络，从计算可行问题上的专家演示中学习价值函数和策略，然后将这些学习到的启发式应用于指导更大问题上的蒙特卡洛树搜索。在标准POMDP基准测试上的实验结果表明，当在相同规模问题上训练和测试时，GammaZero与BetaZero相当，同时能够独特地实现零样本泛化到比训练时所见大2-4倍的问题，在减少搜索需求的同时保持解决方案质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce an action-centric graph representation framework for learning toguide planning in Partially Observable Markov Decision Processes (POMDPs).Unlike existing approaches that require domain-specific neural architecturesand struggle with scalability, GammaZero leverages a unified graph-based beliefrepresentation that enables generalization across problem sizes within adomain. Our key insight is that belief states can be systematically transformedinto action-centric graphs where structural patterns learned on small problemstransfer to larger instances. We employ a graph neural network with a decoderarchitecture to learn value functions and policies from expert demonstrationson computationally tractable problems, then apply these learned heuristics toguide Monte Carlo tree search on larger problems. Experimental results onstandard POMDP benchmarks demonstrate that GammaZero achieves comparableperformance to BetaZero when trained and tested on the same-sized problems,while uniquely enabling zero-shot generalization to problems 2-4 times largerthan those seen during training, maintaining solution quality with reducedsearch requirements.</description>
      <author>example@mail.com (Rajesh Mangannavar, Prasad Tadepalli)</author>
      <guid isPermaLink="false">2510.14035v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters</title>
      <link>http://arxiv.org/abs/2510.14250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PhysAttnNet的新型物理先验引导双流注意力网络，通过引入衰减双向自注意力和相位差引导的双向交叉注意力模块，有效解决了传统深度学习模型在预测弹性Bragg防波堤运动响应时面临的泛化能力有限问题。实验证明该模型在波浪槽数据集上表现优异，且对未见环境具有良好的适应性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;准确预测弹性Bragg防波堤的运动响应对于其在海洋环境中的结构安全和运行完整性至关重要。然而，传统的深度学习模型在面对未见过的海况时往往表现出有限的泛化能力。这些缺陷源于忽视了海洋系统中自然衰减现象，以及对波-结构相互作用的不充分建模。&lt;h4&gt;目的&lt;/h4&gt;克服传统深度学习模型在预测弹性Bragg防波堤运动响应时面临的泛化能力有限问题，开发一种能够更好处理未见海况的预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为PhysAttnNet的物理先验引导双流注意力网络，包含三个关键模块：1)衰减双向自注意力(DBSA)模块，通过可学习的时间衰减模拟自然衰减现象；2)相位差引导的双向交叉注意力(PDG-BCA)模块，明确捕获波与结构之间的双向相互作用和相位关系；3)全局上下文融合(GCF)模块，协同整合两个流。模型使用混合时频损失函数进行训练，同时最小化时域预测误差和频域频谱差异。&lt;h4&gt;主要发现&lt;/h4&gt;在波浪槽数据集上的综合实验表明，PhysAttnNet显著优于主流模型。此外，跨场景泛化测试验证了模型对未见环境的鲁棒性和适应性。&lt;h4&gt;结论&lt;/h4&gt;PhysAttnNet有潜力作为开发海洋工程复杂系统预测模型的框架，能够有效解决传统深度学习模型在海洋环境预测中面临的泛化能力有限问题。&lt;h4&gt;翻译&lt;/h4&gt;准确预测弹性Bragg防波堤的运动响应对于其在海洋环境中的结构安全和运行完整性至关重要。然而，传统的深度学习模型在面对未见过的海况时往往表现出有限的泛化能力。这些缺陷源于忽视了海洋系统中自然衰减现象，以及对波-结构相互作用的不充分建模。为克服这些挑战，本研究提出了一种新颖的物理先验引导双流注意力网络(PhysAttnNet)。首先，衰减双向自注意力(DBSA)模块纳入了可学习的时间衰减，为最近的状态分配更高的权重，旨在模拟自然衰减现象。同时，相位差引导的双向交叉注意力(PDG-BCA)模块使用基于余弦的偏差在双向交叉计算范式中明确捕获波与结构之间的双向相互作用和相位关系。这些流通过全局上下文融合(GCF)模块协同整合。最后，PhysAttnNet使用混合时频损失进行训练，该损失函数同时最小化时域预测误差和频域频谱差异。在波浪槽数据集上的综合实验表明，PhysAttnNet显著优于主流模型。此外，跨场景泛化测试验证了模型对未见环境的鲁棒性和适应性，突显了其作为开发海洋工程复杂系统预测模型的框架的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion response prediction for elastic Bragg breakwaters is criticalfor their structural safety and operational integrity in marine environments.However, conventional deep learning models often exhibit limited generalizationcapabilities when presented with unseen sea states. These deficiencies stemfrom the neglect of natural decay observed in marine systems and inadequatemodeling of wave-structure interaction (WSI). To overcome these challenges,this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) moduleincorporates a learnable temporal decay to assign higher weights to recentstates, aiming to emulate the natural decay phenomenon. Meanwhile, the phasedifferences guided bidirectional cross-attention (PDG-BCA) module explicitlycaptures the bidirectional interaction and phase relationship between waves andthe structure using a cosine-based bias within a bidirectionalcross-computation paradigm. These streams are synergistically integratedthrough a global context fusion (GCF) module. Finally, PhysAttnNet is trainedwith a hybrid time-frequency loss that jointly minimizes time-domain predictionerrors and frequency-domain spectral discrepancies. Comprehensive experimentson wave flume datasets demonstrate that PhysAttnNet significantly outperformsmainstream models. Furthermore,cross-scenario generalization tests validate themodel's robustness and adaptability to unseen environments, highlighting itspotential as a framework to develop predictive models for complex systems inocean engineering.</description>
      <author>example@mail.com (Lianzi Jiang, Jianxin Zhang, Xinyu Han, Huanhe Dong, Xiangrong Wang)</author>
      <guid isPermaLink="false">2510.14250v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为动态人格完善框架(DPRF)的新方法，用于优化大型语言模型角色扮演代理的行为与目标个体行为的一致性，通过迭代识别认知差异并完善人格配置文件，显著提高了行为对齐度。&lt;h4&gt;背景&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但其人格保真度常因手动创建的配置文件(例如，精心挑选的信息和人格特征)而受损，这些配置文件未经验证是否与目标个体保持一致。&lt;h4&gt;目的&lt;/h4&gt;解决LLM RPAs行为与目标个体行为不一致的问题，通过优化LLM RPAs的行为与目标个体行为的对齐度。&lt;h4&gt;方法&lt;/h4&gt;提出动态人格完善框架(DPRF)，通过迭代识别生成行为与人类真实行为之间的认知差异(无论是自由形式还是基于理论的结构化分析)，并完善人格配置文件以减轻这些差异。在四个多样化的行为预测场景(正式辩论、心理健康问题的社交媒体帖子、公开采访和电影评论)中使用五个大型语言模型评估DPRF。&lt;h4&gt;主要发现&lt;/h4&gt;DPRF能够一致地显著提高基线人格的行为一致性，并且在模型和场景方面具有通用性。&lt;h4&gt;结论&lt;/h4&gt;该研究为创建高保真度人格配置文件和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;h4&gt;翻译&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但其人格保真度常因手动创建的配置文件(例如，精心挑选的信息和人格特征)而受损，这些配置文件未经验证是否与目标个体保持一致。为解决这一限制，我们的工作引入了动态人格完善框架(DPRF)。DPRF旨在通过迭代识别生成行为与人类真实行为之间的认知差异(无论是自由形式还是基于理论的结构化分析)，并完善人格配置文件以减轻这些差异，从而优化LLM RPAs的行为与目标个体行为的一致性。我们在四个多样化的行为预测场景中使用五个大型语言模型评估DPRF：正式辩论、心理健康问题的社交媒体帖子、公开采访和电影评论。DPRF能够一致地显著提高基线人格的行为一致性，并且在模型和场景方面具有通用性。我们的研究为创建高保真度人格配置文件和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences.We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews.DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios.Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation</title>
      <link>http://arxiv.org/abs/2510.12815v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基于强化学习的推荐系统(RL4RS)在离线设置下面临数据效率低和依赖预收集轨迹的挑战。本文提出了一种名为DAC4Rec的新框架，整合扩散过程与强化学习，有效解决噪声数据处理和长期用户偏好捕捉问题。&lt;h4&gt;背景&lt;/h4&gt;基于强化学习的推荐系统能够适应用户的动态偏好，但在离线设置下面临数据效率低和依赖预收集轨迹的挑战。离线强化学习方法利用大量数据解决这些问题，但往往难以处理嘈杂数据且无法捕捉长期用户偏好。&lt;h4&gt;目的&lt;/h4&gt;克服现有离线强化学习推荐系统的局限性，提出一种新的框架来更有效地建模复杂的用户偏好。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Diffusion-enhanced Actor-Critic for Offline RL4RS (DAC4Rec)的新框架，该框架整合了扩散过程与强化学习。DAC4Rec利用扩散模型的去噪能力增强离线强化学习算法的鲁棒性，并采用Q值引导的策略优化策略来更好地处理次优轨迹。此外，还引入了一种基于能量的采样策略来减少推荐生成过程中的随机性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在六个真实世界离线数据集和在线模拟环境中的大量实验验证了DAC4Rec的有效性，证明其能够优化长期用户偏好。此外，提出的扩散策略可以无缝集成到RL4RS中其他常用的强化学习算法中，展示了其多功能性和广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;DAC4Rec框架通过整合扩散过程与强化学习，有效解决了离线强化学习推荐系统中的数据效率、噪声处理和长期偏好捕捉等问题。&lt;h4&gt;翻译&lt;/h4&gt;基于强化学习的推荐系统(RL4RS)因其能够适应动态用户偏好而受到关注。然而，这些系统面临挑战，特别是在离线设置中，数据效率低下和对预收集轨迹的依赖限制了它们的广泛应用。虽然离线强化学习方法利用大量数据来解决这些问题，但它们通常难以处理嘈杂数据且无法捕捉长期用户偏好，导致次优的推荐策略。为了克服这些局限性，我们提出了用于离线RL4RS的扩散增强型Actor-Critic(DAC4Rec)，这是一个将扩散过程与强化学习相结合的新颖框架，能够更有效地建模复杂的用户偏好。DAC4Rec利用扩散模型的去噪能力增强离线强化学习算法的鲁棒性，并采用Q值引导的策略优化策略来更好地处理次优轨迹。此外，我们引入了一种基于能量的采样策略来减少推荐生成过程中的随机性，确保更有针对性和可靠的结果。我们在六个真实世界的离线数据集和在线模拟环境中通过大量实验验证了DAC4Rec的有效性，证明了其优化长期用户偏好的能力。此外，我们表明所提出的扩散策略可以无缝集成到RL4RS中其他常用的强化学习算法中，突显了其多功能性和广泛的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning-based recommender systems (RL4RS) have gainedattention for their ability to adapt to dynamic user preferences. However,these systems face challenges, particularly in offline settings, where datainefficiency and reliance on pre-collected trajectories limit their broaderapplicability. While offline reinforcement learning methods leverage extensivedatasets to address these issues, they often struggle with noisy data and failto capture long-term user preferences, resulting in suboptimal recommendationpolicies. To overcome these limitations, we propose Diffusion-enhancedActor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integratesdiffusion processes with reinforcement learning to model complex userpreferences more effectively. DAC4Rec leverages the denoising capabilities ofdiffusion models to enhance the robustness of offline RL algorithms andincorporates a Q-value-guided policy optimization strategy to better handlesuboptimal trajectories. Additionally, we introduce an energy-based samplingstrategy to reduce randomness during recommendation generation, ensuring moretargeted and reliable outcomes. We validate the effectiveness of DAC4Recthrough extensive experiments on six real-world offline datasets and in anonline simulation environment, demonstrating its ability to optimize long-termuser preferences. Furthermore, we show that the proposed diffusion policy canbe seamlessly integrated into other commonly used RL algorithms in RL4RS,highlighting its versatility and wide applicability.</description>
      <author>example@mail.com (Xiaocong Chen, Siyu Wang, Lina Yao)</author>
      <guid isPermaLink="false">2510.12815v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>UrbanTwin: Synthetic LiDAR Datasets (LUMPI, V2X-Real-IC, and TUMTraf-I)</title>
      <link>http://arxiv.org/abs/2509.06781v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真真实副本，每个包含10K个注释帧，具有丰富的标注信息，能够有效支持深度学习模型训练。&lt;h4&gt;背景&lt;/h4&gt;激光雷达感知任务需要大量高质量数据集进行模型训练，但真实数据集获取和标注成本高，且场景多样性有限。&lt;h4&gt;目的&lt;/h4&gt;创建高保真合成数据集，能够独立使用或增强现有数据集，用于激光雷达感知任务，并探索其能否替代同领域真实世界数据集。&lt;h4&gt;方法&lt;/h4&gt;基于实际位置的几何特征、道路对齐和交通模式构建数字孪生环境，使用模拟激光雷达传感器合成数据，添加3D边界框、实例分割和语义分割等标注，并通过统计和结构相似性分析评估数据质量。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据集与真实数据高度相似，仅使用合成数据训练的模型在真实未见数据上表现优于使用真实数据训练的模型，数据集通过增加样本量和场景多样性有效增强了基准数据集。&lt;h4&gt;结论&lt;/h4&gt;UrbanTwin数据集是首批能够替换同领域真实世界数据集的数字合成数据集，提供了高保真数据副本，支持自定义场景测试，已公开可供使用。&lt;h4&gt;翻译&lt;/h4&gt;这篇文章介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真真实副本。每个UrbanTwin数据集包含10K个注释帧，对应一个公共数据集。注释包括6个类别的3D边界框、实例分割标签和跟踪ID，以及9个类别的语义分割标签。这些数据集使用模拟激光雷达传感器在真实数字孪生中合成，基于实际位置的周围几何形状、车道级别的道路对齐以及交叉口的车道拓扑和车辆移动模式进行建模。由于精确的数字孪生建模，合成数据集与真实数据集很好地对齐，为训练深度学习模型提供了强大的独立和增强价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决高质量激光雷达数据集创建困难的问题。真实世界数据收集和标注成本高、耗时长，限制了智能交通系统感知算法的发展。这个问题很重要，因为激光雷达是智能交通系统中的关键技术，而高质量数据集对于训练和评估3D感知算法至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有模拟环境虽然功能强大，但与真实世界存在差距。他们借鉴了数字孪生概念，结合了CARLA模拟器和现有路边激光雷达数据集的特点。作者强调需要同时建模静态元素(如几何结构)和动态行为(如交通模式)，而非仅依赖手工制作的3D资产和简化的物理假设。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用数字孪生技术创建真实世界场景的高保真虚拟副本，模拟激光雷达传感器生成与真实数据相似的点云，并提供丰富一致的标注。实现流程包括：1)使用卫星图像和真实位置数据构建环境；2)配置虚拟传感器匹配真实规格；3)随机生成符合交通规则的动态元素；4)在CARLA模拟器中生成10K帧带标注的合成数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门为路边激光雷达应用创建合成数据集；采用高保真数字孪生同时整合静态和动态元素；合成数据与真实数据高度相似；证明完全在模拟数据上训练的模型可匹敌真实数据训练效果。相比之前工作，UrbanTwin专门增强真实世界基准而非通用模拟，在模拟过程中而非事后缩小sim-to-real差距，提供完整标注支持多种感知任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanTwin通过创建基于数字孪生的高保真合成激光雷达数据集，成功解决了真实世界数据集创建成本高昂且sim-to-real差距大的问题，使模型能在合成数据上训练并有效应用于真实世界感知任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents UrbanTwin datasets, high-fidelity, realistic replicasof three public roadside lidar datasets: LUMPI, V2X-Real-IC}}, and TUMTraf-I.Each UrbanTwin dataset contains 10K annotated frames corresponding to one ofthe public datasets. Annotations include 3D bounding boxes, instancesegmentation labels, and tracking IDs for six object classes, along withsemantic segmentation labels for nine classes. These datasets are synthesizedusing emulated lidar sensors within realistic digital twins, modeled based onsurrounding geometry, road alignment at lane level, and the lane topology andvehicle movement patterns at intersections of the actual locationscorresponding to each real dataset. Due to the precise digital twin modeling,the synthetic datasets are well aligned with their real counterparts, offeringstrong standalone and augmentative value for training deep learning models ontasks such as 3D object detection, tracking, and semantic and instancesegmentation. We evaluate the alignment of the synthetic replicas throughstatistical and structural similarity analysis with real data, and furtherdemonstrate their utility by training 3D object detection models solely onsynthetic data and testing them on real, unseen data. The high similarityscores and improved detection performance, compared to the models trained onreal data, indicate that the UrbanTwin datasets effectively enhance existingbenchmark datasets by increasing sample size and scene diversity. In addition,the digital twins can be adapted to test custom scenarios by modifying thedesign and dynamics of the simulations. To our knowledge, these are the firstdigitally synthesized datasets that can replace in-domain real-world datasetsfor lidar perception tasks. UrbanTwin datasets are publicly available athttps://dataverse.harvard.edu/dataverse/ucf-ut.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.06781v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Backdoor Unlearning by Linear Task Decomposition</title>
      <link>http://arxiv.org/abs/2510.14845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究解决了基础模型中后门攻击的安全问题，提出了一种基于后门与良性任务解耦特性的简单遗忘方法，能够在不损害模型通用能力的情况下有效移除后门。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过在多样化任务中实现广泛的泛化能力彻底改变了计算机视觉领域。然而，它们仍然容易受到对抗性扰动和定向后门攻击的影响。缓解此类脆弱性仍然是一个开放的挑战，特别是考虑到模型的大规模性质使得重新训练以确保安全性变得不可行。&lt;h4&gt;目的&lt;/h4&gt;回答后门是否可以在不损害模型通用能力的情况下被移除这一问题，并研究后门如何在模型权重空间中被编码。&lt;h4&gt;方法&lt;/h4&gt;研究后门与良性任务在模型权重空间中的解耦特性，基于这种分离开发一种简单的遗忘方法，能够隔离和擦除后门对模型的影响，同时保持干净性能。通过基于CLIP的模型和常见对抗触发器进行大量实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;后门与其他良性任务是解耦的；给定攻击知识的情况下，方法实现了近乎完美的遗忘，同时平均保留了96%的干净准确率；即使当攻击及其存在未知时，方法也能通过反向工程触发器的适当估计成功遗忘后门；与当前最先进的防御相比，方法始终产生更好的遗忘和干净准确率权衡。&lt;h4&gt;结论&lt;/h4&gt;该方法在移除后门的同时，有效保留了模型的通用能力，为解决基础模型的安全问题提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过在多样化任务中实现广泛的泛化能力彻底改变了计算机视觉领域。然而，它们仍然容易受到对抗性扰动和定向后门攻击的影响。缓解此类脆弱性仍然是一个开放的挑战，特别是考虑到模型的大规模性质使得重新训练以确保安全性变得不可行。现有的后门移除方法依赖于昂贵的微调来覆盖有害行为，并且通常会降低在其他不相关任务上的性能。这引发了一个问题：后门是否可以在不损害模型通用能力的情况下被移除。在本研究中，我们解决了这个问题，并研究了后门如何在模型权重空间中被编码，发现它们与其他良性任务是解耦的。具体而言，这种分离使得能够隔离和擦除后门对模型的影响，同时对干净性能的影响最小。基于这一见解，我们引入了一种利用这种解耦特性的简单遗忘方法。通过对基于CLIP的模型和常见对抗触发器的大量实验，我们表明，给定攻击知识的情况下，我们的方法实现了近乎完美的遗忘，同时平均保留了96%的干净准确率。此外，我们证明即使当攻击及其存在未知时，我们的方法也能通过使用反向工程触发器的适当估计成功遗忘后门。总体而言，与当前最先进的防御相比，我们的方法始终产生更好的遗忘和干净准确率权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have revolutionized computer vision by enabling broadgeneralization across diverse tasks. Yet, they remain highly susceptible toadversarial perturbations and targeted backdoor attacks. Mitigating suchvulnerabilities remains an open challenge, especially given that thelarge-scale nature of the models prohibits retraining to ensure safety.Existing backdoor removal approaches rely on costly fine-tuning to override theharmful behavior, and can often degrade performance on other unrelated tasks.This raises the question of whether backdoors can be removed withoutcompromising the general capabilities of the models. In this work, we addressthis question and study how backdoors are encoded in the model weight space,finding that they are disentangled from other benign tasks. Specifically, thisseparation enables the isolation and erasure of the backdoor's influence on themodel with minimal impact on clean performance. Building on this insight, weintroduce a simple unlearning method that leverages such disentanglement.Through extensive experiments with CLIP-based models and common adversarialtriggers, we show that, given the knowledge of the attack, our method achievesapproximately perfect unlearning, while retaining, on average, 96% of cleanaccuracy. Additionally, we demonstrate that even when the attack and itspresence are unknown, our method successfully unlearns backdoors by properestimation using reverse-engineered triggers. Overall, our method consistentlyyields better unlearning and clean accuracy tradeoffs when compared to presentstate-of-the-art defenses.</description>
      <author>example@mail.com (Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard)</author>
      <guid isPermaLink="false">2510.14845v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&amp;E Whole Slide Images</title>
      <link>http://arxiv.org/abs/2510.14800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种名为PRISM的新型可解释AI模型，用于结直肠癌预后预测。该模型通过整合连续变异性谱的形态学信息，能够更准确地捕捉肿瘤的渐进式进化过程，并在III期结直肠癌患者中展现出优异的预后预测性能。&lt;h4&gt;背景&lt;/h4&gt;结直肠癌是全球第三大常见恶性肿瘤，预计2025年将有约154,000新病例和54,000例死亡。当前计算机病理学中的基础模型主要采用任务无关的方法学，可能忽略器官特定的关键形态学模式，而这些模式对肿瘤行为、治疗反应和患者结局有重要影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型、可解释的AI模型PRISM（预后性整合空间形态表征），纳入每种不同形态内的连续变异性谱以表征表型多样性，反映恶性肿瘤转化是通过渐进式进化过程而非表型急剧转变发生的原理。&lt;h4&gt;方法&lt;/h4&gt;PRISM模型在874万张组织学图像上进行训练，这些图像来自424名III期结直肠癌患者的手术切除标本。模型整合了空间形态学信息，以捕捉肿瘤的形态学变异性。&lt;h4&gt;主要发现&lt;/h4&gt;PRISM在五年总生存期(OS)预后方面表现优越：AUC = 0.70 ± 0.04；准确率 = 68.37% ± 4.75%；风险比(HR) = 3.34，95% CI = 2.28-4.90，p &lt; 0.0001。模型优于现有的结直肠癌特异性方法15%，比AI基础模型高约23%的准确率。PRISM显示性别无关的稳健性，在临床病理亚组中表现稳定，在不同治疗方案间的准确率波动最小（差值 = 1.44%），复现了Alliance队列的研究结果，即两种治疗方案之间无生存差异。&lt;h4&gt;结论&lt;/h4&gt;PRISM模型在结直肠癌预后预测方面表现优异，能够更好地捕捉肿瘤的形态学变异性，对不同治疗方案的患者预后有稳定的预测能力，为临床决策提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;结直肠癌(CRC)仍然是全球第三大常见恶性肿瘤，预计2025年将有约154,000新病例和54,000例死亡。最近，计算病理学中基础模型的进展主要是由任务无关的方法学推动的，这些方法可能忽略器官特定的关键形态学模式，这些模式代表不同的生物学过程，能从根本上影响肿瘤行为、治疗反应和患者结局。本研究旨在开发一种新型、可解释的AI模型PRISM（预后性整合空间形态表征），该模型纳入了每种不同形态内的连续变异性谱，以表征表型多样性，并反映恶性肿瘤转化是通过渐进式进化过程而非表型急剧转变发生的原理。PRISM在从424名III期CRC患者的手术切除标本中提取的874万张组织学图像上进行训练。PRISM在五年OS预后方面取得了优异的性能（AUC = 0.70 ± 0.04；准确率 = 68.37% ± 4.75%；HR = 3.34，95% CI = 2.28-4.90；p &lt; 0.0001），比现有的CRC特异性方法高出15%，比AI基础模型高出约23%的准确率。它显示出性别无关的稳健性（AUC差值 = 0.02；准确率差值 = 0.15%），并在临床病理亚组中表现稳定，在5FU/LV和CPT-11/5FU/LV治疗方案之间的准确率波动最小（差值 = 1.44%），复现了Alliance队列的研究结果，即两种治疗方案之间无生存差异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colorectal cancer (CRC) remains the third most prevalent malignancy globally,with approximately 154,000 new cases and 54,000 projected deaths anticipatedfor 2025. The recent advancement of foundation models in computationalpathology has been largely propelled by task agnostic methodologies that canoverlook organ-specific crucial morphological patterns that represent distinctbiological processes that can fundamentally influence tumor behavior,therapeutic response, and patient outcomes. The aim of this study is to developa novel, interpretable AI model, PRISM (Prognostic Representation of IntegratedSpatial Morphology), that incorporates a continuous variability spectrum withineach distinct morphology to characterize phenotypic diversity and reflectingthe principle that malignant transformation occurs through incrementalevolutionary processes rather than abrupt phenotypic shifts. PRISM is trainedon 8.74 million histological images extracted from surgical resection specimensof 424 patients with stage III CRC. PRISM achieved superior prognosticperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;HR = 3.34, 95% CI = 2.28-4.90; p &lt; 0.0001), outperforming existing CRC-specificmethods by 15% and AI foundation models by ~23% accuracy. It showedsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stableperformance across clinicopathological subgroups, with minimal accuracyfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,replicating the Alliance cohort finding of no survival difference betweentreatments.</description>
      <author>example@mail.com (Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi)</author>
      <guid isPermaLink="false">2510.14800v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes</title>
      <link>http://arxiv.org/abs/2510.14763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对大型语言模型在创意写作方面的局限性，特别是在非英语环境中的不足，提出了一个新颖的中文创意写作数据集COIG-Writer，并通过实验确定了创意写作的双组分模型及其关键发现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在创意写作方面存在系统性缺陷，特别是在非英语语境中，训练数据稀缺且缺乏过程层面的监督。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉多样化输出及其背后思维过程的中文创意写作数据集，并研究创意写作的构成要素和优化方法。&lt;h4&gt;方法&lt;/h4&gt;创建了COIG-Writer数据集，包含1665个精心挑选的三元组，涵盖51个体裁，每个三元组包含逆向工程提示、详细创意推理和最终文本。通过全面实验分析创意写作的构成要素和优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;1. 过程监督非常有效，但需要通用数据稳定化，至少需要一个创意样本对应十二个通用样本才能实现最佳性能；2. 创意能力具有文化局限性，没有跨语言迁移能力，中文和英文表现之间有89.26百分点的差距；3. 词汇多样性与创意质量呈负相关（TTR悖论），高多样性信号表明对逻辑缺陷的补偿行为。&lt;h4&gt;结论&lt;/h4&gt;创意卓越来自于逻辑支架和语言基础的相互作用，类似于数学推理如何增强但不能替代基础模型中的语言能力。创意写作需要过程监督和通用数据的适当平衡。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在创意写作方面表现出系统性缺陷，特别是在非英语环境中，训练数据稀缺且缺乏过程层面的监督。我们提出了COIG-Writer，这是一个新颖的中文创意写作数据集，通过对高质量文本进行系统性的逆向工程，捕捉多样化的输出及其背后的思维过程。与仅提供输入-输出对的数据集不同，COIG-Writer包含1665个精心挑选的三元组，涵盖51个体裁，每个三元组包含：(1)逆向工程提示，(2)详细创意推理记录决策过程，(3)最终文本。通过全面实验，我们确定了创意写作的双组分模型：叙事逻辑（由过程监督提供）和语言表达（由通用数据维持）。我们的研究揭示了三个关键见解：(1)过程监督非常有效，但需要通用数据稳定化。至少需要一个创意样本对应十二个通用样本的比例才能实现最佳性能；低于此阈值，胜率会逐渐下降（从62.75%降至35.78%）；(2)创意能力具有文化局限性，没有跨语言迁移能力（中文和英文表现之间有89.26百分点的差距）；(3)词汇多样性与创意质量呈负相关（TTR悖论），表明高多样性信号表明对逻辑缺陷的补偿行为。这些发现表明，创意卓越来自于逻辑支架和语言基础的相互作用，类似于数学推理如何增强但不能替代基础模型中的语言能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models exhibit systematic deficiencies in creative writing,particularly in non-English contexts where training data is scarce and lacksprocess-level supervision. We present COIG-Writer, a novel Chinese creativewriting dataset that captures both diverse outputs and their underlying thoughtprocesses through systematic reverse-engineering of high-quality texts. Unlikeexisting datasets that provide only input-output pairs, COIG-Writer comprises1,665 meticulously curated triplets spanning 51 genres, each containing: (1) areverse-engineered prompt, (2) detailed creative reasoning documentingdecision-making processes, and (3) the final text. Through comprehensiveexperiments, we identify a two-component model of creative writing: narrativelogic (provided by process supervision) and linguistic expression (maintainedby general-purpose data). Our findings reveal three critical insights: (1)Process supervision is highly effective but requires stabilization with generaldata. A ratio of at least one creative sample to twelve general samples isneeded to achieve optimal performance; below this threshold, the win rateprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilitiesare culturally-bound with no cross-lingual transfer (89.26pp gap betweenChinese and English performance), and (3) lexical diversity inverselycorrelates with creative quality (TTR paradox), suggesting high diversitysignals compensatory behavior for logical deficiencies. These findingsestablish that creative excellence emerges from the interaction between logicalscaffolding and linguistic grounding, analogous to how mathematical reasoningenhances but cannot replace linguistic competence in foundation models.</description>
      <author>example@mail.com (Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang)</author>
      <guid isPermaLink="false">2510.14763v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>State-Space Models for Tabular Prior-Data Fitted Networks</title>
      <link>http://arxiv.org/abs/2510.14573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用Hydra（一种双向线性时间结构状态空间模型）替代TabPFN中的Transformer架构，以解决Transformer的二次复杂度问题，同时保持预测性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在表格数据领域取得了进展，如TabPFN展示了预训练Transformer架构可以高预测性能近似贝叶斯推断。然而，Transformer在序列长度上具有二次复杂度，促使人们探索更高效的序列模型。&lt;h4&gt;目的&lt;/h4&gt;研究Hydra作为TabPFN中Transformer替代方案的潜力，解决SSM对输入标记顺序的固有敏感性这一关键挑战，特别是在表格数据集中行顺序语义无意义的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究双向方法能在多大程度上保持效率并实现对称上下文聚合，以减少SSM对输入顺序的依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种方法减少了顺序依赖性，实现了与原始TabPFN模型相当的预测性能。&lt;h4&gt;结论&lt;/h4&gt;双向Hydra模型可以作为TabPFN中Transformer的有效替代方案，在保持预测性能的同时提高效率。&lt;h4&gt;翻译&lt;/h4&gt;最近在表格数据基础模型方面的进展，如TabPFN，表明预训练的Transformer架构可以以高预测性能近似贝叶斯推断。然而，Transformer在序列长度上具有二次复杂度，促使人们探索更高效的序列模型。在这项工作中，我们研究了使用Hydra（一种双向线性时间结构状态空间模型SSM）作为TabPFN中Transformer替代方案的潜力。一个关键挑战在于SSM对输入标记顺序的固有敏感性——对于行顺序在语义上无意义的表格数据集来说，这是一个不希望有的特性。我们研究了双向方法在多大程度上可以保持效率并实现对称上下文聚合。我们的实验表明，这种方法减少了顺序依赖性，实现了与原始TabPFN模型具有竞争力的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models for tabular data, such as TabPFN,demonstrated that pretrained Transformer architectures can approximate Bayesianinference with high predictive performance. However, Transformers suffer fromquadratic complexity with respect to sequence length, motivating theexploration of more efficient sequence models. In this work, we investigate thepotential of using Hydra, a bidirectional linear-time structured state spacemodel (SSM), as an alternative to Transformers in TabPFN. A key challenge liesin SSM's inherent sensitivity to the order of input tokens - an undesirableproperty for tabular datasets where the row order is semantically meaningless.We investigate to what extent a bidirectional approach can preserve efficiencyand enable symmetric context aggregation. Our experiments show that thisapproach reduces the order-dependence, achieving predictive performancecompetitive to the original TabPFN model.</description>
      <author>example@mail.com (Felix Koch, Marcel Wever, Fabian Raisch, Benjamin Tischler)</author>
      <guid isPermaLink="false">2510.14573v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</title>
      <link>http://arxiv.org/abs/2510.14532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DentVFM是首个专为牙科设计的视觉基础模型系列，解决了现有牙科AI系统的局限性，通过自监督学习和大规模多模态数据集训练，展现出卓越的泛化能力和跨模态诊断性能。&lt;h4&gt;背景&lt;/h4&gt;口腔颌面放射学在牙科医疗中至关重要，但受专业人才短缺限制。现有牙科AI系统因单一模态关注、任务特定设计和依赖标记数据而泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服现有AI系统局限性的牙科视觉基础模型，实现更广泛的应用和更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;创建DentVFM模型系列，使用DentVista数据集(约160万多模态放射图像)进行自监督学习，基于Vision Transformer架构开发2D和3D变体，并建立DentBench基准测试涵盖8个牙科亚专科。&lt;h4&gt;主要发现&lt;/h4&gt;DentVFM表现出通用智能，能推广到多种牙科任务；显著优于各类基线模型；提供更好的泛化能力、标签效率和可扩展性；在跨模态诊断中表现优于经验丰富的牙医。&lt;h4&gt;结论&lt;/h4&gt;DentVFM为牙科AI树立新范式，提供可扩展、适应性强且标签高效的模型，有助于改善智能牙科医疗保健并解决全球口腔医疗保健差距。&lt;h4&gt;翻译&lt;/h4&gt;口腔颌面放射学在牙科医疗保健中起着重要作用，但放射图像解读受到训练专业人员短缺的限制。虽然AI方法显示出前景，但现有牙科AI系统受限于其单一模态关注、任务特定设计和依赖昂贵的标记数据，阻碍了它们在多样化临床场景中的泛化能力。为解决这些挑战，我们引入了DentVFM，这是首个为牙科设计的视觉基础模型系列。DentVFM为广泛的牙科应用生成任务无关的视觉表示，并在DentVista上使用自监督学习，这是一个精心策划的大型牙科成像数据集，包含来自不同医疗中心的约160万张多模态放射图像。DentVFM基于Vision Transformer架构包含2D和3D变体。为解决牙科智能评估和基准测试的空白，我们引入了DentBench，这是一个全面的基准测试，涵盖八个牙科亚专科、更多疾病、成像方式和广泛的地理分布。DentVFM表现出令人印象深刻的通用智能，展示了向多样化牙科任务的稳健泛化能力，如疾病诊断、治疗分析、生物标志物识别以及解剖标志物检测和分割。实验结果表明，DentVFM显著优于监督、自监督和弱监督基线，提供更好的泛化能力、标签效率和可扩展性。此外，DentVFM实现跨模态诊断，在常规成像不可用的情况下提供比经验丰富的牙医更可靠的结果。DentVFM为牙科AI树立了新范式，提供可扩展、适应性强且标签高效的模型，以改善智能牙科医疗保健并解决全球口腔医疗保健中的关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Oral and maxillofacial radiology plays a vital role in dental healthcare, butradiographic image interpretation is limited by a shortage of trainedprofessionals. While AI approaches have shown promise, existing dental AIsystems are restricted by their single-modality focus, task-specific design,and reliance on costly labeled data, hindering their generalization acrossdiverse clinical scenarios. To address these challenges, we introduce DentVFM,the first family of vision foundation models (VFMs) designed for dentistry.DentVFM generates task-agnostic visual representations for a wide range ofdental applications and uses self-supervised learning on DentVista, a largecurated dental imaging dataset with approximately 1.6 million multi-modalradiographic images from various medical centers. DentVFM includes 2D and 3Dvariants based on the Vision Transformer (ViT) architecture. To address gaps indental intelligence assessment and benchmarks, we introduce DentBench, acomprehensive benchmark covering eight dental subspecialties, more diseases,imaging modalities, and a wide geographical distribution. DentVFM showsimpressive generalist intelligence, demonstrating robust generalization todiverse dental tasks, such as disease diagnosis, treatment analysis, biomarkeridentification, and anatomical landmark detection and segmentation.Experimental results indicate DentVFM significantly outperforms supervised,self-supervised, and weakly supervised baselines, offering superiorgeneralization, label efficiency, and scalability. Additionally, DentVFMenables cross-modality diagnostics, providing more reliable results thanexperienced dentists in situations where conventional imaging is unavailable.DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, andlabel-efficient model to improve intelligent dental healthcare and addresscritical gaps in global oral healthcare.</description>
      <author>example@mail.com (Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang)</author>
      <guid isPermaLink="false">2510.14532v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vision Mamba for Permeability Prediction of Porous Media</title>
      <link>http://arxiv.org/abs/2510.14516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次引入使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络，并证明了其相比ViTs和CNNs的优势。&lt;h4&gt;背景&lt;/h4&gt;Vision Mamba最近作为Vision Transformers(ViTs)的替代方案在图像分类领域受到关注。Vision Mamba的网络规模随输入图像分辨率线性增长，而ViTs则是二次增长，这使得Vision Mamba在计算和内存效率方面更具优势。此外，Vision Mamba比传统卷积神经网络(CNN)需要更少的可训练参数，因此内存效率更高。&lt;h4&gt;目的&lt;/h4&gt;首次引入使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络，比较Vision Mamba与ViT和CNN模型在渗透率预测多个方面的性能，并进行消融研究以评估其组件对准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;构建了一个使用Vision Mamba作为主干网络的神经网络来预测三维多孔介质的渗透率，并与ViT和CNN模型进行了性能比较，进行了消融研究评估组件对准确性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实践证明了Vision Mamba在三维多孔介质渗透率预测方面相比ViTs和CNNs具有计算效率高、内存占用少、参数量少等优势。&lt;h4&gt;结论&lt;/h4&gt;作者认为提出的框架有潜力集成到使用Vision Mamba替代ViTs的大型视觉模型中，并已公开源代码以促进可重复性并使其他研究人员能够在此基础上进行扩展。&lt;h4&gt;翻译&lt;/h4&gt;Vision Mamba最近作为Vision Transformers(ViTs)的替代方案在图像分类领域受到关注。Vision Mamba的网络规模随输入图像分辨率线性增长，而ViTs则是二次增长，这一特性提高了计算和内存效率。此外，Vision Mamba比传统卷积神经网络(CNN)需要少得多的可训练参数，因此可以更节省内存。由于这些特性，我们首次引入了一个使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络。我们在渗透率预测的多个方面比较了Vision Mamba与ViT和CNN模型的性能，并进行了消融研究以评估其组件对准确性的影响。我们通过实践证明了Vision Mamba在三维多孔介质渗透率预测方面相比ViTs和CNNs具有上述优势。我们公开源代码以促进可重复性，并使其他研究人员能够在此基础上进行扩展和延伸。我们认为，在Vision Mamba替代ViTs的大型视觉模型中，所提出的框架具有集成潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Mamba has recently received attention as an alternative to VisionTransformers (ViTs) for image classification. The network size of Vision Mambascales linearly with input image resolution, whereas ViTs scale quadratically,a feature that improves computational and memory efficiency. Moreover, VisionMamba requires a significantly smaller number of trainable parameters thantraditional convolutional neural networks (CNNs), and thus, they can be morememory efficient. Because of these features, we introduce, for the first time,a neural network that uses Vision Mamba as its backbone for predicting thepermeability of three-dimensional porous media. We compare the performance ofVision Mamba with ViT and CNN models across multiple aspects of permeabilityprediction and perform an ablation study to assess the effects of itscomponents on accuracy. We demonstrate in practice the aforementionedadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction ofthree-dimensional porous media. We make the source code publicly available tofacilitate reproducibility and to enable other researchers to build on andextend this work. We believe the proposed framework has the potential to beintegrated into large vision models in which Vision Mamba is used instead ofViTs.</description>
      <author>example@mail.com (Ali Kashefi, Tapan Mukerji)</author>
      <guid isPermaLink="false">2510.14516v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review</title>
      <link>http://arxiv.org/abs/2510.14462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇PRISMA指导的范围综述综合了无监督深度生成模型在神经影像学中异常检测的最新研究进展，涵盖了2018-2025年间的49项研究，表明这些模型在大局灶性病变检测和微妙异常识别方面取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;无监督深度生成模型正在成为脑成像异常检测和分割的有前景方法，与需要大量体素级标注数据且仅限于已表征病理的完全监督方法不同，这些模型可以仅使用健康数据进行训练，并将异常识别为从学习到的正常脑结构中出现的偏差。&lt;h4&gt;目的&lt;/h4&gt;综合关于无监督深度生成模型在神经影像学中异常检测的最新工作，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型，并比较其性能指标和架构设计选择。&lt;h4&gt;方法&lt;/h4&gt;采用PRISMA指导的范围综述方法，系统检索并分析了2018-2025年间发表的49项研究，这些研究应用了各种生成模型于脑MRI和CT影像，用于检测肿瘤、中风、多发性硬化和小血管疾病等多种病理。&lt;h4&gt;主要发现&lt;/h4&gt;生成模型在大局灶性病变方面取得了令人鼓舞的性能，并在处理更微妙的异常方面取得了进展；其关键优势是能够产生可解释的伪健康重建，这在注释数据稀缺的情况下（如罕见或异质性疾病）特别有价值。&lt;h4&gt;结论&lt;/h4&gt;这些模型为异常检测提供了有吸引力的方向，能够实现半监督学习，支持新成像生物标志物的发现，并促进统一端到端框架内的疾病内和跨疾病偏差映射；未来工作应优先考虑解剖感知建模、基础模型开发、任务适当的评估指标和严格的临床验证。&lt;h4&gt;翻译&lt;/h4&gt;无监督深度生成模型正在成为脑成像异常检测和分割的替代性有前景方法，与需要大量体素级标注数据且仅限于已表征病理的完全监督方法不同，这些模型可以仅使用健康数据进行训练，并将异常识别为从学习到的正常脑结构中出现的偏差。这篇PRISMA指导的范围综述综合了无监督深度生成模型在神经影像学中异常检测的最新工作，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型。共确定了2018-2025年间发表的49项研究，涵盖了脑MRI和较少见的CT应用，应用于肿瘤、中风、多发性硬化和小血管疾病等多种病理。报告的性能指标与架构设计选择进行了比较。在纳入的研究中，生成模型在大局灶性病变方面取得了令人鼓舞的性能，并在处理更微妙的异常方面取得了进展。生成模型的一个关键优势是它们能够产生可解释的伪健康（也称为反事实）重建，这在注释数据稀缺时（如罕见或异质性疾病）特别有价值。展望未来，这些模型为异常检测提供了有吸引力的方向，能够实现半监督学习，支持新成像生物标志物的发现，并促进统一端到端框架内的疾病内和跨疾病偏差映射。为实现临床影响，未来工作应优先考虑解剖感知建模、基础模型开发、任务适当的评估指标和严格的临床验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised deep generative models are emerging as a promising alternativeto supervised methods for detecting and segmenting anomalies in brain imaging.Unlike fully supervised approaches, which require large voxel-level annotateddatasets and are limited to well-characterised pathologies, these models can betrained exclusively on healthy data and identify anomalies as deviations fromlearned normative brain structures. This PRISMA-guided scoping reviewsynthesises recent work on unsupervised deep generative models for anomalydetection in neuroimaging, including autoencoders, variational autoencoders,generative adversarial networks, and denoising diffusion models. A total of 49studies published between 2018 - 2025 were identified, covering applications tobrain MRI and, less frequently, CT across diverse pathologies such as tumours,stroke, multiple sclerosis, and small vessel disease. Reported performancemetrics are compared alongside architectural design choices. Across theincluded studies, generative models achieved encouraging performance for largefocal lesions and demonstrated progress in addressing more subtleabnormalities. A key strength of generative models is their ability to produceinterpretable pseudo-healthy (also referred to as counterfactual)reconstructions, which is particularly valuable when annotated data are scarce,as in rare or heterogeneous diseases. Looking ahead, these models offer acompelling direction for anomaly detection, enabling semi-supervised learning,supporting the discovery of novel imaging biomarkers, and facilitating within-and cross-disease deviation mapping in unified end-to-end frameworks. Torealise clinical impact, future work should prioritise anatomy-aware modelling,development of foundation models, task-appropriate evaluation metrics, andrigorous clinical validation.</description>
      <author>example@mail.com (Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi)</author>
      <guid isPermaLink="false">2510.14462v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Centric Activation and Coordination for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2510.14349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了VaCo方法，通过视觉中心激活和多视觉基础模型的协调来优化多模态大语言模型(MLLMs)的表示，提高模型在视觉理解方面的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)通过整合视觉编码器的图像特征与LLMs，展现出先进的理解能力。然而，主流MLLMs仅通过文本标记的下一个标记预测进行监督，忽略了分析能力所需的关键视觉中心信息。&lt;h4&gt;目的&lt;/h4&gt;解决主流MLLMs忽视关键视觉中心信息的问题，通过引入视觉中心激活和协调机制，优化MLLMs的表示，提高其视觉理解能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了VaCo方法，包括：视觉判别对齐整合从多个视觉基础模型(VFMs)中提取的任务感知特征；可学习的模块化任务查询(MTQs)在多种VFMs的监督下激活特定视觉信号；视觉对齐层(VALs)整合到MLLMs中；标记网关掩码(TGM)限制多组MTQs之间的信息流，协调VFMs之间的表示冲突。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，VaCo显著提高了不同MLLMs在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。&lt;h4&gt;结论&lt;/h4&gt;VaCo通过有效整合多种视觉基础模型的特征，解决了主流MLLMs忽视视觉中心信息的问题，显著提升了模型在视觉理解任务上的表现。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)整合视觉编码器中的图像特征与LLMs，展现出先进的理解能力。然而，主流MLLMs仅通过文本标记的下一个标记预测进行监督，忽略了分析能力所需的关键视觉中心信息。为了解决这一困境，我们引入了VaCo，它通过多个视觉基础模型(VFMs)的视觉中心激活和协调来优化MLLM表示。VaCo引入视觉判别对齐来整合从VFMs中提取的任务感知特征，从而统一MLMs中文本和视觉输出的优化。具体来说，我们将可学习的模块化任务查询(MTQs)和视觉对齐层(VALs)整合到MLLMs中，在多种VFMs的监督下激活特定的视觉信号。为了协调VFMs之间的表示冲突，精心设计的标记网关掩码(TGM)限制了多组MTQs之间的信息流。大量实验证明，VaCo显著提高了不同MLLMs在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) integrate image features from visualencoders with LLMs, demonstrating advanced comprehension capabilities. However,mainstream MLLMs are solely supervised by the next-token prediction of textualtokens, neglecting critical vision-centric information essential for analyticalabilities. To track this dilemma, we introduce VaCo, which optimizes MLLMrepresentations through Vision-Centric activation and Coordination frommultiple vision foundation models (VFMs). VaCo introduces visual discriminativealignment to integrate task-aware perceptual features extracted from VFMs,thereby unifying the optimization of both textual and visual outputs in MLLMs.Specifically, we incorporate the learnable Modular Task Queries (MTQs) andVisual Alignment Layers (VALs) into MLLMs, activating specific visual signalsunder the supervision of diverse VFMs. To coordinate representation conflictsacross VFMs, the crafted Token Gateway Mask (TGM) restricts the informationflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCosignificantly improves the performance of different MLLMs on variousbenchmarks, showcasing its superior capabilities in visual comprehension.</description>
      <author>example@mail.com (Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin)</author>
      <guid isPermaLink="false">2510.14349v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering</title>
      <link>http://arxiv.org/abs/2510.14270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GauSSmart的混合方法，通过结合2D基础模型和3D高斯飞溅重建技术，解决了Gaussian Splatting在捕捉精细细节和稀疏覆盖区域保持真实感方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;场景重建是计算机视觉中的核心挑战，NeRF和Gaussian Splatting等方法取得了显著进展。但Gaussian Splatting在大规模数据集上表现良好时，往往难以捕捉精细细节或在稀疏覆盖区域保持真实感，这主要是由于稀疏3D训练数据的固有局限性。&lt;h4&gt;目的&lt;/h4&gt;提出GauSSmart，一种有效桥接2D基础模型和3D高斯飞溅重建的混合方法，以提升场景重建的质量和细节表现。&lt;h4&gt;方法&lt;/h4&gt;集成成熟的2D计算机视觉技术，包括凸滤波和来自基础模型(如DINO)的语义特征监督，利用2D分割先验和高维特征嵌入，指导高斯飞溅的密集化和细化，改善代表性不足区域的覆盖，并保持复杂的结构细节。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的验证表明，GauSSmart在大多数评估场景中一致性地优于现有的高斯飞溅方法，能够更好地捕捉场景细节并提高稀疏覆盖区域的重建质量。&lt;h4&gt;结论&lt;/h4&gt;混合2D-3D方法具有巨大潜力，将2D基础模型与3D重建管道的巧妙结合可以克服单独使用任何一种方法的固有局限性。&lt;h4&gt;翻译&lt;/h4&gt;场景重建已成为计算机视觉中的一个核心挑战，诸如神经辐射场和高斯飞溅等方法已取得显著进展。虽然高斯飞溅在大规模数据集上表现出色，但它往往难以捕捉精细细节或在稀疏覆盖区域保持真实感，这主要是由于稀疏3D训练数据的固有局限性。在本工作中，我们提出了GauSSmart，一种有效桥接2D基础模型和3D高斯飞溅重建的混合方法。我们的方法集成了成熟的2D计算机视觉技术，包括凸滤波和来自基础模型(如DINO)的语义特征监督，以增强基于高斯的场景重建。通过利用2D分割先验和高维特征嵌入，我们的方法指导高斯飞溅的密集化和细化，改善了代表性不足区域的覆盖并保持了复杂的结构细节。我们在三个数据集上验证了我们的方法，其中GauSSmart在大多数评估场景中一致性地优于现有的高斯飞溅方法。我们的结果证明了混合2D-3D方法的巨大潜力，强调了如何将2D基础模型与3D重建管道的巧妙结合可以克服单独使用任何一种方法所固有的局限性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景重建中细节捕捉不足和稀疏覆盖区域真实感差的问题。这个问题很重要，因为高质量的3D重建对虚拟现实、增强现实、自动驾驶等应用至关重要，而现有方法在处理细节和稀疏区域时存在局限性，限制了重建质量和技术应用范围。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何结合2D基础模型和3D重建的优势，认识到2D视觉技术（如分割和特征提取）成熟而3D方法擅长空间建模。他们借鉴了DINO等基础模型的语义特征、SAM的图像分割能力，以及凸包过滤技术，并将这些2D方法与3D高斯溅射流程巧妙融合，形成互补优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型的语义信息指导3D高斯溅射优化，改善点云质量并增强稀疏区域。整体流程包括：1)使用凸包过滤去除点云异常值；2)通过相机聚类选择代表性图像；3)应用SAM进行图像分割并关联3D点；4)基于分割掩码有针对性地增强点云密度；5)引入DINOv3特征嵌入损失提高语义一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)凸包引导的异常值去除方法；2)感知的点云增强策略，考虑语义区域重要性；3)基于DINOv3的嵌入对齐训练损失。相比之前工作，不同之处在于：不是简单拼接2D和3D方法，而是设计真正融合框架；利用语义先验指导3D重建；点云增强考虑语义区域重要性；使用特征嵌入损失而非仅传统光度损失。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GauSSmart通过融合2D基础模型的语义理解与3D高斯溅射的空间建模能力，有效提升了3D场景重建中的细节捕捉和稀疏区域真实感。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene reconstruction has emerged as a central challenge in computer vision,with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splattingachieving remarkable progress. While Gaussian Splatting demonstrates strongperformance on large-scale datasets, it often struggles to capture fine detailsor maintain realism in regions with sparse coverage, largely due to theinherent limitations of sparse 3D training data.  In this work, we propose GauSSmart, a hybrid method that effectively bridges2D foundational models and 3D Gaussian Splatting reconstruction. Our approachintegrates established 2D computer vision techniques, including convexfiltering and semantic feature supervision from foundational models such asDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2Dsegmentation priors and high-dimensional feature embeddings, our method guidesthe densification and refinement of Gaussian splats, improving coverage inunderrepresented areas and preserving intricate structural details.  We validate our approach across three datasets, where GauSSmart consistentlyoutperforms existing Gaussian Splatting in the majority of evaluated scenes.Our results demonstrate the significant potential of hybrid 2D-3D approaches,highlighting how the thoughtful combination of 2D foundational models with 3Dreconstruction pipelines can overcome the limitations inherent in eitherapproach alone.</description>
      <author>example@mail.com (Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang)</author>
      <guid isPermaLink="false">2510.14270v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals</title>
      <link>http://arxiv.org/abs/2510.14254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了基础模型在时间序列分析中的应用，特别是比较了专家模型和通用模型在生理信号处理（特别是PPG信号）上的性能差异。&lt;h4&gt;背景&lt;/h4&gt;基础模型是大规模机器学习模型，在大规模数据上预训练后可适应各种下游任务，已广泛应用于自然语言处理和计算机视觉领域。时间序列分析领域，特别是生理信号处理，正逐渐受到关注，但大多数时间序列基础模型是专家模型，只在同类型数据上预训练和测试，如心电图、脑电图和光电容积脉搏波(PPG)。最近的工作如MOMENT尝试训练跨多个领域的通用时间序列基础模型。&lt;h4&gt;目的&lt;/h4&gt;进行全面的基准测试研究，比较专家模型和通用模型的性能，特别关注PPG信号。&lt;h4&gt;方法&lt;/h4&gt;通过总共51个任务组成的测试套件进行评估，包括心脏状态评估、实验室值估计和跨模态推理。在七个维度上全面评估两种模型：获胜分数、平均性能、特征质量、调优增益、性能方差、可转移性和可扩展性。这些指标共同捕捉模型在不同微调策略下的能力、适应性和效率。在完整微调场景下比较模型性能，并提供泛化、公平性、注意力可视化和训练数据选择重要性的进一步分析。&lt;h4&gt;主要发现&lt;/h4&gt;在完整微调场景下，专家模型的获胜分数比通用模型高27%。&lt;h4&gt;结论&lt;/h4&gt;论文提供了专家模型和通用模型在多样化下游场景中的优势和局限性的全面理解。&lt;h4&gt;翻译&lt;/h4&gt;基础模型是大规模机器学习模型，在大规模数据上预训练，并可适应各种下游任务。它们已广泛应用于自然语言处理和计算机视觉任务，如GPT、BERT和CLIP等模型。现在，时间序列分析领域，特别是生理信号处理，也日益受到关注。然而，大多数时间序列基础模型是专家模型，其预训练和测试使用相同类型的数据，如心电图、脑电图和光电容积脉搏波(PPG)。最近的工作如MOMENT，使用来自多个领域（如天气、交通和电力）的数据训练通用时间序列基础模型。本文旨在进行全面的基准测试研究，比较专家模型和通用模型的性能，特别关注PPG信号。通过涵盖心脏状态评估、实验室值估计和跨模态推理的51个任务，我们在七个维度上全面评估了两种模型，包括获胜分数、平均性能、特征质量、调优增益、性能方差、可转移性和可扩展性。这些指标共同捕捉了模型在不同微调策略下的能力、适应性和效率，为它们在多样化下游场景中的优势和局限性提供了全面理解。在完整微调场景下，我们证明专家模型的获胜分数高出27%。最后，我们对泛化、公平性、注意力可视化和训练数据选择的重要性进行了进一步分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are large-scale machine learning models that arepre-trained on massive amounts of data and can be adapted for variousdownstream tasks. They have been extensively applied to tasks in NaturalLanguage Processing and Computer Vision with models such as GPT, BERT, andCLIP. They are now also increasingly gaining attention in time-series analysis,particularly for physiological sensing. However, most time series foundationmodels are specialist models - with data in pre-training and testing of thesame type, such as Electrocardiogram, Electroencephalogram, andPhotoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist timeseries foundation model with data from multiple domains, such as weather,traffic, and electricity. This paper aims to conduct a comprehensivebenchmarking study to compare the performance of generalist and specialistmodels, with a focus on PPG signals. Through an extensive suite of total 51tasks covering cardiac state assessment, laboratory value estimation, andcross-modal inference, we comprehensively evaluate both models across sevendimensions, including win score, average performance, feature quality, tuninggain, performance variance, transferability, and scalability. These metricsjointly capture not only the models' capability but also their adaptability,robustness, and efficiency under different fine-tuning strategies, providing aholistic understanding of their strengths and limitations for diversedownstream scenarios. In a full-tuning scenario, we demonstrate that thespecialist model achieves a 27% higher win score. Finally, we provide furtheranalysis on generalization, fairness, attention visualizations, and theimportance of training data choice.</description>
      <author>example@mail.com (Saurabh Kataria, Yi Wu, Zhaoliang Chen, Hyunjung Gloria Kwak, Yuhao Xu, Lovely Yeswanth Panchumarthi, Ran Xiao, Jiaying Lu, Ayca Ermis, Anni Zhao, Runze Yan, Alex Federov, Zewen Liu, Xu Wu, Wei Jin, Carl Yang, Jocelyn Grunwell, Stephanie R. Brown, Amit Shah, Craig Jabaley, Tim Buchman, Sivasubramanium V Bhavani, Randall J. Lee, Xiao Hu)</author>
      <guid isPermaLink="false">2510.14254v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization</title>
      <link>http://arxiv.org/abs/2510.14217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures, 3 tables, SI: 8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次对QM9数据集上的核岭回归进行了全面的谱分析，研究了不同分子表示方法（分子指纹、预训练Transformer、全局和局部3D表示）在七种分子属性上的谱特性，发现更丰富的谱特征并不总能提高准确性，主要特征值捕获了最有信息量的特征。&lt;h4&gt;背景&lt;/h4&gt;理解核的谱特性为泛化和表示质量提供了原则性的视角。虽然深度模型在分子属性预测中实现了最先进的准确性，但核方法因其在小数据环境下的鲁棒性和透明的理论基础而被广泛使用。然而，对分子核的系统性谱分析仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;提供对QM9数据集上核岭回归的首次全面谱分析，研究不同分子表示方法在七种分子属性上的谱特性，探索谱特性与预测性能之间的关系。&lt;h4&gt;方法&lt;/h4&gt;使用四种不同的谱指标测量谱丰富度，实施截断核方法探究谱与预测性能的关系，分析七种分子属性，比较分子指纹、预训练Transformer、全局和局部3D表示等不同表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;1) 更丰富的谱特征并不一致地提高准确性；2) 对于基于Transformer和局部3D表示，谱丰富度甚至可能与性能呈负相关；3) 在许多核中，仅保留前2%的特征值就能恢复几乎所有性能；4) 主要特征值捕获了最有信息量的特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明表示、核特征和预测性能之间存在微妙的关系，挑战了关于谱丰富度与性能关系的传统观点。这些发现对如何在数据有限的科学和实际任务中评估核方法和自监督学习方法提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;理解核的谱特性为泛化和表示质量提供了原则性的视角。虽然深度模型在分子属性预测中实现了最先进的准确性，但核方法因其在小数据环境下的鲁棒性和透明的理论基础而被广泛使用。尽管机器学习中核谱的研究广泛，但对分子核的系统性谱分析仍然稀缺。在这项工作中，我们首次对QM9数据集上的核岭回归进行了全面的谱分析，研究了分子指纹、预训练Transformer、全局和局部3D表示在七种分子属性上的谱特性。令人惊讶的是，通过四种不同的谱指标测量的更丰富的谱特征并不一致地提高准确性。皮尔逊相关性测试进一步表明，对于基于Transformer和局部3D表示，谱丰富度甚至可能与性能呈负相关。我们还实现了截断核来探究谱与预测性能之间的关系：在许多核中，仅保留前2%的特征值就能恢复几乎所有性能，这表明主要特征值捕获了最有信息量的特征。我们的结果挑战了'更丰富的谱产生更好的泛化'这一常见启发式方法，并突出了表示、核特征和预测性能之间的微妙关系。除了分子属性预测外，这些发现还指导了如何在数据有限的科学和实际任务中评估核方法和自监督学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the spectral properties of kernels offers a principledperspective on generalization and representation quality. While deep modelsachieve state-of-the-art accuracy in molecular property prediction, kernelmethods remain widely used for their robustness in low-data regimes andtransparent theoretical grounding. Despite extensive studies of kernel spectrain machine learning, systematic spectral analyses of molecular kernels arescarce. In this work, we provide the first comprehensive spectral analysis ofkernel ridge regression on the QM9 dataset, molecular fingerprint, pretrainedtransformer-based, global and local 3D representations across seven molecularproperties. Surprisingly, richer spectral features, measured by four differentspectral metrics, do not consistently improve accuracy. Pearson correlationtests further reveal that for transformer-based and local 3D representations,spectral richness can even have a negative correlation with performance. Wealso implement truncated kernels to probe the relationship between spectrum andpredictive performance: in many kernels, retaining only the top 2% ofeigenvalues recovers nearly all performance, indicating that the leadingeigenvalues capture the most informative features. Our results challenge thecommon heuristic that "richer spectra yield better generalization" andhighlight nuanced relationships between representation, kernel features, andpredictive performance. Beyond molecular property prediction, these findingsinform how kernel and self-supervised learning methods are evaluated indata-limited scientific and real-world tasks.</description>
      <author>example@mail.com (Asma Jamali, Tin Sum Cheng, Rodrigo A. Vargas-Hernández)</author>
      <guid isPermaLink="false">2510.14217v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ARM-FM是一种利用基础模型高级推理能力的框架，用于强化学习中自动化、组合式的奖励设计，解决了强化学习算法对奖励函数设定敏感的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;强化学习算法对奖励函数的设定高度敏感，这仍然是限制其广泛应用的核心挑战。&lt;h4&gt;目的&lt;/h4&gt;提出ARM-FM框架，实现强化学习中自动化、组合式的奖励设计，利用基础模型的高级推理能力来自动构建奖励机。&lt;h4&gt;方法&lt;/h4&gt;使用奖励机(RMs)作为强化学习目标设定的机制，通过基础模型自动构建奖励机；将语言嵌入与每个奖励机自动机状态相关联以实现跨任务泛化；在多样化挑战性环境中评估框架效果。&lt;h4&gt;主要发现&lt;/h4&gt;ARM-FM框架在多样化的挑战性环境中展现出有效性，包括实现零样本泛化的能力；基础模型能够从自然语言规范自动生成奖励机；结构化的奖励机形式化方法能实现有效的任务分解。&lt;h4&gt;结论&lt;/h4&gt;基础模型与奖励机的结构化形式化方法相结合，能够实现有效的自动化奖励设计，促进强化学习在更广泛领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;强化学习(RL)算法对奖励函数的设定高度敏感，这仍然是限制其广泛适用性的核心挑战。我们提出了ARM-FM：基于基础模型的自动奖励机，这是一个用于强化学习中自动化、组合式奖励设计的框架，利用了基础模型(FMs)的高级推理能力。奖励机(RMs)——一种基于自动机的奖励规范形式化方法——被用作强化学习目标设定的机制，并通过基础模型的使用自动构建。奖励机的结构化形式化方法能够实现有效的任务分解，而基础模型的使用则允许用自然语言进行目标规范。具体而言，我们(i)使用基础模型从自然语言规范自动生成奖励机；(ii)将语言嵌入与每个奖励机自动机状态相关联，以实现跨任务泛化；(iii)在一系列多样化的挑战性环境中提供了ARM-FM有效性的实证证据，包括零样本泛化的证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) algorithms are highly sensitive to rewardfunction specification, which remains a central challenge limiting their broadapplicability. We present ARM-FM: Automated Reward Machines via FoundationModels, a framework for automated, compositional reward design in RL thatleverages the high-level reasoning capabilities of foundation models (FMs).Reward machines (RMs) -- an automata-based formalism for reward specification-- are used as the mechanism for RL objective specification, and areautomatically constructed via the use of FMs. The structured formalism of RMsyields effective task decompositions, while the use of FMs enables objectivespecifications in natural language. Concretely, we (i) use FMs to automaticallygenerate RMs from natural language specifications; (ii) associate languageembeddings with each RM automata-state to enable generalization across tasks;and (iii) provide empirical evidence of ARM-FM's effectiveness in a diversesuite of challenging environments, including evidence of zero-shotgeneralization.</description>
      <author>example@mail.com (Roger Creus Castanyer, Faisal Mohamed, Pablo Samuel Castro, Cyrus Neary, Glen Berseth)</author>
      <guid isPermaLink="false">2510.14176v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems</title>
      <link>http://arxiv.org/abs/2510.14133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种代理AI系统的统一建模框架，由主机代理模型和任务生命周期模型组成，解决了当前代理间通信生态系统碎片化的问题，为多AI代理系统提供了形式化验证基础。&lt;h4&gt;背景&lt;/h4&gt;代理AI系统利用多个自主代理和大语言模型解决复杂多步骤任务，在高风险应用中安全性和功能性至关重要。当前代理间通信生态系统碎片化，各种协议被孤立分析，造成语义鸿沟，阻碍系统属性严格分析并引入架构不协调等风险。&lt;h4&gt;目的&lt;/h4&gt;解决代理AI系统中因通信碎片化导致的语义鸿沟问题，提供统一语义框架实现多AI代理系统行为的推理，支持系统化分析、设计和部署正确、可靠、稳健的代理AI系统。&lt;h4&gt;方法&lt;/h4&gt;引入由两个基础模型组成的框架：主机代理模型（正式化顶层实体与用户交互、任务分解和执行协调）和任务生命周期模型（详细说明子任务状态和转换）。基于此框架定义31个属性（主机代理17个，任务生命周期14个），分为活性、安全性、完整性和公平性四类，用时态逻辑表达以实现形式化验证。&lt;h4&gt;主要发现&lt;/h4&gt;两个基础模型共同为多AI代理系统行为推理提供统一语义框架，定义的属性能实现系统行为形式化验证，检测协调边缘情况，防止死锁和安全漏洞。&lt;h4&gt;结论&lt;/h4&gt;引入了第一个严格基础、领域无关的框架，用于代理AI系统的系统化分析、设计和部署，确保系统正确性、可靠性和稳健性。&lt;h4&gt;翻译&lt;/h4&gt;代理AI系统，即利用多个自主代理和大语言模型的系统，正被越来越多地用于解决复杂的多步骤任务。这些系统的安全性、安全性和功能性至关重要，特别是在高风险应用中。然而，当前代理间通信生态系统是碎片化的，诸如用于工具访问的模型上下文协议和用于协调的代理到代理等协议被孤立地分析。这种碎片化造成了语义鸿沟，阻碍了对系统属性的严格分析，并引入了架构不协调和可利用的协调问题等风险。为应对这些挑战，我们引入了一个由两个基础模型组成的代理AI系统建模框架。第一个是主机代理模型，它正式化与用户交互、分解任务并通过利用外部代理和工具协调执行的最高级别实体。第二个是任务生命周期模型，它详细说明从创建到完成的各个子任务的状态和转换，提供细粒度的任务管理和错误处理视图。这两个模型共同为多AI代理系统行为推理提供了统一的语义框架。基于此框架，我们为主机代理定义了17个属性，为任务生命周期定义了14个属性，分为活性、安全性、完整性和公平性四类。用时态逻辑表达的这些属性，能够实现系统行为的正式验证，检测协调边缘情况，并防止死锁和安全漏洞。通过这项工作，我们引入了第一个严格基础、领域无关的框架，用于代理AI系统的系统化分析、设计和部署，以确保正确、可靠和稳健的系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agentic AI systems, which leverage multiple autonomous agents and LargeLanguage Models (LLMs), are increasingly used to address complex, multi-steptasks. The safety, security, and functionality of these systems are critical,especially in high-stakes applications. However, the current ecosystem ofinter-agent communication is fragmented, with protocols such as the ModelContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocolfor coordination being analyzed in isolation. This fragmentation creates asemantic gap that prevents the rigorous analysis of system properties andintroduces risks such as architectural misalignment and exploitablecoordination issues. To address these challenges, we introduce a modelingframework for agentic AI systems composed of two foundational models. Thefirst, the host agent model, formalizes the top-level entity that interactswith the user, decomposes tasks, and orchestrates their execution by leveragingexternal agents and tools. The second, the task lifecycle model, details thestates and transitions of individual sub-tasks from creation to completion,providing a fine-grained view of task management and error handling. Together,these models provide a unified semantic framework for reasoning about thebehavior of multi-AI agent systems. Grounded in this framework, we define 17properties for the host agent and 14 for the task lifecycle, categorized intoliveness, safety, completeness, and fairness. Expressed in temporal logic,these properties enable formal verification of system behavior, detection ofcoordination edge cases, and prevention of deadlocks and securityvulnerabilities. Through this effort, we introduce the first rigorouslygrounded, domain-agnostic framework for the systematic analysis, design, anddeployment of correct, reliable, and robust agentic AI systems.</description>
      <author>example@mail.com (Edoardo Allegrini, Ananth Shreekumar, Z. Berkay Celik)</author>
      <guid isPermaLink="false">2510.14133v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Shadow Molecular Dynamics for Flexible Multipole Models</title>
      <link>http://arxiv.org/abs/2510.14132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将阴影分子动力学扩展到柔性多极模型，处理长程静电相互作用，提供稳定高效的原子模拟框架。&lt;h4&gt;背景&lt;/h4&gt;阴影分子动力学是处理具有长程静电相互作用的柔性电荷模型的高效稳定原子模拟框架，但之前实现仅限于原子单极电荷分布。&lt;h4&gt;目的&lt;/h4&gt;扩展阴影分子动力学方法以支持柔性多极模型，实现更准确的原子相互作用模拟。&lt;h4&gt;方法&lt;/h4&gt;推导阴影能量函数、势能和力项的详细表达式，明确包含单极-单极、偶极-单极和偶极-偶极相互作用；将原子单极和偶极视为扩展动力学变量；提出单极固定而偶极柔性的分子动力学方案。&lt;h4&gt;主要发现&lt;/h4&gt;引入额外偶极自由度保留了仅单极阴影分子动力学模拟的稳定性和准确性；扩展的阴影动力学为涉及柔性多极长程相互作用的稳定、计算高效且多功能的分子动力学模拟提供了框架。&lt;h4&gt;结论&lt;/h4&gt;该方法与现代人工智能和机器学习技术结合特别有意义，有助于开发可转移的高精度原子相互作用表示，适用于各种分子系统。&lt;h4&gt;翻译&lt;/h4&gt;阴影分子动力学为具有长程静电相互作用的柔性电荷模型提供了一种高效稳定的原子模拟框架。虽然之前的实现仅限于原子单极电荷分布，但我们将这种方法扩展到了柔性多极模型。我们推导了阴影能量函数、势能和力项的详细表达式，明确包含了单极-单极、偶极-单极和偶极-偶极相互作用。在我们的公式中，原子单极和原子偶极都被视为扩展的动力学变量，与核自由度的传播一起处理。我们证明引入额外的偶极自由度保留了之前在仅单极阴影分子动力学模拟中看到的稳定性和准确性。此外，我们提出了一种阴影分子动力学方案，其中单极电荷保持固定，而偶极保持柔性。我们的扩展阴影动力学为涉及柔性多极之间长程相互作用的稳定、计算高效且多功能的分子动力学模拟提供了框架。这与现代人工智能和机器学习技术结合特别有意义，这些技术越来越多地用于开发原子模拟的物理信息驱动和数据驱动的基础模型。这些模型旨在提供可转移的高精度原子相互作用表示，适用于各种分子系统，这需要准确处理长程电荷相互作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shadow molecular dynamics provide an efficient and stable atomisticsimulation framework for flexible charge models with long-range electrostaticinteractions. While previous implementations have been limited to atomicmonopole charge distributions, we extend this approach to flexible multipolemodels. We derive detailed expressions for the shadow energy functions,potentials, and force terms, explicitly incorporating monopole-monopole,dipole-monopole, and dipole-dipole interactions. In our formulation, bothatomic monopoles and atomic dipoles are treated as extended dynamical variablesalongside the propagation of the nuclear degrees of freedom. We demonstratethat introducing the additional dipole degrees of freedom preserves thestability and accuracy previously seen in monopole-only shadow moleculardynamics simulations. Additionally, we present a shadow molecular dynamicsscheme where the monopole charges are held fixed while the dipoles remainflexible. Our extended shadow dynamics provide a framework for stable,computationally efficient, and versatile molecular dynamics simulationsinvolving long-range interactions between flexible multipoles. This is ofparticular interest in combination with modern artificial intelligence andmachine learning techniques, which are increasingly used to developphysics-informed and data-driven foundation models for atomistic simulations.These models aim to provide transferable, high-accuracy representations ofatomic interactions that are applicable across diverse sets of molecularsystems, which requires accurate treatment of long-range charge interactions.</description>
      <author>example@mail.com (Rae A. Corrigan Grove, Robert Stanton, Michael E. Wall, Anders M. N. Niklasson)</author>
      <guid isPermaLink="false">2510.14132v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Exploratory Causal Inference in SAEnce</title>
      <link>http://arxiv.org/abs/2510.14073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为Neural Effect Search的新方法，可以直接从数据中发现未知的因果效应，解决了传统随机对照试验的局限性。&lt;h4&gt;背景&lt;/h4&gt;随机对照试验是科学的重要支柱，但它们依赖于手工制作的假设和昂贵的分析。这些限制阻碍了大规模因果效应估计，可能导致依赖于流行但不完整的假设。&lt;h4&gt;目的&lt;/h4&gt;直接从数据中发现治疗的未知效应。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的基础模型将试验中的非结构化数据转换为有意义的表示，通过稀疏自编码器解释这些表示，并引入Neural Effect Search这一新颖的递归过程，通过渐进分层解决多重测试问题和效应纠缠问题。&lt;h4&gt;主要发现&lt;/h4&gt;在半合成实验中评估了算法的稳健性，并在实验生态学背景下展示了在真实世界科学试验中首次成功的无监督因果效应识别。&lt;h4&gt;结论&lt;/h4&gt;Neural Effect Search方法成功解决了在神经水平发现显著因果效应的挑战。&lt;h4&gt;翻译&lt;/h4&gt;随机对照试验是科学的重要支柱；然而，它们依赖于手工制作的假设和昂贵的分析。这些限制阻碍了大规模因果效应估计，可能导致依赖于流行但不完整的假设。我们提出直接从数据中发现治疗的未知效应。为此，我们通过预训练的基础模型将试验中的非结构化数据转换为有意义的表示，并通过稀疏自编码器解释它们。然而，由于多重测试问题和效应纠缠，在神经水平发现显著的因果效应并不简单。为了解决这些挑战，我们引入了Neural Effect Search，这是一种新颖的递归过程，通过渐进分层解决了这两个问题。在半合成实验中评估了我们算法的稳健性后，我们在实验生态学的背景下展示了在真实世界科学试验中首次成功的无监督因果效应识别。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Randomized Controlled Trials are one of the pillars of science; nevertheless,they rely on hand-crafted hypotheses and expensive analysis. Such constraintsprevent causal effect estimation at scale, potentially anchoring on popular yetincomplete hypotheses. We propose to discover the unknown effects of atreatment directly from data. For this, we turn unstructured data from a trialinto meaningful representations via pretrained foundation models and interpretthem via a sparse autoencoder. However, discovering significant causal effectsat the neural level is not trivial due to multiple-testing issues and effectsentanglement. To address these challenges, we introduce Neural Effect Search, anovel recursive procedure solving both issues by progressive stratification.After assessing the robustness of our algorithm on semi-synthetic experiments,we showcase, in the context of experimental ecology, the first successfulunsupervised causal effect identification on a real-world scientific trial.</description>
      <author>example@mail.com (Tommaso Mencattini, Riccardo Cadei, Francesco Locatello)</author>
      <guid isPermaLink="false">2510.14073v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Context-Selective State Space Models: Feedback is All You Need</title>
      <link>http://arxiv.org/abs/2510.14027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了COFFEE模型，一种新颖的时变状态空间模型，通过状态反馈实现上下文相关的选择性，有效捕获长距离依赖关系，并在多项任务上取得了优于现有S6模型的结果。&lt;h4&gt;背景&lt;/h4&gt;Transformers模型基于注意力机制，是大多数基础模型的骨干，但它们具有二次复杂度，并且在处理输入序列中的长距离依赖关系时存在困难。状态空间模型(SSMs)提供了一种高效的替代方案，其中S6模块在长序列基准测试上取得了最先进的结果。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理长距离依赖关系的高效序列模型，解决Transformers模型的二次复杂度问题，并超越现有状态空间模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出COFFEE(COntext From FEEdback)模型，一种新颖的时变SSM，结合状态反馈以实现上下文相关的选择性。与S6不同，COFFEE从内部状态计算选择性，该状态作为序列历史的紧凑表示，使模型能够根据积累的上下文调节其动态。此外，采用高效的模型参数化方法消除冗余，实现更紧凑和可训练的公式。&lt;h4&gt;主要发现&lt;/h4&gt;在归纳头任务上，COFFEE与S6相比，使用少两个数量级的参数和训练序列实现了接近完美的准确性；在MNIST上，仅用3585个参数就达到了97%的准确率，大大优于S6在相同架构上的表现。&lt;h4&gt;结论&lt;/h4&gt;状态反馈是构建可扩展和高效序列模型的关键机制，COFFEE模型通过结合状态反馈和高效参数化，显著提升了序列建模能力，特别是在处理长距离依赖关系方面。&lt;h4&gt;翻译&lt;/h4&gt;Transformers模型由注意力机制驱动，是大多数基础模型的骨干，但它们受二次复杂度的困扰，并且在处理输入序列中的长距离依赖关系时存在困难。最近的研究表明，状态空间模型(SSMs)提供了一种高效的替代方案，其中S6模块作为Mamba架构的核心，在长序列基准测试上取得了最先进的结果。在本文中，我们介绍了COFFEE(COntext From FEEdback)模型，一种新颖的时变SSM，它结合了状态反馈以实现上下文相关的选择性，同时仍允许并行实现。而S6的选择性机制仅依赖于当前输入，COFFEE从内部状态计算选择性，该状态作为序列历史的紧凑表示。这种转变使模型能够根据积累的上下文调节其动态，提高其捕获长距离依赖关系的能力。除了状态反馈外，我们还采用了一种高效的模型参数化方法，消除了S6中存在的冗余，导致更紧凑和可训练的公式。在归纳头任务上，COFFEE与S6相比，使用少两个数量级的参数和训练序列实现了接近完美的准确性。在MNIST上，COFFEE在相同架构上大大优于S6，仅用3585个参数就达到了97%的准确率。这些结果展示了状态反馈作为构建可扩展和高效序列模型的关键机制的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers, powered by the attention mechanism, are the backbone of mostfoundation models, yet they suffer from quadratic complexity and difficultiesin dealing with long-range dependencies in the input sequence. Recent work hasshown that state space models (SSMs) provide an efficient alternative, with theS6 module at the core of the Mamba architecture achieving state-of-the-artresults on long-sequence benchmarks. In this paper, we introduce the COFFEE(COntext From FEEdback) model, a novel time-varying SSM that incorporates statefeedback to enable context-dependent selectivity, while still allowing forparallel implementation. Whereas the selectivity mechanism of S6 only dependson the current input, COFFEE computes it from the internal state, which servesas a compact representation of the sequence history. This shift allows themodel to regulate its dynamics based on accumulated context, improving itsability to capture long-range dependencies. In addition to state feedback, weemploy an efficient model parametrization that removes redundancies present inS6 and leads to a more compact and trainable formulation. On the induction headtask, COFFEE achieves near-perfect accuracy with two orders of magnitude fewerparameters and training sequences compared to S6. On MNIST, COFFEE largelyoutperforms S6 within the same architecture, reaching 97% accuracy with only3585 parameters. These results showcase the role of state feedback as a keymechanism for building scalable and efficient sequence models.</description>
      <author>example@mail.com (Riccardo Zattra, Giacomo Baggio, Umberto Casti, Augusto Ferrante, Francesco Ticozzi)</author>
      <guid isPermaLink="false">2510.14027v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</title>
      <link>http://arxiv.org/abs/2510.13721v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NExT-OMNI是一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任何到任何的跨模态生成和多轮交互，克服了现有自回归架构的局限性。&lt;h4&gt;背景&lt;/h4&gt;下一代多模态基础模型将成为人工通用智能系统的核心，但现有多模态模型受限于自回归架构，无法平衡整合理解与生成能力。混合和解耦策略虽被探索，但其冗余设计限制了在广泛场景如跨模态检索中的应用。&lt;h4&gt;目的&lt;/h4&gt;引入NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任何到任何的理解和生成，并扩展应用场景。&lt;h4&gt;方法&lt;/h4&gt;利用度量诱导的概率路径和动力学最优速度，原生支持任何到任何的理解和生成，增强响应效率；通过简洁的统一表示而非任务解耦设计实现更广泛应用；在大规模交错文本、图像、视频和音频数据上训练。&lt;h4&gt;主要发现&lt;/h4&gt;NExT-OMNI在多模态生成和理解基准测试中具有竞争力，在多模态交互和跨模态检索方面优于之前的统一模型，展现了其作为下一代多模态基础模型的架构优势。&lt;h4&gt;结论&lt;/h4&gt;发布训练细节、数据协议，并开源代码和模型检查点，以促进多模态基础模型领域的进一步研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;能够进行任何到任何跨模态生成和多轮交互的下一代多模态基础模型将成为人工通用智能系统的核心组成部分，在人机交互中发挥关键作用。然而，大多数现有多模态模型仍受限于自回归架构，其固有局限性阻碍了理解与生成能力的平衡整合。虽然混合和解耦策略已被探索用于在统一框架内分别解决这些问题，但它们的冗余、非集成设计限制了它们在更广泛场景（如跨模态检索）中的适用性。在这项工作中，我们引入了NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。通过利用度量诱导的概率路径和动力学最优速度，NExT-OMNI原生支持任何到任何的理解和生成，同时通过简洁的统一表示而非任务解耦设计，实现更广泛的应用场景，增强响应效率。在大规模交错文本、图像、视频和音频数据上训练后，NExT-OMNI在多模态生成和理解基准测试中具有竞争力，同时在多模态交互和跨模态检索方面优于之前的统一模型，凸显了其作为下一代多模态基础模型的架构优势。为进一步推进研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation multimodal foundation models capable of any-to-anycross-modal generation and multi-turn interaction will serve as core componentsof artificial general intelligence systems, playing a pivotal role inhuman-machine interaction. However, most existing multimodal models remainconstrained by autoregressive architectures, whose inherent limitations preventa balanced integration of understanding and generation capabilities. Althoughhybrid and decoupling strategies have been explored to address these taskswithin unified frameworks separately, their redundant, non-integrated designslimit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation modelthat achieves unified modeling through discrete flow paradigms. By leveragingmetric-induced probability paths and kinetic optimal velocities, NExT-OMNInatively supports any-to-any understanding and generation with enhancedresponse efficiency, while enabling broader application scenarios throughconcise unified representations rather than task-decoupled designs. Trained onlarge-scale interleaved text, image, video, and audio data, NExT-OMNI deliverscompetitive performance on multimodal generation and understanding benchmarks,while outperforming prior unified models in multi-turn multimodal interactionand cross-modal retrieval, highlighting its architectural advantages as anext-generation multimodal foundation model. To advance further research, werelease training details, data protocols, and open-source both the code andmodel checkpoints.</description>
      <author>example@mail.com (Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2510.13721v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning</title>
      <link>http://arxiv.org/abs/2510.13909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种知识推理语言模型(KRLM)，用于解决归纳知识图谱推理中LLM知识与KG上下文协调的问题。通过设计KRL指令格式、KRL分词器、KRL注意力层和结构感知的下一个实体预测器，模型能够在KGR过程中实现LLM知识与KG上下文的统一协调，有效约束LLM的生成幻觉，提高推理结果的可信度。&lt;h4&gt;背景&lt;/h4&gt;归纳知识图谱推理旨在发现包含未知实体和关系的开放域知识图谱中的事实，这给KGR模型在理解不确定的KG组件方面带来了挑战。现有研究提出了知识图谱基础模型来处理这种不确定性，而大型语言模型在开放域知识推理方面展示了强大能力。最新的研究集中在基于LLM的KGFMs上，这些模型整合了LLM知识与KG上下文进行归纳KGR。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于LLM的KGR方法中LLM知识被稀疏KG上下文掩盖导致知识扭曲的问题，以及难以完全约束LLM生成幻觉的问题，提出一个知识推理语言模型(KRLM)，在KGR过程中实现LLM知识与KG上下文的统一协调。&lt;h4&gt;方法&lt;/h4&gt;设计了一种知识推理语言(KRL)指令格式和KRL分词器，以对齐LLM知识与KG表示；提出了一种KRL注意力层，通过动态知识记忆机制协调内在的LLM知识与额外的KG上下文；提出了一种结构感知的下一个实体预测器，将推理结果严格限制在可信的知识域内。&lt;h4&gt;主要发现&lt;/h4&gt;在25个真实世界的归纳KGR数据集上进行了广泛的实验，结果表明所提出的KRLM在零样本推理和微调场景下都具有显著的优越性。&lt;h4&gt;结论&lt;/h4&gt;KRLM模型有效地解决了LLM知识与KG上下文协调的问题，通过结构感知的下一个实体预测器提高了推理结果的可信度，在多个数据集上表现优异，证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;归纳知识图谱推理旨在发现包含未知实体和关系的开放域知识图谱中的事实，这给KGR模型在理解不确定的KG组件方面带来了挑战。现有研究提出了知识图谱基础模型，这些模型学习跨知识图谱的结构不变性来处理这种不确定性。最近，大型语言模型在开放域知识推理方面展示了强大的能力。因此，最新的研究集中在基于LLM的知识图谱基础模型上，这些模型整合了LLM知识与KG上下文进行归纳KGR。然而，LLM的内在知识可能被稀疏的KG上下文掩盖，导致LLM知识扭曲，这可能对模型推理造成不可逆的损害。此外，现有的基于LLM的KGR方法仍然难以完全约束LLM中的生成幻觉，严重限制了推理结果的可信度。为解决这些局限性，我们提出了一种知识推理语言模型(KRLM)，在KGR过程中实现LLM知识与KG上下文的统一协调。具体来说，我们设计了一种知识推理语言(KRL)指令格式和KRL分词器，以对齐LLM知识与KG表示。然后，我们提出了一种KRL注意力层，通过动态知识记忆机制协调内在的LLM知识与额外的KG上下文。最后，提出了一种结构感知的下一个实体预测器，将推理结果严格限制在可信的知识域内。在25个真实世界的归纳KGR数据集上的广泛实验结果表明，所提出的KRLM在零样本推理和微调场景下都具有显著的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inductive Knowledge Graph Reasoning (KGR) aims to discover facts inopen-domain KGs containing unknown entities and relations, which poses achallenge for KGR models in comprehending uncertain KG components. Existingstudies have proposed Knowledge Graph Foundation Models (KGFMs) that learnstructural invariances across KGs to handle this uncertainty. Recently, LargeLanguage Models (LLMs) have demonstrated strong capabilities for open-domainknowledge reasoning. As a result, the latest research has focused on LLM-basedKGFMs that integrate LLM knowledge with KG context for inductive KGR. However,the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,leading to LLM knowledge distortion, which can cause irreversible damage tomodel reasoning. Moreover, existing LLM-based KGR methods still struggle tofully constrain generative hallucinations in LLMs, severely limiting thecredibility of reasoning results. To address these limitations, we propose aKnowledge Reasoning Language Model (KRLM) that achieves unified coordinationbetween LLM knowledge and KG context throughout the KGR process. Specifically,we design a Knowledge Reasoning Language (KRL) instruction format and a KRLtokenizer to align LLM knowledge with KG representations. Then, we propose aKRL attention layer that coordinates intrinsic LLM knowledge with additional KGcontext through a dynamic knowledge memory mechanism. Finally, astructure-aware next-entity predictor is proposed, which strictly constrainsthe reasoning results within a trustworthy knowledge domain. Extensiveexperimental results on 25 real-world inductive KGR datasets demonstrate thesignificant superiority of the proposed KRLM\footnote{Our source codes areavailable at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shotreasoning and fine-tuning scenarios.</description>
      <author>example@mail.com (Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Zhongyuan Wang, Jichen Zhang, Shirui Pan, Xindong Wu)</author>
      <guid isPermaLink="false">2510.13909v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.14810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SPHeRe（结构投影Hebbian表示）方法，一种新型无监督学习技术，通过整合正交性和结构信息保留解决了传统Hebbian学习在机器学习中的局限性，在多个任务中取得了优异表现。&lt;h4&gt;背景&lt;/h4&gt;Hebbian学习是一种描述神经元通过重复刺激调整连接的生物原理，但在机器学习应用中存在连接更新无约束和缺乏反馈中介考虑等问题，限制了其在复杂网络架构和任务中的有效扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服传统Hebbian学习局限性的无监督学习方法，使其能够有效扩展到复杂网络架构和任务中。&lt;h4&gt;方法&lt;/h4&gt;SPHeRe通过局部的辅助非线性块整合正交性和结构信息保留，结构信息保留的损失通过辅助轻量级投影反向传播到输入（充当反馈中介），正交性约束则确保更新幅度的有界性。&lt;h4&gt;主要发现&lt;/h4&gt;SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试中达到无监督突触可塑性方法的最新性能；在持续学习和迁移学习场景中表现有效；图像重建任务证明了提取特征的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖严格反向传播的高效且受生物启发的学习算法的可能性，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;Hebbian学习是一种生物原理，直观地描述了神经元如何通过重复刺激来调整其连接。然而，当应用于机器学习时，由于连接更新的无约束性和缺乏对反馈中介的考虑，它存在严重问题。这些缺点限制了其在复杂网络架构和任务中的有效扩展。为此，我们在此引入结构投影Hebbian表示（SPHeRe），一种新型无监督学习方法，它通过一个局部的辅助非线性块整合了正交性和结构信息保留。结构信息保留的损失通过一个辅助的轻量级投影反向传播到输入，这个投影在概念上充当反馈中介，而正交性约束则考虑了更新幅度的有界性。大量实验结果表明，SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试的无监督突触可塑性方法中达到了最先进性能。此外，该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。这项工作证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖于严格反向传播的高效且受生物启发的学习算法的可能性。我们的代码可在https://github.com/brain-intelligence-lab/SPHeRe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hebbian learning is a biological principle that intuitively describes howneurons adapt their connections through repeated stimuli. However, when appliedto machine learning, it suffers serious issues due to the unconstrained updatesof the connections and the lack of accounting for feedback mediation. Suchshortcomings limit its effective scaling to complex network architectures andtasks. To this end, here we introduce the Structural Projection HebbianRepresentation (SPHeRe), a novel unsupervised learning method that integratesorthogonality and structural information preservation through a local auxiliarynonlinear block. The loss for structural information preservationbackpropagates to the input through an auxiliary lightweight projection thatconceptually serves as feedback mediation while the orthogonality constraintsaccount for the boundedness of updating magnitude. Extensive experimentalresults show that SPHeRe achieves SOTA performance among unsupervised synapticplasticity approaches on standard image classification benchmarks, includingCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strongeffectiveness in continual learning and transfer learning scenarios, and imagereconstruction tasks show the robustness and generalizability of the extractedfeatures. This work demonstrates the competitiveness and potential of Hebbianunsupervised learning rules within modern deep learning frameworks,demonstrating the possibility of efficient and biologically inspired learningalgorithms without the strong dependence on strict backpropagation. Our code isavailable at https://github.com/brain-intelligence-lab/SPHeRe.</description>
      <author>example@mail.com (Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu)</author>
      <guid isPermaLink="false">2510.14810v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning to Recognize Quantum Phases of Matter</title>
      <link>http://arxiv.org/abs/2510.14742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用无监督学习方法来确定多体系统的量子相图，该方法能够自主识别并可能揭示量子物质的新相位。&lt;h4&gt;背景&lt;/h4&gt;将多体系统的量子相图绘制视为学习问题，需要根据某种分类标准对其基态进行标记以定义不同的相。&lt;h4&gt;目的&lt;/h4&gt;采用无监督学习方法来确定多体系统的量子相图，算法无需访问任何预先标记的状态。&lt;h4&gt;方法&lt;/h4&gt;算法直接处理量子态，基于量子态之间的保真度相似性标准对基态配置进行分组。使用基于谱聚类的无监督学习算法，并结合'轮廓'和'肘部'方法来确定相位的最佳数量。&lt;h4&gt;主要发现&lt;/h4&gt;通过两个具体的自旋-1/2链进行基准测试，发现基于谱聚类的无监督学习算法能够准确重现相图。&lt;h4&gt;结论&lt;/h4&gt;无监督学习可以自主识别并可能揭示量子物质的新相位，为量子相图的确定提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;在哈密顿量参数空间中绘制多体系统的量子相图可以被视为一个学习问题，这需要根据定义相位的某种分类标准来标记相应的基态。在本工作中，我们采用无监督学习方法，其中算法无法访问任何预先标记的状态，作为确定多体系统量子相图的一种工具。该算法直接处理量子态：给定不同哈密顿量参数的基态配置，该过程基于量子态之间保真度的相似性标准揭示了对它们进行分组的最重要的方式，这种标准即使通过实验也容易估计。我们使用两个特定的自旋-1/2链来基准测试我们的方法，其状态通过张量网络技术确定。我们发现，基于谱聚类的无监督学习算法，结合用于确定相位最佳数量的'轮廓'和'肘部'方法，可以准确重现相图。我们的结果表明，无监督学习如何能够自主识别并可能揭示量子物质的新相位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drawing the quantum phase diagram of a many-body system in the parameterspace of its Hamiltonian can be seen as a learning problem, which implieslabelling the corresponding ground states according to some classificationcriterium that defines the phases. In this work we adopt unsupervised learning,where the algorithm has no access to any priorly labeled states, as a tool fordetermining quantum phase diagrams of many-body systems. The algorithm directlyworks with quantum states: given the ground-state configurations for differentvalues of the Hamiltonian parameters, the process uncovers the most significantway of grouping them based on a similarity criterion that refers to thefidelity between quantum states, that can be easily estimated, evenexperimentally. We benchmark our method with two specific spin-$\frac{1}{2}$chains, with states determined via tensor network techniques. We find thatunsupervised learning algorithms based on spectral clustering, combined with``silhouette'' and ``elbow'' methods for determining the optimal number ofphases, can accurately reproduce the phase diagrams. Our results show howunsupervised learning can autonomously recognize and possibly unveil novelphases of quantum matter.</description>
      <author>example@mail.com (Mehran Khosrojerdi, Alessandro Cuccoli, Paola Verrucchi, Leonardo Banchi)</author>
      <guid isPermaLink="false">2510.14742v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach</title>
      <link>http://arxiv.org/abs/2510.14415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文开发了一个敏感性分析框架，将完全观测网络中的源数据的平均总处理效应转移到网络完全未知的目标数据中，以估计政策的平均社会影响。&lt;h4&gt;背景&lt;/h4&gt;研究假设源数据和目标数据共享相同的条件均值结果（协变量漂移类型假设），但由于目标网络未被观测，这一假设本身不足以确定目标数据的ATTE。&lt;h4&gt;目的&lt;/h4&gt;解决目标网络未观测情况下如何估计ATTE的问题，通过基于目标网络度分布不确定性的敏感性分析来构建ATTE的界限。&lt;h4&gt;方法&lt;/h4&gt;考虑基于目标网络度分布不确定性的敏感性分析，不确定性程度由给定参考度分布的Wasserstein距离衡量；使用基于线性规划的估计量构建目标ATTE的界限；通过函数delta方法推导界限估计量的极限分布；开发wild bootstrap方法来近似该分布。&lt;h4&gt;主要发现&lt;/h4&gt;构建了目标ATTE的界限估计量，推导了其极限分布，并开发了wild bootstrap方法来近似该分布。&lt;h4&gt;结论&lt;/h4&gt;该框架允许在目标网络完全未知的情况下，通过敏感性分析来估计政策的平均社会影响。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文开发了一个敏感性分析框架，将具有完全观测网络的源数据中的平均总处理效应（ATTE）转移到网络完全未知的目标数据中。ATTE代表了对数据集中每个个体实施政策的平均社会影响。我们提出了一个协变量漂移类型的假设，即源数据和目标数据共享相同的条件均值结果。然而，由于目标网络未被观测，这一假设本身不足以确定目标数据的ATTE。为了解决这个问题，我们考虑了基于目标网络度分布不确定性的敏感性分析，其中不确定性程度由给定参考度分布的Wasserstein距离来衡量。然后，我们使用基于线性规划的估计量构建了目标ATTE的界限。通过函数delta方法推导了界限估计量的极限分布，并开发了wild bootstrap方法来近似该分布。作为一个实证说明，我们重新研究了Cai等人（2015）关于中国农民天气保险采用的社会网络实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper develops a sensitivity analysis framework that transfers theaverage total treatment effect (ATTE) from source data with a fully observednetwork to target data whose network is completely unknown. The ATTE representsthe average social impact of a policy that assigns the treatment to everyindividual in the dataset. We postulate a covariate-shift type assumption thatboth source and target datasets share the same conditional mean outcome.However, because the target network is unobserved, this assumption alone is notsufficient to pin down the ATTE for the target data. To address this issue, weconsider a sensitivity analysis based on the uncertainty of the targetnetwork's degree distribution, where the extent of uncertainty is measured bythe Wasserstein distance from a given reference degree distribution. We thenconstruct bounds on the target ATTE using a linear programming-based estimator.The limiting distribution of the bound estimator is derived via the functionaldelta method, and we develop a wild bootstrap approach to approximate thedistribution. As an empirical illustration, we revisit the social networkexperiment on farmers' weather insurance adoption in China by Cai et al.(2015).</description>
      <author>example@mail.com (Tadao Hoshino)</author>
      <guid isPermaLink="false">2510.14415v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Glitch noise classification in KAGRA O3GK observing data using unsupervised machine learning</title>
      <link>http://arxiv.org/abs/2510.14291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures, accepted to Physics Letters B&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了使用无监督机器学习方法对KAGRA O3GK数据中的非平稳噪声进行图像分类的有效性，成功识别出八种不同的故障噪声类别，提高了引力波观测的可靠性。&lt;h4&gt;背景&lt;/h4&gt;引力波干涉仪受到各种非平稳噪声（称为故障噪声）的干扰，这些噪声影响数据分析和干涉仪的灵敏度。&lt;h4&gt;目的&lt;/h4&gt;准确识别和分类故障噪声，以提高引力波观测的可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用变分自编码器(VAE)结合谱聚类的无监督机器学习方法，对KAGRA O3GK数据中的非平稳噪声图像进行分类，将潜在变量降维后在三维空间中可视化并进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;成功识别出八种不同的故障噪声类别，并更好地理解了KAGRA在O3GK期间的故障噪声特征。&lt;h4&gt;结论&lt;/h4&gt;无监督学习在故障噪声分类方面显示出潜力，这有助于干涉仪升级和未来第三代引力波天文台的发展。&lt;h4&gt;翻译&lt;/h4&gt;引力波干涉仪受到各种类型的非平稳噪声干扰，称为故障噪声，这些噪声影响数据分析和干涉仪灵敏度。准确识别和分类故障噪声对于提高引力波观测的可靠性至关重要。在本研究中，我们展示了无监督机器学习在KAGRA O3GK数据中分类含有非平稳噪声图像的有效性。使用变分自编码器(VAE)结合谱聚类，我们识别出八种不同的故障噪声类别。从VAE获得的潜在变量被降维，在三维空间中进行可视化，并使用谱聚类进行分类，以便更好地理解KAGRA在O3GK期间的故障噪声特征。我们的结果强调了无监督学习在高效故障噪声分类方面的潜力，这可能反过来促进干涉仪升级和未来第三代引力波天文台的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gravitational wave interferometers are disrupted by various types ofnonstationary noise, referred to as glitch noise, that affect data analysis andinterferometer sensitivity. The accurate identification and classification ofglitch noise are essential for improving the reliability of gravitational waveobservations. In this study, we demonstrated the effectiveness of unsupervisedmachine learning for classifying images with nonstationary noise in the KAGRAO3GK data. Using a variational autoencoder (VAE) combined with spectralclustering, we identified eight distinct glitch noise categories. The latentvariables obtained from VAE were dimensionally compressed, visualized inthree-dimensional space, and classified using spectral clustering to betterunderstand the glitch noise characteristics of KAGRA during the O3GK period.Our results highlight the potential of unsupervised learning for efficientglitch noise classification, which may in turn potentially facilitateinterferometer upgrades and the development of future third-generationgravitational wave observatories.</description>
      <author>example@mail.com (Shoichi Oshino, Yusuke Sakai, Marco Meyer-Conde, Takashi Uchiyama, Yousuke Itoh, Yutaka Shikano, Yoshikazu Terada, Hirotaka Takahashi)</author>
      <guid isPermaLink="false">2510.14291v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data</title>
      <link>http://arxiv.org/abs/2510.14145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的稳健非参数聚类验证框架HD-BWDM，用于解决高维或受污染数据中确定适当聚类数量的问题。&lt;h4&gt;背景&lt;/h4&gt;确定无监督学习中适当的聚类数量是统计学和数据科学中的核心问题。传统的有效性指标如Calinski-Harabasz、Silhouette和Davies-Bouldin依赖于基于质心的距离，在高维或受污染数据中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的稳健的非参数聚类验证框架HD-BWDM，将BWDM标准扩展到高维空间，解决传统方法在高维数据中的局限性。&lt;h4&gt;方法&lt;/h4&gt;HD-BWDM整合随机投影和主成分分析缓解维度诅咒，应用修剪聚类和基于medoid的距离确保对离群点的稳健性。作者推导了理论结果，证明在Johnson-Lindenstrauss嵌入下的一致性和收敛性。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的模拟表明，HD-BWDM在高维投影和污染情况下保持稳定性和可解释性，为传统基于质心的验证标准提供了稳健的替代方案。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为现代高维应用中的非参数聚类提供了理论基础充分、计算效率高的停止规则。&lt;h4&gt;翻译&lt;/h4&gt;确定无监督学习中适当的聚类数量是统计学和数据科学中的核心问题。传统的有效性指标如Calinski-Harabasz、Silhouette和Davies-Bouldin依赖于基于质心的距离，因此在高维或受污染数据中表现不佳。本文提出了一种新的稳健的非参数聚类验证框架，即高维组内组间距离中位数（HD-BWDM），将最近引入的BWDM标准扩展到高维空间。HD-BWDM整合了随机投影和主成分分析来缓解维度诅咒，并应用修剪聚类和基于medoid的距离以确保对离群点的稳健性。作者推导了理论结果，证明了在Johnson-Lindenstrauss嵌入下的一致性和收敛性。广泛的模拟表明，在高维投影和污染情况下，HD-BWDM保持稳定性和可解释性，为传统的基于质心的验证标准提供了一个稳健的替代方案。所提出的方法为现代高维应用中的非参数聚类提供了理论基础充分、计算效率高的停止规则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Determining the appropriate number of clusters in unsupervised learning is acentral problem in statistics and data science. Traditional validity indicessuch as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend oncentroid-based distances and therefore degrade in high-dimensional orcontaminated data. This paper proposes a new robust, nonparametric clusteringvalidation framework, the High-Dimensional Between-Within Distance Median(HD-BWDM), which extends the recently introduced BWDM criterion tohigh-dimensional spaces. HD-BWDM integrates random projection and principalcomponent analysis to mitigate the curse of dimensionality and applies trimmedclustering and medoid-based distances to ensure robustness against outliers. Wederive theoretical results showing consistency and convergence underJohnson-Lindenstrauss embeddings. Extensive simulations demonstrate thatHD-BWDM remains stable and interpretable under high-dimensional projections andcontamination, providing a robust alternative to traditional centroid-basedvalidation criteria. The proposed method provides a theoretically grounded,computationally efficient stopping rule for nonparametric clustering in modernhigh-dimensional applications.</description>
      <author>example@mail.com (Mohammed Baragilly, Hend Gabr)</author>
      <guid isPermaLink="false">2510.14145v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为RoboGPT-R1的两阶段微调框架，用于提升具身智能体的推理能力，使其能够更好地完成复杂环境中的长时程操作任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和基于监督微调的视觉语言模型在规划任务中取得成功，但在复杂现实环境中执行长时程操作任务时仍面临挑战，原因是它们有限的常识和推理能力。将通用视觉语言模型通过监督微调对齐到机器人规划任务存在泛化能力差和对物理理解不足的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，提升具身智能体在复杂环境中的推理和规划能力，特别是完成长时程操作任务的能力。&lt;h4&gt;方法&lt;/h4&gt;提出RoboGPT-R1框架，包含两个阶段：首先通过监督训练使用专家序列获取基础知识，然后利用强化学习解决模型在视觉空间理解和推理方面的不足。同时设计了基于规则的奖励函数，考虑长时程性能和环境动作约束，并在Qwen2.5-VL-3B上训练推理模型。&lt;h4&gt;主要发现&lt;/h4&gt;在EmbodiedBench基准测试上，训练的推理模型显著优于更大规模的GPT-4o-mini模型，性能高出21.33%；同时超越了在Qwen2.5-VL-7B上训练的其他工作，高出20.33%。&lt;h4&gt;结论&lt;/h4&gt;RoboGPT-R1框架有效提升了具身智能体的推理能力和规划能力，使其能够更好地完成复杂环境中的长时程操作任务。&lt;h4&gt;翻译&lt;/h4&gt;提升具身智能体的推理能力对于机器人在长时程操作任务中成功完成复杂的人类指令至关重要。尽管基于监督微调的大型语言模型和视觉语言模型在规划任务中取得了成功，但由于常识和推理能力的限制，它们在复杂现实环境中执行长时程操作任务时仍面临挑战。考虑到通过监督微调将通用视觉语言模型对齐到机器人规划任务存在泛化能力差和物理理解不足的问题，我们提出了RoboGPT-R1，这是一个用于具身规划的两阶段微调框架。在该框架中，监督训练通过专家序列获取基础知识，随后使用强化学习解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，同时考虑长时程性能和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，高出21.33%，并超越了在Qwen2.5-VL-7B上训练的其他工作，高出20.33%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决提升具身智能体在复杂长视野操作任务中的推理能力问题。当前基于监督微调的大语言模型在真实世界环境中执行长期任务时面临泛化能力不足和物理理解有限的问题。这一问题在现实中非常重要，因为机器人需要处理如'打扫厨房'或'准备晚餐'等复杂、长期的指令，而现有方法难以在动态环境中有效适应和自我纠正。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有SFT-only范式的局限性，包括缺乏环境适应能力和奖励函数设计不足。他们借鉴了强化学习在其他领域（如视频推理、数学推理）的成功应用，以及DeepSeek-R1中的'aha moment'概念。具体设计上，作者结合了REBP项目中的数据集和GRPO算法，同时创新性地设计了包含格式奖励和LCS准确奖励的奖励函数，以解决多步推理任务中的物理理解和动作序列一致性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两阶段训练框架结合监督微调和强化学习的优势，并设计针对具身任务的奖励函数。整体流程包括：1)数据准备阶段，使用从Gemini-2.0-flash提炼的SFT数据集和增强的RFT数据集；2)两阶段训练，第一阶段SFT赋予模型基础规划能力，第二阶段使用GRPO进行强化微调提升推理和泛化能力；3)奖励设计，结合格式奖励(评估结构完整性和动作有效性)和LCS准确奖励(关注动作序列顺序)；4)在EmbodiedBench基准上评估性能，包括域内(EB-ALFRED)和域外(EB-Habitat)场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段训练框架，结合SFT和GRPO强化学习；2)创新的奖励函数设计，包含格式奖励和LCS准确奖励；3)仅使用3B参数的小型模型实现高性能；4)采用零样本处理提高训练效率和泛化能力。相比之前的工作，RoboGPT-R1在EB-ALFRED基准上比GPT-4o-mini高21.33%，比其他基于Qwen2.5-VL-7B的工作高20.33%，特别是在长视野任务上达到50%的准确率，显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoboGPT-R1通过结合监督微调和强化学习的两阶段训练框架，以及针对具身任务设计的基于规则的奖励函数，显著提升了小型视觉语言模型在复杂长视野机器人规划任务中的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improving the reasoning capabilities of embodied agents is crucial for robotsto complete complex human instructions in long-view manipulation taskssuccessfully. Despite the success of large language models and vision languagemodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continuefacing challenges in performing long-horizon manipulation tasks in complexreal-world environments, owing to their restricted common sense and reasoningcapabilities. Considering that aligning general-purpose vision language modelsto robotic planning tasks via supervised fine-tuning suffers from poorgeneralization and insufficient physical understanding, we propose RoboGPT-R1,a two-stage fine-tuning framework for embodied planning. In this framework,supervised training acquires foundational knowledge through expert sequences,followed by RL to address the model's shortcomings in visual-spatialunderstanding and reasoning. To achieve physical understanding and actionsequence consistency in multi-step reasoning tasks, we design a rule-basedreward function that simultaneously considers long-horizon performance andaction constraint in the environment. The reasoning model, trained onQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on theEmbodiedBench benchmark.</description>
      <author>example@mail.com (Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li)</author>
      <guid isPermaLink="false">2510.14828v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatially anchored Tactile Awareness for Robust Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2510.14647v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SaTA的空间锚定触觉感知方法，用于解决灵巧操作中的高精度几何推理问题。该方法通过将触觉特征锚定到手部运动学框架，实现了无需物体模型或显式姿态估计的精确几何推理。&lt;h4&gt;背景&lt;/h4&gt;灵巧操作需要精确的几何推理，但现有的视觉-触觉学习方法在处理亚毫米精度任务时存在困难，而传统基于模型的方法可以轻松处理这些任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用触觉信号的感知丰富性及其与手部运动学空间关系的框架，以实现高精度的灵巧操作。&lt;h4&gt;方法&lt;/h4&gt;提出了SaTA（Spatially-anchored Tactile Awareness for dexterous manipulation）框架，一种端到端策略框架，通过正向运动学将触觉特征锚定到手部运动学框架中。&lt;h4&gt;主要发现&lt;/h4&gt;空间锚定的触觉表示使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。SaTA在多个基准测试中显著优于强视觉-触觉基线，成功率提高高达30个百分点，任务完成时间减少27%。&lt;h4&gt;结论&lt;/h4&gt;SaTA通过将触觉特征锚定到手部运动学框架，成功解决了现有学习框架在处理高精度灵巧操作任务时的局限性，实现了无需物体模型或显式姿态估计的精确几何推理。&lt;h4&gt;翻译&lt;/h4&gt;灵巧操作需要精确的几何推理，然而现有的视觉-触觉学习方法在处理亚毫米精度任务时存在困难，而这些任务对于传统基于模型的方法来说则是常规操作。我们确定了一个关键限制：虽然触觉传感器提供了丰富的接触信息，但现有学习框架未能有效利用触觉信号的感知丰富性及其与手部运动学的空间关系。我们认为理想的触觉表示应将接触测量明确地锚定在稳定的参考框架中，同时保留详细的感官信息，使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。我们引入了SaTA（用于灵巧操作的空间锚定触觉感知），一种端到端策略框架，通过正向运动学将触觉特征明确锚定到手部运动学框架，无需物体模型或显式姿态估计即可实现准确的几何推理。我们的关键见解是空间锚定的触觉表示使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。我们在具有挑战性的灵巧操作任务上验证了SaTA，包括自由空间中的双臂USB-C连接（需要亚毫米级对齐精度）、需要精确螺纹啮合和旋转控制的灯泡安装，以及需要精细力调制和角度精度的卡片滑动。这些任务由于其严格的精度要求，对基于学习的方法构成了重大挑战。在多个基准测试中，SaTA显著优于强视觉-触觉基线，成功率提高高达30个百分点，同时任务完成时间减少27%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决灵巧操作中如何有效利用触觉信号进行精确几何推理的问题，特别是在需要亚毫米级精度的任务中。这个问题很重要，因为在多指多接触场景中，毫米级误差就可能导致任务失败（如USB连接器无法插入），而在接触关键时刻，视觉信息常被遮挡或失效，精确的几何信息对成功操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出触觉传感器虽提供丰富信息但现有学习框架未能有效利用其感知丰富性和空间关系这一关键限制。他们认为理想的触觉表示应将接触测量稳定在参考框架中，同时保留详细感官信息。设计方法借鉴了ACT框架作为基础架构，使用FiLM机制整合空间上下文与触觉特征，应用Fourier特征编码捕获多尺度几何变化，并采用模仿学习策略使用专家演示数据进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将触觉特征锚定到手部运动学坐标系中，同时保留完整几何信息，使策略能准确推断接触状态和物体几何形状，直接输出精确操作动作。整体流程是：接收多模态输入（RGB图像、触觉图像、关节角度）；通过正向运动学计算触觉传感器6D姿态；用Fourier特征编码空间信息；通过FiLM整合空间上下文与触觉特征；将多模态信息通过Transformer处理；生成动作序列实现亚毫米精度操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：空间锚定触觉表示方法；端到端操作策略框架SaTA；高精度灵巧操作任务的成功验证。相比之前工作，SaTA将触觉测量显式锚定到手部坐标系而非处理为抽象特征；保留了完整触觉图像特征而非转换为简化几何形式；直接输出操作动作而非专注于感知重建；不依赖显式物体模型或离线优化过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SaTA通过空间锚定触觉表示，使基于学习的方法能够实现亚毫米级精度的灵巧操作，成功解决了传统视觉-触觉学习方法在需要高精度几何推理任务中的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous manipulation requires precise geometric reasoning, yet existingvisuo-tactile learning methods struggle with sub-millimeter precision tasksthat are routine for traditional model-based approaches. We identify a keylimitation: while tactile sensors provide rich contact information, currentlearning frameworks fail to effectively leverage both the perceptual richnessof tactile signals and their spatial relationship with hand kinematics. Webelieve an ideal tactile representation should explicitly ground contactmeasurements in a stable reference frame while preserving detailed sensoryinformation, enabling policies to not only detect contact occurrence but alsoprecisely infer object geometry in the hand's coordinate system. We introduceSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), anend-to-end policy framework that explicitly anchors tactile features to thehand's kinematic frame through forward kinematics, enabling accurate geometricreasoning without requiring object models or explicit pose estimation. Our keyinsight is that spatially grounded tactile representations allow policies tonot only detect contact occurrence but also precisely infer object geometry inthe hand's coordinate system. We validate SaTA on challenging dexterousmanipulation tasks, including bimanual USB-C mating in free space, a taskdemanding sub-millimeter alignment precision, as well as light bulbinstallation requiring precise thread engagement and rotational control, andcard sliding that demands delicate force modulation and angular precision.These tasks represent significant challenges for learning-based methods due totheir stringent precision requirements. Across multiple benchmarks, SaTAsignificantly outperforms strong visuo-tactile baselines, improving successrates by up to 30 percentage while reducing task completion times by 27percentage.</description>
      <author>example@mail.com (Jialei Huang, Yang Ye, Yuanqing Gong, Xuezhou Zhu, Yang Gao, Kaifeng Zhang)</author>
      <guid isPermaLink="false">2510.14647v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps</title>
      <link>http://arxiv.org/abs/2510.14546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICRA 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用视觉语言模型嵌入表示机器人地图语义的方法，通过自然语言同义词和反义词来训练分类器，解决机器人确定环境中与查询相关部分的挑战。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型的嵌入表示被越来越多地用于表示机器人地图中的语义，提供开放词汇的场景理解，超越了传统有限标签的表示方法。&lt;h4&gt;目的&lt;/h4&gt;解决机器人确定环境中与查询相关部分的关键挑战，提高地图和图像的查询能力。&lt;h4&gt;方法&lt;/h4&gt;利用嵌入空间中与查询相关的自然语言同义词和反义词，应用启发式方法估计与查询相关的语言空间，并使用该语言空间训练分类器来将环境划分为匹配和不匹配的部分。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验表明，该方法能够显著提高地图和图像的查询能力，且该查询技术与表示和编码器无关，只需要有限的训练。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了机器人确定环境中与查询相关部分的挑战，提高了地图和图像的查询能力，具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型的嵌入表示越来越多地被用于表示机器人地图中的语义，提供开放词汇的场景理解，超越了传统的有限标签。嵌入表示通过相似度比较将嵌入的用户文本提示与地图嵌入，实现按需查询。执行查询任务的关键挑战是机器人必须确定环境中与查询相关的部分。本文提出了这一挑战的解决方案。我们利用嵌入空间中与查询相关的自然语言同义词和反义词，应用启发式方法估计与查询相关的语言空间，并使用该语言空间训练分类器将环境划分为匹配和不匹配的部分。我们通过大量实验评估了该方法，包括对地图和标准图像基准的查询。结果表明地图和图像的查询能力得到了提高。我们的查询技术与表示和编码器无关，只需要有限的训练。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是如何更有效地从视觉-语言模型(VLM)的嵌入表示中查询相关信息的问题。具体来说，当机器人需要根据自然语言查询在地图或图像中找到相关物体或区域时，现有方法无法准确确定环境中与查询相关的部分。这个问题很重要，因为随着视觉-语言模型的发展，机器人地图能够包含更丰富的语义信息，开放词汇的场景理解能力对机器人执行复杂任务至关重要，而现有方法在匹配查询与地图嵌入时性能有限，限制了机器人对环境的理解和交互能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：现有方法主要采用阈值化余弦相似度或使用单个互补查询（如'other'）的策略，但这些方法假设所有维度对查询的重要性相同，且仅使用单个查询和单个负例无法准确估计相关区域的范围。基于这些分析，作者设计了QuASH方法，利用自然语言同义词和反义词来估计与查询相关的语言空间，通过启发式方法生成语义相关的同义词和反义词，并基于这些样本训练一个分类器。该方法借鉴了现有工作中的视觉-语言嵌入表示和查询机制，但改进了查询策略，不再依赖简单的阈值或单个负例比较。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用自然语言语义知识，通过生成查询的同义词和反义词样本来训练一个分类器，从而更准确地确定嵌入空间中与查询相关的区域。整体实现流程包括：1) 给定文本查询，生成一组语义同义词和反义词，并添加通用的负例查询；2) 使用嵌入函数将所有文本转换为嵌入表示；3) 使用这些嵌入表示作为训练数据，训练一个分类器；4) 给定一个地图，使用训练好的分类器对地图进行分类，得到与查询匹配的区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了一种新的查询形式化方法，将查询过程视为在嵌入空间中的分类问题；2) 设计了QuASH方法，利用自然语言启发式方法生成同义词和反义词样本来训练分类器；3) 该方法不依赖于特定的嵌入表示或编码器，具有通用性；4) 通过非线性分类器而非简单的相似度阈值或线性分割来估计相关区域。相比之前的工作，不同之处在于不再依赖单一的查询嵌入和单一的负例嵌入进行比较，不使用固定的相似度阈值，而是通过训练的分类器动态确定决策边界，考虑了嵌入空间中不同维度可能具有不同语义重要性的事实，方法更加灵活，可以适应不同的视觉-语言模型和编码器。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; QuASH通过利用自然语言启发式方法生成同义词和反义词样本来训练分类器，显著提高了机器人地图和图像中基于自然语言查询的准确性，同时保持了方法的通用性和灵活性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embeddings from Visual-Language Models are increasingly utilized to representsemantics in robotic maps, offering an open-vocabulary scene understanding thatsurpasses traditional, limited labels. Embeddings enable on-demand querying bycomparing embedded user text prompts to map embeddings via a similarity metric.The key challenge in performing the task indicated in a query is that the robotmust determine the parts of the environment relevant to the query.  This paper proposes a solution to this challenge. We leveragenatural-language synonyms and antonyms associated with the query within theembedding space, applying heuristics to estimate the language space relevant tothe query, and use that to train a classifier to partition the environment intomatches and non-matches. We evaluate our method through extensive experiments,querying both maps and standard image benchmarks. The results demonstrateincreased queryability of maps and images. Our querying technique is agnosticto the representation and encoder used, and requires limited training.</description>
      <author>example@mail.com (Matti Pekkanen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2510.14546v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Preference Rewarding for MLLMs Spatial Understanding</title>
      <link>http://arxiv.org/abs/2510.14374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SPR（空间偏好奖励）方法，通过奖励多模态大语言模型生成具有精确物体定位的详细响应，增强其细粒度空间理解能力，实验证明该方法有效且训练开销小。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已展现出空间理解能力，但在细粒度空间感知方面仍有不足，如无法生成详细区域描述或准确定位物体，且常无法满足用户对细粒度空间理解的需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有MLLM方法缺乏对实际响应直接监督的问题，通过SPR方法提升MLLM的细粒度空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;SPR方法通过随机选择图像区域和描述，引入语义和定位分数评估MLLM生成描述的质量；使用高定位精度描述完善MLLM输出，并将最佳完善与初始最低分描述配对进行直接偏好优化，增强与视觉输入的细粒度对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在标准引用和定位基准上的大量实验表明，SPR有效提高了MLLM的空间理解能力，同时训练开销最小。&lt;h4&gt;结论&lt;/h4&gt;SPR方法能够显著增强MLLM的细粒度空间理解能力，相关数据和代码将在https://github.com/hanqiu-hq/SPR发布。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型已展现出有希望的空间理解能力，如引用和定位物体描述。尽管取得了成功，MLLMs在细粒度空间感知能力方面仍有不足，例如生成详细的区域描述或准确定位物体。此外，它们经常无法响应用户对所需细粒度空间理解的要求。这个问题可能是因为现有方法主要专注于调整MLLMs以建模预标注的指令数据来注入空间知识，而没有直接监督MLLMs的实际响应。我们通过SPR（空间偏好奖励）方法解决这个问题，通过奖励MLLMs具有精确物体定位的详细响应，而不是模糊或不准确的响应，从而增强MLLMs的空间能力。使用从MLLMs中随机选择的图像区域和区域描述，SPR引入语义和定位分数来全面评估MLLM生成描述中的文本质量和定位质量。我们还使用更好的定位精度来完善MLLM描述，并将得分最高的完善与初始得分最低的描述配对，用于直接偏好优化，从而增强与视觉输入的细粒度对齐。在标准引用和基准测试上的大量实验表明，SPR有效地提高了MLLM的空间理解能力，同时训练开销最小。数据和代码将在https://github.com/hanqiu-hq/SPR发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models~(MLLMs) have demonstrated promising spatialunderstanding capabilities, such as referencing and grounding objectdescriptions. Despite their successes, MLLMs still fall short in fine-grainedspatial perception abilities, such as generating detailed region descriptionsor accurately localizing objects. Additionally, they often fail to respond tothe user's requirements for desired fine-grained spatial understanding. Thisissue might arise because existing approaches primarily focus on tuning MLLMsto model pre-annotated instruction data to inject spatial knowledge, withoutdirect supervision of MLLMs' actual responses. We address this issue by SPR, aSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatialcapabilities by rewarding MLLMs' detailed responses with precise objectlocalization over vague or inaccurate responses. With randomly selected imageregions and region descriptions from MLLMs, SPR introduces semantic andlocalization scores to comprehensively evaluate the text quality andlocalization quality in MLLM-generated descriptions. We also refine the MLLMdescriptions with better localization accuracy and pair the best-scoredrefinement with the initial descriptions of the lowest score for directpreference optimization, thereby enhancing fine-grained alignment with visualinput. Extensive experiments over standard referring and grounding benchmarksshow that SPR improves MLLM spatial understanding capabilities effectively withminimal overhead in training. Data and code will be released athttps://github.com/hanqiu-hq/SPR</description>
      <author>example@mail.com (Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu)</author>
      <guid isPermaLink="false">2510.14374v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</title>
      <link>http://arxiv.org/abs/2510.14357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SUM-AgriVLN方法，通过空间理解记忆模块改进农业视觉语言导航，解决了现有方法忽略过去经验提供空间上下文的问题，在A2A基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;农业机器人正在成为各种农业任务的有力助手，但仍严重依赖人工操作或固定轨道系统进行移动。AgriVLN方法和A2A基准率先将视觉语言导航扩展到农业领域，使机器人能够遵循自然语言指令导航到目标位置。&lt;h4&gt;目的&lt;/h4&gt;解决现有AgriVLN方法将每个导航指令视为独立片段而忽略过去经验提供空间上下文的问题，特别是在农业场景中经常出现重复导航指令的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出空间理解记忆用于农业视觉语言导航(SUM-AgriVLN)方法，其中SUM模块利用空间理解并通过三维重建和表示保存空间记忆。&lt;h4&gt;主要发现&lt;/h4&gt;在A2A基准测试上，SUM-AgriVLN成功将成功率从0.47提高到0.54，导航误差仅从2.91米略微增加到2.93米，展示了在农业领域最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;SUM-AgriVLN方法有效利用了空间记忆来改进农业视觉语言导航性能，证明了在农业机器人导航中考虑历史经验的重要性。&lt;h4&gt;翻译&lt;/h4&gt;农业机器人正在成为各种农业任务的有力助手，然而，它们仍然严重依赖人工操作或固定轨道系统进行移动。AgriVLN方法和A2A基准率先将视觉语言导航扩展到农业领域，使机器人能够遵循自然语言指令导航到目标位置。在实际农业场景中，导航指令经常重复出现，但AgriVLN将每个指令视为独立片段，忽略了过去经验为后续指令提供空间上下文的潜力。为了弥合这一差距，我们提出了用于农业视觉语言导航的空间理解记忆方法，其中SUM模块利用空间理解并通过三维重建和表示保存空间记忆。在A2A基准测试上评估时，我们的SUM-AgriVLN成功将成功率从0.47提高到0.54，导航误差仅从2.91米略微增加到2.93米，展示了在农业领域最先进的性能。代码：https://github.com/AlexTraveling/SUM-AgriVLN。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决农业机器人在视觉语言导航任务中缺乏空间长期记忆的问题。这个问题很重要，因为实际农业场景中经常需要重复执行相似导航指令，而现有方法将每个指令视为独立事件，无法利用过去经验提供的空间上下文，导致机器人导航效率低下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类导航时会自发形成空间记忆并在后续任务中利用，而现有农业机器人缺乏这种能力。他们受日常生活中第一次和第二次去陌生地方的差异启发，设计出空间理解记忆模块。该方法借鉴了VGGT视觉编码器用于3D重建，参考了结构运动和多视图立体等3D重建技术，并在AgriVLN基础上进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入空间理解记忆模块，使机器人能够保存和利用空间记忆。整体流程包括：1)空间理解：从相机图像集中采样10帧，用VGGT生成3D重建；2)空间记忆：将3D重建渲染为点云，提取正面和倾斜两种视角的2D RGB表示并存储；3)基础模型集成：将SUM模块融入AgriVLN，在每一步加载空间记忆，结合语言指令和视觉输入预测行动；4)任务执行：持续更新子任务列表直到满足结束条件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将空间记忆引入农业视觉语言导航；2)通过3D重建和表示保存空间记忆，而非传统基于图的方法；3)提取多视角空间表示提供丰富上下文；4)能够利用任务间经验。相比之前工作，SUM-AgriVLN不同于传统VLN方法专注于农业场景，不同于AgriVLN将任务视为独立事件，不同于现有空间记忆方法依赖图结构，也不同于传统3D重建只关注几何准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的SUM-AgriVLN方法通过引入空间理解记忆模块，使农业机器人能够保存和利用空间记忆，显著提高了在重复导航任务中的成功率，从0.47提升到0.54。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agricultural robots are emerging as powerful assistants across a wide rangeof agricultural tasks, nevertheless, still heavily rely on manual operation orfixed rail systems for movement. The AgriVLN method and the A2A benchmarkpioneeringly extend Vision-and-Language Navigation (VLN) to the agriculturaldomain, enabling robots to navigate to the target positions following thenatural language instructions. In practical agricultural scenarios, navigationinstructions often repeatedly occur, yet AgriVLN treat each instruction as anindependent episode, overlooking the potential of past experiences to providespatial context for subsequent ones. To bridge this gap, we propose the methodof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation(SUM-AgriVLN), in which the SUM module employs spatial understanding and savespatial memory through 3D reconstruction and representation. When evaluated onthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,demonstrating the state-of-the-art performance in the agricultural domain.Code: https://github.com/AlexTraveling/SUM-AgriVLN.</description>
      <author>example@mail.com (Xiaobei Zhao, Xingqi Lyu, Xiang Li)</author>
      <guid isPermaLink="false">2510.14357v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration</title>
      <link>http://arxiv.org/abs/2510.14354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, accepted at ICRA 2024 (International Conference on Robotics  and Automation)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用未标记RGB-D数据进行场景几何推理的新方法，通过循环一致的关键点和结合GRU循环单元的姿态模块，提高了RGB-D配准的准确性。&lt;h4&gt;背景&lt;/h4&gt;随着消费级深度相机的普及，大量未标记的RGB-D数据变得可用，如何有效利用这些数据进行场景几何推理成为一个重要问题。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用未标记的RGB-D数据进行场景的几何推理，提高RGB-D配准的准确性。&lt;h4&gt;方法&lt;/h4&gt;不同于传统的基于几何和特征相似性的RGB-D配准方法，作者使用循环一致的关键点作为显著点强制执行空间一致性约束，并引入结合GRU循环单元和变换同步的姿态模块来融合历史和多视图数据。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet和3DMatch数据集上，该方法超越了之前的自监督配准方法，甚至优于一些旧的监督方法；将组件集成到现有方法中也证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;通过创新的循环一致关键点和姿态模块设计，有效提高了RGB-D配准的准确性，为未标记RGB-D数据的利用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;随着消费级深度相机的兴起，大量未标记的RGB-D数据变得可用。这引发了一个问题：如何利用这些数据进行场景的几何推理。虽然许多RGB-D配准方法依赖于几何和基于特征的相似性，我们采取了不同的方法。我们使用循环一致的关键点作为显著点，在匹配过程中强制执行空间一致性约束，提高对应点准确性。此外，我们引入了一个新的姿态模块，将GRU循环单元与变换同步相结合，融合历史和多视图数据。我们的方法在ScanNet和3DMatch上超越了之前的自监督配准方法，甚至优于一些旧的监督方法。我们还将我们的组件集成到现有方法中，证明了它们的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大量无标签的RGB-D数据进行场景几何推理，特别是RGB-D配准问题。这个问题很重要，因为随着消费级深度相机的普及，有大量无标签RGB-D数据可用，而RGB-D数据在机器人任务(如SLAM、无人机导航和物体姿态估计)中非常关键。传统配准方法在有噪声或特征稀少环境下表现不佳，且现有自监督方法未充分利用场景中的显著点信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考利用场景中的显著点作为锚点，这些点在多视角下易被识别且循环一致。通过空间一致性约束改善对应关系搜索，并结合GRU循环单元和变换同步融合历史和多视图信息。作者借鉴了多项现有工作：使用ResNet-18作为特征提取网络，采用LofTr的匹配策略，利用Sinkhorn归一化，参考矩阵分解方法获得循环一致匹配，受启发于GRU单元和变换同步方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用循环一致的显著点(锚点)施加空间一致性约束提高对应准确性，结合GRU和变换同步进行姿态估计。整体流程：1)使用ResNet-18提取特征；2)通过Sinkhorn归一化获得软匹配并转换为循环一致的锚点；3)使用锚点距离编码修改自注意力模块；4)定义空间一致性成本函数；5)迭代进行像素级匹配和姿态更新；6)结合GRU和变换同步改进姿态估计；7)通过内部迭代(20次)和外部迭代(3次)优化结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)循环一致关键点匹配模块，施加空间约束；2)无RANSAC的姿态估计方法，结合GRU和变换同步；3)空间一致性成本函数；4)迭代优化框架。不同之处：大多数自监督方法依赖特征相似性或几何信息，而本文利用场景显著点；之前循环一致性方法应用于所有像素，本文仅用于定位显著点；与[24]不同，本文用空间约束学习锚点而非修剪离群值；结合GRU和变换同步，而非仅使用一种方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过循环一致锚点和空间一致性约束提出新自监督RGB-D配准方法，显著提高配准精度，超越之前自监督方法并接近一些有监督方法的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA57147.2024.10610738&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data hasbecome available. This prompts the question of how to utilize this data forgeometric reasoning of scenes. While many RGB-D registration meth- ods rely ongeometric and feature-based similarity, we take a different approach. We usecycle-consistent keypoints as salient points to enforce spatial coherenceconstraints during matching, improving correspondence accuracy. Additionally,we introduce a novel pose block that combines a GRU recurrent unit withtransformation synchronization, blending historical and multi-view data. Ourapproach surpasses previous self- supervised registration methods on ScanNetand 3DMatch, even outperforming some older supervised methods. We alsointegrate our components into existing methods, showing their effectiveness.</description>
      <author>example@mail.com (Siddharth Tourani, Jayaram Reddy, Sarvesh Thakur, K Madhava Krishna, Muhammad Haris Khan, N Dinesh Reddy)</author>
      <guid isPermaLink="false">2510.14354v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.13993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 8 tables. To be published in Applied AI Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了结合传统视觉模型与视觉语言模型(VLMs)以增强遥感图像分析，特别是在飞机检测和场景理解方面的应用。通过集成YOLO与LLaVA、ChatGPT和Gemini等VLMs，实现了更准确和具有上下文意识的图像解释。&lt;h4&gt;背景&lt;/h4&gt;遥感已成为城市规划、环境监测和灾害响应等领域的关键工具，数据量显著增加。然而，传统视觉模型受限于需要大量领域特定标记数据且在理解复杂环境上下文方面能力有限。视觉语言模型虽能整合视觉和文本数据，但在遥感领域的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究视觉模型与VLMs的结合，以增强遥感图像分析，专注于飞机检测和场景理解任务。&lt;h4&gt;方法&lt;/h4&gt;集成YOLO与VLMs(如LLaVA、ChatGPT和Gemini)，在标记和未标记的遥感数据以及退化图像场景上评估性能，旨在实现更准确和具有上下文意识的图像解释。&lt;h4&gt;主要发现&lt;/h4&gt;在原始和退化场景中，特别是在具有挑战性的条件下，飞机检测和计数的准确性平均提高了48.46%。在遥感图像的全面理解方面，CLIPScore提高了6.17%。&lt;h4&gt;结论&lt;/h4&gt;结合传统视觉模型和VLMs的方法为更先进和高效的遥感图像分析铺平了道路，特别在少样本学习场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;遥感已成为城市规划、环境监测和灾害响应等跨领域的关键工具。尽管生成的数据量显著增加，但传统视觉模型通常受限于需要大量领域特定标记数据及其在理解复杂环境中上下文能力的有限性。视觉语言模型通过整合视觉和文本数据提供了一种互补方法；然而，它们在遥感领域的应用仍未得到充分探索，特别是考虑到它们的通用性质。本研究探讨了结合视觉模型和VLMs以增强遥感图像分析，专注于飞机检测和场景理解。将YOLO与LLaVA、ChatGPT和Gemini等VLMs的集成旨在实现更准确和具有上下文意识的图像解释。性能在标记和未标记的遥感数据以及退化图像场景上进行了评估，这些场景对遥感至关重要。研究显示，在原始和退化场景中，特别是在具有挑战性的条件下，飞机检测和计数的准确性在各类模型中平均提高了48.46%。在遥感图像的全面理解方面，获得了6.17%的CLIPScore提升。结合传统视觉模型和VLMs的方法为更先进和高效的遥感图像分析铺平了道路，特别是在少样本学习场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing has become a vital tool across sectors such as urban planning,environmental monitoring, and disaster response. While the volume of datagenerated has increased significantly, traditional vision models are oftenconstrained by the requirement for extensive domain-specific labelled data andtheir limited ability to understand the context within complex environments.Vision Language Models offer a complementary approach by integrating visual andtextual data; however, their application to remote sensing remainsunderexplored, particularly given their generalist nature. This workinvestigates the combination of vision models and VLMs to enhance imageanalysis in remote sensing, with a focus on aircraft detection and sceneunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, andGemini aims to achieve more accurate and contextually aware imageinterpretation. Performance is evaluated on both labelled and unlabelled remotesensing data, as well as degraded image scenarios which are crucial for remotesensing. The findings show an average MAE improvement of 48.46% across modelsin the accuracy of aircraft detection and counting, especially in challengingconditions, in both raw and degraded scenarios. A 6.17% improvement inCLIPScore for comprehensive understanding of remote sensing images is obtained.The proposed approach combining traditional vision models and VLMs paves theway for more advanced and efficient remote sensing image analysis, especiallyin few-shot learning scenarios.</description>
      <author>example@mail.com (Jia Yun Chua, Argyrios Zolotas, Miguel Arana-Catania)</author>
      <guid isPermaLink="false">2510.13993v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.07944v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CVD-STORM是一个跨视图视频扩散模型，利用空间-时间重建变分自编码器生成长期多视图视频并具备4D重建能力。&lt;h4&gt;背景&lt;/h4&gt;生成模型已被广泛应用于世界建模和环境模拟、未来状态预测。随着自动驾驶的发展，对高质量视频生成以及深度估计等多样化有意义信息的需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;提出CVD-STORM模型，能够在各种控制输入下生成长期多视图视频，具备4D重建能力。&lt;h4&gt;方法&lt;/h4&gt;首先使用辅助的4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力；然后将这个VAE集成到视频扩散过程中提高生成质量；联合训练的高斯溅射解码器有效重建动态场景，为场景理解提供几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型在FID和FVD指标上都取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;CVD-STORM模型能够在各种控制条件下生成高质量的多视图视频，并有效重建动态场景，为场景理解提供几何信息。&lt;h4&gt;翻译&lt;/h4&gt;生成模型已被广泛应用于世界建模和环境模拟以及未来状态预测。随着自动驾驶的发展，不仅需要高质量的视频生成，还需要产生多样化和有意义的信息如深度估计。为此，我们提出了CVD-STORM，这是一个利用空间-时间重建变分自编码器的跨视图视频扩散模型，能够在各种控制输入下生成具有4D重建能力的长期多视图视频。我们的方法首先使用辅助的4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力。随后，我们将这个VAE集成到视频扩散过程中，显著提高了生成质量。实验结果表明，我们的模型在FID和FVD指标上都取得了显著改进。此外，联合训练的高斯溅射解码器有效地重建动态场景，为全面场景理解提供了有价值的几何信息。我们的项目页面是https://sensetime-fvg.github.io/CVD-STORM。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域中高质量视频生成和4D场景重建的问题。具体来说，现有方法难以同时生成长期、多视角的视频并提供准确的深度信息，这限制了自动驾驶系统对环境的模拟和未来状态的预测能力。这个问题在现实中非常重要，因为自动驾驶需要准确的环境模拟来训练决策算法和验证规划输出，而深度信息对于理解场景的3D结构至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有视频扩散模型在生成长期、多视角视频方面的局限性，以及缺乏明确3D信息的问题。他们借鉴了多项现有工作：基于现有的扩散模型架构（如DiT），参考了STORM模型的空间-时间重建方法，采用了UniMLVG的多模态DiT架构和训练策略，利用了3D高斯溅射技术进行场景重建，并结合了VAE进行表示学习。作者通过整合这些技术，设计了一个两阶段训练策略：先学习场景重建，再训练条件世界模型，以实现高质量的视频生成和4D场景重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过STORM-VAE（一个扩展的VAE模型，集成了高斯溅射解码器）进行4D场景重建，并利用CVD-STORM框架同时生成多视角视频和重建4D场景。整体实现流程分为三部分：1）STORM-VAE训练：使用预训练的图像VAE，添加高斯溅射解码器分支，通过多视图图像和相机姿态进行训练；2）CVD-STORM训练：使用STORM-VAE作为潜在编码器，在扩散模型中集成STORM-VAE，使用三个不同的transformer块处理不同维度；3）推理过程：生成长期六视角视频，高斯溅射解码器直接从生成的潜在表示重建4D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）STORM-VAE：一个能进行4D场景重建的扩展VAE模型；2）CVD-STORM：统一框架同时生成多视角视频和重建4D场景；3）两阶段训练策略：先学习场景重建，再训练条件世界模型；4）增强的表示学习：通过空间-时间重建模型提高生成质量；5）单阶段训练策略：简化训练过程，降低计算成本。相比之前的工作，CVD-STORM实现了真正的端到端交互（不同于MagicDrive3D的两阶段流水线），提供绝对深度估计（不同于UniFuture和GEM的相对深度），重建过程对生成模型有直接影响，并能同时完成多视角视频生成和4D场景重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CVD-STORM通过引入STORM-VAE和统一的生成-重建框架，实现了高质量的多视角视频生成和准确的4D场景重建，为自动驾驶提供了更强大的世界模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models have been widely applied to world modeling for environmentsimulation and future state prediction. With advancements in autonomousdriving, there is a growing demand not only for high-fidelity video generationunder various controls, but also for producing diverse and meaningfulinformation such as depth estimation. To address this, we propose CVD-STORM, across-view video diffusion model utilizing a spatial-temporal reconstructionVariational Autoencoder (VAE) that generates long-term, multi-view videos with4D reconstruction capabilities under various control inputs. Our approach firstfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing itsability to encode 3D structures and temporal dynamics. Subsequently, weintegrate this VAE into the video diffusion process to significantly improvegeneration quality. Experimental results demonstrate that our model achievessubstantial improvements in both FID and FVD metrics. Additionally, thejointly-trained Gaussian Splatting Decoder effectively reconstructs dynamicscenes, providing valuable geometric information for comprehensive sceneunderstanding. Our project page is https://sensetime-fvg.github.io/CVD-STORM.</description>
      <author>example@mail.com (Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu)</author>
      <guid isPermaLink="false">2510.07944v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</title>
      <link>http://arxiv.org/abs/2510.14672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了VTimeCoT框架，一种无需训练的方法，用于解决多模态大语言模型在视频时序定位和推理方面的缺陷，通过引入进度条视觉工具和跨模态推理过程，实现了显著的性能提升和可解释的推理过程。&lt;h4&gt;背景&lt;/h4&gt;视频问答基于多模态大语言模型近年来受到关注，但这类模型在视频时序定位和推理方面存在明显缺陷，对有效现实世界视频理解系统的发展构成挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一个简单而有效的无需训练框架，用于高性能的视频时序定位和推理，解决现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出VTimeCoT框架，包含两个新颖的视觉工具：即插即用的进度条集成工具和高效率高亮工具，同时引入整合视频和文本跨模态推理的视觉时序思考链过程。&lt;h4&gt;主要发现&lt;/h4&gt;在视频时序定位和基于推理的问答任务中，该方法对Qwen2VL-7B和GPT4o基线模型显示出显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架实现了可组合且可解释的推理过程，有效提升了视频理解系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，基于多模态大语言模型(MLLM)的视频问答因其受益于LLMs的显著进步而受到广泛关注。然而，这些模型在视频时序定位和推理领域存在明显缺陷，对有效现实世界视频理解系统的发展构成挑战。受人类使用视频播放器与进度条交互以理解视频的启发，我们引入了VTimeCoT，一个简单而有效的无需训练框架，专为高性能视频定位和推理而设计。该框架包含两个新颖的进度条视觉工具：即插即用的进度条集成工具和高效率高亮工具。此外，为解决传统基于文本的思考链(CoT)方法的局限性，我们引入了一个整合视频和文本跨模态推理的视觉时序思考链过程。我们的方法在视频时序定位和基于推理的问答任务中，对Qwen2VL-7B和GPT4o基线模型均显示出显著的性能改进。最后，我们展示了所提出的框架实现了可组合且可解释的推理过程。项目页面：https://vtimecot.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, video question answering based on multimodal large languagemodels (MLLM) has garnered considerable attention, due to the benefits from thesubstantial advancements in LLMs. However, these models have a notabledeficiency in the domains of video temporal grounding and reasoning, posingchallenges to the development of effective real-world video understandingsystems. Inspired by how humans use video players to interact with the progressbar for video comprehension, we introduce VTimeCoT, a simple yet effectivetraining-free framework, designed for high-performance video grounding andreasoning. The proposed framework incorporates two novel visual tools of theprogress bar: a plug-and-play progress bar integration tool and ahigh-efficiency highlighting tool. In addition, to address the limitations ofconventional text-based chain-of-thought (CoT) approaches, we introduce avisuotemporal CoT process that integrates cross-modality reasoning across bothvideo and text. Our approach demonstrates significant performance improvementson both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding andreasoning-based question answering. Finally, we showcase that the proposedframework achieves a compositional and interpretable reasoning process. Projectpage: https://vtimecot.github.io</description>
      <author>example@mail.com (Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma)</author>
      <guid isPermaLink="false">2510.14672v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.14032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 (Spotlight). Webpage at  https://xiaoqian-shen.github.io/Vgent&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Vgent，一种基于图的检索-推理-增强生成框架，用于增强大型视频语言模型对长视频的理解能力。通过结构化图表示视频和引入中间推理步骤，有效解决了长视频处理中的时间依赖性问题和检索噪声问题，在多个基准测试上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;理解和推理长视频对大型视频语言模型(LVLMs)构成重大挑战，主要因为难以处理超出上下文窗口密集的视频token，并保留长期顺序信息。检索增强生成(RAG)虽然对处理长上下文有效，但应用于长视频时面临时间依赖性被打乱和包含无关信息等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架增强LVLMs对长视频的理解能力，解决长视频处理中的时间依赖性问题和检索噪声问题，提高模型在长视频理解任务中的准确性和上下文感知能力。&lt;h4&gt;方法&lt;/h4&gt;提出Vgent框架，包含两个关键创新：(1)使用结构化图表示视频，保留视频片段间的语义关系以提高检索效果；(2)引入中间推理步骤，利用结构化验证减少检索噪声，促进相关信息片段的显式聚合。&lt;h4&gt;主要发现&lt;/h4&gt;在MLVU基准测试上，与基础模型相比，总体性能提升了3.0%~5.4%，并比最先进的视频RAG方法高出8.6%。代码已在https://xiaoqian-shen.github.io/Vgent公开。&lt;h4&gt;结论&lt;/h4&gt;Vgent框架通过结构化图表示和中间推理步骤，有效解决了长视频理解中的关键挑战，显著提升了LVLMs的性能，为长视频理解任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解和推理长视频对大型视频语言模型(LVLMs)构成重大挑战，因为难以处理超出上下文窗口密集的视频token并保留长期顺序信息。检索增强生成(RAG)在处理大型语言模型(LLMs)的长上下文方面已显示出有效性；然而，将RAG应用于长视频面临时间依赖性被打乱和包含无关信息等挑战，这些都会妨碍准确推理。为解决这些局限性，我们提出了Vgent，一种新颖的基于图的检索-推理-增强生成框架，用于增强LVLMs对长视频的理解能力。我们的方法引入了两个关键创新：(i)它通过保留视频片段间的语义关系，使用结构化图表示视频，以提高检索效果。(ii)它引入中间推理步骤，缓解LVLMs的推理局限性，利用结构化验证减少检索噪声，促进相关信息的显式聚合，从而产生更准确和上下文感知的响应。我们在三个长视频理解基准测试上使用各种开源LVLMs全面评估了我们的框架。与基础模型相比，我们的方法在MLVU上总体性能提升了3.0%~5.4%，并比最先进的视频RAG方法高出8.6%。我们的代码已在https://xiaoqian-shen.github.io/Vgent公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and reasoning over long videos pose significant challenges forlarge video language models (LVLMs) due to the difficulty in processingintensive video tokens beyond context window and retaining long-term sequentialinformation. Retrieval-Augmented Generation (RAG) has demonstratedeffectiveness in processing long context for Large Language Models (LLMs);however, applying RAG to long video faces challenges such as disrupted temporaldependencies and inclusion of irrelevant information that can hinder accuratereasoning. To address these limitations, we propose Vgent, a novel graph-basedretrieval-reasoning-augmented generation framework to enhance LVLMs for longvideo understanding. Our approach introduces two key innovations: (i) Itrepresents videos by structured graphs with semantic relationships across videoclips preserved to improve retrieval effectiveness. (ii) It introduces anintermediate reasoning step to mitigate the reasoning limitation of LVLMs,which leverages structured verification to reduce retrieval noise andfacilitate the explicit aggregation of relevant information across clips,resulting in more accurate and context-aware responses. We comprehensivelyevaluate our framework with various open-source LVLMs on three long-videounderstanding benchmarks. Our approach yielded an overall performanceimprovement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformedstate-of-the-art video RAG methods by $8.6\%$. Our code is publicly availableat https://xiaoqian-shen.github.io/Vgent.</description>
      <author>example@mail.com (Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny)</author>
      <guid isPermaLink="false">2510.14032v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</title>
      <link>http://arxiv.org/abs/2510.13016v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了时空视频动作定位(SVAG)任务，要求模型同时检测、跟踪和基于自然语言描述对视频中的相关对象进行时空定位。研究团队构建了SVAG-Bench基准数据集，提出了SVAGFormer基线框架，并开发了SVAGEVal评估工具。实验表明现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中。&lt;h4&gt;背景&lt;/h4&gt;细粒度动作理解和准确时空定位是推进下一代AI系统的基本能力，包括具身智能体、自主平台和人机交互框架。尽管视频理解最近有所进展，但现有方法主要解决粗粒度动作识别或通用目标跟踪问题，忽略了根据动作联合检测和跟踪多个对象并进行时空定位的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在联合检测、跟踪和时空定位视频中的相关对象方面的不足，推进细粒度动作理解和对象-动作交互的推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出了时空视频动作定位(SVAG)任务，构建了SVAG-Bench大型基准数据集（包含688个视频、19,590条标注记录和903个独特动词），提出了SVAGFormer基线框架（适应最先进的视觉语言模型进行联合时空定位），并开发了SVAGEVal标准化评估工具包。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中，这凸显了对长视频中细粒度对象-动作交互进行更高级推理的必要性。&lt;h4&gt;结论&lt;/h4&gt;该研究为细粒度视频理解和对象-动作交互建立了新的基准和评估框架，强调了开发能够处理复杂场景和长视频中高级推理能力的模型的重要性。&lt;h4&gt;翻译&lt;/h4&gt;理解细粒度动作并准确定位其在空间和时间中对应的执行者是推进下一代AI系统的基本能力，包括具身智能体、自主平台和人机交互框架。尽管视频理解最近有所进展，但现有方法主要解决粗粒度动作识别或通用目标跟踪问题，因此忽略了根据动作联合检测和跟踪多个对象并进行时空定位的挑战。为解决这一差距，我们引入了时空视频动作定位(SVAG)，这是一个新任务，要求模型基于自然语言描述的动作同时检测、跟踪和时空定位视频中所有相关对象。为支持此任务，我们构建了SVAG-Bench，这是一个大规模基准，包含688个视频、19,590条标注记录和903个独特动词，涵盖了多样化的对象、动作和现实世界场景。我们进一步提出了SVAGFormer，这是一个基线框架，它适应了最先进的视觉语言模型进行联合时空定位，并引入了SVAGEVal，这是一个标准化的评估工具包，用于公平和可复现的基准测试。实验结果表明，现有模型在SVAG上表现不佳，特别是在密集或复杂场景中，这凸显了需要对长视频中细粒度对象-动作交互进行更高级推理的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding fine-grained actions and accurately localizing theircorresponding actors in space and time are fundamental capabilities foradvancing next-generation AI systems, including embodied agents, autonomousplatforms, and human-AI interaction frameworks. Despite recent progress invideo understanding, existing methods predominantly address eithercoarse-grained action recognition or generic object tracking, therebyoverlooking the challenge of jointly detecting and tracking multiple objectsaccording to their actions while grounding them temporally. To address thisgap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel taskthat requires models to simultaneously detect, track, and temporally localizeall referent objects in videos based on natural language descriptions of theiractions. To support this task, we construct SVAG-Bench, a large-scale benchmarkcomprising 688 videos, 19,590 annotated records, and 903 unique verbs, coveringa diverse range of objects, actions, and real-world scenes. We further proposeSVAGFormer, a baseline framework that adapts state of the art vision languagemodels for joint spatial and temporal grounding, and introduce SVAGEval, astandardized evaluation toolkit for fair and reproducible benchmarking.Empirical results show that existing models perform poorly on SVAG,particularly in dense or complex scenes, underscoring the need for moreadvanced reasoning over fine-grained object-action interactions in long videos.</description>
      <author>example@mail.com (Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl)</author>
      <guid isPermaLink="false">2510.13016v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding</title>
      <link>http://arxiv.org/abs/2510.13891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态大语言模型在长视频理解方面面临上下文窗口和计算成本限制，现有关键帧选择方法存在信息丢失和场景连续性问题。作者提出K-frames方法，通过预测语义连贯的视频片段而非单个帧，保持时间连续性，支持灵活的多尺度关键帧选择。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在图像理解方面表现出色，但在长视频理解方面受到上下文窗口和计算成本的限制。均匀采样通常会导致大量信息丢失。&lt;h4&gt;目的&lt;/h4&gt;解决现有关键帧选择方法的问题，提出一种能够保持时间连续性的场景驱动关键帧选择方法。&lt;h4&gt;方法&lt;/h4&gt;K-frames方法预测语义连贯、与查询相关的视频片段而非单个帧，支持任意数量的关键帧选择。作者构建了包含20万个基于查询条件的视频亮点的PeakClips数据集，并采用三阶段渐进式课程学习：两个监督微调阶段（时间定位和关键片段感知）和一个强化学习阶段（优化场景驱动的预测策略）。&lt;h4&gt;主要发现&lt;/h4&gt;在主要的长视频理解基准上的大量实验表明，K-frames在各种规模的关键帧选择方面提供了有效、可解释且即插即用的解决方案。&lt;h4&gt;结论&lt;/h4&gt;K-frames方法解决了长视频理解中的关键帧选择问题，作者公开的数据集和模型将为该领域提供支持。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在图像理解方面已展现出显著能力，但在长视频处理中受限于上下文窗口和计算成本。均匀采样常导致大量信息丢失。同时，现有的关键帧选择方法如文本-帧检索或基于强化学习的帧优化通常产生稀疏且时间上不连续的帧，忽略了场景连续性，缺乏多尺度帧选择的灵活性。为解决这些问题，我们引入K-frames，一种保持时间连续性的场景驱动关键帧选择新范式。K-frames不选择单个帧，而是预测语义连贯、与查询相关的片段，支持任意数量的关键帧选择以满足不同用户需求。为实现这一方法，我们首先引入PeakClips数据集，包含20万个基于查询条件的视频亮点。基于此数据集，K-frames使用三阶段渐进式课程学习clip2frame选择，包括两个监督微调阶段（用于时间定位和关键片段感知）和一个强化学习阶段（直接优化场景驱动的预测策略，无需额外注释）。在主要长视频理解基准上的大量实验表明，K-frames为各种规模的关键帧选择提供了有效、可解释且即插即用的解决方案。我们的数据集和模型将会公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantcapabilities in image understanding, but long-video are constrained by contextwindows and computational cost. Uniform frame sampling often leads tosubstantial information loss. Meanwhile existing keyframe selection methodssuch as text-frame retrieval or RL-based frame optimization typically yieldsparse and temporally disjointed frames, overlooking scene continuity andlacking flexibility for multi-scale frame selection. To address theselimitations, we introduce K-frames, a novel paradigm for scene-driven keyframeselection that preserves temporal continuity. Instead of selecting individualframes, K-frames predicts semantically coherent, query-relevant clips, whichenables any-k keyframes selection to meet diverse user budgets. To achieve thisapproach, we first introduce PeakClips, a dataset of 200K video highlightsconditioned by query. Building on this dataset, K-frames learns clip2frameselection using a three-stage progressive curriculum. It involves twoSupervised Fine-Tuning stages for temporal grounding and key-clip perception,followed by a Reinforcement Learning stage that directly optimizes thescene-driven prediction policy for downstream task without further annotations.Extensive experiments on major long-video understanding benchmarks demonstratethat K-frames provides an effective, interpretable, and plug-and-play solutionfor keyframe selection at various scales. Our dataset and model will beavailable.</description>
      <author>example@mail.com (Yifeng Yao, Yike Yun, Jing Wang, Huishuai Zhang, Dongyan Zhao, Ke Tian, Zhihao Wang, Minghui Qiu, Tao Wang)</author>
      <guid isPermaLink="false">2510.13891v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ChangingGrounding: 3D Visual Grounding in Changing Scenes</title>
      <link>http://arxiv.org/abs/2510.14965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了ChangingGrounding基准和Mem-ChangingGrounder方法，用于解决动态场景中3D视觉目标定位问题，通过利用过去观察信息减少探索成本。&lt;h4&gt;背景&lt;/h4&gt;现实世界机器人需要从自然语言指令中定位物体，同时周围场景不断变化。现有3D视觉目标定位方法假设已重建且最新的点云，这导致需要昂贵的重新扫描，阻碍了实际部署。&lt;h4&gt;目的&lt;/h4&gt;将3DVG表述为主动的、内存驱动的问题，引入ChangingGrounding基准来衡量代理如何有效利用过去观察、只在需要处探索，并在变化场景中提供精确3D边界框。&lt;h4&gt;方法&lt;/h4&gt;提出Mem-ChangingGrounder零样本方法，结合跨模态检索与轻量级多视图融合：识别物体类型、检索相关记忆指导动作、高效探索目标、操作无效时回退、多视图扫描目标、融合多视图证据获取准确边界框。&lt;h4&gt;主要发现&lt;/h4&gt;在ChangingGrounding基准上评估不同基线方法，Mem-ChangingGrounder实现最高定位精度，同时显著降低探索成本。&lt;h4&gt;结论&lt;/h4&gt;希望该基准和方法能推动面向实际应用的、以内存为中心的3DVG研究转变。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的机器人从自然语言指令中定位物体，同时周围场景不断变化。然而，大多数现有的3D视觉目标定位方法仍然假设已重建且最新的点云，这种假设迫使昂贵的重新扫描并阻碍部署。我们认为3DVG应表述为主动的、内存驱动的问题，并引入ChangingGrounding，这是第一个明确衡量代理如何有效利用过去观察、只在需要处探索并在变化场景中提供精确3D边界框的基准。为设定强参考点，我们还提出了Mem-ChangingGrounder，这是一种针对此任务的零样本方法，它结合了跨模态检索与轻量级多视图融合：识别查询暗示的物体类型，检索相关记忆指导动作，然后在场景中高效探索目标，在先前操作无效时回退，对目标进行多视图扫描，并将多视图扫描的融合证据投影以获得准确的物体边界框。我们在ChangingGrounding上评估了不同的基线方法，我们的Mem-ChangingGrounder实现了最高的定位精度，同时大大减少了探索成本。我们希望这个基准和方法能够推动面向实际应用的、以内存为中心的3DVG研究转变。项目页面：https://hm123450.github.io/CGB/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉定位在动态变化场景中的挑战。现有方法假设场景静态且拥有完整点云，但真实环境中物体会移动或被遮挡，导致机器人需要频繁重新扫描整个场景，这非常耗时且成本高昂。这个问题在现实中很重要，因为它限制了机器人在动态环境（如家庭、办公室）中的实用性，增加了能耗并降低了效率，而人类却能利用过去记忆快速适应变化环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类认知方式获得灵感，人类在动态环境中会利用过去记忆高效定位目标。作者将3D视觉定位重新定义为'主动的、记忆驱动的问题'。他们借鉴了VLM-Grounder的框架（使用2D图像而非点云）、3RScan数据集（提供不同时间点的场景扫描和物体对应关系），以及视觉语言模型和开放词汇检测器等现有技术。设计的Mem-ChangingGrounder方法结合了记忆检索、智能探索和回退策略，以应对场景变化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用机器人对过去场景的记忆来指导在当前变化场景中的高效探索，避免盲目扫描整个场景。整体流程包括：1)查询分类：将查询分为'可验证'（即使目标移动，记忆中的目标仍匹配查询）和'不可验证'（记忆中的目标可能不再匹配）；2)记忆检索与定位：根据查询类型选择策略，使用全景扫描或空间关系感知扫描寻找目标；3)回退策略：当主策略失败时，从记忆检索目标类别并进行360度搜索；4)多视图投影：结合多视图信息生成精确3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次定义动态场景中的3D视觉定位任务，强调利用过去记忆；2)提出ChangingGrounding基准数据集，包含267K个参照性描述，评估定位准确性和探索成本；3)设计Mem-ChangingGrounder方法，结合记忆检索和智能探索；4)引入探索成本指标，强调效率。相比之前工作，本文不再假设场景静态，而是设计基于智能体的方法，利用2D图像和记忆避免昂贵的点云重建，同时关注准确性和效率的平衡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了首个面向动态场景的3D视觉定位基准和方法，通过结合记忆检索和智能探索策略，实现了在变化环境中高效且准确的物体定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world robots localize objects from natural-language instructions whilescenes around them keep changing. Yet most of the existing 3D visual grounding(3DVG) method still assumes a reconstructed and up-to-date point cloud, anassumption that forces costly re-scans and hinders deployment. We argue that3DVG should be formulated as an active, memory-driven problem, and we introduceChangingGrounding, the first benchmark that explicitly measures how well anagent can exploit past observations, explore only where needed, and stilldeliver precise 3D boxes in changing scenes. To set a strong reference point,we also propose Mem-ChangingGrounder, a zero-shot method for this task thatmarries cross-modal retrieval with lightweight multi-view fusion: it identifiesthe object type implied by the query, retrieves relevant memories to guideactions, then explores the target efficiently in the scene, falls back whenprevious operations are invalid, performs multi-view scanning of the target,and projects the fused evidence from multi-view scans to get accurate objectbounding boxes. We evaluate different baselines on ChangingGrounding, and ourMem-ChangingGrounder achieves the highest localization accuracy while greatlyreducing exploration cost. We hope this benchmark and method catalyze a shifttoward practical, memory-centric 3DVG research for real-world applications.Project page: https://hm123450.github.io/CGB/ .</description>
      <author>example@mail.com (Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu)</author>
      <guid isPermaLink="false">2510.14965v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://lei-kun.github.io/RL-100/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了RL-100，一个基于扩散视觉运动策略的真实世界强化学习训练框架，通过三阶段流程实现高效可靠的机器人操作，并在多个任务上达到100%成功率。&lt;h4&gt;背景&lt;/h4&gt;家庭和工厂中的真实世界机器人操作需要可靠性、效率和鲁棒性，达到或超越熟练人类操作员的水平。&lt;h4&gt;目的&lt;/h4&gt;开发一个真实世界的强化学习训练框架，实现高效、可靠且通用的机器人操作能力。&lt;h4&gt;方法&lt;/h4&gt;RL-100框架采用三阶段流程：首先通过模仿学习利用人类先验知识；其次使用离线策略评估(OPE)进行迭代离线强化学习；最后通过在线强化学习消除剩余失败模式。此外，添加轻量级一致性蒸馏头将多步采样压缩为单步策略，实现高频控制并降低延迟。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实机器人任务上评估，包括动态刚体控制、流体倾倒、布料折叠、拧螺丝和橙汁制作等，RL-100实现了900/900的100%成功率，包括连续250次试验全部成功。该方法达到接近人类远程操作或更好的时间效率，并展示了多小时的鲁棒性，可连续运行长达两小时。&lt;h4&gt;结论&lt;/h4&gt;RL-100是一个与任务、具身和表示无关的通用框架，支持多种输入和机器人平台，能够实现与人类相当或更好的机器人操作性能。&lt;h4&gt;翻译&lt;/h4&gt;家庭和工厂中的真实世界机器人操作需要可靠性、效率和鲁棒性，达到或超越熟练人类操作员的水平。我们提出了RL-100，一个基于通过监督学习训练的扩散视觉运动策略构建的真实世界强化学习训练框架。RL-100引入了一个三阶段流程。首先，模仿学习利用人类先验知识。其次，迭代离线强化学习使用离线策略评估(OPE)程序来筛选PPO风格的更新，并在去噪过程中应用这些更新，以实现保守可靠的改进。第三，在线强化学习消除剩余的失败模式。此外，添加的轻量级一致性蒸馏头将扩散中的多步采样过程压缩为单步策略，实现了高频控制，同时延迟减少一个数量级，并保留了任务性能。该框架与任务、具身和表示无关，支持3D点云和2D RGB输入，各种机器人平台，以及单步和动作块策略。我们在七个真实机器人任务上评估了RL-100，包括动态刚体控制（如推-T和敏捷保龄球）、流体和颗粒倾倒、可变形布料折叠、精确灵巧拧螺丝和多阶段橙汁制作。RL-100在总共900个评估试验中实现了100%成功率，包括在一个任务上连续250次试验全部成功。该方法实现了接近人类远程操作或更好的时间效率，并展示了多小时的鲁棒性，不间断运行时间长达两小时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world robotic manipulation in homes and factories demands reliability,efficiency, and robustness that approach or surpass skilled human operators. Wepresent RL-100, a real-world reinforcement learning training framework built ondiffusion visuomotor policies trained bu supervised learning. RL-100 introducesa three-stage pipeline. First, imitation learning leverages human priors.Second, iterative offline reinforcement learning uses an Offline PolicyEvaluation procedure, abbreviated OPE, to gate PPO-style updates that areapplied in the denoising process for conservative and reliable improvement.Third, online reinforcement learning eliminates residual failure modes. Anadditional lightweight consistency distillation head compresses the multi-stepsampling process in diffusion into a single-step policy, enablinghigh-frequency control with an order-of-magnitude reduction in latency whilepreserving task performance. The framework is task-, embodiment-, andrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, avariety of robot platforms, and both single-step and action-chunk policies. Weevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,such as Push-T and Agile Bowling, fluids and granular pouring, deformable clothfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100attains 100\% success across evaluated trials for a total of 900 out of 900episodes, including up to 250 out of 250 consecutive trials on one task. Themethod achieves near-human teleoperation or better time efficiency anddemonstrates multi-hour robustness with uninterrupted operation lasting up totwo hours.</description>
      <author>example@mail.com (Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, Huazhe Xu)</author>
      <guid isPermaLink="false">2510.14830v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery</title>
      <link>http://arxiv.org/abs/2510.14768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为接触感知动态恢复(CADRE)的强化学习框架，用于在灵巧操作中处理意外错误和干扰，特别是接住下落物体并系统重置以恢复主要任务。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的灵巧操作经常遇到意外错误和干扰，可能导致灾难性故障，如掉落被操作物体。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，在物体仍在抓取范围内时接住下落物体，并将系统重置为有利于恢复主要操作任务的配置。&lt;h4&gt;方法&lt;/h4&gt;提出接触感知动态恢复(CADRE)框架，这是一个强化学习框架，集成了受神经描述场(NDF)启发的模块来提取隐式接触特征，直接推理手指-物体对应关系并适应不同物体几何形状。&lt;h4&gt;主要发现&lt;/h4&gt;整合接触特征提高了训练效率，增强了强化学习的收敛性能，并最终导致更成功的恢复操作。&lt;h4&gt;结论&lt;/h4&gt;CADRE框架可以零样本泛化到具有不同几何形状的未见物体上，证明了其在实际应用中的有效性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的灵巧操作经常遇到意外错误和干扰，可能导致灾难性故障，如掉落被操作物体。为了应对这一挑战，我们专注于在物体仍在抓取范围内时接住下落物体，并将系统重置为有利于恢复主要操作任务的配置。我们提出了接触感知动态恢复(CADRE)，这是一个强化学习框架，集成了受神经描述场(NDF)启发的模块来提取隐式接触特征。与仅依赖物体姿态或点云输入的方法相比，NDF可以直接推理手指-物体对应关系并适应不同的物体几何形状。实验表明，整合接触特征提高了训练效率，增强了强化学习的收敛性能，并最终导致更成功的恢复操作。此外，我们证明了CADRE可以零样本泛化到具有不同几何形状的未见物体上。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人在灵巧操作中遇到意外错误和干扰时的恢复问题，特别是如何抓住掉落的物体并恢复到有利于继续主要操作任务的状态。这个问题在现实中很重要，因为机器人执行实际任务时经常遇到意外干扰，如螺丝卡住导致产生意外扭矩，可能导致物体掉落。仅仅抓住物体是不够的，还需要恢复到适合继续主要任务的状态，例如从强力抓取切换到精确抓取，这对实际应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到动态恢复问题的重要性，特别是抓住掉落物体并恢复到有利于继续主要操作任务的状态。他们观察到接触在灵巧操作中的重要性，并认为保持不同物体几何形状间一致的接触行为是成功泛化的基本因素。作者借鉴了Neural Descriptor Fields (NDF)来提取隐式接触特征，NDF能够捕获3D坐标和物体点云之间的几何对应关系。他们设计了Contact-Aware Dynamic Recovery (CADRE)框架，将NDF特征作为隐式接触信息整合到强化学习中。同时，他们借鉴了强化学习(特别是PPO算法)、点云表示方法以及DexPoint中利用接触信息提高泛化的思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用神经描述场(NDF)提取隐式接触特征，使机器人能够推理手指-物体对应关系并适应不同物体几何形状，同时不仅关注抓住掉落物体，还关注恢复到有利于继续主要操作任务的状态。整体实现流程包括：1)使用预训练的NDF模型提取接触特征，在机器人手上预定义关键点并查询这些点的NDF特征形成抓取特征；2)设计强化学习框架，将观察(机器人关节角度、物体姿态、物体速度)和抓取特征结合作为输入，使用PPO算法优化策略，并设计多目标奖励函数；3)在螺丝刀和螺母恢复任务上评估方法，测试泛化能力，并在真实机器人硬件上部署验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出通过抓取进行恢复的问题，机器人不仅要抓住掉落的物体，还要实现能够无缝恢复主要操作任务的抓取配置；2)开发了基于NDF的隐式接触表示，用于接触丰富的灵巧操作，有效捕获手部和操作物体之间的几何对应关系；3)提出了用于动态恢复的强化学习框架，利用这种表示实现成功的抓取和有利于后续操作任务的状态；4)证明了这种接触表示能够在不同几何形状的动态恢复任务中实现有效的泛化。相比之前工作，我们的方法不仅关注稳定抓取，还考虑后续操作任务的需求；NDF可以直接推理手指-物体对应关系并适应不同物体几何形状；提供了比点云方法更全面的接触建模；相比DexPoint，我们的方法区分了应该接触和应该避免接触的区域。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了CADRE，一种利用神经描述场提取隐式接触特征的强化学习框架，使机器人能够在灵巧操作中从掉落物体中恢复并回到有利于继续主要操作任务的状态，同时实现了对不同物体几何形状的零样本泛化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world dexterous manipulation often encounters unexpected errors anddisturbances, which can lead to catastrophic failures, such as dropping themanipulated object. To address this challenge, we focus on the problem ofcatching a falling object while it remains within grasping range and,importantly, resetting the system to a configuration favorable for resuming theprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), areinforcement learning framework that incorporates a Neural Descriptor Field(NDF)-inspired module to extract implicit contact features. Compared to methodsthat rely solely on object pose or point cloud input, NDFs can directly reasonabout finger-object correspondence and adapt to different object geometries.Our experiments show that incorporating contact features improves trainingefficiency, enhances convergence performance for RL training, and ultimatelyleads to more successful recoveries. Additionally, we demonstrate that CADREcan generalize zero-shot to unseen objects with different geometries.</description>
      <author>example@mail.com (Fan Yang, Zixuan Huang, Abhinav Kumar, Sergio Aguilera Marinovic, Soshi Iba, Rana Soltani Zarrin, Dmitry Berenson)</author>
      <guid isPermaLink="false">2510.14768v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning</title>
      <link>http://arxiv.org/abs/2510.14584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的可放置性度量方法，可以直接从嘈杂点云评估放置姿态，无需任何形状先验知识。该方法联合评分稳定性、可抓取性和间隙，实现无需模型的统一抓取-放置推理。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的传感噪声下可靠地抓取和放置未知物体仍然具有挑战性。现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了抓取和放置之间的泛化和统一推理能力。&lt;h4&gt;目的&lt;/h4&gt;引入一种通用的可放置性度量方法，直接从嘈杂点云评估放置姿态，无需任何形状先验；实现统一抓取-放置推理；在未见过的真实物体和非平面物体支撑上提供准确的稳定性预测和物理合理的放置结果。&lt;h4&gt;方法&lt;/h4&gt;引入通用的可放置性度量方法；从原始几何形状中提取物体的支撑表面；生成多样化的多方向放置候选；采样满足碰撞和稳定性约束的接触点；将抓取分数与每个候选放置相关联，实现无需模型的统一抓取-放置推理。&lt;h4&gt;主要发现&lt;/h4&gt;在未见过的真实物体和非平面物体支撑上，该方法与CAD模型相当的准确性预测稳定性损失；比基于学习的方法产生更物理合理的放置结果。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够实现无需模型的统一抓取-放置推理；在现实世界的噪声条件下，能够准确预测稳定性损失并产生物理合理的放置结果；克服了现有方法对强物体先验和平面支撑假设的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的传感噪声下可靠地抓取和放置未知物体仍然是一项具有挑战性的任务，因为现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了抓取和放置之间的泛化和统一推理能力。在这项工作中，我们引入了一种通用的可放置性度量方法，直接从嘈杂点云评估放置姿态，无需任何形状先验。该度量方法联合评分稳定性、可抓取性和间隙。从原始几何形状中，我们提取物体的支撑表面，生成多样化的多方向放置候选，并采样满足碰撞和稳定性约束的接触点。通过将抓取分数与每个候选放置相关联，我们提出的方法实现了无需模型的统一抓取-放置推理，并选择导致稳定、无碰撞放置的抓取-放置对。在未见过的真实物体和非平面物体支撑上，我们的度量方法在预测稳定性损失方面提供了与CAD模型相当的准确性，并且通常比基于学习方法的预测器产生更物理合理的放置结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在现实世界感知噪声下可靠地抓取和放置未知物体的问题。这个问题很重要，因为抓取和放置能力对仓库物流、家庭辅助和医疗保健等机器人应用至关重要，而现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了在复杂和噪声环境中的泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：大多数方法使用强物体先验评估放置稳定性，或只评估少量预定义的放置姿态。作者借鉴了统一抓取和放置推理的思想，从点云处理中学习物体重建方法，并改进了稳定性评估以处理部分和噪声观测。新方法设计了一个通用的可放置性度量，直接从嘈杂点云评估放置姿态，融合物理可行性和机器人约束，实现无模型统一的抓取和放置推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个通用的可放置性度量，直接从传感器数据评估放置姿态，融合稳定性、放置条件下的可抓取性和间隙三个因素，通过统一评分抓取和放置候选，选择最佳组合。整体流程包括：1)感知阶段重建工作空间和物体点云；2)生成和评分候选抓取；3)生成多样化放置候选；4)计算可放置性评分（稳定性、PCG、间隙）；5)统一推理选择最佳抓取-放置组合进行执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)通用可放置性度量，直接从嘈杂点云评估放置姿态；2)无模型统一抓取和放置推理，在共同物体框架中评估；3)物理有效性验证，在非平面支撑和边缘情况表现优异；4)任务驱动的放置评估，支持操作偏好。相比之前工作：不需要CAD模型或平面支撑假设；能处理边缘附近的物体；提供通用分数而非单一稳定平面；适合在线部署，不依赖重型预测网络。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通用的可放置性度量，使机器人能够直接从嘈杂点云评估未知物体的稳定放置，实现无模型统一的抓取和放置推理，显著提高了在复杂环境中的操作成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To reliably pick and place unknown objects under real-world sensing noiseremains a challenging task, as existing methods rely on strong object priors(e.g., CAD models), or planar-support assumptions, limiting generalization andunified reasoning between grasping and placing. In this work, we introduce ageneralized placeability metric that evaluates placement poses directly fromnoisy point clouds, without any shape priors. The metric jointly scoresstability, graspability, and clearance. From raw geometry, we extract thesupport surfaces of the object to generate diverse candidates formulti-orientation placement and sample contacts that satisfy collision andstability constraints. By conditioning grasp scores on each candidateplacement, our proposed method enables model-free unified pick-and-placereasoning and selects grasp-place pairs that lead to stable, collision-freeplacements. On unseen real objects and non-planar object supports, our metricdelivers CAD-comparable accuracy in predicting stability loss and generallyproduces more physically plausible placements than learning-based predictors.</description>
      <author>example@mail.com (Benno Wingender, Nils Dengler, Rohit Menon, Sicong Pan, Maren Bennewitz)</author>
      <guid isPermaLink="false">2510.14584v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification</title>
      <link>http://arxiv.org/abs/2510.14576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了CALM-Net，一种基于曲率感知的激光雷达点云多分支神经网络，用于车辆重识别任务。&lt;h4&gt;背景&lt;/h4&gt;车辆重识别面临的主要挑战是从三维点云中学习判别性和互补性特征来区分不同车辆。&lt;h4&gt;目的&lt;/h4&gt;提出CALM-Net模型，通过整合曲率感知信息提高车辆重识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用多分支架构，整合了边缘卷积、点注意力和曲率嵌入（用于表征点云中的局部表面变化），学习丰富的几何和上下文特征。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，CALM-Net相比最强基线方法，平均重识别准确率提高了约1.97个百分点。&lt;h4&gt;结论&lt;/h4&gt;将曲率信息整合到深度学习架构中并采用多分支特征学习，能有效提升基于激光雷达点云的车辆重识别性能。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了CALM-Net，一种基于曲率感知的激光雷达点云多分支神经网络，用于车辆重识别。所提出的模型解决了从三维点云中学习判别性和互补性特征以区分车辆这一挑战。CALM-Net采用多分支架构，整合了边缘卷积、点注意力和曲率嵌入，后者用于表征点云中的局部表面变化。通过结合这些机制，模型学习更适合重识别任务的丰富几何和上下文特征。在大型nuScenes数据集上的实验评估表明，CALM-Net比我们研究中最强的基线方法平均提高了约1.97个百分点的重识别准确率。结果证实了将曲率信息整合到深度学习架构中的有效性，并突出了多分支特征学习对基于激光雷达点云的车辆重识别的益处。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于LiDAR点云的车辆重识别问题，即如何从三维点云数据中学习判别性特征来区分不同车辆。这个问题在智能交通系统中至关重要，因为它支持跨摄像头跟踪、交通分析和自动驾驶安全，能够解决传统运动跟踪在遮挡、轨迹碎片化等情况下的失败问题，同时点云数据提供准确的3D几何信息，相比摄像头数据对光照变化和视角变化具有更好的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了基于摄像头的方法在复杂环境下的局限性，认识到点云数据的优势。他们借鉴了车辆重识别领域的多种方法，包括视角感知学习、多分支特征融合和注意力机制，同时借鉴了机器人学中利用特征值确定车辆方向的思想。基于这些现有工作，作者设计了CALM-Net，整合边缘卷积（处理局部几何）、点注意力（捕获全局上下文）和曲率嵌入（编码表面变化）三种机制，并通过混合采样策略（训练时随机采样，推理时最远点采样）进一步提升了性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CALM-Net的核心思想是通过多分支神经网络同时捕捉点云数据中的局部几何结构、全局上下文信息和表面曲率特征，学习对视角和环境变化鲁棒的车辆嵌入。整体流程包括：1)输入点云进行下采样；2)并行处理三个分支-边缘卷支提取局部几何、点注意力捕获全局依赖、曲率嵌入编码表面变化；3)将各分支特征融合并通过卷积和批归一化处理；4)通过ReLU激活得到最终嵌入；5)使用二元交叉熵损失进行训练，判断两个点云是否对应同一车辆。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在LiDAR点云重识别中引入曲率感知机制，通过可学习的曲率嵌入实现细粒度几何推理；2)设计多分支架构，同时处理局部、上下文和结构特征；3)提出混合点下采样策略，结合随机采样的数据增强和FPS的结构保持优势；4)精心设计特征融合方法。相比之前工作，本文专注于点云而非图像数据，引入曲率嵌入这一新特征，采用多分支而非单一架构，并使用混合采样策略，在nuScenes数据集上实现了比最强基线高1.97%的准确率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了CALM-Net，一个结合边缘卷积、点注意力和曲率嵌入的多分支神经网络，通过从LiDAR点云中学习判别性和几何驱动的特征，显著提高了车辆重识别的准确率，特别是在复杂城市环境中的视角变化和光照变化情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents CALM-Net, a curvature-aware LiDAR point cloud-basedmulti-branch neural network for vehicle re-identification. The proposed modeladdresses the challenge of learning discriminative and complementary featuresfrom three-dimensional point clouds to distinguish between vehicles. CALM-Netemploys a multi-branch architecture that integrates edge convolution, pointattention, and a curvature embedding that characterizes local surface variationin point clouds. By combining these mechanisms, the model learns richergeometric and contextual features that are well suited for there-identification task. Experimental evaluation on the large-scale nuScenesdataset demonstrates that CALM-Net achieves a mean re-identification accuracyimprovement of approximately 1.97\% points compared with the strongest baselinein our study. The results confirms the effectiveness of incorporating curvatureinformation into deep learning architectures and highlight the benefit ofmulti-branch feature learning for LiDAR point cloud-based vehiclere-identification.</description>
      <author>example@mail.com (Dongwook Lee, Sol Han, Jinwhan Kim)</author>
      <guid isPermaLink="false">2510.14576v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>High-Order Meshfree Surface Integration, Including Singular Integrands</title>
      <link>http://arxiv.org/abs/2510.14236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发并测试了针对表面点云的高阶积分方法，解决了在任意分段光滑表面上进行精确积分的问题。&lt;h4&gt;背景&lt;/h4&gt;表面积分在工程和科学领域的多种应用中至关重要，特别是在涉及偏微分方程的各种积分方法中。基于网格的方法需要曲面网格才能实现高阶收敛，这在许多表面上难以可靠获得；而无网格方法通常需要在感兴趣域上精确积分一组函数，但这些积分在大多数表面上没有闭式形式。&lt;h4&gt;目的&lt;/h4&gt;开发能够在任意、分段光滑表面（有边界或无边界）上进行高精度积分的方法，且不需要特定的点排列或初始三角剖分。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两种完全无网格的积分方法，适用于任意分段光滑表面。这些方法不需要特定的点排列或表面的初始三角剖分。此外，作者还展示了如何扩展这些方法以处理奇异积分。&lt;h4&gt;主要发现&lt;/h4&gt;1. 开发了两种在任意分段光滑表面上进行积分的方法；2. 这些方法完全无网格，不需要特定的点排列或初始三角剖分；3. 方法可以处理奇异积分，同时保持高精度；4. 无需在奇点附近改变点密度即可维持高精度。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为在任意表面上进行高阶积分提供了有效解决方案，克服了传统网格方法和无网格方法的局限性，并能处理奇异积分情况。&lt;h4&gt;翻译&lt;/h4&gt;我们开发并测试了针对表面点云的高阶积分方法。在表面上积分函数的任务在工程和科学的一系列应用中出现，特别是在涉及偏微分方程的各种积分方法中。基于网格的方法需要曲面网格才能实现高阶收敛，这在许多表面上难以可靠获得，而大多数无网格方法需要在感兴趣域上精确积分一组函数（如径向基函数）；这些积分在大多数表面上通常没有闭式形式。我们描述了两种在任意、分段光滑表面（有边界或无边界）上进行积分的方法。我们的方法不需要特定的点排列或表面的初始三角剖分，使它们完全无网格。我们还展示了如何扩展这些方法以处理奇异积分，同时保持高精度，而无需在奇点附近改变点密度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We develop and test high-order methods for integration on surface pointclouds. The task of integrating a function on a surface arises in a range ofapplications in engineering and the sciences, particularly those involvingvarious integral methods for partial differential equations. Mesh-based methodsrequire a curved mesh for high-order convergence, which can be difficult toreliably obtain on many surfaces, and most meshfree methods require the abilityto integrate a set of functions (such as radial basis functions) exactly on thedomain of interest; these integrals are generally not known in closed form onmost surfaces. We describe two methods for integrating on arbitrary,piecewise-smooth surfaces with or without boundary. Our approaches do notrequire a particular arrangement of points or an initial triangulation of thesurface, making them completely meshfree. We also show how the methods can beextended to handle singular integrals while maintaining high accuracy withoutchanging the point density near singularities.</description>
      <author>example@mail.com (Daniel R. Venn, Steven J. Ruuth)</author>
      <guid isPermaLink="false">2510.14236v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space</title>
      <link>http://arxiv.org/abs/2510.14234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的无模型方法，用于带有关键点约束的三维可变形物体形状控制。该方法通过深度学习从点云中提取关键点作为特征向量，保留了物体的空间信息同时降低了特征空间维度。将操控问题简化为视觉伺服问题，使用变形雅可比矩阵描述形状动力学，并通过结合障碍李雅普诺夫函数的预设性能控制方法提高控制精度。实验验证了该方法的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;操控三维可变形物体对机器人系统具有显著挑战，主要因为可变形物体具有无限维状态空间和复杂的变形动力学特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的无模型方法，用于带有关键点约束的可变形物体形状控制，提高操控的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;不同于依赖特征降维的现有方法，所提出的控制器利用从可变形物体点云中通过深度学习方法提取的关键点坐标作为特征向量。通过提取关键点，将可变形物体操控简化为视觉伺服问题，使用变形雅可比矩阵描述形状动力学。同时，开发了一种结合障碍李雅普诺夫函数的预设性能控制方法，以强制执行关键点的约束，提高控制精度。&lt;h4&gt;主要发现&lt;/h4&gt;通过提取关键点，成功降低了特征空间维度同时保留了物体空间信息；结合障碍李雅普诺夫函数的预设性能控制方法有效提高了控制精度；实验结果验证了所提出方法的有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的无模型方法通过深度学习提取关键点并结合预设性能控制，有效解决了三维可变形物体形状控制问题，具有较好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;操控三维可变形物体对机器人系统具有显著挑战，因为它们具有无限维状态空间和复杂的变形动力学。本文提出了一种新型的带有关键点约束的无模型形状控制方法。与依赖特征降维的现有方法不同，所提出的控制器利用从可变形物体点云中通过深度学习方法提取的关键点坐标作为特征向量。这种方法不仅降低了特征空间的维度，还保留了物体的空间信息。通过提取关键点，可变形物体的操控被简化为一个视觉伺服问题，其中形状动力学使用变形雅可比矩阵描述。为了提高控制精度，开发了一种结合障碍李雅普诺夫函数的预设性能控制方法，以强制执行关键点的约束。使用李雅普诺夫方法严格分析并验证了闭环系统的稳定性。实验结果进一步证明了所提出方法的有效性和鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作三维可变形物体的控制挑战。可变形物体（如海绵、布料等）由于其形状可以无限变化，状态空间维度极高，且具有复杂的变形动力学特性，使得传统的机器人控制方法难以有效处理。这个问题在现实中非常重要，因为可变形物体操作在医疗手术、工业焊接、自动折叠衣物等领域有广泛应用，提高机器人对这类物体的操作能力可以扩展机器人在这些领域的应用，提高自动化水平，减少人工干预，并提高任务执行的精度和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计方法时，首先分析了现有可变形物体操作方法的局限性，包括基于模型的方法依赖物理模型但参数估计困难，以及无模型方法面临高维状态空间的挑战。作者借鉴了现有工作中的深度学习方法提取关键点、基于Jacobian的视觉伺服控制以及规定性能控制（PPC）和障碍Lyapunov函数（BLF）等技术。作者的创新点在于将PPC方法从已知Jacobian矩阵的视觉伺服任务迁移到Jacobian矩阵完全未知的可变形物体操作任务中，并结合关键点提取方法，在保留空间信息的同时降低特征维度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从可变形物体的3D点云中提取关键点作为特征向量，这些关键点保留了物体的空间信息同时降低了维度；将可变形物体操作简化为视觉伺服问题，使用变形Jacobian矩阵描述形状动力学；设计规定性能控制器，通过障碍Lyapunov函数强制执行关键点的约束；使用神经网络近似未知的Jacobian矩阵。整体实现流程包括：使用Key-Grid神经网络从点云中提取关键点；计算关键点误差；使用规定性能函数定义误差边界；将误差转换为转换误差；设计基于Jacobian的控制器，使用神经网络近似Jacobian矩阵；应用自适应律更新神经网络权重；通过Lyapunov分析确保系统稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出一种新的无模型方法，使用关键点坐标作为特征向量，保留了空间信息同时降低了维度；将规定性能控制方法从视觉伺服任务迁移到可变形物体操作任务，其中Jacobian矩阵完全未知；设计障碍Lyapunov函数来确保关键点误差的边界约束；结合深度学习和自适应控制方法，提高了控制精度和鲁棒性。相比之前工作，该方法直接从3D点云提取关键点，而不是使用手动标记的关键点；使用改进的PPC框架，而不是基于图网络的MPC控制器；与传统降维方法相比，保留了物理和空间信息；与其他避免潜在抽象的方法相比，不局限于二维结构、刚性假设或强模型依赖。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于空间潜在空间的关键点约束规定性能控制方法，有效解决了三维可变形物体操作中的高维状态空间和复杂变形动力学挑战，显著提高了控制精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manipulating three-dimensional (3D) deformable objects presents significantchallenges for robotic systems due to their infinite-dimensional state spaceand complex deformable dynamics. This paper proposes a novel model-freeapproach for shape control with constraints imposed on key points. Unlikeexisting methods that rely on feature dimensionality reduction, the proposedcontroller leverages the coordinates of key points as the feature vector, whichare extracted from the deformable object's point cloud using deep learningmethods. This approach not only reduces the dimensionality of the feature spacebut also retains the spatial information of the object. By extracting keypoints, the manipulation of deformable objects is simplified into a visualservoing problem, where the shape dynamics are described using a deformationJacobian matrix. To enhance control accuracy, a prescribed performance controlmethod is developed by integrating barrier Lyapunov functions (BLF) to enforceconstraints on the key points. The stability of the closed-loop system isrigorously analyzed and verified using the Lyapunov method. Experimentalresults further demonstrate the effectiveness and robustness of the proposedmethod.</description>
      <author>example@mail.com (Ning Han, Gu Gong, Bin Zhang, Yuexuan Xu, Bohan Yang, Yunhui Liu, David Navarro-Alarcon)</author>
      <guid isPermaLink="false">2510.14234v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction</title>
      <link>http://arxiv.org/abs/2510.14147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图，在各种数据集和度量标准下表现出卓越的性能和并行扩展性。&lt;h4&gt;背景&lt;/h4&gt;计算固定半径近邻图是许多数据分析算法的重要第一步。近邻图在某种度量下连接接近的点，为点云赋予组合结构。随着计算能力和数据获取方法的进步，各种大型科学数据集需要可扩展的解决方案来处理下游分析中的常见子程序。&lt;h4&gt;目的&lt;/h4&gt;解决现有并行近邻搜索工作在精确解和非欧几里得度量方面的局限性，提供一个可扩展的稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图。&lt;h4&gt;方法&lt;/h4&gt;提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法。提供了覆盖树构建的共享内存算法，并展示了其与最先进的固定半径搜索数据结构的竞争力。然后介绍了两种分布式内存算法：简单的点分区策略和空间分区策略，它们利用每个节点上的覆盖树算法。&lt;h4&gt;主要发现&lt;/h4&gt;算法在各种真实和合成数据集上表现出并行扩展性，适用于传统和非传统度量。在包含一百万个点的真实世界高维数据集上，对于每个顶点平均70个邻居的图，使用1024个核心实现了高达678.34倍的速度提升；对于每个顶点平均500个邻居的图，使用4096个核心实现了高达1590.99倍的速度提升。&lt;h4&gt;结论&lt;/h4&gt;该算法能够有效处理大规模数据集的近邻图计算，在多种数据集和度量标准下表现出良好的并行扩展性。&lt;h4&gt;翻译&lt;/h4&gt;计算固定半径近邻图是许多数据分析算法的重要第一步。近邻图在某种度量下连接接近的点，为点云赋予组合结构。随着计算能力和数据获取方法的进步，各种大型科学数据集需要可扩展的解决方案来处理下游分析中的常见子程序。现有的并行近邻搜索工作在最近邻和近似最近邻搜索问题上取得了很大进展，特别关注欧几里得空间。然而，许多应用程序需要精确解和非欧几里得度量。本文提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图。我们提供了覆盖树构建的共享内存算法，并展示了其与最先进的固定半径搜索数据结构的竞争力。然后，我们介绍了用于近邻图问题的两种分布式内存算法：一种简单的点分区策略和一种空间分区策略，它们利用每个节点上的覆盖树算法。我们的算法在各种真实和合成数据集上表现出并行扩展性，适用于传统和非传统度量。在包含一百万个点的真实世界高维数据集上，对于每个顶点平均70个邻居的图，使用1024个核心实现了比最先进方法高达678.34倍的速度提升；对于每个顶点平均500个邻居的图，使用4096个核心实现了高达1590.99倍的速度提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing fixed-radius near-neighbor graphs is an important first step formany data analysis algorithms. Near-neighbor graphs connect points that areclose under some metric, endowing point clouds with a combinatorial structure.As computing power and data acquisition methods advance, diverse sources oflarge scientific datasets would greatly benefit from scalable solutions to thiscommon subroutine for downstream analysis. Prior work on parallel nearestneighbors has made great progress in problems like k-nearest and approximatenearest neighbor search problems, with particular attention on Euclideanspaces. Yet many applications need exact solutions and non-Euclidean metrics.This paper presents a scalable sparsity-aware distributed memory algorithmusing cover trees to compute near-neighbor graphs in general metric spaces. Weprovide a shared-memory algorithm for cover tree construction and demonstrateits competitiveness with state-of-the-art fixed-radius search data structures.We then introduce two distributed-memory algorithms for the near-neighbor graphproblem, a simple point-partitioning strategy and a spatial-partitioningstrategy, which leverage the cover tree algorithm on each node. Our algorithmsexhibit parallel scaling across a variety of real and synthetic datasets forboth traditional and non-traditional metrics. On real world high dimensionaldatasets with one million points, we achieve speedups up to 678.34x over thestate-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (onaverage), and up to 1590.99x using 4096 cores for graphs with 500 neighbors pervertex (on average).</description>
      <author>example@mail.com (Gabriel Raulet, Dmitriy Morozov, Aydin Buluc, Katherine Yelick)</author>
      <guid isPermaLink="false">2510.14147v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Geometric local parameterization for solving Hele-Shaw problems with surface tension</title>
      <link>http://arxiv.org/abs/2510.14088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种解决二维Hele-Shaw自由边界问题（带有表面张力）的新型计算框架。该方法使用点云表示移动边界，无需全局参数化，并通过广义移动最小二乘法构建局部几何图表，实现高阶几何量近似。研究提供了严格的收敛分析，并通过数值实验验证了方法的有效性，展示了复杂形状在表面张力作用下向圆形平衡状态的正确演化。&lt;h4&gt;背景&lt;/h4&gt;Hele-Shaw自由边界问题是流体力学中的重要问题，特别是在研究具有表面张力的界面动力学时。传统方法通常需要全局参数化来表示移动边界，这在处理复杂几何形状时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的计算框架，能够高效、准确地解决带有表面张力的二维Hele-Shaw自由边界问题，克服传统方法在处理复杂几何形状时的局限性，并实现高阶收敛精度。&lt;h4&gt;方法&lt;/h4&gt;1. 使用点云表示移动边界，消除全局参数化的需求；2. 应用广义移动最小二乘法构建局部几何图表；3. 直接从点云数据高阶近似几何量（如曲率）；4. 使用局部参数化离散化控制边界积分方程；5. 包含奇异积分的解析公式；6. 进行严格的收敛分析，建立一致性和稳定性条件。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所提出的方法实现了高阶空间收敛；2. 获得了预期的时域收敛率；3. 误差界限与均匀采样点云数据大小、边界光滑度和数值积分规则阶数相关；4. 复杂初始形状在表面张力作用下正确演变为圆形平衡状态。&lt;h4&gt;结论&lt;/h4&gt;该新型计算框架为解决二维Hele-Shaw自由边界问题提供了有效方法，点云表示和局部几何图表的构建使得方法能够处理复杂几何形状，同时保持高阶收敛精度。数值实验验证了理论分析的正确性和方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们介绍了一种解决带有表面张力的二维Hele-Shaw自由边界问题的新型计算框架。移动边界由点云表示，消除了对全局参数化的需求。我们的方法利用广义移动最小二乘法构建局部几何图表，能够直接从点云数据高阶近似几何量（如曲率）。这种局部参数化被系统地用于离散化控制边界积分方程，包括奇异积分的解析公式。我们为所提出的空间离散化提供了严格的收敛分析，在特定条件下建立了一致性和稳定性。导出的误差界限基于移动边界上均匀采样点云数据的大小、边界的光滑度和数值积分规则的阶数。数值实验验证了理论结果，展示了高阶空间收敛和预期的时域收敛率。通过复杂初始形状的模拟进一步说明了该方法的有效性，这些形状在表面张力的影响下正确地演变为圆形平衡状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce a novel computational framework for solving thetwo-dimensional Hele-Shaw free boundary problem with surface tension. Themoving boundary is represented by point clouds, eliminating the need for aglobal parameterization. Our approach leverages Generalized Moving LeastSquares (GMLS) to construct local geometric charts, enabling high-orderapproximations of geometric quantities such as curvature directly from thepoint cloud data. This local parameterization is systematically employed todiscretize the governing boundary integral equation, including an analyticalformula of the singular integrals. We provide a rigorous convergence analysisfor the proposed spatial discretization, establishing consistency and stabilityunder certain conditions. The resulting error bound is derived in terms of thesize of the uniformly sampled point cloud data on the moving boundary, thesmoothness of the boundary, and the order of the numerical quadrature rule.Numerical experiments confirm the theoretical findings, demonstratinghigh-order spatial convergence and the expected temporal convergence rates. Themethod's effectiveness is further illustrated through simulations of complexinitial shapes, which correctly evolve towards circular equilibrium statesunder the influence of surface tension.</description>
      <author>example@mail.com (Zengyan Zhang, Wenrui Hao, John Harlim)</author>
      <guid isPermaLink="false">2510.14088v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation</title>
      <link>http://arxiv.org/abs/2510.14190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ConDA(对比扩散对齐)框架，通过对比学习在扩散嵌入中组织潜在空间，使其与系统动力学对齐，从而实现更可控和可解释的生成操作。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在生成任务上表现出色，但其潜在空间没有被明确组织用于可解释的控制，限制了其在需要精确控制的应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来组织扩散模型的潜在空间，使其能够支持忠实插值、外推和可控生成，同时保持生成质量。&lt;h4&gt;方法&lt;/h4&gt;提出ConDA框架，应用对比学习于扩散嵌入中，将潜在几何结构与系统动力学对齐，使遍历方向反映潜在的动力学因素，并支持非线性轨迹遍历。&lt;h4&gt;主要发现&lt;/h4&gt;在流体动力学、神经钙成像、治疗性神经刺激和面部表情等多个基准测试中，ConDA产生了比线性遍历和基于条件的基线更具可解释性的潜在表示，同时提高了可控性。&lt;h4&gt;结论&lt;/h4&gt;扩散潜变量编码了与动力学相关的结构，但要有效利用这种结构，需要沿着潜在流形进行潜在组织和遍历。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型在生成方面表现出色，但它们的潜在空间没有被明确组织用于可解释的控制。我们引入了ConDA(对比扩散对齐)，这是一个在扩散嵌入中应用对比学习的框架，将潜在几何结构与系统动力学对齐。受最近进展的启发，这些进展表明对比目标可以恢复更多解缠和结构化的表示，ConDA组织扩散潜变量，使得遍历方向反映潜在的动力学因素。在这个对比结构化的空间中，ConDA支持非线性轨迹遍历，实现忠实插值、外推和可控生成。在流体动力学、神经钙成像、治疗性神经刺激和面部表情的基准测试中，ConDA与线性遍历和基于条件的基线相比，产生了具有改进可解释性的潜在表示。这些结果表明扩散潜变量编码了动力学相关的结构，但利用这种结构需要沿着潜在流形进行潜在组织和遍历。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models excel at generation, but their latent spaces are notexplicitly organized for interpretable control. We introduce ConDA (ContrastiveDiffusion Alignment), a framework that applies contrastive learning withindiffusion embeddings to align latent geometry with system dynamics. Motivatedby recent advances showing that contrastive objectives can recover moredisentangled and structured representations, ConDA organizes diffusion latentssuch that traversal directions reflect underlying dynamical factors. Withinthis contrastively structured space, ConDA enables nonlinear trajectorytraversal that supports faithful interpolation, extrapolation, and controllablegeneration. Across benchmarks in fluid dynamics, neural calcium imaging,therapeutic neurostimulation, and facial expression, ConDA producesinterpretable latent representations with improved controllability compared tolinear traversals and conditioning-based baselines. These results suggest thatdiffusion latents encode dynamics-relevant structure, but exploiting thisstructure requires latent organization and traversal along the latent manifold.</description>
      <author>example@mail.com (Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick)</author>
      <guid isPermaLink="false">2510.14190v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ViTacGen: Robotic Pushing with Vision-to-Touch Generation</title>
      <link>http://arxiv.org/abs/2510.14117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViTacGen是一个创新的机器人操作框架，通过视觉到触觉的生成解决了触觉传感器限制问题，结合视觉和生成触觉数据通过强化学习实现高性能的机器人推操作，在模拟和真实实验中表现出色，成功率高达86%。&lt;h4&gt;背景&lt;/h4&gt;机器人推操作需要触觉反馈来捕捉末端执行器和物体之间的接触力和动力学，但真实触觉传感器面临高成本、脆弱性、校准困难和传感器差异等挑战，而仅基于视觉的策略难以获得满意性能。&lt;h4&gt;目的&lt;/h4&gt;提出ViTacGen框架，用于视觉机器人推操作，在强化学习中实现视觉到触觉的生成，消除对高分辨率真实触觉传感器的依赖，实现视觉系统上的有效零样本部署。&lt;h4&gt;方法&lt;/h4&gt;ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化触觉表示），以及一个基于视觉和生成触觉观察的对比学习融合视觉-触觉数据的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中验证了方法的有效性，展示了其卓越的性能，成功率达到86%。&lt;h4&gt;结论&lt;/h4&gt;ViTacGen框架能够在不依赖高分辨率触觉传感器的情况下实现有效的机器人推操作，通过视觉到触觉的生成实现了在视觉系统上的零样本部署。&lt;h4&gt;翻译&lt;/h4&gt;机器人推操作是一种基础的操作任务，需要触觉反馈来捕捉末端执行器和物体之间的微妙接触力和动力学。然而，真实的触觉传感器通常面临硬件限制，如高成本和脆弱性，以及部署挑战，包括校准和不同传感器之间的差异，而仅基于视觉的策略难以获得令人满意的性能。受人类从视觉推断触觉状态能力的启发，我们提出了ViTacGen，一个新颖的机器人操作框架，专为视觉机器人推操作设计，在强化学习中实现视觉到触觉的生成，以消除对高分辨率真实触觉传感器的依赖，实现仅在视觉系统上的有效零样本部署。具体而言，ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化的触觉表示），随后是一个基于视觉和生成触觉观察的对比学习融合视觉-触觉数据的强化学习策略。我们在模拟和真实世界实验中都验证了我们方法的有效性，展示了其卓越的性能，成功率达到86%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是机器人推动任务中依赖昂贵且脆弱的触觉传感器的问题。这个问题很重要，因为触觉反馈对捕捉物体间细微接触力和动态至关重要，但真实触觉传感器成本高、易损坏、需要精确校准，且不同传感器间存在差异，限制了高性能机器人操作系统的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类能从视觉推断触觉状态的启发，观察到现有方法要么依赖昂贵触觉传感器，要么仅使用视觉但性能不足。他们设计了编码器-解码器的视觉到触觉生成网络(VT-Gen)和强化学习策略网络(VT-Con)。借鉴了人类视觉-触觉交互能力、Soft Actor-Critic强化学习算法、MoCo对比学习框架、Tactile Gym模拟平台、注意力机制和VGG损失函数等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模拟人类从视觉推断触觉的能力，让机器人仅通过视觉'感知'触觉，使用生成的触觉接触深度图像作为标准化表示，并通过对比学习对齐视觉和触觉特征。整体流程：1)在模拟环境中收集配对的视觉和触觉数据；2)训练VT-Gen网络从视觉生成触觉深度图像；3)冻结VT-Gen，训练VT-Con强化学习策略；4)零样本部署到真实视觉-only机器人系统。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出ViTacGen框架消除对触觉传感器的依赖；2)设计VT-Gen生成标准化触觉表示；3)提出VT-Con通过对比学习融合视觉-触觉特征；4)实现零样本部署；5)使用接触深度图解决传感器差异问题。不同之处：相比仅视觉方法提供更丰富感知；相比触觉传感器方法降低成本复杂度；相比简单特征拼接实现更有效跨模态对齐；相比校准方法具有更好通用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ViTacGen通过模拟人类从视觉推断触觉的能力，实现了仅使用视觉信息的机器人精确推动，消除了对昂贵触觉传感器的依赖，同时保持了高性能操作能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic pushing is a fundamental manipulation task that requires tactilefeedback to capture subtle contact forces and dynamics between the end-effectorand the object. However, real tactile sensors often face hardware limitationssuch as high costs and fragility, and deployment challenges involvingcalibration and variations between different sensors, while vision-onlypolicies struggle with satisfactory performance. Inspired by humans' ability toinfer tactile states from vision, we propose ViTacGen, a novel robotmanipulation framework designed for visual robotic pushing with vision-to-touchgeneration in reinforcement learning to eliminate the reliance onhigh-resolution real tactile sensors, enabling effective zero-shot deploymenton visual-only robotic systems. Specifically, ViTacGen consists of anencoder-decoder vision-to-touch generation network that generates contact depthimages, a standardized tactile representation, directly from visual imagesequence, followed by a reinforcement learning policy that fuses visual-tactiledata with contrastive learning based on visual and generated tactileobservations. We validate the effectiveness of our approach in both simulationand real world experiments, demonstrating its superior performance andachieving a success rate of up to 86\%.</description>
      <author>example@mail.com (Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo)</author>
      <guid isPermaLink="false">2510.14117v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title>
      <link>http://arxiv.org/abs/2510.13245v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SketchSem3D数据集和Cylinder Mamba Diffusion (CymbaDiff)方法，用于从手绘草图生成高质量的3D室外语义场景。&lt;h4&gt;背景&lt;/h4&gt;室外3D语义场景生成在都市仿真和自动驾驶等领域有重要应用，但该领域的发展受到缺乏公开可用、良好注释的数据集的制约。&lt;h4&gt;目的&lt;/h4&gt;引入首个大规模基准数据集SketchSem3D，用于从抽象手绘草图和卫星图像的伪标记注释生成3D室外语义场景，并提出一种增强空间一致性的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出了Cylinder Mamba Diffusion (CymbaDiff)模型，该方法施加结构化的空间排序，明确捕获圆柱连续性和垂直层次结构，并保留生成的场景中的物理邻域关系和全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在SketchSem3D上的大量实验表明，CymbaDiff实现了优越的语义一致性、空间真实性和跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SketchSem3数据集和CymbaDiff方法为室外3D语义场景生成提供了新的基准和解决方案，有助于推动该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;室外3D语义场景生成为都市仿真和自动驾驶等应用生成真实且语义丰富的环境。然而，这一方向的发展受到缺乏公开可用、良好注释的数据集的限制。我们引入了SketchSem3D，这是第一个大规模基准，用于从抽象手绘草图和卫星图像的伪标记注释生成3D室外语义场景。SketchSem3D包含两个子集：基于语义的KITTI草图和基于KITTI-360的草图（包含LiDAR体素及其相应的草图和注释卫星图像），以实现标准化、严格和多样化的评估。我们还提出了圆柱形Mamba扩散模型（CymbaDiff），显著增强了室外3D场景生成的空间一致性。CymbaDiff施加结构化的空间排序，明确捕获圆柱连续性和垂直层次结构，并保留生成的场景中的物理邻域关系和全局上下文。在SketchSem3D上的大量实验表明，CymbaDiff实现了优越的语义一致性、空间真实性和跨数据集泛化能力。代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于草图的3D户外语义场景生成问题，特别是缺乏公开的大规模标注数据集和现有方法在户外场景中的局限性。这个问题在现实中非常重要，因为高质量的城市场景生成对自动驾驶模拟、城市规划等应用至关重要，而传统方法要么依赖昂贵的传感器数据，要么无法生成复杂且语义丰富的户外环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D户外场景生成方法的局限性，如鸟瞰图(BEV)方法缺乏3D结构信息，多尺度方法计算复杂。他们借鉴了状态空间模型(SSMs)在图像处理和点云分析中的成功应用，结合扩散模型在生成任务中的优势。作者还利用了CLIP和SAM等现有模型进行数据集构建，但创新性地将这些技术整合到一个专门针对户外场景的框架中，通过圆柱坐标系统改进了空间表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化空间扩散模型，结合笛卡尔和圆柱坐标系统的优势，增强3D场景生成的空间连贯性。整体流程包括：1)构建SketchSem3D数据集，包含草图、卫星图像和3D体素；2)使用场景结构估计网络(SSEN)提取结构信息；3)通过潜在映射网络(LMN)压缩输入条件；4)利用CymbaDiff去噪网络，结合三重Mamba模块和圆柱Mamba层进行生成；5)从噪声逐步去噪，最终生成高质量的3D语义场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'基于草图的3D户外场景生成'新任务；2)构建首个专门的大规模基准数据集SketchSem3D；3)提出CymbaDiff模型，结合圆柱Mamba块增强空间连贯性；4)引入圆柱坐标系统来更好地表示户外场景的空间关系。相比之前工作，CymbaDiff避免了多尺度方法的计算复杂性，解决了BEV方法缺乏3D结构信息的问题，并首次将基于草本的3D生成扩展到复杂户外场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过CymbaDiff方法和SketchSem3D数据集，首次实现了从简单草图和卫星图像生成高质量、语义连贯的大规模3D城市场景，为自动驾驶和城市规划等应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor 3D semantic scene generation produces realistic and semantically richenvironments for applications such as urban simulation and autonomous driving.However, advances in this direction are constrained by the absence of publiclyavailable, well-annotated datasets. We introduce SketchSem3D, the firstlarge-scale benchmark for generating 3D outdoor semantic scenes from abstractfreehand sketches and pseudo-labeled annotations of satellite images.SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-basedKITTI-360 (containing LiDAR voxels along with their corresponding sketches andannotated satellite images), to enable standardized, rigorous, and diverseevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) thatsignificantly enhances spatial coherence in outdoor 3D scene generation.CymbaDiff imposes structured spatial ordering, explicitly captures cylindricalcontinuity and vertical hierarchy, and preserves both physical neighborhoodrelationships and global context within the generated scenes. Extensiveexperiments on SketchSem3D demonstrate that CymbaDiff achieves superiorsemantic consistency, spatial realism, and cross-dataset generalization. Thecode and dataset will be available athttps://github.com/Lillian-research-hub/CymbaDiff</description>
      <author>example@mail.com (Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian)</author>
      <guid isPermaLink="false">2510.13245v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration</title>
      <link>http://arxiv.org/abs/2510.13729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Symposium on Visual Computing (ISVC)  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LiFMCR，一个用于多微透镜阵列光场相机配准的新数据集，提供同步图像序列和高精度姿态数据，用于严格评估多相机光场配准方法。&lt;h4&gt;背景&lt;/h4&gt;现有光场数据集仅限于单相机设置，通常缺乏外部真实值，限制了多相机光场配准方法的评估。&lt;h4&gt;目的&lt;/h4&gt;创建一个独特的多相机光场数据集，结合高分辨率光场相机图像和精确的6自由度姿态数据，以实现多相机光场配准方法的严格评估。&lt;h4&gt;方法&lt;/h4&gt;提供两种互补的配准方法：1)基于RANSAC的鲁棒3D变换估计，使用跨视点点云；2)从单个光场图像估计外源性6-DoF姿态的光场PnP算法。两种方法都明确集成了光场相机模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法与真实值显示出良好的对齐，支持可靠的多视点光场处理。&lt;h4&gt;结论&lt;/h4&gt;LiFMCR数据集及其配套方法为多相机光场配准提供了基准，能够准确且可扩展地进行多相机配准。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了LiFMCR，一个用于多微透镜阵列光场相机配准的新颖数据集。虽然现有的光场数据集仅限于单相机设置且通常缺乏外部真实值，但LiFMCR提供了来自两个高分辨率Raytrix R32光场相机的同步图像序列，以及由Vicon动作捕捉系统记录的高精度6自由度姿态。这种独特组合能够严格评估多相机光场配准方法。作为基准，我们提供了两种互补的配准方法：一种基于RANSAC的鲁棒3D变换估计，使用跨视点点云；以及一种从单个光场图像估计外源性6-DoF姿态的光场PnP算法。两种方法都明确集成了光场相机模型，实现准确且可扩展的多相机配准。实验显示与真实值有良好的对齐，支持可靠的多视点光场处理。项目页面：https://lifmcr.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多相机光场相机（plenoptic cameras）注册缺乏标准数据集和基准测试的问题。这个问题很重要，因为准确的3D重建对自主系统和机器人应用至关重要，而结合多个光场相机可以通过立体视觉优势扩展深度范围和精度，提高深度感知能力和场景理解能力。现有的光场数据集通常局限于单相机设置且缺乏外部真实值，限制了多相机注册方法的评估和改进。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有光场数据集的局限性，特别是缺乏多相机设置和精确真实值的问题。他们借鉴了现有工作：使用LiFCal进行内参校准，采用SIFT特征提取和匹配，以及基于RANSAC的3D变换估计方法。作者设计了两种互补的注册方法：一种基于3D点云对齐的RANSAC方法，另一种是首次应用于光场数据的PnP算法。两种方法都明确集成了光场相机模型，以准确处理光场相机的特殊光学和几何特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供一个包含同步多视角光场数据和精确6-DoF姿态真实值的数据集，并设计两种互补的多相机注册方法，一种基于3D点云对齐，另一种基于光场PnP算法，都考虑光场相机的特殊光学特性。3D RANSAC方法流程：内参校准→点云生成→SIFT特征提取与匹配→3D RANSAC对齐→计算相机间相对变换。光场PnP方法流程：仅参考相机点云→镜头畸变校正→光场模型透视投影→特征匹配→鲁棒基础矩阵估计→RANSAC PnP→Levenberg-Marquardt优化细化姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出LiFMCR数据集，首次提供同步多视角光场序列和精确6-DoF真实值；2)提出两种互补注册方法，包括基于RANSAC的3D变换估计和首个光场PnP算法；3)两种方法都明确集成光场相机模型；4)提供完整内参和外参校准流程；5)使用Vicon系统提供亚毫米级精度真实值。相比之前工作：现有数据集多为单相机且缺乏真实值，而LiFMCR提供多相机同步数据和精确姿态；现有方法不专门针对光场多相机注册，而本文方法考虑了光场相机特性；首次将PnP算法应用于光场数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了LiFMCR数据集和两种互补的光场多相机注册方法，填补了多视角光场数据与精确姿态真实值结合的空白，为光场相机在3D重建、SLAM等应用中的可靠使用提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LiFMCR, a novel dataset for the registration of multiple microlens array (MLA)-based light field cameras. While existing light field datasetsare limited to single-camera setups and typically lack external ground truth,LiFMCR provides synchronized image sequences from two high-resolution RaytrixR32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)poses recorded by a Vicon motion capture system. This unique combinationenables rigorous evaluation of multi-camera light field registration methods.  As a baseline, we provide two complementary registration approaches: a robust3D transformation estimation via a RANSAC-based method using cross-view pointclouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses fromsingle light field images. Both explicitly integrate the plenoptic cameramodel, enabling accurate and scalable multi-camera registration. Experimentsshow strong alignment with the ground truth, supporting reliable multi-viewlight field processing.  Project page: https://lifmcr.github.io/</description>
      <author>example@mail.com (Aymeric Fleith, Julian Zirbel, Daniel Cremers, Niclas Zeller)</author>
      <guid isPermaLink="false">2510.13729v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
  <item>
      <title>Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization</title>
      <link>http://arxiv.org/abs/2510.13619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the preprint version of the paper published in: Proceedings  of the 37th International Technical Meeting of the Satellite Division of The  Institute of Navigation (ION GNSS+ 2024), September 2024 The final version is  available at https://doi.org/10.33012/2024.19864&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可视化方法，用于辅助分析师分类影响激光雷达扫描匹配的逆境模式，通过生成矢量场图揭示点云数据中的差异模式。&lt;h4&gt;背景&lt;/h4&gt;激光雷达扫描匹配过程中存在多种逆境模式影响数据质量，分析师需要有效方法来识别和分类这些模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种离线可视化分析方法，帮助分析师识别和理解影响激光雷达扫描匹配的逆境机制。&lt;h4&gt;方法&lt;/h4&gt;提出一种生成矢量场图的可视化方法，该图能描述一对已配准点云之间的局部差异，揭示难以从原始数据中提取的模式。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟研究和现场实验验证，该方法能够帮助分析师识别和迭代移除逆境机制，逐步聚焦于更细微的数据差异。&lt;h4&gt;结论&lt;/h4&gt;所提出的可视化方法有效辅助了分析师对激光雷达扫描匹配中逆境模式的分类和分析，提高了数据处理的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一种可视化方法，用于帮助人类分析师分类影响激光雷达扫描匹配的逆境模式。我们的方法适用于离线分析而非实时分析。该方法生成一个矢量场图，用于描述一对已配准点云之间的局部差异。矢量场图能够揭示分析师难以从原始点云数据中提取的模式。在介绍我们的方法后，我们将该过程应用于两个概念验证示例：一个是模拟研究，另一个是现场实验。对于这两个数据集，人类分析师能够推理一系列逆境机制，并从原始数据中迭代地移除这些机制，以帮助将注意力集中在逐渐变小的差异上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.33012/2024.19864&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we introduce a visualization methodology to aid a human analystin classifying adversity modes that impact lidar scan matching. Our methodologyis intended for offline rather than real-time analysis. The method generates avector-field plot that characterizes local discrepancies between a pair ofregistered point clouds. The vector field plot reveals patterns that would bedifficult for the analyst to extract from raw point-cloud data. Afterintroducing our methodology, we apply the process to two proof-of-conceptexamples: one a simulation study and the other a field experiment. For bothdata sets, a human analyst was able to reason about a series of adversitymechanisms and iteratively remove those mechanisms from the raw data, to helpfocus attention on progressively smaller discrepancies.</description>
      <author>example@mail.com (Daniel Choate, Jason Rife)</author>
      <guid isPermaLink="false">2510.13619v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title>
      <link>http://arxiv.org/abs/2510.13307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构因果模型（SCM）的点云分割新类别发现方法，通过因果表示与推理的联合学习，解决仅使用已标记类别监督信息对新类别进行分割的问题。&lt;h4&gt;背景&lt;/h4&gt;点云分割中的新类别发现（3D-NCD）是一个挑战性问题，需要仅使用已标记（基础）3D类别的监督信息来学习能够分割未标记（新）3D类别的模型。&lt;h4&gt;目的&lt;/h4&gt;学习一个模型，仅使用已标记（基础）3D类别的监督信息，能够对未标记（新）3D类别进行分割。&lt;h4&gt;方法&lt;/h4&gt;引入结构因果模型（SCM）重新形式化3D-NCD问题，提出因果表示与推理的联合学习方法。通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别和新类别之间的因果关系；设计消除混杂因素的因果表示原型；使用图结构建模基础类别因果表示原型与新类别原型之间的因果关系，实现从基础到新类别的因果推理。&lt;h4&gt;主要发现&lt;/h4&gt;粗略或统计相关性学习可能导致新类别推理的混淆；通过引入因果约束可以准确发现与类别对应的点云表示；所提出的方法在3D和2D NCD语义分割任务上表现出优越性。&lt;h4&gt;结论&lt;/h4&gt;基于结构因果模型的方法能够有效解决点云分割中的新类别发现问题，通过因果表示与推理的联合学习，实现仅使用已标记类别监督信息对新类别的准确分割。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们专注于点云分割的新类别发现（3D-NCD），旨在学习一个模型，仅使用已标记（基础）3D类别的监督信息，能够对未标记（新）3D类别进行分割。这项任务的关键在于建立点表示与基础类别标签之间的准确相关性，以及基础类别和新类别点之间的表示相关性。粗略或统计相关性学习可能导致新类别推理的混淆。如果在学习过程中施加因果关系作为强相关约束，应该能够准确发现与类别对应的本质点云表示。为此，我们引入结构因果模型（SCM）重新形式化3D-NCD问题，并提出一种新方法，即因果表示与推理的联合学习。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别和新类别之间的因果关系。我们设计了一个消除混杂因素的因果表示原型，以捕获基础类别的因果表示。然后使用图结构建模基础类别因果表示原型与新类别原型之间的因果关系，实现从基础到新类别的因果推理。在3D和2D NCD语义分割任务上的大量实验和可视化结果证明了我们方法的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是点云分割中的新类别发现问题，即如何仅使用已知类别的监督信息来训练模型，使其能够分割场景中未标记的新类别物体。这个问题在自动驾驶、机器人感知等真实场景中非常重要，因为这些环境中可能出现各种未预先定义的物体类别，传统'封闭世界'假设的方法无法应对这种开放世界环境，而新类别发现能够减少人工标注负担，使模型能够适应动态变化的环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法的局限性，指出它们倾向于学习捷径特征而非本质特征，且忽视了基类与新类别之间的因果关系。基于此，作者引入结构因果模型重新形式化问题，并借鉴了因果表示学习、结构因果模型、图卷积网络和生成对抗网络等现有工作。通过这些借鉴，作者设计了因果表示原型学习来消除混杂因素，并使用图结构建模基类与新类别间的因果关系，实现了从未知类别中学习更准确的分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过因果表示学习消除点云数据中的非因果特征，捕获基类的本质表示，并建立基类和新类别之间的因果关系模型，实现从已知到未知的知识迁移。整体流程分为三部分：1)因果表示原型学习，使用对抗训练消除混杂因素，生成基类的因果表示原型；2)因果推理图构建，创建包含基类和新类原型的图结构，设计因果自适应邻接矩阵和约束优化图结构；3)基于GCN的伪标签生成，利用图卷积网络处理优化后的图，通过多层传播和邻居聚合为新类别生成高质量伪标签。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将因果学习引入3D NCD领域，专注于学习因果关系而非统计相关性；2)提出因果表示原型学习，通过对抗机制消除混杂因素；3)提出基于图的因果推理方法，显式建模基类到新类别的因果路径。相比之前的工作，本文方法能够处理点云数据中的复杂因果关系，而非仅依赖表面特征相似性，通过因果推理更好地处理新类别的语义关系，减少错误分类，提高了在开放世界环境中的适应性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入因果表示学习和因果推理，首次解决了点云分割中新类别发现问题中的因果机制建模，实现了从未知类别中学习更准确、更鲁棒的语义分割。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classesusing only the supervision from labeled (base) 3D classes. The key to this taskis to setup the exact correlations between the point representations and theirbase class labels, as well as the representation correlations between thepoints from base and novel classes. A coarse or statistical correlationlearning may lead to the confusion in novel class inference. lf we impose acausal relationship as a strong correlated constraint upon the learningprocess, the essential point cloud representations that accurately correspondto the classes should be uncovered. To this end, we introduce a structuralcausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,i.e., Joint Learning of Causal Representation and Reasoning. Specifically, wefirst analyze hidden confounders in the base class representations and thecausal relationships between the base and novel classes through SCM. We devisea causal representation prototype that eliminates confounders to capture thecausal representations of base classes. A graph structure is then used to modelthe causal relationships between the base classes' causal representationprototypes and the novel class prototypes, enabling causal reasoning from baseto novel classes. Extensive experiments and visualization results on 3D and 2DNCD semantic segmentation demonstrate the superiorities of our method.</description>
      <author>example@mail.com (Yang Li, Aming Wu, Zihao Zhang, Yahong Han)</author>
      <guid isPermaLink="false">2510.13307v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</title>
      <link>http://arxiv.org/abs/2510.13287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IROS Active Perception Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAMM-LOAM的新型LiDAR SLAM系统，通过点云分类和退化感知算法解决了特征稀疏环境下的定位建图问题，显著提高了室内环境中的导航精度。&lt;h4&gt;背景&lt;/h4&gt;LiDAR SLAM系统对精确导航和环境重建至关重要。当前点对平面ICP算法在结构化、特征丰富的环境中表现良好，但在特征稀疏、重复几何结构和高频运动场景下表现不佳，导致6自由度姿态估计退化。虽然最先进算法通过添加额外传感模态应对挑战，但纯LiDAR解决方案在这种条件下仍有限制。&lt;h4&gt;目的&lt;/h4&gt;解决特征稀疏、重复几何结构和高频运动场景下的SLAM退化问题，提出一种新颖的退化感知多度量LiDAR里程计与建图(DAMM-LOAM)模块。&lt;h4&gt;方法&lt;/h4&gt;通过基于表面法线和邻域分析的点云分类提高建图精度，将点分类为地面、墙壁、屋顶、边缘和非平面点以实现准确对应；应用基于退化的加权最小二乘ICP算法进行精确里程计估计；实现基于ScanContext的后端以支持稳健的回环闭合。&lt;h4&gt;主要发现&lt;/h4&gt;DAMM-LOAM在里程计准确性方面有显著改进，特别是在长走廊等室内环境中表现突出。&lt;h4&gt;结论&lt;/h4&gt;DAMM-LOAM系统有效解决了传统LiDAR SLAM在特定场景下的退化问题，通过创新的点云分类和退化感知算法，提高了室内环境中的导航精度。&lt;h4&gt;翻译&lt;/h4&gt;激光雷达同步定位与建图系统对于在各种应用中实现精确导航和环境重建至关重要。尽管当前点对平面ICP算法在结构化、特征丰富的环境中能有效工作，但在特征稀疏、重复几何结构和高频运动场景下表现不佳。这会导致6自由度姿态估计的退化。大多数最先进算法通过结合额外的传感模态来解决这些挑战，但纯激光雷达解决方案在这种条件下仍然面临限制。为解决这些问题，我们提出了一种新颖的退化感知多度量激光雷达里程计与建图模块。我们的系统通过基于表面法线和邻域分析的点云分类提高了建图精度。点被分类为地面、墙壁、屋顶、边缘和非平面点，从而实现准确的对应关系。然后应用基于退化的加权最小二乘ICP算法进行精确的里程计估计。此外，实现了基于扫描上下文的后端以支持稳健的回环闭合。DAMM-LOAM在里程计准确性方面表现出显著改进，特别是在长走廊等室内环境中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR SLAM系统在特征稀疏、重复几何结构和高频运动等场景下的退化问题，导致6自由度位姿估计不准确。这个问题很重要，因为许多实际应用场景（如走廊、隧道）都存在特征稀疏问题，而机器人导航、自动驾驶等领域需要在复杂环境中进行精确定位和地图构建，当前系统在这些挑战性场景中表现不佳，限制了技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析传统点对点或点对平面ICP算法的局限性，认识到在变化几何结构和特征稀疏环境下表现不佳的问题。设计方法时借鉴了现有工作：利用NV-LIOM的球形投影法线提取方法，但进一步进行几何分类；借鉴了条件数和特征值分析来检测退化，但设计了新的点级加权方案；并整合了现有的Scan Context算法作为后端。作者不是完全重新发明方法，而是在现有基础上进行了改进和整合创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合几何特征分类和退化感知的自适应加权来提高LiDAR SLAM系统在特征稀疏环境中的鲁棒性和准确性。整体流程包括：1)几何特征提取：将点云投影到球形范围图像，估计表面法线，并进行五类几何分类（地面、墙壁、屋顶、边缘、非平面点）；2)点云处理：自适应下采样并建立类别对应的点对；3)退化感知的位姿估计：分析Hessian矩阵特征值，为点分配权重，结合点对点和点对平面残差进行优化；4)后端处理：使用Scan Context进行回环检测和全局优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于法线图的语义特征提取：将点云分为五类而非传统的平面/非平面二分类；2)退化感知的逐点自适应加权：基于Hessian特征值分析为每个点分配权重，而非仅基于点类型数量；3)多度量残差整合：结合点对点和点对平面残差并动态调整权重；4)完整的端到端框架：整合几何特征提取、自适应加权优化和回环检测。相比之前工作，该方法提供了更细致的语义信息、更精确的退化处理和更全面的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAMM-LOAM通过基于法线图的语义特征分类和退化感知的自适应加权，显著提高了LiDAR SLAM系统在特征稀疏环境（如长走廊）中的定位精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential forenabling precise navigation and environmental reconstruction across variousapplications. Although current point-to-plane ICP algorithms perform effec-tively in structured, feature-rich environments, they struggle in scenarioswith sparse features, repetitive geometric structures, and high-frequencymotion. This leads to degeneracy in 6- DOF pose estimation. Moststate-of-the-art algorithms address these challenges by incorporatingadditional sensing modalities, but LiDAR-only solutions continue to facelimitations under such conditions. To address these issues, we propose a novelDegeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.Our system improves mapping accuracy through point cloud classification basedon surface normals and neighborhood analysis. Points are classified intoground, walls, roof, edges, and non-planar points, enabling accuratecorrespondences. A Degeneracy-based weighted least squares-based ICP algorithmis then applied for accurate odom- etry estimation. Additionally, a ScanContext based back-end is implemented to support robust loop closures.DAMM-LOAM demonstrates significant improvements in odometry accuracy,especially in indoor environments such as long corridors</description>
      <author>example@mail.com (Nishant Chandna, Akshat Kaushal)</author>
      <guid isPermaLink="false">2510.13287v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-based Adaptation in Large-scale Vision Models: A Survey</title>
      <link>http://arxiv.org/abs/2510.13219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是一篇关于视觉提示(Visual Prompting, VP)和视觉提示调优(Visual Prompt Tuning, VPT)的综合调查，提出了一种称为基于提示的适应(Prompt-based Adaptation, PA)的统一框架，对现有方法进行了分类，并探讨了PA在不同领域的应用、挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉领域，VP和VPT作为大规模视觉模型适应的轻量级有效替代方法，在'预训练后微调'范式中迅速发展。然而，当前研究中VP和VPT经常被互换使用，缺乏系统区分这些技术及其各自应用的明确界限。&lt;h4&gt;目的&lt;/h4&gt;重新审视VP和VPT的设计，将它们概念化为一个统一的PA框架，提供清晰的方法分类，并探索PA在不同领域的应用、挑战和未来方向，为研究人员和实践者提供明确的路线图。&lt;h4&gt;方法&lt;/h4&gt;提供了一种分类法，将现有方法分为可学习提示、生成提示和非可学习提示，并按注入粒度（像素级和令牌级）进一步组织。同时检查了PA在医学成像、3D点云和视觉语言任务等领域的整合，以及其在测试时适应和可信AI中的作用。&lt;h4&gt;主要发现&lt;/h4&gt;PA在医学成像、3D点云和视觉语言任务等不同领域有广泛应用，并且在测试时适应和可信AI中发挥重要作用。作者总结了当前基准，并确定了关键挑战和未来方向。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是第一篇专门针对PA的方法和应用的综合调查，旨在为研究人员和实践者提供清晰的路线图，以理解和探索PA相关研究的不断发展的格局。&lt;h4&gt;翻译&lt;/h4&gt;在计算机视觉中，视觉提示(Visual Prompting, VP)和视觉提示调优(Visual Prompt Tuning, VPT)最近已经出现作为轻量级且有效的替代方法，用于在'预训练后微调'范式中适应大规模视觉模型。然而，尽管进展迅速，它们的概念边界仍然模糊，因为VP和VPT在当前研究中经常被互换使用，反映了这些技术及其各自应用之间缺乏系统区分。在本调查中，我们从基本原理重新审视VP和VPT的设计，并将它们概念化为一个称为基于提示的适应(Prompt-based Adaptation, PA)的统一框架。我们提供了一个分类法，将现有方法分为可学习提示、生成提示和非可学习提示，并按注入粒度（像素级和令牌级）进一步组织。除了核心方法外，我们检查了PA在医学成像、3D点云和视觉语言任务等不同领域的整合，以及其在测试时适应和可信AI中的作用。我们还总结了当前基准，并确定了关键挑战和未来方向。据我们所知，我们是第一个专门针对PA的方法和应用的全面调查，考虑其独特特征。我们的调查旨在为所有领域的研究人员和实践者提供清晰的路线图，以理解和探索PA相关研究的不断发展的格局。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) haverecently emerged as lightweight and effective alternatives to full fine-tuningfor adapting large-scale vision models within the ``pretrain-then-finetune''paradigm. However, despite rapid progress, their conceptual boundaries remainblurred, as VP and VPT are frequently used interchangeably in current research,reflecting a lack of systematic distinction between these techniques and theirrespective applications. In this survey, we revisit the designs of VP and VPTfrom first principles, and conceptualize them within a unified framework termedPrompt-based Adaptation (PA). We provide a taxonomy that categorizes existingmethods into learnable, generative, and non-learnable prompts, and furtherorganizes them by injection granularity -- pixel-level and token-level. Beyondthe core methodologies, we examine PA's integrations across diverse domains,including medical imaging, 3D point clouds, and vision-language tasks, as wellas its role in test-time adaptation and trustworthy AI. We also summarizecurrent benchmarks and identify key challenges and future directions. To thebest of our knowledge, we are the first comprehensive survey dedicated to PA'smethodologies and applications in light of their distinct characteristics. Oursurvey aims to provide a clear roadmap for researchers and practitioners in allarea to understand and explore the evolving landscape of PA-related research.</description>
      <author>example@mail.com (Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han)</author>
      <guid isPermaLink="false">2510.13219v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>ADPerf: Investigating and Testing Performance in Autonomous Driving Systems</title>
      <link>http://arxiv.org/abs/2510.13078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, accepted by ASE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了自动驾驶系统中障碍物检测模块的性能和延迟问题，开发了一个名为ADPerf的工具用于测试和暴露检测延迟，并评估了其对系统整体可靠性的影响。&lt;h4&gt;背景&lt;/h4&gt;障碍物检测对自动驾驶系统运行至关重要，系统依赖多种传感器结合深度学习模型进行时间敏感决策。然而，障碍物检测模块的延迟及其对LiDAR点云数据变化的适应性尚未被充分了解。&lt;h4&gt;目的&lt;/h4&gt;首次全面测量和建模Apollo和Autoware两个行业级自动驾驶系统中障碍物检测模块的性能，开发ADPerf工具生成测试用例以暴露检测延迟增加，并评估其对后续模块的影响。&lt;h4&gt;方法&lt;/h4&gt;对Apollo和Autoware系统中的障碍物检测模块进行性能测量和建模，开发ADPerf工具生成真实点云数据测试用例，对3D障碍物检测模块进行压力测试，并评估这些测试对轨迹预测模块的传播影响。&lt;h4&gt;主要发现&lt;/h4&gt;障碍物检测组件（特别是3D障碍物检测）的性能测试非常必要，障碍物检测可能成为自动驾驶系统延迟增加的主要瓶颈，延迟增加的不利影响会传播到其他模块，降低系统整体可靠性。&lt;h4&gt;结论&lt;/h4&gt;需要对障碍物检测组件进行性能测试，特别是3D障碍物检测，因为它们是自动驾驶系统延迟增加的主要瓶颈，会进一步影响其他模块，降低整体系统可靠性。&lt;h4&gt;翻译&lt;/h4&gt;障碍物检测对自动驾驶系统的运行至关重要，这些系统依赖多种传感器（如摄像头和LiDAR）结合代码逻辑和深度学习模型来检测障碍物，以便进行时间敏感的决策。因此，障碍物检测延迟对自动驾驶系统的安全性和有效性至关重要。然而，障碍物检测模块的延迟及其对LiDAR点云数据各种变化的适应性尚未被充分了解。在这项工作中，我们首次对两个行业级自动驾驶系统（即Apollo和Autoware）中的障碍物检测模块性能进行了全面的测量和建模研究。从这项研究中，我们引入了ADPerf，这是一个旨在生成真实点云数据测试用例的工具，这些测试用例可以暴露检测延迟的增加。延迟降低会减少检测到障碍物的可用性，并对自动驾驶系统中后续模块的能力造成压力，即这些模块可能受到障碍物检测延迟增加的负面影响。我们将ADPerf应用于压力测试自动驾驶系统中广泛使用的3D障碍物检测模块的性能，以及此类测试对轨迹预测模块的传播影响。我们的评估强调了需要对障碍物检测组件进行性能测试，特别是3D障碍物检测，因为它们可能成为自动驾驶系统延迟增加的主要瓶颈。这种不利结果还会进一步传播到其他模块，降低自动驾驶系统的整体可靠性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中障碍物检测模块的性能问题，特别是延迟(latency)问题。这个问题在现实中非常重要，因为障碍物检测延迟直接影响自动驾驶系统的安全性和有效性；延迟过大会导致系统无法及时做出决策，就像未检测到障碍物一样危险。同时，现有研究大多关注检测器的准确性和鲁棒性，而对其性能的研究相对不足，这导致自动驾驶系统在实际部署中可能存在未被发现性能瓶颈的风险。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶系统架构，识别出感知模块(特别是3D障碍物检测)是性能瓶颈。他们通过排队网络和排队Petri网对Apollo和Autoware系统进行性能建模，确认了3D障碍物检测的延迟问题。基于这些发现，他们设计了ADPerf工具，通过三种简单方法修改点云数据来增加检测延迟：添加障碍物边界外的噪声、添加新障碍物、移动现有障碍物。作者借鉴了现有的性能测试技术和障碍物检测鲁棒性测试方法，但专注于性能而非准确性，与现有的对抗攻击方法(如SlowLidar)相比，ADPerf采用更简单、更现实的修改方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过生成能增加3D障碍物检测延迟的测试场景，评估自动驾驶系统在性能压力下的行为及其对后续模块的影响。整体实现流程包括：1)数据准备，从真实世界驾驶场景数据集中提取点云表示和障碍物历史数据；2)测试场景生成，通过添加噪声、添加障碍物或移动障碍物来修改点云；3)模型执行与延迟测量，在修改和未修改的点云上运行检测模型并测量延迟；4)帧可用性估计，基于检测延迟估计哪些帧会被丢弃；5)轨迹预测评估，分析检测延迟对轨迹预测模块的级联影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次对自动驾驶系统障碍物检测模块性能进行综合研究；2)提出ADPerf工具，专门用于生成性能测试用例；3)研究性能问题的级联影响，关注检测延迟对整个系统的影响；4)采用更现实的测试方法。相比之前的工作，本文的关注点从准确性转向性能，方法上从复杂的对抗攻击转向简单的点云修改，评估范围从单个模块扩展到整个系统，且生成的测试场景更接近真实世界，具有更高的实用价值。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ADPerf工具，通过生成增加障碍物检测延迟的测试场景，首次系统性地研究了自动驾驶系统中障碍物检测模块的性能瓶颈及其对整体系统可靠性的影响。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Obstacle detection is crucial to the operation of autonomous driving systems,which rely on multiple sensors, such as cameras and LiDARs, combined with codelogic and deep learning models to detect obstacles for time-sensitivedecisions. Consequently, obstacle detection latency is critical to the safetyand effectiveness of autonomous driving systems. However, the latency of theobstacle detection module and its resilience to various changes in the LiDARpoint cloud data are not yet fully understood. In this work, we present thefirst comprehensive investigation on measuring and modeling the performance ofthe obstacle detection modules in two industry-grade autonomous drivingsystems, i.e., Apollo and Autoware. Learning from this investigation, weintroduce ADPerf, a tool that aims to generate realistic point cloud data testcases that can expose increased detection latency. Increasing latency decreasesthe availability of the detected obstacles and stresses the capabilities ofsubsequent modules in autonomous driving systems, i.e., the modules may benegatively impacted by the increased latency in obstacle detection.  We applied ADPerf to stress-test the performance of widely used 3D obstacledetection modules in autonomous driving systems, as well as the propagation ofsuch tests on trajectory prediction modules. Our evaluation highlights the needto conduct performance testing of obstacle detection components, especially 3Dobstacle detection, as they can be a major bottleneck to increased latency ofthe autonomous driving system. Such an adverse outcome will also furtherpropagate to other modules, reducing the overall reliability of autonomousdriving systems.</description>
      <author>example@mail.com (Tri Minh-Triet Pham, Diego Elias Costa, Weiyi Shang, Jinqiu Yang)</author>
      <guid isPermaLink="false">2510.13078v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations</title>
      <link>http://arxiv.org/abs/2510.13774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanFusion是一个地理基础模型(GeoFM)，采用随机多模态融合(SMF)技术，能够有效整合多种地理空间数据，在预测城市现象方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;预测城市现象如房价和公共健康指标需要有效整合各种地理空间数据。当前方法主要使用任务特定模型，而最近的用于空间表示的基础模型通常只支持有限模态，缺乏多模态融合能力。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法的局限性，开发一个能够处理多种地理数据模态并具有强大泛化能力的地理基础模型。&lt;h4&gt;方法&lt;/h4&gt;UrbanFusion采用模态特定编码器处理街景图像、遥感数据、地图和兴趣点(POIs)数据，并通过基于Transformer的融合模块整合这些多模态输入，学习统一的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在全球56个城市41个任务的评估中，UrbanFusion表现出强大的泛化能力和预测性能：1)在位置编码方面优于之前的基础模型；2)在推理过程中允许多模态输入；3)对训练中未见过的区域泛化良好。&lt;h4&gt;结论&lt;/h4&gt;UrbanFusion可在预训练和推理过程中灵活利用任何可用模态的子集，使模型在不同数据可用性场景下具有广泛的适用性，所有源代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;预测城市现象如房价和公共健康指标需要有效整合各种地理空间数据。当前方法主要使用任务特定的模型，而最近的用于空间表示的基础模型通常只支持有限的模态，且缺乏多模态融合能力。为了克服这些挑战，我们提出了UrbanFusion，这是一个具有随机多模态融合(SMF)的地理基础模型(GeoFM)。该框架采用模态特定编码器处理不同类型的输入，包括街景图像、遥感数据、地图和兴趣点(POIs)数据。这些多模态输入通过基于Transformer的融合模块进行整合，学习统一的表示。在全球56个城市41个任务的广泛评估中，UrbanFusion与最先进的GeoAI模型相比表现出强大的泛化能力和预测性能。具体来说，它1)在位置编码方面优于之前的基础模型；2)在推理过程中允许多模态输入；3)对训练中未见过的区域泛化良好。UrbanFusion可以在预训练和推理过程中灵活利用任何可用模态的子集，使模型在不同数据可用性场景下具有广泛的适用性。所有源代码均可通过https://github.com/DominikM198/UrbanFusion获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting urban phenomena such as housing prices and public healthindicators requires the effective integration of various geospatial data.Current methods primarily utilize task-specific models, while recent foundationmodels for spatial representations often support only limited modalities andlack multimodal fusion capabilities. To overcome these challenges, we presentUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic MultimodalFusion (SMF). The framework employs modality-specific encoders to processdifferent types of inputs, including street view imagery, remote sensing data,cartographic maps, and points of interest (POIs) data. These multimodal inputsare integrated via a Transformer-based fusion module that learns unifiedrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwidedemonstrates UrbanFusion's strong generalization and predictive performancecompared to state-of-the-art GeoAI models. Specifically, it 1) outperformsprior foundation models on location-encoding, 2) allows multimodal input duringinference, and 3) generalizes well to regions unseen during training.UrbanFusion can flexibly utilize any subset of available modalities for a givenlocation during both pretraining and inference, enabling broad applicabilityacross diverse data availability scenarios. All source code is available athttps://github.com/DominikM198/UrbanFusion.</description>
      <author>example@mail.com (Dominik J. Mühlematter, Lin Che, Ye Hong, Martin Raubal, Nina Wiedemann)</author>
      <guid isPermaLink="false">2510.13774v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Vision Transformers for Functional MRI with Flat Maps</title>
      <link>http://arxiv.org/abs/2510.13768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Workshop, Foundation Models for the Brain and Body;  Code: https://github.com/MedARC-AI/fmri-fm; Discord:  https://discord.gg/tVR4TWnRM9&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了将现代深度学习架构适应功能磁共振成像(fMRI)的方法，通过将4D体积fMRI数据转换为2D平面图视频，并使用时空掩码自编码器框架训练视觉Transformer模型。&lt;h4&gt;背景&lt;/h4&gt;现代深度学习架构如何适应功能磁共振成像(fMRI)是一个关键问题，需要解决fMRI与自然图像之间的模态差距。&lt;h4&gt;目的&lt;/h4&gt;研究如何将fMRI数据表示为模型输入，构建fMRI数据的基础模型。&lt;h4&gt;方法&lt;/h4&gt;将4D体积fMRI数据转换为2D fMRI活动平面图视频，使用时空掩码自编码器框架在人类连接体项目的2.3K小时fMRI平面图视频上训练视觉Transformer，并进行掩码建模和下游分类基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;掩码fMRI建模性能随数据集大小严格遵循幂律缩放规律而提高；模型能够学习丰富的表示，支持跨受试者的精细状态解码和跨脑状态变化的受试者特异性特征解码。&lt;h4&gt;结论&lt;/h4&gt;这是构建fMRI数据基础模型的开放科学项目的一部分，代码和数据已公开共享。&lt;h4&gt;翻译&lt;/h4&gt;将现代深度学习架构适应功能磁共振成像(fMRI)的一个关键问题是如何为模型输入表示数据。为弥合fMRI与自然图像之间的模态差距，我们将4D体积fMRI数据转换为2D fMRI活动平面图视频。我们在人类连接体项目的2.3K小时fMRI平面图视频上使用时空掩码自编码器框架训练视觉Transformer。我们观察到，根据严格的幂律缩放规律，掩码fMRI建模性能随数据集大小增加而提高。下游分类基准测试表明，我们的模型学习了丰富的表示，既支持跨受试者的精细状态解码，也支持跨脑状态变化的受试者特异性特征解码。这项工作是构建fMRI数据基础模型的开放科学项目的一部分。我们的代码和数据可在https://github.com/MedARC-AI/fmri-fm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A key question for adapting modern deep learning architectures to functionalMRI (fMRI) is how to represent the data for model input. To bridge the modalitygap between fMRI and natural images, we transform the 4D volumetric fMRI datainto videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3Khours of fMRI flat map videos from the Human Connectome Project using thespatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRImodeling performance improves with dataset size according to a strict powerscaling law. Downstream classification benchmarks show that our model learnsrich representations supporting both fine-grained state decoding acrosssubjects, as well as subject-specific trait decoding across changes in brainstate. This work is part of an ongoing open science project to build foundationmodels for fMRI data. Our code and datasets are available athttps://github.com/MedARC-AI/fmri-fm.</description>
      <author>example@mail.com (Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti)</author>
      <guid isPermaLink="false">2510.13768v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</title>
      <link>http://arxiv.org/abs/2510.13721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NExT-OMNI是一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任意到任意的跨模态理解与生成，在多模态生成、理解、多轮交互和跨模态检索方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;下一代多模态基础模型将成为人工通用智能系统的核心组件，但现有多模态模型受限于自回归架构，难以平衡理解与生成能力；混合和解耦策略虽有探索，但冗余设计限制了在广泛场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个支持任何到任何跨模态生成和多轮交互的多模态基础模型，克服现有模型的局限性，实现更高效、更广泛的应用。&lt;h4&gt;方法&lt;/h4&gt;引入NExT-OMNI模型，利用度量诱导的概率路径和动力学最优速度，通过离散流范式实现统一建模，并在大规模交错文本、图像、视频和音频数据上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;NExT-OMNI在多模态生成和理解基准测试中具有竞争力，在多轮多模态交互和跨模态检索方面优于之前的统一模型，突显了其作为下一代多模态基础模型的架构优势。&lt;h4&gt;结论&lt;/h4&gt;NExT-OMNI通过简洁的统一表示而非任务解耦设计，实现了更广泛的应用场景，为促进进一步研究，已公开训练细节、数据协议，并开源了代码和模型检查点。&lt;h4&gt;翻译&lt;/h4&gt;能够进行任意到任意跨模态生成和多轮交互的下一代多模态基础模型将成为人工通用智能系统的核心组件，在人机交互中发挥关键作用。然而，大多数现有的多模态模型仍受限于自回归架构，其固有局限性阻碍了理解与生成能力的平衡整合。尽管已经探索了混合和解耦策略来在统一框架内分别解决这些问题，但这些冗余、非集成的设计限制了它们在更广泛场景（如跨模态检索）中的适用性。在本工作中，我们引入了NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。通过利用度量诱导的概率路径和动力学最优速度，NExT-OMNI原生支持任意到任意的理解和生成，同时通过简洁的统一表示而非任务解耦设计，实现更广泛的应用场景，并提高响应效率。在大型交错文本、图像、视频和音频数据上训练后，NExT-OMNI在多模态生成和理解基准测试中具有竞争力，同时在多轮多模态交互和跨模态检索方面优于之前的统一模型，突显了其作为下一代多模态基础模型的架构优势。为了促进进一步研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation multimodal foundation models capable of any-to-anycross-modal generation and multi-turn interaction will serve as core componentsof artificial general intelligence systems, playing a pivotal role inhuman-machine interaction. However, most existing multimodal models remainconstrained by autoregressive architectures, whose inherent limitations preventa balanced integration of understanding and generation capabilities. Althoughhybrid and decoupling strategies have been explored to address these taskswithin unified frameworks separately, their redundant, non-integrated designslimit their applicability to broader scenarios, such as cross-modalretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodalfoundation model that achieves unified modeling through discrete flowparadigms. By leveraging metric-induced probability paths and kinetic optimalvelocities, NExT-OMNI natively supports any-to-any understanding and generationwith enhanced response efficiency, while enabling broader application scenariosthrough concise unified representations rather than task-decoupled designs.Trained on large-scale interleaved text, image, video, and audio data,NExT-OMNI delivers competitive performance on multimodal generation andunderstanding benchmarks, while outperforming prior unified models inmulti-turn multimodal interaction and cross-modal retrieval, highlighting itsarchitectural advantages as a next-generation multimodal foundation model. Toadvance further research, we release training details, data protocols, andopen-source both the code and model checkpoints.</description>
      <author>example@mail.com (Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2510.13721v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Axial Neural Networks for Dimension-Free Foundation Models</title>
      <link>http://arxiv.org/abs/2510.13665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轴向神经网络(XNN)架构，解决了在物理数据上训练基础模型时面临的维度变化挑战，使模型能够有效处理不同维度的偏微分方程数据，同时保持计算效率和性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在AI中的出现显著推进了通用学习，在零样本推理和上下文学习方面表现出色。然而，在物理数据（包括偏微分方程PDEs的解）上训练此类模型面临独特挑战，因为不同系统的维度各不相同。&lt;h4&gt;目的&lt;/h4&gt;提出一种维度不可知的神经网络架构，解决传统方法在处理不同维度数据时效率低下的问题。&lt;h4&gt;方法&lt;/h4&gt;提出轴向神经网络(XNN)，受Deep Sets和图神经网络等参数共享结构的启发。将现有的PDE基础模型转换为轴向神经网络，并在三种训练场景下评估性能：从头开始训练、在多个PDE上预训练以及在单个PDE上微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，XNNs与原始模型表现相当，并且对未见维度表现出更好的泛化能力，突显了多维预训练对基础模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;XNN架构解决了在物理数据上训练基础模型的维度挑战，同时保持了性能和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在AI中的出现显著推进了通用学习，使零样本推理和上下文学习能力显著提升。然而，在包括偏微分方程(PDEs)解在内的物理数据上训练此类模型，由于不同系统间维度的变化，带来了独特挑战。传统方法要么固定最大维度，要么为不同维度使用单独的编码器，导致效率低下。为此，我们提出了一种维度不可知的神经网络架构——轴向神经网络(XNN)，其灵感来自Deep Sets和图神经网络等参数共享结构。XNN能够在保持计算效率的同时，推广到变化的张量维度。我们将现有的PDE基础模型转换为轴向神经网络，并在三种训练场景下评估其性能：从头开始训练、在多个PDE上预训练以及在单个PDE上微调。实验表明，XNNs与原始模型表现相当，并且对未见维度表现出更好的泛化能力，突显了多维预训练对基础模型的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of foundation models in AI has significantly advancedgeneral-purpose learning, enabling remarkable capabilities in zero-shotinference and in-context learning. However, training such models on physicsdata, including solutions to partial differential equations (PDEs), poses aunique challenge due to varying dimensionalities across different systems.Traditional approaches either fix a maximum dimension or employ separateencoders for different dimensionalities, resulting in inefficiencies. Toaddress this, we propose a dimension-agnostic neural network architecture, theAxial Neural Network (XNN), inspired by parameter-sharing structures such asDeep Sets and Graph Neural Networks. XNN generalizes across varying tensordimensions while maintaining computational efficiency. We convert existing PDEfoundation models into axial neural networks and evaluate their performanceacross three training scenarios: training from scratch, pretraining on multiplePDEs, and fine-tuning on a single PDE. Our experiments show that XNNs performcompetitively with original models and exhibit superior generalization tounseen dimensions, highlighting the importance of multidimensional pretrainingfor foundation models.</description>
      <author>example@mail.com (Hyunsu Kim, Jonggeon Park, Joan Bruna, Hongseok Yang, Juho Lee)</author>
      <guid isPermaLink="false">2510.13665v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Time Series Foundation Models: Benchmarking Challenges and Requirements</title>
      <link>http://arxiv.org/abs/2510.13654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)是一种新的时间序列预测范式，具有零样本预测能力，但其评估面临多个挑战，包括数据集代表性问题、缺乏时空评估、信息泄露风险和全局模式记忆问题。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)代表了一种新的时间序列预测范式，提供无需领域特定预训练或微调的零样本预测能力。与大型语言模型(LLMs)类似，随着训练集的不断扩大，确保基准测试数据的完整性变得越来越困难。&lt;h4&gt;目的&lt;/h4&gt;调查现有TSFM评估的挑战，并提出改进评估方法的建议，以保障TSFM评估的完整性。&lt;h4&gt;方法&lt;/h4&gt;通过调查现有TSFM评估实践，分析数据分区问题，并提出新的评估方法建议，如在真正外来的未来数据上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;现有TSFM评估存在多个挑战，包括基准数据集的代表性问题、缺乏时空评估、信息泄露风险和全局模式记忆问题。此外，关于数据分区的普遍混乱可能导致性能估计膨胀和全球知识错误地转移到局部时间序列。&lt;h4&gt;结论&lt;/h4&gt;需要开发强大的评估方法来防止在LLM和经典时间序列基准测试中已经观察到的陷阱，并呼吁研究社区设计新的、有原则的评估方法，如在真正外来的未来数据上进行评估，以保障TSFM评估的完整性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)代表了一种新的时间序列预测范式，提供无需领域特定预训练或微调的零样本预测能力。然而，与大型语言模型(LLMs)一样，评估TSFMs很棘手，因为随着训练集的不断扩展，确保基准测试数据的完整性变得越来越具有挑战性。我们对现有TSFM评估的调查揭示了多个挑战，从基准数据集的代表性、缺乏时空评估，到由于数据集重叠和不透明导致的信息泄露风险，以及由经济危机或疫情等外部冲击引起的全局模式记忆问题。我们的发现揭示了关于数据分区的普遍混乱，这可能导致性能估计膨胀和全球知识错误地转移到局部时间序列。我们呼吁开发强大的评估方法，以防止在LLM和经典时间序列基准测试中已经观察到的陷阱，并呼吁研究社区设计新的、有原则的方法，如在真正外来的未来数据上进行评估，以保障TSFM评估的完整性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) represent a new paradigm for timeseries forecasting, offering zero-shot forecasting capabilities without theneed for domain-specific pre-training or fine-tuning. However, as with LargeLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensivetraining sets, it becomes more and more challenging to ensure the integrity ofbenchmarking data. Our investigation of existing TSFM evaluation highlightsmultiple challenges, ranging from the representativeness of the benchmarkdatasets, over the lack of spatiotemporal evaluation, to risks of informationleakage due to overlapping and obscure datasets, and the memorization of globalpatterns caused by external shocks like economic crises or pandemics. Ourfindings reveal widespread confusion regarding data partitions, riskinginflated performance estimates and incorrect transfer of global knowledge tolocal time series. We argue for the development of robust evaluationmethodologies to prevent pitfalls already observed in LLM and classical timeseries benchmarking, and call upon the research community to design new,principled approaches, such as evaluations on truly out-of-sample future data,to safeguard the integrity of TSFM assessment.</description>
      <author>example@mail.com (Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Oliver Müller)</author>
      <guid isPermaLink="false">2510.13654v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.13643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了基于DINOv2等基础模型的小样本异常检测器的对抗性扰动敏感性和不确定性校准问题。作者通过在冻结的DINOv2特征上附加轻量级线性头创建对抗性攻击，评估了FGSM攻击的影响，并发现微小扰动可显著降低检测性能。同时，原始异常分数校准性较差，通过应用Platt缩放方法，作者提出了实用的攻击检测机制并降低了校准误差。&lt;h4&gt;背景&lt;/h4&gt;基础模型如DINOv2在小样本异常检测中表现出强大性能，但两个关键问题尚未得到研究：(1)这些检测器对抗性扰动的敏感性如何；(2)它们的异常分数在多大程度上反映了校准的不确定性。&lt;h4&gt;目的&lt;/h4&gt;研究DINOv2等基础模型在小样本异常检测中的对抗性鲁棒性和不确定性校准问题，并提出实用的攻击检测机制，以提高异常检测系统的可信度和安全性。&lt;h4&gt;方法&lt;/h4&gt;基于AnomalyDINO（一种在DINOv2特征上的训练深度最近邻检测器），作者在冻结的DINOv2特征上附加轻量级线性头仅用于创建对抗性扰动。评估FGSM攻击在MVTec-AD和VisA数据集上的影响，并应用后验Platt缩放方法对异常分数进行不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;1) 微小对抗性扰动显著降低检测性能(F1、AUROC、AP和G-mean指标均下降)；2) 扰动可在特征空间翻转最近邻关系，导致有把握的错误分类；3) 原始异常分数校准性较差，置信度与正确性存在差距；4) Platt缩放得到的校验后验分布在对抗性扰动输入上产生更高预测熵；5) 该方法可用于实用攻击检测机制，同时降低校准误差(ECE)。&lt;h4&gt;结论&lt;/h4&gt;DINOv2基础的小样本异常检测器存在具体脆弱性，对抗性鲁棒性和有原则的不确定性量化不是可选的附加功能，而是异常检测系统可信度和为真实世界部署做好准备所必需的基本能力。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如DINOv2在小样本异常检测中表现出强大的性能，但两个关键问题尚未得到研究：(i)这些检测器对抗性扰动的敏感性如何；(ii)它们的异常分数在多大程度上反映了校准的不确定性。基于AnomalyDINO（一种在DINOv2特征上的训练深度最近邻检测器），我们进行了此设置中对抗性攻击和不确定性估计的首次系统性研究之一。为了在保持测试时间行为的同时实现白盒梯度攻击，我们仅在创建扰动时为冻结的DINOv2特征附加了一个轻量级线性头。使用这种启发式方法，我们评估了FGSM在MVTec-AD和VisA数据集上的影响，并观察到F1、AUROC、AP和G-mean指标的一致下降，表明微小的扰动可以在特征空间中翻转最近邻关系，导致有把握的错误分类。除了鲁棒性外，我们还探测了可靠性，发现原始异常分数的校准性较差，揭示了置信度与正确性之间的差距，这限制了安全关键应用。作为迈向可信度的简单、强基线，我们对异常分数应用了后验Platt缩放进行不确定性估计。所得的校验后验分布在对抗性扰动输入上产生显著更高的预测熵，能够用于实用的攻击检测机制，同时降低校准误差（ECE）。我们的研究结果揭示了DINOv2基础小样本异常检测器的具体脆弱性，并为鲁棒、不确定性感知的异常检测建立了评估协议和基线。我们认为，对抗性鲁棒性和有原则的不确定性量化不是可选的附加功能，而是异常检测系统可信度和为真实世界部署做好准备所必需的基本能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as DINOv2 have shown strong performance in few-shotanomaly detection, yet two key questions remain unexamined: (i) how susceptibleare these detectors to adversarial perturbations; and (ii) how well do theiranomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, atraining-free deep nearest-neighbor detector over DINOv2 features, we presentone of the first systematic studies of adversarial attacks and uncertaintyestimation in this setting. To enable white-box gradient attacks whilepreserving test-time behavior, we attach a lightweight linear head to frozenDINOv2 features only for crafting perturbations. Using this heuristic, weevaluate the impact of FGSM across the MVTec-AD and VisA datasets and observeconsistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptibleperturbations can flip nearest-neighbor relations in feature space to induceconfident misclassification. Complementing robustness, we probe reliability andfind that raw anomaly scores are poorly calibrated, revealing a gap betweenconfidence and correctness that limits safety-critical use. As a simple, strongbaseline toward trustworthiness, we apply post-hoc Platt scaling to the anomalyscores for uncertainty estimation. The resulting calibrated posteriors yieldsignificantly higher predictive entropy on adversarially perturbed inputs thanon clean ones, enabling a practical flagging mechanism for attack detectionwhile reducing calibration error (ECE). Our findings surface concretevulnerabilities in DINOv2-based few-shot anomaly detectors and establish anevaluation protocol and baseline for robust, uncertainty-aware anomalydetection. We argue that adversarial robustness and principled uncertaintyquantification are not optional add-ons but essential capabilities if anomalydetection systems are to be trustworthy and ready for real-world deployment.</description>
      <author>example@mail.com (Akib Mohammed Khan, Bartosz Krawczyk)</author>
      <guid isPermaLink="false">2510.13643v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>The Role of Computing Resources in Publishing Foundation Model Research</title>
      <link>http://arxiv.org/abs/2510.13621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了计算资源与基础模型科学进展之间的关系，发现增加计算资源与国家资金分配和引用量相关，但与研究环境、领域或研究方法无强相关性。&lt;h4&gt;背景&lt;/h4&gt;前沿的人工智能研究需要大量资源，包括图形处理器(GPU)、数据和人力资源。&lt;h4&gt;目的&lt;/h4&gt;评估这些资源与基础模型科学进展之间的关系。&lt;h4&gt;方法&lt;/h4&gt;回顾了2022年至2024年间发表的6517篇基础模型论文，并调查了229位第一作者关于计算资源对科研产出影响的情况。&lt;h4&gt;主要发现&lt;/h4&gt;增加的计算资源与国家资金分配和引用量相关，但未发现与研究环境(学术界或工业界)、领域或研究方法有强相关性。&lt;h4&gt;结论&lt;/h4&gt;建议个人和机构专注于创建共享且负担得起的计算机会，以减少资源不足研究者的入门障碍，这些步骤可以帮助扩大基础模型研究的参与度，促进思想贡献者的多样性，并维持人工智能的创新和进步。&lt;h4&gt;翻译&lt;/h4&gt;尖端的人工智能研究需要大量资源，包括图形处理器、数据和人力资源。在本文中，我们评估了这些资源与基础模型科学进展之间的关系。我们回顾了2022年至2024年间发表的6517篇基础模型论文，并对229位第一作者进行了调查，了解计算资源对科研产出的影响。我们发现，增加的计算资源与国家资金分配和引用量相关，但我们的研究结果未观察到与研究环境(学术界或工业界)、领域或研究方法有强相关性。我们建议个人和机构专注于创建共享且负担得起的计算机会，以降低资源不足研究者的入门门槛。这些步骤可以帮助扩大基础模型研究的参与度，促进思想贡献者的多样性，并维持人工智能的创新和进步。数据将在https://mit-calc.csail.mit.edu/提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cutting-edge research in Artificial Intelligence (AI) requires considerableresources, including Graphics Processing Units (GPUs), data, and humanresources. In this paper, we evaluate of the relationship between theseresources and the scientific advancement of foundation models (FM). We reviewed6517 FM papers published between 2022 to 2024, and surveyed 229 first-authorsto the impact of computing resources on scientific output. We find thatincreased computing is correlated with national funding allocations andcitations, but our findings don't observe the strong correlations with researchenvironment (academic or industrial), domain, or study methodology. We advisethat individuals and institutions focus on creating shared and affordablecomputing opportunities to lower the entry barrier for under-resourcedresearchers. These steps can help expand participation in FM research, fosterdiversity of ideas and contributors, and sustain innovation and progress in AI.The data will be available at: https://mit-calc.csail.mit.edu/</description>
      <author>example@mail.com (Yuexing Hao, Yue Huang, Haoran Zhang, Chenyang Zhao, Zhenwen Liang, Paul Pu Liang, Yue Zhao, Lichao Sun, Saleh Kalantari, Xiangliang Zhang, Marzyeh Ghassemi)</author>
      <guid isPermaLink="false">2510.13621v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment</title>
      <link>http://arxiv.org/abs/2510.13390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ICPADS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLSDA的新型泛化框架，利用预训练大型基础模型的语义先验来增强WiFi手势识别的泛化能力和语义表达能力，通过双路径CSI编码、多尺度语义编码、语义感知软监督和鲁棒双蒸馏策略，实现了在域内和跨域场景中的高性能手势识别。&lt;h4&gt;背景&lt;/h4&gt;WiFi手势识别作为一种有前途的RF传感范式，可在AIoT环境中实现非接触式和隐私保护的人机交互。然而，现有方法因信道状态信息的域敏感特性和高级手势抽象的缺乏，面临泛化能力和语义表达能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有WiFi手势识别方法泛化能力有限和语义表达能力不足的问题，提出一种能够增强域内和跨域场景中手势表示学习的新型框架。&lt;h4&gt;方法&lt;/h4&gt;1) 设计双路径CSI编码管道，通过CSI-Ratio相位序列和多普勒频谱图捕获手势模式；2) 开发多尺度语义编码器，学习时序嵌入并通过跨模态注意力机制与手势语义对齐；3) 引入语义感知软监督方案，编码类间相关性并减少标签模糊性；4) 开发鲁棒双蒸馏策略，将对齐模型压缩为轻量级网络。&lt;h4&gt;主要发现&lt;/h4&gt;在Widar3.0基准上的实验表明，GLSDA在域内和跨域手势识别任务中均优于现有最先进方法，同时显著减小了模型大小和推理延迟。&lt;h4&gt;结论&lt;/h4&gt;GLSDA为现实世界AIoT应用中的通用RF手势界面提供了可扩展和可部署的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于WiFi的手势识别已成为一种有前途的RF传感范式，能够在AIoT环境中实现非接触式和隐私保护的人机交互。然而，由于信道状态信息的域敏感特性和高级手势抽象的缺乏，现有方法通常面临泛化能力和语义表达能力有限的问题。为解决这些挑战，我们提出了一种名为Large-Model-Aware Semantic Distillation and Alignment (GLSDA)的新型泛化框架，它利用预训练大型基础模型的语义先验来增强域内和跨域场景中的手势表示学习。具体而言，我们首先设计了一个双路径CSI编码管道，通过CSI-Ratio相位序列和多普勒频谱图捕获几何和动态手势模式。然后将这些表示输入多尺度语义编码器，学习鲁棒的时序嵌入，并通过跨模态注意力机制将其与手势语义对齐。为进一步增强类别区分度，我们引入了一种语义感知软监督方案，编码类间相关性并减少标签模糊性，特别是对于语义相似的手势。最后，我们开发了一种鲁棒双蒸馏策略，将对齐的模型压缩为轻量级学生网络，从教师模型联合蒸馏中间特征和语义感知软标签。在Widar3.0基准上的大量实验表明，GLSDA在域内和跨域手势识别任务中始终优于最先进的方法，同时显著减小了模型大小和推理延迟。我们的方法为现实世界AIoT应用中的通用RF手势界面提供了可扩展和可部署的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; WiFi-based gesture recognition has emerged as a promising RF sensing paradigmfor enabling non-contact and privacy-preserving human-computer interaction inAIoT environments. However, existing methods often suffer from limitedgeneralization and semantic expressiveness due to the domain-sensitive natureof Channel State Information and the lack of high-level gesture abstraction. Toaddress these challenges, we propose a novel generalization framework, termedLarge-Model-Aware Semantic Distillation and Alignment (GLSDA), which leveragesthe semantic prior of pre-trained large foundation models to enhance gesturerepresentation learning in both in-domain and cross-domain scenarios.Specifically, we first design a dual-path CSI encoding pipeline that capturesgeometric and dynamic gesture patterns via CSI-Ratio phase sequences andDoppler spectrograms. These representations are then fed into a MultiscaleSemantic Encoder, which learns robust temporal embeddings and aligns them withgesture semantics through cross-modal attention mechanisms. To further enhancecategory discrimination, we introduce a Semantic-Aware Soft Supervision schemethat encodes inter-class correlations and reduces label ambiguity, especiallyfor semantically similar gestures. Finally, we develop a RobustDual-Distillation strategy to compress the aligned model into a lightweightstudent network, jointly distilling intermediate features and semantic-informedsoft labels from the teacher model. Extensive experiments on the Widar3.0benchmark show that GLSDA consistently outperforms state-of-the-art methods inboth in-domain and cross-domain gesture recognition tasks, while significantlyreducing model size and inference latency. Our method offers a scalable anddeployable solution for generalized RF-based gesture interfaces in real-worldAIoT applications.</description>
      <author>example@mail.com (Feng-Qi Cui, Yu-Tong Guo, Tianyue Zheng, Jinyang Huang)</author>
      <guid isPermaLink="false">2510.13390v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Document Intelligence in the Era of Large Language Models: A Survey</title>
      <link>http://arxiv.org/abs/2510.13366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了Document AI (DAI)领域在大语言模型(LLMs)影响下的发展，探讨了多模态、多语言和检索增强DAI的进展与挑战，并提出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;Document AI已成为重要应用领域，大语言模型的出现显著改变了这一领域，从早期的编码器-解码器架构发展为仅使用解码器的LLMs。&lt;h4&gt;目的&lt;/h4&gt;提供DAI演变的全面概述，突出LLMs在该领域的当前研究和未来前景，为DAI的最先进技术提供结构化分析。&lt;h4&gt;方法&lt;/h4&gt;通过综述形式，探索多模态、多语言和检索增强DAI的关键进展和挑战，并提出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;解码器-only LLMs彻底改变了DAI，带来了理解和生成方面的显著进步；多模态、多语言和检索增强DAI面临关键进展和挑战；基于代理的方法和文档特定基础模型是有前景的未来研究方向。&lt;h4&gt;结论&lt;/h4&gt;DAI在大语言模型的影响下正在快速发展，为学术和实践应用提供了新的可能性和挑战。&lt;h4&gt;翻译&lt;/h4&gt;文档人工智能(DAI)已成为一个重要的应用领域，并因大型语言模型(LLMs)的出现而发生了显著变化。虽然早期方法依赖于编码器-解码器架构，但仅使用解码器的LLMs彻底改变了DAI，在理解和生成方面带来了显著进步。本综述提供了DAI演变的全面概述，突出了LLMs在该领域的当前研究和未来前景。我们探索了多模态、多语言和检索增强DAI的关键进展和挑战，同时提出了未来研究方向，包括基于代理的方法和文档特定基础模型。本文旨在为DAI的最先进技术提供结构化分析，及其对学术和实践应用的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Document AI (DAI) has emerged as a vital application area, and issignificantly transformed by the advent of large language models (LLMs). Whileearlier approaches relied on encoder-decoder architectures, decoder-only LLMshave revolutionized DAI, bringing remarkable advancements in understanding andgeneration. This survey provides a comprehensive overview of DAI's evolution,highlighting current research attempts and future prospects of LLMs in thisfield. We explore key advancements and challenges in multimodal, multilingual,and retrieval-augmented DAI, while also suggesting future research directions,including agent-based approaches and document-specific foundation models. Thispaper aims to provide a structured analysis of the state-of-the-art in DAI andits implications for both academic and practical applications.</description>
      <author>example@mail.com (Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, Daniel Dahlmeier)</author>
      <guid isPermaLink="false">2510.13366v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Generative model for information metamaterial design</title>
      <link>http://arxiv.org/abs/2510.13264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为InfoMetaGen的通用生成模型，用于信息超材料设计，结合预训练基础模型和轻量级功能适配器，能够智能生成从元原子到任意空间编码模式的人工结构，相比传统方法具有更高的效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;生成式模型如AlphaFold和MatterGen可直接生成具有理想特性的新型材料结构，AlphaFold专注于蛋白质预测，MatterGen专注于预测周期性晶体结构，而超材料的通用设计更为复杂，需要设计元原子及其在空间中的任意非均匀分布。&lt;h4&gt;目的&lt;/h4&gt;提出一个通用的生成式模型InfoMetaGen用于信息超材料设计，解决超材料设计中的复杂性问题，实现从元原子到任意空间编码模式的智能生成。&lt;h4&gt;方法&lt;/h4&gt;InfoMetaGen结合预训练基础模型和轻量级功能适配器，通过微调轻量级适配器使单一通用生成模型能够切换不同功能，避免了传统方法需要为特定功能训练专用模型的局限。&lt;h4&gt;主要发现&lt;/h4&gt;InfoMetaGen能够加速新型超材料的多样化发现，在超材料性能方面取得突破，填补了设计人工材料时通用生成框架的空白。&lt;h4&gt;结论&lt;/h4&gt;该工作将生成模型的能力从微观自然材料的被动发现扩展到宏观人工材料的主动创造，为生成模型在材料设计领域开辟了前所未有的机会。&lt;h4&gt;翻译&lt;/h4&gt;生成模型如AlphaFold和MatterGen可以直接生成具有理想特性的新型材料结构，加速新材料发现并将材料设计范式从传统的试错方法转变为智能按需生成。AlphaFold专注于具有特定非周期结构的蛋白质预测；而MatterGen专注于预测周期性和稳定的晶体结构。超材料的通用设计要复杂得多，因为它涉及设计元原子（类似于周期结构）及其在空间中的任意非均匀分布。在此，我们提出了InfoMetaGen，一种用于信息超材料设计的通用生成模型，它结合了预训练基础模型和轻量级功能适配器，智能生成从元原子到任意空间编码模式的人工结构。与需要为特定功能训练专用模型的传统智能超材料设计方法相比，InfoMetaGen使单个通用生成模型能够通过微调轻量级适配器切换不同功能，显著提高了效率和泛化能力。实验结果表明，InfoMetaGen不仅可以加速新型超材料的多样化发现，还能在超材料性能方面取得突破。这项工作填补了设计人工材料时通用生成框架的空白，并为将生成模型的能力从微观自然材料的被动发现扩展到宏观人工材料的主动创造开辟了前所未有的机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决信息超材料的智能设计问题。传统超材料设计依赖于试错法，周期长、效率低，且现有智能方法多局限于单一功能。超材料能实现自然材料中不存在的奇特物理特性，在无线通信、传感和超分辨率成像等领域有广泛应用，因此高效设计方法对推动这些领域发展至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了AlphaFold和MatterGen等生成模型的成功经验，认识到超材料设计比蛋白质或晶体设计更复杂，因为它涉及设计超原子和它们在空间中的任意非均匀分布。他们采用两阶段训练策略：首先预训练无条件扩散模型捕获不同设计空间中的功能编码模式，然后引入条件适配器模块微调模型引导生成过程。这种方法冻结基础模型参数，只微调轻量级适配器，使单个模型能处理多种功能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个预训练的基础模型结合轻量级功能适配器，智能生成从单个超原子到整个超阵列的结构配置，实现多功能的统一生成。流程包括：1)预训练阶段训练无条件扩散模型捕获不同功能的设计模式；2)微调阶段引入条件适配器，冻结基础模型参数只微调适配器；3)生成阶段根据特定功能条件将随机噪声转换为功能数字比特流；4)应用阶段实现超原子设计、波束成形、电磁聚焦和全息成像等多种功能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出InfoMetaGen通用生成框架填补设计空白；2)实现多尺度设计能力从超原子到超阵列；3)通过轻量级适配器实现高效多任务处理；4)创新比特表示方法解决离散编码与连续扩散模型兼容性；5)强大生成能力产生新颖超原子和1/3位非均匀超阵列。相比之前工作，它专注于宏观人工材料而非微观自然材料，使用单一通用模型替代专用模型，实现多功能的统一生成，并能突破训练数据集限制生成高性能宽带超原子。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InfoMetaGen开创了信息超材料设计的通用生成范式，通过结合预训练基础模型与轻量级功能适配器，实现了从超原子到超阵列的多尺度、多功能智能生成，显著加速了新超材料的发现并突破了超材料性能的极限。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models such as AlphaFold and MatterGen can directly generate novelmaterial structures with desired properties, accelerating the new materialsdiscovery and revolutionizing the material design paradigm from traditionaltrial-and-error approach to intelligent on-demand generation. AlphaFold isfocused on protein prediction with specific aperiodic structures; whileMatterGen is focused on predicting periodic and stable crystal structures. Theuniversal design of metamaterials is much more complicated, since it involvesto design meta-atoms (similar to the periodic structures) and their arbitrarilyinhomogeneous distributions in space. Here, we propose InfoMetaGen, a universalgenerative model for information metamaterial design, which combines apre-trained foundation model with lightweight functional adapters tointelligently generate artificial structures on-demand spanning from meta-atomsto arbitrary space coding patterns. In contrast to conventional intelligentmetamaterial design methods that require training dedicated models for specificfunctionalities, InfoMetaGen enables a single universal generative modelcapable of switching across diverse functionalities by fine-tuning thelightweight adapters, significantly improving both efficiency andgeneralizability. Experimental results demonstrate that InfoMetaGen can notonly accelerate the diverse discovery of new metamaterials, but also achievebreakthroughs in metamaterial performance. This work fills the gap of universalgenerative framework in designing artificial materials, and opens upunprecedented opportunities to expand the capability of generative models fromthe passive discovery of microscopic natural material to the active creation ofmacroscopic artificial materials.</description>
      <author>example@mail.com (Jun Ming Hou, Long Chen, Xuan Zheng, Jia Wei Wu, Jian Wei You, Zi Xuan Cai, Jiahan Huang, Chen Xu Wu, Jian Lin Su, Lianlin Li, Jia Nan Zhang, Tie Jun Cui)</author>
      <guid isPermaLink="false">2510.13264v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models</title>
      <link>http://arxiv.org/abs/2510.13068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NeuroRVQ的新型脑电图基础模型，通过改进的信号令牌化方法解决了现有模型在高频动态处理和信号重建方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)捕获了多个时间和频谱尺度的神经活动，产生的信号丰富但复杂，难以进行表示学习。现有的EEG基础模型在信号令牌化方面存在不足，无法保持高频动态，限制了信号重建的保真度。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获完整频率神经频谱、支持高分辨率编码并实现高效训练的EEG信号令牌化器，以提高EEG信号的表示能力和重建质量。&lt;h4&gt;方法&lt;/h4&gt;NeuroRVQ令牌化器包含三个关键组件：多尺度特征提取模块，捕获完整频率神经频谱；分层残差矢量量化(RVQ)码本，用于高分辨率编码；以及EEG信号相位和幅度感知损失函数，用于高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;NeuroRVQ设计支持所有频段的准确重建，同时实现高效的EEG压缩，实现了强大的生成掩码建模。实证结果表明，NeuroRVQ实现了更低的重建误差，并在各种下游任务上优于现有的大脑波模型。&lt;h4&gt;结论&lt;/h4&gt;NeuroRVQ令牌化器为基于码本的通用脑波模型建立了强有力的先验，有望推动神经解码、生成建模和多模态生物信号集成等领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)捕获了多个时间和频谱尺度的神经活动，产生的信号丰富但复杂，难以进行表示学习。最近，经过训练以预测掩码信号令牌的EEG基础模型在学习可泛化表示方面显示出前景。然而，它们的性能受到信号令牌化模块的限制。现有的神经令牌化器无法保持高频动态，限制了它们以高保真度重建EEG信号的能力。我们引入了NeuroRVQ，这是一个基于码本令牌化器的可扩展大脑波模型(LBM)。我们的令牌化器整合了：(i)捕获完整频率神经频谱的多尺度特征提取模块；(ii)用于高分辨率编码的分层残差矢量量化(RVQ)码本；以及(iii)用于高效训练的EEG信号相位和幅度感知损失函数。这种设计支持所有频段的准确重建，同时实现高效的EEG压缩，从而实现强大的生成掩码建模。我们的实证结果表明，NeuroRVQ实现了更低的重建误差，并在各种下游任务上优于现有的LBM。更广泛地说，NeuroRVQ令牌化器为基于码本的通用脑波模型建立了强有力的先验，推动了神经解码、生成建模和多模态生物信号集成的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) captures neural activity across multipletemporal and spectral scales, yielding signals that are rich but complex forrepresentation learning. Recently, EEG foundation models trained to predictmasked signal-tokens have shown promise for learning generalizablerepresentations. However, their performance is hindered by their signaltokenization modules. Existing neural tokenizers fail to preservehigh-frequency dynamics, limiting their ability to reconstruct EEG signals withhigh fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)centered on a codebook-based tokenizer. Our tokenizer integrates: (i)multi-scale feature extraction modules that capture the full frequency neuralspectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks forhigh-resolution encoding; and, (iii) an EEG signal phase- and amplitude-awareloss function for efficient training. This design enables efficient EEGcompression while supporting accurate reconstruction across all frequencybands, leading to robust generative masked modeling. Our empirical resultsdemonstrate that NeuroRVQ achieves lower reconstruction error and outperformsexisting LBMs on a variety of downstream tasks. More broadly, NeuroRVQtokenizer establishes a strong prior for codebook-based general-purposebrainwave models, enabling advances in neural decoding, generative modeling andmultimodal biosignal integration.</description>
      <author>example@mail.com (Konstantinos Barmpas, Na Lee, Alexandros Koliousis, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou)</author>
      <guid isPermaLink="false">2510.13068v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation</title>
      <link>http://arxiv.org/abs/2510.12953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了FetalMind，一个专为胎儿超声检查设计的医学AI系统，用于报告生成和诊断。通过引入显著认识解耦（SED）方法和构建大规模数据集FetalSigma-1M，FetalMind在所有妊娠阶段都优于现有基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;近期的医学视觉-语言模型在VQA、报告生成和异常检测等任务上表现出色，但大多数模型适应于结构化的成人影像，在胎儿超声检查方面表现不佳。胎儿超声面临多视图图像推理、疾病多样性和图像多样性的挑战。&lt;h4&gt;目的&lt;/h4&gt;弥合现有医学视觉-语言模型在胎儿超声领域的应用差距，开发一个专门针对胎儿超声检查的AI系统，用于报告生成和诊断。&lt;h4&gt;方法&lt;/h4&gt;在临床工作流程指导下提出显著认识解耦（SED）方法，将专家构建的二分图注入模型以解耦视图-疾病关联，并通过强化学习引导偏好选择。同时构建了FetalSigma-1M数据集，包含来自十二个医疗中心的20K份胎儿超声报告，解决了领域数据稀缺问题。&lt;h4&gt;主要发现&lt;/h4&gt;FetalMind在所有妊娠阶段都优于开源和闭源基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;FetalMind是一个有效的胎儿超声AI系统，通过SED方法和大规模数据集训练，成功解决了胎儿超声图像推理的挑战，在报告生成和诊断方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;近期的医学视觉-语言模型在VQA、报告生成和异常检测等任务上显示出潜力。然而，大多数模型适应于结构化的成人影像，在胎儿超声检查方面表现不佳，这带来了多视图图像推理、疾病多样性和图像多样性的挑战。为了弥合这一差距，我们引入了FetalMind，一个专为胎儿超声设计的医学AI系统，用于报告生成和诊断。在临床工作流程的指导下，我们提出了显著认识解耦（SED）方法，将专家构建的二分图注入模型中，解耦视图-疾病关联，并通过强化学习引导偏好选择，遵循临床忠实步骤。这种设计减轻了疾病间的变异性和视图间的异质性，减少了学习瓶颈，同时使模型的推理与产科实践保持一致。为了大规模训练FetalMind，我们整理了FetalSigma-1M数据集，这是第一个大规模胎儿超声报告语料库，包含来自十二个医疗中心的20K份报告，解决了领域数据稀缺的问题。大量实验表明，FetalMind在所有妊娠阶段都优于开源和闭源基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。项目页面：https://hexiao0275.github.io/FetalMind。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent medical vision-language models have shown promise on tasks such asVQA, report generation, and anomaly detection. However, most are adapted tostructured adult imaging and underperform in fetal ultrasound, which poseschallenges of multi-view image reasoning, numerous diseases, and imagediversity. To bridge this gap, we introduce FetalMind, a medical AI systemtailored to fetal ultrasound for both report generation and diagnosis. Guidedby clinical workflow, we propose Salient Epistemic Disentanglement (SED), whichinjects an expert-curated bipartite graph into the model to decoupleview-disease associations and to steer preference selection along clinicallyfaithful steps via reinforcement learning. This design mitigates variabilityacross diseases and heterogeneity across views, reducing learning bottleneckswhile aligning the model's inference with obstetric practice. To trainFetalMind at scale, we curate FetalSigma-1M dataset, the first large-scalefetal ultrasound report corpus, comprising 20K reports from twelve medicalcenters, addressing the scarcity of domain data. Extensive experiments showthat FetalMind outperforms open- and closed-source baselines across allgestational stages, achieving +14% average gains and +61.2% higher accuracy oncritical conditions while remaining efficient, stable, and scalable. ProjectPage: https://hexiao0275.github.io/FetalMind.</description>
      <author>example@mail.com (Xiao He, Huangxuan Zhao, Guojia Wan, Wei Zhou, Yanxing Liu, Juhua Liu, Yongchao Xu, Yong Luo, Dacheng Tao, Bo Du)</author>
      <guid isPermaLink="false">2510.12953v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>An Investigation of Memorization Risk in Healthcare Foundation Models</title>
      <link>http://arxiv.org/abs/2510.12950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一套用于评估在电子健康记录上训练的基础模型中隐私相关记忆风险的黑盒评估测试，包括在嵌入层和生成层探测记忆的方法，并发布了开源工具包促进医疗AI中的隐私评估。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大型去标识化的电子健康记录上训练有临床应用前景，但它们可能记住患者信息，引发隐私问题。&lt;h4&gt;目的&lt;/h4&gt;引入一套黑盒评估测试来评估在结构化EHR数据上训练的基础模型中的隐私相关记忆风险。&lt;h4&gt;方法&lt;/h4&gt;开发了一个框架，包括在嵌入层和生成层探测记忆的方法，旨在区分模型泛化和有害记忆，特别是在临床相关环境中。&lt;h4&gt;主要发现&lt;/h4&gt;将记忆放在可能损害患者隐私的背景下，特别是对弱势亚群体。&lt;h4&gt;结论&lt;/h4&gt;在公开可用的EHR基础模型上验证了这种方法，并发布了一个开源工具包，以促进医疗AI中可复现和协作的隐私评估。&lt;h4&gt;翻译&lt;/h4&gt;在大型去标识化电子健康记录(EHRs)上训练的基础模型在临床应用方面具有潜力。然而，它们记忆患者信息的能力引发了重要的隐私问题。在这项工作中，我们引入了一套黑盒评估测试，用于评估在结构化EHR数据上训练的基础模型中的隐私相关记忆风险。我们的框架包括在嵌入层和生成层探测记忆的方法，旨在区分临床相关环境中的模型泛化和有害记忆。我们将记忆放在可能损害患者隐私的背景下，特别是对弱势亚群体。我们在公开可用的EHR基础模型上验证了我们的方法，并发布了一个开源工具包，以促进医疗AI中可复现和协作的隐私评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on large-scale de-identified electronic healthrecords (EHRs) hold promise for clinical applications. However, their capacityto memorize patient information raises important privacy concerns. In thiswork, we introduce a suite of black-box evaluation tests to assessprivacy-related memorization risks in foundation models trained on structuredEHR data. Our framework includes methods for probing memorization at both theembedding and generative levels, and aims to distinguish between modelgeneralization and harmful memorization in clinically relevant settings. Wecontextualize memorization in terms of its potential to compromise patientprivacy, particularly for vulnerable subgroups. We validate our approach on apublicly available EHR foundation model and release an open-source toolkit tofacilitate reproducible and collaborative privacy assessments in healthcare AI.</description>
      <author>example@mail.com (Sana Tonekaboni, Lena Stempfle, Adibvafa Fallahpour, Walter Gerych, Marzyeh Ghassemi)</author>
      <guid isPermaLink="false">2510.12950v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</title>
      <link>http://arxiv.org/abs/2510.12709v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAIL-Embedding是一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决了现有多模态嵌入模型在现实应用中面临的挑战，包括模态支持有限、训练机制不稳定和工业领域差距等问题。&lt;h4&gt;背景&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示以支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展前景广阔，但现有工作在现实应用和业务场景中仍面临不可避免的挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍SAIL-Embedding，一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决现有多模态嵌入模型面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出多阶段训练方案提高表示学习效果，包括内容感知渐进训练增强模型对多样化下游任务的适应性，协作感知推荐增强训练使多模态表示适应推荐场景，以及开发随机专业化和数据集驱动的模式匹配增强模型训练的灵活性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SAIL-Embedding在不同检索任务中实现了SOTA性能。在线实验显示，Lifetime (LT)显著增加，在抖音精选场景中带来+0.5%的7天LT增益，为抖音feed排序模型带来+0.1%的AUC增益。&lt;h4&gt;结论&lt;/h4&gt;SAIL-Embedding是一个有效的全模态嵌入基础模型，通过定制化训练策略和架构设计，在各种场景中表现出色，特别是在推荐系统中。&lt;h4&gt;翻译&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示，支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展前景广阔，但现有工作在现实应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定和工业领域差距等。在本工作中，我们介绍了SAIL-Embedding，一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决这些问题。在优化过程中，我们提出多阶段训练方案以提高表示学习的多方面有效性。具体而言，内容感知渐进训练旨在增强模型对多样化下游任务的适应性，掌握丰富的跨模态能力。协作感知推荐增强训练通过从序列到项目和ID到项目嵌入中提炼知识，同时挖掘用户历史兴趣，使多模态表示适应推荐场景。同时，我们开发了随机专业化和数据集驱动的模式匹配，增强模型训练的灵活性和泛化能力。实验结果表明，SAIL-Embedding在不同检索任务中与其他方法相比实现了SOTA性能。在各种集成我们模型的现实场景的在线实验中，我们观察到作为推荐体验关键指标的Lifetime (LT)显著增加。例如，在抖音精选场景中，模型带来了+0.5%的7天LT增益。对于抖音feed排序模型，SAIL-Embedding产生的匹配特征带来了+0.1%的AUC增益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal embedding models aim to yield informative unified representationsthat empower diverse cross-modal tasks. Despite promising developments in theevolution from CLIP-based dual-tower architectures to large vision-languagemodels, prior works still face unavoidable challenges in real-worldapplications and business scenarios, such as the limited modality support,unstable training mechanisms, and industrial domain gaps. In this work, weintroduce SAIL-Embedding, an omni-modal embedding foundation model thataddresses these issues through tailored training strategies and architecturaldesign. In the optimization procedure, we propose a multi-stage training schemeto boost the multifaceted effectiveness of representation learning.Specifically, the content-aware progressive training aims to enhance themodel's adaptability to diverse downstream tasks and master enrichedcross-modal proficiency. The collaboration-aware recommendation enhancementtraining further adapts multimodal representations for recommendation scenariosby distilling knowledge from sequence-to-item and ID-to-item embeddings whilemining user historical interests. Concurrently, we develop the stochasticspecialization and dataset-driven pattern matching to strengthen model trainingflexibility and generalizability. Experimental results show that SAIL-Embeddingachieves SOTA performance compared to other methods in different retrievaltasks. In online experiments across various real-world scenarios integratedwith our model, we observe a significant increase in Lifetime (LT), which is acrucial indicator for the recommendation experience. For instance, the modeldelivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For theDouyin feed rank model, the match features produced by SAIL-Embedding yield a+0.1% AUC gain.</description>
      <author>example@mail.com (Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng)</author>
      <guid isPermaLink="false">2510.12709v2</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with {Thermal} IR {(LWIR/MWIR)} and RGB</title>
      <link>http://arxiv.org/abs/2510.13404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于合成短波红外（SWIR）的多模态融合框架，用于改善恶劣能见度条件下的场景理解，解决了传统成像模态在融合时难以提供全面场景信息的问题。&lt;h4&gt;背景&lt;/h4&gt;在恶劣能见度条件下增强场景理解对监控和自主导航系统是一个关键挑战。传统的RGB和热红外成像模态在融合时难以在大气干扰或照明不足条件下提供全面场景信息。&lt;h4&gt;目的&lt;/h4&gt;解决传统成像模态的局限性，克服SWIR成像发展和实施中因缺乏公开SWIR数据集而面临的障碍，提高融合图像质量。&lt;h4&gt;方法&lt;/h4&gt;利用先进对比度增强技术从现有长波红外（LWIR）数据合成类似SWIR的结构/对比度线索图像，提出多模态融合框架集成合成SWIR、LWIR和RGB模态，采用优化的编码器-解码器神经网络架构和softmax门控融合头。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共RGB-LWIR基准和私有真实RGB-MWIR-SWIR数据集上的实验表明，该框架提高了融合图像质量（对比度、边缘定义、结构保真度）同时保持实时性能，并优于其他三模态融合方法。&lt;h4&gt;结论&lt;/h4&gt;合成SWIR增强的多模态融合框架在监控和自主系统中有实际应用的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在恶劣能见度条件下增强场景理解对监控和自主导航系统仍然是一个关键挑战。传统的成像模态，如RGB和热红外（中波/长波），在融合时往往难以提供全面的场景信息，特别是在大气干扰或照明不足的条件下。为解决这些限制，短波红外（SWIR）成像因其能够穿透大气干扰并以更清晰的分辨率区分材料而成为一种有前景的模态。然而，基于SWIR系统的发展和广泛实施面临重大障碍，主要是由于缺乏公开可访问的SWIR数据集。为应对这一挑战，我们的研究提出了一种方法，利用先进的对比度增强技术从现有的LWIR数据中合成类似SWIR的结构/对比度线索图像（不声称光谱再现）。然后我们提出了一个多模态融合框架，集成合成SWIR、LWIR和RGB模态，采用具有模态特定编码器和softmax门控融合头的优化编码器-解码器神经网络架构。在公共RGB-LWIR基准（M3FD、TNO、CAMEL、MSRS、RoadScene）和额外的私有真实RGB-MWIR-SWIR数据集上的全面实验表明，我们的合成SWIR增强融合框架提高了融合图像质量（对比度、边缘定义、结构保真度）同时保持实时性能。我们还添加了公平的三模态基线（LP、LatLRR、GFF）和U2Fusion/SwinFusion的级联三模态变体，采用统一协议。结果突显了在监控和自主系统中实际应用的巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在不良能见度条件下增强场景理解的问题，这对监控系统和自主导航系统至关重要。传统RGB和热红外成像融合在恶劣天气（如雾、烟、低光）下难以提供全面场景信息，而短波红外（SWIR）虽能穿透大气干扰并更好区分材料，但因公开SWIR数据集稀缺限制了其应用。这一问题在自动驾驶、安防监控等领域尤为重要，因为系统需要在各种环境条件下保持可靠的环境感知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到RGB和热红外融合的局限性，注意到SWIR的优势但受限于数据集稀缺。他们借鉴了之前自己的LightFusion工作（使用灰度作为第三模态），但发现灰度无法充分体现SWIR优势。作者还参考了传统多尺度变换技术、深度学习方法（自编码器、CNN、GAN）以及红外-可见光图像融合（IVIF）领域的研究。基于这些思考，他们设计了一种使用对比度受限自适应直方图均衡化（CLAHE）从LWIR合成SWIR的方法，并开发了三模态融合框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过合成SWIR图像解决数据稀缺问题，并融合RGB、热红外和合成SWIR三种模态以提升场景理解能力。整体流程包括：1) 数据预处理（统一分辨率）；2) 使用CLAHE技术从LWIR生成合成SWIR图像；3) 使用三个模态特定编码器（RGB、MWIR和合成SWIR）独立提取特征，每个编码器采用轻量级梯度残块（Light-GRLB）；4) 通过softmax门控融合头整合多模态特征；5) 使用解码器重建高质量融合图像；6) 采用语义损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 使用CLAHE从LWIR合成SWIR图像解决数据稀缺问题；2) 设计三模态融合框架（RGB、LWIR/MWIR和合成SWIR）；3) 开发轻量级梯度残块（Light-GRLB）进行高效特征提取；4) 为每种模态使用独立编码器确保精确特征提取；5) 采用softmax门控融合头加权整合多模态特征。相比之前工作（如作者自己的LightFusion），不同之处在于使用合成SWIR替代灰度作为第三模态，采用专门设计的Light-GRLB而非标准卷积层，以及引入三重语义一致性损失函数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的多模态图像融合方法，通过合成生成短波红外图像并与RGB和热红外图像融合，显著提升了在不良能见度条件下的场景理解能力，同时保持了实时处理的效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enhancing scene understanding in adverse visibility conditions remains acritical challenge for surveillance and autonomous navigation systems.Conventional imaging modalities, such as RGB and thermal infrared (MWIR /LWIR), when fused, often struggle to deliver comprehensive scene information,particularly under conditions of atmospheric interference or inadequateillumination. To address these limitations, Short-Wave Infrared (SWIR) imaginghas emerged as a promising modality due to its ability to penetrate atmosphericdisturbances and differentiate materials with improved clarity. However, theadvancement and widespread implementation of SWIR-based systems facesignificant hurdles, primarily due to the scarcity of publicly accessible SWIRdatasets. In response to this challenge, our research introduces an approach tosynthetically generate SWIR-like structural/contrast cues (without claimingspectral reproduction) images from existing LWIR data using advanced contrastenhancement techniques. We then propose a multimodal fusion frameworkintegrating synthetic SWIR, LWIR, and RGB modalities, employing an optimizedencoder-decoder neural network architecture with modality-specific encoders anda softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIRbenchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private realRGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusionframework improves fused-image quality (contrast, edge definition, structuralfidelity) while maintaining real-time performance. We also add fair trimodalbaselines (LP, LatLRR, GFF) and cascaded trimodal variants ofU2Fusion/SwinFusion under a unified protocol. The outcomes highlightsubstantial potential for real-world applications in surveillance andautonomous systems.</description>
      <author>example@mail.com (Muhammad Ishfaq Hussain, Ma Van Linh, Zubia Naz, Unse Fatima, Yeongmin Ko, Moongu Jeon)</author>
      <guid isPermaLink="false">2510.13404v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2510.13375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DepthVLA是一种简单而有效的VLA架构，通过预训练的深度预测模块明确整合空间感知能力，采用混合变压器设计统一VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成具有增强空间推理能力的端到端模型。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action (VLA) 模型最近展示了出色的泛化和语言引导的操作能力，但在需要精确空间推理的任务上表现不佳，这是由于从Vision-Language Models (VLMs) 继承的有限空间推理能力造成的。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLA模型在需要精确空间推理的任务上表现不佳的问题，提高模型的空间理解和推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出DepthVLA架构，通过预训练的深度预测模块明确整合空间感知能力，采用混合变压器设计统一VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成端到端模型。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界和模拟环境中的广泛评估表明，DepthVLA优于最先进的方法，在现实世界任务中取得了78.5%对比65.0%的进展，在LIBERO模拟器中为94.9%对比93.6%，在Simpler模拟器中为74.8%对比58.8%。&lt;h4&gt;结论&lt;/h4&gt;DepthVLA通过明确整合空间感知能力，显著提高了VLA模型在需要精确空间推理任务上的表现，代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型最近展示了出色的泛化和语言引导的操作能力。然而，在需要精确空间推理的任务上，它们的性能会下降，这是由于从Vision-Language Models (VLMs) 继承的有限空间推理能力。现有的VLA依赖于大量的动作数据预训练来将VLMs定位在3D空间中，这降低了训练效率，并且仍然不足以进行准确的空间理解。在这项工作中，我们提出了DepthVLA，一种简单而有效的VLA架构，通过预训练的深度预测模块明确地整合了空间感知能力。DepthVLA采用混合变压器设计，统一了VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成具有增强空间推理能力的端到端模型。在现实世界和模拟环境中的广泛评估表明，DepthVLA优于最先进的方法，在现实世界任务中取得了78.5%对比65.0%的进展，在LIBERO模拟器中为94.9%对比93.6%，在Simpler模拟器中为74.8%对比58.8%。我们的代码将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决Vision-Language-Action(VLA)模型在需要精确空间推理的任务上性能下降的问题。这个问题很重要，因为机器人需要精确的空间感知能力来完成精细操作，如抓取小物体、执行精确操作或避免碰撞。现有的VLA模型依赖大量动作数据预训练来将VLMs嵌入3D空间，这降低了训练效率，且仍无法满足精确空间理解的需求，限制了机器人在实际应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有VLA模型在空间推理方面的局限性，以及现有方法（如大量动作数据预训练或生成世界模型）的不足来设计新方法。他们借鉴了π0的mixture-of-transformers(MoT)设计，并利用3D感知领域的最新进展，特别是Depth Anything V2作为深度专家的基础。作者提出通过预训练的深度预测模块显式整合空间感知能力，采用混合transformers架构统一VLM、深度专家和动作专家，并使用块状掩码保持预训练模块的学习能力，同时允许每个组件在不同数据集上分别预训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过预训练的深度预测模块显式整合空间推理能力到VLA模型中，利用混合transformers架构统一三个专家（VLM、深度专家和动作专家），在保持预训练知识的同时融合语义和空间线索以生成精确动作。整体实现流程包括：1)模型架构设计，包含VLM专家（编码图像和指令）、深度专家（处理图像推断几何信息）和动作专家（生成连续动作）；2)首先在多样化3D数据集上预训练深度专家；3)然后在具身动作数据上训练整个DepthVLA模型，使用模仿学习目标和流动匹配损失；4)在推理过程中，三个专家并行处理输入，共享注意力机制，动作专家基于融合的特征生成连续动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DepthVLA架构，首次将预训练的深度预测专家集成到VLA框架中；2)按专家预训练策略，允许每个专家在不同数据集上分别预训练；3)在所有中间层执行空间推理，提供更丰富的几何特征；4)端到端联合优化空间推理和动作生成。与之前工作的不同之处在于：相比现有VLA模型，不依赖大量动作数据预训练；相比SpatialVLA，深度专家是端到端优化的；相比生成世界模型，明确编码当前场景的3D知识且无高延迟；相比CoT推理方法，避免了自回归生成空间令牌的高延迟问题，推理时间仅增加20毫秒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DepthVLA通过集成预训练的深度专家到混合transformers框架中，显著提升了机器人在需要精确空间推理任务上的性能，同时保持了高效的训练和推理速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have recently shown impressivegeneralization and language-guided manipulation capabilities. However, theirperformance degrades on tasks requiring precise spatial reasoning due tolimited spatial reasoning inherited from Vision-Language Models (VLMs).Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3Dspace, which reduces training efficiency and is still insufficient for accuratespatial understanding. In this work, we present DepthVLA, a simple yeteffective VLA architecture that explicitly incorporates spatial awarenessthrough a pretrained depth prediction module. DepthVLA adopts amixture-of-transformers design that unifies a VLM, a depth transformer, and anaction expert with fully shared attentions, forming an end-to-end model withenhanced spatial reasoning. Extensive evaluations in both real-world andsimulated environments show that DepthVLA outperforms state-of-the-artapproaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.Our code will be made publicly available.</description>
      <author>example@mail.com (Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao)</author>
      <guid isPermaLink="false">2510.13375v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding</title>
      <link>http://arxiv.org/abs/2510.13243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 7 figures, 10 tables, data and code available&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlyAwareV2是一个新的多模态数据集，包含真实和合成的无人机图像，专为城市场景理解任务设计，解决了真实数据收集和标注的挑战。&lt;h4&gt;背景&lt;/h4&gt;城市环境中无人机应用的计算机视觉算法开发严重依赖于大规模、准确标注的数据集，但收集和标注真实世界无人机数据极其困难且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决真实数据收集和标注的局限性，提供一个包含真实和合成无人机图像的多模态数据集，用于城市场景理解任务。&lt;h4&gt;方法&lt;/h4&gt;基于SynDrone和FlyAware数据集开发FlyAwareV2，引入多模态数据(RGB、深度、语义标签)覆盖不同环境条件；通过最先进单目深度估计计算真实样本深度图；提供RGB和多模态语义分割基准；研究合成到真实域适应以评估模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;FlyAwareV2具有丰富的标注和环境多样性，为基于无人机的3D城市场景理解研究提供了宝贵资源。&lt;h4&gt;结论&lt;/h4&gt;FlyAwareV2通过其丰富的标注集和环境多样性，为基于无人机的3D城市场景理解研究提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;针对城市环境中无人机应用开发的计算机视觉算法严重依赖于具有准确标注的大规模数据集的可用性。然而，收集和标注真实世界的无人机数据极其具有挑战性和成本高昂。为了解决这一局限性，我们提出了FlyAwareV2，这是一个新颖的多模态数据集，包含专为城市场景理解任务定制的真实和合成无人机图像。基于最近引入的SynDrone和FlyAware数据集，FlyAwareV2引入了几个新的关键贡献：1)跨不同环境条件的多模态数据(RGB、深度、语义标签)，包括变化的天气和白天时间；2)通过最先进的单目深度估计计算的真实样本深度图；3)基于标准架构的RGB和多模态语义分割基准；4)关于合成到真实域适应的研究，以评估在合成数据上训练的模型的泛化能力。凭借其丰富的标注集和环境多样性，FlyAwareV2为基于无人机的3D城市场景理解研究提供了宝贵的资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of computer vision algorithms for Unmanned Aerial Vehicle(UAV) applications in urban environments heavily relies on the availability oflarge-scale datasets with accurate annotations. However, collecting andannotating real-world UAV data is extremely challenging and costly. To addressthis limitation, we present FlyAwareV2, a novel multimodal dataset encompassingboth real and synthetic UAV imagery tailored for urban scene understandingtasks. Building upon the recently introduced SynDrone and FlyAware datasets,FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,depth, semantic labels) across diverse environmental conditions includingvarying weather and daytime; 2) Depth maps for real samples computed viastate-of-the-art monocular depth estimation; 3) Benchmarks for RGB andmultimodal semantic segmentation on standard architectures; 4) Studies onsynthetic-to-real domain adaptation to assess the generalization capabilitiesof models trained on the synthetic data. With its rich set of annotations andenvironmental diversity, FlyAwareV2 provides a valuable resource for researchon UAV-based 3D urban scene understanding.</description>
      <author>example@mail.com (Francesco Barbato, Matteo Caligiuri, Pietro Zanuttigh)</author>
      <guid isPermaLink="false">2510.13243v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>True Self-Supervised Novel View Synthesis is Transferable</title>
      <link>http://arxiv.org/abs/2510.13063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了XFactor，第一个无需几何信息且能实现真正新颖视图合成的自监督模型，其关键标准是姿态表示的可转移性。&lt;h4&gt;背景&lt;/h4&gt;先前关于自监督新颖视图合成的工作分析表明，它们预测的姿态不具备可转移性——相同姿态在不同3D场景中会导致不同的相机轨迹。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现真正新颖视图合成的模型，其关键标准是姿态表示的可转移性。&lt;h4&gt;方法&lt;/h4&gt;XFactor结合了成对姿态估计和简单的输入输出增强方案，能够将相机姿态与场景内容分离并促进几何推理，使用不受约束的潜在姿态变量，无需任何3D归纳偏置或多视图几何概念。&lt;h4&gt;主要发现&lt;/h4&gt;XFactor实现了姿态表示的可转移性；引入了一种新的可转移性量化指标；大规模实验表明XFactor显著优于之前无需姿态的NVS变换器；探测实验显示潜在姿态与现实世界姿态高度相关。&lt;h4&gt;结论&lt;/h4&gt;XFactor是第一个无需几何信息且能实现真正新颖视图合成的自监督模型，通过结合成对姿态估计和输入输出增强方案，成功实现了姿态表示的可转移性。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们确定判断一个模型是否真正具备新颖视图合成(NVS)能力的关键标准是可转移性：即从一段视频序列中提取的任何姿态表示是否可用于在另一场景中重新渲染相同的相机轨迹。我们分析了之前关于自监督NVS的工作，发现它们预测的姿态不具备可转移性：相同的姿态集在不同3D场景中会导致不同的相机轨迹。在这里，我们提出了XFactor，这是第一个无需几何信息且能实现真正NVS的自监督模型。XFactor结合了成对姿态估计和简单的输入输出增强方案，能够将相机姿态与场景内容分离并促进几何推理。值得注意的是，我们证明XFactor使用不受约束的潜在姿态变量实现了可转移性，无需任何3D归纳偏置或多视图几何概念——例如将姿态显式参数化为SE(3)的元素。我们引入了一种新的量化可转移性的指标，并通过大规模实验证明XFactor显著优于之前无需姿态的NVS变换器，并通过探测实验显示潜在姿态与现实世界姿态高度相关。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自监督新视角合成(NVS)的可转移性问题，即能否从一个视频序列中提取的姿态表示用于重新渲染另一个视频序列中的相同相机轨迹。这个问题很重要，因为真正的NVS应该允许用户控制视角，相同的相机姿态应该总是渲染相同的视角。如果模型无法做到这一点，它就不是真正的NVS模型，而只是帧插值器，限制了用户在任意场景中定义想要渲染的视图的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先确定可转移性是NVS的关键标准，然后分析现有自监督方法发现它们预测的姿态不能跨场景转移。作者提出两个关键见解：1)通过从必须外推的双视图模型开始训练来防止插值；2)将可转移性明确为训练目标，使用保持相机姿态但最小化像素内容重叠的增强方案。作者借鉴了RUST的无几何方法和CroCo的单目渲染思想，但通过创新的训练目标和架构设计解决了可转移性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是XFactor模型，通过成对姿态估计和输入输出的简单增强方案相结合，解耦相机姿态和场景内容，实现无几何约束的可转移NVS。整体流程：1)训练立体-单目模型(只用一对图像)，消除插值路径；2)应用可转移性目标训练，确保一个序列的姿态能用于渲染另一序列；3)使用保持相机姿态的增强方案(如逆掩码)生成像素差异大的图像对；4)通过二次训练将立体模型扩展为多视图模型，支持更复杂的场景渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出可转移性作为真正NVS的标准并引入TPS指标；2)识别现有方法实际是插值而非推理视角；3)提出促进可转移性的训练目标和增强策略；4)提出XFactor，首个完全自监督的可转移NVS模型；5)通过大规模实验验证有效性。相比RayZer(使用SE(3)参数化但降低可转移性)和RUST(仍受插值偏差影响)，XFactor不需要任何几何先验，实现了真正的跨场景姿态控制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XFactor提出了第一个完全自监督且无几何的新视角合成模型，通过可转移性训练目标实现了真正的相机姿态控制，使相同的相机姿态能够在不同场景间产生一致的视角渲染。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we identify that the key criterion for determining whether amodel is truly capable of novel view synthesis (NVS) is transferability:Whether any pose representation extracted from one video sequence can be usedto re-render the same camera trajectory in another. We analyze prior work onself-supervised NVS and find that their predicted poses do not transfer: Thesame set of poses lead to different camera trajectories in different 3D scenes.Here, we present XFactor, the first geometry-free self-supervised model capableof true NVS. XFactor combines pair-wise pose estimation with a simpleaugmentation scheme of the inputs and outputs that jointly enablesdisentangling camera pose from scene content and facilitates geometricreasoning. Remarkably, we show that XFactor achieves transferability withunconstrained latent pose variables, without any 3D inductive biases orconcepts from multi-view geometry -- such as an explicit parameterization ofposes as elements of SE(3). We introduce a new metric to quantifytransferability, and through large-scale experiments, we demonstrate thatXFactor significantly outperforms prior pose-free NVS transformers, and showthat latent poses are highly correlated with real-world poses through probingexperiments.</description>
      <author>example@mail.com (Thomas W. Mitchel, Hyunwoo Ryu, Vincent Sitzmann)</author>
      <guid isPermaLink="false">2510.13063v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages</title>
      <link>http://arxiv.org/abs/2510.12845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一个新的多语言基准测试VLURes，用于评估视觉语言模型(VLMs)在细粒度视觉和语言理解能力方面的表现，特别是在长文本设置下。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)对推进智能体的感知能力至关重要，但目前的评估主要局限于以英语为中心的基准测试，且图像-文本对仅包含短文本。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在四种语言(英语、日语、斯瓦希里语和乌尔都语)的长文本设置下的细粒度能力，特别是物体识别、场景理解和关系理解等对智能体至关重要的任务。&lt;h4&gt;方法&lt;/h4&gt;开发了包含八个视觉语言任务和一个不相关性任务的多语言基准测试VLURes；从目标语言网页资源收集包含十个不同图像类别和丰富文本上下文的数据集；通过提示VLMs生成回答和理由，并由自动系统和母语人士进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了VLMs在不同语言和任务上的表现差异；表现最好的模型GPT-4o总体准确率为90.8%，比人类表现低6.7%；开源模型与人类表现之间的差距更大。&lt;h4&gt;结论&lt;/h4&gt;VLURes基准测试在开发能够处理多模态视觉推理的智能体方面发挥着关键作用，特别是在低资源语言环境中的应用。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)对推进智能体的感知能力至关重要。然而，对VLMs的评估仍然主要局限于以英语为中心的基准测试，其中图像-文本对仅包含短文本。为了评估VLMs在四种语言下的细粒度能力，特别是在长文本设置下，我们引入了一个新的多语言基准测试VLURes，包含八个视觉语言任务和一个开创性的不相关性任务，用于探测VLMs在英语、日语以及低资源语言斯瓦希里语和乌尔都语中的细粒度视觉和语言理解能力。我们的数据集从目标语言的网页资源中精心策划，包含十个不同的图像类别和丰富的文本上下文，为斯瓦希里语和乌尔都语引入了宝贵的视觉语言资源。通过提示VLMs生成回答和理由，并由自动系统和母语人士评估，我们揭示了VLMs在不同语言和任务上的表现差异，这些任务对智能体至关重要，如物体识别、场景理解和关系理解。我们使用VLURes评估了十个VLMs。表现最好的模型GPT-4o总体准确率达到90.8%，比人类表现低6.7%，尽管开源模型之间的差距更大。这一差距凸显了VLURes在开发能够处理多模态视觉推理的智能体方面的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) are pivotal for advancing perception inintelligent agents. Yet, evaluation of VLMs remains limited to predominantlyEnglish-centric benchmarks in which the image-text pairs comprise short texts.To evaluate VLM fine-grained abilities, in four languages under long-textsettings, we introduce a novel multilingual benchmark VLURes featuring eightvision-and-language tasks, and a pioneering unrelatedness task, to probe thefine-grained Visual and Linguistic Understanding capabilities of VLMs acrossEnglish, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,curated from web resources in the target language, encompass ten diverse imagecategories and rich textual context, introducing valuable vision-languageresources for Swahili and Urdu. By prompting VLMs to generate responses andrationales, evaluated automatically and by native speakers, we uncoverperformance disparities across languages and tasks critical to intelligentagents, such as object recognition, scene understanding, and relationshipunderstanding. We conducted evaluations of ten VLMs with VLURes. The bestperforming model, GPT-4o, achieves an overall accuracy of 90.8% and lags humanperformance by 6.7%, though the gap is larger for open-source models. The gaphighlights VLURes' critical role in developing intelligent agents to tacklemulti-modal visual reasoning.</description>
      <author>example@mail.com (Jesse Atuhurra, Iqra Ali, Tomoya Iwakura, Hidetaka Kamigaito, Tatsuya Hiraoka)</author>
      <guid isPermaLink="false">2510.12845v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</title>
      <link>http://arxiv.org/abs/2510.07869v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://vincentgu2000.github.io/u0project/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为USIM的水下机器人模拟数据集和一个名为U0的VLA模型，它们共同解决了水下环境中机器人操作面临的挑战，特别是在数据稀缺的情况下，通过提供大规模高质量数据集和有效的多任务学习方法。&lt;h4&gt;背景&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的视野和受限的通信。虽然数据驱动的方法已经在陆地机器人上推动了具身智能的发展，并使专用水下机器人能够自主工作，但开发能够自主执行多项任务的水下智能仍然非常困难，因为大规模、高质量的水下数据集仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些限制，作者引入了USIM，这是一个基于模拟的多任务视觉-语言-动作数据集，用于水下机器人。&lt;h4&gt;方法&lt;/h4&gt;USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9个不同场景中的20项任务，范围从视觉导航到移动操作。基于这个数据集，作者提出了U0，这是一个用于通用水下机器人的VLA模型，它通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知增强模块(CAP)来提高空间理解和移动操作能力。&lt;h4&gt;主要发现&lt;/h4&gt;在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展的数据集构建、改进的任务自主性和智能通用水下机器人的实际实现提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的视野和受限的通信。虽然数据驱动的方法已经在陆地机器人上推动了具身智能的发展，并使专用水下机器人能够自主工作，但开发能够自主执行多项任务的水下智能仍然非常困难，因为大规模、高质量的水下数据集仍然稀缺。为了解决这些限制，我们引入了USIM，这是一个基于模拟的多任务视觉-语言-动作数据集，用于水下机器人。USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9个不同场景中的20项任务，范围从视觉导航到移动操作。基于这个数据集，我们提出了U0，这是一个用于通用水下机器人的VLA模型，它通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知增强模块(CAP)来提高空间理解和移动操作能力。在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展的数据集构建、改进的任务自主性和智能通用水下机器人的实际实现提供了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决水下机器人自主执行多任务的困难问题，原因是水下环境存在复杂流体动力学、有限可见性和受限通信等挑战，同时大规模高质量水下数据集稀缺。这个问题很重要，因为水下环境覆盖地球71%的面积，涉及海洋生态调查、资源开发、管道检查等多种应用，而水下操作对人类来说危险且困难，自主水下机器人能大幅提高效率和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了水下机器人面临的特殊挑战，然后选择使用仿真环境解决真实数据收集成本高的问题。他们基于现有Isaac-GR00T N1.5模型进行改进，而非从头训练，并整合了双目视觉和其他传感器模态。借鉴了Stonefish模拟器构建环境、ROS框架进行数据收集、Vision-Language Model和Diffusion Transformer架构，以及PID控制器和MoveIt进行机械手控制等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过仿真环境构建大规模水下VLA数据集，开发适应水下环境的VLA模型，建立可扩展的数据到任务框架。流程包括：1)用Stonefish模拟器构建9种水下场景和BlueROV2机器人模型；2)收集20个任务的数据，共561K帧和15.6小时交互数据；3)基于Isaac-GR00T N1.5开发U0模型，整合多模态传感器数据和CAP感知增强模块；4)通过开环离线评估和闭环在线测试验证模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个大规模水下多任务VLA数据集USIM，覆盖20个任务和9种场景；2)首个专为水下机器人设计的VLA模型U0，整合多模态传感器数据和CAP模块；3)使用以机器人为中心的坐标系表示目标位置。相比之前工作，USIM是首个多任务VLA数据集，而现有数据集多为特定任务；U0是首个专门针对水下环境的VLA模型，考虑了水下视觉退化和特殊运动特性，能处理多种任务而非单一任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过构建首个大规模水下多任务Vision-Language-Action数据集USIM和开发专门的水下机器人通用模型U0，解决了水下机器人高质量数据稀缺和通用任务执行能力不足的问题，为构建可扩展的水下智能机器人框架奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater environments present unique challenges for robotic operation,including complex hydrodynamics, limited visibility, and constrainedcommunication. Although data-driven approaches have advanced embodiedintelligence in terrestrial robots and enabled task-specific autonomousunderwater robots, developing underwater intelligence capable of autonomouslyperforming multiple tasks remains highly challenging, as large-scale,high-quality underwater datasets are still scarce. To address theselimitations, we introduce USIM, a simulation-based multi-taskVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over561K frames from 1,852 trajectories, totaling approximately 15.6 hours ofBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging fromvisual navigation to mobile manipulation. Building upon this dataset, wepropose U0, a VLA model for general underwater robots, which integratesbinocular vision and other sensor modalities through multimodal fusion, andfurther incorporates a convolution-attention-based perception focus enhancementmodule (CAP) to improve spatial understanding and mobile manipulation. Acrosstasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,the framework achieves a success rate of 80%, while in challenging mobilemanipulation tasks, it reduces the distance to the target by 21.2% comparedwith baseline methods, demonstrating its effectiveness. USIM and U0 show thatVLA models can be effectively applied to underwater robotic applications,providing a foundation for scalable dataset construction, improved taskautonomy, and the practical realization of intelligent general underwaterrobots.</description>
      <author>example@mail.com (Junwen Gu, Zhiheng Wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu)</author>
      <guid isPermaLink="false">2510.07869v3</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</title>
      <link>http://arxiv.org/abs/2510.13740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the Third Learning on Graphs  Conference (LoG 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的图构建方法Logarithmic Scalable Graph Construction (LSGC)和混合CNN-GNN模型LogViG，用于解决视觉图神经网络在大图像上计算成本高的问题，并通过引入高分辨率分支和多尺度特征融合提升了性能。&lt;h4&gt;背景&lt;/h4&gt;Vision graph neural networks (ViG)作为传统卷积神经网络(CNN)和视觉Transformer(ViT)的竞争性替代方案在视觉任务中显示出潜力，但常见的图构建方法如k近邻(KNN)在大图像上计算成本高，而现有的Sparse Vision Graph Attention (SVGA)方法存在固定步长导致的过度压缩和错过多连接的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图构建方法，通过限制长距离链接的数量来提高视觉图神经网络的性能，同时降低计算复杂度，并构建一个结合CNN和GNN优势的混合模型。&lt;h4&gt;方法&lt;/h4&gt;提出Logarithmic Scalable Graph Construction (LSGC)方法来增强性能，并设计了LogViG这一新型混合CNN-GNN模型；同时引入高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了多尺度高分辨率视觉GNN网络。&lt;h4&gt;主要发现&lt;/h4&gt;LogViG在图像分类和语义分割任务上的准确率、GMACs和参数方面均优于现有的ViG、CNN和ViT架构；最小模型Ti-LogViG在ImageNet-1K上达到79.9%的平均top-1准确率，比Vision GNN高1.7%，参数减少24.3%，GMACs减少35.3%。&lt;h4&gt;结论&lt;/h4&gt;通过提出的LSGC方法在ViG中利用长距离链接可以超过当前最先进ViG的性能，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;视觉图神经网络(ViG)作为传统卷积神经网络(CNN)和视觉Transformer(ViT)的竞争性替代方案，在视觉任务中显示出前景；然而，常见的图构建方法如k近邻(KNN)在大图像上可能计算成本高昂。虽然Sparse Vision Graph Attention (SVGA)等方法显示出前景，但SVGA的固定步长可能导致过度压缩和错过多个连接，无法从长距离链接中获取相同信息。基于这一观察，我们提出了一种新的图构建方法——对数可扩展图构建(LSGC)，通过限制长距离链接的数量来增强性能。为此，我们提出了LogViG，一种利用LSGC的新型混合CNN-GNN模型。此外，受多尺度和高分辨率架构成功的启发，我们引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了多尺度高分辨率视觉GNN网络。大量实验表明，LogViG在图像分类和语义分割任务上的准确率、GMACs和参数方面均优于现有的ViG、CNN和ViT架构。我们的最小模型Ti-LogViG在ImageNet-1K上达到79.9%的平均top-1准确率，标准差为0.2%，比Vision GNN高1.7%的平均准确率，参数减少24.3%，GMACs减少35.3%。我们的工作表明，通过我们提出的LSGC在ViG中利用长距离链接可以超过当前最先进ViG的性能。代码可在https://github.com/mmunir127/LogViG-Official获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision graph neural networks (ViG) have demonstrated promise in vision tasksas a competitive alternative to conventional convolutional neural nets (CNN)and transformers (ViTs); however, common graph construction methods, such ask-nearest neighbor (KNN), can be expensive on larger images. While methods suchas Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed stepscale can lead to over-squashing and missing multiple connections to gain thesame information that could be gained from a long-range link. Through thisobservation, we propose a new graph construction method, Logarithmic ScalableGraph Construction (LSGC) to enhance performance by limiting the number oflong-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN modelthat utilizes LSGC. Furthermore, inspired by the successes of multi-scale andhigh-resolution architectures, we introduce and apply a high-resolution branchand fuse features between our high-resolution and low-resolution branches for amulti-scale high-resolution Vision GNN network. Extensive experiments show thatLogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,GMACs, and parameters on image classification and semantic segmentation tasks.Our smallest model, Ti-LogViG, achieves an average top-1 accuracy onImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher averageaccuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%reduction in GMACs. Our work shows that leveraging long-range links in graphconstruction for ViGs through our proposed LSGC can exceed the performance ofcurrent state-of-the-art ViGs. Code is available athttps://github.com/mmunir127/LogViG-Official.</description>
      <author>example@mail.com (Mustafa Munir, Alex Zhang, Radu Marculescu)</author>
      <guid isPermaLink="false">2510.13740v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Message Passing on the Edge: Towards Scalable and Expressive GNNs</title>
      <link>http://arxiv.org/abs/2510.13615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EB-1WL和EB-GNN，一种基于边的颜色细化测试和相应的图神经网络架构。该架构受经典三角形计数算法启发，在消息传递过程中明确使用三角形信息。研究表明，EB-1WL比1-WL具有更强的表达能力，同时保持了接近线性的时间和内存复杂度。实验证明，EB-GNN是一种高效的通用架构，显著优于简单MPNN，且与任务专用GNN相比保持竞争力同时计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNN)领域存在对更具表达力且计算效率高的架构的需求，之前的提案在计算效率上存在问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于边的颜色细化测试(EB-1WL)和相应的GNN架构(EB-GNN)，以提高表达能力同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出EB-1WL（基于边的颜色细化测试）和EB-GNN架构，受Chiba和Nishizeki的经典三角形计数算法启发，在消息传递过程中明确使用三角形信息。&lt;h4&gt;主要发现&lt;/h4&gt;EB-1WL比1-WL具有更强的表达能力；提供了基于一阶逻辑的EB-1WL完整逻辑表征和基于同态计数的匹配区分度结果；EB-1WL和EB-GNN在实际图学习任务中需要接近线性的时间和内存；EB-GNN显著优于简单MPNN，与任务专用GNN相比保持竞争力同时计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;EB-GNN是一种高效、通用的GNN架构，在表达能力与计算效率之间取得了良好的平衡。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了EB-1WL，一种基于边的颜色细化测试，以及相应的GNN架构EB-GNN。我们的架构受到Chiba和Nishizeki经典三角形计数算法的启发，并在消息传递过程中明确使用三角形。我们取得了以下结果：(1) EB-1WL比1-WL具有显著更强的表达能力。此外，我们基于一阶逻辑提供了EB-1WL的完整逻辑表征，并基于同态计数提供了匹配的区分度结果。(2) 与之前提出的更具表达力的GNN架构的重要区别在于，EB-1WL和EB-GNN在实际图学习任务中需要接近线性的时间和内存。(3) 从经验上看，我们表明EB-GNN是一种高效的通用架构：它显著优于简单的MPNN，并且在计算效率方面远高于任务专用的GNN的同时，与它们保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose EB-1WL, an edge-based color-refinement test, and a correspondingGNN architecture, EB-GNN. Our architecture is inspired by a classic trianglecounting algorithm by Chiba and Nishizeki, and explicitly uses triangles duringmessage passing. We achieve the following results: (1)~EB-1WL is significantlymore expressive than 1-WL. Further, we provide a complete logicalcharacterization of EB-1WL based on first-order logic, and matchingdistinguishability results based on homomorphism counting. (2)~In an importantdistinction from previous proposals for more expressive GNN architectures,EB-1WL and EB-GNN require near-linear time and memory on practical graphlearning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficientgeneral-purpose architecture: It substantially outperforms simple MPNNs, andremains competitive with task-specialized GNNs while being significantly morecomputationally efficient.</description>
      <author>example@mail.com (Pablo Barceló, Fabian Jogl, Alexander Kozachinskiy, Matthias Lanzinger, Stefan Neumann, Cristóbal Rojas)</author>
      <guid isPermaLink="false">2510.13615v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs</title>
      <link>http://arxiv.org/abs/2510.13401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Workshop on New Approaches for Addressing the Computing  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为F-BFQ的灵活块浮点量化加速器，用于提高BFP量化大语言模型在边缘设备上的推理效率，能够在两种BFP量化变体间动态切换而无需重新配置。&lt;h4&gt;背景&lt;/h4&gt;大语言模型(LLMs)在日常任务中应用广泛，通过llama.cpp等推理框架的优化（如KV缓存和量化），使在边缘设备上部署LLMs变得更加可行。量化技术是使LLMs在资源受限的边缘设备上运行的关键，llama.cpp采用块浮点(BFP)量化来减小模型权重和输入张量的位宽、内存占用和计算需求。通常，LLMs在各层采用混合BFP量化以减少精度损失。&lt;h4&gt;目的&lt;/h4&gt;为了高效加速BFP量化LLMs的各层计算，需要开发一种专门的加速器，使其能够支持不同的BFP量化变体而无需重新配置。&lt;h4&gt;方法&lt;/h4&gt;作者提出了F-BFQ（Flexible Block Floating Point Quantization）加速器，该加速器可以动态切换两种BFP量化变体并执行矩阵乘法(MatMul)操作。&lt;h4&gt;主要发现&lt;/h4&gt;在AMD Kria板上部署的初始F-BFQ加速器设计，在三种BFP量化LLMs上相比基于Arm NEON的CPU执行，平均减少了1.4倍的推理时间，实现了每秒5.2个token（约3.9个单词）的处理速度。&lt;h4&gt;结论&lt;/h4&gt;F-BFQ加速器有效地提高了BFP量化LLMs在边缘设备上的推理效率，通过支持多种BFP量化变体的动态切换，无需重新配置即可加速模型各层的计算。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型(LLMs)在日常任务中日益突出，从改善语音转文本翻译到生成最新视频游戏的额外帧等。借助llama.cpp等支持KV缓存和量化等优化的LLM推理框架，现在在边缘设备上部署LLMs比以往任何时候都更容易。量化是使LLMs在资源受限的边缘设备上运行的基本技术，llama.cpp利用块浮点(BFP)量化大幅减小权重和输入张量的位宽、内存占用以及运行LLMs所需的计算能力。LLMs通常在模型各层采用混合BFP量化，以减少量化导致的模型精度损失。因此，为了高效加速BFP量化LLMs的各层，专门的加速器需要支持不同的BFP变体而无需重新配置。为解决这一问题，我们提出了F-BFQ（Flexible Block Floating Point Quantization）加速器，它可以在两种BFP量化变体间动态切换并执行矩阵乘法(MatMul)操作。我们在AMD Kria板上部署的初始F-BFQ加速器设计，在三种BFP量化LLMs上相比基于Arm NEON的CPU执行，平均减少了1.4倍的推理时间，同时实现了每秒5.2个token（约3.9个单词）的处理速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have become increasingly prominent for dailytasks, from improving sound-totext translation to generating additional framesfor the latest video games. With the help of LLM inference frameworks, such asllama.cpp, which support optimizations such as KV-caching and quantization, itis now easier than ever to deploy LLMs on edge devices. Quantization isfundamental to enable LLMs on resource-constrained edge devices, and llama.cpputilizes block floating point (BFP) quantization to drastically reduce the bitwidth of weights and input tensors, the memory footprint, and the computationalpower required to run LLMs. LLMs are typically quantized with mixed BFPquantization across the model layers to reduce the loss of model accuracy dueto quantization. Therefore, to efficiently accelerate across the layers ofBFP-quantized LLMs, specialized accelerators need to support different BFPvariants without reconfiguration. To address this issue, we propose a FlexibleBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamicallyswitch between two BFP quantization variants and perform matrix multiplication(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMDKria board, reduces inference time by 1.4x on average over the Arm NEON-basedCPU execution across three BFP quantized LLMs while achieving 5.2 tokens persecond (~3.9 words per second).</description>
      <author>example@mail.com (Jude Haris, José Cano)</author>
      <guid isPermaLink="false">2510.13401v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.13391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 8 figures, 11-page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种使用图神经网络(GNN)来近似计算网络流游戏中Banzhaf值的方法，解决了传统方法在处理大规模系统时的计算效率问题。&lt;h4&gt;背景&lt;/h4&gt;Banzhaf值用于量化多智能体系统中智能体的影响力，应用领域广泛，但精确计算对于超过约20个智能体的系统由于指数级复杂度而不可行；蒙特卡洛采样方法虽然可提供估计但存在样本复杂度高且无法跨网络配置泛化的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法来近似计算网络流游戏中的Banzhaf值，使其能够处理大规模和动态系统，并具备良好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)将Banzhaf值计算问题框架化为图级预测任务，直接从网络拓扑和控制结构中学习智能体影响力的模式；比较了三种GNN架构(GAT、GINE和EdgeConv)在大型合成数据集上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;训练后的GNN模型实现了高保真的Banzhaf值近似，计算速度比传统方法快数量级；模型展示了强大的零样本泛化能力，能够在未见过的网络结构上准确预测Banzhaf值而无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;图神经网络可以作为复杂网络化系统可扩展合作博弈论分析的实用工具。&lt;h4&gt;翻译&lt;/h4&gt;计算网络流游戏中的Banzhaf值对于量化多智能体系统中的智能体影响力至关重要，应用范围从网络安全到基础设施规划。然而，对于超过约20个智能体的系统，由于指数级复杂度，精确计算是不可行的。虽然蒙特卡洛采样方法可以提供统计估计，但它们存在样本复杂度高的问题，并且无法在不同网络配置之间转移知识，使其对于大规模或动态系统不切实际。我们提出了一种基于学习的新方法，使用图神经网络(GNN)来近似基数网络流游戏中的Banzhaf值。通过将问题框架化为图级预测任务，我们的方法直接从网络拓扑和控制结构中学习智能体影响力的可泛化模式。我们进行了全面的实证研究，比较了三种最先进的GNN架构-图注意力网络(GAT)、具有边特征的图同构网络(GINE)和EdgeConv-在每个配置200,000个图的大规模合成数据集上的性能，数据集在大小(20-100个节点)、智能体数量(5-20)和边概率(0.5-1.0)上有所不同。我们的结果表明，训练后的GNN模型实现了高保真的Banzhaf值近似，与精确和基于采样的方法相比，速度提高了数量级。最重要的是，我们展示了强大的零样本泛化能力：在特定大小和拓扑的图上训练的模型，能够准确预测具有完全不同结构特性的全新网络的Banzhaf值，而无需重新训练。这项工作确立了GNN作为复杂网络化系统可扩展合作博弈论分析的实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing the Banzhaf value in network flow games is fundamental forquantifying agent influence in multi-agent systems, with applications rangingfrom cybersecurity to infrastructure planning. However, exact computation isintractable for systems with more than $\sim20$ agents due to exponentialcomplexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods providestatistical estimates, they suffer from high sample complexity and cannottransfer knowledge across different network configurations, making themimpractical for large-scale or dynamic systems. We present a novellearning-based approach using Graph Neural Networks (GNNs) to approximateBanzhaf values in cardinal network flow games. By framing the problem as agraph-level prediction task, our method learns generalisable patterns of agentinfluence directly from network topology and control structure. We conduct acomprehensive empirical study comparing three state-of-the-art GNNarchitectures-Graph Attention Networks (GAT), Graph Isomorphism Networks withEdge features (GINE), and EdgeConv-on a large-scale synthetic dataset of200,000 graphs per configuration, varying in size (20-100 nodes), agent count(5-20), and edge probability (0.5-1.0). Our results demonstrate that trainedGNN models achieve high-fidelity Banzhaf value approximation withorder-of-magnitude speedups compared to exact and sampling-based methods. Mostsignificantly, we show strong zero-shot generalisation: models trained ongraphs of a specific size and topology accurately predict Banzhaf values forentirely new networks with different structural properties, without requiringretraining. This work establishes GNNs as a practical tool for scalablecooperative game-theoretic analysis of complex networked systems.</description>
      <author>example@mail.com (Benjamin Kempinski, Tal Kachman)</author>
      <guid isPermaLink="false">2510.13391v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Domain Adaptation: A Spectral Contrastive Perspective</title>
      <link>http://arxiv.org/abs/2510.13254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by ECML-PKDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FracNet是一种频率感知对比图网络，通过频谱分析将图分解为高频和低频成分，解决了图神经网络在领域适应中的挑战，通过对比学习框架改善了领域适应的模糊边界问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在各种领域取得了显著成功，但由于结构分布的显著变化和可转移模式探索不足，它们在领域适应方面常常遇到困难。传统方法没有区分处理全局和局部模式，导致多层GNN后图中的一些局部细节可能被破坏。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来更好地理解和处理领域适应中的分布变化，特别是通过频谱分析来识别和利用不同频率成分中的模式，以提高图神经网络在领域适应中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FracNet（频率感知对比图网络），包含两个协同模块，将原始图分解为高频和低频成分，并进行频率感知的领域适应。通过与对比学习框架集成，改善了领域适应的模糊边界问题。&lt;h4&gt;主要发现&lt;/h4&gt;领域变化可以通过频谱分析更好地理解，其中低频成分通常编码领域不变的全局模式，高频成分捕获领域特定的局部细节。通过分解图的不同频率成分，可以更有效地进行领域适应。&lt;h4&gt;结论&lt;/h4&gt;FracNet通过频谱分析和对比学习显著提高了领域适应的性能，实验证明其优于最先进的方法。研究不仅提供了实际应用价值，还提供了严格的理论证明来证明FracNet的优越性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在各种领域取得了显著成功，但由于结构分布的显著变化和可转移模式探索不足，它们在领域适应方面常常遇到困难。传统方法没有区分处理全局和局部模式，导致多层GNN后图中的一些局部细节可能被破坏。我们的关键见解是，领域变化可以通过频谱分析更好地理解，其中低频成分通常编码领域不变的全局模式，高频成分捕获领域特定的局部细节。因此，我们提出FracNet（频率感知对比图网络），包含两个协同模块，将原始图分解为高频和低频成分，并进行频率感知的领域适应。此外，通过与对比学习框架集成，改善了领域适应的模糊边界问题。除了实际应用意义外，我们还提供了严格的理论证明来证明FracNet的优越性。大量实验进一步证明了其优于最先进方法的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-06106-5_26&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved remarkable success in variousdomains, yet they often struggle with domain adaptation due to significantstructural distribution shifts and insufficient exploration of transferablepatterns. One of the main reasons behind this is that traditional approaches donot treat global and local patterns discriminatingly so that some local detailsin the graph may be violated after multi-layer GNN. Our key insight is thatdomain shifts can be better understood through spectral analysis, wherelow-frequency components often encode domain-invariant global patterns, andhigh-frequency components capture domain-specific local details. As such, wepropose FracNet (\underline{\textbf{Fr}}equency \underline{\textbf{A}}ware\underline{\textbf{C}}ontrastive Graph \underline{\textbf{Net}}work) with twosynergic modules to decompose the original graph into high-frequency andlow-frequency components and perform frequency-aware domain adaption. Moreover,the blurring boundary problem of domain adaptation is improved by integratingwith a contrastive learning framework. Besides the practical implication, wealso provide rigorous theoretical proof to demonstrate the superiority ofFracNet. Extensive experiments further demonstrate significant improvementsover state-of-the-art approaches.</description>
      <author>example@mail.com (Haoyu Zhang, Yuxuan Cheng, Wenqi Fan, Yulong Chen, Yifan Zhang)</author>
      <guid isPermaLink="false">2510.13254v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Universally Invariant Learning in Equivariant GNNs</title>
      <link>http://arxiv.org/abs/2510.13169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种理论上健全的框架，用于构建高效且实用的完备等变图神经网络(GNNs)，通过两个关键组件实现：完备的标量函数和满秩的可转向基集。&lt;h4&gt;背景&lt;/h4&gt;等变图神经网络在各种应用中显示出显著成功。为了实现完备性（即在等变函数空间上的通用逼近性质），网络必须有效捕捉不同节点之间复杂的多体相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一个理论上健全的框架，用于构建高效且实用的完备等变GNN，解决现有方法计算成本高且没有多项式时间解决方案的问题。&lt;h4&gt;方法&lt;/h4&gt;证明完备的等变GNN可以通过两个关键组件实现：1) 完备的标量函数，称为几何图的规范形式；2) 满秩的可转向基集。基于这一发现，提出了基于EGNN和TFN两种常见模型的高效算法。&lt;h4&gt;主要发现&lt;/h4&gt;实证结果表明，该模型在仅有几层的情况下展现出优越的完备性和出色的性能，从而显著降低了计算开销，同时保持强大的实际效能。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为构建高效且实用的完备等变GNN提供了理论基础，解决了现有方法的计算效率问题。&lt;h4&gt;翻译&lt;/h4&gt;等变图神经网络在各种应用中已显示出显著成功。为了实现完备性——即在等变函数空间上的通用逼近性质——网络必须有效捕捉不同节点之间复杂的多体相互作用。现有方法通过更深层次的结构、增强的体阶数或增加可转向特征的维度来实现这一目标，通常计算成本高且没有多项式时间解决方案。在这项工作中，我们提出了一个理论上健全的框架，用于构建高效且实用的完备等变GNN。我们证明，完备的等变GNN可以通过两个关键组件实现：1) 完备的标量函数，称为几何图的规范形式；2) 满秩的可转向基集。利用这一发现，我们提出了基于两种常见模型(EGNN和TFN)的构建完备等变GNN的高效算法。实证结果表明，我们的模型在仅有几层的情况下展现出优越的完备性和出色的性能，从而显著降低了计算开销，同时保持强大的实际效能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决等变图神经网络(GNN)的完备性问题，即模型能否近似任何连续函数的能力。这个问题在科学计算和物理模拟中至关重要，因为它决定了模型能否准确捕捉复杂几何数据（如分子结构、蛋白质等）中的多体相互作用。现有方法需要通过增加网络深度、提高阶数或增加特征维度来获得更好的表达能力，但这会导致计算成本大幅增加，限制了等变GNN在实际应用中的使用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先重新审视了现有等变GNN模型（如EGNN、TFN、MACE等），将它们统一为基于多体高阶基函数扩展的形式，从而识别出当前方法的局限性。然后，作者从输出空间角度提出新的动态方法，借鉴了几何同构问题的研究成果，提出完全等变GNN需要两个关键组件：几何图的规范形式和满秩基集。作者还借鉴了FastEGNN等工作中关于虚拟节点学习的思想，并基于四点定位原理提出了多项式时间算法来构建规范形式。此外，作者借鉴了不对称图上的着色理论，证明了在非对称图上总能构建满秩基集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个关键组件构建完全等变GNN：几何图的规范形式（完全标量函数）和满秩的可引导基集，而不需要通过增加网络深度、阶数或特征维度。实现流程包括：1) 构建几何图的规范形式，对于一般图使用四点定位原理（O(N^6)复杂度），对于非对称图使用E(3)等变函数生成参考点（O(N^2)复杂度）；2) 构建满秩基集，通过节点着色（⊕或⊗方法）使每个节点具有唯一特征；3) 实际模型实现，如EGNNcpl和TFNcpl，通过着色、构建虚拟节点、更新特征和全局操作来实现单层完备模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出新的完备性框架，将完全等变GNN构建转化为规范形式和满秩基集两个组件；2) 提出几何同构问题的多项式时间算法，为一般图提供O(N^6)算法，为非对称图提供O(N^2)算法；3) 证明在非对称几何图上总能构建任意度数的满秩基集；4) 提出EGNN/TFNcpl-global和EGNN/TFNcpl-local两种实际实现。相比之前的工作，传统方法需要通过增加阶数、层数或特征维度来实现完备性，计算成本高且无法保证在有限复杂度下实现完备性，而本文方法通过动态方法在保持低计算复杂度的同时实现了完备性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种名为Uni-EGNN的高效框架，通过几何图的规范形式和满秩基集两个关键组件，使等变图神经网络在保持低计算复杂度的同时实现了完备性，显著提升了模型在科学计算和物理模拟任务中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Equivariant Graph Neural Networks (GNNs) have demonstrated significantsuccess across various applications. To achieve completeness -- that is, theuniversal approximation property over the space of equivariant functions -- thenetwork must effectively capture the intricate multi-body interactions amongdifferent nodes. Prior methods attain this via deeper architectures, augmentedbody orders, or increased degrees of steerable features, often at highcomputational cost and without polynomial-time solutions. In this work, wepresent a theoretically grounded framework for constructing completeequivariant GNNs that is both efficient and practical. We prove that a completeequivariant GNN can be achieved through two key components: 1) a completescalar function, referred to as the canonical form of the geometric graph; and2) a full-rank steerable basis set. Leveraging this finding, we propose anefficient algorithm for constructing complete equivariant GNNs based on twocommon models: EGNN and TFN. Empirical results demonstrate that our modeldemonstrates superior completeness and excellent performance with only a fewlayers, thereby significantly reducing computational overhead while maintainingstrong practical efficacy.</description>
      <author>example@mail.com (Jiacheng Cen, Anyi Li, Ning Lin, Tingyang Xu, Yu Rong, Deli Zhao, Zihe Wang, Wenbing Huang)</author>
      <guid isPermaLink="false">2510.13169v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering</title>
      <link>http://arxiv.org/abs/2510.12959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种后验流行度去偏置(PPD)方法，用于纠正基于图神经网络的协同过滤中的流行度偏见问题。&lt;h4&gt;背景&lt;/h4&gt;用户历史交互数据是协同过滤中学习用户偏好的主要信号，但训练数据通常呈现长尾分布，导致模型学习流行度偏见，降低推荐质量。图神经网络虽然有效，但其聚合过程会进一步传播和放大这种偏见。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接对抗GNN邻域聚合过程中传播的流行度偏见的方法，提高推荐的个性化程度和质量。&lt;h4&gt;方法&lt;/h4&gt;提出PPD方法，该方法在预训练嵌入上操作，不需要重新训练。通过估计交互级别的流行度并使用流行度方向向量从节点表示中移除流行度组件，从而减少偏见同时保留用户偏好。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法通过修改训练目标解决偏见问题，但无法直接对抗GNN邻域聚合过程中的偏见传播；在聚合过程中对交互应用权重可能缓解问题，但由于训练早期节点表示不稳定，可能导致模型学习扭曲。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，PPD方法在基于GNN的CF的流行度偏见纠正方面优于现有最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;用户历史交互数据是协同过滤中学习用户偏好的主要信号。然而，训练数据通常呈现长尾分布，只有少数项目拥有大部分交互。直接在这种不平衡数据上训练的CF模型容易学习流行度偏见，降低个性化程度，导致次优的推荐质量。图神经网络(GNN)由于其消息传递机制对CF有效，但可以通过聚合过程进一步传播和放大流行度偏见。现有方法通常通过修改训练目标来解决流行度偏见，但未能直接对抗GNN在邻域聚合过程中传播的偏见。在聚合过程中对交互应用权重可以帮助缓解此问题，但由于训练早期阶段节点表示不稳定，可能会扭曲模型学习。在本文中，我们提出了一种后验流行度去偏置(PPD)方法，用于纠正基于GNN的CF中的流行度偏见，并直接在预训练嵌入上操作，无需重新训练。通过估计交互级别的流行度并使用流行度方向向量从节点表示中移除流行度组件，PPD减少了偏见同时保留了用户偏好。实验结果表明，我们的方法在基于GNN的CF的流行度偏见纠正方面优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User historical interaction data is the primary signal for learning userpreferences in collaborative filtering (CF). However, the training data oftenexhibits a long-tailed distribution, where only a few items have the majorityof interactions. CF models trained directly on such imbalanced data are proneto learning popularity bias, which reduces personalization and leads tosuboptimal recommendation quality. Graph Neural Networks (GNNs), whileeffective for CF due to their message passing mechanism, can further propagateand amplify popularity bias through their aggregation process. Existingapproaches typically address popularity bias by modifying training objectivesbut fail to directly counteract the bias propagated during GNN's neighborhoodaggregation. Applying weights to interactions during aggregation can helpalleviate this problem, yet it risks distorting model learning due to unstablenode representations in the early stages of training. In this paper, we proposea Post-hoc Popularity Debiasing (PPD) method that corrects for popularity biasin GNN-based CF and operates directly on pre-trained embeddings withoutrequiring retraining. By estimating interaction-level popularity and removingpopularity components from node representations via a popularity directionvector, PPD reduces bias while preserving user preferences. Experimentalresults show that our method outperforms state-of-the-art approaches forpopularity bias correction in GNN-based CF.</description>
      <author>example@mail.com (Md Aminul Islam, Elena Zheleva, Ren Wang)</author>
      <guid isPermaLink="false">2510.12959v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand</title>
      <link>http://arxiv.org/abs/2510.12328v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的物理信息图神经网络方法，结合极值分析技术，有效提高了泰国地区降雨预测的准确性，特别是对极端事件的预测能力。&lt;h4&gt;背景&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。泰国地区的站点降雨预测面临特殊挑战。&lt;h4&gt;目的&lt;/h4&gt;开发结合物理信息的图神经网络和极值分析技术，改进泰国地区的站点降雨预测，特别是提高极端事件的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;使用图结构表示站点捕捉时空模式；预处理影响区域降雨的气候指标；提出Attention-LSTM模型，利用地形降水物理公式推导边特征；采用空间季节感知广义帕累托分布方法进行阈值超限映射，解决极值预测问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在大多数地区优于成熟的基线模型，包括易发生极端事件的区域；与最先进方法保持竞争力；相比SEAS5业务预报系统，显著改进了极端事件预测；提供高分辨率地图支持长期水资源管理决策。&lt;h4&gt;结论&lt;/h4&gt;该方法在实际应用中提高了极端事件的预测能力，为长期水资源管理中的决策提供了实用增强。&lt;h4&gt;翻译&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。本文提出了新颖的物理信息图神经网络结合极值分析技术，以改进泰国地区的站点降雨预测。该模型利用站点图的图结构表示来捕捉复杂的时空模式，并通过遥相关提供可解释性。我们预处理了可能影响区域降雨的相关气候指标。提出的带有长短期记忆的图注意力网络使用简单地形降水物理公式推导的初始边特征应用注意力机制。嵌入随后由LSTM层处理。为解决极值问题，我们使用新颖的空间季节感知广义帕累托分布方法进行阈值超限映射，这克服了传统机器学习模型的局限性。实验表明，我们的方法在大多数地区都优于成熟的基线模型，包括易发生极端事件的区域，并且与最先进的方法保持强劲竞争力。与业务预报系统SEAS5相比，我们的实际应用改进了极端事件的预测，并提供了实用增强，以支持长期水资源管理中的决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate rainfall forecasting, particularly for extreme events, remains asignificant challenge in climatology and the Earth system. This paper presentsnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-valueanalysis techniques to improve gauge-station rainfall predictions acrossThailand. The model leverages a graph-structured representation of gaugestations to capture complex spatiotemporal patterns, and it offersexplainability through teleconnections. We preprocess relevant climate indicesthat potentially influence regional rainfall. The proposed Graph AttentionNetwork with Long Short-Term Memory (Attention-LSTM) applies the attentionmechanism using initial edge features derived from simpleorographic-precipitation physics formulation. The embeddings are subsequentlyprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold(POT) mapping using the novel Spatial Season-aware Generalized ParetoDistribution (GPD) method, which overcomes limitations of traditionalmachine-learning models. Experiments demonstrate that our method outperformswell-established baselines across most regions, including areas prone toextremes, and remains strongly competitive with the state of the art. Comparedwith the operational forecasting system SEAS5, our real-world applicationimproves extreme-event prediction and offers a practical enhancement to producefine-resolution maps that support decision-making in long-term watermanagement.</description>
      <author>example@mail.com (Kiattikun Chobtham, Kanoksri Sarinnapakorn, Kritanai Torsri, Prattana Deeprasertkul, Jirawan Kamma)</author>
      <guid isPermaLink="false">2510.12328v2</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis</title>
      <link>http://arxiv.org/abs/2510.13735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为循环自监督扩散(CSS-Diff)的框架，用于从低场MRI合成高质量高场MRI图像，解决了现有方法中存在的临床保真度差距问题。&lt;h4&gt;背景&lt;/h4&gt;低场MRI具有成本低、可及性高、安全性好等优点，但存在分辨率低和信噪比差的问题。从低场MRI合成高质量图像可以减少对昂贵采集的依赖并扩大数据可用性。&lt;h4&gt;目的&lt;/h4&gt;解决从低场MRI合成高场MRI时存在的临床保真度差距，保留解剖保真度，增强细粒度结构细节，弥合图像对比度中的域差距。&lt;h4&gt;方法&lt;/h4&gt;提出循环自监督扩散(CSS-Diff)框架，在循环一致性约束下重新制定基于扩散的合成过程，强制在整个生成过程中保持解剖结构。框架包含两个新过程：切片级差距感知网络通过对比学习对齐切片间不一致性；局部结构校正网络通过掩码和扰动块的自重建增强局部特征恢复。&lt;h4&gt;主要发现&lt;/h4&gt;在跨场合成任务上取得了最先进的性能，包括PSNR、SSIM和LPIPS指标的提升。与原始低场MRI相比，保留了细粒度解剖结构，左脑白质误差从12.1%降至2.1%，皮层从4.2%降至3.7%。&lt;h4&gt;结论&lt;/h4&gt;CSS-Diff可以合成既定量可靠又解剖一致的图像。&lt;h4&gt;翻译&lt;/h4&gt;从低场MRI合成高质量图像具有巨大潜力。低场MRI更便宜、更易获取且更安全，但分辨率低且信噪比差。这种合成过程可以减少对昂贵采集的依赖并扩大数据可用性。然而，从低场MRI合成高场MRI仍存在临床保真度差距。需要保留解剖保真度，增强细粒度结构细节，并弥合图像对比度中的域差距。为解决这些问题，我们提出了一个用于从真实低场MRI数据合成高场MRI的循环自监督扩散(CSS-Diff)框架。我们的核心思想是在循环一致性约束下重新制定基于扩散的合成过程。它在整个生成过程中强制保持解剖结构，而不仅仅依赖成对的像素级监督。CSS-Diff框架还进一步整合了两个新过程：切片级差距感知网络通过对比学习对齐切片间不一致性；局部结构校正网络通过掩码和扰动块的自重建增强局部特征恢复。在跨场合成任务上的广泛实验证明了我们方法的有效性，取得了最先进的性能。除了像素级保真度外，与原始低场MRI相比，我们的方法还保留了细粒度解剖结构。总之，我们的CSS-Diff可以合成既定量可靠又解剖一致的图像。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthesizing high-quality images from low-field MRI holds significantpotential. Low-field MRI is cheaper, more accessible, and safer, but suffersfrom low resolution and poor signal-to-noise ratio. This synthesis process canreduce reliance on costly acquisitions and expand data availability. However,synthesizing high-field MRI still suffers from a clinical fidelity gap. Thereis a need to preserve anatomical fidelity, enhance fine-grained structuraldetails, and bridge domain gaps in image contrast. To address these issues, wepropose a \emph{cyclic self-supervised diffusion (CSS-Diff)} framework forhigh-field MRI synthesis from real low-field MRI data. Our core idea is toreformulate diffusion-based synthesis under a cycle-consistent constraint. Itenforces anatomical preservation throughout the generative process rather thanjust relying on paired pixel-level supervision. The CSS-Diff framework furtherincorporates two novel processes. The slice-wise gap perception network alignsinter-slice inconsistencies via contrastive learning. The local structurecorrection network enhances local feature restoration throughself-reconstruction of masked and perturbed patches. Extensive experiments oncross-field synthesis tasks demonstrate the effectiveness of our method,achieving state-of-the-art performance (e.g., 31.80 $\pm$ 2.70 dB in PSNR,0.943 $\pm$ 0.102 in SSIM, and 0.0864 $\pm$ 0.0689 in LPIPS). Beyond pixel-wisefidelity, our method also preserves fine-grained anatomical structures comparedwith the original low-field MRI (e.g., left cerebral white matter error dropsfrom 12.1$\%$ to 2.1$\%$, cortex from 4.2$\%$ to 3.7$\%$). To conclude, ourCSS-Diff can synthesize images that are both quantitatively reliable andanatomically consistent.</description>
      <author>example@mail.com (Zhenxuan Zhang, Peiyuan Jing, Zi Wang, Ula Briski, Coraline Beitone, Yue Yang, Yinzhe Wu, Fanwen Wang, Liutao Yang, Jiahao Huang, Zhifan Gao, Zhaolin Chen, Kh Tohidul Islam, Guang Yang, Peter J. Lally)</author>
      <guid isPermaLink="false">2510.13735v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.13675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为KnowCoL（Knowledge-guided Contrastive Learning）的框架，用于开放域视觉实体识别，通过结合图像和文本描述，利用维基数据的结构化信息，将视觉和文本输入抽象到概念层面，支持零样本实体识别。&lt;h4&gt;背景&lt;/h4&gt;开放域视觉实体识别旨在识别和链接图像中描绘的实体，与维基数据等庞大且不断变化的真实世界概念集合相关联。与传统分类任务不同，它在开放集条件下运行，大多数目标实体在训练过程中未见，且呈现长尾分布，导致任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决开放域视觉实体识别任务中的挑战，包括有限的监督、高视觉歧义性和语义消歧的需要，特别是针对训练过程中未见到的实体。&lt;h4&gt;方法&lt;/h4&gt;提出KnowCoL框架，将图像和文本描述结合到由维基数据结构化信息支持的共享语义空间中。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层次结构和关系上下文来支持零样本实体识别。&lt;h4&gt;主要发现&lt;/h4&gt;在OVEN基准测试上评估显示，使用视觉、文本和结构化知识大大提高了准确性，特别是对于稀有和未见实体。最小的模型相比最先进的方法在未见实体上的准确性提高了10.5%，尽管模型尺寸缩小了35倍。&lt;h4&gt;结论&lt;/h4&gt;KnowCoL框架有效地解决了开放域视觉实体识别中的挑战，特别是在处理稀有和未见实体方面表现出色，同时模型尺寸显著减小。&lt;h4&gt;翻译&lt;/h4&gt;开放域视觉实体识别旨在识别和链接图像中描绘的实体，与维基数据等庞大且不断变化的真实世界概念集合相关联。与具有固定标签集的传统分类任务不同，它在开放集条件下运行，大多数目标实体在训练过程中未见，且呈现长尾分布。这使任务本身具有挑战性，因为监督有限、视觉歧义度高，且需要语义消歧。在这项工作中，我们提出了一个知识引导的对比学习框架，将图像和文本描述结合到由维基数据结构化信息支持的共享语义空间中。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层次结构和关系上下文来支持零样本实体识别。我们在OVEN基准上评估了我们的方法，OVEN是一个大规模开放域视觉识别数据集，以维基数据ID作为标签空间。我们的实验表明，使用视觉、文本和结构化知识大大提高了准确性，特别是对于稀有和未见实体。与最先进的方法相比，我们的最小模型在未见实体上的准确性提高了10.5%，尽管模型尺寸缩小了35倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-domain visual entity recognition aims to identify and link entitiesdepicted in images to a vast and evolving set of real-world concepts, such asthose found in Wikidata. Unlike conventional classification tasks with fixedlabel sets, it operates under open-set conditions, where most target entitiesare unseen during training and exhibit long-tail distributions. This makes thetask inherently challenging due to limited supervision, high visual ambiguity,and the need for semantic disambiguation. In this work, we propose aKnowledge-guided Contrastive Learning (KnowCoL) framework that combines bothimages and text descriptions into a shared semantic space grounded bystructured information from Wikidata. By abstracting visual and textual inputsto a conceptual level, the model leverages entity descriptions, typehierarchies, and relational context to support zero-shot entity recognition. Weevaluate our approach on the OVEN benchmark, a large-scale open-domain visualrecognition dataset with Wikidata IDs as the label space. Our experiments showthat using visual, textual, and structured knowledge greatly improves accuracy,especially for rare and unseen entities. Our smallest model improves theaccuracy on unseen entities by 10.5% compared to the state-of-the-art, despitebeing 35 times smaller.</description>
      <author>example@mail.com (Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Jingcheng Wu, Nadeem Nazer, Steffen Staab)</author>
      <guid isPermaLink="false">2510.13675v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services</title>
      <link>http://arxiv.org/abs/2510.13368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合对比学习的依赖建模和异常检测方法，解决了云服务环境中复杂依赖关系和多样化异常模式的挑战&lt;h4&gt;背景&lt;/h4&gt;云服务环境中存在复杂依赖关系和多样化异常模式的挑战&lt;h4&gt;目的&lt;/h4&gt;提出一种结合对比学习的依赖建模和异常检测方法，解决云服务环境中的异常检测问题&lt;h4&gt;方法&lt;/h4&gt;将服务交互抽象为依赖图，通过嵌入函数提取时间和结构特征，使用图卷积机制聚合邻域信息实现上下文感知的服务表示，引入对比学习框架构建正负样本对增强正常和异常模式可分性，设计时间一致性约束保持表示稳定性，结合对比损失和时间一致性损失进行整体优化&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上从超参数、环境和数据敏感性角度系统评估了该方法，在精确率、召回率、F1分数和AUC等关键指标上显著优于现有方法，在稀疏标记、监控噪声和流量波动条件下保持鲁棒性&lt;h4&gt;结论&lt;/h4&gt;验证了将依赖建模与对比学习结合的有效性，为云服务异常检测提供了完整的技术解决方案，在复杂环境中表现出强大的适应性和稳定性&lt;h4&gt;翻译&lt;/h4&gt;本文通过提出一种结合对比学习的依赖建模和异常检测方法，解决了云服务环境中复杂依赖关系和多样化异常模式的挑战。该方法将服务交互抽象为依赖图，通过嵌入函数提取时间和结构特征，并采用图卷积机制聚合邻域信息以实现上下文感知的服务表示。随后引入对比学习框架，构建正负样本对以增强正常和异常模式在表示空间中的可分性。此外，设计了时间一致性约束以保持跨时间步的表示稳定性，减少短期波动和噪声的影响。整体优化结合了对比损失和时间一致性损失，确保在多维度特征下的稳定可靠检测。在公共数据集上从超参数、环境和数据敏感性角度对该方法进行了系统评估。结果表明，在精确率、召回率、F1分数和AUC等关键指标上，所提出的方法显著优于现有方法，同时在稀疏标记、监控噪声和流量波动条件下保持鲁棒性。本研究验证了结合依赖建模与对比学习的有效性，为云服务异常检测提供了完整的技术解决方案，并在复杂环境中表现出强大的适应性和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges of complex dependencies and diverseanomaly patterns in cloud service environments by proposing a dependencymodeling and anomaly detection method that integrates contrastive learning. Themethod abstracts service interactions into a dependency graph, extractstemporal and structural features through embedding functions, and employs agraph convolution mechanism to aggregate neighborhood information forcontext-aware service representations. A contrastive learning framework is thenintroduced, constructing positive and negative sample pairs to enhance theseparability of normal and abnormal patterns in the representation space.Furthermore, a temporal consistency constraint is designed to maintainrepresentation stability across time steps and reduce the impact of short-termfluctuations and noise. The overall optimization combines contrastive loss andtemporal consistency loss to ensure stable and reliable detection acrossmulti-dimensional features. Experiments on public datasets systematicallyevaluate the method from hyperparameter, environmental, and data sensitivityperspectives. Results show that the proposed approach significantly outperformsexisting methods on key metrics such as Precision, Recall, F1-Score, and AUC,while maintaining robustness under conditions of sparse labeling, monitoringnoise, and traffic fluctuations. This study verifies the effectiveness ofintegrating dependency modeling with contrastive learning, provides a completetechnical solution for cloud service anomaly detection, and demonstrates strongadaptability and stability in complex environments.</description>
      <author>example@mail.com (Yue Xing, Yingnan Deng, Heyao Liu, Ming Wang, Yun Zi, Xiaoxuan Sun)</author>
      <guid isPermaLink="false">2510.13368v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Universal Image Restoration Pre-training via Masked Degradation Classification</title>
      <link>http://arxiv.org/abs/2510.13282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种掩码退化分类预训练方法（MaskDCPT），用于图像退化类型分类，从而实现全面的图像恢复预训练。该方法使用图像退化类型作为弱监督，同时利用图像重建增强性能和鲁棒性。MaskDCPT包含一个编码器和两个解码器，分别用于特征提取、退化类型分类和高质量图像重建。该方法结合了掩码图像建模和对比学习的优势，显著提升了CNN和Transformer在图像恢复任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;传统预训练方法在图像恢复任务中存在局限性，需要一种能够处理多种退化类型的通用图像恢复方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够分类图像退化类型的预训练方法，实现全面的图像恢复预训练，提高模型在通用图像恢复任务中的性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出MaskDCPT方法，使用图像退化类型作为弱监督。构建包含一个编码器和两个解码器的架构：编码器从掩码的低质量输入图像中提取特征；分类解码器使用这些特征识别退化类型；重建解码器重建对应的高质量图像。利用掩码图像建模和对比学习的好处。构建UIR-2.5M数据集，包含250万对恢复样本，覆盖19种退化类型和200多种退化水平。&lt;h4&gt;主要发现&lt;/h4&gt;MaskDCPT显著提高了CNN和Transformer的性能，在5D全一恢复任务中PSNR至少提高3.77分贝，在真实退化场景中PIQE减少34.8%。模型对未见过的退化类型和水平表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MaskDCPT是一种简单而强大的预训练方法，可用于通用图像恢复，能够处理多种退化类型并在各种场景中表现出色。发布的UIR-2.5M数据集、源代码和模型可供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了一种掩码退化分类预训练方法（MaskDCPT），旨在促进输入图像中退化类型的分类，从而实现全面的图像恢复预训练。与传统预训练方法不同，MaskDCPT使用图像的退化类型作为极弱监督，同时利用图像重建来增强性能和鲁棒性。MaskDCPT包含一个编码器和两个解码器：编码器从掩码的低质量输入图像中提取特征；分类解码器使用这些特征识别退化类型，而重建解码器旨在重建相应的高质量图像。这种设计使预训练能够受益于掩码图像建模和对比学习，从而生成适合恢复任务的通用表示。得益于简单而强大的MaskDCPT，预训练的编码器可用于解决通用图像恢复并取得卓越性能。实施MaskDCPT显著提高了卷积神经网络（CNN）和Transformer的性能，在5D全一恢复任务中PSNR最小提高3.77分贝，在真实退化场景中与基线相比PIQE减少34.8%。它还对以前未见过的退化类型和水平表现出强大的泛化能力。此外，我们整理并发布了UIR-2.5M数据集，包含250万对恢复样本，涵盖19种退化类型和200多种退化水平，包括合成和真实世界数据。该数据集、源代码和模型可在https://github.com/MILab-PKU/MaskDCPT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a Masked Degradation Classification Pre-Training method(MaskDCPT), designed to facilitate the classification of degradation types ininput images, leading to comprehensive image restoration pre-training. Unlikeconventional pre-training methods, MaskDCPT uses the degradation type of theimage as an extremely weak supervision, while simultaneously leveraging theimage reconstruction to enhance performance and robustness. MaskDCPT includesan encoder and two decoders: the encoder extracts features from the maskedlow-quality input image. The classification decoder uses these features toidentify the degradation type, whereas the reconstruction decoder aims toreconstruct a corresponding high-quality image. This design allows thepre-training to benefit from both masked image modeling and contrastivelearning, resulting in a generalized representation suited for restorationtasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trainedencoder can be used to address universal image restoration and achieveoutstanding performance. Implementing MaskDCPT significantly improvesperformance for both convolution neural networks (CNNs) and Transformers, witha minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task anda 34.8% reduction in PIQE compared to baseline in real-world degradationscenarios. It also emergences strong generalization to previously unseendegradation types and levels. In addition, we curate and release the UIR-2.5Mdataset, which includes 2.5 million paired restoration samples across 19degradation types and over 200 degradation levels, incorporating both syntheticand real-world data. The dataset, source code, and models are available athttps://github.com/MILab-PKU/MaskDCPT.</description>
      <author>example@mail.com (JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu)</author>
      <guid isPermaLink="false">2510.13282v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding</title>
      <link>http://arxiv.org/abs/2510.13244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure. demo page: https://motionbeat2025.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MotionBeat是一个运动对齐的音乐表示学习框架，通过具身对比损失和结构节奏对齐损失，以及小节等变相旋转和接触引导注意力等创新架构，成功捕捉了音乐的具身维度，在音乐到舞蹈生成和多种音频处理任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;音乐既是听觉现象也是具身现象，与人体运动密切相关，但现有音频表示忽略了这种具身维度，限制了捕捉驱动运动的节奏和结构线索的能力。&lt;h4&gt;目的&lt;/h4&gt;提出MotionBeat框架，用于学习能够捕捉音乐运动特性的音乐表示。&lt;h4&gt;方法&lt;/h4&gt;采用两个训练目标：具身对比损失(ECL)实现细粒度节奏区分，结构节奏对齐损失(SRAL)确保节奏一致性；架构上引入小节等变相旋转捕捉循环节奏模式，以及接触引导注意力强调与音乐重音同步的运动事件。&lt;h4&gt;主要发现&lt;/h4&gt;MotionBeat在音乐到舞蹈生成方面优于最先进的音频编码器，并在节拍跟踪、音乐标记、流派和乐器分类、情感识别以及视听检索等任务中有效迁移。&lt;h4&gt;结论&lt;/h4&gt;MotionBeat框架成功捕捉了音乐的具身维度，提高了音乐表示的质量，在多种音频处理任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;音乐既是听觉现象也是具身现象，与人体运动密切相关，并通过舞蹈自然表达。然而，大多数现有的音频表示忽略了这种具身维度，限制了它们捕捉驱动运动的节奏和结构线索的能力。我们提出了MotionBeat，一个用于运动对齐的音乐表示学习框架。MotionBeat通过两个新提出的目标进行训练：具身对比损失(ECL)，一种具有速度感知和节拍抖动负样本的增强型InfoNCE公式，用于实现细粒度节奏区分；以及结构节奏对齐损失(SRAL)，通过将音乐重音与相应运动事件对齐来确保节奏一致性。在架构上，MotionBeat引入了小节等变相旋转来捕捉循环节奏模式，以及接触引导注意力来强调与音乐重音同步的运动事件。实验表明，MotionBeat在音乐到舞蹈生成方面优于最先进的音频编码器，并有效迁移到节拍跟踪、音乐标记、流派和乐器分类、情感识别以及视听检索等任务。我们的项目演示页面：https://motionbeat2025.github.io/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Music is both an auditory and an embodied phenomenon, closely linked to humanmotion and naturally expressed through dance. However, most existing audiorepresentations neglect this embodied dimension, limiting their ability tocapture rhythmic and structural cues that drive movement. We proposeMotionBeat, a framework for motion-aligned music representation learning.MotionBeat is trained with two newly proposed objectives: the EmbodiedContrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware andbeat-jitter negatives to achieve fine-grained rhythmic discrimination, and theStructural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency byaligning music accents with corresponding motion events. Architecturally,MotionBeat introduces bar-equivariant phase rotations to capture cyclicrhythmic patterns and contact-guided attention to emphasize motion eventssynchronized with musical accents. Experiments show that MotionBeat outperformsstate-of-the-art audio encoders in music-to-dance generation and transferseffectively to beat tracking, music tagging, genre and instrumentclassification, emotion recognition, and audio-visual retrieval. Our projectdemo page: https://motionbeat2025.github.io/.</description>
      <author>example@mail.com (Xuanchen Wang, Heng Wang, Weidong Cai)</author>
      <guid isPermaLink="false">2510.13244v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning</title>
      <link>http://arxiv.org/abs/2510.13176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRACE是一个创新的编译器自动调优框架，通过利用通道协同性和加权评分方法缩小搜索空间，使用对比学习和相似感知聚类创建程序嵌入，并在聚类内进行进化搜索，生成针对未见程序具有强泛化能力的核心集通道序列。实验表明，GRACE在LLVM IR指令计数优化方面达到了最先进的性能，同时保持了高效的调优时间。&lt;h4&gt;背景&lt;/h4&gt;编译器通道选择和阶段排序是实现最优程序性能的重大挑战，特别是对于代码大小缩减等目标。标准编译器启发式方法具有通用适用性，但由于其'一刀切'的特性，通常会产生次优的、程序特定的结果。虽然迭代编译可以找到量身定制的解决方案，但其高昂的搜索成本限制了实际应用。机器学习方法承诺更快的推理速度，但经常难以泛化到未见程序。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的编译器自动调优框架，能够在保持快速调优时间的同时，为未见程序提供高质量的优化解决方案，特别是在LLVM IR指令计数优化方面。&lt;h4&gt;方法&lt;/h4&gt;GRACE框架首先利用通道协同性和加权评分方法生成初始高质量候选序列和通道池，有效缩小搜索空间。然后采用对比学习方法，使用基于通道序列的数据增强技术创建程序嵌入，促进相似感知聚类。在这些聚类内进行进化搜索，生成k个专门设计的通道序列核心集，旨在对未见程序实现强泛化能力。在测试时，GRACE高效选择最佳核心集序列并使用轻量级技术进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在七个不同的数据集上的实验结果表明，GRACE与opt -Oz相比，在LLVM 10.0.0上将LLVM IR指令计数平均减少了10.09%，在LLVM 18.1.6上平均减少了10.19%，同时每个程序的调优时间平均不到1秒，展示了其最先进的性能和实际有效性。&lt;h4&gt;结论&lt;/h4&gt;GRACE框架成功解决了编译器自动调优中的搜索空间过大和泛化能力不足的问题，通过结合通道协同性、加权评分、对比学习和进化搜索等技术，实现了在保持高效调优时间的同时，为未见程序提供高质量优化的能力，在LLVM IR指令计数优化方面达到了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;编译器通道选择和阶段排序是实现最优程序性能的重大挑战，特别是对于代码大小缩减等目标。标准编译器启发式方法具有通用适用性，但由于其'一刀切'的特性，通常会产生次优的、程序特定的结果。虽然迭代编译可以找到量身定制的解决方案，但其高昂的搜索成本限制了实际应用。机器学习方法承诺更快的推理速度，但经常难以泛化到未见程序。本文介绍了GRACE，一个用于编译器自动调优的新颖框架，已在LLVM IR指令计数优化中得到验证。GRACE通过利用通道协同性和加权评分方法有效缩小搜索空间，生成初始高质量候选序列和通道池。然后采用对比学习方法，使用基于通道序列的数据增强技术创建程序嵌入，促进相似感知聚类。在这些聚类内进行进化搜索，生成k个专门设计的通道序列核心集，旨在对未见程序实现强泛化能力。在测试时，GRACE高效选择最佳核心集序列并使用轻量级技术进行优化。在七个不同数据集上的实验结果表明，GRACE与opt -Oz相比，在LLVM 10.0.0上将LLVM IR指令计数平均减少了10.09%，在LLVM 18.1.6上平均减少了10.19%，同时每个程序的调优时间平均不到1秒，展示了其最先进的性能和实际有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compiler pass selection and phase ordering present a significant challenge inachieving optimal program performance, particularly for objectives like codesize reduction. Standard compiler heuristics offer general applicability butoften yield suboptimal, program-specific results due to their one-size-fits-allnature. While iterative compilation can find tailored solutions, itsprohibitive search cost limits practical use. Machine learning approachespromise faster inference but frequently struggle with generalization to unseenprograms. This paper introduces GRACE, a novel framework for compilerauto-tuning, demonstrated for LLVM IR instruction count optimization. GRACEeffectively curtails the search space by leveraging pass synergies and aweighted scoring method to generate initial high-quality candidate sequencesand a pass pool. It then employs contrastive learning, using passsequence-based data augmentation, to create program embeddings that facilitatesimilarity-aware clustering. Evolutionary search within these clusters yields acoreset of $k$ specialized pass sequences designed for robust generalization tounseen programs. At test time, GRACE efficiently selects the best coresetsequence and refines it using lightweight techniques. Experimental results onseven diverse datasets show that GRACE reduces LLVM IR instruction count by anaverage of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,while incurring an average tuning time of less than 1s per program,demonstrating its state-of-the-art performance and practical effectiveness.</description>
      <author>example@mail.com (Haolin Pan, Chao Zha, Jinyuan Dong, Mingjie Xing, Yanjun Wu)</author>
      <guid isPermaLink="false">2510.13176v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>VCTR: A Transformer-Based Model for Non-parallel Voice Conversion</title>
      <link>http://arxiv.org/abs/2510.12964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VCTR的高效非并行语音转换方法，结合了混合感知块和双剪枝自注意力机制，采用基于对比学习的对抗方法，解决了现有方法中存在的长距离依赖捕获不足的问题。&lt;h4&gt;背景&lt;/h4&gt;非并行语音转换技术旨在无需配对训练数据的情况下将源语音域转换为目标语音域。现有的CycleGAN、VAE和CVC等方法在训练效果和语义捕获方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获语音中长距离依赖关系的高效非并行语音转换方法，以提升转换质量和全局语义表达能力。&lt;h4&gt;方法&lt;/h4&gt;提出VCTR方法，结合了Hybrid Perception Block (HPB)和Dual Pruned Self-Attention (DPSA)技术，采用基于对比学习的对抗训练框架，能够更好地捕获语音中的长距离依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;基于CNN的生成器虽然能捕获局部语义，但缺乏捕获全局语义所需的长距离依赖能力；所提出的VCTR方法通过结合HPB和DPSA有效解决了这一问题。&lt;h4&gt;结论&lt;/h4&gt;VCTR是一种高效的非并行语音转换方法，通过创新的网络结构和训练策略，显著提升了语音转换的质量和全局语义表达能力。&lt;h4&gt;翻译&lt;/h4&gt;非并行语音转换旨在无需配对训练数据的情况下将语音从源域转换到目标域。循环一致性生成对抗网络(CycleGAN)和变分自编码器(VAE)已被用于此任务，但这些模型面临训练困难和结果不理想的问题。后来，对比语音转换(Contrastive Voice Conversion, CVC)被提出，利用基于对比学习的方法解决这些问题。然而，这些方法使用基于CNN的生成器，虽然可以捕获局部语义，但缺乏捕获全局语义所需的长距离依赖能力。在本文中，我们提出了VCTR，一种用于非并行语音转换的高效方法，它结合了混合感知块(HPB)和双剪枝自注意力(DPSA)，以及基于对比学习的对抗方法。代码可在https://github.com/Maharnab-Saikia/VCTR找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-parallel voice conversion aims to convert voice from a source domain to atarget domain without paired training data. Cycle-Consistent GenerativeAdversarial Networks (CycleGAN) and Variational Autoencoders (VAE) have beenused for this task, but these models suffer from difficult training andunsatisfactory results. Later, Contrastive Voice Conversion (CVC) wasintroduced, utilizing a contrastive learning-based approach to address theseissues. However, these methods use CNN-based generators, which can capturelocal semantics but lacks the ability to capture long-range dependenciesnecessary for global semantics. In this paper, we propose VCTR, an efficientmethod for non-parallel voice conversion that leverages the Hybrid PerceptionBlock (HPB) and Dual Pruned Self-Attention (DPSA) along with a contrastivelearning-based adversarial approach. The code can be found inhttps://github.com/Maharnab-Saikia/VCTR.</description>
      <author>example@mail.com (Maharnab Saikia)</author>
      <guid isPermaLink="false">2510.12964v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title>
      <link>http://arxiv.org/abs/2510.13245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SketchSem3D，第一个从抽象手绘草图和卫星图像伪标签注释生成3D户外语义场景的大规模基准数据集，以及Cylinder Mamba Diffusion (CymbaDiff)方法，显著增强了户外3D场景生成的空间连贯性。&lt;h4&gt;背景&lt;/h4&gt;户外3D语义场景生成技术为城市模拟和自动驾驶等应用提供逼真且语义丰富的环境，但该领域发展受限于缺乏公开可用的、良好注释的数据集。&lt;h4&gt;目的&lt;/h4&gt;引入SketchSem3D基准数据集，用于从抽象手绘草图和卫星图像的伪标签注释生成3D户外语义场景。&lt;h4&gt;方法&lt;/h4&gt;SketchSem3D包含两个子集：基于Sketch的SemanticKITTI和基于Sketch的KITTI-360（包含LiDAR体素及其相应的草图和注释卫星图像）。提出Cylinder Mamba Diffusion (CymbaDiff)方法，施加结构化空间排序，捕获圆柱连续性和垂直层次结构，保持物理邻域关系和全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在SketchSem3D上的大量实验表明，CymbaDiff实现了卓越的语义一致性、空间真实性和跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;翻译&lt;/h4&gt;户外3D语义场景生成为城市模拟和自动驾驶等应用生成逼真且语义丰富的环境。然而，这一方向的进展受到缺乏公开可用、良好注释的数据集的限制。我们引入SketchSem3D，这是第一个从抽象手绘草图和卫星图像的伪标签注释生成3D户外语义场景的大规模基准。SketchSem3D包含两个子集：基于Sketch的SemanticKITTI和基于Sketch的KITTI-360（包含LiDAR体素及其相应的草图和注释卫星图像），以实现标准化、严格和多样化的评估。我们还提出了Cylinder Mamba Diffusion (CymbaDiff)，显著增强了户外3D场景生成的空间连贯性。CymbaDiff施加结构化空间排序，明确捕获圆柱连续性和垂直层次结构，并在生成的场景中保持物理邻域关系和全局上下文。在SketchSem3D上的大量实验表明，CymbaDiff实现了卓越的语义一致性、空间真实性和跨数据集泛化能力。代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决户外3D语义场景生成的问题，特别是从手绘草图和伪标记卫星图像注释生成3D城市场景。这个问题很重要，因为生成逼真且语义丰富的户外环境对城市模拟和自动驾驶等应用至关重要，但该领域缺乏公开、良好标注的数据集，且现有方法难以处理户外场景的高语义多样性、复杂空间结构和动态上下文依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到户外3D场景生成的重要性及现有方法的局限性，然后构建了SketchSem3D数据集作为基础。方法设计借鉴了状态空间模型(SSMs)在捕获长程依赖关系方面的优势，以及扩散模型在生成任务中的成功经验。作者创新性地结合了笛卡尔和圆柱坐标系统，设计了圆柱Mamba块(CylMa)来增强空间一致性，同时保留了三重Mamba模块以保持精确几何距离。整体架构包括场景结构估计网络、潜在映射网络和去噪网络，通过多尺度特征提取和维度分解残差块来提升性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化空间扩散结合圆柱和笛卡尔坐标系统的优势，增强户外3D场景生成的空间一致性。整体流程包括：1)数据预处理，生成草图和伪标记卫星图像注释；2)使用场景结构估计网络提取抽象结构信息；3)通过变分自编码器将输入条件压缩为潜在表示；4)在潜在空间中使用圆柱Mamba块进行去噪扩散；5)融合三重Mamba和圆柱Mamba的特征，结合径向和轴对齐的空间线索；6)生成最终的3D语义场景，每个体素被分配语义类标签。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'基于草图的3D户外场景生成'新任务；2)构建SketchSem3D数据集，提供更高分辨率(256×256×32)、更多语义类别(20类)和更丰富的地理空间语义；3)设计圆柱Mamba扩散(CymbaDiff)模型；4)通过结构化空间排序捕获圆柱连续性和垂直层次结构；5)利用草图和伪标记卫星图像注释作为多模态条件输入。相比之前工作，CymbaDiff结合了圆柱和笛卡尔坐标系统，更好地表示户外场景结构；通过状态空间模型和扩散模型结合，更高效地捕获长程依赖；将草图生成从孤立对象和简单室内场景扩展到复杂户外场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了SketchSem3D数据集和CymbaDiff方法，首次实现了从手绘草图和伪标记卫星图像注释生成高质量、空间一致的3D户外语义场景，为城市模拟和自动驾驶等应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor 3D semantic scene generation produces realistic and semantically richenvironments for applications such as urban simulation and autonomous driving.However, advances in this direction are constrained by the absence of publiclyavailable, well-annotated datasets. We introduce SketchSem3D, the firstlarge-scale benchmark for generating 3D outdoor semantic scenes from abstractfreehand sketches and pseudo-labeled annotations of satellite images.SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-basedKITTI-360 (containing LiDAR voxels along with their corresponding sketches andannotated satellite images), to enable standardized, rigorous, and diverseevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) thatsignificantly enhances spatial coherence in outdoor 3D scene generation.CymbaDiff imposes structured spatial ordering, explicitly captures cylindricalcontinuity and vertical hierarchy, and preserves both physical neighborhoodrelationships and global context within the generated scenes. Extensiveexperiments on SketchSem3D demonstrate that CymbaDiff achieves superiorsemantic consistency, spatial realism, and cross-dataset generalization. Thecode and dataset will be available athttps://github.com/Lillian-research-hub/CymbaDiff</description>
      <author>example@mail.com (Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian)</author>
      <guid isPermaLink="false">2510.13245v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</title>
      <link>http://arxiv.org/abs/2510.13747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InteractiveOmni是一个统一的开源多模态大语言模型，参数规模从4B到8B，专注于音频视觉多轮交互，整合了视觉编码器、音频编码器、大语言模型和语音解码器，采用多阶段训练策略，具有强大的跨模态能力和类人长期对话能力。&lt;h4&gt;背景&lt;/h4&gt;轻量级模型领域需要全面的多模态理解和语音生成能力，现有模型可能在这一方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的、开源的多模态大语言模型，引领轻量级模型领域，提供全面的多模态理解和语音生成能力。&lt;h4&gt;方法&lt;/h4&gt;将视觉编码器、音频编码器、大语言模型和语音解码器整合到统一模型中；设计多阶段训练策略：预训练用于多模态理解，然后进行语音对话和视听交互的后训练；精心策划多轮训练数据集，增强处理复杂多轮交互的能力；构建多模态多轮记忆基准和多轮语音交互基准，用于评估多轮记忆和语音交互能力。&lt;h4&gt;主要发现&lt;/h4&gt;InteractiveOmni显著优于领先的开源模型；InteractiveOmni-4B在通用基准上可与更大的Qwen2.5-Omni-7B模型相媲美；InteractiveOmni-4B仅使用50%的模型大小就能保留InteractiveOmni-8B 97%的性能；在图像、音频、视频理解和语音生成任务上，与同等规模的模型相比取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;InteractiveOmni是下一代智能交互系统的可访问开源基础模型，提供了更智能的多轮音频视觉体验，特别是在长期记忆能力方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了InteractiveOmni，这是一个统一的开源多模态大语言模型，参数规模从4B到8B，专为音频视觉多轮交互设计，通过提供全面的多模态理解和语音生成能力引领轻量级模型领域。为此，我们将视觉编码器、音频编码器、大语言模型和语音解码器整合到一个统一模型中，用于理解和生成任务。我们设计了一个多阶段训练策略，以确保强大的跨模态能力，包括预训练用于多模态理解，随后进行语音对话和视听交互的后训练。为了实现类人长期对话能力，我们精心策划了一个多轮训练数据集，增强模型处理复杂多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验证明，InteractiveOmni显著优于领先的开源模型，提供了更智能的多轮音频视觉体验，特别是在其长期记忆能力方面。值得注意的是，InteractiveOmni-4B在通用基准上可与大得多的Qwen2.5-Omni-7B模型相媲美，同时仅利用50%的模型大小就能保留InteractiveOmni-8B 97%的性能。在图像、音频、视频理解和语音生成任务上，与同等规模的模型相比取得了最先进的结果，InteractiveOmni是下一代智能交互系统的可访问开源基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce InteractiveOmni, a unified and open-source omni-modal largelanguage model for audio-visual multi-turn interaction, ranging from 4B to 8Bparameters, designed to lead the field of lightweight models by offeringcomprehensive omni-modal understanding and speech generation capabilities. Toachieve this, we integrate the vision encoder, audio encoder, large languagemodel, and speech decoder into a unified model for understanding and generationtasks. We design a multi-stage training strategy to ensure robust cross-modalcapabilities, including pre-training for omni-modal understanding, followed bypost-training with speech conversation and audio-visual interaction. To enablehuman-like long-term conversational ability, we meticulously curate amulti-turn training dataset that enhances the model's ability to handle complexand multi-turn interactions. To effectively evaluate the multi-turn memory andspeech interaction capabilities, we construct the multi-modal multi-turn memorybenchmark and the multi-turn speech interaction benchmark. Experimentsdemonstrate that InteractiveOmni significantly outperforms leading open-sourcemodels and provides a more intelligent multi-turn audio-visual experience,particularly in its long-term memory capabilities. Notably, InteractiveOmni-4Bis comparable to the much larger model like Qwen2.5-Omni-7B on generalbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8Bwhile utilizing only 50% of the model size. Achieving state-of-the-art resultsagainst similarly sized models across image, audio, video understanding, andspeech generation tasks, InteractiveOmni is an accessible, open-sourcefoundation for next-generation intelligent interactive systems.</description>
      <author>example@mail.com (Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu)</author>
      <guid isPermaLink="false">2510.13747v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping</title>
      <link>http://arxiv.org/abs/2510.13672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 12 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用贝叶斯分层时空模型分析了巴西累西腓市2015-2024年的登革热病例，探讨了多种社会环境和气候因素对登革热风险的影响，并识别了高风险区域。&lt;h4&gt;背景&lt;/h4&gt;登革热是巴西主要的流行病学挑战之一，表现为城市内部的不平等以及气候和社会环境因素的影响。&lt;h4&gt;目的&lt;/h4&gt;分析2015-2024年累西腓市的登革热确诊病例，评估多种因素对登革热风险的影响。&lt;h4&gt;方法&lt;/h4&gt;使用R-INLA实现的贝叶斯分层时空模型，结合BYM2空间结构和RW1时间成分。纳入的协变量包括人口密度、家庭规模、收入、排水渠道、滞后降水量和平均温度。&lt;h4&gt;主要发现&lt;/h4&gt;人口密度和家庭规模增加登革热风险；收入和排水渠道具有保护作用；滞后降水量增加风险；高温显示反向关联，表明媒介活动的热阈值；模型拟合良好，收敛稳定；北部和西部存在持续高风险集群，与高密度和社会脆弱性区域重叠。&lt;h4&gt;结论&lt;/h4&gt;贝叶斯模型支持概率预测和早期预警系统。与经典模型相比，INLA明确整合了不确定性和时空依赖性，为城市健康管理决策提供了可信区间推断。&lt;h4&gt;翻译&lt;/h4&gt;登革热仍然是巴西主要的流行病学挑战之一，表现为城市内部的不平等以及气候和社会环境因素的影响。本研究使用R-INLA实现的贝叶斯分层时空模型分析了2015-2024年累西腓市的登革热确诊病例，结合了BYM2空间结构和RW1时间成分。协变量包括人口密度、家庭规模、收入、排水渠道、滞后降水量和平均温度。人口密度和家庭规模对登革热风险有正向影响，而收入和渠道存在具有保护作用。滞后降水量增加风险，较高温度显示反向关联，表明媒介活动的热阈值。模型拟合良好，收敛稳定，具有中等程度的残差空间自相关和2016-2019年间平滑的时间趋势。空间时间估计显示累西腓北部和西部持续存在高风险集群，与较高密度和社会脆弱性区域重叠。除了重现历史模式外，贝叶斯模型还支持概率预测和早期预警系统。与经典模型相比，INLA明确整合了不确定性和时空依赖性，为城市健康管理决策提供了可信区间推断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dengue remains one of Brazil's major epidemiological challenges, marked bystrong intra-urban inequalities and the influence of climatic andsocio-environmental factors. This study analyzed confirmed dengue cases inRecife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal modelimplemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporalcomponent. Covariates included population density, household size, income,drainage channels, lagged precipitation, and mean temperature. Populationdensity and household size had positive effects on dengue risk, while incomeand channel presence were protective. Lagged precipitation increased risk, andhigher temperatures showed an inverse association, suggesting thermalthresholds for vector activity. The model achieved good fit (DIC=65817;WAIC=64506) and stable convergence, with moderate residual spatialautocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.Spatio-temporal estimates revealed persistent high-risk clusters in northernand western Recife, overlapping with areas of higher density and socialvulnerability. Beyond reproducing historical patterns, the Bayesian modelsupports probabilistic forecasting and early warning systems. Compared withclassical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertaintyand spatial-temporal dependence, offering credible interval inference fordecision-making in urban health management.</description>
      <author>example@mail.com (Marcílio Ferreira dos Santos, Andreza dos Santos Rodrigues de Melo)</author>
      <guid isPermaLink="false">2510.13672v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning</title>
      <link>http://arxiv.org/abs/2510.13614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MemoTime是一种记忆增强的时间知识图谱框架，通过结构化基础、递归推理和持续经验学习解决大型语言模型在时间理解方面的挑战，显著提升了模型在时间问答任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型已展现出强大的推理能力，但在处理涉及多个实体、复合运算符和演变事件序列的时间理解问题时存在困难。时间知识图谱虽提供了结构化的时间事实，但现有基于TKG的LLM推理方法仍面临四大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有TKG-based LLM推理方法面临的四大挑战：保持多跳推理的时间忠实性、实现多实体时间同步、适应不同时间运算符的检索、重用先验推理经验以提高稳定性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出MemoTime框架，将复杂时间问题分解为层次化的时间树，实现运算符感知推理；包含动态证据检索层自适应选择策略；以及自我演化的经验记忆存储已验证推理轨迹、工具包决策和子问题嵌入用于跨类型重用。&lt;h4&gt;主要发现&lt;/h4&gt;在多个时间问答基准上，MemoTime取得了最先进的结果，比强基线模型高出24.0%；使较小模型(如Qwen3-4B)能达到与GPT-4-Turbo相当的推理性能。&lt;h4&gt;结论&lt;/h4&gt;MemoTime有效解决了现有方法面临的四大挑战，显著提升了大型语言模型在时间理解方面的能力，并使较小模型也能达到高性能水平。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)已经取得了令人印象深刻的推理能力，但在时间理解方面存在困难，特别是当问题涉及多个实体、复合运算符和不断演变的事件序列时。时间知识图谱(TKGs)以结构化格式捕获大量时间事实，为时间推理提供了可靠来源。然而，现有的基于TKG的LLM推理方法仍面临四大挑战：在多跳推理中保持时间忠实性、实现多实体时间同步、使检索适应不同的时间运算符、重用先前的推理经验以提高稳定性和效率。为解决这些问题，我们提出了MemoTime，这是一个记忆增强的时间知识图谱框架，通过结构化基础、递归推理和持续经验学习来增强LLM推理。MemoTime将复杂的时间问题分解为层次化的时间树，实现运算符感知推理，强制单调时间戳并在统一时间边界下共同约束多个实体。动态证据检索层自适应地选择特定运算符的检索策略，而自我演化的经验记忆存储已验证的推理轨迹、工具包决策和子问题嵌入用于跨类型重用。在多个时间问答基准上的综合实验显示，MemoTime取得了最先进的结果，比强大的基线模型高出24.0%。此外，MemoTime使较小的模型(如Qwen3-4B)能够实现与GPT-4-Turbo相当的推理性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved impressive reasoning abilities,but struggle with temporal understanding, especially when questions involvemultiple entities, compound operators, and evolving event sequences. TemporalKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in astructured format, offer a reliable source for temporal reasoning. However,existing TKG-based LLM reasoning methods still struggle with four majorchallenges: maintaining temporal faithfulness in multi-hop reasoning, achievingmulti-entity temporal synchronization, adapting retrieval to diverse temporaloperators, and reusing prior reasoning experience for stability and efficiency.To address these issues, we propose MemoTime, a memory-augmented temporalknowledge graph framework that enhances LLM reasoning through structuredgrounding, recursive reasoning, and continual experience learning. MemoTimedecomposes complex temporal questions into a hierarchical Tree of Time,enabling operator-aware reasoning that enforces monotonic timestamps andco-constrains multiple entities under unified temporal bounds. A dynamicevidence retrieval layer adaptively selects operator-specific retrievalstrategies, while a self-evolving experience memory stores verified reasoningtraces, toolkit decisions, and sub-question embeddings for cross-type reuse.Comprehensive experiments on multiple temporal QA benchmarks show that MemoTimeachieves overall state-of-the-art results, outperforming the strong baseline byup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) toachieve reasoning performance comparable to that of GPT-4-Turbo.</description>
      <author>example@mail.com (Xingyu Tan, Xiaoyang Wang, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang)</author>
      <guid isPermaLink="false">2510.13614v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs</title>
      <link>http://arxiv.org/abs/2510.13251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 28 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了视频大语言模型（VideoLLMs）在视频问答任务中的内部工作机制和信息流动模式。通过可解释性技术分析，研究者发现了VideoLLMs处理视频和文本信息的特定阶段和模式，并展示了如何通过选择有效信息通路来保持模型性能。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型（VideoLLMs）将视觉-语言模型的能力扩展到时空输入，使视频问答（VideoQA）等任务成为可能。尽管近期VideoLLMs取得了进展，但它们在提取和传播视频与文本信息方面的内部机制仍较少被探索。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探究VideoLLMs的内部信息流动机制，特别是它们在视频问答任务中如何进行时序推理以及如何整合视频和文本信息。&lt;h4&gt;方法&lt;/h4&gt;研究者使用可解释性技术来分析VideoLLMs的内部信息流动模式。&lt;h4&gt;主要发现&lt;/h4&gt;1. 时序推理从早期到中间层开始，涉及帧间积极交互；2. 随后在中间层进行视频-语言逐步整合，这得益于视频表示与包含时间概念的词嵌入之间的对齐；3. 完成整合后，模型在中间到后期层准备生成正确答案；4. 通过选择有效信息通路并抑制大量注意力边缘（例如在LLaVA-NeXT-7B-Video-FT中为58%），VideoLLMs可以保持其视频问答性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现为理解VideoLLMs如何执行时序推理提供了蓝图，并为提高模型可解释性和下游泛化能力提供了实用见解。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型（VideoLLMs）将视觉-语言模型的能力扩展到时空输入，使视频问答（VideoQA）等任务成为可能。尽管近期VideoLLMs取得了进展，但它们在提取和传播视频与文本信息方面的内部机制仍较少被探索。在本研究中，我们使用可解释性技术研究了VideoLLMs的内部信息流动。我们的分析揭示了跨不同视频问答任务的一致模式：（1）VideoLLMs中的时序推理从早期到中间层的帧间积极交互开始，（2）随后在中间层进行视频-语言逐步整合。这得益于视频表示与包含时间概念的词嵌入之间的对齐。（3）完成此整合后，模型在中间到后期层准备生成正确答案。（4）基于我们的分析，我们表明VideoLLMs可以通过选择这些有效信息通路同时抑制大量注意力边缘来保持其视频问答性能，例如在LLaVA-NeXT-7B-Video-FT中为58%。这些发现为VideoLLMs如何执行时序推理提供了蓝图，并为提高模型可解释性和下游泛化能力提供了实用见解。我们的项目页面和源代码可在https://map-the-flow.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VideoLLMs) extend the capabilities ofvision-language models to spatiotemporal inputs, enabling tasks such as videoquestion answering (VideoQA). Despite recent advances in VideoLLMs, theirinternal mechanisms on where and how they extract and propagate video andtextual information remain less explored. In this study, we investigate theinternal information flow of VideoLLMs using mechanistic interpretabilitytechniques. Our analysis reveals consistent patterns across diverse VideoQAtasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frameinteractions in early-to-middle layers, (2) followed by progressivevideo-language integration in middle layers. This is facilitated by alignmentbetween video representations and linguistic embeddings containing temporalconcepts. (3) Upon completion of this integration, the model is ready togenerate correct answers in middle-to-late layers. (4) Based on our analysis,we show that VideoLLMs can retain their VideoQA performance by selecting theseeffective information pathways while suppressing a substantial amount ofattention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide ablueprint on how VideoLLMs perform temporal reasoning and offer practicalinsights for improving model interpretability and downstream generalization.Our project page with the source code is available athttps://map-the-flow.github.io</description>
      <author>example@mail.com (Minji Kim, Taekyung Kim, Bohyung Han)</author>
      <guid isPermaLink="false">2510.13251v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation</title>
      <link>http://arxiv.org/abs/2510.13084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Edit-Your-Interest的轻量级、文本驱动、零样本视频编辑方法，通过时空特征内存和特征传播技术解决了现有视频编辑方法计算开销大、内存消耗高和视觉保真度低的问题。&lt;h4&gt;背景&lt;/h4&gt;现有文本到图像扩散模型在视频编辑方面取得了显著进展，但现有视频编辑方法受高计算开销和内存消耗的限制，且往往牺牲视觉保真度，导致时间不一致性和模糊、马赛克状伪影等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级、文本驱动、零样本的视频编辑方法，以提高效率和视觉保真度。&lt;h4&gt;方法&lt;/h4&gt;Edit-Your-Interest方法包含三个核心技术：1)时空特征内存(SFM)缓存来自先前帧的关键图像标记；2)特征最相似传播(FMP)方法将最相关标记从前一帧传播到后续帧；3)SFM更新算法持续刷新缓存特征。此外，还利用交叉注意图自动提取感兴趣实例的掩码，并将其集成到扩散去噪过程中实现细粒度控制。&lt;h4&gt;主要发现&lt;/h4&gt;SFM显著减少了计算开销；FMP保留了时间一致性；SFM更新算法确保了特征的长期相关性和有效性；掩码集成方法实现了对目标对象的高度准确编辑，同时保持背景完整性。&lt;h4&gt;结论&lt;/h4&gt;Edit-Your-Interest在效率和视觉保真度上都优于现有最先进方法，验证了其优越的有效性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;文本到图像(T2I)扩散模型最近在视频编辑方面展示了显著进展。然而，现有的视频编辑方法受到高计算开销和内存消耗的严重限制。此外，这些方法通常牺牲视觉保真度，导致不期望的时间不一致性和伪影，如模糊和明显的马赛克状图案。我们提出了Edit-Your-Interest，一种轻量级、文本驱动、零样本的视频编辑方法。Edit-Your-Interest引入了一个时空特征内存来缓存来自先前帧的特征，与完整序列时空建模方法相比显著减少了计算开销。具体来说，我们首先引入了一个时空特征内存库(SFM)，它被设计用来高效缓存和保留由空间注意力处理的关键图像标记。其次，我们提出了特征最相似传播(FMP)方法。FMP将最相关的标记从先前帧传播到后续帧，保持时间一致性。最后，我们引入了一个SFM更新算法，它不断刷新缓存的特征，确保它们在整个视频序列中的长期相关性和有效性。此外，我们利用交叉注意图自动提取感兴趣实例的掩码。这些掩码无缝集成到扩散去噪过程中，实现对目标对象的细粒度控制，并允许Edit-Your-Interest在稳健保持背景完整性的同时执行高度准确的编辑。大量实验明确证明，所提出的Edit-Your-Interest在效率和视觉保真度上都优于最先进的方法，验证了其优越的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-image (T2I) diffusion models have recently demonstrated significantprogress in video editing.  However, existing video editing methods are severely limited by their highcomputational overhead and memory consumption.  Furthermore, these approaches often sacrifice visual fidelity, leading toundesirable temporal inconsistencies and artifacts such as blurring andpronounced mosaic-like patterns.  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot videoediting method.  Edit-Your-Interest introduces a spatio-temporal feature memory to cachefeatures from previous frames, significantly reducing computational overheadcompared to full-sequence spatio-temporal modeling approaches.  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),which is designed to efficiently cache and retain the crucial image tokensprocessed by spatial attention.  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMPpropagates the most relevant tokens from previous frames to subsequent ones,preserving temporal consistency.  Finally, we introduce an SFM update algorithm that continuously refreshes thecached features, ensuring their long-term relevance and effectivenessthroughout the video sequence.  Furthermore, we leverage cross-attention maps to automatically extract masksfor the instances of interest.  These masks are seamlessly integrated into the diffusion denoising process,enabling fine-grained control over target objects and allowingEdit-Your-Interest to perform highly accurate edits while robustly preservingthe background integrity.  Extensive experiments decisively demonstrate that the proposedEdit-Your-Interest outperforms state-of-the-art methods in both efficiency andvisual fidelity, validating its superior effectiveness and practicality.</description>
      <author>example@mail.com (Yi Zuo, Zitao Wang, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao)</author>
      <guid isPermaLink="false">2510.13084v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</title>
      <link>http://arxiv.org/abs/2510.13016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了时空视频动作定位(SVAG)任务，旨在解决细粒度动作理解和对象时空定位的挑战，构建了大规模基准数据集SVAG-Bench，提出了基线框架SVAGFormer和评估工具包SVAGEval，发现现有模型在复杂场景中表现不佳，需要更高级的推理能力。&lt;h4&gt;背景&lt;/h4&gt;细粒度动作理解和准确定位其对应的时间和空间位置是推进下一代AI系统的基础能力。然而，现有视频理解方法主要处理粗粒度动作识别或通用目标跟踪，忽略了根据动作联合检测和跟踪多个目标并在时间上定位它们的挑战。&lt;h4&gt;目的&lt;/h4&gt;引入时空视频动作定位(SVAG)新任务，构建支持该任务的基准数据集，提出基线框架，并开发标准化评估工具包，以促进细粒度动作理解和对象时空定位的研究。&lt;h4&gt;方法&lt;/h4&gt;构建SVAG-Bench基准测试，包含688个视频、19,590条标注记录和903个独特动词；提出SVAGFormer框架，适配最先进的视觉语言模型进行联合时空定位；开发SVAGEval标准化评估工具包以确保公平和可复现的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中，这突显了在长视频中针对细粒度对象-动作交互进行更高级推理的必要性。&lt;h4&gt;结论&lt;/h4&gt;需要开发能够同时处理细粒度动作理解、对象跟踪和时空定位的AI系统，SVAG任务和基准测试为这一研究方向提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;理解细粒度动作并准确定位它们在空间和时间中对应的执行者是推进下一代AI系统的基础能力，包括具身智能体、自主平台和人机交互框架。尽管最近视频理解取得了进展，但现有方法主要处理粗粒度动作识别或通用目标跟踪，从而忽略了根据动作联合检测和跟踪多个目标并在时间上定位它们的挑战。为解决这一差距，我们引入时空视频动作定位(SVAG)，这是一个新任务，要求模型基于自然语言描述的动作同时检测、跟踪和时域定位视频中的所有指代对象。为支持此任务，我们构建了SVAG-Bench，这是一个大规模基准，包含688个视频、19,590条标注记录和903个独特动词，涵盖了多样化的对象、动作和现实世界场景。我们进一步提出了SVAGFormer，这是一个基线框架，适配最先进的视觉语言模型进行联合时空定位，并引入了SVAGEval，这是一个标准化评估工具包，用于公平和可复现的基准测试。实验结果表明，现有模型在SVAG上表现不佳，特别是在密集或复杂场景中，这突显了在长视频中针对细粒度对象-动作交互进行更高级推理的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding fine-grained actions and accurately localizing theircorresponding actors in space and time are fundamental capabilities foradvancing next-generation AI systems, including embodied agents, autonomousplatforms, and human-AI interaction frameworks. Despite recent progress invideo understanding, existing methods predominantly address eithercoarse-grained action recognition or generic object tracking, therebyoverlooking the challenge of jointly detecting and tracking multiple objectsaccording to their actions while grounding them temporally. To address thisgap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel taskthat requires models to simultaneously detect, track, and temporally localizeall referent objects in videos based on natural language descriptions of theiractions. To support this task, we construct SVAG-Bench, a large-scale benchmarkcomprising 688 videos, 19,590 annotated records, and 903 unique verbs, coveringa diverse range of objects, actions, and real-world scenes. We further proposeSVAGFormer, a baseline framework that adapts state of the art vision languagemodels for joint spatial and temporal grounding, and introduce SVAGEval, astandardized evaluation toolkit for fair and reproducible benchmarking.Empirical results show that existing models perform poorly on SVAG,particularly in dense or complex scenes, underscoring the need for moreadvanced reasoning over fine-grained object-action interactions in long videos.</description>
      <author>example@mail.com (Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl)</author>
      <guid isPermaLink="false">2510.13016v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets</title>
      <link>http://arxiv.org/abs/2510.13443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习的膝关节角度预测框架，使用轻量级注意力CNN-LSTM模型，仅需新受试者少量步态周期数据即可实现高精度预测。&lt;h4&gt;背景&lt;/h4&gt;肌电信号(EMG)广泛用于通过机器学习和深度学习预测身体关节角度，但现有方法面临实时应用性有限、测试条件不具代表性以及需要大量数据集等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅需少量新受试者数据即可预测膝关节角度的迁移学习框架，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用Georgia Tech、UCI和SMLE三个包含四个与膝关节运动相关EMG通道的数据集；开发轻量级基于注意力机制的CNN-LSTM模型，在Georgia Tech数据集上预训练后迁移到其他数据集；仅使用EMG输入，以及结合历史膝关节角度和多种传感器输入进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用EMG输入时，模型在异常受试者的一步和50步预测中NMAE分别为6.8%和13.7%；结合历史膝关节角度后，正常受试者NMAE降至3.1%和3.5%，异常受试者降至2.8%和7.5%；当使用EMG、运动学和相互作用力多种输入时，模型在一步和50步预测中NMAE分别达到1.09%和3.1%。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习框架在短期和长期康复场景中均表现出稳健的性能和强大的泛化能力，仅需少量新受试者数据即可实现高精度膝关节角度预测。&lt;h4&gt;翻译&lt;/h4&gt;肌电(EMG)信号被广泛用于通过机器学习(ML)和深度学习(DL)方法预测身体关节角度。然而，这些方法通常面临实时应用性有限、测试条件不具代表性以及需要大量数据集才能实现最佳性能等挑战。本文提出了一个膝关节角度预测的迁移学习框架，只需要新受试者几个步态周期的数据。利用了三个数据集——Georgia Tech、加州大学欧文分校(UCI)和Sharif机械实验室外骨骼(SMLE)，这些数据集包含四个与膝关节运动相关的EMG通道。开发了一个轻量级的基于注意力机制的CNN-LSTM模型，在Georgia Tech数据集上进行预训练，然后转移到UCI和SMLE数据集。所提出的模型仅使用EMG输入，在异常受试者的一步和50步预测中实现了6.8%和13.7%的归一化平均绝对误差(NMAE)。结合历史膝关节角度将正常受试者的NMAE降低到3.1%和3.5%，异常受试者降低到2.8%和7.5%。当进一步适应SMLE外骨骼，使用EMG、运动学和相互作用力输入时，模型在一步和50步预测中分别实现了1.09%和3.1%的NMAE。这些结果表明模型在短期和长期康复场景中都具有稳健的性能和强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electromyography (EMG) signals are widely used for predicting body jointangles through machine learning (ML) and deep learning (DL) methods. However,these approaches often face challenges such as limited real-time applicability,non-representative test conditions, and the need for large datasets to achieveoptimal performance. This paper presents a transfer-learning framework for kneejoint angle prediction that requires only a few gait cycles from new subjects.Three datasets - Georgia Tech, the University of California Irvine (UCI), andthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channelsrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTMmodel was developed and pre-trained on the Georgia Tech dataset, thentransferred to the UCI and SMLE datasets. The proposed model achievedNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent forone-step and 50-step predictions on abnormal subjects using EMG inputs alone.Incorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5percent for normal subjects, and to 2.8 percent and 7.5 percent for abnormalsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, andinteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAEfor one- and 50-step predictions, respectively. These results demonstraterobust performance and strong generalization for both short- and long-termrehabilitation scenarios.</description>
      <author>example@mail.com (Mojtaba Mollahossein, Gholamreza Vossoughi, Mohammad Hossein Rohban)</author>
      <guid isPermaLink="false">2510.13443v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment</title>
      <link>http://arxiv.org/abs/2510.13023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 6 page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端的机器学习工作流程，用于解决自动化超声波焊接检测中的数据有限和环境波动问题，通过结合降阶建模、扩散分布对齐和U-Net分割反演技术，实现了真实工业环境下的焊接缺陷检测。&lt;h4&gt;背景&lt;/h4&gt;自动化超声波焊接检测在无损评估领域面临两大挑战：训练数据有限（由于实验标本整理或高保真模拟的复杂性）和工业环境的环境波动性（导致实时测量数据损坏）。&lt;h4&gt;目的&lt;/h4&gt;开发一种在真实工业环境中进行声学焊接检测的端到端机器学习工作流程，克服数据整理和信号损坏问题。&lt;h4&gt;方法&lt;/h4&gt;提出的工作流程包括：1)基于Lamb波理论的降阶Helmholtz模型生成综合数据集；2)使用相对廉价的低阶解为反演模型提供训练数据，并通过迁移学习优化；3)利用引导扩散处理分布外实验LDV扫描数据，生成分布内表示供反演模型处理。&lt;h4&gt;主要发现&lt;/h4&gt;降阶模型能够生成全面的焊接异质性和裂纹缺陷数据集；迁移学习可有效利用有限的全3D弹性动力学模拟；扩散模型能够处理具有不可预测噪声分布的真实世界测量数据。&lt;h4&gt;结论&lt;/h4&gt;该集成框架为真实数据上的自动化焊接检测提供了有效的端到端解决方案，克服了传统方法在数据有限和环境波动情况下的局限性。&lt;h4&gt;翻译&lt;/h4&gt;自动化超声波焊接检测在无损评估领域仍是一个重大挑战，原因包括训练数据有限（由于整理实验标本或高保真模拟的复杂性）和许多工业环境的环境波动性（导致实时测量数据损坏）。因此，在真实（即工业）环境中进行声学焊接检测的端到端机器学习工作流程一直是一个难以实现的目标。本文通过提出包含降阶建模方案、基于扩散的分布对齐以及基于U-Net的分割和反演的工作流程，解决了数据整理和信号损坏的挑战。使用基于Lamb波理论的降阶Helmholtz模型，在变化的焊接异质性和裂纹缺陷上生成综合数据集。相对廉价的低阶解为反演模型提供了强大的训练数据集，这些模型通过使用有限的全3D弹性动力学模拟集的迁移学习阶段进行优化。为了处理具有变化且不可预测的噪声分布的分布外真实世界测量（即激光多普勒测振仪扫描），引导扩散生成OOD实验LDV扫描的分布内表示，随后由反演模型处理。这种集成框架为真实数据上的自动化焊接检测提供了端到端解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated ultrasonic weld inspection remains a significant challenge in thenondestructive evaluation (NDE) community to factors such as limited trainingdata (due to the complexity of curating experimental specimens or high-fidelitysimulations) and environmental volatility of many industrial settings(resulting in the corruption of on-the-fly measurements). Thus, an end-to-endmachine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,industrial) settings has remained an elusive goal. This work addresses thechallenges of data curation and signal corruption by proposing workflowconsisting of a reduced-order modeling scheme, diffusion based distributionalignment, and U-Net-based segmentation and inversion. A reduced-orderHelmholtz model based on Lamb wave theory is used to generate a comprehensivedataset over varying weld heterogeneity and crack defects. The relativelyinexpensive low-order solutions provide a robust training dateset for inversionmodels which are refined through a transfer learning stage using a limited setof full 3D elastodynamic simulations. To handle out-of-distribution (OOD)real-world measurements with varying and unpredictable noise distributions,i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distributionrepresentations of OOD experimental LDV scans which are subsequently processedby the inversion models. This integrated framework provides an end-to-endsolution for automated weld inspection on real data.</description>
      <author>example@mail.com (Joshua R. Tempelman, Adam J. Wachtor, Eric B. Flynn)</author>
      <guid isPermaLink="false">2510.13023v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.13809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://sihuiji.github.io/PhysMaster-Page/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysMaster是一个通过物理知识表示指导视频生成模型的框架，利用输入图像中的物理先验信息，通过强化学习和人类反馈优化物理表示，能够生成物理上更合理的视频。&lt;h4&gt;背景&lt;/h4&gt;当前视频生成模型虽能生成视觉逼真的视频，但常常不遵守物理定律，限制了其生成物理合理视频的能力，使其无法成为有效的'世界模型'。&lt;h4&gt;目的&lt;/h4&gt;提出PhysMaster模型，通过捕获物理知识作为表示来指导视频生成模型，增强其物理感知能力，使其能够生成物理上合理的视频。&lt;h4&gt;方法&lt;/h4&gt;PhysMaster基于图像到视频任务，设计PhysEncoder从输入图像编码物理信息作为额外条件；采用强化学习与人类反馈相结合的方法，使用直接偏好优化(DPO)以端到端方式优化物理表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;PhysMaster为提高PhysEncoder的物理感知能力提供了可行解决方案，在简单代理任务上证明了其能力，并展现出广泛物理场景的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;PhysMaster通过在强化学习范式中通过表示学习统一解决各种物理过程，可作为物理感知视频生成的通用即插即用解决方案，具有更广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;当今的视频生成模型能够生成视觉上逼真的视频，但常常不遵守物理定律，限制了它们生成物理上合理的视频的能力，使其无法成为'世界模型'。为解决这一问题，我们提出了PhysMaster，它将物理知识捕获为一种表示，用于指导视频生成模型增强其物理感知能力。具体而言，PhysMaster基于图像到视频任务，模型需要从输入图像预测物理上合理的动态。由于输入图像提供了物理先验信息，如场景中物体的相对位置和潜在交互，我们设计了PhysEncoder从中编码物理信息作为额外条件，将物理知识注入视频生成过程。除了外观之外，模型物理性能缺乏适当的监督，这促使PhysEncoder应用强化学习与人类反馈相结合的方法进行物理表示学习，利用生成模型的反馈通过直接偏好优化(DPO)以端到端方式优化物理表示。PhysMaster为提高PhysEncoder的物理感知能力提供了可行解决方案，从而提高了视频生成的物理合理性，在简单代理任务上证明了其能力，并具有广泛物理场景的泛化能力。这意味着我们的PhysMaster通过在强化学习范式中通过表示学习统一解决各种物理过程，可以作为物理感知视频生成和更广泛应用的通用即插即用解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视频生成模型不遵守物理规律的问题。这个问题很重要，因为物理真实性是视频生成模型能否作为'世界模型'的关键，限制了它们在模拟真实世界场景、预测物理交互等应用场景中的实用性，也阻碍了视频生成模型从内容创作者向世界模拟器的转变。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了视频生成模型在物理规律遵循方面的两个主要挑战：MSE损失关注外观拟合而非物理理解，以及生成模型难以从图像中提取物理知识。他们提出学习物理表示作为桥梁，借鉴了基于物理仿真和无仿真方法的思路，但避免了它们的局限性。同时采用了大型语言模型中的RLHF框架和DPO训练方法，设计了三阶段训练pipeline：SFT微调基础模型和PhysEncoder，然后两阶段DPO分别优化DiT模型和PhysEncoder，利用生成反馈改进物理表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习物理表示作为物理知识和视频生成之间的桥梁，通过PhysEncoder从输入图像提取物理特征作为额外条件指导视频生成。整体流程包括：1)基于DiT的扩散模型架构，结合3D VAE和T5编码器；2)PhysEncoder基于DINOv2编码器和物理头部设计；3)三阶段训练pipeline - SFT阶段同时训练DiT和PhysEncoder，DPO阶段先优化DiT模型再优化PhysEncoder；4)从'自由落体'代理任务开始，验证后扩展到一般开放世界场景；5)使用PisaBench和VIDEOPHY等评估方法验证效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)物理表示学习作为物理知识和视频生成的桥梁；2)自上而下的优化策略，基于最终视频的物理合理性优化物理编码器；3)三阶段训练pipeline结合SFT和DPO；4)插件式物理知识注入实现通用物理属性学习；5)从特定任务到开放世界场景的泛化能力。相比之前工作，PhysMaster不依赖特定物理仿真引擎，能处理更广泛物理现象；不依赖大规模物理数据集或昂贵人工注释；专注于优化物理编码器而非整个模型；效率更高，生成5秒视频仅需26秒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PhysMaster通过学习物理表示并作为插件注入视频生成模型，利用强化学习优化物理编码器，显著提升了视频生成模型的物理合理性，使其能够从内容创作者转变为遵循物理规律的世界模拟器。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation models nowadays are capable of generating visually realisticvideos, but often fail to adhere to physical laws, limiting their ability togenerate physically plausible videos and serve as ''world models''. To addressthis issue, we propose PhysMaster, which captures physical knowledge as arepresentation for guiding video generation models to enhance theirphysics-awareness. Specifically, PhysMaster is based on the image-to-video taskwhere the model is expected to predict physically plausible dynamics from theinput image. Since the input image provides physical priors like relativepositions and potential interactions of objects in the scenario, we devisePhysEncoder to encode physical information from it as an extra condition toinject physical knowledge into the video generation process. The lack of propersupervision on the model's physical performance beyond mere appearancemotivates PhysEncoder to apply reinforcement learning with human feedback tophysical representation learning, which leverages feedback from generationmodels to optimize physical representations with Direct Preference Optimization(DPO) in an end-to-end manner. PhysMaster provides a feasible solution forimproving physics-awareness of PhysEncoder and thus of video generation,proving its ability on a simple proxy task and generalizability to wide-rangingphysical scenarios. This implies that our PhysMaster, which unifies solutionsfor various physical processes via representation learning in the reinforcementlearning paradigm, can act as a generic and plug-in solution for physics-awarevideo generation and broader applications.</description>
      <author>example@mail.com (Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2510.13809v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</title>
      <link>http://arxiv.org/abs/2510.13515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为UniME-V2的新型通用多模态嵌入模型，通过利用大型多模态语言模型的先进理解能力来增强表示学习。该方法通过MLLM-as-a-Judge机制评估语义对齐，生成软语义匹配分数，用于高质量困难负样本挖掘和模型优化，显著提升了模型的判别能力，并在多个检索任务上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的通用多模态嵌入模型通常采用批内负样本挖掘方法测量查询-候选对的相似性，但这些方法存在几个局限：难以捕捉候选者之间的细微语义差异，负样本缺乏多样性，以及在区分错误负样本和困难负样本方面的判别能力有限。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决现有多模态嵌入模型的局限性，提高其捕捉细微语义差异的能力，增加负样本的多样性，并增强模型对困难负样本的判别能力，从而提升通用多模态嵌入模型的整体性能。&lt;h4&gt;方法&lt;/h4&gt;研究团队提出了一种名为UniME-V2的新型通用多模态嵌入模型，主要方法包括：1) 通过全局检索构建潜在困难负样本集；2) 引入MLLM-as-a-Judge机制，利用大型多模态语言模型评估查询-候选对的语义对齐并生成软语义匹配分数；3) 将这些分数作为困难负样本挖掘的基础，减轻错误负样本影响，识别多样化高质量困难负样本；4) 将语义匹配分数作为软标签，缓解一对一映射约束；5) 通过对齐相似度矩阵与软语义匹配分数矩阵，学习候选者间的语义区别；6) 提出UniME-V2-Reranker重排序模型，通过联合成对和列表级优化方法训练。&lt;h4&gt;主要发现&lt;/h4&gt;在MMEB基准和多个检索任务上的全面实验表明，该方法在所有任务上平均达到了最先进的性能，证明了所提出方法的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;通过利用大型多模态语言模型的先进理解能力和创新的负样本挖掘方法，UniME-V2模型显著提高了通用多模态嵌入模型的性能，特别是在捕捉细微语义差异和区分困难负样本方面，为多模态表示学习领域提供了新的思路和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;通用多模态嵌入模型是各种任务的基础。现有方法通常采用批内负样本挖掘来测量查询-候选对的相似性。然而，这些方法往往难以捕捉候选者之间的细微语义差异，且负样本缺乏多样性。此外，嵌入模型在区分错误负样本和困难负样本方面的判别能力有限。在本文中，我们利用大型多模态语言模型的先进理解能力来增强表示学习，并提出了一种新颖的通用多模态嵌入模型。我们的方法首先通过全局检索构建潜在的困难负样本集。然后我们引入MLLM-as-a-Judge机制，利用大型多模态语言模型评估查询-候选对的语义对齐情况，并生成软语义匹配分数。这些分数作为困难负样本挖掘的基础，减轻了错误负样本的影响，并能够识别出多样化的高质量困难负样本。此外，语义匹配分数还被用作软标签，以缓解严格的一对一映射约束。通过将相似度矩阵与软语义匹配分数矩阵对齐，模型能够学习候选者之间的语义区别，显著提高其判别能力。为了进一步提高性能，我们提出了UniME-V2-Reranker，这是一个通过联合成对和列表级优化方法在我们挖掘的困难负样本上训练的重排序模型。我们在MMEB基准和多个检索任务上进行了全面实验，证明我们的方法在所有任务上平均达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal multimodal embedding models are foundational to various tasks.Existing approaches typically employ in-batch negative mining by measuring thesimilarity of query-candidate pairs. However, these methods often struggle tocapture subtle semantic differences among candidates and lack diversity innegative samples. Moreover, the embeddings exhibit limited discriminativeability in distinguishing false and hard negatives. In this paper, we leveragethe advanced understanding capabilities of MLLMs to enhance representationlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.Our approach first constructs a potential hard negative set through globalretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizesMLLMs to assess the semantic alignment of query-candidate pairs and generatesoft semantic matching scores. These scores serve as a foundation for hardnegative mining, mitigating the impact of false negatives and enabling theidentification of diverse, high-quality hard negatives. Furthermore, thesemantic matching scores are used as soft labels to mitigate the rigidone-to-one mapping constraint. By aligning the similarity matrix with the softsemantic matching score matrix, the model learns semantic distinctions amongcandidates, significantly enhancing its discriminative capacity. To furtherimprove performance, we propose UniME-V2-Reranker, a reranking model trained onour mined hard negatives through a joint pairwise and listwise optimizationapproach. We conduct comprehensive experiments on the MMEB benchmark andmultiple retrieval tasks, demonstrating that our method achievesstate-of-the-art performance on average across all tasks.</description>
      <author>example@mail.com (Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing)</author>
      <guid isPermaLink="false">2510.13515v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.13497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 9 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于CLIP框架的多模态模型DistilCLIP-EEG，整合脑电图信号和文本描述进行癫痫检测，并通过知识蒸馏方法创建轻量级学生模型，在多个数据集上实现了超过97%的准确率。&lt;h4&gt;背景&lt;/h4&gt;癫痫是一种常见的神经系统疾病，特征是突然、短暂的大脑神经元过度活动，由异常放电引起。目前大多数癫痫检测的深度学习方法仅依赖单模态的脑电图信号，忽视了多模态信息的潜在优势。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的多模态模型DistilCLIP-EEG，基于CLIP框架，整合脑电图信号和文本描述，以捕捉癫痫发作的全面特征，并通过知识蒸馏方法提高效率和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于CLIP框架的多模态模型DistilCLIP-EEG，整合脑电图信号和文本描述。该模型包含基于Conformer架构的脑电图编码器作为文本编码器，以及可学习BERT(BERT-LP)作为编码器内的提示学习。两者在共享的潜在空间中运行，实现有效的跨模态表示学习。同时引入知识蒸馏方法，训练好的DistilCLIP-EEG作为教师模型，指导一个更紧凑的学生模型。&lt;h4&gt;主要发现&lt;/h4&gt;在TUSZ、AUBMC和CHB-MIT数据集上，教师模型和学生模型的准确率均超过97%。在所有数据集上，F1分数持续高于0.94，证明了所提出框架的鲁棒性和可靠性。学生模型的参数数量和模型大小约为教师模型的58.1%，显著降低了模型复杂性和存储需求，同时保持高性能。&lt;h4&gt;结论&lt;/h4&gt;该模型突显了在基于脑电图的癫痫检测中的潜力，并为在资源受限环境中部署轻量级模型奠定了坚实基础。&lt;h4&gt;翻译&lt;/h4&gt;癫痫是一种常见的神经系统疾病，特征是突然、短暂的大脑神经元过度活动 episodes，由异常放电引起，可能导致一些精神障碍。目前大多数用于癫痫检测的深度学习方法仅依赖单模态的脑电图(EEG)信号，忽视了多模态信息的潜在优势。为此，我们提出了一种新颖的多模态模型 DistilCLIP-EEG，基于CLIP框架，整合了脑电图信号和文本描述，以捕捉癫痫发作的全面特征。该模型包含基于Conformer架构的脑电图编码器作为文本编码器，以及我们提出的可学习BERT(BERT-LP)作为编码器内的提示学习。两者在共享的潜在空间中运行，实现有效的跨模态表示学习。为了提高效率和适应性，我们引入了一种知识蒸馏方法，其中训练好的DistilCLIP-EEG作为教师模型，指导一个更紧凑的学生模型，以降低训练复杂度和时间。在TUSZ、AUBMC和CHB-MIT数据集上，教师模型和学生模型的准确率均超过97%。在所有数据集上，F1分数持续高于0.94，证明了所提出框架的鲁棒性和可靠性。此外，学生模型的参数数量和模型大小约为教师模型的58.1%，显著降低了模型复杂性和存储需求，同时保持高性能。这些结果突显了我们提出的模型在基于脑电图的癫痫检测中的潜力，并为在资源受限环境中部署轻量级模型奠定了坚实基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/JBHI.2025.3603022&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Epilepsy is a prevalent neurological disorder marked by sudden, briefepisodes of excessive neuronal activity caused by abnormal electricaldischarges, which may lead to some mental disorders. Most existing deeplearning methods for epilepsy detection rely solely on unimodal EEG signals,neglecting the potential benefits of multimodal information. To address this,we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIPframework, which integrates both EEG signals and text descriptions to capturecomprehensive features of epileptic seizures. The model involves an EEG encoderbased on the Conformer architecture as a text encoder, the proposed LearnableBERT (BERT-LP) as prompt learning within the encoders. Both operate in a sharedlatent space for effective cross-modal representation learning. To enhanceefficiency and adaptability, we introduce a knowledge distillation method wherethe trained DistilCLIP-EEG serves as a teacher to guide a more compact studentmodel to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MITdatasets, both the teacher and student models achieved accuracy rates exceeding97%. Across all datasets, the F1-scores were consistently above 0.94,demonstrating the robustness and reliability of the proposed framework.Moreover, the student model's parameter count and model size are approximately58.1% of those of the teacher model, significantly reducing model complexityand storage requirements while maintaining high performance. These resultshighlight the potential of our proposed model for EEG-based epilepsy detectionand establish a solid foundation for deploying lightweight models inresource-constrained settings.</description>
      <author>example@mail.com (Zexin Wang, Lin Shi, Haoyu Wu, Junru Luo, Xiangzeng Kong, Jun Qi)</author>
      <guid isPermaLink="false">2510.13497v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Multi-Modal Diffusion Mamba</title>
      <link>http://arxiv.org/abs/2510.13253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MDM（多模态扩散Mamba）的新型架构，通过统一的变分自编码器实现多模态处理的统一，在多个任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;当前端到端多模态模型使用不同的编码器和解码器处理输入和输出信息，这种分离阻碍了不同模态的联合表示学习。&lt;h4&gt;目的&lt;/h4&gt;为了统一多模态处理，解决现有模型中不同模态处理分离的问题。&lt;h4&gt;方法&lt;/h4&gt;MDM利用基于Mamba的多步选择扩散模型，通过统一的变分自编码器逐步生成和优化模态特定信息。&lt;h4&gt;主要发现&lt;/h4&gt;在图像生成、图像描述、视觉问答、文本理解和推理任务等领域的评估表明，MDM显著优于现有的端到端模型（如MonoFormer、LlamaGen和Chameleon等），并能与GPT-4V、Gemini Pro和Mistral等最先进模型有效竞争。&lt;h4&gt;结论&lt;/h4&gt;研究结果验证了MDM在统一多模态处理的同时保持计算效率方面的有效性，为端到端多模态架构建立了新方向。&lt;h4&gt;翻译&lt;/h4&gt;当前端到端多模态模型使用不同的编码器和解码器来处理输入和输出信息。这种分离阻碍了不同模态的联合表示学习。为了统一多模态处理，我们提出了一种名为MDM（多模态扩散Mamba）的新型架构。MDM利用基于Mamba的多步选择扩散模型，通过统一的变分自编码器逐步生成和优化模态特定信息。这种创新方法使MDM在处理高维数据时能够实现卓越的性能，特别是在同时生成高分辨率图像和扩展文本序列方面。我们在图像生成、图像描述、视觉问答、文本理解和推理任务等领域的评估表明，MDM显著优于现有的端到端模型（MonoFormer、LlamaGen和Chameleon等），并能与GPT-4V、Gemini Pro和Mistral等最先进模型有效竞争。我们的结果验证了MDM在统一多模态处理的同时保持计算效率方面的有效性，为端到端多模态架构建立了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current end-to-end multi-modal models utilize different encoders and decodersto process input and output information. This separation hinders the jointrepresentation learning of various modalities. To unify multi-modal processing,we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDMutilizes a Mamba-based multi-step selection diffusion model to progressivelygenerate and refine modality-specific information through a unified variationalautoencoder for both encoding and decoding. This innovative approach allows MDMto achieve superior performance when processing high-dimensional data,particularly in generating high-resolution images and extended text sequencessimultaneously. Our evaluations in areas such as image generation, imagecaptioning, visual question answering, text comprehension, and reasoning tasksdemonstrate that MDM significantly outperforms existing end-to-end models(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTAmodels like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM'seffectiveness in unifying multi-modal processes while maintaining computationalefficiency, establishing a new direction for end-to-end multi-modalarchitectures.</description>
      <author>example@mail.com (Chunhao Lu, Qiang Lu, Meichen Dong, Jake Luo)</author>
      <guid isPermaLink="false">2510.13253v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>A Matter of Representation: Towards Graph-Based Abstract Code Generation</title>
      <link>http://arxiv.org/abs/2510.13163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于图的抽象代码生成，提出并评估了JSON表示方法，使大型语言模型能够高精度地执行此类任务。&lt;h4&gt;背景&lt;/h4&gt;大多数大型语言模型擅长生成原始顺序代码，但很少研究基于图的抽象代码生成，这种方法在可视化编程语言和原始源代码不可用的情况下很有价值。&lt;h4&gt;目的&lt;/h4&gt;提出并评估JSON表示方法，以实现高精度的基于图的抽象代码生成，并研究不同表示方法对生成准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;使用ScratchTest（基于Scratch Python重新实现的迷你基准测试）评估不同的JSON图表示方法，测试LLM在代码图空间中的表现。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型可以在单次通过中执行基于图的抽象代码生成任务，无需依赖专门或复杂的管道，且不同表示方法会导致显著不同的准确性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为基于图的抽象代码生成的表示学习奠定了基础，突显了适当表示方法的重要性。&lt;h4&gt;翻译&lt;/h4&gt;目前大多数大型语言模型擅长生成具有最小抽象和自定义结构的原始顺序代码。然而，很少有关于基于图的抽象代码生成的工作，其中重要逻辑被封装在预定义节点中，执行流程由边决定。这对于可视化编程语言，以及原始源代码对用户和LLM训练集不可用的情况相关。在这项工作中，我们提出并评估了用于图的JSON表示，以实现高精度的基于图的抽象代码生成。我们在ScratchTest上评估了这些表示，这是一个基于我们自定义的Scratch Python重新实现的迷你基准测试，用于测试LLM在代码图空间中的表现。我们的研究结果表明，LLM确实可以在单次通过中执行上述生成任务，而不依赖于专门的或复杂的管道，前提是使用正确的图表示。我们还表明，不同的表示会导致显著不同的准确性，突显了表示在此生成任务中的重要作用。总而言之，这项工作为基于图的抽象代码生成的表示学习建立了第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most large language models (LLMs) today excel at generating raw, sequentialcode with minimal abstractions and custom structures. However, there has beenlittle work on graph-based abstract code generation, where significant logic isencapsulated in predefined nodes and execution flow is determined by edges.This is relevant for visual programming languages, and in cases where rawsource code is inaccessible to users and LLM training sets. In this work, wepropose and evaluate JSON representations for graphs to enable high accuracygraph-based abstract code generation. We evaluate these representations onScratchTest, a mini-benchmark based on our custom Python re-implementation ofScratch, which tests the LLM in code graph space. Our findings demonstrate thatLLMs can indeed perform the aforementioned generation task in a single passwithout relying on specialized or complex pipelines, given the correct graphrepresentations. We also show that different representations inducesignificantly different accuracies, highlighting the instrumental role ofrepresentations in this generation task. All in all, this work establishes thefirst steps towards representation learning for graph-based abstract codegeneration.</description>
      <author>example@mail.com (Nyx Iskandar, Hisham Bedri, Andy Tsen)</author>
      <guid isPermaLink="false">2510.13163v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Information Shapes Koopman Representation</title>
      <link>http://arxiv.org/abs/2510.13025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过信息论视角重新思考Koopman学习，提出一种平衡表示简单性和表达性的新方法，解决了Koopman算子在深度架构中面临的子空间选择挑战。&lt;h4&gt;背景&lt;/h4&gt;Koopman算子为建模动力系统提供了强大框架，受到机器学习社区日益关注，但其无限维特性使得识别合适的有限维子空间具有挑战性，特别是在深度架构中。&lt;h4&gt;目的&lt;/h4&gt;解决Koopman学习中次优表示学习的问题，平衡潜在变量在表达性和简单性之间的权衡，克服信息瓶颈困境。&lt;h4&gt;方法&lt;/h4&gt;提出一种信息论拉格朗日公式化，明确平衡简单性和表达性的权衡；基于该公式开发新算法，促进潜在互信息（简单性）和冯·诺依曼熵（表达性）的共同优化。&lt;h4&gt;主要发现&lt;/h4&gt;潜在互信息促进简单性但过度强调可能导致潜在空间崩溃；冯·诺依曼熵维持表达性并防止崩溃，鼓励模式多样性；所提方法产生稳定且可解释的Koopman表示。&lt;h4&gt;结论&lt;/h4&gt;通过信息论视角重新审视Koopman学习，提出的新方法在多种动力系统上验证优于现有方法，实现了更好的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;Koopman算子为建模动力系统提供了强大框架，并吸引了机器学习界的日益关注。然而，其无限维特性使得识别合适的有限维子空间具有挑战性，特别是对于深度架构。我们认为这些困难来自于次优的表示学习，其中潜在变量无法平衡表达性和简单性。这种张力与信息瓶颈(IB)困境密切相关：构建既紧凑又有预测能力的压缩表示。通过这一视角重新思考Koopman学习，我们证明潜在互信息促进简单性，但过度强调简单性可能导致潜在空间崩溃到少数主导模式。相比之下，表达性由冯·诺依曼熵维持，防止这种崩溃并鼓励模式多样性。这一见解促使我们提出一种明确平衡这种权衡的信息论拉格朗日公式化。此外，我们基于该公式提出新算法，鼓励简单性和表达性，产生稳定且可解释的Koopman表示。除了定量评估外，我们还可视化了在我们表示下学习到的流形，观察到与理论预测一致的实证结果。最后，我们在多种动力系统上验证了我们的方法，展示了与现有Koopman学习方法相比的改进性能。实现已在https://github.com/Wenxuan52/InformationKoopman公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Koopman operator provides a powerful framework for modeling dynamicalsystems and has attracted growing interest from the machine learning community.However, its infinite-dimensional nature makes identifying suitablefinite-dimensional subspaces challenging, especially for deep architectures. Weargue that these difficulties come from suboptimal representation learning,where latent variables fail to balance expressivity and simplicity. Thistension is closely related to the information bottleneck (IB) dilemma:constructing compressed representations that are both compact and predictive.Rethinking Koopman learning through this lens, we demonstrate that latentmutual information promotes simplicity, yet an overemphasis on simplicity maycause latent space to collapse onto a few dominant modes. In contrast,expressiveness is sustained by the von Neumann entropy, which prevents suchcollapse and encourages mode diversity. This insight leads us to propose aninformation-theoretic Lagrangian formulation that explicitly balances thistradeoff. Furthermore, we propose a new algorithm based on the Lagrangianformulation that encourages both simplicity and expressiveness, leading to astable and interpretable Koopman representation. Beyond quantitativeevaluations, we further visualize the learned manifolds under ourrepresentations, observing empirical results consistent with our theoreticalpredictions. Finally, we validate our approach across a diverse range ofdynamical systems, demonstrating improved performance over existing Koopmanlearning methods. The implementation is publicly available athttps://github.com/Wenxuan52/InformationKoopman.</description>
      <author>example@mail.com (Xiaoyuan Cheng, Wenxuan Yuan, Yiming Yang, Yuanzhao Zhang, Sibo Cheng, Yi He, Zhuo Sun)</author>
      <guid isPermaLink="false">2510.13025v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning</title>
      <link>http://arxiv.org/abs/2510.12957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的多模态可解释AI框架，通过注意力增强特征融合、Grad-CAM++局部解释和Reveal-to-Revise反馈循环解决偏差检测和减轻问题，在多模态MNIST上实现了高准确率和解释保真度。&lt;h4&gt;背景&lt;/h4&gt;标准基准数据集如MNIST无法揭示潜在的偏差和多模态特征复杂性，限制了深度神经网络在高风险应用中的可信度。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态可解释AI框架，实现偏差检测和减轻，提高AI系统的透明度和可信度。&lt;h4&gt;方法&lt;/h4&gt;统一了注意力增强的特征融合、基于Grad-CAM++的局部解释以及Reveal-to-Revise反馈循环，形成一个完整的偏差检测和减轻框架。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态扩展的MNIST上实现了93.2%的分类准确率、91.6%的F1分数和78.1%的解释保真度，优于单模态和不可解释的基线方法；消融研究表明可解释性与偏差感知学习的结合增强了模型的鲁棒性和人类对齐。&lt;h4&gt;结论&lt;/h4&gt;该工作弥合了性能、透明度和公平性之间的差距，为敏感领域可信AI的实际应用提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;标准基准数据集如MNIST往往无法揭示潜在的偏差和多模态特征复杂性，限制了深度神经网络在高风险应用中的可信度。我们提出了一种新颖的多模态可解释AI(XAI)框架，统一了注意力增强的特征融合、基于Grad-CAM++的局部解释以及Reveal-to-Revise反馈循环，用于偏差检测和减轻。在多模态扩展的MNIST上评估，我们的方法实现了93.2%的分类准确率、91.6%的F1分数和78.1%的解释保真度，优于单模态和不可解释的基线。消融研究表明，将可解释性与偏差感知学习相结合可以增强鲁棒性和人类对齐。我们的工作弥合了性能、透明度和公平性之间的差距，突显了敏感领域可信AI的实际应用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Standard benchmark datasets, such as MNIST, often fail to expose latentbiases and multimodal feature complexities, limiting the trustworthiness ofdeep neural networks in high-stakes applications. We propose a novel multimodalExplainable AI (XAI) framework that unifies attention-augmented feature fusion,Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop forbias detection and mitigation. Evaluated on multimodal extensions of MNIST, ourapproach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1%explanation fidelity (IoU-XAI), outperforming unimodal and non-explainablebaselines. Ablation studies demonstrate that integrating interpretability withbias-aware learning enhances robustness and human alignment. Our work bridgesthe gap between performance, transparency, and fairness, highlighting apractical pathway for trustworthy AI in sensitive domains.</description>
      <author>example@mail.com (Noor Islam S. Mohammad)</author>
      <guid isPermaLink="false">2510.12957v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment</title>
      <link>http://arxiv.org/abs/2510.12927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedGTEA框架，用于联邦增量学习，通过高斯任务嵌入和实现对任务特定知识和模型不确定性的高效捕捉，具有可扩展性和通信效率优势。&lt;h4&gt;背景&lt;/h4&gt;联邦增量学习领域需要有效捕捉任务特定知识和模型不确定性，同时确保可扩展性和通信效率。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够捕捉任务特定知识和模型不确定性，同时保持可扩展性和通信效率的联邦学习框架。&lt;h4&gt;方法&lt;/h4&gt;客户端使用Cardinality-Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入，编码任务知识并解决统计异构性；服务器端利用2-Wasserstein距离衡量任务间差距，通过Wasserstein损失强制任务间分离，同时保护任务级隐私。&lt;h4&gt;主要发现&lt;/h4&gt;在多个流行数据集上的实证评估显示，FedGTEA实现了卓越的分类性能，显著减轻了遗忘问题，持续优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;FedGTEA框架在联邦增量学习任务中表现优异，能够有效处理任务特定知识、模型不确定性，同时保持可扩展性和通信效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种联邦增量学习的新框架，称为联邦高斯任务嵌入与对齐（FedGTEA）。FedGTEA旨在以可扩展且通信高效的方式捕捉任务特定知识和模型不确定性。在客户端，Cardinality-Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入，这些嵌入编码任务知识，解决统计异构性问题，并量化数据不确定性。重要的是，CATE保持固定参数大小，无论任务数量如何，这确保了长任务序列的可扩展性。在服务器端，FedGTEA利用2-Wasserstein距离来衡量高斯嵌入之间的任务间差距。我们制定Wasserstein损失以强制实现任务间分离。这种概率性表述不仅增强了表示学习，还通过避免直接传输潜在嵌入来保护任务级隐私，符合联邦学习中的隐私约束。在流行数据集上的大量实证评估表明，FedGTEA实现了卓越的分类性能，显著减轻了遗忘问题，持续优于现有的强大基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel framework for Federated Class Incremental Learning,called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA isdesigned to capture task-specific knowledge and model uncertainty in a scalableand communication-efficient manner. At the client side, theCardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed taskembeddings that encode task knowledge, address statistical heterogeneity, andquantify data uncertainty. Importantly, CATE maintains a fixed parameter sizeregardless of the number of tasks, which ensures scalability across long tasksequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance tomeasure inter-task gaps between Gaussian embeddings. We formulate theWasserstein loss to enforce inter-task separation. This probabilisticformulation not only enhances representation learning but also preservestask-level privacy by avoiding the direct transmission of latent embeddings,aligning with the privacy constraints in federated learning. Extensiveempirical evaluations on popular datasets demonstrate that FedGTEA achievessuperior classification performance and significantly mitigates forgetting,consistently outperforming strong existing baselines.</description>
      <author>example@mail.com (Haolin Li, Hoda Bidkhori)</author>
      <guid isPermaLink="false">2510.12927v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model</title>
      <link>http://arxiv.org/abs/2510.09764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ProtoMM，一种新的自监督学习框架，用于多模态时间序列数据建模，特别是在生物信号处理方面。该方法通过引入共享原型字典，将不同模态锚定在共同的嵌入空间中，克服了现有对比学习方法的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态时间序列数据建模对于捕捉系统级动态至关重要，特别是在生物信号领域，如ECG、PPG、EDA和加速度计等多种模态提供了关于相互关联生理过程的互补视角。尽管最近的自监督学习(SSL)进展已经改善了单模态表征学习，但现有的多模态方法往往依赖于CLIP风格的对比目标函数，这些方法容易过拟合到容易对齐的特征，并将有效的跨模态关系误分类为负样本，导致碎片化和不可泛化的嵌入。&lt;h4&gt;目的&lt;/h4&gt;克服现有多模态自监督学习方法中的局限性，特别是对比学习方法中存在的问题，如过拟合和对有效跨模态关系的错误分类。&lt;h4&gt;方法&lt;/h4&gt;提出ProtoMM，一种新的自监督学习框架，引入共享原型字典来将异构模态锚定在共同的嵌入空间中。通过围绕共享原型聚类表征，而不是显式的负采样，该方法能够捕获模态间的互补信息，并为生理信号提供连贯的'通用语言'。&lt;h4&gt;主要发现&lt;/h4&gt;1. 使用ProtoMM开发的Pulse Motion基础模型优于仅使用对比方法和先前的多模态SSL方法；2. 该方法实现了最先进的性能；3. 提供了更好的学习特征可解释性。&lt;h4&gt;结论&lt;/h4&gt;ProtoMM框架有效解决了多模态生物信号处理中的关键挑战，通过共享原型字典方法，不仅提高了性能，还增强了特征的可解释性，为生理信号处理提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;对多模态时间序列数据进行建模对于捕捉系统级动态至关重要，特别是在生物信号领域，如ECG、PPG、EDA和加速度计等多种模态提供了关于相互关联生理过程的互补视角。尽管最近的自监督学习(SSL)进展已经改善了单模态表征学习，但现有的多模态方法往往依赖于CLIP风格的对比目标函数，这些方法容易过拟合到容易对齐的特征，并将有效的跨模态关系误分类为负样本，导致碎片化和不可泛化的嵌入。为了克服这些局限性，我们提出了ProtoMM，一种新的SSL框架，引入共享原型字典将异构模态锚定在共同的嵌入空间中。通过围绕共享原型聚类表征而不是显式的负采样，我们的方法捕获了模态间的互补信息，并为生理信号提供了连贯的'通用语言'。在这项工作中，我们专注于使用ProtoMM开发Pulse Motion基础模型，并证明我们的方法优于仅使用对比方法和先前的多模态SSL方法，在实现最先进性能的同时，提供了更好的学习特征可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling multi-modal time-series data is critical for capturing system-leveldynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,and accelerometry provide complementary perspectives on interconnectedphysiological processes. While recent self-supervised learning (SSL) advanceshave improved unimodal representation learning, existing multi-modal approachesoften rely on CLIP-style contrastive objectives that overfit to easily alignedfeatures and misclassify valid cross-modal relationships as negatives,resulting in fragmented and non-generalizable embeddings. To overcome theselimitations, we propose ProtoMM, a novel SSL framework that introduces a sharedprototype dictionary to anchor heterogeneous modalities in a common embeddingspace. By clustering representations around shared prototypes rather thanexplicit negative sampling, our method captures complementary informationacross modalities and provides a coherent "common language" for physiologicalsignals. In this work, we focus on developing a Pulse Motion foundation modelwith ProtoMM and demonstrate that our approach outperforms contrastive-only andprior multimodal SSL methods, achieving state-of-the-art performance whileoffering improved interpretability of learned features.</description>
      <author>example@mail.com (Wanting Mao, Maxwell A Xu, Harish Haresamudram, Mithun Saha, Santosh Kumar, James Matthew Rehg)</author>
      <guid isPermaLink="false">2510.09764v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
  <item>
      <title>VideoLucy: Deep Memory Backtracking for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.12422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS-2025 Accepted Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoLucy是一种深度记忆回溯框架，用于解决长视频理解中的时序上下文捕捉和关键信息保留问题，通过分层记忆结构和智能体迭代回溯机制实现优越性能。&lt;h4&gt;背景&lt;/h4&gt;基于智能体的系统利用大型语言模型进行关键信息检索和整合已成为长视频理解的有前景的方法，但面临两大挑战：难以捕捉连续帧的时序上下文，以及稀疏帧采样可能导致丢弃关键信息。&lt;h4&gt;目的&lt;/h4&gt;克服现有长视频理解系统的局限性，提出VideoLucy框架以有效捕捉时序上下文并保留关键信息。&lt;h4&gt;方法&lt;/h4&gt;VideoLucy采用受人类从粗到细回忆过程启发的分层记忆结构，具有渐进粒度，在不同层次深度明确定义记忆的细节级别和时序范围。通过基于智能体的迭代回溯机制，系统性地挖掘视频范围内、问题相关的深度记忆，直到收集到足够信息提供自信答案。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明VideoLucy的优越性，基于开源模型构建的VideoLucy在多个长视频理解基准上显著优于最先进方法，性能甚至超过了最新的专有模型如GPT-4o。&lt;h4&gt;结论&lt;/h4&gt;VideoLucy通过创新的深度记忆回溯框架有效解决了长视频理解中的关键挑战，同时引入EgoMem基准用于全面评估模型能力，代码和数据集将公开。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，利用大型语言模型进行关键信息检索和整合的智能体系统已成为长视频理解的一种有前景的方法。然而，这些系统面临两大挑战。首先，它们通常在单帧上进行建模和推理，难以捕捉连续帧的时序上下文。其次，为降低密集帧级标注的成本，它们采用稀疏帧采样，这可能会丢弃关键信息。为克服这些局限性，我们提出了VideoLucy，一种用于长视频理解的深度记忆回溯框架。受人类从粗到细回忆过程的启发，VideoLucy采用具有渐进粒度的分层记忆结构。该结构在不同层次深度明确定义了记忆的细节级别和时序范围。通过基于智能体的迭代回溯机制，VideoLucy系统性地挖掘视频范围内、问题相关的深度记忆，直到收集到足够信息以提供自信的答案。这种设计能够有效理解连续帧的时序关系，同时保留关键细节。此外，我们引入了EgoMem，一个用于长视频理解的新基准。EgoMem旨在全面评估模型理解随时间展开的复杂事件和捕捉极长视频中细粒度细节的能力。大量实验证明了VideoLucy的优越性。基于开源模型构建的VideoLucy在多个长视频理解基准上显著优于最先进的方法，性能甚至超过了最新的专有模型如GPT-4o。我们的代码和数据集将在https://videolucy.github.io公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have shown that agent-based systems leveraging large languagemodels (LLMs) for key information retrieval and integration have emerged as apromising approach for long video understanding. However, these systems facetwo major challenges. First, they typically perform modeling and reasoning onindividual frames, struggling to capture the temporal context of consecutiveframes. Second, to reduce the cost of dense frame-level captioning, they adoptsparse frame sampling, which risks discarding crucial information. To overcomethese limitations, we propose VideoLucy, a deep memory backtracking frameworkfor long video understanding. Inspired by the human recollection process fromcoarse to fine, VideoLucy employs a hierarchical memory structure withprogressive granularity. This structure explicitly defines the detail level andtemporal scope of memory at different hierarchical depths. Through anagent-based iterative backtracking mechanism, VideoLucy systematically minesvideo-wide, question-relevant deep memories until sufficient information isgathered to provide a confident answer. This design enables effective temporalunderstanding of consecutive frames while preserving critical details. Inaddition, we introduce EgoMem, a new benchmark for long video understanding.EgoMem is designed to comprehensively evaluate a model's ability to understandcomplex events that unfold over time and capture fine-grained details inextremely long videos. Extensive experiments demonstrate the superiority ofVideoLucy. Built on open-source models, VideoLucy significantly outperformsstate-of-the-art methods on multiple long video understanding benchmarks,achieving performance even surpassing the latest proprietary models such asGPT-4o. Our code and dataset will be made publicly athttps://videolucy.github.io</description>
      <author>example@mail.com (Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao)</author>
      <guid isPermaLink="false">2510.12422v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling</title>
      <link>http://arxiv.org/abs/2510.12385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 7 figures and 5 tables in the main paper and one figure and  table in the appendix. To be published in Computer Vision and Image  Understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了STORM-PSR模型，一种双流框架的程序步骤识别方法，通过结合空间和时间特征，有效解决了物体部分遮挡情况下的步骤识别问题。&lt;h4&gt;背景&lt;/h4&gt;现有的程序步骤识别模型仅依靠检测单个视频帧中的装配对象状态，忽略了时间特征，导致模型鲁棒性和准确性有限，尤其在物体部分遮挡时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一种能够处理部分遮挡情况下的程序步骤识别方法，提高识别准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出STORM-PSR（时空遮挡弹性建模程序步骤识别）双流框架：装配状态检测流在物体无遮挡时有效工作；时空流捕捉空间和时间特征，即使在部分遮挡下也能识别步骤完成情况。时空流包含使用弱监督方法预训练的空间编码器和基于transformer的时间编码器。&lt;h4&gt;主要发现&lt;/h4&gt;在MECCANO和IndustReal数据集上评估，与之前方法相比，分别减少了11.2%和26.1%的实际和预测装配步骤完成之间的平均延迟。这种改进主要由时空流实现，它不依赖物体的无遮挡视图。&lt;h4&gt;结论&lt;/h4&gt;STORM-PSR能有效处理部分遮挡情况下的程序步骤识别问题，显著提高了识别准确性和及时性。相关代码和数据集已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;程序步骤识别旨在识别视频中程序任务中所有正确完成的步骤及其顺序。现有的最先进模型仅依靠检测单个视频帧中的装配对象状态。通过忽略时间特征，模型的鲁棒性和准确性受到限制，特别是当物体部分遮挡时。为克服这些限制，我们提出了用于程序步骤识别的时空遮挡弹性建模（STORM-PSR），这是一个用于程序步骤识别的双流框架，同时利用空间和时间特征。装配状态检测流在物体无遮挡视图下有效工作，而时空流捕捉空间和时间特征，即使在部分遮挡下也能识别步骤完成情况。该流包括一个空间编码器，使用新颖的弱监督方法预训练以捕获有意义的空间表示，以及一个基于transformer的时间编码器，学习这些空间特征随时间的关系。STORM-PSR在MECCANO和IndustReal数据集上进行了评估，与之前的方法相比，分别减少了11.2%和26.1%的实际和预测装配步骤完成之间的平均延迟。我们证明这种延迟减少是由时空流驱动的，它不依赖物体的无遮挡视图来推断完成的步骤。STORM-PSR的代码以及新标注的MECCANO标签已在https://timschoonbeek.github.io/stormpsr公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Procedure step recognition (PSR) aims to identify all correctly completedsteps and their sequential order in videos of procedural tasks. The existingstate-of-the-art models rely solely on detecting assembly object states inindividual video frames. By neglecting temporal features, model robustness andaccuracy are limited, especially when objects are partially occluded. Toovercome these limitations, we propose Spatio-Temporal Occlusion-ResilientModeling for Procedure Step Recognition (STORM-PSR), a dual-stream frameworkfor PSR that leverages both spatial and temporal features. The assembly statedetection stream operates effectively with unobstructed views of the object,while the spatio-temporal stream captures both spatial and temporal features torecognize step completions even under partial occlusion. This stream includes aspatial encoder, pre-trained using a novel weakly supervised approach tocapture meaningful spatial representations, and a transformer-based temporalencoder that learns how these spatial features relate over time. STORM-PSR isevaluated on the MECCANO and IndustReal datasets, reducing the average delaybetween actual and predicted assembly step completions by 11.2% and 26.1%,respectively, compared to prior methods. We demonstrate that this reduction indelay is driven by the spatio-temporal stream, which does not rely onunobstructed views of the object to infer completed steps. The code forSTORM-PSR, along with the newly annotated MECCANO labels, is made publiclyavailable at https://timschoonbeek.github.io/stormpsr .</description>
      <author>example@mail.com (Tim J. Schoonbeek, Shao-Hsuan Hung, Dan Lehman, Hans Onvlee, Jacek Kustra, Peter H. N. de With, Fons van der Sommen)</author>
      <guid isPermaLink="false">2510.12385v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding</title>
      <link>http://arxiv.org/abs/2510.12160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种状态空间提示方法，通过结合帧内和帧间提示来有效捕捉和传播视频中的时空信息，显著提升了视频分类性能。&lt;h4&gt;背景&lt;/h4&gt;预训练的状态空间模型在视频分类方面展现出巨大潜力，它们以线性复杂度顺序压缩视频中的视觉标记，提高处理效率的同时保持高性能。提示学习被用于将这些强大模型高效适配到下游任务。&lt;h4&gt;目的&lt;/h4&gt;解决顺序压缩的视觉提示标记无法充分捕捉视频时空上下文信息的问题，以增强状态压缩模型中空间信息在帧内的传播以及时间信息在帧间的提取。&lt;h4&gt;方法&lt;/h4&gt;提出状态空间提示方法，结合帧内和帧间提示聚合和传播视频中的关键时空信息。具体包括帧内聚合模块和帧间扩散模块，通过自适应平衡和压缩帧内及帧间的关键时空信息，以互补方式有效传播判别性信息。&lt;h4&gt;主要发现&lt;/h4&gt;在四个视频基准数据集上的实验表明，该方法平均比现有最先进方法高出2.76%，同时减少了微调参数的开销。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合帧内和帧间提示，有效解决了状态空间模型在视频理解中时空信息捕捉不足的问题，在保持高性能的同时提高了处理效率并减少了微调参数。&lt;h4&gt;翻译&lt;/h4&gt;最近，预训练的状态空间模型在视频分类方面显示出巨大潜力，它们以线性复杂度顺序压缩视频中的视觉标记，从而提高视频数据的处理效率同时保持高性能。为了将强大的预训练模型应用于下游任务，提示学习被提出，只需少量微调参数即可实现高效的下游任务适应。然而，顺序压缩的视觉提示标记无法捕捉视频中的空间和时间上下文信息，从而限制了状态压缩模型内空间信息在视频帧内的有效传播以及帧间时间信息的提取和判别性信息的提取。为解决上述问题，我们提出了一种用于视频理解的状态空间提示方法，该方法结合帧内和帧间提示来聚合和传播视频中的关键时空信息。具体来说，设计了帧内聚合模块来聚合每个帧内的空间关键信息。此外，设计了帧间扩散模块来传播不同帧间的判别性时空信息。通过自适应平衡和压缩帧内及帧间的关键时空信息，我们的方法以互补方式有效传播视频中的判别性信息。在四个视频基准数据集上的大量实验验证了我们的方法平均比现有最先进方法高出2.76%，同时减少了微调参数的开销。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, pre-trained state space models have shown great potential for videoclassification, which sequentially compresses visual tokens in videos withlinear complexity, thereby improving the processing efficiency of video datawhile maintaining high performance. To apply powerful pre-trained models todownstream tasks, prompt learning is proposed to achieve efficient downstreamtask adaptation with only a small number of fine-tuned parameters. However, thesequentially compressed visual prompt tokens fail to capture the spatial andtemporal contextual information in the video, thus limiting the effectivepropagation of spatial information within a video frame and temporalinformation between frames in the state compression model and the extraction ofdiscriminative information. To tackle the above issue, we proposed a StateSpace Prompting (SSP) method for video understanding, which combinesintra-frame and inter-frame prompts to aggregate and propagate keyspatiotemporal information in the video. Specifically, an Intra-Frame Gathering(IFG) module is designed to aggregate spatial key information within eachframe. Besides, an Inter-Frame Spreading (IFS) module is designed to spreaddiscriminative spatio-temporal information across different frames. Byadaptively balancing and compressing key spatio-temporal information within andbetween frames, our SSP effectively propagates discriminative information invideos in a complementary manner. Extensive experiments on four video benchmarkdatasets verify that our SSP significantly outperforms existing SOTA methods by2.76% on average while reducing the overhead of fine-tuning parameters.</description>
      <author>example@mail.com (Jiahuan Zhou, Kai Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua)</author>
      <guid isPermaLink="false">2510.12160v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis</title>
      <link>http://arxiv.org/abs/2510.11907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种双模型框架，结合VideoLLaMA和Qwen2.5-VL的优势，通过分离训练字幕生成和视觉问答任务来最小化任务干扰，提高交通安全分析能力。实验证明该方法在AI城市挑战赛中表现优异，分离训练策略优于联合训练。&lt;h4&gt;背景&lt;/h4&gt;交通安全分析需要复杂的视频理解技术，以捕获细粒度的行为模式并生成全面的描述用于事故预防。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效进行交通安全分析的框架，通过结合不同模型的优势提高视频理解和分析能力，从而更好地捕捉交通行为模式和预防事故。&lt;h4&gt;方法&lt;/h4&gt;提出独特的双模型框架，战略性地利用VideoLLaMA和Qwen2.5-VL的互补优势。核心思路是分离字幕生成和视觉问答任务的训练，以最小化任务干扰并使每个模型能够更有效地专业化。通过在WTS数据集上进行大量实验评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;VideoLLaMA在时间推理方面特别有效，达到1.1001的CIDEr分数；Qwen2.5-VL在视觉理解方面表现出色，VQA准确率达到60.80%；该方法在2025年AI城市挑战赛第二赛道中达到45.7572的S2分数，排名第10位；分离训练策略比联合训练在VQA准确率上提高8.6%，同时保持字幕生成质量。&lt;h4&gt;结论&lt;/h4&gt;通过分离训练策略，双模型框架能够有效地结合不同模型的专长，提高交通安全分析的性能。VideoLLaMA擅长时间推理，而Qwen2.5-VL在视觉理解方面表现优异，为交通安全视频分析提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;交通安全分析需要复杂的视频理解来捕获细粒度的行为模式并生成全面的描述以预防事故。在这项工作中，我们提出了一种独特的双模型框架，通过针对特定任务的优化，战略性地利用VideoLLaMA和Qwen2.5-VL的互补优势来解决这一问题。我们方法的核心见解是，分离字幕生成和视觉问答任务的训练可以最小化任务干扰，并使每个模型能够更有效地专业化。实验结果表明，VideoLLaMA在时间推理方面特别有效，达到1.1001的CIDEr分数，而Qwen2.5-VL在视觉理解方面表现出色，VQA准确率为60.80%。通过在WTS数据集上的大量实验，我们的方法在2025年AI城市挑战赛第二赛道中实现了45.7572的S2分数，在挑战排行榜上排名第10位。消融研究验证了我们的分离训练策略在VQA准确率上比联合训练提高了8.6%，同时保持了字幕生成质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic safety analysis requires complex video understanding to capturefine-grained behavioral patterns and generate comprehensive descriptions foraccident prevention. In this work, we present a unique dual-model frameworkthat strategically utilizes the complementary strengths of VideoLLaMA andQwen2.5-VL through task-specific optimization to address this issue. The coreinsight behind our approach is that separating training for captioning andvisual question answering (VQA) tasks minimizes task interference and allowseach model to specialize more effectively. Experimental results demonstratethat VideoLLaMA is particularly effective in temporal reasoning, achieving aCIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with aVQA accuracy of 60.80\%. Through extensive experiments on the WTS dataset, ourmethod achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,placing 10th on the challenge leaderboard. Ablation studies validate that ourseparate training strategy outperforms joint training by 8.6\% in VQA accuracywhile maintaining captioning quality.</description>
      <author>example@mail.com (Blessing Agyei Kyem, Neema Jakisa Owor, Andrews Danyo, Joshua Kofi Asamoah, Eugene Denteh, Tanner Muturi, Anthony Dontoh, Yaw Adu-Gyamfi, Armstrong Aboah)</author>
      <guid isPermaLink="false">2510.11907v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Audio-Guided Visual Perception for Audio-Visual Navigation</title>
      <link>http://arxiv.org/abs/2510.11760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main paper (6 pages). Accepted for publication by International  Conference on Virtual Reality and Visualization 2025 (ICVRV 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视听具身导航(AVN)旨在使智能体能够使用听觉线索在未知3D环境中自主导航到声源。当前方法在已知声源上表现良好，但在面对新声源时泛化能力差。AGVP框架通过跨模态对齐和区域重加权解决了这一问题，提高了导航效率和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前的视听具身导航方法在分布内的声源上表现良好，但在跨声源泛化方面表现较差，当遇到未听过的声音或未见过的环境时，导航成功率大幅下降，搜索路径变得过长。&lt;h4&gt;目的&lt;/h4&gt;解决当前AVN方法在跨声源泛化方面的局限性，提高智能体在面对新声源时的导航效率和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出AGVP框架，该框架首先通过音频自注意力提取全局听觉上下文，然后使用此上下文作为查询来引导视觉特征注意力，在特征级别突出显示与声源相关的区域，随后进行时间建模和策略优化。设计以可解释的跨模态对齐和区域重加权为中心，减少对特定声学指纹的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;AGVP框架通过将声音从策略可记忆的声学指纹线索转换为空间引导，解决了当前方法中缺乏听觉信号与相应视觉区域之间明确对齐机制的问题，避免了策略在训练期间记忆虚假的'声学指纹-场景'相关性。&lt;h4&gt;结论&lt;/h4&gt;AGVP框架提高了导航效率和鲁棒性，同时对先前未听过的声音实现了跨场景的优越泛化，减少了方法对特定声学指纹的依赖。&lt;h4&gt;翻译&lt;/h4&gt;视听具身导航旨在使智能体能够使用听觉线索在未知3D环境中自主导航到声源。虽然当前的AVN方法在分布内的声源上表现出色，但在跨声源泛化方面表现不佳：当智能体遇到未听过的声音或未见过的环境时，导航成功率大幅下降，搜索路径变得过长。这种限制源于缺乏听觉信号与相应视觉区域之间的明确对齐机制。策略倾向于在训练期间记忆虚假的'声学指纹-场景'相关性，当遇到新的声源时导致盲目探索。为解决此问题，我们提出了AGVP框架，将声音从策略可记忆的声学指纹线索转换为空间引导。该框架首先通过音频自注意力提取全局听觉上下文，然后使用此上下文作为查询来引导视觉特征注意力，在特征级别突出显示与声源相关的区域。随后进行时间建模和策略优化。这种以可解释的跨模态对齐和区域重加权为中心的设计，减少了对特定声学指纹的依赖。实验结果表明，AGVP提高了导航效率和鲁棒性，同时对先前未听过的声音实现了跨场景的优越泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决音频视觉导航(AVN)中的跨源泛化能力不足问题。当前方法在训练时听过的声音源上表现很好(成功率超过95%)，但在遇到未听过的声音源或未见过的环境时，导航成功率急剧下降，搜索路径变得过长。这个问题在现实中非常重要，因为现实世界中的声音源是多样化的，我们无法预训练智能体处理所有可能的声音。在紧急情况下(如火灾中求救声)，智能体需要能够有效定位未知声音源，提高跨场景泛化能力对于构建真正实用的自主导航系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前AVN方法的局限性：它们在策略级别连接视觉和听觉特征，缺乏声音源相关视觉区域与听觉信号之间的明确对齐，导致策略网络倾向于记忆'声纹-场景'的虚假相关性。作者从人类导航行为中获得灵感：在视觉受阻时，人类会先转向声音方向，锁定大致方向，然后关注声音可能出现的区域。基于此，作者设计了'声音优先，视觉跟随'的多模态融合机制。该方法借鉴了SoundSpaces平台、Transformer架构中的自注意力机制和引导注意力机制，以及GRU进行时间建模，同时参考了现有音频视觉导航方法如AV-WaN、SAVi等的优缺点。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; AGVP框架的核心思想是将声音从策略可记忆的声纹线索转变为空间引导。在复杂视觉推理之前，使用音频上下文重新校准视觉特征图，突出显示与声音源最相关的区域，使音频决定'看哪里'，而视觉决定'如何看'。整体流程分为三阶段：1)观察阶段：智能体通过传感器获取视觉(深度图或RGB图像)和听觉(双通道频谱图)输入；2)观察编码阶段：音频特征通过自注意力构建全局上下文，然后作为查询引导视觉特征注意力，突出声音源相关区域，随后通过GRU进行时间建模；3)策略更新阶段：基于PPO的Actor-Critic头根据GRU隐藏状态生成动作分布和状态值，完成从感知到决策的闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)'声音优先，视觉跟随'的多模态融合机制，将融合从后端策略级别提升到感知特征级别；2)音频引导的视觉特征重新加权，使用音频上下文引导视觉注意力；3)通过可解释的跨模态对齐和区域重新加权，减少对特定声纹的依赖。相比之前工作，AGVP在特征级别实现明确的音频视觉对齐，而非传统方法在策略级别的简单连接；不再依赖'声纹-场景'的记忆映射，而是将声音转变为空间引导；在未听过的声音源任务上表现显著优于现有方法，如Replica数据集上实现66.5%的成功率，比之前最佳方法提高约15个百分点。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AGVP框架通过在特征级别实现音频到视觉的明确对齐，将声音从可记忆的声纹线索转变为空间引导，显著提升了音频视觉导航系统在未知声音源和未见环境中的泛化能力和导航效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Embodied Navigation aims to enable agents to autonomouslynavigate to sound sources in unknown 3D environments using auditory cues. Whilecurrent AVN methods excel on in-distribution sound sources, they exhibit poorcross-source generalization: navigation success rates plummet and search pathsbecome excessively long when agents encounter unheard sounds or unseenenvironments. This limitation stems from the lack of explicit alignmentmechanisms between auditory signals and corresponding visual regions. Policiestend to memorize spurious \enquote{acoustic fingerprint-scenario} correlationsduring training, leading to blind exploration when exposed to novel soundsources. To address this, we propose the AGVP framework, which transforms soundfrom policy-memorable acoustic fingerprint cues into spatial guidance. Theframework first extracts global auditory context via audio self-attention, thenuses this context as queries to guide visual feature attention, highlightingsound-source-related regions at the feature level. Subsequent temporal modelingand policy optimization are then performed. This design, centered oninterpretable cross-modal alignment and region reweighting, reduces dependencyon specific acoustic fingerprints. Experimental results demonstrate that AGVPimproves both navigation efficiency and robustness while achieving superiorcross-scenario generalization on previously unheard sounds.</description>
      <author>example@mail.com (Yi Wang, Yinfeng Yu, Fuchun Sun, Liejun Wang, Wendong Zheng)</author>
      <guid isPermaLink="false">2510.11760v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding</title>
      <link>http://arxiv.org/abs/2510.12749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Multimedia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为SPORTS的新型框架，通过紧密集成视频全景分割、视觉里程计和场景渲染任务，实现整体场景理解，解决了现有方法中的分割不足、动态物体干扰、传感器数据稀疏和视角限制等问题。&lt;h4&gt;背景&lt;/h4&gt;场景感知、理解和模拟是具身AI代理的基础技术，但现有解决方案仍存在分割不足、动态物体干扰、传感器数据稀疏和视角限制等挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一个统一的框架，通过整合视频全景分割、视觉里程计和场景渲染任务，实现更准确的整体场景理解。&lt;h4&gt;方法&lt;/h4&gt;1) VPS部分：设计基于自适应注意力的几何融合机制，通过引入姿态、深度和光流模态对齐跨帧特征，并集成后匹配策略改进身份跟踪；2) VO部分：结合VPS的全景分割结果和光流图，提高动态物体置信度估计，增强相机姿态估计精度和深度图生成完整性；3) SR部分：将稀疏点云转换为神经场，合成高保真RGB视图和双重视图。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的实验表明，基于注意力的特征融合在里程计、跟踪、分割和新视角合成任务上优于大多数现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;SPORTS框架通过迭代和统一的视角整合多种任务，有效解决了场景理解中的多个挑战性问题，提升了整体性能。&lt;h4&gt;翻译&lt;/h4&gt;场景感知、理解和模拟是具身AI代理的基础技术，而现有解决方案仍然容易受到分割不足、动态物体干扰、传感器数据稀疏和视角限制等问题的影响。本文提出了一种名为SPORTS的新型框架，通过将视频全景分割、视觉里程计和场景渲染任务紧密集成到一个迭代和统一的视角中，实现整体场景理解。首先，VPS设计了一种基于自适应注意力的几何融合机制，通过引入姿态、深度和光流模态来对齐跨帧特征，自动调整不同解码阶段的特征图，并集成了后匹配策略以改进身份跟踪。在VO中，VPS的全景分割结果与光流图相结合，提高了动态物体的置信度估计，通过基于学习的方法增强了相机姿态估计的精度和深度图生成的完整性。此外，SR的点渲染受益于VO，将稀疏点云转换为神经场，以合成高保真的RGB视图和双重视图。在三个公共数据集上的大量实验证明，我们的基于注意力的特征融合在里程计、跟踪、分割和新视角合成任务上优于大多数现有的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有场景感知、理解和模拟技术中的四个关键问题：分割缺陷、动态物体干扰、传感器数据稀疏和视角限制。这些问题在现实中非常重要，因为随着自动驾驶车辆、四足机器人和人形机器人的普及，对城市场景的整体理解能力对这些智能体执行感知、定位和碰撞避免等任务至关重要。此外，整体场景理解可用于创建城市环境的数字孪生，作为智能体学习和验证意外情况的模拟平台，从而以较低成本提高自动驾驶安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法已开始解决整体场景理解中的信息孤岛问题，但VO和VPS性能仍需改进。他们注意到动态物体处理和稀疏点云重建是关键挑战。设计上，作者借鉴了Video K-Net的核学习机制、PVO的集成方法、DROID-SLAM的优化策略以及READ的点云渲染方法，但通过引入基于注意力的自适应几何融合机制和后匹配策略进行创新，解决了现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出名为SPORTS的框架，通过紧密集成视频全景分割(VPS)、视觉里程计(VO)和场景渲染(SR)任务，实现迭代统一的城市场景理解。整体流程为：1)输入单目视频序列；2)VPS模块利用基于注意力的几何融合机制对齐跨帧特征，并加入后匹配策略提高跟踪质量；3)VO模块利用VPS结果增强动态物体置信度估计，提高姿态估计精度；4)SR模块将高精度姿态保证的稀疏点云转换为神经场，合成高保真场景；5)输出稀疏点云地图、相机姿态和合成的新场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次统一视觉里程计、渲染、物体跟踪和全景分割任务的框架；2)提出基于注意力的几何融合机制和后匹配策略，提高分割和跟踪质量3.07%；3)提出两阶段全景感知流感知深度传播模块，增强基于学习的视觉里程计；4)利用基础模型创建更多评估数据集，验证泛化能力。相比之前工作，SPORTS更充分地考虑了相邻帧特征，采用更先进的解码网络，解决了长序列中的误差传播问题，并更好地区分了静止可移动物体和真正移动的物体。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPORTS通过统一视觉里程计、渲染、物体跟踪和全景分割任务，并提出基于注意力的自适应几何融合机制和后匹配策略，实现了对城市场景的高效理解和精确重建，显著提升了动态环境下的感知、定位和场景渲染性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scene perception, understanding, and simulation are fundamentaltechniques for embodied-AI agents, while existing solutions are still prone tosegmentation deficiency, dynamic objects' interference, sensor data sparsity,and view-limitation problems. This paper proposes a novel framework, namedSPORTS, for holistic scene understanding via tightly integrating Video PanopticSegmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks intoan iterative and unified perspective. Firstly, VPS designs an adaptiveattention-based geometric fusion mechanism to align cross-frame features viaenrolling the pose, depth, and optical flow modality, which automaticallyadjust feature maps for different decoding stages. And a post-matching strategyis integrated to improve identities tracking. In VO, panoptic segmentationresults from VPS are combined with the optical flow map to improve theconfidence estimation of dynamic objects, which enhances the accuracy of thecamera pose estimation and completeness of the depth map generation via thelearning-based paradigm. Furthermore, the point-based rendering of SR isbeneficial from VO, transforming sparse point clouds into neural fields tosynthesize high-fidelity RGB views and twin panoptic views. Extensiveexperiments on three public datasets demonstrate that our attention-basedfeature fusion outperforms most existing state-of-the-art methods on theodometry, tracking, segmentation, and novel view synthesis tasks.</description>
      <author>example@mail.com (Zhiliu Yang, Jinyu Dai, Jianyuan Zhang, Zhu Yang)</author>
      <guid isPermaLink="false">2510.12749v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Gaussian Semantic Field for One-shot LiDAR Global Localization</title>
      <link>http://arxiv.org/abs/2510.12101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于轻量级三层场景图的一次性激光雷达全局定位算法，具有语义消歧能力。&lt;h4&gt;背景&lt;/h4&gt;基于地标语义注册的全局定位方法相比纯几何方法已经显示出有前景的性能提升，但地标可能是重复的且可能误导对应关系的建立。&lt;h4&gt;目的&lt;/h4&gt;通过使用高斯过程群体学习到的连续函数来建模语义分布，缓解地标重复和误导性的问题。&lt;h4&gt;方法&lt;/h4&gt;将连续函数作为中间层插入到物体层和度量-语义层之间，形成三层3D场景图，作为一次性定位的轻量级高性能后端。&lt;h4&gt;主要发现&lt;/h4&gt;与离散语义标签相比，连续函数能够捕获更细粒度的地理语义信息，并为对应关系建立提供更详细的度量信息。&lt;h4&gt;结论&lt;/h4&gt;将全局定位管道命名为Outram-GSF（高斯语义场），并在公开可用数据集上进行了广泛实验，验证了其与当前最先进方法相比的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于轻量级三层场景图的一次性激光雷达全局定位算法，具有语义消歧能力。虽然基于地标语义注册的方法与纯几何方法相比已经在全局定位中显示出有前景的性能提升，但地标可能是重复的且可能误导对应关系的建立。我们提出通过使用从高斯过程群体学习到的连续函数来建模语义分布来缓解这一问题。与离散语义标签相比，连续函数能够捕获更细粒度的地理语义信息，并为对应关系建立提供更详细的度量信息。我们将这个连续函数作为中间层插入到物体层和度量-语义层之间，形成三层3D场景图，作为一次性定位的轻量级高性能后端。我们将我们的全局定位管道命名为Outram-GSF（高斯语义场），并在公开可用数据集上进行了广泛实验，验证了其与当前最先进方法相比的优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决激光雷达全局定位中的语义歧义问题，即在具有重复结构的环境（如城市街道、停车场）中，传统方法难以区分相似但位置不同的地标。这个问题在现实中非常重要，因为精确的全局定位是自动驾驶和机器人导航的基础能力，而一次性定位方法对于机器人快速重新定位至关重要，特别是在GPS信号弱或不可用的环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有基于场景图的全局定位方法主要依赖语义簇的质心和拓扑连接，忽略了簇内丰富的空间语义分布。他们借鉴了3D场景图概念、Outram的分层搜索思想以及高斯过程建模方法，在此基础上创新性地引入高斯语义场作为中间层，形成三层结构。作者通过连续建模空间语义分布来区分几何相似但语义分布不同的区域，解决语义歧义问题，特别是在重复语义结构环境中提高性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用高斯语义场(GSF)建模连续的语义分布，而不是使用离散的语义标签，在传统3D场景图的对象层和语义点云层之间插入GSF中间层。整体流程包括：1)生成三层3D场景图，通过稀疏高斯过程创建高斯语义场层；2)使用网格探测方法生成度量语义特征；3)基于图的子结构匹配，利用Wasserstein距离进行相似性测量；4)通过最大团过程选择内点对应关系，生成最终姿态估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入高斯语义场作为3D场景图的中间层，实现连续语义分布建模；2)基于高斯过程的概率框架学习语义分布并提供相似度度量；3)三层场景图结构作为轻量级高性能定位后端；4)语义稳定性掩码处理场景变化。相比之前工作，Outram-GSF不再仅依赖实例级质心和拓扑关系，而是捕捉簇内丰富的空间语义分布，提供更详细的度量信息，在语义歧义环境中表现更好，特别是在重复结构场景中显著提高了定位性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于高斯语义场的一次性激光雷达全局定位方法，通过连续建模空间语义分布解决了传统方法在语义歧义环境中的局限性，显著提高了在重复结构场景中的定位性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a one-shot LiDAR global localization algorithm featuring semanticdisambiguation ability based on a lightweight tri-layered scene graph. Whilelandmark semantic registration-based methods have shown promising performanceimprovements in global localization compared with geometric-only methods,landmarks can be repetitive and misleading for correspondence establishment. Wepropose to mitigate this problem by modeling semantic distributions withcontinuous functions learned from a population of Gaussian processes. Comparedwith discrete semantic labels, the continuous functions capture finer-grainedgeo-semantic information and also provide more detailed metric information forcorrespondence establishment. We insert this continuous function as the middlelayer between the object layer and the metric-semantic layer, forming atri-layered 3D scene graph, serving as a light-weight yet performant backendfor one-shot localization. We term our global localization pipeline Outram-GSF(Gaussian semantic field) and conduct a wide range of experiments on publiclyavailable data sets, validating the superior performance against the currentstate-of-the-art.</description>
      <author>example@mail.com (Pengyu Yin, Shenghai Yuan, Haozhi Cao, Xingyu Ji, Ruofei Bai, Siyu Chen, Lihua Xie)</author>
      <guid isPermaLink="false">2510.12101v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning</title>
      <link>http://arxiv.org/abs/2510.11996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper was accepted at ICCV Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个专门的空间推理框架，通过将掩模尺寸嵌入输入提示中，增强模型对物体几何和布局的理解能力，并在四个特定问题类别上进行微调，最终在AI City Challenge的Track 3中排名第四，得分为73.0606，证明了该方法在现实工业环境中空间推理的有效性。&lt;h4&gt;背景&lt;/h4&gt;大规模3D环境（如仓库）中的空间推理对视觉语言系统仍然是一个重大挑战，主要困难包括场景杂乱、遮挡以及需要精确的空间理解。现有模型在这样的环境中往往难以泛化，因为它们严重依赖局部外观，缺乏明确的空间基础。&lt;h4&gt;目的&lt;/h4&gt;为2025年AI City Challenge的Track 3中介绍的Physical AI Spatial Intelligence Warehouse数据集引入一个专门的空间推理框架，以改善视觉语言系统在复杂3D环境中的空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;通过将掩模尺寸以边界框坐标的形式直接嵌入输入提示中，增强空间理解能力，使模型能够对物体几何和布局进行推理。在四个问题类别上进行微调框架：距离估计、物体计数、多选基础和空间关系推理，使用任务特定的监督。为了进一步提高与评估系统的一致性，将标准化答案附加到训练集中的GPT响应中。&lt;h4&gt;主要发现&lt;/h4&gt;综合管道最终得分为73.0606，在公共排行榜上总体排名第四。这些结果表明，结构化提示增强和有针对性的优化在推进现实工业环境中的空间推理方面是有效的。&lt;h4&gt;结论&lt;/h4&gt;结构化提示增强和有针对性的优化在推进现实工业环境中的空间推理方面是有效的，通过嵌入掩模尺寸和任务特定微调，可以显著提升视觉语言系统在复杂3D环境中的空间推理能力。&lt;h4&gt;翻译&lt;/h4&gt;在大规模3D环境（如仓库）中进行空间推理对于视觉语言系统来说仍然是一个重大挑战，因为场景杂乱、遮挡以及需要精确的空间理解。现有模型在这样的环境中往往难以泛化，因为它们严重依赖局部外观，缺乏明确的空间基础。在这项工作中，我们为2025年AI City Challenge的Track 3中介绍的Physical AI Spatial Intelligence Warehouse数据集引入了一个专门的空间推理框架。我们的方法通过将掩模尺寸以边界框坐标的形式直接嵌入输入提示中，增强空间理解能力，使模型能够对物体几何和布局进行推理。我们在四个问题类别上微调框架：距离估计、物体计数、多选基础和空间关系推理，使用任务特定的监督。为了进一步提高与评估系统的一致性，将标准化答案附加到训练集中的GPT响应中。我们的综合管道最终得分为73.0606，在公共排行榜上总体排名第四。这些结果表明，结构化提示增强和有针对性的优化在推进现实工业环境中的空间推理方面是有效的。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大型3D环境（如仓库）中的空间推理挑战。视觉-语言系统在这些场景中面临杂乱环境、遮挡和需要精确空间理解的问题，现有模型往往难以泛化，因为它们过度依赖局部外观而缺乏明确的空间基础。这个问题在工业环境中至关重要，因为空间推理对仓库导航、库存管理和安全监控等任务必不可少，而这些环境具有不规则结构、多样物体类型和频繁遮挡等特点，需要系统能够同时捕捉细粒度视觉细节和场景的广泛空间组织。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有视觉-语言模型在空间推理方面的局限性，特别是在处理2D图像和缺乏几何基础方面。他们分析了仓库环境的复杂性，认为需要结合对象检测和空间理解的方法。作者借鉴了SpatialBot框架，因为它已证明在空间智能方面的优越性，能够将单目深度估计生成的深度信息整合到RGB输入中。在此基础上，作者设计了提示级别的增强，将区域掩码编码为边界框坐标，提供结构化空间线索，并添加标准化答案格式确保与评估系统一致。他们还采用了LoRA微调技术来减少训练时间和内存需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在提示中嵌入边界框坐标和掩码尺寸，将几何信息直接注入到视觉-语言模型的输入中，结合深度信息增强模型对空间关系的理解。整体实现流程包括：1) 基于SpatialBot的模型架构，处理RGB和深度输入；2) 提示增强，在输入中注入边界框坐标和区域ID；3) 答案标准化，添加模板后缀确保格式一致；4) 在仓库数据集上进行任务特定微调，使用LoRA技术优化训练过程。这种方法使模型能够推理物体几何和布局，提高在复杂仓库环境中的空间推理能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 专门针对3D工业环境的空间问答框架；2) 提示增强方法，将物体级别的几何特征嵌入提示；3) 在仓库数据集上扩展SpatialBot架构；4) 输出标准化模块确保预测一致性；5) 在AI City Challenge中取得第四名的优异表现。相比之前工作，该方法明确注入几何信息而非依赖纯2D图像，专门针对仓库环境复杂性优化，通过提示工程提供更精确的空间定位，并通过答案标准化确保输出一致性，超越了仅使用语言输入引导空间预测的传统方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过在提示中嵌入边界框坐标和深度信息，并应用答案标准化技术，显著提升了视觉-语言模型在复杂仓库环境中的细粒度空间推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial reasoning in large-scale 3D environments such as warehouses remains asignificant challenge for vision-language systems due to scene clutter,occlusions, and the need for precise spatial understanding. Existing modelsoften struggle with generalization in such settings, as they rely heavily onlocal appearance and lack explicit spatial grounding. In this work, weintroduce a dedicated spatial reasoning framework for the Physical AI SpatialIntelligence Warehouse dataset introduced in the Track 3 2025 AI CityChallenge. Our approach enhances spatial comprehension by embedding maskdimensions in the form of bounding box coordinates directly into the inputprompts, enabling the model to reason over object geometry and layout. Wefine-tune the framework across four question categories namely: DistanceEstimation, Object Counting, Multi-choice Grounding, and Spatial RelationInference using task-specific supervision. To further improve consistency withthe evaluation system, normalized answers are appended to the GPT responsewithin the training set. Our comprehensive pipeline achieves a final score of73.0606, placing 4th overall on the public leaderboard. These resultsdemonstrate the effectiveness of structured prompt enrichment and targetedoptimization in advancing spatial reasoning for real-world industrialenvironments.</description>
      <author>example@mail.com (Tanner Muturi, Blessing Agyei Kyem, Joshua Kofi Asamoah, Neema Jakisa Owor, Richard Dyzinela, Andrews Danyo, Yaw Adu-Gyamfi, Armstrong Aboah)</author>
      <guid isPermaLink="false">2510.11996v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>REACT3D: Recovering Articulations for Interactive Physical 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.11340v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REACT3D是一个可扩展的零样本框架，将静态3D场景转换为具有一致几何形状的模拟就绪交互式副本，可直接用于各种下游任务，通过四个主要贡献实现高效处理，为具身智能研究提供了实用工具。&lt;h4&gt;背景&lt;/h4&gt;交互式3D场景对具身智能日益重要，但由于注释部分分割、运动类型和运动轨迹的劳动密集型过程，现有数据集仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出REACT3D框架，将静态3D场景转换为模拟就绪的交互式副本，使其能够直接用于各种下游任务，并降低关节场景理解大规模研究的门槛。&lt;h4&gt;方法&lt;/h4&gt;包括四个主要贡献：(i)可打开物体检测和分割，从静态场景中提取候选可移动部分；(ii)关节估计，推断关节类型和运动参数；(iii)隐藏几何形状补全，然后进行交互式物体组装；(iv)交互式场景集成，以广泛支持的格式确保与标准模拟平台的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在各种室内场景的检测/分割和关节指标上实现了最先进的性能，证明了框架的有效性，并为可扩展的交互式场景生成提供了实用基础。&lt;h4&gt;结论&lt;/h4&gt;REACT3D为可扩展的交互式场景生成提供了实用基础，从而降低了关节场景理解大规模研究的门槛。&lt;h4&gt;翻译&lt;/h4&gt;交互式3D场景对具身智能日益重要，但由于注释部分分割、运动类型和运动轨迹的劳动密集型过程，现有数据集仍然有限。我们提出了REACT3D，一个可扩展的零样本框架，将静态3D场景转换为具有一致几何形状的模拟就绪交互式副本，可直接用于各种下游任务。我们的贡献包括：(i)可打开物体检测和分割，从静态场景中提取候选可移动部分；(ii)关节估计，推断关节类型和运动参数；(iii)隐藏几何形状补全，然后进行交互式物体组装；(iv)交互式场景集成，以广泛支持的格式确保与标准模拟平台的兼容性。我们在各种室内场景的检测/分割和关节指标上实现了最先进的性能，证明了我们框架的有效性，并为可扩展的交互式场景生成提供了实用基础，从而降低了关节场景理解大规模研究的门槛。我们的项目页面是https://react3d.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将静态3D场景转换为交互式物理3D场景的问题。这个问题很重要，因为现有数据集在标注部分分割、运动类型和运动轨迹方面非常耗时，限制了交互式3D场景的发展。而交互式3D场景对虚拟现实、游戏制作和机器人系统训练等应用至关重要，这些应用需要大量既能提供照片级真实感渲染，又能支持物理上合理交互的3D数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到静态3D场景生成已相对成熟，但交互式场景生成仍处于早期阶段。他们提出利用视觉基础模型和视觉语言模型进行零样本转换，无需额外数据收集或计算密集型生成。他们借鉴了现有工作：使用RAM++进行对象识别，Grounded SAM进行分割，OPDMulti进行关节估计，以及类似DRAWER的多视图融合方法，但进行了改进以提高鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型和视觉语言模型从静态3D场景中恢复物体铰接，生成物理启发的3D场景，同时保持原始几何和外观。整体流程包括：1)开放词汇检测和分割，识别可打开物体并提取可移动部分；2)关节估计，确定运动类型和参数；3)隐藏几何生成，完成物体内部空腔；4)交互场景集成，将交互对象与静态背景结合并导出为兼容格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开放词汇检测减轻标签偏差；2)改进的2D到3D分割提高鲁棒性；3)关节细化基于物体几何提高准确性；4)隐藏几何生成使交互更真实；5)广泛的平台兼容性。相比之前工作，REACT3D无需多状态观察或用户交互，不需要仔细分割处理遮挡，比单图像方法提供更一致结果，且在把手缺失情况下表现更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; REACT3D提出了一种可扩展的零样本框架，将静态3D场景转换为具有物理交互能力的数字孪生体，为具身智能研究提供了实用基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D scenes are increasingly vital for embodied intelligence, yetexisting datasets remain limited due to the labor-intensive process ofannotating part segmentation, kinematic types, and motion trajectories. Wepresent REACT3D, a scalable zero-shot framework that converts static 3D scenesinto simulation-ready interactive replicas with consistent geometry, enablingdirect use in diverse downstream tasks. Our contributions include: (i)openable-object detection and segmentation to extract candidate movable partsfrom static scenes, (ii) articulation estimation that infers joint types andmotion parameters, (iii) hidden-geometry completion followed by interactiveobject assembly, and (iv) interactive scene integration in widely supportedformats to ensure compatibility with standard simulation platforms. We achievestate-of-the-art performance on detection/segmentation and articulation metricsacross diverse indoor scenes, demonstrating the effectiveness of our frameworkand providing a practical foundation for scalable interactive scene generation,thereby lowering the barrier to large-scale research on articulated sceneunderstanding. Our project page is https://react3d.github.io/</description>
      <author>example@mail.com (Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys)</author>
      <guid isPermaLink="false">2510.11340v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>IP-Augmented Multi-Modal Malicious URL Detection Via Token-Contrastive Representation Enhancement and Multi-Granularity Fusion</title>
      <link>http://arxiv.org/abs/2510.12395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CURL-IP的新型多模态恶意URL检测框架，解决了现有方法在处理URL的非自然层次结构、字符级混淆以及整合网络级信号方面的局限性。该框架包含三个关键创新组件，能够同时保留细粒度的词汇线索、上下文语义并整合网络级信号。&lt;h4&gt;背景&lt;/h4&gt;恶意URL检测仍然是网络安全的关键挑战，因为攻击者越来越多地采用复杂的规避技术，包括混淆、字符级扰动和对抗性攻击。虽然预训练语言模型如BERT在URL分析任务中显示出潜力，但当前实现存在三个主要局限：无法有效建模URL的非自然层次结构、对字符级混淆的敏感性不足，以及缺乏整合辅助网络级信号（如IP地址）的机制。&lt;h4&gt;目的&lt;/h4&gt;解决当前恶意URL检测方法中的三个主要局限：无法有效建模URL的非自然层次结构、对字符级混淆的敏感性不足，以及缺乏整合网络级信号的机制。提出一个能够同时保留细粒度词汇线索、上下文语义并整合网络级信号的先进检测框架。&lt;h4&gt;方法&lt;/h4&gt;提出CURL-IP，一个先进的多模态检测框架，包含三个关键创新：(1) Token-Contrastive Representation Enhancer：通过令牌感知对比学习增强子词令牌表示，产生更具区分性和各向同性的嵌入；(2) Cross-Layer Multi-Scale Aggregator：通过卷积操作和门控MLP对Transformer输出进行层次聚合，捕获跨层的局部和全局语义模式；(3) Blockwise Multi-Modal Coupler：将URL-IP特征分解为局部块单元，并在块级别计算跨模态注意力权重，实现细粒度的跨模态交互。&lt;h4&gt;主要发现&lt;/h4&gt;在大型真实世界数据集上的评估显示，该框架在二元和多类分类任务中显著优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;CURL-IP框架通过其创新的架构设计，能够同时处理URL的细粒度特征、上下文语义和网络级信号，有效提高了恶意URL检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;恶意URL检测仍然是一个关键的网络安全挑战，因为对手越来越多地使用复杂的规避技术，包括混淆、字符级扰动和对抗性攻击。虽然像BERT这样的预训练语言模型在URL分析任务中显示出潜力，但当前实现中仍然存在三个局限性：无法有效建模URL的非自然层次结构，对字符级混淆的敏感性不足，以及缺乏整合辅助网络级信号（如IP地址）的机制——这些都是稳健检测所必需的。为了解决这些挑战，我们提出了CURL-IP，一个先进的多模态检测框架，包含三个关键创新：令牌对比表示增强器，通过令牌感知对比学习增强子词令牌表示；跨层多尺度聚合器，通过卷积操作和门控MLP对Transformer输出进行层次聚合；块级多模态耦合器，将URL-IP特征分解为局部块单元并计算跨模态注意力权重。这种架构能够同时保留细粒度的词汇线索、上下文语义和整合网络级信号。我们在大型真实世界数据集上的评估显示，该框架在二元和多类分类任务中显著优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malicious URL detection remains a critical cybersecurity challenge asadversaries increasingly employ sophisticated evasion techniques includingobfuscation, character-level perturbations, and adversarial attacks. Althoughpre-trained language models (PLMs) like BERT have shown potential for URLanalysis tasks, three limitations persist in current implementations: (1)inability to effectively model the non-natural hierarchical structure of URLs,(2) insufficient sensitivity to character-level obfuscation, and (3) lack ofmechanisms to incorporate auxiliary network-level signals such as IPaddresses-all essential for robust detection. To address these challenges, wepropose CURL-IP, an advanced multi-modal detection framework incorporatingthree key innovations: (1) Token-Contrastive Representation Enhancer, whichenhances subword token representations through token-aware contrastive learningto produce more discriminative and isotropic embeddings; (2) Cross-LayerMulti-Scale Aggregator, employing hierarchical aggregation of Transformeroutputs via convolutional operations and gated MLPs to capture both local andglobal semantic patterns across layers; and (3) Blockwise Multi-Modal Couplerthat decomposes URL-IP features into localized block units and computescross-modal attention weights at the block level, enabling fine-grainedinter-modal interaction. This architecture enables simultaneous preservation offine-grained lexical cues, contextual semantics, and integration ofnetwork-level signals. Our evaluation on large-scale real-world datasets showsthe framework significantly outperforms state-of-the-art baselines acrossbinary and multi-class classification tasks.</description>
      <author>example@mail.com (Ye Tian, Yanqiu Yu, Liangliang Song, Zhiquan Liu, Yanbin Wang, Jianguo Sun)</author>
      <guid isPermaLink="false">2510.12395v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?</title>
      <link>http://arxiv.org/abs/2510.12087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文本属性图(TAGs)上的表示学习结合了结构连接与丰富的文本语义，但在当前方法中，过度对齐会导致性能下降。作者提出了LLM4GTA框架，通过保留表示间隙来维持模态特定知识并提高迁移性能。&lt;h4&gt;背景&lt;/h4&gt;当前文本属性图表示学习方法主要依赖对比学习来最大化跨模态相似性，假设图和文本表示之间的更紧密耦合可以提高迁移性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中过度对齐导致的性能下降问题，提出一种保留表示间隙的框架，以维持模态特定知识并提高迁移性能。&lt;h4&gt;方法&lt;/h4&gt;提出LLM4GTA框架，包含自适应间隙保留模块防止过度对齐，以及模内补偿机制使用辅助分类器增强图空间的判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;经验分析表明，无论是自然间隙扩大还是强制间隙减小都会导致性能下降，这是因为图编码器捕获拓扑模式而文本编码器捕获语义结构，两者之间存在几何不兼容性。&lt;h4&gt;结论&lt;/h4&gt;保留表示间隙作为维持模态特定知识的几何必要性可以改善零样本和少样本场景下的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图(TAGs)上的表示学习将结构连接与丰富的文本语义相结合，使能够在多个领域应用。当前方法主要依赖对比学习来最大化跨模态相似性，假设图和文本表示之间的更紧密耦合可以提高迁移性能。然而，我们的经验分析显示，无论是自然间隙扩大还是强制间隙减小都会通过破坏预训练知识结构和损害泛化能力而导致性能下降。这是由于编码器之间的几何不兼容性造成的，其中图编码器捕获拓扑模式，而文本编码器捕获语义结构。过度对齐将这些不同的空间压缩到共享子空间中，导致结构崩溃，同时削弱了拓扑推理和语义理解。我们提出LLM4GTA，一个间隙感知的对齐框架，将表示间隙保留为维持模态特定知识和提高迁移性能的几何必要性。LLM4GTA包括自适应间隙保留模块，通过监控相似性演变防止过度对齐，以及模内补偿机制，使用图空间中的辅助分类器增强判别能力。大量实验表明，在零样本和少样本场景下，与现有方法相比显示出显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning on text-attributed graphs (TAGs) integratesstructural connectivity with rich textual semantics, enabling applications indiverse domains. Current methods largely rely on contrastive learning tomaximize cross-modal similarity, assuming tighter coupling between graph andtext representations improves transfer performance. However, our empiricalanalysis reveals that both natural gap expansion and forced gap reductionresult in performance degradation by disrupting pre-trained knowledgestructures and impairing generalization. This arises from the geometricincompatibility between encoders, where graph encoders capture topologicalpatterns, while text encoders capture semantic structures. Over-alignmentcompresses these distinct spaces into shared subspaces, causing structurecollapse that diminishes both topological reasoning and semantic understanding.We propose \textbf{LLM4GTA}, a gap-aware alignment framework that preservesrepresentation gaps as geometric necessities for maintaining modality-specificknowledge and improving transfer performance. LLM4GTA includes an adaptive gappreservation module to prevent over-alignment by monitoring similarityevolution and an intra-modal compensation mechanism that boosts discriminativepower using auxiliary classifiers in graph space. Extensive experiments showsignificant improvements over existing methods in zero-shot and few-shotscenarios.</description>
      <author>example@mail.com (Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Zijian Zhang, Yilei Yuan, Hao Zhang, Jin Huang)</author>
      <guid isPermaLink="false">2510.12087v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs</title>
      <link>http://arxiv.org/abs/2510.12085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出GraphShaper几何感知框架，通过多几何专业化解决图基础模型在结构边界处性能下降问题，实现零样本设置下在引用网络和社会网络上分别提高9.47%和7.63%的准确率。&lt;h4&gt;背景&lt;/h4&gt;图基础模型是一种在不同图域中学习可迁移表示的变革性范式。最近方法利用大型语言模型通过对比学习将图和文本模态统一到共享表示空间，但系统评估显示在结构边界处性能显著下降，准确率损失超过20个百分点。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够尊重图结构内在几何多样性的对齐框架，解决当前图基础模型在结构边界处性能下降的问题。&lt;h4&gt;方法&lt;/h4&gt;提出GraphShaper框架，采用针对不同几何空间的专业网络，动态计算融合权重，基于局部结构特征自适应地集成几何特性，在对齐文本嵌入前保持结构完整性。&lt;h4&gt;主要发现&lt;/h4&gt;当前方法假设所有图结构可在单一欧几里得空间编码，但实际上树结构需要双曲几何保持分层分支，循环模式依赖球面几何的闭合性质。在结构边界处，节点经历冲突的几何约束，统一编码空间无法解决。&lt;h4&gt;结论&lt;/h4&gt;GraphShaper通过多几何专业化和自适应融合策略，有效解决了图基础模型在结构边界处的性能下降问题，显著提高了零样本设置下的准确率。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型代表了一种在不同图域中学习可迁移表示的变革性范式。最近的方法利用大型语言模型通过对比学习将图和文本模态统一到共享表示空间中。然而，系统评估显示，在具有不同拓扑模式汇聚的结构边界处，性能显著下降，准确率损失超过20个百分点。这个问题源于一个关键限制：当前方法假设所有图结构都可以在单一欧几里得空间内编码。实际上，树结构需要双曲几何来保持分层分支，而循环模式则依赖于球面几何的闭合性质。在结构边界处，节点经历冲突的几何约束，统一的编码空间无法解决。这提出了一个关键挑战：能否设计尊重图结构内在几何多样性的对齐框架？我们介绍了GraphShaper，一个几何感知框架，通过多几何专业化增强图编码。我们的方法采用针对不同几何空间的专业网络，动态计算融合权重，基于局部结构特征自适应地集成几何特性。这种自适应融合在对齐文本嵌入之前保持结构完整性。大量实验证明，GraphShaper在零样本设置下在引用网络上实现了9.47%的准确率提升，在社会网络上实现了7.63%的准确率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph foundation models represent a transformative paradigm for learningtransferable representations across diverse graph domains. Recent methodsleverage large language models to unify graph and text modalities into a sharedrepresentation space using contrastive learning. However, systematicevaluations reveal significant performance degradation at structural boundarieswhere distinct topological patterns converge, with accuracy losses exceeding 20percentage points. This issue arises from a key limitation: current methodsassume all graph structures can be encoded within a single Euclidean space. Inreality, tree structures require hyperbolic geometry to preserve hierarchicalbranching, while cyclic patterns depend on spherical geometry for closureproperties. At structural boundaries, nodes experience conflicting geometricconstraints that uniform encoding spaces cannot resolve. This raises a crucialchallenge: \textbf{Can alignment frameworks be designed to respect theintrinsic geometric diversity of graph structures?} We introduce\textbf{GraphShaper}, a geometry-aware framework that enhances graph encodingthrough multi-geometric specialization. Our approach employs expert networkstailored to different geometric spaces, dynamically computing fusion weights toadaptively integrate geometric properties based on local structuralcharacteristics. This adaptive fusion preserves structural integrity beforealignment with text embeddings. Extensive experiments demonstrate thatGraphShaper achieves 9.47\% accuracy improvements on citation networks and7.63\% on social networks in zero-shot settings.</description>
      <author>example@mail.com (Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Haochen You, Zijian Zhang, Yilei Yuan, Jin Huang)</author>
      <guid isPermaLink="false">2510.12085v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging</title>
      <link>http://arxiv.org/abs/2510.12070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 page, 7 figures, uses IEEE.sty&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MEASURE的新型深度学习框架，用于解决睡眠分期模型在分布外场景中的泛化问题。该框架通过减少领域相关信息同时保留重要的时频特征，显著提升了模型在未见过的数据上的性能。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的自动睡眠分期技术近年来性能显著提升，对睡眠障碍诊断至关重要。然而，这些模型在处理不同受试者的生理信号时存在泛化困难，导致在分布外场景中性能下降。领域泛化方法，特别是对比学习，被研究用于解决这一问题，但现有方法往往无法充分提取领域不变特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效减少领域相关信息同时保留睡眠分期所需关键时频特征的新型框架，以提高模型在未见数据上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了MEASURE（Multi-scale Minimal Sufficient Representation Learning）框架，该框架采用多尺度方法最小化充分表示学习，能够在减少领域相关信息的同时，保留多级特征中编码的多样时频信息。&lt;h4&gt;主要发现&lt;/h4&gt;在SleepEDF-20和MASS等公开睡眠分期基准数据集上的详尽实验表明，所提出的MEASURE方法持续优于当前最先进的方法，证明了其在处理领域差异方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;MEASURE框架通过针对性地减少过度领域相关信息同时保留关键特征，成功弥合了领域差距，显著提高了睡眠分期模型在分布外场景中的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的自动睡眠分期在性能上已取得显著进展，并在睡眠障碍诊断中起着关键作用。然而，由于生理信号的变异性，这些模型往往难以在未见过的受试者上泛化，导致在分布外场景中性能下降。为了解决这个问题，最近研究了领域泛化方法，以确保在训练期间对未见领域有泛化性能。在这些技术中，对比学习已证明通过在不同领域间对齐同类样本来学习领域不变特征的有效性。尽管有潜力，但许多现有方法不足以提取充分的领域不变表示，因为它们没有明确解决样本间非共享信息中嵌入的领域特征。在本文中，我们认为减轻这种领域相关属性（称为过度领域相关信息）是弥合领域差距的关键。然而，直接减轻领域相关属性的策略往往对高级信息特征过拟合，限制了利用多级特征中编码的多样时频信息的能力。为了解决这些局限性，我们提出了一个新颖的MEASURE（多尺度最小充分表示学习）框架，该框架在减少领域相关信息的同时，有效保留了睡眠分期分类的基本时频特征。在公开可用的睡眠分期基准数据集SleepEDF-20和MASS上进行的详尽实验中，我们提出的方法持续优于最先进的方法。我们的代码可在 https://github.com/ku-milab/Measure 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based automatic sleep staging has significantly advanced inperformance and plays a crucial role in the diagnosis of sleep disorders.However, those models often struggle to generalize on unseen subjects due tovariability in physiological signals, resulting in degraded performance inout-of-distribution scenarios. To address this issue, domain generalizationapproaches have recently been studied to ensure generalized performance onunseen domains during training. Among those techniques, contrastive learninghas proven its validity in learning domain-invariant features by aligningsamples of the same class across different domains. Despite its potential, manyexisting methods are insufficient to extract adequately domain-invariantrepresentations, as they do not explicitly address domain characteristicsembedded within the unshared information across samples. In this paper, weposit that mitigating such domain-relevant attributes-referred to as excessdomain-relevant information-is key to bridging the domain gap. However, thedirect strategy to mitigate the domain-relevant attributes often overfitsfeatures at the high-level information, limiting their ability to leverage thediverse temporal and spectral information encoded in the multiple featurelevels. To address these limitations, we propose a novel MEASURE (Multi-scalEminimAl SUfficient Representation lEarning) framework, which effectivelyreduces domain-relevant information while preserving essential temporal andspectral features for sleep stage classification. In our exhaustive experimentson publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,our proposed method consistently outperformed state-of-the-art methods. Ourcode is available at : https://github.com/ku-milab/Measure</description>
      <author>example@mail.com (Sangmin Jo, Jee Seok Yoon, Wootaek Jeong, Kwanseok Oh, Heung-Il Suk)</author>
      <guid isPermaLink="false">2510.12070v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images</title>
      <link>http://arxiv.org/abs/2510.11883v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MammoDINO是一种专门为乳腺X线摄影设计的自监督学习框架，通过在大规模数据上预训练，实现了在乳腺癌筛查任务上的最先进性能，并具有良好的泛化能力，为计算机辅助诊断提供了无标注的基础。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已在一般领域的视觉编码器训练中取得变革性进展，但在医学影像领域应用不足，主要原因是数据有限和领域特定偏差。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于乳腺X线摄影的自监督学习框架，捕捉临床上有意义的特征，提高乳腺癌筛查的诊断效率，减轻放射科医生的工作量。&lt;h4&gt;方法&lt;/h4&gt;构建MammoDINO框架，在140万张乳腺X线图像上预训练；引入乳腺组织感知的数据增强采样器，用于图像级和补丁级监督；设计跨切片对比学习目标，利用3D数字乳腺断层合成结构进行2D预训练。&lt;h4&gt;主要发现&lt;/h4&gt;MammoDINO在多个乳腺癌筛查任务上取得了最先进的性能，并在五个基准数据集上表现出良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MammoDINO为乳腺X线摄影的多用途计算机辅助诊断工具提供了一种可扩展的无标注基础，有助于减轻放射科医生的工作量，提高乳腺癌筛查的诊断效率。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习(SSL)已经在一般领域的视觉编码器训练中取得变革性进展，但由于数据有限和领域特定偏差，在医学影像中的应用仍然不足。我们提出了MammoDINO，这是一种用于乳腺X线摄影的新型SSL框架，在140万张乳腺X线图像上进行了预训练。为了捕捉临床上有意义的特征，我们引入了一种乳腺组织感知的数据增强采样器，用于图像级和补丁级监督，以及跨切片对比学习目标，利用3D数字乳腺断层合成(DBT)结构进行2D预训练。MammoDINO在多个乳腺癌筛查任务上取得了最先进的性能，并在五个基准数据集上表现出良好的泛化能力。它为乳腺X线摄影的多用途计算机辅助诊断(CAD)工具提供了一种可扩展的无标注基础，有助于减轻放射科医生的工作量，提高乳腺癌筛查的诊断效率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自监督学习在乳腺X光摄影(mammography)领域的应用不足问题。这个问题很重要，因为乳腺癌是美国女性最常见的癌症，乳腺X光是主要筛查方式，但准确解读具有挑战性，且现有计算机辅助诊断工具依赖大量标注数据，而标注的乳腺X光数据有限，限制了模型效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有自监督学习框架在乳腺X光图像中的局限性：随机裁剪可能过度采样不相关背景，传统对比学习仅限单个2D切片无法捕捉3D DBT数据的跨切片结构连贯性。作者借鉴了DINOv2框架，参考了RAD-DINO和MedCoSS等医学影像自监督学习方法，并针对乳腺X光特点进行了创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1)确保模型专注于临床有意义的乳腺组织区域而非背景；2)利用3D DBT的跨切片结构连贯性。整体流程：收集140万乳腺X光图像→预处理→基于ViT架构的模型预训练→结合乳腺组织感知的DINO损失、iBOT损失和3D DBT相邻切片损失→在五个基准数据集上微调和评估多种乳腺癌筛查任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)乳腺组织感知的数据增强采样器，确保模型关注临床相关区域；2)3D DBT相邻切片损失，捕捉跨切片解剖连续性。不同之处：相比通用SSL框架(如DINOv2)，避免了过度采样背景和忽略3D结构；相比放射学定制SSL(如RadDINO)，专门针对乳腺解剖结构优化；相比文本引导方法(如BiomedCLIP)，无需文本监督仍能取得优越性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MammoDINO通过引入乳腺组织感知的数据增强和3D跨切片学习机制，解决了自监督学习在乳腺X光图像中的局限性，实现了乳腺癌筛查任务的先进性能，为开发更有效的计算机辅助诊断系统提供了无需标注的基础模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has transformed vision encoder training ingeneral domains but remains underutilized in medical imaging due to limiteddata and domain specific biases. We present MammoDINO, a novel SSL frameworkfor mammography, pretrained on 1.4 million mammographic images. To captureclinically meaningful features, we introduce a breast tissue aware dataaugmentation sampler for both image-level and patch-level supervision and across-slice contrastive learning objective that leverages 3D digital breasttomosynthesis (DBT) structure into 2D pretraining. MammoDINO achievesstate-of-the-art performance on multiple breast cancer screening tasks andgeneralizes well across five benchmark datasets. It offers a scalable,annotation-free foundation for multipurpose computer-aided diagnosis (CAD)tools for mammogram, helping reduce radiologists' workload and improvediagnostic efficiency in breast cancer screening.</description>
      <author>example@mail.com (Sicheng Zhou, Lei Wu, Cao Xiao, Parminder Bhatia, Taha Kass-Hout)</author>
      <guid isPermaLink="false">2510.11883v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Improving Knowledge Graph Embeddings through Contrastive Learning with Negative Statements</title>
      <link>http://arxiv.org/abs/2510.11868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Thirteenth International Conference on Knowledge  Capture (K-CAP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的知识图谱嵌入方法，通过整合明确的否定陈述来改进知识嵌入学习过程，在链接预测和三元组分类任务上取得了优于现有模型的性能。&lt;h4&gt;背景&lt;/h4&gt;知识图谱以结构化三元组表示信息，是问答、链接预测和推荐系统等多种应用的基础。现有图嵌入方法大多依赖封闭世界假设，将缺失三元组视为错误，这与现实世界知识图谱的开放世界假设相矛盾，且很少考虑明确的否定陈述。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，将明确声明的否定陈述整合到知识嵌入学习过程中，以改进知识图谱的表示和预测能力。&lt;h4&gt;方法&lt;/h4&gt;采用双模型架构，两个嵌入模型并行训练：一个在正陈述上训练，另一个在负陈述上训练。训练过程中，每个模型通过破坏正样本生成负样本，并使用另一个模型的评分选择最可能的候选样本。&lt;h4&gt;主要发现&lt;/h4&gt;在通用和特定领域知识图谱上的大量实验表明，该方法在链接预测和三元组分类任务上优于最先进的嵌入模型，证明了整合有意义的负知识到嵌入学习中的价值。&lt;h4&gt;结论&lt;/h4&gt;通过整合明确的否定陈述，该方法有效提高了知识图谱嵌入的预测性能，为处理开放世界假设下的知识图谱提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱将信息表示为结构化三元组，并作为问答、链接预测和推荐系统等广泛应用的基础。探索知识图谱的一个主要研究方向是图嵌入方法，其中实体和关系在低维向量空间中表示，以捕获底层语义和结构。然而，大多数现有方法依赖于封闭世界假设或局部封闭世界假设等假设，将缺失的三元组视为错误。这与许多现实世界知识图谱所基于的开放世界假设形成对比。此外，虽然明确陈述的否定陈述有助于区分错误和未知的三元组，但它们很少被纳入知识图谱，在嵌入训练过程中也经常被忽视。在这项工作中，我们介绍了一种新方法，将明确声明的否定陈述整合到知识嵌入学习过程中。我们的方法采用双模型架构，两个嵌入模型并行训练，一个在正陈述上训练，另一个在负陈述上训练。在训练过程中，每个模型通过破坏正样本生成负样本，并使用另一个模型的评分选择最可能的候选样本。所提出的方法在通用和特定领域知识图谱上进行了评估，重点关注链接预测和三元组分类任务。大量实验表明，我们的方法优于最先进的嵌入模型，证明了将有意义的负知识整合到嵌入学习中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graphs represent information as structured triples and serve as thebackbone for a wide range of applications, including question answering, linkprediction, and recommendation systems. A prominent line of research forexploring knowledge graphs involves graph embedding methods, where entities andrelations are represented in low-dimensional vector spaces that captureunderlying semantics and structure. However, most existing methods rely onassumptions such as the Closed World Assumption or Local Closed WorldAssumption, treating missing triples as false. This contrasts with the OpenWorld Assumption underlying many real-world knowledge graphs. Furthermore,while explicitly stated negative statements can help distinguish between falseand unknown triples, they are rarely included in knowledge graphs and are oftenoverlooked during embedding training.  In this work, we introduce a novel approach that integrates explicitlydeclared negative statements into the knowledge embedding learning process. Ourapproach employs a dual-model architecture, where two embedding models aretrained in parallel, one on positive statements and the other on negativestatements. During training, each model generates negative samples bycorrupting positive samples and selecting the most likely candidates as scoredby the other model. The proposed approach is evaluated on both general-purposeand domain-specific knowledge graphs, with a focus on link prediction andtriple classification tasks. The extensive experiments demonstrate that ourapproach improves predictive performance over state-of-the-art embeddingmodels, demonstrating the value of integrating meaningful negative knowledgeinto embedding learning.</description>
      <author>example@mail.com (Rita T. Sousa, Heiko Paulheim)</author>
      <guid isPermaLink="false">2510.11868v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.11827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Janus框架通过联合利用欧几里得和双曲图神经网络，有效解决了节点级异常检测的挑战，通过对比学习对齐不同空间中的嵌入，识别难以协调的节点视图来检测异常。&lt;h4&gt;背景&lt;/h4&gt;节点级异常检测(NAD)具有挑战性，因为存在多样的结构模式和特征分布。NAD是一个关键任务，应用于欺诈检测、网络安全、推荐系统等多个领域。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为Janus的框架，用于有效解决节点级异常检测问题。&lt;h4&gt;方法&lt;/h4&gt;每个节点由两个视图描述：原始特征和从随机游走和度派生的结构特征，然后嵌入到欧几里得和双曲空间中。使用配备对比学习目标作为正则化项的多图自编码器框架，对齐欧几里得和双曲空间中的嵌入，突出显示那些视图难以协调的节点，这些节点可能是异常的。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实数据集上的实验表明，Janus始终优于浅层和深度基线方法， empirically demonstrating that combining multiple geometric representations provides a robust and effective approach for identifying subtle and complex anomalies in graphs.&lt;h4&gt;结论&lt;/h4&gt;组合多种几何表示是一种稳健有效的方法，用于识别图中的微妙和复杂异常。&lt;h4&gt;翻译&lt;/h4&gt;节点级异常检测(NAD)具有挑战性，因为存在多样的结构模式和特征分布。因此，NAD是一个关键任务，应用于从欺诈检测、网络安全到推荐系统的多个领域。我们引入了Janus，一个联合利用欧几里得和双曲图神经网络来捕捉节点表示互补方面的框架。每个节点由两个视图描述，由原始特征和从随机游走和度派生的结构特征组成，然后嵌入到欧几里得和双曲空间中。配备对比学习目标作为正则化项的多图自编码器框架，对齐欧几里得和双曲空间中的嵌入，突出显示那些视图难以协调的节点，因此可能是异常的。在四个真实数据集上的实验表明，Janus始终优于浅层和深度基线方法， empirically demonstrating that combining multiple geometric representations provides a robust and effective approach for identifying subtle and complex anomalies in graphs.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Node-level anomaly detection (NAD) is challenging due to diverse structuralpatterns and feature distributions. As such, NAD is a critical task withseveral applications which range from fraud detection, cybersecurity, torecommendation systems. We introduce Janus, a framework that jointly leveragesEuclidean and Hyperbolic Graph Neural Networks to capture complementary aspectsof node representations. Each node is described by two views, composed by theoriginal features and structural features derived from random walks anddegrees, then embedded into Euclidean and Hyperbolic spaces. A multiGraph-Autoencoder framework, equipped with a contrastive learning objective asregularization term, aligns the embeddings across the Euclidean and Hyperbolicspaces, highlighting nodes whose views are difficult to reconcile and are thuslikely anomalous. Experiments on four real-world datasets show that Janusconsistently outperforms shallow and deep baselines, empirically demonstratingthat combining multiple geometric representations provides a robust andeffective approach for identifying subtle and complex anomalies in graphs.</description>
      <author>example@mail.com (Simone Mungari, Ettore Ritacco, Pietro Sabatino)</author>
      <guid isPermaLink="false">2510.11827v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare</title>
      <link>http://arxiv.org/abs/2510.12741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the Symposium on Model Accountability, Sustainability and  Healthcare (SMASH) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的个性化联邦微调方法，通过学习正交LoRA适配器来解耦通用知识和客户特定知识，使医疗领域的每个参与方能够充分利用自己的数据和他人的数据，解决了医疗数据隐私保护与模型性能之间的矛盾。&lt;h4&gt;背景&lt;/h4&gt;基础模型为AI在医疗领域的应用开辟了新可能性，但即使在健康数据上预训练，仍需针对特定下游任务微调。尽管基础模型减少了训练数据需求，但获取足够数据仍具挑战性，部分原因是医疗数据共享和聚合受到患者隐私保护限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在保护患者隐私的前提下，有效利用多方医疗数据的联邦微调方法，使各参与机构能够充分利用自有数据和他人的数据来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的个性化联邦微调方法，学习正交LoRA适配器来解耦通用知识和客户特定知识，使每个客户端能够同时利用自有数据和来自其他参与方的数据。&lt;h4&gt;主要发现&lt;/h4&gt;在实际联邦医学成像任务上的初步结果表明，该方法与当前联邦微调方法具有竞争力，能够有效平衡数据隐私保护与模型性能提升。&lt;h4&gt;结论&lt;/h4&gt;通过联邦学习框架下的正交LoRA适配器方法，解决了医疗AI中数据隐私保护与模型性能之间的矛盾，为医疗领域的基础模型应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基础模型为AI在医疗领域的应用开辟了新的可能性。然而，即使在健康数据上预训练，它们仍需要针对特定的下游任务进行微调。此外，尽管基础模型减少了实现良好性能所需的训练数据量，但获取足够的数据仍然是一个挑战。这部分是由于为了保护患者隐私，限制了不同来源数据的共享和聚合。一个可能的解决方案是通过多个参与方（即医院、诊所等）之间的联邦学习来微调基础模型。在这项工作中，我们提出了一种新的个性化联邦微调方法，学习正交LoRA适配器来解耦通用知识和客户特定知识，使每个客户能够充分利用自己的数据和他人数据。我们在实际联邦医学成像任务上的初步结果表明，我们的方法与当前的联邦微调方法具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models open up new possibilities for the use of AI in healthcare.However, even when pre-trained on health data, they still need to be fine-tunedfor specific downstream tasks. Furthermore, although foundation models reducethe amount of training data required to achieve good performance, obtainingsufficient data is still a challenge. This is due, in part, to restrictions onsharing and aggregating data from different sources to protect patients'privacy. One possible solution to this is to fine-tune foundation models viafederated learning across multiple participating clients (i.e., hospitals,clinics, etc.). In this work, we propose a new personalized federatedfine-tuning method that learns orthogonal LoRA adapters to disentangle generaland client-specific knowledge, enabling each client to fully exploit both theirown data and the data of others. Our preliminary results on real-worldfederated medical imaging tasks demonstrate that our approach is competitiveagainst current federated fine-tuning methods.</description>
      <author>example@mail.com (Adam Tupper, Christian Gagné)</author>
      <guid isPermaLink="false">2510.12741v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2510.12724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;T(R,O)Grasp是一种基于扩散的框架，能够高效生成准确和多样化的抓取动作，适用于多种机器人手，在实验中取得了94.83%的平均成功率，推理速度为0.21秒，每秒可处理41个抓取动作，显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;灵巧抓取在机器人学中仍然是一个核心挑战，这主要由于其高维状态和动作空间的复杂性。&lt;h4&gt;目的&lt;/h4&gt;引入T(R,O)Grasp，一种基于扩散的框架，用于高效生成准确和多样化的抓取动作，适用于多种机器人手。&lt;h4&gt;方法&lt;/h4&gt;核心是T(R,O)图，一种统一表示方法，建模机器人手和物体间的空间变换并编码其几何属性；结合图扩散模型和高效的逆运动学求解器，支持无条件和有条件的抓取合成。&lt;h4&gt;主要发现&lt;/h4&gt;在多种灵巧手上进行的实验显示，T(R,O)Grasp平均成功率达94.83%，推理速度0.21秒，在NVIDIA A100 40GB GPU上每秒处理41个抓取动作；该方法在不同实现上具有鲁棒性和泛化能力，显著减少内存消耗，高推理速度使闭环灵巧操作成为可能。&lt;h4&gt;结论&lt;/h4&gt;T(R,O)Grasp有潜力扩展为灵巧抓取的基础模型。&lt;h4&gt;翻译&lt;/h4&gt;灵巧抓取由于高维状态和动作空间的复杂性，在机器人学中仍然是一个核心挑战。我们引入了T(R,O)Grasp，一种基于扩散的框架，能够高效生成准确和多样化的抓取动作，适用于多种机器人手。其核心是T(R,O)图，一种统一表示方法，建模机器人手和物体间的空间变换并编码其几何属性。结合图扩散模型和高效的逆运动学求解器，支持无条件和有条件的抓取合成。在多种灵巧手上的广泛实验表明，T(R,O)Grasp在NVIDIA A100 40GB GPU上达到94.83%的平均成功率、0.21秒的推理速度和每秒41个抓取的吞吐量，显著优于现有基线方法。此外，我们的方法在不同实现上具有鲁棒性和泛化能力，同时显著减少内存消耗。更重要的是，高推理速度使闭环灵巧操作成为可能，突显了T(R,O)Grasp扩展为灵巧抓取基础模型的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效生成精确且多样化的灵巧抓取动作（dexterous grasping）问题，特别是在跨机器人平台（cross-embodiment）情况下。这个问题很重要，因为灵巧抓取是实现人类级精确操作的基础能力，对于机器人完成日常任务至关重要；现有方法要么计算效率低下，要么难以在不同机器人平台间泛化；高效的抓取生成对于实时应用和闭环控制至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性（机器人中心方法泛化能力差、物体中心方法计算成本高、交互中心方法如D(R,O)内存消耗大且依赖初始状态）设计了新方法。作者借鉴了扩散模型（DDPM/DDIM）、图神经网络、空间变换表示（SE(3)）和逆运动学求解等现有技术，但创新性地提出了T(R,O) Graph表示和图扩散模型，解决了效率和泛化问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是T(R,O) Graph表示和图扩散模型。T(R,O) Graph将物体和机器人手表示为图中的节点，它们之间的空间变换表示为边；图扩散模型基于此表示生成抓取。整体流程：1) 构建T(R,O) Graph（物体节点和手节点及其空间变换）；2) 图扩散模型前向过程添加噪声；3) 去噪模型预测噪声；4) 反向过程恢复干净抓取；5) 使用Pyroki进行逆运动学求解得到关节值。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) T(R,O) Graph统一表示，显著减少内存使用；2) 高效图扩散模型，支持非条件和条件生成；3) 不依赖初始状态，避免D(R,O)的局限性；4) 高效训练推理（内存减少57%，速度提高3倍）；5) 强大的跨平台泛化能力。相比D(R,O)，成功率更高（94.83% vs 87.53%），内存更少，速度更快，不依赖初始状态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; T(R,O) Grasp通过创新的图扩散模型和高效的T(R,O) Graph表示，实现了跨平台灵巧抓取的高效生成，在保持高成功率的同时显著提高了推理速度并降低了内存消耗，为实时闭环抓取提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous grasping remains a central challenge in robotics due to thecomplexity of its high-dimensional state and action space. We introduce T(R,O)Grasp, a diffusion-based framework that efficiently generates accurate anddiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,a unified representation that models spatial transformations between robotichands and objects while encoding their geometric properties. A graph diffusionmodel, coupled with an efficient inverse kinematics solver, supports bothunconditioned and conditioned grasp synthesis. Extensive experiments on adiverse set of dexterous hands show that T(R,O) Grasp achieves average successrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps persecond on an NVIDIA A100 40GB GPU, substantially outperforming existingbaselines. In addition, our approach is robust and generalizable acrossembodiments while significantly reducing memory consumption. More importantly,the high inference speed enables closed-loop dexterous manipulation,underscoring the potential of T(R,O) Grasp to scale into a foundation model fordexterous grasping.</description>
      <author>example@mail.com (Xin Fei, Zhixuan Xu, Huaicong Fang, Tianrui Zhang, Lin Shao)</author>
      <guid isPermaLink="false">2510.12724v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction</title>
      <link>http://arxiv.org/abs/2510.12719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了化学预训练模型在药物发现中的应用，特别是在多任务学习框架下的微调效果。&lt;h4&gt;背景&lt;/h4&gt;化学预训练模型（基础模型）在药物发现领域受到广泛关注。从自监督训练中提取的一般化学知识有望提高对关键药物发现终点的预测，包括靶点效力和ADMET性质。多任务学习已被成功用于改进预测模型。&lt;h4&gt;目的&lt;/h4&gt;研究在化学预训练图神经网络模型（如KERMT和KGPT）的微调过程中启用多任务学习的效果。&lt;h4&gt;方法&lt;/h4&gt;通过比较多任务微调与非预训练图神经网络模型的性能差异，评估了Kinetic GROVER Multi-Task (KERMT)和Knowledge-guided Pre-training of Graph Transformer (KGPT)模型的表现。&lt;h4&gt;主要发现&lt;/h4&gt;1. 多任务微调显著提高了预训练图神经网络模型的性能；2. 数据量越大，多任务微调KERMT带来的性能提升越显著；3. 发布了两个多任务ADMET数据分割，便于更准确地基准测试多任务深度学习方法；4. 在GitHub上提供了KERMT模型的加速实现，支持工业药物发现工作流程。&lt;h4&gt;结论&lt;/h4&gt;多任务微调能显著提升化学预训练图神经网络模型在药物发现应用中的性能，特别是在大数据集上，并提供了相关工具和数据资源促进该领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;化学预训练模型，有时被称为基础模型，正在药物发现应用中引起相当大的关注。从自监督训练中提取的一般化学知识有可能提高对关键药物发现终点的预测，包括靶点效力和ADMET性质。多任务学习已被成功用于改进预测模型。在这里，我们表明，在化学预训练图神经网络模型的微调中启用多任务，如Kinetic GROVER Multi-Task (KERMT)（GROVER模型的增强版本）和Knowledge-guided Pre-training of Graph Transformer (KGPT)，显著优于非预训练的图神经网络模型。令人惊讶的是，我们发现以多任务方式微调KERMT带来的性能提升在数据量较大时最为显著。此外，我们发布了两个多任务ADMET数据分割，以便更准确地基准测试多任务深度学习方法用于药物性质预测。最后，我们在GitHub上提供了KERMT模型的加速实现，使工业药物发现工作流程中的大规模预训练、微调和推理成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chemical pretrained models, sometimes referred to as foundation models, arereceiving considerable interest for drug discovery applications. The generalchemical knowledge extracted from self-supervised training has the potential toimprove predictions for critical drug discovery endpoints, including on-targetpotency and ADMET properties. Multi-task learning has previously beensuccessfully leveraged to improve predictive models. Here, we show thatenabling multitasking in finetuning of chemical pretrained graph neural networkmodels such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of theGROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)significantly improves performance over non-pretrained graph neural networkmodels. Surprisingly, we find that the performance improvement from finetuningKERMT in a multitask manner is most significant at larger data sizes.Additionally, we publish two multitask ADMET data splits to enable moreaccurate benchmarking of multitask deep learning methods for drug propertyprediction. Finally, we provide an accelerated implementation of the KERMTmodel on GitHub, unlocking large-scale pretraining, finetuning, and inferencein industrial drug discovery workflows.</description>
      <author>example@mail.com (Matthew Adrian, Yunsie Chung, Kevin Boyd, Saee Paliwal, Srimukh Prasad Veccham, Alan C. Cheng)</author>
      <guid isPermaLink="false">2510.12719v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</title>
      <link>http://arxiv.org/abs/2510.12709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAIL-Embedding是一种全模态嵌入基础模型，通过定制的训练策略和架构设计解决了现有多模态嵌入模型在实际应用中面临的挑战，并在各种检索任务和实际业务场景中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示以支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展有前景，但现有模型在实际应用和业务场景中仍面临模态支持有限、训练机制不稳定和工业领域差距等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够解决现有多模态嵌入模型在实际应用中面临挑战的全模态嵌入基础模型，提高其在各种跨模态任务中的表现和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出SAIL-Embedding模型，采用多阶段训练方案：(1)内容感知渐进式训练增强模型对不同下游任务的适应性和跨模态能力；(2)协作感知推荐增强训练通过提取序列到项目和ID到项目嵌入知识并挖掘用户历史兴趣来优化推荐场景；(3)随机专业化和数据集驱动的模式匹配加强模型训练的灵活性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;SAIL-Embedding在不同检索任务中实现了最先进性能；在抖音精选场景中，7天LT增长+0.158%，14天LT增长+0.144%；在抖音feed排序模型中，匹配特征带来+0.08%的AUC增益。&lt;h4&gt;结论&lt;/h4&gt;SAIL-Embedding通过创新的训练策略和架构设计有效解决了现有多模态嵌入模型在实际应用中的局限性，显著提升了模型性能和业务指标。&lt;h4&gt;翻译&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示，支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展有前景，但先前的工作在实际应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定和工业领域差距。在这项工作中，我们引入了SAIL-Embedding，一个全模态嵌入基础模型，通过定制的训练策略和架构设计解决这些问题。在优化过程中，我们提出了多阶段训练方案，以提高表示学习的多方面有效性。具体而言，内容感知渐进式训练旨在增强模型对不同下游任务的适应性，掌握丰富的跨模态能力。协作感知推荐增强训练通过从序列到项目和ID到项目嵌入中提取知识，同时挖掘用户历史兴趣，进一步使多模态表示适应推荐场景。同时，我们开发了随机专业化和数据集驱动的模式匹配，以加强模型训练的灵活性和泛化能力。实验结果表明，与其他方法相比，SAIL-Embedding在不同的检索任务中实现了最先进的性能。在我们模型集成的各种现实场景的在线实验中，我们观察到Lifetime (LT)显著增加，这是推荐体验的关键指标。例如，在抖音精选场景中，模型实现了7天LT增长+0.158%和14天LT增长+0.144%。对于抖音feed排序模型，SAIL-Embedding产生的匹配特征带来了+0.08%的AUC增益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal embedding models aim to yield informative unified representationsthat empower diverse cross-modal tasks. Despite promising developments in theevolution from CLIP-based dual-tower architectures to large vision-languagemodels, prior works still face unavoidable challenges in real-worldapplications and business scenarios, such as the limited modality support,unstable training mechanisms, and industrial domain gaps. In this work, weintroduce SAIL-Embedding, an omni-modal embedding foundation model thataddresses these issues through tailored training strategies and architecturaldesign. In the optimization procedure, we propose a multi-stage training schemeto boost the multifaceted effectiveness of representation learning.Specifically, the content-aware progressive training aims to enhance themodel's adaptability to diverse downstream tasks and master enrichedcross-modal proficiency. The collaboration-aware recommendation enhancementtraining further adapts multimodal representations for recommendation scenariosby distilling knowledge from sequence-to-item and ID-to-item embeddings whilemining user historical interests. Concurrently, we develop the stochasticspecialization and dataset-driven pattern matching to strengthen model trainingflexibility and generalizability. Experimental results show that SAIL-Embeddingachieves SOTA performance compared to other methods in different retrievaltasks. In online experiments across various real-world scenarios integratedwith our model, we observe a significant increase in Lifetime (LT), which is acrucial indicator for the recommendation experience. For instance, the modeldelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in theDouyin-Selected scenario. For the Douyin feed rank model, the match featuresproduced by SAIL-Embedding yield a +0.08% AUC gain.</description>
      <author>example@mail.com (Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng)</author>
      <guid isPermaLink="false">2510.12709v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>CoRA: Covariate-Aware Adaptation of Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.12681v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种协变量感知适应框架(CoRA)，用于增强时间序列基础模型(TSFMs)的性能，使其能够有效整合来自不同模态的外部协变量信息，显著提升预测质量。&lt;h4&gt;背景&lt;/h4&gt;当前大多数TSFMs在单变量时间序列上进行预训练，这限制了它们在真实世界预测任务中利用不同协变量中的重要信息。这种限制源于变量间依赖性的异质性和在大规模多变量数据集上的骨干模型扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;为了进一步提高TSFMs的性能，提出一个通用的协变量感知适应框架(CoRA)，使TSFMs能够有效整合来自时间序列、语言和图像等不同模态的外部协变量信息。&lt;h4&gt;方法&lt;/h4&gt;CoRA框架利用预训练的基础模型骨干作为冻结特征提取器，采用Granger因果嵌入(GCE)自动评估协变量相对于目标变量的因果预测能力，并通过零初始化的条件注入机制整合加权嵌入，避免灾难性遗忘并逐渐融入外部信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，TSFMs的CoRA超越了具有完全或少样本训练的最先进协变量感知深度预测器，在协变量感知预测上实现了31.1%的MSE降低。CoRA与各种先进TSFMs兼容性强，并将协变量范围扩展到其他模态。&lt;h4&gt;结论&lt;/h4&gt;CoRA为TSFMs的应用提供了实用范式，能够有效整合多模态协变量信息，显著提升预测性能，扩展了TSFMs在实际应用中的能力范围。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)已通过其模型容量、可扩展性和零样本泛化能力显示出显著影响。然而，由于变量间依赖性的异质性和在大规模多变量数据集上的骨干模型扩展性，大多数TSFMs通常在单变量时间序列上进行预训练。这一限制使它们在真实世界预测任务中忽略了来自不同协变量的关键信息。为了进一步提高TSFMs的性能，我们提出了一个通用的协变量感知适应(CoRA)框架。它利用基础模型的预训练骨干，同时有效整合来自时间序列、语言和图像等不同模态的外部协变量，以提高预测质量。技术上，CoRA在适应过程中保持初始化等价性和参数一致性。将基础模型的骨干作为冻结特征提取器，实证表明基础模型的输出嵌入比原始数据更具信息量。此外，CoRA采用了一种新的Granger因果嵌入(GCE)来自动评估协变量相对于目标变量的因果预测能力。我们将这些加权嵌入与零初始化的条件注入机制相结合，避免了对预训练基础模型的灾难性遗忘，并逐渐整合外部信息。大量实验表明，TSFMs的CoRA超越了具有完全或少样本训练的最先进协变量感知深度预测器，在协变量感知预测上实现了31.1%的MSE降低。与其他适应方法相比，CoRA与各种先进的TSFMs具有强大的兼容性，并将协变量的范围扩展到其他模态，为TSFMs的应用提供了实用的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) have shown significant impact throughtheir model capacity, scalability, and zero-shot generalization. However, dueto the heterogeneity of inter-variate dependencies and the backbone scalabilityon large-scale multivariate datasets, most TSFMs are typically pre-trained onunivariate time series. This limitation renders them oblivious to crucialinformation from diverse covariates in real-world forecasting tasks. To furtherenhance the performance of TSFMs, we propose a general covariate-awareadaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones offoundation models while effectively incorporating exogenous covariates fromvarious modalities, including time series, language, and images, to improve thequality of predictions. Technically, CoRA maintains the equivalence ofinitialization and parameter consistency during adaptation. With preservedbackbones of foundation models as frozen feature extractors, the outcomeembeddings from foundation models are empirically demonstrated more informativethan raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE)to automatically evaluate covariates regarding their causal predictability withrespect to the target variate. We incorporate these weighted embeddings with azero-initialized condition-injection mechanism, avoiding catastrophicforgetting of pre-trained foundation models and gradually integrates exogenousinformation. Extensive experiments show that CoRA of TSFMs surpassesstate-of-the-art covariate-aware deep forecasters with full or few-shottraining samples, achieving 31.1% MSE reduction on covariate-aware forecasting.Compared to other adaptation methods, CoRA exhibits strong compatibility withvarious advanced TSFMs and extends the scope of covariates to other modalities,presenting a practical paradigm for the application of TSFMs.</description>
      <author>example@mail.com (Guo Qin, Zhi Chen, Yong Liu, Zhiyuan Shi, Haixuan Liu, Xiangdong Huang, Jianmin Wang, Mingsheng Long)</author>
      <guid isPermaLink="false">2510.12681v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.12660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCVW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究旨在开发简单高效的人体网格恢复(HMR)模型及其前置任务人体姿态估计(HPE)模型，通过利用分层视觉基础模型的早期阶段作为编码器，实现了在准确性和计算效率之间的更好权衡。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的HMR方法（如HMR2.0及其后续版本）依赖于大型、非分层的视觉Transformer作为编码器，这些编码器是从相应的HPE模型（如ViTPose）继承而来的。&lt;h4&gt;目的&lt;/h4&gt;开发简单高效的人体网格恢复(HMR)模型及其前置任务人体姿态估计(HPE)模型。&lt;h4&gt;方法&lt;/h4&gt;构建三种轻量级HMR2.0变体；提出利用Swin Transformer、GroupMixFormer和VMamba等分层视觉基础模型的早期阶段作为编码器；对27种基于分层VFMs的HMR和HPE模型进行全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用分层VFMs的前两或三个阶段就能达到与完整阶段模型相当的性能；截断后的模型在准确性和计算效率之间表现出比现有轻量级替代方案更好的权衡。&lt;h4&gt;结论&lt;/h4&gt;通过利用分层视觉基础模型的早期阶段，可以开发出既高效又准确的人体网格恢复和姿态估计模型。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们旨在开发用于人体网格恢复(HMR)及其前置任务人体姿态估计(HPE)的简单高效模型。最先进的HMR方法（如HMR2.0及其后续版本）依赖于大型、非分层的视觉Transformer作为编码器，这些编码器是从相应的HPE模型（如ViTPose）继承而来的。为了在不同计算预算下建立基线，我们首先通过调整相应的ViTPose模型构建了三种轻量级HMR2.0变体。此外，我们提出利用Swin Transformer、GroupMixFormer和VMamba等分层视觉基础模型(VFMs)的早期阶段作为编码器。这种设计灵感来自于分层VFMs的中间阶段产生的特征图分辨率与非分层模型相当或更高。我们对27种基于分层VFMs的HMR和HPE模型进行了全面评估，证明仅使用前两或三个阶段就能达到与完整阶段模型相当的性能。此外，我们表明与现有的轻量级替代方案相比，截断后的模型在准确性和计算效率之间表现出更好的权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we aim to develop simple and efficient models for human meshrecovery (HMR) and its predecessor task, human pose estimation (HPE).State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large,non-hierarchical vision transformers as encoders, which are inherited from thecorresponding HPE models like ViTPose. To establish baselines across varyingcomputational budgets, we first construct three lightweight HMR2.0 variants byadapting the corresponding ViTPose models. In addition, we propose leveragingthe early stages of hierarchical vision foundation models (VFMs), includingSwin Transformer, GroupMixFormer, and VMamba, as encoders. This design ismotivated by the observation that intermediate stages of hierarchical VFMsproduce feature maps with resolutions comparable to or higher than those ofnon-hierarchical counterparts. We conduct a comprehensive evaluation of 27hierarchical-VFM-based HMR and HPE models, demonstrating that using only thefirst two or three stages achieves performance on par with full-stage models.Moreover, we show that the resulting truncated models exhibit better trade-offsbetween accuracy and computational efficiency compared to existing lightweightalternatives.</description>
      <author>example@mail.com (Shuhei Tarashima, Yushan Wang, Norio Tagawa)</author>
      <guid isPermaLink="false">2510.12660v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery</title>
      <link>http://arxiv.org/abs/2510.12640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于基础模型的新方法，用于分析科学领域中的时间序列事件数据，无需为每个新数据集重新训练模型，从而加速科学发现。&lt;h4&gt;背景&lt;/h4&gt;许多科学领域（从医学到地震学）依赖于分析随时间变化的事件序列来理解复杂系统。传统机器学习模型需要为每个新数据集从头构建和训练，这是一个缓慢且昂贵的过程。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的事件分析方法，使复杂事件分析更加易于访问，并加快科学发现的步伐。&lt;h4&gt;方法&lt;/h4&gt;创建一个单一的强大'基础模型'，在数百万个模拟事件序列上训练，学习事件数据的基本模式和事件如何展开的通用理解。&lt;h4&gt;主要发现&lt;/h4&gt;该模型可以即时分析新的科学数据，无需重新训练，只需查看数据集中的几个示例；同时可以快速微调以获得更高的准确性。&lt;h4&gt;结论&lt;/h4&gt;这种方法使复杂事件分析更加易于访问，并加速了科学发现的步伐。&lt;h4&gt;翻译&lt;/h4&gt;许多科学领域，从医学到地震学，都依赖于分析随时间变化的事件序列来理解复杂系统。传统上，机器学习模型必须为每个新数据集从头构建和训练，这是一个缓慢且昂贵的过程。我们介绍了一种新方法：一个单一的、强大的模型，学习上下文中事件数据的基本模式。我们在数百万个模拟事件序列上训练了这个'基础模型'，教会它事件如何展开的通用理解。因此，我们的模型可以即时分析新的科学数据，无需重新训练，只需查看数据集中的几个示例。它还可以快速微调以获得更高的准确性。这种方法使复杂事件分析更加易于访问，并加快了科学发现的步伐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many scientific fields, from medicine to seismology, rely on analyzingsequences of events over time to understand complex systems. Traditionally,machine learning models must be built and trained from scratch for each newdataset, which is a slow and costly process. We introduce a new approach: asingle, powerful model that learns the underlying patterns of event data incontext. We trained this "foundation model" on millions of simulated eventsequences, teaching it a general-purpose understanding of how events canunfold. As a result, our model can analyze new scientific data instantly,without retraining, simply by looking at a few examples from the dataset. Itcan also be quickly fine-tuned for even higher accuracy. This approach makessophisticated event analysis more accessible and accelerates the pace ofscientific discovery.</description>
      <author>example@mail.com (David Berghaus, Patrick Seifner, Kostadin Cvejoski, Ramses J. Sanchez)</author>
      <guid isPermaLink="false">2510.12640v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence</title>
      <link>http://arxiv.org/abs/2510.12579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合Plantnet、DinoV2和SAM的农业图像零样本分割方法，利用Plantnet的植物识别能力生成粗略掩码，再通过SAM细化得到详细分割结果，无需收集新数据集。&lt;h4&gt;背景&lt;/h4&gt;农业图像分割面临训练数据有限和田间条件复杂等挑战，这些因素常常阻碍纯监督方法的有效性，现有方法往往需要大量难以获取的标注数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需收集和标注新数据集的农业图像分割方法，解决农业场景中数据标注瓶颈问题，并在各种复杂农业场景中实现有效的分割。&lt;h4&gt;方法&lt;/h4&gt;利用Plantnet（大型植物分类模型）及其DinoV2主干网络，结合Segment Anything Model (SAM)，使用Plantnet的专门植物表示来识别植物区域并生成粗略分割掩码，然后通过SAM进一步细化掩码以获得详细分割结果。&lt;h4&gt;主要发现&lt;/h4&gt;使用Plantnet微调的DinoV2相比基础DinoV2模型在Jaccard指数(IoU)测量上展现出一致的性能提升，结合基础模型与专门的植物中心模型可以缓解标注瓶颈问题，并在各种农业场景中实现有效分割。&lt;h4&gt;结论&lt;/h4&gt;将基础模型与专门的植物中心模型相结合具有潜力，可以减轻农业图像分割中的标注负担，并在多样化的农业场景中实现有效的分割结果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种农业图像的零样本分割方法，该方法结合了Plantnet（一种大规模植物分类模型）、其DinoV2主干网络和Segment Anything Model (SAM)。我们无需收集和标注新数据集，而是利用Plantnet专门的植物表示来识别植物区域并生成粗略分割掩码。然后，这些掩码通过SAM进行细化以产生详细分割结果。我们在四个公开可用数据集上进行了评估，这些数据集在对比度方面具有不同复杂度，包括一些训练数据有限且田间条件复杂常常阻碍纯监督方法的数据集。我们的结果显示，与基础DinoV2模型相比，使用Plantnet微调的DinoV2在Jaccard指数(IoU)测量上展现出一致的性能提升。这些发现强调了将基础模型与专门的植物中心模型相结合的潜力，可以减轻标注瓶颈，并在多样化的农业场景中实现有效分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a zero-shot segmentation approach for agricultural imagery thatleverages Plantnet, a large-scale plant classification model, in conjunctionwith its DinoV2 backbone and the Segment Anything Model (SAM). Rather thancollecting and annotating new datasets, our method exploits Plantnet'sspecialized plant representations to identify plant regions and produce coarsesegmentation masks. These masks are then refined by SAM to yield detailedsegmentations. We evaluate on four publicly available datasets of variouscomplexity in terms of contrast including some where the limited size of thetraining data and complex field conditions often hinder purely supervisedmethods. Our results show consistent performance gains when usingPlantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by theJaccard Index (IoU). These findings highlight the potential of combiningfoundation models with specialized plant-centric models to alleviate theannotation bottleneck and enable effective segmentation in diverse agriculturalscenarios.</description>
      <author>example@mail.com (Simon Ravé, Jean-Christophe Lombardo, Pejman Rasti, Alexis Joly, David Rousseau)</author>
      <guid isPermaLink="false">2510.12579v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>HEAR: An EEG Foundation Model with Heterogeneous Electrode Adaptive Representation</title>
      <link>http://arxiv.org/abs/2510.12515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HEAR，这是首个专门设计用于支持异构EEG设备的EEG基础模型，能够适应不同的电极布局和电极数量，通过可学习的基于坐标的空间嵌入和空间引导transformer实现统一表示空间，实验结果表明HEAR在支持异构EEG设备和跨任务跨主体泛化方面显著优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;EEG是神经科学研究和脑机接口应用的重要技术，近期开发的大规模EEG基础模型展现出强大的跨任务和跨主体泛化能力，但EEG设备的异质性阻碍了这些模型的广泛采用和进一步发展。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够支持异构EEG设备的EEG基础模型，解决不同电极布局和电极数量带来的兼容性问题。&lt;h4&gt;方法&lt;/h4&gt;HEAR采用可学习的基于坐标的空间嵌入将不同布局和数量的电极映射到统一表示空间，并通过空间引导transformer处理这些表示以捕获电极间的时空依赖关系；同时构建了一个包含8,782小时数据、来自150多种电极布局的数据集来支持模型开发。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明HEAR在支持异构EEG设备方面显著优于现有EEG基础模型，并在多样化的认知任务和主体间表现出良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;HEAR为解决EEG设备异质性挑战提供了有效方法，有助于EEG基础模型的广泛应用和进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;脑电图是神经科学研究和脑机接口应用的关键技术。最近，大规模EEG基础模型已被开发，展现出跨不同任务和主体的强大泛化能力。然而，EEG设备的异质性不仅阻碍了这些模型的广泛采用，也对其进一步扩展和发展提出了重大挑战。在本文中，我们介绍了HEAR，这是首个专门设计用于支持异构EEG设备的EEG基础模型，能够适应不同的电极布局和电极数量。HEAR采用可学习的基于坐标的空间嵌入，将具有不同布局和数量的电极映射到统一的表示空间。然后，这种统一的空间表示由新颖的空间引导transformer处理，有效捕获了电极间的时空依赖关系。为支持HEAR的开发，我们构建了一个包含8,782小时数据的大规模EEG数据集，数据来自150多种不同的电极布局，电极数量最多达1,132个。实验结果表明，HEAR在支持异构EEG设备方面显著优于现有的EEG基础模型，并能很好地泛化到多样化的认知任务和主体中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) is an essential technique for neuroscienceresearch and brain-computer interface (BCI) applications. Recently, large-scaleEEG foundation models have been developed, exhibiting robust generalizationcapabilities across diverse tasks and subjects. However, the heterogeneity ofEEG devices not only hinders the widespread adoption of these models but alsoposes significant challenges to their further scaling and development. In thispaper, we introduce HEAR, the first EEG foundation model explicitly designed tosupport heterogeneous EEG devices, accommodating varying electrode layouts andelectrode counts. HEAR employs a learnable, coordinate-based spatial embeddingto map electrodes with diverse layouts and varying counts into a unifiedrepresentational space. This unified spatial representation is then processedby a novel spatially-guided transformer, which effectively capturesspatiotemporal dependencies across electrodes. To support the development ofHEAR, we construct a large-scale EEG dataset comprising 8,782 hours of datacollected from over 150 distinct electrode layouts with up to 1,132 electrodes.Experimental results demonstrate that HEAR substantially outperforms existingEEG foundation models in supporting heterogeneous EEG devices and generalizingacross diverse cognitive tasks and subjects.</description>
      <author>example@mail.com (Zhige Chen, Chengxuan Qin, Wenlong You, Rui Liu, Congying Chu, Rui Yang, Kay Chen Tan, Jibin Wu)</author>
      <guid isPermaLink="false">2510.12515v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation</title>
      <link>http://arxiv.org/abs/2510.12498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人工智能虚拟细胞(AIVCs)旨在从多模态、多尺度数据中学习细胞状态模型，当前研究面临跨实验室迁移性有限、数据分割偏差、剂量时间效应处理不足以及跨尺度耦合受限等挑战。作者提出细胞状态潜在(CSL)视角，通过操作符语法组织学习，并建议改进评估方法和数据设计。&lt;h4&gt;背景&lt;/h4&gt;AIVCs致力于从多模态、多尺度测量中学习可执行的、决策相关的细胞状态模型。近期研究已引入单细胞和空间基础模型，改进跨模态对齐，扩展扰动图谱，并探索通路水平读出。&lt;h4&gt;目的&lt;/h4&gt;提出一种与模型无关的细胞状态潜在(CSL)视角，通过操作符语法组织学习，并建立跨模态、尺度、背景和干预的决策对齐评估蓝图。&lt;h4&gt;方法&lt;/h4&gt;采用操作符语法组织学习：测量、提升/投影(用于跨尺度耦合)和干预(用于剂量和调度)。强调功能空间读出，如通路活性、空间邻域和临床相关终点。&lt;h4&gt;主要发现&lt;/h4&gt;当前评估主要局限于单个数据集和设置；跨实验室和平台的可迁移性有限；某些数据分割易受泄漏和覆盖偏差影响；剂量、时间和组合效应未得到系统处理；跨尺度耦合受限，分子、细胞和组织水平的锚点稀少。&lt;h4&gt;结论&lt;/h4&gt;建议采用操作符感知的数据设计、抗泄漏分区和透明校准与报告，以实现可重复的、一对一的比较，改进AIVCs的评估方法。&lt;h4&gt;翻译&lt;/h4&gt;人工智能虚拟细胞(AIVCs)旨在从多模态、多尺度测量中学习可执行的、决策相关的细胞状态模型。近期研究已引入单细胞和空间基础模型，改进跨模态对齐，扩展扰动图谱，并探索通路水平读出。然而，尽管保留验证是标准实践，评估仍主要局限于单个数据集和设置；证据表明跨实验室和平台的可迁移性通常有限，某些数据分割易受泄漏和覆盖偏差影响，剂量、时间和组合效应尚未得到系统处理。跨尺度耦合仍然受限，因为连接分子、细胞和组织水平的锚点稀少，且与科学或临床读出的对齐在不同研究中各不相同。我们提出了一种与模型无关的细胞状态潜在(CSL)视角，通过操作符语法组织学习：测量、提升/投影(用于跨尺度耦合)和干预(用于剂量和调度)。这一观点激发了跨模态、尺度、背景和干预的决策对齐评估蓝图，并强调功能空间读出，如通路活性、空间邻域和临床相关终点。我们建议采用操作符感知的数据设计、抗泄漏分区和透明校准与报告，以实现可重复的、一对一的比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,decision-relevant models of cell state from multimodal, multiscalemeasurements. Recent studies have introduced single-cell and spatial foundationmodels, improved cross-modality alignment, scaled perturbation atlases, andexplored pathway-level readouts. Nevertheless, although held-out validation isstandard practice, evaluations remain predominantly within single datasets andsettings; evidence indicates that transport across laboratories and platformsis often limited, that some data splits are vulnerable to leakage and coveragebias, and that dose, time and combination effects are not yet systematicallyhandled. Cross-scale coupling also remains constrained, as anchors linkingmolecular, cellular and tissue levels are sparse, and alignment to scientificor clinical readouts varies across studies. We propose a model-agnosticCell-State Latent (CSL) perspective that organizes learning via an operatorgrammar: measurement, lift/project for cross-scale coupling, and interventionfor dosing and scheduling. This view motivates a decision-aligned evaluationblueprint across modality, scale, context and intervention, and emphasizesfunction-space readouts such as pathway activity, spatial neighborhoods andclinically relevant endpoints. We recommend operator-aware data design,leakage-resistant partitions, and transparent calibration and reporting toenable reproducible, like-for-like comparisons.</description>
      <author>example@mail.com (Chengpeng Hu, Calvin Yu-Chian Chen)</author>
      <guid isPermaLink="false">2510.12498v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2510.12369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层量化框架，通过自加权机制实现跨尺度的任务自适应聚合，在保持编码器冻结的同时，通过轻量级门控过程调节信息流，实现参数高效的下游任务适应。&lt;h4&gt;背景&lt;/h4&gt;语言和视觉基础模型的进展表明，将复杂输入转换为紧凑序列的离散标记接口对大规模建模至关重要。将此范式扩展到图需要处理非欧几里得结构和多尺度依赖关系的标记化方案。&lt;h4&gt;目的&lt;/h4&gt;解决现有图标记化方法（线性化、连续和量化）在适应性和效率上的局限性，特别是解决基于量化的标记化方法在组织分层信息时缺乏任务自适应性的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种分层量化框架，引入自加权机制用于跨尺度的任务自适应聚合。该方法保持编码器冻结，通过轻量级门控过程调节信息流，实现参数高效的下游任务适应。&lt;h4&gt;主要发现&lt;/h4&gt;在节点分类和链接预测的基准数据集上，所提出的方法在可比的计算预算下，与强大的基线方法相比取得了持续改进。&lt;h4&gt;结论&lt;/h4&gt;该分层量化框架能够有效处理非欧几里得结构和多尺度依赖关系，通过自加权机制实现任务自适应信息聚合，为图建模提供了新的高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语言和视觉基础模型的最新进展表明，将复杂输入转换为紧凑序列的离散标记接口对大规模建模至关重要。将这一范式扩展到图需要一种能够高效处理非欧几里得结构和多尺度依赖关系的标记化方案。现有的图标记化方法，包括线性化、连续和量化方法，在适应性和效率方面仍然存在局限性。特别是，大多数当前基于量化的标记化方法以固定或任务无关的方式组织分层信息，这可能导致过度表示或未充分利用结构线索，并且无法在不重新编码器的情况下动态重新加权不同级别的贡献。本文提出了一种分层量化框架，引入了跨多个尺度进行任务自适应聚合的自加权机制。所提出的方法保持编码器冻结，同时通过轻量级门控过程调节信息流，实现参数高效地适应多样化的下游任务。在节点分类和链接预测的基准数据集上的实验表明，在可比的计算预算下，该方法比强大的基线方法取得了持续改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in language and vision foundation models demonstrates theimportance of discrete token interfaces that transform complex inputs intocompact sequences for large-scale modeling. Extending this paradigm to graphsrequires a tokenization scheme that handles non-Euclidean structures andmulti-scale dependencies efficiently. Existing approaches to graphtokenization, linearized, continuous, and quantized, remain limited inadaptability and efficiency. In particular, most current quantization-basedtokenizers organize hierarchical information in fixed or task-agnostic ways,which may either over-represent or under-utilize structural cues, and lack theability to dynamically reweight contributions from different levels withoutretraining the encoder. This work presents a hierarchical quantizationframework that introduces a self-weighted mechanism for task-adaptiveaggregation across multiple scales. The proposed method maintains a frozenencoder while modulating information flow through a lightweight gating process,enabling parameter-efficient adaptation to diverse downstream tasks.Experiments on benchmark datasets for node classification and link predictiondemonstrate consistent improvements over strong baselines under comparablecomputational budgets.</description>
      <author>example@mail.com (Yang Xiang, Li Fan, Chenke Yin, Chengtao Ji)</author>
      <guid isPermaLink="false">2510.12369v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DeePAQ: A Perceptual Audio Quality Metric Based On Foundational Models and Weakly Supervised Learning</title>
      <link>http://arxiv.org/abs/2510.12326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了DeePAQ，一种基于深度学习的感知音频质量评估指标，用于评估通用音频质量。该方法结合了度量学习和音乐基础模型MERT，通过代理标签指导构建捕获音频失真强度的嵌入空间。研究首次在通用音频质量领域利用弱监督标签和度量学习微调音乐基础模型，使用低秩适应(LoRA)技术。实验表明，该方法在检测编码伪影方面优于现有指标，并能良好泛化到未见过的失真类型。&lt;h4&gt;背景&lt;/h4&gt;在音频质量评估领域，需要能够准确评估通用音频质量的指标。现有方法可能在处理不同类型的音频失真时存在局限性，特别是在编码伪影和源分离等场景中。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的音频质量评估指标，能够准确捕捉通用音频中的失真强度，并在多种音频处理场景中表现良好。&lt;h4&gt;方法&lt;/h4&gt;研究采用度量学习结合音乐基础模型MERT的方法，通过代理标签指导，构建一个能够捕获通用音频失真强度的嵌入空间。该方法创新性地在通用音频质量领域应用弱监督标签和度量学习来微调音乐基础模型，并使用低秩适应(LoRA)技术进行参数高效调整。&lt;h4&gt;主要发现&lt;/h4&gt;在音频编码和源分离的听力测试中，DeePAQ超越了现有的最先进目标音频质量指标。特别是在检测编码伪影方面表现优异，并且对未见过的失真类型（如源分离）具有良好的泛化能力，展示了其鲁棒性和多功能性。&lt;h4&gt;结论&lt;/h4&gt;DeePAQ是一种创新的音频质量评估方法，通过结合度量学习和音乐基础模型，有效地解决了通用音频质量评估的挑战。其优越的性能和泛化能力表明该方法在音频处理领域具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了用于评估通用音频质量的基于深度学习的感知音频质量指标(DeePAQ)。我们的方法结合了度量学习和音乐基础模型MERT，通过代理标签指导，构建一个捕获通用音频中失真强度的嵌入空间。据我们所知，DeePAQ是通用音频质量领域中首个利用弱监督标签和度量学习来使用低秩适应(LoRA)微调音乐基础模型的方法，这一方向尚未被其他最先进方法探索。我们在涵盖音频编码和源分离的听力测试中，将所提出模型与最先进的目标音频质量指标进行了基准测试。结果表明，我们的方法在检测编码伪影方面超越了现有指标，并且对源分离等未见过的失真具有良好的泛化能力，突显了其鲁棒性和多功能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Deep learning-based Perceptual Audio Quality metric(DeePAQ) for evaluating general audio quality. Our approach leverages metriclearning together with the music foundation model MERT, guided by surrogatelabels, to construct an embedding space that captures distortion intensity ingeneral audio. To the best of our knowledge, DeePAQ is the first in the generalaudio quality domain to leverage weakly supervised labels and metric learningfor fine-tuning a music foundation model with Low-Rank Adaptation (LoRA), adirection not yet explored by other state-of-the-art methods. We benchmark theproposed model against state-of-the-art objective audio quality metrics acrosslistening tests spanning audio coding and source separation. Results show thatour method surpasses existing metrics in detecting coding artifacts andgeneralizes well to unseen distortions such as source separation, highlightingits robustness and versatility.</description>
      <author>example@mail.com (Guanxin Jiang, Andreas Brendel, Pablo M. Delgado, Jürgen Herre)</author>
      <guid isPermaLink="false">2510.12326v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</title>
      <link>http://arxiv.org/abs/2510.12276v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了空间强制(Spatial Forcing, SF)策略，一种简单有效的对齐方法，使视觉-语言-动作(VLA)模型能够隐式发展空间理解能力，无需依赖显式3D输入或深度估计器。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言-动作(VLA)模型在机器人执行语言指令和精确动作方面显示出强大潜力，但大多数VLA模型基于仅在2D数据上预训练的视觉-语言模型构建，缺乏准确的空间感知能力，限制了它们在3D物理世界中的操作能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单有效的对齐策略，使VLA模型能够在不依赖显式3D输入或深度估计器的情况下隐式发展空间理解能力，从而提高动作精确度。&lt;h4&gt;方法&lt;/h4&gt;提出了空间强制(SF)策略，将VLA的中间视觉嵌入与预训练的3D基础模型产生的几何表示对齐。通过在中间层强制对齐，引导VLA编码更丰富的空间表示。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实环境中的大量实验表明，SF实现了最先进的结果，超越了基于2D和3D的VLA模型。SF最多可加速训练3.8倍，并在多样化的机器人任务中提高了数据效率。&lt;h4&gt;结论&lt;/h4&gt;SF是一种有效的策略，可以增强VLA模型的空间理解能力，不需要显式的3D输入或深度估计器，在性能、训练速度和数据效率方面都有显著提升。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型最近在使机器人能够遵循语言指令并执行精确动作方面显示出强大潜力。然而，大多数VLA模型构建于仅在2D数据上预训练的视觉-语言模型之上，这些模型缺乏准确的空间感知能力，阻碍了它们在3D物理世界中的操作能力。现有解决方案尝试整合显式的3D传感器输入，如深度图或点云，但由于传感器噪声、硬件异构性和现有数据集中深度覆盖不完整，这些方法面临挑战。从2D图像估计3D线索的替代方法也受到深度估计器性能有限的困扰。我们提出了空间强制(SF)，一种简单而有效的对齐策略，隐式地强制VLA模型发展空间理解能力，而不依赖显式3D输入或深度估计器。SF将VLA的中间视觉嵌入与预训练的3D基础模型产生的几何表示对齐。通过在中间层强制对齐，SF引导VLA编码更丰富的空间表示，从而提高动作精确度。在模拟和真实环境中的大量实验表明，SF实现了最先进的结果，超越了基于2D和3D的VLA。SF最多可加速训练3.8倍，并在多样化的机器人任务中提高了数据效率。项目页面位于https://spatial-forcing.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言-行动模型缺乏准确空间感知能力的问题。大多数VLA模型构建于仅在2D数据上预训练的视觉-语言模型之上，难以在3D物理世界中有效操作。这个问题很重要，因为机器人操作需要将语义推理与3D物理世界的精确空间控制相结合，缺乏空间感知能力会阻碍机器人在真实世界中的任务执行。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过深度探测实验发现现有VLA模型的视觉嵌入无法产生有意义的空间结构，从而假设需要隐式地增强模型的空间感知能力。作者借鉴了表示监督（representation supervision）的思路，特别是受到ROSS、REPA等工作的启发，采用表示对齐的方法。同时，作者利用了VGGT作为预训练的3D基础模型来生成空间表示，并借鉴了VLA模型中自回归机制的设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过空间强制（SF）策略隐式地强制VLA模型发展空间理解能力，而不依赖显式的3D输入。实现流程包括：1) 将多视角图像输入到VGGT 3D基础模型生成空间表示；2) 添加位置嵌入保留空间顺序；3) 使用余弦相似度对齐VLA的视觉标记与空间表示；4) 监督较深但不是最深的层（如第24层）；5) 结合动作生成损失和对齐损失进行训练；6) 推理时与标准VLA相同，无额外计算开销。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出空间强制（SF）对齐策略；2) 不依赖显式3D输入或深度估计器；3) 利用VGGT保证多视角一致性；4) 发现特定层（第24层）监督最有效。相比之前工作，SF避免了3D传感器噪声和硬件异构性问题，不依赖深度估计器性能限制，不仅提高了性能，还加速了训练（最高3.8倍）并提高了数据效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了空间强制策略，通过隐式对齐VLA模型的视觉嵌入与3D基础模型的空间表示，在不依赖显式3D输入的情况下，显著提升了模型的空间感知能力、训练效率和数据效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have recently shown strong potential inenabling robots to follow language instructions and execute precise actions.However, most VLAs are built upon vision-language models pretrained solely on2D data, which lack accurate spatial awareness and hinder their ability tooperate in the 3D physical world. Existing solutions attempt to incorporateexplicit 3D sensor inputs such as depth maps or point clouds, but theseapproaches face challenges due to sensor noise, hardware heterogeneity, andincomplete depth coverage in existing datasets. Alternative methods thatestimate 3D cues from 2D images also suffer from the limited performance ofdepth estimators.We propose Spatial Forcing (SF), a simple yet effectivealignment strategy that implicitly forces VLA models to develop spatialcomprehension capabilities without relying on explicit 3D inputs or depthestimators. SF aligns intermediate visual embeddings of VLAs with geometricrepresentations produced by pretrained 3D foundation models. By enforcingalignment at intermediate layers, SF guides VLAs to encode richer spatialrepresentations that enhance action precision.Extensive experiments insimulation and real-world environments demonstrate that SF achievesstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF furtheraccelerates training by up to 3.8x and improves data efficiency across diverserobotic tasks. Project page is at https://spatial-forcing.github.io/</description>
      <author>example@mail.com (Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li)</author>
      <guid isPermaLink="false">2510.12276v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey</title>
      <link>http://arxiv.org/abs/2510.12178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述论文全面介绍了Meta AI的LLaMA系列模型（从LLaMA 1到LLaMA 4）及其参数高效微调(PEFT)方法的发展历程，为机器学习研究人员和实践者提供了一站式资源。&lt;h4&gt;背景&lt;/h4&gt;Meta AI的LLaMA系列模型经历了快速演进，从最初的LLaMA 1发展到LLaMA 4，同时针对这些模型开发了一系列专门的参数高效微调方法。&lt;h4&gt;目的&lt;/h4&gt;提供对LLaMA模型家族和PEFT方法的全面概述，包括模型架构、性能特征、微调方法及其应用，帮助研究人员和实践者了解这一领域的最新进展。&lt;h4&gt;方法&lt;/h4&gt;描述了LLaMA基础模型（参数量从7B-65B到288B）、架构（包括原生多模态和专家混合变体）和关键性能特征；讨论了PEFT概念和五种应用于LLaMA的PEFT方法；分析了模型和适配器架构、参数量和基准测试结果；考察了实际应用案例。&lt;h4&gt;主要发现&lt;/h4&gt;详细讨论了LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor和QLoRA等PEFT方法的机制、参数节省和应用示例；展示了微调后的LLaMA模型在某些任务上优于更大的基线模型；总结了LLaMA模型和PEFT在法律和医疗等领域的成功应用。&lt;h4&gt;结论&lt;/h4&gt;指出了当前面临的挑战和未来研究方向，如扩展到更大的上下文窗口和改进模型鲁棒性等，为后续研究提供了指导方向。&lt;h4&gt;翻译&lt;/h4&gt;本综述回顾了Meta AI的LLaMA（大型语言模型Meta AI）系列的快速演进历程 - 从LLaMA 1到LLaMA 4，以及为这些模型开发的专门参数高效微调(PEFT)方法。我们首先描述了LLaMA基础模型家族（7B-65B到288B参数）、它们的架构（包括原生多模态和专家混合变体）以及关键性能特征。然后我们描述并讨论了PEFT概念，它通过仅更新一小部分参数来适应大型预训练模型，并回顾了五种已应用于LLaMA的PEFT方法：LoRA（低秩自适应）、LLaMA-Adapter V1和V2、LLaMA-Excitor和QLoRA（量化LoRA）。我们讨论了每种方法的机制、参数节省以及在LLaMA上的应用示例（如指令微调、多模态任务）。我们对模型和适配器架构、参数量和基准测试结果进行了结构化讨论和分析（包括微调后的LLaMA模型优于更大基线模型的示例）。最后，我们考察了LLaMA模型和PEFT已成功应用的实际情况（如法律和医疗领域），并讨论了当前面临的挑战和未来研究方向（如扩展到更大的上下文和改进鲁棒性）。这篇综述论文为对LLaMA模型和高效微调策略感兴趣的机器学习研究人员和实践者提供了一站式资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This review surveys the rapid evolution of Meta AI's LLaMA (Large LanguageModel Meta AI) series - from LLaMA 1 through LLaMA 4 and the specializedparameter-efficient fine-tuning (PEFT) methods developed for these models. Wefirst describe the LLaMA family of foundation models (7B-65B to 288Bparameters), their architectures (including native multimodal andMixtureof-Experts variants), and key performance characteristics. We thendescribe and discuss the concept of PEFT, which adapts large pre-trained modelsby updating only a small subset of parameters, and review five PEFT methodsthat have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method'smechanism, parameter savings, and example application to LLaMA (e.g.,instruction tuning, multimodal tasks). We provide structured discussion andanalysis of model and adapter architectures, parameter counts, and benchmarkresults (including examples where fine-tuned LLaMA models outperform largerbaselines). Finally, we examine real-world use cases where LLaMA-based modelsand PEFT have been successfully applied (e.g., legal and medical domains), andwe discuss ongoing challenges and future research directions (such as scalingto even larger contexts and improving robustness). This survey paper provides aone-stop resource for ML researchers and practitioners interested in LLaMAmodels and efficient fine-tuning strategies.</description>
      <author>example@mail.com (Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi)</author>
      <guid isPermaLink="false">2510.12178v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</title>
      <link>http://arxiv.org/abs/2510.12089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个基于DiT的框架和一种无需训练的多角色音频驱动动画方法，解决了现有技术在口型同步、长视频连贯性和多角色动画方面的挑战，实现了高质量、时间连贯且支持多角色的音频驱动视频生成。&lt;h4&gt;背景&lt;/h4&gt;扩散模型的最新进展显著提高了音频驱动人体视频生成的质量和可控性，超越了传统方法。然而，现有方法仍面临口型同步准确性、长视频生成的时间连贯性以及多角色动画的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成任意长度逼真说话视频的框架，以及一种无需训练的多角色音频驱动动画方法，以解决现有技术面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;采用基于LoRA的训练策略结合位置推理方法实现高效长视频生成；结合部分参数更新与奖励反馈增强口型同步和自然身体运动；提出无需训练的Mask分类器自由引导方法用于多角色动画，支持三个或更多角色的音频驱动动画。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法优于现有的最先进方法，以简单、高效和经济的方式实现了高质量、时间连贯且支持多角色的音频驱动视频生成。&lt;h4&gt;结论&lt;/h4&gt;所提出的DiT框架和Mask-CFG方法有效解决了音频驱动人体视频生成中的关键挑战，为高质量、时间连贯的多角色视频生成提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型的最新进展显著提高了音频驱动人体视频生成的质量，在质量和可控性方面超越了传统方法。然而，现有方法仍面临口型同步准确性、长视频生成的时间连贯性以及多角色动画的挑战。在这项工作中，我们提出了一个基于扩散变换器(DiT)的框架，用于生成任意长度的逼真说话视频，并引入了一种无需训练的多角色音频驱动动画方法。首先，我们采用基于LoRA的训练策略结合位置推理方法，使能够高效生成长视频同时保留基础模型能力。此外，我们将部分参数更新与奖励反馈相结合，以增强口型同步和自然的身体运动。最后，我们提出了一种无需训练的方法，即掩码分类器自由引导(Mask-CFG)，用于多角色动画，这不需要专门的数据集或模型修改，并支持三个或更多角色的音频驱动动画。实验结果表明，我们的方法优于现有的最先进方法，以简单、高效和经济的方式实现了高质量、时间连贯且支持多角色的音频驱动视频生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in diffusion models have significantly improved audio-drivenhuman video generation, surpassing traditional methods in both quality andcontrollability. However, existing approaches still face challenges in lip-syncaccuracy, temporal coherence for long video generation, and multi-characteranimation. In this work, we propose a diffusion transformer (DiT)-basedframework for generating lifelike talking videos of arbitrary length, andintroduce a training-free method for multi-character audio-driven animation.First, we employ a LoRA-based training strategy combined with a position shiftinference approach, which enables efficient long video generation whilepreserving the capabilities of the foundation model. Moreover, we combinepartial parameter updates with reward feedback to enhance both lipsynchronization and natural body motion. Finally, we propose a training-freeapproach, Mask Classifier-Free Guidance (Mask-CFG), for multi-characteranimation, which requires no specialized datasets or model modifications andsupports audio-driven animation for three or more characters. Experimentalresults demonstrate that our method outperforms existing state-of-the-artapproaches, achieving high-quality, temporally coherent, and multi-characteraudio-driven video generation in a simple, efficient, and cost-effectivemanner.</description>
      <author>example@mail.com (Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang)</author>
      <guid isPermaLink="false">2510.12089v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Conjecturing: An Overlooked Step in Formal Mathematical Reasoning</title>
      <link>http://arxiv.org/abs/2510.11986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了数学自动形式化中的猜想步骤，创建了ConjectureBench数据集和评估框架，发现LLMs的自动形式化性能被高估，并提出Lean-FIRe方法实现了PutnamBench问题的端到端自动形式化。&lt;h4&gt;背景&lt;/h4&gt;自动形式化通常被视为直接翻译过程，忽略了关键的猜想步骤。许多数学问题需要先做出猜想才能形式化，而LLMs在自动形式化方面已存在困难，且对它们猜想能力的评估有限且常与自动形式化或证明纠缠。&lt;h4&gt;目的&lt;/h4&gt;创建专门评估LLMs猜想能力的框架，既作为独立任务也作为自动形式化流程的一部分，探究猜想对自动形式化的影响，并提出改进方法。&lt;h4&gt;方法&lt;/h4&gt;创建ConjectureBench数据集，重新设计评估框架和指标评估LLMs的猜想能力，设计Lean-FIRe推理时间方法改进猜想和自动形式化性能。&lt;h4&gt;主要发现&lt;/h4&gt;当评估中考虑猜想时，GPT-4.1和DeepSeek-V3.1等基础模型的自动形式化性能被大大高估；Lean-FIRe方法首次实现了PutnamBench中13个问题(GPT-4.1)和7个问题(DeepSeek-V3.1)的端到端自动形式化。&lt;h4&gt;结论&lt;/h4&gt;虽然LLMs拥有生成准确猜想所需的知识，但提高自动形式化性能需要将猜想视为独立任务，并研究如何将其正确整合到自动形式化中。&lt;h4&gt;翻译&lt;/h4&gt;自动形式化是将非正式数学语言表达为正式数学语言的任务，通常被视为直接翻译过程。然而，这忽略了一个关键的先行步骤：猜想。许多数学问题不能直接形式化，需要先做出结论性猜想，如明确答案或特定界限。由于大型语言模型已经难以进行自动形式化，且对其猜想能力的评估有限且常与自动形式化或证明纠缠在一起，理解其影响尤其具有挑战性。为解决这一差距，我们扩充现有数据集创建了ConjectureBench，并重新设计了评估框架和指标，专门用于测量LLMs的猜想能力，既作为独立任务，也作为自动形式化流程的一部分。我们对基础模型的评估（包括GPT-4.1和DeepSeek-V3.1）显示，当评估中考虑猜想时，它们的自动形式化性能被大大高估。然而，不应假设猜想会预先提供。我们设计了一种推理时间方法Lean-FIRe来改进猜想和自动形式化，据我们所知，这首次实现了GPT-4.1对13个PutnamBench问题和DeepSeek-V3.1对7个问题的端到端自动形式化。我们证明，虽然LLMs拥有生成准确猜想所需的知识，但提高自动形式化性能需要将猜想视为独立任务，并进一步研究如何将其正确整合到自动形式化中。最后，我们提供前瞻性指导，引导未来研究关注改进猜想这一被忽视的正式数学推理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autoformalisation, the task of expressing informal mathematical statements informal language, is often viewed as a direct translation process. This,however, disregards a critical preceding step: conjecturing. Many mathematicalproblems cannot be formalised directly without first conjecturing a conclusionsuch as an explicit answer, or a specific bound. Since Large Language Models(LLMs) already struggle with autoformalisation, and the evaluation of theirconjecturing ability is limited and often entangled within autoformalisation orproof, it is particularly challenging to understand its effect. To address thisgap, we augment existing datasets to create ConjectureBench, and redesign theevaluation framework and metric specifically to measure the conjecturingcapabilities of LLMs both as a distinct task and within the autoformalisationpipeline. Our evaluation of foundational models, including GPT-4.1 andDeepSeek-V3.1, reveals that their autoformalisation performance issubstantially overestimated when the conjecture is accounted for duringevaluation. However, the conjecture should not be assumed to be provided. Wedesign an inference-time method, Lean-FIRe to improve conjecturing andautoformalisation, which, to the best of our knowledge, achieves the firstsuccessful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisiteknowledge to generate accurate conjectures, improving autoformalisationperformance requires treating conjecturing as an independent task, andinvestigating further how to correctly integrate it within autoformalisation.Finally, we provide forward-looking guidance to steer future research towardimproving conjecturing, an overlooked step of formal mathematical reasoning.</description>
      <author>example@mail.com (Jasivan Alex Sivakumar, Philipp Borchert, Ronald Cardenas, Gerasimos Lampouras)</author>
      <guid isPermaLink="false">2510.11986v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities</title>
      <link>http://arxiv.org/abs/2510.11842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at 39th Conference on Neural Information Processing Systems  (NeurIPS 2025) Workshop on Continual and Compatible Foundation Model Updates  (CCFM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了在语言模型持续预训练适应新任务时，如何平衡任务性能和知识保留的问题，特别关注了重放比率配置与计算预算之间的相互作用，并提供了基于实证的选择指南。&lt;h4&gt;背景&lt;/h4&gt;通过持续预训练使语言模型适应新任务面临一个基本权衡：模型必须学习新能力，同时避免对现有知识的灾难性遗忘。先前的研究已经研究了合成数据生成技术，但在计算约束下平衡任务性能和知识保留的最佳重放比率仍不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究重放比率配置与计算预算在语言模型适应新任务时的相互作用，分析不同总令牌预算和重放比率配置对任务掌握和通用知识保留的影响，并提供基于实证的重放比率选择指南。&lt;h4&gt;方法&lt;/h4&gt;使用bAbI推理任务作为目标，应用合成数据生成方法，系统地评估不同的总令牌预算和重放比率配置，分析它们对任务掌握和通用知识保留的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验揭示了一种最优配置，能够平衡特定任务性能与通用知识保留。基于研究发现，研究提供了基于计算预算选择重放比率的实证指导，使实践者能够在显著降低训练成本的情况下实现强大的任务适应。&lt;h4&gt;结论&lt;/h4&gt;通过合理配置重放比率，实践者可以在有限的计算预算下实现有效的任务适应，同时保持模型的通用知识，从而显著降低训练成本。&lt;h4&gt;翻译&lt;/h4&gt;通过持续预训练使语言模型适应新任务面临一个基本权衡：模型必须学习新能力，同时避免对现有知识的灾难性遗忘。虽然先前的工作已经研究了合成数据生成技术，但在计算约束下平衡任务性能和知识保留的最佳重放比率仍然知之甚少。我们提出了一个全面的实证研究，调查了在将语言模型适应新任务时，重放比率配置与计算预算之间的相互作用。使用bAbI推理任务作为我们的目标，我们应用合成数据生成方法，并系统评估不同的总令牌预算和重放比率配置。我们分析了它们对任务掌握和通用知识保留的影响。我们的实验揭示了一种平衡特定任务性能与通用知识保留的最优配置。基于我们的发现，我们提供了基于计算预算选择重放比率的实证指导，使实践者能够在显著降低训练成本的情况下实现强大的任务适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adapting language models to new tasks through continued pretraining faces afundamental trade-off: models must learn new capabilities while avoidingcatastrophic forgetting of existing knowledge. While prior work has studiedsynthetic data generation techniques, the optimal replay ratios for balancingtask performance and knowledge retention under computational constraints remainpoorly understood. We present a comprehensive empirical study investigating theinterplay between replay ratio configuration and computational budget whenadapting language models to new tasks. Using the bAbI reasoning tasks as ourtarget objective, we apply synthetic data generation and systematicallyevaluate different total token budgets and replay ratio configurations. Weanalyze their effects on both task mastery and general knowledge retention. Ourexperiments reveal an optimal configuration that balances task-specificperformance with general knowledge retention. Based on our findings, we provideempirically-grounded guidelines for selecting replay ratios based oncomputational budget, enabling practitioners to achieve strong task adaptationwith significantly reduced training costs.</description>
      <author>example@mail.com (Urs Spiegelhalter, Jörg K. H. Franke, Frank Hutter)</author>
      <guid isPermaLink="false">2510.11842v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping</title>
      <link>http://arxiv.org/abs/2510.11576v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  currently being reviewed for WHISPERS conference ( Workshop on  Hyperspectral Image and Signal Processing: Evolution in Remote Sensing )&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了三种基础模型在高光谱作物制图中的性能，发现SpectralEarth预训练模型表现最佳，准确率达到93.5%，强调了模型架构在跨区域和传感器平台泛化能力中的重要性。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在改变地球观测领域，但它们在高光谱作物制图方面的潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;对比评估三种基础模型（HyperSigma、DOFA和SpectralEarth数据集预训练的Vision Transformers）用于高光谱作物制图的性能。&lt;h4&gt;方法&lt;/h4&gt;在训练区域的手动标记数据上对模型进行微调，在独立的测试区域评估模型性能，使用总体准确率、平均准确率和F1分数作为性能指标。&lt;h4&gt;主要发现&lt;/h4&gt;HyperSigma的OA为34.5%（+/- 1.8%），DOFA达到62.6%（+/- 3.5%），SpectralEarth模型达到93.5%的OA（+/- 0.8%）；从头开始训练的紧凑型SpectralEarth变体达到91%的准确率，突显了模型架构对跨区域和传感器平台泛化能力的重要性。&lt;h4&gt;结论&lt;/h4&gt;这些结果为基础模型用于实际高光谱作物制图提供了系统评估，为未来模型开发指明了方向。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在改变地球观测，但它们在高光谱作物制图方面的潜力尚未被充分探索。本研究使用高光谱图像对三种基础模型（HyperSigma、DOFA和SpectralEarth数据集预训练的Vision Transformers）进行了基准测试，用于谷物作物制图。模型在训练区域的手动标记数据上进行了微调，并在独立的测试区域进行了评估。性能通过总体准确率、平均准确率和F1分数进行衡量。HyperSigma达到34.5%的OA（+/- 1.8%），DOFA达到62.6%（+/- 3.5%），SpectralEarth模型达到93.5%的OA（+/- 0.8%）。从零开始训练的紧凑型SpectralEarth变体达到91%，突显了模型架构对于在地理区域和传感器平台间实现强泛化能力的重要性。这些结果为基础模型用于实际高光谱作物制图提供了系统评估，并概述了未来模型开发的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are transforming Earth observation, but their potential forhyperspectral crop mapping remains underexplored. This study benchmarks threefoundation models for cereal crop mapping using hyperspectral imagery:HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarthdataset (a large multitemporal hyperspectral archive). Models were fine-tunedon manually labeled data from a training region and evaluated on an independenttest region. Performance was measured with overall accuracy (OA), averageaccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved91%, highlighting the importance of model architecture for stronggeneralization across geographic regions and sensor platforms. These resultsprovide a systematic evaluation of foundation models for operationalhyperspectral crop mapping and outline directions for future model development.</description>
      <author>example@mail.com (Walid Elbarz, Mohamed Bourriz, Hicham Hajji, Hamd Ait Abdelali, François Bourzeix)</author>
      <guid isPermaLink="false">2510.11576v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.12796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DriveVLA-W0是一种通过世界模型预测未来图像的训练范式，解决了VLA模型的监督不足问题，显著提升了驾驶智能性能，并随着数据量增加性能提升加速。&lt;h4&gt;背景&lt;/h4&gt;在大型数据上扩展视觉-语言-行动(VLA)模型是实现更通用驾驶智能的有前景路径，但VLA模型受到监督不足限制：模型容量大但仅由稀疏、低维度的行动监督，导致大部分表征能力未被利用。&lt;h4&gt;目的&lt;/h4&gt;解决VLA模型的监督不足问题，使模型能够更好地学习驾驶环境的基本动态，提高驾驶智能的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出DriveVLA-W0训练范式，采用世界模型预测未来图像生成密集自监督信号；为两种VLA架构实现：离散视觉令牌的自回归世界模型和连续视觉特征的扩散世界模型；引入轻量级行动专家解决实时部署推理延迟问题。&lt;h4&gt;主要发现&lt;/h4&gt;在NAVSIM v1/v2基准测试和680倍大的内部数据集上，DriveVLA-W0显著优于BEV和VLA基线；放大了数据扩展定律，表明随着训练数据集大小增加，性能提升加速。&lt;h4&gt;结论&lt;/h4&gt;DriveVLA-W0通过世界模型生成密集自监督信号有效解决VLA模型监督不足问题，提高模型表征能力同时通过轻量级行动专家解决实时部署问题，具有良好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;在大型数据上扩展视觉-语言-行动(VLA)模型是实现更通用驾驶智能的有前景路径。然而，VLA模型受到监督不足的限制：模型容量巨大但仅由稀疏、低维度的行动监督，导致其大部分表征能力未被充分利用。为解决此问题，我们提出DriveVLA-W0训练范式，采用世界模型来预测未来图像。此任务生成密集的自监督信号，迫使模型学习驾驶环境的基本动态。我们通过为两种主导的VLA架构实现该范式来展示其多功能性：用于使用离散视觉令牌的VLA的自回归世界模型，以及用于在连续视觉特征上操作的VLA的扩散世界模型。基于从世界模型学到的丰富表征，我们引入轻量级行动专家以解决实时部署的推理延迟问题。在NAVSIM v1/v2基准测试和680倍大的内部数据集上进行的大量实验表明，DriveVLA-W0显著优于BEV和VLA基线。关键是，它放大了数据扩展定律，表明随着训练数据集大小的增加，性能提升会加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling Vision-Language-Action (VLA) models on large-scale data offers apromising path to achieving a more generalized driving intelligence. However,VLA models are limited by a ``supervision deficit'': the vast model capacity issupervised by sparse, low-dimensional actions, leaving much of theirrepresentational power underutilized. To remedy this, we propose\textbf{DriveVLA-W0}, a training paradigm that employs world modeling topredict future images. This task generates a dense, self-supervised signal thatcompels the model to learn the underlying dynamics of the driving environment.We showcase the paradigm's versatility by instantiating it for two dominant VLAarchetypes: an autoregressive world model for VLAs that use discrete visualtokens, and a diffusion world model for those operating on continuous visualfeatures. Building on the rich representations learned from world modeling, weintroduce a lightweight action expert to address the inference latency forreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a680x larger in-house dataset demonstrate that DriveVLA-W0 significantlyoutperforms BEV and VLA baselines. Crucially, it amplifies the data scalinglaw, showing that performance gains accelerate as the training dataset sizeincreases.</description>
      <author>example@mail.com (Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang)</author>
      <guid isPermaLink="false">2510.12796v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models</title>
      <link>http://arxiv.org/abs/2510.12618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解耦方法，利用基础推理模型(FIMs)来识别高维动态系统中的潜在变量，通过冻结FIM权重并只训练编码器-解码器映射，实现稳定高效的表示学习。&lt;h4&gt;背景&lt;/h4&gt;高维动态过程通常由更小的有效变量集合表征，这些变量在低维流形上演化。识别这些潜在动态需要解决两个交织的问题：发现适当的粗粒度变量和同时拟合控制方程。&lt;h4&gt;目的&lt;/h4&gt;将变量发现和动态拟合两个问题解耦，利用预训练的基础推理模型(FIMs)简化高维动态系统的分析过程。&lt;h4&gt;方法&lt;/h4&gt;通过利用预训练的基础推理模型(FIMs)来估计动态系统的无穷小生成器，使用具有冻结权重的FIM来推断动态，同时只训练编码器-解码器映射，定义了一个简单、模拟一致的损失函数来稳定表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在具有半圆扩散的随机双阱系统（嵌入到合成视频数据中）的概念验证实验表明，该方法具有快速和可重用的粗粒度处理流程的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过解耦变量发现和动态拟合的问题，并利用预训练的FIMs，提出了一种稳定且高效的表示学习方法，适用于高维动态系统的粗粒度化。&lt;h4&gt;翻译&lt;/h4&gt;高维动态过程的记录通常由更小的有效变量集合表征，这些变量在低维流形上演化。识别这些潜在动态需要解决两个交织的问题：发现适当的粗粒度变量和同时拟合控制方程。大多数机器学习方法通过联合训练自动编码器和强制动态一致性的模型来解决这些任务。我们提出通过利用最近引入的基础推理模型(FIMs)来解耦这两个问题。FIMs是预训练模型，可以零样本模式估计动态系统的无穷小生成器（例如随机微分方程的漂移和扩散）。通过使用具有冻结权重的FIM来推断动态，并且只训练编码器-解码器映射，我们定义了一个简单、模拟一致的损失函数，稳定了表示学习。在一个具有半圆扩散的随机双阱系统上进行的嵌入合成视频数据的概念证明，展示了这种方法在快速和可重用的粗粒度处理流程方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional recordings of dynamical processes are often characterized bya much smaller set of effective variables, evolving on low-dimensionalmanifolds. Identifying these latent dynamics requires solving two intertwinedproblems: discovering appropriate coarse-grained variables and simultaneouslyfitting the governing equations. Most machine learning approaches tackle thesetasks jointly by training autoencoders together with models that enforcedynamical consistency. We propose to decouple the two problems by leveragingthe recently introduced Foundation Inference Models (FIMs). FIMs are pretrainedmodels that estimate the infinitesimal generators of dynamical systems (e.g.,the drift and diffusion of a stochastic differential equation) in zero-shotmode. By amortizing the inference of the dynamics through a FIM with frozenweights, and training only the encoder-decoder map, we define a simple,simulation-consistent loss that stabilizes representation learning. A proof ofconcept on a stochastic double-well system with semicircle diffusion, embeddedinto synthetic video data, illustrates the potential of this approach for fastand reusable coarse-graining pipelines.</description>
      <author>example@mail.com (Manuel Hinz, Maximilian Mauel, Patrick Seifner, David Berghaus, Kostadin Cvejoski, Ramses J. Sanchez)</author>
      <guid isPermaLink="false">2510.12618v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>LayerSync: Self-aligning Intermediate Layers</title>
      <link>http://arxiv.org/abs/2510.12581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerSync是一种领域无关的方法，用于提高扩散模型的生成质量和训练效率。它通过利用模型自身的中间表示来正则化模型，减少对外部监督的需求，是一种自给自足、即插即用的正则化项，不需要预训练模型或额外数据，可应用于多种模态。&lt;h4&gt;背景&lt;/h4&gt;先前的研究已经指出扩散模型的生成质量与模型学习的表示之间存在联系，表明对模型中间表示的外部指导可以加速训练。然而，扩散模型不同层的表示质量存在差异。&lt;h4&gt;目的&lt;/h4&gt;重新构想扩散模型的训练范式，通过利用模型自身的中间表示来正则化模型，从而减少对外部监督的需求，提高生成质量和训练效率。&lt;h4&gt;方法&lt;/h4&gt;LayerSync基于扩散模型不同层表示质量不同的观察，将语义最丰富的表示作为较弱表示的内在指导。这是一种自给自足、即插即用的正则化项，不需要在扩散模型训练中增加额外开销，可以推广到视觉领域以外的其他模态。&lt;h4&gt;主要发现&lt;/h4&gt;LayerSync不需要预训练模型或额外数据；在图像生成上进行了广泛评估，并展示了其在音频、视频和运动生成等其他领域的适用性；它持续提高了生成质量和训练效率；例如，在ImageNet数据集上将基于流的变压器的训练速度提高了8.75倍以上，并将生成质量提高了23.6%。&lt;h4&gt;结论&lt;/h4&gt;LayerSync是一种有效的方法，可以提高扩散模型的生成质量和训练效率，适用于多种模态，且不需要额外的计算资源或数据。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LayerSync，一种领域无关的方法，用于提高扩散模型的生成质量和训练效率。先前的研究已经强调了扩散模型生成的质量与模型学习的表示之间的联系，表明对模型中间表示的外部指导可以加速训练。我们通过用模型的自身中间表示来正则化扩散模型，重新构想了这一范式。基于扩散模型不同层的表示质量存在差异的观察，我们证明语义最丰富的表示可以作为较弱表示的内在指导，减少对外部监督的需求。我们的方法LayerSync是一种自给自足、即插即用的正则化项，在扩散模型训练中没有额外开销，并且可以推广到视觉领域以外的其他模态。LayerSync不需要预训练模型或额外数据。我们在图像生成上广泛评估了该方法，并展示了其在音频、视频和运动生成等其他领域的适用性。我们表明它持续提高了生成质量和训练效率。例如，我们在ImageNet数据集上将基于流变压器的训练速度提高了8.75倍以上，并将生成质量提高了23.6%。代码可在https://github.com/vita-epfl/LayerSync获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose LayerSync, a domain-agnostic approach for improving the generationquality and the training efficiency of diffusion models. Prior studies havehighlighted the connection between the quality of generation and therepresentations learned by diffusion models, showing that external guidance onmodel intermediate representations accelerates training. We reconceptualizethis paradigm by regularizing diffusion models with their own intermediaterepresentations. Building on the observation that representation quality variesacross diffusion model layers, we show that the most semantically richrepresentations can act as an intrinsic guidance for weaker ones, reducing theneed for external supervision. Our approach, LayerSync, is a self-sufficient,plug-and-play regularizer term with no overhead on diffusion model training andgeneralizes beyond the visual domain to other modalities. LayerSync requires nopretrained models nor additional data. We extensively evaluate the method onimage generation and demonstrate its applicability to other domains such asaudio, video, and motion generation. We show that it consistently improves thegeneration quality and the training efficiency. For example, we speed up thetraining of flow-based transformer by over 8.75x on ImageNet dataset andimproved the generation quality by 23.6%. The code is available athttps://github.com/vita-epfl/LayerSync.</description>
      <author>example@mail.com (Yasaman Haghighi, Bastien van Delft, Mariam Hassan, Alexandre Alahi)</author>
      <guid isPermaLink="false">2510.12581v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>I-DCCRN-VAE: An Improved Deep Representation Learning Framework for Complex VAE-based Single-channel Speech Enhancement</title>
      <link>http://arxiv.org/abs/2510.12485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的DCCRN-VAE单通道语音增强系统，通过移除预训练VAE中的跳跃连接、使用β-VAE进行预训练、以及让NSVAE同时生成语音和噪声潜在表示，提高了系统在不匹配数据集上的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;最近提出了一种基于复杂变分自编码器(VAE)的单通道语音增强系统，该系统基于DCCRN架构。在这个系统中，噪声抑制VAE(NSVAE)使用预训练的干净语音和噪声VAE以及跳跃连接来从嘈杂语音中提取干净语音表示。&lt;h4&gt;目的&lt;/h4&gt;改进DCCRN-VAE系统，提高其在不同数据集上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过三个关键改进：1) 移除预训练VAE中的跳跃连接，以鼓励更具信息性的语音和噪声潜在表示；2) 在预训练中使用β-VAE，以更好地平衡重建和潜在空间正则化；3) NSVAE同时生成语音和噪声潜在表示。&lt;h4&gt;主要发现&lt;/h4&gt;在匹配的DNS3数据集上，所提出的系统与DCCRN和DCCRN-VAE基线实现了相当的性能；在不匹配的数据集(WSJ0-QUT, Voicebank-DEMEND)上，优于基线，显示出改进的泛化能力；消融研究表明，可以使用传统的微调而非对抗训练实现类似性能，从而简化训练流程。&lt;h4&gt;结论&lt;/h4&gt;所提出的改进提高了系统在不匹配数据集上的泛化能力；简化的训练流程(使用传统微调而非对抗训练)也能获得类似性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，提出了一种基于复杂变分自编码器(VAE)的基于DCCRN架构的单通道语音增强系统。在该系统中，噪声抑制VAE(NSVAE)使用预训练的干净语音和噪声VAE以及跳跃连接，从嘈杂语音中学习提取干净语音表示。在本文中，我们通过三个关键改进来改进DCCRN-VAE：1) 移除预训练VAE中的跳跃连接，以鼓励更具信息性的语音和噪声潜在表示；2) 在预训练中使用β-VAE，以更好地平衡重建和潜在空间正则化；3) NSVAE生成语音和噪声潜在表示。实验表明，所提出的系统在匹配的DNS3数据集上实现了与DCCRN和DCCRN-VAE基线相当的性能，但在不匹配的数据集(WSJ0-QUT, Voicebank-DEMEND)上优于基线，显示出改进的泛化能力。此外，消融研究表明，可以使用传统的微调而非对抗训练实现类似性能，从而简化训练流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, a complex variational autoencoder (VAE)-based single-channel speechenhancement system based on the DCCRN architecture has been proposed. In thissystem, a noise suppression VAE (NSVAE) learns to extract clean speechrepresentations from noisy speech using pretrained clean speech and noise VAEswith skip connections. In this paper, we improve DCCRN-VAE by incorporatingthree key modifications: 1) removing the skip connections in the pretrainedVAEs to encourage more informative speech and noise latent representations; 2)using $\beta$-VAE in pretraining to better balance reconstruction and latentspace regularization; and 3) a NSVAE generating both speech and noise latentrepresentations. Experiments show that the proposed system achieves comparableperformance as the DCCRN and DCCRN-VAE baselines on the matched DNS3 datasetbut outperforms the baselines on mismatched datasets (WSJ0-QUT,Voicebank-DEMEND), demonstrating improved generalization ability. In addition,an ablation study shows that a similar performance can be achieved withclassical fine-tuning instead of adversarial training, resulting in a simplertraining pipeline.</description>
      <author>example@mail.com (Jiatong Li, Simon Doclo)</author>
      <guid isPermaLink="false">2510.12485v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression</title>
      <link>http://arxiv.org/abs/2510.12474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SMEC的新型训练框架，用于压缩大型语言模型的高维嵌入向量，解决了高维度带来的计算复杂度和存储需求问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型生成的高维嵌入向量虽然能捕捉丰富的语义和句法信息，但高维度加剧了计算复杂度和存储需求，阻碍了实际部署。&lt;h4&gt;目的&lt;/h4&gt;解决高维嵌入带来的计算复杂度和存储需求问题，实现有效的维度压缩而不损失性能。&lt;h4&gt;方法&lt;/h4&gt;提出Sequential Matryoshka Embedding Compression (SMEC)框架，包含三个核心组件：Sequential Matryoshka Representation Learning (SMRL)方法减轻训练中的梯度方差，Adaptive Dimension Selection (ADS)模块减少维度修剪时的信息损失，Selectable Cross-batch Memory (S-XBM)模块增强高维和低维嵌入间的无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;在图像、文本和多模态数据集上的实验表明，SMEC能在显著降低维度的同时保持性能。在BEIR数据集上，与Matryoshka-Adaptor和Search-Adaptor模型相比，SMEC将压缩后的LLM2Vec嵌入向量(256维)的性能分别提高了1.1分和2.7分。&lt;h4&gt;结论&lt;/h4&gt;SMEC框架能够有效压缩大型语言模型的高维嵌入向量，在减少计算复杂度和存储需求的同时保持或提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型生成的高维嵌入向量能够捕捉丰富的语义和句法信息。然而，高维嵌入向量加剧了计算复杂度和存储需求，从而阻碍了实际部署。为解决这些挑战，我们提出了一种名为Sequential Matryoshka Embedding Compression (SMEC)的新型训练框架。该框架引入了Sequential Matryoshka Representation Learning (SMRL)方法来减轻训练过程中的梯度方差，Adaptive Dimension Selection (ADS)模块来减少维度修剪过程中的信息损失，以及Selectable Cross-batch Memory (S-XBM)模块来增强高维和低维嵌入之间的无监督学习。在图像、文本和多模态数据集上的实验表明，SMEC在显著降低维度的同时保持了性能。例如，在BEIR数据集上，与Matryoshka-Adaptor和Search-Adaptor模型相比，我们的方法将压缩后的LLM2Vec嵌入向量(256维)的性能分别提高了1.1分和2.7分。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) generate high-dimensional embeddings thatcapture rich semantic and syntactic information. However, high-dimensionalembeddings exacerbate computational complexity and storage requirements,thereby hindering practical deployment. To address these challenges, we proposea novel training framework named Sequential Matryoshka Embedding Compression(SMEC). This framework introduces the Sequential Matryoshka RepresentationLearning(SMRL) method to mitigate gradient variance during training, theAdaptive Dimension Selection (ADS) module to reduce information degradationduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) moduleto enhance unsupervised learning between high- and low-dimensional embeddings.Experiments on image, text, and multimodal datasets demonstrate that SMECachieves significant dimensionality reduction while maintaining performance.For instance, on the BEIR dataset, our approach improves the performance ofcompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 pointscompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.</description>
      <author>example@mail.com (Biao Zhang, Lixin Chen, Tong Liu, Bo Zheng)</author>
      <guid isPermaLink="false">2510.12474v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Deep SPI: Safe Policy Improvement via World Models</title>
      <link>http://arxiv.org/abs/2510.12312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages main text, 17 pages appendix (excluding references)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究在线强化学习环境下的安全策略改进(SPI)理论，结合世界模型和表示学习，提出DeepSPI算法，在保持理论保证的同时实现了优异性能。&lt;h4&gt;背景&lt;/h4&gt;现有SPI保证主要关注离线、表格强化学习环境，缺乏在线设置下的理论支持。&lt;h4&gt;目的&lt;/h4&gt;开发理论框架，限制策略更新到当前策略的明确定义邻域，确保策略单调改进和收敛。&lt;h4&gt;方法&lt;/h4&gt;分析转换和奖励预测损失与表示质量的关系，开发在线、深度版本的经典SPI定理，提出DeepSPI算法，结合局部转换和奖励损失与正则化策略更新。&lt;h4&gt;主要发现&lt;/h4&gt;限制策略更新到当前策略的邻域可以确保单调改进和收敛；转换和奖励预测损失与表示质量相关联；DeepSPI在ALE-57基准测试中匹配或超过PPO和DeepMDPs等强基线方法。&lt;h4&gt;结论&lt;/h4&gt;DeepSPI算法在保持理论保证的同时，在实际应用中展现出与最先进方法相当或更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;安全策略改进(SPI)为策略更新提供了理论控制，但现有保证主要涉及离线、表格强化学习(RL)。我们研究结合世界模型和表示学习的在线设置下的SPI。我们开发了一个理论框架，显示将策略更新限制在当前策略的明确定义邻域内可以确保单调改进和收敛。该分析将转换和奖励预测损失与表示质量联系起来，产生了来自离线RL文献的经典SPI定理的在线、'深度'类比。基于这些结果，我们引入了DeepSPI，一种将局部转换和奖励损失与正则化策略更新相结合的原则性在线策略算法。在ALE-57基准测试中，DeepSPI匹配或超过了包括PPO和DeepMDPs在内的强基线方法，同时保留了理论保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safe policy improvement (SPI) offers theoretical control over policy updates,yet existing guarantees largely concern offline, tabular reinforcement learning(RL). We study SPI in general online settings, when combined with world modeland representation learning. We develop a theoretical framework showing thatrestricting policy updates to a well-defined neighborhood of the current policyensures monotonic improvement and convergence. This analysis links transitionand reward prediction losses to representation quality, yielding online, "deep"analogues of classical SPI theorems from the offline RL literature. Building onthese results, we introduce DeepSPI, a principled on-policy algorithm thatcouples local transition and reward losses with regularised policy updates. Onthe ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, includingPPO and DeepMDPs, while retaining theoretical guarantees.</description>
      <author>example@mail.com (Florent Delgrange, Raphael Avalos, Willem Röpke)</author>
      <guid isPermaLink="false">2510.12312v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs</title>
      <link>http://arxiv.org/abs/2510.12233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了IMDGA框架，一种统一的多维图攻击方法，针对图神经网络与大型语言模型结合的文本属性图，通过协调图结构和文本特征的多层次扰动，实现对Graph-LLMs的有效攻击，同时保持高度可解释性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为建模图结构数据的关键框架，通过整合大型语言模型，文本属性图利用丰富的文本语义显著提高了图学习能力。然而，这种协同作用也引入了关键漏洞，使Graph-LLMs容易受到对其结构拓扑和文本属性的对抗攻击。&lt;h4&gt;目的&lt;/h4&gt;虽然已有针对结构拓扑和文本属性的专门攻击方法，但缺乏统一的多维攻击框架。本研究旨在提出一种同时考虑图结构和文本特征的对抗攻击方法，并平衡攻击效果与可解释性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了可解释的多维图攻击（IMDGA）框架，该框架设计用于协调图结构和文本特征的多层次扰动。IMDGA利用三个紧密集成的模块构建攻击，平衡可解释性和影响力，帮助更深入理解Graph-LLM的漏洞。&lt;h4&gt;主要发现&lt;/h4&gt;通过在不同数据集和架构上的理论分析和实证评估，IMDGA在可解释性、攻击有效性、隐蔽性和鲁棒性方面均优于现有方法。研究揭示了TAG表示学习中的关键弱点，发现了Graph-LLMs中一个先前未被充分探索的语义维度漏洞。&lt;h4&gt;结论&lt;/h4&gt;通过暴露Graph-LLMs中的关键弱点，这项工作为提高系统弹性提供了有价值的见解，相关代码和资源已公开分享。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为建模图结构数据的关键框架，应用于从社交网络分析到分子化学的广泛领域。通过整合大型语言模型，文本属性图利用丰富的文本语义增强节点表示，显著提高了基于图的学习能力。然而，这种复杂的协同作用引入了关键漏洞，因为图-LLMs容易受到对其结构拓扑和文本属性的对抗攻击。虽然已经为这些方面的每个方面设计了专门的攻击方法，但还没有将它们统一为全面的方法。在本工作中，我们提出了可解释的多维图攻击（IMDGA），这是一种新型的人本主义对抗攻击框架，旨在协调图结构和文本特征的多层次扰动。IMDGA利用三个紧密集成的模块来构建攻击，平衡可解释性和影响力，使人们能够更深入地理解Graph-LLM的漏洞。通过在不同数据集和架构上进行严格的理论分析和全面的实证评估，IMDGA显示出比现有方法更好的可解释性、攻击有效性、隐蔽性和鲁棒性。通过揭示TAG表示学习中的关键弱点，这项工作揭示了Graph-LLMs中一个先前未被充分探索的语义维度漏洞，为提高它们的弹性提供了有价值的见解。我们的代码和资源已在https://anonymous.4open.science/r/IMDGA-7289公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a pivotal framework for modelinggraph-structured data, enabling a wide range of applications from socialnetwork analysis to molecular chemistry. By integrating large language models(LLMs), text-attributed graphs (TAGs) enhance node representations with richtextual semantics, significantly boosting the expressive power of graph-basedlearning. However, this sophisticated synergy introduces criticalvulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on boththeir structural topology and textual attributes. Although specialized attackmethods have been designed for each of these aspects, no work has yet unifiedthem into a comprehensive approach. In this work, we propose the InterpretableMulti-Dimensional Graph Attack (IMDGA), a novel human-centric adversarialattack framework designed to orchestrate multi-level perturbations across bothgraph structure and textual features. IMDGA utilizes three tightly integratedmodules to craft attacks that balance interpretability and impact, enabling adeeper understanding of Graph-LLM vulnerabilities. Through rigorous theoreticalanalysis and comprehensive empirical evaluations on diverse datasets andarchitectures, IMDGA demonstrates superior interpretability, attackeffectiveness, stealthiness, and robustness compared to existing methods. Byexposing critical weaknesses in TAG representation learning, this work uncoversa previously underexplored semantic dimension of vulnerability in Graph-LLMs,offering valuable insights for improving their resilience. Our code andresources are publicly available athttps://anonymous.4open.science/r/IMDGA-7289.</description>
      <author>example@mail.com (Bowen Fan, Zhilin Guo, Xunkai Li, Yihan Zhou, Bing Zhou, Zhenjun Li, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2510.12233v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification</title>
      <link>http://arxiv.org/abs/2510.12214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE BIBM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DE3S的医疗早期时间序列分类方法，通过双增强软稀疏形状学习解决了医疗场景中早期分类面临的准确性和早期性权衡问题。&lt;h4&gt;背景&lt;/h4&gt;早期时间序列分类在医疗应用中至关重要，特别是在ICU败血症预测等时间敏感场景中，延迟预测会导致大量死亡。ETSC可提高ICU资源利用效率和医疗精准度，但面临初始信号弱和类别不平衡的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决ETSC中准确性和早期性的冲突目标，捕捉早期细微模式，找到具有高可解释性的区分性子序列（形状）。&lt;h4&gt;方法&lt;/h4&gt;提出DE3S方法，包含三个创新：1)结合传统时间增强和基于注意力的全局时间增强的双增强策略；2)基于注意力分数的软形状稀疏化机制；3)双路径MoE和Inception模块融合架构。使用加权交叉熵损失处理类别不平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实医疗数据集上进行了广泛实验，结果显示了最先进的性能，消融研究证实了各组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;DE3S方法成功解决了医疗早期时间序列分类中的准确性和早期性权衡问题，能够有效捕捉早期细微模式，提高ICU资源利用效率和医疗精准度。&lt;h4&gt;翻译&lt;/h4&gt;医疗应用中的早期时间序列分类(ETSC)对于ICU中败血症预测等时间敏感场景至关重要，大量死亡是由延迟预测引起的。ETSC可以显著提高ICU资源利用效率和医疗精准度。然而，它面临准确性和早期性的冲突目标，现有方法常常在两者之间权衡，由于初始信号弱和类别不平衡，难以捕捉早期的细微模式。解决这些挑战的关键是找到具有高可解释性的区分性子序列（或形状）。本文提出了用于医疗早期时间序列分类的双增强软稀疏形状学习(DE3S)，它引入了一种新的双增强软形状学习框架，通过三个创新精确找出形状：1)结合传统时间增强和基于注意力的全局时间增强的全面双增强策略，实现鲁棒的表示学习；2)基于注意力分数的软形状稀疏化机制，动态保留区分性模式，同时将不太重要的形状聚合成代表性标记；3)双路径专家混合网络(MoE)和Inception模块融合架构，其中MoE在形状内执行局部学习，多尺度Inception模块跨形状捕获全局模式。该框架使用加权交叉熵损失处理类别不平衡，并在主体一致性数据集上表现出鲁棒性。在六个真实医疗数据集上的广泛实验显示了最先进的性能，消融研究证实了组件的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early time-series classification (ETSC) in medical applications is crucialfor time-sensitive scenarios such as sepsis prediction in intensive care units(ICUs), where a large number of deaths are caused by delayed prediction. ETSCcan significantly improve ICU resource utilization efficiency and healthcareprecision. However, it faces conflicting goals of accuracy and earliness, withexisting methods often trading one for the other, struggling to capture subtleearly-stage patterns due to weak initial signals and class imbalance. The keyto solve these challenges is to find shapelets, which are discriminativesubsequences (or shapes) with high interpretability in time-seriesclassification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learningfor Medical Early Time-Series Classification (DE3S), which introduces a novelDual-Enhanced Soft-Shape Learning framework to figure out shapelets preciselythrough three innovations: (1) a comprehensive dual-enhancement strategycombines traditional temporal augmentation with attention-based global temporalenhancement for robust representation learning, (2) an attention-score-basedsoft shapelet sparsification mechanism dynamically preserves discriminativepatterns while aggregating less important shapelets into representative tokens,and (3) a dual-path Mixture of Experts Network (MoE) and Inception modulesfusion architecture where MoE performs local learning within shapelets andmulti-scale Inception modules capture global patterns across shapelets. Theframework employs weighted cross-entropy loss for class imbalance handling anddemonstrates robustness on subject-consistency datasets. Extensive experimentson six real-world medical datasets show state-of-the-art performance, withablation studies confirming component efficacy.</description>
      <author>example@mail.com (Tao Xie, Zexi Tan, Haoyi Xiao, Binbin Sun, Yiqun Zhang)</author>
      <guid isPermaLink="false">2510.12214v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>SDGraph: Multi-Level Sketch Representation Learning by Sparse-Dense Graph Architecture</title>
      <link>http://arxiv.org/abs/2510.12192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对手绘草图的独特稀疏性和抽象性，提出了多级草图表示方案和SDGraph深度学习架构，有效利用草图中的有效信息，在多个下游任务中取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;手绘草图具有独特的稀疏性和抽象性，需要与图像不同的学习流程。然而，对于什么是有效的草图信息的研究有限，这限制了现有方法的性能。&lt;h4&gt;目的&lt;/h4&gt;系统地识别和利用草图中的有效信息，以提升草图学习方法的性能。&lt;h4&gt;方法&lt;/h4&gt;提出多级草图表示方案，将草图表示分为草图级、笔画级和点级三个层次；基于此开发了SDGraph深度学习架构，包含稀疏图和密集图两个互补模块，以及信息融合模块。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析和实验评估，确定了草图中的有效信息；SDGraph在分类、检索和矢量草图生成任务上分别比最先进方法提高了1.15%、1.70%和36.58%的性能。&lt;h4&gt;结论&lt;/h4&gt;多级草图表示方案能够系统地识别有效信息，SDGraph架构能够有效利用这些信息，提升各种草图相关任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;手绘草图具有独特的稀疏性和抽象性，需要不同于图像的学习流程。对于草图学习方法，主要目标是充分利用草图中的有效信息。然而，关于什么是有效的草图信息的研究有限，这限制了现有方法的性能。为解决这一问题，我们首先提出了多级草图表示方案，系统地识别有效信息。该方案将草图表示分为三个层次：草图级、笔画级和点级。此设计基于分析元素的粒度，从粗（草图级）到细（点级），从而确保更全面地覆盖草图信息。对每个层次，我们进行了理论分析和实验评估，以识别和验证有效信息。基于上述研究，我们开发了SDGraph，这是一个深度学习架构，旨在利用三个层次中识别出的有效信息。SDGraph包含两个互补模块：稀疏图将笔画作为节点，用于草图级和笔画级表示学习；密集图将点作为节点，用于草图级和点级表示学习。两个模块都采用图卷积以及下采样和上采样操作，使其能够作为编码器和解码器。此外，信息融合模块连接两个图，进一步增强特征提取。SDGraph支持各种草图相关的下游任务，在分类和检索方面分别比最先进方法提高了1.15%和1.70%，在矢量草图生成质量上提高了36.58%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Freehand sketches exhibit unique sparsity and abstraction, necessitatinglearning pipelines distinct from those designed for images. For sketch learningmethods, the central objective is to fully exploit the effective informationembedded in sketches. However, there is limited research on what constituteseffective sketch information, which in turn constrains the performance ofexisting approaches. To tackle this issue, we first proposed the Multi-LevelSketch Representation Scheme to systematically identify the effectiveinformation. The scheme organizes sketch representation into three levels:sketch-level, stroke-level, and point-level. This design is based on thegranularity of analytical elements, from coarse (sketch-level) to fine(point-level), thereby ensuring more comprehensive coverage of the sketchinformation. For each level, we conducted theoretical analyses and experimentalevaluations to identify and validate the effective information. Building on theabove studies, we developed SDGraph, a deep learning architecture designed toexploit the identified effective information across the three levels. SDGraphcomprises two complementary modules: a Sparse Graph that treats strokes asnodes for sketch-level and stroke-level representation learning, and a DenseGraph that treats points as nodes for sketch-level and point-levelrepresentation learning. Both modules employ graph convolution along withdown-sampling and up-sampling operations, enabling them to function as bothencoder and decoder. Besides that, an information fusion module bridges the twographs to further enhance feature extraction. SDGraph supports a wide range ofsketch-related downstream tasks, achieving accuracy improvements of 1.15\% and1.70\% over the state-of-the-art in classification and retrieval, respectively,and 36.58\% improvement in vector sketch generation quality.</description>
      <author>example@mail.com (Xi Cheng, Pingfa Feng, Zhichao Liao, Mingyu Fan, Long Zeng)</author>
      <guid isPermaLink="false">2510.12192v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning</title>
      <link>http://arxiv.org/abs/2510.12107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了区分性表示学习(DRL)框架，通过增量并行适配器网络和解耦锚监督方法，解决了非重排类增量学习中的三大挑战，在保持高效率的同时显著提升了性能。&lt;h4&gt;背景&lt;/h4&gt;预训练模型(PTMs)在非重排类增量学习(CIL)研究中表现出色，但仍面临三大挑战：模型复杂度不断增加、增量学习过程中表示不平稳、以及阶段性子问题优化与全局推理之间不一致。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够有效解决非重排类增量学习中的三大挑战的框架，实现平稳的表示转移，并缩小阶段性局部优化与全局推理之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出区分性表示学习(DRL)框架，包含增量并行适配器(IPA)网络和解耦锚监督(DAS)机制。IPA网络基于预训练模型构建，通过学习轻量级适配器在每个增量阶段逐步增强模型；DAS机制通过分别比较正负样本与虚拟锚来解耦约束，促进区分性表示学习并实现对齐不同阶段特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准测试上的实验表明，DRL在整个CIL期间持续优于其他最先进方法，同时在训练和推理阶段都保持高效率，有效解决了模型复杂度、表示不平稳和优化不一致问题。&lt;h4&gt;结论&lt;/h4&gt;DRL框架通过创新性的网络架构和监督机制，成功解决了非重排类增量学习中的关键挑战，为该领域提供了高效且有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;凭借预训练模型(PTMs)的优秀表示能力，非重排类增量学习(CIL)研究取得了显著进展。然而，由于三个难题：日益增长的大模型复杂度、增量学习过程中不平稳的表示转移、以及阶段性子问题优化与全局推理之间的不一致性，这仍然是一个极具挑战性的任务。在这项工作中，我们提出了区分性表示学习(DRL)框架来专门解决这些挑战。为了有效且高效地进行增量学习，DRL的网络称为增量并行适配器(IPA)网络，它基于PTM构建，并通过在每个增量阶段学习轻量级适配器来逐步增强模型，参数学习开销小。该适配器负责使模型适应新类别，它可以通过它们之间的并行连接和传输门继承并传播当前模型的表示能力。因此，这种设计保证了不同增量阶段之间的平稳表示转移。此外，为了缓解不一致性并实现跨增量阶段可比的特征表示，我们设计了解耦锚监督(DAS)。它通过将正样本和负样本分别与虚拟锚进行比较来解耦它们的约束。这种解耦促进了区分性表示学习并对齐了在不同阶段学习的特征空间，从而缩小了在数据子集上进行阶段性局部优化与在所有类别上进行全局推理之间的差距。在六个基准测试上的大量实验表明，我们的DRL在整个CIL期间持续优于其他最先进方法，同时在训练和推理阶段都保持高效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the excellent representation capabilities of Pre-Trained Models (PTMs),remarkable progress has been made in non-rehearsal Class-Incremental Learning(CIL) research. However, it remains an extremely challenging task due to threeconundrums: increasingly large model complexity, non-smooth representationshift during incremental learning and inconsistency between stage-wisesub-problem optimization and global inference. In this work, we propose theDiscriminative Representation Learning (DRL) framework to specifically addressthese challenges. To conduct incremental learning effectively and yetefficiently, the DRL's network, called Incremental Parallel Adapter (IPA)network, is built upon a PTM and increasingly augments the model by learning alightweight adapter with a small amount of parameter learning overhead in eachincremental stage. The adapter is responsible for adapting the model to newclasses, it can inherit and propagate the representation capability from thecurrent model through parallel connection between them by a transfer gate. As aresult, this design guarantees a smooth representation shift between differentincremental stages. Furthermore, to alleviate inconsistency and enablecomparable feature representations across incremental stages, we design theDecoupled Anchor Supervision (DAS). It decouples constraints of positive andnegative samples by respectively comparing them with the virtual anchor. Thisdecoupling promotes discriminative representation learning and aligns thefeature spaces learned at different stages, thereby narrowing the gap betweenstage-wise local optimization over a subset of data and global inference acrossall classes. Extensive experiments on six benchmarks reveal that our DRLconsistently outperforms other state-of-the-art methods throughout the entireCIL period while maintaining high efficiency in both training and inferencephases.</description>
      <author>example@mail.com (Jiawei Zhan, Jun Liu, Jinlong Peng, Xiaochen Chen, Bin-Bin Gao, Yong Liu, Chengjie Wang)</author>
      <guid isPermaLink="false">2510.12107v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation</title>
      <link>http://arxiv.org/abs/2510.12054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为MIARec的学术论文推荐模型，通过基于引力的方法衡量学者之间的相互学术影响，并将其整合到图表示学习的特征聚合过程中，以解决传统基于图的推荐方法忽视学术网络中不对称学术影响的问题。&lt;h4&gt;背景&lt;/h4&gt;随着科学文献的快速扩张，学者们对精确且高质量的论文推荐需求日益增加。在各种推荐方法中，基于图的方法因其能有效利用学术网络中固有的结构特征而受到关注。然而，这些方法在学习图表示时往往忽视了学术网络中普遍存在的不对称学术影响。&lt;h4&gt;目的&lt;/h4&gt;解决传统基于图的推荐方法在学术网络推荐中忽视不对称学术影响的问题，提高科学论文推荐的准确性和质量。&lt;h4&gt;方法&lt;/h4&gt;提出Mutual-Influence-Aware Recommendation (MIARec)模型，采用基于引力的方法衡量学者之间的相互学术影响，并将这种影响整合到图表示学习中的消息传播过程的特征聚合中。此外，模型利用多通道聚合方法来捕获不同单一关系子网络的个体嵌入及其相互依赖的嵌入，从而能够更全面地理解异构学术网络。&lt;h4&gt;主要发现&lt;/h4&gt;在真实数据集上进行的大量实验表明，MIARec模型在三个主要评估指标上均优于基线模型，证明了其在科学论文推荐任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;MIARec模型通过考虑学者之间的相互学术影响和使用多通道聚合方法，能够更有效地进行科学论文推荐，其性能优于现有的基线模型。&lt;h4&gt;翻译&lt;/h4&gt;随着科学文献的快速扩张，学者们越来越需要精确和高质量的论文推荐。在各种推荐方法中，基于图的方法通过有效利用学术网络中固有的结构特征而受到关注。然而，这些方法在学习图表示时往往忽视了学术网络中普遍存在的不对称学术影响。为了解决这一局限，本研究提出了相互影响感知推荐(MIARec)模型，该模型采用基于引力的方法来衡量学者之间的相互学术影响，并将这种影响整合到图表示学习过程中消息传播的特征聚合中。此外，该模型利用多通道聚合方法来捕获不同单一关系子网络的个体嵌入及其相互依赖的嵌入，从而能够更全面地理解异构学术网络。在真实数据集上进行的大量实验表明，MIARec模型在三个主要评估指标上均优于基线模型，表明其在科学论文推荐任务中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid expansion of scientific literature, scholars increasinglydemand precise and high-quality paper recommendations. Among variousrecommendation methodologies, graph-based approaches have garnered attention byeffectively exploiting the structural characteristics inherent in scholarlynetworks. However, these methods often overlook the asymmetric academicinfluence that is prevalent in scholarly networks when learning graphrepresentations. To address this limitation, this study proposes theMutual-Influence-Aware Recommendation (MIARec) model, which employs agravity-based approach to measure the mutual academic influence betweenscholars and incorporates this influence into the feature aggregation processduring message propagation in graph representation learning. Additionally, themodel utilizes a multi-channel aggregation method to capture both individualembeddings of distinct single relational sub-networks and their interdependentembeddings, thereby enabling a more comprehensive understanding of theheterogeneous scholarly network. Extensive experiments conducted on real-worlddatasets demonstrate that the MIARec model outperforms baseline models acrossthree primary evaluation metrics, indicating its effectiveness in scientificpaper recommendation tasks.</description>
      <author>example@mail.com (Wenjin Xie, Tao Jia)</author>
      <guid isPermaLink="false">2510.12054v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition</title>
      <link>http://arxiv.org/abs/2510.11944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了TopoAlign框架，利用代码仓库作为训练资源来提高大型语言模型在数学自动形式化任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在非正式和正式数学推理方面表现出色，但在将非正式数学陈述转换为正式陈述的自动形式化任务上仍有困难。当前数学LLMs的性能受限于包含非正式和正式陈述配对的大规模语料库的稀缺性。&lt;h4&gt;目的&lt;/h4&gt;解决当前数学LLMs在自动形式化任务上的局限性，利用更广泛可用的代码仓库作为训练资源来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出TopoAlign框架，将代码分解为文档字符串、主函数和依赖函数，然后重新组装成结构上模仿正式陈述的类似物，产生结构对齐的代码数据用于训练。训练了DeepSeek-Math和Herald两个模型，并在minif2f、Putnam和ProofNet基准上评估。&lt;h4&gt;主要发现&lt;/h4&gt;TopoAlign显著提升了DeepSeek-Math的性能，在BEq@10上提高17.77%，在typecheck@10上提高68.82%。即使对于专业模型Herald，也在BEq@10和typecheck@10上分别提高了0.12%和1.09%，表明训练结构对齐的代码数据对各种模型都有益。&lt;h4&gt;结论&lt;/h4&gt;TopoAlign框架成功利用广泛可用的代码仓库作为训练资源，结构对齐的代码数据能有效提高数学LLMs在自动形式化任务上的性能，即使对专业模型也有改进作用。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在非正式和正式(如Lean 4)数学推理方面表现出色，但它们在自动形式化(即将非正式数学陈述转换为正式陈述的任务)方面仍有困难。自动形式化有助于将LLMs的非正式推理与形式证明助手配对，实现机器可验证的生成并减少幻觉。然而，当前数学LLMs的性能受限于大规模语料库的稀缺性，特别是包含非正式和正式陈述配对的语料库。尽管当前模型被训练为从自然语言指令生成代码，但这些代码与形式数学之间的结构和语法差异限制了有效的迁移学习。我们提出了TopoAlign框架，它解锁了广泛可用的代码仓库作为数学LLMs的训练资源。TopoAlign将代码分解为文档字符串、主函数和依赖函数，并将这些组件重新组装成结构上模仿正式陈述的类似物。这产生了结构对齐的代码数据，可用于训练数学LLMs而无需额外的人工注释。我们训练了两个最先进的模型DeepSeek-Math和Herald，并在minif2f、Putnam和ProofNet基准上评估它们。TopoAlign为DeepSeek-Math提供了显著提升，在BEq@10上提高17.77%，在typecheck@10上提高68.82%。尽管没有引入新的数学知识，我们的框架使Herald在BEq@10和typecheck@10上分别提高了0.12%和1.09%，证明了即使在专业模型上，训练结构对齐的代码数据也是有益的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)mathematical reasoning but still struggle with autoformalisation, the task oftransforming informal into formal mathematical statements. Autoformalisationhelps pair the informal reasoning of LLMs with formal proof assistants whichenable machine-verifiable generation and mitigate hallucinations. Yet, theperformance of current Math LLMs is constrained by the scarcity of large-scalecorpora, particularly those containing pairs of informal and formal statements.Although current models are trained to generate code from natural languageinstructions, structural and syntactic differences between these and formalmathematics limit effective transfer learning. We propose TopoAlign, aframework that unlocks widely available code repositories as training resourcesfor Math LLMs. TopoAlign decomposes code into docstrings, main functions, anddependency functions, and reassembles these components into analogues thatstructurally mirror formal statements. This produces structurally aligned codedata that can be used for training Math LLMs without requiring additional humanannotation. We train two state-of-the-art models, DeepSeek-Math and Herald, andevaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlignprovides substantial gains for DeepSeek-Math, improving performance by 17.77%on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematicalknowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10and typecheck@10, respectively, demonstrating that training on aligned codedata is beneficial even for specialized models.</description>
      <author>example@mail.com (Yupei Li, Philipp Borchert, Gerasimos Lampouras)</author>
      <guid isPermaLink="false">2510.11944v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer</title>
      <link>http://arxiv.org/abs/2510.11926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 12 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Locaris是一种仅解码器的大型语言模型，用于室内Wi-Fi定位，能够直接处理原始信号数据而无需预处理，在各种条件下表现出色且无需大量校准。&lt;h4&gt;背景&lt;/h4&gt;室内Wi-Fi定位具有挑战性，因为无线电信号对环境动态、信道传播特性和硬件异构性高度敏感。传统方法需要密集校准且在条件变化时性能迅速下降。&lt;h4&gt;目的&lt;/h4&gt;引入Locaris，一种仅解码器的大型语言模型，用于室内定位，解决传统方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;Locaris将每个接入点测量视为token，摄取原始Wi-Fi遥测数据无需预处理。通过在不同Wi-Fi数据集上微调LLM，学习从原始信号到设备位置的轻量级且可泛化的映射。&lt;h4&gt;主要发现&lt;/h4&gt;Locaris匹配或超越现有技术；紧凑LLM可作为无校准回归模型；少样本适应实验显示高精度；亚米级精度仅需几百个样本；在缺少AP情况下仍稳健；支持所有可用遥测数据。&lt;h4&gt;结论&lt;/h4&gt;Locaris在室内定位实际应用中具有实用可行性，特别适用于大规模部署中广泛校准不可行的情况。&lt;h4&gt;翻译&lt;/h4&gt;室内Wi-Fi定位由于无线电信号对环境动态、信道传播特性和硬件异构性的高度敏感性而仍然是一个具有挑战性的问题。传统的指纹识别和基于模型的方法通常需要密集的校准，并且在设备、信道或部署条件变化时性能迅速下降。在本文中，我们引入了Locaris，一种用于室内定位的仅解码器大型语言模型（LLM）。Locaris将每个接入点（AP）测量视为一个token，能够摄取原始的Wi-Fi遥测数据而无需预处理。通过在不同的Wi-Fi数据集上微调其LLM，Locaris学习从原始信号直接到设备位置的轻量级且可泛化的映射。我们将Locaris与最先进的方法进行比较实验研究，一致表明Locaris在各种类型的遥测数据上匹配或超越现有技术。我们的结果表明，紧凑的LLM可以作为室内定位的无校准回归模型，在异构Wi-Fi部署中提供可扩展和稳健的跨环境性能。使用每个设备仅少数几个校准点的少样本适应实验进一步表明，当应用于未见过的设备和部署场景时，Locaris保持高精度。这仅需几百个样本就能实现亚米级精度，在缺少AP的情况下保持稳健性能，并支持所有可用的遥测数据。我们的发现突显了Locaris在现实场景室内定位中的实际可行性，特别是在广泛校准不可行的大规模部署中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor Wi-Fi positioning remains a challenging problem due to the highsensitivity of radio signals to environmental dynamics, channel propagationcharacteristics, and hardware heterogeneity. Conventional fingerprinting andmodel-based approaches typically require labor-intensive calibration and sufferrapid performance degradation when devices, channel or deployment conditionschange. In this paper, we introduce Locaris, a decoder-only large languagemodel (LLM) for indoor localization. Locaris treats each access point (AP)measurement as a token, enabling the ingestion of raw Wi-Fi telemetry withoutpre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locarislearns a lightweight and generalizable mapping from raw signals directly todevice location. Our experimental study comparing Locaris with state-of-the-artmethods consistently shows that Locaris matches or surpasses existingtechniques for various types of telemetry. Our results demonstrate that compactLLMs can serve as calibration-free regression models for indoor localization,offering scalable and robust cross-environment performance in heterogeneousWi-Fi deployments. Few-shot adaptation experiments, using only a handful ofcalibration points per device, further show that Locaris maintains highaccuracy when applied to previously unseen devices and deployment scenarios.This yields sub-meter accuracy with just a few hundred samples, robustperformance under missing APs and supports any and all available telemetry. Ourfindings highlight the practical viability of Locaris for indoor positioning inthe real-world scenarios, particularly in large-scale deployments whereextensive calibration is infeasible.</description>
      <author>example@mail.com (Nayan Sanjay Bhatia, Pranay Kocheta, Russell Elliott, Harikrishna S. Kuttivelil, Katia Obraczka)</author>
      <guid isPermaLink="false">2510.11926v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis</title>
      <link>http://arxiv.org/abs/2510.11829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了生成式AI与Schrödinger bridge问题的联系，提出了一种软约束方法来解决经典SBP的稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;生成式AI可视为学习将简单参考映射为复杂数据分布的模型，与Schrödinger bridge问题有强联系，因两者都通过熵正则化随机动力学在指定边际间插值。&lt;h4&gt;目的&lt;/h4&gt;解决经典SBP强制执行硬终端约束导致的不稳定性问题，特别是在高维或数据稀缺情况下。&lt;h4&gt;方法&lt;/h4&gt;采用软约束Schrödinger bridge问题(SCSBP)框架，将终端约束替换为一般惩罚函数，建立McKean-Vlasov类型随机控制公式，并证明随惩罚增加，控制和值函数以线性速率收敛到经典SBP。&lt;h4&gt;主要发现&lt;/h4&gt;建立了所有惩罚水平下最优解的存在性；首次为软约束桥提供定量收敛保证；揭示惩罚正则化如何实现鲁棒的生成建模、微调和迁移学习。&lt;h4&gt;结论&lt;/h4&gt;软约束SBP为解决经典SBP在高维和数据稀缺情况下的不稳定性提供了有效方法，对生成式AI领域有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;生成式AI可以被构建为学习将简单参考测度映射为复杂数据分布的问题，最近由于它们通过熵正则化随机动力学在指定边际之间插值的共同特性，它与经典的Schrödinger bridge问题理论有很强的联系。然而，经典SBP强制执行硬终端约束，这往往导致实际实现中的不稳定性，特别是在高维或数据稀缺的情况下。为应对这一挑战，我们遵循所谓的软约束Schrödinger bridge问题的思路，其中终端约束被一般惩罚函数所取代。这种松弛导致更灵活的McKean-Vlasov类型的随机控制公式。我们建立了所有惩罚水平下最优解的存在性，并证明随着惩罚的增加，控制和值函数以线性速率收敛到经典SBP。我们的分析基于Doob的h变换表示、Schrödinger势的稳定性结果、Gamma-收敛以及一种新的固定点论据，该论据将测度空间上的优化问题与辅助的熵最优传输问题耦合。这些结果不仅首次为软约束桥提供了定量收敛保证，还揭示了惩罚正则化如何实现鲁棒的生成建模、微调和迁移学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative AI can be framed as the problem of learning a model that mapssimple reference measures into complex data distributions, and it has recentlyfound a strong connection to the classical theory of the Schr\"odinger bridgeproblems (SBPs) due partly to their common nature of interpolating betweenprescribed marginals via entropy-regularized stochastic dynamics. However, theclassical SBP enforces hard terminal constraints, which often leads toinstability in practical implementations, especially in high-dimensional ordata-scarce regimes. To address this challenge, we follow the idea of theso-called soft-constrained Schr\"odinger bridge problem (SCSBP), in which theterminal constraint is replaced by a general penalty function. This relaxationleads to a more flexible stochastic control formulation of McKean-Vlasov type.  We establish the existence of optimal solutions for all penalty levels andprove that, as the penalty grows, both the controls and value functionsconverge to those of the classical SBP at a linear rate. Our analysis builds onDoob's h-transform representations, the stability results of Schr\"odingerpotentials, Gamma-convergence, and a novel fixed-point argument that couples anoptimization problem over the space of measures with an auxiliary entropicoptimal transport problem. These results not only provide the firstquantitative convergence guarantees for soft-constrained bridges but also shedlight on how penalty regularization enables robust generative modeling,fine-tuning, and transfer learning.</description>
      <author>example@mail.com (Jin Ma, Ying Tan, Renyuan Xu)</author>
      <guid isPermaLink="false">2510.11829v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Denoised Diffusion for Object-Focused Image Augmentation</title>
      <link>http://arxiv.org/abs/2510.08955v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对数据稀缺条件下动物健康监测的数据增强框架，通过分割动物图像并进行变换和扩散合成，生成多样化场景，提高动物检测和监测性能。&lt;h4&gt;背景&lt;/h4&gt;现代农业依赖集成监测系统，其中基于无人机的动物健康监测是关键，但面临数据有限、动物体积小、被遮挡或部分可见等问题。迁移学习方法因缺乏反映特定农场条件的大型数据集而效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对特定问题、以动物为中心的数据增强策略，专为数据受限条件下的动物健康监测设计，解决数据稀缺与实际应用之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出面向对象的数据增强框架，将动物从背景中分割出来，通过变换和基于扩散的合成技术增强图像，创建真实、多样化的场景，以提高动物检测和监测性能。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，与基线模型相比，增强数据集在动物检测任务上表现更优。通过生成领域特定数据，该方法支持数据稀缺场景下的实时动物健康监测。&lt;h4&gt;结论&lt;/h4&gt;该数据增强方法能够弥合有限数据与实际应用之间的差距，即使在数据稀缺的情况下也能支持实时动物健康监测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现代农业生产操作越来越依赖于集成监测系统，这些系统结合多种数据源以优化农场运营。基于空中无人机的动物健康监测是关键组成部分，但面临数据可用性有限的问题，加上场景特定问题如动物体积小、被遮挡或部分可见。由于缺乏反映特定农场条件（包括动物品种、环境和行为的差异）的大型数据集，迁移学习方法通常无法解决这一限制。因此，需要开发一种针对特定问题、以动物为中心的数据增强策略，专门为这些独特挑战量身定制。为解决这一差距，我们提出了一种面向对象的数据增强框架，专门为数据受限条件下的动物健康监测设计。我们的方法将动物从背景中分割出来，并通过变换和基于扩散的合成来增强它们，创建真实、多样化的场景，以提高动物检测和监测性能。我们的初步实验表明，与基线模型相比，我们的增强数据集在动物检测任务上取得了更好的性能。通过生成领域特定的数据，我们的方法即使在数据稀缺的情况下也能支持实时动物健康监测解决方案，弥合了有限数据与实际应用之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern agricultural operations increasingly rely on integrated monitoringsystems that combine multiple data sources for farm optimization. Aerialdrone-based animal health monitoring serves as a key component but faceslimited data availability, compounded by scene-specific issues such as small,occluded, or partially visible animals. Transfer learning approaches often failto address this limitation due to the unavailability of large datasets thatreflect specific farm conditions, including variations in animal breeds,environments, and behaviors. Therefore, there is a need for developing aproblem-specific, animal-focused data augmentation strategy tailored to theseunique challenges. To address this gap, we propose an object-focused dataaugmentation framework designed explicitly for animal health monitoring inconstrained data settings. Our approach segments animals from backgrounds andaugments them through transformations and diffusion-based synthesis to createrealistic, diverse scenes that enhance animal detection and monitoringperformance. Our initial experiments demonstrate that our augmented datasetyields superior performance compared to our baseline models on the animaldetection task. By generating domain-specific data, our method empowersreal-time animal health monitoring solutions even in data-scarce scenarios,bridging the gap between limited data and practical applicability.</description>
      <author>example@mail.com (Nisha Pillai, Aditi Virupakshaiah, Harrison W. Smith, Amanda J. Ashworth, Prasanna Gowda, Phillip R. Owens, Adam R. Rivers, Bindu Nanduri, Mahalingam Ramkumar)</author>
      <guid isPermaLink="false">2510.08955v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>TopROI: A topology-informed network approach for tissue partitioning</title>
      <link>http://arxiv.org/abs/2510.12772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 11 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为TopROI的新方法，用于将点云数据分割为感兴趣区域(ROI)。该方法结合了几何感知网络和持续同调理论，能够同时保留局部几何结构和高级组织架构。在模拟腺体结构和结直肠癌活检数据上验证，该方法优于传统分割方法，能够更好地保留生物学上有意义的结构，并揭示了从健康到癌变的连续性组织变化。&lt;h4&gt;背景&lt;/h4&gt;哺乳动物组织架构对生物功能至关重要，其破坏是疾病的标志。医学成像技术可生成大型点云数据集捕捉疾病进展中的细胞变化，但传统感兴趣区域(ROI)定义方法基于象限(quadrat-based)，忽略了组织内在结构，可能导致有意义的特征碎片化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保留局部几何结构和高级架构的点云分割方法，用于定义生物学上有意义的ROI，从而更准确地量化组织结构并获取与疾病进展相关结构变化的新见解。&lt;h4&gt;方法&lt;/h4&gt;TopROI是一种基于拓扑感知和网络的方法，将几何感知网络与持续同调(persistent homology)相结合，利用细胞邻域和多重尺度循环来指导社区检测，从而识别有意义的ROI。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在模拟腺体结构的合成点云上，TopROI优于传统方法，能维持生物学合理的ROI几何并保留真实结构；2) 在结直肠癌活检数据上，TopROI生成保留类似隐窝结构的ROI，允许进行持续同调分析；3) 研究揭示了从健康黏膜到癌变的连续性组织变化，反映了结构渐进性无序化。&lt;h4&gt;结论&lt;/h4&gt;TopROI为定义大型点云中生物学上有意义的ROI提供了原则性和灵活的框架，能更准确量化组织结构，并提供与疾病进展相关结构变化的新见解。&lt;h4&gt;翻译&lt;/h4&gt;哺乳动物组织架构对生物功能至关重要，其破坏是疾病的标志。医学成像技术可以生成大型点云数据集，捕捉疾病进展过程中组织细胞成分的变化。然而，感兴趣区域(ROI)通常基于象限方法定义，这些方法忽略了内在结构，可能导致有意义的特征碎片化。在此，我们介绍TopROI，一种基于拓扑感知的网络方法，用于将点云分割为ROI，同时保留局部几何结构和高级架构。TopROI将几何感知网络与持续同调相结合，利用细胞邻域和多重尺度循环来指导社区检测。应用于模拟腺体结构的合成点云时，TopROI通过维持生物学上合理的ROI几何形状和更好地保留真实结构，优于基于象限和纯粹几何的分割方法。应用于从人类结直肠癌活检获得的细胞点云时，TopROI生成保留类似隐窝结构的ROI，并允许对单个区域进行持续同调分析。本研究揭示了从健康黏膜到癌变的连续性 architectural 变化，反映了组织结构的渐进性无序化。因此，TopROI为定义大型点云中生物学上有意义的ROI提供了一个原则性和灵活的框架，能够更准确地量化组织结构并提供与疾病进展相关的结构变化的新见解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学成像生成的大规模点云数据中如何定义'感兴趣区域'(ROIs)的问题。当前常用的基于网格的分区方法忽略了组织结构的内在特性，可能导致有意义的结构特征被分割。这个问题很重要，因为哺乳动物组织结构对生物功能至关重要，其破坏是疾病的标志，而准确的ROI定义对于研究疾病进展过程中组织结构变化、进行下游分析和诊断至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到传统网格方法无法保留组织拓扑结构，因此考虑结合计算几何和拓扑数据分析的最新进展。他们借鉴了Delaunay三角剖分来表示局部细胞关系、持久同调技术提取拓扑特征、以及Leiden算法进行社区检测。作者创新性地将这些现有方法整合到一个统一框架中，同时考虑几何和拓扑信息，从而更有效地定义ROI。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合几何信息和拓扑信息来定义ROI，使分区既能保留局部几何结构，又能捕捉更高阶的组织架构。流程包括：1)构建Delaunay几何网络并赋予权重；2)通过持久同调计算拓扑特征并构建拓扑网络；3)整合几何和拓扑网络为一个加权网络；4)使用Leiden算法进行社区检测，最终得到的社区即为定义的ROI。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创新性地整合几何网络与持久同调特征；2)提供多尺度分析能力；3)能保留如腺体结构等生物学相关结构；4)方法具有可扩展性和灵活性。相比之前工作，TopROI不同于传统网格方法(不考虑组织内在特性)、纯几何方法(无法捕捉高阶结构)和纯拓扑方法(缺乏几何细节)，是首个将计算几何、网络科学和拓扑数据分析整合用于ROI定义的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TopROI通过整合几何网络和拓扑数据分析，提供了一种能够同时保留局部几何结构和更高阶组织架构的灵活框架，用于定义生物学上有意义的感兴趣区域，从而更准确地量化组织结构并揭示与疾病进展相关的结构变化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mammalian tissue architecture is central to biological function, and itsdisruption is a hallmark of disease. Medical imaging techniques can generatelarge point cloud datasets that capture changes in the cellular composition ofsuch tissues with disease progression. However, regions of interest (ROIs) areusually defined by quadrat-based methods that ignore intrinsic structure andrisk fragmenting meaningful features. Here, we introduce TopROI, atopology-informed, network-based method for partitioning point clouds into ROIsthat preserves both local geometry and higher-order architecture. TopROIintegrates geometry-informed networks with persistent homology, combining cellneighbourhoods and multiscale cycles to guide community detection. Applied tosynthetic point clouds that mimic glandular structure, TopROI outperformsquadrat-based and purely geometric partitions by maintaining biologicallyplausible ROI geometry and better preserving ground-truth structures. Appliedto cellular point clouds obtained from human colorectal cancer biopsies, TopROIgenerates ROIs that preserve crypt-like structures and enable persistenthomology analysis of individual regions. This study reveals a continuum ofarchitectural changes from healthy mucosa to carcinoma, reflecting progressivedisorganisation in tissue structure. TopROI thus provides a principled andflexible framework for defining biologically meaningful ROIs in large pointclouds, enabling more accurate quantification of tissue organization and newinsights into structural changes associated with disease progression.</description>
      <author>example@mail.com (Sergio Serrano de Haro Iváñez, Joshua W. Moore, Lucile Grzesiak, Eoghan J. Mullholand, Heather Harrington, Simon J. Leedham, Helen M. Byrne)</author>
      <guid isPermaLink="false">2510.12772v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points</title>
      <link>http://arxiv.org/abs/2510.12524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Voronoi-Assisted Diffusion (VAD)的轻量级、无网络方法，用于直接从无方向点云计算无符号距离场(UDF)。该方法通过双向法线分配、法线扩散和UDF梯度场积分实现，能够高效稳定地处理各种复杂几何结构。&lt;h4&gt;背景&lt;/h4&gt;无符号距离场(UDF)可以表示具有任意拓扑结构的3D形状，包括开放和封闭表面、可定向和不可定向几何以及非流形结构。然而，现有的神经方法在数值稳定性、计算成本和可控性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、计算稳定且高效的方法来直接从无方向点云计算UDF，解决现有神经方法的数值不稳定性和高计算成本问题。&lt;h4&gt;方法&lt;/h4&gt;VAD方法包括三个主要步骤：(1)基于Voronoi的几何标准通过能量函数引导为输入点分配双向法线；(2)将法线扩散形成近似UDF梯度场；(3)积分梯度场恢复最终UDF。&lt;h4&gt;主要发现&lt;/h4&gt;VAD能够稳健处理封闭和开放表面，以及复杂的非流形和不可定向几何，同时保持计算效率和稳定性。&lt;h4&gt;结论&lt;/h4&gt;VAD是一种有效的方法，可以克服现有神经方法在计算UDF时的数值不稳定性和高计算成本问题，同时提供更好的可控性。&lt;h4&gt;翻译&lt;/h4&gt;无符号距离场(UDF)为具有任意拓扑结构的3D形状提供了灵活的表示，包括开放和封闭表面、可定向和不可定向几何以及非流形结构。虽然最近的神经方法在学习UDF方面显示出潜力，但它们常常面临数值不稳定、计算成本高和可控性有限等问题。我们提出了一种轻量级、无网络的方法Voronoi-Assisted Diffusion (VAD)，用于直接从无方向点云计算UDF。我们的方法首先通过能量函数中编码的两个基于Voronoi的几何标准引导，为输入点分配双向法线以实现最佳对齐。然后将对齐的法线扩散形成近似UDF梯度场，随后积分恢复最终UDF。实验证明，VAD能够稳健处理封闭和开放表面，以及复杂的非流形和不可定向几何，同时保持计算效率和稳定性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从无方向点云计算无符号距离场（UDFs）的问题。这个问题在3D重建、计算机图形学和几何处理等领域非常重要，因为UDFs可以表示具有任意拓扑的3D形状，包括开放和封闭表面、可定向和不可定向几何体以及非流形结构，而现有的神经学习方法往往面临数值不稳定、计算成本高和可控性有限的问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到热方法（heat method）的启发，该方法用于在定向数据上计算有符号距离场。他们引入了投影距离场概念，并将其与Voronoi图联系起来，将点方向视为优化变量。作者借鉴了现有工作中的多个方面：热方法用于扩散法线、投影距离场概念、Voronoi图在几何处理中的应用、泊松表面重建框架以及基于广义回转数的方法，但将这些元素组合成一种新的方法来解决UDF计算问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用Voronoi辅助扩散（VAD）框架，首先对输入点分配双向法线，然后将对齐的法线扩散以形成近似的UDF梯度场，最后通过积分恢复最终的UDF。整体流程包括：1) 构建Voronoi图并初始化双向法线；2) 通过最小化Voronoi边界上的不连续性来优化双向法线；3) （可选）对于嘈杂输入，优化点位置；4) 将双向法线扩散并融合成一致的向量场；5) 求解泊松方程重建UDF。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Voronoi辅助框架优化双向法线；2) 投影距离场概念与Voronoi图结合；3) 将点方向视为优化变量；4) 提出轻量级、无网络方法；5) 能够处理复杂几何结构。相比之前的工作，该方法避免了神经网络的数值不稳定和高计算成本问题；不需要明确的内外区分，能够处理非封闭表面；不需要边界方向，能够处理非流形结构；相比其他UDF方法提供了更好的可控性和稳定性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Voronoi辅助扩散（VAD）方法，一种从无方向点云计算无符号距离场的轻量级、无网络方法，能够高效准确地处理开放表面、非流形结构和不可定向几何体等复杂情况。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsigned Distance Fields (UDFs) provide a flexible representation for 3Dshapes with arbitrary topology, including open and closed surfaces, orientableand non-orientable geometries, and non-manifold structures. While recent neuralapproaches have shown promise in learning UDFs, they often suffer fromnumerical instability, high computational cost, and limited controllability. Wepresent a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD),for computing UDFs directly from unoriented point clouds. Our approach beginsby assigning bi-directional normals to input points, guided by twoVoronoi-based geometric criteria encoded in an energy function for optimalalignment. The aligned normals are then diffused to form an approximate UDFgradient field, which is subsequently integrated to recover the final UDF.Experiments demonstrate that VAD robustly handles watertight and open surfaces,as well as complex non-manifold and non-orientable geometries, while remainingcomputationally efficient and stable.</description>
      <author>example@mail.com (Jiayi Kong, Chen Zong, Junkai Deng, Xuhui Chen, Fei Hou, Shiqing Xin, Junhui Hou, Chen Qian, Ying He)</author>
      <guid isPermaLink="false">2510.12524v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Scene Coordinate Reconstruction Priors</title>
      <link>http://arxiv.org/abs/2510.12387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025, Project page: https://nianticspatial.github.io/scr-priors/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种对场景坐标回归(SCR)模型进行概率性重新解释的方法，通过引入高级重建先验来改善3D场景表示。研究团队探索了多种先验方法，并训练了3D点云扩散模型，这些先验有助于学习更好的场景表示，提高点云质量、配准率和相机姿态，并对下游任务产生积极影响。&lt;h4&gt;背景&lt;/h4&gt;场景坐标回归(SCR)模型已被证明是3D视觉中强大的隐式场景表示方法，能够实现视觉重定位和运动恢复。然而，这些模型是针对单个场景专门训练的，如果训练图像暗示了不足的多视图约束，SCR模型就会退化。&lt;h4&gt;目的&lt;/h4&gt;通过引入高级重建先验来改善SCR模型的学习过程，提高场景表示质量，解决在多视图约束不足情况下模型退化的问题。&lt;h4&gt;方法&lt;/h4&gt;研究团队提出了一种对SCR模型进行概率性重新解释的方法，并探索了多种先验技术：1)简单的深度值分布先验；2)学习合理的场景坐标配置先验；3)在大型室内扫描数据集上训练3D点云扩散模型。这些先验在每个训练步骤中将预测的3D场景点推向合理的几何形状。&lt;h4&gt;主要发现&lt;/h4&gt;在三个室内数据集上，研究团队发现：1)引入的先验有助于学习更好的场景表示；2)产生了更一致的场景点云；3)提高了配准率；4)改善了相机姿态；5)对下游任务如新视图合成和相机重定位有积极影响。&lt;h4&gt;结论&lt;/h4&gt;通过概率性重新解释SCR模型并引入高级重建先验，可以显著改善场景表示的质量，即使在多视图约束不足的情况下也能有效工作，从而提高各种3D视觉任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;场景坐标回归(SCR)模型已被证明是3D视觉中强大的隐式场景表示方法，能够实现视觉重定位和运动恢复。SCR模型是针对单个场景专门训练的。如果训练图像暗示了不足的多视图约束，SCR模型就会退化。我们提出了一种对训练SCR模型进行概率性重新解释的方法，使我们能够注入高级重建先验。我们研究了多种这样的先验，从对重建深度值分布的简单先验，到对合理场景坐标配置的学习先验。对于后者，我们在大型室内扫描语料库上训练了一个3D点云扩散模型。我们的先验在每个训练步骤中将预测的3D场景点推向合理的几何形状，以提高它们的可能性。在三个室内数据集上，我们的先验有助于学习更好的场景表示，产生更一致的场景点云，更高的配准率和更好的相机姿态，对新视图合成和相机重定位等下游任务有积极影响。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决场景坐标回归(SCR)模型在多视图约束不足时的退化问题，特别是在纹理贫乏区域、重复结构等场景下出现的点云分散、相机姿态估计不准确等问题。这个问题在现实中很重要，因为它直接影响室内场景重建质量、相机重定位精度和新视图合成效果，进而影响AR/VR等应用的用户体验；在研究中，它代表了提升神经SfM模型鲁棒性的重要挑战，特别是在缺乏足够视觉重叠的场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了SCR模型在缺乏多视图约束时退化的原因，然后通过概率重新解释SCR训练过程，将重建先验作为负对数似然项融入训练目标。他们设计了三种先验：深度分布先验、深度先验(RGB-D)和3D点云扩散先验。该方法借鉴了ACE框架的架构，从DiffusioNeRF获取灵感但改为在3D空间直接正则化，并首次将扩散模型应用于场景级别的3D点云生成，而非仅限于单个对象。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将SCR训练重新表述为最大似然学习，引入重建先验来引导模型学习更合理的场景表示，使预测的3D场景点朝向合理的几何形状。整体流程包括：1)使用ACE框架进行场景坐标回归，包含特征提取器和回归头；2)将训练目标重新表述为最大化场景坐标的概率，添加负对数先验作为正则化项；3)实现三种先验：深度分布先验、深度先验和点云扩散先验；4)在训练过程中联合优化重投影误差和先验项，扩散先验只在训练迭代5k后应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)概率性重新解释SCR训练以融入重建先验；2)提出多种重建先验（深度分布先验、深度先验和3D点云扩散先验）；3)首次将扩散模型应用于场景级别的3D点云生成；4)开发有效的RGB-D版本的ACE。相比之前的工作，本文方法通过高级先验正则化SCR训练，而非简单依赖场景特定训练；与特征匹配方法相比，提供更强的几何约束；与ACE框架相比，联合优化重投影误差和先验项而非交替优化；与3D扩散模型相比，应用于整个室内场景而非单个对象。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过概率性重新解释场景坐标回归训练并引入多种重建先验，显著提升了室内场景重建质量和相机姿态估计准确性，同时保持了测试时的高效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene coordinate regression (SCR) models have proven to be powerful implicitscene representations for 3D vision, enabling visual relocalization andstructure-from-motion. SCR models are trained specifically for one scene. Iftraining images imply insufficient multi-view constraints SCR modelsdegenerate. We present a probabilistic reinterpretation of training SCR models,which allows us to infuse high-level reconstruction priors. We investigatemultiple such priors, ranging from simple priors over the distribution ofreconstructed depth values to learned priors over plausible scene coordinateconfigurations. For the latter, we train a 3D point cloud diffusion model on alarge corpus of indoor scans. Our priors push predicted 3D scene points towardsplausible geometry at each training step to increase their likelihood. On threeindoor datasets our priors help learning better scene representations,resulting in more coherent scene point clouds, higher registration rates andbetter camera poses, with a positive effect on down-stream tasks such as novelview synthesis and camera relocalization.</description>
      <author>example@mail.com (Wenjing Bian, Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann)</author>
      <guid isPermaLink="false">2510.12387v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation</title>
      <link>http://arxiv.org/abs/2510.12095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages main paper; 15 pages references and appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了IL3D，一个专为大型语言模型驱动的3D场景生成设计的大规模数据集，包含27,816个室内布局和29,215个高保真3D对象资源。&lt;h4&gt;背景&lt;/h4&gt;室内布局设计领域对多样化、高质量训练数据有迫切需求，现有数据集可能无法满足大型语言模型训练的要求。&lt;h4&gt;目的&lt;/h4&gt;创建一个支持多模态学习的3D场景生成数据集，用于提升大语言模型在视觉语言任务中的表现，并推动3D场景生成和具身智能的研究。&lt;h4&gt;方法&lt;/h4&gt;构建包含18种常见房间类型的室内布局数据集，添加实例级自然语言注释，建立严格的评估基准，测试监督微调方法在数据集上的效果。&lt;h4&gt;主要发现&lt;/h4&gt;在IL3D上对大型语言模型进行监督微调显著提高了模型的泛化能力，性能优于在其他数据集上的微调结果；数据集提供多种多模态数据导出格式，可适应各种视觉任务需求。&lt;h4&gt;结论&lt;/h4&gt;IL3D作为一个多功能且强大的资源，通过提供高保真场景数据，显著推动了3D场景生成和具身智能的研究进展，特别是支持了具身智能体的环境感知任务。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们提出了IL3D，一个精心设计的大型数据集，用于大型语言模型驱动的3D场景生成，解决了室内布局设计中多样化、高质量训练数据的迫切需求。IL3D包含18种常见房间类型的27,816个室内布局和29,215个高保真3D对象资源库，并添加了实例级自然语言注释，以支持视觉语言任务的多模态学习。我们建立了严格的基准来评估LLM驱动的场景生成。实验结果表明，在IL3D上对LLM进行监督微调显著提高了泛化能力，并优于在其他数据集上的SFT性能。IL3D提供灵活的多模态数据导出功能，包括点云、3D边界框、多视图图像、深度图、法线图和语义掩码，能够无缝适应各种视觉任务。作为一个多功能且强大的资源，IL3D通过提供高保真场景数据来支持具身智能体的环境感知任务，显著推动了3D场景生成和具身智能的研究进展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏专门为LLM驱动的3D场景生成设计的大规模高质量室内布局数据集问题。这个问题很重要，因为3D室内场景生成是连接具身智能、智能家居设计、虚拟现实交互和机器人环境感知的关键技术，而精确的室内场景建模依赖于高质量的合成数据集，现有数据集在场景多样性、注释完整性和多模态适应性方面存在明显局限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，然后整合了3D-FRONT和HSSD数据集资源，通过人工清理和有针对性的合成数据补充不足。方法设计上采用USD格式使LLM可直接读取场景信息，并使用Qwen3-VL生成详细实例级描述。作者借鉴了现有数据集的经验，整合了3D-FRONT和HSSD的资源，采用HOLODECK方法合成缺失场景类型，并在格式设计和评估指标方面参考了现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多样化的室内布局数据集，提供实例级自然语言注释以支持多模态学习，确保室内布局符合现实世界的功能逻辑，并覆盖不同面积和物体密度的室内场景。整体流程包括：整合现有数据集并人工清理；使用HOLODECK方法合成缺失场景类型；将数据转换为USDZ和USDA格式；为对象提供多级注释并使用Qwen3-VL生成详细描述；设计客观和主观评估指标；支持多种数据格式的灵活导出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：大规模数据集（27,816个室内布局和29,215个对象模型）；覆盖18种常见房间类型；提供实例级自然语言注释；支持多种数据格式导出；使用USD格式实现文本可读性。相比之前工作，IL3D规模更大（超过3D-FRONT和HSSD），提供更全面的注释（大多数现有数据集缺乏自然语言注释），确保更好的功能逻辑，具有更强的多模态适应性，并专为LLM设计使其可直接读取和解析场景信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IL3D数据集通过提供大规模、多样化的室内布局和丰富的自然语言注释，显著提升了LLM驱动的3D室内场景生成质量和3D感知任务的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we present IL3D, a large-scale dataset meticulously designedfor large language model (LLM)-driven 3D scene generation, addressing thepressing demand for diverse, high-quality training data in indoor layoutdesign. Comprising 27,816 indoor layouts across 18 prevalent room types and alibrary of 29,215 high-fidelity 3D object assets, IL3D is enriched withinstance-level natural language annotations to support robust multimodallearning for vision-language tasks. We establish rigorous benchmarks toevaluate LLM-driven scene generation. Experimental results show that supervisedfine-tuning (SFT) of LLMs on IL3D significantly improves generalization andsurpasses the performance of SFT on other datasets. IL3D offers flexiblemultimodal data export capabilities, including point clouds, 3D bounding boxes,multiview images, depth maps, normal maps, and semantic masks, enablingseamless adaptation to various visual tasks. As a versatile and robustresource, IL3D significantly advances research in 3D scene generation andembodied intelligence, by providing high-fidelity scene data to supportenvironment perception tasks of embodied agents.</description>
      <author>example@mail.com (Wenxu Zhou, Kaixuan Nie, Hang Du, Dong Yin, Wei Huang, Siqiang Guo, Xiaobo Zhang, Pengbo Hu)</author>
      <guid isPermaLink="false">2510.12095v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>dN/dx Reconstruction with Deep Learning for High-Granularity TPCs</title>
      <link>http://arxiv.org/abs/2510.10628v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Graph Point Transformer (GraphPT)的深度学习模型，用于高粒度时间投影室中的dN/dx重建，以提高粒子识别性能。该模型将TPC数据表示为点云，采用基于图神经网络的U-Net架构，并使用针对点云处理优化的注意力机制。实验表明，GraphPT模型在K/π粒子识别方面比传统方法提高了10%至20%的分离能力。&lt;h4&gt;背景&lt;/h4&gt;粒子识别(PID)对未来的粒子物理实验如圆形正负电子对撞机和未来圆形对撞机至关重要。高粒度时间投影室(TPC)不仅能提供精确的跟踪，还能实现dN/dx测量用于粒子识别。&lt;h4&gt;目的&lt;/h4&gt;引入一种深度学习模型Graph Point Transformer (GraphPT)用于dN/dx重建，解决准确重建面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;将TPC数据表示为点云，采用基于图神经网络的U-Net架构作为网络主干，并融入针对点云处理优化的注意力机制进行节点聚合。&lt;h4&gt;主要发现&lt;/h4&gt;GraphPT模型在PID性能上超越了传统的截断均值方法，特别是在5到20 GeV/c的动量区间内，K/π分离能力提高了约10%至20%。&lt;h4&gt;结论&lt;/h4&gt;GraphPT模型是dN/dx重建的有效方法，能显著提高粒子识别性能，对未来粒子物理实验具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;粒子识别对于未来的粒子物理实验（如圆形正负电子对撞机和未来圆形对撞机）至关重要。高粒度时间投影室不仅能提供精确的跟踪，还能实现dN/dx测量用于粒子识别。dN/dx方法估计初级电离电子的数量，为PID性能提供了显著改进。然而，准确的重建仍然是该方法面临的主要挑战。在本文中，我们介绍了一种深度学习模型——图点变换器（GraphPT），用于dN/dx重建。在我们的方法中，TPC数据被表示为点云。然后网络主干采用基于图神经网络的U-Net架构，结合了针对点云处理优化的注意力机制进行节点聚合。所提出的GraphPT模型在PID性能上超越了传统的截断均值方法。特别是在5到20 GeV/c的动量区间内，K/π分离能力提高了约10%至20%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决高粒度时间投影室(TPC)中的dN/dx重建挑战。dN/dx方法通过估计初级电离电子数量来提高粒子识别(PID)性能，这对未来粒子物理实验如环形正负电子对撞机至关重要。准确重建面临长漂移距离导致的电子扩散、重叠簇区分困难等问题，解决这些问题能显著提升粒子在高动量区域的区分能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了传统截断均值方法的局限性，然后借鉴了多个现有工作：受点变换器(Point Transformer)启发采用U-Net架构；将TPC数据表示为点云并使用图神经网络(GNN)处理；结合自注意力机制和GNN优势设计GraphPT模型；探索了减法算子和点积算子两种注意力机制，其中点积算子是本文创新。作者通过将轨迹表示为点云，利用图神经网络学习点间关系来区分初级和次级电子。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将TPC电子轨迹表示为点云，利用图神经网络和变换器架构学习点间关系，区分初级和次级电子。流程包括：1)将电子击中点表示为点云；2)构建k最近邻图；3)采用U-Net编码器-解码器结构；4)通过编码器提取高维特征，解码器映射回低维空间；5)融入变换器层使用注意力机制聚合信息；6)输出每个节点概率；7)根据概率和阈值计算dN/dx值。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：提出GraphPT模型；将TPC数据表示为点云并利用GNN处理；设计U-Net架构用于点云；提出点积算子作为新注意力机制；将传统两步重建统一到单一模型；采用端到端训练。不同之处：之前工作主要处理一维波形，本文处理点云数据；之前多用规则方法或简单神经网络，本文使用先进图神经网络和变换器；点积算子比减法算子性能更好；在高粒度TPC上K/π分离能力提升显著。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出基于图点变换器的深度学习方法，通过将TPC数据表示为点云并利用图神经网络和注意力机制，显著提高了高粒度TPC中dN/dx重建的准确性，在K/π粒子识别能力上比传统方法提升了10%到20%。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Particle identification (PID) is essential for future particle physicsexperiments such as the Circular Electron-Positron Collider and the FutureCircular Collider. A high-granularity Time Projection Chamber (TPC) not onlyprovides precise tracking but also enables dN/dx measurements for PID. ThedN/dx method estimates the number of primary ionization electrons, offeringsignificant improvements in PID performance. However, accurate reconstructionremains a major challenge for this approach. In this paper, we introduce a deeplearning model, the Graph Point Transformer (GraphPT), for dN/dxreconstruction. In our approach, TPC data are represented as point clouds. Thenetwork backbone adopts a U-Net architecture built upon graph neural networks,incorporating an attention mechanism for node aggregation specificallyoptimized for point cloud processing. The proposed GraphPT model surpasses thetraditional truncated mean method in PID performance. In particular, the$K/\pi$ separation power improves by approximately 10% to 20% in the momentuminterval from 5 to 20 GeV/c.</description>
      <author>example@mail.com (Guang Zhao, Yue Chang, Jinxian Zhang, Linghui Wu, Huirong Qi, Xin She, Mingyi Dong, Shengsen Sun, Jianchun Wang, Yifang Wang, Chunxu Yu)</author>
      <guid isPermaLink="false">2510.10628v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Graph Learning with Graph-Tuple</title>
      <link>http://arxiv.org/abs/2510.10341v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to TAG workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多视图图元组框架，用于解决图神经网络在密集图上的效率问题，通过将图划分为不相交的子图来捕捉多尺度交互信息，并在分子性质预测和宇宙学参数推断两个应用中展示了优越性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络通常随图边数增加而扩展，适合稀疏图但在密集图(如点云或分子相互作用)上效率较低。常见的稀疏化方法通过相似性阈值或距离修剪强制选择单一交互尺度，丢弃其他尺度的重要信息。&lt;h4&gt;目的&lt;/h4&gt;克服单一交互尺度的限制，保留多尺度信息，提高图神经网络在密集图上的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;引入多视图图元组框架，将图划分为不相交的子图捕捉主要局部相互作用和远程连接；通过受非交换算子理论启发的异构消息传递架构学习多视图表示；证明该框架比单图消息传递模型更具表达力并保证更低风险。&lt;h4&gt;主要发现&lt;/h4&gt;在分子性质预测(从特征稀缺的库仑矩阵)和宇宙学参数推断(从几何点云)两个应用中，多视图图元组模型都表现出比单图基线更好的性能。&lt;h4&gt;结论&lt;/h4&gt;多视图方法在处理密集图数据时具有强大的功能和通用性，能够有效捕捉多尺度交互信息，提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通常随图边数的增加而扩展，使其适合稀疏图但在密集图(如点云或分子相互作用)上效率较低。常见的解决方案是通过相似性阈值或距离修剪来稀疏化图，但这强制选择单一交互尺度并丢弃其他尺度的重要信息。为克服这一限制，我们引入了多视图图元组框架。与单一图不同，我们的图元组框架将图划分为不相交的子图，捕捉主要局部相互作用和较弱的远程连接。然后，我们通过受非交换算子理论启发的异构消息传递架构从图元组中学习多视图表示，我们正式证明这比单图消息传递模型更具表达力，并保证更低的风险。我们在两个科学领域实例化了我们的框架：从特征稀缺的库仑矩阵进行分子性质预测，以及从几何点云进行宇宙学参数推断。在这两种应用中，我们的多视图图元组模型都表现出比单图基线更好的性能，突显了我们多视图方法的强大功能和通用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图神经网络（GNNs）在处理密集图（如点云或分子相互作用）时的效率问题。传统方法通过对图进行稀疏化（如相似性阈值化）来提高效率，但这会强制选择单一交互尺度并丢弃其他尺度的重要信息。这个问题在科学和现实应用中很重要，因为许多数据自然表现为密集图结构，如分子中的原子相互作用或宇宙学中的暗物质分布，传统方法会丢失关键信息从而影响模型性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到GNNs在密集图上的计算效率问题和传统稀疏化方法的信息丢失问题。他们考虑了现有解决方案如不变量特征模型和多视图方法，发现这些方法要么依赖低秩假设，要么专为异构图设计。作者设计思路是构建多视图图表示，将单个图根据交互强度划分为强连接图和弱连接图，并受GtNN框架启发，在单层中整合多个消息传递操作。他们借鉴了异构图学习、多视图表示学习和多尺度GNNs的思想，但进行了创新改进以适用于同构图和连续边特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是多视图图元组框架，将单个图划分为多个子图（视图）捕捉不同尺度的交互信息，并通过异构消息传递架构同时学习这些视图。实现流程包括：1) 图元组表示：将图G分解为图元组(G1,...,Gk)，每个子图在同一节点集上但具有不相交边集；2) 异构消息传递：在单层中整合尺度内操作（每个图视图内）和尺度间操作（跨不同图视图），公式为H(l+1) = H(l) + Σ(ci·Hi) + Σ(cij·Hi→j + cji·Hj→i)；3) 两种具体实现：GINE-Gt用于一般图，EGNN-Gt用于几何数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多视图图元组框架，同时保留不同尺度的交互信息；2) 异构消息传递架构，建模不同视图间的算子顺序；3) 理论保证，证明框架更具表现力且保证更低风险。相比之前工作，不同之处在于：不丢弃任何尺度信息（vs 传统稀疏化）；不依赖低秩假设（vs 不变量特征模型）；将异构图学习扩展到同构图，基于物理交互强度构建多视图（vs 多视图方法）；在同一节点集上定义多个图，避免跨级别对齐（vs 多尺度GNNs）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出的多视图图元框架通过异构消息传递架构同时学习不同尺度的图交互，解决了图神经网络在密集图上的效率和表示能力之间的权衡问题，并在科学应用中展示了优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) typically scale with the number of graph edges,making them well suited for sparse graphs but less efficient on dense graphs,such as point clouds or molecular interactions. A common remedy is to sparsifythe graph via similarity thresholding or distance pruning, but this forces anarbitrary choice of a single interaction scale and discards crucial informationfrom other scales. To overcome this limitation, we introduce a multi-viewgraph-tuple framework. Instead of a single graph, our graph-tuple frameworkpartitions the graph into disjoint subgraphs, capturing primary localinteractions and weaker, long-range connections. We then learn multi-viewrepresentations from the graph-tuple via a heterogeneous message-passingarchitecture inspired by the theory of non-commuting operators, which weformally prove is strictly more expressive and guarantees a lower oracle riskcompared to single-graph message-passing models. We instantiate our frameworkon two scientific domains: molecular property prediction from feature-scarceCoulomb matrices and cosmological parameter inference from geometric pointclouds. On both applications, our multi-view graph-tuple models demonstratebetter performance than single-graph baselines, highlighting the power andversatility of our multi-view approach.</description>
      <author>example@mail.com (Shiyu Chen, Ningyuan Huang, Soledad Villar)</author>
      <guid isPermaLink="false">2510.10341v2</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Disentangling Neurodegeneration with Brain Age Gap Prediction Models: A Graph Signal Processing Perspective</title>
      <link>http://arxiv.org/abs/2510.12763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Signal Processing Magazine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了脑年龄差距作为神经退行性疾病生物标志物的应用，提出基于图信号处理和图神经网络的方法，特别是协方差神经网络(VNN)，以改进脑年龄差距预测模型的可靠性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;神经退行性疾病通常通过结构MRI显示的皮层厚度或脑体积减少来评估，但传统方法无法完全捕捉神经退行性病变在空间上的相关性和异质性。脑年龄差距作为一种数据驱动生物标志物虽有潜力，但其实际应用受限于方法学不明确和泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;提供BAGP的概述，并基于图信号处理的最新进展引入一个有原则的应用框架，特别是开发协方差神经网络(VNN)以实现稳健的脑年龄差距预测。&lt;h4&gt;方法&lt;/h4&gt;采用图信号处理、机器学习和网络神经学的综合视角，特别关注图神经网络(GNN)和协方差神经网络(VNN)，后者利用结构MRI推导的解剖协方差矩阵来提供理论基础和操作可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;脑年龄差距是脑健康的紧凑生物标志物，对疾病进展和严重程度具有预测效用。基于图神经网络和协方差神经网络的方法能够提供强大的理论支持和操作可解释性，实现可靠的脑年龄差距预测。&lt;h4&gt;结论&lt;/h4&gt;通过整合多学科视角，阐明了可靠和可解释的BAGP模型的发展路径，并指出了个性化医学的未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;神经退行性疾病以神经元结构或功能的进行性丧失为特征，临床上通常通过结构MRI显示的皮层厚度或脑体积减少来评估。虽然这些方法提供了信息，但传统方法缺乏足够的统计复杂性来完全捕捉神经退行性病变在空间上的相关性和异质性。为解决这些限制，脑年龄差距已成为一种有前途的脑健康数据驱动生物标志物。脑年龄差距预测模型估计从神经影像数据预测的脑年龄与实际年龄之间的差异。由此产生的脑年龄差距作为脑健康的紧凑生物标志物，最近的研究表明它对疾病进展和严重程度具有预测效用。然而，BAGP模型在实际应用中受到其方法学不明确和泛化能力有限的阻碍。本教程文章概述了BAGP，并基于图信号处理的最新进展，为这一应用引入了一个有原则的框架。特别是，我们关注图神经网络，并引入了协方差神经网络，它利用结构MRI推导的解剖协方差矩阵。VNN提供了坚实的理论基础和操作可解释性，能够实现可靠的脑年龄差距预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neurodegeneration, characterized by the progressive loss of neuronalstructure or function, is commonly assessed in clinical practice throughreductions in cortical thickness or brain volume, as visualized by structuralMRI. While informative, these conventional approaches lack the statisticalsophistication required to fully capture the spatially correlated andheterogeneous nature of neurodegeneration, which manifests both in healthyaging and in neurological disorders. To address these limitations, brain agegap has emerged as a promising data-driven biomarker of brain health. The brainage gap prediction (BAGP) models estimate the difference between a person'spredicted brain age from neuroimaging data and their chronological age. Theresulting brain age gap serves as a compact biomarker of brain health, withrecent studies demonstrating its predictive utility for disease progression andseverity. However, practical adoption of BAGP models is hindered by theirmethodological obscurities and limited generalizability across diverse clinicalpopulations. This tutorial article provides an overview of BAGP and introducesa principled framework for this application based on recent advancements ingraph signal processing (GSP). In particular, we focus on graph neural networks(GNNs) and introduce the coVariance neural network (VNN), which leverages theanatomical covariance matrices derived from structural MRI. VNNs offer strongtheoretical grounding and operational interpretability, enabling robustestimation of brain age gap predictions. By integrating perspectives from GSP,machine learning, and network neuroscience, this work clarifies the pathforward for reliable and interpretable BAGP models and outlines future researchdirections in personalized medicine.</description>
      <author>example@mail.com (Saurabh Sihag, Gonzalo Mateos, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2510.12763v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction</title>
      <link>http://arxiv.org/abs/2510.12703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE Consumer Communications &amp; Networking Conference  (CCNC) 2026 - Las Vegas, NV, USA 9 - 12 January 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;自动驾驶面临安全挑战，车辆传感器存在视野受限问题，车辆间通信特别是协作感知消息(CAM)可有效解决这一问题，本文提出的CAMNet模型证明了CAM数据在车辆轨迹预测中的有效性&lt;h4&gt;背景&lt;/h4&gt;自动驾驶任务具有挑战性，现代车辆虽配备LiDAR、摄像头和雷达等昂贵传感器，但存在视野和视线可能被其他车辆遮挡的固有局限性，从而降低态势感知能力&lt;h4&gt;目的&lt;/h4&gt;研究使用协作感知消息(CAM)数据进行车辆轨迹预测，评估CAM数据是否可以被有效利用&lt;h4&gt;方法&lt;/h4&gt;设计并训练名为CAMNet的神经网络模型，在广泛使用的运动预测数据集上进行训练，并在使用CAM数据从头创建的第二个数据集上进行评估&lt;h4&gt;主要发现&lt;/h4&gt;CAM数据确实可以支持车辆轨迹预测，CAMNet模型显示出有希望的结果&lt;h4&gt;结论&lt;/h4&gt;该方法存在一些局限性，这些局限性为未来研究提供了方向&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶仍然是一项具有挑战性的任务，主要由于安全问题。现代车辆通常配备昂贵的传感器，如LiDAR、摄像头和雷达，以降低事故风险。然而，这些传感器存在固有局限性：它们的视野和视线可能被其他车辆遮挡，从而降低态势感知能力。在此背景下，车辆间通信起着关键作用，因为它使车辆能够共享信息，即使在传感器被遮挡的情况下也能保持彼此的感知。实现这一点的一种方式是通过使用协作感知消息(CAM)。在本文中，我们研究使用CAM数据进行车辆轨迹预测。具体来说，我们在广泛使用的运动预测数据集上设计和训练了一个神经网络——基于协作感知消息的图神经网络(CAMNet)。然后，我们在使用协作感知消息从头创建的第二个数据集上评估该模型，以评估这种类型的数据是否可以被有效利用。我们的方法显示出有希望的结果，表明CAM确实可以支持车辆轨迹预测。同时，我们讨论了该方法的几种局限性，这些局限性突出了未来研究的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving remains a challenging task, particularly due to safetyconcerns. Modern vehicles are typically equipped with expensive sensors such asLiDAR, cameras, and radars to reduce the risk of accidents. However, thesesensors face inherent limitations: their field of view and line of sight can beobstructed by other vehicles, thereby reducing situational awareness. In thiscontext, vehicle-to-vehicle communication plays a crucial role, as it enablescars to share information and remain aware of each other even when sensors areoccluded. One way to achieve this is through the use of Cooperative AwarenessMessages (CAMs). In this paper, we investigate the use of CAM data for vehicletrajectory prediction. Specifically, we design and train a neural network,Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widelyused motion forecasting dataset. We then evaluate the model on a second datasetthat we created from scratch using Cooperative Awareness Messages, in order toassess whether this type of data can be effectively exploited. Our approachdemonstrates promising results, showing that CAMs can indeed support vehicletrajectory prediction. At the same time, we discuss several limitations of theapproach, which highlight opportunities for future research.</description>
      <author>example@mail.com (Mattia Grasselli, Angelo Porrello, Carlo Augusto Grazia)</author>
      <guid isPermaLink="false">2510.12703v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>PromoGuardian: Detecting Promotion Abuse Fraud with Multi-Relation Fused Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.12652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The final version of this paper is going to appear in IEEE Symposium  on Security and Privacy 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对电子商务平台中的促销滥用欺诈问题，提出了PROMOGUARDIAN模型，一种多关系融合图神经网络，通过整合交易数据的空间和时间信息来检测欺诈。实验表明该模型能有效识别促销滥用欺诈行为。&lt;h4&gt;背景&lt;/h4&gt;随着电子商务平台的发展，欺诈活动日益增多，对平台的安全性和稳定性构成威胁。促销滥用是近年来增长最快的欺诈类型之一，用户通过利用促销活动从平台获取经济利益。&lt;h4&gt;目的&lt;/h4&gt;研究电子商务平台美团中的促销滥用欺诈问题，并提出有效的检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出PROMOGUARDIAN，一种新颖的多关系融合图神经网络，将交易数据的空间和时间信息整合到同质图中，以检测促销滥用欺诈。&lt;h4&gt;主要发现&lt;/h4&gt;促销滥用欺诈是基于群体的欺诈活动，包含囤积和返现滥用两种类型。与传统欺诈不同，它通常涉及普通客户进行合法交易，且两种欺诈活动常常相互交织。&lt;h4&gt;结论&lt;/h4&gt;在美团真实数据上的实验表明，该模型达到93.15%的精确度，能检测到2.1至5.0倍更多的欺诈者，在生产环境中可防止1.5至8.8倍更多的经济损失，性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;随着电子商务平台的发展，欺诈活动日益增多，对平台的安全性和稳定性构成重大威胁。促销滥用是近年来增长最快的欺诈类型之一，其特点是用户利用促销活动从平台获取经济利益。为研究此问题，我们对电子商务平台美团进行了首次促销滥用欺诈研究。我们发现促销滥用欺诈是基于群体的欺诈活动，包含囤积和返现滥用两种类型。与虚假评论等传统欺诈活动不同，促销滥用欺诈通常涉及普通客户进行合法交易，且这两种欺诈活动常常相互交织。为解决此问题，我们提议利用空间和时间角度的额外信息来检测促销滥用欺诈。在本文中，我们介绍了PROMOGUARDIAN，一种新颖的多关系融合图神经网络，将交易数据的空间和时间信息整合到同质图中以检测促销滥用欺诈。我们在美团的现实数据上进行了广泛实验，结果表明我们提出的模型在促销滥用欺诈检测方面优于最先进的方法，达到93.15%的精确度，能检测到2.1至5.0倍更多的欺诈者，并在生产环境中防止1.5至8.8倍更多的经济损失。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As e-commerce platforms develop, fraudulent activities are increasinglyemerging, posing significant threats to the security and stability of theseplatforms. Promotion abuse is one of the fastest-growing types of fraud inrecent years and is characterized by users exploiting promotional activities togain financial benefits from the platform. To investigate this issue, weconduct the first study on promotion abuse fraud in e-commerce platformsMEITUAN. We find that promotion abuse fraud is a group-based fraudulentactivity with two types of fraudulent activities: Stocking Up and CashbackAbuse. Unlike traditional fraudulent activities such as fake reviews, promotionabuse fraud typically involves ordinary customers conducting legitimatetransactions and these two types of fraudulent activities are oftenintertwined. To address this issue, we propose leveraging additionalinformation from the spatial and temporal perspectives to detect promotionabuse fraud. In this paper, we introduce PROMOGUARDIAN, a novel multi-relationfused graph neural network that integrates the spatial and temporal informationof transaction data into a homogeneous graph to detect promotion abuse fraud.We conduct extensive experiments on real-world data from MEITUAN, and theresults demonstrate that our proposed model outperforms state-of-the-artmethods in promotion abuse fraud detection, achieving 93.15% precision,detecting 2.1 to 5.0 times more fraudsters, and preventing 1.5 to 8.8 timesmore financial losses in production environments.</description>
      <author>example@mail.com (Shaofei Li, Xiao Han, Ziqi Zhang, Minyao Hua, Shuli Gao, Zhenkai Liang, Yao Guo, Xiangqun Chen, Ding Li)</author>
      <guid isPermaLink="false">2510.12652v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs</title>
      <link>http://arxiv.org/abs/2510.12401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在大规模异构图上预训练图神经网络的有效框架，解决了现有方法仅适用于同构图且未考虑语义不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络促进了图数据挖掘发展，但训练需要大量昂贵且有时不可用的有标签数据。现有自监督预训练方法主要针对同构图设计，而现实世界多为异构图，且未考虑语义不匹配问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个在大规模异构图上预训练GNNs的有效框架，解决语义不匹配问题并提高模型的可转移性。&lt;h4&gt;方法&lt;/h4&gt;设计了结构感知的预训练任务捕获异构图结构特性，以及语义感知的预训练任务解决语义不匹配。通过构建由语义邻居组成的扰动子空间，使模型更关注语义空间中的通用知识，学习具有更好可转移性的知识。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界大规模异构图上的大量实验表明，所提出的方法优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架能有效在大规模异构图上预训练GNNs，解决了现有方法在同构图和语义不匹配方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络促进了图数据挖掘的发展。然而，训练GNNs需要足够的有标签任务特定数据，这些数据昂贵且有时不可用。为减少对有标签数据的依赖，最近研究提出通过自监督方式预训练GNNs，然后在有有限标签数据的下游任务中应用预训练的GNNs。然而，大多数现有方法仅针对同构图设计（现实世界中的图大多是异构图），且未考虑语义不匹配问题（原始数据与包含更多可转移语义信息的理想数据之间的语义差异）。本文提出了一种在大规模异构图上预训练GNNs的有效框架。我们首先设计了一个结构感知的预训练任务，旨在捕获异构图中的结构特性。然后，设计了一个语义感知的预训练任务来解决不匹配问题。具体而言，我们构建了一个由语义邻居组成的扰动子空间，帮助处理语义不匹配。语义邻居使模型更专注于语义空间中的通用知识，进而帮助模型学习具有更好可转移性的知识。最后，在真实世界大规模异构图上进行了大量实验，证明了所提出方法优于最先进的基线方法。代码可在https://github.com/sunshy-1/PHE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, graph neural networks (GNNs) have facilitated thedevelopment of graph data mining. However, training GNNs requires sufficientlabeled task-specific data, which is expensive and sometimes unavailable. To beless dependent on labeled data, recent studies propose to pre-train GNNs in aself-supervised manner and then apply the pre-trained GNNs to downstream taskswith limited labeled data. However, most existing methods are designed solelyfor homogeneous graphs (real-world graphs are mostly heterogeneous) and do notconsider semantic mismatch (the semantic difference between the original dataand the ideal data containing more transferable semantic information). In thispaper, we propose an effective framework to pre-train GNNs on the large-scaleheterogeneous graph. We first design a structure-aware pre-training task, whichaims to capture structural properties in heterogeneous graphs. Then, we designa semantic-aware pre-training task to tackle the mismatch. Specifically, weconstruct a perturbation subspace composed of semantic neighbors to help dealwith the semantic mismatch. Semantic neighbors make the model focus more on thegeneral knowledge in the semantic space, which in turn assists the model inlearning knowledge with better transferability. Finally, extensive experimentsare conducted on real-world large-scale heterogeneous graphs to demonstrate thesuperiority of the proposed method over state-of-the-art baselines. Codeavailable at https://github.com/sunshy-1/PHE.</description>
      <author>example@mail.com (Shengyin Sun, Chen Ma, Jiehao Chen)</author>
      <guid isPermaLink="false">2510.12401v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand</title>
      <link>http://arxiv.org/abs/2510.12328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合物理信息的图神经网络与极值分析技术相结合的方法，用于提高泰国地区的降雨预测准确性，特别是对极端事件的预测。&lt;h4&gt;背景&lt;/h4&gt;准确的降雨预测，特别是极端事件的预测，在气候学和地球系统中仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法，结合物理信息的图神经网络与极值分析技术，以提高泰国各气象站的降雨预测能力。&lt;h4&gt;方法&lt;/h4&gt;使用图结构表示气象站捕捉时空模式，预处理相关气候指标，提出Attention-LSTM模型，使用基于地形降水物理公式的边特征，并通过空间季节感知GPD方法进行POT映射处理极端值。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在大多数地区（包括易发生极端事件的地区）优于成熟的基线方法，并与最先进方法保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;与业务预测系统SEAS5相比，该方法改进了极端事件的预测，并为生产支持长期水资源管理的精细分辨率地图提供了实际改进。&lt;h4&gt;翻译&lt;/h4&gt;准确的降雨预测，特别是对于极端事件，在气候学和地球系统中仍然是一个重大挑战。本文提出了一种新颖的物理信息图神经网络(GNNs)结合极值分析技术，以提高泰国各气象站的降雨预测。该模型利用气象站的图结构表示来捕捉复杂的时空模式，并通过遥相关提供可解释性。我们预处理了可能影响区域降雨的相关气候指标。所提出的图注意力网络与长短期记忆网络(Attention-LSTM)应用了注意力机制，使用基于简单地形降水物理公式推导的初始边特征。嵌入随后由LSTM层处理。为解决极值问题，我们使用新颖的空间季节感知广义帕累托分布(GPD)方法进行阈值超限(POT)映射，克服了传统机器学习模型的局限性。实验表明，我们的方法在大多数地区（包括易发生极端事件的地区）优于成熟的基线方法，并与最先进方法保持强劲竞争力。与业务预测系统SEAS5相比，我们的实际应用改进了极端事件的预测，并为生产支持长期水资源管理的精细分辨率地图提供了实际增强。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate rainfall forecasting, particularly for extreme events, remains asignificant challenge in climatology and the Earth system. This paper presentsnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-valueanalysis techniques to improve gauge-station rainfall predictions acrossThailand. The model leverages a graph-structured representation of gaugestations to capture complex spatiotemporal patterns, and it offersexplainability through teleconnections. We preprocess relevant climate indicesthat potentially influence regional rainfall. The proposed Graph AttentionNetwork with Long Short-Term Memory (Attention-LSTM) applies the attentionmechanism using initial edge features derived from simpleorographic-precipitation physics formulation. The embeddings are subsequentlyprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold(POT) mapping using the novel Spatial Season-aware Generalized ParetoDistribution (GPD) method, which overcomes limitations of traditionalmachine-learning models. Experiments demonstrate that our method outperformswell-established baselines across most regions, including areas prone toextremes, and remains strongly competitive with the state of the art. Comparedwith the operational forecasting system SEAS5, our real-world applicationimproves extreme-event prediction and offers a practical enhancement to producefine-resolution maps that support decision-making in long-term watermanagement.</description>
      <author>example@mail.com (Kiattikun Chobtham, Kanoksri Sarinnapakorn, Kritanai Torsri, Prattana Deeprasertkul, Jirawan Kamma)</author>
      <guid isPermaLink="false">2510.12328v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Using STAR-IRS to Secure Indoor Communications Through Symbol-Level Random Phase Modulation</title>
      <link>http://arxiv.org/abs/2510.11925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于同时传输与反射智能反射面(STAR-IRS)的安全室内通信方案，通过动态分割电磁波并控制反射和传输信号来增强安全通信性能。&lt;h4&gt;背景&lt;/h4&gt;在室内通信环境中，发射方(Alice)需要向室内用户(Bob)发送机密信息，同时存在室外窃听者(Eves)的威胁，传统通信方案难以有效保障通信安全。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够保护传输免受窃听的通信方案，最大化安全速率，并通过硬件加速降低计算延迟。&lt;h4&gt;方法&lt;/h4&gt;部署STAR-IRS在墙壁或窗户上，将入射电磁波动态分割为透射和反射两个分量；控制反射信号增强Bob的接收质量，对透射信号进行符号级随机相位调制降低Eves的信号质量；提出基于图神经网络(GNN)的方案解决安全速率最大化问题；设计基于FPGA的GNN加速器减少计算延迟。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的策略在安全性方面优于传统方案和仅反射方案；GNN方法在解决优化问题时比MRT、ZF和MMSE等基准技术取得更优结果；基于FPGA的加速器实现了低推理延迟。&lt;h4&gt;结论&lt;/h4&gt;STAR-IRS结合GNN和FPGA加速器可以有效提高室内通信的安全性，为安全通信提供了一种高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于同时传输与反射智能反射面(STAR-IRS)的安全室内通信方案。具体而言，发射方(Alice)向室内目标用户(Bob)发送机密信息，同时有几个窃听者(Eves)潜伏在外部。为了保护传输免受窃听，在墙壁或窗户上部署了STAR-IRS。当电磁波撞击到STAR-IRS时，入射电磁波被动态分割为两个分量，实现通过表面的传输和表面的反射。反射信号被控制以增强Bob的接收，而透射信号则用符号级随机相移进行调制，以降低Eves的信号质量。基于这种设置，构建了安全速率最大化问题。为解决这一问题，开发了基于图神经网络(GNN)的方案。此外，还设计了一个基于FPGA的GNN加速器以减少计算延迟。仿真结果表明，所提出的策略在安全性方面优于传统方案和仅反射方案。此外，在解决优化问题时，GNN方法比最大比传输(MRT)、迫零(ZF)和最小均方误差(MMSE)等基准技术取得更好的结果。最后，实验评估确认基于FPGA的加速器实现了低推理延迟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a secure indoor communication scheme based onsimultaneous transmitting and reflecting intelligent reflecting surface(STAR-IRS). Specifically, a transmitter (Alice) sends confidential informationto its intended user (Bob) indoors, while several eavesdroppers (Eves) lurkoutside. To safeguard the transmission from eavesdropping, the STAR-IRS isdeployed on walls or windows. Upon impinging on the STAR-IRS, the incomingelectromagnetic wave is dynamically partitioned into two components, enablingboth transmission through and reflection from the surface. The reflected signalis controlled to enhance reception at Bob, while the transmitted signal ismodulated with symbol-level random phase shifts to degrade the signal qualityat Eves. Based on such a setting, the secrecy rate maximization problem isformulated. To solve it, a graph neural network (GNN)-based scheme isdeveloped. Furthermore, a field-programmable gate array (FPGA)-based GNNaccelerator is designed to reduce computational latency. Simulation resultsdemonstrate that the proposed strategy outperforms both the conventional schemeand the reflection-only scheme in terms of secrecy performance. Moreover, theGNN-based approach achieves superior results compared to benchmark techniquessuch as maximum ratio transmission (MRT), zero forcing (ZF), and minimum meansquare error (MMSE) in solving the optimization problem. Finally, experimentalevaluations confirm that the FPGA-based accelerator enables low inferencelatency.</description>
      <author>example@mail.com (Yanan Du, Zeyang Sun, Yilan Zhang, Sai Xu, Beiyuan Liu)</author>
      <guid isPermaLink="false">2510.11925v1</guid>
      <pubDate>Wed, 15 Oct 2025 15:25:27 +0800</pubDate>
    </item>
    <item>
      <title>Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs</title>
      <link>http://arxiv.org/abs/2406.11569v5</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages, 10 figures, submitted for possible journal publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了基于元学习的个性化联邦学习(meta-pFL)在无线环境中的泛化性能，分析了泛化到新代理和任务与收敛性之间的权衡关系。&lt;h4&gt;背景&lt;/h4&gt;现代AI应用如大型语言模型的训练范式已转变为预训练后微调；由于开放数据减少和AI模型访问民主化，预训练正从集中式部署转向联邦学习实现。&lt;h4&gt;目的&lt;/h4&gt;研究meta-pFL在无线环境中的泛化性能，探索对新代理和任务的泛化与收敛性之间的权衡。&lt;h4&gt;方法&lt;/h4&gt;采用空中计算技术，研究通过共享无线信道连接到服务器的无线环境中参与预训练阶段的代理的情况。&lt;h4&gt;主要发现&lt;/h4&gt;信道损伤可能会增强泛化能力同时降低收敛性，存在泛化与收敛性之间的权衡关系。&lt;h4&gt;结论&lt;/h4&gt;大量的数值结果验证了所提出的理论。&lt;h4&gt;翻译&lt;/h4&gt;对于现代人工智能应用（如大型语言模型），训练范式已转变为预训练后微调。此外，由于开放数据存储库减少以及AI模型访问民主化的努力，预训练预计将从当前集中式部署转向联邦学习实现。元学习提供了一个可以将预训练和微调形式化的通用框架。基于元学习的个性化联邦学习(meta-pFL)通过针对新代理和任务的泛化能力，超越了基本个性化。本文研究了meta-pFL在无线环境中的泛化性能，其中参与预训练阶段的代理通过共享无线信道连接到服务器。采用空中计算，我们研究了泛化到新代理和任务与收敛性之间的权衡。这种权衡源于信道损伤可能增强泛化同时降低收敛性的事实。大量的数值结果验证了该理论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-06-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For modern artificial intelligence (AI) applications such as large languagemodels (LLMs), the training paradigm has recently shifted to pre-trainingfollowed by fine-tuning. Furthermore, owing to dwindling open repositories ofdata and thanks to efforts to democratize access to AI models, pre-training isexpected to increasingly migrate from the current centralized deployments tofederated learning (FL) implementations. Meta-learning provides a generalframework in which pre-training and fine-tuning can be formalized.Meta-learning-based personalized FL (meta-pFL) moves beyond basicpersonalization by targeting generalization to new agents and tasks. This paperstudies the generalization performance of meta-pFL for a wireless setting inwhich the agents participating in the pre-training phase, i.e., meta-learning,are connected via a shared wireless channel to the server. Adoptingover-the-air computing, we study the trade-off between generalization to newagents and tasks, on the one hand, and convergence, on the other hand. Thetrade-off arises from the fact that channel impairments may enhancegeneralization, while degrading convergence. Extensive numerical resultsvalidate the theory.</description>
      <author>example@mail.com (Haifeng Wen, Hong Xing, Osvaldo Simeone)</author>
      <guid isPermaLink="false">2406.11569v5</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
  <item>
      <title>High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.11017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新框架，通过扩展Mamba模型来分别学习全局和局部高分辨率时空表示，用于视频人体姿态估计(VHPE)。该框架包含全局时空Mamba和局部细化Mamba两个组件，有效解决了现有方法在平衡全局和局部动态建模方面的困难。&lt;h4&gt;背景&lt;/h4&gt;高分辨率时空表示建模对于视频人体姿态估计至关重要，需要同时考虑全局动态上下文和局部运动细节。当前最先进方法在单一建模结构中统一时空学习，难以平衡全局和局部动态建模，且在捕获全局依赖时具有二次复杂度，限制了在高分辨率序列中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，从两个方面扩展Mamba模型，分别学习VHPE的全局和局部高分辨率时空表示，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出全局时空Mamba，执行6D选择性时空扫描和时空调制扫描合并，从高分辨率序列中高效提取全局表示；引入基于窗口时空扫描的局部细化Mamba，增强局部关键点运动的高频细节。这种方法结合了Mamba的线性复杂度和处理长程上下文的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上的大量实验表明，所提出的模型优于最先进的VHPE方法，同时实现了更好的计算权衡。&lt;h4&gt;结论&lt;/h4&gt;通过分别建模全局和局部时空表示，可以提高VHPE性能。扩展Mamba框架可以有效地处理高分辨率视频数据，解决了现有方法在计算效率和表示能力之间的平衡问题。&lt;h4&gt;翻译&lt;/h4&gt;建模高分辨率时空表示，包括全局动态上下文(如整体人体运动趋势)和局部运动细节(如关键点的高频变化)，对于基于视频的人体姿态估计(VHPE)至关重要。当前最先进方法通常在单一类型的建模结构(卷积或基于注意力的块)中统一时空学习，这些方法本质上难以平衡全局和局部动态建模，可能导致网络偏向其中一种，从而产生次优性能。此外，现有VHPE模型在捕获全局依赖时具有二次复杂度，限制了它们在高分辨率序列中的应用。最近，状态空间模型(称为Mamba)在建模具有线性复杂度的长程上下文方面显示出巨大潜力；然而，它们仅限于1D序列数据。在本文中，我们提出了一种新框架，从两个方面扩展Mamba，分别学习VHPE的全局和局部高分辨率时空表示。具体而言，我们首先提出了全局时空Mamba，它执行6D选择性时空扫描和时空调制扫描合并，从高分辨率序列中高效提取全局表示。我们进一步引入了基于窗口时空扫描的局部细化Mamba，以增强局部关键点运动的高频细节。在四个基准数据集上的大量实验表明，所提出的模型优于最先进的VHPE方法，同时实现了更好的计算权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling high-resolution spatiotemporal representations, including bothglobal dynamic contexts (e.g., holistic human motion tendencies) and localmotion details (e.g., high-frequency changes of keypoints), is essential forvideo-based human pose estimation (VHPE). Current state-of-the-art methodstypically unify spatiotemporal learning within a single type of modelingstructure (convolution or attention-based blocks), which inherently havedifficulties in balancing global and local dynamic modeling and may bias thenetwork to one of them, leading to suboptimal performance. Moreover, existingVHPE models suffer from quadratic complexity when capturing globaldependencies, limiting their applicability especially for high-resolutionsequences. Recently, the state space models (known as Mamba) have demonstratedsignificant potential in modeling long-range contexts with linear complexity;however, they are restricted to 1D sequential data. In this paper, we present anovel framework that extends Mamba from two aspects to separately learn globaland local high-resolution spatiotemporal representations for VHPE.Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6Dselective space-time scan and spatial- and temporal-modulated scan merging toefficiently extract global representations from high-resolution sequences. Wefurther introduce a windowed space-time scan-based Local Refinement Mamba toenhance the high-frequency details of localized keypoint motions. Extensiveexperiments on four benchmark datasets demonstrate that the proposed modeloutperforms state-of-the-art VHPE approaches while achieving bettercomputational trade-offs.</description>
      <author>example@mail.com (Runyang Feng, Hyung Jin Chang, Tze Ho Elden Tse, Boeun Kim, Yi Chang, Yixing Gao)</author>
      <guid isPermaLink="false">2510.11017v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2510.10365v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PointMAC是一种创新的元学习框架，用于点云补全的测试时适应，通过自监督辅助目标和元辅助学习策略，解决了现有模型无法适应新结构模式和传感器失真的问题，实现了高质量的个体样本补全。&lt;h4&gt;背景&lt;/h4&gt;点云补全对机器人和增强现实等安全关键应用中的鲁棒3D感知至关重要。现有模型执行静态推理，严重依赖训练期间归纳偏置，限制了它们适应新结构模式和传感器引起失真的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型无法适应测试时新结构模式和传感器失真的问题，提出PointMAC框架实现测试时鲁棒适应。&lt;h4&gt;方法&lt;/h4&gt;提出PointMAC框架，通过两个自监督辅助目标模拟结构和传感器级别的不完整性；基于模型无关元学习的元辅助学习策略确保适应与主要任务一致；推理时通过优化辅助损失实时适应共享编码器；引入自适应λ校准机制平衡主要和辅助目标间的梯度。&lt;h4&gt;主要发现&lt;/h4&gt;在合成、模拟和真实世界数据集上的广泛实验表明，PointMAC通过单独细化每个样本来产生高质量补全，实现了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;据我们所知，这是首次将元辅助测试时适应应用于点云补全的工作。&lt;h4&gt;翻译&lt;/h4&gt;点云补全对于机器人和增强现实等安全关键应用中的鲁棒3D感知至关重要。然而，现有模型执行静态推理，并严重依赖训练期间学习的归纳偏置，限制了它们在测试时适应新结构模式和传感器引起失真的能力。为了解决这一限制，我们提出了PointMAC，一种用于点云补全的元学习框架，实现测试时的鲁棒适应。它无需额外监督即可实现样本特定细化。我们的方法在两个自监督辅助目标下优化补全模型，这些目标模拟结构和传感器级别的不完整性。基于模型无关元学习的元辅助学习策略确保由辅助目标驱动的适应与主要补全任务保持一致。在推理过程中，我们通过优化辅助损失实时适应共享编码器，同时保持解码器固定。为了进一步稳定适应，我们引入了自适应λ校准，一种用于平衡主要和辅助目标之间梯度的元学习机制。在合成、模拟和真实世界数据集上的广泛实验表明，PointMAC通过单独细化每个样本来产生高质量补全，实现了最先进的结果。据我们所知，这是首次将元辅助测试时适应应用于点云补全的工作。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决点云补全中的静态推理问题，即现有模型在测试时使用固定推理方式，难以适应新的结构模式和传感器引起的失真。这个问题在现实中很重要，因为点云补全对机器人、自动驾驶和增强现实等安全关键应用至关重要，而现实世界中的点云常因遮挡、有限覆盖和传感器噪声而不完整，现有模型在处理这些情况时表现不佳，生成'通用补全'而非针对特定样本的'样本特定补全'。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从静态推理转向动态、样本特定适应的思路，利用测试时适应框架让模型自我调整。他们设计PointMAC框架，包含Bi-Aux Units执行自监督任务，并采用MAML规范适应过程。该方法借鉴了TTA在动态场景去模糊等领域的应用，元学习在少样本学习中的成功经验，以及点云补全领域的编码器-解码器架构和transformer模型设计思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过元辅助学习实现测试时样本特定适应，使模型能根据每个输入的独特几何形状和噪声动态调整内部表示。流程包括：1)网络架构(共享编码器、主要解码器和Bi-Aux Units)；2)训练流程(内部辅助适应、外部主要对齐和自适应λ校准)；3)推理流程(对每个测试样本执行自监督梯度步骤，细化共享编码器，生成样本特定补全)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个将元辅助学习和测试时适应应用于点云补全的框架；2)Bi-Aux Units设计自监督双辅助任务；3)基于MAML的元辅助学习策略；4)自适应λ校准机制。相比之前工作，PointMAC实现了从静态到动态的转变，采用自监督适应而非额外监督，解决了辅助任务与主要任务对齐问题，提高了泛化能力，并提供了端到端的框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointMAC通过元辅助学习的测试时适应框架，使点云补全模型能动态调整每个输入样本的内部表示，在不依赖额外监督的情况下生成高质量、样本特定的补全结果，显著提高了模型对未见过的结构模式和传感器噪声的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is essential for robust 3D perception insafety-critical applications such as robotics and augmented reality. However,existing models perform static inference and rely heavily on inductive biaseslearned during training, limiting their ability to adapt to novel structuralpatterns and sensor-induced distortions at test time. To address thislimitation, we propose PointMAC, a meta-learned framework for robust test-timeadaptation in point cloud completion. It enables sample-specific refinementwithout requiring additional supervision. Our method optimizes the completionmodel under two self-supervised auxiliary objectives that simulate structuraland sensor-level incompleteness. A meta-auxiliary learning strategy based onModel-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliaryobjectives is consistently aligned with the primary completion task. Duringinference, we adapt the shared encoder on-the-fly by optimizing auxiliarylosses, with the decoder kept fixed. To further stabilize adaptation, weintroduce Adaptive $\lambda$-Calibration, a meta-learned mechanism forbalancing gradients between primary and auxiliary objectives. Extensiveexperiments on synthetic, simulated, and real-world datasets demonstrate thatPointMAC achieves state-of-the-art results by refining each sample individuallyto produce high-quality completions. To the best of our knowledge, this is thefirst work to apply meta-auxiliary test-time adaptation to point cloudcompletion.</description>
      <author>example@mail.com (Linlian Jiang, Rui Ma, Li Gu, Ziqiang Wang, Xinxin Zuo, Yang Wang)</author>
      <guid isPermaLink="false">2510.10365v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View</title>
      <link>http://arxiv.org/abs/2510.11687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的、类别不可知的框架，可以从单个RGB-D图像同时预测物体的6D姿态、大小和密集形状，无需模板、CAD模型或类别标签，在多个基准数据集上实现了最先进性能，并展现出强大的零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;从视觉输入估计物体的6D姿态、大小和形状是计算机视觉中的基础问题，在机器人抓取和操作中有关键应用。现有方法要么依赖特定于对象的先验，要么由于姿态-形状纠缠和多阶段管道而泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需对象特定先验的统一框架，能够从单个RGB-D图像同时预测6D姿态、大小和密集形状，并实现跨类别的强泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用Transformer编码器（由Mixture-of-Experts增强）融合来自视觉基础模型的密集2D特征和部分3D点云，采用并行解码器进行姿态-大小估计和形状重建，实现28 FPS的实时推理。仅在SOPE数据集的149个类别的合成数据上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同基准数据集（涵盖300多个类别）上评估，在已见类别上达到最先进精度，同时对未见到的真实世界物体表现出强大的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架为机器人和具身AI中的开放集6D理解建立了新标准，无需对象特定先验即可实现高精度和强泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;从视觉输入估计物体的6D姿态、大小和形状是计算机视觉中的一个基础问题，在机器人抓取和操作中具有关键应用。现有方法要么依赖于特定于对象的先验（如CAD模型或模板），要么由于姿态-形状纠缠和多阶段管道而在跨类别泛化方面受到限制。在这项工作中，我们提出了一个统一的、类别不可知的框架，可以从单个RGB-D图像同时预测6D姿态、大小和密集形状，测试时不需要模板、CAD模型或类别标签。我们的模型使用由Mixture-of-Experts增强的Transformer编码器融合来自视觉基础模型的密集2D特征和部分3D点云，并采用并行解码器进行姿态-大小估计和形状重建，实现28 FPS的实时推理。仅在SOPE数据集的149个类别的合成数据上进行训练后，我们的框架在四个不同的基准数据集SOPE、ROPE、ObjaversePose和HANDAL上进行了评估，涵盖300多个类别。它在已见类别上实现了最先进的精度，同时展现出对未见真实世界物体 remarkably强的零样本泛化能力，为机器人和具身AI中的开放集6D理解建立了新标准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单张RGB-D图像中估计物体6D位姿、大小和形状的问题，而不需要依赖特定模板、CAD模型或类别标签。这个问题在机器人抓取、操作以及具身AI领域至关重要，因为现有方法要么需要物体特定先验知识，要么在跨类别泛化能力上有限，限制了在开放场景下的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：实例级方法需要参考图像或CAD模型，类别级方法存在姿态-形状纠缠和多阶段流水线问题。他们设计了一个统一框架，融合视觉基础模型的密集2D特征与3D点云，使用增强Mixture-of-Experts的Transformer编码器，并采用并行解码器。该方法借鉴了DGCNN处理特征、Transformer架构、RADIOv2.5基础模型提取语义特征、DenseFusion的特征融合方式以及NOCS坐标系表示等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的、类别无关的框架，同时预测6D位姿、大小和形状，避免多阶段流水线，实现测试时无需模板或类别标签。整体流程：1)使用RADIOv2.5提取RGB图像的密集2D特征；2)将2D特征与3D点云坐标融合；3)通过DGCNN处理融合特征；4)使用带有Mixture-of-Experts的Transformer编码器生成全局表示；5)通过并行解码器进行姿态-大小直接回归和形状重建(采用粗到细策略)；6)结合多种损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个统一类别无关框架同时估计6D位姿、大小和形状；2)设计可扩展架构融合2D特征与3D点云，使用增强Mixture-of-Experts的Transformer；3)实现28 FPS实时推理；4)仅合成数据训练但展现强大零样本泛化；5)构建ObjaversePose数据集。不同之处：无需测试时模板/CAD模型/类别标签；统一端到端框架避免多阶段处理；同时处理位姿、大小和形状捕获相互依赖；使用MoE提高对不同形状分布建模能力；合成数据训练但能泛化到真实世界和未见类别。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种统一的、类别无关的框架，能够从单张RGB-D图像中实时估计物体的6D位姿、大小和完整形状，无需测试时的模板或类别标签，并在多种未见过的真实物体上展示了强大的零样本泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating an object's 6D pose, size, and shape from visual input is afundamental problem in computer vision, with critical applications in roboticgrasping and manipulation. Existing methods either rely on object-specificpriors such as CAD models or templates, or suffer from limited generalizationacross categories due to pose-shape entanglement and multi-stage pipelines. Inthis work, we propose a unified, category-agnostic framework thatsimultaneously predicts 6D pose, size, and dense shape from a single RGB-Dimage, without requiring templates, CAD models, or category labels at testtime. Our model fuses dense 2D features from vision foundation models withpartial 3D point clouds using a Transformer encoder enhanced by aMixture-of-Experts, and employs parallel decoders for pose-size estimation andshape reconstruction, achieving real-time inference at 28 FPS. Trained solelyon synthetic data from 149 categories in the SOPE dataset, our framework isevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,spanning over 300 categories. It achieves state-of-the-art accuracy on seencategories while demonstrating remarkably strong zero-shot generalization tounseen real-world objects, establishing a new standard for open-set 6Dunderstanding in robotics and embodied AI.</description>
      <author>example@mail.com (Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu)</author>
      <guid isPermaLink="false">2510.11687v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection</title>
      <link>http://arxiv.org/abs/2510.11632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NV3D的新型3D物体检测模型，利用从体素邻居获取的局部法向量特征来提高检测性能，并通过两种采样策略减少数据量同时保持性能。&lt;h4&gt;背景&lt;/h4&gt;近期自动驾驶车辆3D物体检测研究试图通过多模态设置或从LiDAR点云中提取局部模式来丰富特征，但多模态方法面临特征对齐挑战，局部特征获取对于复杂任务可能过于简化。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的3D物体检测模型NV3D，解决现有方法的局限性，提高检测性能并减少数据量。&lt;h4&gt;方法&lt;/h4&gt;NV3D利用基于K近邻和主成分分析计算每个体素的法向量作为局部特征，确定表面与目标实体间关系；提供两种采样策略：基于法向量密度的采样和基于视场感知的基于bin的采样；应用元素级注意力融合机制，将体素特征作为查询和值，法向量特征作为键。&lt;h4&gt;主要发现&lt;/h4&gt;使用两种采样策略可消除高达55%的数据同时保持性能；在KITTI数据集上，NV3D在汽车和骑行者检测方面优于基线；不使用采样时，NV3D比Voxel R-CNN分别高出2.61%和4.23% mAP；使用采样后，仍比基线高1.56% mAP，同时过滤了约55%体素。&lt;h4&gt;结论&lt;/h4&gt;NV3D模型通过利用法向量特征和有效采样策略，能够在减少数据量的同时提高3D物体检测性能，特别是在具有特定空间形状的物体检测上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;近期关于自动驾驶车辆3D物体检测的研究试图通过多模态设置或从LiDAR点云中提取局部模式来丰富特征。然而，多模态方法面临特征对齐的重大挑战，而局部特征获取对于复杂的3D物体检测任务可能过于简化。在本文中，我们提出了一种新型模型NV3D，它利用从体素邻居获取的局部特征，作为使用K近邻和主成分分析按每个体素基础计算的法向量。这种信息丰富的特征使NV3D能够确定表面与相关目标实体之间的关系，包括汽车、行人或骑行者。在法向量提取过程中，NV3D提供两种不同的采样策略：基于法向量密度的采样和基于视场感知的基于bin的采样，允许消除高达55%的数据同时保持性能。此外，我们应用了元素级注意力融合，将体素特征作为查询和值，法向量特征作为键，类似于注意力机制。我们的方法在KITTI数据集上训练，并在汽车和骑行者检测方面表现出优越性能，这得益于它们的空间形状。在验证集上，不使用采样的NV3D达到86.60%和80.18%的平均精度均值(mAP)，分别比基线Voxel R-CNN高出2.61%和4.23% mAP。使用两种采样后，NV3D在汽车检测中达到85.54% mAP，比基线高出1.56% mAP，尽管大约55%的体素被过滤掉。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D目标检测中的特征提取效率和计算复杂度问题。在自动驾驶领域，精确检测车辆、行人和骑行者等对象至关重要，但现有方法要么依赖多模态数据融合面临特征对齐挑战，要么仅使用局部特征提取对复杂任务过于简化。提高检测精度和效率对于确保自动驾驶系统的安全性和可靠性具有现实意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察发现点云数据中存在冗余，特别是近距离密集点云集群，并提出几何特征可从邻近点提取的假设。他们借鉴了Voxel R-CNN作为基础架构，采用KNN和PCA方法提取法向量特征，受VirConv中关于远距离点影响更大的启发，并参考了注意力机制设计了元素级注意力融合。这种方法结合了传统几何处理和深度学习的优势，既减少了计算复杂度又保留了关键空间信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用局部体素特征计算法向量作为新特征表示，通过采样减少冗余数据，并融合体素和法向量特征。实现流程包括：1)将LiDAR点云转换为体素；2)使用KNN和PCA提取法向量特征；3)应用法向量密度采样(去除密度&gt;0.7的50%体素)和FOV感知的基于bin采样(保持空间连续性)；4)通过元素级注意力机制融合两种特征；5)将融合特征输入Voxel R-CNN架构进行目标检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引入法向量作为3D目标检测的新特征表示；2)提出两种创新采样策略(法向量密度采样和FOV感知的基于bin采样)；3)设计元素级注意力融合机制。相比之前工作，NV3D专注于单模态LiDAR数据处理避免了特征对齐问题；使用法向量而非直接处理点云减少了计算复杂度；采样策略考虑了法向量和视野因素保留了更多有用信息；将局部特征转换为法向量表示提高了效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NV3D通过引入法向量特征和创新的采样策略，在保持高性能的同时显著提高了3D目标检测的效率，特别是在车辆和骑行者检测任务中表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies in 3D object detection for autonomous vehicles aim to enrichfeatures through the utilization of multi-modal setups or the extraction oflocal patterns within LiDAR point clouds. However, multi-modal methods facesignificant challenges in feature alignment, and gaining features locally canbe oversimplified for complex 3D object detection tasks. In this paper, wepropose a novel model, NV3D, which utilizes local features acquired from voxelneighbors, as normal vectors computed per voxel basis using K-nearest neighbors(KNN) and principal component analysis (PCA). This informative feature enablesNV3D to determine the relationship between the surface and pertinent targetentities, including cars, pedestrians, or cyclists. During the normal vectorextraction process, NV3D offers two distinct sampling strategies: normal vectordensity-based sampling and FOV-aware bin-based sampling, allowing eliminationof up to 55% of data while maintaining performance. In addition, we appliedelement-wise attention fusion, which accepts voxel features as the query andvalue and normal vector features as the key, similar to the attentionmechanism. Our method is trained on the KITTI dataset and has demonstratedsuperior performance in car and cyclist detection owing to their spatialshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP incar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% ofvoxels being filtered out.</description>
      <author>example@mail.com (Krittin Chaowakarn, Paramin Sangwongngam, Nang Htet Htet Aung, Chalie Charoenlarpnopparut)</author>
      <guid isPermaLink="false">2510.11632v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SNAP: Towards Segmenting Anything in Any Point Cloud</title>
      <link>http://arxiv.org/abs/2510.11565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page, https://neu-vi.github.io/SNAP/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SNAP是一个统一的交互式3D点云分割模型，支持跨不同领域的点和文本提示，通过多数据集训练和领域自适应归一化实现跨领域通用性，在多个基准测试上取得了最先进或具有竞争力的结果。&lt;h4&gt;背景&lt;/h4&gt;交互式3D点云分割通过用户引导提示能够高效标注复杂3D场景，但当前方法通常局限于单一领域（室内或室外）和单一形式的用户交互（空间点击或文本提示）。在多个数据集上训练通常会导致负迁移，导致缺乏通用性的领域特定工具。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一模型，支持跨不同领域的基于点和基于文本的提示进行交互式3D分割，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出SNAP（Segment Anything in Any Point cloud）模型，通过在涵盖室内、室外和空中环境的7个数据集上训练实现跨领域通用性，使用领域自适应归一化来防止负迁移。对于文本提示的分割，自动生成掩码提案并与文本查询的CLIP嵌入进行匹配，支持全景和开放词汇分割。&lt;h4&gt;主要发现&lt;/h4&gt;SNAP在空间提示分割的9个零样本基准测试中，有8个达到了最先进的性能，在所有5个文本提示基准测试中展示了具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;统一模型可以匹配或超越专门的领域特定方法，为可扩展的3D标注提供实用工具。&lt;h4&gt;翻译&lt;/h4&gt;交互式3D点云分割通过用户引导提示能够高效标注复杂3D场景。然而，当前方法通常在范围上局限于单一领域（室内或室外），并且局限于单一形式的用户交互（空间点击或文本提示）。此外，在多个数据集上训练通常会导致负迁移，产生缺乏通用性的领域特定工具。为解决这些限制，我们提出了SNAP（Segment Anything in Any Point cloud），这是一个统一的交互式3D分割模型，支持跨不同领域的基于点和基于文本的提示。我们的方法通过在涵盖室内、室外和空中环境的7个数据集上训练实现跨领域通用性，同时采用领域自适应归一化来防止负迁移。对于文本提示的分割，我们无需人工干预自动生成掩码提案，并将其与文本查询的CLIP嵌入进行匹配，实现全景和开放词汇分割。大量实验证明，SNAP始终提供高质量的分割结果。我们在空间提示分割的9个零样本基准测试中的8个上取得了最先进的性能，并在所有5个文本提示基准测试上展示了具有竞争力的结果。这些结果表明，统一模型可以匹配或超越专门的领域特定方法，为可扩展的3D标注提供实用工具。项目页面位于https://neu-vi.github.io/SNAP/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决当前3D点云交互分割方法局限于单一领域（室内或室外）和单一交互形式（空间点击或文本提示）的问题，以及多数据集训练导致的负迁移问题。这个问题很重要，因为3D场景标注需要大量人工努力，而缺乏通用性的工具限制了它们作为高效标注工具的采用，领域特定工具需要单独训练增加了使用成本，缺乏灵活性也限制了用户适应不同的标注需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到2D图像中SAM成功的启发，认识到当前方法缺乏泛化能力和交互灵活性，因此决定创建一个统一的跨领域模型。他们设计时借鉴了现有工作：使用SAM的掩码解码器设计，借鉴AGILE3D和Interactive4D的点击采样策略，利用CLIP模型处理文本提示和嵌入匹配。核心创新是提出领域自适应归一化来解决跨领域训练的负迁移问题，并设计自动提示点生成算法实现无需人工干预的分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的模型，支持在不同领域（室内、室外、空中）的3D点云上进行多模态（空间和文本）交互式分割。整体流程包括：1)点云编码：使用Point Transformer V3提取点嵌入并应用领域自适应归一化；2)空间提示分割：编码点击点，通过掩码解码器生成掩码；3)文本提示分割：自动生成提示点，生成掩码提案并匹配CLIP嵌入；4)训练：结合多种损失函数优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)领域自适应归一化解决跨领域训练负迁移；2)统一的多领域模型支持室内、室外和空中场景；3)多模态提示同时支持空间点击和文本描述；4)自动提示点生成算法实现无需人工干预的分割；5)开放词汇分割支持新类别。相比之前工作，SNAP突破了单一领域限制，支持多种提示方式，解决了负迁移问题，可直接处理点云无需RGB图像，并在多个零样本测试中达到最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SNAP是一个统一的、支持多模态提示的3D点云交互分割模型，通过领域自适应归一化实现了跨领域泛化能力，并在多种场景的分割任务中达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D point cloud segmentation enables efficient annotation ofcomplex 3D scenes through user-guided prompts. However, current approaches aretypically restricted in scope to a single domain (indoor or outdoor), and to asingle form of user interaction (either spatial clicks or textual prompts).Moreover, training on multiple datasets often leads to negative transfer,resulting in domain-specific tools that lack generalizability. To address theselimitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in\textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3Dsegmentation that supports both point-based and text-based prompts acrossdiverse domains. Our approach achieves cross-domain generalizability bytraining on 7 datasets spanning indoor, outdoor, and aerial environments, whileemploying domain-adaptive normalization to prevent negative transfer. Fortext-prompted segmentation, we automatically generate mask proposals withouthuman intervention and match them against CLIP embeddings of textual queries,enabling both panoptic and open-vocabulary segmentation. Extensive experimentsdemonstrate that SNAP consistently delivers high-quality segmentation results.We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks forspatial-prompted segmentation and demonstrate competitive results on all 5text-prompted benchmarks. These results show that a unified model can match orexceed specialized domain-specific approaches, providing a practical tool forscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/</description>
      <author>example@mail.com (Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang)</author>
      <guid isPermaLink="false">2510.11565v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model</title>
      <link>http://arxiv.org/abs/2510.11509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 Datasets and Benchmarks Track. Dataset and  Code: https://github.com/RuipingL/Situat3DChange&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Situat3DChange数据集和SCReasoner方法，用于解决3D动态场景和情境理解的不完整问题，通过大规模数据集和高效3D MLLM方法提升动态环境变化的理解能力。&lt;h4&gt;背景&lt;/h4&gt;物理环境和情况本质上是动态的，但当前3D数据集和评估基准往往只专注于动态场景或动态情况的孤立研究，导致对动态环境的理解不完整。&lt;h4&gt;目的&lt;/h4&gt;克服现有3D数据集的局限性，引入支持情境感知变化理解任务的大规模数据集，并开发高效方法进行点云比较和动态场景理解。&lt;h4&gt;方法&lt;/h4&gt;构建Situat3DChange数据集，包含121K问答对、36K变化描述和17K重排指令；利用11K人类环境变化观察建立共享心智模型；融合自我中心和他者中心视角及空间关系；提出SCReasoner方法进行高效点云比较。&lt;h4&gt;主要发现&lt;/h4&gt;Situat3DChange任务上的评估突显了MLLMs在动态场景和情境理解方面的进展和局限性；数据扩展和跨域迁移实验证明了使用Situat3DChange作为训练数据集的任务无关有效性。&lt;h4&gt;结论&lt;/h4&gt;Situat3DChange数据集和SCReasoner方法为动态场景和情境理解提供了新的工具和视角，有助于提升AI对环境动态变化的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;物理环境和情况本质上是动态的，然而当前的3D数据集和评估基准往往只专注于动态场景或动态情况的孤立研究，导致理解不完整。为克服这些限制，我们引入Situat3DChange，一个支持三种情境感知变化理解任务的大规模数据集：121K问答对，36K用于感知任务的变化描述，以及17K用于行动任务的重排指令。为构建这一大规模数据集，Situat3DChange利用11K人类对环境变化的观察来建立人类-AI协作的共享心智模型和共享情境感知。这些观察融合了自我中心和他者中心视角以及分类和坐标空间关系，通过LLM集成以支持对情境变化的理解。为解决比较同一场景中具有微小变化的点云对这一挑战，我们提出SCReasoner，一种高效的3D MLLM方法，能够以最小的参数开销进行有效的点云比较，且语言解码器不需要额外令牌。在Situat3DChange任务上的全面评估突显了MLLMs在动态场景和情境理解方面的进展和局限性。关于数据扩展和跨域迁移的额外实验证明了使用Situat3DChange作为MLLMs训练数据集的任务无关有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前3D数据集和评估基准孤立关注动态场景或动态情境的问题，导致对环境变化的理解不完整。这个问题很重要，因为物理环境和情境本质上是动态的，即使是微小的位置变化对视障人士也可能造成障碍，有效的人机协作需要建立共享的心理模型和情境感知能力，而现有方法无法同时捕捉动态场景和情境感知。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析人类与机器人认知差异发现人类以柱面坐标感知环境而机器人以笛卡尔坐标感知，这导致无法共享心理地图。作者收集11K人类注释建立基于人类感知的共享模型，并整合自我中心和异中心视角以及分类和坐标空间关系。作者借鉴了3RScan数据集、3DSSG场景图、MSQA的情境采样方法、LEO框架，并使用GPT-4生成类人文本，同时采用Mamba的选择性状态空间模型和星操作进行token选择与融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建情境感知的3D变化理解数据集并开发高效的SCReasoner架构，通过结合人类注释和LLM生成保留人类感知框架。数据集构建流程包括情境采样、长文本生成、查询生成、问答生成和数据质量控制。SCReasoner实现流程使用共同编码器将两个点云嵌入token，从前一场景选择信息丰富token并与当前场景token融合，使用Mamba进行token选择，星操作进行token融合，构建在LEO框架上仅添加少量额外参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Situat3DChange数据集首个整合动态场景和情境感知，包含121K问答对、36K变化描述和17K重新排列指令；2) SCReasoner架构专门处理成对点云，使用Mamba和星操作实现高效比较；3) 基于人类感知框架整合自我中心和异中心视角。相比之前工作，该数据集同时关注动态场景和情境感知，SCReasoner专门设计用于点云比较并关注差异而非冗余，评估方法更全面且包含特殊距离评估指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了首个情境感知的3D变化理解数据集和高效的SCReasoner架构，通过整合人类感知与AI系统，实现了对动态环境和情境变化的全面理解，为人机协作在动态环境中的适应性交互提供了新的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physical environments and circumstances are fundamentally dynamic, yetcurrent 3D datasets and evaluation benchmarks tend to concentrate on eitherdynamic scenarios or dynamic situations in isolation, resulting in incompletecomprehension. To overcome these constraints, we introduce Situat3DChange, anextensive dataset supporting three situation-aware change understanding tasksfollowing the perception-action model: 121K question-answer pairs, 36K changedescriptions for perception tasks, and 17K rearrangement instructions for theaction task. To construct this large-scale dataset, Situat3DChange leverages11K human observations of environmental changes to establish shared mentalmodels and shared situational awareness for human-AI collaboration. Theseobservations, enriched with egocentric and allocentric perspectives as well ascategorical and coordinate spatial relations, are integrated using an LLM tosupport understanding of situated changes. To address the challenge ofcomparing pairs of point clouds from the same scene with minor changes, wepropose SCReasoner, an efficient 3D MLLM approach that enables effective pointcloud comparison with minimal parameter overhead and no additional tokensrequired for the language decoder. Comprehensive evaluation on Situat3DChangetasks highlights both the progress and limitations of MLLMs in dynamic sceneand situation understanding. Additional experiments on data scaling andcross-domain transfer demonstrate the task-agnostic effectiveness of usingSituat3DChange as a training dataset for MLLMs.</description>
      <author>example@mail.com (Ruiping Liu, Junwei Zheng, Yufan Chen, Zirui Wang, Kunyu Peng, Kailun Yang, Jiaming Zhang, Marc Pollefeys, Rainer Stiefelhagen)</author>
      <guid isPermaLink="false">2510.11509v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces</title>
      <link>http://arxiv.org/abs/2510.11014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于采样的管道，利用大规模预训练生成模型在零样本方式下产生概率先验，捕捉环境不确定性和空间-语义关系，用于部分可观察性条件下的机器人规划。&lt;h4&gt;背景&lt;/h4&gt;在部分可观察性条件下进行规划时，先验信息非常重要，但在实践中难以获取。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用生成模型产生概率先验的方法，以零样本方式捕捉环境不确定性和空间-语义关系，并直接用于配置空间规划。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于采样的管道，基于部分观察条件恢复完整的RGB-D点云样本，包含占用信息和目标语义；建立Matterport3D基准测试，场景为只能通过门看到部分区域的房间，机器人需要导航到未观察到的目标物体。&lt;h4&gt;主要发现&lt;/h4&gt;有效先验必须表示未观察区域中的占用和目标位置不确定性；方法恢复了与真实情况一致的常识空间语义，生成了多样化的、干净的3D点云，可用于运动规划。&lt;h4&gt;结论&lt;/h4&gt;生成模型作为机器人规划中先验信息的丰富来源具有很大潜力。&lt;h4&gt;翻译&lt;/h4&gt;先验信息对于部分可观察性条件下的规划至关重要，但在实践中难以获取。我们提出了一种基于采样的管道，利用大规模预训练生成模型以零样本方式产生概率先验，捕捉环境不确定性和空间-语义关系。基于部分观察条件，该管道能够恢复包含占用信息和目标语义的完整RGB-D点云样本，可直接用于配置空间规划。我们建立了一个Matterport3D基准测试，场景为只能通过门看到部分区域的房间，机器人必须导航到未观察到的目标物体。此场景的有效先验必须表示未观察区域中的占用和目标位置不确定性。实验表明，我们的方法恢复了与真实情况一致的常识空间语义，生成了多样化的、干净的3D点云，可用于运动规划，突显了生成模型作为机器人规划中丰富先验来源的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人如何在部分可观测环境中（如只能通过门缝看到房间一部分）获取环境不确定性先验信息的问题。这个问题很重要，因为随着机器人应用扩展到真实世界环境，环境不确定性不可避免，而传统的规划方法依赖于难以获取且可能不准确的手工或预编程先验信息。准确的环境不确定性先验对机器人在未知区域进行有效导航和规划至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出在部分可观测环境下的规划需要先验信息但难以获取，然后注意到生成式视觉模型能生成符合数据分布的内容并可根据条件输入。他们设计了一个基于采样的流程，借鉴了多项现有工作：VLM用于图像分类和物体提示生成、FLUX图像外推模型用于场景扩展、单目深度估计器用于3D重建、以及现有的采样规划算法。作者将这些现有技术整合成一个完整的流水线，用于生成条件于部分观测的完整环境样本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模预训练的生成模型作为环境采样器，根据部分观测生成捕捉环境不确定性和空间-语义关系的3D环境样本，然后使用这些样本作为先验信息进行配置空间规划。整体流程包括：1) VLM提示机制对输入图像分类并生成相关物体提示；2) 基于图像的生成使用FLUX模型扩展场景图像；3) 物体分割和地板估计进行语义分割和对齐；4) 深度估计和对齐将RGB-D转换为点云；5) 配置空间规划使用采样先验进行运动规划。整个流程约需10.5秒生成一个样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 利用预训练生成模型采样环境不确定性先验的流程；2) 零样本方法直接生成有用先验；3) 空间-语义先验的形式化方法；4) 基于Matterport3D的新数据集；5) 通过模拟运动规划验证实用性。相比之前工作，本文与传统场景完成方法不同，后者追求单一一致重建，而本文捕捉多样性同时确保干净几何；区别于现有生成模型应用，后者专注于目标分布或策略优化，而本文构建环境先验；不同于显式环境先验，后者是空间-语义地图或场景完成模型，而本文采样扩展视野上的分布；也区别于高计算需求的3D生成方法，本文使用更高效的2D生成模型结合单目深度估计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种利用预训练生成模型从部分观测采样环境不确定性先验的创新方法，使机器人能够在未观测区域进行有效的空间-语义推理和规划。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Priors are vital for planning under partial observability, yet difficult toobtain in practice. We present a sampling-based pipeline that leverageslarge-scale pretrained generative models to produce probabilistic priorscapturing environmental uncertainty and spatio-semantic relationships in azero-shot manner. Conditioned on partial observations, the pipeline recoverscomplete RGB-D point cloud samples with occupancy and target semantics,formulated to be directly useful in configuration-space planning. We establisha Matterport3D benchmark of rooms partially visible through doorways, where arobot must navigate to an unobserved target object. Effective priors for thissetting must represent both occupancy and target-location uncertainty inunobserved regions. Experiments show that our approach recovers commonsensespatial semantics consistent with ground truth, yielding diverse, clean 3Dpoint clouds usable in motion planning, highlight the promise of generativemodels as a rich source of priors for robotic planning.</description>
      <author>example@mail.com (Subhransu S. Bhattacharjee, Hao Lu, Dylan Campbell, Rahul Shome)</author>
      <guid isPermaLink="false">2510.11014v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>rareboost3d: a synthetic lidar dataset with enhanced rare classes</title>
      <link>http://arxiv.org/abs/2510.10876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过引入合成数据集和跨域语义对齐方法解决了点云数据中的长尾问题，提高了激光雷达感知技术的性能。&lt;h4&gt;背景&lt;/h4&gt;真实世界点云数据集对基于激光雷达的感知技术（如自动驾驶中的物体分割）有重要贡献，但某些罕见类别的实例数量有限导致长尾问题仍然存在。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集中罕见类别实例不足的问题，通过合成数据补充真实世界数据。&lt;h4&gt;方法&lt;/h4&gt;提出了名为RareBoost3D的合成点云数据集，以及名为CS Loss的跨域语义对齐方法，用于对齐不同域中相同类别的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，跨域语义对齐方法显著提高了激光雷达点云分割模型在真实世界数据上的性能。&lt;h4&gt;结论&lt;/h4&gt;结合合成数据和真实世界数据，并应用跨域语义对齐方法，可以有效解决长尾问题，提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;真实世界的点云数据集对基于激光雷达的感知技术的发展做出了重大贡献，例如自动驾驶中的物体分割。然而，由于某些罕见类别中的实例数量有限，长尾问题仍然是现有数据集的主要挑战。为了解决这个问题，我们引入了一个名为RareBoost3D的新型合成点云数据集，通过为真实世界数据集中稀少的物体类别提供更多实例来补充现有的真实世界数据集。为了有效利用合成和真实世界数据，我们进一步提出了一个名为CS Loss的跨域语义对齐方法，用于对齐不同域中相同类别的特征表示。实验结果表明，这种对齐方法显著提高了激光雷达点云分割模型在真实世界数据上的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是LiDAR点云数据集中的'长尾问题'，即常见类别（如汽车）的实例数量远多于稀有类别（如行人、自行车等）。这个问题在自动驾驶领域非常重要，因为模型需要准确识别各种类别的对象，而稀有类别样本不足会导致对这些关键对象的识别能力下降，可能影响自动驾驶系统的安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到真实世界点云数据集的局限性：获取标注成本高、耗时长且存在类别不平衡。他们借鉴了多数据集联合训练和合成到真实领域迁移学习的思路，选择使用CARLA模拟器生成合成数据。特别的是，他们没有简单复制真实数据的分布，而是主动增加稀有类别的实例数量。为了解决合成与真实数据间的域差距，他们基于PointDR的跨域特征对齐方法，采用对比学习技术设计了CSC损失函数，这借鉴了对比学习在特征对齐方面的成功应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个专门增强稀有类别的大规模合成LiDAR数据集，并通过对比学习对齐合成与真实数据中相同类别的特征表示。实现流程包括：1) 使用CARLA模拟器生成8个不同地图的LiDAR序列，特别增加稀有类别的实例；2) 为真实和合成数据分别构建类别特征原型并存储在记忆库中；3) 使用对比学习使相同类别的特征在共享语义空间中接近，不同类别相互分离；4) 结合语义分割损失和对比损失进行模型训练，减少域差距的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了RareBoost3D数据集，不仅规模大，还特别增加了稀有类别的实例数量；2) 设计了跨域语义一致性(CSC)损失函数，通过对比学习而非复杂的对抗训练来实现域对齐；3) 实验证明调整稀有类别在合成数据中的比例可以显著提升这些类别的分割性能。相比之前的工作，不同之处在于：与SynthmanticLiDAR不同，RareBoost3D不复制而是主动改变类别分布；与SynLiDAR和ePointDA不同，CSC损失不需要对抗训练；传统数据增强方法主要在几何层面操作，而RareBoost3D引入了全新样本。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RareBoost3D数据集和跨域语义一致性损失函数通过增加稀有类别的样本数量并有效对齐合成与真实数据的特征表示，显著提升了LiDAR点云分割模型在稀有类别上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world point cloud datasets have made significant contributions to thedevelopment of LiDAR-based perception technologies, such as object segmentationfor autonomous driving. However, due to the limited number of instances in somerare classes, the long-tail problem remains a major challenge in existingdatasets. To address this issue, we introduce a novel, synthetic point clouddataset named RareBoost3D, which complements existing real-world datasets byproviding significantly more instances for object classes that are rare inreal-world datasets. To effectively leverage both synthetic and real-worlddata, we further propose a cross-domain semantic alignment method named CSCloss that aligns feature representations of the same class across differentdomains. Experimental results demonstrate that this alignment significantlyenhances the performance of LiDAR point cloud segmentation models overreal-world data.</description>
      <author>example@mail.com (Shutong Lin, Zhengkang Xiang, Jianzhong Qi, Kourosh Khoshelham)</author>
      <guid isPermaLink="false">2510.10876v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MATStruct: High-Quality Medial Mesh Computation via Structure-aware Variational Optimization</title>
      <link>http://arxiv.org/abs/2510.10751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的优化框架，用于计算中轴变换，同时保留中轴结构并确保高质量的中轴网格。该方法基于结构感知的粒子优化流程，由限制性幂图引导，通过球形二次误差度量和高斯核能量来约束和优化中轴球的分布。相比现有方法，该技术产生更清洁、准确的中轴结构，具有更好的几何保真度、拓扑正确性和明确的结构分解。&lt;h4&gt;背景&lt;/h4&gt;中轴结构由相互连接的薄片、接缝和连接点组成，为3D形状提供自然的体积分解。现有的中轴变换计算方法存在一些局限性，如特征保持方法(MATFP和MATTopo)产生的中轴结构不够清洁和准确，而基于体素、点云和变分的方法未能将结构感知集成到优化过程中。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的优化框架，能够同时保留中轴结构并确保高质量的中轴网格，克服现有方法的局限性，产生具有更好几何保真度、拓扑正确性和明确结构分解的中轴网格。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于结构感知的粒子优化流程，由限制性幂图(RPD)引导，将输入体积划分为凸单元，其对偶编码了中轴网格的连通性。通过球形二次误差度量(SQEM)投影强制执行结构感知，约束中轴球的移动，同时使用高斯核能量鼓励均匀的空间分布。&lt;h4&gt;主要发现&lt;/h4&gt;1. 与特征保持方法(MATFP和MATTopo)相比，新方法产生更清洁、更准确的中轴结构，网格质量显著提高；2. 与基于体素、点云和变分的方法相比，该框架首次将结构感知集成到优化过程中；3. 产生的中轴网格具有优越的几何保真度、拓扑正确性和明确的结构分解。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发了一种新的优化框架，能够有效计算中轴变换并生成高质量的中轴网格。该方法通过结构感知的粒子优化流程和限制性幂图引导，克服了现有方法的局限性，为中轴变换计算提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于计算中轴变换的新型优化框架，该框架同时保留中轴结构并确保高质量的中轴网格。中轴结构由相互连接的薄片、接缝和连接点组成，为3D形状提供自然的体积分解。我们的方法引入了一种基于结构感知的粒子优化流程，由限制性幂图(RPD)引导，该图将输入体积划分为凸单元，其对偶编码了中轴网格的连通性。通过球形二次误差度量(SQEM)投影强制执行结构感知，约束中轴球的移动，同时高斯核能量鼓励均匀的空间分布。与特征保持方法如MATFP和MATTopo相比，我们的方法产生更清洁、更准确的中轴结构，网格质量显著提高。与基于体素、点云和变分的方法相比，我们的框架是第一个将结构感知集成到优化过程中的方法，产生具有优越几何保真度、拓扑正确性和明确结构分解的中轴网格。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决计算高质量中轴网格（medial mesh）时难以同时保持中轴结构（medial structure）和网格质量的问题。中轴变换作为形状分析的基础，能捕捉形状的拓扑和几何特性，在形状分析、识别、匹配等下游应用中至关重要。特别是在CAD模型中，清晰的中轴结构对工程设计、制造和模拟有重要价值，而现有方法无法同时保证结构清晰度和几何质量，限制了中轴变换的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法如MATFP和MATTopo的局限性，特别是在球体分类和球体过度拥挤方面的问题。作者发现基于表面的RPD分类方法在球体偏离中轴时会导致误分类，因此提出使用体积RPD而非表面RPD来解决分类问题。作者借鉴了粒子优化方法来促进球体均匀分布，并设计了球形二次误差度量（SQEM）来约束球体移动，确保结构感知的优化。同时借鉴了MATTopo的拓扑保持策略和球形收缩算法用于投影回中轴，但进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构感知的变分优化同时保持中轴结构和确保高质量的中轴网格，使用体积RPD进行更准确的球体分类，并利用基于粒子的优化方案结合SQEM约束确保球体沿着正确的子结构移动。整体流程包括：1)初始化：使用球形收缩算法在形状表面均匀采样初始球体；2)迭代优化：计算RPD、采样RPC、计算球体间作用力和能量、梯度投影约束移动、L-BFGS优化；3)投影步骤：将优化后的球体投影回中轴；4)重复优化直到结构收敛；5)计算中轴网格作为RPD对偶；6)后处理修剪无效连接。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出全面的RPD-based优化框架生成高质量结构感知的3D中轴网格；2)提出基于体积RPD的鲁棒球体分类策略提高分类准确性；3)引入中轴结构误差率（MSER）作为新的定量评估指标。相比之前工作，不同之处在于：优化了插入球体的位置而非固定它们；使用体积RPD而非表面RPD进行分类；通过结构感知梯度投影确保球体沿正确子结构移动；在优化过程中促进球体均匀分布减少过度拥挤；引入MSER指标评估中轴结构准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATStruct通过结合结构感知的变分优化和基于体积RPD的球体分类，首次实现了高质量中轴网格的生成，同时保持了中轴结构的完整性和几何保真度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3757377.3763840&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel optimization framework for computing the medial axistransform that simultaneously preserves the medial structure and ensures highmedial mesh quality. The medial structure, consisting of interconnected sheets,seams, and junctions, provides a natural volumetric decomposition of a 3Dshape. Our method introduces a structure-aware, particle-based optimizationpipeline guided by the restricted power diagram (RPD), which partitions theinput volume into convex cells whose dual encodes the connectivity of themedial mesh. Structure-awareness is enforced through a spherical quadraticerror metric (SQEM) projection that constrains the movement of medial spheres,while a Gaussian kernel energy encourages an even spatial distribution.Compared to feature-preserving methods such as MATFP and MATTopo, our approachproduces cleaner and more accurate medial structures with significantlyimproved mesh quality. In contrast to voxel-based, point-cloud-based, andvariational methods, our framework is the first to integrate structuralawareness into the optimization process, yielding medial meshes with superiorgeometric fidelity, topological correctness, and explicit structuraldecomposition.</description>
      <author>example@mail.com (Ningna Wang, Rui Xu, Yibo Yin, Zichun Zhong, Taku Komura, Wenping Wang, Xiaohu Guo)</author>
      <guid isPermaLink="false">2510.10751v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>WorldMirror: Universal 3D World Reconstruction with Any-Prior Prompting</title>
      <link>http://arxiv.org/abs/2510.10726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page, code, and models will be publicly available soon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了WorldMirror，一个用于多样化3D几何预测任务的一体化前馈模型，能够整合多种几何先验信息并同时生成多种3D表示，在各种任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的3D几何预测方法要么仅限于图像输入，要么针对特定任务定制，缺乏灵活性和通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够灵活整合多种几何先验信息，同时生成多种3D表示的统一框架，解决结构歧义问题，并实现高效的3D几何预测。&lt;h4&gt;方法&lt;/h4&gt;提出WorldMirror模型，一种前馈架构，能够整合相机位姿、内参和深度图等几何先验信息，同时生成密集点云、多视角深度图、相机参数、表面法向量和3D高斯等多种3D表示。&lt;h4&gt;主要发现&lt;/h4&gt;WorldMirror在各种基准测试中实现了最先进的性能，包括相机估计、点图估计、深度估计、表面法向量估计和新视角合成，同时保持了前向推理的效率。&lt;h4&gt;结论&lt;/h4&gt;WorldMirror提供了一个优雅且统一的解决方案，能够利用可用的先验信息解决结构歧义，并在单次前向传播中生成几何一致的3D输出，为多样化3D几何预测任务提供了高效工具。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了WorldMirror，这是一个用于多样化3D几何预测任务的一体化前馈模型。与仅限于图像输入或针对特定任务定制的现有方法不同，我们的框架灵活整合了多种几何先验信息，包括相机位姿、内参和深度图，同时生成多种3D表示：密集点云、多视角深度图、相机参数、表面法向量和3D高斯。这种优雅且统一的架构利用可用的先验信息解决结构歧义，并在单次前向传播中生成几何一致的3D输出。WorldMirror在从相机、点图、深度和表面法向量估计到新视角合成的各种基准测试中实现了最先进的性能，同时保持了前向推理的效率。代码和模型即将公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有3D重建方法的两个局限性：一是输入限制，大多数方法只能处理原始图像，无法利用校准内参、相机位姿和深度测量等有用的先验信息；二是输出限制，方法通常只针对单一任务优化，很少在统一框架内整合多个任务。这些问题在现实中很重要，因为先验信息可以解决尺度模糊、确保多视图一致性，并在图像线索不足区域提供基础；统一框架能更高效处理各种3D重建任务，确保不同输出间的几何一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法在输入和输出空间都存在局限性，提出关键问题：能否通过有效利用多样化先验知识，在通用3D重建架构中解决这些挑战？他们设计了多模态先验提示机制和通用几何预测架构，借鉴了DUSt3R、VGGT等前馈3D重建模型的思想，以及3D高斯溅射在新视图合成中的应用，同时参考了传统优化方法利用已知相机参数提高重建质量的思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的前馈模型，能灵活整合多种几何先验信息，同时生成多种3D表示（点云、深度图、相机参数、表面法线、3D高斯），利用先验解决结构歧义，提供几何一致的3D输出。整体流程：1）多模态先验提示 - 将相机位姿、内参、深度图转换为令牌并整合；2）通用几何预测 - 使用DPT头和Transformer层预测各种几何属性；3）动态训练策略 - 随机采样不同先验组合，采用课程学习从简单到复杂优化训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）多模态先验提示机制，首次系统探索多模态几何先验注入；2）通用几何预测架构，支持全方位3D重建任务；3）动态先验注入方案，适应任意先验组合；4）系统课程学习策略，优化训练效率。相比之前工作：输入上能灵活利用多种先验而非仅图像；输出上能同时处理多种任务而非单一优化；性能上在多个基准测试实现最先进结果；同时保持前向推理效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; WorldMirror是一个统一的前馈3D重建模型，通过灵活整合多种几何先验信息，在单次前向传播中同时生成多种高质量3D表示，实现了在各种3D重建任务上的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present WorldMirror, an all-in-one, feed-forward model for versatile 3Dgeometric prediction tasks. Unlike existing methods constrained to image-onlyinputs or customized for a specific task, our framework flexibly integratesdiverse geometric priors, including camera poses, intrinsics, and depth maps,while simultaneously generating multiple 3D representations: dense pointclouds, multi-view depth maps, camera parameters, surface normals, and 3DGaussians. This elegant and unified architecture leverages available priorinformation to resolve structural ambiguities and delivers geometricallyconsistent 3D outputs in a single forward pass. WorldMirror achievesstate-of-the-art performance across diverse benchmarks from camera, point map,depth, and surface normal estimation to novel view synthesis, while maintainingthe efficiency of feed-forward inference. Code and models will be publiclyavailable soon.</description>
      <author>example@mail.com (Yifan Liu, Zhiyuan Min, Zhenwei Wang, Junta Wu, Tengfei Wang, Yixuan Yuan, Yawei Luo, Chunchao Guo)</author>
      <guid isPermaLink="false">2510.10726v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>dN/dx Reconstruction with Deep Learning for High-Granularity TPCs</title>
      <link>http://arxiv.org/abs/2510.10628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为GraphPT的深度学习模型，用于粒子物理实验中的dN/dx重建，该模型在粒子识别性能上超过了传统方法，特别是在K/π分离方面有显著提升。&lt;h4&gt;背景&lt;/h4&gt;粒子识别对未来的粒子物理实验（如圆形正负电子对撞机和未来圆形对撞机）至关重要。高granularity时间投影室能提供精确跟踪和dN/dx测量用于粒子识别，但准确重建仍是一大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度学习模型（GraphPT）来解决dN/dx重建的挑战，提高粒子识别性能，特别是改善K/π粒子的分离能力。&lt;h4&gt;方法&lt;/h4&gt;将TPC数据表示为点云，采用基于图神经网络的U-Net架构作为网络主干，并融入针对点云处理优化的注意力机制用于节点聚合。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的GraphPT模型在粒子识别性能上超过了传统的截断均值方法。在动量区间5到20 GeV/c时，K/π分离能力提高了约10%到20%。&lt;h4&gt;结论&lt;/h4&gt;GraphPT模型是一种有效的dN/dx重建方法，能够显著提高粒子物理实验中的粒子识别性能，特别是对于动量在5到20 GeV/c范围内的K/π粒子分离。&lt;h4&gt;翻译&lt;/h4&gt;粒子识别对未来的粒子物理实验（如圆形正负电子对撞机和未来圆形对撞机）至关重要。高granularity时间投影室不仅提供精确的跟踪，还能实现dN/dx测量用于粒子识别。dN/dx方法估计初级电离电子的数量，显著提高了粒子识别性能。然而，准确的重建对于这种方法仍然是一个主要挑战。在本文中，我们介绍了一种深度学习模型——图点变换器，用于dN/dx重建。在我们的方法中，TPC数据被表示为点云。网络主干采用基于图神经网络的U-Net架构，并融入了针对点云处理优化的注意力机制用于节点聚合。所提出的GraphPT模型在粒子识别性能上超过了传统的截断均值方法。特别是在动量区间从5到20 GeV/c时，K/π分离能力提高了约10%到20%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高粒度时间投影室（TPC）中dN/dx重建的挑战。dN/dx指单位长度轨迹上的初级电离电子数，用于粒子识别（PID）。这个问题很重要，因为准确的PID对下一代粒子物理实验（如CEPC和FCC）至关重要，特别是高动量下的强子识别。dN/dx方法相比传统dE/dx能显著提高PID性能，因为它直接测量初级电离簇数量，抑制了次级电离和能量波动的干扰，但高粒度TPC中的电子漂移和扩散使得准确重建变得困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到传统规则方法难以应对高粒度TPC中的dN/dx重建挑战，而深度学习可以提取数据中的复杂特征。他们将TPC数据表示为点云，设计了基于图神经网络的U-Net架构，并引入针对点云处理优化的注意力机制。该方法借鉴了多个现有工作：自注意力机制和Transformer架构、PointNet和PointNet++的点云处理方法、PointTransformer的自适应自注意力机制，以及之前在dN/dx重建中使用LSTM和DGCNN的研究。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将TPC数据表示为点云，使用基于图神经网络的U-Net架构处理这些点云，并通过注意力机制优化节点聚合。整体流程包括：1) 将TPC轨迹表示为点云，每个点包含电荷和定时信息；2) 构建k近邻图；3) 使用U-Net编码器-解码器结构处理图数据；4) 在每个层使用Transformer层进行节点间信息聚合；5) 使用端到端训练优化模型；6) 根据输出概率进行dN/dx重建，计算轨迹上被分类为正的命中数除以轨迹长度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出GraphPT模型专门用于高粒度TPC的dN/dx重建；2) 将TPC数据表示为点云并用图神经网络处理；3) 引入针对点云优化的注意力机制；4) 研究减法和点积两种注意力操作符，发现点积结合多头机制效果更好；5) 将之前的两步处理统一为单一模型。相比传统截断均值法，它不依赖人工规则，能处理复杂数据模式，K/π分离能力提高10-20%；相比之前基于神经网络的dN/dx工作，它专门针对高粒度TPC的三维点云数据，使用更先进的图神经网络和Transformer架构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于图神经网络和Transformer的GraphPT深度学习方法，显著提高了高粒度TPC中dN/dx重建的粒子识别性能，相比传统方法实现了10-20%的K/π分离能力提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Particle identification (PID) is essential for future particle physicsexperiments such as the Circular Electron-Positron Collider and the FutureCircular Collider. A high-granularity Time Projection Chamber (TPC) not onlyprovides precise tracking but also enables dN/dx measurements for PID. ThedN/dx method estimates the number of primary ionization electrons, offeringsignificant improvements in PID performance. However, accurate reconstructionremains a major challenge for this approach. In this paper, we introduce a deeplearning model, the Graph Point Transformer (GraphPT), for dN/dxreconstruction. In our approach, TPC data are represented as point clouds. Thenetwork backbone adopts a U-Net architecture built upon graph neural networks,incorporating an attention mechanism for node aggregation specificallyoptimized for point cloud processing. The proposed GraphPT model surpasses thetraditional truncated mean method in PID performance. In particular, the$K/\pi$ separation power improves by approximately 10% to 20% in the momentuminterval from 5 to 20 GeV/c.</description>
      <author>example@mail.com (Guang Zhao, Yue Chang, Jinxian Zhang, Linghui Wu, Huirong Qi, Xin She, Mingyi Dong, Shengsen Sun, Jianchun Wang, Yifang Wang, Chunxu Yu)</author>
      <guid isPermaLink="false">2510.10628v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams</title>
      <link>http://arxiv.org/abs/2510.10602v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpikeGrasp的神经启发的机器人抓取框架，它模仿生物视觉运动通路，直接从立体尖峰摄像机的原始异步事件推断抓取姿态，无需重建点云，在杂乱和无纹理场景中表现优于传统方法，且数据效率高。&lt;h4&gt;背景&lt;/h4&gt;大多数机器人抓取系统依赖于将传感器数据转换为显式的3D点云，这是生物智能中不存在的计算步骤，表明现有方法与生物智能有根本差异。&lt;h4&gt;目的&lt;/h4&gt;探索一种 fundamentally different、神经启发的6-DoF抓取检测范式，模仿生物视觉运动通路，实现更高效、更自然的机器人抓取。&lt;h4&gt;方法&lt;/h4&gt;引入SpikeGrasp框架，处理来自立体尖峰摄像机的原始异步事件，融合这些立体尖峰流，使用循环尖峰神经网络迭代改进抓取假设，构建大规模合成基准数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;SpikeGrasp超越了基于点云的传统基线方法，特别在杂乱和无纹理场景中表现更好，展示了卓越的数据效率。&lt;h4&gt;结论&lt;/h4&gt;通过建立这种端到端的神经启发方法的可行性，SpikeGrasp为未来能够实现自然界中流畅高效操作的系统铺平了道路，特别是对于动态物体。&lt;h4&gt;翻译&lt;/h4&gt;大多数机器人抓取系统依赖于将传感器数据转换为显式的3D点云，这是生物智能中不存在的计算步骤。本文探索了一种根本不同的、神经启发的6-DoF抓取检测范式。我们引入了SpikeGrasp框架，它模仿生物视觉运动通路，处理来自立体尖峰摄像机的原始、异步事件（类似于视网膜），直接推断抓取姿态。我们的模型融合这些立体尖峰流，并使用循环尖峰神经网络（类似于高级视觉处理）来迭代改进抓取假设，而无需重建点云。为验证这一方法，我们构建了一个大规模合成基准数据集。实验表明，SpikeGrasp超越了传统的基于点云的基线方法，特别是在杂乱和无纹理场景中，并表现出卓越的数据效率。通过建立这种端到端的神经启发方法的可行性，SpikeGrasp为未来能够实现自然界中流畅高效操作的系统铺平了道路，特别是对于动态物体。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从立体尖峰相机的原始异步事件流中直接检测6自由度抓取姿态，而不需要将传感器数据转换为3D点云。这个问题很重要，因为当前大多数机器人抓取系统依赖点云重建，这是一个计算密集且易受噪声影响的步骤，而生物系统并不依赖显式点云表示来抓取物体。直接从原始事件流推断抓取姿态可以减少计算负担，提高系统在杂乱场景中的鲁棒性，更接近生物系统的效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到生物视觉运动系统的启发，将系统分为生物启发的组件：人工视网膜（立体尖峰相机）、视觉通路（左右尖峰流处理和融合）、整合皮层（循环尖峰神经网络）和运动系统。他们借鉴了尖峰相机在图像重建、目标检测等任务中的应用，以及基于点云的抓取检测方法的评估框架和表示方法，但避免了显式的点云处理步骤，直接从原始事件流推断抓取姿态。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模仿生物视觉运动系统，直接从原始异步事件流中推断抓取姿态，而不需要显式的3D点云重建。整体流程分为三部分：1）视觉通路网络：从左右尖峰流提取特征，计算相关性，使用循环尖峰神经网络迭代更新；2）可抓取网络：解码隐藏状态生成物体存在概率和可抓取性概率图；3）抓取检测网络：从隐藏状态和可抓取位置预测完整的6-DoF抓取配置，选择最高分数的抓取作为最终估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）生物启发的端到端框架，直接从原始立体尖峰流检测抓取姿态；2）构建了第一个用于6-DoF抓取姿态检测的大规模合成尖峰流数据集；3）使用循环尖峰神经网络处理时空信息并迭代完善抓取假设；4）表现出强大的数据效率。相比之前工作，SpikeGrasp避免了点云重建的中间步骤，能处理完整6-DoF抓取姿态，在杂乱场景中表现更好，计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpikeGrasp通过引入一种受生物启发的端到端框架，首次实现了从原始立体尖峰流直接检测6-DoF抓取姿态，避免了点云重建的中间步骤，并在杂乱场景中表现出更高的准确性和数据效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most robotic grasping systems rely on converting sensor data into explicit 3Dpoint clouds, which is a computational step not found in biologicalintelligence. This paper explores a fundamentally different, neuro-inspiredparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework thatmimics the biological visuomotor pathway, processing raw, asynchronous eventsfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.Our model fuses these stereo spike streams and uses a recurrent spiking neuralnetwork, analogous to high-level visual processing, to iteratively refine grasphypotheses without ever reconstructing a point cloud. To validate thisapproach, we built a large-scale synthetic benchmark dataset. Experiments showthat SpikeGrasp surpasses traditional point-cloud-based baselines, especiallyin cluttered and textureless scenes, and demonstrates remarkable dataefficiency. By establishing the viability of this end-to-end, neuro-inspiredapproach, SpikeGrasp paves the way for future systems capable of the fluid andefficient manipulation seen in nature, particularly for dynamic objects.</description>
      <author>example@mail.com (Zhuoheng Gao, Jiyao Zhang, Zhiyong Xie, Hao Dong, Zhaofei Yu, Rongmei Chen, Guozhang Chen, Tiejun Huang)</author>
      <guid isPermaLink="false">2510.10602v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>DAGLFNet:Deep Attention-Guided Global-Local Feature Fusion for Pseudo-Image Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2510.10471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAGLFNet的伪图像语义分割框架，用于高效处理非结构化点云并提取结构化语义信息，在保持实时性能的同时实现了高精度。&lt;h4&gt;背景&lt;/h4&gt;环境感知系统在高精度测绘和自主导航中扮演关键角色，LiDAR作为核心传感器提供准确的3D点云数据。然而，如何高效处理非结构化点云并提取结构化语义信息仍是一个重大挑战。现有的基于伪图像的表示方法往往忽略了点云的结构和语义细节，导致特征融合和判别性有限。&lt;h4&gt;目的&lt;/h4&gt;设计一个伪图像语义分割框架，提取判别性特征，平衡处理效率和性能，同时保持点云的结构和语义细节。&lt;h4&gt;方法&lt;/h4&gt;提出DAGLFNet框架，包含三个主要模块：1) 全局-局部特征融合编码模块，增强局部特征相关性并捕获全局上下文信息；2) 多分支特征提取网络，捕获更多邻域信息并增强轮廓特征的判别性；3) 基于深度特征引导的注意力机制的特征融合，提高跨通道特征融合的精度。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估显示，DAGLFNet在SemanticKITTI和nuScenes验证集上分别达到了69.83%和78.65%的性能，平衡了高性能与实时能力，展示了基于LiDAR实时应用的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;DAGLFNet方法通过创新的特征提取和融合机制，有效解决了点云处理中的效率和性能平衡问题，为基于LiDAR的实时应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;环境感知系统在高精度测绘和自主导航中起着关键作用，LiDAR作为提供准确3D点云数据的核心传感器。如何高效处理非结构化点云同时提取结构化语义信息仍是一个重大挑战，近年来，出现了许多基于伪图像的表示方法以在效率和性能之间取得平衡。然而，它们通常忽略了点云的结构和语义细节，导致特征融合和判别性有限。在这项工作中，我们提出了DAGLFNet，一种基于伪图像的语义分割框架，旨在提取判别性特征。首先，使用全局-局部特征融合编码模块来增强集合内局部特征之间的相关性并捕获全局上下文信息。其次，采用多分支特征提取网络来捕获更多邻域信息并增强轮廓特征的判别性。最后，引入基于深度特征引导的注意力机制进行特征融合，以提高跨通道特征融合的精度。实验评估表明，DAGLFNet在SemanticKITTI和nuScenes的验证集上分别达到了69.83%和78.65%。该方法平衡了高性能与实时能力，展示了基于LiDAR实时应用的巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效处理非结构化的LiDAR点云数据并提取结构化语义信息的问题，特别是在基于伪图像的点云分割方法中，如何解决结构扭曲、边界模糊和语义模糊等挑战。这个问题在现实世界中非常重要，因为环境感知系统是高精度地图和自主导航的核心，LiDAR作为关键传感器提供的3D点云数据需要被准确理解和解析，这对自动驾驶、机器人等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前点云分割的三种主要方法（点方法、体素方法和混合策略）及其局限性，然后关注到基于范围图像的方法在计算效率和性能之间的平衡。作者发现现有伪图像方法忽略了点云的结构和语义细节，导致特征融合和判别能力有限。针对这些问题，作者设计了DAGLFNet框架，借鉴了现有点云处理的基本方法、范围图像表示方式以及深度学习中的注意力机制和多分支网络等设计理念，但进行了创新性改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过全局-局部特征融合增强局部特征间的相关性并捕获全局上下文信息，使用多分支特征提取网络捕获更多邻域信息并增强边界特征的判别能力，以及引入深度特征引导的注意力机制提高跨通道特征融合的精度。整体实现流程包括：1)特征编码器将点云分组并提取点级和组级特征；2)图像特征提取器使用多分支架构捕获不同感受野的特征；3)特征更新模块通过深度引导的注意力机制融合点级和组级特征；4)融合头模块聚合多阶段特征并生成最终语义预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了DAGLFNet网络架构，将点云几何特征与二维伪图像表示紧密集成；2)提出了包含三个关键模块的全面特征增强策略：GL-FFE模块捕获长程依赖并稳定局部几何表示，MB-FE网络扩大感受野并加强边界特征表达，FFDFA机制利用距离感知加权提高跨通道特征集成精度。相比之前的工作，DAGLFNet不仅处理投影到图像上的点，还解决了多点映射冲突问题，考虑了遮挡点，保留了完整三维结构，并强调了对子集内特征关系的一致建模同时系统考虑了空间距离的影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAGLFNet通过全局-局部特征融合、多分支特征提取和深度特征引导的注意力机制，有效解决了伪图像点云分割中特征表达不足的问题，在保持实时性能的同时显著提高了语义分割精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Environmental perception systems play a critical role in high-precisionmapping and autonomous navigation, with LiDAR serving as a core sensor thatprovides accurate 3D point cloud data. How to efficiently process unstructuredpoint clouds while extracting structured semantic information remains asignificant challenge, and in recent years, numerous pseudo-image-basedrepresentation methods have emerged to achieve a balance between efficiency andperformance. However, they often overlook the structural and semantic detailsof point clouds, resulting in limited feature fusion and discriminability. Inthis work, we propose DAGLFNet, a pseudo-image-based semantic segmentationframework designed to extract discriminative features. First, the Global-LocalFeature Fusion Encoding module is used to enhance the correlation among localfeatures within a set and capture global contextual information. Second, theMulti-Branch Feature Extraction network is employed to capture moreneighborhood information and enhance the discriminability of contour features.Finally, a Feature Fusion via Deep Feature-guided Attention mechanism isintroduced to improve the precision of cross-channel feature fusion.Experimental evaluations show that DAGLFNet achieves 69.83\% and 78.65\% on thevalidation sets of SemanticKITTI and nuScenes, respectively. The methodbalances high performance with real-time capability, demonstrating greatpotential for LiDAR-based real-time applications.</description>
      <author>example@mail.com (Chuang Chen, Wenyi Ge)</author>
      <guid isPermaLink="false">2510.10471v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Graph Learning with Graph-Tuple</title>
      <link>http://arxiv.org/abs/2510.10341v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to TAG workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多视图图元组框架，解决了传统图神经网络在密集图上的效率问题&lt;h4&gt;背景&lt;/h4&gt;图神经网络通常随图边数扩展，适合稀疏图但在密集图(如点云或分子相互作用)上效率较低&lt;h4&gt;目的&lt;/h4&gt;克服传统稀疏化方法强制选择单一交互尺度并丢弃其他尺度信息的限制&lt;h4&gt;方法&lt;/h4&gt;将图划分为不相交的子图，捕获主要局部相互作用和远程连接；通过受非交换算子理论启发的异构消息传递架构学习多视图表示&lt;h4&gt;主要发现&lt;/h4&gt;多视图图元组模型在分子属性预测和宇宙学参数推断两个应用中均优于单图基线模型&lt;h4&gt;结论&lt;/h4&gt;多视图方法具有强大功能和通用性，能有效处理密集图数据&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通常随图边数扩展，使其适合稀疏图但在密集图(如点云或分子相互作用)上效率较低。常见补救措施是通过相似度阈值或距离修剪稀疏化图，但这强制选择单一交互尺度并丢弃其他尺度的重要信息。为克服这一限制，我们引入了多视图图元组框架。我们的图元组框架将图划分为不相交的子图，捕获主要局部相互作用和较弱的远程连接。然后，我们通过受非交换算子理论启发的异构消息传递架构从图元组学习多视图表示，理论上证明这比单图消息传递模型更具表现力，并能保证更低的oracle风险。我们在两个科学领域实现了我们的框架：特征稀缺的库仑矩阵的分子属性预测和几何点云的宇宙学参数推断。在这两种应用中，我们的多视图图元组模型都表现出比单图基线模型更好的性能，突显了我们多视图方法的强大功能和通用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图神经网络（GNNs）在处理密集图（如点云或分子相互作用）时的效率问题。传统方法需要通过稀疏化图来提高计算效率，但这会强制选择单一交互尺度并丢失其他尺度的重要信息。这个问题在科学应用中尤为重要，因为分子、宇宙学等领域的密集数据包含多种尺度的交互信息，单一尺度方法无法同时捕捉局部强相互作用和全局弱相互作用，导致信息损失和性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了GNNs在密集图上的计算瓶颈和传统稀疏化方法的局限性。他们注意到现有多视图方法和异构图学习方法不适用于具有连续边特征的同构图。受异构图神经网络（如R-GCN、HAN）的启发，作者扩展了这些方法到同构图；借鉴了多视图表示学习的思想，但基于物理交互强度构建视图；受到多尺度GNNs的启发，但使用相同节点集的不同边集。作者还从数学上证明了新框架的理论优势，设计了异构消息传递架构，同时进行视图内和视图间的消息传递，以捕获不同尺度的交互信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将单一密集图分解为多个视图（子图），每个视图捕捉不同尺度的交互，通过异构消息传递架构同时学习这些视图的表示。实现流程包括：1) 构建图元组，将原始图分解为不相交的子图（如强连接图和弱连接图）；2) 进行异构消息传递，包括视图内消息传递（每个子图内计算节点表示）和视图间消息传递（跨子图计算表示）；3) 融合多视图表示，使用可学习的标量权重组合所有视图信息；4) 具体实现分为GINE-Gt（用于一般图）和EGNN-Gt（用于几何数据）两种架构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出多视图图元组表示，将单一密集图分解为多个子图，保留多尺度信息；2) 设计异构消息传递架构，同时进行视图内和视图间的消息传递，考虑操作顺序敏感性；3) 提供理论保证，证明新框架比单图模型更具表现力且风险更低；4) 在分子性质预测和宇宙学参数推断等科学应用中验证了框架的有效性。相比之前工作，不同之处在于：不丢弃任何尺度信息，避免任意选择单一交互尺度；扩展异构图方法到同构图；基于物理交互强度而非自监督构建视图；在相同节点集上定义多个图，避免跨级别对齐的复杂性；不依赖低秩假设，能更好保留原始数据信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个多视图图元组框架，通过分解密集图为多个子图并使用异构消息传递架构同时学习不同尺度的交互，有效解决了图神经网络在密集图上的计算效率与信息保留之间的权衡问题，并在科学应用中展示了优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) typically scale with the number of graph edges,making them well suited for sparse graphs but less efficient on dense graphs,such as point clouds or molecular interactions. A common remedy is to sparsifythe graph via similarity thresholding or distance pruning, but this forces anarbitrary choice of a single interaction scale and discards crucial informationfrom other scales. To overcome this limitation, we introduce a multi-viewgraph-tuple framework. Instead of a single graph, our graph-tuple frameworkpartitions the graph into disjoint subgraphs, capturing primary localinteractions and weaker, long-range connections. We then learn multi-viewrepresentations from the graph-tuple via a heterogeneous message-passingarchitecture inspired by the theory of non-commuting operators, which weformally prove is strictly more expressive and guarantees a lower oracle riskcompared to single-graph message-passing models. We instantiate our frameworkon two scientific domains: molecular property prediction from feature-scarceCoulomb matrices and cosmological parameter inference from geometric pointclouds. On both applications, our multi-view graph-tuple models demonstratebetter performance than single-graph baselines, highlighting the power andversatility of our multi-view approach.</description>
      <author>example@mail.com (Shiyu Chen, Ningyuan, Huang, Soledad Villar)</author>
      <guid isPermaLink="false">2510.10341v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2510.10097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Gesplat，一个基于3DGS的框架，能够从未配准的稀疏图像中进行鲁棒的新型视图合成和几何一致的3D重建&lt;h4&gt;背景&lt;/h4&gt;NeRF和3DGS已推动3D重建和新型视图合成发展，但严重依赖准确的相机姿态和密集视角覆盖，限制了在稀疏视图场景中的应用&lt;h4&gt;目的&lt;/h4&gt;克服现有方法在稀疏视图场景中的局限性，实现无需准确相机姿态的鲁棒3D重建和视图合成&lt;h4&gt;方法&lt;/h4&gt;利用VGGT基础模型替代COLMAP进行初始姿态估计；采用混合高斯表示结合视图间匹配一致性进行双位置-形状优化；引入图引导的属性细化模块增强场景细节；使用基于流的深度正则化提高深度估计准确性&lt;h4&gt;主要发现&lt;/h4&gt;通过定量和定性实验证明，相比其他无姿态方法，Ges在前向-facing和大规模复杂数据集上实现了更鲁棒的性能&lt;h4&gt;结论&lt;/h4&gt;Gesplat框架能够在稀疏视图条件下实现更鲁棒的3D重建和视图合成，拓展了NeRF和3DGS的应用范围&lt;h4&gt;翻译&lt;/h4&gt;神经辐射场和3D高斯飞溅已经推动了3D重建和新型视图合成的发展，但仍然严重依赖准确的相机姿态和密集的视角覆盖。这些要求限制了它们在稀疏视图场景中的应用，在这些场景中姿态估计变得不可靠且监督不足。为了克服这些挑战，我们引入了Gesplat，一个基于3DGS的框架，能够从未配准的稀疏图像中进行鲁棒的新型视图合成和几何一致的重建。与之前依赖COLMAP进行稀疏点云初始化的工作不同，我们利用VGGT基础模型获得更可靠的初始姿态和密集点云。我们的方法整合了几个关键创新：1) 通过视图间匹配一致性增强的双位置-形状优化的混合高斯表示；2) 增强场景细节的图引导属性细化模块；3) 基于流的深度正则化，提高深度估计准确性以实现更有效的监督。全面的定量和定性实验表明，与其他无姿态方法相比，我们的方法在前向-facing和大规模复杂数据集上实现了更鲁棒的性能&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从稀疏视角且没有相机位姿信息的图像中进行鲁棒的3D重建和新视角合成的问题。这个问题在现实中很重要，因为在实际场景中获取密集、覆盖良好的图像集通常不切实际且成本高昂，而稀疏视角的3D重建在自主导航、VR/AR和机器人技术等应用中至关重要。有限视角导致训练期间监督不足，会造成伪影和有缺陷的重建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有NeRF和3DGS方法在稀疏视角和无位姿设置下的局限性，特别是传统COLMAP方法在稀疏视角下的不可靠性。他们引入VGGT基础模型替代COLMAP进行初始点云和位姿估计，并设计了混合高斯表示结合普通和基于射线的高斯，通过多视图匹配一致性进行优化。作者借鉴了VGGT进行初始场景重建，受[28]启发采用混合高斯表示，使用图神经网络优化属性，并参考[31]的方法利用光流进行深度估计。整个设计思路是在保留3DGS高效性的同时，解决其在稀疏视角下的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入适当的几何先验约束场景结构，同时采用优化和正则化技术细化场景细节。整体流程包括：1)使用VGGT生成初始密集点云和相机位姿；2)采用混合高斯表示(普通高斯和基于射线的高斯)；3)利用多视图匹配一致性进行位置和形状双重优化；4)应用图神经网络优化高斯属性；5)使用基于流的深度正则化提高渲染质量；6)联合优化高斯参数和相机位姿。测试阶段使用训练好的高斯模型细化测试相机位姿。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合高斯表示，结合普通和基于射线的高斯，通过绑定高斯到匹配射线增强多视图一致性；2)图引导属性优化模块，使用图神经网络优化高斯属性；3)基于流的深度正则化，在极线几何框架内通过光流估计可靠深度图；4)使用VGGT替代COLMAP进行初始化，在稀疏视角下更可靠。相比之前工作，Gesplat不依赖密集视角覆盖和已知准确位姿，计算成本更低，能处理更稀疏的输入，且在几何一致性和细节质量上表现更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Gesplat通过引入混合高斯表示、图引导属性优化和基于流的深度正则化，实现了从稀疏视角无位姿图像中进行鲁棒3D重建和新视角合成，显著提高了场景几何一致性和细节质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced3D reconstruction and novel view synthesis, but remain heavily dependent onaccurate camera poses and dense viewpoint coverage. These requirements limittheir applicability in sparse-view settings, where pose estimation becomesunreliable and supervision is insufficient. To overcome these challenges, weintroduce Gesplat, a 3DGS-based framework that enables robust novel viewsynthesis and geometrically consistent reconstruction from unposed sparseimages. Unlike prior works that rely on COLMAP for sparse point cloudinitialization, we leverage the VGGT foundation model to obtain more reliableinitial poses and dense point clouds. Our approach integrates several keyinnovations: 1) a hybrid Gaussian representation with dual position-shapeoptimization enhanced by inter-view matching consistency; 2) a graph-guidedattribute refinement module to enhance scene details; and 3) flow-based depthregularization that improves depth estimation accuracy for more effectivesupervision. Comprehensive quantitative and qualitative experiments demonstratethat our approach achieves more robust performance on both forward-facing andlarge-scale complex datasets compared to other pose-free methods.</description>
      <author>example@mail.com (Jiahui Lu, Haihong Xiao, Xueyan Zhao, Wenxiong Kang)</author>
      <guid isPermaLink="false">2510.10097v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification</title>
      <link>http://arxiv.org/abs/2510.09367v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Minkowski-MambaNet是一种创新的深度学习框架，能够直接从原始LiDAR数据估算森林木材体积和地上生物量，显著提高了森林生物量量化的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确的森林生物量量化对碳循环监测至关重要。虽然机载LiDAR在捕捉森林三维结构方面表现出色，但由于难以建模区分树木所需的长程依赖关系，直接从点云估算木材体积和地上生物量(AGB)具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架，能够直接从原始LiDAR数据估算体积和AGB，提高森林生物量量化的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出Minkowski-MambaNet，将Mamba模型的选择性状态空间模型(SSM)集成到Minkowski网络中，有效编码全局上下文和长程依赖关系以提高树木区分能力，并融入跳跃连接以增强特征并加速收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在丹麦国家森林清单LiDAR数据上评估，Minkowski-MambaNet显著优于最先进的方法，提供了更准确和稳健的估计。该方法不需要数字地形模型(DTM)，并且对边界伪影具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Minkowski-MambaNet为大规模森林生物量分析提供了强大的工具，推动了基于LiDAR的森林清查的发展。&lt;h4&gt;翻译&lt;/h4&gt;准确的森林生物量量化对碳循环监测至关重要。虽然机载LiDAR在捕捉森林三维结构方面表现出色，但由于难以建模区分树木所需的长程依赖关系，直接从点云估算木材体积和地上生物量(AGB)具有挑战性。我们提出了Minkowski-MambaNet，一种创新的深度学习框架，可直接从原始LiDAR估算体积和AGB。其关键创新是将Mamba模型的选择性状态空间模型(SSM)集成到Minkowski网络中，从而有效编码全局上下文和长程依赖关系，以提高树木区分能力。融入了跳跃连接以增强特征并加速收敛。在丹麦国家森林清单LiDAR数据上评估，Minkowski-MambaNet显著优于最先进的方法，提供了更准确和稳健的估计。重要的是，它不需要数字地形模型(DTM)，并且对边界伪影具有鲁棒性。这项工作为大规模森林生物量分析提供了强大的工具，推动了基于LiDAR的森林清查的发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何直接从原始LiDAR点云数据中准确估算森林生物量（包括木材体积和地上生物量AGB）的问题。这个问题非常重要，因为森林是陆地生态系统中最大的碳库，约占全球陆地碳储量的40%，准确量化森林生物量对于全球碳循环监测、气候变化研究和森林管理至关重要。传统方法要么成本高昂、难以大范围应用，要么无法充分捕捉森林的三维结构信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统CNN处理点云效率低，基于局部卷积的方法难以捕获长距离依赖关系，而自注意力机制计算复杂度高。作者借鉴了Minkowski引擎（高效处理稀疏数据）和Mamba状态空间模型（高效序列建模）的工作，以MSENet50为基础骨干网络，设计了两个关键模块：Mamba-SEBottleneck（解决长距离依赖问题）和特征融合修改层（解决多尺度特征利用问题），形成了一个端到端的直接回归框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合Mamba的选择性状态空间模型与Minkowski稀疏卷积，有效捕获点云中的长距离依赖关系和多层次特征，直接从原始LiDAR点云估算森林生物量。实现流程包括：1)数据预处理（保留异质性、排除不一致样本、设置高度阈值）；2)基于MSENet50构建网络架构；3)Mamba-SEBottleneck模块将点特征转换为序列并处理，生成动态注意力权重；4)特征融合修改层通过跳跃连接融合中间层与深层特征；5)训练与评估使用丹麦国家森林调查数据，通过RMSE、R2等指标比较性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Mamba-SEBottleneck模块，将Mamba状态空间模型与Minkowski稀疏卷积结合，以线性复杂度捕获长距离依赖；2)特征融合修改层，通过跳跃连接保留多尺度特征；3)端到端直接回归方法，避免中间步骤。相比之前工作，它比传统方法不依赖手工特征，比基于局部卷积的方法更好地捕获全局结构，比自注意力机制计算效率更高，且无需数字地形模型预处理，对边界噪声更鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Minkowski-MambaNet通过创新性地结合Mamba状态空间模型与Minkowski稀疏卷积，首次实现了从原始LiDAR点云中高效准确地直接估算森林生物量，为森林碳汇监测和资源管理提供了强大的新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate forest biomass quantification is vital for carbon cycle monitoring.While airborne LiDAR excels at capturing 3D forest structure, directlyestimating woody volume and Aboveground Biomass (AGB) from point clouds ischallenging due to difficulties in modeling long-range dependencies needed todistinguish trees.We propose Minkowski-MambaNet, a novel deep learningframework that directly estimates volume and AGB from raw LiDAR. Its keyinnovation is integrating the Mamba model's Selective State Space Model (SSM)into a Minkowski network, enabling effective encoding of global context andlong-range dependencies for improved tree differentiation. Skip connections areincorporated to enhance features and accelerate convergence.Evaluated on DanishNational Forest Inventory LiDAR data, Minkowski-MambaNet significantlyoutperforms state-of-the-art methods, providing more accurate and robustestimates. Crucially, it requires no Digital Terrain Model (DTM) and is robustto boundary artifacts. This work offers a powerful tool for large-scale forestbiomass analysis, advancing LiDAR-based forest inventories.</description>
      <author>example@mail.com (Jinxiang Tu, Dayong Ren, Fei Shi, Zhenhong Jia, Yahong Ren, Jiwei Qin, Fang He)</author>
      <guid isPermaLink="false">2510.09367v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes</title>
      <link>http://arxiv.org/abs/2510.09364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VAD-GS，一种针对具有挑战性城市场景的3DGS框架，通过体素可视性推理、多样性感知视图选择和基于补丁匹配的多视立体重建来解决3DGS在无界动态环境中初始化点云不完整导致的几何失真和伪影问题。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian splatting (3DGS)在合成高质量新视角方面表现出色，但其效果严重依赖于初始化点云的质量。在无界、动态的城市环境中，实现场景结构的均匀和完整点覆盖需要重叠的观察视锥，这一假设常常不成立，导致训练的高斯模型出现失真和伪影。&lt;h4&gt;目的&lt;/h4&gt;解决3DGS在无界动态城市环境中因初始化点云不完整导致的几何失真和伪影问题，提高静态和动态对象的几何重建质量。&lt;h4&gt;方法&lt;/h4&gt;提出VAD-GS框架，包含三个关键组件：1) 基于体素的可视性推理识别不可靠的几何结构；2) 通过多样性感知的视图选择选择信息量大的支持视图；3) 通过基于补丁匹配的多视立体重建恢复缺失结构。这种设计能够在缺乏初始点的区域中，由可靠的几何先验引导生成新的高斯基元。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo和nuScenes数据集上的实验表明，VAD-GS优于最先进的3DGS方法，并显著提高了静态和动态对象重建几何的质量。&lt;h4&gt;结论&lt;/h4&gt;VAD-GS框架能够有效解决3DGS在具有挑战性的城市环境中的几何恢复问题，即使在没有初始点的区域也能生成高质量的几何重建。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯溅射(3DGS)在合成高质量新视角方面展示了令人印象深刻的性能。尽管如此，其有效性严重依赖于初始化点云的质量。具体来说，在底层场景结构上实现均匀和完整的点覆盖需要重叠的观察视锥，这一假设在无界、动态的城市环境中常常被违反。使用部分初始化的点云训练高斯模型通常会导致失真和伪影，因为相机射线可能无法与有效表面相交，导致与被遮挡或不可见几何相关联的高斯基元出现错误的梯度传播。此外，现有的密集化策略只是简单地从现有高斯基元克隆和分割，无法重建缺失的结构。为了解决这些局限性，我们提出了VAD-GS，一种针对具有挑战性的城市场景几何恢复的3DGS框架。我们的方法通过基于体素的可视性推理识别不可靠的几何结构，通过多样性感知的视图选择选择信息量大的支持视图，并通过基于补丁匹配的多视立体重建恢复缺失结构。这种设计使得即使在缺乏初始点的区域中，也能够由可靠的几何先验引导生成新的高斯基元。在Waymo和nuScenes数据集上的大量实验表明，VAD-GS优于最先进的3DGS方法，并显著提高了静态和动态对象重建几何的质量。源代码将在发表后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯泼溅(3DGS)在动态城市场景中重建完整几何结构的问题。具体来说，3DGS方法的有效性依赖于初始点云质量，但在无边界的动态城市环境中，难以实现均匀且完整的点覆盖，导致训练出的模型出现失真和伪影。这个问题在现实中对自动驾驶系统至关重要，因为它们需要高质量的场景重建来进行模拟和验证，而传统模拟器缺乏场景多样性和可扩展性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3DGS在动态城市场景中的局限性，特别是初始点云不足导致的几何失真问题。他们注意到现有密集化方法只是简单克隆和分割现有高斯原语，无法处理未初始化区域。作者借鉴了多视图立体视觉(MVS)技术、体素化技术和z-buffer可见性推理，并结合实例分割方法来处理动态对象。通过整合这些技术，他们设计了一个主动评估结构完整性并选择性重建不完整区域的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可见性感知的密集化策略主动评估并重建缺失的几何结构，即使在初始点云不完整的区域也能生成新的高斯原语。整体流程包括：1)对初始点云进行体素化以获得均匀空间密度；2)进行基于体素的可见性推理，识别不可靠几何结构；3)使用多样性感知的视图选择策略选择信息量大的支持视图；4)通过基于块匹配的MVS算法重建深度和法线信息；5)使用这些几何先验指导高斯密集化和优化；6)结合颜色、法线和深度损失进行模型训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)针对动态城市场景的高斯泼溅框架，主动使用多摄像头、跨帧观测完成缺失几何；2)基于体素表面可见性推理方法，识别不可靠的静态和动态对象几何；3)多样性感知的采样策略，提高MVS重建质量；4)将MVS重建扩展到动态多摄像头驾驶场景。相比之前工作，VAD-GS不局限于现有高斯原语区域，能处理未初始化区域；能处理动态对象而非仅限于静态场景；利用多摄像头和跨帧观测而非仅单摄像头相邻帧；通过可见性推理区分可见和被遮挡几何，避免错误更新被遮挡的高斯原语。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VAD-GS通过可见性感知的密集化策略和多视图立体视觉重建，显著提高了3D高斯泼溅在动态城市场景中的几何质量和渲染保真度，解决了初始点云不完整导致的几何失真问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian splatting (3DGS) has demonstrated impressive performance insynthesizing high-fidelity novel views. Nonetheless, its effectivenesscritically depends on the quality of the initialized point cloud. Specifically,achieving uniform and complete point coverage over the underlying scenestructure requires overlapping observation frustums, an assumption that isoften violated in unbounded, dynamic urban environments. Training Gaussianmodels with partially initialized point clouds often leads to distortions andartifacts, as camera rays may fail to intersect valid surfaces, resulting inincorrect gradient propagation to Gaussian primitives associated with occludedor invisible geometry. Additionally, existing densification strategies simplyclone and split Gaussian primitives from existing ones, incapable ofreconstructing missing structures. To address these limitations, we proposeVAD-GS, a 3DGS framework tailored for geometry recovery in challenging urbanscenes. Our method identifies unreliable geometry structures via voxel-basedvisibility reasoning, selects informative supporting views throughdiversity-aware view selection, and recovers missing structures via patchmatching-based multi-view stereo reconstruction. This design enables thegeneration of new Gaussian primitives guided by reliable geometric priors, evenin regions lacking initial points. Extensive experiments on the Waymo andnuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGSapproaches and significantly improves the quality of reconstructed geometry forboth static and dynamic objects. Source code will be released upon publication.</description>
      <author>example@mail.com (Yikang Zhang, Rui Fan)</author>
      <guid isPermaLink="false">2510.09364v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Obstacle Avoidance using Dynamic Movement Primitives and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.09254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从单个人工演示快速生成平滑、次优、无碰撞的三维笛卡尔轨迹的方法，通过结合动态运动基元和强化学习技术，实现了对多种障碍配置的高效轨迹规划。&lt;h4&gt;背景&lt;/h4&gt;基于学习的运动规划虽能快速生成次优轨迹，但通常需要大型训练数据集或昂贵的人类演示收集，限制了其在实际应用中的可行性。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代方法，仅从单个人工演示就能快速生成高质量的三维笛卡尔轨迹，减少对大量训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;将演示编码为动态运动基元(DMP)，使用基于策略的强化学习迭代重塑DMP以创建多样化轨迹数据集，然后训练神经网络输入障碍物参数并输出相应的DMP参数。&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和真实机器人实验中，该方法在计算时间、执行时间和轨迹长度方面均优于RRT-Connect基线算法，并能支持针对不同障碍几何形状和末端执行器尺寸的多模态轨迹生成。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了传统基于学习的运动规划对大量训练数据的依赖问题，为机器人轨迹规划提供了一种高效、实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于学习的运动规划可以快速生成次优轨迹。然而，它通常需要大型训练数据集或昂贵的人类演示收集工作。本文提出了一种替代方法，从单个人工演示快速生成平滑、次优、无碰撞的三维笛卡尔轨迹。该演示被编码为动态运动基元(DMP)，并使用基于策略的强化学习进行迭代重塑，为不同的障碍物配置创建多样化的轨迹数据集。该数据集用于训练神经网络，输入是从点云自动导出的描述障碍物尺寸和位置的任务参数，输出生成轨迹的DMP参数。该方法在仿真和真实机器人实验中得到验证，在计算和执行时间以及轨迹长度方面优于RRT-Connect基线，同时支持针对不同障碍几何形状和末端执行器尺寸的多模态轨迹生成。视频和实现代码可在https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人快速生成平滑、接近最优的无碰撞轨迹问题，特别是在避障场景中的应用。这个问题在现实中很重要，因为机器人需要在复杂环境中自主导航和操作，快速生成轨迹对于实时应用至关重要，而平滑的轨迹可以减少机械磨损并提高执行效率。此外，减少对大量演示数据的依赖可以降低部署成本，同时能够处理多种障碍物配置和末端执行器尺寸变化，增强了机器人的适应性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到基于学习的方法在运动规划中的优势，但注意到它们通常需要大量训练数据。因此，他们想到使用动态运动基元（DMP）来编码和泛化演示轨迹，利用其良好的泛化能力。然后采用策略改进与路径积分（PI²）强化学习算法来迭代调整演示轨迹，生成多样化的避障轨迹数据集。最后将生成的轨迹数据集映射到一个神经网络中，实现快速在线轨迹生成。该方法借鉴了现有工作，包括使用DMP作为运动基元、PI²作为强化学习算法、点云处理技术来检测障碍物，以及参考现有避障方法作为比较基准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用单个人工演示作为初始轨迹，通过强化学习迭代调整生成多样化的避障轨迹，训练神经网络将任务参数映射到DMP参数，并从点云中自动提取任务参数以适应新场景。整体实现流程分为两个阶段：1）离线训练阶段：将演示编码为DMP参数，使用PI²算法根据不同成本函数调整参数生成多样化轨迹数据集，训练神经网络将任务参数映射到DMP参数；2）在线执行阶段：从点云中提取任务参数，使用神经网络推断适合当前场景的DMP参数，生成并执行无碰撞轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）数据效率高，只需一个人工演示；2）能自动从点云提取任务参数；3）支持多模态轨迹生成；4）确保轨迹平滑；5）支持多达三个连续任务参数；6）实时性能好，在线生成时间仅0.2秒。相比之前的工作，该方法比传统采样方法（如RRT-Connect）计算更快，轨迹更平滑；比其他基于学习的方法需要更少训练数据；比仅使用PI²优化的方法将计算负担转移到离线阶段；比其他DMP方法支持更多任务参数和更复杂场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合动态运动基元和强化学习的高效避障方法，只需一个人工演示即可快速生成平滑、接近最优的无碰撞轨迹，并能自动适应不同的障碍物配置和末端执行器尺寸。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning-based motion planning can quickly generate near-optimaltrajectories. However, it often requires either large training datasets orcostly collection of human demonstrations. This work proposes an alternativeapproach that quickly generates smooth, near-optimal collision-free 3DCartesian trajectories from a single artificial demonstration. Thedemonstration is encoded as a Dynamic Movement Primitive (DMP) and iterativelyreshaped using policy-based reinforcement learning to create a diversetrajectory dataset for varying obstacle configurations. This dataset is used totrain a neural network that takes as inputs the task parameters describing theobstacle dimensions and location, derived automatically from a point cloud, andoutputs the DMP parameters that generate the trajectory. The approach isvalidated in simulation and real-robot experiments, outperforming a RRT-Connectbaseline in terms of computation and execution time, as well as trajectorylength, while supporting multi-modal trajectory generation for differentobstacle geometries and end-effector dimensions. Videos and the implementationcode are available at https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2.</description>
      <author>example@mail.com (Dominik Urbaniak, Alejandro Agostini, Pol Ramon, Jan Rosell, Raúl Suárez, Michael Suppa)</author>
      <guid isPermaLink="false">2510.09254v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling</title>
      <link>http://arxiv.org/abs/2510.09088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MambaH-Fit，一种专门用于基于超曲面拟合的点云法线估计的状态空间建模框架。该方法通过注意力驱动的分层特征融合和基于块的状态空间模型，有效解决了现有方法在建模细粒度几何结构方面的不足，显著提高了点云法线估计的准确性、鲁棒性和灵活性。&lt;h4&gt;背景&lt;/h4&gt;现有的点云法线估计方法在建模细粒度几何结构方面存在不足，限制了预测法线的准确性。虽然状态空间模型(特别是Mamba)已显示出强大的建模能力，能够以线性复杂度捕捉长程依赖关系，但现有的基于Mamba的方法主要关注全局形状结构的理解，对局部、细粒度几何细节的建模探索不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模局部、细粒度几何细节的点云法线估计方法，以提高预测法线的准确性、鲁棒性和灵活性，解决现有方法在精细几何结构建模方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;首先提出了一种注意力驱动的分层特征融合(AHFF)方案，用于自适应融合多尺度点云块特征，显著增强局部点云邻域中的几何上下文学习。在此基础上，进一步提出了基于块的状态空间模型(PSSM)，通过状态动力学将点云块建模为隐式超曲面，实现法线预测的有效细粒度几何理解。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，MambaH-Fit方法在准确性、鲁棒性和灵活性方面均优于现有方法。消融研究进一步验证了所提出的AHFF和PSSM组件对方法性能的重要贡献。&lt;h4&gt;结论&lt;/h4&gt;MambaH-Fit通过结合注意力驱动的分层特征融合和基于块的状态空间模型，成功解决了点云法线估计中细粒度几何结构建模的挑战，为点云处理提供了新的思路和方法，具有重要的理论和实践意义。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了MambaH-Fit，一种专门用于基于超曲面拟合的点云法线估计的状态空间建模框架。现有的法线估计方法在建模细粒度几何结构方面往往表现不足，从而限制了预测法线的准确性。最近，状态空间模型(SSMs)，特别是Mamba，已经展示了强大的建模能力，能够以线性复杂度捕捉长程依赖关系，并启发了点云处理的适应性方法。然而，现有的基于Mamba的方法主要关注理解全局形状结构，而对局部、细粒度几何细节的建模探索不足。为了解决上述问题，我们首先引入了一种注意力驱动的分层特征融合(AHFF)方案，用于自适应融合多尺度点云块特征，显著增强了局部点云邻域中的几何上下文学习。在此基础上，我们进一步提出了基于块的状态空间模型(PSSM)，通过状态动力学将点云块建模为隐式超曲面，实现了法线预测的有效细粒度几何理解。在基准数据集上的大量实验表明，我们的方法在准确性、鲁棒性和灵活性方面优于现有方法。消融研究进一步验证了所提出组件的贡献。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云法向量估计问题，即从3D点云数据中准确预测每个点的表面法线方向。这个问题在3D视觉领域非常重要，因为准确的法向量是许多应用的基础，包括点云过滤、点云配准和表面重建等。原始点云缺乏连接信息且通常带有噪声，使得法向量估计变得困难，而现有方法在建模细粒度几何结构方面存在不足，限制了预测法向量的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，指出它们在建模细粒度几何结构方面的不足。他们借鉴了状态空间模型（特别是Mamba）在建模长程依赖关系方面的能力，这种模型最初在自然语言处理领域表现出色。同时，他们参考了HSurf-Net的隐式超表面拟合思想，但发现其残差块结构独立处理点特征，没有明确建模补丁内的点间关系。作者还注意到Transformer的自注意力机制虽然有效，但其二次方计算复杂度不适合处理大规模点云。基于这些观察，他们设计了两个关键模块：注意力驱动的分层特征融合模块和基于补丁的状态空间模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用状态空间模型（特别是Mamba）来建模点云补丁内的隐式超表面，从而更准确地估计点云法向量。方法通过注意力机制自适应地融合多尺度几何特征，并利用Mamba的高效序列建模能力捕捉点云补丁内的长程依赖关系。整体流程包括：1)从点云中提取局部邻域并进行归一化处理；2)使用点特征编码器提取几何特征；3)通过AHFF模块融合多尺度特征；4)使用PSSM模块将点特征作为序列输入，通过Mamba块建模隐式超表面；5)估计点权重并预测法向量；6)对预测结果进行归一化和方向调整。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)注意力驱动的分层特征融合模块，能自适应融合多尺度特征并学习相关几何区域；2)基于补丁的状态空间模型，首次将Mamba应用于点云法向量估计，有效建模补丁内点间关系。相比之前的工作，不同之处在于：不需要预定义多项式阶数（优于n-jet拟合）；明确建模点间关系（优于HSurf-Net的独立点处理）；计算复杂度为线性（优于Transformer的二次复杂度）；专注于局部细粒度几何细节（优于现有Mamba方法的全局关注）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MambaH-Fit首次将状态空间模型引入点云法向量估计领域，通过创新的注意力驱动的分层特征融合和基于补丁的状态空间模型，显著提高了法向量估计的准确性、鲁棒性和对复杂几何结构的适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MambaH-Fit, a state space modelling framework tailored forhyper-surface fitting-based point cloud normal estimation. Existing normalestimation methods often fall short in modelling fine-grained geometricstructures, thereby limiting the accuracy of the predicted normals. Recently,state space models (SSMs), particularly Mamba, have demonstrated strongmodelling capability by capturing long-range dependencies with linearcomplexity and inspired adaptations to point cloud processing. However,existing Mamba-based approaches primarily focus on understanding global shapestructures, leaving the modelling of local, fine-grained geometric detailslargely under-explored. To address the issues above, we first introduce anAttention-driven Hierarchical Feature Fusion (AHFF) scheme to adaptively fusemulti-scale point cloud patch features, significantly enhancing geometriccontext learning in local point cloud neighbourhoods. Building upon this, wefurther propose Patch-wise State Space Model (PSSM) that models point cloudpatches as implicit hyper-surfaces via state dynamics, enabling effectivefine-grained geometric understanding for normal prediction. Extensiveexperiments on benchmark datasets show that our method outperforms existingones in terms of accuracy, robustness, and flexibility. Ablation studiesfurther validate the contribution of the proposed components.</description>
      <author>example@mail.com (Weijia Wang, Yuanzhi Su, Pei-Gen Ye, Yuan-Gen Wang, Xuequan Lu)</author>
      <guid isPermaLink="false">2510.09088v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels</title>
      <link>http://arxiv.org/abs/2510.09035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DuNe的双视图框架，用于解决带噪声标签的LiDAR语义分割领域泛化问题，在不同数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;准确感知对车辆安全至关重要，LiDAR是自动驾驶的关键技术。LiDAR标注常因传感器不完美、遮挡和人为错误而存在噪声，降低分割精度并在领域转移时进一步放大，威胁系统可靠性。点云的稀疏和不规则结构限制了2D噪声学习方法在3D LiDAR分割中的直接应用。&lt;h4&gt;目的&lt;/h4&gt;引入带噪声标签的LiDAR语义分割领域泛化(DGLSS-NL)这一新任务，建立首个基准，并提出有效方法解决现有噪声标签学习方法对LiDAR数据适应性差的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DuNe双视图框架，包含强分支和弱分支，强制执行特征级别的一致性，并基于置信感知的预测过滤应用交叉熵损失。&lt;h4&gt;主要发现&lt;/h4&gt;在10%对称标签噪声下，在SemanticKITTI上达到56.86% mIoU，在nuScenes上达到42.28% mIoU，在SemanticPOSS上达到52.58% mIoU，总体算术平均(AM)为49.57%，调和平均(HM)为48.50%。&lt;h4&gt;结论&lt;/h4&gt;DuNe框架展示了在DGLSS-NL任务中具有强大的领域泛化能力，代码已在项目页面公开。&lt;h4&gt;翻译&lt;/h4&gt;准确的感知对车辆安全至关重要，LiDAR是自动驾驶的关键使能技术。为确保在不同环境、传感器类型和天气条件下的鲁棒性能且无需昂贵的重新标注，基于LiDAR的3D语义分割中的领域泛化是必不可少的。然而，由于传感器不完美、遮挡和人为错误，LiDAR标注通常存在噪声。这种噪声会降低分割精度，并在领域转移时进一步放大，威胁系统可靠性。虽然噪声标签学习在图像中已被广泛研究，但其扩展到领域泛化下的3D LiDAR分割基本上仍未被探索，因为点云的稀疏和不规则结构限制了2D方法的直接使用。为解决这一差距，我们引入了带噪声标签的LiDAR语义分割领域泛化这一新任务，并通过将三种代表性的噪声标签学习策略从图像分类调整到3D分割，建立了首个基准。然而，我们发现现有的噪声标签学习方法对LiDAR数据的适应性较差。因此，我们提出了DuNe，一个具有强分支和弱分支的双视图框架，强制执行特征级别的一致性，并基于置信感知的预测过滤应用交叉熵损失。我们的方法在SemanticKITTI上实现了56.86% mIoU，在nuScenes上实现了42.28%，在SemanticPOSS上实现了52.58%，在10%对称标签噪声下展示了最先进的性能，总体算术平均(AM)为49.57%，调和平均(HM)为48.50%，从而证明了在DGLSS-NL任务中具有强大的领域泛化能力。代码可在我们的项目页面上获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在带有不完美标签（噪声标签）条件下实现激光雷达语义分割的单域泛化问题。这个问题在现实中非常重要，因为自动驾驶安全依赖于准确的3D环境感知，而激光雷达是核心传感器；实际应用中激光雷达标注不可避免地存在噪声，且现有方法大多假设完美标注；噪声标签会降低分割精度，在域转移情况下这种影响会被放大，威胁系统可靠性；同时，重新标注不同环境下的数据成本高昂，域泛化可以避免这一成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有域泛化方法在噪声标签下的局限性，并观察到图像领域的噪声标签学习方法不能直接迁移到3D点云，因为点云是稀疏、不规则和无序的。作者借鉴了图像领域的三种代表性噪声标签学习方法（TCL、DISC、NPN），以及DGLSS框架中的稀疏不变特征一致性和语义相关性一致性，还借鉴了PolarMix数据增强技术。基于这些现有工作，作者设计了DuNe双视图框架，结合了几何感知的强视图和互补的弱视图，通过瓶颈一致性对齐它们，并采用基于置信度过滤的部分和负监督，针对性地解决了点云特性带来的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双视图学习（强视图和弱视图）来处理噪声标签下的域泛化问题。强视图使用几何混合生成增强点云用于构建噪声鲁棒标签，弱视图使用原始和稀疏增强版本形成配对输入并强制执行一致性损失。整体流程包括：1)使用PolarMix将点云增强为强视图和弱视图；2)通过稀疏增强生成四个派生视图；3)使用稀疏卷积网络编码特征；4)在强视图和弱视图中分别生成预测；5)结合DGLSS损失、NPN损失和双视图特征一致性损失进行训练；6)推理时仅使用强分支以提高效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出DGLSS-NL新任务并建立首个基准；2)设计DuNe双视图框架，结合几何感知视图、一致性学习和噪声感知监督；3)实现噪声感知域泛化，能抵抗标签污染和域转移。相比之前工作，不同之处在于：不同于DGLSS假设完美标注，本文处理噪声标签；不同于图像噪声学习方法，本文针对3D点云特性；不同于大多数3D方法只关注单一问题，本文同时处理域转移和噪声标签；方法设计上创新性地采用双视图、自适应选择策略和多种损失函数结合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了DuNe双视图框架，通过结合几何感知视图、一致性学习和噪声感知监督，首次实现了在噪声标签下具有强域泛化能力的激光雷达语义分割，并建立了首个DGLSS-NL基准，显著提升了自动驾驶感知系统在真实世界复杂环境中的可靠性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate perception is critical for vehicle safety, with LiDAR as a keyenabler in autonomous driving. To ensure robust performance acrossenvironments, sensor types, and weather conditions without costlyre-annotation, domain generalization in LiDAR-based 3D semantic segmentation isessential. However, LiDAR annotations are often noisy due to sensorimperfections, occlusions, and human errors. Such noise degrades segmentationaccuracy and is further amplified under domain shifts, threatening systemreliability. While noisy-label learning is well-studied in images, itsextension to 3D LiDAR segmentation under domain generalization remains largelyunexplored, as the sparse and irregular structure of point clouds limits directuse of 2D methods. To address this gap, we introduce the novel task DomainGeneralization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL)and establish the first benchmark by adapting three representative noisy-labellearning strategies from image classification to 3D segmentation. However, wefind that existing noisy-label learning approaches adapt poorly to LiDAR data.We therefore propose DuNe, a dual-view framework with strong and weak branchesthat enforce feature-level consistency and apply cross-entropy loss based onconfidence-aware filtering of predictions. Our approach shows state-of-the-artperformance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and52.58% on SemanticPOSS under 10% symmetric label noise, with an overallArithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, therebydemonstrating robust domain generalization in DGLSS-NL tasks. The code isavailable on our project page.</description>
      <author>example@mail.com (Weitong Kong, Zichao Zeng, Di Wen, Jiale Wei, Kunyu Peng, June Moh Goo, Jan Boehm, Rainer Stiefelhagen)</author>
      <guid isPermaLink="false">2510.09035v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion</title>
      <link>http://arxiv.org/abs/2510.09020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  52 pages, 14 figures, 12 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MagicDock是一个创新的前瞻性框架，基于渐进流程和可微表面建模，解决了从头配体设计中的伪从头设计、有限对接建模和不灵活配体类型等限制问题。&lt;h4&gt;背景&lt;/h4&gt;从头配体设计是一项基础任务，旨在从头开始生成能够有效对接蛋白质受体并实现强结合亲和力的蛋白质或分子候选物。它对广泛的生物医学应用具有极其重要的意义。&lt;h4&gt;目的&lt;/h4&gt;解决现有从头配体设计研究中存在的伪从头设计、有限对接建模和不灵活配体类型三大限制问题。&lt;h4&gt;方法&lt;/h4&gt;MagicDock采用精心设计的梯度反转框架，整合受体和配体的通用对接知识；强调对接过程中的可微表面建模，利用可学习的3D点云表示精确捕获结合细节；为不同类型配体引入定制设计并整合到统一框架中。&lt;h4&gt;主要发现&lt;/h4&gt;在9种场景中的广泛实验表明，MagicDock比专门针对蛋白质或分子配体设计的最先进基线方法分别实现了27.1%和11.7%的平均改进。&lt;h4&gt;结论&lt;/h4&gt;MagicDock通过创新的方法解决了从头配体设计中的关键限制，在多种场景中表现出优越的性能，为生物医学应用提供了更有效的配体设计解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从头配体设计是一项基础任务，旨在从头开始生成能够有效对接蛋白质受体并实现强结合亲和力的蛋白质或分子候选物。它对广泛的生物医学应用具有极其重要的意义。然而，大多数现有研究受限于伪从头设计、有限的对接建模和不灵活的配体类型。为解决这些问题，我们提出了MagicDock，一个基于渐进流程和可微表面建模的前瞻性框架。我们采用精心设计的梯度反转框架，首先将受体和配体的通用对接知识整合到骨干模型中，然后通过结合预测将对接知识实例化为反向梯度流，迭代指导配体的从头生成。我们强调对接过程中的可微表面建模，利用可学习的3D点云表示来精确捕获结合细节，确保生成的配体通过直接和可解释的空间指纹保持对接有效性。我们为不同类型的配体引入定制设计，并将它们整合到具有灵活触发器的统一梯度反转框架中，确保广泛适用性。此外，我们为MagicDock的每个组件提供严格的理论保证。在9种场景中的广泛实验表明，MagicDock比专门针对蛋白质或分子配体设计的最先进基线方法分别实现了27.1%和11.7%的平均改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; De novo ligand design is a fundamental task that seeks to generate protein ormolecule candidates that can effectively dock with protein receptors andachieve strong binding affinity entirely from scratch. It holds paramountsignificance for a wide spectrum of biomedical applications. However, mostexisting studies are constrained by the \textbf{Pseudo De Novo},\textbf{Limited Docking Modeling}, and \textbf{Inflexible Ligand Type}. Toaddress these issues, we propose MagicDock, a forward-looking frameworkgrounded in the progressive pipeline and differentiable surface modeling. (1)We adopt a well-designed gradient inversion framework. To begin with, generaldocking knowledge of receptors and ligands is incorporated into the backbonemodel. Subsequently, the docking knowledge is instantiated as reverse gradientflows by binding prediction, which iteratively guide the de novo generation ofligands. (2) We emphasize differentiable surface modeling in the dockingprocess, leveraging learnable 3D point-cloud representations to preciselycapture binding details, thereby ensuring that the generated ligands preservedocking validity through direct and interpretable spatial fingerprints. (3) Weintroduce customized designs for different ligand types and integrate them intoa unified gradient inversion framework with flexible triggers, thereby ensuringbroad applicability. Moreover, we provide rigorous theoretical guarantees foreach component of MagicDock. Extensive experiments across 9 scenariosdemonstrate that MagicDock achieves average improvements of 27.1\% and 11.7\%over SOTA baselines specialized for protein or molecule ligand design,respectively.</description>
      <author>example@mail.com (Zekai Chen, Xunkai Li, Sirui Zhang, Henan Sun, Jia Li, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2510.09020v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.08849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FOLK的快速开放词汇3D实例分割方法，通过标签引导的知识蒸馏技术，解决了现有方法中因2D遮挡引入的噪声问题，并显著提高了推理速度。&lt;h4&gt;背景&lt;/h4&gt;开放词汇3D实例分割旨在分割和分类超出标注标签空间的实例。现有方法通常将3D实例映射到2D RGB-D图像，然后使用视觉语言模型进行分类，但这种映射策略会引入来自2D遮挡的噪声，并且在推理过程中需要大量计算和内存成本，降低了推理速度。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中由2D遮挡引入的噪声问题，减少计算和内存成本，加速推理过程。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于标签引导知识蒸馏的快速开放词汇3D实例分割方法(FOLK)。设计一个教师模型提取高质量的实例嵌入，并将其开放词汇知识蒸馏到3D学生模型中。教师模型为每个3D实例生成2D CLIP嵌入，结合可见性和视角多样性，作为蒸馏的学习目标。开发3D学生模型直接为每个3D实例生成3D嵌入。提出标签引导的蒸馏算法，将标签一致的2D嵌入中的开放词汇知识蒸馏到学生模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet200和Replica数据集上进行了实验，在ScanNet200数据集上达到了最先进的性能，AP50得分为35.7，比之前的方法运行速度大约快6.0倍到152.2倍。&lt;h4&gt;结论&lt;/h4&gt;FOLK方法有效解决了现有方法中的噪声和计算效率问题，代码将在论文被接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇3D实例分割(Open-vocabulary 3D instance segmentation)：指能够分割和分类训练时未见过类别的3D实例的技术。知识蒸馏(Knowledge distillation)：将大型教师模型的知识转移到小型学生模型的技术。CLIP嵌入：由CLIP模型生成的表示文本和图像之间关系的向量。AP50：在IoU阈值为0.5时的平均精度，常用于评估实例分割性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放词汇3D实例分割中的效率和准确性问题。现有方法通常将3D实例映射到2D图像再进行分类，但这种方式会受到遮挡引入噪声，同时计算量大导致推理速度慢。这个问题在现实中很重要，因为开放词汇3D实例分割能识别训练中未见过的物体类别，这对自动驾驶、机器人导航等需要处理多样化场景的应用至关重要，而现有方法的速度瓶颈限制了这些技术的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D映射引入噪声和计算效率低。他们提出核心思路是通过知识蒸馏将2D视觉语言模型(如CLIP)的知识转移到3D模型中，使3D模型能直接从点云分类。具体设计包括：1)教师模型使用多视图选择和密度引导掩码完成算法生成高质量2D嵌入；2)学生模型直接生成3D嵌入；3)标签引导蒸馏算法确保知识传递质量。作者借鉴了Mask3D用于3D提议生成、CLIP的表示能力、MaskCLIP++的掩码特征提取以及知识蒸馏技术，但进行了创新整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过标签引导的知识蒸馏，将2D视觉语言模型的知识转移到3D学生模型中，使3D模型能直接从点云提取嵌入并分类，避免2D映射带来的噪声和计算开销。整体流程：1)教师模型阶段：对每个3D实例，选择多视图图像，生成精确掩码，提取高质量2D嵌入；2)学生模型阶段：从点云提取特征，生成3D实例嵌入；3)蒸馏阶段：过滤语义不一致的2D嵌入，通过对比损失和标签损失训练学生模型；4)推理阶段：仅用训练好的3D学生模型直接处理点云，高效生成分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)多视图选择算法：同时考虑可见性和视角多样性，选择代表性图像；2)密度引导掩码完成算法：从稀疏掩码生成精确密集掩码，减少背景噪声；3)标签引导蒸馏算法：过滤语义不一致的嵌入，确保知识质量；4)端到端3D嵌入提取：训练后直接从点云分类，无需2D映射。不同之处：传统方法需将3D映射到2D再分类，易受遮挡影响且计算量大；FOLK直接处理3D数据，避免了噪声问题，推理速度提高了6-152倍，同时保持了高精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出FOLK方法，通过标签引导的知识蒸馏将2D视觉语言模型的知识转移到3D学生模型中，实现了直接从点云进行高效准确的开放词汇3D实例分割，避免了传统2D映射方法的噪声和计算瓶颈问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D instance segmentation seeks to segment and classifyinstances beyond the annotated label space. Existing methods typically map 3Dinstances to 2D RGB-D images, and then employ vision-language models (VLMs) forclassification. However, such a mapping strategy usually introduces noise from2D occlusions and incurs substantial computational and memory costs duringinference, slowing down the inference speed. To address the above problems, wepropose a Fast Open-vocabulary 3D instance segmentation method via Label-guidedKnowledge distillation (FOLK). Our core idea is to design a teacher model thatextracts high-quality instance embeddings and distills its open-vocabularyknowledge into a 3D student model. In this way, during inference, the distilled3D model can directly classify instances from the 3D point cloud, avoidingnoise caused by occlusions and significantly accelerating the inferenceprocess. Specifically, we first design a teacher model to generate a 2D CLIPembedding for each 3D instance, incorporating both visibility and viewpointdiversity, which serves as the learning target for distillation. We thendevelop a 3D student model that directly produces a 3D embedding for each 3Dinstance. During training, we propose a label-guided distillation algorithm todistill open-vocabulary knowledge from label-consistent 2D embeddings into thestudent model. FOLK conducted experiments on the ScanNet200 and Replicadatasets, achieving state-of-the-art performance on the ScanNet200 dataset withan AP50 score of 35.7, while running approximately 6.0x to 152.2x faster thanprevious methods. All codes will be released after the paper is accepted.</description>
      <author>example@mail.com (Hongrui Wu, Zhicheng Gao, Jin Cao, Kelu Yao, Wen Shen, Zhihua Wei)</author>
      <guid isPermaLink="false">2510.08849v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SatDreamer360: Multiview-Consistent Generation of Ground-Level Scenes from Satellite Imagery</title>
      <link>http://arxiv.org/abs/2506.00600v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SatDreamer360的框架，能够从单张卫星图像生成几何上多视角一致的地面全景图，解决了现有方法难以产生多视角一致序列的问题。&lt;h4&gt;背景&lt;/h4&gt;生成多视角一致的360度地面场景卫星影像是一个具有挑战性的任务，在模拟、自主导航和数字孪生城市等领域有广泛应用。现有方法主要专注于合成单个地面全景图，通常依赖高度图或手工制作的投影等辅助输入，难以产生多视角一致的序列。&lt;h4&gt;目的&lt;/h4&gt;提出SatDreamer360框架，从单张卫星图像生成几何上多视角一致的地面全景图，给定预定义的位置轨迹。&lt;h4&gt;方法&lt;/h4&gt;采用三平面表示法编码场景特征；设计基于射线的像素注意力机制从三平面检索特定视角特征；引入全景极线约束注意力模块根据已知相对姿态对齐跨帧特征；扩展VIGOR数据集创建VIGOR++，包含更多地面图像及其姿态标注。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SatDreamer360在卫星到地面对齐和多视角一致性方面都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;SatDreamer360能够有效地从卫星图像生成多视角一致的360度地面场景，解决了卫星与地面图像之间的大视角差异问题。&lt;h4&gt;翻译&lt;/h4&gt;从卫星影像生成多视角一致的360度地面场景是一项具有挑战性的任务，在模拟、自主导航和数字孪生城市等领域有广泛应用。现有方法主要专注于合成单个地面全景图，通常依赖高度图或手工制作的投影等辅助输入，难以产生多视角一致的序列。本文提出SatDreamer360框架，能够从单张卫星图像生成几何上多视角一致的地面全景图，给定预定义的位置轨迹。为解决地面与卫星图像之间的大视角差异问题，我们采用三平面表示法编码场景特征，并设计基于射线的像素注意力机制从三平面中检索特定视角特征。为保持多帧一致性，我们引入全景极线约束注意力模块，根据已知的相对姿态对齐跨帧特征。为支持评估，我们通过增加更多地面图像及其姿态标注扩展了原始VIGOR数据集，创建了VIGOR++数据集。实验表明，SatDreamer360在卫星到地面对齐和多视角一致性方面都优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从卫星图像生成多视角一致的地面全景场景问题。这个问题在现实世界中非常重要，因为它有广泛的应用，包括模拟环境、自动驾驶和数字孪生城市建设。卫星图像覆盖范围广且获取成本低，但与地面视角差异巨大，现有方法难以生成连续、几何一致的地面场景序列，限制了这些应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括依赖辅助输入(如高度图)和难以保证多视角一致性。他们借鉴了三平面表示技术来编码3D场景特征，并从针孔相机的极线约束概念中获得灵感，将其扩展到全景图像。方法设计基于扩散模型，特别是Stable Diffusion 1.5，并添加了两个关键模块：基于射线的像素注意力机制和全景极线约束注意力模块。这些创新使模型能够从单个卫星图像生成连续且几何一致的地面场景序列。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用三平面表示编码卫星图像中的3D场景特征，并通过基于射线的像素注意力机制从三平面中检索视图特定特征，同时利用全景极线约束注意力模块确保多帧之间的一致性。整体流程是：首先将卫星图像转换为三平面表示；然后为每个地面像素定义3D射线并沿射线采样点；接着从三平面中提取这些点的特征；再利用极线约束对齐不同帧的特征；最后通过扩散模型迭代去噪生成连续的地面全景图像序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一框架SatDreamer360，可从单个卫星图像生成连续地面场景；2) 三平面表示编码场景几何，避免依赖高度图；3) 基于射线的像素注意力机制，实现像素级几何感知；4) 全景极线约束注意力模块，确保多视角一致性；5) 新建VIGOR++数据集。相比之前工作，不同之处在于：不需要辅助输入；只需单个卫星图像；能生成多视角一致序列；显式处理几何一致性；支持大规模场景生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SatDreamer360通过创新的三平面表示和极线约束注意力机制，实现了从单个卫星图像生成多视角一致的地面全景场景，为模拟、自动驾驶和数字孪生城市等应用提供了新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating multiview-consistent $360^\circ$ ground-level scenes fromsatellite imagery is a challenging task with broad applications in simulation,autonomous navigation, and digital twin cities. Existing approaches primarilyfocus on synthesizing individual ground-view panoramas, often relying onauxiliary inputs like height maps or handcrafted projections, and struggle toproduce multiview consistent sequences. In this paper, we proposeSatDreamer360, a framework that generates geometrically consistent multi-viewground-level panoramas from a single satellite image, given a predefined posetrajectory. To address the large viewpoint discrepancy between ground andsatellite images, we adopt a triplane representation to encode scene featuresand design a ray-based pixel attention mechanism that retrieves view-specificfeatures from the triplane. To maintain multi-frame consistency, we introduce apanoramic epipolar-constrained attention module that aligns features acrossframes based on known relative poses. To support the evaluation, we introduce{VIGOR++}, a large-scale dataset for generating multi-view ground panoramasfrom a satellite image, by augmenting the original VIGOR dataset with moreground-view images and their pose annotations. Experiments show thatSatDreamer360 outperforms existing methods in both satellite-to-groundalignment and multiview consistency.</description>
      <author>example@mail.com (Xianghui Ze, Beiyi Zhu, Zhenbo Song, Jianfeng Lu, Yujiao Shi)</author>
      <guid isPermaLink="false">2506.00600v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Language-Centric Omnimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.11693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于多模态大语言模型(MLLMs)并通过对比学习(CL)微调的多模态嵌入方法优越性的根本原因，提出了一种以语言为中心的全模态嵌入框架LCO-Emb，并通过实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;最近基于多模态大语言模型(MLLMs)并通过对比学习(CL)微调的多模态嵌入方法显示出有希望的结果，但它们优越性的根本原因尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;探究MLLM方法优越性的根本原因，并基于此提出一种新的嵌入框架，提高多模态表示的性能。&lt;h4&gt;方法&lt;/h4&gt;通过各向异性和核相似性结构的分析，确认MLLM表示中存在潜在的对齐，使CL能够作为一个轻量级的精炼阶段。基于这一见解，提出了一个以语言为中心的全模态嵌入框架LCO-Emb，并在不同骨干网络和基准上进行了广泛实验。&lt;h4&gt;主要发现&lt;/h4&gt;1. MLLM方法的关键优势来自于生成预训练过程中实现的隐式跨模态对齐；2. 提出了表示能力-生成能力缩放定律(GRSL)，表明通过对比精炼获得的表示能力与MLLM的生成能力呈正相关；3. 提供了GRSL的理论解释，正式将MLLM的生成质量与其表示性能的上限联系起来。&lt;h4&gt;结论&lt;/h4&gt;提高生成能力是提升表示质量的有效范式，在CL之前进行持续的生成预训练可以进一步增强模型的嵌入能力潜力。&lt;h4&gt;翻译&lt;/h4&gt;最近利用通过对比学习(CL)微调的多模态大语言模型(MLLMs)的多模态嵌入方法显示出有希望的结果，但它们优越性的根本原因仍未被充分探索。本文认为，基于MLLM方法的关键优势来自于生成预训练过程中实现的隐式跨模态对齐，其中语言解码器学习在共享表示空间中利用多模态信号来生成单模态输出。通过各向异性和核相似性结构的分析，我们经验上确认潜在对齐出现在MLLM表示中，使CL能够作为一个轻量级的精炼阶段。利用这一见解，我们提出了一个以语言为中心的全模态嵌入框架，称为LCO-Emb。在不同骨干网络和基准上的广泛实验证明了其有效性，在各模态上实现了最先进的性能。此外，我们确定了表示能力-生成能力缩放定律(GRSL)，表明通过对比精炼获得的表示能力与MLLM的生成能力呈正相关。这表明提高生成能力是提升表示质量的有效范式。我们提供了GRSL的理论解释，正式将MLLM的生成质量与其表示性能的上限联系起来，并在一个具有挑战性的低资源视觉文档检索任务上验证了这一点，表明在CL之前进行持续的生成预训练可以进一步增强模型嵌入能力的潜力。代码、模型和资源可在 https://github.com/LCO-Embedding/LCO-Embedding 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent multimodal embedding approaches leveraging multimodal large languagemodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promisingresults, yet the underlying reasons behind their superiority remainunderexplored. This work argues that a crucial advantage of MLLM-basedapproaches stems from implicit cross-modal alignment achieved during generativepretraining, where the language decoder learns to exploit multimodal signalswithin a shared representation space for generating unimodal outputs. Throughanalysis of anisotropy and kernel similarity structure, we empirically confirmthat latent alignment emerges within MLLM representations, allowing CL to serveas a lightweight refinement stage. Leveraging this insight, we propose aLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensiveexperiments across diverse backbones and benchmarks demonstrate itseffectiveness, achieving state-of-the-art performance across modalities.Furthermore, we identify a Generation-Representation Scaling Law (GRSL),showing that the representational capabilities gained through contrastiverefinement scales positively with the MLLM's generative capabilities. Thissuggests that improving generative abilities evolves as an effective paradigmfor enhancing representation quality. We provide a theoretical explanation ofGRSL, which formally links the MLLM's generative quality to the upper bound onits representation performance, and validate it on a challenging, low-resourcevisual-document retrieval task, showing that continual generative pretrainingbefore CL can further enhance the potential of a model's embeddingcapabilities. Codes, models, and resources are available athttps://github.com/LCO-Embedding/LCO-Embedding.</description>
      <author>example@mail.com (Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong)</author>
      <guid isPermaLink="false">2510.11693v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation</title>
      <link>http://arxiv.org/abs/2510.11541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于多跳问题检索的新型图表示学习框架，通过多信息级知识图和基于查询的图神经网络解决了RAG系统在处理复杂多跳问题时面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;检索增强生成(RAG)能够通过整合外部知识源增强大语言模型的能力，但在处理需要识别多个知识目标形成综合答案的多跳问题时面临新挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种图表示学习框架，解决多跳问题中现有方法难以理解复杂语义结构和易受噪声影响的问题。&lt;h4&gt;方法&lt;/h4&gt;引入多信息级知识图(Multi-L KG)建模不同信息级别，设计基于查询的图神经网络(QSGNN)进行表示学习，采用层内/层间消息传递机制并由查询引导信息聚合，同时提出两种综合数据生成策略用于预训练QSGNN。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该框架在多跳场景中有效，特别是在高跳问题上改进可达33.8%。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能有效解决多跳问题中的挑战，提高RAG系统在复杂问题上的性能。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成(RAG)已证明其通过整合外部知识源增强大语言模型的能力。然而，需要识别多个知识目标以形成综合答案的多跳问题为RAG系统带来了新的挑战。在多跳设置下，现有方法往往难以完全理解具有复杂语义结构的问题，并且在检索多个信息目标时容易受到无关噪声的影响。为解决这些局限性，我们提出了一种用于多跳问题检索的新型图表示学习框架。我们首先引入多信息级知识图(Multi-L KG)来建模不同信息级别，以更全面地理解多跳问题。基于此，我们设计了基于查询的图神经网络(QSGNN)在Multi-L KG上进行表示学习。QSGNN采用层内/层间消息传递机制，每次信息聚合都由查询引导，这不仅促进了多粒度信息聚合，还显著减少了噪声的影响。为增强其学习鲁棒表示的能力，我们进一步提出了两种综合数据生成策略用于预训练QSGNN。广泛的实验结果证明了我们的框架在多跳场景中的有效性，特别是在高跳问题上改进可达33.8%。代码可在以下网址获取：https://github.com/Jerry2398/QSGNN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-augmented generation (RAG) has demonstrated its ability to enhanceLarge Language Models (LLMs) by integrating external knowledge sources.However, multi-hop questions, which require the identification of multipleknowledge targets to form a synthesized answer, raise new challenges for RAGsystems. Under the multi-hop settings, existing methods often struggle to fullyunderstand the questions with complex semantic structures and are susceptibleto irrelevant noise during the retrieval of multiple information targets. Toaddress these limitations, we propose a novel graph representation learningframework for multi-hop question retrieval. We first introduce aMulti-information Level Knowledge Graph (Multi-L KG) to model variousinformation levels for a more comprehensive understanding of multi-hopquestions. Based on this, we design a Query-Specific Graph Neural Network(QSGNN) for representation learning on the Multi-L KG. QSGNN employsintra/inter-level message passing mechanisms, and in each message passing theinformation aggregation is guided by the query, which not only facilitatesmulti-granular information aggregation but also significantly reduces theimpact of noise. To enhance its ability to learn robust representations, wefurther propose two synthesized data generation strategies for pre-training theQSGNN. Extensive experimental results demonstrate the effectiveness of ourframework in multi-hop scenarios, especially in high-hop questions theimprovement can reach 33.8\%. The code is available at:https://github.com/Jerry2398/QSGNN.</description>
      <author>example@mail.com (Yuchen Yan, Zhihua Liu, Hao Wang, Weiming Li, Xiaoshuai Hao)</author>
      <guid isPermaLink="false">2510.11541v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment</title>
      <link>http://arxiv.org/abs/2510.11369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为RALI的新算法，用于解决基于推理的图像质量评估模型的高能耗和高延迟问题。通过对比学习直接对齐图像与可泛化文本表示，该方法实现了与基于推理的模型相当的泛化性能，同时显著减少了模型参数和推理时间。&lt;h4&gt;背景&lt;/h4&gt;基于强化学习的推理图像质量评估模型表现出出色的泛化能力，但其背后的机制和关键因素尚未被充分探索。此外，这些模型虽然性能优越，但推理能耗和延迟比早期模型高出几个数量级，限制了它们在特定场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在阐明基于推理的IQA模型泛化能力的来源，并提出一种高效的新算法，以减少模型参数和推理时间，同时保持相当的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;通过大量实验验证，研究揭示了多模态大语言模型(MLLMs)通过强化学习训练，利用推理能力将冗余的视觉表示转换为紧凑的、跨域对齐的文本表示，这是泛化能力的来源。基于此，作者提出了RALI算法，采用对比学习直接将图像与通过RL学习到的可泛化文本表示对齐，消除了对推理过程的依赖和加载大语言模型的必要性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过RL训练，MLLMs利用推理能力将冗余的视觉表示转换为紧凑的、跨域对齐的文本表示。2. 这种转换是基于推理的IQA模型泛化能力的来源。3. RALI算法通过对比学习直接对齐图像与可泛化文本表示，消除了推理过程的依赖。4. RALI实现了与基于推理的模型相当的泛化性能，同时只需要不到5%的模型参数和推理时间。&lt;h4&gt;结论&lt;/h4&gt;RALI算法成功地解决了基于推理的IQA模型的高能耗和高延迟问题，通过直接对齐图像与文本表示，显著减少了模型复杂度和推理时间，同时保持了相当的泛化性能，为图像质量评估领域提供了一种高效的新方法。&lt;h4&gt;翻译&lt;/h4&gt;基于强化学习训练的推理图像质量评估模型表现出出色的泛化能力，但其背后的机制和关键驱动因素在当前研究中仍未得到充分探索。此外，尽管这些模型性能优越，但它们的推理能耗和延迟比早期模型高出几个数量级，限制了它们在特定场景中的部署。通过大量实验，本文验证并阐述，通过RL训练，多模态大语言模型利用其推理能力将冗余的视觉表示转换为紧凑的、跨域对齐的文本表示。这种转换正是这些基于推理的IQA模型所表现出的泛化能力的来源。基于这一基本洞察，我们提出了RALI这一新颖算法，它采用对比学习直接将图像与通过RL学习到的可泛化文本表示对齐。这种方法消除了对推理过程的依赖，甚至不需要加载大语言模型。对于质量评分任务，该框架实现了与基于推理的模型相当的泛化性能，同时只需要不到5%的模型参数和推理时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning-based image quality assessment (IQA) models trained throughreinforcement learning (RL) exhibit exceptional generalization, yet theunderlying mechanisms and critical factors driving this capability remainunderexplored in current research. Moreover, despite their superiorperformance, these models incur inference energy usage and latency orders ofmagnitude higher than their earlier counterparts, restricting their deploymentin specific scenarios. Through extensive experiments, this paper verifies andelaborates that through RL training, MLLMs leverage their reasoning capabilityto convert redundant visual representations into compact, cross-domain alignedtext representations. This conversion is precisely the source of thegeneralization exhibited by these reasoning-based IQA models. Building on thisfundamental insight, we propose a novel algorithm, RALI, which employscontrastive learning to directly align images with these generalizable textrepresentations learned by RL. This approach eliminates the reliance onreasoning processes and even obviates the need to load an LLM. For the qualityscoring task, this framework achieves generalization performance comparable toreasoning-based models while requiring less than 5% of their model parametersand inference time.</description>
      <author>example@mail.com (Shijie Zhao, Xuanyu Zhang, Weiqi Li, Junlin Li, Li Zhang, Tianfan Xue, Jian Zhang)</author>
      <guid isPermaLink="false">2510.11369v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data</title>
      <link>http://arxiv.org/abs/2510.11321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 39th Conference on Neural Information Processing Systems  (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种自监督框架，用于学习层次化的操作概念，通过跨模态感官相关性和多级时间抽象捕捉不变的操作模式，无需人工标注。结合跨模态相关网络和多时间尺度预测器，使策略能够专注于可转移的关系模式，同时保持对即时行动和长期目标的意识。实验证明概念增强策略在模拟和实际环境中表现显著提升，学习到的概念类似于人类可解释的操作基元。&lt;h4&gt;背景&lt;/h4&gt;机器人操作中的有效泛化需要能够捕捉环境和任务间不变交互模式的表示。传统的机器人学习方法通常需要大量人工标注，且难以在不同环境和任务间有效迁移。&lt;h4&gt;目的&lt;/h4&gt;开发一种自监督学习方法，使机器人能够学习层次化的操作概念，捕捉跨环境和任务的不变交互模式，无需人工标注，从而提高机器人在复杂场景中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出结合两种主要组件的自监督框架：1) 跨模态相关网络：识别跨感官模态的持久模式；2) 多时间尺度预测器：在不同时间尺度上组织表示层次结构。这种双重结构使策略能够专注于可转移的关系模式，同时保持对即时行动和长期目标的意识。&lt;h4&gt;主要发现&lt;/h4&gt;1) 概念增强的策略在模拟基准测试和实际部署中表现出显著的性能改进；2) 学习到的概念类似于人类可解释的操作基元，尽管没有接受语义监督；3) 该框架能够捕捉跨环境和任务的不变操作模式。&lt;h4&gt;结论&lt;/h4&gt;这项研究不仅推进了对操作表示学习的理解，还为在复杂场景中增强机器人性能提供了实用方法。通过自监督学习层次化操作概念，机器人能够在无需人工标注的情况下实现更好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作中的有效泛化需要能够捕捉环境和任务间不变交互模式的表示。我们提出了一个自监督框架，用于学习层次化的操作概念，这些概念通过跨模态感官相关性和多级时间抽象来编码这些不变模式，无需人工标注。我们的方法结合了跨模态相关网络（识别跨感官模态的持久模式）和多时间尺度预测器（在不同时间尺度上组织表示层次结构）。通过这种双重结构学习到的操作概念，使策略能够专注于可转移的关系模式，同时保持对即时行动和长期目标的意识。在模拟基准测试和实际部署中的经验评估表明，我们的概念增强策略具有显著的性能改进。分析显示，学习到的概念类似于人类可解释的操作基元，尽管没有接受语义监督。这项工作不仅推进了对操作表示学习的理解，还为在复杂场景中增强机器人性能提供了实用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective generalization in robotic manipulation requires representationsthat capture invariant patterns of interaction across environments and tasks.We present a self-supervised framework for learning hierarchical manipulationconcepts that encode these invariant patterns through cross-modal sensorycorrelations and multi-level temporal abstractions without requiring humanannotation. Our approach combines a cross-modal correlation network thatidentifies persistent patterns across sensory modalities with a multi-horizonpredictor that organizes representations hierarchically across temporal scales.Manipulation concepts learned through this dual structure enable policies tofocus on transferable relational patterns while maintaining awareness of bothimmediate actions and longer-term goals. Empirical evaluation across simulatedbenchmarks and real-world deployments demonstrates significant performanceimprovements with our concept-enhanced policies. Analysis reveals that thelearned concepts resemble human-interpretable manipulation primitives despitereceiving no semantic supervision. This work advances both the understanding ofrepresentation learning for manipulation and provides a practical approach toenhancing robotic performance in complex scenarios.</description>
      <author>example@mail.com (Ruizhe Liu, Pei Zhou, Qian Luo, Li Sun, Jun Cen, Yibing Song, Yanchao Yang)</author>
      <guid isPermaLink="false">2510.11321v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Causal Disentanglement Learning for Accurate Anomaly Detection in Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2510.11084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 4 Figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CDRL4AD的因果解缠结表示学习方法，用于在多元时间序列中检测异常并识别因果关系，解决了传统方法无法在不同时间段明确推断因果关系的问题。&lt;h4&gt;背景&lt;/h4&gt;在多元时间序列分析中，数据变量之间的动态交互随时间变化，使因果关系的解释变得复杂。传统方法在无监督设置中假设变量间的统计独立性，而最近的方法通过图表示学习捕获特征相关性，但这些表示无法在不同时间段明确推断因果关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够检测异常并识别其因果关系的方法，特别是在多元时间序列数据中，解决现有方法无法明确推断不同时间段因果关系的问题。&lt;h4&gt;方法&lt;/h4&gt;提出CDRL4AD（用于异常检测的因果解缠结表示学习）方法，设计因果过程作为模型输入（包括时间异质图和因果关系），使表示能够识别不同时间段的因果关系并解缠结潜在变量以推断相应的因果因子。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的实验表明，CDRL4AD在准确性和根本原因分析方面优于最先进的方法；模型分析验证了超参数敏感性和CDRL4AD的时间复杂度；案例研究展示了该方法如何帮助人类专家诊断异常的根本原因。&lt;h4&gt;结论&lt;/h4&gt;CDRL4AD方法有效解决了多元时间序列中异常检测和因果关系推断的挑战，为异常诊断提供了更准确的工具，并能帮助人类专家理解异常的根本原因。&lt;h4&gt;翻译&lt;/h4&gt;解缠结复杂的因果关系对于准确检测异常很重要。在多元时间序列分析中，数据变量之间的动态交互随时间变化，使因果关系的解释变得复杂。传统方法在无监督设置中假设变量间的统计独立性，而最近的方法通过图表示学习捕获特征相关性。然而，这些表示无法在不同时间段明确推断因果关系。为解决这个问题，我们提出了用于异常检测的因果解缠结表示学习（CDRL4AD），用于在多元时间序列中检测异常并识别其因果关系。首先，我们将因果过程设计为模型输入，包括时间异质图和因果关系。其次，我们的表示能够识别不同时间段的因果关系，并解缠结潜在变量以推断相应的因果因子。第三，我们在真实世界数据集上的实验表明，CDRL4AD在准确性和根本原因分析方面优于最先进的方法。第四，我们的模型分析验证了超参数敏感性和CDRL4AD的时间复杂度。最后，我们进行了案例研究，展示我们的方法如何帮助人类专家诊断异常的根本原因。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Disentangling complex causal relationships is important for accuratedetection of anomalies. In multivariate time series analysis, dynamicinteractions among data variables over time complicate the interpretation ofcausal relationships. Traditional approaches assume statistical independencebetween variables in unsupervised settings, whereas recent methods capturefeature correlations through graph representation learning. However, theirrepresentations fail to explicitly infer the causal relationships overdifferent time periods. To solve the problem, we propose Causally DisentangledRepresentation Learning for Anomaly Detection (CDRL4AD) to detect anomalies andidentify their causal relationships in multivariate time series. First, wedesign the causal process as model input, the temporal heterogeneous graph, andcausal relationships. Second, our representation identifies causalrelationships over different time periods and disentangles latent variables toinfer the corresponding causal factors. Third, our experiments on real-worlddatasets demonstrate that CDRL4AD outperforms state-of-the-art methods in termsof accuracy and root cause analysis. Fourth, our model analysis validateshyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conducta case study to show how our approach assists human experts in diagnosing theroot causes of anomalies.</description>
      <author>example@mail.com (Wonah Kim, Jeonghyeon Park, Dongsan Jun, Jungkyu Han, Sejin Chun)</author>
      <guid isPermaLink="false">2510.11084v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction</title>
      <link>http://arxiv.org/abs/2510.11066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了解耦多模态融合(DMF)方法，通过模态增强建模策略实现基于ID的协同表示和多模态表示的细粒度交互，用于用户兴趣建模。DMF构建目标感知特征桥接不同嵌入空间，设计推理优化注意力机制减轻计算瓶颈，并全面整合多模态表示。实验证明DMF有效，已在Lazada平台部署并取得显著业务指标提升。&lt;h4&gt;背景&lt;/h4&gt;现代工业推荐系统通过整合预训练模型的多模态表示到基于ID的点击率预测框架中来提高推荐性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法捕获内容语义和行为信号之间细粒度交互的问题，提高推荐系统的性能。&lt;h4&gt;方法&lt;/h4&gt;提出解耦多模态融合(DMF)方法，包括：1)构建目标感知特征桥接不同嵌入空间的语义差距；2)设计推理优化注意力机制解耦计算；3)结合模态中心和模态增强建模策略下的用户兴趣表示实现全面多模态整合。&lt;h4&gt;主要发现&lt;/h4&gt;在公共和工业数据集上的离线实验证明了DMF的有效性；在Lazada平台部署后，CTCVR提升5.30%，GMV提升7.43%，且计算开销可忽略不计。&lt;h4&gt;结论&lt;/h4&gt;DMF通过模态增强建模策略有效解决了现有方法无法捕获细粒度交互的问题，显著提升了推荐系统性能，且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;现代工业推荐系统通过将预训练模型的多模态表示整合到基于ID的点击率预测框架中来提高推荐性能。然而，现有方法通常采用模态中心建模策略，独立处理基于ID和多模态的嵌入，无法捕获内容语义和行为信号之间的细粒度交互。本文提出了解耦多模态融合(DMF)，它引入了模态增强建模策略，使基于ID的协同表示和多模态表示能够进行细粒度交互，用于用户兴趣建模。具体而言，我们构建目标感知特征来桥接不同嵌入空间之间的语义差距，并将其作为辅助信息来增强用户兴趣建模的效果。此外，我们设计了一种推理优化的注意力机制，在注意力层之前解耦目标感知特征和基于ID的嵌入的计算，从而减轻了引入目标感知特征带来的计算瓶颈。为了实现全面的多模态整合，DMF结合了在模态中心和模态增强建模策略下学习到的用户兴趣表示。在公共和工业数据集上的离线实验证明了DMF的有效性。此外，DMF已被部署在跨境电商平台Lazada的产品推荐系统上，实现了CTCVR提升5.30%和GMV提升7.43%，且计算开销可忽略不计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern industrial recommendation systems improve recommendation performanceby integrating multimodal representations from pre-trained models into ID-basedClick-Through Rate (CTR) prediction frameworks. However, existing approachestypically adopt modality-centric modeling strategies that process ID-based andmultimodal embeddings independently, failing to capture fine-grainedinteractions between content semantics and behavioral signals. In this paper,we propose Decoupled Multimodal Fusion (DMF), which introduces amodality-enriched modeling strategy to enable fine-grained interactions betweenID-based collaborative representations and multimodal representations for userinterest modeling. Specifically, we construct target-aware features to bridgethe semantic gap across different embedding spaces and leverage them as sideinformation to enhance the effectiveness of user interest modeling.Furthermore, we design an inference-optimized attention mechanism thatdecouples the computation of target-aware features and ID-based embeddingsbefore the attention layer, thereby alleviating the computational bottleneckintroduced by incorporating target-aware features. To achieve comprehensivemultimodal integration, DMF combines user interest representations learnedunder the modality-centric and modality-enriched modeling strategies. Offlineexperiments on public and industrial datasets demonstrate the effectiveness ofDMF. Moreover, DMF has been deployed on the product recommendation system ofthe international e-commerce platform Lazada, achieving relative improvementsof 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.</description>
      <author>example@mail.com (Alin Fan, Hanqing Li, Sihan Lu, Jingsong Yuan, Jiandong Zhang)</author>
      <guid isPermaLink="false">2510.11066v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Instruction-aware User Embedding via Synergistic Language and Representation Modeling</title>
      <link>http://arxiv.org/abs/2510.11016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InstructUE是一种指令感知的用户嵌入基础模型，利用大型语言模型生成通用和指令感知的用户表示，通过多编码器架构和对比-自回归训练框架解决了现有方法在跨领域泛化和噪声敏感性方面的问题。&lt;h4&gt;背景&lt;/h4&gt;用户表示建模对个性化应用日益重要，但现有方法在跨领域泛化能力和对噪声行为信号的敏感性方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出InstructUE模型，利用大型语言模型生成通用和指令感知的用户表示，提高用户建模的泛化能力和噪声鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入多编码器架构配备轻量级适配器处理异构数据；提出对比-自回归训练框架，通过UserQA数据集连接语言和表示空间，同时利用自回归学习捕获领域知识和对比学习对齐用户-文本嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;通过现实世界应用的广泛实验，证明InstructUE在用户预测、营销和推荐等多个场景中显著优于现有方法，实现了指令引导的用户信息去噪。&lt;h4&gt;结论&lt;/h4&gt;指令感知的用户建模可有效实现特定场景下用户信息的指令引导去噪，为更具泛化性和鲁棒性的用户表示学习铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;用户表示建模已成为个性化应用中日益重要的环节，然而现有方法在跨领域泛化能力和对噪声行为信号的敏感性方面存在挑战。我们提出了InstructUE，一种指令感知的用户嵌入基础模型，它利用大型语言模型生成通用且具有指令感知能力的用户表示。InstructUE引入了一个多编码器架构，配备轻量级适配器，能够高效处理来自六个不同来源的异构数据，同时保留其结构特征。此外，它提出了一种新颖的对比-自回归训练框架，通过精心策划的UserQA数据集连接语言和表示空间。该对比-自回归训练框架同时利用自回归学习捕获语言空间中的领域知识，以及对比学习对齐表示空间中的用户-文本嵌入，从而增强了用户嵌入的指令感知能力和噪声鲁棒性。通过在现实世界应用中的广泛实验，我们证明InstructUE在用户预测、营销和推荐等多个场景中显著优于现有方法。我们的结果表明，指令感知的用户建模可以在特定场景下有效实现用户信息的指令引导去噪，为更具泛化性和鲁棒性的用户表示学习铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User representation modeling has become increasingly crucial for personalizedapplications, yet existing approaches struggle with generalizability acrossdomains and sensitivity to noisy behavioral signals. We present InstructUE, aninstruction-aware user embedding foundation model that leverages large languagemodels (LLMs) to generate general and instruction-aware user representations.InstructUE introduces a multi-encoder architecture with a lightweight adapterthat efficiently processes heterogeneous data from six different sources whilepreserving their structural characteristics. Additionally, it proposes a novelcontrastive-autoregressive training framework that bridges language andrepresentation spaces through a curated UserQA dataset. Thecontrastive-autoregressive training framework simultaneously leveragesautoregressive learning to capture domain knowledge in language space andcontrastive learning to align user-text embeddings in representation space,thereby enhancing the instruction-awareness and noise-robustness of userembeddings. Through extensive experiments on real-world applications, wedemonstrate that InstructUE significantly outperforms existing methods acrossmultiple domains including user prediction, marketing, and recommendationscenarios. Our results show that instruction-aware user modeling caneffectively achieve instruction-guided denoising of user information inspecific scenarios, paving the way for more generalizable and robust userrepresentation learning.</description>
      <author>example@mail.com (Ziyi Gao, Yike Xu, Jiahao Yuan, Baokun Wang, Jinyong Wen, Xiaotong Lin, Yun Liu, Xing Fu, Yu Cheng, Yongchao Liu, Weiqiang Wang, Zhongle Xie)</author>
      <guid isPermaLink="false">2510.11016v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Unify Variables in Neural Scaling Laws for General Audio Representations via Embedding Effective Rank</title>
      <link>http://arxiv.org/abs/2510.10948v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统探讨了通用音频表示学习中的缩放定律，引入嵌入有效秩（RankMe）作为统一指标，揭示了RankMe与表示质量之间的幂律关系，为音频基础模型的缩放策略提供了理论依据和实践框架。&lt;h4&gt;背景&lt;/h4&gt;缩放定律在计算机视觉和自然语言处理中对模型性能的理解有深远影响，但在通用音频表示学习中的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究通用音频表示学习中的缩放定律，探索如何评估和预测模型性能。&lt;h4&gt;方法&lt;/h4&gt;利用嵌入有效秩（RankMe）作为统一指标，封装各种变量对表示质量的影响，在广泛的超参数空间（包括模型大小、训练数据量、计算预算、架构配置等）中检查缩放行为。&lt;h4&gt;主要发现&lt;/h4&gt;实证研究表明RankMe与表示质量之间存在一致的关系，表明嵌入有效秩可作为评估和预测音频表示学习中模型性能的可靠代理。&lt;h4&gt;结论&lt;/h4&gt;验证了经典缩放原理适用于通用音频领域，为音频基础模型的未来模型缩放策略提供了理论依据和经验上稳健的框架。&lt;h4&gt;翻译&lt;/h4&gt;缩放定律已经深刻地改变了我们在计算机视觉和自然语言处理中对模型性能的理解，但它们在通用音频表示学习中的应用仍然探索不足。一个关键挑战在于通用音频表示的多因素性质——表示质量受到音频长度、嵌入维度、模型深度、模型架构、数据量等多种变量的共同影响，其中许多变量难以分离或用分析方式表达。在这项工作中，我们通过使用嵌入有效秩（RankMe）作为统一指标，系统地研究了通用音频表示的缩放定律，该指标封装了各种变量对表示质量的影响。RankMe实现了对音频嵌入的无标签、信息论量化，使我们能够检查包括模型大小、训练数据量、计算预算、架构配置等在内的广泛超参数空间中的缩放行为。我们的实证发现揭示了RankMe与表示质量之间的一致的幂律关系，表明嵌入有效秩可作为评估和预测音频表示学习中模型性能的可靠代理。这项工作不仅验证了经典缩放原理适用于通用音频领域，还为音频基础模型的未来模型缩放策略提供了理论依据和经验上稳健的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling laws have profoundly shaped our understanding of model performance incomputer vision and natural language processing, yet their application togeneral audio representation learning remains underexplored. A key challengelies in the multifactorial nature of general audiorepresentation-representation quality is jointly influenced by variables suchas audio length, embedding dimensionality, model depth, model architecture,data volume, etc., many of which are difficult to isolate or expressanalytically. In this work, we present a systematic study of scaling laws forgeneral audio representations by utilizing embedding effective rank (RankMe) asa unifying metric that encapsulates the impact of diverse variables onrepresentation quality. RankMe enables a label-free, information-theoreticquantification of audio embeddings, allowing us to examine scaling behaviorsacross a wide hyper-parameter space, including model size, training datavolume, computational budget, architectural configurations, etc. Our empiricalfindings reveal a consistent power-law relationship between RankMe andrepresentation quality, suggesting that embedding effective rank serves as areliable proxy for assessing and predicting model performance in audiorepresentation learning. This work not only validates the applicability ofclassical scaling principles to the general audio domain but also offers atheoretically grounded and empirically robust framework for guiding futuremodel scaling strategies in audio foundation models.</description>
      <author>example@mail.com (Xuyao Deng, Yanjie Sun, Yong Dou, Kele Xu)</author>
      <guid isPermaLink="false">2510.10948v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition</title>
      <link>http://arxiv.org/abs/2510.10927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IJCNN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为GapDNER的新型模型，用于生物医学领域中的不连续命名实体识别，通过关注实体片段间的上下文间隙解决了传统方法中的错误传播和解码歧义问题。&lt;h4&gt;背景&lt;/h4&gt;在生物医学领域，一个命名实体可能由一系列不相邻的标记组成，并与其他实体重叠。先前的方法通过连接实体片段或内部标记来识别不连续实体，但由于跨度或单词组合的多样性，面临着错误传播和解码歧义的挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，作者深入探索不连续实体的结构，并提出一种有效的间隙感知网格标记模型（GapDNER）来提升不连续命名实体识别的性能。&lt;h4&gt;方法&lt;/h4&gt;GapDNER创新性地在实体片段间的上下文间隙上应用表示学习，将上下文间隙视为额外跨度类型，将跨度分类转换为标记对网格标记任务。设计了两个交互组件：内部跨度规则提取模块使用双仿射机制和线性注意力捕获每个跨度的内部规则；跨跨度关系增强模块利用交叉交叉注意力获取不同跨度间的语义关系。在推理阶段，为每个实体片段和上下文间隙分配有向边，使用BFS算法搜索有效路径。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的实验结果表明，GapDNER在不连续NER方面取得了新的最先进性能，并且在识别复杂实体结构方面表现出显著优势。&lt;h4&gt;结论&lt;/h4&gt;GapDNER模型通过创新地处理上下文间隙和设计专门的组件来建模实体间关系，有效解决了不连续命名实体识别中的挑战，显著提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;在生物医学领域，一个命名实体可能由一系列不相邻的标记组成，并与其他实体重叠。先前的方法通过连接实体片段或内部标记来识别不连续实体，但由于跨度或单词组合的多样性，面临着错误传播和解码歧义的挑战。为了解决这些问题，我们深入探索了不连续实体的结构，并提出了一种有效的间隙感知网格标记模型用于不连续命名实体识别，称为GapDNER。我们的GapDNER创新性地在实体片段之间的上下文间隙上应用表示学习，以解决解码歧义并增强不连续NER性能。具体来说，我们将上下文间隙视为额外的跨度类型，并将跨度分类转换为标记对网格标记任务。随后，我们设计了两个交互组件，从内部和跨跨度两个角度全面建模标记对网格特征。内部跨度规则提取模块使用双仿射机制和线性注意力来捕获每个跨度的内部规则，而跨跨度关系增强模块则利用交叉交叉注意力来获取不同跨度之间的语义关系。在实体解码的推理阶段，我们为每个实体片段和上下文间隙分配有向边，然后使用BFS算法搜索网格中从头部到尾部的所有带有实体标签的有效路径。在三个数据集上的实验结果表明，我们的GapDNER在不连续NER方面取得了新的最先进性能，并且在识别复杂实体结构方面表现出显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In biomedical fields, one named entity may consist of a series ofnon-adjacent tokens and overlap with other entities. Previous methods recognizediscontinuous entities by connecting entity fragments or internal tokens, whichface challenges of error propagation and decoding ambiguity due to the widevariety of span or word combinations. To address these issues, we deeplyexplore discontinuous entity structures and propose an effective Gap-aware gridtagging model for Discontinuous Named Entity Recognition, named GapDNER. OurGapDNER innovatively applies representation learning on the context gapsbetween entity fragments to resolve decoding ambiguity and enhancediscontinuous NER performance. Specifically, we treat the context gap as anadditional type of span and convert span classification into a token-pair gridtagging task. Subsequently, we design two interactive components tocomprehensively model token-pair grid features from both intra- and inter-spanperspectives. The intra-span regularity extraction module employs the biaffinemechanism along with linear attention to capture the internal regularity ofeach span, while the inter-span relation enhancement module utilizescriss-cross attention to obtain semantic relations among different spans. Atthe inference stage of entity decoding, we assign a directed edge to eachentity fragment and context gap, then use the BFS algorithm to search for allvalid paths from the head to tail of grids with entity tags. Experimentalresults on three datasets demonstrate that our GapDNER achieves newstate-of-the-art performance on discontinuous NER and exhibits remarkableadvantages in recognizing complex entity structures.</description>
      <author>example@mail.com (Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen)</author>
      <guid isPermaLink="false">2510.10927v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Topological Alignment of Shared Vision-Language Embedding Space</title>
      <link>http://arxiv.org/abs/2510.10889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 5 figures, 19 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ToMCLIP的拓扑感知框架，用于改进多语言视觉语言模型的跨模态对齐，通过拓扑保持约束增强嵌入空间的结构连贯性，提高零样本准确率和多语言检索性能。&lt;h4&gt;背景&lt;/h4&gt;对比视觉语言模型(VLMs)展示了强大的零样本能力，但由于有限的多语言多模态数据，其跨模态对齐偏向英语。现有多语言扩展虽缓解了这一问题，但仅关注实例级对齐，忽略了共享嵌入空间的几何结构。&lt;h4&gt;目的&lt;/h4&gt;解决多语言视觉语言模型中嵌入空间的全局几何结构对齐问题，通过拓扑感知方法提升多语言表示的结构连贯性和性能。&lt;h4&gt;方法&lt;/h4&gt;提出ToMCLIP框架，应用持续同调定义拓扑对齐损失，并利用图稀疏化策略近似持久图，确保在理论误差边界内实现拓扑保持的嵌入空间对齐。&lt;h4&gt;主要发现&lt;/h4&gt;ToMCLIP增强了多语言表示的结构连贯性，在CIFAR-100上提高了零样本准确率，在xFlickr&amp;CO上增强了多语言检索性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的拓扑对齐方法不仅适用于视觉语言模型，还为表示学习中融入拓扑对齐提供了通用方法。&lt;h4&gt;翻译&lt;/h4&gt;对比视觉语言模型(VLMs)已经展示了强大的零样本能力。然而，由于有限的多语言多模态数据，它们的跨模态对齐仍然偏向英语。最近的多语言扩展缓解了这一差距，但强制执行实例级对齐，同时忽略了共享嵌入空间的几何结构。我们通过引入ToMCLIP(多语言CLIP的拓扑对齐)，一种拓扑感知的框架，使用拓扑保持约束对齐嵌入空间，来解决这一问题。所提出的方法应用持续同调来定义拓扑对齐损失，并使用图稀疏化策略近似持久图，具有理论误差边界。这项工作验证了所提出的方法，展示了多语言表示增强的结构连贯性，在CIFAR-100上更高的零样本准确率，以及在xFlickr&amp;CO上更强的多语言检索性能。除了VLMs，所提出的方法为在表示学习中融入拓扑对齐提供了通用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shotcapabilities. However, their cross-modal alignment remains biased towardEnglish due to limited multilingual multimodal data. Recent multilingualextensions have alleviated this gap but enforce instance-level alignment whileneglecting the global geometry of the shared embedding space. We address thisproblem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), atopology-aware framework aligning embedding spaces with topology-preservingconstraints. The proposed method applies persistent homology to define atopological alignment loss and approximates persistence diagram withtheoretical error bounds using graph sparsification strategy. This workvalidates the proposed approach, showing enhanced structural coherence ofmultilingual representations, higher zero-shot accuracy on the CIFAR-100, andstronger multilingual retrieval performance on the xFlickr&amp;CO. Beyond VLMs, theproposed approach provides a general method for incorporating topologicalalignment into representation learning.</description>
      <author>example@mail.com (Junwon You, Dasol Kang, Jae-Hun Jung)</author>
      <guid isPermaLink="false">2510.10889v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning</title>
      <link>http://arxiv.org/abs/2510.10642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UniCoD模型，结合了理解、规划和连续未来表示学习的优势，通过大规模预训练和微调，显著提升了机器人策略学习在多样化任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;构建能够在开放环境中处理多样化任务的全能机器人策略是机器人领域的核心挑战。现有方法通常基于视觉语言理解模型或生成模型，但语义理解和视觉动力学建模对具身机器人都至关重要。&lt;h4&gt;目的&lt;/h4&gt;利用最近出现的统一生成和理解模型的优势，结合理解、规划和连续未来表示学习，提升机器人策略学习的效果。&lt;h4&gt;方法&lt;/h4&gt;提出UniCoD模型，通过在超过100万个互联网规模的 instructional manipulation 视频上进行预训练，获得动态建模高维视觉特征的能力，然后在机器人具身收集的数据上进行微调，学习从预测表征到动作令牌的映射。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在模拟环境和真实世界分布外任务中，分别比基线方法高出9%和12%的性能。&lt;h4&gt;结论&lt;/h4&gt;UniCoD通过结合理解、规划和连续未来表示学习的优势，在机器人策略学习方面取得了显著的性能提升，为构建全能机器人策略提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;构建能够在开放环境中处理多样化任务的全能机器人策略是机器人领域的中心挑战。为了利用大规模预训练的知识，先前的工作通常在视觉语言理解模型(VLMs)或生成模型的基础上构建全能策略。然而，来自视觉语言预训练的语义理解和来自视觉生成预训练的视觉动力学建模对具身机器人都至关重要。最近的统一生成和理解模型通过大规模预训练在理解和生成方面都展示了强大的能力。我们认为机器人策略学习同样可以从理解、规划和连续未来表示学习的综合优势中受益。基于这一见解，我们引入了UniCoD，它通过在超过100万个互联网规模的 instructional manipulation 视频上进行预训练，获得动态建模高维视觉特征的能力。随后，UniCoD在从机器人具身收集的数据上进行微调，实现了从预测表征到动作令牌的映射学习。大量实验表明，我们的方法在模拟环境和真实世界分布外任务中，始终比基线方法高出9%和12%的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building generalist robot policies that can handle diverse tasks inopen-ended environments is a central challenge in robotics. To leverageknowledge from large-scale pretraining, prior work has typically builtgeneralist policies either on top of vision-language understanding models(VLMs) or generative models. However, both semantic understanding fromvision-language pretraining and visual dynamics modeling from visual-generationpretraining are crucial for embodied robots. Recent unified models ofgeneration and understanding have demonstrated strong capabilities in bothcomprehension and generation through large-scale pretraining. We posit thatrobotic policy learning can likewise benefit from the combined strengths ofunderstanding, planning and continuous future representation learning. Buildingon this insight, we introduce UniCoD, which acquires the ability to dynamicallymodel high-dimensional visual features through pretraining on over 1Minternet-scale instructional manipulation videos. Subsequently, UniCoD isfine-tuned on data collected from the robot embodiment, enabling the learningof mappings from predictive representations to action tokens. Extensiveexperiments show our approach consistently outperforms baseline methods interms of 9\% and 12\% across simulation environments and real-worldout-of-distribution tasks.</description>
      <author>example@mail.com (Jianke Zhang, Yucheng Hu, Yanjiang Guo, Xiaoyu Chen, Yichen Liu, Wenna Chen, Chaochao Lu, Jianyu Chen)</author>
      <guid isPermaLink="false">2510.10642v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation</title>
      <link>http://arxiv.org/abs/2510.10604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FusionGen的新型EEG数据生成框架，通过解耦表征学习和特征融合技术解决脑机接口领域的数据稀缺和受试者间变异性问题，显著提高了EEG解码模型的分类准确性。&lt;h4&gt;背景&lt;/h4&gt;脑机接口(BCIs)通过脑电图(EEG)在大脑和外部设备间建立直接通信，应用范围从医疗康复到认知状态评估。然而，基于EEG的BCI受到数据稀缺和显著受试者间变异性的严重限制，阻碍了EEG解码模型在实际环境中的泛化和应用。&lt;h4&gt;目的&lt;/h4&gt;解决EEG数据稀缺和受试者间变异性问题，提高EEG解码模型在实际环境中的泛化能力和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出FusionGen框架，基于解耦表征学习和特征融合技术。通过特征匹配融合模块整合跨试验特征，并结合轻量级特征提取和重建管道，确保在有限数据条件下的数据多样性和可训练性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公开EEG数据集上的实验表明，FusionGen显著优于现有增强技术，在分类准确性方面取得了明显改进。&lt;h4&gt;结论&lt;/h4&gt;FusionGen有效解决了BCI领域的数据稀缺和受试者间变异性挑战，是一种有前景的EEG数据生成方法。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口(BCIs)通过脑电图(EEG)在大脑和外部设备之间建立直接通信途径，其应用范围从医疗康复到认知状态评估。然而，基于EEG的BCI受到数据稀缺和显著受试者间变异性的严重限制，这阻碍了EEG解码模型在实际环境中的泛化能力和适用性。为应对这些挑战，我们提出了FusionGen，一种基于解耦表征学习和特征融合的新型EEG数据生成框架。通过特征匹配融合模块整合跨试验特征，并与轻量级特征提取和重建管道相结合，FusionGen确保了在有限数据条件下的数据多样性和可训练性。在多个公开可用的EEG数据集上进行的大量实验表明，FusionGen显著优于现有的增强技术，在分类准确性方面取得了显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-computer interfaces (BCIs) provide potential for applications rangingfrom medical rehabilitation to cognitive state assessment by establishingdirect communication pathways between the brain and external devices viaelectroencephalography (EEG). However, EEG-based BCIs are severely constrainedby data scarcity and significant inter-subject variability, which hinder thegeneralization and applicability of EEG decoding models in practical settings.To address these challenges, we propose FusionGen, a novel EEG data generationframework based on disentangled representation learning and feature fusion. Byintegrating features across trials through a feature matching fusion module andcombining them with a lightweight feature extraction and reconstructionpipeline, FusionGen ensures both data diversity and trainability under limiteddata constraints. Extensive experiments on multiple publicly available EEGdatasets demonstrate that FusionGen significantly outperforms existingaugmentation techniques, yielding notable improvements in classificationaccuracy.</description>
      <author>example@mail.com (Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu)</author>
      <guid isPermaLink="false">2510.10604v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Self-supervised Contrastive Learning through Supervised Objectives</title>
      <link>http://arxiv.org/abs/2510.10572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at TMLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提供了一种理论视角，将自监督表示学习表述为监督表示学习目标的近似，推导出与流行对比损失相关的损失函数，并引入原型表示偏差和平衡对比损失的概念，以解释和改进自监督学习算法的行为。&lt;h4&gt;背景&lt;/h4&gt;自监督表示学习已经取得了显著的实证成功，但其理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供理论视角，将自监督表示学习表述为监督表示学习目标的近似，深入理解对比损失函数的原理。&lt;h4&gt;方法&lt;/h4&gt;基于自监督表示学习作为监督表示学习目标近似的表述，推导损失函数，引入原型表示偏差和平衡对比损失的概念，并对应到对比学习的既定实践。&lt;h4&gt;主要发现&lt;/h4&gt;原型表示偏差和平衡对比损失的概念有助于解释和改进自监督学习算法的行为，理论框架的组成部分对应于对比学习中的既定实践，平衡正负样本对的交互具有实证效果。&lt;h4&gt;结论&lt;/h4&gt;通过理论推导和实证验证，本文提供了自监督表示学习的理论框架，所有理论证明在附录中提供，代码包含在补充材料中。&lt;h4&gt;翻译&lt;/h4&gt;自监督表示学习已经取得了令人印象深刻的实证成功，但其理论理解仍然有限。在这项工作中，我们通过将自监督表示学习表述为监督表示学习目标的近似，提供了一种理论视角。基于这一表述，我们推导出一个与流行的对比损失（如InfoNCE）密切相关的损失函数，揭示了它们的基本原理。我们的推导自然地引入了原型表示偏差和平衡对比损失的概念，这些概念有助于解释和改进自监督学习算法的行为。我们进一步展示了理论框架的组成部分如何对应于对比学习中的既定实践。最后，我们通过实证验证了平衡正负样本对交互的效果。所有理论证明都在附录中提供，我们的代码包含在补充材料中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised representation learning has achieved impressive empiricalsuccess, yet its theoretical understanding remains limited. In this work, weprovide a theoretical perspective by formulating self-supervised representationlearning as an approximation to supervised representation learning objectives.Based on this formulation, we derive a loss function closely related to popularcontrastive losses such as InfoNCE, offering insight into their underlyingprinciples. Our derivation naturally introduces the concepts of prototyperepresentation bias and a balanced contrastive loss, which help explain andimprove the behavior of self-supervised learning algorithms. We further showhow components of our theoretical framework correspond to established practicesin contrastive learning. Finally, we empirically validate the effect ofbalancing positive and negative pair interactions. All theoretical proofs areprovided in the appendix, and our code is included in the supplementarymaterial.</description>
      <author>example@mail.com (Byeongchan Lee)</author>
      <guid isPermaLink="false">2510.10572v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2510.10556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SICSRec是一种创新的顺序推荐模型，通过自监督表示学习和ID-Content模态对齐解决了内容顺序推荐中的三个关键挑战，在有限交互历史情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐模型通常基于用户历史交互的物品ID捕捉用户偏好，但在交互历史有限时表现不佳。基于内容的顺序推荐利用物品的文本和视觉特征增强偏好学习，但仍面临语义差距、偏好联合建模和表示对齐等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决内容顺序推荐中的三个关键挑战：减少不同内容模态表示间的语义差距；联合建模用户行为偏好和内容偏好；设计有效训练策略对齐ID表示和内容表示。&lt;h4&gt;方法&lt;/h4&gt;提出SICSRec模型，包含：基于LLM的样本构建方法和监督微调方法对齐物品级模态表示；基于Transformer的顺序模型，包括ID模态序列编码器、内容模态序列编码器和混合模态序列解码器；两步训练策略结合内容感知对比学习任务对齐模态表示和ID表示。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共视频流数据集上，SICSRec在NDCG@5上平均比最先进的ID模态顺序推荐器高出8.04%，在NDCG@10上平均高出6.62%。&lt;h4&gt;结论&lt;/h4&gt;SICSRec通过有效对齐ID表示和内容表示，成功解决了内容顺序推荐中的关键挑战，在有限交互历史情况下提供了更优的推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;顺序推荐(SR)模型通常基于历史交互的物品ID来捕捉用户偏好，当交互历史有限时通常表现不佳。基于内容的顺序推荐最近已成为一种有前途的方向，利用物品的文本和视觉特征来增强偏好学习。然而，仍存在三个关键挑战：(i)如何减少不同内容模态表示之间的语义差距；(ii)如何联合建模用户行为偏好和内容偏好；(iii)如何设计有效的训练策略来对齐ID表示和内容表示。为应对这些挑战，我们提出了一种新模型，即带有ID-Content模态对齐的自监督表示学习，名为SICSRec。首先，我们提出了一种基于LLM的样本构建方法，并开发了监督微调方法来对齐物品级模态表示。其次，我们设计了一种新颖的基于Transformer的顺序模型，其中ID模态序列编码器捕捉用户行为偏好，内容模态序列编码器学习用户内容偏好，混合模态序列解码器把握这两种偏好之间的内在关系。第三，我们提出了一个包含内容感知对比学习任务的两步训练策略，用于对齐模态表示和ID表示，从而解耦内容模态依赖和物品协同依赖的训练过程。在四个公共视频流数据集上进行的广泛实验表明，我们的SICSRec在NDCG@5上平均比最先进的ID模态顺序推荐器高出8.04%，在NDCG@10上平均高出6.62%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation (SR) models often capture user preferences based onthe historically interacted item IDs, which usually obtain sub-optimalperformance when the interaction history is limited. Content-based sequentialrecommendation has recently emerged as a promising direction that exploitsitems' textual and visual features to enhance preference learning. However,there are still three key challenges: (i) how to reduce the semantic gapbetween different content modality representations; (ii) how to jointly modeluser behavior preferences and content preferences; and (iii) how to design aneffective training strategy to align ID representations and contentrepresentations. To address these challenges, we propose a novel model,self-supervised representation learning with ID-Content modality alignment,named SICSRec. Firstly, we propose a LLM-driven sample construction method anddevelop a supervised fine-tuning approach to align item-level modalityrepresentations. Secondly, we design a novel Transformer-based sequentialmodel, where an ID-modality sequence encoder captures user behaviorpreferences, a content-modality sequence encoder learns user contentpreferences, and a mix-modality sequence decoder grasps the intrinsicrelationship between these two types of preferences. Thirdly, we propose atwo-step training strategy with a content-aware contrastive learning task toalign modality representations and ID representations, which decouples thetraining process of content modality dependency and item collaborativedependency. Extensive experiments conducted on four public video streamingdatasets demonstrate our SICSRec outperforms the state-of-the-art ID-modalitysequential recommenders and content-modality sequential recommenders by 8.04%on NDCG@5 and 6.62% on NDCD@10 on average, respectively.</description>
      <author>example@mail.com (Donglin Zhou, Weike Pan, Zhong Ming)</author>
      <guid isPermaLink="false">2510.10556v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Unified Open-World Segmentation with Multi-Modal Prompts</title>
      <link>http://arxiv.org/abs/2510.10524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了COSINE，一个统一的开放世界分割模型，整合了开放词汇分割和上下文分割功能，支持多模态提示。&lt;h4&gt;背景&lt;/h4&gt;现有的开放词汇分割和上下文分割方法存在架构差异、不同的学习目标和表示学习策略问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型来解决开放词汇分割和上下文分割的架构和策略不一致问题。&lt;h4&gt;方法&lt;/h4&gt;COSINE利用基础模型提取图像和多模态提示的表示，并通过SegDecoder对齐这些表示、建模交互，生成不同粒度的掩码。&lt;h4&gt;主要发现&lt;/h4&gt;COSINE在开放词汇和上下文分割任务中表现出显著的性能提升，且多模态提示的协同合作相比单模态方法提高了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;COSINE通过统一架构和策略，成功解决了现有开放世界分割方法的局限性，实现了更强大的分割性能。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了COSINE，一个统一的开放世界分割模型，它整合了开放词汇分割和上下文分割功能，并支持多模态提示（例如文本和图像）。COSINE利用基础模型提取输入图像和对应多模态提示的表示，并通过SegDecoder对齐这些表示、建模它们的交互，并获得不同粒度下由输入提示指定的掩码。这样，COSINE克服了先前开放词汇分割和上下文分割管道的架构差异、不同的学习目标和表示学习策略。全面的实验证明COSINE在开放词汇和上下文分割任务中都有显著的性能提升。我们的探索性分析强调，使用视觉和文本提示之间的协同合作相比单模态方法显著提高了泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we present COSINE, a unified open-world segmentation model thatconsolidates open-vocabulary segmentation and in-context segmentation withmulti-modal prompts (e.g., text and image). COSINE exploits foundation modelsto extract representations for an input image and corresponding multi-modalprompts, and a SegDecoder to align these representations, model theirinteraction, and obtain masks specified by input prompts across differentgranularities. In this way, COSINE overcomes architectural discrepancies,divergent learning objectives, and distinct representation learning strategiesof previous pipelines for open-vocabulary segmentation and in-contextsegmentation. Comprehensive experiments demonstrate that COSINE has significantperformance improvements in both open-vocabulary and in-context segmentationtasks. Our exploratory analyses highlight that the synergistic collaborationbetween using visual and textual prompts leads to significantly improvedgeneralization over single-modality approaches.</description>
      <author>example@mail.com (Yang Liu, Yufei Yin, Chenchen Jing, Muzhi Zhu, Hao Chen, Yuling Xi, Bo Feng, Hao Wang, Shiyu Li, Chunhua Shen)</author>
      <guid isPermaLink="false">2510.10524v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes</title>
      <link>http://arxiv.org/abs/2510.10406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Mesh-Gait是一种创新的端到端多模态步态识别框架，通过从2D剪影重建3D热图作为中间表示，有效结合了2D和3D表示的优势，在保持计算效率的同时实现了最先进的识别准确性。&lt;h4&gt;背景&lt;/h4&gt;步态识别是一种利用独特行走模式进行个人识别的生物识别技术，传统方法使用2D表示如剪影或骨架，但在视角变化、遮挡和噪声方面存在困难。结合3D身体形状信息的多模态方法虽能提高鲁棒性，但计算成本高，限制了实时应用的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效解决现有步态识别方法局限性，同时结合2D和3D优势的步态识别框架，提高识别准确性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;Mesh-Gait直接从2D剪影重建3D表示，使用3D热图作为中间表示，在训练过程中逐步重建并提高准确性，通过计算重建的3D关节、虚拟标记和3D网格与真实值之间的损失来确保精确对齐。该方法从剪影和重建的3D热图中提取判别性特征，使网络专注于运动动力学而非无关视觉细节。&lt;h4&gt;主要发现&lt;/h4&gt;Mesh-Gait能够以计算高效的方式捕获空间和结构步态特征，避免了从RGB视频直接进行3D重建的巨大开销，使网络能够专注于运动动力学而非无关的视觉细节。&lt;h4&gt;结论&lt;/h4&gt;大量实验证明Mesh-Gait达到了最先进的准确性，代码将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;步态识别是一种基础的生物识别技术，利用独特的行走模式进行个人识别，通常使用剪影或骨架等二维表示。然而，这些方法往往难以处理视角变化、遮挡和噪声问题。结合三维身体形状信息的多模态方法虽能提高鲁棒性，但计算成本高，限制了其在实时应用中的可行性。为解决这些挑战，我们引入了Mesh-Gait，一种新颖的端到端多模态步态识别框架，直接从二维剪影重建三维表示，有效结合了两种模态的优势。与现有方法相比，直接从三维关节或网格学习三维特征复杂且难以与基于剪影的步态特征融合。为克服这一问题，Mesh-Gait将三维热图重建为中间表示，使模型能够有效捕获三维几何信息，同时保持简单性和计算效率。在训练过程中，中间的三维热图被逐步重建，并在监督学习下变得越来越准确，通过计算重建的三维关节、虚拟标记和三维网格与其对应真实值之间的损失，确保精确的空间对齐和一致的三维结构。Mesh-Gait以计算高效的方式从剪影和重建的三维热图中提取判别性特征。这种设计使模型能够捕获空间和结构步态特征，同时避免了从RGB视频直接进行三维重建的巨大开销，使网络能够专注于运动动力学而非无关的视觉细节。大量实验证明Mesh-Gait达到了最先进的准确性。代码将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决步态识别(gait recognition)中传统2D方法在视角变化、遮挡和环境噪声方面表现不佳，以及多模态3D方法计算成本高、难以实时应用的问题。这个问题在现实中很重要，因为步态识别是一种非接触式生物识别技术，可在远距离识别个人，适用于监控、安全认证和法医分析等场景，但现有方法难以在实际复杂环境中保持高准确率和实时性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了2D方法的局限性和3D方法的计算成本问题，然后提出使用3D热图作为中间表示来桥接2D和3D信息。他们设计了一个双分支架构：一个处理2D轮廓特征，另一个处理重建的3D特征。在训练过程中使用监督学习逐步优化3D热图。该方法借鉴了HRNet作为3D估计器，受到虚拟标记概念的启发，并参考了现有的步态识别方法如GaitSet、GaitPart等，同时使用了多种损失函数的组合进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过3D热图作为中间表示，直接从2D轮廓重建3D信息，结合2D和3D特征进行多模态步态识别，并在推理阶段避免网格重建以提高效率。整体流程：1)输入2D轮廓序列；2)双分支处理：2D分支提取轮廓特征，3D分支使用HRNet估计3D热图；3)从热图重建3D关节、虚拟标记和网格；4)分别从2D轮廓和3D热图提取特征；5)拼接融合2D和3D特征；6)通过时间池化和金字塔池化处理；7)使用全连接层计算嵌入并进行识别；8)应用多种损失函数进行训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一框架直接从2D轮廓重建3D表示，消除复杂多视角需求；2)使用3D热图作为中间表示，便于特征提取和融合；3)监督学习渐进式优化3D热图，确保精确重建；4)推理阶段不需网格重建，计算效率高(比传统方法快72倍)。相比之前工作的不同：传统2D方法难以处理视角变化和遮挡；现有多模态方法需要额外3D重建模型且计算成本高；直接3D特征学习方法处理点云复杂且难以与2D特征融合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Mesh-Gait提出了一种创新的多模态步态识别框架，通过直接从2D轮廓重建3D热图表示，结合了2D和3D信息的优势，显著提高了识别准确率和鲁棒性，同时大幅降低了计算成本，使实时步态识别成为可能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gait recognition, a fundamental biometric technology, leverages uniquewalking patterns for individual identification, typically using 2Drepresentations such as silhouettes or skeletons. However, these methods oftenstruggle with viewpoint variations, occlusions, and noise. Multi-modalapproaches that incorporate 3D body shape information offer improved robustnessbut are computationally expensive, limiting their feasibility for real-timeapplications. To address these challenges, we introduce Mesh-Gait, a novelend-to-end multi-modal gait recognition framework that directly reconstructs 3Drepresentations from 2D silhouettes, effectively combining the strengths ofboth modalities. Compared to existing methods, directly learning 3D featuresfrom 3D joints or meshes is complex and difficult to fuse with silhouette-basedgait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as anintermediate representation, enabling the model to effectively capture 3Dgeometric information while maintaining simplicity and computationalefficiency. During training, the intermediate 3D heatmaps are graduallyreconstructed and become increasingly accurate under supervised learning, wherethe loss is calculated between the reconstructed 3D joints, virtual markers,and 3D meshes and their corresponding ground truth, ensuring precise spatialalignment and consistent 3D structure. Mesh-Gait extracts discriminativefeatures from both silhouettes and reconstructed 3D heatmaps in acomputationally efficient manner. This design enables the model to capturespatial and structural gait characteristics while avoiding the heavy overheadof direct 3D reconstruction from RGB videos, allowing the network to focus onmotion dynamics rather than irrelevant visual details. Extensive experimentsdemonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will bereleased upon acceptance of the paper.</description>
      <author>example@mail.com (Zhao-Yang Wang, Jieneng Chen, Jiang Liu, Yuxiang Guo, Rama Chellappa)</author>
      <guid isPermaLink="false">2510.10406v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Text2Token: Unsupervised Text Representation Learning with Token Target Prediction</title>
      <link>http://arxiv.org/abs/2510.10224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Text2Token的无监督文本表示学习框架，通过token目标预测任务构建高质量的目标token分布，在MTEB v2基准测试上取得了与最先进方法相媲美的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督文本表示学习是自然语言处理的基础任务，对利用网络未标记文本改进搜索和推荐系统至关重要。研究表明，高质量的文本表示与输入文本的关键词对齐，揭示了表示空间和词汇空间之间的潜在联系。&lt;h4&gt;目的&lt;/h4&gt;开发一个无监督生成框架Text2Token，用于文本表示学习，探索表示空间和词汇空间之间的联系，并通过token目标预测任务提升表示学习性能。&lt;h4&gt;方法&lt;/h4&gt;Text2Token框架基于token目标预测任务，利用精心构建的目标token分布作为监督信号。作者确定了两种关键token类别：文本中有意义的token和文本外语义派生的token，并提出了数据驱动和模型派生两种方法来构建合成token目标。&lt;h4&gt;主要发现&lt;/h4&gt;在MTEB v2基准测试上，Text2Token的性能与最先进的无监督对比学习方法LLM2Vec具有竞争力。词汇和表示空间在训练过程中共同优化并趋向最优解。&lt;h4&gt;结论&lt;/h4&gt;Text2Token框架成功探索了表示空间和词汇空间之间的联系，为无监督文本表示学习提供了新思路，证明了通过精心设计的token目标预测任务可以有效提升表示学习性能。&lt;h4&gt;翻译&lt;/h4&gt;无监督文本表示学习(TRL)是自然语言处理的一项基础任务，它有助于利用网络上的未标记文本改进搜索和推荐系统。最近的一项实证研究发现，高质量的表示与输入文本的关键词对齐，揭示了表示空间和词汇空间之间的潜在联系。受这一发现的启发，我们重新审视了生成任务，并开发了一个用于TRL的无监督生成框架Text2Token。该框架基于token目标预测任务，利用精心构建的目标token分布作为监督信号。为了构建高质量的目标token分布，我们分析了与高级嵌入器的token对齐特性，并确定了两种关键token类别：(1)文本中有意义的token和(2)文本外语义派生的token。基于这些见解，我们提出了两种方法——数据驱动和模型派生——从数据或LLM骨干构建合成token目标。在MTEB v2基准测试上的实验表明，Text2Token的性能与最先进的无监督对比学习方法LLM2Vec具有竞争力。我们的进一步分析表明，词汇和表示空间在训练过程中共同优化并趋向最优解，为未来工作提供了新的思路和见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised text representation learning (TRL) is a fundamental task innatural language processing, which is beneficial for improving search andrecommendations with the web's unlabeled texts. A recent empirical study findsthat the high-quality representation aligns with the key token of the inputtext, uncovering the potential connection between representation space andvocabulary space. Inspired by the findings, we revisit the generative tasks anddevelop an unsupervised generative framework for TRL, Text2Token. The frameworkis based on the token target prediction task, utilizing carefully constructedtarget token distribution as supervisory signals. To construct the high-qualitytarget token distribution, we analyze the token-alignment properties withadvanced embedders and identify two essential categories of key tokens: (1) themeaningful tokens in the text and (2) semantically derived tokens beyond thetext. Based on these insights, we propose two methods -- data-driven andmodel-derived -- to construct synthetic token targets from data or the LLMbackbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Tokenachieves performance competitive with the state-of-the-art embedder withunsupervised contrastive learning, LLM2Vec. Our analysis further shows thatvocabulary and representation spaces optimize together and toward the optimumsolution during training, providing new ideas and insights for future work.</description>
      <author>example@mail.com (Ruize An, Richong Zhang, Zhijie Nie, Zhanyu Wu, Yanzhao Zhang, Dingkun Long)</author>
      <guid isPermaLink="false">2510.10224v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</title>
      <link>http://arxiv.org/abs/2510.10102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了PANTHER，一个混合生成-判别框架，用于用户行为建模和表示学习。通过生成式预训练从无标签行为数据中学习可迁移的表示，结合结构化标记化、序列模式识别、统一用户画像嵌入和实时可扩展性等技术，在微信支付的实际应用中取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型能够通过生成式预训练将大量世界知识压缩到紧凑的标记表示中。然而，在建模用户交互历史中的行为知识方面存在局限。用户行为形成独特模态，每个行为（由时间、上下文和交易类型等多维属性定义）构成行为标记，建模这些高基数序列具有挑战性，判别模型在监督有限时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;将生成式预训练扩展到用户行为领域，类似于LLMs从文本中学习的方式，从无标签行为数据中学习可迁移的表示，开发能够实现大规模序列用户表示学习和实时推理的框架。&lt;h4&gt;方法&lt;/h4&gt;提出了PANTHER框架，包含四个主要组件：(1)结构化标记化：将多维交易属性压缩为可解释的词汇表；(2)序列模式识别模块(SPRM)：用于建模周期性交易模式；(3)统一用户画像嵌入：融合静态人口统计信息和动态交易历史；(4)实时可扩展性：通过预训练嵌入的离线缓存实现毫秒级推理。&lt;h4&gt;主要发现&lt;/h4&gt;在微信支付上部署PANTHER后，实现了下一交易预测的HitRate@1提升25.6%，欺诈检测召回率相对提高38.6%。在公共基准测试上显示出强大泛化能力，相比transformer基线最高实现21%的HitRate@1提升。&lt;h4&gt;结论&lt;/h4&gt;PANTHER被确立为一种可扩展、高性能的工业级序列用户行为建模框架，通过结合生成式预训练和判别式建模，有效解决了用户行为建模中的挑战，并在实际应用中取得了显著效果。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已经证明，生成式预训练可以将大量世界知识压缩到紧凑的标记表示中。虽然LLMs包含广泛的世界知识，但在建模用户交互历史中包含的行为知识方面仍然有限。用户行为形成一种独特的模态，其中每个行为（由时间、上下文和交易类型等多维属性定义）构成一个行为标记。建模这些高基数序列具有挑战性，判别模型在监督有限的情况下往往表现不佳。为了填补这一空白，我们将生成式预训练扩展到用户行为，类似于LLMs从文本中学习的方式，从无标签行为数据中学习可迁移的表示。我们提出了PANTHER，一个混合生成-判别框架，统一了用户行为预训练和下游适应，实现了大规模序列用户表示学习和实时推理。PANTHER引入了：(1)结构化标记化，将多维交易属性压缩为可解释的词汇表；(2)序列模式识别模块(SPRM)，用于建模周期性交易模式；(3)统一用户画像嵌入，融合静态人口统计信息和动态交易历史；(4)实时可扩展性，通过预训练嵌入的离线缓存实现毫秒级推理。在微信支付上全面部署和在线运行后，PANTHER相比基线实现了下一交易预测HitRate@1提升25.6%，欺诈检测召回率相对提高38.6%。在公共基准上的跨领域评估显示出强大的泛化能力，相比transformer基线最高实现21%的HitRate@1提升，确立了PANTHER作为工业级序列用户行为建模的可扩展、高性能框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have shown that generative pretraining candistill vast world knowledge into compact token representations. While LLMsencapsulate extensive world knowledge, they remain limited in modeling thebehavioral knowledge contained within user interaction histories. User behaviorforms a distinct modality, where each action, defined by multi-dimensionalattributes such as time, context, and transaction type, constitutes abehavioral token. Modeling these high-cardinality sequences is challenging, anddiscriminative models often falter under limited supervision. To bridge thisgap, we extend generative pretraining to user behavior, learning transferablerepresentations from unlabeled behavioral data analogous to how LLMs learn fromtext. We present PANTHER, a hybrid generative-discriminative framework thatunifies user behavior pretraining and downstream adaptation, enablinglarge-scale sequential user representation learning and real-time inference.PANTHER introduces: (1) Structured Tokenization to compress multi-dimensionaltransaction attributes into an interpretable vocabulary; (2) Sequence PatternRecognition Module (SPRM) for modeling periodic transaction motifs; (3) aUnified User-Profile Embedding that fuses static demographics with dynamictransaction histories; and (4) Real-time scalability enabled by offline cachingof pretrained embeddings for millisecond-level inference. Fully deployed andoperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost innext-transaction prediction HitRate@1 and a 38.6 percent relative improvementin fraud detection recall over baselines. Cross-domain evaluations on publicbenchmarks show strong generalization, achieving up to 21 percent HitRate@1gains over transformer baselines, establishing PANTHER as a scalable,high-performance framework for industrial sequential user behavior modeling.</description>
      <author>example@mail.com (Guilin Li, Yun Zhang, Xiuyuan Chen, Chengqi Li, Bo Wang, Linghe Kong, Wenjia Wang, Weiran Huang, Matthias Hwai Yong Tan)</author>
      <guid isPermaLink="false">2510.10102v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Cooperative Pseudo Labeling for Unsupervised Federated Classification</title>
      <link>http://arxiv.org/abs/2510.10100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FedCoPL（联邦合作伪标签）的新方法，首次将无监督联邦学习(UFL)扩展到分类问题，利用CLIP模型的零样本预测能力，通过客户端上传伪标签分布和服务器重新分配来解决类别不平衡问题，并引入部分提示聚合协议促进协作和个性化。&lt;h4&gt;背景&lt;/h4&gt;无监督联邦学习(UFL)旨在分布式客户端之间协作训练全局模型而不共享数据或标签信息。之前的UFL工作主要集中在表示学习和聚类任务上。视觉语言模型(如CLIP)因其强大的零样本预测能力而受到广泛关注，使UFL范式下的分类问题成为可能，但这一领域仍 largely未被探索。&lt;h4&gt;目的&lt;/h4&gt;将UFL扩展到分类问题，利用CLIP模型解决UFL中的分类挑战，并提出一种新的联邦学习方法来有效处理此类问题。&lt;h4&gt;方法&lt;/h4&gt;提出了FedCoPL方法：客户端估计并上传伪标签分布，服务器调整并重新分配以避免类别不平衡；引入部分提示聚合协议实现有效协作和个性化，其中视觉提示在服务器端聚合，文本提示保留在本地。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，FedCoPL与基线方法相比具有优越性能，成功解决了UFL范式下的分类问题。&lt;h4&gt;结论&lt;/h4&gt;FedCoPL成功将UFL扩展到分类问题，通过伪标签分布的协作和部分提示聚合，实现了有效的联邦学习，为UFL范式下的分类问题提供了新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;无监督联邦学习(UFL)旨在分布式客户端之间协作训练全局模型，而不共享数据或访问标签信息。之前的UFL工作主要集中在表示学习和聚类任务上。最近，视觉语言模型（如CLIP）因其强大的零样本预测能力而受到广泛关注。利用这一进展，之前在UFL范式下被认为不可行的分类问题现在呈现出有希望的新机会，但仍然 largely未被探索。在本文中，我们首次将UFL扩展到使用CLIP的分类问题，并提出了一种新方法，联邦合作伪标签(FedCoPL)。具体来说，客户端估计并上传其伪标签分布，服务器调整并重新分配它们以避免类别之间的全局不平衡。此外，我们引入了部分提示聚合协议以实现有效的协作和个性化。特别是，包含通用图像特征的视觉提示在服务器端聚合，而编码个性化知识的文本提示则保留在本地。大量实验证明了我们的FedCoPL与基线方法相比的优越性能。我们的代码可在https://github.com/krumpguo/FedCoPL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised Federated Learning (UFL) aims to collaboratively train a globalmodel across distributed clients without sharing data or accessing labelinformation. Previous UFL works have predominantly focused on representationlearning and clustering tasks. Recently, vision language models (e.g., CLIP)have gained significant attention for their powerful zero-shot predictioncapabilities. Leveraging this advancement, classification problems that werepreviously infeasible under the UFL paradigm now present promising newopportunities, yet remain largely unexplored. In this paper, we extend UFL tothe classification problem with CLIP for the first time and propose a novelmethod, \underline{\textbf{Fed}}erated \underline{\textbf{Co}}operative\underline{\textbf{P}}seudo \underline{\textbf{L}}abeling (\textbf{FedCoPL}).Specifically, clients estimate and upload their pseudo label distribution, andthe server adjusts and redistributes them to avoid global imbalance amongclasses. Moreover, we introduce a partial prompt aggregation protocol foreffective collaboration and personalization. In particular, visual promptscontaining general image features are aggregated at the server, while textprompts encoding personalized knowledge are retained locally. Extensiveexperiments demonstrate the superior performance of our FedCoPL compared tobaseline methods. Our code is available at\href{https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}.</description>
      <author>example@mail.com (Kuangpu Guo, Lijun Sheng, Yongcan Yu, Jian Liang, Zilei Wang, Ran He)</author>
      <guid isPermaLink="false">2510.10100v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling</title>
      <link>http://arxiv.org/abs/2510.10060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Translution的新型操作，它结合了自注意力和卷积的优势，能够在计算机视觉和自然语言处理任务上实现更高的准确性。&lt;h4&gt;背景&lt;/h4&gt;在建模数据时，现有方法如自注意力和卷积各有优缺点。自注意力可以自适应识别相关元素，但依赖绝对位置嵌入；卷积以相对方式编码元素，但固定的核大小限制了其自适应选择能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一自注意力自适应识别能力和卷积相对编码优势的操作，同时解决参数数量过多的问题。&lt;h4&gt;方法&lt;/h4&gt;提出Translution操作，并结合其轻量级变体α-Translution，以减少参数数量，使其适合实际应用。&lt;h4&gt;主要发现&lt;/h4&gt;Translution（包括α-Translution）在计算机视觉和自然语言处理任务上实现了比自注意力更高的准确性，同时保持了计算效率。&lt;h4&gt;结论&lt;/h4&gt;Translution操作成功统一了自注意力和卷积的优势，为数据建模提供了新的有效方法，其轻量级变体使其能够在实际应用中部署。&lt;h4&gt;翻译&lt;/h4&gt;在建模给定类型的数据时，我们认为它涉及两个关键方面：1)识别与中心元素相关的元素（如卷积感受野中的图像像素）或与查询元素相关的元素（如自注意力中的文本单词），以及2)有效编码这些标记。自注意力可以自适应地识别这些元素，但依赖于绝对位置嵌入来进行结构表示学习。相比之下，卷积以相对方式编码元素，但其固定的核大小限制了它们自适应选择相关元素的能力。在本文中，我们引入了Translution，这是一种统一了自注意力自适应识别能力和卷积相对编码优势的操作。然而，这种整合导致参数数量大幅增加，超过了大多数现有计算资源的能力。因此，我们提出了Translution的轻量级变体，称为α-Translution。在计算机视觉和自然语言处理任务上的实验表明，Translution（包括α-Translution）实现了比自注意力更高的准确性。代码可在https://github.com/hehefan/Translution获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When modeling a given type of data, we consider it to involve two keyaspects: 1) identifying relevant elements (e.g., image pixels or textual words)to a central element, as in a convolutional receptive field, or to a queryelement, as in self-attention, and 2) encoding these tokens effectively.Self-attention can adaptively identify these elements but relies on absolutepositional embedding for structural representation learning. In contrast,convolution encodes elements in a relative manner, yet their fixed kernel sizelimits their ability to adaptively select the relevant elements. In this paper,we introduce Translution, an operation that unifies the adaptive identificationcapability of self-attention and the relative encoding advantage ofconvolution. However, this integration leads to a substantial increase in thenumber of parameters, exceeding most currently available computationalresources. Therefore, we propose a lightweight variant of Translution, named{\alpha}-Translution. Experiments on computer vision and natural languageprocessing tasks show that Translution (including {\alpha}-Translution)achieves superior accuracy compared to self-attention. The code is available athttps://github.com/hehefan/Translution.</description>
      <author>example@mail.com (Hehe Fan, Yi Yang, Mohan Kankanhalli, Fei Wu)</author>
      <guid isPermaLink="false">2510.10060v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.09894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AETHER是一个轻量级框架，通过兴趣点(POIs)引导的多模态对齐，将AlphaEarth模型扩展到以人为中心的urban分析，在土地使用分类和社会经济映射方面取得显著改进。&lt;h4&gt;背景&lt;/h4&gt;通用空间表示对构建可迁移的地理空间基础模型(GFMs)至关重要。AlphaEarthFoundation(AE)代表了地球表面全球统一表示的重要进展，但它主要编码物理和光谱模式，难以捕捉城市的功能和社会经济维度。&lt;h4&gt;目的&lt;/h4&gt;提出AETHER框架，通过兴趣点(POIs)引导的多模态对齐，使AlphaEarth适应以人为中心的urban分析，丰富其物理特征与语义线索。&lt;h4&gt;方法&lt;/h4&gt;AETHER将AE嵌入与POIs的文本表示对齐，用关于城市功能和社会经济语境的语义线索丰富基于物理的EO特征。它构建在预训练的AE之上，利用轻量级多模态对齐来丰富以人为中心的语义，同时保持计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;在大伦敦地区，AETHER相对于AE基线实现了一致的改进：土地使用分类F1相对提高7.2%，社会经济映射的Kullback-Leibler散度相对减少23.6%。&lt;h4&gt;结论&lt;/h4&gt;通过将地球观测数据与以人为中心的语义相结合，AETHER推进了地理空间基础模型向通用城市表示发展，这些表示同时整合物理形态和功能意义。&lt;h4&gt;翻译&lt;/h4&gt;通用空间表示对于构建可迁移的地理空间基础模型(GFMs)至关重要。其中，AlphaEarthFoundation(AE)代表了地球表面全球统一表示的重要进展，它从多源地球观测(EO)数据中学习10米嵌入，捕捉不同景观中丰富的物理和环境模式。然而，这类EO驱动的表示在捕捉城市的功能和社会经济维度方面仍然有限，因为它们主要编码物理和光谱模式，而不是人类活动或空间功能。我们提出了AETHER(AlphaEarth-POI Enriched Representation Learning)，这是一个轻量级框架，通过兴趣点(POIs)引导的多模态对齐，使AlphaEarth适应以人为中心的urban分析。AETHER将AE嵌入与POIs的文本表示对齐，用关于城市功能和社会经济语境的语义线索丰富基于物理的EO特征。在大伦敦地区，AETHER相对于AE基线实现了一致的改进，土地使用分类F1相对提高7.2%，社会经济映射的Kullback-Leibler散度相对减少23.6%。构建在预训练的AE之上，AETHER利用轻量级多模态对齐来丰富它以人为中心的语义，同时保持计算效率和对城市应用的可扩展性。通过将EO与以人为中心的语义相结合，AETHER推进了地理空间基础模型向通用城市表示发展，这些表示同时整合物理形态和功能意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose spatial representations are essential for buildingtransferable geospatial foundation models (GFMs). Among them, the AlphaEarthFoundation (AE) represents a major step toward a global, unified representationof the Earth's surface, learning 10-meter embeddings from multi-source EarthObservation (EO) data that capture rich physical and environmental patternsacross diverse landscapes. However, such EO-driven representations remainlimited in capturing the functional and socioeconomic dimensions of cities, asthey primarily encode physical and spectral patterns rather than humanactivities or spatial functions. We propose AETHER (AlphaEarth-POI EnrichedRepresentation Learning), a lightweight framework that adapts AlphaEarth tohuman-centered urban analysis through multimodal alignment guided by Points ofInterest (POIs). AETHER aligns AE embeddings with textual representations ofPOIs, enriching physically grounded EO features with semantic cues about urbanfunctions and socioeconomic contexts. In Greater London, AETHER achievesconsistent gains over the AE baseline, with a 7.2% relative improvement inland-use classification F1 and a 23.6% relative reduction in Kullback-Leiblerdivergence for socioeconomic mapping. Built upon pretrained AE, AETHERleverages a lightweight multimodal alignment to enrich it with human-centeredsemantics while remaining computationally efficient and scalable for urbanapplications. By coupling EO with human-centered semantics, it advancesgeospatial foundation models toward general-purpose urban representations thatintegrate both physical form and functional meaning.</description>
      <author>example@mail.com (Junyuan Liu, Quan Qin, Guangsheng Dong, Xinglei Wang, Jiazhuang Feng, Zichao Zeng, Tao Cheng)</author>
      <guid isPermaLink="false">2510.09894v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>TAWRMAC: A Novel Dynamic Graph Representation Learning Method</title>
      <link>http://arxiv.org/abs/2510.09884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TAWRMAC框架，通过整合带重启的临时匿名游走、内存增强和邻居共现嵌入技术，解决了动态图表示学习中的三个关键挑战：嵌入过时、上下文感知不足和结构动态捕捉不充分。该方法在动态链接预测和节点分类任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;动态图表示学习对于分析社交网络、推荐系统和交通分析等领域中的演化网络至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有连续时间动态图表示学习方法面临的三个关键挑战：节点嵌入过时、邻域相关性捕获不足以及结构动态捕捉不充分。&lt;h4&gt;方法&lt;/h4&gt;提出TAWRMAC框架，整合带重启的临时匿名游走、内存增强GNN和邻居共现嵌入技术。通过固定时间编码的内存增强GNN提高嵌入稳定性，通过明确捕获邻居相关性改善上下文表示，通过区分重复交互节点和形成新连接的节点来更好地捕获结构动态。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验表明，TAWRMAC在三种不同的负采样策略下，无论是在归纳还是直推设置中的动态链接预测和节点分类任务中，都始终优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;TAWRMAC通过提供稳定、可推广和上下文感知的嵌入，推进了连续时间动态图学习的最新技术水平。&lt;h4&gt;翻译&lt;/h4&gt;动态图表示学习已成为分析社交网络分析、推荐系统和交通分析等领域中演化网络的关键技术。然而，现有的连续时间方法面临三个关键挑战：(1)一些方法仅依赖节点特定内存而未有效整合邻居节点信息，导致嵌入过时；(2)大多数未能明确捕获节点邻域间的相关性，限制了上下文感知能力；(3)许多方法在缺乏丰富链接属性的情况下无法完全捕捉演化图的结构动态。为解决这些局限性，我们引入了TAWRMAC——一个整合了带重启的临时匿名游走、内存增强和邻居共现嵌入的新颖框架。TAWRMAC通过具有固定时间编码的内存增强GNN提高嵌入稳定性，并通过明确捕获邻居相关性来改善上下文表示。此外，其带重启的临时匿名游走机制区分了展示重复交互的节点和形成超出其直接邻域新连接的节点。这种方法能更好地捕获结构动态并支持强归纳学习。在多个基准数据集上的广泛实验表明，TAWRMAC在三种不同负采样策略下，无论是在归纳还是直推设置中的动态链接预测和节点分类任务中，都始终优于最先进的方法。通过提供稳定、可推广和上下文感知的嵌入，TAWRMAC推进了连续时间动态图学习的最新技术水平。代码可在https://anonymous.4open.science/r/tawrmac-A253获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic graph representation learning has become essential for analyzingevolving networks in domains such as social network analysis, recommendationsystems, and traffic analysis. However, existing continuous-time methods facethree key challenges: (1) some methods depend solely on node-specific memorywithout effectively incorporating information from neighboring nodes, resultingin embedding staleness; (2) most fail to explicitly capture correlationsbetween node neighborhoods, limiting contextual awareness; and (3) many fail tofully capture the structural dynamics of evolving graphs, especially in absenceof rich link attributes. To address these limitations, we introduce TAWRMAC-anovel framework that integrates Temporal Anonymous Walks with Restart, MemoryAugmentation, and Neighbor Co-occurrence embedding. TAWRMAC enhances embeddingstability through a memory-augmented GNN with fixedtime encoding and improvescontextual representation by explicitly capturing neighbor correlations.Additionally, its Temporal Anonymous Walks with Restart mechanism distinguishesbetween nodes exhibiting repetitive interactions and those forming newconnections beyond their immediate neighborhood. This approach capturesstructural dynamics better and supports strong inductive learning. Extensiveexperiments on multiple benchmark datasets demonstrate that TAWRMACconsistently outperforms state-of-the-art methods in dynamic link predictionand node classification under both transductive and inductive settings acrossthree different negative sampling strategies. By providing stable,generalizable, and context-aware embeddings, TAWRMAC advances the state of theart in continuous-time dynamic graph learning. The code is available athttps://anonymous.4open.science/r/tawrmac-A253 .</description>
      <author>example@mail.com (Soheila Farokhi, Xiaojun Qi, Hamid Karimi)</author>
      <guid isPermaLink="false">2510.09884v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Lifting as Latent-Space Regularization for Continuous-Time Flow Models in AI Systems</title>
      <link>http://arxiv.org/abs/2510.09805v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure, 1 table, 1 algorithm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对连续时间动力系统的自适应时间重参数化的隐空间公式，称为时间提升方法。&lt;h4&gt;背景&lt;/h4&gt;连续时间动力系统在处理近奇异行为时存在挑战，特别是在湍流等复杂系统中。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来规范底层流的近奇异行为，同时保持其守恒定律，使轨迹变得全局平滑。&lt;h4&gt;方法&lt;/h4&gt;引入一个平滑单调映射作为时间提升操作符，可以看作是连续时间归一化或时间扭曲算子。&lt;h4&gt;主要发现&lt;/h4&gt;在提升坐标中，不可压缩Navier-Stokes方程在环面上的轨迹变得全局平滑，可以稳定物理信息神经网络和其他AI系统中使用的潜在流架构。&lt;h4&gt;结论&lt;/h4&gt;该框架将解析正则性理论与用于刚性或湍流过程的表示学习方法联系起来。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了连续时间动力系统的自适应时间重参数化的隐空间公式。这种方法称为时间提升，引入了一个平滑单调映射，它可以规范底层流的近奇异行为，同时保持其守恒定律。在提升坐标中，如环面上不可压缩Navier-Stokes方程的轨迹变得全局平滑。从机器学习动力学的角度来看，时间提升作为连续时间归一化或时间扭曲算子，可以稳定物理信息神经网络和其他AI系统中使用的潜在流架构。该框架将解析正则性理论与用于处理刚性或湍流过程的表示学习方法联系起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a latent-space formulation of adaptive temporal reparametrizationfor continuous-time dynamical systems. The method, called *temporal lifting*,introduces a smooth monotone mapping $t \mapsto \tau(t)$ that regularizesnear-singular behavior of the underlying flow while preserving its conservationlaws. In the lifted coordinate, trajectories such as those of theincompressible Navier-Stokes equations on the torus $\mathbb{T}^3$ becomeglobally smooth. From the standpoint of machine-learning dynamics, temporallifting acts as a continuous-time normalization or time-warping operator thatcan stabilize physics-informed neural networks and other latent-flowarchitectures used in AI systems. The framework links analytic regularitytheory with representation-learning methods for stiff or turbulent processes.</description>
      <author>example@mail.com (Jeffrey Camlin)</author>
      <guid isPermaLink="false">2510.09805v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Combined Representation and Generation with Diffusive State Predictive Information Bottleneck</title>
      <link>http://arxiv.org/abs/2510.09784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Diffusive State Predictive Information Bottleneck (D-SPIB)的新方法，结合时间延迟信息瓶颈和扩散模型，用于分子科学中的生成建模，实现表征学习和生成目标的平衡。&lt;h4&gt;背景&lt;/h4&gt;生成建模在高维空间中变得日益数据密集型，而在分子科学中，数据收集成本高且重要事件稀少，因此压缩到低维流形对各种下游任务（包括生成）特别重要。&lt;h4&gt;目的&lt;/h4&gt;结合时间延迟信息瓶颈和扩散模型，在一个联合训练目标中实现表征学习和生成目标的平衡，构建灵活的架构，并学习热力学的连贯内部表征。&lt;h4&gt;方法&lt;/h4&gt;将时间延迟信息瓶颈与扩散模型结合在一个联合训练目标中，创建名为D-SPIB的协议，使模型能够结合来自不同分子模拟轨迹的温度信息。&lt;h4&gt;主要发现&lt;/h4&gt;D-SPIB能够在表征学习和生成目标之间取得平衡，且模型能够学习热力学的连贯且有用的内部表征。&lt;h4&gt;结论&lt;/h4&gt;在多个分子任务上对D-SPIB进行了基准测试，展示了其探索训练集外物理条件的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在高维空间中，生成建模变得越来越数据密集型。在分子科学领域，数据收集成本高昂且重要事件稀少，压缩到低维流形对各种下游任务（包括生成）尤为重要。我们将一种旨在表征分子重要表示的时间延迟信息瓶颈与扩散模型结合在一个联合训练目标中。由此产生的协议，我们称之为扩散状态预测信息瓶颈，能够在一种灵活的架构中平衡表征学习和生成目标。此外，该模型能够结合来自不同分子模拟轨迹的温度信息，学习热力学的连贯且有用的内部表征。我们在多个分子任务上对D-SPIB进行了基准测试，展示了其探索训练集外物理条件的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling becomes increasingly data-intensive in high-dimensionalspaces. In molecular science, where data collection is expensive and importantevents are rare, compression to lower-dimensional manifolds is especiallyimportant for various downstream tasks, including generation. We combine atime-lagged information bottleneck designed to characterize molecular importantrepresentations and a diffusion model in one joint training objective. Theresulting protocol, which we term Diffusive State Predictive InformationBottleneck (D-SPIB), enables the balancing of representation learning andgeneration aims in one flexible architecture. Additionally, the model iscapable of combining temperature information from different molecularsimulation trajectories to learn a coherent and useful internal representationof thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcaseits potential for exploring physical conditions outside the training set.</description>
      <author>example@mail.com (Richard John, Yunrui Qiu, Lukas Herron, Pratyush Tiwary)</author>
      <guid isPermaLink="false">2510.09784v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network</title>
      <link>http://arxiv.org/abs/2510.09767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HeSRN是一种新型异构图表示学习网络，通过槽感知结构和基于保留的编码器解决了图变换器计算复杂度高和无法有效建模异构语义的问题，在节点分类任务上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;图变换器通过自注意力机制在图表示学习中取得了显著进展，但其二次方计算复杂度和无法有效建模异构语义严重限制了其在真实世界异构图上的可扩展性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出HeSRN，一种新型异构图槽感知保留网络，用于高效且表达性强的异构图表示学习，解决图变换器的计算复杂度和异构语义建模问题。&lt;h4&gt;方法&lt;/h4&gt;HeSRN引入了槽感知结构编码器，通过将异构特征投影到独立槽并使用槽归一化和基于保留的融合来分离节点类型语义；用基于保留的编码器取代自注意力机制，在线性时间复杂度内建模依赖关系；采用异构保留编码器通过多尺度保留层联合捕获局部结构信号和全局异构语义。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界异构图数据集上的实验表明，HeSRN在节点分类任务上始终优于最先进的异构图神经网络和图变换器基线，同时具有显著更低的计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;HeSRN通过创新的槽感知结构和基于保留的编码机制，有效解决了图变换器在异构图表示学习中的局限性，实现了高效且表达性强的学习性能。&lt;h4&gt;翻译&lt;/h4&gt;图变换器最近通过自注意力机制捕获长距离依赖关系，在图表示学习中取得了显著进展。然而，它们的二次方计算复杂度和无法有效建模异构语义严重限制了它们在真实世界异构图上的可扩展性和泛化能力。为了解决这些问题，我们提出了HeSRN，一种用于高效且表达性强的异构图表示学习的新型异构槽感知保留网络。HeSRN引入了一种槽感知结构编码器，通过将异构特征投影到独立槽并通过槽归一化和基于保留的融合来对齐它们的分布，从而显式地分离节点类型语义，有效缓解了先前基于Transformer模型中强制特征空间统一引起的语义纠缠。此外，我们用基于保留的编码器取代了自注意力机制，该编码器在线性时间复杂度内建模结构和上下文依赖关系，同时保持强大的表达能力。进一步采用异构保留编码器通过多尺度保留层联合捕获局部结构信号和全局异构语义。在四个真实世界异构图数据集上的大量实验表明，HeSRN在节点分类任务上始终优于最先进的异构图神经网络和图变换器基线，以显著更低的计算复杂度实现了更高的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers have recently achieved remarkable progress in graphrepresentation learning by capturing long-range dependencies throughself-attention. However, their quadratic computational complexity and inabilityto effectively model heterogeneous semantics severely limit their scalabilityand generalization on real-world heterogeneous graphs. To address these issues,we propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network forefficient and expressive heterogeneous graph representation learning. HeSRNintroduces a slot-aware structure encoder that explicitly disentanglesnode-type semantics by projecting heterogeneous features into independent slotsand aligning their distributions through slot normalization and retention-basedfusion, effectively mitigating the semantic entanglement caused by forcedfeature-space unification in previous Transformer-based models. Furthermore, wereplace the self-attention mechanism with a retention-based encoder, whichmodels structural and contextual dependencies in linear time complexity whilemaintaining strong expressive power. A heterogeneous retentive encoder isfurther employed to jointly capture both local structural signals and globalheterogeneous semantics through multi-scale retention layers. Extensiveexperiments on four real-world heterogeneous graph datasets demonstrate thatHeSRN consistently outperforms state-of-the-art heterogeneous graph neuralnetworks and Graph Transformer baselines on node classification tasks,achieving superior accuracy with significantly lower computational complexity.</description>
      <author>example@mail.com (Yifan Lu, Ziyun Zou, Belal Alsinglawi, Islam Al-Qudah, Izzat Alsmadi, Feilong Tang, Pengfei Jiao, Shoaib Jameel)</author>
      <guid isPermaLink="false">2510.09767v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging</title>
      <link>http://arxiv.org/abs/2510.09593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 4 tables. Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STaTS是一种轻量级无监督框架，用于结构感知的时间序列总结，能自适应压缩时间序列为保留信息的令牌序列，实现高达30倍的压缩率，同时保留核心时间动态。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据常含潜在时间结构、状态转换、重复模式和变异性爆发，但现有模型通常处理原始或固定窗口序列，将所有时间步视为同等重要，导致在长序列或噪声序列中效率低下、鲁棒性差和可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个轻量级无监督框架STaTS，用于结构感知的时间序列总结，自适应压缩单变量和多变量时间序列为紧凑的信息保留令牌序列。&lt;h4&gt;方法&lt;/h4&gt;STaTS使用基于BIC的统计散度标准在多个时间分辨率上检测变化点，然后用简单函数（如均值）或生成模型（如GMM）对每个段进行总结，作为模型无关的预处理器可集成到现有无监督时间序列编码器中无需重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在150多个数据集上的实验表明，STaTS可实现全模型85-90%的性能，同时大幅降低计算成本；提高噪声下的鲁棒性，保留判别性结构；优于均匀和基于聚类的压缩基线。&lt;h4&gt;结论&lt;/h4&gt;STaTS是一种原则性的、通用的解决方案，用于高效、结构感知的时间序列建模。&lt;h4&gt;翻译&lt;/h4&gt;时间序列数据通常包含潜在的时间结构、局部平稳状态之间的转换、重复的motifs和变异性爆发，这些特征在标准表示学习管道中很少被利用。现有模型通常处理原始或固定窗口序列，将所有时间步视为同等重要，这导致在长序列或有噪声序列中效率低下、鲁棒性差和可扩展性有限。我们提出了STaTS，一种用于结构感知时间总结的轻量级无监督框架，能自适应地将单变量和多变量时间序列压缩为紧凑的、保留信息的令牌序列。STaTS使用基于BIC的统计散度标准在多个时间分辨率上检测变化点，然后使用简单函数如均值或生成模型如GMM对每个段进行总结。这一过程实现了高达30倍的序列压缩，同时保留核心时间动态。STaTS作为模型无关的预处理器，可以与现有无监督时间序列编码器集成而无需重新训练。在150多个数据集上的广泛实验，包括UCR-85、UCR-128和UEA-30档案上的分类任务，以及ETTh1和ETTh2、ETTm1和Electricity上的预测，表明STaTS可实现全模型85-90%的性能，同时显著降低计算成本。此外，STaTS提高了噪声下的鲁棒性并保留了判别性结构，优于均匀和基于聚类的压缩基线。这些结果将STaTS定位为一种原则性的、通用的解决方案，用于高效、结构感知的时间序列建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series data often contain latent temporal structure, transitions betweenlocally stationary regimes, repeated motifs, and bursts of variability, thatare rarely leveraged in standard representation learning pipelines. Existingmodels typically operate on raw or fixed-window sequences, treating all timesteps as equally informative, which leads to inefficiencies, poor robustness,and limited scalability in long or noisy sequences. We propose STaTS, alightweight, unsupervised framework for Structure-Aware Temporal Summarizationthat adaptively compresses both univariate and multivariate time series intocompact, information-preserving token sequences. STaTS detects change pointsacross multiple temporal resolutions using a BIC-based statistical divergencecriterion, then summarizes each segment using simple functions like the mean orgenerative models such as GMMs. This process achieves up to 30x sequencecompression while retaining core temporal dynamics. STaTS operates as amodel-agnostic preprocessor and can be integrated with existing unsupervisedtime series encoders without retraining. Extensive experiments on 150+datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,demonstrate that STaTS enables 85-90\% of the full-model performance whileoffering dramatic reductions in computational cost. Moreover, STaTS improvesrobustness under noise and preserves discriminative structure, outperforminguniform and clustering-based compression baselines. These results positionSTaTS as a principled, general-purpose solution for efficient, structure-awaretime series modeling.</description>
      <author>example@mail.com (Disharee Bhowmick, Ranjith Ramanathan, Sathyanarayanan N. Aakur)</author>
      <guid isPermaLink="false">2510.09593v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>IF-D: A High-Frequency, General-Purpose Inertial Foundation Dataset for Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2510.09539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 5 figures. Submitted to IEEE ICASSP 2026. Copyright 2026  IEEE. Personal use of this material is permitted. Permission from IEEE must  be obtained for all other uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一个名为IF-D的大型惯性数据集，旨在支持IMU时间序列的自监督学习和基础学习。该数据集包含连续、长时间、多通道的记录，经过精心采集和校准，为鲁棒表示学习和下游任务提供了高质量数据支持。&lt;h4&gt;背景&lt;/h4&gt;惯性测量单元(IMU)时间序列数据在自动驾驶、运动分析和导航等领域有广泛应用。然而，现有数据集可能存在平台特定偏差，且缺乏多样化的运动模式，限制了模型的学习效果和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个大型惯性数据集IF-D，以减轻平台特定运动偏差，让模型接触物理动力学和典型测量噪声，从而促进鲁棒表示学习和支持下游任务如事件检测、运动模式识别和惯性导航。&lt;h4&gt;方法&lt;/h4&gt;使用UM7 IMU传感器，安装在3D打印的球形外壳内，在车辆行驶过程中采集加速度计、陀螺仪和磁力计数据。采样率为200Hz，收集时长约135分钟，产生约160万个样本。采用六方向加速度计校准、静止陀螺仪偏差估计和磁力计硬/软铁校正的椭球拟合等方法进行数据预处理和校准。&lt;h4&gt;主要发现&lt;/h4&gt;成功构建了一个包含九个传感器通道、约160万个样本的大型惯性数据集。通过精心设计的校准程序，有效减轻了平台特定偏差，并提供了定量的校准结果，验证了数据集的质量。&lt;h4&gt;结论&lt;/h4&gt;IF-D数据集通过多样化的自由旋转运动和全面的校准过程，为IMU时间序列的自监督学习和基础学习提供了高质量数据支持。该数据集能够促进鲁棒表示学习，支持事件检测、运动模式识别和惯性导航等多种下游任务。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了IF-D，这是一个大规模惯性数据集，旨在支持IMU时间序列的自监督学习和基础学习。IF-D包含连续、长时间、多通道记录（加速度计、陀螺仪、磁力计），采样率为200Hz，使用安装在3D打印球形外壳内的UM7 IMU采集，该外壳在车辆行驶期间促进多样化的自由旋转。收集时长约135分钟，产生约160万个样本，涵盖九个传感器通道。我们描述了数据采集设置、预处理和校准程序（六方向加速度计校准、静止陀螺仪偏差估计，以及磁力计硬/软铁校正的椭球拟合），并提供了定量的校准结果。IF-D旨在减轻平台特定运动偏差，让模型接触物理动力学和典型测量噪声，从而促进鲁棒表示学习和下游任务，如事件检测、运动模式识别和惯性导航。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present IF-D, a large-scale inertial dataset designed to enableself-supervised and foundational learning for IMU time series. IF-D comprisescontinuous, long-duration multichannel recordings (accelerometer, gyroscope,magnetometer) sampled at 200Hz using a UM7 IMU mounted inside a 3D-printedspherical enclosure that promotes diverse, free rotations during vehicletraversal. The collection spans approximately 135 minutes of recording,yielding around 1.6 million samples across nine sensor channels. We describethe data acquisition setup, preprocessing, and calibration procedures(six-orientation accelerometer calibration, stationary gyroscope biasestimation, and ellipsoid fitting for magnetometer hard-/soft-iron correction),and provide quantitative calibration results. IF-D is designed to mitigateplatform specific motion bias and expose models to both physical dynamics andtypical measurement noise, thereby facilitating robust representation learningand downstream tasks such as event detection, motion mode recognition, andinertial navigation.</description>
      <author>example@mail.com (Patrick Ferreira, Paula Costa)</author>
      <guid isPermaLink="false">2510.09539v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>What Do Temporal Graph Learning Models Learn?</title>
      <link>http://arxiv.org/abs/2510.09416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统评估了七种时间图学习模型捕捉时间图链接结构八种基本属性的能力，揭示了模型在捕捉某些属性上的优势和局限性，为时间图学习模型的应用提供了实践见解。&lt;h4&gt;背景&lt;/h4&gt;时间图学习已成为图表示学习的中心主题，最先进的模型在多个基准测试中表现出色。然而，最近的研究对基准测试结果的可靠性提出了质疑，指出了常用评估协议的问题以及简单启发式方法的竞争力，引发了对模型实际利用底层图哪些特性的疑问。&lt;h4&gt;目的&lt;/h4&gt;探究时间图学习模型实际利用了底层图的哪些特性进行预测，系统评估模型捕捉时间图链接结构相关基本属性的能力。&lt;h4&gt;方法&lt;/h4&gt;系统地评估七种时间图学习模型在捕捉八种与时间图链接结构相关的基本属性方面的能力。这些属性包括结构特性（如密度）、时间模式（如近期性）和边形成机制（如同质性）。研究使用了合成和真实世界数据集进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果呈现出复杂的图景：模型能够很好地捕捉某些属性，但无法重现其他属性。这暴露了模型在时间图学习方面的重要局限性。&lt;h4&gt;结论&lt;/h4&gt;研究结果为时间图学习模型的应用提供了实践见解，并激励时间图学习研究进行更多可解释性驱动的评估，以更好地理解模型的工作原理和局限性。&lt;h4&gt;翻译&lt;/h4&gt;时间图学习已成为图表示学习的中心主题，众多基准测试表明最先进模型的强大性能。然而，最近的工作对基准测试结果的可靠性提出了担忧，指出了常用评估协议的问题以及简单启发式方法的惊人竞争力。这种对比引发了关于时间图学习模型实际上利用了底层图的哪些特性来进行预测的问题。我们通过系统地评估七种模型捕捉时间图链接结构相关八种基本属性的能力来解决这一问题。这些属性包括结构特性如密度、时间模式如近期性，以及边形成机制如同质性。使用合成和真实世界数据集，我们分析了模型学习这些属性的程度。我们的研究结果呈现出复杂的图景：模型能够很好地捕捉某些属性，但无法重现其他属性。通过这一点，我们暴露了重要的局限性。总体而言，我们相信我们的结果为时间图学习模型的应用提供了实践见解，并激励时间图学习研究进行更多可解释性驱动的评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning on temporal graphs has become a central topic in graphrepresentation learning, with numerous benchmarks indicating the strongperformance of state-of-the-art models. However, recent work has raisedconcerns about the reliability of benchmark results, noting issues withcommonly used evaluation protocols and the surprising competitiveness of simpleheuristics. This contrast raises the question of which properties of theunderlying graphs temporal graph learning models actually use to form theirpredictions. We address this by systematically evaluating seven models on theirability to capture eight fundamental attributes related to the link structureof temporal graphs. These include structural characteristics such as density,temporal patterns such as recency, and edge formation mechanisms such ashomophily. Using both synthetic and real-world datasets, we analyze how wellmodels learn these attributes. Our findings reveal a mixed picture: modelscapture some attributes well but fail to reproduce others. With this, we exposeimportant limitations. Overall, we believe that our results provide practicalinsights for the application of temporal graph learning models, and motivatemore interpretability-driven evaluations in temporal graph learning research.</description>
      <author>example@mail.com (Abigail J. Hayes, Tobias Schumacher, Markus Strohmaier)</author>
      <guid isPermaLink="false">2510.09416v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Receiver Generalization for RF Fingerprint Identification via Feature Disentanglement and Adversarial Training</title>
      <link>http://arxiv.org/abs/2510.09405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;射频指纹识别(RFFI)是一种利用设备制造过程中引入的硬件级不完善性来实现精确发射器识别的关键技术。然而，接收器引起的变异性限制了深度神经网络在实际部署中的应用。作者提出了一种对抗训练和风格转移相结合的框架，能够分离发射器和接收器特征，提高跨接收器变化的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;射频指纹识别是无线网络安全的关键技术，它利用设备制造过程中引入的硬件级不完善性来实现精确的发射器识别。深度神经网络在提取判别性特征方面表现出色，但它们的实际部署受到接收器引起的变化性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;解决接收器引起的特征偏移问题，防止RFFI模型过度拟合接收器特定模式，提高模型在不同接收器环境下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出一种对抗接收器变化的RFFI框架，整合对抗训练和风格转移，明确分离发射器和接收器特征。通过强制执行域不变表示学习，将真实的硬件签名与接收器伪影隔离，确保对接收器变化的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在多接收器数据集上的广泛实验表明，该方法始终优于最先进的基线方法，在各种接收器设置下平均准确率提高了高达10%。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了接收器变化导致的性能下降问题，显著提高了RFFI系统在实际部署中的鲁棒性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;射频指纹识别是一种关键技术，用于无线网络安全，它利用设备制造过程中引入的硬件级内在不完善性来实现精确的发射器识别。虽然深度神经网络在提取判别性特征方面表现出色，但它们的实际部署受到接收器引起的变化性的阻碍。在实践中，射频指纹信号包含发射器特定特征以及信道失真和接收器引起的偏差。尽管信道均衡可以减轻信道噪声，但接收器引起的特征偏移仍然在很大程度上未得到解决，导致RFFI模型过度拟合接收器特定模式。当训练和评估使用相同的接收器时，这一限制尤其成问题，因为在部署中更换接收器可能导致性能大幅下降。为了应对这一挑战，我们提出了一种对抗接收器变化的RFFI框架，整合对抗训练和风格转移，明确分离发射器和接收器特征。通过强制执行域不变表示学习，我们的方法将真实的硬件签名与接收器伪影隔离，确保对接收器变化的鲁棒性。在多接收器数据集上的广泛实验表明，我们的方法始终优于最先进的基线方法，在各种接收器设置下平均准确率提高了高达10%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radio frequency fingerprint identification (RFFI) is a critical technique forwireless network security, leveraging intrinsic hardware-level imperfectionsintroduced during device manufacturing to enable precise transmitteridentification. While deep neural networks have shown remarkable capability inextracting discriminative features, their real-world deployment is hindered byreceiver-induced variability. In practice, RF fingerprint signals comprisetransmitter-specific features as well as channel distortions andreceiver-induced biases. Although channel equalization can mitigate channelnoise, receiver-induced feature shifts remain largely unaddressed, causing theRFFI models to overfit to receiver-specific patterns. This limitation isparticularly problematic when training and evaluation share the same receiver,as replacing the receiver in deployment can cause substantial performancedegradation. To tackle this challenge, we propose an RFFI framework robust tocross-receiver variability, integrating adversarial training and style transferto explicitly disentangle transmitter and receiver features. By enforcingdomain-invariant representation learning, our method isolates genuine hardwaresignatures from receiver artifacts, ensuring robustness against receiverchanges. Extensive experiments on multi-receiver datasets demonstrate that ourapproach consistently outperforms state-of-the-art baselines, achieving up to a10% improvement in average accuracy across diverse receiver settings.</description>
      <author>example@mail.com (Yuhao Pan, Xiucheng Wang, Nan Cheng, Wenchao Xu)</author>
      <guid isPermaLink="false">2510.09405v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Music Sample Identification with Multi-Track Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.11507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自监督学习的音频采样识别方法，通过多轨数据集创建人工混合正样本对，并设计新型对比学习目标，显著提高了采样内容检测和原始素材检索的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;采样是现代音乐制作中常见的技术，指利用现有音频片段创建新的音乐内容。自动识别采样内容是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动样本识别系统，能够检测采样内容并检索其原始素材来源。&lt;h4&gt;方法&lt;/h4&gt;采用自监督学习方法，利用多轨数据集创建人工混合的正样本对，设计了一种新的对比学习目标函数。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著优于先前最先进的基线方法，对各种音乐类型具有鲁棒性，并在参考数据库规模扩大时表现出良好的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;详细分析了训练流程中不同组件的贡献，特别强调了高质量分离音轨对于此任务的必要性。&lt;h4&gt;翻译&lt;/h4&gt;采样，即利用现有音频片段创建新音乐内容的技术，在现代音乐制作中非常普遍。在本文中，我们解决了自动样本识别这一具有挑战性的任务，即检测采样内容并检索其原始素材。为此，我们采用了一种自监督学习方法，利用多轨数据集创建人工混合的正样本对，并设计了一种新颖的对比学习目标。我们证明该方法显著优于先前最先进的基线方法，对各种音乐类型具有鲁棒性，并且在增加参考数据库中的噪音歌曲数量时具有良好的可扩展性。此外，我们详细分析了训练流程中不同组件的贡献，并特别强调了高质量分离音轨对于此任务的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sampling, the technique of reusing pieces of existing audio tracks to createnew music content, is a very common practice in modern music production. Inthis paper, we tackle the challenging task of automatic sample identification,that is, detecting such sampled content and retrieving the material from whichit originates. To do so, we adopt a self-supervised learning approach thatleverages a multi-track dataset to create positive pairs of artificial mixes,and design a novel contrastive learning objective. We show that such methodsignificantly outperforms previous state-of-the-art baselines, that is robustto various genres, and that scales well when increasing the number of noisesongs in the reference database. In addition, we extensively analyze thecontribution of the different components of our training pipeline andhighlight, in particular, the need for high-quality separated stems for thistask.</description>
      <author>example@mail.com (Alain Riou, Joan Serrà, Yuki Mitsufuji)</author>
      <guid isPermaLink="false">2510.11507v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features</title>
      <link>http://arxiv.org/abs/2510.11223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了仅通过面部表情的动态成分而非静态面部外观是否能够识别个体。研究使用FLAME 3D可变形模型分离面部形状和表情动态，从对话视频中提取参数。在包含1,429名说话者的CANDOR数据集上，使用带有监督对比学习的Conformer模型实现61.14%的识别准确率，证明面部动态包含强烈的身份特征。研究还引入了漂移-噪声比率(DNR)指标来量化形状-表情分离的可靠性，发现DNR与识别性能呈负相关。研究结果表明对话面部动态中存在特定于个人的特征，对社会感知和临床评估具有重要意义。&lt;h4&gt;背景&lt;/h4&gt;面部识别研究通常关注静态面部特征，而对面部表情动态与身份识别的关系研究较少。&lt;h4&gt;目的&lt;/h4&gt;探究是否仅通过面部表情的动态成分就能识别个体，而不依赖于静态面部外观。&lt;h4&gt;方法&lt;/h4&gt;使用FLAME 3D可变形模型实现面部形状和表情动态的分离，从对话视频中逐帧提取参数并仅保留表情和下颌系数。应用带有监督对比学习的Conformer模型进行1,429路分类任务。引入漂移-噪声比率(DNR)指标量化形状-表情分离的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 面部动态携带强烈的身份特征，识别准确率达到61.14%，是随机猜测的458倍；2) 漂移-噪声比率(DNR)与识别性能呈强负相关，表明不稳定的形状估计会损害动态识别能力；3) 对话面部动态中存在特定于个人的特征。&lt;h4&gt;结论&lt;/h4&gt;面部表情的动态成分包含足够的信息用于个体识别，这种能力对社会感知研究和临床评估具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查个体是否仅能通过面部表情的纯动态成分被识别，而独立于静态面部外观。我们利用FLAME 3D可变形模型来实现面部形状和表情动态之间的明确分离，从对话视频中逐帧提取参数，同时仅保留表情和下颌系数。在包含1,429名说话者在自然对话中的CANDOR数据集上，我们采用监督对比学习的Conformer模型在1,429路分类任务上达到61.14%的准确率——比随机猜测高458倍——证明了面部动态携带强烈的身份特征。我们引入了漂移-噪声比率(DNR)，通过测量跨会话形状变化相对于会话内变异性来量化形状-表情分离的可靠性。DNR与识别性能呈强负相关，证实不稳定的形状估计会损害动态识别。我们的发现揭示了对话面部动态中存在特定于个人的特征，对社会感知和临床评估有启示意义。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要研究是否可以仅通过面部表情的动态成分（即面部运动的方式）来识别一个人，而不依赖于静态的面部外观特征。这个问题在临床评估中很重要，因为可以区分个人特定的表情模式与神经系统症状；在社会互动研究中可以区分个人风格与情感内容；在技术系统中可以创建更自然的个性化虚拟角色动画。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过使用FLAME 3D可变形模型来实现面部形状和表情动态的显式分离，从对话视频中逐帧提取参数，只保留表情和下颌系数。他们借鉴了FLAME模型用于参数化面部，VGGHeads用于从2D视频估计参数，Conformer模型结合自注意力和卷积进行时间建模，以及监督对比学习来学习判别性表示。这些现有技术被创新性地组合以解决身份识别问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是每个人的面部表情动态（如微笑、说话和情绪表达的方式）具有独特的个体特征，这些特征可以作为身份识别的信号，即使不考虑静态的面部外观。整体流程包括：1)使用VGGHeads从视频帧提取FLAME参数；2)只保留动态成分（表情系数和下颌旋转参数）；3)使用监督对比学习训练Conformer模型处理这些动态序列；4)训练线性分类器进行身份识别；5)引入漂移-噪声比(DNR)量化特征分离质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次大规模证明纯面部动态（完全分离外观后）包含身份信号；2)证明Conformer的混合架构能最优捕获多尺度面部模式；3)提出漂移-噪声比(DNR)度量量化分离质量；4)全面分析基于动态识别的上下文和数据需求。相比之前工作，早期方法混合了外观和运动，无法确定识别是基于运动还是几何，而本文通过FLAME模型实现数学上的形状和表情分离，确保分析仅关注动态行为。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次证明可以通过纯面部动态特征（独立于静态面部外观）实现高精度身份识别，并提出了新的度量方法评估特征分离质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates whether individuals can be identified solely throughthe pure dynamical components of their facial expressions, independent ofstatic facial appearance. We leverage the FLAME 3D morphable model to achieveexplicit disentanglement between facial shape and expression dynamics,extracting frame-by-frame parameters from conversational videos while retainingonly expression and jaw coefficients. On the CANDOR dataset of 1,429 speakersin naturalistic conversations, our Conformer model with supervised contrastivelearning achieves 61.14\%accuracy on 1,429-way classification -- 458 timesabove chance -- demonstrating that facial dynamics carry strong identitysignatures. We introduce a drift-to-noise ratio (DNR) that quantifies thereliability of shape expression separation by measuring across-session shapechanges relative to within-session variability. DNR strongly negativelycorrelates with recognition performance, confirming that unstable shapeestimation compromises dynamic identification. Our findings revealperson-specific signatures in conversational facial dynamics, with implicationsfor social perception and clinical assessment.</description>
      <author>example@mail.com (Masoumeh Chapariniya, Pierre Vuillecard, Jean-Marc Odobez, Volker Dellwo, Teodora Vukovic)</author>
      <guid isPermaLink="false">2510.11223v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos</title>
      <link>http://arxiv.org/abs/2510.11204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at CVPR 2023&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于类原型的监督对比学习方法，用于检测在线视频中的教育内容，特别是识字和数学两个类别。该方法使用多模态Transformer网络捕捉视频中的视觉和音频线索交互，并在新创建的APPROVE数据集上进行了验证，结果表明该方法优于现有基线。&lt;h4&gt;背景&lt;/h4&gt;儿童早期在线媒体消费的增长需要数据驱动工具来帮助教育工作者筛选适合的教育内容。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够检测在线视频中教育内容的方法，特别关注识字和数学两个广泛使用的教育内容类别。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于类原型的监督对比学习方法，将其视为细粒度多标签分类问题。使用多模态Transformer网络捕捉视频中的视觉和音频线索之间的交互。学习每个类别的类原型，通过损失函数最小化类原型与其样本之间的距离，同时最大化类原型与其他类别样本之间的距离。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在APPROVE数据集（包含193小时专家标注的视频，共19个类别）和其他基准测试（如Youtube-8M和COIN）上优于强大的基线方法。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效处理多标签细粒度样本，为教育工作者筛选适合的教育内容提供了有效工具。APPROVE数据集已在https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE公开可用。&lt;h4&gt;翻译&lt;/h4&gt;儿童早期在线媒体消费的近期增长需要数据驱动工具，使教育工作者能够为幼儿筛选适当的教育内容。本文提出了一种检测在线视频中教育内容的方法。我们专注于两个广泛使用的教育内容类别：识字和数学。对于每个类别，我们基于共同核心标准选择突出的代码（子类）。例如，识字代码包括'字母名称'、'字母发音'，数学代码包括'计数'、'分类'。我们将此视为细粒度多标签分类问题，因为视频可能包含多种类型的教育内容，且内容类别可能在视觉上相似（例如，'字母名称'与'字母发音'）。我们提出了一种新颖的基于类原型的监督对比学习方法，能够处理与多个标签相关的细粒度样本。我们为每个类别学习一个类原型，并采用损失函数来最小化类原型与其类别样本之间的距离。同样，类原型与其他类别样本之间的距离被最大化。由于视觉和音频线索的 alignment 对有效理解至关重要，我们考虑使用多模态Transformer网络在视频学习嵌入的同时捕捉视频中视觉和音频线索之间的交互。为了评估，我们提出了一个数据集APPROVE，使用YouTube上的教育视频，由教育研究人员用细粒度教育类别进行标注。APPROVE包含193小时专家标注的视频，共19个类别。提出的方法在APPROVE和其他基准测试（如Youtube-8M和COIN）上优于强大的基线方法。数据集可在https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent growth in the consumption of online media by children during earlychildhood necessitates data-driven tools enabling educators to filter outappropriate educational content for young learners. This paper presents anapproach for detecting educational content in online videos. We focus on twowidely used educational content classes: literacy and math. For each class, wechoose prominent codes (sub-classes) based on the Common Core Standards. Forexample, literacy codes include `letter names', `letter sounds', and math codesinclude `counting', `sorting'. We pose this as a fine-grained multilabelclassification problem as videos can contain multiple types of educationalcontent and the content classes can get visually similar (e.g., `letter names'vs `letter sounds'). We propose a novel class prototypes based supervisedcontrastive learning approach that can handle fine-grained samples associatedwith multiple labels. We learn a class prototype for each class and a lossfunction is employed to minimize the distances between a class prototype andthe samples from the class. Similarly, distances between a class prototype andthe samples from other classes are maximized. As the alignment between visualand audio cues are crucial for effective comprehension, we consider amultimodal transformer network to capture the interaction between visual andaudio cues in videos while learning the embedding for videos. For evaluation,we present a dataset, APPROVE, employing educational videos from YouTubelabeled with fine-grained education classes by education researchers. APPROVEconsists of 193 hours of expert-annotated videos with 19 classes. The proposedapproach outperforms strong baselines on APPROVE and other benchmarks such asYoutube-8M, and COIN. The dataset is available athttps://github.com/rohit-gupta/MMContrast/tree/main/APPROVE</description>
      <author>example@mail.com (Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah)</author>
      <guid isPermaLink="false">2510.11204v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities</title>
      <link>http://arxiv.org/abs/2510.11110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysioME是一种鲁棒框架，能够在生理信号医疗应用中处理模态缺失问题，通过多模态自监督学习、专门设计的网络架构和恢复解码器，确保在各种缺失场景下的可靠性能。&lt;h4&gt;背景&lt;/h4&gt;生理信号在医疗应用中经常出现缺失或损坏的情况，这主要是由于硬件限制或运动伪影造成的。然而，大多数现有方法假设所有模态都可用，这导致在任何模态缺失时性能会显著下降。&lt;h4&gt;目的&lt;/h4&gt;克服模态缺失导致性能下降的局限性，提出PhysioME框架，确保在模态缺失条件下保持可靠性能。&lt;h4&gt;方法&lt;/h4&gt;PhysioME采用了三种主要方法：(1) 多模态自监督学习方法，结合对比学习和掩码预测；(2) 专门用于捕捉每种生理信号模态时间动态的Dual-Path NeuroNet主干网络；(3) 恢复解码器，用于重建缺失的模态令牌，能够灵活处理不完整的输入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PhysioME在各种模态缺失场景下都实现了高度一致性和泛化性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了PhysioME作为可靠工具的潜力，可以在数据不完善的现实环境中支持临床决策。&lt;h4&gt;翻译&lt;/h4&gt;缺失或损坏的模态在基于生理信号的医疗应用中很常见，这是由于硬件限制或运动伪影造成的。然而，大多数现有方法假设所有模态都可用，这导致在任何模态缺失时性能显著下降。为了克服这一局限性，本研究提出了PhysioME，一个鲁棒框架，旨在确保在模态缺失条件下保持可靠性能。PhysioME采用：(1) 多模态自监督学习方法，结合对比学习和掩码预测；(2) 专门用于捕捉每种生理信号模态时间动态的Dual-Path NeuroNet主干网络；(3) 恢复解码器，用于重建缺失的模态令牌，能够灵活处理不完整的输入。实验结果表明，PhysioME在各种模态缺失场景下都实现了高度一致性和泛化性能。这些发现强调了PhysioME作为可靠工具的潜力，可以在数据不完善的现实环境中支持临床决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Missing or corrupted modalities are common in physiological signal-basedmedical applications owing to hardware constraints or motion artifacts.However, most existing methods assume the availability of all modalities,resulting in substantial performance degradation in the absence of anymodality. To overcome this limitation, this study proposes PhysioME, a robustframework designed to ensure reliable performance under missing modalityconditions. PhysioME adopts: (1) a multimodal self-supervised learning approachthat combines contrastive learning with masked prediction; (2) aDual-PathNeuroNet backbone tailored to capture the temporal dynamics of eachphysiological signal modality; and (3) a restoration decoder that reconstructsmissing modality tokens, enabling flexible processing of incomplete inputs. Theexperimental results show that PhysioME achieves high consistency andgeneralization performance across various missing modality scenarios. Thesefindings highlight the potential of PhysioME as a reliable tool for supportingclinical decision-making in real-world settings with imperfect dataavailability.</description>
      <author>example@mail.com (Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, Dong-Joo Kim)</author>
      <guid isPermaLink="false">2510.11110v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Source-Free Object Detection with Detection Transformer</title>
      <link>http://arxiv.org/abs/2510.11090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Transactions on Image Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FRANCK(Feature Reweighting ANd Contrastive Learning NetworK)，一种专门为DETR架构设计的源域无目标检测(SFOD)框架，通过四个关键组件实现查询为中心的特征增强，有效提升了模型在目标域的鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;源域无目标检测(SFOD)允许在不访问源数据的情况下，将知识从源域转移到无监督的目标域进行目标检测。然而，现有方法要么局限于传统目标检测模型(如Faster R-CNN)，要么缺乏针对新型目标检测架构(尤其是Detection Transformer/DETR)的专门适配。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门为DETR模型设计的SFOD框架，解决现有方法在新型目标检测架构上的局限性，提高目标检测在目标域的性能。&lt;h4&gt;方法&lt;/h4&gt;FRANCK框架包含四个关键组件：(1)基于目标性评分的样本重加权(OSSR)模块，通过计算注意力目标性评分并重加权检测损失来强调难以识别区域；(2)基于匹配的记忆库对比学习(CMMB)模块，集成多级特征到记忆库中增强类别对比学习；(3)不确定性加权的查询融合特征蒸馏(UQFD)模块，通过预测质量重加权和查询特征融合改进特征蒸馏；(4)改进的自训练流程，具有动态教师更新间隔(DTUI)以优化伪标签质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过这些组件，FRANCK有效将源预训练的DETR模型适应到目标域，显著增强了模型的鲁棒性和泛化能力。在多个基准测试上的广泛实验表明，该方法达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FRANCK是一种有效的、与基于DETR的SFOD模型兼容的方法，为目标检测领域特别是DETR架构的源域自适应提供了创新解决方案，具有显著的应用价值和兼容性。&lt;h4&gt;翻译&lt;/h4&gt;源域无目标检测(SFOD)使知识能够从源域转移到无监督的目标域进行目标检测，而无需访问源数据。大多数现有的SFOD方法要么局限于传统的目标检测(OD)模型(如Faster R-CNN)，要么被设计为通用解决方案，没有针对新型OD架构(尤其是Detection Transformer/DETR)进行专门适配。在本文中，我们引入了特征重加权与对比学习网络(FRANCK)，一种新型SFOD框架，专门为DETR执行以查询为中心的特征增强。FRANCK包含四个关键组件：(1)基于目标性评分的样本重加权(OSSR)模块，在多尺度编码器特征图上计算基于注意力的目标性评分，对检测损失进行重加权，以强调难以识别的区域；(2)基于匹配的记忆库对比学习(CMMB)模块，将多级特征集成到记忆库中，增强类别的对比学习；(3)不确定性加权的查询融合特征蒸馏(UQFD)模块，通过预测质量重加权和查询特征融合来改进特征蒸馏；(4)改进的自训练流程，具有动态教师更新间隔(DTUI)，优化伪标签质量。通过利用这些组件，FRANCK有效地将源预训练的DETR模型适应到目标域，增强了鲁棒性和泛化能力。在几个广泛使用的基准测试上的广泛实验表明，我们的方法达到了最先进的性能，突显了其有效性与基于DETR的SFOD模型的兼容性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TIP.2025.3607621&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source-Free Object Detection (SFOD) enables knowledge transfer from a sourcedomain to an unsupervised target domain for object detection without access tosource data. Most existing SFOD approaches are either confined to conventionalobject detection (OD) models like Faster R-CNN or designed as general solutionswithout tailored adaptations for novel OD architectures, especially DetectionTransformer (DETR). In this paper, we introduce Feature Reweighting ANdContrastive Learning NetworK (FRANCK), a novel SFOD framework specificallydesigned to perform query-centric feature enhancement for DETRs. FRANCKcomprises four key components: (1) an Objectness Score-based Sample Reweighting(OSSR) module that computes attention-based objectness scores on multi-scaleencoder feature maps, reweighting the detection loss to emphasizeless-recognized regions; (2) a Contrastive Learning with Matching-based MemoryBank (CMMB) module that integrates multi-level features into memory banks,enhancing class-wise contrastive learning; (3) an Uncertainty-weightedQuery-fused Feature Distillation (UQFD) module that improves featuredistillation through prediction quality reweighting and query feature fusion;and (4) an improved self-training pipeline with a Dynamic Teacher UpdatingInterval (DTUI) that optimizes pseudo-label quality. By leveraging thesecomponents, FRANCK effectively adapts a source-pre-trained DETR model to atarget domain with enhanced robustness and generalization. Extensiveexperiments on several widely used benchmarks demonstrate that our methodachieves state-of-the-art performance, highlighting its effectiveness andcompatibility with DETR-based SFOD models.</description>
      <author>example@mail.com (Huizai Yao, Sicheng Zhao, Shuo Lu, Hui Chen, Yangyang Li, Guoping Liu, Tengfei Xing, Chenggang Yan, Jianhua Tao, Guiguang Ding)</author>
      <guid isPermaLink="false">2510.11090v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation</title>
      <link>http://arxiv.org/abs/2510.11036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;XGrasp是一个实时的夹爪感知抓取检测框架，能够高效处理多种夹爪配置，解决了传统机器人抓取方法仅针对单一夹爪类型的问题。&lt;h4&gt;背景&lt;/h4&gt;大多数机器人抓取方法通常针对单一夹爪类型设计，这在需要多样化末端执行器的现实世界场景中限制了它们的适用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理多种夹爪配置的实时抓取检测框架，提高机器人在多样化环境中的抓取能力。&lt;h4&gt;方法&lt;/h4&gt;通过系统性地为现有数据集添加多夹爪注释解决数据稀缺问题；采用分层两阶段架构，包括使用全局场景信息和夹爪规格确定最佳位置的抓取点预测器(GPP)，以及使用局部特征细化抓取角度和宽度的角度-宽度预测器(AWP)；在AWP模块中使用对比学习实现零样本泛化；模块化框架与视觉基础模型无缝集成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示XGrasp在各种夹爪类型上具有竞争力的抓取成功率，同时与现有的夹爪感知方法相比，在推理速度上有了显著提高。&lt;h4&gt;结论&lt;/h4&gt;XGrasp提供了一个有效的解决方案，使机器人能够适应不同的夹爪配置，并在性能和计算效率上优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;大多数机器人抓取方法通常针对单一夹爪类型设计，这限制了它们在需要多样化末端执行器的现实世界场景中的适用性。我们提出了XGrasp，一个实时的夹爪感知抓取检测框架，能够高效处理多种夹爪配置。该方法通过系统性地为现有数据集添加多夹爪注释来解决数据稀缺问题。XGrasp采用分层两阶段架构。在第一阶段，抓取点预测器(GPP)使用全局场景信息和夹爪规格确定最佳位置。在第二阶段，角度-宽度预测器(AWP)使用局部特征细化抓取角度和宽度。AWP模块中的对比学习通过学习基本抓取特征，实现对未见夹爪的零样本泛化。模块化框架与视觉基础模型无缝集成，为未来的视觉语言能力提供途径。实验结果表明，在各种夹爪类型上具有竞争力的抓取成功率，同时与现有的夹爪感知方法相比，在推理速度上取得了显著提高。项目页面：https://sites.google.com/view/xgrasp&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most robotic grasping methods are typically designed for single grippertypes, which limits their applicability in real-world scenarios requiringdiverse end-effectors. We propose XGrasp, a real-time gripper-aware graspdetection framework that efficiently handles multiple gripper configurations.The proposed method addresses data scarcity by systematically augmentingexisting datasets with multi-gripper annotations. XGrasp employs a hierarchicaltwo-stage architecture. In the first stage, a Grasp Point Predictor (GPP)identifies optimal locations using global scene information and gripperspecifications. In the second stage, an Angle-Width Predictor (AWP) refines thegrasp angle and width using local features. Contrastive learning in the AWPmodule enables zero-shot generalization to unseen grippers by learningfundamental grasping characteristics. The modular framework integratesseamlessly with vision foundation models, providing pathways for futurevision-language capabilities. The experimental results demonstrate competitivegrasp success rates across various gripper types, while achieving substantialimprovements in inference speed compared to existing gripper-aware methods.Project page: https://sites.google.com/view/xgrasp</description>
      <author>example@mail.com (Yeonseo Lee, Jungwook Mun, Hyosup Shin, Guebin Hwang, Junhee Nam, Taeyeop Lee, Sungho Jo)</author>
      <guid isPermaLink="false">2510.11036v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Joint Learning Approach to Hardware Caching and Prefetching</title>
      <link>http://arxiv.org/abs/2510.10862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ML for Systems Workshop at the 39th Conference on Neural  Information Processing Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了硬件缓存领域中缓存替换和预取策略之间的相互依赖关系，提出了一种联合学习方法，通过共享表示来同时训练这两种策略，并展示了两种实现方法的初步结果。&lt;h4&gt;背景&lt;/h4&gt;现代系统中已提出多种学习方法替代启发式算法用于调度、缓存等系统组件，这些模型利用多样特征、历史学习和行为预测来应对工作负载动态性和硬件发展。&lt;h4&gt;目的&lt;/h4&gt;研究缓存替换和预取策略之间的双向相互依赖关系，提出并验证联合训练这两种策略的方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于特征共享表示的联合学习方法，包括两种实现方式：基于联合编码器和基于嵌入对比学习的方法。&lt;h4&gt;主要发现&lt;/h4&gt;两种共享表示开发方法都展示了有希望的初步结果，证明了联合训练策略的有效性。&lt;h4&gt;结论&lt;/h4&gt;为基于相互依赖关系的系统策略联合研究方向制定了未来研究议程。&lt;h4&gt;翻译&lt;/h4&gt;已经提出了多种学习策略来替代现代系统中的调度、缓存和其他系统组件的启发式算法。通过利用多样特征、从历史趋势学习和预测未来行为，这类模型有望跟上不断增加的工作负载动态性和持续的硬件发展。然而，单独训练的策略在组合使用时可能仍然表现不佳。本文在硬件缓存领域研究了一个这样的实例——针对缓存替换和预取策略。我们认为这两种策略是双向相互依赖的，并论证了联合训练这两种策略的必要性。我们提出了一种联合学习方法，基于为两种策略使用的特征开发共享表示。我们介绍了两种开发这些共享表示的方法，一种基于联合编码器，另一种基于嵌入的对比学习，并展示了这两种方法都取得了有希望的初步结果。最后，我们为这一方向的未来研究制定了议程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several learned policies have been proposed to replace heuristics forscheduling, caching, and other system components in modern systems. Byleveraging diverse features, learning from historical trends, and predictingfuture behaviors, such models promise to keep pace with ever-increasingworkload dynamism and continuous hardware evolution. However, policies trainedin isolation may still achieve suboptimal performance when placed together. Inthis paper, we inspect one such instance in the domain of hardware caching --for the policies of cache replacement and prefetching. We argue that these twopolicies are bidirectionally interdependent and make the case for training thetwo jointly. We propose a joint learning approach based on developing sharedrepresentations for the features used by the two policies. We present twoapproaches to develop these shared representations, one based on a jointencoder and another based on contrastive learning of the embeddings, anddemonstrate promising preliminary results for both of these. Finally, we laydown an agenda for future research in this direction.</description>
      <author>example@mail.com (Samuel Yuan, Divyanshu Saxena, Jiayi Chen, Nihal Sharma, Aditya Akella)</author>
      <guid isPermaLink="false">2510.10862v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SS-DPPN: A self-supervised dual-path foundation model for the generalizable cardiac audio representation</title>
      <link>http://arxiv.org/abs/2510.10719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自监督双路径原型网络（SS-DPPN），用于从无标签数据中进行心脏音频表示和分类的基础模型，解决了监督式深度学习受限于专家标注数据稀缺性的问题。&lt;h4&gt;背景&lt;/h4&gt;心音图的自动分析对心血管疾病的早期诊断至关重要，但监督式深度学习常受限于专家标注数据的稀缺性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从无标签数据中进行心脏音频表示和分类的基础模型，减少对专家标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出自监督双路径原型网络（SS-DPPN），采用基于双路径对比学习的架构，同时处理一维波形和二维频谱图，使用新型混合损失函数；下游任务采用基于度量学习的原型网络方法，提高敏感性并产生校准良好且可信赖的预测。&lt;h4&gt;主要发现&lt;/h4&gt;SS-DPPN在四个心脏音频基准测试上达到最先进性能；在数据效率方面表现出色，标注数据减少三倍；学习到的表示在肺部声音分类和心率估计任务上成功泛化。&lt;h4&gt;结论&lt;/h4&gt;实验和发现验证了SS-DPPN作为生理信号的一种强大、可靠且可扩展的基础模型。&lt;h4&gt;翻译&lt;/h4&gt;心音图的自动分析对心血管疾病的早期诊断至关重要，然而监督式深度学习常受限于专家标注数据的稀缺性。在本文中，我们提出了自监督双路径原型网络（SS-DPPN），这是一种用于从无标签数据进行心脏音频表示和分类的基础模型。该框架引入了一种基于双路径对比学习的架构，同时使用新型混合损失函数处理一维波形和二维频谱图。对于下游任务，使用基于度量学习的原型网络方法，提高了敏感性并产生校准良好且可信赖的预测。SS-DPPN在四个心脏音频基准测试上实现了最先进的性能。该框架在数据效率方面表现出色，与全监督模型相比，标注数据减少三倍。最后，学习到的表示在肺部声音分类和心率估计任务上成功泛化。我们的实验和发现验证了SS-DPPN作为生理信号的一种强大、可靠且可扩展的基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The automated analysis of phonocardiograms is vital for the early diagnosisof cardiovascular disease, yet supervised deep learning is often constrained bythe scarcity of expert-annotated data. In this paper, we propose theSelf-Supervised Dual-Path Prototypical Network (SS-DPPN), a foundation modelfor cardiac audio representation and classification from unlabeled data. Theframework introduces a dual-path contrastive learning based architecture thatsimultaneously processes 1D waveforms and 2D spectrograms using a novel hybridloss. For the downstream task, a metric-learning approach using a PrototypicalNetwork was used that enhances sensitivity and produces well-calibrated andtrustworthy predictions. SS-DPPN achieves state-of-the-art performance on fourcardiac audio benchmarks. The framework demonstrates exceptional dataefficiency with a fully supervised model on three-fold reduction in labeleddata. Finally, the learned representations generalize successfully across lungsound classification and heart rate estimation. Our experiments and findingsvalidate SS-DPPN as a robust, reliable, and scalable foundation model forphysiological signals.</description>
      <author>example@mail.com (Ummy Maria Muna, Md Mehedi Hasan Shawon, Md Jobayer, Sumaiya Akter, Md Rakibul Hasan, Md. Golam Rabiul Alam)</author>
      <guid isPermaLink="false">2510.10719v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion</title>
      <link>http://arxiv.org/abs/2510.10633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多智能体强化学习框架，用于解决多模态文本到图像生成中语义对齐和专业细节保持的难题。&lt;h4&gt;背景&lt;/h4&gt;多模态文本到图像生成面临保持语义对齐和专业级别细节的困难，特别是在不同视觉领域之间。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够协调领域专业化智能体的多智能体强化学习框架，提升跨视觉域的语义对齐和专业细节生成能力。&lt;h4&gt;方法&lt;/h4&gt;构建包含文本增强模块和图像生成模块的双耦合子系统，每个模块配备多模态集成组件；使用近端策略优化(PPO)训练智能体，通过复合奖励函数平衡语义相似性、视觉质量和内容多样性；采用对比学习、双向注意和迭代反馈实现跨模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;系统显著丰富了生成内容（字数增加1614%），同时降低ROUGE-1分数69.7%；基于Transformer的融合方法获得最高复合分数(0.521)，但存在稳定性问题；多模态集成一致性中等(0.444-0.481)，反映跨模态语义基础的持续挑战。&lt;h4&gt;结论&lt;/h4&gt;协作的、专业化驱动的架构在推进可靠的多模态生成系统方面显示出广阔前景。&lt;h4&gt;翻译&lt;/h4&gt;多模态文本到图像生成仍然受到在多样化视觉领域中保持语义对齐和专业级别细节的困难限制。我们提出了一种多智能体强化学习框架，在两个耦合子系统中协调领域专业化智能体（例如，专注于建筑、肖像和风景图像）：文本增强模块和图像生成模块，每个模块都增加了多模态集成组件。智能体在复合奖励函数下使用近端策略优化(PPO)进行训练，该函数平衡语义相似性、语言视觉质量和内容多样性。通过对比学习、双向注意和文本与图像之间的迭代反馈来强制跨模态对齐。在六种实验设置中，我们的系统显著丰富了生成内容（字数增加了1614%），同时将ROUGE-1分数降低了69.7%。在融合方法中，基于Transformer的策略获得最高复合分数(0.521)，尽管偶尔存在稳定性问题。多模态集成产生中等一致性（范围从0.444到0.481），反映了跨模态语义基础的持续挑战。这些研究结果强调了协作的、专业化驱动的架构在推进可靠多模态生成系统方面的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal text-to-image generation remains constrained by the difficulty ofmaintaining semantic alignment and professional-level detail across diversevisual domains. We propose a multi-agent reinforcement learning framework thatcoordinates domain-specialized agents (e.g., focused on architecture,portraiture, and landscape imagery) within two coupled subsystems: a textenhancement module and an image generation module, each augmented withmultimodal integration components. Agents are trained using Proximal PolicyOptimization (PPO) under a composite reward function that balances semanticsimilarity, linguistic visual quality, and content diversity. Cross-modalalignment is enforced through contrastive learning, bidirectional attention,and iterative feedback between text and image. Across six experimentalsettings, our system significantly enriches generated content (word countincreased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusionmethods, Transformer-based strategies achieve the highest composite score(0.521), despite occasional stability issues. Multimodal ensembles yieldmoderate consistency (ranging from 0.444 to 0.481), reflecting the persistentchallenges of cross-modal semantic grounding. These findings underscore thepromise of collaborative, specialization-driven architectures for advancingreliable multimodal generative systems.</description>
      <author>example@mail.com (Jiabao Shi, Minfeng Qi, Lefeng Zhang, Di Wang, Yingjie Zhao, Ziying Li, Yalong Xing, Ningran Li)</author>
      <guid isPermaLink="false">2510.10633v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2510.10564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MGSD-WSS的多粒度序列去噪方法，通过弱监督信号同时处理项目粒度和兴趣粒度噪声，显著提升了序列推荐性能。&lt;h4&gt;背景&lt;/h4&gt;序列推荐基于用户历史交互序列预测下一个项目，但历史序列中的不相关噪声项严重影响推荐效果。现有无监督方法缺乏明确噪声标签，容易误判用户感兴趣项目，且仅关注项目粒度噪声而忽略兴趣粒度噪声。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中噪声标签缺乏导致的误判问题，以及忽略兴趣粒度噪声的问题，实现更全面有效的序列去噪推荐。&lt;h4&gt;方法&lt;/h4&gt;提出MGSD-WSS方法，包含多核感知器模块映射序列到共同表示空间，利用弱监督信号识别噪声项，通过带噪声加权对比学习的项目粒度去噪模块处理项目噪声，提取目标兴趣表示处理兴趣粒度噪声，最后基于去噪后的项目和兴趣表示预测下一项目。&lt;h4&gt;主要发现&lt;/h4&gt;在五个数据集上的大量实验表明，所提出的方法显著优于最先进的序列推荐和去噪模型。&lt;h4&gt;结论&lt;/h4&gt;MGSD-WSS能有效解决现有序列推荐中的噪声问题，提升推荐性能，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;Sequential recommendation aims to predict the next item based on user interests in historical interaction sequences. Historical interaction sequences often contain irrelevant noisy items, which significantly hinders the performance of recommendation systems. Existing research employs unsupervised methods that indirectly identify item-granularity irrelevant noise by predicting the ground truth item. Since these methods lack explicit noise labels, they are prone to misidentify users' interested items as noise. Additionally, while these methods focus on removing item-granularity noise driven by the ground truth item, they overlook interest-granularity noise, limiting their ability to perform broader denoising based on user interests. To address these issues, we propose Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation (MGSD-WSS). MGSD-WSS first introduces the Multiple Gaussian Kernel Perceptron module to map the original and enhance sequence into a common representation space and utilizes weakly supervised signals to accurately identify noisy items in the historical interaction sequence. Subsequently, it employs the item-granularity denoising module with noise-weighted contrastive learning to obtain denoised item representations. Then, it extracts target interest representations from the ground truth item and applies noise-weighted contrastive learning to obtain denoised interest representations. Finally, based on the denoised item and interest representations, MGSD-WSS predicts the next item. Extensive experiments on five datasets demonstrate that the proposed method significantly outperforms state-of-the-art sequence recommendation and denoising models. Our code is available at https://github.com/lalunex/MGSD-WSS.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation aims to predict the next item based on userinterests in historical interaction sequences. Historical interaction sequencesoften contain irrelevant noisy items, which significantly hinders theperformance of recommendation systems. Existing research employs unsupervisedmethods that indirectly identify item-granularity irrelevant noise bypredicting the ground truth item. Since these methods lack explicit noiselabels, they are prone to misidentify users' interested items as noise.Additionally, while these methods focus on removing item-granularity noisedriven by the ground truth item, they overlook interest-granularity noise,limiting their ability to perform broader denoising based on user interests. Toaddress these issues, we propose Multi-Granularity Sequence Denoising withWeakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSSfirst introduces the Multiple Gaussian Kernel Perceptron module to map theoriginal and enhance sequence into a common representation space and utilizesweakly supervised signals to accurately identify noisy items in the historicalinteraction sequence. Subsequently, it employs the item-granularity denoisingmodule with noise-weighted contrastive learning to obtain denoised itemrepresentations. Then, it extracts target interest representations from theground truth item and applies noise-weighted contrastive learning to obtaindenoised interest representations. Finally, based on the denoised item andinterest representations, MGSD-WSS predicts the next item. Extensiveexperiments on five datasets demonstrate that the proposed method significantlyoutperforms state-of-the-art sequence recommendation and denoising models. Ourcode is available at https://github.com/lalunex/MGSD-WSS.</description>
      <author>example@mail.com (Liang Li, Zhou Yang, Xiaofei Zhu)</author>
      <guid isPermaLink="false">2510.10564v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction</title>
      <link>http://arxiv.org/abs/2510.10490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 Pages, Plus Appendices, EACL 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了VOLTAGE，一种基于对比学习的OCR方法，用于低资源语言（特别是Takri文字）的数字化保护，以防止语言灭绝。&lt;h4&gt;背景&lt;/h4&gt;全球7000种语言中有2500种被列为濒危语言，语言流失导致传统智慧、民间文学和社区本质的丧失。低资源语言面临更高的灭绝风险，而缺乏针对低资源语言的无监督OCR方法是阻碍其数字化的原因之一。&lt;h4&gt;目的&lt;/h4&gt;为低资源语言提供数字包容性，避免语言灭绝，开发适用于低资源语言的OCR方法。&lt;h4&gt;方法&lt;/h4&gt;提出VOLTAGE - 一种基于对比学习的OCR方法，利用自动字形特征推荐进行基于聚类的标记，使用图像转换和生成对抗网络增加标记数据的多样性和数量，使用Takri文字进行设计，并在多种印度文字上测试。&lt;h4&gt;主要发现&lt;/h4&gt;在Takri文字上实现了95%的机器印刷样本和87%的手写样本的准确率，进行了基线和消融研究，为Takri构建了下游应用案例，证明了工作的实用性。&lt;h4&gt;结论&lt;/h4&gt;VOLTAGE方法具有普适性，适用于不同类型的印度文字，有助于保护低资源语言的数字化。&lt;h4&gt;翻译&lt;/h4&gt;联合国教科文组织已将全球使用的7000种语言中的2500种列为濒危语言。语言的流失导致传统智慧、民间文学和使用该语言的社区本质的丧失。因此，必须为这些语言提供数字包容性，避免其灭绝。低资源语言面临更高的灭绝风险。缺乏针对低资源语言的无监督光学字符识别方法是阻碍其数字化的原因之一。我们提出了VOLTAGE - 一种基于对比学习的OCR方法，利用自动字形特征推荐进行基于聚类的标记。我们使用图像转换和生成对抗网络来增加标记数据的多样性和数量。VOLTAGE是使用Takri文字设计的 - 这是一组在16至20世纪印度喜马拉雅地区使用的文字。我们展示了Takri文字以及其他印度文字（包括低资源和高资源）的结果，以证明该方法的普适性。在Takri文字上，机器印刷样本的准确率达到95%，手写样本达到87%。我们进行了基线和消融研究，并为Takri构建了下游应用案例，证明了我们工作的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.18653/v1/2024.eacl-long.53&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; UNESCO has classified 2500 out of 7000 languages spoken worldwide asendangered. Attrition of a language leads to loss of traditional wisdom, folkliterature, and the essence of the community that uses it. It is thereforeimperative to bring digital inclusion to these languages and avoid itsextinction. Low resource languages are at a greater risk of extinction. Lack ofunsupervised Optical Character Recognition(OCR) methodologies for low resourcelanguages is one of the reasons impeding their digital inclusion. We proposeVOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyphfeature recommendation for cluster-based labelling. We augment the labelleddata for diversity and volume using image transformations and GenerativeAdversarial Networks. Voltage has been designed using Takri - a family ofscripts used in 16th to 20th century in the Himalayan regions of India. Wepresent results for Takri along with other Indic scripts (both low and highresource) to substantiate the universal behavior of the methodology. Anaccuracy of 95% for machine printed and 87% for handwritten samples on Takriscript has been achieved. We conduct baseline and ablation studies along withbuilding downstream use cases for Takri, demonstrating the usefulness of ourwork.</description>
      <author>example@mail.com (Prawaal Sharma, Poonam Goyal, Vidisha Sharma, Navneet Goyal)</author>
      <guid isPermaLink="false">2510.10490v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Complementary and Contrastive Learning for Audio-Visual Segmentation</title>
      <link>http://arxiv.org/abs/2510.10051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE Transactions on Multimedia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CCFormer的新型音频-视觉分割框架，结合了CNN和Transformer的优势，能够有效处理局部和全局信息并全面捕捉空间-时间上下文。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉分割(AVS)旨在生成与物体听觉信号相关的像素级分割图。传统CNN方法通过基本操作处理音频-视觉交互但受限于局部感受野；较新的Transformer方法将听觉作为查询增强帧内协作，但难以充分提取多模态系数和时间动态。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，开发一个能同时处理局部和全局信息并全面捕捉空间-时间上下文的音频-视觉分割框架。&lt;h4&gt;方法&lt;/h4&gt;提出CCFormer框架，包含：1)早期集成模块(EIM)，采用并行双边架构融合多尺度视觉特征与音频数据；2)多查询Transformer模块(MTM)，动态赋予音频查询学习能力并建模帧级和视频级关系；3)双模态对比学习(BCL)，促进统一特征空间中跨模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;通过有效结合这些设计，该方法在S4、MS3和AVSS数据集上设立了新的最先进基准，源代码和模型权重将在GitHub公开。&lt;h4&gt;结论&lt;/h4&gt;CCFormer框架通过创新设计成功解决了传统方法和现有Transformer方法在音频-视觉分割中的局限性，实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;音频-视觉分割(AVS)旨在生成与物体听觉信号相关的像素级分割图。该领域已取得显著进展，许多基于CNN和Transformer的方法提高了分割的准确性和鲁棒性。传统CNN方法通过填充和乘法等基本操作处理音频-视觉交互，但受限于CNN的有限局部感受野。较新的基于Transformer的方法将听觉线索作为查询，利用注意力机制增强帧内音频-视觉协作，但通常难以充分提取多模态系数和时间动态。为克服这些局限性，我们提出了互补和对比Transformer(CCFormer)，这是一个新型框架，擅长处理局部和全局信息，并全面捕捉空间-时间上下文。我们的CCFormer首先采用早期集成模块(EIM)，该模块采用并行双边架构，将多尺度视觉特征与音频数据融合，以增强跨模态互补性。为了提取帧内空间特征并促进时间相干性的感知，我们引入了多查询Transformer模块(MTM)，该模块动态赋予音频查询学习能力，同时建模帧级和视频级关系。此外，我们提出了双模态对比学习(BCL)，以促进统一特征空间中跨模态的对齐。通过有效结合这些设计，我们的方法在S4、MS3和AVSS数据集上设立了新的最先进基准。我们的源代码和模型权重将在https://github.com/SitongGong/CCFormer公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation mapsthat correlate with the auditory signals of objects. This field has seensignificant progress with numerous CNN and Transformer-based methods enhancingthe segmentation accuracy and robustness. Traditional CNN approaches manageaudio-visual interactions through basic operations like padding andmultiplications but are restricted by CNNs' limited local receptive field. Morerecently, Transformer-based methods treat auditory cues as queries, utilizingattention mechanisms to enhance audio-visual cooperation within frames.Nevertheless, they typically struggle to extract multimodal coefficients andtemporal dynamics adequately. To overcome these limitations, we present theComplementary and Contrastive Transformer (CCFormer), a novel framework adeptat processing both local and global information and capturing spatial-temporalcontext comprehensively. Our CCFormer initiates with the Early IntegrationModule (EIM) that employs a parallel bilateral architecture, mergingmulti-scale visual features with audio data to boost cross-modalcomplementarity. To extract the intra-frame spatial features and facilitate theperception of temporal coherence, we introduce the Multi-query TransformerModule (MTM), which dynamically endows audio queries with learning capabilitiesand models the frame and video-level relations simultaneously. Furthermore, wepropose the Bi-modal Contrastive Learning (BCL) to promote the alignment acrossboth modalities in the unified feature space. Through the effective combinationof those designs, our method sets new state-of-the-art benchmarks across theS4, MS3 and AVSS datasets. Our source code and model weights will be madepublicly available at https://github.com/SitongGong/CCFormer</description>
      <author>example@mail.com (Sitong Gong, Yunzhi Zhuge, Lu Zhang, Pingping Zhang, Huchuan Lu)</author>
      <guid isPermaLink="false">2510.10051v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning</title>
      <link>http://arxiv.org/abs/2510.09915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了通过微调大型语言模型来减少生成摘要中的不忠实片段，引入了一种新的数据集和三种微调技术，实验结果表明所有方法都能提高摘要忠实度，其中似然训练最有效。&lt;h4&gt;背景&lt;/h4&gt;使用大型语言模型进行摘要生成已成为信息压缩的重要工具，但这些模型有时会产生不忠实的摘要，包含单词、短语或概念级别的幻觉。&lt;h4&gt;目的&lt;/h4&gt;研究微调策略以减少生成摘要中不忠实片段的出现，提高摘要的忠实度。&lt;h4&gt;方法&lt;/h4&gt;首先使用多种LLM为训练集中的源文档自动生成摘要，然后使用GPT-4o标注检测到的片段级幻觉；利用这些标注，使用无幻觉摘要和标注的不忠实片段微调LLM；引入一个包含忠实和不忠实摘要以及片段级标签的新数据集；评估三种微调技术：梯度上升、似然训练和任务向量否定。&lt;h4&gt;主要发现&lt;/h4&gt;所有三种方法都成功利用片段级标注提高了忠实度，其中似然训练最有效。&lt;h4&gt;结论&lt;/h4&gt;通过使用片段级标注进行微调，可以有效提高LLM生成摘要的忠实度。&lt;h4&gt;翻译&lt;/h4&gt;使用大型语言模型进行抽象式摘要已成为信息压缩的重要工具。然而，尽管这些模型能够生成流畅的摘要，但有时会产生不忠实的摘要，在单词、短语或概念层面引入幻觉。现有的缓解策略，如后处理校正或使用合成生成的负样本进行对比学习，无法完全解决LLM生成摘要中可能出现的各种错误。在本文中，我们研究了微调策略以减少生成摘要中不忠实片段的出现。首先，我们使用多种LLM为训练集中的源文档自动生成摘要，然后使用GPT-4o标注其检测到的片段级幻觉。利用这些标注，我们使用无幻觉摘要和标注的不忠实片段来微调LLM，以提高模型的忠实度。在本文中，我们引入了一个包含忠实和不忠实摘要以及片段级标签的新数据集，并评估了三种微调技术来提高LLM生成摘要的忠实度：梯度上升、似然训练和任务向量否定。实验结果表明，所有三种方法都成功利用片段级标注提高了忠实度，其中似然训练最为有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Abstractive summarization using large language models (LLMs) has become anessential tool for condensing information. However, despite their ability togenerate fluent summaries, these models sometimes produce unfaithful summaries,introducing hallucinations at the word, phrase, or concept level. Existingmitigation strategies, such as post-processing corrections or contrastivelearning with synthetically generated negative samples, fail to fully addressthe diverse errors that can occur in LLM-generated summaries. In this paper, weinvestigate fine-tuning strategies to reduce the occurrence of unfaithful spansin generated summaries. First, we automatically generate summaries for the setof source documents in the training set with a variety of LLMs and then useGPT-4o to annotate any hallucinations it detects at the span-level. Leveragingthese annotations, we fine-tune LLMs with both hallucination-free summaries andannotated unfaithful spans to enhance model faithfulness. In this paper, weintroduce a new dataset that contains both faithful and unfaithful summarieswith span-level labels and we evaluate three techniques to fine-tuning a LLM toimprove the faithfulness of the resulting summarization: gradient ascent,unlikelihood training, and task vector negation. Experimental results show thatall three approaches successfully leverage span-level annotations to improvefaithfulness, with unlikelihood training being the most effective.</description>
      <author>example@mail.com (Sicong Huang, Qianqi Yan, Shengze Wang, Ian Lane)</author>
      <guid isPermaLink="false">2510.09915v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations</title>
      <link>http://arxiv.org/abs/2510.09293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DualCSE的句子嵌入方法，通过为每个句子分配两个嵌入向量（分别代表显式语义和隐式语义）来解决传统方法难以捕捉句子隐式语义的问题。&lt;h4&gt;背景&lt;/h4&gt;句子嵌入方法虽然取得了显著进展，但仍难以捕捉句子中的隐式语义，这归因于传统方法为每个句子只分配一个向量的固有局限性。&lt;h4&gt;目的&lt;/h4&gt;克服传统句子嵌入方法的局限性，开发一种能够同时捕捉句子显式和隐式语义的方法。&lt;h4&gt;方法&lt;/h4&gt;提出DualCSE方法，为每个句子分配两个嵌入：一个代表显式语义，另一个代表隐式语义，这两个嵌入共存于共享空间中，可根据特定任务需求选择合适的语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DualCSE能够有效编码句子的显式和隐式含义，并提高下游任务（如信息检索和文本分类）的性能。&lt;h4&gt;结论&lt;/h4&gt;DualCSE是一种有效的句子嵌入方法，通过双嵌入机制同时捕捉句子的显式和隐式语义，提升了下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;句子嵌入方法已经取得了显著进展，但仍然难以捕捉句子中的隐式语义。这可以归因于传统句子嵌入方法的固有局限性，即为每个句子只分配一个向量。为了克服这一局限性，我们提出了DualCSE，一种为每个句子分配两个嵌入的句子嵌入方法：一个代表显式语义，另一个代表隐式语义。这些嵌入共存于共享空间中，使得能够为特定目的（如信息检索和文本分类）选择所需的语义。实验结果表明，DualCSE能够有效编码显式和隐式含义，并提高下游任务的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sentence embedding methods have made remarkable progress, yet they stillstruggle to capture the implicit semantics within sentences. This can beattributed to the inherent limitations of conventional sentence embeddingmethods that assign only a single vector per sentence. To overcome thislimitation, we propose DualCSE, a sentence embedding method that assigns twoembeddings to each sentence: one representing the explicit semantics and theother representing the implicit semantics. These embeddings coexist in theshared space, enabling the selection of the desired semantics for specificpurposes such as information retrieval and text classification. Experimentalresults demonstrate that DualCSE can effectively encode both explicit andimplicit meanings and improve the performance of the downstream task.</description>
      <author>example@mail.com (Kohei Oda, Po-Min Chuang, Kiyoaki Shirai, Natthawut Kertkeidkachorn)</author>
      <guid isPermaLink="false">2510.09293v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Generative Data Augmentation in Graph Contrastive Learning for Recommendation</title>
      <link>http://arxiv.org/abs/2510.09129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 34th ACM International Conference on Information and Knowledge  Management&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GDA4Rec的新颖框架，用于在推荐系统中通过图对比学习和生成数据增强来学习高质量的用户-物品嵌入表示。&lt;h4&gt;背景&lt;/h4&gt;推荐系统已成为各种在线平台不可或缺的组成部分，但面临从稀疏用户-物品交互中学习有效嵌入表示的基本挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有随机数据增强方法在对比学习中常常改变原始语义信息的问题，提供高质量的增强视图和强大的自监督信号。&lt;h4&gt;方法&lt;/h4&gt;GDA4Rec采用噪声生成模块利用深度生成模型近似原始数据分布进行数据增强，提取物品互补矩阵表征物品间潜在相关性，并使用整合推荐、数据增强和对比学习的联合目标函数。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的广泛实验证明了该模型的优越性。&lt;h4&gt;结论&lt;/h4&gt;GDA4Rec通过生成高质量增强视图和提供强大自监督信号，解决了推荐系统中从稀疏交互学习有效嵌入的挑战。&lt;h4&gt;翻译&lt;/h4&gt;推荐系统已成为各种在线平台中不可或缺的组成部分，从电子商务到流媒体服务。该领域的一个基本挑战是如何从稀疏的用户-物品交互中学习有效的嵌入表示。虽然对比学习最近已成为解决这个问题的有前景的方法，但通过大多数现有随机数据增强方法为对比学习生成增强视图常常导致原始语义信息的改变。在本文中，我们提出了一个新颖的框架GDA4Rec（用于推荐的图对比学习中的生成数据增强），以生成高质量的增强视图并提供强大的自监督信号。具体来说，我们采用了一个噪声生成模块，利用深度生成模型来近似原始数据的分布用于数据增强。此外，GDA4Rec进一步提取了一个物品互补矩阵来表征物品之间的潜在相关性，并提供额外的自监督信号。最后，使用一个整合了推荐、数据增强和对比学习的联合目标函数，强制模型学习更有效和信息量更大的嵌入。在三个公共数据集上进行了广泛的实验，以证明该模型的优越性。代码可在以下网址获取：https://github.com/MrYansong/GDA4Rec。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761248&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommendation systems have become indispensable in various online platforms,from e-commerce to streaming services. A fundamental challenge in this domainis learning effective embeddings from sparse user-item interactions. Whilecontrastive learning has recently emerged as a promising solution to thisissue, generating augmented views for contrastive learning through mostexisting random data augmentation methods often leads to the alteration oforiginal semantic information. In this paper, we propose a novel framework,GDA4Rec (Generative Data Augmentation in graph contrastive learning forRecommendation) to generate high-quality augmented views and provide robustself-supervised signals. Specifically, we employ a noise generation module thatleverages deep generative models to approximate the distribution of originaldata for data augmentation. Additionally, GDA4Rec further extracts an itemcomplement matrix to characterize the latent correlations between items andprovide additional self-supervised signals. Lastly, a joint objective thatintegrates recommendation, data augmentation and contrastive learning is usedto enforce the model to learn more effective and informative embeddings.Extensive experiments are conducted on three public datasets to demonstrate thesuperiority of the model. The code is available at:https://github.com/MrYansong/GDA4Rec.</description>
      <author>example@mail.com (Yansong Wang, Qihui Lin, Junjie Huang, Tao Jia)</author>
      <guid isPermaLink="false">2510.09129v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Biomedical Named Entity Recognition Framework with Large Language Models</title>
      <link>http://arxiv.org/abs/2510.08902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a short paper at BIBM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型的统一生物医学命名实体识别框架，解决了嵌套实体、实体边界模糊和跨语言泛化问题，在多个数据集上实现了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;生物医学命名实体识别对医学信息提取和知识发现至关重要，但现有方法在处理嵌套实体、实体边界模糊和跨语言泛化方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于大型语言模型的统一生物医学命名实体识别框架，提高识别准确性和跨语言泛化能力。&lt;h4&gt;方法&lt;/h4&gt;将BioNER重新表述为文本生成任务，设计符号标记策略处理扁平和嵌套实体，通过中英文数据集进行双语联合微调，引入基于对比学习的实体选择器过滤错误预测。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集和两个未见语料库上的实验结果表明，该方法实现了最先进的性能，并在跨语言场景下展现出强大的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于大型语言模型的BioNER框架能有效处理复杂的生物医学实体识别任务，具有出色的跨语言泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确的生物医学命名实体识别对医学信息提取和知识发现至关重要。然而，现有方法通常难以处理嵌套实体、实体边界模糊和跨语言泛化问题。在本文中，我们提出了一种基于大型语言模型的统一生物医学命名实体识别框架。我们首先将BioNER重新表述为文本生成任务，并设计了一种符号标记策略，通过明确的边界标注联合处理扁平实体和嵌套实体。为了增强多语言和多任务泛化能力，我们在多个中英文数据集上进行双语联合微调。此外，我们引入了一种基于对比学习的实体选择器，利用边界敏感的正负样本过滤不正确或虚假的预测。在四个基准数据集和两个未见语料库上的实验结果表明，我们的方法实现了最先进的性能，并在跨语言场景下展现出强大的零样本泛化能力。源代码可在https://github.com/dreamer-tx/LLMNER免费获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate recognition of biomedical named entities is critical for medicalinformation extraction and knowledge discovery. However, existing methods oftenstruggle with nested entities, entity boundary ambiguity, and cross-lingualgeneralization. In this paper, we propose a unified Biomedical Named EntityRecognition (BioNER) framework based on Large Language Models (LLMs). We firstreformulate BioNER as a text generation task and design a symbolic taggingstrategy to jointly handle both flat and nested entities with explicit boundaryannotation. To enhance multilingual and multi-task generalization, we performbilingual joint fine-tuning across multiple Chinese and English datasets.Additionally, we introduce a contrastive learning-based entity selector thatfilters incorrect or spurious predictions by leveraging boundary-sensitivepositive and negative samples. Experimental results on four benchmark datasetsand two unseen corpora show that our method achieves state-of-the-artperformance and robust zero-shot generalization across languages. The sourcecodes are freely available at https://github.com/dreamer-tx/LLMNER.</description>
      <author>example@mail.com (Tengxiao Lv, Ling Luo, Juntao Li, Yanhua Wang, Yuchen Pan, Chao Liu, Yanan Wang, Yan Jiang, Huiyi Lv, Yuanyuan Sun, Jian Wang, Hongfei Lin)</author>
      <guid isPermaLink="false">2510.08902v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>On the Alignment Between Supervised and Self-Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.08852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了自监督对比学习(CL)与负样本监督对比学习(NSCL)在表示层面的一致性，发现尽管两种方法的损失函数相似，但在训练过程中它们的表示确实保持对齐，而参数空间则可能出现指数级差异。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习已取得显著成功，其表示能力可与监督预训练相媲美。最近理论表明，当类别数量增加时，CL损失函数近似于监督代理NSCL损失函数。&lt;h4&gt;目的&lt;/h4&gt;探究CL和NSCL是否不仅在目标函数层面，而且在表示层面也保持一致，以及这种对齐如何随训练条件变化。&lt;h4&gt;方法&lt;/h4&gt;分析在相同初始化、批量和增强条件下训练的CL和NSCL模型的表示对齐情况，提供理论证明和实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;1) CL和NSCL的表示保持相似，相似度矩阵在现实条件下保持接近；2) 提供了CKA和RSA等对齐度量的高概率保证；3) 对齐随类别数量增加、温度升高而改善，且依赖于批量大小；4) 参数空间耦合不稳定，权重差异可能随训练时间呈指数级增长；5) 实验验证显示CL-NSCL对齐随规模和温度增强，NSCL比其他监督目标更紧密跟踪CL。&lt;h4&gt;结论&lt;/h4&gt;NSCL可作为自监督学习和监督学习之间的原则性桥梁，为理解自监督学习提供了理论框架。&lt;h4&gt;翻译&lt;/h4&gt;自监督对比学习(CL)已取得显著的实证成功，其产生的表示通常能在下游任务上与监督预训练相媲美。最近的理论表明，当类别数量增加时，CL损失函数近似于监督代理NSCL损失函数。然而，这种损失层面的相似性留下了一个开放问题：CL和NSCL在训练过程中是否也在表示层面保持对齐，而不仅仅是在目标函数层面？我们通过分析在共享随机性(相同初始化、批量和增强)条件下训练的CL和NSCL模型的表示对齐来解决这个问题。首先，我们证明它们诱导的表示保持相似：具体而言，我们在现实条件下证明了CL和NSCL的相似度矩阵保持接近。我们的边界为CKA和RSA等对齐度量提供了高概率保证，并阐明了对齐如何随类别数量增加、温度升高而改善，以及其对批量大小的依赖性。相比之下，我们证明了参数空间耦合本质上是不稳定的：CL和NSCL权重之间的差异可能随训练时间呈指数级增长。最后，我们通过实验验证了这些预测，表明CL-NSCL对齐随规模和温度增强，NSCL比其他监督目标更紧密地跟踪CL。这使NSCL成为自监督学习和监督学习之间的原则性桥梁。我们的代码和项目页面可在[链接]获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised contrastive learning (CL) has achieved remarkable empiricalsuccess, often producing representations that rival supervised pre-training ondownstream tasks. Recent theory explains this by showing that the CL lossclosely approximates a supervised surrogate, Negatives-Only SupervisedContrastive Learning (NSCL) loss, as the number of classes grows. Yet thisloss-level similarity leaves an open question: {\em Do CL and NSCL also remainaligned at the representation level throughout training, not just in theirobjectives?}  We address this by analyzing the representation alignment of CL and NSCLmodels trained under shared randomness (same initialization, batches, andaugmentations). First, we show that their induced representations remainsimilar: specifically, we prove that the similarity matrices of CL and NSCLstay close under realistic conditions. Our bounds provide high-probabilityguarantees on alignment metrics such as centered kernel alignment (CKA) andrepresentational similarity analysis (RSA), and they clarify how alignmentimproves with more classes, higher temperatures, and its dependence on batchsize. In contrast, we demonstrate that parameter-space coupling is inherentlyunstable: divergence between CL and NSCL weights can grow exponentially withtraining time.  Finally, we validate these predictions empirically, showing that CL-NSCLalignment strengthens with scale and temperature, and that NSCL tracks CL moreclosely than other supervised objectives. This positions NSCL as a principledbridge between self-supervised and supervised learning. Our code and projectpage are available at[\href{https://github.com/DLFundamentals/understanding_ssl_v2}{code},\href{https://dlfundamentals.github.io/cl-nscl-representation-alignment/}{projectpage}].</description>
      <author>example@mail.com (Achleshwar Luthra, Priyadarsi Mishra, Tomer Galanti)</author>
      <guid isPermaLink="false">2510.08852v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering</title>
      <link>http://arxiv.org/abs/2510.08791v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR2025 Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种医学视觉问答(Med-VQA)框架，通过统一模态对齐、难例挖掘和门控交叉注意力模块解决现有方法的局限性，并在多个标准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉问答(Med-VQA)需要对医学图像和文本问题有深入理解，尽管近期基于医学视觉语言预训练(Med-VLP)的方法表现良好，但仍存在模态对齐不统一、难例探索不足以及知识融合可能引入无关信息等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架解决医学视觉问答中的模态对齐、难例处理和知识融合挑战。&lt;h4&gt;方法&lt;/h4&gt;提出三个关键贡献：(1)统一解决方案处理多级别、多模态、多视图和多阶段的异构模态对齐，利用对比学习和最优传输理论；(2)难例挖掘方法，使用软标签进行多模态对齐并强制区分难例对；(3)门控交叉注意力模块，将答案词汇作为先验知识集成并选择相关信息。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在RAD-VQA、SLAKE、PathVQA和VQA-2019等广泛使用的Med-VQA数据集上超越了之前的最佳性能。&lt;h4&gt;结论&lt;/h4&gt;通过统一模态对齐策略、有效的难例挖掘机制和门控交叉注意力模块，该框架显著提升了医学视觉问答任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉问答(Med-VQA)是一个具有挑战性的任务，需要对医学图像和文本问题有深入理解。尽管近期利用医学视觉语言预训练(Med-VLP)的工作已在Med-VQA任务上展现出强大性能，但仍没有统一的模态对齐解决方案，且难例问题尚未得到充分探索。此外，Med-VQA常用的知识融合技术可能引入无关信息。在这项工作中，我们通过三个关键贡献提出一个框架来解决这些挑战：(1)一种统一解决方案，用于处理多级别、多模态、多视图和多阶段的异构模态对齐，利用对比学习和最优传输理论等方法；(2)一种难例挖掘方法，使用软标签进行多模态对齐，并强制区分难例对；(3)一种用于Med-VQA的门控交叉注意力模块，将答案词汇作为先验知识集成，并从中选择相关信息。我们的框架在广泛使用的Med-VQA数据集(如RAD-VQA、SLAKE、PathVQA和VQA-2019)上超越了之前的最佳性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical Visual Question Answering (Med-VQA) is a challenging task thatrequires a deep understanding of both medical images and textual questions.Although recent works leveraging Medical Vision-Language Pre-training (Med-VLP)have shown strong performance on the Med-VQA task, there is still no unifiedsolution for modality alignment, and the issue of hard negatives remainsunder-explored. Additionally, commonly used knowledge fusion techniques forMed-VQA may introduce irrelevant information. In this work, we propose aframework to address these challenges through three key contributions: (1) aunified solution for heterogeneous modality alignments across multiple levels,modalities, views, and stages, leveraging methods like contrastive learning andoptimal transport theory; (2) a hard negative mining method that employs softlabels for multi-modality alignments and enforces the hard negative pairdiscrimination; and (3) a Gated Cross-Attention Module for Med-VQA thatintegrates the answer vocabulary as prior knowledge and selects relevantinformation from it. Our framework outperforms the previous state-of-the-art onwidely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019.</description>
      <author>example@mail.com (Yuanhao Zou, Zhaozheng Yin)</author>
      <guid isPermaLink="false">2510.08791v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Lecture Notes on Verifying Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.11617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇讲义回顾了图神经网络、Weisfeiler-Lehman测试与逻辑之间的联系，提出了一种包含计数模态的模态逻辑，用于图神经网络验证，并描述了相应的可满足性问题算法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络、Weisfeiler-Lehman测试与一阶逻辑、分级模态逻辑之间的联系&lt;h4&gt;目的&lt;/h4&gt;提出一种模态逻辑，其中计数模态以线性不等式形式出现，用于解决图神经网络的验证任务&lt;h4&gt;方法&lt;/h4&gt;描述了该逻辑可满足性问题的算法，该方法基于普通模态逻辑的tableau方法，并扩展了对无量化子句布尔代数与Presburger算术的推理&lt;h4&gt;主要发现&lt;/h4&gt;计数模态可以以线性不等式形式出现在模态逻辑中，用于解决图神经网络验证问题&lt;h4&gt;结论&lt;/h4&gt;通过扩展的tableau方法，可以有效地解决所提出模态逻辑的可满足性问题&lt;h4&gt;翻译&lt;/h4&gt;在这些讲义中，我们首先回顾了图神经网络、Weisfeiler-Lehman测试与一阶逻辑和分级模态逻辑等逻辑之间的联系。然后，我们提出了一个模态逻辑，其中计数模态以线性不等式的形式出现，用于解决图神经网络的验证任务。我们描述了该逻辑可满足性问题的算法。该算法受到普通模态逻辑的tableau方法的启发，并扩展了对无量化子句布尔代数与Presburger算术的推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In these lecture notes, we first recall the connection between graph neuralnetworks, Weisfeiler-Lehman tests and logics such as first-order logic andgraded modal logic. We then present a modal logic in which counting modalitiesappear in linear inequalities in order to solve verification tasks on graphneural networks. We describe an algorithm for the satisfiability problem ofthat logic. It is inspired from the tableau method of vanilla modal logic,extended with reasoning in quantifier-free fragment Boolean algebra withPresburger arithmetic.</description>
      <author>example@mail.com (François Schwarzentruber)</author>
      <guid isPermaLink="false">2510.11617v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity</title>
      <link>http://arxiv.org/abs/2510.11347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新颖的多视图特征传播(MFP)框架，用于解决图神经网络在节点分类任务中因特征稀疏和隐私问题导致的性能下降。该框架通过将特征划分为多个添加高斯噪声的视图，独立传播信息并聚合结果，提高了分类性能同时保护隐私。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在节点分类任务中表现出色，但其效果通常依赖于完整的节点特征。在现实场景中，特征矩阵往往高度稀疏或包含敏感信息，导致性能下降和隐私风险增加。直接暴露信息还可能导致意外数据泄露，使攻击者能够推断敏感信息。&lt;h4&gt;目的&lt;/h4&gt;解决特征稀疏问题，提高节点分类性能，同时促进隐私保护，平衡效用与隐私的关系。&lt;h4&gt;方法&lt;/h4&gt;提出多视图特征传播(MFP)框架，将可用特征划分为多个添加高斯噪声的视图，每个视图独立通过图拓扑传播信息，聚合后的表示生成具有表达能力和鲁棒性的节点嵌入。该框架创新性地提高了极端稀疏条件下的鲁棒性，并提供了平衡效用与隐私的原则性方法。&lt;h4&gt;主要发现&lt;/h4&gt;在图数据集上的大量实验表明，MFP在节点分类方面优于最先进的基线方法，同时显著减少了隐私泄露。传播的输出作为原始特征的替代插补值而非重建值，保留了效用而不损害隐私。全面的敏感性分析证实了MFP在不同场景下的稳定性和实际适用性。&lt;h4&gt;结论&lt;/h4&gt;MFP为具有缺失或敏感特征的图学习领域提供了一个有效且具有隐私意识的框架。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在关系数据上的节点分类任务中已经显示出显著的成功，然而它们的有效性往往依赖于完整节点特征的可用性。然而，在许多现实场景中，特征矩阵高度稀疏或包含敏感信息，导致性能下降和隐私风险增加。此外，直接暴露信息可能导致意外数据泄露，使攻击者能够推断敏感信息。为应对这些挑战，我们提出了一种新颖的多视图特征传播(MFP)框架，该框架在特征稀疏条件下增强节点分类同时促进隐私保护。MFP通过将可用特征划分为多个添加高斯噪声的视图来扩展传统特征传播(FP)，每个视图独立通过图拓扑传播信息。聚合后的表示产生具有表达能力和鲁棒性的节点嵌入。该框架在两个方面具有创新性：它引入了一种在极端稀疏条件下提高鲁棒性的机制，并提供了一种平衡效用与隐私的原则性方法。在图数据集上进行的大量实验表明，MFP在节点分类方面优于最先进的基线方法，同时显著减少了隐私泄露。此外，我们的分析表明传播的输出作为原始特征的替代插补值而非重建值，保留了效用而不损害隐私。全面的敏感性分析进一步证实了MFP在不同场景下的稳定性和实际适用性。总体而言，MFP为具有缺失或敏感特征的图学习领域提供了一个有效且具有隐私意识的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable success in nodeclassification tasks over relational data, yet their effectiveness oftendepends on the availability of complete node features. In many real-worldscenarios, however, feature matrices are highly sparse or contain sensitiveinformation, leading to degraded performance and increased privacy risks.Furthermore, direct exposure of information can result in unintended dataleakage, enabling adversaries to infer sensitive information. To address thesechallenges, we propose a novel Multi-view Feature Propagation (MFP) frameworkthat enhances node classification under feature sparsity while promotingprivacy preservation. MFP extends traditional Feature Propagation (FP) bydividing the available features into multiple Gaussian-noised views, eachpropagating information independently through the graph topology. Theaggregated representations yield expressive and robust node embeddings. Thisframework is novel in two respects: it introduces a mechanism that improvesrobustness under extreme sparsity, and it provides a principled way to balanceutility with privacy. Extensive experiments conducted on graph datasetsdemonstrate that MFP outperforms state-of-the-art baselines in nodeclassification while substantially reducing privacy leakage. Moreover, ouranalysis demonstrates that propagated outputs serve as alternative imputationsrather than reconstructions of the original features, preserving utilitywithout compromising privacy. A comprehensive sensitivity analysis furtherconfirms the stability and practical applicability of MFP across diversescenarios. Overall, MFP provides an effective and privacy-aware framework forgraph learning in domains characterized by missing or sensitive features.</description>
      <author>example@mail.com (Etzion Harari, Moshe Unger)</author>
      <guid isPermaLink="false">2510.11347v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Event-Aware Prompt Learning for Dynamic Graphs</title>
      <link>http://arxiv.org/abs/2510.11339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EVP的事件感知动态图提示学习框架，可作为现有方法的插件，增强其利用历史事件知识的能力。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的图通常通过一系列事件演化，建模跨领域对象之间的动态交互。动态图神经网络(DGNNs)已成为动态图学习的流行解决方案，而提示学习方法最近也被探索应用于动态图。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法专注于捕捉节点与时间关系而忽视历史事件影响的问题，提出能够增强现有方法利用历史事件知识能力的框架。&lt;h4&gt;方法&lt;/h4&gt;首先为每个节点提取一系列历史事件并引入事件适应机制对齐事件特征与下游任务；其次提出事件聚合机制将历史知识整合到节点表示中；最后在四个公共数据集上进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了EVP框架能够有效利用历史事件知识增强动态图学习性能。&lt;h4&gt;结论&lt;/h4&gt;EVP框架作为插件可以增强现有动态图学习方法的能力，特别在利用历史事件知识方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的图通常通过一系列事件演化，建模跨领域对象之间的动态交互。在动态图学习中，动态图神经网络(DGNNs)已成为流行的解决方案。最近，提示学习方法已被探索应用于动态图。然而，现有方法通常专注于捕捉节点与时间之间的关系，而忽视了历史事件的影响。在本文中，我们提出了EVP，一种事件感知的动态图提示学习框架，可作为现有方法的插件，增强它们利用历史事件知识的能力。首先，我们为每个节点提取一系列历史事件，并引入事件适应机制将这些事件的细粒度特征与下游任务对齐。其次，我们提出事件聚合机制，有效将历史知识整合到节点表示中。最后，我们在四个公共数据集上进行广泛的实验来评估和分析EVP。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world graph typically evolve via a series of events, modeling dynamicinteractions between objects across various domains. For dynamic graphlearning, dynamic graph neural networks (DGNNs) have emerged as popularsolutions. Recently, prompt learning methods have been explored on dynamicgraphs. However, existing methods generally focus on capturing the relationshipbetween nodes and time, while overlooking the impact of historical events. Inthis paper, we propose EVP, an event-aware dynamic graph prompt learningframework that can serve as a plug-in to existing methods, enhancing theirability to leverage historical events knowledge. First, we extract a series ofhistorical events for each node and introduce an event adaptation mechanism toalign the fine-grained characteristics of these events with downstream tasks.Second, we propose an event aggregation mechanism to effectively integratehistorical knowledge into node representations. Finally, we conduct extensiveexperiments on four public datasets to evaluate and analyze EVP.</description>
      <author>example@mail.com (Xingtong Yu, Ruijuan Liang, Xinming Zhang, Yuan Fang)</author>
      <guid isPermaLink="false">2510.11339v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Edge-to-Cloud Computations-as-a-Service in Software-Defined Energy Networks for Smart Grids</title>
      <link>http://arxiv.org/abs/2510.11286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDEN（软件定义能源网络）的创新架构，用于解决现代电网中数据生成与处理位置不匹配的问题，通过统一边缘、雾和云计算资源，结合5G URLLC、SDN和NFV技术，实现能源、延迟和可靠性的协同优化。&lt;h4&gt;背景&lt;/h4&gt;现代电网面临数据生成位置与数据处理位置严重不匹配的挑战：保护继电器、电动汽车充电和分布式可再生能源需要在边缘进行毫秒级分析，而耗能大的工作负载通常位于远程云中，导致错过实时截止日期和浪费电力。&lt;h4&gt;目的&lt;/h4&gt;提出首个SDEN（软件定义能源网络）用于CaaS（计算即服务），将碎片化的电网计算转变为单一的、可编程的基础设施，提供可靠、节能的实时分析。&lt;h4&gt;方法&lt;/h4&gt;统一边缘、雾和云计算与5G URLLC（超可靠低延迟通信）、SDN（软件定义网络）和NFV（网络功能虚拟化）技术；提出联合任务卸载公式，在明确URLLC约束下将计算放置与网络容量耦合；开发可行性保持的轻量级贪婪启发式算法；构建分层AI管道，包括边缘反应式、雾预测性和云战略性层，使用隐私保护的联邦GNN（图神经网络）进行故障检测和微电网协调。&lt;h4&gt;主要发现&lt;/h4&gt;SDEN架构能够有效解决电网计算资源碎片化问题，通过协同优化能源消耗、网络延迟和系统可靠性，提供端到端的实时分析能力；所提出的轻量级贪婪启发式算法能够在可扩展性的同时，紧密跟踪最佳能源和延迟权衡；分层AI管道能够在不同层级提供不同特性的智能分析能力。&lt;h4&gt;结论&lt;/h4&gt;SDEN建立了首个软件定义的路径，实现了实用的、电网规模的CaaS（计算即服务），与仅边缘或仅云的方案不同，它能够将碎片化的电网计算资源整合为单一、可编程的基础设施，提供可靠、节能的实时分析能力。&lt;h4&gt;翻译&lt;/h4&gt;现代电网面临数据生成位置与数据处理位置严重不匹配的挑战：保护继电器、电动汽车充电和分布式可再生能源需要在边缘进行毫秒级分析，而耗能大的工作负载通常位于远程云中，导致错过实时截止日期和浪费电力。我们通过提出据我们所知首个用于CaaS（计算即服务）的SDEN（软件定义能源网络）来解决这一问题，该架构统一了边缘、雾和云计算资源，并结合5G URLLC（超可靠低延迟通信）、SDN（软件定义网络）和NFV（网络功能虚拟化）技术，共同优化端到端的能源、延迟和可靠性。我们的贡献有三方面：（i）联合任务卸载公式，在明确URLLC约束下将计算放置与网络容量耦合；（ii）可行性保持的轻量级贪婪启发式算法，可扩展并紧密跟踪最佳能源和延迟权衡；（iii）分层AI管道，边缘层具有反应性，雾层具有预测性，云层具有战略性，具有隐私保护的联邦GNN（图神经网络）用于故障检测和微电网协调。与仅边缘或仅云的方案不同，SDEN将碎片化的电网计算转变为单一的、可编程的基础设施，提供可靠、节能的实时分析，建立了首个软件定义的路径，实现实用的、电网规模的CaaS。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern power grids face an acute mismatch between where data is generated andwhere it can be processed: protection relays, EV (Electric Vehicle) charging,and distributed renewables demand millisecond analytics at the edge, whileenergy-hungry workloads often sit in distant clouds leading to missed real-timedeadlines and wasted power. We address this by proposing, to our knowledge, thefirst-ever SDEN (Software Defined Energy Network) for CaaS(Computations-as-a-Service) that unifies edge, fog, and cloud compute with 5GURLLC (Ultra-Reliable Low-Latency Communications), SDN (Software DefinedNetworking), and NFV (Network Functions Virtualization) to co-optimize energy,latency, and reliability end-to-end. Our contributions are threefold: (i) ajoint task offloading formulation that couples computation placement withnetwork capacity under explicit URLLC constraints; (ii) a feasibilitypreserving, lightweight greedy heuristic that scales while closely trackingoptimal energy and latency trade-offs; and (iii) a tiered AI (ArtificialIntelligence) pipeline-reactive at the edge, predictive in the fog, strategicin the cloud-featuring privacy-preserving, federated GNNs (Graph NeuralNetworks) for fault detection and microgrid coordination. Unlike prioredge-only or cloud-only schemes, SDEN turns fragmented grid compute into asingle, programmable substrate that delivers dependable, energy-aware, realtime analytics establishing a first-ever, software defined path to practical,grid-scale CaaS.</description>
      <author>example@mail.com (Jack Jackman, David Ryan, Arun Narayanan, Pedro Nardelli, Indrakshi Dey)</author>
      <guid isPermaLink="false">2510.11286v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Enforcing convex constraints in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.11227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ProjNet，一个满足输入依赖约束的图神经网络框架，结合了稀疏向量裁剪和CAD算法，并通过GPU加速实现高效处理大规模输入的能力。&lt;h4&gt;背景&lt;/h4&gt;许多机器学习应用需要满足复杂、动态约束的输出，但在图神经网络模型中，由于图结构数据的可变输出大小，这一任务尤其具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够满足输入依赖约束的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;ProjNet结合稀疏向量裁剪方法和Component-Averaged Dykstra (CAD)算法，建立CAD的收敛结果，开发GPU加速实现，并引入计算效率高且适合优化的替代梯度用于端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;在四类约束优化问题（线性规划、两类非凸二次规划和无线发射功率优化）上验证了ProjNet的有效性。&lt;h4&gt;结论&lt;/h4&gt;ProjNet在各种问题设置中表现出有效性，能够满足复杂、动态的约束要求。&lt;h4&gt;翻译&lt;/h4&gt;许多机器学习应用需要满足复杂、动态约束的输出。在图神经网络模型中，由于图结构数据的可变输出大小，这一任务尤其具有挑战性。本文介绍了ProjNet，一个满足输入依赖约束的图神经网络框架。ProjNet结合了稀疏向量裁剪方法和Component-Averaged Dykstra (CAD)算法，一种解决最佳逼近问题的迭代方案。我们建立了CAD的收敛结果，并开发了能够高效处理大规模输入的GPU加速实现。为了实现端到端训练，我们引入了一个计算效率高且比精确梯度更适合优化的替代梯度。我们在四类约束优化问题上验证了ProjNet：线性规划、两类非凸二次规划和无线发射功率优化，证明了其在各种问题设置中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many machine learning applications require outputs that satisfy complex,dynamic constraints. This task is particularly challenging in Graph NeuralNetwork models due to the variable output sizes of graph-structured data. Inthis paper, we introduce ProjNet, a Graph Neural Network framework whichsatisfies input-dependant constraints. ProjNet combines a sparse vectorclipping method with the Component-Averaged Dykstra (CAD) algorithm, aniterative scheme for solving the best-approximation problem. We establish aconvergence result for CAD and develop a GPU-accelerated implementation capableof handling large-scale inputs efficiently. To enable end-to-end training, weintroduce a surrogate gradient for CAD that is both computationally efficientand better suited for optimization than the exact gradient. We validate ProjNeton four classes of constrained optimisation problems: linear programming, twoclasses of non-convex quadratic programs, and radio transmit poweroptimization, demonstrating its effectiveness across diverse problem settings.</description>
      <author>example@mail.com (Ahmed Rashwan, Keith Briggs, Chris Budd, Lisa Kreusser)</author>
      <guid isPermaLink="false">2510.11227v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Multicast Routing for On-Demand Streaming Services in 6G Networks</title>
      <link>http://arxiv.org/abs/2510.11109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络(GNN)的多播路由框架，用于解决6G网络中带宽密集型应用的路由问题，能够同时最小化传输成本并支持用户特定的视频质量需求。&lt;h4&gt;背景&lt;/h4&gt;6G无线网络中带宽密集型应用（如实时体积流和多感官扩展现实）的增长需要智能多播路由解决方案，能够大规模提供差异化服务质量。&lt;h4&gt;目的&lt;/h4&gt;解决传统路由算法计算复杂、结构僵化、无法支持异构用户需求的问题，以及基于神经网络的方法缺乏拓扑泛化能力和可扩展性的局限。&lt;h4&gt;方法&lt;/h4&gt;将路由问题表述为约束最小流优化任务，开发强化学习算法顺序构建高效多播树；使用图注意力网络(GAT)作为编码器提取上下文感知节点嵌入，使用长短期记忆(LSTM)模块建模路由决策中的序列依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;该方法接近基于动态规划的最优解，同时显著降低计算复杂度；对大规模和动态网络拓扑具有强泛化能力，适合6G多媒体交付场景的实时部署。&lt;h4&gt;结论&lt;/h4&gt;提出的GNN-based多播路由框架能有效解决6G网络中带宽密集型应用的路由问题，平衡了性能和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;随着第六代（6G）无线网络中带宽密集型应用的增加，如实时体积流和多感官扩展现实，需要能够大规模提供差异化服务质量（QoS）的智能多播路由解决方案。传统的最短路径和多播路由算法要么计算上不可行，要么结构上僵化，它们通常无法支持异构用户需求，导致资源利用次优。基于神经网络的方法虽然提供了改进的推理速度，但通常缺乏拓扑泛化能力和可扩展性。为了解决这些限制，本文提出了一个基于图神经网络（GNN）的多播路由框架，该框架同时最小化总传输成本并支持用户特定的视频质量要求。路由问题被表述为约束最小流优化任务，并开发了一种强化学习算法，通过重用路径和适应网络动态来顺序构建高效的多播树。采用图注意力网络（GAT）作为编码器来提取上下文感知的节点嵌入，同时使用长短期记忆（LSTM）模块来建模路由决策中的序列依赖关系。大量模拟表明，该方法接近最优的基于动态规划的解决方案，同时显著降低了计算复杂度。结果还证实了该方法对大规模和动态网络拓扑的强泛化能力，突显了该方法在6G多媒体交付场景中实时部署的潜力。代码可在https://github.com/UNIC-Lab/GNN-Routing获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increase of bandwidth-intensive applications in sixth-generation (6G)wireless networks, such as real-time volumetric streaming and multi-sensoryextended reality, demands intelligent multicast routing solutions capable ofdelivering differentiated quality-of-service (QoS) at scale. Traditionalshortest-path and multicast routing algorithms are either computationallyprohibitive or structurally rigid, and they often fail to support heterogeneoususer demands, leading to suboptimal resource utilization. Neural network-basedapproaches, while offering improved inference speed, typically lack topologicalgeneralization and scalability. To address these limitations, this paperpresents a graph neural network (GNN)-based multicast routing framework thatjointly minimizes total transmission cost and supports user-specific videoquality requirements. The routing problem is formulated as a constrainedminimum-flow optimization task, and a reinforcement learning algorithm isdeveloped to sequentially construct efficient multicast trees by reusing pathsand adapting to network dynamics. A graph attention network (GAT) is employedas the encoder to extract context-aware node embeddings, while a longshort-term memory (LSTM) module models the sequential dependencies in routingdecisions. Extensive simulations demonstrate that the proposed method closelyapproximates optimal dynamic programming-based solutions while significantlyreducing computational complexity. The results also confirm stronggeneralization to large-scale and dynamic network topologies, highlighting themethod's potential for real-time deployment in 6G multimedia deliveryscenarios. Code is available at https://github.com/UNIC-Lab/GNN-Routing.</description>
      <author>example@mail.com (Xiucheng Wang, Zien Wang, Nan Cheng, Wenchao Xu, Wei Quan, Xuemin Shen)</author>
      <guid isPermaLink="false">2510.11109v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments</title>
      <link>http://arxiv.org/abs/2510.10954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 15th International Workshop on Structural Health Monitoring  (IWSHM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了图神经网络、卷积神经网络和标准前馈神经网络在预测人类空间偏好方面的可推广性，使用合成数据评估它们在未见环境中的表现。&lt;h4&gt;背景&lt;/h4&gt;预测人类在建成环境中的空间偏好对开发赛博物理社会基础设施系统至关重要，但偏好模型的可推广性是一个重大挑战，特别是在预测训练过程中未遇到的环境配置时。&lt;h4&gt;目的&lt;/h4&gt;确定哪种神经网络架构在推广到未见过的布局方面最有效，并进行不同神经网络架构的比较研究。&lt;h4&gt;方法&lt;/h4&gt;使用从简化的合成口袋公园环境生成的合成数据，评估模型预测受异构物理、环境和社会特征影响的偏好的能力，使用精确率-召回率曲线下面积计算可推广性分数。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习模型在学习复杂的空间和上下文依赖关系方面显示出潜力，但不同神经网络架构在推广到未见过的空间场景方面存在差异。&lt;h4&gt;结论&lt;/h4&gt;可推广性分数提供了关于每种神经网络架构在未见过的建成环境中进行偏好感知人类行为建模的适用性的见解。&lt;h4&gt;翻译&lt;/h4&gt;预测建成环境中人类空间偏好的能力对于开发赛博物理社会基础设施系统(CPSIS)至关重要。该领域的一个重大挑战是偏好模型的可推广性，特别是在预测训练过程中未遇到的环境配置时的有效性。虽然深度学习模型在学习复杂的空间和上下文依赖关系方面显示出潜力，但目前尚不清楚哪种神经网络架构在推广到未见过的布局方面最有效。为此，我们使用从简化的合成口袋公园环境生成的合成数据，对图神经网络、卷积神经网络和标准前馈神经网络进行了比较研究。从这个案例研究开始，可以控制分析每种模型将学习到的偏好模式转移到未见空间场景的能力。模型根据其预测受异构物理、环境和社会特征影响的偏好的能力进行评估。使用精确率-召回率曲线下面积计算可推广性分数，这种方法适用于不平衡数据，提供了关于每种神经网络架构在未见过的建成环境中进行偏好感知人类行为建模的适用性的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The capacity to predict human spatial preferences within built environmentsis instrumental for developing Cyber-Physical-Social Infrastructure Systems(CPSIS). A significant challenge in this domain is the generalizability ofpreference models, particularly their efficacy in predicting preferences withinenvironmental configurations not encountered during training. While deeplearning models have shown promise in learning complex spatial and contextualdependencies, it remains unclear which neural network architectures are mosteffective at generalizing to unseen layouts. To address this, we conduct acomparative study of Graph Neural Networks, Convolutional Neural Networks, andstandard feedforward Neural Networks using synthetic data generated from asimplified and synthetic pocket park environment. Beginning with thisillustrative case study, allows for controlled analysis of each model's abilityto transfer learned preference patterns to unseen spatial scenarios. The modelsare evaluated based on their capacity to predict preferences influenced byheterogeneous physical, environmental, and social features. Generalizabilityscore is calculated using the area under the precision-recall curve for theseen and unseen layouts. This generalizability score is appropriate forimbalanced data, providing insights into the suitability of each neural networkarchitecture for preference-aware human behavior modeling in unseen builtenvironments.</description>
      <author>example@mail.com (Maral Doctorarastoo, Katherine A. Flanigan, Mario Bergés, Christopher McComb)</author>
      <guid isPermaLink="false">2510.10954v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations</title>
      <link>http://arxiv.org/abs/2510.10864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图异质性与谱滤波器之间的关系，发现它们之间的联系比之前认为的更为复杂，提出了自适应滤波的必要性，并提出了一个简单而强大的GNN方法[METHOD NAME]，在实验中表现出色。&lt;h4&gt;背景&lt;/h4&gt;图异质性（连接节点具有不同标签）最近引起了广泛关注。大多数现有工作采用简化的方法——对同质性图使用低通滤波器，对异质性图使用高通滤波器。&lt;h4&gt;目的&lt;/h4&gt;探究图异质性与谱滤波器之间的复杂关系，设计能够适应不同异质性连接的自适应滤波器，以提高GNN在各类图上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了[METHOD NAME]，一个简单而强大的GNN方法，它能够提取异质性谱中的信息，并通过自适应混合来结合显著的表示。&lt;h4&gt;主要发现&lt;/h4&gt;图异质性与谱滤波器之间的关系更为复杂——最优滤波器响应在不同频率分量上有所不同，并且与异质性程度没有严格的单调相关性。GNNs的平均频率响应和图异质性程度之间没有严格的单调相关性。&lt;h4&gt;结论&lt;/h4&gt;需要自适应图滤波器来保证良好的泛化性能。[METHOD NAME]在同类质性和异质性图上相比领先基线实现了高达9.2%的准确率提升。&lt;h4&gt;翻译&lt;/h4&gt;图异质性，即连接的节点具有不同标签，最近引起了广泛关注。大多数现有工作采用简化的方法——对同质性图使用低通滤波器，对异质性图使用高通滤波器。然而，我们发现图异质性与谱滤波器之间的关系更为复杂——最优滤波器响应在不同频率分量上有所不同，并且与异质性程度没有严格的单调相关性。这一发现挑战了传统的固定滤波器设计，并表明需要自适应滤波来保持图嵌入的表达能力。正式地说，自然产生的问题有：给定一个异质性图G，G的异质性程度的变化如何以及会在多大程度上影响GNNs的性能？如何设计自适应滤波器来适应这些变化的异质性连接？我们的理论分析表明，GNNs的平均频率响应和图异质性程度之间没有严格的单调相关性，这需要自适应图滤波器来保证良好的泛化性能。因此，我们提出了[METHOD NAME]，一个简单而强大的GNN，它提取异质性谱中的信息，并通过自适应混合来结合显著的表示。[METHOD NAME]的优越性能在同类质性和异质性图上相比领先基线实现了高达9.2%的准确率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph heterophily, where connected nodes have different labels, has attractedsignificant interest recently. Most existing works adopt a simplified approach- using low-pass filters for homophilic graphs and high-pass filters forheterophilic graphs. However, we discover that the relationship between graphheterophily and spectral filters is more complex - the optimal filter responsevaries across frequency components and does not follow a strict monotoniccorrelation with heterophily degree. This finding challenges conventional fixedfilter designs and suggests the need for adaptive filtering to preserveexpressiveness in graph embeddings. Formally, natural questions arise: Given aheterophilic graph G, how and to what extent will the varying heterophilydegree of G affect the performance of GNNs? How can we design adaptive filtersto fit those varying heterophilic connections? Our theoretical analysis revealsthat the average frequency response of GNNs and graph heterophily degree do notfollow a strict monotonic correlation, necessitating adaptive graph filters toguarantee good generalization performance. Hence, we propose [METHOD NAME], asimple yet powerful GNN, which extracts information across the heterophilyspectrum and combines salient representations through adaptive mixing. [METHODNAME]'s superior performance achieves up to 9.2% accuracy improvement overleading baselines across homophilic and heterophilic graphs.</description>
      <author>example@mail.com (Shuaicheng Zhang, Haohui Wang, Junhong Lin, Xiaojie Guo, Yada Zhu, Si Zhang, Dongqi Fu, Dawei Zhou)</author>
      <guid isPermaLink="false">2510.10864v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion</title>
      <link>http://arxiv.org/abs/2510.10849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLANCE的自适应GNN-LLM融合框架，通过轻量级路由器选择性调用LLM来优化GNN预测，在异质节点上获得显著性能提升，同时保持整体性能最优。&lt;h4&gt;背景&lt;/h4&gt;文本属性图的学习促进了大型语言模型(LLMs)在图学习中的应用。然而，大多数融合策略在所有节点上统一应用，只获得较小的整体性能提升。&lt;h4&gt;目的&lt;/h4&gt;重新设计LLM-GNN融合框架，专注于GNN通常表现不佳的节点，提高图学习性能。&lt;h4&gt;方法&lt;/h4&gt;提出GLANCE(GNN with LLM Assistance for Neighbor- and Context-aware Embeddings)框架，使用轻量级路由器根据每个节点的低成本信号决定是否查询LLM。路由器使用基于优势的目标进行训练，比较查询LLM与仅依赖GNN的效用。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和LLM在性能上存在显著差异，各自在不同的结构模式上表现出色；GLANCE在节点子组之间实现了最佳的性能平衡，在异质节点上获得最高13%的性能提升，同时实现顶级整体性能。&lt;h4&gt;结论&lt;/h4&gt;自适应的、节点感知的GNN-LLM架构具有重要价值，选择性调用LLM enables在大图上的可扩展部署，而不会产生高计算成本。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图的学习激发了大型语言模型(LLMs)在图学习中的应用。然而，大多数融合策略统一应用于所有节点，仅获得较小的整体性能提升。我们认为这一结果源于聚合指标掩盖了LLMs何时提供益处，阻碍了新策略的可操作信号。在这项工作中，我们围绕GNN通常表现不佳的节点重新设计了LLM-GNN融合。我们首先展示了GNN和LLM在性能上可以显著不同，各自在不同的结构模式上表现出色，例如局部同质性。为了利用这一发现，我们提出了GLANCE(GNN with LLM Assistance for Neighbor- and Context-aware Embeddings)框架，该框架调用LLM来优化GNN的预测。GLANCE采用一个轻量级路由器，根据每个节点的低成本信号，决定是否查询LLM。由于LLM调用是不可微分的，路由器使用基于优势的目标进行训练，比较查询LLM与仅依赖GNN的效用。在多个基准测试中，GLANCE在节点子组之间实现了最佳的性能平衡，在异质节点上获得显著提升(最高+13%)，同时实现顶级整体性能。我们的研究结果表明自适应的、节点感知的GNN-LLM架构的价值，选择性调用LLM enables在大图上的可扩展部署，而不会产生高计算成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning on text-attributed graphs has motivated the use of Large LanguageModels (LLMs) for graph learning. However, most fusion strategies are applieduniformly across all nodes and attain only small overall performance gains. Weargue this result stems from aggregate metrics that obscure when LLMs providebenefit, inhibiting actionable signals for new strategies. In this work, wereframe LLM-GNN fusion around nodes where GNNs typically falter. We first showthat performance can significantly differ between GNNs and LLMs, with eachexcelling on distinct structural patterns, such as local homophily. To leveragethis finding, we propose GLANCE (GNN with LLM Assistance for Neighbor- andContext-aware Embeddings), a framework that invokes an LLM to refine a GNN'sprediction. GLANCE employs a lightweight router that, given inexpensiveper-node signals, decides whether to query the LLM. Since the LLM calls arenon-differentiable, the router is trained with an advantage-based objectivethat compares the utility of querying the LLM against relying solely on theGNN. Across multiple benchmarks, GLANCE achieves the best performance balanceacross node subgroups, achieving significant gains on heterophilous nodes (upto $+13\%$) while simultaneously achieving top overall performance. Ourfindings highlight the value of adaptive, node-aware GNN-LLM architectures,where selectively invoking the LLM enables scalable deployment on large graphswithout incurring high computational costs.</description>
      <author>example@mail.com (Donald Loveland, Yao-An Yang, Danai Koutra)</author>
      <guid isPermaLink="false">2510.10849v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Fast and the Furious: Hot Starts in Pursuit-Evasion Games</title>
      <link>http://arxiv.org/abs/2510.10830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at AAMAS Workshop on Autonomous Robots and Multirobot  Systems (ARMS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合博弈论控制理论与图神经网络的新方法，用于解决在没有事先了解逃亡者位置情况下的追捕者部署问题。通过构建图特征空间和训练图卷积网络生成战略有效的初始配置，显著提高了追捕效率。&lt;h4&gt;背景&lt;/h4&gt;在追逃游戏中，没有事先了解逃亡者位置的情况下有效部署追捕者仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，使追捕者能够在没有先验知识的情况下有效地部署，提高追捕效率。&lt;h4&gt;方法&lt;/h4&gt;结合博弈论控制理论与图神经网络，将追捕者配置表示为图，通过多目标优化构建图特征空间识别帕累托最优配置，并在这些最优图上训练图卷积网络生成'热启动'配置。&lt;h4&gt;主要发现&lt;/h4&gt;经验评估表明，GCN生成的热启动相比随机配置具有显著优势；在多追捕者和多逃亡者场景中，该方法加速了逃亡者生存率下降，减少了追捕者移动距离，并增强了围捕效果。&lt;h4&gt;结论&lt;/h4&gt;该方法在追逃游戏中展示了明显的战略优势，能有效提高追捕效率。&lt;h4&gt;翻译&lt;/h4&gt;在没有逃亡者位置先验知识的情况下，在追逃游戏中有效部署追捕者仍然是一个重大挑战。本文介绍了一种结合博弈论控制理论与图神经网络的新方法。通过将追捕者配置概念化为战略安排并表示为图，通过多目标优化构建图特征空间以识别帕累托最优配置。在这些帕累托最优图上训练图卷积网络(GCN)，生成战略上有效的初始配置，称为'热启动'。经验评估表明，GCN生成的热启动相比随机配置具有显著优势。在考虑多个追捕者和逃亡者的场景中，这种方法加速了逃亡者生存率的下降，减少了追捕者的移动距离，并增强了围捕效果，展示了明显的战略优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effectively positioning pursuers in pursuit-evasion games without priorknowledge of evader locations remains a significant challenge. A novel approachthat combines game-theoretic control theory with Graph Neural Networks isintroduced in this work. By conceptualizing pursuer configurations as strategicarrangements and representing them as graphs, a Graph Characteristic Space isconstructed via multi-objective optimization to identify Pareto-optimalconfigurations. A Graph Convolutional Network (GCN) is trained on thesePareto-optimal graphs to generate strategically effective initialconfigurations, termed "hot starts". Empirical evaluations demonstrate that theGCN-generated hot starts provide a significant advantage over randomconfigurations. In scenarios considering multiple pursuers and evaders, thismethod hastens the decline in evader survival rates, reduces pursuer traveldistances, and enhances containment, showcasing clear strategic benefits.</description>
      <author>example@mail.com (Gabriel Smithline, Scott Nivison)</author>
      <guid isPermaLink="false">2510.10830v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction</title>
      <link>http://arxiv.org/abs/2510.10775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OmniGNN，一个基于注意力的多关系动态图神经网络，通过异构节点和边类型整合宏观经济背景，实现稳健的消息传递，特别在宏观经济冲击期间表现出色。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在金融市场中已被成功应用于建模关系数据，有效捕捉股票间的非线性依赖关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型在宏观经济冲击期间无法有效传播消息的问题，提出一个能够整合宏观经济背景的稳健图神经网络模型。&lt;h4&gt;方法&lt;/h4&gt;引入一个作为全局中介的行业节点实现快速冲击传播；利用图注意力网络(GAT)加权邻居贡献；采用Transformer捕捉多关系间的时间动态。&lt;h4&gt;主要发现&lt;/h4&gt;OmniGNN在公共数据集上的股票预测模型中表现优于现有模型，特别是在COVID-19期间表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;OmniGNN通过整合宏观经济背景和优化的消息传递机制，显著提升了股票预测模型的性能，特别是在宏观经济冲击期间的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;在金融市场中，图神经网络已被成功应用于建模关系数据，有效捕捉股票间的非线性依赖关系。然而，现有模型通常在宏观经济冲击期间无法有效传播消息。在本文中，我们提出了OmniGNN，一个基于注意力的多关系动态图神经网络，通过异构节点和边类型整合宏观经济背景，实现稳健的消息传递。OmniGNN的核心是一个作为全局中介的行业节点，使冲击能够在图中快速传播，而不依赖长距离多跳扩散。该模型利用图注意力网络(GAT)来加权邻居的贡献，并采用Transformer来捕捉多关系间的时间动态。实验表明，OmniGNN在公共数据集上的股票预测模型中优于现有模型，特别是在COVID-19期间表现出强大的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In financial markets, Graph Neural Networks have been successfully applied tomodeling relational data, effectively capturing nonlinear inter-stockdependencies. Yet, existing models often fail to efficiently propagate messagesduring macroeconomic shocks. In this paper, we propose OmniGNN, anattention-based multi-relational dynamic GNN that integrates macroeconomiccontext via heterogeneous node and edge types for robust message passing.Central to OmniGNN is a sector node acting as a global intermediary, enablingrapid shock propagation across the graph without relying on long-rangemulti-hop diffusion. The model leverages Graph Attention Networks (GAT) toweigh neighbor contributions and employs Transformers to capture temporaldynamics across multiplex relations. Experiments show that OmniGNN outperformsexisting stock prediction models on public datasets, particularly demonstratingstrong robustness during the COVID-19 period.</description>
      <author>example@mail.com (Amber Li, Aruzhan Abil, Juno Marques Oda)</author>
      <guid isPermaLink="false">2510.10775v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Mapping the Urban Mobility Intelligence Frontier: A Scientometric Analysis of Data-Driven Pedestrian Trajectory Prediction and Simulation</title>
      <link>http://arxiv.org/abs/2510.10327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对数据驱动的行人轨迹预测和人群模拟进行了全面的文献计量分析，揭示了人工智能、城市信息和人群行为建模之间的融合趋势，并探讨了这些技术如何应用于城市移动设计、公共安全规划和智慧城市数字孪生开发。&lt;h4&gt;背景&lt;/h4&gt;理解和预测行人动力学对于塑造更安全、更响应性、以人为中心的城市环境至关重要。随着人工智能技术的发展，数据驱动的行人轨迹预测和人群模拟研究日益增多，需要系统梳理该领域的发展脉络。&lt;h4&gt;目的&lt;/h4&gt;通过文献计量分析，绘制数据驱动的行人轨迹预测和人群模拟研究的知识演进图和跨学科结构，识别主要趋势、有影响力的贡献者和新兴前沿领域。&lt;h4&gt;方法&lt;/h4&gt;使用Web of Science核心合集的文献计量数据，采用SciExplorer和Bibliometrix工具进行分析，识别该领域的主要研究趋势、有影响力的贡献者和新兴前沿领域。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现人工智能、城市信息和人群行为建模之间存在强烈的融合趋势，这种融合由图神经网络、transformers和生成模型驱动。除了技术进步外，该领域越来越多地为城市移动设计、公共安全规划和智慧城市数字孪生开发提供信息。然而，该领域在确保可解释性、包容性和跨领域可转移性方面仍然存在挑战。&lt;h4&gt;结论&lt;/h4&gt;通过将方法轨迹与城市应用联系起来，这项工作强调了数据驱动方法如何丰富城市治理，并为未来城市的自适应、社会负责的移动智能铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;理解和预测行人动力学对于塑造更安全、更具响应性、以人为中心的城市环境已成为必不可少的工作。本研究对数据驱动的行人轨迹预测和人群模拟研究进行了全面的文献计量分析，绘制了其知识演进和跨学科结构。利用Web of Science核心合集的文献计量数据，我们采用SciExplorer和Bibliometrix来识别主要趋势、有影响力的贡献者和新兴前沿。结果表明，人工智能、城市信息和人群行为建模之间存在强烈的融合趋势，这种趋势由图神经网络、transformers和生成模型驱动。除了技术进步外，该领域越来越多地影响着城市移动设计、公共安全规划和智慧城市数字孪生开发。然而，在确保可解释性、包容性和跨领域可转移性方面仍然存在挑战。通过将方法轨迹与城市应用联系起来，这项工作强调了数据驱动方法如何丰富城市治理，并为未来城市的自适应、社会负责的移动智能铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and predicting pedestrian dynamics has become essential forshaping safer, more responsive, and human-centered urban environments. Thisstudy conducts a comprehensive scientometric analysis of research ondata-driven pedestrian trajectory prediction and crowd simulation, mapping itsintellectual evolution and interdisciplinary structure. Using bibliometric datafrom the Web of Science Core Collection, we employ SciExplorer and Bibliometrixto identify major trends, influential contributors, and emerging frontiers.Results reveal a strong convergence between artificial intelligence, urbaninformatics, and crowd behavior modeling--driven by graph neural networks,transformers, and generative models. Beyond technical advances, the fieldincreasingly informs urban mobility design, public safety planning, and digitaltwin development for smart cities. However, challenges remain in ensuringinterpretability, inclusivity, and cross-domain transferability. By connectingmethodological trajectories with urban applications, this work highlights howdata-driven approaches can enrich urban governance and pave the way foradaptive, socially responsible mobility intelligence in future cities.</description>
      <author>example@mail.com (Junhao Xu, Hui Zeng)</author>
      <guid isPermaLink="false">2510.10327v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Preference-driven Knowledge Distillation for Few-shot Node Classification</title>
      <link>http://arxiv.org/abs/2510.10116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种偏好驱动的知识蒸馏(PKD)框架，结合大型语言模型和图神经网络的优势，用于少样本节点分类任务。&lt;h4&gt;背景&lt;/h4&gt;图神经网络能高效处理带文本属性的图，但训练依赖人工标注标签；现实世界中TAGs节点的复杂多样局部拓扑结构使单一机制难以处理；大型语言模型在TAGs的零样本/少样本学习中表现良好，但面临可扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;协同大型语言模型和多种图神经网络的优势，解决少样本节点分类问题。&lt;h4&gt;方法&lt;/h4&gt;提出偏好驱动的知识蒸馏框架，包括：(1)GNN偏好驱动的节点选择器，促进从LLMs到教师GNN的预测蒸馏；(2)节点偏好驱动的GNN选择器，为每个节点识别最合适的教师GNN，实现定制化知识蒸馏。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在真实世界TAGs的少样本节点分类任务中表现出色，能够有效处理节点复杂的局部拓扑结构。&lt;h4&gt;结论&lt;/h4&gt;PKD框架能够有效结合大型语言模型和图神经网络的优势，解决少样本节点分类问题，并在真实数据集上验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)由于其消息传递机制能够高效处理带文本属性的图(TAGs)，但它们的训练严重依赖人工标注的标签。此外，现实世界中TAGs节点的复杂多样的局部拓扑结构使得单一机制难以处理。大型语言模型(LLMs)在TAGs的零样本/少样本学习中表现良好，但面临可扩展性挑战。因此，我们提出了一种偏好驱动的知识蒸馏(PKD)框架，协同大型语言模型和多种图神经网络的优势用于少样本节点分类。具体而言，我们开发了一个GNN偏好驱动的节点选择器，有效促进从LLMs到教师GNN的预测蒸馏。为进一步处理节点复杂的局部拓扑结构，我们开发了一个节点偏好驱动的GNN选择器，为每个节点识别最合适的教师GNN，从而促进从教师GNN到学生GNN的定制化知识蒸馏。大量实验验证了我们所提出的框架在真实世界TAGs的少样本节点分类中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) can efficiently process text-attributed graphs(TAGs) due to their message-passing mechanisms, but their training heavilyrelies on the human-annotated labels. Moreover, the complex and diverse localtopologies of nodes of real-world TAGs make it challenging for a singlemechanism to handle. Large language models (LLMs) perform well inzero-/few-shot learning on TAGs but suffer from a scalability challenge.Therefore, we propose a preference-driven knowledge distillation (PKD)framework to synergize the complementary strengths of LLMs and various GNNs forfew-shot node classification. Specifically, we develop a GNN-preference-drivennode selector that effectively promotes prediction distillation from LLMs toteacher GNNs. To further tackle nodes' intricate local topologies, we develop anode-preference-driven GNN selector that identifies the most suitable teacherGNN for each node, thereby facilitating tailored knowledge distillation fromteacher GNNs to the student GNN. Extensive experiments validate the efficacy ofour proposed framework in few-shot node classification on real-world TAGs.</description>
      <author>example@mail.com (Xing Wei, Chunchun Chen, Rui Fan, Xiaofeng Cao, Sourav Medya, Wei Ye)</author>
      <guid isPermaLink="false">2510.10116v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Structure-Aware Attention and Knowledge Graphs in Explainable Recommendation Systems</title>
      <link>http://arxiv.org/abs/2510.10109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文设计并实现了一个结合知识图谱和结构感知注意力机制的可解释推荐模型，该模型基于图神经网络，采用多跳邻居聚合策略，能够捕获用户隐式偏好关系。&lt;h4&gt;背景&lt;/h4&gt;推荐系统领域需要更有效地利用知识图谱信息来提高推荐性能，同时需要模型具有可解释性。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够整合知识图谱结构信息，并通过注意力机制动态分配邻居重要性的推荐模型，以提高推荐的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;构建基于图神经网络的推荐模型，将用户和项目嵌入统一图结构，利用知识图谱构建多级语义路径提取上下文信息，通过用户和项目表示交互生成推荐，使用二元交叉熵损失函数优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;在Amazon Books数据集上的实验表明，所提出模型在各种评估指标上表现优越，具有良好的收敛性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;结构感知注意力机制在知识图谱增强推荐中具有有效性和实用性，能够提高推荐性能并提供更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文设计并实现了一个可解释的推荐模型，该模型将知识图谱与结构感知注意力机制相结合。该模型基于图神经网络构建，并采用了多跳邻居聚合策略。通过整合知识图谱的结构信息，并通过注意力机制动态分配不同邻居的重要性，模型增强了捕获隐式偏好关系的能力。在所提出的方法中，用户和项目被嵌入到统一的图结构中。基于知识图谱中的实体和关系构建多级语义路径，以提取更丰富的上下文信息。在评分预测阶段，通过用户和目标项目表示之间的交互生成推荐。模型使用二元交叉熵损失函数进行优化。在Amazon Books数据集上进行的实验验证了所提出模型在各种评估指标上的优越性能。该模型还表现出良好的收敛性和稳定性。这些结果进一步证明了结构感知注意力机制在知识图谱增强推荐中的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper designs and implements an explainable recommendation model thatintegrates knowledge graphs with structure-aware attention mechanisms. Themodel is built on graph neural networks and incorporates a multi-hop neighboraggregation strategy. By integrating the structural information of knowledgegraphs and dynamically assigning importance to different neighbors through anattention mechanism, the model enhances its ability to capture implicitpreference relationships. In the proposed method, users and items are embeddedinto a unified graph structure. Multi-level semantic paths are constructedbased on entities and relations in the knowledge graph to extract richercontextual information. During the rating prediction phase, recommendations aregenerated through the interaction between user and target item representations.The model is optimized using a binary cross-entropy loss function. Experimentsconducted on the Amazon Books dataset validate the superior performance of theproposed model across various evaluation metrics. The model also shows goodconvergence and stability. These results further demonstrate the effectivenessand practicality of structure-aware attention mechanisms in knowledgegraph-enhanced recommendation.</description>
      <author>example@mail.com (Shuangquan Lyu, Ming Wang, Huajun Zhang, Jiasen Zheng, Junjiang Lin, Xiaoxuan Sun)</author>
      <guid isPermaLink="false">2510.10109v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Lighter-X: An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation</title>
      <link>http://arxiv.org/abs/2510.10105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Lighter-X框架，解决了传统图神经网络推荐系统在大规模部署中的可扩展性问题，通过高效压缩和解耦框架实现了参数和计算复杂度的显著降低。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在推荐系统中表现优异，但传统方法如LightGCN需要为每个节点维护嵌入向量，导致参数复杂度为O(n×d)，其中n是用户和物品总数，这在大规模应用中面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效且模块化的框架，能够无缝集成到现有GNN推荐器架构中，减少参数大小和计算复杂度，同时保持理论保证和经验性能，实现大规模实际部署。&lt;h4&gt;方法&lt;/h4&gt;分析原始结构和参数中的固有冗余，提出稀疏邻接结构和高维嵌入矩阵的高效压缩方案，将参数复杂度从O(n×d)降低到O(h×d)（h&lt;&lt;n），并通过解耦框架优化模型，减少训练过程中的计算复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;Lighter-X在保持与基线模型相当性能的同时，显著减少了参数需求；在有数百万条边的大规模交互图上，仅使用LightGCN 1%的参数就能获得更好的结果。&lt;h4&gt;结论&lt;/h4&gt;Lighter-X框架能够在保持推荐性能的同时大幅降低参数需求，使大规模图神经网络推荐系统的实际部署成为可能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在推荐系统中已展现出显著的有效性。然而，传统的基于图的推荐系统，如LightGCN，需要为每个节点维护大小为d的嵌入，导致参数复杂度为O(n×d)，其中n代表用户和物品的总数。这种扩展模式在实际应用的大规模图部署中构成了重大挑战。为解决这一可扩展性限制，我们提出了Lighter-X，这是一个高效且模块化的框架，可以无缝集成到现有的基于GNN的推荐器架构中。我们的方法显著减少了参数大小和计算复杂度，同时保留了基础模型的理论保证和经验性能，从而实现了大规模的实际部署。具体而言，我们分析了原始结构和参数中的固有冗余，识别了优化机会。基于这一洞察，我们提出了稀疏邻接结构和高维嵌入矩阵的高效压缩方案，实现了O(h×d)的参数复杂度，其中h&lt;&lt;n。此外，模型通过解耦框架进行优化，减少了训练过程中的计算复杂度并提高了可扩展性。大量实验表明，Lighter-X以显著更少的参数实现了与基线模型相当的性能。特别是在有数百万条边的大规模交互图上，我们仅使用LightGCN 1%的参数就能获得更好的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness inrecommendation systems. However, conventional graph-based recommenders, such asLightGCN, require maintaining embeddings of size $d$ for each node, resultingin a parameter complexity of $\mathcal{O}(n \times d)$, where $n$ representsthe total number of users and items. This scaling pattern poses significantchallenges for deployment on large-scale graphs encountered in real-worldapplications. To address this scalability limitation, we propose\textbf{Lighter-X}, an efficient and modular framework that can be seamlesslyintegrated with existing GNN-based recommender architectures. Our approachsubstantially reduces both parameter size and computational complexity whilepreserving the theoretical guarantees and empirical performance of the basemodels, thereby enabling practical deployment at scale. Specifically, weanalyze the original structure and inherent redundancy in their parameters,identifying opportunities for optimization. Based on this insight, we proposean efficient compression scheme for the sparse adjacency structure andhigh-dimensional embedding matrices, achieving a parameter complexity of$\mathcal{O}(h \times d)$, where $h \ll n$. Furthermore, the model is optimizedthrough a decoupled framework, reducing computational complexity during thetraining process and enhancing scalability. Extensive experiments demonstratethat Lighter-X achieves comparable performance to baseline models withsignificantly fewer parameters. In particular, on large-scale interactiongraphs with millions of edges, we are able to attain even better results withonly 1\% of the parameter over LightGCN.</description>
      <author>example@mail.com (Yanping Zheng, Zhewei Wei, Frank de Hoog, Xu Chen, Hongteng Xu, Yuhang Ye, Jiadeng Huang)</author>
      <guid isPermaLink="false">2510.10105v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Rademacher Meets Colors: More Expressivity, but at What Cost ?</title>
      <link>http://arxiv.org/abs/2510.10101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过着色算法的视角，揭示了图神经网络表达能力和泛化能力之间的权衡关系，证明WL着色诱导的等价类数量直接限制了GNN的Rademacher复杂度，从而解释了为什么更强的表达能力往往导致更弱的泛化保证。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)的表达能力通常通过它们与图同构测试(如Weisfeiler-Leman层次结构)的对应关系来理解。更具表达能力的GNN能够区分更多种类的图，但也观察到它们有更高的泛化误差。&lt;h4&gt;目的&lt;/h4&gt;提供图神经网络表达能力和泛化能力之间权衡关系的理论解释，统一表达能力和泛化的研究，为增加表达能力以泛化为代价的现象提供原则性理解。&lt;h4&gt;方法&lt;/h4&gt;通过着色算法的视角，将表达能力和泛化能力联系起来，分析WL着色诱导的等价类数量对GNN的Rademacher复杂度的影响，并研究Rademacher复杂度在不同样本的颜色计数扰动下的稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;1) WL着色诱导的等价类数量直接限制了GNN的Rademacher复杂度；2) 更强的表达能力导致更高的复杂度，从而更弱的泛化保证；3) Rademacher复杂度在不同样本的颜色计数扰动下是稳定的，确保了数据集间采样变异性的鲁棒性；4) 该框架适用于任意GNN架构和表达能力度量。&lt;h4&gt;结论&lt;/h4&gt;图神经网络的表达能力和泛化能力之间存在权衡关系，增加表达能力通常以泛化为代价。这一发现为理解和设计GNN提供了重要指导，强调了在追求高表达能力的同时需要考虑泛化性能的重要性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)的表达能力通常通过它们与图同构测试(如Weisfeiler-Leman(WL)层次结构)的对应关系来理解。虽然更具表达能力的GNN能够区分更多种类的图，但也观察到它们有更高的泛化误差。这项工作通过着色算法的视角为这种权衡提供了理论解释。具体来说，我们证明WL着色诱导的等价类数量直接限制了GNN的Rademacher复杂度——这是泛化的一个关键数据相关度量。我们的分析表明，更强的表达能力导致更高的复杂度，从而更弱的泛化保证。此外，我们证明了Rademacher复杂度在不同样本的颜色计数扰动下是稳定的，确保了数据集间采样变异性的鲁棒性。重要的是，我们的框架不仅限于消息传递GNN或1-WL，还扩展到将图划分为等价类的任意GNN架构和表达能力度量。这些结果统一了GNN中表达能力和泛化的研究，为为什么增加表达能力通常以泛化为代价提供了原则性理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The expressive power of graph neural networks (GNNs) is typically understoodthrough their correspondence with graph isomorphism tests such as theWeisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish aricher set of graphs, they are also observed to suffer from highergeneralization error. This work provides a theoretical explanation for thistrade-off by linking expressivity and generalization through the lens ofcoloring algorithms. Specifically, we show that the number of equivalenceclasses induced by WL colorings directly bounds the GNNs Rademacher complexity-- a key data-dependent measure of generalization. Our analysis reveals thatgreater expressivity leads to higher complexity and thus weaker generalizationguarantees. Furthermore, we prove that the Rademacher complexity is stableunder perturbations in the color counts across different samples, ensuringrobustness to sampling variability across datasets. Importantly, our frameworkis not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNNarchitectures and expressivity measures that partition graphs into equivalenceclasses. These results unify the study of expressivity and generalization inGNNs, providing a principled understanding of why increasing expressive poweroften comes at the cost of generalization.</description>
      <author>example@mail.com (Martin Carrasco, Caio Deberaldini Netto, Vahan A. Martirosyan, Aneeqa Mehrab, Ehimare Okoyomon, Caterina Graziani)</author>
      <guid isPermaLink="false">2510.10101v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Learning Joint Embeddings of Function and Process Call Graphs for Malware Detection</title>
      <link>http://arxiv.org/abs/2510.09984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GeminiNet的统一神经网络方法，用于从函数调用图(FCGs)和进程调用图(PCGs)中学习联合嵌入，实现对软件系统的多视角分析。&lt;h4&gt;背景&lt;/h4&gt;软件系统可以表示为图来捕获函数和进程间的依赖关系，根据不同目标可构建不同类型的图，如函数调用图和进程交互图。虽然这些图相关但视角不同，提供互补信息。先前研究多关注单一图表示，联合建模两种图的方法研究不足。&lt;h4&gt;目的&lt;/h4&gt;探索对函数调用图和进程调用图进行联合建模，实现对软件系统更深层次、多视角的分析。&lt;h4&gt;方法&lt;/h4&gt;提出GeminiNet统一神经网络方法，构建包含635个Windows可执行文件(318个恶意和317个良性)的数据集，使用Ghidra提取FCGs，Any.Run沙箱提取PCGs。采用双图卷积分支和自适应门控机制，平衡静态和动态视图的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;联合嵌入方法优于单图模型，能够提供更全面的软件系统分析。&lt;h4&gt;结论&lt;/h4&gt;通过联合建模函数调用图和进程调用图，可以实现对软件系统更全面、准确的分析和理解，有助于软件行为分析和安全检测。&lt;h4&gt;翻译&lt;/h4&gt;软件系统可以表示为图，捕获函数和进程之间的依赖关系。软件系统的一个有趣方面是，根据提取目标和优先级的不同，它们可以表示为不同类型的图。例如，可以捕获软件内的函数调用以创建函数调用图，突出函数之间的关系和依赖。或者，可以对软件生成的进程进行建模，生成进程交互图，关注运行时行为和进程间通信。虽然这些图表示相关，但每个都捕获了系统的不同视角，提供了对其结构和操作的互补见解。虽然先前的研究利用图神经网络分析软件行为，但大多数工作只关注单一类型的图表示。函数调用图和进程交互图的联合建模在很大程度上仍未被探索，留下了对软件系统进行更深层次、多视角分析的机会。本文提出了一个构建和训练函数调用图和进程调用图以及学习联合嵌入的流程。我们证明联合嵌入优于单图模型。在本文中，我们提出了GeminiNet，一种统一的神经网络方法，用于从函数调用图和进程调用图中学习联合嵌入。我们构建了一个包含635个Windows可执行文件的新数据集，使用Ghidra提取函数调用图，使用Any.Run沙箱提取进程调用图。GeminiNet采用双图卷积分支和自适应门控机制，以平衡静态和动态视图的贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Software systems can be represented as graphs, capturing dependencies amongfunctions and processes. An interesting aspect of software systems is that theycan be represented as different types of graphs, depending on the extractiongoals and priorities. For example, function calls within the software can becaptured to create function call graphs, which highlight the relationshipsbetween functions and their dependencies. Alternatively, the processes spawnedby the software can be modeled to generate process interaction graphs, whichfocus on runtime behavior and inter-process communication. While these graphrepresentations are related, each captures a distinct perspective of thesystem, providing complementary insights into its structure and operation.While previous studies have leveraged graph neural networks (GNNs) to analyzesoftware behaviors, most of this work has focused on a single type of graphrepresentation. The joint modeling of both function call graphs and processinteraction graphs remains largely underexplored, leaving opportunities fordeeper, multi-perspective analysis of software systems. This paper presents apipeline for constructing and training Function Call Graphs (FCGs) and ProcessCall Graphs (PCGs) and learning joint embeddings. We demonstrate that jointembeddings outperform a single-graph model. In this paper, we proposeGeminiNet, a unified neural network approach that learns joint embeddings fromboth FCGs and PCGs. We construct a new dataset of 635 Windows executables (318malicious and 317 benign), extracting FCGs via Ghidra and PCGs via Any.Runsandbox. GeminiNet employs dual graph convolutional branches with an adaptivegating mechanism that balances contributions from static and dynamic views.</description>
      <author>example@mail.com (Kartikeya Aneja, Nagender Aneja, Murat Kantarcioglu)</author>
      <guid isPermaLink="false">2510.09984v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Phase-Aware Deep Learning with Complex-Valued CNNs for Audio Signal Applications</title>
      <link>http://arxiv.org/abs/2510.09926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了复值卷积神经网络(CVCNNs)在音频信号处理中的设计与应用，重点关注实值网络中常被忽略的相位信息的保留和利用。通过理论基础介绍、训练技术调整和三个阶段的实验评估，证明了复值架构的表现能力和相位作为音频处理中可利用特征的价值。&lt;h4&gt;背景&lt;/h4&gt;在音频信号处理中，相位信息通常被实值神经网络所忽略，而复值神经网络为保留和利用这些信息提供了可能。&lt;h4&gt;目的&lt;/h4&gt;研究复值卷积神经网络(CVCNNs)在音频信号处理中的设计和应用，探索如何有效利用通常被实值网络忽略的相位信息。&lt;h4&gt;方法&lt;/h4&gt;介绍CVCNNs的基础理论概念(复卷积、池化层、基于Wirtinger的微分和复值激活函数)，调整训练技术(复值批归一化和权重初始化方案)，并通过三个阶段实验评估：在图像数据集上基准测试、使用MFCC进行音频分类、引入GNN通过边权重建模相位信息。&lt;h4&gt;主要发现&lt;/h4&gt;CVCNNs在图像数据集上表现与实值CNN相当；在音频分类中，CVCNNs在实值MFCC上训练时略微优于实值CNN，但保留相位存在挑战；包含相位的GNNs在二元和多流派分类中带来可衡量的性能提升；心脏形激活函数等方法显示出前景。&lt;h4&gt;结论&lt;/h4&gt;复值架构具有强大的表达能力，相位是音频处理中一个有意义且可利用的特征。未来在相位感知设计方面的进步对于利用复表示在神经网络中的潜力至关重要。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了复值卷积神经网络(CVCNNs)在音频信号处理中的设计与应用，重点关注实值网络中常被忽略的相位信息的保留和利用。我们首先介绍CVCNNs的基础理论概念，包括复卷积、池化层、基于Wirtinger的微分以及各种复值激活函数。这些理论概念辅以关键的训练技术调整，包括复值批归一化和权重初始化方案，以确保训练动力学的稳定性。实证评估分为三个阶段进行。首先，在标准图像数据集上对CVCNNs进行基准测试，它们表现出与实值CNNs相当的竞争力，即使在合成复扰动下也是如此。在第二个实验中，我们专注于使用梅尔频率倒谱系数(MFCC)进行音频分类。在实值MFCC上训练的CVCNNs略微优于实值CNN，而在输入工作流中保留相位则凸显了在没有架构修改的情况下利用相位的挑战。最后，第三个实验引入了GNNs通过边权重建模相位信息，其中包含相位在二元和多流派分类中都带来了可衡量的性能提升。这些结果强调了复值架构的表现能力，并确认了相位是音频处理应用中一个有意义且可利用的特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study explores the design and application of Complex-ValuedConvolutional Neural Networks (CVCNNs) in audio signal processing, with a focuson preserving and utilizing phase information often neglected in real-valuednetworks. We begin by presenting the foundational theoretical concepts ofCVCNNs, including complex convolutions, pooling layers, Wirtinger-baseddifferentiation, and various complex-valued activation functions. These arecomplemented by critical adaptations of training techniques, including complexbatch normalization and weight initialization schemes, to ensure stability intraining dynamics. Empirical evaluations are conducted across three stages.First, CVCNNs are benchmarked on standard image datasets, where theydemonstrate competitive performance with real-valued CNNs, even under syntheticcomplex perturbations. Although our focus is audio signal processing, we firstevaluate CVCNNs on image datasets to establish baseline performance andvalidate training stability before applying them to audio tasks. In the secondexperiment, we focus on audio classification using Mel-Frequency CepstralCoefficients (MFCCs). CVCNNs trained on real-valued MFCCs slightly outperformreal CNNs, while preserving phase in input workflows highlights challenges inexploiting phase without architectural modifications. Finally, a thirdexperiment introduces GNNs to model phase information via edge weighting, wherethe inclusion of phase yields measurable gains in both binary and multi-classgenre classification. These results underscore the expressive capacity ofcomplex-valued architectures and confirm phase as a meaningful and exploitablefeature in audio processing applications. While current methods show promise,especially with activations like cardioid, future advances in phase-awaredesign will be essential to leverage the potential of complex representationsin neural networks.</description>
      <author>example@mail.com (Naman Agrawal)</author>
      <guid isPermaLink="false">2510.09926v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering</title>
      <link>http://arxiv.org/abs/2510.09854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为NG-Router的新框架，用于解决营养问答系统中的推理能力有限和上下文过载问题，通过知识图引导的多智能体协作提高系统性能。&lt;h4&gt;背景&lt;/h4&gt;饮食在人类健康中起核心作用，营养问答系统为个性化饮食指导和预防饮食相关慢性疾病提供了有前景的路径。然而，现有方法面临单智能体系统推理能力有限、设计有效多智能体架构复杂以及上下文过载阻碍准确决策等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理复杂营养健康任务的多智能体推理框架，解决现有营养问答系统的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出Nutritional-Graph Router (NG-Router)框架，将营养问答视为有监督的、知识图引导的多智能体协作问题。该框架将智能体节点整合到异构知识图中，使用图神经网络学习任务感知的路由分布，并采用基于梯度的子图检索机制来解决上下文过载问题。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试和骨干模型上的实验表明，NG-Router始终优于单智能体和集成基线方法，能够有效增强多跳和关系推理能力。&lt;h4&gt;结论&lt;/h4&gt;NG-Router为复杂营养健康任务提供了一种有原则的领域感知多智能体推理方法，代表了营养问答系统的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;饮食在人类健康中扮演核心角色，营养问答为个性化饮食指导和预防饮食相关慢性疾病提供了有前景的路径。然而，现有方法面临两个基本挑战：单智能体系统的有限推理能力以及设计有效多智能体架构的复杂性，还有阻碍准确决策的上下文过载。我们引入了营养图路由器，这是一个新框架，将营养问答制定为一个有监督的、知识图引导的多智能体协作问题。营养图路由器将智能体节点整合到异构知识图中，并采用图神经网络来学习智能体上的任务感知路由分布，利用从经验智能体性能中获得的软监督。为了进一步解决上下文过载，我们提出了一种基于梯度的子图检索机制，在训练过程中识别显著证据，从而增强多跳和关系推理。在多个基准测试和骨干模型上的广泛实验表明，营养图路由器始终优于单智能体和集成基线，为复杂营养健康任务提供了一种有原则的领域感知多智能体推理方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diet plays a central role in human health, and Nutrition Question Answering(QA) offers a promising path toward personalized dietary guidance and theprevention of diet-related chronic diseases. However, existing methods face twofundamental challenges: the limited reasoning capacity of single-agent systemsand the complexity of designing effective multi-agent architectures, as well ascontextual overload that hinders accurate decision-making. We introduceNutritional-Graph Router (NG-Router), a novel framework that formulatesnutritional QA as a supervised, knowledge-graph-guided multi-agentcollaboration problem. NG-Router integrates agent nodes into heterogeneousknowledge graphs and employs a graph neural network to learn task-aware routingdistributions over agents, leveraging soft supervision derived from empiricalagent performance. To further address contextual overload, we propose agradient-based subgraph retrieval mechanism that identifies salient evidenceduring training, thereby enhancing multi-hop and relational reasoning.Extensive experiments across multiple benchmarks and backbone modelsdemonstrate that NG-Router consistently outperforms both single-agent andensemble baselines, offering a principled approach to domain-aware multi-agentreasoning for complex nutritional health tasks.</description>
      <author>example@mail.com (Kaiwen Shi, Zheyuan Zhang, Zhengqing Yuan, Keerthiram Murugesan, Vincent Galass, Chuxu Zhang, Yanfang Ye)</author>
      <guid isPermaLink="false">2510.09854v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Geo-Aware Models for Stream Temperature Prediction across Different Spatial Regions and Scales</title>
      <link>http://arxiv.org/abs/2510.09500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Geo-STARS，一个地理感知时空建模框架，用于预测不同流域和空间尺度的河流水温，通过引入地理感知嵌入解决了现有模型在跨区域和尺度推广方面的问题。&lt;h4&gt;背景&lt;/h4&gt;理解环境生态系统对地球可持续管理至关重要，但现有基于物理和数据驱动的模型因数据异质性和有限观测样本而难以推广到不同空间区域和尺度。&lt;h4&gt;目的&lt;/h4&gt;开发Geo-STARS框架，实现跨流域和空间尺度的河流水温预测，解决现有模型的泛化问题。&lt;h4&gt;方法&lt;/h4&gt;Geo-STARS引入地理感知嵌入来捕捉跨空间区域和尺度的共享原则和模式，并将其整合到门控时空图神经网络中，使模型能够在地理和水文背景下学习复杂时空模式，即使数据稀疏或缺失。&lt;h4&gt;主要发现&lt;/h4&gt;在美国东部海岸多个流域37年真实数据集上评估，Geo-STARS在区域和尺度上都表现出优于最先进基线的泛化性能。&lt;h4&gt;结论&lt;/h4&gt;Geo-STARS为可扩展、数据高效的环境监测和决策制定提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解环境生态系统对我们星球的可持续管理至关重要。然而，现有的基于物理和数据驱动的模型往往由于现实环境生态系统中固有的数据异质性而无法推广到不同的空间区域和尺度。由于可用于模型训练的观测样本有限，这种推广问题进一步加剧。为解决这些问题，我们提出了Geo-STARS，一个用于预测不同流域和空间尺度河流水温的地理感知时空建模框架。Geo-STARS的主要创新是引入地理感知嵌入，它利用地理信息来明确捕捉跨空间区域和尺度的共享原则和模式。我们将地理感知嵌入进一步整合到门控时空图神经网络中。这种设计使模型能够在地理和水文背景的指导下学习复杂的时空模式，即使有稀疏或无观测数据也是如此。我们在预测河流水温方面评估了Geo-STARS的有效性，河流水质是水质量的主导因素。使用美国东部海岸多个流域跨越37年的真实世界数据集，Geo-STARS展示了其在区域和尺度上的优越泛化性能，优于最先进的基线。这些结果突显了Geo-STARS在可扩展、数据高效的环境监测和决策制定方面的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding environmental ecosystems is vital for the sustainablemanagement of our planet. However,existing physics-based and data-driven modelsoften fail to generalize to varying spatial regions and scales due to theinherent data heterogeneity presented in real environmental ecosystems. Thisgeneralization issue is further exacerbated by the limited observation samplesavailable for model training. To address these issues, we propose Geo-STARS, ageo-aware spatio-temporal modeling framework for predicting stream watertemperature across different watersheds and spatial scales. The majorinnovation of Geo-STARS is the introduction of geo-aware embedding, whichleverages geographic information to explicitly capture shared principles andpatterns across spatial regions and scales. We further integrate the geo-awareembedding into a gated spatio-temporal graph neural network. This designenables the model to learn complex spatial and temporal patterns guided bygeographic and hydrological context, even with sparse or no observational data.We evaluate Geo-STARS's efficacy in predicting stream water temperature, whichis a master factor for water quality. Using real-world datasets spanning 37years across multiple watersheds along the eastern coast of the United States,Geo-STARS demonstrates its superior generalization performance across bothregions and scales, outperforming state-of-the-art baselines. These resultshighlight the promise of Geo-STARS for scalable, data-efficient environmentalmonitoring and decision-making.</description>
      <author>example@mail.com (Shiyuan Luo, Runlong Yu, Shengyu Chen, Yingda Fan, Yiqun Xie, Yanhua Li, Xiaowei Jia)</author>
      <guid isPermaLink="false">2510.09500v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Precoder Design in Multi-User FDD Systems with VQ-VAE and GNN</title>
      <link>http://arxiv.org/abs/2510.09495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于向量量化-变分自编码器(VQ-VAE)的鲁棒预编码方法，用于频率双工系统，解决了传统高斯混合模型(GMM)组件数量随反馈比特数指数增长的问题。&lt;h4&gt;背景&lt;/h4&gt;在频率双工系统中，鲁棒预编码的有效实现需要结合传播环境的统计信息，传统方法使用高斯混合模型和图神经网络设计特定站点的预编码器。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预编码框架，解决GMM组件数量随反馈比特数指数增长的问题，并实现端到端训练，以提高多用户无线系统的性能。&lt;h4&gt;方法&lt;/h4&gt;利用向量量化-变分自编码器(VQ-VAE)替代GMM，构建结合图神经网络(GNN)、VQ-VAE和pilot优化的端到端(E2E)模型。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多用户无线系统中实现了显著的速率增益，性能优于传统的子离散傅里叶变换(DFT) pilot 矩量和迭代预编码算法。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架支持使用更少pilot或反馈比特的系统部署，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;通过生成模型整合传播环境的学习统计信息，频率双工系统中的鲁棒预编码可以有效实现。我们基于先前成功结合高斯混合模型(GMM)和图神经网络(GNN)设计特定站点预编码器的工作。本文通过使用向量量化-变分自编码器(VQ-VAE)，避免了GMM的一个关键缺点，即GMM组件数量随反馈比特数呈指数增长。此外，VQ-VAE的深度学习架构允许我们将GNN与VQ-VAE以及pilot优化联合训练，形成一个端到端(E2E)模型，从而在多用户无线系统中实现显著的速率增益。仿真结果表明，所提出的框架优于涉及子离散傅里叶变换(DFT) pilot矩阵和迭代预编码算法的传统方法，能够支持部署具有更少pilot或反馈比特的系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust precoding is efficiently feasible in frequency division duplex (FDD)systems by incorporating the learnt statistics of the propagation environmentthrough a generative model. We build on previous work that successfullydesigned site-specific precoders based on a combination of Gaussian mixturemodels (GMMs) and graph neural networks (GNNs). In this paper, by utilizing avector quantized-variational autoencoder (VQ-VAE), we circumvent one of the keydrawbacks of GMMs, i.e., the number of GMM components scales exponentially tothe feedback bits. In addition, the deep learning architecture of the VQ-VAEallows us to jointly train the GNN together with VQ-VAE along with pilotoptimization forming an end-to-end (E2E) model, resulting in considerableperformance gains in sum rate for multi-user wireless systems. Simulationsdemonstrate the superiority of the proposed frameworks over the conventionalmethods involving the sub-discrete Fourier transform (DFT) pilot matrix anditerative precoder algorithms enabling the deployment of systems characterizedby fewer pilots or feedback bits.</description>
      <author>example@mail.com (Srikar Allaparapu, Michael Baur, Benedikt Böck, Michael Joham, Wolfgang Utschick)</author>
      <guid isPermaLink="false">2510.09495v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models</title>
      <link>http://arxiv.org/abs/2510.09735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种结合图神经网络和大语言模型的跨模态框架InterCorpRel-LLM，用于识别企业间的供应链和竞争关系，在关系识别任务上显著优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;识别企业间的供应链和竞争关系对财务分析和公司治理至关重要，但由于企业数据的规模、稀疏性和上下文依赖性，这一任务具有挑战性。基于图的方法能捕捉结构但缺乏语义深度，而大语言模型擅长文本处理但在表示关系依赖方面能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效建模企业网络结构和语义信息的框架，以准确识别企业间的供应链和竞争关系。&lt;h4&gt;方法&lt;/h4&gt;提出InterCorpRel-LLM，一个结合GNNs和LLMs的跨模态框架，使用来自FactSet供应链记录的专有数据集，并设计了三个定制训练任务：公司图匹配、行业分类和供应链关系预测。&lt;h4&gt;主要发现&lt;/h4&gt;InterCorpRel-LLM在供应链关系识别任务上显著优于强基线模型(包括GPT-5)，仅使用7B参数主干和轻量级训练就达到了0.8543的F分数(基线为0.2287)。该模型还能推广到零样本竞争者识别，展示了捕捉微妙企业间动态的能力。&lt;h4&gt;结论&lt;/h4&gt;InterCorpRel-LLM为分析师和战略家提供了绘制和推理复杂企业网络的强大工具，增强了动态市场中的决策制定和风险管理能力。&lt;h4&gt;翻译&lt;/h4&gt;识别企业间的供应链和竞争关系对财务分析和公司治理至关重要，但由于企业数据的规模、稀疏性和上下文依赖性，这一任务仍然具有挑战性。基于图的方法能捕捉结构但缺乏语义深度，而大语言模型擅长文本但在表示关系依赖方面能力有限。为此，我们提出了InterCorpRel-LLM，这是一个结合GNNs和LLMs的跨模态框架，支持来自FactSet供应链记录的专有数据集和三个定制训练任务：公司图匹配、行业分类和供应链关系预测。这种设计能够有效建模结构和语义的联合表示。实验表明，在供应链关系识别任务上，InterCorpRel-LLM显著优于强基线模型(包括GPT-5)，仅使用7B参数主干和轻量级训练就达到了0.8543的F分数(对比基线的0.2287)。该模型还能推广到零样本竞争者识别，突显了其捕捉微妙企业间动态的能力。因此，我们的框架为分析师和战略家提供了绘制和推理复杂企业网络的强大工具，增强了动态市场中的决策制定和风险管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying inter-firm relationships such as supply and competitive ties iscritical for financial analysis and corporate governance, yet remainschallenging due to the scale, sparsity, and contextual dependence of corporatedata. Graph-based methods capture structure but miss semantic depth, whilelarge language models (LLMs) excel at text but remain limited in their abilityto represent relational dependencies. To address this, we proposeInterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,supported by a proprietary dataset derived from FactSet supply chain recordsand three tailored training tasks: company graph matching, industryclassification, and supply relation prediction. This design enables effectivejoint modeling of structure and semantics. Experiments show thatInterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,on a supply relation identification task, achieving an F-score of 0.8543 vs.0.2287 with only a 7B-parameter backbone and lightweight training. The modelalso generalizes to zero-shot competitor identification, underscoring itsability to capture nuanced inter-firm dynamics. Our framework thus providesanalysts and strategists with a robust tool for mapping and reasoning aboutcomplex corporate networks, enhancing decision-making and risk management indynamic markets.</description>
      <author>example@mail.com (Qianyou Sun, Jiexin Zheng, Bohan Jin, Lihua Chen, Yijie Peng)</author>
      <guid isPermaLink="false">2510.09735v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Approach to SME Credit Scoring Integrating Transaction and Ownership Networks</title>
      <link>http://arxiv.org/abs/2510.09407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的中小企业信贷风险评估新方法，通过结合企业网络数据与传统数据提高了预测准确性，并揭示了企业间风险传染机制。&lt;h4&gt;背景&lt;/h4&gt;中小企业在经济增长、就业和创新中发挥重要作用，但面临获取信贷的挑战，包括有限财务历史、抵押品约束和宏观经济冲击风险。中小企业常在相互关联的网络中运营，违约风险可能通过网络传播。&lt;h4&gt;目的&lt;/h4&gt;提出并测试一种新的中小企业信贷风险建模方法，准确评估信贷风险，特别关注企业网络间的违约风险传播问题。&lt;h4&gt;方法&lt;/h4&gt;使用来自知名金融机构的独特大型中小企业贷款数据集，采用图神经网络预测中小企业违约，基于企业间共同所有权和金融交易的多层网络数据进行建模，并将网络数据与传统结构化数据结合。&lt;h4&gt;主要发现&lt;/h4&gt;结合网络数据和传统数据提高了申请评分性能；明确模拟了公司间的风险传染；连接的方向性和强度影响金融风险传染；网络数据具有预测能力；供应链网络使中小企业面临相关违约风险。&lt;h4&gt;结论&lt;/h4&gt;网络数据对预测中小企业违约风险具有重要作用，供应链网络在使中小企业面临相关违约风险方面扮演关键角色。&lt;h4&gt;翻译&lt;/h4&gt;中小企业(SMEs)在经济增长、就业和创新方面发挥着至关重要的作用。然而，由于有限的财务历史、抵押品约束和暴露于宏观经济冲击，它们在获取信贷方面往往面临重大挑战。这些挑战使得贷款人进行准确的信贷风险评估变得至关重要，特别是因为中小企业经常在相互关联的企业网络中运营，违约风险可以通过这些网络传播。本文提出并测试了一种新的中小企业信贷风险建模方法，使用来自知名金融机构的独特大型中小企业贷款数据集。具体而言，我们的方法采用图神经网络来预测中小企业违约，使用来自企业间共同所有权和金融交易的多层网络数据。我们表明，将此信息与传统结构化数据相结合不仅提高了申请评分性能，还明确模拟了公司之间的风险传染风险。进一步分析显示，这些连接的方向性和强度如何影响金融风险传染，从而提供了对潜在过程的更深入理解。我们的研究结果强调了网络数据的预测能力，以及供应链网络在使中小企业面临相关违约风险方面的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Small and Medium-sized Enterprises (SMEs) are known to play a vital role ineconomic growth, employment, and innovation. However, they tend to facesignificant challenges in accessing credit due to limited financial histories,collateral constraints, and exposure to macroeconomic shocks. These challengesmake an accurate credit risk assessment by lenders crucial, particularly sinceSMEs frequently operate within interconnected firm networks through whichdefault risk can propagate. This paper presents and tests a novel approach formodelling the risk of SME credit, using a unique large data set of SME loansprovided by a prominent financial institution. Specifically, our approachemploys Graph Neural Networks to predict SME default using multilayer networkdata derived from common ownership and financial transactions between firms. Weshow that combining this information with traditional structured data not onlyimproves application scoring performance, but also explicitly models contagionrisk between companies. Further analysis shows how the directionality andintensity of these connections influence financial risk contagion, offering adeeper understanding of the underlying processes. Our findings highlight thepredictive power of network data, as well as the role of supply chain networksin exposing SMEs to correlated default risk.</description>
      <author>example@mail.com (Sahab Zandi, Kamesh Korangi, Juan C. Moreno-Paredes, María Óskarsdóttir, Christophe Mues, Cristián Bravo)</author>
      <guid isPermaLink="false">2510.09407v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning to Identify the Spatio-Temporal Cascading Effects of Train Delays in a High-Density Network</title>
      <link>http://arxiv.org/abs/2510.09350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at SIGSPATIAL 2025 - GeoAI Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为XGeoAI的新框架，用于实时、可解释、多步列车延误预测，解决了现有研究中在网络规模多步自回归预测和实时可解释性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;铁路网络是现代经济的基石，但其运营效率持续受到列车延误级联效应的影响。准确预测延误传播对实时交通管理至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发并评估一种新的XGeoAI框架，用于实时、可解释、多步列车延误预测，为决策支持提供可靠工具。&lt;h4&gt;方法&lt;/h4&gt;构建了一个两阶段自回归图注意力网络(GAT)模型，使用覆盖荷兰铁路网络40%以上的真实世界数据集训练。该模型将系统表示为操作事件的时空图，并增加了站台和车站拥堵等细粒度特征。通过顺序的k步前预测协议进行评估，模拟真实世界中预测误差可能累积的情况。&lt;h4&gt;主要发现&lt;/h4&gt;提出的GATv2模型在纯误差指标(MAE)上比简单的持久性基线更具挑战性，但在分类延误事件方面实现了一贯的更高精度，这对可靠的决策支持工具至关重要。&lt;h4&gt;结论&lt;/h4&gt;XGeoAI框架能够提供实时、可解释的多步列车延误预测，特别适合作为决策支持工具，在延误事件分类方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;铁路网络的运营效率作为现代经济的基石，持续受到列车延误级联效应的破坏。准确预测这种延误传播是实时交通管理的关键挑战。尽管最近的研究利用图神经网络(GNNs)对铁路网络结构进行建模，但在开发能提供网络规模多步自回归预测的框架方面仍存在显著差距，同时缺乏决策支持所需的实时、可解释的解释。本文通过开发和评估一种新的XGeoAI框架来解决这一差距，该框架用于实时、可解释、多步列车延误预测。这项工作的核心是一个两阶段自回归图注意力网络(GAT)模型，在覆盖荷兰铁路网络40%以上的真实世界数据集上进行训练。该模型将系统表示为操作事件(到达和出发)的时空图，并增加了包括站台和车站拥堵在内的细粒度特征。为测试其在实时部署中的可行性，使用顺序的k步前预测协议对模型进行了严格评估，该协议模拟了预测误差可能累积的真实世界条件。结果表明，虽然提出的GATv2模型在纯误差指标(MAE)上比简单的持久性基线更具挑战性，但在分类延误事件方面实现了一贯的更高精度，这对可靠的决策支持工具至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3764912.3770828&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The operational efficiency of railway networks, a cornerstone of moderneconomies, is persistently undermined by the cascading effects of train delays.Accurately forecasting this delay propagation is a critical challenge forreal-time traffic management. While recent research has leveraged Graph NeuralNetworks (GNNs) to model the network structure of railways, a significant gapremains in developing frameworks that provide multi-step autoregressiveforecasts at a network-wide scale, while simultaneously offering the live,interpretable explanations needed for decision support. This paper addressesthis gap by developing and evaluating a novel XGeoAI framework for live,explainable, multi-step train delay forecasting. The core of this work is atwo-stage, autoregressive Graph Attention Network (GAT) model, trained on areal-world dataset covering over 40% of the Dutch railway network. The modelrepresents the system as a spatio-temporal graph of operational events(arrivals and departures) and is enriched with granular features, includingplatform and station congestion. To test its viability for live deployment, themodel is rigorously evaluated using a sequential, k-step-ahead forecastingprotocol that simulates real-world conditions where prediction errors cancompound. The results demonstrate that while the proposed GATv2 model ischallenged on pure error metrics (MAE) by a simpler Persistence baseline, itachieves consistently higher precision in classifying delay events -- a crucialadvantage for a reliable decision support tool.</description>
      <author>example@mail.com (Vu Duc Anh Nguyen, Ziyue Li)</author>
      <guid isPermaLink="false">2510.09350v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics</title>
      <link>http://arxiv.org/abs/2510.09082v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于复杂网络长期动态预测的高阶网络动力学识别方法，解决了现有方法只能处理成对关系且预测模型要么缺乏准确性要么缺乏可解释性的问题。&lt;h4&gt;背景&lt;/h4&gt;学习复杂网络动力学对于理解、建模和控制现实世界复杂系统至关重要。现有方法通常使用简单图来描述复杂网络中的关系，只能捕获成对关系，而网络中可能存在丰富的非成对结构化关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种用于复杂网络长期动态预测的高阶网络动力学识别方法，解决传统图机器学习只能处理成对关系的问题，同时提高预测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;引入动态超图学习来捕获复杂网络中的高阶非成对关系，提高复杂网络建模的准确性；提出物理数据双驱动动态预测模块，利用Koopman算子理论将非线性动力学微分方程转化为线性系统求解，同时利用物理信息神经微分方程方法确保动态演化符合物理定律。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在公共数据集和自建产业链网络数据集上具有良好的预测精度和长期预测性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过动态超图学习和双驱动动态预测模块，有效解决了复杂网络动态预测中的成对关系限制和预测准确性-可解释性权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;学习复杂网络动力学对于理解、建模和控制现实世界复杂系统至关重要。在预测复杂网络动态演化的任务中存在两个主要问题：一方面，现有方法通常使用简单图来描述复杂网络中的关系，然而这种方法只能捕获成对关系，而网络中可能存在丰富的非成对结构化关系。一阶GNN难以捕获动态非成对关系。另一方面，理论预测模型缺乏准确性，数据驱动预测模型缺乏可解释性。为解决上述问题，本文提出了一种用于复杂网络长期动态预测的高阶网络动力学识别方法。首先，为解决传统图机器学习只能处理成对关系的问题，引入动态超图学习来捕获复杂网络中的高阶非成对关系，提高复杂网络建模的准确性。然后，提出了物理数据双驱动动态预测模块。引入Koopman算子理论将复杂网络动态演化的非线性动力学微分方程转化为线性系统求解。同时，利用物理信息神经微分方程方法确保动态演化符合物理定律。双驱动动态预测模块确保了预测的准确性和可解释性。在公共数据集和自建产业链网络数据集上验证的实验结果表明，本文方法具有良好的预测精度和长期预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning complex network dynamics is fundamental to understanding, modellingand controlling real-world complex systems. There are two main problems in thetask of predicting the dynamic evolution of complex networks: on the one hand,existing methods usually use simple graphs to describe the relationships incomplex networks; however, this approach can only capture pairwiserelationships, while there may be rich non-pairwise structured relationships inthe network. First-order GNNs have difficulty in capturing dynamic non-pairwiserelationships. On the other hand, theoretical prediction models lack accuracyand data-driven prediction models lack interpretability. To address the aboveproblems, this paper proposes a higher-order network dynamics identificationmethod for long-term dynamic prediction of complex networks. Firstly, toaddress the problem that traditional graph machine learning can only deal withpairwise relations, dynamic hypergraph learning is introduced to capture thehigher-order non-pairwise relations among complex networks and improve theaccuracy of complex network modelling. Then, a dual-driven dynamic predictionmodule for physical data is proposed. The Koopman operator theory is introducedto transform the nonlinear dynamical differential equations for the dynamicevolution of complex networks into linear systems for solving. Meanwhile, thephysical information neural differential equation method is utilised to ensurethat the dynamic evolution conforms to the physical laws. The dual-drivedynamic prediction module ensures both accuracy and interpretability of theprediction. Validated on public datasets and self-built industrial chainnetwork datasets, the experimental results show that the method in this paperhas good prediction accuracy and long-term prediction performance.</description>
      <author>example@mail.com (Bicheng Wang, Junping Wang, Yibo Xue)</author>
      <guid isPermaLink="false">2510.09082v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A review of cultural heritage inspection: Toward terahertz from mid-infrared region</title>
      <link>http://arxiv.org/abs/2510.11521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述探讨了用于检测和分析文化遗产文物的非侵入式成像方法，覆盖中远红外到太赫兹光谱区域，并总结了这些技术的应用和最新信号处理进展。&lt;h4&gt;背景&lt;/h4&gt;文化遗产文物的保护和修复需要先进的无损检测技术，非侵入式成像(NII)方法在文化遗产研究中具有广泛应用。&lt;h4&gt;目的&lt;/h4&gt;总结NII技术在文化遗产研究中的应用，以及信号处理技术的最新进展，特别是深度学习在自动化分析中的革命性作用。&lt;h4&gt;方法&lt;/h4&gt;热红外域利用材料自发射特性；近红外通过外部照明增强表面细节；远红外和太赫兹技术以透射和反射模式穿透表面层；整合可见光和红外成像丰富诊断能力；应用深度学习进行自动分类、特征提取和缺陷检测。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习可通过监督和非监督学习可靠识别细微异常和材料变化，这些变化可能指示过去的修复或早期退化阶段；先进光谱成像、信号处理和神经网络的融合提供了更准确的数据驱动分析方法。&lt;h4&gt;结论&lt;/h4&gt;先进光谱成像、复杂信号处理和深度神经网络的结合为文化遗产分析提供了更准确、高效和数据驱动的途径，最终支持更明智的保护和修复决策。&lt;h4&gt;翻译&lt;/h4&gt;本综述探讨了覆盖中远红外至太赫兹光谱区域(最高约1000微米)的非侵入式成像(NII)方法，用于检测和分析文化遗产文物。在遵循普朗克定律的热红外域，材料的自发射揭示了材料的内在特性和内部退化。相比之下，在近红外范围内，外部照明增强了表面细节和颜料区分。远红外和太赫兹技术以透射和反射模式工作，通过穿透表面层提供亚表面结构和隐藏特征的补充见解。整合可见光和红外成像通过关联常规视觉评估与光谱信息进一步丰富了诊断能力。除了综述这些NII技术在文化遗产研究中的广泛应用外，本文还总结了信号处理的最新进展，包括硬件和软件发展。特别是，深度学习通过实现自动分类、特征提取、缺陷检测和超分辨率成像彻底改变了该领域。通过监督和非监督学习策略，神经网络可以可靠地识别指示过去修复或早期退化阶段的细微异常和材料变化。总之，先进光谱成像、复杂信号处理和深度神经网络的融合为文化遗产分析提供了更准确、高效和数据驱动的变革性途径，最终支持更明智的保护和修复决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This review explores non-invasive imaging (NII) methods covering the mid- andfar-infrared to the terahertz spectral regions (up to approximately 1000 um)for the detection and analysis of cultural heritage artifacts. In the thermalinfrared domain, where radiation follows Planck's law, the self-emission ofmaterials reveals intrinsic properties and internal degradation. By contrast,in the near-infrared range, external illumination enhances surface details andpigment differentiation. Far-infrared and terahertz techniques, operating inboth transmission and reflection modes, provide complementary insights bypenetrating surface layers to uncover subsurface structures and concealedfeatures. Integrating visible and infrared imaging further enriches diagnosticcapabilities by correlating conventional visual assessments with spectralinformation. Beyond reviewing the wide applications of these NII techniques incultural heritage research, this work also summarizes recent advances in signalprocessing, encompassing both hardware and software developments. Inparticular, deep learning has revolutionized the field by enabling automatedclassification, feature extraction, defect detection, and super-resolutionimaging. Through supervised and unsupervised learning strategies, neuralnetworks can reliably identify subtle anomalies and material variationsindicative of past restorations or early stages of deterioration. Inconclusion, the convergence of advanced spectral imaging, sophisticated signalprocessing, and deep neural networks offers a transformative pathway towardmore accurate, efficient, and data-driven cultural heritage analysis,ultimately supporting more informed conservation and restoration decisions.</description>
      <author>example@mail.com (Pengfei Zhu, Hai Zhang, Stefano Sfarra, Dazhi Yang, Xavier Maldague)</author>
      <guid isPermaLink="false">2510.11521v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches</title>
      <link>http://arxiv.org/abs/2510.11243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了尼泊尔手语的首个基准数据集，并评估了深度学习方法在手语识别中的有效性。&lt;h4&gt;背景&lt;/h4&gt;手语是听力和言语障碍人群的重要交流系统，但像尼泊尔手语这样的代表性不足的手语，其数字语言数据集资源仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;创建尼泊尔手语的基准数据集，并评估深度学习方法在手语识别任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含36个手势类别，每类1500个样本的数据集，并使用MobileNetV2和ResNet50架构进行微调来评估分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;MobileNetV2达到了90.45%的分类准确率，ResNet50达到了88.78%的分类准确率，证明卷积神经网络在手语识别任务中有效，特别是在资源有限的环境中。&lt;h4&gt;结论&lt;/h4&gt;这项工作代表了构建尼泊尔手语基准数据集和评估深度学习方法的第一系统性努力，突出了迁移学习和微调在推进代表性不足手语研究中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;手语是听力和言语障碍人士的重要交流系统。然而，对于代表性不足的手语，如尼泊尔手语，其数字语言数据集资源仍然稀缺。本研究引入了尼泊尔手语的第一个基准数据集，包含36个手势类别，每类1500个样本，旨在捕捉该语言的结构和视觉特征。为了评估识别性能，我们在数据集上微调了MobileNetV2和ResNet50架构，分别达到了90.45%和88.78%的分类准确率。这些发现证明了卷积神经网络在手语识别任务中的有效性，特别是在资源有限的环境中。据我们所知，这项工作代表了构建尼泊尔手语识别基准数据集和评估深度学习方法的第一系统性努力，突出了迁移学习和微调在推进代表性不足手语研究中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign languages serve as essential communication systems for individuals withhearing and speech impairments. However, digital linguistic dataset resourcesfor underrepresented sign languages, such as Nepali Sign Language (NSL), remainscarce. This study introduces the first benchmark dataset for NSL, consistingof 36 gesture classes with 1,500 samples per class, designed to capture thestructural and visual features of the language. To evaluate recognitionperformance, we fine-tuned MobileNetV2 and ResNet50 architectures on thedataset, achieving classification accuracies of 90.45% and 88.78%,respectively. These findings demonstrate the effectiveness of convolutionalneural networks in sign recognition tasks, particularly within low-resourcesettings. To the best of our knowledge, this work represents the firstsystematic effort to construct a benchmark dataset and assess deep learningapproaches for NSL recognition, highlighting the potential of transfer learningand fine-tuning for advancing research in underexplored sign languages.</description>
      <author>example@mail.com (Birat Poudel, Satyam Ghimire, Sijan Bhattarai, Saurav Bhandari, Suramya Sharma Dahal)</author>
      <guid isPermaLink="false">2510.11243v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning with Distance Covariance for Random Forest: Error Bounds and an EHR Application</title>
      <link>http://arxiv.org/abs/2510.10870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于距离协方差的中心随机森林(CRF)方法，用于非参数回归的转移学习，特别是在源域和目标域回归函数在某些特征上稀疏不同的情况下。该方法通过两阶段CRF拟合过程，结合距离协方差特征权重，理论上证明了随机森林中转移学习的优势，并在模拟和实际医疗数据应用中验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;随机森林是机器学习中的重要方法，在结构化表格数据方面优于其他竞争方法。然而，在源域和目标域分布不同的情况下，如何有效地应用随机森林进行转移学习仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于中心随机森林的转移学习方法，利用距离协方差的特征权重，以解决源域和目标域回归函数在某些特征上稀疏不同的问题，提高跨域预测性能。&lt;h4&gt;方法&lt;/h4&gt;首先使用源域训练的CRF预测目标域响应值并获取残差；然后使用另一个CRF拟合这些残差，特征分割概率与特征和残差之间的样本距离协方差成比例；推导均方误差率上界作为样本大小和差异维度的函数。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了随机森林中转移学习的好处；模拟结果表明CRF的结果在数值上也适用于标准随机森林；基于距离协方差的特征权重能提升RF性能；在预测ICU患者死亡率的应用中显示出显著优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的CRF方法结合距离协方差特征权重，能有效解决非参数回归中的转移学习问题，特别是在源域和目标域回归函数稀疏不同的情况下，为跨域预测提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随机森林是一种重要的机器学习方法，由于其在对结构化表格数据的广泛应用中优于其他竞争方法。我们提出了一种使用基于距离协方差的中心随机森林(CRF)进行非参数回归转移学习的方法，假设未知源域和目标域的回归函数在某些特征上有所不同(稀疏不同)。我们的方法首先使用源域训练的CRF预测目标域的响应值并获取残差。然后，我们使用另一个CRF拟合这些残差，但特征分割概率与特征和残差之间的样本距离协方差成比例。我们推导了该方法的均方误差率上界作为样本大小和差异维度的函数，理论上证明了随机森林中转移学习的好处。在模拟中，我们证明CRF的结果在数值上也适用于具有数据驱动特征分割选择的标准随机森林(SRF)方法。除了转移学习，我们的结果还显示了基于距离协方差的权重在某些情况下对RF性能的益处。我们的方法在使用包含20万ICU患者电子健康记录的大型多医院数据集预测小型床位目标医院ICU患者死亡率方面显示出显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Random forest is an important method for ML applications due to its broadoutperformance over competing methods for structured tabular data. We propose amethod for transfer learning in nonparametric regression using a centeredrandom forest (CRF) with distance covariance-based feature weights, assumingthe unknown source and target regression functions are different for a fewfeatures (sparsely different). Our method first obtains residuals frompredicting the response in the target domain using a source domain-trained CRF.Then, we fit another CRF to the residuals, but with feature splittingprobabilities proportional to the sample distance covariance between thefeatures and the residuals in an independent sample. We derive an upper boundon the mean square error rate of the procedure as a function of sample sizesand difference dimension, theoretically demonstrating transfer learningbenefits in random forests. In simulations, we show that the results obtainedfor the CRFs also hold numerically for the standard random forest (SRF) methodwith data-driven feature split selection. Beyond transfer learning, our resultsalso show the benefit of distance-covariance-based weights on the performanceof RF in some situations. Our method shows significant gains in predicting themortality of ICU patients in smaller-bed target hospitals using a largemulti-hospital dataset of electronic health records for 200,000 ICU patients.</description>
      <author>example@mail.com (Chenze Li, Subhadeep Paul)</author>
      <guid isPermaLink="false">2510.10870v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Dataset Similarity to Guide Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.10866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为交叉学习分数(CLS)的新度量标准，用于衡量数据集相似性并提供迁移学习可迁移性的定量指导。该指标通过领域间的双向泛化性能评估数据集相似性，具有理论依据且计算高效。&lt;h4&gt;背景&lt;/h4&gt;迁移学习已成为现代机器学习的基石，可通过利用相关领域知识提高模型学习效果。然而，从对齐不良的数据进行迁移可能损害性能而非提高，因此在实施前确定迁移是否 beneficial 至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的度量标准来衡量数据集相似性，为迁移学习的可迁移性提供定量指导。&lt;h4&gt;方法&lt;/h4&gt;提出交叉学习分数(CLS)度量标准，通过领域间双向泛化性能衡量数据集相似性；建立CLS与目标/源数据集决策边界余弦相似性的理论联系；引入通用框架将源数据集分类为正/模糊/负迁移区；扩展至深度学习编码器-头部架构。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法主要关注特征分布而忽略标签信息和预测关系；CLS可可靠预测迁移是提高还是降低性能；CLS为迁移学习数据选择提供原则性工具。&lt;h4&gt;结论&lt;/h4&gt;通过在多种合成和真实世界任务上的广泛实验，证明CLS可有效预测迁移学习性能变化，指导数据选择决策。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习已成为现代机器学习的基石，因为它可以通过利用相关领域的知识来提高模型的学习效果。然而，从对齐不良的数据进行迁移可能会损害而非提高性能，因此在实施前确定迁移是否 beneficial 至关重要。本研究旨在通过提出一种创新的度量标准来衡量数据集相似性并提供可迁移性的定量指导来解决这一挑战。在现有文献中，现有方法主要关注特征分布而忽略了标签信息和预测关系，可能错失关键的可迁移性见解。相比之下，我们提出的度量标准交叉学习分数(CLS)通过领域之间的双向泛化性能来衡量数据集相似性。我们通过建立CLS与目标数据集和源数据集决策边界之间余弦相似性的联系，为CLS提供了理论依据。从计算角度看，CLS高效且计算快速，因为它绕过了高维问题中昂贵的分布估计问题。我们进一步引入了一个通用框架，根据CLS相对于基线误差将源数据集分类为正迁移区、模糊迁移区或负迁移区，从而能够做出明智的决策。此外，我们将这种方法扩展到深度学习中的编码器-头部架构，以更好地反映现代迁移流程。在多种合成和真实世界任务上的广泛实验表明，CLS可以可靠地预测迁移是提高还是降低性能，为迁移学习中的数据选择提供了原则性工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning has become a cornerstone of modern machine learning, as itcan empower models by leveraging knowledge from related domains to improvelearning effectiveness. However, transferring from poorly aligned data can harmrather than help performance, making it crucial to determine whether thetransfer will be beneficial before implementation. This work aims to addressthis challenge by proposing an innovative metric to measure dataset similarityand provide quantitative guidance on transferability. In the literature,existing methods largely focus on feature distributions while overlooking labelinformation and predictive relationships, potentially missing criticaltransferability insights. In contrast, our proposed metric, the Cross-LearningScore (CLS), measures dataset similarity through bidirectional generalizationperformance between domains. We provide a theoretical justification for CLS byestablishing its connection to the cosine similarity between the decisionboundaries for the target and source datasets. Computationally, CLS isefficient and fast to compute as it bypasses the problem of expensivedistribution estimation for high-dimensional problems. We further introduce ageneral framework that categorizes source datasets into positive, ambiguous, ornegative transfer zones based on their CLS relative to the baseline error,enabling informed decisions. Additionally, we extend this approach toencoder-head architectures in deep learning to better reflect modern transferpipelines. Extensive experiments on diverse synthetic and real-world tasksdemonstrate that CLS can reliably predict whether transfer will improve ordegrade performance, offering a principled tool for guiding data selection intransfer learning.</description>
      <author>example@mail.com (Shudong Sun, Hao Helen Zhang)</author>
      <guid isPermaLink="false">2510.10866v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey</title>
      <link>http://arxiv.org/abs/2510.10671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Draft version, work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是对图像到视频迁移学习这一新兴领域的首次全面综述，探讨了如何将图像-语言基础模型(ILFM)的能力扩展到视频领域，以减轻从头训练视频-语言基础模型的数据和计算需求。&lt;h4&gt;背景&lt;/h4&gt;图像-语言基础模型在图像-文本理解和生成任务中表现出色，视频-文本研究的发展促使人们将基于图像的模型扩展到视频领域。图像到视频迁移学习范式可以显著降低训练成本。&lt;h4&gt;目的&lt;/h4&gt;提供对图像到视频迁移学习领域的全面回顾，建立基于现有ILFM推进视频-文本学习的结构化路线图，并启发未来的研究方向。&lt;h4&gt;方法&lt;/h4&gt;系统总结广泛使用的ILFM及其能力；将现有策略分为冻结特征和修改特征两类；详细阐述这些策略在从细粒度到粗粒度的各种视频-文本学习任务中的应用；通过实验分析不同迁移学习范式的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;图像到视频迁移学习能有效扩展ILFM能力到视频领域；不同迁移策略在各种视频任务上表现各异；从时空视频定位到视频问答等多种任务均可受益于这种迁移学习范式。&lt;h4&gt;结论&lt;/h4&gt;图像到视频迁移学习是视频-文本学习领域的有前途方向，现有ILFM可作为视频-文本学习的基础，该领域仍面临挑战需要进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;图像-语言基础模型(ILFM)在图像-文本理解/生成任务中展示了显著的成功，提供了可迁移的多模态表示，能够泛化到各种下游基于图像的任务。视频-文本研究的进步促使人们越来越有兴趣将基于图像的模型扩展到视频领域。这种被称为图像到视频迁移学习的范式，成功减轻了从头开始训练视频-语言基础模型以满足视频-文本学习的巨大数据和计算需求。本调查提供了这一新兴领域的首次全面回顾，首先总结了广泛使用的ILFM及其能力。然后我们将现有的图像到视频迁移学习策略系统地分为两类：冻结特征和修改特征，取决于是否保留ILFM的原始表示或进行修改。基于图像到视频迁移的任务特定性质，本调查系统地阐述这些策略，并详细说明了它们在从细粒度(如时空视频定位)到粗粒度(如视频问答)的多种视频-文本学习任务中的应用。我们进一步提供了详细的实验分析，研究不同的图像到视频迁移学习范式在多种下游视频理解任务上的有效性。最后，我们确定了当前面临的挑战并指出了未来研究的有希望方向。通过提供全面和结构化的概述，本调查旨在建立基于现有ILFM推进视频-文本学习的结构化路线图，并启发这一快速发展领域的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-Language Foundation Models (ILFM) have demonstrated remarkable successin image-text understanding/generation tasks, providing transferable multimodalrepresentations that generalize across diverse downstream image-based tasks.The advancement of video-text research has spurred growing interest inextending image-based models to the video domain. This paradigm, known asimage-to-video transfer learning, succeeds in alleviating the substantial dataand computational requirements associated with training video-languagefoundation models from scratch for video-text learning. This survey providesthe first comprehensive review of this emerging field, which begins bysummarizing the widely used ILFM and their capabilities. We then systematicallyclassify existing image-to-video transfer learning strategies into twocategories: frozen features and modified features, depending on whether theoriginal representations from ILFM are preserved or undergo modifications.Building upon the task-specific nature of image-to-video transfer, this surveymethodically elaborates these strategies and details their applications acrossa spectrum of video-text learning tasks, ranging from fine-grained (e.g.,spatio-temporal video grounding) to coarse-grained (e.g., video questionanswering). We further present a detailed experimental analysis to investigatethe efficacy of different image-to-video transfer learning paradigms on a rangeof downstream video understanding tasks. Finally, we identify prevailingchallenges and highlight promising directions for future research. By offeringa comprehensive and structured overview, this survey aims to establish astructured roadmap for advancing video-text learning based on existing ILFM,and to inspire future research directions in this rapidly evolving domain.</description>
      <author>example@mail.com (Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai)</author>
      <guid isPermaLink="false">2510.10671v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Towards Cybersickness Severity Classification from VR Gameplay Videos Using Transfer Learning and Temporal Modeling</title>
      <link>http://arxiv.org/abs/2510.10422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种利用迁移学习和长短期记忆网络预测虚拟现实网络疾病严重程度的方法，通过分析VR游戏视频实现68.4%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;虚拟现实技术快速发展并在医疗、教育和娱乐等领域广泛应用，但网络疾病(类似晕动症的症状)持续阻碍VR的广泛接受。&lt;h4&gt;目的&lt;/h4&gt;利用视频数据预测网络疾病严重程度，填补现有研究中基于视频特征预测网络疾病的空白。&lt;h4&gt;方法&lt;/h4&gt;使用在ImageNet数据集上预训练的InceptionV3模型从VR游戏视频中提取高级视觉特征，然后将这些特征传递给LSTM网络以捕捉VR体验的时间动态并预测网络疾病严重程度随时间的变化。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有效利用了视频数据的时间序列特性，实现了68.4%的网络疾病严重度分类准确率，超越了仅使用视频数据训练的现有模型的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为VR开发者提供了评估和减轻虚拟环境中网络疾病的实用工具，并为未来基于视频的时间建模研究奠定基础，以增强VR应用中的用户舒适度。&lt;h4&gt;翻译&lt;/h4&gt;随着虚拟现实(VR)技术的快速发展，其在医疗、教育和娱乐等领域的应用显著增长。然而，持续存在的网络疾病问题(其症状类似于晕动症)继续阻碍VR的广泛接受。虽然近期研究探索了利用集成VR传感器(如眼睛和头部跟踪)数据的多模态深度学习方法，但使用基于视频特征预测网络疾病的研究仍然有限。在本研究中，我们通过使用在ImageNet数据集上预训练的InceptionV3模型，利用迁移学习从VR游戏视频中提取高级视觉特征，解决了这一研究空白。然后将这些特征传递给长短期记忆(LSTM)网络，以捕捉VR体验的时间动态并预测网络疾病严重程度随时间的变化。我们的方法有效利用了视频数据的时间序列特性，实现了68.4%的网络疾病严重度分类准确率。这超越了仅使用视频数据训练的现有模型的性能，为VR开发者提供了评估和减轻虚拟环境中网络疾病的实用工具。此外，这项工作为未来基于视频的时间建模研究奠定了基础，以增强VR应用中的用户舒适度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of virtual reality (VR) technology, its adoptionacross domains such as healthcare, education, and entertainment has grownsignificantly. However, the persistent issue of cybersickness, marked bysymptoms resembling motion sickness, continues to hinder widespread acceptanceof VR. While recent research has explored multimodal deep learning approachesleveraging data from integrated VR sensors like eye and head tracking, thereremains limited investigation into the use of video-based features forpredicting cybersickness. In this study, we address this gap by utilizingtransfer learning to extract high-level visual features from VR gameplay videosusing the InceptionV3 model pretrained on the ImageNet dataset. These featuresare then passed to a Long Short-Term Memory (LSTM) network to capture thetemporal dynamics of the VR experience and predict cybersickness severity overtime. Our approach effectively leverages the time-series nature of video data,achieving a 68.4% classification accuracy for cybersickness severity. Thissurpasses the performance of existing models trained solely on video data,providing a practical tool for VR developers to evaluate and mitigatecybersickness in virtual environments. Furthermore, this work lays thefoundation for future research on video-based temporal modeling for enhancinguser comfort in VR applications.</description>
      <author>example@mail.com (Jyotirmay Nag Setu, Kevin Desai, John Quarles)</author>
      <guid isPermaLink="false">2510.10422v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling Gamer Archetypes through Multi modal feature Correlations and Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.10263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Peer Review Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种综合数据驱动框架，结合心理测量、行为分析和机器学习来识别游戏玩家人格类型，通过结构化调查和多种分析方法确定了四种玩家原型。&lt;h4&gt;背景&lt;/h4&gt;游戏玩家画像研究对自适应游戏设计、行为理解和数字福祉至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个整合框架，揭示潜在游戏者人格类型，连接游戏动机与心理和健康结果。&lt;h4&gt;方法&lt;/h4&gt;对250名参与者（含113名活跃游戏玩家）进行结构化调查；整合特征工程、关联网络、知识图谱分析和无监督聚类；使用多种相关性统计和网络中心度分析；结合多种降维技术和聚类算法；使用多种评估指标优化模型。&lt;h4&gt;主要发现&lt;/h4&gt;PCA与K-Means（k=4）模型达到最佳聚类质量，确定了四种玩家原型：沉浸式社交故事追求者、自律优化者、战略系统导航者和竞争团队建设者。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了将相关性驱动网络洞察与无监督学习联系的可复现流程，行为相关性网络与聚类的结合提高了分类准确性，并提供了理解游戏动机与心理健康结果的整体视角。&lt;h4&gt;翻译&lt;/h4&gt;游戏玩家画像为自适应游戏设计、行为理解和数字福祉提供了关键见解。本研究提出了一种综合的、数据驱动的框架，结合心理测量、行为分析和机器学习来揭示潜在的游戏者人格类型。对250名参与者（包括113名活跃游戏玩家）的结构化调查捕获了多维行为、动机和社会数据。分析流程整合了特征工程、关联网络、知识图谱分析和无监督聚类，以提取有意义的模式。相关性统计量化特征关联，网络中心度指导特征选择。将降维技术与聚类算法相结合，使用多种指数进行评估。PCA与K-Means模型实现了最佳聚类质量，确定了四种原型。这项研究贡献了一个可复现的流程，将相关性驱动的网络洞察与无监督学习联系起来。行为相关性网络与聚类的整合不仅提高了分类准确性，还提供了整体视角，将游戏动机与心理和健康结果联系起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Profiling gamers provides critical insights for adaptive game design,behavioral understanding, and digital well-being. This study proposes anintegrated, data-driven framework that combines psychological measures,behavioral analytics, and machine learning to reveal underlying gamer personas.A structured survey of 250 participants, including 113 active gamers, capturedmultidimensional behavioral, motivational, and social data. The analysispipeline integrated feature engineering, association-network, knowledge-graphanalysis, and unsupervised clustering to extract meaningful patterns.Correlation statistics uses Cramers V, Tschuprows T, Theils U, and Spearmansquantified feature associations, and network centrality guided featureselection. Dimensionality-reduction techniques such as PCA, SVD, t-SNE arecoupled with clustering algorithms like K-Means, Agglomerative, Spectral,DBSCAN, evaluated using Silhouette, Calinski Harabasz, and Davies Bouldinindices. The PCA with K-Means with k = 4 model achieved optimal cluster qualitywith Silhouette = 0.4, identifying four archetypes as Immersive SocialStory-Seekers, Disciplined Optimizers, Strategic Systems Navigators, andCompetitive Team-Builders. This research contributes a reproducible pipelinethat links correlation-driven network insights with unsupervised learning. Theintegration of behavioral correlation networks with clustering not onlyenhances classification accuracy but also offers a holistic lens to connectgameplay motivations with psychological and wellness outcomes.</description>
      <author>example@mail.com (Moona Kanwal, Muhammad Sami Siddiqui, Syed Anael Ali)</author>
      <guid isPermaLink="false">2510.10263v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning of the Biswas-Chatterjee-Sen Model</title>
      <link>http://arxiv.org/abs/2510.09446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 8 figures. arXiv admin note: text overlap with  arXiv:2509.14155&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用深度学习技术研究了动力学连续意见动力学模型的临界性质，通过神经网络、主成分分析和变分自编码器等方法成功识别了临界点并研究了相变行为。&lt;h4&gt;背景&lt;/h4&gt;动力学连续意见动力学模型是研究集体行为和相变的重要模型，其中系统由连续自旋变量组成，取值区间为[-1,1]，类似于自旋系统中的意见形成过程。&lt;h4&gt;目的&lt;/h4&gt;探究动力学连续意见动力学模型的临界性质，准确识别临界点，并研究相变行为，特别是使用深度学习技术来发现传统方法可能难以捕捉的规律。&lt;h4&gt;方法&lt;/h4&gt;使用深度神经网络在动力学蒙特卡洛模拟生成的自旋构型数据上进行训练；采用主成分分析进行无监督学习；实现变分自编码器并通过损失函数研究相变；定义真实数据与重构数据之间的相关函数。&lt;h4&gt;主要发现&lt;/h4&gt;深度神经网络能够准确识别方形和三角形晶格上的临界点；主成分分析可以重现磁化现象并估计临界指数；变分自编码器的损失函数可作为序参量；真实数据与重构数据之间的相关函数在临界点表现出普适性。&lt;h4&gt;结论&lt;/h4&gt;深度学习技术为研究动力学连续意见动力学模型的临界性质提供了有效工具，能够准确识别临界点并揭示相变过程中的普适性行为。&lt;h4&gt;翻译&lt;/h4&gt;我们使用深度学习技术研究动力学连续意见动力学模型的临界性质。该系统由区间[-1,1]内的N个连续自旋变量组成。在通过动力学蒙特卡洛模拟生成的自旋构型数据上训练密集神经网络，能够准确识别方形和三角形晶格上的临界点。使用主成分分析进行经典无监督学习重现了磁化现象，并允许估计临界指数。此外，实现变分自编码器通过损失函数研究相变，该损失函数表现为序参量。定义了真实数据和重构数据之间的相关函数，发现在临界点具有普适性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate the critical properties of kinetic continuous opinion dynamicsusing deep learning techniques. The system consists of $N$ continuous spinvariables in the interval $[-1,1]$. Dense neural networks are trained on spinconfiguration data generated via kinetic Monte Carlo simulations, accuratelyidentifying the critical point on both square and triangular lattices.Classical unsupervised learning with principal component analysis reproducesthe magnetization and allows estimation of critical exponents. Additionally,variational autoencoders are implemented to study the phase transition throughthe loss function, which behaves as an order parameter. A correlation functionbetween real and reconstructed data is defined and found to be universal at thecritical point.</description>
      <author>example@mail.com (J. F. Silva Neto, D. S. M. Alencar, L. T. Brito, G. A. Alves, F. W. S. Lima, A. Macedo-Filho, R. S. Ferreira, T. F. A. Alves)</author>
      <guid isPermaLink="false">2510.09446v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>deep-REMAP: Probabilistic Parameterization of Stellar Spectra Using Regularized Multi-Task Learning</title>
      <link>http://arxiv.org/abs/2510.09362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages. Accepted for publication in RASTI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了deep-REMAP，一种用于从观测光谱预测恒星大气参数的深度学习框架，结合正则化多任务学习和迁移学习，能够准确预测恒星的有效温度、表面重力和金属licity，具有可解释性和鲁棒性，可扩展到其他调查和合成库。&lt;h4&gt;背景&lt;/h4&gt;在天文学调查数据量爆炸式增长的时代，传统的光谱分析方法已达到其处理能力的极限。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架，用于从观测光谱中预测恒星大气参数。&lt;h4&gt;方法&lt;/h4&gt;在PHOENIX合成光谱库上训练深度卷积神经网络，使用迁移学习在MARVELS调查的一小部分观测FGK矮星光谱上微调模型，然后应用于732个未表征的FGK巨星候选体。结合非对称损失函数和嵌入损失，构建回归分类框架。&lt;h4&gt;主要发现&lt;/h4&gt;在30个MARVELS校准恒星上验证时，deep-REMAP准确恢复了有效温度、表面重力和金属licity，例如在有效温度上实现了约75 K的精度。该框架具有可解释性，对参数不平衡具有鲁棒性，能够捕捉非高斯不确定性。&lt;h4&gt;结论&lt;/h4&gt;虽然最初为MARVELS调查开发，但deep-REMAP框架可扩展到其他调查和合成库，为恒星特征表征提供了一种强大且自动化的方法。&lt;h4&gt;翻译&lt;/h4&gt;在调查量爆炸式增长的时代，传统的光谱分析方法已达到其极限。为此，我们开发了deep-REMAP，一种新颖的深度学习框架，利用正则化多任务方法从观测光谱预测恒星大气参数。我们在PHOENIX合成光谱库上训练深度卷积神经网络，并使用迁移学习在MARVELS调查的一小部分观测FGK矮星光谱上微调模型。然后我们将该模型应用于同一调查中的732个未表征的FGK巨星候选体。在30个MARVELS校准恒星上进行验证时，deep-REMAP准确恢复了有效温度、表面重力和金属licity，例如在有效温度上实现了约75 K的精度。通过结合非对称损失函数和嵌入损失，我们的回归分类框架具有可解释性，对参数不平衡具有鲁棒性，并且能够捕捉非高斯不确定性。虽然是为MARVELS开发的，但deep-REMAP框架可扩展到其他调查和合成库，展示了一种强大且自动化的恒星特征表征途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of exploding survey volumes, traditional methods of spectroscopicanalysis are being pushed to their limits. In response, we develop deep-REMAP,a novel deep learning framework that utilizes a regularized, multi-taskapproach to predict stellar atmospheric parameters from observed spectra. Wetrain a deep convolutional neural network on the PHOENIX synthetic spectrallibrary and use transfer learning to fine-tune the model on a small subset ofobserved FGK dwarf spectra from the MARVELS survey. We then apply the model to732 uncharacterized FGK giant candidates from the same survey. When validatedon 30 MARVELS calibration stars, deep-REMAP accurately recovers the effectivetemperature ($T_{\rm{eff}}$), surface gravity ($\log \rm{g}$), and metallicity([Fe/H]), achieving a precision of, for instance, approximately 75 K in$T_{\rm{eff}}$. By combining an asymmetric loss function with an embeddingloss, our regression-as-classification framework is interpretable, robust toparameter imbalances, and capable of capturing non-Gaussian uncertainties.While developed for MARVELS, the deep-REMAP framework is extensible to othersurveys and synthetic libraries, demonstrating a powerful and automated pathwayfor stellar characterization.</description>
      <author>example@mail.com (Sankalp Gilda)</author>
      <guid isPermaLink="false">2510.09362v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MPA-DNN: Projection-Aware Unsupervised Learning for Multi-period DC-OPF</title>
      <link>http://arxiv.org/abs/2510.09349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种MPA-DNN方法，解决了深度神经网络在最优潮流问题中无法满足关键操作约束的问题，特别是在涉及时间间耦合的情况下。该方法通过引入投影层强制物理可行性，实现了无需标记数据的端到端学习，实验表明其在变化负载条件下能实现接近最优性能并严格满足所有约束。&lt;h4&gt;背景&lt;/h4&gt;现代电力系统在高比例可再生能源和储能的情况下，最优潮流(OPF)操作的可行性和效率变得越来越重要。深度神经网络作为OPF求解器的快速代理很有前景，但常常无法满足关键的操作约束，特别是涉及时间间耦合的约束。&lt;h4&gt;目的&lt;/h4&gt;解决深度神经网络在最优潮流问题中无法满足关键操作约束的问题，特别是涉及时间间耦合的约束，如发电机爬坡限制和储能操作。&lt;h4&gt;方法&lt;/h4&gt;提出一个多周期投影感知深度神经网络(MPA-DNN)，它在网络中集成了一个用于多周期调度的投影层，通过投影强制物理可行性，实现端到端的符合约束的调度轨迹学习，无需依赖标记数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在变化的负载条件下实现了接近最优的性能，同时严格满足所有约束。&lt;h4&gt;结论&lt;/h4&gt;MPA-DNN方法能够确保最优潮流操作的可行性和效率，特别是在高比例可再生能源和储能的现代电力系统中。&lt;h4&gt;翻译&lt;/h4&gt;确保现代电力系统中高比例可再生能源和储能情况下的最优潮流(OPF)操作的可行性和效率变得越来越重要。虽然深度神经网络(DNN)已成为OPF求解器有前景的快速代理，但它们常常无法满足关键操作约束，特别是涉及时间间耦合的约束，如发电机爬坡限制和储能操作。为了解决这些问题，我们提出了一种多周期投影感知深度神经网络(MPA-DNN)，它在网络中集成了一个用于多周期调度的投影层。通过这样做，我们的模型通过投影强制物理可行性，使得无需依赖标记数据即可进行端到端的符合约束的调度轨迹学习。实验结果表明，所提出的方法在变化的负载条件下实现了接近最优的性能，同时严格满足所有约束。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring both feasibility and efficiency in optimal power flow (OPF)operations has become increasingly important in modern power systems with highpenetrations of renewable energy and energy storage. While deep neural networks(DNNs) have emerged as promising fast surrogates for OPF solvers, they oftenfail to satisfy critical operational constraints, especially those involvinginter-temporal coupling, such as generator ramping limits and energy storageoperations. To deal with these issues, we propose a Multi-PeriodProjection-Aware Deep Neural Network (MPA-DNN) that incorporates a projectionlayer for multi-period dispatch into the network. By doing so, our modelenforces physical feasibility through the projection, enabling end-to-endlearning of constraint-compliant dispatch trajectories without relying onlabeled data. Experimental results demonstrate that the proposed methodachieves near-optimal performance while strictly satisfying all constraints invarying load conditions.</description>
      <author>example@mail.com (Yeomoon Kim, Minsoo Kim, Jip Kim)</author>
      <guid isPermaLink="false">2510.09349v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Rewiring Development in Brain Segmentation: Leveraging Adult Brain Priors for Enhancing Infant MRI Segmentation</title>
      <link>http://arxiv.org/abs/2510.09306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LODi是一种利用成人脑部MRI分割模型先验知识增强婴儿脑部MRI分割性能的新框架，通过迁移学习和领域适应策略，实现了快速、准确、年龄自适应的分割，减轻了扫描仪和特定地点的偏差。&lt;h4&gt;背景&lt;/h4&gt;婴儿脑部MRI分割对于研究早期神经发育和诊断神经系统疾病至关重要，但面临受试者解剖结构不断变化、运动伪影以及高质量标记数据稀缺等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用成人脑部MRI分割模型先验知识来提高婴儿脑部MRI分割性能的方法，解决婴儿脑部MRI分割中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;LODi框架首先在大量成人脑部MRI数据上预训练分割模型，然后通过迁移学习和领域适应策略逐步适应0-2岁人群，利用弱监督学习和FreeSurfer获得的银标准真实标签进行调整，并引入分层特征细化和多级别一致性约束的新训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;在内部和外部数据集上的广泛实验表明，LODi方法优于传统监督学习和领域特定模型，能够实现快速、准确、年龄自适应的分割，同时减轻扫描仪和特定地点的偏差。&lt;h4&gt;结论&lt;/h4&gt;利用成人脑部先验作为年龄灵活神经成像分析的基础具有显著优势，为整个生命周期内更可靠和通用的脑部MRI分割铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;婴儿脑部MRI的精确分割对于研究早期神经发育和诊断神经系统疾病至关重要。然而，由于受试者解剖结构的持续变化、运动伪影以及高质量标记数据的稀缺，它仍然是一个基本挑战。在这项工作中，我们提出了LODi，一个新颖的框架，利用成人脑部MRI分割模型的先验知识来增强婴儿扫描的分割性能。鉴于大量公开可用的成人脑部MRI数据，我们在大型成人数据集上预训练分割模型作为起点。通过迁移学习和领域适应策略，我们将模型逐步适应0-2岁人群，使其能够考虑婴儿扫描中典型的解剖和成像变异性。成人模型的调整是通过在婴儿脑部扫描上进行弱监督学习进行的，利用使用FreeSurfer获得的银标准真实标签。通过引入一种结合分层特征细化和多级别一致性约束的新训练策略，我们的方法能够实现快速、准确、年龄自适应的分割，同时减轻扫描仪和特定地点的偏差。在内部和外部数据集上的广泛实验证明了我们的方法优于传统监督学习和领域特定模型。我们的研究结果突显了利用成人脑部先验作为年龄灵活神经成像分析基础的优势，为整个生命周期内更可靠和通用的脑部MRI分割铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of infant brain MRI is critical for studying earlyneurodevelopment and diagnosing neurological disorders. Yet, it remains afundamental challenge due to continuously evolving anatomy of the subjects,motion artifacts, and the scarcity of high-quality labeled data. In this work,we present LODi, a novel framework that utilizes prior knowledge from an adultbrain MRI segmentation model to enhance the segmentation performance of infantscans. Given the abundance of publicly available adult brain MRI data, wepre-train a segmentation model on a large adult dataset as a starting point.Through transfer learning and domain adaptation strategies, we progressivelyadapt the model to the 0-2 year-old population, enabling it to account for theanatomical and imaging variability typical of infant scans. The adaptation ofthe adult model is carried out using weakly supervised learning on infant brainscans, leveraging silver-standard ground truth labels obtained with FreeSurfer.By introducing a novel training strategy that integrates hierarchical featurerefinement and multi-level consistency constraints, our method enables fast,accurate, age-adaptive segmentation, while mitigating scanner and site-specificbiases. Extensive experiments on both internal and external datasetsdemonstrate the superiority of our approach over traditional supervisedlearning and domain-specific models. Our findings highlight the advantage ofleveraging adult brain priors as a foundation for age-flexible neuroimaginganalysis, paving the way for more reliable and generalizable brain MRIsegmentation across the lifespan.</description>
      <author>example@mail.com (Alemu Sisay Nigru, Michele Svanera, Austin Dibble, Connor Dalby, Mattia Savardi, Sergio Benini)</author>
      <guid isPermaLink="false">2510.09306v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study</title>
      <link>http://arxiv.org/abs/2510.09187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对板球击球分类进行了全面的基线研究，比较了七种不同的深度学习方法，发现在学术文献中报告的性能与实际实现结果之间存在显著差距，而现代架构结合系统优化可实现92.25%的准确率。&lt;h4&gt;背景&lt;/h4&gt;板球击球分类在体育视频分析中仍然是一个具有挑战性的问题，需要有效地建模空间和时间特征。&lt;h4&gt;目的&lt;/h4&gt;进行一项全面的基线研究，比较七种不同的深度学习方法，用于板球击球分类。&lt;h4&gt;方法&lt;/h4&gt;在统一的基准上实现了和系统评估了传统的CNN-LSTM架构、基于注意力的模型、视觉变换器、迁移学习方法以及现代的EfficientNet-GRU组合。&lt;h4&gt;主要发现&lt;/h4&gt;学术文献中报告的准确率（96%、99.2%和93%）与实际重新实现的准确率（46.0%、55.6%和57.7%）之间存在显著差距；现代最先进的方法（EfficientNet-B0与基于GRU的时间模型组合）达到了92.25%的准确率。&lt;h4&gt;结论&lt;/h4&gt;现代架构和系统优化可以带来实质性的改进；遵循现代MLOps实践并提供可重现的研究平台突显了标准化评估协议在体育视频分析研究中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;从视频序列中进行板球击球分类在体育视频分析中仍然是一个具有挑战性的问题，需要有效地建模空间和时间特征。本文首次进行了全面的基线研究，比较了四种不同研究范式下的七种深度学习方法。我们在统一的基准上实现了并系统评估了传统的CNN-LSTM架构、基于注意力的模型、视觉变换器、迁移学习方法以及现代的EfficientNet-GRU组合。我们研究的一个关键发现是学术文献中的声明与实际实现结果之间存在显著性能差距。虽然之前的论文报告了96%（Balaji LRCN）、99.2%（IJERCSE）和93%（Sensors）的准确率，但我们的标准化重新实现分别实现了46.0%、55.6%和57.7%的准确率。我们的现代最先进方法，结合EfficientNet-B0和基于GRU的时间模型，实现了92.25%的准确率，证明使用现代架构和系统优化可以实现实质性改进。所有实现都遵循使用PyTorch Lightning的现代MLOps实践，提供了一个可重现的研究平台，突显了标准化评估协议在体育视频分析研究中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cricket shot classification from video sequences remains a challengingproblem in sports video analysis, requiring effective modeling of both spatialand temporal features. This paper presents the first comprehensive baselinestudy comparing seven different deep learning approaches across four distinctresearch paradigms for cricket shot classification. We implement andsystematically evaluate traditional CNN-LSTM architectures, attention-basedmodels, vision transformers, transfer learning approaches, and modernEfficientNet-GRU combinations on a unified benchmark. A critical finding of ourstudy is the significant performance gap between claims in academic literatureand practical implementation results. While previous papers reported accuraciesof 96\% (Balaji LRCN), 99.2\% (IJERCSE), and 93\% (Sensors), our standardizedre-implementations achieve 46.0\%, 55.6\%, and 57.7\% respectively. Our modernSOTA approach, combining EfficientNet-B0 with a GRU-based temporal model,achieves 92.25\% accuracy, demonstrating that substantial improvements arepossible with modern architectures and systematic optimization. Allimplementations follow modern MLOps practices with PyTorch Lightning, providinga reproducible research platform that exposes the critical importance ofstandardized evaluation protocols in sports video analysis research.</description>
      <author>example@mail.com (Sungwoo Kang)</author>
      <guid isPermaLink="false">2510.09187v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle Pathological Features in CT Scans</title>
      <link>http://arxiv.org/abs/2510.09107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种专门用于医学图像分析的新型多分支ConvNeXt架构，通过整合三种并行分支特征提取方法，在COVID-19诊断任务上取得了优异性能，超越了之前报道的所有模型。&lt;h4&gt;背景&lt;/h4&gt;智能医学影像分析在辅助临床诊断中起着至关重要的作用，特别是在识别细微病理特征方面，但医学图像分析面临着独特的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对医学图像分析独特挑战的新型多分支ConvNeXt架构，应用于COVID-19诊断，并提供一个可推广的框架用于从CT扫描中分类广泛的病理。&lt;h4&gt;方法&lt;/h4&gt;提出包含严格端到端流程的模型，包括细致的数据预处理和增强，采用两阶段训练策略有效利用迁移学习，架构整合了全局平均池化、全局最大池化和新的注意力加权池化三个并行分支的特征，在2609个CT切片的组合数据集上训练和验证。&lt;h4&gt;主要发现&lt;/h4&gt;模型在验证集上表现出色，COVID-19病例的ROC-AUC达到0.9937，验证准确率为0.9757，F1得分为0.9825，超越了此数据集上之前报道的所有模型。&lt;h4&gt;结论&lt;/h4&gt;现代多分支架构与谨慎的数据处理相结合，可以实现与当代最先进模型相当或更好的性能，证明先进的深度学习技术为稳健医疗诊断提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学影像的智能分析在辅助临床诊断中起着至关重要的作用，特别是对于识别细微的病理特征。本文介绍了一种新型的多分支ConvNeXt架构，专门为医学图像分析的细微挑战而设计。虽然在此应用于COVID-19诊断的具体问题，但该方法为从CT扫描中广泛分类病理提供了一种可推广的框架。所提出的模型包含严格的端到端流程，从细致的数据预处理和增强到利用迁移学习有效利用的纪律性两阶段训练策略。该架构独特地整合了从三个并行分支提取的特征：全局平均池化、全局最大池化以及新的注意力加权池化机制。该模型在来自两个不同数据集的2609个CT切片的组合数据集上进行了训练和验证。实验结果表明，在验证集上表现出色，COVID-19病例的最终ROC-AUC达到0.9937，验证准确率为0.9757，F1得分为0.9825，超越了这个数据集上之前报道的所有模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligent analysis of medical imaging plays a crucial role in assistingclinical diagnosis, especially for identifying subtle pathological features.This paper introduces a novel multi-branch ConvNeXt architecture designedspecifically for the nuanced challenges of medical image analysis. Whileapplied here to the specific problem of COVID-19 diagnosis, the methodologyoffers a generalizable framework for classifying a wide range of pathologiesfrom CT scans. The proposed model incorporates a rigorous end-to-end pipeline,from meticulous data preprocessing and augmentation to a disciplined two-phasetraining strategy that leverages transfer learning effectively. Thearchitecture uniquely integrates features extracted from three parallelbranches: Global Average Pooling, Global Max Pooling, and a newAttention-weighted Pooling mechanism. The model was trained and validated on acombined dataset of 2,609 CT slices derived from two distinct datasets.Experimental results demonstrate a superior performance on the validation set,achieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and anF1-score of 0.9825 for COVID-19 cases, outperforming all previously reportedmodels on this dataset. These findings indicate that a modern, multi-brancharchitecture, coupled with careful data handling, can achieve performancecomparable to or exceeding contemporary state-of-the-art models, therebyproving the efficacy of advanced deep learning techniques for robust medicaldiagnostics.</description>
      <author>example@mail.com (Irash Perera, Uthayasanker Thayasivam)</author>
      <guid isPermaLink="false">2510.09107v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning-Enabled Efficient Raman Pump Tuning under Dynamic Launch Power for C+L Band Transmission</title>
      <link>http://arxiv.org/abs/2510.09047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Asia Communications and Photonics Conference 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于迁移学习的Transformer框架，用于C+L波段系统中的精确建模和拉曼泵浦设计。&lt;h4&gt;背景&lt;/h4&gt;C+L波段系统中的精确建模和拉曼泵浦设计面临挑战，需要新的方法来提高性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时实现精确建模和拉曼泵浦设计的框架，应用于C+L波段系统。&lt;h4&gt;方法&lt;/h4&gt;使用基于迁移学习的Transformer框架。&lt;h4&gt;主要发现&lt;/h4&gt;建模的均方根误差在0.22分贝以内，峰峰值光信噪比变化/偏差在0.86/0.1分贝以内。&lt;h4&gt;结论&lt;/h4&gt;该框架能够有效实现C+L波段系统中的精确建模和拉曼泵浦设计，性能指标达到预期。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于迁移学习的Transformer框架，用于在C+L波段系统中同时实现精确建模和拉曼泵浦设计。建模的均方根误差和峰峰值光信噪比变化/偏差分别在0.22分贝和0.86/0.1分贝以内。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a transfer learning-enabled Transformer framework tosimultaneously realize accurate modeling and Raman pump design in C+L-bandsystems. The RMSE for modeling and peak-to-peak GSNR variation/deviation iswithin 0.22 dB and 0.86/0.1 dB, respectively.</description>
      <author>example@mail.com (Jiaming Liu, Rui Wang, JinJiang Li, Hong Lin, Jing Zhang, Kun Qiu)</author>
      <guid isPermaLink="false">2510.09047v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language</title>
      <link>http://arxiv.org/abs/2510.09032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对数据有限的Chakma语言，引入了一个新的孟加拉语转写的Chakma语料库，并对多种transformer模型进行了微调。实验表明，微调后的多语言模型在适应Chakma时表现优异，且数据质量对模型性能有重要影响。&lt;h4&gt;背景&lt;/h4&gt;Chakma作为一种达罗毗荼语系语言，可用数据有限，在语言模型中代表性不足。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的上下文连贯的孟加拉语转写的Chakma语料库，并利用该数据集对多种模型进行微调，以提高Chakma语言在语言模型中的表现。&lt;h4&gt;方法&lt;/h4&gt;从Chakma文学中整理了一个经过母语人士验证的语料库，并使用该数据集在掩码语言建模任务上微调了六种基于编码器的多语言和区域transformer模型（mBERT、XLM-RoBERTa、DistilBERT、DeBERTaV3、BanglaBERT和IndicBERT）。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的多语言模型在适应孟加拉语转写的Chakma时优于其预训练版本，达到高达73.54%的标记准确度和低至2.90的困惑度。分析还强调了数据质量对模型性能的影响，以及OCR管道在形态丰富的印度文字方面的局限性。&lt;h4&gt;结论&lt;/h4&gt;孟加拉语转写的Chakma对于Chakma语言的迁移学习非常有效，研究人员发布了手动验证的单语数据集以鼓励对低资源语言的多语言语言建模进行进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;作为一种达罗毗荼语系语言且可用数据有限，Chakma在语言模型中仍然代表性不足。在这项工作中，我们引入了一个新颖的上下文连贯的孟加拉语转写的Chakma语料库，该语料库从Chakma文学中精心整理，并由母语人士验证。使用这个数据集，我们在掩码语言建模任务上微调了六种基于编码器的多语言和区域transformer模型（mBERT、XLM-RoBERTa、DistilBERT、DeBERTaV3、BanglaBERT和IndicBERT）。我们的实验表明，当适应到孟加拉语转写的Chakma时，微调后的多语言模型优于其预训练版本，达到高达73.54%的标记准确度和低至2.90的困惑度。我们的分析进一步强调了数据质量对模型性能的影响，并展示了OCR管道在形态丰富的印度文字方面的局限性。我们的研究证明了孟加拉语转写的Chakma对于Chakma语言的迁移学习非常有效，我们发布了手动验证的单语数据集以鼓励对低资源语言的多语言语言建模进行进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As an Indo-Aryan language with limited available data, Chakma remains largelyunderrepresented in language models. In this work, we introduce a novel corpusof contextually coherent Bangla-transliterated Chakma, curated from Chakmaliterature, and validated by native speakers. Using this dataset, we fine-tunesix encoder-based multilingual and regional transformer models (mBERT,XLM-RoBERTa, DistilBERT, DeBERTaV3, BanglaBERT, and IndicBERT) on maskedlanguage modeling (MLM) tasks. Our experiments show that fine-tunedmultilingual models outperform their pre-trained counterparts when adapted toBangla-transliterated Chakma, achieving up to 73.54% token accuracy and aperplexity as low as 2.90. Our analysis further highlights the impact of dataquality on model performance and shows the limitations of OCR pipelines formorphologically rich Indic scripts. Our research demonstrates thatBangla-transliterated Chakma can be very effective for transfer learning forChakma language, and we release our manually validated monolingual dataset toencourage further research on multilingual language modeling for low-resourcelanguages.</description>
      <author>example@mail.com (Adity Khisa, Nusrat Jahan Lia, Tasnim Mahfuz Nafis, Zarif Masud, Tanzir Pial, Shebuti Rayana, Ahmedul Kabir)</author>
      <guid isPermaLink="false">2510.09032v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Denoised Diffusion for Object-Focused Image Augmentation</title>
      <link>http://arxiv.org/abs/2510.08955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种面向对象的数据增强框架，专门用于解决数据受限条件下动物健康监测的问题，通过从背景中分割动物并进行变换和基于扩散的合成，创建真实多样的场景，提高动物检测和监测性能。&lt;h4&gt;背景&lt;/h4&gt;现代农业操作依赖集成监控系统进行农场优化，基于无人机的动物健康监测是关键组成部分，但面临数据有限的问题，包括动物小、被遮挡或部分可见等场景特定问题，且迁移学习方法因缺乏反映特定农场条件的大型数据集而效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发针对特定问题、以动物为中心的数据增强策略，应对数据有限条件下动物健康监测的挑战，弥合有限数据与实际应用之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出一种面向对象的数据增强框架，专门为数据受限环境下的动物健康监测设计，通过从背景中分割动物，并利用变换和基于扩散的合成技术来增强动物，创建真实多样的场景。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，与基线模型相比，增强数据集在动物检测任务上表现出更好的性能；通过生成领域特定数据，即使在数据稀缺的情况下，该方法也能支持实时动物健康监测解决方案。&lt;h4&gt;结论&lt;/h4&gt;该数据增强方法成功解决了数据稀缺条件下动物健康监测的挑战，弥合了有限数据与实际应用之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;现代农业操作越来越多地依赖集成监控系统，结合多种数据源进行农场优化。基于空中无人机的动物健康监测是关键组成部分，但面临数据有限的问题，加之场景特定问题如动物小、被遮挡或部分可见。迁移学习方法通常无法解决这一限制，因为缺乏反映特定农场条件（包括动物品种、环境和行为变化）的大型数据集。因此，需要开发针对特定问题、以动物为中心的数据增强策略，应对这些独特挑战。为解决这一差距，我们提出了一种面向对象的数据增强框架，专门为数据受限条件下的动物健康监测设计。我们的方法从背景中分割动物，并通过变换和基于扩散的合成来增强它们，创建真实、多样的场景，提高动物检测和监测性能。初步实验表明，与基线模型相比，我们的增强数据集在动物检测任务上表现出更好的性能。通过生成领域特定数据，我们的方法即使在数据稀缺的情况下也能支持实时动物健康监测解决方案，弥合了有限数据与实际应用之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern agricultural operations increasingly rely on integrated monitoringsystems that combine multiple data sources for farm optimization. Aerialdrone-based animal health monitoring serves as a key component but faceslimited data availability, compounded by scene-specific issues such as small,occluded, or partially visible animals. Transfer learning approaches often failto address this limitation due to the unavailability of large datasets thatreflect specific farm conditions, including variations in animal breeds,environments, and behaviors. Therefore, there is a need for developing aproblem-specific, animal-focused data augmentation strategy tailored to theseunique challenges. To address this gap, we propose an object-focused dataaugmentation framework designed explicitly for animal health monitoring inconstrained data settings. Our approach segments animals from backgrounds andaugments them through transformations and diffusion-based synthesis to createrealistic, diverse scenes that enhance animal detection and monitoringperformance. Our initial experiments demonstrate that our augmented datasetyields superior performance compared to our baseline models on the animaldetection task. By generating domain-specific data, our method empowersreal-time animal health monitoring solutions even in data-scarce scenarios,bridging the gap between limited data and practical applicability.</description>
      <author>example@mail.com (Nisha Pillai, Aditi Virupakshaiah, Harrison W. Smith, Amanda J. Ashworth, Prasanna Gowda, Phillip R. Owens, Adam R. Rivers, Bindu Nanduri, Mahalingam Ramkumar)</author>
      <guid isPermaLink="false">2510.08955v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Structured Output Regularization: a framework for few-shot transfer learning</title>
      <link>http://arxiv.org/abs/2510.08728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了结构化输出正则化（SOR）框架，通过冻结网络结构并使用正则化方法，解决了传统迁移学习在适应领域特定特征和防止过拟合方面的局限性，在少样本医学影像分类任务中取得了与基准相当的结果。&lt;h4&gt;背景&lt;/h4&gt;传统迁移学习方法通过冻结预训练网络的部分权重并添加任务特定层来重用大型预训练网络，这种方法计算效率高，但限制了模型适应领域特定特征的能力，并且在数据有限时仍可能导致过拟合。&lt;h4&gt;目的&lt;/h4&gt;解决传统迁移学习方法在适应领域特定特征和防止过拟合方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出结构化输出正则化（SOR）框架，冻结内部网络结构（如卷积滤波器），同时使用组套索和L1惩罚的组合，使模型能够以最少的额外参数适应特定数据，并可以轻松应用于各种网络组件。&lt;h4&gt;主要发现&lt;/h4&gt;在三个少样本医学影像分类任务上评估了SOR，使用DenseNet121和EfficientNetB4作为基础模型，与已建立的基准相比取得了具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;SOR框架是一种简单有效的方法，可以解决传统迁移学习在适应领域特定特征和防止过拟合方面的局限性，并且具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;传统的迁移学习通常通过冻结大型预训练网络的一些权重并添加任务特定层来重用这些网络。虽然这种方法计算效率高，但它限制了模型适应领域特定特征的能力，并且在数据非常有限的情况下仍可能导致过拟合。为了解决这些局限性，我们提出了结构化输出正则化（SOR），这是一个简单而有效的框架，它冻结内部网络结构（例如卷积滤波器），同时使用组套索和L1惩罚的组合。该框架使模型能够以最少的额外参数适应特定数据，并且可以轻松应用于各种网络组件，例如神经网络中的卷积滤波器或各种块，从而为迁移学习任务提供了广泛的适用性。我们在三个少样本医学影像分类任务上评估了SOR，使用DenseNet121和EfficientNetB4作为基础模型，与已建立的基准相比取得了具有竞争力的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional transfer learning typically reuses large pre-trained networks byfreezing some of their weights and adding task-specific layers. While thisapproach is computationally efficient, it limits the model's ability to adaptto domain-specific features and can still lead to overfitting with very limiteddata. To address these limitations, we propose Structured Output Regularization(SOR), a simple yet effective framework that freezes the internal networkstructures (e.g., convolutional filters) while using a combination of grouplasso and $L_1$ penalties. This framework tailors the model to specific datawith minimal additional parameters and is easily applicable to various networkcomponents, such as convolutional filters or various blocks in neural networksenabling broad applicability for transfer learning tasks. We evaluate SOR onthree few shot medical imaging classification tasks and we achieve competitiveresults using DenseNet121, and EfficientNetB4 bases compared to establishedbenchmarks.</description>
      <author>example@mail.com (Nicolas Ewen, Jairo Diaz-Rodriguez, Kelly Ramsay)</author>
      <guid isPermaLink="false">2510.08728v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices</title>
      <link>http://arxiv.org/abs/2510.07545v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the EMNLP 2025 Industry Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型视觉-语言模型(LVLMs)作为图表理解任务中自动化评判者的能力，提出多标准提示和领域自适应迁移学习两种方法，成功使小型模型(2B参数)成为有效的图表评判者(ChartJudge)，实现了资源受限环境下的低成本评估。&lt;h4&gt;背景&lt;/h4&gt;具有70亿参数的大型视觉-语言模型(LVLMs)已显示出作为图表理解任务中自动化评判者的潜力。然而，小型模型(参数量≤2B)作为评判者时表现仍然不佳，限制了它们在资源受限环境中的实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决小型模型在图表理解任务中作为评判者表现不佳的问题，确保评估过程具有成本效益，使小型模型能够在资源受限环境中有效应用。&lt;h4&gt;方法&lt;/h4&gt;作者提出两种方法：(i)多标准提示：将单独的评估标准组合到一个查询中；(ii)领域自适应迁移学习：在图表数据集上的合成判断上微调一个具有20亿参数的LVLM，创建出ChartJudge模型。&lt;h4&gt;主要发现&lt;/h4&gt;多标准提示暴露了模型的鲁棒性差距，导致70亿参数模型(包括专业的LVLM评判者如LLaVA-Critic)性能大幅下降；小型LVLM(ChartJudge)可以有效将知识从一个数据集迁移到另一个数据集，使其成为更专业的模型；对不同图表类型和查询复杂性的细粒度分析提供了关于模型大小、提示设计和可迁移性之间权衡的可操作见解。&lt;h4&gt;结论&lt;/h4&gt;通过结合多标准提示和领域自适应迁移学习，ChartJudge模型能够在资源受限环境中实现可扩展的低成本评估，为图表推理任务提供有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;仅具有70亿参数的大型视觉-语言模型(LVLMs)已显示出作为图表理解任务中自动化评判者的潜力。然而，小型模型(参数量≤2B)作为评判者时仍然表现不佳，限制了它们在资源受限环境中的实际应用。为此，我们提出两种方法来确保成本高效的评估：(i)多标准提示，将单独的评估标准组合到一个查询中；(ii)领域自适应迁移学习，我们在图表数据集上的合成判断上微调一个20亿参数的LVLM，创建出ChartJudge。实验表明，多标准提示暴露了模型的鲁棒性差距，导致70亿参数模型(包括专业的LVLM评判者如LLaVA-Critic)性能大幅下降。此外，我们发现我们的小型LVLM(ChartJudge)可以有效地将知识从一个数据集迁移到另一个数据集，使其成为更专业的模型。我们对不同图表类型和查询复杂性的细粒度分析提供了关于模型大小、提示设计和可迁移性之间权衡的可操作见解，使图表推理任务能够实现可扩展的低成本评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision-Language Models (LVLMs) with only 7B parameters have shownpromise as automated judges in chart comprehension tasks. However, tiny models(&lt;=2B parameters) still perform poorly as judges, limiting their real-world usein resource-constrained settings. To address this, we propose two approaches toensure cost-efficient evaluation: (i) multi-criteria prompting, which combinesseparate evaluation criteria into a single query, and (ii) domain-adaptivetransfer learning, in which we fine-tune a 2B-parameter LVLM on syntheticjudgments in a chart dataset to create the ChartJudge. Experiments show thatmulti-criteria prompting exposes robustness gaps, which led to a huge drop inperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.In addition, we find that our tiny LVLM (ChartJudge) can effectively transferknowledge from one dataset to another to make it a more specialized model. Ourfine-grained analysis across chart types and query complexities offersactionable insights into trade-offs between model size, prompt design, andtransferability, enabling scalable, low-cost evaluation for chart reasoningtasks.</description>
      <author>example@mail.com (Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Mizanur Rahman, Amran Bhuiyan, Israt Jahan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang)</author>
      <guid isPermaLink="false">2510.07545v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object Detection and Tracking</title>
      <link>http://arxiv.org/abs/2510.10287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了DualViewDistill混合检测和跟踪框架，结合透视视图和鸟瞰图特征，提高自动驾驶中的3D物体检测和跟踪性能。&lt;h4&gt;背景&lt;/h4&gt;基于相机的3D物体检测和跟踪对自动驾驶感知至关重要，但当前最先进方法通常仅依赖透视视图或鸟瞰图特征，限制了利用精细物体细节和空间结构化场景表示的能力。&lt;h4&gt;目的&lt;/h4&gt;开发能够同时利用透视视图和鸟瞰图特征优势的混合检测和跟踪框架，提升3D物体检测和跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;提出DualViewDistill框架，引入由基础模型指导的BEV地图，利用DINOv2特征通过新颖蒸馏过程生成BEV表示，并将PV特征与增强的BEV地图集成，通过可变形聚合增强3D物体检测和跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Argoverse 2基准测试上，DualViewDistill达到最先进性能，展示了基础模型BEV地图在实现更可靠自动驾驶感知方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;结合透视视图和鸟瞰图特征的优势，并利用基础模型提供的丰富语义和几何信息，显著提高了3D物体检测和跟踪性能，为自动驾驶感知提供了更可靠解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于相机的3D物体检测和跟踪对于自动驾驶感知至关重要。当前最先进的方法通常仅依赖于透视视图或鸟瞰图特征，限制了它们利用精细物体细节和空间结构化场景表示的能力。在这项工作中，我们提出了DualViewDistill，一个结合了PV和BEV相机图像特征的混合检测和跟踪框架，以利用它们的互补优势。我们的方法引入了由基础模型指导的BEV地图，利用描述性的DINOv2特征，并通过一种新颖的蒸馏过程将其蒸馏成BEV表示。通过将PV特征与由DINOv2的语义和几何特征增强的BEV地图集成，我们的模型通过可变形聚合利用这种混合表示来增强3D物体检测和跟踪。在nuScenes和Argoverse 2基准测试上的大量实验表明，DualViewDistill达到了最先进的性能。结果展示了基础模型BEV地图在实现更可靠自动驾驶感知方面的潜力。我们在https://dualviewdistill.cs.uni-freiburg.de提供了代码和预训练模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于相机的3D目标检测和跟踪方法中只依赖单一视图特征(透视视图PV或鸟瞰视图BEV)的问题，无法同时利用细粒度物体细节和空间结构化场景表示。这个问题在自动驾驶领域非常重要，因为可靠的3D感知系统需要既能识别物体细节，又能理解空间布局，这对安全驾驶至关重要。单一视图特征的限制导致现有方法难以同时满足这两方面的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前方法的局限性，发现BEV-based方法提供结构化空间表示但缺乏物体细节，而PV-based方法有丰富细节但空间推理能力有限。他们借鉴了BEVFormer、LSS等BEV方法的表示思想，以及Sparse4Dv3等查询设计方法，还参考了DINOv2等基础模型的一般特征能力和HDNet等地图感知方法的空间先验思想。在此基础上，作者创新性地设计了双视图融合框架和基础模型引导的BEV地图蒸馏方法，同时利用两种视图的优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是双视图融合与基础模型引导的特征蒸馏。方法同时利用透视视图(PV)的物体细节优势和鸟瞰视图(BEV)的空间结构优势，并通过DINOv2基础模型的特征蒸馏增强BEV表示。整体流程包括：1)输入多视角RGB图像；2)提取PV特征和DINOv2特征；3)使用LSS机制将PV特征提升到BEV空间；4)通过Transformer处理对象查询，结合可变形聚合与PV和BEV特征交互；5)将DINOv2特征投影到点云生成BEV伪标签并蒸馏；6)输出3D边界框和对象轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双视图检测框架，首次统一PV和BEV特征；2)基础模型引导的BEV地图，利用DINOv2特征蒸馏提供丰富的在线神经地图；3)可变形聚合机制有效融合两种视图特征；4)混合监督策略结合检测/跟踪监督和BEV特征蒸馏。相比之前工作，传统方法只使用单一视图特征或依赖手动标注的高清地图，而本文同时使用两种视图并通过基础模型在线生成神经地图，无需人工标注，在nuScenes和Argoverse 2上实现了最先进的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了DualViewDistill，一种结合透视视图和鸟瞰视图特征并利用DINOv2引导的BEV地图进行3D目标检测和跟踪的混合框架，显著提升了自动驾驶系统的感知性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-based 3D object detection and tracking are essential for perception inautonomous driving. Current state-of-the-art approaches often rely exclusivelyon either perspective-view (PV) or bird's-eye-view (BEV) features, limitingtheir ability to leverage both fine-grained object details and spatiallystructured scene representations. In this work, we propose DualViewDistill, ahybrid detection and tracking framework that incorporates both PV and BEVcamera image features to leverage their complementary strengths. Our approachintroduces BEV maps guided by foundation models, leveraging descriptive DINOv2features that are distilled into BEV representations through a noveldistillation process. By integrating PV features with BEV maps enriched withsemantic and geometric features from DINOv2, our model leverages this hybridrepresentation via deformable aggregation to enhance 3D object detection andtracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarksdemonstrate that DualViewDistill achieves state-of-the-art performance. Theresults showcase the potential of foundation model BEV maps to enable morereliable perception for autonomous driving. We make the code and pre-trainedmodels available at https://dualviewdistill.cs.uni-freiburg.de .</description>
      <author>example@mail.com (Markus Käppeler, Özgün Çiçek, Daniele Cattaneo, Claudius Gläser, Yakov Miron, Abhinav Valada)</author>
      <guid isPermaLink="false">2510.10287v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2510.06582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages (28 main text), 20 figures, 4 supplementary materials; links  to 3D point animations are included in the last table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种半自动的、不确定性感知的地面激光扫描点云语义分割管道，通过球面投影、特征丰富、集成学习和目标标注相结合，减少标注工作量的同时保持高准确性。构建了Mangrove3D数据集并评估了数据效率和特征重要性。&lt;h4&gt;背景&lt;/h4&gt;准确的地面激光扫描点云语义分割受限于昂贵的手动标注，需要一种方法来减少标注工作量同时保持高准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种半自动的、不确定性感知的管道，构建红树林森林语义分割TLS数据集(Mangrove3D)，并评估数据效率和特征重要性，回答需要多少标注数据以及哪些特征最重要的问题。&lt;h4&gt;方法&lt;/h4&gt;将3D点投影到2D球面网格，使用多源特征丰富像素，训练分割网络集成生成伪标签和不确定性地图，不确定性地图指导模糊区域标注，将2D输出投影回3D，开发三层可视化套件，构建Mangrove3D数据集，评估数据效率和特征重要性，通过跨数据集测试验证特征丰富化策略的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;性能在约12个标注扫描后趋于饱和，几何特征贡献最大，紧凑的九通道堆叠捕获了几乎所有的判别能力，平均交并比(mIoU)稳定在约0.76。&lt;h4&gt;结论&lt;/h4&gt;提出了稳健的、不确定性感知的TLS标注管道和可视化工具，构建了Mangrove3D数据集，提供了关于数据效率和特征重要性的经验指导，使得生态监测等领域的可扩展、高质量的TLS点云分割成为可能。&lt;h4&gt;翻译&lt;/h4&gt;准确的地面激光扫描点云语义分割受限于昂贵的手动标注。我们提出了一种半自动的、不确定性感知的管道，结合球面投影、特征丰富、集成学习和目标标注，以减少标注工作量，同时保持高准确性。我们的方法将3D点投影到2D球面网格，用多源特征丰富像素，并训练分割网络集成来生成伪标签和不确定性地图，后者指导模糊区域的标注。将2D输出投影回3D，得到密集标注的点云，并配有三层可视化套件(2D特征图、3D着色点云和紧凑虚拟球体)用于快速分类和审阅者指导。使用此管道，我们构建了Mangrove3D，一个用于红树林森林的语义分割TLS数据集。我们进一步评估数据效率和特征重要性，以解决两个关键问题：(1)需要多少标注数据，(2)哪些特征最重要。结果表明，性能在约12个标注扫描后趋于饱和，几何特征贡献最大，紧凑的九通道堆叠捕获了几乎所有的判别能力，平均交并比(mIoU)稳定在约0.76。最后，我们通过在ForestSemantic和Semantic3D上的跨数据集测试，验证了我们的特征丰富化策略的泛化能力。我们的贡献包括：(i)带有可视化工具的稳健的、不确定性感知的TLS标注管道；(ii)Mangrove3D数据集；以及(iii)关于数据效率和特征重要性的经验指导，从而使得生态监测等领域的可扩展、高质量的TLS点云分割成为可能。数据集和处理脚本可在https://fz-rit.github.io/through-the-lidars-eye/公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决地面激光扫描(TLS)点云语义分割中高质量标注数据集稀缺的问题。这个问题很重要，因为手动标注全分辨率TLS扫描非常耗费人力，特别是在生态场景中，由于严重的遮挡、不规则的几何形状和交织的树结构，这一问题尤为突出。这阻碍了自动分析在林业和环境监测中的采用，尽管这些领域有迫切需求，而准确的语义分割是生态研究中分析树木指标、生物量和栖息地特征的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何降低标注复杂度并提高效率，考虑将不规则的3D数据转换为结构化的2D地图，集成特征增强的分割方法，并纳入不确定性分析来指导人工校正。作者借鉴了现有工作：球面投影概念来自地图学，应用于将3D点云转换为2D图像；主动学习范式用于迭代查询标注员获取信息量最大的样本；自训练策略让模型重用高置信度预测；特征融合方法利用几何结构、辐射度响应和位置上下文增强分割。作者的创新在于将这些方法整合并专门针对TLS点云和生态场景进行了优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将3D点云投影到2D球面网格上，在2D空间中进行高效的标注和分割，然后再将结果投影回3D，从而降低标注复杂度并提高效率。整体流程分为三阶段：1)球面投影与可视化：将3D点云转换为2D球面网格，创建多通道特征图和三种可视化(2D特征图、3D彩色点云、虚拟球体)；2)混合标注：使用少量初始标注训练三个分割模型集合，生成伪标签和不确定性图，高不确定性区域人工精修，高置信度预测保留为伪标签；3)反向投影与精修：将2D分割结果投影回3D，应用几何平滑和特征驱动的修复，生成最终标注结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)不确定性感知的TLS标注流程，集成球面投影、特征增强和集成学习；2)三级可视化套件，便于标注和检查；3)系统化的特征增强策略，识别最优特征组合；4)创建首个针对复杂红树林生态系统的TLS数据集Mangrove3D。相比之前工作，不同之处在于：现有3D标注工具主要为简单场景设计；早期球面投影方法使用传统聚类而非深度学习；现有主动学习方法主要在RGB数据集上应用；PointPainting等方法依赖多传感器数据，而本文方法仅使用LiDAR数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合球面投影、特征增强和不确定性分析的半自动标注流程，显著降低了地面激光扫描点云语义分割的标注成本，同时创建了首个针对复杂红树林生态系统的TLS数据集，为生态监测等领域提供了高效、高质量的分析工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate semantic segmentation of terrestrial laser scanning (TLS) pointclouds is limited by costly manual annotation. We propose a semi-automated,uncertainty-aware pipeline that integrates spherical projection, featureenrichment, ensemble learning, and targeted annotation to reduce labelingeffort, while sustaining high accuracy. Our approach projects 3D points to a 2Dspherical grid, enriches pixels with multi-source features, and trains anensemble of segmentation networks to produce pseudo-labels and uncertaintymaps, the latter guiding annotation of ambiguous regions. The 2D outputs areback-projected to 3D, yielding densely annotated point clouds supported by athree-tier visualization suite (2D feature maps, 3D colorized point clouds, andcompact virtual spheres) for rapid triage and reviewer guidance. Using thispipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangroveforests. We further evaluate data efficiency and feature importance to addresstwo key questions: (1) how much annotated data are needed and (2) whichfeatures matter most. Results show that performance saturates after ~12annotated scans, geometric features contribute the most, and compactnine-channel stacks capture nearly all discriminative power, with the meanIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirmthe generalization of our feature-enrichment strategy through cross-datasettests on ForestSemantic and Semantic3D.  Our contributions include: (i) a robust, uncertainty-aware TLS annotationpipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)empirical guidance on data efficiency and feature importance, thus enablingscalable, high-quality segmentation of TLS point clouds for ecologicalmonitoring and beyond. The dataset and processing scripts are publiclyavailable at https://fz-rit.github.io/through-the-lidars-eye/.</description>
      <author>example@mail.com (Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt)</author>
      <guid isPermaLink="false">2510.06582v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation</title>
      <link>http://arxiv.org/abs/2509.15886v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了将视觉基础模型SAM2应用于LiDAR点云分割的range-view方法，通过结合2D特征提取与投影/反投影操作，实现了在保持2D方法效率的同时获得有竞争力的3D分割性能。&lt;h4&gt;背景&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心技术。目前基于体素和点的方法虽能捕获细粒度几何信息，但计算成本高，内存访问不规则，实时效率有限。相比之下，range-view方法相对未被充分探索，但可利用成熟的2D语义分割技术进行快速准确预测。&lt;h4&gt;目的&lt;/h4&gt;研究当前最先进的视觉基础模型SAM2是否可作为LiDAR点云在range view中的强大backbone，探索VFMs作为3D感知通用backbone的可行性。&lt;h4&gt;方法&lt;/h4&gt;提出了首个适应SAM2进行3D分割的range-view框架，结合高效2D特征提取与标准投影/反投影操作处理点云。对编码器进行了三种架构修改：(1)强调LiDAR范围图像中水平空间依赖性的新模块；(2)针对球面投影几何特性的定制配置；(3)专门设计用于捕获range-view伪图像中独特空间模式和间断性的适应机制。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在SemanticKITTI数据集上实现了具有竞争力的性能，同时受益于2D为中心的管道的速度、可扩展性和部署简单性。&lt;h4&gt;结论&lt;/h4&gt;VFMs作为3D感知通用backbone具有可行性，为统一的基础模型驱动的LiDAR分割开辟了道路，使用VFMs的range-view分割方法取得了有希望的结果。&lt;h4&gt;翻译&lt;/h4&gt;点云分割是自动驾驶和3D场景理解的核心。虽然最近的体素和基于点的方法因其与深度架构的兼容性和捕获细粒度几何的能力而主导研究，但它们通常带来高计算成本、不规则的内存访问和有限的实时效率。相比之下，range-view方法虽然相对未被充分探索，但可以利用成熟的2D语义分割技术进行快速准确的预测。受视觉基础模型(VFMs)在描述、零样本识别和多模态任务中快速进展的启发，我们研究了当前最先进的分割VFM SAM2是否可以作为LiDAR点云在range view中的强大backbone。据我们所知，我们提出了第一个适应SAM2进行3D分割的range-view框架，将高效的2D特征提取与标准投影/反投影相结合以处理点云。为了优化SAM2对range-view表示的处理，我们对编码器实现了几种架构修改：(1)一个强调LiDAR范围图像中固有水平空间依赖性的新模块；(2)针对球面投影几何特性的定制配置；(3)编码器主干中的一种适应机制，专门设计用于捕获range-view伪图像中存在的独特空间模式和间断性。我们的方法在SemanticKITTI上实现了具有竞争力的性能，同时受益于2D为中心的管道的速度、可扩展性和部署简单性。这项工作证明了VFMs作为3D感知通用backbone的可行性，并为统一的基础模型驱动的LiDAR分割开辟了道路。结果让我们得出结论，使用VFMs的range-view分割方法取得了有希望的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云分割的效率和准确性问题。现有的基于体素和点的方法计算成本高、内存访问不规则、运行效率有限。这个问题在自动驾驶和3D场景理解中至关重要，因为准确分割点云可以帮助车辆识别道路、车辆、行人等关键元素，确保安全导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，注意到范围视图方法可以利用成熟的2D语义分割技术。受SAM2等视觉基础模型在图像分割中成功的启发，作者将其应用于3D点云分割。为了适应范围视图表示，作者对编码器进行了关键修改：设计了新的Stem模块、自定义了Hiera块配置、调整了窗口注意力机制。该方法借鉴了SAM2模型、Receptive Field Blocks、k-NN插值和多种损失函数等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将SAM2视觉基础模型应用于3D点云分割，通过将点云转换为范围视图表示，利用2D分割模型的能力，再投影回3D空间。流程包括：1)范围投影预处理，将3D点云转换为64×2048像素的范围图像；2)模型架构，包含Stem模块、基于Hiera的编码器和Receptive Field Block解码器；3)后处理，使用k-NN插值将标签传播回3D点云；4)训练，使用复合损失函数和数据增强技术。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个将SAM2适应3D分割的范围视图框架；2)针对范围视图的编码器修改(Stem模块、自定义Hiera块、调整的注意力机制)；3)多组件编码器架构结合感受野块；4)有效的k-NN插值后处理技术；5)复合损失函数。相比之前的工作，RangeSAM利用了最先进的SAM2模型，设计了非对称注意力窗口强调水平空间关系，发现多数据集训练比2D预训练更有效，实现了与现有方法相竞争的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RangeSAM创新性地将SAM2视觉基础模型适应到范围视图表示的LiDAR点云分割中，通过专门设计的架构修改和训练策略，实现了与现有方法相竞争的性能，同时利用了2D分割模型的效率和可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud segmentation is central to autonomous driving and 3D sceneunderstanding. While voxel- and point-based methods dominate recent researchdue to their compatibility with deep architectures and ability to capturefine-grained geometry, they often incur high computational cost, irregularmemory access, and limited real-time efficiency. In contrast, range-viewmethods, though relatively underexplored - can leverage mature 2D semanticsegmentation techniques for fast and accurate predictions. Motivated by therapid progress in Visual Foundation Models (VFMs) for captioning, zero-shotrecognition, and multimodal tasks, we investigate whether SAM2, the currentstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone forLiDAR point cloud segmentation in the range view. We present , to ourknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,coupling efficient 2D feature extraction with standardprojection/back-projection to operate on point clouds. To optimize SAM2 forrange-view representations, we implement several architectural modifications tothe encoder: (1) a novel module that emphasizes horizontal spatial dependenciesinherent in LiDAR range images, (2) a customized configuration of tailored tothe geometric properties of spherical projections, and (3) an adapted mechanismin the encoder backbone specifically designed to capture the unique spatialpatterns and discontinuities present in range-view pseudo-images. Our approachachieves competitive performance on SemanticKITTI while benefiting from thespeed, scalability, and deployment simplicity of 2D-centric pipelines. Thiswork highlights the viability of VFMs as general-purpose backbones for 3Dperception and opens a path toward unified, foundation-model-driven LiDARsegmentation. Results lets us conclude that range-view segmentation methodsusing VFMs leads to promising results.</description>
      <author>example@mail.com (Paul Julius Kühn, Duc Anh Nguyen, Arjan Kuijper, Holger Graf, Dieter Fellner, Saptarshi Neil Sinha)</author>
      <guid isPermaLink="false">2509.15886v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</title>
      <link>http://arxiv.org/abs/2510.11649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM SIGGraphAsia 2025. Project website:  https://yuxuan-xue.com/physic&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhySIC是一个用于物理合理的人体-场景交互和接触重建的框架，可以从单张RGB图像中恢复度量一致的3D人体和场景，处理遮挡和深度模糊等问题，实现高效且高质量的重建。&lt;h4&gt;背景&lt;/h4&gt;从单张图像重建具有度量准确性的三维人体及其周围场景对于虚拟现实、机器人和全面的3D场景理解至关重要。然而，现有方法面临深度模糊、遮挡和物理不一致接触等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法面临的深度模糊、遮挡和物理不一致接触等问题，实现从单张图像中重建物理合理的人体-场景交互。&lt;h4&gt;方法&lt;/h4&gt;PhySIC从单张RGB图像恢复度量一致的SMPL-X人体网格、密集场景表面和顶点级接触图；执行遮挡感知的图像修复；融合可见深度与未缩放几何形状；合成缺失支撑表面；通过加权置信度优化优化人体姿态、相机参数和全局尺度；使用显式遮挡掩码保护不可见区域；高效处理多个人体交互。&lt;h4&gt;主要发现&lt;/h4&gt;PhySIC在单图像基线上表现更优，将场景平均每顶点误差从641毫米降低到227毫米，将PA-MPJPE减半至42毫米，并将接触F1从0.09提高到0.51；定性结果显示了真实的脚-地面交互、自然的坐姿以及对严重遮挡家具的合理重建。&lt;h4&gt;结论&lt;/h4&gt;通过将单张图像转换为物理合理的3D人体-场景对，PhySIC推动了可扩展的3D场景理解，实现仅需9秒的联合人体-场景优化和不到27秒的端到端处理。&lt;h4&gt;翻译&lt;/h4&gt;从单张图像重建具有度量准确性的人体及其周围场景对于虚拟现实、机器人和全面的3D场景理解至关重要。然而，现有方法难以处理深度模糊、遮挡和物理不一致接触等问题。为应对这些挑战，我们引入了PhySIC，一个用于物理合理的人体-场景交互和接触重建的框架。PhySIC从单张RGB图像中恢复度量一致的SMPL-X人体网格、密集场景表面和顶点级接触图，并在共享坐标系中表示。从粗略的单目深度和人体估计开始，PhySIC执行遮挡感知的图像修复，将可见深度与未缩放的几何形状融合以获得稳健的度量支架，并合成缺失的支撑表面如地板。通过加权置信度优化，通过联合强制深度对齐、接触先验、避免穿透和2D重投影一致性来优化人体姿态、相机参数和全局尺度。显式遮挡掩码保护不可见区域避免不合理的配置。PhySIC效率高，仅需9秒进行联合人体-场景优化，端到端时间不到27秒。自然处理多个人体，能够重建多样化的交互。经验表明，PhySIC优于单图像基线方法，将场景平均每顶点误差从641毫米降低到227毫米，将PA-MPJPE减半至42毫米，并将接触F1从0.09提高到0.51。定性结果显示了真实的脚-地面交互、自然的坐姿以及对严重遮挡家具的合理重建。通过将单张图像转换为物理合理的3D人体-场景对，PhySIC推动了可扩展的3D场景理解。我们的实现已在https://yuxuan-xue.com/physic公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从单张RGB图像中重建度量准确的3D人体和场景几何，以及它们之间的物理交互关系的问题。这个问题在现实和研究中非常重要，因为对于虚拟现实、机器人和全面的3D场景理解至关重要，但现有方法难以处理深度歧义、遮挡和物理上不一致的接触等挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人体和场景是相互约束的：场景在物理上限制了可能的人体姿势，而人体姿势为估计场景几何和规模提供了重要线索。基于这一观察，作者借鉴了多种现有技术，包括使用SAM2进行人体分割、OmniEraser进行图像修复、DepthPro预测度量深度、MoGe获取详细几何、SMPL-X表示人体等。作者设计了一个三阶段方法：首先估计度量规模场景，然后重建人体并与场景对齐，最后通过联合优化确保物理合理性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是联合优化人体姿势、场景几何和全局尺度，产生物理上合理的人体-场景对，同时利用人体和场景之间的物理约束关系提高重建质量。整体流程分为三阶段：1)度量规模场景估计：通过图像修复、结合度量深度和详细几何、地面平面拟合和组合场景点来获取完整场景；2)人体重建与场景对齐：将人体点与场景点对齐，使用SMPL-X模型表示人体，并优化网格与场景对齐；3)联合人体-场景优化：通过接触损失、遮挡感知的穿透损失和正则化项确保物理合理性，并能处理多个人体情况。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个能处理多个人体、多样场景和交互类型的度量规模人体-场景重建方法；2)引入了强大的初始化策略和遮挡感知的联合优化；3)高效的重建管道，27秒内完成端到端重建；4)能处理复杂交互如坐姿和脚-地面接触。相比之前的工作，PhySIC不需要视频输入或多视图图像(如HSR和HSfM)，也不需要预定义的场景扫描(如PROX)，能处理室内外多种场景，而不仅限于特定室内环境(如HolisticMesh)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PhySIC通过联合优化人体姿势、场景几何和接触信息，实现了从单张RGB图像中快速重建物理上合理的3D人体-场景交互，显著提高了重建的准确性和效率，为虚拟现实、机器人和3D场景理解提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3757377.3763862&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing metrically accurate humans and their surrounding scenes from asingle image is crucial for virtual reality, robotics, and comprehensive 3Dscene understanding. However, existing methods struggle with depth ambiguity,occlusions, and physically inconsistent contacts. To address these challenges,we introduce PhySIC, a framework for physically plausible Human-SceneInteraction and Contact reconstruction. PhySIC recovers metrically consistentSMPL-X human meshes, dense scene surfaces, and vertex-level contact maps withina shared coordinate frame from a single RGB image. Starting from coarsemonocular depth and body estimates, PhySIC performs occlusion-aware inpainting,fuses visible depth with unscaled geometry for a robust metric scaffold, andsynthesizes missing support surfaces like floors. A confidence-weightedoptimization refines body pose, camera parameters, and global scale by jointlyenforcing depth alignment, contact priors, interpenetration avoidance, and 2Dreprojection consistency. Explicit occlusion masking safeguards invisibleregions against implausible configurations. PhySIC is efficient, requiring only9 seconds for joint human-scene optimization and under 27 seconds end-to-end.It naturally handles multiple humans, enabling reconstruction of diverseinteractions. Empirically, PhySIC outperforms single-image baselines, reducingmean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,and improving contact F1 from 0.09 to 0.51. Qualitative results show realisticfoot-floor interactions, natural seating, and plausible reconstructions ofheavily occluded furniture. By converting a single image into a physicallyplausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.Our implementation is publicly available at https://yuxuan-xue.com/physic.</description>
      <author>example@mail.com (Pradyumna Yalandur Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll)</author>
      <guid isPermaLink="false">2510.11649v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2510.11567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新框架，利用扩散模型和不完美伪标签将合成数据适应到目标域，生成高保真图像，解决了合成数据与真实数据之间的差距问题，实验证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;合成数据集被广泛用于训练城市场景识别模型，但即使高度逼真的渲染图像与真实图像之间仍然存在明显差距。当适应特定目标领域（如Cityscapes）时，这种差距尤为明显，因为建筑、植被、物体外观和相机特性的差异限制了下游性能。&lt;h4&gt;目的&lt;/h4&gt;解决合成数据与真实数据之间的差距问题，避免使用昂贵的3D建模来缩小这一差距，因为这会违背低成本标记数据的目的。&lt;h4&gt;方法&lt;/h4&gt;提出一种新框架，使用不完美的伪标签将现成的扩散模型适应到目标域。训练后的模型可以从任何合成数据集的语义图中生成高保真、目标对齐的图像。该方法过滤次优生成结果，修正图像-标签不对齐问题，并标准化不同数据集的语义，将弱合成数据转换为具有竞争力的真实域训练集。&lt;h4&gt;主要发现&lt;/h4&gt;在五个合成数据集和两个真实目标数据集上的实验显示，与最先进的翻译方法相比，分割性能提升了高达+8.0% mIoU，使快速构建的合成数据集变得与需要大量手动设计的高投入、时间密集型合成数据集一样有效。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了一个有价值的协作范式，其中快速语义原型与生成模型相结合，为城市场景理解实现了可扩展的高质量训练数据创建。&lt;h4&gt;翻译&lt;/h4&gt;合成数据集被广泛用于训练城市场景识别模型，但即使高度逼真的渲染图像与真实图像之间仍然存在明显差距。当适应特定目标领域（如Cityscapes）时，这种差距尤为明显，因为建筑、植被、物体外观和相机特性的差异限制了下游性能。使用更详细的3D建模来缩小这一差距需要昂贵的资产和场景设计，违背了低成本标记数据的目的。为此，我们提出了一种新框架，仅使用不完美的伪标签将现成的扩散模型适应到目标域。一旦训练完成，它可以从任何合成数据集的语义图中生成高保真、目标对齐的图像，包括只需几小时而非数月创建的低投入来源。该方法过滤次优生成结果，修正图像-标签不对齐问题，并标准化不同数据集的语义，将弱合成数据转换为具有竞争力的真实域训练集。在五个合成数据集和两个真实目标数据集上的实验显示，与最先进的翻译方法相比，分割性能提升了高达+8.0% mIoU，使快速构建的合成数据集变得与需要大量手动设计的高投入、时间密集型合成数据集一样有效。这项工作展示了一个有价值的协作范式，其中快速语义原型与生成模型相结合，为城市场景理解实现了可扩展的高质量训练数据创建。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效生成高质量城市场景语义分割训练数据的问题，特别是如何将低成本的合成数据转换为真实世界场景的有效训练数据。这个问题重要是因为真实世界标注数据收集成本高、耗时长；即使最逼真的合成数据与真实图像间也存在明显差距，限制了下游模型性能；而传统3D建模方法需要昂贵资产和场景设计，违背了低成本标记数据的目的。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是：认识到合成数据与真实数据差距问题→探索扩散模型替代昂贵光照建模→提出3D建模与生成建模协作范式→设计两阶段策略解决视觉风格学习和语义对齐两个竞争任务。借鉴了现有工作包括：使用预训练扩散模型(Stable Diffusion 2.1)作为基础；采用伪标签提供语义条件；使用分类器自由引导提高样本质量；结合深度图作为正则化输入；利用连通组件分析评估生成质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过微调现成扩散模型适应特定目标域，使用不完美伪标签生成高保真目标对齐图像，采用对象中心过滤策略优化结果，并标准化不同数据集间的语义。整体流程：1)两阶段微调-第一阶段学习目标域视觉风格，第二阶段实现语义布局对齐；2)正则化技术-使用粗略伪标签、伪深度图和零条件引导提高鲁棒性；3)自动数据生成-为单个语义图生成多样样本，用MCOC评分筛选高质量样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)源无关框架，能从任何合成数据集生成目标域图像；2)两阶段训练策略，分别学习视觉风格和语义控制；3)多种正则化技术组合提高鲁棒性；4)对象中心样本选择机制提高数据质量。相比不同：1)与传统I2I方法相比，无需成对数据，能生成多样化样本，对源质量要求低，图像质量更高；2)与UDA方法相比，明确生成目标域图像，提供透明度，解耦数据生成与下游模型；3)与其他扩散模型方法相比，针对特定目标域微调，使用伪标签而非真实标签，结合多种正则化技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于扩散模型的框架，通过利用不完美伪标签和两阶段训练策略，将低努力合成的语义布局高效转换为高质量、目标域对齐的训练图像，显著提升了城市场景语义分割性能，并展示了快速语义场景原型与生成模型协作创建大规模高质量训练数据的范式转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic datasets are widely used for training urban scene recognitionmodels, but even highly realistic renderings show a noticeable gap to realimagery. This gap is particularly pronounced when adapting to a specific targetdomain, such as Cityscapes, where differences in architecture, vegetation,object appearance, and camera characteristics limit downstream performance.Closing this gap with more detailed 3D modelling would require expensive assetand scene design, defeating the purpose of low-cost labelled data. To addressthis, we present a new framework that adapts an off-the-shelf diffusion modelto a target domain using only imperfect pseudo-labels. Once trained, itgenerates high-fidelity, target-aligned images from semantic maps of anysynthetic dataset, including low-effort sources created in hours rather thanmonths. The method filters suboptimal generations, rectifies image-labelmisalignments, and standardises semantics across datasets, transforming weaksynthetic data into competitive real-domain training sets. Experiments on fivesynthetic datasets and two real target datasets show segmentation gains of upto +8.0%pt. mIoU over state-of-the-art translation methods, making rapidlyconstructed synthetic datasets as effective as high-effort, time-intensivesynthetic datasets requiring extensive manual design. This work highlights avaluable collaborative paradigm where fast semantic prototyping, combined withgenerative models, enables scalable, high-quality training data creation forurban scene understanding.</description>
      <author>example@mail.com (Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother)</author>
      <guid isPermaLink="false">2510.11567v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>mmWalk: Towards Multi-modal Multi-view Walking Assistance</title>
      <link>http://arxiv.org/abs/2510.11520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 Datasets and Benchmarks Track. Data and  Code: https://github.com/KediYing/mmWalk&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究构建了mmWalk，一个针对盲人或低视力人群的多模态数据集，以及mmWalkVQA基准，用于户外安全导航辅助。研究评估了现有视觉语言模型的表现，并展示了所构建数据集的有效性。&lt;h4&gt;背景&lt;/h4&gt;盲人或低视力人群在极端或复杂环境中的行走辅助仍然是一个重大挑战，主要是因为缺乏整体场景理解能力。&lt;h4&gt;目的&lt;/h4&gt;为了满足盲人或低视力社区的实际需求，构建一个模拟的多模态数据集，用于户外安全导航。&lt;h4&gt;方法&lt;/h4&gt;构建了mmWalk数据集，包含120条手动控制的、场景分类的行走轨迹和62k个同步帧；收集了超过559k张RGB、深度和语义模态的全景图像；每条轨迹都包含户外极端情况和可访问性特定地标；创建了包含69k个视觉问答三元组的mmWalkVQA基准；评估了最先进的视觉语言模型在零样本和少样本设置下的表现。&lt;h4&gt;主要发现&lt;/h4&gt;最先进的视觉语言模型在风险评估和导航任务上表现不佳；在真实世界数据集上验证了mmWalk微调模型的有效性。&lt;h4&gt;结论&lt;/h4&gt;所构建的数据集对于推进多模态行走辅助技术有效。&lt;h4&gt;翻译&lt;/h4&gt;在极端或复杂环境中为盲人或低视力人群(BLV)提供行走辅助仍然是一个重大挑战，主要由于缺乏整体场景理解能力。受BLV社区的现实需求驱动，我们构建了mmWalk，这是一个模拟的多模态数据集，集成了多视图传感器和面向可访问性的特征，用于户外安全导航。我们的数据集包含120条手动控制、场景分类的行走轨迹，有62k个同步帧。它包含RGB、深度和语义模态下的超过559k张全景图像。此外，为了强调实际相关性，每条轨迹都包含户外极端情况和BLV用户特定的可访问性地标。此外，我们生成了mmWalkVQA，一个包含9个类别、超过69k个视觉问答三元组的VQA基准，专为安全和知情行走辅助而定制。我们使用零样本和少样本设置评估了最先进的视觉语言模型(VLMs)，发现它们在风险评估和导航任务上表现不佳。我们在真实世界数据集上验证了mmWalk微调模型，展示了我们的数据集在推进多模态行走辅助方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决盲人或低视力人士在极端或复杂环境中行走的安全辅助问题。这个问题很重要，因为全球有超过22亿人受盲症或低视力影响，超过63%的BLV人士在户外导航时受过伤害，7%的人每月至少摔倒一次，而现有导航系统未能充分识别危险状况、不平整表面和临时障碍物。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于BLV社区的真实需求设计方法，在Carla模拟器中手动收集多视角(步行者、导盲犬、无人机)和多模态(RGB、深度、语义分割)数据。借鉴了现有多视图辅助系统如OpenMPR和MSSP，以及视觉辅助数据集如VizWiz、GuideDog和SideGuide。同时参考了ATmaps统计中的地标信息，并定义了8种BLV相关的特殊情况。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个多模态、多视角的行走辅助数据集，特别关注BLV用户的安全需求，通过全景图像和多种传感器数据提供全面的场景理解。流程包括：在模拟器中收集120条轨迹和559k全景图像；标注轨迹元数据和特殊情况；使用GPT-4o生成69k视觉问答三元组；评估多种视觉语言模型；在真实世界数据集上验证微调模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个结合多视角和可访问性特征的多模态数据集；包含RGB、深度和语义分割的全景图像；专门针对BLV的特殊情况和导航地标；包含69k视觉问答三元组的基准测试；全面评估现有模型局限性。相比之前工作，它特别关注BLV安全需求，提供多视角同步数据，结合全景图像和多种传感器模态，且VQA数据集规模更大、类别更丰富。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; mmWalk论文贡献了一个多模态、多视角的行走辅助数据集和基准测试，特别关注盲人和低视力人士的安全需求，为开发更安全、更全面的行走辅助系统提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Walking assistance in extreme or complex environments remains a significantchallenge for people with blindness or low vision (BLV), largely due to thelack of a holistic scene understanding. Motivated by the real-world needs ofthe BLV community, we build mmWalk, a simulated multi-modal dataset thatintegrates multi-view sensor and accessibility-oriented features for outdoorsafe navigation. Our dataset comprises 120 manually controlled,scenario-categorized walking trajectories with 62k synchronized frames. Itcontains over 559k panoramic images across RGB, depth, and semantic modalities.Furthermore, to emphasize real-world relevance, each trajectory involvesoutdoor corner cases and accessibility-specific landmarks for BLV users.Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visualquestion-answer triplets across 9 categories tailored for safe and informedwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)using zero- and few-shot settings and found they struggle with our riskassessment and navigational tasks. We validate our mmWalk-finetuned model onreal-world datasets and show the effectiveness of our dataset for advancingmulti-modal walking assistance.</description>
      <author>example@mail.com (Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen)</author>
      <guid isPermaLink="false">2510.11520v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>REACT3D: Recovering Articulations for Interactive Physical 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.11340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REACT3D是一个可扩展的零样本框架，能够将静态3D场景转换为模拟就绪的交互式副本，具有一致的几何形状，可直接用于各种下游任务。&lt;h4&gt;背景&lt;/h4&gt;交互式3D场景对具身智能日益重要，但现有数据集有限，因为注释部分分割、运动类型和运动轨迹的过程是劳动密集型的。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的零样本框架，解决静态3D场景转换为交互式副本的难题，降低关节场景理解的大规模研究门槛。&lt;h4&gt;方法&lt;/h4&gt;包括四个主要贡献：可打开物体检测和分割；关节估计推断关节类型和运动参数；隐藏几何形状补全与交互式物体组装；交互式场景集成以确保与标准模拟平台的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的室内场景中，REACT3D在检测/分割和关节度量方面取得了最先进的性能，证明了框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;REACT3D为可扩展的交互式场景生成提供了实际基础，为大规模关节场景理解研究创造了条件。&lt;h4&gt;翻译&lt;/h4&gt;交互式3D场景对于具身智能越来越重要，然而现有的数据集仍然有限，因为注释部分分割、运动类型和运动轨迹的过程是劳动密集型的。我们提出了REACT3D，一个可扩展的零样本框架，将静态3D场景转换为模拟就绪的交互式副本，具有一致的几何形状，能够直接用于各种下游任务。我们的贡献包括：(i)可打开物体检测和分割，从静态场景中提取候选可移动部分；(ii)关节估计，推断关节类型和运动参数；(iii)隐藏几何形状补全，然后进行交互式物体组装；(iv)交互式场景集成，以广泛支持的格式确保与标准模拟平台的兼容性。我们在多样化的室内场景中，在检测/分割和关节度量方面取得了最先进的性能，证明了我们框架的有效性，并为可扩展的交互式场景生成提供了实际基础，从而降低了关节场景理解的大规模研究门槛。我们的项目页面是react3d.github.io。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将静态3D场景转换为具有交互功能的物理3D场景，特别是识别和恢复场景中可动关节（如门、抽屉等可开合物体）的问题。这个问题在现实中很重要，因为交互式3D场景对虚拟现实、游戏、电影制作以及机器人系统开发至关重要，而现有数据集由于标注过程劳动密集而受限，自动化生成此类场景能有效扩展研究规模并降低使用门槛。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者设计REACT3D时借鉴了多项现有工作：使用RAM++和LLaVA进行语义识别，Grounded SAM进行分割，OPDMulti进行关节估计，并改进了DRAWER的多视图融合方法。作者认识到现有方法在处理开放词汇物体、关节估计精度和隐藏几何生成方面的不足，因此设计了结合视觉基础模型和视觉语言模型的零样本框架，通过开放词汇检测、关节细化、隐藏几何生成和场景集成四个步骤实现静态到交互场景的转换。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉基础模型和视觉语言模型从静态3D场景中恢复物体关节，生成物理启用的交互式数字孪生。整体流程分为四个主要步骤：1) 开放物体检测和分割，识别可动物体并提取可动部分；2) 关节估计，推断关节类型和运动参数并进行细化；3) 隐藏几何生成，完成物体内部腔体结构；4) 交互场景集成，将可动物体与静态背景结合，生成纹理并导出为兼容多种模拟平台的格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 开放词汇检测方法减轻了标签偏差，提高了对长尾可动物体的覆盖；2) 基于定向边界框的关节细化提高了参数准确性；3) 隐藏几何生成解决了物体内部结构缺失问题；4) 多平台兼容的导出格式确保了广泛适用性。相比前人工作，REACT3D是零样本方法，无需针对特定类别训练；改进了多视图融合策略；引入了几何驱动的关节细化；并考虑了隐藏几何生成，而不仅仅是表面几何。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; REACT3D提供了一个创新的零样本框架，能够将静态3D场景转换为具有物理交互功能的数字孪生体，通过开放词汇检测、关节细化、隐藏几何生成和无缝集成，实现了在多种平台上就绪的交互式场景生成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interactive 3D scenes are increasingly vital for embodied intelligence, yetexisting datasets remain limited due to the labor-intensive process ofannotating part segmentation, kinematic types, and motion trajectories. Wepresent REACT3D, a scalable zero-shot framework that converts static 3D scenesinto simulation-ready interactive replicas with consistent geometry, enablingdirect use in diverse downstream tasks. Our contributions include: (i)openable-object detection and segmentation to extract candidate movable partsfrom static scenes, (ii) articulation estimation that infers joint types andmotion parameters, (iii) hidden-geometry completion followed by interactiveobject assembly, and (iv) interactive scene integration in widely supportedformats to ensure compatibility with standard simulation platforms. We achievestate-of-the-art performance on detection/segmentation and articulation metricsacross diverse indoor scenes, demonstrating the effectiveness of our frameworkand providing a practical foundation for scalable interactive scene generation,thereby lowering the barrier to large-scale research on articulated sceneunderstanding. Our project page is\textit{\hypersetup{urlcolor=black}\href{https://react3d.github.io/}{react3d.github.io}}.</description>
      <author>example@mail.com (Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys)</author>
      <guid isPermaLink="false">2510.11340v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Real2USD: Scene Representations in Universal Scene Description Language</title>
      <link>http://arxiv.org/abs/2510.10778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 10 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用通用场景描述（USD）语言作为大型语言模型（LLMs）机器人任务中环境表示的有效通用方法，并展示了'Real to USD'系统的实际应用。&lt;h4&gt;背景&lt;/h4&gt;现有的大语言模型机器人方法都是针对特定任务的，如用于导航的视觉语言模型、用于映射的语言引导神经辐射场等，缺乏通用性。&lt;h4&gt;目的&lt;/h4&gt;论证USD语言是LLM机器人任务中环境几何、光度学和语义信息的有效且通用的表示方法。&lt;h4&gt;方法&lt;/h4&gt;开发'Real to USD'系统，使用搭载LiDAR和RGB相机的Unitree Go2四足机器人构建室内环境USD表示，并用谷歌Gemini解析USD实现场景理解、推理和规划；同时在模拟仓库和医院环境中测试系统。&lt;h4&gt;主要发现&lt;/h4&gt;USD是基于XML的场景图，可被LLM和人类阅读，足够丰富以支持几乎所有任务，皮克斯开发此语言用于存储资产、场景甚至电影。&lt;h4&gt;结论&lt;/h4&gt;USD是LLM机器人任务中环境表示的有效通用方法，能够处理多样化物体和具有挑战性的环境。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）可以帮助机器人对抽象任务规范进行推理。这需要在机器人使用的经典环境表示基础上增加基于自然语言的先验知识。现有方法都是针对特定任务的，例如用于导航的视觉语言模型、用于映射的语言引导神经辐射场等。本文提出通用场景描述（USD）语言作为LLM机器人任务中环境几何、光度学和语义信息的有效且通用的表示方法。我们的论点很简单：USD是一种基于XML的场景图，可被LLM和人类阅读，足够丰富以支持几乎所有任务——皮克斯开发这种语言是为了存储资产、场景甚至电影。我们使用搭载LiDAR和RGB相机的Unitree Go2四足机器人展示了'Real to USD'系统，该系统能够（i）构建包含多样物体和大量玻璃等挑战性室内环境的显式USD表示，以及（ii）使用谷歌的Gemini解析USD以展示场景理解、复杂推理和规划能力。我们还使用Nvidia的Issac Sim在模拟仓库和医院环境中研究了该系统的不同方面。代码可在https://github.com/grasp-lyrl/Real2USD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何为机器人创建一个通用的场景表示方法，该方法能够结合几何、光学和语义信息，并且能被大型语言模型读取和理解。这个问题很重要，因为当前机器人系统主要使用度量表示进行环境建模，缺乏人类使用的丰富语义信息，而结合语义和度量信息对机器人构建有效的空间表示至关重要。现有方法通常针对特定任务设计，需要一种通用且有效的场景表示方法来支持基于自然语言的机器人任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到机器人需要结合语义和度量信息，然后提出使用USD（Universal Scene Description）语言作为解决方案，因为它是基于XML的场景图，可被LLMs和人类读取，且足够丰富支持各种任务。作者借鉴了3D度量-语义场景图方法（如SLAM、神经辐射场）和'Real to Sim to Real'研究思路，以及使用深度学习进行语义分割和基础模型进行大型数据库搜索的方法。作者设计了'Real2USD'系统，使用四足机器人携带传感器构建室内环境的USD表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用USD语言作为通用场景表示，将真实世界场景转换为USD格式，使其能被大型语言模型读取和理解，从而支持基于自然语言的任务规划。整体流程包括：1)资产识别和检索（使用YOLOE检测对象，CLIP和FAISS检索资产）；2)资产定位（使用模拟器生成资产点云，通过ICP算法配准）；3)资产协调（使用非极大值抑制和评分系统选择最佳资产，物理模拟确保合理性）；4)场景理解与任务规划（使用LLM解析USD，生成路径点和导航计划）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用USD作为通用场景表示；2)开发了完整的Real2USD系统；3)提出模拟循环配准方法提高准确性；4)设计物理协调机制确保场景合理性；5)实现与LLM的集成支持复杂语义任务。相比之前工作，本文方法更具通用性（而非针对特定任务），使用结构化资产表示（而非非结构化网格），可直接被LLM解析（无需额外注释），并能处理更大更复杂的场景（而非单个图像或文本提示生成的小场景）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出并验证了一种使用USD语言作为通用场景表示的方法，使机器人能够将真实世界环境转换为可被大型语言模型读取和理解的格式，从而支持复杂的语义任务规划。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) can help robots reason about abstract taskspecifications. This requires augmenting classical representations of theenvironment used by robots with natural language-based priors. There are anumber of existing approaches to doing so, but they are tailored to specifictasks, e.g., visual-language models for navigation, language-guided neuralradiance fields for mapping, etc. This paper argues that the Universal SceneDescription (USD) language is an effective and general representation ofgeometric, photometric and semantic information in the environment forLLM-based robotics tasks. Our argument is simple: a USD is an XML-based scenegraph, readable by LLMs and humans alike, and rich enough to supportessentially any task -- Pixar developed this language to store assets, scenesand even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2quadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USDrepresentation of indoor environments with diverse objects and challengingsettings with lots of glass, and (ii) parses the USD using Google's Gemini todemonstrate scene understanding, complex inferences, and planning. We alsostudy different aspects of this system in simulated warehouse and hospitalsettings using Nvidia's Issac Sim. Code is available athttps://github.com/grasp-lyrl/Real2USD .</description>
      <author>example@mail.com (Christopher D. Hsu, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2510.10778v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding</title>
      <link>http://arxiv.org/abs/2510.10194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;使用自然语言定位3D物体对机器人场景理解至关重要。然而，描述中通常涉及多个空间关系来区分相似物体，这使得3D-语言对齐变得困难。当前方法仅对成对物体建模关系，忽略了多模态关系理解中n元组合的全局感知显著性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在3D物体定位中只考虑成对关系而忽视n元组合全局感知的问题，提出一种新的渐进式关系学习框架。&lt;h4&gt;方法&lt;/h4&gt;将关系学习从二元扩展到n元，识别与参照描述全局匹配的视觉关系；设计分组监督损失促进n元关系学习（因训练数据中缺乏参照对象的特定标注）；在n元关系创建的场景图中，使用具有混合注意力机制的多模态网络进一步定位n元组合中的目标。&lt;h4&gt;主要发现&lt;/h4&gt;在ReferIt3D和ScanRefer基准上的实验和消融研究表明，该方法优于现有最先进技术，证明了n元关系感知在3D定位中的优势。&lt;h4&gt;结论&lt;/h4&gt;n元关系感知对3D物体定位至关重要，提出的渐进式关系学习框架有效解决了现有方法的局限性，提高了3D物体定位的准确性。&lt;h4&gt;翻译&lt;/h4&gt;使用自然语言定位3D物体对机器人场景理解至关重要。描述通常涉及多个空间关系来区分相似物体，这使得3D-语言对齐变得困难。当前方法仅对成对物体建模关系，忽略了多模态关系理解中n元组合的全局感知显著性。为解决这一问题，我们提出了一种用于3D物体定位的新型渐进式关系学习框架。我们将关系学习从二元扩展到n元，以识别与参照描述全局匹配的视觉关系。鉴于训练数据中缺乏参照对象的特定标注，我们设计了一种分组监督损失来促进n元关系学习。在使用n元关系创建的场景图中，我们使用具有混合注意力机制的多模态网络进一步定位n元组合中的目标。在ReferIt3D和ScanRefer基准上的实验和消融研究表明，我们的方法优于最先进技术，证明了n元关系感知在3D定位中的优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D物体定位问题，即在3D场景中根据自然语言描述来准确定位特定物体。这个问题在机器人场景理解中至关重要，因为现实世界任务通常需要自然语言指令来指导行动。当场景中有多个相似物体时，人们必须通过多个空间关系来描述目标位置，这需要模型能够同时理解多个关系，而当前方法仅能处理成对物体关系，难以应对复杂场景中的全局关系理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考：当前方法仅建模成对物体关系，忽略了多关系描述的全局感知。作者借鉴了Transformer框架、场景图构建、图神经网络和大型语言模型等现有技术，但创新性地提出了从二元到n元的渐进式关系学习框架。作者特别利用LLM提取实体关系作为训练监督，并设计了分组监督损失来处理指代物体不确定性，从而实现了更全面的关系理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过渐进式关系学习从简单二元关系扩展到复杂n元关系，实现全局关系感知。整体流程包括：1) 编码文本和3D物体特征；2) 使用B2N-PRL模块进行二元关系建模，再基于此进行n元关系建模；3) 选择top K2个n元组合构建场景图；4) 通过混合注意力机制(自注意力、图注意力、交叉注意力)更新节点特征；5) 使用MLP输出目标置信度并定位。整个过程通过分组监督损失进行训练，优化关系学习和目标定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 从二元到n元的渐进式关系学习框架，实现全局关系感知；2) 分组监督损失设计，处理训练数据中指代物体不确定性；3) 基于n元关系而非所有相邻实体构建场景图，减少噪声；4) 混合注意力机制的多模态网络增强空间感知。相比之前工作，本文突破了仅处理成对关系的局限，通过全局n元关系理解显著提升了复杂多关系描述下的定位准确性，特别是在有相似物体干扰的场景中表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; B2N3D通过从二元到n元的渐进式关系学习和注意力驱动的图学习，显著提升了复杂多关系描述下的3D物体定位性能，实现了全局关系感知和更准确的目标定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Localizing 3D objects using natural language is essential for robotic sceneunderstanding. The descriptions often involve multiple spatial relationships todistinguish similar objects, making 3D-language alignment difficult. Currentmethods only model relationships for pairwise objects, ignoring the globalperceptual significance of n-ary combinations in multi-modal relationalunderstanding. To address this, we propose a novel progressive relationallearning framework for 3D object grounding. We extend relational learning frombinary to n-ary to identify visual relations that match the referentialdescription globally. Given the absence of specific annotations for referredobjects in the training data, we design a grouped supervision loss tofacilitate n-ary relational learning. In the scene graph created with n-aryrelationships, we use a multi-modal network with hybrid attention mechanisms tofurther localize the target within the n-ary combinations. Experiments andablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that ourmethod outperforms the state-of-the-art, and proves the advantages of the n-aryrelational perception in 3D localization.</description>
      <author>example@mail.com (Feng Xiao, Hongbin Xu, Hai Ci, Wenxiong Kang)</author>
      <guid isPermaLink="false">2510.10194v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>CapGeo: A Caption-Assisted Approach to Geometric Reasoning</title>
      <link>http://arxiv.org/abs/2510.09302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出CapGeo框架，通过将几何图形转换为文本描述来提升多模态大语言模型的几何推理能力，并创建了CapGeo-Bench评估基准数据集。&lt;h4&gt;背景&lt;/h4&gt;几何推理是多模态大语言模型的核心挑战，即使是最先进的系统如GPT-O3和Gemini-2.5-Pro在解决几何问题时仍不可靠，尽管它们在文本推理任务上表现优异，表明瓶颈在于理解几何图形而非推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种将视觉内容转换为文本描述的方法，以提升MLLMs的几何推理能力，并创建相应的评估基准。&lt;h4&gt;方法&lt;/h4&gt;引入CapGeo标题辅助推理框架连接视觉和文本模态，提出CapGeo-Bench数据集包含4,641个精选图形-标题对，并开发基于关键点的评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;配备标题后模型性能显著提升：Qwen2.5-VL-72B从8.6%提升到59.0%，Claude-Opus-4从44.8%提升到73.0%。&lt;h4&gt;结论&lt;/h4&gt;CapGeo框架和CapGeo-Bench基准为提升多模态大语言模型中的几何推理能力提供了一条新途径。&lt;h4&gt;翻译&lt;/h4&gt;几何推理仍然是对多模态大语言模型的核心挑战。即使是最先进的闭源系统，如GPT-O3和Gemini-2.5-Pro，在解决几何问题时仍然不可靠，尽管它们在国际数学奥林匹克竞赛等任务上表现出强大的文本推理能力。这一差距表明，瓶颈在于理解几何图形而非推理本身。由于几何图形通常可以用简洁的文本形式忠实描述，将视觉内容转换为标题是一个有前景的方向。受此启发，我们引入了CapGeo，一种标题辅助推理框架，连接视觉和文本模态。实验表明，当模型配备标题时，性能有显著提升：Qwen2.5-VL-72B从仅使用视觉的8.6%提升到59.0%，而Claude-Opus-4从44.8%提升到73.0%。为了系统评估和识别高质量的几何标题生成模型，我们进一步提出了CapGeo-Bench，一个包含4,641个精选图形-标题对的数据集。重要的是，CapGeo-Bench包含一个基于关键点的评估指标，该指标与下游CapGeo性能高度相关，能够可靠评估几何标题生成能力。我们的框架和基准共同为提升多模态大语言模型中的几何推理能力指明了一条新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric reasoning remains a core challenge for Multimodal Large LanguageModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3and Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despiteexhibiting strong textual reasoning abilities on tasks like the InternationalMathematical Olympiad (IMO). This gap suggests that the bottleneck lies inunderstanding geometric diagrams rather than reasoning itself. Since geometricfigures can often be faithfully described in concise textual form, convertingvisual content into captions offers a promising direction. Motivated by thisinsight, we introduce CapGeo, a caption-assisted reasoning framework thatbridges visual and textual modalities. Experiments show substantialimprovements when models are equipped with captions: Qwen2.5-VL-72B improvesfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to73.0%. To systematically evaluate and identify high-quality geometriccaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curatedfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-basedevaluation metric that correlates strongly with downstream CapGeo performance,enabling reliable assessment of geometric captioning ability. Together, ourframework and benchmark highlight a new pathway toward advancing geometricreasoning in MLLMs.</description>
      <author>example@mail.com (Yuying Li, Siyi Qian, Hao Liang, Leqi Zheng, Ruichuan An, Yongzhen Guo, Wentao Zhang)</author>
      <guid isPermaLink="false">2510.09302v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2510.09266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CFVBench基准和自适应视觉细化(AVR)框架，解决了多模态检索增强生成模型在捕捉细粒度多模态细节方面的瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;多模态检索增强生成(MRAG)使多模态大语言模型能够利用外部多模态证据生成响应，但现有基准在模态覆盖和格式多样性方面有限，常专注于单模态任务或粗粒度场景理解。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准在模态覆盖和格式多样性方面的局限性，引入一个大规模、人工验证的基准来评估模型在检索和生成阶段的能力。&lt;h4&gt;方法&lt;/h4&gt;构建CFVBench基准，包含599个公开视频产生5,360个开放式问答对，涵盖图表密集报告、新闻广播和软件教程等多种格式和领域；评估7种检索方法和14种MLLMs；提出自适应视觉细化(AVR)框架，自适应增加帧采样密度并选择性调用外部工具。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型(即使是GPT-4或Gemini)难以捕捉短暂但重要的细粒度多模态细节；AVR框架能够增强细粒度多模态理解，提高所有评估的MLLMs的性能。&lt;h4&gt;结论&lt;/h4&gt;AVR是一种简单而有效的框架，可以解决当前模型在捕捉细粒度多模态细节方面的瓶颈，提升多模态检索增强生成模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态检索增强生成(MRAG)使多模态大语言模型能够利用外部多模态证据生成响应，许多基于视频的MRAG基准已被提出以评估模型在检索和生成阶段的能力。然而，现有基准在模态覆盖和格式多样性方面仍然有限，常专注于单模态任务或粗粒度场景理解。为解决这些差距，我们引入了CFVBench，这是一个从599个公开视频中构建的大规模、人工验证的基准，产生5,360个开放式问答对。CFVBench涵盖图表密集报告、新闻广播和软件教程等高密度格式和领域，要求模型检索和推理长时间视频跨度，同时保持细粒度多模态信息。使用CFVBench，我们系统评估了7种检索方法和14种广泛使用的MLLMs，揭示了一个关键瓶颈：当前模型(即使是GPT-4或Gemini)难以捕捉短暂但重要的细粒度多模态细节。为缓解这一问题，我们提出了自适应视觉细化(AVR)，这是一个简单而有效的框架，自适应增加帧采样密度并在必要时选择性调用外部工具。实验表明，AVR一致地增强细粒度多模态理解，并提高所有评估的MLLMs的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal LargeLanguage Models (MLLMs) to generate responses with external multimodalevidence, and numerous video-based MRAG benchmarks have been proposed toevaluate model capabilities across retrieval and generation stages. However,existing benchmarks remain limited in modality coverage and format diversity,often focusing on single- or limited-modality tasks, or coarse-grained sceneunderstanding. To address these gaps, we introduce CFVBench, a large-scale,manually verified benchmark constructed from 599 publicly available videos,yielding 5,360 open-ended QA pairs. CFVBench spans high-density formats anddomains such as chart-heavy reports, news broadcasts, and software tutorials,requiring models to retrieve and reason over long temporal video spans whilemaintaining fine-grained multimodal information. Using CFVBench, wesystematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealinga critical bottleneck: current models (even GPT5 or Gemini) struggle to capturetransient yet essential fine-grained multimodal details. To mitigate this, wepropose Adaptive Visual Refinement (AVR), a simple yet effective framework thatadaptively increases frame sampling density and selectively invokes externaltools when necessary. Experiments show that AVR consistently enhancesfine-grained multimodal comprehension and improves performance across allevaluated MLLMs</description>
      <author>example@mail.com (Kaiwen Wei, Xiao Liu, Jie Zhang, Zijian Wang, Ruida Liu, Yuming Yang, Xin Xiao, Xiao Sun, Haoyang Zeng, Changzai Pan, Yidan Zhang, Jiang Zhong, Peijin Wang, Yingchao Feng)</author>
      <guid isPermaLink="false">2510.09266v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities</title>
      <link>http://arxiv.org/abs/2510.08759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了BEAR基准测试，用于全面评估多模态大语言模型(MLLMs)的具身能力，并提出了BEAR-Agent模型来提升这些能力。研究显示现有MLLMs在具身能力方面存在局限，而新模型能显著改善性能。&lt;h4&gt;背景&lt;/h4&gt;具身能力是指代理感知、理解和与物理世界交互的基本能力。尽管MLLMs作为具身代理有潜力，但现有基准测试主要关注特定领域，缺乏对MLLMs具身能力的全面系统性评估。&lt;h4&gt;目的&lt;/h4&gt;弥补对MLLMs具身能力评估的空白，引入一个全面且细致的基准测试来评估MLLMs在基本具身能力方面的表现。&lt;h4&gt;方法&lt;/h4&gt;提出BEAR基准测试，包含6个类别14个领域的4469个交错图像-视频-文本条目，涵盖从低级指向、轨迹理解到高级规划的任务；评估20个代表性MLLMs；提出BEAR-Agent，一个整合预训练视觉模型的多模态可对话代理，以增强MLLMs的感知、3D理解和规划能力。&lt;h4&gt;主要发现&lt;/h4&gt;20个代表性MLLMs在所有具身能力领域都存在持续限制；BEAR-Agent显著提高了MLLMs在BEAR上的表现，在GPT-5上实现了9.12%的绝对增益和17.5%的相对改进；提高MLLMs的具身能力有利于模拟环境中的具身任务。&lt;h4&gt;结论&lt;/h4&gt;BEAR基准测试填补了对MLLMs具身能力系统评估的空白；BEAR-Agent模型能有效提升MLLMs在具身能力方面的表现，为具身AI研究提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;具身能力是指代理用于感知、理解和与物理世界交互的一系列基本能力。虽然多模态大语言模型(MLLMs)作为具身代理显示出潜力，但对它们具身能力的全面系统性评估仍然不足，因为现有基准测试主要关注规划或空间理解等特定领域。为了弥补这一差距，我们引入了BEAR，这是一个全面且细致的基准，用于评估MLLMs在基本具身能力方面的表现。BEAR包含6个类别14个领域中的4469个交错图像-视频-文本条目，包括从低级指向、轨迹理解、空间推理到高级规划的任务。对20个代表性MLLMs的广泛评估结果显示，它们在所有具身能力领域都存在持续的限制。为了解决这一不足，我们提出了BEAR-Agent，一个整合预训练视觉模型的多模态可对话代理，以增强MLLMs的感知、3D理解和规划能力。它在BEAR上显著提高了MLLMs在各种具身能力方面的表现，在GPT-5上实现了9.12%的绝对增益和17.5%的相对改进。此外，我们的实验表明，提高MLLMs的具身能力有利于模拟环境中的具身任务。项目网站：https://bear-official66.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)缺乏系统性的具身能力评估问题。这个问题很重要，因为具身能力是AI系统感知、理解和与物理世界交互的基础能力，对开发能在开放环境中有效工作的AI代理至关重要。现有评估基准主要关注特定领域，无法全面评估MLLMs的具身能力，限制了我们对它们潜力的理解和开发方向的指导。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有工作的局限性来设计BEAR基准。他们发现现有工作要么专注于特定领域(如指向、空间理解)，要么关注能力导向任务但未分解为逐步技能。作者从大规模具身家庭活动数据集(如BEHAVIOR-1K和ALFRED)中归纳分类，从人类认知过程中获取灵感，设计了将具身能力组织为6个类别和14个原子技能的基准。他们确实借鉴了现有工作，包括使用多种视觉模型和工具来增强BEAR-Agent，并参考了现有数据集和评估方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建全面基准(BEAR)系统评估MLLMs的具身能力，并基于评估结果开发增强型代理(BEAR-Agent)提升这些能力。BEAR基准包含4,469个交错图像-视频-文本条目，组织为6个类别和14个原子技能。BEAR-Agent是一个多模态可对话代理，通过对话与MLLM交互，提供工具增强视觉和空间能力，为不同类别提供特定模块(如对象检测、深度估计)，通过提供额外线索帮助模型生成更准确答案，最终提升MLLMs在具身任务中的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) BEAR基准首次系统评估MLLMs具身能力，将能力组织为6个类别和14个原子技能；2) 包含4,469个交错图像-视频-文本条目，首次将具身任务分解为面向技能的步骤；3) BEAR-Agent多模态可对话代理显著提升性能，GPT-5提升9.12%；4) 在模拟环境中也表现优异，提升超过20.17%。相比之前工作，BEAR首次提供全面细粒度评估，BEAR-Agent不仅提升离线评估能力，还改进实际任务执行，并提供详细失败分析揭示MLLMs瓶颈。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了BEAR基准首次系统评估多模态大语言模型的具身能力，并基于评估结果开发了BEAR-Agent，显著提升了这些能力在评估和实际任务中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied capabilities refer to a suite of fundamental abilities for an agentto perceive, comprehend, and interact with the physical world. While multimodallarge language models (MLLMs) show promise as embodied agents, a thorough andsystematic evaluation of their embodied capabilities remains underexplored, asexisting benchmarks primarily focus on specific domains such as planning orspatial understanding. To bridge this gap, we introduce BEAR, a comprehensiveand fine-grained benchmark that evaluates MLLMs on atomic embodiedcapabilities. BEAR comprises 4,469 interleaved image-video-text entries across14 domains in 6 categories, including tasks from low-level pointing, trajectoryunderstanding, spatial reasoning, to high-level planning. Extensive evaluationresults of 20 representative MLLMs reveal their persistent limitations acrossall domains of embodied capabilities. To tackle the shortfall, we proposeBEAR-Agent, a multimodal conversable agent that integrates pretrained visionmodels to strengthen MLLM perception, 3D understanding, and planningcapabilities. It substantially enhances MLLM performance across diverseembodied capabilities on BEAR, yielding a 9.12% absolute gain and a relativeimprovement of 17.5% on GPT-5. Furthermore, our experiments indicate thatimproving MLLM embodied capabilities can benefit embodied tasks in simulatedenvironments. Project website: https://bear-official66.github.io/</description>
      <author>example@mail.com (Yu Qi, Haibo Zhao, Ziyu Guo, Siyuan Ma, Ziyan Chen, Yaokun Han, Renrui Zhang, Zitiantao Lin, Shiji Xin, Yijian Huang, Kai Cheng, Peiheng Wang, Jiazheng Liu, Jiayi Zhang, Yizhe Zhu, Wenqing Wang, Yiran Qin, Xupeng Zhu, Haojie Huang, Lawson L. S. Wong)</author>
      <guid isPermaLink="false">2510.08759v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</title>
      <link>http://arxiv.org/abs/2510.07869v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://vincentgu2000.github.io/u0project/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了USIM数据集和U0模型，用于解决水下机器人面临的挑战，通过多任务视觉-语言-动作框架实现水下机器人的自主操作。&lt;h4&gt;背景&lt;/h4&gt;水下环境对机器人操作提出了独特挑战，包括复杂流体动力学、能见度有限和通信受限。虽然数据驱动方法已推动陆地机器人发展，但开发能自主执行多项任务的水下智能仍然困难，因为大规模高质量水下数据集稀缺。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些限制，作者引入了USIM，这是一个基于模拟的水下机器人多任务视觉-语言-动作数据集。&lt;h4&gt;方法&lt;/h4&gt;USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9种不同场景中的20项任务。基于此数据集，作者提出了U0模型，通过多模态融合整合双目视觉和其他传感器模态，并采用基于卷积-注意力的感知焦点增强模块提高空间理解和移动操作能力。&lt;h4&gt;主要发现&lt;/h4&gt;在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率；在具有挑战性的移动操作任务中，与基线方法相比，将到目标的距离减少了21.2%。&lt;h4&gt;结论&lt;/h4&gt;USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展数据集构建、提高任务自主性和实现智能通用水下机器人的实际应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的可见性和受限的通信。尽管数据驱动方法已经推动了陆地机器人的具身智能发展，并使特定任务的水下自主机器人成为可能，但开发能够自主执行多项任务的水下智能仍然极具挑战性，因为大规模、高质量的水下数据集仍然稀缺。为了解决这些限制，我们引入了USIM，这是一个基于模拟的水下机器人多任务视觉-语言-动作数据集。USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9种不同场景中的20项任务，范围从视觉导航到移动操作。基于此数据集，我们提出了U0，这是一个面向通用水下机器人的VLA模型，该模型通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知焦点增强模块（CAP）来提高空间理解和移动操作能力。在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展数据集构建、提高任务自主性和实现智能通用水下机器人的实际应用奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决水下机器人领域缺乏大规模高质量数据集的问题，导致难以开发能够自主执行多任务的通用水下智能。这个问题很重要，因为水下环境对人类操作极具挑战性，而海洋覆盖地球表面71%，开发自主水下机器人能极大扩展人类探索海洋的能力；同时，水下任务目前仍严重依赖人工远程操作，成本高且效率低，而真实水下环境收集数据既昂贵又有风险。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到水下环境的独特挑战（流体动力学、能见度限制、通信约束）和现有水下数据集的局限性，因此决定采用模拟环境来收集大规模数据。他们借鉴了室内具身智能领域的进展，如DROID、Open X-Embodiment等数据集和RT-2、GR00T N1.5等VLA模型，同时参考了Stonefish等水下模拟器。基于这些现有工作，作者设计了USIM数据集和U0模型，专门针对水下环境的特性进行优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建大规模模拟水下数据集和专门设计的视觉-语言-动作模型来解决水下机器人数据稀缺问题。整体流程包括：1) 使用Stonefish模拟器构建9种多样化水下场景；2) 在这些场景中收集20个任务的561K帧数据，形成USIM数据集；3) 基于Isaac-GR00T N1.5构建U0模型，整合多模态传感器数据融合和卷积-注意力感知焦点增强模块(CAP)；4) 通过离线评估和在线测试验证模型效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) USIM数据集：首个大规模多任务水下VLA数据集，覆盖9个场景中的20个任务；2) U0模型：专为水下机器人设计的VLA模型，整合多模态传感器融合和CAP模块；3) 可扩展的数据到任务框架。相比之前工作，USIM解决了现有水下数据集任务单一、多样性不足的问题；U0则针对水下环境的独特挑战进行了优化，特别是在空间理解和移动操作方面表现出色；整体框架实现了80%的任务成功率，比基线方法提升显著。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过构建首个大规模水下多任务视觉-语言-动作数据集USIM并提出专门针对水下环境的U0模型，为开发具有多任务自主能力的通用水下机器人奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater environments present unique challenges for robotic operation,including complex hydrodynamics, limited visibility, and constrainedcommunication. Although data-driven approaches have advanced embodiedintelligence in terrestrial robots and enabled task-specific autonomousunderwater robots, developing underwater intelligence capable of autonomouslyperforming multiple tasks remains highly challenging, as large-scale,high-quality underwater datasets are still scarce. To address theselimitations, we introduce USIM, a simulation-based multi-taskVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over561K frames from 1,852 trajectories, totaling approximately 15.6 hours ofBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging fromvisual navigation to mobile manipulation. Building upon this dataset, wepropose U0, a VLA model for general underwater robots, which integratesbinocular vision and other sensor modalities through multimodal fusion, andfurther incorporates a convolution-attention-based perception focus enhancementmodule (CAP) to improve spatial understanding and mobile manipulation. Acrosstasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,the framework achieves a success rate of 80%, while in challenging mobilemanipulation tasks, it reduces the distance to the target by 21.2% comparedwith baseline methods, demonstrating its effectiveness. USIM and U0 show thatVLA models can be effectively applied to underwater robotic applications,providing a foundation for scalable dataset construction, improved taskautonomy, and the practical realization of intelligent general underwaterrobots.</description>
      <author>example@mail.com (Junwen Gu, Zhiheng wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu)</author>
      <guid isPermaLink="false">2510.07869v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs</title>
      <link>http://arxiv.org/abs/2510.08631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无监督的分布外(OOD)对象检测方法，通过深度神经网络特征空间中高斯混合模型参数的层次贝叶斯建模来提取认知不确定性，无需辅助数据或额外训练阶段即可实现显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;除了通过LiDAR点云的精确语义分割实现准确场景理解外，检测分布外(OOD)对象（即训练过程中未遇到的实例）对于防止将未知对象错误分配到已知类别至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有无监督OOD检测方法中认知不确定性和偶然不确定性混淆的问题，避免将分布中的模糊区域错误分类为OOD。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于深度神经网络特征空间中高斯混合模型参数层次贝叶斯建模的无监督OOD检测方法，专门提取认知不确定性而非依赖预测熵。&lt;h4&gt;主要发现&lt;/h4&gt;在SemanticKITTI数据集上，该方法优于现有基于不确定性的方法，AUROC提高18%，AUPRC增加22%，FPR95降低36%（从76%降至40%）。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法无需辅助数据或额外训练阶段，即可有效区分已知和未知对象，显著提升了无监督OOD检测的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;除了通过LiDAR点云的精确语义分割实现准确场景理解外，检测分布外(OOD)对象（即在训练过程中未遇到的实例）对于防止将未知对象错误分配到已知类别至关重要。虽然监督式OOD检测方法依赖于辅助的OOD数据集，但无监督方法避免了这一需求，通常仅依赖于预测熵，即通过对集成或多个后验权重样本的平均获得的预测分布的熵。然而，这些方法经常将认知（模型）不确定性和偶然（数据）不确定性混淆，将分布中的模糊区域错误地分类为OOD。为解决这一问题，我们提出了一种无监督OOD检测方法，该方法采用基于深度神经网络特征空间中高斯混合模型参数层次贝叶斯建模的认知不确定性。无需辅助数据或额外训练阶段，我们的方法在SemanticKITTI数据集上优于现有的基于不确定性的方法，与先前工作中使用的预测熵方法相比，AUROC提高了18%，AUPRC增加了22%，FPR95降低了36%（从76%降至40%）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR语义分割中检测分布外(Out-of-Distribution, OOD)对象的问题，即那些在训练过程中未遇到的实例。这个问题在自动驾驶等安全关键应用中非常重要，因为现实世界环境中经常包含训练数据中未见过的对象，而深度模型往往对这些OOD对象做出过度自信的错误预测。有效的OOD检测可以防止系统将未知对象错误分类为已知类别，提高自主系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：监督方法需要辅助OOD数据集，而非监督方法通常依赖预测熵，但这种方法会将认识不确定性(模型不确定性)和偶然不确定性(数据不确定性)混为一谈，导致误判。作者借鉴了GMMSeg使用高斯混合模型(GMM)在特征空间中建模语义类的思想，以及分层贝叶斯不确定性建模方法。通过结合这些现有工作的优势，作者设计了一种基于认识不确定性的无监督OOD检测方法，能够在不需要额外数据或训练的情况下有效区分OOD样本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用从GMM参数的分层贝叶斯建模中获得的认识不确定性来进行OOD检测，避免传统预测熵方法将认识不确定性和偶然不确定性混合的问题。整体流程包括：1)使用深度神经网络提取LiDAR点云特征；2)在特征空间中使用类条件高斯混合模型建模每个语义类；3)通过分层贝叶斯方法对GMM参数进行建模；4)从参数后验分布中采样并计算认识不确定性；5)使用熵值作为不确定性的度量；6)将高不确定性像素识别为OOD样本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于认识不确定性的无监督OOD检测方法；2)有效区分认识不确定性和偶然不确定性；3)不需要辅助数据或额外训练阶段；4)在SemanticKITTI数据集上实现了显著性能提升。相比之前的工作，与监督方法不同，它不需要辅助OOD数据；与传统预测熵方法不同，它能更好地区分两种不确定性；与MC Dropout和深度集合方法不同，它不需要重新训练网络；与原始GMMSeg相比，它增加了分层贝叶斯不确定性估计，提供了更强的检测能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于特征空间中高斯混合模型参数认识不确定性的无监督方法，有效解决了LiDAR语义分割中分布外对象检测问题，显著提高了检测准确性和系统安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In addition to accurate scene understanding through precise semanticsegmentation of LiDAR point clouds, detecting out-of-distribution (OOD)objects, instances not encountered during training, is essential to prevent theincorrect assignment of unknown objects to known classes. While supervised OODdetection methods depend on auxiliary OOD datasets, unsupervised methods avoidthis requirement but typically rely on predictive entropy, the entropy of thepredictive distribution obtained by averaging over an ensemble or multipleposterior weight samples. However, these methods often conflate epistemic(model) and aleatoric (data) uncertainties, misclassifying ambiguous indistribution regions as OOD. To address this issue, we present an unsupervisedOOD detection approach that employs epistemic uncertainty derived fromhierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters inthe feature space of a deep neural network. Without requiring auxiliary data oradditional training stages, our approach outperforms existing uncertainty-basedmethods on the SemanticKITTI dataset, achieving an 18\% improvement in AUROC,22\% increase in AUPRC, and 36\% reduction in FPR95 (from 76\% to 40\%),compared to the predictive entropy approach used in prior works.</description>
      <author>example@mail.com (Hanieh Shojaei Miandashti, Claus Brenner)</author>
      <guid isPermaLink="false">2510.08631v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes</title>
      <link>http://arxiv.org/abs/2510.08589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对比了传统CNN模型、零样本预训练的多模态大型语言模型(LLMs)和微调后的多模态LLMs在图像中人工文本叠加检测任务上的表现，发现LLMs在少量数据(少于1,000张图像)微调后可显著提升性能，达到36%的准确率提升，匹配或超过需要大量数据的CNN基线方法。&lt;h4&gt;背景&lt;/h4&gt;物体检测和理解领域受传统CNN模型(如ResNet和YOLO)和新兴多模态大型语言模型(LLMs)的共同推动发展。CNN模型在图像任务中仍然有效，而基于transformer的LLMs引入了动态上下文推理、语言引导提示和整体场景理解等新能力，但即用型LLMs在专业视觉任务中表现往往不佳。&lt;h4&gt;目的&lt;/h4&gt;通过对比微调的传统CNN、零样本预训练的多模态LLMs和微调的多模态LLMs，研究如何有效利用LLMs进行图像中人工文本叠加检测这一具有挑战性的任务，探索语言引导模型在最小监督下适应精确视觉理解的方法。&lt;h4&gt;方法&lt;/h4&gt;进行了全面的比较研究，探索了在少量数据(少于1,000张图像)上微调LLMs的方法，研究如何将语言引导模型适应精确视觉理解，并评估了这些方法在人工文本叠加检测任务上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs可以在非常有限的数据(少于1,000张图像)上进行有效微调，实现高达36%的准确率提升，匹配或超过基于CNN的基线方法，而这些方法通常需要数量级更多的数据。这表明基于LLM的方法在真实世界物体检测任务中具有高适应性和数据效率。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，通过少量数据微调，LLMs可以在专业视觉任务上取得优异表现，为在低资源视觉环境中应用多模态transformer提供了可行的指导。研究团队已将微调模型的代码公开在GitHub上，以支持该领域的持续进步。&lt;h4&gt;翻译&lt;/h4&gt;物体检测和理解领域正迅速发展，这既得益于传统基于CNN模型的进步，也得益于新兴的多模态大型语言模型(LLMs)的发展。虽然ResNet和YOLO等CNN模型在基于图像的任务中仍然非常有效，但最近的基于transformer的LLMs引入了动态上下文推理、语言引导提示和整体场景理解等新能力。然而，当即用型LLMs用于专业视觉任务时，其全部潜力仍未被充分开发，往往导致次优性能。在本工作中，我们在图像中人工文本叠加检测这一具有挑战性的任务上，对微调的传统CNN、零样本预训练的多模态LLMs和微调的多模态LLMs进行了全面比较。我们研究的一个关键贡献是证明了LLMs可以在非常有限的数据(少于1,000张图像)上进行有效微调，实现高达36%的准确率提升，匹配或超过通常需要数量级更多数据的基于CNN的基线方法。通过探索语言引导模型如何在最小监督下适应精确视觉理解，我们的研究为弥合视觉和语言的更广泛努力做出了贡献，为高效的跨模态学习策略提供了新的见解。这些研究结果突显了基于LLM的方法在真实世界物体检测任务中的适应性和数据效率，并提供了在低资源视觉环境中应用多模态transformer的可行指导。为了支持该领域的持续进步，我们已在GitHub上公开了用于微调模型的代码，以便未来改进和相关应用中的重用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of object detection and understanding is rapidly evolving, drivenby advances in both traditional CNN-based models and emerging multi-modal largelanguage models (LLMs). While CNNs like ResNet and YOLO remain highly effectivefor image-based tasks, recent transformer-based LLMs introduce new capabilitiessuch as dynamic context reasoning, language-guided prompts, and holistic sceneunderstanding. However, when used out-of-the-box, the full potential of LLMsremains underexploited, often resulting in suboptimal performance onspecialized visual tasks. In this work, we conduct a comprehensive comparisonof fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, andfine-tuned multi-modal LLMs on the challenging task of artificial text overlaydetection in images. A key contribution of our study is demonstrating that LLMscan be effectively fine-tuned on very limited data (fewer than 1,000 images) toachieve up to 36% accuracy improvement, matching or surpassing CNN-basedbaselines that typically require orders of magnitude more data. By exploringhow language-guided models can be adapted for precise visual understanding withminimal supervision, our work contributes to the broader effort of bridgingvision and language, offering novel insights into efficient cross-modallearning strategies. These findings highlight the adaptability and dataefficiency of LLM-based approaches for real-world object detection tasks andprovide actionable guidance for applying multi-modal transformers inlow-resource visual environments. To support continued progress in this area,we have made the code used to fine-tune the models available in our GitHub,enabling future improvements and reuse in related applications.</description>
      <author>example@mail.com (Nirmal Elamon, Rouzbeh Davoudi)</author>
      <guid isPermaLink="false">2510.08589v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer</title>
      <link>http://arxiv.org/abs/2510.03342v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了Gemini Robotics模型家族的最新版本，包括Gemini Robotics 1.5和Gemini Robotics-ER1.5，通过三项创新提高了机器人的通用推理和任务执行能力。&lt;h4&gt;背景&lt;/h4&gt;通用机器人需要深入理解物理世界、高级推理能力和通用灵巧的控制能力。&lt;h4&gt;目的&lt;/h4&gt;介绍Gemini Robotics模型家族的最新版本，提高机器人解决复杂多步骤任务的能力。&lt;h4&gt;方法&lt;/h4&gt;三项主要创新：1) 采用新颖架构和运动转移机制，从异构多形态机器人数据中学习；2) 在自然语言中将动作与多级内部推理过程交错进行，实现'行动前思考'；3) 建立新的具身推理最先进水平，提升视觉空间理解、任务规划和进度估计能力。&lt;h4&gt;主要发现&lt;/h4&gt;Gemini Robotics 1.5能够更好地分解和执行复杂多步骤任务，行为更具可解释性；Gemini Robotics-ER1.5在具身推理方面达到新水平。&lt;h4&gt;结论&lt;/h4&gt;这一系列模型使机器人能够感知、思考然后行动，朝着解决复杂多步骤任务的物理代理时代迈进。&lt;h4&gt;翻译&lt;/h4&gt;通用机器人需要深入理解物理世界、高级推理能力和通用灵巧的控制。本报告介绍了Gemini Robotics模型家族的最新一代：Gemini Robotics 1.5，一个多形态视觉-语言-动作模型，以及Gemini Robotics-ER1.5，一个最先进的具身推理模型。我们带来了三大创新。首先，Gemini Robotics 1.5采用新颖架构和运动转移机制，使其能够从异构的多形态机器人数据中学习，使VLA更加通用。其次，Gemini Robotics 1.5在自然语言中将动作与多级内部推理过程交错进行，使机器人能够在行动前'思考'，显著提高其分解和执行复杂多步骤任务的能力，并使机器人的行为对用户更具可解释性。第三，Gemini Robotics-ER 1.5为具身推理建立了新的最先进水平，即对机器人至关重要的推理能力，如视觉和空间理解、任务规划和进度估计。总之，这一系列模型使我们迈向物理代理的新时代，使机器人能够感知、思考然后行动，从而解决复杂的多步骤任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何开发通用的机器人系统，使机器人能够深入理解物理世界、进行高级推理并执行灵活控制。这个问题非常重要，因为当前机器人大多只能执行特定任务，缺乏适应不同环境和任务的能力，而通用机器人可以大大扩展应用范围，从工业制造到家庭服务等多个领域，解决劳动力短缺和提高生活质量等问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到通用机器人需要三个核心能力：物理世界理解、高级推理和灵活控制。他们设计了一个包含两个主要模型的系统：Gemini Robotics 1.5(VLA模型)和Gemini Robotics-ER 1.5(ER模型)。作者借鉴了现有的Gemini模型基础架构、VLA模型范式和具身推理概念，同时引入了新的Motion Transfer机制和Thinking机制，使机器人能够在执行动作前进行思考，并实现不同机器人形态间的技能迁移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将机器人系统分为高级推理规划模块和低级执行模块，引入'Thinking'机制使机器人在执行前进行思考，并使用Motion Transfer技术实现跨形态技能迁移。整体流程是：用户输入任务→高级推理模块理解需求、制定计划并调用外部工具→低级执行模块接收指令、分解步骤、执行前思考并转化为具体动作→两个模块协同工作，形成完整智能体系统处理复杂多步骤任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Thinking VLA机制，提高复杂任务处理能力；2)Motion Transfer机制，实现跨形态机器人技能零样本迁移；3)高级具身推理能力，在多种推理任务上达到最先进性能；4)多形态通用机器人系统，单一模型控制多种不同机器人。相比之前工作，1.5版本引入思考机制提升复杂任务执行能力，能够处理多种机器人形态，在多步骤任务上有显著提升，并在保持通用能力的同时在具身推理任务上达到最先进水平。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Gemini Robotics 1.5通过引入思考机制、跨形态技能迁移和高级具身推理能力，显著提升了通用机器人在复杂物理世界中的感知、思考和行动能力，为实现真正的通用机器人系统提供了重要进展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General-purpose robots need a deep understanding of the physical world,advanced reasoning, and general and dexterous control. This report introducesthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing togetherthree major innovations. First, Gemini Robotics 1.5 features a novelarchitecture and a Motion Transfer (MT) mechanism, which enables it to learnfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.Second, Gemini Robotics 1.5 interleaves actions with a multi-level internalreasoning process in natural language. This enables the robot to "think beforeacting" and notably improves its ability to decompose and execute complex,multi-step tasks, and also makes the robot's behavior more interpretable to theuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art forembodied reasoning, i.e., for reasoning capabilities that are critical forrobots, such as visual and spatial understanding, task planning, and progressestimation. Together, this family of models takes us a step towards an era ofphysical agents-enabling robots to perceive, think and then act so they cansolve complex multi-step tasks.</description>
      <author>example@mail.com (Gemini Robotics Team, Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Li Yang Ku, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Amaris Paryag, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Peter Pastor Sampedro, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, René Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou, Yuxiang Zhou)</author>
      <guid isPermaLink="false">2510.03342v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps</title>
      <link>http://arxiv.org/abs/2510.11107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025, project page:  https://jiahuilei.com/projects/momap/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的像素对齐运动图(MoMap)表示方法，用于从真实世界视频中学习语义和功能上有意义的3D运动先验，实现从单图像预测未来3D场景运动。&lt;h4&gt;背景&lt;/h4&gt;从真实世界视频中学习语义和功能上有意义的3D运动先验是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;能够从单个输入图像预测未来的3D场景运动。&lt;h4&gt;方法&lt;/h4&gt;提出像素对齐的运动图(MoMap)表示方法，从超过50,000个真实视频中创建大规模MoMap数据库，并训练扩散模型；运动生成流程包括先生成MoMap，然后扭曲图像并完成扭曲的点基渲染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法能够生成合理且语义一致的3D场景运动。&lt;h4&gt;结论&lt;/h4&gt;通过MoMap表示和扩散模型训练，可以有效地从真实视频中学习3D运动先验，并实现从单图像预测未来3D场景运动。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了从真实世界视频中学习语义和功能上有意义的3D运动先验的挑战，目的是能够从单个输入图像预测未来的3D场景运动。我们提出了一种新颖的像素对齐的运动图(MoMap)表示方法用于3D场景运动，可以从现有的生成图像模型生成，以促进高效和有效的运动预测。为了学习有意义的运动分布，我们从超过50,000个真实视频中创建了大规模的MoMap数据库，并基于这些表示训练了一个扩散模型。我们的运动生成不仅在3D中合成轨迹，还为2D视频合成提出了新流程：先生成一个MoMap，然后相应地扭曲图像，并完成扭曲的点基渲染。实验结果表明，我们的方法能够生成合理且语义一致的3D场景运动。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从真实世界视频中学习语义上和功能上有意义的3D运动先验知识，以便从单个输入图像预测未来3D场景运动。这个问题在计算机视觉领域非常重要，因为理解、重建和预测物体在3D空间中的运动对于增强现实、自动驾驶和机器人等与物理环境交互的应用至关重要。现有方法要么局限于2D视频生成，要么只能处理小规模的短轨迹，缺乏大规模学习3D生成运动先验的有效方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到需要一种适合神经网络处理的3D场景运动表示方式，避免2D轨迹中常见的遮挡问题。他们受到近期重用图像扩散模型进行其他任务（如深度预测）的启发，提出像素对齐的Motion Map表示。设计过程中借鉴了多个现有工作：利用4D重建技术（MoSca）处理真实视频，采用视频深度估计（DepthCrafter）和3D点跟踪（SpaTracker）技术，并应用视频对象分割（DEVA）获取语义信息。核心创新在于巧妙地将强大的预训练图像扩散模型（如Stable Diffusion）重用于3D运动预测任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D场景运动表示为像素对齐的Motion Map（MoMap），这种'轨迹图像'保留了图像结构但编码了3D位置信息，使能利用大型预训练图像扩散模型进行运动预测。整体流程分为四步：1)数据准备：从真实视频中提取深度、跟踪3D点、优化轨迹并生成MoMap；2)MoMap压缩：将高维运动数据编码为紧凑的潜在特征；3)MoMap扩散：修改预训练U-Net生成未来运动；4)应用：通过渲染和图像完成生成视频，或使用视觉语言模型进行精细控制。这种方法解耦了相机和物体运动，减少了问题复杂度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出MoMap这一像素对齐的3D运动表示；2)构建了从5万+真实视频提取的大规模MoMap数据库；3)重用预训练图像扩散模型进行3D运动预测；4)提出先生成MoMap再完成视频的新范式；5)引入DSL语言实现VLM对运动的精细控制。相比之前工作，不同之处在于：专注于长期密集的3D运动而非短轨迹；直接学习3D轨迹而非通过像素变化隐式学习运动；使用真实世界大规模数据而非合成数据；利用预训练模型而非从头训练；实现了相机与物体运动的解耦。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种名为MoMap的像素对齐3D运动表示方法，利用大型预训练图像扩散模型从真实世界视频中学习语义上有意义的3D运动先验，实现了从单张输入图像预测未来3D场景运动，并展示了其在视频生成等应用中的潜力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of learning semantically and functionallymeaningful 3D motion priors from real-world videos, in order to enableprediction of future 3D scene motion from a single input image. We propose anovel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,which can be generated from existing generative image models to facilitateefficient and effective motion prediction. To learn meaningful distributionsover motion, we create a large-scale database of MoMaps from over 50,000 realvideos and train a diffusion model on these representations. Our motiongeneration not only synthesizes trajectories in 3D but also suggests a newpipeline for 2D video synthesis: first generate a MoMap, then warp an imageaccordingly and complete the warped point-based renderings. Experimentalresults demonstrate that our approach generates plausible and semanticallyconsistent 3D scene motion.</description>
      <author>example@mail.com (Jiahui Lei, Kyle Genova, George Kopanas, Noah Snavely, Leonidas Guibas)</author>
      <guid isPermaLink="false">2510.11107v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Seeing My Future: Predicting Situated Interaction Behavior in Virtual Reality</title>
      <link>http://arxiv.org/abs/2510.10742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://xy02-05.github.io/Seeing_My_Future/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种分层的、意图感知框架，用于在虚拟和增强现实系统中预测人类行为，通过理解人类意图和利用认知机制，实现了对未来情境行为的准确预测，并在实验中取得了优越性能。&lt;h4&gt;背景&lt;/h4&gt;虚拟和增强现实系统需要智能适应用户行为以增强交互体验。准确理解人类意图并预测未来的情境行为（如视线方向和物体交互）对创建响应式VR/AR环境和应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够对人类意图建模并预测详细情境行为的框架，以实现VR/AR系统对用户行为的智能适应，从而创建更响应式的交互环境。&lt;h4&gt;方法&lt;/h4&gt;引入了一个分层的、意图感知框架，利用认知机制对人类意图建模并预测情境行为。该框架基于历史人类动态和场景上下文观察，识别潜在交互目标并预测细粒度未来行为。采用动态图卷积网络(GCN)来捕捉人-环境关系。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的真实世界基准测试和实时VR环境上的实验表明，该方法在所有指标上都取得了优越性能，能够实现主动VR系统的实际应用，这些系统能够预测用户行为并相应调整虚拟环境。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架有效解决了VR/AR系统中智能适应的关键挑战，通过理解人类意图和预测未来行为，使系统能够主动调整以提供更好的用户体验，为未来VR/AR应用的发展提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;虚拟和增强现实系统日益需要智能适应用户行为以增强交互体验。实现这一点需要准确理解人类意图并预测未来的情境行为——如视线方向和物体交互——这对于创建响应式的VR/AR环境和个性化助手等应用至关重要。然而，准确的行为预测需要对驱动人-环境交互的潜在认知过程进行建模。在本工作中，我们引入了一个分层的、意图感知框架，通过利用认知机制对人类意图建模并预测详细的情境行为。给定历史人类动态和场景上下文观察，我们的框架首先识别潜在的交互目标并预测细粒度的未来行为。我们提出了一种动态图卷积网络(GCN)来有效捕捉人-环境关系。在具有挑战性的真实世界基准测试和实时VR环境上的大量实验证明了我们方法的有效性，在所有指标上均取得了优越性能，并实现了主动VR系统的实际应用，这些系统能够预测用户行为并相应地调整虚拟环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在虚拟现实(VR)环境中准确预测用户的情境化交互行为问题，包括用户视线方向、移动轨迹和物体交互。这个问题很重要，因为VR/AR系统需要智能适应用户行为以增强交互体验，准确预测用户行为能创建响应式环境，支持个性化助手、游戏环境调整和人机协作等应用，使虚拟环境能主动适应而非被动响应人类行为。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从认知科学研究中获取灵感，注意到人类通常会先形成交互意图再执行具体动作，视线在交互意图形成中起关键作用。方法设计借鉴了现有工作：1)视线-身体相关性研究，利用视线信息提高预测准确性；2)视线预测技术，同时预测视线、轨迹和物体交互；3)物体交互预测方法，但创新性地采用符合人类认知的分层框架，先预测潜在目标再预测详细行为。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个分层、意图感知框架，模拟人类认知过程，先预测潜在交互目标再预测详细行为，使用动态图卷积网络捕获人-环境关系。整体流程：1)观察编码模块：将历史人类状态和场景上下文编码为时空特征；2)分层意图感知解码模块：先预测潜在交互目标的交互概率，再解码人类和物体的下一个状态；3)动态GCN模块：使用自适应权重矩阵建模视线、人体特征与物体间关系；4)多任务训练：使用多个损失函数监督所有预测输出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)分层意图感知框架，首次模拟人类认知过程预测交互行为；2)动态GCN设计，通过自适应权重矩阵有效捕获人-环境关系；3)多任务预测，同时预测视线、轨迹和物体交互。相比之前工作的不同：现有方法如Pose2Gaze缺乏环境上下文，SIF3D等虽利用环境信息但缺乏人-环境关系建模，本文方法在所有指标上表现优越，特别是在识别下一个活跃物体方面有显著优势，且在真实VR环境中验证了实用性和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种受认知科学启发的分层意图感知框架，通过动态图卷积网络建模人-环境关系，实现了在VR环境中对用户视线、移动轨迹和物体交互的准确预测，为主动式VR系统提供了新的技术基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Virtual and augmented reality systems increasingly demand intelligentadaptation to user behaviors for enhanced interaction experiences. Achievingthis requires accurately understanding human intentions and predicting futuresituated behaviors - such as gaze direction and object interactions - which isvital for creating responsive VR/AR environments and applications likepersonalized assistants. However, accurate behavioral prediction demandsmodeling the underlying cognitive processes that drive human-environmentinteractions. In this work, we introduce a hierarchical, intention-awareframework that models human intentions and predicts detailed situated behaviorsby leveraging cognitive mechanisms. Given historical human dynamics and theobservation of scene contexts, our framework first identifies potentialinteraction targets and forecasts fine-grained future behaviors. We propose adynamic Graph Convolutional Network (GCN) to effectively capturehuman-environment relationships. Extensive experiments on challengingreal-world benchmarks and live VR environment demonstrate the effectiveness ofour approach, achieving superior performance across all metrics and enablingpractical applications for proactive VR systems that anticipate user behaviorsand adapt virtual environments accordingly.</description>
      <author>example@mail.com (Yuan Xu, Zimu Zhang, Xiaoxuan Ma, Wentao Zhu, Yu Qiao, Yizhou Wang)</author>
      <guid isPermaLink="false">2510.10742v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning</title>
      <link>http://arxiv.org/abs/2510.10451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度强化学习和反事实模拟的数据驱动多动物行为模拟器，解决了在生物学中实现真实多动物模拟的关键挑战，能够高保真地复现物种特异性行为，支持反事实行为预测和多个体建模。&lt;h4&gt;背景&lt;/h4&gt;动物运动模拟器在行为研究中扮演重要角色。模仿学习在机器人学中的进步为复制人类和动物运动提供了新的可能性。然而，在生物学中实现真实的多动物模拟面临关键挑战：弥合未知现实世界转换模型与其模拟对应物之间的差距。由于运动动态很少是已知的，仅依靠数学模型是不够的。&lt;h4&gt;目的&lt;/h4&gt;构建一个既能复现真实轨迹又支持奖励驱动优化的模拟器，解决高自由度运动引起的病态问题，实现物种特异性行为的准确复现，支持反事实行为预测和多个体建模。&lt;h4&gt;方法&lt;/h4&gt;研究团队基于深度强化学习和反事实模拟构建了数据驱动模拟器。他们通过在强化学习框架中将不完整转换模型的运动变量估计为动作，来解决运动中高自由度引起的病态问题。此外，他们使用基于距离的伪奖励来对齐和比较赛博空间与物理空间之间的状态。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在人工代理、苍蝇、蝾螈和家蚕上得到了验证，与标准模仿和强化学习方法相比，实现了更高的物种特异性行为可再现性和改进的奖励获取。此外，它支持在新实验环境中的反事实行为预测，并支持多个个体的建模以实现灵活的假设轨迹生成。&lt;h4&gt;结论&lt;/h4&gt;该数据驱动的多动物行为模拟器能够有效模拟和阐明复杂的多动物行为，为生物学研究提供了强大的工具，具有广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;动物运动模拟器在行为研究中扮演着重要角色。模仿学习在机器人学中的进步为复制人类和动物运动提供了新的可能性。在生物学中实现真实多动物模拟的一个关键挑战是弥合未知现实世界转换模型与其模拟对应物之间的差距。由于运动动态很少是已知的，仅依靠数学模型是不够的；构建一个既能复现真实轨迹又支持奖励驱动优化的模拟器仍然是一个开放问题。我们介绍了一种基于深度强化学习和反事实模拟的多动物行为数据驱动模拟器。我们通过在强化学习框架中将不完整转换模型的运动变量估计为动作，解决了运动中高自由度引起的病态问题。我们还使用基于距离的伪奖励来对齐和比较赛博空间与物理空间之间的状态。在人工代理、苍蝇、蝾螈和家蚕上的验证表明，与标准模仿和强化学习方法相比，我们的方法实现了更高的物种特异性行为可再现性和改进的奖励获取。此外，它支持在新实验环境中的反事实行为预测，并支持多个个体的建模以实现灵活的假设轨迹生成，表明其模拟和阐明复杂多动物行为的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulators of animal movements play a valuable role in studying behavior.Advances in imitation learning for robotics have expanded possibilities forreproducing human and animal movements. A key challenge for realisticmulti-animal simulation in biology is bridging the gap between unknownreal-world transition models and their simulated counterparts. Becauselocomotion dynamics are seldom known, relying solely on mathematical models isinsufficient; constructing a simulator that both reproduces real trajectoriesand supports reward-driven optimization remains an open problem. We introduce adata-driven simulator for multi-animal behavior based on deep reinforcementlearning and counterfactual simulation. We address the ill-posed nature of theproblem caused by high degrees of freedom in locomotion by estimating movementvariables of an incomplete transition model as actions within an RL framework.We also employ a distance-based pseudo-reward to align and compare statesbetween cyber and physical spaces. Validated on artificial agents, flies,newts, and silkmoth, our approach achieves higher reproducibility ofspecies-specific behaviors and improved reward acquisition compared withstandard imitation and RL methods. Moreover, it enables counterfactual behaviorprediction in novel experimental settings and supports multi-individualmodeling for flexible what-if trajectory generation, suggesting its potentialto simulate and elucidate complex multi-animal behaviors.</description>
      <author>example@mail.com (Keisuke Fujii, Kazushi Tsutsui, Yu Teshima, Makoto Itoh, Naoya Takeishi, Nozomi Nishiumi, Ryoya Tanaka, Shunsuke Shigaki, Yoshinobu Kawahara)</author>
      <guid isPermaLink="false">2510.10451v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?</title>
      <link>http://arxiv.org/abs/2510.10254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了大型视觉模型（LVM）在医学影像任务中的零样本学习能力，发现即使没有医学数据训练，该模型也能在器官分割、去噪、超分辨率和运动预测等任务上实现具有竞争力的性能，特别是在放射治疗运动预测中表现优异。&lt;h4&gt;背景&lt;/h4&gt;最近大型生成模型的发展表明，适当扩展的自回归公式可以在不同领域表现出强大的零样本泛化能力。这一趋势启发研究者探索自回归视频建模原则在医学影像领域的应用潜力。&lt;h4&gt;目的&lt;/h4&gt;研究旨在验证自回归视频建模原则是否可以直接应用于医学影像任务，评估大型视觉模型在从未接触过医学数据的情况下，在多种医学影像任务上的零样本性能。&lt;h4&gt;方法&lt;/h4&gt;研究评估了一个大型视觉模型（LVM）在四个代表性医学影像任务上的零样本性能：器官分割、去噪、超分辨率和运动预测。研究使用了来自122名患者的4D CT数据，总计超过1,820个3D CT体积进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;1. 即使没有领域特定的微调，LVM也能在CT扫描中勾勒出解剖结构；2. LVM在分割、去噪和超分辨率任务上实现了具有竞争力的性能；3. 在放射治疗运动预测中，LVM能够直接从前4D CT扫描的前期阶段预测未来的3D CT阶段；4. 预测结果解剖上一致，能够以真实的时间连贯性捕捉患者特定的呼吸动力学；5. LVM在运动预测任务上超越了专门的DVF-based和生成式基线，达到了最先进的空间精度。&lt;h4&gt;结论&lt;/h4&gt;这些发现揭示了医学视频建模中零样本能力的出现，突显了通用视频模型作为统一学习者和推理者的潜力，为建立在视频模型上的未来医学基础模型奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;最近大型生成模型的进展表明，适当地扩展简单的自回归公式可以在不同领域表现出强大的零样本泛化能力。受这一趋势启发，我们研究是否可以将自回归视频建模原则直接应用于医学影像任务，尽管该模型从未在医学数据上训练过。具体来说，我们在四个代表性任务上评估了一个大型视觉模型（LVM）的零样本性能：器官分割、去噪、超分辨率和运动预测。值得注意的是，即使没有领域特定的微调，LVM也能在CT扫描中勾勒出解剖结构，并在分割、去噪和超分辨率任务上实现具有竞争力的性能。最显著的是，在放射治疗运动预测中，该模型直接从前4D CT扫描的前期阶段预测未来的3D CT阶段，产生解剖上一致的预测，能够以真实的时间连贯性捕捉患者特定的呼吸动力学。我们在122名患者的4D CT数据上评估了LVM，总计超过1,820个3D CT体积。尽管没有预先接触医学数据，该模型在所有任务上都实现了强大的性能，并在运动预测方面超越了专门的DVF-based和生成式基线，达到了最先进的空间精度。这些发现揭示了医学视频建模中零样本能力的出现，并突显了通用视频模型作为统一学习者和推理者的潜力，为建立在视频模型上的未来医学基础模型奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文探讨大型视频模型能否在未经医疗数据训练的情况下，直接应用于医疗影像任务并表现出色。这个问题很重要，因为传统医疗AI系统需要针对特定任务（如分割、去噪）专门训练，成本高昂且系统碎片化。如果通用视频模型能零样本应用于医疗领域，将大大降低医疗AI开发成本，提高系统通用性和适应性，使医疗AI更可扩展和实用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受大型语言模型展现的跨领域零样本泛化能力启发，思考自回归视频建模原则是否可直接应用于医疗影像。他们选择了大型视觉模型（LVM）作为基础，该模型在大规模自然图像和视频上训练，能通过提示适应不同视觉任务。作者借鉴了LLMs和VLMs的统一框架思想、自回归视频建模方法（如VQGAN和Transformer架构）、医疗分割框架（如nnUNet）和变形矢量场方法，创新性地将这些技术应用于医疗领域，特别是放疗运动预测任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是大型视频模型通过自回归学习时空表示，即使未经医疗数据训练，也能通过零样本学习在医疗影像任务上取得有竞争力表现。整体流程：1) 预处理：用nnUNet分割4D CT序列获取器官掩码；2) CT分词化：用VQGAN将CT图像编码为离散令牌序列；3) 序列建模：使用单向Transformer对CT阶段序列进行自回归建模，预测未来阶段；4) 评估：在分割、去噪、超分辨率和运动预测任务上评估性能，使用IoU、DSC等指标和定性可视化验证结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 首次证明大型视频模型可在无医疗训练数据下直接应用于多种医疗任务；2) 展示单一模型可处理分割、去噪、超分辨率和运动预测等多种任务，无需任务特定重训练；3) 在放疗运动预测上超越专门方法，能准确预测未来CT阶段；4) 整合CT图像和分割掩码提高运动建模准确性。相比之前工作：传统医疗AI需针对每任务设计专门模型，而本文使用通用模型处理多任务；现有医疗基础模型多局限于静态影像，本文专注于动态医疗数据；传统方法依赖预计算的DVF，本文直接从图像序列学习运动模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次证明大型视频模型可以在未经医疗数据训练的情况下，通过零样本学习直接应用于多种医疗影像任务，特别是在放疗运动预测上超越了专门方法，为构建统一的医疗影像基础模型铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large generative models have shown that simpleautoregressive formulations, when scaled appropriately, can exhibit strongzero-shot generalization across domains. Motivated by this trend, weinvestigate whether autoregressive video modeling principles can be directlyapplied to medical imaging tasks, despite the model never being trained onmedical data. Specifically, we evaluate a large vision model (LVM) in azero-shot setting across four representative tasks: organ segmentation,denoising, super-resolution, and motion prediction. Remarkably, even withoutdomain-specific fine-tuning, the LVM can delineate anatomical structures in CTscans and achieve competitive performance on segmentation, denoising, andsuper-resolution. Most notably, in radiotherapy motion prediction, the modelforecasts future 3D CT phases directly from prior phases of a 4D CT scan,producing anatomically consistent predictions that capture patient-specificrespiratory dynamics with realistic temporal coherence. We evaluate the LVM on4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite noprior exposure to medical data, the model achieves strong performance acrossall tasks and surpasses specialized DVF-based and generative baselines inmotion prediction, achieving state-of-the-art spatial accuracy. These findingsreveal the emergence of zero-shot capabilities in medical video modeling andhighlight the potential of general-purpose video models to serve as unifiedlearners and reasoners laying the groundwork for future medical foundationmodels built on video models.</description>
      <author>example@mail.com (Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2510.10254v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>ExpVid: A Benchmark for Experiment Video Understanding &amp; Reasoning</title>
      <link>http://arxiv.org/abs/2510.11606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Data &amp; Code: https://github.com/OpenGVLab/ExpVid&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了ExpVid基准测试，用于系统评估多模态大语言模型在科学实验视频上的能力，发现现有模型在细粒度感知、状态变化跟踪和科学推理方面存在明显不足。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型有望加速科学发现，但现有基准测试忽视了真实实验室工作的细粒度和长期特性，特别是在湿实验室环境中，导致对其真实能力的理解不足。&lt;h4&gt;目的&lt;/h4&gt;弥补这一差距，引入ExpVid基准测试，系统性评估MLLMs在科学实验视频上的能力。&lt;h4&gt;方法&lt;/h4&gt;ExpVid从同行评审的视频出版物中策划，包含三级任务层次结构：细粒度感知、程序理解和科学推理。采用视觉为中心的注释流程，结合自动生成和多学科专家验证，确保任务需要视觉基础。&lt;h4&gt;主要发现&lt;/h4&gt;评估19个领先MLLMs后发现，它们擅长粗粒度识别，但在区分细粒度细节、跟踪状态变化和将实验程序与科学结果联系方面存在困难。专有模型和开源模型间存在明显性能差距，特别是在高阶推理方面。&lt;h4&gt;结论&lt;/h4&gt;ExpVid不仅提供了诊断工具，还为开发能够成为科学实验可信伙伴的MLLMs绘制了路线图。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型（MLLMs）有望通过解释复杂的实验流程来加速科学发现。然而，它们真实的能力未被充分理解，因为现有的基准测试忽视了真实实验室工作的细粒度和长期特性，特别是在湿实验室环境中。为了弥补这一差距，我们引入了ExpVid，这是第一个旨在系统性评估MLLMs在科学实验视频上的基准测试。从同行评审的视频出版物中策划，ExpVid具有一个新的三级任务层次结构，反映科学过程：（1）对工具、材料和动作的细粒度感知；（2）对步骤顺序和完整性的程序理解；（3）将整个实验与其已发表结论联系起来的科学推理。我们的视觉为中心的注释流程，结合自动生成和多学科专家验证，确保任务需要视觉基础。我们在ExpVid上评估了19个领先的MLLMs，发现虽然它们擅长粗粒度识别，但在区分细粒度细节、跟踪随时间变化的状态以及将实验程序与科学结果联系起来方面存在困难。我们的结果揭示了专有模型和开源模型之间的显著性能差距，特别是在高阶推理方面。ExpVid不仅提供了诊断工具，还为开发能够成为科学实验可信伙伴的MLLMs绘制了路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) hold promise for acceleratingscientific discovery by interpreting complex experimental procedures. However,their true capabilities are poorly understood, as existing benchmarks neglectthe fine-grained and long-horizon nature of authentic laboratory work,especially in wet-lab settings. To bridge this gap, we introduce ExpVid, thefirst benchmark designed to systematically evaluate MLLMs on scientificexperiment videos. Curated from peer-reviewed video publications, ExpVidfeatures a new three-level task hierarchy that mirrors the scientific process:(1) Fine-grained Perception of tools, materials, and actions; (2) ProceduralUnderstanding of step order and completeness; and (3) Scientific Reasoning thatconnects the full experiment to its published conclusions. Our vision-centricannotation pipeline, combining automated generation with multi-disciplinaryexpert validation, ensures that tasks require visual grounding. We evaluate 19leading MLLMs on ExpVid and find that while they excel at coarse-grainedrecognition, they struggle with disambiguating fine details, tracking statechanges over time, and linking experimental procedures to scientific outcomes.Our results reveal a notable performance gap between proprietary andopen-source models, particularly in high-order reasoning. ExpVid not onlyprovides a diagnostic tool but also charts a roadmap for developing MLLMscapable of becoming trustworthy partners in scientific experimentation.</description>
      <author>example@mail.com (Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang)</author>
      <guid isPermaLink="false">2510.11606v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?</title>
      <link>http://arxiv.org/abs/2510.11549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ODI-Bench基准测试和Omni-CoT方法，用于提升多模态大语言模型对全景图像的理解能力。&lt;h4&gt;背景&lt;/h4&gt;全景图像提供360度全方位视角，广泛应用于VR、AR和具身智能，但多模态大语言模型对这类沉浸式环境的理解能力尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;填补多模态大语言模型在全景图像理解方面的研究空白，提供专门的基准测试和改进方法。&lt;h4&gt;方法&lt;/h4&gt;创建包含2000张全景图像和4000多个问答对的ODI-Bench基准，测试20个代表性MLLMs；提出Omni-CoT方法，通过思维链推理增强模型理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;当前MLLMs难以捕捉全景图像提供的沉浸式上下文信息。&lt;h4&gt;结论&lt;/h4&gt;研究将发布ODI-Bench基准测试和Omni-CoT代码，促进全景图像理解领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;全景图像(ODIs)提供360x180度的全方位视角，广泛应用于VR、AR和具身智能应用。虽然多模态大语言模型(MLLMs)在传统2D图像和视频理解基准测试上表现出色，但它们对ODIs捕捉的沉浸式环境的理解能力仍 largely未被探索。为解决这一差距，我们首先提出了ODI-Bench，这是一个专为全景图像理解设计的新型综合基准测试。ODI-Bench包含2000张高质量全景图像和4000多个人工标注的问答对，涵盖10个细粒度任务，包括一般层面和空间层面的全景图像理解。我们在封闭式和开放式两种设置下对20个代表性MLLMs进行了广泛测试，包括专有和开源模型。实验结果表明，当前MLLMs仍然难以捕捉全景图像提供的沉浸式上下文。为此，我们进一步引入了Omni-CoT，这是一种无需训练的方法，通过在文本信息和视觉线索之间进行思维链推理，显著增强MLLMs在全景环境中的理解能力。基准测试和代码将在发表后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决评估多模态大语言模型(MLLMs)对全方向图像(ODI)的理解能力问题。ODIs提供360°全景视野，广泛应用于VR、AR和具身智能等领域，但MLLMs对这类沉浸式环境的理解能力尚未被充分探索。这个问题很重要，因为ODIs包含比传统2D图像更丰富的空间信息，需要更高级的空间推理能力，对推进具身智能和交互式多模态系统发展至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有ODI基准的局限性(分辨率低、场景多样性有限、问题领域受限、视角限制)来设计方法。他们结合了自动化管道和人工标注两种QA构建方式，借鉴了传统2D图像理解任务设计了5个通用级任务，同时参考空间理解任务设计了5个空间级任务。作者还借鉴了链式思维(Chain-of-Thought)推理方法，设计了Omni-CoT框架来增强模型性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全面基准(ODI-Bench)评估MLLMs对全方向图像的理解，并通过Omni-CoT框架提升其理解能力。ODI-Bench包含2000张高质量ODIs和4000+问答对，涵盖10个细粒度任务。Omni-CoT框架包含三步：1)视角引导回答(将ODI投影为六个视角并生成描述)；2)裁剪线索的定位和细化(识别相关图像区域并过滤)；3)回答细化(结合视觉线索重新思考答案)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的ODI-Bench基准，同时评估通用级和空间级理解；2)高分辨率(&gt;8K)和多样化场景(室内+室外)的图像集；3)细粒度任务设计(10个任务涵盖属性识别、计数、方向判断等)；4)Omni-CoT训练增强框架显著提升模型性能。相比之前工作，ODI-Bench分辨率更高、场景更多样、任务更全面、标注质量更高，且采用封闭式和开放式双格式评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ODI-Bench全面基准和Omni-CoT增强框架，显著提升了多模态大语言模型对沉浸式全方向环境的理解能力，为评估和改进此类模型提供了新标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omnidirectional images (ODIs) provide full 360x180 view which are widelyadopted in VR, AR and embodied intelligence applications. While multi-modallarge language models (MLLMs) have demonstrated remarkable performance onconventional 2D image and video understanding benchmarks, their ability tocomprehend the immersive environments captured by ODIs remains largelyunexplored. To address this gap, we first present ODI-Bench, a novelcomprehensive benchmark specifically designed for omnidirectional imageunderstanding. ODI-Bench contains 2,000 high-quality omnidirectional images andover 4,000 manually annotated question-answering (QA) pairs across 10fine-grained tasks, covering both general-level and spatial-level ODIunderstanding. Extensive experiments are conducted to benchmark 20representative MLLMs, including proprietary and open-source models, under bothclose-ended and open-ended settings. Experimental results reveal that currentMLLMs still struggle to capture the immersive context provided by ODIs. To thisend, we further introduce Omni-CoT, a training-free method which significantlyenhances MLLMs' comprehension ability in the omnidirectional environmentthrough chain-of-thought reasoning across both textual information and visualcues. Both the benchmark and the code will be released upon the publication.</description>
      <author>example@mail.com (Liu Yang, Huiyu Duan, Ran Tao, Juntao Cheng, Sijing Wu, Yunhao Li, Jing Liu, Xiongkuo Min, Guangtao Zhai)</author>
      <guid isPermaLink="false">2510.11549v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory</title>
      <link>http://arxiv.org/abs/2510.11129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了video-SALMONN S，一种流式视听LLM，能够在固定内存预算下以1 FPS和360p分辨率处理3小时长视频，通过测试时训练内存模块和提示依赖内存读取器实现高效处理，并在多个长视频基准测试上超越离线和流式基线。&lt;h4&gt;背景&lt;/h4&gt;连续、高帧率、高分辨率处理长视频流对未来AI代理至关重要，但当前视频理解LLM难以扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理长时间视频流的模型，解决现有方法中离线方法需要适应帧率、流式方法因合并或丢弃令牌导致信息丢失的问题。&lt;h4&gt;方法&lt;/h4&gt;提出video-SALMONN S，包含(i)测试时训练(TTT)内存模块，持续更新令牌表示以捕获长程依赖；(ii)提示依赖内存读取器，从固定大小内存中选择性检索上下文相关内容；使用无Hessian共轭梯度过程(TTT_HF)优化TTT模块。&lt;h4&gt;主要发现&lt;/h4&gt;在长视频基准测试(Video-MME, LVBench, VideoEvalPro)上，video-SALMONN S能够在包含10k帧和1M令牌的多小时视频上保持高质量理解。&lt;h4&gt;结论&lt;/h4&gt;80亿参数的video-SALMONN S模型在Video-MME长分集上达到74.2%总体得分和67.8%的得分，优于离线和流式基线，证明了处理长视频流的有效性。&lt;h4&gt;翻译&lt;/h4&gt;连续、高帧率、高分辨率处理长视频流对未来AI代理至关重要，但当前视频理解LLM难以扩展。离线时，固定帧数方法需要流长度适应帧率；流式方法通过合并或丢弃令牌来限制内存，导致信息丢失。我们提出了video-SALMONN S，一种流式视听LLM，据我们所知是首个在固定内存预算下以1 FPS和360p分辨率处理3小时视频的模型。我们的模型引入了(i)测试时训练(TTT)内存模块，通过替换令牌合并持续更新令牌表示以捕获长程依赖；(ii)提示依赖内存读取器，从固定大小内存中选择性检索上下文相关内容。TTT模块使用无Hessian共轭梯度过程(TTT_HF)进行优化，实现高效适应。在长视频基准测试(Video-MME, LVBench, VideoEvalPro)上，video-SALMONN S在包含10k帧和1M令牌的多小时视频上保持高质量理解。我们的80亿参数模型在Video-MME长分集上达到74.2%总体得分和67.8%的得分，优于离线和流式基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuous, high-frame-rate, high-resolution processing of long video streamsis critical for future AI agents, yet current video-understanding LLMs struggleto scale. Offline, fixed-frame-number methods require the stream length toadapt frame rates; streaming methods constrain memory by merging or discardingtokens, losing information. We propose video-SALMONN S, a streamingaudio-visual LLM that, to our knowledge, is the first to process 3-hour videosat 1 FPS and 360p resolution under a fixed memory budget. Our model introduces(i) a test-time-training (TTT) memory module that continually updates tokenrepresentations to capture long-range dependencies by replacing token merging,and (ii) a prompt-dependent memory reader that selectively retrievescontext-relevant content from fixed-size memory. The TTT module is optimisedwith a Hessian-free conjugate-gradient procedure (TTT_HF) for efficientadaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),video-SALMONN S sustains high-quality understanding on multi-hour videos with10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and67.8% on the Video-MME long split, outperforming both offline and streamingbaselines.</description>
      <author>example@mail.com (Guangzhi Sun, Yixuan Li, Xiaodong Wu, Yudong Yang, Wei Li, Zejun Ma, Chao Zhang)</author>
      <guid isPermaLink="false">2510.11129v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Robust Photoplethysmography Signal Denoising via Mamba Networks</title>
      <link>http://arxiv.org/abs/2510.11058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度学习的PPG去噪框架，通过DPNet网络和创新的损失函数设计，有效解决了PPG信号中的噪声问题，同时保留了重要的生理信息。&lt;h4&gt;背景&lt;/h4&gt;光电容积描记法(PPG)被广泛应用于可穿戴健康监测，但其可靠性常因噪声和运动伪影而降低，限制了心率(HR)估计等下游应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种深度学习框架用于PPG去噪，重点是保留生理信息。&lt;h4&gt;方法&lt;/h4&gt;提出DPNet，一种基于Mamba的去噪主干网络，专为有效的时间建模而设计；采用尺度不变信号失真比(SI-SDR)损失函数提高波形保真度；引入辅助心率预测器(HRP)通过基于心率的监督提供生理一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在BIDMC数据集上的实验表明，该方法对合成噪声和真实世界运动伪影都具有很强的鲁棒性，优于传统滤波和现有神经模型；能有效恢复PPG信号同时保持心率准确性；证明了SI-SDR损失和心率引导监督的互补作用。&lt;h4&gt;结论&lt;/h4&gt;该方法在可穿戴健康系统实际部署中具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;光电容积描记法(PPG)被广泛应用于可穿戴健康监测，但其可靠性常因噪声和运动伪影而降低，限制了心率(HR)估计等下游应用。本文提出了一种深度学习框架用于PPG去噪，重点是保留生理信息。在该框架中，我们提出了DPNet，一种基于Mamba的去噪主干网络，专为有效的时间建模而设计。为进一步增强去噪性能，该框架还采用了尺度不变信号失真比(SI-SDR)损失函数来提高波形保真度，以及一个辅助心率预测器(HRP)，通过基于心率的监督提供生理一致性。在BIDMC数据集上的实验表明，我们的方法对合成噪声和真实世界运动伪影都具有很强的鲁棒性，优于传统滤波和现有神经模型。我们的方法能有效恢复PPG信号同时保持心率准确性，突显了SI-SDR损失和心率引导监督的互补作用。这些结果证明了我们的方法在可穿戴健康系统实际部署中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmography (PPG) is widely used in wearable health monitoring, butits reliability is often degraded by noise and motion artifacts, limitingdownstream applications such as heart rate (HR) estimation. This paper presentsa deep learning framework for PPG denoising with an emphasis on preservingphysiological information. In this framework, we propose DPNet, a Mamba-baseddenoising backbone designed for effective temporal modeling. To further enhancedenoising performance, the framework also incorporates a scale-invariantsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and anauxiliary HR predictor (HRP) that provides physiological consistency throughHR-based supervision. Experiments on the BIDMC dataset show that our methodachieves strong robustness against both synthetic noise and real-world motionartifacts, outperforming conventional filtering and existing neural models. Ourmethod can effectively restore PPG signals while maintaining HR accuracy,highlighting the complementary roles of SI-SDR loss and HR-guided supervision.These results demonstrate the potential of our approach for practicaldeployment in wearable healthcare systems.</description>
      <author>example@mail.com (I Chiu, Yu-Tung Liu, Kuan-Chen Wang, Hung-Yu Wei, Yu Tsao)</author>
      <guid isPermaLink="false">2510.11058v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Mixup Helps Understanding Multimodal Video Better</title>
      <link>http://arxiv.org/abs/2510.10986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的多模态视频理解方法，通过动态调整模态混合比例来解决强模态过拟合问题，提高模型的泛化能力和多模态鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;多模态视频理解在动作识别和情感分类等任务中至关重要，通过结合不同模态的信息。然而，多模态模型容易对强模态过拟合，导致强模态主导学习并抑制弱模态的贡献。&lt;h4&gt;目的&lt;/h4&gt;解决多模态模型中强模态过拟合问题，提高模型的泛化能力和多模态鲁棒性，同时考虑模态不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;首先提出Multimodal Mixup (MM)，在聚合的多模态特征级别应用Mixup策略生成虚拟特征-标签对以减轻过拟合；然后进一步提出Balanced Multimodal Mixup (B-MM)，根据各模态对学习目标的相对贡献动态调整每个模态的混合比例。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的广泛实验表明，所提出的方法能有效提高模型的泛化能力和多模态鲁棒性，解决了强模态过拟合和模态不平衡问题。&lt;h4&gt;结论&lt;/h4&gt;通过动态调整模态混合比例，B-MM方法能够有效平衡不同模态的贡献，减轻过拟合，提高多模态视频理解模型的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;多模态视频理解通过结合不同模态的信息，在动作识别和情感分类等任务中发挥着关键作用。然而，多模态模型容易对强模态过拟合，这些强模态会主导学习并抑制弱模态的贡献。为了应对这一挑战，我们首先提出多模态混合(MM)，在聚合的多模态特征级别应用混合策略，通过生成虚拟特征-标签对来减轻过拟合。虽然MM有效提高了泛化能力，但它对所有模态一视同仁，没有考虑训练过程中的模态不平衡问题。基于MM，我们进一步引入平衡多模态混合(B-MM)，根据各模态对学习目标的相对贡献动态调整每个模态的混合比例。在多个数据集上的广泛实验证明了我们的方法在提高泛化能力和多模态鲁棒性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal video understanding plays a crucial role in tasks such as actionrecognition and emotion classification by combining information from differentmodalities. However, multimodal models are prone to overfitting strongmodalities, which can dominate learning and suppress the contributions ofweaker ones. To address this challenge, we first propose Multimodal Mixup (MM),which applies the Mixup strategy at the aggregated multimodal feature level tomitigate overfitting by generating virtual feature-label pairs. While MMeffectively improves generalization, it treats all modalities uniformly anddoes not account for modality imbalance during training. Building on MM, wefurther introduce Balanced Multimodal Mixup (B-MM), which dynamically adjuststhe mixing ratios for each modality based on their relative contributions tothe learning objective. Extensive experiments on several datasets demonstratethe effectiveness of our methods in improving generalization and multimodalrobustness.</description>
      <author>example@mail.com (Xiaoyu Ma, Ding Ding, Hao Chen)</author>
      <guid isPermaLink="false">2510.10986v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph</title>
      <link>http://arxiv.org/abs/2510.10976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Video-STR，一种基于图的强化学习方法，用于解决多模态大语言模型在精确时空理解方面的不足，通过引入GRPO推理机制和构建STV-205k数据集，在各种基准测试上取得最先进结果，比基础模型提高13%。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在语义理解方面表现出色，但在精确时空理解方面存在困难。现有时空方法主要关注视频本身，忽略了视频中的物理信息（如多物体布局和运动），限制了MLLM在具身智能和VR等需要高精度的下游应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在精确时空理解方面的不足，开发一种能够进行精确视频时空推理的方法。&lt;h4&gt;方法&lt;/h4&gt;基于可验证奖励的强化学习(RLVR)提高模型能力，引入基于图的组相对策略优化(GRPO)推理机制，指导模型在思考过程中推断场景的潜在时空拓扑结构，并构建包含205k个问答对的STV-205k数据集，涵盖室内和室外环境中的动态多物体场景。&lt;h4&gt;主要发现&lt;/h4&gt;Video-STR在各种基准测试上取得了最先进的结果，在STI-Bench上比基础模型提高了13%，证明了该方法和数据集的有效性。&lt;h4&gt;结论&lt;/h4&gt;Video-STR成功解决了多模态大语言模型在精确时空理解方面的不足，代码、模型和数据将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的最新进展展示了强大的语义理解能力，但在执行精确时空理解方面存在困难。现有的时空方法主要关注视频本身，而忽略了视频中的物理信息，如多物体布局和运动。这些限制限制了MLLM在需要高精度的下游应用中的使用，包括具身智能和VR。为解决这个问题，我们提出了Video-STR，一种基于图的强化学习方法，用于精确的视频时空推理。基于可验证奖励的强化学习(RLVR)提高模型能力的能力，我们引入了一种使用基于图的组相对策略优化(GRPO)方法的推理机制，指导模型在思考过程中推断场景的潜在时空拓扑结构。为解决时空训练数据的缺乏，我们构建了包含205k个问答对的STV-205k数据集，涵盖室内和室外环境中的动态多物体场景，以支持模型训练。实验表明，Video-STR在各种基准测试上取得了最先进的结果，在STI-Bench上比基础模型提高了13%，证明了我们方法和数据集的有效性。代码、模型和数据将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型（MLLMs）在精确时空理解方面的不足。现有模型虽然语义理解能力强，但在理解视频中的物体位置、布局、运动轨迹等物理信息方面表现不佳。这个问题很重要，因为它限制了MLLMs在需要高精度的下游应用（如具身智能和VR）中的使用，而这些应用对物体间的空间关系和时间动态有严格要求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有时空方法的局限性，发现它们主要关注视频本身而忽视物理信息，或者使用像素级定位和2D认知图等方法，这些方法无法准确推断物体在物理空间中的布局和分布。基于这些分析，作者设计了一个基于图的表示方法，将物体建模为节点，物体间关系建模为边，这种方法具有旋转不变性且更鲁棒。作者借鉴了强化学习与可验证奖励（RLVR）框架和Group Relative Policy Optimization（GRPO）方法，但扩展了它们以适应视频时空推理任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于图的表示来建模多物体场景的拓扑结构，并通过强化学习框架训练模型理解时空关系。整体流程包括：1）从TAO、ScanNet和KITTI收集数据；2）构建STV-205k数据集（205k问答对）；3）设计多种可验证奖励函数（格式、多选、数值、IoU奖励）；4）引入图推理机制帮助模型理解空间拓扑；5）使用扩展的GRPO算法进行训练；6）在多个基准测试上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）构建STV-205k数据集，解决视频时空训练数据稀缺问题；2）首次使用物体间关系图表征多物体场景，扩展GRPO引入图推理机制；3）设计特定奖励函数和图推理机制监督模型理解时空信息。相比之前工作，本文强调视频嵌入的物理信息而非仅关注视频本身；使用图结构而非像素定位或2D认知图表示场景，具有旋转不变性；通过特定奖励函数更有效监督模型对时空信息的理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Video-STR通过引入基于图的时空推理机制和STV-205k数据集，显著提升了多模态大语言模型在视频时空推理任务上的性能，实现了现有方法的最佳效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Multimodal Large Language Models (MLLMs) has demonstratedstrong semantic understanding capabilities, but struggles to perform precisespatio-temporal understanding. Existing spatio-temporal methods primarily focuson the video itself, while overlooking the physical information within thevideo, such as multi-object layouts and motion. Such limitations restrict theuse of MLLMs in downstream applications that demand high precision, includingembodied intelligence and VR. To address this issue, we present Video-STR, anovel graph-based reinforcement method for precise Video Spatio-TemporalReasoning. Building upon the capacity of Reinforcement Learning with VerifiableReward (RLVR) to improve model abilities, we introduce a reasoning mechanismusing graph-based Group Relative Policy Optimization (GRPO) method to guide themodel in inferring the underlying spatio-temporal topology of scenarios duringthe thinking process. To resolve the lack of spatio-temporal training data, weconstruct the STV-205k dataset with 205k question-answering pairs, coveringdynamic multi-object scenes in both indoor and outdoor environments, to supportthe model training. Experiments show that Video-STR achieves state-of-the-artresults on various benchmarks, outperforming the base model by 13% onSTI-Bench, and demonstrating the effectiveness of our approach and dataset.Code, model, and data will be released.</description>
      <author>example@mail.com (Wentao Wang, Heqing Zou, Tianze Luo, Rui Huang, Yutian Zhao, Zhuochen Wang, Hansheng Zhang, Chengwei Qin, Yan Wang, Lin Zhao, Huaijian Zhang)</author>
      <guid isPermaLink="false">2510.10976v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs</title>
      <link>http://arxiv.org/abs/2510.10689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了OmniVideoBench，一个专门用于评估多模态大语言模型协同视听理解能力的大规模基准测试。该基准测试包含1000个高质量问答对，覆盖13种问题类型，评估结果显示当前模型与人类推理能力存在明显差距，开源模型表现尤其不佳。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视频理解方面显示出巨大潜力，但现有基准测试无法全面评估跨音频和视觉模态的协同推理能力，往往忽略其中一个模态或以逻辑不一致的方式整合它们。&lt;h4&gt;目的&lt;/h4&gt;弥补现有基准测试的不足，引入一个专门用于评估协同视听理解能力的大规模且精心设计的基准测试，强调模态互补性和逻辑一致性。&lt;h4&gt;方法&lt;/h4&gt;构建OmniVideoBench基准测试，包含1000个高质量问答对，每个标注有逐步推理轨迹；数据来源于628个多样化视频，时长从几秒到30分钟不等；包含13种精心设计的问题类型，涵盖时间推理、空间定位、计数、因果推断和总结等；所有数据经过人工验证确保正确性和唯一性。&lt;h4&gt;主要发现&lt;/h4&gt;在OmniVideoBench上对多个多模态大语言模型的评估显示，模型性能与人类推理之间存在明显差距；开源模型显著落后于闭源模型，这突显了真实视听推理的内在难度。&lt;h4&gt;结论&lt;/h4&gt;将发布OmniVideoBench基准测试以促进具有更强和更可泛化推理能力的多模态大语言模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;最近多模态大语言模型的进展在视频理解方面展示了巨大潜力。然而，现有基准测试无法全面评估跨音频和视觉模态的协同推理能力，常常忽略其中一个模态或以逻辑不一致的方式整合它们。为了弥补这一差距，我们引入了OmniVideoBench，一个大规模且精心设计的基准测试，专门用于评估协同视听理解，特别强调模态互补性和逻辑一致性。具体来说，OmniVideoBench包含1000个高质量的问答对，每个都标注了逐步推理轨迹，来源于628个从几秒到30分钟不等的多样化视频，并经过人工验证以确保完全正确性和唯一性。此外，OmniVideoBench包含13种精心设计的问题类型，涵盖时间推理、空间定位、计数、因果推断、总结等，从而捕捉视频理解的基本挑战。在OmniVideoBench上对多个多模态大语言模型的评估揭示了模型性能与人类推理之间的明显差距，其中开源模型显著落后于闭源模型，这突显了真实视听推理的内在难度。我们将发布OmniVideoBench以促进具有更强和更可泛化推理能力的多模态大语言模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal large language models (MLLMs) have demonstratedsubstantial potential in video understanding. However, existing benchmarks failto comprehensively evaluate synergistic reasoning capabilities across audio andvisual modalities, often neglecting either one of the modalities or integratingthem in a logically inconsistent manner. To bridge this gap, we introduceOmniVideoBench, a large-scale and rigorously designed benchmark dedicated toassessing synergistic audio-visual understanding, with a strong emphasis onmodality complementarity and logical consistency. Specifically, OmniVideoBenchcomprises 1000 high-quality question-answer(QA) pairs, each annotated withstep-by-step reasoning traces, derived from 628 diverse videos ranging fromseveral seconds to 30 minutes, and manually verified to guarantee completecorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefullydesigned question types, covering temporal reasoning, spatial localization,counting, causal inference, summarization, and beyond, thereby capturing theessential challenges of video understanding. Evaluation of multiple MLLMs onOmniVideoBench reveals a pronounced gap between model performance and humanreasoning, with open-source models lagging significantly behind theirclosed-source counterparts, underscoring the inherent difficulty of genuineaudio-visual reasoning. We will release OmniVideoBench to foster thedevelopment of MLLMs with stronger and more generalizable reasoningcapabilities.</description>
      <author>example@mail.com (Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, Jiaheng Liu)</author>
      <guid isPermaLink="false">2510.10689v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction</title>
      <link>http://arxiv.org/abs/2510.10454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 GenAI4Health Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Traj-CoA是一个多智能体系统，通过智能体链处理电子健康记录数据，减少噪声并保留完整时间线，在患者轨迹建模任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型为患者轨迹建模提供了通用方法，但电子健康记录数据的时间推理存在数据冗长和嘈杂的问题。&lt;h4&gt;目的&lt;/h4&gt;解决EHR数据在时间推理中的长序列和噪声问题，提高患者轨迹建模的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Traj-CoA多智能体系统，使用工作智能体顺序处理EHR数据，将关键事件提炼到EHRMem记忆模块中，最后由管理智能体进行综合预测。&lt;h4&gt;主要发现&lt;/h4&gt;在基于五年EHR数据的一年肺癌风险预测的零样本任务中，Traj-CoA优于四类基线方法，展现出与临床实践一致的时间推理能力。&lt;h4&gt;结论&lt;/h4&gt;Traj-CoA是一种有前途的、鲁棒且通用的方法，用于建模复杂患者轨迹。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型为患者轨迹建模提供了一种通用方法，但受电子健康记录数据在时间推理中冗长和嘈杂的特性所困扰。为应对这些挑战，我们引入了Traj-CoA，一个涉及智能体链的多智能体系统，用于患者轨迹建模。Traj-CoA采用一系列工作智能体顺序处理可管理的EHR数据块，将关键事件提炼到共享的长期记忆模块EHRMem中，以减少噪声并保留完整时间线。最终管理智能体综合工作智能体的摘要和EHRMem中提取的时间线进行预测。在基于五年EHR数据的一年肺癌风险预测的零样本任务中，Traj-CoA优于四类基线方法。分析表明，Traj-CoA展现出与临床实践一致的时间推理能力，使其成为建模复杂患者轨迹的一种前景广阔的鲁棒且通用的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) offer a generalizable approach for modelingpatient trajectories, but suffer from the long and noisy nature of electronichealth records (EHR) data in temporal reasoning. To address these challenges,we introduce Traj-CoA, a multi-agent system involving chain-of-agents forpatient trajectory modeling. Traj-CoA employs a chain of worker agents toprocess EHR data in manageable chunks sequentially, distilling critical eventsinto a shared long-term memory module, EHRMem, to reduce noise and preserve acomprehensive timeline. A final manager agent synthesizes the worker agents'summary and the extracted timeline in EHRMem to make predictions. In azero-shot one-year lung cancer risk prediction task based on five-year EHRdata, Traj-CoA outperforms baselines of four categories. Analysis reveals thatTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as apromisingly robust and generalizable approach for modeling complex patienttrajectories.</description>
      <author>example@mail.com (Sihang Zeng, Yujuan Fu, Sitong Zhou, Zixuan Yu, Lucas Jing Liu, Jun Wen, Matthew Thompson, Ruth Etzioni, Meliha Yetisgen)</author>
      <guid isPermaLink="false">2510.10454v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration</title>
      <link>http://arxiv.org/abs/2510.10395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://avocado-captioner.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AVoCaDO，一个由音频和视觉模态时间编排驱动的强大音视频视频字幕生成器，通过两阶段微调管道显著提升了字幕生成质量。&lt;h4&gt;背景&lt;/h4&gt;音视频视频字幕生成旨在生成语义丰富的描述，并使视觉和听觉事件之间保持时间对齐，从而有助于视频理解和生成。&lt;h4&gt;目的&lt;/h4&gt;开发一个强大的音视频视频字幕生成器，通过音频和视觉模态之间的时间编排来提高字幕生成的质量。&lt;h4&gt;方法&lt;/h4&gt;提出了一个两阶段的微调管道：(1) AVoCaDO SFT，在一个新整理的包含107K高质量、时间对齐的音视频字幕的数据集上微调模型；(2) AVoCaDO GRPO，利用定制化的奖励函数来进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少崩溃。&lt;h4&gt;主要发现&lt;/h4&gt;1. AVoCaDO在四个音视频视频字幕生成基准测试中显著优于现有的开源模型。2. AVoCaDO在仅视觉设置下的VDC和DREAM-1K基准测试中实现了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;AVoCaDO通过两阶段微调管道有效地提高了音视频视频字幕生成的质量，不仅在音视频设置下表现出色，而且在仅视觉设置下也能保持竞争力。&lt;h4&gt;翻译&lt;/h4&gt;音视频视频字幕生成旨在生成语义丰富的描述，并使视觉和听觉事件之间保持时间对齐，从而有助于视频理解和生成。在本文中，我们提出了AVoCaDO，一个由音频和视觉模态之间时间编排驱动的强大音视频视频字幕生成器。我们提出了一个两阶段的微调管道：(1) AVoCaDO SFT，在一个新整理的包含107K高质量、时间对齐的音视频字幕的数据集上微调模型；(2) AVoCaDO GRPO，利用定制化的奖励函数来进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少崩溃。实验结果表明，AVoCaDO在四个音视频视频字幕生成基准测试中显著优于现有的开源模型，并且在仅视觉设置下的VDC和DREAM-1K基准测试中也实现了具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audiovisual video captioning aims to generate semantically rich descriptionswith temporal alignment between visual and auditory events, thereby benefitingboth video understanding and generation. In this paper, we present AVoCaDO, apowerful audiovisual video captioner driven by the temporal orchestrationbetween audio and visual modalities. We propose a two-stage post-trainingpipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curateddataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)AVoCaDO GRPO, which leverages tailored reward functions to further enhancetemporal coherence and dialogue accuracy while regularizing caption length andreducing collapse. Experimental results demonstrate that AVoCaDO significantlyoutperforms existing open-source models across four audiovisual videocaptioning benchmarks, and also achieves competitive performance on the VDC andDREAM-1K benchmark under visual-only settings.</description>
      <author>example@mail.com (Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, Liang Wang, Tieniu Tan)</author>
      <guid isPermaLink="false">2510.10395v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel Understanding</title>
      <link>http://arxiv.org/abs/2510.09274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一框架，用于联合优化时间句子定位和视频对象分割，解决了现有方法中忽视时间线索或增加系统复杂性的问题。&lt;h4&gt;背景&lt;/h4&gt;Referring Video Object Segmentation (RefVOS) 需要根据自然语言描述在视频中分割目标对象，这需要时间推理和细粒度的视觉理解能力。现有的基于LLM的采样策略通常依赖手工设计的启发式方法或外部关键帧模型，前者忽视重要时间线索，后者增加系统复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架，联合优化时间句子定位（TSG）和RefVOS，自然融合关键时刻定位能力。&lt;h4&gt;方法&lt;/h4&gt;1) 训练阶段：引入新的TSG范式，使用[FIND]标记通过时间标记相似度匹配识别关键时刻，避免外部时间戳编码；2) 推理阶段：设计以时刻为中心的采样(MCS)策略，密集采样信息丰富时刻，稀疏采样非必要帧；3) 开发双向锚点更新传播(BAP)，利用最相关时刻初始化高质量掩码，动态更新减轻累积误差。&lt;h4&gt;主要发现&lt;/h4&gt;通过联合优化TSG和RefVOS，以及创新的采样策略和传播方法，能够有效保留运动细节和全局上下文，同时提高跟踪稳定性。&lt;h4&gt;结论&lt;/h4&gt;该框架解决了现有采样策略的局限性，通过自然集成关键时刻定位能力，实现了更高效的视频对象分割。&lt;h4&gt;翻译&lt;/h4&gt;该论文提出了一种引用视频对象分割方法，通过自然语言描述引导视频中的目标对象分割，同时需要时间推理和细粒度视觉理解能力。基于LLM的现有采样策略通常依赖手工设计启发式方法或外部关键帧模型。前者常常忽视重要时间线索，后者增加系统复杂性。为此，我们提出统一框架，联合优化时间句子定位和引用视频对象分割，自然融入关键时刻定位能力。训练阶段，我们引入新型TSG范式，使用专用[FIND]标记通过时间标记相似度匹配识别关键时刻，避免需要外部时间戳编码。推理阶段，我们设计以时刻为中心的采样策略，密集采样信息丰富时刻，同时稀疏采样非必要帧，保留运动细节和全局上下文。为进一步增强跟踪稳定性，我们开发双向锚点更新传播，利用最相关时刻作为高质量掩码初始化起点，并在采样点动态更新以减轻累积误差。代码和模型将发布于：https://github.com/Dmmm1997/MomentSeg&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring Video Object Segmentation (RefVOS) seeks to segment target objectsin videos guided by natural language descriptions, demanding both temporalreasoning and fine-grained visual comprehension. Existing sampling strategiesfor LLM-based approaches typically rely on either handcrafted heuristics orexternal keyframe models. The former often overlooks essential temporal cues,while the latter increases system complexity. To address this, we propose aunified framework that jointly optimizes Temporal Sentence Grounding (TSG) andRefVOS, naturally incorporating key moment grounding capability. Duringtraining, we introduce a novel TSG paradigm that employs a dedicated\texttt{[FIND]} token for key moment identification through temporal tokensimilarity matching, thereby avoiding the need for external timestampencodings. For inference, we design a Moment-Centric Sampling (MCS) strategythat densely samples informative moments while sparsely sampling non-essentialframes, preserving both motion details and global context. To further enhancetracking stability, we develop Bidirectional Anchor-updated Propagation (BAP),which leverages the most relevant moment as start point for high-quality maskinitialization and dynamically updates at sampled points to mitigateaccumulated errors. Code and model will be available at:https://github.com/Dmmm1997/MomentSeg</description>
      <author>example@mail.com (Ming Dai, Sen Yang, Boqiang Duan, Wankou Yang, Jingdong Wang)</author>
      <guid isPermaLink="false">2510.09274v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras</title>
      <link>http://arxiv.org/abs/2510.09230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于消费级设备视频的肩部疾病诊断框架HMVDx，利用多模态大语言模型分离动作理解和疾病诊断任务，显著提高了诊断准确率。&lt;h4&gt;背景&lt;/h4&gt;肩部疾病是全球常见疾病，在老年人和从事重复肩部任务的工作者中发病率高。在医疗资源稀缺地区，早期准确诊断面临挑战，需要低成本且易于扩展的辅助诊断方案。&lt;h4&gt;目的&lt;/h4&gt;引入消费级设备拍摄的视频作为诊断基础降低成本，研究多模态大语言模型在肩部疾病初步诊断中的应用，提出HMVDx框架。&lt;h4&gt;方法&lt;/h4&gt;HMVDx框架将动作理解和疾病诊断两个任务分开，由两个多模态大语言模型分别完成。提出'可用性指数'新型指标，基于医疗决策逻辑过程评估多模态大语言模型在医疗领域的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;HMVDx在诊断肩关节损伤方面的准确率比直接视频诊断提高了79.6%，显示低成本多模态大语言模型在医疗应用中的潜在价值。&lt;h4&gt;结论&lt;/h4&gt;HMVDx对多模态大语言模型在医学领域视频理解应用的研究具有重大技术贡献。&lt;h4&gt;翻译&lt;/h4&gt;肩部疾病，如冻结肩（又名粘连性关节囊炎），是影响全球人民健康的常见疾病，在老年人和从事重复肩部任务的工作者中发病率高。在医疗资源稀缺的地区，实现早期准确诊断面临重大挑战，迫切需要低成本且易于扩展的辅助诊断解决方案。本研究引入消费级设备拍摄的视频作为诊断基础，降低用户成本。我们专注于多模态大语言模型在肩部疾病初步诊断中的创新应用，并提出混合运动视频诊断框架。该框架将动作理解和疾病诊断两个任务分开，分别由两个多模态大语言模型完成。除了传统评估指标外，本研究还提出了一种名为'可用性指数'的新型指标，基于医疗决策的逻辑过程（动作识别、运动诊断和最终诊断）。该指数从整个医疗诊断路径的角度评估多模态大语言模型在医疗领域的有效性，揭示了低成本多模态大语言模型在医疗应用中对医疗从业者的潜在价值。在实验比较中，HMVDx在诊断肩关节损伤方面的准确率比直接视频诊断提高了79.6%，这是多模态大语言模型在医学领域视频理解应用研究中的重大技术贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),are common conditions affecting the health of people worldwide, and have a highincidence rate among the elderly and workers engaged in repetitive shouldertasks. In regions with scarce medical resources, achieving early and accuratediagnosis poses significant challenges, and there is an urgent need forlow-cost and easily scalable auxiliary diagnostic solutions. This researchintroduces videos captured by consumer-grade devices as the basis fordiagnosis, reducing the cost for users. We focus on the innovative applicationof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis ofshoulder disorders and propose a Hybrid Motion Video Diagnosis framework(HMVDx). This framework divides the two tasks of action understanding anddisease diagnosis, which are respectively completed by two MLLMs. In additionto traditional evaluation indicators, this work proposes a novel metric calledUsability Index by the logical process of medical decision-making (actionrecognition, movement diagnosis, and final diagnosis). This index evaluates theeffectiveness of MLLMs in the medical field from the perspective of the entiremedical diagnostic pathway, revealing the potential value of low-cost MLLMs inmedical applications for medical practitioners. In experimental comparisons,the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by79.6\% compared with direct video diagnosis, a significant technicalcontribution to future research on the application of MLLMs for videounderstanding in the medical field.</description>
      <author>example@mail.com (Jindong Hong, Wencheng Zhang, Shiqin Qiao, Jianhai Chen, Jianing Qiu, Chuanyang Zheng, Qian Xu, Yun Ji, Qianyue Wen, Weiwei Sun, Hao Li, Huizhen Li, Huichao Wang, Kai Wu, Meng Li, Yijun He, Lingjie Luo, Jiankai Sun)</author>
      <guid isPermaLink="false">2510.09230v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Spatio-Temporal Graph Convolutional Networks for EV Charging Demand Forecasting Using Real-World Multi-Modal Data Integration</title>
      <link>http://arxiv.org/abs/2510.09048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出TW-GCN框架，结合图卷积网络和时间架构预测电动汽车充电需求，解决了充电设施分布不均和利用不规律对电网稳定性和投资规划的挑战&lt;h4&gt;背景&lt;/h4&gt;交通运输是温室气体的主要来源，向电动汽车等可持续替代品转型非常紧迫，但充电设施的空间分布不均和利用不规律对电网稳定性和投资规划构成挑战&lt;h4&gt;目的&lt;/h4&gt;开发TW-GCN框架，结合图卷积网络和时间架构，预测美国田纳西州的电动汽车充电需求&lt;h4&gt;方法&lt;/h4&gt;利用真实世界的交通流量、天气条件和美国最大的电动汽车基础设施公司提供的专有数据，捕捉空间依赖性和时间动态&lt;h4&gt;主要发现&lt;/h4&gt;中期（3小时）预测在响应性和稳定性之间取得最佳平衡；1DCNN在时间模型中表现持续优于其他模型；东、中、西田纳西州的预测准确性存在差异，反映了站点密度、人口和当地需求变异性对模型性能的影响&lt;h4&gt;结论&lt;/h4&gt;TW-GCN框架推动了数据驱动智能与电动汽车基础设施规划的整合，支持可持续交通转型和弹性电网管理&lt;h4&gt;翻译&lt;/h4&gt;交通仍然是温室气体的主要来源，这凸显了向电动汽车等可持续替代品过渡的紧迫性。然而，充电设施的空间分布不均和使用不规则对电网稳定性和投资规划构成了挑战。本研究引入了TW-GCN，一个结合图卷积网络和时间架构的时空预测框架，用于预测美国田纳西州的电动汽车充电需求。我们利用真实的交通流量、天气条件以及美国最大的电动汽车基础设施公司提供的专有数据，来捕捉空间依赖性和时间动态。在不同滞后时间、聚类策略和序列长度上的广泛实验表明，中期（3小时）预测在响应性和稳定性之间取得了最佳平衡，1DCNN持续优于其他时间模型。区域分析显示东、中、西田纳西州的预测准确性存在差异，反映了站点密度、人口和当地需求变异性如何影响模型性能。所提出的TW-GCN框架推动了数据驱动智能与电动汽车基础设施规划的整合，支持可持续交通转型和弹性电网管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transportation remains a major contributor to greenhouse gas emissions,highlighting the urgency of transitioning toward sustainable alternatives suchas electric vehicles (EVs). Yet, uneven spatial distribution and irregularutilization of charging infrastructure create challenges for both power gridstability and investment planning. This study introduces TW-GCN, aspatio-temporal forecasting framework that combines Graph ConvolutionalNetworks with temporal architectures to predict EV charging demand inTennessee, United States (U.S.). We utilize real-world traffic flows, weatherconditions, and proprietary data provided by one of the largest EVinfrastructure company in the U.S. to capture both spatial dependencies andtemporal dynamics. Extensive experiments across varying lag horizons,clustering strategies, and sequence lengths reveal that mid-horizon (3-hour)forecasts achieve the best balance between responsiveness and stability, with1DCNN consistently outperforming other temporal models. Regional analysis showsdisparities in predictive accuracy across East, Middle, and West Tennessee,reflecting how station density, population, and local demand variability shapemodel performance. The proposed TW-GCN framework advances the integration ofdata-driven intelligence into EV infrastructure planning, supporting bothsustainable mobility transitions and resilient grid management.</description>
      <author>example@mail.com (Jose Tupayachi, Mustafa C. Camur, Kevin Heaslip, Xueping Li)</author>
      <guid isPermaLink="false">2510.09048v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos</title>
      <link>http://arxiv.org/abs/2510.08936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了Ro-Bench，首个用于评估多模态大语言模型在动态分布外反事实视频测试集上的基准，发现当前模型在面对被操纵的视频内容时鲁棒性不足，但通过反事实数据微调可显著提升性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在各种视频理解任务中表现出显著性能，但当面对被操纵的视频内容时，它们的鲁棒性在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;引入Ro-Bench，首个用于评估多模态大语言模型在动态分布外反事实视频测试集上的基准。&lt;h4&gt;方法&lt;/h4&gt;Ro-Bench通过编辑风格、物体、背景及其组合，整合高质量、多样化的时间相关视频数据；评估了八个最近的视频多模态大语言模型；并通过反事实数据微调多模态大语言模型以增强鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型在面对反事实视频内容时，在Ro-Bench上表现出显著的性能下降；使用反事实数据微调多模态大语言模型可以增强鲁棒性，在Ro-Bench上实现了21.73%的性能提升，在MVBench数据集的20个任务上实现了12.78%的改进。&lt;h4&gt;结论&lt;/h4&gt;反事实数据在增强多模态大语言模型的视频理解能力方面具有显著有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近，多模态大语言模型在各种视频理解任务中展示了显著的性能。然而，它们的鲁棒性，特别是在面对被操纵的视频内容时，在很大程度上仍未被探索。在本文中，我们引入了Ro-Bench，这是首个用于评估多模态大语言模型在动态分布外反事实视频测试集上的基准。Ro-Bench通过编辑风格、物体、背景及其组合，整合了高质量、多样化且时间相关的视频数据。我们评估了八个最近的视频多模态大语言模型，发现当面对反事实视频内容时，当前模型在Ro-Bench上表现出显著的性能下降。此外，我们证明使用反事实数据微调多模态大语言模型可以增强鲁棒性，在Ro-Bench上实现了21.73%的性能提升，在MVBench数据集的20个任务上实现了12.78%的改进。这些发现强调了反事实数据在增强多模态大语言模型视频理解能力方面的有效性。代码和数据将很快发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Multi-modal Large Language Models (MLLMs) have demonstratedsignificant performance across various video understanding tasks. However,their robustness, particularly when faced with manipulated video content,remains largely unexplored. In this paper, we introduce Ro-Bench, the firstbenchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)counterfactual video test sets. Ro-Bench incorporates high-quality, diverse andtemporally relevant video data, by editing Style, Object, Background and theircompositions. We evaluated eight recent video MLLMs and found that currentmodels exhibit substantial performance degradation on Ro-Bench when exposed tocounterfactual video content. Furthermore, we demonstrate that fine-tuningMLLMs with counterfactual data enhances robustness, achieving a 21.73%performance increase on Ro-Bench and a 12.78% improvement across 20 tasks inthe MVBench dataset. These findings underscore the effectiveness ofcounterfactual data in enhancing the video understanding ability of MLLMs. Thecode and data will be released shortly.</description>
      <author>example@mail.com (Zixi Yang, Jiapeng Li, Muxi Diao, Yinuo Jing, Kongming Liang)</author>
      <guid isPermaLink="false">2510.08936v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition</title>
      <link>http://arxiv.org/abs/2510.08818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted to EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了D-CoDe，一个无需训练的适应框架，用于解决将图像预训练的视觉语言模型扩展到视频领域时面临的感知瓶颈和令牌过载问题。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在多样化的视频语言任务中表现出色，可以通过适应图像预训练的视觉语言模型来有效构建。然而，这种适应具有挑战性，因为需要处理密集且时间上延展的视觉输入，这超出了基于图像模型的处理能力。&lt;h4&gt;目的&lt;/h4&gt;解决将基于图像的视觉语言模型扩展到视频领域时面临的感知瓶颈和令牌过载这两个关键挑战。&lt;h4&gt;方法&lt;/h4&gt;提出D-CoDe，一个无需训练的适应框架，结合了动态压缩和问题分解两种技术。动态压缩通过自适应选择代表性帧和空间令牌的内容感知聚合来减轻感知瓶颈；问题分解通过将原始查询重新表述为子问题来缓解令牌过载。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明D-CoDe在各种基准测试中有效提高了视频理解能力，特别是在具有挑战性的长视频基准测试上表现出色，突显了其处理复杂视频语言任务的潜力。&lt;h4&gt;结论&lt;/h4&gt;D-CoDe框架能够有效解决视频理解中的感知瓶颈和令牌过载问题，为视频大语言模型的构建提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型在多样化的视频语言任务中表现出色，可以通过适应图像预训练的视觉语言模型来有效构建。然而，这种适应仍然具有挑战性，因为它需要处理密集且时间上延展的视觉输入，这超出了基于图像模型的处理能力。本文确定了感知瓶颈和令牌过载是将基于图像的视觉语言模型扩展到视频领域时的关键挑战。为了解决这些问题，我们提出了D-CoDe，一个无需训练的适应框架，结合了动态压缩和问题分解。具体而言，动态压缩通过自适应选择代表性帧和空间令牌的内容感知聚合来减轻感知瓶颈，从而减少冗余同时保留信息内容。同时，问题分解通过将原始查询重新表述为子问题来缓解令牌过载，指导模型关注视频的不同方面，实现更全面的理解。实验证明D-CoDe在各种基准测试中有效提高了视频理解能力。此外，在具有挑战性的长视频基准测试上的良好表现突显了D-CoDe处理复杂视频语言任务的潜力。代码可在https://github.com/hukcc/D-CoDe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video large language models (Vid-LLMs), which excel in diverse video-languagetasks, can be effectively constructed by adapting image-pretrainedvision-language models (VLMs). However, this adaptation remains challenging, asit requires processing dense and temporally extended visual inputs that exceedthe capacity of image-based models. This paper identifies the perceptionbottleneck and token overload as key challenges in extending image-based VLMsto the video domain. To address these issues, we propose D-CoDe, atraining-free adaptation framework that incorporates dynamic compression andquestion decomposition. Specifically, dynamic compression alleviates theperception bottleneck through adaptive selection of representative frames andcontent-aware aggregation of spatial tokens, thereby reducing redundancy whilepreserving informative content. In parallel, question decomposition mitigatestoken overload by reformulating the original query into sub-questions, guidingthe model to focus on distinct aspects of the video and enabling morecomprehensive understanding. Experiments demonstrate that D-CoDe effectivelyimproves video understanding across various benchmarks. Furthermore, strongperformance on the challenging long-video benchmark highlights the potential ofD-CoDe in handling complex video-language tasks. Code is available athttps://github.com/hukcc/D-CoDe.</description>
      <author>example@mail.com (Yiyang Huang, Yizhou Wang, Yun Fu)</author>
      <guid isPermaLink="false">2510.08818v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Edu-EmotionNet: Cross-Modality Attention Alignment with Temporal Feedback Loops</title>
      <link>http://arxiv.org/abs/2510.08802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages, 6 Figures, 3 Tables, Accepted as a Regular Research paper at  ICMLA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Edu-EmotionNet框架，用于在线教育中的学习者情绪识别，通过联合建模时间情绪演变和模态可靠性，实现了稳健的情感识别。&lt;h4&gt;背景&lt;/h4&gt;在线教育中理解学习者情绪对提高参与度和个性化教学至关重要。现有情绪识别方法通常采用静态融合策略，并假设模态输入始终可靠，这在真实学习环境中很少成立。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，联合建模时间情绪演变和模态可靠性，以实现稳健的情感识别，特别适用于在线教育环境。&lt;h4&gt;方法&lt;/h4&gt;该模型包含三个关键组件：跨模态注意力对齐模块用于动态跨模态上下文共享；模态重要性估计器为每个模态在每一步分配基于置信度的权重；时间反馈循环利用先前的预测来强制时间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;Edu-EmotionNet在IEMOCAP和MOSEI的教育子集上取得了最先进的性能，并显示出对缺失或有噪声模态的强大鲁棒性。可视化证实了其捕捉情绪转变和自适应优先考虑可靠信号的能力。&lt;h4&gt;结论&lt;/h4&gt;该模型适合部署在实时学习系统中，能够有效识别学习者的情绪状态，为个性化教学提供支持。&lt;h4&gt;翻译&lt;/h4&gt;理解在线教育中的学习者情绪对于提高参与度和个性化教学至关重要。虽然先前在情绪识别方面的工作探索了多模态融合和时间建模，但现有方法通常依赖静态融合策略，并假设模态输入始终可靠，这在真实学习环境中很少成立。我们引入了Edu-EmotionNet，一个新颖的框架，联合建模时间情绪演变和模态可靠性，以实现稳健的情感识别。我们的模型包含三个关键组件：用于动态跨模态上下文共享的跨模态注意力对齐模块，为每个模态在每一步分配基于置信度的权重的模态重要性估计器，以及利用先前预测强制时间一致性的时间反馈循环。在为困惑、好奇、无聊和沮丧重新注释的教育子集IEMOCAP和MOSEI上评估，Edu-EmotionNet取得了最先进的性能，并显示出对缺失或有噪声模态的强大鲁棒性。可视化证实了其捕捉情绪转变和自适应优先考虑可靠信号的能力，使其非常适合部署在实时学习系统中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding learner emotions in online education is critical for improvingengagement and personalized instruction. While prior work in emotionrecognition has explored multimodal fusion and temporal modeling, existingmethods often rely on static fusion strategies and assume that modality inputsare consistently reliable, which is rarely the case in real-world learningenvironments. We introduce Edu-EmotionNet, a novel framework that jointlymodels temporal emotion evolution and modality reliability for robust affectrecognition. Our model incorporates three key components: a Cross-ModalityAttention Alignment (CMAA) module for dynamic cross-modal context sharing, aModality Importance Estimator (MIE) that assigns confidence-based weights toeach modality at every time step, and a Temporal Feedback Loop (TFL) thatleverages previous predictions to enforce temporal consistency. Evaluated oneducational subsets of IEMOCAP and MOSEI, re-annotated for confusion,curiosity, boredom, and frustration, Edu-EmotionNet achieves state-of-the-artperformance and demonstrates strong robustness to missing or noisy modalities.Visualizations confirm its ability to capture emotional transitions andadaptively prioritize reliable signals, making it well suited for deployment inreal-time learning systems</description>
      <author>example@mail.com (S M Rafiuddin)</author>
      <guid isPermaLink="false">2510.08802v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.07791v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Geo-Temporal Reasoning基准测试(GTR-Bench)，用于评估视觉-语言模型在结合图像/视频和图形上下文时的地理时空智能能力。评估显示当前最佳模型表现显著落后于人类，并揭示了三个主要缺陷。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型的时空智能在自动驾驶、具身AI和通用人工智能领域受到关注。现有基准测试主要关注自我视角推理或地理视角推理，无法评估结合图像/视频和图形上下文时的地理时空智能，这对交通管理和应急响应等领域很重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准测试的不足，引入Geo-Temporal Reasoning基准测试(GTR-Bench)，用于评估大规模摄像头网络中移动目标的地理时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建GTR-Bench基准测试，要求模型在地图和视频之间进行多视角切换，对多个具有非重叠视野的视频进行联合推理，并对任何视频上下文都未观察到的时空区域进行推理。评估了10多种流行的视觉-语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;即使最佳专有模型Gemini-2.5-Pro(34.9%)在地理时间推理上也显著落后于人类表现(78.61%)。当前模型存在三个主要缺陷：(1)时空上下文利用不平衡；(2)时间预测能力较弱，时间强调任务表现差；(3)缺乏理解或对齐地图数据与多视角视频输入的能力。&lt;h4&gt;结论&lt;/h4&gt;GTR-Bench为时空智能的研究和应用提供了有价值的见解和新的机会。基准测试和代码将在https://github.com/X-Luffy/GTR-Bench上发布。&lt;h4&gt;翻译&lt;/h4&gt;最近，视觉-语言模型的时空智能因其对自动驾驶、具身AI和通用人工智能的重要性而受到广泛关注。现有的时空基准测试主要关注基于图像/视频上下文的自我视角推理，或基于图形上下文(如地图)的地理视角推理，因此无法评估VLMs在结合图像/视频和图形上下文时的地理时空智能，这对交通管理和应急响应等领域很重要。为解决这些差距，我们引入了Geo-Temporal Reasoning基准测试(GTR-Bench)，这是一个在大规模摄像头网络中对移动目标进行地理时间推理的新挑战。GTR-Bench更具挑战性，因为它需要在地图和视频之间进行多视角切换，对多个具有非重叠视野的视频进行联合推理，以及对任何视频上下文都未观察到的时空区域进行推理。对GTR-Bench上10多种流行VLMs的评估表明，即使是最佳专有模型Gemini-2.5-Pro(34.9%)在地理时间推理上也显著落后于人类表现(78.61%)。此外，我们对GTR-Bench的全面分析揭示了当前模型在地理时间推理方面的三个主要缺陷。(1)VLMs的推理受到时空上下文不平衡利用的影响。(2)VLMs在时间预测方面能力较弱，导致在时间强调任务上的表现比空间强调任务差。(3)VLMs缺乏理解或对齐地图数据与多视角视频输入的能力。我们相信GTR-Bench为时空智能的研究和应用提供了有价值的见解和新的机会。基准测试和代码将在https://github.com/X-Luffy/GTR-Bench上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently spatial-temporal intelligence of Visual-Language Models (VLMs) hasattracted much attention due to its importance for Autonomous Driving, EmbodiedAI and General Artificial Intelligence. Existing spatial-temporal benchmarksmainly focus on egocentric perspective reasoning with images/video context, orgeographic perspective reasoning with graphics context (eg. a map), thus failto assess VLMs' geographic spatial-temporal intelligence with both images/videoand graphics context, which is important for areas like traffic management andemergency response. To address the gaps, we introduce Geo-Temporal Reasoningbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning ofmoving targets in a large-scale camera network. GTR-Bench is more challengingas it requires multiple perspective switches between maps and videos, jointreasoning across multiple videos with non-overlapping fields of view, andinference over spatial-temporal regions that are unobserved by any videocontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate thateven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lagsbehind human performance (78.61%) on geo-temporal reasoning. Moreover, ourcomprehensive analysis on GTR-Bench reveals three primary deficiencies ofcurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired byan imbalanced utilization of spatial-temporal context. (2) VLMs are weak intemporal forecasting, which leads to worse performance on temporal-emphasizedtasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency tocomprehend or align the map data with multi-view video inputs. We believeGTR-Bench offers valuable insights and opens up new opportunities for researchand applications in spatial-temporal intelligence. Benchmark and code will bereleased at https://github.com/X-Luffy/GTR-Bench.</description>
      <author>example@mail.com (Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng)</author>
      <guid isPermaLink="false">2510.07791v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>From Captions to Keyframes: KeyScore for Multimodal Frame Scoring and Video-Language Understanding</title>
      <link>http://arxiv.org/abs/2510.06509v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了KeyScore和STACFP两种方法，用于选择信息量大的关键帧，提高视频理解效率和准确性。KeyScore是一种基于字幕感知的帧评分方法，STACFP是一种时空自适应聚类方法，两者结合可减少无用帧，保留关键内容，实现更快更准确的推理。&lt;h4&gt;背景&lt;/h4&gt;选择信息量大的关键帧对于高效的视频理解至关重要，但现有方法往往依赖于启发式方法，忽略语义，或者产生冗余帧。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够结合语义相似性、时间代表性和上下文下降影响三种互补信号的帧评分方法，生成帧级别的重要性分数，用于训练关键帧提取器或指导视频语言模型。&lt;h4&gt;方法&lt;/h4&gt;1. KeyScore：字幕感知的帧评分方法，结合三种互补信号：与字幕的语义相似性、时间代表性和上下文下降影响。2. STACFP：时空自适应聚类方法，在长视频中生成多样且紧凑的帧提案。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标准视频语言基准（MSRVTT、MSVD、DiDeMo）上的实验表明，结合STACFP和KeyScore与全帧处理相比可实现高达99%的帧减少，同时在视频文本检索、关键帧提取和动作识别任务中优于均匀8帧编码器。&lt;h4&gt;结论&lt;/h4&gt;通过专注于语义相关的帧，该方法提高了效率和性能，实现了可扩展的、基于字幕的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;选择信息量大的关键帧对于高效的视频理解至关重要，但现有方法往往依赖于启发式方法，忽略语义，或者产生冗余帧。我们提出了KeyScore，一种字幕感知的帧评分方法，结合了三种互补信号：与字幕的语义相似性、时间代表性和上下文下降影响。应用于大规模视频字幕数据集，KeyScore生成帧级别的重要性分数，使能够训练关键帧提取器或指导视频语言模型。为此，我们还提出了STACFP，一种时空自适应聚类方法，能够在长视频中生成多样且紧凑的帧提案。KeyScore和STACFP共同减少无用帧，同时保留关键内容，实现更快更准确的推理。我们在三个标准视频语言基准（MSRVTT、MSVD、DiDeMo）上的实验表明，与全帧处理相比，结合STACFP和KeyScore可以实现高达99%的帧减少，同时在视频文本检索、关键帧提取和动作识别任务中优于均匀8帧编码器。通过专注于语义相关的帧，我们的方法提高了效率和性能，实现了可扩展的、基于字幕的视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Selecting informative keyframes is critical for efficient videounderstanding, yet existing approaches often rely on heuristics, ignoresemantics, or produce redundant frames. We propose KeyScore, a caption-awareframe scoring method that combines three complementary signals: semanticsimilarity to captions, temporal representativeness, and contextual dropimpact. Applied to large-scale video-caption datasets, KeyScore generatesframe-level importance scores that enable training keyframe extractors orguiding video-language models. To support this, we also propose STACFP, aSpatio-Temporal Adaptive Clustering method that generates diverse and compactframe proposals across long videos. Together, KeyScore and STACFP reduceuninformative frames while preserving critical content, resulting in faster andmore accurate inference. Our experiments on three standard video-languagebenchmarks (MSRVTT, MSVD, DiDeMo) show that combining STACFP and KeyScoreenables up to 99% frame reduction compared to full-frame processing, whileoutperforming uniform 8-frame encoders in video-text retrieval, keyframeextraction, and action recognition tasks. By focusing on semantically relevantframes, our method enhances both efficiency and performance, enabling scalableand caption-grounded video understanding.</description>
      <author>example@mail.com (Shih-Yao Lin, Sibendu Paul, Caren Chen)</author>
      <guid isPermaLink="false">2510.06509v2</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>InfiniHuman: Infinite 3D Human Creation with Precise Control</title>
      <link>http://arxiv.org/abs/2510.11650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM SIGGRAPH Asia 2025. Project website:  https://yuxuan-xue.com/infini-human&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了InfiniHuman框架，通过利用现有基础模型生成丰富注释的3D人体数据，解决了大规模人体数据集收集和标注成本高的问题。该框架包括InfiniHumanData（自动数据生成管道）和InfiniHumanGen（基于扩散的生成管道），能够生成高质量、可精确控制的3D人体化身。&lt;h4&gt;背景&lt;/h4&gt;生成真实且可控的3D人体化身是一项长期挑战，尤其是在覆盖广泛属性范围（如种族、年龄、服装风格和详细身体形状）时。捕获和注释用于训练生成模型的大规模人体数据集成本极高，且在规模和多样性上受到限制。&lt;h4&gt;目的&lt;/h4&gt;研究核心问题是：现有基础模型是否可以被提炼，以生成理论上无限的、丰富注释的3D人体数据？论文旨在提出一种方法，以最低成本和理论上无限的扩展性生成丰富注释的人体数据。&lt;h4&gt;方法&lt;/h4&gt;提出了InfiniHuman框架协同提炼现有模型；InfiniHumanData作为全自动管道利用视觉语言和图像生成模型创建大规模多模态数据集；InfiniHumanGen作为基于扩散的生成管道以文本、身体形状和服装资源为条件。数据集包含111K个身份，每个身份都有多粒度文本描述、多视图RGB图像、详细服装图像和SMPL身体形状参数。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究表明自动生成的身份与扫描渲染无法区分；InfiniHumanData包含111K个身份，覆盖前所未有的多样性；实验证明在视觉质量、生成速度和可控性方面显著优于最先进方法；通过实用且经济实惠的解决方案实现了在有效无限规模下的高质量化身生成和细粒度控制。&lt;h4&gt;结论&lt;/h4&gt;InfiniHuman框架提供了一种实用且经济实惠的解决方案，能够以有效无限的规模生成高质量、细粒度控制的3D人体化身。研究团队将公开自动数据生成管道、全面的InfiniHumanData数据集和InfiniHumanGen模型。&lt;h4&gt;翻译&lt;/h4&gt;生成真实且可控的3D人体化身是一项长期存在的挑战，尤其是在覆盖广泛的属性范围（如种族、年龄、服装风格和详细的身体形状）时。捕获和注释用于训练生成模型的大规模人体数据集成本极高，且在规模和多样性上受到限制。我们在本文中要解决的核心问题是：现有的基础模型是否可以被提炼，以生成理论上无限的、丰富注释的3D人体数据？我们介绍了InfiniHuman，一个协同提炼这些模型以最低成本和理论上无限的扩展性生成丰富注释的人体数据的框架。我们提出了InfiniHumanData，一个全自动管道，利用视觉语言和图像生成模型创建大规模多模态数据集。用户研究表明，我们自动生成的身份与扫描渲染无法区分。InfiniHumanData包含111K个身份，覆盖前所未有的多样性。每个身份都注释有多粒度文本描述、多视图RGB图像、详细服装图像和SMPL身体形状参数。基于此数据集，我们提出了InfiniHumanGen，一个基于扩散的生成管道，以文本、身体形状和服装资源为条件。InfiniHumanGen能够实现快速、真实且精确可控的化身生成。大量实验表明，在视觉质量、生成速度和可控性方面显著优于最先进的方法。我们的方法通过实用且经济实惠的解决方案，实现了在有效无限规模下的高质量化身生成和细粒度控制。我们将在https://yuxuan-xue.com/infini-human公开自动数据生成管道、全面的InfiniHumanData数据集和InfiniHumanGen模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决生成真实且可控制的3D人类化身（avatar）的挑战，特别是在覆盖广泛属性如种族、年龄、服装风格和身体形状时。这个问题在虚拟现实、数字时尚、游戏和社会远程呈现等领域至关重要，因为这些领域对逼真且可定制的个性化化身需求日益增长，而现有方法要么依赖成本高昂的真实扫描数据，要么生成质量有限或控制性不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过'蒸馏'现有基础模型来生成大规模3D人类数据，设计了一个完全自动化的框架。他们利用了多种现有技术：使用GPT-4o生成文本描述，微调FLUX模型生成正交视图的'扫描式'图像，借鉴虚拟试衣技术提取服装图像，使用SMPL模型表示人体形状和姿势，应用扩散模型生成一致的多视图图像，并基于OminiControl2进行微调实现高分辨率生成。这种方法整合了多个领域的前沿技术，形成了一个协同工作的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合和重新利用现有基础模型创建一个完全自动化的框架，生成大规模、丰富注释的3D人类数据，并基于这些数据训练生成模型以实现精确控制。整体流程分为两部分：1) 数据生成（InfiniHumanData）：包括多粒度文本描述生成、正交文本到图像转换、虚拟试衣提取服装图像、单目身体拟合获取SMPL参数、正交多视图扩散生成高分辨率多视图图像；2) 生成模型（InfiniHumanGen）：包括Gen-Schnell（快速生成3D高斯点云）和Gen-HRes（高分辨率纹理网格生成），两者都支持基于文本、身体形状和服装图像的条件化生成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) InfiniHuman框架，实现完全自动化的无限3D人类数据生成；2) InfiniHumanData数据集，包含111K个多样化身份，具有前所未有的种族、年龄、服装风格等多样性；3) InfiniHumanGen生成模型，提供Gen-Schnell和Gen-HRes两种互补模型，支持精确控制。相比之前工作，该方法解决了视觉质量、生成速度和属性可控性的局限性，提供了对服装的精确控制，生成的身份在视觉上与真实扫描无法区分，数据集规模和多样性远超现有公开数据集，生成速度比现有高分辨率方法快8倍以上。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InfiniHuman通过整合现有基础模型创建了一个完全自动化的框架，能够生成大规模、多样化的3D人类数据集，并基于此实现了高质量、高速度且具有精确控制的3D人类化身生成，从而降低了高质量化身创建的门槛并实现了无限规模的扩展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3757377.3763815&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic and controllable 3D human avatars is a long-standingchallenge, particularly when covering broad attribute ranges such as ethnicity,age, clothing styles, and detailed body shapes. Capturing and annotatinglarge-scale human datasets for training generative models is prohibitivelyexpensive and limited in scale and diversity. The central question we addressin this paper is: Can existing foundation models be distilled to generatetheoretically unbounded, richly annotated 3D human data? We introduceInfiniHuman, a framework that synergistically distills these models to producerichly annotated human data at minimal cost and with theoretically unlimitedscalability. We propose InfiniHumanData, a fully automatic pipeline thatleverages vision-language and image generation models to create a large-scalemulti-modal dataset. User study shows our automatically generated identitiesare undistinguishable from scan renderings. InfiniHumanData contains 111Kidentities spanning unprecedented diversity. Each identity is annotated withmulti-granularity text descriptions, multi-view RGB images, detailed clothingimages, and SMPL body-shape parameters. Building on this dataset, we proposeInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, bodyshape, and clothing assets. InfiniHumanGen enables fast, realistic, andprecisely controllable avatar generation. Extensive experiments demonstratesignificant improvements over state-of-the-art methods in visual quality,generation speed, and controllability. Our approach enables high-quality avatargeneration with fine-grained control at effectively unbounded scale through apractical and affordable solution. We will publicly release the automatic datageneration pipeline, the comprehensive InfiniHumanData dataset, and theInfiniHumanGen models at https://yuxuan-xue.com/infini-human.</description>
      <author>example@mail.com (Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll)</author>
      <guid isPermaLink="false">2510.11650v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping</title>
      <link>http://arxiv.org/abs/2510.11576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Being reviewed for WHISPERS conference ( Workshop on Hyperspectral  Image and Signal Processing: Evolution in Remote Sensing )&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了三种基础模型在超光谱作物制图中的性能，发现SpectralEarth预训练的Vision Transformer表现最佳，达到93.5%的总体准确率，强调了模型架构对跨区域泛化能力的重要性。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在改变地球观测领域，但它们在超光谱作物制图中的潜力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;基准测试三种基础模型用于超光谱作物制图，评估它们在不同地理区域和传感器平台上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;测试了三种基础模型（HyperSigma、DOFA和在SpectralEarth数据集上预训练的Vision Transformer），在手动标记的训练区域数据上进行微调，并在独立的测试区域进行评估。性能测量包括总体准确率(OA)、平均准确率(AA)和F1分数。&lt;h4&gt;主要发现&lt;/h4&gt;HyperSigma达到34.5%的OA（±1.8%），DOFA达到62.6%的OA（±3.5%），SpectralEarth模型达到93.5%的OA（±0.8%）。从头开始训练的紧凑型SpectralEarth变体达到了91%的性能，突显了模型架构对于在地理区域和传感器平台之间强泛化能力的重要性。&lt;h4&gt;结论&lt;/h4&gt;这些结果为操作超光谱作物制图的基础模型提供了系统评估，并概述了未来模型开发的方向。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在改变地球观测，但它们在超光谱作物制图中的潜力仍然未被充分探索。本研究使用超光谱图像对三种基础模型进行了基准测试，用于谷物作物制图：HyperSigma、DOFA以及在SpectralEarth数据集上预训练的Vision Transformer（一个大的多时相超光谱档案）。模型在手动标记的训练区域数据上进行微调，并在独立的测试区域进行评估。性能通过总体准确率(OA)、平均准确率(AA)和F1分数来衡量。HyperSigma实现了34.5%的OA（±1.8%），DOFA达到62.6%（±3.5%），而SpectralEarth模型实现了93.5%的OA（±0.8%）。从头开始训练的紧凑型SpectralEarth变体达到了91%，突显了模型架构对于在地理区域和传感器平台之间强泛化能力的重要性。这些结果为操作超光谱作物制图的基础模型提供了系统评估，并概述了未来模型开发的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are transforming Earth observation, but their potential forhyperspectral crop mapping remains underexplored. This study benchmarks threefoundation models for cereal crop mapping using hyperspectral imagery:HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarthdataset (a large multitemporal hyperspectral archive). Models were fine-tunedon manually labeled data from a training region and evaluated on an independenttest region. Performance was measured with overall accuracy (OA), averageaccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved91%, highlighting the importance of model architecture for stronggeneralization across geographic regions and sensor platforms. These resultsprovide a systematic evaluation of foundation models for operationalhyperspectral crop mapping and outline directions for future model development.</description>
      <author>example@mail.com (Walid Elbarz, Mohamed Bourriz, Hicham Hajji, Hamd Ait Abdelali, François Bourzeix)</author>
      <guid isPermaLink="false">2510.11576v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>How many samples to label for an application given a foundation model? Chest X-ray classification study</title>
      <link>http://arxiv.org/abs/2510.11553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了胸部X光分类中基础模型对标注数据的需求量，发现XrayCLIP和XraySigLIP模型在显著少于传统ResNet-50基线模型的情况下仍能实现高性能，且仅需50个标注样本即可准确预测最终性能表现。&lt;h4&gt;背景&lt;/h4&gt;胸部X光分类对医学诊断至关重要，但传统方法需要大量标注数据，资源消耗大。基础模型虽可减少对标注数据的依赖，但具体需要多少标注样本尚不明确。&lt;h4&gt;目的&lt;/h4&gt;系统性评估使用幂律拟合方法预测达到特定ROC-AUC阈值所需训练样本量的可行性，旨在确定胸部X光分类任务中基础模型的最优标注样本数量。&lt;h4&gt;方法&lt;/h4&gt;研究测试了多种病理学和基础模型，应用幂律拟合来预测达到特定性能阈值所需的训练样本量，并验证了从少量标注样本(仅50个)的学习曲线斜率预测最终性能的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;XrayCLIP和XraySigLIP模型相比ResNet-50基线模型，在显著更少的标注样本下实现了同等或更好的性能；仅使用50个标注病例的学习曲线就能准确预测模型最终的性能平台期。&lt;h4&gt;结论&lt;/h4&gt;研究结果使医学影像从业者能够通过仅标注针对目标性能所必需的样本，有效降低数据标注成本，优化资源分配。&lt;h4&gt;翻译&lt;/h4&gt;胸部X光分类至关重要但资源密集，通常需要大量标注数据才能实现准确诊断。基础模型可以减少这种依赖，但需要多少标注样本尚不清楚。我们系统性评估了使用幂律拟合来预测达到特定ROC-AUC阈值所需训练样本量的方法。通过测试多种病理学和基础模型，我们发现XrayCLIP和XraySigLIP在显著少于ResNet-50基线的标注样本下实现了强大性能。重要的是，仅从50个标注病例的学习曲线斜率就能准确预测最终性能平台期。我们的结果使从业者能够通过仅标注针对目标性能所必需的样本来最小化标注成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chest X-ray classification is vital yet resource-intensive, typicallydemanding extensive annotated data for accurate diagnosis. Foundation modelsmitigate this reliance, but how many labeled samples are required remainsunclear. We systematically evaluate the use of power-law fits to predict thetraining size necessary for specific ROC-AUC thresholds. Testing multiplepathologies and foundation models, we find XrayCLIP and XraySigLIP achievestrong performance with significantly fewer labeled examples than a ResNet-50baseline. Importantly, learning curve slopes from just 50 labeled casesaccurately forecast final performance plateaus. Our results enablepractitioners to minimize annotation costs by labeling only the essentialsamples for targeted performance.</description>
      <author>example@mail.com (Nikolay Nechaev, Evgenia Przhezdzetskaya, Viktor Gombolevskiy, Dmitry Umerenkov, Dmitry Dylov)</author>
      <guid isPermaLink="false">2510.11553v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification</title>
      <link>http://arxiv.org/abs/2510.11537v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure. Submitted to VLSP 2025 and reviewed&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TextGraphFuseGAT的新型神经网络架构，结合了预训练的transformer编码器和图注意力网络，用于标记级分类任务，在多个越南语数据集上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的标记级分类模型在处理标记间复杂关系时存在局限性，特别是在特定领域如医疗、COVID-19等需要精确识别实体类型的任务中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获标记间依赖关系并结合预训练语义特征的模型，以提高标记级分类任务在多个领域的性能。&lt;h4&gt;方法&lt;/h4&gt;提出TextGraphFuseGAT模型，在PhoBERT生成的标记嵌入上构建全连接图，使用GAT层捕获标记间依赖关系，并应用Transformer风格的自注意力层增强上下文化，最后通过分类头进行序列标注。&lt;h4&gt;主要发现&lt;/h4&gt;在三个越南语基准数据集（PhoNER-COVID19、PhoDisfluency和VietMed-NER）上的实验表明，该方法始终优于强基线模型，包括仅使用transformer的模型和混合神经网络模型。&lt;h4&gt;结论&lt;/h4&gt;将预训练语义特征与基于图的建模相结合是提高跨领域标记分类性能的有效方法，特别是在处理具有专业词汇和领域特定表达的数据时。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为TextGraphFuseGAT的新型神经架构，该架构集成了预训练的transformer编码器（PhoBERT）和图注意力网络，用于标记级分类任务。所提出的模型在PhoBERT生成的标记嵌入上构建了一个全连接图，使GAT层能够捕获丰富的标记间依赖关系，而不仅仅是顺序上下文建模。为了进一步增强上下文化，在图增强的嵌入上应用了Transformer风格的自注意力层。最终的标记表示通过分类头进行序列标注。我们在三个越南语基准数据集上评估了我们的方法：PhoNER-COVID19用于COVID-19领域的命名实体识别，PhoDisfluency用于言语不流畅性检测，以及VietMed-NER用于医疗领域NER。VietMed-NER是第一个越南语医疗口语NER数据集，包含18种从真实医疗语音转录中收集的实体类型，并使用BIO标记方案进行标注。其专业词汇和领域特定表达使其成为标记级分类模型的一个具有挑战性的基准。实验结果表明，我们的方法始终优于强基线模型，包括仅使用transformer的模型和混合神经网络模型（如BiLSTM + CNN + CRF），证实了结合预训练语义特征和基于图的关系建模对提高跨领域标记分类的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel neural architecture named TextGraphFuseGAT, whichintegrates a pretrained transformer encoder (PhoBERT) with Graph AttentionNetworks for token-level classification tasks. The proposed model constructs afully connected graph over the token embeddings produced by PhoBERT, enablingthe GAT layer to capture rich inter-token dependencies beyond those modeled bysequential context alone. To further enhance contextualization, aTransformer-style self-attention layer is applied on top of the graph-enhancedembeddings. The final token representations are passed through a classificationhead to perform sequence labeling. We evaluate our approach on three Vietnamesebenchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19domain, PhoDisfluency for speech disfluency detection, and VietMed-NER formedical-domain NER. VietMed-NER is the first Vietnamese medical spoken NERdataset, featuring 18 entity types collected from real-world medical speechtranscripts and annotated with the BIO tagging scheme. Its specializedvocabulary and domain-specific expressions make it a challenging benchmark fortoken-level classification models. Experimental results show that our methodconsistently outperforms strong baselines, including transformer-only andhybrid neural models such as BiLSTM + CNN + CRF, confirming the effectivenessof combining pretrained semantic features with graph-based relational modelingfor improved token classification across multiple domains.</description>
      <author>example@mail.com (Ba-Quang Nguyen)</author>
      <guid isPermaLink="false">2510.11537v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation</title>
      <link>http://arxiv.org/abs/2510.11346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at ICCV Workshops 2025, "The 4th Workshop  on What is Next in Multimodal Foundation Models?" (MMFM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用未标记领域数据训练ControlNet的方法，通过引入不确定性概念到控制机制中，使模型能够创建具有高不确定性的目标领域标注数据，显著提高了分割结果，无需额外监督。&lt;h4&gt;背景&lt;/h4&gt;生成模型是创建高质量图像数据的有价值工具。ControlNet等受控扩散模型已允许创建标记分布，可用于增强原始训练分布。然而，ControlNet倾向于重现原始训练分布，限制了增强效果。以视网膜OCT为例，高质量的Spectralis图像有真实分割可用于训练，但Home-OCT设备产生的图像质量较低且存在较大域偏移，使得现成的分割网络无法应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法利用未标记领域数据训练ControlNet，通过引入不确定性概念，创建具有高不确定性的目标领域标注数据，解决域偏移问题，提高分割性能。&lt;h4&gt;方法&lt;/h4&gt;将不确定性概念引入控制机制中，不确定性表示给定图像不属于下游任务（如分割）的训练分布。最终网络结合两种控制：来自未标记数据集的不确定性控制和来自标记数据集的语义控制。这种方法允许创建来自目标领域的具有高不确定性的标注数据，即来自未标记分布的带标签合成数据。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ControlNet能够创建来自Home-OCT域的带注释图像，显著提高了分割结果，无需额外监督。与风格迁移相比，不确定性引导能够实现任意的域偏移，而无需严格学习图像风格，这一点在交通场景实验中也得到了验证。&lt;h4&gt;结论&lt;/h4&gt;通过引入不确定性概念到ControlNet的控制机制中，可以利用未标记领域数据训练模型，创建高质量的标注数据，有效解决域偏移问题，显著提高分割性能，且无需额外监督。&lt;h4&gt;翻译&lt;/h4&gt;生成模型是用于高质量图像数据受控创建的有价值工具。像ControlNet这样的受控扩散模型已允许创建标记分布。当训练判别模型（如语义分割）时，这类合成数据可以增强原始训练分布。然而，这种增强效果有限，因为ControlNet倾向于重现原始训练分布。这项工作引入了一种利用未标记领域数据训练ControlNet的方法，通过将不确定性概念引入控制机制中。不确定性表示给定图像不属于下游任务（如分割）的训练分布。因此，最终网络涉及两种控制：来自未标记数据集的不确定性控制和来自标记数据集的语义控制。所得到的ControlNet允许我们创建来自目标领域的高不确定性标注数据，即来自未标记分布的带标签合成数据。在我们的场景中，我们考虑视网膜OCT，通常高质量的Spectralis图像有给定的真实分割，可用于训练分割网络。然而，Home-OCT设备的最新发展产生了质量较低且存在较大域偏移的视网膜OCT，使得现成的分割网络无法应用于此类数据。使用所提出的方法合成来自Home-OCT域的带注释图像弥合了这一差距，并在不添加任何额外监督的情况下显著提高了分割结果。与风格迁移相比，不确定性引导的优势很明显：它能够实现任意的域偏移，而无需严格学习图像风格。这一点在交通场景实验中也得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Models are a valuable tool for the controlled creation ofhigh-quality image data. Controlled diffusion models like the ControlNet haveallowed the creation of labeled distributions. Such synthetic datasets canaugment the original training distribution when discriminative models, likesemantic segmentation, are trained. However, this augmentation effect islimited since ControlNets tend to reproduce the original training distribution.  This work introduces a method to utilize data from unlabeled domains to trainControlNets by introducing the concept of uncertainty into the controlmechanism. The uncertainty indicates that a given image was not part of thetraining distribution of a downstream task, e.g., segmentation. Thus, two typesof control are engaged in the final network: an uncertainty control from anunlabeled dataset and a semantic control from the labeled dataset. Theresulting ControlNet allows us to create annotated data with high uncertaintyfrom the target domain, i.e., synthetic data from the unlabeled distributionwith labels. In our scenario, we consider retinal OCTs, where typicallyhigh-quality Spectralis images are available with given ground truthsegmentations, enabling the training of segmentation networks. The recentdevelopment in Home-OCT devices, however, yields retinal OCTs with lowerquality and a large domain shift, such that out-of-the-pocket segmentationnetworks cannot be applied for this type of data. Synthesizing annotated imagesfrom the Home-OCT domain using the proposed approach closes this gap and leadsto significantly improved segmentation results without adding any furthersupervision. The advantage of uncertainty-guidance becomes obvious whencompared to style transfer: it enables arbitrary domain shifts without anystrict learning of an image style. This is also demonstrated in a traffic sceneexperiment.</description>
      <author>example@mail.com (Joshua Niemeijer, Jan Ehrhardt, Heinz Handels, Hristina Uzunova)</author>
      <guid isPermaLink="false">2510.11346v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Protein as a Second Language for LLMs</title>
      <link>http://arxiv.org/abs/2510.11188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Main paper: 9 pages, 6 figures. With references and appendix: 18  pages, 9 figures total. Submitted to ICLR 2026 (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出'蛋白质作为第二语言'框架，将氨基酸序列重新表述为大型语言模型可解释的符号语言，通过自适应构建序列-问题-答案三元组在零样本设置中揭示蛋白质功能线索，无需额外训练。&lt;h4&gt;背景&lt;/h4&gt;解析未知蛋白质序列的功能是具有广泛科学影响的基本挑战，但现有方法大多依赖于特定任务适配器或大规模监督微调。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖特定任务适配器或大规模监督微调的蛋白质功能解析方法。&lt;h4&gt;方法&lt;/h4&gt;引入'蛋白质作为第二语言'框架，将氨基酸序列重新表述为新型符号语言中的句子，大型语言模型可通过上下文示例解释。该方法自适应构建序列-问题-答案三元组，在零样本设置中揭示功能线索。为此，整理了包含79,926个蛋白质-QA实例的双语语料库，涵盖属性预测、描述性理解和扩展推理。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在各种开源大型语言模型和GPT-4上取得了一致的改进，ROUGE-L最高提升17.2%（平均+7%），甚至超过了微调的蛋白质特定语言模型。&lt;h4&gt;结论&lt;/h4&gt;当用蛋白质作为语言的线索引导时，通用大型语言模型可以超越领域专用模型，为基础模型中的蛋白质理解提供了可扩展的途径。&lt;h4&gt;翻译&lt;/h4&gt;解析未知蛋白质序列的功能是一个具有广泛科学影响的基本挑战，但大多数现有方法依赖于特定任务的适配器或大规模监督微调。我们引入了'蛋白质作为第二语言'框架，将氨基酸序列重新表述为一种新型符号语言中的句子，大型语言模型可以通过上下文示例来解释。我们的方法自适应地构建序列-问题-答案三元组，在零样本设置中揭示功能线索，无需任何进一步训练。为此，我们整理了一个包含79,926个蛋白质-QA实例的双语语料库，涵盖属性预测、描述性理解和扩展推理。实验上，我们的方法在各种开源大型语言模型和GPT-4上取得了一致的改进，ROUGE-L最高提升17.2%（平均+7%），甚至超过了微调的蛋白质特定语言模型。这些结果表明，当用蛋白质作为语言的线索引导时，通用大型语言模型可以超越领域专用模型，为基础模型中的蛋白质理解提供了可扩展的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deciphering the function of unseen protein sequences is a fundamentalchallenge with broad scientific impact, yet most existing methods depend ontask-specific adapters or large-scale supervised fine-tuning. We introduce the"Protein-as-Second-Language" framework, which reformulates amino-acid sequencesas sentences in a novel symbolic language that large language models caninterpret through contextual exemplars. Our approach adaptively constructssequence-question-answer triples that reveal functional cues in a zero-shotsetting, without any further training. To support this process, we curate abilingual corpus of 79,926 protein-QA instances spanning attribute prediction,descriptive understanding, and extended reasoning. Empirically, our methoddelivers consistent gains across diverse open-source LLMs and GPT-4, achievingup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tunedprotein-specific language models. These results highlight that generic LLMs,when guided with protein-as-language cues, can outperform domain-specializedmodels, offering a scalable pathway for protein understanding in foundationmodels.</description>
      <author>example@mail.com (Xinhui Chen, Zuchao Li, Mengqi Gao, Yufeng Zhang, Chak Tou Leong, Haoyang Li, Jiaqi Chen)</author>
      <guid isPermaLink="false">2510.11188v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.11176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为G2L框架的新策略，通过知识蒸馏技术使大规模病理学模型（仅占giga-scale模型15%参数）在癌症特定任务上达到与giga-scale模型相当的性能，同时显著降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;近期研究表明，扩大训练数据规模、增加癌症类型多样性和增大模型尺寸可提升病理学基础模型性能。然而，giga-scale基础模型（训练于数十万张玻片，覆盖数十种癌症类型，含数十亿参数）因开发和部署中的巨大计算成本，实际应用面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新策略(G2L框架)，使大规模基础模型（仅占giga-scale模型15%的参数）在癌症特定任务上达到与giga-scale模型相当的性能水平。&lt;h4&gt;方法&lt;/h4&gt;应用知识蒸馏技术，将giga-scale模型的能力转移到大规模模型，仅使用目标癌症（如乳腺癌、前列腺癌等）的1K张病理玻片进行知识蒸馏。&lt;h4&gt;主要发现&lt;/h4&gt;蒸馏后的模型在多个基准测试中优于同规模的最先进模型，甚至在某些测试中超过了giga-scale教师模型和huge-scale模型；蒸馏模型表现出更高的鲁棒性指数，对来自多个机构的图像变化具有更强的适应能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的蒸馏方法是一种数据和参数高效的方式，可以在癌症特定应用中达到giga-scale级别的性能，同时避免过高的计算负担。&lt;h4&gt;翻译&lt;/h4&gt;近期病理学基础模型研究表明，扩展训练数据、增加癌症类型多样性和提升模型尺寸能持续改善模型性能。然而，giga-scale基础模型（训练于覆盖数十种癌症类型的数十万张玻片，包含数十亿参数）因开发和部署中的巨大计算成本，给实际应用带来重大挑战。本研究提出了一种名为G2L框架的新策略，使大规模基础模型（仅占giga-scale模型15%的参数）在癌症特定任务上达到与giga-scale模型相当的性能水平。我们的方法应用知识蒸馏技术，将giga-scale模型的能力转移到大规模模型，仅使用目标癌症（如乳腺癌、前列腺癌等）的1K张病理玻片。所得蒸馏模型不仅在多个基准测试中优于同规模的最先进模型，而且有趣的是，在某些基准测试中甚至超过了giga-scale教师模型和huge-scale模型。此外，蒸馏模型表现出更高的鲁棒性指数，表明对来自多个机构的图像变化具有更强的适应能力。这些发现表明，针对大规模模型提出的蒸馏方法是实现giga-scale级别性能的数据和参数高效途径，且不会带来过高的计算负担。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies in pathology foundation models have shown that scalingtraining data, diversifying cancer types, and increasing model sizeconsistently improve their performance. However, giga-scale foundation models,which are trained on hundreds of thousands of slides covering tens of cancertypes and contain billions of parameters, pose significant challenges forpractical use due to their tremendous computational costs in both developmentand deployment. In this work, we present a novel strategy, named the G2Lframework, to increase the performance of large-scale foundation models, whichconsist of only $15\%$ of the parameters of giga-scale models, to a comparableperformance level of giga-scale models in cancer-specific tasks. Our approachapplies knowledge distillation, transferring the capabilities of a giga-scalemodel to a large-scale model, using just 1K pathology slides of a target cancer(e.g., breast, prostate, etc.). The resulting distilled model not onlyoutperformed state-of-the-art models of the same size (i.e., large-scale)across several benchmarks but also, interestingly, surpassed the giga-scaleteacher and huge-scale models in some benchmarks. In addition, the distilledmodel exhibited a higher robustness index, indicating improved resilience toimage variations originating from multiple institutions. These findings suggestthat the proposed distillation approach for a large-scale model is a data- andparameter-efficient way to achieve giga-scale-level performance forcancer-specific applications without prohibitive computational burden.</description>
      <author>example@mail.com (Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin)</author>
      <guid isPermaLink="false">2510.11176v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times</title>
      <link>http://arxiv.org/abs/2510.11138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对Foundation Models (FMs)驱动的FMware生态系统进行了首次大规模分析，研究其应用领域、开发挑战和问题解决需求，为改进FMware工具、工作流程和社区支持提供指导。&lt;h4&gt;背景&lt;/h4&gt;Foundation Models如OpenAI的GPT正在改变软件工程实践，催生了FMware（围绕这些模型构建的应用和基础设施）。FMware支持代码生成、自然语言交互、知识集成和多模态内容创建，但其设计、实现和演化在云平台和本地部署环境中带来了新的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究FMware在云平台和开源仓库中的开发情况，通过三个重点领域进行实证分析：(1)最常见的应用领域，(2)开发者遇到的关键挑战，(3)需要最大努力解决的问题类型。&lt;h4&gt;方法&lt;/h4&gt;从GitHub仓库和领先的FMware平台（包括HuggingFace、GPTStore、Ora和Poe）收集数据进行实证调查分析。&lt;h4&gt;主要发现&lt;/h4&gt;FMware强烈关注教育、内容创建和商业战略；在内存管理、依赖处理和tokenizer配置方面存在持久技术挑战；GitHub上最常报告的是错误报告和核心功能问题；最耗时的解决方案是代码审查、相似性搜索和提示模板设计。&lt;h4&gt;结论&lt;/h4&gt;通过揭示开发者的实践和痛点，研究指出了改进FMware工具、工作流程和社区支持的机会，提供了指导FMware开发未来的可行见解。&lt;h4&gt;翻译&lt;/h4&gt;基础模型（FMs），如OpenAI的GPT，正在从根本上改变软件工程的实践，使能够开发围绕这些模型的FMware——应用和基础设施。FMware系统现在支持代码生成、自然语言交互、知识集成和多模态内容创建等任务，凸显了它们对当前软件工程工作流程的颠覆性影响。然而，FMware的设计、实现和演化带来了显著的新挑战，特别是在云平台和本地部署平台上，这些平台的目标、流程和工具往往与传统软件开发不同。据我们所知，这是首次对云平台和开源仓库中的FMware开发进行大规模分析。我们通过三个重点领域对FMware生态系统进行了实证研究：(1)FMware最常见的应用领域，(2)开发者遇到的关键挑战，(3)需要最大努力解决的问题类型。我们的分析借鉴了GitHub仓库以及领先的FMware平台（包括HuggingFace、GPTStore、Ora和Poe）的数据。我们的研究结果显示，FMware强烈关注教育、内容创建和商业战略，同时在内存管理、依赖处理和tokenizer配置方面存在持久的技术挑战。在GitHub上，错误报告和核心功能问题是最常报告的问题，而代码审查、相似性搜索和提示模板设计是最耗时的解决方案。通过揭示开发者的实践和痛点，这项研究指出了改进FMware工具、工作流程和社区支持的机会，并提供了可行的见解，帮助指导FMware开发的未来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transformingthe practice of software engineering by enabling the development of\emph{FMware} -- applications and infrastructures built around these models.FMware systems now support tasks such as code generation, natural-languageinteraction, knowledge integration, and multi-modal content creation,underscoring their disruptive impact on current software engineering workflows.However, the design, implementation, and evolution of FMware presentsignificant new challenges, particularly across cloud-based and on-premiseplatforms where goals, processes, and tools often diverge from those oftraditional software development.  To our knowledge, this is the first large-scale analysis of FMwaredevelopment across both cloud-based platforms and open-source repositories. Weempirically investigate the FMware ecosystem through three focus areas: (1) themost common application domains of FMware, (2) the key challenges developersencounter, and (3) the types of issues that demand the greatest effort toresolve. Our analysis draws on data from GitHub repositories and from leadingFMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findingsreveal a strong focus on education, content creation, and business strategy,alongside persistent technical challenges in memory management, dependencyhandling, and tokenizer configuration. On GitHub, bug reports and corefunctionality issues are the most frequently reported problems, while codereview, similarity search, and prompt template design are the mosttime-consuming to resolve.  By uncovering developer practices and pain points, this study points toopportunities to improve FMware tools, workflows, and community support, andprovides actionable insights to help guide the future of FMware development.</description>
      <author>example@mail.com (Zitao Wang, Zhimin Zhao, Michael W. Godfrey)</author>
      <guid isPermaLink="false">2510.11138v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Improving AI Efficiency in Data Centres by Power Dynamic Response</title>
      <link>http://arxiv.org/abs/2510.11119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于创新方法的AI数据中心电力管理解决方案的能力和局限性，提出将部分输入电力动态化以提高可持续性。&lt;h4&gt;背景&lt;/h4&gt;人工智能近年来在大型语言模型和基础模型等复杂模型的推动下加速发展，但AI数据中心对电力需求极大，其电力管理问题对环境和可持续发展造成影响。&lt;h4&gt;目的&lt;/h4&gt;研究基于创新方法的AI数据中心电力管理解决方案的能力和局限性，即让部分输入电力与数据计算功能使用的电力一样动态化。&lt;h4&gt;方法&lt;/h4&gt;通过分析全球多个数据平台的电力趋势，量化比较被动设备和主动设备在计算增益、能源效率、资本支出减少和管理成本方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;这种动态电力管理策略代表了AI数据中心电力管理的一种范式转变，有潜力显著提高AI超算的可持续性。&lt;h4&gt;结论&lt;/h4&gt;该策略可以增强AI在环境、财务和社会领域的影响，促进AI的可持续发展。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的稳定增长近年来有所加速，这得益于大型语言模型和基础模型等复杂模型的发展。确保强大可靠的基础设施对于充分发挥AI的潜力至关重要。然而，AI数据中心极其耗电，使其电力管理问题成为焦点，特别是它们对环境和可持续发展的影响。在这项工作中，我们研究了基于创新方法的AI数据中心电力管理解决方案的能力和局限性，即使部分输入电源与用于数据计算功能的电源一样动态化。通过分析来自全球多个数据平台的电力趋势，我们以计算增益、能源效率、资本支出减少和管理成本为量化指标，比较了被动设备和主动设备的性能。这种策略代表了AI数据中心电力管理的一种范式转变，有潜力显著提高AI超算的可持续性，增强其在环境、财务和社会领域的影响力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The steady growth of artificial intelligence (AI) has accelerated in therecent years, facilitated by the development of sophisticated models such aslarge language models and foundation models. Ensuring robust and reliable powerinfrastructures is fundamental to take advantage of the full potential of AI.However, AI data centres are extremely hungry for power, putting the problem oftheir power management in the spotlight, especially with respect to theirimpact on environment and sustainable development. In this work, we investigatethe capacity and limits of solutions based on an innovative approach for thepower management of AI data centres, i.e., making part of the input power asdynamic as the power used for data-computing functions. The performance ofpassive and active devices are quantified and compared in terms ofcomputational gain, energy efficiency, reduction of capital expenditure, andmanagement costs by analysing power trends from multiple data platformsworldwide. This strategy, which identifies a paradigm shift in the AI datacentre power management, has the potential to strongly improve thesustainability of AI hyperscalers, enhancing their footprint on environmental,financial, and societal fields.</description>
      <author>example@mail.com (Andrea Marinoni, Sai Shivareddy, Pietro Lio', Weisi Lin, Erik Cambria, Clare Grey)</author>
      <guid isPermaLink="false">2510.11119v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts</title>
      <link>http://arxiv.org/abs/2510.11028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by PRCV&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的两阶段框架，用于工业异常检测中的零样本异常分割任务，有效结合了CLIP的异常定位能力和SAM的边界感知能力，解决了基础模型在下游任务中的引导问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型展现出强大的泛化能力，为零样本异常分割任务带来了新解决方案，但正确引导这些模型解决下游任务仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效利用CLIP和SAM优势的两阶段框架，提高零样本异常分割的准确性和精确性。&lt;h4&gt;方法&lt;/h4&gt;1) 提出Co-Feature Point Prompt Generation (PPG)模块，协同使用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域而非整个对象；2) 引入Cascaded Prompts for SAM (CPS)模块，采用混合提示与SAM轻量级解码器级联，优化分割结果并减少边界粗糙和孤立噪声。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上验证了该方法的有效性，取得了最先进的零样本异常分割结果，特别是在Visa数据集上，F1-max和AP指标分别比最先进方法高出10.3%和7.7%。&lt;h4&gt;结论&lt;/h4&gt;该两阶段框架成功解决了SAM在异常分割中的局限性，通过协同利用CLIP和SAM的优势，实现了异常区域的精确分割，为工业异常检测提供了有效的零样本解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近，基础模型展现出的强大泛化能力为零样本异常分割任务带来了新的解决方案。然而，正确引导这些基础模型解决下游任务仍然是一个挑战。本文提出了一种用于工业异常检测中零样本异常分割任务的新型两阶段框架。该框架出色地利用了CLIP的强大异常定位能力和SAM的边界感知能力。1) 为缓解SAM倾向于目标分割的问题，我们提出了Co-Feature Point Prompt Generation (PPG)模块。该模块协同使用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域而非整个对象。2) 为进一步优化SAM的分割结果并缓解边界粗糙和孤立噪声，我们引入了Cascaded Prompts for SAM (CPS)模块。该模块采用与SAM轻量级解码器级联的混合提示，实现异常区域的精确分割。在多个数据集上的一致实验验证表明，我们的方法取得了最先进的零样本异常分割结果。特别值得注意的是，我们在Visa数据集上的表现，在F1-max和AP指标上分别比最先进方法高出10.3%和7.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, the powerful generalization ability exhibited by foundation modelshas brought forth new solutions for zero-shot anomaly segmentation tasks.However, guiding these foundation models correctly to address downstream tasksremains a challenge. This paper proposes a novel two-stage framework, forzero-shot anomaly segmentation tasks in industrial anomaly detection. Thisframework excellently leverages the powerful anomaly localization capability ofCLIP and the boundary perception ability of SAM.(1) To mitigate SAM'sinclination towards object segmentation, we propose the Co-Feature Point PromptGeneration (PPG) module. This module collaboratively utilizes CLIP and SAM togenerate positive and negative point prompts, guiding SAM to focus onsegmenting anomalous regions rather than the entire object. (2) To furtheroptimize SAM's segmentation results and mitigate rough boundaries and isolatednoise, we introduce the Cascaded Prompts for SAM (CPS) module. This moduleemploys hybrid prompts cascaded with a lightweight decoder of SAM, achievingprecise segmentation of anomalous regions. Across multiple datasets, consistentexperimental validation demonstrates that our approach achievesstate-of-the-art zero-shot anomaly segmentation results. Particularlynoteworthy is our performance on the Visa dataset, where we outperform thestate-of-the-art methods by 10.3\% and 7.7\% in terms of {$F_1$-max} and APmetrics, respectively.</description>
      <author>example@mail.com (Yanning Hou, Ke Xu, Junfa Li, Yanran Ruan, Jianfeng Qiu)</author>
      <guid isPermaLink="false">2510.11028v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2510.11005v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种前景感知频谱分割(FASS)框架，用于解决医学图像中肿瘤和相邻正常组织分割的挑战，特别是在复杂、低对比度背景下。&lt;h4&gt;背景&lt;/h4&gt;在医学图像中准确分割肿瘤和相邻正常组织对手术规划和肿瘤分期至关重要。基础模型在分割任务中表现良好，但在复杂、低对比度背景下往往难以聚焦前景区域，因为某些恶性肿瘤与正常器官相似，难以区分。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂条件下提高分割鲁棒性和精细结构识别能力的分割框架。&lt;h4&gt;方法&lt;/h4&gt;FASS框架包含三个主要模块：1) 前景感知模块，增强背景与整个体积空间之间的区别；2) 基于小波变换的特征级频率增强模块，提取判别性高频特征以增强边界识别；3) 边缘约束模块，保持分割边界的几何连续性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个医学数据集上的实验表明，该框架在所有指标上表现优越，特别是在复杂条件下的鲁棒性和精细结构识别方面效果显著。&lt;h4&gt;结论&lt;/h4&gt;该框架显著提高了低对比度图像的分割效果，为更多样化和复杂的医学成像场景中的应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;医学图像中肿瘤和相邻正常组织的准确分割对于手术规划和肿瘤分期至关重要。尽管基础模型在分割任务中通常表现良好，但它们往往难以在复杂、低对比度背景下聚焦前景区域，因为某些恶性肿瘤与正常器官相似，使上下文区分复杂化。为解决这些挑战，我们提出了前景感知频谱分割(FASS)框架。首先，我们引入了前景感知模块来增强背景与整个体积空间之间的区别，使模型能够更有效地集中注意力在目标区域。其次，基于小波变换的特征级频率增强模块提取判别性高频特征，以增强边界识别和细节感知。最后，我们引入了边缘约束模块来保持分割边界的几何连续性。在多个医学数据集上的大量实验证明，所有指标上均表现出优越性能，验证了我们框架的有效性，特别是在复杂条件下的鲁棒性和精细结构识别方面。我们的框架显著提高了低对比度图像的分割效果，为更多样化和复杂的医学成像场景中的应用铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决低对比度腹部医学图像中肿瘤和正常组织的准确分割问题。这个问题在现实中非常重要，因为准确的肿瘤分割对手术计划和肿瘤分期至关重要，直接影响治疗效果和患者生存率。同时，手动标注非常耗时费力，需要大量临床专业知识，特别是在处理复杂解剖结构或模糊边界时。低对比度图像中肿瘤与周围组织灰度值相似，边界模糊，使得现有分割方法难以准确区分肿瘤和正常组织，导致分割结果不完整或边界断裂。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了低对比度医学图像分割的关键挑战，识别出现有方法在处理复杂背景和相似组织时的局限性。他们从频率域角度思考解决方案，利用频率增强来放大高频成分，使模型能更好地捕捉细微特征。设计过程中，作者借鉴了深度学习在医学图像分割中的应用经验，参考了两阶段策略和单阶段方法的优缺点，并利用了小波变换在图像处理中的成功经验。同时，他们整合了注意力机制和对抗训练等现有技术，但创新性地将它们结合起来，形成了前景感知模块、特征级频率增强模块和边缘约束模块三个互补组件，共同解决低对比度分割的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用频率域信息增强来改善低对比度医学图像的分割效果，通过对抗训练使模型更专注于前景区域，并结合边缘约束确保分割结果的几何连续性。整体实现流程包括：1) 前景感知模块：通过对抗训练学习前景和背景特征的异质性，使模型能专注于目标区域；2) 特征级频率增强模块：利用小波变换将特征分解为不同频率成分，选择性增强判别性高频特征，提高边界识别和细节感知能力；3) 边缘约束模块：通过边界关键点集约束确保分割边界的几何连续性；4) 整体框架将这三个模块有机结合，通过端到端训练实现高质量的分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 前景感知模块：创新性地使用对抗训练策略最大化背景和输入图像特征间的分布差异，使模型能有效抵抗复杂背景干扰；2) 特征级频率增强模块：首次将小波变换与交叉注意力机制结合，选择性增强和利用判别性高频信息，减少噪声干扰；3) 边缘约束模块：将物理模型先验知识整合到深度学习框架中，确保分割边界的几何连续性；4) 整体框架设计：首次将频率域分析与前景感知、边缘约束相结合。相比之前的工作，这种方法能更有效地处理低对比度图像中的细微差异，选择性利用判别性特征而非盲目增强所有高频成分，并通过边缘约束保持边界完整性，整体性能显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于频率域分析的前景感知谱分割框架，通过结合前景感知、特征级频率增强和边缘约束三个创新模块，显著提高了低对比度腹部医学图像中肿瘤和器官的分割精度和边界连续性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of tumors and adjacent normal tissues in medical imagesis essential for surgical planning and tumor staging. Although foundationmodels generally perform well in segmentation tasks, they often struggle tofocus on foreground areas in complex, low-contrast backgrounds, where somemalignant tumors closely resemble normal organs, complicating contextualdifferentiation. To address these challenges, we propose the Foreground-AwareSpectrum Segmentation (FASS) framework. First, we introduce a foreground-awaremodule to amplify the distinction between background and the entire volumespace, allowing the model to concentrate more effectively on target areas.Next, a feature-level frequency enhancement module, based on wavelet transform,extracts discriminative high-frequency features to enhance boundary recognitionand detail perception. Eventually, we introduce an edge constraint module topreserve geometric continuity in segmentation boundaries. Extensive experimentson multiple medical datasets demonstrate superior performance across allmetrics, validating the effectiveness of our framework, particularly inrobustness under complex conditions and fine structure recognition. Ourframework significantly enhances segmentation of low-contrast images, pavingthe way for applications in more diverse and complex medical imaging scenarios.</description>
      <author>example@mail.com (Kai Han, Siqi Ma, Chengxuan Qian, Jun Chen, Chongwen Lyu, Yuqing Song, Zhe Liu)</author>
      <guid isPermaLink="false">2510.11005v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning in Astrophysics</title>
      <link>http://arxiv.org/abs/2510.10713v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript submitted to Annual Review of Astronomy and Astrophysics  for Volume 64. This is the authors' version. Revisions and the final version  will be available at https://www.annualreviews.org/content/journals/astro&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度学习为天文学提供了多样化视角，通过将物理对称性、守恒定律和微分方程编码到网络架构中，扩展了数据分析工具包。尽管面临未标记数据庞大而确认样本稀少的挑战，深度学习仍通过架构设计整合领域知识，为天文学提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;深度学习在天文学中产生了多样化视角，支持者和怀疑者之间的持续讨论促使了这篇综述。天文学提供了独特机会，可以通过编码物理对称性、守恒定律和微分方程直接到架构中，创建能推广到训练数据之外的模型。&lt;h4&gt;目的&lt;/h4&gt;检查神经网络如何补充经典统计学，扩展现代调查的数据分析工具包；评估深度学习方法提供真正进步的领域和需要仔细审查的主张；展示如何通过架构设计将领域知识整合到深度学习中。&lt;h4&gt;方法&lt;/h4&gt;通过将物理对称性、守恒定律和微分方程直接编码到网络架构中，创建能推广到训练数据之外的模型；通过架构设计将领域知识整合到模型中，使模型朝着物理上有意义的解决方案发展；评估深度学习在天文学不同应用领域的效果。&lt;h4&gt;主要发现&lt;/h4&gt;神经架构通过将物理对称性和守恒定律编码到网络结构中，克服了可扩展性、表达能力和数据效率之间的权衡；基于模拟的推理和异常检测能够从复杂、非高斯分布中提取信息，使宇宙学场级分析和罕见现象的系统性发现成为可能；多尺度神经建模弥合了天文模拟中的分辨率差距，从高保真运行中学习有效的次网格物理；强化学习用于望远镜操作，基础模型从最少示例中学习，大型语言模型代理用于研究自动化等新兴范式显示出潜力。&lt;h4&gt;结论&lt;/h4&gt;深度学习通过将领域知识整合到架构设计中，为天文学提供了新的数据分析工具，能够处理大规模数据并从有限标记数据中学习。尽管面临数据挑战，但深度学习方法在多个天文应用领域显示出实质性进步，特别是在处理复杂分布、弥合分辨率差距和研究自动化方面。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在天文学中产生了多样化视角，支持者和怀疑者之间的持续讨论促使了这篇综述。我们检查了神经网络如何补充经典统计学，扩展了现代调查的数据分析工具包。天文学提供了独特机会，可以通过编码物理对称性、守恒定律和微分方程直接到架构中，创建能推广到训练数据之外的模型。然而挑战仍然存在，因为未标记观测数据数量达数十亿，而具有已知属性的确认样本仍然稀少且昂贵。这篇综述展示了深度学习如何通过架构设计整合领域知识，内置假设引导模型朝向物理上有意义的解决方案。我们评估了这些方法在哪些方面提供了真正进步，以及哪些主张需要仔细审查。神经架构通过将物理对称性和守恒定律编码到网络结构中，克服了可扩展性、表达能力和数据效率之间的权衡，能够从有限的标记数据中学习。基于模拟的推理和异常检测从复杂、非高斯分布中提取信息，在这些分布中分析似然失败，使宇宙学场级分析和罕见现象的系统性发现成为可能。多尺度神经建模弥合了天文模拟中的分辨率差距，从昂贵的高保真运行中学习有效的次网格物理，以增强直接计算仍然不可行的大体积计算。新兴范式——用于望远镜操作的强化学习，从最少示例中学习的基础模型，以及用于研究自动化的大型语言模型代理——显示出潜力，尽管在天文学应用中仍在发展中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has generated diverse perspectives in astronomy, with ongoingdiscussions between proponents and skeptics motivating this review. We examinehow neural networks complement classical statistics, extending our dataanalytical toolkit for modern surveys. Astronomy offers unique opportunitiesthrough encoding physical symmetries, conservation laws, and differentialequations directly into architectures, creating models that generalize beyondtraining data. Yet challenges persist as unlabeled observations number inbillions while confirmed examples with known properties remain scarce andexpensive. This review demonstrates how deep learning incorporates domainknowledge through architectural design, with built-in assumptions guidingmodels toward physically meaningful solutions. We evaluate where these methodsoffer genuine advances versus claims requiring careful scrutiny. - Neuralarchitectures overcome trade-offs between scalability, expressivity, and dataefficiency by encoding physical symmetries and conservation laws into networkstructure, enabling learning from limited labeled data. - Simulation-basedinference and anomaly detection extract information from complex, non-Gaussiandistributions where analytical likelihoods fail, enabling field-levelcosmological analysis and systematic discovery of rare phenomena. - Multi-scaleneural modeling bridges resolution gaps in astronomical simulations, learningeffective subgrid physics from expensive high-fidelity runs to enhancelarge-volume calculations where direct computation remains prohibitive. -Emerging paradigms-reinforcement learning for telescope operations, foundationmodels learning from minimal examples, and large language model agents forresearch automation-show promise though are still developing in astronomicalapplications.</description>
      <author>example@mail.com (Yuan-Sen Ting)</author>
      <guid isPermaLink="false">2510.10713v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection</title>
      <link>http://arxiv.org/abs/2510.10663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures, project page:  https://fsfm-3c.github.io/fsvfm.html&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出FS-VFM，一个可扩展的自监督预训练框架，用于学习真实人脸图像的基本表示，通过结合掩码图像建模和实例判别，提高人脸安全任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;如何利用大量未标记的真实人脸图像学习鲁棒且可迁移的人脸表示，以提升各种人脸安全任务的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个可扩展的自监督预训练框架，学习真实人脸图像的基本表示，并在多种人脸安全任务上实现更好的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;引入3C学习目标，结合掩码图像建模(MIM)和实例判别(ID)；设计CRFR-P掩码策略；提出可靠的自蒸馏机制建立局部到全局的对应关系；使用普通视觉变压器(ViTs)作为下游任务的通用视觉基础模型；提出FS-Adapter轻量级即插即用瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;在11个公共基准测试上，FS-VFM在各种视觉基础模型中泛化能力更好，包括自然和人脸领域；在不同监督范式和ViT规模上都表现出色；甚至优于最先进的任务特定方法；FS-Adapter提供了出色的效率-性能权衡。&lt;h4&gt;结论&lt;/h4&gt;FS-VFM框架能够有效学习人脸表示，并在各种人脸安全任务上实现更好的泛化性能，代码和模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;利用大量未标记的真实人脸，我们如何学习鲁棒且可迁移的人脸表示来提高各种人脸安全任务的泛化能力？我们首次尝试并提出FS-VFM，一个可扩展的自监督预训练框架，用于学习真实人脸图像的基本表示。我们引入三个学习目标，即3C，协同结合掩码图像建模(MIM)和实例判别(ID)，使FS-VFM能够编码真实人脸的局部模式和全局语义。具体而言，我们为MIM制定了各种面部掩码策略，并设计了一种简单而有效的CRFR-P掩码，明确提示模型追求有意义的区域内一致性和挑战性的区域间连贯性。我们提出了一个可靠的自蒸馏机制，将MIM与ID无缝耦合，建立底层局部到全局的对应关系。预训练后，普通视觉变压器(ViTs)作为下游人脸安全任务的通用视觉基础模型：跨数据集深度伪造检测、跨域人脸防欺骗和未见扩散人脸取证。为了高效迁移预训练的FS-VFM，我们进一步提出FS-Adapter，一种新颖的真实锚点对比目标的轻量级即插即用瓶颈，位于冻结骨干网络之上。在11个公共基准上的广泛实验表明，我们的FS-VFM比各种视觉基础模型泛化能力更好，包括自然和人脸领域，完全、弱和自监督范式，小型、基础和大型的ViT规模，甚至优于最先进的任务特定方法，而FS-Adapter提供了出色的效率-性能权衡。代码和模型可在https://fsfm-3c.github.io/fsvfm.html获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With abundant, unlabeled real faces, how can we learn robust and transferablefacial representations to boost generalization across various face securitytasks? We make the first attempt and propose FS-VFM, a scalable self-supervisedpre-training framework, to learn fundamental representations of real faceimages. We introduce three learning objectives, namely 3C, that synergizemasked image modeling (MIM) and instance discrimination (ID), empowering FS-VFMto encode both local patterns and global semantics of real faces. Specifically,we formulate various facial masking strategies for MIM and devise a simple yeteffective CRFR-P masking, which explicitly prompts the model to pursuemeaningful intra-region Consistency and challenging inter-region Coherency. Wepresent a reliable self-distillation mechanism that seamlessly couples MIM withID to establish underlying local-to-global Correspondence. After pre-training,vanilla vision transformers (ViTs) serve as universal Vision Foundation Modelsfor downstream Face Security tasks: cross-dataset deepfake detection,cross-domain face anti-spoofing, and unseen diffusion facial forensics. Toefficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, alightweight plug-and-play bottleneck atop the frozen backbone with a novelreal-anchor contrastive objective. Extensive experiments on 11 publicbenchmarks demonstrate that our FS-VFM consistently generalizes better thandiverse VFMs, spanning natural and facial domains, fully, weakly, andself-supervised paradigms, small, base, and large ViT scales, and evenoutperforms SOTA task-specific methods, while FS-Adapter offers an excellentefficiency-performance trade-off. The code and models are available onhttps://fsfm-3c.github.io/fsvfm.html.</description>
      <author>example@mail.com (Gaojian Wang, Feng Lin, Tong Wu, Zhisheng Yan, Kui Ren)</author>
      <guid isPermaLink="false">2510.10663v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Equipping Vision Foundation Model with Mixture of Experts for Out-of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2510.10584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地探索了预训练视觉基础模型在分布外检测任务中的应用，发现DINOv2模型无需微调即可提供高度判别性的特征空间，并提出了MoFE模块和Dynamic-β Mixup策略来解决语义空间较大场景中的性能问题。&lt;h4&gt;背景&lt;/h4&gt;预训练视觉基础模型已改变众多计算机视觉任务，它们在学习判别性和可泛化特征方面能力强大，这些特征对分布外检测至关重要，但它们对这一任务的影响尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统性地研究代表性的视觉基础模型在分布外检测任务中的应用和性能。&lt;h4&gt;方法&lt;/h4&gt;研究预训练DINOv2模型在分布外检测中的表现；探索在领域内数据上微调基础模型如何增强分布外检测；提出Mixture of Feature Experts (MoFE)模块将特征划分为子空间；引入Dynamic-β Mixup策略从动态beta分布中采样插值权重。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的DINOv2模型无需在领域内数据上微调就能提供高度判别性的特征空间，性能媲美现有最先进方法；在语义空间较大的场景中，视觉基础模型的性能仍然不令人满意，这是因为类别数量增加导致决策边界复杂化，使优化过程变得复杂。&lt;h4&gt;结论&lt;/h4&gt;MoFE模块和Dynamic-β Mixup策略能有效捕获复杂数据分布并细化决策边界，大量实验证明该方法显著优于基线方法。&lt;h4&gt;翻译&lt;/h4&gt;预训练视觉基础模型已改变许多计算机视觉任务。尽管它们在学习对分布外检测至关重要的判别性和可泛化特征方面能力强大，它们对该任务的影响仍未被充分探索。受此差距启发，我们系统性地研究了代表性的视觉基础模型用于分布外检测。我们的发现表明，预训练的DINOv2模型即使在领域内数据上未进行微调，也能为分布外检测提供高度判别性的特征空间，实现与现有最先进方法相当的性能，而无需复杂设计。除此之外，我们探索了在领域内数据上微调基础模型如何增强分布外检测。然而，我们观察到在语义空间较大的场景中，视觉基础模型的性能仍然不令人满意。这是因为随着类别数量增加，决策边界的复杂度提高，使优化过程变得复杂。为缓解这一问题，我们提出了特征专家混合（MoFE）模块，该模块将特征划分为子空间，有效捕获复杂数据分布并细化决策边界。此外，我们引入了动态-β混合策略，从动态beta分布中采样插值权重。这使模型能够适应不同类别间的不同学习难度，提升更具挑战性类别的特征学习。大量实验证明了我们方法的有效性，显著优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained vision foundation models have transformed many computer visiontasks. Despite their strong ability to learn discriminative and generalizablefeatures crucial for out-of-distribution (OOD) detection, their impact on thistask remains underexplored. Motivated by this gap, we systematicallyinvestigate representative vision foundation models for OOD detection. Ourfindings reveal that a pre-trained DINOv2 model, even without fine-tuning onin-domain (ID) data, naturally provides a highly discriminative feature spacefor OOD detection, achieving performance comparable to existingstate-of-the-art methods without requiring complex designs. Beyond this, weexplore how fine-tuning foundation models on in-domain (ID) data can enhanceOOD detection. However, we observe that the performance of vision foundationmodels remains unsatisfactory in scenarios with a large semantic space. This isdue to the increased complexity of decision boundaries as the number ofcategories grows, which complicates the optimization process. To mitigate this,we propose the Mixture of Feature Experts (MoFE) module, which partitionsfeatures into subspaces, effectively capturing complex data distributions andrefining decision boundaries. Further, we introduce a Dynamic-$\beta$ Mixupstrategy, which samples interpolation weights from a dynamic beta distribution.This adapts to varying levels of learning difficulty across categories,improving feature learning for more challenging categories. Extensiveexperiments demonstrate the effectiveness of our approach, significantlyoutperforming baseline methods.</description>
      <author>example@mail.com (Shizhen Zhao, Jiahui Liu, Xin Wen, Haoru Tan, Xiaojuan Qi)</author>
      <guid isPermaLink="false">2510.10584v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment</title>
      <link>http://arxiv.org/abs/2510.10464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  81 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了首个用于TIPS预后的公开多中心数据集MultiTIPS，并基于此开发了一种新的多模态预后框架，包含双选项分割、多模态交互和多任务预测三个核心模块，解决了当前研究面临的ROI标注量大、单模态方法可靠性差和单终点评估不完整等挑战。&lt;h4&gt;背景&lt;/h4&gt;TIPS是治疗门静脉高压的既定方法，但存在生存结果差异大和频繁出现明显肝性脑病(OHE)的问题，需要准确的术前预后模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种准确、可靠的TIPS预后模型，解决当前研究中存在的ROI标注量大、单模态方法可靠性差和单终点评估不完整等挑战，并提供公开数据集促进该领域研究。&lt;h4&gt;方法&lt;/h4&gt;提出MultiTIPS数据集和一种新的多模态预后框架，该框架包含三个核心模块：(1)双选项分割：结合半监督和基础模型实现有限标注下的鲁棒ROI分割；(2)多模态交互：引入MGRA、POD和CGPE技术实现跨模态特征交互；(3)多任务预测：使用分阶段训练策略同时优化生存、PPG和OHE预测。&lt;h4&gt;主要发现&lt;/h4&gt;在MultiTIPS上的大量实验表明，所提出的方法优于最先进的方法，具有强大的跨域泛化性和可解释性，显示出临床应用的潜力。&lt;h4&gt;结论&lt;/h4&gt;MultiTIPS数据集和所提出的多模态预后框架为TIPS预后评估提供了有效解决方案，有望在临床实践中应用。&lt;h4&gt;翻译&lt;/h4&gt;经颈静脉肝内门体分流术(TIPS)是治疗门静脉高压的既定方法，但提供不同的生存结果和频繁明显的肝性脑病(OHE)，表明需要准确的术前预后建模。当前研究通常从术前CT图像或临床特征构建机器学习模型，但面临三个关键挑战：(1)劳动密集型的感兴趣区域(ROI)标注，(2)单模态方法的可靠性和泛化能力差，(3)单终点预测评估不完整。此外，缺乏公开可访问的数据集限制了该领域的研究。因此，我们提出了MultiTIPS，这是首个用于TIPS预后的公共多中心数据集，并基于它提出了一个新的多模态预后框架。该框架包含三个核心模块：(1)双选项分割，结合半监督和基于基础模型的流程，实现有限标注下的鲁棒ROI分割并促进后续特征提取；(2)多模态交互，引入多粒度放射组学注意力(MGRA)、渐进正交解耦(POD)和临床引导预后增强(CGPE)技术，实现跨模态特征交互和互补表示集成，从而提高模型准确性和鲁棒性；(3)多任务预测，使用分阶段训练策略对生存、门静脉压力梯度(PPG)和OHE预测进行稳定优化，实现全面预后评估。在MultiTIPS上的大量实验证明了所提出方法优于最先进的方法，同时具有强大的跨域泛化性和可解释性，表明其临床应用前景。该数据集和代码是公开可用的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transjugular intrahepatic portosystemic shunt (TIPS) is an establishedprocedure for portal hypertension, but provides variable survival outcomes andfrequent overt hepatic encephalopathy (OHE), indicating the necessity ofaccurate preoperative prognostic modeling. Current studies typically buildmachine learning models from preoperative CT images or clinicalcharacteristics, but face three key challenges: (1) labor-intensiveregion-of-interest (ROI) annotation, (2) poor reliability and generalizabilityof unimodal methods, and (3) incomplete assessment from single-endpointprediction. Moreover, the lack of publicly accessible datasets constrainsresearch in this field. Therefore, we present MultiTIPS, the first publicmulti-center dataset for TIPS prognosis, and propose a novel multimodalprognostic framework based on it. The framework comprises three core modules:(1) dual-option segmentation, which integrates semi-supervised and foundationmodel-based pipelines to achieve robust ROI segmentation with limitedannotations and facilitate subsequent feature extraction; (2) multimodalinteraction, where three techniques, multi-grained radiomics attention (MGRA),progressive orthogonal disentanglement (POD), and clinically guided prognosticenhancement (CGPE), are introduced to enable cross-modal feature interactionand complementary representation integration, thus improving model accuracy androbustness; and (3) multi-task prediction, where a staged training strategy isused to perform stable optimization of survival, portal pressure gradient(PPG), and OHE prediction for comprehensive prognostic assessment. Extensiveexperiments on MultiTIPS demonstrate the superiority of the proposed methodover state-of-the-art approaches, along with strong cross-domain generalizationand interpretability, indicating its promise for clinical application. Thedataset and code are available.</description>
      <author>example@mail.com (Junhao Dong, Dejia Liu, Ruiqi Ding, Zongxing Chen, Yingjie Huang, Zhu Meng, Jianbo Zhao, Zhicheng Zhao, Fei Su)</author>
      <guid isPermaLink="false">2510.10464v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Vision4PPG: Emergent PPG Analysis Capability of Vision Foundation Models for Vital Signs like Blood Pressure</title>
      <link>http://arxiv.org/abs/2510.10366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BHI abstract extended&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了使用视觉基础模型(VFM)处理光电容积脉搏波描记术(PPG)信号的可能性，发现将一维PPG信号转换为二维图像表示后，视觉模型在血压估计等多种生理任务上达到最先进性能，并展示了良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;光电容积脉搏波描记术(PPG)传感器在可穿戴和临床设备中能够以非侵入式和实时方式提供有价值的生理信息。目前通常使用专门的基础模型或重新利用的时间序列基础模型来基准化生理任务。&lt;h4&gt;目的&lt;/h4&gt;研究视觉基础模型(VFM)在PPG信号处理中的应用潜力，评估其在各种生理任务中的性能，并与现有时间序列基础模型进行比较。&lt;h4&gt;方法&lt;/h4&gt;将一维PPG信号转换为二维图像表示，如短时傅里叶变换(STFT)，然后使用最新的视觉基础模型(如DINOv3和SIGLIP-2)进行微调。采用参数高效微调(PEFT)技术提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;视觉基础模型(VFM)在PPG信号处理中表现优异，特别是在血压估计任务上达到了最先进(SOTA)的性能。该方法在其他生命体征和血液实验室测量任务中也取得了有希望的结果，并且可以推广到STFT相位和递归图等其他2D输入表示。&lt;h4&gt;结论&lt;/h4&gt;提出的Vision4PPG方法解锁了一类新的基础模型用于PPG处理，能够实现最先进性能并具有良好的泛化能力。这些工具为临床科学家提供了计算效率高的新选择，通过参数高效微调技术实现。&lt;h4&gt;翻译&lt;/h4&gt;光电容积脉搏波描记术(PPG)传感器在可穿戴和临床设备中以非侵入式和实时方式提供有价值的生理洞见。专门的基础模型(FM)或重新利用的时间序列FM被用于基准化生理任务。我们对微调FM的实验表明，视觉FM(VFM)也可用于此目的，事实上在许多任务上(特别是血压估计)出乎意料地达到了最先进(SOTA)性能。我们通过简单地将一维PPG信号转换为类图像的二维表示(如短时傅里叶变换STFT)来利用VFMs。使用最新的VFMs(如DINOv3和SIGLIP-2)，我们在其他生命体征和血液实验室测量任务中也取得了有希望的性能。我们的提案Vision4PPG解锁了一类新的FM，实现了SOTA性能，并能显著推广到其他2D输入表示，包括STFT相位和递归图。我们的工作通过进行全面研究、将其与最先进的时间序列FM进行比较，并在六个额外任务上报告结果，改进了先前关于视觉模型用于PPG的研究。因此，我们为临床科学家提供了一套新的强大工具，由于参数高效微调(PEFT)技术，这些工具也具有计算效率高的特点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmography (PPG) sensor in wearable and clinical devices providesvaluable physiological insights in a non-invasive and real-time fashion.Specialized Foundation Models (FM) or repurposed time-series FMs are used tobenchmark physiological tasks. Our experiments with fine-tuning FMs reveal thatVision FM (VFM) can also be utilized for this purpose and, in fact,surprisingly leads to state-of-the-art (SOTA) performance on many tasks,notably blood pressure estimation. We leverage VFMs by simply transformingone-dimensional PPG signals into image-like two-dimensional representations,such as the Short-Time Fourier transform (STFT). Using the latest VFMs, such asDINOv3 and SIGLIP-2, we achieve promising performance on other vital signs andblood lab measurement tasks as well. Our proposal, Vision4PPG, unlocks a newclass of FMs to achieve SOTA performance with notable generalization to other2D input representations, including STFT phase and recurrence plots. Our workimproves upon prior investigations of vision models for PPG by conducting acomprehensive study, comparing them to state-of-the-art time-series FMs, anddemonstrating the general PPG processing ability by reporting results on sixadditional tasks. Thus, we provide clinician-scientists with a new set ofpowerful tools that is also computationally efficient, thanks toParameter-Efficient Fine-Tuning (PEFT) techniques.</description>
      <author>example@mail.com (Saurabh Kataria, Ayca Ermis, Lovely Yeswanth Panchumarthi, Minxiao Wang, Xiao Hu)</author>
      <guid isPermaLink="false">2510.10366v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs</title>
      <link>http://arxiv.org/abs/2510.10329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探索了结合预训练语音编码器和大型语言模型的端到端架构，用于同时进行语音识别和语音翻译，在英语到德语任务上取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;语音翻译是将一种语言的语音信号转换为另一种语言对应文本的机器翻译任务，存在传统级联方法和端到端方法两种不同途径。&lt;h4&gt;目的&lt;/h4&gt;探索一种结合预训练语音编码器和大型语言模型的端到端架构，实现同时进行自动语音识别和语音翻译。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的语音编码器和大型语言模型构建端到端架构，并在英语到德语的语言对上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;最佳模型不仅比SeamlessM4T大型基础端到端多模态翻译模型取得更好的翻译结果，还能匹配使用Whisper和NLLB的级联系统性能，在COMET-DA22指标上获得高达8%的分数提升。&lt;h4&gt;结论&lt;/h4&gt;结合预训练语音编码器和大型语言模型的端到端架构在语音翻译任务上表现出色，超越了现有模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;语音翻译是一种机器翻译任务，涉及将一种语言的语音信号转换为另一种语言的对应文本；该任务有两种不同的方法，即传统的级联方法和最近的端到端方法。本文探索了结合预训练语音编码器和大型语言模型的端到端架构，用于同时执行自动语音识别和语音翻译。英语到德语语言对的实验表明，我们的最佳模型不仅能够比SeamlessM4T（大型基础端到端多模态翻译模型）获得更好的翻译结果，还能匹配使用Whisper和NLLB的级联系统性能，在COMET-DA22指标上获得高达8%的分数提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech Translation (ST) is a machine translation task that involvesconverting speech signals from one language to the corresponding text inanother language; this task has two different approaches, namely thetraditional cascade and the more recent end-to-end. This paper explores acombined end-to-end architecture of pre-trained speech encoders and LargeLanguage Models (LLMs) for performing both Automatic Speech Recognition (ASR)and ST simultaneously. Experiments with the English-to-German language pairshow that our best model not only can achieve better translation results thanSeamlessM4T, a large foundational end-to-end, multi-modal translation model,but can also match the performance of a cascaded system with Whisper and NLLB,with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.</description>
      <author>example@mail.com (Nam Luu, Ondřej Bojar)</author>
      <guid isPermaLink="false">2510.10329v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>From Generic to Specialized: A Subspecialty Diagnostic System Powered by Self-Supervised Learning for Cervical Histopathology</title>
      <link>http://arxiv.org/abs/2510.10196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了名为CerS-Path的宫颈癌亚专科病理诊断系统，通过两个预训练阶段构建，支持八种诊断功能，在前瞻性测试中达到99.38%的筛查敏感性，展示了优秀的泛化能力和临床应用潜力。&lt;h4&gt;背景&lt;/h4&gt;宫颈癌是一种主要恶性肿瘤，需要广泛复杂的组织病理学评估。现有深度学习模型缺乏准确性和泛化能力，通用基础模型在捕获亚专科特定特征和任务适应性方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对宫颈癌病理的诊断系统，提高宫颈癌病理诊断的准确性和泛化能力，解决现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;开发CerS-Path诊断系统，通过两个协同预训练阶段：1)自监督学习使用约1.9亿个组织块构建宫颈特异性特征提取器；2)多模态增强使用250万对图像-文本对进行增强，然后整合多个下游诊断功能。&lt;h4&gt;主要发现&lt;/h4&gt;CerS-Path在范围和临床适用性方面超越了以前的基础模型，全面评估显示在宫颈癌病理方面取得显著进展，在五个中心对3173例病例的前瞻性测试中保持99.38%的筛查敏感性和优秀泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CerS-Path在亚专科诊断转化和宫颈癌筛查方面具有潜力，代表了宫颈癌病理诊断的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;宫颈癌仍然是一种主要恶性肿瘤，需要广泛而复杂的组织病理学评估和全面的支持工具。尽管深度学习显示出前景，但这些模型仍然缺乏准确性和泛化能力。通用基础模型提供了更广泛的覆盖范围，但在捕获亚专科特定特征和任务适应性方面仍然存在局限。我们引入了宫颈亚专科病理学(CerS-Path)诊断系统，通过两个协同的预训练阶段开发：自监督学习来自约14万张幻灯片的1.9亿个组织块以构建宫颈特异性特征提取器，以及使用250万对图像-文本对进行多模态增强，随后整合多个下游诊断功能。支持包括罕见癌症分类和多模态问答在内的八种诊断功能，CerS-Path在范围和临床适用性方面超越了以前的基础模型。全面评估显示在宫颈癌病理方面取得了显著进展，在五个中心对3173例病例的前瞻性测试中保持了99.38%的筛查敏感性和优秀的泛化能力，突显了其在亚专科诊断转化和宫颈癌筛查方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cervical cancer remains a major malignancy, necessitating extensive andcomplex histopathological assessments and comprehensive support tools. Althoughdeep learning shows promise, these models still lack accuracy andgeneralizability. General foundation models offer a broader reach but remainlimited in capturing subspecialty-specific features and task adaptability. Weintroduce the Cervical Subspecialty Pathology (CerS-Path) diagnostic system,developed through two synergistic pretraining stages: self-supervised learningon approximately 190 million tissue patches from 140,000 slides to build acervical-specific feature extractor, and multimodal enhancement with 2.5million image-text pairs, followed by integration with multiple downstreamdiagnostic functions. Supporting eight diagnostic functions, including rarecancer classification and multimodal Q&amp;A, CerS-Path surpasses prior foundationmodels in scope and clinical applicability. Comprehensive evaluationsdemonstrate a significant advance in cervical pathology, with prospectivetesting on 3,173 cases across five centers maintaining 99.38% screeningsensitivity and excellent generalizability, highlighting its potential forsubspecialty diagnostic translation and cervical cancer screening.</description>
      <author>example@mail.com (Yizhi Wang, Li Chen, Qiang Huang, Tian Guan, Xi Deng, Zhiyuan Shen, Jiawen Li, Xinrui Chen, Bin Hu, Xitong Ling, Taojie Zhu, Zirui Huang, Deshui Yu, Yan Liu, Jiurun Chen, Lianghui Zhu, Qiming He, Yiqing Liu, Diwei Shi, Hanzhong Liu, Junbo Hu, Hongyi Gao, Zhen Song, Xilong Zhao, Chao He, Ming Zhao, Yonghong He)</author>
      <guid isPermaLink="false">2510.10196v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2510.10163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SparseUWSeg是一个新型框架，通过主动采样策略和混合方法解决水下图像语义分割中稀疏点标注的挑战，实现了比现有方法更高的分割精度。&lt;h4&gt;背景&lt;/h4&gt;语义分割对自动化水下图像分析和生态监测至关重要，但细粒度的水下场景分析仍是开放问题，获取密集专家标注标签成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决水下图像语义分割中稀疏点标注的选择和传播问题，提高分割模型的性能。&lt;h4&gt;方法&lt;/h4&gt;SparseUWSeg采用主动采样策略指导标注者选择有价值的点，并结合SAM2和基于超像素方法的混合技术传播稀疏标签。&lt;h4&gt;主要发现&lt;/h4&gt;在两个不同水下数据集上，SparseUWSeg相比最先进方法实现了最高5%的mIoU提升。&lt;h4&gt;结论&lt;/h4&gt;SparseUWSeg框架和其集成的交互式标注工具使生态研究人员能够高效利用基础模型和计算机视觉技术生成高质量分割掩模。&lt;h4&gt;翻译&lt;/h4&gt;语义分割对自动化水下图像分析和生态监测至关重要。不幸的是，即使对于最先进的分割模型，细粒度的水下场景分析仍然是一个开放问题。获取密集的、专家标注的分割标签成本很高，阻碍了该领域模型的监督学习。虽然稀疏点标签更容易获取，但它们在标注哪些点和如何传播稀疏信息方面带来了挑战。我们提出了SparseUWSeg，一个解决这两个问题的新型框架。SparseUWSeg采用主动采样策略指导标注者，最大化其点标签的价值。然后，它使用结合SAM2和基于超像素方法优点的混合方法传播这些稀疏标签。在两个不同水下数据集上的实验证明了SparseUWSeg相比最先进方法的优势，相比D+NN方法实现了最高5%的mIoU提升。我们的主要贡献是设计并发布了一个简单但有效的交互式标注工具，整合了我们的算法。它使生态研究人员能够利用基础模型和计算机视觉高效生成高质量的分割掩模来处理他们的数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation is essential to automate underwater imagery analysiswith ecology monitoring purposes. Unfortunately, fine grained underwater sceneanalysis is still an open problem even for top performing segmentation models.The high cost of obtaining dense, expert-annotated, segmentation labels hindersthe supervision of models in this domain. While sparse point-labels are easierto obtain, they introduce challenges regarding which points to annotate and howto propagate the sparse information. We present SparseUWSeg, a novel frameworkthat addresses both issues. SparseUWSeg employs an active sampling strategy toguide annotators, maximizing the value of their point labels. Then, itpropagates these sparse labels with a hybrid approach leverages both the bestof SAM2 and superpixel-based methods. Experiments on two diverse underwaterdatasets demonstrate the benefits of SparseUWSeg over state-of-the-artapproaches, achieving up to +5\% mIoU over D+NN. Our main contribution is thedesign and release of a simple but effective interactive annotation tool,integrating our algorithms. It enables ecology researchers to leveragefoundation models and computer vision to efficiently generate high-qualitysegmentation masks to process their data.</description>
      <author>example@mail.com (César Borja, Carlos Plou, Rubén Martinez-Cantín, Ana C. Murillo)</author>
      <guid isPermaLink="false">2510.10163v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework</title>
      <link>http://arxiv.org/abs/2510.10084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于视觉基础模型的新框架，能够通过将离散遥感图像转换为连续视频序列，实现大规模滑坡疤痕时空演化的连续追踪，为滑坡早期预警和灾害评估提供了有效工具。&lt;h4&gt;背景&lt;/h4&gt;追踪大规模滑坡疤痕的时空演化对于理解演化机制和破坏前兆、实现有效预警至关重要。然而，现有研究多关注单阶段或破坏前后的双阶段滑坡识别，难以追踪滑坡疤痕的时空演化过程。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以追踪滑坡疤痕时空演化的问题，提出一个新框架用于追踪大规模滑坡疤痕的时空演化。&lt;h4&gt;方法&lt;/h4&gt;使用视觉基础模型，将离散的光学遥感图像重建为连续的视频序列，使专为视频分割开发的视觉基础模型可用于追踪滑坡疤痕演化。该框架在知识引导、自动传播和交互式精炼的范式中运行，确保滑坡疤痕的连续和准确识别。&lt;h4&gt;主要发现&lt;/h4&gt;该框架通过白格滑坡和色拉滑坡(2017-2025)两个案例得到验证，能够连续追踪滑坡疤痕，捕捉对预警至关重要的破坏前兆，以及对评估次生灾害和长期稳定性至关重要的破坏后演化。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为滑坡疤痕的时空演化追踪提供了有效方法，有助于早期预警和灾害评估。&lt;h4&gt;翻译&lt;/h4&gt;追踪大规模滑坡疤痕的时空演化对于理解演化机制和破坏前兆、实现有效预警至关重要。然而，大多数现有研究只关注单阶段或破坏前后的双阶段滑坡识别。虽然这些方法能够确定破坏后的滑坡边界，但难以追踪滑坡疤痕的时空演化。为解决这一问题，本研究提出了一种新的通用框架，使用视觉基础模型追踪大规模滑坡疤痕的时空演化。该框架的关键思路是将离散的光学遥感图像重建为连续的视频序列。这种转换使得专为视频分割开发的视觉基础模型可以用于追踪滑坡疤痕的演化。该框架在知识引导、自动传播和交互式精炼的范式中运行，以确保滑坡疤痕的连续和准确识别。该框架已通过两个代表性案例的应用得到验证：破坏后的白格滑坡和活跃的色拉滑坡(2017-2025)。结果表明，所提出的框架能够连续追踪滑坡疤痕，捕捉对预警至关重要的破坏前兆，以及对评估次生灾害和长期稳定性至关重要的破坏后演化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking the spatiotemporal evolution of large-scale landslide scars iscritical for understanding the evolution mechanisms and failure precursors,enabling effective early-warning. However, most existing studies have focusedon single-phase or pre- and post-failure dual-phase landslide identification.Although these approaches delineate post-failure landslide boundaries, it ischallenging to track the spatiotemporal evolution of landslide scars. Toaddress this problem, this study proposes a novel and universal framework fortracking the spatiotemporal evolution of large-scale landslide scars using avision foundation model. The key idea behind the proposed framework is toreconstruct discrete optical remote sensing images into a continuous videosequence. This transformation enables a vision foundation model, which isdeveloped for video segmentation, to be used for tracking the evolution oflandslide scars. The proposed framework operates within a knowledge-guided,auto-propagation, and interactive refinement paradigm to ensure the continuousand accurate identification of landslide scars. The proposed framework wasvalidated through application to two representative cases: the post-failureBaige landslide and the active Sela landslide (2017-2025). Results indicatethat the proposed framework enables continuous tracking of landslide scars,capturing both failure precursors critical for early warning and post-failureevolution essential for assessing secondary hazards and long-term stability.</description>
      <author>example@mail.com (Meijun Zhou, Gang Mei, Zhengjing Ma, Nengxiong Xu, Jianbing Peng)</author>
      <guid isPermaLink="false">2510.10084v1</guid>
      <pubDate>Tue, 14 Oct 2025 18:18:22 +0800</pubDate>
    </item>
    <item>
      <title>Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow</title>
      <link>http://arxiv.org/abs/2510.05836v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV' 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Flow4Agent框架，通过引入光流运动先验来促进基于LLM的长视频理解，解决了长视频时空内容冗余和MLLMs上下文长度有限的问题。&lt;h4&gt;背景&lt;/h4&gt;长视频理解一直是一个具有挑战性的问题，因为时空内容中存在大量冗余，同时多模态大语言模型(MLLMs)的有限上下文长度进一步加剧了这一挑战。&lt;h4&gt;目的&lt;/h4&gt;解决长视频理解中的冗余问题，提出一种新颖的框架，利用光流运动先验来增强LLM对长视频的理解能力。&lt;h4&gt;方法&lt;/h4&gt;Flow4Agent框架包含两个核心模块：1) 时间粒度优化(TGO)自适应细化帧级层次结构，利用粗略光流先验分组相似视觉内容，再应用语义先验过滤无关场景信息；2) 运动标记剪枝(MTP)使用细粒度光流信息剪枝高冗余视频标记，进一步优化帧内视觉表示。&lt;h4&gt;主要发现&lt;/h4&gt;Flow4Agent在多个视频MLLM基准测试中优于现有方法，特别是在小时级视频理解任务中表现突出：在Video-MME上达到64.7%，在MLVU上达到71.4%，在LongVideoBench上达到60.4%。&lt;h4&gt;结论&lt;/h4&gt;Flow4Agent通过引入光流运动先验，有效解决了长视频理解中的时空冗余问题，在长视频理解任务中取得了显著性能提升。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解一直是一个具有挑战性的问题，因为时空内容中存在显著的冗余。多模态大语言模型(MLLMs)的有限上下文长度进一步加剧了这一挑战。为解决这一问题，许多先前的工作尝试提取关键视频信息，其中'关键'通常是语义感知的，并严重依赖CLIP模型作为先验。在本文中，我们提出了Flow4Agent，一个新颖的框架，开创性地引入光流运动先验来促进基于LLM的长视频理解。Flow4Agent通过两个核心模块在时空层面减轻长视频的冗余：时间粒度优化(TGO)自适应地细化帧级层次结构，首先利用粗略光流先验分组相似的视觉内容，然后应用语义先验过滤掉高度不相关的场景信息。运动标记剪枝(MTP)进一步细化帧内视觉表示，使用细粒度光流信息剪枝高冗余的视频标记。大量实验表明，我们的Flow4Agent在广泛的视频MLLM基准测试中优于现有方法，特别是在小时级视频理解任务中，在Video-MME上达到64.7%，在MLVU上达到71.4%，在LongVideoBench上达到60.4%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-form video understanding has always been a challenging problem due tothe significant redundancy in both temporal and spatial contents. Thischallenge is further exacerbated by the limited context length of MultimodalLarge Language Models (MLLMs). To address this issue, many previous works haveattempted to extract key video information, where the "key" is typicallysemantic-aware and heavily dependent on the CLIP model as prior. In this paper,we propose Flow4Agent, a novel framework that pioneeringly incorporates motionpriors from optical flow to facilitate LLM-based long video understanding.Flow4Agent mitigates the redundancy in long videos at both temporal and spatiallevels through two core modules: Temporal Granularity Optimization (TGO)adaptively refines framelevel hierarchies, which first leverages coarse flowpriors to group similar visual contents and then applies semantic priors tofilter out highly irrelevant scene information. Motion Token Pruning (MTP)further refines the intra-frame visual representations, pruning high-redundancyvideo tokens using fine-grained optical flow information. Extensive experimentsdemonstrate that our Flow4Agent outperforms existing methods across a widerange of video MLLM benchmarks, especially for hour-level video understandingtasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.</description>
      <author>example@mail.com (Ruyang Liu, Shangkun Sun, Haoran Tang, Ge Li, Wei Gao)</author>
      <guid isPermaLink="false">2510.05836v1</guid>
      <pubDate>Mon, 13 Oct 2025 14:01:29 +0800</pubDate>
    </item>
  <item>
      <title>Long-tailed Recognition with Model Rebalancing</title>
      <link>http://arxiv.org/abs/2510.08177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MORE的新型框架，通过直接重平衡模型参数空间来解决长尾识别问题，显著提高了模型对尾部类别的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;长尾识别在深度学习和基础模型下游微调中普遍存在且具有挑战性，偏斜的类别分布通常阻碍模型对尾部类别的泛化。&lt;h4&gt;目的&lt;/h4&gt;深入研究长尾情境下基本模型容量的影响，并提出一种有效的不平衡缓解方法。&lt;h4&gt;方法&lt;/h4&gt;提出MORE框架，引入低秩参数组件在定制化损失和正弦重加权计划指导下调解参数空间分配，不增加模型复杂度或推理成本。&lt;h4&gt;主要发现&lt;/h4&gt;MORE在多样化的长尾基准测试上显著提高了泛化能力，特别是对尾部类别，并能有效补充现有不平衡缓解方法。&lt;h4&gt;结论&lt;/h4&gt;MORE在长尾设置中可作为强大的即插即用模块，有效解决长尾识别问题。&lt;h4&gt;翻译&lt;/h4&gt;长尾识别在深度学习和基础模型的下游微调中普遍存在且具有挑战性，因为偏斜的类别分布通常阻碍模型对尾部类别的泛化能力。尽管之前从数据增强、损失重平衡和解耦训练等角度的方法有潜力，但在多标签长尾识别等广泛场景中取得一致的改进仍然困难。本研究深入探讨长尾情境下基本模型容量的影响，并提出一种名为MORE的新型框架，通过直接重平衡模型参数空间来缓解不平衡问题。具体而言，MORE引入低秩参数组件，在定制化损失和正弦重加权计划的指导下调解参数空间分配，同时不增加整体模型复杂度或推理成本。在多样化的长尾基准测试上进行的广泛实验（涵盖多类和多标签任务）表明，MORE显著提高了泛化能力，特别是对尾部类别，并有效补充了现有的不平衡缓解方法。这些结果突显了MORE在长尾设置中作为强大即插即用模块的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-tailed recognition is ubiquitous and challenging in deep learning andeven in the downstream finetuning of foundation models, since the skew classdistribution generally prevents the model generalization to the tail classes.Despite the promise of previous methods from the perspectives of dataaugmentation, loss rebalancing and decoupled training etc., consistentimprovement in the broad scenarios like multi-label long-tailed recognition isdifficult. In this study, we dive into the essential model capacity impactunder long-tailed context, and propose a novel framework, Model Rebalancing(MORE), which mitigates imbalance by directly rebalancing the model's parameterspace. Specifically, MORE introduces a low-rank parameter component to mediatethe parameter space allocation guided by a tailored loss and sinusoidalreweighting schedule, but without increasing the overall model complexity orinference costs. Extensive experiments on diverse long-tailed benchmarks,spanning multi-class and multi-label tasks, demonstrate that MORE significantlyimproves generalization, particularly for tail classes, and effectivelycomplements existing imbalance mitigation methods. These results highlightMORE's potential as a robust plug-and-play module in long-tailed settings.</description>
      <author>example@mail.com (Jiaan Luo, Feng Hong, Qiang Hu, Xiaofeng Cao, Feng Liu, Jiangchao Yao)</author>
      <guid isPermaLink="false">2510.08177v1</guid>
      <pubDate>Mon, 13 Oct 2025 14:01:29 +0800</pubDate>
    </item>
    </channel>
</rss>